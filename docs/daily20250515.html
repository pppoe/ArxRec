<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250514.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or\n  Robot Hardware", "author": "Justin Yu and Letian Fu and Huang Huang and Karim El-Refai and Rares Andrei Ambrus and Richard Cheng and Muhammad Zubair Irshad and Ken Goldberg", "abstract": "  Scaling robot learning requires vast and diverse datasets. Yet the prevailing\ndata collection paradigm-human teleoperation-remains costly and constrained by\nmanual effort and physical robot access. We introduce Real2Render2Real (R2R2R),\na novel approach for generating robot training data without relying on object\ndynamics simulation or teleoperation of robot hardware. The input is a\nsmartphone-captured scan of one or more objects and a single video of a human\ndemonstration. R2R2R renders thousands of high visual fidelity robot-agnostic\ndemonstrations by reconstructing detailed 3D object geometry and appearance,\nand tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to\nenable flexible asset generation and trajectory synthesis for both rigid and\narticulated objects, converting these representations to meshes to maintain\ncompatibility with scalable rendering engines like IsaacLab but with collision\nmodeling off. Robot demonstration data generated by R2R2R integrates directly\nwith models that operate on robot proprioceptive states and image observations,\nsuch as vision-language-action models (VLA) and imitation learning policies.\nPhysical experiments suggest that models trained on R2R2R data from a single\nhuman demonstration can match the performance of models trained on 150 human\nteleoperation demonstrations. Project page: https://real2render2real.com\n", "link": "http://arxiv.org/abs/2505.09601v1", "date": "2025-05-14", "relevancy": 3.0887, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6274}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6254}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real2Render2Real%3A%20Scaling%20Robot%20Data%20Without%20Dynamics%20Simulation%20or%0A%20%20Robot%20Hardware&body=Title%3A%20Real2Render2Real%3A%20Scaling%20Robot%20Data%20Without%20Dynamics%20Simulation%20or%0A%20%20Robot%20Hardware%0AAuthor%3A%20Justin%20Yu%20and%20Letian%20Fu%20and%20Huang%20Huang%20and%20Karim%20El-Refai%20and%20Rares%20Andrei%20Ambrus%20and%20Richard%20Cheng%20and%20Muhammad%20Zubair%20Irshad%20and%20Ken%20Goldberg%0AAbstract%3A%20%20%20Scaling%20robot%20learning%20requires%20vast%20and%20diverse%20datasets.%20Yet%20the%20prevailing%0Adata%20collection%20paradigm-human%20teleoperation-remains%20costly%20and%20constrained%20by%0Amanual%20effort%20and%20physical%20robot%20access.%20We%20introduce%20Real2Render2Real%20%28R2R2R%29%2C%0Aa%20novel%20approach%20for%20generating%20robot%20training%20data%20without%20relying%20on%20object%0Adynamics%20simulation%20or%20teleoperation%20of%20robot%20hardware.%20The%20input%20is%20a%0Asmartphone-captured%20scan%20of%20one%20or%20more%20objects%20and%20a%20single%20video%20of%20a%20human%0Ademonstration.%20R2R2R%20renders%20thousands%20of%20high%20visual%20fidelity%20robot-agnostic%0Ademonstrations%20by%20reconstructing%20detailed%203D%20object%20geometry%20and%20appearance%2C%0Aand%20tracking%206-DoF%20object%20motion.%20R2R2R%20uses%203D%20Gaussian%20Splatting%20%283DGS%29%20to%0Aenable%20flexible%20asset%20generation%20and%20trajectory%20synthesis%20for%20both%20rigid%20and%0Aarticulated%20objects%2C%20converting%20these%20representations%20to%20meshes%20to%20maintain%0Acompatibility%20with%20scalable%20rendering%20engines%20like%20IsaacLab%20but%20with%20collision%0Amodeling%20off.%20Robot%20demonstration%20data%20generated%20by%20R2R2R%20integrates%20directly%0Awith%20models%20that%20operate%20on%20robot%20proprioceptive%20states%20and%20image%20observations%2C%0Asuch%20as%20vision-language-action%20models%20%28VLA%29%20and%20imitation%20learning%20policies.%0APhysical%20experiments%20suggest%20that%20models%20trained%20on%20R2R2R%20data%20from%20a%20single%0Ahuman%20demonstration%20can%20match%20the%20performance%20of%20models%20trained%20on%20150%20human%0Ateleoperation%20demonstrations.%20Project%20page%3A%20https%3A//real2render2real.com%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal2Render2Real%253A%2520Scaling%2520Robot%2520Data%2520Without%2520Dynamics%2520Simulation%2520or%250A%2520%2520Robot%2520Hardware%26entry.906535625%3DJustin%2520Yu%2520and%2520Letian%2520Fu%2520and%2520Huang%2520Huang%2520and%2520Karim%2520El-Refai%2520and%2520Rares%2520Andrei%2520Ambrus%2520and%2520Richard%2520Cheng%2520and%2520Muhammad%2520Zubair%2520Irshad%2520and%2520Ken%2520Goldberg%26entry.1292438233%3D%2520%2520Scaling%2520robot%2520learning%2520requires%2520vast%2520and%2520diverse%2520datasets.%2520Yet%2520the%2520prevailing%250Adata%2520collection%2520paradigm-human%2520teleoperation-remains%2520costly%2520and%2520constrained%2520by%250Amanual%2520effort%2520and%2520physical%2520robot%2520access.%2520We%2520introduce%2520Real2Render2Real%2520%2528R2R2R%2529%252C%250Aa%2520novel%2520approach%2520for%2520generating%2520robot%2520training%2520data%2520without%2520relying%2520on%2520object%250Adynamics%2520simulation%2520or%2520teleoperation%2520of%2520robot%2520hardware.%2520The%2520input%2520is%2520a%250Asmartphone-captured%2520scan%2520of%2520one%2520or%2520more%2520objects%2520and%2520a%2520single%2520video%2520of%2520a%2520human%250Ademonstration.%2520R2R2R%2520renders%2520thousands%2520of%2520high%2520visual%2520fidelity%2520robot-agnostic%250Ademonstrations%2520by%2520reconstructing%2520detailed%25203D%2520object%2520geometry%2520and%2520appearance%252C%250Aand%2520tracking%25206-DoF%2520object%2520motion.%2520R2R2R%2520uses%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520to%250Aenable%2520flexible%2520asset%2520generation%2520and%2520trajectory%2520synthesis%2520for%2520both%2520rigid%2520and%250Aarticulated%2520objects%252C%2520converting%2520these%2520representations%2520to%2520meshes%2520to%2520maintain%250Acompatibility%2520with%2520scalable%2520rendering%2520engines%2520like%2520IsaacLab%2520but%2520with%2520collision%250Amodeling%2520off.%2520Robot%2520demonstration%2520data%2520generated%2520by%2520R2R2R%2520integrates%2520directly%250Awith%2520models%2520that%2520operate%2520on%2520robot%2520proprioceptive%2520states%2520and%2520image%2520observations%252C%250Asuch%2520as%2520vision-language-action%2520models%2520%2528VLA%2529%2520and%2520imitation%2520learning%2520policies.%250APhysical%2520experiments%2520suggest%2520that%2520models%2520trained%2520on%2520R2R2R%2520data%2520from%2520a%2520single%250Ahuman%2520demonstration%2520can%2520match%2520the%2520performance%2520of%2520models%2520trained%2520on%2520150%2520human%250Ateleoperation%2520demonstrations.%2520Project%2520page%253A%2520https%253A//real2render2real.com%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real2Render2Real%3A%20Scaling%20Robot%20Data%20Without%20Dynamics%20Simulation%20or%0A%20%20Robot%20Hardware&entry.906535625=Justin%20Yu%20and%20Letian%20Fu%20and%20Huang%20Huang%20and%20Karim%20El-Refai%20and%20Rares%20Andrei%20Ambrus%20and%20Richard%20Cheng%20and%20Muhammad%20Zubair%20Irshad%20and%20Ken%20Goldberg&entry.1292438233=%20%20Scaling%20robot%20learning%20requires%20vast%20and%20diverse%20datasets.%20Yet%20the%20prevailing%0Adata%20collection%20paradigm-human%20teleoperation-remains%20costly%20and%20constrained%20by%0Amanual%20effort%20and%20physical%20robot%20access.%20We%20introduce%20Real2Render2Real%20%28R2R2R%29%2C%0Aa%20novel%20approach%20for%20generating%20robot%20training%20data%20without%20relying%20on%20object%0Adynamics%20simulation%20or%20teleoperation%20of%20robot%20hardware.%20The%20input%20is%20a%0Asmartphone-captured%20scan%20of%20one%20or%20more%20objects%20and%20a%20single%20video%20of%20a%20human%0Ademonstration.%20R2R2R%20renders%20thousands%20of%20high%20visual%20fidelity%20robot-agnostic%0Ademonstrations%20by%20reconstructing%20detailed%203D%20object%20geometry%20and%20appearance%2C%0Aand%20tracking%206-DoF%20object%20motion.%20R2R2R%20uses%203D%20Gaussian%20Splatting%20%283DGS%29%20to%0Aenable%20flexible%20asset%20generation%20and%20trajectory%20synthesis%20for%20both%20rigid%20and%0Aarticulated%20objects%2C%20converting%20these%20representations%20to%20meshes%20to%20maintain%0Acompatibility%20with%20scalable%20rendering%20engines%20like%20IsaacLab%20but%20with%20collision%0Amodeling%20off.%20Robot%20demonstration%20data%20generated%20by%20R2R2R%20integrates%20directly%0Awith%20models%20that%20operate%20on%20robot%20proprioceptive%20states%20and%20image%20observations%2C%0Asuch%20as%20vision-language-action%20models%20%28VLA%29%20and%20imitation%20learning%20policies.%0APhysical%20experiments%20suggest%20that%20models%20trained%20on%20R2R2R%20data%20from%20a%20single%0Ahuman%20demonstration%20can%20match%20the%20performance%20of%20models%20trained%20on%20150%20human%0Ateleoperation%20demonstrations.%20Project%20page%3A%20https%3A//real2render2real.com%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09601v1&entry.124074799=Read"},
{"title": "Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians", "author": "Ma Changfeng and Bi Ran and Guo Jie and Wang Chongjun and Guo Yanwen", "abstract": "  Current learning-based methods predict NeRF or 3D Gaussians from point clouds\nto achieve photo-realistic rendering but still depend on categorical priors,\ndense point clouds, or additional refinements. Hence, we introduce a novel\npoint cloud rendering method by predicting 2D Gaussians from point clouds. Our\nmethod incorporates two identical modules with an entire-patch architecture\nenabling the network to be generalized to multiple datasets. The module\nnormalizes and initializes the Gaussians utilizing the point cloud information\nincluding normals, colors and distances. Then, splitting decoders are employed\nto refine the initial Gaussians by duplicating them and predicting more\naccurate results, making our methodology effectively accommodate sparse point\nclouds as well. Once trained, our approach exhibits direct generalization to\npoint clouds across different categories. The predicted Gaussians are employed\ndirectly for rendering without additional refinement on the rendered images,\nretaining the benefits of 2D Gaussians. We conduct extensive experiments on\nvarious datasets, and the results demonstrate the superiority and\ngeneralization of our method, which achieves SOTA performance. The code is\navailable at\nhttps://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.\n", "link": "http://arxiv.org/abs/2505.09413v1", "date": "2025-05-14", "relevancy": 3.0788, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6211}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6157}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Point%20Cloud%20Patches%20Rendering%20via%20Splitting%202D%20Gaussians&body=Title%3A%20Sparse%20Point%20Cloud%20Patches%20Rendering%20via%20Splitting%202D%20Gaussians%0AAuthor%3A%20Ma%20Changfeng%20and%20Bi%20Ran%20and%20Guo%20Jie%20and%20Wang%20Chongjun%20and%20Guo%20Yanwen%0AAbstract%3A%20%20%20Current%20learning-based%20methods%20predict%20NeRF%20or%203D%20Gaussians%20from%20point%20clouds%0Ato%20achieve%20photo-realistic%20rendering%20but%20still%20depend%20on%20categorical%20priors%2C%0Adense%20point%20clouds%2C%20or%20additional%20refinements.%20Hence%2C%20we%20introduce%20a%20novel%0Apoint%20cloud%20rendering%20method%20by%20predicting%202D%20Gaussians%20from%20point%20clouds.%20Our%0Amethod%20incorporates%20two%20identical%20modules%20with%20an%20entire-patch%20architecture%0Aenabling%20the%20network%20to%20be%20generalized%20to%20multiple%20datasets.%20The%20module%0Anormalizes%20and%20initializes%20the%20Gaussians%20utilizing%20the%20point%20cloud%20information%0Aincluding%20normals%2C%20colors%20and%20distances.%20Then%2C%20splitting%20decoders%20are%20employed%0Ato%20refine%20the%20initial%20Gaussians%20by%20duplicating%20them%20and%20predicting%20more%0Aaccurate%20results%2C%20making%20our%20methodology%20effectively%20accommodate%20sparse%20point%0Aclouds%20as%20well.%20Once%20trained%2C%20our%20approach%20exhibits%20direct%20generalization%20to%0Apoint%20clouds%20across%20different%20categories.%20The%20predicted%20Gaussians%20are%20employed%0Adirectly%20for%20rendering%20without%20additional%20refinement%20on%20the%20rendered%20images%2C%0Aretaining%20the%20benefits%20of%202D%20Gaussians.%20We%20conduct%20extensive%20experiments%20on%0Avarious%20datasets%2C%20and%20the%20results%20demonstrate%20the%20superiority%20and%0Ageneralization%20of%20our%20method%2C%20which%20achieves%20SOTA%20performance.%20The%20code%20is%0Aavailable%20at%0Ahttps%3A//github.com/murcherful/GauPCRender%7D%7Bhttps%3A//github.com/murcherful/GauPCRender.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Point%2520Cloud%2520Patches%2520Rendering%2520via%2520Splitting%25202D%2520Gaussians%26entry.906535625%3DMa%2520Changfeng%2520and%2520Bi%2520Ran%2520and%2520Guo%2520Jie%2520and%2520Wang%2520Chongjun%2520and%2520Guo%2520Yanwen%26entry.1292438233%3D%2520%2520Current%2520learning-based%2520methods%2520predict%2520NeRF%2520or%25203D%2520Gaussians%2520from%2520point%2520clouds%250Ato%2520achieve%2520photo-realistic%2520rendering%2520but%2520still%2520depend%2520on%2520categorical%2520priors%252C%250Adense%2520point%2520clouds%252C%2520or%2520additional%2520refinements.%2520Hence%252C%2520we%2520introduce%2520a%2520novel%250Apoint%2520cloud%2520rendering%2520method%2520by%2520predicting%25202D%2520Gaussians%2520from%2520point%2520clouds.%2520Our%250Amethod%2520incorporates%2520two%2520identical%2520modules%2520with%2520an%2520entire-patch%2520architecture%250Aenabling%2520the%2520network%2520to%2520be%2520generalized%2520to%2520multiple%2520datasets.%2520The%2520module%250Anormalizes%2520and%2520initializes%2520the%2520Gaussians%2520utilizing%2520the%2520point%2520cloud%2520information%250Aincluding%2520normals%252C%2520colors%2520and%2520distances.%2520Then%252C%2520splitting%2520decoders%2520are%2520employed%250Ato%2520refine%2520the%2520initial%2520Gaussians%2520by%2520duplicating%2520them%2520and%2520predicting%2520more%250Aaccurate%2520results%252C%2520making%2520our%2520methodology%2520effectively%2520accommodate%2520sparse%2520point%250Aclouds%2520as%2520well.%2520Once%2520trained%252C%2520our%2520approach%2520exhibits%2520direct%2520generalization%2520to%250Apoint%2520clouds%2520across%2520different%2520categories.%2520The%2520predicted%2520Gaussians%2520are%2520employed%250Adirectly%2520for%2520rendering%2520without%2520additional%2520refinement%2520on%2520the%2520rendered%2520images%252C%250Aretaining%2520the%2520benefits%2520of%25202D%2520Gaussians.%2520We%2520conduct%2520extensive%2520experiments%2520on%250Avarious%2520datasets%252C%2520and%2520the%2520results%2520demonstrate%2520the%2520superiority%2520and%250Ageneralization%2520of%2520our%2520method%252C%2520which%2520achieves%2520SOTA%2520performance.%2520The%2520code%2520is%250Aavailable%2520at%250Ahttps%253A//github.com/murcherful/GauPCRender%257D%257Bhttps%253A//github.com/murcherful/GauPCRender.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Point%20Cloud%20Patches%20Rendering%20via%20Splitting%202D%20Gaussians&entry.906535625=Ma%20Changfeng%20and%20Bi%20Ran%20and%20Guo%20Jie%20and%20Wang%20Chongjun%20and%20Guo%20Yanwen&entry.1292438233=%20%20Current%20learning-based%20methods%20predict%20NeRF%20or%203D%20Gaussians%20from%20point%20clouds%0Ato%20achieve%20photo-realistic%20rendering%20but%20still%20depend%20on%20categorical%20priors%2C%0Adense%20point%20clouds%2C%20or%20additional%20refinements.%20Hence%2C%20we%20introduce%20a%20novel%0Apoint%20cloud%20rendering%20method%20by%20predicting%202D%20Gaussians%20from%20point%20clouds.%20Our%0Amethod%20incorporates%20two%20identical%20modules%20with%20an%20entire-patch%20architecture%0Aenabling%20the%20network%20to%20be%20generalized%20to%20multiple%20datasets.%20The%20module%0Anormalizes%20and%20initializes%20the%20Gaussians%20utilizing%20the%20point%20cloud%20information%0Aincluding%20normals%2C%20colors%20and%20distances.%20Then%2C%20splitting%20decoders%20are%20employed%0Ato%20refine%20the%20initial%20Gaussians%20by%20duplicating%20them%20and%20predicting%20more%0Aaccurate%20results%2C%20making%20our%20methodology%20effectively%20accommodate%20sparse%20point%0Aclouds%20as%20well.%20Once%20trained%2C%20our%20approach%20exhibits%20direct%20generalization%20to%0Apoint%20clouds%20across%20different%20categories.%20The%20predicted%20Gaussians%20are%20employed%0Adirectly%20for%20rendering%20without%20additional%20refinement%20on%20the%20rendered%20images%2C%0Aretaining%20the%20benefits%20of%202D%20Gaussians.%20We%20conduct%20extensive%20experiments%20on%0Avarious%20datasets%2C%20and%20the%20results%20demonstrate%20the%20superiority%20and%0Ageneralization%20of%20our%20method%2C%20which%20achieves%20SOTA%20performance.%20The%20code%20is%0Aavailable%20at%0Ahttps%3A//github.com/murcherful/GauPCRender%7D%7Bhttps%3A//github.com/murcherful/GauPCRender.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09413v1&entry.124074799=Read"},
{"title": "Unsupervised Multiview Contrastive Language-Image Joint Learning with\n  Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression\n  Recognition", "author": "Muzammil Behzad", "abstract": "  In this paper, we introduce MultiviewVLM, a vision-language model designed\nfor unsupervised contrastive multiview representation learning of facial\nemotions from 3D/4D data. Our architecture integrates pseudo-labels derived\nfrom generated textual prompts to guide implicit alignment of emotional\nsemantics. To capture shared information across multi-views, we propose a joint\nembedding space that aligns multiview representations without requiring\nexplicit supervision. We further enhance the discriminability of our model\nthrough a novel multiview contrastive learning strategy that leverages stable\npositive-negative pair sampling. A gradient-friendly loss function is\nintroduced to promote smoother and more stable convergence, and the model is\noptimized for distributed training to ensure scalability. Extensive experiments\ndemonstrate that MultiviewVLM outperforms existing state-of-the-art methods and\ncan be easily adapted to various real-world applications with minimal\nmodifications.\n", "link": "http://arxiv.org/abs/2505.09336v1", "date": "2025-05-14", "relevancy": 3.0423, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6229}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6012}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Multiview%20Contrastive%20Language-Image%20Joint%20Learning%20with%0A%20%20Pseudo-Labeled%20Prompts%20Via%20Vision-Language%20Model%20for%203D/4D%20Facial%20Expression%0A%20%20Recognition&body=Title%3A%20Unsupervised%20Multiview%20Contrastive%20Language-Image%20Joint%20Learning%20with%0A%20%20Pseudo-Labeled%20Prompts%20Via%20Vision-Language%20Model%20for%203D/4D%20Facial%20Expression%0A%20%20Recognition%0AAuthor%3A%20Muzammil%20Behzad%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20MultiviewVLM%2C%20a%20vision-language%20model%20designed%0Afor%20unsupervised%20contrastive%20multiview%20representation%20learning%20of%20facial%0Aemotions%20from%203D/4D%20data.%20Our%20architecture%20integrates%20pseudo-labels%20derived%0Afrom%20generated%20textual%20prompts%20to%20guide%20implicit%20alignment%20of%20emotional%0Asemantics.%20To%20capture%20shared%20information%20across%20multi-views%2C%20we%20propose%20a%20joint%0Aembedding%20space%20that%20aligns%20multiview%20representations%20without%20requiring%0Aexplicit%20supervision.%20We%20further%20enhance%20the%20discriminability%20of%20our%20model%0Athrough%20a%20novel%20multiview%20contrastive%20learning%20strategy%20that%20leverages%20stable%0Apositive-negative%20pair%20sampling.%20A%20gradient-friendly%20loss%20function%20is%0Aintroduced%20to%20promote%20smoother%20and%20more%20stable%20convergence%2C%20and%20the%20model%20is%0Aoptimized%20for%20distributed%20training%20to%20ensure%20scalability.%20Extensive%20experiments%0Ademonstrate%20that%20MultiviewVLM%20outperforms%20existing%20state-of-the-art%20methods%20and%0Acan%20be%20easily%20adapted%20to%20various%20real-world%20applications%20with%20minimal%0Amodifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Multiview%2520Contrastive%2520Language-Image%2520Joint%2520Learning%2520with%250A%2520%2520Pseudo-Labeled%2520Prompts%2520Via%2520Vision-Language%2520Model%2520for%25203D/4D%2520Facial%2520Expression%250A%2520%2520Recognition%26entry.906535625%3DMuzammil%2520Behzad%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MultiviewVLM%252C%2520a%2520vision-language%2520model%2520designed%250Afor%2520unsupervised%2520contrastive%2520multiview%2520representation%2520learning%2520of%2520facial%250Aemotions%2520from%25203D/4D%2520data.%2520Our%2520architecture%2520integrates%2520pseudo-labels%2520derived%250Afrom%2520generated%2520textual%2520prompts%2520to%2520guide%2520implicit%2520alignment%2520of%2520emotional%250Asemantics.%2520To%2520capture%2520shared%2520information%2520across%2520multi-views%252C%2520we%2520propose%2520a%2520joint%250Aembedding%2520space%2520that%2520aligns%2520multiview%2520representations%2520without%2520requiring%250Aexplicit%2520supervision.%2520We%2520further%2520enhance%2520the%2520discriminability%2520of%2520our%2520model%250Athrough%2520a%2520novel%2520multiview%2520contrastive%2520learning%2520strategy%2520that%2520leverages%2520stable%250Apositive-negative%2520pair%2520sampling.%2520A%2520gradient-friendly%2520loss%2520function%2520is%250Aintroduced%2520to%2520promote%2520smoother%2520and%2520more%2520stable%2520convergence%252C%2520and%2520the%2520model%2520is%250Aoptimized%2520for%2520distributed%2520training%2520to%2520ensure%2520scalability.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520MultiviewVLM%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520and%250Acan%2520be%2520easily%2520adapted%2520to%2520various%2520real-world%2520applications%2520with%2520minimal%250Amodifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Multiview%20Contrastive%20Language-Image%20Joint%20Learning%20with%0A%20%20Pseudo-Labeled%20Prompts%20Via%20Vision-Language%20Model%20for%203D/4D%20Facial%20Expression%0A%20%20Recognition&entry.906535625=Muzammil%20Behzad&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20MultiviewVLM%2C%20a%20vision-language%20model%20designed%0Afor%20unsupervised%20contrastive%20multiview%20representation%20learning%20of%20facial%0Aemotions%20from%203D/4D%20data.%20Our%20architecture%20integrates%20pseudo-labels%20derived%0Afrom%20generated%20textual%20prompts%20to%20guide%20implicit%20alignment%20of%20emotional%0Asemantics.%20To%20capture%20shared%20information%20across%20multi-views%2C%20we%20propose%20a%20joint%0Aembedding%20space%20that%20aligns%20multiview%20representations%20without%20requiring%0Aexplicit%20supervision.%20We%20further%20enhance%20the%20discriminability%20of%20our%20model%0Athrough%20a%20novel%20multiview%20contrastive%20learning%20strategy%20that%20leverages%20stable%0Apositive-negative%20pair%20sampling.%20A%20gradient-friendly%20loss%20function%20is%0Aintroduced%20to%20promote%20smoother%20and%20more%20stable%20convergence%2C%20and%20the%20model%20is%0Aoptimized%20for%20distributed%20training%20to%20ensure%20scalability.%20Extensive%20experiments%0Ademonstrate%20that%20MultiviewVLM%20outperforms%20existing%20state-of-the-art%20methods%20and%0Acan%20be%20easily%20adapted%20to%20various%20real-world%20applications%20with%20minimal%0Amodifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09336v1&entry.124074799=Read"},
{"title": "Neural Video Compression using 2D Gaussian Splatting", "author": "Lakshya Gupta and Imran N. Junejo", "abstract": "  The computer vision and image processing research community has been involved\nin standardizing video data communications for the past many decades, leading\nto standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent\ngroundbreaking works have focused on employing deep learning-based techniques\nto replace the traditional video codec pipeline to a greater affect. Neural\nvideo codecs (NVC) create an end-to-end ML-based solution that does not rely on\nany handcrafted features (motion or edge-based) and have the ability to learn\ncontent-aware compression strategies, offering better adaptability and higher\ncompression efficiency than traditional methods. This holds a great potential\nnot only for hardware design, but also for various video streaming platforms\nand applications, especially video conferencing applications such as MS-Teams\nor Zoom that have found extensive usage in classrooms and workplaces. However,\ntheir high computational demands currently limit their use in real-time\napplications like video conferencing. To address this, we propose a\nregion-of-interest (ROI) based neural video compression model that leverages 2D\nGaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable\nof real-time decoding and can be optimized using fewer data points, requiring\nonly thousands of Gaussians for decent quality outputs as opposed to millions\nin 3D scenes. In this work, we designed a video pipeline that speeds up the\nencoding time of the previous Gaussian splatting-based image codec by 88% by\nusing a content-aware initialization strategy paired with a novel Gaussian\ninter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be\nused for a video-codec solution, the first of its kind solution in this neural\nvideo codec space.\n", "link": "http://arxiv.org/abs/2505.09324v1", "date": "2025-05-14", "relevancy": 3.0215, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6213}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6063}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Video%20Compression%20using%202D%20Gaussian%20Splatting&body=Title%3A%20Neural%20Video%20Compression%20using%202D%20Gaussian%20Splatting%0AAuthor%3A%20Lakshya%20Gupta%20and%20Imran%20N.%20Junejo%0AAbstract%3A%20%20%20The%20computer%20vision%20and%20image%20processing%20research%20community%20has%20been%20involved%0Ain%20standardizing%20video%20data%20communications%20for%20the%20past%20many%20decades%2C%20leading%0Ato%20standards%20such%20as%20AVC%2C%20HEVC%2C%20VVC%2C%20AV1%2C%20AV2%2C%20etc.%20However%2C%20recent%0Agroundbreaking%20works%20have%20focused%20on%20employing%20deep%20learning-based%20techniques%0Ato%20replace%20the%20traditional%20video%20codec%20pipeline%20to%20a%20greater%20affect.%20Neural%0Avideo%20codecs%20%28NVC%29%20create%20an%20end-to-end%20ML-based%20solution%20that%20does%20not%20rely%20on%0Aany%20handcrafted%20features%20%28motion%20or%20edge-based%29%20and%20have%20the%20ability%20to%20learn%0Acontent-aware%20compression%20strategies%2C%20offering%20better%20adaptability%20and%20higher%0Acompression%20efficiency%20than%20traditional%20methods.%20This%20holds%20a%20great%20potential%0Anot%20only%20for%20hardware%20design%2C%20but%20also%20for%20various%20video%20streaming%20platforms%0Aand%20applications%2C%20especially%20video%20conferencing%20applications%20such%20as%20MS-Teams%0Aor%20Zoom%20that%20have%20found%20extensive%20usage%20in%20classrooms%20and%20workplaces.%20However%2C%0Atheir%20high%20computational%20demands%20currently%20limit%20their%20use%20in%20real-time%0Aapplications%20like%20video%20conferencing.%20To%20address%20this%2C%20we%20propose%20a%0Aregion-of-interest%20%28ROI%29%20based%20neural%20video%20compression%20model%20that%20leverages%202D%0AGaussian%20Splatting.%20Unlike%20traditional%20codecs%2C%202D%20Gaussian%20Splatting%20is%20capable%0Aof%20real-time%20decoding%20and%20can%20be%20optimized%20using%20fewer%20data%20points%2C%20requiring%0Aonly%20thousands%20of%20Gaussians%20for%20decent%20quality%20outputs%20as%20opposed%20to%20millions%0Ain%203D%20scenes.%20In%20this%20work%2C%20we%20designed%20a%20video%20pipeline%20that%20speeds%20up%20the%0Aencoding%20time%20of%20the%20previous%20Gaussian%20splatting-based%20image%20codec%20by%2088%25%20by%0Ausing%20a%20content-aware%20initialization%20strategy%20paired%20with%20a%20novel%20Gaussian%0Ainter-frame%20redundancy-reduction%20mechanism%2C%20enabling%20Gaussian%20splatting%20to%20be%0Aused%20for%20a%20video-codec%20solution%2C%20the%20first%20of%20its%20kind%20solution%20in%20this%20neural%0Avideo%20codec%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Video%2520Compression%2520using%25202D%2520Gaussian%2520Splatting%26entry.906535625%3DLakshya%2520Gupta%2520and%2520Imran%2520N.%2520Junejo%26entry.1292438233%3D%2520%2520The%2520computer%2520vision%2520and%2520image%2520processing%2520research%2520community%2520has%2520been%2520involved%250Ain%2520standardizing%2520video%2520data%2520communications%2520for%2520the%2520past%2520many%2520decades%252C%2520leading%250Ato%2520standards%2520such%2520as%2520AVC%252C%2520HEVC%252C%2520VVC%252C%2520AV1%252C%2520AV2%252C%2520etc.%2520However%252C%2520recent%250Agroundbreaking%2520works%2520have%2520focused%2520on%2520employing%2520deep%2520learning-based%2520techniques%250Ato%2520replace%2520the%2520traditional%2520video%2520codec%2520pipeline%2520to%2520a%2520greater%2520affect.%2520Neural%250Avideo%2520codecs%2520%2528NVC%2529%2520create%2520an%2520end-to-end%2520ML-based%2520solution%2520that%2520does%2520not%2520rely%2520on%250Aany%2520handcrafted%2520features%2520%2528motion%2520or%2520edge-based%2529%2520and%2520have%2520the%2520ability%2520to%2520learn%250Acontent-aware%2520compression%2520strategies%252C%2520offering%2520better%2520adaptability%2520and%2520higher%250Acompression%2520efficiency%2520than%2520traditional%2520methods.%2520This%2520holds%2520a%2520great%2520potential%250Anot%2520only%2520for%2520hardware%2520design%252C%2520but%2520also%2520for%2520various%2520video%2520streaming%2520platforms%250Aand%2520applications%252C%2520especially%2520video%2520conferencing%2520applications%2520such%2520as%2520MS-Teams%250Aor%2520Zoom%2520that%2520have%2520found%2520extensive%2520usage%2520in%2520classrooms%2520and%2520workplaces.%2520However%252C%250Atheir%2520high%2520computational%2520demands%2520currently%2520limit%2520their%2520use%2520in%2520real-time%250Aapplications%2520like%2520video%2520conferencing.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Aregion-of-interest%2520%2528ROI%2529%2520based%2520neural%2520video%2520compression%2520model%2520that%2520leverages%25202D%250AGaussian%2520Splatting.%2520Unlike%2520traditional%2520codecs%252C%25202D%2520Gaussian%2520Splatting%2520is%2520capable%250Aof%2520real-time%2520decoding%2520and%2520can%2520be%2520optimized%2520using%2520fewer%2520data%2520points%252C%2520requiring%250Aonly%2520thousands%2520of%2520Gaussians%2520for%2520decent%2520quality%2520outputs%2520as%2520opposed%2520to%2520millions%250Ain%25203D%2520scenes.%2520In%2520this%2520work%252C%2520we%2520designed%2520a%2520video%2520pipeline%2520that%2520speeds%2520up%2520the%250Aencoding%2520time%2520of%2520the%2520previous%2520Gaussian%2520splatting-based%2520image%2520codec%2520by%252088%2525%2520by%250Ausing%2520a%2520content-aware%2520initialization%2520strategy%2520paired%2520with%2520a%2520novel%2520Gaussian%250Ainter-frame%2520redundancy-reduction%2520mechanism%252C%2520enabling%2520Gaussian%2520splatting%2520to%2520be%250Aused%2520for%2520a%2520video-codec%2520solution%252C%2520the%2520first%2520of%2520its%2520kind%2520solution%2520in%2520this%2520neural%250Avideo%2520codec%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Video%20Compression%20using%202D%20Gaussian%20Splatting&entry.906535625=Lakshya%20Gupta%20and%20Imran%20N.%20Junejo&entry.1292438233=%20%20The%20computer%20vision%20and%20image%20processing%20research%20community%20has%20been%20involved%0Ain%20standardizing%20video%20data%20communications%20for%20the%20past%20many%20decades%2C%20leading%0Ato%20standards%20such%20as%20AVC%2C%20HEVC%2C%20VVC%2C%20AV1%2C%20AV2%2C%20etc.%20However%2C%20recent%0Agroundbreaking%20works%20have%20focused%20on%20employing%20deep%20learning-based%20techniques%0Ato%20replace%20the%20traditional%20video%20codec%20pipeline%20to%20a%20greater%20affect.%20Neural%0Avideo%20codecs%20%28NVC%29%20create%20an%20end-to-end%20ML-based%20solution%20that%20does%20not%20rely%20on%0Aany%20handcrafted%20features%20%28motion%20or%20edge-based%29%20and%20have%20the%20ability%20to%20learn%0Acontent-aware%20compression%20strategies%2C%20offering%20better%20adaptability%20and%20higher%0Acompression%20efficiency%20than%20traditional%20methods.%20This%20holds%20a%20great%20potential%0Anot%20only%20for%20hardware%20design%2C%20but%20also%20for%20various%20video%20streaming%20platforms%0Aand%20applications%2C%20especially%20video%20conferencing%20applications%20such%20as%20MS-Teams%0Aor%20Zoom%20that%20have%20found%20extensive%20usage%20in%20classrooms%20and%20workplaces.%20However%2C%0Atheir%20high%20computational%20demands%20currently%20limit%20their%20use%20in%20real-time%0Aapplications%20like%20video%20conferencing.%20To%20address%20this%2C%20we%20propose%20a%0Aregion-of-interest%20%28ROI%29%20based%20neural%20video%20compression%20model%20that%20leverages%202D%0AGaussian%20Splatting.%20Unlike%20traditional%20codecs%2C%202D%20Gaussian%20Splatting%20is%20capable%0Aof%20real-time%20decoding%20and%20can%20be%20optimized%20using%20fewer%20data%20points%2C%20requiring%0Aonly%20thousands%20of%20Gaussians%20for%20decent%20quality%20outputs%20as%20opposed%20to%20millions%0Ain%203D%20scenes.%20In%20this%20work%2C%20we%20designed%20a%20video%20pipeline%20that%20speeds%20up%20the%0Aencoding%20time%20of%20the%20previous%20Gaussian%20splatting-based%20image%20codec%20by%2088%25%20by%0Ausing%20a%20content-aware%20initialization%20strategy%20paired%20with%20a%20novel%20Gaussian%0Ainter-frame%20redundancy-reduction%20mechanism%2C%20enabling%20Gaussian%20splatting%20to%20be%0Aused%20for%20a%20video-codec%20solution%2C%20the%20first%20of%20its%20kind%20solution%20in%20this%20neural%0Avideo%20codec%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09324v1&entry.124074799=Read"},
{"title": "Learning Autonomy: Off-Road Navigation Enhanced by Human Input", "author": "Akhil Nagariya and Dimitar Filev and Srikanth Saripalli and Gaurav Pandey", "abstract": "  In the area of autonomous driving, navigating off-road terrains presents a\nunique set of challenges, from unpredictable surfaces like grass and dirt to\nunexpected obstacles such as bushes and puddles. In this work, we present a\nnovel learning-based local planner that addresses these challenges by directly\ncapturing human driving nuances from real-world demonstrations using only a\nmonocular camera. The key features of our planner are its ability to navigate\nin challenging off-road environments with various terrain types and its fast\nlearning capabilities. By utilizing minimal human demonstration data (5-10\nmins), it quickly learns to navigate in a wide array of off-road conditions.\nThe local planner significantly reduces the real world data required to learn\nhuman driving preferences. This allows the planner to apply learned behaviors\nto real-world scenarios without the need for manual fine-tuning, demonstrating\nquick adjustment and adaptability in off-road autonomous driving technology.\n", "link": "http://arxiv.org/abs/2502.18760v2", "date": "2025-05-14", "relevancy": 3.0148, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6284}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6031}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Autonomy%3A%20Off-Road%20Navigation%20Enhanced%20by%20Human%20Input&body=Title%3A%20Learning%20Autonomy%3A%20Off-Road%20Navigation%20Enhanced%20by%20Human%20Input%0AAuthor%3A%20Akhil%20Nagariya%20and%20Dimitar%20Filev%20and%20Srikanth%20Saripalli%20and%20Gaurav%20Pandey%0AAbstract%3A%20%20%20In%20the%20area%20of%20autonomous%20driving%2C%20navigating%20off-road%20terrains%20presents%20a%0Aunique%20set%20of%20challenges%2C%20from%20unpredictable%20surfaces%20like%20grass%20and%20dirt%20to%0Aunexpected%20obstacles%20such%20as%20bushes%20and%20puddles.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20learning-based%20local%20planner%20that%20addresses%20these%20challenges%20by%20directly%0Acapturing%20human%20driving%20nuances%20from%20real-world%20demonstrations%20using%20only%20a%0Amonocular%20camera.%20The%20key%20features%20of%20our%20planner%20are%20its%20ability%20to%20navigate%0Ain%20challenging%20off-road%20environments%20with%20various%20terrain%20types%20and%20its%20fast%0Alearning%20capabilities.%20By%20utilizing%20minimal%20human%20demonstration%20data%20%285-10%0Amins%29%2C%20it%20quickly%20learns%20to%20navigate%20in%20a%20wide%20array%20of%20off-road%20conditions.%0AThe%20local%20planner%20significantly%20reduces%20the%20real%20world%20data%20required%20to%20learn%0Ahuman%20driving%20preferences.%20This%20allows%20the%20planner%20to%20apply%20learned%20behaviors%0Ato%20real-world%20scenarios%20without%20the%20need%20for%20manual%20fine-tuning%2C%20demonstrating%0Aquick%20adjustment%20and%20adaptability%20in%20off-road%20autonomous%20driving%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18760v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Autonomy%253A%2520Off-Road%2520Navigation%2520Enhanced%2520by%2520Human%2520Input%26entry.906535625%3DAkhil%2520Nagariya%2520and%2520Dimitar%2520Filev%2520and%2520Srikanth%2520Saripalli%2520and%2520Gaurav%2520Pandey%26entry.1292438233%3D%2520%2520In%2520the%2520area%2520of%2520autonomous%2520driving%252C%2520navigating%2520off-road%2520terrains%2520presents%2520a%250Aunique%2520set%2520of%2520challenges%252C%2520from%2520unpredictable%2520surfaces%2520like%2520grass%2520and%2520dirt%2520to%250Aunexpected%2520obstacles%2520such%2520as%2520bushes%2520and%2520puddles.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Anovel%2520learning-based%2520local%2520planner%2520that%2520addresses%2520these%2520challenges%2520by%2520directly%250Acapturing%2520human%2520driving%2520nuances%2520from%2520real-world%2520demonstrations%2520using%2520only%2520a%250Amonocular%2520camera.%2520The%2520key%2520features%2520of%2520our%2520planner%2520are%2520its%2520ability%2520to%2520navigate%250Ain%2520challenging%2520off-road%2520environments%2520with%2520various%2520terrain%2520types%2520and%2520its%2520fast%250Alearning%2520capabilities.%2520By%2520utilizing%2520minimal%2520human%2520demonstration%2520data%2520%25285-10%250Amins%2529%252C%2520it%2520quickly%2520learns%2520to%2520navigate%2520in%2520a%2520wide%2520array%2520of%2520off-road%2520conditions.%250AThe%2520local%2520planner%2520significantly%2520reduces%2520the%2520real%2520world%2520data%2520required%2520to%2520learn%250Ahuman%2520driving%2520preferences.%2520This%2520allows%2520the%2520planner%2520to%2520apply%2520learned%2520behaviors%250Ato%2520real-world%2520scenarios%2520without%2520the%2520need%2520for%2520manual%2520fine-tuning%252C%2520demonstrating%250Aquick%2520adjustment%2520and%2520adaptability%2520in%2520off-road%2520autonomous%2520driving%2520technology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18760v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Autonomy%3A%20Off-Road%20Navigation%20Enhanced%20by%20Human%20Input&entry.906535625=Akhil%20Nagariya%20and%20Dimitar%20Filev%20and%20Srikanth%20Saripalli%20and%20Gaurav%20Pandey&entry.1292438233=%20%20In%20the%20area%20of%20autonomous%20driving%2C%20navigating%20off-road%20terrains%20presents%20a%0Aunique%20set%20of%20challenges%2C%20from%20unpredictable%20surfaces%20like%20grass%20and%20dirt%20to%0Aunexpected%20obstacles%20such%20as%20bushes%20and%20puddles.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20learning-based%20local%20planner%20that%20addresses%20these%20challenges%20by%20directly%0Acapturing%20human%20driving%20nuances%20from%20real-world%20demonstrations%20using%20only%20a%0Amonocular%20camera.%20The%20key%20features%20of%20our%20planner%20are%20its%20ability%20to%20navigate%0Ain%20challenging%20off-road%20environments%20with%20various%20terrain%20types%20and%20its%20fast%0Alearning%20capabilities.%20By%20utilizing%20minimal%20human%20demonstration%20data%20%285-10%0Amins%29%2C%20it%20quickly%20learns%20to%20navigate%20in%20a%20wide%20array%20of%20off-road%20conditions.%0AThe%20local%20planner%20significantly%20reduces%20the%20real%20world%20data%20required%20to%20learn%0Ahuman%20driving%20preferences.%20This%20allows%20the%20planner%20to%20apply%20learned%20behaviors%0Ato%20real-world%20scenarios%20without%20the%20need%20for%20manual%20fine-tuning%2C%20demonstrating%0Aquick%20adjustment%20and%20adaptability%20in%20off-road%20autonomous%20driving%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18760v2&entry.124074799=Read"},
{"title": "Q-space Guided Collaborative Attention Translation Network for Flexible\n  Diffusion-Weighted Images Synthesis", "author": "Pengli Zhu and Yingji Fu and Nanguang Chen and Anqi Qiu", "abstract": "  This study, we propose a novel Q-space Guided Collaborative Attention\nTranslation Networks (Q-CATN) for multi-shell, high-angular resolution DWI\n(MS-HARDI) synthesis from flexible q-space sampling, leveraging the commonly\nacquired structural MRI data. Q-CATN employs a collaborative attention\nmechanism to effectively extract complementary information from multiple\nmodalities and dynamically adjust its internal representations based on\nflexible q-space information, eliminating the need for fixed sampling schemes.\nAdditionally, we introduce a range of task-specific constraints to preserve\nanatomical fidelity in DWI, enabling Q-CATN to accurately learn the intrinsic\nrelationships between directional DWI signal distributions and q-space.\nExtensive experiments on the Human Connectome Project (HCP) dataset demonstrate\nthat Q-CATN outperforms existing methods, including 1D-qDL, 2D-qDL, MESC-SD,\nand QGAN, in estimating parameter maps and fiber tracts both quantitatively and\nqualitatively, while preserving fine-grained details. Notably, its ability to\naccommodate flexible q-space sampling highlights its potential as a promising\ntoolkit for clinical and research applications. Our code is available at\nhttps://github.com/Idea89560041/Q-CATN.\n", "link": "http://arxiv.org/abs/2505.09323v1", "date": "2025-05-14", "relevancy": 2.9915, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6167}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6167}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-space%20Guided%20Collaborative%20Attention%20Translation%20Network%20for%20Flexible%0A%20%20Diffusion-Weighted%20Images%20Synthesis&body=Title%3A%20Q-space%20Guided%20Collaborative%20Attention%20Translation%20Network%20for%20Flexible%0A%20%20Diffusion-Weighted%20Images%20Synthesis%0AAuthor%3A%20Pengli%20Zhu%20and%20Yingji%20Fu%20and%20Nanguang%20Chen%20and%20Anqi%20Qiu%0AAbstract%3A%20%20%20This%20study%2C%20we%20propose%20a%20novel%20Q-space%20Guided%20Collaborative%20Attention%0ATranslation%20Networks%20%28Q-CATN%29%20for%20multi-shell%2C%20high-angular%20resolution%20DWI%0A%28MS-HARDI%29%20synthesis%20from%20flexible%20q-space%20sampling%2C%20leveraging%20the%20commonly%0Aacquired%20structural%20MRI%20data.%20Q-CATN%20employs%20a%20collaborative%20attention%0Amechanism%20to%20effectively%20extract%20complementary%20information%20from%20multiple%0Amodalities%20and%20dynamically%20adjust%20its%20internal%20representations%20based%20on%0Aflexible%20q-space%20information%2C%20eliminating%20the%20need%20for%20fixed%20sampling%20schemes.%0AAdditionally%2C%20we%20introduce%20a%20range%20of%20task-specific%20constraints%20to%20preserve%0Aanatomical%20fidelity%20in%20DWI%2C%20enabling%20Q-CATN%20to%20accurately%20learn%20the%20intrinsic%0Arelationships%20between%20directional%20DWI%20signal%20distributions%20and%20q-space.%0AExtensive%20experiments%20on%20the%20Human%20Connectome%20Project%20%28HCP%29%20dataset%20demonstrate%0Athat%20Q-CATN%20outperforms%20existing%20methods%2C%20including%201D-qDL%2C%202D-qDL%2C%20MESC-SD%2C%0Aand%20QGAN%2C%20in%20estimating%20parameter%20maps%20and%20fiber%20tracts%20both%20quantitatively%20and%0Aqualitatively%2C%20while%20preserving%20fine-grained%20details.%20Notably%2C%20its%20ability%20to%0Aaccommodate%20flexible%20q-space%20sampling%20highlights%20its%20potential%20as%20a%20promising%0Atoolkit%20for%20clinical%20and%20research%20applications.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Idea89560041/Q-CATN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-space%2520Guided%2520Collaborative%2520Attention%2520Translation%2520Network%2520for%2520Flexible%250A%2520%2520Diffusion-Weighted%2520Images%2520Synthesis%26entry.906535625%3DPengli%2520Zhu%2520and%2520Yingji%2520Fu%2520and%2520Nanguang%2520Chen%2520and%2520Anqi%2520Qiu%26entry.1292438233%3D%2520%2520This%2520study%252C%2520we%2520propose%2520a%2520novel%2520Q-space%2520Guided%2520Collaborative%2520Attention%250ATranslation%2520Networks%2520%2528Q-CATN%2529%2520for%2520multi-shell%252C%2520high-angular%2520resolution%2520DWI%250A%2528MS-HARDI%2529%2520synthesis%2520from%2520flexible%2520q-space%2520sampling%252C%2520leveraging%2520the%2520commonly%250Aacquired%2520structural%2520MRI%2520data.%2520Q-CATN%2520employs%2520a%2520collaborative%2520attention%250Amechanism%2520to%2520effectively%2520extract%2520complementary%2520information%2520from%2520multiple%250Amodalities%2520and%2520dynamically%2520adjust%2520its%2520internal%2520representations%2520based%2520on%250Aflexible%2520q-space%2520information%252C%2520eliminating%2520the%2520need%2520for%2520fixed%2520sampling%2520schemes.%250AAdditionally%252C%2520we%2520introduce%2520a%2520range%2520of%2520task-specific%2520constraints%2520to%2520preserve%250Aanatomical%2520fidelity%2520in%2520DWI%252C%2520enabling%2520Q-CATN%2520to%2520accurately%2520learn%2520the%2520intrinsic%250Arelationships%2520between%2520directional%2520DWI%2520signal%2520distributions%2520and%2520q-space.%250AExtensive%2520experiments%2520on%2520the%2520Human%2520Connectome%2520Project%2520%2528HCP%2529%2520dataset%2520demonstrate%250Athat%2520Q-CATN%2520outperforms%2520existing%2520methods%252C%2520including%25201D-qDL%252C%25202D-qDL%252C%2520MESC-SD%252C%250Aand%2520QGAN%252C%2520in%2520estimating%2520parameter%2520maps%2520and%2520fiber%2520tracts%2520both%2520quantitatively%2520and%250Aqualitatively%252C%2520while%2520preserving%2520fine-grained%2520details.%2520Notably%252C%2520its%2520ability%2520to%250Aaccommodate%2520flexible%2520q-space%2520sampling%2520highlights%2520its%2520potential%2520as%2520a%2520promising%250Atoolkit%2520for%2520clinical%2520and%2520research%2520applications.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Idea89560041/Q-CATN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-space%20Guided%20Collaborative%20Attention%20Translation%20Network%20for%20Flexible%0A%20%20Diffusion-Weighted%20Images%20Synthesis&entry.906535625=Pengli%20Zhu%20and%20Yingji%20Fu%20and%20Nanguang%20Chen%20and%20Anqi%20Qiu&entry.1292438233=%20%20This%20study%2C%20we%20propose%20a%20novel%20Q-space%20Guided%20Collaborative%20Attention%0ATranslation%20Networks%20%28Q-CATN%29%20for%20multi-shell%2C%20high-angular%20resolution%20DWI%0A%28MS-HARDI%29%20synthesis%20from%20flexible%20q-space%20sampling%2C%20leveraging%20the%20commonly%0Aacquired%20structural%20MRI%20data.%20Q-CATN%20employs%20a%20collaborative%20attention%0Amechanism%20to%20effectively%20extract%20complementary%20information%20from%20multiple%0Amodalities%20and%20dynamically%20adjust%20its%20internal%20representations%20based%20on%0Aflexible%20q-space%20information%2C%20eliminating%20the%20need%20for%20fixed%20sampling%20schemes.%0AAdditionally%2C%20we%20introduce%20a%20range%20of%20task-specific%20constraints%20to%20preserve%0Aanatomical%20fidelity%20in%20DWI%2C%20enabling%20Q-CATN%20to%20accurately%20learn%20the%20intrinsic%0Arelationships%20between%20directional%20DWI%20signal%20distributions%20and%20q-space.%0AExtensive%20experiments%20on%20the%20Human%20Connectome%20Project%20%28HCP%29%20dataset%20demonstrate%0Athat%20Q-CATN%20outperforms%20existing%20methods%2C%20including%201D-qDL%2C%202D-qDL%2C%20MESC-SD%2C%0Aand%20QGAN%2C%20in%20estimating%20parameter%20maps%20and%20fiber%20tracts%20both%20quantitatively%20and%0Aqualitatively%2C%20while%20preserving%20fine-grained%20details.%20Notably%2C%20its%20ability%20to%0Aaccommodate%20flexible%20q-space%20sampling%20highlights%20its%20potential%20as%20a%20promising%0Atoolkit%20for%20clinical%20and%20research%20applications.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Idea89560041/Q-CATN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09323v1&entry.124074799=Read"},
{"title": "F$^3$Loc: Fusion and Filtering for Floorplan Localization", "author": "Changan Chen and Rui Wang and Christoph Vogel and Marc Pollefeys", "abstract": "  In this paper we propose an efficient data-driven solution to\nself-localization within a floorplan. Floorplan data is readily available,\nlong-term persistent and inherently robust to changes in the visual appearance.\nOur method does not require retraining per map and location or demand a large\ndatabase of images of the area of interest. We propose a novel probabilistic\nmodel consisting of an observation and a novel temporal filtering module.\nOperating internally with an efficient ray-based representation, the\nobservation module consists of a single and a multiview module to predict\nhorizontal depth from images and fuses their results to benefit from advantages\noffered by either methodology. Our method operates on conventional consumer\nhardware and overcomes a common limitation of competing methods that often\ndemand upright images. Our full system meets real-time requirements, while\noutperforming the state-of-the-art by a significant margin.\n", "link": "http://arxiv.org/abs/2403.03370v2", "date": "2025-05-14", "relevancy": 2.9474, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6192}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5855}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20F%24%5E3%24Loc%3A%20Fusion%20and%20Filtering%20for%20Floorplan%20Localization&body=Title%3A%20F%24%5E3%24Loc%3A%20Fusion%20and%20Filtering%20for%20Floorplan%20Localization%0AAuthor%3A%20Changan%20Chen%20and%20Rui%20Wang%20and%20Christoph%20Vogel%20and%20Marc%20Pollefeys%0AAbstract%3A%20%20%20In%20this%20paper%20we%20propose%20an%20efficient%20data-driven%20solution%20to%0Aself-localization%20within%20a%20floorplan.%20Floorplan%20data%20is%20readily%20available%2C%0Along-term%20persistent%20and%20inherently%20robust%20to%20changes%20in%20the%20visual%20appearance.%0AOur%20method%20does%20not%20require%20retraining%20per%20map%20and%20location%20or%20demand%20a%20large%0Adatabase%20of%20images%20of%20the%20area%20of%20interest.%20We%20propose%20a%20novel%20probabilistic%0Amodel%20consisting%20of%20an%20observation%20and%20a%20novel%20temporal%20filtering%20module.%0AOperating%20internally%20with%20an%20efficient%20ray-based%20representation%2C%20the%0Aobservation%20module%20consists%20of%20a%20single%20and%20a%20multiview%20module%20to%20predict%0Ahorizontal%20depth%20from%20images%20and%20fuses%20their%20results%20to%20benefit%20from%20advantages%0Aoffered%20by%20either%20methodology.%20Our%20method%20operates%20on%20conventional%20consumer%0Ahardware%20and%20overcomes%20a%20common%20limitation%20of%20competing%20methods%20that%20often%0Ademand%20upright%20images.%20Our%20full%20system%20meets%20real-time%20requirements%2C%20while%0Aoutperforming%20the%20state-of-the-art%20by%20a%20significant%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03370v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DF%2524%255E3%2524Loc%253A%2520Fusion%2520and%2520Filtering%2520for%2520Floorplan%2520Localization%26entry.906535625%3DChangan%2520Chen%2520and%2520Rui%2520Wang%2520and%2520Christoph%2520Vogel%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520propose%2520an%2520efficient%2520data-driven%2520solution%2520to%250Aself-localization%2520within%2520a%2520floorplan.%2520Floorplan%2520data%2520is%2520readily%2520available%252C%250Along-term%2520persistent%2520and%2520inherently%2520robust%2520to%2520changes%2520in%2520the%2520visual%2520appearance.%250AOur%2520method%2520does%2520not%2520require%2520retraining%2520per%2520map%2520and%2520location%2520or%2520demand%2520a%2520large%250Adatabase%2520of%2520images%2520of%2520the%2520area%2520of%2520interest.%2520We%2520propose%2520a%2520novel%2520probabilistic%250Amodel%2520consisting%2520of%2520an%2520observation%2520and%2520a%2520novel%2520temporal%2520filtering%2520module.%250AOperating%2520internally%2520with%2520an%2520efficient%2520ray-based%2520representation%252C%2520the%250Aobservation%2520module%2520consists%2520of%2520a%2520single%2520and%2520a%2520multiview%2520module%2520to%2520predict%250Ahorizontal%2520depth%2520from%2520images%2520and%2520fuses%2520their%2520results%2520to%2520benefit%2520from%2520advantages%250Aoffered%2520by%2520either%2520methodology.%2520Our%2520method%2520operates%2520on%2520conventional%2520consumer%250Ahardware%2520and%2520overcomes%2520a%2520common%2520limitation%2520of%2520competing%2520methods%2520that%2520often%250Ademand%2520upright%2520images.%2520Our%2520full%2520system%2520meets%2520real-time%2520requirements%252C%2520while%250Aoutperforming%2520the%2520state-of-the-art%2520by%2520a%2520significant%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03370v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=F%24%5E3%24Loc%3A%20Fusion%20and%20Filtering%20for%20Floorplan%20Localization&entry.906535625=Changan%20Chen%20and%20Rui%20Wang%20and%20Christoph%20Vogel%20and%20Marc%20Pollefeys&entry.1292438233=%20%20In%20this%20paper%20we%20propose%20an%20efficient%20data-driven%20solution%20to%0Aself-localization%20within%20a%20floorplan.%20Floorplan%20data%20is%20readily%20available%2C%0Along-term%20persistent%20and%20inherently%20robust%20to%20changes%20in%20the%20visual%20appearance.%0AOur%20method%20does%20not%20require%20retraining%20per%20map%20and%20location%20or%20demand%20a%20large%0Adatabase%20of%20images%20of%20the%20area%20of%20interest.%20We%20propose%20a%20novel%20probabilistic%0Amodel%20consisting%20of%20an%20observation%20and%20a%20novel%20temporal%20filtering%20module.%0AOperating%20internally%20with%20an%20efficient%20ray-based%20representation%2C%20the%0Aobservation%20module%20consists%20of%20a%20single%20and%20a%20multiview%20module%20to%20predict%0Ahorizontal%20depth%20from%20images%20and%20fuses%20their%20results%20to%20benefit%20from%20advantages%0Aoffered%20by%20either%20methodology.%20Our%20method%20operates%20on%20conventional%20consumer%0Ahardware%20and%20overcomes%20a%20common%20limitation%20of%20competing%20methods%20that%20often%0Ademand%20upright%20images.%20Our%20full%20system%20meets%20real-time%20requirements%2C%20while%0Aoutperforming%20the%20state-of-the-art%20by%20a%20significant%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03370v2&entry.124074799=Read"},
{"title": "APR-Transformer: Initial Pose Estimation for Localization in Complex\n  Environments through Absolute Pose Regression", "author": "Srinivas Ravuri and Yuan Xu and Martin Ludwig Zehetner and Ketan Motlag and Sahin Albayrak", "abstract": "  Precise initialization plays a critical role in the performance of\nlocalization algorithms, especially in the context of robotics, autonomous\ndriving, and computer vision. Poor localization accuracy is often a consequence\nof inaccurate initial poses, particularly noticeable in GNSS-denied\nenvironments where GPS signals are primarily relied upon for initialization.\nRecent advances in leveraging deep neural networks for pose regression have led\nto significant improvements in both accuracy and robustness, especially in\nestimating complex spatial relationships and orientations. In this paper, we\nintroduce APR-Transformer, a model architecture inspired by state-of-the-art\nmethods, which predicts absolute pose (3D position and 3D orientation) using\neither image or LiDAR data. We demonstrate that our proposed method achieves\nstate-of-the-art performance on established benchmark datasets such as the\nRadar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our\nexperiments to include our custom complex APR-BeIntelli dataset. Additionally,\nwe validate the reliability of our approach in GNSS-denied environments by\ndeploying the model in real-time on an autonomous test vehicle. This showcases\nthe practical feasibility and effectiveness of our approach. The source code is\navailable at:https://github.com/GT-ARC/APR-Transformer.\n", "link": "http://arxiv.org/abs/2505.09356v1", "date": "2025-05-14", "relevancy": 2.8915, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5933}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5801}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APR-Transformer%3A%20Initial%20Pose%20Estimation%20for%20Localization%20in%20Complex%0A%20%20Environments%20through%20Absolute%20Pose%20Regression&body=Title%3A%20APR-Transformer%3A%20Initial%20Pose%20Estimation%20for%20Localization%20in%20Complex%0A%20%20Environments%20through%20Absolute%20Pose%20Regression%0AAuthor%3A%20Srinivas%20Ravuri%20and%20Yuan%20Xu%20and%20Martin%20Ludwig%20Zehetner%20and%20Ketan%20Motlag%20and%20Sahin%20Albayrak%0AAbstract%3A%20%20%20Precise%20initialization%20plays%20a%20critical%20role%20in%20the%20performance%20of%0Alocalization%20algorithms%2C%20especially%20in%20the%20context%20of%20robotics%2C%20autonomous%0Adriving%2C%20and%20computer%20vision.%20Poor%20localization%20accuracy%20is%20often%20a%20consequence%0Aof%20inaccurate%20initial%20poses%2C%20particularly%20noticeable%20in%20GNSS-denied%0Aenvironments%20where%20GPS%20signals%20are%20primarily%20relied%20upon%20for%20initialization.%0ARecent%20advances%20in%20leveraging%20deep%20neural%20networks%20for%20pose%20regression%20have%20led%0Ato%20significant%20improvements%20in%20both%20accuracy%20and%20robustness%2C%20especially%20in%0Aestimating%20complex%20spatial%20relationships%20and%20orientations.%20In%20this%20paper%2C%20we%0Aintroduce%20APR-Transformer%2C%20a%20model%20architecture%20inspired%20by%20state-of-the-art%0Amethods%2C%20which%20predicts%20absolute%20pose%20%283D%20position%20and%203D%20orientation%29%20using%0Aeither%20image%20or%20LiDAR%20data.%20We%20demonstrate%20that%20our%20proposed%20method%20achieves%0Astate-of-the-art%20performance%20on%20established%20benchmark%20datasets%20such%20as%20the%0ARadar%20Oxford%20Robot-Car%20and%20DeepLoc%20datasets.%20Furthermore%2C%20we%20extend%20our%0Aexperiments%20to%20include%20our%20custom%20complex%20APR-BeIntelli%20dataset.%20Additionally%2C%0Awe%20validate%20the%20reliability%20of%20our%20approach%20in%20GNSS-denied%20environments%20by%0Adeploying%20the%20model%20in%20real-time%20on%20an%20autonomous%20test%20vehicle.%20This%20showcases%0Athe%20practical%20feasibility%20and%20effectiveness%20of%20our%20approach.%20The%20source%20code%20is%0Aavailable%20at%3Ahttps%3A//github.com/GT-ARC/APR-Transformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPR-Transformer%253A%2520Initial%2520Pose%2520Estimation%2520for%2520Localization%2520in%2520Complex%250A%2520%2520Environments%2520through%2520Absolute%2520Pose%2520Regression%26entry.906535625%3DSrinivas%2520Ravuri%2520and%2520Yuan%2520Xu%2520and%2520Martin%2520Ludwig%2520Zehetner%2520and%2520Ketan%2520Motlag%2520and%2520Sahin%2520Albayrak%26entry.1292438233%3D%2520%2520Precise%2520initialization%2520plays%2520a%2520critical%2520role%2520in%2520the%2520performance%2520of%250Alocalization%2520algorithms%252C%2520especially%2520in%2520the%2520context%2520of%2520robotics%252C%2520autonomous%250Adriving%252C%2520and%2520computer%2520vision.%2520Poor%2520localization%2520accuracy%2520is%2520often%2520a%2520consequence%250Aof%2520inaccurate%2520initial%2520poses%252C%2520particularly%2520noticeable%2520in%2520GNSS-denied%250Aenvironments%2520where%2520GPS%2520signals%2520are%2520primarily%2520relied%2520upon%2520for%2520initialization.%250ARecent%2520advances%2520in%2520leveraging%2520deep%2520neural%2520networks%2520for%2520pose%2520regression%2520have%2520led%250Ato%2520significant%2520improvements%2520in%2520both%2520accuracy%2520and%2520robustness%252C%2520especially%2520in%250Aestimating%2520complex%2520spatial%2520relationships%2520and%2520orientations.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520APR-Transformer%252C%2520a%2520model%2520architecture%2520inspired%2520by%2520state-of-the-art%250Amethods%252C%2520which%2520predicts%2520absolute%2520pose%2520%25283D%2520position%2520and%25203D%2520orientation%2529%2520using%250Aeither%2520image%2520or%2520LiDAR%2520data.%2520We%2520demonstrate%2520that%2520our%2520proposed%2520method%2520achieves%250Astate-of-the-art%2520performance%2520on%2520established%2520benchmark%2520datasets%2520such%2520as%2520the%250ARadar%2520Oxford%2520Robot-Car%2520and%2520DeepLoc%2520datasets.%2520Furthermore%252C%2520we%2520extend%2520our%250Aexperiments%2520to%2520include%2520our%2520custom%2520complex%2520APR-BeIntelli%2520dataset.%2520Additionally%252C%250Awe%2520validate%2520the%2520reliability%2520of%2520our%2520approach%2520in%2520GNSS-denied%2520environments%2520by%250Adeploying%2520the%2520model%2520in%2520real-time%2520on%2520an%2520autonomous%2520test%2520vehicle.%2520This%2520showcases%250Athe%2520practical%2520feasibility%2520and%2520effectiveness%2520of%2520our%2520approach.%2520The%2520source%2520code%2520is%250Aavailable%2520at%253Ahttps%253A//github.com/GT-ARC/APR-Transformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APR-Transformer%3A%20Initial%20Pose%20Estimation%20for%20Localization%20in%20Complex%0A%20%20Environments%20through%20Absolute%20Pose%20Regression&entry.906535625=Srinivas%20Ravuri%20and%20Yuan%20Xu%20and%20Martin%20Ludwig%20Zehetner%20and%20Ketan%20Motlag%20and%20Sahin%20Albayrak&entry.1292438233=%20%20Precise%20initialization%20plays%20a%20critical%20role%20in%20the%20performance%20of%0Alocalization%20algorithms%2C%20especially%20in%20the%20context%20of%20robotics%2C%20autonomous%0Adriving%2C%20and%20computer%20vision.%20Poor%20localization%20accuracy%20is%20often%20a%20consequence%0Aof%20inaccurate%20initial%20poses%2C%20particularly%20noticeable%20in%20GNSS-denied%0Aenvironments%20where%20GPS%20signals%20are%20primarily%20relied%20upon%20for%20initialization.%0ARecent%20advances%20in%20leveraging%20deep%20neural%20networks%20for%20pose%20regression%20have%20led%0Ato%20significant%20improvements%20in%20both%20accuracy%20and%20robustness%2C%20especially%20in%0Aestimating%20complex%20spatial%20relationships%20and%20orientations.%20In%20this%20paper%2C%20we%0Aintroduce%20APR-Transformer%2C%20a%20model%20architecture%20inspired%20by%20state-of-the-art%0Amethods%2C%20which%20predicts%20absolute%20pose%20%283D%20position%20and%203D%20orientation%29%20using%0Aeither%20image%20or%20LiDAR%20data.%20We%20demonstrate%20that%20our%20proposed%20method%20achieves%0Astate-of-the-art%20performance%20on%20established%20benchmark%20datasets%20such%20as%20the%0ARadar%20Oxford%20Robot-Car%20and%20DeepLoc%20datasets.%20Furthermore%2C%20we%20extend%20our%0Aexperiments%20to%20include%20our%20custom%20complex%20APR-BeIntelli%20dataset.%20Additionally%2C%0Awe%20validate%20the%20reliability%20of%20our%20approach%20in%20GNSS-denied%20environments%20by%0Adeploying%20the%20model%20in%20real-time%20on%20an%20autonomous%20test%20vehicle.%20This%20showcases%0Athe%20practical%20feasibility%20and%20effectiveness%20of%20our%20approach.%20The%20source%20code%20is%0Aavailable%20at%3Ahttps%3A//github.com/GT-ARC/APR-Transformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09356v1&entry.124074799=Read"},
{"title": "DetailCLIP: Injecting Image Details into CLIP's Feature Space", "author": "Zilun Zhang and Cuifeng Shen and Yuan Shen and Huixin Xiong and Xinyu Zhou and Tiancheng Zhao and Jianwei Yin", "abstract": "  Although CLIP-like Visual Language Models provide a functional joint feature\nspace for image and text, due to the limitation of the CILP-like model's image\ninput size (e.g., 224), subtle details are lost in the feature representation\nif we input high-resolution images (e.g., 2240). In this work, we introduce an\nefficient framework that can produce a single feature representation for a\nhigh-resolution image that injects image details and shares the same semantic\nspace as the original CLIP. In the framework, we train a feature fusing model\nbased on CLIP features extracted from a carefully designed image patch method\nthat can cover objects of any scale, weakly supervised by image-agnostic class\nprompted queries. We validate our framework by retrieving images from class\nprompted queries on the real world and synthetic datasets, showing significant\nperformance improvement on these tasks. Furthermore, to fully demonstrate our\nframework's detail retrieval ability, we construct a CLEVR-like synthetic\ndataset called CLVER-DS, which is fully annotated and has a controllable object\nscale.\n", "link": "http://arxiv.org/abs/2208.14649v6", "date": "2025-05-14", "relevancy": 2.7886, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5637}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DetailCLIP%3A%20Injecting%20Image%20Details%20into%20CLIP%27s%20Feature%20Space&body=Title%3A%20DetailCLIP%3A%20Injecting%20Image%20Details%20into%20CLIP%27s%20Feature%20Space%0AAuthor%3A%20Zilun%20Zhang%20and%20Cuifeng%20Shen%20and%20Yuan%20Shen%20and%20Huixin%20Xiong%20and%20Xinyu%20Zhou%20and%20Tiancheng%20Zhao%20and%20Jianwei%20Yin%0AAbstract%3A%20%20%20Although%20CLIP-like%20Visual%20Language%20Models%20provide%20a%20functional%20joint%20feature%0Aspace%20for%20image%20and%20text%2C%20due%20to%20the%20limitation%20of%20the%20CILP-like%20model%27s%20image%0Ainput%20size%20%28e.g.%2C%20224%29%2C%20subtle%20details%20are%20lost%20in%20the%20feature%20representation%0Aif%20we%20input%20high-resolution%20images%20%28e.g.%2C%202240%29.%20In%20this%20work%2C%20we%20introduce%20an%0Aefficient%20framework%20that%20can%20produce%20a%20single%20feature%20representation%20for%20a%0Ahigh-resolution%20image%20that%20injects%20image%20details%20and%20shares%20the%20same%20semantic%0Aspace%20as%20the%20original%20CLIP.%20In%20the%20framework%2C%20we%20train%20a%20feature%20fusing%20model%0Abased%20on%20CLIP%20features%20extracted%20from%20a%20carefully%20designed%20image%20patch%20method%0Athat%20can%20cover%20objects%20of%20any%20scale%2C%20weakly%20supervised%20by%20image-agnostic%20class%0Aprompted%20queries.%20We%20validate%20our%20framework%20by%20retrieving%20images%20from%20class%0Aprompted%20queries%20on%20the%20real%20world%20and%20synthetic%20datasets%2C%20showing%20significant%0Aperformance%20improvement%20on%20these%20tasks.%20Furthermore%2C%20to%20fully%20demonstrate%20our%0Aframework%27s%20detail%20retrieval%20ability%2C%20we%20construct%20a%20CLEVR-like%20synthetic%0Adataset%20called%20CLVER-DS%2C%20which%20is%20fully%20annotated%20and%20has%20a%20controllable%20object%0Ascale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.14649v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetailCLIP%253A%2520Injecting%2520Image%2520Details%2520into%2520CLIP%2527s%2520Feature%2520Space%26entry.906535625%3DZilun%2520Zhang%2520and%2520Cuifeng%2520Shen%2520and%2520Yuan%2520Shen%2520and%2520Huixin%2520Xiong%2520and%2520Xinyu%2520Zhou%2520and%2520Tiancheng%2520Zhao%2520and%2520Jianwei%2520Yin%26entry.1292438233%3D%2520%2520Although%2520CLIP-like%2520Visual%2520Language%2520Models%2520provide%2520a%2520functional%2520joint%2520feature%250Aspace%2520for%2520image%2520and%2520text%252C%2520due%2520to%2520the%2520limitation%2520of%2520the%2520CILP-like%2520model%2527s%2520image%250Ainput%2520size%2520%2528e.g.%252C%2520224%2529%252C%2520subtle%2520details%2520are%2520lost%2520in%2520the%2520feature%2520representation%250Aif%2520we%2520input%2520high-resolution%2520images%2520%2528e.g.%252C%25202240%2529.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%250Aefficient%2520framework%2520that%2520can%2520produce%2520a%2520single%2520feature%2520representation%2520for%2520a%250Ahigh-resolution%2520image%2520that%2520injects%2520image%2520details%2520and%2520shares%2520the%2520same%2520semantic%250Aspace%2520as%2520the%2520original%2520CLIP.%2520In%2520the%2520framework%252C%2520we%2520train%2520a%2520feature%2520fusing%2520model%250Abased%2520on%2520CLIP%2520features%2520extracted%2520from%2520a%2520carefully%2520designed%2520image%2520patch%2520method%250Athat%2520can%2520cover%2520objects%2520of%2520any%2520scale%252C%2520weakly%2520supervised%2520by%2520image-agnostic%2520class%250Aprompted%2520queries.%2520We%2520validate%2520our%2520framework%2520by%2520retrieving%2520images%2520from%2520class%250Aprompted%2520queries%2520on%2520the%2520real%2520world%2520and%2520synthetic%2520datasets%252C%2520showing%2520significant%250Aperformance%2520improvement%2520on%2520these%2520tasks.%2520Furthermore%252C%2520to%2520fully%2520demonstrate%2520our%250Aframework%2527s%2520detail%2520retrieval%2520ability%252C%2520we%2520construct%2520a%2520CLEVR-like%2520synthetic%250Adataset%2520called%2520CLVER-DS%252C%2520which%2520is%2520fully%2520annotated%2520and%2520has%2520a%2520controllable%2520object%250Ascale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.14649v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DetailCLIP%3A%20Injecting%20Image%20Details%20into%20CLIP%27s%20Feature%20Space&entry.906535625=Zilun%20Zhang%20and%20Cuifeng%20Shen%20and%20Yuan%20Shen%20and%20Huixin%20Xiong%20and%20Xinyu%20Zhou%20and%20Tiancheng%20Zhao%20and%20Jianwei%20Yin&entry.1292438233=%20%20Although%20CLIP-like%20Visual%20Language%20Models%20provide%20a%20functional%20joint%20feature%0Aspace%20for%20image%20and%20text%2C%20due%20to%20the%20limitation%20of%20the%20CILP-like%20model%27s%20image%0Ainput%20size%20%28e.g.%2C%20224%29%2C%20subtle%20details%20are%20lost%20in%20the%20feature%20representation%0Aif%20we%20input%20high-resolution%20images%20%28e.g.%2C%202240%29.%20In%20this%20work%2C%20we%20introduce%20an%0Aefficient%20framework%20that%20can%20produce%20a%20single%20feature%20representation%20for%20a%0Ahigh-resolution%20image%20that%20injects%20image%20details%20and%20shares%20the%20same%20semantic%0Aspace%20as%20the%20original%20CLIP.%20In%20the%20framework%2C%20we%20train%20a%20feature%20fusing%20model%0Abased%20on%20CLIP%20features%20extracted%20from%20a%20carefully%20designed%20image%20patch%20method%0Athat%20can%20cover%20objects%20of%20any%20scale%2C%20weakly%20supervised%20by%20image-agnostic%20class%0Aprompted%20queries.%20We%20validate%20our%20framework%20by%20retrieving%20images%20from%20class%0Aprompted%20queries%20on%20the%20real%20world%20and%20synthetic%20datasets%2C%20showing%20significant%0Aperformance%20improvement%20on%20these%20tasks.%20Furthermore%2C%20to%20fully%20demonstrate%20our%0Aframework%27s%20detail%20retrieval%20ability%2C%20we%20construct%20a%20CLEVR-like%20synthetic%0Adataset%20called%20CLVER-DS%2C%20which%20is%20fully%20annotated%20and%20has%20a%20controllable%20object%0Ascale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.14649v6&entry.124074799=Read"},
{"title": "A 2D Semantic-Aware Position Encoding for Vision Transformers", "author": "Xi Chen and Shiyang Zhou and Muqi Huang and Jiaxu Feng and Yun Xiong and Kun Zhou and Biao Yang and Yuhui Zhang and Huishuai Bao and Sijia Peng and Chuan Li and Feng Shi", "abstract": "  Vision transformers have demonstrated significant advantages in computer\nvision tasks due to their ability to capture long-range dependencies and\ncontextual relationships through self-attention. However, existing position\nencoding techniques, which are largely borrowed from natural language\nprocessing, fail to effectively capture semantic-aware positional relationships\nbetween image patches. Traditional approaches like absolute position encoding\nand relative position encoding primarily focus on 1D linear position\nrelationship, often neglecting the semantic similarity between distant yet\ncontextually related patches. These limitations hinder model generalization,\ntranslation equivariance, and the ability to effectively handle repetitive or\nstructured patterns in images. In this paper, we propose 2-Dimensional\nSemantic-Aware Position Encoding ($\\text{SaPE}^2$), a novel position encoding\nmethod with semantic awareness that dynamically adapts position representations\nby leveraging local content instead of fixed linear position relationship or\nspatial coordinates. Our method enhances the model's ability to generalize\nacross varying image resolutions and scales, improves translation equivariance,\nand better aggregates features for visually similar but spatially distant\npatches. By integrating $\\text{SaPE}^2$ into vision transformers, we bridge the\ngap between position encoding and perceptual similarity, thereby improving\nperformance on computer vision tasks.\n", "link": "http://arxiv.org/abs/2505.09466v1", "date": "2025-05-14", "relevancy": 2.786, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%202D%20Semantic-Aware%20Position%20Encoding%20for%20Vision%20Transformers&body=Title%3A%20A%202D%20Semantic-Aware%20Position%20Encoding%20for%20Vision%20Transformers%0AAuthor%3A%20Xi%20Chen%20and%20Shiyang%20Zhou%20and%20Muqi%20Huang%20and%20Jiaxu%20Feng%20and%20Yun%20Xiong%20and%20Kun%20Zhou%20and%20Biao%20Yang%20and%20Yuhui%20Zhang%20and%20Huishuai%20Bao%20and%20Sijia%20Peng%20and%20Chuan%20Li%20and%20Feng%20Shi%0AAbstract%3A%20%20%20Vision%20transformers%20have%20demonstrated%20significant%20advantages%20in%20computer%0Avision%20tasks%20due%20to%20their%20ability%20to%20capture%20long-range%20dependencies%20and%0Acontextual%20relationships%20through%20self-attention.%20However%2C%20existing%20position%0Aencoding%20techniques%2C%20which%20are%20largely%20borrowed%20from%20natural%20language%0Aprocessing%2C%20fail%20to%20effectively%20capture%20semantic-aware%20positional%20relationships%0Abetween%20image%20patches.%20Traditional%20approaches%20like%20absolute%20position%20encoding%0Aand%20relative%20position%20encoding%20primarily%20focus%20on%201D%20linear%20position%0Arelationship%2C%20often%20neglecting%20the%20semantic%20similarity%20between%20distant%20yet%0Acontextually%20related%20patches.%20These%20limitations%20hinder%20model%20generalization%2C%0Atranslation%20equivariance%2C%20and%20the%20ability%20to%20effectively%20handle%20repetitive%20or%0Astructured%20patterns%20in%20images.%20In%20this%20paper%2C%20we%20propose%202-Dimensional%0ASemantic-Aware%20Position%20Encoding%20%28%24%5Ctext%7BSaPE%7D%5E2%24%29%2C%20a%20novel%20position%20encoding%0Amethod%20with%20semantic%20awareness%20that%20dynamically%20adapts%20position%20representations%0Aby%20leveraging%20local%20content%20instead%20of%20fixed%20linear%20position%20relationship%20or%0Aspatial%20coordinates.%20Our%20method%20enhances%20the%20model%27s%20ability%20to%20generalize%0Aacross%20varying%20image%20resolutions%20and%20scales%2C%20improves%20translation%20equivariance%2C%0Aand%20better%20aggregates%20features%20for%20visually%20similar%20but%20spatially%20distant%0Apatches.%20By%20integrating%20%24%5Ctext%7BSaPE%7D%5E2%24%20into%20vision%20transformers%2C%20we%20bridge%20the%0Agap%20between%20position%20encoding%20and%20perceptual%20similarity%2C%20thereby%20improving%0Aperformance%20on%20computer%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%25202D%2520Semantic-Aware%2520Position%2520Encoding%2520for%2520Vision%2520Transformers%26entry.906535625%3DXi%2520Chen%2520and%2520Shiyang%2520Zhou%2520and%2520Muqi%2520Huang%2520and%2520Jiaxu%2520Feng%2520and%2520Yun%2520Xiong%2520and%2520Kun%2520Zhou%2520and%2520Biao%2520Yang%2520and%2520Yuhui%2520Zhang%2520and%2520Huishuai%2520Bao%2520and%2520Sijia%2520Peng%2520and%2520Chuan%2520Li%2520and%2520Feng%2520Shi%26entry.1292438233%3D%2520%2520Vision%2520transformers%2520have%2520demonstrated%2520significant%2520advantages%2520in%2520computer%250Avision%2520tasks%2520due%2520to%2520their%2520ability%2520to%2520capture%2520long-range%2520dependencies%2520and%250Acontextual%2520relationships%2520through%2520self-attention.%2520However%252C%2520existing%2520position%250Aencoding%2520techniques%252C%2520which%2520are%2520largely%2520borrowed%2520from%2520natural%2520language%250Aprocessing%252C%2520fail%2520to%2520effectively%2520capture%2520semantic-aware%2520positional%2520relationships%250Abetween%2520image%2520patches.%2520Traditional%2520approaches%2520like%2520absolute%2520position%2520encoding%250Aand%2520relative%2520position%2520encoding%2520primarily%2520focus%2520on%25201D%2520linear%2520position%250Arelationship%252C%2520often%2520neglecting%2520the%2520semantic%2520similarity%2520between%2520distant%2520yet%250Acontextually%2520related%2520patches.%2520These%2520limitations%2520hinder%2520model%2520generalization%252C%250Atranslation%2520equivariance%252C%2520and%2520the%2520ability%2520to%2520effectively%2520handle%2520repetitive%2520or%250Astructured%2520patterns%2520in%2520images.%2520In%2520this%2520paper%252C%2520we%2520propose%25202-Dimensional%250ASemantic-Aware%2520Position%2520Encoding%2520%2528%2524%255Ctext%257BSaPE%257D%255E2%2524%2529%252C%2520a%2520novel%2520position%2520encoding%250Amethod%2520with%2520semantic%2520awareness%2520that%2520dynamically%2520adapts%2520position%2520representations%250Aby%2520leveraging%2520local%2520content%2520instead%2520of%2520fixed%2520linear%2520position%2520relationship%2520or%250Aspatial%2520coordinates.%2520Our%2520method%2520enhances%2520the%2520model%2527s%2520ability%2520to%2520generalize%250Aacross%2520varying%2520image%2520resolutions%2520and%2520scales%252C%2520improves%2520translation%2520equivariance%252C%250Aand%2520better%2520aggregates%2520features%2520for%2520visually%2520similar%2520but%2520spatially%2520distant%250Apatches.%2520By%2520integrating%2520%2524%255Ctext%257BSaPE%257D%255E2%2524%2520into%2520vision%2520transformers%252C%2520we%2520bridge%2520the%250Agap%2520between%2520position%2520encoding%2520and%2520perceptual%2520similarity%252C%2520thereby%2520improving%250Aperformance%2520on%2520computer%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%202D%20Semantic-Aware%20Position%20Encoding%20for%20Vision%20Transformers&entry.906535625=Xi%20Chen%20and%20Shiyang%20Zhou%20and%20Muqi%20Huang%20and%20Jiaxu%20Feng%20and%20Yun%20Xiong%20and%20Kun%20Zhou%20and%20Biao%20Yang%20and%20Yuhui%20Zhang%20and%20Huishuai%20Bao%20and%20Sijia%20Peng%20and%20Chuan%20Li%20and%20Feng%20Shi&entry.1292438233=%20%20Vision%20transformers%20have%20demonstrated%20significant%20advantages%20in%20computer%0Avision%20tasks%20due%20to%20their%20ability%20to%20capture%20long-range%20dependencies%20and%0Acontextual%20relationships%20through%20self-attention.%20However%2C%20existing%20position%0Aencoding%20techniques%2C%20which%20are%20largely%20borrowed%20from%20natural%20language%0Aprocessing%2C%20fail%20to%20effectively%20capture%20semantic-aware%20positional%20relationships%0Abetween%20image%20patches.%20Traditional%20approaches%20like%20absolute%20position%20encoding%0Aand%20relative%20position%20encoding%20primarily%20focus%20on%201D%20linear%20position%0Arelationship%2C%20often%20neglecting%20the%20semantic%20similarity%20between%20distant%20yet%0Acontextually%20related%20patches.%20These%20limitations%20hinder%20model%20generalization%2C%0Atranslation%20equivariance%2C%20and%20the%20ability%20to%20effectively%20handle%20repetitive%20or%0Astructured%20patterns%20in%20images.%20In%20this%20paper%2C%20we%20propose%202-Dimensional%0ASemantic-Aware%20Position%20Encoding%20%28%24%5Ctext%7BSaPE%7D%5E2%24%29%2C%20a%20novel%20position%20encoding%0Amethod%20with%20semantic%20awareness%20that%20dynamically%20adapts%20position%20representations%0Aby%20leveraging%20local%20content%20instead%20of%20fixed%20linear%20position%20relationship%20or%0Aspatial%20coordinates.%20Our%20method%20enhances%20the%20model%27s%20ability%20to%20generalize%0Aacross%20varying%20image%20resolutions%20and%20scales%2C%20improves%20translation%20equivariance%2C%0Aand%20better%20aggregates%20features%20for%20visually%20similar%20but%20spatially%20distant%0Apatches.%20By%20integrating%20%24%5Ctext%7BSaPE%7D%5E2%24%20into%20vision%20transformers%2C%20we%20bridge%20the%0Agap%20between%20position%20encoding%20and%20perceptual%20similarity%2C%20thereby%20improving%0Aperformance%20on%20computer%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09466v1&entry.124074799=Read"},
{"title": "Variational Visual Question Answering", "author": "Tobias Jan Wieczorek and Nathalie Daun and Mohammad Emtiyaz Khan and Marcus Rohrbach", "abstract": "  Despite remarkable progress in multimodal models for Visual Question\nAnswering (VQA), there remain major reliability concerns because the models can\noften be overconfident and miscalibrated, especially in out-of-distribution\n(OOD) settings. Plenty has been done to address such issues for unimodal\nmodels, but little work exists for multimodal cases. Here, we address\nunreliability in multimodal models by proposing a Variational VQA approach.\nSpecifically, instead of fine-tuning vision-language models by using AdamW, we\nemploy a recently proposed variational algorithm called IVON, which yields a\nposterior distribution over model parameters. Through extensive experiments, we\nshow that our approach improves calibration and abstentions without sacrificing\nthe accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce\nExpected Calibration Error by more than 50% compared to the AdamW baseline and\nraise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of\ndistribution shifts, the performance gain is even higher, achieving 8% Coverage\n(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we\npresent variational learning as a viable option to enhance the reliability of\nmultimodal models.\n", "link": "http://arxiv.org/abs/2505.09591v1", "date": "2025-05-14", "relevancy": 2.7772, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Visual%20Question%20Answering&body=Title%3A%20Variational%20Visual%20Question%20Answering%0AAuthor%3A%20Tobias%20Jan%20Wieczorek%20and%20Nathalie%20Daun%20and%20Mohammad%20Emtiyaz%20Khan%20and%20Marcus%20Rohrbach%0AAbstract%3A%20%20%20Despite%20remarkable%20progress%20in%20multimodal%20models%20for%20Visual%20Question%0AAnswering%20%28VQA%29%2C%20there%20remain%20major%20reliability%20concerns%20because%20the%20models%20can%0Aoften%20be%20overconfident%20and%20miscalibrated%2C%20especially%20in%20out-of-distribution%0A%28OOD%29%20settings.%20Plenty%20has%20been%20done%20to%20address%20such%20issues%20for%20unimodal%0Amodels%2C%20but%20little%20work%20exists%20for%20multimodal%20cases.%20Here%2C%20we%20address%0Aunreliability%20in%20multimodal%20models%20by%20proposing%20a%20Variational%20VQA%20approach.%0ASpecifically%2C%20instead%20of%20fine-tuning%20vision-language%20models%20by%20using%20AdamW%2C%20we%0Aemploy%20a%20recently%20proposed%20variational%20algorithm%20called%20IVON%2C%20which%20yields%20a%0Aposterior%20distribution%20over%20model%20parameters.%20Through%20extensive%20experiments%2C%20we%0Ashow%20that%20our%20approach%20improves%20calibration%20and%20abstentions%20without%20sacrificing%0Athe%20accuracy%20of%20AdamW.%20For%20instance%2C%20compared%20to%20AdamW%20fine-tuning%2C%20we%20reduce%0AExpected%20Calibration%20Error%20by%20more%20than%2050%25%20compared%20to%20the%20AdamW%20baseline%20and%0Araise%20Coverage%20by%204%25%20vs.%20SOTA%20%28for%20a%20fixed%20risk%20of%201%25%29.%20In%20the%20presence%20of%0Adistribution%20shifts%2C%20the%20performance%20gain%20is%20even%20higher%2C%20achieving%208%25%20Coverage%0A%28%40%201%25%20risk%29%20improvement%20vs.%20SOTA%20when%2050%25%20of%20test%20cases%20are%20OOD.%20Overall%2C%20we%0Apresent%20variational%20learning%20as%20a%20viable%20option%20to%20enhance%20the%20reliability%20of%0Amultimodal%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Visual%2520Question%2520Answering%26entry.906535625%3DTobias%2520Jan%2520Wieczorek%2520and%2520Nathalie%2520Daun%2520and%2520Mohammad%2520Emtiyaz%2520Khan%2520and%2520Marcus%2520Rohrbach%26entry.1292438233%3D%2520%2520Despite%2520remarkable%2520progress%2520in%2520multimodal%2520models%2520for%2520Visual%2520Question%250AAnswering%2520%2528VQA%2529%252C%2520there%2520remain%2520major%2520reliability%2520concerns%2520because%2520the%2520models%2520can%250Aoften%2520be%2520overconfident%2520and%2520miscalibrated%252C%2520especially%2520in%2520out-of-distribution%250A%2528OOD%2529%2520settings.%2520Plenty%2520has%2520been%2520done%2520to%2520address%2520such%2520issues%2520for%2520unimodal%250Amodels%252C%2520but%2520little%2520work%2520exists%2520for%2520multimodal%2520cases.%2520Here%252C%2520we%2520address%250Aunreliability%2520in%2520multimodal%2520models%2520by%2520proposing%2520a%2520Variational%2520VQA%2520approach.%250ASpecifically%252C%2520instead%2520of%2520fine-tuning%2520vision-language%2520models%2520by%2520using%2520AdamW%252C%2520we%250Aemploy%2520a%2520recently%2520proposed%2520variational%2520algorithm%2520called%2520IVON%252C%2520which%2520yields%2520a%250Aposterior%2520distribution%2520over%2520model%2520parameters.%2520Through%2520extensive%2520experiments%252C%2520we%250Ashow%2520that%2520our%2520approach%2520improves%2520calibration%2520and%2520abstentions%2520without%2520sacrificing%250Athe%2520accuracy%2520of%2520AdamW.%2520For%2520instance%252C%2520compared%2520to%2520AdamW%2520fine-tuning%252C%2520we%2520reduce%250AExpected%2520Calibration%2520Error%2520by%2520more%2520than%252050%2525%2520compared%2520to%2520the%2520AdamW%2520baseline%2520and%250Araise%2520Coverage%2520by%25204%2525%2520vs.%2520SOTA%2520%2528for%2520a%2520fixed%2520risk%2520of%25201%2525%2529.%2520In%2520the%2520presence%2520of%250Adistribution%2520shifts%252C%2520the%2520performance%2520gain%2520is%2520even%2520higher%252C%2520achieving%25208%2525%2520Coverage%250A%2528%2540%25201%2525%2520risk%2529%2520improvement%2520vs.%2520SOTA%2520when%252050%2525%2520of%2520test%2520cases%2520are%2520OOD.%2520Overall%252C%2520we%250Apresent%2520variational%2520learning%2520as%2520a%2520viable%2520option%2520to%2520enhance%2520the%2520reliability%2520of%250Amultimodal%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Visual%20Question%20Answering&entry.906535625=Tobias%20Jan%20Wieczorek%20and%20Nathalie%20Daun%20and%20Mohammad%20Emtiyaz%20Khan%20and%20Marcus%20Rohrbach&entry.1292438233=%20%20Despite%20remarkable%20progress%20in%20multimodal%20models%20for%20Visual%20Question%0AAnswering%20%28VQA%29%2C%20there%20remain%20major%20reliability%20concerns%20because%20the%20models%20can%0Aoften%20be%20overconfident%20and%20miscalibrated%2C%20especially%20in%20out-of-distribution%0A%28OOD%29%20settings.%20Plenty%20has%20been%20done%20to%20address%20such%20issues%20for%20unimodal%0Amodels%2C%20but%20little%20work%20exists%20for%20multimodal%20cases.%20Here%2C%20we%20address%0Aunreliability%20in%20multimodal%20models%20by%20proposing%20a%20Variational%20VQA%20approach.%0ASpecifically%2C%20instead%20of%20fine-tuning%20vision-language%20models%20by%20using%20AdamW%2C%20we%0Aemploy%20a%20recently%20proposed%20variational%20algorithm%20called%20IVON%2C%20which%20yields%20a%0Aposterior%20distribution%20over%20model%20parameters.%20Through%20extensive%20experiments%2C%20we%0Ashow%20that%20our%20approach%20improves%20calibration%20and%20abstentions%20without%20sacrificing%0Athe%20accuracy%20of%20AdamW.%20For%20instance%2C%20compared%20to%20AdamW%20fine-tuning%2C%20we%20reduce%0AExpected%20Calibration%20Error%20by%20more%20than%2050%25%20compared%20to%20the%20AdamW%20baseline%20and%0Araise%20Coverage%20by%204%25%20vs.%20SOTA%20%28for%20a%20fixed%20risk%20of%201%25%29.%20In%20the%20presence%20of%0Adistribution%20shifts%2C%20the%20performance%20gain%20is%20even%20higher%2C%20achieving%208%25%20Coverage%0A%28%40%201%25%20risk%29%20improvement%20vs.%20SOTA%20when%2050%25%20of%20test%20cases%20are%20OOD.%20Overall%2C%20we%0Apresent%20variational%20learning%20as%20a%20viable%20option%20to%20enhance%20the%20reliability%20of%0Amultimodal%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09591v1&entry.124074799=Read"},
{"title": "Advancing Drug Discovery with Enhanced Chemical Understanding via\n  Asymmetric Contrastive Multimodal Learning", "author": "Yifei Wang and Yunrui Li and Lin Liu and Pengyu Hong and Hao Xu", "abstract": "  The versatility of multimodal deep learning holds tremendous promise for\nadvancing scientific research and practical applications. As this field\ncontinues to evolve, the collective power of cross-modal analysis promises to\ndrive transformative innovations, opening new frontiers in chemical\nunderstanding and drug discovery. Hence, we introduce Asymmetric Contrastive\nMultimodal Learning (ACML), a specifically designed approach to enhance\nmolecular understanding and accelerate advancements in drug discovery. ACML\nharnesses the power of effective asymmetric contrastive learning to seamlessly\ntransfer information from various chemical modalities to molecular graph\nrepresentations. By combining pre-trained chemical unimodal encoders and a\nshallow-designed graph encoder with 5 layers, ACML facilitates the assimilation\nof coordinated chemical semantics from different modalities, leading to\ncomprehensive representation learning with efficient training. We demonstrate\nthe effectiveness of this framework through large-scale cross-modality\nretrieval and isomer discrimination tasks. Additionally, ACML enhances\ninterpretability by revealing chemical semantics in graph presentations and\nbolsters the expressive power of graph neural networks, as evidenced by\nimproved performance in molecular property prediction tasks from MoleculeNet\nand Therapeutics Data Commons (TDC). Ultimately, ACML exemplifies its potential\nto revolutionize molecular representational learning, offering deeper insights\ninto the chemical semantics of diverse modalities and paving the way for\ngroundbreaking advancements in chemical research and drug discovery.\n", "link": "http://arxiv.org/abs/2311.06456v6", "date": "2025-05-14", "relevancy": 2.7708, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Drug%20Discovery%20with%20Enhanced%20Chemical%20Understanding%20via%0A%20%20Asymmetric%20Contrastive%20Multimodal%20Learning&body=Title%3A%20Advancing%20Drug%20Discovery%20with%20Enhanced%20Chemical%20Understanding%20via%0A%20%20Asymmetric%20Contrastive%20Multimodal%20Learning%0AAuthor%3A%20Yifei%20Wang%20and%20Yunrui%20Li%20and%20Lin%20Liu%20and%20Pengyu%20Hong%20and%20Hao%20Xu%0AAbstract%3A%20%20%20The%20versatility%20of%20multimodal%20deep%20learning%20holds%20tremendous%20promise%20for%0Aadvancing%20scientific%20research%20and%20practical%20applications.%20As%20this%20field%0Acontinues%20to%20evolve%2C%20the%20collective%20power%20of%20cross-modal%20analysis%20promises%20to%0Adrive%20transformative%20innovations%2C%20opening%20new%20frontiers%20in%20chemical%0Aunderstanding%20and%20drug%20discovery.%20Hence%2C%20we%20introduce%20Asymmetric%20Contrastive%0AMultimodal%20Learning%20%28ACML%29%2C%20a%20specifically%20designed%20approach%20to%20enhance%0Amolecular%20understanding%20and%20accelerate%20advancements%20in%20drug%20discovery.%20ACML%0Aharnesses%20the%20power%20of%20effective%20asymmetric%20contrastive%20learning%20to%20seamlessly%0Atransfer%20information%20from%20various%20chemical%20modalities%20to%20molecular%20graph%0Arepresentations.%20By%20combining%20pre-trained%20chemical%20unimodal%20encoders%20and%20a%0Ashallow-designed%20graph%20encoder%20with%205%20layers%2C%20ACML%20facilitates%20the%20assimilation%0Aof%20coordinated%20chemical%20semantics%20from%20different%20modalities%2C%20leading%20to%0Acomprehensive%20representation%20learning%20with%20efficient%20training.%20We%20demonstrate%0Athe%20effectiveness%20of%20this%20framework%20through%20large-scale%20cross-modality%0Aretrieval%20and%20isomer%20discrimination%20tasks.%20Additionally%2C%20ACML%20enhances%0Ainterpretability%20by%20revealing%20chemical%20semantics%20in%20graph%20presentations%20and%0Abolsters%20the%20expressive%20power%20of%20graph%20neural%20networks%2C%20as%20evidenced%20by%0Aimproved%20performance%20in%20molecular%20property%20prediction%20tasks%20from%20MoleculeNet%0Aand%20Therapeutics%20Data%20Commons%20%28TDC%29.%20Ultimately%2C%20ACML%20exemplifies%20its%20potential%0Ato%20revolutionize%20molecular%20representational%20learning%2C%20offering%20deeper%20insights%0Ainto%20the%20chemical%20semantics%20of%20diverse%20modalities%20and%20paving%20the%20way%20for%0Agroundbreaking%20advancements%20in%20chemical%20research%20and%20drug%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.06456v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Drug%2520Discovery%2520with%2520Enhanced%2520Chemical%2520Understanding%2520via%250A%2520%2520Asymmetric%2520Contrastive%2520Multimodal%2520Learning%26entry.906535625%3DYifei%2520Wang%2520and%2520Yunrui%2520Li%2520and%2520Lin%2520Liu%2520and%2520Pengyu%2520Hong%2520and%2520Hao%2520Xu%26entry.1292438233%3D%2520%2520The%2520versatility%2520of%2520multimodal%2520deep%2520learning%2520holds%2520tremendous%2520promise%2520for%250Aadvancing%2520scientific%2520research%2520and%2520practical%2520applications.%2520As%2520this%2520field%250Acontinues%2520to%2520evolve%252C%2520the%2520collective%2520power%2520of%2520cross-modal%2520analysis%2520promises%2520to%250Adrive%2520transformative%2520innovations%252C%2520opening%2520new%2520frontiers%2520in%2520chemical%250Aunderstanding%2520and%2520drug%2520discovery.%2520Hence%252C%2520we%2520introduce%2520Asymmetric%2520Contrastive%250AMultimodal%2520Learning%2520%2528ACML%2529%252C%2520a%2520specifically%2520designed%2520approach%2520to%2520enhance%250Amolecular%2520understanding%2520and%2520accelerate%2520advancements%2520in%2520drug%2520discovery.%2520ACML%250Aharnesses%2520the%2520power%2520of%2520effective%2520asymmetric%2520contrastive%2520learning%2520to%2520seamlessly%250Atransfer%2520information%2520from%2520various%2520chemical%2520modalities%2520to%2520molecular%2520graph%250Arepresentations.%2520By%2520combining%2520pre-trained%2520chemical%2520unimodal%2520encoders%2520and%2520a%250Ashallow-designed%2520graph%2520encoder%2520with%25205%2520layers%252C%2520ACML%2520facilitates%2520the%2520assimilation%250Aof%2520coordinated%2520chemical%2520semantics%2520from%2520different%2520modalities%252C%2520leading%2520to%250Acomprehensive%2520representation%2520learning%2520with%2520efficient%2520training.%2520We%2520demonstrate%250Athe%2520effectiveness%2520of%2520this%2520framework%2520through%2520large-scale%2520cross-modality%250Aretrieval%2520and%2520isomer%2520discrimination%2520tasks.%2520Additionally%252C%2520ACML%2520enhances%250Ainterpretability%2520by%2520revealing%2520chemical%2520semantics%2520in%2520graph%2520presentations%2520and%250Abolsters%2520the%2520expressive%2520power%2520of%2520graph%2520neural%2520networks%252C%2520as%2520evidenced%2520by%250Aimproved%2520performance%2520in%2520molecular%2520property%2520prediction%2520tasks%2520from%2520MoleculeNet%250Aand%2520Therapeutics%2520Data%2520Commons%2520%2528TDC%2529.%2520Ultimately%252C%2520ACML%2520exemplifies%2520its%2520potential%250Ato%2520revolutionize%2520molecular%2520representational%2520learning%252C%2520offering%2520deeper%2520insights%250Ainto%2520the%2520chemical%2520semantics%2520of%2520diverse%2520modalities%2520and%2520paving%2520the%2520way%2520for%250Agroundbreaking%2520advancements%2520in%2520chemical%2520research%2520and%2520drug%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.06456v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Drug%20Discovery%20with%20Enhanced%20Chemical%20Understanding%20via%0A%20%20Asymmetric%20Contrastive%20Multimodal%20Learning&entry.906535625=Yifei%20Wang%20and%20Yunrui%20Li%20and%20Lin%20Liu%20and%20Pengyu%20Hong%20and%20Hao%20Xu&entry.1292438233=%20%20The%20versatility%20of%20multimodal%20deep%20learning%20holds%20tremendous%20promise%20for%0Aadvancing%20scientific%20research%20and%20practical%20applications.%20As%20this%20field%0Acontinues%20to%20evolve%2C%20the%20collective%20power%20of%20cross-modal%20analysis%20promises%20to%0Adrive%20transformative%20innovations%2C%20opening%20new%20frontiers%20in%20chemical%0Aunderstanding%20and%20drug%20discovery.%20Hence%2C%20we%20introduce%20Asymmetric%20Contrastive%0AMultimodal%20Learning%20%28ACML%29%2C%20a%20specifically%20designed%20approach%20to%20enhance%0Amolecular%20understanding%20and%20accelerate%20advancements%20in%20drug%20discovery.%20ACML%0Aharnesses%20the%20power%20of%20effective%20asymmetric%20contrastive%20learning%20to%20seamlessly%0Atransfer%20information%20from%20various%20chemical%20modalities%20to%20molecular%20graph%0Arepresentations.%20By%20combining%20pre-trained%20chemical%20unimodal%20encoders%20and%20a%0Ashallow-designed%20graph%20encoder%20with%205%20layers%2C%20ACML%20facilitates%20the%20assimilation%0Aof%20coordinated%20chemical%20semantics%20from%20different%20modalities%2C%20leading%20to%0Acomprehensive%20representation%20learning%20with%20efficient%20training.%20We%20demonstrate%0Athe%20effectiveness%20of%20this%20framework%20through%20large-scale%20cross-modality%0Aretrieval%20and%20isomer%20discrimination%20tasks.%20Additionally%2C%20ACML%20enhances%0Ainterpretability%20by%20revealing%20chemical%20semantics%20in%20graph%20presentations%20and%0Abolsters%20the%20expressive%20power%20of%20graph%20neural%20networks%2C%20as%20evidenced%20by%0Aimproved%20performance%20in%20molecular%20property%20prediction%20tasks%20from%20MoleculeNet%0Aand%20Therapeutics%20Data%20Commons%20%28TDC%29.%20Ultimately%2C%20ACML%20exemplifies%20its%20potential%0Ato%20revolutionize%20molecular%20representational%20learning%2C%20offering%20deeper%20insights%0Ainto%20the%20chemical%20semantics%20of%20diverse%20modalities%20and%20paving%20the%20way%20for%0Agroundbreaking%20advancements%20in%20chemical%20research%20and%20drug%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.06456v6&entry.124074799=Read"},
{"title": "MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for\n  Zero-shot Dermatological Assessment", "author": "Siyuan Yan and Xieji Li and Ming Hu and Yiwen Jiang and Zhen Yu and Zongyuan Ge", "abstract": "  Dermatological diagnosis represents a complex multimodal challenge that\nrequires integrating visual features with specialized clinical knowledge. While\nvision-language pretraining (VLP) has advanced medical AI, its effectiveness in\ndermatology is limited by text length constraints and the lack of structured\ntexts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced\nvision-language pretraining framework for zero-shot dermatological tasks.\nRecognizing that comprehensive dermatological descriptions require multiple\nknowledge aspects that exceed standard text constraints, our framework\nintroduces: (1) a multi-aspect contrastive learning strategy that decomposes\nclinical narratives into knowledge-enhanced sub-texts through large language\nmodels, (2) a fine-grained alignment mechanism that connects subcaptions with\ndiagnostically relevant image features, and (3) a diagnosis-guided weighting\nscheme that adaptively prioritizes different sub-captions based on clinical\nsignificance prior. Through pretraining on 403,563 dermatological image-text\npairs collected from education resources, MAKE significantly outperforms\nstate-of-the-art VLP models on eight datasets across zero-shot skin disease\nclassification, concept annotation, and cross-modal retrieval tasks. Our code\nwill be made publicly available at https: //github.com/SiyuanYan1/MAKE.\n", "link": "http://arxiv.org/abs/2505.09372v1", "date": "2025-05-14", "relevancy": 2.7535, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAKE%3A%20Multi-Aspect%20Knowledge-Enhanced%20Vision-Language%20Pretraining%20for%0A%20%20Zero-shot%20Dermatological%20Assessment&body=Title%3A%20MAKE%3A%20Multi-Aspect%20Knowledge-Enhanced%20Vision-Language%20Pretraining%20for%0A%20%20Zero-shot%20Dermatological%20Assessment%0AAuthor%3A%20Siyuan%20Yan%20and%20Xieji%20Li%20and%20Ming%20Hu%20and%20Yiwen%20Jiang%20and%20Zhen%20Yu%20and%20Zongyuan%20Ge%0AAbstract%3A%20%20%20Dermatological%20diagnosis%20represents%20a%20complex%20multimodal%20challenge%20that%0Arequires%20integrating%20visual%20features%20with%20specialized%20clinical%20knowledge.%20While%0Avision-language%20pretraining%20%28VLP%29%20has%20advanced%20medical%20AI%2C%20its%20effectiveness%20in%0Adermatology%20is%20limited%20by%20text%20length%20constraints%20and%20the%20lack%20of%20structured%0Atexts.%20In%20this%20paper%2C%20we%20introduce%20MAKE%2C%20a%20Multi-Aspect%20Knowledge-Enhanced%0Avision-language%20pretraining%20framework%20for%20zero-shot%20dermatological%20tasks.%0ARecognizing%20that%20comprehensive%20dermatological%20descriptions%20require%20multiple%0Aknowledge%20aspects%20that%20exceed%20standard%20text%20constraints%2C%20our%20framework%0Aintroduces%3A%20%281%29%20a%20multi-aspect%20contrastive%20learning%20strategy%20that%20decomposes%0Aclinical%20narratives%20into%20knowledge-enhanced%20sub-texts%20through%20large%20language%0Amodels%2C%20%282%29%20a%20fine-grained%20alignment%20mechanism%20that%20connects%20subcaptions%20with%0Adiagnostically%20relevant%20image%20features%2C%20and%20%283%29%20a%20diagnosis-guided%20weighting%0Ascheme%20that%20adaptively%20prioritizes%20different%20sub-captions%20based%20on%20clinical%0Asignificance%20prior.%20Through%20pretraining%20on%20403%2C563%20dermatological%20image-text%0Apairs%20collected%20from%20education%20resources%2C%20MAKE%20significantly%20outperforms%0Astate-of-the-art%20VLP%20models%20on%20eight%20datasets%20across%20zero-shot%20skin%20disease%0Aclassification%2C%20concept%20annotation%2C%20and%20cross-modal%20retrieval%20tasks.%20Our%20code%0Awill%20be%20made%20publicly%20available%20at%20https%3A%20//github.com/SiyuanYan1/MAKE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAKE%253A%2520Multi-Aspect%2520Knowledge-Enhanced%2520Vision-Language%2520Pretraining%2520for%250A%2520%2520Zero-shot%2520Dermatological%2520Assessment%26entry.906535625%3DSiyuan%2520Yan%2520and%2520Xieji%2520Li%2520and%2520Ming%2520Hu%2520and%2520Yiwen%2520Jiang%2520and%2520Zhen%2520Yu%2520and%2520Zongyuan%2520Ge%26entry.1292438233%3D%2520%2520Dermatological%2520diagnosis%2520represents%2520a%2520complex%2520multimodal%2520challenge%2520that%250Arequires%2520integrating%2520visual%2520features%2520with%2520specialized%2520clinical%2520knowledge.%2520While%250Avision-language%2520pretraining%2520%2528VLP%2529%2520has%2520advanced%2520medical%2520AI%252C%2520its%2520effectiveness%2520in%250Adermatology%2520is%2520limited%2520by%2520text%2520length%2520constraints%2520and%2520the%2520lack%2520of%2520structured%250Atexts.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MAKE%252C%2520a%2520Multi-Aspect%2520Knowledge-Enhanced%250Avision-language%2520pretraining%2520framework%2520for%2520zero-shot%2520dermatological%2520tasks.%250ARecognizing%2520that%2520comprehensive%2520dermatological%2520descriptions%2520require%2520multiple%250Aknowledge%2520aspects%2520that%2520exceed%2520standard%2520text%2520constraints%252C%2520our%2520framework%250Aintroduces%253A%2520%25281%2529%2520a%2520multi-aspect%2520contrastive%2520learning%2520strategy%2520that%2520decomposes%250Aclinical%2520narratives%2520into%2520knowledge-enhanced%2520sub-texts%2520through%2520large%2520language%250Amodels%252C%2520%25282%2529%2520a%2520fine-grained%2520alignment%2520mechanism%2520that%2520connects%2520subcaptions%2520with%250Adiagnostically%2520relevant%2520image%2520features%252C%2520and%2520%25283%2529%2520a%2520diagnosis-guided%2520weighting%250Ascheme%2520that%2520adaptively%2520prioritizes%2520different%2520sub-captions%2520based%2520on%2520clinical%250Asignificance%2520prior.%2520Through%2520pretraining%2520on%2520403%252C563%2520dermatological%2520image-text%250Apairs%2520collected%2520from%2520education%2520resources%252C%2520MAKE%2520significantly%2520outperforms%250Astate-of-the-art%2520VLP%2520models%2520on%2520eight%2520datasets%2520across%2520zero-shot%2520skin%2520disease%250Aclassification%252C%2520concept%2520annotation%252C%2520and%2520cross-modal%2520retrieval%2520tasks.%2520Our%2520code%250Awill%2520be%2520made%2520publicly%2520available%2520at%2520https%253A%2520//github.com/SiyuanYan1/MAKE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAKE%3A%20Multi-Aspect%20Knowledge-Enhanced%20Vision-Language%20Pretraining%20for%0A%20%20Zero-shot%20Dermatological%20Assessment&entry.906535625=Siyuan%20Yan%20and%20Xieji%20Li%20and%20Ming%20Hu%20and%20Yiwen%20Jiang%20and%20Zhen%20Yu%20and%20Zongyuan%20Ge&entry.1292438233=%20%20Dermatological%20diagnosis%20represents%20a%20complex%20multimodal%20challenge%20that%0Arequires%20integrating%20visual%20features%20with%20specialized%20clinical%20knowledge.%20While%0Avision-language%20pretraining%20%28VLP%29%20has%20advanced%20medical%20AI%2C%20its%20effectiveness%20in%0Adermatology%20is%20limited%20by%20text%20length%20constraints%20and%20the%20lack%20of%20structured%0Atexts.%20In%20this%20paper%2C%20we%20introduce%20MAKE%2C%20a%20Multi-Aspect%20Knowledge-Enhanced%0Avision-language%20pretraining%20framework%20for%20zero-shot%20dermatological%20tasks.%0ARecognizing%20that%20comprehensive%20dermatological%20descriptions%20require%20multiple%0Aknowledge%20aspects%20that%20exceed%20standard%20text%20constraints%2C%20our%20framework%0Aintroduces%3A%20%281%29%20a%20multi-aspect%20contrastive%20learning%20strategy%20that%20decomposes%0Aclinical%20narratives%20into%20knowledge-enhanced%20sub-texts%20through%20large%20language%0Amodels%2C%20%282%29%20a%20fine-grained%20alignment%20mechanism%20that%20connects%20subcaptions%20with%0Adiagnostically%20relevant%20image%20features%2C%20and%20%283%29%20a%20diagnosis-guided%20weighting%0Ascheme%20that%20adaptively%20prioritizes%20different%20sub-captions%20based%20on%20clinical%0Asignificance%20prior.%20Through%20pretraining%20on%20403%2C563%20dermatological%20image-text%0Apairs%20collected%20from%20education%20resources%2C%20MAKE%20significantly%20outperforms%0Astate-of-the-art%20VLP%20models%20on%20eight%20datasets%20across%20zero-shot%20skin%20disease%0Aclassification%2C%20concept%20annotation%2C%20and%20cross-modal%20retrieval%20tasks.%20Our%20code%0Awill%20be%20made%20publicly%20available%20at%20https%3A%20//github.com/SiyuanYan1/MAKE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09372v1&entry.124074799=Read"},
{"title": "HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh\n  Quality Assessment", "author": "Armin Shafiee Sarvestani and Sheyang Tang and Zhou Wang", "abstract": "  Mesh quality assessment (MQA) models play a critical role in the design,\noptimization, and evaluation of mesh operation systems in a wide variety of\napplications. Current MQA models, whether model-based methods using\ntopology-aware features or projection-based approaches working on rendered 2D\nprojections, often fail to capture the intricate interactions between texture\nand 3D geometry. We introduce HybridMQA, a first-of-its-kind hybrid\nfull-reference colored MQA framework that integrates model-based and\nprojection-based approaches, capturing complex interactions between textural\ninformation and 3D structures for enriched quality representations. Our method\nemploys graph learning to extract detailed 3D representations, which are then\nprojected to 2D using a novel feature rendering process that precisely aligns\nthem with colored projections. This enables the exploration of geometry-texture\ninteractions via cross-attention, producing comprehensive mesh quality\nrepresentations. Extensive experiments demonstrate HybridMQA's superior\nperformance across diverse datasets, highlighting its ability to effectively\nleverage geometry-texture interactions for a thorough understanding of mesh\nquality. Our implementation will be made publicly available.\n", "link": "http://arxiv.org/abs/2412.01986v2", "date": "2025-05-14", "relevancy": 2.7414, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6133}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5159}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HybridMQA%3A%20Exploring%20Geometry-Texture%20Interactions%20for%20Colored%20Mesh%0A%20%20Quality%20Assessment&body=Title%3A%20HybridMQA%3A%20Exploring%20Geometry-Texture%20Interactions%20for%20Colored%20Mesh%0A%20%20Quality%20Assessment%0AAuthor%3A%20Armin%20Shafiee%20Sarvestani%20and%20Sheyang%20Tang%20and%20Zhou%20Wang%0AAbstract%3A%20%20%20Mesh%20quality%20assessment%20%28MQA%29%20models%20play%20a%20critical%20role%20in%20the%20design%2C%0Aoptimization%2C%20and%20evaluation%20of%20mesh%20operation%20systems%20in%20a%20wide%20variety%20of%0Aapplications.%20Current%20MQA%20models%2C%20whether%20model-based%20methods%20using%0Atopology-aware%20features%20or%20projection-based%20approaches%20working%20on%20rendered%202D%0Aprojections%2C%20often%20fail%20to%20capture%20the%20intricate%20interactions%20between%20texture%0Aand%203D%20geometry.%20We%20introduce%20HybridMQA%2C%20a%20first-of-its-kind%20hybrid%0Afull-reference%20colored%20MQA%20framework%20that%20integrates%20model-based%20and%0Aprojection-based%20approaches%2C%20capturing%20complex%20interactions%20between%20textural%0Ainformation%20and%203D%20structures%20for%20enriched%20quality%20representations.%20Our%20method%0Aemploys%20graph%20learning%20to%20extract%20detailed%203D%20representations%2C%20which%20are%20then%0Aprojected%20to%202D%20using%20a%20novel%20feature%20rendering%20process%20that%20precisely%20aligns%0Athem%20with%20colored%20projections.%20This%20enables%20the%20exploration%20of%20geometry-texture%0Ainteractions%20via%20cross-attention%2C%20producing%20comprehensive%20mesh%20quality%0Arepresentations.%20Extensive%20experiments%20demonstrate%20HybridMQA%27s%20superior%0Aperformance%20across%20diverse%20datasets%2C%20highlighting%20its%20ability%20to%20effectively%0Aleverage%20geometry-texture%20interactions%20for%20a%20thorough%20understanding%20of%20mesh%0Aquality.%20Our%20implementation%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01986v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybridMQA%253A%2520Exploring%2520Geometry-Texture%2520Interactions%2520for%2520Colored%2520Mesh%250A%2520%2520Quality%2520Assessment%26entry.906535625%3DArmin%2520Shafiee%2520Sarvestani%2520and%2520Sheyang%2520Tang%2520and%2520Zhou%2520Wang%26entry.1292438233%3D%2520%2520Mesh%2520quality%2520assessment%2520%2528MQA%2529%2520models%2520play%2520a%2520critical%2520role%2520in%2520the%2520design%252C%250Aoptimization%252C%2520and%2520evaluation%2520of%2520mesh%2520operation%2520systems%2520in%2520a%2520wide%2520variety%2520of%250Aapplications.%2520Current%2520MQA%2520models%252C%2520whether%2520model-based%2520methods%2520using%250Atopology-aware%2520features%2520or%2520projection-based%2520approaches%2520working%2520on%2520rendered%25202D%250Aprojections%252C%2520often%2520fail%2520to%2520capture%2520the%2520intricate%2520interactions%2520between%2520texture%250Aand%25203D%2520geometry.%2520We%2520introduce%2520HybridMQA%252C%2520a%2520first-of-its-kind%2520hybrid%250Afull-reference%2520colored%2520MQA%2520framework%2520that%2520integrates%2520model-based%2520and%250Aprojection-based%2520approaches%252C%2520capturing%2520complex%2520interactions%2520between%2520textural%250Ainformation%2520and%25203D%2520structures%2520for%2520enriched%2520quality%2520representations.%2520Our%2520method%250Aemploys%2520graph%2520learning%2520to%2520extract%2520detailed%25203D%2520representations%252C%2520which%2520are%2520then%250Aprojected%2520to%25202D%2520using%2520a%2520novel%2520feature%2520rendering%2520process%2520that%2520precisely%2520aligns%250Athem%2520with%2520colored%2520projections.%2520This%2520enables%2520the%2520exploration%2520of%2520geometry-texture%250Ainteractions%2520via%2520cross-attention%252C%2520producing%2520comprehensive%2520mesh%2520quality%250Arepresentations.%2520Extensive%2520experiments%2520demonstrate%2520HybridMQA%2527s%2520superior%250Aperformance%2520across%2520diverse%2520datasets%252C%2520highlighting%2520its%2520ability%2520to%2520effectively%250Aleverage%2520geometry-texture%2520interactions%2520for%2520a%2520thorough%2520understanding%2520of%2520mesh%250Aquality.%2520Our%2520implementation%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01986v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HybridMQA%3A%20Exploring%20Geometry-Texture%20Interactions%20for%20Colored%20Mesh%0A%20%20Quality%20Assessment&entry.906535625=Armin%20Shafiee%20Sarvestani%20and%20Sheyang%20Tang%20and%20Zhou%20Wang&entry.1292438233=%20%20Mesh%20quality%20assessment%20%28MQA%29%20models%20play%20a%20critical%20role%20in%20the%20design%2C%0Aoptimization%2C%20and%20evaluation%20of%20mesh%20operation%20systems%20in%20a%20wide%20variety%20of%0Aapplications.%20Current%20MQA%20models%2C%20whether%20model-based%20methods%20using%0Atopology-aware%20features%20or%20projection-based%20approaches%20working%20on%20rendered%202D%0Aprojections%2C%20often%20fail%20to%20capture%20the%20intricate%20interactions%20between%20texture%0Aand%203D%20geometry.%20We%20introduce%20HybridMQA%2C%20a%20first-of-its-kind%20hybrid%0Afull-reference%20colored%20MQA%20framework%20that%20integrates%20model-based%20and%0Aprojection-based%20approaches%2C%20capturing%20complex%20interactions%20between%20textural%0Ainformation%20and%203D%20structures%20for%20enriched%20quality%20representations.%20Our%20method%0Aemploys%20graph%20learning%20to%20extract%20detailed%203D%20representations%2C%20which%20are%20then%0Aprojected%20to%202D%20using%20a%20novel%20feature%20rendering%20process%20that%20precisely%20aligns%0Athem%20with%20colored%20projections.%20This%20enables%20the%20exploration%20of%20geometry-texture%0Ainteractions%20via%20cross-attention%2C%20producing%20comprehensive%20mesh%20quality%0Arepresentations.%20Extensive%20experiments%20demonstrate%20HybridMQA%27s%20superior%0Aperformance%20across%20diverse%20datasets%2C%20highlighting%20its%20ability%20to%20effectively%0Aleverage%20geometry-texture%20interactions%20for%20a%20thorough%20understanding%20of%20mesh%0Aquality.%20Our%20implementation%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01986v2&entry.124074799=Read"},
{"title": "Zero-Shot Multi-modal Large Language Model v.s. Supervised Deep\n  Learning: A Comparative Study on CT-Based Intracranial Hemorrhage Subtyping", "author": "Yinuo Wang and Yue Zeng and Kai Chen and Cai Meng and Chao Pan and Zhouping Tang", "abstract": "  Introduction: Timely identification of intracranial hemorrhage (ICH) subtypes\non non-contrast computed tomography is critical for prognosis prediction and\ntherapeutic decision-making, yet remains challenging due to low contrast and\nblurring boundaries. This study evaluates the performance of zero-shot\nmulti-modal large language models (MLLMs) compared to traditional deep learning\nmethods in ICH binary classification and subtyping. Methods: We utilized a\ndataset provided by RSNA, comprising 192 NCCT volumes. The study compares\nvarious MLLMs, including GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2,\nwith conventional deep learning models, including ResNet50 and Vision\nTransformer. Carefully crafted prompts were used to guide MLLMs in tasks such\nas ICH presence, subtype classification, localization, and volume estimation.\nResults: The results indicate that in the ICH binary classification task,\ntraditional deep learning models outperform MLLMs comprehensively. For subtype\nclassification, MLLMs also exhibit inferior performance compared to traditional\ndeep learning models, with Gemini 2.0 Flash achieving an macro-averaged\nprecision of 0.41 and a macro-averaged F1 score of 0.31. Conclusion: While\nMLLMs excel in interactive capabilities, their overall accuracy in ICH\nsubtyping is inferior to deep networks. However, MLLMs enhance interpretability\nthrough language interactions, indicating potential in medical imaging\nanalysis. Future efforts will focus on model refinement and developing more\nprecise MLLMs to improve performance in three-dimensional medical image\nprocessing.\n", "link": "http://arxiv.org/abs/2505.09252v1", "date": "2025-05-14", "relevancy": 2.7155, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Multi-modal%20Large%20Language%20Model%20v.s.%20Supervised%20Deep%0A%20%20Learning%3A%20A%20Comparative%20Study%20on%20CT-Based%20Intracranial%20Hemorrhage%20Subtyping&body=Title%3A%20Zero-Shot%20Multi-modal%20Large%20Language%20Model%20v.s.%20Supervised%20Deep%0A%20%20Learning%3A%20A%20Comparative%20Study%20on%20CT-Based%20Intracranial%20Hemorrhage%20Subtyping%0AAuthor%3A%20Yinuo%20Wang%20and%20Yue%20Zeng%20and%20Kai%20Chen%20and%20Cai%20Meng%20and%20Chao%20Pan%20and%20Zhouping%20Tang%0AAbstract%3A%20%20%20Introduction%3A%20Timely%20identification%20of%20intracranial%20hemorrhage%20%28ICH%29%20subtypes%0Aon%20non-contrast%20computed%20tomography%20is%20critical%20for%20prognosis%20prediction%20and%0Atherapeutic%20decision-making%2C%20yet%20remains%20challenging%20due%20to%20low%20contrast%20and%0Ablurring%20boundaries.%20This%20study%20evaluates%20the%20performance%20of%20zero-shot%0Amulti-modal%20large%20language%20models%20%28MLLMs%29%20compared%20to%20traditional%20deep%20learning%0Amethods%20in%20ICH%20binary%20classification%20and%20subtyping.%20Methods%3A%20We%20utilized%20a%0Adataset%20provided%20by%20RSNA%2C%20comprising%20192%20NCCT%20volumes.%20The%20study%20compares%0Avarious%20MLLMs%2C%20including%20GPT-4o%2C%20Gemini%202.0%20Flash%2C%20and%20Claude%203.5%20Sonnet%20V2%2C%0Awith%20conventional%20deep%20learning%20models%2C%20including%20ResNet50%20and%20Vision%0ATransformer.%20Carefully%20crafted%20prompts%20were%20used%20to%20guide%20MLLMs%20in%20tasks%20such%0Aas%20ICH%20presence%2C%20subtype%20classification%2C%20localization%2C%20and%20volume%20estimation.%0AResults%3A%20The%20results%20indicate%20that%20in%20the%20ICH%20binary%20classification%20task%2C%0Atraditional%20deep%20learning%20models%20outperform%20MLLMs%20comprehensively.%20For%20subtype%0Aclassification%2C%20MLLMs%20also%20exhibit%20inferior%20performance%20compared%20to%20traditional%0Adeep%20learning%20models%2C%20with%20Gemini%202.0%20Flash%20achieving%20an%20macro-averaged%0Aprecision%20of%200.41%20and%20a%20macro-averaged%20F1%20score%20of%200.31.%20Conclusion%3A%20While%0AMLLMs%20excel%20in%20interactive%20capabilities%2C%20their%20overall%20accuracy%20in%20ICH%0Asubtyping%20is%20inferior%20to%20deep%20networks.%20However%2C%20MLLMs%20enhance%20interpretability%0Athrough%20language%20interactions%2C%20indicating%20potential%20in%20medical%20imaging%0Aanalysis.%20Future%20efforts%20will%20focus%20on%20model%20refinement%20and%20developing%20more%0Aprecise%20MLLMs%20to%20improve%20performance%20in%20three-dimensional%20medical%20image%0Aprocessing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Multi-modal%2520Large%2520Language%2520Model%2520v.s.%2520Supervised%2520Deep%250A%2520%2520Learning%253A%2520A%2520Comparative%2520Study%2520on%2520CT-Based%2520Intracranial%2520Hemorrhage%2520Subtyping%26entry.906535625%3DYinuo%2520Wang%2520and%2520Yue%2520Zeng%2520and%2520Kai%2520Chen%2520and%2520Cai%2520Meng%2520and%2520Chao%2520Pan%2520and%2520Zhouping%2520Tang%26entry.1292438233%3D%2520%2520Introduction%253A%2520Timely%2520identification%2520of%2520intracranial%2520hemorrhage%2520%2528ICH%2529%2520subtypes%250Aon%2520non-contrast%2520computed%2520tomography%2520is%2520critical%2520for%2520prognosis%2520prediction%2520and%250Atherapeutic%2520decision-making%252C%2520yet%2520remains%2520challenging%2520due%2520to%2520low%2520contrast%2520and%250Ablurring%2520boundaries.%2520This%2520study%2520evaluates%2520the%2520performance%2520of%2520zero-shot%250Amulti-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520compared%2520to%2520traditional%2520deep%2520learning%250Amethods%2520in%2520ICH%2520binary%2520classification%2520and%2520subtyping.%2520Methods%253A%2520We%2520utilized%2520a%250Adataset%2520provided%2520by%2520RSNA%252C%2520comprising%2520192%2520NCCT%2520volumes.%2520The%2520study%2520compares%250Avarious%2520MLLMs%252C%2520including%2520GPT-4o%252C%2520Gemini%25202.0%2520Flash%252C%2520and%2520Claude%25203.5%2520Sonnet%2520V2%252C%250Awith%2520conventional%2520deep%2520learning%2520models%252C%2520including%2520ResNet50%2520and%2520Vision%250ATransformer.%2520Carefully%2520crafted%2520prompts%2520were%2520used%2520to%2520guide%2520MLLMs%2520in%2520tasks%2520such%250Aas%2520ICH%2520presence%252C%2520subtype%2520classification%252C%2520localization%252C%2520and%2520volume%2520estimation.%250AResults%253A%2520The%2520results%2520indicate%2520that%2520in%2520the%2520ICH%2520binary%2520classification%2520task%252C%250Atraditional%2520deep%2520learning%2520models%2520outperform%2520MLLMs%2520comprehensively.%2520For%2520subtype%250Aclassification%252C%2520MLLMs%2520also%2520exhibit%2520inferior%2520performance%2520compared%2520to%2520traditional%250Adeep%2520learning%2520models%252C%2520with%2520Gemini%25202.0%2520Flash%2520achieving%2520an%2520macro-averaged%250Aprecision%2520of%25200.41%2520and%2520a%2520macro-averaged%2520F1%2520score%2520of%25200.31.%2520Conclusion%253A%2520While%250AMLLMs%2520excel%2520in%2520interactive%2520capabilities%252C%2520their%2520overall%2520accuracy%2520in%2520ICH%250Asubtyping%2520is%2520inferior%2520to%2520deep%2520networks.%2520However%252C%2520MLLMs%2520enhance%2520interpretability%250Athrough%2520language%2520interactions%252C%2520indicating%2520potential%2520in%2520medical%2520imaging%250Aanalysis.%2520Future%2520efforts%2520will%2520focus%2520on%2520model%2520refinement%2520and%2520developing%2520more%250Aprecise%2520MLLMs%2520to%2520improve%2520performance%2520in%2520three-dimensional%2520medical%2520image%250Aprocessing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Multi-modal%20Large%20Language%20Model%20v.s.%20Supervised%20Deep%0A%20%20Learning%3A%20A%20Comparative%20Study%20on%20CT-Based%20Intracranial%20Hemorrhage%20Subtyping&entry.906535625=Yinuo%20Wang%20and%20Yue%20Zeng%20and%20Kai%20Chen%20and%20Cai%20Meng%20and%20Chao%20Pan%20and%20Zhouping%20Tang&entry.1292438233=%20%20Introduction%3A%20Timely%20identification%20of%20intracranial%20hemorrhage%20%28ICH%29%20subtypes%0Aon%20non-contrast%20computed%20tomography%20is%20critical%20for%20prognosis%20prediction%20and%0Atherapeutic%20decision-making%2C%20yet%20remains%20challenging%20due%20to%20low%20contrast%20and%0Ablurring%20boundaries.%20This%20study%20evaluates%20the%20performance%20of%20zero-shot%0Amulti-modal%20large%20language%20models%20%28MLLMs%29%20compared%20to%20traditional%20deep%20learning%0Amethods%20in%20ICH%20binary%20classification%20and%20subtyping.%20Methods%3A%20We%20utilized%20a%0Adataset%20provided%20by%20RSNA%2C%20comprising%20192%20NCCT%20volumes.%20The%20study%20compares%0Avarious%20MLLMs%2C%20including%20GPT-4o%2C%20Gemini%202.0%20Flash%2C%20and%20Claude%203.5%20Sonnet%20V2%2C%0Awith%20conventional%20deep%20learning%20models%2C%20including%20ResNet50%20and%20Vision%0ATransformer.%20Carefully%20crafted%20prompts%20were%20used%20to%20guide%20MLLMs%20in%20tasks%20such%0Aas%20ICH%20presence%2C%20subtype%20classification%2C%20localization%2C%20and%20volume%20estimation.%0AResults%3A%20The%20results%20indicate%20that%20in%20the%20ICH%20binary%20classification%20task%2C%0Atraditional%20deep%20learning%20models%20outperform%20MLLMs%20comprehensively.%20For%20subtype%0Aclassification%2C%20MLLMs%20also%20exhibit%20inferior%20performance%20compared%20to%20traditional%0Adeep%20learning%20models%2C%20with%20Gemini%202.0%20Flash%20achieving%20an%20macro-averaged%0Aprecision%20of%200.41%20and%20a%20macro-averaged%20F1%20score%20of%200.31.%20Conclusion%3A%20While%0AMLLMs%20excel%20in%20interactive%20capabilities%2C%20their%20overall%20accuracy%20in%20ICH%0Asubtyping%20is%20inferior%20to%20deep%20networks.%20However%2C%20MLLMs%20enhance%20interpretability%0Athrough%20language%20interactions%2C%20indicating%20potential%20in%20medical%20imaging%0Aanalysis.%20Future%20efforts%20will%20focus%20on%20model%20refinement%20and%20developing%20more%0Aprecise%20MLLMs%20to%20improve%20performance%20in%20three-dimensional%20medical%20image%0Aprocessing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09252v1&entry.124074799=Read"},
{"title": "DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in\n  Multivariate Time Series", "author": "Zahra Zamanzadeh Darban and Yiyuan Yang and Geoffrey I. Webb and Charu C. Aggarwal and Qingsong Wen and Shirui Pan and Mahsa Salehi", "abstract": "  In time series anomaly detection (TSAD), the scarcity of labeled data poses a\nchallenge to the development of accurate models. Unsupervised domain adaptation\n(UDA) offers a solution by leveraging labeled data from a related domain to\ndetect anomalies in an unlabeled target domain. However, existing UDA methods\nassume consistent anomalous classes across domains. To address this limitation,\nwe propose a novel Domain Adaptation Contrastive learning model for Anomaly\nDetection in multivariate time series (DACAD), combining UDA with contrastive\nlearning. DACAD utilizes an anomaly injection mechanism that enhances\ngeneralization across unseen anomalous classes, improving adaptability and\nrobustness. Additionally, our model employs supervised contrastive loss for the\nsource domain and self-supervised contrastive triplet loss for the target\ndomain, ensuring comprehensive feature representation learning and\ndomain-invariant feature extraction. Finally, an effective Center-based Entropy\nClassifier (CEC) accurately learns normal boundaries in the source domain.\nExtensive evaluations on multiple real-world datasets and a synthetic dataset\nhighlight DACAD's superior performance in transferring knowledge across domains\nand mitigating the challenge of limited labeled data in TSAD.\n", "link": "http://arxiv.org/abs/2404.11269v3", "date": "2025-05-14", "relevancy": 2.7, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5577}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5522}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DACAD%3A%20Domain%20Adaptation%20Contrastive%20Learning%20for%20Anomaly%20Detection%20in%0A%20%20Multivariate%20Time%20Series&body=Title%3A%20DACAD%3A%20Domain%20Adaptation%20Contrastive%20Learning%20for%20Anomaly%20Detection%20in%0A%20%20Multivariate%20Time%20Series%0AAuthor%3A%20Zahra%20Zamanzadeh%20Darban%20and%20Yiyuan%20Yang%20and%20Geoffrey%20I.%20Webb%20and%20Charu%20C.%20Aggarwal%20and%20Qingsong%20Wen%20and%20Shirui%20Pan%20and%20Mahsa%20Salehi%0AAbstract%3A%20%20%20In%20time%20series%20anomaly%20detection%20%28TSAD%29%2C%20the%20scarcity%20of%20labeled%20data%20poses%20a%0Achallenge%20to%20the%20development%20of%20accurate%20models.%20Unsupervised%20domain%20adaptation%0A%28UDA%29%20offers%20a%20solution%20by%20leveraging%20labeled%20data%20from%20a%20related%20domain%20to%0Adetect%20anomalies%20in%20an%20unlabeled%20target%20domain.%20However%2C%20existing%20UDA%20methods%0Aassume%20consistent%20anomalous%20classes%20across%20domains.%20To%20address%20this%20limitation%2C%0Awe%20propose%20a%20novel%20Domain%20Adaptation%20Contrastive%20learning%20model%20for%20Anomaly%0ADetection%20in%20multivariate%20time%20series%20%28DACAD%29%2C%20combining%20UDA%20with%20contrastive%0Alearning.%20DACAD%20utilizes%20an%20anomaly%20injection%20mechanism%20that%20enhances%0Ageneralization%20across%20unseen%20anomalous%20classes%2C%20improving%20adaptability%20and%0Arobustness.%20Additionally%2C%20our%20model%20employs%20supervised%20contrastive%20loss%20for%20the%0Asource%20domain%20and%20self-supervised%20contrastive%20triplet%20loss%20for%20the%20target%0Adomain%2C%20ensuring%20comprehensive%20feature%20representation%20learning%20and%0Adomain-invariant%20feature%20extraction.%20Finally%2C%20an%20effective%20Center-based%20Entropy%0AClassifier%20%28CEC%29%20accurately%20learns%20normal%20boundaries%20in%20the%20source%20domain.%0AExtensive%20evaluations%20on%20multiple%20real-world%20datasets%20and%20a%20synthetic%20dataset%0Ahighlight%20DACAD%27s%20superior%20performance%20in%20transferring%20knowledge%20across%20domains%0Aand%20mitigating%20the%20challenge%20of%20limited%20labeled%20data%20in%20TSAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11269v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDACAD%253A%2520Domain%2520Adaptation%2520Contrastive%2520Learning%2520for%2520Anomaly%2520Detection%2520in%250A%2520%2520Multivariate%2520Time%2520Series%26entry.906535625%3DZahra%2520Zamanzadeh%2520Darban%2520and%2520Yiyuan%2520Yang%2520and%2520Geoffrey%2520I.%2520Webb%2520and%2520Charu%2520C.%2520Aggarwal%2520and%2520Qingsong%2520Wen%2520and%2520Shirui%2520Pan%2520and%2520Mahsa%2520Salehi%26entry.1292438233%3D%2520%2520In%2520time%2520series%2520anomaly%2520detection%2520%2528TSAD%2529%252C%2520the%2520scarcity%2520of%2520labeled%2520data%2520poses%2520a%250Achallenge%2520to%2520the%2520development%2520of%2520accurate%2520models.%2520Unsupervised%2520domain%2520adaptation%250A%2528UDA%2529%2520offers%2520a%2520solution%2520by%2520leveraging%2520labeled%2520data%2520from%2520a%2520related%2520domain%2520to%250Adetect%2520anomalies%2520in%2520an%2520unlabeled%2520target%2520domain.%2520However%252C%2520existing%2520UDA%2520methods%250Aassume%2520consistent%2520anomalous%2520classes%2520across%2520domains.%2520To%2520address%2520this%2520limitation%252C%250Awe%2520propose%2520a%2520novel%2520Domain%2520Adaptation%2520Contrastive%2520learning%2520model%2520for%2520Anomaly%250ADetection%2520in%2520multivariate%2520time%2520series%2520%2528DACAD%2529%252C%2520combining%2520UDA%2520with%2520contrastive%250Alearning.%2520DACAD%2520utilizes%2520an%2520anomaly%2520injection%2520mechanism%2520that%2520enhances%250Ageneralization%2520across%2520unseen%2520anomalous%2520classes%252C%2520improving%2520adaptability%2520and%250Arobustness.%2520Additionally%252C%2520our%2520model%2520employs%2520supervised%2520contrastive%2520loss%2520for%2520the%250Asource%2520domain%2520and%2520self-supervised%2520contrastive%2520triplet%2520loss%2520for%2520the%2520target%250Adomain%252C%2520ensuring%2520comprehensive%2520feature%2520representation%2520learning%2520and%250Adomain-invariant%2520feature%2520extraction.%2520Finally%252C%2520an%2520effective%2520Center-based%2520Entropy%250AClassifier%2520%2528CEC%2529%2520accurately%2520learns%2520normal%2520boundaries%2520in%2520the%2520source%2520domain.%250AExtensive%2520evaluations%2520on%2520multiple%2520real-world%2520datasets%2520and%2520a%2520synthetic%2520dataset%250Ahighlight%2520DACAD%2527s%2520superior%2520performance%2520in%2520transferring%2520knowledge%2520across%2520domains%250Aand%2520mitigating%2520the%2520challenge%2520of%2520limited%2520labeled%2520data%2520in%2520TSAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11269v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DACAD%3A%20Domain%20Adaptation%20Contrastive%20Learning%20for%20Anomaly%20Detection%20in%0A%20%20Multivariate%20Time%20Series&entry.906535625=Zahra%20Zamanzadeh%20Darban%20and%20Yiyuan%20Yang%20and%20Geoffrey%20I.%20Webb%20and%20Charu%20C.%20Aggarwal%20and%20Qingsong%20Wen%20and%20Shirui%20Pan%20and%20Mahsa%20Salehi&entry.1292438233=%20%20In%20time%20series%20anomaly%20detection%20%28TSAD%29%2C%20the%20scarcity%20of%20labeled%20data%20poses%20a%0Achallenge%20to%20the%20development%20of%20accurate%20models.%20Unsupervised%20domain%20adaptation%0A%28UDA%29%20offers%20a%20solution%20by%20leveraging%20labeled%20data%20from%20a%20related%20domain%20to%0Adetect%20anomalies%20in%20an%20unlabeled%20target%20domain.%20However%2C%20existing%20UDA%20methods%0Aassume%20consistent%20anomalous%20classes%20across%20domains.%20To%20address%20this%20limitation%2C%0Awe%20propose%20a%20novel%20Domain%20Adaptation%20Contrastive%20learning%20model%20for%20Anomaly%0ADetection%20in%20multivariate%20time%20series%20%28DACAD%29%2C%20combining%20UDA%20with%20contrastive%0Alearning.%20DACAD%20utilizes%20an%20anomaly%20injection%20mechanism%20that%20enhances%0Ageneralization%20across%20unseen%20anomalous%20classes%2C%20improving%20adaptability%20and%0Arobustness.%20Additionally%2C%20our%20model%20employs%20supervised%20contrastive%20loss%20for%20the%0Asource%20domain%20and%20self-supervised%20contrastive%20triplet%20loss%20for%20the%20target%0Adomain%2C%20ensuring%20comprehensive%20feature%20representation%20learning%20and%0Adomain-invariant%20feature%20extraction.%20Finally%2C%20an%20effective%20Center-based%20Entropy%0AClassifier%20%28CEC%29%20accurately%20learns%20normal%20boundaries%20in%20the%20source%20domain.%0AExtensive%20evaluations%20on%20multiple%20real-world%20datasets%20and%20a%20synthetic%20dataset%0Ahighlight%20DACAD%27s%20superior%20performance%20in%20transferring%20knowledge%20across%20domains%0Aand%20mitigating%20the%20challenge%20of%20limited%20labeled%20data%20in%20TSAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11269v3&entry.124074799=Read"},
{"title": "Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using\n  Implicit Neural Representations", "author": "Maik Dannecker and Thomas Sanchez and Meritxell Bach Cuadra and \u00d6zg\u00fcn Turgut and Anthony N. Price and Lucilio Cordero-Grande and Vanessa Kyriakopoulou and Joseph V. Hajnal and Daniel Rueckert", "abstract": "  High-resolution slice-to-volume reconstruction (SVR) from multiple\nmotion-corrupted low-resolution 2D slices constitutes a critical step in\nimage-based diagnostics of moving subjects, such as fetal brain Magnetic\nResonance Imaging (MRI). Existing solutions struggle with image artifacts and\nsevere subject motion or require slice pre-alignment to achieve satisfying\nreconstruction performance. We propose a novel SVR method to enable fast and\naccurate MRI reconstruction even in cases of severe image and motion\ncorruption. Our approach performs motion correction, outlier handling, and\nsuper-resolution reconstruction with all operations being entirely based on\nimplicit neural representations. The model can be initialized with\ntask-specific priors through fully self-supervised meta-learning on either\nsimulated or real-world data. In extensive experiments including over 480\nreconstructions of simulated and clinical MRI brain data from different\ncenters, we prove the utility of our method in cases of severe subject motion\nand image artifacts. Our results demonstrate improvements in reconstruction\nquality, especially in the presence of severe motion, compared to\nstate-of-the-art methods, and up to 50% reduction in reconstruction time.\n", "link": "http://arxiv.org/abs/2505.09565v1", "date": "2025-05-14", "relevancy": 2.6993, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5522}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5355}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-learning%20Slice-to-Volume%20Reconstruction%20in%20Fetal%20Brain%20MRI%20using%0A%20%20Implicit%20Neural%20Representations&body=Title%3A%20Meta-learning%20Slice-to-Volume%20Reconstruction%20in%20Fetal%20Brain%20MRI%20using%0A%20%20Implicit%20Neural%20Representations%0AAuthor%3A%20Maik%20Dannecker%20and%20Thomas%20Sanchez%20and%20Meritxell%20Bach%20Cuadra%20and%20%C3%96zg%C3%BCn%20Turgut%20and%20Anthony%20N.%20Price%20and%20Lucilio%20Cordero-Grande%20and%20Vanessa%20Kyriakopoulou%20and%20Joseph%20V.%20Hajnal%20and%20Daniel%20Rueckert%0AAbstract%3A%20%20%20High-resolution%20slice-to-volume%20reconstruction%20%28SVR%29%20from%20multiple%0Amotion-corrupted%20low-resolution%202D%20slices%20constitutes%20a%20critical%20step%20in%0Aimage-based%20diagnostics%20of%20moving%20subjects%2C%20such%20as%20fetal%20brain%20Magnetic%0AResonance%20Imaging%20%28MRI%29.%20Existing%20solutions%20struggle%20with%20image%20artifacts%20and%0Asevere%20subject%20motion%20or%20require%20slice%20pre-alignment%20to%20achieve%20satisfying%0Areconstruction%20performance.%20We%20propose%20a%20novel%20SVR%20method%20to%20enable%20fast%20and%0Aaccurate%20MRI%20reconstruction%20even%20in%20cases%20of%20severe%20image%20and%20motion%0Acorruption.%20Our%20approach%20performs%20motion%20correction%2C%20outlier%20handling%2C%20and%0Asuper-resolution%20reconstruction%20with%20all%20operations%20being%20entirely%20based%20on%0Aimplicit%20neural%20representations.%20The%20model%20can%20be%20initialized%20with%0Atask-specific%20priors%20through%20fully%20self-supervised%20meta-learning%20on%20either%0Asimulated%20or%20real-world%20data.%20In%20extensive%20experiments%20including%20over%20480%0Areconstructions%20of%20simulated%20and%20clinical%20MRI%20brain%20data%20from%20different%0Acenters%2C%20we%20prove%20the%20utility%20of%20our%20method%20in%20cases%20of%20severe%20subject%20motion%0Aand%20image%20artifacts.%20Our%20results%20demonstrate%20improvements%20in%20reconstruction%0Aquality%2C%20especially%20in%20the%20presence%20of%20severe%20motion%2C%20compared%20to%0Astate-of-the-art%20methods%2C%20and%20up%20to%2050%25%20reduction%20in%20reconstruction%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-learning%2520Slice-to-Volume%2520Reconstruction%2520in%2520Fetal%2520Brain%2520MRI%2520using%250A%2520%2520Implicit%2520Neural%2520Representations%26entry.906535625%3DMaik%2520Dannecker%2520and%2520Thomas%2520Sanchez%2520and%2520Meritxell%2520Bach%2520Cuadra%2520and%2520%25C3%2596zg%25C3%25BCn%2520Turgut%2520and%2520Anthony%2520N.%2520Price%2520and%2520Lucilio%2520Cordero-Grande%2520and%2520Vanessa%2520Kyriakopoulou%2520and%2520Joseph%2520V.%2520Hajnal%2520and%2520Daniel%2520Rueckert%26entry.1292438233%3D%2520%2520High-resolution%2520slice-to-volume%2520reconstruction%2520%2528SVR%2529%2520from%2520multiple%250Amotion-corrupted%2520low-resolution%25202D%2520slices%2520constitutes%2520a%2520critical%2520step%2520in%250Aimage-based%2520diagnostics%2520of%2520moving%2520subjects%252C%2520such%2520as%2520fetal%2520brain%2520Magnetic%250AResonance%2520Imaging%2520%2528MRI%2529.%2520Existing%2520solutions%2520struggle%2520with%2520image%2520artifacts%2520and%250Asevere%2520subject%2520motion%2520or%2520require%2520slice%2520pre-alignment%2520to%2520achieve%2520satisfying%250Areconstruction%2520performance.%2520We%2520propose%2520a%2520novel%2520SVR%2520method%2520to%2520enable%2520fast%2520and%250Aaccurate%2520MRI%2520reconstruction%2520even%2520in%2520cases%2520of%2520severe%2520image%2520and%2520motion%250Acorruption.%2520Our%2520approach%2520performs%2520motion%2520correction%252C%2520outlier%2520handling%252C%2520and%250Asuper-resolution%2520reconstruction%2520with%2520all%2520operations%2520being%2520entirely%2520based%2520on%250Aimplicit%2520neural%2520representations.%2520The%2520model%2520can%2520be%2520initialized%2520with%250Atask-specific%2520priors%2520through%2520fully%2520self-supervised%2520meta-learning%2520on%2520either%250Asimulated%2520or%2520real-world%2520data.%2520In%2520extensive%2520experiments%2520including%2520over%2520480%250Areconstructions%2520of%2520simulated%2520and%2520clinical%2520MRI%2520brain%2520data%2520from%2520different%250Acenters%252C%2520we%2520prove%2520the%2520utility%2520of%2520our%2520method%2520in%2520cases%2520of%2520severe%2520subject%2520motion%250Aand%2520image%2520artifacts.%2520Our%2520results%2520demonstrate%2520improvements%2520in%2520reconstruction%250Aquality%252C%2520especially%2520in%2520the%2520presence%2520of%2520severe%2520motion%252C%2520compared%2520to%250Astate-of-the-art%2520methods%252C%2520and%2520up%2520to%252050%2525%2520reduction%2520in%2520reconstruction%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-learning%20Slice-to-Volume%20Reconstruction%20in%20Fetal%20Brain%20MRI%20using%0A%20%20Implicit%20Neural%20Representations&entry.906535625=Maik%20Dannecker%20and%20Thomas%20Sanchez%20and%20Meritxell%20Bach%20Cuadra%20and%20%C3%96zg%C3%BCn%20Turgut%20and%20Anthony%20N.%20Price%20and%20Lucilio%20Cordero-Grande%20and%20Vanessa%20Kyriakopoulou%20and%20Joseph%20V.%20Hajnal%20and%20Daniel%20Rueckert&entry.1292438233=%20%20High-resolution%20slice-to-volume%20reconstruction%20%28SVR%29%20from%20multiple%0Amotion-corrupted%20low-resolution%202D%20slices%20constitutes%20a%20critical%20step%20in%0Aimage-based%20diagnostics%20of%20moving%20subjects%2C%20such%20as%20fetal%20brain%20Magnetic%0AResonance%20Imaging%20%28MRI%29.%20Existing%20solutions%20struggle%20with%20image%20artifacts%20and%0Asevere%20subject%20motion%20or%20require%20slice%20pre-alignment%20to%20achieve%20satisfying%0Areconstruction%20performance.%20We%20propose%20a%20novel%20SVR%20method%20to%20enable%20fast%20and%0Aaccurate%20MRI%20reconstruction%20even%20in%20cases%20of%20severe%20image%20and%20motion%0Acorruption.%20Our%20approach%20performs%20motion%20correction%2C%20outlier%20handling%2C%20and%0Asuper-resolution%20reconstruction%20with%20all%20operations%20being%20entirely%20based%20on%0Aimplicit%20neural%20representations.%20The%20model%20can%20be%20initialized%20with%0Atask-specific%20priors%20through%20fully%20self-supervised%20meta-learning%20on%20either%0Asimulated%20or%20real-world%20data.%20In%20extensive%20experiments%20including%20over%20480%0Areconstructions%20of%20simulated%20and%20clinical%20MRI%20brain%20data%20from%20different%0Acenters%2C%20we%20prove%20the%20utility%20of%20our%20method%20in%20cases%20of%20severe%20subject%20motion%0Aand%20image%20artifacts.%20Our%20results%20demonstrate%20improvements%20in%20reconstruction%0Aquality%2C%20especially%20in%20the%20presence%20of%20severe%20motion%2C%20compared%20to%0Astate-of-the-art%20methods%2C%20and%20up%20to%2050%25%20reduction%20in%20reconstruction%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09565v1&entry.124074799=Read"},
{"title": "FedSaaS: Class-Consistency Federated Semantic Segmentation via Global\n  Prototype Supervision and Local Adversarial Harmonization", "author": "Xiaoyang Yu and Xiaoming Wu and Xin Wang and Dongrun Li and Ming Yang and Peng Cheng", "abstract": "  Federated semantic segmentation enables pixel-level classification in images\nthrough collaborative learning while maintaining data privacy. However,\nexisting research commonly overlooks the fine-grained class relationships\nwithin the semantic space when addressing heterogeneous problems, particularly\ndomain shift. This oversight results in ambiguities between class\nrepresentation. To overcome this challenge, we propose a novel federated\nsegmentation framework that strikes class consistency, termed FedSaaS.\nSpecifically, we introduce class exemplars as a criterion for both local- and\nglobal-level class representations. On the server side, the uploaded class\nexemplars are leveraged to model class prototypes, which supervise global\nbranch of clients, ensuring alignment with global-level representation. On the\nclient side, we incorporate an adversarial mechanism to harmonize contributions\nof global and local branches, leading to consistent output. Moreover,\nmultilevel contrastive losses are employed on both sides to enforce consistency\nbetween two-level representations in the same semantic space. Extensive\nexperiments on several driving scene segmentation datasets demonstrate that our\nframework outperforms state-of-the-art methods, significantly improving average\nsegmentation accuracy and effectively addressing the class-consistency\nrepresentation problem.\n", "link": "http://arxiv.org/abs/2505.09385v1", "date": "2025-05-14", "relevancy": 2.6978, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5527}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5377}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedSaaS%3A%20Class-Consistency%20Federated%20Semantic%20Segmentation%20via%20Global%0A%20%20Prototype%20Supervision%20and%20Local%20Adversarial%20Harmonization&body=Title%3A%20FedSaaS%3A%20Class-Consistency%20Federated%20Semantic%20Segmentation%20via%20Global%0A%20%20Prototype%20Supervision%20and%20Local%20Adversarial%20Harmonization%0AAuthor%3A%20Xiaoyang%20Yu%20and%20Xiaoming%20Wu%20and%20Xin%20Wang%20and%20Dongrun%20Li%20and%20Ming%20Yang%20and%20Peng%20Cheng%0AAbstract%3A%20%20%20Federated%20semantic%20segmentation%20enables%20pixel-level%20classification%20in%20images%0Athrough%20collaborative%20learning%20while%20maintaining%20data%20privacy.%20However%2C%0Aexisting%20research%20commonly%20overlooks%20the%20fine-grained%20class%20relationships%0Awithin%20the%20semantic%20space%20when%20addressing%20heterogeneous%20problems%2C%20particularly%0Adomain%20shift.%20This%20oversight%20results%20in%20ambiguities%20between%20class%0Arepresentation.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%20novel%20federated%0Asegmentation%20framework%20that%20strikes%20class%20consistency%2C%20termed%20FedSaaS.%0ASpecifically%2C%20we%20introduce%20class%20exemplars%20as%20a%20criterion%20for%20both%20local-%20and%0Aglobal-level%20class%20representations.%20On%20the%20server%20side%2C%20the%20uploaded%20class%0Aexemplars%20are%20leveraged%20to%20model%20class%20prototypes%2C%20which%20supervise%20global%0Abranch%20of%20clients%2C%20ensuring%20alignment%20with%20global-level%20representation.%20On%20the%0Aclient%20side%2C%20we%20incorporate%20an%20adversarial%20mechanism%20to%20harmonize%20contributions%0Aof%20global%20and%20local%20branches%2C%20leading%20to%20consistent%20output.%20Moreover%2C%0Amultilevel%20contrastive%20losses%20are%20employed%20on%20both%20sides%20to%20enforce%20consistency%0Abetween%20two-level%20representations%20in%20the%20same%20semantic%20space.%20Extensive%0Aexperiments%20on%20several%20driving%20scene%20segmentation%20datasets%20demonstrate%20that%20our%0Aframework%20outperforms%20state-of-the-art%20methods%2C%20significantly%20improving%20average%0Asegmentation%20accuracy%20and%20effectively%20addressing%20the%20class-consistency%0Arepresentation%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedSaaS%253A%2520Class-Consistency%2520Federated%2520Semantic%2520Segmentation%2520via%2520Global%250A%2520%2520Prototype%2520Supervision%2520and%2520Local%2520Adversarial%2520Harmonization%26entry.906535625%3DXiaoyang%2520Yu%2520and%2520Xiaoming%2520Wu%2520and%2520Xin%2520Wang%2520and%2520Dongrun%2520Li%2520and%2520Ming%2520Yang%2520and%2520Peng%2520Cheng%26entry.1292438233%3D%2520%2520Federated%2520semantic%2520segmentation%2520enables%2520pixel-level%2520classification%2520in%2520images%250Athrough%2520collaborative%2520learning%2520while%2520maintaining%2520data%2520privacy.%2520However%252C%250Aexisting%2520research%2520commonly%2520overlooks%2520the%2520fine-grained%2520class%2520relationships%250Awithin%2520the%2520semantic%2520space%2520when%2520addressing%2520heterogeneous%2520problems%252C%2520particularly%250Adomain%2520shift.%2520This%2520oversight%2520results%2520in%2520ambiguities%2520between%2520class%250Arepresentation.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520federated%250Asegmentation%2520framework%2520that%2520strikes%2520class%2520consistency%252C%2520termed%2520FedSaaS.%250ASpecifically%252C%2520we%2520introduce%2520class%2520exemplars%2520as%2520a%2520criterion%2520for%2520both%2520local-%2520and%250Aglobal-level%2520class%2520representations.%2520On%2520the%2520server%2520side%252C%2520the%2520uploaded%2520class%250Aexemplars%2520are%2520leveraged%2520to%2520model%2520class%2520prototypes%252C%2520which%2520supervise%2520global%250Abranch%2520of%2520clients%252C%2520ensuring%2520alignment%2520with%2520global-level%2520representation.%2520On%2520the%250Aclient%2520side%252C%2520we%2520incorporate%2520an%2520adversarial%2520mechanism%2520to%2520harmonize%2520contributions%250Aof%2520global%2520and%2520local%2520branches%252C%2520leading%2520to%2520consistent%2520output.%2520Moreover%252C%250Amultilevel%2520contrastive%2520losses%2520are%2520employed%2520on%2520both%2520sides%2520to%2520enforce%2520consistency%250Abetween%2520two-level%2520representations%2520in%2520the%2520same%2520semantic%2520space.%2520Extensive%250Aexperiments%2520on%2520several%2520driving%2520scene%2520segmentation%2520datasets%2520demonstrate%2520that%2520our%250Aframework%2520outperforms%2520state-of-the-art%2520methods%252C%2520significantly%2520improving%2520average%250Asegmentation%2520accuracy%2520and%2520effectively%2520addressing%2520the%2520class-consistency%250Arepresentation%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedSaaS%3A%20Class-Consistency%20Federated%20Semantic%20Segmentation%20via%20Global%0A%20%20Prototype%20Supervision%20and%20Local%20Adversarial%20Harmonization&entry.906535625=Xiaoyang%20Yu%20and%20Xiaoming%20Wu%20and%20Xin%20Wang%20and%20Dongrun%20Li%20and%20Ming%20Yang%20and%20Peng%20Cheng&entry.1292438233=%20%20Federated%20semantic%20segmentation%20enables%20pixel-level%20classification%20in%20images%0Athrough%20collaborative%20learning%20while%20maintaining%20data%20privacy.%20However%2C%0Aexisting%20research%20commonly%20overlooks%20the%20fine-grained%20class%20relationships%0Awithin%20the%20semantic%20space%20when%20addressing%20heterogeneous%20problems%2C%20particularly%0Adomain%20shift.%20This%20oversight%20results%20in%20ambiguities%20between%20class%0Arepresentation.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%20novel%20federated%0Asegmentation%20framework%20that%20strikes%20class%20consistency%2C%20termed%20FedSaaS.%0ASpecifically%2C%20we%20introduce%20class%20exemplars%20as%20a%20criterion%20for%20both%20local-%20and%0Aglobal-level%20class%20representations.%20On%20the%20server%20side%2C%20the%20uploaded%20class%0Aexemplars%20are%20leveraged%20to%20model%20class%20prototypes%2C%20which%20supervise%20global%0Abranch%20of%20clients%2C%20ensuring%20alignment%20with%20global-level%20representation.%20On%20the%0Aclient%20side%2C%20we%20incorporate%20an%20adversarial%20mechanism%20to%20harmonize%20contributions%0Aof%20global%20and%20local%20branches%2C%20leading%20to%20consistent%20output.%20Moreover%2C%0Amultilevel%20contrastive%20losses%20are%20employed%20on%20both%20sides%20to%20enforce%20consistency%0Abetween%20two-level%20representations%20in%20the%20same%20semantic%20space.%20Extensive%0Aexperiments%20on%20several%20driving%20scene%20segmentation%20datasets%20demonstrate%20that%20our%0Aframework%20outperforms%20state-of-the-art%20methods%2C%20significantly%20improving%20average%0Asegmentation%20accuracy%20and%20effectively%20addressing%20the%20class-consistency%0Arepresentation%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09385v1&entry.124074799=Read"},
{"title": "MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for\n  Few-Shot WSI Classification", "author": "Anh-Tien Nguyen and Duy Minh Ho Nguyen and Nghiem Tuong Diep and Trung Quoc Nguyen and Nhat Ho and Jacqueline Michelle Metsch and Miriam Cindy Maurer and Daniel Sonntag and Hanibal Bohnenberger and Anne-Christin Hauschild", "abstract": "  Whole slide pathology image classification presents challenges due to\ngigapixel image sizes and limited annotation labels, hindering model\ngeneralization. This paper introduces a prompt learning method to adapt large\nvision-language models for few-shot pathology classification. We first extend\nthe Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology\nimage tiles, into a vision-language model by adding adaptors and aligning it\nwith medical text encoders via contrastive learning on 923K image-text pairs.\nThe model is then used to extract visual features and text embeddings from\nfew-shot annotations and fine-tunes with learnable prompt embeddings. Unlike\nprior methods that combine prompts with frozen features using prefix embeddings\nor self-attention, we propose multi-granular attention that compares\ninteractions between learnable prompts with individual image patches and groups\nof them. This approach improves the model's ability to capture both\nfine-grained details and broader context, enhancing its recognition of complex\npatterns across sub-regions. To further improve accuracy, we leverage\n(unbalanced) optimal transport-based visual-text distance to secure model\nrobustness by mitigating perturbations that might occur during the data\naugmentation process. Empirical experiments on lung, kidney, and breast\npathology modalities validate the effectiveness of our approach; thereby, we\nsurpass several of the latest competitors and consistently improve performance\nacross diverse architectures, including CLIP, PLIP, and Prov-GigaPath\nintegrated PLIP. We release our implementations and pre-trained models at this\nMGPATH.\n", "link": "http://arxiv.org/abs/2502.07409v3", "date": "2025-05-14", "relevancy": 2.6947, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGPATH%3A%20Vision-Language%20Model%20with%20Multi-Granular%20Prompt%20Learning%20for%0A%20%20Few-Shot%20WSI%20Classification&body=Title%3A%20MGPATH%3A%20Vision-Language%20Model%20with%20Multi-Granular%20Prompt%20Learning%20for%0A%20%20Few-Shot%20WSI%20Classification%0AAuthor%3A%20Anh-Tien%20Nguyen%20and%20Duy%20Minh%20Ho%20Nguyen%20and%20Nghiem%20Tuong%20Diep%20and%20Trung%20Quoc%20Nguyen%20and%20Nhat%20Ho%20and%20Jacqueline%20Michelle%20Metsch%20and%20Miriam%20Cindy%20Maurer%20and%20Daniel%20Sonntag%20and%20Hanibal%20Bohnenberger%20and%20Anne-Christin%20Hauschild%0AAbstract%3A%20%20%20Whole%20slide%20pathology%20image%20classification%20presents%20challenges%20due%20to%0Agigapixel%20image%20sizes%20and%20limited%20annotation%20labels%2C%20hindering%20model%0Ageneralization.%20This%20paper%20introduces%20a%20prompt%20learning%20method%20to%20adapt%20large%0Avision-language%20models%20for%20few-shot%20pathology%20classification.%20We%20first%20extend%0Athe%20Prov-GigaPath%20vision%20foundation%20model%2C%20pre-trained%20on%201.3%20billion%20pathology%0Aimage%20tiles%2C%20into%20a%20vision-language%20model%20by%20adding%20adaptors%20and%20aligning%20it%0Awith%20medical%20text%20encoders%20via%20contrastive%20learning%20on%20923K%20image-text%20pairs.%0AThe%20model%20is%20then%20used%20to%20extract%20visual%20features%20and%20text%20embeddings%20from%0Afew-shot%20annotations%20and%20fine-tunes%20with%20learnable%20prompt%20embeddings.%20Unlike%0Aprior%20methods%20that%20combine%20prompts%20with%20frozen%20features%20using%20prefix%20embeddings%0Aor%20self-attention%2C%20we%20propose%20multi-granular%20attention%20that%20compares%0Ainteractions%20between%20learnable%20prompts%20with%20individual%20image%20patches%20and%20groups%0Aof%20them.%20This%20approach%20improves%20the%20model%27s%20ability%20to%20capture%20both%0Afine-grained%20details%20and%20broader%20context%2C%20enhancing%20its%20recognition%20of%20complex%0Apatterns%20across%20sub-regions.%20To%20further%20improve%20accuracy%2C%20we%20leverage%0A%28unbalanced%29%20optimal%20transport-based%20visual-text%20distance%20to%20secure%20model%0Arobustness%20by%20mitigating%20perturbations%20that%20might%20occur%20during%20the%20data%0Aaugmentation%20process.%20Empirical%20experiments%20on%20lung%2C%20kidney%2C%20and%20breast%0Apathology%20modalities%20validate%20the%20effectiveness%20of%20our%20approach%3B%20thereby%2C%20we%0Asurpass%20several%20of%20the%20latest%20competitors%20and%20consistently%20improve%20performance%0Aacross%20diverse%20architectures%2C%20including%20CLIP%2C%20PLIP%2C%20and%20Prov-GigaPath%0Aintegrated%20PLIP.%20We%20release%20our%20implementations%20and%20pre-trained%20models%20at%20this%0AMGPATH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07409v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGPATH%253A%2520Vision-Language%2520Model%2520with%2520Multi-Granular%2520Prompt%2520Learning%2520for%250A%2520%2520Few-Shot%2520WSI%2520Classification%26entry.906535625%3DAnh-Tien%2520Nguyen%2520and%2520Duy%2520Minh%2520Ho%2520Nguyen%2520and%2520Nghiem%2520Tuong%2520Diep%2520and%2520Trung%2520Quoc%2520Nguyen%2520and%2520Nhat%2520Ho%2520and%2520Jacqueline%2520Michelle%2520Metsch%2520and%2520Miriam%2520Cindy%2520Maurer%2520and%2520Daniel%2520Sonntag%2520and%2520Hanibal%2520Bohnenberger%2520and%2520Anne-Christin%2520Hauschild%26entry.1292438233%3D%2520%2520Whole%2520slide%2520pathology%2520image%2520classification%2520presents%2520challenges%2520due%2520to%250Agigapixel%2520image%2520sizes%2520and%2520limited%2520annotation%2520labels%252C%2520hindering%2520model%250Ageneralization.%2520This%2520paper%2520introduces%2520a%2520prompt%2520learning%2520method%2520to%2520adapt%2520large%250Avision-language%2520models%2520for%2520few-shot%2520pathology%2520classification.%2520We%2520first%2520extend%250Athe%2520Prov-GigaPath%2520vision%2520foundation%2520model%252C%2520pre-trained%2520on%25201.3%2520billion%2520pathology%250Aimage%2520tiles%252C%2520into%2520a%2520vision-language%2520model%2520by%2520adding%2520adaptors%2520and%2520aligning%2520it%250Awith%2520medical%2520text%2520encoders%2520via%2520contrastive%2520learning%2520on%2520923K%2520image-text%2520pairs.%250AThe%2520model%2520is%2520then%2520used%2520to%2520extract%2520visual%2520features%2520and%2520text%2520embeddings%2520from%250Afew-shot%2520annotations%2520and%2520fine-tunes%2520with%2520learnable%2520prompt%2520embeddings.%2520Unlike%250Aprior%2520methods%2520that%2520combine%2520prompts%2520with%2520frozen%2520features%2520using%2520prefix%2520embeddings%250Aor%2520self-attention%252C%2520we%2520propose%2520multi-granular%2520attention%2520that%2520compares%250Ainteractions%2520between%2520learnable%2520prompts%2520with%2520individual%2520image%2520patches%2520and%2520groups%250Aof%2520them.%2520This%2520approach%2520improves%2520the%2520model%2527s%2520ability%2520to%2520capture%2520both%250Afine-grained%2520details%2520and%2520broader%2520context%252C%2520enhancing%2520its%2520recognition%2520of%2520complex%250Apatterns%2520across%2520sub-regions.%2520To%2520further%2520improve%2520accuracy%252C%2520we%2520leverage%250A%2528unbalanced%2529%2520optimal%2520transport-based%2520visual-text%2520distance%2520to%2520secure%2520model%250Arobustness%2520by%2520mitigating%2520perturbations%2520that%2520might%2520occur%2520during%2520the%2520data%250Aaugmentation%2520process.%2520Empirical%2520experiments%2520on%2520lung%252C%2520kidney%252C%2520and%2520breast%250Apathology%2520modalities%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%253B%2520thereby%252C%2520we%250Asurpass%2520several%2520of%2520the%2520latest%2520competitors%2520and%2520consistently%2520improve%2520performance%250Aacross%2520diverse%2520architectures%252C%2520including%2520CLIP%252C%2520PLIP%252C%2520and%2520Prov-GigaPath%250Aintegrated%2520PLIP.%2520We%2520release%2520our%2520implementations%2520and%2520pre-trained%2520models%2520at%2520this%250AMGPATH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07409v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGPATH%3A%20Vision-Language%20Model%20with%20Multi-Granular%20Prompt%20Learning%20for%0A%20%20Few-Shot%20WSI%20Classification&entry.906535625=Anh-Tien%20Nguyen%20and%20Duy%20Minh%20Ho%20Nguyen%20and%20Nghiem%20Tuong%20Diep%20and%20Trung%20Quoc%20Nguyen%20and%20Nhat%20Ho%20and%20Jacqueline%20Michelle%20Metsch%20and%20Miriam%20Cindy%20Maurer%20and%20Daniel%20Sonntag%20and%20Hanibal%20Bohnenberger%20and%20Anne-Christin%20Hauschild&entry.1292438233=%20%20Whole%20slide%20pathology%20image%20classification%20presents%20challenges%20due%20to%0Agigapixel%20image%20sizes%20and%20limited%20annotation%20labels%2C%20hindering%20model%0Ageneralization.%20This%20paper%20introduces%20a%20prompt%20learning%20method%20to%20adapt%20large%0Avision-language%20models%20for%20few-shot%20pathology%20classification.%20We%20first%20extend%0Athe%20Prov-GigaPath%20vision%20foundation%20model%2C%20pre-trained%20on%201.3%20billion%20pathology%0Aimage%20tiles%2C%20into%20a%20vision-language%20model%20by%20adding%20adaptors%20and%20aligning%20it%0Awith%20medical%20text%20encoders%20via%20contrastive%20learning%20on%20923K%20image-text%20pairs.%0AThe%20model%20is%20then%20used%20to%20extract%20visual%20features%20and%20text%20embeddings%20from%0Afew-shot%20annotations%20and%20fine-tunes%20with%20learnable%20prompt%20embeddings.%20Unlike%0Aprior%20methods%20that%20combine%20prompts%20with%20frozen%20features%20using%20prefix%20embeddings%0Aor%20self-attention%2C%20we%20propose%20multi-granular%20attention%20that%20compares%0Ainteractions%20between%20learnable%20prompts%20with%20individual%20image%20patches%20and%20groups%0Aof%20them.%20This%20approach%20improves%20the%20model%27s%20ability%20to%20capture%20both%0Afine-grained%20details%20and%20broader%20context%2C%20enhancing%20its%20recognition%20of%20complex%0Apatterns%20across%20sub-regions.%20To%20further%20improve%20accuracy%2C%20we%20leverage%0A%28unbalanced%29%20optimal%20transport-based%20visual-text%20distance%20to%20secure%20model%0Arobustness%20by%20mitigating%20perturbations%20that%20might%20occur%20during%20the%20data%0Aaugmentation%20process.%20Empirical%20experiments%20on%20lung%2C%20kidney%2C%20and%20breast%0Apathology%20modalities%20validate%20the%20effectiveness%20of%20our%20approach%3B%20thereby%2C%20we%0Asurpass%20several%20of%20the%20latest%20competitors%20and%20consistently%20improve%20performance%0Aacross%20diverse%20architectures%2C%20including%20CLIP%2C%20PLIP%2C%20and%20Prov-GigaPath%0Aintegrated%20PLIP.%20We%20release%20our%20implementations%20and%20pre-trained%20models%20at%20this%0AMGPATH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07409v3&entry.124074799=Read"},
{"title": "Rhomboid Tiling for Geometric Graph Deep Learning", "author": "Yipeng Zhang and Longlong Li and Kelin Xia", "abstract": "  Graph Neural Networks (GNNs) have proven effective for learning from\ngraph-structured data through their neighborhood-based message passing\nframework. Many hierarchical graph clustering pooling methods modify this\nframework by introducing clustering-based strategies, enabling the construction\nof more expressive and powerful models. However, all of these message passing\nframework heavily rely on the connectivity structure of graphs, limiting their\nability to capture the rich geometric features inherent in geometric graphs. To\naddress this, we propose Rhomboid Tiling (RT) clustering, a novel clustering\nmethod based on the rhomboid tiling structure, which performs clustering by\nleveraging the complex geometric information of the data and effectively\nextracts its higher-order geometric structures. Moreover, we design RTPool, a\nhierarchical graph clustering pooling model based on RT clustering for graph\nclassification tasks. The proposed model demonstrates superior performance,\noutperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.\n", "link": "http://arxiv.org/abs/2505.09586v1", "date": "2025-05-14", "relevancy": 2.6726, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5563}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5337}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rhomboid%20Tiling%20for%20Geometric%20Graph%20Deep%20Learning&body=Title%3A%20Rhomboid%20Tiling%20for%20Geometric%20Graph%20Deep%20Learning%0AAuthor%3A%20Yipeng%20Zhang%20and%20Longlong%20Li%20and%20Kelin%20Xia%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20proven%20effective%20for%20learning%20from%0Agraph-structured%20data%20through%20their%20neighborhood-based%20message%20passing%0Aframework.%20Many%20hierarchical%20graph%20clustering%20pooling%20methods%20modify%20this%0Aframework%20by%20introducing%20clustering-based%20strategies%2C%20enabling%20the%20construction%0Aof%20more%20expressive%20and%20powerful%20models.%20However%2C%20all%20of%20these%20message%20passing%0Aframework%20heavily%20rely%20on%20the%20connectivity%20structure%20of%20graphs%2C%20limiting%20their%0Aability%20to%20capture%20the%20rich%20geometric%20features%20inherent%20in%20geometric%20graphs.%20To%0Aaddress%20this%2C%20we%20propose%20Rhomboid%20Tiling%20%28RT%29%20clustering%2C%20a%20novel%20clustering%0Amethod%20based%20on%20the%20rhomboid%20tiling%20structure%2C%20which%20performs%20clustering%20by%0Aleveraging%20the%20complex%20geometric%20information%20of%20the%20data%20and%20effectively%0Aextracts%20its%20higher-order%20geometric%20structures.%20Moreover%2C%20we%20design%20RTPool%2C%20a%0Ahierarchical%20graph%20clustering%20pooling%20model%20based%20on%20RT%20clustering%20for%20graph%0Aclassification%20tasks.%20The%20proposed%20model%20demonstrates%20superior%20performance%2C%0Aoutperforming%2021%20state-of-the-art%20competitors%20on%20all%20the%207%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRhomboid%2520Tiling%2520for%2520Geometric%2520Graph%2520Deep%2520Learning%26entry.906535625%3DYipeng%2520Zhang%2520and%2520Longlong%2520Li%2520and%2520Kelin%2520Xia%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520proven%2520effective%2520for%2520learning%2520from%250Agraph-structured%2520data%2520through%2520their%2520neighborhood-based%2520message%2520passing%250Aframework.%2520Many%2520hierarchical%2520graph%2520clustering%2520pooling%2520methods%2520modify%2520this%250Aframework%2520by%2520introducing%2520clustering-based%2520strategies%252C%2520enabling%2520the%2520construction%250Aof%2520more%2520expressive%2520and%2520powerful%2520models.%2520However%252C%2520all%2520of%2520these%2520message%2520passing%250Aframework%2520heavily%2520rely%2520on%2520the%2520connectivity%2520structure%2520of%2520graphs%252C%2520limiting%2520their%250Aability%2520to%2520capture%2520the%2520rich%2520geometric%2520features%2520inherent%2520in%2520geometric%2520graphs.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520Rhomboid%2520Tiling%2520%2528RT%2529%2520clustering%252C%2520a%2520novel%2520clustering%250Amethod%2520based%2520on%2520the%2520rhomboid%2520tiling%2520structure%252C%2520which%2520performs%2520clustering%2520by%250Aleveraging%2520the%2520complex%2520geometric%2520information%2520of%2520the%2520data%2520and%2520effectively%250Aextracts%2520its%2520higher-order%2520geometric%2520structures.%2520Moreover%252C%2520we%2520design%2520RTPool%252C%2520a%250Ahierarchical%2520graph%2520clustering%2520pooling%2520model%2520based%2520on%2520RT%2520clustering%2520for%2520graph%250Aclassification%2520tasks.%2520The%2520proposed%2520model%2520demonstrates%2520superior%2520performance%252C%250Aoutperforming%252021%2520state-of-the-art%2520competitors%2520on%2520all%2520the%25207%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rhomboid%20Tiling%20for%20Geometric%20Graph%20Deep%20Learning&entry.906535625=Yipeng%20Zhang%20and%20Longlong%20Li%20and%20Kelin%20Xia&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20proven%20effective%20for%20learning%20from%0Agraph-structured%20data%20through%20their%20neighborhood-based%20message%20passing%0Aframework.%20Many%20hierarchical%20graph%20clustering%20pooling%20methods%20modify%20this%0Aframework%20by%20introducing%20clustering-based%20strategies%2C%20enabling%20the%20construction%0Aof%20more%20expressive%20and%20powerful%20models.%20However%2C%20all%20of%20these%20message%20passing%0Aframework%20heavily%20rely%20on%20the%20connectivity%20structure%20of%20graphs%2C%20limiting%20their%0Aability%20to%20capture%20the%20rich%20geometric%20features%20inherent%20in%20geometric%20graphs.%20To%0Aaddress%20this%2C%20we%20propose%20Rhomboid%20Tiling%20%28RT%29%20clustering%2C%20a%20novel%20clustering%0Amethod%20based%20on%20the%20rhomboid%20tiling%20structure%2C%20which%20performs%20clustering%20by%0Aleveraging%20the%20complex%20geometric%20information%20of%20the%20data%20and%20effectively%0Aextracts%20its%20higher-order%20geometric%20structures.%20Moreover%2C%20we%20design%20RTPool%2C%20a%0Ahierarchical%20graph%20clustering%20pooling%20model%20based%20on%20RT%20clustering%20for%20graph%0Aclassification%20tasks.%20The%20proposed%20model%20demonstrates%20superior%20performance%2C%0Aoutperforming%2021%20state-of-the-art%20competitors%20on%20all%20the%207%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09586v1&entry.124074799=Read"},
{"title": "Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy\n  Records", "author": "Yili He and Yan Zhu and Peiyao Fu and Ruijie Yang and Tianyi Chen and Zhihua Wang and Quanlin Li and Pinghong Zhou and Xian Yang and Shuo Wang", "abstract": "  Pre-training on image-text colonoscopy records offers substantial potential\nfor improving endoscopic image analysis, but faces challenges including\nnon-informative background images, complex medical terminology, and ambiguous\nmulti-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised\nframework that enhances Contrastive Language-Image Pre-training (CLIP) for this\ndomain. Endo-CLIP's three-stage framework--cleansing, attunement, and\nunification--addresses these challenges by (1) removing background frames, (2)\nleveraging large language models to extract clinical attributes for\nfine-grained contrastive learning, and (3) employing patient-level\ncross-attention to resolve multi-polyp ambiguities. Extensive experiments\ndemonstrate that Endo-CLIP significantly outperforms state-of-the-art\npre-training methods in zero-shot and few-shot polyp detection and\nclassification, paving the way for more accurate and clinically relevant\nendoscopic analysis.\n", "link": "http://arxiv.org/abs/2505.09435v1", "date": "2025-05-14", "relevancy": 2.6243, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5587}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Endo-CLIP%3A%20Progressive%20Self-Supervised%20Pre-training%20on%20Raw%20Colonoscopy%0A%20%20Records&body=Title%3A%20Endo-CLIP%3A%20Progressive%20Self-Supervised%20Pre-training%20on%20Raw%20Colonoscopy%0A%20%20Records%0AAuthor%3A%20Yili%20He%20and%20Yan%20Zhu%20and%20Peiyao%20Fu%20and%20Ruijie%20Yang%20and%20Tianyi%20Chen%20and%20Zhihua%20Wang%20and%20Quanlin%20Li%20and%20Pinghong%20Zhou%20and%20Xian%20Yang%20and%20Shuo%20Wang%0AAbstract%3A%20%20%20Pre-training%20on%20image-text%20colonoscopy%20records%20offers%20substantial%20potential%0Afor%20improving%20endoscopic%20image%20analysis%2C%20but%20faces%20challenges%20including%0Anon-informative%20background%20images%2C%20complex%20medical%20terminology%2C%20and%20ambiguous%0Amulti-lesion%20descriptions.%20We%20introduce%20Endo-CLIP%2C%20a%20novel%20self-supervised%0Aframework%20that%20enhances%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20for%20this%0Adomain.%20Endo-CLIP%27s%20three-stage%20framework--cleansing%2C%20attunement%2C%20and%0Aunification--addresses%20these%20challenges%20by%20%281%29%20removing%20background%20frames%2C%20%282%29%0Aleveraging%20large%20language%20models%20to%20extract%20clinical%20attributes%20for%0Afine-grained%20contrastive%20learning%2C%20and%20%283%29%20employing%20patient-level%0Across-attention%20to%20resolve%20multi-polyp%20ambiguities.%20Extensive%20experiments%0Ademonstrate%20that%20Endo-CLIP%20significantly%20outperforms%20state-of-the-art%0Apre-training%20methods%20in%20zero-shot%20and%20few-shot%20polyp%20detection%20and%0Aclassification%2C%20paving%20the%20way%20for%20more%20accurate%20and%20clinically%20relevant%0Aendoscopic%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEndo-CLIP%253A%2520Progressive%2520Self-Supervised%2520Pre-training%2520on%2520Raw%2520Colonoscopy%250A%2520%2520Records%26entry.906535625%3DYili%2520He%2520and%2520Yan%2520Zhu%2520and%2520Peiyao%2520Fu%2520and%2520Ruijie%2520Yang%2520and%2520Tianyi%2520Chen%2520and%2520Zhihua%2520Wang%2520and%2520Quanlin%2520Li%2520and%2520Pinghong%2520Zhou%2520and%2520Xian%2520Yang%2520and%2520Shuo%2520Wang%26entry.1292438233%3D%2520%2520Pre-training%2520on%2520image-text%2520colonoscopy%2520records%2520offers%2520substantial%2520potential%250Afor%2520improving%2520endoscopic%2520image%2520analysis%252C%2520but%2520faces%2520challenges%2520including%250Anon-informative%2520background%2520images%252C%2520complex%2520medical%2520terminology%252C%2520and%2520ambiguous%250Amulti-lesion%2520descriptions.%2520We%2520introduce%2520Endo-CLIP%252C%2520a%2520novel%2520self-supervised%250Aframework%2520that%2520enhances%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520for%2520this%250Adomain.%2520Endo-CLIP%2527s%2520three-stage%2520framework--cleansing%252C%2520attunement%252C%2520and%250Aunification--addresses%2520these%2520challenges%2520by%2520%25281%2529%2520removing%2520background%2520frames%252C%2520%25282%2529%250Aleveraging%2520large%2520language%2520models%2520to%2520extract%2520clinical%2520attributes%2520for%250Afine-grained%2520contrastive%2520learning%252C%2520and%2520%25283%2529%2520employing%2520patient-level%250Across-attention%2520to%2520resolve%2520multi-polyp%2520ambiguities.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520Endo-CLIP%2520significantly%2520outperforms%2520state-of-the-art%250Apre-training%2520methods%2520in%2520zero-shot%2520and%2520few-shot%2520polyp%2520detection%2520and%250Aclassification%252C%2520paving%2520the%2520way%2520for%2520more%2520accurate%2520and%2520clinically%2520relevant%250Aendoscopic%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Endo-CLIP%3A%20Progressive%20Self-Supervised%20Pre-training%20on%20Raw%20Colonoscopy%0A%20%20Records&entry.906535625=Yili%20He%20and%20Yan%20Zhu%20and%20Peiyao%20Fu%20and%20Ruijie%20Yang%20and%20Tianyi%20Chen%20and%20Zhihua%20Wang%20and%20Quanlin%20Li%20and%20Pinghong%20Zhou%20and%20Xian%20Yang%20and%20Shuo%20Wang&entry.1292438233=%20%20Pre-training%20on%20image-text%20colonoscopy%20records%20offers%20substantial%20potential%0Afor%20improving%20endoscopic%20image%20analysis%2C%20but%20faces%20challenges%20including%0Anon-informative%20background%20images%2C%20complex%20medical%20terminology%2C%20and%20ambiguous%0Amulti-lesion%20descriptions.%20We%20introduce%20Endo-CLIP%2C%20a%20novel%20self-supervised%0Aframework%20that%20enhances%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20for%20this%0Adomain.%20Endo-CLIP%27s%20three-stage%20framework--cleansing%2C%20attunement%2C%20and%0Aunification--addresses%20these%20challenges%20by%20%281%29%20removing%20background%20frames%2C%20%282%29%0Aleveraging%20large%20language%20models%20to%20extract%20clinical%20attributes%20for%0Afine-grained%20contrastive%20learning%2C%20and%20%283%29%20employing%20patient-level%0Across-attention%20to%20resolve%20multi-polyp%20ambiguities.%20Extensive%20experiments%0Ademonstrate%20that%20Endo-CLIP%20significantly%20outperforms%20state-of-the-art%0Apre-training%20methods%20in%20zero-shot%20and%20few-shot%20polyp%20detection%20and%0Aclassification%2C%20paving%20the%20way%20for%20more%20accurate%20and%20clinically%20relevant%0Aendoscopic%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09435v1&entry.124074799=Read"},
{"title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model\n  Merging", "author": "Wenju Sun and Qingyong Li and Yangli-ao Geng and Boyang Li", "abstract": "  Multi-task model merging offers a promising paradigm for integrating multiple\nexpert models into a unified model without additional training. Existing\nstate-of-the-art techniques, such as Task Arithmetic and its variants, merge\nmodels by accumulating task vectors -- the parameter differences between\npretrained and finetuned models. However, task vector accumulation is often\nhindered by knowledge conflicts, leading to performance degradation. To address\nthis challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel\ntraining-free framework that selectively trims conflict-prone components from\nthe task vectors. CAT Merging introduces several parameter-specific strategies,\nincluding projection for linear weights and masking for scaling and shifting\nparameters in normalization layers. Extensive experiments on vision, language,\nand vision-language tasks demonstrate that CAT Merging effectively suppresses\nknowledge conflicts, achieving average accuracy improvements of up to 2.5%\n(ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2505.06977v2", "date": "2025-05-14", "relevancy": 2.6066, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.551}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5065}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAT%20Merging%3A%20A%20Training-Free%20Approach%20for%20Resolving%20Conflicts%20in%20Model%0A%20%20Merging&body=Title%3A%20CAT%20Merging%3A%20A%20Training-Free%20Approach%20for%20Resolving%20Conflicts%20in%20Model%0A%20%20Merging%0AAuthor%3A%20Wenju%20Sun%20and%20Qingyong%20Li%20and%20Yangli-ao%20Geng%20and%20Boyang%20Li%0AAbstract%3A%20%20%20Multi-task%20model%20merging%20offers%20a%20promising%20paradigm%20for%20integrating%20multiple%0Aexpert%20models%20into%20a%20unified%20model%20without%20additional%20training.%20Existing%0Astate-of-the-art%20techniques%2C%20such%20as%20Task%20Arithmetic%20and%20its%20variants%2C%20merge%0Amodels%20by%20accumulating%20task%20vectors%20--%20the%20parameter%20differences%20between%0Apretrained%20and%20finetuned%20models.%20However%2C%20task%20vector%20accumulation%20is%20often%0Ahindered%20by%20knowledge%20conflicts%2C%20leading%20to%20performance%20degradation.%20To%20address%0Athis%20challenge%2C%20we%20propose%20Conflict-Aware%20Task%20Merging%20%28CAT%20Merging%29%2C%20a%20novel%0Atraining-free%20framework%20that%20selectively%20trims%20conflict-prone%20components%20from%0Athe%20task%20vectors.%20CAT%20Merging%20introduces%20several%20parameter-specific%20strategies%2C%0Aincluding%20projection%20for%20linear%20weights%20and%20masking%20for%20scaling%20and%20shifting%0Aparameters%20in%20normalization%20layers.%20Extensive%20experiments%20on%20vision%2C%20language%2C%0Aand%20vision-language%20tasks%20demonstrate%20that%20CAT%20Merging%20effectively%20suppresses%0Aknowledge%20conflicts%2C%20achieving%20average%20accuracy%20improvements%20of%20up%20to%202.5%25%0A%28ViT-B/32%29%20and%202.0%25%20%28ViT-L/14%29%20over%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06977v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAT%2520Merging%253A%2520A%2520Training-Free%2520Approach%2520for%2520Resolving%2520Conflicts%2520in%2520Model%250A%2520%2520Merging%26entry.906535625%3DWenju%2520Sun%2520and%2520Qingyong%2520Li%2520and%2520Yangli-ao%2520Geng%2520and%2520Boyang%2520Li%26entry.1292438233%3D%2520%2520Multi-task%2520model%2520merging%2520offers%2520a%2520promising%2520paradigm%2520for%2520integrating%2520multiple%250Aexpert%2520models%2520into%2520a%2520unified%2520model%2520without%2520additional%2520training.%2520Existing%250Astate-of-the-art%2520techniques%252C%2520such%2520as%2520Task%2520Arithmetic%2520and%2520its%2520variants%252C%2520merge%250Amodels%2520by%2520accumulating%2520task%2520vectors%2520--%2520the%2520parameter%2520differences%2520between%250Apretrained%2520and%2520finetuned%2520models.%2520However%252C%2520task%2520vector%2520accumulation%2520is%2520often%250Ahindered%2520by%2520knowledge%2520conflicts%252C%2520leading%2520to%2520performance%2520degradation.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520Conflict-Aware%2520Task%2520Merging%2520%2528CAT%2520Merging%2529%252C%2520a%2520novel%250Atraining-free%2520framework%2520that%2520selectively%2520trims%2520conflict-prone%2520components%2520from%250Athe%2520task%2520vectors.%2520CAT%2520Merging%2520introduces%2520several%2520parameter-specific%2520strategies%252C%250Aincluding%2520projection%2520for%2520linear%2520weights%2520and%2520masking%2520for%2520scaling%2520and%2520shifting%250Aparameters%2520in%2520normalization%2520layers.%2520Extensive%2520experiments%2520on%2520vision%252C%2520language%252C%250Aand%2520vision-language%2520tasks%2520demonstrate%2520that%2520CAT%2520Merging%2520effectively%2520suppresses%250Aknowledge%2520conflicts%252C%2520achieving%2520average%2520accuracy%2520improvements%2520of%2520up%2520to%25202.5%2525%250A%2528ViT-B/32%2529%2520and%25202.0%2525%2520%2528ViT-L/14%2529%2520over%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06977v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAT%20Merging%3A%20A%20Training-Free%20Approach%20for%20Resolving%20Conflicts%20in%20Model%0A%20%20Merging&entry.906535625=Wenju%20Sun%20and%20Qingyong%20Li%20and%20Yangli-ao%20Geng%20and%20Boyang%20Li&entry.1292438233=%20%20Multi-task%20model%20merging%20offers%20a%20promising%20paradigm%20for%20integrating%20multiple%0Aexpert%20models%20into%20a%20unified%20model%20without%20additional%20training.%20Existing%0Astate-of-the-art%20techniques%2C%20such%20as%20Task%20Arithmetic%20and%20its%20variants%2C%20merge%0Amodels%20by%20accumulating%20task%20vectors%20--%20the%20parameter%20differences%20between%0Apretrained%20and%20finetuned%20models.%20However%2C%20task%20vector%20accumulation%20is%20often%0Ahindered%20by%20knowledge%20conflicts%2C%20leading%20to%20performance%20degradation.%20To%20address%0Athis%20challenge%2C%20we%20propose%20Conflict-Aware%20Task%20Merging%20%28CAT%20Merging%29%2C%20a%20novel%0Atraining-free%20framework%20that%20selectively%20trims%20conflict-prone%20components%20from%0Athe%20task%20vectors.%20CAT%20Merging%20introduces%20several%20parameter-specific%20strategies%2C%0Aincluding%20projection%20for%20linear%20weights%20and%20masking%20for%20scaling%20and%20shifting%0Aparameters%20in%20normalization%20layers.%20Extensive%20experiments%20on%20vision%2C%20language%2C%0Aand%20vision-language%20tasks%20demonstrate%20that%20CAT%20Merging%20effectively%20suppresses%0Aknowledge%20conflicts%2C%20achieving%20average%20accuracy%20improvements%20of%20up%20to%202.5%25%0A%28ViT-B/32%29%20and%202.0%25%20%28ViT-L/14%29%20over%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06977v2&entry.124074799=Read"},
{"title": "Optimal-state Dynamics Estimation for Physics-based Human Motion Capture\n  from Videos", "author": "Cuong Le and Viktor Johansson and Manon Kok and Bastian Wandt", "abstract": "  Human motion capture from monocular videos has made significant progress in\nrecent years. However, modern approaches often produce temporal artifacts, e.g.\nin form of jittery motion and struggle to achieve smooth and physically\nplausible motions. Explicitly integrating physics, in form of internal forces\nand exterior torques, helps alleviating these artifacts. Current\nstate-of-the-art approaches make use of an automatic PD controller to predict\ntorques and reaction forces in order to re-simulate the input kinematics, i.e.\nthe joint angles of a predefined skeleton. However, due to imperfect physical\nmodels, these methods often require simplifying assumptions and extensive\npreprocessing of the input kinematics to achieve good performance. To this end,\nwe propose a novel method to selectively incorporate the physics models with\nthe kinematics observations in an online setting, inspired by a neural\nKalman-filtering approach. We develop a control loop as a meta-PD controller to\npredict internal joint torques and external reaction forces, followed by a\nphysics-based motion simulation. A recurrent neural network is introduced to\nrealize a Kalman filter that attentively balances the kinematics input and\nsimulated motion, resulting in an optimal-state dynamics prediction. We show\nthat this filtering step is crucial to provide an online supervision that helps\nbalancing the shortcoming of the respective input motions, thus being important\nfor not only capturing accurate global motion trajectories but also producing\nphysically plausible human poses. The proposed approach excels in the\nphysics-based human pose estimation task and demonstrates the physical\nplausibility of the predictive dynamics, compared to state of the art. The code\nis available on https://github.com/cuongle1206/OSDCap\n", "link": "http://arxiv.org/abs/2410.07795v4", "date": "2025-05-14", "relevancy": 2.539, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6519}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6247}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal-state%20Dynamics%20Estimation%20for%20Physics-based%20Human%20Motion%20Capture%0A%20%20from%20Videos&body=Title%3A%20Optimal-state%20Dynamics%20Estimation%20for%20Physics-based%20Human%20Motion%20Capture%0A%20%20from%20Videos%0AAuthor%3A%20Cuong%20Le%20and%20Viktor%20Johansson%20and%20Manon%20Kok%20and%20Bastian%20Wandt%0AAbstract%3A%20%20%20Human%20motion%20capture%20from%20monocular%20videos%20has%20made%20significant%20progress%20in%0Arecent%20years.%20However%2C%20modern%20approaches%20often%20produce%20temporal%20artifacts%2C%20e.g.%0Ain%20form%20of%20jittery%20motion%20and%20struggle%20to%20achieve%20smooth%20and%20physically%0Aplausible%20motions.%20Explicitly%20integrating%20physics%2C%20in%20form%20of%20internal%20forces%0Aand%20exterior%20torques%2C%20helps%20alleviating%20these%20artifacts.%20Current%0Astate-of-the-art%20approaches%20make%20use%20of%20an%20automatic%20PD%20controller%20to%20predict%0Atorques%20and%20reaction%20forces%20in%20order%20to%20re-simulate%20the%20input%20kinematics%2C%20i.e.%0Athe%20joint%20angles%20of%20a%20predefined%20skeleton.%20However%2C%20due%20to%20imperfect%20physical%0Amodels%2C%20these%20methods%20often%20require%20simplifying%20assumptions%20and%20extensive%0Apreprocessing%20of%20the%20input%20kinematics%20to%20achieve%20good%20performance.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20method%20to%20selectively%20incorporate%20the%20physics%20models%20with%0Athe%20kinematics%20observations%20in%20an%20online%20setting%2C%20inspired%20by%20a%20neural%0AKalman-filtering%20approach.%20We%20develop%20a%20control%20loop%20as%20a%20meta-PD%20controller%20to%0Apredict%20internal%20joint%20torques%20and%20external%20reaction%20forces%2C%20followed%20by%20a%0Aphysics-based%20motion%20simulation.%20A%20recurrent%20neural%20network%20is%20introduced%20to%0Arealize%20a%20Kalman%20filter%20that%20attentively%20balances%20the%20kinematics%20input%20and%0Asimulated%20motion%2C%20resulting%20in%20an%20optimal-state%20dynamics%20prediction.%20We%20show%0Athat%20this%20filtering%20step%20is%20crucial%20to%20provide%20an%20online%20supervision%20that%20helps%0Abalancing%20the%20shortcoming%20of%20the%20respective%20input%20motions%2C%20thus%20being%20important%0Afor%20not%20only%20capturing%20accurate%20global%20motion%20trajectories%20but%20also%20producing%0Aphysically%20plausible%20human%20poses.%20The%20proposed%20approach%20excels%20in%20the%0Aphysics-based%20human%20pose%20estimation%20task%20and%20demonstrates%20the%20physical%0Aplausibility%20of%20the%20predictive%20dynamics%2C%20compared%20to%20state%20of%20the%20art.%20The%20code%0Ais%20available%20on%20https%3A//github.com/cuongle1206/OSDCap%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07795v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal-state%2520Dynamics%2520Estimation%2520for%2520Physics-based%2520Human%2520Motion%2520Capture%250A%2520%2520from%2520Videos%26entry.906535625%3DCuong%2520Le%2520and%2520Viktor%2520Johansson%2520and%2520Manon%2520Kok%2520and%2520Bastian%2520Wandt%26entry.1292438233%3D%2520%2520Human%2520motion%2520capture%2520from%2520monocular%2520videos%2520has%2520made%2520significant%2520progress%2520in%250Arecent%2520years.%2520However%252C%2520modern%2520approaches%2520often%2520produce%2520temporal%2520artifacts%252C%2520e.g.%250Ain%2520form%2520of%2520jittery%2520motion%2520and%2520struggle%2520to%2520achieve%2520smooth%2520and%2520physically%250Aplausible%2520motions.%2520Explicitly%2520integrating%2520physics%252C%2520in%2520form%2520of%2520internal%2520forces%250Aand%2520exterior%2520torques%252C%2520helps%2520alleviating%2520these%2520artifacts.%2520Current%250Astate-of-the-art%2520approaches%2520make%2520use%2520of%2520an%2520automatic%2520PD%2520controller%2520to%2520predict%250Atorques%2520and%2520reaction%2520forces%2520in%2520order%2520to%2520re-simulate%2520the%2520input%2520kinematics%252C%2520i.e.%250Athe%2520joint%2520angles%2520of%2520a%2520predefined%2520skeleton.%2520However%252C%2520due%2520to%2520imperfect%2520physical%250Amodels%252C%2520these%2520methods%2520often%2520require%2520simplifying%2520assumptions%2520and%2520extensive%250Apreprocessing%2520of%2520the%2520input%2520kinematics%2520to%2520achieve%2520good%2520performance.%2520To%2520this%2520end%252C%250Awe%2520propose%2520a%2520novel%2520method%2520to%2520selectively%2520incorporate%2520the%2520physics%2520models%2520with%250Athe%2520kinematics%2520observations%2520in%2520an%2520online%2520setting%252C%2520inspired%2520by%2520a%2520neural%250AKalman-filtering%2520approach.%2520We%2520develop%2520a%2520control%2520loop%2520as%2520a%2520meta-PD%2520controller%2520to%250Apredict%2520internal%2520joint%2520torques%2520and%2520external%2520reaction%2520forces%252C%2520followed%2520by%2520a%250Aphysics-based%2520motion%2520simulation.%2520A%2520recurrent%2520neural%2520network%2520is%2520introduced%2520to%250Arealize%2520a%2520Kalman%2520filter%2520that%2520attentively%2520balances%2520the%2520kinematics%2520input%2520and%250Asimulated%2520motion%252C%2520resulting%2520in%2520an%2520optimal-state%2520dynamics%2520prediction.%2520We%2520show%250Athat%2520this%2520filtering%2520step%2520is%2520crucial%2520to%2520provide%2520an%2520online%2520supervision%2520that%2520helps%250Abalancing%2520the%2520shortcoming%2520of%2520the%2520respective%2520input%2520motions%252C%2520thus%2520being%2520important%250Afor%2520not%2520only%2520capturing%2520accurate%2520global%2520motion%2520trajectories%2520but%2520also%2520producing%250Aphysically%2520plausible%2520human%2520poses.%2520The%2520proposed%2520approach%2520excels%2520in%2520the%250Aphysics-based%2520human%2520pose%2520estimation%2520task%2520and%2520demonstrates%2520the%2520physical%250Aplausibility%2520of%2520the%2520predictive%2520dynamics%252C%2520compared%2520to%2520state%2520of%2520the%2520art.%2520The%2520code%250Ais%2520available%2520on%2520https%253A//github.com/cuongle1206/OSDCap%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07795v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal-state%20Dynamics%20Estimation%20for%20Physics-based%20Human%20Motion%20Capture%0A%20%20from%20Videos&entry.906535625=Cuong%20Le%20and%20Viktor%20Johansson%20and%20Manon%20Kok%20and%20Bastian%20Wandt&entry.1292438233=%20%20Human%20motion%20capture%20from%20monocular%20videos%20has%20made%20significant%20progress%20in%0Arecent%20years.%20However%2C%20modern%20approaches%20often%20produce%20temporal%20artifacts%2C%20e.g.%0Ain%20form%20of%20jittery%20motion%20and%20struggle%20to%20achieve%20smooth%20and%20physically%0Aplausible%20motions.%20Explicitly%20integrating%20physics%2C%20in%20form%20of%20internal%20forces%0Aand%20exterior%20torques%2C%20helps%20alleviating%20these%20artifacts.%20Current%0Astate-of-the-art%20approaches%20make%20use%20of%20an%20automatic%20PD%20controller%20to%20predict%0Atorques%20and%20reaction%20forces%20in%20order%20to%20re-simulate%20the%20input%20kinematics%2C%20i.e.%0Athe%20joint%20angles%20of%20a%20predefined%20skeleton.%20However%2C%20due%20to%20imperfect%20physical%0Amodels%2C%20these%20methods%20often%20require%20simplifying%20assumptions%20and%20extensive%0Apreprocessing%20of%20the%20input%20kinematics%20to%20achieve%20good%20performance.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20method%20to%20selectively%20incorporate%20the%20physics%20models%20with%0Athe%20kinematics%20observations%20in%20an%20online%20setting%2C%20inspired%20by%20a%20neural%0AKalman-filtering%20approach.%20We%20develop%20a%20control%20loop%20as%20a%20meta-PD%20controller%20to%0Apredict%20internal%20joint%20torques%20and%20external%20reaction%20forces%2C%20followed%20by%20a%0Aphysics-based%20motion%20simulation.%20A%20recurrent%20neural%20network%20is%20introduced%20to%0Arealize%20a%20Kalman%20filter%20that%20attentively%20balances%20the%20kinematics%20input%20and%0Asimulated%20motion%2C%20resulting%20in%20an%20optimal-state%20dynamics%20prediction.%20We%20show%0Athat%20this%20filtering%20step%20is%20crucial%20to%20provide%20an%20online%20supervision%20that%20helps%0Abalancing%20the%20shortcoming%20of%20the%20respective%20input%20motions%2C%20thus%20being%20important%0Afor%20not%20only%20capturing%20accurate%20global%20motion%20trajectories%20but%20also%20producing%0Aphysically%20plausible%20human%20poses.%20The%20proposed%20approach%20excels%20in%20the%0Aphysics-based%20human%20pose%20estimation%20task%20and%20demonstrates%20the%20physical%0Aplausibility%20of%20the%20predictive%20dynamics%2C%20compared%20to%20state%20of%20the%20art.%20The%20code%0Ais%20available%20on%20https%3A//github.com/cuongle1206/OSDCap%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07795v4&entry.124074799=Read"},
{"title": "Multilingual Machine Translation with Quantum Encoder Decoder\n  Attention-based Convolutional Variational Circuits", "author": "Subrit Dikshit and Ritu Tiwari and Priyank Jain", "abstract": "  Cloud-based multilingual translation services like Google Translate and\nMicrosoft Translator achieve state-of-the-art translation capabilities. These\nservices inherently use large multilingual language models such as GRU, LSTM,\nBERT, GPT, T5, or similar encoder-decoder architectures with attention\nmechanisms as the backbone. Also, new age natural language systems, for\ninstance ChatGPT and DeepSeek, have established huge potential in multiple\ntasks in natural language processing. At the same time, they also possess\noutstanding multilingual translation capabilities. However, these models use\nthe classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder\nAttention-based Convolutional Variational Circuits) is an alternate solution\nthat explores the quantum computing realm instead of the classical computing\nrealm to study and demonstrate multilingual machine translation. QEDACVC\nintroduces the quantum encoder-decoder architecture that simulates and runs on\nquantum computing hardware via quantum convolution, quantum pooling, quantum\nvariational circuit, and quantum attention as software alterations. QEDACVC\nachieves an Accuracy of 82% when trained on the OPUS dataset for English,\nFrench, German, and Hindi corpora for multilingual translations.\n", "link": "http://arxiv.org/abs/2505.09407v1", "date": "2025-05-14", "relevancy": 2.5365, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.522}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multilingual%20Machine%20Translation%20with%20Quantum%20Encoder%20Decoder%0A%20%20Attention-based%20Convolutional%20Variational%20Circuits&body=Title%3A%20Multilingual%20Machine%20Translation%20with%20Quantum%20Encoder%20Decoder%0A%20%20Attention-based%20Convolutional%20Variational%20Circuits%0AAuthor%3A%20Subrit%20Dikshit%20and%20Ritu%20Tiwari%20and%20Priyank%20Jain%0AAbstract%3A%20%20%20Cloud-based%20multilingual%20translation%20services%20like%20Google%20Translate%20and%0AMicrosoft%20Translator%20achieve%20state-of-the-art%20translation%20capabilities.%20These%0Aservices%20inherently%20use%20large%20multilingual%20language%20models%20such%20as%20GRU%2C%20LSTM%2C%0ABERT%2C%20GPT%2C%20T5%2C%20or%20similar%20encoder-decoder%20architectures%20with%20attention%0Amechanisms%20as%20the%20backbone.%20Also%2C%20new%20age%20natural%20language%20systems%2C%20for%0Ainstance%20ChatGPT%20and%20DeepSeek%2C%20have%20established%20huge%20potential%20in%20multiple%0Atasks%20in%20natural%20language%20processing.%20At%20the%20same%20time%2C%20they%20also%20possess%0Aoutstanding%20multilingual%20translation%20capabilities.%20However%2C%20these%20models%20use%0Athe%20classical%20computing%20realm%20as%20a%20backend.%20QEDACVC%20%28Quantum%20Encoder%20Decoder%0AAttention-based%20Convolutional%20Variational%20Circuits%29%20is%20an%20alternate%20solution%0Athat%20explores%20the%20quantum%20computing%20realm%20instead%20of%20the%20classical%20computing%0Arealm%20to%20study%20and%20demonstrate%20multilingual%20machine%20translation.%20QEDACVC%0Aintroduces%20the%20quantum%20encoder-decoder%20architecture%20that%20simulates%20and%20runs%20on%0Aquantum%20computing%20hardware%20via%20quantum%20convolution%2C%20quantum%20pooling%2C%20quantum%0Avariational%20circuit%2C%20and%20quantum%20attention%20as%20software%20alterations.%20QEDACVC%0Aachieves%20an%20Accuracy%20of%2082%25%20when%20trained%20on%20the%20OPUS%20dataset%20for%20English%2C%0AFrench%2C%20German%2C%20and%20Hindi%20corpora%20for%20multilingual%20translations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultilingual%2520Machine%2520Translation%2520with%2520Quantum%2520Encoder%2520Decoder%250A%2520%2520Attention-based%2520Convolutional%2520Variational%2520Circuits%26entry.906535625%3DSubrit%2520Dikshit%2520and%2520Ritu%2520Tiwari%2520and%2520Priyank%2520Jain%26entry.1292438233%3D%2520%2520Cloud-based%2520multilingual%2520translation%2520services%2520like%2520Google%2520Translate%2520and%250AMicrosoft%2520Translator%2520achieve%2520state-of-the-art%2520translation%2520capabilities.%2520These%250Aservices%2520inherently%2520use%2520large%2520multilingual%2520language%2520models%2520such%2520as%2520GRU%252C%2520LSTM%252C%250ABERT%252C%2520GPT%252C%2520T5%252C%2520or%2520similar%2520encoder-decoder%2520architectures%2520with%2520attention%250Amechanisms%2520as%2520the%2520backbone.%2520Also%252C%2520new%2520age%2520natural%2520language%2520systems%252C%2520for%250Ainstance%2520ChatGPT%2520and%2520DeepSeek%252C%2520have%2520established%2520huge%2520potential%2520in%2520multiple%250Atasks%2520in%2520natural%2520language%2520processing.%2520At%2520the%2520same%2520time%252C%2520they%2520also%2520possess%250Aoutstanding%2520multilingual%2520translation%2520capabilities.%2520However%252C%2520these%2520models%2520use%250Athe%2520classical%2520computing%2520realm%2520as%2520a%2520backend.%2520QEDACVC%2520%2528Quantum%2520Encoder%2520Decoder%250AAttention-based%2520Convolutional%2520Variational%2520Circuits%2529%2520is%2520an%2520alternate%2520solution%250Athat%2520explores%2520the%2520quantum%2520computing%2520realm%2520instead%2520of%2520the%2520classical%2520computing%250Arealm%2520to%2520study%2520and%2520demonstrate%2520multilingual%2520machine%2520translation.%2520QEDACVC%250Aintroduces%2520the%2520quantum%2520encoder-decoder%2520architecture%2520that%2520simulates%2520and%2520runs%2520on%250Aquantum%2520computing%2520hardware%2520via%2520quantum%2520convolution%252C%2520quantum%2520pooling%252C%2520quantum%250Avariational%2520circuit%252C%2520and%2520quantum%2520attention%2520as%2520software%2520alterations.%2520QEDACVC%250Aachieves%2520an%2520Accuracy%2520of%252082%2525%2520when%2520trained%2520on%2520the%2520OPUS%2520dataset%2520for%2520English%252C%250AFrench%252C%2520German%252C%2520and%2520Hindi%2520corpora%2520for%2520multilingual%2520translations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilingual%20Machine%20Translation%20with%20Quantum%20Encoder%20Decoder%0A%20%20Attention-based%20Convolutional%20Variational%20Circuits&entry.906535625=Subrit%20Dikshit%20and%20Ritu%20Tiwari%20and%20Priyank%20Jain&entry.1292438233=%20%20Cloud-based%20multilingual%20translation%20services%20like%20Google%20Translate%20and%0AMicrosoft%20Translator%20achieve%20state-of-the-art%20translation%20capabilities.%20These%0Aservices%20inherently%20use%20large%20multilingual%20language%20models%20such%20as%20GRU%2C%20LSTM%2C%0ABERT%2C%20GPT%2C%20T5%2C%20or%20similar%20encoder-decoder%20architectures%20with%20attention%0Amechanisms%20as%20the%20backbone.%20Also%2C%20new%20age%20natural%20language%20systems%2C%20for%0Ainstance%20ChatGPT%20and%20DeepSeek%2C%20have%20established%20huge%20potential%20in%20multiple%0Atasks%20in%20natural%20language%20processing.%20At%20the%20same%20time%2C%20they%20also%20possess%0Aoutstanding%20multilingual%20translation%20capabilities.%20However%2C%20these%20models%20use%0Athe%20classical%20computing%20realm%20as%20a%20backend.%20QEDACVC%20%28Quantum%20Encoder%20Decoder%0AAttention-based%20Convolutional%20Variational%20Circuits%29%20is%20an%20alternate%20solution%0Athat%20explores%20the%20quantum%20computing%20realm%20instead%20of%20the%20classical%20computing%0Arealm%20to%20study%20and%20demonstrate%20multilingual%20machine%20translation.%20QEDACVC%0Aintroduces%20the%20quantum%20encoder-decoder%20architecture%20that%20simulates%20and%20runs%20on%0Aquantum%20computing%20hardware%20via%20quantum%20convolution%2C%20quantum%20pooling%2C%20quantum%0Avariational%20circuit%2C%20and%20quantum%20attention%20as%20software%20alterations.%20QEDACVC%0Aachieves%20an%20Accuracy%20of%2082%25%20when%20trained%20on%20the%20OPUS%20dataset%20for%20English%2C%0AFrench%2C%20German%2C%20and%20Hindi%20corpora%20for%20multilingual%20translations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09407v1&entry.124074799=Read"},
{"title": "Accelerating Multiscale Modeling with Hybrid Solvers: Coupling FEM and\n  Neural Operators with Domain Decomposition", "author": "Wei Wang and Maryam Hakimzadeh and Haihui Ruan and Somdatta Goswami", "abstract": "  Numerical solvers for PDEs face challenges in balancing computational cost\nand accuracy, particularly for multiscale and dynamical systems. Neural\noperators (NOs) can significantly speed up simulations; however, they face\nchallenges such as error accumulation for dynamical systems and limited\ngeneralization in multiphysics problems. This work introduces a novel hybrid\nframework that integrates PI-NO with finite element method (FE) through domain\ndecomposition and leverages numerical analysis for time marching. The core\ninnovation lies in efficient coupling FE and NO subdomains via a Schwarz\nalternating method: regions with complex, nonlinear, or high-gradient behavior\nare resolved using a pretrained NO, while the remainder is handled by\nconventional FE. To address the challenges of dynamic systems, we embed a\ntime-stepping scheme directly into the NO architecture, substantially reducing\nlong-term error propagation. Also, an adaptive subdomain evolution strategy\nenables the ML resolved region to expand dynamically, capturing emerging fine\nscale features without remeshing. The framework efficacy has been validated\nacross a range of problems, spanning static, quasi-static, and dynamic regimes\n(e.g., linear elasticity, hyperelasticity, and elastodynamics), demonstrating\naccelerated convergence (up to 20% improvement in convergence compared to\nconventional FE coupling) while preserving solution fidelity with error margins\nconsistently below 1%. Our study shows that our hybrid solver: (1) maintains\nsolution continuity across subdomain interfaces, (2) reduces computational\ncosts by eliminating fine mesh requirements, (3) mitigates error accumulation\nin time dependent simulations, and (4) enables automatic adaptation to evolving\nphysical phenomena. This work bridges the gap between numerical methods and\nAI-driven surrogates, offering a scalable pathway for high-fidelity multiscale\nsimulations.\n", "link": "http://arxiv.org/abs/2504.11383v3", "date": "2025-05-14", "relevancy": 2.5358, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5214}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5039}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Multiscale%20Modeling%20with%20Hybrid%20Solvers%3A%20Coupling%20FEM%20and%0A%20%20Neural%20Operators%20with%20Domain%20Decomposition&body=Title%3A%20Accelerating%20Multiscale%20Modeling%20with%20Hybrid%20Solvers%3A%20Coupling%20FEM%20and%0A%20%20Neural%20Operators%20with%20Domain%20Decomposition%0AAuthor%3A%20Wei%20Wang%20and%20Maryam%20Hakimzadeh%20and%20Haihui%20Ruan%20and%20Somdatta%20Goswami%0AAbstract%3A%20%20%20Numerical%20solvers%20for%20PDEs%20face%20challenges%20in%20balancing%20computational%20cost%0Aand%20accuracy%2C%20particularly%20for%20multiscale%20and%20dynamical%20systems.%20Neural%0Aoperators%20%28NOs%29%20can%20significantly%20speed%20up%20simulations%3B%20however%2C%20they%20face%0Achallenges%20such%20as%20error%20accumulation%20for%20dynamical%20systems%20and%20limited%0Ageneralization%20in%20multiphysics%20problems.%20This%20work%20introduces%20a%20novel%20hybrid%0Aframework%20that%20integrates%20PI-NO%20with%20finite%20element%20method%20%28FE%29%20through%20domain%0Adecomposition%20and%20leverages%20numerical%20analysis%20for%20time%20marching.%20The%20core%0Ainnovation%20lies%20in%20efficient%20coupling%20FE%20and%20NO%20subdomains%20via%20a%20Schwarz%0Aalternating%20method%3A%20regions%20with%20complex%2C%20nonlinear%2C%20or%20high-gradient%20behavior%0Aare%20resolved%20using%20a%20pretrained%20NO%2C%20while%20the%20remainder%20is%20handled%20by%0Aconventional%20FE.%20To%20address%20the%20challenges%20of%20dynamic%20systems%2C%20we%20embed%20a%0Atime-stepping%20scheme%20directly%20into%20the%20NO%20architecture%2C%20substantially%20reducing%0Along-term%20error%20propagation.%20Also%2C%20an%20adaptive%20subdomain%20evolution%20strategy%0Aenables%20the%20ML%20resolved%20region%20to%20expand%20dynamically%2C%20capturing%20emerging%20fine%0Ascale%20features%20without%20remeshing.%20The%20framework%20efficacy%20has%20been%20validated%0Aacross%20a%20range%20of%20problems%2C%20spanning%20static%2C%20quasi-static%2C%20and%20dynamic%20regimes%0A%28e.g.%2C%20linear%20elasticity%2C%20hyperelasticity%2C%20and%20elastodynamics%29%2C%20demonstrating%0Aaccelerated%20convergence%20%28up%20to%2020%25%20improvement%20in%20convergence%20compared%20to%0Aconventional%20FE%20coupling%29%20while%20preserving%20solution%20fidelity%20with%20error%20margins%0Aconsistently%20below%201%25.%20Our%20study%20shows%20that%20our%20hybrid%20solver%3A%20%281%29%20maintains%0Asolution%20continuity%20across%20subdomain%20interfaces%2C%20%282%29%20reduces%20computational%0Acosts%20by%20eliminating%20fine%20mesh%20requirements%2C%20%283%29%20mitigates%20error%20accumulation%0Ain%20time%20dependent%20simulations%2C%20and%20%284%29%20enables%20automatic%20adaptation%20to%20evolving%0Aphysical%20phenomena.%20This%20work%20bridges%20the%20gap%20between%20numerical%20methods%20and%0AAI-driven%20surrogates%2C%20offering%20a%20scalable%20pathway%20for%20high-fidelity%20multiscale%0Asimulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11383v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Multiscale%2520Modeling%2520with%2520Hybrid%2520Solvers%253A%2520Coupling%2520FEM%2520and%250A%2520%2520Neural%2520Operators%2520with%2520Domain%2520Decomposition%26entry.906535625%3DWei%2520Wang%2520and%2520Maryam%2520Hakimzadeh%2520and%2520Haihui%2520Ruan%2520and%2520Somdatta%2520Goswami%26entry.1292438233%3D%2520%2520Numerical%2520solvers%2520for%2520PDEs%2520face%2520challenges%2520in%2520balancing%2520computational%2520cost%250Aand%2520accuracy%252C%2520particularly%2520for%2520multiscale%2520and%2520dynamical%2520systems.%2520Neural%250Aoperators%2520%2528NOs%2529%2520can%2520significantly%2520speed%2520up%2520simulations%253B%2520however%252C%2520they%2520face%250Achallenges%2520such%2520as%2520error%2520accumulation%2520for%2520dynamical%2520systems%2520and%2520limited%250Ageneralization%2520in%2520multiphysics%2520problems.%2520This%2520work%2520introduces%2520a%2520novel%2520hybrid%250Aframework%2520that%2520integrates%2520PI-NO%2520with%2520finite%2520element%2520method%2520%2528FE%2529%2520through%2520domain%250Adecomposition%2520and%2520leverages%2520numerical%2520analysis%2520for%2520time%2520marching.%2520The%2520core%250Ainnovation%2520lies%2520in%2520efficient%2520coupling%2520FE%2520and%2520NO%2520subdomains%2520via%2520a%2520Schwarz%250Aalternating%2520method%253A%2520regions%2520with%2520complex%252C%2520nonlinear%252C%2520or%2520high-gradient%2520behavior%250Aare%2520resolved%2520using%2520a%2520pretrained%2520NO%252C%2520while%2520the%2520remainder%2520is%2520handled%2520by%250Aconventional%2520FE.%2520To%2520address%2520the%2520challenges%2520of%2520dynamic%2520systems%252C%2520we%2520embed%2520a%250Atime-stepping%2520scheme%2520directly%2520into%2520the%2520NO%2520architecture%252C%2520substantially%2520reducing%250Along-term%2520error%2520propagation.%2520Also%252C%2520an%2520adaptive%2520subdomain%2520evolution%2520strategy%250Aenables%2520the%2520ML%2520resolved%2520region%2520to%2520expand%2520dynamically%252C%2520capturing%2520emerging%2520fine%250Ascale%2520features%2520without%2520remeshing.%2520The%2520framework%2520efficacy%2520has%2520been%2520validated%250Aacross%2520a%2520range%2520of%2520problems%252C%2520spanning%2520static%252C%2520quasi-static%252C%2520and%2520dynamic%2520regimes%250A%2528e.g.%252C%2520linear%2520elasticity%252C%2520hyperelasticity%252C%2520and%2520elastodynamics%2529%252C%2520demonstrating%250Aaccelerated%2520convergence%2520%2528up%2520to%252020%2525%2520improvement%2520in%2520convergence%2520compared%2520to%250Aconventional%2520FE%2520coupling%2529%2520while%2520preserving%2520solution%2520fidelity%2520with%2520error%2520margins%250Aconsistently%2520below%25201%2525.%2520Our%2520study%2520shows%2520that%2520our%2520hybrid%2520solver%253A%2520%25281%2529%2520maintains%250Asolution%2520continuity%2520across%2520subdomain%2520interfaces%252C%2520%25282%2529%2520reduces%2520computational%250Acosts%2520by%2520eliminating%2520fine%2520mesh%2520requirements%252C%2520%25283%2529%2520mitigates%2520error%2520accumulation%250Ain%2520time%2520dependent%2520simulations%252C%2520and%2520%25284%2529%2520enables%2520automatic%2520adaptation%2520to%2520evolving%250Aphysical%2520phenomena.%2520This%2520work%2520bridges%2520the%2520gap%2520between%2520numerical%2520methods%2520and%250AAI-driven%2520surrogates%252C%2520offering%2520a%2520scalable%2520pathway%2520for%2520high-fidelity%2520multiscale%250Asimulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11383v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Multiscale%20Modeling%20with%20Hybrid%20Solvers%3A%20Coupling%20FEM%20and%0A%20%20Neural%20Operators%20with%20Domain%20Decomposition&entry.906535625=Wei%20Wang%20and%20Maryam%20Hakimzadeh%20and%20Haihui%20Ruan%20and%20Somdatta%20Goswami&entry.1292438233=%20%20Numerical%20solvers%20for%20PDEs%20face%20challenges%20in%20balancing%20computational%20cost%0Aand%20accuracy%2C%20particularly%20for%20multiscale%20and%20dynamical%20systems.%20Neural%0Aoperators%20%28NOs%29%20can%20significantly%20speed%20up%20simulations%3B%20however%2C%20they%20face%0Achallenges%20such%20as%20error%20accumulation%20for%20dynamical%20systems%20and%20limited%0Ageneralization%20in%20multiphysics%20problems.%20This%20work%20introduces%20a%20novel%20hybrid%0Aframework%20that%20integrates%20PI-NO%20with%20finite%20element%20method%20%28FE%29%20through%20domain%0Adecomposition%20and%20leverages%20numerical%20analysis%20for%20time%20marching.%20The%20core%0Ainnovation%20lies%20in%20efficient%20coupling%20FE%20and%20NO%20subdomains%20via%20a%20Schwarz%0Aalternating%20method%3A%20regions%20with%20complex%2C%20nonlinear%2C%20or%20high-gradient%20behavior%0Aare%20resolved%20using%20a%20pretrained%20NO%2C%20while%20the%20remainder%20is%20handled%20by%0Aconventional%20FE.%20To%20address%20the%20challenges%20of%20dynamic%20systems%2C%20we%20embed%20a%0Atime-stepping%20scheme%20directly%20into%20the%20NO%20architecture%2C%20substantially%20reducing%0Along-term%20error%20propagation.%20Also%2C%20an%20adaptive%20subdomain%20evolution%20strategy%0Aenables%20the%20ML%20resolved%20region%20to%20expand%20dynamically%2C%20capturing%20emerging%20fine%0Ascale%20features%20without%20remeshing.%20The%20framework%20efficacy%20has%20been%20validated%0Aacross%20a%20range%20of%20problems%2C%20spanning%20static%2C%20quasi-static%2C%20and%20dynamic%20regimes%0A%28e.g.%2C%20linear%20elasticity%2C%20hyperelasticity%2C%20and%20elastodynamics%29%2C%20demonstrating%0Aaccelerated%20convergence%20%28up%20to%2020%25%20improvement%20in%20convergence%20compared%20to%0Aconventional%20FE%20coupling%29%20while%20preserving%20solution%20fidelity%20with%20error%20margins%0Aconsistently%20below%201%25.%20Our%20study%20shows%20that%20our%20hybrid%20solver%3A%20%281%29%20maintains%0Asolution%20continuity%20across%20subdomain%20interfaces%2C%20%282%29%20reduces%20computational%0Acosts%20by%20eliminating%20fine%20mesh%20requirements%2C%20%283%29%20mitigates%20error%20accumulation%0Ain%20time%20dependent%20simulations%2C%20and%20%284%29%20enables%20automatic%20adaptation%20to%20evolving%0Aphysical%20phenomena.%20This%20work%20bridges%20the%20gap%20between%20numerical%20methods%20and%0AAI-driven%20surrogates%2C%20offering%20a%20scalable%20pathway%20for%20high-fidelity%20multiscale%0Asimulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11383v3&entry.124074799=Read"},
{"title": "Depth-Based Local Center Clustering: A Framework for Handling Different\n  Clustering Scenarios", "author": "Siyi Wang and Alexandre Leblanc and Paul D. McNicholas", "abstract": "  Cluster analysis, or clustering, plays a crucial role across numerous\nscientific and engineering domains. Despite the wealth of clustering methods\nproposed over the past decades, each method is typically designed for specific\nscenarios and presents certain limitations in practical applications. In this\npaper, we propose depth-based local center clustering (DLCC). This novel method\nmakes use of data depth, which is known to produce a center-outward ordering of\nsample points in a multivariate space. However, data depth typically fails to\ncapture the multimodal characteristics of {data}, something of the utmost\nimportance in the context of clustering. To overcome this, DLCC makes use of a\nlocal version of data depth that is based on subsets of {data}. From this,\nlocal centers can be identified as well as clusters of varying shapes.\nFurthermore, we propose a new internal metric based on density-based clustering\nto evaluate clustering performance on {non-convex clusters}. Overall, DLCC is a\nflexible clustering approach that seems to overcome some limitations of\ntraditional clustering methods, thereby enhancing data analysis capabilities\nacross a wide range of application scenarios.\n", "link": "http://arxiv.org/abs/2505.09516v1", "date": "2025-05-14", "relevancy": 2.5217, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4984}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-Based%20Local%20Center%20Clustering%3A%20A%20Framework%20for%20Handling%20Different%0A%20%20Clustering%20Scenarios&body=Title%3A%20Depth-Based%20Local%20Center%20Clustering%3A%20A%20Framework%20for%20Handling%20Different%0A%20%20Clustering%20Scenarios%0AAuthor%3A%20Siyi%20Wang%20and%20Alexandre%20Leblanc%20and%20Paul%20D.%20McNicholas%0AAbstract%3A%20%20%20Cluster%20analysis%2C%20or%20clustering%2C%20plays%20a%20crucial%20role%20across%20numerous%0Ascientific%20and%20engineering%20domains.%20Despite%20the%20wealth%20of%20clustering%20methods%0Aproposed%20over%20the%20past%20decades%2C%20each%20method%20is%20typically%20designed%20for%20specific%0Ascenarios%20and%20presents%20certain%20limitations%20in%20practical%20applications.%20In%20this%0Apaper%2C%20we%20propose%20depth-based%20local%20center%20clustering%20%28DLCC%29.%20This%20novel%20method%0Amakes%20use%20of%20data%20depth%2C%20which%20is%20known%20to%20produce%20a%20center-outward%20ordering%20of%0Asample%20points%20in%20a%20multivariate%20space.%20However%2C%20data%20depth%20typically%20fails%20to%0Acapture%20the%20multimodal%20characteristics%20of%20%7Bdata%7D%2C%20something%20of%20the%20utmost%0Aimportance%20in%20the%20context%20of%20clustering.%20To%20overcome%20this%2C%20DLCC%20makes%20use%20of%20a%0Alocal%20version%20of%20data%20depth%20that%20is%20based%20on%20subsets%20of%20%7Bdata%7D.%20From%20this%2C%0Alocal%20centers%20can%20be%20identified%20as%20well%20as%20clusters%20of%20varying%20shapes.%0AFurthermore%2C%20we%20propose%20a%20new%20internal%20metric%20based%20on%20density-based%20clustering%0Ato%20evaluate%20clustering%20performance%20on%20%7Bnon-convex%20clusters%7D.%20Overall%2C%20DLCC%20is%20a%0Aflexible%20clustering%20approach%20that%20seems%20to%20overcome%20some%20limitations%20of%0Atraditional%20clustering%20methods%2C%20thereby%20enhancing%20data%20analysis%20capabilities%0Aacross%20a%20wide%20range%20of%20application%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-Based%2520Local%2520Center%2520Clustering%253A%2520A%2520Framework%2520for%2520Handling%2520Different%250A%2520%2520Clustering%2520Scenarios%26entry.906535625%3DSiyi%2520Wang%2520and%2520Alexandre%2520Leblanc%2520and%2520Paul%2520D.%2520McNicholas%26entry.1292438233%3D%2520%2520Cluster%2520analysis%252C%2520or%2520clustering%252C%2520plays%2520a%2520crucial%2520role%2520across%2520numerous%250Ascientific%2520and%2520engineering%2520domains.%2520Despite%2520the%2520wealth%2520of%2520clustering%2520methods%250Aproposed%2520over%2520the%2520past%2520decades%252C%2520each%2520method%2520is%2520typically%2520designed%2520for%2520specific%250Ascenarios%2520and%2520presents%2520certain%2520limitations%2520in%2520practical%2520applications.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520depth-based%2520local%2520center%2520clustering%2520%2528DLCC%2529.%2520This%2520novel%2520method%250Amakes%2520use%2520of%2520data%2520depth%252C%2520which%2520is%2520known%2520to%2520produce%2520a%2520center-outward%2520ordering%2520of%250Asample%2520points%2520in%2520a%2520multivariate%2520space.%2520However%252C%2520data%2520depth%2520typically%2520fails%2520to%250Acapture%2520the%2520multimodal%2520characteristics%2520of%2520%257Bdata%257D%252C%2520something%2520of%2520the%2520utmost%250Aimportance%2520in%2520the%2520context%2520of%2520clustering.%2520To%2520overcome%2520this%252C%2520DLCC%2520makes%2520use%2520of%2520a%250Alocal%2520version%2520of%2520data%2520depth%2520that%2520is%2520based%2520on%2520subsets%2520of%2520%257Bdata%257D.%2520From%2520this%252C%250Alocal%2520centers%2520can%2520be%2520identified%2520as%2520well%2520as%2520clusters%2520of%2520varying%2520shapes.%250AFurthermore%252C%2520we%2520propose%2520a%2520new%2520internal%2520metric%2520based%2520on%2520density-based%2520clustering%250Ato%2520evaluate%2520clustering%2520performance%2520on%2520%257Bnon-convex%2520clusters%257D.%2520Overall%252C%2520DLCC%2520is%2520a%250Aflexible%2520clustering%2520approach%2520that%2520seems%2520to%2520overcome%2520some%2520limitations%2520of%250Atraditional%2520clustering%2520methods%252C%2520thereby%2520enhancing%2520data%2520analysis%2520capabilities%250Aacross%2520a%2520wide%2520range%2520of%2520application%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-Based%20Local%20Center%20Clustering%3A%20A%20Framework%20for%20Handling%20Different%0A%20%20Clustering%20Scenarios&entry.906535625=Siyi%20Wang%20and%20Alexandre%20Leblanc%20and%20Paul%20D.%20McNicholas&entry.1292438233=%20%20Cluster%20analysis%2C%20or%20clustering%2C%20plays%20a%20crucial%20role%20across%20numerous%0Ascientific%20and%20engineering%20domains.%20Despite%20the%20wealth%20of%20clustering%20methods%0Aproposed%20over%20the%20past%20decades%2C%20each%20method%20is%20typically%20designed%20for%20specific%0Ascenarios%20and%20presents%20certain%20limitations%20in%20practical%20applications.%20In%20this%0Apaper%2C%20we%20propose%20depth-based%20local%20center%20clustering%20%28DLCC%29.%20This%20novel%20method%0Amakes%20use%20of%20data%20depth%2C%20which%20is%20known%20to%20produce%20a%20center-outward%20ordering%20of%0Asample%20points%20in%20a%20multivariate%20space.%20However%2C%20data%20depth%20typically%20fails%20to%0Acapture%20the%20multimodal%20characteristics%20of%20%7Bdata%7D%2C%20something%20of%20the%20utmost%0Aimportance%20in%20the%20context%20of%20clustering.%20To%20overcome%20this%2C%20DLCC%20makes%20use%20of%20a%0Alocal%20version%20of%20data%20depth%20that%20is%20based%20on%20subsets%20of%20%7Bdata%7D.%20From%20this%2C%0Alocal%20centers%20can%20be%20identified%20as%20well%20as%20clusters%20of%20varying%20shapes.%0AFurthermore%2C%20we%20propose%20a%20new%20internal%20metric%20based%20on%20density-based%20clustering%0Ato%20evaluate%20clustering%20performance%20on%20%7Bnon-convex%20clusters%7D.%20Overall%2C%20DLCC%20is%20a%0Aflexible%20clustering%20approach%20that%20seems%20to%20overcome%20some%20limitations%20of%0Atraditional%20clustering%20methods%2C%20thereby%20enhancing%20data%20analysis%20capabilities%0Aacross%20a%20wide%20range%20of%20application%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09516v1&entry.124074799=Read"},
{"title": "Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through\n  Differentiable Object Shapes", "author": "Nicola Marinello and Simen Cassiman and Jonas Heylen and Marc Proesmans and Luc Van Gool", "abstract": "  Autonomous vehicles need a complete map of their surroundings to plan and\nact. This has sparked research into the tasks of 3D occupancy prediction, 3D\nscene completion, and 3D panoptic scene completion, which predict a dense map\nof the ego vehicle's surroundings as a voxel grid. Scene completion extends\noccupancy prediction by predicting occluded regions of the voxel grid, and\npanoptic scene completion further extends this task by also distinguishing\nobject instances within the same class; both aspects are crucial for path\nplanning and decision-making. However, 3D panoptic scene completion is\ncurrently underexplored. This work introduces a novel framework for 3D panoptic\nscene completion that extends existing 3D semantic scene completion models. We\npropose an Object Module and Panoptic Module that can easily be integrated with\n3D occupancy and scene completion methods presented in the literature. Our\napproach leverages the available annotations in occupancy benchmarks, allowing\nindividual object shapes to be learned as a differentiable problem. The code is\navailable at https://github.com/nicolamarinello/OffsetOcc .\n", "link": "http://arxiv.org/abs/2505.09562v1", "date": "2025-05-14", "relevancy": 2.5193, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6568}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Camera-Only%203D%20Panoptic%20Scene%20Completion%20for%20Autonomous%20Driving%20through%0A%20%20Differentiable%20Object%20Shapes&body=Title%3A%20Camera-Only%203D%20Panoptic%20Scene%20Completion%20for%20Autonomous%20Driving%20through%0A%20%20Differentiable%20Object%20Shapes%0AAuthor%3A%20Nicola%20Marinello%20and%20Simen%20Cassiman%20and%20Jonas%20Heylen%20and%20Marc%20Proesmans%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Autonomous%20vehicles%20need%20a%20complete%20map%20of%20their%20surroundings%20to%20plan%20and%0Aact.%20This%20has%20sparked%20research%20into%20the%20tasks%20of%203D%20occupancy%20prediction%2C%203D%0Ascene%20completion%2C%20and%203D%20panoptic%20scene%20completion%2C%20which%20predict%20a%20dense%20map%0Aof%20the%20ego%20vehicle%27s%20surroundings%20as%20a%20voxel%20grid.%20Scene%20completion%20extends%0Aoccupancy%20prediction%20by%20predicting%20occluded%20regions%20of%20the%20voxel%20grid%2C%20and%0Apanoptic%20scene%20completion%20further%20extends%20this%20task%20by%20also%20distinguishing%0Aobject%20instances%20within%20the%20same%20class%3B%20both%20aspects%20are%20crucial%20for%20path%0Aplanning%20and%20decision-making.%20However%2C%203D%20panoptic%20scene%20completion%20is%0Acurrently%20underexplored.%20This%20work%20introduces%20a%20novel%20framework%20for%203D%20panoptic%0Ascene%20completion%20that%20extends%20existing%203D%20semantic%20scene%20completion%20models.%20We%0Apropose%20an%20Object%20Module%20and%20Panoptic%20Module%20that%20can%20easily%20be%20integrated%20with%0A3D%20occupancy%20and%20scene%20completion%20methods%20presented%20in%20the%20literature.%20Our%0Aapproach%20leverages%20the%20available%20annotations%20in%20occupancy%20benchmarks%2C%20allowing%0Aindividual%20object%20shapes%20to%20be%20learned%20as%20a%20differentiable%20problem.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/nicolamarinello/OffsetOcc%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCamera-Only%25203D%2520Panoptic%2520Scene%2520Completion%2520for%2520Autonomous%2520Driving%2520through%250A%2520%2520Differentiable%2520Object%2520Shapes%26entry.906535625%3DNicola%2520Marinello%2520and%2520Simen%2520Cassiman%2520and%2520Jonas%2520Heylen%2520and%2520Marc%2520Proesmans%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Autonomous%2520vehicles%2520need%2520a%2520complete%2520map%2520of%2520their%2520surroundings%2520to%2520plan%2520and%250Aact.%2520This%2520has%2520sparked%2520research%2520into%2520the%2520tasks%2520of%25203D%2520occupancy%2520prediction%252C%25203D%250Ascene%2520completion%252C%2520and%25203D%2520panoptic%2520scene%2520completion%252C%2520which%2520predict%2520a%2520dense%2520map%250Aof%2520the%2520ego%2520vehicle%2527s%2520surroundings%2520as%2520a%2520voxel%2520grid.%2520Scene%2520completion%2520extends%250Aoccupancy%2520prediction%2520by%2520predicting%2520occluded%2520regions%2520of%2520the%2520voxel%2520grid%252C%2520and%250Apanoptic%2520scene%2520completion%2520further%2520extends%2520this%2520task%2520by%2520also%2520distinguishing%250Aobject%2520instances%2520within%2520the%2520same%2520class%253B%2520both%2520aspects%2520are%2520crucial%2520for%2520path%250Aplanning%2520and%2520decision-making.%2520However%252C%25203D%2520panoptic%2520scene%2520completion%2520is%250Acurrently%2520underexplored.%2520This%2520work%2520introduces%2520a%2520novel%2520framework%2520for%25203D%2520panoptic%250Ascene%2520completion%2520that%2520extends%2520existing%25203D%2520semantic%2520scene%2520completion%2520models.%2520We%250Apropose%2520an%2520Object%2520Module%2520and%2520Panoptic%2520Module%2520that%2520can%2520easily%2520be%2520integrated%2520with%250A3D%2520occupancy%2520and%2520scene%2520completion%2520methods%2520presented%2520in%2520the%2520literature.%2520Our%250Aapproach%2520leverages%2520the%2520available%2520annotations%2520in%2520occupancy%2520benchmarks%252C%2520allowing%250Aindividual%2520object%2520shapes%2520to%2520be%2520learned%2520as%2520a%2520differentiable%2520problem.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/nicolamarinello/OffsetOcc%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Camera-Only%203D%20Panoptic%20Scene%20Completion%20for%20Autonomous%20Driving%20through%0A%20%20Differentiable%20Object%20Shapes&entry.906535625=Nicola%20Marinello%20and%20Simen%20Cassiman%20and%20Jonas%20Heylen%20and%20Marc%20Proesmans%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Autonomous%20vehicles%20need%20a%20complete%20map%20of%20their%20surroundings%20to%20plan%20and%0Aact.%20This%20has%20sparked%20research%20into%20the%20tasks%20of%203D%20occupancy%20prediction%2C%203D%0Ascene%20completion%2C%20and%203D%20panoptic%20scene%20completion%2C%20which%20predict%20a%20dense%20map%0Aof%20the%20ego%20vehicle%27s%20surroundings%20as%20a%20voxel%20grid.%20Scene%20completion%20extends%0Aoccupancy%20prediction%20by%20predicting%20occluded%20regions%20of%20the%20voxel%20grid%2C%20and%0Apanoptic%20scene%20completion%20further%20extends%20this%20task%20by%20also%20distinguishing%0Aobject%20instances%20within%20the%20same%20class%3B%20both%20aspects%20are%20crucial%20for%20path%0Aplanning%20and%20decision-making.%20However%2C%203D%20panoptic%20scene%20completion%20is%0Acurrently%20underexplored.%20This%20work%20introduces%20a%20novel%20framework%20for%203D%20panoptic%0Ascene%20completion%20that%20extends%20existing%203D%20semantic%20scene%20completion%20models.%20We%0Apropose%20an%20Object%20Module%20and%20Panoptic%20Module%20that%20can%20easily%20be%20integrated%20with%0A3D%20occupancy%20and%20scene%20completion%20methods%20presented%20in%20the%20literature.%20Our%0Aapproach%20leverages%20the%20available%20annotations%20in%20occupancy%20benchmarks%2C%20allowing%0Aindividual%20object%20shapes%20to%20be%20learned%20as%20a%20differentiable%20problem.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/nicolamarinello/OffsetOcc%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09562v1&entry.124074799=Read"},
{"title": "On the Learning with Augmented Class via Forests", "author": "Fan Xu and Wuyang Chen and Wei Gao", "abstract": "  Decision trees and forests have achieved successes in various real\napplications, most working with all testing classes known in training data. In\nthis work, we focus on learning with augmented class via forests, where an\naugmented class may appear in testing data yet not in training data. We\nincorporate information of augmented class into trees' splitting, i.e., a new\nsplitting criterion, called augmented Gini impurity, is introduced to exploit\nsome unlabeled data from testing distribution. We then develop the approach\nnamed Learning with Augmented Class via Forests (LACForest), which constructs\nshallow forests based on the augmented Gini impurity and then splits forests\nwith pseudo-labeled augmented instances for better performance. We also develop\ndeep neural forests with a novel optimization objective based on our augmented\nGini impurity, so as to utilize the representation power of neural networks for\nforests. Theoretically, we present the convergence analysis for augmented Gini\nimpurity, and finally conduct experiments to verify the effectiveness of our\napproaches. The code is available at https://github.com/nju-xuf/LACForest/.\n", "link": "http://arxiv.org/abs/2505.09294v1", "date": "2025-05-14", "relevancy": 2.496, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5454}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4865}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Learning%20with%20Augmented%20Class%20via%20Forests&body=Title%3A%20On%20the%20Learning%20with%20Augmented%20Class%20via%20Forests%0AAuthor%3A%20Fan%20Xu%20and%20Wuyang%20Chen%20and%20Wei%20Gao%0AAbstract%3A%20%20%20Decision%20trees%20and%20forests%20have%20achieved%20successes%20in%20various%20real%0Aapplications%2C%20most%20working%20with%20all%20testing%20classes%20known%20in%20training%20data.%20In%0Athis%20work%2C%20we%20focus%20on%20learning%20with%20augmented%20class%20via%20forests%2C%20where%20an%0Aaugmented%20class%20may%20appear%20in%20testing%20data%20yet%20not%20in%20training%20data.%20We%0Aincorporate%20information%20of%20augmented%20class%20into%20trees%27%20splitting%2C%20i.e.%2C%20a%20new%0Asplitting%20criterion%2C%20called%20augmented%20Gini%20impurity%2C%20is%20introduced%20to%20exploit%0Asome%20unlabeled%20data%20from%20testing%20distribution.%20We%20then%20develop%20the%20approach%0Anamed%20Learning%20with%20Augmented%20Class%20via%20Forests%20%28LACForest%29%2C%20which%20constructs%0Ashallow%20forests%20based%20on%20the%20augmented%20Gini%20impurity%20and%20then%20splits%20forests%0Awith%20pseudo-labeled%20augmented%20instances%20for%20better%20performance.%20We%20also%20develop%0Adeep%20neural%20forests%20with%20a%20novel%20optimization%20objective%20based%20on%20our%20augmented%0AGini%20impurity%2C%20so%20as%20to%20utilize%20the%20representation%20power%20of%20neural%20networks%20for%0Aforests.%20Theoretically%2C%20we%20present%20the%20convergence%20analysis%20for%20augmented%20Gini%0Aimpurity%2C%20and%20finally%20conduct%20experiments%20to%20verify%20the%20effectiveness%20of%20our%0Aapproaches.%20The%20code%20is%20available%20at%20https%3A//github.com/nju-xuf/LACForest/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Learning%2520with%2520Augmented%2520Class%2520via%2520Forests%26entry.906535625%3DFan%2520Xu%2520and%2520Wuyang%2520Chen%2520and%2520Wei%2520Gao%26entry.1292438233%3D%2520%2520Decision%2520trees%2520and%2520forests%2520have%2520achieved%2520successes%2520in%2520various%2520real%250Aapplications%252C%2520most%2520working%2520with%2520all%2520testing%2520classes%2520known%2520in%2520training%2520data.%2520In%250Athis%2520work%252C%2520we%2520focus%2520on%2520learning%2520with%2520augmented%2520class%2520via%2520forests%252C%2520where%2520an%250Aaugmented%2520class%2520may%2520appear%2520in%2520testing%2520data%2520yet%2520not%2520in%2520training%2520data.%2520We%250Aincorporate%2520information%2520of%2520augmented%2520class%2520into%2520trees%2527%2520splitting%252C%2520i.e.%252C%2520a%2520new%250Asplitting%2520criterion%252C%2520called%2520augmented%2520Gini%2520impurity%252C%2520is%2520introduced%2520to%2520exploit%250Asome%2520unlabeled%2520data%2520from%2520testing%2520distribution.%2520We%2520then%2520develop%2520the%2520approach%250Anamed%2520Learning%2520with%2520Augmented%2520Class%2520via%2520Forests%2520%2528LACForest%2529%252C%2520which%2520constructs%250Ashallow%2520forests%2520based%2520on%2520the%2520augmented%2520Gini%2520impurity%2520and%2520then%2520splits%2520forests%250Awith%2520pseudo-labeled%2520augmented%2520instances%2520for%2520better%2520performance.%2520We%2520also%2520develop%250Adeep%2520neural%2520forests%2520with%2520a%2520novel%2520optimization%2520objective%2520based%2520on%2520our%2520augmented%250AGini%2520impurity%252C%2520so%2520as%2520to%2520utilize%2520the%2520representation%2520power%2520of%2520neural%2520networks%2520for%250Aforests.%2520Theoretically%252C%2520we%2520present%2520the%2520convergence%2520analysis%2520for%2520augmented%2520Gini%250Aimpurity%252C%2520and%2520finally%2520conduct%2520experiments%2520to%2520verify%2520the%2520effectiveness%2520of%2520our%250Aapproaches.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/nju-xuf/LACForest/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Learning%20with%20Augmented%20Class%20via%20Forests&entry.906535625=Fan%20Xu%20and%20Wuyang%20Chen%20and%20Wei%20Gao&entry.1292438233=%20%20Decision%20trees%20and%20forests%20have%20achieved%20successes%20in%20various%20real%0Aapplications%2C%20most%20working%20with%20all%20testing%20classes%20known%20in%20training%20data.%20In%0Athis%20work%2C%20we%20focus%20on%20learning%20with%20augmented%20class%20via%20forests%2C%20where%20an%0Aaugmented%20class%20may%20appear%20in%20testing%20data%20yet%20not%20in%20training%20data.%20We%0Aincorporate%20information%20of%20augmented%20class%20into%20trees%27%20splitting%2C%20i.e.%2C%20a%20new%0Asplitting%20criterion%2C%20called%20augmented%20Gini%20impurity%2C%20is%20introduced%20to%20exploit%0Asome%20unlabeled%20data%20from%20testing%20distribution.%20We%20then%20develop%20the%20approach%0Anamed%20Learning%20with%20Augmented%20Class%20via%20Forests%20%28LACForest%29%2C%20which%20constructs%0Ashallow%20forests%20based%20on%20the%20augmented%20Gini%20impurity%20and%20then%20splits%20forests%0Awith%20pseudo-labeled%20augmented%20instances%20for%20better%20performance.%20We%20also%20develop%0Adeep%20neural%20forests%20with%20a%20novel%20optimization%20objective%20based%20on%20our%20augmented%0AGini%20impurity%2C%20so%20as%20to%20utilize%20the%20representation%20power%20of%20neural%20networks%20for%0Aforests.%20Theoretically%2C%20we%20present%20the%20convergence%20analysis%20for%20augmented%20Gini%0Aimpurity%2C%20and%20finally%20conduct%20experiments%20to%20verify%20the%20effectiveness%20of%20our%0Aapproaches.%20The%20code%20is%20available%20at%20https%3A//github.com/nju-xuf/LACForest/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09294v1&entry.124074799=Read"},
{"title": "SpecSphere: Dual-Pass Spectral-Spatial Graph Neural Networks with\n  Certified Robustness", "author": "Yoonhyuk Choi and Chong-Kwon Kim", "abstract": "  We introduce SpecSphere, the first dual-pass spectral-spatial GNN that\ncertifies every prediction against both $\\ell\\_{0}$ edge flips and\n$\\ell\\_{\\infty}$ feature perturbations, adapts to the full\nhomophily-heterophily spectrum, and surpasses the expressive power of\n1-Weisfeiler-Lehman while retaining linear-time complexity. Our model couples a\nChebyshev-polynomial spectral branch with an attention-gated spatial branch and\nfuses their representations through a lightweight MLP trained in a\ncooperative-adversarial min-max game. We further establish (i) a uniform\nChebyshev approximation theorem, (ii) minimax-optimal risk across the\nhomophily-heterophily spectrum, (iii) closed-form robustness certificates, and\n(iv) universal approximation strictly beyond 1-WL. SpecSphere achieves\nstate-of-the-art node-classification accuracy and delivers tighter certified\nrobustness guarantees on real-world benchmarks. These results demonstrate that\nhigh expressivity, heterophily adaptation, and provable robustness can coexist\nwithin a single, scalable architecture.\n", "link": "http://arxiv.org/abs/2505.08320v2", "date": "2025-05-14", "relevancy": 2.494, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5034}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4994}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecSphere%3A%20Dual-Pass%20Spectral-Spatial%20Graph%20Neural%20Networks%20with%0A%20%20Certified%20Robustness&body=Title%3A%20SpecSphere%3A%20Dual-Pass%20Spectral-Spatial%20Graph%20Neural%20Networks%20with%0A%20%20Certified%20Robustness%0AAuthor%3A%20Yoonhyuk%20Choi%20and%20Chong-Kwon%20Kim%0AAbstract%3A%20%20%20We%20introduce%20SpecSphere%2C%20the%20first%20dual-pass%20spectral-spatial%20GNN%20that%0Acertifies%20every%20prediction%20against%20both%20%24%5Cell%5C_%7B0%7D%24%20edge%20flips%20and%0A%24%5Cell%5C_%7B%5Cinfty%7D%24%20feature%20perturbations%2C%20adapts%20to%20the%20full%0Ahomophily-heterophily%20spectrum%2C%20and%20surpasses%20the%20expressive%20power%20of%0A1-Weisfeiler-Lehman%20while%20retaining%20linear-time%20complexity.%20Our%20model%20couples%20a%0AChebyshev-polynomial%20spectral%20branch%20with%20an%20attention-gated%20spatial%20branch%20and%0Afuses%20their%20representations%20through%20a%20lightweight%20MLP%20trained%20in%20a%0Acooperative-adversarial%20min-max%20game.%20We%20further%20establish%20%28i%29%20a%20uniform%0AChebyshev%20approximation%20theorem%2C%20%28ii%29%20minimax-optimal%20risk%20across%20the%0Ahomophily-heterophily%20spectrum%2C%20%28iii%29%20closed-form%20robustness%20certificates%2C%20and%0A%28iv%29%20universal%20approximation%20strictly%20beyond%201-WL.%20SpecSphere%20achieves%0Astate-of-the-art%20node-classification%20accuracy%20and%20delivers%20tighter%20certified%0Arobustness%20guarantees%20on%20real-world%20benchmarks.%20These%20results%20demonstrate%20that%0Ahigh%20expressivity%2C%20heterophily%20adaptation%2C%20and%20provable%20robustness%20can%20coexist%0Awithin%20a%20single%2C%20scalable%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08320v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecSphere%253A%2520Dual-Pass%2520Spectral-Spatial%2520Graph%2520Neural%2520Networks%2520with%250A%2520%2520Certified%2520Robustness%26entry.906535625%3DYoonhyuk%2520Choi%2520and%2520Chong-Kwon%2520Kim%26entry.1292438233%3D%2520%2520We%2520introduce%2520SpecSphere%252C%2520the%2520first%2520dual-pass%2520spectral-spatial%2520GNN%2520that%250Acertifies%2520every%2520prediction%2520against%2520both%2520%2524%255Cell%255C_%257B0%257D%2524%2520edge%2520flips%2520and%250A%2524%255Cell%255C_%257B%255Cinfty%257D%2524%2520feature%2520perturbations%252C%2520adapts%2520to%2520the%2520full%250Ahomophily-heterophily%2520spectrum%252C%2520and%2520surpasses%2520the%2520expressive%2520power%2520of%250A1-Weisfeiler-Lehman%2520while%2520retaining%2520linear-time%2520complexity.%2520Our%2520model%2520couples%2520a%250AChebyshev-polynomial%2520spectral%2520branch%2520with%2520an%2520attention-gated%2520spatial%2520branch%2520and%250Afuses%2520their%2520representations%2520through%2520a%2520lightweight%2520MLP%2520trained%2520in%2520a%250Acooperative-adversarial%2520min-max%2520game.%2520We%2520further%2520establish%2520%2528i%2529%2520a%2520uniform%250AChebyshev%2520approximation%2520theorem%252C%2520%2528ii%2529%2520minimax-optimal%2520risk%2520across%2520the%250Ahomophily-heterophily%2520spectrum%252C%2520%2528iii%2529%2520closed-form%2520robustness%2520certificates%252C%2520and%250A%2528iv%2529%2520universal%2520approximation%2520strictly%2520beyond%25201-WL.%2520SpecSphere%2520achieves%250Astate-of-the-art%2520node-classification%2520accuracy%2520and%2520delivers%2520tighter%2520certified%250Arobustness%2520guarantees%2520on%2520real-world%2520benchmarks.%2520These%2520results%2520demonstrate%2520that%250Ahigh%2520expressivity%252C%2520heterophily%2520adaptation%252C%2520and%2520provable%2520robustness%2520can%2520coexist%250Awithin%2520a%2520single%252C%2520scalable%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08320v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecSphere%3A%20Dual-Pass%20Spectral-Spatial%20Graph%20Neural%20Networks%20with%0A%20%20Certified%20Robustness&entry.906535625=Yoonhyuk%20Choi%20and%20Chong-Kwon%20Kim&entry.1292438233=%20%20We%20introduce%20SpecSphere%2C%20the%20first%20dual-pass%20spectral-spatial%20GNN%20that%0Acertifies%20every%20prediction%20against%20both%20%24%5Cell%5C_%7B0%7D%24%20edge%20flips%20and%0A%24%5Cell%5C_%7B%5Cinfty%7D%24%20feature%20perturbations%2C%20adapts%20to%20the%20full%0Ahomophily-heterophily%20spectrum%2C%20and%20surpasses%20the%20expressive%20power%20of%0A1-Weisfeiler-Lehman%20while%20retaining%20linear-time%20complexity.%20Our%20model%20couples%20a%0AChebyshev-polynomial%20spectral%20branch%20with%20an%20attention-gated%20spatial%20branch%20and%0Afuses%20their%20representations%20through%20a%20lightweight%20MLP%20trained%20in%20a%0Acooperative-adversarial%20min-max%20game.%20We%20further%20establish%20%28i%29%20a%20uniform%0AChebyshev%20approximation%20theorem%2C%20%28ii%29%20minimax-optimal%20risk%20across%20the%0Ahomophily-heterophily%20spectrum%2C%20%28iii%29%20closed-form%20robustness%20certificates%2C%20and%0A%28iv%29%20universal%20approximation%20strictly%20beyond%201-WL.%20SpecSphere%20achieves%0Astate-of-the-art%20node-classification%20accuracy%20and%20delivers%20tighter%20certified%0Arobustness%20guarantees%20on%20real-world%20benchmarks.%20These%20results%20demonstrate%20that%0Ahigh%20expressivity%2C%20heterophily%20adaptation%2C%20and%20provable%20robustness%20can%20coexist%0Awithin%20a%20single%2C%20scalable%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08320v2&entry.124074799=Read"},
{"title": "Text-driven Motion Generation: Overview, Challenges and Directions", "author": "Ali Rida Sahili and Najett Neji and Hedi Tabia", "abstract": "  Text-driven motion generation offers a powerful and intuitive way to create\nhuman movements directly from natural language. By removing the need for\npredefined motion inputs, it provides a flexible and accessible approach to\ncontrolling animated characters. This makes it especially useful in areas like\nvirtual reality, gaming, human-computer interaction, and robotics. In this\nreview, we first revisit the traditional perspective on motion synthesis, where\nmodels focused on predicting future poses from observed initial sequences,\noften conditioned on action labels. We then provide a comprehensive and\nstructured survey of modern text-to-motion generation approaches, categorizing\nthem from two complementary perspectives: (i) architectural, dividing methods\ninto VAE-based, diffusion-based, and hybrid models; and (ii) motion\nrepresentation, distinguishing between discrete and continuous motion\ngeneration strategies. In addition, we explore the most widely used datasets,\nevaluation methods, and recent benchmarks that have shaped progress in this\narea. With this survey, we aim to capture where the field currently stands,\nbring attention to its key challenges and limitations, and highlight promising\ndirections for future exploration. We hope this work offers a valuable starting\npoint for researchers and practitioners working to push the boundaries of\nlanguage-driven human motion synthesis.\n", "link": "http://arxiv.org/abs/2505.09379v1", "date": "2025-05-14", "relevancy": 2.4409, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6359}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6039}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-driven%20Motion%20Generation%3A%20Overview%2C%20Challenges%20and%20Directions&body=Title%3A%20Text-driven%20Motion%20Generation%3A%20Overview%2C%20Challenges%20and%20Directions%0AAuthor%3A%20Ali%20Rida%20Sahili%20and%20Najett%20Neji%20and%20Hedi%20Tabia%0AAbstract%3A%20%20%20Text-driven%20motion%20generation%20offers%20a%20powerful%20and%20intuitive%20way%20to%20create%0Ahuman%20movements%20directly%20from%20natural%20language.%20By%20removing%20the%20need%20for%0Apredefined%20motion%20inputs%2C%20it%20provides%20a%20flexible%20and%20accessible%20approach%20to%0Acontrolling%20animated%20characters.%20This%20makes%20it%20especially%20useful%20in%20areas%20like%0Avirtual%20reality%2C%20gaming%2C%20human-computer%20interaction%2C%20and%20robotics.%20In%20this%0Areview%2C%20we%20first%20revisit%20the%20traditional%20perspective%20on%20motion%20synthesis%2C%20where%0Amodels%20focused%20on%20predicting%20future%20poses%20from%20observed%20initial%20sequences%2C%0Aoften%20conditioned%20on%20action%20labels.%20We%20then%20provide%20a%20comprehensive%20and%0Astructured%20survey%20of%20modern%20text-to-motion%20generation%20approaches%2C%20categorizing%0Athem%20from%20two%20complementary%20perspectives%3A%20%28i%29%20architectural%2C%20dividing%20methods%0Ainto%20VAE-based%2C%20diffusion-based%2C%20and%20hybrid%20models%3B%20and%20%28ii%29%20motion%0Arepresentation%2C%20distinguishing%20between%20discrete%20and%20continuous%20motion%0Ageneration%20strategies.%20In%20addition%2C%20we%20explore%20the%20most%20widely%20used%20datasets%2C%0Aevaluation%20methods%2C%20and%20recent%20benchmarks%20that%20have%20shaped%20progress%20in%20this%0Aarea.%20With%20this%20survey%2C%20we%20aim%20to%20capture%20where%20the%20field%20currently%20stands%2C%0Abring%20attention%20to%20its%20key%20challenges%20and%20limitations%2C%20and%20highlight%20promising%0Adirections%20for%20future%20exploration.%20We%20hope%20this%20work%20offers%20a%20valuable%20starting%0Apoint%20for%20researchers%20and%20practitioners%20working%20to%20push%20the%20boundaries%20of%0Alanguage-driven%20human%20motion%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-driven%2520Motion%2520Generation%253A%2520Overview%252C%2520Challenges%2520and%2520Directions%26entry.906535625%3DAli%2520Rida%2520Sahili%2520and%2520Najett%2520Neji%2520and%2520Hedi%2520Tabia%26entry.1292438233%3D%2520%2520Text-driven%2520motion%2520generation%2520offers%2520a%2520powerful%2520and%2520intuitive%2520way%2520to%2520create%250Ahuman%2520movements%2520directly%2520from%2520natural%2520language.%2520By%2520removing%2520the%2520need%2520for%250Apredefined%2520motion%2520inputs%252C%2520it%2520provides%2520a%2520flexible%2520and%2520accessible%2520approach%2520to%250Acontrolling%2520animated%2520characters.%2520This%2520makes%2520it%2520especially%2520useful%2520in%2520areas%2520like%250Avirtual%2520reality%252C%2520gaming%252C%2520human-computer%2520interaction%252C%2520and%2520robotics.%2520In%2520this%250Areview%252C%2520we%2520first%2520revisit%2520the%2520traditional%2520perspective%2520on%2520motion%2520synthesis%252C%2520where%250Amodels%2520focused%2520on%2520predicting%2520future%2520poses%2520from%2520observed%2520initial%2520sequences%252C%250Aoften%2520conditioned%2520on%2520action%2520labels.%2520We%2520then%2520provide%2520a%2520comprehensive%2520and%250Astructured%2520survey%2520of%2520modern%2520text-to-motion%2520generation%2520approaches%252C%2520categorizing%250Athem%2520from%2520two%2520complementary%2520perspectives%253A%2520%2528i%2529%2520architectural%252C%2520dividing%2520methods%250Ainto%2520VAE-based%252C%2520diffusion-based%252C%2520and%2520hybrid%2520models%253B%2520and%2520%2528ii%2529%2520motion%250Arepresentation%252C%2520distinguishing%2520between%2520discrete%2520and%2520continuous%2520motion%250Ageneration%2520strategies.%2520In%2520addition%252C%2520we%2520explore%2520the%2520most%2520widely%2520used%2520datasets%252C%250Aevaluation%2520methods%252C%2520and%2520recent%2520benchmarks%2520that%2520have%2520shaped%2520progress%2520in%2520this%250Aarea.%2520With%2520this%2520survey%252C%2520we%2520aim%2520to%2520capture%2520where%2520the%2520field%2520currently%2520stands%252C%250Abring%2520attention%2520to%2520its%2520key%2520challenges%2520and%2520limitations%252C%2520and%2520highlight%2520promising%250Adirections%2520for%2520future%2520exploration.%2520We%2520hope%2520this%2520work%2520offers%2520a%2520valuable%2520starting%250Apoint%2520for%2520researchers%2520and%2520practitioners%2520working%2520to%2520push%2520the%2520boundaries%2520of%250Alanguage-driven%2520human%2520motion%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-driven%20Motion%20Generation%3A%20Overview%2C%20Challenges%20and%20Directions&entry.906535625=Ali%20Rida%20Sahili%20and%20Najett%20Neji%20and%20Hedi%20Tabia&entry.1292438233=%20%20Text-driven%20motion%20generation%20offers%20a%20powerful%20and%20intuitive%20way%20to%20create%0Ahuman%20movements%20directly%20from%20natural%20language.%20By%20removing%20the%20need%20for%0Apredefined%20motion%20inputs%2C%20it%20provides%20a%20flexible%20and%20accessible%20approach%20to%0Acontrolling%20animated%20characters.%20This%20makes%20it%20especially%20useful%20in%20areas%20like%0Avirtual%20reality%2C%20gaming%2C%20human-computer%20interaction%2C%20and%20robotics.%20In%20this%0Areview%2C%20we%20first%20revisit%20the%20traditional%20perspective%20on%20motion%20synthesis%2C%20where%0Amodels%20focused%20on%20predicting%20future%20poses%20from%20observed%20initial%20sequences%2C%0Aoften%20conditioned%20on%20action%20labels.%20We%20then%20provide%20a%20comprehensive%20and%0Astructured%20survey%20of%20modern%20text-to-motion%20generation%20approaches%2C%20categorizing%0Athem%20from%20two%20complementary%20perspectives%3A%20%28i%29%20architectural%2C%20dividing%20methods%0Ainto%20VAE-based%2C%20diffusion-based%2C%20and%20hybrid%20models%3B%20and%20%28ii%29%20motion%0Arepresentation%2C%20distinguishing%20between%20discrete%20and%20continuous%20motion%0Ageneration%20strategies.%20In%20addition%2C%20we%20explore%20the%20most%20widely%20used%20datasets%2C%0Aevaluation%20methods%2C%20and%20recent%20benchmarks%20that%20have%20shaped%20progress%20in%20this%0Aarea.%20With%20this%20survey%2C%20we%20aim%20to%20capture%20where%20the%20field%20currently%20stands%2C%0Abring%20attention%20to%20its%20key%20challenges%20and%20limitations%2C%20and%20highlight%20promising%0Adirections%20for%20future%20exploration.%20We%20hope%20this%20work%20offers%20a%20valuable%20starting%0Apoint%20for%20researchers%20and%20practitioners%20working%20to%20push%20the%20boundaries%20of%0Alanguage-driven%20human%20motion%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09379v1&entry.124074799=Read"},
{"title": "Towards Fair In-Context Learning with Tabular Foundation Models", "author": "Patrik Kenfack and Samira Ebrahimi Kaho and Ulrich A\u00efvodji", "abstract": "  Tabular foundational models have exhibited strong in-context learning (ICL)\ncapabilities on structured data, allowing them to make accurate predictions on\ntest sets without parameter updates, using training examples as context. This\nemerging approach positions itself as a competitive alternative to traditional\ngradient-boosted tree methods. However, while biases in conventional machine\nlearning models are well documented, it remains unclear how these biases\nmanifest in tabular ICL. The paper investigates the fairness implications of\ntabular ICL and explores three preprocessing strategies--correlation removal,\ngroup-balanced demonstration selection, and uncertainty-based demonstration\nselection--to address bias. Comprehensive experiments indicate that\nuncertainty-based demonstration selection consistently enhances group fairness\nof in-context predictions. The source code for reproducing the results of this\nwork can be found at https://github.com/patrikken/Fair-TabICL.\n", "link": "http://arxiv.org/abs/2505.09503v1", "date": "2025-05-14", "relevancy": 2.4278, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4919}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4887}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Fair%20In-Context%20Learning%20with%20Tabular%20Foundation%20Models&body=Title%3A%20Towards%20Fair%20In-Context%20Learning%20with%20Tabular%20Foundation%20Models%0AAuthor%3A%20Patrik%20Kenfack%20and%20Samira%20Ebrahimi%20Kaho%20and%20Ulrich%20A%C3%AFvodji%0AAbstract%3A%20%20%20Tabular%20foundational%20models%20have%20exhibited%20strong%20in-context%20learning%20%28ICL%29%0Acapabilities%20on%20structured%20data%2C%20allowing%20them%20to%20make%20accurate%20predictions%20on%0Atest%20sets%20without%20parameter%20updates%2C%20using%20training%20examples%20as%20context.%20This%0Aemerging%20approach%20positions%20itself%20as%20a%20competitive%20alternative%20to%20traditional%0Agradient-boosted%20tree%20methods.%20However%2C%20while%20biases%20in%20conventional%20machine%0Alearning%20models%20are%20well%20documented%2C%20it%20remains%20unclear%20how%20these%20biases%0Amanifest%20in%20tabular%20ICL.%20The%20paper%20investigates%20the%20fairness%20implications%20of%0Atabular%20ICL%20and%20explores%20three%20preprocessing%20strategies--correlation%20removal%2C%0Agroup-balanced%20demonstration%20selection%2C%20and%20uncertainty-based%20demonstration%0Aselection--to%20address%20bias.%20Comprehensive%20experiments%20indicate%20that%0Auncertainty-based%20demonstration%20selection%20consistently%20enhances%20group%20fairness%0Aof%20in-context%20predictions.%20The%20source%20code%20for%20reproducing%20the%20results%20of%20this%0Awork%20can%20be%20found%20at%20https%3A//github.com/patrikken/Fair-TabICL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Fair%2520In-Context%2520Learning%2520with%2520Tabular%2520Foundation%2520Models%26entry.906535625%3DPatrik%2520Kenfack%2520and%2520Samira%2520Ebrahimi%2520Kaho%2520and%2520Ulrich%2520A%25C3%25AFvodji%26entry.1292438233%3D%2520%2520Tabular%2520foundational%2520models%2520have%2520exhibited%2520strong%2520in-context%2520learning%2520%2528ICL%2529%250Acapabilities%2520on%2520structured%2520data%252C%2520allowing%2520them%2520to%2520make%2520accurate%2520predictions%2520on%250Atest%2520sets%2520without%2520parameter%2520updates%252C%2520using%2520training%2520examples%2520as%2520context.%2520This%250Aemerging%2520approach%2520positions%2520itself%2520as%2520a%2520competitive%2520alternative%2520to%2520traditional%250Agradient-boosted%2520tree%2520methods.%2520However%252C%2520while%2520biases%2520in%2520conventional%2520machine%250Alearning%2520models%2520are%2520well%2520documented%252C%2520it%2520remains%2520unclear%2520how%2520these%2520biases%250Amanifest%2520in%2520tabular%2520ICL.%2520The%2520paper%2520investigates%2520the%2520fairness%2520implications%2520of%250Atabular%2520ICL%2520and%2520explores%2520three%2520preprocessing%2520strategies--correlation%2520removal%252C%250Agroup-balanced%2520demonstration%2520selection%252C%2520and%2520uncertainty-based%2520demonstration%250Aselection--to%2520address%2520bias.%2520Comprehensive%2520experiments%2520indicate%2520that%250Auncertainty-based%2520demonstration%2520selection%2520consistently%2520enhances%2520group%2520fairness%250Aof%2520in-context%2520predictions.%2520The%2520source%2520code%2520for%2520reproducing%2520the%2520results%2520of%2520this%250Awork%2520can%2520be%2520found%2520at%2520https%253A//github.com/patrikken/Fair-TabICL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Fair%20In-Context%20Learning%20with%20Tabular%20Foundation%20Models&entry.906535625=Patrik%20Kenfack%20and%20Samira%20Ebrahimi%20Kaho%20and%20Ulrich%20A%C3%AFvodji&entry.1292438233=%20%20Tabular%20foundational%20models%20have%20exhibited%20strong%20in-context%20learning%20%28ICL%29%0Acapabilities%20on%20structured%20data%2C%20allowing%20them%20to%20make%20accurate%20predictions%20on%0Atest%20sets%20without%20parameter%20updates%2C%20using%20training%20examples%20as%20context.%20This%0Aemerging%20approach%20positions%20itself%20as%20a%20competitive%20alternative%20to%20traditional%0Agradient-boosted%20tree%20methods.%20However%2C%20while%20biases%20in%20conventional%20machine%0Alearning%20models%20are%20well%20documented%2C%20it%20remains%20unclear%20how%20these%20biases%0Amanifest%20in%20tabular%20ICL.%20The%20paper%20investigates%20the%20fairness%20implications%20of%0Atabular%20ICL%20and%20explores%20three%20preprocessing%20strategies--correlation%20removal%2C%0Agroup-balanced%20demonstration%20selection%2C%20and%20uncertainty-based%20demonstration%0Aselection--to%20address%20bias.%20Comprehensive%20experiments%20indicate%20that%0Auncertainty-based%20demonstration%20selection%20consistently%20enhances%20group%20fairness%0Aof%20in-context%20predictions.%20The%20source%20code%20for%20reproducing%20the%20results%20of%20this%0Awork%20can%20be%20found%20at%20https%3A//github.com/patrikken/Fair-TabICL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09503v1&entry.124074799=Read"},
{"title": "UMotion: Uncertainty-driven Human Motion Estimation from Inertial and\n  Ultra-wideband Units", "author": "Huakun Liu and Hiroki Ota and Xin Wei and Yutaro Hirao and Monica Perusquia-Hernandez and Hideaki Uchiyama and Kiyoshi Kiyokawa", "abstract": "  Sparse wearable inertial measurement units (IMUs) have gained popularity for\nestimating 3D human motion. However, challenges such as pose ambiguity, data\ndrift, and limited adaptability to diverse bodies persist. To address these\nissues, we propose UMotion, an uncertainty-driven, online fusing-all state\nestimation framework for 3D human shape and pose estimation, supported by six\nintegrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB\nsensors measure inter-node distances to infer spatial relationships, aiding in\nresolving pose ambiguities and body shape variations when combined with\nanthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors\nare affected by body occlusions. Consequently, we develop a tightly coupled\nUnscented Kalman Filter (UKF) framework that fuses uncertainties from sensor\ndata and estimated human motion based on individual body shape. The UKF\niteratively refines IMU and UWB measurements by aligning them with uncertain\nhuman motion constraints in real-time, producing optimal estimates for each.\nExperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of UMotion in stabilizing sensor data and the improvement over\nstate of the art in pose accuracy.\n", "link": "http://arxiv.org/abs/2505.09393v1", "date": "2025-05-14", "relevancy": 2.4202, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6113}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6007}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UMotion%3A%20Uncertainty-driven%20Human%20Motion%20Estimation%20from%20Inertial%20and%0A%20%20Ultra-wideband%20Units&body=Title%3A%20UMotion%3A%20Uncertainty-driven%20Human%20Motion%20Estimation%20from%20Inertial%20and%0A%20%20Ultra-wideband%20Units%0AAuthor%3A%20Huakun%20Liu%20and%20Hiroki%20Ota%20and%20Xin%20Wei%20and%20Yutaro%20Hirao%20and%20Monica%20Perusquia-Hernandez%20and%20Hideaki%20Uchiyama%20and%20Kiyoshi%20Kiyokawa%0AAbstract%3A%20%20%20Sparse%20wearable%20inertial%20measurement%20units%20%28IMUs%29%20have%20gained%20popularity%20for%0Aestimating%203D%20human%20motion.%20However%2C%20challenges%20such%20as%20pose%20ambiguity%2C%20data%0Adrift%2C%20and%20limited%20adaptability%20to%20diverse%20bodies%20persist.%20To%20address%20these%0Aissues%2C%20we%20propose%20UMotion%2C%20an%20uncertainty-driven%2C%20online%20fusing-all%20state%0Aestimation%20framework%20for%203D%20human%20shape%20and%20pose%20estimation%2C%20supported%20by%20six%0Aintegrated%2C%20body-worn%20ultra-wideband%20%28UWB%29%20distance%20sensors%20with%20IMUs.%20UWB%0Asensors%20measure%20inter-node%20distances%20to%20infer%20spatial%20relationships%2C%20aiding%20in%0Aresolving%20pose%20ambiguities%20and%20body%20shape%20variations%20when%20combined%20with%0Aanthropometric%20data.%20Unfortunately%2C%20IMUs%20are%20prone%20to%20drift%2C%20and%20UWB%20sensors%0Aare%20affected%20by%20body%20occlusions.%20Consequently%2C%20we%20develop%20a%20tightly%20coupled%0AUnscented%20Kalman%20Filter%20%28UKF%29%20framework%20that%20fuses%20uncertainties%20from%20sensor%0Adata%20and%20estimated%20human%20motion%20based%20on%20individual%20body%20shape.%20The%20UKF%0Aiteratively%20refines%20IMU%20and%20UWB%20measurements%20by%20aligning%20them%20with%20uncertain%0Ahuman%20motion%20constraints%20in%20real-time%2C%20producing%20optimal%20estimates%20for%20each.%0AExperiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20UMotion%20in%20stabilizing%20sensor%20data%20and%20the%20improvement%20over%0Astate%20of%20the%20art%20in%20pose%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUMotion%253A%2520Uncertainty-driven%2520Human%2520Motion%2520Estimation%2520from%2520Inertial%2520and%250A%2520%2520Ultra-wideband%2520Units%26entry.906535625%3DHuakun%2520Liu%2520and%2520Hiroki%2520Ota%2520and%2520Xin%2520Wei%2520and%2520Yutaro%2520Hirao%2520and%2520Monica%2520Perusquia-Hernandez%2520and%2520Hideaki%2520Uchiyama%2520and%2520Kiyoshi%2520Kiyokawa%26entry.1292438233%3D%2520%2520Sparse%2520wearable%2520inertial%2520measurement%2520units%2520%2528IMUs%2529%2520have%2520gained%2520popularity%2520for%250Aestimating%25203D%2520human%2520motion.%2520However%252C%2520challenges%2520such%2520as%2520pose%2520ambiguity%252C%2520data%250Adrift%252C%2520and%2520limited%2520adaptability%2520to%2520diverse%2520bodies%2520persist.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520UMotion%252C%2520an%2520uncertainty-driven%252C%2520online%2520fusing-all%2520state%250Aestimation%2520framework%2520for%25203D%2520human%2520shape%2520and%2520pose%2520estimation%252C%2520supported%2520by%2520six%250Aintegrated%252C%2520body-worn%2520ultra-wideband%2520%2528UWB%2529%2520distance%2520sensors%2520with%2520IMUs.%2520UWB%250Asensors%2520measure%2520inter-node%2520distances%2520to%2520infer%2520spatial%2520relationships%252C%2520aiding%2520in%250Aresolving%2520pose%2520ambiguities%2520and%2520body%2520shape%2520variations%2520when%2520combined%2520with%250Aanthropometric%2520data.%2520Unfortunately%252C%2520IMUs%2520are%2520prone%2520to%2520drift%252C%2520and%2520UWB%2520sensors%250Aare%2520affected%2520by%2520body%2520occlusions.%2520Consequently%252C%2520we%2520develop%2520a%2520tightly%2520coupled%250AUnscented%2520Kalman%2520Filter%2520%2528UKF%2529%2520framework%2520that%2520fuses%2520uncertainties%2520from%2520sensor%250Adata%2520and%2520estimated%2520human%2520motion%2520based%2520on%2520individual%2520body%2520shape.%2520The%2520UKF%250Aiteratively%2520refines%2520IMU%2520and%2520UWB%2520measurements%2520by%2520aligning%2520them%2520with%2520uncertain%250Ahuman%2520motion%2520constraints%2520in%2520real-time%252C%2520producing%2520optimal%2520estimates%2520for%2520each.%250AExperiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520of%2520UMotion%2520in%2520stabilizing%2520sensor%2520data%2520and%2520the%2520improvement%2520over%250Astate%2520of%2520the%2520art%2520in%2520pose%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UMotion%3A%20Uncertainty-driven%20Human%20Motion%20Estimation%20from%20Inertial%20and%0A%20%20Ultra-wideband%20Units&entry.906535625=Huakun%20Liu%20and%20Hiroki%20Ota%20and%20Xin%20Wei%20and%20Yutaro%20Hirao%20and%20Monica%20Perusquia-Hernandez%20and%20Hideaki%20Uchiyama%20and%20Kiyoshi%20Kiyokawa&entry.1292438233=%20%20Sparse%20wearable%20inertial%20measurement%20units%20%28IMUs%29%20have%20gained%20popularity%20for%0Aestimating%203D%20human%20motion.%20However%2C%20challenges%20such%20as%20pose%20ambiguity%2C%20data%0Adrift%2C%20and%20limited%20adaptability%20to%20diverse%20bodies%20persist.%20To%20address%20these%0Aissues%2C%20we%20propose%20UMotion%2C%20an%20uncertainty-driven%2C%20online%20fusing-all%20state%0Aestimation%20framework%20for%203D%20human%20shape%20and%20pose%20estimation%2C%20supported%20by%20six%0Aintegrated%2C%20body-worn%20ultra-wideband%20%28UWB%29%20distance%20sensors%20with%20IMUs.%20UWB%0Asensors%20measure%20inter-node%20distances%20to%20infer%20spatial%20relationships%2C%20aiding%20in%0Aresolving%20pose%20ambiguities%20and%20body%20shape%20variations%20when%20combined%20with%0Aanthropometric%20data.%20Unfortunately%2C%20IMUs%20are%20prone%20to%20drift%2C%20and%20UWB%20sensors%0Aare%20affected%20by%20body%20occlusions.%20Consequently%2C%20we%20develop%20a%20tightly%20coupled%0AUnscented%20Kalman%20Filter%20%28UKF%29%20framework%20that%20fuses%20uncertainties%20from%20sensor%0Adata%20and%20estimated%20human%20motion%20based%20on%20individual%20body%20shape.%20The%20UKF%0Aiteratively%20refines%20IMU%20and%20UWB%20measurements%20by%20aligning%20them%20with%20uncertain%0Ahuman%20motion%20constraints%20in%20real-time%2C%20producing%20optimal%20estimates%20for%20each.%0AExperiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20UMotion%20in%20stabilizing%20sensor%20data%20and%20the%20improvement%20over%0Astate%20of%20the%20art%20in%20pose%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09393v1&entry.124074799=Read"},
{"title": "Design of a Formation Control System to Assist Human Operators in Flying\n  a Swarm of Robotic Blimps", "author": "Tianfu Wu and Jiaqi Fu and Wugang Meng and Sungjin Cho and Huanzhe Zhan and Fumin Zhang", "abstract": "  Formation control is essential for swarm robotics, enabling coordinated\nbehavior in complex environments. In this paper, we introduce a novel formation\ncontrol system for an indoor blimp swarm using a specialized leader-follower\napproach enhanced with a dynamic leader-switching mechanism. This strategy\nallows any blimp to take on the leader role, distributing maneuvering demands\nacross the swarm and enhancing overall formation stability. Only the leader\nblimp is manually controlled by a human operator, while follower blimps use\nonboard monocular cameras and a laser altimeter for relative position and\naltitude estimation. A leader-switching scheme is proposed to assist the human\noperator to maintain stability of the swarm, especially when a sharp turn is\nperformed. Experimental results confirm that the leader-switching mechanism\neffectively maintains stable formations and adapts to dynamic indoor\nenvironments while assisting human operator.\n", "link": "http://arxiv.org/abs/2505.09511v1", "date": "2025-05-14", "relevancy": 2.4097, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5099}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4687}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Design%20of%20a%20Formation%20Control%20System%20to%20Assist%20Human%20Operators%20in%20Flying%0A%20%20a%20Swarm%20of%20Robotic%20Blimps&body=Title%3A%20Design%20of%20a%20Formation%20Control%20System%20to%20Assist%20Human%20Operators%20in%20Flying%0A%20%20a%20Swarm%20of%20Robotic%20Blimps%0AAuthor%3A%20Tianfu%20Wu%20and%20Jiaqi%20Fu%20and%20Wugang%20Meng%20and%20Sungjin%20Cho%20and%20Huanzhe%20Zhan%20and%20Fumin%20Zhang%0AAbstract%3A%20%20%20Formation%20control%20is%20essential%20for%20swarm%20robotics%2C%20enabling%20coordinated%0Abehavior%20in%20complex%20environments.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20formation%0Acontrol%20system%20for%20an%20indoor%20blimp%20swarm%20using%20a%20specialized%20leader-follower%0Aapproach%20enhanced%20with%20a%20dynamic%20leader-switching%20mechanism.%20This%20strategy%0Aallows%20any%20blimp%20to%20take%20on%20the%20leader%20role%2C%20distributing%20maneuvering%20demands%0Aacross%20the%20swarm%20and%20enhancing%20overall%20formation%20stability.%20Only%20the%20leader%0Ablimp%20is%20manually%20controlled%20by%20a%20human%20operator%2C%20while%20follower%20blimps%20use%0Aonboard%20monocular%20cameras%20and%20a%20laser%20altimeter%20for%20relative%20position%20and%0Aaltitude%20estimation.%20A%20leader-switching%20scheme%20is%20proposed%20to%20assist%20the%20human%0Aoperator%20to%20maintain%20stability%20of%20the%20swarm%2C%20especially%20when%20a%20sharp%20turn%20is%0Aperformed.%20Experimental%20results%20confirm%20that%20the%20leader-switching%20mechanism%0Aeffectively%20maintains%20stable%20formations%20and%20adapts%20to%20dynamic%20indoor%0Aenvironments%20while%20assisting%20human%20operator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesign%2520of%2520a%2520Formation%2520Control%2520System%2520to%2520Assist%2520Human%2520Operators%2520in%2520Flying%250A%2520%2520a%2520Swarm%2520of%2520Robotic%2520Blimps%26entry.906535625%3DTianfu%2520Wu%2520and%2520Jiaqi%2520Fu%2520and%2520Wugang%2520Meng%2520and%2520Sungjin%2520Cho%2520and%2520Huanzhe%2520Zhan%2520and%2520Fumin%2520Zhang%26entry.1292438233%3D%2520%2520Formation%2520control%2520is%2520essential%2520for%2520swarm%2520robotics%252C%2520enabling%2520coordinated%250Abehavior%2520in%2520complex%2520environments.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520formation%250Acontrol%2520system%2520for%2520an%2520indoor%2520blimp%2520swarm%2520using%2520a%2520specialized%2520leader-follower%250Aapproach%2520enhanced%2520with%2520a%2520dynamic%2520leader-switching%2520mechanism.%2520This%2520strategy%250Aallows%2520any%2520blimp%2520to%2520take%2520on%2520the%2520leader%2520role%252C%2520distributing%2520maneuvering%2520demands%250Aacross%2520the%2520swarm%2520and%2520enhancing%2520overall%2520formation%2520stability.%2520Only%2520the%2520leader%250Ablimp%2520is%2520manually%2520controlled%2520by%2520a%2520human%2520operator%252C%2520while%2520follower%2520blimps%2520use%250Aonboard%2520monocular%2520cameras%2520and%2520a%2520laser%2520altimeter%2520for%2520relative%2520position%2520and%250Aaltitude%2520estimation.%2520A%2520leader-switching%2520scheme%2520is%2520proposed%2520to%2520assist%2520the%2520human%250Aoperator%2520to%2520maintain%2520stability%2520of%2520the%2520swarm%252C%2520especially%2520when%2520a%2520sharp%2520turn%2520is%250Aperformed.%2520Experimental%2520results%2520confirm%2520that%2520the%2520leader-switching%2520mechanism%250Aeffectively%2520maintains%2520stable%2520formations%2520and%2520adapts%2520to%2520dynamic%2520indoor%250Aenvironments%2520while%2520assisting%2520human%2520operator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%20of%20a%20Formation%20Control%20System%20to%20Assist%20Human%20Operators%20in%20Flying%0A%20%20a%20Swarm%20of%20Robotic%20Blimps&entry.906535625=Tianfu%20Wu%20and%20Jiaqi%20Fu%20and%20Wugang%20Meng%20and%20Sungjin%20Cho%20and%20Huanzhe%20Zhan%20and%20Fumin%20Zhang&entry.1292438233=%20%20Formation%20control%20is%20essential%20for%20swarm%20robotics%2C%20enabling%20coordinated%0Abehavior%20in%20complex%20environments.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20formation%0Acontrol%20system%20for%20an%20indoor%20blimp%20swarm%20using%20a%20specialized%20leader-follower%0Aapproach%20enhanced%20with%20a%20dynamic%20leader-switching%20mechanism.%20This%20strategy%0Aallows%20any%20blimp%20to%20take%20on%20the%20leader%20role%2C%20distributing%20maneuvering%20demands%0Aacross%20the%20swarm%20and%20enhancing%20overall%20formation%20stability.%20Only%20the%20leader%0Ablimp%20is%20manually%20controlled%20by%20a%20human%20operator%2C%20while%20follower%20blimps%20use%0Aonboard%20monocular%20cameras%20and%20a%20laser%20altimeter%20for%20relative%20position%20and%0Aaltitude%20estimation.%20A%20leader-switching%20scheme%20is%20proposed%20to%20assist%20the%20human%0Aoperator%20to%20maintain%20stability%20of%20the%20swarm%2C%20especially%20when%20a%20sharp%20turn%20is%0Aperformed.%20Experimental%20results%20confirm%20that%20the%20leader-switching%20mechanism%0Aeffectively%20maintains%20stable%20formations%20and%20adapts%20to%20dynamic%20indoor%0Aenvironments%20while%20assisting%20human%20operator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09511v1&entry.124074799=Read"},
{"title": "FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question\n  Answering", "author": "Siqiao Xue and Xiaojing Li and Fan Zhou and Qingyang Dai and Zhixuan Chu and Hongyuan Mei", "abstract": "  In this paper, we introduce FAMMA, an open-source benchmark for\n\\underline{f}in\\underline{a}ncial \\underline{m}ultilingual\n\\underline{m}ultimodal question \\underline{a}nswering (QA). Our benchmark aims\nto evaluate the abilities of large language models (LLMs) in answering complex\nreasoning questions that require advanced financial knowledge. The benchmark\nhas two versions: FAMMA-Basic consists of 1,945 questions extracted from\nuniversity textbooks and exams, along with human-annotated answers and\nrationales; FAMMA-LivePro consists of 103 novel questions created by human\ndomain experts, with answers and rationales held out from the public for a\ncontamination-free evaluation. These questions cover advanced knowledge of 8\nmajor subfields in finance (e.g., corporate finance, derivatives, and portfolio\nmanagement). Some are in Chinese or French, while a majority of them are in\nEnglish. Each question has some non-text data such as charts, diagrams, or\ntables. Our experiments reveal that FAMMA poses a significant challenge on\nLLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally,\nwe curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data,\nand fine-tuned a series of open-source Qwen models using this reasoning data.\nWe found that training a model on these reasoning trajectories can\nsignificantly improve its performance on FAMMA-LivePro. We released our\nleaderboard, data, code, and trained models at\nhttps://famma-bench.github.io/famma/.\n", "link": "http://arxiv.org/abs/2410.04526v3", "date": "2025-05-14", "relevancy": 2.395, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4915}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4915}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAMMA%3A%20A%20Benchmark%20for%20Financial%20Domain%20Multilingual%20Multimodal%20Question%0A%20%20Answering&body=Title%3A%20FAMMA%3A%20A%20Benchmark%20for%20Financial%20Domain%20Multilingual%20Multimodal%20Question%0A%20%20Answering%0AAuthor%3A%20Siqiao%20Xue%20and%20Xiaojing%20Li%20and%20Fan%20Zhou%20and%20Qingyang%20Dai%20and%20Zhixuan%20Chu%20and%20Hongyuan%20Mei%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20FAMMA%2C%20an%20open-source%20benchmark%20for%0A%5Cunderline%7Bf%7Din%5Cunderline%7Ba%7Dncial%20%5Cunderline%7Bm%7Dultilingual%0A%5Cunderline%7Bm%7Dultimodal%20question%20%5Cunderline%7Ba%7Dnswering%20%28QA%29.%20Our%20benchmark%20aims%0Ato%20evaluate%20the%20abilities%20of%20large%20language%20models%20%28LLMs%29%20in%20answering%20complex%0Areasoning%20questions%20that%20require%20advanced%20financial%20knowledge.%20The%20benchmark%0Ahas%20two%20versions%3A%20FAMMA-Basic%20consists%20of%201%2C945%20questions%20extracted%20from%0Auniversity%20textbooks%20and%20exams%2C%20along%20with%20human-annotated%20answers%20and%0Arationales%3B%20FAMMA-LivePro%20consists%20of%20103%20novel%20questions%20created%20by%20human%0Adomain%20experts%2C%20with%20answers%20and%20rationales%20held%20out%20from%20the%20public%20for%20a%0Acontamination-free%20evaluation.%20These%20questions%20cover%20advanced%20knowledge%20of%208%0Amajor%20subfields%20in%20finance%20%28e.g.%2C%20corporate%20finance%2C%20derivatives%2C%20and%20portfolio%0Amanagement%29.%20Some%20are%20in%20Chinese%20or%20French%2C%20while%20a%20majority%20of%20them%20are%20in%0AEnglish.%20Each%20question%20has%20some%20non-text%20data%20such%20as%20charts%2C%20diagrams%2C%20or%0Atables.%20Our%20experiments%20reveal%20that%20FAMMA%20poses%20a%20significant%20challenge%20on%0ALLMs%2C%20including%20reasoning%20models%20such%20as%20GPT-o1%20and%20DeepSeek-R1.%20Additionally%2C%0Awe%20curated%201%2C270%20reasoning%20trajectories%20of%20DeepSeek-R1%20on%20the%20FAMMA-Basic%20data%2C%0Aand%20fine-tuned%20a%20series%20of%20open-source%20Qwen%20models%20using%20this%20reasoning%20data.%0AWe%20found%20that%20training%20a%20model%20on%20these%20reasoning%20trajectories%20can%0Asignificantly%20improve%20its%20performance%20on%20FAMMA-LivePro.%20We%20released%20our%0Aleaderboard%2C%20data%2C%20code%2C%20and%20trained%20models%20at%0Ahttps%3A//famma-bench.github.io/famma/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04526v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAMMA%253A%2520A%2520Benchmark%2520for%2520Financial%2520Domain%2520Multilingual%2520Multimodal%2520Question%250A%2520%2520Answering%26entry.906535625%3DSiqiao%2520Xue%2520and%2520Xiaojing%2520Li%2520and%2520Fan%2520Zhou%2520and%2520Qingyang%2520Dai%2520and%2520Zhixuan%2520Chu%2520and%2520Hongyuan%2520Mei%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520FAMMA%252C%2520an%2520open-source%2520benchmark%2520for%250A%255Cunderline%257Bf%257Din%255Cunderline%257Ba%257Dncial%2520%255Cunderline%257Bm%257Dultilingual%250A%255Cunderline%257Bm%257Dultimodal%2520question%2520%255Cunderline%257Ba%257Dnswering%2520%2528QA%2529.%2520Our%2520benchmark%2520aims%250Ato%2520evaluate%2520the%2520abilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520answering%2520complex%250Areasoning%2520questions%2520that%2520require%2520advanced%2520financial%2520knowledge.%2520The%2520benchmark%250Ahas%2520two%2520versions%253A%2520FAMMA-Basic%2520consists%2520of%25201%252C945%2520questions%2520extracted%2520from%250Auniversity%2520textbooks%2520and%2520exams%252C%2520along%2520with%2520human-annotated%2520answers%2520and%250Arationales%253B%2520FAMMA-LivePro%2520consists%2520of%2520103%2520novel%2520questions%2520created%2520by%2520human%250Adomain%2520experts%252C%2520with%2520answers%2520and%2520rationales%2520held%2520out%2520from%2520the%2520public%2520for%2520a%250Acontamination-free%2520evaluation.%2520These%2520questions%2520cover%2520advanced%2520knowledge%2520of%25208%250Amajor%2520subfields%2520in%2520finance%2520%2528e.g.%252C%2520corporate%2520finance%252C%2520derivatives%252C%2520and%2520portfolio%250Amanagement%2529.%2520Some%2520are%2520in%2520Chinese%2520or%2520French%252C%2520while%2520a%2520majority%2520of%2520them%2520are%2520in%250AEnglish.%2520Each%2520question%2520has%2520some%2520non-text%2520data%2520such%2520as%2520charts%252C%2520diagrams%252C%2520or%250Atables.%2520Our%2520experiments%2520reveal%2520that%2520FAMMA%2520poses%2520a%2520significant%2520challenge%2520on%250ALLMs%252C%2520including%2520reasoning%2520models%2520such%2520as%2520GPT-o1%2520and%2520DeepSeek-R1.%2520Additionally%252C%250Awe%2520curated%25201%252C270%2520reasoning%2520trajectories%2520of%2520DeepSeek-R1%2520on%2520the%2520FAMMA-Basic%2520data%252C%250Aand%2520fine-tuned%2520a%2520series%2520of%2520open-source%2520Qwen%2520models%2520using%2520this%2520reasoning%2520data.%250AWe%2520found%2520that%2520training%2520a%2520model%2520on%2520these%2520reasoning%2520trajectories%2520can%250Asignificantly%2520improve%2520its%2520performance%2520on%2520FAMMA-LivePro.%2520We%2520released%2520our%250Aleaderboard%252C%2520data%252C%2520code%252C%2520and%2520trained%2520models%2520at%250Ahttps%253A//famma-bench.github.io/famma/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04526v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAMMA%3A%20A%20Benchmark%20for%20Financial%20Domain%20Multilingual%20Multimodal%20Question%0A%20%20Answering&entry.906535625=Siqiao%20Xue%20and%20Xiaojing%20Li%20and%20Fan%20Zhou%20and%20Qingyang%20Dai%20and%20Zhixuan%20Chu%20and%20Hongyuan%20Mei&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20FAMMA%2C%20an%20open-source%20benchmark%20for%0A%5Cunderline%7Bf%7Din%5Cunderline%7Ba%7Dncial%20%5Cunderline%7Bm%7Dultilingual%0A%5Cunderline%7Bm%7Dultimodal%20question%20%5Cunderline%7Ba%7Dnswering%20%28QA%29.%20Our%20benchmark%20aims%0Ato%20evaluate%20the%20abilities%20of%20large%20language%20models%20%28LLMs%29%20in%20answering%20complex%0Areasoning%20questions%20that%20require%20advanced%20financial%20knowledge.%20The%20benchmark%0Ahas%20two%20versions%3A%20FAMMA-Basic%20consists%20of%201%2C945%20questions%20extracted%20from%0Auniversity%20textbooks%20and%20exams%2C%20along%20with%20human-annotated%20answers%20and%0Arationales%3B%20FAMMA-LivePro%20consists%20of%20103%20novel%20questions%20created%20by%20human%0Adomain%20experts%2C%20with%20answers%20and%20rationales%20held%20out%20from%20the%20public%20for%20a%0Acontamination-free%20evaluation.%20These%20questions%20cover%20advanced%20knowledge%20of%208%0Amajor%20subfields%20in%20finance%20%28e.g.%2C%20corporate%20finance%2C%20derivatives%2C%20and%20portfolio%0Amanagement%29.%20Some%20are%20in%20Chinese%20or%20French%2C%20while%20a%20majority%20of%20them%20are%20in%0AEnglish.%20Each%20question%20has%20some%20non-text%20data%20such%20as%20charts%2C%20diagrams%2C%20or%0Atables.%20Our%20experiments%20reveal%20that%20FAMMA%20poses%20a%20significant%20challenge%20on%0ALLMs%2C%20including%20reasoning%20models%20such%20as%20GPT-o1%20and%20DeepSeek-R1.%20Additionally%2C%0Awe%20curated%201%2C270%20reasoning%20trajectories%20of%20DeepSeek-R1%20on%20the%20FAMMA-Basic%20data%2C%0Aand%20fine-tuned%20a%20series%20of%20open-source%20Qwen%20models%20using%20this%20reasoning%20data.%0AWe%20found%20that%20training%20a%20model%20on%20these%20reasoning%20trajectories%20can%0Asignificantly%20improve%20its%20performance%20on%20FAMMA-LivePro.%20We%20released%20our%0Aleaderboard%2C%20data%2C%20code%2C%20and%20trained%20models%20at%0Ahttps%3A//famma-bench.github.io/famma/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04526v3&entry.124074799=Read"},
{"title": "FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous\n  Driving via Point-Level Dynamic-Static Decoupling", "author": "Yue Wen and Liang Song and Yijia Liu and Siting Zhu and Yanzi Miao and Lijun Han and Hesheng Wang", "abstract": "  Dynamic scene reconstruction for autonomous driving enables vehicles to\nperceive and interpret complex scene changes more precisely. Dynamic Neural\nRadiance Fields (NeRFs) have recently shown promising capability in scene\nmodeling. However, many existing methods rely heavily on accurate poses inputs\nand multi-sensor data, leading to increased system complexity. To address this,\nwe propose FreeDriveRF, which reconstructs dynamic driving scenes using only\nsequential RGB images without requiring poses inputs. We innovatively decouple\ndynamic and static parts at the early sampling level using semantic\nsupervision, mitigating image blurring and artifacts. To overcome the\nchallenges posed by object motion and occlusion in monocular camera, we\nintroduce a warped ray-guided dynamic object rendering consistency loss,\nutilizing optical flow to better constrain the dynamic modeling process.\nAdditionally, we incorporate estimated dynamic flow to constrain the pose\noptimization process, improving the stability and accuracy of unbounded scene\nreconstruction. Extensive experiments conducted on the KITTI and Waymo datasets\ndemonstrate the superior performance of our method in dynamic scene modeling\nfor autonomous driving.\n", "link": "http://arxiv.org/abs/2505.09406v1", "date": "2025-05-14", "relevancy": 2.3888, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6307}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5925}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeDriveRF%3A%20Monocular%20RGB%20Dynamic%20NeRF%20without%20Poses%20for%20Autonomous%0A%20%20Driving%20via%20Point-Level%20Dynamic-Static%20Decoupling&body=Title%3A%20FreeDriveRF%3A%20Monocular%20RGB%20Dynamic%20NeRF%20without%20Poses%20for%20Autonomous%0A%20%20Driving%20via%20Point-Level%20Dynamic-Static%20Decoupling%0AAuthor%3A%20Yue%20Wen%20and%20Liang%20Song%20and%20Yijia%20Liu%20and%20Siting%20Zhu%20and%20Yanzi%20Miao%20and%20Lijun%20Han%20and%20Hesheng%20Wang%0AAbstract%3A%20%20%20Dynamic%20scene%20reconstruction%20for%20autonomous%20driving%20enables%20vehicles%20to%0Aperceive%20and%20interpret%20complex%20scene%20changes%20more%20precisely.%20Dynamic%20Neural%0ARadiance%20Fields%20%28NeRFs%29%20have%20recently%20shown%20promising%20capability%20in%20scene%0Amodeling.%20However%2C%20many%20existing%20methods%20rely%20heavily%20on%20accurate%20poses%20inputs%0Aand%20multi-sensor%20data%2C%20leading%20to%20increased%20system%20complexity.%20To%20address%20this%2C%0Awe%20propose%20FreeDriveRF%2C%20which%20reconstructs%20dynamic%20driving%20scenes%20using%20only%0Asequential%20RGB%20images%20without%20requiring%20poses%20inputs.%20We%20innovatively%20decouple%0Adynamic%20and%20static%20parts%20at%20the%20early%20sampling%20level%20using%20semantic%0Asupervision%2C%20mitigating%20image%20blurring%20and%20artifacts.%20To%20overcome%20the%0Achallenges%20posed%20by%20object%20motion%20and%20occlusion%20in%20monocular%20camera%2C%20we%0Aintroduce%20a%20warped%20ray-guided%20dynamic%20object%20rendering%20consistency%20loss%2C%0Autilizing%20optical%20flow%20to%20better%20constrain%20the%20dynamic%20modeling%20process.%0AAdditionally%2C%20we%20incorporate%20estimated%20dynamic%20flow%20to%20constrain%20the%20pose%0Aoptimization%20process%2C%20improving%20the%20stability%20and%20accuracy%20of%20unbounded%20scene%0Areconstruction.%20Extensive%20experiments%20conducted%20on%20the%20KITTI%20and%20Waymo%20datasets%0Ademonstrate%20the%20superior%20performance%20of%20our%20method%20in%20dynamic%20scene%20modeling%0Afor%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeDriveRF%253A%2520Monocular%2520RGB%2520Dynamic%2520NeRF%2520without%2520Poses%2520for%2520Autonomous%250A%2520%2520Driving%2520via%2520Point-Level%2520Dynamic-Static%2520Decoupling%26entry.906535625%3DYue%2520Wen%2520and%2520Liang%2520Song%2520and%2520Yijia%2520Liu%2520and%2520Siting%2520Zhu%2520and%2520Yanzi%2520Miao%2520and%2520Lijun%2520Han%2520and%2520Hesheng%2520Wang%26entry.1292438233%3D%2520%2520Dynamic%2520scene%2520reconstruction%2520for%2520autonomous%2520driving%2520enables%2520vehicles%2520to%250Aperceive%2520and%2520interpret%2520complex%2520scene%2520changes%2520more%2520precisely.%2520Dynamic%2520Neural%250ARadiance%2520Fields%2520%2528NeRFs%2529%2520have%2520recently%2520shown%2520promising%2520capability%2520in%2520scene%250Amodeling.%2520However%252C%2520many%2520existing%2520methods%2520rely%2520heavily%2520on%2520accurate%2520poses%2520inputs%250Aand%2520multi-sensor%2520data%252C%2520leading%2520to%2520increased%2520system%2520complexity.%2520To%2520address%2520this%252C%250Awe%2520propose%2520FreeDriveRF%252C%2520which%2520reconstructs%2520dynamic%2520driving%2520scenes%2520using%2520only%250Asequential%2520RGB%2520images%2520without%2520requiring%2520poses%2520inputs.%2520We%2520innovatively%2520decouple%250Adynamic%2520and%2520static%2520parts%2520at%2520the%2520early%2520sampling%2520level%2520using%2520semantic%250Asupervision%252C%2520mitigating%2520image%2520blurring%2520and%2520artifacts.%2520To%2520overcome%2520the%250Achallenges%2520posed%2520by%2520object%2520motion%2520and%2520occlusion%2520in%2520monocular%2520camera%252C%2520we%250Aintroduce%2520a%2520warped%2520ray-guided%2520dynamic%2520object%2520rendering%2520consistency%2520loss%252C%250Autilizing%2520optical%2520flow%2520to%2520better%2520constrain%2520the%2520dynamic%2520modeling%2520process.%250AAdditionally%252C%2520we%2520incorporate%2520estimated%2520dynamic%2520flow%2520to%2520constrain%2520the%2520pose%250Aoptimization%2520process%252C%2520improving%2520the%2520stability%2520and%2520accuracy%2520of%2520unbounded%2520scene%250Areconstruction.%2520Extensive%2520experiments%2520conducted%2520on%2520the%2520KITTI%2520and%2520Waymo%2520datasets%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520our%2520method%2520in%2520dynamic%2520scene%2520modeling%250Afor%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeDriveRF%3A%20Monocular%20RGB%20Dynamic%20NeRF%20without%20Poses%20for%20Autonomous%0A%20%20Driving%20via%20Point-Level%20Dynamic-Static%20Decoupling&entry.906535625=Yue%20Wen%20and%20Liang%20Song%20and%20Yijia%20Liu%20and%20Siting%20Zhu%20and%20Yanzi%20Miao%20and%20Lijun%20Han%20and%20Hesheng%20Wang&entry.1292438233=%20%20Dynamic%20scene%20reconstruction%20for%20autonomous%20driving%20enables%20vehicles%20to%0Aperceive%20and%20interpret%20complex%20scene%20changes%20more%20precisely.%20Dynamic%20Neural%0ARadiance%20Fields%20%28NeRFs%29%20have%20recently%20shown%20promising%20capability%20in%20scene%0Amodeling.%20However%2C%20many%20existing%20methods%20rely%20heavily%20on%20accurate%20poses%20inputs%0Aand%20multi-sensor%20data%2C%20leading%20to%20increased%20system%20complexity.%20To%20address%20this%2C%0Awe%20propose%20FreeDriveRF%2C%20which%20reconstructs%20dynamic%20driving%20scenes%20using%20only%0Asequential%20RGB%20images%20without%20requiring%20poses%20inputs.%20We%20innovatively%20decouple%0Adynamic%20and%20static%20parts%20at%20the%20early%20sampling%20level%20using%20semantic%0Asupervision%2C%20mitigating%20image%20blurring%20and%20artifacts.%20To%20overcome%20the%0Achallenges%20posed%20by%20object%20motion%20and%20occlusion%20in%20monocular%20camera%2C%20we%0Aintroduce%20a%20warped%20ray-guided%20dynamic%20object%20rendering%20consistency%20loss%2C%0Autilizing%20optical%20flow%20to%20better%20constrain%20the%20dynamic%20modeling%20process.%0AAdditionally%2C%20we%20incorporate%20estimated%20dynamic%20flow%20to%20constrain%20the%20pose%0Aoptimization%20process%2C%20improving%20the%20stability%20and%20accuracy%20of%20unbounded%20scene%0Areconstruction.%20Extensive%20experiments%20conducted%20on%20the%20KITTI%20and%20Waymo%20datasets%0Ademonstrate%20the%20superior%20performance%20of%20our%20method%20in%20dynamic%20scene%20modeling%0Afor%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09406v1&entry.124074799=Read"},
{"title": "Evaluating GPT- and Reasoning-based Large Language Models on Physics\n  Olympiad Problems: Surpassing Human Performance and Implications for\n  Educational Assessment", "author": "Paul Tschisgale and Holger Maus and Fabian Kieser and Ben Kroehs and Stefan Petersen and Peter Wulff", "abstract": "  Large language models (LLMs) are now widely accessible, reaching learners at\nall educational levels. This development has raised concerns that their use may\ncircumvent essential learning processes and compromise the integrity of\nestablished assessment formats. In physics education, where problem solving\nplays a central role in instruction and assessment, it is therefore essential\nto understand the physics-specific problem-solving capabilities of LLMs. Such\nunderstanding is key to informing responsible and pedagogically sound\napproaches to integrating LLMs into instruction and assessment. This study\ntherefore compares the problem-solving performance of a general-purpose LLM\n(GPT-4o, using varying prompting techniques) and a reasoning-optimized model\n(o1-preview) with that of participants of the German Physics Olympiad, based on\na set of well-defined Olympiad problems. In addition to evaluating the\ncorrectness of the generated solutions, the study analyzes characteristic\nstrengths and limitations of LLM-generated solutions. The findings of this\nstudy indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate\nadvanced problem-solving capabilities on Olympiad-type physics problems, on\naverage outperforming the human participants. Prompting techniques had little\neffect on GPT-4o's performance, while o1-preview almost consistently\noutperformed both GPT-4o and the human benchmark. Based on these findings, the\nstudy discusses implications for the design of summative and formative\nassessment in physics education, including how to uphold assessment integrity\nand support students in critically engaging with LLMs.\n", "link": "http://arxiv.org/abs/2505.09438v1", "date": "2025-05-14", "relevancy": 2.3827, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.481}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20GPT-%20and%20Reasoning-based%20Large%20Language%20Models%20on%20Physics%0A%20%20Olympiad%20Problems%3A%20Surpassing%20Human%20Performance%20and%20Implications%20for%0A%20%20Educational%20Assessment&body=Title%3A%20Evaluating%20GPT-%20and%20Reasoning-based%20Large%20Language%20Models%20on%20Physics%0A%20%20Olympiad%20Problems%3A%20Surpassing%20Human%20Performance%20and%20Implications%20for%0A%20%20Educational%20Assessment%0AAuthor%3A%20Paul%20Tschisgale%20and%20Holger%20Maus%20and%20Fabian%20Kieser%20and%20Ben%20Kroehs%20and%20Stefan%20Petersen%20and%20Peter%20Wulff%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20now%20widely%20accessible%2C%20reaching%20learners%20at%0Aall%20educational%20levels.%20This%20development%20has%20raised%20concerns%20that%20their%20use%20may%0Acircumvent%20essential%20learning%20processes%20and%20compromise%20the%20integrity%20of%0Aestablished%20assessment%20formats.%20In%20physics%20education%2C%20where%20problem%20solving%0Aplays%20a%20central%20role%20in%20instruction%20and%20assessment%2C%20it%20is%20therefore%20essential%0Ato%20understand%20the%20physics-specific%20problem-solving%20capabilities%20of%20LLMs.%20Such%0Aunderstanding%20is%20key%20to%20informing%20responsible%20and%20pedagogically%20sound%0Aapproaches%20to%20integrating%20LLMs%20into%20instruction%20and%20assessment.%20This%20study%0Atherefore%20compares%20the%20problem-solving%20performance%20of%20a%20general-purpose%20LLM%0A%28GPT-4o%2C%20using%20varying%20prompting%20techniques%29%20and%20a%20reasoning-optimized%20model%0A%28o1-preview%29%20with%20that%20of%20participants%20of%20the%20German%20Physics%20Olympiad%2C%20based%20on%0Aa%20set%20of%20well-defined%20Olympiad%20problems.%20In%20addition%20to%20evaluating%20the%0Acorrectness%20of%20the%20generated%20solutions%2C%20the%20study%20analyzes%20characteristic%0Astrengths%20and%20limitations%20of%20LLM-generated%20solutions.%20The%20findings%20of%20this%0Astudy%20indicate%20that%20both%20tested%20LLMs%20%28GPT-4o%20and%20o1-preview%29%20demonstrate%0Aadvanced%20problem-solving%20capabilities%20on%20Olympiad-type%20physics%20problems%2C%20on%0Aaverage%20outperforming%20the%20human%20participants.%20Prompting%20techniques%20had%20little%0Aeffect%20on%20GPT-4o%27s%20performance%2C%20while%20o1-preview%20almost%20consistently%0Aoutperformed%20both%20GPT-4o%20and%20the%20human%20benchmark.%20Based%20on%20these%20findings%2C%20the%0Astudy%20discusses%20implications%20for%20the%20design%20of%20summative%20and%20formative%0Aassessment%20in%20physics%20education%2C%20including%20how%20to%20uphold%20assessment%20integrity%0Aand%20support%20students%20in%20critically%20engaging%20with%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520GPT-%2520and%2520Reasoning-based%2520Large%2520Language%2520Models%2520on%2520Physics%250A%2520%2520Olympiad%2520Problems%253A%2520Surpassing%2520Human%2520Performance%2520and%2520Implications%2520for%250A%2520%2520Educational%2520Assessment%26entry.906535625%3DPaul%2520Tschisgale%2520and%2520Holger%2520Maus%2520and%2520Fabian%2520Kieser%2520and%2520Ben%2520Kroehs%2520and%2520Stefan%2520Petersen%2520and%2520Peter%2520Wulff%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520now%2520widely%2520accessible%252C%2520reaching%2520learners%2520at%250Aall%2520educational%2520levels.%2520This%2520development%2520has%2520raised%2520concerns%2520that%2520their%2520use%2520may%250Acircumvent%2520essential%2520learning%2520processes%2520and%2520compromise%2520the%2520integrity%2520of%250Aestablished%2520assessment%2520formats.%2520In%2520physics%2520education%252C%2520where%2520problem%2520solving%250Aplays%2520a%2520central%2520role%2520in%2520instruction%2520and%2520assessment%252C%2520it%2520is%2520therefore%2520essential%250Ato%2520understand%2520the%2520physics-specific%2520problem-solving%2520capabilities%2520of%2520LLMs.%2520Such%250Aunderstanding%2520is%2520key%2520to%2520informing%2520responsible%2520and%2520pedagogically%2520sound%250Aapproaches%2520to%2520integrating%2520LLMs%2520into%2520instruction%2520and%2520assessment.%2520This%2520study%250Atherefore%2520compares%2520the%2520problem-solving%2520performance%2520of%2520a%2520general-purpose%2520LLM%250A%2528GPT-4o%252C%2520using%2520varying%2520prompting%2520techniques%2529%2520and%2520a%2520reasoning-optimized%2520model%250A%2528o1-preview%2529%2520with%2520that%2520of%2520participants%2520of%2520the%2520German%2520Physics%2520Olympiad%252C%2520based%2520on%250Aa%2520set%2520of%2520well-defined%2520Olympiad%2520problems.%2520In%2520addition%2520to%2520evaluating%2520the%250Acorrectness%2520of%2520the%2520generated%2520solutions%252C%2520the%2520study%2520analyzes%2520characteristic%250Astrengths%2520and%2520limitations%2520of%2520LLM-generated%2520solutions.%2520The%2520findings%2520of%2520this%250Astudy%2520indicate%2520that%2520both%2520tested%2520LLMs%2520%2528GPT-4o%2520and%2520o1-preview%2529%2520demonstrate%250Aadvanced%2520problem-solving%2520capabilities%2520on%2520Olympiad-type%2520physics%2520problems%252C%2520on%250Aaverage%2520outperforming%2520the%2520human%2520participants.%2520Prompting%2520techniques%2520had%2520little%250Aeffect%2520on%2520GPT-4o%2527s%2520performance%252C%2520while%2520o1-preview%2520almost%2520consistently%250Aoutperformed%2520both%2520GPT-4o%2520and%2520the%2520human%2520benchmark.%2520Based%2520on%2520these%2520findings%252C%2520the%250Astudy%2520discusses%2520implications%2520for%2520the%2520design%2520of%2520summative%2520and%2520formative%250Aassessment%2520in%2520physics%2520education%252C%2520including%2520how%2520to%2520uphold%2520assessment%2520integrity%250Aand%2520support%2520students%2520in%2520critically%2520engaging%2520with%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20GPT-%20and%20Reasoning-based%20Large%20Language%20Models%20on%20Physics%0A%20%20Olympiad%20Problems%3A%20Surpassing%20Human%20Performance%20and%20Implications%20for%0A%20%20Educational%20Assessment&entry.906535625=Paul%20Tschisgale%20and%20Holger%20Maus%20and%20Fabian%20Kieser%20and%20Ben%20Kroehs%20and%20Stefan%20Petersen%20and%20Peter%20Wulff&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20now%20widely%20accessible%2C%20reaching%20learners%20at%0Aall%20educational%20levels.%20This%20development%20has%20raised%20concerns%20that%20their%20use%20may%0Acircumvent%20essential%20learning%20processes%20and%20compromise%20the%20integrity%20of%0Aestablished%20assessment%20formats.%20In%20physics%20education%2C%20where%20problem%20solving%0Aplays%20a%20central%20role%20in%20instruction%20and%20assessment%2C%20it%20is%20therefore%20essential%0Ato%20understand%20the%20physics-specific%20problem-solving%20capabilities%20of%20LLMs.%20Such%0Aunderstanding%20is%20key%20to%20informing%20responsible%20and%20pedagogically%20sound%0Aapproaches%20to%20integrating%20LLMs%20into%20instruction%20and%20assessment.%20This%20study%0Atherefore%20compares%20the%20problem-solving%20performance%20of%20a%20general-purpose%20LLM%0A%28GPT-4o%2C%20using%20varying%20prompting%20techniques%29%20and%20a%20reasoning-optimized%20model%0A%28o1-preview%29%20with%20that%20of%20participants%20of%20the%20German%20Physics%20Olympiad%2C%20based%20on%0Aa%20set%20of%20well-defined%20Olympiad%20problems.%20In%20addition%20to%20evaluating%20the%0Acorrectness%20of%20the%20generated%20solutions%2C%20the%20study%20analyzes%20characteristic%0Astrengths%20and%20limitations%20of%20LLM-generated%20solutions.%20The%20findings%20of%20this%0Astudy%20indicate%20that%20both%20tested%20LLMs%20%28GPT-4o%20and%20o1-preview%29%20demonstrate%0Aadvanced%20problem-solving%20capabilities%20on%20Olympiad-type%20physics%20problems%2C%20on%0Aaverage%20outperforming%20the%20human%20participants.%20Prompting%20techniques%20had%20little%0Aeffect%20on%20GPT-4o%27s%20performance%2C%20while%20o1-preview%20almost%20consistently%0Aoutperformed%20both%20GPT-4o%20and%20the%20human%20benchmark.%20Based%20on%20these%20findings%2C%20the%0Astudy%20discusses%20implications%20for%20the%20design%20of%20summative%20and%20formative%0Aassessment%20in%20physics%20education%2C%20including%20how%20to%20uphold%20assessment%20integrity%0Aand%20support%20students%20in%20critically%20engaging%20with%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09438v1&entry.124074799=Read"},
{"title": "Don't Forget your Inverse DDIM for Image Editing", "author": "Guillermo Gomez-Trenado and Pablo Mesejo and Oscar Cord\u00f3n and St\u00e9phane Lathuili\u00e8re", "abstract": "  The field of text-to-image generation has undergone significant advancements\nwith the introduction of diffusion models. Nevertheless, the challenge of\nediting real images persists, as most methods are either computationally\nintensive or produce poor reconstructions. This paper introduces SAGE\n(Self-Attention Guidance for image Editing) - a novel technique leveraging\npre-trained diffusion models for image editing. SAGE builds upon the DDIM\nalgorithm and incorporates a novel guidance mechanism utilizing the\nself-attention layers of the diffusion U-Net. This mechanism computes a\nreconstruction objective based on attention maps generated during the inverse\nDDIM process, enabling efficient reconstruction of unedited regions without the\nneed to precisely reconstruct the entire input image. Thus, SAGE directly\naddresses the key challenges in image editing. The superiority of SAGE over\nother methods is demonstrated through quantitative and qualitative evaluations\nand confirmed by a statistically validated comprehensive user study, in which\nall 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE\nranks as the top-performing method in seven out of 10 quantitative analyses and\nsecures second and third places in the remaining three.\n", "link": "http://arxiv.org/abs/2505.09571v1", "date": "2025-05-14", "relevancy": 2.3816, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6247}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5908}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Forget%20your%20Inverse%20DDIM%20for%20Image%20Editing&body=Title%3A%20Don%27t%20Forget%20your%20Inverse%20DDIM%20for%20Image%20Editing%0AAuthor%3A%20Guillermo%20Gomez-Trenado%20and%20Pablo%20Mesejo%20and%20Oscar%20Cord%C3%B3n%20and%20St%C3%A9phane%20Lathuili%C3%A8re%0AAbstract%3A%20%20%20The%20field%20of%20text-to-image%20generation%20has%20undergone%20significant%20advancements%0Awith%20the%20introduction%20of%20diffusion%20models.%20Nevertheless%2C%20the%20challenge%20of%0Aediting%20real%20images%20persists%2C%20as%20most%20methods%20are%20either%20computationally%0Aintensive%20or%20produce%20poor%20reconstructions.%20This%20paper%20introduces%20SAGE%0A%28Self-Attention%20Guidance%20for%20image%20Editing%29%20-%20a%20novel%20technique%20leveraging%0Apre-trained%20diffusion%20models%20for%20image%20editing.%20SAGE%20builds%20upon%20the%20DDIM%0Aalgorithm%20and%20incorporates%20a%20novel%20guidance%20mechanism%20utilizing%20the%0Aself-attention%20layers%20of%20the%20diffusion%20U-Net.%20This%20mechanism%20computes%20a%0Areconstruction%20objective%20based%20on%20attention%20maps%20generated%20during%20the%20inverse%0ADDIM%20process%2C%20enabling%20efficient%20reconstruction%20of%20unedited%20regions%20without%20the%0Aneed%20to%20precisely%20reconstruct%20the%20entire%20input%20image.%20Thus%2C%20SAGE%20directly%0Aaddresses%20the%20key%20challenges%20in%20image%20editing.%20The%20superiority%20of%20SAGE%20over%0Aother%20methods%20is%20demonstrated%20through%20quantitative%20and%20qualitative%20evaluations%0Aand%20confirmed%20by%20a%20statistically%20validated%20comprehensive%20user%20study%2C%20in%20which%0Aall%2047%20surveyed%20users%20preferred%20SAGE%20over%20competing%20methods.%20Additionally%2C%20SAGE%0Aranks%20as%20the%20top-performing%20method%20in%20seven%20out%20of%2010%20quantitative%20analyses%20and%0Asecures%20second%20and%20third%20places%20in%20the%20remaining%20three.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Forget%2520your%2520Inverse%2520DDIM%2520for%2520Image%2520Editing%26entry.906535625%3DGuillermo%2520Gomez-Trenado%2520and%2520Pablo%2520Mesejo%2520and%2520Oscar%2520Cord%25C3%25B3n%2520and%2520St%25C3%25A9phane%2520Lathuili%25C3%25A8re%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520text-to-image%2520generation%2520has%2520undergone%2520significant%2520advancements%250Awith%2520the%2520introduction%2520of%2520diffusion%2520models.%2520Nevertheless%252C%2520the%2520challenge%2520of%250Aediting%2520real%2520images%2520persists%252C%2520as%2520most%2520methods%2520are%2520either%2520computationally%250Aintensive%2520or%2520produce%2520poor%2520reconstructions.%2520This%2520paper%2520introduces%2520SAGE%250A%2528Self-Attention%2520Guidance%2520for%2520image%2520Editing%2529%2520-%2520a%2520novel%2520technique%2520leveraging%250Apre-trained%2520diffusion%2520models%2520for%2520image%2520editing.%2520SAGE%2520builds%2520upon%2520the%2520DDIM%250Aalgorithm%2520and%2520incorporates%2520a%2520novel%2520guidance%2520mechanism%2520utilizing%2520the%250Aself-attention%2520layers%2520of%2520the%2520diffusion%2520U-Net.%2520This%2520mechanism%2520computes%2520a%250Areconstruction%2520objective%2520based%2520on%2520attention%2520maps%2520generated%2520during%2520the%2520inverse%250ADDIM%2520process%252C%2520enabling%2520efficient%2520reconstruction%2520of%2520unedited%2520regions%2520without%2520the%250Aneed%2520to%2520precisely%2520reconstruct%2520the%2520entire%2520input%2520image.%2520Thus%252C%2520SAGE%2520directly%250Aaddresses%2520the%2520key%2520challenges%2520in%2520image%2520editing.%2520The%2520superiority%2520of%2520SAGE%2520over%250Aother%2520methods%2520is%2520demonstrated%2520through%2520quantitative%2520and%2520qualitative%2520evaluations%250Aand%2520confirmed%2520by%2520a%2520statistically%2520validated%2520comprehensive%2520user%2520study%252C%2520in%2520which%250Aall%252047%2520surveyed%2520users%2520preferred%2520SAGE%2520over%2520competing%2520methods.%2520Additionally%252C%2520SAGE%250Aranks%2520as%2520the%2520top-performing%2520method%2520in%2520seven%2520out%2520of%252010%2520quantitative%2520analyses%2520and%250Asecures%2520second%2520and%2520third%2520places%2520in%2520the%2520remaining%2520three.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Forget%20your%20Inverse%20DDIM%20for%20Image%20Editing&entry.906535625=Guillermo%20Gomez-Trenado%20and%20Pablo%20Mesejo%20and%20Oscar%20Cord%C3%B3n%20and%20St%C3%A9phane%20Lathuili%C3%A8re&entry.1292438233=%20%20The%20field%20of%20text-to-image%20generation%20has%20undergone%20significant%20advancements%0Awith%20the%20introduction%20of%20diffusion%20models.%20Nevertheless%2C%20the%20challenge%20of%0Aediting%20real%20images%20persists%2C%20as%20most%20methods%20are%20either%20computationally%0Aintensive%20or%20produce%20poor%20reconstructions.%20This%20paper%20introduces%20SAGE%0A%28Self-Attention%20Guidance%20for%20image%20Editing%29%20-%20a%20novel%20technique%20leveraging%0Apre-trained%20diffusion%20models%20for%20image%20editing.%20SAGE%20builds%20upon%20the%20DDIM%0Aalgorithm%20and%20incorporates%20a%20novel%20guidance%20mechanism%20utilizing%20the%0Aself-attention%20layers%20of%20the%20diffusion%20U-Net.%20This%20mechanism%20computes%20a%0Areconstruction%20objective%20based%20on%20attention%20maps%20generated%20during%20the%20inverse%0ADDIM%20process%2C%20enabling%20efficient%20reconstruction%20of%20unedited%20regions%20without%20the%0Aneed%20to%20precisely%20reconstruct%20the%20entire%20input%20image.%20Thus%2C%20SAGE%20directly%0Aaddresses%20the%20key%20challenges%20in%20image%20editing.%20The%20superiority%20of%20SAGE%20over%0Aother%20methods%20is%20demonstrated%20through%20quantitative%20and%20qualitative%20evaluations%0Aand%20confirmed%20by%20a%20statistically%20validated%20comprehensive%20user%20study%2C%20in%20which%0Aall%2047%20surveyed%20users%20preferred%20SAGE%20over%20competing%20methods.%20Additionally%2C%20SAGE%0Aranks%20as%20the%20top-performing%20method%20in%20seven%20out%20of%2010%20quantitative%20analyses%20and%0Asecures%20second%20and%20third%20places%20in%20the%20remaining%20three.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09571v1&entry.124074799=Read"},
{"title": "Customizing a Large Language Model for VHDL Design of High-Performance\n  Microprocessors", "author": "Nicolas Dupuis and Ravi Nair and Shyam Ramji and Sean McClintock and Nishant Chauhan and Priyanka Nagpal and Bart Blaner and Ken Valk and Leon Stok and Ruchir Puri", "abstract": "  The use of Large Language Models (LLMs) in hardware design has taken off in\nrecent years, principally through its incorporation in tools that increase chip\ndesigner productivity. There has been considerable discussion about the use of\nLLMs in RTL specifications of chip designs, for which the two most popular\nlanguages are Verilog and VHDL. LLMs and their use in Verilog design has\nreceived significant attention due to the higher popularity of the language,\nbut little attention so far has been given to VHDL despite its continued\npopularity in the industry. There has also been little discussion about the\nunique needs of organizations that engage in high-performance processor design,\nand techniques to deploy AI solutions in these settings. In this paper, we\ndescribe our journey in developing a Large Language Model (LLM) specifically\nfor the purpose of explaining VHDL code, a task that has particular importance\nin an organization with decades of experience and assets in high-performance\nprocessor design. We show how we developed test sets specific to our needs and\nused them for evaluating models as we performed extended pretraining (EPT) of a\nbase LLM. Expert evaluation of the code explanations produced by the EPT model\nincreased to 69% compared to a base model rating of 43%. We further show how we\ndeveloped an LLM-as-a-judge to gauge models similar to expert evaluators. This\nled us to deriving and evaluating a host of new models, including an\ninstruction-tuned version of the EPT model with an expected expert evaluator\nrating of 71%. Our experiments also indicate that with the potential use of\nnewer base models, this rating can be pushed to 85% and beyond. We conclude\nwith a discussion on further improving the quality of hardware design LLMs\nusing exciting new developments in the Generative AI world.\n", "link": "http://arxiv.org/abs/2505.09610v1", "date": "2025-05-14", "relevancy": 2.3702, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Customizing%20a%20Large%20Language%20Model%20for%20VHDL%20Design%20of%20High-Performance%0A%20%20Microprocessors&body=Title%3A%20Customizing%20a%20Large%20Language%20Model%20for%20VHDL%20Design%20of%20High-Performance%0A%20%20Microprocessors%0AAuthor%3A%20Nicolas%20Dupuis%20and%20Ravi%20Nair%20and%20Shyam%20Ramji%20and%20Sean%20McClintock%20and%20Nishant%20Chauhan%20and%20Priyanka%20Nagpal%20and%20Bart%20Blaner%20and%20Ken%20Valk%20and%20Leon%20Stok%20and%20Ruchir%20Puri%0AAbstract%3A%20%20%20The%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20hardware%20design%20has%20taken%20off%20in%0Arecent%20years%2C%20principally%20through%20its%20incorporation%20in%20tools%20that%20increase%20chip%0Adesigner%20productivity.%20There%20has%20been%20considerable%20discussion%20about%20the%20use%20of%0ALLMs%20in%20RTL%20specifications%20of%20chip%20designs%2C%20for%20which%20the%20two%20most%20popular%0Alanguages%20are%20Verilog%20and%20VHDL.%20LLMs%20and%20their%20use%20in%20Verilog%20design%20has%0Areceived%20significant%20attention%20due%20to%20the%20higher%20popularity%20of%20the%20language%2C%0Abut%20little%20attention%20so%20far%20has%20been%20given%20to%20VHDL%20despite%20its%20continued%0Apopularity%20in%20the%20industry.%20There%20has%20also%20been%20little%20discussion%20about%20the%0Aunique%20needs%20of%20organizations%20that%20engage%20in%20high-performance%20processor%20design%2C%0Aand%20techniques%20to%20deploy%20AI%20solutions%20in%20these%20settings.%20In%20this%20paper%2C%20we%0Adescribe%20our%20journey%20in%20developing%20a%20Large%20Language%20Model%20%28LLM%29%20specifically%0Afor%20the%20purpose%20of%20explaining%20VHDL%20code%2C%20a%20task%20that%20has%20particular%20importance%0Ain%20an%20organization%20with%20decades%20of%20experience%20and%20assets%20in%20high-performance%0Aprocessor%20design.%20We%20show%20how%20we%20developed%20test%20sets%20specific%20to%20our%20needs%20and%0Aused%20them%20for%20evaluating%20models%20as%20we%20performed%20extended%20pretraining%20%28EPT%29%20of%20a%0Abase%20LLM.%20Expert%20evaluation%20of%20the%20code%20explanations%20produced%20by%20the%20EPT%20model%0Aincreased%20to%2069%25%20compared%20to%20a%20base%20model%20rating%20of%2043%25.%20We%20further%20show%20how%20we%0Adeveloped%20an%20LLM-as-a-judge%20to%20gauge%20models%20similar%20to%20expert%20evaluators.%20This%0Aled%20us%20to%20deriving%20and%20evaluating%20a%20host%20of%20new%20models%2C%20including%20an%0Ainstruction-tuned%20version%20of%20the%20EPT%20model%20with%20an%20expected%20expert%20evaluator%0Arating%20of%2071%25.%20Our%20experiments%20also%20indicate%20that%20with%20the%20potential%20use%20of%0Anewer%20base%20models%2C%20this%20rating%20can%20be%20pushed%20to%2085%25%20and%20beyond.%20We%20conclude%0Awith%20a%20discussion%20on%20further%20improving%20the%20quality%20of%20hardware%20design%20LLMs%0Ausing%20exciting%20new%20developments%20in%20the%20Generative%20AI%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCustomizing%2520a%2520Large%2520Language%2520Model%2520for%2520VHDL%2520Design%2520of%2520High-Performance%250A%2520%2520Microprocessors%26entry.906535625%3DNicolas%2520Dupuis%2520and%2520Ravi%2520Nair%2520and%2520Shyam%2520Ramji%2520and%2520Sean%2520McClintock%2520and%2520Nishant%2520Chauhan%2520and%2520Priyanka%2520Nagpal%2520and%2520Bart%2520Blaner%2520and%2520Ken%2520Valk%2520and%2520Leon%2520Stok%2520and%2520Ruchir%2520Puri%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520hardware%2520design%2520has%2520taken%2520off%2520in%250Arecent%2520years%252C%2520principally%2520through%2520its%2520incorporation%2520in%2520tools%2520that%2520increase%2520chip%250Adesigner%2520productivity.%2520There%2520has%2520been%2520considerable%2520discussion%2520about%2520the%2520use%2520of%250ALLMs%2520in%2520RTL%2520specifications%2520of%2520chip%2520designs%252C%2520for%2520which%2520the%2520two%2520most%2520popular%250Alanguages%2520are%2520Verilog%2520and%2520VHDL.%2520LLMs%2520and%2520their%2520use%2520in%2520Verilog%2520design%2520has%250Areceived%2520significant%2520attention%2520due%2520to%2520the%2520higher%2520popularity%2520of%2520the%2520language%252C%250Abut%2520little%2520attention%2520so%2520far%2520has%2520been%2520given%2520to%2520VHDL%2520despite%2520its%2520continued%250Apopularity%2520in%2520the%2520industry.%2520There%2520has%2520also%2520been%2520little%2520discussion%2520about%2520the%250Aunique%2520needs%2520of%2520organizations%2520that%2520engage%2520in%2520high-performance%2520processor%2520design%252C%250Aand%2520techniques%2520to%2520deploy%2520AI%2520solutions%2520in%2520these%2520settings.%2520In%2520this%2520paper%252C%2520we%250Adescribe%2520our%2520journey%2520in%2520developing%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520specifically%250Afor%2520the%2520purpose%2520of%2520explaining%2520VHDL%2520code%252C%2520a%2520task%2520that%2520has%2520particular%2520importance%250Ain%2520an%2520organization%2520with%2520decades%2520of%2520experience%2520and%2520assets%2520in%2520high-performance%250Aprocessor%2520design.%2520We%2520show%2520how%2520we%2520developed%2520test%2520sets%2520specific%2520to%2520our%2520needs%2520and%250Aused%2520them%2520for%2520evaluating%2520models%2520as%2520we%2520performed%2520extended%2520pretraining%2520%2528EPT%2529%2520of%2520a%250Abase%2520LLM.%2520Expert%2520evaluation%2520of%2520the%2520code%2520explanations%2520produced%2520by%2520the%2520EPT%2520model%250Aincreased%2520to%252069%2525%2520compared%2520to%2520a%2520base%2520model%2520rating%2520of%252043%2525.%2520We%2520further%2520show%2520how%2520we%250Adeveloped%2520an%2520LLM-as-a-judge%2520to%2520gauge%2520models%2520similar%2520to%2520expert%2520evaluators.%2520This%250Aled%2520us%2520to%2520deriving%2520and%2520evaluating%2520a%2520host%2520of%2520new%2520models%252C%2520including%2520an%250Ainstruction-tuned%2520version%2520of%2520the%2520EPT%2520model%2520with%2520an%2520expected%2520expert%2520evaluator%250Arating%2520of%252071%2525.%2520Our%2520experiments%2520also%2520indicate%2520that%2520with%2520the%2520potential%2520use%2520of%250Anewer%2520base%2520models%252C%2520this%2520rating%2520can%2520be%2520pushed%2520to%252085%2525%2520and%2520beyond.%2520We%2520conclude%250Awith%2520a%2520discussion%2520on%2520further%2520improving%2520the%2520quality%2520of%2520hardware%2520design%2520LLMs%250Ausing%2520exciting%2520new%2520developments%2520in%2520the%2520Generative%2520AI%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Customizing%20a%20Large%20Language%20Model%20for%20VHDL%20Design%20of%20High-Performance%0A%20%20Microprocessors&entry.906535625=Nicolas%20Dupuis%20and%20Ravi%20Nair%20and%20Shyam%20Ramji%20and%20Sean%20McClintock%20and%20Nishant%20Chauhan%20and%20Priyanka%20Nagpal%20and%20Bart%20Blaner%20and%20Ken%20Valk%20and%20Leon%20Stok%20and%20Ruchir%20Puri&entry.1292438233=%20%20The%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20hardware%20design%20has%20taken%20off%20in%0Arecent%20years%2C%20principally%20through%20its%20incorporation%20in%20tools%20that%20increase%20chip%0Adesigner%20productivity.%20There%20has%20been%20considerable%20discussion%20about%20the%20use%20of%0ALLMs%20in%20RTL%20specifications%20of%20chip%20designs%2C%20for%20which%20the%20two%20most%20popular%0Alanguages%20are%20Verilog%20and%20VHDL.%20LLMs%20and%20their%20use%20in%20Verilog%20design%20has%0Areceived%20significant%20attention%20due%20to%20the%20higher%20popularity%20of%20the%20language%2C%0Abut%20little%20attention%20so%20far%20has%20been%20given%20to%20VHDL%20despite%20its%20continued%0Apopularity%20in%20the%20industry.%20There%20has%20also%20been%20little%20discussion%20about%20the%0Aunique%20needs%20of%20organizations%20that%20engage%20in%20high-performance%20processor%20design%2C%0Aand%20techniques%20to%20deploy%20AI%20solutions%20in%20these%20settings.%20In%20this%20paper%2C%20we%0Adescribe%20our%20journey%20in%20developing%20a%20Large%20Language%20Model%20%28LLM%29%20specifically%0Afor%20the%20purpose%20of%20explaining%20VHDL%20code%2C%20a%20task%20that%20has%20particular%20importance%0Ain%20an%20organization%20with%20decades%20of%20experience%20and%20assets%20in%20high-performance%0Aprocessor%20design.%20We%20show%20how%20we%20developed%20test%20sets%20specific%20to%20our%20needs%20and%0Aused%20them%20for%20evaluating%20models%20as%20we%20performed%20extended%20pretraining%20%28EPT%29%20of%20a%0Abase%20LLM.%20Expert%20evaluation%20of%20the%20code%20explanations%20produced%20by%20the%20EPT%20model%0Aincreased%20to%2069%25%20compared%20to%20a%20base%20model%20rating%20of%2043%25.%20We%20further%20show%20how%20we%0Adeveloped%20an%20LLM-as-a-judge%20to%20gauge%20models%20similar%20to%20expert%20evaluators.%20This%0Aled%20us%20to%20deriving%20and%20evaluating%20a%20host%20of%20new%20models%2C%20including%20an%0Ainstruction-tuned%20version%20of%20the%20EPT%20model%20with%20an%20expected%20expert%20evaluator%0Arating%20of%2071%25.%20Our%20experiments%20also%20indicate%20that%20with%20the%20potential%20use%20of%0Anewer%20base%20models%2C%20this%20rating%20can%20be%20pushed%20to%2085%25%20and%20beyond.%20We%20conclude%0Awith%20a%20discussion%20on%20further%20improving%20the%20quality%20of%20hardware%20design%20LLMs%0Ausing%20exciting%20new%20developments%20in%20the%20Generative%20AI%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09610v1&entry.124074799=Read"},
{"title": "Improving Network Threat Detection by Knowledge Graph, Large Language\n  Model, and Imbalanced Learning", "author": "Lili Zhang and Quanyan Zhu and Herman Ray and Ying Xie", "abstract": "  Network threat detection has been challenging due to the complexities of\nattack activities and the limitation of historical threat data to learn from.\nTo help enhance the existing practices of using analytics, machine learning,\nand artificial intelligence methods to detect the network threats, we propose\nan integrated modelling framework, where Knowledge Graph is used to analyze the\nusers' activity patterns, Imbalanced Learning techniques are used to prune and\nweigh Knowledge Graph, and LLM is used to retrieve and interpret the users'\nactivities from Knowledge Graph. The proposed framework is applied to Agile\nThreat Detection through Online Sequential Learning. The preliminary results\nshow the improved threat capture rate by 3%-4% and the increased\ninterpretabilities of risk predictions based on the users' activities.\n", "link": "http://arxiv.org/abs/2501.16393v2", "date": "2025-05-14", "relevancy": 2.3585, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4854}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4688}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Network%20Threat%20Detection%20by%20Knowledge%20Graph%2C%20Large%20Language%0A%20%20Model%2C%20and%20Imbalanced%20Learning&body=Title%3A%20Improving%20Network%20Threat%20Detection%20by%20Knowledge%20Graph%2C%20Large%20Language%0A%20%20Model%2C%20and%20Imbalanced%20Learning%0AAuthor%3A%20Lili%20Zhang%20and%20Quanyan%20Zhu%20and%20Herman%20Ray%20and%20Ying%20Xie%0AAbstract%3A%20%20%20Network%20threat%20detection%20has%20been%20challenging%20due%20to%20the%20complexities%20of%0Aattack%20activities%20and%20the%20limitation%20of%20historical%20threat%20data%20to%20learn%20from.%0ATo%20help%20enhance%20the%20existing%20practices%20of%20using%20analytics%2C%20machine%20learning%2C%0Aand%20artificial%20intelligence%20methods%20to%20detect%20the%20network%20threats%2C%20we%20propose%0Aan%20integrated%20modelling%20framework%2C%20where%20Knowledge%20Graph%20is%20used%20to%20analyze%20the%0Ausers%27%20activity%20patterns%2C%20Imbalanced%20Learning%20techniques%20are%20used%20to%20prune%20and%0Aweigh%20Knowledge%20Graph%2C%20and%20LLM%20is%20used%20to%20retrieve%20and%20interpret%20the%20users%27%0Aactivities%20from%20Knowledge%20Graph.%20The%20proposed%20framework%20is%20applied%20to%20Agile%0AThreat%20Detection%20through%20Online%20Sequential%20Learning.%20The%20preliminary%20results%0Ashow%20the%20improved%20threat%20capture%20rate%20by%203%25-4%25%20and%20the%20increased%0Ainterpretabilities%20of%20risk%20predictions%20based%20on%20the%20users%27%20activities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16393v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Network%2520Threat%2520Detection%2520by%2520Knowledge%2520Graph%252C%2520Large%2520Language%250A%2520%2520Model%252C%2520and%2520Imbalanced%2520Learning%26entry.906535625%3DLili%2520Zhang%2520and%2520Quanyan%2520Zhu%2520and%2520Herman%2520Ray%2520and%2520Ying%2520Xie%26entry.1292438233%3D%2520%2520Network%2520threat%2520detection%2520has%2520been%2520challenging%2520due%2520to%2520the%2520complexities%2520of%250Aattack%2520activities%2520and%2520the%2520limitation%2520of%2520historical%2520threat%2520data%2520to%2520learn%2520from.%250ATo%2520help%2520enhance%2520the%2520existing%2520practices%2520of%2520using%2520analytics%252C%2520machine%2520learning%252C%250Aand%2520artificial%2520intelligence%2520methods%2520to%2520detect%2520the%2520network%2520threats%252C%2520we%2520propose%250Aan%2520integrated%2520modelling%2520framework%252C%2520where%2520Knowledge%2520Graph%2520is%2520used%2520to%2520analyze%2520the%250Ausers%2527%2520activity%2520patterns%252C%2520Imbalanced%2520Learning%2520techniques%2520are%2520used%2520to%2520prune%2520and%250Aweigh%2520Knowledge%2520Graph%252C%2520and%2520LLM%2520is%2520used%2520to%2520retrieve%2520and%2520interpret%2520the%2520users%2527%250Aactivities%2520from%2520Knowledge%2520Graph.%2520The%2520proposed%2520framework%2520is%2520applied%2520to%2520Agile%250AThreat%2520Detection%2520through%2520Online%2520Sequential%2520Learning.%2520The%2520preliminary%2520results%250Ashow%2520the%2520improved%2520threat%2520capture%2520rate%2520by%25203%2525-4%2525%2520and%2520the%2520increased%250Ainterpretabilities%2520of%2520risk%2520predictions%2520based%2520on%2520the%2520users%2527%2520activities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16393v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Network%20Threat%20Detection%20by%20Knowledge%20Graph%2C%20Large%20Language%0A%20%20Model%2C%20and%20Imbalanced%20Learning&entry.906535625=Lili%20Zhang%20and%20Quanyan%20Zhu%20and%20Herman%20Ray%20and%20Ying%20Xie&entry.1292438233=%20%20Network%20threat%20detection%20has%20been%20challenging%20due%20to%20the%20complexities%20of%0Aattack%20activities%20and%20the%20limitation%20of%20historical%20threat%20data%20to%20learn%20from.%0ATo%20help%20enhance%20the%20existing%20practices%20of%20using%20analytics%2C%20machine%20learning%2C%0Aand%20artificial%20intelligence%20methods%20to%20detect%20the%20network%20threats%2C%20we%20propose%0Aan%20integrated%20modelling%20framework%2C%20where%20Knowledge%20Graph%20is%20used%20to%20analyze%20the%0Ausers%27%20activity%20patterns%2C%20Imbalanced%20Learning%20techniques%20are%20used%20to%20prune%20and%0Aweigh%20Knowledge%20Graph%2C%20and%20LLM%20is%20used%20to%20retrieve%20and%20interpret%20the%20users%27%0Aactivities%20from%20Knowledge%20Graph.%20The%20proposed%20framework%20is%20applied%20to%20Agile%0AThreat%20Detection%20through%20Online%20Sequential%20Learning.%20The%20preliminary%20results%0Ashow%20the%20improved%20threat%20capture%20rate%20by%203%25-4%25%20and%20the%20increased%0Ainterpretabilities%20of%20risk%20predictions%20based%20on%20the%20users%27%20activities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16393v2&entry.124074799=Read"},
{"title": "Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low\n  Latency and High Throughput", "author": "Bo Zhang and Shuo Li and Runhe Tian and Yang Yang and Jixin Tang and Jinhao Zhou and Lin Ma", "abstract": "  In this paper, we introduce Flash-VL 2B, a novel approach to optimizing\nVision-Language Models (VLMs) for real-time applications, targeting ultra-low\nlatency and high throughput without sacrificing accuracy. Leveraging advanced\narchitectural enhancements and efficient computational strategies, Flash-VL 2B\nis designed to maximize throughput by reducing processing time while\nmaintaining competitive performance across multiple vision-language benchmarks.\nOur approach includes tailored architectural choices, token compression\nmechanisms, data curation, training schemes, and a novel image processing\ntechnique called implicit semantic stitching that effectively balances\ncomputational load and model performance. Through extensive evaluations on 11\nstandard VLM benchmarks, we demonstrate that Flash-VL 2B achieves\nstate-of-the-art results in both speed and accuracy, making it a promising\nsolution for deployment in resource-constrained environments and large-scale\nreal-time applications.\n", "link": "http://arxiv.org/abs/2505.09498v1", "date": "2025-05-14", "relevancy": 2.3501, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flash-VL%202B%3A%20Optimizing%20Vision-Language%20Model%20Performance%20for%20Ultra-Low%0A%20%20Latency%20and%20High%20Throughput&body=Title%3A%20Flash-VL%202B%3A%20Optimizing%20Vision-Language%20Model%20Performance%20for%20Ultra-Low%0A%20%20Latency%20and%20High%20Throughput%0AAuthor%3A%20Bo%20Zhang%20and%20Shuo%20Li%20and%20Runhe%20Tian%20and%20Yang%20Yang%20and%20Jixin%20Tang%20and%20Jinhao%20Zhou%20and%20Lin%20Ma%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Flash-VL%202B%2C%20a%20novel%20approach%20to%20optimizing%0AVision-Language%20Models%20%28VLMs%29%20for%20real-time%20applications%2C%20targeting%20ultra-low%0Alatency%20and%20high%20throughput%20without%20sacrificing%20accuracy.%20Leveraging%20advanced%0Aarchitectural%20enhancements%20and%20efficient%20computational%20strategies%2C%20Flash-VL%202B%0Ais%20designed%20to%20maximize%20throughput%20by%20reducing%20processing%20time%20while%0Amaintaining%20competitive%20performance%20across%20multiple%20vision-language%20benchmarks.%0AOur%20approach%20includes%20tailored%20architectural%20choices%2C%20token%20compression%0Amechanisms%2C%20data%20curation%2C%20training%20schemes%2C%20and%20a%20novel%20image%20processing%0Atechnique%20called%20implicit%20semantic%20stitching%20that%20effectively%20balances%0Acomputational%20load%20and%20model%20performance.%20Through%20extensive%20evaluations%20on%2011%0Astandard%20VLM%20benchmarks%2C%20we%20demonstrate%20that%20Flash-VL%202B%20achieves%0Astate-of-the-art%20results%20in%20both%20speed%20and%20accuracy%2C%20making%20it%20a%20promising%0Asolution%20for%20deployment%20in%20resource-constrained%20environments%20and%20large-scale%0Areal-time%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlash-VL%25202B%253A%2520Optimizing%2520Vision-Language%2520Model%2520Performance%2520for%2520Ultra-Low%250A%2520%2520Latency%2520and%2520High%2520Throughput%26entry.906535625%3DBo%2520Zhang%2520and%2520Shuo%2520Li%2520and%2520Runhe%2520Tian%2520and%2520Yang%2520Yang%2520and%2520Jixin%2520Tang%2520and%2520Jinhao%2520Zhou%2520and%2520Lin%2520Ma%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Flash-VL%25202B%252C%2520a%2520novel%2520approach%2520to%2520optimizing%250AVision-Language%2520Models%2520%2528VLMs%2529%2520for%2520real-time%2520applications%252C%2520targeting%2520ultra-low%250Alatency%2520and%2520high%2520throughput%2520without%2520sacrificing%2520accuracy.%2520Leveraging%2520advanced%250Aarchitectural%2520enhancements%2520and%2520efficient%2520computational%2520strategies%252C%2520Flash-VL%25202B%250Ais%2520designed%2520to%2520maximize%2520throughput%2520by%2520reducing%2520processing%2520time%2520while%250Amaintaining%2520competitive%2520performance%2520across%2520multiple%2520vision-language%2520benchmarks.%250AOur%2520approach%2520includes%2520tailored%2520architectural%2520choices%252C%2520token%2520compression%250Amechanisms%252C%2520data%2520curation%252C%2520training%2520schemes%252C%2520and%2520a%2520novel%2520image%2520processing%250Atechnique%2520called%2520implicit%2520semantic%2520stitching%2520that%2520effectively%2520balances%250Acomputational%2520load%2520and%2520model%2520performance.%2520Through%2520extensive%2520evaluations%2520on%252011%250Astandard%2520VLM%2520benchmarks%252C%2520we%2520demonstrate%2520that%2520Flash-VL%25202B%2520achieves%250Astate-of-the-art%2520results%2520in%2520both%2520speed%2520and%2520accuracy%252C%2520making%2520it%2520a%2520promising%250Asolution%2520for%2520deployment%2520in%2520resource-constrained%2520environments%2520and%2520large-scale%250Areal-time%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flash-VL%202B%3A%20Optimizing%20Vision-Language%20Model%20Performance%20for%20Ultra-Low%0A%20%20Latency%20and%20High%20Throughput&entry.906535625=Bo%20Zhang%20and%20Shuo%20Li%20and%20Runhe%20Tian%20and%20Yang%20Yang%20and%20Jixin%20Tang%20and%20Jinhao%20Zhou%20and%20Lin%20Ma&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Flash-VL%202B%2C%20a%20novel%20approach%20to%20optimizing%0AVision-Language%20Models%20%28VLMs%29%20for%20real-time%20applications%2C%20targeting%20ultra-low%0Alatency%20and%20high%20throughput%20without%20sacrificing%20accuracy.%20Leveraging%20advanced%0Aarchitectural%20enhancements%20and%20efficient%20computational%20strategies%2C%20Flash-VL%202B%0Ais%20designed%20to%20maximize%20throughput%20by%20reducing%20processing%20time%20while%0Amaintaining%20competitive%20performance%20across%20multiple%20vision-language%20benchmarks.%0AOur%20approach%20includes%20tailored%20architectural%20choices%2C%20token%20compression%0Amechanisms%2C%20data%20curation%2C%20training%20schemes%2C%20and%20a%20novel%20image%20processing%0Atechnique%20called%20implicit%20semantic%20stitching%20that%20effectively%20balances%0Acomputational%20load%20and%20model%20performance.%20Through%20extensive%20evaluations%20on%2011%0Astandard%20VLM%20benchmarks%2C%20we%20demonstrate%20that%20Flash-VL%202B%20achieves%0Astate-of-the-art%20results%20in%20both%20speed%20and%20accuracy%2C%20making%20it%20a%20promising%0Asolution%20for%20deployment%20in%20resource-constrained%20environments%20and%20large-scale%0Areal-time%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09498v1&entry.124074799=Read"},
{"title": "Feature Extractor or Decision Maker: Rethinking the Role of Visual\n  Encoders in Visuomotor Policies", "author": "Ruiyu Wang and Zheyu Zhuang and Shutong Jin and Nils Ingelhag and Danica Kragic and Florian T. Pokorny", "abstract": "  An end-to-end (E2E) visuomotor policy is typically treated as a unified\nwhole, but recent approaches using out-of-domain (OOD) data to pretrain the\nvisual encoder have cleanly separated the visual encoder from the network, with\nthe remainder referred to as the policy. We propose Visual Alignment Testing,\nan experimental framework designed to evaluate the validity of this functional\nseparation. Our results indicate that in E2E-trained models, visual encoders\nactively contribute to decision-making resulting from motor data supervision,\ncontradicting the assumed functional separation. In contrast, OOD-pretrained\nmodels, where encoders lack this capability, experience an average performance\ndrop of 42\\% in our benchmark results, compared to the state-of-the-art\nperformance achieved by E2E policies. We believe this initial exploration of\nvisual encoders' role can provide a first step towards guiding future\npretraining methods to address their decision-making ability, such as\ndeveloping task-conditioned or context-aware encoders.\n", "link": "http://arxiv.org/abs/2409.20248v2", "date": "2025-05-14", "relevancy": 2.3421, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5963}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5963}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Extractor%20or%20Decision%20Maker%3A%20Rethinking%20the%20Role%20of%20Visual%0A%20%20Encoders%20in%20Visuomotor%20Policies&body=Title%3A%20Feature%20Extractor%20or%20Decision%20Maker%3A%20Rethinking%20the%20Role%20of%20Visual%0A%20%20Encoders%20in%20Visuomotor%20Policies%0AAuthor%3A%20Ruiyu%20Wang%20and%20Zheyu%20Zhuang%20and%20Shutong%20Jin%20and%20Nils%20Ingelhag%20and%20Danica%20Kragic%20and%20Florian%20T.%20Pokorny%0AAbstract%3A%20%20%20An%20end-to-end%20%28E2E%29%20visuomotor%20policy%20is%20typically%20treated%20as%20a%20unified%0Awhole%2C%20but%20recent%20approaches%20using%20out-of-domain%20%28OOD%29%20data%20to%20pretrain%20the%0Avisual%20encoder%20have%20cleanly%20separated%20the%20visual%20encoder%20from%20the%20network%2C%20with%0Athe%20remainder%20referred%20to%20as%20the%20policy.%20We%20propose%20Visual%20Alignment%20Testing%2C%0Aan%20experimental%20framework%20designed%20to%20evaluate%20the%20validity%20of%20this%20functional%0Aseparation.%20Our%20results%20indicate%20that%20in%20E2E-trained%20models%2C%20visual%20encoders%0Aactively%20contribute%20to%20decision-making%20resulting%20from%20motor%20data%20supervision%2C%0Acontradicting%20the%20assumed%20functional%20separation.%20In%20contrast%2C%20OOD-pretrained%0Amodels%2C%20where%20encoders%20lack%20this%20capability%2C%20experience%20an%20average%20performance%0Adrop%20of%2042%5C%25%20in%20our%20benchmark%20results%2C%20compared%20to%20the%20state-of-the-art%0Aperformance%20achieved%20by%20E2E%20policies.%20We%20believe%20this%20initial%20exploration%20of%0Avisual%20encoders%27%20role%20can%20provide%20a%20first%20step%20towards%20guiding%20future%0Apretraining%20methods%20to%20address%20their%20decision-making%20ability%2C%20such%20as%0Adeveloping%20task-conditioned%20or%20context-aware%20encoders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.20248v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Extractor%2520or%2520Decision%2520Maker%253A%2520Rethinking%2520the%2520Role%2520of%2520Visual%250A%2520%2520Encoders%2520in%2520Visuomotor%2520Policies%26entry.906535625%3DRuiyu%2520Wang%2520and%2520Zheyu%2520Zhuang%2520and%2520Shutong%2520Jin%2520and%2520Nils%2520Ingelhag%2520and%2520Danica%2520Kragic%2520and%2520Florian%2520T.%2520Pokorny%26entry.1292438233%3D%2520%2520An%2520end-to-end%2520%2528E2E%2529%2520visuomotor%2520policy%2520is%2520typically%2520treated%2520as%2520a%2520unified%250Awhole%252C%2520but%2520recent%2520approaches%2520using%2520out-of-domain%2520%2528OOD%2529%2520data%2520to%2520pretrain%2520the%250Avisual%2520encoder%2520have%2520cleanly%2520separated%2520the%2520visual%2520encoder%2520from%2520the%2520network%252C%2520with%250Athe%2520remainder%2520referred%2520to%2520as%2520the%2520policy.%2520We%2520propose%2520Visual%2520Alignment%2520Testing%252C%250Aan%2520experimental%2520framework%2520designed%2520to%2520evaluate%2520the%2520validity%2520of%2520this%2520functional%250Aseparation.%2520Our%2520results%2520indicate%2520that%2520in%2520E2E-trained%2520models%252C%2520visual%2520encoders%250Aactively%2520contribute%2520to%2520decision-making%2520resulting%2520from%2520motor%2520data%2520supervision%252C%250Acontradicting%2520the%2520assumed%2520functional%2520separation.%2520In%2520contrast%252C%2520OOD-pretrained%250Amodels%252C%2520where%2520encoders%2520lack%2520this%2520capability%252C%2520experience%2520an%2520average%2520performance%250Adrop%2520of%252042%255C%2525%2520in%2520our%2520benchmark%2520results%252C%2520compared%2520to%2520the%2520state-of-the-art%250Aperformance%2520achieved%2520by%2520E2E%2520policies.%2520We%2520believe%2520this%2520initial%2520exploration%2520of%250Avisual%2520encoders%2527%2520role%2520can%2520provide%2520a%2520first%2520step%2520towards%2520guiding%2520future%250Apretraining%2520methods%2520to%2520address%2520their%2520decision-making%2520ability%252C%2520such%2520as%250Adeveloping%2520task-conditioned%2520or%2520context-aware%2520encoders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20248v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Extractor%20or%20Decision%20Maker%3A%20Rethinking%20the%20Role%20of%20Visual%0A%20%20Encoders%20in%20Visuomotor%20Policies&entry.906535625=Ruiyu%20Wang%20and%20Zheyu%20Zhuang%20and%20Shutong%20Jin%20and%20Nils%20Ingelhag%20and%20Danica%20Kragic%20and%20Florian%20T.%20Pokorny&entry.1292438233=%20%20An%20end-to-end%20%28E2E%29%20visuomotor%20policy%20is%20typically%20treated%20as%20a%20unified%0Awhole%2C%20but%20recent%20approaches%20using%20out-of-domain%20%28OOD%29%20data%20to%20pretrain%20the%0Avisual%20encoder%20have%20cleanly%20separated%20the%20visual%20encoder%20from%20the%20network%2C%20with%0Athe%20remainder%20referred%20to%20as%20the%20policy.%20We%20propose%20Visual%20Alignment%20Testing%2C%0Aan%20experimental%20framework%20designed%20to%20evaluate%20the%20validity%20of%20this%20functional%0Aseparation.%20Our%20results%20indicate%20that%20in%20E2E-trained%20models%2C%20visual%20encoders%0Aactively%20contribute%20to%20decision-making%20resulting%20from%20motor%20data%20supervision%2C%0Acontradicting%20the%20assumed%20functional%20separation.%20In%20contrast%2C%20OOD-pretrained%0Amodels%2C%20where%20encoders%20lack%20this%20capability%2C%20experience%20an%20average%20performance%0Adrop%20of%2042%5C%25%20in%20our%20benchmark%20results%2C%20compared%20to%20the%20state-of-the-art%0Aperformance%20achieved%20by%20E2E%20policies.%20We%20believe%20this%20initial%20exploration%20of%0Avisual%20encoders%27%20role%20can%20provide%20a%20first%20step%20towards%20guiding%20future%0Apretraining%20methods%20to%20address%20their%20decision-making%20ability%2C%20such%20as%0Adeveloping%20task-conditioned%20or%20context-aware%20encoders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.20248v2&entry.124074799=Read"},
{"title": "3D Cartoon Face Generation with Controllable Expressions from a Single\n  GAN Image", "author": "Hao Wang and Wenhao Shen and Guosheng Lin and Steven C. H. Hoi and Chunyan Miao", "abstract": "  In this paper, we investigate an open research task of generating 3D cartoon\nface shapes from single 2D GAN generated human faces and without 3D\nsupervision, where we can also manipulate the facial expressions of the 3D\nshapes. To this end, we discover the semantic meanings of StyleGAN latent\nspace, such that we are able to produce face images of various expressions,\nposes, and lighting conditions by controlling the latent codes. Specifically,\nwe first finetune the pretrained StyleGAN face model on the cartoon datasets.\nBy feeding the same latent codes to face and cartoon generation models, we aim\nto realize the translation from 2D human face images to cartoon styled avatars.\nWe then discover semantic directions of the GAN latent space, in an attempt to\nchange the facial expressions while preserving the original identity. As we do\nnot have any 3D annotations for cartoon faces, we manipulate the latent codes\nto generate images with different poses and lighting conditions, such that we\ncan reconstruct the 3D cartoon face shapes. We validate the efficacy of our\nmethod on three cartoon datasets qualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2207.14425v2", "date": "2025-05-14", "relevancy": 2.3386, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5964}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5823}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Cartoon%20Face%20Generation%20with%20Controllable%20Expressions%20from%20a%20Single%0A%20%20GAN%20Image&body=Title%3A%203D%20Cartoon%20Face%20Generation%20with%20Controllable%20Expressions%20from%20a%20Single%0A%20%20GAN%20Image%0AAuthor%3A%20Hao%20Wang%20and%20Wenhao%20Shen%20and%20Guosheng%20Lin%20and%20Steven%20C.%20H.%20Hoi%20and%20Chunyan%20Miao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20an%20open%20research%20task%20of%20generating%203D%20cartoon%0Aface%20shapes%20from%20single%202D%20GAN%20generated%20human%20faces%20and%20without%203D%0Asupervision%2C%20where%20we%20can%20also%20manipulate%20the%20facial%20expressions%20of%20the%203D%0Ashapes.%20To%20this%20end%2C%20we%20discover%20the%20semantic%20meanings%20of%20StyleGAN%20latent%0Aspace%2C%20such%20that%20we%20are%20able%20to%20produce%20face%20images%20of%20various%20expressions%2C%0Aposes%2C%20and%20lighting%20conditions%20by%20controlling%20the%20latent%20codes.%20Specifically%2C%0Awe%20first%20finetune%20the%20pretrained%20StyleGAN%20face%20model%20on%20the%20cartoon%20datasets.%0ABy%20feeding%20the%20same%20latent%20codes%20to%20face%20and%20cartoon%20generation%20models%2C%20we%20aim%0Ato%20realize%20the%20translation%20from%202D%20human%20face%20images%20to%20cartoon%20styled%20avatars.%0AWe%20then%20discover%20semantic%20directions%20of%20the%20GAN%20latent%20space%2C%20in%20an%20attempt%20to%0Achange%20the%20facial%20expressions%20while%20preserving%20the%20original%20identity.%20As%20we%20do%0Anot%20have%20any%203D%20annotations%20for%20cartoon%20faces%2C%20we%20manipulate%20the%20latent%20codes%0Ato%20generate%20images%20with%20different%20poses%20and%20lighting%20conditions%2C%20such%20that%20we%0Acan%20reconstruct%20the%203D%20cartoon%20face%20shapes.%20We%20validate%20the%20efficacy%20of%20our%0Amethod%20on%20three%20cartoon%20datasets%20qualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.14425v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Cartoon%2520Face%2520Generation%2520with%2520Controllable%2520Expressions%2520from%2520a%2520Single%250A%2520%2520GAN%2520Image%26entry.906535625%3DHao%2520Wang%2520and%2520Wenhao%2520Shen%2520and%2520Guosheng%2520Lin%2520and%2520Steven%2520C.%2520H.%2520Hoi%2520and%2520Chunyan%2520Miao%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520an%2520open%2520research%2520task%2520of%2520generating%25203D%2520cartoon%250Aface%2520shapes%2520from%2520single%25202D%2520GAN%2520generated%2520human%2520faces%2520and%2520without%25203D%250Asupervision%252C%2520where%2520we%2520can%2520also%2520manipulate%2520the%2520facial%2520expressions%2520of%2520the%25203D%250Ashapes.%2520To%2520this%2520end%252C%2520we%2520discover%2520the%2520semantic%2520meanings%2520of%2520StyleGAN%2520latent%250Aspace%252C%2520such%2520that%2520we%2520are%2520able%2520to%2520produce%2520face%2520images%2520of%2520various%2520expressions%252C%250Aposes%252C%2520and%2520lighting%2520conditions%2520by%2520controlling%2520the%2520latent%2520codes.%2520Specifically%252C%250Awe%2520first%2520finetune%2520the%2520pretrained%2520StyleGAN%2520face%2520model%2520on%2520the%2520cartoon%2520datasets.%250ABy%2520feeding%2520the%2520same%2520latent%2520codes%2520to%2520face%2520and%2520cartoon%2520generation%2520models%252C%2520we%2520aim%250Ato%2520realize%2520the%2520translation%2520from%25202D%2520human%2520face%2520images%2520to%2520cartoon%2520styled%2520avatars.%250AWe%2520then%2520discover%2520semantic%2520directions%2520of%2520the%2520GAN%2520latent%2520space%252C%2520in%2520an%2520attempt%2520to%250Achange%2520the%2520facial%2520expressions%2520while%2520preserving%2520the%2520original%2520identity.%2520As%2520we%2520do%250Anot%2520have%2520any%25203D%2520annotations%2520for%2520cartoon%2520faces%252C%2520we%2520manipulate%2520the%2520latent%2520codes%250Ato%2520generate%2520images%2520with%2520different%2520poses%2520and%2520lighting%2520conditions%252C%2520such%2520that%2520we%250Acan%2520reconstruct%2520the%25203D%2520cartoon%2520face%2520shapes.%2520We%2520validate%2520the%2520efficacy%2520of%2520our%250Amethod%2520on%2520three%2520cartoon%2520datasets%2520qualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.14425v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Cartoon%20Face%20Generation%20with%20Controllable%20Expressions%20from%20a%20Single%0A%20%20GAN%20Image&entry.906535625=Hao%20Wang%20and%20Wenhao%20Shen%20and%20Guosheng%20Lin%20and%20Steven%20C.%20H.%20Hoi%20and%20Chunyan%20Miao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20an%20open%20research%20task%20of%20generating%203D%20cartoon%0Aface%20shapes%20from%20single%202D%20GAN%20generated%20human%20faces%20and%20without%203D%0Asupervision%2C%20where%20we%20can%20also%20manipulate%20the%20facial%20expressions%20of%20the%203D%0Ashapes.%20To%20this%20end%2C%20we%20discover%20the%20semantic%20meanings%20of%20StyleGAN%20latent%0Aspace%2C%20such%20that%20we%20are%20able%20to%20produce%20face%20images%20of%20various%20expressions%2C%0Aposes%2C%20and%20lighting%20conditions%20by%20controlling%20the%20latent%20codes.%20Specifically%2C%0Awe%20first%20finetune%20the%20pretrained%20StyleGAN%20face%20model%20on%20the%20cartoon%20datasets.%0ABy%20feeding%20the%20same%20latent%20codes%20to%20face%20and%20cartoon%20generation%20models%2C%20we%20aim%0Ato%20realize%20the%20translation%20from%202D%20human%20face%20images%20to%20cartoon%20styled%20avatars.%0AWe%20then%20discover%20semantic%20directions%20of%20the%20GAN%20latent%20space%2C%20in%20an%20attempt%20to%0Achange%20the%20facial%20expressions%20while%20preserving%20the%20original%20identity.%20As%20we%20do%0Anot%20have%20any%203D%20annotations%20for%20cartoon%20faces%2C%20we%20manipulate%20the%20latent%20codes%0Ato%20generate%20images%20with%20different%20poses%20and%20lighting%20conditions%2C%20such%20that%20we%0Acan%20reconstruct%20the%203D%20cartoon%20face%20shapes.%20We%20validate%20the%20efficacy%20of%20our%0Amethod%20on%20three%20cartoon%20datasets%20qualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.14425v2&entry.124074799=Read"},
{"title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks", "author": "Wenqi Zhang and Mengna Wang and Gangao Liu and Xu Huixin and Yiwei Jiang and Yongliang Shen and Guiyang Hou and Zhe Zheng and Hang Zhang and Xin Li and Weiming Lu and Peng Li and Yueting Zhuang", "abstract": "  Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.\n", "link": "http://arxiv.org/abs/2503.21696v2", "date": "2025-05-14", "relevancy": 2.3363, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5788}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embodied-Reasoner%3A%20Synergizing%20Visual%20Search%2C%20Reasoning%2C%20and%20Action%20for%0A%20%20Embodied%20Interactive%20Tasks&body=Title%3A%20Embodied-Reasoner%3A%20Synergizing%20Visual%20Search%2C%20Reasoning%2C%20and%20Action%20for%0A%20%20Embodied%20Interactive%20Tasks%0AAuthor%3A%20Wenqi%20Zhang%20and%20Mengna%20Wang%20and%20Gangao%20Liu%20and%20Xu%20Huixin%20and%20Yiwei%20Jiang%20and%20Yongliang%20Shen%20and%20Guiyang%20Hou%20and%20Zhe%20Zheng%20and%20Hang%20Zhang%20and%20Xin%20Li%20and%20Weiming%20Lu%20and%20Peng%20Li%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Recent%20advances%20in%20deep%20thinking%20models%20have%20demonstrated%20remarkable%0Areasoning%20capabilities%20on%20mathematical%20and%20coding%20tasks.%20However%2C%20their%0Aeffectiveness%20in%20embodied%20domains%20which%20require%20continuous%20interaction%20with%0Aenvironments%20through%20image%20action%20interleaved%20trajectories%20remains%20largely%0A-unexplored.%20We%20present%20Embodied%20Reasoner%2C%20a%20model%20that%20extends%20o1%20style%0Areasoning%20to%20interactive%20embodied%20search%20tasks.%20Unlike%20mathematical%20reasoning%0Athat%20relies%20primarily%20on%20logical%20deduction%2C%20embodied%20scenarios%20demand%20spatial%0Aunderstanding%2C%20temporal%20reasoning%2C%20and%20ongoing%20self-reflection%20based%20on%0Ainteraction%20history.%20To%20address%20these%20challenges%2C%20we%20synthesize%209.3k%20coherent%0AObservation-Thought-Action%20trajectories%20containing%2064k%20interactive%20images%20and%0A90k%20diverse%20thinking%20processes%20%28analysis%2C%20spatial%20reasoning%2C%20reflection%2C%0Aplanning%2C%20and%20verification%29.%20We%20develop%20a%20three-stage%20training%20pipeline%20that%0Aprogressively%20enhances%20the%20model%27s%20capabilities%20through%20imitation%20learning%2C%0Aself-exploration%20via%20rejection%20sampling%2C%20and%20self-correction%20through%20reflection%0Atuning.%20The%20evaluation%20shows%20that%20our%20model%20significantly%20outperforms%20those%0Aadvanced%20visual%20reasoning%20models%2C%20e.g.%2C%20it%20exceeds%20OpenAI%20o1%2C%20o3-mini%2C%20and%0AClaude-3.7%20by%20%2B9%5C%25%2C%2024%5C%25%2C%20and%20%2B13%5C%25.%20Analysis%20reveals%20our%20model%20exhibits%20fewer%0Arepeated%20searches%20and%20logical%20inconsistencies%2C%20with%20particular%20advantages%20in%0Acomplex%20long-horizon%20tasks.%20Real-world%20environments%20also%20show%20our%20superiority%0Awhile%20exhibiting%20fewer%20repeated%20searches%20and%20logical%20inconsistency%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21696v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodied-Reasoner%253A%2520Synergizing%2520Visual%2520Search%252C%2520Reasoning%252C%2520and%2520Action%2520for%250A%2520%2520Embodied%2520Interactive%2520Tasks%26entry.906535625%3DWenqi%2520Zhang%2520and%2520Mengna%2520Wang%2520and%2520Gangao%2520Liu%2520and%2520Xu%2520Huixin%2520and%2520Yiwei%2520Jiang%2520and%2520Yongliang%2520Shen%2520and%2520Guiyang%2520Hou%2520and%2520Zhe%2520Zheng%2520and%2520Hang%2520Zhang%2520and%2520Xin%2520Li%2520and%2520Weiming%2520Lu%2520and%2520Peng%2520Li%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520deep%2520thinking%2520models%2520have%2520demonstrated%2520remarkable%250Areasoning%2520capabilities%2520on%2520mathematical%2520and%2520coding%2520tasks.%2520However%252C%2520their%250Aeffectiveness%2520in%2520embodied%2520domains%2520which%2520require%2520continuous%2520interaction%2520with%250Aenvironments%2520through%2520image%2520action%2520interleaved%2520trajectories%2520remains%2520largely%250A-unexplored.%2520We%2520present%2520Embodied%2520Reasoner%252C%2520a%2520model%2520that%2520extends%2520o1%2520style%250Areasoning%2520to%2520interactive%2520embodied%2520search%2520tasks.%2520Unlike%2520mathematical%2520reasoning%250Athat%2520relies%2520primarily%2520on%2520logical%2520deduction%252C%2520embodied%2520scenarios%2520demand%2520spatial%250Aunderstanding%252C%2520temporal%2520reasoning%252C%2520and%2520ongoing%2520self-reflection%2520based%2520on%250Ainteraction%2520history.%2520To%2520address%2520these%2520challenges%252C%2520we%2520synthesize%25209.3k%2520coherent%250AObservation-Thought-Action%2520trajectories%2520containing%252064k%2520interactive%2520images%2520and%250A90k%2520diverse%2520thinking%2520processes%2520%2528analysis%252C%2520spatial%2520reasoning%252C%2520reflection%252C%250Aplanning%252C%2520and%2520verification%2529.%2520We%2520develop%2520a%2520three-stage%2520training%2520pipeline%2520that%250Aprogressively%2520enhances%2520the%2520model%2527s%2520capabilities%2520through%2520imitation%2520learning%252C%250Aself-exploration%2520via%2520rejection%2520sampling%252C%2520and%2520self-correction%2520through%2520reflection%250Atuning.%2520The%2520evaluation%2520shows%2520that%2520our%2520model%2520significantly%2520outperforms%2520those%250Aadvanced%2520visual%2520reasoning%2520models%252C%2520e.g.%252C%2520it%2520exceeds%2520OpenAI%2520o1%252C%2520o3-mini%252C%2520and%250AClaude-3.7%2520by%2520%252B9%255C%2525%252C%252024%255C%2525%252C%2520and%2520%252B13%255C%2525.%2520Analysis%2520reveals%2520our%2520model%2520exhibits%2520fewer%250Arepeated%2520searches%2520and%2520logical%2520inconsistencies%252C%2520with%2520particular%2520advantages%2520in%250Acomplex%2520long-horizon%2520tasks.%2520Real-world%2520environments%2520also%2520show%2520our%2520superiority%250Awhile%2520exhibiting%2520fewer%2520repeated%2520searches%2520and%2520logical%2520inconsistency%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21696v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embodied-Reasoner%3A%20Synergizing%20Visual%20Search%2C%20Reasoning%2C%20and%20Action%20for%0A%20%20Embodied%20Interactive%20Tasks&entry.906535625=Wenqi%20Zhang%20and%20Mengna%20Wang%20and%20Gangao%20Liu%20and%20Xu%20Huixin%20and%20Yiwei%20Jiang%20and%20Yongliang%20Shen%20and%20Guiyang%20Hou%20and%20Zhe%20Zheng%20and%20Hang%20Zhang%20and%20Xin%20Li%20and%20Weiming%20Lu%20and%20Peng%20Li%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Recent%20advances%20in%20deep%20thinking%20models%20have%20demonstrated%20remarkable%0Areasoning%20capabilities%20on%20mathematical%20and%20coding%20tasks.%20However%2C%20their%0Aeffectiveness%20in%20embodied%20domains%20which%20require%20continuous%20interaction%20with%0Aenvironments%20through%20image%20action%20interleaved%20trajectories%20remains%20largely%0A-unexplored.%20We%20present%20Embodied%20Reasoner%2C%20a%20model%20that%20extends%20o1%20style%0Areasoning%20to%20interactive%20embodied%20search%20tasks.%20Unlike%20mathematical%20reasoning%0Athat%20relies%20primarily%20on%20logical%20deduction%2C%20embodied%20scenarios%20demand%20spatial%0Aunderstanding%2C%20temporal%20reasoning%2C%20and%20ongoing%20self-reflection%20based%20on%0Ainteraction%20history.%20To%20address%20these%20challenges%2C%20we%20synthesize%209.3k%20coherent%0AObservation-Thought-Action%20trajectories%20containing%2064k%20interactive%20images%20and%0A90k%20diverse%20thinking%20processes%20%28analysis%2C%20spatial%20reasoning%2C%20reflection%2C%0Aplanning%2C%20and%20verification%29.%20We%20develop%20a%20three-stage%20training%20pipeline%20that%0Aprogressively%20enhances%20the%20model%27s%20capabilities%20through%20imitation%20learning%2C%0Aself-exploration%20via%20rejection%20sampling%2C%20and%20self-correction%20through%20reflection%0Atuning.%20The%20evaluation%20shows%20that%20our%20model%20significantly%20outperforms%20those%0Aadvanced%20visual%20reasoning%20models%2C%20e.g.%2C%20it%20exceeds%20OpenAI%20o1%2C%20o3-mini%2C%20and%0AClaude-3.7%20by%20%2B9%5C%25%2C%2024%5C%25%2C%20and%20%2B13%5C%25.%20Analysis%20reveals%20our%20model%20exhibits%20fewer%0Arepeated%20searches%20and%20logical%20inconsistencies%2C%20with%20particular%20advantages%20in%0Acomplex%20long-horizon%20tasks.%20Real-world%20environments%20also%20show%20our%20superiority%0Awhile%20exhibiting%20fewer%20repeated%20searches%20and%20logical%20inconsistency%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21696v2&entry.124074799=Read"},
{"title": "ON as ALC: Active Loop Closing Object Goal Navigation", "author": "Daiki Iwata and Kanji Tanaka and Shoya Miyazaki and Kouki Terashima", "abstract": "  In simultaneous localization and mapping, active loop closing (ALC) is an\nactive vision problem that aims to visually guide a robot to maximize the\nchances of revisiting previously visited points, thereby resetting the drift\nerrors accumulated in the incrementally built map during travel. However,\ncurrent mainstream navigation strategies that leverage such incomplete maps as\nworkspace prior knowledge often fail in modern long-term autonomy long-distance\ntravel scenarios where map accumulation errors become significant. To address\nthese limitations of map-based navigation, this paper is the first to explore\nmapless navigation in the embodied AI field, in particular, to utilize\nobject-goal navigation (commonly abbreviated as ON, ObjNav, or OGN) techniques\nthat efficiently explore target objects without using such a prior map.\nSpecifically, in this work, we start from an off-the-shelf mapless ON planner,\nextend it to utilize a prior map, and further show that the performance in\nlong-distance ALC (LD-ALC) can be maximized by minimizing ``ALC loss\" and ``ON\nloss\". This study highlights a simple and effective approach, called ALC-ON\n(ALCON), to accelerate the progress of challenging long-distance ALC technology\nby leveraging the growing frontier-guided, data-driven, and LLM-guided ON\ntechnologies.\n", "link": "http://arxiv.org/abs/2412.11523v2", "date": "2025-05-14", "relevancy": 2.3296, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5914}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5773}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ON%20as%20ALC%3A%20Active%20Loop%20Closing%20Object%20Goal%20Navigation&body=Title%3A%20ON%20as%20ALC%3A%20Active%20Loop%20Closing%20Object%20Goal%20Navigation%0AAuthor%3A%20Daiki%20Iwata%20and%20Kanji%20Tanaka%20and%20Shoya%20Miyazaki%20and%20Kouki%20Terashima%0AAbstract%3A%20%20%20In%20simultaneous%20localization%20and%20mapping%2C%20active%20loop%20closing%20%28ALC%29%20is%20an%0Aactive%20vision%20problem%20that%20aims%20to%20visually%20guide%20a%20robot%20to%20maximize%20the%0Achances%20of%20revisiting%20previously%20visited%20points%2C%20thereby%20resetting%20the%20drift%0Aerrors%20accumulated%20in%20the%20incrementally%20built%20map%20during%20travel.%20However%2C%0Acurrent%20mainstream%20navigation%20strategies%20that%20leverage%20such%20incomplete%20maps%20as%0Aworkspace%20prior%20knowledge%20often%20fail%20in%20modern%20long-term%20autonomy%20long-distance%0Atravel%20scenarios%20where%20map%20accumulation%20errors%20become%20significant.%20To%20address%0Athese%20limitations%20of%20map-based%20navigation%2C%20this%20paper%20is%20the%20first%20to%20explore%0Amapless%20navigation%20in%20the%20embodied%20AI%20field%2C%20in%20particular%2C%20to%20utilize%0Aobject-goal%20navigation%20%28commonly%20abbreviated%20as%20ON%2C%20ObjNav%2C%20or%20OGN%29%20techniques%0Athat%20efficiently%20explore%20target%20objects%20without%20using%20such%20a%20prior%20map.%0ASpecifically%2C%20in%20this%20work%2C%20we%20start%20from%20an%20off-the-shelf%20mapless%20ON%20planner%2C%0Aextend%20it%20to%20utilize%20a%20prior%20map%2C%20and%20further%20show%20that%20the%20performance%20in%0Along-distance%20ALC%20%28LD-ALC%29%20can%20be%20maximized%20by%20minimizing%20%60%60ALC%20loss%22%20and%20%60%60ON%0Aloss%22.%20This%20study%20highlights%20a%20simple%20and%20effective%20approach%2C%20called%20ALC-ON%0A%28ALCON%29%2C%20to%20accelerate%20the%20progress%20of%20challenging%20long-distance%20ALC%20technology%0Aby%20leveraging%20the%20growing%20frontier-guided%2C%20data-driven%2C%20and%20LLM-guided%20ON%0Atechnologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11523v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DON%2520as%2520ALC%253A%2520Active%2520Loop%2520Closing%2520Object%2520Goal%2520Navigation%26entry.906535625%3DDaiki%2520Iwata%2520and%2520Kanji%2520Tanaka%2520and%2520Shoya%2520Miyazaki%2520and%2520Kouki%2520Terashima%26entry.1292438233%3D%2520%2520In%2520simultaneous%2520localization%2520and%2520mapping%252C%2520active%2520loop%2520closing%2520%2528ALC%2529%2520is%2520an%250Aactive%2520vision%2520problem%2520that%2520aims%2520to%2520visually%2520guide%2520a%2520robot%2520to%2520maximize%2520the%250Achances%2520of%2520revisiting%2520previously%2520visited%2520points%252C%2520thereby%2520resetting%2520the%2520drift%250Aerrors%2520accumulated%2520in%2520the%2520incrementally%2520built%2520map%2520during%2520travel.%2520However%252C%250Acurrent%2520mainstream%2520navigation%2520strategies%2520that%2520leverage%2520such%2520incomplete%2520maps%2520as%250Aworkspace%2520prior%2520knowledge%2520often%2520fail%2520in%2520modern%2520long-term%2520autonomy%2520long-distance%250Atravel%2520scenarios%2520where%2520map%2520accumulation%2520errors%2520become%2520significant.%2520To%2520address%250Athese%2520limitations%2520of%2520map-based%2520navigation%252C%2520this%2520paper%2520is%2520the%2520first%2520to%2520explore%250Amapless%2520navigation%2520in%2520the%2520embodied%2520AI%2520field%252C%2520in%2520particular%252C%2520to%2520utilize%250Aobject-goal%2520navigation%2520%2528commonly%2520abbreviated%2520as%2520ON%252C%2520ObjNav%252C%2520or%2520OGN%2529%2520techniques%250Athat%2520efficiently%2520explore%2520target%2520objects%2520without%2520using%2520such%2520a%2520prior%2520map.%250ASpecifically%252C%2520in%2520this%2520work%252C%2520we%2520start%2520from%2520an%2520off-the-shelf%2520mapless%2520ON%2520planner%252C%250Aextend%2520it%2520to%2520utilize%2520a%2520prior%2520map%252C%2520and%2520further%2520show%2520that%2520the%2520performance%2520in%250Along-distance%2520ALC%2520%2528LD-ALC%2529%2520can%2520be%2520maximized%2520by%2520minimizing%2520%2560%2560ALC%2520loss%2522%2520and%2520%2560%2560ON%250Aloss%2522.%2520This%2520study%2520highlights%2520a%2520simple%2520and%2520effective%2520approach%252C%2520called%2520ALC-ON%250A%2528ALCON%2529%252C%2520to%2520accelerate%2520the%2520progress%2520of%2520challenging%2520long-distance%2520ALC%2520technology%250Aby%2520leveraging%2520the%2520growing%2520frontier-guided%252C%2520data-driven%252C%2520and%2520LLM-guided%2520ON%250Atechnologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11523v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ON%20as%20ALC%3A%20Active%20Loop%20Closing%20Object%20Goal%20Navigation&entry.906535625=Daiki%20Iwata%20and%20Kanji%20Tanaka%20and%20Shoya%20Miyazaki%20and%20Kouki%20Terashima&entry.1292438233=%20%20In%20simultaneous%20localization%20and%20mapping%2C%20active%20loop%20closing%20%28ALC%29%20is%20an%0Aactive%20vision%20problem%20that%20aims%20to%20visually%20guide%20a%20robot%20to%20maximize%20the%0Achances%20of%20revisiting%20previously%20visited%20points%2C%20thereby%20resetting%20the%20drift%0Aerrors%20accumulated%20in%20the%20incrementally%20built%20map%20during%20travel.%20However%2C%0Acurrent%20mainstream%20navigation%20strategies%20that%20leverage%20such%20incomplete%20maps%20as%0Aworkspace%20prior%20knowledge%20often%20fail%20in%20modern%20long-term%20autonomy%20long-distance%0Atravel%20scenarios%20where%20map%20accumulation%20errors%20become%20significant.%20To%20address%0Athese%20limitations%20of%20map-based%20navigation%2C%20this%20paper%20is%20the%20first%20to%20explore%0Amapless%20navigation%20in%20the%20embodied%20AI%20field%2C%20in%20particular%2C%20to%20utilize%0Aobject-goal%20navigation%20%28commonly%20abbreviated%20as%20ON%2C%20ObjNav%2C%20or%20OGN%29%20techniques%0Athat%20efficiently%20explore%20target%20objects%20without%20using%20such%20a%20prior%20map.%0ASpecifically%2C%20in%20this%20work%2C%20we%20start%20from%20an%20off-the-shelf%20mapless%20ON%20planner%2C%0Aextend%20it%20to%20utilize%20a%20prior%20map%2C%20and%20further%20show%20that%20the%20performance%20in%0Along-distance%20ALC%20%28LD-ALC%29%20can%20be%20maximized%20by%20minimizing%20%60%60ALC%20loss%22%20and%20%60%60ON%0Aloss%22.%20This%20study%20highlights%20a%20simple%20and%20effective%20approach%2C%20called%20ALC-ON%0A%28ALCON%29%2C%20to%20accelerate%20the%20progress%20of%20challenging%20long-distance%20ALC%20technology%0Aby%20leveraging%20the%20growing%20frontier-guided%2C%20data-driven%2C%20and%20LLM-guided%20ON%0Atechnologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11523v2&entry.124074799=Read"},
{"title": "Graph-structured Small Molecule Drug Discovery Through Deep Learning:\n  Progress, Challenges, and Opportunities", "author": "Kun Li and Yida Xiong and Hongzhi Zhang and Xiantao Cai and Jia Wu and Bo Du and Wenbin Hu", "abstract": "  Due to their excellent drug-like and pharmacokinetic properties, small\nmolecule drugs are widely used to treat various diseases, making them a\ncritical component of drug discovery. In recent years, with the rapid\ndevelopment of deep learning (DL) techniques, DL-based small molecule drug\ndiscovery methods have achieved excellent performance in prediction accuracy,\nspeed, and complex molecular relationship modeling compared to traditional\nmachine learning approaches. These advancements enhance drug screening\nefficiency and optimization and provide more precise and effective solutions\nfor various drug discovery tasks. Contributing to this field's development,\nthis paper aims to systematically summarize and generalize the recent key tasks\nand representative techniques in graph-structured small molecule drug discovery\nin recent years. Specifically, we provide an overview of the major tasks in\nsmall molecule drug discovery and their interrelationships. Next, we analyze\nthe six core tasks, summarizing the related methods, commonly used datasets,\nand technological development trends. Finally, we discuss key challenges, such\nas interpretability and out-of-distribution generalization, and offer our\ninsights into future research directions for small molecule drug discovery.\n", "link": "http://arxiv.org/abs/2502.08975v2", "date": "2025-05-14", "relevancy": 2.3289, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4674}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-structured%20Small%20Molecule%20Drug%20Discovery%20Through%20Deep%20Learning%3A%0A%20%20Progress%2C%20Challenges%2C%20and%20Opportunities&body=Title%3A%20Graph-structured%20Small%20Molecule%20Drug%20Discovery%20Through%20Deep%20Learning%3A%0A%20%20Progress%2C%20Challenges%2C%20and%20Opportunities%0AAuthor%3A%20Kun%20Li%20and%20Yida%20Xiong%20and%20Hongzhi%20Zhang%20and%20Xiantao%20Cai%20and%20Jia%20Wu%20and%20Bo%20Du%20and%20Wenbin%20Hu%0AAbstract%3A%20%20%20Due%20to%20their%20excellent%20drug-like%20and%20pharmacokinetic%20properties%2C%20small%0Amolecule%20drugs%20are%20widely%20used%20to%20treat%20various%20diseases%2C%20making%20them%20a%0Acritical%20component%20of%20drug%20discovery.%20In%20recent%20years%2C%20with%20the%20rapid%0Adevelopment%20of%20deep%20learning%20%28DL%29%20techniques%2C%20DL-based%20small%20molecule%20drug%0Adiscovery%20methods%20have%20achieved%20excellent%20performance%20in%20prediction%20accuracy%2C%0Aspeed%2C%20and%20complex%20molecular%20relationship%20modeling%20compared%20to%20traditional%0Amachine%20learning%20approaches.%20These%20advancements%20enhance%20drug%20screening%0Aefficiency%20and%20optimization%20and%20provide%20more%20precise%20and%20effective%20solutions%0Afor%20various%20drug%20discovery%20tasks.%20Contributing%20to%20this%20field%27s%20development%2C%0Athis%20paper%20aims%20to%20systematically%20summarize%20and%20generalize%20the%20recent%20key%20tasks%0Aand%20representative%20techniques%20in%20graph-structured%20small%20molecule%20drug%20discovery%0Ain%20recent%20years.%20Specifically%2C%20we%20provide%20an%20overview%20of%20the%20major%20tasks%20in%0Asmall%20molecule%20drug%20discovery%20and%20their%20interrelationships.%20Next%2C%20we%20analyze%0Athe%20six%20core%20tasks%2C%20summarizing%20the%20related%20methods%2C%20commonly%20used%20datasets%2C%0Aand%20technological%20development%20trends.%20Finally%2C%20we%20discuss%20key%20challenges%2C%20such%0Aas%20interpretability%20and%20out-of-distribution%20generalization%2C%20and%20offer%20our%0Ainsights%20into%20future%20research%20directions%20for%20small%20molecule%20drug%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08975v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-structured%2520Small%2520Molecule%2520Drug%2520Discovery%2520Through%2520Deep%2520Learning%253A%250A%2520%2520Progress%252C%2520Challenges%252C%2520and%2520Opportunities%26entry.906535625%3DKun%2520Li%2520and%2520Yida%2520Xiong%2520and%2520Hongzhi%2520Zhang%2520and%2520Xiantao%2520Cai%2520and%2520Jia%2520Wu%2520and%2520Bo%2520Du%2520and%2520Wenbin%2520Hu%26entry.1292438233%3D%2520%2520Due%2520to%2520their%2520excellent%2520drug-like%2520and%2520pharmacokinetic%2520properties%252C%2520small%250Amolecule%2520drugs%2520are%2520widely%2520used%2520to%2520treat%2520various%2520diseases%252C%2520making%2520them%2520a%250Acritical%2520component%2520of%2520drug%2520discovery.%2520In%2520recent%2520years%252C%2520with%2520the%2520rapid%250Adevelopment%2520of%2520deep%2520learning%2520%2528DL%2529%2520techniques%252C%2520DL-based%2520small%2520molecule%2520drug%250Adiscovery%2520methods%2520have%2520achieved%2520excellent%2520performance%2520in%2520prediction%2520accuracy%252C%250Aspeed%252C%2520and%2520complex%2520molecular%2520relationship%2520modeling%2520compared%2520to%2520traditional%250Amachine%2520learning%2520approaches.%2520These%2520advancements%2520enhance%2520drug%2520screening%250Aefficiency%2520and%2520optimization%2520and%2520provide%2520more%2520precise%2520and%2520effective%2520solutions%250Afor%2520various%2520drug%2520discovery%2520tasks.%2520Contributing%2520to%2520this%2520field%2527s%2520development%252C%250Athis%2520paper%2520aims%2520to%2520systematically%2520summarize%2520and%2520generalize%2520the%2520recent%2520key%2520tasks%250Aand%2520representative%2520techniques%2520in%2520graph-structured%2520small%2520molecule%2520drug%2520discovery%250Ain%2520recent%2520years.%2520Specifically%252C%2520we%2520provide%2520an%2520overview%2520of%2520the%2520major%2520tasks%2520in%250Asmall%2520molecule%2520drug%2520discovery%2520and%2520their%2520interrelationships.%2520Next%252C%2520we%2520analyze%250Athe%2520six%2520core%2520tasks%252C%2520summarizing%2520the%2520related%2520methods%252C%2520commonly%2520used%2520datasets%252C%250Aand%2520technological%2520development%2520trends.%2520Finally%252C%2520we%2520discuss%2520key%2520challenges%252C%2520such%250Aas%2520interpretability%2520and%2520out-of-distribution%2520generalization%252C%2520and%2520offer%2520our%250Ainsights%2520into%2520future%2520research%2520directions%2520for%2520small%2520molecule%2520drug%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08975v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-structured%20Small%20Molecule%20Drug%20Discovery%20Through%20Deep%20Learning%3A%0A%20%20Progress%2C%20Challenges%2C%20and%20Opportunities&entry.906535625=Kun%20Li%20and%20Yida%20Xiong%20and%20Hongzhi%20Zhang%20and%20Xiantao%20Cai%20and%20Jia%20Wu%20and%20Bo%20Du%20and%20Wenbin%20Hu&entry.1292438233=%20%20Due%20to%20their%20excellent%20drug-like%20and%20pharmacokinetic%20properties%2C%20small%0Amolecule%20drugs%20are%20widely%20used%20to%20treat%20various%20diseases%2C%20making%20them%20a%0Acritical%20component%20of%20drug%20discovery.%20In%20recent%20years%2C%20with%20the%20rapid%0Adevelopment%20of%20deep%20learning%20%28DL%29%20techniques%2C%20DL-based%20small%20molecule%20drug%0Adiscovery%20methods%20have%20achieved%20excellent%20performance%20in%20prediction%20accuracy%2C%0Aspeed%2C%20and%20complex%20molecular%20relationship%20modeling%20compared%20to%20traditional%0Amachine%20learning%20approaches.%20These%20advancements%20enhance%20drug%20screening%0Aefficiency%20and%20optimization%20and%20provide%20more%20precise%20and%20effective%20solutions%0Afor%20various%20drug%20discovery%20tasks.%20Contributing%20to%20this%20field%27s%20development%2C%0Athis%20paper%20aims%20to%20systematically%20summarize%20and%20generalize%20the%20recent%20key%20tasks%0Aand%20representative%20techniques%20in%20graph-structured%20small%20molecule%20drug%20discovery%0Ain%20recent%20years.%20Specifically%2C%20we%20provide%20an%20overview%20of%20the%20major%20tasks%20in%0Asmall%20molecule%20drug%20discovery%20and%20their%20interrelationships.%20Next%2C%20we%20analyze%0Athe%20six%20core%20tasks%2C%20summarizing%20the%20related%20methods%2C%20commonly%20used%20datasets%2C%0Aand%20technological%20development%20trends.%20Finally%2C%20we%20discuss%20key%20challenges%2C%20such%0Aas%20interpretability%20and%20out-of-distribution%20generalization%2C%20and%20offer%20our%0Ainsights%20into%20future%20research%20directions%20for%20small%20molecule%20drug%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08975v2&entry.124074799=Read"},
{"title": "PRISM: A Unified Framework for Photorealistic Reconstruction and\n  Intrinsic Scene Modeling", "author": "Alara Dirik and Tuanfeng Wang and Duygu Ceylan and Stefanos Zafeiriou and Anna Fr\u00fchst\u00fcck", "abstract": "  We present PRISM, a unified framework that enables multiple image generation\nand editing tasks in a single foundational model. Starting from a pre-trained\ntext-to-image diffusion model, PRISM proposes an effective fine-tuning strategy\nto produce RGB images along with intrinsic maps (referred to as X layers)\nsimultaneously. Unlike previous approaches, which infer intrinsic properties\nindividually or require separate models for decomposition and conditional\ngeneration, PRISM maintains consistency across modalities by generating all\nintrinsic layers jointly. It supports diverse tasks, including text-to-RGBX\ngeneration, RGB-to-X decomposition, and X-to-RGBX conditional generation.\nAdditionally, PRISM enables both global and local image editing through\nconditioning on selected intrinsic layers and text prompts. Extensive\nexperiments demonstrate the competitive performance of PRISM both for intrinsic\nimage decomposition and conditional image generation while preserving the base\nmodel's text-to-image generation capability.\n", "link": "http://arxiv.org/abs/2504.14219v2", "date": "2025-05-14", "relevancy": 2.321, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRISM%3A%20A%20Unified%20Framework%20for%20Photorealistic%20Reconstruction%20and%0A%20%20Intrinsic%20Scene%20Modeling&body=Title%3A%20PRISM%3A%20A%20Unified%20Framework%20for%20Photorealistic%20Reconstruction%20and%0A%20%20Intrinsic%20Scene%20Modeling%0AAuthor%3A%20Alara%20Dirik%20and%20Tuanfeng%20Wang%20and%20Duygu%20Ceylan%20and%20Stefanos%20Zafeiriou%20and%20Anna%20Fr%C3%BChst%C3%BCck%0AAbstract%3A%20%20%20We%20present%20PRISM%2C%20a%20unified%20framework%20that%20enables%20multiple%20image%20generation%0Aand%20editing%20tasks%20in%20a%20single%20foundational%20model.%20Starting%20from%20a%20pre-trained%0Atext-to-image%20diffusion%20model%2C%20PRISM%20proposes%20an%20effective%20fine-tuning%20strategy%0Ato%20produce%20RGB%20images%20along%20with%20intrinsic%20maps%20%28referred%20to%20as%20X%20layers%29%0Asimultaneously.%20Unlike%20previous%20approaches%2C%20which%20infer%20intrinsic%20properties%0Aindividually%20or%20require%20separate%20models%20for%20decomposition%20and%20conditional%0Ageneration%2C%20PRISM%20maintains%20consistency%20across%20modalities%20by%20generating%20all%0Aintrinsic%20layers%20jointly.%20It%20supports%20diverse%20tasks%2C%20including%20text-to-RGBX%0Ageneration%2C%20RGB-to-X%20decomposition%2C%20and%20X-to-RGBX%20conditional%20generation.%0AAdditionally%2C%20PRISM%20enables%20both%20global%20and%20local%20image%20editing%20through%0Aconditioning%20on%20selected%20intrinsic%20layers%20and%20text%20prompts.%20Extensive%0Aexperiments%20demonstrate%20the%20competitive%20performance%20of%20PRISM%20both%20for%20intrinsic%0Aimage%20decomposition%20and%20conditional%20image%20generation%20while%20preserving%20the%20base%0Amodel%27s%20text-to-image%20generation%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14219v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRISM%253A%2520A%2520Unified%2520Framework%2520for%2520Photorealistic%2520Reconstruction%2520and%250A%2520%2520Intrinsic%2520Scene%2520Modeling%26entry.906535625%3DAlara%2520Dirik%2520and%2520Tuanfeng%2520Wang%2520and%2520Duygu%2520Ceylan%2520and%2520Stefanos%2520Zafeiriou%2520and%2520Anna%2520Fr%25C3%25BChst%25C3%25BCck%26entry.1292438233%3D%2520%2520We%2520present%2520PRISM%252C%2520a%2520unified%2520framework%2520that%2520enables%2520multiple%2520image%2520generation%250Aand%2520editing%2520tasks%2520in%2520a%2520single%2520foundational%2520model.%2520Starting%2520from%2520a%2520pre-trained%250Atext-to-image%2520diffusion%2520model%252C%2520PRISM%2520proposes%2520an%2520effective%2520fine-tuning%2520strategy%250Ato%2520produce%2520RGB%2520images%2520along%2520with%2520intrinsic%2520maps%2520%2528referred%2520to%2520as%2520X%2520layers%2529%250Asimultaneously.%2520Unlike%2520previous%2520approaches%252C%2520which%2520infer%2520intrinsic%2520properties%250Aindividually%2520or%2520require%2520separate%2520models%2520for%2520decomposition%2520and%2520conditional%250Ageneration%252C%2520PRISM%2520maintains%2520consistency%2520across%2520modalities%2520by%2520generating%2520all%250Aintrinsic%2520layers%2520jointly.%2520It%2520supports%2520diverse%2520tasks%252C%2520including%2520text-to-RGBX%250Ageneration%252C%2520RGB-to-X%2520decomposition%252C%2520and%2520X-to-RGBX%2520conditional%2520generation.%250AAdditionally%252C%2520PRISM%2520enables%2520both%2520global%2520and%2520local%2520image%2520editing%2520through%250Aconditioning%2520on%2520selected%2520intrinsic%2520layers%2520and%2520text%2520prompts.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520competitive%2520performance%2520of%2520PRISM%2520both%2520for%2520intrinsic%250Aimage%2520decomposition%2520and%2520conditional%2520image%2520generation%2520while%2520preserving%2520the%2520base%250Amodel%2527s%2520text-to-image%2520generation%2520capability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14219v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRISM%3A%20A%20Unified%20Framework%20for%20Photorealistic%20Reconstruction%20and%0A%20%20Intrinsic%20Scene%20Modeling&entry.906535625=Alara%20Dirik%20and%20Tuanfeng%20Wang%20and%20Duygu%20Ceylan%20and%20Stefanos%20Zafeiriou%20and%20Anna%20Fr%C3%BChst%C3%BCck&entry.1292438233=%20%20We%20present%20PRISM%2C%20a%20unified%20framework%20that%20enables%20multiple%20image%20generation%0Aand%20editing%20tasks%20in%20a%20single%20foundational%20model.%20Starting%20from%20a%20pre-trained%0Atext-to-image%20diffusion%20model%2C%20PRISM%20proposes%20an%20effective%20fine-tuning%20strategy%0Ato%20produce%20RGB%20images%20along%20with%20intrinsic%20maps%20%28referred%20to%20as%20X%20layers%29%0Asimultaneously.%20Unlike%20previous%20approaches%2C%20which%20infer%20intrinsic%20properties%0Aindividually%20or%20require%20separate%20models%20for%20decomposition%20and%20conditional%0Ageneration%2C%20PRISM%20maintains%20consistency%20across%20modalities%20by%20generating%20all%0Aintrinsic%20layers%20jointly.%20It%20supports%20diverse%20tasks%2C%20including%20text-to-RGBX%0Ageneration%2C%20RGB-to-X%20decomposition%2C%20and%20X-to-RGBX%20conditional%20generation.%0AAdditionally%2C%20PRISM%20enables%20both%20global%20and%20local%20image%20editing%20through%0Aconditioning%20on%20selected%20intrinsic%20layers%20and%20text%20prompts.%20Extensive%0Aexperiments%20demonstrate%20the%20competitive%20performance%20of%20PRISM%20both%20for%20intrinsic%0Aimage%20decomposition%20and%20conditional%20image%20generation%20while%20preserving%20the%20base%0Amodel%27s%20text-to-image%20generation%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14219v2&entry.124074799=Read"},
{"title": "Denoising and Alignment: Rethinking Domain Generalization for Multimodal\n  Face Anti-Spoofing", "author": "Yingjie Ma and Xun Lin and Zitong Yu and Xin Liu and Xiaochen Yuan and Weicheng Xie and Linlin Shen", "abstract": "  Face Anti-Spoofing (FAS) is essential for the security of facial recognition\nsystems in diverse scenarios such as payment processing and surveillance.\nCurrent multimodal FAS methods often struggle with effective generalization,\nmainly due to modality-specific biases and domain shifts. To address these\nchallenges, we introduce the \\textbf{M}ulti\\textbf{m}odal \\textbf{D}enoising\nand \\textbf{A}lignment (\\textbf{MMDA}) framework. By leveraging the zero-shot\ngeneralization capability of CLIP, the MMDA framework effectively suppresses\nnoise in multimodal data through denoising and alignment mechanisms, thereby\nsignificantly enhancing the generalization performance of cross-modal\nalignment. The \\textbf{M}odality-\\textbf{D}omain Joint \\textbf{D}ifferential\n\\textbf{A}ttention (\\textbf{MD2A}) module in MMDA concurrently mitigates the\nimpacts of domain and modality noise by refining the attention mechanism based\non extracted common noise features. Furthermore, the \\textbf{R}epresentation\n\\textbf{S}pace \\textbf{S}oft (\\textbf{RS2}) Alignment strategy utilizes the\npre-trained CLIP model to align multi-domain multimodal data into a generalized\nrepresentation space in a flexible manner, preserving intricate representations\nand enhancing the model's adaptability to various unseen conditions. We also\ndesign a \\textbf{U}-shaped \\textbf{D}ual \\textbf{S}pace \\textbf{A}daptation\n(\\textbf{U-DSA}) module to enhance the adaptability of representations while\nmaintaining generalization performance. These improvements not only enhance the\nframework's generalization capabilities but also boost its ability to represent\ncomplex representations. Our experimental results on four benchmark datasets\nunder different evaluation protocols demonstrate that the MMDA framework\noutperforms existing state-of-the-art methods in terms of cross-domain\ngeneralization and multimodal detection accuracy. The code will be released\nsoon.\n", "link": "http://arxiv.org/abs/2505.09484v1", "date": "2025-05-14", "relevancy": 2.3126, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.62}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.553}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Denoising%20and%20Alignment%3A%20Rethinking%20Domain%20Generalization%20for%20Multimodal%0A%20%20Face%20Anti-Spoofing&body=Title%3A%20Denoising%20and%20Alignment%3A%20Rethinking%20Domain%20Generalization%20for%20Multimodal%0A%20%20Face%20Anti-Spoofing%0AAuthor%3A%20Yingjie%20Ma%20and%20Xun%20Lin%20and%20Zitong%20Yu%20and%20Xin%20Liu%20and%20Xiaochen%20Yuan%20and%20Weicheng%20Xie%20and%20Linlin%20Shen%0AAbstract%3A%20%20%20Face%20Anti-Spoofing%20%28FAS%29%20is%20essential%20for%20the%20security%20of%20facial%20recognition%0Asystems%20in%20diverse%20scenarios%20such%20as%20payment%20processing%20and%20surveillance.%0ACurrent%20multimodal%20FAS%20methods%20often%20struggle%20with%20effective%20generalization%2C%0Amainly%20due%20to%20modality-specific%20biases%20and%20domain%20shifts.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20the%20%5Ctextbf%7BM%7Dulti%5Ctextbf%7Bm%7Dodal%20%5Ctextbf%7BD%7Denoising%0Aand%20%5Ctextbf%7BA%7Dlignment%20%28%5Ctextbf%7BMMDA%7D%29%20framework.%20By%20leveraging%20the%20zero-shot%0Ageneralization%20capability%20of%20CLIP%2C%20the%20MMDA%20framework%20effectively%20suppresses%0Anoise%20in%20multimodal%20data%20through%20denoising%20and%20alignment%20mechanisms%2C%20thereby%0Asignificantly%20enhancing%20the%20generalization%20performance%20of%20cross-modal%0Aalignment.%20The%20%5Ctextbf%7BM%7Dodality-%5Ctextbf%7BD%7Domain%20Joint%20%5Ctextbf%7BD%7Differential%0A%5Ctextbf%7BA%7Dttention%20%28%5Ctextbf%7BMD2A%7D%29%20module%20in%20MMDA%20concurrently%20mitigates%20the%0Aimpacts%20of%20domain%20and%20modality%20noise%20by%20refining%20the%20attention%20mechanism%20based%0Aon%20extracted%20common%20noise%20features.%20Furthermore%2C%20the%20%5Ctextbf%7BR%7Depresentation%0A%5Ctextbf%7BS%7Dpace%20%5Ctextbf%7BS%7Doft%20%28%5Ctextbf%7BRS2%7D%29%20Alignment%20strategy%20utilizes%20the%0Apre-trained%20CLIP%20model%20to%20align%20multi-domain%20multimodal%20data%20into%20a%20generalized%0Arepresentation%20space%20in%20a%20flexible%20manner%2C%20preserving%20intricate%20representations%0Aand%20enhancing%20the%20model%27s%20adaptability%20to%20various%20unseen%20conditions.%20We%20also%0Adesign%20a%20%5Ctextbf%7BU%7D-shaped%20%5Ctextbf%7BD%7Dual%20%5Ctextbf%7BS%7Dpace%20%5Ctextbf%7BA%7Ddaptation%0A%28%5Ctextbf%7BU-DSA%7D%29%20module%20to%20enhance%20the%20adaptability%20of%20representations%20while%0Amaintaining%20generalization%20performance.%20These%20improvements%20not%20only%20enhance%20the%0Aframework%27s%20generalization%20capabilities%20but%20also%20boost%20its%20ability%20to%20represent%0Acomplex%20representations.%20Our%20experimental%20results%20on%20four%20benchmark%20datasets%0Aunder%20different%20evaluation%20protocols%20demonstrate%20that%20the%20MMDA%20framework%0Aoutperforms%20existing%20state-of-the-art%20methods%20in%20terms%20of%20cross-domain%0Ageneralization%20and%20multimodal%20detection%20accuracy.%20The%20code%20will%20be%20released%0Asoon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenoising%2520and%2520Alignment%253A%2520Rethinking%2520Domain%2520Generalization%2520for%2520Multimodal%250A%2520%2520Face%2520Anti-Spoofing%26entry.906535625%3DYingjie%2520Ma%2520and%2520Xun%2520Lin%2520and%2520Zitong%2520Yu%2520and%2520Xin%2520Liu%2520and%2520Xiaochen%2520Yuan%2520and%2520Weicheng%2520Xie%2520and%2520Linlin%2520Shen%26entry.1292438233%3D%2520%2520Face%2520Anti-Spoofing%2520%2528FAS%2529%2520is%2520essential%2520for%2520the%2520security%2520of%2520facial%2520recognition%250Asystems%2520in%2520diverse%2520scenarios%2520such%2520as%2520payment%2520processing%2520and%2520surveillance.%250ACurrent%2520multimodal%2520FAS%2520methods%2520often%2520struggle%2520with%2520effective%2520generalization%252C%250Amainly%2520due%2520to%2520modality-specific%2520biases%2520and%2520domain%2520shifts.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520the%2520%255Ctextbf%257BM%257Dulti%255Ctextbf%257Bm%257Dodal%2520%255Ctextbf%257BD%257Denoising%250Aand%2520%255Ctextbf%257BA%257Dlignment%2520%2528%255Ctextbf%257BMMDA%257D%2529%2520framework.%2520By%2520leveraging%2520the%2520zero-shot%250Ageneralization%2520capability%2520of%2520CLIP%252C%2520the%2520MMDA%2520framework%2520effectively%2520suppresses%250Anoise%2520in%2520multimodal%2520data%2520through%2520denoising%2520and%2520alignment%2520mechanisms%252C%2520thereby%250Asignificantly%2520enhancing%2520the%2520generalization%2520performance%2520of%2520cross-modal%250Aalignment.%2520The%2520%255Ctextbf%257BM%257Dodality-%255Ctextbf%257BD%257Domain%2520Joint%2520%255Ctextbf%257BD%257Differential%250A%255Ctextbf%257BA%257Dttention%2520%2528%255Ctextbf%257BMD2A%257D%2529%2520module%2520in%2520MMDA%2520concurrently%2520mitigates%2520the%250Aimpacts%2520of%2520domain%2520and%2520modality%2520noise%2520by%2520refining%2520the%2520attention%2520mechanism%2520based%250Aon%2520extracted%2520common%2520noise%2520features.%2520Furthermore%252C%2520the%2520%255Ctextbf%257BR%257Depresentation%250A%255Ctextbf%257BS%257Dpace%2520%255Ctextbf%257BS%257Doft%2520%2528%255Ctextbf%257BRS2%257D%2529%2520Alignment%2520strategy%2520utilizes%2520the%250Apre-trained%2520CLIP%2520model%2520to%2520align%2520multi-domain%2520multimodal%2520data%2520into%2520a%2520generalized%250Arepresentation%2520space%2520in%2520a%2520flexible%2520manner%252C%2520preserving%2520intricate%2520representations%250Aand%2520enhancing%2520the%2520model%2527s%2520adaptability%2520to%2520various%2520unseen%2520conditions.%2520We%2520also%250Adesign%2520a%2520%255Ctextbf%257BU%257D-shaped%2520%255Ctextbf%257BD%257Dual%2520%255Ctextbf%257BS%257Dpace%2520%255Ctextbf%257BA%257Ddaptation%250A%2528%255Ctextbf%257BU-DSA%257D%2529%2520module%2520to%2520enhance%2520the%2520adaptability%2520of%2520representations%2520while%250Amaintaining%2520generalization%2520performance.%2520These%2520improvements%2520not%2520only%2520enhance%2520the%250Aframework%2527s%2520generalization%2520capabilities%2520but%2520also%2520boost%2520its%2520ability%2520to%2520represent%250Acomplex%2520representations.%2520Our%2520experimental%2520results%2520on%2520four%2520benchmark%2520datasets%250Aunder%2520different%2520evaluation%2520protocols%2520demonstrate%2520that%2520the%2520MMDA%2520framework%250Aoutperforms%2520existing%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520cross-domain%250Ageneralization%2520and%2520multimodal%2520detection%2520accuracy.%2520The%2520code%2520will%2520be%2520released%250Asoon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Denoising%20and%20Alignment%3A%20Rethinking%20Domain%20Generalization%20for%20Multimodal%0A%20%20Face%20Anti-Spoofing&entry.906535625=Yingjie%20Ma%20and%20Xun%20Lin%20and%20Zitong%20Yu%20and%20Xin%20Liu%20and%20Xiaochen%20Yuan%20and%20Weicheng%20Xie%20and%20Linlin%20Shen&entry.1292438233=%20%20Face%20Anti-Spoofing%20%28FAS%29%20is%20essential%20for%20the%20security%20of%20facial%20recognition%0Asystems%20in%20diverse%20scenarios%20such%20as%20payment%20processing%20and%20surveillance.%0ACurrent%20multimodal%20FAS%20methods%20often%20struggle%20with%20effective%20generalization%2C%0Amainly%20due%20to%20modality-specific%20biases%20and%20domain%20shifts.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20the%20%5Ctextbf%7BM%7Dulti%5Ctextbf%7Bm%7Dodal%20%5Ctextbf%7BD%7Denoising%0Aand%20%5Ctextbf%7BA%7Dlignment%20%28%5Ctextbf%7BMMDA%7D%29%20framework.%20By%20leveraging%20the%20zero-shot%0Ageneralization%20capability%20of%20CLIP%2C%20the%20MMDA%20framework%20effectively%20suppresses%0Anoise%20in%20multimodal%20data%20through%20denoising%20and%20alignment%20mechanisms%2C%20thereby%0Asignificantly%20enhancing%20the%20generalization%20performance%20of%20cross-modal%0Aalignment.%20The%20%5Ctextbf%7BM%7Dodality-%5Ctextbf%7BD%7Domain%20Joint%20%5Ctextbf%7BD%7Differential%0A%5Ctextbf%7BA%7Dttention%20%28%5Ctextbf%7BMD2A%7D%29%20module%20in%20MMDA%20concurrently%20mitigates%20the%0Aimpacts%20of%20domain%20and%20modality%20noise%20by%20refining%20the%20attention%20mechanism%20based%0Aon%20extracted%20common%20noise%20features.%20Furthermore%2C%20the%20%5Ctextbf%7BR%7Depresentation%0A%5Ctextbf%7BS%7Dpace%20%5Ctextbf%7BS%7Doft%20%28%5Ctextbf%7BRS2%7D%29%20Alignment%20strategy%20utilizes%20the%0Apre-trained%20CLIP%20model%20to%20align%20multi-domain%20multimodal%20data%20into%20a%20generalized%0Arepresentation%20space%20in%20a%20flexible%20manner%2C%20preserving%20intricate%20representations%0Aand%20enhancing%20the%20model%27s%20adaptability%20to%20various%20unseen%20conditions.%20We%20also%0Adesign%20a%20%5Ctextbf%7BU%7D-shaped%20%5Ctextbf%7BD%7Dual%20%5Ctextbf%7BS%7Dpace%20%5Ctextbf%7BA%7Ddaptation%0A%28%5Ctextbf%7BU-DSA%7D%29%20module%20to%20enhance%20the%20adaptability%20of%20representations%20while%0Amaintaining%20generalization%20performance.%20These%20improvements%20not%20only%20enhance%20the%0Aframework%27s%20generalization%20capabilities%20but%20also%20boost%20its%20ability%20to%20represent%0Acomplex%20representations.%20Our%20experimental%20results%20on%20four%20benchmark%20datasets%0Aunder%20different%20evaluation%20protocols%20demonstrate%20that%20the%20MMDA%20framework%0Aoutperforms%20existing%20state-of-the-art%20methods%20in%20terms%20of%20cross-domain%0Ageneralization%20and%20multimodal%20detection%20accuracy.%20The%20code%20will%20be%20released%0Asoon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09484v1&entry.124074799=Read"},
{"title": "Decentralized Nonlinear Model Predictive Control-Based Flock Navigation\n  with Real-Time Obstacle Avoidance in Unknown Obstructed Environments", "author": "Nuthasith Gerdpratoom and Kaoru Yamamoto", "abstract": "  This work extends our prior work on the distributed nonlinear model\npredictive control (NMPC) for navigating a robot fleet following a certain\nflocking behavior in unknown obstructed environments with a more realistic\nlocal obstacle avoidance strategy. More specifically, we integrate the local\nobstacle avoidance constraint using point clouds into the NMPC framework. Here,\neach agent relies on data from its local sensor to perceive and respond to\nnearby obstacles. A point cloud processing technique is presented for both\ntwo-dimensional and three-dimensional point clouds to minimize the\ncomputational burden during the optimization. The process consists of\ndirectional filtering and down-sampling that significantly reduce the number of\ndata points. The algorithm's performance is validated through realistic 3D\nsimulations in Gazebo, and its practical feasibility is further explored via\nhardware-in-the-loop (HIL) simulations on embedded platforms.\n", "link": "http://arxiv.org/abs/2505.09434v1", "date": "2025-05-14", "relevancy": 2.3063, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5989}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5678}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Nonlinear%20Model%20Predictive%20Control-Based%20Flock%20Navigation%0A%20%20with%20Real-Time%20Obstacle%20Avoidance%20in%20Unknown%20Obstructed%20Environments&body=Title%3A%20Decentralized%20Nonlinear%20Model%20Predictive%20Control-Based%20Flock%20Navigation%0A%20%20with%20Real-Time%20Obstacle%20Avoidance%20in%20Unknown%20Obstructed%20Environments%0AAuthor%3A%20Nuthasith%20Gerdpratoom%20and%20Kaoru%20Yamamoto%0AAbstract%3A%20%20%20This%20work%20extends%20our%20prior%20work%20on%20the%20distributed%20nonlinear%20model%0Apredictive%20control%20%28NMPC%29%20for%20navigating%20a%20robot%20fleet%20following%20a%20certain%0Aflocking%20behavior%20in%20unknown%20obstructed%20environments%20with%20a%20more%20realistic%0Alocal%20obstacle%20avoidance%20strategy.%20More%20specifically%2C%20we%20integrate%20the%20local%0Aobstacle%20avoidance%20constraint%20using%20point%20clouds%20into%20the%20NMPC%20framework.%20Here%2C%0Aeach%20agent%20relies%20on%20data%20from%20its%20local%20sensor%20to%20perceive%20and%20respond%20to%0Anearby%20obstacles.%20A%20point%20cloud%20processing%20technique%20is%20presented%20for%20both%0Atwo-dimensional%20and%20three-dimensional%20point%20clouds%20to%20minimize%20the%0Acomputational%20burden%20during%20the%20optimization.%20The%20process%20consists%20of%0Adirectional%20filtering%20and%20down-sampling%20that%20significantly%20reduce%20the%20number%20of%0Adata%20points.%20The%20algorithm%27s%20performance%20is%20validated%20through%20realistic%203D%0Asimulations%20in%20Gazebo%2C%20and%20its%20practical%20feasibility%20is%20further%20explored%20via%0Ahardware-in-the-loop%20%28HIL%29%20simulations%20on%20embedded%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Nonlinear%2520Model%2520Predictive%2520Control-Based%2520Flock%2520Navigation%250A%2520%2520with%2520Real-Time%2520Obstacle%2520Avoidance%2520in%2520Unknown%2520Obstructed%2520Environments%26entry.906535625%3DNuthasith%2520Gerdpratoom%2520and%2520Kaoru%2520Yamamoto%26entry.1292438233%3D%2520%2520This%2520work%2520extends%2520our%2520prior%2520work%2520on%2520the%2520distributed%2520nonlinear%2520model%250Apredictive%2520control%2520%2528NMPC%2529%2520for%2520navigating%2520a%2520robot%2520fleet%2520following%2520a%2520certain%250Aflocking%2520behavior%2520in%2520unknown%2520obstructed%2520environments%2520with%2520a%2520more%2520realistic%250Alocal%2520obstacle%2520avoidance%2520strategy.%2520More%2520specifically%252C%2520we%2520integrate%2520the%2520local%250Aobstacle%2520avoidance%2520constraint%2520using%2520point%2520clouds%2520into%2520the%2520NMPC%2520framework.%2520Here%252C%250Aeach%2520agent%2520relies%2520on%2520data%2520from%2520its%2520local%2520sensor%2520to%2520perceive%2520and%2520respond%2520to%250Anearby%2520obstacles.%2520A%2520point%2520cloud%2520processing%2520technique%2520is%2520presented%2520for%2520both%250Atwo-dimensional%2520and%2520three-dimensional%2520point%2520clouds%2520to%2520minimize%2520the%250Acomputational%2520burden%2520during%2520the%2520optimization.%2520The%2520process%2520consists%2520of%250Adirectional%2520filtering%2520and%2520down-sampling%2520that%2520significantly%2520reduce%2520the%2520number%2520of%250Adata%2520points.%2520The%2520algorithm%2527s%2520performance%2520is%2520validated%2520through%2520realistic%25203D%250Asimulations%2520in%2520Gazebo%252C%2520and%2520its%2520practical%2520feasibility%2520is%2520further%2520explored%2520via%250Ahardware-in-the-loop%2520%2528HIL%2529%2520simulations%2520on%2520embedded%2520platforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Nonlinear%20Model%20Predictive%20Control-Based%20Flock%20Navigation%0A%20%20with%20Real-Time%20Obstacle%20Avoidance%20in%20Unknown%20Obstructed%20Environments&entry.906535625=Nuthasith%20Gerdpratoom%20and%20Kaoru%20Yamamoto&entry.1292438233=%20%20This%20work%20extends%20our%20prior%20work%20on%20the%20distributed%20nonlinear%20model%0Apredictive%20control%20%28NMPC%29%20for%20navigating%20a%20robot%20fleet%20following%20a%20certain%0Aflocking%20behavior%20in%20unknown%20obstructed%20environments%20with%20a%20more%20realistic%0Alocal%20obstacle%20avoidance%20strategy.%20More%20specifically%2C%20we%20integrate%20the%20local%0Aobstacle%20avoidance%20constraint%20using%20point%20clouds%20into%20the%20NMPC%20framework.%20Here%2C%0Aeach%20agent%20relies%20on%20data%20from%20its%20local%20sensor%20to%20perceive%20and%20respond%20to%0Anearby%20obstacles.%20A%20point%20cloud%20processing%20technique%20is%20presented%20for%20both%0Atwo-dimensional%20and%20three-dimensional%20point%20clouds%20to%20minimize%20the%0Acomputational%20burden%20during%20the%20optimization.%20The%20process%20consists%20of%0Adirectional%20filtering%20and%20down-sampling%20that%20significantly%20reduce%20the%20number%20of%0Adata%20points.%20The%20algorithm%27s%20performance%20is%20validated%20through%20realistic%203D%0Asimulations%20in%20Gazebo%2C%20and%20its%20practical%20feasibility%20is%20further%20explored%20via%0Ahardware-in-the-loop%20%28HIL%29%20simulations%20on%20embedded%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09434v1&entry.124074799=Read"},
{"title": "Exploring Pose-Guided Imitation Learning for Robotic Precise Insertion", "author": "Han Sun and Yizhao Wang and Zhenning Zhou and Shuai Wang and Haibo Yang and Jingyuan Sun and Qixin Cao", "abstract": "  Recent studies have proved that imitation learning shows strong potential in\nthe field of robotic manipulation. However, existing methods still struggle\nwith precision manipulation task and rely on inefficient image/point cloud\nobservations. In this paper, we explore to introduce SE(3) object pose into\nimitation learning and propose the pose-guided efficient imitation learning\nmethods for robotic precise insertion task. First, we propose a precise\ninsertion diffusion policy which utilizes the relative SE(3) pose as the\nobservation-action pair. The policy models the source object SE(3) pose\ntrajectory relative to the target object. Second, we explore to introduce the\nRGBD data to the pose-guided diffusion policy. Specifically, we design a\ngoal-conditioned RGBD encoder to capture the discrepancy between the current\nstate and the goal state. In addition, a pose-guided residual gated fusion\nmethod is proposed, which takes pose features as the backbone, and the RGBD\nfeatures selectively compensate for pose feature deficiencies through an\nadaptive gating mechanism. Our methods are evaluated on 6 robotic precise\ninsertion tasks, demonstrating competitive performance with only 7-10\ndemonstrations. Experiments demonstrate that the proposed methods can\nsuccessfully complete precision insertion tasks with a clearance of about 0.01\nmm. Experimental results highlight its superior efficiency and generalization\ncapability compared to existing baselines. Code will be available at\nhttps://github.com/sunhan1997/PoseInsert.\n", "link": "http://arxiv.org/abs/2505.09424v1", "date": "2025-05-14", "relevancy": 2.2853, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5765}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5689}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Pose-Guided%20Imitation%20Learning%20for%20Robotic%20Precise%20Insertion&body=Title%3A%20Exploring%20Pose-Guided%20Imitation%20Learning%20for%20Robotic%20Precise%20Insertion%0AAuthor%3A%20Han%20Sun%20and%20Yizhao%20Wang%20and%20Zhenning%20Zhou%20and%20Shuai%20Wang%20and%20Haibo%20Yang%20and%20Jingyuan%20Sun%20and%20Qixin%20Cao%0AAbstract%3A%20%20%20Recent%20studies%20have%20proved%20that%20imitation%20learning%20shows%20strong%20potential%20in%0Athe%20field%20of%20robotic%20manipulation.%20However%2C%20existing%20methods%20still%20struggle%0Awith%20precision%20manipulation%20task%20and%20rely%20on%20inefficient%20image/point%20cloud%0Aobservations.%20In%20this%20paper%2C%20we%20explore%20to%20introduce%20SE%283%29%20object%20pose%20into%0Aimitation%20learning%20and%20propose%20the%20pose-guided%20efficient%20imitation%20learning%0Amethods%20for%20robotic%20precise%20insertion%20task.%20First%2C%20we%20propose%20a%20precise%0Ainsertion%20diffusion%20policy%20which%20utilizes%20the%20relative%20SE%283%29%20pose%20as%20the%0Aobservation-action%20pair.%20The%20policy%20models%20the%20source%20object%20SE%283%29%20pose%0Atrajectory%20relative%20to%20the%20target%20object.%20Second%2C%20we%20explore%20to%20introduce%20the%0ARGBD%20data%20to%20the%20pose-guided%20diffusion%20policy.%20Specifically%2C%20we%20design%20a%0Agoal-conditioned%20RGBD%20encoder%20to%20capture%20the%20discrepancy%20between%20the%20current%0Astate%20and%20the%20goal%20state.%20In%20addition%2C%20a%20pose-guided%20residual%20gated%20fusion%0Amethod%20is%20proposed%2C%20which%20takes%20pose%20features%20as%20the%20backbone%2C%20and%20the%20RGBD%0Afeatures%20selectively%20compensate%20for%20pose%20feature%20deficiencies%20through%20an%0Aadaptive%20gating%20mechanism.%20Our%20methods%20are%20evaluated%20on%206%20robotic%20precise%0Ainsertion%20tasks%2C%20demonstrating%20competitive%20performance%20with%20only%207-10%0Ademonstrations.%20Experiments%20demonstrate%20that%20the%20proposed%20methods%20can%0Asuccessfully%20complete%20precision%20insertion%20tasks%20with%20a%20clearance%20of%20about%200.01%0Amm.%20Experimental%20results%20highlight%20its%20superior%20efficiency%20and%20generalization%0Acapability%20compared%20to%20existing%20baselines.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/sunhan1997/PoseInsert.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Pose-Guided%2520Imitation%2520Learning%2520for%2520Robotic%2520Precise%2520Insertion%26entry.906535625%3DHan%2520Sun%2520and%2520Yizhao%2520Wang%2520and%2520Zhenning%2520Zhou%2520and%2520Shuai%2520Wang%2520and%2520Haibo%2520Yang%2520and%2520Jingyuan%2520Sun%2520and%2520Qixin%2520Cao%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520proved%2520that%2520imitation%2520learning%2520shows%2520strong%2520potential%2520in%250Athe%2520field%2520of%2520robotic%2520manipulation.%2520However%252C%2520existing%2520methods%2520still%2520struggle%250Awith%2520precision%2520manipulation%2520task%2520and%2520rely%2520on%2520inefficient%2520image/point%2520cloud%250Aobservations.%2520In%2520this%2520paper%252C%2520we%2520explore%2520to%2520introduce%2520SE%25283%2529%2520object%2520pose%2520into%250Aimitation%2520learning%2520and%2520propose%2520the%2520pose-guided%2520efficient%2520imitation%2520learning%250Amethods%2520for%2520robotic%2520precise%2520insertion%2520task.%2520First%252C%2520we%2520propose%2520a%2520precise%250Ainsertion%2520diffusion%2520policy%2520which%2520utilizes%2520the%2520relative%2520SE%25283%2529%2520pose%2520as%2520the%250Aobservation-action%2520pair.%2520The%2520policy%2520models%2520the%2520source%2520object%2520SE%25283%2529%2520pose%250Atrajectory%2520relative%2520to%2520the%2520target%2520object.%2520Second%252C%2520we%2520explore%2520to%2520introduce%2520the%250ARGBD%2520data%2520to%2520the%2520pose-guided%2520diffusion%2520policy.%2520Specifically%252C%2520we%2520design%2520a%250Agoal-conditioned%2520RGBD%2520encoder%2520to%2520capture%2520the%2520discrepancy%2520between%2520the%2520current%250Astate%2520and%2520the%2520goal%2520state.%2520In%2520addition%252C%2520a%2520pose-guided%2520residual%2520gated%2520fusion%250Amethod%2520is%2520proposed%252C%2520which%2520takes%2520pose%2520features%2520as%2520the%2520backbone%252C%2520and%2520the%2520RGBD%250Afeatures%2520selectively%2520compensate%2520for%2520pose%2520feature%2520deficiencies%2520through%2520an%250Aadaptive%2520gating%2520mechanism.%2520Our%2520methods%2520are%2520evaluated%2520on%25206%2520robotic%2520precise%250Ainsertion%2520tasks%252C%2520demonstrating%2520competitive%2520performance%2520with%2520only%25207-10%250Ademonstrations.%2520Experiments%2520demonstrate%2520that%2520the%2520proposed%2520methods%2520can%250Asuccessfully%2520complete%2520precision%2520insertion%2520tasks%2520with%2520a%2520clearance%2520of%2520about%25200.01%250Amm.%2520Experimental%2520results%2520highlight%2520its%2520superior%2520efficiency%2520and%2520generalization%250Acapability%2520compared%2520to%2520existing%2520baselines.%2520Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/sunhan1997/PoseInsert.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Pose-Guided%20Imitation%20Learning%20for%20Robotic%20Precise%20Insertion&entry.906535625=Han%20Sun%20and%20Yizhao%20Wang%20and%20Zhenning%20Zhou%20and%20Shuai%20Wang%20and%20Haibo%20Yang%20and%20Jingyuan%20Sun%20and%20Qixin%20Cao&entry.1292438233=%20%20Recent%20studies%20have%20proved%20that%20imitation%20learning%20shows%20strong%20potential%20in%0Athe%20field%20of%20robotic%20manipulation.%20However%2C%20existing%20methods%20still%20struggle%0Awith%20precision%20manipulation%20task%20and%20rely%20on%20inefficient%20image/point%20cloud%0Aobservations.%20In%20this%20paper%2C%20we%20explore%20to%20introduce%20SE%283%29%20object%20pose%20into%0Aimitation%20learning%20and%20propose%20the%20pose-guided%20efficient%20imitation%20learning%0Amethods%20for%20robotic%20precise%20insertion%20task.%20First%2C%20we%20propose%20a%20precise%0Ainsertion%20diffusion%20policy%20which%20utilizes%20the%20relative%20SE%283%29%20pose%20as%20the%0Aobservation-action%20pair.%20The%20policy%20models%20the%20source%20object%20SE%283%29%20pose%0Atrajectory%20relative%20to%20the%20target%20object.%20Second%2C%20we%20explore%20to%20introduce%20the%0ARGBD%20data%20to%20the%20pose-guided%20diffusion%20policy.%20Specifically%2C%20we%20design%20a%0Agoal-conditioned%20RGBD%20encoder%20to%20capture%20the%20discrepancy%20between%20the%20current%0Astate%20and%20the%20goal%20state.%20In%20addition%2C%20a%20pose-guided%20residual%20gated%20fusion%0Amethod%20is%20proposed%2C%20which%20takes%20pose%20features%20as%20the%20backbone%2C%20and%20the%20RGBD%0Afeatures%20selectively%20compensate%20for%20pose%20feature%20deficiencies%20through%20an%0Aadaptive%20gating%20mechanism.%20Our%20methods%20are%20evaluated%20on%206%20robotic%20precise%0Ainsertion%20tasks%2C%20demonstrating%20competitive%20performance%20with%20only%207-10%0Ademonstrations.%20Experiments%20demonstrate%20that%20the%20proposed%20methods%20can%0Asuccessfully%20complete%20precision%20insertion%20tasks%20with%20a%20clearance%20of%20about%200.01%0Amm.%20Experimental%20results%20highlight%20its%20superior%20efficiency%20and%20generalization%0Acapability%20compared%20to%20existing%20baselines.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/sunhan1997/PoseInsert.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09424v1&entry.124074799=Read"},
{"title": "MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D\n  Object Detection", "author": "Xiangyuan Peng and Yu Wang and Miao Tang and Bierzynski Kay and Lorenzo Servadei and Robert Wille", "abstract": "  Reliable autonomous driving systems require accurate detection of traffic\nparticipants. To this end, multi-modal fusion has emerged as an effective\nstrategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame\nradar point clouds have demonstrated the effectiveness in bridging the point\ndensity gap. However, they often neglect radar point clouds' inter-frame\nmisalignment caused by object movement during accumulation and do not fully\nexploit the object dynamic information from 4D radar. In this paper, we propose\nMoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for\nrobust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is\ndesigned to compensate for inter-frame radar misalignment from moving objects.\nLater, a Motion Attention Gated Fusion (MAGF) module integrate radar motion\nfeatures to guide LiDAR features to focus on dynamic foreground objects.\nExtensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL\noutperforms existing methods, achieving the highest mAP of 73.30% in the entire\narea and 88.68% in the driving corridor. Notably, our method also achieves the\nbest AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in\nthe driving corridor.\n", "link": "http://arxiv.org/abs/2505.09422v1", "date": "2025-05-14", "relevancy": 2.271, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5823}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5618}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoRAL%3A%20Motion-aware%20Multi-Frame%204D%20Radar%20and%20LiDAR%20Fusion%20for%20Robust%203D%0A%20%20Object%20Detection&body=Title%3A%20MoRAL%3A%20Motion-aware%20Multi-Frame%204D%20Radar%20and%20LiDAR%20Fusion%20for%20Robust%203D%0A%20%20Object%20Detection%0AAuthor%3A%20Xiangyuan%20Peng%20and%20Yu%20Wang%20and%20Miao%20Tang%20and%20Bierzynski%20Kay%20and%20Lorenzo%20Servadei%20and%20Robert%20Wille%0AAbstract%3A%20%20%20Reliable%20autonomous%20driving%20systems%20require%20accurate%20detection%20of%20traffic%0Aparticipants.%20To%20this%20end%2C%20multi-modal%20fusion%20has%20emerged%20as%20an%20effective%0Astrategy.%20In%20particular%2C%204D%20radar%20and%20LiDAR%20fusion%20methods%20based%20on%20multi-frame%0Aradar%20point%20clouds%20have%20demonstrated%20the%20effectiveness%20in%20bridging%20the%20point%0Adensity%20gap.%20However%2C%20they%20often%20neglect%20radar%20point%20clouds%27%20inter-frame%0Amisalignment%20caused%20by%20object%20movement%20during%20accumulation%20and%20do%20not%20fully%0Aexploit%20the%20object%20dynamic%20information%20from%204D%20radar.%20In%20this%20paper%2C%20we%20propose%0AMoRAL%2C%20a%20motion-aware%20multi-frame%204D%20radar%20and%20LiDAR%20fusion%20framework%20for%0Arobust%203D%20object%20detection.%20First%2C%20a%20Motion-aware%20Radar%20Encoder%20%28MRE%29%20is%0Adesigned%20to%20compensate%20for%20inter-frame%20radar%20misalignment%20from%20moving%20objects.%0ALater%2C%20a%20Motion%20Attention%20Gated%20Fusion%20%28MAGF%29%20module%20integrate%20radar%20motion%0Afeatures%20to%20guide%20LiDAR%20features%20to%20focus%20on%20dynamic%20foreground%20objects.%0AExtensive%20evaluations%20on%20the%20View-of-Delft%20%28VoD%29%20dataset%20demonstrate%20that%20MoRAL%0Aoutperforms%20existing%20methods%2C%20achieving%20the%20highest%20mAP%20of%2073.30%25%20in%20the%20entire%0Aarea%20and%2088.68%25%20in%20the%20driving%20corridor.%20Notably%2C%20our%20method%20also%20achieves%20the%0Abest%20AP%20of%2069.67%25%20for%20pedestrians%20in%20the%20entire%20area%20and%2096.25%25%20for%20cyclists%20in%0Athe%20driving%20corridor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoRAL%253A%2520Motion-aware%2520Multi-Frame%25204D%2520Radar%2520and%2520LiDAR%2520Fusion%2520for%2520Robust%25203D%250A%2520%2520Object%2520Detection%26entry.906535625%3DXiangyuan%2520Peng%2520and%2520Yu%2520Wang%2520and%2520Miao%2520Tang%2520and%2520Bierzynski%2520Kay%2520and%2520Lorenzo%2520Servadei%2520and%2520Robert%2520Wille%26entry.1292438233%3D%2520%2520Reliable%2520autonomous%2520driving%2520systems%2520require%2520accurate%2520detection%2520of%2520traffic%250Aparticipants.%2520To%2520this%2520end%252C%2520multi-modal%2520fusion%2520has%2520emerged%2520as%2520an%2520effective%250Astrategy.%2520In%2520particular%252C%25204D%2520radar%2520and%2520LiDAR%2520fusion%2520methods%2520based%2520on%2520multi-frame%250Aradar%2520point%2520clouds%2520have%2520demonstrated%2520the%2520effectiveness%2520in%2520bridging%2520the%2520point%250Adensity%2520gap.%2520However%252C%2520they%2520often%2520neglect%2520radar%2520point%2520clouds%2527%2520inter-frame%250Amisalignment%2520caused%2520by%2520object%2520movement%2520during%2520accumulation%2520and%2520do%2520not%2520fully%250Aexploit%2520the%2520object%2520dynamic%2520information%2520from%25204D%2520radar.%2520In%2520this%2520paper%252C%2520we%2520propose%250AMoRAL%252C%2520a%2520motion-aware%2520multi-frame%25204D%2520radar%2520and%2520LiDAR%2520fusion%2520framework%2520for%250Arobust%25203D%2520object%2520detection.%2520First%252C%2520a%2520Motion-aware%2520Radar%2520Encoder%2520%2528MRE%2529%2520is%250Adesigned%2520to%2520compensate%2520for%2520inter-frame%2520radar%2520misalignment%2520from%2520moving%2520objects.%250ALater%252C%2520a%2520Motion%2520Attention%2520Gated%2520Fusion%2520%2528MAGF%2529%2520module%2520integrate%2520radar%2520motion%250Afeatures%2520to%2520guide%2520LiDAR%2520features%2520to%2520focus%2520on%2520dynamic%2520foreground%2520objects.%250AExtensive%2520evaluations%2520on%2520the%2520View-of-Delft%2520%2528VoD%2529%2520dataset%2520demonstrate%2520that%2520MoRAL%250Aoutperforms%2520existing%2520methods%252C%2520achieving%2520the%2520highest%2520mAP%2520of%252073.30%2525%2520in%2520the%2520entire%250Aarea%2520and%252088.68%2525%2520in%2520the%2520driving%2520corridor.%2520Notably%252C%2520our%2520method%2520also%2520achieves%2520the%250Abest%2520AP%2520of%252069.67%2525%2520for%2520pedestrians%2520in%2520the%2520entire%2520area%2520and%252096.25%2525%2520for%2520cyclists%2520in%250Athe%2520driving%2520corridor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoRAL%3A%20Motion-aware%20Multi-Frame%204D%20Radar%20and%20LiDAR%20Fusion%20for%20Robust%203D%0A%20%20Object%20Detection&entry.906535625=Xiangyuan%20Peng%20and%20Yu%20Wang%20and%20Miao%20Tang%20and%20Bierzynski%20Kay%20and%20Lorenzo%20Servadei%20and%20Robert%20Wille&entry.1292438233=%20%20Reliable%20autonomous%20driving%20systems%20require%20accurate%20detection%20of%20traffic%0Aparticipants.%20To%20this%20end%2C%20multi-modal%20fusion%20has%20emerged%20as%20an%20effective%0Astrategy.%20In%20particular%2C%204D%20radar%20and%20LiDAR%20fusion%20methods%20based%20on%20multi-frame%0Aradar%20point%20clouds%20have%20demonstrated%20the%20effectiveness%20in%20bridging%20the%20point%0Adensity%20gap.%20However%2C%20they%20often%20neglect%20radar%20point%20clouds%27%20inter-frame%0Amisalignment%20caused%20by%20object%20movement%20during%20accumulation%20and%20do%20not%20fully%0Aexploit%20the%20object%20dynamic%20information%20from%204D%20radar.%20In%20this%20paper%2C%20we%20propose%0AMoRAL%2C%20a%20motion-aware%20multi-frame%204D%20radar%20and%20LiDAR%20fusion%20framework%20for%0Arobust%203D%20object%20detection.%20First%2C%20a%20Motion-aware%20Radar%20Encoder%20%28MRE%29%20is%0Adesigned%20to%20compensate%20for%20inter-frame%20radar%20misalignment%20from%20moving%20objects.%0ALater%2C%20a%20Motion%20Attention%20Gated%20Fusion%20%28MAGF%29%20module%20integrate%20radar%20motion%0Afeatures%20to%20guide%20LiDAR%20features%20to%20focus%20on%20dynamic%20foreground%20objects.%0AExtensive%20evaluations%20on%20the%20View-of-Delft%20%28VoD%29%20dataset%20demonstrate%20that%20MoRAL%0Aoutperforms%20existing%20methods%2C%20achieving%20the%20highest%20mAP%20of%2073.30%25%20in%20the%20entire%0Aarea%20and%2088.68%25%20in%20the%20driving%20corridor.%20Notably%2C%20our%20method%20also%20achieves%20the%0Abest%20AP%20of%2069.67%25%20for%20pedestrians%20in%20the%20entire%20area%20and%2096.25%25%20for%20cyclists%20in%0Athe%20driving%20corridor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09422v1&entry.124074799=Read"},
{"title": "One Homography is All You Need: IMM-based Joint Homography and Multiple\n  Object State Estimation", "author": "Paul Johannes Claasen and Johan Pieter de Villiers", "abstract": "  A novel online MOT algorithm, IMM Joint Homography State Estimation\n(IMM-JHSE), is proposed. IMM-JHSE uses an initial homography estimate as the\nonly additional 3D information, whereas other 3D MOT methods use regular 3D\nmeasurements. By jointly modelling the homography matrix and its dynamics as\npart of track state vectors, IMM-JHSE removes the explicit influence of camera\nmotion compensation techniques on predicted track position states, which was\nprevalent in previous approaches. Expanding upon this, static and dynamic\ncamera motion models are combined using an IMM filter. A simple bounding box\nmotion model is used to predict bounding box positions to incorporate image\nplane information. In addition to applying an IMM to camera motion, a\nnon-standard IMM approach is applied where bounding-box-based BIoU scores are\nmixed with ground-plane-based Mahalanobis distances in an IMM-like fashion to\nperform association only, making IMM-JHSE robust to motion away from the ground\nplane. Finally, IMM-JHSE makes use of dynamic process and measurement noise\nestimation techniques. IMM-JHSE improves upon related techniques, including\nUCMCTrack, OC-SORT, C-BIoU and ByteTrack on the DanceTrack and KITTI-car\ndatasets, increasing HOTA by 2.64 and 2.11, respectively, while offering\ncompetitive performance on the MOT17, MOT20 and KITTI-pedestrian datasets.\nUsing publicly available detections, IMM-JHSE outperforms almost all other 2D\nMOT methods and is outperformed only by 3D MOT methods -- some of which are\noffline -- on the KITTI-car dataset. Compared to tracking-by-attention methods,\nIMM-JHSE shows remarkably similar performance on the DanceTrack dataset and\noutperforms them on the MOT17 dataset. The code is publicly available:\nhttps://github.com/Paulkie99/imm-jhse.\n", "link": "http://arxiv.org/abs/2409.02562v3", "date": "2025-05-14", "relevancy": 2.2704, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5889}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5679}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Homography%20is%20All%20You%20Need%3A%20IMM-based%20Joint%20Homography%20and%20Multiple%0A%20%20Object%20State%20Estimation&body=Title%3A%20One%20Homography%20is%20All%20You%20Need%3A%20IMM-based%20Joint%20Homography%20and%20Multiple%0A%20%20Object%20State%20Estimation%0AAuthor%3A%20Paul%20Johannes%20Claasen%20and%20Johan%20Pieter%20de%20Villiers%0AAbstract%3A%20%20%20A%20novel%20online%20MOT%20algorithm%2C%20IMM%20Joint%20Homography%20State%20Estimation%0A%28IMM-JHSE%29%2C%20is%20proposed.%20IMM-JHSE%20uses%20an%20initial%20homography%20estimate%20as%20the%0Aonly%20additional%203D%20information%2C%20whereas%20other%203D%20MOT%20methods%20use%20regular%203D%0Ameasurements.%20By%20jointly%20modelling%20the%20homography%20matrix%20and%20its%20dynamics%20as%0Apart%20of%20track%20state%20vectors%2C%20IMM-JHSE%20removes%20the%20explicit%20influence%20of%20camera%0Amotion%20compensation%20techniques%20on%20predicted%20track%20position%20states%2C%20which%20was%0Aprevalent%20in%20previous%20approaches.%20Expanding%20upon%20this%2C%20static%20and%20dynamic%0Acamera%20motion%20models%20are%20combined%20using%20an%20IMM%20filter.%20A%20simple%20bounding%20box%0Amotion%20model%20is%20used%20to%20predict%20bounding%20box%20positions%20to%20incorporate%20image%0Aplane%20information.%20In%20addition%20to%20applying%20an%20IMM%20to%20camera%20motion%2C%20a%0Anon-standard%20IMM%20approach%20is%20applied%20where%20bounding-box-based%20BIoU%20scores%20are%0Amixed%20with%20ground-plane-based%20Mahalanobis%20distances%20in%20an%20IMM-like%20fashion%20to%0Aperform%20association%20only%2C%20making%20IMM-JHSE%20robust%20to%20motion%20away%20from%20the%20ground%0Aplane.%20Finally%2C%20IMM-JHSE%20makes%20use%20of%20dynamic%20process%20and%20measurement%20noise%0Aestimation%20techniques.%20IMM-JHSE%20improves%20upon%20related%20techniques%2C%20including%0AUCMCTrack%2C%20OC-SORT%2C%20C-BIoU%20and%20ByteTrack%20on%20the%20DanceTrack%20and%20KITTI-car%0Adatasets%2C%20increasing%20HOTA%20by%202.64%20and%202.11%2C%20respectively%2C%20while%20offering%0Acompetitive%20performance%20on%20the%20MOT17%2C%20MOT20%20and%20KITTI-pedestrian%20datasets.%0AUsing%20publicly%20available%20detections%2C%20IMM-JHSE%20outperforms%20almost%20all%20other%202D%0AMOT%20methods%20and%20is%20outperformed%20only%20by%203D%20MOT%20methods%20--%20some%20of%20which%20are%0Aoffline%20--%20on%20the%20KITTI-car%20dataset.%20Compared%20to%20tracking-by-attention%20methods%2C%0AIMM-JHSE%20shows%20remarkably%20similar%20performance%20on%20the%20DanceTrack%20dataset%20and%0Aoutperforms%20them%20on%20the%20MOT17%20dataset.%20The%20code%20is%20publicly%20available%3A%0Ahttps%3A//github.com/Paulkie99/imm-jhse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02562v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Homography%2520is%2520All%2520You%2520Need%253A%2520IMM-based%2520Joint%2520Homography%2520and%2520Multiple%250A%2520%2520Object%2520State%2520Estimation%26entry.906535625%3DPaul%2520Johannes%2520Claasen%2520and%2520Johan%2520Pieter%2520de%2520Villiers%26entry.1292438233%3D%2520%2520A%2520novel%2520online%2520MOT%2520algorithm%252C%2520IMM%2520Joint%2520Homography%2520State%2520Estimation%250A%2528IMM-JHSE%2529%252C%2520is%2520proposed.%2520IMM-JHSE%2520uses%2520an%2520initial%2520homography%2520estimate%2520as%2520the%250Aonly%2520additional%25203D%2520information%252C%2520whereas%2520other%25203D%2520MOT%2520methods%2520use%2520regular%25203D%250Ameasurements.%2520By%2520jointly%2520modelling%2520the%2520homography%2520matrix%2520and%2520its%2520dynamics%2520as%250Apart%2520of%2520track%2520state%2520vectors%252C%2520IMM-JHSE%2520removes%2520the%2520explicit%2520influence%2520of%2520camera%250Amotion%2520compensation%2520techniques%2520on%2520predicted%2520track%2520position%2520states%252C%2520which%2520was%250Aprevalent%2520in%2520previous%2520approaches.%2520Expanding%2520upon%2520this%252C%2520static%2520and%2520dynamic%250Acamera%2520motion%2520models%2520are%2520combined%2520using%2520an%2520IMM%2520filter.%2520A%2520simple%2520bounding%2520box%250Amotion%2520model%2520is%2520used%2520to%2520predict%2520bounding%2520box%2520positions%2520to%2520incorporate%2520image%250Aplane%2520information.%2520In%2520addition%2520to%2520applying%2520an%2520IMM%2520to%2520camera%2520motion%252C%2520a%250Anon-standard%2520IMM%2520approach%2520is%2520applied%2520where%2520bounding-box-based%2520BIoU%2520scores%2520are%250Amixed%2520with%2520ground-plane-based%2520Mahalanobis%2520distances%2520in%2520an%2520IMM-like%2520fashion%2520to%250Aperform%2520association%2520only%252C%2520making%2520IMM-JHSE%2520robust%2520to%2520motion%2520away%2520from%2520the%2520ground%250Aplane.%2520Finally%252C%2520IMM-JHSE%2520makes%2520use%2520of%2520dynamic%2520process%2520and%2520measurement%2520noise%250Aestimation%2520techniques.%2520IMM-JHSE%2520improves%2520upon%2520related%2520techniques%252C%2520including%250AUCMCTrack%252C%2520OC-SORT%252C%2520C-BIoU%2520and%2520ByteTrack%2520on%2520the%2520DanceTrack%2520and%2520KITTI-car%250Adatasets%252C%2520increasing%2520HOTA%2520by%25202.64%2520and%25202.11%252C%2520respectively%252C%2520while%2520offering%250Acompetitive%2520performance%2520on%2520the%2520MOT17%252C%2520MOT20%2520and%2520KITTI-pedestrian%2520datasets.%250AUsing%2520publicly%2520available%2520detections%252C%2520IMM-JHSE%2520outperforms%2520almost%2520all%2520other%25202D%250AMOT%2520methods%2520and%2520is%2520outperformed%2520only%2520by%25203D%2520MOT%2520methods%2520--%2520some%2520of%2520which%2520are%250Aoffline%2520--%2520on%2520the%2520KITTI-car%2520dataset.%2520Compared%2520to%2520tracking-by-attention%2520methods%252C%250AIMM-JHSE%2520shows%2520remarkably%2520similar%2520performance%2520on%2520the%2520DanceTrack%2520dataset%2520and%250Aoutperforms%2520them%2520on%2520the%2520MOT17%2520dataset.%2520The%2520code%2520is%2520publicly%2520available%253A%250Ahttps%253A//github.com/Paulkie99/imm-jhse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02562v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Homography%20is%20All%20You%20Need%3A%20IMM-based%20Joint%20Homography%20and%20Multiple%0A%20%20Object%20State%20Estimation&entry.906535625=Paul%20Johannes%20Claasen%20and%20Johan%20Pieter%20de%20Villiers&entry.1292438233=%20%20A%20novel%20online%20MOT%20algorithm%2C%20IMM%20Joint%20Homography%20State%20Estimation%0A%28IMM-JHSE%29%2C%20is%20proposed.%20IMM-JHSE%20uses%20an%20initial%20homography%20estimate%20as%20the%0Aonly%20additional%203D%20information%2C%20whereas%20other%203D%20MOT%20methods%20use%20regular%203D%0Ameasurements.%20By%20jointly%20modelling%20the%20homography%20matrix%20and%20its%20dynamics%20as%0Apart%20of%20track%20state%20vectors%2C%20IMM-JHSE%20removes%20the%20explicit%20influence%20of%20camera%0Amotion%20compensation%20techniques%20on%20predicted%20track%20position%20states%2C%20which%20was%0Aprevalent%20in%20previous%20approaches.%20Expanding%20upon%20this%2C%20static%20and%20dynamic%0Acamera%20motion%20models%20are%20combined%20using%20an%20IMM%20filter.%20A%20simple%20bounding%20box%0Amotion%20model%20is%20used%20to%20predict%20bounding%20box%20positions%20to%20incorporate%20image%0Aplane%20information.%20In%20addition%20to%20applying%20an%20IMM%20to%20camera%20motion%2C%20a%0Anon-standard%20IMM%20approach%20is%20applied%20where%20bounding-box-based%20BIoU%20scores%20are%0Amixed%20with%20ground-plane-based%20Mahalanobis%20distances%20in%20an%20IMM-like%20fashion%20to%0Aperform%20association%20only%2C%20making%20IMM-JHSE%20robust%20to%20motion%20away%20from%20the%20ground%0Aplane.%20Finally%2C%20IMM-JHSE%20makes%20use%20of%20dynamic%20process%20and%20measurement%20noise%0Aestimation%20techniques.%20IMM-JHSE%20improves%20upon%20related%20techniques%2C%20including%0AUCMCTrack%2C%20OC-SORT%2C%20C-BIoU%20and%20ByteTrack%20on%20the%20DanceTrack%20and%20KITTI-car%0Adatasets%2C%20increasing%20HOTA%20by%202.64%20and%202.11%2C%20respectively%2C%20while%20offering%0Acompetitive%20performance%20on%20the%20MOT17%2C%20MOT20%20and%20KITTI-pedestrian%20datasets.%0AUsing%20publicly%20available%20detections%2C%20IMM-JHSE%20outperforms%20almost%20all%20other%202D%0AMOT%20methods%20and%20is%20outperformed%20only%20by%203D%20MOT%20methods%20--%20some%20of%20which%20are%0Aoffline%20--%20on%20the%20KITTI-car%20dataset.%20Compared%20to%20tracking-by-attention%20methods%2C%0AIMM-JHSE%20shows%20remarkably%20similar%20performance%20on%20the%20DanceTrack%20dataset%20and%0Aoutperforms%20them%20on%20the%20MOT17%20dataset.%20The%20code%20is%20publicly%20available%3A%0Ahttps%3A//github.com/Paulkie99/imm-jhse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02562v3&entry.124074799=Read"},
{"title": "BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation\n  Models for Biomedical Image Analysis", "author": "Jiarun Liu and Hong-Yu Zhou and Weijian Huang and Hao Yang and Dongning Song and Tao Tan and Yong Liang and Shanshan Wang", "abstract": "  Scaling up model and data size have demonstrated impressive performance\nimprovement over a wide range of tasks. Despite extensive studies on scaling\nbehaviors for general-purpose tasks, medical images exhibit substantial\ndifferences from natural data. It remains unclear the key factors in developing\nmedical vision foundation models at scale due to the absence of an extensive\nunderstanding of scaling behavior in the medical domain. In this paper, we\nexplored the scaling behavior across model sizes, training algorithms, data\nsizes, and imaging modalities in developing scalable medical vision foundation\nmodels by self-supervised learning. To support scalable pretraining, we\nintroduce BioVFM-21M, a large-scale biomedical image dataset encompassing a\nwide range of biomedical image modalities and anatomies. We observed that\nscaling up does provide benefits but varies across tasks. Additional analysis\nreveals several factors correlated with scaling benefits. Finally, we propose\nBioVFM, a large-scale medical vision foundation model pretrained on 21 million\nbiomedical images, which outperforms the previous state-of-the-art foundation\nmodels across 12 medical benchmarks. Our results highlight that while scaling\nup is beneficial for pursuing better performance, task characteristics, data\ndiversity, pretraining methods, and computational efficiency remain critical\nconsiderations for developing scalable medical foundation models.\n", "link": "http://arxiv.org/abs/2505.09329v1", "date": "2025-05-14", "relevancy": 2.2535, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BioVFM-21M%3A%20Benchmarking%20and%20Scaling%20Self-Supervised%20Vision%20Foundation%0A%20%20Models%20for%20Biomedical%20Image%20Analysis&body=Title%3A%20BioVFM-21M%3A%20Benchmarking%20and%20Scaling%20Self-Supervised%20Vision%20Foundation%0A%20%20Models%20for%20Biomedical%20Image%20Analysis%0AAuthor%3A%20Jiarun%20Liu%20and%20Hong-Yu%20Zhou%20and%20Weijian%20Huang%20and%20Hao%20Yang%20and%20Dongning%20Song%20and%20Tao%20Tan%20and%20Yong%20Liang%20and%20Shanshan%20Wang%0AAbstract%3A%20%20%20Scaling%20up%20model%20and%20data%20size%20have%20demonstrated%20impressive%20performance%0Aimprovement%20over%20a%20wide%20range%20of%20tasks.%20Despite%20extensive%20studies%20on%20scaling%0Abehaviors%20for%20general-purpose%20tasks%2C%20medical%20images%20exhibit%20substantial%0Adifferences%20from%20natural%20data.%20It%20remains%20unclear%20the%20key%20factors%20in%20developing%0Amedical%20vision%20foundation%20models%20at%20scale%20due%20to%20the%20absence%20of%20an%20extensive%0Aunderstanding%20of%20scaling%20behavior%20in%20the%20medical%20domain.%20In%20this%20paper%2C%20we%0Aexplored%20the%20scaling%20behavior%20across%20model%20sizes%2C%20training%20algorithms%2C%20data%0Asizes%2C%20and%20imaging%20modalities%20in%20developing%20scalable%20medical%20vision%20foundation%0Amodels%20by%20self-supervised%20learning.%20To%20support%20scalable%20pretraining%2C%20we%0Aintroduce%20BioVFM-21M%2C%20a%20large-scale%20biomedical%20image%20dataset%20encompassing%20a%0Awide%20range%20of%20biomedical%20image%20modalities%20and%20anatomies.%20We%20observed%20that%0Ascaling%20up%20does%20provide%20benefits%20but%20varies%20across%20tasks.%20Additional%20analysis%0Areveals%20several%20factors%20correlated%20with%20scaling%20benefits.%20Finally%2C%20we%20propose%0ABioVFM%2C%20a%20large-scale%20medical%20vision%20foundation%20model%20pretrained%20on%2021%20million%0Abiomedical%20images%2C%20which%20outperforms%20the%20previous%20state-of-the-art%20foundation%0Amodels%20across%2012%20medical%20benchmarks.%20Our%20results%20highlight%20that%20while%20scaling%0Aup%20is%20beneficial%20for%20pursuing%20better%20performance%2C%20task%20characteristics%2C%20data%0Adiversity%2C%20pretraining%20methods%2C%20and%20computational%20efficiency%20remain%20critical%0Aconsiderations%20for%20developing%20scalable%20medical%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBioVFM-21M%253A%2520Benchmarking%2520and%2520Scaling%2520Self-Supervised%2520Vision%2520Foundation%250A%2520%2520Models%2520for%2520Biomedical%2520Image%2520Analysis%26entry.906535625%3DJiarun%2520Liu%2520and%2520Hong-Yu%2520Zhou%2520and%2520Weijian%2520Huang%2520and%2520Hao%2520Yang%2520and%2520Dongning%2520Song%2520and%2520Tao%2520Tan%2520and%2520Yong%2520Liang%2520and%2520Shanshan%2520Wang%26entry.1292438233%3D%2520%2520Scaling%2520up%2520model%2520and%2520data%2520size%2520have%2520demonstrated%2520impressive%2520performance%250Aimprovement%2520over%2520a%2520wide%2520range%2520of%2520tasks.%2520Despite%2520extensive%2520studies%2520on%2520scaling%250Abehaviors%2520for%2520general-purpose%2520tasks%252C%2520medical%2520images%2520exhibit%2520substantial%250Adifferences%2520from%2520natural%2520data.%2520It%2520remains%2520unclear%2520the%2520key%2520factors%2520in%2520developing%250Amedical%2520vision%2520foundation%2520models%2520at%2520scale%2520due%2520to%2520the%2520absence%2520of%2520an%2520extensive%250Aunderstanding%2520of%2520scaling%2520behavior%2520in%2520the%2520medical%2520domain.%2520In%2520this%2520paper%252C%2520we%250Aexplored%2520the%2520scaling%2520behavior%2520across%2520model%2520sizes%252C%2520training%2520algorithms%252C%2520data%250Asizes%252C%2520and%2520imaging%2520modalities%2520in%2520developing%2520scalable%2520medical%2520vision%2520foundation%250Amodels%2520by%2520self-supervised%2520learning.%2520To%2520support%2520scalable%2520pretraining%252C%2520we%250Aintroduce%2520BioVFM-21M%252C%2520a%2520large-scale%2520biomedical%2520image%2520dataset%2520encompassing%2520a%250Awide%2520range%2520of%2520biomedical%2520image%2520modalities%2520and%2520anatomies.%2520We%2520observed%2520that%250Ascaling%2520up%2520does%2520provide%2520benefits%2520but%2520varies%2520across%2520tasks.%2520Additional%2520analysis%250Areveals%2520several%2520factors%2520correlated%2520with%2520scaling%2520benefits.%2520Finally%252C%2520we%2520propose%250ABioVFM%252C%2520a%2520large-scale%2520medical%2520vision%2520foundation%2520model%2520pretrained%2520on%252021%2520million%250Abiomedical%2520images%252C%2520which%2520outperforms%2520the%2520previous%2520state-of-the-art%2520foundation%250Amodels%2520across%252012%2520medical%2520benchmarks.%2520Our%2520results%2520highlight%2520that%2520while%2520scaling%250Aup%2520is%2520beneficial%2520for%2520pursuing%2520better%2520performance%252C%2520task%2520characteristics%252C%2520data%250Adiversity%252C%2520pretraining%2520methods%252C%2520and%2520computational%2520efficiency%2520remain%2520critical%250Aconsiderations%2520for%2520developing%2520scalable%2520medical%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BioVFM-21M%3A%20Benchmarking%20and%20Scaling%20Self-Supervised%20Vision%20Foundation%0A%20%20Models%20for%20Biomedical%20Image%20Analysis&entry.906535625=Jiarun%20Liu%20and%20Hong-Yu%20Zhou%20and%20Weijian%20Huang%20and%20Hao%20Yang%20and%20Dongning%20Song%20and%20Tao%20Tan%20and%20Yong%20Liang%20and%20Shanshan%20Wang&entry.1292438233=%20%20Scaling%20up%20model%20and%20data%20size%20have%20demonstrated%20impressive%20performance%0Aimprovement%20over%20a%20wide%20range%20of%20tasks.%20Despite%20extensive%20studies%20on%20scaling%0Abehaviors%20for%20general-purpose%20tasks%2C%20medical%20images%20exhibit%20substantial%0Adifferences%20from%20natural%20data.%20It%20remains%20unclear%20the%20key%20factors%20in%20developing%0Amedical%20vision%20foundation%20models%20at%20scale%20due%20to%20the%20absence%20of%20an%20extensive%0Aunderstanding%20of%20scaling%20behavior%20in%20the%20medical%20domain.%20In%20this%20paper%2C%20we%0Aexplored%20the%20scaling%20behavior%20across%20model%20sizes%2C%20training%20algorithms%2C%20data%0Asizes%2C%20and%20imaging%20modalities%20in%20developing%20scalable%20medical%20vision%20foundation%0Amodels%20by%20self-supervised%20learning.%20To%20support%20scalable%20pretraining%2C%20we%0Aintroduce%20BioVFM-21M%2C%20a%20large-scale%20biomedical%20image%20dataset%20encompassing%20a%0Awide%20range%20of%20biomedical%20image%20modalities%20and%20anatomies.%20We%20observed%20that%0Ascaling%20up%20does%20provide%20benefits%20but%20varies%20across%20tasks.%20Additional%20analysis%0Areveals%20several%20factors%20correlated%20with%20scaling%20benefits.%20Finally%2C%20we%20propose%0ABioVFM%2C%20a%20large-scale%20medical%20vision%20foundation%20model%20pretrained%20on%2021%20million%0Abiomedical%20images%2C%20which%20outperforms%20the%20previous%20state-of-the-art%20foundation%0Amodels%20across%2012%20medical%20benchmarks.%20Our%20results%20highlight%20that%20while%20scaling%0Aup%20is%20beneficial%20for%20pursuing%20better%20performance%2C%20task%20characteristics%2C%20data%0Adiversity%2C%20pretraining%20methods%2C%20and%20computational%20efficiency%20remain%20critical%0Aconsiderations%20for%20developing%20scalable%20medical%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09329v1&entry.124074799=Read"},
{"title": "SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation", "author": "Achref Doula and Max M\u00fchl\u00e4user and Alejandro Sanchez Guinea", "abstract": "  Large Language Models (LLMs) show growing promise in autonomous driving by\nreasoning over complex traffic scenarios to generate path plans. However, their\ntendencies toward overconfidence, and hallucinations raise critical safety\nconcerns. We introduce SafePath, a modular framework that augments LLM-based\npath planning with formal safety guarantees using conformal prediction.\nSafePath operates in three stages. In the first stage, we use an LLM that\ngenerates a set of diverse candidate paths, exploring possible trajectories\nbased on agent behaviors and environmental cues. In the second stage, SafePath\nfilters out high-risk trajectories while guaranteeing that at least one safe\noption is included with a user-defined probability, through a multiple-choice\nquestion-answering formulation that integrates conformal prediction. In the\nfinal stage, our approach selects the path with the lowest expected collision\nrisk when uncertainty is low or delegates control to a human when uncertainty\nis high. We theoretically prove that SafePath guarantees a safe trajectory with\na user-defined probability, and we show how its human delegation rate can be\ntuned to balance autonomy and safety. Extensive experiments on nuScenes and\nHighway-env show that SafePath reduces planning uncertainty by 77\\% and\ncollision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven\npath planning more safer.\n", "link": "http://arxiv.org/abs/2505.09427v1", "date": "2025-05-14", "relevancy": 2.2494, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5905}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5847}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafePath%3A%20Conformal%20Prediction%20for%20Safe%20LLM-Based%20Autonomous%20Navigation&body=Title%3A%20SafePath%3A%20Conformal%20Prediction%20for%20Safe%20LLM-Based%20Autonomous%20Navigation%0AAuthor%3A%20Achref%20Doula%20and%20Max%20M%C3%BChl%C3%A4user%20and%20Alejandro%20Sanchez%20Guinea%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20show%20growing%20promise%20in%20autonomous%20driving%20by%0Areasoning%20over%20complex%20traffic%20scenarios%20to%20generate%20path%20plans.%20However%2C%20their%0Atendencies%20toward%20overconfidence%2C%20and%20hallucinations%20raise%20critical%20safety%0Aconcerns.%20We%20introduce%20SafePath%2C%20a%20modular%20framework%20that%20augments%20LLM-based%0Apath%20planning%20with%20formal%20safety%20guarantees%20using%20conformal%20prediction.%0ASafePath%20operates%20in%20three%20stages.%20In%20the%20first%20stage%2C%20we%20use%20an%20LLM%20that%0Agenerates%20a%20set%20of%20diverse%20candidate%20paths%2C%20exploring%20possible%20trajectories%0Abased%20on%20agent%20behaviors%20and%20environmental%20cues.%20In%20the%20second%20stage%2C%20SafePath%0Afilters%20out%20high-risk%20trajectories%20while%20guaranteeing%20that%20at%20least%20one%20safe%0Aoption%20is%20included%20with%20a%20user-defined%20probability%2C%20through%20a%20multiple-choice%0Aquestion-answering%20formulation%20that%20integrates%20conformal%20prediction.%20In%20the%0Afinal%20stage%2C%20our%20approach%20selects%20the%20path%20with%20the%20lowest%20expected%20collision%0Arisk%20when%20uncertainty%20is%20low%20or%20delegates%20control%20to%20a%20human%20when%20uncertainty%0Ais%20high.%20We%20theoretically%20prove%20that%20SafePath%20guarantees%20a%20safe%20trajectory%20with%0Aa%20user-defined%20probability%2C%20and%20we%20show%20how%20its%20human%20delegation%20rate%20can%20be%0Atuned%20to%20balance%20autonomy%20and%20safety.%20Extensive%20experiments%20on%20nuScenes%20and%0AHighway-env%20show%20that%20SafePath%20reduces%20planning%20uncertainty%20by%2077%5C%25%20and%0Acollision%20rates%20by%20up%20to%2070%5C%25%2C%20demonstrating%20effectiveness%20in%20making%20LLM-driven%0Apath%20planning%20more%20safer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafePath%253A%2520Conformal%2520Prediction%2520for%2520Safe%2520LLM-Based%2520Autonomous%2520Navigation%26entry.906535625%3DAchref%2520Doula%2520and%2520Max%2520M%25C3%25BChl%25C3%25A4user%2520and%2520Alejandro%2520Sanchez%2520Guinea%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520show%2520growing%2520promise%2520in%2520autonomous%2520driving%2520by%250Areasoning%2520over%2520complex%2520traffic%2520scenarios%2520to%2520generate%2520path%2520plans.%2520However%252C%2520their%250Atendencies%2520toward%2520overconfidence%252C%2520and%2520hallucinations%2520raise%2520critical%2520safety%250Aconcerns.%2520We%2520introduce%2520SafePath%252C%2520a%2520modular%2520framework%2520that%2520augments%2520LLM-based%250Apath%2520planning%2520with%2520formal%2520safety%2520guarantees%2520using%2520conformal%2520prediction.%250ASafePath%2520operates%2520in%2520three%2520stages.%2520In%2520the%2520first%2520stage%252C%2520we%2520use%2520an%2520LLM%2520that%250Agenerates%2520a%2520set%2520of%2520diverse%2520candidate%2520paths%252C%2520exploring%2520possible%2520trajectories%250Abased%2520on%2520agent%2520behaviors%2520and%2520environmental%2520cues.%2520In%2520the%2520second%2520stage%252C%2520SafePath%250Afilters%2520out%2520high-risk%2520trajectories%2520while%2520guaranteeing%2520that%2520at%2520least%2520one%2520safe%250Aoption%2520is%2520included%2520with%2520a%2520user-defined%2520probability%252C%2520through%2520a%2520multiple-choice%250Aquestion-answering%2520formulation%2520that%2520integrates%2520conformal%2520prediction.%2520In%2520the%250Afinal%2520stage%252C%2520our%2520approach%2520selects%2520the%2520path%2520with%2520the%2520lowest%2520expected%2520collision%250Arisk%2520when%2520uncertainty%2520is%2520low%2520or%2520delegates%2520control%2520to%2520a%2520human%2520when%2520uncertainty%250Ais%2520high.%2520We%2520theoretically%2520prove%2520that%2520SafePath%2520guarantees%2520a%2520safe%2520trajectory%2520with%250Aa%2520user-defined%2520probability%252C%2520and%2520we%2520show%2520how%2520its%2520human%2520delegation%2520rate%2520can%2520be%250Atuned%2520to%2520balance%2520autonomy%2520and%2520safety.%2520Extensive%2520experiments%2520on%2520nuScenes%2520and%250AHighway-env%2520show%2520that%2520SafePath%2520reduces%2520planning%2520uncertainty%2520by%252077%255C%2525%2520and%250Acollision%2520rates%2520by%2520up%2520to%252070%255C%2525%252C%2520demonstrating%2520effectiveness%2520in%2520making%2520LLM-driven%250Apath%2520planning%2520more%2520safer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafePath%3A%20Conformal%20Prediction%20for%20Safe%20LLM-Based%20Autonomous%20Navigation&entry.906535625=Achref%20Doula%20and%20Max%20M%C3%BChl%C3%A4user%20and%20Alejandro%20Sanchez%20Guinea&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20show%20growing%20promise%20in%20autonomous%20driving%20by%0Areasoning%20over%20complex%20traffic%20scenarios%20to%20generate%20path%20plans.%20However%2C%20their%0Atendencies%20toward%20overconfidence%2C%20and%20hallucinations%20raise%20critical%20safety%0Aconcerns.%20We%20introduce%20SafePath%2C%20a%20modular%20framework%20that%20augments%20LLM-based%0Apath%20planning%20with%20formal%20safety%20guarantees%20using%20conformal%20prediction.%0ASafePath%20operates%20in%20three%20stages.%20In%20the%20first%20stage%2C%20we%20use%20an%20LLM%20that%0Agenerates%20a%20set%20of%20diverse%20candidate%20paths%2C%20exploring%20possible%20trajectories%0Abased%20on%20agent%20behaviors%20and%20environmental%20cues.%20In%20the%20second%20stage%2C%20SafePath%0Afilters%20out%20high-risk%20trajectories%20while%20guaranteeing%20that%20at%20least%20one%20safe%0Aoption%20is%20included%20with%20a%20user-defined%20probability%2C%20through%20a%20multiple-choice%0Aquestion-answering%20formulation%20that%20integrates%20conformal%20prediction.%20In%20the%0Afinal%20stage%2C%20our%20approach%20selects%20the%20path%20with%20the%20lowest%20expected%20collision%0Arisk%20when%20uncertainty%20is%20low%20or%20delegates%20control%20to%20a%20human%20when%20uncertainty%0Ais%20high.%20We%20theoretically%20prove%20that%20SafePath%20guarantees%20a%20safe%20trajectory%20with%0Aa%20user-defined%20probability%2C%20and%20we%20show%20how%20its%20human%20delegation%20rate%20can%20be%0Atuned%20to%20balance%20autonomy%20and%20safety.%20Extensive%20experiments%20on%20nuScenes%20and%0AHighway-env%20show%20that%20SafePath%20reduces%20planning%20uncertainty%20by%2077%5C%25%20and%0Acollision%20rates%20by%20up%20to%2070%5C%25%2C%20demonstrating%20effectiveness%20in%20making%20LLM-driven%0Apath%20planning%20more%20safer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09427v1&entry.124074799=Read"},
{"title": "Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive\n  CVaR Barrier Functions", "author": "Xinyi Wang and Taekyung Kim and Bardh Hoxha and Georgios Fainekos and Dimitra Panagou", "abstract": "  Robot navigation in dynamic, crowded environments poses a significant\nchallenge due to the inherent uncertainties in the obstacle model. In this\nwork, we propose a risk-adaptive approach based on the Conditional\nValue-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically\nadjusted to accept the minimum necessary risk, achieving a good performance in\nterms of safety and optimization feasibility under uncertainty. Additionally,\nwe introduce a dynamic zone-based barrier function which characterizes the\ncollision likelihood by evaluating the relative state between the robot and the\nobstacle. By integrating risk adaptation with this new function, our approach\nadaptively expands the safety margin, enabling the robot to proactively avoid\nobstacles in highly dynamic environments. Comparisons and ablation studies\ndemonstrate that our method outperforms existing social navigation approaches,\nand validate the effectiveness of our proposed framework.\n", "link": "http://arxiv.org/abs/2504.06513v2", "date": "2025-05-14", "relevancy": 2.2485, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5853}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5617}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Navigation%20in%20Uncertain%20Crowded%20Environments%20Using%20Risk%20Adaptive%0A%20%20CVaR%20Barrier%20Functions&body=Title%3A%20Safe%20Navigation%20in%20Uncertain%20Crowded%20Environments%20Using%20Risk%20Adaptive%0A%20%20CVaR%20Barrier%20Functions%0AAuthor%3A%20Xinyi%20Wang%20and%20Taekyung%20Kim%20and%20Bardh%20Hoxha%20and%20Georgios%20Fainekos%20and%20Dimitra%20Panagou%0AAbstract%3A%20%20%20Robot%20navigation%20in%20dynamic%2C%20crowded%20environments%20poses%20a%20significant%0Achallenge%20due%20to%20the%20inherent%20uncertainties%20in%20the%20obstacle%20model.%20In%20this%0Awork%2C%20we%20propose%20a%20risk-adaptive%20approach%20based%20on%20the%20Conditional%0AValue-at-Risk%20Barrier%20Function%20%28CVaR-BF%29%2C%20where%20the%20risk%20level%20is%20automatically%0Aadjusted%20to%20accept%20the%20minimum%20necessary%20risk%2C%20achieving%20a%20good%20performance%20in%0Aterms%20of%20safety%20and%20optimization%20feasibility%20under%20uncertainty.%20Additionally%2C%0Awe%20introduce%20a%20dynamic%20zone-based%20barrier%20function%20which%20characterizes%20the%0Acollision%20likelihood%20by%20evaluating%20the%20relative%20state%20between%20the%20robot%20and%20the%0Aobstacle.%20By%20integrating%20risk%20adaptation%20with%20this%20new%20function%2C%20our%20approach%0Aadaptively%20expands%20the%20safety%20margin%2C%20enabling%20the%20robot%20to%20proactively%20avoid%0Aobstacles%20in%20highly%20dynamic%20environments.%20Comparisons%20and%20ablation%20studies%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20social%20navigation%20approaches%2C%0Aand%20validate%20the%20effectiveness%20of%20our%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06513v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Navigation%2520in%2520Uncertain%2520Crowded%2520Environments%2520Using%2520Risk%2520Adaptive%250A%2520%2520CVaR%2520Barrier%2520Functions%26entry.906535625%3DXinyi%2520Wang%2520and%2520Taekyung%2520Kim%2520and%2520Bardh%2520Hoxha%2520and%2520Georgios%2520Fainekos%2520and%2520Dimitra%2520Panagou%26entry.1292438233%3D%2520%2520Robot%2520navigation%2520in%2520dynamic%252C%2520crowded%2520environments%2520poses%2520a%2520significant%250Achallenge%2520due%2520to%2520the%2520inherent%2520uncertainties%2520in%2520the%2520obstacle%2520model.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520risk-adaptive%2520approach%2520based%2520on%2520the%2520Conditional%250AValue-at-Risk%2520Barrier%2520Function%2520%2528CVaR-BF%2529%252C%2520where%2520the%2520risk%2520level%2520is%2520automatically%250Aadjusted%2520to%2520accept%2520the%2520minimum%2520necessary%2520risk%252C%2520achieving%2520a%2520good%2520performance%2520in%250Aterms%2520of%2520safety%2520and%2520optimization%2520feasibility%2520under%2520uncertainty.%2520Additionally%252C%250Awe%2520introduce%2520a%2520dynamic%2520zone-based%2520barrier%2520function%2520which%2520characterizes%2520the%250Acollision%2520likelihood%2520by%2520evaluating%2520the%2520relative%2520state%2520between%2520the%2520robot%2520and%2520the%250Aobstacle.%2520By%2520integrating%2520risk%2520adaptation%2520with%2520this%2520new%2520function%252C%2520our%2520approach%250Aadaptively%2520expands%2520the%2520safety%2520margin%252C%2520enabling%2520the%2520robot%2520to%2520proactively%2520avoid%250Aobstacles%2520in%2520highly%2520dynamic%2520environments.%2520Comparisons%2520and%2520ablation%2520studies%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520social%2520navigation%2520approaches%252C%250Aand%2520validate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06513v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Navigation%20in%20Uncertain%20Crowded%20Environments%20Using%20Risk%20Adaptive%0A%20%20CVaR%20Barrier%20Functions&entry.906535625=Xinyi%20Wang%20and%20Taekyung%20Kim%20and%20Bardh%20Hoxha%20and%20Georgios%20Fainekos%20and%20Dimitra%20Panagou&entry.1292438233=%20%20Robot%20navigation%20in%20dynamic%2C%20crowded%20environments%20poses%20a%20significant%0Achallenge%20due%20to%20the%20inherent%20uncertainties%20in%20the%20obstacle%20model.%20In%20this%0Awork%2C%20we%20propose%20a%20risk-adaptive%20approach%20based%20on%20the%20Conditional%0AValue-at-Risk%20Barrier%20Function%20%28CVaR-BF%29%2C%20where%20the%20risk%20level%20is%20automatically%0Aadjusted%20to%20accept%20the%20minimum%20necessary%20risk%2C%20achieving%20a%20good%20performance%20in%0Aterms%20of%20safety%20and%20optimization%20feasibility%20under%20uncertainty.%20Additionally%2C%0Awe%20introduce%20a%20dynamic%20zone-based%20barrier%20function%20which%20characterizes%20the%0Acollision%20likelihood%20by%20evaluating%20the%20relative%20state%20between%20the%20robot%20and%20the%0Aobstacle.%20By%20integrating%20risk%20adaptation%20with%20this%20new%20function%2C%20our%20approach%0Aadaptively%20expands%20the%20safety%20margin%2C%20enabling%20the%20robot%20to%20proactively%20avoid%0Aobstacles%20in%20highly%20dynamic%20environments.%20Comparisons%20and%20ablation%20studies%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20social%20navigation%20approaches%2C%0Aand%20validate%20the%20effectiveness%20of%20our%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06513v2&entry.124074799=Read"},
{"title": "aUToPath: Unified Planning and Control for Autonomous Vehicles in Urban\n  Environments Using Hybrid Lattice and Free-Space Search", "author": "Tanmay P. Patel and Connor Wilson and Ellina R. Zhang and Morgan Tran and Chang Keun Paik and Steven L. Waslander and Timothy D. Barfoot", "abstract": "  This paper presents aUToPath, a unified online framework for global\npath-planning and control to address the challenge of autonomous navigation in\ncluttered urban environments. A key component of our framework is a novel\nhybrid planner that combines pre-computed lattice maps with dynamic free-space\nsampling to efficiently generate optimal driveable corridors in cluttered\nscenarios. Our system also features sequential convex programming (SCP)-based\nmodel predictive control (MPC) to refine the corridors into smooth, dynamically\nconsistent trajectories. A single optimization problem is used to both generate\na trajectory and its corresponding control commands; this addresses limitations\nof decoupled approaches by guaranteeing a safe and feasible path. Simulation\nresults of the novel planner on randomly generated obstacle-rich scenarios\ndemonstrate the success rate of a free-space Adaptively Informed Trees*\n(AIT*)-based planner, and runtimes comparable to a lattice-based planner.\nReal-world experiments of the full system on a Chevrolet Bolt EUV further\nvalidate performance in dense obstacle fields, demonstrating no violations of\ntraffic, kinematic, or vehicle constraints, and a 100% success rate across\neight trials.\n", "link": "http://arxiv.org/abs/2505.09475v1", "date": "2025-05-14", "relevancy": 2.2234, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5792}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5432}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20aUToPath%3A%20Unified%20Planning%20and%20Control%20for%20Autonomous%20Vehicles%20in%20Urban%0A%20%20Environments%20Using%20Hybrid%20Lattice%20and%20Free-Space%20Search&body=Title%3A%20aUToPath%3A%20Unified%20Planning%20and%20Control%20for%20Autonomous%20Vehicles%20in%20Urban%0A%20%20Environments%20Using%20Hybrid%20Lattice%20and%20Free-Space%20Search%0AAuthor%3A%20Tanmay%20P.%20Patel%20and%20Connor%20Wilson%20and%20Ellina%20R.%20Zhang%20and%20Morgan%20Tran%20and%20Chang%20Keun%20Paik%20and%20Steven%20L.%20Waslander%20and%20Timothy%20D.%20Barfoot%0AAbstract%3A%20%20%20This%20paper%20presents%20aUToPath%2C%20a%20unified%20online%20framework%20for%20global%0Apath-planning%20and%20control%20to%20address%20the%20challenge%20of%20autonomous%20navigation%20in%0Acluttered%20urban%20environments.%20A%20key%20component%20of%20our%20framework%20is%20a%20novel%0Ahybrid%20planner%20that%20combines%20pre-computed%20lattice%20maps%20with%20dynamic%20free-space%0Asampling%20to%20efficiently%20generate%20optimal%20driveable%20corridors%20in%20cluttered%0Ascenarios.%20Our%20system%20also%20features%20sequential%20convex%20programming%20%28SCP%29-based%0Amodel%20predictive%20control%20%28MPC%29%20to%20refine%20the%20corridors%20into%20smooth%2C%20dynamically%0Aconsistent%20trajectories.%20A%20single%20optimization%20problem%20is%20used%20to%20both%20generate%0Aa%20trajectory%20and%20its%20corresponding%20control%20commands%3B%20this%20addresses%20limitations%0Aof%20decoupled%20approaches%20by%20guaranteeing%20a%20safe%20and%20feasible%20path.%20Simulation%0Aresults%20of%20the%20novel%20planner%20on%20randomly%20generated%20obstacle-rich%20scenarios%0Ademonstrate%20the%20success%20rate%20of%20a%20free-space%20Adaptively%20Informed%20Trees%2A%0A%28AIT%2A%29-based%20planner%2C%20and%20runtimes%20comparable%20to%20a%20lattice-based%20planner.%0AReal-world%20experiments%20of%20the%20full%20system%20on%20a%20Chevrolet%20Bolt%20EUV%20further%0Avalidate%20performance%20in%20dense%20obstacle%20fields%2C%20demonstrating%20no%20violations%20of%0Atraffic%2C%20kinematic%2C%20or%20vehicle%20constraints%2C%20and%20a%20100%25%20success%20rate%20across%0Aeight%20trials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DaUToPath%253A%2520Unified%2520Planning%2520and%2520Control%2520for%2520Autonomous%2520Vehicles%2520in%2520Urban%250A%2520%2520Environments%2520Using%2520Hybrid%2520Lattice%2520and%2520Free-Space%2520Search%26entry.906535625%3DTanmay%2520P.%2520Patel%2520and%2520Connor%2520Wilson%2520and%2520Ellina%2520R.%2520Zhang%2520and%2520Morgan%2520Tran%2520and%2520Chang%2520Keun%2520Paik%2520and%2520Steven%2520L.%2520Waslander%2520and%2520Timothy%2520D.%2520Barfoot%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520aUToPath%252C%2520a%2520unified%2520online%2520framework%2520for%2520global%250Apath-planning%2520and%2520control%2520to%2520address%2520the%2520challenge%2520of%2520autonomous%2520navigation%2520in%250Acluttered%2520urban%2520environments.%2520A%2520key%2520component%2520of%2520our%2520framework%2520is%2520a%2520novel%250Ahybrid%2520planner%2520that%2520combines%2520pre-computed%2520lattice%2520maps%2520with%2520dynamic%2520free-space%250Asampling%2520to%2520efficiently%2520generate%2520optimal%2520driveable%2520corridors%2520in%2520cluttered%250Ascenarios.%2520Our%2520system%2520also%2520features%2520sequential%2520convex%2520programming%2520%2528SCP%2529-based%250Amodel%2520predictive%2520control%2520%2528MPC%2529%2520to%2520refine%2520the%2520corridors%2520into%2520smooth%252C%2520dynamically%250Aconsistent%2520trajectories.%2520A%2520single%2520optimization%2520problem%2520is%2520used%2520to%2520both%2520generate%250Aa%2520trajectory%2520and%2520its%2520corresponding%2520control%2520commands%253B%2520this%2520addresses%2520limitations%250Aof%2520decoupled%2520approaches%2520by%2520guaranteeing%2520a%2520safe%2520and%2520feasible%2520path.%2520Simulation%250Aresults%2520of%2520the%2520novel%2520planner%2520on%2520randomly%2520generated%2520obstacle-rich%2520scenarios%250Ademonstrate%2520the%2520success%2520rate%2520of%2520a%2520free-space%2520Adaptively%2520Informed%2520Trees%252A%250A%2528AIT%252A%2529-based%2520planner%252C%2520and%2520runtimes%2520comparable%2520to%2520a%2520lattice-based%2520planner.%250AReal-world%2520experiments%2520of%2520the%2520full%2520system%2520on%2520a%2520Chevrolet%2520Bolt%2520EUV%2520further%250Avalidate%2520performance%2520in%2520dense%2520obstacle%2520fields%252C%2520demonstrating%2520no%2520violations%2520of%250Atraffic%252C%2520kinematic%252C%2520or%2520vehicle%2520constraints%252C%2520and%2520a%2520100%2525%2520success%2520rate%2520across%250Aeight%2520trials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=aUToPath%3A%20Unified%20Planning%20and%20Control%20for%20Autonomous%20Vehicles%20in%20Urban%0A%20%20Environments%20Using%20Hybrid%20Lattice%20and%20Free-Space%20Search&entry.906535625=Tanmay%20P.%20Patel%20and%20Connor%20Wilson%20and%20Ellina%20R.%20Zhang%20and%20Morgan%20Tran%20and%20Chang%20Keun%20Paik%20and%20Steven%20L.%20Waslander%20and%20Timothy%20D.%20Barfoot&entry.1292438233=%20%20This%20paper%20presents%20aUToPath%2C%20a%20unified%20online%20framework%20for%20global%0Apath-planning%20and%20control%20to%20address%20the%20challenge%20of%20autonomous%20navigation%20in%0Acluttered%20urban%20environments.%20A%20key%20component%20of%20our%20framework%20is%20a%20novel%0Ahybrid%20planner%20that%20combines%20pre-computed%20lattice%20maps%20with%20dynamic%20free-space%0Asampling%20to%20efficiently%20generate%20optimal%20driveable%20corridors%20in%20cluttered%0Ascenarios.%20Our%20system%20also%20features%20sequential%20convex%20programming%20%28SCP%29-based%0Amodel%20predictive%20control%20%28MPC%29%20to%20refine%20the%20corridors%20into%20smooth%2C%20dynamically%0Aconsistent%20trajectories.%20A%20single%20optimization%20problem%20is%20used%20to%20both%20generate%0Aa%20trajectory%20and%20its%20corresponding%20control%20commands%3B%20this%20addresses%20limitations%0Aof%20decoupled%20approaches%20by%20guaranteeing%20a%20safe%20and%20feasible%20path.%20Simulation%0Aresults%20of%20the%20novel%20planner%20on%20randomly%20generated%20obstacle-rich%20scenarios%0Ademonstrate%20the%20success%20rate%20of%20a%20free-space%20Adaptively%20Informed%20Trees%2A%0A%28AIT%2A%29-based%20planner%2C%20and%20runtimes%20comparable%20to%20a%20lattice-based%20planner.%0AReal-world%20experiments%20of%20the%20full%20system%20on%20a%20Chevrolet%20Bolt%20EUV%20further%0Avalidate%20performance%20in%20dense%20obstacle%20fields%2C%20demonstrating%20no%20violations%20of%0Atraffic%2C%20kinematic%2C%20or%20vehicle%20constraints%2C%20and%20a%20100%25%20success%20rate%20across%0Aeight%20trials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09475v1&entry.124074799=Read"},
{"title": "UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing", "author": "Yung-Hsuan Lai and Janek Ebbers and Yu-Chiang Frank Wang and Fran\u00e7ois Germain and Michael Jeffrey Jones and Moitreya Chatterjee", "abstract": "  Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing\nboth uni-modal events (i.e., those occurring exclusively in either the visual\nor acoustic modality of a video) and multi-modal events (i.e., those occurring\nin both modalities concurrently). Moreover, the prohibitive cost of annotating\ntraining data with the class labels of all these events, along with their start\nand end times, imposes constraints on the scalability of AVVP techniques unless\nthey can be trained in a weakly-supervised setting, where only\nmodality-agnostic, video-level labels are available in the training data. To\nthis end, recently proposed approaches seek to generate segment-level\npseudo-labels to better guide model training. However, the absence of\ninter-segment dependencies when generating these pseudo-labels and the general\nbias towards predicting labels that are absent in a segment limit their\nperformance. This work proposes a novel approach towards overcoming these\nweaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video\nParsing (UWAV). Additionally, our innovative approach factors in the\nuncertainty associated with these estimated pseudo-labels and incorporates a\nfeature mixup based training regularization for improved training. Empirical\nresults show that UWAV outperforms state-of-the-art methods for the AVVP task\non multiple metrics, across two different datasets, attesting to its\neffectiveness and generalizability.\n", "link": "http://arxiv.org/abs/2505.09615v1", "date": "2025-05-14", "relevancy": 2.2195, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5772}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5749}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UWAV%3A%20Uncertainty-weighted%20Weakly-supervised%20Audio-Visual%20Video%20Parsing&body=Title%3A%20UWAV%3A%20Uncertainty-weighted%20Weakly-supervised%20Audio-Visual%20Video%20Parsing%0AAuthor%3A%20Yung-Hsuan%20Lai%20and%20Janek%20Ebbers%20and%20Yu-Chiang%20Frank%20Wang%20and%20Fran%C3%A7ois%20Germain%20and%20Michael%20Jeffrey%20Jones%20and%20Moitreya%20Chatterjee%0AAbstract%3A%20%20%20Audio-Visual%20Video%20Parsing%20%28AVVP%29%20entails%20the%20challenging%20task%20of%20localizing%0Aboth%20uni-modal%20events%20%28i.e.%2C%20those%20occurring%20exclusively%20in%20either%20the%20visual%0Aor%20acoustic%20modality%20of%20a%20video%29%20and%20multi-modal%20events%20%28i.e.%2C%20those%20occurring%0Ain%20both%20modalities%20concurrently%29.%20Moreover%2C%20the%20prohibitive%20cost%20of%20annotating%0Atraining%20data%20with%20the%20class%20labels%20of%20all%20these%20events%2C%20along%20with%20their%20start%0Aand%20end%20times%2C%20imposes%20constraints%20on%20the%20scalability%20of%20AVVP%20techniques%20unless%0Athey%20can%20be%20trained%20in%20a%20weakly-supervised%20setting%2C%20where%20only%0Amodality-agnostic%2C%20video-level%20labels%20are%20available%20in%20the%20training%20data.%20To%0Athis%20end%2C%20recently%20proposed%20approaches%20seek%20to%20generate%20segment-level%0Apseudo-labels%20to%20better%20guide%20model%20training.%20However%2C%20the%20absence%20of%0Ainter-segment%20dependencies%20when%20generating%20these%20pseudo-labels%20and%20the%20general%0Abias%20towards%20predicting%20labels%20that%20are%20absent%20in%20a%20segment%20limit%20their%0Aperformance.%20This%20work%20proposes%20a%20novel%20approach%20towards%20overcoming%20these%0Aweaknesses%20called%20Uncertainty-weighted%20Weakly-supervised%20Audio-visual%20Video%0AParsing%20%28UWAV%29.%20Additionally%2C%20our%20innovative%20approach%20factors%20in%20the%0Auncertainty%20associated%20with%20these%20estimated%20pseudo-labels%20and%20incorporates%20a%0Afeature%20mixup%20based%20training%20regularization%20for%20improved%20training.%20Empirical%0Aresults%20show%20that%20UWAV%20outperforms%20state-of-the-art%20methods%20for%20the%20AVVP%20task%0Aon%20multiple%20metrics%2C%20across%20two%20different%20datasets%2C%20attesting%20to%20its%0Aeffectiveness%20and%20generalizability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUWAV%253A%2520Uncertainty-weighted%2520Weakly-supervised%2520Audio-Visual%2520Video%2520Parsing%26entry.906535625%3DYung-Hsuan%2520Lai%2520and%2520Janek%2520Ebbers%2520and%2520Yu-Chiang%2520Frank%2520Wang%2520and%2520Fran%25C3%25A7ois%2520Germain%2520and%2520Michael%2520Jeffrey%2520Jones%2520and%2520Moitreya%2520Chatterjee%26entry.1292438233%3D%2520%2520Audio-Visual%2520Video%2520Parsing%2520%2528AVVP%2529%2520entails%2520the%2520challenging%2520task%2520of%2520localizing%250Aboth%2520uni-modal%2520events%2520%2528i.e.%252C%2520those%2520occurring%2520exclusively%2520in%2520either%2520the%2520visual%250Aor%2520acoustic%2520modality%2520of%2520a%2520video%2529%2520and%2520multi-modal%2520events%2520%2528i.e.%252C%2520those%2520occurring%250Ain%2520both%2520modalities%2520concurrently%2529.%2520Moreover%252C%2520the%2520prohibitive%2520cost%2520of%2520annotating%250Atraining%2520data%2520with%2520the%2520class%2520labels%2520of%2520all%2520these%2520events%252C%2520along%2520with%2520their%2520start%250Aand%2520end%2520times%252C%2520imposes%2520constraints%2520on%2520the%2520scalability%2520of%2520AVVP%2520techniques%2520unless%250Athey%2520can%2520be%2520trained%2520in%2520a%2520weakly-supervised%2520setting%252C%2520where%2520only%250Amodality-agnostic%252C%2520video-level%2520labels%2520are%2520available%2520in%2520the%2520training%2520data.%2520To%250Athis%2520end%252C%2520recently%2520proposed%2520approaches%2520seek%2520to%2520generate%2520segment-level%250Apseudo-labels%2520to%2520better%2520guide%2520model%2520training.%2520However%252C%2520the%2520absence%2520of%250Ainter-segment%2520dependencies%2520when%2520generating%2520these%2520pseudo-labels%2520and%2520the%2520general%250Abias%2520towards%2520predicting%2520labels%2520that%2520are%2520absent%2520in%2520a%2520segment%2520limit%2520their%250Aperformance.%2520This%2520work%2520proposes%2520a%2520novel%2520approach%2520towards%2520overcoming%2520these%250Aweaknesses%2520called%2520Uncertainty-weighted%2520Weakly-supervised%2520Audio-visual%2520Video%250AParsing%2520%2528UWAV%2529.%2520Additionally%252C%2520our%2520innovative%2520approach%2520factors%2520in%2520the%250Auncertainty%2520associated%2520with%2520these%2520estimated%2520pseudo-labels%2520and%2520incorporates%2520a%250Afeature%2520mixup%2520based%2520training%2520regularization%2520for%2520improved%2520training.%2520Empirical%250Aresults%2520show%2520that%2520UWAV%2520outperforms%2520state-of-the-art%2520methods%2520for%2520the%2520AVVP%2520task%250Aon%2520multiple%2520metrics%252C%2520across%2520two%2520different%2520datasets%252C%2520attesting%2520to%2520its%250Aeffectiveness%2520and%2520generalizability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UWAV%3A%20Uncertainty-weighted%20Weakly-supervised%20Audio-Visual%20Video%20Parsing&entry.906535625=Yung-Hsuan%20Lai%20and%20Janek%20Ebbers%20and%20Yu-Chiang%20Frank%20Wang%20and%20Fran%C3%A7ois%20Germain%20and%20Michael%20Jeffrey%20Jones%20and%20Moitreya%20Chatterjee&entry.1292438233=%20%20Audio-Visual%20Video%20Parsing%20%28AVVP%29%20entails%20the%20challenging%20task%20of%20localizing%0Aboth%20uni-modal%20events%20%28i.e.%2C%20those%20occurring%20exclusively%20in%20either%20the%20visual%0Aor%20acoustic%20modality%20of%20a%20video%29%20and%20multi-modal%20events%20%28i.e.%2C%20those%20occurring%0Ain%20both%20modalities%20concurrently%29.%20Moreover%2C%20the%20prohibitive%20cost%20of%20annotating%0Atraining%20data%20with%20the%20class%20labels%20of%20all%20these%20events%2C%20along%20with%20their%20start%0Aand%20end%20times%2C%20imposes%20constraints%20on%20the%20scalability%20of%20AVVP%20techniques%20unless%0Athey%20can%20be%20trained%20in%20a%20weakly-supervised%20setting%2C%20where%20only%0Amodality-agnostic%2C%20video-level%20labels%20are%20available%20in%20the%20training%20data.%20To%0Athis%20end%2C%20recently%20proposed%20approaches%20seek%20to%20generate%20segment-level%0Apseudo-labels%20to%20better%20guide%20model%20training.%20However%2C%20the%20absence%20of%0Ainter-segment%20dependencies%20when%20generating%20these%20pseudo-labels%20and%20the%20general%0Abias%20towards%20predicting%20labels%20that%20are%20absent%20in%20a%20segment%20limit%20their%0Aperformance.%20This%20work%20proposes%20a%20novel%20approach%20towards%20overcoming%20these%0Aweaknesses%20called%20Uncertainty-weighted%20Weakly-supervised%20Audio-visual%20Video%0AParsing%20%28UWAV%29.%20Additionally%2C%20our%20innovative%20approach%20factors%20in%20the%0Auncertainty%20associated%20with%20these%20estimated%20pseudo-labels%20and%20incorporates%20a%0Afeature%20mixup%20based%20training%20regularization%20for%20improved%20training.%20Empirical%0Aresults%20show%20that%20UWAV%20outperforms%20state-of-the-art%20methods%20for%20the%20AVVP%20task%0Aon%20multiple%20metrics%2C%20across%20two%20different%20datasets%2C%20attesting%20to%20its%0Aeffectiveness%20and%20generalizability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09615v1&entry.124074799=Read"},
{"title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and\n  Segmentation", "author": "Guan Gui and Bin-Bin Gao and Jun Liu and Chengjie Wang and Yunsheng Wu", "abstract": "  Anomaly detection is a practical and challenging task due to the scarcity of\nanomaly samples in industrial inspection. Some existing anomaly detection\nmethods address this issue by synthesizing anomalies with noise or external\ndata. However, there is always a large semantic gap between synthetic and\nreal-world anomalies, resulting in weak performance in anomaly detection. To\nsolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)\nmethod, which guides the diffusion model to generate realistic and diverse\nanomalies with only a few real anomalies, thereby benefiting training anomaly\ndetection models. Specifically, our work is divided into three stages. In the\nfirst stage, we learn the anomaly distribution based on a few given real\nanomalies and inject the learned knowledge into an embedding. In the second\nstage, we use the embedding and given bounding boxes to guide the diffusion\nmodel to generate realistic and diverse anomalies on specific objects (or\ntextures). In the final stage, we propose a weakly-supervised anomaly detection\nmethod to train a more powerful model with generated anomalies. Our method\nbuilds upon DRAEM and DesTSeg as the foundation model and conducts experiments\non the commonly used industrial anomaly detection dataset, MVTec. The\nexperiments demonstrate that our generated anomalies effectively improve the\nmodel performance of both anomaly classification and segmentation tasks\nsimultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement\nin AU-PR metric on segmentation task, respectively. The code and generated\nanomalous data are available at https://github.com/gaobb/AnoGen.\n", "link": "http://arxiv.org/abs/2505.09263v1", "date": "2025-05-14", "relevancy": 2.2189, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5639}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5489}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Anomaly-Driven%20Generation%20for%20Anomaly%20Classification%20and%0A%20%20Segmentation&body=Title%3A%20Few-Shot%20Anomaly-Driven%20Generation%20for%20Anomaly%20Classification%20and%0A%20%20Segmentation%0AAuthor%3A%20Guan%20Gui%20and%20Bin-Bin%20Gao%20and%20Jun%20Liu%20and%20Chengjie%20Wang%20and%20Yunsheng%20Wu%0AAbstract%3A%20%20%20Anomaly%20detection%20is%20a%20practical%20and%20challenging%20task%20due%20to%20the%20scarcity%20of%0Aanomaly%20samples%20in%20industrial%20inspection.%20Some%20existing%20anomaly%20detection%0Amethods%20address%20this%20issue%20by%20synthesizing%20anomalies%20with%20noise%20or%20external%0Adata.%20However%2C%20there%20is%20always%20a%20large%20semantic%20gap%20between%20synthetic%20and%0Areal-world%20anomalies%2C%20resulting%20in%20weak%20performance%20in%20anomaly%20detection.%20To%0Asolve%20the%20problem%2C%20we%20propose%20a%20few-shot%20Anomaly-driven%20Generation%20%28AnoGen%29%0Amethod%2C%20which%20guides%20the%20diffusion%20model%20to%20generate%20realistic%20and%20diverse%0Aanomalies%20with%20only%20a%20few%20real%20anomalies%2C%20thereby%20benefiting%20training%20anomaly%0Adetection%20models.%20Specifically%2C%20our%20work%20is%20divided%20into%20three%20stages.%20In%20the%0Afirst%20stage%2C%20we%20learn%20the%20anomaly%20distribution%20based%20on%20a%20few%20given%20real%0Aanomalies%20and%20inject%20the%20learned%20knowledge%20into%20an%20embedding.%20In%20the%20second%0Astage%2C%20we%20use%20the%20embedding%20and%20given%20bounding%20boxes%20to%20guide%20the%20diffusion%0Amodel%20to%20generate%20realistic%20and%20diverse%20anomalies%20on%20specific%20objects%20%28or%0Atextures%29.%20In%20the%20final%20stage%2C%20we%20propose%20a%20weakly-supervised%20anomaly%20detection%0Amethod%20to%20train%20a%20more%20powerful%20model%20with%20generated%20anomalies.%20Our%20method%0Abuilds%20upon%20DRAEM%20and%20DesTSeg%20as%20the%20foundation%20model%20and%20conducts%20experiments%0Aon%20the%20commonly%20used%20industrial%20anomaly%20detection%20dataset%2C%20MVTec.%20The%0Aexperiments%20demonstrate%20that%20our%20generated%20anomalies%20effectively%20improve%20the%0Amodel%20performance%20of%20both%20anomaly%20classification%20and%20segmentation%20tasks%0Asimultaneously%2C%20%5Ceg%2C%20DRAEM%20and%20DseTSeg%20achieved%20a%205.8%5C%25%20and%201.5%5C%25%20improvement%0Ain%20AU-PR%20metric%20on%20segmentation%20task%2C%20respectively.%20The%20code%20and%20generated%0Aanomalous%20data%20are%20available%20at%20https%3A//github.com/gaobb/AnoGen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Anomaly-Driven%2520Generation%2520for%2520Anomaly%2520Classification%2520and%250A%2520%2520Segmentation%26entry.906535625%3DGuan%2520Gui%2520and%2520Bin-Bin%2520Gao%2520and%2520Jun%2520Liu%2520and%2520Chengjie%2520Wang%2520and%2520Yunsheng%2520Wu%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520is%2520a%2520practical%2520and%2520challenging%2520task%2520due%2520to%2520the%2520scarcity%2520of%250Aanomaly%2520samples%2520in%2520industrial%2520inspection.%2520Some%2520existing%2520anomaly%2520detection%250Amethods%2520address%2520this%2520issue%2520by%2520synthesizing%2520anomalies%2520with%2520noise%2520or%2520external%250Adata.%2520However%252C%2520there%2520is%2520always%2520a%2520large%2520semantic%2520gap%2520between%2520synthetic%2520and%250Areal-world%2520anomalies%252C%2520resulting%2520in%2520weak%2520performance%2520in%2520anomaly%2520detection.%2520To%250Asolve%2520the%2520problem%252C%2520we%2520propose%2520a%2520few-shot%2520Anomaly-driven%2520Generation%2520%2528AnoGen%2529%250Amethod%252C%2520which%2520guides%2520the%2520diffusion%2520model%2520to%2520generate%2520realistic%2520and%2520diverse%250Aanomalies%2520with%2520only%2520a%2520few%2520real%2520anomalies%252C%2520thereby%2520benefiting%2520training%2520anomaly%250Adetection%2520models.%2520Specifically%252C%2520our%2520work%2520is%2520divided%2520into%2520three%2520stages.%2520In%2520the%250Afirst%2520stage%252C%2520we%2520learn%2520the%2520anomaly%2520distribution%2520based%2520on%2520a%2520few%2520given%2520real%250Aanomalies%2520and%2520inject%2520the%2520learned%2520knowledge%2520into%2520an%2520embedding.%2520In%2520the%2520second%250Astage%252C%2520we%2520use%2520the%2520embedding%2520and%2520given%2520bounding%2520boxes%2520to%2520guide%2520the%2520diffusion%250Amodel%2520to%2520generate%2520realistic%2520and%2520diverse%2520anomalies%2520on%2520specific%2520objects%2520%2528or%250Atextures%2529.%2520In%2520the%2520final%2520stage%252C%2520we%2520propose%2520a%2520weakly-supervised%2520anomaly%2520detection%250Amethod%2520to%2520train%2520a%2520more%2520powerful%2520model%2520with%2520generated%2520anomalies.%2520Our%2520method%250Abuilds%2520upon%2520DRAEM%2520and%2520DesTSeg%2520as%2520the%2520foundation%2520model%2520and%2520conducts%2520experiments%250Aon%2520the%2520commonly%2520used%2520industrial%2520anomaly%2520detection%2520dataset%252C%2520MVTec.%2520The%250Aexperiments%2520demonstrate%2520that%2520our%2520generated%2520anomalies%2520effectively%2520improve%2520the%250Amodel%2520performance%2520of%2520both%2520anomaly%2520classification%2520and%2520segmentation%2520tasks%250Asimultaneously%252C%2520%255Ceg%252C%2520DRAEM%2520and%2520DseTSeg%2520achieved%2520a%25205.8%255C%2525%2520and%25201.5%255C%2525%2520improvement%250Ain%2520AU-PR%2520metric%2520on%2520segmentation%2520task%252C%2520respectively.%2520The%2520code%2520and%2520generated%250Aanomalous%2520data%2520are%2520available%2520at%2520https%253A//github.com/gaobb/AnoGen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Anomaly-Driven%20Generation%20for%20Anomaly%20Classification%20and%0A%20%20Segmentation&entry.906535625=Guan%20Gui%20and%20Bin-Bin%20Gao%20and%20Jun%20Liu%20and%20Chengjie%20Wang%20and%20Yunsheng%20Wu&entry.1292438233=%20%20Anomaly%20detection%20is%20a%20practical%20and%20challenging%20task%20due%20to%20the%20scarcity%20of%0Aanomaly%20samples%20in%20industrial%20inspection.%20Some%20existing%20anomaly%20detection%0Amethods%20address%20this%20issue%20by%20synthesizing%20anomalies%20with%20noise%20or%20external%0Adata.%20However%2C%20there%20is%20always%20a%20large%20semantic%20gap%20between%20synthetic%20and%0Areal-world%20anomalies%2C%20resulting%20in%20weak%20performance%20in%20anomaly%20detection.%20To%0Asolve%20the%20problem%2C%20we%20propose%20a%20few-shot%20Anomaly-driven%20Generation%20%28AnoGen%29%0Amethod%2C%20which%20guides%20the%20diffusion%20model%20to%20generate%20realistic%20and%20diverse%0Aanomalies%20with%20only%20a%20few%20real%20anomalies%2C%20thereby%20benefiting%20training%20anomaly%0Adetection%20models.%20Specifically%2C%20our%20work%20is%20divided%20into%20three%20stages.%20In%20the%0Afirst%20stage%2C%20we%20learn%20the%20anomaly%20distribution%20based%20on%20a%20few%20given%20real%0Aanomalies%20and%20inject%20the%20learned%20knowledge%20into%20an%20embedding.%20In%20the%20second%0Astage%2C%20we%20use%20the%20embedding%20and%20given%20bounding%20boxes%20to%20guide%20the%20diffusion%0Amodel%20to%20generate%20realistic%20and%20diverse%20anomalies%20on%20specific%20objects%20%28or%0Atextures%29.%20In%20the%20final%20stage%2C%20we%20propose%20a%20weakly-supervised%20anomaly%20detection%0Amethod%20to%20train%20a%20more%20powerful%20model%20with%20generated%20anomalies.%20Our%20method%0Abuilds%20upon%20DRAEM%20and%20DesTSeg%20as%20the%20foundation%20model%20and%20conducts%20experiments%0Aon%20the%20commonly%20used%20industrial%20anomaly%20detection%20dataset%2C%20MVTec.%20The%0Aexperiments%20demonstrate%20that%20our%20generated%20anomalies%20effectively%20improve%20the%0Amodel%20performance%20of%20both%20anomaly%20classification%20and%20segmentation%20tasks%0Asimultaneously%2C%20%5Ceg%2C%20DRAEM%20and%20DseTSeg%20achieved%20a%205.8%5C%25%20and%201.5%5C%25%20improvement%0Ain%20AU-PR%20metric%20on%20segmentation%20task%2C%20respectively.%20The%20code%20and%20generated%0Aanomalous%20data%20are%20available%20at%20https%3A//github.com/gaobb/AnoGen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09263v1&entry.124074799=Read"},
{"title": "Video-R1: Reinforcing Video Reasoning in MLLMs", "author": "Kaituo Feng and Kaixiong Gong and Bohao Li and Zonghao Guo and Yibing Wang and Tianshuo Peng and Junfei Wu and Xiaoying Zhang and Benyou Wang and Xiangyu Yue", "abstract": "  Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for incentivizing video\nreasoning within multimodal large language models (MLLMs). However, directly\napplying RL training with the GRPO algorithm to video reasoning presents two\nprimary challenges: (i) a lack of temporal modeling for video reasoning, and\n(ii) the scarcity of high-quality video-reasoning data. To address these\nissues, we first propose the T-GRPO algorithm, which encourages models to\nutilize temporal information in videos for reasoning. Additionally, instead of\nrelying solely on video data, we incorporate high-quality image-reasoning data\ninto the training process. We have constructed two datasets: Video-R1-CoT-165k\nfor SFT cold start and Video-R1-260k for RL training, both comprising image and\nvideo data. Experimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncode, models, and data are released in: https://github.com/tulerfeng/Video-R1.\n", "link": "http://arxiv.org/abs/2503.21776v2", "date": "2025-05-14", "relevancy": 2.212, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-R1%3A%20Reinforcing%20Video%20Reasoning%20in%20MLLMs&body=Title%3A%20Video-R1%3A%20Reinforcing%20Video%20Reasoning%20in%20MLLMs%0AAuthor%3A%20Kaituo%20Feng%20and%20Kaixiong%20Gong%20and%20Bohao%20Li%20and%20Zonghao%20Guo%20and%20Yibing%20Wang%20and%20Tianshuo%20Peng%20and%20Junfei%20Wu%20and%20Xiaoying%20Zhang%20and%20Benyou%20Wang%20and%20Xiangyu%20Yue%0AAbstract%3A%20%20%20Inspired%20by%20DeepSeek-R1%27s%20success%20in%20eliciting%20reasoning%20abilities%20through%0Arule-based%20reinforcement%20learning%20%28RL%29%2C%20we%20introduce%20Video-R1%20as%20the%20first%0Aattempt%20to%20systematically%20explore%20the%20R1%20paradigm%20for%20incentivizing%20video%0Areasoning%20within%20multimodal%20large%20language%20models%20%28MLLMs%29.%20However%2C%20directly%0Aapplying%20RL%20training%20with%20the%20GRPO%20algorithm%20to%20video%20reasoning%20presents%20two%0Aprimary%20challenges%3A%20%28i%29%20a%20lack%20of%20temporal%20modeling%20for%20video%20reasoning%2C%20and%0A%28ii%29%20the%20scarcity%20of%20high-quality%20video-reasoning%20data.%20To%20address%20these%0Aissues%2C%20we%20first%20propose%20the%20T-GRPO%20algorithm%2C%20which%20encourages%20models%20to%0Autilize%20temporal%20information%20in%20videos%20for%20reasoning.%20Additionally%2C%20instead%20of%0Arelying%20solely%20on%20video%20data%2C%20we%20incorporate%20high-quality%20image-reasoning%20data%0Ainto%20the%20training%20process.%20We%20have%20constructed%20two%20datasets%3A%20Video-R1-CoT-165k%0Afor%20SFT%20cold%20start%20and%20Video-R1-260k%20for%20RL%20training%2C%20both%20comprising%20image%20and%0Avideo%20data.%20Experimental%20results%20demonstrate%20that%20Video-R1%20achieves%20significant%0Aimprovements%20on%20video%20reasoning%20benchmarks%20such%20as%20VideoMMMU%20and%20VSI-Bench%2C%20as%0Awell%20as%20on%20general%20video%20benchmarks%20including%20MVBench%20and%20TempCompass%2C%20etc.%0ANotably%2C%20Video-R1-7B%20attains%20a%2037.1%25%20accuracy%20on%20video%20spatial%20reasoning%0Abenchmark%20VSI-bench%2C%20surpassing%20the%20commercial%20proprietary%20model%20GPT-4o.%20All%0Acode%2C%20models%2C%20and%20data%20are%20released%20in%3A%20https%3A//github.com/tulerfeng/Video-R1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21776v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-R1%253A%2520Reinforcing%2520Video%2520Reasoning%2520in%2520MLLMs%26entry.906535625%3DKaituo%2520Feng%2520and%2520Kaixiong%2520Gong%2520and%2520Bohao%2520Li%2520and%2520Zonghao%2520Guo%2520and%2520Yibing%2520Wang%2520and%2520Tianshuo%2520Peng%2520and%2520Junfei%2520Wu%2520and%2520Xiaoying%2520Zhang%2520and%2520Benyou%2520Wang%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3D%2520%2520Inspired%2520by%2520DeepSeek-R1%2527s%2520success%2520in%2520eliciting%2520reasoning%2520abilities%2520through%250Arule-based%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520we%2520introduce%2520Video-R1%2520as%2520the%2520first%250Aattempt%2520to%2520systematically%2520explore%2520the%2520R1%2520paradigm%2520for%2520incentivizing%2520video%250Areasoning%2520within%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520However%252C%2520directly%250Aapplying%2520RL%2520training%2520with%2520the%2520GRPO%2520algorithm%2520to%2520video%2520reasoning%2520presents%2520two%250Aprimary%2520challenges%253A%2520%2528i%2529%2520a%2520lack%2520of%2520temporal%2520modeling%2520for%2520video%2520reasoning%252C%2520and%250A%2528ii%2529%2520the%2520scarcity%2520of%2520high-quality%2520video-reasoning%2520data.%2520To%2520address%2520these%250Aissues%252C%2520we%2520first%2520propose%2520the%2520T-GRPO%2520algorithm%252C%2520which%2520encourages%2520models%2520to%250Autilize%2520temporal%2520information%2520in%2520videos%2520for%2520reasoning.%2520Additionally%252C%2520instead%2520of%250Arelying%2520solely%2520on%2520video%2520data%252C%2520we%2520incorporate%2520high-quality%2520image-reasoning%2520data%250Ainto%2520the%2520training%2520process.%2520We%2520have%2520constructed%2520two%2520datasets%253A%2520Video-R1-CoT-165k%250Afor%2520SFT%2520cold%2520start%2520and%2520Video-R1-260k%2520for%2520RL%2520training%252C%2520both%2520comprising%2520image%2520and%250Avideo%2520data.%2520Experimental%2520results%2520demonstrate%2520that%2520Video-R1%2520achieves%2520significant%250Aimprovements%2520on%2520video%2520reasoning%2520benchmarks%2520such%2520as%2520VideoMMMU%2520and%2520VSI-Bench%252C%2520as%250Awell%2520as%2520on%2520general%2520video%2520benchmarks%2520including%2520MVBench%2520and%2520TempCompass%252C%2520etc.%250ANotably%252C%2520Video-R1-7B%2520attains%2520a%252037.1%2525%2520accuracy%2520on%2520video%2520spatial%2520reasoning%250Abenchmark%2520VSI-bench%252C%2520surpassing%2520the%2520commercial%2520proprietary%2520model%2520GPT-4o.%2520All%250Acode%252C%2520models%252C%2520and%2520data%2520are%2520released%2520in%253A%2520https%253A//github.com/tulerfeng/Video-R1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21776v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-R1%3A%20Reinforcing%20Video%20Reasoning%20in%20MLLMs&entry.906535625=Kaituo%20Feng%20and%20Kaixiong%20Gong%20and%20Bohao%20Li%20and%20Zonghao%20Guo%20and%20Yibing%20Wang%20and%20Tianshuo%20Peng%20and%20Junfei%20Wu%20and%20Xiaoying%20Zhang%20and%20Benyou%20Wang%20and%20Xiangyu%20Yue&entry.1292438233=%20%20Inspired%20by%20DeepSeek-R1%27s%20success%20in%20eliciting%20reasoning%20abilities%20through%0Arule-based%20reinforcement%20learning%20%28RL%29%2C%20we%20introduce%20Video-R1%20as%20the%20first%0Aattempt%20to%20systematically%20explore%20the%20R1%20paradigm%20for%20incentivizing%20video%0Areasoning%20within%20multimodal%20large%20language%20models%20%28MLLMs%29.%20However%2C%20directly%0Aapplying%20RL%20training%20with%20the%20GRPO%20algorithm%20to%20video%20reasoning%20presents%20two%0Aprimary%20challenges%3A%20%28i%29%20a%20lack%20of%20temporal%20modeling%20for%20video%20reasoning%2C%20and%0A%28ii%29%20the%20scarcity%20of%20high-quality%20video-reasoning%20data.%20To%20address%20these%0Aissues%2C%20we%20first%20propose%20the%20T-GRPO%20algorithm%2C%20which%20encourages%20models%20to%0Autilize%20temporal%20information%20in%20videos%20for%20reasoning.%20Additionally%2C%20instead%20of%0Arelying%20solely%20on%20video%20data%2C%20we%20incorporate%20high-quality%20image-reasoning%20data%0Ainto%20the%20training%20process.%20We%20have%20constructed%20two%20datasets%3A%20Video-R1-CoT-165k%0Afor%20SFT%20cold%20start%20and%20Video-R1-260k%20for%20RL%20training%2C%20both%20comprising%20image%20and%0Avideo%20data.%20Experimental%20results%20demonstrate%20that%20Video-R1%20achieves%20significant%0Aimprovements%20on%20video%20reasoning%20benchmarks%20such%20as%20VideoMMMU%20and%20VSI-Bench%2C%20as%0Awell%20as%20on%20general%20video%20benchmarks%20including%20MVBench%20and%20TempCompass%2C%20etc.%0ANotably%2C%20Video-R1-7B%20attains%20a%2037.1%25%20accuracy%20on%20video%20spatial%20reasoning%0Abenchmark%20VSI-bench%2C%20surpassing%20the%20commercial%20proprietary%20model%20GPT-4o.%20All%0Acode%2C%20models%2C%20and%20data%20are%20released%20in%3A%20https%3A//github.com/tulerfeng/Video-R1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21776v2&entry.124074799=Read"},
{"title": "MCP-MedSAM: A Powerful Lightweight Medical Segment Anything Model\n  Trained with a Single GPU in Just One Day", "author": "Donghang Lyu and Ruochen Gao and Marius Staring", "abstract": "  Medical image segmentation involves partitioning medical images into\nmeaningful regions, with a focus on identifying anatomical structures and\nlesions. It has broad applications in healthcare, and deep learning methods\nhave enabled significant advancements in automating this process. Recently, the\nintroduction of the Segmentation Anything Model (SAM), the first foundation\nmodel for segmentation task, has prompted researchers to adapt it for the\nmedical domain to improve performance across various tasks. However, SAM's\nlarge model size and high GPU requirements hinder its scalability and\ndevelopment in the medical domain. In this work, we propose MCP-MedSAM, a\npowerful and lightweight medical SAM model designed to be trainable on a single\nA100 GPU with 40GB of memory within one day while delivering superior\nsegmentation performance. Recognizing the significant internal differences\nbetween modalities and the need for direct segmentation target information\nwithin bounding boxes, we introduce two kinds of prompts: the modality prompt\nand the content prompt. After passing through the prompt encoder, their\nembedding representations can further improve the segmentation performance by\nincorporating more relevant information without adding significant training\noverhead. Additionally, we adopt an effective modality-based data sampling\nstrategy to address data imbalance between modalities, ensuring more balanced\nperformance across all modalities. Our method was trained and evaluated using a\nlarge-scale challenge dataset, compared to top-ranking methods on the challenge\nleaderboard, MCP-MedSAM achieved superior performance while requiring only one\nday of training on a single GPU. The code is publicly available at\n\\textcolor{blue}{https://github.com/dong845/MCP-MedSAM}.}\n", "link": "http://arxiv.org/abs/2412.05888v2", "date": "2025-05-14", "relevancy": 2.2033, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5682}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5582}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCP-MedSAM%3A%20A%20Powerful%20Lightweight%20Medical%20Segment%20Anything%20Model%0A%20%20Trained%20with%20a%20Single%20GPU%20in%20Just%20One%20Day&body=Title%3A%20MCP-MedSAM%3A%20A%20Powerful%20Lightweight%20Medical%20Segment%20Anything%20Model%0A%20%20Trained%20with%20a%20Single%20GPU%20in%20Just%20One%20Day%0AAuthor%3A%20Donghang%20Lyu%20and%20Ruochen%20Gao%20and%20Marius%20Staring%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20involves%20partitioning%20medical%20images%20into%0Ameaningful%20regions%2C%20with%20a%20focus%20on%20identifying%20anatomical%20structures%20and%0Alesions.%20It%20has%20broad%20applications%20in%20healthcare%2C%20and%20deep%20learning%20methods%0Ahave%20enabled%20significant%20advancements%20in%20automating%20this%20process.%20Recently%2C%20the%0Aintroduction%20of%20the%20Segmentation%20Anything%20Model%20%28SAM%29%2C%20the%20first%20foundation%0Amodel%20for%20segmentation%20task%2C%20has%20prompted%20researchers%20to%20adapt%20it%20for%20the%0Amedical%20domain%20to%20improve%20performance%20across%20various%20tasks.%20However%2C%20SAM%27s%0Alarge%20model%20size%20and%20high%20GPU%20requirements%20hinder%20its%20scalability%20and%0Adevelopment%20in%20the%20medical%20domain.%20In%20this%20work%2C%20we%20propose%20MCP-MedSAM%2C%20a%0Apowerful%20and%20lightweight%20medical%20SAM%20model%20designed%20to%20be%20trainable%20on%20a%20single%0AA100%20GPU%20with%2040GB%20of%20memory%20within%20one%20day%20while%20delivering%20superior%0Asegmentation%20performance.%20Recognizing%20the%20significant%20internal%20differences%0Abetween%20modalities%20and%20the%20need%20for%20direct%20segmentation%20target%20information%0Awithin%20bounding%20boxes%2C%20we%20introduce%20two%20kinds%20of%20prompts%3A%20the%20modality%20prompt%0Aand%20the%20content%20prompt.%20After%20passing%20through%20the%20prompt%20encoder%2C%20their%0Aembedding%20representations%20can%20further%20improve%20the%20segmentation%20performance%20by%0Aincorporating%20more%20relevant%20information%20without%20adding%20significant%20training%0Aoverhead.%20Additionally%2C%20we%20adopt%20an%20effective%20modality-based%20data%20sampling%0Astrategy%20to%20address%20data%20imbalance%20between%20modalities%2C%20ensuring%20more%20balanced%0Aperformance%20across%20all%20modalities.%20Our%20method%20was%20trained%20and%20evaluated%20using%20a%0Alarge-scale%20challenge%20dataset%2C%20compared%20to%20top-ranking%20methods%20on%20the%20challenge%0Aleaderboard%2C%20MCP-MedSAM%20achieved%20superior%20performance%20while%20requiring%20only%20one%0Aday%20of%20training%20on%20a%20single%20GPU.%20The%20code%20is%20publicly%20available%20at%0A%5Ctextcolor%7Bblue%7D%7Bhttps%3A//github.com/dong845/MCP-MedSAM%7D.%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05888v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCP-MedSAM%253A%2520A%2520Powerful%2520Lightweight%2520Medical%2520Segment%2520Anything%2520Model%250A%2520%2520Trained%2520with%2520a%2520Single%2520GPU%2520in%2520Just%2520One%2520Day%26entry.906535625%3DDonghang%2520Lyu%2520and%2520Ruochen%2520Gao%2520and%2520Marius%2520Staring%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520involves%2520partitioning%2520medical%2520images%2520into%250Ameaningful%2520regions%252C%2520with%2520a%2520focus%2520on%2520identifying%2520anatomical%2520structures%2520and%250Alesions.%2520It%2520has%2520broad%2520applications%2520in%2520healthcare%252C%2520and%2520deep%2520learning%2520methods%250Ahave%2520enabled%2520significant%2520advancements%2520in%2520automating%2520this%2520process.%2520Recently%252C%2520the%250Aintroduction%2520of%2520the%2520Segmentation%2520Anything%2520Model%2520%2528SAM%2529%252C%2520the%2520first%2520foundation%250Amodel%2520for%2520segmentation%2520task%252C%2520has%2520prompted%2520researchers%2520to%2520adapt%2520it%2520for%2520the%250Amedical%2520domain%2520to%2520improve%2520performance%2520across%2520various%2520tasks.%2520However%252C%2520SAM%2527s%250Alarge%2520model%2520size%2520and%2520high%2520GPU%2520requirements%2520hinder%2520its%2520scalability%2520and%250Adevelopment%2520in%2520the%2520medical%2520domain.%2520In%2520this%2520work%252C%2520we%2520propose%2520MCP-MedSAM%252C%2520a%250Apowerful%2520and%2520lightweight%2520medical%2520SAM%2520model%2520designed%2520to%2520be%2520trainable%2520on%2520a%2520single%250AA100%2520GPU%2520with%252040GB%2520of%2520memory%2520within%2520one%2520day%2520while%2520delivering%2520superior%250Asegmentation%2520performance.%2520Recognizing%2520the%2520significant%2520internal%2520differences%250Abetween%2520modalities%2520and%2520the%2520need%2520for%2520direct%2520segmentation%2520target%2520information%250Awithin%2520bounding%2520boxes%252C%2520we%2520introduce%2520two%2520kinds%2520of%2520prompts%253A%2520the%2520modality%2520prompt%250Aand%2520the%2520content%2520prompt.%2520After%2520passing%2520through%2520the%2520prompt%2520encoder%252C%2520their%250Aembedding%2520representations%2520can%2520further%2520improve%2520the%2520segmentation%2520performance%2520by%250Aincorporating%2520more%2520relevant%2520information%2520without%2520adding%2520significant%2520training%250Aoverhead.%2520Additionally%252C%2520we%2520adopt%2520an%2520effective%2520modality-based%2520data%2520sampling%250Astrategy%2520to%2520address%2520data%2520imbalance%2520between%2520modalities%252C%2520ensuring%2520more%2520balanced%250Aperformance%2520across%2520all%2520modalities.%2520Our%2520method%2520was%2520trained%2520and%2520evaluated%2520using%2520a%250Alarge-scale%2520challenge%2520dataset%252C%2520compared%2520to%2520top-ranking%2520methods%2520on%2520the%2520challenge%250Aleaderboard%252C%2520MCP-MedSAM%2520achieved%2520superior%2520performance%2520while%2520requiring%2520only%2520one%250Aday%2520of%2520training%2520on%2520a%2520single%2520GPU.%2520The%2520code%2520is%2520publicly%2520available%2520at%250A%255Ctextcolor%257Bblue%257D%257Bhttps%253A//github.com/dong845/MCP-MedSAM%257D.%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05888v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCP-MedSAM%3A%20A%20Powerful%20Lightweight%20Medical%20Segment%20Anything%20Model%0A%20%20Trained%20with%20a%20Single%20GPU%20in%20Just%20One%20Day&entry.906535625=Donghang%20Lyu%20and%20Ruochen%20Gao%20and%20Marius%20Staring&entry.1292438233=%20%20Medical%20image%20segmentation%20involves%20partitioning%20medical%20images%20into%0Ameaningful%20regions%2C%20with%20a%20focus%20on%20identifying%20anatomical%20structures%20and%0Alesions.%20It%20has%20broad%20applications%20in%20healthcare%2C%20and%20deep%20learning%20methods%0Ahave%20enabled%20significant%20advancements%20in%20automating%20this%20process.%20Recently%2C%20the%0Aintroduction%20of%20the%20Segmentation%20Anything%20Model%20%28SAM%29%2C%20the%20first%20foundation%0Amodel%20for%20segmentation%20task%2C%20has%20prompted%20researchers%20to%20adapt%20it%20for%20the%0Amedical%20domain%20to%20improve%20performance%20across%20various%20tasks.%20However%2C%20SAM%27s%0Alarge%20model%20size%20and%20high%20GPU%20requirements%20hinder%20its%20scalability%20and%0Adevelopment%20in%20the%20medical%20domain.%20In%20this%20work%2C%20we%20propose%20MCP-MedSAM%2C%20a%0Apowerful%20and%20lightweight%20medical%20SAM%20model%20designed%20to%20be%20trainable%20on%20a%20single%0AA100%20GPU%20with%2040GB%20of%20memory%20within%20one%20day%20while%20delivering%20superior%0Asegmentation%20performance.%20Recognizing%20the%20significant%20internal%20differences%0Abetween%20modalities%20and%20the%20need%20for%20direct%20segmentation%20target%20information%0Awithin%20bounding%20boxes%2C%20we%20introduce%20two%20kinds%20of%20prompts%3A%20the%20modality%20prompt%0Aand%20the%20content%20prompt.%20After%20passing%20through%20the%20prompt%20encoder%2C%20their%0Aembedding%20representations%20can%20further%20improve%20the%20segmentation%20performance%20by%0Aincorporating%20more%20relevant%20information%20without%20adding%20significant%20training%0Aoverhead.%20Additionally%2C%20we%20adopt%20an%20effective%20modality-based%20data%20sampling%0Astrategy%20to%20address%20data%20imbalance%20between%20modalities%2C%20ensuring%20more%20balanced%0Aperformance%20across%20all%20modalities.%20Our%20method%20was%20trained%20and%20evaluated%20using%20a%0Alarge-scale%20challenge%20dataset%2C%20compared%20to%20top-ranking%20methods%20on%20the%20challenge%0Aleaderboard%2C%20MCP-MedSAM%20achieved%20superior%20performance%20while%20requiring%20only%20one%0Aday%20of%20training%20on%20a%20single%20GPU.%20The%20code%20is%20publicly%20available%20at%0A%5Ctextcolor%7Bblue%7D%7Bhttps%3A//github.com/dong845/MCP-MedSAM%7D.%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05888v2&entry.124074799=Read"},
{"title": "Test-Time Augmentation for Pose-invariant Face Recognition", "author": "Jaemin Jung and Youngjoon Jang and Joon Son Chung", "abstract": "  The goal of this paper is to enhance face recognition performance by\naugmenting head poses during the testing phase. Existing methods often rely on\ntraining on frontalised images or learning pose-invariant representations, yet\nboth approaches typically require re-training and testing for each dataset,\ninvolving a substantial amount of effort. In contrast, this study proposes\nPose-TTA, a novel approach that aligns faces at inference time without\nadditional training. To achieve this, we employ a portrait animator that\ntransfers the source image identity into the pose of a driving image. Instead\nof frontalising a side-profile face -- which can introduce distortion --\nPose-TTA generates matching side-profile images for comparison, thereby\nreducing identity information loss. Furthermore, we propose a weighted feature\naggregation strategy to address any distortions or biases arising from the\nsynthetic data, thus enhancing the reliability of the augmented images.\nExtensive experiments on diverse datasets and with various pre-trained face\nrecognition models demonstrate that Pose-TTA consistently improves inference\nperformance. Moreover, our method is straightforward to integrate into existing\nface recognition pipelines, as it requires no retraining or fine-tuning of the\nunderlying recognition models.\n", "link": "http://arxiv.org/abs/2505.09256v1", "date": "2025-05-14", "relevancy": 2.1851, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5693}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5311}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Augmentation%20for%20Pose-invariant%20Face%20Recognition&body=Title%3A%20Test-Time%20Augmentation%20for%20Pose-invariant%20Face%20Recognition%0AAuthor%3A%20Jaemin%20Jung%20and%20Youngjoon%20Jang%20and%20Joon%20Son%20Chung%0AAbstract%3A%20%20%20The%20goal%20of%20this%20paper%20is%20to%20enhance%20face%20recognition%20performance%20by%0Aaugmenting%20head%20poses%20during%20the%20testing%20phase.%20Existing%20methods%20often%20rely%20on%0Atraining%20on%20frontalised%20images%20or%20learning%20pose-invariant%20representations%2C%20yet%0Aboth%20approaches%20typically%20require%20re-training%20and%20testing%20for%20each%20dataset%2C%0Ainvolving%20a%20substantial%20amount%20of%20effort.%20In%20contrast%2C%20this%20study%20proposes%0APose-TTA%2C%20a%20novel%20approach%20that%20aligns%20faces%20at%20inference%20time%20without%0Aadditional%20training.%20To%20achieve%20this%2C%20we%20employ%20a%20portrait%20animator%20that%0Atransfers%20the%20source%20image%20identity%20into%20the%20pose%20of%20a%20driving%20image.%20Instead%0Aof%20frontalising%20a%20side-profile%20face%20--%20which%20can%20introduce%20distortion%20--%0APose-TTA%20generates%20matching%20side-profile%20images%20for%20comparison%2C%20thereby%0Areducing%20identity%20information%20loss.%20Furthermore%2C%20we%20propose%20a%20weighted%20feature%0Aaggregation%20strategy%20to%20address%20any%20distortions%20or%20biases%20arising%20from%20the%0Asynthetic%20data%2C%20thus%20enhancing%20the%20reliability%20of%20the%20augmented%20images.%0AExtensive%20experiments%20on%20diverse%20datasets%20and%20with%20various%20pre-trained%20face%0Arecognition%20models%20demonstrate%20that%20Pose-TTA%20consistently%20improves%20inference%0Aperformance.%20Moreover%2C%20our%20method%20is%20straightforward%20to%20integrate%20into%20existing%0Aface%20recognition%20pipelines%2C%20as%20it%20requires%20no%20retraining%20or%20fine-tuning%20of%20the%0Aunderlying%20recognition%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-Time%2520Augmentation%2520for%2520Pose-invariant%2520Face%2520Recognition%26entry.906535625%3DJaemin%2520Jung%2520and%2520Youngjoon%2520Jang%2520and%2520Joon%2520Son%2520Chung%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520this%2520paper%2520is%2520to%2520enhance%2520face%2520recognition%2520performance%2520by%250Aaugmenting%2520head%2520poses%2520during%2520the%2520testing%2520phase.%2520Existing%2520methods%2520often%2520rely%2520on%250Atraining%2520on%2520frontalised%2520images%2520or%2520learning%2520pose-invariant%2520representations%252C%2520yet%250Aboth%2520approaches%2520typically%2520require%2520re-training%2520and%2520testing%2520for%2520each%2520dataset%252C%250Ainvolving%2520a%2520substantial%2520amount%2520of%2520effort.%2520In%2520contrast%252C%2520this%2520study%2520proposes%250APose-TTA%252C%2520a%2520novel%2520approach%2520that%2520aligns%2520faces%2520at%2520inference%2520time%2520without%250Aadditional%2520training.%2520To%2520achieve%2520this%252C%2520we%2520employ%2520a%2520portrait%2520animator%2520that%250Atransfers%2520the%2520source%2520image%2520identity%2520into%2520the%2520pose%2520of%2520a%2520driving%2520image.%2520Instead%250Aof%2520frontalising%2520a%2520side-profile%2520face%2520--%2520which%2520can%2520introduce%2520distortion%2520--%250APose-TTA%2520generates%2520matching%2520side-profile%2520images%2520for%2520comparison%252C%2520thereby%250Areducing%2520identity%2520information%2520loss.%2520Furthermore%252C%2520we%2520propose%2520a%2520weighted%2520feature%250Aaggregation%2520strategy%2520to%2520address%2520any%2520distortions%2520or%2520biases%2520arising%2520from%2520the%250Asynthetic%2520data%252C%2520thus%2520enhancing%2520the%2520reliability%2520of%2520the%2520augmented%2520images.%250AExtensive%2520experiments%2520on%2520diverse%2520datasets%2520and%2520with%2520various%2520pre-trained%2520face%250Arecognition%2520models%2520demonstrate%2520that%2520Pose-TTA%2520consistently%2520improves%2520inference%250Aperformance.%2520Moreover%252C%2520our%2520method%2520is%2520straightforward%2520to%2520integrate%2520into%2520existing%250Aface%2520recognition%2520pipelines%252C%2520as%2520it%2520requires%2520no%2520retraining%2520or%2520fine-tuning%2520of%2520the%250Aunderlying%2520recognition%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Augmentation%20for%20Pose-invariant%20Face%20Recognition&entry.906535625=Jaemin%20Jung%20and%20Youngjoon%20Jang%20and%20Joon%20Son%20Chung&entry.1292438233=%20%20The%20goal%20of%20this%20paper%20is%20to%20enhance%20face%20recognition%20performance%20by%0Aaugmenting%20head%20poses%20during%20the%20testing%20phase.%20Existing%20methods%20often%20rely%20on%0Atraining%20on%20frontalised%20images%20or%20learning%20pose-invariant%20representations%2C%20yet%0Aboth%20approaches%20typically%20require%20re-training%20and%20testing%20for%20each%20dataset%2C%0Ainvolving%20a%20substantial%20amount%20of%20effort.%20In%20contrast%2C%20this%20study%20proposes%0APose-TTA%2C%20a%20novel%20approach%20that%20aligns%20faces%20at%20inference%20time%20without%0Aadditional%20training.%20To%20achieve%20this%2C%20we%20employ%20a%20portrait%20animator%20that%0Atransfers%20the%20source%20image%20identity%20into%20the%20pose%20of%20a%20driving%20image.%20Instead%0Aof%20frontalising%20a%20side-profile%20face%20--%20which%20can%20introduce%20distortion%20--%0APose-TTA%20generates%20matching%20side-profile%20images%20for%20comparison%2C%20thereby%0Areducing%20identity%20information%20loss.%20Furthermore%2C%20we%20propose%20a%20weighted%20feature%0Aaggregation%20strategy%20to%20address%20any%20distortions%20or%20biases%20arising%20from%20the%0Asynthetic%20data%2C%20thus%20enhancing%20the%20reliability%20of%20the%20augmented%20images.%0AExtensive%20experiments%20on%20diverse%20datasets%20and%20with%20various%20pre-trained%20face%0Arecognition%20models%20demonstrate%20that%20Pose-TTA%20consistently%20improves%20inference%0Aperformance.%20Moreover%2C%20our%20method%20is%20straightforward%20to%20integrate%20into%20existing%0Aface%20recognition%20pipelines%2C%20as%20it%20requires%20no%20retraining%20or%20fine-tuning%20of%20the%0Aunderlying%20recognition%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09256v1&entry.124074799=Read"},
{"title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures", "author": "Chenggang Zhao and Chengqi Deng and Chong Ruan and Damai Dai and Huazuo Gao and Jiashi Li and Liyue Zhang and Panpan Huang and Shangyan Zhou and Shirong Ma and Wenfeng Liang and Ying He and Yuqing Wang and Yuxuan Liu and Y. X. Wei", "abstract": "  The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems.\n", "link": "http://arxiv.org/abs/2505.09343v1", "date": "2025-05-14", "relevancy": 2.18, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Insights%20into%20DeepSeek-V3%3A%20Scaling%20Challenges%20and%20Reflections%20on%0A%20%20Hardware%20for%20AI%20Architectures&body=Title%3A%20Insights%20into%20DeepSeek-V3%3A%20Scaling%20Challenges%20and%20Reflections%20on%0A%20%20Hardware%20for%20AI%20Architectures%0AAuthor%3A%20Chenggang%20Zhao%20and%20Chengqi%20Deng%20and%20Chong%20Ruan%20and%20Damai%20Dai%20and%20Huazuo%20Gao%20and%20Jiashi%20Li%20and%20Liyue%20Zhang%20and%20Panpan%20Huang%20and%20Shangyan%20Zhou%20and%20Shirong%20Ma%20and%20Wenfeng%20Liang%20and%20Ying%20He%20and%20Yuqing%20Wang%20and%20Yuxuan%20Liu%20and%20Y.%20X.%20Wei%0AAbstract%3A%20%20%20The%20rapid%20scaling%20of%20large%20language%20models%20%28LLMs%29%20has%20unveiled%20critical%0Alimitations%20in%20current%20hardware%20architectures%2C%20including%20constraints%20in%20memory%0Acapacity%2C%20computational%20efficiency%2C%20and%20interconnection%20bandwidth.%20DeepSeek-V3%2C%0Atrained%20on%202%2C048%20NVIDIA%20H800%20GPUs%2C%20demonstrates%20how%20hardware-aware%20model%0Aco-design%20can%20effectively%20address%20these%20challenges%2C%20enabling%20cost-efficient%0Atraining%20and%20inference%20at%20scale.%20This%20paper%20presents%20an%20in-depth%20analysis%20of%0Athe%20DeepSeek-V3/R1%20model%20architecture%20and%20its%20AI%20infrastructure%2C%20highlighting%0Akey%20innovations%20such%20as%20Multi-head%20Latent%20Attention%20%28MLA%29%20for%20enhanced%20memory%0Aefficiency%2C%20Mixture%20of%20Experts%20%28MoE%29%20architectures%20for%20optimized%0Acomputation-communication%20trade-offs%2C%20FP8%20mixed-precision%20training%20to%20unlock%0Athe%20full%20potential%20of%20hardware%20capabilities%2C%20and%20a%20Multi-Plane%20Network%20Topology%0Ato%20minimize%20cluster-level%20network%20overhead.%20Building%20on%20the%20hardware%0Abottlenecks%20encountered%20during%20DeepSeek-V3%27s%20development%2C%20we%20engage%20in%20a%0Abroader%20discussion%20with%20academic%20and%20industry%20peers%20on%20potential%20future%0Ahardware%20directions%2C%20including%20precise%20low-precision%20computation%20units%2C%0Ascale-up%20and%20scale-out%20convergence%2C%20and%20innovations%20in%20low-latency%0Acommunication%20fabrics.%20These%20insights%20underscore%20the%20critical%20role%20of%20hardware%0Aand%20model%20co-design%20in%20meeting%20the%20escalating%20demands%20of%20AI%20workloads%2C%20offering%0Aa%20practical%20blueprint%20for%20innovation%20in%20next-generation%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsights%2520into%2520DeepSeek-V3%253A%2520Scaling%2520Challenges%2520and%2520Reflections%2520on%250A%2520%2520Hardware%2520for%2520AI%2520Architectures%26entry.906535625%3DChenggang%2520Zhao%2520and%2520Chengqi%2520Deng%2520and%2520Chong%2520Ruan%2520and%2520Damai%2520Dai%2520and%2520Huazuo%2520Gao%2520and%2520Jiashi%2520Li%2520and%2520Liyue%2520Zhang%2520and%2520Panpan%2520Huang%2520and%2520Shangyan%2520Zhou%2520and%2520Shirong%2520Ma%2520and%2520Wenfeng%2520Liang%2520and%2520Ying%2520He%2520and%2520Yuqing%2520Wang%2520and%2520Yuxuan%2520Liu%2520and%2520Y.%2520X.%2520Wei%26entry.1292438233%3D%2520%2520The%2520rapid%2520scaling%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520unveiled%2520critical%250Alimitations%2520in%2520current%2520hardware%2520architectures%252C%2520including%2520constraints%2520in%2520memory%250Acapacity%252C%2520computational%2520efficiency%252C%2520and%2520interconnection%2520bandwidth.%2520DeepSeek-V3%252C%250Atrained%2520on%25202%252C048%2520NVIDIA%2520H800%2520GPUs%252C%2520demonstrates%2520how%2520hardware-aware%2520model%250Aco-design%2520can%2520effectively%2520address%2520these%2520challenges%252C%2520enabling%2520cost-efficient%250Atraining%2520and%2520inference%2520at%2520scale.%2520This%2520paper%2520presents%2520an%2520in-depth%2520analysis%2520of%250Athe%2520DeepSeek-V3/R1%2520model%2520architecture%2520and%2520its%2520AI%2520infrastructure%252C%2520highlighting%250Akey%2520innovations%2520such%2520as%2520Multi-head%2520Latent%2520Attention%2520%2528MLA%2529%2520for%2520enhanced%2520memory%250Aefficiency%252C%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520architectures%2520for%2520optimized%250Acomputation-communication%2520trade-offs%252C%2520FP8%2520mixed-precision%2520training%2520to%2520unlock%250Athe%2520full%2520potential%2520of%2520hardware%2520capabilities%252C%2520and%2520a%2520Multi-Plane%2520Network%2520Topology%250Ato%2520minimize%2520cluster-level%2520network%2520overhead.%2520Building%2520on%2520the%2520hardware%250Abottlenecks%2520encountered%2520during%2520DeepSeek-V3%2527s%2520development%252C%2520we%2520engage%2520in%2520a%250Abroader%2520discussion%2520with%2520academic%2520and%2520industry%2520peers%2520on%2520potential%2520future%250Ahardware%2520directions%252C%2520including%2520precise%2520low-precision%2520computation%2520units%252C%250Ascale-up%2520and%2520scale-out%2520convergence%252C%2520and%2520innovations%2520in%2520low-latency%250Acommunication%2520fabrics.%2520These%2520insights%2520underscore%2520the%2520critical%2520role%2520of%2520hardware%250Aand%2520model%2520co-design%2520in%2520meeting%2520the%2520escalating%2520demands%2520of%2520AI%2520workloads%252C%2520offering%250Aa%2520practical%2520blueprint%2520for%2520innovation%2520in%2520next-generation%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insights%20into%20DeepSeek-V3%3A%20Scaling%20Challenges%20and%20Reflections%20on%0A%20%20Hardware%20for%20AI%20Architectures&entry.906535625=Chenggang%20Zhao%20and%20Chengqi%20Deng%20and%20Chong%20Ruan%20and%20Damai%20Dai%20and%20Huazuo%20Gao%20and%20Jiashi%20Li%20and%20Liyue%20Zhang%20and%20Panpan%20Huang%20and%20Shangyan%20Zhou%20and%20Shirong%20Ma%20and%20Wenfeng%20Liang%20and%20Ying%20He%20and%20Yuqing%20Wang%20and%20Yuxuan%20Liu%20and%20Y.%20X.%20Wei&entry.1292438233=%20%20The%20rapid%20scaling%20of%20large%20language%20models%20%28LLMs%29%20has%20unveiled%20critical%0Alimitations%20in%20current%20hardware%20architectures%2C%20including%20constraints%20in%20memory%0Acapacity%2C%20computational%20efficiency%2C%20and%20interconnection%20bandwidth.%20DeepSeek-V3%2C%0Atrained%20on%202%2C048%20NVIDIA%20H800%20GPUs%2C%20demonstrates%20how%20hardware-aware%20model%0Aco-design%20can%20effectively%20address%20these%20challenges%2C%20enabling%20cost-efficient%0Atraining%20and%20inference%20at%20scale.%20This%20paper%20presents%20an%20in-depth%20analysis%20of%0Athe%20DeepSeek-V3/R1%20model%20architecture%20and%20its%20AI%20infrastructure%2C%20highlighting%0Akey%20innovations%20such%20as%20Multi-head%20Latent%20Attention%20%28MLA%29%20for%20enhanced%20memory%0Aefficiency%2C%20Mixture%20of%20Experts%20%28MoE%29%20architectures%20for%20optimized%0Acomputation-communication%20trade-offs%2C%20FP8%20mixed-precision%20training%20to%20unlock%0Athe%20full%20potential%20of%20hardware%20capabilities%2C%20and%20a%20Multi-Plane%20Network%20Topology%0Ato%20minimize%20cluster-level%20network%20overhead.%20Building%20on%20the%20hardware%0Abottlenecks%20encountered%20during%20DeepSeek-V3%27s%20development%2C%20we%20engage%20in%20a%0Abroader%20discussion%20with%20academic%20and%20industry%20peers%20on%20potential%20future%0Ahardware%20directions%2C%20including%20precise%20low-precision%20computation%20units%2C%0Ascale-up%20and%20scale-out%20convergence%2C%20and%20innovations%20in%20low-latency%0Acommunication%20fabrics.%20These%20insights%20underscore%20the%20critical%20role%20of%20hardware%0Aand%20model%20co-design%20in%20meeting%20the%20escalating%20demands%20of%20AI%20workloads%2C%20offering%0Aa%20practical%20blueprint%20for%20innovation%20in%20next-generation%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09343v1&entry.124074799=Read"},
{"title": "Hakim: Farsi Text Embedding Model", "author": "Mehran Sarmadi and Morteza Alikhani and Erfan Zinvandi and Zahra Pourbahman", "abstract": "  Recent advancements in text embedding have significantly improved natural\nlanguage understanding across many languages, yet Persian remains notably\nunderrepresented in large-scale embedding research. In this paper, we present\nHakim, a novel state-of-the-art Persian text embedding model that achieves a\n8.5% performance improvement over existing approaches on the FaMTEB benchmark,\noutperforming all previously developed Persian language models. As part of this\nwork, we introduce three new datasets - Corpesia, Pairsia-sup, and\nPairsia-unsup - to support supervised and unsupervised training scenarios.\nAdditionally, Hakim is designed for applications in chatbots and\nretrieval-augmented generation (RAG) systems, particularly addressing retrieval\ntasks that require incorporating message history within these systems. We also\npropose a new baseline model built on the BERT architecture. Our language model\nconsistently achieves higher accuracy across various Persian NLP tasks, while\nthe RetroMAE-based model proves particularly effective for textual information\nretrieval applications. Together, these contributions establish a new\nfoundation for advancing Persian language understanding.\n", "link": "http://arxiv.org/abs/2505.08435v2", "date": "2025-05-14", "relevancy": 2.1689, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4271}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hakim%3A%20Farsi%20Text%20Embedding%20Model&body=Title%3A%20Hakim%3A%20Farsi%20Text%20Embedding%20Model%0AAuthor%3A%20Mehran%20Sarmadi%20and%20Morteza%20Alikhani%20and%20Erfan%20Zinvandi%20and%20Zahra%20Pourbahman%0AAbstract%3A%20%20%20Recent%20advancements%20in%20text%20embedding%20have%20significantly%20improved%20natural%0Alanguage%20understanding%20across%20many%20languages%2C%20yet%20Persian%20remains%20notably%0Aunderrepresented%20in%20large-scale%20embedding%20research.%20In%20this%20paper%2C%20we%20present%0AHakim%2C%20a%20novel%20state-of-the-art%20Persian%20text%20embedding%20model%20that%20achieves%20a%0A8.5%25%20performance%20improvement%20over%20existing%20approaches%20on%20the%20FaMTEB%20benchmark%2C%0Aoutperforming%20all%20previously%20developed%20Persian%20language%20models.%20As%20part%20of%20this%0Awork%2C%20we%20introduce%20three%20new%20datasets%20-%20Corpesia%2C%20Pairsia-sup%2C%20and%0APairsia-unsup%20-%20to%20support%20supervised%20and%20unsupervised%20training%20scenarios.%0AAdditionally%2C%20Hakim%20is%20designed%20for%20applications%20in%20chatbots%20and%0Aretrieval-augmented%20generation%20%28RAG%29%20systems%2C%20particularly%20addressing%20retrieval%0Atasks%20that%20require%20incorporating%20message%20history%20within%20these%20systems.%20We%20also%0Apropose%20a%20new%20baseline%20model%20built%20on%20the%20BERT%20architecture.%20Our%20language%20model%0Aconsistently%20achieves%20higher%20accuracy%20across%20various%20Persian%20NLP%20tasks%2C%20while%0Athe%20RetroMAE-based%20model%20proves%20particularly%20effective%20for%20textual%20information%0Aretrieval%20applications.%20Together%2C%20these%20contributions%20establish%20a%20new%0Afoundation%20for%20advancing%20Persian%20language%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHakim%253A%2520Farsi%2520Text%2520Embedding%2520Model%26entry.906535625%3DMehran%2520Sarmadi%2520and%2520Morteza%2520Alikhani%2520and%2520Erfan%2520Zinvandi%2520and%2520Zahra%2520Pourbahman%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520text%2520embedding%2520have%2520significantly%2520improved%2520natural%250Alanguage%2520understanding%2520across%2520many%2520languages%252C%2520yet%2520Persian%2520remains%2520notably%250Aunderrepresented%2520in%2520large-scale%2520embedding%2520research.%2520In%2520this%2520paper%252C%2520we%2520present%250AHakim%252C%2520a%2520novel%2520state-of-the-art%2520Persian%2520text%2520embedding%2520model%2520that%2520achieves%2520a%250A8.5%2525%2520performance%2520improvement%2520over%2520existing%2520approaches%2520on%2520the%2520FaMTEB%2520benchmark%252C%250Aoutperforming%2520all%2520previously%2520developed%2520Persian%2520language%2520models.%2520As%2520part%2520of%2520this%250Awork%252C%2520we%2520introduce%2520three%2520new%2520datasets%2520-%2520Corpesia%252C%2520Pairsia-sup%252C%2520and%250APairsia-unsup%2520-%2520to%2520support%2520supervised%2520and%2520unsupervised%2520training%2520scenarios.%250AAdditionally%252C%2520Hakim%2520is%2520designed%2520for%2520applications%2520in%2520chatbots%2520and%250Aretrieval-augmented%2520generation%2520%2528RAG%2529%2520systems%252C%2520particularly%2520addressing%2520retrieval%250Atasks%2520that%2520require%2520incorporating%2520message%2520history%2520within%2520these%2520systems.%2520We%2520also%250Apropose%2520a%2520new%2520baseline%2520model%2520built%2520on%2520the%2520BERT%2520architecture.%2520Our%2520language%2520model%250Aconsistently%2520achieves%2520higher%2520accuracy%2520across%2520various%2520Persian%2520NLP%2520tasks%252C%2520while%250Athe%2520RetroMAE-based%2520model%2520proves%2520particularly%2520effective%2520for%2520textual%2520information%250Aretrieval%2520applications.%2520Together%252C%2520these%2520contributions%2520establish%2520a%2520new%250Afoundation%2520for%2520advancing%2520Persian%2520language%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hakim%3A%20Farsi%20Text%20Embedding%20Model&entry.906535625=Mehran%20Sarmadi%20and%20Morteza%20Alikhani%20and%20Erfan%20Zinvandi%20and%20Zahra%20Pourbahman&entry.1292438233=%20%20Recent%20advancements%20in%20text%20embedding%20have%20significantly%20improved%20natural%0Alanguage%20understanding%20across%20many%20languages%2C%20yet%20Persian%20remains%20notably%0Aunderrepresented%20in%20large-scale%20embedding%20research.%20In%20this%20paper%2C%20we%20present%0AHakim%2C%20a%20novel%20state-of-the-art%20Persian%20text%20embedding%20model%20that%20achieves%20a%0A8.5%25%20performance%20improvement%20over%20existing%20approaches%20on%20the%20FaMTEB%20benchmark%2C%0Aoutperforming%20all%20previously%20developed%20Persian%20language%20models.%20As%20part%20of%20this%0Awork%2C%20we%20introduce%20three%20new%20datasets%20-%20Corpesia%2C%20Pairsia-sup%2C%20and%0APairsia-unsup%20-%20to%20support%20supervised%20and%20unsupervised%20training%20scenarios.%0AAdditionally%2C%20Hakim%20is%20designed%20for%20applications%20in%20chatbots%20and%0Aretrieval-augmented%20generation%20%28RAG%29%20systems%2C%20particularly%20addressing%20retrieval%0Atasks%20that%20require%20incorporating%20message%20history%20within%20these%20systems.%20We%20also%0Apropose%20a%20new%20baseline%20model%20built%20on%20the%20BERT%20architecture.%20Our%20language%20model%0Aconsistently%20achieves%20higher%20accuracy%20across%20various%20Persian%20NLP%20tasks%2C%20while%0Athe%20RetroMAE-based%20model%20proves%20particularly%20effective%20for%20textual%20information%0Aretrieval%20applications.%20Together%2C%20these%20contributions%20establish%20a%20new%0Afoundation%20for%20advancing%20Persian%20language%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08435v2&entry.124074799=Read"},
{"title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning", "author": "Bin-Bin Gao", "abstract": "  Zero- and few-shot visual anomaly segmentation relies on powerful\nvision-language models that detect unseen anomalies using manually designed\ntextual prompts. However, visual representations are inherently independent of\nlanguage. In this paper, we explore the potential of a pure visual foundation\nmodel as an alternative to widely used vision-language models for universal\nvisual anomaly segmentation. We present a novel paradigm that unifies anomaly\nsegmentation into change segmentation. This paradigm enables us to leverage\nlarge-scale synthetic image pairs, featuring object-level and local region\nchanges, derived from existing image datasets, which are independent of target\nanomaly datasets. We propose a one-prompt Meta-learning framework for Universal\nAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and\nthen generalizes well to segment any novel or unseen visual anomalies in the\nreal world. To handle geometrical variations between prompt and query images,\nwe propose a soft feature alignment module that bridges paired-image change\nperception and single-image semantic segmentation. This is the first work to\nachieve universal anomaly segmentation using a pure vision model without\nrelying on special anomaly detection datasets and pre-trained visual-language\nmodels. Our method effectively and efficiently segments any anomalies with only\none normal image prompt and enjoys training-free without guidance from\nlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,\nand even full-shot anomaly segmentation methods. The code and pre-trained\nmodels are available at https://github.com/gaobb/MetaUAS.\n", "link": "http://arxiv.org/abs/2505.09265v1", "date": "2025-05-14", "relevancy": 2.1657, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.574}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5378}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaUAS%3A%20Universal%20Anomaly%20Segmentation%20with%20One-Prompt%20Meta-Learning&body=Title%3A%20MetaUAS%3A%20Universal%20Anomaly%20Segmentation%20with%20One-Prompt%20Meta-Learning%0AAuthor%3A%20Bin-Bin%20Gao%0AAbstract%3A%20%20%20Zero-%20and%20few-shot%20visual%20anomaly%20segmentation%20relies%20on%20powerful%0Avision-language%20models%20that%20detect%20unseen%20anomalies%20using%20manually%20designed%0Atextual%20prompts.%20However%2C%20visual%20representations%20are%20inherently%20independent%20of%0Alanguage.%20In%20this%20paper%2C%20we%20explore%20the%20potential%20of%20a%20pure%20visual%20foundation%0Amodel%20as%20an%20alternative%20to%20widely%20used%20vision-language%20models%20for%20universal%0Avisual%20anomaly%20segmentation.%20We%20present%20a%20novel%20paradigm%20that%20unifies%20anomaly%0Asegmentation%20into%20change%20segmentation.%20This%20paradigm%20enables%20us%20to%20leverage%0Alarge-scale%20synthetic%20image%20pairs%2C%20featuring%20object-level%20and%20local%20region%0Achanges%2C%20derived%20from%20existing%20image%20datasets%2C%20which%20are%20independent%20of%20target%0Aanomaly%20datasets.%20We%20propose%20a%20one-prompt%20Meta-learning%20framework%20for%20Universal%0AAnomaly%20Segmentation%20%28MetaUAS%29%20that%20is%20trained%20on%20this%20synthetic%20dataset%20and%0Athen%20generalizes%20well%20to%20segment%20any%20novel%20or%20unseen%20visual%20anomalies%20in%20the%0Areal%20world.%20To%20handle%20geometrical%20variations%20between%20prompt%20and%20query%20images%2C%0Awe%20propose%20a%20soft%20feature%20alignment%20module%20that%20bridges%20paired-image%20change%0Aperception%20and%20single-image%20semantic%20segmentation.%20This%20is%20the%20first%20work%20to%0Aachieve%20universal%20anomaly%20segmentation%20using%20a%20pure%20vision%20model%20without%0Arelying%20on%20special%20anomaly%20detection%20datasets%20and%20pre-trained%20visual-language%0Amodels.%20Our%20method%20effectively%20and%20efficiently%20segments%20any%20anomalies%20with%20only%0Aone%20normal%20image%20prompt%20and%20enjoys%20training-free%20without%20guidance%20from%0Alanguage.%20Our%20MetaUAS%20significantly%20outperforms%20previous%20zero-shot%2C%20few-shot%2C%0Aand%20even%20full-shot%20anomaly%20segmentation%20methods.%20The%20code%20and%20pre-trained%0Amodels%20are%20available%20at%20https%3A//github.com/gaobb/MetaUAS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaUAS%253A%2520Universal%2520Anomaly%2520Segmentation%2520with%2520One-Prompt%2520Meta-Learning%26entry.906535625%3DBin-Bin%2520Gao%26entry.1292438233%3D%2520%2520Zero-%2520and%2520few-shot%2520visual%2520anomaly%2520segmentation%2520relies%2520on%2520powerful%250Avision-language%2520models%2520that%2520detect%2520unseen%2520anomalies%2520using%2520manually%2520designed%250Atextual%2520prompts.%2520However%252C%2520visual%2520representations%2520are%2520inherently%2520independent%2520of%250Alanguage.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520potential%2520of%2520a%2520pure%2520visual%2520foundation%250Amodel%2520as%2520an%2520alternative%2520to%2520widely%2520used%2520vision-language%2520models%2520for%2520universal%250Avisual%2520anomaly%2520segmentation.%2520We%2520present%2520a%2520novel%2520paradigm%2520that%2520unifies%2520anomaly%250Asegmentation%2520into%2520change%2520segmentation.%2520This%2520paradigm%2520enables%2520us%2520to%2520leverage%250Alarge-scale%2520synthetic%2520image%2520pairs%252C%2520featuring%2520object-level%2520and%2520local%2520region%250Achanges%252C%2520derived%2520from%2520existing%2520image%2520datasets%252C%2520which%2520are%2520independent%2520of%2520target%250Aanomaly%2520datasets.%2520We%2520propose%2520a%2520one-prompt%2520Meta-learning%2520framework%2520for%2520Universal%250AAnomaly%2520Segmentation%2520%2528MetaUAS%2529%2520that%2520is%2520trained%2520on%2520this%2520synthetic%2520dataset%2520and%250Athen%2520generalizes%2520well%2520to%2520segment%2520any%2520novel%2520or%2520unseen%2520visual%2520anomalies%2520in%2520the%250Areal%2520world.%2520To%2520handle%2520geometrical%2520variations%2520between%2520prompt%2520and%2520query%2520images%252C%250Awe%2520propose%2520a%2520soft%2520feature%2520alignment%2520module%2520that%2520bridges%2520paired-image%2520change%250Aperception%2520and%2520single-image%2520semantic%2520segmentation.%2520This%2520is%2520the%2520first%2520work%2520to%250Aachieve%2520universal%2520anomaly%2520segmentation%2520using%2520a%2520pure%2520vision%2520model%2520without%250Arelying%2520on%2520special%2520anomaly%2520detection%2520datasets%2520and%2520pre-trained%2520visual-language%250Amodels.%2520Our%2520method%2520effectively%2520and%2520efficiently%2520segments%2520any%2520anomalies%2520with%2520only%250Aone%2520normal%2520image%2520prompt%2520and%2520enjoys%2520training-free%2520without%2520guidance%2520from%250Alanguage.%2520Our%2520MetaUAS%2520significantly%2520outperforms%2520previous%2520zero-shot%252C%2520few-shot%252C%250Aand%2520even%2520full-shot%2520anomaly%2520segmentation%2520methods.%2520The%2520code%2520and%2520pre-trained%250Amodels%2520are%2520available%2520at%2520https%253A//github.com/gaobb/MetaUAS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaUAS%3A%20Universal%20Anomaly%20Segmentation%20with%20One-Prompt%20Meta-Learning&entry.906535625=Bin-Bin%20Gao&entry.1292438233=%20%20Zero-%20and%20few-shot%20visual%20anomaly%20segmentation%20relies%20on%20powerful%0Avision-language%20models%20that%20detect%20unseen%20anomalies%20using%20manually%20designed%0Atextual%20prompts.%20However%2C%20visual%20representations%20are%20inherently%20independent%20of%0Alanguage.%20In%20this%20paper%2C%20we%20explore%20the%20potential%20of%20a%20pure%20visual%20foundation%0Amodel%20as%20an%20alternative%20to%20widely%20used%20vision-language%20models%20for%20universal%0Avisual%20anomaly%20segmentation.%20We%20present%20a%20novel%20paradigm%20that%20unifies%20anomaly%0Asegmentation%20into%20change%20segmentation.%20This%20paradigm%20enables%20us%20to%20leverage%0Alarge-scale%20synthetic%20image%20pairs%2C%20featuring%20object-level%20and%20local%20region%0Achanges%2C%20derived%20from%20existing%20image%20datasets%2C%20which%20are%20independent%20of%20target%0Aanomaly%20datasets.%20We%20propose%20a%20one-prompt%20Meta-learning%20framework%20for%20Universal%0AAnomaly%20Segmentation%20%28MetaUAS%29%20that%20is%20trained%20on%20this%20synthetic%20dataset%20and%0Athen%20generalizes%20well%20to%20segment%20any%20novel%20or%20unseen%20visual%20anomalies%20in%20the%0Areal%20world.%20To%20handle%20geometrical%20variations%20between%20prompt%20and%20query%20images%2C%0Awe%20propose%20a%20soft%20feature%20alignment%20module%20that%20bridges%20paired-image%20change%0Aperception%20and%20single-image%20semantic%20segmentation.%20This%20is%20the%20first%20work%20to%0Aachieve%20universal%20anomaly%20segmentation%20using%20a%20pure%20vision%20model%20without%0Arelying%20on%20special%20anomaly%20detection%20datasets%20and%20pre-trained%20visual-language%0Amodels.%20Our%20method%20effectively%20and%20efficiently%20segments%20any%20anomalies%20with%20only%0Aone%20normal%20image%20prompt%20and%20enjoys%20training-free%20without%20guidance%20from%0Alanguage.%20Our%20MetaUAS%20significantly%20outperforms%20previous%20zero-shot%2C%20few-shot%2C%0Aand%20even%20full-shot%20anomaly%20segmentation%20methods.%20The%20code%20and%20pre-trained%0Amodels%20are%20available%20at%20https%3A//github.com/gaobb/MetaUAS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09265v1&entry.124074799=Read"},
{"title": "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents", "author": "Jian Liu and Xiongtao Shi and Thai Duy Nguyen and Haitian Zhang and Tianxiang Zhang and Wei Sun and Yanjie Li and Athanasios V. Vasilakos and Giovanni Iacca and Arshad Ali Khan and Arvind Kumar and Jae Won Cho and Ajmal Mian and Lihua Xie and Erik Cambria and Lin Wang", "abstract": "  The rapid evolution of artificial intelligence (AI) has shifted from static,\ndata-driven models to dynamic systems capable of perceiving and interacting\nwith real-world environments. Despite advancements in pattern recognition and\nsymbolic reasoning, current AI systems, such as large language models, remain\ndisembodied, unable to physically engage with the world. This limitation has\ndriven the rise of embodied AI, where autonomous agents, such as humanoid\nrobots, must navigate and manipulate unstructured environments with human-like\nadaptability. At the core of this challenge lies the concept of Neural Brain, a\ncentral intelligence system designed to drive embodied agents with human-like\nadaptability. A Neural Brain must seamlessly integrate multimodal sensing and\nperception with cognitive capabilities. Achieving this also requires an\nadaptive memory system and energy-efficient hardware-software co-design,\nenabling real-time action in dynamic environments. This paper introduces a\nunified framework for the Neural Brain of embodied agents, addressing two\nfundamental challenges: (1) defining the core components of Neural Brain and\n(2) bridging the gap between static AI models and the dynamic adaptability\nrequired for real-world deployment. To this end, we propose a biologically\ninspired architecture that integrates multimodal active sensing,\nperception-cognition-action function, neuroplasticity-based memory storage and\nupdating, and neuromorphic hardware/software optimization. Furthermore, we also\nreview the latest research on embodied agents across these four aspects and\nanalyze the gap between current AI systems and human intelligence. By\nsynthesizing insights from neuroscience, we outline a roadmap towards the\ndevelopment of generalizable, autonomous agents capable of human-level\nintelligence in real-world scenarios.\n", "link": "http://arxiv.org/abs/2505.07634v2", "date": "2025-05-14", "relevancy": 2.1606, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5802}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.536}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Brain%3A%20A%20Neuroscience-inspired%20Framework%20for%20Embodied%20Agents&body=Title%3A%20Neural%20Brain%3A%20A%20Neuroscience-inspired%20Framework%20for%20Embodied%20Agents%0AAuthor%3A%20Jian%20Liu%20and%20Xiongtao%20Shi%20and%20Thai%20Duy%20Nguyen%20and%20Haitian%20Zhang%20and%20Tianxiang%20Zhang%20and%20Wei%20Sun%20and%20Yanjie%20Li%20and%20Athanasios%20V.%20Vasilakos%20and%20Giovanni%20Iacca%20and%20Arshad%20Ali%20Khan%20and%20Arvind%20Kumar%20and%20Jae%20Won%20Cho%20and%20Ajmal%20Mian%20and%20Lihua%20Xie%20and%20Erik%20Cambria%20and%20Lin%20Wang%0AAbstract%3A%20%20%20The%20rapid%20evolution%20of%20artificial%20intelligence%20%28AI%29%20has%20shifted%20from%20static%2C%0Adata-driven%20models%20to%20dynamic%20systems%20capable%20of%20perceiving%20and%20interacting%0Awith%20real-world%20environments.%20Despite%20advancements%20in%20pattern%20recognition%20and%0Asymbolic%20reasoning%2C%20current%20AI%20systems%2C%20such%20as%20large%20language%20models%2C%20remain%0Adisembodied%2C%20unable%20to%20physically%20engage%20with%20the%20world.%20This%20limitation%20has%0Adriven%20the%20rise%20of%20embodied%20AI%2C%20where%20autonomous%20agents%2C%20such%20as%20humanoid%0Arobots%2C%20must%20navigate%20and%20manipulate%20unstructured%20environments%20with%20human-like%0Aadaptability.%20At%20the%20core%20of%20this%20challenge%20lies%20the%20concept%20of%20Neural%20Brain%2C%20a%0Acentral%20intelligence%20system%20designed%20to%20drive%20embodied%20agents%20with%20human-like%0Aadaptability.%20A%20Neural%20Brain%20must%20seamlessly%20integrate%20multimodal%20sensing%20and%0Aperception%20with%20cognitive%20capabilities.%20Achieving%20this%20also%20requires%20an%0Aadaptive%20memory%20system%20and%20energy-efficient%20hardware-software%20co-design%2C%0Aenabling%20real-time%20action%20in%20dynamic%20environments.%20This%20paper%20introduces%20a%0Aunified%20framework%20for%20the%20Neural%20Brain%20of%20embodied%20agents%2C%20addressing%20two%0Afundamental%20challenges%3A%20%281%29%20defining%20the%20core%20components%20of%20Neural%20Brain%20and%0A%282%29%20bridging%20the%20gap%20between%20static%20AI%20models%20and%20the%20dynamic%20adaptability%0Arequired%20for%20real-world%20deployment.%20To%20this%20end%2C%20we%20propose%20a%20biologically%0Ainspired%20architecture%20that%20integrates%20multimodal%20active%20sensing%2C%0Aperception-cognition-action%20function%2C%20neuroplasticity-based%20memory%20storage%20and%0Aupdating%2C%20and%20neuromorphic%20hardware/software%20optimization.%20Furthermore%2C%20we%20also%0Areview%20the%20latest%20research%20on%20embodied%20agents%20across%20these%20four%20aspects%20and%0Aanalyze%20the%20gap%20between%20current%20AI%20systems%20and%20human%20intelligence.%20By%0Asynthesizing%20insights%20from%20neuroscience%2C%20we%20outline%20a%20roadmap%20towards%20the%0Adevelopment%20of%20generalizable%2C%20autonomous%20agents%20capable%20of%20human-level%0Aintelligence%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Brain%253A%2520A%2520Neuroscience-inspired%2520Framework%2520for%2520Embodied%2520Agents%26entry.906535625%3DJian%2520Liu%2520and%2520Xiongtao%2520Shi%2520and%2520Thai%2520Duy%2520Nguyen%2520and%2520Haitian%2520Zhang%2520and%2520Tianxiang%2520Zhang%2520and%2520Wei%2520Sun%2520and%2520Yanjie%2520Li%2520and%2520Athanasios%2520V.%2520Vasilakos%2520and%2520Giovanni%2520Iacca%2520and%2520Arshad%2520Ali%2520Khan%2520and%2520Arvind%2520Kumar%2520and%2520Jae%2520Won%2520Cho%2520and%2520Ajmal%2520Mian%2520and%2520Lihua%2520Xie%2520and%2520Erik%2520Cambria%2520and%2520Lin%2520Wang%26entry.1292438233%3D%2520%2520The%2520rapid%2520evolution%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520has%2520shifted%2520from%2520static%252C%250Adata-driven%2520models%2520to%2520dynamic%2520systems%2520capable%2520of%2520perceiving%2520and%2520interacting%250Awith%2520real-world%2520environments.%2520Despite%2520advancements%2520in%2520pattern%2520recognition%2520and%250Asymbolic%2520reasoning%252C%2520current%2520AI%2520systems%252C%2520such%2520as%2520large%2520language%2520models%252C%2520remain%250Adisembodied%252C%2520unable%2520to%2520physically%2520engage%2520with%2520the%2520world.%2520This%2520limitation%2520has%250Adriven%2520the%2520rise%2520of%2520embodied%2520AI%252C%2520where%2520autonomous%2520agents%252C%2520such%2520as%2520humanoid%250Arobots%252C%2520must%2520navigate%2520and%2520manipulate%2520unstructured%2520environments%2520with%2520human-like%250Aadaptability.%2520At%2520the%2520core%2520of%2520this%2520challenge%2520lies%2520the%2520concept%2520of%2520Neural%2520Brain%252C%2520a%250Acentral%2520intelligence%2520system%2520designed%2520to%2520drive%2520embodied%2520agents%2520with%2520human-like%250Aadaptability.%2520A%2520Neural%2520Brain%2520must%2520seamlessly%2520integrate%2520multimodal%2520sensing%2520and%250Aperception%2520with%2520cognitive%2520capabilities.%2520Achieving%2520this%2520also%2520requires%2520an%250Aadaptive%2520memory%2520system%2520and%2520energy-efficient%2520hardware-software%2520co-design%252C%250Aenabling%2520real-time%2520action%2520in%2520dynamic%2520environments.%2520This%2520paper%2520introduces%2520a%250Aunified%2520framework%2520for%2520the%2520Neural%2520Brain%2520of%2520embodied%2520agents%252C%2520addressing%2520two%250Afundamental%2520challenges%253A%2520%25281%2529%2520defining%2520the%2520core%2520components%2520of%2520Neural%2520Brain%2520and%250A%25282%2529%2520bridging%2520the%2520gap%2520between%2520static%2520AI%2520models%2520and%2520the%2520dynamic%2520adaptability%250Arequired%2520for%2520real-world%2520deployment.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520biologically%250Ainspired%2520architecture%2520that%2520integrates%2520multimodal%2520active%2520sensing%252C%250Aperception-cognition-action%2520function%252C%2520neuroplasticity-based%2520memory%2520storage%2520and%250Aupdating%252C%2520and%2520neuromorphic%2520hardware/software%2520optimization.%2520Furthermore%252C%2520we%2520also%250Areview%2520the%2520latest%2520research%2520on%2520embodied%2520agents%2520across%2520these%2520four%2520aspects%2520and%250Aanalyze%2520the%2520gap%2520between%2520current%2520AI%2520systems%2520and%2520human%2520intelligence.%2520By%250Asynthesizing%2520insights%2520from%2520neuroscience%252C%2520we%2520outline%2520a%2520roadmap%2520towards%2520the%250Adevelopment%2520of%2520generalizable%252C%2520autonomous%2520agents%2520capable%2520of%2520human-level%250Aintelligence%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Brain%3A%20A%20Neuroscience-inspired%20Framework%20for%20Embodied%20Agents&entry.906535625=Jian%20Liu%20and%20Xiongtao%20Shi%20and%20Thai%20Duy%20Nguyen%20and%20Haitian%20Zhang%20and%20Tianxiang%20Zhang%20and%20Wei%20Sun%20and%20Yanjie%20Li%20and%20Athanasios%20V.%20Vasilakos%20and%20Giovanni%20Iacca%20and%20Arshad%20Ali%20Khan%20and%20Arvind%20Kumar%20and%20Jae%20Won%20Cho%20and%20Ajmal%20Mian%20and%20Lihua%20Xie%20and%20Erik%20Cambria%20and%20Lin%20Wang&entry.1292438233=%20%20The%20rapid%20evolution%20of%20artificial%20intelligence%20%28AI%29%20has%20shifted%20from%20static%2C%0Adata-driven%20models%20to%20dynamic%20systems%20capable%20of%20perceiving%20and%20interacting%0Awith%20real-world%20environments.%20Despite%20advancements%20in%20pattern%20recognition%20and%0Asymbolic%20reasoning%2C%20current%20AI%20systems%2C%20such%20as%20large%20language%20models%2C%20remain%0Adisembodied%2C%20unable%20to%20physically%20engage%20with%20the%20world.%20This%20limitation%20has%0Adriven%20the%20rise%20of%20embodied%20AI%2C%20where%20autonomous%20agents%2C%20such%20as%20humanoid%0Arobots%2C%20must%20navigate%20and%20manipulate%20unstructured%20environments%20with%20human-like%0Aadaptability.%20At%20the%20core%20of%20this%20challenge%20lies%20the%20concept%20of%20Neural%20Brain%2C%20a%0Acentral%20intelligence%20system%20designed%20to%20drive%20embodied%20agents%20with%20human-like%0Aadaptability.%20A%20Neural%20Brain%20must%20seamlessly%20integrate%20multimodal%20sensing%20and%0Aperception%20with%20cognitive%20capabilities.%20Achieving%20this%20also%20requires%20an%0Aadaptive%20memory%20system%20and%20energy-efficient%20hardware-software%20co-design%2C%0Aenabling%20real-time%20action%20in%20dynamic%20environments.%20This%20paper%20introduces%20a%0Aunified%20framework%20for%20the%20Neural%20Brain%20of%20embodied%20agents%2C%20addressing%20two%0Afundamental%20challenges%3A%20%281%29%20defining%20the%20core%20components%20of%20Neural%20Brain%20and%0A%282%29%20bridging%20the%20gap%20between%20static%20AI%20models%20and%20the%20dynamic%20adaptability%0Arequired%20for%20real-world%20deployment.%20To%20this%20end%2C%20we%20propose%20a%20biologically%0Ainspired%20architecture%20that%20integrates%20multimodal%20active%20sensing%2C%0Aperception-cognition-action%20function%2C%20neuroplasticity-based%20memory%20storage%20and%0Aupdating%2C%20and%20neuromorphic%20hardware/software%20optimization.%20Furthermore%2C%20we%20also%0Areview%20the%20latest%20research%20on%20embodied%20agents%20across%20these%20four%20aspects%20and%0Aanalyze%20the%20gap%20between%20current%20AI%20systems%20and%20human%20intelligence.%20By%0Asynthesizing%20insights%20from%20neuroscience%2C%20we%20outline%20a%20roadmap%20towards%20the%0Adevelopment%20of%20generalizable%2C%20autonomous%20agents%20capable%20of%20human-level%0Aintelligence%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07634v2&entry.124074799=Read"},
{"title": "Online Isolation Forest", "author": "Filippo Leveni and Guilherme Weigert Cassales and Bernhard Pfahringer and Albert Bifet and Giacomo Boracchi", "abstract": "  The anomaly detection literature is abundant with offline methods, which\nrequire repeated access to data in memory, and impose impractical assumptions\nwhen applied to a streaming context. Existing online anomaly detection methods\nalso generally fail to address these constraints, resorting to periodic\nretraining to adapt to the online context. We propose Online-iForest, a novel\nmethod explicitly designed for streaming conditions that seamlessly tracks the\ndata generating process as it evolves over time. Experimental validation on\nreal-world datasets demonstrated that Online-iForest is on par with online\nalternatives and closely rivals state-of-the-art offline anomaly detection\ntechniques that undergo periodic retraining. Notably, Online-iForest\nconsistently outperforms all competitors in terms of efficiency, making it a\npromising solution in applications where fast identification of anomalies is of\nprimary importance such as cybersecurity, fraud and fault detection.\n", "link": "http://arxiv.org/abs/2505.09593v1", "date": "2025-05-14", "relevancy": 2.159, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4437}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4325}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Isolation%20Forest&body=Title%3A%20Online%20Isolation%20Forest%0AAuthor%3A%20Filippo%20Leveni%20and%20Guilherme%20Weigert%20Cassales%20and%20Bernhard%20Pfahringer%20and%20Albert%20Bifet%20and%20Giacomo%20Boracchi%0AAbstract%3A%20%20%20The%20anomaly%20detection%20literature%20is%20abundant%20with%20offline%20methods%2C%20which%0Arequire%20repeated%20access%20to%20data%20in%20memory%2C%20and%20impose%20impractical%20assumptions%0Awhen%20applied%20to%20a%20streaming%20context.%20Existing%20online%20anomaly%20detection%20methods%0Aalso%20generally%20fail%20to%20address%20these%20constraints%2C%20resorting%20to%20periodic%0Aretraining%20to%20adapt%20to%20the%20online%20context.%20We%20propose%20Online-iForest%2C%20a%20novel%0Amethod%20explicitly%20designed%20for%20streaming%20conditions%20that%20seamlessly%20tracks%20the%0Adata%20generating%20process%20as%20it%20evolves%20over%20time.%20Experimental%20validation%20on%0Areal-world%20datasets%20demonstrated%20that%20Online-iForest%20is%20on%20par%20with%20online%0Aalternatives%20and%20closely%20rivals%20state-of-the-art%20offline%20anomaly%20detection%0Atechniques%20that%20undergo%20periodic%20retraining.%20Notably%2C%20Online-iForest%0Aconsistently%20outperforms%20all%20competitors%20in%20terms%20of%20efficiency%2C%20making%20it%20a%0Apromising%20solution%20in%20applications%20where%20fast%20identification%20of%20anomalies%20is%20of%0Aprimary%20importance%20such%20as%20cybersecurity%2C%20fraud%20and%20fault%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Isolation%2520Forest%26entry.906535625%3DFilippo%2520Leveni%2520and%2520Guilherme%2520Weigert%2520Cassales%2520and%2520Bernhard%2520Pfahringer%2520and%2520Albert%2520Bifet%2520and%2520Giacomo%2520Boracchi%26entry.1292438233%3D%2520%2520The%2520anomaly%2520detection%2520literature%2520is%2520abundant%2520with%2520offline%2520methods%252C%2520which%250Arequire%2520repeated%2520access%2520to%2520data%2520in%2520memory%252C%2520and%2520impose%2520impractical%2520assumptions%250Awhen%2520applied%2520to%2520a%2520streaming%2520context.%2520Existing%2520online%2520anomaly%2520detection%2520methods%250Aalso%2520generally%2520fail%2520to%2520address%2520these%2520constraints%252C%2520resorting%2520to%2520periodic%250Aretraining%2520to%2520adapt%2520to%2520the%2520online%2520context.%2520We%2520propose%2520Online-iForest%252C%2520a%2520novel%250Amethod%2520explicitly%2520designed%2520for%2520streaming%2520conditions%2520that%2520seamlessly%2520tracks%2520the%250Adata%2520generating%2520process%2520as%2520it%2520evolves%2520over%2520time.%2520Experimental%2520validation%2520on%250Areal-world%2520datasets%2520demonstrated%2520that%2520Online-iForest%2520is%2520on%2520par%2520with%2520online%250Aalternatives%2520and%2520closely%2520rivals%2520state-of-the-art%2520offline%2520anomaly%2520detection%250Atechniques%2520that%2520undergo%2520periodic%2520retraining.%2520Notably%252C%2520Online-iForest%250Aconsistently%2520outperforms%2520all%2520competitors%2520in%2520terms%2520of%2520efficiency%252C%2520making%2520it%2520a%250Apromising%2520solution%2520in%2520applications%2520where%2520fast%2520identification%2520of%2520anomalies%2520is%2520of%250Aprimary%2520importance%2520such%2520as%2520cybersecurity%252C%2520fraud%2520and%2520fault%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Isolation%20Forest&entry.906535625=Filippo%20Leveni%20and%20Guilherme%20Weigert%20Cassales%20and%20Bernhard%20Pfahringer%20and%20Albert%20Bifet%20and%20Giacomo%20Boracchi&entry.1292438233=%20%20The%20anomaly%20detection%20literature%20is%20abundant%20with%20offline%20methods%2C%20which%0Arequire%20repeated%20access%20to%20data%20in%20memory%2C%20and%20impose%20impractical%20assumptions%0Awhen%20applied%20to%20a%20streaming%20context.%20Existing%20online%20anomaly%20detection%20methods%0Aalso%20generally%20fail%20to%20address%20these%20constraints%2C%20resorting%20to%20periodic%0Aretraining%20to%20adapt%20to%20the%20online%20context.%20We%20propose%20Online-iForest%2C%20a%20novel%0Amethod%20explicitly%20designed%20for%20streaming%20conditions%20that%20seamlessly%20tracks%20the%0Adata%20generating%20process%20as%20it%20evolves%20over%20time.%20Experimental%20validation%20on%0Areal-world%20datasets%20demonstrated%20that%20Online-iForest%20is%20on%20par%20with%20online%0Aalternatives%20and%20closely%20rivals%20state-of-the-art%20offline%20anomaly%20detection%0Atechniques%20that%20undergo%20periodic%20retraining.%20Notably%2C%20Online-iForest%0Aconsistently%20outperforms%20all%20competitors%20in%20terms%20of%20efficiency%2C%20making%20it%20a%0Apromising%20solution%20in%20applications%20where%20fast%20identification%20of%20anomalies%20is%20of%0Aprimary%20importance%20such%20as%20cybersecurity%2C%20fraud%20and%20fault%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09593v1&entry.124074799=Read"},
{"title": "Guaranteed Rejection-free Sampling Method Using Past Behaviours for\n  Motion Planning of Autonomous Systems", "author": "Thomas T. Enevoldsen and Roberto Galeazzi", "abstract": "  The paper presents a novel learning-based sampling strategy that guarantees\nrejection-free sampling of the free space under both biased and approximately\nuniform conditions, leveraging multivariate kernel densities. Historical data\nfrom a given autonomous system is leveraged to estimate a non-parametric\nprobabilistic description of the domain, which also describes the free space\nwhere feasible solutions of the motion planning problem are likely to be found.\nThe tuning parameters of the kernel density estimator, the bandwidth and the\nkernel, are used to alter the description of the free space so that no samples\ncan fall outside the originally defined space.The proposed method is\ndemonstrated in two real-life case studies: An autonomous surface vessel (2D)\nand an autonomous drone (3D). Two planning problems are solved, showing that\nthe proposed approximately uniform sampling scheme is capable of guaranteeing\nrejection-free samples of the considered workspace. Furthermore, the\neffectiveness of the proposed method is statistically validated using Monte\nCarlo simulations.\n", "link": "http://arxiv.org/abs/2109.14687v3", "date": "2025-05-14", "relevancy": 2.1321, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5728}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5461}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guaranteed%20Rejection-free%20Sampling%20Method%20Using%20Past%20Behaviours%20for%0A%20%20Motion%20Planning%20of%20Autonomous%20Systems&body=Title%3A%20Guaranteed%20Rejection-free%20Sampling%20Method%20Using%20Past%20Behaviours%20for%0A%20%20Motion%20Planning%20of%20Autonomous%20Systems%0AAuthor%3A%20Thomas%20T.%20Enevoldsen%20and%20Roberto%20Galeazzi%0AAbstract%3A%20%20%20The%20paper%20presents%20a%20novel%20learning-based%20sampling%20strategy%20that%20guarantees%0Arejection-free%20sampling%20of%20the%20free%20space%20under%20both%20biased%20and%20approximately%0Auniform%20conditions%2C%20leveraging%20multivariate%20kernel%20densities.%20Historical%20data%0Afrom%20a%20given%20autonomous%20system%20is%20leveraged%20to%20estimate%20a%20non-parametric%0Aprobabilistic%20description%20of%20the%20domain%2C%20which%20also%20describes%20the%20free%20space%0Awhere%20feasible%20solutions%20of%20the%20motion%20planning%20problem%20are%20likely%20to%20be%20found.%0AThe%20tuning%20parameters%20of%20the%20kernel%20density%20estimator%2C%20the%20bandwidth%20and%20the%0Akernel%2C%20are%20used%20to%20alter%20the%20description%20of%20the%20free%20space%20so%20that%20no%20samples%0Acan%20fall%20outside%20the%20originally%20defined%20space.The%20proposed%20method%20is%0Ademonstrated%20in%20two%20real-life%20case%20studies%3A%20An%20autonomous%20surface%20vessel%20%282D%29%0Aand%20an%20autonomous%20drone%20%283D%29.%20Two%20planning%20problems%20are%20solved%2C%20showing%20that%0Athe%20proposed%20approximately%20uniform%20sampling%20scheme%20is%20capable%20of%20guaranteeing%0Arejection-free%20samples%20of%20the%20considered%20workspace.%20Furthermore%2C%20the%0Aeffectiveness%20of%20the%20proposed%20method%20is%20statistically%20validated%20using%20Monte%0ACarlo%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2109.14687v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuaranteed%2520Rejection-free%2520Sampling%2520Method%2520Using%2520Past%2520Behaviours%2520for%250A%2520%2520Motion%2520Planning%2520of%2520Autonomous%2520Systems%26entry.906535625%3DThomas%2520T.%2520Enevoldsen%2520and%2520Roberto%2520Galeazzi%26entry.1292438233%3D%2520%2520The%2520paper%2520presents%2520a%2520novel%2520learning-based%2520sampling%2520strategy%2520that%2520guarantees%250Arejection-free%2520sampling%2520of%2520the%2520free%2520space%2520under%2520both%2520biased%2520and%2520approximately%250Auniform%2520conditions%252C%2520leveraging%2520multivariate%2520kernel%2520densities.%2520Historical%2520data%250Afrom%2520a%2520given%2520autonomous%2520system%2520is%2520leveraged%2520to%2520estimate%2520a%2520non-parametric%250Aprobabilistic%2520description%2520of%2520the%2520domain%252C%2520which%2520also%2520describes%2520the%2520free%2520space%250Awhere%2520feasible%2520solutions%2520of%2520the%2520motion%2520planning%2520problem%2520are%2520likely%2520to%2520be%2520found.%250AThe%2520tuning%2520parameters%2520of%2520the%2520kernel%2520density%2520estimator%252C%2520the%2520bandwidth%2520and%2520the%250Akernel%252C%2520are%2520used%2520to%2520alter%2520the%2520description%2520of%2520the%2520free%2520space%2520so%2520that%2520no%2520samples%250Acan%2520fall%2520outside%2520the%2520originally%2520defined%2520space.The%2520proposed%2520method%2520is%250Ademonstrated%2520in%2520two%2520real-life%2520case%2520studies%253A%2520An%2520autonomous%2520surface%2520vessel%2520%25282D%2529%250Aand%2520an%2520autonomous%2520drone%2520%25283D%2529.%2520Two%2520planning%2520problems%2520are%2520solved%252C%2520showing%2520that%250Athe%2520proposed%2520approximately%2520uniform%2520sampling%2520scheme%2520is%2520capable%2520of%2520guaranteeing%250Arejection-free%2520samples%2520of%2520the%2520considered%2520workspace.%2520Furthermore%252C%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520method%2520is%2520statistically%2520validated%2520using%2520Monte%250ACarlo%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2109.14687v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guaranteed%20Rejection-free%20Sampling%20Method%20Using%20Past%20Behaviours%20for%0A%20%20Motion%20Planning%20of%20Autonomous%20Systems&entry.906535625=Thomas%20T.%20Enevoldsen%20and%20Roberto%20Galeazzi&entry.1292438233=%20%20The%20paper%20presents%20a%20novel%20learning-based%20sampling%20strategy%20that%20guarantees%0Arejection-free%20sampling%20of%20the%20free%20space%20under%20both%20biased%20and%20approximately%0Auniform%20conditions%2C%20leveraging%20multivariate%20kernel%20densities.%20Historical%20data%0Afrom%20a%20given%20autonomous%20system%20is%20leveraged%20to%20estimate%20a%20non-parametric%0Aprobabilistic%20description%20of%20the%20domain%2C%20which%20also%20describes%20the%20free%20space%0Awhere%20feasible%20solutions%20of%20the%20motion%20planning%20problem%20are%20likely%20to%20be%20found.%0AThe%20tuning%20parameters%20of%20the%20kernel%20density%20estimator%2C%20the%20bandwidth%20and%20the%0Akernel%2C%20are%20used%20to%20alter%20the%20description%20of%20the%20free%20space%20so%20that%20no%20samples%0Acan%20fall%20outside%20the%20originally%20defined%20space.The%20proposed%20method%20is%0Ademonstrated%20in%20two%20real-life%20case%20studies%3A%20An%20autonomous%20surface%20vessel%20%282D%29%0Aand%20an%20autonomous%20drone%20%283D%29.%20Two%20planning%20problems%20are%20solved%2C%20showing%20that%0Athe%20proposed%20approximately%20uniform%20sampling%20scheme%20is%20capable%20of%20guaranteeing%0Arejection-free%20samples%20of%20the%20considered%20workspace.%20Furthermore%2C%20the%0Aeffectiveness%20of%20the%20proposed%20method%20is%20statistically%20validated%20using%20Monte%0ACarlo%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2109.14687v3&entry.124074799=Read"},
{"title": "Efficient LiDAR Reflectance Compression via Scanning Serialization", "author": "Jiahao Zhu and Kang You and Dandan Ding and Zhan Ma", "abstract": "  Reflectance attributes in LiDAR point clouds provide essential information\nfor downstream tasks but remain underexplored in neural compression methods. To\naddress this, we introduce SerLiC, a serialization-based neural compression\nframework to fully exploit the intrinsic characteristics of LiDAR reflectance.\nSerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order\nserialization, offering a device-centric perspective for reflectance analysis.\nEach point is then tokenized into a contextual representation comprising its\nsensor scanning index, radial distance, and prior reflectance, for effective\ndependencies exploration. For efficient sequential modeling, Mamba is\nincorporated with a dual parallelization scheme, enabling simultaneous\nautoregressive dependency capture and fast processing. Extensive experiments\ndemonstrate that SerLiC attains over 2x volume reduction against the original\nreflectance data, outperforming the state-of-the-art method by up to 22%\nreduction of compressed bits while using only 2% of its parameters. Moreover, a\nlightweight version of SerLiC achieves > 10 fps (frames per second) with just\n111K parameters, which is attractive for real-world applications.\n", "link": "http://arxiv.org/abs/2505.09433v1", "date": "2025-05-14", "relevancy": 2.1066, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5451}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5162}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20LiDAR%20Reflectance%20Compression%20via%20Scanning%20Serialization&body=Title%3A%20Efficient%20LiDAR%20Reflectance%20Compression%20via%20Scanning%20Serialization%0AAuthor%3A%20Jiahao%20Zhu%20and%20Kang%20You%20and%20Dandan%20Ding%20and%20Zhan%20Ma%0AAbstract%3A%20%20%20Reflectance%20attributes%20in%20LiDAR%20point%20clouds%20provide%20essential%20information%0Afor%20downstream%20tasks%20but%20remain%20underexplored%20in%20neural%20compression%20methods.%20To%0Aaddress%20this%2C%20we%20introduce%20SerLiC%2C%20a%20serialization-based%20neural%20compression%0Aframework%20to%20fully%20exploit%20the%20intrinsic%20characteristics%20of%20LiDAR%20reflectance.%0ASerLiC%20first%20transforms%203D%20LiDAR%20point%20clouds%20into%201D%20sequences%20via%20scan-order%0Aserialization%2C%20offering%20a%20device-centric%20perspective%20for%20reflectance%20analysis.%0AEach%20point%20is%20then%20tokenized%20into%20a%20contextual%20representation%20comprising%20its%0Asensor%20scanning%20index%2C%20radial%20distance%2C%20and%20prior%20reflectance%2C%20for%20effective%0Adependencies%20exploration.%20For%20efficient%20sequential%20modeling%2C%20Mamba%20is%0Aincorporated%20with%20a%20dual%20parallelization%20scheme%2C%20enabling%20simultaneous%0Aautoregressive%20dependency%20capture%20and%20fast%20processing.%20Extensive%20experiments%0Ademonstrate%20that%20SerLiC%20attains%20over%202x%20volume%20reduction%20against%20the%20original%0Areflectance%20data%2C%20outperforming%20the%20state-of-the-art%20method%20by%20up%20to%2022%25%0Areduction%20of%20compressed%20bits%20while%20using%20only%202%25%20of%20its%20parameters.%20Moreover%2C%20a%0Alightweight%20version%20of%20SerLiC%20achieves%20%3E%2010%20fps%20%28frames%20per%20second%29%20with%20just%0A111K%20parameters%2C%20which%20is%20attractive%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520LiDAR%2520Reflectance%2520Compression%2520via%2520Scanning%2520Serialization%26entry.906535625%3DJiahao%2520Zhu%2520and%2520Kang%2520You%2520and%2520Dandan%2520Ding%2520and%2520Zhan%2520Ma%26entry.1292438233%3D%2520%2520Reflectance%2520attributes%2520in%2520LiDAR%2520point%2520clouds%2520provide%2520essential%2520information%250Afor%2520downstream%2520tasks%2520but%2520remain%2520underexplored%2520in%2520neural%2520compression%2520methods.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520SerLiC%252C%2520a%2520serialization-based%2520neural%2520compression%250Aframework%2520to%2520fully%2520exploit%2520the%2520intrinsic%2520characteristics%2520of%2520LiDAR%2520reflectance.%250ASerLiC%2520first%2520transforms%25203D%2520LiDAR%2520point%2520clouds%2520into%25201D%2520sequences%2520via%2520scan-order%250Aserialization%252C%2520offering%2520a%2520device-centric%2520perspective%2520for%2520reflectance%2520analysis.%250AEach%2520point%2520is%2520then%2520tokenized%2520into%2520a%2520contextual%2520representation%2520comprising%2520its%250Asensor%2520scanning%2520index%252C%2520radial%2520distance%252C%2520and%2520prior%2520reflectance%252C%2520for%2520effective%250Adependencies%2520exploration.%2520For%2520efficient%2520sequential%2520modeling%252C%2520Mamba%2520is%250Aincorporated%2520with%2520a%2520dual%2520parallelization%2520scheme%252C%2520enabling%2520simultaneous%250Aautoregressive%2520dependency%2520capture%2520and%2520fast%2520processing.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520SerLiC%2520attains%2520over%25202x%2520volume%2520reduction%2520against%2520the%2520original%250Areflectance%2520data%252C%2520outperforming%2520the%2520state-of-the-art%2520method%2520by%2520up%2520to%252022%2525%250Areduction%2520of%2520compressed%2520bits%2520while%2520using%2520only%25202%2525%2520of%2520its%2520parameters.%2520Moreover%252C%2520a%250Alightweight%2520version%2520of%2520SerLiC%2520achieves%2520%253E%252010%2520fps%2520%2528frames%2520per%2520second%2529%2520with%2520just%250A111K%2520parameters%252C%2520which%2520is%2520attractive%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20LiDAR%20Reflectance%20Compression%20via%20Scanning%20Serialization&entry.906535625=Jiahao%20Zhu%20and%20Kang%20You%20and%20Dandan%20Ding%20and%20Zhan%20Ma&entry.1292438233=%20%20Reflectance%20attributes%20in%20LiDAR%20point%20clouds%20provide%20essential%20information%0Afor%20downstream%20tasks%20but%20remain%20underexplored%20in%20neural%20compression%20methods.%20To%0Aaddress%20this%2C%20we%20introduce%20SerLiC%2C%20a%20serialization-based%20neural%20compression%0Aframework%20to%20fully%20exploit%20the%20intrinsic%20characteristics%20of%20LiDAR%20reflectance.%0ASerLiC%20first%20transforms%203D%20LiDAR%20point%20clouds%20into%201D%20sequences%20via%20scan-order%0Aserialization%2C%20offering%20a%20device-centric%20perspective%20for%20reflectance%20analysis.%0AEach%20point%20is%20then%20tokenized%20into%20a%20contextual%20representation%20comprising%20its%0Asensor%20scanning%20index%2C%20radial%20distance%2C%20and%20prior%20reflectance%2C%20for%20effective%0Adependencies%20exploration.%20For%20efficient%20sequential%20modeling%2C%20Mamba%20is%0Aincorporated%20with%20a%20dual%20parallelization%20scheme%2C%20enabling%20simultaneous%0Aautoregressive%20dependency%20capture%20and%20fast%20processing.%20Extensive%20experiments%0Ademonstrate%20that%20SerLiC%20attains%20over%202x%20volume%20reduction%20against%20the%20original%0Areflectance%20data%2C%20outperforming%20the%20state-of-the-art%20method%20by%20up%20to%2022%25%0Areduction%20of%20compressed%20bits%20while%20using%20only%202%25%20of%20its%20parameters.%20Moreover%2C%20a%0Alightweight%20version%20of%20SerLiC%20achieves%20%3E%2010%20fps%20%28frames%20per%20second%29%20with%20just%0A111K%20parameters%2C%20which%20is%20attractive%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09433v1&entry.124074799=Read"},
{"title": "Conformal Bounds on Full-Reference Image Quality for Imaging Inverse\n  Problems", "author": "Jeffrey Wen and Rizwan Ahmad and Philip Schniter", "abstract": "  In imaging inverse problems, we would like to know how close the recovered\nimage is to the true image in terms of full-reference image quality (FRIQ)\nmetrics like PSNR, SSIM, LPIPS, etc. This is especially important in\nsafety-critical applications like medical imaging, where knowing that, say, the\nSSIM was poor could potentially avoid a costly misdiagnosis. But since we don't\nknow the true image, computing FRIQ is non-trivial. In this work, we combine\nconformal prediction with approximate posterior sampling to construct bounds on\nFRIQ that are guaranteed to hold up to a user-specified error probability. We\ndemonstrate our approach on image denoising and accelerated magnetic resonance\nimaging (MRI) problems. Code is available at\nhttps://github.com/jwen307/quality_uq.\n", "link": "http://arxiv.org/abs/2505.09528v1", "date": "2025-05-14", "relevancy": 2.1047, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5481}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5201}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Bounds%20on%20Full-Reference%20Image%20Quality%20for%20Imaging%20Inverse%0A%20%20Problems&body=Title%3A%20Conformal%20Bounds%20on%20Full-Reference%20Image%20Quality%20for%20Imaging%20Inverse%0A%20%20Problems%0AAuthor%3A%20Jeffrey%20Wen%20and%20Rizwan%20Ahmad%20and%20Philip%20Schniter%0AAbstract%3A%20%20%20In%20imaging%20inverse%20problems%2C%20we%20would%20like%20to%20know%20how%20close%20the%20recovered%0Aimage%20is%20to%20the%20true%20image%20in%20terms%20of%20full-reference%20image%20quality%20%28FRIQ%29%0Ametrics%20like%20PSNR%2C%20SSIM%2C%20LPIPS%2C%20etc.%20This%20is%20especially%20important%20in%0Asafety-critical%20applications%20like%20medical%20imaging%2C%20where%20knowing%20that%2C%20say%2C%20the%0ASSIM%20was%20poor%20could%20potentially%20avoid%20a%20costly%20misdiagnosis.%20But%20since%20we%20don%27t%0Aknow%20the%20true%20image%2C%20computing%20FRIQ%20is%20non-trivial.%20In%20this%20work%2C%20we%20combine%0Aconformal%20prediction%20with%20approximate%20posterior%20sampling%20to%20construct%20bounds%20on%0AFRIQ%20that%20are%20guaranteed%20to%20hold%20up%20to%20a%20user-specified%20error%20probability.%20We%0Ademonstrate%20our%20approach%20on%20image%20denoising%20and%20accelerated%20magnetic%20resonance%0Aimaging%20%28MRI%29%20problems.%20Code%20is%20available%20at%0Ahttps%3A//github.com/jwen307/quality_uq.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Bounds%2520on%2520Full-Reference%2520Image%2520Quality%2520for%2520Imaging%2520Inverse%250A%2520%2520Problems%26entry.906535625%3DJeffrey%2520Wen%2520and%2520Rizwan%2520Ahmad%2520and%2520Philip%2520Schniter%26entry.1292438233%3D%2520%2520In%2520imaging%2520inverse%2520problems%252C%2520we%2520would%2520like%2520to%2520know%2520how%2520close%2520the%2520recovered%250Aimage%2520is%2520to%2520the%2520true%2520image%2520in%2520terms%2520of%2520full-reference%2520image%2520quality%2520%2528FRIQ%2529%250Ametrics%2520like%2520PSNR%252C%2520SSIM%252C%2520LPIPS%252C%2520etc.%2520This%2520is%2520especially%2520important%2520in%250Asafety-critical%2520applications%2520like%2520medical%2520imaging%252C%2520where%2520knowing%2520that%252C%2520say%252C%2520the%250ASSIM%2520was%2520poor%2520could%2520potentially%2520avoid%2520a%2520costly%2520misdiagnosis.%2520But%2520since%2520we%2520don%2527t%250Aknow%2520the%2520true%2520image%252C%2520computing%2520FRIQ%2520is%2520non-trivial.%2520In%2520this%2520work%252C%2520we%2520combine%250Aconformal%2520prediction%2520with%2520approximate%2520posterior%2520sampling%2520to%2520construct%2520bounds%2520on%250AFRIQ%2520that%2520are%2520guaranteed%2520to%2520hold%2520up%2520to%2520a%2520user-specified%2520error%2520probability.%2520We%250Ademonstrate%2520our%2520approach%2520on%2520image%2520denoising%2520and%2520accelerated%2520magnetic%2520resonance%250Aimaging%2520%2528MRI%2529%2520problems.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/jwen307/quality_uq.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Bounds%20on%20Full-Reference%20Image%20Quality%20for%20Imaging%20Inverse%0A%20%20Problems&entry.906535625=Jeffrey%20Wen%20and%20Rizwan%20Ahmad%20and%20Philip%20Schniter&entry.1292438233=%20%20In%20imaging%20inverse%20problems%2C%20we%20would%20like%20to%20know%20how%20close%20the%20recovered%0Aimage%20is%20to%20the%20true%20image%20in%20terms%20of%20full-reference%20image%20quality%20%28FRIQ%29%0Ametrics%20like%20PSNR%2C%20SSIM%2C%20LPIPS%2C%20etc.%20This%20is%20especially%20important%20in%0Asafety-critical%20applications%20like%20medical%20imaging%2C%20where%20knowing%20that%2C%20say%2C%20the%0ASSIM%20was%20poor%20could%20potentially%20avoid%20a%20costly%20misdiagnosis.%20But%20since%20we%20don%27t%0Aknow%20the%20true%20image%2C%20computing%20FRIQ%20is%20non-trivial.%20In%20this%20work%2C%20we%20combine%0Aconformal%20prediction%20with%20approximate%20posterior%20sampling%20to%20construct%20bounds%20on%0AFRIQ%20that%20are%20guaranteed%20to%20hold%20up%20to%20a%20user-specified%20error%20probability.%20We%0Ademonstrate%20our%20approach%20on%20image%20denoising%20and%20accelerated%20magnetic%20resonance%0Aimaging%20%28MRI%29%20problems.%20Code%20is%20available%20at%0Ahttps%3A//github.com/jwen307/quality_uq.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09528v1&entry.124074799=Read"},
{"title": "Learning to Detect Multi-class Anomalies with Just One Normal Image\n  Prompt", "author": "Bin-Bin Gao", "abstract": "  Unsupervised reconstruction networks using self-attention transformers have\nachieved state-of-the-art performance for multi-class (unified) anomaly\ndetection with a single model. However, these self-attention reconstruction\nmodels primarily operate on target features, which may result in perfect\nreconstruction for both normal and anomaly features due to high consistency\nwith context, leading to failure in detecting anomalies. Additionally, these\nmodels often produce inaccurate anomaly segmentation due to performing\nreconstruction in a low spatial resolution latent space. To enable\nreconstruction models enjoying high efficiency while enhancing their\ngeneralization for unified anomaly detection, we propose a simple yet effective\nmethod that reconstructs normal features and restores anomaly features with\njust One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP\nallows for the first time to reconstruct or restore anomalies with just one\nnormal image prompt, effectively boosting unified anomaly detection\nperformance. Furthermore, we propose a supervised refiner that regresses\nreconstruction errors by using both real normal and synthesized anomalous\nimages, which significantly improves pixel-level anomaly segmentation. OneNIP\noutperforms previous methods on three industry anomaly detection benchmarks:\nMVTec, BTAD, and VisA. The code and pre-trained models are available at\nhttps://github.com/gaobb/OneNIP.\n", "link": "http://arxiv.org/abs/2505.09264v1", "date": "2025-05-14", "relevancy": 2.0972, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5495}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5255}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Detect%20Multi-class%20Anomalies%20with%20Just%20One%20Normal%20Image%0A%20%20Prompt&body=Title%3A%20Learning%20to%20Detect%20Multi-class%20Anomalies%20with%20Just%20One%20Normal%20Image%0A%20%20Prompt%0AAuthor%3A%20Bin-Bin%20Gao%0AAbstract%3A%20%20%20Unsupervised%20reconstruction%20networks%20using%20self-attention%20transformers%20have%0Aachieved%20state-of-the-art%20performance%20for%20multi-class%20%28unified%29%20anomaly%0Adetection%20with%20a%20single%20model.%20However%2C%20these%20self-attention%20reconstruction%0Amodels%20primarily%20operate%20on%20target%20features%2C%20which%20may%20result%20in%20perfect%0Areconstruction%20for%20both%20normal%20and%20anomaly%20features%20due%20to%20high%20consistency%0Awith%20context%2C%20leading%20to%20failure%20in%20detecting%20anomalies.%20Additionally%2C%20these%0Amodels%20often%20produce%20inaccurate%20anomaly%20segmentation%20due%20to%20performing%0Areconstruction%20in%20a%20low%20spatial%20resolution%20latent%20space.%20To%20enable%0Areconstruction%20models%20enjoying%20high%20efficiency%20while%20enhancing%20their%0Ageneralization%20for%20unified%20anomaly%20detection%2C%20we%20propose%20a%20simple%20yet%20effective%0Amethod%20that%20reconstructs%20normal%20features%20and%20restores%20anomaly%20features%20with%0Ajust%20One%20Normal%20Image%20Prompt%20%28OneNIP%29.%20In%20contrast%20to%20previous%20work%2C%20OneNIP%0Aallows%20for%20the%20first%20time%20to%20reconstruct%20or%20restore%20anomalies%20with%20just%20one%0Anormal%20image%20prompt%2C%20effectively%20boosting%20unified%20anomaly%20detection%0Aperformance.%20Furthermore%2C%20we%20propose%20a%20supervised%20refiner%20that%20regresses%0Areconstruction%20errors%20by%20using%20both%20real%20normal%20and%20synthesized%20anomalous%0Aimages%2C%20which%20significantly%20improves%20pixel-level%20anomaly%20segmentation.%20OneNIP%0Aoutperforms%20previous%20methods%20on%20three%20industry%20anomaly%20detection%20benchmarks%3A%0AMVTec%2C%20BTAD%2C%20and%20VisA.%20The%20code%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/gaobb/OneNIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Detect%2520Multi-class%2520Anomalies%2520with%2520Just%2520One%2520Normal%2520Image%250A%2520%2520Prompt%26entry.906535625%3DBin-Bin%2520Gao%26entry.1292438233%3D%2520%2520Unsupervised%2520reconstruction%2520networks%2520using%2520self-attention%2520transformers%2520have%250Aachieved%2520state-of-the-art%2520performance%2520for%2520multi-class%2520%2528unified%2529%2520anomaly%250Adetection%2520with%2520a%2520single%2520model.%2520However%252C%2520these%2520self-attention%2520reconstruction%250Amodels%2520primarily%2520operate%2520on%2520target%2520features%252C%2520which%2520may%2520result%2520in%2520perfect%250Areconstruction%2520for%2520both%2520normal%2520and%2520anomaly%2520features%2520due%2520to%2520high%2520consistency%250Awith%2520context%252C%2520leading%2520to%2520failure%2520in%2520detecting%2520anomalies.%2520Additionally%252C%2520these%250Amodels%2520often%2520produce%2520inaccurate%2520anomaly%2520segmentation%2520due%2520to%2520performing%250Areconstruction%2520in%2520a%2520low%2520spatial%2520resolution%2520latent%2520space.%2520To%2520enable%250Areconstruction%2520models%2520enjoying%2520high%2520efficiency%2520while%2520enhancing%2520their%250Ageneralization%2520for%2520unified%2520anomaly%2520detection%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%250Amethod%2520that%2520reconstructs%2520normal%2520features%2520and%2520restores%2520anomaly%2520features%2520with%250Ajust%2520One%2520Normal%2520Image%2520Prompt%2520%2528OneNIP%2529.%2520In%2520contrast%2520to%2520previous%2520work%252C%2520OneNIP%250Aallows%2520for%2520the%2520first%2520time%2520to%2520reconstruct%2520or%2520restore%2520anomalies%2520with%2520just%2520one%250Anormal%2520image%2520prompt%252C%2520effectively%2520boosting%2520unified%2520anomaly%2520detection%250Aperformance.%2520Furthermore%252C%2520we%2520propose%2520a%2520supervised%2520refiner%2520that%2520regresses%250Areconstruction%2520errors%2520by%2520using%2520both%2520real%2520normal%2520and%2520synthesized%2520anomalous%250Aimages%252C%2520which%2520significantly%2520improves%2520pixel-level%2520anomaly%2520segmentation.%2520OneNIP%250Aoutperforms%2520previous%2520methods%2520on%2520three%2520industry%2520anomaly%2520detection%2520benchmarks%253A%250AMVTec%252C%2520BTAD%252C%2520and%2520VisA.%2520The%2520code%2520and%2520pre-trained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/gaobb/OneNIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Detect%20Multi-class%20Anomalies%20with%20Just%20One%20Normal%20Image%0A%20%20Prompt&entry.906535625=Bin-Bin%20Gao&entry.1292438233=%20%20Unsupervised%20reconstruction%20networks%20using%20self-attention%20transformers%20have%0Aachieved%20state-of-the-art%20performance%20for%20multi-class%20%28unified%29%20anomaly%0Adetection%20with%20a%20single%20model.%20However%2C%20these%20self-attention%20reconstruction%0Amodels%20primarily%20operate%20on%20target%20features%2C%20which%20may%20result%20in%20perfect%0Areconstruction%20for%20both%20normal%20and%20anomaly%20features%20due%20to%20high%20consistency%0Awith%20context%2C%20leading%20to%20failure%20in%20detecting%20anomalies.%20Additionally%2C%20these%0Amodels%20often%20produce%20inaccurate%20anomaly%20segmentation%20due%20to%20performing%0Areconstruction%20in%20a%20low%20spatial%20resolution%20latent%20space.%20To%20enable%0Areconstruction%20models%20enjoying%20high%20efficiency%20while%20enhancing%20their%0Ageneralization%20for%20unified%20anomaly%20detection%2C%20we%20propose%20a%20simple%20yet%20effective%0Amethod%20that%20reconstructs%20normal%20features%20and%20restores%20anomaly%20features%20with%0Ajust%20One%20Normal%20Image%20Prompt%20%28OneNIP%29.%20In%20contrast%20to%20previous%20work%2C%20OneNIP%0Aallows%20for%20the%20first%20time%20to%20reconstruct%20or%20restore%20anomalies%20with%20just%20one%0Anormal%20image%20prompt%2C%20effectively%20boosting%20unified%20anomaly%20detection%0Aperformance.%20Furthermore%2C%20we%20propose%20a%20supervised%20refiner%20that%20regresses%0Areconstruction%20errors%20by%20using%20both%20real%20normal%20and%20synthesized%20anomalous%0Aimages%2C%20which%20significantly%20improves%20pixel-level%20anomaly%20segmentation.%20OneNIP%0Aoutperforms%20previous%20methods%20on%20three%20industry%20anomaly%20detection%20benchmarks%3A%0AMVTec%2C%20BTAD%2C%20and%20VisA.%20The%20code%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/gaobb/OneNIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09264v1&entry.124074799=Read"},
{"title": "Gradient Attention Map Based Verification of Deep Convolutional Neural\n  Networks with Application to X-ray Image Datasets", "author": "Omid Halimi Milani and Amanda Nikho and Lauren Mills and Marouane Tliba and Ahmet Enis Cetin and Mohammed H. Elnagar", "abstract": "  Deep learning models have great potential in medical imaging, including\northodontics and skeletal maturity assessment. However, applying a model to\ndata different from its training set can lead to unreliable predictions that\nmay impact patient care. To address this, we propose a comprehensive\nverification framework that evaluates model suitability through multiple\ncomplementary strategies. First, we introduce a Gradient Attention Map\n(GAM)-based approach that analyzes attention patterns using Grad-CAM and\ncompares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine\nSimilarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.\nSecond, we extend verification to early convolutional feature maps, capturing\nstructural mis-alignments missed by attention alone. Finally, we incorporate an\nadditional garbage class into the classification model to explicitly reject\nout-of-distribution inputs. Experimental results demonstrate that these\ncombined methods effectively identify unsuitable models and inputs, promoting\nsafer and more reliable deployment of deep learning in medical imaging.\n", "link": "http://arxiv.org/abs/2504.21227v2", "date": "2025-05-14", "relevancy": 2.0921, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5328}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5222}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Attention%20Map%20Based%20Verification%20of%20Deep%20Convolutional%20Neural%0A%20%20Networks%20with%20Application%20to%20X-ray%20Image%20Datasets&body=Title%3A%20Gradient%20Attention%20Map%20Based%20Verification%20of%20Deep%20Convolutional%20Neural%0A%20%20Networks%20with%20Application%20to%20X-ray%20Image%20Datasets%0AAuthor%3A%20Omid%20Halimi%20Milani%20and%20Amanda%20Nikho%20and%20Lauren%20Mills%20and%20Marouane%20Tliba%20and%20Ahmet%20Enis%20Cetin%20and%20Mohammed%20H.%20Elnagar%0AAbstract%3A%20%20%20Deep%20learning%20models%20have%20great%20potential%20in%20medical%20imaging%2C%20including%0Aorthodontics%20and%20skeletal%20maturity%20assessment.%20However%2C%20applying%20a%20model%20to%0Adata%20different%20from%20its%20training%20set%20can%20lead%20to%20unreliable%20predictions%20that%0Amay%20impact%20patient%20care.%20To%20address%20this%2C%20we%20propose%20a%20comprehensive%0Averification%20framework%20that%20evaluates%20model%20suitability%20through%20multiple%0Acomplementary%20strategies.%20First%2C%20we%20introduce%20a%20Gradient%20Attention%20Map%0A%28GAM%29-based%20approach%20that%20analyzes%20attention%20patterns%20using%20Grad-CAM%20and%0Acompares%20them%20via%20similarity%20metrics%20such%20as%20IoU%2C%20Dice%20Similarity%2C%20SSIM%2C%20Cosine%0ASimilarity%2C%20Pearson%20Correlation%2C%20KL%20Divergence%2C%20and%20Wasserstein%20Distance.%0ASecond%2C%20we%20extend%20verification%20to%20early%20convolutional%20feature%20maps%2C%20capturing%0Astructural%20mis-alignments%20missed%20by%20attention%20alone.%20Finally%2C%20we%20incorporate%20an%0Aadditional%20garbage%20class%20into%20the%20classification%20model%20to%20explicitly%20reject%0Aout-of-distribution%20inputs.%20Experimental%20results%20demonstrate%20that%20these%0Acombined%20methods%20effectively%20identify%20unsuitable%20models%20and%20inputs%2C%20promoting%0Asafer%20and%20more%20reliable%20deployment%20of%20deep%20learning%20in%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21227v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Attention%2520Map%2520Based%2520Verification%2520of%2520Deep%2520Convolutional%2520Neural%250A%2520%2520Networks%2520with%2520Application%2520to%2520X-ray%2520Image%2520Datasets%26entry.906535625%3DOmid%2520Halimi%2520Milani%2520and%2520Amanda%2520Nikho%2520and%2520Lauren%2520Mills%2520and%2520Marouane%2520Tliba%2520and%2520Ahmet%2520Enis%2520Cetin%2520and%2520Mohammed%2520H.%2520Elnagar%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520have%2520great%2520potential%2520in%2520medical%2520imaging%252C%2520including%250Aorthodontics%2520and%2520skeletal%2520maturity%2520assessment.%2520However%252C%2520applying%2520a%2520model%2520to%250Adata%2520different%2520from%2520its%2520training%2520set%2520can%2520lead%2520to%2520unreliable%2520predictions%2520that%250Amay%2520impact%2520patient%2520care.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520comprehensive%250Averification%2520framework%2520that%2520evaluates%2520model%2520suitability%2520through%2520multiple%250Acomplementary%2520strategies.%2520First%252C%2520we%2520introduce%2520a%2520Gradient%2520Attention%2520Map%250A%2528GAM%2529-based%2520approach%2520that%2520analyzes%2520attention%2520patterns%2520using%2520Grad-CAM%2520and%250Acompares%2520them%2520via%2520similarity%2520metrics%2520such%2520as%2520IoU%252C%2520Dice%2520Similarity%252C%2520SSIM%252C%2520Cosine%250ASimilarity%252C%2520Pearson%2520Correlation%252C%2520KL%2520Divergence%252C%2520and%2520Wasserstein%2520Distance.%250ASecond%252C%2520we%2520extend%2520verification%2520to%2520early%2520convolutional%2520feature%2520maps%252C%2520capturing%250Astructural%2520mis-alignments%2520missed%2520by%2520attention%2520alone.%2520Finally%252C%2520we%2520incorporate%2520an%250Aadditional%2520garbage%2520class%2520into%2520the%2520classification%2520model%2520to%2520explicitly%2520reject%250Aout-of-distribution%2520inputs.%2520Experimental%2520results%2520demonstrate%2520that%2520these%250Acombined%2520methods%2520effectively%2520identify%2520unsuitable%2520models%2520and%2520inputs%252C%2520promoting%250Asafer%2520and%2520more%2520reliable%2520deployment%2520of%2520deep%2520learning%2520in%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21227v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Attention%20Map%20Based%20Verification%20of%20Deep%20Convolutional%20Neural%0A%20%20Networks%20with%20Application%20to%20X-ray%20Image%20Datasets&entry.906535625=Omid%20Halimi%20Milani%20and%20Amanda%20Nikho%20and%20Lauren%20Mills%20and%20Marouane%20Tliba%20and%20Ahmet%20Enis%20Cetin%20and%20Mohammed%20H.%20Elnagar&entry.1292438233=%20%20Deep%20learning%20models%20have%20great%20potential%20in%20medical%20imaging%2C%20including%0Aorthodontics%20and%20skeletal%20maturity%20assessment.%20However%2C%20applying%20a%20model%20to%0Adata%20different%20from%20its%20training%20set%20can%20lead%20to%20unreliable%20predictions%20that%0Amay%20impact%20patient%20care.%20To%20address%20this%2C%20we%20propose%20a%20comprehensive%0Averification%20framework%20that%20evaluates%20model%20suitability%20through%20multiple%0Acomplementary%20strategies.%20First%2C%20we%20introduce%20a%20Gradient%20Attention%20Map%0A%28GAM%29-based%20approach%20that%20analyzes%20attention%20patterns%20using%20Grad-CAM%20and%0Acompares%20them%20via%20similarity%20metrics%20such%20as%20IoU%2C%20Dice%20Similarity%2C%20SSIM%2C%20Cosine%0ASimilarity%2C%20Pearson%20Correlation%2C%20KL%20Divergence%2C%20and%20Wasserstein%20Distance.%0ASecond%2C%20we%20extend%20verification%20to%20early%20convolutional%20feature%20maps%2C%20capturing%0Astructural%20mis-alignments%20missed%20by%20attention%20alone.%20Finally%2C%20we%20incorporate%20an%0Aadditional%20garbage%20class%20into%20the%20classification%20model%20to%20explicitly%20reject%0Aout-of-distribution%20inputs.%20Experimental%20results%20demonstrate%20that%20these%0Acombined%20methods%20effectively%20identify%20unsuitable%20models%20and%20inputs%2C%20promoting%0Asafer%20and%20more%20reliable%20deployment%20of%20deep%20learning%20in%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21227v2&entry.124074799=Read"},
{"title": "ARCANE -- Early Detection of Interplanetary Coronal Mass Ejections", "author": "H. T. R\u00fcdisser and G. Nguyen and J. Le Lou\u00ebdec and C. M\u00f6stl", "abstract": "  Interplanetary coronal mass ejections (ICMEs) are major drivers of space\nweather disturbances, posing risks to both technological infrastructure and\nhuman activities. Automatic detection of ICMEs in solar wind in situ data is\nessential for early warning systems. While several methods have been proposed\nto identify these structures in time series data, robust real-time detection\nremains a significant challenge. In this work, we present ARCANE - the first\nframework explicitly designed for early ICME detection in streaming solar wind\ndata under realistic operational constraints, enabling event identification\nwithout requiring observation of the full structure. Our approach evaluates the\nstrengths and limitations of detection models by comparing a machine\nlearning-based method to a threshold-based baseline. The ResUNet++ model,\npreviously validated on science data, significantly outperforms the baseline,\nparticularly in detecting high-impact events, while retaining solid performance\non lower-impact cases. Notably, we find that using real-time solar wind (RTSW)\ndata instead of high-resolution science data leads to only minimal performance\ndegradation. Despite the challenges of operational settings, our detection\npipeline achieves an F1 score of 0.53, with an average detection delay of 21.5%\nof the event's duration while only seeing a minimal amount of data. As more\ndata becomes available, the performance increases significantly. These results\nmark a substantial step forward in automated space weather monitoring and lay\nthe groundwork for enhanced real-time forecasting capabilities.\n", "link": "http://arxiv.org/abs/2505.09365v1", "date": "2025-05-14", "relevancy": 2.0866, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4458}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4083}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.3978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARCANE%20--%20Early%20Detection%20of%20Interplanetary%20Coronal%20Mass%20Ejections&body=Title%3A%20ARCANE%20--%20Early%20Detection%20of%20Interplanetary%20Coronal%20Mass%20Ejections%0AAuthor%3A%20H.%20T.%20R%C3%BCdisser%20and%20G.%20Nguyen%20and%20J.%20Le%20Lou%C3%ABdec%20and%20C.%20M%C3%B6stl%0AAbstract%3A%20%20%20Interplanetary%20coronal%20mass%20ejections%20%28ICMEs%29%20are%20major%20drivers%20of%20space%0Aweather%20disturbances%2C%20posing%20risks%20to%20both%20technological%20infrastructure%20and%0Ahuman%20activities.%20Automatic%20detection%20of%20ICMEs%20in%20solar%20wind%20in%20situ%20data%20is%0Aessential%20for%20early%20warning%20systems.%20While%20several%20methods%20have%20been%20proposed%0Ato%20identify%20these%20structures%20in%20time%20series%20data%2C%20robust%20real-time%20detection%0Aremains%20a%20significant%20challenge.%20In%20this%20work%2C%20we%20present%20ARCANE%20-%20the%20first%0Aframework%20explicitly%20designed%20for%20early%20ICME%20detection%20in%20streaming%20solar%20wind%0Adata%20under%20realistic%20operational%20constraints%2C%20enabling%20event%20identification%0Awithout%20requiring%20observation%20of%20the%20full%20structure.%20Our%20approach%20evaluates%20the%0Astrengths%20and%20limitations%20of%20detection%20models%20by%20comparing%20a%20machine%0Alearning-based%20method%20to%20a%20threshold-based%20baseline.%20The%20ResUNet%2B%2B%20model%2C%0Apreviously%20validated%20on%20science%20data%2C%20significantly%20outperforms%20the%20baseline%2C%0Aparticularly%20in%20detecting%20high-impact%20events%2C%20while%20retaining%20solid%20performance%0Aon%20lower-impact%20cases.%20Notably%2C%20we%20find%20that%20using%20real-time%20solar%20wind%20%28RTSW%29%0Adata%20instead%20of%20high-resolution%20science%20data%20leads%20to%20only%20minimal%20performance%0Adegradation.%20Despite%20the%20challenges%20of%20operational%20settings%2C%20our%20detection%0Apipeline%20achieves%20an%20F1%20score%20of%200.53%2C%20with%20an%20average%20detection%20delay%20of%2021.5%25%0Aof%20the%20event%27s%20duration%20while%20only%20seeing%20a%20minimal%20amount%20of%20data.%20As%20more%0Adata%20becomes%20available%2C%20the%20performance%20increases%20significantly.%20These%20results%0Amark%20a%20substantial%20step%20forward%20in%20automated%20space%20weather%20monitoring%20and%20lay%0Athe%20groundwork%20for%20enhanced%20real-time%20forecasting%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARCANE%2520--%2520Early%2520Detection%2520of%2520Interplanetary%2520Coronal%2520Mass%2520Ejections%26entry.906535625%3DH.%2520T.%2520R%25C3%25BCdisser%2520and%2520G.%2520Nguyen%2520and%2520J.%2520Le%2520Lou%25C3%25ABdec%2520and%2520C.%2520M%25C3%25B6stl%26entry.1292438233%3D%2520%2520Interplanetary%2520coronal%2520mass%2520ejections%2520%2528ICMEs%2529%2520are%2520major%2520drivers%2520of%2520space%250Aweather%2520disturbances%252C%2520posing%2520risks%2520to%2520both%2520technological%2520infrastructure%2520and%250Ahuman%2520activities.%2520Automatic%2520detection%2520of%2520ICMEs%2520in%2520solar%2520wind%2520in%2520situ%2520data%2520is%250Aessential%2520for%2520early%2520warning%2520systems.%2520While%2520several%2520methods%2520have%2520been%2520proposed%250Ato%2520identify%2520these%2520structures%2520in%2520time%2520series%2520data%252C%2520robust%2520real-time%2520detection%250Aremains%2520a%2520significant%2520challenge.%2520In%2520this%2520work%252C%2520we%2520present%2520ARCANE%2520-%2520the%2520first%250Aframework%2520explicitly%2520designed%2520for%2520early%2520ICME%2520detection%2520in%2520streaming%2520solar%2520wind%250Adata%2520under%2520realistic%2520operational%2520constraints%252C%2520enabling%2520event%2520identification%250Awithout%2520requiring%2520observation%2520of%2520the%2520full%2520structure.%2520Our%2520approach%2520evaluates%2520the%250Astrengths%2520and%2520limitations%2520of%2520detection%2520models%2520by%2520comparing%2520a%2520machine%250Alearning-based%2520method%2520to%2520a%2520threshold-based%2520baseline.%2520The%2520ResUNet%252B%252B%2520model%252C%250Apreviously%2520validated%2520on%2520science%2520data%252C%2520significantly%2520outperforms%2520the%2520baseline%252C%250Aparticularly%2520in%2520detecting%2520high-impact%2520events%252C%2520while%2520retaining%2520solid%2520performance%250Aon%2520lower-impact%2520cases.%2520Notably%252C%2520we%2520find%2520that%2520using%2520real-time%2520solar%2520wind%2520%2528RTSW%2529%250Adata%2520instead%2520of%2520high-resolution%2520science%2520data%2520leads%2520to%2520only%2520minimal%2520performance%250Adegradation.%2520Despite%2520the%2520challenges%2520of%2520operational%2520settings%252C%2520our%2520detection%250Apipeline%2520achieves%2520an%2520F1%2520score%2520of%25200.53%252C%2520with%2520an%2520average%2520detection%2520delay%2520of%252021.5%2525%250Aof%2520the%2520event%2527s%2520duration%2520while%2520only%2520seeing%2520a%2520minimal%2520amount%2520of%2520data.%2520As%2520more%250Adata%2520becomes%2520available%252C%2520the%2520performance%2520increases%2520significantly.%2520These%2520results%250Amark%2520a%2520substantial%2520step%2520forward%2520in%2520automated%2520space%2520weather%2520monitoring%2520and%2520lay%250Athe%2520groundwork%2520for%2520enhanced%2520real-time%2520forecasting%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARCANE%20--%20Early%20Detection%20of%20Interplanetary%20Coronal%20Mass%20Ejections&entry.906535625=H.%20T.%20R%C3%BCdisser%20and%20G.%20Nguyen%20and%20J.%20Le%20Lou%C3%ABdec%20and%20C.%20M%C3%B6stl&entry.1292438233=%20%20Interplanetary%20coronal%20mass%20ejections%20%28ICMEs%29%20are%20major%20drivers%20of%20space%0Aweather%20disturbances%2C%20posing%20risks%20to%20both%20technological%20infrastructure%20and%0Ahuman%20activities.%20Automatic%20detection%20of%20ICMEs%20in%20solar%20wind%20in%20situ%20data%20is%0Aessential%20for%20early%20warning%20systems.%20While%20several%20methods%20have%20been%20proposed%0Ato%20identify%20these%20structures%20in%20time%20series%20data%2C%20robust%20real-time%20detection%0Aremains%20a%20significant%20challenge.%20In%20this%20work%2C%20we%20present%20ARCANE%20-%20the%20first%0Aframework%20explicitly%20designed%20for%20early%20ICME%20detection%20in%20streaming%20solar%20wind%0Adata%20under%20realistic%20operational%20constraints%2C%20enabling%20event%20identification%0Awithout%20requiring%20observation%20of%20the%20full%20structure.%20Our%20approach%20evaluates%20the%0Astrengths%20and%20limitations%20of%20detection%20models%20by%20comparing%20a%20machine%0Alearning-based%20method%20to%20a%20threshold-based%20baseline.%20The%20ResUNet%2B%2B%20model%2C%0Apreviously%20validated%20on%20science%20data%2C%20significantly%20outperforms%20the%20baseline%2C%0Aparticularly%20in%20detecting%20high-impact%20events%2C%20while%20retaining%20solid%20performance%0Aon%20lower-impact%20cases.%20Notably%2C%20we%20find%20that%20using%20real-time%20solar%20wind%20%28RTSW%29%0Adata%20instead%20of%20high-resolution%20science%20data%20leads%20to%20only%20minimal%20performance%0Adegradation.%20Despite%20the%20challenges%20of%20operational%20settings%2C%20our%20detection%0Apipeline%20achieves%20an%20F1%20score%20of%200.53%2C%20with%20an%20average%20detection%20delay%20of%2021.5%25%0Aof%20the%20event%27s%20duration%20while%20only%20seeing%20a%20minimal%20amount%20of%20data.%20As%20more%0Adata%20becomes%20available%2C%20the%20performance%20increases%20significantly.%20These%20results%0Amark%20a%20substantial%20step%20forward%20in%20automated%20space%20weather%20monitoring%20and%20lay%0Athe%20groundwork%20for%20enhanced%20real-time%20forecasting%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09365v1&entry.124074799=Read"},
{"title": "Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D\n  Cardiac CT Segmentation", "author": "Anne-Marie Rickmann and Stephanie L. Thorn and Shawn S. Ahn and Supum Lee and Selen Uman and Taras Lysyy and Rachel Burns and Nicole Guerrera and Francis G. Spinale and Jason A. Burdick and Albert J. Sinusas and James S. Duncan", "abstract": "  Cardiac image segmentation is an important step in many cardiac image\nanalysis and modeling tasks such as motion tracking or simulations of cardiac\nmechanics. While deep learning has greatly advanced segmentation in clinical\nsettings, there is limited work on pre-clinical imaging, notably in porcine\nmodels, which are often used due to their anatomical and physiological\nsimilarity to humans. However, differences between species create a domain\nshift that complicates direct model transfer from human to pig data.\n  Recently, foundation models trained on large human datasets have shown\npromise for robust medical image segmentation; yet their applicability to\nporcine data remains largely unexplored. In this work, we investigate whether\nfoundation models can generate sufficiently accurate pseudo-labels for pig\ncardiac CT and propose a simple self-training approach to iteratively refine\nthese labels. Our method requires no manually annotated pig data, relying\ninstead on iterative updates to improve segmentation quality. We demonstrate\nthat this self-training process not only enhances segmentation accuracy but\nalso smooths out temporal inconsistencies across consecutive frames. Although\nour results are encouraging, there remains room for improvement, for example by\nincorporating more sophisticated self-training strategies and by exploring\nadditional foundation models and other cardiac imaging technologies.\n", "link": "http://arxiv.org/abs/2505.09564v1", "date": "2025-05-14", "relevancy": 2.0849, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Foundation%20Models%20as%20Pseudo-Label%20Generators%20for%20Pre-Clinical%204D%0A%20%20Cardiac%20CT%20Segmentation&body=Title%3A%20Using%20Foundation%20Models%20as%20Pseudo-Label%20Generators%20for%20Pre-Clinical%204D%0A%20%20Cardiac%20CT%20Segmentation%0AAuthor%3A%20Anne-Marie%20Rickmann%20and%20Stephanie%20L.%20Thorn%20and%20Shawn%20S.%20Ahn%20and%20Supum%20Lee%20and%20Selen%20Uman%20and%20Taras%20Lysyy%20and%20Rachel%20Burns%20and%20Nicole%20Guerrera%20and%20Francis%20G.%20Spinale%20and%20Jason%20A.%20Burdick%20and%20Albert%20J.%20Sinusas%20and%20James%20S.%20Duncan%0AAbstract%3A%20%20%20Cardiac%20image%20segmentation%20is%20an%20important%20step%20in%20many%20cardiac%20image%0Aanalysis%20and%20modeling%20tasks%20such%20as%20motion%20tracking%20or%20simulations%20of%20cardiac%0Amechanics.%20While%20deep%20learning%20has%20greatly%20advanced%20segmentation%20in%20clinical%0Asettings%2C%20there%20is%20limited%20work%20on%20pre-clinical%20imaging%2C%20notably%20in%20porcine%0Amodels%2C%20which%20are%20often%20used%20due%20to%20their%20anatomical%20and%20physiological%0Asimilarity%20to%20humans.%20However%2C%20differences%20between%20species%20create%20a%20domain%0Ashift%20that%20complicates%20direct%20model%20transfer%20from%20human%20to%20pig%20data.%0A%20%20Recently%2C%20foundation%20models%20trained%20on%20large%20human%20datasets%20have%20shown%0Apromise%20for%20robust%20medical%20image%20segmentation%3B%20yet%20their%20applicability%20to%0Aporcine%20data%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20investigate%20whether%0Afoundation%20models%20can%20generate%20sufficiently%20accurate%20pseudo-labels%20for%20pig%0Acardiac%20CT%20and%20propose%20a%20simple%20self-training%20approach%20to%20iteratively%20refine%0Athese%20labels.%20Our%20method%20requires%20no%20manually%20annotated%20pig%20data%2C%20relying%0Ainstead%20on%20iterative%20updates%20to%20improve%20segmentation%20quality.%20We%20demonstrate%0Athat%20this%20self-training%20process%20not%20only%20enhances%20segmentation%20accuracy%20but%0Aalso%20smooths%20out%20temporal%20inconsistencies%20across%20consecutive%20frames.%20Although%0Aour%20results%20are%20encouraging%2C%20there%20remains%20room%20for%20improvement%2C%20for%20example%20by%0Aincorporating%20more%20sophisticated%20self-training%20strategies%20and%20by%20exploring%0Aadditional%20foundation%20models%20and%20other%20cardiac%20imaging%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Foundation%2520Models%2520as%2520Pseudo-Label%2520Generators%2520for%2520Pre-Clinical%25204D%250A%2520%2520Cardiac%2520CT%2520Segmentation%26entry.906535625%3DAnne-Marie%2520Rickmann%2520and%2520Stephanie%2520L.%2520Thorn%2520and%2520Shawn%2520S.%2520Ahn%2520and%2520Supum%2520Lee%2520and%2520Selen%2520Uman%2520and%2520Taras%2520Lysyy%2520and%2520Rachel%2520Burns%2520and%2520Nicole%2520Guerrera%2520and%2520Francis%2520G.%2520Spinale%2520and%2520Jason%2520A.%2520Burdick%2520and%2520Albert%2520J.%2520Sinusas%2520and%2520James%2520S.%2520Duncan%26entry.1292438233%3D%2520%2520Cardiac%2520image%2520segmentation%2520is%2520an%2520important%2520step%2520in%2520many%2520cardiac%2520image%250Aanalysis%2520and%2520modeling%2520tasks%2520such%2520as%2520motion%2520tracking%2520or%2520simulations%2520of%2520cardiac%250Amechanics.%2520While%2520deep%2520learning%2520has%2520greatly%2520advanced%2520segmentation%2520in%2520clinical%250Asettings%252C%2520there%2520is%2520limited%2520work%2520on%2520pre-clinical%2520imaging%252C%2520notably%2520in%2520porcine%250Amodels%252C%2520which%2520are%2520often%2520used%2520due%2520to%2520their%2520anatomical%2520and%2520physiological%250Asimilarity%2520to%2520humans.%2520However%252C%2520differences%2520between%2520species%2520create%2520a%2520domain%250Ashift%2520that%2520complicates%2520direct%2520model%2520transfer%2520from%2520human%2520to%2520pig%2520data.%250A%2520%2520Recently%252C%2520foundation%2520models%2520trained%2520on%2520large%2520human%2520datasets%2520have%2520shown%250Apromise%2520for%2520robust%2520medical%2520image%2520segmentation%253B%2520yet%2520their%2520applicability%2520to%250Aporcine%2520data%2520remains%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520investigate%2520whether%250Afoundation%2520models%2520can%2520generate%2520sufficiently%2520accurate%2520pseudo-labels%2520for%2520pig%250Acardiac%2520CT%2520and%2520propose%2520a%2520simple%2520self-training%2520approach%2520to%2520iteratively%2520refine%250Athese%2520labels.%2520Our%2520method%2520requires%2520no%2520manually%2520annotated%2520pig%2520data%252C%2520relying%250Ainstead%2520on%2520iterative%2520updates%2520to%2520improve%2520segmentation%2520quality.%2520We%2520demonstrate%250Athat%2520this%2520self-training%2520process%2520not%2520only%2520enhances%2520segmentation%2520accuracy%2520but%250Aalso%2520smooths%2520out%2520temporal%2520inconsistencies%2520across%2520consecutive%2520frames.%2520Although%250Aour%2520results%2520are%2520encouraging%252C%2520there%2520remains%2520room%2520for%2520improvement%252C%2520for%2520example%2520by%250Aincorporating%2520more%2520sophisticated%2520self-training%2520strategies%2520and%2520by%2520exploring%250Aadditional%2520foundation%2520models%2520and%2520other%2520cardiac%2520imaging%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Foundation%20Models%20as%20Pseudo-Label%20Generators%20for%20Pre-Clinical%204D%0A%20%20Cardiac%20CT%20Segmentation&entry.906535625=Anne-Marie%20Rickmann%20and%20Stephanie%20L.%20Thorn%20and%20Shawn%20S.%20Ahn%20and%20Supum%20Lee%20and%20Selen%20Uman%20and%20Taras%20Lysyy%20and%20Rachel%20Burns%20and%20Nicole%20Guerrera%20and%20Francis%20G.%20Spinale%20and%20Jason%20A.%20Burdick%20and%20Albert%20J.%20Sinusas%20and%20James%20S.%20Duncan&entry.1292438233=%20%20Cardiac%20image%20segmentation%20is%20an%20important%20step%20in%20many%20cardiac%20image%0Aanalysis%20and%20modeling%20tasks%20such%20as%20motion%20tracking%20or%20simulations%20of%20cardiac%0Amechanics.%20While%20deep%20learning%20has%20greatly%20advanced%20segmentation%20in%20clinical%0Asettings%2C%20there%20is%20limited%20work%20on%20pre-clinical%20imaging%2C%20notably%20in%20porcine%0Amodels%2C%20which%20are%20often%20used%20due%20to%20their%20anatomical%20and%20physiological%0Asimilarity%20to%20humans.%20However%2C%20differences%20between%20species%20create%20a%20domain%0Ashift%20that%20complicates%20direct%20model%20transfer%20from%20human%20to%20pig%20data.%0A%20%20Recently%2C%20foundation%20models%20trained%20on%20large%20human%20datasets%20have%20shown%0Apromise%20for%20robust%20medical%20image%20segmentation%3B%20yet%20their%20applicability%20to%0Aporcine%20data%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20investigate%20whether%0Afoundation%20models%20can%20generate%20sufficiently%20accurate%20pseudo-labels%20for%20pig%0Acardiac%20CT%20and%20propose%20a%20simple%20self-training%20approach%20to%20iteratively%20refine%0Athese%20labels.%20Our%20method%20requires%20no%20manually%20annotated%20pig%20data%2C%20relying%0Ainstead%20on%20iterative%20updates%20to%20improve%20segmentation%20quality.%20We%20demonstrate%0Athat%20this%20self-training%20process%20not%20only%20enhances%20segmentation%20accuracy%20but%0Aalso%20smooths%20out%20temporal%20inconsistencies%20across%20consecutive%20frames.%20Although%0Aour%20results%20are%20encouraging%2C%20there%20remains%20room%20for%20improvement%2C%20for%20example%20by%0Aincorporating%20more%20sophisticated%20self-training%20strategies%20and%20by%20exploring%0Aadditional%20foundation%20models%20and%20other%20cardiac%20imaging%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09564v1&entry.124074799=Read"},
{"title": "Adaptively-weighted Nearest Neighbors for Matrix Completion", "author": "Tathagata Sadhukhan and Manit Paul and Raaz Dwivedi", "abstract": "  In this technical note, we introduce and analyze AWNN: an adaptively weighted\nnearest neighbor method for performing matrix completion. Nearest neighbor (NN)\nmethods are widely used in missing data problems across multiple disciplines\nsuch as in recommender systems and for performing counterfactual inference in\npanel data settings. Prior works have shown that in addition to being very\nintuitive and easy to implement, NN methods enjoy nice theoretical guarantees.\nHowever, the performance of majority of the NN methods rely on the appropriate\nchoice of the radii and the weights assigned to each member in the nearest\nneighbor set and despite several works on nearest neighbor methods in the past\ntwo decades, there does not exist a systematic approach of choosing the radii\nand the weights without relying on methods like cross-validation. AWNN\naddresses this challenge by judiciously balancing the bias variance trade off\ninherent in weighted nearest-neighbor regression. We provide theoretical\nguarantees for the proposed method under minimal assumptions and support the\ntheory via synthetic experiments.\n", "link": "http://arxiv.org/abs/2505.09612v1", "date": "2025-05-14", "relevancy": 2.0833, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4223}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4165}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptively-weighted%20Nearest%20Neighbors%20for%20Matrix%20Completion&body=Title%3A%20Adaptively-weighted%20Nearest%20Neighbors%20for%20Matrix%20Completion%0AAuthor%3A%20Tathagata%20Sadhukhan%20and%20Manit%20Paul%20and%20Raaz%20Dwivedi%0AAbstract%3A%20%20%20In%20this%20technical%20note%2C%20we%20introduce%20and%20analyze%20AWNN%3A%20an%20adaptively%20weighted%0Anearest%20neighbor%20method%20for%20performing%20matrix%20completion.%20Nearest%20neighbor%20%28NN%29%0Amethods%20are%20widely%20used%20in%20missing%20data%20problems%20across%20multiple%20disciplines%0Asuch%20as%20in%20recommender%20systems%20and%20for%20performing%20counterfactual%20inference%20in%0Apanel%20data%20settings.%20Prior%20works%20have%20shown%20that%20in%20addition%20to%20being%20very%0Aintuitive%20and%20easy%20to%20implement%2C%20NN%20methods%20enjoy%20nice%20theoretical%20guarantees.%0AHowever%2C%20the%20performance%20of%20majority%20of%20the%20NN%20methods%20rely%20on%20the%20appropriate%0Achoice%20of%20the%20radii%20and%20the%20weights%20assigned%20to%20each%20member%20in%20the%20nearest%0Aneighbor%20set%20and%20despite%20several%20works%20on%20nearest%20neighbor%20methods%20in%20the%20past%0Atwo%20decades%2C%20there%20does%20not%20exist%20a%20systematic%20approach%20of%20choosing%20the%20radii%0Aand%20the%20weights%20without%20relying%20on%20methods%20like%20cross-validation.%20AWNN%0Aaddresses%20this%20challenge%20by%20judiciously%20balancing%20the%20bias%20variance%20trade%20off%0Ainherent%20in%20weighted%20nearest-neighbor%20regression.%20We%20provide%20theoretical%0Aguarantees%20for%20the%20proposed%20method%20under%20minimal%20assumptions%20and%20support%20the%0Atheory%20via%20synthetic%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptively-weighted%2520Nearest%2520Neighbors%2520for%2520Matrix%2520Completion%26entry.906535625%3DTathagata%2520Sadhukhan%2520and%2520Manit%2520Paul%2520and%2520Raaz%2520Dwivedi%26entry.1292438233%3D%2520%2520In%2520this%2520technical%2520note%252C%2520we%2520introduce%2520and%2520analyze%2520AWNN%253A%2520an%2520adaptively%2520weighted%250Anearest%2520neighbor%2520method%2520for%2520performing%2520matrix%2520completion.%2520Nearest%2520neighbor%2520%2528NN%2529%250Amethods%2520are%2520widely%2520used%2520in%2520missing%2520data%2520problems%2520across%2520multiple%2520disciplines%250Asuch%2520as%2520in%2520recommender%2520systems%2520and%2520for%2520performing%2520counterfactual%2520inference%2520in%250Apanel%2520data%2520settings.%2520Prior%2520works%2520have%2520shown%2520that%2520in%2520addition%2520to%2520being%2520very%250Aintuitive%2520and%2520easy%2520to%2520implement%252C%2520NN%2520methods%2520enjoy%2520nice%2520theoretical%2520guarantees.%250AHowever%252C%2520the%2520performance%2520of%2520majority%2520of%2520the%2520NN%2520methods%2520rely%2520on%2520the%2520appropriate%250Achoice%2520of%2520the%2520radii%2520and%2520the%2520weights%2520assigned%2520to%2520each%2520member%2520in%2520the%2520nearest%250Aneighbor%2520set%2520and%2520despite%2520several%2520works%2520on%2520nearest%2520neighbor%2520methods%2520in%2520the%2520past%250Atwo%2520decades%252C%2520there%2520does%2520not%2520exist%2520a%2520systematic%2520approach%2520of%2520choosing%2520the%2520radii%250Aand%2520the%2520weights%2520without%2520relying%2520on%2520methods%2520like%2520cross-validation.%2520AWNN%250Aaddresses%2520this%2520challenge%2520by%2520judiciously%2520balancing%2520the%2520bias%2520variance%2520trade%2520off%250Ainherent%2520in%2520weighted%2520nearest-neighbor%2520regression.%2520We%2520provide%2520theoretical%250Aguarantees%2520for%2520the%2520proposed%2520method%2520under%2520minimal%2520assumptions%2520and%2520support%2520the%250Atheory%2520via%2520synthetic%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptively-weighted%20Nearest%20Neighbors%20for%20Matrix%20Completion&entry.906535625=Tathagata%20Sadhukhan%20and%20Manit%20Paul%20and%20Raaz%20Dwivedi&entry.1292438233=%20%20In%20this%20technical%20note%2C%20we%20introduce%20and%20analyze%20AWNN%3A%20an%20adaptively%20weighted%0Anearest%20neighbor%20method%20for%20performing%20matrix%20completion.%20Nearest%20neighbor%20%28NN%29%0Amethods%20are%20widely%20used%20in%20missing%20data%20problems%20across%20multiple%20disciplines%0Asuch%20as%20in%20recommender%20systems%20and%20for%20performing%20counterfactual%20inference%20in%0Apanel%20data%20settings.%20Prior%20works%20have%20shown%20that%20in%20addition%20to%20being%20very%0Aintuitive%20and%20easy%20to%20implement%2C%20NN%20methods%20enjoy%20nice%20theoretical%20guarantees.%0AHowever%2C%20the%20performance%20of%20majority%20of%20the%20NN%20methods%20rely%20on%20the%20appropriate%0Achoice%20of%20the%20radii%20and%20the%20weights%20assigned%20to%20each%20member%20in%20the%20nearest%0Aneighbor%20set%20and%20despite%20several%20works%20on%20nearest%20neighbor%20methods%20in%20the%20past%0Atwo%20decades%2C%20there%20does%20not%20exist%20a%20systematic%20approach%20of%20choosing%20the%20radii%0Aand%20the%20weights%20without%20relying%20on%20methods%20like%20cross-validation.%20AWNN%0Aaddresses%20this%20challenge%20by%20judiciously%20balancing%20the%20bias%20variance%20trade%20off%0Ainherent%20in%20weighted%20nearest-neighbor%20regression.%20We%20provide%20theoretical%0Aguarantees%20for%20the%20proposed%20method%20under%20minimal%20assumptions%20and%20support%20the%0Atheory%20via%20synthetic%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09612v1&entry.124074799=Read"},
{"title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives\n  in Large Language Models", "author": "Abdullah Mushtaq and Imran Taj and Rafay Naeem and Ibrahim Ghaznavi and Junaid Qadir", "abstract": "  Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems.\n", "link": "http://arxiv.org/abs/2505.09595v1", "date": "2025-05-14", "relevancy": 2.0772, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WorldView-Bench%3A%20A%20Benchmark%20for%20Evaluating%20Global%20Cultural%20Perspectives%0A%20%20in%20Large%20Language%20Models&body=Title%3A%20WorldView-Bench%3A%20A%20Benchmark%20for%20Evaluating%20Global%20Cultural%20Perspectives%0A%20%20in%20Large%20Language%20Models%0AAuthor%3A%20Abdullah%20Mushtaq%20and%20Imran%20Taj%20and%20Rafay%20Naeem%20and%20Ibrahim%20Ghaznavi%20and%20Junaid%20Qadir%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20predominantly%20trained%20and%20aligned%20in%20ways%0Athat%20reinforce%20Western-centric%20epistemologies%20and%20socio-cultural%20norms%2C%20leading%0Ato%20cultural%20homogenization%20and%20limiting%20their%20ability%20to%20reflect%20global%0Acivilizational%20plurality.%20Existing%20benchmarking%20frameworks%20fail%20to%20adequately%0Acapture%20this%20bias%2C%20as%20they%20rely%20on%20rigid%2C%20closed-form%20assessments%20that%20overlook%0Athe%20complexity%20of%20cultural%20inclusivity.%20To%20address%20this%2C%20we%20introduce%0AWorldView-Bench%2C%20a%20benchmark%20designed%20to%20evaluate%20Global%20Cultural%20Inclusivity%0A%28GCI%29%20in%20LLMs%20by%20analyzing%20their%20ability%20to%20accommodate%20diverse%20worldviews.%20Our%0Aapproach%20is%20grounded%20in%20the%20Multiplex%20Worldview%20proposed%20by%20Senturk%20et%20al.%2C%0Awhich%20distinguishes%20between%20Uniplex%20models%2C%20reinforcing%20cultural%0Ahomogenization%2C%20and%20Multiplex%20models%2C%20which%20integrate%20diverse%20perspectives.%0AWorldView-Bench%20measures%20Cultural%20Polarization%2C%20the%20exclusion%20of%20alternative%0Aperspectives%2C%20through%20free-form%20generative%20evaluation%20rather%20than%20conventional%0Acategorical%20benchmarks.%20We%20implement%20applied%20multiplexity%20through%20two%0Aintervention%20strategies%3A%20%281%29%20Contextually-Implemented%20Multiplex%20LLMs%2C%20where%0Asystem%20prompts%20embed%20multiplexity%20principles%2C%20and%20%282%29%20Multi-Agent%20System%0A%28MAS%29-Implemented%20Multiplex%20LLMs%2C%20where%20multiple%20LLM%20agents%20representing%0Adistinct%20cultural%20perspectives%20collaboratively%20generate%20responses.%20Our%20results%0Ademonstrate%20a%20significant%20increase%20in%20Perspectives%20Distribution%20Score%20%28PDS%29%0Aentropy%20from%2013%25%20at%20baseline%20to%2094%25%20with%20MAS-Implemented%20Multiplex%20LLMs%2C%0Aalongside%20a%20shift%20toward%20positive%20sentiment%20%2867.7%25%29%20and%20enhanced%20cultural%0Abalance.%20These%20findings%20highlight%20the%20potential%20of%20multiplex-aware%20AI%0Aevaluation%20in%20mitigating%20cultural%20bias%20in%20LLMs%2C%20paving%20the%20way%20for%20more%0Ainclusive%20and%20ethically%20aligned%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorldView-Bench%253A%2520A%2520Benchmark%2520for%2520Evaluating%2520Global%2520Cultural%2520Perspectives%250A%2520%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DAbdullah%2520Mushtaq%2520and%2520Imran%2520Taj%2520and%2520Rafay%2520Naeem%2520and%2520Ibrahim%2520Ghaznavi%2520and%2520Junaid%2520Qadir%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520predominantly%2520trained%2520and%2520aligned%2520in%2520ways%250Athat%2520reinforce%2520Western-centric%2520epistemologies%2520and%2520socio-cultural%2520norms%252C%2520leading%250Ato%2520cultural%2520homogenization%2520and%2520limiting%2520their%2520ability%2520to%2520reflect%2520global%250Acivilizational%2520plurality.%2520Existing%2520benchmarking%2520frameworks%2520fail%2520to%2520adequately%250Acapture%2520this%2520bias%252C%2520as%2520they%2520rely%2520on%2520rigid%252C%2520closed-form%2520assessments%2520that%2520overlook%250Athe%2520complexity%2520of%2520cultural%2520inclusivity.%2520To%2520address%2520this%252C%2520we%2520introduce%250AWorldView-Bench%252C%2520a%2520benchmark%2520designed%2520to%2520evaluate%2520Global%2520Cultural%2520Inclusivity%250A%2528GCI%2529%2520in%2520LLMs%2520by%2520analyzing%2520their%2520ability%2520to%2520accommodate%2520diverse%2520worldviews.%2520Our%250Aapproach%2520is%2520grounded%2520in%2520the%2520Multiplex%2520Worldview%2520proposed%2520by%2520Senturk%2520et%2520al.%252C%250Awhich%2520distinguishes%2520between%2520Uniplex%2520models%252C%2520reinforcing%2520cultural%250Ahomogenization%252C%2520and%2520Multiplex%2520models%252C%2520which%2520integrate%2520diverse%2520perspectives.%250AWorldView-Bench%2520measures%2520Cultural%2520Polarization%252C%2520the%2520exclusion%2520of%2520alternative%250Aperspectives%252C%2520through%2520free-form%2520generative%2520evaluation%2520rather%2520than%2520conventional%250Acategorical%2520benchmarks.%2520We%2520implement%2520applied%2520multiplexity%2520through%2520two%250Aintervention%2520strategies%253A%2520%25281%2529%2520Contextually-Implemented%2520Multiplex%2520LLMs%252C%2520where%250Asystem%2520prompts%2520embed%2520multiplexity%2520principles%252C%2520and%2520%25282%2529%2520Multi-Agent%2520System%250A%2528MAS%2529-Implemented%2520Multiplex%2520LLMs%252C%2520where%2520multiple%2520LLM%2520agents%2520representing%250Adistinct%2520cultural%2520perspectives%2520collaboratively%2520generate%2520responses.%2520Our%2520results%250Ademonstrate%2520a%2520significant%2520increase%2520in%2520Perspectives%2520Distribution%2520Score%2520%2528PDS%2529%250Aentropy%2520from%252013%2525%2520at%2520baseline%2520to%252094%2525%2520with%2520MAS-Implemented%2520Multiplex%2520LLMs%252C%250Aalongside%2520a%2520shift%2520toward%2520positive%2520sentiment%2520%252867.7%2525%2529%2520and%2520enhanced%2520cultural%250Abalance.%2520These%2520findings%2520highlight%2520the%2520potential%2520of%2520multiplex-aware%2520AI%250Aevaluation%2520in%2520mitigating%2520cultural%2520bias%2520in%2520LLMs%252C%2520paving%2520the%2520way%2520for%2520more%250Ainclusive%2520and%2520ethically%2520aligned%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorldView-Bench%3A%20A%20Benchmark%20for%20Evaluating%20Global%20Cultural%20Perspectives%0A%20%20in%20Large%20Language%20Models&entry.906535625=Abdullah%20Mushtaq%20and%20Imran%20Taj%20and%20Rafay%20Naeem%20and%20Ibrahim%20Ghaznavi%20and%20Junaid%20Qadir&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20predominantly%20trained%20and%20aligned%20in%20ways%0Athat%20reinforce%20Western-centric%20epistemologies%20and%20socio-cultural%20norms%2C%20leading%0Ato%20cultural%20homogenization%20and%20limiting%20their%20ability%20to%20reflect%20global%0Acivilizational%20plurality.%20Existing%20benchmarking%20frameworks%20fail%20to%20adequately%0Acapture%20this%20bias%2C%20as%20they%20rely%20on%20rigid%2C%20closed-form%20assessments%20that%20overlook%0Athe%20complexity%20of%20cultural%20inclusivity.%20To%20address%20this%2C%20we%20introduce%0AWorldView-Bench%2C%20a%20benchmark%20designed%20to%20evaluate%20Global%20Cultural%20Inclusivity%0A%28GCI%29%20in%20LLMs%20by%20analyzing%20their%20ability%20to%20accommodate%20diverse%20worldviews.%20Our%0Aapproach%20is%20grounded%20in%20the%20Multiplex%20Worldview%20proposed%20by%20Senturk%20et%20al.%2C%0Awhich%20distinguishes%20between%20Uniplex%20models%2C%20reinforcing%20cultural%0Ahomogenization%2C%20and%20Multiplex%20models%2C%20which%20integrate%20diverse%20perspectives.%0AWorldView-Bench%20measures%20Cultural%20Polarization%2C%20the%20exclusion%20of%20alternative%0Aperspectives%2C%20through%20free-form%20generative%20evaluation%20rather%20than%20conventional%0Acategorical%20benchmarks.%20We%20implement%20applied%20multiplexity%20through%20two%0Aintervention%20strategies%3A%20%281%29%20Contextually-Implemented%20Multiplex%20LLMs%2C%20where%0Asystem%20prompts%20embed%20multiplexity%20principles%2C%20and%20%282%29%20Multi-Agent%20System%0A%28MAS%29-Implemented%20Multiplex%20LLMs%2C%20where%20multiple%20LLM%20agents%20representing%0Adistinct%20cultural%20perspectives%20collaboratively%20generate%20responses.%20Our%20results%0Ademonstrate%20a%20significant%20increase%20in%20Perspectives%20Distribution%20Score%20%28PDS%29%0Aentropy%20from%2013%25%20at%20baseline%20to%2094%25%20with%20MAS-Implemented%20Multiplex%20LLMs%2C%0Aalongside%20a%20shift%20toward%20positive%20sentiment%20%2867.7%25%29%20and%20enhanced%20cultural%0Abalance.%20These%20findings%20highlight%20the%20potential%20of%20multiplex-aware%20AI%0Aevaluation%20in%20mitigating%20cultural%20bias%20in%20LLMs%2C%20paving%20the%20way%20for%20more%0Ainclusive%20and%20ethically%20aligned%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09595v1&entry.124074799=Read"},
{"title": "TREET: TRansfer Entropy Estimation via Transformers", "author": "Omer Luxembourg and Dor Tsur and Haim Permuter", "abstract": "  Transfer entropy (TE) is an information theoretic measure that reveals the\ndirectional flow of information between processes, providing valuable insights\nfor a wide range of real-world applications. This work proposes Transfer\nEntropy Estimation via Transformers (TREET), a novel attention-based approach\nfor estimating TE for stationary processes. The proposed approach employs\nDonsker-Varadhan representation to TE and leverages the attention mechanism for\nthe task of neural estimation. We propose a detailed theoretical and empirical\nstudy of the TREET, comparing it to existing methods on a dedicated estimation\nbenchmark. To increase its applicability, we design an estimated TE\noptimization scheme that is motivated by the functional representation lemma,\nand use it to estimate the capacity of communication channels with memory,\nwhich is a canonical optimization problem in information theory. We further\ndemonstrate how an optimized TREET can be used to estimate underlying\ndensities, providing experimental results. Finally, we apply TREET to feature\nanalysis of patients with Apnea, demonstrating its applicability to real-world\nphysiological data. Our work, applied with state-of-the-art deep learning\nmethods, opens a new door for communication problems which are yet to be\nsolved.\n", "link": "http://arxiv.org/abs/2402.06919v3", "date": "2025-05-14", "relevancy": 2.0713, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5499}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5254}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TREET%3A%20TRansfer%20Entropy%20Estimation%20via%20Transformers&body=Title%3A%20TREET%3A%20TRansfer%20Entropy%20Estimation%20via%20Transformers%0AAuthor%3A%20Omer%20Luxembourg%20and%20Dor%20Tsur%20and%20Haim%20Permuter%0AAbstract%3A%20%20%20Transfer%20entropy%20%28TE%29%20is%20an%20information%20theoretic%20measure%20that%20reveals%20the%0Adirectional%20flow%20of%20information%20between%20processes%2C%20providing%20valuable%20insights%0Afor%20a%20wide%20range%20of%20real-world%20applications.%20This%20work%20proposes%20Transfer%0AEntropy%20Estimation%20via%20Transformers%20%28TREET%29%2C%20a%20novel%20attention-based%20approach%0Afor%20estimating%20TE%20for%20stationary%20processes.%20The%20proposed%20approach%20employs%0ADonsker-Varadhan%20representation%20to%20TE%20and%20leverages%20the%20attention%20mechanism%20for%0Athe%20task%20of%20neural%20estimation.%20We%20propose%20a%20detailed%20theoretical%20and%20empirical%0Astudy%20of%20the%20TREET%2C%20comparing%20it%20to%20existing%20methods%20on%20a%20dedicated%20estimation%0Abenchmark.%20To%20increase%20its%20applicability%2C%20we%20design%20an%20estimated%20TE%0Aoptimization%20scheme%20that%20is%20motivated%20by%20the%20functional%20representation%20lemma%2C%0Aand%20use%20it%20to%20estimate%20the%20capacity%20of%20communication%20channels%20with%20memory%2C%0Awhich%20is%20a%20canonical%20optimization%20problem%20in%20information%20theory.%20We%20further%0Ademonstrate%20how%20an%20optimized%20TREET%20can%20be%20used%20to%20estimate%20underlying%0Adensities%2C%20providing%20experimental%20results.%20Finally%2C%20we%20apply%20TREET%20to%20feature%0Aanalysis%20of%20patients%20with%20Apnea%2C%20demonstrating%20its%20applicability%20to%20real-world%0Aphysiological%20data.%20Our%20work%2C%20applied%20with%20state-of-the-art%20deep%20learning%0Amethods%2C%20opens%20a%20new%20door%20for%20communication%20problems%20which%20are%20yet%20to%20be%0Asolved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06919v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTREET%253A%2520TRansfer%2520Entropy%2520Estimation%2520via%2520Transformers%26entry.906535625%3DOmer%2520Luxembourg%2520and%2520Dor%2520Tsur%2520and%2520Haim%2520Permuter%26entry.1292438233%3D%2520%2520Transfer%2520entropy%2520%2528TE%2529%2520is%2520an%2520information%2520theoretic%2520measure%2520that%2520reveals%2520the%250Adirectional%2520flow%2520of%2520information%2520between%2520processes%252C%2520providing%2520valuable%2520insights%250Afor%2520a%2520wide%2520range%2520of%2520real-world%2520applications.%2520This%2520work%2520proposes%2520Transfer%250AEntropy%2520Estimation%2520via%2520Transformers%2520%2528TREET%2529%252C%2520a%2520novel%2520attention-based%2520approach%250Afor%2520estimating%2520TE%2520for%2520stationary%2520processes.%2520The%2520proposed%2520approach%2520employs%250ADonsker-Varadhan%2520representation%2520to%2520TE%2520and%2520leverages%2520the%2520attention%2520mechanism%2520for%250Athe%2520task%2520of%2520neural%2520estimation.%2520We%2520propose%2520a%2520detailed%2520theoretical%2520and%2520empirical%250Astudy%2520of%2520the%2520TREET%252C%2520comparing%2520it%2520to%2520existing%2520methods%2520on%2520a%2520dedicated%2520estimation%250Abenchmark.%2520To%2520increase%2520its%2520applicability%252C%2520we%2520design%2520an%2520estimated%2520TE%250Aoptimization%2520scheme%2520that%2520is%2520motivated%2520by%2520the%2520functional%2520representation%2520lemma%252C%250Aand%2520use%2520it%2520to%2520estimate%2520the%2520capacity%2520of%2520communication%2520channels%2520with%2520memory%252C%250Awhich%2520is%2520a%2520canonical%2520optimization%2520problem%2520in%2520information%2520theory.%2520We%2520further%250Ademonstrate%2520how%2520an%2520optimized%2520TREET%2520can%2520be%2520used%2520to%2520estimate%2520underlying%250Adensities%252C%2520providing%2520experimental%2520results.%2520Finally%252C%2520we%2520apply%2520TREET%2520to%2520feature%250Aanalysis%2520of%2520patients%2520with%2520Apnea%252C%2520demonstrating%2520its%2520applicability%2520to%2520real-world%250Aphysiological%2520data.%2520Our%2520work%252C%2520applied%2520with%2520state-of-the-art%2520deep%2520learning%250Amethods%252C%2520opens%2520a%2520new%2520door%2520for%2520communication%2520problems%2520which%2520are%2520yet%2520to%2520be%250Asolved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06919v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TREET%3A%20TRansfer%20Entropy%20Estimation%20via%20Transformers&entry.906535625=Omer%20Luxembourg%20and%20Dor%20Tsur%20and%20Haim%20Permuter&entry.1292438233=%20%20Transfer%20entropy%20%28TE%29%20is%20an%20information%20theoretic%20measure%20that%20reveals%20the%0Adirectional%20flow%20of%20information%20between%20processes%2C%20providing%20valuable%20insights%0Afor%20a%20wide%20range%20of%20real-world%20applications.%20This%20work%20proposes%20Transfer%0AEntropy%20Estimation%20via%20Transformers%20%28TREET%29%2C%20a%20novel%20attention-based%20approach%0Afor%20estimating%20TE%20for%20stationary%20processes.%20The%20proposed%20approach%20employs%0ADonsker-Varadhan%20representation%20to%20TE%20and%20leverages%20the%20attention%20mechanism%20for%0Athe%20task%20of%20neural%20estimation.%20We%20propose%20a%20detailed%20theoretical%20and%20empirical%0Astudy%20of%20the%20TREET%2C%20comparing%20it%20to%20existing%20methods%20on%20a%20dedicated%20estimation%0Abenchmark.%20To%20increase%20its%20applicability%2C%20we%20design%20an%20estimated%20TE%0Aoptimization%20scheme%20that%20is%20motivated%20by%20the%20functional%20representation%20lemma%2C%0Aand%20use%20it%20to%20estimate%20the%20capacity%20of%20communication%20channels%20with%20memory%2C%0Awhich%20is%20a%20canonical%20optimization%20problem%20in%20information%20theory.%20We%20further%0Ademonstrate%20how%20an%20optimized%20TREET%20can%20be%20used%20to%20estimate%20underlying%0Adensities%2C%20providing%20experimental%20results.%20Finally%2C%20we%20apply%20TREET%20to%20feature%0Aanalysis%20of%20patients%20with%20Apnea%2C%20demonstrating%20its%20applicability%20to%20real-world%0Aphysiological%20data.%20Our%20work%2C%20applied%20with%20state-of-the-art%20deep%20learning%0Amethods%2C%20opens%20a%20new%20door%20for%20communication%20problems%20which%20are%20yet%20to%20be%0Asolved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06919v3&entry.124074799=Read"},
{"title": "The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan", "author": "Zhengyan Sheng and Jinghao He and Liping Chen and Kong Aik Lee and Zhen-Hua Ling", "abstract": "  Voice timbre refers to the unique quality or character of a person's voice\nthat distinguishes it from others as perceived by human hearing. The Voice\nTimbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the\nvoice timbre attribute in a comparative manner. In this challenge, the human\nimpression of voice timbre is verbalized with a set of sensory descriptors,\nincluding bright, coarse, soft, magnetic, and so on. The timbre is explained\nfrom the comparison between two voices in their intensity within a specific\ndescriptor dimension. The VtaD 2025 challenge starts in May and culminates in a\nspecial proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang,\nChina.\n", "link": "http://arxiv.org/abs/2505.09382v1", "date": "2025-05-14", "relevancy": 2.0707, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4143}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Voice%20Timbre%20Attribute%20Detection%202025%20Challenge%20Evaluation%20Plan&body=Title%3A%20The%20Voice%20Timbre%20Attribute%20Detection%202025%20Challenge%20Evaluation%20Plan%0AAuthor%3A%20Zhengyan%20Sheng%20and%20Jinghao%20He%20and%20Liping%20Chen%20and%20Kong%20Aik%20Lee%20and%20Zhen-Hua%20Ling%0AAbstract%3A%20%20%20Voice%20timbre%20refers%20to%20the%20unique%20quality%20or%20character%20of%20a%20person%27s%20voice%0Athat%20distinguishes%20it%20from%20others%20as%20perceived%20by%20human%20hearing.%20The%20Voice%0ATimbre%20Attribute%20Detection%20%28VtaD%29%202025%20challenge%20focuses%20on%20explaining%20the%0Avoice%20timbre%20attribute%20in%20a%20comparative%20manner.%20In%20this%20challenge%2C%20the%20human%0Aimpression%20of%20voice%20timbre%20is%20verbalized%20with%20a%20set%20of%20sensory%20descriptors%2C%0Aincluding%20bright%2C%20coarse%2C%20soft%2C%20magnetic%2C%20and%20so%20on.%20The%20timbre%20is%20explained%0Afrom%20the%20comparison%20between%20two%20voices%20in%20their%20intensity%20within%20a%20specific%0Adescriptor%20dimension.%20The%20VtaD%202025%20challenge%20starts%20in%20May%20and%20culminates%20in%20a%0Aspecial%20proposal%20at%20the%20NCMMSC2025%20conference%20in%20October%202025%20in%20Zhenjiang%2C%0AChina.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Voice%2520Timbre%2520Attribute%2520Detection%25202025%2520Challenge%2520Evaluation%2520Plan%26entry.906535625%3DZhengyan%2520Sheng%2520and%2520Jinghao%2520He%2520and%2520Liping%2520Chen%2520and%2520Kong%2520Aik%2520Lee%2520and%2520Zhen-Hua%2520Ling%26entry.1292438233%3D%2520%2520Voice%2520timbre%2520refers%2520to%2520the%2520unique%2520quality%2520or%2520character%2520of%2520a%2520person%2527s%2520voice%250Athat%2520distinguishes%2520it%2520from%2520others%2520as%2520perceived%2520by%2520human%2520hearing.%2520The%2520Voice%250ATimbre%2520Attribute%2520Detection%2520%2528VtaD%2529%25202025%2520challenge%2520focuses%2520on%2520explaining%2520the%250Avoice%2520timbre%2520attribute%2520in%2520a%2520comparative%2520manner.%2520In%2520this%2520challenge%252C%2520the%2520human%250Aimpression%2520of%2520voice%2520timbre%2520is%2520verbalized%2520with%2520a%2520set%2520of%2520sensory%2520descriptors%252C%250Aincluding%2520bright%252C%2520coarse%252C%2520soft%252C%2520magnetic%252C%2520and%2520so%2520on.%2520The%2520timbre%2520is%2520explained%250Afrom%2520the%2520comparison%2520between%2520two%2520voices%2520in%2520their%2520intensity%2520within%2520a%2520specific%250Adescriptor%2520dimension.%2520The%2520VtaD%25202025%2520challenge%2520starts%2520in%2520May%2520and%2520culminates%2520in%2520a%250Aspecial%2520proposal%2520at%2520the%2520NCMMSC2025%2520conference%2520in%2520October%25202025%2520in%2520Zhenjiang%252C%250AChina.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Voice%20Timbre%20Attribute%20Detection%202025%20Challenge%20Evaluation%20Plan&entry.906535625=Zhengyan%20Sheng%20and%20Jinghao%20He%20and%20Liping%20Chen%20and%20Kong%20Aik%20Lee%20and%20Zhen-Hua%20Ling&entry.1292438233=%20%20Voice%20timbre%20refers%20to%20the%20unique%20quality%20or%20character%20of%20a%20person%27s%20voice%0Athat%20distinguishes%20it%20from%20others%20as%20perceived%20by%20human%20hearing.%20The%20Voice%0ATimbre%20Attribute%20Detection%20%28VtaD%29%202025%20challenge%20focuses%20on%20explaining%20the%0Avoice%20timbre%20attribute%20in%20a%20comparative%20manner.%20In%20this%20challenge%2C%20the%20human%0Aimpression%20of%20voice%20timbre%20is%20verbalized%20with%20a%20set%20of%20sensory%20descriptors%2C%0Aincluding%20bright%2C%20coarse%2C%20soft%2C%20magnetic%2C%20and%20so%20on.%20The%20timbre%20is%20explained%0Afrom%20the%20comparison%20between%20two%20voices%20in%20their%20intensity%20within%20a%20specific%0Adescriptor%20dimension.%20The%20VtaD%202025%20challenge%20starts%20in%20May%20and%20culminates%20in%20a%0Aspecial%20proposal%20at%20the%20NCMMSC2025%20conference%20in%20October%202025%20in%20Zhenjiang%2C%0AChina.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09382v1&entry.124074799=Read"},
{"title": "CXMArena: Unified Dataset to benchmark performance in realistic CXM\n  Scenarios", "author": "Raghav Garg and Kapil Sharma and Karan Gupta", "abstract": "  Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques.\n", "link": "http://arxiv.org/abs/2505.09436v1", "date": "2025-05-14", "relevancy": 2.067, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5224}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5224}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CXMArena%3A%20Unified%20Dataset%20to%20benchmark%20performance%20in%20realistic%20CXM%0A%20%20Scenarios&body=Title%3A%20CXMArena%3A%20Unified%20Dataset%20to%20benchmark%20performance%20in%20realistic%20CXM%0A%20%20Scenarios%0AAuthor%3A%20Raghav%20Garg%20and%20Kapil%20Sharma%20and%20Karan%20Gupta%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20hold%20immense%20potential%20for%20revolutionizing%0ACustomer%20Experience%20Management%20%28CXM%29%2C%20particularly%20in%20contact%20center%0Aoperations.%20However%2C%20evaluating%20their%20practical%20utility%20in%20complex%20operational%0Aenvironments%20is%20hindered%20by%20data%20scarcity%20%28due%20to%20privacy%20concerns%29%20and%20the%0Alimitations%20of%20current%20benchmarks.%20Existing%20benchmarks%20often%20lack%20realism%2C%0Afailing%20to%20incorporate%20deep%20knowledge%20base%20%28KB%29%20integration%2C%20real-world%20noise%2C%0Aor%20critical%20operational%20tasks%20beyond%20conversational%20fluency.%20To%20bridge%20this%0Agap%2C%20we%20introduce%20CXMArena%2C%20a%20novel%2C%20large-scale%20synthetic%20benchmark%20dataset%0Aspecifically%20designed%20for%20evaluating%20AI%20in%20operational%20CXM%20contexts.%20Given%20the%0Adiversity%20in%20possible%20contact%20center%20features%2C%20we%20have%20developed%20a%20scalable%0ALLM-powered%20pipeline%20that%20simulates%20the%20brand%27s%20CXM%20entities%20that%20form%20the%0Afoundation%20of%20our%20datasets-such%20as%20knowledge%20articles%20including%20product%0Aspecifications%2C%20issue%20taxonomies%2C%20and%20contact%20center%20conversations.%20The%0Aentities%20closely%20represent%20real-world%20distribution%20because%20of%20controlled%20noise%0Ainjection%20%28informed%20by%20domain%20experts%29%20and%20rigorous%20automated%20validation.%0ABuilding%20on%20this%2C%20we%20release%20CXMArena%2C%20which%20provides%20dedicated%20benchmarks%0Atargeting%20five%20important%20operational%20tasks%3A%20Knowledge%20Base%20Refinement%2C%20Intent%0APrediction%2C%20Agent%20Quality%20Adherence%2C%20Article%20Search%2C%20and%20Multi-turn%20RAG%20with%0AIntegrated%20Tools.%20Our%20baseline%20experiments%20underscore%20the%20benchmark%27s%0Adifficulty%3A%20even%20state%20of%20the%20art%20embedding%20and%20generation%20models%20achieve%20only%0A68%25%20accuracy%20on%20article%20search%2C%20while%20standard%20embedding%20methods%20yield%20a%20low%20F1%0Ascore%20of%200.3%20for%20knowledge%20base%20refinement%2C%20highlighting%20significant%20challenges%0Afor%20current%20models%20necessitating%20complex%20pipelines%20and%20solutions%20over%0Aconventional%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCXMArena%253A%2520Unified%2520Dataset%2520to%2520benchmark%2520performance%2520in%2520realistic%2520CXM%250A%2520%2520Scenarios%26entry.906535625%3DRaghav%2520Garg%2520and%2520Kapil%2520Sharma%2520and%2520Karan%2520Gupta%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520hold%2520immense%2520potential%2520for%2520revolutionizing%250ACustomer%2520Experience%2520Management%2520%2528CXM%2529%252C%2520particularly%2520in%2520contact%2520center%250Aoperations.%2520However%252C%2520evaluating%2520their%2520practical%2520utility%2520in%2520complex%2520operational%250Aenvironments%2520is%2520hindered%2520by%2520data%2520scarcity%2520%2528due%2520to%2520privacy%2520concerns%2529%2520and%2520the%250Alimitations%2520of%2520current%2520benchmarks.%2520Existing%2520benchmarks%2520often%2520lack%2520realism%252C%250Afailing%2520to%2520incorporate%2520deep%2520knowledge%2520base%2520%2528KB%2529%2520integration%252C%2520real-world%2520noise%252C%250Aor%2520critical%2520operational%2520tasks%2520beyond%2520conversational%2520fluency.%2520To%2520bridge%2520this%250Agap%252C%2520we%2520introduce%2520CXMArena%252C%2520a%2520novel%252C%2520large-scale%2520synthetic%2520benchmark%2520dataset%250Aspecifically%2520designed%2520for%2520evaluating%2520AI%2520in%2520operational%2520CXM%2520contexts.%2520Given%2520the%250Adiversity%2520in%2520possible%2520contact%2520center%2520features%252C%2520we%2520have%2520developed%2520a%2520scalable%250ALLM-powered%2520pipeline%2520that%2520simulates%2520the%2520brand%2527s%2520CXM%2520entities%2520that%2520form%2520the%250Afoundation%2520of%2520our%2520datasets-such%2520as%2520knowledge%2520articles%2520including%2520product%250Aspecifications%252C%2520issue%2520taxonomies%252C%2520and%2520contact%2520center%2520conversations.%2520The%250Aentities%2520closely%2520represent%2520real-world%2520distribution%2520because%2520of%2520controlled%2520noise%250Ainjection%2520%2528informed%2520by%2520domain%2520experts%2529%2520and%2520rigorous%2520automated%2520validation.%250ABuilding%2520on%2520this%252C%2520we%2520release%2520CXMArena%252C%2520which%2520provides%2520dedicated%2520benchmarks%250Atargeting%2520five%2520important%2520operational%2520tasks%253A%2520Knowledge%2520Base%2520Refinement%252C%2520Intent%250APrediction%252C%2520Agent%2520Quality%2520Adherence%252C%2520Article%2520Search%252C%2520and%2520Multi-turn%2520RAG%2520with%250AIntegrated%2520Tools.%2520Our%2520baseline%2520experiments%2520underscore%2520the%2520benchmark%2527s%250Adifficulty%253A%2520even%2520state%2520of%2520the%2520art%2520embedding%2520and%2520generation%2520models%2520achieve%2520only%250A68%2525%2520accuracy%2520on%2520article%2520search%252C%2520while%2520standard%2520embedding%2520methods%2520yield%2520a%2520low%2520F1%250Ascore%2520of%25200.3%2520for%2520knowledge%2520base%2520refinement%252C%2520highlighting%2520significant%2520challenges%250Afor%2520current%2520models%2520necessitating%2520complex%2520pipelines%2520and%2520solutions%2520over%250Aconventional%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CXMArena%3A%20Unified%20Dataset%20to%20benchmark%20performance%20in%20realistic%20CXM%0A%20%20Scenarios&entry.906535625=Raghav%20Garg%20and%20Kapil%20Sharma%20and%20Karan%20Gupta&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20hold%20immense%20potential%20for%20revolutionizing%0ACustomer%20Experience%20Management%20%28CXM%29%2C%20particularly%20in%20contact%20center%0Aoperations.%20However%2C%20evaluating%20their%20practical%20utility%20in%20complex%20operational%0Aenvironments%20is%20hindered%20by%20data%20scarcity%20%28due%20to%20privacy%20concerns%29%20and%20the%0Alimitations%20of%20current%20benchmarks.%20Existing%20benchmarks%20often%20lack%20realism%2C%0Afailing%20to%20incorporate%20deep%20knowledge%20base%20%28KB%29%20integration%2C%20real-world%20noise%2C%0Aor%20critical%20operational%20tasks%20beyond%20conversational%20fluency.%20To%20bridge%20this%0Agap%2C%20we%20introduce%20CXMArena%2C%20a%20novel%2C%20large-scale%20synthetic%20benchmark%20dataset%0Aspecifically%20designed%20for%20evaluating%20AI%20in%20operational%20CXM%20contexts.%20Given%20the%0Adiversity%20in%20possible%20contact%20center%20features%2C%20we%20have%20developed%20a%20scalable%0ALLM-powered%20pipeline%20that%20simulates%20the%20brand%27s%20CXM%20entities%20that%20form%20the%0Afoundation%20of%20our%20datasets-such%20as%20knowledge%20articles%20including%20product%0Aspecifications%2C%20issue%20taxonomies%2C%20and%20contact%20center%20conversations.%20The%0Aentities%20closely%20represent%20real-world%20distribution%20because%20of%20controlled%20noise%0Ainjection%20%28informed%20by%20domain%20experts%29%20and%20rigorous%20automated%20validation.%0ABuilding%20on%20this%2C%20we%20release%20CXMArena%2C%20which%20provides%20dedicated%20benchmarks%0Atargeting%20five%20important%20operational%20tasks%3A%20Knowledge%20Base%20Refinement%2C%20Intent%0APrediction%2C%20Agent%20Quality%20Adherence%2C%20Article%20Search%2C%20and%20Multi-turn%20RAG%20with%0AIntegrated%20Tools.%20Our%20baseline%20experiments%20underscore%20the%20benchmark%27s%0Adifficulty%3A%20even%20state%20of%20the%20art%20embedding%20and%20generation%20models%20achieve%20only%0A68%25%20accuracy%20on%20article%20search%2C%20while%20standard%20embedding%20methods%20yield%20a%20low%20F1%0Ascore%20of%200.3%20for%20knowledge%20base%20refinement%2C%20highlighting%20significant%20challenges%0Afor%20current%20models%20necessitating%20complex%20pipelines%20and%20solutions%20over%0Aconventional%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09436v1&entry.124074799=Read"},
{"title": "How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of\n  LLM Inference", "author": "Nidhal Jegham and Marwen Abdelatti and Lassad Elmoubarki and Abdeltawab Hendawi", "abstract": "  As large language models (LLMs) spread across industries, understanding their\nenvironmental footprint at the inference level is no longer optional; it is\nessential. However, most existing studies exclude proprietary models, overlook\ninfrastructural variability and overhead, or focus solely on training, even as\ninference increasingly dominates AI's environmental impact. To bridge this gap,\nthis paper introduces a novel infrastructure-aware benchmarking framework for\nquantifying the environmental footprint of LLM inference across 30\nstate-of-the-art models as deployed in commercial data centers. Our framework\ncombines public API performance data with region-specific environmental\nmultipliers and statistical inference of hardware configurations. We\nadditionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank\nmodels by performance relative to environmental cost. Our results show that o3\nand DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33\nWh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and\nthat Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short\nGPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results\nin substantial annual environmental impacts. These include electricity use\ncomparable to 35,000 U.S. homes, freshwater evaporation matching the annual\ndrinking needs of 1.2 million people, and carbon emissions requiring a\nChicago-sized forest to offset. These findings illustrate a growing paradox:\nalthough individual queries are efficient, their global scale drives\ndisproportionate resource consumption. Our study provides a standardized,\nempirically grounded methodology for benchmarking the sustainability of LLM\ndeployments, laying a foundation for future environmental accountability in AI\ndevelopment and sustainability standards.\n", "link": "http://arxiv.org/abs/2505.09598v1", "date": "2025-05-14", "relevancy": 2.0668, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Hungry%20is%20AI%3F%20Benchmarking%20Energy%2C%20Water%2C%20and%20Carbon%20Footprint%20of%0A%20%20LLM%20Inference&body=Title%3A%20How%20Hungry%20is%20AI%3F%20Benchmarking%20Energy%2C%20Water%2C%20and%20Carbon%20Footprint%20of%0A%20%20LLM%20Inference%0AAuthor%3A%20Nidhal%20Jegham%20and%20Marwen%20Abdelatti%20and%20Lassad%20Elmoubarki%20and%20Abdeltawab%20Hendawi%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20spread%20across%20industries%2C%20understanding%20their%0Aenvironmental%20footprint%20at%20the%20inference%20level%20is%20no%20longer%20optional%3B%20it%20is%0Aessential.%20However%2C%20most%20existing%20studies%20exclude%20proprietary%20models%2C%20overlook%0Ainfrastructural%20variability%20and%20overhead%2C%20or%20focus%20solely%20on%20training%2C%20even%20as%0Ainference%20increasingly%20dominates%20AI%27s%20environmental%20impact.%20To%20bridge%20this%20gap%2C%0Athis%20paper%20introduces%20a%20novel%20infrastructure-aware%20benchmarking%20framework%20for%0Aquantifying%20the%20environmental%20footprint%20of%20LLM%20inference%20across%2030%0Astate-of-the-art%20models%20as%20deployed%20in%20commercial%20data%20centers.%20Our%20framework%0Acombines%20public%20API%20performance%20data%20with%20region-specific%20environmental%0Amultipliers%20and%20statistical%20inference%20of%20hardware%20configurations.%20We%0Aadditionally%20utilize%20cross-efficiency%20Data%20Envelopment%20Analysis%20%28DEA%29%20to%20rank%0Amodels%20by%20performance%20relative%20to%20environmental%20cost.%20Our%20results%20show%20that%20o3%0Aand%20DeepSeek-R1%20emerge%20as%20the%20most%20energy-intensive%20models%2C%20consuming%20over%2033%0AWh%20per%20long%20prompt%2C%20more%20than%2070%20times%20the%20consumption%20of%20GPT-4.1%20nano%2C%20and%0Athat%20Claude-3.7%20Sonnet%20ranks%20highest%20in%20eco-efficiency.%20While%20a%20single%20short%0AGPT-4o%20query%20consumes%200.43%20Wh%2C%20scaling%20this%20to%20700%20million%20queries/day%20results%0Ain%20substantial%20annual%20environmental%20impacts.%20These%20include%20electricity%20use%0Acomparable%20to%2035%2C000%20U.S.%20homes%2C%20freshwater%20evaporation%20matching%20the%20annual%0Adrinking%20needs%20of%201.2%20million%20people%2C%20and%20carbon%20emissions%20requiring%20a%0AChicago-sized%20forest%20to%20offset.%20These%20findings%20illustrate%20a%20growing%20paradox%3A%0Aalthough%20individual%20queries%20are%20efficient%2C%20their%20global%20scale%20drives%0Adisproportionate%20resource%20consumption.%20Our%20study%20provides%20a%20standardized%2C%0Aempirically%20grounded%20methodology%20for%20benchmarking%20the%20sustainability%20of%20LLM%0Adeployments%2C%20laying%20a%20foundation%20for%20future%20environmental%20accountability%20in%20AI%0Adevelopment%20and%20sustainability%20standards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Hungry%2520is%2520AI%253F%2520Benchmarking%2520Energy%252C%2520Water%252C%2520and%2520Carbon%2520Footprint%2520of%250A%2520%2520LLM%2520Inference%26entry.906535625%3DNidhal%2520Jegham%2520and%2520Marwen%2520Abdelatti%2520and%2520Lassad%2520Elmoubarki%2520and%2520Abdeltawab%2520Hendawi%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520spread%2520across%2520industries%252C%2520understanding%2520their%250Aenvironmental%2520footprint%2520at%2520the%2520inference%2520level%2520is%2520no%2520longer%2520optional%253B%2520it%2520is%250Aessential.%2520However%252C%2520most%2520existing%2520studies%2520exclude%2520proprietary%2520models%252C%2520overlook%250Ainfrastructural%2520variability%2520and%2520overhead%252C%2520or%2520focus%2520solely%2520on%2520training%252C%2520even%2520as%250Ainference%2520increasingly%2520dominates%2520AI%2527s%2520environmental%2520impact.%2520To%2520bridge%2520this%2520gap%252C%250Athis%2520paper%2520introduces%2520a%2520novel%2520infrastructure-aware%2520benchmarking%2520framework%2520for%250Aquantifying%2520the%2520environmental%2520footprint%2520of%2520LLM%2520inference%2520across%252030%250Astate-of-the-art%2520models%2520as%2520deployed%2520in%2520commercial%2520data%2520centers.%2520Our%2520framework%250Acombines%2520public%2520API%2520performance%2520data%2520with%2520region-specific%2520environmental%250Amultipliers%2520and%2520statistical%2520inference%2520of%2520hardware%2520configurations.%2520We%250Aadditionally%2520utilize%2520cross-efficiency%2520Data%2520Envelopment%2520Analysis%2520%2528DEA%2529%2520to%2520rank%250Amodels%2520by%2520performance%2520relative%2520to%2520environmental%2520cost.%2520Our%2520results%2520show%2520that%2520o3%250Aand%2520DeepSeek-R1%2520emerge%2520as%2520the%2520most%2520energy-intensive%2520models%252C%2520consuming%2520over%252033%250AWh%2520per%2520long%2520prompt%252C%2520more%2520than%252070%2520times%2520the%2520consumption%2520of%2520GPT-4.1%2520nano%252C%2520and%250Athat%2520Claude-3.7%2520Sonnet%2520ranks%2520highest%2520in%2520eco-efficiency.%2520While%2520a%2520single%2520short%250AGPT-4o%2520query%2520consumes%25200.43%2520Wh%252C%2520scaling%2520this%2520to%2520700%2520million%2520queries/day%2520results%250Ain%2520substantial%2520annual%2520environmental%2520impacts.%2520These%2520include%2520electricity%2520use%250Acomparable%2520to%252035%252C000%2520U.S.%2520homes%252C%2520freshwater%2520evaporation%2520matching%2520the%2520annual%250Adrinking%2520needs%2520of%25201.2%2520million%2520people%252C%2520and%2520carbon%2520emissions%2520requiring%2520a%250AChicago-sized%2520forest%2520to%2520offset.%2520These%2520findings%2520illustrate%2520a%2520growing%2520paradox%253A%250Aalthough%2520individual%2520queries%2520are%2520efficient%252C%2520their%2520global%2520scale%2520drives%250Adisproportionate%2520resource%2520consumption.%2520Our%2520study%2520provides%2520a%2520standardized%252C%250Aempirically%2520grounded%2520methodology%2520for%2520benchmarking%2520the%2520sustainability%2520of%2520LLM%250Adeployments%252C%2520laying%2520a%2520foundation%2520for%2520future%2520environmental%2520accountability%2520in%2520AI%250Adevelopment%2520and%2520sustainability%2520standards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Hungry%20is%20AI%3F%20Benchmarking%20Energy%2C%20Water%2C%20and%20Carbon%20Footprint%20of%0A%20%20LLM%20Inference&entry.906535625=Nidhal%20Jegham%20and%20Marwen%20Abdelatti%20and%20Lassad%20Elmoubarki%20and%20Abdeltawab%20Hendawi&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20spread%20across%20industries%2C%20understanding%20their%0Aenvironmental%20footprint%20at%20the%20inference%20level%20is%20no%20longer%20optional%3B%20it%20is%0Aessential.%20However%2C%20most%20existing%20studies%20exclude%20proprietary%20models%2C%20overlook%0Ainfrastructural%20variability%20and%20overhead%2C%20or%20focus%20solely%20on%20training%2C%20even%20as%0Ainference%20increasingly%20dominates%20AI%27s%20environmental%20impact.%20To%20bridge%20this%20gap%2C%0Athis%20paper%20introduces%20a%20novel%20infrastructure-aware%20benchmarking%20framework%20for%0Aquantifying%20the%20environmental%20footprint%20of%20LLM%20inference%20across%2030%0Astate-of-the-art%20models%20as%20deployed%20in%20commercial%20data%20centers.%20Our%20framework%0Acombines%20public%20API%20performance%20data%20with%20region-specific%20environmental%0Amultipliers%20and%20statistical%20inference%20of%20hardware%20configurations.%20We%0Aadditionally%20utilize%20cross-efficiency%20Data%20Envelopment%20Analysis%20%28DEA%29%20to%20rank%0Amodels%20by%20performance%20relative%20to%20environmental%20cost.%20Our%20results%20show%20that%20o3%0Aand%20DeepSeek-R1%20emerge%20as%20the%20most%20energy-intensive%20models%2C%20consuming%20over%2033%0AWh%20per%20long%20prompt%2C%20more%20than%2070%20times%20the%20consumption%20of%20GPT-4.1%20nano%2C%20and%0Athat%20Claude-3.7%20Sonnet%20ranks%20highest%20in%20eco-efficiency.%20While%20a%20single%20short%0AGPT-4o%20query%20consumes%200.43%20Wh%2C%20scaling%20this%20to%20700%20million%20queries/day%20results%0Ain%20substantial%20annual%20environmental%20impacts.%20These%20include%20electricity%20use%0Acomparable%20to%2035%2C000%20U.S.%20homes%2C%20freshwater%20evaporation%20matching%20the%20annual%0Adrinking%20needs%20of%201.2%20million%20people%2C%20and%20carbon%20emissions%20requiring%20a%0AChicago-sized%20forest%20to%20offset.%20These%20findings%20illustrate%20a%20growing%20paradox%3A%0Aalthough%20individual%20queries%20are%20efficient%2C%20their%20global%20scale%20drives%0Adisproportionate%20resource%20consumption.%20Our%20study%20provides%20a%20standardized%2C%0Aempirically%20grounded%20methodology%20for%20benchmarking%20the%20sustainability%20of%20LLM%0Adeployments%2C%20laying%20a%20foundation%20for%20future%20environmental%20accountability%20in%20AI%0Adevelopment%20and%20sustainability%20standards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09598v1&entry.124074799=Read"},
{"title": "Recent Advances in Medical Imaging Segmentation: A Survey", "author": "Fares Bougourzi and Abdenour Hadid", "abstract": "  Medical imaging is a cornerstone of modern healthcare, driving advancements\nin diagnosis, treatment planning, and patient care. Among its various tasks,\nsegmentation remains one of the most challenging problem due to factors such as\ndata accessibility, annotation complexity, structural variability, variation in\nmedical imaging modalities, and privacy constraints. Despite recent progress,\nachieving robust generalization and domain adaptation remains a significant\nhurdle, particularly given the resource-intensive nature of some proposed\nmodels and their reliance on domain expertise. This survey explores\ncutting-edge advancements in medical image segmentation, focusing on\nmethodologies such as Generative AI, Few-Shot Learning, Foundation Models, and\nUniversal Models. These approaches offer promising solutions to longstanding\nchallenges. We provide a comprehensive overview of the theoretical foundations,\nstate-of-the-art techniques, and recent applications of these methods. Finally,\nwe discuss inherent limitations, unresolved issues, and future research\ndirections aimed at enhancing the practicality and accessibility of\nsegmentation models in medical imaging. We are maintaining a\n\\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub\nRepository} to continue tracking and updating innovations in this field.\n", "link": "http://arxiv.org/abs/2505.09274v1", "date": "2025-05-14", "relevancy": 2.0626, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5222}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5169}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recent%20Advances%20in%20Medical%20Imaging%20Segmentation%3A%20A%20Survey&body=Title%3A%20Recent%20Advances%20in%20Medical%20Imaging%20Segmentation%3A%20A%20Survey%0AAuthor%3A%20Fares%20Bougourzi%20and%20Abdenour%20Hadid%0AAbstract%3A%20%20%20Medical%20imaging%20is%20a%20cornerstone%20of%20modern%20healthcare%2C%20driving%20advancements%0Ain%20diagnosis%2C%20treatment%20planning%2C%20and%20patient%20care.%20Among%20its%20various%20tasks%2C%0Asegmentation%20remains%20one%20of%20the%20most%20challenging%20problem%20due%20to%20factors%20such%20as%0Adata%20accessibility%2C%20annotation%20complexity%2C%20structural%20variability%2C%20variation%20in%0Amedical%20imaging%20modalities%2C%20and%20privacy%20constraints.%20Despite%20recent%20progress%2C%0Aachieving%20robust%20generalization%20and%20domain%20adaptation%20remains%20a%20significant%0Ahurdle%2C%20particularly%20given%20the%20resource-intensive%20nature%20of%20some%20proposed%0Amodels%20and%20their%20reliance%20on%20domain%20expertise.%20This%20survey%20explores%0Acutting-edge%20advancements%20in%20medical%20image%20segmentation%2C%20focusing%20on%0Amethodologies%20such%20as%20Generative%20AI%2C%20Few-Shot%20Learning%2C%20Foundation%20Models%2C%20and%0AUniversal%20Models.%20These%20approaches%20offer%20promising%20solutions%20to%20longstanding%0Achallenges.%20We%20provide%20a%20comprehensive%20overview%20of%20the%20theoretical%20foundations%2C%0Astate-of-the-art%20techniques%2C%20and%20recent%20applications%20of%20these%20methods.%20Finally%2C%0Awe%20discuss%20inherent%20limitations%2C%20unresolved%20issues%2C%20and%20future%20research%0Adirections%20aimed%20at%20enhancing%20the%20practicality%20and%20accessibility%20of%0Asegmentation%20models%20in%20medical%20imaging.%20We%20are%20maintaining%20a%0A%5Chref%7Bhttps%3A//github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation%7D%7BGitHub%0ARepository%7D%20to%20continue%20tracking%20and%20updating%20innovations%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecent%2520Advances%2520in%2520Medical%2520Imaging%2520Segmentation%253A%2520A%2520Survey%26entry.906535625%3DFares%2520Bougourzi%2520and%2520Abdenour%2520Hadid%26entry.1292438233%3D%2520%2520Medical%2520imaging%2520is%2520a%2520cornerstone%2520of%2520modern%2520healthcare%252C%2520driving%2520advancements%250Ain%2520diagnosis%252C%2520treatment%2520planning%252C%2520and%2520patient%2520care.%2520Among%2520its%2520various%2520tasks%252C%250Asegmentation%2520remains%2520one%2520of%2520the%2520most%2520challenging%2520problem%2520due%2520to%2520factors%2520such%2520as%250Adata%2520accessibility%252C%2520annotation%2520complexity%252C%2520structural%2520variability%252C%2520variation%2520in%250Amedical%2520imaging%2520modalities%252C%2520and%2520privacy%2520constraints.%2520Despite%2520recent%2520progress%252C%250Aachieving%2520robust%2520generalization%2520and%2520domain%2520adaptation%2520remains%2520a%2520significant%250Ahurdle%252C%2520particularly%2520given%2520the%2520resource-intensive%2520nature%2520of%2520some%2520proposed%250Amodels%2520and%2520their%2520reliance%2520on%2520domain%2520expertise.%2520This%2520survey%2520explores%250Acutting-edge%2520advancements%2520in%2520medical%2520image%2520segmentation%252C%2520focusing%2520on%250Amethodologies%2520such%2520as%2520Generative%2520AI%252C%2520Few-Shot%2520Learning%252C%2520Foundation%2520Models%252C%2520and%250AUniversal%2520Models.%2520These%2520approaches%2520offer%2520promising%2520solutions%2520to%2520longstanding%250Achallenges.%2520We%2520provide%2520a%2520comprehensive%2520overview%2520of%2520the%2520theoretical%2520foundations%252C%250Astate-of-the-art%2520techniques%252C%2520and%2520recent%2520applications%2520of%2520these%2520methods.%2520Finally%252C%250Awe%2520discuss%2520inherent%2520limitations%252C%2520unresolved%2520issues%252C%2520and%2520future%2520research%250Adirections%2520aimed%2520at%2520enhancing%2520the%2520practicality%2520and%2520accessibility%2520of%250Asegmentation%2520models%2520in%2520medical%2520imaging.%2520We%2520are%2520maintaining%2520a%250A%255Chref%257Bhttps%253A//github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation%257D%257BGitHub%250ARepository%257D%2520to%2520continue%2520tracking%2520and%2520updating%2520innovations%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recent%20Advances%20in%20Medical%20Imaging%20Segmentation%3A%20A%20Survey&entry.906535625=Fares%20Bougourzi%20and%20Abdenour%20Hadid&entry.1292438233=%20%20Medical%20imaging%20is%20a%20cornerstone%20of%20modern%20healthcare%2C%20driving%20advancements%0Ain%20diagnosis%2C%20treatment%20planning%2C%20and%20patient%20care.%20Among%20its%20various%20tasks%2C%0Asegmentation%20remains%20one%20of%20the%20most%20challenging%20problem%20due%20to%20factors%20such%20as%0Adata%20accessibility%2C%20annotation%20complexity%2C%20structural%20variability%2C%20variation%20in%0Amedical%20imaging%20modalities%2C%20and%20privacy%20constraints.%20Despite%20recent%20progress%2C%0Aachieving%20robust%20generalization%20and%20domain%20adaptation%20remains%20a%20significant%0Ahurdle%2C%20particularly%20given%20the%20resource-intensive%20nature%20of%20some%20proposed%0Amodels%20and%20their%20reliance%20on%20domain%20expertise.%20This%20survey%20explores%0Acutting-edge%20advancements%20in%20medical%20image%20segmentation%2C%20focusing%20on%0Amethodologies%20such%20as%20Generative%20AI%2C%20Few-Shot%20Learning%2C%20Foundation%20Models%2C%20and%0AUniversal%20Models.%20These%20approaches%20offer%20promising%20solutions%20to%20longstanding%0Achallenges.%20We%20provide%20a%20comprehensive%20overview%20of%20the%20theoretical%20foundations%2C%0Astate-of-the-art%20techniques%2C%20and%20recent%20applications%20of%20these%20methods.%20Finally%2C%0Awe%20discuss%20inherent%20limitations%2C%20unresolved%20issues%2C%20and%20future%20research%0Adirections%20aimed%20at%20enhancing%20the%20practicality%20and%20accessibility%20of%0Asegmentation%20models%20in%20medical%20imaging.%20We%20are%20maintaining%20a%0A%5Chref%7Bhttps%3A//github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation%7D%7BGitHub%0ARepository%7D%20to%20continue%20tracking%20and%20updating%20innovations%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09274v1&entry.124074799=Read"},
{"title": "Public Constitutional AI", "author": "Gilad Abiri", "abstract": "  We are increasingly subjected to the power of AI authorities. As AI decisions\nbecome inescapable, entering domains such as healthcare, education, and law, we\nmust confront a vital question: how can we ensure AI systems have the\nlegitimacy necessary for effective governance? This essay argues that to secure\nAI legitimacy, we need methods that engage the public in designing and\nconstraining AI systems, ensuring these technologies reflect the community's\nshared values. Constitutional AI, proposed by Anthropic, represents a step\ntowards this goal, offering a model for democratic control of AI. However,\nwhile Constitutional AI's commitment to hardcoding explicit principles into AI\nmodels enhances transparency and accountability, it falls short in two crucial\naspects: addressing the opacity of individual AI decisions and fostering\ngenuine democratic legitimacy. To overcome these limitations, this essay\nproposes \"Public Constitutional AI.\" This approach envisions a participatory\nprocess where diverse stakeholders, including ordinary citizens, deliberate on\nthe principles guiding AI development. The resulting \"AI Constitution\" would\ncarry the legitimacy of popular authorship, grounding AI governance in the\npublic will. Furthermore, the essay proposes \"AI Courts\" to develop \"AI case\nlaw,\" providing concrete examples for operationalizing constitutional\nprinciples in AI training. This evolving combination of constitutional\nprinciples and case law aims to make AI governance more responsive to public\nvalues. By grounding AI governance in deliberative democratic processes, Public\nConstitutional AI offers a path to imbue automated authorities with genuine\ndemocratic legitimacy, addressing the unique challenges posed by increasingly\npowerful AI systems while ensuring their alignment with the public interest.\n", "link": "http://arxiv.org/abs/2406.16696v2", "date": "2025-05-14", "relevancy": 2.0591, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4216}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4212}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Public%20Constitutional%20AI&body=Title%3A%20Public%20Constitutional%20AI%0AAuthor%3A%20Gilad%20Abiri%0AAbstract%3A%20%20%20We%20are%20increasingly%20subjected%20to%20the%20power%20of%20AI%20authorities.%20As%20AI%20decisions%0Abecome%20inescapable%2C%20entering%20domains%20such%20as%20healthcare%2C%20education%2C%20and%20law%2C%20we%0Amust%20confront%20a%20vital%20question%3A%20how%20can%20we%20ensure%20AI%20systems%20have%20the%0Alegitimacy%20necessary%20for%20effective%20governance%3F%20This%20essay%20argues%20that%20to%20secure%0AAI%20legitimacy%2C%20we%20need%20methods%20that%20engage%20the%20public%20in%20designing%20and%0Aconstraining%20AI%20systems%2C%20ensuring%20these%20technologies%20reflect%20the%20community%27s%0Ashared%20values.%20Constitutional%20AI%2C%20proposed%20by%20Anthropic%2C%20represents%20a%20step%0Atowards%20this%20goal%2C%20offering%20a%20model%20for%20democratic%20control%20of%20AI.%20However%2C%0Awhile%20Constitutional%20AI%27s%20commitment%20to%20hardcoding%20explicit%20principles%20into%20AI%0Amodels%20enhances%20transparency%20and%20accountability%2C%20it%20falls%20short%20in%20two%20crucial%0Aaspects%3A%20addressing%20the%20opacity%20of%20individual%20AI%20decisions%20and%20fostering%0Agenuine%20democratic%20legitimacy.%20To%20overcome%20these%20limitations%2C%20this%20essay%0Aproposes%20%22Public%20Constitutional%20AI.%22%20This%20approach%20envisions%20a%20participatory%0Aprocess%20where%20diverse%20stakeholders%2C%20including%20ordinary%20citizens%2C%20deliberate%20on%0Athe%20principles%20guiding%20AI%20development.%20The%20resulting%20%22AI%20Constitution%22%20would%0Acarry%20the%20legitimacy%20of%20popular%20authorship%2C%20grounding%20AI%20governance%20in%20the%0Apublic%20will.%20Furthermore%2C%20the%20essay%20proposes%20%22AI%20Courts%22%20to%20develop%20%22AI%20case%0Alaw%2C%22%20providing%20concrete%20examples%20for%20operationalizing%20constitutional%0Aprinciples%20in%20AI%20training.%20This%20evolving%20combination%20of%20constitutional%0Aprinciples%20and%20case%20law%20aims%20to%20make%20AI%20governance%20more%20responsive%20to%20public%0Avalues.%20By%20grounding%20AI%20governance%20in%20deliberative%20democratic%20processes%2C%20Public%0AConstitutional%20AI%20offers%20a%20path%20to%20imbue%20automated%20authorities%20with%20genuine%0Ademocratic%20legitimacy%2C%20addressing%20the%20unique%20challenges%20posed%20by%20increasingly%0Apowerful%20AI%20systems%20while%20ensuring%20their%20alignment%20with%20the%20public%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16696v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPublic%2520Constitutional%2520AI%26entry.906535625%3DGilad%2520Abiri%26entry.1292438233%3D%2520%2520We%2520are%2520increasingly%2520subjected%2520to%2520the%2520power%2520of%2520AI%2520authorities.%2520As%2520AI%2520decisions%250Abecome%2520inescapable%252C%2520entering%2520domains%2520such%2520as%2520healthcare%252C%2520education%252C%2520and%2520law%252C%2520we%250Amust%2520confront%2520a%2520vital%2520question%253A%2520how%2520can%2520we%2520ensure%2520AI%2520systems%2520have%2520the%250Alegitimacy%2520necessary%2520for%2520effective%2520governance%253F%2520This%2520essay%2520argues%2520that%2520to%2520secure%250AAI%2520legitimacy%252C%2520we%2520need%2520methods%2520that%2520engage%2520the%2520public%2520in%2520designing%2520and%250Aconstraining%2520AI%2520systems%252C%2520ensuring%2520these%2520technologies%2520reflect%2520the%2520community%2527s%250Ashared%2520values.%2520Constitutional%2520AI%252C%2520proposed%2520by%2520Anthropic%252C%2520represents%2520a%2520step%250Atowards%2520this%2520goal%252C%2520offering%2520a%2520model%2520for%2520democratic%2520control%2520of%2520AI.%2520However%252C%250Awhile%2520Constitutional%2520AI%2527s%2520commitment%2520to%2520hardcoding%2520explicit%2520principles%2520into%2520AI%250Amodels%2520enhances%2520transparency%2520and%2520accountability%252C%2520it%2520falls%2520short%2520in%2520two%2520crucial%250Aaspects%253A%2520addressing%2520the%2520opacity%2520of%2520individual%2520AI%2520decisions%2520and%2520fostering%250Agenuine%2520democratic%2520legitimacy.%2520To%2520overcome%2520these%2520limitations%252C%2520this%2520essay%250Aproposes%2520%2522Public%2520Constitutional%2520AI.%2522%2520This%2520approach%2520envisions%2520a%2520participatory%250Aprocess%2520where%2520diverse%2520stakeholders%252C%2520including%2520ordinary%2520citizens%252C%2520deliberate%2520on%250Athe%2520principles%2520guiding%2520AI%2520development.%2520The%2520resulting%2520%2522AI%2520Constitution%2522%2520would%250Acarry%2520the%2520legitimacy%2520of%2520popular%2520authorship%252C%2520grounding%2520AI%2520governance%2520in%2520the%250Apublic%2520will.%2520Furthermore%252C%2520the%2520essay%2520proposes%2520%2522AI%2520Courts%2522%2520to%2520develop%2520%2522AI%2520case%250Alaw%252C%2522%2520providing%2520concrete%2520examples%2520for%2520operationalizing%2520constitutional%250Aprinciples%2520in%2520AI%2520training.%2520This%2520evolving%2520combination%2520of%2520constitutional%250Aprinciples%2520and%2520case%2520law%2520aims%2520to%2520make%2520AI%2520governance%2520more%2520responsive%2520to%2520public%250Avalues.%2520By%2520grounding%2520AI%2520governance%2520in%2520deliberative%2520democratic%2520processes%252C%2520Public%250AConstitutional%2520AI%2520offers%2520a%2520path%2520to%2520imbue%2520automated%2520authorities%2520with%2520genuine%250Ademocratic%2520legitimacy%252C%2520addressing%2520the%2520unique%2520challenges%2520posed%2520by%2520increasingly%250Apowerful%2520AI%2520systems%2520while%2520ensuring%2520their%2520alignment%2520with%2520the%2520public%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16696v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Public%20Constitutional%20AI&entry.906535625=Gilad%20Abiri&entry.1292438233=%20%20We%20are%20increasingly%20subjected%20to%20the%20power%20of%20AI%20authorities.%20As%20AI%20decisions%0Abecome%20inescapable%2C%20entering%20domains%20such%20as%20healthcare%2C%20education%2C%20and%20law%2C%20we%0Amust%20confront%20a%20vital%20question%3A%20how%20can%20we%20ensure%20AI%20systems%20have%20the%0Alegitimacy%20necessary%20for%20effective%20governance%3F%20This%20essay%20argues%20that%20to%20secure%0AAI%20legitimacy%2C%20we%20need%20methods%20that%20engage%20the%20public%20in%20designing%20and%0Aconstraining%20AI%20systems%2C%20ensuring%20these%20technologies%20reflect%20the%20community%27s%0Ashared%20values.%20Constitutional%20AI%2C%20proposed%20by%20Anthropic%2C%20represents%20a%20step%0Atowards%20this%20goal%2C%20offering%20a%20model%20for%20democratic%20control%20of%20AI.%20However%2C%0Awhile%20Constitutional%20AI%27s%20commitment%20to%20hardcoding%20explicit%20principles%20into%20AI%0Amodels%20enhances%20transparency%20and%20accountability%2C%20it%20falls%20short%20in%20two%20crucial%0Aaspects%3A%20addressing%20the%20opacity%20of%20individual%20AI%20decisions%20and%20fostering%0Agenuine%20democratic%20legitimacy.%20To%20overcome%20these%20limitations%2C%20this%20essay%0Aproposes%20%22Public%20Constitutional%20AI.%22%20This%20approach%20envisions%20a%20participatory%0Aprocess%20where%20diverse%20stakeholders%2C%20including%20ordinary%20citizens%2C%20deliberate%20on%0Athe%20principles%20guiding%20AI%20development.%20The%20resulting%20%22AI%20Constitution%22%20would%0Acarry%20the%20legitimacy%20of%20popular%20authorship%2C%20grounding%20AI%20governance%20in%20the%0Apublic%20will.%20Furthermore%2C%20the%20essay%20proposes%20%22AI%20Courts%22%20to%20develop%20%22AI%20case%0Alaw%2C%22%20providing%20concrete%20examples%20for%20operationalizing%20constitutional%0Aprinciples%20in%20AI%20training.%20This%20evolving%20combination%20of%20constitutional%0Aprinciples%20and%20case%20law%20aims%20to%20make%20AI%20governance%20more%20responsive%20to%20public%0Avalues.%20By%20grounding%20AI%20governance%20in%20deliberative%20democratic%20processes%2C%20Public%0AConstitutional%20AI%20offers%20a%20path%20to%20imbue%20automated%20authorities%20with%20genuine%0Ademocratic%20legitimacy%2C%20addressing%20the%20unique%20challenges%20posed%20by%20increasingly%0Apowerful%20AI%20systems%20while%20ensuring%20their%20alignment%20with%20the%20public%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16696v2&entry.124074799=Read"},
{"title": "METDrive: Multi-modal End-to-end Autonomous Driving with Temporal\n  Guidance", "author": "Ziang Guo and Xinhao Lin and Zakhar Yagudin and Artem Lykov and Yong Wang and Yanqiang Li and Dzmitry Tsetserukou", "abstract": "  Multi-modal end-to-end autonomous driving has shown promising advancements in\nrecent work. By embedding more modalities into end-to-end networks, the\nsystem's understanding of both static and dynamic aspects of the driving\nenvironment is enhanced, thereby improving the safety of autonomous driving. In\nthis paper, we introduce METDrive, an end-to-end system that leverages temporal\nguidance from the embedded time series features of ego states, including\nrotation angles, steering, throttle signals, and waypoint vectors. The\ngeometric features derived from perception sensor data and the time series\nfeatures of ego state data jointly guide the waypoint prediction with the\nproposed temporal guidance loss function. We evaluated METDrive on the CARLA\nleaderboard benchmarks, achieving a driving score of 70%, a route completion\nscore of 94%, and an infraction score of 0.78.\n", "link": "http://arxiv.org/abs/2409.12667v3", "date": "2025-05-14", "relevancy": 2.0479, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5456}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.509}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20METDrive%3A%20Multi-modal%20End-to-end%20Autonomous%20Driving%20with%20Temporal%0A%20%20Guidance&body=Title%3A%20METDrive%3A%20Multi-modal%20End-to-end%20Autonomous%20Driving%20with%20Temporal%0A%20%20Guidance%0AAuthor%3A%20Ziang%20Guo%20and%20Xinhao%20Lin%20and%20Zakhar%20Yagudin%20and%20Artem%20Lykov%20and%20Yong%20Wang%20and%20Yanqiang%20Li%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20Multi-modal%20end-to-end%20autonomous%20driving%20has%20shown%20promising%20advancements%20in%0Arecent%20work.%20By%20embedding%20more%20modalities%20into%20end-to-end%20networks%2C%20the%0Asystem%27s%20understanding%20of%20both%20static%20and%20dynamic%20aspects%20of%20the%20driving%0Aenvironment%20is%20enhanced%2C%20thereby%20improving%20the%20safety%20of%20autonomous%20driving.%20In%0Athis%20paper%2C%20we%20introduce%20METDrive%2C%20an%20end-to-end%20system%20that%20leverages%20temporal%0Aguidance%20from%20the%20embedded%20time%20series%20features%20of%20ego%20states%2C%20including%0Arotation%20angles%2C%20steering%2C%20throttle%20signals%2C%20and%20waypoint%20vectors.%20The%0Ageometric%20features%20derived%20from%20perception%20sensor%20data%20and%20the%20time%20series%0Afeatures%20of%20ego%20state%20data%20jointly%20guide%20the%20waypoint%20prediction%20with%20the%0Aproposed%20temporal%20guidance%20loss%20function.%20We%20evaluated%20METDrive%20on%20the%20CARLA%0Aleaderboard%20benchmarks%2C%20achieving%20a%20driving%20score%20of%2070%25%2C%20a%20route%20completion%0Ascore%20of%2094%25%2C%20and%20an%20infraction%20score%20of%200.78.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12667v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMETDrive%253A%2520Multi-modal%2520End-to-end%2520Autonomous%2520Driving%2520with%2520Temporal%250A%2520%2520Guidance%26entry.906535625%3DZiang%2520Guo%2520and%2520Xinhao%2520Lin%2520and%2520Zakhar%2520Yagudin%2520and%2520Artem%2520Lykov%2520and%2520Yong%2520Wang%2520and%2520Yanqiang%2520Li%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520Multi-modal%2520end-to-end%2520autonomous%2520driving%2520has%2520shown%2520promising%2520advancements%2520in%250Arecent%2520work.%2520By%2520embedding%2520more%2520modalities%2520into%2520end-to-end%2520networks%252C%2520the%250Asystem%2527s%2520understanding%2520of%2520both%2520static%2520and%2520dynamic%2520aspects%2520of%2520the%2520driving%250Aenvironment%2520is%2520enhanced%252C%2520thereby%2520improving%2520the%2520safety%2520of%2520autonomous%2520driving.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520METDrive%252C%2520an%2520end-to-end%2520system%2520that%2520leverages%2520temporal%250Aguidance%2520from%2520the%2520embedded%2520time%2520series%2520features%2520of%2520ego%2520states%252C%2520including%250Arotation%2520angles%252C%2520steering%252C%2520throttle%2520signals%252C%2520and%2520waypoint%2520vectors.%2520The%250Ageometric%2520features%2520derived%2520from%2520perception%2520sensor%2520data%2520and%2520the%2520time%2520series%250Afeatures%2520of%2520ego%2520state%2520data%2520jointly%2520guide%2520the%2520waypoint%2520prediction%2520with%2520the%250Aproposed%2520temporal%2520guidance%2520loss%2520function.%2520We%2520evaluated%2520METDrive%2520on%2520the%2520CARLA%250Aleaderboard%2520benchmarks%252C%2520achieving%2520a%2520driving%2520score%2520of%252070%2525%252C%2520a%2520route%2520completion%250Ascore%2520of%252094%2525%252C%2520and%2520an%2520infraction%2520score%2520of%25200.78.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12667v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=METDrive%3A%20Multi-modal%20End-to-end%20Autonomous%20Driving%20with%20Temporal%0A%20%20Guidance&entry.906535625=Ziang%20Guo%20and%20Xinhao%20Lin%20and%20Zakhar%20Yagudin%20and%20Artem%20Lykov%20and%20Yong%20Wang%20and%20Yanqiang%20Li%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20Multi-modal%20end-to-end%20autonomous%20driving%20has%20shown%20promising%20advancements%20in%0Arecent%20work.%20By%20embedding%20more%20modalities%20into%20end-to-end%20networks%2C%20the%0Asystem%27s%20understanding%20of%20both%20static%20and%20dynamic%20aspects%20of%20the%20driving%0Aenvironment%20is%20enhanced%2C%20thereby%20improving%20the%20safety%20of%20autonomous%20driving.%20In%0Athis%20paper%2C%20we%20introduce%20METDrive%2C%20an%20end-to-end%20system%20that%20leverages%20temporal%0Aguidance%20from%20the%20embedded%20time%20series%20features%20of%20ego%20states%2C%20including%0Arotation%20angles%2C%20steering%2C%20throttle%20signals%2C%20and%20waypoint%20vectors.%20The%0Ageometric%20features%20derived%20from%20perception%20sensor%20data%20and%20the%20time%20series%0Afeatures%20of%20ego%20state%20data%20jointly%20guide%20the%20waypoint%20prediction%20with%20the%0Aproposed%20temporal%20guidance%20loss%20function.%20We%20evaluated%20METDrive%20on%20the%20CARLA%0Aleaderboard%20benchmarks%2C%20achieving%20a%20driving%20score%20of%2070%25%2C%20a%20route%20completion%0Ascore%20of%2094%25%2C%20and%20an%20infraction%20score%20of%200.78.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12667v3&entry.124074799=Read"},
{"title": "Layered Unlearning for Adversarial Relearning", "author": "Timothy Qian and Vinith Suriyakumar and Ashia Wilson and Dylan Hadfield-Menell", "abstract": "  Our goal is to understand how post-training methods, such as fine-tuning,\nalignment, and unlearning, modify language model behavior and representations.\nWe are particularly interested in the brittle nature of these modifications\nthat makes them easy to bypass through prompt engineering or relearning. Recent\nresults suggest that post-training induces shallow context-dependent\n``circuits'' that suppress specific response patterns. This could be one\nexplanation for the brittleness of post-training. To test this hypothesis, we\ndesign an unlearning algorithm, Layered Unlearning (LU), that creates distinct\ninhibitory mechanisms for a growing subset of the data. By unlearning the first\n$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU\nlimits the ability of relearning on a subset of data to recover the full\ndataset. We evaluate LU through a combination of synthetic and large language\nmodel (LLM) experiments. We find that LU improves robustness to adversarial\nrelearning for several different unlearning methods. Our results contribute to\nthe state-of-the-art of machine unlearning and provide insight into the effect\nof post-training updates.\n", "link": "http://arxiv.org/abs/2505.09500v1", "date": "2025-05-14", "relevancy": 2.0468, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5248}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5077}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Layered%20Unlearning%20for%20Adversarial%20Relearning&body=Title%3A%20Layered%20Unlearning%20for%20Adversarial%20Relearning%0AAuthor%3A%20Timothy%20Qian%20and%20Vinith%20Suriyakumar%20and%20Ashia%20Wilson%20and%20Dylan%20Hadfield-Menell%0AAbstract%3A%20%20%20Our%20goal%20is%20to%20understand%20how%20post-training%20methods%2C%20such%20as%20fine-tuning%2C%0Aalignment%2C%20and%20unlearning%2C%20modify%20language%20model%20behavior%20and%20representations.%0AWe%20are%20particularly%20interested%20in%20the%20brittle%20nature%20of%20these%20modifications%0Athat%20makes%20them%20easy%20to%20bypass%20through%20prompt%20engineering%20or%20relearning.%20Recent%0Aresults%20suggest%20that%20post-training%20induces%20shallow%20context-dependent%0A%60%60circuits%27%27%20that%20suppress%20specific%20response%20patterns.%20This%20could%20be%20one%0Aexplanation%20for%20the%20brittleness%20of%20post-training.%20To%20test%20this%20hypothesis%2C%20we%0Adesign%20an%20unlearning%20algorithm%2C%20Layered%20Unlearning%20%28LU%29%2C%20that%20creates%20distinct%0Ainhibitory%20mechanisms%20for%20a%20growing%20subset%20of%20the%20data.%20By%20unlearning%20the%20first%0A%24i%24%20folds%20while%20retaining%20the%20remaining%20%24k%20-%20i%24%20at%20the%20%24i%24th%20of%20%24k%24%20stages%2C%20LU%0Alimits%20the%20ability%20of%20relearning%20on%20a%20subset%20of%20data%20to%20recover%20the%20full%0Adataset.%20We%20evaluate%20LU%20through%20a%20combination%20of%20synthetic%20and%20large%20language%0Amodel%20%28LLM%29%20experiments.%20We%20find%20that%20LU%20improves%20robustness%20to%20adversarial%0Arelearning%20for%20several%20different%20unlearning%20methods.%20Our%20results%20contribute%20to%0Athe%20state-of-the-art%20of%20machine%20unlearning%20and%20provide%20insight%20into%20the%20effect%0Aof%20post-training%20updates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayered%2520Unlearning%2520for%2520Adversarial%2520Relearning%26entry.906535625%3DTimothy%2520Qian%2520and%2520Vinith%2520Suriyakumar%2520and%2520Ashia%2520Wilson%2520and%2520Dylan%2520Hadfield-Menell%26entry.1292438233%3D%2520%2520Our%2520goal%2520is%2520to%2520understand%2520how%2520post-training%2520methods%252C%2520such%2520as%2520fine-tuning%252C%250Aalignment%252C%2520and%2520unlearning%252C%2520modify%2520language%2520model%2520behavior%2520and%2520representations.%250AWe%2520are%2520particularly%2520interested%2520in%2520the%2520brittle%2520nature%2520of%2520these%2520modifications%250Athat%2520makes%2520them%2520easy%2520to%2520bypass%2520through%2520prompt%2520engineering%2520or%2520relearning.%2520Recent%250Aresults%2520suggest%2520that%2520post-training%2520induces%2520shallow%2520context-dependent%250A%2560%2560circuits%2527%2527%2520that%2520suppress%2520specific%2520response%2520patterns.%2520This%2520could%2520be%2520one%250Aexplanation%2520for%2520the%2520brittleness%2520of%2520post-training.%2520To%2520test%2520this%2520hypothesis%252C%2520we%250Adesign%2520an%2520unlearning%2520algorithm%252C%2520Layered%2520Unlearning%2520%2528LU%2529%252C%2520that%2520creates%2520distinct%250Ainhibitory%2520mechanisms%2520for%2520a%2520growing%2520subset%2520of%2520the%2520data.%2520By%2520unlearning%2520the%2520first%250A%2524i%2524%2520folds%2520while%2520retaining%2520the%2520remaining%2520%2524k%2520-%2520i%2524%2520at%2520the%2520%2524i%2524th%2520of%2520%2524k%2524%2520stages%252C%2520LU%250Alimits%2520the%2520ability%2520of%2520relearning%2520on%2520a%2520subset%2520of%2520data%2520to%2520recover%2520the%2520full%250Adataset.%2520We%2520evaluate%2520LU%2520through%2520a%2520combination%2520of%2520synthetic%2520and%2520large%2520language%250Amodel%2520%2528LLM%2529%2520experiments.%2520We%2520find%2520that%2520LU%2520improves%2520robustness%2520to%2520adversarial%250Arelearning%2520for%2520several%2520different%2520unlearning%2520methods.%2520Our%2520results%2520contribute%2520to%250Athe%2520state-of-the-art%2520of%2520machine%2520unlearning%2520and%2520provide%2520insight%2520into%2520the%2520effect%250Aof%2520post-training%2520updates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Layered%20Unlearning%20for%20Adversarial%20Relearning&entry.906535625=Timothy%20Qian%20and%20Vinith%20Suriyakumar%20and%20Ashia%20Wilson%20and%20Dylan%20Hadfield-Menell&entry.1292438233=%20%20Our%20goal%20is%20to%20understand%20how%20post-training%20methods%2C%20such%20as%20fine-tuning%2C%0Aalignment%2C%20and%20unlearning%2C%20modify%20language%20model%20behavior%20and%20representations.%0AWe%20are%20particularly%20interested%20in%20the%20brittle%20nature%20of%20these%20modifications%0Athat%20makes%20them%20easy%20to%20bypass%20through%20prompt%20engineering%20or%20relearning.%20Recent%0Aresults%20suggest%20that%20post-training%20induces%20shallow%20context-dependent%0A%60%60circuits%27%27%20that%20suppress%20specific%20response%20patterns.%20This%20could%20be%20one%0Aexplanation%20for%20the%20brittleness%20of%20post-training.%20To%20test%20this%20hypothesis%2C%20we%0Adesign%20an%20unlearning%20algorithm%2C%20Layered%20Unlearning%20%28LU%29%2C%20that%20creates%20distinct%0Ainhibitory%20mechanisms%20for%20a%20growing%20subset%20of%20the%20data.%20By%20unlearning%20the%20first%0A%24i%24%20folds%20while%20retaining%20the%20remaining%20%24k%20-%20i%24%20at%20the%20%24i%24th%20of%20%24k%24%20stages%2C%20LU%0Alimits%20the%20ability%20of%20relearning%20on%20a%20subset%20of%20data%20to%20recover%20the%20full%0Adataset.%20We%20evaluate%20LU%20through%20a%20combination%20of%20synthetic%20and%20large%20language%0Amodel%20%28LLM%29%20experiments.%20We%20find%20that%20LU%20improves%20robustness%20to%20adversarial%0Arelearning%20for%20several%20different%20unlearning%20methods.%20Our%20results%20contribute%20to%0Athe%20state-of-the-art%20of%20machine%20unlearning%20and%20provide%20insight%20into%20the%20effect%0Aof%20post-training%20updates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09500v1&entry.124074799=Read"},
{"title": "State-of-the-Art Periorbital Distance Prediction and Disease\n  Classification Using Periorbital Features", "author": "George R. Nahass and Sasha Hubschman and Jeffrey C. Peterson and Ghasem Yazdanpanah and Nicholas Tomaras and Madison Cheung and Alex Palacios and Kevin Heinze and Chad A. Purnell and Pete Setabutr and Ann Q. Tran and Darvin Yi", "abstract": "  Periorbital distances are critical markers for diagnosing and monitoring a\nrange of oculoplastic and craniofacial conditions. Manual measurement, however,\nis subjective and prone to intergrader variability. Automated methods have been\ndeveloped but remain limited by standardized imaging requirements, small\ndatasets, and a narrow focus on individual measurements. We developed a\nsegmentation pipeline trained on a domain-specific dataset of healthy eyes and\ncompared its performance against the Segment Anything Model (SAM) and the prior\nbenchmark, PeriorbitAI. Segmentation accuracy was evaluated across multiple\ndisease classes and imaging conditions. We further investigated the use of\npredicted periorbital distances as features for disease classification under\nin-distribution (ID) and out-of-distribution (OOD) settings, comparing shallow\nclassifiers, CNNs, and fusion models. Our segmentation model achieved\nstate-of-the-art accuracy across all datasets, with error rates within\nintergrader variability and superior performance relative to SAM and\nPeriorbitAI. In classification tasks, models trained on periorbital distances\nmatched CNN performance on ID data (77--78\\% accuracy) and substantially\noutperformed CNNs under OOD conditions (63--68\\% accuracy vs. 14\\%). Fusion\nmodels achieved the highest ID accuracy (80\\%) but were sensitive to degraded\nCNN features under OOD shifts. Segmentation-derived periorbital distances\nprovide robust, explainable features for disease classification and generalize\nbetter under domain shift than CNN image classifiers. These results establish a\nnew benchmark for periorbital distance prediction and highlight the potential\nof anatomy-based AI pipelines for real-world deployment in oculoplastic and\ncraniofacial care.\n", "link": "http://arxiv.org/abs/2409.18769v5", "date": "2025-05-14", "relevancy": 2.0366, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5246}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5045}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20State-of-the-Art%20Periorbital%20Distance%20Prediction%20and%20Disease%0A%20%20Classification%20Using%20Periorbital%20Features&body=Title%3A%20State-of-the-Art%20Periorbital%20Distance%20Prediction%20and%20Disease%0A%20%20Classification%20Using%20Periorbital%20Features%0AAuthor%3A%20George%20R.%20Nahass%20and%20Sasha%20Hubschman%20and%20Jeffrey%20C.%20Peterson%20and%20Ghasem%20Yazdanpanah%20and%20Nicholas%20Tomaras%20and%20Madison%20Cheung%20and%20Alex%20Palacios%20and%20Kevin%20Heinze%20and%20Chad%20A.%20Purnell%20and%20Pete%20Setabutr%20and%20Ann%20Q.%20Tran%20and%20Darvin%20Yi%0AAbstract%3A%20%20%20Periorbital%20distances%20are%20critical%20markers%20for%20diagnosing%20and%20monitoring%20a%0Arange%20of%20oculoplastic%20and%20craniofacial%20conditions.%20Manual%20measurement%2C%20however%2C%0Ais%20subjective%20and%20prone%20to%20intergrader%20variability.%20Automated%20methods%20have%20been%0Adeveloped%20but%20remain%20limited%20by%20standardized%20imaging%20requirements%2C%20small%0Adatasets%2C%20and%20a%20narrow%20focus%20on%20individual%20measurements.%20We%20developed%20a%0Asegmentation%20pipeline%20trained%20on%20a%20domain-specific%20dataset%20of%20healthy%20eyes%20and%0Acompared%20its%20performance%20against%20the%20Segment%20Anything%20Model%20%28SAM%29%20and%20the%20prior%0Abenchmark%2C%20PeriorbitAI.%20Segmentation%20accuracy%20was%20evaluated%20across%20multiple%0Adisease%20classes%20and%20imaging%20conditions.%20We%20further%20investigated%20the%20use%20of%0Apredicted%20periorbital%20distances%20as%20features%20for%20disease%20classification%20under%0Ain-distribution%20%28ID%29%20and%20out-of-distribution%20%28OOD%29%20settings%2C%20comparing%20shallow%0Aclassifiers%2C%20CNNs%2C%20and%20fusion%20models.%20Our%20segmentation%20model%20achieved%0Astate-of-the-art%20accuracy%20across%20all%20datasets%2C%20with%20error%20rates%20within%0Aintergrader%20variability%20and%20superior%20performance%20relative%20to%20SAM%20and%0APeriorbitAI.%20In%20classification%20tasks%2C%20models%20trained%20on%20periorbital%20distances%0Amatched%20CNN%20performance%20on%20ID%20data%20%2877--78%5C%25%20accuracy%29%20and%20substantially%0Aoutperformed%20CNNs%20under%20OOD%20conditions%20%2863--68%5C%25%20accuracy%20vs.%2014%5C%25%29.%20Fusion%0Amodels%20achieved%20the%20highest%20ID%20accuracy%20%2880%5C%25%29%20but%20were%20sensitive%20to%20degraded%0ACNN%20features%20under%20OOD%20shifts.%20Segmentation-derived%20periorbital%20distances%0Aprovide%20robust%2C%20explainable%20features%20for%20disease%20classification%20and%20generalize%0Abetter%20under%20domain%20shift%20than%20CNN%20image%20classifiers.%20These%20results%20establish%20a%0Anew%20benchmark%20for%20periorbital%20distance%20prediction%20and%20highlight%20the%20potential%0Aof%20anatomy-based%20AI%20pipelines%20for%20real-world%20deployment%20in%20oculoplastic%20and%0Acraniofacial%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18769v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DState-of-the-Art%2520Periorbital%2520Distance%2520Prediction%2520and%2520Disease%250A%2520%2520Classification%2520Using%2520Periorbital%2520Features%26entry.906535625%3DGeorge%2520R.%2520Nahass%2520and%2520Sasha%2520Hubschman%2520and%2520Jeffrey%2520C.%2520Peterson%2520and%2520Ghasem%2520Yazdanpanah%2520and%2520Nicholas%2520Tomaras%2520and%2520Madison%2520Cheung%2520and%2520Alex%2520Palacios%2520and%2520Kevin%2520Heinze%2520and%2520Chad%2520A.%2520Purnell%2520and%2520Pete%2520Setabutr%2520and%2520Ann%2520Q.%2520Tran%2520and%2520Darvin%2520Yi%26entry.1292438233%3D%2520%2520Periorbital%2520distances%2520are%2520critical%2520markers%2520for%2520diagnosing%2520and%2520monitoring%2520a%250Arange%2520of%2520oculoplastic%2520and%2520craniofacial%2520conditions.%2520Manual%2520measurement%252C%2520however%252C%250Ais%2520subjective%2520and%2520prone%2520to%2520intergrader%2520variability.%2520Automated%2520methods%2520have%2520been%250Adeveloped%2520but%2520remain%2520limited%2520by%2520standardized%2520imaging%2520requirements%252C%2520small%250Adatasets%252C%2520and%2520a%2520narrow%2520focus%2520on%2520individual%2520measurements.%2520We%2520developed%2520a%250Asegmentation%2520pipeline%2520trained%2520on%2520a%2520domain-specific%2520dataset%2520of%2520healthy%2520eyes%2520and%250Acompared%2520its%2520performance%2520against%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520and%2520the%2520prior%250Abenchmark%252C%2520PeriorbitAI.%2520Segmentation%2520accuracy%2520was%2520evaluated%2520across%2520multiple%250Adisease%2520classes%2520and%2520imaging%2520conditions.%2520We%2520further%2520investigated%2520the%2520use%2520of%250Apredicted%2520periorbital%2520distances%2520as%2520features%2520for%2520disease%2520classification%2520under%250Ain-distribution%2520%2528ID%2529%2520and%2520out-of-distribution%2520%2528OOD%2529%2520settings%252C%2520comparing%2520shallow%250Aclassifiers%252C%2520CNNs%252C%2520and%2520fusion%2520models.%2520Our%2520segmentation%2520model%2520achieved%250Astate-of-the-art%2520accuracy%2520across%2520all%2520datasets%252C%2520with%2520error%2520rates%2520within%250Aintergrader%2520variability%2520and%2520superior%2520performance%2520relative%2520to%2520SAM%2520and%250APeriorbitAI.%2520In%2520classification%2520tasks%252C%2520models%2520trained%2520on%2520periorbital%2520distances%250Amatched%2520CNN%2520performance%2520on%2520ID%2520data%2520%252877--78%255C%2525%2520accuracy%2529%2520and%2520substantially%250Aoutperformed%2520CNNs%2520under%2520OOD%2520conditions%2520%252863--68%255C%2525%2520accuracy%2520vs.%252014%255C%2525%2529.%2520Fusion%250Amodels%2520achieved%2520the%2520highest%2520ID%2520accuracy%2520%252880%255C%2525%2529%2520but%2520were%2520sensitive%2520to%2520degraded%250ACNN%2520features%2520under%2520OOD%2520shifts.%2520Segmentation-derived%2520periorbital%2520distances%250Aprovide%2520robust%252C%2520explainable%2520features%2520for%2520disease%2520classification%2520and%2520generalize%250Abetter%2520under%2520domain%2520shift%2520than%2520CNN%2520image%2520classifiers.%2520These%2520results%2520establish%2520a%250Anew%2520benchmark%2520for%2520periorbital%2520distance%2520prediction%2520and%2520highlight%2520the%2520potential%250Aof%2520anatomy-based%2520AI%2520pipelines%2520for%2520real-world%2520deployment%2520in%2520oculoplastic%2520and%250Acraniofacial%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18769v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=State-of-the-Art%20Periorbital%20Distance%20Prediction%20and%20Disease%0A%20%20Classification%20Using%20Periorbital%20Features&entry.906535625=George%20R.%20Nahass%20and%20Sasha%20Hubschman%20and%20Jeffrey%20C.%20Peterson%20and%20Ghasem%20Yazdanpanah%20and%20Nicholas%20Tomaras%20and%20Madison%20Cheung%20and%20Alex%20Palacios%20and%20Kevin%20Heinze%20and%20Chad%20A.%20Purnell%20and%20Pete%20Setabutr%20and%20Ann%20Q.%20Tran%20and%20Darvin%20Yi&entry.1292438233=%20%20Periorbital%20distances%20are%20critical%20markers%20for%20diagnosing%20and%20monitoring%20a%0Arange%20of%20oculoplastic%20and%20craniofacial%20conditions.%20Manual%20measurement%2C%20however%2C%0Ais%20subjective%20and%20prone%20to%20intergrader%20variability.%20Automated%20methods%20have%20been%0Adeveloped%20but%20remain%20limited%20by%20standardized%20imaging%20requirements%2C%20small%0Adatasets%2C%20and%20a%20narrow%20focus%20on%20individual%20measurements.%20We%20developed%20a%0Asegmentation%20pipeline%20trained%20on%20a%20domain-specific%20dataset%20of%20healthy%20eyes%20and%0Acompared%20its%20performance%20against%20the%20Segment%20Anything%20Model%20%28SAM%29%20and%20the%20prior%0Abenchmark%2C%20PeriorbitAI.%20Segmentation%20accuracy%20was%20evaluated%20across%20multiple%0Adisease%20classes%20and%20imaging%20conditions.%20We%20further%20investigated%20the%20use%20of%0Apredicted%20periorbital%20distances%20as%20features%20for%20disease%20classification%20under%0Ain-distribution%20%28ID%29%20and%20out-of-distribution%20%28OOD%29%20settings%2C%20comparing%20shallow%0Aclassifiers%2C%20CNNs%2C%20and%20fusion%20models.%20Our%20segmentation%20model%20achieved%0Astate-of-the-art%20accuracy%20across%20all%20datasets%2C%20with%20error%20rates%20within%0Aintergrader%20variability%20and%20superior%20performance%20relative%20to%20SAM%20and%0APeriorbitAI.%20In%20classification%20tasks%2C%20models%20trained%20on%20periorbital%20distances%0Amatched%20CNN%20performance%20on%20ID%20data%20%2877--78%5C%25%20accuracy%29%20and%20substantially%0Aoutperformed%20CNNs%20under%20OOD%20conditions%20%2863--68%5C%25%20accuracy%20vs.%2014%5C%25%29.%20Fusion%0Amodels%20achieved%20the%20highest%20ID%20accuracy%20%2880%5C%25%29%20but%20were%20sensitive%20to%20degraded%0ACNN%20features%20under%20OOD%20shifts.%20Segmentation-derived%20periorbital%20distances%0Aprovide%20robust%2C%20explainable%20features%20for%20disease%20classification%20and%20generalize%0Abetter%20under%20domain%20shift%20than%20CNN%20image%20classifiers.%20These%20results%20establish%20a%0Anew%20benchmark%20for%20periorbital%20distance%20prediction%20and%20highlight%20the%20potential%0Aof%20anatomy-based%20AI%20pipelines%20for%20real-world%20deployment%20in%20oculoplastic%20and%0Acraniofacial%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18769v5&entry.124074799=Read"},
{"title": "Variational Rank Reduction Autoencoder", "author": "Jad Mounayer and Alicia Tierz and Jerome Tomezyk and Chady Ghnatios and Francisco Chinesta", "abstract": "  Deterministic Rank Reduction Autoencoders (RRAEs) enforce by construction a\nregularization on the latent space by applying a truncated SVD. While this\nregularization makes Autoencoders more powerful, using them for generative\npurposes is counter-intuitive due to their deterministic nature. On the other\nhand, Variational Autoencoders (VAEs) are well known for their generative\nabilities by learning a probabilistic latent space. In this paper, we present\nVariational Rank Reduction Autoencoders (VRRAEs), a model that leverages the\nadvantages of both RRAEs and VAEs. Our claims and results show that when\ncarefully sampling the latent space of RRAEs and further regularizing with the\nKullback-Leibler (KL) divergence (similarly to VAEs), VRRAEs outperform RRAEs\nand VAEs. Additionally, we show that the regularization induced by the SVD not\nonly makes VRRAEs better generators than VAEs, but also reduces the possibility\nof posterior collapse. Our results include a synthetic dataset of a small size\nthat showcases the robustness of VRRAEs against collapse, and three real-world\ndatasets; the MNIST, CelebA, and CIFAR-10, over which VRRAEs are shown to\noutperform both VAEs and RRAEs on many random generation and interpolation\ntasks based on the FID score.\n", "link": "http://arxiv.org/abs/2505.09458v1", "date": "2025-05-14", "relevancy": 2.034, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.538}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4954}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Rank%20Reduction%20Autoencoder&body=Title%3A%20Variational%20Rank%20Reduction%20Autoencoder%0AAuthor%3A%20Jad%20Mounayer%20and%20Alicia%20Tierz%20and%20Jerome%20Tomezyk%20and%20Chady%20Ghnatios%20and%20Francisco%20Chinesta%0AAbstract%3A%20%20%20Deterministic%20Rank%20Reduction%20Autoencoders%20%28RRAEs%29%20enforce%20by%20construction%20a%0Aregularization%20on%20the%20latent%20space%20by%20applying%20a%20truncated%20SVD.%20While%20this%0Aregularization%20makes%20Autoencoders%20more%20powerful%2C%20using%20them%20for%20generative%0Apurposes%20is%20counter-intuitive%20due%20to%20their%20deterministic%20nature.%20On%20the%20other%0Ahand%2C%20Variational%20Autoencoders%20%28VAEs%29%20are%20well%20known%20for%20their%20generative%0Aabilities%20by%20learning%20a%20probabilistic%20latent%20space.%20In%20this%20paper%2C%20we%20present%0AVariational%20Rank%20Reduction%20Autoencoders%20%28VRRAEs%29%2C%20a%20model%20that%20leverages%20the%0Aadvantages%20of%20both%20RRAEs%20and%20VAEs.%20Our%20claims%20and%20results%20show%20that%20when%0Acarefully%20sampling%20the%20latent%20space%20of%20RRAEs%20and%20further%20regularizing%20with%20the%0AKullback-Leibler%20%28KL%29%20divergence%20%28similarly%20to%20VAEs%29%2C%20VRRAEs%20outperform%20RRAEs%0Aand%20VAEs.%20Additionally%2C%20we%20show%20that%20the%20regularization%20induced%20by%20the%20SVD%20not%0Aonly%20makes%20VRRAEs%20better%20generators%20than%20VAEs%2C%20but%20also%20reduces%20the%20possibility%0Aof%20posterior%20collapse.%20Our%20results%20include%20a%20synthetic%20dataset%20of%20a%20small%20size%0Athat%20showcases%20the%20robustness%20of%20VRRAEs%20against%20collapse%2C%20and%20three%20real-world%0Adatasets%3B%20the%20MNIST%2C%20CelebA%2C%20and%20CIFAR-10%2C%20over%20which%20VRRAEs%20are%20shown%20to%0Aoutperform%20both%20VAEs%20and%20RRAEs%20on%20many%20random%20generation%20and%20interpolation%0Atasks%20based%20on%20the%20FID%20score.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Rank%2520Reduction%2520Autoencoder%26entry.906535625%3DJad%2520Mounayer%2520and%2520Alicia%2520Tierz%2520and%2520Jerome%2520Tomezyk%2520and%2520Chady%2520Ghnatios%2520and%2520Francisco%2520Chinesta%26entry.1292438233%3D%2520%2520Deterministic%2520Rank%2520Reduction%2520Autoencoders%2520%2528RRAEs%2529%2520enforce%2520by%2520construction%2520a%250Aregularization%2520on%2520the%2520latent%2520space%2520by%2520applying%2520a%2520truncated%2520SVD.%2520While%2520this%250Aregularization%2520makes%2520Autoencoders%2520more%2520powerful%252C%2520using%2520them%2520for%2520generative%250Apurposes%2520is%2520counter-intuitive%2520due%2520to%2520their%2520deterministic%2520nature.%2520On%2520the%2520other%250Ahand%252C%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520are%2520well%2520known%2520for%2520their%2520generative%250Aabilities%2520by%2520learning%2520a%2520probabilistic%2520latent%2520space.%2520In%2520this%2520paper%252C%2520we%2520present%250AVariational%2520Rank%2520Reduction%2520Autoencoders%2520%2528VRRAEs%2529%252C%2520a%2520model%2520that%2520leverages%2520the%250Aadvantages%2520of%2520both%2520RRAEs%2520and%2520VAEs.%2520Our%2520claims%2520and%2520results%2520show%2520that%2520when%250Acarefully%2520sampling%2520the%2520latent%2520space%2520of%2520RRAEs%2520and%2520further%2520regularizing%2520with%2520the%250AKullback-Leibler%2520%2528KL%2529%2520divergence%2520%2528similarly%2520to%2520VAEs%2529%252C%2520VRRAEs%2520outperform%2520RRAEs%250Aand%2520VAEs.%2520Additionally%252C%2520we%2520show%2520that%2520the%2520regularization%2520induced%2520by%2520the%2520SVD%2520not%250Aonly%2520makes%2520VRRAEs%2520better%2520generators%2520than%2520VAEs%252C%2520but%2520also%2520reduces%2520the%2520possibility%250Aof%2520posterior%2520collapse.%2520Our%2520results%2520include%2520a%2520synthetic%2520dataset%2520of%2520a%2520small%2520size%250Athat%2520showcases%2520the%2520robustness%2520of%2520VRRAEs%2520against%2520collapse%252C%2520and%2520three%2520real-world%250Adatasets%253B%2520the%2520MNIST%252C%2520CelebA%252C%2520and%2520CIFAR-10%252C%2520over%2520which%2520VRRAEs%2520are%2520shown%2520to%250Aoutperform%2520both%2520VAEs%2520and%2520RRAEs%2520on%2520many%2520random%2520generation%2520and%2520interpolation%250Atasks%2520based%2520on%2520the%2520FID%2520score.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Rank%20Reduction%20Autoencoder&entry.906535625=Jad%20Mounayer%20and%20Alicia%20Tierz%20and%20Jerome%20Tomezyk%20and%20Chady%20Ghnatios%20and%20Francisco%20Chinesta&entry.1292438233=%20%20Deterministic%20Rank%20Reduction%20Autoencoders%20%28RRAEs%29%20enforce%20by%20construction%20a%0Aregularization%20on%20the%20latent%20space%20by%20applying%20a%20truncated%20SVD.%20While%20this%0Aregularization%20makes%20Autoencoders%20more%20powerful%2C%20using%20them%20for%20generative%0Apurposes%20is%20counter-intuitive%20due%20to%20their%20deterministic%20nature.%20On%20the%20other%0Ahand%2C%20Variational%20Autoencoders%20%28VAEs%29%20are%20well%20known%20for%20their%20generative%0Aabilities%20by%20learning%20a%20probabilistic%20latent%20space.%20In%20this%20paper%2C%20we%20present%0AVariational%20Rank%20Reduction%20Autoencoders%20%28VRRAEs%29%2C%20a%20model%20that%20leverages%20the%0Aadvantages%20of%20both%20RRAEs%20and%20VAEs.%20Our%20claims%20and%20results%20show%20that%20when%0Acarefully%20sampling%20the%20latent%20space%20of%20RRAEs%20and%20further%20regularizing%20with%20the%0AKullback-Leibler%20%28KL%29%20divergence%20%28similarly%20to%20VAEs%29%2C%20VRRAEs%20outperform%20RRAEs%0Aand%20VAEs.%20Additionally%2C%20we%20show%20that%20the%20regularization%20induced%20by%20the%20SVD%20not%0Aonly%20makes%20VRRAEs%20better%20generators%20than%20VAEs%2C%20but%20also%20reduces%20the%20possibility%0Aof%20posterior%20collapse.%20Our%20results%20include%20a%20synthetic%20dataset%20of%20a%20small%20size%0Athat%20showcases%20the%20robustness%20of%20VRRAEs%20against%20collapse%2C%20and%20three%20real-world%0Adatasets%3B%20the%20MNIST%2C%20CelebA%2C%20and%20CIFAR-10%2C%20over%20which%20VRRAEs%20are%20shown%20to%0Aoutperform%20both%20VAEs%20and%20RRAEs%20on%20many%20random%20generation%20and%20interpolation%0Atasks%20based%20on%20the%20FID%20score.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09458v1&entry.124074799=Read"},
{"title": "Preserving Plasticity in Continual Learning with Adaptive Linearity\n  Injection", "author": "Seyed Roozbeh Razavi Rohani and Khashayar Khajavi and Wesley Chung and Mo Chen and Sharan Vaswani", "abstract": "  Loss of plasticity in deep neural networks is the gradual reduction in a\nmodel's capacity to incrementally learn and has been identified as a key\nobstacle to learning in non-stationary problem settings. Recent work has shown\nthat deep linear networks tend to be resilient towards loss of plasticity.\nMotivated by this observation, we propose Adaptive Linearization (AdaLin), a\ngeneral approach that dynamically adapts each neuron's activation function to\nmitigate plasticity loss. Unlike prior methods that rely on regularization or\nperiodic resets, AdaLin equips every neuron with a learnable parameter and a\ngating mechanism that injects linearity into the activation function based on\nits gradient flow. This adaptive modulation ensures sufficient gradient signal\nand sustains continual learning without introducing additional hyperparameters\nor requiring explicit task boundaries. When used with conventional activation\nfunctions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can\nsignificantly improve performance on standard benchmarks, including Random\nLabel and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split\nCIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such\nas class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in\nmitigating plasticity loss in off-policy reinforcement learning agents. We\nperform a systematic set of ablations that show that neuron-level adaptation is\ncrucial for good performance and analyze a number of metrics in the network\nthat might be correlated to loss of plasticity.\n", "link": "http://arxiv.org/abs/2505.09486v1", "date": "2025-05-14", "relevancy": 2.0267, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.528}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4954}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preserving%20Plasticity%20in%20Continual%20Learning%20with%20Adaptive%20Linearity%0A%20%20Injection&body=Title%3A%20Preserving%20Plasticity%20in%20Continual%20Learning%20with%20Adaptive%20Linearity%0A%20%20Injection%0AAuthor%3A%20Seyed%20Roozbeh%20Razavi%20Rohani%20and%20Khashayar%20Khajavi%20and%20Wesley%20Chung%20and%20Mo%20Chen%20and%20Sharan%20Vaswani%0AAbstract%3A%20%20%20Loss%20of%20plasticity%20in%20deep%20neural%20networks%20is%20the%20gradual%20reduction%20in%20a%0Amodel%27s%20capacity%20to%20incrementally%20learn%20and%20has%20been%20identified%20as%20a%20key%0Aobstacle%20to%20learning%20in%20non-stationary%20problem%20settings.%20Recent%20work%20has%20shown%0Athat%20deep%20linear%20networks%20tend%20to%20be%20resilient%20towards%20loss%20of%20plasticity.%0AMotivated%20by%20this%20observation%2C%20we%20propose%20Adaptive%20Linearization%20%28AdaLin%29%2C%20a%0Ageneral%20approach%20that%20dynamically%20adapts%20each%20neuron%27s%20activation%20function%20to%0Amitigate%20plasticity%20loss.%20Unlike%20prior%20methods%20that%20rely%20on%20regularization%20or%0Aperiodic%20resets%2C%20AdaLin%20equips%20every%20neuron%20with%20a%20learnable%20parameter%20and%20a%0Agating%20mechanism%20that%20injects%20linearity%20into%20the%20activation%20function%20based%20on%0Aits%20gradient%20flow.%20This%20adaptive%20modulation%20ensures%20sufficient%20gradient%20signal%0Aand%20sustains%20continual%20learning%20without%20introducing%20additional%20hyperparameters%0Aor%20requiring%20explicit%20task%20boundaries.%20When%20used%20with%20conventional%20activation%0Afunctions%20like%20ReLU%2C%20Tanh%2C%20and%20GeLU%2C%20we%20demonstrate%20that%20AdaLin%20can%0Asignificantly%20improve%20performance%20on%20standard%20benchmarks%2C%20including%20Random%0ALabel%20and%20Permuted%20MNIST%2C%20Random%20Label%20and%20Shuffled%20CIFAR-10%2C%20and%20Class-Split%0ACIFAR-100.%20Furthermore%2C%20its%20efficacy%20is%20shown%20in%20more%20complex%20scenarios%2C%20such%0Aas%20class-incremental%20learning%20on%20CIFAR-100%20with%20a%20ResNet-18%20backbone%2C%20and%20in%0Amitigating%20plasticity%20loss%20in%20off-policy%20reinforcement%20learning%20agents.%20We%0Aperform%20a%20systematic%20set%20of%20ablations%20that%20show%20that%20neuron-level%20adaptation%20is%0Acrucial%20for%20good%20performance%20and%20analyze%20a%20number%20of%20metrics%20in%20the%20network%0Athat%20might%20be%20correlated%20to%20loss%20of%20plasticity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreserving%2520Plasticity%2520in%2520Continual%2520Learning%2520with%2520Adaptive%2520Linearity%250A%2520%2520Injection%26entry.906535625%3DSeyed%2520Roozbeh%2520Razavi%2520Rohani%2520and%2520Khashayar%2520Khajavi%2520and%2520Wesley%2520Chung%2520and%2520Mo%2520Chen%2520and%2520Sharan%2520Vaswani%26entry.1292438233%3D%2520%2520Loss%2520of%2520plasticity%2520in%2520deep%2520neural%2520networks%2520is%2520the%2520gradual%2520reduction%2520in%2520a%250Amodel%2527s%2520capacity%2520to%2520incrementally%2520learn%2520and%2520has%2520been%2520identified%2520as%2520a%2520key%250Aobstacle%2520to%2520learning%2520in%2520non-stationary%2520problem%2520settings.%2520Recent%2520work%2520has%2520shown%250Athat%2520deep%2520linear%2520networks%2520tend%2520to%2520be%2520resilient%2520towards%2520loss%2520of%2520plasticity.%250AMotivated%2520by%2520this%2520observation%252C%2520we%2520propose%2520Adaptive%2520Linearization%2520%2528AdaLin%2529%252C%2520a%250Ageneral%2520approach%2520that%2520dynamically%2520adapts%2520each%2520neuron%2527s%2520activation%2520function%2520to%250Amitigate%2520plasticity%2520loss.%2520Unlike%2520prior%2520methods%2520that%2520rely%2520on%2520regularization%2520or%250Aperiodic%2520resets%252C%2520AdaLin%2520equips%2520every%2520neuron%2520with%2520a%2520learnable%2520parameter%2520and%2520a%250Agating%2520mechanism%2520that%2520injects%2520linearity%2520into%2520the%2520activation%2520function%2520based%2520on%250Aits%2520gradient%2520flow.%2520This%2520adaptive%2520modulation%2520ensures%2520sufficient%2520gradient%2520signal%250Aand%2520sustains%2520continual%2520learning%2520without%2520introducing%2520additional%2520hyperparameters%250Aor%2520requiring%2520explicit%2520task%2520boundaries.%2520When%2520used%2520with%2520conventional%2520activation%250Afunctions%2520like%2520ReLU%252C%2520Tanh%252C%2520and%2520GeLU%252C%2520we%2520demonstrate%2520that%2520AdaLin%2520can%250Asignificantly%2520improve%2520performance%2520on%2520standard%2520benchmarks%252C%2520including%2520Random%250ALabel%2520and%2520Permuted%2520MNIST%252C%2520Random%2520Label%2520and%2520Shuffled%2520CIFAR-10%252C%2520and%2520Class-Split%250ACIFAR-100.%2520Furthermore%252C%2520its%2520efficacy%2520is%2520shown%2520in%2520more%2520complex%2520scenarios%252C%2520such%250Aas%2520class-incremental%2520learning%2520on%2520CIFAR-100%2520with%2520a%2520ResNet-18%2520backbone%252C%2520and%2520in%250Amitigating%2520plasticity%2520loss%2520in%2520off-policy%2520reinforcement%2520learning%2520agents.%2520We%250Aperform%2520a%2520systematic%2520set%2520of%2520ablations%2520that%2520show%2520that%2520neuron-level%2520adaptation%2520is%250Acrucial%2520for%2520good%2520performance%2520and%2520analyze%2520a%2520number%2520of%2520metrics%2520in%2520the%2520network%250Athat%2520might%2520be%2520correlated%2520to%2520loss%2520of%2520plasticity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preserving%20Plasticity%20in%20Continual%20Learning%20with%20Adaptive%20Linearity%0A%20%20Injection&entry.906535625=Seyed%20Roozbeh%20Razavi%20Rohani%20and%20Khashayar%20Khajavi%20and%20Wesley%20Chung%20and%20Mo%20Chen%20and%20Sharan%20Vaswani&entry.1292438233=%20%20Loss%20of%20plasticity%20in%20deep%20neural%20networks%20is%20the%20gradual%20reduction%20in%20a%0Amodel%27s%20capacity%20to%20incrementally%20learn%20and%20has%20been%20identified%20as%20a%20key%0Aobstacle%20to%20learning%20in%20non-stationary%20problem%20settings.%20Recent%20work%20has%20shown%0Athat%20deep%20linear%20networks%20tend%20to%20be%20resilient%20towards%20loss%20of%20plasticity.%0AMotivated%20by%20this%20observation%2C%20we%20propose%20Adaptive%20Linearization%20%28AdaLin%29%2C%20a%0Ageneral%20approach%20that%20dynamically%20adapts%20each%20neuron%27s%20activation%20function%20to%0Amitigate%20plasticity%20loss.%20Unlike%20prior%20methods%20that%20rely%20on%20regularization%20or%0Aperiodic%20resets%2C%20AdaLin%20equips%20every%20neuron%20with%20a%20learnable%20parameter%20and%20a%0Agating%20mechanism%20that%20injects%20linearity%20into%20the%20activation%20function%20based%20on%0Aits%20gradient%20flow.%20This%20adaptive%20modulation%20ensures%20sufficient%20gradient%20signal%0Aand%20sustains%20continual%20learning%20without%20introducing%20additional%20hyperparameters%0Aor%20requiring%20explicit%20task%20boundaries.%20When%20used%20with%20conventional%20activation%0Afunctions%20like%20ReLU%2C%20Tanh%2C%20and%20GeLU%2C%20we%20demonstrate%20that%20AdaLin%20can%0Asignificantly%20improve%20performance%20on%20standard%20benchmarks%2C%20including%20Random%0ALabel%20and%20Permuted%20MNIST%2C%20Random%20Label%20and%20Shuffled%20CIFAR-10%2C%20and%20Class-Split%0ACIFAR-100.%20Furthermore%2C%20its%20efficacy%20is%20shown%20in%20more%20complex%20scenarios%2C%20such%0Aas%20class-incremental%20learning%20on%20CIFAR-100%20with%20a%20ResNet-18%20backbone%2C%20and%20in%0Amitigating%20plasticity%20loss%20in%20off-policy%20reinforcement%20learning%20agents.%20We%0Aperform%20a%20systematic%20set%20of%20ablations%20that%20show%20that%20neuron-level%20adaptation%20is%0Acrucial%20for%20good%20performance%20and%20analyze%20a%20number%20of%20metrics%20in%20the%20network%0Athat%20might%20be%20correlated%20to%20loss%20of%20plasticity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09486v1&entry.124074799=Read"},
{"title": "Detecting Multimedia Generated by Large AI Models: A Survey", "author": "Li Lin and Neeraj Gupta and Yue Zhang and Hainan Ren and Chun-Hao Liu and Feng Ding and Xin Wang and Xin Li and Luisa Verdoliva and Shu Hu", "abstract": "  The rapid advancement of Large AI Models (LAIMs), particularly diffusion\nmodels and large language models, has marked a new era where AI-generated\nmultimedia is increasingly integrated into various aspects of daily life.\nAlthough beneficial in numerous fields, this content presents significant\nrisks, including potential misuse, societal disruptions, and ethical concerns.\nConsequently, detecting multimedia generated by LAIMs has become crucial, with\na marked rise in related research. Despite this, there remains a notable gap in\nsystematic surveys that focus specifically on detecting LAIM-generated\nmultimedia. Addressing this, we provide the first survey to comprehensively\ncover existing research on detecting multimedia (such as text, images, videos,\naudio, and multimodal content) created by LAIMs. Specifically, we introduce a\nnovel taxonomy for detection methods, categorized by media modality, and\naligned with two perspectives: pure detection (aiming to enhance detection\nperformance) and beyond detection (adding attributes like generalizability,\nrobustness, and interpretability to detectors). Additionally, we have presented\na brief overview of generation mechanisms, public datasets, online detection\ntools, and evaluation metrics to provide a valuable resource for researchers\nand practitioners in this field. Most importantly, we offer a focused analysis\nfrom a social media perspective to highlight their broader societal impact.\nFurthermore, we identify current challenges in detection and propose directions\nfor future research that address unexplored, ongoing, and emerging issues in\ndetecting multimedia generated by LAIMs. Our aim for this survey is to fill an\nacademic gap and contribute to global AI security efforts, helping to ensure\nthe integrity of information in the digital realm. The project link is\nhttps://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.\n", "link": "http://arxiv.org/abs/2402.00045v5", "date": "2025-05-14", "relevancy": 2.0195, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.536}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5098}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Multimedia%20Generated%20by%20Large%20AI%20Models%3A%20A%20Survey&body=Title%3A%20Detecting%20Multimedia%20Generated%20by%20Large%20AI%20Models%3A%20A%20Survey%0AAuthor%3A%20Li%20Lin%20and%20Neeraj%20Gupta%20and%20Yue%20Zhang%20and%20Hainan%20Ren%20and%20Chun-Hao%20Liu%20and%20Feng%20Ding%20and%20Xin%20Wang%20and%20Xin%20Li%20and%20Luisa%20Verdoliva%20and%20Shu%20Hu%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Large%20AI%20Models%20%28LAIMs%29%2C%20particularly%20diffusion%0Amodels%20and%20large%20language%20models%2C%20has%20marked%20a%20new%20era%20where%20AI-generated%0Amultimedia%20is%20increasingly%20integrated%20into%20various%20aspects%20of%20daily%20life.%0AAlthough%20beneficial%20in%20numerous%20fields%2C%20this%20content%20presents%20significant%0Arisks%2C%20including%20potential%20misuse%2C%20societal%20disruptions%2C%20and%20ethical%20concerns.%0AConsequently%2C%20detecting%20multimedia%20generated%20by%20LAIMs%20has%20become%20crucial%2C%20with%0Aa%20marked%20rise%20in%20related%20research.%20Despite%20this%2C%20there%20remains%20a%20notable%20gap%20in%0Asystematic%20surveys%20that%20focus%20specifically%20on%20detecting%20LAIM-generated%0Amultimedia.%20Addressing%20this%2C%20we%20provide%20the%20first%20survey%20to%20comprehensively%0Acover%20existing%20research%20on%20detecting%20multimedia%20%28such%20as%20text%2C%20images%2C%20videos%2C%0Aaudio%2C%20and%20multimodal%20content%29%20created%20by%20LAIMs.%20Specifically%2C%20we%20introduce%20a%0Anovel%20taxonomy%20for%20detection%20methods%2C%20categorized%20by%20media%20modality%2C%20and%0Aaligned%20with%20two%20perspectives%3A%20pure%20detection%20%28aiming%20to%20enhance%20detection%0Aperformance%29%20and%20beyond%20detection%20%28adding%20attributes%20like%20generalizability%2C%0Arobustness%2C%20and%20interpretability%20to%20detectors%29.%20Additionally%2C%20we%20have%20presented%0Aa%20brief%20overview%20of%20generation%20mechanisms%2C%20public%20datasets%2C%20online%20detection%0Atools%2C%20and%20evaluation%20metrics%20to%20provide%20a%20valuable%20resource%20for%20researchers%0Aand%20practitioners%20in%20this%20field.%20Most%20importantly%2C%20we%20offer%20a%20focused%20analysis%0Afrom%20a%20social%20media%20perspective%20to%20highlight%20their%20broader%20societal%20impact.%0AFurthermore%2C%20we%20identify%20current%20challenges%20in%20detection%20and%20propose%20directions%0Afor%20future%20research%20that%20address%20unexplored%2C%20ongoing%2C%20and%20emerging%20issues%20in%0Adetecting%20multimedia%20generated%20by%20LAIMs.%20Our%20aim%20for%20this%20survey%20is%20to%20fill%20an%0Aacademic%20gap%20and%20contribute%20to%20global%20AI%20security%20efforts%2C%20helping%20to%20ensure%0Athe%20integrity%20of%20information%20in%20the%20digital%20realm.%20The%20project%20link%20is%0Ahttps%3A//github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00045v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Multimedia%2520Generated%2520by%2520Large%2520AI%2520Models%253A%2520A%2520Survey%26entry.906535625%3DLi%2520Lin%2520and%2520Neeraj%2520Gupta%2520and%2520Yue%2520Zhang%2520and%2520Hainan%2520Ren%2520and%2520Chun-Hao%2520Liu%2520and%2520Feng%2520Ding%2520and%2520Xin%2520Wang%2520and%2520Xin%2520Li%2520and%2520Luisa%2520Verdoliva%2520and%2520Shu%2520Hu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520Large%2520AI%2520Models%2520%2528LAIMs%2529%252C%2520particularly%2520diffusion%250Amodels%2520and%2520large%2520language%2520models%252C%2520has%2520marked%2520a%2520new%2520era%2520where%2520AI-generated%250Amultimedia%2520is%2520increasingly%2520integrated%2520into%2520various%2520aspects%2520of%2520daily%2520life.%250AAlthough%2520beneficial%2520in%2520numerous%2520fields%252C%2520this%2520content%2520presents%2520significant%250Arisks%252C%2520including%2520potential%2520misuse%252C%2520societal%2520disruptions%252C%2520and%2520ethical%2520concerns.%250AConsequently%252C%2520detecting%2520multimedia%2520generated%2520by%2520LAIMs%2520has%2520become%2520crucial%252C%2520with%250Aa%2520marked%2520rise%2520in%2520related%2520research.%2520Despite%2520this%252C%2520there%2520remains%2520a%2520notable%2520gap%2520in%250Asystematic%2520surveys%2520that%2520focus%2520specifically%2520on%2520detecting%2520LAIM-generated%250Amultimedia.%2520Addressing%2520this%252C%2520we%2520provide%2520the%2520first%2520survey%2520to%2520comprehensively%250Acover%2520existing%2520research%2520on%2520detecting%2520multimedia%2520%2528such%2520as%2520text%252C%2520images%252C%2520videos%252C%250Aaudio%252C%2520and%2520multimodal%2520content%2529%2520created%2520by%2520LAIMs.%2520Specifically%252C%2520we%2520introduce%2520a%250Anovel%2520taxonomy%2520for%2520detection%2520methods%252C%2520categorized%2520by%2520media%2520modality%252C%2520and%250Aaligned%2520with%2520two%2520perspectives%253A%2520pure%2520detection%2520%2528aiming%2520to%2520enhance%2520detection%250Aperformance%2529%2520and%2520beyond%2520detection%2520%2528adding%2520attributes%2520like%2520generalizability%252C%250Arobustness%252C%2520and%2520interpretability%2520to%2520detectors%2529.%2520Additionally%252C%2520we%2520have%2520presented%250Aa%2520brief%2520overview%2520of%2520generation%2520mechanisms%252C%2520public%2520datasets%252C%2520online%2520detection%250Atools%252C%2520and%2520evaluation%2520metrics%2520to%2520provide%2520a%2520valuable%2520resource%2520for%2520researchers%250Aand%2520practitioners%2520in%2520this%2520field.%2520Most%2520importantly%252C%2520we%2520offer%2520a%2520focused%2520analysis%250Afrom%2520a%2520social%2520media%2520perspective%2520to%2520highlight%2520their%2520broader%2520societal%2520impact.%250AFurthermore%252C%2520we%2520identify%2520current%2520challenges%2520in%2520detection%2520and%2520propose%2520directions%250Afor%2520future%2520research%2520that%2520address%2520unexplored%252C%2520ongoing%252C%2520and%2520emerging%2520issues%2520in%250Adetecting%2520multimedia%2520generated%2520by%2520LAIMs.%2520Our%2520aim%2520for%2520this%2520survey%2520is%2520to%2520fill%2520an%250Aacademic%2520gap%2520and%2520contribute%2520to%2520global%2520AI%2520security%2520efforts%252C%2520helping%2520to%2520ensure%250Athe%2520integrity%2520of%2520information%2520in%2520the%2520digital%2520realm.%2520The%2520project%2520link%2520is%250Ahttps%253A//github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00045v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Multimedia%20Generated%20by%20Large%20AI%20Models%3A%20A%20Survey&entry.906535625=Li%20Lin%20and%20Neeraj%20Gupta%20and%20Yue%20Zhang%20and%20Hainan%20Ren%20and%20Chun-Hao%20Liu%20and%20Feng%20Ding%20and%20Xin%20Wang%20and%20Xin%20Li%20and%20Luisa%20Verdoliva%20and%20Shu%20Hu&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Large%20AI%20Models%20%28LAIMs%29%2C%20particularly%20diffusion%0Amodels%20and%20large%20language%20models%2C%20has%20marked%20a%20new%20era%20where%20AI-generated%0Amultimedia%20is%20increasingly%20integrated%20into%20various%20aspects%20of%20daily%20life.%0AAlthough%20beneficial%20in%20numerous%20fields%2C%20this%20content%20presents%20significant%0Arisks%2C%20including%20potential%20misuse%2C%20societal%20disruptions%2C%20and%20ethical%20concerns.%0AConsequently%2C%20detecting%20multimedia%20generated%20by%20LAIMs%20has%20become%20crucial%2C%20with%0Aa%20marked%20rise%20in%20related%20research.%20Despite%20this%2C%20there%20remains%20a%20notable%20gap%20in%0Asystematic%20surveys%20that%20focus%20specifically%20on%20detecting%20LAIM-generated%0Amultimedia.%20Addressing%20this%2C%20we%20provide%20the%20first%20survey%20to%20comprehensively%0Acover%20existing%20research%20on%20detecting%20multimedia%20%28such%20as%20text%2C%20images%2C%20videos%2C%0Aaudio%2C%20and%20multimodal%20content%29%20created%20by%20LAIMs.%20Specifically%2C%20we%20introduce%20a%0Anovel%20taxonomy%20for%20detection%20methods%2C%20categorized%20by%20media%20modality%2C%20and%0Aaligned%20with%20two%20perspectives%3A%20pure%20detection%20%28aiming%20to%20enhance%20detection%0Aperformance%29%20and%20beyond%20detection%20%28adding%20attributes%20like%20generalizability%2C%0Arobustness%2C%20and%20interpretability%20to%20detectors%29.%20Additionally%2C%20we%20have%20presented%0Aa%20brief%20overview%20of%20generation%20mechanisms%2C%20public%20datasets%2C%20online%20detection%0Atools%2C%20and%20evaluation%20metrics%20to%20provide%20a%20valuable%20resource%20for%20researchers%0Aand%20practitioners%20in%20this%20field.%20Most%20importantly%2C%20we%20offer%20a%20focused%20analysis%0Afrom%20a%20social%20media%20perspective%20to%20highlight%20their%20broader%20societal%20impact.%0AFurthermore%2C%20we%20identify%20current%20challenges%20in%20detection%20and%20propose%20directions%0Afor%20future%20research%20that%20address%20unexplored%2C%20ongoing%2C%20and%20emerging%20issues%20in%0Adetecting%20multimedia%20generated%20by%20LAIMs.%20Our%20aim%20for%20this%20survey%20is%20to%20fill%20an%0Aacademic%20gap%20and%20contribute%20to%20global%20AI%20security%20efforts%2C%20helping%20to%20ensure%0Athe%20integrity%20of%20information%20in%20the%20digital%20realm.%20The%20project%20link%20is%0Ahttps%3A//github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00045v5&entry.124074799=Read"},
{"title": "Llama-Nemotron: Efficient Reasoning Models", "author": "Akhiad Bercovich and Itay Levy and Izik Golan and Mohammad Dabbah and Ran El-Yaniv and Omri Puny and Ido Galil and Zach Moshe and Tomer Ronen and Najeeb Nabwani and Ido Shahaf and Oren Tropp and Ehud Karpas and Ran Zilberstein and Jiaqi Zeng and Soumye Singhal and Alexander Bukharin and Yian Zhang and Tugrul Konuk and Gerald Shen and Ameya Sunil Mahabaleshwarkar and Bilal Kartal and Yoshi Suhara and Olivier Delalleau and Zijia Chen and Zhilin Wang and David Mosallanezhad and Adi Renduchintala and Haifeng Qian and Dima Rekesh and Fei Jia and Somshubra Majumdar and Vahid Noroozi and Wasi Uddin Ahmad and Sean Narenthiran and Aleksander Ficek and Mehrzad Samadi and Jocelyn Huang and Siddhartha Jain and Igor Gitman and Ivan Moshkov and Wei Du and Shubham Toshniwal and George Armstrong and Branislav Kisacanin and Matvei Novikov and Daria Gitman and Evelina Bakhturina and Jane Polak Scowcroft and John Kamalu and Dan Su and Kezhi Kong and Markus Kliegl and Rabeeh Karimi and Ying Lin and Sanjeev Satheesh and Jupinder Parmar and Pritam Gundecha and Brandon Norick and Joseph Jennings and Shrimai Prabhumoye and Syeda Nahida Akter and Mostofa Patwary and Abhinav Khattar and Deepak Narayanan and Roger Waleffe and Jimmy Zhang and Bor-Yiing Su and Guyue Huang and Terry Kong and Parth Chadha and Sahil Jain and Christine Harvey and Elad Segal and Jining Huang and Sergey Kashirsky and Robert McQueen and Izzy Putterman and George Lam and Arun Venkatesan and Sherry Wu and Vinh Nguyen and Manoj Kilaru and Andrew Wang and Anna Warno and Abhilash Somasamudramath and Sandip Bhaskar and Maka Dong and Nave Assaf and Shahar Mor and Omer Ullman Argov and Scot Junkin and Oleksandr Romanenko and Pedro Larroy and Monika Katariya and Marco Rovinelli and Viji Balas and Nicholas Edelman and Anahita Bhiwandiwalla and Muthu Subramaniam and Smita Ithape and Karthik Ramamoorthy and Yuting Wu and Suguna Varshini Velury and Omri Almog and Joyjit Daw and Denys Fridman and Erick Galinkin and Michael Evans and Shaona Ghosh and Katherine Luna and Leon Derczynski and Nikki Pope and Eileen Long and Seth Schneider and Guillermo Siman and Tomasz Grzegorzek and Pablo Ribalta and Monika Katariya and Chris Alexiuk and Joey Conway and Trisha Saar and Ann Guan and Krzysztof Pawelec and Shyamala Prayaga and Oleksii Kuchaiev and Boris Ginsburg and Oluwatobi Olabiyi and Kari Briski and Jonathan Cohen and Bryan Catanzaro and Jonah Alben and Yonatan Geifman and Eric Chung", "abstract": "  We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.\n", "link": "http://arxiv.org/abs/2505.00949v3", "date": "2025-05-14", "relevancy": 2.0179, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Llama-Nemotron%3A%20Efficient%20Reasoning%20Models&body=Title%3A%20Llama-Nemotron%3A%20Efficient%20Reasoning%20Models%0AAuthor%3A%20Akhiad%20Bercovich%20and%20Itay%20Levy%20and%20Izik%20Golan%20and%20Mohammad%20Dabbah%20and%20Ran%20El-Yaniv%20and%20Omri%20Puny%20and%20Ido%20Galil%20and%20Zach%20Moshe%20and%20Tomer%20Ronen%20and%20Najeeb%20Nabwani%20and%20Ido%20Shahaf%20and%20Oren%20Tropp%20and%20Ehud%20Karpas%20and%20Ran%20Zilberstein%20and%20Jiaqi%20Zeng%20and%20Soumye%20Singhal%20and%20Alexander%20Bukharin%20and%20Yian%20Zhang%20and%20Tugrul%20Konuk%20and%20Gerald%20Shen%20and%20Ameya%20Sunil%20Mahabaleshwarkar%20and%20Bilal%20Kartal%20and%20Yoshi%20Suhara%20and%20Olivier%20Delalleau%20and%20Zijia%20Chen%20and%20Zhilin%20Wang%20and%20David%20Mosallanezhad%20and%20Adi%20Renduchintala%20and%20Haifeng%20Qian%20and%20Dima%20Rekesh%20and%20Fei%20Jia%20and%20Somshubra%20Majumdar%20and%20Vahid%20Noroozi%20and%20Wasi%20Uddin%20Ahmad%20and%20Sean%20Narenthiran%20and%20Aleksander%20Ficek%20and%20Mehrzad%20Samadi%20and%20Jocelyn%20Huang%20and%20Siddhartha%20Jain%20and%20Igor%20Gitman%20and%20Ivan%20Moshkov%20and%20Wei%20Du%20and%20Shubham%20Toshniwal%20and%20George%20Armstrong%20and%20Branislav%20Kisacanin%20and%20Matvei%20Novikov%20and%20Daria%20Gitman%20and%20Evelina%20Bakhturina%20and%20Jane%20Polak%20Scowcroft%20and%20John%20Kamalu%20and%20Dan%20Su%20and%20Kezhi%20Kong%20and%20Markus%20Kliegl%20and%20Rabeeh%20Karimi%20and%20Ying%20Lin%20and%20Sanjeev%20Satheesh%20and%20Jupinder%20Parmar%20and%20Pritam%20Gundecha%20and%20Brandon%20Norick%20and%20Joseph%20Jennings%20and%20Shrimai%20Prabhumoye%20and%20Syeda%20Nahida%20Akter%20and%20Mostofa%20Patwary%20and%20Abhinav%20Khattar%20and%20Deepak%20Narayanan%20and%20Roger%20Waleffe%20and%20Jimmy%20Zhang%20and%20Bor-Yiing%20Su%20and%20Guyue%20Huang%20and%20Terry%20Kong%20and%20Parth%20Chadha%20and%20Sahil%20Jain%20and%20Christine%20Harvey%20and%20Elad%20Segal%20and%20Jining%20Huang%20and%20Sergey%20Kashirsky%20and%20Robert%20McQueen%20and%20Izzy%20Putterman%20and%20George%20Lam%20and%20Arun%20Venkatesan%20and%20Sherry%20Wu%20and%20Vinh%20Nguyen%20and%20Manoj%20Kilaru%20and%20Andrew%20Wang%20and%20Anna%20Warno%20and%20Abhilash%20Somasamudramath%20and%20Sandip%20Bhaskar%20and%20Maka%20Dong%20and%20Nave%20Assaf%20and%20Shahar%20Mor%20and%20Omer%20Ullman%20Argov%20and%20Scot%20Junkin%20and%20Oleksandr%20Romanenko%20and%20Pedro%20Larroy%20and%20Monika%20Katariya%20and%20Marco%20Rovinelli%20and%20Viji%20Balas%20and%20Nicholas%20Edelman%20and%20Anahita%20Bhiwandiwalla%20and%20Muthu%20Subramaniam%20and%20Smita%20Ithape%20and%20Karthik%20Ramamoorthy%20and%20Yuting%20Wu%20and%20Suguna%20Varshini%20Velury%20and%20Omri%20Almog%20and%20Joyjit%20Daw%20and%20Denys%20Fridman%20and%20Erick%20Galinkin%20and%20Michael%20Evans%20and%20Shaona%20Ghosh%20and%20Katherine%20Luna%20and%20Leon%20Derczynski%20and%20Nikki%20Pope%20and%20Eileen%20Long%20and%20Seth%20Schneider%20and%20Guillermo%20Siman%20and%20Tomasz%20Grzegorzek%20and%20Pablo%20Ribalta%20and%20Monika%20Katariya%20and%20Chris%20Alexiuk%20and%20Joey%20Conway%20and%20Trisha%20Saar%20and%20Ann%20Guan%20and%20Krzysztof%20Pawelec%20and%20Shyamala%20Prayaga%20and%20Oleksii%20Kuchaiev%20and%20Boris%20Ginsburg%20and%20Oluwatobi%20Olabiyi%20and%20Kari%20Briski%20and%20Jonathan%20Cohen%20and%20Bryan%20Catanzaro%20and%20Jonah%20Alben%20and%20Yonatan%20Geifman%20and%20Eric%20Chung%0AAbstract%3A%20%20%20We%20introduce%20the%20Llama-Nemotron%20series%20of%20models%2C%20an%20open%20family%20of%0Aheterogeneous%20reasoning%20models%20that%20deliver%20exceptional%20reasoning%20capabilities%2C%0Ainference%20efficiency%2C%20and%20an%20open%20license%20for%20enterprise%20use.%20The%20family%20comes%0Ain%20three%20sizes%20--%20Nano%20%288B%29%2C%20Super%20%2849B%29%2C%20and%20Ultra%20%28253B%29%20--%20and%20performs%0Acompetitively%20with%20state-of-the-art%20reasoning%20models%20such%20as%20DeepSeek-R1%20while%0Aoffering%20superior%20inference%20throughput%20and%20memory%20efficiency.%20In%20this%20report%2C%0Awe%20discuss%20the%20training%20procedure%20for%20these%20models%2C%20which%20entails%20using%20neural%0Aarchitecture%20search%20from%20Llama%203%20models%20for%20accelerated%20inference%2C%20knowledge%0Adistillation%2C%20and%20continued%20pretraining%2C%20followed%20by%20a%20reasoning-focused%0Apost-training%20stage%20consisting%20of%20two%20main%20parts%3A%20supervised%20fine-tuning%20and%0Alarge%20scale%20reinforcement%20learning.%20Llama-Nemotron%20models%20are%20the%20first%0Aopen-source%20models%20to%20support%20a%20dynamic%20reasoning%20toggle%2C%20allowing%20users%20to%0Aswitch%20between%20standard%20chat%20and%20reasoning%20modes%20during%20inference.%20To%20further%0Asupport%20open%20research%20and%20facilitate%20model%20development%2C%20we%20provide%20the%0Afollowing%20resources%3A%201.%20We%20release%20the%20Llama-Nemotron%20reasoning%20models%20--%0ALN-Nano%2C%20LN-Super%2C%20and%20LN-Ultra%20--%20under%20the%20commercially%20permissive%20NVIDIA%0AOpen%20Model%20License%20Agreement.%202.%20We%20release%20the%20complete%20post-training%20dataset%3A%0ALlama-Nemotron-Post-Training-Dataset.%203.%20We%20also%20release%20our%20training%0Acodebases%3A%20NeMo%2C%20NeMo-Aligner%2C%20and%20Megatron-LM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00949v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLlama-Nemotron%253A%2520Efficient%2520Reasoning%2520Models%26entry.906535625%3DAkhiad%2520Bercovich%2520and%2520Itay%2520Levy%2520and%2520Izik%2520Golan%2520and%2520Mohammad%2520Dabbah%2520and%2520Ran%2520El-Yaniv%2520and%2520Omri%2520Puny%2520and%2520Ido%2520Galil%2520and%2520Zach%2520Moshe%2520and%2520Tomer%2520Ronen%2520and%2520Najeeb%2520Nabwani%2520and%2520Ido%2520Shahaf%2520and%2520Oren%2520Tropp%2520and%2520Ehud%2520Karpas%2520and%2520Ran%2520Zilberstein%2520and%2520Jiaqi%2520Zeng%2520and%2520Soumye%2520Singhal%2520and%2520Alexander%2520Bukharin%2520and%2520Yian%2520Zhang%2520and%2520Tugrul%2520Konuk%2520and%2520Gerald%2520Shen%2520and%2520Ameya%2520Sunil%2520Mahabaleshwarkar%2520and%2520Bilal%2520Kartal%2520and%2520Yoshi%2520Suhara%2520and%2520Olivier%2520Delalleau%2520and%2520Zijia%2520Chen%2520and%2520Zhilin%2520Wang%2520and%2520David%2520Mosallanezhad%2520and%2520Adi%2520Renduchintala%2520and%2520Haifeng%2520Qian%2520and%2520Dima%2520Rekesh%2520and%2520Fei%2520Jia%2520and%2520Somshubra%2520Majumdar%2520and%2520Vahid%2520Noroozi%2520and%2520Wasi%2520Uddin%2520Ahmad%2520and%2520Sean%2520Narenthiran%2520and%2520Aleksander%2520Ficek%2520and%2520Mehrzad%2520Samadi%2520and%2520Jocelyn%2520Huang%2520and%2520Siddhartha%2520Jain%2520and%2520Igor%2520Gitman%2520and%2520Ivan%2520Moshkov%2520and%2520Wei%2520Du%2520and%2520Shubham%2520Toshniwal%2520and%2520George%2520Armstrong%2520and%2520Branislav%2520Kisacanin%2520and%2520Matvei%2520Novikov%2520and%2520Daria%2520Gitman%2520and%2520Evelina%2520Bakhturina%2520and%2520Jane%2520Polak%2520Scowcroft%2520and%2520John%2520Kamalu%2520and%2520Dan%2520Su%2520and%2520Kezhi%2520Kong%2520and%2520Markus%2520Kliegl%2520and%2520Rabeeh%2520Karimi%2520and%2520Ying%2520Lin%2520and%2520Sanjeev%2520Satheesh%2520and%2520Jupinder%2520Parmar%2520and%2520Pritam%2520Gundecha%2520and%2520Brandon%2520Norick%2520and%2520Joseph%2520Jennings%2520and%2520Shrimai%2520Prabhumoye%2520and%2520Syeda%2520Nahida%2520Akter%2520and%2520Mostofa%2520Patwary%2520and%2520Abhinav%2520Khattar%2520and%2520Deepak%2520Narayanan%2520and%2520Roger%2520Waleffe%2520and%2520Jimmy%2520Zhang%2520and%2520Bor-Yiing%2520Su%2520and%2520Guyue%2520Huang%2520and%2520Terry%2520Kong%2520and%2520Parth%2520Chadha%2520and%2520Sahil%2520Jain%2520and%2520Christine%2520Harvey%2520and%2520Elad%2520Segal%2520and%2520Jining%2520Huang%2520and%2520Sergey%2520Kashirsky%2520and%2520Robert%2520McQueen%2520and%2520Izzy%2520Putterman%2520and%2520George%2520Lam%2520and%2520Arun%2520Venkatesan%2520and%2520Sherry%2520Wu%2520and%2520Vinh%2520Nguyen%2520and%2520Manoj%2520Kilaru%2520and%2520Andrew%2520Wang%2520and%2520Anna%2520Warno%2520and%2520Abhilash%2520Somasamudramath%2520and%2520Sandip%2520Bhaskar%2520and%2520Maka%2520Dong%2520and%2520Nave%2520Assaf%2520and%2520Shahar%2520Mor%2520and%2520Omer%2520Ullman%2520Argov%2520and%2520Scot%2520Junkin%2520and%2520Oleksandr%2520Romanenko%2520and%2520Pedro%2520Larroy%2520and%2520Monika%2520Katariya%2520and%2520Marco%2520Rovinelli%2520and%2520Viji%2520Balas%2520and%2520Nicholas%2520Edelman%2520and%2520Anahita%2520Bhiwandiwalla%2520and%2520Muthu%2520Subramaniam%2520and%2520Smita%2520Ithape%2520and%2520Karthik%2520Ramamoorthy%2520and%2520Yuting%2520Wu%2520and%2520Suguna%2520Varshini%2520Velury%2520and%2520Omri%2520Almog%2520and%2520Joyjit%2520Daw%2520and%2520Denys%2520Fridman%2520and%2520Erick%2520Galinkin%2520and%2520Michael%2520Evans%2520and%2520Shaona%2520Ghosh%2520and%2520Katherine%2520Luna%2520and%2520Leon%2520Derczynski%2520and%2520Nikki%2520Pope%2520and%2520Eileen%2520Long%2520and%2520Seth%2520Schneider%2520and%2520Guillermo%2520Siman%2520and%2520Tomasz%2520Grzegorzek%2520and%2520Pablo%2520Ribalta%2520and%2520Monika%2520Katariya%2520and%2520Chris%2520Alexiuk%2520and%2520Joey%2520Conway%2520and%2520Trisha%2520Saar%2520and%2520Ann%2520Guan%2520and%2520Krzysztof%2520Pawelec%2520and%2520Shyamala%2520Prayaga%2520and%2520Oleksii%2520Kuchaiev%2520and%2520Boris%2520Ginsburg%2520and%2520Oluwatobi%2520Olabiyi%2520and%2520Kari%2520Briski%2520and%2520Jonathan%2520Cohen%2520and%2520Bryan%2520Catanzaro%2520and%2520Jonah%2520Alben%2520and%2520Yonatan%2520Geifman%2520and%2520Eric%2520Chung%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Llama-Nemotron%2520series%2520of%2520models%252C%2520an%2520open%2520family%2520of%250Aheterogeneous%2520reasoning%2520models%2520that%2520deliver%2520exceptional%2520reasoning%2520capabilities%252C%250Ainference%2520efficiency%252C%2520and%2520an%2520open%2520license%2520for%2520enterprise%2520use.%2520The%2520family%2520comes%250Ain%2520three%2520sizes%2520--%2520Nano%2520%25288B%2529%252C%2520Super%2520%252849B%2529%252C%2520and%2520Ultra%2520%2528253B%2529%2520--%2520and%2520performs%250Acompetitively%2520with%2520state-of-the-art%2520reasoning%2520models%2520such%2520as%2520DeepSeek-R1%2520while%250Aoffering%2520superior%2520inference%2520throughput%2520and%2520memory%2520efficiency.%2520In%2520this%2520report%252C%250Awe%2520discuss%2520the%2520training%2520procedure%2520for%2520these%2520models%252C%2520which%2520entails%2520using%2520neural%250Aarchitecture%2520search%2520from%2520Llama%25203%2520models%2520for%2520accelerated%2520inference%252C%2520knowledge%250Adistillation%252C%2520and%2520continued%2520pretraining%252C%2520followed%2520by%2520a%2520reasoning-focused%250Apost-training%2520stage%2520consisting%2520of%2520two%2520main%2520parts%253A%2520supervised%2520fine-tuning%2520and%250Alarge%2520scale%2520reinforcement%2520learning.%2520Llama-Nemotron%2520models%2520are%2520the%2520first%250Aopen-source%2520models%2520to%2520support%2520a%2520dynamic%2520reasoning%2520toggle%252C%2520allowing%2520users%2520to%250Aswitch%2520between%2520standard%2520chat%2520and%2520reasoning%2520modes%2520during%2520inference.%2520To%2520further%250Asupport%2520open%2520research%2520and%2520facilitate%2520model%2520development%252C%2520we%2520provide%2520the%250Afollowing%2520resources%253A%25201.%2520We%2520release%2520the%2520Llama-Nemotron%2520reasoning%2520models%2520--%250ALN-Nano%252C%2520LN-Super%252C%2520and%2520LN-Ultra%2520--%2520under%2520the%2520commercially%2520permissive%2520NVIDIA%250AOpen%2520Model%2520License%2520Agreement.%25202.%2520We%2520release%2520the%2520complete%2520post-training%2520dataset%253A%250ALlama-Nemotron-Post-Training-Dataset.%25203.%2520We%2520also%2520release%2520our%2520training%250Acodebases%253A%2520NeMo%252C%2520NeMo-Aligner%252C%2520and%2520Megatron-LM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00949v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Llama-Nemotron%3A%20Efficient%20Reasoning%20Models&entry.906535625=Akhiad%20Bercovich%20and%20Itay%20Levy%20and%20Izik%20Golan%20and%20Mohammad%20Dabbah%20and%20Ran%20El-Yaniv%20and%20Omri%20Puny%20and%20Ido%20Galil%20and%20Zach%20Moshe%20and%20Tomer%20Ronen%20and%20Najeeb%20Nabwani%20and%20Ido%20Shahaf%20and%20Oren%20Tropp%20and%20Ehud%20Karpas%20and%20Ran%20Zilberstein%20and%20Jiaqi%20Zeng%20and%20Soumye%20Singhal%20and%20Alexander%20Bukharin%20and%20Yian%20Zhang%20and%20Tugrul%20Konuk%20and%20Gerald%20Shen%20and%20Ameya%20Sunil%20Mahabaleshwarkar%20and%20Bilal%20Kartal%20and%20Yoshi%20Suhara%20and%20Olivier%20Delalleau%20and%20Zijia%20Chen%20and%20Zhilin%20Wang%20and%20David%20Mosallanezhad%20and%20Adi%20Renduchintala%20and%20Haifeng%20Qian%20and%20Dima%20Rekesh%20and%20Fei%20Jia%20and%20Somshubra%20Majumdar%20and%20Vahid%20Noroozi%20and%20Wasi%20Uddin%20Ahmad%20and%20Sean%20Narenthiran%20and%20Aleksander%20Ficek%20and%20Mehrzad%20Samadi%20and%20Jocelyn%20Huang%20and%20Siddhartha%20Jain%20and%20Igor%20Gitman%20and%20Ivan%20Moshkov%20and%20Wei%20Du%20and%20Shubham%20Toshniwal%20and%20George%20Armstrong%20and%20Branislav%20Kisacanin%20and%20Matvei%20Novikov%20and%20Daria%20Gitman%20and%20Evelina%20Bakhturina%20and%20Jane%20Polak%20Scowcroft%20and%20John%20Kamalu%20and%20Dan%20Su%20and%20Kezhi%20Kong%20and%20Markus%20Kliegl%20and%20Rabeeh%20Karimi%20and%20Ying%20Lin%20and%20Sanjeev%20Satheesh%20and%20Jupinder%20Parmar%20and%20Pritam%20Gundecha%20and%20Brandon%20Norick%20and%20Joseph%20Jennings%20and%20Shrimai%20Prabhumoye%20and%20Syeda%20Nahida%20Akter%20and%20Mostofa%20Patwary%20and%20Abhinav%20Khattar%20and%20Deepak%20Narayanan%20and%20Roger%20Waleffe%20and%20Jimmy%20Zhang%20and%20Bor-Yiing%20Su%20and%20Guyue%20Huang%20and%20Terry%20Kong%20and%20Parth%20Chadha%20and%20Sahil%20Jain%20and%20Christine%20Harvey%20and%20Elad%20Segal%20and%20Jining%20Huang%20and%20Sergey%20Kashirsky%20and%20Robert%20McQueen%20and%20Izzy%20Putterman%20and%20George%20Lam%20and%20Arun%20Venkatesan%20and%20Sherry%20Wu%20and%20Vinh%20Nguyen%20and%20Manoj%20Kilaru%20and%20Andrew%20Wang%20and%20Anna%20Warno%20and%20Abhilash%20Somasamudramath%20and%20Sandip%20Bhaskar%20and%20Maka%20Dong%20and%20Nave%20Assaf%20and%20Shahar%20Mor%20and%20Omer%20Ullman%20Argov%20and%20Scot%20Junkin%20and%20Oleksandr%20Romanenko%20and%20Pedro%20Larroy%20and%20Monika%20Katariya%20and%20Marco%20Rovinelli%20and%20Viji%20Balas%20and%20Nicholas%20Edelman%20and%20Anahita%20Bhiwandiwalla%20and%20Muthu%20Subramaniam%20and%20Smita%20Ithape%20and%20Karthik%20Ramamoorthy%20and%20Yuting%20Wu%20and%20Suguna%20Varshini%20Velury%20and%20Omri%20Almog%20and%20Joyjit%20Daw%20and%20Denys%20Fridman%20and%20Erick%20Galinkin%20and%20Michael%20Evans%20and%20Shaona%20Ghosh%20and%20Katherine%20Luna%20and%20Leon%20Derczynski%20and%20Nikki%20Pope%20and%20Eileen%20Long%20and%20Seth%20Schneider%20and%20Guillermo%20Siman%20and%20Tomasz%20Grzegorzek%20and%20Pablo%20Ribalta%20and%20Monika%20Katariya%20and%20Chris%20Alexiuk%20and%20Joey%20Conway%20and%20Trisha%20Saar%20and%20Ann%20Guan%20and%20Krzysztof%20Pawelec%20and%20Shyamala%20Prayaga%20and%20Oleksii%20Kuchaiev%20and%20Boris%20Ginsburg%20and%20Oluwatobi%20Olabiyi%20and%20Kari%20Briski%20and%20Jonathan%20Cohen%20and%20Bryan%20Catanzaro%20and%20Jonah%20Alben%20and%20Yonatan%20Geifman%20and%20Eric%20Chung&entry.1292438233=%20%20We%20introduce%20the%20Llama-Nemotron%20series%20of%20models%2C%20an%20open%20family%20of%0Aheterogeneous%20reasoning%20models%20that%20deliver%20exceptional%20reasoning%20capabilities%2C%0Ainference%20efficiency%2C%20and%20an%20open%20license%20for%20enterprise%20use.%20The%20family%20comes%0Ain%20three%20sizes%20--%20Nano%20%288B%29%2C%20Super%20%2849B%29%2C%20and%20Ultra%20%28253B%29%20--%20and%20performs%0Acompetitively%20with%20state-of-the-art%20reasoning%20models%20such%20as%20DeepSeek-R1%20while%0Aoffering%20superior%20inference%20throughput%20and%20memory%20efficiency.%20In%20this%20report%2C%0Awe%20discuss%20the%20training%20procedure%20for%20these%20models%2C%20which%20entails%20using%20neural%0Aarchitecture%20search%20from%20Llama%203%20models%20for%20accelerated%20inference%2C%20knowledge%0Adistillation%2C%20and%20continued%20pretraining%2C%20followed%20by%20a%20reasoning-focused%0Apost-training%20stage%20consisting%20of%20two%20main%20parts%3A%20supervised%20fine-tuning%20and%0Alarge%20scale%20reinforcement%20learning.%20Llama-Nemotron%20models%20are%20the%20first%0Aopen-source%20models%20to%20support%20a%20dynamic%20reasoning%20toggle%2C%20allowing%20users%20to%0Aswitch%20between%20standard%20chat%20and%20reasoning%20modes%20during%20inference.%20To%20further%0Asupport%20open%20research%20and%20facilitate%20model%20development%2C%20we%20provide%20the%0Afollowing%20resources%3A%201.%20We%20release%20the%20Llama-Nemotron%20reasoning%20models%20--%0ALN-Nano%2C%20LN-Super%2C%20and%20LN-Ultra%20--%20under%20the%20commercially%20permissive%20NVIDIA%0AOpen%20Model%20License%20Agreement.%202.%20We%20release%20the%20complete%20post-training%20dataset%3A%0ALlama-Nemotron-Post-Training-Dataset.%203.%20We%20also%20release%20our%20training%0Acodebases%3A%20NeMo%2C%20NeMo-Aligner%2C%20and%20Megatron-LM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00949v3&entry.124074799=Read"},
{"title": "Deep-SITAR: A SITAR-Based Deep Learning Framework for Growth Curve\n  Modeling via Autoencoders", "author": "Mar\u00eda Alejandra Hern\u00e1ndez and Oscar Rodriguez and Dae-Jin Lee", "abstract": "  Several approaches have been developed to capture the complexity and\nnonlinearity of human growth. One widely used is the Super Imposition by\nTranslation and Rotation (SITAR) model, which has become popular in studies of\nadolescent growth. SITAR is a shape-invariant mixed-effects model that\nrepresents the shared growth pattern of a population using a natural cubic\nspline mean curve while incorporating three subject-specific random effects --\ntiming, size, and growth intensity -- to account for variations among\nindividuals. In this work, we introduce a supervised deep learning framework\nbased on an autoencoder architecture that integrates a deep neural network\n(neural network) with a B-spline model to estimate the SITAR model. In this\napproach, the encoder estimates the random effects for each individual, while\nthe decoder performs a fitting based on B-splines similar to the classic SITAR\nmodel. We refer to this method as the Deep-SITAR model. This innovative\napproach enables the prediction of the random effects of new individuals\nentering a population without requiring a full model re-estimation. As a\nresult, Deep-SITAR offers a powerful approach to predicting growth\ntrajectories, combining the flexibility and efficiency of deep learning with\nthe interpretability of traditional mixed-effects models.\n", "link": "http://arxiv.org/abs/2505.09506v1", "date": "2025-05-14", "relevancy": 2.0133, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5088}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5041}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep-SITAR%3A%20A%20SITAR-Based%20Deep%20Learning%20Framework%20for%20Growth%20Curve%0A%20%20Modeling%20via%20Autoencoders&body=Title%3A%20Deep-SITAR%3A%20A%20SITAR-Based%20Deep%20Learning%20Framework%20for%20Growth%20Curve%0A%20%20Modeling%20via%20Autoencoders%0AAuthor%3A%20Mar%C3%ADa%20Alejandra%20Hern%C3%A1ndez%20and%20Oscar%20Rodriguez%20and%20Dae-Jin%20Lee%0AAbstract%3A%20%20%20Several%20approaches%20have%20been%20developed%20to%20capture%20the%20complexity%20and%0Anonlinearity%20of%20human%20growth.%20One%20widely%20used%20is%20the%20Super%20Imposition%20by%0ATranslation%20and%20Rotation%20%28SITAR%29%20model%2C%20which%20has%20become%20popular%20in%20studies%20of%0Aadolescent%20growth.%20SITAR%20is%20a%20shape-invariant%20mixed-effects%20model%20that%0Arepresents%20the%20shared%20growth%20pattern%20of%20a%20population%20using%20a%20natural%20cubic%0Aspline%20mean%20curve%20while%20incorporating%20three%20subject-specific%20random%20effects%20--%0Atiming%2C%20size%2C%20and%20growth%20intensity%20--%20to%20account%20for%20variations%20among%0Aindividuals.%20In%20this%20work%2C%20we%20introduce%20a%20supervised%20deep%20learning%20framework%0Abased%20on%20an%20autoencoder%20architecture%20that%20integrates%20a%20deep%20neural%20network%0A%28neural%20network%29%20with%20a%20B-spline%20model%20to%20estimate%20the%20SITAR%20model.%20In%20this%0Aapproach%2C%20the%20encoder%20estimates%20the%20random%20effects%20for%20each%20individual%2C%20while%0Athe%20decoder%20performs%20a%20fitting%20based%20on%20B-splines%20similar%20to%20the%20classic%20SITAR%0Amodel.%20We%20refer%20to%20this%20method%20as%20the%20Deep-SITAR%20model.%20This%20innovative%0Aapproach%20enables%20the%20prediction%20of%20the%20random%20effects%20of%20new%20individuals%0Aentering%20a%20population%20without%20requiring%20a%20full%20model%20re-estimation.%20As%20a%0Aresult%2C%20Deep-SITAR%20offers%20a%20powerful%20approach%20to%20predicting%20growth%0Atrajectories%2C%20combining%20the%20flexibility%20and%20efficiency%20of%20deep%20learning%20with%0Athe%20interpretability%20of%20traditional%20mixed-effects%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep-SITAR%253A%2520A%2520SITAR-Based%2520Deep%2520Learning%2520Framework%2520for%2520Growth%2520Curve%250A%2520%2520Modeling%2520via%2520Autoencoders%26entry.906535625%3DMar%25C3%25ADa%2520Alejandra%2520Hern%25C3%25A1ndez%2520and%2520Oscar%2520Rodriguez%2520and%2520Dae-Jin%2520Lee%26entry.1292438233%3D%2520%2520Several%2520approaches%2520have%2520been%2520developed%2520to%2520capture%2520the%2520complexity%2520and%250Anonlinearity%2520of%2520human%2520growth.%2520One%2520widely%2520used%2520is%2520the%2520Super%2520Imposition%2520by%250ATranslation%2520and%2520Rotation%2520%2528SITAR%2529%2520model%252C%2520which%2520has%2520become%2520popular%2520in%2520studies%2520of%250Aadolescent%2520growth.%2520SITAR%2520is%2520a%2520shape-invariant%2520mixed-effects%2520model%2520that%250Arepresents%2520the%2520shared%2520growth%2520pattern%2520of%2520a%2520population%2520using%2520a%2520natural%2520cubic%250Aspline%2520mean%2520curve%2520while%2520incorporating%2520three%2520subject-specific%2520random%2520effects%2520--%250Atiming%252C%2520size%252C%2520and%2520growth%2520intensity%2520--%2520to%2520account%2520for%2520variations%2520among%250Aindividuals.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520supervised%2520deep%2520learning%2520framework%250Abased%2520on%2520an%2520autoencoder%2520architecture%2520that%2520integrates%2520a%2520deep%2520neural%2520network%250A%2528neural%2520network%2529%2520with%2520a%2520B-spline%2520model%2520to%2520estimate%2520the%2520SITAR%2520model.%2520In%2520this%250Aapproach%252C%2520the%2520encoder%2520estimates%2520the%2520random%2520effects%2520for%2520each%2520individual%252C%2520while%250Athe%2520decoder%2520performs%2520a%2520fitting%2520based%2520on%2520B-splines%2520similar%2520to%2520the%2520classic%2520SITAR%250Amodel.%2520We%2520refer%2520to%2520this%2520method%2520as%2520the%2520Deep-SITAR%2520model.%2520This%2520innovative%250Aapproach%2520enables%2520the%2520prediction%2520of%2520the%2520random%2520effects%2520of%2520new%2520individuals%250Aentering%2520a%2520population%2520without%2520requiring%2520a%2520full%2520model%2520re-estimation.%2520As%2520a%250Aresult%252C%2520Deep-SITAR%2520offers%2520a%2520powerful%2520approach%2520to%2520predicting%2520growth%250Atrajectories%252C%2520combining%2520the%2520flexibility%2520and%2520efficiency%2520of%2520deep%2520learning%2520with%250Athe%2520interpretability%2520of%2520traditional%2520mixed-effects%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep-SITAR%3A%20A%20SITAR-Based%20Deep%20Learning%20Framework%20for%20Growth%20Curve%0A%20%20Modeling%20via%20Autoencoders&entry.906535625=Mar%C3%ADa%20Alejandra%20Hern%C3%A1ndez%20and%20Oscar%20Rodriguez%20and%20Dae-Jin%20Lee&entry.1292438233=%20%20Several%20approaches%20have%20been%20developed%20to%20capture%20the%20complexity%20and%0Anonlinearity%20of%20human%20growth.%20One%20widely%20used%20is%20the%20Super%20Imposition%20by%0ATranslation%20and%20Rotation%20%28SITAR%29%20model%2C%20which%20has%20become%20popular%20in%20studies%20of%0Aadolescent%20growth.%20SITAR%20is%20a%20shape-invariant%20mixed-effects%20model%20that%0Arepresents%20the%20shared%20growth%20pattern%20of%20a%20population%20using%20a%20natural%20cubic%0Aspline%20mean%20curve%20while%20incorporating%20three%20subject-specific%20random%20effects%20--%0Atiming%2C%20size%2C%20and%20growth%20intensity%20--%20to%20account%20for%20variations%20among%0Aindividuals.%20In%20this%20work%2C%20we%20introduce%20a%20supervised%20deep%20learning%20framework%0Abased%20on%20an%20autoencoder%20architecture%20that%20integrates%20a%20deep%20neural%20network%0A%28neural%20network%29%20with%20a%20B-spline%20model%20to%20estimate%20the%20SITAR%20model.%20In%20this%0Aapproach%2C%20the%20encoder%20estimates%20the%20random%20effects%20for%20each%20individual%2C%20while%0Athe%20decoder%20performs%20a%20fitting%20based%20on%20B-splines%20similar%20to%20the%20classic%20SITAR%0Amodel.%20We%20refer%20to%20this%20method%20as%20the%20Deep-SITAR%20model.%20This%20innovative%0Aapproach%20enables%20the%20prediction%20of%20the%20random%20effects%20of%20new%20individuals%0Aentering%20a%20population%20without%20requiring%20a%20full%20model%20re-estimation.%20As%20a%0Aresult%2C%20Deep-SITAR%20offers%20a%20powerful%20approach%20to%20predicting%20growth%0Atrajectories%2C%20combining%20the%20flexibility%20and%20efficiency%20of%20deep%20learning%20with%0Athe%20interpretability%20of%20traditional%20mixed-effects%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09506v1&entry.124074799=Read"},
{"title": "IAEmu: Learning Galaxy Intrinsic Alignment Correlations", "author": "Sneh Pandya and Yuanyuan Yang and Nicholas Van Alfen and Jonathan Blazek and Robin Walters", "abstract": "  The intrinsic alignments (IA) of galaxies, a key contaminant in weak lensing\nanalyses, arise from correlations in galaxy shapes driven by tidal interactions\nand galaxy formation processes. Accurate IA modeling is essential for robust\ncosmological inference, but current approaches rely on perturbative methods\nthat break down on nonlinear scales or on expensive simulations. We introduce\nIAEmu, a neural network-based emulator that predicts the galaxy\nposition-position ($\\xi$), position-orientation ($\\omega$), and\norientation-orientation ($\\eta$) correlation functions and their uncertainties\nusing mock catalogs based on the halo occupation distribution (HOD) framework.\nCompared to simulations, IAEmu achieves ~3% average error for $\\xi$ and ~5% for\n$\\omega$, while capturing the stochasticity of $\\eta$ without overfitting. The\nemulator provides both aleatoric and epistemic uncertainties, helping identify\nregions where predictions may be less reliable. We also demonstrate\ngeneralization to non-HOD alignment signals by fitting to IllustrisTNG\nhydrodynamical simulation data. As a fully differentiable neural network, IAEmu\nenables $\\sim$10,000$\\times$ speed-ups in mapping HOD parameters to correlation\nfunctions on GPUs, compared to CPU-based simulations. This acceleration\nfacilitates inverse modeling via gradient-based sampling, making IAEmu a\npowerful surrogate model for galaxy bias and IA studies with direct\napplications to Stage IV weak lensing surveys.\n", "link": "http://arxiv.org/abs/2504.05235v2", "date": "2025-05-14", "relevancy": 2.013, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5329}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4827}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IAEmu%3A%20Learning%20Galaxy%20Intrinsic%20Alignment%20Correlations&body=Title%3A%20IAEmu%3A%20Learning%20Galaxy%20Intrinsic%20Alignment%20Correlations%0AAuthor%3A%20Sneh%20Pandya%20and%20Yuanyuan%20Yang%20and%20Nicholas%20Van%20Alfen%20and%20Jonathan%20Blazek%20and%20Robin%20Walters%0AAbstract%3A%20%20%20The%20intrinsic%20alignments%20%28IA%29%20of%20galaxies%2C%20a%20key%20contaminant%20in%20weak%20lensing%0Aanalyses%2C%20arise%20from%20correlations%20in%20galaxy%20shapes%20driven%20by%20tidal%20interactions%0Aand%20galaxy%20formation%20processes.%20Accurate%20IA%20modeling%20is%20essential%20for%20robust%0Acosmological%20inference%2C%20but%20current%20approaches%20rely%20on%20perturbative%20methods%0Athat%20break%20down%20on%20nonlinear%20scales%20or%20on%20expensive%20simulations.%20We%20introduce%0AIAEmu%2C%20a%20neural%20network-based%20emulator%20that%20predicts%20the%20galaxy%0Aposition-position%20%28%24%5Cxi%24%29%2C%20position-orientation%20%28%24%5Comega%24%29%2C%20and%0Aorientation-orientation%20%28%24%5Ceta%24%29%20correlation%20functions%20and%20their%20uncertainties%0Ausing%20mock%20catalogs%20based%20on%20the%20halo%20occupation%20distribution%20%28HOD%29%20framework.%0ACompared%20to%20simulations%2C%20IAEmu%20achieves%20~3%25%20average%20error%20for%20%24%5Cxi%24%20and%20~5%25%20for%0A%24%5Comega%24%2C%20while%20capturing%20the%20stochasticity%20of%20%24%5Ceta%24%20without%20overfitting.%20The%0Aemulator%20provides%20both%20aleatoric%20and%20epistemic%20uncertainties%2C%20helping%20identify%0Aregions%20where%20predictions%20may%20be%20less%20reliable.%20We%20also%20demonstrate%0Ageneralization%20to%20non-HOD%20alignment%20signals%20by%20fitting%20to%20IllustrisTNG%0Ahydrodynamical%20simulation%20data.%20As%20a%20fully%20differentiable%20neural%20network%2C%20IAEmu%0Aenables%20%24%5Csim%2410%2C000%24%5Ctimes%24%20speed-ups%20in%20mapping%20HOD%20parameters%20to%20correlation%0Afunctions%20on%20GPUs%2C%20compared%20to%20CPU-based%20simulations.%20This%20acceleration%0Afacilitates%20inverse%20modeling%20via%20gradient-based%20sampling%2C%20making%20IAEmu%20a%0Apowerful%20surrogate%20model%20for%20galaxy%20bias%20and%20IA%20studies%20with%20direct%0Aapplications%20to%20Stage%20IV%20weak%20lensing%20surveys.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.05235v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIAEmu%253A%2520Learning%2520Galaxy%2520Intrinsic%2520Alignment%2520Correlations%26entry.906535625%3DSneh%2520Pandya%2520and%2520Yuanyuan%2520Yang%2520and%2520Nicholas%2520Van%2520Alfen%2520and%2520Jonathan%2520Blazek%2520and%2520Robin%2520Walters%26entry.1292438233%3D%2520%2520The%2520intrinsic%2520alignments%2520%2528IA%2529%2520of%2520galaxies%252C%2520a%2520key%2520contaminant%2520in%2520weak%2520lensing%250Aanalyses%252C%2520arise%2520from%2520correlations%2520in%2520galaxy%2520shapes%2520driven%2520by%2520tidal%2520interactions%250Aand%2520galaxy%2520formation%2520processes.%2520Accurate%2520IA%2520modeling%2520is%2520essential%2520for%2520robust%250Acosmological%2520inference%252C%2520but%2520current%2520approaches%2520rely%2520on%2520perturbative%2520methods%250Athat%2520break%2520down%2520on%2520nonlinear%2520scales%2520or%2520on%2520expensive%2520simulations.%2520We%2520introduce%250AIAEmu%252C%2520a%2520neural%2520network-based%2520emulator%2520that%2520predicts%2520the%2520galaxy%250Aposition-position%2520%2528%2524%255Cxi%2524%2529%252C%2520position-orientation%2520%2528%2524%255Comega%2524%2529%252C%2520and%250Aorientation-orientation%2520%2528%2524%255Ceta%2524%2529%2520correlation%2520functions%2520and%2520their%2520uncertainties%250Ausing%2520mock%2520catalogs%2520based%2520on%2520the%2520halo%2520occupation%2520distribution%2520%2528HOD%2529%2520framework.%250ACompared%2520to%2520simulations%252C%2520IAEmu%2520achieves%2520~3%2525%2520average%2520error%2520for%2520%2524%255Cxi%2524%2520and%2520~5%2525%2520for%250A%2524%255Comega%2524%252C%2520while%2520capturing%2520the%2520stochasticity%2520of%2520%2524%255Ceta%2524%2520without%2520overfitting.%2520The%250Aemulator%2520provides%2520both%2520aleatoric%2520and%2520epistemic%2520uncertainties%252C%2520helping%2520identify%250Aregions%2520where%2520predictions%2520may%2520be%2520less%2520reliable.%2520We%2520also%2520demonstrate%250Ageneralization%2520to%2520non-HOD%2520alignment%2520signals%2520by%2520fitting%2520to%2520IllustrisTNG%250Ahydrodynamical%2520simulation%2520data.%2520As%2520a%2520fully%2520differentiable%2520neural%2520network%252C%2520IAEmu%250Aenables%2520%2524%255Csim%252410%252C000%2524%255Ctimes%2524%2520speed-ups%2520in%2520mapping%2520HOD%2520parameters%2520to%2520correlation%250Afunctions%2520on%2520GPUs%252C%2520compared%2520to%2520CPU-based%2520simulations.%2520This%2520acceleration%250Afacilitates%2520inverse%2520modeling%2520via%2520gradient-based%2520sampling%252C%2520making%2520IAEmu%2520a%250Apowerful%2520surrogate%2520model%2520for%2520galaxy%2520bias%2520and%2520IA%2520studies%2520with%2520direct%250Aapplications%2520to%2520Stage%2520IV%2520weak%2520lensing%2520surveys.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.05235v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IAEmu%3A%20Learning%20Galaxy%20Intrinsic%20Alignment%20Correlations&entry.906535625=Sneh%20Pandya%20and%20Yuanyuan%20Yang%20and%20Nicholas%20Van%20Alfen%20and%20Jonathan%20Blazek%20and%20Robin%20Walters&entry.1292438233=%20%20The%20intrinsic%20alignments%20%28IA%29%20of%20galaxies%2C%20a%20key%20contaminant%20in%20weak%20lensing%0Aanalyses%2C%20arise%20from%20correlations%20in%20galaxy%20shapes%20driven%20by%20tidal%20interactions%0Aand%20galaxy%20formation%20processes.%20Accurate%20IA%20modeling%20is%20essential%20for%20robust%0Acosmological%20inference%2C%20but%20current%20approaches%20rely%20on%20perturbative%20methods%0Athat%20break%20down%20on%20nonlinear%20scales%20or%20on%20expensive%20simulations.%20We%20introduce%0AIAEmu%2C%20a%20neural%20network-based%20emulator%20that%20predicts%20the%20galaxy%0Aposition-position%20%28%24%5Cxi%24%29%2C%20position-orientation%20%28%24%5Comega%24%29%2C%20and%0Aorientation-orientation%20%28%24%5Ceta%24%29%20correlation%20functions%20and%20their%20uncertainties%0Ausing%20mock%20catalogs%20based%20on%20the%20halo%20occupation%20distribution%20%28HOD%29%20framework.%0ACompared%20to%20simulations%2C%20IAEmu%20achieves%20~3%25%20average%20error%20for%20%24%5Cxi%24%20and%20~5%25%20for%0A%24%5Comega%24%2C%20while%20capturing%20the%20stochasticity%20of%20%24%5Ceta%24%20without%20overfitting.%20The%0Aemulator%20provides%20both%20aleatoric%20and%20epistemic%20uncertainties%2C%20helping%20identify%0Aregions%20where%20predictions%20may%20be%20less%20reliable.%20We%20also%20demonstrate%0Ageneralization%20to%20non-HOD%20alignment%20signals%20by%20fitting%20to%20IllustrisTNG%0Ahydrodynamical%20simulation%20data.%20As%20a%20fully%20differentiable%20neural%20network%2C%20IAEmu%0Aenables%20%24%5Csim%2410%2C000%24%5Ctimes%24%20speed-ups%20in%20mapping%20HOD%20parameters%20to%20correlation%0Afunctions%20on%20GPUs%2C%20compared%20to%20CPU-based%20simulations.%20This%20acceleration%0Afacilitates%20inverse%20modeling%20via%20gradient-based%20sampling%2C%20making%20IAEmu%20a%0Apowerful%20surrogate%20model%20for%20galaxy%20bias%20and%20IA%20studies%20with%20direct%0Aapplications%20to%20Stage%20IV%20weak%20lensing%20surveys.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.05235v2&entry.124074799=Read"},
{"title": "Accelerating Machine Learning Systems via Category Theory: Applications\n  to Spherical Attention for Gene Regulatory Networks", "author": "Vincent Abbott and Kotaro Kamiya and Gerard Glowacki and Yu Atsumi and Gioele Zardini and Yoshihiro Maruyama", "abstract": "  How do we enable artificial intelligence models to improve themselves? This\nis central to exponentially improving generalized artificial intelligence\nmodels, which can improve their own architecture to handle new problem domains\nin an efficient manner that leverages the latest hardware. However, current\nautomated compilation methods are poor, and efficient algorithms require years\nof human development. In this paper, we use neural circuit diagrams, based in\ncategory theory, to prove a general theorem related to deep learning\nalgorithms, guide the development of a novel attention algorithm catered to the\ndomain of gene regulatory networks, and produce a corresponding efficient\nkernel. The algorithm we propose, spherical attention, shows that neural\ncircuit diagrams enable a principled and systematic method for reasoning about\ndeep learning architectures and providing high-performance code. By replacing\nSoftMax with an $L^2$ norm as suggested by diagrams, it overcomes the special\nfunction unit bottleneck of standard attention while retaining the streaming\nproperty essential to high-performance. Our diagrammatically derived\n\\textit{FlashSign} kernel achieves comparable performance to the\nstate-of-the-art, fine-tuned FlashAttention algorithm on an A100, and\n$3.6\\times$ the performance of PyTorch. Overall, this investigation shows\nneural circuit diagrams' suitability as a high-level framework for the\nautomated development of efficient, novel artificial intelligence\narchitectures.\n", "link": "http://arxiv.org/abs/2505.09326v1", "date": "2025-05-14", "relevancy": 2.0102, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.505}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5026}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Machine%20Learning%20Systems%20via%20Category%20Theory%3A%20Applications%0A%20%20to%20Spherical%20Attention%20for%20Gene%20Regulatory%20Networks&body=Title%3A%20Accelerating%20Machine%20Learning%20Systems%20via%20Category%20Theory%3A%20Applications%0A%20%20to%20Spherical%20Attention%20for%20Gene%20Regulatory%20Networks%0AAuthor%3A%20Vincent%20Abbott%20and%20Kotaro%20Kamiya%20and%20Gerard%20Glowacki%20and%20Yu%20Atsumi%20and%20Gioele%20Zardini%20and%20Yoshihiro%20Maruyama%0AAbstract%3A%20%20%20How%20do%20we%20enable%20artificial%20intelligence%20models%20to%20improve%20themselves%3F%20This%0Ais%20central%20to%20exponentially%20improving%20generalized%20artificial%20intelligence%0Amodels%2C%20which%20can%20improve%20their%20own%20architecture%20to%20handle%20new%20problem%20domains%0Ain%20an%20efficient%20manner%20that%20leverages%20the%20latest%20hardware.%20However%2C%20current%0Aautomated%20compilation%20methods%20are%20poor%2C%20and%20efficient%20algorithms%20require%20years%0Aof%20human%20development.%20In%20this%20paper%2C%20we%20use%20neural%20circuit%20diagrams%2C%20based%20in%0Acategory%20theory%2C%20to%20prove%20a%20general%20theorem%20related%20to%20deep%20learning%0Aalgorithms%2C%20guide%20the%20development%20of%20a%20novel%20attention%20algorithm%20catered%20to%20the%0Adomain%20of%20gene%20regulatory%20networks%2C%20and%20produce%20a%20corresponding%20efficient%0Akernel.%20The%20algorithm%20we%20propose%2C%20spherical%20attention%2C%20shows%20that%20neural%0Acircuit%20diagrams%20enable%20a%20principled%20and%20systematic%20method%20for%20reasoning%20about%0Adeep%20learning%20architectures%20and%20providing%20high-performance%20code.%20By%20replacing%0ASoftMax%20with%20an%20%24L%5E2%24%20norm%20as%20suggested%20by%20diagrams%2C%20it%20overcomes%20the%20special%0Afunction%20unit%20bottleneck%20of%20standard%20attention%20while%20retaining%20the%20streaming%0Aproperty%20essential%20to%20high-performance.%20Our%20diagrammatically%20derived%0A%5Ctextit%7BFlashSign%7D%20kernel%20achieves%20comparable%20performance%20to%20the%0Astate-of-the-art%2C%20fine-tuned%20FlashAttention%20algorithm%20on%20an%20A100%2C%20and%0A%243.6%5Ctimes%24%20the%20performance%20of%20PyTorch.%20Overall%2C%20this%20investigation%20shows%0Aneural%20circuit%20diagrams%27%20suitability%20as%20a%20high-level%20framework%20for%20the%0Aautomated%20development%20of%20efficient%2C%20novel%20artificial%20intelligence%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Machine%2520Learning%2520Systems%2520via%2520Category%2520Theory%253A%2520Applications%250A%2520%2520to%2520Spherical%2520Attention%2520for%2520Gene%2520Regulatory%2520Networks%26entry.906535625%3DVincent%2520Abbott%2520and%2520Kotaro%2520Kamiya%2520and%2520Gerard%2520Glowacki%2520and%2520Yu%2520Atsumi%2520and%2520Gioele%2520Zardini%2520and%2520Yoshihiro%2520Maruyama%26entry.1292438233%3D%2520%2520How%2520do%2520we%2520enable%2520artificial%2520intelligence%2520models%2520to%2520improve%2520themselves%253F%2520This%250Ais%2520central%2520to%2520exponentially%2520improving%2520generalized%2520artificial%2520intelligence%250Amodels%252C%2520which%2520can%2520improve%2520their%2520own%2520architecture%2520to%2520handle%2520new%2520problem%2520domains%250Ain%2520an%2520efficient%2520manner%2520that%2520leverages%2520the%2520latest%2520hardware.%2520However%252C%2520current%250Aautomated%2520compilation%2520methods%2520are%2520poor%252C%2520and%2520efficient%2520algorithms%2520require%2520years%250Aof%2520human%2520development.%2520In%2520this%2520paper%252C%2520we%2520use%2520neural%2520circuit%2520diagrams%252C%2520based%2520in%250Acategory%2520theory%252C%2520to%2520prove%2520a%2520general%2520theorem%2520related%2520to%2520deep%2520learning%250Aalgorithms%252C%2520guide%2520the%2520development%2520of%2520a%2520novel%2520attention%2520algorithm%2520catered%2520to%2520the%250Adomain%2520of%2520gene%2520regulatory%2520networks%252C%2520and%2520produce%2520a%2520corresponding%2520efficient%250Akernel.%2520The%2520algorithm%2520we%2520propose%252C%2520spherical%2520attention%252C%2520shows%2520that%2520neural%250Acircuit%2520diagrams%2520enable%2520a%2520principled%2520and%2520systematic%2520method%2520for%2520reasoning%2520about%250Adeep%2520learning%2520architectures%2520and%2520providing%2520high-performance%2520code.%2520By%2520replacing%250ASoftMax%2520with%2520an%2520%2524L%255E2%2524%2520norm%2520as%2520suggested%2520by%2520diagrams%252C%2520it%2520overcomes%2520the%2520special%250Afunction%2520unit%2520bottleneck%2520of%2520standard%2520attention%2520while%2520retaining%2520the%2520streaming%250Aproperty%2520essential%2520to%2520high-performance.%2520Our%2520diagrammatically%2520derived%250A%255Ctextit%257BFlashSign%257D%2520kernel%2520achieves%2520comparable%2520performance%2520to%2520the%250Astate-of-the-art%252C%2520fine-tuned%2520FlashAttention%2520algorithm%2520on%2520an%2520A100%252C%2520and%250A%25243.6%255Ctimes%2524%2520the%2520performance%2520of%2520PyTorch.%2520Overall%252C%2520this%2520investigation%2520shows%250Aneural%2520circuit%2520diagrams%2527%2520suitability%2520as%2520a%2520high-level%2520framework%2520for%2520the%250Aautomated%2520development%2520of%2520efficient%252C%2520novel%2520artificial%2520intelligence%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Machine%20Learning%20Systems%20via%20Category%20Theory%3A%20Applications%0A%20%20to%20Spherical%20Attention%20for%20Gene%20Regulatory%20Networks&entry.906535625=Vincent%20Abbott%20and%20Kotaro%20Kamiya%20and%20Gerard%20Glowacki%20and%20Yu%20Atsumi%20and%20Gioele%20Zardini%20and%20Yoshihiro%20Maruyama&entry.1292438233=%20%20How%20do%20we%20enable%20artificial%20intelligence%20models%20to%20improve%20themselves%3F%20This%0Ais%20central%20to%20exponentially%20improving%20generalized%20artificial%20intelligence%0Amodels%2C%20which%20can%20improve%20their%20own%20architecture%20to%20handle%20new%20problem%20domains%0Ain%20an%20efficient%20manner%20that%20leverages%20the%20latest%20hardware.%20However%2C%20current%0Aautomated%20compilation%20methods%20are%20poor%2C%20and%20efficient%20algorithms%20require%20years%0Aof%20human%20development.%20In%20this%20paper%2C%20we%20use%20neural%20circuit%20diagrams%2C%20based%20in%0Acategory%20theory%2C%20to%20prove%20a%20general%20theorem%20related%20to%20deep%20learning%0Aalgorithms%2C%20guide%20the%20development%20of%20a%20novel%20attention%20algorithm%20catered%20to%20the%0Adomain%20of%20gene%20regulatory%20networks%2C%20and%20produce%20a%20corresponding%20efficient%0Akernel.%20The%20algorithm%20we%20propose%2C%20spherical%20attention%2C%20shows%20that%20neural%0Acircuit%20diagrams%20enable%20a%20principled%20and%20systematic%20method%20for%20reasoning%20about%0Adeep%20learning%20architectures%20and%20providing%20high-performance%20code.%20By%20replacing%0ASoftMax%20with%20an%20%24L%5E2%24%20norm%20as%20suggested%20by%20diagrams%2C%20it%20overcomes%20the%20special%0Afunction%20unit%20bottleneck%20of%20standard%20attention%20while%20retaining%20the%20streaming%0Aproperty%20essential%20to%20high-performance.%20Our%20diagrammatically%20derived%0A%5Ctextit%7BFlashSign%7D%20kernel%20achieves%20comparable%20performance%20to%20the%0Astate-of-the-art%2C%20fine-tuned%20FlashAttention%20algorithm%20on%20an%20A100%2C%20and%0A%243.6%5Ctimes%24%20the%20performance%20of%20PyTorch.%20Overall%2C%20this%20investigation%20shows%0Aneural%20circuit%20diagrams%27%20suitability%20as%20a%20high-level%20framework%20for%20the%0Aautomated%20development%20of%20efficient%2C%20novel%20artificial%20intelligence%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09326v1&entry.124074799=Read"},
{"title": "Embodied Intelligent Industrial Robotics: Concepts and Techniques", "author": "Chaoran Zhang and Chenhao Zhang and Zhaobo Xu and Qinghongbing Xie and Pingfa Feng and Long Zeng", "abstract": "  In recent years, embodied intelligent robotics (EIR) has made significant\nprogress in multi-modal perception, autonomous decision-making, and physical\ninteraction. Some robots have already been tested in general-purpose scenarios\nsuch as homes and shopping malls. We aim to advance the research and\napplication of embodied intelligence in industrial scenes. However, current EIR\nlacks a deep understanding of industrial environment semantics and the\nnormative constraints between industrial operating objects. To address this\ngap, this paper first reviews the history of industrial robotics and the\nmainstream EIR frameworks. We then introduce the concept of the embodied\nintelligent industrial robotics (EIIR) and propose a knowledge-driven EIIR\ntechnology framework for industrial environments. The framework includes four\nmain modules: world model, high-level task planner, low-level skill controller,\nand simulator. We also review the current development of technologies related\nto each module and highlight recent progress in adapting them to industrial\napplications. Finally, we summarize the key challenges EIIR faces in industrial\nscenarios and suggest future research directions. We believe that EIIR\ntechnology will shape the next generation of industrial robotics. Industrial\nsystems based on embodied intelligent industrial robots offer strong potential\nfor enabling intelligent manufacturing. We will continue to track and summarize\nnew research in this area and hope this review will serve as a valuable\nreference for scholars and engineers interested in industrial embodied\nintelligence. Together, we can help drive the rapid advancement and application\nof this technology. The associated project can be found at\nhttps://github.com/jackeyzengl/Embodied_Intelligent_Industrial_Robotics_Paper_List.\n", "link": "http://arxiv.org/abs/2505.09305v1", "date": "2025-05-14", "relevancy": 1.5469, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5461}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5189}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embodied%20Intelligent%20Industrial%20Robotics%3A%20Concepts%20and%20Techniques&body=Title%3A%20Embodied%20Intelligent%20Industrial%20Robotics%3A%20Concepts%20and%20Techniques%0AAuthor%3A%20Chaoran%20Zhang%20and%20Chenhao%20Zhang%20and%20Zhaobo%20Xu%20and%20Qinghongbing%20Xie%20and%20Pingfa%20Feng%20and%20Long%20Zeng%0AAbstract%3A%20%20%20In%20recent%20years%2C%20embodied%20intelligent%20robotics%20%28EIR%29%20has%20made%20significant%0Aprogress%20in%20multi-modal%20perception%2C%20autonomous%20decision-making%2C%20and%20physical%0Ainteraction.%20Some%20robots%20have%20already%20been%20tested%20in%20general-purpose%20scenarios%0Asuch%20as%20homes%20and%20shopping%20malls.%20We%20aim%20to%20advance%20the%20research%20and%0Aapplication%20of%20embodied%20intelligence%20in%20industrial%20scenes.%20However%2C%20current%20EIR%0Alacks%20a%20deep%20understanding%20of%20industrial%20environment%20semantics%20and%20the%0Anormative%20constraints%20between%20industrial%20operating%20objects.%20To%20address%20this%0Agap%2C%20this%20paper%20first%20reviews%20the%20history%20of%20industrial%20robotics%20and%20the%0Amainstream%20EIR%20frameworks.%20We%20then%20introduce%20the%20concept%20of%20the%20embodied%0Aintelligent%20industrial%20robotics%20%28EIIR%29%20and%20propose%20a%20knowledge-driven%20EIIR%0Atechnology%20framework%20for%20industrial%20environments.%20The%20framework%20includes%20four%0Amain%20modules%3A%20world%20model%2C%20high-level%20task%20planner%2C%20low-level%20skill%20controller%2C%0Aand%20simulator.%20We%20also%20review%20the%20current%20development%20of%20technologies%20related%0Ato%20each%20module%20and%20highlight%20recent%20progress%20in%20adapting%20them%20to%20industrial%0Aapplications.%20Finally%2C%20we%20summarize%20the%20key%20challenges%20EIIR%20faces%20in%20industrial%0Ascenarios%20and%20suggest%20future%20research%20directions.%20We%20believe%20that%20EIIR%0Atechnology%20will%20shape%20the%20next%20generation%20of%20industrial%20robotics.%20Industrial%0Asystems%20based%20on%20embodied%20intelligent%20industrial%20robots%20offer%20strong%20potential%0Afor%20enabling%20intelligent%20manufacturing.%20We%20will%20continue%20to%20track%20and%20summarize%0Anew%20research%20in%20this%20area%20and%20hope%20this%20review%20will%20serve%20as%20a%20valuable%0Areference%20for%20scholars%20and%20engineers%20interested%20in%20industrial%20embodied%0Aintelligence.%20Together%2C%20we%20can%20help%20drive%20the%20rapid%20advancement%20and%20application%0Aof%20this%20technology.%20The%20associated%20project%20can%20be%20found%20at%0Ahttps%3A//github.com/jackeyzengl/Embodied_Intelligent_Industrial_Robotics_Paper_List.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodied%2520Intelligent%2520Industrial%2520Robotics%253A%2520Concepts%2520and%2520Techniques%26entry.906535625%3DChaoran%2520Zhang%2520and%2520Chenhao%2520Zhang%2520and%2520Zhaobo%2520Xu%2520and%2520Qinghongbing%2520Xie%2520and%2520Pingfa%2520Feng%2520and%2520Long%2520Zeng%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520embodied%2520intelligent%2520robotics%2520%2528EIR%2529%2520has%2520made%2520significant%250Aprogress%2520in%2520multi-modal%2520perception%252C%2520autonomous%2520decision-making%252C%2520and%2520physical%250Ainteraction.%2520Some%2520robots%2520have%2520already%2520been%2520tested%2520in%2520general-purpose%2520scenarios%250Asuch%2520as%2520homes%2520and%2520shopping%2520malls.%2520We%2520aim%2520to%2520advance%2520the%2520research%2520and%250Aapplication%2520of%2520embodied%2520intelligence%2520in%2520industrial%2520scenes.%2520However%252C%2520current%2520EIR%250Alacks%2520a%2520deep%2520understanding%2520of%2520industrial%2520environment%2520semantics%2520and%2520the%250Anormative%2520constraints%2520between%2520industrial%2520operating%2520objects.%2520To%2520address%2520this%250Agap%252C%2520this%2520paper%2520first%2520reviews%2520the%2520history%2520of%2520industrial%2520robotics%2520and%2520the%250Amainstream%2520EIR%2520frameworks.%2520We%2520then%2520introduce%2520the%2520concept%2520of%2520the%2520embodied%250Aintelligent%2520industrial%2520robotics%2520%2528EIIR%2529%2520and%2520propose%2520a%2520knowledge-driven%2520EIIR%250Atechnology%2520framework%2520for%2520industrial%2520environments.%2520The%2520framework%2520includes%2520four%250Amain%2520modules%253A%2520world%2520model%252C%2520high-level%2520task%2520planner%252C%2520low-level%2520skill%2520controller%252C%250Aand%2520simulator.%2520We%2520also%2520review%2520the%2520current%2520development%2520of%2520technologies%2520related%250Ato%2520each%2520module%2520and%2520highlight%2520recent%2520progress%2520in%2520adapting%2520them%2520to%2520industrial%250Aapplications.%2520Finally%252C%2520we%2520summarize%2520the%2520key%2520challenges%2520EIIR%2520faces%2520in%2520industrial%250Ascenarios%2520and%2520suggest%2520future%2520research%2520directions.%2520We%2520believe%2520that%2520EIIR%250Atechnology%2520will%2520shape%2520the%2520next%2520generation%2520of%2520industrial%2520robotics.%2520Industrial%250Asystems%2520based%2520on%2520embodied%2520intelligent%2520industrial%2520robots%2520offer%2520strong%2520potential%250Afor%2520enabling%2520intelligent%2520manufacturing.%2520We%2520will%2520continue%2520to%2520track%2520and%2520summarize%250Anew%2520research%2520in%2520this%2520area%2520and%2520hope%2520this%2520review%2520will%2520serve%2520as%2520a%2520valuable%250Areference%2520for%2520scholars%2520and%2520engineers%2520interested%2520in%2520industrial%2520embodied%250Aintelligence.%2520Together%252C%2520we%2520can%2520help%2520drive%2520the%2520rapid%2520advancement%2520and%2520application%250Aof%2520this%2520technology.%2520The%2520associated%2520project%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/jackeyzengl/Embodied_Intelligent_Industrial_Robotics_Paper_List.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embodied%20Intelligent%20Industrial%20Robotics%3A%20Concepts%20and%20Techniques&entry.906535625=Chaoran%20Zhang%20and%20Chenhao%20Zhang%20and%20Zhaobo%20Xu%20and%20Qinghongbing%20Xie%20and%20Pingfa%20Feng%20and%20Long%20Zeng&entry.1292438233=%20%20In%20recent%20years%2C%20embodied%20intelligent%20robotics%20%28EIR%29%20has%20made%20significant%0Aprogress%20in%20multi-modal%20perception%2C%20autonomous%20decision-making%2C%20and%20physical%0Ainteraction.%20Some%20robots%20have%20already%20been%20tested%20in%20general-purpose%20scenarios%0Asuch%20as%20homes%20and%20shopping%20malls.%20We%20aim%20to%20advance%20the%20research%20and%0Aapplication%20of%20embodied%20intelligence%20in%20industrial%20scenes.%20However%2C%20current%20EIR%0Alacks%20a%20deep%20understanding%20of%20industrial%20environment%20semantics%20and%20the%0Anormative%20constraints%20between%20industrial%20operating%20objects.%20To%20address%20this%0Agap%2C%20this%20paper%20first%20reviews%20the%20history%20of%20industrial%20robotics%20and%20the%0Amainstream%20EIR%20frameworks.%20We%20then%20introduce%20the%20concept%20of%20the%20embodied%0Aintelligent%20industrial%20robotics%20%28EIIR%29%20and%20propose%20a%20knowledge-driven%20EIIR%0Atechnology%20framework%20for%20industrial%20environments.%20The%20framework%20includes%20four%0Amain%20modules%3A%20world%20model%2C%20high-level%20task%20planner%2C%20low-level%20skill%20controller%2C%0Aand%20simulator.%20We%20also%20review%20the%20current%20development%20of%20technologies%20related%0Ato%20each%20module%20and%20highlight%20recent%20progress%20in%20adapting%20them%20to%20industrial%0Aapplications.%20Finally%2C%20we%20summarize%20the%20key%20challenges%20EIIR%20faces%20in%20industrial%0Ascenarios%20and%20suggest%20future%20research%20directions.%20We%20believe%20that%20EIIR%0Atechnology%20will%20shape%20the%20next%20generation%20of%20industrial%20robotics.%20Industrial%0Asystems%20based%20on%20embodied%20intelligent%20industrial%20robots%20offer%20strong%20potential%0Afor%20enabling%20intelligent%20manufacturing.%20We%20will%20continue%20to%20track%20and%20summarize%0Anew%20research%20in%20this%20area%20and%20hope%20this%20review%20will%20serve%20as%20a%20valuable%0Areference%20for%20scholars%20and%20engineers%20interested%20in%20industrial%20embodied%0Aintelligence.%20Together%2C%20we%20can%20help%20drive%20the%20rapid%20advancement%20and%20application%0Aof%20this%20technology.%20The%20associated%20project%20can%20be%20found%20at%0Ahttps%3A//github.com/jackeyzengl/Embodied_Intelligent_Industrial_Robotics_Paper_List.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09305v1&entry.124074799=Read"},
{"title": "Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:\n  Open Research Challenges", "author": "Mark Harman and Peter O'Hearn and Shubho Sengupta", "abstract": "  Despite decades of research and practice in automated software testing,\nseveral fundamental concepts remain ill-defined and under-explored, yet offer\nenormous potential real-world impact. We show that these concepts raise\nexciting new challenges in the context of Large Language Models for software\ntest generation. More specifically, we formally define and investigate the\nproperties of hardening and catching tests. A hardening test is one that seeks\nto protect against future regressions, while a catching test is one that\ncatches such a regression or a fault in new functionality introduced by a code\nchange. Hardening tests can be generated at any time and may become catching\ntests when a future regression is caught. We also define and motivate the\nCatching 'Just-in-Time' (JiTTest) Challenge, in which tests are generated\n'just-in-time' to catch new faults before they land into production. We show\nthat any solution to Catching JiTTest generation can also be repurposed to\ncatch latent faults in legacy code. We enumerate possible outcomes for\nhardening and catching tests and JiTTests, and discuss open research problems,\ndeployment options, and initial results from our work on automated LLM-based\nhardening at Meta. This paper was written to accompany the keynote by the\nauthors at the ACM International Conference on the Foundations of Software\nEngineering (FSE) 2025. Author order is alphabetical. The corresponding author\nis Mark Harman.\n", "link": "http://arxiv.org/abs/2504.16472v2", "date": "2025-05-14", "relevancy": 1.2429, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4294}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4263}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harden%20and%20Catch%20for%20Just-in-Time%20Assured%20LLM-Based%20Software%20Testing%3A%0A%20%20Open%20Research%20Challenges&body=Title%3A%20Harden%20and%20Catch%20for%20Just-in-Time%20Assured%20LLM-Based%20Software%20Testing%3A%0A%20%20Open%20Research%20Challenges%0AAuthor%3A%20Mark%20Harman%20and%20Peter%20O%27Hearn%20and%20Shubho%20Sengupta%0AAbstract%3A%20%20%20Despite%20decades%20of%20research%20and%20practice%20in%20automated%20software%20testing%2C%0Aseveral%20fundamental%20concepts%20remain%20ill-defined%20and%20under-explored%2C%20yet%20offer%0Aenormous%20potential%20real-world%20impact.%20We%20show%20that%20these%20concepts%20raise%0Aexciting%20new%20challenges%20in%20the%20context%20of%20Large%20Language%20Models%20for%20software%0Atest%20generation.%20More%20specifically%2C%20we%20formally%20define%20and%20investigate%20the%0Aproperties%20of%20hardening%20and%20catching%20tests.%20A%20hardening%20test%20is%20one%20that%20seeks%0Ato%20protect%20against%20future%20regressions%2C%20while%20a%20catching%20test%20is%20one%20that%0Acatches%20such%20a%20regression%20or%20a%20fault%20in%20new%20functionality%20introduced%20by%20a%20code%0Achange.%20Hardening%20tests%20can%20be%20generated%20at%20any%20time%20and%20may%20become%20catching%0Atests%20when%20a%20future%20regression%20is%20caught.%20We%20also%20define%20and%20motivate%20the%0ACatching%20%27Just-in-Time%27%20%28JiTTest%29%20Challenge%2C%20in%20which%20tests%20are%20generated%0A%27just-in-time%27%20to%20catch%20new%20faults%20before%20they%20land%20into%20production.%20We%20show%0Athat%20any%20solution%20to%20Catching%20JiTTest%20generation%20can%20also%20be%20repurposed%20to%0Acatch%20latent%20faults%20in%20legacy%20code.%20We%20enumerate%20possible%20outcomes%20for%0Ahardening%20and%20catching%20tests%20and%20JiTTests%2C%20and%20discuss%20open%20research%20problems%2C%0Adeployment%20options%2C%20and%20initial%20results%20from%20our%20work%20on%20automated%20LLM-based%0Ahardening%20at%20Meta.%20This%20paper%20was%20written%20to%20accompany%20the%20keynote%20by%20the%0Aauthors%20at%20the%20ACM%20International%20Conference%20on%20the%20Foundations%20of%20Software%0AEngineering%20%28FSE%29%202025.%20Author%20order%20is%20alphabetical.%20The%20corresponding%20author%0Ais%20Mark%20Harman.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16472v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarden%2520and%2520Catch%2520for%2520Just-in-Time%2520Assured%2520LLM-Based%2520Software%2520Testing%253A%250A%2520%2520Open%2520Research%2520Challenges%26entry.906535625%3DMark%2520Harman%2520and%2520Peter%2520O%2527Hearn%2520and%2520Shubho%2520Sengupta%26entry.1292438233%3D%2520%2520Despite%2520decades%2520of%2520research%2520and%2520practice%2520in%2520automated%2520software%2520testing%252C%250Aseveral%2520fundamental%2520concepts%2520remain%2520ill-defined%2520and%2520under-explored%252C%2520yet%2520offer%250Aenormous%2520potential%2520real-world%2520impact.%2520We%2520show%2520that%2520these%2520concepts%2520raise%250Aexciting%2520new%2520challenges%2520in%2520the%2520context%2520of%2520Large%2520Language%2520Models%2520for%2520software%250Atest%2520generation.%2520More%2520specifically%252C%2520we%2520formally%2520define%2520and%2520investigate%2520the%250Aproperties%2520of%2520hardening%2520and%2520catching%2520tests.%2520A%2520hardening%2520test%2520is%2520one%2520that%2520seeks%250Ato%2520protect%2520against%2520future%2520regressions%252C%2520while%2520a%2520catching%2520test%2520is%2520one%2520that%250Acatches%2520such%2520a%2520regression%2520or%2520a%2520fault%2520in%2520new%2520functionality%2520introduced%2520by%2520a%2520code%250Achange.%2520Hardening%2520tests%2520can%2520be%2520generated%2520at%2520any%2520time%2520and%2520may%2520become%2520catching%250Atests%2520when%2520a%2520future%2520regression%2520is%2520caught.%2520We%2520also%2520define%2520and%2520motivate%2520the%250ACatching%2520%2527Just-in-Time%2527%2520%2528JiTTest%2529%2520Challenge%252C%2520in%2520which%2520tests%2520are%2520generated%250A%2527just-in-time%2527%2520to%2520catch%2520new%2520faults%2520before%2520they%2520land%2520into%2520production.%2520We%2520show%250Athat%2520any%2520solution%2520to%2520Catching%2520JiTTest%2520generation%2520can%2520also%2520be%2520repurposed%2520to%250Acatch%2520latent%2520faults%2520in%2520legacy%2520code.%2520We%2520enumerate%2520possible%2520outcomes%2520for%250Ahardening%2520and%2520catching%2520tests%2520and%2520JiTTests%252C%2520and%2520discuss%2520open%2520research%2520problems%252C%250Adeployment%2520options%252C%2520and%2520initial%2520results%2520from%2520our%2520work%2520on%2520automated%2520LLM-based%250Ahardening%2520at%2520Meta.%2520This%2520paper%2520was%2520written%2520to%2520accompany%2520the%2520keynote%2520by%2520the%250Aauthors%2520at%2520the%2520ACM%2520International%2520Conference%2520on%2520the%2520Foundations%2520of%2520Software%250AEngineering%2520%2528FSE%2529%25202025.%2520Author%2520order%2520is%2520alphabetical.%2520The%2520corresponding%2520author%250Ais%2520Mark%2520Harman.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16472v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harden%20and%20Catch%20for%20Just-in-Time%20Assured%20LLM-Based%20Software%20Testing%3A%0A%20%20Open%20Research%20Challenges&entry.906535625=Mark%20Harman%20and%20Peter%20O%27Hearn%20and%20Shubho%20Sengupta&entry.1292438233=%20%20Despite%20decades%20of%20research%20and%20practice%20in%20automated%20software%20testing%2C%0Aseveral%20fundamental%20concepts%20remain%20ill-defined%20and%20under-explored%2C%20yet%20offer%0Aenormous%20potential%20real-world%20impact.%20We%20show%20that%20these%20concepts%20raise%0Aexciting%20new%20challenges%20in%20the%20context%20of%20Large%20Language%20Models%20for%20software%0Atest%20generation.%20More%20specifically%2C%20we%20formally%20define%20and%20investigate%20the%0Aproperties%20of%20hardening%20and%20catching%20tests.%20A%20hardening%20test%20is%20one%20that%20seeks%0Ato%20protect%20against%20future%20regressions%2C%20while%20a%20catching%20test%20is%20one%20that%0Acatches%20such%20a%20regression%20or%20a%20fault%20in%20new%20functionality%20introduced%20by%20a%20code%0Achange.%20Hardening%20tests%20can%20be%20generated%20at%20any%20time%20and%20may%20become%20catching%0Atests%20when%20a%20future%20regression%20is%20caught.%20We%20also%20define%20and%20motivate%20the%0ACatching%20%27Just-in-Time%27%20%28JiTTest%29%20Challenge%2C%20in%20which%20tests%20are%20generated%0A%27just-in-time%27%20to%20catch%20new%20faults%20before%20they%20land%20into%20production.%20We%20show%0Athat%20any%20solution%20to%20Catching%20JiTTest%20generation%20can%20also%20be%20repurposed%20to%0Acatch%20latent%20faults%20in%20legacy%20code.%20We%20enumerate%20possible%20outcomes%20for%0Ahardening%20and%20catching%20tests%20and%20JiTTests%2C%20and%20discuss%20open%20research%20problems%2C%0Adeployment%20options%2C%20and%20initial%20results%20from%20our%20work%20on%20automated%20LLM-based%0Ahardening%20at%20Meta.%20This%20paper%20was%20written%20to%20accompany%20the%20keynote%20by%20the%0Aauthors%20at%20the%20ACM%20International%20Conference%20on%20the%20Foundations%20of%20Software%0AEngineering%20%28FSE%29%202025.%20Author%20order%20is%20alphabetical.%20The%20corresponding%20author%0Ais%20Mark%20Harman.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16472v2&entry.124074799=Read"},
{"title": "Access Controls Will Solve the Dual-Use Dilemma", "author": "Ev\u017een Wybitul", "abstract": "  AI safety systems face a dual-use dilemma. Since the same request can be\neither harmless or harmful depending on who made it and why, if the system\nmakes decisions based solely on the request's content, it will refuse some\nlegitimate queries and let pass harmful ones. To address this, we propose a\nconceptual access control framework, based on verified user credentials (such\nas institutional affiliation) and classifiers that assign model outputs to risk\ncategories (such as advanced virology). The system permits responses only when\nthe user's verified credentials match the category's requirements. For\nimplementation of the model output classifiers, we introduce a theoretical\napproach utilizing small, gated expert modules integrated into the generator\nmodel, trained with gradient routing, that enable efficient risk detection\nwithout the capability gap problems of external monitors. While open questions\nremain about the verification mechanisms, risk categories, and the technical\nimplementation, our framework makes the first step toward enabling granular\ngovernance of AI capabilities: verified users gain access to specialized\nknowledge without arbitrary restrictions, while adversaries are blocked from\nit. This contextual approach reconciles model utility with robust safety,\naddressing the dual-use dilemma.\n", "link": "http://arxiv.org/abs/2505.09341v1", "date": "2025-05-14", "relevancy": 1.9107, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.491}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4802}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Access%20Controls%20Will%20Solve%20the%20Dual-Use%20Dilemma&body=Title%3A%20Access%20Controls%20Will%20Solve%20the%20Dual-Use%20Dilemma%0AAuthor%3A%20Ev%C5%BEen%20Wybitul%0AAbstract%3A%20%20%20AI%20safety%20systems%20face%20a%20dual-use%20dilemma.%20Since%20the%20same%20request%20can%20be%0Aeither%20harmless%20or%20harmful%20depending%20on%20who%20made%20it%20and%20why%2C%20if%20the%20system%0Amakes%20decisions%20based%20solely%20on%20the%20request%27s%20content%2C%20it%20will%20refuse%20some%0Alegitimate%20queries%20and%20let%20pass%20harmful%20ones.%20To%20address%20this%2C%20we%20propose%20a%0Aconceptual%20access%20control%20framework%2C%20based%20on%20verified%20user%20credentials%20%28such%0Aas%20institutional%20affiliation%29%20and%20classifiers%20that%20assign%20model%20outputs%20to%20risk%0Acategories%20%28such%20as%20advanced%20virology%29.%20The%20system%20permits%20responses%20only%20when%0Athe%20user%27s%20verified%20credentials%20match%20the%20category%27s%20requirements.%20For%0Aimplementation%20of%20the%20model%20output%20classifiers%2C%20we%20introduce%20a%20theoretical%0Aapproach%20utilizing%20small%2C%20gated%20expert%20modules%20integrated%20into%20the%20generator%0Amodel%2C%20trained%20with%20gradient%20routing%2C%20that%20enable%20efficient%20risk%20detection%0Awithout%20the%20capability%20gap%20problems%20of%20external%20monitors.%20While%20open%20questions%0Aremain%20about%20the%20verification%20mechanisms%2C%20risk%20categories%2C%20and%20the%20technical%0Aimplementation%2C%20our%20framework%20makes%20the%20first%20step%20toward%20enabling%20granular%0Agovernance%20of%20AI%20capabilities%3A%20verified%20users%20gain%20access%20to%20specialized%0Aknowledge%20without%20arbitrary%20restrictions%2C%20while%20adversaries%20are%20blocked%20from%0Ait.%20This%20contextual%20approach%20reconciles%20model%20utility%20with%20robust%20safety%2C%0Aaddressing%20the%20dual-use%20dilemma.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09341v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccess%2520Controls%2520Will%2520Solve%2520the%2520Dual-Use%2520Dilemma%26entry.906535625%3DEv%25C5%25BEen%2520Wybitul%26entry.1292438233%3D%2520%2520AI%2520safety%2520systems%2520face%2520a%2520dual-use%2520dilemma.%2520Since%2520the%2520same%2520request%2520can%2520be%250Aeither%2520harmless%2520or%2520harmful%2520depending%2520on%2520who%2520made%2520it%2520and%2520why%252C%2520if%2520the%2520system%250Amakes%2520decisions%2520based%2520solely%2520on%2520the%2520request%2527s%2520content%252C%2520it%2520will%2520refuse%2520some%250Alegitimate%2520queries%2520and%2520let%2520pass%2520harmful%2520ones.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Aconceptual%2520access%2520control%2520framework%252C%2520based%2520on%2520verified%2520user%2520credentials%2520%2528such%250Aas%2520institutional%2520affiliation%2529%2520and%2520classifiers%2520that%2520assign%2520model%2520outputs%2520to%2520risk%250Acategories%2520%2528such%2520as%2520advanced%2520virology%2529.%2520The%2520system%2520permits%2520responses%2520only%2520when%250Athe%2520user%2527s%2520verified%2520credentials%2520match%2520the%2520category%2527s%2520requirements.%2520For%250Aimplementation%2520of%2520the%2520model%2520output%2520classifiers%252C%2520we%2520introduce%2520a%2520theoretical%250Aapproach%2520utilizing%2520small%252C%2520gated%2520expert%2520modules%2520integrated%2520into%2520the%2520generator%250Amodel%252C%2520trained%2520with%2520gradient%2520routing%252C%2520that%2520enable%2520efficient%2520risk%2520detection%250Awithout%2520the%2520capability%2520gap%2520problems%2520of%2520external%2520monitors.%2520While%2520open%2520questions%250Aremain%2520about%2520the%2520verification%2520mechanisms%252C%2520risk%2520categories%252C%2520and%2520the%2520technical%250Aimplementation%252C%2520our%2520framework%2520makes%2520the%2520first%2520step%2520toward%2520enabling%2520granular%250Agovernance%2520of%2520AI%2520capabilities%253A%2520verified%2520users%2520gain%2520access%2520to%2520specialized%250Aknowledge%2520without%2520arbitrary%2520restrictions%252C%2520while%2520adversaries%2520are%2520blocked%2520from%250Ait.%2520This%2520contextual%2520approach%2520reconciles%2520model%2520utility%2520with%2520robust%2520safety%252C%250Aaddressing%2520the%2520dual-use%2520dilemma.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09341v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Access%20Controls%20Will%20Solve%20the%20Dual-Use%20Dilemma&entry.906535625=Ev%C5%BEen%20Wybitul&entry.1292438233=%20%20AI%20safety%20systems%20face%20a%20dual-use%20dilemma.%20Since%20the%20same%20request%20can%20be%0Aeither%20harmless%20or%20harmful%20depending%20on%20who%20made%20it%20and%20why%2C%20if%20the%20system%0Amakes%20decisions%20based%20solely%20on%20the%20request%27s%20content%2C%20it%20will%20refuse%20some%0Alegitimate%20queries%20and%20let%20pass%20harmful%20ones.%20To%20address%20this%2C%20we%20propose%20a%0Aconceptual%20access%20control%20framework%2C%20based%20on%20verified%20user%20credentials%20%28such%0Aas%20institutional%20affiliation%29%20and%20classifiers%20that%20assign%20model%20outputs%20to%20risk%0Acategories%20%28such%20as%20advanced%20virology%29.%20The%20system%20permits%20responses%20only%20when%0Athe%20user%27s%20verified%20credentials%20match%20the%20category%27s%20requirements.%20For%0Aimplementation%20of%20the%20model%20output%20classifiers%2C%20we%20introduce%20a%20theoretical%0Aapproach%20utilizing%20small%2C%20gated%20expert%20modules%20integrated%20into%20the%20generator%0Amodel%2C%20trained%20with%20gradient%20routing%2C%20that%20enable%20efficient%20risk%20detection%0Awithout%20the%20capability%20gap%20problems%20of%20external%20monitors.%20While%20open%20questions%0Aremain%20about%20the%20verification%20mechanisms%2C%20risk%20categories%2C%20and%20the%20technical%0Aimplementation%2C%20our%20framework%20makes%20the%20first%20step%20toward%20enabling%20granular%0Agovernance%20of%20AI%20capabilities%3A%20verified%20users%20gain%20access%20to%20specialized%0Aknowledge%20without%20arbitrary%20restrictions%2C%20while%20adversaries%20are%20blocked%20from%0Ait.%20This%20contextual%20approach%20reconciles%20model%20utility%20with%20robust%20safety%2C%0Aaddressing%20the%20dual-use%20dilemma.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09341v1&entry.124074799=Read"},
{"title": "Predicting butterfly species presence from satellite imagery using soft\n  contrastive regularisation", "author": "Thijs L van der Plas and Stephen Law and Michael JO Pocock", "abstract": "  The growing demand for scalable biodiversity monitoring methods has fuelled\ninterest in remote sensing data, due to its widespread availability and\nextensive coverage. Traditionally, the application of remote sensing to\nbiodiversity research has focused on mapping and monitoring habitats, but with\nincreasing availability of large-scale citizen-science wildlife observation\ndata, recent methods have started to explore predicting multi-species presence\ndirectly from satellite images. This paper presents a new data set for\npredicting butterfly species presence from satellite data in the United\nKingdom. We experimentally optimise a Resnet-based model to predict\nmulti-species presence from 4-band satellite images, and find that this model\nespecially outperforms the mean rate baseline for locations with high species\nbiodiversity. To improve performance, we develop a soft, supervised contrastive\nregularisation loss that is tailored to probabilistic labels (such as\nspecies-presence data), and demonstrate that this improves prediction accuracy.\nIn summary, our new data set and contrastive regularisation method contribute\nto the open challenge of accurately predicting species biodiversity from remote\nsensing data, which is key for efficient biodiversity monitoring.\n", "link": "http://arxiv.org/abs/2505.09306v1", "date": "2025-05-14", "relevancy": 1.481, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5059}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4953}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20butterfly%20species%20presence%20from%20satellite%20imagery%20using%20soft%0A%20%20contrastive%20regularisation&body=Title%3A%20Predicting%20butterfly%20species%20presence%20from%20satellite%20imagery%20using%20soft%0A%20%20contrastive%20regularisation%0AAuthor%3A%20Thijs%20L%20van%20der%20Plas%20and%20Stephen%20Law%20and%20Michael%20JO%20Pocock%0AAbstract%3A%20%20%20The%20growing%20demand%20for%20scalable%20biodiversity%20monitoring%20methods%20has%20fuelled%0Ainterest%20in%20remote%20sensing%20data%2C%20due%20to%20its%20widespread%20availability%20and%0Aextensive%20coverage.%20Traditionally%2C%20the%20application%20of%20remote%20sensing%20to%0Abiodiversity%20research%20has%20focused%20on%20mapping%20and%20monitoring%20habitats%2C%20but%20with%0Aincreasing%20availability%20of%20large-scale%20citizen-science%20wildlife%20observation%0Adata%2C%20recent%20methods%20have%20started%20to%20explore%20predicting%20multi-species%20presence%0Adirectly%20from%20satellite%20images.%20This%20paper%20presents%20a%20new%20data%20set%20for%0Apredicting%20butterfly%20species%20presence%20from%20satellite%20data%20in%20the%20United%0AKingdom.%20We%20experimentally%20optimise%20a%20Resnet-based%20model%20to%20predict%0Amulti-species%20presence%20from%204-band%20satellite%20images%2C%20and%20find%20that%20this%20model%0Aespecially%20outperforms%20the%20mean%20rate%20baseline%20for%20locations%20with%20high%20species%0Abiodiversity.%20To%20improve%20performance%2C%20we%20develop%20a%20soft%2C%20supervised%20contrastive%0Aregularisation%20loss%20that%20is%20tailored%20to%20probabilistic%20labels%20%28such%20as%0Aspecies-presence%20data%29%2C%20and%20demonstrate%20that%20this%20improves%20prediction%20accuracy.%0AIn%20summary%2C%20our%20new%20data%20set%20and%20contrastive%20regularisation%20method%20contribute%0Ato%20the%20open%20challenge%20of%20accurately%20predicting%20species%20biodiversity%20from%20remote%0Asensing%20data%2C%20which%20is%20key%20for%20efficient%20biodiversity%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520butterfly%2520species%2520presence%2520from%2520satellite%2520imagery%2520using%2520soft%250A%2520%2520contrastive%2520regularisation%26entry.906535625%3DThijs%2520L%2520van%2520der%2520Plas%2520and%2520Stephen%2520Law%2520and%2520Michael%2520JO%2520Pocock%26entry.1292438233%3D%2520%2520The%2520growing%2520demand%2520for%2520scalable%2520biodiversity%2520monitoring%2520methods%2520has%2520fuelled%250Ainterest%2520in%2520remote%2520sensing%2520data%252C%2520due%2520to%2520its%2520widespread%2520availability%2520and%250Aextensive%2520coverage.%2520Traditionally%252C%2520the%2520application%2520of%2520remote%2520sensing%2520to%250Abiodiversity%2520research%2520has%2520focused%2520on%2520mapping%2520and%2520monitoring%2520habitats%252C%2520but%2520with%250Aincreasing%2520availability%2520of%2520large-scale%2520citizen-science%2520wildlife%2520observation%250Adata%252C%2520recent%2520methods%2520have%2520started%2520to%2520explore%2520predicting%2520multi-species%2520presence%250Adirectly%2520from%2520satellite%2520images.%2520This%2520paper%2520presents%2520a%2520new%2520data%2520set%2520for%250Apredicting%2520butterfly%2520species%2520presence%2520from%2520satellite%2520data%2520in%2520the%2520United%250AKingdom.%2520We%2520experimentally%2520optimise%2520a%2520Resnet-based%2520model%2520to%2520predict%250Amulti-species%2520presence%2520from%25204-band%2520satellite%2520images%252C%2520and%2520find%2520that%2520this%2520model%250Aespecially%2520outperforms%2520the%2520mean%2520rate%2520baseline%2520for%2520locations%2520with%2520high%2520species%250Abiodiversity.%2520To%2520improve%2520performance%252C%2520we%2520develop%2520a%2520soft%252C%2520supervised%2520contrastive%250Aregularisation%2520loss%2520that%2520is%2520tailored%2520to%2520probabilistic%2520labels%2520%2528such%2520as%250Aspecies-presence%2520data%2529%252C%2520and%2520demonstrate%2520that%2520this%2520improves%2520prediction%2520accuracy.%250AIn%2520summary%252C%2520our%2520new%2520data%2520set%2520and%2520contrastive%2520regularisation%2520method%2520contribute%250Ato%2520the%2520open%2520challenge%2520of%2520accurately%2520predicting%2520species%2520biodiversity%2520from%2520remote%250Asensing%2520data%252C%2520which%2520is%2520key%2520for%2520efficient%2520biodiversity%2520monitoring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20butterfly%20species%20presence%20from%20satellite%20imagery%20using%20soft%0A%20%20contrastive%20regularisation&entry.906535625=Thijs%20L%20van%20der%20Plas%20and%20Stephen%20Law%20and%20Michael%20JO%20Pocock&entry.1292438233=%20%20The%20growing%20demand%20for%20scalable%20biodiversity%20monitoring%20methods%20has%20fuelled%0Ainterest%20in%20remote%20sensing%20data%2C%20due%20to%20its%20widespread%20availability%20and%0Aextensive%20coverage.%20Traditionally%2C%20the%20application%20of%20remote%20sensing%20to%0Abiodiversity%20research%20has%20focused%20on%20mapping%20and%20monitoring%20habitats%2C%20but%20with%0Aincreasing%20availability%20of%20large-scale%20citizen-science%20wildlife%20observation%0Adata%2C%20recent%20methods%20have%20started%20to%20explore%20predicting%20multi-species%20presence%0Adirectly%20from%20satellite%20images.%20This%20paper%20presents%20a%20new%20data%20set%20for%0Apredicting%20butterfly%20species%20presence%20from%20satellite%20data%20in%20the%20United%0AKingdom.%20We%20experimentally%20optimise%20a%20Resnet-based%20model%20to%20predict%0Amulti-species%20presence%20from%204-band%20satellite%20images%2C%20and%20find%20that%20this%20model%0Aespecially%20outperforms%20the%20mean%20rate%20baseline%20for%20locations%20with%20high%20species%0Abiodiversity.%20To%20improve%20performance%2C%20we%20develop%20a%20soft%2C%20supervised%20contrastive%0Aregularisation%20loss%20that%20is%20tailored%20to%20probabilistic%20labels%20%28such%20as%0Aspecies-presence%20data%29%2C%20and%20demonstrate%20that%20this%20improves%20prediction%20accuracy.%0AIn%20summary%2C%20our%20new%20data%20set%20and%20contrastive%20regularisation%20method%20contribute%0Ato%20the%20open%20challenge%20of%20accurately%20predicting%20species%20biodiversity%20from%20remote%0Asensing%20data%2C%20which%20is%20key%20for%20efficient%20biodiversity%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09306v1&entry.124074799=Read"},
{"title": "EDBench: Large-Scale Electron Density Data for Molecular Modeling", "author": "Hongxin Xiang and Ke Li and Mingquan Liu and Zhixiang Cheng and Bin Yao and Wenjie Du and Jun Xia and Li Zeng and Xin Jin and Xiangxiang Zeng", "abstract": "  Existing molecular machine learning force fields (MLFFs) generally focus on\nthe learning of atoms, molecules, and simple quantum chemical properties (such\nas energy and force), but ignore the importance of electron density (ED)\n$\\rho(r)$ in accurately understanding molecular force fields (MFFs). ED\ndescribes the probability of finding electrons at specific locations around\natoms or molecules, which uniquely determines all ground state properties (such\nas energy, molecular structure, etc.) of interactive multi-particle systems\naccording to the Hohenberg-Kohn theorem. However, the calculation of ED relies\non the time-consuming first-principles density functional theory (DFT) which\nleads to the lack of large-scale ED data and limits its application in MLFFs.\nIn this paper, we introduce EDBench, a large-scale, high-quality dataset of ED\ndesigned to advance learning-based research at the electronic scale. Built upon\nthe PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million\nmolecules. To comprehensively evaluate the ability of models to understand and\nutilize electronic information, we design a suite of ED-centric benchmark tasks\nspanning prediction, retrieval, and generation. Our evaluation on several\nstate-of-the-art methods demonstrates that learning from EDBench is not only\nfeasible but also achieves high accuracy. Moreover, we show that learning-based\nmethod can efficiently calculate ED with comparable precision while\nsignificantly reducing the computational cost relative to traditional DFT\ncalculations. All data and benchmarks from EDBench will be freely available,\nlaying a robust foundation for ED-driven drug discovery and materials science.\n", "link": "http://arxiv.org/abs/2505.09262v1", "date": "2025-05-14", "relevancy": 1.7822, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4864}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EDBench%3A%20Large-Scale%20Electron%20Density%20Data%20for%20Molecular%20Modeling&body=Title%3A%20EDBench%3A%20Large-Scale%20Electron%20Density%20Data%20for%20Molecular%20Modeling%0AAuthor%3A%20Hongxin%20Xiang%20and%20Ke%20Li%20and%20Mingquan%20Liu%20and%20Zhixiang%20Cheng%20and%20Bin%20Yao%20and%20Wenjie%20Du%20and%20Jun%20Xia%20and%20Li%20Zeng%20and%20Xin%20Jin%20and%20Xiangxiang%20Zeng%0AAbstract%3A%20%20%20Existing%20molecular%20machine%20learning%20force%20fields%20%28MLFFs%29%20generally%20focus%20on%0Athe%20learning%20of%20atoms%2C%20molecules%2C%20and%20simple%20quantum%20chemical%20properties%20%28such%0Aas%20energy%20and%20force%29%2C%20but%20ignore%20the%20importance%20of%20electron%20density%20%28ED%29%0A%24%5Crho%28r%29%24%20in%20accurately%20understanding%20molecular%20force%20fields%20%28MFFs%29.%20ED%0Adescribes%20the%20probability%20of%20finding%20electrons%20at%20specific%20locations%20around%0Aatoms%20or%20molecules%2C%20which%20uniquely%20determines%20all%20ground%20state%20properties%20%28such%0Aas%20energy%2C%20molecular%20structure%2C%20etc.%29%20of%20interactive%20multi-particle%20systems%0Aaccording%20to%20the%20Hohenberg-Kohn%20theorem.%20However%2C%20the%20calculation%20of%20ED%20relies%0Aon%20the%20time-consuming%20first-principles%20density%20functional%20theory%20%28DFT%29%20which%0Aleads%20to%20the%20lack%20of%20large-scale%20ED%20data%20and%20limits%20its%20application%20in%20MLFFs.%0AIn%20this%20paper%2C%20we%20introduce%20EDBench%2C%20a%20large-scale%2C%20high-quality%20dataset%20of%20ED%0Adesigned%20to%20advance%20learning-based%20research%20at%20the%20electronic%20scale.%20Built%20upon%0Athe%20PCQM4Mv2%2C%20EDBench%20provides%20accurate%20ED%20data%2C%20covering%203.3%20million%0Amolecules.%20To%20comprehensively%20evaluate%20the%20ability%20of%20models%20to%20understand%20and%0Autilize%20electronic%20information%2C%20we%20design%20a%20suite%20of%20ED-centric%20benchmark%20tasks%0Aspanning%20prediction%2C%20retrieval%2C%20and%20generation.%20Our%20evaluation%20on%20several%0Astate-of-the-art%20methods%20demonstrates%20that%20learning%20from%20EDBench%20is%20not%20only%0Afeasible%20but%20also%20achieves%20high%20accuracy.%20Moreover%2C%20we%20show%20that%20learning-based%0Amethod%20can%20efficiently%20calculate%20ED%20with%20comparable%20precision%20while%0Asignificantly%20reducing%20the%20computational%20cost%20relative%20to%20traditional%20DFT%0Acalculations.%20All%20data%20and%20benchmarks%20from%20EDBench%20will%20be%20freely%20available%2C%0Alaying%20a%20robust%20foundation%20for%20ED-driven%20drug%20discovery%20and%20materials%20science.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEDBench%253A%2520Large-Scale%2520Electron%2520Density%2520Data%2520for%2520Molecular%2520Modeling%26entry.906535625%3DHongxin%2520Xiang%2520and%2520Ke%2520Li%2520and%2520Mingquan%2520Liu%2520and%2520Zhixiang%2520Cheng%2520and%2520Bin%2520Yao%2520and%2520Wenjie%2520Du%2520and%2520Jun%2520Xia%2520and%2520Li%2520Zeng%2520and%2520Xin%2520Jin%2520and%2520Xiangxiang%2520Zeng%26entry.1292438233%3D%2520%2520Existing%2520molecular%2520machine%2520learning%2520force%2520fields%2520%2528MLFFs%2529%2520generally%2520focus%2520on%250Athe%2520learning%2520of%2520atoms%252C%2520molecules%252C%2520and%2520simple%2520quantum%2520chemical%2520properties%2520%2528such%250Aas%2520energy%2520and%2520force%2529%252C%2520but%2520ignore%2520the%2520importance%2520of%2520electron%2520density%2520%2528ED%2529%250A%2524%255Crho%2528r%2529%2524%2520in%2520accurately%2520understanding%2520molecular%2520force%2520fields%2520%2528MFFs%2529.%2520ED%250Adescribes%2520the%2520probability%2520of%2520finding%2520electrons%2520at%2520specific%2520locations%2520around%250Aatoms%2520or%2520molecules%252C%2520which%2520uniquely%2520determines%2520all%2520ground%2520state%2520properties%2520%2528such%250Aas%2520energy%252C%2520molecular%2520structure%252C%2520etc.%2529%2520of%2520interactive%2520multi-particle%2520systems%250Aaccording%2520to%2520the%2520Hohenberg-Kohn%2520theorem.%2520However%252C%2520the%2520calculation%2520of%2520ED%2520relies%250Aon%2520the%2520time-consuming%2520first-principles%2520density%2520functional%2520theory%2520%2528DFT%2529%2520which%250Aleads%2520to%2520the%2520lack%2520of%2520large-scale%2520ED%2520data%2520and%2520limits%2520its%2520application%2520in%2520MLFFs.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520EDBench%252C%2520a%2520large-scale%252C%2520high-quality%2520dataset%2520of%2520ED%250Adesigned%2520to%2520advance%2520learning-based%2520research%2520at%2520the%2520electronic%2520scale.%2520Built%2520upon%250Athe%2520PCQM4Mv2%252C%2520EDBench%2520provides%2520accurate%2520ED%2520data%252C%2520covering%25203.3%2520million%250Amolecules.%2520To%2520comprehensively%2520evaluate%2520the%2520ability%2520of%2520models%2520to%2520understand%2520and%250Autilize%2520electronic%2520information%252C%2520we%2520design%2520a%2520suite%2520of%2520ED-centric%2520benchmark%2520tasks%250Aspanning%2520prediction%252C%2520retrieval%252C%2520and%2520generation.%2520Our%2520evaluation%2520on%2520several%250Astate-of-the-art%2520methods%2520demonstrates%2520that%2520learning%2520from%2520EDBench%2520is%2520not%2520only%250Afeasible%2520but%2520also%2520achieves%2520high%2520accuracy.%2520Moreover%252C%2520we%2520show%2520that%2520learning-based%250Amethod%2520can%2520efficiently%2520calculate%2520ED%2520with%2520comparable%2520precision%2520while%250Asignificantly%2520reducing%2520the%2520computational%2520cost%2520relative%2520to%2520traditional%2520DFT%250Acalculations.%2520All%2520data%2520and%2520benchmarks%2520from%2520EDBench%2520will%2520be%2520freely%2520available%252C%250Alaying%2520a%2520robust%2520foundation%2520for%2520ED-driven%2520drug%2520discovery%2520and%2520materials%2520science.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EDBench%3A%20Large-Scale%20Electron%20Density%20Data%20for%20Molecular%20Modeling&entry.906535625=Hongxin%20Xiang%20and%20Ke%20Li%20and%20Mingquan%20Liu%20and%20Zhixiang%20Cheng%20and%20Bin%20Yao%20and%20Wenjie%20Du%20and%20Jun%20Xia%20and%20Li%20Zeng%20and%20Xin%20Jin%20and%20Xiangxiang%20Zeng&entry.1292438233=%20%20Existing%20molecular%20machine%20learning%20force%20fields%20%28MLFFs%29%20generally%20focus%20on%0Athe%20learning%20of%20atoms%2C%20molecules%2C%20and%20simple%20quantum%20chemical%20properties%20%28such%0Aas%20energy%20and%20force%29%2C%20but%20ignore%20the%20importance%20of%20electron%20density%20%28ED%29%0A%24%5Crho%28r%29%24%20in%20accurately%20understanding%20molecular%20force%20fields%20%28MFFs%29.%20ED%0Adescribes%20the%20probability%20of%20finding%20electrons%20at%20specific%20locations%20around%0Aatoms%20or%20molecules%2C%20which%20uniquely%20determines%20all%20ground%20state%20properties%20%28such%0Aas%20energy%2C%20molecular%20structure%2C%20etc.%29%20of%20interactive%20multi-particle%20systems%0Aaccording%20to%20the%20Hohenberg-Kohn%20theorem.%20However%2C%20the%20calculation%20of%20ED%20relies%0Aon%20the%20time-consuming%20first-principles%20density%20functional%20theory%20%28DFT%29%20which%0Aleads%20to%20the%20lack%20of%20large-scale%20ED%20data%20and%20limits%20its%20application%20in%20MLFFs.%0AIn%20this%20paper%2C%20we%20introduce%20EDBench%2C%20a%20large-scale%2C%20high-quality%20dataset%20of%20ED%0Adesigned%20to%20advance%20learning-based%20research%20at%20the%20electronic%20scale.%20Built%20upon%0Athe%20PCQM4Mv2%2C%20EDBench%20provides%20accurate%20ED%20data%2C%20covering%203.3%20million%0Amolecules.%20To%20comprehensively%20evaluate%20the%20ability%20of%20models%20to%20understand%20and%0Autilize%20electronic%20information%2C%20we%20design%20a%20suite%20of%20ED-centric%20benchmark%20tasks%0Aspanning%20prediction%2C%20retrieval%2C%20and%20generation.%20Our%20evaluation%20on%20several%0Astate-of-the-art%20methods%20demonstrates%20that%20learning%20from%20EDBench%20is%20not%20only%0Afeasible%20but%20also%20achieves%20high%20accuracy.%20Moreover%2C%20we%20show%20that%20learning-based%0Amethod%20can%20efficiently%20calculate%20ED%20with%20comparable%20precision%20while%0Asignificantly%20reducing%20the%20computational%20cost%20relative%20to%20traditional%20DFT%0Acalculations.%20All%20data%20and%20benchmarks%20from%20EDBench%20will%20be%20freely%20available%2C%0Alaying%20a%20robust%20foundation%20for%20ED-driven%20drug%20discovery%20and%20materials%20science.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09262v1&entry.124074799=Read"},
{"title": "Contactless Cardiac Pulse Monitoring Using Event Cameras", "author": "Mohamed Moustafa and Joseph Lemley and Peter Corcoran", "abstract": "  Time event cameras are a novel technology for recording scene information at\nextremely low latency and with low power consumption. Event cameras output a\nstream of events that encapsulate pixel-level light intensity changes within\nthe scene, capturing information with a higher dynamic range and temporal\nresolution than traditional cameras. This study investigates the contact-free\nreconstruction of an individual's cardiac pulse signal from time event\nrecording of their face using a supervised convolutional neural network (CNN)\nmodel. An end-to-end model is trained to extract the cardiac signal from a\ntwo-dimensional representation of the event stream, with model performance\nevaluated based on the accuracy of the calculated heart rate. The experimental\nresults confirm that physiological cardiac information in the facial region is\neffectively preserved within the event stream, showcasing the potential of this\nnovel sensor for remote heart rate monitoring. The model trained on event\nframes achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)\ncompared to the RMSE of 2.92 bpm achieved by the baseline model trained on\nstandard camera frames. Furthermore, models trained on event frames generated\nat 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an\nRMSE of 2.54 and 2.13 bpm, respectively.\n", "link": "http://arxiv.org/abs/2505.09529v1", "date": "2025-05-14", "relevancy": 1.546, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5738}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5086}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contactless%20Cardiac%20Pulse%20Monitoring%20Using%20Event%20Cameras&body=Title%3A%20Contactless%20Cardiac%20Pulse%20Monitoring%20Using%20Event%20Cameras%0AAuthor%3A%20Mohamed%20Moustafa%20and%20Joseph%20Lemley%20and%20Peter%20Corcoran%0AAbstract%3A%20%20%20Time%20event%20cameras%20are%20a%20novel%20technology%20for%20recording%20scene%20information%20at%0Aextremely%20low%20latency%20and%20with%20low%20power%20consumption.%20Event%20cameras%20output%20a%0Astream%20of%20events%20that%20encapsulate%20pixel-level%20light%20intensity%20changes%20within%0Athe%20scene%2C%20capturing%20information%20with%20a%20higher%20dynamic%20range%20and%20temporal%0Aresolution%20than%20traditional%20cameras.%20This%20study%20investigates%20the%20contact-free%0Areconstruction%20of%20an%20individual%27s%20cardiac%20pulse%20signal%20from%20time%20event%0Arecording%20of%20their%20face%20using%20a%20supervised%20convolutional%20neural%20network%20%28CNN%29%0Amodel.%20An%20end-to-end%20model%20is%20trained%20to%20extract%20the%20cardiac%20signal%20from%20a%0Atwo-dimensional%20representation%20of%20the%20event%20stream%2C%20with%20model%20performance%0Aevaluated%20based%20on%20the%20accuracy%20of%20the%20calculated%20heart%20rate.%20The%20experimental%0Aresults%20confirm%20that%20physiological%20cardiac%20information%20in%20the%20facial%20region%20is%0Aeffectively%20preserved%20within%20the%20event%20stream%2C%20showcasing%20the%20potential%20of%20this%0Anovel%20sensor%20for%20remote%20heart%20rate%20monitoring.%20The%20model%20trained%20on%20event%0Aframes%20achieves%20a%20root%20mean%20square%20error%20%28RMSE%29%20of%203.32%20beats%20per%20minute%20%28bpm%29%0Acompared%20to%20the%20RMSE%20of%202.92%20bpm%20achieved%20by%20the%20baseline%20model%20trained%20on%0Astandard%20camera%20frames.%20Furthermore%2C%20models%20trained%20on%20event%20frames%20generated%0Aat%2060%20and%20120%20FPS%20outperformed%20the%2030%20FPS%20standard%20camera%20results%2C%20achieving%20an%0ARMSE%20of%202.54%20and%202.13%20bpm%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContactless%2520Cardiac%2520Pulse%2520Monitoring%2520Using%2520Event%2520Cameras%26entry.906535625%3DMohamed%2520Moustafa%2520and%2520Joseph%2520Lemley%2520and%2520Peter%2520Corcoran%26entry.1292438233%3D%2520%2520Time%2520event%2520cameras%2520are%2520a%2520novel%2520technology%2520for%2520recording%2520scene%2520information%2520at%250Aextremely%2520low%2520latency%2520and%2520with%2520low%2520power%2520consumption.%2520Event%2520cameras%2520output%2520a%250Astream%2520of%2520events%2520that%2520encapsulate%2520pixel-level%2520light%2520intensity%2520changes%2520within%250Athe%2520scene%252C%2520capturing%2520information%2520with%2520a%2520higher%2520dynamic%2520range%2520and%2520temporal%250Aresolution%2520than%2520traditional%2520cameras.%2520This%2520study%2520investigates%2520the%2520contact-free%250Areconstruction%2520of%2520an%2520individual%2527s%2520cardiac%2520pulse%2520signal%2520from%2520time%2520event%250Arecording%2520of%2520their%2520face%2520using%2520a%2520supervised%2520convolutional%2520neural%2520network%2520%2528CNN%2529%250Amodel.%2520An%2520end-to-end%2520model%2520is%2520trained%2520to%2520extract%2520the%2520cardiac%2520signal%2520from%2520a%250Atwo-dimensional%2520representation%2520of%2520the%2520event%2520stream%252C%2520with%2520model%2520performance%250Aevaluated%2520based%2520on%2520the%2520accuracy%2520of%2520the%2520calculated%2520heart%2520rate.%2520The%2520experimental%250Aresults%2520confirm%2520that%2520physiological%2520cardiac%2520information%2520in%2520the%2520facial%2520region%2520is%250Aeffectively%2520preserved%2520within%2520the%2520event%2520stream%252C%2520showcasing%2520the%2520potential%2520of%2520this%250Anovel%2520sensor%2520for%2520remote%2520heart%2520rate%2520monitoring.%2520The%2520model%2520trained%2520on%2520event%250Aframes%2520achieves%2520a%2520root%2520mean%2520square%2520error%2520%2528RMSE%2529%2520of%25203.32%2520beats%2520per%2520minute%2520%2528bpm%2529%250Acompared%2520to%2520the%2520RMSE%2520of%25202.92%2520bpm%2520achieved%2520by%2520the%2520baseline%2520model%2520trained%2520on%250Astandard%2520camera%2520frames.%2520Furthermore%252C%2520models%2520trained%2520on%2520event%2520frames%2520generated%250Aat%252060%2520and%2520120%2520FPS%2520outperformed%2520the%252030%2520FPS%2520standard%2520camera%2520results%252C%2520achieving%2520an%250ARMSE%2520of%25202.54%2520and%25202.13%2520bpm%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contactless%20Cardiac%20Pulse%20Monitoring%20Using%20Event%20Cameras&entry.906535625=Mohamed%20Moustafa%20and%20Joseph%20Lemley%20and%20Peter%20Corcoran&entry.1292438233=%20%20Time%20event%20cameras%20are%20a%20novel%20technology%20for%20recording%20scene%20information%20at%0Aextremely%20low%20latency%20and%20with%20low%20power%20consumption.%20Event%20cameras%20output%20a%0Astream%20of%20events%20that%20encapsulate%20pixel-level%20light%20intensity%20changes%20within%0Athe%20scene%2C%20capturing%20information%20with%20a%20higher%20dynamic%20range%20and%20temporal%0Aresolution%20than%20traditional%20cameras.%20This%20study%20investigates%20the%20contact-free%0Areconstruction%20of%20an%20individual%27s%20cardiac%20pulse%20signal%20from%20time%20event%0Arecording%20of%20their%20face%20using%20a%20supervised%20convolutional%20neural%20network%20%28CNN%29%0Amodel.%20An%20end-to-end%20model%20is%20trained%20to%20extract%20the%20cardiac%20signal%20from%20a%0Atwo-dimensional%20representation%20of%20the%20event%20stream%2C%20with%20model%20performance%0Aevaluated%20based%20on%20the%20accuracy%20of%20the%20calculated%20heart%20rate.%20The%20experimental%0Aresults%20confirm%20that%20physiological%20cardiac%20information%20in%20the%20facial%20region%20is%0Aeffectively%20preserved%20within%20the%20event%20stream%2C%20showcasing%20the%20potential%20of%20this%0Anovel%20sensor%20for%20remote%20heart%20rate%20monitoring.%20The%20model%20trained%20on%20event%0Aframes%20achieves%20a%20root%20mean%20square%20error%20%28RMSE%29%20of%203.32%20beats%20per%20minute%20%28bpm%29%0Acompared%20to%20the%20RMSE%20of%202.92%20bpm%20achieved%20by%20the%20baseline%20model%20trained%20on%0Astandard%20camera%20frames.%20Furthermore%2C%20models%20trained%20on%20event%20frames%20generated%0Aat%2060%20and%20120%20FPS%20outperformed%20the%2030%20FPS%20standard%20camera%20results%2C%20achieving%20an%0ARMSE%20of%202.54%20and%202.13%20bpm%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09529v1&entry.124074799=Read"},
{"title": "Generating Full-field Evolution of Physical Dynamics from Irregular\n  Sparse Observations", "author": "Panqi Chen and Yifan Sun and Lei Cheng and Yang Yang and Weichang Li and Yang Liu and Weiqing Liu and Jiang Bian and Shikai Fang", "abstract": "  Modeling and reconstructing multidimensional physical dynamics from sparse\nand off-grid observations presents a fundamental challenge in scientific\nresearch. Recently, diffusion-based generative modeling shows promising\npotential for physical simulation. However, current approaches typically\noperate on on-grid data with preset spatiotemporal resolution, but struggle\nwith the sparsely observed and continuous nature of real-world physical\ndynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in\nFunctional Tucker space, a novel framework that generates full-field evolution\nof physical dynamics from irregular sparse observations. SDIFT leverages the\nfunctional Tucker model as the latent space representer with proven universal\napproximation property, and represents observations as latent functions and\nTucker core sequences. We then construct a sequential diffusion model with\ntemporally augmented UNet in the functional Tucker space, denoising noise drawn\nfrom a Gaussian process to generate the sequence of core tensors.\n  At the posterior sampling stage, we propose a Message-Passing Posterior\nSampling mechanism, enabling conditional generation of the entire sequence\nguided by observations at limited time steps. We validate SDIFT on three\nphysical systems spanning astronomical (supernova explosions, light-year\nscale), environmental (ocean sound speed fields, kilometer scale), and\nmolecular (organic liquid, millimeter scale) domains, demonstrating significant\nimprovements in both reconstruction accuracy and computational efficiency\ncompared to state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2505.09284v1", "date": "2025-05-14", "relevancy": 1.8385, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6316}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6137}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Full-field%20Evolution%20of%20Physical%20Dynamics%20from%20Irregular%0A%20%20Sparse%20Observations&body=Title%3A%20Generating%20Full-field%20Evolution%20of%20Physical%20Dynamics%20from%20Irregular%0A%20%20Sparse%20Observations%0AAuthor%3A%20Panqi%20Chen%20and%20Yifan%20Sun%20and%20Lei%20Cheng%20and%20Yang%20Yang%20and%20Weichang%20Li%20and%20Yang%20Liu%20and%20Weiqing%20Liu%20and%20Jiang%20Bian%20and%20Shikai%20Fang%0AAbstract%3A%20%20%20Modeling%20and%20reconstructing%20multidimensional%20physical%20dynamics%20from%20sparse%0Aand%20off-grid%20observations%20presents%20a%20fundamental%20challenge%20in%20scientific%0Aresearch.%20Recently%2C%20diffusion-based%20generative%20modeling%20shows%20promising%0Apotential%20for%20physical%20simulation.%20However%2C%20current%20approaches%20typically%0Aoperate%20on%20on-grid%20data%20with%20preset%20spatiotemporal%20resolution%2C%20but%20struggle%0Awith%20the%20sparsely%20observed%20and%20continuous%20nature%20of%20real-world%20physical%0Adynamics.%20To%20fill%20the%20gaps%2C%20we%20present%20SDIFT%2C%20Sequential%20DIffusion%20in%0AFunctional%20Tucker%20space%2C%20a%20novel%20framework%20that%20generates%20full-field%20evolution%0Aof%20physical%20dynamics%20from%20irregular%20sparse%20observations.%20SDIFT%20leverages%20the%0Afunctional%20Tucker%20model%20as%20the%20latent%20space%20representer%20with%20proven%20universal%0Aapproximation%20property%2C%20and%20represents%20observations%20as%20latent%20functions%20and%0ATucker%20core%20sequences.%20We%20then%20construct%20a%20sequential%20diffusion%20model%20with%0Atemporally%20augmented%20UNet%20in%20the%20functional%20Tucker%20space%2C%20denoising%20noise%20drawn%0Afrom%20a%20Gaussian%20process%20to%20generate%20the%20sequence%20of%20core%20tensors.%0A%20%20At%20the%20posterior%20sampling%20stage%2C%20we%20propose%20a%20Message-Passing%20Posterior%0ASampling%20mechanism%2C%20enabling%20conditional%20generation%20of%20the%20entire%20sequence%0Aguided%20by%20observations%20at%20limited%20time%20steps.%20We%20validate%20SDIFT%20on%20three%0Aphysical%20systems%20spanning%20astronomical%20%28supernova%20explosions%2C%20light-year%0Ascale%29%2C%20environmental%20%28ocean%20sound%20speed%20fields%2C%20kilometer%20scale%29%2C%20and%0Amolecular%20%28organic%20liquid%2C%20millimeter%20scale%29%20domains%2C%20demonstrating%20significant%0Aimprovements%20in%20both%20reconstruction%20accuracy%20and%20computational%20efficiency%0Acompared%20to%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Full-field%2520Evolution%2520of%2520Physical%2520Dynamics%2520from%2520Irregular%250A%2520%2520Sparse%2520Observations%26entry.906535625%3DPanqi%2520Chen%2520and%2520Yifan%2520Sun%2520and%2520Lei%2520Cheng%2520and%2520Yang%2520Yang%2520and%2520Weichang%2520Li%2520and%2520Yang%2520Liu%2520and%2520Weiqing%2520Liu%2520and%2520Jiang%2520Bian%2520and%2520Shikai%2520Fang%26entry.1292438233%3D%2520%2520Modeling%2520and%2520reconstructing%2520multidimensional%2520physical%2520dynamics%2520from%2520sparse%250Aand%2520off-grid%2520observations%2520presents%2520a%2520fundamental%2520challenge%2520in%2520scientific%250Aresearch.%2520Recently%252C%2520diffusion-based%2520generative%2520modeling%2520shows%2520promising%250Apotential%2520for%2520physical%2520simulation.%2520However%252C%2520current%2520approaches%2520typically%250Aoperate%2520on%2520on-grid%2520data%2520with%2520preset%2520spatiotemporal%2520resolution%252C%2520but%2520struggle%250Awith%2520the%2520sparsely%2520observed%2520and%2520continuous%2520nature%2520of%2520real-world%2520physical%250Adynamics.%2520To%2520fill%2520the%2520gaps%252C%2520we%2520present%2520SDIFT%252C%2520Sequential%2520DIffusion%2520in%250AFunctional%2520Tucker%2520space%252C%2520a%2520novel%2520framework%2520that%2520generates%2520full-field%2520evolution%250Aof%2520physical%2520dynamics%2520from%2520irregular%2520sparse%2520observations.%2520SDIFT%2520leverages%2520the%250Afunctional%2520Tucker%2520model%2520as%2520the%2520latent%2520space%2520representer%2520with%2520proven%2520universal%250Aapproximation%2520property%252C%2520and%2520represents%2520observations%2520as%2520latent%2520functions%2520and%250ATucker%2520core%2520sequences.%2520We%2520then%2520construct%2520a%2520sequential%2520diffusion%2520model%2520with%250Atemporally%2520augmented%2520UNet%2520in%2520the%2520functional%2520Tucker%2520space%252C%2520denoising%2520noise%2520drawn%250Afrom%2520a%2520Gaussian%2520process%2520to%2520generate%2520the%2520sequence%2520of%2520core%2520tensors.%250A%2520%2520At%2520the%2520posterior%2520sampling%2520stage%252C%2520we%2520propose%2520a%2520Message-Passing%2520Posterior%250ASampling%2520mechanism%252C%2520enabling%2520conditional%2520generation%2520of%2520the%2520entire%2520sequence%250Aguided%2520by%2520observations%2520at%2520limited%2520time%2520steps.%2520We%2520validate%2520SDIFT%2520on%2520three%250Aphysical%2520systems%2520spanning%2520astronomical%2520%2528supernova%2520explosions%252C%2520light-year%250Ascale%2529%252C%2520environmental%2520%2528ocean%2520sound%2520speed%2520fields%252C%2520kilometer%2520scale%2529%252C%2520and%250Amolecular%2520%2528organic%2520liquid%252C%2520millimeter%2520scale%2529%2520domains%252C%2520demonstrating%2520significant%250Aimprovements%2520in%2520both%2520reconstruction%2520accuracy%2520and%2520computational%2520efficiency%250Acompared%2520to%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Full-field%20Evolution%20of%20Physical%20Dynamics%20from%20Irregular%0A%20%20Sparse%20Observations&entry.906535625=Panqi%20Chen%20and%20Yifan%20Sun%20and%20Lei%20Cheng%20and%20Yang%20Yang%20and%20Weichang%20Li%20and%20Yang%20Liu%20and%20Weiqing%20Liu%20and%20Jiang%20Bian%20and%20Shikai%20Fang&entry.1292438233=%20%20Modeling%20and%20reconstructing%20multidimensional%20physical%20dynamics%20from%20sparse%0Aand%20off-grid%20observations%20presents%20a%20fundamental%20challenge%20in%20scientific%0Aresearch.%20Recently%2C%20diffusion-based%20generative%20modeling%20shows%20promising%0Apotential%20for%20physical%20simulation.%20However%2C%20current%20approaches%20typically%0Aoperate%20on%20on-grid%20data%20with%20preset%20spatiotemporal%20resolution%2C%20but%20struggle%0Awith%20the%20sparsely%20observed%20and%20continuous%20nature%20of%20real-world%20physical%0Adynamics.%20To%20fill%20the%20gaps%2C%20we%20present%20SDIFT%2C%20Sequential%20DIffusion%20in%0AFunctional%20Tucker%20space%2C%20a%20novel%20framework%20that%20generates%20full-field%20evolution%0Aof%20physical%20dynamics%20from%20irregular%20sparse%20observations.%20SDIFT%20leverages%20the%0Afunctional%20Tucker%20model%20as%20the%20latent%20space%20representer%20with%20proven%20universal%0Aapproximation%20property%2C%20and%20represents%20observations%20as%20latent%20functions%20and%0ATucker%20core%20sequences.%20We%20then%20construct%20a%20sequential%20diffusion%20model%20with%0Atemporally%20augmented%20UNet%20in%20the%20functional%20Tucker%20space%2C%20denoising%20noise%20drawn%0Afrom%20a%20Gaussian%20process%20to%20generate%20the%20sequence%20of%20core%20tensors.%0A%20%20At%20the%20posterior%20sampling%20stage%2C%20we%20propose%20a%20Message-Passing%20Posterior%0ASampling%20mechanism%2C%20enabling%20conditional%20generation%20of%20the%20entire%20sequence%0Aguided%20by%20observations%20at%20limited%20time%20steps.%20We%20validate%20SDIFT%20on%20three%0Aphysical%20systems%20spanning%20astronomical%20%28supernova%20explosions%2C%20light-year%0Ascale%29%2C%20environmental%20%28ocean%20sound%20speed%20fields%2C%20kilometer%20scale%29%2C%20and%0Amolecular%20%28organic%20liquid%2C%20millimeter%20scale%29%20domains%2C%20demonstrating%20significant%0Aimprovements%20in%20both%20reconstruction%20accuracy%20and%20computational%20efficiency%0Acompared%20to%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09284v1&entry.124074799=Read"},
{"title": "Counterfactual Strategies for Markov Decision Processes", "author": "Paul Kobialka and Lina Gerlach and Francesco Leofante and Erika \u00c1brah\u00e1m and Silvia Lizeth Tapia Tarifa and Einar Broch Johnsen", "abstract": "  Counterfactuals are widely used in AI to explain how minimal changes to a\nmodel's input can lead to a different output. However, established methods for\ncomputing counterfactuals typically focus on one-step decision-making, and are\nnot directly applicable to sequential decision-making tasks. This paper fills\nthis gap by introducing counterfactual strategies for Markov Decision Processes\n(MDPs). During MDP execution, a strategy decides which of the enabled actions\n(with known probabilistic effects) to execute next. Given an initial strategy\nthat reaches an undesired outcome with a probability above some limit, we\nidentify minimal changes to the initial strategy to reduce that probability\nbelow the limit. We encode such counterfactual strategies as solutions to\nnon-linear optimization problems, and further extend our encoding to synthesize\ndiverse counterfactual strategies. We evaluate our approach on four real-world\ndatasets and demonstrate its practical viability in sophisticated sequential\ndecision-making tasks.\n", "link": "http://arxiv.org/abs/2505.09412v1", "date": "2025-05-14", "relevancy": 0.9072, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4717}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4447}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counterfactual%20Strategies%20for%20Markov%20Decision%20Processes&body=Title%3A%20Counterfactual%20Strategies%20for%20Markov%20Decision%20Processes%0AAuthor%3A%20Paul%20Kobialka%20and%20Lina%20Gerlach%20and%20Francesco%20Leofante%20and%20Erika%20%C3%81brah%C3%A1m%20and%20Silvia%20Lizeth%20Tapia%20Tarifa%20and%20Einar%20Broch%20Johnsen%0AAbstract%3A%20%20%20Counterfactuals%20are%20widely%20used%20in%20AI%20to%20explain%20how%20minimal%20changes%20to%20a%0Amodel%27s%20input%20can%20lead%20to%20a%20different%20output.%20However%2C%20established%20methods%20for%0Acomputing%20counterfactuals%20typically%20focus%20on%20one-step%20decision-making%2C%20and%20are%0Anot%20directly%20applicable%20to%20sequential%20decision-making%20tasks.%20This%20paper%20fills%0Athis%20gap%20by%20introducing%20counterfactual%20strategies%20for%20Markov%20Decision%20Processes%0A%28MDPs%29.%20During%20MDP%20execution%2C%20a%20strategy%20decides%20which%20of%20the%20enabled%20actions%0A%28with%20known%20probabilistic%20effects%29%20to%20execute%20next.%20Given%20an%20initial%20strategy%0Athat%20reaches%20an%20undesired%20outcome%20with%20a%20probability%20above%20some%20limit%2C%20we%0Aidentify%20minimal%20changes%20to%20the%20initial%20strategy%20to%20reduce%20that%20probability%0Abelow%20the%20limit.%20We%20encode%20such%20counterfactual%20strategies%20as%20solutions%20to%0Anon-linear%20optimization%20problems%2C%20and%20further%20extend%20our%20encoding%20to%20synthesize%0Adiverse%20counterfactual%20strategies.%20We%20evaluate%20our%20approach%20on%20four%20real-world%0Adatasets%20and%20demonstrate%20its%20practical%20viability%20in%20sophisticated%20sequential%0Adecision-making%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterfactual%2520Strategies%2520for%2520Markov%2520Decision%2520Processes%26entry.906535625%3DPaul%2520Kobialka%2520and%2520Lina%2520Gerlach%2520and%2520Francesco%2520Leofante%2520and%2520Erika%2520%25C3%2581brah%25C3%25A1m%2520and%2520Silvia%2520Lizeth%2520Tapia%2520Tarifa%2520and%2520Einar%2520Broch%2520Johnsen%26entry.1292438233%3D%2520%2520Counterfactuals%2520are%2520widely%2520used%2520in%2520AI%2520to%2520explain%2520how%2520minimal%2520changes%2520to%2520a%250Amodel%2527s%2520input%2520can%2520lead%2520to%2520a%2520different%2520output.%2520However%252C%2520established%2520methods%2520for%250Acomputing%2520counterfactuals%2520typically%2520focus%2520on%2520one-step%2520decision-making%252C%2520and%2520are%250Anot%2520directly%2520applicable%2520to%2520sequential%2520decision-making%2520tasks.%2520This%2520paper%2520fills%250Athis%2520gap%2520by%2520introducing%2520counterfactual%2520strategies%2520for%2520Markov%2520Decision%2520Processes%250A%2528MDPs%2529.%2520During%2520MDP%2520execution%252C%2520a%2520strategy%2520decides%2520which%2520of%2520the%2520enabled%2520actions%250A%2528with%2520known%2520probabilistic%2520effects%2529%2520to%2520execute%2520next.%2520Given%2520an%2520initial%2520strategy%250Athat%2520reaches%2520an%2520undesired%2520outcome%2520with%2520a%2520probability%2520above%2520some%2520limit%252C%2520we%250Aidentify%2520minimal%2520changes%2520to%2520the%2520initial%2520strategy%2520to%2520reduce%2520that%2520probability%250Abelow%2520the%2520limit.%2520We%2520encode%2520such%2520counterfactual%2520strategies%2520as%2520solutions%2520to%250Anon-linear%2520optimization%2520problems%252C%2520and%2520further%2520extend%2520our%2520encoding%2520to%2520synthesize%250Adiverse%2520counterfactual%2520strategies.%2520We%2520evaluate%2520our%2520approach%2520on%2520four%2520real-world%250Adatasets%2520and%2520demonstrate%2520its%2520practical%2520viability%2520in%2520sophisticated%2520sequential%250Adecision-making%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20Strategies%20for%20Markov%20Decision%20Processes&entry.906535625=Paul%20Kobialka%20and%20Lina%20Gerlach%20and%20Francesco%20Leofante%20and%20Erika%20%C3%81brah%C3%A1m%20and%20Silvia%20Lizeth%20Tapia%20Tarifa%20and%20Einar%20Broch%20Johnsen&entry.1292438233=%20%20Counterfactuals%20are%20widely%20used%20in%20AI%20to%20explain%20how%20minimal%20changes%20to%20a%0Amodel%27s%20input%20can%20lead%20to%20a%20different%20output.%20However%2C%20established%20methods%20for%0Acomputing%20counterfactuals%20typically%20focus%20on%20one-step%20decision-making%2C%20and%20are%0Anot%20directly%20applicable%20to%20sequential%20decision-making%20tasks.%20This%20paper%20fills%0Athis%20gap%20by%20introducing%20counterfactual%20strategies%20for%20Markov%20Decision%20Processes%0A%28MDPs%29.%20During%20MDP%20execution%2C%20a%20strategy%20decides%20which%20of%20the%20enabled%20actions%0A%28with%20known%20probabilistic%20effects%29%20to%20execute%20next.%20Given%20an%20initial%20strategy%0Athat%20reaches%20an%20undesired%20outcome%20with%20a%20probability%20above%20some%20limit%2C%20we%0Aidentify%20minimal%20changes%20to%20the%20initial%20strategy%20to%20reduce%20that%20probability%0Abelow%20the%20limit.%20We%20encode%20such%20counterfactual%20strategies%20as%20solutions%20to%0Anon-linear%20optimization%20problems%2C%20and%20further%20extend%20our%20encoding%20to%20synthesize%0Adiverse%20counterfactual%20strategies.%20We%20evaluate%20our%20approach%20on%20four%20real-world%0Adatasets%20and%20demonstrate%20its%20practical%20viability%20in%20sophisticated%20sequential%0Adecision-making%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09412v1&entry.124074799=Read"},
{"title": "What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms\n  Behind Attacks", "author": "Nathalie Kirch and Constantin Weisser and Severin Field and Helen Yannakoudakis and Stephen Casper", "abstract": "  Jailbreaks have been a central focus of research regarding the safety and\nreliability of large language models (LLMs), yet the mechanisms underlying\nthese attacks remain poorly understood. While previous studies have\npredominantly relied on linear methods to detect jailbreak attempts and model\nrefusals, we take a different approach by examining both linear and non-linear\nfeatures in prompts that lead to successful jailbreaks. First, we introduce a\nnovel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack\nmethods. Leveraging this dataset, we train probes to classify successful from\nunsuccessful jailbreaks using the latent representations corresponding to\nprompt tokens. Notably, we find that even when probes achieve high accuracy in\npredicting the success of jailbreaks, their performance often fails to\ngeneralize to unseen attack methods. This reveals that different jailbreaking\nstrategies exploit different non-linear, non-universal features. Next, we\ndemonstrate that non-linear probes provide a powerful tool for steering model\nbehavior. Specifically, we use these probes to guide targeted latent space\nperturbations, enabling us to effectively modulate the model's robustness\nagainst jailbreaks. Overall, our findings challenge the assumption that\njailbreaks can be fully understood through linear or simple universal prompt\nfeatures alone, highlighting the importance of a nuanced understanding of the\nmechanisms behind LLM vulnerabilities.\n", "link": "http://arxiv.org/abs/2411.03343v2", "date": "2025-05-14", "relevancy": 1.7839, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4548}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Features%20in%20Prompts%20Jailbreak%20LLMs%3F%20Investigating%20the%20Mechanisms%0A%20%20Behind%20Attacks&body=Title%3A%20What%20Features%20in%20Prompts%20Jailbreak%20LLMs%3F%20Investigating%20the%20Mechanisms%0A%20%20Behind%20Attacks%0AAuthor%3A%20Nathalie%20Kirch%20and%20Constantin%20Weisser%20and%20Severin%20Field%20and%20Helen%20Yannakoudakis%20and%20Stephen%20Casper%0AAbstract%3A%20%20%20Jailbreaks%20have%20been%20a%20central%20focus%20of%20research%20regarding%20the%20safety%20and%0Areliability%20of%20large%20language%20models%20%28LLMs%29%2C%20yet%20the%20mechanisms%20underlying%0Athese%20attacks%20remain%20poorly%20understood.%20While%20previous%20studies%20have%0Apredominantly%20relied%20on%20linear%20methods%20to%20detect%20jailbreak%20attempts%20and%20model%0Arefusals%2C%20we%20take%20a%20different%20approach%20by%20examining%20both%20linear%20and%20non-linear%0Afeatures%20in%20prompts%20that%20lead%20to%20successful%20jailbreaks.%20First%2C%20we%20introduce%20a%0Anovel%20dataset%20comprising%2010%2C800%20jailbreak%20attempts%20spanning%2035%20diverse%20attack%0Amethods.%20Leveraging%20this%20dataset%2C%20we%20train%20probes%20to%20classify%20successful%20from%0Aunsuccessful%20jailbreaks%20using%20the%20latent%20representations%20corresponding%20to%0Aprompt%20tokens.%20Notably%2C%20we%20find%20that%20even%20when%20probes%20achieve%20high%20accuracy%20in%0Apredicting%20the%20success%20of%20jailbreaks%2C%20their%20performance%20often%20fails%20to%0Ageneralize%20to%20unseen%20attack%20methods.%20This%20reveals%20that%20different%20jailbreaking%0Astrategies%20exploit%20different%20non-linear%2C%20non-universal%20features.%20Next%2C%20we%0Ademonstrate%20that%20non-linear%20probes%20provide%20a%20powerful%20tool%20for%20steering%20model%0Abehavior.%20Specifically%2C%20we%20use%20these%20probes%20to%20guide%20targeted%20latent%20space%0Aperturbations%2C%20enabling%20us%20to%20effectively%20modulate%20the%20model%27s%20robustness%0Aagainst%20jailbreaks.%20Overall%2C%20our%20findings%20challenge%20the%20assumption%20that%0Ajailbreaks%20can%20be%20fully%20understood%20through%20linear%20or%20simple%20universal%20prompt%0Afeatures%20alone%2C%20highlighting%20the%20importance%20of%20a%20nuanced%20understanding%20of%20the%0Amechanisms%20behind%20LLM%20vulnerabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03343v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Features%2520in%2520Prompts%2520Jailbreak%2520LLMs%253F%2520Investigating%2520the%2520Mechanisms%250A%2520%2520Behind%2520Attacks%26entry.906535625%3DNathalie%2520Kirch%2520and%2520Constantin%2520Weisser%2520and%2520Severin%2520Field%2520and%2520Helen%2520Yannakoudakis%2520and%2520Stephen%2520Casper%26entry.1292438233%3D%2520%2520Jailbreaks%2520have%2520been%2520a%2520central%2520focus%2520of%2520research%2520regarding%2520the%2520safety%2520and%250Areliability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520yet%2520the%2520mechanisms%2520underlying%250Athese%2520attacks%2520remain%2520poorly%2520understood.%2520While%2520previous%2520studies%2520have%250Apredominantly%2520relied%2520on%2520linear%2520methods%2520to%2520detect%2520jailbreak%2520attempts%2520and%2520model%250Arefusals%252C%2520we%2520take%2520a%2520different%2520approach%2520by%2520examining%2520both%2520linear%2520and%2520non-linear%250Afeatures%2520in%2520prompts%2520that%2520lead%2520to%2520successful%2520jailbreaks.%2520First%252C%2520we%2520introduce%2520a%250Anovel%2520dataset%2520comprising%252010%252C800%2520jailbreak%2520attempts%2520spanning%252035%2520diverse%2520attack%250Amethods.%2520Leveraging%2520this%2520dataset%252C%2520we%2520train%2520probes%2520to%2520classify%2520successful%2520from%250Aunsuccessful%2520jailbreaks%2520using%2520the%2520latent%2520representations%2520corresponding%2520to%250Aprompt%2520tokens.%2520Notably%252C%2520we%2520find%2520that%2520even%2520when%2520probes%2520achieve%2520high%2520accuracy%2520in%250Apredicting%2520the%2520success%2520of%2520jailbreaks%252C%2520their%2520performance%2520often%2520fails%2520to%250Ageneralize%2520to%2520unseen%2520attack%2520methods.%2520This%2520reveals%2520that%2520different%2520jailbreaking%250Astrategies%2520exploit%2520different%2520non-linear%252C%2520non-universal%2520features.%2520Next%252C%2520we%250Ademonstrate%2520that%2520non-linear%2520probes%2520provide%2520a%2520powerful%2520tool%2520for%2520steering%2520model%250Abehavior.%2520Specifically%252C%2520we%2520use%2520these%2520probes%2520to%2520guide%2520targeted%2520latent%2520space%250Aperturbations%252C%2520enabling%2520us%2520to%2520effectively%2520modulate%2520the%2520model%2527s%2520robustness%250Aagainst%2520jailbreaks.%2520Overall%252C%2520our%2520findings%2520challenge%2520the%2520assumption%2520that%250Ajailbreaks%2520can%2520be%2520fully%2520understood%2520through%2520linear%2520or%2520simple%2520universal%2520prompt%250Afeatures%2520alone%252C%2520highlighting%2520the%2520importance%2520of%2520a%2520nuanced%2520understanding%2520of%2520the%250Amechanisms%2520behind%2520LLM%2520vulnerabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03343v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Features%20in%20Prompts%20Jailbreak%20LLMs%3F%20Investigating%20the%20Mechanisms%0A%20%20Behind%20Attacks&entry.906535625=Nathalie%20Kirch%20and%20Constantin%20Weisser%20and%20Severin%20Field%20and%20Helen%20Yannakoudakis%20and%20Stephen%20Casper&entry.1292438233=%20%20Jailbreaks%20have%20been%20a%20central%20focus%20of%20research%20regarding%20the%20safety%20and%0Areliability%20of%20large%20language%20models%20%28LLMs%29%2C%20yet%20the%20mechanisms%20underlying%0Athese%20attacks%20remain%20poorly%20understood.%20While%20previous%20studies%20have%0Apredominantly%20relied%20on%20linear%20methods%20to%20detect%20jailbreak%20attempts%20and%20model%0Arefusals%2C%20we%20take%20a%20different%20approach%20by%20examining%20both%20linear%20and%20non-linear%0Afeatures%20in%20prompts%20that%20lead%20to%20successful%20jailbreaks.%20First%2C%20we%20introduce%20a%0Anovel%20dataset%20comprising%2010%2C800%20jailbreak%20attempts%20spanning%2035%20diverse%20attack%0Amethods.%20Leveraging%20this%20dataset%2C%20we%20train%20probes%20to%20classify%20successful%20from%0Aunsuccessful%20jailbreaks%20using%20the%20latent%20representations%20corresponding%20to%0Aprompt%20tokens.%20Notably%2C%20we%20find%20that%20even%20when%20probes%20achieve%20high%20accuracy%20in%0Apredicting%20the%20success%20of%20jailbreaks%2C%20their%20performance%20often%20fails%20to%0Ageneralize%20to%20unseen%20attack%20methods.%20This%20reveals%20that%20different%20jailbreaking%0Astrategies%20exploit%20different%20non-linear%2C%20non-universal%20features.%20Next%2C%20we%0Ademonstrate%20that%20non-linear%20probes%20provide%20a%20powerful%20tool%20for%20steering%20model%0Abehavior.%20Specifically%2C%20we%20use%20these%20probes%20to%20guide%20targeted%20latent%20space%0Aperturbations%2C%20enabling%20us%20to%20effectively%20modulate%20the%20model%27s%20robustness%0Aagainst%20jailbreaks.%20Overall%2C%20our%20findings%20challenge%20the%20assumption%20that%0Ajailbreaks%20can%20be%20fully%20understood%20through%20linear%20or%20simple%20universal%20prompt%0Afeatures%20alone%2C%20highlighting%20the%20importance%20of%20a%20nuanced%20understanding%20of%20the%0Amechanisms%20behind%20LLM%20vulnerabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03343v2&entry.124074799=Read"},
{"title": "Deploying Foundation Model-Enabled Air and Ground Robots in the Field:\n  Challenges and Opportunities", "author": "Zachary Ravichandran and Fernando Cladera and Jason Hughes and Varun Murali and M. Ani Hsieh and George J. Pappas and Camillo J. Taylor and Vijay Kumar", "abstract": "  The integration of foundation models (FMs) into robotics has enabled robots\nto understand natural language and reason about the semantics in their\nenvironments. However, existing FM-enabled robots primary operate in\nclosed-world settings, where the robot is given a full prior map or has a full\nview of its workspace. This paper addresses the deployment of FM-enabled robots\nin the field, where missions often require a robot to operate in large-scale\nand unstructured environments. To effectively accomplish these missions, robots\nmust actively explore their environments, navigate obstacle-cluttered terrain,\nhandle unexpected sensor inputs, and operate with compute constraints. We\ndiscuss recent deployments of SPINE, our LLM-enabled autonomy framework, in\nfield robotic settings. To the best of our knowledge, we present the first\ndemonstration of large-scale LLM-enabled robot planning in unstructured\nenvironments with several kilometers of missions. SPINE is agnostic to a\nparticular LLM, which allows us to distill small language models capable of\nrunning onboard size, weight and power (SWaP) limited platforms. Via\npreliminary model distillation work, we then present the first language-driven\nUAV planner using on-device language models. We conclude our paper by proposing\nseveral promising directions for future research.\n", "link": "http://arxiv.org/abs/2505.09477v1", "date": "2025-05-14", "relevancy": 1.6855, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5729}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deploying%20Foundation%20Model-Enabled%20Air%20and%20Ground%20Robots%20in%20the%20Field%3A%0A%20%20Challenges%20and%20Opportunities&body=Title%3A%20Deploying%20Foundation%20Model-Enabled%20Air%20and%20Ground%20Robots%20in%20the%20Field%3A%0A%20%20Challenges%20and%20Opportunities%0AAuthor%3A%20Zachary%20Ravichandran%20and%20Fernando%20Cladera%20and%20Jason%20Hughes%20and%20Varun%20Murali%20and%20M.%20Ani%20Hsieh%20and%20George%20J.%20Pappas%20and%20Camillo%20J.%20Taylor%20and%20Vijay%20Kumar%0AAbstract%3A%20%20%20The%20integration%20of%20foundation%20models%20%28FMs%29%20into%20robotics%20has%20enabled%20robots%0Ato%20understand%20natural%20language%20and%20reason%20about%20the%20semantics%20in%20their%0Aenvironments.%20However%2C%20existing%20FM-enabled%20robots%20primary%20operate%20in%0Aclosed-world%20settings%2C%20where%20the%20robot%20is%20given%20a%20full%20prior%20map%20or%20has%20a%20full%0Aview%20of%20its%20workspace.%20This%20paper%20addresses%20the%20deployment%20of%20FM-enabled%20robots%0Ain%20the%20field%2C%20where%20missions%20often%20require%20a%20robot%20to%20operate%20in%20large-scale%0Aand%20unstructured%20environments.%20To%20effectively%20accomplish%20these%20missions%2C%20robots%0Amust%20actively%20explore%20their%20environments%2C%20navigate%20obstacle-cluttered%20terrain%2C%0Ahandle%20unexpected%20sensor%20inputs%2C%20and%20operate%20with%20compute%20constraints.%20We%0Adiscuss%20recent%20deployments%20of%20SPINE%2C%20our%20LLM-enabled%20autonomy%20framework%2C%20in%0Afield%20robotic%20settings.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20present%20the%20first%0Ademonstration%20of%20large-scale%20LLM-enabled%20robot%20planning%20in%20unstructured%0Aenvironments%20with%20several%20kilometers%20of%20missions.%20SPINE%20is%20agnostic%20to%20a%0Aparticular%20LLM%2C%20which%20allows%20us%20to%20distill%20small%20language%20models%20capable%20of%0Arunning%20onboard%20size%2C%20weight%20and%20power%20%28SWaP%29%20limited%20platforms.%20Via%0Apreliminary%20model%20distillation%20work%2C%20we%20then%20present%20the%20first%20language-driven%0AUAV%20planner%20using%20on-device%20language%20models.%20We%20conclude%20our%20paper%20by%20proposing%0Aseveral%20promising%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.09477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeploying%2520Foundation%2520Model-Enabled%2520Air%2520and%2520Ground%2520Robots%2520in%2520the%2520Field%253A%250A%2520%2520Challenges%2520and%2520Opportunities%26entry.906535625%3DZachary%2520Ravichandran%2520and%2520Fernando%2520Cladera%2520and%2520Jason%2520Hughes%2520and%2520Varun%2520Murali%2520and%2520M.%2520Ani%2520Hsieh%2520and%2520George%2520J.%2520Pappas%2520and%2520Camillo%2520J.%2520Taylor%2520and%2520Vijay%2520Kumar%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520foundation%2520models%2520%2528FMs%2529%2520into%2520robotics%2520has%2520enabled%2520robots%250Ato%2520understand%2520natural%2520language%2520and%2520reason%2520about%2520the%2520semantics%2520in%2520their%250Aenvironments.%2520However%252C%2520existing%2520FM-enabled%2520robots%2520primary%2520operate%2520in%250Aclosed-world%2520settings%252C%2520where%2520the%2520robot%2520is%2520given%2520a%2520full%2520prior%2520map%2520or%2520has%2520a%2520full%250Aview%2520of%2520its%2520workspace.%2520This%2520paper%2520addresses%2520the%2520deployment%2520of%2520FM-enabled%2520robots%250Ain%2520the%2520field%252C%2520where%2520missions%2520often%2520require%2520a%2520robot%2520to%2520operate%2520in%2520large-scale%250Aand%2520unstructured%2520environments.%2520To%2520effectively%2520accomplish%2520these%2520missions%252C%2520robots%250Amust%2520actively%2520explore%2520their%2520environments%252C%2520navigate%2520obstacle-cluttered%2520terrain%252C%250Ahandle%2520unexpected%2520sensor%2520inputs%252C%2520and%2520operate%2520with%2520compute%2520constraints.%2520We%250Adiscuss%2520recent%2520deployments%2520of%2520SPINE%252C%2520our%2520LLM-enabled%2520autonomy%2520framework%252C%2520in%250Afield%2520robotic%2520settings.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520we%2520present%2520the%2520first%250Ademonstration%2520of%2520large-scale%2520LLM-enabled%2520robot%2520planning%2520in%2520unstructured%250Aenvironments%2520with%2520several%2520kilometers%2520of%2520missions.%2520SPINE%2520is%2520agnostic%2520to%2520a%250Aparticular%2520LLM%252C%2520which%2520allows%2520us%2520to%2520distill%2520small%2520language%2520models%2520capable%2520of%250Arunning%2520onboard%2520size%252C%2520weight%2520and%2520power%2520%2528SWaP%2529%2520limited%2520platforms.%2520Via%250Apreliminary%2520model%2520distillation%2520work%252C%2520we%2520then%2520present%2520the%2520first%2520language-driven%250AUAV%2520planner%2520using%2520on-device%2520language%2520models.%2520We%2520conclude%2520our%2520paper%2520by%2520proposing%250Aseveral%2520promising%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deploying%20Foundation%20Model-Enabled%20Air%20and%20Ground%20Robots%20in%20the%20Field%3A%0A%20%20Challenges%20and%20Opportunities&entry.906535625=Zachary%20Ravichandran%20and%20Fernando%20Cladera%20and%20Jason%20Hughes%20and%20Varun%20Murali%20and%20M.%20Ani%20Hsieh%20and%20George%20J.%20Pappas%20and%20Camillo%20J.%20Taylor%20and%20Vijay%20Kumar&entry.1292438233=%20%20The%20integration%20of%20foundation%20models%20%28FMs%29%20into%20robotics%20has%20enabled%20robots%0Ato%20understand%20natural%20language%20and%20reason%20about%20the%20semantics%20in%20their%0Aenvironments.%20However%2C%20existing%20FM-enabled%20robots%20primary%20operate%20in%0Aclosed-world%20settings%2C%20where%20the%20robot%20is%20given%20a%20full%20prior%20map%20or%20has%20a%20full%0Aview%20of%20its%20workspace.%20This%20paper%20addresses%20the%20deployment%20of%20FM-enabled%20robots%0Ain%20the%20field%2C%20where%20missions%20often%20require%20a%20robot%20to%20operate%20in%20large-scale%0Aand%20unstructured%20environments.%20To%20effectively%20accomplish%20these%20missions%2C%20robots%0Amust%20actively%20explore%20their%20environments%2C%20navigate%20obstacle-cluttered%20terrain%2C%0Ahandle%20unexpected%20sensor%20inputs%2C%20and%20operate%20with%20compute%20constraints.%20We%0Adiscuss%20recent%20deployments%20of%20SPINE%2C%20our%20LLM-enabled%20autonomy%20framework%2C%20in%0Afield%20robotic%20settings.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20present%20the%20first%0Ademonstration%20of%20large-scale%20LLM-enabled%20robot%20planning%20in%20unstructured%0Aenvironments%20with%20several%20kilometers%20of%20missions.%20SPINE%20is%20agnostic%20to%20a%0Aparticular%20LLM%2C%20which%20allows%20us%20to%20distill%20small%20language%20models%20capable%20of%0Arunning%20onboard%20size%2C%20weight%20and%20power%20%28SWaP%29%20limited%20platforms.%20Via%0Apreliminary%20model%20distillation%20work%2C%20we%20then%20present%20the%20first%20language-driven%0AUAV%20planner%20using%20on-device%20language%20models.%20We%20conclude%20our%20paper%20by%20proposing%0Aseveral%20promising%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.09477v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


