<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240820.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Self-supervised Photographic Image Layout Representation Learning", "author": "Zhaoran Zhao and Peng Lu and Xujun Peng and Wenhao Guo", "abstract": "  In the domain of image layout representation learning, the critical process\nof translating image layouts into succinct vector forms is increasingly\nsignificant across diverse applications, such as image retrieval, manipulation,\nand generation. Most approaches in this area heavily rely on costly labeled\ndatasets and notably lack in adapting their modeling and learning methods to\nthe specific nuances of photographic image layouts. This shortfall makes the\nlearning process for photographic image layouts suboptimal. In our research, we\ndirectly address these challenges. We innovate by defining basic layout\nprimitives that encapsulate various levels of layout information and by mapping\nthese, along with their interconnections, onto a heterogeneous graph structure.\nThis graph is meticulously engineered to capture the intricate layout\ninformation within the pixel domain explicitly. Advancing further, we introduce\nnovel pretext tasks coupled with customized loss functions, strategically\ndesigned for effective self-supervised learning of these layout graphs.\nBuilding on this foundation, we develop an autoencoder-based network\narchitecture skilled in compressing these heterogeneous layout graphs into\nprecise, dimensionally-reduced layout representations. Additionally, we\nintroduce the LODB dataset, which features a broader range of layout categories\nand richer semantics, serving as a comprehensive benchmark for evaluating the\neffectiveness of layout representation learning methods. Our extensive\nexperimentation on this dataset demonstrates the superior performance of our\napproach in the realm of photographic image layout representation learning.\n", "link": "http://arxiv.org/abs/2403.03740v2", "date": "2024-08-20", "relevancy": 3.5131, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 1.0}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.558}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Photographic%20Image%20Layout%20Representation%20Learning&body=Title%3A%20Self-supervised%20Photographic%20Image%20Layout%20Representation%20Learning%0AAuthor%3A%20Zhaoran%20Zhao%20and%20Peng%20Lu%20and%20Xujun%20Peng%20and%20Wenhao%20Guo%0AAbstract%3A%20%20%20In%20the%20domain%20of%20image%20layout%20representation%20learning%2C%20the%20critical%20process%0Aof%20translating%20image%20layouts%20into%20succinct%20vector%20forms%20is%20increasingly%0Asignificant%20across%20diverse%20applications%2C%20such%20as%20image%20retrieval%2C%20manipulation%2C%0Aand%20generation.%20Most%20approaches%20in%20this%20area%20heavily%20rely%20on%20costly%20labeled%0Adatasets%20and%20notably%20lack%20in%20adapting%20their%20modeling%20and%20learning%20methods%20to%0Athe%20specific%20nuances%20of%20photographic%20image%20layouts.%20This%20shortfall%20makes%20the%0Alearning%20process%20for%20photographic%20image%20layouts%20suboptimal.%20In%20our%20research%2C%20we%0Adirectly%20address%20these%20challenges.%20We%20innovate%20by%20defining%20basic%20layout%0Aprimitives%20that%20encapsulate%20various%20levels%20of%20layout%20information%20and%20by%20mapping%0Athese%2C%20along%20with%20their%20interconnections%2C%20onto%20a%20heterogeneous%20graph%20structure.%0AThis%20graph%20is%20meticulously%20engineered%20to%20capture%20the%20intricate%20layout%0Ainformation%20within%20the%20pixel%20domain%20explicitly.%20Advancing%20further%2C%20we%20introduce%0Anovel%20pretext%20tasks%20coupled%20with%20customized%20loss%20functions%2C%20strategically%0Adesigned%20for%20effective%20self-supervised%20learning%20of%20these%20layout%20graphs.%0ABuilding%20on%20this%20foundation%2C%20we%20develop%20an%20autoencoder-based%20network%0Aarchitecture%20skilled%20in%20compressing%20these%20heterogeneous%20layout%20graphs%20into%0Aprecise%2C%20dimensionally-reduced%20layout%20representations.%20Additionally%2C%20we%0Aintroduce%20the%20LODB%20dataset%2C%20which%20features%20a%20broader%20range%20of%20layout%20categories%0Aand%20richer%20semantics%2C%20serving%20as%20a%20comprehensive%20benchmark%20for%20evaluating%20the%0Aeffectiveness%20of%20layout%20representation%20learning%20methods.%20Our%20extensive%0Aexperimentation%20on%20this%20dataset%20demonstrates%20the%20superior%20performance%20of%20our%0Aapproach%20in%20the%20realm%20of%20photographic%20image%20layout%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03740v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Photographic%2520Image%2520Layout%2520Representation%2520Learning%26entry.906535625%3DZhaoran%2520Zhao%2520and%2520Peng%2520Lu%2520and%2520Xujun%2520Peng%2520and%2520Wenhao%2520Guo%26entry.1292438233%3D%2520%2520In%2520the%2520domain%2520of%2520image%2520layout%2520representation%2520learning%252C%2520the%2520critical%2520process%250Aof%2520translating%2520image%2520layouts%2520into%2520succinct%2520vector%2520forms%2520is%2520increasingly%250Asignificant%2520across%2520diverse%2520applications%252C%2520such%2520as%2520image%2520retrieval%252C%2520manipulation%252C%250Aand%2520generation.%2520Most%2520approaches%2520in%2520this%2520area%2520heavily%2520rely%2520on%2520costly%2520labeled%250Adatasets%2520and%2520notably%2520lack%2520in%2520adapting%2520their%2520modeling%2520and%2520learning%2520methods%2520to%250Athe%2520specific%2520nuances%2520of%2520photographic%2520image%2520layouts.%2520This%2520shortfall%2520makes%2520the%250Alearning%2520process%2520for%2520photographic%2520image%2520layouts%2520suboptimal.%2520In%2520our%2520research%252C%2520we%250Adirectly%2520address%2520these%2520challenges.%2520We%2520innovate%2520by%2520defining%2520basic%2520layout%250Aprimitives%2520that%2520encapsulate%2520various%2520levels%2520of%2520layout%2520information%2520and%2520by%2520mapping%250Athese%252C%2520along%2520with%2520their%2520interconnections%252C%2520onto%2520a%2520heterogeneous%2520graph%2520structure.%250AThis%2520graph%2520is%2520meticulously%2520engineered%2520to%2520capture%2520the%2520intricate%2520layout%250Ainformation%2520within%2520the%2520pixel%2520domain%2520explicitly.%2520Advancing%2520further%252C%2520we%2520introduce%250Anovel%2520pretext%2520tasks%2520coupled%2520with%2520customized%2520loss%2520functions%252C%2520strategically%250Adesigned%2520for%2520effective%2520self-supervised%2520learning%2520of%2520these%2520layout%2520graphs.%250ABuilding%2520on%2520this%2520foundation%252C%2520we%2520develop%2520an%2520autoencoder-based%2520network%250Aarchitecture%2520skilled%2520in%2520compressing%2520these%2520heterogeneous%2520layout%2520graphs%2520into%250Aprecise%252C%2520dimensionally-reduced%2520layout%2520representations.%2520Additionally%252C%2520we%250Aintroduce%2520the%2520LODB%2520dataset%252C%2520which%2520features%2520a%2520broader%2520range%2520of%2520layout%2520categories%250Aand%2520richer%2520semantics%252C%2520serving%2520as%2520a%2520comprehensive%2520benchmark%2520for%2520evaluating%2520the%250Aeffectiveness%2520of%2520layout%2520representation%2520learning%2520methods.%2520Our%2520extensive%250Aexperimentation%2520on%2520this%2520dataset%2520demonstrates%2520the%2520superior%2520performance%2520of%2520our%250Aapproach%2520in%2520the%2520realm%2520of%2520photographic%2520image%2520layout%2520representation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03740v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Photographic%20Image%20Layout%20Representation%20Learning&entry.906535625=Zhaoran%20Zhao%20and%20Peng%20Lu%20and%20Xujun%20Peng%20and%20Wenhao%20Guo&entry.1292438233=%20%20In%20the%20domain%20of%20image%20layout%20representation%20learning%2C%20the%20critical%20process%0Aof%20translating%20image%20layouts%20into%20succinct%20vector%20forms%20is%20increasingly%0Asignificant%20across%20diverse%20applications%2C%20such%20as%20image%20retrieval%2C%20manipulation%2C%0Aand%20generation.%20Most%20approaches%20in%20this%20area%20heavily%20rely%20on%20costly%20labeled%0Adatasets%20and%20notably%20lack%20in%20adapting%20their%20modeling%20and%20learning%20methods%20to%0Athe%20specific%20nuances%20of%20photographic%20image%20layouts.%20This%20shortfall%20makes%20the%0Alearning%20process%20for%20photographic%20image%20layouts%20suboptimal.%20In%20our%20research%2C%20we%0Adirectly%20address%20these%20challenges.%20We%20innovate%20by%20defining%20basic%20layout%0Aprimitives%20that%20encapsulate%20various%20levels%20of%20layout%20information%20and%20by%20mapping%0Athese%2C%20along%20with%20their%20interconnections%2C%20onto%20a%20heterogeneous%20graph%20structure.%0AThis%20graph%20is%20meticulously%20engineered%20to%20capture%20the%20intricate%20layout%0Ainformation%20within%20the%20pixel%20domain%20explicitly.%20Advancing%20further%2C%20we%20introduce%0Anovel%20pretext%20tasks%20coupled%20with%20customized%20loss%20functions%2C%20strategically%0Adesigned%20for%20effective%20self-supervised%20learning%20of%20these%20layout%20graphs.%0ABuilding%20on%20this%20foundation%2C%20we%20develop%20an%20autoencoder-based%20network%0Aarchitecture%20skilled%20in%20compressing%20these%20heterogeneous%20layout%20graphs%20into%0Aprecise%2C%20dimensionally-reduced%20layout%20representations.%20Additionally%2C%20we%0Aintroduce%20the%20LODB%20dataset%2C%20which%20features%20a%20broader%20range%20of%20layout%20categories%0Aand%20richer%20semantics%2C%20serving%20as%20a%20comprehensive%20benchmark%20for%20evaluating%20the%0Aeffectiveness%20of%20layout%20representation%20learning%20methods.%20Our%20extensive%0Aexperimentation%20on%20this%20dataset%20demonstrates%20the%20superior%20performance%20of%20our%0Aapproach%20in%20the%20realm%20of%20photographic%20image%20layout%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03740v2&entry.124074799=Read"},
{"title": "Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark\n  Images Using Gaussian Splatting", "author": "Sheng Ye and Zhen-Hui Dong and Yubin Hu and Yu-Hui Wen and Yong-Jin Liu", "abstract": "  3D Gaussian Splatting has recently emerged as a powerful representation that\ncan synthesize remarkable novel views using consistent multi-view images as\ninput. However, we notice that images captured in dark environments where the\nscenes are not fully illuminated can exhibit considerable brightness variations\nand multi-view inconsistency, which poses great challenges to 3D Gaussian\nSplatting and severely degrades its performance. To tackle this problem, we\npropose Gaussian-DK. Observing that inconsistencies are mainly caused by camera\nimaging, we represent a consistent radiance field of the physical world using a\nset of anisotropic 3D Gaussians, and design a camera response module to\ncompensate for multi-view inconsistencies. We also introduce a step-based\ngradient scaling strategy to constrain Gaussians near the camera, which turn\nout to be floaters, from splitting and cloning. Experiments on our proposed\nbenchmark dataset demonstrate that Gaussian-DK produces high-quality renderings\nwithout ghosting and floater artifacts and significantly outperforms existing\nmethods. Furthermore, we can also synthesize light-up images by controlling\nexposure levels that clearly show details in shadow areas.\n", "link": "http://arxiv.org/abs/2408.09130v2", "date": "2024-08-20", "relevancy": 3.2495, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7141}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6713}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20in%20the%20Dark%3A%20Real-Time%20View%20Synthesis%20From%20Inconsistent%20Dark%0A%20%20Images%20Using%20Gaussian%20Splatting&body=Title%3A%20Gaussian%20in%20the%20Dark%3A%20Real-Time%20View%20Synthesis%20From%20Inconsistent%20Dark%0A%20%20Images%20Using%20Gaussian%20Splatting%0AAuthor%3A%20Sheng%20Ye%20and%20Zhen-Hui%20Dong%20and%20Yubin%20Hu%20and%20Yu-Hui%20Wen%20and%20Yong-Jin%20Liu%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20recently%20emerged%20as%20a%20powerful%20representation%20that%0Acan%20synthesize%20remarkable%20novel%20views%20using%20consistent%20multi-view%20images%20as%0Ainput.%20However%2C%20we%20notice%20that%20images%20captured%20in%20dark%20environments%20where%20the%0Ascenes%20are%20not%20fully%20illuminated%20can%20exhibit%20considerable%20brightness%20variations%0Aand%20multi-view%20inconsistency%2C%20which%20poses%20great%20challenges%20to%203D%20Gaussian%0ASplatting%20and%20severely%20degrades%20its%20performance.%20To%20tackle%20this%20problem%2C%20we%0Apropose%20Gaussian-DK.%20Observing%20that%20inconsistencies%20are%20mainly%20caused%20by%20camera%0Aimaging%2C%20we%20represent%20a%20consistent%20radiance%20field%20of%20the%20physical%20world%20using%20a%0Aset%20of%20anisotropic%203D%20Gaussians%2C%20and%20design%20a%20camera%20response%20module%20to%0Acompensate%20for%20multi-view%20inconsistencies.%20We%20also%20introduce%20a%20step-based%0Agradient%20scaling%20strategy%20to%20constrain%20Gaussians%20near%20the%20camera%2C%20which%20turn%0Aout%20to%20be%20floaters%2C%20from%20splitting%20and%20cloning.%20Experiments%20on%20our%20proposed%0Abenchmark%20dataset%20demonstrate%20that%20Gaussian-DK%20produces%20high-quality%20renderings%0Awithout%20ghosting%20and%20floater%20artifacts%20and%20significantly%20outperforms%20existing%0Amethods.%20Furthermore%2C%20we%20can%20also%20synthesize%20light-up%20images%20by%20controlling%0Aexposure%20levels%20that%20clearly%20show%20details%20in%20shadow%20areas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09130v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520in%2520the%2520Dark%253A%2520Real-Time%2520View%2520Synthesis%2520From%2520Inconsistent%2520Dark%250A%2520%2520Images%2520Using%2520Gaussian%2520Splatting%26entry.906535625%3DSheng%2520Ye%2520and%2520Zhen-Hui%2520Dong%2520and%2520Yubin%2520Hu%2520and%2520Yu-Hui%2520Wen%2520and%2520Yong-Jin%2520Liu%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520has%2520recently%2520emerged%2520as%2520a%2520powerful%2520representation%2520that%250Acan%2520synthesize%2520remarkable%2520novel%2520views%2520using%2520consistent%2520multi-view%2520images%2520as%250Ainput.%2520However%252C%2520we%2520notice%2520that%2520images%2520captured%2520in%2520dark%2520environments%2520where%2520the%250Ascenes%2520are%2520not%2520fully%2520illuminated%2520can%2520exhibit%2520considerable%2520brightness%2520variations%250Aand%2520multi-view%2520inconsistency%252C%2520which%2520poses%2520great%2520challenges%2520to%25203D%2520Gaussian%250ASplatting%2520and%2520severely%2520degrades%2520its%2520performance.%2520To%2520tackle%2520this%2520problem%252C%2520we%250Apropose%2520Gaussian-DK.%2520Observing%2520that%2520inconsistencies%2520are%2520mainly%2520caused%2520by%2520camera%250Aimaging%252C%2520we%2520represent%2520a%2520consistent%2520radiance%2520field%2520of%2520the%2520physical%2520world%2520using%2520a%250Aset%2520of%2520anisotropic%25203D%2520Gaussians%252C%2520and%2520design%2520a%2520camera%2520response%2520module%2520to%250Acompensate%2520for%2520multi-view%2520inconsistencies.%2520We%2520also%2520introduce%2520a%2520step-based%250Agradient%2520scaling%2520strategy%2520to%2520constrain%2520Gaussians%2520near%2520the%2520camera%252C%2520which%2520turn%250Aout%2520to%2520be%2520floaters%252C%2520from%2520splitting%2520and%2520cloning.%2520Experiments%2520on%2520our%2520proposed%250Abenchmark%2520dataset%2520demonstrate%2520that%2520Gaussian-DK%2520produces%2520high-quality%2520renderings%250Awithout%2520ghosting%2520and%2520floater%2520artifacts%2520and%2520significantly%2520outperforms%2520existing%250Amethods.%2520Furthermore%252C%2520we%2520can%2520also%2520synthesize%2520light-up%2520images%2520by%2520controlling%250Aexposure%2520levels%2520that%2520clearly%2520show%2520details%2520in%2520shadow%2520areas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09130v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20in%20the%20Dark%3A%20Real-Time%20View%20Synthesis%20From%20Inconsistent%20Dark%0A%20%20Images%20Using%20Gaussian%20Splatting&entry.906535625=Sheng%20Ye%20and%20Zhen-Hui%20Dong%20and%20Yubin%20Hu%20and%20Yu-Hui%20Wen%20and%20Yong-Jin%20Liu&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20recently%20emerged%20as%20a%20powerful%20representation%20that%0Acan%20synthesize%20remarkable%20novel%20views%20using%20consistent%20multi-view%20images%20as%0Ainput.%20However%2C%20we%20notice%20that%20images%20captured%20in%20dark%20environments%20where%20the%0Ascenes%20are%20not%20fully%20illuminated%20can%20exhibit%20considerable%20brightness%20variations%0Aand%20multi-view%20inconsistency%2C%20which%20poses%20great%20challenges%20to%203D%20Gaussian%0ASplatting%20and%20severely%20degrades%20its%20performance.%20To%20tackle%20this%20problem%2C%20we%0Apropose%20Gaussian-DK.%20Observing%20that%20inconsistencies%20are%20mainly%20caused%20by%20camera%0Aimaging%2C%20we%20represent%20a%20consistent%20radiance%20field%20of%20the%20physical%20world%20using%20a%0Aset%20of%20anisotropic%203D%20Gaussians%2C%20and%20design%20a%20camera%20response%20module%20to%0Acompensate%20for%20multi-view%20inconsistencies.%20We%20also%20introduce%20a%20step-based%0Agradient%20scaling%20strategy%20to%20constrain%20Gaussians%20near%20the%20camera%2C%20which%20turn%0Aout%20to%20be%20floaters%2C%20from%20splitting%20and%20cloning.%20Experiments%20on%20our%20proposed%0Abenchmark%20dataset%20demonstrate%20that%20Gaussian-DK%20produces%20high-quality%20renderings%0Awithout%20ghosting%20and%20floater%20artifacts%20and%20significantly%20outperforms%20existing%0Amethods.%20Furthermore%2C%20we%20can%20also%20synthesize%20light-up%20images%20by%20controlling%0Aexposure%20levels%20that%20clearly%20show%20details%20in%20shadow%20areas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09130v2&entry.124074799=Read"},
{"title": "Human-Aware 3D Scene Generation with Spatially-constrained Diffusion\n  Models", "author": "Xiaolin Hong and Hongwei Yi and Fazhi He and Qiong Cao", "abstract": "  Generating 3D scenes from human motion sequences supports numerous\napplications, including virtual reality and architectural design. However,\nprevious auto-regression-based human-aware 3D scene generation methods have\nstruggled to accurately capture the joint distribution of multiple objects and\ninput humans, often resulting in overlapping object generation in the same\nspace. To address this limitation, we explore the potential of diffusion models\nthat simultaneously consider all input humans and the floor plan to generate\nplausible 3D scenes. Our approach not only satisfies all input human\ninteractions but also adheres to spatial constraints with the floor plan.\nFurthermore, we introduce two spatial collision guidance mechanisms:\nhuman-object collision avoidance and object-room boundary constraints. These\nmechanisms help avoid generating scenes that conflict with human motions while\nrespecting layout constraints. To enhance the diversity and accuracy of\nhuman-guided scene generation, we have developed an automated pipeline that\nimproves the variety and plausibility of human-object interactions in the\nexisting 3D FRONT HUMAN dataset. Extensive experiments on both synthetic and\nreal-world datasets demonstrate that our framework can generate more natural\nand plausible 3D scenes with precise human-scene interactions, while\nsignificantly reducing human-object collisions compared to previous\nstate-of-the-art methods. Our code and data will be made publicly available\nupon publication of this work.\n", "link": "http://arxiv.org/abs/2406.18159v2", "date": "2024-08-20", "relevancy": 3.2072, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6591}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6591}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Aware%203D%20Scene%20Generation%20with%20Spatially-constrained%20Diffusion%0A%20%20Models&body=Title%3A%20Human-Aware%203D%20Scene%20Generation%20with%20Spatially-constrained%20Diffusion%0A%20%20Models%0AAuthor%3A%20Xiaolin%20Hong%20and%20Hongwei%20Yi%20and%20Fazhi%20He%20and%20Qiong%20Cao%0AAbstract%3A%20%20%20Generating%203D%20scenes%20from%20human%20motion%20sequences%20supports%20numerous%0Aapplications%2C%20including%20virtual%20reality%20and%20architectural%20design.%20However%2C%0Aprevious%20auto-regression-based%20human-aware%203D%20scene%20generation%20methods%20have%0Astruggled%20to%20accurately%20capture%20the%20joint%20distribution%20of%20multiple%20objects%20and%0Ainput%20humans%2C%20often%20resulting%20in%20overlapping%20object%20generation%20in%20the%20same%0Aspace.%20To%20address%20this%20limitation%2C%20we%20explore%20the%20potential%20of%20diffusion%20models%0Athat%20simultaneously%20consider%20all%20input%20humans%20and%20the%20floor%20plan%20to%20generate%0Aplausible%203D%20scenes.%20Our%20approach%20not%20only%20satisfies%20all%20input%20human%0Ainteractions%20but%20also%20adheres%20to%20spatial%20constraints%20with%20the%20floor%20plan.%0AFurthermore%2C%20we%20introduce%20two%20spatial%20collision%20guidance%20mechanisms%3A%0Ahuman-object%20collision%20avoidance%20and%20object-room%20boundary%20constraints.%20These%0Amechanisms%20help%20avoid%20generating%20scenes%20that%20conflict%20with%20human%20motions%20while%0Arespecting%20layout%20constraints.%20To%20enhance%20the%20diversity%20and%20accuracy%20of%0Ahuman-guided%20scene%20generation%2C%20we%20have%20developed%20an%20automated%20pipeline%20that%0Aimproves%20the%20variety%20and%20plausibility%20of%20human-object%20interactions%20in%20the%0Aexisting%203D%20FRONT%20HUMAN%20dataset.%20Extensive%20experiments%20on%20both%20synthetic%20and%0Areal-world%20datasets%20demonstrate%20that%20our%20framework%20can%20generate%20more%20natural%0Aand%20plausible%203D%20scenes%20with%20precise%20human-scene%20interactions%2C%20while%0Asignificantly%20reducing%20human-object%20collisions%20compared%20to%20previous%0Astate-of-the-art%20methods.%20Our%20code%20and%20data%20will%20be%20made%20publicly%20available%0Aupon%20publication%20of%20this%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18159v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Aware%25203D%2520Scene%2520Generation%2520with%2520Spatially-constrained%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DXiaolin%2520Hong%2520and%2520Hongwei%2520Yi%2520and%2520Fazhi%2520He%2520and%2520Qiong%2520Cao%26entry.1292438233%3D%2520%2520Generating%25203D%2520scenes%2520from%2520human%2520motion%2520sequences%2520supports%2520numerous%250Aapplications%252C%2520including%2520virtual%2520reality%2520and%2520architectural%2520design.%2520However%252C%250Aprevious%2520auto-regression-based%2520human-aware%25203D%2520scene%2520generation%2520methods%2520have%250Astruggled%2520to%2520accurately%2520capture%2520the%2520joint%2520distribution%2520of%2520multiple%2520objects%2520and%250Ainput%2520humans%252C%2520often%2520resulting%2520in%2520overlapping%2520object%2520generation%2520in%2520the%2520same%250Aspace.%2520To%2520address%2520this%2520limitation%252C%2520we%2520explore%2520the%2520potential%2520of%2520diffusion%2520models%250Athat%2520simultaneously%2520consider%2520all%2520input%2520humans%2520and%2520the%2520floor%2520plan%2520to%2520generate%250Aplausible%25203D%2520scenes.%2520Our%2520approach%2520not%2520only%2520satisfies%2520all%2520input%2520human%250Ainteractions%2520but%2520also%2520adheres%2520to%2520spatial%2520constraints%2520with%2520the%2520floor%2520plan.%250AFurthermore%252C%2520we%2520introduce%2520two%2520spatial%2520collision%2520guidance%2520mechanisms%253A%250Ahuman-object%2520collision%2520avoidance%2520and%2520object-room%2520boundary%2520constraints.%2520These%250Amechanisms%2520help%2520avoid%2520generating%2520scenes%2520that%2520conflict%2520with%2520human%2520motions%2520while%250Arespecting%2520layout%2520constraints.%2520To%2520enhance%2520the%2520diversity%2520and%2520accuracy%2520of%250Ahuman-guided%2520scene%2520generation%252C%2520we%2520have%2520developed%2520an%2520automated%2520pipeline%2520that%250Aimproves%2520the%2520variety%2520and%2520plausibility%2520of%2520human-object%2520interactions%2520in%2520the%250Aexisting%25203D%2520FRONT%2520HUMAN%2520dataset.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%250Areal-world%2520datasets%2520demonstrate%2520that%2520our%2520framework%2520can%2520generate%2520more%2520natural%250Aand%2520plausible%25203D%2520scenes%2520with%2520precise%2520human-scene%2520interactions%252C%2520while%250Asignificantly%2520reducing%2520human-object%2520collisions%2520compared%2520to%2520previous%250Astate-of-the-art%2520methods.%2520Our%2520code%2520and%2520data%2520will%2520be%2520made%2520publicly%2520available%250Aupon%2520publication%2520of%2520this%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18159v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Aware%203D%20Scene%20Generation%20with%20Spatially-constrained%20Diffusion%0A%20%20Models&entry.906535625=Xiaolin%20Hong%20and%20Hongwei%20Yi%20and%20Fazhi%20He%20and%20Qiong%20Cao&entry.1292438233=%20%20Generating%203D%20scenes%20from%20human%20motion%20sequences%20supports%20numerous%0Aapplications%2C%20including%20virtual%20reality%20and%20architectural%20design.%20However%2C%0Aprevious%20auto-regression-based%20human-aware%203D%20scene%20generation%20methods%20have%0Astruggled%20to%20accurately%20capture%20the%20joint%20distribution%20of%20multiple%20objects%20and%0Ainput%20humans%2C%20often%20resulting%20in%20overlapping%20object%20generation%20in%20the%20same%0Aspace.%20To%20address%20this%20limitation%2C%20we%20explore%20the%20potential%20of%20diffusion%20models%0Athat%20simultaneously%20consider%20all%20input%20humans%20and%20the%20floor%20plan%20to%20generate%0Aplausible%203D%20scenes.%20Our%20approach%20not%20only%20satisfies%20all%20input%20human%0Ainteractions%20but%20also%20adheres%20to%20spatial%20constraints%20with%20the%20floor%20plan.%0AFurthermore%2C%20we%20introduce%20two%20spatial%20collision%20guidance%20mechanisms%3A%0Ahuman-object%20collision%20avoidance%20and%20object-room%20boundary%20constraints.%20These%0Amechanisms%20help%20avoid%20generating%20scenes%20that%20conflict%20with%20human%20motions%20while%0Arespecting%20layout%20constraints.%20To%20enhance%20the%20diversity%20and%20accuracy%20of%0Ahuman-guided%20scene%20generation%2C%20we%20have%20developed%20an%20automated%20pipeline%20that%0Aimproves%20the%20variety%20and%20plausibility%20of%20human-object%20interactions%20in%20the%0Aexisting%203D%20FRONT%20HUMAN%20dataset.%20Extensive%20experiments%20on%20both%20synthetic%20and%0Areal-world%20datasets%20demonstrate%20that%20our%20framework%20can%20generate%20more%20natural%0Aand%20plausible%203D%20scenes%20with%20precise%20human-scene%20interactions%2C%20while%0Asignificantly%20reducing%20human-object%20collisions%20compared%20to%20previous%0Astate-of-the-art%20methods.%20Our%20code%20and%20data%20will%20be%20made%20publicly%20available%0Aupon%20publication%20of%20this%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18159v2&entry.124074799=Read"},
{"title": "DynaPix SLAM: A Pixel-Based Dynamic Visual SLAM Approach", "author": "Chenghao Xu and Elia Bonetto and Aamir Ahmad", "abstract": "  Visual Simultaneous Localization and Mapping (V-SLAM) methods achieve\nremarkable performance in static environments, but face challenges in dynamic\nscenes where moving objects severely affect their core modules. To avoid this,\ndynamic V-SLAM approaches often leverage semantic information, geometric\nconstraints, or optical flow. However, these methods are limited by imprecise\nestimations and their reliance on the accuracy of deep-learning models.\nMoreover, predefined thresholds for static/dynamic classification, the a-priori\nselection of dynamic object classes, and the inability to recognize unknown or\nunexpected moving objects, often degrade their performance. To address these\nlimitations, we introduce DynaPix, a novel semantic-free V-SLAM system based on\nper-pixel motion probability estimation and an improved pose optimization\nprocess. The per-pixel motion probability is estimated using a static\nbackground differencing method on image data and optical flows computed on\nsplatted frames. With DynaPix, we fully integrate these probabilities into map\npoint selection and apply them through weighted bundle adjustment within the\ntracking and optimization modules of ORB-SLAM2. We thoroughly evaluate our\nmethod using the GRADE and TUM RGB-D datasets, showing significantly lower\ntrajectory errors and longer tracking times in both static and dynamic\nsequences. The source code, datasets, and results are available at\nhttps://dynapix.is.tue.mpg.de/.\n", "link": "http://arxiv.org/abs/2309.09879v2", "date": "2024-08-20", "relevancy": 3.1564, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6674}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6289}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaPix%20SLAM%3A%20A%20Pixel-Based%20Dynamic%20Visual%20SLAM%20Approach&body=Title%3A%20DynaPix%20SLAM%3A%20A%20Pixel-Based%20Dynamic%20Visual%20SLAM%20Approach%0AAuthor%3A%20Chenghao%20Xu%20and%20Elia%20Bonetto%20and%20Aamir%20Ahmad%0AAbstract%3A%20%20%20Visual%20Simultaneous%20Localization%20and%20Mapping%20%28V-SLAM%29%20methods%20achieve%0Aremarkable%20performance%20in%20static%20environments%2C%20but%20face%20challenges%20in%20dynamic%0Ascenes%20where%20moving%20objects%20severely%20affect%20their%20core%20modules.%20To%20avoid%20this%2C%0Adynamic%20V-SLAM%20approaches%20often%20leverage%20semantic%20information%2C%20geometric%0Aconstraints%2C%20or%20optical%20flow.%20However%2C%20these%20methods%20are%20limited%20by%20imprecise%0Aestimations%20and%20their%20reliance%20on%20the%20accuracy%20of%20deep-learning%20models.%0AMoreover%2C%20predefined%20thresholds%20for%20static/dynamic%20classification%2C%20the%20a-priori%0Aselection%20of%20dynamic%20object%20classes%2C%20and%20the%20inability%20to%20recognize%20unknown%20or%0Aunexpected%20moving%20objects%2C%20often%20degrade%20their%20performance.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20DynaPix%2C%20a%20novel%20semantic-free%20V-SLAM%20system%20based%20on%0Aper-pixel%20motion%20probability%20estimation%20and%20an%20improved%20pose%20optimization%0Aprocess.%20The%20per-pixel%20motion%20probability%20is%20estimated%20using%20a%20static%0Abackground%20differencing%20method%20on%20image%20data%20and%20optical%20flows%20computed%20on%0Asplatted%20frames.%20With%20DynaPix%2C%20we%20fully%20integrate%20these%20probabilities%20into%20map%0Apoint%20selection%20and%20apply%20them%20through%20weighted%20bundle%20adjustment%20within%20the%0Atracking%20and%20optimization%20modules%20of%20ORB-SLAM2.%20We%20thoroughly%20evaluate%20our%0Amethod%20using%20the%20GRADE%20and%20TUM%20RGB-D%20datasets%2C%20showing%20significantly%20lower%0Atrajectory%20errors%20and%20longer%20tracking%20times%20in%20both%20static%20and%20dynamic%0Asequences.%20The%20source%20code%2C%20datasets%2C%20and%20results%20are%20available%20at%0Ahttps%3A//dynapix.is.tue.mpg.de/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.09879v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaPix%2520SLAM%253A%2520A%2520Pixel-Based%2520Dynamic%2520Visual%2520SLAM%2520Approach%26entry.906535625%3DChenghao%2520Xu%2520and%2520Elia%2520Bonetto%2520and%2520Aamir%2520Ahmad%26entry.1292438233%3D%2520%2520Visual%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528V-SLAM%2529%2520methods%2520achieve%250Aremarkable%2520performance%2520in%2520static%2520environments%252C%2520but%2520face%2520challenges%2520in%2520dynamic%250Ascenes%2520where%2520moving%2520objects%2520severely%2520affect%2520their%2520core%2520modules.%2520To%2520avoid%2520this%252C%250Adynamic%2520V-SLAM%2520approaches%2520often%2520leverage%2520semantic%2520information%252C%2520geometric%250Aconstraints%252C%2520or%2520optical%2520flow.%2520However%252C%2520these%2520methods%2520are%2520limited%2520by%2520imprecise%250Aestimations%2520and%2520their%2520reliance%2520on%2520the%2520accuracy%2520of%2520deep-learning%2520models.%250AMoreover%252C%2520predefined%2520thresholds%2520for%2520static/dynamic%2520classification%252C%2520the%2520a-priori%250Aselection%2520of%2520dynamic%2520object%2520classes%252C%2520and%2520the%2520inability%2520to%2520recognize%2520unknown%2520or%250Aunexpected%2520moving%2520objects%252C%2520often%2520degrade%2520their%2520performance.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520DynaPix%252C%2520a%2520novel%2520semantic-free%2520V-SLAM%2520system%2520based%2520on%250Aper-pixel%2520motion%2520probability%2520estimation%2520and%2520an%2520improved%2520pose%2520optimization%250Aprocess.%2520The%2520per-pixel%2520motion%2520probability%2520is%2520estimated%2520using%2520a%2520static%250Abackground%2520differencing%2520method%2520on%2520image%2520data%2520and%2520optical%2520flows%2520computed%2520on%250Asplatted%2520frames.%2520With%2520DynaPix%252C%2520we%2520fully%2520integrate%2520these%2520probabilities%2520into%2520map%250Apoint%2520selection%2520and%2520apply%2520them%2520through%2520weighted%2520bundle%2520adjustment%2520within%2520the%250Atracking%2520and%2520optimization%2520modules%2520of%2520ORB-SLAM2.%2520We%2520thoroughly%2520evaluate%2520our%250Amethod%2520using%2520the%2520GRADE%2520and%2520TUM%2520RGB-D%2520datasets%252C%2520showing%2520significantly%2520lower%250Atrajectory%2520errors%2520and%2520longer%2520tracking%2520times%2520in%2520both%2520static%2520and%2520dynamic%250Asequences.%2520The%2520source%2520code%252C%2520datasets%252C%2520and%2520results%2520are%2520available%2520at%250Ahttps%253A//dynapix.is.tue.mpg.de/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.09879v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaPix%20SLAM%3A%20A%20Pixel-Based%20Dynamic%20Visual%20SLAM%20Approach&entry.906535625=Chenghao%20Xu%20and%20Elia%20Bonetto%20and%20Aamir%20Ahmad&entry.1292438233=%20%20Visual%20Simultaneous%20Localization%20and%20Mapping%20%28V-SLAM%29%20methods%20achieve%0Aremarkable%20performance%20in%20static%20environments%2C%20but%20face%20challenges%20in%20dynamic%0Ascenes%20where%20moving%20objects%20severely%20affect%20their%20core%20modules.%20To%20avoid%20this%2C%0Adynamic%20V-SLAM%20approaches%20often%20leverage%20semantic%20information%2C%20geometric%0Aconstraints%2C%20or%20optical%20flow.%20However%2C%20these%20methods%20are%20limited%20by%20imprecise%0Aestimations%20and%20their%20reliance%20on%20the%20accuracy%20of%20deep-learning%20models.%0AMoreover%2C%20predefined%20thresholds%20for%20static/dynamic%20classification%2C%20the%20a-priori%0Aselection%20of%20dynamic%20object%20classes%2C%20and%20the%20inability%20to%20recognize%20unknown%20or%0Aunexpected%20moving%20objects%2C%20often%20degrade%20their%20performance.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20DynaPix%2C%20a%20novel%20semantic-free%20V-SLAM%20system%20based%20on%0Aper-pixel%20motion%20probability%20estimation%20and%20an%20improved%20pose%20optimization%0Aprocess.%20The%20per-pixel%20motion%20probability%20is%20estimated%20using%20a%20static%0Abackground%20differencing%20method%20on%20image%20data%20and%20optical%20flows%20computed%20on%0Asplatted%20frames.%20With%20DynaPix%2C%20we%20fully%20integrate%20these%20probabilities%20into%20map%0Apoint%20selection%20and%20apply%20them%20through%20weighted%20bundle%20adjustment%20within%20the%0Atracking%20and%20optimization%20modules%20of%20ORB-SLAM2.%20We%20thoroughly%20evaluate%20our%0Amethod%20using%20the%20GRADE%20and%20TUM%20RGB-D%20datasets%2C%20showing%20significantly%20lower%0Atrajectory%20errors%20and%20longer%20tracking%20times%20in%20both%20static%20and%20dynamic%0Asequences.%20The%20source%20code%2C%20datasets%2C%20and%20results%20are%20available%20at%0Ahttps%3A//dynapix.is.tue.mpg.de/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09879v2&entry.124074799=Read"},
{"title": "Scene123: One Prompt to 3D Scene Generation via Video-Assisted and\n  Consistency-Enhanced MAE", "author": "Yiying Yang and Fukun Yin and Jiayuan Fan and Xin Chen and Wanzhang Li and Gang Yu", "abstract": "  As Artificial Intelligence Generated Content (AIGC) advances, a variety of\nmethods have been developed to generate text, images, videos, and 3D objects\nfrom single or multimodal inputs, contributing efforts to emulate human-like\ncognitive content creation. However, generating realistic large-scale scenes\nfrom a single input presents a challenge due to the complexities involved in\nensuring consistency across extrapolated views generated by models. Benefiting\nfrom recent video generation models and implicit neural representations, we\npropose Scene123, a 3D scene generation model, that not only ensures realism\nand diversity through the video generation framework but also uses implicit\nneural fields combined with Masked Autoencoders (MAE) to effectively ensures\nthe consistency of unseen areas across views. Specifically, we initially warp\nthe input image (or an image generated from text) to simulate adjacent views,\nfilling the invisible areas with the MAE model. However, these filled images\nusually fail to maintain view consistency, thus we utilize the produced views\nto optimize a neural radiance field, enhancing geometric consistency.\n  Moreover, to further enhance the details and texture fidelity of generated\nviews, we employ a GAN-based Loss against images derived from the input image\nthrough the video generation model. Extensive experiments demonstrate that our\nmethod can generate realistic and consistent scenes from a single prompt. Both\nqualitative and quantitative results indicate that our approach surpasses\nexisting state-of-the-art methods. We show encourage video examples at\nhttps://yiyingyang12.github.io/Scene123.github.io/.\n", "link": "http://arxiv.org/abs/2408.05477v2", "date": "2024-08-20", "relevancy": 3.142, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6428}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6212}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scene123%3A%20One%20Prompt%20to%203D%20Scene%20Generation%20via%20Video-Assisted%20and%0A%20%20Consistency-Enhanced%20MAE&body=Title%3A%20Scene123%3A%20One%20Prompt%20to%203D%20Scene%20Generation%20via%20Video-Assisted%20and%0A%20%20Consistency-Enhanced%20MAE%0AAuthor%3A%20Yiying%20Yang%20and%20Fukun%20Yin%20and%20Jiayuan%20Fan%20and%20Xin%20Chen%20and%20Wanzhang%20Li%20and%20Gang%20Yu%0AAbstract%3A%20%20%20As%20Artificial%20Intelligence%20Generated%20Content%20%28AIGC%29%20advances%2C%20a%20variety%20of%0Amethods%20have%20been%20developed%20to%20generate%20text%2C%20images%2C%20videos%2C%20and%203D%20objects%0Afrom%20single%20or%20multimodal%20inputs%2C%20contributing%20efforts%20to%20emulate%20human-like%0Acognitive%20content%20creation.%20However%2C%20generating%20realistic%20large-scale%20scenes%0Afrom%20a%20single%20input%20presents%20a%20challenge%20due%20to%20the%20complexities%20involved%20in%0Aensuring%20consistency%20across%20extrapolated%20views%20generated%20by%20models.%20Benefiting%0Afrom%20recent%20video%20generation%20models%20and%20implicit%20neural%20representations%2C%20we%0Apropose%20Scene123%2C%20a%203D%20scene%20generation%20model%2C%20that%20not%20only%20ensures%20realism%0Aand%20diversity%20through%20the%20video%20generation%20framework%20but%20also%20uses%20implicit%0Aneural%20fields%20combined%20with%20Masked%20Autoencoders%20%28MAE%29%20to%20effectively%20ensures%0Athe%20consistency%20of%20unseen%20areas%20across%20views.%20Specifically%2C%20we%20initially%20warp%0Athe%20input%20image%20%28or%20an%20image%20generated%20from%20text%29%20to%20simulate%20adjacent%20views%2C%0Afilling%20the%20invisible%20areas%20with%20the%20MAE%20model.%20However%2C%20these%20filled%20images%0Ausually%20fail%20to%20maintain%20view%20consistency%2C%20thus%20we%20utilize%20the%20produced%20views%0Ato%20optimize%20a%20neural%20radiance%20field%2C%20enhancing%20geometric%20consistency.%0A%20%20Moreover%2C%20to%20further%20enhance%20the%20details%20and%20texture%20fidelity%20of%20generated%0Aviews%2C%20we%20employ%20a%20GAN-based%20Loss%20against%20images%20derived%20from%20the%20input%20image%0Athrough%20the%20video%20generation%20model.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20can%20generate%20realistic%20and%20consistent%20scenes%20from%20a%20single%20prompt.%20Both%0Aqualitative%20and%20quantitative%20results%20indicate%20that%20our%20approach%20surpasses%0Aexisting%20state-of-the-art%20methods.%20We%20show%20encourage%20video%20examples%20at%0Ahttps%3A//yiyingyang12.github.io/Scene123.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScene123%253A%2520One%2520Prompt%2520to%25203D%2520Scene%2520Generation%2520via%2520Video-Assisted%2520and%250A%2520%2520Consistency-Enhanced%2520MAE%26entry.906535625%3DYiying%2520Yang%2520and%2520Fukun%2520Yin%2520and%2520Jiayuan%2520Fan%2520and%2520Xin%2520Chen%2520and%2520Wanzhang%2520Li%2520and%2520Gang%2520Yu%26entry.1292438233%3D%2520%2520As%2520Artificial%2520Intelligence%2520Generated%2520Content%2520%2528AIGC%2529%2520advances%252C%2520a%2520variety%2520of%250Amethods%2520have%2520been%2520developed%2520to%2520generate%2520text%252C%2520images%252C%2520videos%252C%2520and%25203D%2520objects%250Afrom%2520single%2520or%2520multimodal%2520inputs%252C%2520contributing%2520efforts%2520to%2520emulate%2520human-like%250Acognitive%2520content%2520creation.%2520However%252C%2520generating%2520realistic%2520large-scale%2520scenes%250Afrom%2520a%2520single%2520input%2520presents%2520a%2520challenge%2520due%2520to%2520the%2520complexities%2520involved%2520in%250Aensuring%2520consistency%2520across%2520extrapolated%2520views%2520generated%2520by%2520models.%2520Benefiting%250Afrom%2520recent%2520video%2520generation%2520models%2520and%2520implicit%2520neural%2520representations%252C%2520we%250Apropose%2520Scene123%252C%2520a%25203D%2520scene%2520generation%2520model%252C%2520that%2520not%2520only%2520ensures%2520realism%250Aand%2520diversity%2520through%2520the%2520video%2520generation%2520framework%2520but%2520also%2520uses%2520implicit%250Aneural%2520fields%2520combined%2520with%2520Masked%2520Autoencoders%2520%2528MAE%2529%2520to%2520effectively%2520ensures%250Athe%2520consistency%2520of%2520unseen%2520areas%2520across%2520views.%2520Specifically%252C%2520we%2520initially%2520warp%250Athe%2520input%2520image%2520%2528or%2520an%2520image%2520generated%2520from%2520text%2529%2520to%2520simulate%2520adjacent%2520views%252C%250Afilling%2520the%2520invisible%2520areas%2520with%2520the%2520MAE%2520model.%2520However%252C%2520these%2520filled%2520images%250Ausually%2520fail%2520to%2520maintain%2520view%2520consistency%252C%2520thus%2520we%2520utilize%2520the%2520produced%2520views%250Ato%2520optimize%2520a%2520neural%2520radiance%2520field%252C%2520enhancing%2520geometric%2520consistency.%250A%2520%2520Moreover%252C%2520to%2520further%2520enhance%2520the%2520details%2520and%2520texture%2520fidelity%2520of%2520generated%250Aviews%252C%2520we%2520employ%2520a%2520GAN-based%2520Loss%2520against%2520images%2520derived%2520from%2520the%2520input%2520image%250Athrough%2520the%2520video%2520generation%2520model.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Amethod%2520can%2520generate%2520realistic%2520and%2520consistent%2520scenes%2520from%2520a%2520single%2520prompt.%2520Both%250Aqualitative%2520and%2520quantitative%2520results%2520indicate%2520that%2520our%2520approach%2520surpasses%250Aexisting%2520state-of-the-art%2520methods.%2520We%2520show%2520encourage%2520video%2520examples%2520at%250Ahttps%253A//yiyingyang12.github.io/Scene123.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scene123%3A%20One%20Prompt%20to%203D%20Scene%20Generation%20via%20Video-Assisted%20and%0A%20%20Consistency-Enhanced%20MAE&entry.906535625=Yiying%20Yang%20and%20Fukun%20Yin%20and%20Jiayuan%20Fan%20and%20Xin%20Chen%20and%20Wanzhang%20Li%20and%20Gang%20Yu&entry.1292438233=%20%20As%20Artificial%20Intelligence%20Generated%20Content%20%28AIGC%29%20advances%2C%20a%20variety%20of%0Amethods%20have%20been%20developed%20to%20generate%20text%2C%20images%2C%20videos%2C%20and%203D%20objects%0Afrom%20single%20or%20multimodal%20inputs%2C%20contributing%20efforts%20to%20emulate%20human-like%0Acognitive%20content%20creation.%20However%2C%20generating%20realistic%20large-scale%20scenes%0Afrom%20a%20single%20input%20presents%20a%20challenge%20due%20to%20the%20complexities%20involved%20in%0Aensuring%20consistency%20across%20extrapolated%20views%20generated%20by%20models.%20Benefiting%0Afrom%20recent%20video%20generation%20models%20and%20implicit%20neural%20representations%2C%20we%0Apropose%20Scene123%2C%20a%203D%20scene%20generation%20model%2C%20that%20not%20only%20ensures%20realism%0Aand%20diversity%20through%20the%20video%20generation%20framework%20but%20also%20uses%20implicit%0Aneural%20fields%20combined%20with%20Masked%20Autoencoders%20%28MAE%29%20to%20effectively%20ensures%0Athe%20consistency%20of%20unseen%20areas%20across%20views.%20Specifically%2C%20we%20initially%20warp%0Athe%20input%20image%20%28or%20an%20image%20generated%20from%20text%29%20to%20simulate%20adjacent%20views%2C%0Afilling%20the%20invisible%20areas%20with%20the%20MAE%20model.%20However%2C%20these%20filled%20images%0Ausually%20fail%20to%20maintain%20view%20consistency%2C%20thus%20we%20utilize%20the%20produced%20views%0Ato%20optimize%20a%20neural%20radiance%20field%2C%20enhancing%20geometric%20consistency.%0A%20%20Moreover%2C%20to%20further%20enhance%20the%20details%20and%20texture%20fidelity%20of%20generated%0Aviews%2C%20we%20employ%20a%20GAN-based%20Loss%20against%20images%20derived%20from%20the%20input%20image%0Athrough%20the%20video%20generation%20model.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20can%20generate%20realistic%20and%20consistent%20scenes%20from%20a%20single%20prompt.%20Both%0Aqualitative%20and%20quantitative%20results%20indicate%20that%20our%20approach%20surpasses%0Aexisting%20state-of-the-art%20methods.%20We%20show%20encourage%20video%20examples%20at%0Ahttps%3A//yiyingyang12.github.io/Scene123.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05477v2&entry.124074799=Read"},
{"title": "Large Point-to-Gaussian Model for Image-to-3D Generation", "author": "Longfei Lu and Huachen Gao and Tao Dai and Yaohua Zha and Zhi Hou and Junta Wu and Shu-Tao Xia", "abstract": "  Recently, image-to-3D approaches have significantly advanced the generation\nquality and speed of 3D assets based on large reconstruction models,\nparticularly 3D Gaussian reconstruction models. Existing large 3D Gaussian\nmodels directly map 2D image to 3D Gaussian parameters, while regressing 2D\nimage to 3D Gaussian representations is challenging without 3D priors. In this\npaper, we propose a large Point-to-Gaussian model, that inputs the initial\npoint cloud produced from large 3D diffusion model conditional on 2D image to\ngenerate the Gaussian parameters, for image-to-3D generation. The point cloud\nprovides initial 3D geometry prior for Gaussian generation, thus significantly\nfacilitating image-to-3D Generation. Moreover, we present the\n\\textbf{A}ttention mechanism, \\textbf{P}rojection mechanism, and \\textbf{P}oint\nfeature extractor, dubbed as \\textbf{APP} block, for fusing the image features\nwith point cloud features. The qualitative and quantitative experiments\nextensively demonstrate the effectiveness of the proposed approach on GSO and\nObjaverse datasets, and show the proposed method achieves state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2408.10935v1", "date": "2024-08-20", "relevancy": 3.0867, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6233}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6143}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Point-to-Gaussian%20Model%20for%20Image-to-3D%20Generation&body=Title%3A%20Large%20Point-to-Gaussian%20Model%20for%20Image-to-3D%20Generation%0AAuthor%3A%20Longfei%20Lu%20and%20Huachen%20Gao%20and%20Tao%20Dai%20and%20Yaohua%20Zha%20and%20Zhi%20Hou%20and%20Junta%20Wu%20and%20Shu-Tao%20Xia%0AAbstract%3A%20%20%20Recently%2C%20image-to-3D%20approaches%20have%20significantly%20advanced%20the%20generation%0Aquality%20and%20speed%20of%203D%20assets%20based%20on%20large%20reconstruction%20models%2C%0Aparticularly%203D%20Gaussian%20reconstruction%20models.%20Existing%20large%203D%20Gaussian%0Amodels%20directly%20map%202D%20image%20to%203D%20Gaussian%20parameters%2C%20while%20regressing%202D%0Aimage%20to%203D%20Gaussian%20representations%20is%20challenging%20without%203D%20priors.%20In%20this%0Apaper%2C%20we%20propose%20a%20large%20Point-to-Gaussian%20model%2C%20that%20inputs%20the%20initial%0Apoint%20cloud%20produced%20from%20large%203D%20diffusion%20model%20conditional%20on%202D%20image%20to%0Agenerate%20the%20Gaussian%20parameters%2C%20for%20image-to-3D%20generation.%20The%20point%20cloud%0Aprovides%20initial%203D%20geometry%20prior%20for%20Gaussian%20generation%2C%20thus%20significantly%0Afacilitating%20image-to-3D%20Generation.%20Moreover%2C%20we%20present%20the%0A%5Ctextbf%7BA%7Dttention%20mechanism%2C%20%5Ctextbf%7BP%7Drojection%20mechanism%2C%20and%20%5Ctextbf%7BP%7Doint%0Afeature%20extractor%2C%20dubbed%20as%20%5Ctextbf%7BAPP%7D%20block%2C%20for%20fusing%20the%20image%20features%0Awith%20point%20cloud%20features.%20The%20qualitative%20and%20quantitative%20experiments%0Aextensively%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20on%20GSO%20and%0AObjaverse%20datasets%2C%20and%20show%20the%20proposed%20method%20achieves%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Point-to-Gaussian%2520Model%2520for%2520Image-to-3D%2520Generation%26entry.906535625%3DLongfei%2520Lu%2520and%2520Huachen%2520Gao%2520and%2520Tao%2520Dai%2520and%2520Yaohua%2520Zha%2520and%2520Zhi%2520Hou%2520and%2520Junta%2520Wu%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3D%2520%2520Recently%252C%2520image-to-3D%2520approaches%2520have%2520significantly%2520advanced%2520the%2520generation%250Aquality%2520and%2520speed%2520of%25203D%2520assets%2520based%2520on%2520large%2520reconstruction%2520models%252C%250Aparticularly%25203D%2520Gaussian%2520reconstruction%2520models.%2520Existing%2520large%25203D%2520Gaussian%250Amodels%2520directly%2520map%25202D%2520image%2520to%25203D%2520Gaussian%2520parameters%252C%2520while%2520regressing%25202D%250Aimage%2520to%25203D%2520Gaussian%2520representations%2520is%2520challenging%2520without%25203D%2520priors.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520large%2520Point-to-Gaussian%2520model%252C%2520that%2520inputs%2520the%2520initial%250Apoint%2520cloud%2520produced%2520from%2520large%25203D%2520diffusion%2520model%2520conditional%2520on%25202D%2520image%2520to%250Agenerate%2520the%2520Gaussian%2520parameters%252C%2520for%2520image-to-3D%2520generation.%2520The%2520point%2520cloud%250Aprovides%2520initial%25203D%2520geometry%2520prior%2520for%2520Gaussian%2520generation%252C%2520thus%2520significantly%250Afacilitating%2520image-to-3D%2520Generation.%2520Moreover%252C%2520we%2520present%2520the%250A%255Ctextbf%257BA%257Dttention%2520mechanism%252C%2520%255Ctextbf%257BP%257Drojection%2520mechanism%252C%2520and%2520%255Ctextbf%257BP%257Doint%250Afeature%2520extractor%252C%2520dubbed%2520as%2520%255Ctextbf%257BAPP%257D%2520block%252C%2520for%2520fusing%2520the%2520image%2520features%250Awith%2520point%2520cloud%2520features.%2520The%2520qualitative%2520and%2520quantitative%2520experiments%250Aextensively%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520on%2520GSO%2520and%250AObjaverse%2520datasets%252C%2520and%2520show%2520the%2520proposed%2520method%2520achieves%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Point-to-Gaussian%20Model%20for%20Image-to-3D%20Generation&entry.906535625=Longfei%20Lu%20and%20Huachen%20Gao%20and%20Tao%20Dai%20and%20Yaohua%20Zha%20and%20Zhi%20Hou%20and%20Junta%20Wu%20and%20Shu-Tao%20Xia&entry.1292438233=%20%20Recently%2C%20image-to-3D%20approaches%20have%20significantly%20advanced%20the%20generation%0Aquality%20and%20speed%20of%203D%20assets%20based%20on%20large%20reconstruction%20models%2C%0Aparticularly%203D%20Gaussian%20reconstruction%20models.%20Existing%20large%203D%20Gaussian%0Amodels%20directly%20map%202D%20image%20to%203D%20Gaussian%20parameters%2C%20while%20regressing%202D%0Aimage%20to%203D%20Gaussian%20representations%20is%20challenging%20without%203D%20priors.%20In%20this%0Apaper%2C%20we%20propose%20a%20large%20Point-to-Gaussian%20model%2C%20that%20inputs%20the%20initial%0Apoint%20cloud%20produced%20from%20large%203D%20diffusion%20model%20conditional%20on%202D%20image%20to%0Agenerate%20the%20Gaussian%20parameters%2C%20for%20image-to-3D%20generation.%20The%20point%20cloud%0Aprovides%20initial%203D%20geometry%20prior%20for%20Gaussian%20generation%2C%20thus%20significantly%0Afacilitating%20image-to-3D%20Generation.%20Moreover%2C%20we%20present%20the%0A%5Ctextbf%7BA%7Dttention%20mechanism%2C%20%5Ctextbf%7BP%7Drojection%20mechanism%2C%20and%20%5Ctextbf%7BP%7Doint%0Afeature%20extractor%2C%20dubbed%20as%20%5Ctextbf%7BAPP%7D%20block%2C%20for%20fusing%20the%20image%20features%0Awith%20point%20cloud%20features.%20The%20qualitative%20and%20quantitative%20experiments%0Aextensively%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20on%20GSO%20and%0AObjaverse%20datasets%2C%20and%20show%20the%20proposed%20method%20achieves%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10935v1&entry.124074799=Read"},
{"title": "Learning Part-aware 3D Representations by Fusing 2D Gaussians and\n  Superquadrics", "author": "Zhirui Gao and Renjiao Yi and Yuhang Huang and Wei Chen and Chenyang Zhu and Kai Xu", "abstract": "  Low-level 3D representations, such as point clouds, meshes, NeRFs, and 3D\nGaussians, are commonly used to represent 3D objects or scenes. However, humans\nusually perceive 3D objects or scenes at a higher level as a composition of\nparts or structures rather than points or voxels. Representing 3D as semantic\nparts can benefit further understanding and applications. We aim to solve\npart-aware 3D reconstruction, which parses objects or scenes into semantic\nparts. In this paper, we introduce a hybrid representation of superquadrics and\n2D Gaussians, trying to dig 3D structural clues from multi-view image inputs.\nAccurate structured geometry reconstruction and high-quality rendering are\nachieved at the same time. We incorporate parametric superquadrics in mesh\nforms into 2D Gaussians by attaching Gaussian centers to faces in meshes.\nDuring the training, superquadrics parameters are iteratively optimized, and\nGaussians are deformed accordingly, resulting in an efficient hybrid\nrepresentation. On the one hand, this hybrid representation inherits the\nadvantage of superquadrics to represent different shape primitives, supporting\nflexible part decomposition of scenes. On the other hand, 2D Gaussians are\nincorporated to model the complex texture and geometry details, ensuring\nhigh-quality rendering and geometry reconstruction. The reconstruction is fully\nunsupervised. We conduct extensive experiments on data from DTU and ShapeNet\ndatasets, in which the method decomposes scenes into reasonable parts,\noutperforming existing state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2408.10789v1", "date": "2024-08-20", "relevancy": 3.0664, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.647}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6174}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Part-aware%203D%20Representations%20by%20Fusing%202D%20Gaussians%20and%0A%20%20Superquadrics&body=Title%3A%20Learning%20Part-aware%203D%20Representations%20by%20Fusing%202D%20Gaussians%20and%0A%20%20Superquadrics%0AAuthor%3A%20Zhirui%20Gao%20and%20Renjiao%20Yi%20and%20Yuhang%20Huang%20and%20Wei%20Chen%20and%20Chenyang%20Zhu%20and%20Kai%20Xu%0AAbstract%3A%20%20%20Low-level%203D%20representations%2C%20such%20as%20point%20clouds%2C%20meshes%2C%20NeRFs%2C%20and%203D%0AGaussians%2C%20are%20commonly%20used%20to%20represent%203D%20objects%20or%20scenes.%20However%2C%20humans%0Ausually%20perceive%203D%20objects%20or%20scenes%20at%20a%20higher%20level%20as%20a%20composition%20of%0Aparts%20or%20structures%20rather%20than%20points%20or%20voxels.%20Representing%203D%20as%20semantic%0Aparts%20can%20benefit%20further%20understanding%20and%20applications.%20We%20aim%20to%20solve%0Apart-aware%203D%20reconstruction%2C%20which%20parses%20objects%20or%20scenes%20into%20semantic%0Aparts.%20In%20this%20paper%2C%20we%20introduce%20a%20hybrid%20representation%20of%20superquadrics%20and%0A2D%20Gaussians%2C%20trying%20to%20dig%203D%20structural%20clues%20from%20multi-view%20image%20inputs.%0AAccurate%20structured%20geometry%20reconstruction%20and%20high-quality%20rendering%20are%0Aachieved%20at%20the%20same%20time.%20We%20incorporate%20parametric%20superquadrics%20in%20mesh%0Aforms%20into%202D%20Gaussians%20by%20attaching%20Gaussian%20centers%20to%20faces%20in%20meshes.%0ADuring%20the%20training%2C%20superquadrics%20parameters%20are%20iteratively%20optimized%2C%20and%0AGaussians%20are%20deformed%20accordingly%2C%20resulting%20in%20an%20efficient%20hybrid%0Arepresentation.%20On%20the%20one%20hand%2C%20this%20hybrid%20representation%20inherits%20the%0Aadvantage%20of%20superquadrics%20to%20represent%20different%20shape%20primitives%2C%20supporting%0Aflexible%20part%20decomposition%20of%20scenes.%20On%20the%20other%20hand%2C%202D%20Gaussians%20are%0Aincorporated%20to%20model%20the%20complex%20texture%20and%20geometry%20details%2C%20ensuring%0Ahigh-quality%20rendering%20and%20geometry%20reconstruction.%20The%20reconstruction%20is%20fully%0Aunsupervised.%20We%20conduct%20extensive%20experiments%20on%20data%20from%20DTU%20and%20ShapeNet%0Adatasets%2C%20in%20which%20the%20method%20decomposes%20scenes%20into%20reasonable%20parts%2C%0Aoutperforming%20existing%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Part-aware%25203D%2520Representations%2520by%2520Fusing%25202D%2520Gaussians%2520and%250A%2520%2520Superquadrics%26entry.906535625%3DZhirui%2520Gao%2520and%2520Renjiao%2520Yi%2520and%2520Yuhang%2520Huang%2520and%2520Wei%2520Chen%2520and%2520Chenyang%2520Zhu%2520and%2520Kai%2520Xu%26entry.1292438233%3D%2520%2520Low-level%25203D%2520representations%252C%2520such%2520as%2520point%2520clouds%252C%2520meshes%252C%2520NeRFs%252C%2520and%25203D%250AGaussians%252C%2520are%2520commonly%2520used%2520to%2520represent%25203D%2520objects%2520or%2520scenes.%2520However%252C%2520humans%250Ausually%2520perceive%25203D%2520objects%2520or%2520scenes%2520at%2520a%2520higher%2520level%2520as%2520a%2520composition%2520of%250Aparts%2520or%2520structures%2520rather%2520than%2520points%2520or%2520voxels.%2520Representing%25203D%2520as%2520semantic%250Aparts%2520can%2520benefit%2520further%2520understanding%2520and%2520applications.%2520We%2520aim%2520to%2520solve%250Apart-aware%25203D%2520reconstruction%252C%2520which%2520parses%2520objects%2520or%2520scenes%2520into%2520semantic%250Aparts.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520hybrid%2520representation%2520of%2520superquadrics%2520and%250A2D%2520Gaussians%252C%2520trying%2520to%2520dig%25203D%2520structural%2520clues%2520from%2520multi-view%2520image%2520inputs.%250AAccurate%2520structured%2520geometry%2520reconstruction%2520and%2520high-quality%2520rendering%2520are%250Aachieved%2520at%2520the%2520same%2520time.%2520We%2520incorporate%2520parametric%2520superquadrics%2520in%2520mesh%250Aforms%2520into%25202D%2520Gaussians%2520by%2520attaching%2520Gaussian%2520centers%2520to%2520faces%2520in%2520meshes.%250ADuring%2520the%2520training%252C%2520superquadrics%2520parameters%2520are%2520iteratively%2520optimized%252C%2520and%250AGaussians%2520are%2520deformed%2520accordingly%252C%2520resulting%2520in%2520an%2520efficient%2520hybrid%250Arepresentation.%2520On%2520the%2520one%2520hand%252C%2520this%2520hybrid%2520representation%2520inherits%2520the%250Aadvantage%2520of%2520superquadrics%2520to%2520represent%2520different%2520shape%2520primitives%252C%2520supporting%250Aflexible%2520part%2520decomposition%2520of%2520scenes.%2520On%2520the%2520other%2520hand%252C%25202D%2520Gaussians%2520are%250Aincorporated%2520to%2520model%2520the%2520complex%2520texture%2520and%2520geometry%2520details%252C%2520ensuring%250Ahigh-quality%2520rendering%2520and%2520geometry%2520reconstruction.%2520The%2520reconstruction%2520is%2520fully%250Aunsupervised.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520data%2520from%2520DTU%2520and%2520ShapeNet%250Adatasets%252C%2520in%2520which%2520the%2520method%2520decomposes%2520scenes%2520into%2520reasonable%2520parts%252C%250Aoutperforming%2520existing%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Part-aware%203D%20Representations%20by%20Fusing%202D%20Gaussians%20and%0A%20%20Superquadrics&entry.906535625=Zhirui%20Gao%20and%20Renjiao%20Yi%20and%20Yuhang%20Huang%20and%20Wei%20Chen%20and%20Chenyang%20Zhu%20and%20Kai%20Xu&entry.1292438233=%20%20Low-level%203D%20representations%2C%20such%20as%20point%20clouds%2C%20meshes%2C%20NeRFs%2C%20and%203D%0AGaussians%2C%20are%20commonly%20used%20to%20represent%203D%20objects%20or%20scenes.%20However%2C%20humans%0Ausually%20perceive%203D%20objects%20or%20scenes%20at%20a%20higher%20level%20as%20a%20composition%20of%0Aparts%20or%20structures%20rather%20than%20points%20or%20voxels.%20Representing%203D%20as%20semantic%0Aparts%20can%20benefit%20further%20understanding%20and%20applications.%20We%20aim%20to%20solve%0Apart-aware%203D%20reconstruction%2C%20which%20parses%20objects%20or%20scenes%20into%20semantic%0Aparts.%20In%20this%20paper%2C%20we%20introduce%20a%20hybrid%20representation%20of%20superquadrics%20and%0A2D%20Gaussians%2C%20trying%20to%20dig%203D%20structural%20clues%20from%20multi-view%20image%20inputs.%0AAccurate%20structured%20geometry%20reconstruction%20and%20high-quality%20rendering%20are%0Aachieved%20at%20the%20same%20time.%20We%20incorporate%20parametric%20superquadrics%20in%20mesh%0Aforms%20into%202D%20Gaussians%20by%20attaching%20Gaussian%20centers%20to%20faces%20in%20meshes.%0ADuring%20the%20training%2C%20superquadrics%20parameters%20are%20iteratively%20optimized%2C%20and%0AGaussians%20are%20deformed%20accordingly%2C%20resulting%20in%20an%20efficient%20hybrid%0Arepresentation.%20On%20the%20one%20hand%2C%20this%20hybrid%20representation%20inherits%20the%0Aadvantage%20of%20superquadrics%20to%20represent%20different%20shape%20primitives%2C%20supporting%0Aflexible%20part%20decomposition%20of%20scenes.%20On%20the%20other%20hand%2C%202D%20Gaussians%20are%0Aincorporated%20to%20model%20the%20complex%20texture%20and%20geometry%20details%2C%20ensuring%0Ahigh-quality%20rendering%20and%20geometry%20reconstruction.%20The%20reconstruction%20is%20fully%0Aunsupervised.%20We%20conduct%20extensive%20experiments%20on%20data%20from%20DTU%20and%20ShapeNet%0Adatasets%2C%20in%20which%20the%20method%20decomposes%20scenes%20into%20reasonable%20parts%2C%0Aoutperforming%20existing%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10789v1&entry.124074799=Read"},
{"title": "ShapeSplat: A Large-scale Dataset of Gaussian Splats and Their\n  Self-Supervised Pretraining", "author": "Qi Ma and Yue Li and Bin Ren and Nicu Sebe and Ender Konukoglu and Theo Gevers and Luc Van Gool and Danda Pani Paudel", "abstract": "  3D Gaussian Splatting (3DGS) has become the de facto method of 3D\nrepresentation in many vision tasks. This calls for the 3D understanding\ndirectly in this representation space. To facilitate the research in this\ndirection, we first build a large-scale dataset of 3DGS using the commonly used\nShapeNet and ModelNet datasets. Our dataset ShapeSplat consists of 65K objects\nfrom 87 unique categories, whose labels are in accordance with the respective\ndatasets. The creation of this dataset utilized the compute equivalent of 2 GPU\nyears on a TITAN XP GPU.\n  We utilize our dataset for unsupervised pretraining and supervised finetuning\nfor classification and segmentation tasks. To this end, we introduce\n\\textbf{\\textit{Gaussian-MAE}}, which highlights the unique benefits of\nrepresentation learning from Gaussian parameters. Through exhaustive\nexperiments, we provide several valuable insights. In particular, we show that\n(1) the distribution of the optimized GS centroids significantly differs from\nthe uniformly sampled point cloud (used for initialization) counterpart; (2)\nthis change in distribution results in degradation in classification but\nimprovement in segmentation tasks when using only the centroids; (3) to\nleverage additional Gaussian parameters, we propose Gaussian feature grouping\nin a normalized feature space, along with splats pooling layer, offering a\ntailored solution to effectively group and embed similar Gaussians, which leads\nto notable improvement in finetuning tasks.\n", "link": "http://arxiv.org/abs/2408.10906v1", "date": "2024-08-20", "relevancy": 3.0048, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6463}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6032}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShapeSplat%3A%20A%20Large-scale%20Dataset%20of%20Gaussian%20Splats%20and%20Their%0A%20%20Self-Supervised%20Pretraining&body=Title%3A%20ShapeSplat%3A%20A%20Large-scale%20Dataset%20of%20Gaussian%20Splats%20and%20Their%0A%20%20Self-Supervised%20Pretraining%0AAuthor%3A%20Qi%20Ma%20and%20Yue%20Li%20and%20Bin%20Ren%20and%20Nicu%20Sebe%20and%20Ender%20Konukoglu%20and%20Theo%20Gevers%20and%20Luc%20Van%20Gool%20and%20Danda%20Pani%20Paudel%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20become%20the%20de%20facto%20method%20of%203D%0Arepresentation%20in%20many%20vision%20tasks.%20This%20calls%20for%20the%203D%20understanding%0Adirectly%20in%20this%20representation%20space.%20To%20facilitate%20the%20research%20in%20this%0Adirection%2C%20we%20first%20build%20a%20large-scale%20dataset%20of%203DGS%20using%20the%20commonly%20used%0AShapeNet%20and%20ModelNet%20datasets.%20Our%20dataset%20ShapeSplat%20consists%20of%2065K%20objects%0Afrom%2087%20unique%20categories%2C%20whose%20labels%20are%20in%20accordance%20with%20the%20respective%0Adatasets.%20The%20creation%20of%20this%20dataset%20utilized%20the%20compute%20equivalent%20of%202%20GPU%0Ayears%20on%20a%20TITAN%20XP%20GPU.%0A%20%20We%20utilize%20our%20dataset%20for%20unsupervised%20pretraining%20and%20supervised%20finetuning%0Afor%20classification%20and%20segmentation%20tasks.%20To%20this%20end%2C%20we%20introduce%0A%5Ctextbf%7B%5Ctextit%7BGaussian-MAE%7D%7D%2C%20which%20highlights%20the%20unique%20benefits%20of%0Arepresentation%20learning%20from%20Gaussian%20parameters.%20Through%20exhaustive%0Aexperiments%2C%20we%20provide%20several%20valuable%20insights.%20In%20particular%2C%20we%20show%20that%0A%281%29%20the%20distribution%20of%20the%20optimized%20GS%20centroids%20significantly%20differs%20from%0Athe%20uniformly%20sampled%20point%20cloud%20%28used%20for%20initialization%29%20counterpart%3B%20%282%29%0Athis%20change%20in%20distribution%20results%20in%20degradation%20in%20classification%20but%0Aimprovement%20in%20segmentation%20tasks%20when%20using%20only%20the%20centroids%3B%20%283%29%20to%0Aleverage%20additional%20Gaussian%20parameters%2C%20we%20propose%20Gaussian%20feature%20grouping%0Ain%20a%20normalized%20feature%20space%2C%20along%20with%20splats%20pooling%20layer%2C%20offering%20a%0Atailored%20solution%20to%20effectively%20group%20and%20embed%20similar%20Gaussians%2C%20which%20leads%0Ato%20notable%20improvement%20in%20finetuning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShapeSplat%253A%2520A%2520Large-scale%2520Dataset%2520of%2520Gaussian%2520Splats%2520and%2520Their%250A%2520%2520Self-Supervised%2520Pretraining%26entry.906535625%3DQi%2520Ma%2520and%2520Yue%2520Li%2520and%2520Bin%2520Ren%2520and%2520Nicu%2520Sebe%2520and%2520Ender%2520Konukoglu%2520and%2520Theo%2520Gevers%2520and%2520Luc%2520Van%2520Gool%2520and%2520Danda%2520Pani%2520Paudel%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520become%2520the%2520de%2520facto%2520method%2520of%25203D%250Arepresentation%2520in%2520many%2520vision%2520tasks.%2520This%2520calls%2520for%2520the%25203D%2520understanding%250Adirectly%2520in%2520this%2520representation%2520space.%2520To%2520facilitate%2520the%2520research%2520in%2520this%250Adirection%252C%2520we%2520first%2520build%2520a%2520large-scale%2520dataset%2520of%25203DGS%2520using%2520the%2520commonly%2520used%250AShapeNet%2520and%2520ModelNet%2520datasets.%2520Our%2520dataset%2520ShapeSplat%2520consists%2520of%252065K%2520objects%250Afrom%252087%2520unique%2520categories%252C%2520whose%2520labels%2520are%2520in%2520accordance%2520with%2520the%2520respective%250Adatasets.%2520The%2520creation%2520of%2520this%2520dataset%2520utilized%2520the%2520compute%2520equivalent%2520of%25202%2520GPU%250Ayears%2520on%2520a%2520TITAN%2520XP%2520GPU.%250A%2520%2520We%2520utilize%2520our%2520dataset%2520for%2520unsupervised%2520pretraining%2520and%2520supervised%2520finetuning%250Afor%2520classification%2520and%2520segmentation%2520tasks.%2520To%2520this%2520end%252C%2520we%2520introduce%250A%255Ctextbf%257B%255Ctextit%257BGaussian-MAE%257D%257D%252C%2520which%2520highlights%2520the%2520unique%2520benefits%2520of%250Arepresentation%2520learning%2520from%2520Gaussian%2520parameters.%2520Through%2520exhaustive%250Aexperiments%252C%2520we%2520provide%2520several%2520valuable%2520insights.%2520In%2520particular%252C%2520we%2520show%2520that%250A%25281%2529%2520the%2520distribution%2520of%2520the%2520optimized%2520GS%2520centroids%2520significantly%2520differs%2520from%250Athe%2520uniformly%2520sampled%2520point%2520cloud%2520%2528used%2520for%2520initialization%2529%2520counterpart%253B%2520%25282%2529%250Athis%2520change%2520in%2520distribution%2520results%2520in%2520degradation%2520in%2520classification%2520but%250Aimprovement%2520in%2520segmentation%2520tasks%2520when%2520using%2520only%2520the%2520centroids%253B%2520%25283%2529%2520to%250Aleverage%2520additional%2520Gaussian%2520parameters%252C%2520we%2520propose%2520Gaussian%2520feature%2520grouping%250Ain%2520a%2520normalized%2520feature%2520space%252C%2520along%2520with%2520splats%2520pooling%2520layer%252C%2520offering%2520a%250Atailored%2520solution%2520to%2520effectively%2520group%2520and%2520embed%2520similar%2520Gaussians%252C%2520which%2520leads%250Ato%2520notable%2520improvement%2520in%2520finetuning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShapeSplat%3A%20A%20Large-scale%20Dataset%20of%20Gaussian%20Splats%20and%20Their%0A%20%20Self-Supervised%20Pretraining&entry.906535625=Qi%20Ma%20and%20Yue%20Li%20and%20Bin%20Ren%20and%20Nicu%20Sebe%20and%20Ender%20Konukoglu%20and%20Theo%20Gevers%20and%20Luc%20Van%20Gool%20and%20Danda%20Pani%20Paudel&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20become%20the%20de%20facto%20method%20of%203D%0Arepresentation%20in%20many%20vision%20tasks.%20This%20calls%20for%20the%203D%20understanding%0Adirectly%20in%20this%20representation%20space.%20To%20facilitate%20the%20research%20in%20this%0Adirection%2C%20we%20first%20build%20a%20large-scale%20dataset%20of%203DGS%20using%20the%20commonly%20used%0AShapeNet%20and%20ModelNet%20datasets.%20Our%20dataset%20ShapeSplat%20consists%20of%2065K%20objects%0Afrom%2087%20unique%20categories%2C%20whose%20labels%20are%20in%20accordance%20with%20the%20respective%0Adatasets.%20The%20creation%20of%20this%20dataset%20utilized%20the%20compute%20equivalent%20of%202%20GPU%0Ayears%20on%20a%20TITAN%20XP%20GPU.%0A%20%20We%20utilize%20our%20dataset%20for%20unsupervised%20pretraining%20and%20supervised%20finetuning%0Afor%20classification%20and%20segmentation%20tasks.%20To%20this%20end%2C%20we%20introduce%0A%5Ctextbf%7B%5Ctextit%7BGaussian-MAE%7D%7D%2C%20which%20highlights%20the%20unique%20benefits%20of%0Arepresentation%20learning%20from%20Gaussian%20parameters.%20Through%20exhaustive%0Aexperiments%2C%20we%20provide%20several%20valuable%20insights.%20In%20particular%2C%20we%20show%20that%0A%281%29%20the%20distribution%20of%20the%20optimized%20GS%20centroids%20significantly%20differs%20from%0Athe%20uniformly%20sampled%20point%20cloud%20%28used%20for%20initialization%29%20counterpart%3B%20%282%29%0Athis%20change%20in%20distribution%20results%20in%20degradation%20in%20classification%20but%0Aimprovement%20in%20segmentation%20tasks%20when%20using%20only%20the%20centroids%3B%20%283%29%20to%0Aleverage%20additional%20Gaussian%20parameters%2C%20we%20propose%20Gaussian%20feature%20grouping%0Ain%20a%20normalized%20feature%20space%2C%20along%20with%20splats%20pooling%20layer%2C%20offering%20a%0Atailored%20solution%20to%20effectively%20group%20and%20embed%20similar%20Gaussians%2C%20which%20leads%0Ato%20notable%20improvement%20in%20finetuning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10906v1&entry.124074799=Read"},
{"title": "PersonViT: Large-scale Self-supervised Vision Transformer for Person\n  Re-Identification", "author": "Bin Hu and Xinggang Wang and Wenyu Liu", "abstract": "  Person Re-Identification (ReID) aims to retrieve relevant individuals in\nnon-overlapping camera images and has a wide range of applications in the field\nof public safety. In recent years, with the development of Vision Transformer\n(ViT) and self-supervised learning techniques, the performance of person ReID\nbased on self-supervised pre-training has been greatly improved. Person ReID\nrequires extracting highly discriminative local fine-grained features of the\nhuman body, while traditional ViT is good at extracting context-related global\nfeatures, making it difficult to focus on local human body features. To this\nend, this article introduces the recently emerged Masked Image Modeling (MIM)\nself-supervised learning method into person ReID, and effectively extracts\nhigh-quality global and local features through large-scale unsupervised\npre-training by combining masked image modeling and discriminative contrastive\nlearning, and then conducts supervised fine-tuning training in the person ReID\ntask. This person feature extraction method based on ViT with masked image\nmodeling (PersonViT) has the good characteristics of unsupervised, scalable,\nand strong generalization capabilities, overcoming the problem of difficult\nannotation in supervised person ReID, and achieves state-of-the-art results on\npublicly available benchmark datasets, including MSMT17, Market1501,\nDukeMTMC-reID, and Occluded-Duke. The code and pre-trained models of the\nPersonViT method are released at \\url{https://github.com/hustvl/PersonViT} to\npromote further research in the person ReID field.\n", "link": "http://arxiv.org/abs/2408.05398v2", "date": "2024-08-20", "relevancy": 2.8528, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6042}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5809}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PersonViT%3A%20Large-scale%20Self-supervised%20Vision%20Transformer%20for%20Person%0A%20%20Re-Identification&body=Title%3A%20PersonViT%3A%20Large-scale%20Self-supervised%20Vision%20Transformer%20for%20Person%0A%20%20Re-Identification%0AAuthor%3A%20Bin%20Hu%20and%20Xinggang%20Wang%20and%20Wenyu%20Liu%0AAbstract%3A%20%20%20Person%20Re-Identification%20%28ReID%29%20aims%20to%20retrieve%20relevant%20individuals%20in%0Anon-overlapping%20camera%20images%20and%20has%20a%20wide%20range%20of%20applications%20in%20the%20field%0Aof%20public%20safety.%20In%20recent%20years%2C%20with%20the%20development%20of%20Vision%20Transformer%0A%28ViT%29%20and%20self-supervised%20learning%20techniques%2C%20the%20performance%20of%20person%20ReID%0Abased%20on%20self-supervised%20pre-training%20has%20been%20greatly%20improved.%20Person%20ReID%0Arequires%20extracting%20highly%20discriminative%20local%20fine-grained%20features%20of%20the%0Ahuman%20body%2C%20while%20traditional%20ViT%20is%20good%20at%20extracting%20context-related%20global%0Afeatures%2C%20making%20it%20difficult%20to%20focus%20on%20local%20human%20body%20features.%20To%20this%0Aend%2C%20this%20article%20introduces%20the%20recently%20emerged%20Masked%20Image%20Modeling%20%28MIM%29%0Aself-supervised%20learning%20method%20into%20person%20ReID%2C%20and%20effectively%20extracts%0Ahigh-quality%20global%20and%20local%20features%20through%20large-scale%20unsupervised%0Apre-training%20by%20combining%20masked%20image%20modeling%20and%20discriminative%20contrastive%0Alearning%2C%20and%20then%20conducts%20supervised%20fine-tuning%20training%20in%20the%20person%20ReID%0Atask.%20This%20person%20feature%20extraction%20method%20based%20on%20ViT%20with%20masked%20image%0Amodeling%20%28PersonViT%29%20has%20the%20good%20characteristics%20of%20unsupervised%2C%20scalable%2C%0Aand%20strong%20generalization%20capabilities%2C%20overcoming%20the%20problem%20of%20difficult%0Aannotation%20in%20supervised%20person%20ReID%2C%20and%20achieves%20state-of-the-art%20results%20on%0Apublicly%20available%20benchmark%20datasets%2C%20including%20MSMT17%2C%20Market1501%2C%0ADukeMTMC-reID%2C%20and%20Occluded-Duke.%20The%20code%20and%20pre-trained%20models%20of%20the%0APersonViT%20method%20are%20released%20at%20%5Curl%7Bhttps%3A//github.com/hustvl/PersonViT%7D%20to%0Apromote%20further%20research%20in%20the%20person%20ReID%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05398v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonViT%253A%2520Large-scale%2520Self-supervised%2520Vision%2520Transformer%2520for%2520Person%250A%2520%2520Re-Identification%26entry.906535625%3DBin%2520Hu%2520and%2520Xinggang%2520Wang%2520and%2520Wenyu%2520Liu%26entry.1292438233%3D%2520%2520Person%2520Re-Identification%2520%2528ReID%2529%2520aims%2520to%2520retrieve%2520relevant%2520individuals%2520in%250Anon-overlapping%2520camera%2520images%2520and%2520has%2520a%2520wide%2520range%2520of%2520applications%2520in%2520the%2520field%250Aof%2520public%2520safety.%2520In%2520recent%2520years%252C%2520with%2520the%2520development%2520of%2520Vision%2520Transformer%250A%2528ViT%2529%2520and%2520self-supervised%2520learning%2520techniques%252C%2520the%2520performance%2520of%2520person%2520ReID%250Abased%2520on%2520self-supervised%2520pre-training%2520has%2520been%2520greatly%2520improved.%2520Person%2520ReID%250Arequires%2520extracting%2520highly%2520discriminative%2520local%2520fine-grained%2520features%2520of%2520the%250Ahuman%2520body%252C%2520while%2520traditional%2520ViT%2520is%2520good%2520at%2520extracting%2520context-related%2520global%250Afeatures%252C%2520making%2520it%2520difficult%2520to%2520focus%2520on%2520local%2520human%2520body%2520features.%2520To%2520this%250Aend%252C%2520this%2520article%2520introduces%2520the%2520recently%2520emerged%2520Masked%2520Image%2520Modeling%2520%2528MIM%2529%250Aself-supervised%2520learning%2520method%2520into%2520person%2520ReID%252C%2520and%2520effectively%2520extracts%250Ahigh-quality%2520global%2520and%2520local%2520features%2520through%2520large-scale%2520unsupervised%250Apre-training%2520by%2520combining%2520masked%2520image%2520modeling%2520and%2520discriminative%2520contrastive%250Alearning%252C%2520and%2520then%2520conducts%2520supervised%2520fine-tuning%2520training%2520in%2520the%2520person%2520ReID%250Atask.%2520This%2520person%2520feature%2520extraction%2520method%2520based%2520on%2520ViT%2520with%2520masked%2520image%250Amodeling%2520%2528PersonViT%2529%2520has%2520the%2520good%2520characteristics%2520of%2520unsupervised%252C%2520scalable%252C%250Aand%2520strong%2520generalization%2520capabilities%252C%2520overcoming%2520the%2520problem%2520of%2520difficult%250Aannotation%2520in%2520supervised%2520person%2520ReID%252C%2520and%2520achieves%2520state-of-the-art%2520results%2520on%250Apublicly%2520available%2520benchmark%2520datasets%252C%2520including%2520MSMT17%252C%2520Market1501%252C%250ADukeMTMC-reID%252C%2520and%2520Occluded-Duke.%2520The%2520code%2520and%2520pre-trained%2520models%2520of%2520the%250APersonViT%2520method%2520are%2520released%2520at%2520%255Curl%257Bhttps%253A//github.com/hustvl/PersonViT%257D%2520to%250Apromote%2520further%2520research%2520in%2520the%2520person%2520ReID%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05398v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PersonViT%3A%20Large-scale%20Self-supervised%20Vision%20Transformer%20for%20Person%0A%20%20Re-Identification&entry.906535625=Bin%20Hu%20and%20Xinggang%20Wang%20and%20Wenyu%20Liu&entry.1292438233=%20%20Person%20Re-Identification%20%28ReID%29%20aims%20to%20retrieve%20relevant%20individuals%20in%0Anon-overlapping%20camera%20images%20and%20has%20a%20wide%20range%20of%20applications%20in%20the%20field%0Aof%20public%20safety.%20In%20recent%20years%2C%20with%20the%20development%20of%20Vision%20Transformer%0A%28ViT%29%20and%20self-supervised%20learning%20techniques%2C%20the%20performance%20of%20person%20ReID%0Abased%20on%20self-supervised%20pre-training%20has%20been%20greatly%20improved.%20Person%20ReID%0Arequires%20extracting%20highly%20discriminative%20local%20fine-grained%20features%20of%20the%0Ahuman%20body%2C%20while%20traditional%20ViT%20is%20good%20at%20extracting%20context-related%20global%0Afeatures%2C%20making%20it%20difficult%20to%20focus%20on%20local%20human%20body%20features.%20To%20this%0Aend%2C%20this%20article%20introduces%20the%20recently%20emerged%20Masked%20Image%20Modeling%20%28MIM%29%0Aself-supervised%20learning%20method%20into%20person%20ReID%2C%20and%20effectively%20extracts%0Ahigh-quality%20global%20and%20local%20features%20through%20large-scale%20unsupervised%0Apre-training%20by%20combining%20masked%20image%20modeling%20and%20discriminative%20contrastive%0Alearning%2C%20and%20then%20conducts%20supervised%20fine-tuning%20training%20in%20the%20person%20ReID%0Atask.%20This%20person%20feature%20extraction%20method%20based%20on%20ViT%20with%20masked%20image%0Amodeling%20%28PersonViT%29%20has%20the%20good%20characteristics%20of%20unsupervised%2C%20scalable%2C%0Aand%20strong%20generalization%20capabilities%2C%20overcoming%20the%20problem%20of%20difficult%0Aannotation%20in%20supervised%20person%20ReID%2C%20and%20achieves%20state-of-the-art%20results%20on%0Apublicly%20available%20benchmark%20datasets%2C%20including%20MSMT17%2C%20Market1501%2C%0ADukeMTMC-reID%2C%20and%20Occluded-Duke.%20The%20code%20and%20pre-trained%20models%20of%20the%0APersonViT%20method%20are%20released%20at%20%5Curl%7Bhttps%3A//github.com/hustvl/PersonViT%7D%20to%0Apromote%20further%20research%20in%20the%20person%20ReID%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05398v2&entry.124074799=Read"},
{"title": "OpenScan: A Benchmark for Generalized Open-Vocabulary 3D Scene\n  Understanding", "author": "Youjun Zhao and Jiaying Lin and Shuquan Ye and Qianshi Pang and Rynson W. H. Lau", "abstract": "  Open-vocabulary 3D scene understanding (OV-3D) aims to localize and classify\nnovel objects beyond the closed object classes. However, existing approaches\nand benchmarks primarily focus on the open vocabulary problem within the\ncontext of object classes, which is insufficient to provide a holistic\nevaluation to what extent a model understands the 3D scene. In this paper, we\nintroduce a more challenging task called Generalized Open-Vocabulary 3D Scene\nUnderstanding (GOV-3D) to explore the open vocabulary problem beyond object\nclasses. It encompasses an open and diverse set of generalized knowledge,\nexpressed as linguistic queries of fine-grained and object-specific attributes.\nTo this end, we contribute a new benchmark named OpenScan, which consists of 3D\nobject attributes across eight representative linguistic aspects, including\naffordance, property, material, and more. We further evaluate state-of-the-art\nOV-3D methods on our OpenScan benchmark, and discover that these methods\nstruggle to comprehend the abstract vocabularies of the GOV-3D task, a\nchallenge that cannot be addressed by simply scaling up object classes during\ntraining. We highlight the limitations of existing methodologies and explore a\npromising direction to overcome the identified shortcomings. Data and code are\navailable at https://github.com/YoujunZhao/OpenScan\n", "link": "http://arxiv.org/abs/2408.11030v1", "date": "2024-08-20", "relevancy": 2.7892, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5602}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5602}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenScan%3A%20A%20Benchmark%20for%20Generalized%20Open-Vocabulary%203D%20Scene%0A%20%20Understanding&body=Title%3A%20OpenScan%3A%20A%20Benchmark%20for%20Generalized%20Open-Vocabulary%203D%20Scene%0A%20%20Understanding%0AAuthor%3A%20Youjun%20Zhao%20and%20Jiaying%20Lin%20and%20Shuquan%20Ye%20and%20Qianshi%20Pang%20and%20Rynson%20W.%20H.%20Lau%0AAbstract%3A%20%20%20Open-vocabulary%203D%20scene%20understanding%20%28OV-3D%29%20aims%20to%20localize%20and%20classify%0Anovel%20objects%20beyond%20the%20closed%20object%20classes.%20However%2C%20existing%20approaches%0Aand%20benchmarks%20primarily%20focus%20on%20the%20open%20vocabulary%20problem%20within%20the%0Acontext%20of%20object%20classes%2C%20which%20is%20insufficient%20to%20provide%20a%20holistic%0Aevaluation%20to%20what%20extent%20a%20model%20understands%20the%203D%20scene.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20more%20challenging%20task%20called%20Generalized%20Open-Vocabulary%203D%20Scene%0AUnderstanding%20%28GOV-3D%29%20to%20explore%20the%20open%20vocabulary%20problem%20beyond%20object%0Aclasses.%20It%20encompasses%20an%20open%20and%20diverse%20set%20of%20generalized%20knowledge%2C%0Aexpressed%20as%20linguistic%20queries%20of%20fine-grained%20and%20object-specific%20attributes.%0ATo%20this%20end%2C%20we%20contribute%20a%20new%20benchmark%20named%20OpenScan%2C%20which%20consists%20of%203D%0Aobject%20attributes%20across%20eight%20representative%20linguistic%20aspects%2C%20including%0Aaffordance%2C%20property%2C%20material%2C%20and%20more.%20We%20further%20evaluate%20state-of-the-art%0AOV-3D%20methods%20on%20our%20OpenScan%20benchmark%2C%20and%20discover%20that%20these%20methods%0Astruggle%20to%20comprehend%20the%20abstract%20vocabularies%20of%20the%20GOV-3D%20task%2C%20a%0Achallenge%20that%20cannot%20be%20addressed%20by%20simply%20scaling%20up%20object%20classes%20during%0Atraining.%20We%20highlight%20the%20limitations%20of%20existing%20methodologies%20and%20explore%20a%0Apromising%20direction%20to%20overcome%20the%20identified%20shortcomings.%20Data%20and%20code%20are%0Aavailable%20at%20https%3A//github.com/YoujunZhao/OpenScan%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenScan%253A%2520A%2520Benchmark%2520for%2520Generalized%2520Open-Vocabulary%25203D%2520Scene%250A%2520%2520Understanding%26entry.906535625%3DYoujun%2520Zhao%2520and%2520Jiaying%2520Lin%2520and%2520Shuquan%2520Ye%2520and%2520Qianshi%2520Pang%2520and%2520Rynson%2520W.%2520H.%2520Lau%26entry.1292438233%3D%2520%2520Open-vocabulary%25203D%2520scene%2520understanding%2520%2528OV-3D%2529%2520aims%2520to%2520localize%2520and%2520classify%250Anovel%2520objects%2520beyond%2520the%2520closed%2520object%2520classes.%2520However%252C%2520existing%2520approaches%250Aand%2520benchmarks%2520primarily%2520focus%2520on%2520the%2520open%2520vocabulary%2520problem%2520within%2520the%250Acontext%2520of%2520object%2520classes%252C%2520which%2520is%2520insufficient%2520to%2520provide%2520a%2520holistic%250Aevaluation%2520to%2520what%2520extent%2520a%2520model%2520understands%2520the%25203D%2520scene.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520more%2520challenging%2520task%2520called%2520Generalized%2520Open-Vocabulary%25203D%2520Scene%250AUnderstanding%2520%2528GOV-3D%2529%2520to%2520explore%2520the%2520open%2520vocabulary%2520problem%2520beyond%2520object%250Aclasses.%2520It%2520encompasses%2520an%2520open%2520and%2520diverse%2520set%2520of%2520generalized%2520knowledge%252C%250Aexpressed%2520as%2520linguistic%2520queries%2520of%2520fine-grained%2520and%2520object-specific%2520attributes.%250ATo%2520this%2520end%252C%2520we%2520contribute%2520a%2520new%2520benchmark%2520named%2520OpenScan%252C%2520which%2520consists%2520of%25203D%250Aobject%2520attributes%2520across%2520eight%2520representative%2520linguistic%2520aspects%252C%2520including%250Aaffordance%252C%2520property%252C%2520material%252C%2520and%2520more.%2520We%2520further%2520evaluate%2520state-of-the-art%250AOV-3D%2520methods%2520on%2520our%2520OpenScan%2520benchmark%252C%2520and%2520discover%2520that%2520these%2520methods%250Astruggle%2520to%2520comprehend%2520the%2520abstract%2520vocabularies%2520of%2520the%2520GOV-3D%2520task%252C%2520a%250Achallenge%2520that%2520cannot%2520be%2520addressed%2520by%2520simply%2520scaling%2520up%2520object%2520classes%2520during%250Atraining.%2520We%2520highlight%2520the%2520limitations%2520of%2520existing%2520methodologies%2520and%2520explore%2520a%250Apromising%2520direction%2520to%2520overcome%2520the%2520identified%2520shortcomings.%2520Data%2520and%2520code%2520are%250Aavailable%2520at%2520https%253A//github.com/YoujunZhao/OpenScan%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenScan%3A%20A%20Benchmark%20for%20Generalized%20Open-Vocabulary%203D%20Scene%0A%20%20Understanding&entry.906535625=Youjun%20Zhao%20and%20Jiaying%20Lin%20and%20Shuquan%20Ye%20and%20Qianshi%20Pang%20and%20Rynson%20W.%20H.%20Lau&entry.1292438233=%20%20Open-vocabulary%203D%20scene%20understanding%20%28OV-3D%29%20aims%20to%20localize%20and%20classify%0Anovel%20objects%20beyond%20the%20closed%20object%20classes.%20However%2C%20existing%20approaches%0Aand%20benchmarks%20primarily%20focus%20on%20the%20open%20vocabulary%20problem%20within%20the%0Acontext%20of%20object%20classes%2C%20which%20is%20insufficient%20to%20provide%20a%20holistic%0Aevaluation%20to%20what%20extent%20a%20model%20understands%20the%203D%20scene.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20more%20challenging%20task%20called%20Generalized%20Open-Vocabulary%203D%20Scene%0AUnderstanding%20%28GOV-3D%29%20to%20explore%20the%20open%20vocabulary%20problem%20beyond%20object%0Aclasses.%20It%20encompasses%20an%20open%20and%20diverse%20set%20of%20generalized%20knowledge%2C%0Aexpressed%20as%20linguistic%20queries%20of%20fine-grained%20and%20object-specific%20attributes.%0ATo%20this%20end%2C%20we%20contribute%20a%20new%20benchmark%20named%20OpenScan%2C%20which%20consists%20of%203D%0Aobject%20attributes%20across%20eight%20representative%20linguistic%20aspects%2C%20including%0Aaffordance%2C%20property%2C%20material%2C%20and%20more.%20We%20further%20evaluate%20state-of-the-art%0AOV-3D%20methods%20on%20our%20OpenScan%20benchmark%2C%20and%20discover%20that%20these%20methods%0Astruggle%20to%20comprehend%20the%20abstract%20vocabularies%20of%20the%20GOV-3D%20task%2C%20a%0Achallenge%20that%20cannot%20be%20addressed%20by%20simply%20scaling%20up%20object%20classes%20during%0Atraining.%20We%20highlight%20the%20limitations%20of%20existing%20methodologies%20and%20explore%20a%0Apromising%20direction%20to%20overcome%20the%20identified%20shortcomings.%20Data%20and%20code%20are%0Aavailable%20at%20https%3A//github.com/YoujunZhao/OpenScan%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11030v1&entry.124074799=Read"},
{"title": "Low-Quality Image Detection by Hierarchical VAE", "author": "Tomoyasu Nanaumi and Kazuhiko Kawamoto and Hiroshi Kera", "abstract": "  To make an employee roster, photo album, or training dataset of generative\nmodels, one needs to collect high-quality images while dismissing low-quality\nones. This study addresses a new task of unsupervised detection of low-quality\nimages. We propose a method that not only detects low-quality images with\nvarious types of degradation but also provides visual clues of them based on an\nobservation that partial reconstruction by hierarchical variational\nautoencoders fails for low-quality images. The experiments show that our method\noutperforms several unsupervised out-of-distribution detection methods and also\ngives visual clues for low-quality images that help humans recognize them even\nin thumbnail view.\n", "link": "http://arxiv.org/abs/2408.10885v1", "date": "2024-08-20", "relevancy": 2.7519, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5522}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5506}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Quality%20Image%20Detection%20by%20Hierarchical%20VAE&body=Title%3A%20Low-Quality%20Image%20Detection%20by%20Hierarchical%20VAE%0AAuthor%3A%20Tomoyasu%20Nanaumi%20and%20Kazuhiko%20Kawamoto%20and%20Hiroshi%20Kera%0AAbstract%3A%20%20%20To%20make%20an%20employee%20roster%2C%20photo%20album%2C%20or%20training%20dataset%20of%20generative%0Amodels%2C%20one%20needs%20to%20collect%20high-quality%20images%20while%20dismissing%20low-quality%0Aones.%20This%20study%20addresses%20a%20new%20task%20of%20unsupervised%20detection%20of%20low-quality%0Aimages.%20We%20propose%20a%20method%20that%20not%20only%20detects%20low-quality%20images%20with%0Avarious%20types%20of%20degradation%20but%20also%20provides%20visual%20clues%20of%20them%20based%20on%20an%0Aobservation%20that%20partial%20reconstruction%20by%20hierarchical%20variational%0Aautoencoders%20fails%20for%20low-quality%20images.%20The%20experiments%20show%20that%20our%20method%0Aoutperforms%20several%20unsupervised%20out-of-distribution%20detection%20methods%20and%20also%0Agives%20visual%20clues%20for%20low-quality%20images%20that%20help%20humans%20recognize%20them%20even%0Ain%20thumbnail%20view.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Quality%2520Image%2520Detection%2520by%2520Hierarchical%2520VAE%26entry.906535625%3DTomoyasu%2520Nanaumi%2520and%2520Kazuhiko%2520Kawamoto%2520and%2520Hiroshi%2520Kera%26entry.1292438233%3D%2520%2520To%2520make%2520an%2520employee%2520roster%252C%2520photo%2520album%252C%2520or%2520training%2520dataset%2520of%2520generative%250Amodels%252C%2520one%2520needs%2520to%2520collect%2520high-quality%2520images%2520while%2520dismissing%2520low-quality%250Aones.%2520This%2520study%2520addresses%2520a%2520new%2520task%2520of%2520unsupervised%2520detection%2520of%2520low-quality%250Aimages.%2520We%2520propose%2520a%2520method%2520that%2520not%2520only%2520detects%2520low-quality%2520images%2520with%250Avarious%2520types%2520of%2520degradation%2520but%2520also%2520provides%2520visual%2520clues%2520of%2520them%2520based%2520on%2520an%250Aobservation%2520that%2520partial%2520reconstruction%2520by%2520hierarchical%2520variational%250Aautoencoders%2520fails%2520for%2520low-quality%2520images.%2520The%2520experiments%2520show%2520that%2520our%2520method%250Aoutperforms%2520several%2520unsupervised%2520out-of-distribution%2520detection%2520methods%2520and%2520also%250Agives%2520visual%2520clues%2520for%2520low-quality%2520images%2520that%2520help%2520humans%2520recognize%2520them%2520even%250Ain%2520thumbnail%2520view.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Quality%20Image%20Detection%20by%20Hierarchical%20VAE&entry.906535625=Tomoyasu%20Nanaumi%20and%20Kazuhiko%20Kawamoto%20and%20Hiroshi%20Kera&entry.1292438233=%20%20To%20make%20an%20employee%20roster%2C%20photo%20album%2C%20or%20training%20dataset%20of%20generative%0Amodels%2C%20one%20needs%20to%20collect%20high-quality%20images%20while%20dismissing%20low-quality%0Aones.%20This%20study%20addresses%20a%20new%20task%20of%20unsupervised%20detection%20of%20low-quality%0Aimages.%20We%20propose%20a%20method%20that%20not%20only%20detects%20low-quality%20images%20with%0Avarious%20types%20of%20degradation%20but%20also%20provides%20visual%20clues%20of%20them%20based%20on%20an%0Aobservation%20that%20partial%20reconstruction%20by%20hierarchical%20variational%0Aautoencoders%20fails%20for%20low-quality%20images.%20The%20experiments%20show%20that%20our%20method%0Aoutperforms%20several%20unsupervised%20out-of-distribution%20detection%20methods%20and%20also%0Agives%20visual%20clues%20for%20low-quality%20images%20that%20help%20humans%20recognize%20them%20even%0Ain%20thumbnail%20view.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10885v1&entry.124074799=Read"},
{"title": "NeCo: Improving DINOv2's spatial representations in 19 GPU hours with\n  Patch Neighbor Consistency", "author": "Valentinos Pariza and Mohammadreza Salehi and Gertjan Burghouts and Francesco Locatello and Yuki M. Asano", "abstract": "  We propose sorting patch representations across views as a novel\nself-supervised learning signal to improve pretrained representations. To this\nend, we introduce NeCo: Patch Neighbor Consistency, a novel training loss that\nenforces patch-level nearest neighbor consistency across a student and teacher\nmodel, relative to reference batches. Our method leverages a differentiable\nsorting method applied on top of pretrained representations, such as\nDINOv2-registers to bootstrap the learning signal and further improve upon\nthem. This dense post-pretraining leads to superior performance across various\nmodels and datasets, despite requiring only 19 hours on a single GPU. We\ndemonstrate that this method generates high-quality dense feature encoders and\nestablish several new state-of-the-art results: +5.5% and + 6% for\nnon-parametric in-context semantic segmentation on ADE20k and Pascal VOC, and\n+7.2% and +5.7% for linear segmentation evaluations on COCO-Things and -Stuff.\n", "link": "http://arxiv.org/abs/2408.11054v1", "date": "2024-08-20", "relevancy": 2.6826, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5587}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.529}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeCo%3A%20Improving%20DINOv2%27s%20spatial%20representations%20in%2019%20GPU%20hours%20with%0A%20%20Patch%20Neighbor%20Consistency&body=Title%3A%20NeCo%3A%20Improving%20DINOv2%27s%20spatial%20representations%20in%2019%20GPU%20hours%20with%0A%20%20Patch%20Neighbor%20Consistency%0AAuthor%3A%20Valentinos%20Pariza%20and%20Mohammadreza%20Salehi%20and%20Gertjan%20Burghouts%20and%20Francesco%20Locatello%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20%20%20We%20propose%20sorting%20patch%20representations%20across%20views%20as%20a%20novel%0Aself-supervised%20learning%20signal%20to%20improve%20pretrained%20representations.%20To%20this%0Aend%2C%20we%20introduce%20NeCo%3A%20Patch%20Neighbor%20Consistency%2C%20a%20novel%20training%20loss%20that%0Aenforces%20patch-level%20nearest%20neighbor%20consistency%20across%20a%20student%20and%20teacher%0Amodel%2C%20relative%20to%20reference%20batches.%20Our%20method%20leverages%20a%20differentiable%0Asorting%20method%20applied%20on%20top%20of%20pretrained%20representations%2C%20such%20as%0ADINOv2-registers%20to%20bootstrap%20the%20learning%20signal%20and%20further%20improve%20upon%0Athem.%20This%20dense%20post-pretraining%20leads%20to%20superior%20performance%20across%20various%0Amodels%20and%20datasets%2C%20despite%20requiring%20only%2019%20hours%20on%20a%20single%20GPU.%20We%0Ademonstrate%20that%20this%20method%20generates%20high-quality%20dense%20feature%20encoders%20and%0Aestablish%20several%20new%20state-of-the-art%20results%3A%20%2B5.5%25%20and%20%2B%206%25%20for%0Anon-parametric%20in-context%20semantic%20segmentation%20on%20ADE20k%20and%20Pascal%20VOC%2C%20and%0A%2B7.2%25%20and%20%2B5.7%25%20for%20linear%20segmentation%20evaluations%20on%20COCO-Things%20and%20-Stuff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeCo%253A%2520Improving%2520DINOv2%2527s%2520spatial%2520representations%2520in%252019%2520GPU%2520hours%2520with%250A%2520%2520Patch%2520Neighbor%2520Consistency%26entry.906535625%3DValentinos%2520Pariza%2520and%2520Mohammadreza%2520Salehi%2520and%2520Gertjan%2520Burghouts%2520and%2520Francesco%2520Locatello%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3D%2520%2520We%2520propose%2520sorting%2520patch%2520representations%2520across%2520views%2520as%2520a%2520novel%250Aself-supervised%2520learning%2520signal%2520to%2520improve%2520pretrained%2520representations.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520NeCo%253A%2520Patch%2520Neighbor%2520Consistency%252C%2520a%2520novel%2520training%2520loss%2520that%250Aenforces%2520patch-level%2520nearest%2520neighbor%2520consistency%2520across%2520a%2520student%2520and%2520teacher%250Amodel%252C%2520relative%2520to%2520reference%2520batches.%2520Our%2520method%2520leverages%2520a%2520differentiable%250Asorting%2520method%2520applied%2520on%2520top%2520of%2520pretrained%2520representations%252C%2520such%2520as%250ADINOv2-registers%2520to%2520bootstrap%2520the%2520learning%2520signal%2520and%2520further%2520improve%2520upon%250Athem.%2520This%2520dense%2520post-pretraining%2520leads%2520to%2520superior%2520performance%2520across%2520various%250Amodels%2520and%2520datasets%252C%2520despite%2520requiring%2520only%252019%2520hours%2520on%2520a%2520single%2520GPU.%2520We%250Ademonstrate%2520that%2520this%2520method%2520generates%2520high-quality%2520dense%2520feature%2520encoders%2520and%250Aestablish%2520several%2520new%2520state-of-the-art%2520results%253A%2520%252B5.5%2525%2520and%2520%252B%25206%2525%2520for%250Anon-parametric%2520in-context%2520semantic%2520segmentation%2520on%2520ADE20k%2520and%2520Pascal%2520VOC%252C%2520and%250A%252B7.2%2525%2520and%2520%252B5.7%2525%2520for%2520linear%2520segmentation%2520evaluations%2520on%2520COCO-Things%2520and%2520-Stuff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeCo%3A%20Improving%20DINOv2%27s%20spatial%20representations%20in%2019%20GPU%20hours%20with%0A%20%20Patch%20Neighbor%20Consistency&entry.906535625=Valentinos%20Pariza%20and%20Mohammadreza%20Salehi%20and%20Gertjan%20Burghouts%20and%20Francesco%20Locatello%20and%20Yuki%20M.%20Asano&entry.1292438233=%20%20We%20propose%20sorting%20patch%20representations%20across%20views%20as%20a%20novel%0Aself-supervised%20learning%20signal%20to%20improve%20pretrained%20representations.%20To%20this%0Aend%2C%20we%20introduce%20NeCo%3A%20Patch%20Neighbor%20Consistency%2C%20a%20novel%20training%20loss%20that%0Aenforces%20patch-level%20nearest%20neighbor%20consistency%20across%20a%20student%20and%20teacher%0Amodel%2C%20relative%20to%20reference%20batches.%20Our%20method%20leverages%20a%20differentiable%0Asorting%20method%20applied%20on%20top%20of%20pretrained%20representations%2C%20such%20as%0ADINOv2-registers%20to%20bootstrap%20the%20learning%20signal%20and%20further%20improve%20upon%0Athem.%20This%20dense%20post-pretraining%20leads%20to%20superior%20performance%20across%20various%0Amodels%20and%20datasets%2C%20despite%20requiring%20only%2019%20hours%20on%20a%20single%20GPU.%20We%0Ademonstrate%20that%20this%20method%20generates%20high-quality%20dense%20feature%20encoders%20and%0Aestablish%20several%20new%20state-of-the-art%20results%3A%20%2B5.5%25%20and%20%2B%206%25%20for%0Anon-parametric%20in-context%20semantic%20segmentation%20on%20ADE20k%20and%20Pascal%20VOC%2C%20and%0A%2B7.2%25%20and%20%2B5.7%25%20for%20linear%20segmentation%20evaluations%20on%20COCO-Things%20and%20-Stuff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11054v1&entry.124074799=Read"},
{"title": "TrackNeRF: Bundle Adjusting NeRF from Sparse and Noisy Views via Feature\n  Tracks", "author": "Jinjie Mai and Wenxuan Zhu and Sara Rojas and Jesus Zarzar and Abdullah Hamdi and Guocheng Qian and Bing Li and Silvio Giancola and Bernard Ghanem", "abstract": "  Neural radiance fields (NeRFs) generally require many images with accurate\nposes for accurate novel view synthesis, which does not reflect realistic\nsetups where views can be sparse and poses can be noisy. Previous solutions for\nlearning NeRFs with sparse views and noisy poses only consider local geometry\nconsistency with pairs of views. Closely following \\textit{bundle adjustment}\nin Structure-from-Motion (SfM), we introduce TrackNeRF for more globally\nconsistent geometry reconstruction and more accurate pose optimization.\nTrackNeRF introduces \\textit{feature tracks}, \\ie connected pixel trajectories\nacross \\textit{all} visible views that correspond to the \\textit{same} 3D\npoints. By enforcing reprojection consistency among feature tracks, TrackNeRF\nencourages holistic 3D consistency explicitly. Through extensive experiments,\nTrackNeRF sets a new benchmark in noisy and sparse view reconstruction. In\nparticular, TrackNeRF shows significant improvements over the state-of-the-art\nBARF and SPARF by $\\sim8$ and $\\sim1$ in terms of PSNR on DTU under various\nsparse and noisy view setups. The code is available at\n\\href{https://tracknerf.github.io/}.\n", "link": "http://arxiv.org/abs/2408.10739v1", "date": "2024-08-20", "relevancy": 2.6686, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5639}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5193}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrackNeRF%3A%20Bundle%20Adjusting%20NeRF%20from%20Sparse%20and%20Noisy%20Views%20via%20Feature%0A%20%20Tracks&body=Title%3A%20TrackNeRF%3A%20Bundle%20Adjusting%20NeRF%20from%20Sparse%20and%20Noisy%20Views%20via%20Feature%0A%20%20Tracks%0AAuthor%3A%20Jinjie%20Mai%20and%20Wenxuan%20Zhu%20and%20Sara%20Rojas%20and%20Jesus%20Zarzar%20and%20Abdullah%20Hamdi%20and%20Guocheng%20Qian%20and%20Bing%20Li%20and%20Silvio%20Giancola%20and%20Bernard%20Ghanem%0AAbstract%3A%20%20%20Neural%20radiance%20fields%20%28NeRFs%29%20generally%20require%20many%20images%20with%20accurate%0Aposes%20for%20accurate%20novel%20view%20synthesis%2C%20which%20does%20not%20reflect%20realistic%0Asetups%20where%20views%20can%20be%20sparse%20and%20poses%20can%20be%20noisy.%20Previous%20solutions%20for%0Alearning%20NeRFs%20with%20sparse%20views%20and%20noisy%20poses%20only%20consider%20local%20geometry%0Aconsistency%20with%20pairs%20of%20views.%20Closely%20following%20%5Ctextit%7Bbundle%20adjustment%7D%0Ain%20Structure-from-Motion%20%28SfM%29%2C%20we%20introduce%20TrackNeRF%20for%20more%20globally%0Aconsistent%20geometry%20reconstruction%20and%20more%20accurate%20pose%20optimization.%0ATrackNeRF%20introduces%20%5Ctextit%7Bfeature%20tracks%7D%2C%20%5Cie%20connected%20pixel%20trajectories%0Aacross%20%5Ctextit%7Ball%7D%20visible%20views%20that%20correspond%20to%20the%20%5Ctextit%7Bsame%7D%203D%0Apoints.%20By%20enforcing%20reprojection%20consistency%20among%20feature%20tracks%2C%20TrackNeRF%0Aencourages%20holistic%203D%20consistency%20explicitly.%20Through%20extensive%20experiments%2C%0ATrackNeRF%20sets%20a%20new%20benchmark%20in%20noisy%20and%20sparse%20view%20reconstruction.%20In%0Aparticular%2C%20TrackNeRF%20shows%20significant%20improvements%20over%20the%20state-of-the-art%0ABARF%20and%20SPARF%20by%20%24%5Csim8%24%20and%20%24%5Csim1%24%20in%20terms%20of%20PSNR%20on%20DTU%20under%20various%0Asparse%20and%20noisy%20view%20setups.%20The%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//tracknerf.github.io/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrackNeRF%253A%2520Bundle%2520Adjusting%2520NeRF%2520from%2520Sparse%2520and%2520Noisy%2520Views%2520via%2520Feature%250A%2520%2520Tracks%26entry.906535625%3DJinjie%2520Mai%2520and%2520Wenxuan%2520Zhu%2520and%2520Sara%2520Rojas%2520and%2520Jesus%2520Zarzar%2520and%2520Abdullah%2520Hamdi%2520and%2520Guocheng%2520Qian%2520and%2520Bing%2520Li%2520and%2520Silvio%2520Giancola%2520and%2520Bernard%2520Ghanem%26entry.1292438233%3D%2520%2520Neural%2520radiance%2520fields%2520%2528NeRFs%2529%2520generally%2520require%2520many%2520images%2520with%2520accurate%250Aposes%2520for%2520accurate%2520novel%2520view%2520synthesis%252C%2520which%2520does%2520not%2520reflect%2520realistic%250Asetups%2520where%2520views%2520can%2520be%2520sparse%2520and%2520poses%2520can%2520be%2520noisy.%2520Previous%2520solutions%2520for%250Alearning%2520NeRFs%2520with%2520sparse%2520views%2520and%2520noisy%2520poses%2520only%2520consider%2520local%2520geometry%250Aconsistency%2520with%2520pairs%2520of%2520views.%2520Closely%2520following%2520%255Ctextit%257Bbundle%2520adjustment%257D%250Ain%2520Structure-from-Motion%2520%2528SfM%2529%252C%2520we%2520introduce%2520TrackNeRF%2520for%2520more%2520globally%250Aconsistent%2520geometry%2520reconstruction%2520and%2520more%2520accurate%2520pose%2520optimization.%250ATrackNeRF%2520introduces%2520%255Ctextit%257Bfeature%2520tracks%257D%252C%2520%255Cie%2520connected%2520pixel%2520trajectories%250Aacross%2520%255Ctextit%257Ball%257D%2520visible%2520views%2520that%2520correspond%2520to%2520the%2520%255Ctextit%257Bsame%257D%25203D%250Apoints.%2520By%2520enforcing%2520reprojection%2520consistency%2520among%2520feature%2520tracks%252C%2520TrackNeRF%250Aencourages%2520holistic%25203D%2520consistency%2520explicitly.%2520Through%2520extensive%2520experiments%252C%250ATrackNeRF%2520sets%2520a%2520new%2520benchmark%2520in%2520noisy%2520and%2520sparse%2520view%2520reconstruction.%2520In%250Aparticular%252C%2520TrackNeRF%2520shows%2520significant%2520improvements%2520over%2520the%2520state-of-the-art%250ABARF%2520and%2520SPARF%2520by%2520%2524%255Csim8%2524%2520and%2520%2524%255Csim1%2524%2520in%2520terms%2520of%2520PSNR%2520on%2520DTU%2520under%2520various%250Asparse%2520and%2520noisy%2520view%2520setups.%2520The%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//tracknerf.github.io/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrackNeRF%3A%20Bundle%20Adjusting%20NeRF%20from%20Sparse%20and%20Noisy%20Views%20via%20Feature%0A%20%20Tracks&entry.906535625=Jinjie%20Mai%20and%20Wenxuan%20Zhu%20and%20Sara%20Rojas%20and%20Jesus%20Zarzar%20and%20Abdullah%20Hamdi%20and%20Guocheng%20Qian%20and%20Bing%20Li%20and%20Silvio%20Giancola%20and%20Bernard%20Ghanem&entry.1292438233=%20%20Neural%20radiance%20fields%20%28NeRFs%29%20generally%20require%20many%20images%20with%20accurate%0Aposes%20for%20accurate%20novel%20view%20synthesis%2C%20which%20does%20not%20reflect%20realistic%0Asetups%20where%20views%20can%20be%20sparse%20and%20poses%20can%20be%20noisy.%20Previous%20solutions%20for%0Alearning%20NeRFs%20with%20sparse%20views%20and%20noisy%20poses%20only%20consider%20local%20geometry%0Aconsistency%20with%20pairs%20of%20views.%20Closely%20following%20%5Ctextit%7Bbundle%20adjustment%7D%0Ain%20Structure-from-Motion%20%28SfM%29%2C%20we%20introduce%20TrackNeRF%20for%20more%20globally%0Aconsistent%20geometry%20reconstruction%20and%20more%20accurate%20pose%20optimization.%0ATrackNeRF%20introduces%20%5Ctextit%7Bfeature%20tracks%7D%2C%20%5Cie%20connected%20pixel%20trajectories%0Aacross%20%5Ctextit%7Ball%7D%20visible%20views%20that%20correspond%20to%20the%20%5Ctextit%7Bsame%7D%203D%0Apoints.%20By%20enforcing%20reprojection%20consistency%20among%20feature%20tracks%2C%20TrackNeRF%0Aencourages%20holistic%203D%20consistency%20explicitly.%20Through%20extensive%20experiments%2C%0ATrackNeRF%20sets%20a%20new%20benchmark%20in%20noisy%20and%20sparse%20view%20reconstruction.%20In%0Aparticular%2C%20TrackNeRF%20shows%20significant%20improvements%20over%20the%20state-of-the-art%0ABARF%20and%20SPARF%20by%20%24%5Csim8%24%20and%20%24%5Csim1%24%20in%20terms%20of%20PSNR%20on%20DTU%20under%20various%0Asparse%20and%20noisy%20view%20setups.%20The%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//tracknerf.github.io/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10739v1&entry.124074799=Read"},
{"title": "MagicID: Flexible ID Fidelity Generation System", "author": "Zhaoli Deng and Wen Liu and Fanyi Wang and Junkang Zhang and Fan Chen and Meng Zhang and Wendong Zhang and Zhenpeng Mi", "abstract": "  Portrait Fidelity Generation is a prominent research area in generative\nmodels, with a primary focus on enhancing both controllability and fidelity.\nCurrent methods face challenges in generating high-fidelity portrait results\nwhen faces occupy a small portion of the image with a low resolution,\nespecially in multi-person group photo settings. To tackle these issues, we\npropose a systematic solution called MagicID, based on a self-constructed\nmillion-level multi-modal dataset named IDZoom. MagicID consists of Multi-Mode\nFusion training strategy (MMF) and DDIM Inversion based ID Restoration\ninference framework (DIIR). During training, MMF iteratively uses the skeleton\nand landmark modalities from IDZoom as conditional guidance. By introducing the\nClone Face Tuning in training stage and Mask Guided Multi-ID Cross Attention\n(MGMICA) in inference stage, explicit constraints on face positional features\nare achieved for multi-ID group photo generation. The DIIR aims to address the\nissue of artifacts. The DDIM Inversion is used in conjunction with face\nlandmarks, global and local face features to achieve face restoration while\nkeeping the background unchanged. Additionally, DIIR is plug-and-play and can\nbe applied to any diffusion-based portrait generation method. To validate the\neffectiveness of MagicID, we conducted extensive comparative and ablation\nexperiments. The experimental results demonstrate that MagicID has significant\nadvantages in both subjective and objective metrics, and achieves controllable\ngeneration in multi-person scenarios.\n", "link": "http://arxiv.org/abs/2408.09248v2", "date": "2024-08-20", "relevancy": 2.6344, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7399}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6122}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicID%3A%20Flexible%20ID%20Fidelity%20Generation%20System&body=Title%3A%20MagicID%3A%20Flexible%20ID%20Fidelity%20Generation%20System%0AAuthor%3A%20Zhaoli%20Deng%20and%20Wen%20Liu%20and%20Fanyi%20Wang%20and%20Junkang%20Zhang%20and%20Fan%20Chen%20and%20Meng%20Zhang%20and%20Wendong%20Zhang%20and%20Zhenpeng%20Mi%0AAbstract%3A%20%20%20Portrait%20Fidelity%20Generation%20is%20a%20prominent%20research%20area%20in%20generative%0Amodels%2C%20with%20a%20primary%20focus%20on%20enhancing%20both%20controllability%20and%20fidelity.%0ACurrent%20methods%20face%20challenges%20in%20generating%20high-fidelity%20portrait%20results%0Awhen%20faces%20occupy%20a%20small%20portion%20of%20the%20image%20with%20a%20low%20resolution%2C%0Aespecially%20in%20multi-person%20group%20photo%20settings.%20To%20tackle%20these%20issues%2C%20we%0Apropose%20a%20systematic%20solution%20called%20MagicID%2C%20based%20on%20a%20self-constructed%0Amillion-level%20multi-modal%20dataset%20named%20IDZoom.%20MagicID%20consists%20of%20Multi-Mode%0AFusion%20training%20strategy%20%28MMF%29%20and%20DDIM%20Inversion%20based%20ID%20Restoration%0Ainference%20framework%20%28DIIR%29.%20During%20training%2C%20MMF%20iteratively%20uses%20the%20skeleton%0Aand%20landmark%20modalities%20from%20IDZoom%20as%20conditional%20guidance.%20By%20introducing%20the%0AClone%20Face%20Tuning%20in%20training%20stage%20and%20Mask%20Guided%20Multi-ID%20Cross%20Attention%0A%28MGMICA%29%20in%20inference%20stage%2C%20explicit%20constraints%20on%20face%20positional%20features%0Aare%20achieved%20for%20multi-ID%20group%20photo%20generation.%20The%20DIIR%20aims%20to%20address%20the%0Aissue%20of%20artifacts.%20The%20DDIM%20Inversion%20is%20used%20in%20conjunction%20with%20face%0Alandmarks%2C%20global%20and%20local%20face%20features%20to%20achieve%20face%20restoration%20while%0Akeeping%20the%20background%20unchanged.%20Additionally%2C%20DIIR%20is%20plug-and-play%20and%20can%0Abe%20applied%20to%20any%20diffusion-based%20portrait%20generation%20method.%20To%20validate%20the%0Aeffectiveness%20of%20MagicID%2C%20we%20conducted%20extensive%20comparative%20and%20ablation%0Aexperiments.%20The%20experimental%20results%20demonstrate%20that%20MagicID%20has%20significant%0Aadvantages%20in%20both%20subjective%20and%20objective%20metrics%2C%20and%20achieves%20controllable%0Ageneration%20in%20multi-person%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09248v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicID%253A%2520Flexible%2520ID%2520Fidelity%2520Generation%2520System%26entry.906535625%3DZhaoli%2520Deng%2520and%2520Wen%2520Liu%2520and%2520Fanyi%2520Wang%2520and%2520Junkang%2520Zhang%2520and%2520Fan%2520Chen%2520and%2520Meng%2520Zhang%2520and%2520Wendong%2520Zhang%2520and%2520Zhenpeng%2520Mi%26entry.1292438233%3D%2520%2520Portrait%2520Fidelity%2520Generation%2520is%2520a%2520prominent%2520research%2520area%2520in%2520generative%250Amodels%252C%2520with%2520a%2520primary%2520focus%2520on%2520enhancing%2520both%2520controllability%2520and%2520fidelity.%250ACurrent%2520methods%2520face%2520challenges%2520in%2520generating%2520high-fidelity%2520portrait%2520results%250Awhen%2520faces%2520occupy%2520a%2520small%2520portion%2520of%2520the%2520image%2520with%2520a%2520low%2520resolution%252C%250Aespecially%2520in%2520multi-person%2520group%2520photo%2520settings.%2520To%2520tackle%2520these%2520issues%252C%2520we%250Apropose%2520a%2520systematic%2520solution%2520called%2520MagicID%252C%2520based%2520on%2520a%2520self-constructed%250Amillion-level%2520multi-modal%2520dataset%2520named%2520IDZoom.%2520MagicID%2520consists%2520of%2520Multi-Mode%250AFusion%2520training%2520strategy%2520%2528MMF%2529%2520and%2520DDIM%2520Inversion%2520based%2520ID%2520Restoration%250Ainference%2520framework%2520%2528DIIR%2529.%2520During%2520training%252C%2520MMF%2520iteratively%2520uses%2520the%2520skeleton%250Aand%2520landmark%2520modalities%2520from%2520IDZoom%2520as%2520conditional%2520guidance.%2520By%2520introducing%2520the%250AClone%2520Face%2520Tuning%2520in%2520training%2520stage%2520and%2520Mask%2520Guided%2520Multi-ID%2520Cross%2520Attention%250A%2528MGMICA%2529%2520in%2520inference%2520stage%252C%2520explicit%2520constraints%2520on%2520face%2520positional%2520features%250Aare%2520achieved%2520for%2520multi-ID%2520group%2520photo%2520generation.%2520The%2520DIIR%2520aims%2520to%2520address%2520the%250Aissue%2520of%2520artifacts.%2520The%2520DDIM%2520Inversion%2520is%2520used%2520in%2520conjunction%2520with%2520face%250Alandmarks%252C%2520global%2520and%2520local%2520face%2520features%2520to%2520achieve%2520face%2520restoration%2520while%250Akeeping%2520the%2520background%2520unchanged.%2520Additionally%252C%2520DIIR%2520is%2520plug-and-play%2520and%2520can%250Abe%2520applied%2520to%2520any%2520diffusion-based%2520portrait%2520generation%2520method.%2520To%2520validate%2520the%250Aeffectiveness%2520of%2520MagicID%252C%2520we%2520conducted%2520extensive%2520comparative%2520and%2520ablation%250Aexperiments.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520MagicID%2520has%2520significant%250Aadvantages%2520in%2520both%2520subjective%2520and%2520objective%2520metrics%252C%2520and%2520achieves%2520controllable%250Ageneration%2520in%2520multi-person%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09248v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicID%3A%20Flexible%20ID%20Fidelity%20Generation%20System&entry.906535625=Zhaoli%20Deng%20and%20Wen%20Liu%20and%20Fanyi%20Wang%20and%20Junkang%20Zhang%20and%20Fan%20Chen%20and%20Meng%20Zhang%20and%20Wendong%20Zhang%20and%20Zhenpeng%20Mi&entry.1292438233=%20%20Portrait%20Fidelity%20Generation%20is%20a%20prominent%20research%20area%20in%20generative%0Amodels%2C%20with%20a%20primary%20focus%20on%20enhancing%20both%20controllability%20and%20fidelity.%0ACurrent%20methods%20face%20challenges%20in%20generating%20high-fidelity%20portrait%20results%0Awhen%20faces%20occupy%20a%20small%20portion%20of%20the%20image%20with%20a%20low%20resolution%2C%0Aespecially%20in%20multi-person%20group%20photo%20settings.%20To%20tackle%20these%20issues%2C%20we%0Apropose%20a%20systematic%20solution%20called%20MagicID%2C%20based%20on%20a%20self-constructed%0Amillion-level%20multi-modal%20dataset%20named%20IDZoom.%20MagicID%20consists%20of%20Multi-Mode%0AFusion%20training%20strategy%20%28MMF%29%20and%20DDIM%20Inversion%20based%20ID%20Restoration%0Ainference%20framework%20%28DIIR%29.%20During%20training%2C%20MMF%20iteratively%20uses%20the%20skeleton%0Aand%20landmark%20modalities%20from%20IDZoom%20as%20conditional%20guidance.%20By%20introducing%20the%0AClone%20Face%20Tuning%20in%20training%20stage%20and%20Mask%20Guided%20Multi-ID%20Cross%20Attention%0A%28MGMICA%29%20in%20inference%20stage%2C%20explicit%20constraints%20on%20face%20positional%20features%0Aare%20achieved%20for%20multi-ID%20group%20photo%20generation.%20The%20DIIR%20aims%20to%20address%20the%0Aissue%20of%20artifacts.%20The%20DDIM%20Inversion%20is%20used%20in%20conjunction%20with%20face%0Alandmarks%2C%20global%20and%20local%20face%20features%20to%20achieve%20face%20restoration%20while%0Akeeping%20the%20background%20unchanged.%20Additionally%2C%20DIIR%20is%20plug-and-play%20and%20can%0Abe%20applied%20to%20any%20diffusion-based%20portrait%20generation%20method.%20To%20validate%20the%0Aeffectiveness%20of%20MagicID%2C%20we%20conducted%20extensive%20comparative%20and%20ablation%0Aexperiments.%20The%20experimental%20results%20demonstrate%20that%20MagicID%20has%20significant%0Aadvantages%20in%20both%20subjective%20and%20objective%20metrics%2C%20and%20achieves%20controllable%0Ageneration%20in%20multi-person%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09248v2&entry.124074799=Read"},
{"title": "SAM Meets UAP: Attacking Segment Anything Model With Universal\n  Adversarial Perturbation", "author": "Dongshen Han and Chaoning Zhang and Sheng Zheng and Chang Lu and Yang Yang and Heng Tao Shen", "abstract": "  As Segment Anything Model (SAM) becomes a popular foundation model in\ncomputer vision, its adversarial robustness has become a concern that cannot be\nignored. This works investigates whether it is possible to attack SAM with\nimage-agnostic Universal Adversarial Perturbation (UAP). In other words, we\nseek a single perturbation that can fool the SAM to predict invalid masks for\nmost (if not all) images. We demonstrate convetional image-centric attack\nframework is effective for image-independent attacks but fails for universal\nadversarial attack. To this end, we propose a novel perturbation-centric\nframework that results in a UAP generation method based on self-supervised\ncontrastive learning (CL), where the UAP is set to the anchor sample and the\npositive sample is augmented from the UAP. The representations of negative\nsamples are obtained from the image encoder in advance and saved in a memory\nbank. The effectiveness of our proposed CL-based UAP generation method is\nvalidated by both quantitative and qualitative results. On top of the ablation\nstudy to understand various components in our proposed method, we shed light on\nthe roles of positive and negative samples in making the generated UAP\neffective for attacking SAM.\n", "link": "http://arxiv.org/abs/2310.12431v2", "date": "2024-08-20", "relevancy": 2.6343, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5467}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5253}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM%20Meets%20UAP%3A%20Attacking%20Segment%20Anything%20Model%20With%20Universal%0A%20%20Adversarial%20Perturbation&body=Title%3A%20SAM%20Meets%20UAP%3A%20Attacking%20Segment%20Anything%20Model%20With%20Universal%0A%20%20Adversarial%20Perturbation%0AAuthor%3A%20Dongshen%20Han%20and%20Chaoning%20Zhang%20and%20Sheng%20Zheng%20and%20Chang%20Lu%20and%20Yang%20Yang%20and%20Heng%20Tao%20Shen%0AAbstract%3A%20%20%20As%20Segment%20Anything%20Model%20%28SAM%29%20becomes%20a%20popular%20foundation%20model%20in%0Acomputer%20vision%2C%20its%20adversarial%20robustness%20has%20become%20a%20concern%20that%20cannot%20be%0Aignored.%20This%20works%20investigates%20whether%20it%20is%20possible%20to%20attack%20SAM%20with%0Aimage-agnostic%20Universal%20Adversarial%20Perturbation%20%28UAP%29.%20In%20other%20words%2C%20we%0Aseek%20a%20single%20perturbation%20that%20can%20fool%20the%20SAM%20to%20predict%20invalid%20masks%20for%0Amost%20%28if%20not%20all%29%20images.%20We%20demonstrate%20convetional%20image-centric%20attack%0Aframework%20is%20effective%20for%20image-independent%20attacks%20but%20fails%20for%20universal%0Aadversarial%20attack.%20To%20this%20end%2C%20we%20propose%20a%20novel%20perturbation-centric%0Aframework%20that%20results%20in%20a%20UAP%20generation%20method%20based%20on%20self-supervised%0Acontrastive%20learning%20%28CL%29%2C%20where%20the%20UAP%20is%20set%20to%20the%20anchor%20sample%20and%20the%0Apositive%20sample%20is%20augmented%20from%20the%20UAP.%20The%20representations%20of%20negative%0Asamples%20are%20obtained%20from%20the%20image%20encoder%20in%20advance%20and%20saved%20in%20a%20memory%0Abank.%20The%20effectiveness%20of%20our%20proposed%20CL-based%20UAP%20generation%20method%20is%0Avalidated%20by%20both%20quantitative%20and%20qualitative%20results.%20On%20top%20of%20the%20ablation%0Astudy%20to%20understand%20various%20components%20in%20our%20proposed%20method%2C%20we%20shed%20light%20on%0Athe%20roles%20of%20positive%20and%20negative%20samples%20in%20making%20the%20generated%20UAP%0Aeffective%20for%20attacking%20SAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12431v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM%2520Meets%2520UAP%253A%2520Attacking%2520Segment%2520Anything%2520Model%2520With%2520Universal%250A%2520%2520Adversarial%2520Perturbation%26entry.906535625%3DDongshen%2520Han%2520and%2520Chaoning%2520Zhang%2520and%2520Sheng%2520Zheng%2520and%2520Chang%2520Lu%2520and%2520Yang%2520Yang%2520and%2520Heng%2520Tao%2520Shen%26entry.1292438233%3D%2520%2520As%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520becomes%2520a%2520popular%2520foundation%2520model%2520in%250Acomputer%2520vision%252C%2520its%2520adversarial%2520robustness%2520has%2520become%2520a%2520concern%2520that%2520cannot%2520be%250Aignored.%2520This%2520works%2520investigates%2520whether%2520it%2520is%2520possible%2520to%2520attack%2520SAM%2520with%250Aimage-agnostic%2520Universal%2520Adversarial%2520Perturbation%2520%2528UAP%2529.%2520In%2520other%2520words%252C%2520we%250Aseek%2520a%2520single%2520perturbation%2520that%2520can%2520fool%2520the%2520SAM%2520to%2520predict%2520invalid%2520masks%2520for%250Amost%2520%2528if%2520not%2520all%2529%2520images.%2520We%2520demonstrate%2520convetional%2520image-centric%2520attack%250Aframework%2520is%2520effective%2520for%2520image-independent%2520attacks%2520but%2520fails%2520for%2520universal%250Aadversarial%2520attack.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520perturbation-centric%250Aframework%2520that%2520results%2520in%2520a%2520UAP%2520generation%2520method%2520based%2520on%2520self-supervised%250Acontrastive%2520learning%2520%2528CL%2529%252C%2520where%2520the%2520UAP%2520is%2520set%2520to%2520the%2520anchor%2520sample%2520and%2520the%250Apositive%2520sample%2520is%2520augmented%2520from%2520the%2520UAP.%2520The%2520representations%2520of%2520negative%250Asamples%2520are%2520obtained%2520from%2520the%2520image%2520encoder%2520in%2520advance%2520and%2520saved%2520in%2520a%2520memory%250Abank.%2520The%2520effectiveness%2520of%2520our%2520proposed%2520CL-based%2520UAP%2520generation%2520method%2520is%250Avalidated%2520by%2520both%2520quantitative%2520and%2520qualitative%2520results.%2520On%2520top%2520of%2520the%2520ablation%250Astudy%2520to%2520understand%2520various%2520components%2520in%2520our%2520proposed%2520method%252C%2520we%2520shed%2520light%2520on%250Athe%2520roles%2520of%2520positive%2520and%2520negative%2520samples%2520in%2520making%2520the%2520generated%2520UAP%250Aeffective%2520for%2520attacking%2520SAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12431v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM%20Meets%20UAP%3A%20Attacking%20Segment%20Anything%20Model%20With%20Universal%0A%20%20Adversarial%20Perturbation&entry.906535625=Dongshen%20Han%20and%20Chaoning%20Zhang%20and%20Sheng%20Zheng%20and%20Chang%20Lu%20and%20Yang%20Yang%20and%20Heng%20Tao%20Shen&entry.1292438233=%20%20As%20Segment%20Anything%20Model%20%28SAM%29%20becomes%20a%20popular%20foundation%20model%20in%0Acomputer%20vision%2C%20its%20adversarial%20robustness%20has%20become%20a%20concern%20that%20cannot%20be%0Aignored.%20This%20works%20investigates%20whether%20it%20is%20possible%20to%20attack%20SAM%20with%0Aimage-agnostic%20Universal%20Adversarial%20Perturbation%20%28UAP%29.%20In%20other%20words%2C%20we%0Aseek%20a%20single%20perturbation%20that%20can%20fool%20the%20SAM%20to%20predict%20invalid%20masks%20for%0Amost%20%28if%20not%20all%29%20images.%20We%20demonstrate%20convetional%20image-centric%20attack%0Aframework%20is%20effective%20for%20image-independent%20attacks%20but%20fails%20for%20universal%0Aadversarial%20attack.%20To%20this%20end%2C%20we%20propose%20a%20novel%20perturbation-centric%0Aframework%20that%20results%20in%20a%20UAP%20generation%20method%20based%20on%20self-supervised%0Acontrastive%20learning%20%28CL%29%2C%20where%20the%20UAP%20is%20set%20to%20the%20anchor%20sample%20and%20the%0Apositive%20sample%20is%20augmented%20from%20the%20UAP.%20The%20representations%20of%20negative%0Asamples%20are%20obtained%20from%20the%20image%20encoder%20in%20advance%20and%20saved%20in%20a%20memory%0Abank.%20The%20effectiveness%20of%20our%20proposed%20CL-based%20UAP%20generation%20method%20is%0Avalidated%20by%20both%20quantitative%20and%20qualitative%20results.%20On%20top%20of%20the%20ablation%0Astudy%20to%20understand%20various%20components%20in%20our%20proposed%20method%2C%20we%20shed%20light%20on%0Athe%20roles%20of%20positive%20and%20negative%20samples%20in%20making%20the%20generated%20UAP%0Aeffective%20for%20attacking%20SAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12431v2&entry.124074799=Read"},
{"title": "Multilevel CNNs for Parametric PDEs based on Adaptive Finite Elements", "author": "Janina Enrica Sch\u00fctte and Martin Eigel", "abstract": "  A neural network architecture is presented that exploits the multilevel\nproperties of high-dimensional parameter-dependent partial differential\nequations, enabling an efficient approximation of parameter-to-solution maps,\nrivaling best-in-class methods such as low-rank tensor regression in terms of\naccuracy and complexity. The neural network is trained with data on adaptively\nrefined finite element meshes, thus reducing data complexity significantly.\nError control is achieved by using a reliable finite element a posteriori error\nestimator, which is also provided as input to the neural network.\n  The proposed U-Net architecture with CNN layers mimics a classical finite\nelement multigrid algorithm. It can be shown that the CNN efficiently\napproximates all operations required by the solver, including the evaluation of\nthe residual-based error estimator. In the CNN, a culling mask set-up according\nto the local corrections due to refinement on each mesh level reduces the\noverall complexity, allowing the network optimization with localized fine-scale\nfinite element data.\n  A complete convergence and complexity analysis is carried out for the\nadaptive multilevel scheme, which differs in several aspects from previous\nnon-adaptive multilevel CNN. Moreover, numerical experiments with common\nbenchmark problems from Uncertainty Quantification illustrate the practical\nperformance of the architecture.\n", "link": "http://arxiv.org/abs/2408.10838v1", "date": "2024-08-20", "relevancy": 2.6015, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5551}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5037}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multilevel%20CNNs%20for%20Parametric%20PDEs%20based%20on%20Adaptive%20Finite%20Elements&body=Title%3A%20Multilevel%20CNNs%20for%20Parametric%20PDEs%20based%20on%20Adaptive%20Finite%20Elements%0AAuthor%3A%20Janina%20Enrica%20Sch%C3%BCtte%20and%20Martin%20Eigel%0AAbstract%3A%20%20%20A%20neural%20network%20architecture%20is%20presented%20that%20exploits%20the%20multilevel%0Aproperties%20of%20high-dimensional%20parameter-dependent%20partial%20differential%0Aequations%2C%20enabling%20an%20efficient%20approximation%20of%20parameter-to-solution%20maps%2C%0Arivaling%20best-in-class%20methods%20such%20as%20low-rank%20tensor%20regression%20in%20terms%20of%0Aaccuracy%20and%20complexity.%20The%20neural%20network%20is%20trained%20with%20data%20on%20adaptively%0Arefined%20finite%20element%20meshes%2C%20thus%20reducing%20data%20complexity%20significantly.%0AError%20control%20is%20achieved%20by%20using%20a%20reliable%20finite%20element%20a%20posteriori%20error%0Aestimator%2C%20which%20is%20also%20provided%20as%20input%20to%20the%20neural%20network.%0A%20%20The%20proposed%20U-Net%20architecture%20with%20CNN%20layers%20mimics%20a%20classical%20finite%0Aelement%20multigrid%20algorithm.%20It%20can%20be%20shown%20that%20the%20CNN%20efficiently%0Aapproximates%20all%20operations%20required%20by%20the%20solver%2C%20including%20the%20evaluation%20of%0Athe%20residual-based%20error%20estimator.%20In%20the%20CNN%2C%20a%20culling%20mask%20set-up%20according%0Ato%20the%20local%20corrections%20due%20to%20refinement%20on%20each%20mesh%20level%20reduces%20the%0Aoverall%20complexity%2C%20allowing%20the%20network%20optimization%20with%20localized%20fine-scale%0Afinite%20element%20data.%0A%20%20A%20complete%20convergence%20and%20complexity%20analysis%20is%20carried%20out%20for%20the%0Aadaptive%20multilevel%20scheme%2C%20which%20differs%20in%20several%20aspects%20from%20previous%0Anon-adaptive%20multilevel%20CNN.%20Moreover%2C%20numerical%20experiments%20with%20common%0Abenchmark%20problems%20from%20Uncertainty%20Quantification%20illustrate%20the%20practical%0Aperformance%20of%20the%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultilevel%2520CNNs%2520for%2520Parametric%2520PDEs%2520based%2520on%2520Adaptive%2520Finite%2520Elements%26entry.906535625%3DJanina%2520Enrica%2520Sch%25C3%25BCtte%2520and%2520Martin%2520Eigel%26entry.1292438233%3D%2520%2520A%2520neural%2520network%2520architecture%2520is%2520presented%2520that%2520exploits%2520the%2520multilevel%250Aproperties%2520of%2520high-dimensional%2520parameter-dependent%2520partial%2520differential%250Aequations%252C%2520enabling%2520an%2520efficient%2520approximation%2520of%2520parameter-to-solution%2520maps%252C%250Arivaling%2520best-in-class%2520methods%2520such%2520as%2520low-rank%2520tensor%2520regression%2520in%2520terms%2520of%250Aaccuracy%2520and%2520complexity.%2520The%2520neural%2520network%2520is%2520trained%2520with%2520data%2520on%2520adaptively%250Arefined%2520finite%2520element%2520meshes%252C%2520thus%2520reducing%2520data%2520complexity%2520significantly.%250AError%2520control%2520is%2520achieved%2520by%2520using%2520a%2520reliable%2520finite%2520element%2520a%2520posteriori%2520error%250Aestimator%252C%2520which%2520is%2520also%2520provided%2520as%2520input%2520to%2520the%2520neural%2520network.%250A%2520%2520The%2520proposed%2520U-Net%2520architecture%2520with%2520CNN%2520layers%2520mimics%2520a%2520classical%2520finite%250Aelement%2520multigrid%2520algorithm.%2520It%2520can%2520be%2520shown%2520that%2520the%2520CNN%2520efficiently%250Aapproximates%2520all%2520operations%2520required%2520by%2520the%2520solver%252C%2520including%2520the%2520evaluation%2520of%250Athe%2520residual-based%2520error%2520estimator.%2520In%2520the%2520CNN%252C%2520a%2520culling%2520mask%2520set-up%2520according%250Ato%2520the%2520local%2520corrections%2520due%2520to%2520refinement%2520on%2520each%2520mesh%2520level%2520reduces%2520the%250Aoverall%2520complexity%252C%2520allowing%2520the%2520network%2520optimization%2520with%2520localized%2520fine-scale%250Afinite%2520element%2520data.%250A%2520%2520A%2520complete%2520convergence%2520and%2520complexity%2520analysis%2520is%2520carried%2520out%2520for%2520the%250Aadaptive%2520multilevel%2520scheme%252C%2520which%2520differs%2520in%2520several%2520aspects%2520from%2520previous%250Anon-adaptive%2520multilevel%2520CNN.%2520Moreover%252C%2520numerical%2520experiments%2520with%2520common%250Abenchmark%2520problems%2520from%2520Uncertainty%2520Quantification%2520illustrate%2520the%2520practical%250Aperformance%2520of%2520the%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilevel%20CNNs%20for%20Parametric%20PDEs%20based%20on%20Adaptive%20Finite%20Elements&entry.906535625=Janina%20Enrica%20Sch%C3%BCtte%20and%20Martin%20Eigel&entry.1292438233=%20%20A%20neural%20network%20architecture%20is%20presented%20that%20exploits%20the%20multilevel%0Aproperties%20of%20high-dimensional%20parameter-dependent%20partial%20differential%0Aequations%2C%20enabling%20an%20efficient%20approximation%20of%20parameter-to-solution%20maps%2C%0Arivaling%20best-in-class%20methods%20such%20as%20low-rank%20tensor%20regression%20in%20terms%20of%0Aaccuracy%20and%20complexity.%20The%20neural%20network%20is%20trained%20with%20data%20on%20adaptively%0Arefined%20finite%20element%20meshes%2C%20thus%20reducing%20data%20complexity%20significantly.%0AError%20control%20is%20achieved%20by%20using%20a%20reliable%20finite%20element%20a%20posteriori%20error%0Aestimator%2C%20which%20is%20also%20provided%20as%20input%20to%20the%20neural%20network.%0A%20%20The%20proposed%20U-Net%20architecture%20with%20CNN%20layers%20mimics%20a%20classical%20finite%0Aelement%20multigrid%20algorithm.%20It%20can%20be%20shown%20that%20the%20CNN%20efficiently%0Aapproximates%20all%20operations%20required%20by%20the%20solver%2C%20including%20the%20evaluation%20of%0Athe%20residual-based%20error%20estimator.%20In%20the%20CNN%2C%20a%20culling%20mask%20set-up%20according%0Ato%20the%20local%20corrections%20due%20to%20refinement%20on%20each%20mesh%20level%20reduces%20the%0Aoverall%20complexity%2C%20allowing%20the%20network%20optimization%20with%20localized%20fine-scale%0Afinite%20element%20data.%0A%20%20A%20complete%20convergence%20and%20complexity%20analysis%20is%20carried%20out%20for%20the%0Aadaptive%20multilevel%20scheme%2C%20which%20differs%20in%20several%20aspects%20from%20previous%0Anon-adaptive%20multilevel%20CNN.%20Moreover%2C%20numerical%20experiments%20with%20common%0Abenchmark%20problems%20from%20Uncertainty%20Quantification%20illustrate%20the%20practical%0Aperformance%20of%20the%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10838v1&entry.124074799=Read"},
{"title": "Coarse-to-Fine Detection of Multiple Seams for Robotic Welding", "author": "Pengkun Wei and Shuo Cheng and Dayou Li and Ran Song and Yipeng Zhang and Wei Zhang", "abstract": "  Efficiently detecting target weld seams while ensuring sub-millimeter\naccuracy has always been an important challenge in autonomous welding, which\nhas significant application in industrial practice. Previous works mostly\nfocused on recognizing and localizing welding seams one by one, leading to\ninferior efficiency in modeling the workpiece. This paper proposes a novel\nframework capable of multiple weld seams extraction using both RGB images and\n3D point clouds. The RGB image is used to obtain the region of interest by\napproximately localizing the weld seams, and the point cloud is used to achieve\nthe fine-edge extraction of the weld seams within the region of interest using\nregion growth. Our method is further accelerated by using a pre-trained deep\nlearning model to ensure both efficiency and generalization ability. The\nperformance of the proposed method has been comprehensively tested on various\nworkpieces featuring both linear and curved weld seams and in physical\nexperiment systems. The results showcase considerable potential for real-world\nindustrial applications, emphasizing the method's efficiency and effectiveness.\nVideos of the real-world experiments can be found at\nhttps://youtu.be/pq162HSP2D4.\n", "link": "http://arxiv.org/abs/2408.10710v1", "date": "2024-08-20", "relevancy": 2.5801, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5177}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5163}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coarse-to-Fine%20Detection%20of%20Multiple%20Seams%20for%20Robotic%20Welding&body=Title%3A%20Coarse-to-Fine%20Detection%20of%20Multiple%20Seams%20for%20Robotic%20Welding%0AAuthor%3A%20Pengkun%20Wei%20and%20Shuo%20Cheng%20and%20Dayou%20Li%20and%20Ran%20Song%20and%20Yipeng%20Zhang%20and%20Wei%20Zhang%0AAbstract%3A%20%20%20Efficiently%20detecting%20target%20weld%20seams%20while%20ensuring%20sub-millimeter%0Aaccuracy%20has%20always%20been%20an%20important%20challenge%20in%20autonomous%20welding%2C%20which%0Ahas%20significant%20application%20in%20industrial%20practice.%20Previous%20works%20mostly%0Afocused%20on%20recognizing%20and%20localizing%20welding%20seams%20one%20by%20one%2C%20leading%20to%0Ainferior%20efficiency%20in%20modeling%20the%20workpiece.%20This%20paper%20proposes%20a%20novel%0Aframework%20capable%20of%20multiple%20weld%20seams%20extraction%20using%20both%20RGB%20images%20and%0A3D%20point%20clouds.%20The%20RGB%20image%20is%20used%20to%20obtain%20the%20region%20of%20interest%20by%0Aapproximately%20localizing%20the%20weld%20seams%2C%20and%20the%20point%20cloud%20is%20used%20to%20achieve%0Athe%20fine-edge%20extraction%20of%20the%20weld%20seams%20within%20the%20region%20of%20interest%20using%0Aregion%20growth.%20Our%20method%20is%20further%20accelerated%20by%20using%20a%20pre-trained%20deep%0Alearning%20model%20to%20ensure%20both%20efficiency%20and%20generalization%20ability.%20The%0Aperformance%20of%20the%20proposed%20method%20has%20been%20comprehensively%20tested%20on%20various%0Aworkpieces%20featuring%20both%20linear%20and%20curved%20weld%20seams%20and%20in%20physical%0Aexperiment%20systems.%20The%20results%20showcase%20considerable%20potential%20for%20real-world%0Aindustrial%20applications%2C%20emphasizing%20the%20method%27s%20efficiency%20and%20effectiveness.%0AVideos%20of%20the%20real-world%20experiments%20can%20be%20found%20at%0Ahttps%3A//youtu.be/pq162HSP2D4.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoarse-to-Fine%2520Detection%2520of%2520Multiple%2520Seams%2520for%2520Robotic%2520Welding%26entry.906535625%3DPengkun%2520Wei%2520and%2520Shuo%2520Cheng%2520and%2520Dayou%2520Li%2520and%2520Ran%2520Song%2520and%2520Yipeng%2520Zhang%2520and%2520Wei%2520Zhang%26entry.1292438233%3D%2520%2520Efficiently%2520detecting%2520target%2520weld%2520seams%2520while%2520ensuring%2520sub-millimeter%250Aaccuracy%2520has%2520always%2520been%2520an%2520important%2520challenge%2520in%2520autonomous%2520welding%252C%2520which%250Ahas%2520significant%2520application%2520in%2520industrial%2520practice.%2520Previous%2520works%2520mostly%250Afocused%2520on%2520recognizing%2520and%2520localizing%2520welding%2520seams%2520one%2520by%2520one%252C%2520leading%2520to%250Ainferior%2520efficiency%2520in%2520modeling%2520the%2520workpiece.%2520This%2520paper%2520proposes%2520a%2520novel%250Aframework%2520capable%2520of%2520multiple%2520weld%2520seams%2520extraction%2520using%2520both%2520RGB%2520images%2520and%250A3D%2520point%2520clouds.%2520The%2520RGB%2520image%2520is%2520used%2520to%2520obtain%2520the%2520region%2520of%2520interest%2520by%250Aapproximately%2520localizing%2520the%2520weld%2520seams%252C%2520and%2520the%2520point%2520cloud%2520is%2520used%2520to%2520achieve%250Athe%2520fine-edge%2520extraction%2520of%2520the%2520weld%2520seams%2520within%2520the%2520region%2520of%2520interest%2520using%250Aregion%2520growth.%2520Our%2520method%2520is%2520further%2520accelerated%2520by%2520using%2520a%2520pre-trained%2520deep%250Alearning%2520model%2520to%2520ensure%2520both%2520efficiency%2520and%2520generalization%2520ability.%2520The%250Aperformance%2520of%2520the%2520proposed%2520method%2520has%2520been%2520comprehensively%2520tested%2520on%2520various%250Aworkpieces%2520featuring%2520both%2520linear%2520and%2520curved%2520weld%2520seams%2520and%2520in%2520physical%250Aexperiment%2520systems.%2520The%2520results%2520showcase%2520considerable%2520potential%2520for%2520real-world%250Aindustrial%2520applications%252C%2520emphasizing%2520the%2520method%2527s%2520efficiency%2520and%2520effectiveness.%250AVideos%2520of%2520the%2520real-world%2520experiments%2520can%2520be%2520found%2520at%250Ahttps%253A//youtu.be/pq162HSP2D4.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coarse-to-Fine%20Detection%20of%20Multiple%20Seams%20for%20Robotic%20Welding&entry.906535625=Pengkun%20Wei%20and%20Shuo%20Cheng%20and%20Dayou%20Li%20and%20Ran%20Song%20and%20Yipeng%20Zhang%20and%20Wei%20Zhang&entry.1292438233=%20%20Efficiently%20detecting%20target%20weld%20seams%20while%20ensuring%20sub-millimeter%0Aaccuracy%20has%20always%20been%20an%20important%20challenge%20in%20autonomous%20welding%2C%20which%0Ahas%20significant%20application%20in%20industrial%20practice.%20Previous%20works%20mostly%0Afocused%20on%20recognizing%20and%20localizing%20welding%20seams%20one%20by%20one%2C%20leading%20to%0Ainferior%20efficiency%20in%20modeling%20the%20workpiece.%20This%20paper%20proposes%20a%20novel%0Aframework%20capable%20of%20multiple%20weld%20seams%20extraction%20using%20both%20RGB%20images%20and%0A3D%20point%20clouds.%20The%20RGB%20image%20is%20used%20to%20obtain%20the%20region%20of%20interest%20by%0Aapproximately%20localizing%20the%20weld%20seams%2C%20and%20the%20point%20cloud%20is%20used%20to%20achieve%0Athe%20fine-edge%20extraction%20of%20the%20weld%20seams%20within%20the%20region%20of%20interest%20using%0Aregion%20growth.%20Our%20method%20is%20further%20accelerated%20by%20using%20a%20pre-trained%20deep%0Alearning%20model%20to%20ensure%20both%20efficiency%20and%20generalization%20ability.%20The%0Aperformance%20of%20the%20proposed%20method%20has%20been%20comprehensively%20tested%20on%20various%0Aworkpieces%20featuring%20both%20linear%20and%20curved%20weld%20seams%20and%20in%20physical%0Aexperiment%20systems.%20The%20results%20showcase%20considerable%20potential%20for%20real-world%0Aindustrial%20applications%2C%20emphasizing%20the%20method%27s%20efficiency%20and%20effectiveness.%0AVideos%20of%20the%20real-world%20experiments%20can%20be%20found%20at%0Ahttps%3A//youtu.be/pq162HSP2D4.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10710v1&entry.124074799=Read"},
{"title": "Segment, Select, Correct: A Framework for Weakly-Supervised Referring\n  Segmentation", "author": "Francisco Eiras and Kemal Oksuz and Adel Bibi and Philip H. S. Torr and Puneet K. Dokania", "abstract": "  Referring Image Segmentation (RIS) - the problem of identifying objects in\nimages through natural language sentences - is a challenging task currently\nmostly solved through supervised learning. However, while collecting referred\nannotation masks is a time-consuming process, the few existing\nweakly-supervised and zero-shot approaches fall significantly short in\nperformance compared to fully-supervised learning ones. To bridge the\nperformance gap without mask annotations, we propose a novel weakly-supervised\nframework that tackles RIS by decomposing it into three steps: obtaining\ninstance masks for the object mentioned in the referencing instruction\n(segment), using zero-shot learning to select a potentially correct mask for\nthe given instruction (select), and bootstrapping a model which allows for\nfixing the mistakes of zero-shot selection (correct). In our experiments, using\nonly the first two steps (zero-shot segment and select) outperforms other\nzero-shot baselines by as much as 16.5%, while our full method improves upon\nthis much stronger baseline and sets the new state-of-the-art for\nweakly-supervised RIS, reducing the gap between the weakly-supervised and\nfully-supervised methods in some cases from around 33% to as little as 7%. Code\nis available at https://github.com/fgirbal/segment-select-correct.\n", "link": "http://arxiv.org/abs/2310.13479v3", "date": "2024-08-20", "relevancy": 2.5453, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5109}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5086}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%2C%20Select%2C%20Correct%3A%20A%20Framework%20for%20Weakly-Supervised%20Referring%0A%20%20Segmentation&body=Title%3A%20Segment%2C%20Select%2C%20Correct%3A%20A%20Framework%20for%20Weakly-Supervised%20Referring%0A%20%20Segmentation%0AAuthor%3A%20Francisco%20Eiras%20and%20Kemal%20Oksuz%20and%20Adel%20Bibi%20and%20Philip%20H.%20S.%20Torr%20and%20Puneet%20K.%20Dokania%0AAbstract%3A%20%20%20Referring%20Image%20Segmentation%20%28RIS%29%20-%20the%20problem%20of%20identifying%20objects%20in%0Aimages%20through%20natural%20language%20sentences%20-%20is%20a%20challenging%20task%20currently%0Amostly%20solved%20through%20supervised%20learning.%20However%2C%20while%20collecting%20referred%0Aannotation%20masks%20is%20a%20time-consuming%20process%2C%20the%20few%20existing%0Aweakly-supervised%20and%20zero-shot%20approaches%20fall%20significantly%20short%20in%0Aperformance%20compared%20to%20fully-supervised%20learning%20ones.%20To%20bridge%20the%0Aperformance%20gap%20without%20mask%20annotations%2C%20we%20propose%20a%20novel%20weakly-supervised%0Aframework%20that%20tackles%20RIS%20by%20decomposing%20it%20into%20three%20steps%3A%20obtaining%0Ainstance%20masks%20for%20the%20object%20mentioned%20in%20the%20referencing%20instruction%0A%28segment%29%2C%20using%20zero-shot%20learning%20to%20select%20a%20potentially%20correct%20mask%20for%0Athe%20given%20instruction%20%28select%29%2C%20and%20bootstrapping%20a%20model%20which%20allows%20for%0Afixing%20the%20mistakes%20of%20zero-shot%20selection%20%28correct%29.%20In%20our%20experiments%2C%20using%0Aonly%20the%20first%20two%20steps%20%28zero-shot%20segment%20and%20select%29%20outperforms%20other%0Azero-shot%20baselines%20by%20as%20much%20as%2016.5%25%2C%20while%20our%20full%20method%20improves%20upon%0Athis%20much%20stronger%20baseline%20and%20sets%20the%20new%20state-of-the-art%20for%0Aweakly-supervised%20RIS%2C%20reducing%20the%20gap%20between%20the%20weakly-supervised%20and%0Afully-supervised%20methods%20in%20some%20cases%20from%20around%2033%25%20to%20as%20little%20as%207%25.%20Code%0Ais%20available%20at%20https%3A//github.com/fgirbal/segment-select-correct.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.13479v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%252C%2520Select%252C%2520Correct%253A%2520A%2520Framework%2520for%2520Weakly-Supervised%2520Referring%250A%2520%2520Segmentation%26entry.906535625%3DFrancisco%2520Eiras%2520and%2520Kemal%2520Oksuz%2520and%2520Adel%2520Bibi%2520and%2520Philip%2520H.%2520S.%2520Torr%2520and%2520Puneet%2520K.%2520Dokania%26entry.1292438233%3D%2520%2520Referring%2520Image%2520Segmentation%2520%2528RIS%2529%2520-%2520the%2520problem%2520of%2520identifying%2520objects%2520in%250Aimages%2520through%2520natural%2520language%2520sentences%2520-%2520is%2520a%2520challenging%2520task%2520currently%250Amostly%2520solved%2520through%2520supervised%2520learning.%2520However%252C%2520while%2520collecting%2520referred%250Aannotation%2520masks%2520is%2520a%2520time-consuming%2520process%252C%2520the%2520few%2520existing%250Aweakly-supervised%2520and%2520zero-shot%2520approaches%2520fall%2520significantly%2520short%2520in%250Aperformance%2520compared%2520to%2520fully-supervised%2520learning%2520ones.%2520To%2520bridge%2520the%250Aperformance%2520gap%2520without%2520mask%2520annotations%252C%2520we%2520propose%2520a%2520novel%2520weakly-supervised%250Aframework%2520that%2520tackles%2520RIS%2520by%2520decomposing%2520it%2520into%2520three%2520steps%253A%2520obtaining%250Ainstance%2520masks%2520for%2520the%2520object%2520mentioned%2520in%2520the%2520referencing%2520instruction%250A%2528segment%2529%252C%2520using%2520zero-shot%2520learning%2520to%2520select%2520a%2520potentially%2520correct%2520mask%2520for%250Athe%2520given%2520instruction%2520%2528select%2529%252C%2520and%2520bootstrapping%2520a%2520model%2520which%2520allows%2520for%250Afixing%2520the%2520mistakes%2520of%2520zero-shot%2520selection%2520%2528correct%2529.%2520In%2520our%2520experiments%252C%2520using%250Aonly%2520the%2520first%2520two%2520steps%2520%2528zero-shot%2520segment%2520and%2520select%2529%2520outperforms%2520other%250Azero-shot%2520baselines%2520by%2520as%2520much%2520as%252016.5%2525%252C%2520while%2520our%2520full%2520method%2520improves%2520upon%250Athis%2520much%2520stronger%2520baseline%2520and%2520sets%2520the%2520new%2520state-of-the-art%2520for%250Aweakly-supervised%2520RIS%252C%2520reducing%2520the%2520gap%2520between%2520the%2520weakly-supervised%2520and%250Afully-supervised%2520methods%2520in%2520some%2520cases%2520from%2520around%252033%2525%2520to%2520as%2520little%2520as%25207%2525.%2520Code%250Ais%2520available%2520at%2520https%253A//github.com/fgirbal/segment-select-correct.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.13479v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%2C%20Select%2C%20Correct%3A%20A%20Framework%20for%20Weakly-Supervised%20Referring%0A%20%20Segmentation&entry.906535625=Francisco%20Eiras%20and%20Kemal%20Oksuz%20and%20Adel%20Bibi%20and%20Philip%20H.%20S.%20Torr%20and%20Puneet%20K.%20Dokania&entry.1292438233=%20%20Referring%20Image%20Segmentation%20%28RIS%29%20-%20the%20problem%20of%20identifying%20objects%20in%0Aimages%20through%20natural%20language%20sentences%20-%20is%20a%20challenging%20task%20currently%0Amostly%20solved%20through%20supervised%20learning.%20However%2C%20while%20collecting%20referred%0Aannotation%20masks%20is%20a%20time-consuming%20process%2C%20the%20few%20existing%0Aweakly-supervised%20and%20zero-shot%20approaches%20fall%20significantly%20short%20in%0Aperformance%20compared%20to%20fully-supervised%20learning%20ones.%20To%20bridge%20the%0Aperformance%20gap%20without%20mask%20annotations%2C%20we%20propose%20a%20novel%20weakly-supervised%0Aframework%20that%20tackles%20RIS%20by%20decomposing%20it%20into%20three%20steps%3A%20obtaining%0Ainstance%20masks%20for%20the%20object%20mentioned%20in%20the%20referencing%20instruction%0A%28segment%29%2C%20using%20zero-shot%20learning%20to%20select%20a%20potentially%20correct%20mask%20for%0Athe%20given%20instruction%20%28select%29%2C%20and%20bootstrapping%20a%20model%20which%20allows%20for%0Afixing%20the%20mistakes%20of%20zero-shot%20selection%20%28correct%29.%20In%20our%20experiments%2C%20using%0Aonly%20the%20first%20two%20steps%20%28zero-shot%20segment%20and%20select%29%20outperforms%20other%0Azero-shot%20baselines%20by%20as%20much%20as%2016.5%25%2C%20while%20our%20full%20method%20improves%20upon%0Athis%20much%20stronger%20baseline%20and%20sets%20the%20new%20state-of-the-art%20for%0Aweakly-supervised%20RIS%2C%20reducing%20the%20gap%20between%20the%20weakly-supervised%20and%0Afully-supervised%20methods%20in%20some%20cases%20from%20around%2033%25%20to%20as%20little%20as%207%25.%20Code%0Ais%20available%20at%20https%3A//github.com/fgirbal/segment-select-correct.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.13479v3&entry.124074799=Read"},
{"title": "SenPa-MAE: Sensor Parameter Aware Masked Autoencoder for Multi-Satellite\n  Self-Supervised Pretraining", "author": "Jonathan Prexl and Michael Schmitt", "abstract": "  This paper introduces SenPa-MAE, a transformer architecture that encodes the\nsensor parameters of an observed multispectral signal into the image\nembeddings. SenPa-MAE can be pre-trained on imagery of different satellites\nwith non-matching spectral or geometrical sensor characteristics. To\nincorporate sensor parameters, we propose a versatile sensor parameter encoding\nmodule as well as a data augmentation strategy for the diversification of the\npre-training dataset. This enables the model to effectively differentiate\nbetween various sensors and gain an understanding of sensor parameters and the\ncorrelation to the observed signal. Given the rising number of Earth\nobservation satellite missions and the diversity in their sensor\nspecifications, our approach paves the way towards a sensor-independent Earth\nobservation foundation model. This opens up possibilities such as cross-sensor\ntraining and sensor-independent inference.\n", "link": "http://arxiv.org/abs/2408.11000v1", "date": "2024-08-20", "relevancy": 2.5285, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5143}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5124}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SenPa-MAE%3A%20Sensor%20Parameter%20Aware%20Masked%20Autoencoder%20for%20Multi-Satellite%0A%20%20Self-Supervised%20Pretraining&body=Title%3A%20SenPa-MAE%3A%20Sensor%20Parameter%20Aware%20Masked%20Autoencoder%20for%20Multi-Satellite%0A%20%20Self-Supervised%20Pretraining%0AAuthor%3A%20Jonathan%20Prexl%20and%20Michael%20Schmitt%0AAbstract%3A%20%20%20This%20paper%20introduces%20SenPa-MAE%2C%20a%20transformer%20architecture%20that%20encodes%20the%0Asensor%20parameters%20of%20an%20observed%20multispectral%20signal%20into%20the%20image%0Aembeddings.%20SenPa-MAE%20can%20be%20pre-trained%20on%20imagery%20of%20different%20satellites%0Awith%20non-matching%20spectral%20or%20geometrical%20sensor%20characteristics.%20To%0Aincorporate%20sensor%20parameters%2C%20we%20propose%20a%20versatile%20sensor%20parameter%20encoding%0Amodule%20as%20well%20as%20a%20data%20augmentation%20strategy%20for%20the%20diversification%20of%20the%0Apre-training%20dataset.%20This%20enables%20the%20model%20to%20effectively%20differentiate%0Abetween%20various%20sensors%20and%20gain%20an%20understanding%20of%20sensor%20parameters%20and%20the%0Acorrelation%20to%20the%20observed%20signal.%20Given%20the%20rising%20number%20of%20Earth%0Aobservation%20satellite%20missions%20and%20the%20diversity%20in%20their%20sensor%0Aspecifications%2C%20our%20approach%20paves%20the%20way%20towards%20a%20sensor-independent%20Earth%0Aobservation%20foundation%20model.%20This%20opens%20up%20possibilities%20such%20as%20cross-sensor%0Atraining%20and%20sensor-independent%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSenPa-MAE%253A%2520Sensor%2520Parameter%2520Aware%2520Masked%2520Autoencoder%2520for%2520Multi-Satellite%250A%2520%2520Self-Supervised%2520Pretraining%26entry.906535625%3DJonathan%2520Prexl%2520and%2520Michael%2520Schmitt%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520SenPa-MAE%252C%2520a%2520transformer%2520architecture%2520that%2520encodes%2520the%250Asensor%2520parameters%2520of%2520an%2520observed%2520multispectral%2520signal%2520into%2520the%2520image%250Aembeddings.%2520SenPa-MAE%2520can%2520be%2520pre-trained%2520on%2520imagery%2520of%2520different%2520satellites%250Awith%2520non-matching%2520spectral%2520or%2520geometrical%2520sensor%2520characteristics.%2520To%250Aincorporate%2520sensor%2520parameters%252C%2520we%2520propose%2520a%2520versatile%2520sensor%2520parameter%2520encoding%250Amodule%2520as%2520well%2520as%2520a%2520data%2520augmentation%2520strategy%2520for%2520the%2520diversification%2520of%2520the%250Apre-training%2520dataset.%2520This%2520enables%2520the%2520model%2520to%2520effectively%2520differentiate%250Abetween%2520various%2520sensors%2520and%2520gain%2520an%2520understanding%2520of%2520sensor%2520parameters%2520and%2520the%250Acorrelation%2520to%2520the%2520observed%2520signal.%2520Given%2520the%2520rising%2520number%2520of%2520Earth%250Aobservation%2520satellite%2520missions%2520and%2520the%2520diversity%2520in%2520their%2520sensor%250Aspecifications%252C%2520our%2520approach%2520paves%2520the%2520way%2520towards%2520a%2520sensor-independent%2520Earth%250Aobservation%2520foundation%2520model.%2520This%2520opens%2520up%2520possibilities%2520such%2520as%2520cross-sensor%250Atraining%2520and%2520sensor-independent%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SenPa-MAE%3A%20Sensor%20Parameter%20Aware%20Masked%20Autoencoder%20for%20Multi-Satellite%0A%20%20Self-Supervised%20Pretraining&entry.906535625=Jonathan%20Prexl%20and%20Michael%20Schmitt&entry.1292438233=%20%20This%20paper%20introduces%20SenPa-MAE%2C%20a%20transformer%20architecture%20that%20encodes%20the%0Asensor%20parameters%20of%20an%20observed%20multispectral%20signal%20into%20the%20image%0Aembeddings.%20SenPa-MAE%20can%20be%20pre-trained%20on%20imagery%20of%20different%20satellites%0Awith%20non-matching%20spectral%20or%20geometrical%20sensor%20characteristics.%20To%0Aincorporate%20sensor%20parameters%2C%20we%20propose%20a%20versatile%20sensor%20parameter%20encoding%0Amodule%20as%20well%20as%20a%20data%20augmentation%20strategy%20for%20the%20diversification%20of%20the%0Apre-training%20dataset.%20This%20enables%20the%20model%20to%20effectively%20differentiate%0Abetween%20various%20sensors%20and%20gain%20an%20understanding%20of%20sensor%20parameters%20and%20the%0Acorrelation%20to%20the%20observed%20signal.%20Given%20the%20rising%20number%20of%20Earth%0Aobservation%20satellite%20missions%20and%20the%20diversity%20in%20their%20sensor%0Aspecifications%2C%20our%20approach%20paves%20the%20way%20towards%20a%20sensor-independent%20Earth%0Aobservation%20foundation%20model.%20This%20opens%20up%20possibilities%20such%20as%20cross-sensor%0Atraining%20and%20sensor-independent%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11000v1&entry.124074799=Read"},
{"title": "MEGen: Generative Backdoor in Large Language Models via Model Editing", "author": "Jiyang Qiu and Xinbei Ma and Zhuosheng Zhang and Hai Zhao", "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities. Their\npowerful generative abilities enable flexible responses based on various\nqueries or instructions. Emerging as widely adopted generalists for diverse\ntasks, LLMs are still vulnerable to backdoors. This paper proposes an\nediting-based generative backdoor, named MEGen, aiming to create a customized\nbackdoor for NLP tasks with the least side effects. In our approach, we first\nleverage a language model to insert a trigger selected on fixed metrics into\nthe input, then design a pipeline of model editing to directly embed a backdoor\ninto an LLM. By adjusting a small set of local parameters with a mini-batch of\nsamples, MEGen significantly enhances time efficiency and achieves high\nrobustness. Experimental results indicate that our backdoor attack strategy\nachieves a high attack success rate on poison data while maintaining the\nmodel's performance on clean data. Notably, the backdoored model, when\ntriggered, can freely output pre-set dangerous information while successfully\ncompleting downstream tasks. This suggests that future LLM applications could\nbe guided to deliver certain dangerous information, thus altering the LLM's\ngenerative style. We believe this approach provides insights for future LLM\napplications and the execution of backdoor attacks on conversational AI\nsystems.\n", "link": "http://arxiv.org/abs/2408.10722v1", "date": "2024-08-20", "relevancy": 2.508, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5644}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4726}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEGen%3A%20Generative%20Backdoor%20in%20Large%20Language%20Models%20via%20Model%20Editing&body=Title%3A%20MEGen%3A%20Generative%20Backdoor%20in%20Large%20Language%20Models%20via%20Model%20Editing%0AAuthor%3A%20Jiyang%20Qiu%20and%20Xinbei%20Ma%20and%20Zhuosheng%20Zhang%20and%20Hai%20Zhao%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities.%20Their%0Apowerful%20generative%20abilities%20enable%20flexible%20responses%20based%20on%20various%0Aqueries%20or%20instructions.%20Emerging%20as%20widely%20adopted%20generalists%20for%20diverse%0Atasks%2C%20LLMs%20are%20still%20vulnerable%20to%20backdoors.%20This%20paper%20proposes%20an%0Aediting-based%20generative%20backdoor%2C%20named%20MEGen%2C%20aiming%20to%20create%20a%20customized%0Abackdoor%20for%20NLP%20tasks%20with%20the%20least%20side%20effects.%20In%20our%20approach%2C%20we%20first%0Aleverage%20a%20language%20model%20to%20insert%20a%20trigger%20selected%20on%20fixed%20metrics%20into%0Athe%20input%2C%20then%20design%20a%20pipeline%20of%20model%20editing%20to%20directly%20embed%20a%20backdoor%0Ainto%20an%20LLM.%20By%20adjusting%20a%20small%20set%20of%20local%20parameters%20with%20a%20mini-batch%20of%0Asamples%2C%20MEGen%20significantly%20enhances%20time%20efficiency%20and%20achieves%20high%0Arobustness.%20Experimental%20results%20indicate%20that%20our%20backdoor%20attack%20strategy%0Aachieves%20a%20high%20attack%20success%20rate%20on%20poison%20data%20while%20maintaining%20the%0Amodel%27s%20performance%20on%20clean%20data.%20Notably%2C%20the%20backdoored%20model%2C%20when%0Atriggered%2C%20can%20freely%20output%20pre-set%20dangerous%20information%20while%20successfully%0Acompleting%20downstream%20tasks.%20This%20suggests%20that%20future%20LLM%20applications%20could%0Abe%20guided%20to%20deliver%20certain%20dangerous%20information%2C%20thus%20altering%20the%20LLM%27s%0Agenerative%20style.%20We%20believe%20this%20approach%20provides%20insights%20for%20future%20LLM%0Aapplications%20and%20the%20execution%20of%20backdoor%20attacks%20on%20conversational%20AI%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEGen%253A%2520Generative%2520Backdoor%2520in%2520Large%2520Language%2520Models%2520via%2520Model%2520Editing%26entry.906535625%3DJiyang%2520Qiu%2520and%2520Xinbei%2520Ma%2520and%2520Zhuosheng%2520Zhang%2520and%2520Hai%2520Zhao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities.%2520Their%250Apowerful%2520generative%2520abilities%2520enable%2520flexible%2520responses%2520based%2520on%2520various%250Aqueries%2520or%2520instructions.%2520Emerging%2520as%2520widely%2520adopted%2520generalists%2520for%2520diverse%250Atasks%252C%2520LLMs%2520are%2520still%2520vulnerable%2520to%2520backdoors.%2520This%2520paper%2520proposes%2520an%250Aediting-based%2520generative%2520backdoor%252C%2520named%2520MEGen%252C%2520aiming%2520to%2520create%2520a%2520customized%250Abackdoor%2520for%2520NLP%2520tasks%2520with%2520the%2520least%2520side%2520effects.%2520In%2520our%2520approach%252C%2520we%2520first%250Aleverage%2520a%2520language%2520model%2520to%2520insert%2520a%2520trigger%2520selected%2520on%2520fixed%2520metrics%2520into%250Athe%2520input%252C%2520then%2520design%2520a%2520pipeline%2520of%2520model%2520editing%2520to%2520directly%2520embed%2520a%2520backdoor%250Ainto%2520an%2520LLM.%2520By%2520adjusting%2520a%2520small%2520set%2520of%2520local%2520parameters%2520with%2520a%2520mini-batch%2520of%250Asamples%252C%2520MEGen%2520significantly%2520enhances%2520time%2520efficiency%2520and%2520achieves%2520high%250Arobustness.%2520Experimental%2520results%2520indicate%2520that%2520our%2520backdoor%2520attack%2520strategy%250Aachieves%2520a%2520high%2520attack%2520success%2520rate%2520on%2520poison%2520data%2520while%2520maintaining%2520the%250Amodel%2527s%2520performance%2520on%2520clean%2520data.%2520Notably%252C%2520the%2520backdoored%2520model%252C%2520when%250Atriggered%252C%2520can%2520freely%2520output%2520pre-set%2520dangerous%2520information%2520while%2520successfully%250Acompleting%2520downstream%2520tasks.%2520This%2520suggests%2520that%2520future%2520LLM%2520applications%2520could%250Abe%2520guided%2520to%2520deliver%2520certain%2520dangerous%2520information%252C%2520thus%2520altering%2520the%2520LLM%2527s%250Agenerative%2520style.%2520We%2520believe%2520this%2520approach%2520provides%2520insights%2520for%2520future%2520LLM%250Aapplications%2520and%2520the%2520execution%2520of%2520backdoor%2520attacks%2520on%2520conversational%2520AI%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEGen%3A%20Generative%20Backdoor%20in%20Large%20Language%20Models%20via%20Model%20Editing&entry.906535625=Jiyang%20Qiu%20and%20Xinbei%20Ma%20and%20Zhuosheng%20Zhang%20and%20Hai%20Zhao&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities.%20Their%0Apowerful%20generative%20abilities%20enable%20flexible%20responses%20based%20on%20various%0Aqueries%20or%20instructions.%20Emerging%20as%20widely%20adopted%20generalists%20for%20diverse%0Atasks%2C%20LLMs%20are%20still%20vulnerable%20to%20backdoors.%20This%20paper%20proposes%20an%0Aediting-based%20generative%20backdoor%2C%20named%20MEGen%2C%20aiming%20to%20create%20a%20customized%0Abackdoor%20for%20NLP%20tasks%20with%20the%20least%20side%20effects.%20In%20our%20approach%2C%20we%20first%0Aleverage%20a%20language%20model%20to%20insert%20a%20trigger%20selected%20on%20fixed%20metrics%20into%0Athe%20input%2C%20then%20design%20a%20pipeline%20of%20model%20editing%20to%20directly%20embed%20a%20backdoor%0Ainto%20an%20LLM.%20By%20adjusting%20a%20small%20set%20of%20local%20parameters%20with%20a%20mini-batch%20of%0Asamples%2C%20MEGen%20significantly%20enhances%20time%20efficiency%20and%20achieves%20high%0Arobustness.%20Experimental%20results%20indicate%20that%20our%20backdoor%20attack%20strategy%0Aachieves%20a%20high%20attack%20success%20rate%20on%20poison%20data%20while%20maintaining%20the%0Amodel%27s%20performance%20on%20clean%20data.%20Notably%2C%20the%20backdoored%20model%2C%20when%0Atriggered%2C%20can%20freely%20output%20pre-set%20dangerous%20information%20while%20successfully%0Acompleting%20downstream%20tasks.%20This%20suggests%20that%20future%20LLM%20applications%20could%0Abe%20guided%20to%20deliver%20certain%20dangerous%20information%2C%20thus%20altering%20the%20LLM%27s%0Agenerative%20style.%20We%20believe%20this%20approach%20provides%20insights%20for%20future%20LLM%0Aapplications%20and%20the%20execution%20of%20backdoor%20attacks%20on%20conversational%20AI%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10722v1&entry.124074799=Read"},
{"title": "AnyGraph: Graph Foundation Model in the Wild", "author": "Lianghao Xia and Chao Huang", "abstract": "  The growing ubiquity of relational data structured as graphs has underscored\nthe need for graph learning models with exceptional generalization\ncapabilities. However, current approaches often struggle to effectively extract\ngeneralizable insights, frequently requiring extensive fine-tuning and limiting\ntheir versatility. Graph foundation models offer a transformative solution,\nwith the potential to learn robust, generalizable representations from graph\ndata. This enables more effective and adaptable applications across a wide\nspectrum of tasks and domains. In this work, we investigate a unified graph\nmodel, AnyGraph, designed to handle key challenges: i) Structure Heterogenity.\nAddressing distribution shift in graph structural information; ii) Feature\nHeterogenity. Handling diverse feature representation spaces across graph\ndatasets; iii) Fast Adaptation. Efficiently adapting the model to new graph\ndomains; iv) Scaling Law Emergence. Enabling the model to exhibit scaling law\nbehavior, where its performance scales favorably with the amount of data and\nparameter sizes. To tackle these critical challenges, we build the AnyGraph\nupon a Graph Mixture-of-Experts (MoE) architecture. This approach empowers the\nmodel to effectively manage both the in-domain and cross-domain distribution\nshift concerning structure-level and feature-level heterogeneity. Furthermore,\na lightweight graph expert routing mechanism is proposed to facilitate\nAnyGraph's fast adaptability to new data and domains. Our extensive experiments\non diverse 38 graph datasets have demonstrated the strong zero-shot learning\nperformance of AnyGraph across diverse graph domains with significant\ndistribution shift. Furthermore, we have validated the model's fast adaptation\nability and scaling law emergence, showcasing its versatility.\n", "link": "http://arxiv.org/abs/2408.10700v1", "date": "2024-08-20", "relevancy": 2.5079, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5083}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5008}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyGraph%3A%20Graph%20Foundation%20Model%20in%20the%20Wild&body=Title%3A%20AnyGraph%3A%20Graph%20Foundation%20Model%20in%20the%20Wild%0AAuthor%3A%20Lianghao%20Xia%20and%20Chao%20Huang%0AAbstract%3A%20%20%20The%20growing%20ubiquity%20of%20relational%20data%20structured%20as%20graphs%20has%20underscored%0Athe%20need%20for%20graph%20learning%20models%20with%20exceptional%20generalization%0Acapabilities.%20However%2C%20current%20approaches%20often%20struggle%20to%20effectively%20extract%0Ageneralizable%20insights%2C%20frequently%20requiring%20extensive%20fine-tuning%20and%20limiting%0Atheir%20versatility.%20Graph%20foundation%20models%20offer%20a%20transformative%20solution%2C%0Awith%20the%20potential%20to%20learn%20robust%2C%20generalizable%20representations%20from%20graph%0Adata.%20This%20enables%20more%20effective%20and%20adaptable%20applications%20across%20a%20wide%0Aspectrum%20of%20tasks%20and%20domains.%20In%20this%20work%2C%20we%20investigate%20a%20unified%20graph%0Amodel%2C%20AnyGraph%2C%20designed%20to%20handle%20key%20challenges%3A%20i%29%20Structure%20Heterogenity.%0AAddressing%20distribution%20shift%20in%20graph%20structural%20information%3B%20ii%29%20Feature%0AHeterogenity.%20Handling%20diverse%20feature%20representation%20spaces%20across%20graph%0Adatasets%3B%20iii%29%20Fast%20Adaptation.%20Efficiently%20adapting%20the%20model%20to%20new%20graph%0Adomains%3B%20iv%29%20Scaling%20Law%20Emergence.%20Enabling%20the%20model%20to%20exhibit%20scaling%20law%0Abehavior%2C%20where%20its%20performance%20scales%20favorably%20with%20the%20amount%20of%20data%20and%0Aparameter%20sizes.%20To%20tackle%20these%20critical%20challenges%2C%20we%20build%20the%20AnyGraph%0Aupon%20a%20Graph%20Mixture-of-Experts%20%28MoE%29%20architecture.%20This%20approach%20empowers%20the%0Amodel%20to%20effectively%20manage%20both%20the%20in-domain%20and%20cross-domain%20distribution%0Ashift%20concerning%20structure-level%20and%20feature-level%20heterogeneity.%20Furthermore%2C%0Aa%20lightweight%20graph%20expert%20routing%20mechanism%20is%20proposed%20to%20facilitate%0AAnyGraph%27s%20fast%20adaptability%20to%20new%20data%20and%20domains.%20Our%20extensive%20experiments%0Aon%20diverse%2038%20graph%20datasets%20have%20demonstrated%20the%20strong%20zero-shot%20learning%0Aperformance%20of%20AnyGraph%20across%20diverse%20graph%20domains%20with%20significant%0Adistribution%20shift.%20Furthermore%2C%20we%20have%20validated%20the%20model%27s%20fast%20adaptation%0Aability%20and%20scaling%20law%20emergence%2C%20showcasing%20its%20versatility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyGraph%253A%2520Graph%2520Foundation%2520Model%2520in%2520the%2520Wild%26entry.906535625%3DLianghao%2520Xia%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520The%2520growing%2520ubiquity%2520of%2520relational%2520data%2520structured%2520as%2520graphs%2520has%2520underscored%250Athe%2520need%2520for%2520graph%2520learning%2520models%2520with%2520exceptional%2520generalization%250Acapabilities.%2520However%252C%2520current%2520approaches%2520often%2520struggle%2520to%2520effectively%2520extract%250Ageneralizable%2520insights%252C%2520frequently%2520requiring%2520extensive%2520fine-tuning%2520and%2520limiting%250Atheir%2520versatility.%2520Graph%2520foundation%2520models%2520offer%2520a%2520transformative%2520solution%252C%250Awith%2520the%2520potential%2520to%2520learn%2520robust%252C%2520generalizable%2520representations%2520from%2520graph%250Adata.%2520This%2520enables%2520more%2520effective%2520and%2520adaptable%2520applications%2520across%2520a%2520wide%250Aspectrum%2520of%2520tasks%2520and%2520domains.%2520In%2520this%2520work%252C%2520we%2520investigate%2520a%2520unified%2520graph%250Amodel%252C%2520AnyGraph%252C%2520designed%2520to%2520handle%2520key%2520challenges%253A%2520i%2529%2520Structure%2520Heterogenity.%250AAddressing%2520distribution%2520shift%2520in%2520graph%2520structural%2520information%253B%2520ii%2529%2520Feature%250AHeterogenity.%2520Handling%2520diverse%2520feature%2520representation%2520spaces%2520across%2520graph%250Adatasets%253B%2520iii%2529%2520Fast%2520Adaptation.%2520Efficiently%2520adapting%2520the%2520model%2520to%2520new%2520graph%250Adomains%253B%2520iv%2529%2520Scaling%2520Law%2520Emergence.%2520Enabling%2520the%2520model%2520to%2520exhibit%2520scaling%2520law%250Abehavior%252C%2520where%2520its%2520performance%2520scales%2520favorably%2520with%2520the%2520amount%2520of%2520data%2520and%250Aparameter%2520sizes.%2520To%2520tackle%2520these%2520critical%2520challenges%252C%2520we%2520build%2520the%2520AnyGraph%250Aupon%2520a%2520Graph%2520Mixture-of-Experts%2520%2528MoE%2529%2520architecture.%2520This%2520approach%2520empowers%2520the%250Amodel%2520to%2520effectively%2520manage%2520both%2520the%2520in-domain%2520and%2520cross-domain%2520distribution%250Ashift%2520concerning%2520structure-level%2520and%2520feature-level%2520heterogeneity.%2520Furthermore%252C%250Aa%2520lightweight%2520graph%2520expert%2520routing%2520mechanism%2520is%2520proposed%2520to%2520facilitate%250AAnyGraph%2527s%2520fast%2520adaptability%2520to%2520new%2520data%2520and%2520domains.%2520Our%2520extensive%2520experiments%250Aon%2520diverse%252038%2520graph%2520datasets%2520have%2520demonstrated%2520the%2520strong%2520zero-shot%2520learning%250Aperformance%2520of%2520AnyGraph%2520across%2520diverse%2520graph%2520domains%2520with%2520significant%250Adistribution%2520shift.%2520Furthermore%252C%2520we%2520have%2520validated%2520the%2520model%2527s%2520fast%2520adaptation%250Aability%2520and%2520scaling%2520law%2520emergence%252C%2520showcasing%2520its%2520versatility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyGraph%3A%20Graph%20Foundation%20Model%20in%20the%20Wild&entry.906535625=Lianghao%20Xia%20and%20Chao%20Huang&entry.1292438233=%20%20The%20growing%20ubiquity%20of%20relational%20data%20structured%20as%20graphs%20has%20underscored%0Athe%20need%20for%20graph%20learning%20models%20with%20exceptional%20generalization%0Acapabilities.%20However%2C%20current%20approaches%20often%20struggle%20to%20effectively%20extract%0Ageneralizable%20insights%2C%20frequently%20requiring%20extensive%20fine-tuning%20and%20limiting%0Atheir%20versatility.%20Graph%20foundation%20models%20offer%20a%20transformative%20solution%2C%0Awith%20the%20potential%20to%20learn%20robust%2C%20generalizable%20representations%20from%20graph%0Adata.%20This%20enables%20more%20effective%20and%20adaptable%20applications%20across%20a%20wide%0Aspectrum%20of%20tasks%20and%20domains.%20In%20this%20work%2C%20we%20investigate%20a%20unified%20graph%0Amodel%2C%20AnyGraph%2C%20designed%20to%20handle%20key%20challenges%3A%20i%29%20Structure%20Heterogenity.%0AAddressing%20distribution%20shift%20in%20graph%20structural%20information%3B%20ii%29%20Feature%0AHeterogenity.%20Handling%20diverse%20feature%20representation%20spaces%20across%20graph%0Adatasets%3B%20iii%29%20Fast%20Adaptation.%20Efficiently%20adapting%20the%20model%20to%20new%20graph%0Adomains%3B%20iv%29%20Scaling%20Law%20Emergence.%20Enabling%20the%20model%20to%20exhibit%20scaling%20law%0Abehavior%2C%20where%20its%20performance%20scales%20favorably%20with%20the%20amount%20of%20data%20and%0Aparameter%20sizes.%20To%20tackle%20these%20critical%20challenges%2C%20we%20build%20the%20AnyGraph%0Aupon%20a%20Graph%20Mixture-of-Experts%20%28MoE%29%20architecture.%20This%20approach%20empowers%20the%0Amodel%20to%20effectively%20manage%20both%20the%20in-domain%20and%20cross-domain%20distribution%0Ashift%20concerning%20structure-level%20and%20feature-level%20heterogeneity.%20Furthermore%2C%0Aa%20lightweight%20graph%20expert%20routing%20mechanism%20is%20proposed%20to%20facilitate%0AAnyGraph%27s%20fast%20adaptability%20to%20new%20data%20and%20domains.%20Our%20extensive%20experiments%0Aon%20diverse%2038%20graph%20datasets%20have%20demonstrated%20the%20strong%20zero-shot%20learning%0Aperformance%20of%20AnyGraph%20across%20diverse%20graph%20domains%20with%20significant%0Adistribution%20shift.%20Furthermore%2C%20we%20have%20validated%20the%20model%27s%20fast%20adaptation%0Aability%20and%20scaling%20law%20emergence%2C%20showcasing%20its%20versatility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10700v1&entry.124074799=Read"},
{"title": "Generative AI in Industrial Machine Vision -- A Review", "author": "Hans Aoyang Zhou and Dominik Wolfschl\u00e4ger and Constantinos Florides and Jonas Werheid and Hannes Behnen and Jan-Henrick Woltersmann and Tiago C. Pinto and Marco Kemmerling and Anas Abdelrazeq and Robert H. Schmitt", "abstract": "  Machine vision enhances automation, quality control, and operational\nefficiency in industrial applications by enabling machines to interpret and act\non visual data. While traditional computer vision algorithms and approaches\nremain widely utilized, machine learning has become pivotal in current research\nactivities. In particular, generative \\gls*{AI} demonstrates promising\npotential by improving pattern recognition capabilities, through data\naugmentation, increasing image resolution, and identifying anomalies for\nquality control. However, the application of generative \\gls*{AI} in machine\nvision is still in its early stages due to challenges in data diversity,\ncomputational requirements, and the necessity for robust validation methods. A\ncomprehensive literature review is essential to understand the current state of\ngenerative \\gls*{AI} in industrial machine vision, focusing on recent\nadvancements, applications, and research trends. Thus, a literature review\nbased on the PRISMA guidelines was conducted, analyzing over 1,200 papers on\ngenerative \\gls*{AI} in industrial machine vision. Our findings reveal various\npatterns in current research, with the primary use of generative \\gls*{AI}\nbeing data augmentation, for machine vision tasks such as classification and\nobject detection. Furthermore, we gather a collection of application challenges\ntogether with data requirements to enable a successful application of\ngenerative \\gls*{AI} in industrial machine vision. This overview aims to\nprovide researchers with insights into the different areas and applications\nwithin current research, highlighting significant advancements and identifying\nopportunities for future work.\n", "link": "http://arxiv.org/abs/2408.10775v1", "date": "2024-08-20", "relevancy": 2.4891, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.518}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.492}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20in%20Industrial%20Machine%20Vision%20--%20A%20Review&body=Title%3A%20Generative%20AI%20in%20Industrial%20Machine%20Vision%20--%20A%20Review%0AAuthor%3A%20Hans%20Aoyang%20Zhou%20and%20Dominik%20Wolfschl%C3%A4ger%20and%20Constantinos%20Florides%20and%20Jonas%20Werheid%20and%20Hannes%20Behnen%20and%20Jan-Henrick%20Woltersmann%20and%20Tiago%20C.%20Pinto%20and%20Marco%20Kemmerling%20and%20Anas%20Abdelrazeq%20and%20Robert%20H.%20Schmitt%0AAbstract%3A%20%20%20Machine%20vision%20enhances%20automation%2C%20quality%20control%2C%20and%20operational%0Aefficiency%20in%20industrial%20applications%20by%20enabling%20machines%20to%20interpret%20and%20act%0Aon%20visual%20data.%20While%20traditional%20computer%20vision%20algorithms%20and%20approaches%0Aremain%20widely%20utilized%2C%20machine%20learning%20has%20become%20pivotal%20in%20current%20research%0Aactivities.%20In%20particular%2C%20generative%20%5Cgls%2A%7BAI%7D%20demonstrates%20promising%0Apotential%20by%20improving%20pattern%20recognition%20capabilities%2C%20through%20data%0Aaugmentation%2C%20increasing%20image%20resolution%2C%20and%20identifying%20anomalies%20for%0Aquality%20control.%20However%2C%20the%20application%20of%20generative%20%5Cgls%2A%7BAI%7D%20in%20machine%0Avision%20is%20still%20in%20its%20early%20stages%20due%20to%20challenges%20in%20data%20diversity%2C%0Acomputational%20requirements%2C%20and%20the%20necessity%20for%20robust%20validation%20methods.%20A%0Acomprehensive%20literature%20review%20is%20essential%20to%20understand%20the%20current%20state%20of%0Agenerative%20%5Cgls%2A%7BAI%7D%20in%20industrial%20machine%20vision%2C%20focusing%20on%20recent%0Aadvancements%2C%20applications%2C%20and%20research%20trends.%20Thus%2C%20a%20literature%20review%0Abased%20on%20the%20PRISMA%20guidelines%20was%20conducted%2C%20analyzing%20over%201%2C200%20papers%20on%0Agenerative%20%5Cgls%2A%7BAI%7D%20in%20industrial%20machine%20vision.%20Our%20findings%20reveal%20various%0Apatterns%20in%20current%20research%2C%20with%20the%20primary%20use%20of%20generative%20%5Cgls%2A%7BAI%7D%0Abeing%20data%20augmentation%2C%20for%20machine%20vision%20tasks%20such%20as%20classification%20and%0Aobject%20detection.%20Furthermore%2C%20we%20gather%20a%20collection%20of%20application%20challenges%0Atogether%20with%20data%20requirements%20to%20enable%20a%20successful%20application%20of%0Agenerative%20%5Cgls%2A%7BAI%7D%20in%20industrial%20machine%20vision.%20This%20overview%20aims%20to%0Aprovide%20researchers%20with%20insights%20into%20the%20different%20areas%20and%20applications%0Awithin%20current%20research%2C%20highlighting%20significant%20advancements%20and%20identifying%0Aopportunities%20for%20future%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520in%2520Industrial%2520Machine%2520Vision%2520--%2520A%2520Review%26entry.906535625%3DHans%2520Aoyang%2520Zhou%2520and%2520Dominik%2520Wolfschl%25C3%25A4ger%2520and%2520Constantinos%2520Florides%2520and%2520Jonas%2520Werheid%2520and%2520Hannes%2520Behnen%2520and%2520Jan-Henrick%2520Woltersmann%2520and%2520Tiago%2520C.%2520Pinto%2520and%2520Marco%2520Kemmerling%2520and%2520Anas%2520Abdelrazeq%2520and%2520Robert%2520H.%2520Schmitt%26entry.1292438233%3D%2520%2520Machine%2520vision%2520enhances%2520automation%252C%2520quality%2520control%252C%2520and%2520operational%250Aefficiency%2520in%2520industrial%2520applications%2520by%2520enabling%2520machines%2520to%2520interpret%2520and%2520act%250Aon%2520visual%2520data.%2520While%2520traditional%2520computer%2520vision%2520algorithms%2520and%2520approaches%250Aremain%2520widely%2520utilized%252C%2520machine%2520learning%2520has%2520become%2520pivotal%2520in%2520current%2520research%250Aactivities.%2520In%2520particular%252C%2520generative%2520%255Cgls%252A%257BAI%257D%2520demonstrates%2520promising%250Apotential%2520by%2520improving%2520pattern%2520recognition%2520capabilities%252C%2520through%2520data%250Aaugmentation%252C%2520increasing%2520image%2520resolution%252C%2520and%2520identifying%2520anomalies%2520for%250Aquality%2520control.%2520However%252C%2520the%2520application%2520of%2520generative%2520%255Cgls%252A%257BAI%257D%2520in%2520machine%250Avision%2520is%2520still%2520in%2520its%2520early%2520stages%2520due%2520to%2520challenges%2520in%2520data%2520diversity%252C%250Acomputational%2520requirements%252C%2520and%2520the%2520necessity%2520for%2520robust%2520validation%2520methods.%2520A%250Acomprehensive%2520literature%2520review%2520is%2520essential%2520to%2520understand%2520the%2520current%2520state%2520of%250Agenerative%2520%255Cgls%252A%257BAI%257D%2520in%2520industrial%2520machine%2520vision%252C%2520focusing%2520on%2520recent%250Aadvancements%252C%2520applications%252C%2520and%2520research%2520trends.%2520Thus%252C%2520a%2520literature%2520review%250Abased%2520on%2520the%2520PRISMA%2520guidelines%2520was%2520conducted%252C%2520analyzing%2520over%25201%252C200%2520papers%2520on%250Agenerative%2520%255Cgls%252A%257BAI%257D%2520in%2520industrial%2520machine%2520vision.%2520Our%2520findings%2520reveal%2520various%250Apatterns%2520in%2520current%2520research%252C%2520with%2520the%2520primary%2520use%2520of%2520generative%2520%255Cgls%252A%257BAI%257D%250Abeing%2520data%2520augmentation%252C%2520for%2520machine%2520vision%2520tasks%2520such%2520as%2520classification%2520and%250Aobject%2520detection.%2520Furthermore%252C%2520we%2520gather%2520a%2520collection%2520of%2520application%2520challenges%250Atogether%2520with%2520data%2520requirements%2520to%2520enable%2520a%2520successful%2520application%2520of%250Agenerative%2520%255Cgls%252A%257BAI%257D%2520in%2520industrial%2520machine%2520vision.%2520This%2520overview%2520aims%2520to%250Aprovide%2520researchers%2520with%2520insights%2520into%2520the%2520different%2520areas%2520and%2520applications%250Awithin%2520current%2520research%252C%2520highlighting%2520significant%2520advancements%2520and%2520identifying%250Aopportunities%2520for%2520future%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20in%20Industrial%20Machine%20Vision%20--%20A%20Review&entry.906535625=Hans%20Aoyang%20Zhou%20and%20Dominik%20Wolfschl%C3%A4ger%20and%20Constantinos%20Florides%20and%20Jonas%20Werheid%20and%20Hannes%20Behnen%20and%20Jan-Henrick%20Woltersmann%20and%20Tiago%20C.%20Pinto%20and%20Marco%20Kemmerling%20and%20Anas%20Abdelrazeq%20and%20Robert%20H.%20Schmitt&entry.1292438233=%20%20Machine%20vision%20enhances%20automation%2C%20quality%20control%2C%20and%20operational%0Aefficiency%20in%20industrial%20applications%20by%20enabling%20machines%20to%20interpret%20and%20act%0Aon%20visual%20data.%20While%20traditional%20computer%20vision%20algorithms%20and%20approaches%0Aremain%20widely%20utilized%2C%20machine%20learning%20has%20become%20pivotal%20in%20current%20research%0Aactivities.%20In%20particular%2C%20generative%20%5Cgls%2A%7BAI%7D%20demonstrates%20promising%0Apotential%20by%20improving%20pattern%20recognition%20capabilities%2C%20through%20data%0Aaugmentation%2C%20increasing%20image%20resolution%2C%20and%20identifying%20anomalies%20for%0Aquality%20control.%20However%2C%20the%20application%20of%20generative%20%5Cgls%2A%7BAI%7D%20in%20machine%0Avision%20is%20still%20in%20its%20early%20stages%20due%20to%20challenges%20in%20data%20diversity%2C%0Acomputational%20requirements%2C%20and%20the%20necessity%20for%20robust%20validation%20methods.%20A%0Acomprehensive%20literature%20review%20is%20essential%20to%20understand%20the%20current%20state%20of%0Agenerative%20%5Cgls%2A%7BAI%7D%20in%20industrial%20machine%20vision%2C%20focusing%20on%20recent%0Aadvancements%2C%20applications%2C%20and%20research%20trends.%20Thus%2C%20a%20literature%20review%0Abased%20on%20the%20PRISMA%20guidelines%20was%20conducted%2C%20analyzing%20over%201%2C200%20papers%20on%0Agenerative%20%5Cgls%2A%7BAI%7D%20in%20industrial%20machine%20vision.%20Our%20findings%20reveal%20various%0Apatterns%20in%20current%20research%2C%20with%20the%20primary%20use%20of%20generative%20%5Cgls%2A%7BAI%7D%0Abeing%20data%20augmentation%2C%20for%20machine%20vision%20tasks%20such%20as%20classification%20and%0Aobject%20detection.%20Furthermore%2C%20we%20gather%20a%20collection%20of%20application%20challenges%0Atogether%20with%20data%20requirements%20to%20enable%20a%20successful%20application%20of%0Agenerative%20%5Cgls%2A%7BAI%7D%20in%20industrial%20machine%20vision.%20This%20overview%20aims%20to%0Aprovide%20researchers%20with%20insights%20into%20the%20different%20areas%20and%20applications%0Awithin%20current%20research%2C%20highlighting%20significant%20advancements%20and%20identifying%0Aopportunities%20for%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10775v1&entry.124074799=Read"},
{"title": "Model Stealing Attack against Graph Classification with Authenticity,\n  Uncertainty and Diversity", "author": "Zhihao Zhu and Chenwang Wu and Rui Fan and Yi Yang and Zhen Wang and Defu Lian and Enhong Chen", "abstract": "  Recent research demonstrates that GNNs are vulnerable to the model stealing\nattack, a nefarious endeavor geared towards duplicating the target model via\nquery permissions. However, they mainly focus on node classification tasks,\nneglecting the potential threats entailed within the domain of graph\nclassification tasks. Furthermore, their practicality is questionable due to\nunreasonable assumptions, specifically concerning the large data requirements\nand extensive model knowledge. To this end, we advocate following strict\nsettings with limited real data and hard-label awareness to generate synthetic\ndata, thereby facilitating the stealing of the target model. Specifically,\nfollowing important data generation principles, we introduce three model\nstealing attacks to adapt to different actual scenarios: MSA-AU is inspired by\nactive learning and emphasizes the uncertainty to enhance query value of\ngenerated samples; MSA-AD introduces diversity based on Mixup augmentation\nstrategy to alleviate the query inefficiency issue caused by over-similar\nsamples generated by MSA-AU; MSA-AUD combines the above two strategies to\nseamlessly integrate the authenticity, uncertainty, and diversity of the\ngenerated samples. Finally, extensive experiments consistently demonstrate the\nsuperiority of the proposed methods in terms of concealment, query efficiency,\nand stealing performance.\n", "link": "http://arxiv.org/abs/2312.10943v3", "date": "2024-08-20", "relevancy": 2.4608, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5034}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4973}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Stealing%20Attack%20against%20Graph%20Classification%20with%20Authenticity%2C%0A%20%20Uncertainty%20and%20Diversity&body=Title%3A%20Model%20Stealing%20Attack%20against%20Graph%20Classification%20with%20Authenticity%2C%0A%20%20Uncertainty%20and%20Diversity%0AAuthor%3A%20Zhihao%20Zhu%20and%20Chenwang%20Wu%20and%20Rui%20Fan%20and%20Yi%20Yang%20and%20Zhen%20Wang%20and%20Defu%20Lian%20and%20Enhong%20Chen%0AAbstract%3A%20%20%20Recent%20research%20demonstrates%20that%20GNNs%20are%20vulnerable%20to%20the%20model%20stealing%0Aattack%2C%20a%20nefarious%20endeavor%20geared%20towards%20duplicating%20the%20target%20model%20via%0Aquery%20permissions.%20However%2C%20they%20mainly%20focus%20on%20node%20classification%20tasks%2C%0Aneglecting%20the%20potential%20threats%20entailed%20within%20the%20domain%20of%20graph%0Aclassification%20tasks.%20Furthermore%2C%20their%20practicality%20is%20questionable%20due%20to%0Aunreasonable%20assumptions%2C%20specifically%20concerning%20the%20large%20data%20requirements%0Aand%20extensive%20model%20knowledge.%20To%20this%20end%2C%20we%20advocate%20following%20strict%0Asettings%20with%20limited%20real%20data%20and%20hard-label%20awareness%20to%20generate%20synthetic%0Adata%2C%20thereby%20facilitating%20the%20stealing%20of%20the%20target%20model.%20Specifically%2C%0Afollowing%20important%20data%20generation%20principles%2C%20we%20introduce%20three%20model%0Astealing%20attacks%20to%20adapt%20to%20different%20actual%20scenarios%3A%20MSA-AU%20is%20inspired%20by%0Aactive%20learning%20and%20emphasizes%20the%20uncertainty%20to%20enhance%20query%20value%20of%0Agenerated%20samples%3B%20MSA-AD%20introduces%20diversity%20based%20on%20Mixup%20augmentation%0Astrategy%20to%20alleviate%20the%20query%20inefficiency%20issue%20caused%20by%20over-similar%0Asamples%20generated%20by%20MSA-AU%3B%20MSA-AUD%20combines%20the%20above%20two%20strategies%20to%0Aseamlessly%20integrate%20the%20authenticity%2C%20uncertainty%2C%20and%20diversity%20of%20the%0Agenerated%20samples.%20Finally%2C%20extensive%20experiments%20consistently%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20methods%20in%20terms%20of%20concealment%2C%20query%20efficiency%2C%0Aand%20stealing%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10943v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Stealing%2520Attack%2520against%2520Graph%2520Classification%2520with%2520Authenticity%252C%250A%2520%2520Uncertainty%2520and%2520Diversity%26entry.906535625%3DZhihao%2520Zhu%2520and%2520Chenwang%2520Wu%2520and%2520Rui%2520Fan%2520and%2520Yi%2520Yang%2520and%2520Zhen%2520Wang%2520and%2520Defu%2520Lian%2520and%2520Enhong%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520research%2520demonstrates%2520that%2520GNNs%2520are%2520vulnerable%2520to%2520the%2520model%2520stealing%250Aattack%252C%2520a%2520nefarious%2520endeavor%2520geared%2520towards%2520duplicating%2520the%2520target%2520model%2520via%250Aquery%2520permissions.%2520However%252C%2520they%2520mainly%2520focus%2520on%2520node%2520classification%2520tasks%252C%250Aneglecting%2520the%2520potential%2520threats%2520entailed%2520within%2520the%2520domain%2520of%2520graph%250Aclassification%2520tasks.%2520Furthermore%252C%2520their%2520practicality%2520is%2520questionable%2520due%2520to%250Aunreasonable%2520assumptions%252C%2520specifically%2520concerning%2520the%2520large%2520data%2520requirements%250Aand%2520extensive%2520model%2520knowledge.%2520To%2520this%2520end%252C%2520we%2520advocate%2520following%2520strict%250Asettings%2520with%2520limited%2520real%2520data%2520and%2520hard-label%2520awareness%2520to%2520generate%2520synthetic%250Adata%252C%2520thereby%2520facilitating%2520the%2520stealing%2520of%2520the%2520target%2520model.%2520Specifically%252C%250Afollowing%2520important%2520data%2520generation%2520principles%252C%2520we%2520introduce%2520three%2520model%250Astealing%2520attacks%2520to%2520adapt%2520to%2520different%2520actual%2520scenarios%253A%2520MSA-AU%2520is%2520inspired%2520by%250Aactive%2520learning%2520and%2520emphasizes%2520the%2520uncertainty%2520to%2520enhance%2520query%2520value%2520of%250Agenerated%2520samples%253B%2520MSA-AD%2520introduces%2520diversity%2520based%2520on%2520Mixup%2520augmentation%250Astrategy%2520to%2520alleviate%2520the%2520query%2520inefficiency%2520issue%2520caused%2520by%2520over-similar%250Asamples%2520generated%2520by%2520MSA-AU%253B%2520MSA-AUD%2520combines%2520the%2520above%2520two%2520strategies%2520to%250Aseamlessly%2520integrate%2520the%2520authenticity%252C%2520uncertainty%252C%2520and%2520diversity%2520of%2520the%250Agenerated%2520samples.%2520Finally%252C%2520extensive%2520experiments%2520consistently%2520demonstrate%2520the%250Asuperiority%2520of%2520the%2520proposed%2520methods%2520in%2520terms%2520of%2520concealment%252C%2520query%2520efficiency%252C%250Aand%2520stealing%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10943v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Stealing%20Attack%20against%20Graph%20Classification%20with%20Authenticity%2C%0A%20%20Uncertainty%20and%20Diversity&entry.906535625=Zhihao%20Zhu%20and%20Chenwang%20Wu%20and%20Rui%20Fan%20and%20Yi%20Yang%20and%20Zhen%20Wang%20and%20Defu%20Lian%20and%20Enhong%20Chen&entry.1292438233=%20%20Recent%20research%20demonstrates%20that%20GNNs%20are%20vulnerable%20to%20the%20model%20stealing%0Aattack%2C%20a%20nefarious%20endeavor%20geared%20towards%20duplicating%20the%20target%20model%20via%0Aquery%20permissions.%20However%2C%20they%20mainly%20focus%20on%20node%20classification%20tasks%2C%0Aneglecting%20the%20potential%20threats%20entailed%20within%20the%20domain%20of%20graph%0Aclassification%20tasks.%20Furthermore%2C%20their%20practicality%20is%20questionable%20due%20to%0Aunreasonable%20assumptions%2C%20specifically%20concerning%20the%20large%20data%20requirements%0Aand%20extensive%20model%20knowledge.%20To%20this%20end%2C%20we%20advocate%20following%20strict%0Asettings%20with%20limited%20real%20data%20and%20hard-label%20awareness%20to%20generate%20synthetic%0Adata%2C%20thereby%20facilitating%20the%20stealing%20of%20the%20target%20model.%20Specifically%2C%0Afollowing%20important%20data%20generation%20principles%2C%20we%20introduce%20three%20model%0Astealing%20attacks%20to%20adapt%20to%20different%20actual%20scenarios%3A%20MSA-AU%20is%20inspired%20by%0Aactive%20learning%20and%20emphasizes%20the%20uncertainty%20to%20enhance%20query%20value%20of%0Agenerated%20samples%3B%20MSA-AD%20introduces%20diversity%20based%20on%20Mixup%20augmentation%0Astrategy%20to%20alleviate%20the%20query%20inefficiency%20issue%20caused%20by%20over-similar%0Asamples%20generated%20by%20MSA-AU%3B%20MSA-AUD%20combines%20the%20above%20two%20strategies%20to%0Aseamlessly%20integrate%20the%20authenticity%2C%20uncertainty%2C%20and%20diversity%20of%20the%0Agenerated%20samples.%20Finally%2C%20extensive%20experiments%20consistently%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20methods%20in%20terms%20of%20concealment%2C%20query%20efficiency%2C%0Aand%20stealing%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10943v3&entry.124074799=Read"},
{"title": "Perception-guided Jailbreak against Text-to-Image Models", "author": "Yihao Huang and Le Liang and Tianlin Li and Xiaojun Jia and Run Wang and Weikai Miao and Geguang Pu and Yang Liu", "abstract": "  In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ.\n", "link": "http://arxiv.org/abs/2408.10848v1", "date": "2024-08-20", "relevancy": 2.4559, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5052}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4906}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception-guided%20Jailbreak%20against%20Text-to-Image%20Models&body=Title%3A%20Perception-guided%20Jailbreak%20against%20Text-to-Image%20Models%0AAuthor%3A%20Yihao%20Huang%20and%20Le%20Liang%20and%20Tianlin%20Li%20and%20Xiaojun%20Jia%20and%20Run%20Wang%20and%20Weikai%20Miao%20and%20Geguang%20Pu%20and%20Yang%20Liu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Text-to-Image%20%28T2I%29%20models%20have%20garnered%20significant%0Aattention%20due%20to%20their%20remarkable%20advancements.%20However%2C%20security%20concerns%20have%0Aemerged%20due%20to%20their%20potential%20to%20generate%20inappropriate%20or%20Not-Safe-For-Work%0A%28NSFW%29%20images.%20In%20this%20paper%2C%20inspired%20by%20the%20observation%20that%20texts%20with%0Adifferent%20semantics%20can%20lead%20to%20similar%20human%20perceptions%2C%20we%20propose%20an%0ALLM-driven%20perception-guided%20jailbreak%20method%2C%20termed%20PGJ.%20It%20is%20a%20black-box%0Ajailbreak%20method%20that%20requires%20no%20specific%20T2I%20model%20%28model-free%29%20and%20generates%0Ahighly%20natural%20attack%20prompts.%20Specifically%2C%20we%20propose%20identifying%20a%20safe%0Aphrase%20that%20is%20similar%20in%20human%20perception%20yet%20inconsistent%20in%20text%20semantics%0Awith%20the%20target%20unsafe%20word%20and%20using%20it%20as%20a%20substitution.%20The%20experiments%0Aconducted%20on%20six%20open-source%20models%20and%20commercial%20online%20services%20with%0Athousands%20of%20prompts%20have%20verified%20the%20effectiveness%20of%20PGJ.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception-guided%2520Jailbreak%2520against%2520Text-to-Image%2520Models%26entry.906535625%3DYihao%2520Huang%2520and%2520Le%2520Liang%2520and%2520Tianlin%2520Li%2520and%2520Xiaojun%2520Jia%2520and%2520Run%2520Wang%2520and%2520Weikai%2520Miao%2520and%2520Geguang%2520Pu%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Text-to-Image%2520%2528T2I%2529%2520models%2520have%2520garnered%2520significant%250Aattention%2520due%2520to%2520their%2520remarkable%2520advancements.%2520However%252C%2520security%2520concerns%2520have%250Aemerged%2520due%2520to%2520their%2520potential%2520to%2520generate%2520inappropriate%2520or%2520Not-Safe-For-Work%250A%2528NSFW%2529%2520images.%2520In%2520this%2520paper%252C%2520inspired%2520by%2520the%2520observation%2520that%2520texts%2520with%250Adifferent%2520semantics%2520can%2520lead%2520to%2520similar%2520human%2520perceptions%252C%2520we%2520propose%2520an%250ALLM-driven%2520perception-guided%2520jailbreak%2520method%252C%2520termed%2520PGJ.%2520It%2520is%2520a%2520black-box%250Ajailbreak%2520method%2520that%2520requires%2520no%2520specific%2520T2I%2520model%2520%2528model-free%2529%2520and%2520generates%250Ahighly%2520natural%2520attack%2520prompts.%2520Specifically%252C%2520we%2520propose%2520identifying%2520a%2520safe%250Aphrase%2520that%2520is%2520similar%2520in%2520human%2520perception%2520yet%2520inconsistent%2520in%2520text%2520semantics%250Awith%2520the%2520target%2520unsafe%2520word%2520and%2520using%2520it%2520as%2520a%2520substitution.%2520The%2520experiments%250Aconducted%2520on%2520six%2520open-source%2520models%2520and%2520commercial%2520online%2520services%2520with%250Athousands%2520of%2520prompts%2520have%2520verified%2520the%2520effectiveness%2520of%2520PGJ.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception-guided%20Jailbreak%20against%20Text-to-Image%20Models&entry.906535625=Yihao%20Huang%20and%20Le%20Liang%20and%20Tianlin%20Li%20and%20Xiaojun%20Jia%20and%20Run%20Wang%20and%20Weikai%20Miao%20and%20Geguang%20Pu%20and%20Yang%20Liu&entry.1292438233=%20%20In%20recent%20years%2C%20Text-to-Image%20%28T2I%29%20models%20have%20garnered%20significant%0Aattention%20due%20to%20their%20remarkable%20advancements.%20However%2C%20security%20concerns%20have%0Aemerged%20due%20to%20their%20potential%20to%20generate%20inappropriate%20or%20Not-Safe-For-Work%0A%28NSFW%29%20images.%20In%20this%20paper%2C%20inspired%20by%20the%20observation%20that%20texts%20with%0Adifferent%20semantics%20can%20lead%20to%20similar%20human%20perceptions%2C%20we%20propose%20an%0ALLM-driven%20perception-guided%20jailbreak%20method%2C%20termed%20PGJ.%20It%20is%20a%20black-box%0Ajailbreak%20method%20that%20requires%20no%20specific%20T2I%20model%20%28model-free%29%20and%20generates%0Ahighly%20natural%20attack%20prompts.%20Specifically%2C%20we%20propose%20identifying%20a%20safe%0Aphrase%20that%20is%20similar%20in%20human%20perception%20yet%20inconsistent%20in%20text%20semantics%0Awith%20the%20target%20unsafe%20word%20and%20using%20it%20as%20a%20substitution.%20The%20experiments%0Aconducted%20on%20six%20open-source%20models%20and%20commercial%20online%20services%20with%0Athousands%20of%20prompts%20have%20verified%20the%20effectiveness%20of%20PGJ.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10848v1&entry.124074799=Read"},
{"title": "LBC: Language-Based-Classifier for Out-Of-Variable Generalization", "author": "Kangjun Noh and Baekryun Seong and Hoyoon Byun and Sungjin Song and Kyungwoo Song", "abstract": "  Large Language Models (LLMs) have great success in natural language\nprocessing tasks such as response generation. However, their use in tabular\ndata has been limited due to their inferior performance compared to traditional\nmachine learning models (TMLs) such as XGBoost. We find that the pre-trained\nknowledge of LLMs enables them to interpret new variables that appear in a test\nwithout additional training, a capability central to the concept of\nOut-of-Variable (OOV). From the findings, we propose a\nLanguage-Based-Classifier (LBC), a classifier that maximizes the benefits of\nLLMs to outperform TMLs on OOV tasks. LBC employs three key methodological\nstrategies: 1) Categorical changes to adjust data to better fit the model's\nunderstanding, 2) Advanced order and indicator to enhance data representation\nto the model, and 3) Using verbalizer to map logit scores to classes during\ninference to generate model predictions. These strategies, combined with the\npre-trained knowledge of LBC, emphasize the model's ability to effectively\nhandle OOV tasks. We empirically and theoretically validate the superiority of\nLBC. LBC is the first study to apply an LLM-based model to OOV tasks. The\nsource code is at\nhttps://github.com/ASDASDanonymous/Language-Based-Classifier-forOOVtasks.\n", "link": "http://arxiv.org/abs/2408.10923v1", "date": "2024-08-20", "relevancy": 2.4239, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5061}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4774}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LBC%3A%20Language-Based-Classifier%20for%20Out-Of-Variable%20Generalization&body=Title%3A%20LBC%3A%20Language-Based-Classifier%20for%20Out-Of-Variable%20Generalization%0AAuthor%3A%20Kangjun%20Noh%20and%20Baekryun%20Seong%20and%20Hoyoon%20Byun%20and%20Sungjin%20Song%20and%20Kyungwoo%20Song%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20great%20success%20in%20natural%20language%0Aprocessing%20tasks%20such%20as%20response%20generation.%20However%2C%20their%20use%20in%20tabular%0Adata%20has%20been%20limited%20due%20to%20their%20inferior%20performance%20compared%20to%20traditional%0Amachine%20learning%20models%20%28TMLs%29%20such%20as%20XGBoost.%20We%20find%20that%20the%20pre-trained%0Aknowledge%20of%20LLMs%20enables%20them%20to%20interpret%20new%20variables%20that%20appear%20in%20a%20test%0Awithout%20additional%20training%2C%20a%20capability%20central%20to%20the%20concept%20of%0AOut-of-Variable%20%28OOV%29.%20From%20the%20findings%2C%20we%20propose%20a%0ALanguage-Based-Classifier%20%28LBC%29%2C%20a%20classifier%20that%20maximizes%20the%20benefits%20of%0ALLMs%20to%20outperform%20TMLs%20on%20OOV%20tasks.%20LBC%20employs%20three%20key%20methodological%0Astrategies%3A%201%29%20Categorical%20changes%20to%20adjust%20data%20to%20better%20fit%20the%20model%27s%0Aunderstanding%2C%202%29%20Advanced%20order%20and%20indicator%20to%20enhance%20data%20representation%0Ato%20the%20model%2C%20and%203%29%20Using%20verbalizer%20to%20map%20logit%20scores%20to%20classes%20during%0Ainference%20to%20generate%20model%20predictions.%20These%20strategies%2C%20combined%20with%20the%0Apre-trained%20knowledge%20of%20LBC%2C%20emphasize%20the%20model%27s%20ability%20to%20effectively%0Ahandle%20OOV%20tasks.%20We%20empirically%20and%20theoretically%20validate%20the%20superiority%20of%0ALBC.%20LBC%20is%20the%20first%20study%20to%20apply%20an%20LLM-based%20model%20to%20OOV%20tasks.%20The%0Asource%20code%20is%20at%0Ahttps%3A//github.com/ASDASDanonymous/Language-Based-Classifier-forOOVtasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLBC%253A%2520Language-Based-Classifier%2520for%2520Out-Of-Variable%2520Generalization%26entry.906535625%3DKangjun%2520Noh%2520and%2520Baekryun%2520Seong%2520and%2520Hoyoon%2520Byun%2520and%2520Sungjin%2520Song%2520and%2520Kyungwoo%2520Song%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520great%2520success%2520in%2520natural%2520language%250Aprocessing%2520tasks%2520such%2520as%2520response%2520generation.%2520However%252C%2520their%2520use%2520in%2520tabular%250Adata%2520has%2520been%2520limited%2520due%2520to%2520their%2520inferior%2520performance%2520compared%2520to%2520traditional%250Amachine%2520learning%2520models%2520%2528TMLs%2529%2520such%2520as%2520XGBoost.%2520We%2520find%2520that%2520the%2520pre-trained%250Aknowledge%2520of%2520LLMs%2520enables%2520them%2520to%2520interpret%2520new%2520variables%2520that%2520appear%2520in%2520a%2520test%250Awithout%2520additional%2520training%252C%2520a%2520capability%2520central%2520to%2520the%2520concept%2520of%250AOut-of-Variable%2520%2528OOV%2529.%2520From%2520the%2520findings%252C%2520we%2520propose%2520a%250ALanguage-Based-Classifier%2520%2528LBC%2529%252C%2520a%2520classifier%2520that%2520maximizes%2520the%2520benefits%2520of%250ALLMs%2520to%2520outperform%2520TMLs%2520on%2520OOV%2520tasks.%2520LBC%2520employs%2520three%2520key%2520methodological%250Astrategies%253A%25201%2529%2520Categorical%2520changes%2520to%2520adjust%2520data%2520to%2520better%2520fit%2520the%2520model%2527s%250Aunderstanding%252C%25202%2529%2520Advanced%2520order%2520and%2520indicator%2520to%2520enhance%2520data%2520representation%250Ato%2520the%2520model%252C%2520and%25203%2529%2520Using%2520verbalizer%2520to%2520map%2520logit%2520scores%2520to%2520classes%2520during%250Ainference%2520to%2520generate%2520model%2520predictions.%2520These%2520strategies%252C%2520combined%2520with%2520the%250Apre-trained%2520knowledge%2520of%2520LBC%252C%2520emphasize%2520the%2520model%2527s%2520ability%2520to%2520effectively%250Ahandle%2520OOV%2520tasks.%2520We%2520empirically%2520and%2520theoretically%2520validate%2520the%2520superiority%2520of%250ALBC.%2520LBC%2520is%2520the%2520first%2520study%2520to%2520apply%2520an%2520LLM-based%2520model%2520to%2520OOV%2520tasks.%2520The%250Asource%2520code%2520is%2520at%250Ahttps%253A//github.com/ASDASDanonymous/Language-Based-Classifier-forOOVtasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LBC%3A%20Language-Based-Classifier%20for%20Out-Of-Variable%20Generalization&entry.906535625=Kangjun%20Noh%20and%20Baekryun%20Seong%20and%20Hoyoon%20Byun%20and%20Sungjin%20Song%20and%20Kyungwoo%20Song&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20great%20success%20in%20natural%20language%0Aprocessing%20tasks%20such%20as%20response%20generation.%20However%2C%20their%20use%20in%20tabular%0Adata%20has%20been%20limited%20due%20to%20their%20inferior%20performance%20compared%20to%20traditional%0Amachine%20learning%20models%20%28TMLs%29%20such%20as%20XGBoost.%20We%20find%20that%20the%20pre-trained%0Aknowledge%20of%20LLMs%20enables%20them%20to%20interpret%20new%20variables%20that%20appear%20in%20a%20test%0Awithout%20additional%20training%2C%20a%20capability%20central%20to%20the%20concept%20of%0AOut-of-Variable%20%28OOV%29.%20From%20the%20findings%2C%20we%20propose%20a%0ALanguage-Based-Classifier%20%28LBC%29%2C%20a%20classifier%20that%20maximizes%20the%20benefits%20of%0ALLMs%20to%20outperform%20TMLs%20on%20OOV%20tasks.%20LBC%20employs%20three%20key%20methodological%0Astrategies%3A%201%29%20Categorical%20changes%20to%20adjust%20data%20to%20better%20fit%20the%20model%27s%0Aunderstanding%2C%202%29%20Advanced%20order%20and%20indicator%20to%20enhance%20data%20representation%0Ato%20the%20model%2C%20and%203%29%20Using%20verbalizer%20to%20map%20logit%20scores%20to%20classes%20during%0Ainference%20to%20generate%20model%20predictions.%20These%20strategies%2C%20combined%20with%20the%0Apre-trained%20knowledge%20of%20LBC%2C%20emphasize%20the%20model%27s%20ability%20to%20effectively%0Ahandle%20OOV%20tasks.%20We%20empirically%20and%20theoretically%20validate%20the%20superiority%20of%0ALBC.%20LBC%20is%20the%20first%20study%20to%20apply%20an%20LLM-based%20model%20to%20OOV%20tasks.%20The%0Asource%20code%20is%20at%0Ahttps%3A//github.com/ASDASDanonymous/Language-Based-Classifier-forOOVtasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10923v1&entry.124074799=Read"},
{"title": "Trustworthy Compression? Impact of AI-based Codecs on Biometrics for Law\n  Enforcement", "author": "Sandra Bergmann and Denise Moussa and Christian Riess", "abstract": "  Image-based biometrics can aid law enforcement in various aspects, for\nexample in iris, fingerprint and soft-biometric recognition. A critical\nprecondition for recognition is the availability of sufficient biometric\ninformation in images. It is visually apparent that strong JPEG compression\nremoves such details. However, latest AI-based image compression seemingly\npreserves many image details even for very strong compression factors. Yet,\nthese perceived details are not necessarily grounded in measurements, which\nraises the question whether these images can still be used for biometric\nrecognition. In this work, we investigate how AI compression impacts iris,\nfingerprint and soft-biometric (fabrics and tattoo) images. We also investigate\nthe recognition performance for iris and fingerprint images after AI\ncompression. It turns out that iris recognition can be strongly affected, while\nfingerprint recognition is quite robust. The loss of detail is qualitatively\nbest seen in fabrics and tattoos images. Overall, our results show that\nAI-compression still permits many biometric tasks, but attention to strong\ncompression factors in sensitive tasks is advisable.\n", "link": "http://arxiv.org/abs/2408.10823v1", "date": "2024-08-20", "relevancy": 2.387, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4899}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4732}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trustworthy%20Compression%3F%20Impact%20of%20AI-based%20Codecs%20on%20Biometrics%20for%20Law%0A%20%20Enforcement&body=Title%3A%20Trustworthy%20Compression%3F%20Impact%20of%20AI-based%20Codecs%20on%20Biometrics%20for%20Law%0A%20%20Enforcement%0AAuthor%3A%20Sandra%20Bergmann%20and%20Denise%20Moussa%20and%20Christian%20Riess%0AAbstract%3A%20%20%20Image-based%20biometrics%20can%20aid%20law%20enforcement%20in%20various%20aspects%2C%20for%0Aexample%20in%20iris%2C%20fingerprint%20and%20soft-biometric%20recognition.%20A%20critical%0Aprecondition%20for%20recognition%20is%20the%20availability%20of%20sufficient%20biometric%0Ainformation%20in%20images.%20It%20is%20visually%20apparent%20that%20strong%20JPEG%20compression%0Aremoves%20such%20details.%20However%2C%20latest%20AI-based%20image%20compression%20seemingly%0Apreserves%20many%20image%20details%20even%20for%20very%20strong%20compression%20factors.%20Yet%2C%0Athese%20perceived%20details%20are%20not%20necessarily%20grounded%20in%20measurements%2C%20which%0Araises%20the%20question%20whether%20these%20images%20can%20still%20be%20used%20for%20biometric%0Arecognition.%20In%20this%20work%2C%20we%20investigate%20how%20AI%20compression%20impacts%20iris%2C%0Afingerprint%20and%20soft-biometric%20%28fabrics%20and%20tattoo%29%20images.%20We%20also%20investigate%0Athe%20recognition%20performance%20for%20iris%20and%20fingerprint%20images%20after%20AI%0Acompression.%20It%20turns%20out%20that%20iris%20recognition%20can%20be%20strongly%20affected%2C%20while%0Afingerprint%20recognition%20is%20quite%20robust.%20The%20loss%20of%20detail%20is%20qualitatively%0Abest%20seen%20in%20fabrics%20and%20tattoos%20images.%20Overall%2C%20our%20results%20show%20that%0AAI-compression%20still%20permits%20many%20biometric%20tasks%2C%20but%20attention%20to%20strong%0Acompression%20factors%20in%20sensitive%20tasks%20is%20advisable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrustworthy%2520Compression%253F%2520Impact%2520of%2520AI-based%2520Codecs%2520on%2520Biometrics%2520for%2520Law%250A%2520%2520Enforcement%26entry.906535625%3DSandra%2520Bergmann%2520and%2520Denise%2520Moussa%2520and%2520Christian%2520Riess%26entry.1292438233%3D%2520%2520Image-based%2520biometrics%2520can%2520aid%2520law%2520enforcement%2520in%2520various%2520aspects%252C%2520for%250Aexample%2520in%2520iris%252C%2520fingerprint%2520and%2520soft-biometric%2520recognition.%2520A%2520critical%250Aprecondition%2520for%2520recognition%2520is%2520the%2520availability%2520of%2520sufficient%2520biometric%250Ainformation%2520in%2520images.%2520It%2520is%2520visually%2520apparent%2520that%2520strong%2520JPEG%2520compression%250Aremoves%2520such%2520details.%2520However%252C%2520latest%2520AI-based%2520image%2520compression%2520seemingly%250Apreserves%2520many%2520image%2520details%2520even%2520for%2520very%2520strong%2520compression%2520factors.%2520Yet%252C%250Athese%2520perceived%2520details%2520are%2520not%2520necessarily%2520grounded%2520in%2520measurements%252C%2520which%250Araises%2520the%2520question%2520whether%2520these%2520images%2520can%2520still%2520be%2520used%2520for%2520biometric%250Arecognition.%2520In%2520this%2520work%252C%2520we%2520investigate%2520how%2520AI%2520compression%2520impacts%2520iris%252C%250Afingerprint%2520and%2520soft-biometric%2520%2528fabrics%2520and%2520tattoo%2529%2520images.%2520We%2520also%2520investigate%250Athe%2520recognition%2520performance%2520for%2520iris%2520and%2520fingerprint%2520images%2520after%2520AI%250Acompression.%2520It%2520turns%2520out%2520that%2520iris%2520recognition%2520can%2520be%2520strongly%2520affected%252C%2520while%250Afingerprint%2520recognition%2520is%2520quite%2520robust.%2520The%2520loss%2520of%2520detail%2520is%2520qualitatively%250Abest%2520seen%2520in%2520fabrics%2520and%2520tattoos%2520images.%2520Overall%252C%2520our%2520results%2520show%2520that%250AAI-compression%2520still%2520permits%2520many%2520biometric%2520tasks%252C%2520but%2520attention%2520to%2520strong%250Acompression%2520factors%2520in%2520sensitive%2520tasks%2520is%2520advisable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trustworthy%20Compression%3F%20Impact%20of%20AI-based%20Codecs%20on%20Biometrics%20for%20Law%0A%20%20Enforcement&entry.906535625=Sandra%20Bergmann%20and%20Denise%20Moussa%20and%20Christian%20Riess&entry.1292438233=%20%20Image-based%20biometrics%20can%20aid%20law%20enforcement%20in%20various%20aspects%2C%20for%0Aexample%20in%20iris%2C%20fingerprint%20and%20soft-biometric%20recognition.%20A%20critical%0Aprecondition%20for%20recognition%20is%20the%20availability%20of%20sufficient%20biometric%0Ainformation%20in%20images.%20It%20is%20visually%20apparent%20that%20strong%20JPEG%20compression%0Aremoves%20such%20details.%20However%2C%20latest%20AI-based%20image%20compression%20seemingly%0Apreserves%20many%20image%20details%20even%20for%20very%20strong%20compression%20factors.%20Yet%2C%0Athese%20perceived%20details%20are%20not%20necessarily%20grounded%20in%20measurements%2C%20which%0Araises%20the%20question%20whether%20these%20images%20can%20still%20be%20used%20for%20biometric%0Arecognition.%20In%20this%20work%2C%20we%20investigate%20how%20AI%20compression%20impacts%20iris%2C%0Afingerprint%20and%20soft-biometric%20%28fabrics%20and%20tattoo%29%20images.%20We%20also%20investigate%0Athe%20recognition%20performance%20for%20iris%20and%20fingerprint%20images%20after%20AI%0Acompression.%20It%20turns%20out%20that%20iris%20recognition%20can%20be%20strongly%20affected%2C%20while%0Afingerprint%20recognition%20is%20quite%20robust.%20The%20loss%20of%20detail%20is%20qualitatively%0Abest%20seen%20in%20fabrics%20and%20tattoos%20images.%20Overall%2C%20our%20results%20show%20that%0AAI-compression%20still%20permits%20many%20biometric%20tasks%2C%20but%20attention%20to%20strong%0Acompression%20factors%20in%20sensitive%20tasks%20is%20advisable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10823v1&entry.124074799=Read"},
{"title": "Deep Generative Models in Robotics: A Survey on Learning from Multimodal\n  Demonstrations", "author": "Julen Urain and Ajay Mandlekar and Yilun Du and Mahi Shafiullah and Danfei Xu and Katerina Fragkiadaki and Georgia Chalvatzaki and Jan Peters", "abstract": "  Learning from Demonstrations, the field that proposes to learn robot behavior\nmodels from data, is gaining popularity with the emergence of deep generative\nmodels. Although the problem has been studied for years under names such as\nImitation Learning, Behavioral Cloning, or Inverse Reinforcement Learning,\nclassical methods have relied on models that don't capture complex data\ndistributions well or don't scale well to large numbers of demonstrations. In\nrecent years, the robot learning community has shown increasing interest in\nusing deep generative models to capture the complexity of large datasets. In\nthis survey, we aim to provide a unified and comprehensive review of the last\nyear's progress in the use of deep generative models in robotics. We present\nthe different types of models that the community has explored, such as\nenergy-based models, diffusion models, action value maps, or generative\nadversarial networks. We also present the different types of applications in\nwhich deep generative models have been used, from grasp generation to\ntrajectory generation or cost learning. One of the most important elements of\ngenerative models is the generalization out of distributions. In our survey, we\nreview the different decisions the community has made to improve the\ngeneralization of the learned models. Finally, we highlight the research\nchallenges and propose a number of future directions for learning deep\ngenerative models in robotics.\n", "link": "http://arxiv.org/abs/2408.04380v2", "date": "2024-08-20", "relevancy": 2.3503, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5966}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5864}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Generative%20Models%20in%20Robotics%3A%20A%20Survey%20on%20Learning%20from%20Multimodal%0A%20%20Demonstrations&body=Title%3A%20Deep%20Generative%20Models%20in%20Robotics%3A%20A%20Survey%20on%20Learning%20from%20Multimodal%0A%20%20Demonstrations%0AAuthor%3A%20Julen%20Urain%20and%20Ajay%20Mandlekar%20and%20Yilun%20Du%20and%20Mahi%20Shafiullah%20and%20Danfei%20Xu%20and%20Katerina%20Fragkiadaki%20and%20Georgia%20Chalvatzaki%20and%20Jan%20Peters%0AAbstract%3A%20%20%20Learning%20from%20Demonstrations%2C%20the%20field%20that%20proposes%20to%20learn%20robot%20behavior%0Amodels%20from%20data%2C%20is%20gaining%20popularity%20with%20the%20emergence%20of%20deep%20generative%0Amodels.%20Although%20the%20problem%20has%20been%20studied%20for%20years%20under%20names%20such%20as%0AImitation%20Learning%2C%20Behavioral%20Cloning%2C%20or%20Inverse%20Reinforcement%20Learning%2C%0Aclassical%20methods%20have%20relied%20on%20models%20that%20don%27t%20capture%20complex%20data%0Adistributions%20well%20or%20don%27t%20scale%20well%20to%20large%20numbers%20of%20demonstrations.%20In%0Arecent%20years%2C%20the%20robot%20learning%20community%20has%20shown%20increasing%20interest%20in%0Ausing%20deep%20generative%20models%20to%20capture%20the%20complexity%20of%20large%20datasets.%20In%0Athis%20survey%2C%20we%20aim%20to%20provide%20a%20unified%20and%20comprehensive%20review%20of%20the%20last%0Ayear%27s%20progress%20in%20the%20use%20of%20deep%20generative%20models%20in%20robotics.%20We%20present%0Athe%20different%20types%20of%20models%20that%20the%20community%20has%20explored%2C%20such%20as%0Aenergy-based%20models%2C%20diffusion%20models%2C%20action%20value%20maps%2C%20or%20generative%0Aadversarial%20networks.%20We%20also%20present%20the%20different%20types%20of%20applications%20in%0Awhich%20deep%20generative%20models%20have%20been%20used%2C%20from%20grasp%20generation%20to%0Atrajectory%20generation%20or%20cost%20learning.%20One%20of%20the%20most%20important%20elements%20of%0Agenerative%20models%20is%20the%20generalization%20out%20of%20distributions.%20In%20our%20survey%2C%20we%0Areview%20the%20different%20decisions%20the%20community%20has%20made%20to%20improve%20the%0Ageneralization%20of%20the%20learned%20models.%20Finally%2C%20we%20highlight%20the%20research%0Achallenges%20and%20propose%20a%20number%20of%20future%20directions%20for%20learning%20deep%0Agenerative%20models%20in%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04380v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Generative%2520Models%2520in%2520Robotics%253A%2520A%2520Survey%2520on%2520Learning%2520from%2520Multimodal%250A%2520%2520Demonstrations%26entry.906535625%3DJulen%2520Urain%2520and%2520Ajay%2520Mandlekar%2520and%2520Yilun%2520Du%2520and%2520Mahi%2520Shafiullah%2520and%2520Danfei%2520Xu%2520and%2520Katerina%2520Fragkiadaki%2520and%2520Georgia%2520Chalvatzaki%2520and%2520Jan%2520Peters%26entry.1292438233%3D%2520%2520Learning%2520from%2520Demonstrations%252C%2520the%2520field%2520that%2520proposes%2520to%2520learn%2520robot%2520behavior%250Amodels%2520from%2520data%252C%2520is%2520gaining%2520popularity%2520with%2520the%2520emergence%2520of%2520deep%2520generative%250Amodels.%2520Although%2520the%2520problem%2520has%2520been%2520studied%2520for%2520years%2520under%2520names%2520such%2520as%250AImitation%2520Learning%252C%2520Behavioral%2520Cloning%252C%2520or%2520Inverse%2520Reinforcement%2520Learning%252C%250Aclassical%2520methods%2520have%2520relied%2520on%2520models%2520that%2520don%2527t%2520capture%2520complex%2520data%250Adistributions%2520well%2520or%2520don%2527t%2520scale%2520well%2520to%2520large%2520numbers%2520of%2520demonstrations.%2520In%250Arecent%2520years%252C%2520the%2520robot%2520learning%2520community%2520has%2520shown%2520increasing%2520interest%2520in%250Ausing%2520deep%2520generative%2520models%2520to%2520capture%2520the%2520complexity%2520of%2520large%2520datasets.%2520In%250Athis%2520survey%252C%2520we%2520aim%2520to%2520provide%2520a%2520unified%2520and%2520comprehensive%2520review%2520of%2520the%2520last%250Ayear%2527s%2520progress%2520in%2520the%2520use%2520of%2520deep%2520generative%2520models%2520in%2520robotics.%2520We%2520present%250Athe%2520different%2520types%2520of%2520models%2520that%2520the%2520community%2520has%2520explored%252C%2520such%2520as%250Aenergy-based%2520models%252C%2520diffusion%2520models%252C%2520action%2520value%2520maps%252C%2520or%2520generative%250Aadversarial%2520networks.%2520We%2520also%2520present%2520the%2520different%2520types%2520of%2520applications%2520in%250Awhich%2520deep%2520generative%2520models%2520have%2520been%2520used%252C%2520from%2520grasp%2520generation%2520to%250Atrajectory%2520generation%2520or%2520cost%2520learning.%2520One%2520of%2520the%2520most%2520important%2520elements%2520of%250Agenerative%2520models%2520is%2520the%2520generalization%2520out%2520of%2520distributions.%2520In%2520our%2520survey%252C%2520we%250Areview%2520the%2520different%2520decisions%2520the%2520community%2520has%2520made%2520to%2520improve%2520the%250Ageneralization%2520of%2520the%2520learned%2520models.%2520Finally%252C%2520we%2520highlight%2520the%2520research%250Achallenges%2520and%2520propose%2520a%2520number%2520of%2520future%2520directions%2520for%2520learning%2520deep%250Agenerative%2520models%2520in%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04380v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Generative%20Models%20in%20Robotics%3A%20A%20Survey%20on%20Learning%20from%20Multimodal%0A%20%20Demonstrations&entry.906535625=Julen%20Urain%20and%20Ajay%20Mandlekar%20and%20Yilun%20Du%20and%20Mahi%20Shafiullah%20and%20Danfei%20Xu%20and%20Katerina%20Fragkiadaki%20and%20Georgia%20Chalvatzaki%20and%20Jan%20Peters&entry.1292438233=%20%20Learning%20from%20Demonstrations%2C%20the%20field%20that%20proposes%20to%20learn%20robot%20behavior%0Amodels%20from%20data%2C%20is%20gaining%20popularity%20with%20the%20emergence%20of%20deep%20generative%0Amodels.%20Although%20the%20problem%20has%20been%20studied%20for%20years%20under%20names%20such%20as%0AImitation%20Learning%2C%20Behavioral%20Cloning%2C%20or%20Inverse%20Reinforcement%20Learning%2C%0Aclassical%20methods%20have%20relied%20on%20models%20that%20don%27t%20capture%20complex%20data%0Adistributions%20well%20or%20don%27t%20scale%20well%20to%20large%20numbers%20of%20demonstrations.%20In%0Arecent%20years%2C%20the%20robot%20learning%20community%20has%20shown%20increasing%20interest%20in%0Ausing%20deep%20generative%20models%20to%20capture%20the%20complexity%20of%20large%20datasets.%20In%0Athis%20survey%2C%20we%20aim%20to%20provide%20a%20unified%20and%20comprehensive%20review%20of%20the%20last%0Ayear%27s%20progress%20in%20the%20use%20of%20deep%20generative%20models%20in%20robotics.%20We%20present%0Athe%20different%20types%20of%20models%20that%20the%20community%20has%20explored%2C%20such%20as%0Aenergy-based%20models%2C%20diffusion%20models%2C%20action%20value%20maps%2C%20or%20generative%0Aadversarial%20networks.%20We%20also%20present%20the%20different%20types%20of%20applications%20in%0Awhich%20deep%20generative%20models%20have%20been%20used%2C%20from%20grasp%20generation%20to%0Atrajectory%20generation%20or%20cost%20learning.%20One%20of%20the%20most%20important%20elements%20of%0Agenerative%20models%20is%20the%20generalization%20out%20of%20distributions.%20In%20our%20survey%2C%20we%0Areview%20the%20different%20decisions%20the%20community%20has%20made%20to%20improve%20the%0Ageneralization%20of%20the%20learned%20models.%20Finally%2C%20we%20highlight%20the%20research%0Achallenges%20and%20propose%20a%20number%20of%20future%20directions%20for%20learning%20deep%0Agenerative%20models%20in%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04380v2&entry.124074799=Read"},
{"title": "Constructing a High Temporal Resolution Global Lakes Dataset via\n  Swin-Unet with Applications to Area Prediction", "author": "Yutian Han and Baoxiang Huang and He Gao", "abstract": "  Lakes provide a wide range of valuable ecosystem services, such as water\nsupply, biodiversity habitats, and carbon sequestration. However, lakes are\nincreasingly threatened by climate change and human activities. Therefore,\ncontinuous global monitoring of lake dynamics is crucial, but remains\nchallenging on a large scale. The recently developed Global Lakes Area Database\n(GLAKES) has mapped over 3.4 million lakes worldwide, but it only provides data\nat decadal intervals, which may be insufficient to capture rapid or short-term\nchanges.This paper introduces an expanded lake database, GLAKES-Additional,\nwhich offers biennial delineations and area measurements for 152,567 lakes\nglobally from 1990 to 2021. We employed the Swin-Unet model, replacing\ntraditional convolution operations, to effectively address the challenges posed\nby the receptive field requirements of high spatial resolution satellite\nimagery. The increased biennial time resolution helps to quantitatively\nattribute lake area changes to climatic and hydrological drivers, such as\nprecipitation and temperature changes.For predicting lake area changes, we used\na Long Short-Term Memory (LSTM) neural network and an extended time series\ndataset for preliminary modeling. Under climate and land use scenarios, our\nmodel achieved an RMSE of 0.317 km^2 in predicting future lake area changes.\n", "link": "http://arxiv.org/abs/2408.10821v1", "date": "2024-08-20", "relevancy": 2.3284, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4825}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4718}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constructing%20a%20High%20Temporal%20Resolution%20Global%20Lakes%20Dataset%20via%0A%20%20Swin-Unet%20with%20Applications%20to%20Area%20Prediction&body=Title%3A%20Constructing%20a%20High%20Temporal%20Resolution%20Global%20Lakes%20Dataset%20via%0A%20%20Swin-Unet%20with%20Applications%20to%20Area%20Prediction%0AAuthor%3A%20Yutian%20Han%20and%20Baoxiang%20Huang%20and%20He%20Gao%0AAbstract%3A%20%20%20Lakes%20provide%20a%20wide%20range%20of%20valuable%20ecosystem%20services%2C%20such%20as%20water%0Asupply%2C%20biodiversity%20habitats%2C%20and%20carbon%20sequestration.%20However%2C%20lakes%20are%0Aincreasingly%20threatened%20by%20climate%20change%20and%20human%20activities.%20Therefore%2C%0Acontinuous%20global%20monitoring%20of%20lake%20dynamics%20is%20crucial%2C%20but%20remains%0Achallenging%20on%20a%20large%20scale.%20The%20recently%20developed%20Global%20Lakes%20Area%20Database%0A%28GLAKES%29%20has%20mapped%20over%203.4%20million%20lakes%20worldwide%2C%20but%20it%20only%20provides%20data%0Aat%20decadal%20intervals%2C%20which%20may%20be%20insufficient%20to%20capture%20rapid%20or%20short-term%0Achanges.This%20paper%20introduces%20an%20expanded%20lake%20database%2C%20GLAKES-Additional%2C%0Awhich%20offers%20biennial%20delineations%20and%20area%20measurements%20for%20152%2C567%20lakes%0Aglobally%20from%201990%20to%202021.%20We%20employed%20the%20Swin-Unet%20model%2C%20replacing%0Atraditional%20convolution%20operations%2C%20to%20effectively%20address%20the%20challenges%20posed%0Aby%20the%20receptive%20field%20requirements%20of%20high%20spatial%20resolution%20satellite%0Aimagery.%20The%20increased%20biennial%20time%20resolution%20helps%20to%20quantitatively%0Aattribute%20lake%20area%20changes%20to%20climatic%20and%20hydrological%20drivers%2C%20such%20as%0Aprecipitation%20and%20temperature%20changes.For%20predicting%20lake%20area%20changes%2C%20we%20used%0Aa%20Long%20Short-Term%20Memory%20%28LSTM%29%20neural%20network%20and%20an%20extended%20time%20series%0Adataset%20for%20preliminary%20modeling.%20Under%20climate%20and%20land%20use%20scenarios%2C%20our%0Amodel%20achieved%20an%20RMSE%20of%200.317%20km%5E2%20in%20predicting%20future%20lake%20area%20changes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstructing%2520a%2520High%2520Temporal%2520Resolution%2520Global%2520Lakes%2520Dataset%2520via%250A%2520%2520Swin-Unet%2520with%2520Applications%2520to%2520Area%2520Prediction%26entry.906535625%3DYutian%2520Han%2520and%2520Baoxiang%2520Huang%2520and%2520He%2520Gao%26entry.1292438233%3D%2520%2520Lakes%2520provide%2520a%2520wide%2520range%2520of%2520valuable%2520ecosystem%2520services%252C%2520such%2520as%2520water%250Asupply%252C%2520biodiversity%2520habitats%252C%2520and%2520carbon%2520sequestration.%2520However%252C%2520lakes%2520are%250Aincreasingly%2520threatened%2520by%2520climate%2520change%2520and%2520human%2520activities.%2520Therefore%252C%250Acontinuous%2520global%2520monitoring%2520of%2520lake%2520dynamics%2520is%2520crucial%252C%2520but%2520remains%250Achallenging%2520on%2520a%2520large%2520scale.%2520The%2520recently%2520developed%2520Global%2520Lakes%2520Area%2520Database%250A%2528GLAKES%2529%2520has%2520mapped%2520over%25203.4%2520million%2520lakes%2520worldwide%252C%2520but%2520it%2520only%2520provides%2520data%250Aat%2520decadal%2520intervals%252C%2520which%2520may%2520be%2520insufficient%2520to%2520capture%2520rapid%2520or%2520short-term%250Achanges.This%2520paper%2520introduces%2520an%2520expanded%2520lake%2520database%252C%2520GLAKES-Additional%252C%250Awhich%2520offers%2520biennial%2520delineations%2520and%2520area%2520measurements%2520for%2520152%252C567%2520lakes%250Aglobally%2520from%25201990%2520to%25202021.%2520We%2520employed%2520the%2520Swin-Unet%2520model%252C%2520replacing%250Atraditional%2520convolution%2520operations%252C%2520to%2520effectively%2520address%2520the%2520challenges%2520posed%250Aby%2520the%2520receptive%2520field%2520requirements%2520of%2520high%2520spatial%2520resolution%2520satellite%250Aimagery.%2520The%2520increased%2520biennial%2520time%2520resolution%2520helps%2520to%2520quantitatively%250Aattribute%2520lake%2520area%2520changes%2520to%2520climatic%2520and%2520hydrological%2520drivers%252C%2520such%2520as%250Aprecipitation%2520and%2520temperature%2520changes.For%2520predicting%2520lake%2520area%2520changes%252C%2520we%2520used%250Aa%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520neural%2520network%2520and%2520an%2520extended%2520time%2520series%250Adataset%2520for%2520preliminary%2520modeling.%2520Under%2520climate%2520and%2520land%2520use%2520scenarios%252C%2520our%250Amodel%2520achieved%2520an%2520RMSE%2520of%25200.317%2520km%255E2%2520in%2520predicting%2520future%2520lake%2520area%2520changes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constructing%20a%20High%20Temporal%20Resolution%20Global%20Lakes%20Dataset%20via%0A%20%20Swin-Unet%20with%20Applications%20to%20Area%20Prediction&entry.906535625=Yutian%20Han%20and%20Baoxiang%20Huang%20and%20He%20Gao&entry.1292438233=%20%20Lakes%20provide%20a%20wide%20range%20of%20valuable%20ecosystem%20services%2C%20such%20as%20water%0Asupply%2C%20biodiversity%20habitats%2C%20and%20carbon%20sequestration.%20However%2C%20lakes%20are%0Aincreasingly%20threatened%20by%20climate%20change%20and%20human%20activities.%20Therefore%2C%0Acontinuous%20global%20monitoring%20of%20lake%20dynamics%20is%20crucial%2C%20but%20remains%0Achallenging%20on%20a%20large%20scale.%20The%20recently%20developed%20Global%20Lakes%20Area%20Database%0A%28GLAKES%29%20has%20mapped%20over%203.4%20million%20lakes%20worldwide%2C%20but%20it%20only%20provides%20data%0Aat%20decadal%20intervals%2C%20which%20may%20be%20insufficient%20to%20capture%20rapid%20or%20short-term%0Achanges.This%20paper%20introduces%20an%20expanded%20lake%20database%2C%20GLAKES-Additional%2C%0Awhich%20offers%20biennial%20delineations%20and%20area%20measurements%20for%20152%2C567%20lakes%0Aglobally%20from%201990%20to%202021.%20We%20employed%20the%20Swin-Unet%20model%2C%20replacing%0Atraditional%20convolution%20operations%2C%20to%20effectively%20address%20the%20challenges%20posed%0Aby%20the%20receptive%20field%20requirements%20of%20high%20spatial%20resolution%20satellite%0Aimagery.%20The%20increased%20biennial%20time%20resolution%20helps%20to%20quantitatively%0Aattribute%20lake%20area%20changes%20to%20climatic%20and%20hydrological%20drivers%2C%20such%20as%0Aprecipitation%20and%20temperature%20changes.For%20predicting%20lake%20area%20changes%2C%20we%20used%0Aa%20Long%20Short-Term%20Memory%20%28LSTM%29%20neural%20network%20and%20an%20extended%20time%20series%0Adataset%20for%20preliminary%20modeling.%20Under%20climate%20and%20land%20use%20scenarios%2C%20our%0Amodel%20achieved%20an%20RMSE%20of%200.317%20km%5E2%20in%20predicting%20future%20lake%20area%20changes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10821v1&entry.124074799=Read"},
{"title": "Unified Domain Adaptive Semantic Segmentation", "author": "Zhe Zhang and Gaochang Wu and Jing Zhang and Xiatian Zhu and Dacheng Tao and Tianyou Chai", "abstract": "  Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS) aims to transfer\nthe supervision from a labeled source domain to an unlabeled target domain. The\nmajority of existing UDA-SS works typically consider images whilst recent\nattempts have extended further to tackle videos by modeling the temporal\ndimension. Although the two lines of research share the major challenges --\novercoming the underlying domain distribution shift, their studies are largely\nindependent, resulting in fragmented insights, a lack of holistic\nunderstanding, and missed opportunities for cross-pollination of ideas. This\nfragmentation prevents the unification of methods, leading to redundant efforts\nand suboptimal knowledge transfer across image and video domains. Under this\nobservation, we advocate unifying the study of UDA-SS across video and image\nscenarios, enabling a more comprehensive understanding, synergistic\nadvancements, and efficient knowledge sharing. To that end, we explore the\nunified UDA-SS from a general data augmentation perspective, serving as a\nunifying conceptual framework, enabling improved generalization, and potential\nfor cross-pollination of ideas, ultimately contributing to the overall progress\nand practical impact of this field of research. Specifically, we propose a\nQuad-directional Mixup (QuadMix) method, characterized by tackling distinct\npoint attributes and feature inconsistencies through four-directional paths for\nintra- and inter-domain mixing in a feature space. To deal with temporal shifts\nwith videos, we incorporate optical flow-guided feature aggregation across\nspatial and temporal dimensions for fine-grained domain alignment. Extensive\nexperiments show that our method outperforms the state-of-the-art works by\nlarge margins on four challenging UDA-SS benchmarks. Our source code and models\nwill be released at \\url{https://github.com/ZHE-SAPI/UDASS}.\n", "link": "http://arxiv.org/abs/2311.13254v2", "date": "2024-08-20", "relevancy": 2.3159, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5876}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5794}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Domain%20Adaptive%20Semantic%20Segmentation&body=Title%3A%20Unified%20Domain%20Adaptive%20Semantic%20Segmentation%0AAuthor%3A%20Zhe%20Zhang%20and%20Gaochang%20Wu%20and%20Jing%20Zhang%20and%20Xiatian%20Zhu%20and%20Dacheng%20Tao%20and%20Tianyou%20Chai%0AAbstract%3A%20%20%20Unsupervised%20Domain%20Adaptive%20Semantic%20Segmentation%20%28UDA-SS%29%20aims%20to%20transfer%0Athe%20supervision%20from%20a%20labeled%20source%20domain%20to%20an%20unlabeled%20target%20domain.%20The%0Amajority%20of%20existing%20UDA-SS%20works%20typically%20consider%20images%20whilst%20recent%0Aattempts%20have%20extended%20further%20to%20tackle%20videos%20by%20modeling%20the%20temporal%0Adimension.%20Although%20the%20two%20lines%20of%20research%20share%20the%20major%20challenges%20--%0Aovercoming%20the%20underlying%20domain%20distribution%20shift%2C%20their%20studies%20are%20largely%0Aindependent%2C%20resulting%20in%20fragmented%20insights%2C%20a%20lack%20of%20holistic%0Aunderstanding%2C%20and%20missed%20opportunities%20for%20cross-pollination%20of%20ideas.%20This%0Afragmentation%20prevents%20the%20unification%20of%20methods%2C%20leading%20to%20redundant%20efforts%0Aand%20suboptimal%20knowledge%20transfer%20across%20image%20and%20video%20domains.%20Under%20this%0Aobservation%2C%20we%20advocate%20unifying%20the%20study%20of%20UDA-SS%20across%20video%20and%20image%0Ascenarios%2C%20enabling%20a%20more%20comprehensive%20understanding%2C%20synergistic%0Aadvancements%2C%20and%20efficient%20knowledge%20sharing.%20To%20that%20end%2C%20we%20explore%20the%0Aunified%20UDA-SS%20from%20a%20general%20data%20augmentation%20perspective%2C%20serving%20as%20a%0Aunifying%20conceptual%20framework%2C%20enabling%20improved%20generalization%2C%20and%20potential%0Afor%20cross-pollination%20of%20ideas%2C%20ultimately%20contributing%20to%20the%20overall%20progress%0Aand%20practical%20impact%20of%20this%20field%20of%20research.%20Specifically%2C%20we%20propose%20a%0AQuad-directional%20Mixup%20%28QuadMix%29%20method%2C%20characterized%20by%20tackling%20distinct%0Apoint%20attributes%20and%20feature%20inconsistencies%20through%20four-directional%20paths%20for%0Aintra-%20and%20inter-domain%20mixing%20in%20a%20feature%20space.%20To%20deal%20with%20temporal%20shifts%0Awith%20videos%2C%20we%20incorporate%20optical%20flow-guided%20feature%20aggregation%20across%0Aspatial%20and%20temporal%20dimensions%20for%20fine-grained%20domain%20alignment.%20Extensive%0Aexperiments%20show%20that%20our%20method%20outperforms%20the%20state-of-the-art%20works%20by%0Alarge%20margins%20on%20four%20challenging%20UDA-SS%20benchmarks.%20Our%20source%20code%20and%20models%0Awill%20be%20released%20at%20%5Curl%7Bhttps%3A//github.com/ZHE-SAPI/UDASS%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13254v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Domain%2520Adaptive%2520Semantic%2520Segmentation%26entry.906535625%3DZhe%2520Zhang%2520and%2520Gaochang%2520Wu%2520and%2520Jing%2520Zhang%2520and%2520Xiatian%2520Zhu%2520and%2520Dacheng%2520Tao%2520and%2520Tianyou%2520Chai%26entry.1292438233%3D%2520%2520Unsupervised%2520Domain%2520Adaptive%2520Semantic%2520Segmentation%2520%2528UDA-SS%2529%2520aims%2520to%2520transfer%250Athe%2520supervision%2520from%2520a%2520labeled%2520source%2520domain%2520to%2520an%2520unlabeled%2520target%2520domain.%2520The%250Amajority%2520of%2520existing%2520UDA-SS%2520works%2520typically%2520consider%2520images%2520whilst%2520recent%250Aattempts%2520have%2520extended%2520further%2520to%2520tackle%2520videos%2520by%2520modeling%2520the%2520temporal%250Adimension.%2520Although%2520the%2520two%2520lines%2520of%2520research%2520share%2520the%2520major%2520challenges%2520--%250Aovercoming%2520the%2520underlying%2520domain%2520distribution%2520shift%252C%2520their%2520studies%2520are%2520largely%250Aindependent%252C%2520resulting%2520in%2520fragmented%2520insights%252C%2520a%2520lack%2520of%2520holistic%250Aunderstanding%252C%2520and%2520missed%2520opportunities%2520for%2520cross-pollination%2520of%2520ideas.%2520This%250Afragmentation%2520prevents%2520the%2520unification%2520of%2520methods%252C%2520leading%2520to%2520redundant%2520efforts%250Aand%2520suboptimal%2520knowledge%2520transfer%2520across%2520image%2520and%2520video%2520domains.%2520Under%2520this%250Aobservation%252C%2520we%2520advocate%2520unifying%2520the%2520study%2520of%2520UDA-SS%2520across%2520video%2520and%2520image%250Ascenarios%252C%2520enabling%2520a%2520more%2520comprehensive%2520understanding%252C%2520synergistic%250Aadvancements%252C%2520and%2520efficient%2520knowledge%2520sharing.%2520To%2520that%2520end%252C%2520we%2520explore%2520the%250Aunified%2520UDA-SS%2520from%2520a%2520general%2520data%2520augmentation%2520perspective%252C%2520serving%2520as%2520a%250Aunifying%2520conceptual%2520framework%252C%2520enabling%2520improved%2520generalization%252C%2520and%2520potential%250Afor%2520cross-pollination%2520of%2520ideas%252C%2520ultimately%2520contributing%2520to%2520the%2520overall%2520progress%250Aand%2520practical%2520impact%2520of%2520this%2520field%2520of%2520research.%2520Specifically%252C%2520we%2520propose%2520a%250AQuad-directional%2520Mixup%2520%2528QuadMix%2529%2520method%252C%2520characterized%2520by%2520tackling%2520distinct%250Apoint%2520attributes%2520and%2520feature%2520inconsistencies%2520through%2520four-directional%2520paths%2520for%250Aintra-%2520and%2520inter-domain%2520mixing%2520in%2520a%2520feature%2520space.%2520To%2520deal%2520with%2520temporal%2520shifts%250Awith%2520videos%252C%2520we%2520incorporate%2520optical%2520flow-guided%2520feature%2520aggregation%2520across%250Aspatial%2520and%2520temporal%2520dimensions%2520for%2520fine-grained%2520domain%2520alignment.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%2520works%2520by%250Alarge%2520margins%2520on%2520four%2520challenging%2520UDA-SS%2520benchmarks.%2520Our%2520source%2520code%2520and%2520models%250Awill%2520be%2520released%2520at%2520%255Curl%257Bhttps%253A//github.com/ZHE-SAPI/UDASS%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.13254v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Domain%20Adaptive%20Semantic%20Segmentation&entry.906535625=Zhe%20Zhang%20and%20Gaochang%20Wu%20and%20Jing%20Zhang%20and%20Xiatian%20Zhu%20and%20Dacheng%20Tao%20and%20Tianyou%20Chai&entry.1292438233=%20%20Unsupervised%20Domain%20Adaptive%20Semantic%20Segmentation%20%28UDA-SS%29%20aims%20to%20transfer%0Athe%20supervision%20from%20a%20labeled%20source%20domain%20to%20an%20unlabeled%20target%20domain.%20The%0Amajority%20of%20existing%20UDA-SS%20works%20typically%20consider%20images%20whilst%20recent%0Aattempts%20have%20extended%20further%20to%20tackle%20videos%20by%20modeling%20the%20temporal%0Adimension.%20Although%20the%20two%20lines%20of%20research%20share%20the%20major%20challenges%20--%0Aovercoming%20the%20underlying%20domain%20distribution%20shift%2C%20their%20studies%20are%20largely%0Aindependent%2C%20resulting%20in%20fragmented%20insights%2C%20a%20lack%20of%20holistic%0Aunderstanding%2C%20and%20missed%20opportunities%20for%20cross-pollination%20of%20ideas.%20This%0Afragmentation%20prevents%20the%20unification%20of%20methods%2C%20leading%20to%20redundant%20efforts%0Aand%20suboptimal%20knowledge%20transfer%20across%20image%20and%20video%20domains.%20Under%20this%0Aobservation%2C%20we%20advocate%20unifying%20the%20study%20of%20UDA-SS%20across%20video%20and%20image%0Ascenarios%2C%20enabling%20a%20more%20comprehensive%20understanding%2C%20synergistic%0Aadvancements%2C%20and%20efficient%20knowledge%20sharing.%20To%20that%20end%2C%20we%20explore%20the%0Aunified%20UDA-SS%20from%20a%20general%20data%20augmentation%20perspective%2C%20serving%20as%20a%0Aunifying%20conceptual%20framework%2C%20enabling%20improved%20generalization%2C%20and%20potential%0Afor%20cross-pollination%20of%20ideas%2C%20ultimately%20contributing%20to%20the%20overall%20progress%0Aand%20practical%20impact%20of%20this%20field%20of%20research.%20Specifically%2C%20we%20propose%20a%0AQuad-directional%20Mixup%20%28QuadMix%29%20method%2C%20characterized%20by%20tackling%20distinct%0Apoint%20attributes%20and%20feature%20inconsistencies%20through%20four-directional%20paths%20for%0Aintra-%20and%20inter-domain%20mixing%20in%20a%20feature%20space.%20To%20deal%20with%20temporal%20shifts%0Awith%20videos%2C%20we%20incorporate%20optical%20flow-guided%20feature%20aggregation%20across%0Aspatial%20and%20temporal%20dimensions%20for%20fine-grained%20domain%20alignment.%20Extensive%0Aexperiments%20show%20that%20our%20method%20outperforms%20the%20state-of-the-art%20works%20by%0Alarge%20margins%20on%20four%20challenging%20UDA-SS%20benchmarks.%20Our%20source%20code%20and%20models%0Awill%20be%20released%20at%20%5Curl%7Bhttps%3A//github.com/ZHE-SAPI/UDASS%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13254v2&entry.124074799=Read"},
{"title": "Open 3D World in Autonomous Driving", "author": "Xinlong Cheng and Lei Li", "abstract": "  The capability for open vocabulary perception represents a significant\nadvancement in autonomous driving systems, facilitating the comprehension and\ninterpretation of a wide array of textual inputs in real-time. Despite\nextensive research in open vocabulary tasks within 2D computer vision, the\napplication of such methodologies to 3D environments, particularly within\nlarge-scale outdoor contexts, remains relatively underdeveloped. This paper\npresents a novel approach that integrates 3D point cloud data, acquired from\nLIDAR sensors, with textual information. The primary focus is on the\nutilization of textual data to directly localize and identify objects within\nthe autonomous driving context. We introduce an efficient framework for the\nfusion of bird's-eye view (BEV) region features with textual features, thereby\nenabling the system to seamlessly adapt to novel textual inputs and enhancing\nthe robustness of open vocabulary detection tasks. The effectiveness of the\nproposed methodology is rigorously evaluated through extensive experimentation\non the newly introduced NuScenes-T dataset, with additional validation of its\nzero-shot performance on the Lyft Level 5 dataset. This research makes a\nsubstantive contribution to the advancement of autonomous driving technologies\nby leveraging multimodal data to enhance open vocabulary perception in 3D\nenvironments, thereby pushing the boundaries of what is achievable in\nautonomous navigation and perception.\n", "link": "http://arxiv.org/abs/2408.10880v1", "date": "2024-08-20", "relevancy": 2.3113, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5939}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5726}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%203D%20World%20in%20Autonomous%20Driving&body=Title%3A%20Open%203D%20World%20in%20Autonomous%20Driving%0AAuthor%3A%20Xinlong%20Cheng%20and%20Lei%20Li%0AAbstract%3A%20%20%20The%20capability%20for%20open%20vocabulary%20perception%20represents%20a%20significant%0Aadvancement%20in%20autonomous%20driving%20systems%2C%20facilitating%20the%20comprehension%20and%0Ainterpretation%20of%20a%20wide%20array%20of%20textual%20inputs%20in%20real-time.%20Despite%0Aextensive%20research%20in%20open%20vocabulary%20tasks%20within%202D%20computer%20vision%2C%20the%0Aapplication%20of%20such%20methodologies%20to%203D%20environments%2C%20particularly%20within%0Alarge-scale%20outdoor%20contexts%2C%20remains%20relatively%20underdeveloped.%20This%20paper%0Apresents%20a%20novel%20approach%20that%20integrates%203D%20point%20cloud%20data%2C%20acquired%20from%0ALIDAR%20sensors%2C%20with%20textual%20information.%20The%20primary%20focus%20is%20on%20the%0Autilization%20of%20textual%20data%20to%20directly%20localize%20and%20identify%20objects%20within%0Athe%20autonomous%20driving%20context.%20We%20introduce%20an%20efficient%20framework%20for%20the%0Afusion%20of%20bird%27s-eye%20view%20%28BEV%29%20region%20features%20with%20textual%20features%2C%20thereby%0Aenabling%20the%20system%20to%20seamlessly%20adapt%20to%20novel%20textual%20inputs%20and%20enhancing%0Athe%20robustness%20of%20open%20vocabulary%20detection%20tasks.%20The%20effectiveness%20of%20the%0Aproposed%20methodology%20is%20rigorously%20evaluated%20through%20extensive%20experimentation%0Aon%20the%20newly%20introduced%20NuScenes-T%20dataset%2C%20with%20additional%20validation%20of%20its%0Azero-shot%20performance%20on%20the%20Lyft%20Level%205%20dataset.%20This%20research%20makes%20a%0Asubstantive%20contribution%20to%20the%20advancement%20of%20autonomous%20driving%20technologies%0Aby%20leveraging%20multimodal%20data%20to%20enhance%20open%20vocabulary%20perception%20in%203D%0Aenvironments%2C%20thereby%20pushing%20the%20boundaries%20of%20what%20is%20achievable%20in%0Aautonomous%20navigation%20and%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%25203D%2520World%2520in%2520Autonomous%2520Driving%26entry.906535625%3DXinlong%2520Cheng%2520and%2520Lei%2520Li%26entry.1292438233%3D%2520%2520The%2520capability%2520for%2520open%2520vocabulary%2520perception%2520represents%2520a%2520significant%250Aadvancement%2520in%2520autonomous%2520driving%2520systems%252C%2520facilitating%2520the%2520comprehension%2520and%250Ainterpretation%2520of%2520a%2520wide%2520array%2520of%2520textual%2520inputs%2520in%2520real-time.%2520Despite%250Aextensive%2520research%2520in%2520open%2520vocabulary%2520tasks%2520within%25202D%2520computer%2520vision%252C%2520the%250Aapplication%2520of%2520such%2520methodologies%2520to%25203D%2520environments%252C%2520particularly%2520within%250Alarge-scale%2520outdoor%2520contexts%252C%2520remains%2520relatively%2520underdeveloped.%2520This%2520paper%250Apresents%2520a%2520novel%2520approach%2520that%2520integrates%25203D%2520point%2520cloud%2520data%252C%2520acquired%2520from%250ALIDAR%2520sensors%252C%2520with%2520textual%2520information.%2520The%2520primary%2520focus%2520is%2520on%2520the%250Autilization%2520of%2520textual%2520data%2520to%2520directly%2520localize%2520and%2520identify%2520objects%2520within%250Athe%2520autonomous%2520driving%2520context.%2520We%2520introduce%2520an%2520efficient%2520framework%2520for%2520the%250Afusion%2520of%2520bird%2527s-eye%2520view%2520%2528BEV%2529%2520region%2520features%2520with%2520textual%2520features%252C%2520thereby%250Aenabling%2520the%2520system%2520to%2520seamlessly%2520adapt%2520to%2520novel%2520textual%2520inputs%2520and%2520enhancing%250Athe%2520robustness%2520of%2520open%2520vocabulary%2520detection%2520tasks.%2520The%2520effectiveness%2520of%2520the%250Aproposed%2520methodology%2520is%2520rigorously%2520evaluated%2520through%2520extensive%2520experimentation%250Aon%2520the%2520newly%2520introduced%2520NuScenes-T%2520dataset%252C%2520with%2520additional%2520validation%2520of%2520its%250Azero-shot%2520performance%2520on%2520the%2520Lyft%2520Level%25205%2520dataset.%2520This%2520research%2520makes%2520a%250Asubstantive%2520contribution%2520to%2520the%2520advancement%2520of%2520autonomous%2520driving%2520technologies%250Aby%2520leveraging%2520multimodal%2520data%2520to%2520enhance%2520open%2520vocabulary%2520perception%2520in%25203D%250Aenvironments%252C%2520thereby%2520pushing%2520the%2520boundaries%2520of%2520what%2520is%2520achievable%2520in%250Aautonomous%2520navigation%2520and%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%203D%20World%20in%20Autonomous%20Driving&entry.906535625=Xinlong%20Cheng%20and%20Lei%20Li&entry.1292438233=%20%20The%20capability%20for%20open%20vocabulary%20perception%20represents%20a%20significant%0Aadvancement%20in%20autonomous%20driving%20systems%2C%20facilitating%20the%20comprehension%20and%0Ainterpretation%20of%20a%20wide%20array%20of%20textual%20inputs%20in%20real-time.%20Despite%0Aextensive%20research%20in%20open%20vocabulary%20tasks%20within%202D%20computer%20vision%2C%20the%0Aapplication%20of%20such%20methodologies%20to%203D%20environments%2C%20particularly%20within%0Alarge-scale%20outdoor%20contexts%2C%20remains%20relatively%20underdeveloped.%20This%20paper%0Apresents%20a%20novel%20approach%20that%20integrates%203D%20point%20cloud%20data%2C%20acquired%20from%0ALIDAR%20sensors%2C%20with%20textual%20information.%20The%20primary%20focus%20is%20on%20the%0Autilization%20of%20textual%20data%20to%20directly%20localize%20and%20identify%20objects%20within%0Athe%20autonomous%20driving%20context.%20We%20introduce%20an%20efficient%20framework%20for%20the%0Afusion%20of%20bird%27s-eye%20view%20%28BEV%29%20region%20features%20with%20textual%20features%2C%20thereby%0Aenabling%20the%20system%20to%20seamlessly%20adapt%20to%20novel%20textual%20inputs%20and%20enhancing%0Athe%20robustness%20of%20open%20vocabulary%20detection%20tasks.%20The%20effectiveness%20of%20the%0Aproposed%20methodology%20is%20rigorously%20evaluated%20through%20extensive%20experimentation%0Aon%20the%20newly%20introduced%20NuScenes-T%20dataset%2C%20with%20additional%20validation%20of%20its%0Azero-shot%20performance%20on%20the%20Lyft%20Level%205%20dataset.%20This%20research%20makes%20a%0Asubstantive%20contribution%20to%20the%20advancement%20of%20autonomous%20driving%20technologies%0Aby%20leveraging%20multimodal%20data%20to%20enhance%20open%20vocabulary%20perception%20in%203D%0Aenvironments%2C%20thereby%20pushing%20the%20boundaries%20of%20what%20is%20achievable%20in%0Aautonomous%20navigation%20and%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10880v1&entry.124074799=Read"},
{"title": "Large Language Models for Multimodal Deformable Image Registration", "author": "Mingrui Ma and Weijie Wang and Jie Ning and Jianfeng He and Nicu Sebe and Bruno Lepri", "abstract": "  The challenge of Multimodal Deformable Image Registration (MDIR) lies in the\nconversion and alignment of features between images of different modalities.\nGenerative models (GMs) cannot retain the necessary information enough from the\nsource modality to the target one, while non-GMs struggle to align features\nacross these two modalities. In this paper, we propose a novel coarse-to-fine\nMDIR framework,LLM-Morph, which is applicable to various pre-trained Large\nLanguage Models (LLMs) to solve these concerns by aligning the deep features\nfrom different modal medical images. Specifically, we first utilize a CNN\nencoder to extract deep visual features from cross-modal image pairs, then we\nuse the first adapter to adjust these tokens, and use LoRA in pre-trained LLMs\nto fine-tune their weights, both aimed at eliminating the domain gap between\nthe pre-trained LLMs and the MDIR task. Third, for the alignment of tokens, we\nutilize other four adapters to transform the LLM-encoded tokens into\nmulti-scale visual features, generating multi-scale deformation fields and\nfacilitating the coarse-to-fine MDIR task. Extensive experiments in MR-CT\nAbdomen and SR-Reg Brain datasets demonstrate the effectiveness of our\nframework and the potential of pre-trained LLMs for MDIR task. Our code is\navailabel at: https://github.com/ninjannn/LLM-Morph.\n", "link": "http://arxiv.org/abs/2408.10703v1", "date": "2024-08-20", "relevancy": 2.3024, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5962}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5692}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20for%20Multimodal%20Deformable%20Image%20Registration&body=Title%3A%20Large%20Language%20Models%20for%20Multimodal%20Deformable%20Image%20Registration%0AAuthor%3A%20Mingrui%20Ma%20and%20Weijie%20Wang%20and%20Jie%20Ning%20and%20Jianfeng%20He%20and%20Nicu%20Sebe%20and%20Bruno%20Lepri%0AAbstract%3A%20%20%20The%20challenge%20of%20Multimodal%20Deformable%20Image%20Registration%20%28MDIR%29%20lies%20in%20the%0Aconversion%20and%20alignment%20of%20features%20between%20images%20of%20different%20modalities.%0AGenerative%20models%20%28GMs%29%20cannot%20retain%20the%20necessary%20information%20enough%20from%20the%0Asource%20modality%20to%20the%20target%20one%2C%20while%20non-GMs%20struggle%20to%20align%20features%0Aacross%20these%20two%20modalities.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20coarse-to-fine%0AMDIR%20framework%2CLLM-Morph%2C%20which%20is%20applicable%20to%20various%20pre-trained%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20solve%20these%20concerns%20by%20aligning%20the%20deep%20features%0Afrom%20different%20modal%20medical%20images.%20Specifically%2C%20we%20first%20utilize%20a%20CNN%0Aencoder%20to%20extract%20deep%20visual%20features%20from%20cross-modal%20image%20pairs%2C%20then%20we%0Ause%20the%20first%20adapter%20to%20adjust%20these%20tokens%2C%20and%20use%20LoRA%20in%20pre-trained%20LLMs%0Ato%20fine-tune%20their%20weights%2C%20both%20aimed%20at%20eliminating%20the%20domain%20gap%20between%0Athe%20pre-trained%20LLMs%20and%20the%20MDIR%20task.%20Third%2C%20for%20the%20alignment%20of%20tokens%2C%20we%0Autilize%20other%20four%20adapters%20to%20transform%20the%20LLM-encoded%20tokens%20into%0Amulti-scale%20visual%20features%2C%20generating%20multi-scale%20deformation%20fields%20and%0Afacilitating%20the%20coarse-to-fine%20MDIR%20task.%20Extensive%20experiments%20in%20MR-CT%0AAbdomen%20and%20SR-Reg%20Brain%20datasets%20demonstrate%20the%20effectiveness%20of%20our%0Aframework%20and%20the%20potential%20of%20pre-trained%20LLMs%20for%20MDIR%20task.%20Our%20code%20is%0Aavailabel%20at%3A%20https%3A//github.com/ninjannn/LLM-Morph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520for%2520Multimodal%2520Deformable%2520Image%2520Registration%26entry.906535625%3DMingrui%2520Ma%2520and%2520Weijie%2520Wang%2520and%2520Jie%2520Ning%2520and%2520Jianfeng%2520He%2520and%2520Nicu%2520Sebe%2520and%2520Bruno%2520Lepri%26entry.1292438233%3D%2520%2520The%2520challenge%2520of%2520Multimodal%2520Deformable%2520Image%2520Registration%2520%2528MDIR%2529%2520lies%2520in%2520the%250Aconversion%2520and%2520alignment%2520of%2520features%2520between%2520images%2520of%2520different%2520modalities.%250AGenerative%2520models%2520%2528GMs%2529%2520cannot%2520retain%2520the%2520necessary%2520information%2520enough%2520from%2520the%250Asource%2520modality%2520to%2520the%2520target%2520one%252C%2520while%2520non-GMs%2520struggle%2520to%2520align%2520features%250Aacross%2520these%2520two%2520modalities.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520coarse-to-fine%250AMDIR%2520framework%252CLLM-Morph%252C%2520which%2520is%2520applicable%2520to%2520various%2520pre-trained%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520solve%2520these%2520concerns%2520by%2520aligning%2520the%2520deep%2520features%250Afrom%2520different%2520modal%2520medical%2520images.%2520Specifically%252C%2520we%2520first%2520utilize%2520a%2520CNN%250Aencoder%2520to%2520extract%2520deep%2520visual%2520features%2520from%2520cross-modal%2520image%2520pairs%252C%2520then%2520we%250Ause%2520the%2520first%2520adapter%2520to%2520adjust%2520these%2520tokens%252C%2520and%2520use%2520LoRA%2520in%2520pre-trained%2520LLMs%250Ato%2520fine-tune%2520their%2520weights%252C%2520both%2520aimed%2520at%2520eliminating%2520the%2520domain%2520gap%2520between%250Athe%2520pre-trained%2520LLMs%2520and%2520the%2520MDIR%2520task.%2520Third%252C%2520for%2520the%2520alignment%2520of%2520tokens%252C%2520we%250Autilize%2520other%2520four%2520adapters%2520to%2520transform%2520the%2520LLM-encoded%2520tokens%2520into%250Amulti-scale%2520visual%2520features%252C%2520generating%2520multi-scale%2520deformation%2520fields%2520and%250Afacilitating%2520the%2520coarse-to-fine%2520MDIR%2520task.%2520Extensive%2520experiments%2520in%2520MR-CT%250AAbdomen%2520and%2520SR-Reg%2520Brain%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aframework%2520and%2520the%2520potential%2520of%2520pre-trained%2520LLMs%2520for%2520MDIR%2520task.%2520Our%2520code%2520is%250Aavailabel%2520at%253A%2520https%253A//github.com/ninjannn/LLM-Morph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20for%20Multimodal%20Deformable%20Image%20Registration&entry.906535625=Mingrui%20Ma%20and%20Weijie%20Wang%20and%20Jie%20Ning%20and%20Jianfeng%20He%20and%20Nicu%20Sebe%20and%20Bruno%20Lepri&entry.1292438233=%20%20The%20challenge%20of%20Multimodal%20Deformable%20Image%20Registration%20%28MDIR%29%20lies%20in%20the%0Aconversion%20and%20alignment%20of%20features%20between%20images%20of%20different%20modalities.%0AGenerative%20models%20%28GMs%29%20cannot%20retain%20the%20necessary%20information%20enough%20from%20the%0Asource%20modality%20to%20the%20target%20one%2C%20while%20non-GMs%20struggle%20to%20align%20features%0Aacross%20these%20two%20modalities.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20coarse-to-fine%0AMDIR%20framework%2CLLM-Morph%2C%20which%20is%20applicable%20to%20various%20pre-trained%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20solve%20these%20concerns%20by%20aligning%20the%20deep%20features%0Afrom%20different%20modal%20medical%20images.%20Specifically%2C%20we%20first%20utilize%20a%20CNN%0Aencoder%20to%20extract%20deep%20visual%20features%20from%20cross-modal%20image%20pairs%2C%20then%20we%0Ause%20the%20first%20adapter%20to%20adjust%20these%20tokens%2C%20and%20use%20LoRA%20in%20pre-trained%20LLMs%0Ato%20fine-tune%20their%20weights%2C%20both%20aimed%20at%20eliminating%20the%20domain%20gap%20between%0Athe%20pre-trained%20LLMs%20and%20the%20MDIR%20task.%20Third%2C%20for%20the%20alignment%20of%20tokens%2C%20we%0Autilize%20other%20four%20adapters%20to%20transform%20the%20LLM-encoded%20tokens%20into%0Amulti-scale%20visual%20features%2C%20generating%20multi-scale%20deformation%20fields%20and%0Afacilitating%20the%20coarse-to-fine%20MDIR%20task.%20Extensive%20experiments%20in%20MR-CT%0AAbdomen%20and%20SR-Reg%20Brain%20datasets%20demonstrate%20the%20effectiveness%20of%20our%0Aframework%20and%20the%20potential%20of%20pre-trained%20LLMs%20for%20MDIR%20task.%20Our%20code%20is%0Aavailabel%20at%3A%20https%3A//github.com/ninjannn/LLM-Morph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10703v1&entry.124074799=Read"},
{"title": "PriorMapNet: Enhancing Online Vectorized HD Map Construction with Priors", "author": "Rongxuan Wang and Xin Lu and Xiaoyang Liu and Xiaoyi Zou and Tongyi Cao and Ying Li", "abstract": "  Online vectorized High-Definition (HD) map construction is crucial for\nsubsequent prediction and planning tasks in autonomous driving. Following MapTR\nparadigm, recent works have made noteworthy achievements. However, reference\npoints are randomly initialized in mainstream methods, leading to unstable\nmatching between predictions and ground truth. To address this issue, we\nintroduce PriorMapNet to enhance online vectorized HD map construction with\npriors. We propose the PPS-Decoder, which provides reference points with\nposition and structure priors. Fitted from the map elements in the dataset,\nprior reference points lower the learning difficulty and achieve stable\nmatching. Furthermore, we propose the PF-Encoder to enhance the image-to-BEV\ntransformation with BEV feature priors. Besides, we propose the DMD\ncross-attention, which decouples cross-attention along multi-scale and\nmulti-sample respectively to achieve efficiency. Our proposed PriorMapNet\nachieves state-of-the-art performance in the online vectorized HD map\nconstruction task on nuScenes and Argoverse2 datasets. The code will be\nreleased publicly soon.\n", "link": "http://arxiv.org/abs/2408.08802v2", "date": "2024-08-20", "relevancy": 2.2927, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6075}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5599}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PriorMapNet%3A%20Enhancing%20Online%20Vectorized%20HD%20Map%20Construction%20with%20Priors&body=Title%3A%20PriorMapNet%3A%20Enhancing%20Online%20Vectorized%20HD%20Map%20Construction%20with%20Priors%0AAuthor%3A%20Rongxuan%20Wang%20and%20Xin%20Lu%20and%20Xiaoyang%20Liu%20and%20Xiaoyi%20Zou%20and%20Tongyi%20Cao%20and%20Ying%20Li%0AAbstract%3A%20%20%20Online%20vectorized%20High-Definition%20%28HD%29%20map%20construction%20is%20crucial%20for%0Asubsequent%20prediction%20and%20planning%20tasks%20in%20autonomous%20driving.%20Following%20MapTR%0Aparadigm%2C%20recent%20works%20have%20made%20noteworthy%20achievements.%20However%2C%20reference%0Apoints%20are%20randomly%20initialized%20in%20mainstream%20methods%2C%20leading%20to%20unstable%0Amatching%20between%20predictions%20and%20ground%20truth.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20PriorMapNet%20to%20enhance%20online%20vectorized%20HD%20map%20construction%20with%0Apriors.%20We%20propose%20the%20PPS-Decoder%2C%20which%20provides%20reference%20points%20with%0Aposition%20and%20structure%20priors.%20Fitted%20from%20the%20map%20elements%20in%20the%20dataset%2C%0Aprior%20reference%20points%20lower%20the%20learning%20difficulty%20and%20achieve%20stable%0Amatching.%20Furthermore%2C%20we%20propose%20the%20PF-Encoder%20to%20enhance%20the%20image-to-BEV%0Atransformation%20with%20BEV%20feature%20priors.%20Besides%2C%20we%20propose%20the%20DMD%0Across-attention%2C%20which%20decouples%20cross-attention%20along%20multi-scale%20and%0Amulti-sample%20respectively%20to%20achieve%20efficiency.%20Our%20proposed%20PriorMapNet%0Aachieves%20state-of-the-art%20performance%20in%20the%20online%20vectorized%20HD%20map%0Aconstruction%20task%20on%20nuScenes%20and%20Argoverse2%20datasets.%20The%20code%20will%20be%0Areleased%20publicly%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08802v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPriorMapNet%253A%2520Enhancing%2520Online%2520Vectorized%2520HD%2520Map%2520Construction%2520with%2520Priors%26entry.906535625%3DRongxuan%2520Wang%2520and%2520Xin%2520Lu%2520and%2520Xiaoyang%2520Liu%2520and%2520Xiaoyi%2520Zou%2520and%2520Tongyi%2520Cao%2520and%2520Ying%2520Li%26entry.1292438233%3D%2520%2520Online%2520vectorized%2520High-Definition%2520%2528HD%2529%2520map%2520construction%2520is%2520crucial%2520for%250Asubsequent%2520prediction%2520and%2520planning%2520tasks%2520in%2520autonomous%2520driving.%2520Following%2520MapTR%250Aparadigm%252C%2520recent%2520works%2520have%2520made%2520noteworthy%2520achievements.%2520However%252C%2520reference%250Apoints%2520are%2520randomly%2520initialized%2520in%2520mainstream%2520methods%252C%2520leading%2520to%2520unstable%250Amatching%2520between%2520predictions%2520and%2520ground%2520truth.%2520To%2520address%2520this%2520issue%252C%2520we%250Aintroduce%2520PriorMapNet%2520to%2520enhance%2520online%2520vectorized%2520HD%2520map%2520construction%2520with%250Apriors.%2520We%2520propose%2520the%2520PPS-Decoder%252C%2520which%2520provides%2520reference%2520points%2520with%250Aposition%2520and%2520structure%2520priors.%2520Fitted%2520from%2520the%2520map%2520elements%2520in%2520the%2520dataset%252C%250Aprior%2520reference%2520points%2520lower%2520the%2520learning%2520difficulty%2520and%2520achieve%2520stable%250Amatching.%2520Furthermore%252C%2520we%2520propose%2520the%2520PF-Encoder%2520to%2520enhance%2520the%2520image-to-BEV%250Atransformation%2520with%2520BEV%2520feature%2520priors.%2520Besides%252C%2520we%2520propose%2520the%2520DMD%250Across-attention%252C%2520which%2520decouples%2520cross-attention%2520along%2520multi-scale%2520and%250Amulti-sample%2520respectively%2520to%2520achieve%2520efficiency.%2520Our%2520proposed%2520PriorMapNet%250Aachieves%2520state-of-the-art%2520performance%2520in%2520the%2520online%2520vectorized%2520HD%2520map%250Aconstruction%2520task%2520on%2520nuScenes%2520and%2520Argoverse2%2520datasets.%2520The%2520code%2520will%2520be%250Areleased%2520publicly%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08802v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PriorMapNet%3A%20Enhancing%20Online%20Vectorized%20HD%20Map%20Construction%20with%20Priors&entry.906535625=Rongxuan%20Wang%20and%20Xin%20Lu%20and%20Xiaoyang%20Liu%20and%20Xiaoyi%20Zou%20and%20Tongyi%20Cao%20and%20Ying%20Li&entry.1292438233=%20%20Online%20vectorized%20High-Definition%20%28HD%29%20map%20construction%20is%20crucial%20for%0Asubsequent%20prediction%20and%20planning%20tasks%20in%20autonomous%20driving.%20Following%20MapTR%0Aparadigm%2C%20recent%20works%20have%20made%20noteworthy%20achievements.%20However%2C%20reference%0Apoints%20are%20randomly%20initialized%20in%20mainstream%20methods%2C%20leading%20to%20unstable%0Amatching%20between%20predictions%20and%20ground%20truth.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20PriorMapNet%20to%20enhance%20online%20vectorized%20HD%20map%20construction%20with%0Apriors.%20We%20propose%20the%20PPS-Decoder%2C%20which%20provides%20reference%20points%20with%0Aposition%20and%20structure%20priors.%20Fitted%20from%20the%20map%20elements%20in%20the%20dataset%2C%0Aprior%20reference%20points%20lower%20the%20learning%20difficulty%20and%20achieve%20stable%0Amatching.%20Furthermore%2C%20we%20propose%20the%20PF-Encoder%20to%20enhance%20the%20image-to-BEV%0Atransformation%20with%20BEV%20feature%20priors.%20Besides%2C%20we%20propose%20the%20DMD%0Across-attention%2C%20which%20decouples%20cross-attention%20along%20multi-scale%20and%0Amulti-sample%20respectively%20to%20achieve%20efficiency.%20Our%20proposed%20PriorMapNet%0Aachieves%20state-of-the-art%20performance%20in%20the%20online%20vectorized%20HD%20map%0Aconstruction%20task%20on%20nuScenes%20and%20Argoverse2%20datasets.%20The%20code%20will%20be%0Areleased%20publicly%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08802v2&entry.124074799=Read"},
{"title": "SDI-Net: Toward Sufficient Dual-View Interaction for Low-light Stereo\n  Image Enhancement", "author": "Linlin Hu and Ao Sun and Shijie Hao and Richang Hong and Meng Wang", "abstract": "  Currently, most low-light image enhancement methods only consider information\nfrom a single view, neglecting the correlation between cross-view information.\nTherefore, the enhancement results produced by these methods are often\nunsatisfactory. In this context, there have been efforts to develop methods\nspecifically for low-light stereo image enhancement. These methods take into\naccount the cross-view disparities and enable interaction between the left and\nright views, leading to improved performance. However, these methods still do\nnot fully exploit the interaction between left and right view information. To\naddress this issue, we propose a model called Toward Sufficient Dual-View\nInteraction for Low-light Stereo Image Enhancement (SDI-Net). The backbone\nstructure of SDI-Net is two encoder-decoder pairs, which are used to learn the\nmapping function from low-light images to normal-light images. Among the\nencoders and the decoders, we design a module named Cross-View Sufficient\nInteraction Module (CSIM), aiming to fully exploit the correlations between the\nbinocular views via the attention mechanism. The quantitative and visual\nresults on public datasets validate the superiority of our method over other\nrelated methods. Ablation studies also demonstrate the effectiveness of the key\nelements in our model.\n", "link": "http://arxiv.org/abs/2408.10934v1", "date": "2024-08-20", "relevancy": 2.2897, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6101}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5769}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDI-Net%3A%20Toward%20Sufficient%20Dual-View%20Interaction%20for%20Low-light%20Stereo%0A%20%20Image%20Enhancement&body=Title%3A%20SDI-Net%3A%20Toward%20Sufficient%20Dual-View%20Interaction%20for%20Low-light%20Stereo%0A%20%20Image%20Enhancement%0AAuthor%3A%20Linlin%20Hu%20and%20Ao%20Sun%20and%20Shijie%20Hao%20and%20Richang%20Hong%20and%20Meng%20Wang%0AAbstract%3A%20%20%20Currently%2C%20most%20low-light%20image%20enhancement%20methods%20only%20consider%20information%0Afrom%20a%20single%20view%2C%20neglecting%20the%20correlation%20between%20cross-view%20information.%0ATherefore%2C%20the%20enhancement%20results%20produced%20by%20these%20methods%20are%20often%0Aunsatisfactory.%20In%20this%20context%2C%20there%20have%20been%20efforts%20to%20develop%20methods%0Aspecifically%20for%20low-light%20stereo%20image%20enhancement.%20These%20methods%20take%20into%0Aaccount%20the%20cross-view%20disparities%20and%20enable%20interaction%20between%20the%20left%20and%0Aright%20views%2C%20leading%20to%20improved%20performance.%20However%2C%20these%20methods%20still%20do%0Anot%20fully%20exploit%20the%20interaction%20between%20left%20and%20right%20view%20information.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20model%20called%20Toward%20Sufficient%20Dual-View%0AInteraction%20for%20Low-light%20Stereo%20Image%20Enhancement%20%28SDI-Net%29.%20The%20backbone%0Astructure%20of%20SDI-Net%20is%20two%20encoder-decoder%20pairs%2C%20which%20are%20used%20to%20learn%20the%0Amapping%20function%20from%20low-light%20images%20to%20normal-light%20images.%20Among%20the%0Aencoders%20and%20the%20decoders%2C%20we%20design%20a%20module%20named%20Cross-View%20Sufficient%0AInteraction%20Module%20%28CSIM%29%2C%20aiming%20to%20fully%20exploit%20the%20correlations%20between%20the%0Abinocular%20views%20via%20the%20attention%20mechanism.%20The%20quantitative%20and%20visual%0Aresults%20on%20public%20datasets%20validate%20the%20superiority%20of%20our%20method%20over%20other%0Arelated%20methods.%20Ablation%20studies%20also%20demonstrate%20the%20effectiveness%20of%20the%20key%0Aelements%20in%20our%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDI-Net%253A%2520Toward%2520Sufficient%2520Dual-View%2520Interaction%2520for%2520Low-light%2520Stereo%250A%2520%2520Image%2520Enhancement%26entry.906535625%3DLinlin%2520Hu%2520and%2520Ao%2520Sun%2520and%2520Shijie%2520Hao%2520and%2520Richang%2520Hong%2520and%2520Meng%2520Wang%26entry.1292438233%3D%2520%2520Currently%252C%2520most%2520low-light%2520image%2520enhancement%2520methods%2520only%2520consider%2520information%250Afrom%2520a%2520single%2520view%252C%2520neglecting%2520the%2520correlation%2520between%2520cross-view%2520information.%250ATherefore%252C%2520the%2520enhancement%2520results%2520produced%2520by%2520these%2520methods%2520are%2520often%250Aunsatisfactory.%2520In%2520this%2520context%252C%2520there%2520have%2520been%2520efforts%2520to%2520develop%2520methods%250Aspecifically%2520for%2520low-light%2520stereo%2520image%2520enhancement.%2520These%2520methods%2520take%2520into%250Aaccount%2520the%2520cross-view%2520disparities%2520and%2520enable%2520interaction%2520between%2520the%2520left%2520and%250Aright%2520views%252C%2520leading%2520to%2520improved%2520performance.%2520However%252C%2520these%2520methods%2520still%2520do%250Anot%2520fully%2520exploit%2520the%2520interaction%2520between%2520left%2520and%2520right%2520view%2520information.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520a%2520model%2520called%2520Toward%2520Sufficient%2520Dual-View%250AInteraction%2520for%2520Low-light%2520Stereo%2520Image%2520Enhancement%2520%2528SDI-Net%2529.%2520The%2520backbone%250Astructure%2520of%2520SDI-Net%2520is%2520two%2520encoder-decoder%2520pairs%252C%2520which%2520are%2520used%2520to%2520learn%2520the%250Amapping%2520function%2520from%2520low-light%2520images%2520to%2520normal-light%2520images.%2520Among%2520the%250Aencoders%2520and%2520the%2520decoders%252C%2520we%2520design%2520a%2520module%2520named%2520Cross-View%2520Sufficient%250AInteraction%2520Module%2520%2528CSIM%2529%252C%2520aiming%2520to%2520fully%2520exploit%2520the%2520correlations%2520between%2520the%250Abinocular%2520views%2520via%2520the%2520attention%2520mechanism.%2520The%2520quantitative%2520and%2520visual%250Aresults%2520on%2520public%2520datasets%2520validate%2520the%2520superiority%2520of%2520our%2520method%2520over%2520other%250Arelated%2520methods.%2520Ablation%2520studies%2520also%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520key%250Aelements%2520in%2520our%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDI-Net%3A%20Toward%20Sufficient%20Dual-View%20Interaction%20for%20Low-light%20Stereo%0A%20%20Image%20Enhancement&entry.906535625=Linlin%20Hu%20and%20Ao%20Sun%20and%20Shijie%20Hao%20and%20Richang%20Hong%20and%20Meng%20Wang&entry.1292438233=%20%20Currently%2C%20most%20low-light%20image%20enhancement%20methods%20only%20consider%20information%0Afrom%20a%20single%20view%2C%20neglecting%20the%20correlation%20between%20cross-view%20information.%0ATherefore%2C%20the%20enhancement%20results%20produced%20by%20these%20methods%20are%20often%0Aunsatisfactory.%20In%20this%20context%2C%20there%20have%20been%20efforts%20to%20develop%20methods%0Aspecifically%20for%20low-light%20stereo%20image%20enhancement.%20These%20methods%20take%20into%0Aaccount%20the%20cross-view%20disparities%20and%20enable%20interaction%20between%20the%20left%20and%0Aright%20views%2C%20leading%20to%20improved%20performance.%20However%2C%20these%20methods%20still%20do%0Anot%20fully%20exploit%20the%20interaction%20between%20left%20and%20right%20view%20information.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20model%20called%20Toward%20Sufficient%20Dual-View%0AInteraction%20for%20Low-light%20Stereo%20Image%20Enhancement%20%28SDI-Net%29.%20The%20backbone%0Astructure%20of%20SDI-Net%20is%20two%20encoder-decoder%20pairs%2C%20which%20are%20used%20to%20learn%20the%0Amapping%20function%20from%20low-light%20images%20to%20normal-light%20images.%20Among%20the%0Aencoders%20and%20the%20decoders%2C%20we%20design%20a%20module%20named%20Cross-View%20Sufficient%0AInteraction%20Module%20%28CSIM%29%2C%20aiming%20to%20fully%20exploit%20the%20correlations%20between%20the%0Abinocular%20views%20via%20the%20attention%20mechanism.%20The%20quantitative%20and%20visual%0Aresults%20on%20public%20datasets%20validate%20the%20superiority%20of%20our%20method%20over%20other%0Arelated%20methods.%20Ablation%20studies%20also%20demonstrate%20the%20effectiveness%20of%20the%20key%0Aelements%20in%20our%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10934v1&entry.124074799=Read"},
{"title": "Unc-TTP: A Method for Classifying LLM Uncertainty to Improve In-Context\n  Example Selection", "author": "Hsiu-Yuan Huang and Zichen Wu and Yutong Yang and Junzhao Zhang and Yunfang Wu", "abstract": "  Nowadays, Large Language Models (LLMs) have demonstrated exceptional\nperformance across various downstream tasks. However, it is challenging for\nusers to discern whether the responses are generated with certainty or are\nfabricated to meet user expectations. Estimating the uncertainty of LLMs is\nparticularly challenging due to their vast scale and the lack of white-box\naccess. In this work, we propose a novel Uncertainty Tripartite Testing\nParadigm (Unc-TTP) to classify LLM uncertainty, via evaluating the consistency\nof LLM outputs when incorporating label interference into the sampling-based\napproach. Based on Unc-TTP outputs, we aggregate instances into certain and\nuncertain categories. Further, we conduct a detailed analysis of the\nuncertainty properties of LLMs and show Unc-TTP's superiority over the existing\nsampling-based methods. In addition, we leverage the obtained uncertainty\ninformation to guide in-context example selection, demonstrating that Unc-TTP\nobviously outperforms retrieval-based and sampling-based approaches in\nselecting more informative examples. Our work paves a new way to classify the\nuncertainty of both open- and closed-source LLMs, and introduces a practical\napproach to exploit this uncertainty to improve LLMs performance.\n", "link": "http://arxiv.org/abs/2408.09172v2", "date": "2024-08-20", "relevancy": 2.2828, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6569}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5715}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unc-TTP%3A%20A%20Method%20for%20Classifying%20LLM%20Uncertainty%20to%20Improve%20In-Context%0A%20%20Example%20Selection&body=Title%3A%20Unc-TTP%3A%20A%20Method%20for%20Classifying%20LLM%20Uncertainty%20to%20Improve%20In-Context%0A%20%20Example%20Selection%0AAuthor%3A%20Hsiu-Yuan%20Huang%20and%20Zichen%20Wu%20and%20Yutong%20Yang%20and%20Junzhao%20Zhang%20and%20Yunfang%20Wu%0AAbstract%3A%20%20%20Nowadays%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20exceptional%0Aperformance%20across%20various%20downstream%20tasks.%20However%2C%20it%20is%20challenging%20for%0Ausers%20to%20discern%20whether%20the%20responses%20are%20generated%20with%20certainty%20or%20are%0Afabricated%20to%20meet%20user%20expectations.%20Estimating%20the%20uncertainty%20of%20LLMs%20is%0Aparticularly%20challenging%20due%20to%20their%20vast%20scale%20and%20the%20lack%20of%20white-box%0Aaccess.%20In%20this%20work%2C%20we%20propose%20a%20novel%20Uncertainty%20Tripartite%20Testing%0AParadigm%20%28Unc-TTP%29%20to%20classify%20LLM%20uncertainty%2C%20via%20evaluating%20the%20consistency%0Aof%20LLM%20outputs%20when%20incorporating%20label%20interference%20into%20the%20sampling-based%0Aapproach.%20Based%20on%20Unc-TTP%20outputs%2C%20we%20aggregate%20instances%20into%20certain%20and%0Auncertain%20categories.%20Further%2C%20we%20conduct%20a%20detailed%20analysis%20of%20the%0Auncertainty%20properties%20of%20LLMs%20and%20show%20Unc-TTP%27s%20superiority%20over%20the%20existing%0Asampling-based%20methods.%20In%20addition%2C%20we%20leverage%20the%20obtained%20uncertainty%0Ainformation%20to%20guide%20in-context%20example%20selection%2C%20demonstrating%20that%20Unc-TTP%0Aobviously%20outperforms%20retrieval-based%20and%20sampling-based%20approaches%20in%0Aselecting%20more%20informative%20examples.%20Our%20work%20paves%20a%20new%20way%20to%20classify%20the%0Auncertainty%20of%20both%20open-%20and%20closed-source%20LLMs%2C%20and%20introduces%20a%20practical%0Aapproach%20to%20exploit%20this%20uncertainty%20to%20improve%20LLMs%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09172v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnc-TTP%253A%2520A%2520Method%2520for%2520Classifying%2520LLM%2520Uncertainty%2520to%2520Improve%2520In-Context%250A%2520%2520Example%2520Selection%26entry.906535625%3DHsiu-Yuan%2520Huang%2520and%2520Zichen%2520Wu%2520and%2520Yutong%2520Yang%2520and%2520Junzhao%2520Zhang%2520and%2520Yunfang%2520Wu%26entry.1292438233%3D%2520%2520Nowadays%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520exceptional%250Aperformance%2520across%2520various%2520downstream%2520tasks.%2520However%252C%2520it%2520is%2520challenging%2520for%250Ausers%2520to%2520discern%2520whether%2520the%2520responses%2520are%2520generated%2520with%2520certainty%2520or%2520are%250Afabricated%2520to%2520meet%2520user%2520expectations.%2520Estimating%2520the%2520uncertainty%2520of%2520LLMs%2520is%250Aparticularly%2520challenging%2520due%2520to%2520their%2520vast%2520scale%2520and%2520the%2520lack%2520of%2520white-box%250Aaccess.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520Uncertainty%2520Tripartite%2520Testing%250AParadigm%2520%2528Unc-TTP%2529%2520to%2520classify%2520LLM%2520uncertainty%252C%2520via%2520evaluating%2520the%2520consistency%250Aof%2520LLM%2520outputs%2520when%2520incorporating%2520label%2520interference%2520into%2520the%2520sampling-based%250Aapproach.%2520Based%2520on%2520Unc-TTP%2520outputs%252C%2520we%2520aggregate%2520instances%2520into%2520certain%2520and%250Auncertain%2520categories.%2520Further%252C%2520we%2520conduct%2520a%2520detailed%2520analysis%2520of%2520the%250Auncertainty%2520properties%2520of%2520LLMs%2520and%2520show%2520Unc-TTP%2527s%2520superiority%2520over%2520the%2520existing%250Asampling-based%2520methods.%2520In%2520addition%252C%2520we%2520leverage%2520the%2520obtained%2520uncertainty%250Ainformation%2520to%2520guide%2520in-context%2520example%2520selection%252C%2520demonstrating%2520that%2520Unc-TTP%250Aobviously%2520outperforms%2520retrieval-based%2520and%2520sampling-based%2520approaches%2520in%250Aselecting%2520more%2520informative%2520examples.%2520Our%2520work%2520paves%2520a%2520new%2520way%2520to%2520classify%2520the%250Auncertainty%2520of%2520both%2520open-%2520and%2520closed-source%2520LLMs%252C%2520and%2520introduces%2520a%2520practical%250Aapproach%2520to%2520exploit%2520this%2520uncertainty%2520to%2520improve%2520LLMs%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09172v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unc-TTP%3A%20A%20Method%20for%20Classifying%20LLM%20Uncertainty%20to%20Improve%20In-Context%0A%20%20Example%20Selection&entry.906535625=Hsiu-Yuan%20Huang%20and%20Zichen%20Wu%20and%20Yutong%20Yang%20and%20Junzhao%20Zhang%20and%20Yunfang%20Wu&entry.1292438233=%20%20Nowadays%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20exceptional%0Aperformance%20across%20various%20downstream%20tasks.%20However%2C%20it%20is%20challenging%20for%0Ausers%20to%20discern%20whether%20the%20responses%20are%20generated%20with%20certainty%20or%20are%0Afabricated%20to%20meet%20user%20expectations.%20Estimating%20the%20uncertainty%20of%20LLMs%20is%0Aparticularly%20challenging%20due%20to%20their%20vast%20scale%20and%20the%20lack%20of%20white-box%0Aaccess.%20In%20this%20work%2C%20we%20propose%20a%20novel%20Uncertainty%20Tripartite%20Testing%0AParadigm%20%28Unc-TTP%29%20to%20classify%20LLM%20uncertainty%2C%20via%20evaluating%20the%20consistency%0Aof%20LLM%20outputs%20when%20incorporating%20label%20interference%20into%20the%20sampling-based%0Aapproach.%20Based%20on%20Unc-TTP%20outputs%2C%20we%20aggregate%20instances%20into%20certain%20and%0Auncertain%20categories.%20Further%2C%20we%20conduct%20a%20detailed%20analysis%20of%20the%0Auncertainty%20properties%20of%20LLMs%20and%20show%20Unc-TTP%27s%20superiority%20over%20the%20existing%0Asampling-based%20methods.%20In%20addition%2C%20we%20leverage%20the%20obtained%20uncertainty%0Ainformation%20to%20guide%20in-context%20example%20selection%2C%20demonstrating%20that%20Unc-TTP%0Aobviously%20outperforms%20retrieval-based%20and%20sampling-based%20approaches%20in%0Aselecting%20more%20informative%20examples.%20Our%20work%20paves%20a%20new%20way%20to%20classify%20the%0Auncertainty%20of%20both%20open-%20and%20closed-source%20LLMs%2C%20and%20introduces%20a%20practical%0Aapproach%20to%20exploit%20this%20uncertainty%20to%20improve%20LLMs%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09172v2&entry.124074799=Read"},
{"title": "Multichannel Attention Networks with Ensembled Transfer Learning to\n  Recognize Bangla Handwritten Charecter", "author": "Farhanul Haque and Md. Al-Hasan and Sumaiya Tabssum Mou and Abu Saleh Musa Miah and Jungpil Shin and Md Abdur Rahim", "abstract": "  The Bengali language is the 5th most spoken native and 7th most spoken\nlanguage in the world, and Bengali handwritten character recognition has\nattracted researchers for decades. However, other languages such as English,\nArabic, Turkey, and Chinese character recognition have contributed\nsignificantly to developing handwriting recognition systems. Still, little\nresearch has been done on Bengali character recognition because of the\nsimilarity of the character, curvature and other complexities. However, many\nresearchers have used traditional machine learning and deep learning models to\nconduct Bengali hand-written recognition. The study employed a convolutional\nneural network (CNN) with ensemble transfer learning and a multichannel\nattention network. We generated the feature from the two branches of the CNN,\nincluding Inception Net and ResNet and then produced an ensemble feature fusion\nby concatenating them. After that, we applied the attention module to produce\nthe contextual information from the ensemble features. Finally, we applied a\nclassification module to refine the features and classification. We evaluated\nthe proposed model using the CAMTERdb 3.1.2 data set and achieved 92\\% accuracy\nfor the raw dataset and 98.00\\% for the preprocessed dataset. We believe that\nour contribution to the Bengali handwritten character recognition domain will\nbe considered a great development.\n", "link": "http://arxiv.org/abs/2408.10955v1", "date": "2024-08-20", "relevancy": 2.2798, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4623}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4545}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multichannel%20Attention%20Networks%20with%20Ensembled%20Transfer%20Learning%20to%0A%20%20Recognize%20Bangla%20Handwritten%20Charecter&body=Title%3A%20Multichannel%20Attention%20Networks%20with%20Ensembled%20Transfer%20Learning%20to%0A%20%20Recognize%20Bangla%20Handwritten%20Charecter%0AAuthor%3A%20Farhanul%20Haque%20and%20Md.%20Al-Hasan%20and%20Sumaiya%20Tabssum%20Mou%20and%20Abu%20Saleh%20Musa%20Miah%20and%20Jungpil%20Shin%20and%20Md%20Abdur%20Rahim%0AAbstract%3A%20%20%20The%20Bengali%20language%20is%20the%205th%20most%20spoken%20native%20and%207th%20most%20spoken%0Alanguage%20in%20the%20world%2C%20and%20Bengali%20handwritten%20character%20recognition%20has%0Aattracted%20researchers%20for%20decades.%20However%2C%20other%20languages%20such%20as%20English%2C%0AArabic%2C%20Turkey%2C%20and%20Chinese%20character%20recognition%20have%20contributed%0Asignificantly%20to%20developing%20handwriting%20recognition%20systems.%20Still%2C%20little%0Aresearch%20has%20been%20done%20on%20Bengali%20character%20recognition%20because%20of%20the%0Asimilarity%20of%20the%20character%2C%20curvature%20and%20other%20complexities.%20However%2C%20many%0Aresearchers%20have%20used%20traditional%20machine%20learning%20and%20deep%20learning%20models%20to%0Aconduct%20Bengali%20hand-written%20recognition.%20The%20study%20employed%20a%20convolutional%0Aneural%20network%20%28CNN%29%20with%20ensemble%20transfer%20learning%20and%20a%20multichannel%0Aattention%20network.%20We%20generated%20the%20feature%20from%20the%20two%20branches%20of%20the%20CNN%2C%0Aincluding%20Inception%20Net%20and%20ResNet%20and%20then%20produced%20an%20ensemble%20feature%20fusion%0Aby%20concatenating%20them.%20After%20that%2C%20we%20applied%20the%20attention%20module%20to%20produce%0Athe%20contextual%20information%20from%20the%20ensemble%20features.%20Finally%2C%20we%20applied%20a%0Aclassification%20module%20to%20refine%20the%20features%20and%20classification.%20We%20evaluated%0Athe%20proposed%20model%20using%20the%20CAMTERdb%203.1.2%20data%20set%20and%20achieved%2092%5C%25%20accuracy%0Afor%20the%20raw%20dataset%20and%2098.00%5C%25%20for%20the%20preprocessed%20dataset.%20We%20believe%20that%0Aour%20contribution%20to%20the%20Bengali%20handwritten%20character%20recognition%20domain%20will%0Abe%20considered%20a%20great%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultichannel%2520Attention%2520Networks%2520with%2520Ensembled%2520Transfer%2520Learning%2520to%250A%2520%2520Recognize%2520Bangla%2520Handwritten%2520Charecter%26entry.906535625%3DFarhanul%2520Haque%2520and%2520Md.%2520Al-Hasan%2520and%2520Sumaiya%2520Tabssum%2520Mou%2520and%2520Abu%2520Saleh%2520Musa%2520Miah%2520and%2520Jungpil%2520Shin%2520and%2520Md%2520Abdur%2520Rahim%26entry.1292438233%3D%2520%2520The%2520Bengali%2520language%2520is%2520the%25205th%2520most%2520spoken%2520native%2520and%25207th%2520most%2520spoken%250Alanguage%2520in%2520the%2520world%252C%2520and%2520Bengali%2520handwritten%2520character%2520recognition%2520has%250Aattracted%2520researchers%2520for%2520decades.%2520However%252C%2520other%2520languages%2520such%2520as%2520English%252C%250AArabic%252C%2520Turkey%252C%2520and%2520Chinese%2520character%2520recognition%2520have%2520contributed%250Asignificantly%2520to%2520developing%2520handwriting%2520recognition%2520systems.%2520Still%252C%2520little%250Aresearch%2520has%2520been%2520done%2520on%2520Bengali%2520character%2520recognition%2520because%2520of%2520the%250Asimilarity%2520of%2520the%2520character%252C%2520curvature%2520and%2520other%2520complexities.%2520However%252C%2520many%250Aresearchers%2520have%2520used%2520traditional%2520machine%2520learning%2520and%2520deep%2520learning%2520models%2520to%250Aconduct%2520Bengali%2520hand-written%2520recognition.%2520The%2520study%2520employed%2520a%2520convolutional%250Aneural%2520network%2520%2528CNN%2529%2520with%2520ensemble%2520transfer%2520learning%2520and%2520a%2520multichannel%250Aattention%2520network.%2520We%2520generated%2520the%2520feature%2520from%2520the%2520two%2520branches%2520of%2520the%2520CNN%252C%250Aincluding%2520Inception%2520Net%2520and%2520ResNet%2520and%2520then%2520produced%2520an%2520ensemble%2520feature%2520fusion%250Aby%2520concatenating%2520them.%2520After%2520that%252C%2520we%2520applied%2520the%2520attention%2520module%2520to%2520produce%250Athe%2520contextual%2520information%2520from%2520the%2520ensemble%2520features.%2520Finally%252C%2520we%2520applied%2520a%250Aclassification%2520module%2520to%2520refine%2520the%2520features%2520and%2520classification.%2520We%2520evaluated%250Athe%2520proposed%2520model%2520using%2520the%2520CAMTERdb%25203.1.2%2520data%2520set%2520and%2520achieved%252092%255C%2525%2520accuracy%250Afor%2520the%2520raw%2520dataset%2520and%252098.00%255C%2525%2520for%2520the%2520preprocessed%2520dataset.%2520We%2520believe%2520that%250Aour%2520contribution%2520to%2520the%2520Bengali%2520handwritten%2520character%2520recognition%2520domain%2520will%250Abe%2520considered%2520a%2520great%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multichannel%20Attention%20Networks%20with%20Ensembled%20Transfer%20Learning%20to%0A%20%20Recognize%20Bangla%20Handwritten%20Charecter&entry.906535625=Farhanul%20Haque%20and%20Md.%20Al-Hasan%20and%20Sumaiya%20Tabssum%20Mou%20and%20Abu%20Saleh%20Musa%20Miah%20and%20Jungpil%20Shin%20and%20Md%20Abdur%20Rahim&entry.1292438233=%20%20The%20Bengali%20language%20is%20the%205th%20most%20spoken%20native%20and%207th%20most%20spoken%0Alanguage%20in%20the%20world%2C%20and%20Bengali%20handwritten%20character%20recognition%20has%0Aattracted%20researchers%20for%20decades.%20However%2C%20other%20languages%20such%20as%20English%2C%0AArabic%2C%20Turkey%2C%20and%20Chinese%20character%20recognition%20have%20contributed%0Asignificantly%20to%20developing%20handwriting%20recognition%20systems.%20Still%2C%20little%0Aresearch%20has%20been%20done%20on%20Bengali%20character%20recognition%20because%20of%20the%0Asimilarity%20of%20the%20character%2C%20curvature%20and%20other%20complexities.%20However%2C%20many%0Aresearchers%20have%20used%20traditional%20machine%20learning%20and%20deep%20learning%20models%20to%0Aconduct%20Bengali%20hand-written%20recognition.%20The%20study%20employed%20a%20convolutional%0Aneural%20network%20%28CNN%29%20with%20ensemble%20transfer%20learning%20and%20a%20multichannel%0Aattention%20network.%20We%20generated%20the%20feature%20from%20the%20two%20branches%20of%20the%20CNN%2C%0Aincluding%20Inception%20Net%20and%20ResNet%20and%20then%20produced%20an%20ensemble%20feature%20fusion%0Aby%20concatenating%20them.%20After%20that%2C%20we%20applied%20the%20attention%20module%20to%20produce%0Athe%20contextual%20information%20from%20the%20ensemble%20features.%20Finally%2C%20we%20applied%20a%0Aclassification%20module%20to%20refine%20the%20features%20and%20classification.%20We%20evaluated%0Athe%20proposed%20model%20using%20the%20CAMTERdb%203.1.2%20data%20set%20and%20achieved%2092%5C%25%20accuracy%0Afor%20the%20raw%20dataset%20and%2098.00%5C%25%20for%20the%20preprocessed%20dataset.%20We%20believe%20that%0Aour%20contribution%20to%20the%20Bengali%20handwritten%20character%20recognition%20domain%20will%0Abe%20considered%20a%20great%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10955v1&entry.124074799=Read"},
{"title": "Exploiting Defenses against GAN-Based Feature Inference Attacks in\n  Federated Learning", "author": "Xinjian Luo and Xianglong Zhang", "abstract": "  Federated learning (FL) is a decentralized model training framework that aims\nto merge isolated data islands while maintaining data privacy. However, recent\nstudies have revealed that Generative Adversarial Network (GAN) based attacks\ncan be employed in FL to learn the distribution of private datasets and\nreconstruct recognizable images. In this paper, we exploit defenses against\nGAN-based attacks in FL and propose a framework, Anti-GAN, to prevent attackers\nfrom learning the real distribution of the victim's data. The core idea of\nAnti-GAN is to manipulate the visual features of private training images to\nmake them indistinguishable to human eyes even restored by attackers.\nSpecifically, Anti-GAN projects the private dataset onto a GAN's generator and\ncombines the generated fake images with the actual images to create the\ntraining dataset, which is then used for federated model training. The\nexperimental results demonstrate that Anti-GAN is effective in preventing\nattackers from learning the distribution of private images while causing\nminimal harm to the accuracy of the federated model.\n", "link": "http://arxiv.org/abs/2004.12571v3", "date": "2024-08-20", "relevancy": 2.2617, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4584}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4501}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Defenses%20against%20GAN-Based%20Feature%20Inference%20Attacks%20in%0A%20%20Federated%20Learning&body=Title%3A%20Exploiting%20Defenses%20against%20GAN-Based%20Feature%20Inference%20Attacks%20in%0A%20%20Federated%20Learning%0AAuthor%3A%20Xinjian%20Luo%20and%20Xianglong%20Zhang%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20decentralized%20model%20training%20framework%20that%20aims%0Ato%20merge%20isolated%20data%20islands%20while%20maintaining%20data%20privacy.%20However%2C%20recent%0Astudies%20have%20revealed%20that%20Generative%20Adversarial%20Network%20%28GAN%29%20based%20attacks%0Acan%20be%20employed%20in%20FL%20to%20learn%20the%20distribution%20of%20private%20datasets%20and%0Areconstruct%20recognizable%20images.%20In%20this%20paper%2C%20we%20exploit%20defenses%20against%0AGAN-based%20attacks%20in%20FL%20and%20propose%20a%20framework%2C%20Anti-GAN%2C%20to%20prevent%20attackers%0Afrom%20learning%20the%20real%20distribution%20of%20the%20victim%27s%20data.%20The%20core%20idea%20of%0AAnti-GAN%20is%20to%20manipulate%20the%20visual%20features%20of%20private%20training%20images%20to%0Amake%20them%20indistinguishable%20to%20human%20eyes%20even%20restored%20by%20attackers.%0ASpecifically%2C%20Anti-GAN%20projects%20the%20private%20dataset%20onto%20a%20GAN%27s%20generator%20and%0Acombines%20the%20generated%20fake%20images%20with%20the%20actual%20images%20to%20create%20the%0Atraining%20dataset%2C%20which%20is%20then%20used%20for%20federated%20model%20training.%20The%0Aexperimental%20results%20demonstrate%20that%20Anti-GAN%20is%20effective%20in%20preventing%0Aattackers%20from%20learning%20the%20distribution%20of%20private%20images%20while%20causing%0Aminimal%20harm%20to%20the%20accuracy%20of%20the%20federated%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2004.12571v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Defenses%2520against%2520GAN-Based%2520Feature%2520Inference%2520Attacks%2520in%250A%2520%2520Federated%2520Learning%26entry.906535625%3DXinjian%2520Luo%2520and%2520Xianglong%2520Zhang%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520a%2520decentralized%2520model%2520training%2520framework%2520that%2520aims%250Ato%2520merge%2520isolated%2520data%2520islands%2520while%2520maintaining%2520data%2520privacy.%2520However%252C%2520recent%250Astudies%2520have%2520revealed%2520that%2520Generative%2520Adversarial%2520Network%2520%2528GAN%2529%2520based%2520attacks%250Acan%2520be%2520employed%2520in%2520FL%2520to%2520learn%2520the%2520distribution%2520of%2520private%2520datasets%2520and%250Areconstruct%2520recognizable%2520images.%2520In%2520this%2520paper%252C%2520we%2520exploit%2520defenses%2520against%250AGAN-based%2520attacks%2520in%2520FL%2520and%2520propose%2520a%2520framework%252C%2520Anti-GAN%252C%2520to%2520prevent%2520attackers%250Afrom%2520learning%2520the%2520real%2520distribution%2520of%2520the%2520victim%2527s%2520data.%2520The%2520core%2520idea%2520of%250AAnti-GAN%2520is%2520to%2520manipulate%2520the%2520visual%2520features%2520of%2520private%2520training%2520images%2520to%250Amake%2520them%2520indistinguishable%2520to%2520human%2520eyes%2520even%2520restored%2520by%2520attackers.%250ASpecifically%252C%2520Anti-GAN%2520projects%2520the%2520private%2520dataset%2520onto%2520a%2520GAN%2527s%2520generator%2520and%250Acombines%2520the%2520generated%2520fake%2520images%2520with%2520the%2520actual%2520images%2520to%2520create%2520the%250Atraining%2520dataset%252C%2520which%2520is%2520then%2520used%2520for%2520federated%2520model%2520training.%2520The%250Aexperimental%2520results%2520demonstrate%2520that%2520Anti-GAN%2520is%2520effective%2520in%2520preventing%250Aattackers%2520from%2520learning%2520the%2520distribution%2520of%2520private%2520images%2520while%2520causing%250Aminimal%2520harm%2520to%2520the%2520accuracy%2520of%2520the%2520federated%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2004.12571v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Defenses%20against%20GAN-Based%20Feature%20Inference%20Attacks%20in%0A%20%20Federated%20Learning&entry.906535625=Xinjian%20Luo%20and%20Xianglong%20Zhang&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20decentralized%20model%20training%20framework%20that%20aims%0Ato%20merge%20isolated%20data%20islands%20while%20maintaining%20data%20privacy.%20However%2C%20recent%0Astudies%20have%20revealed%20that%20Generative%20Adversarial%20Network%20%28GAN%29%20based%20attacks%0Acan%20be%20employed%20in%20FL%20to%20learn%20the%20distribution%20of%20private%20datasets%20and%0Areconstruct%20recognizable%20images.%20In%20this%20paper%2C%20we%20exploit%20defenses%20against%0AGAN-based%20attacks%20in%20FL%20and%20propose%20a%20framework%2C%20Anti-GAN%2C%20to%20prevent%20attackers%0Afrom%20learning%20the%20real%20distribution%20of%20the%20victim%27s%20data.%20The%20core%20idea%20of%0AAnti-GAN%20is%20to%20manipulate%20the%20visual%20features%20of%20private%20training%20images%20to%0Amake%20them%20indistinguishable%20to%20human%20eyes%20even%20restored%20by%20attackers.%0ASpecifically%2C%20Anti-GAN%20projects%20the%20private%20dataset%20onto%20a%20GAN%27s%20generator%20and%0Acombines%20the%20generated%20fake%20images%20with%20the%20actual%20images%20to%20create%20the%0Atraining%20dataset%2C%20which%20is%20then%20used%20for%20federated%20model%20training.%20The%0Aexperimental%20results%20demonstrate%20that%20Anti-GAN%20is%20effective%20in%20preventing%0Aattackers%20from%20learning%20the%20distribution%20of%20private%20images%20while%20causing%0Aminimal%20harm%20to%20the%20accuracy%20of%20the%20federated%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2004.12571v3&entry.124074799=Read"},
{"title": "MPL: Lifting 3D Human Pose from Multi-view 2D Poses", "author": "Seyed Abolfazl Ghasemzadeh and Alexandre Alahi and Christophe De Vleeschouwer", "abstract": "  Estimating 3D human poses from 2D images is challenging due to occlusions and\nprojective acquisition. Learning-based approaches have been largely studied to\naddress this challenge, both in single and multi-view setups. These solutions\nhowever fail to generalize to real-world cases due to the lack of (multi-view)\n'in-the-wild' images paired with 3D poses for training. For this reason, we\npropose combining 2D pose estimation, for which large and rich training\ndatasets exist, and 2D-to-3D pose lifting, using a transformer-based network\nthat can be trained from synthetic 2D-3D pose pairs. Our experiments\ndemonstrate decreases up to 45% in MPJPE errors compared to the 3D pose\nobtained by triangulating the 2D poses. The framework's source code is\navailable at https://github.com/aghasemzadeh/OpenMPL .\n", "link": "http://arxiv.org/abs/2408.10805v1", "date": "2024-08-20", "relevancy": 2.2572, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5802}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5605}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MPL%3A%20Lifting%203D%20Human%20Pose%20from%20Multi-view%202D%20Poses&body=Title%3A%20MPL%3A%20Lifting%203D%20Human%20Pose%20from%20Multi-view%202D%20Poses%0AAuthor%3A%20Seyed%20Abolfazl%20Ghasemzadeh%20and%20Alexandre%20Alahi%20and%20Christophe%20De%20Vleeschouwer%0AAbstract%3A%20%20%20Estimating%203D%20human%20poses%20from%202D%20images%20is%20challenging%20due%20to%20occlusions%20and%0Aprojective%20acquisition.%20Learning-based%20approaches%20have%20been%20largely%20studied%20to%0Aaddress%20this%20challenge%2C%20both%20in%20single%20and%20multi-view%20setups.%20These%20solutions%0Ahowever%20fail%20to%20generalize%20to%20real-world%20cases%20due%20to%20the%20lack%20of%20%28multi-view%29%0A%27in-the-wild%27%20images%20paired%20with%203D%20poses%20for%20training.%20For%20this%20reason%2C%20we%0Apropose%20combining%202D%20pose%20estimation%2C%20for%20which%20large%20and%20rich%20training%0Adatasets%20exist%2C%20and%202D-to-3D%20pose%20lifting%2C%20using%20a%20transformer-based%20network%0Athat%20can%20be%20trained%20from%20synthetic%202D-3D%20pose%20pairs.%20Our%20experiments%0Ademonstrate%20decreases%20up%20to%2045%25%20in%20MPJPE%20errors%20compared%20to%20the%203D%20pose%0Aobtained%20by%20triangulating%20the%202D%20poses.%20The%20framework%27s%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/aghasemzadeh/OpenMPL%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMPL%253A%2520Lifting%25203D%2520Human%2520Pose%2520from%2520Multi-view%25202D%2520Poses%26entry.906535625%3DSeyed%2520Abolfazl%2520Ghasemzadeh%2520and%2520Alexandre%2520Alahi%2520and%2520Christophe%2520De%2520Vleeschouwer%26entry.1292438233%3D%2520%2520Estimating%25203D%2520human%2520poses%2520from%25202D%2520images%2520is%2520challenging%2520due%2520to%2520occlusions%2520and%250Aprojective%2520acquisition.%2520Learning-based%2520approaches%2520have%2520been%2520largely%2520studied%2520to%250Aaddress%2520this%2520challenge%252C%2520both%2520in%2520single%2520and%2520multi-view%2520setups.%2520These%2520solutions%250Ahowever%2520fail%2520to%2520generalize%2520to%2520real-world%2520cases%2520due%2520to%2520the%2520lack%2520of%2520%2528multi-view%2529%250A%2527in-the-wild%2527%2520images%2520paired%2520with%25203D%2520poses%2520for%2520training.%2520For%2520this%2520reason%252C%2520we%250Apropose%2520combining%25202D%2520pose%2520estimation%252C%2520for%2520which%2520large%2520and%2520rich%2520training%250Adatasets%2520exist%252C%2520and%25202D-to-3D%2520pose%2520lifting%252C%2520using%2520a%2520transformer-based%2520network%250Athat%2520can%2520be%2520trained%2520from%2520synthetic%25202D-3D%2520pose%2520pairs.%2520Our%2520experiments%250Ademonstrate%2520decreases%2520up%2520to%252045%2525%2520in%2520MPJPE%2520errors%2520compared%2520to%2520the%25203D%2520pose%250Aobtained%2520by%2520triangulating%2520the%25202D%2520poses.%2520The%2520framework%2527s%2520source%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/aghasemzadeh/OpenMPL%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MPL%3A%20Lifting%203D%20Human%20Pose%20from%20Multi-view%202D%20Poses&entry.906535625=Seyed%20Abolfazl%20Ghasemzadeh%20and%20Alexandre%20Alahi%20and%20Christophe%20De%20Vleeschouwer&entry.1292438233=%20%20Estimating%203D%20human%20poses%20from%202D%20images%20is%20challenging%20due%20to%20occlusions%20and%0Aprojective%20acquisition.%20Learning-based%20approaches%20have%20been%20largely%20studied%20to%0Aaddress%20this%20challenge%2C%20both%20in%20single%20and%20multi-view%20setups.%20These%20solutions%0Ahowever%20fail%20to%20generalize%20to%20real-world%20cases%20due%20to%20the%20lack%20of%20%28multi-view%29%0A%27in-the-wild%27%20images%20paired%20with%203D%20poses%20for%20training.%20For%20this%20reason%2C%20we%0Apropose%20combining%202D%20pose%20estimation%2C%20for%20which%20large%20and%20rich%20training%0Adatasets%20exist%2C%20and%202D-to-3D%20pose%20lifting%2C%20using%20a%20transformer-based%20network%0Athat%20can%20be%20trained%20from%20synthetic%202D-3D%20pose%20pairs.%20Our%20experiments%0Ademonstrate%20decreases%20up%20to%2045%25%20in%20MPJPE%20errors%20compared%20to%20the%203D%20pose%0Aobtained%20by%20triangulating%20the%202D%20poses.%20The%20framework%27s%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/aghasemzadeh/OpenMPL%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10805v1&entry.124074799=Read"},
{"title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments", "author": "Yunzhe Xu and Yiyuan Pan and Zhe Liu and Hesheng Wang", "abstract": "  Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for trajectory summarization, and\nend-to-end training on VLN datasets. The augmented datasets are synthesized\nautomatically. Experimental results demonstrate FLAME's superiority over\nexisting methods, surpassing state-of-the-art methods by a 7.3% increase in\ntask completion rate on Touchdown dataset. This work showcases the potential of\nMultimodal LLMs (MLLMs) in complex navigation tasks, representing an\nadvancement towards practical applications of MLLMs in embodied AI. Project\npage: https://flame-sjtu.github.io\n", "link": "http://arxiv.org/abs/2408.11051v1", "date": "2024-08-20", "relevancy": 2.2536, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.574}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5673}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLAME%3A%20Learning%20to%20Navigate%20with%20Multimodal%20LLM%20in%20Urban%20Environments&body=Title%3A%20FLAME%3A%20Learning%20to%20Navigate%20with%20Multimodal%20LLM%20in%20Urban%20Environments%0AAuthor%3A%20Yunzhe%20Xu%20and%20Yiyuan%20Pan%20and%20Zhe%20Liu%20and%20Hesheng%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20potential%20in%0AVision-and-Language%20Navigation%20%28VLN%29%20tasks%2C%20yet%20current%20applications%20face%0Achallenges.%20While%20LLMs%20excel%20in%20general%20conversation%20scenarios%2C%20they%20struggle%0Awith%20specialized%20navigation%20tasks%2C%20yielding%20suboptimal%20performance%20compared%20to%0Aspecialized%20VLN%20models.%20We%20introduce%20FLAME%20%28FLAMingo-Architected%20Embodied%0AAgent%29%2C%20a%20novel%20Multimodal%20LLM-based%20agent%20and%20architecture%20designed%20for%20urban%0AVLN%20tasks%20that%20efficiently%20handles%20multiple%20observations.%20Our%20approach%0Aimplements%20a%20three-phase%20tuning%20technique%20for%20effective%20adaptation%20to%0Anavigation%20tasks%2C%20including%20single%20perception%20tuning%20for%20street%20view%0Adescription%2C%20multiple%20perception%20tuning%20for%20trajectory%20summarization%2C%20and%0Aend-to-end%20training%20on%20VLN%20datasets.%20The%20augmented%20datasets%20are%20synthesized%0Aautomatically.%20Experimental%20results%20demonstrate%20FLAME%27s%20superiority%20over%0Aexisting%20methods%2C%20surpassing%20state-of-the-art%20methods%20by%20a%207.3%25%20increase%20in%0Atask%20completion%20rate%20on%20Touchdown%20dataset.%20This%20work%20showcases%20the%20potential%20of%0AMultimodal%20LLMs%20%28MLLMs%29%20in%20complex%20navigation%20tasks%2C%20representing%20an%0Aadvancement%20towards%20practical%20applications%20of%20MLLMs%20in%20embodied%20AI.%20Project%0Apage%3A%20https%3A//flame-sjtu.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLAME%253A%2520Learning%2520to%2520Navigate%2520with%2520Multimodal%2520LLM%2520in%2520Urban%2520Environments%26entry.906535625%3DYunzhe%2520Xu%2520and%2520Yiyuan%2520Pan%2520and%2520Zhe%2520Liu%2520and%2520Hesheng%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520potential%2520in%250AVision-and-Language%2520Navigation%2520%2528VLN%2529%2520tasks%252C%2520yet%2520current%2520applications%2520face%250Achallenges.%2520While%2520LLMs%2520excel%2520in%2520general%2520conversation%2520scenarios%252C%2520they%2520struggle%250Awith%2520specialized%2520navigation%2520tasks%252C%2520yielding%2520suboptimal%2520performance%2520compared%2520to%250Aspecialized%2520VLN%2520models.%2520We%2520introduce%2520FLAME%2520%2528FLAMingo-Architected%2520Embodied%250AAgent%2529%252C%2520a%2520novel%2520Multimodal%2520LLM-based%2520agent%2520and%2520architecture%2520designed%2520for%2520urban%250AVLN%2520tasks%2520that%2520efficiently%2520handles%2520multiple%2520observations.%2520Our%2520approach%250Aimplements%2520a%2520three-phase%2520tuning%2520technique%2520for%2520effective%2520adaptation%2520to%250Anavigation%2520tasks%252C%2520including%2520single%2520perception%2520tuning%2520for%2520street%2520view%250Adescription%252C%2520multiple%2520perception%2520tuning%2520for%2520trajectory%2520summarization%252C%2520and%250Aend-to-end%2520training%2520on%2520VLN%2520datasets.%2520The%2520augmented%2520datasets%2520are%2520synthesized%250Aautomatically.%2520Experimental%2520results%2520demonstrate%2520FLAME%2527s%2520superiority%2520over%250Aexisting%2520methods%252C%2520surpassing%2520state-of-the-art%2520methods%2520by%2520a%25207.3%2525%2520increase%2520in%250Atask%2520completion%2520rate%2520on%2520Touchdown%2520dataset.%2520This%2520work%2520showcases%2520the%2520potential%2520of%250AMultimodal%2520LLMs%2520%2528MLLMs%2529%2520in%2520complex%2520navigation%2520tasks%252C%2520representing%2520an%250Aadvancement%2520towards%2520practical%2520applications%2520of%2520MLLMs%2520in%2520embodied%2520AI.%2520Project%250Apage%253A%2520https%253A//flame-sjtu.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLAME%3A%20Learning%20to%20Navigate%20with%20Multimodal%20LLM%20in%20Urban%20Environments&entry.906535625=Yunzhe%20Xu%20and%20Yiyuan%20Pan%20and%20Zhe%20Liu%20and%20Hesheng%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20potential%20in%0AVision-and-Language%20Navigation%20%28VLN%29%20tasks%2C%20yet%20current%20applications%20face%0Achallenges.%20While%20LLMs%20excel%20in%20general%20conversation%20scenarios%2C%20they%20struggle%0Awith%20specialized%20navigation%20tasks%2C%20yielding%20suboptimal%20performance%20compared%20to%0Aspecialized%20VLN%20models.%20We%20introduce%20FLAME%20%28FLAMingo-Architected%20Embodied%0AAgent%29%2C%20a%20novel%20Multimodal%20LLM-based%20agent%20and%20architecture%20designed%20for%20urban%0AVLN%20tasks%20that%20efficiently%20handles%20multiple%20observations.%20Our%20approach%0Aimplements%20a%20three-phase%20tuning%20technique%20for%20effective%20adaptation%20to%0Anavigation%20tasks%2C%20including%20single%20perception%20tuning%20for%20street%20view%0Adescription%2C%20multiple%20perception%20tuning%20for%20trajectory%20summarization%2C%20and%0Aend-to-end%20training%20on%20VLN%20datasets.%20The%20augmented%20datasets%20are%20synthesized%0Aautomatically.%20Experimental%20results%20demonstrate%20FLAME%27s%20superiority%20over%0Aexisting%20methods%2C%20surpassing%20state-of-the-art%20methods%20by%20a%207.3%25%20increase%20in%0Atask%20completion%20rate%20on%20Touchdown%20dataset.%20This%20work%20showcases%20the%20potential%20of%0AMultimodal%20LLMs%20%28MLLMs%29%20in%20complex%20navigation%20tasks%2C%20representing%20an%0Aadvancement%20towards%20practical%20applications%20of%20MLLMs%20in%20embodied%20AI.%20Project%0Apage%3A%20https%3A//flame-sjtu.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11051v1&entry.124074799=Read"},
{"title": "GAIM: Attacking Graph Neural Networks via Adversarial Influence\n  Maximization", "author": "Xiaodong Yang and Xiaoting Li and Huiyuan Chen and Yiwei Cai", "abstract": "  Recent studies show that well-devised perturbations on graph structures or\nnode features can mislead trained Graph Neural Network (GNN) models. However,\nthese methods often overlook practical assumptions, over-rely on heuristics, or\nseparate vital attack components. In response, we present GAIM, an integrated\nadversarial attack method conducted on a node feature basis while considering\nthe strict black-box setting. Specifically, we define an adversarial influence\nfunction to theoretically assess the adversarial impact of node perturbations,\nthereby reframing the GNN attack problem into the adversarial influence\nmaximization problem. In our approach, we unify the selection of the target\nnode and the construction of feature perturbations into a single optimization\nproblem, ensuring a unique and consistent feature perturbation for each target\nnode. We leverage a surrogate model to transform this problem into a solvable\nlinear programming task, streamlining the optimization process. Moreover, we\nextend our method to accommodate label-oriented attacks, broadening its\napplicability. Thorough evaluations on five benchmark datasets across three\npopular models underscore the effectiveness of our method in both untargeted\nand label-oriented targeted attacks. Through comprehensive analysis and\nablation studies, we demonstrate the practical value and efficacy inherent to\nour design choices.\n", "link": "http://arxiv.org/abs/2408.10948v1", "date": "2024-08-20", "relevancy": 2.2349, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4556}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4443}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAIM%3A%20Attacking%20Graph%20Neural%20Networks%20via%20Adversarial%20Influence%0A%20%20Maximization&body=Title%3A%20GAIM%3A%20Attacking%20Graph%20Neural%20Networks%20via%20Adversarial%20Influence%0A%20%20Maximization%0AAuthor%3A%20Xiaodong%20Yang%20and%20Xiaoting%20Li%20and%20Huiyuan%20Chen%20and%20Yiwei%20Cai%0AAbstract%3A%20%20%20Recent%20studies%20show%20that%20well-devised%20perturbations%20on%20graph%20structures%20or%0Anode%20features%20can%20mislead%20trained%20Graph%20Neural%20Network%20%28GNN%29%20models.%20However%2C%0Athese%20methods%20often%20overlook%20practical%20assumptions%2C%20over-rely%20on%20heuristics%2C%20or%0Aseparate%20vital%20attack%20components.%20In%20response%2C%20we%20present%20GAIM%2C%20an%20integrated%0Aadversarial%20attack%20method%20conducted%20on%20a%20node%20feature%20basis%20while%20considering%0Athe%20strict%20black-box%20setting.%20Specifically%2C%20we%20define%20an%20adversarial%20influence%0Afunction%20to%20theoretically%20assess%20the%20adversarial%20impact%20of%20node%20perturbations%2C%0Athereby%20reframing%20the%20GNN%20attack%20problem%20into%20the%20adversarial%20influence%0Amaximization%20problem.%20In%20our%20approach%2C%20we%20unify%20the%20selection%20of%20the%20target%0Anode%20and%20the%20construction%20of%20feature%20perturbations%20into%20a%20single%20optimization%0Aproblem%2C%20ensuring%20a%20unique%20and%20consistent%20feature%20perturbation%20for%20each%20target%0Anode.%20We%20leverage%20a%20surrogate%20model%20to%20transform%20this%20problem%20into%20a%20solvable%0Alinear%20programming%20task%2C%20streamlining%20the%20optimization%20process.%20Moreover%2C%20we%0Aextend%20our%20method%20to%20accommodate%20label-oriented%20attacks%2C%20broadening%20its%0Aapplicability.%20Thorough%20evaluations%20on%20five%20benchmark%20datasets%20across%20three%0Apopular%20models%20underscore%20the%20effectiveness%20of%20our%20method%20in%20both%20untargeted%0Aand%20label-oriented%20targeted%20attacks.%20Through%20comprehensive%20analysis%20and%0Aablation%20studies%2C%20we%20demonstrate%20the%20practical%20value%20and%20efficacy%20inherent%20to%0Aour%20design%20choices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAIM%253A%2520Attacking%2520Graph%2520Neural%2520Networks%2520via%2520Adversarial%2520Influence%250A%2520%2520Maximization%26entry.906535625%3DXiaodong%2520Yang%2520and%2520Xiaoting%2520Li%2520and%2520Huiyuan%2520Chen%2520and%2520Yiwei%2520Cai%26entry.1292438233%3D%2520%2520Recent%2520studies%2520show%2520that%2520well-devised%2520perturbations%2520on%2520graph%2520structures%2520or%250Anode%2520features%2520can%2520mislead%2520trained%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520models.%2520However%252C%250Athese%2520methods%2520often%2520overlook%2520practical%2520assumptions%252C%2520over-rely%2520on%2520heuristics%252C%2520or%250Aseparate%2520vital%2520attack%2520components.%2520In%2520response%252C%2520we%2520present%2520GAIM%252C%2520an%2520integrated%250Aadversarial%2520attack%2520method%2520conducted%2520on%2520a%2520node%2520feature%2520basis%2520while%2520considering%250Athe%2520strict%2520black-box%2520setting.%2520Specifically%252C%2520we%2520define%2520an%2520adversarial%2520influence%250Afunction%2520to%2520theoretically%2520assess%2520the%2520adversarial%2520impact%2520of%2520node%2520perturbations%252C%250Athereby%2520reframing%2520the%2520GNN%2520attack%2520problem%2520into%2520the%2520adversarial%2520influence%250Amaximization%2520problem.%2520In%2520our%2520approach%252C%2520we%2520unify%2520the%2520selection%2520of%2520the%2520target%250Anode%2520and%2520the%2520construction%2520of%2520feature%2520perturbations%2520into%2520a%2520single%2520optimization%250Aproblem%252C%2520ensuring%2520a%2520unique%2520and%2520consistent%2520feature%2520perturbation%2520for%2520each%2520target%250Anode.%2520We%2520leverage%2520a%2520surrogate%2520model%2520to%2520transform%2520this%2520problem%2520into%2520a%2520solvable%250Alinear%2520programming%2520task%252C%2520streamlining%2520the%2520optimization%2520process.%2520Moreover%252C%2520we%250Aextend%2520our%2520method%2520to%2520accommodate%2520label-oriented%2520attacks%252C%2520broadening%2520its%250Aapplicability.%2520Thorough%2520evaluations%2520on%2520five%2520benchmark%2520datasets%2520across%2520three%250Apopular%2520models%2520underscore%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520both%2520untargeted%250Aand%2520label-oriented%2520targeted%2520attacks.%2520Through%2520comprehensive%2520analysis%2520and%250Aablation%2520studies%252C%2520we%2520demonstrate%2520the%2520practical%2520value%2520and%2520efficacy%2520inherent%2520to%250Aour%2520design%2520choices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAIM%3A%20Attacking%20Graph%20Neural%20Networks%20via%20Adversarial%20Influence%0A%20%20Maximization&entry.906535625=Xiaodong%20Yang%20and%20Xiaoting%20Li%20and%20Huiyuan%20Chen%20and%20Yiwei%20Cai&entry.1292438233=%20%20Recent%20studies%20show%20that%20well-devised%20perturbations%20on%20graph%20structures%20or%0Anode%20features%20can%20mislead%20trained%20Graph%20Neural%20Network%20%28GNN%29%20models.%20However%2C%0Athese%20methods%20often%20overlook%20practical%20assumptions%2C%20over-rely%20on%20heuristics%2C%20or%0Aseparate%20vital%20attack%20components.%20In%20response%2C%20we%20present%20GAIM%2C%20an%20integrated%0Aadversarial%20attack%20method%20conducted%20on%20a%20node%20feature%20basis%20while%20considering%0Athe%20strict%20black-box%20setting.%20Specifically%2C%20we%20define%20an%20adversarial%20influence%0Afunction%20to%20theoretically%20assess%20the%20adversarial%20impact%20of%20node%20perturbations%2C%0Athereby%20reframing%20the%20GNN%20attack%20problem%20into%20the%20adversarial%20influence%0Amaximization%20problem.%20In%20our%20approach%2C%20we%20unify%20the%20selection%20of%20the%20target%0Anode%20and%20the%20construction%20of%20feature%20perturbations%20into%20a%20single%20optimization%0Aproblem%2C%20ensuring%20a%20unique%20and%20consistent%20feature%20perturbation%20for%20each%20target%0Anode.%20We%20leverage%20a%20surrogate%20model%20to%20transform%20this%20problem%20into%20a%20solvable%0Alinear%20programming%20task%2C%20streamlining%20the%20optimization%20process.%20Moreover%2C%20we%0Aextend%20our%20method%20to%20accommodate%20label-oriented%20attacks%2C%20broadening%20its%0Aapplicability.%20Thorough%20evaluations%20on%20five%20benchmark%20datasets%20across%20three%0Apopular%20models%20underscore%20the%20effectiveness%20of%20our%20method%20in%20both%20untargeted%0Aand%20label-oriented%20targeted%20attacks.%20Through%20comprehensive%20analysis%20and%0Aablation%20studies%2C%20we%20demonstrate%20the%20practical%20value%20and%20efficacy%20inherent%20to%0Aour%20design%20choices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10948v1&entry.124074799=Read"},
{"title": "Tapping in a Remote Vehicle's onboard LLM to Complement the Ego\n  Vehicle's Field-of-View", "author": "Malsha Ashani Mahawatta Dona and Beatriz Cabrero-Daniel and Yinan Yu and Christian Berger", "abstract": "  Today's advanced automotive systems are turning into intelligent\nCyber-Physical Systems (CPS), bringing computational intelligence to their\ncyber-physical context. Such systems power advanced driver assistance systems\n(ADAS) that observe a vehicle's surroundings for their functionality. However,\nsuch ADAS have clear limitations in scenarios when the direct line-of-sight to\nsurrounding objects is occluded, like in urban areas. Imagine now automated\ndriving (AD) systems that ideally could benefit from other vehicles'\nfield-of-view in such occluded situations to increase traffic safety if, for\nexample, locations about pedestrians can be shared across vehicles. Current\nliterature suggests vehicle-to-infrastructure (V2I) via roadside units (RSUs)\nor vehicle-to-vehicle (V2V) communication to address such issues that stream\nsensor or object data between vehicles. When considering the ongoing revolution\nin vehicle system architectures towards powerful, centralized processing units\nwith hardware accelerators, foreseeing the onboard presence of large language\nmodels (LLMs) to improve the passengers' comfort when using voice assistants\nbecomes a reality. We are suggesting and evaluating a concept to complement the\nego vehicle's field-of-view (FOV) with another vehicle's FOV by tapping into\ntheir onboard LLM to let the machines have a dialogue about what the other\nvehicle ``sees''. Our results show that very recent versions of LLMs, such as\nGPT-4V and GPT-4o, understand a traffic situation to an impressive level of\ndetail, and hence, they can be used even to spot traffic participants. However,\nbetter prompts are needed to improve the detection quality and future work is\nneeded towards a standardised message interchange format between vehicles.\n", "link": "http://arxiv.org/abs/2408.10794v1", "date": "2024-08-20", "relevancy": 2.2088, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5619}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5523}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tapping%20in%20a%20Remote%20Vehicle%27s%20onboard%20LLM%20to%20Complement%20the%20Ego%0A%20%20Vehicle%27s%20Field-of-View&body=Title%3A%20Tapping%20in%20a%20Remote%20Vehicle%27s%20onboard%20LLM%20to%20Complement%20the%20Ego%0A%20%20Vehicle%27s%20Field-of-View%0AAuthor%3A%20Malsha%20Ashani%20Mahawatta%20Dona%20and%20Beatriz%20Cabrero-Daniel%20and%20Yinan%20Yu%20and%20Christian%20Berger%0AAbstract%3A%20%20%20Today%27s%20advanced%20automotive%20systems%20are%20turning%20into%20intelligent%0ACyber-Physical%20Systems%20%28CPS%29%2C%20bringing%20computational%20intelligence%20to%20their%0Acyber-physical%20context.%20Such%20systems%20power%20advanced%20driver%20assistance%20systems%0A%28ADAS%29%20that%20observe%20a%20vehicle%27s%20surroundings%20for%20their%20functionality.%20However%2C%0Asuch%20ADAS%20have%20clear%20limitations%20in%20scenarios%20when%20the%20direct%20line-of-sight%20to%0Asurrounding%20objects%20is%20occluded%2C%20like%20in%20urban%20areas.%20Imagine%20now%20automated%0Adriving%20%28AD%29%20systems%20that%20ideally%20could%20benefit%20from%20other%20vehicles%27%0Afield-of-view%20in%20such%20occluded%20situations%20to%20increase%20traffic%20safety%20if%2C%20for%0Aexample%2C%20locations%20about%20pedestrians%20can%20be%20shared%20across%20vehicles.%20Current%0Aliterature%20suggests%20vehicle-to-infrastructure%20%28V2I%29%20via%20roadside%20units%20%28RSUs%29%0Aor%20vehicle-to-vehicle%20%28V2V%29%20communication%20to%20address%20such%20issues%20that%20stream%0Asensor%20or%20object%20data%20between%20vehicles.%20When%20considering%20the%20ongoing%20revolution%0Ain%20vehicle%20system%20architectures%20towards%20powerful%2C%20centralized%20processing%20units%0Awith%20hardware%20accelerators%2C%20foreseeing%20the%20onboard%20presence%20of%20large%20language%0Amodels%20%28LLMs%29%20to%20improve%20the%20passengers%27%20comfort%20when%20using%20voice%20assistants%0Abecomes%20a%20reality.%20We%20are%20suggesting%20and%20evaluating%20a%20concept%20to%20complement%20the%0Aego%20vehicle%27s%20field-of-view%20%28FOV%29%20with%20another%20vehicle%27s%20FOV%20by%20tapping%20into%0Atheir%20onboard%20LLM%20to%20let%20the%20machines%20have%20a%20dialogue%20about%20what%20the%20other%0Avehicle%20%60%60sees%27%27.%20Our%20results%20show%20that%20very%20recent%20versions%20of%20LLMs%2C%20such%20as%0AGPT-4V%20and%20GPT-4o%2C%20understand%20a%20traffic%20situation%20to%20an%20impressive%20level%20of%0Adetail%2C%20and%20hence%2C%20they%20can%20be%20used%20even%20to%20spot%20traffic%20participants.%20However%2C%0Abetter%20prompts%20are%20needed%20to%20improve%20the%20detection%20quality%20and%20future%20work%20is%0Aneeded%20towards%20a%20standardised%20message%20interchange%20format%20between%20vehicles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTapping%2520in%2520a%2520Remote%2520Vehicle%2527s%2520onboard%2520LLM%2520to%2520Complement%2520the%2520Ego%250A%2520%2520Vehicle%2527s%2520Field-of-View%26entry.906535625%3DMalsha%2520Ashani%2520Mahawatta%2520Dona%2520and%2520Beatriz%2520Cabrero-Daniel%2520and%2520Yinan%2520Yu%2520and%2520Christian%2520Berger%26entry.1292438233%3D%2520%2520Today%2527s%2520advanced%2520automotive%2520systems%2520are%2520turning%2520into%2520intelligent%250ACyber-Physical%2520Systems%2520%2528CPS%2529%252C%2520bringing%2520computational%2520intelligence%2520to%2520their%250Acyber-physical%2520context.%2520Such%2520systems%2520power%2520advanced%2520driver%2520assistance%2520systems%250A%2528ADAS%2529%2520that%2520observe%2520a%2520vehicle%2527s%2520surroundings%2520for%2520their%2520functionality.%2520However%252C%250Asuch%2520ADAS%2520have%2520clear%2520limitations%2520in%2520scenarios%2520when%2520the%2520direct%2520line-of-sight%2520to%250Asurrounding%2520objects%2520is%2520occluded%252C%2520like%2520in%2520urban%2520areas.%2520Imagine%2520now%2520automated%250Adriving%2520%2528AD%2529%2520systems%2520that%2520ideally%2520could%2520benefit%2520from%2520other%2520vehicles%2527%250Afield-of-view%2520in%2520such%2520occluded%2520situations%2520to%2520increase%2520traffic%2520safety%2520if%252C%2520for%250Aexample%252C%2520locations%2520about%2520pedestrians%2520can%2520be%2520shared%2520across%2520vehicles.%2520Current%250Aliterature%2520suggests%2520vehicle-to-infrastructure%2520%2528V2I%2529%2520via%2520roadside%2520units%2520%2528RSUs%2529%250Aor%2520vehicle-to-vehicle%2520%2528V2V%2529%2520communication%2520to%2520address%2520such%2520issues%2520that%2520stream%250Asensor%2520or%2520object%2520data%2520between%2520vehicles.%2520When%2520considering%2520the%2520ongoing%2520revolution%250Ain%2520vehicle%2520system%2520architectures%2520towards%2520powerful%252C%2520centralized%2520processing%2520units%250Awith%2520hardware%2520accelerators%252C%2520foreseeing%2520the%2520onboard%2520presence%2520of%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520to%2520improve%2520the%2520passengers%2527%2520comfort%2520when%2520using%2520voice%2520assistants%250Abecomes%2520a%2520reality.%2520We%2520are%2520suggesting%2520and%2520evaluating%2520a%2520concept%2520to%2520complement%2520the%250Aego%2520vehicle%2527s%2520field-of-view%2520%2528FOV%2529%2520with%2520another%2520vehicle%2527s%2520FOV%2520by%2520tapping%2520into%250Atheir%2520onboard%2520LLM%2520to%2520let%2520the%2520machines%2520have%2520a%2520dialogue%2520about%2520what%2520the%2520other%250Avehicle%2520%2560%2560sees%2527%2527.%2520Our%2520results%2520show%2520that%2520very%2520recent%2520versions%2520of%2520LLMs%252C%2520such%2520as%250AGPT-4V%2520and%2520GPT-4o%252C%2520understand%2520a%2520traffic%2520situation%2520to%2520an%2520impressive%2520level%2520of%250Adetail%252C%2520and%2520hence%252C%2520they%2520can%2520be%2520used%2520even%2520to%2520spot%2520traffic%2520participants.%2520However%252C%250Abetter%2520prompts%2520are%2520needed%2520to%2520improve%2520the%2520detection%2520quality%2520and%2520future%2520work%2520is%250Aneeded%2520towards%2520a%2520standardised%2520message%2520interchange%2520format%2520between%2520vehicles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tapping%20in%20a%20Remote%20Vehicle%27s%20onboard%20LLM%20to%20Complement%20the%20Ego%0A%20%20Vehicle%27s%20Field-of-View&entry.906535625=Malsha%20Ashani%20Mahawatta%20Dona%20and%20Beatriz%20Cabrero-Daniel%20and%20Yinan%20Yu%20and%20Christian%20Berger&entry.1292438233=%20%20Today%27s%20advanced%20automotive%20systems%20are%20turning%20into%20intelligent%0ACyber-Physical%20Systems%20%28CPS%29%2C%20bringing%20computational%20intelligence%20to%20their%0Acyber-physical%20context.%20Such%20systems%20power%20advanced%20driver%20assistance%20systems%0A%28ADAS%29%20that%20observe%20a%20vehicle%27s%20surroundings%20for%20their%20functionality.%20However%2C%0Asuch%20ADAS%20have%20clear%20limitations%20in%20scenarios%20when%20the%20direct%20line-of-sight%20to%0Asurrounding%20objects%20is%20occluded%2C%20like%20in%20urban%20areas.%20Imagine%20now%20automated%0Adriving%20%28AD%29%20systems%20that%20ideally%20could%20benefit%20from%20other%20vehicles%27%0Afield-of-view%20in%20such%20occluded%20situations%20to%20increase%20traffic%20safety%20if%2C%20for%0Aexample%2C%20locations%20about%20pedestrians%20can%20be%20shared%20across%20vehicles.%20Current%0Aliterature%20suggests%20vehicle-to-infrastructure%20%28V2I%29%20via%20roadside%20units%20%28RSUs%29%0Aor%20vehicle-to-vehicle%20%28V2V%29%20communication%20to%20address%20such%20issues%20that%20stream%0Asensor%20or%20object%20data%20between%20vehicles.%20When%20considering%20the%20ongoing%20revolution%0Ain%20vehicle%20system%20architectures%20towards%20powerful%2C%20centralized%20processing%20units%0Awith%20hardware%20accelerators%2C%20foreseeing%20the%20onboard%20presence%20of%20large%20language%0Amodels%20%28LLMs%29%20to%20improve%20the%20passengers%27%20comfort%20when%20using%20voice%20assistants%0Abecomes%20a%20reality.%20We%20are%20suggesting%20and%20evaluating%20a%20concept%20to%20complement%20the%0Aego%20vehicle%27s%20field-of-view%20%28FOV%29%20with%20another%20vehicle%27s%20FOV%20by%20tapping%20into%0Atheir%20onboard%20LLM%20to%20let%20the%20machines%20have%20a%20dialogue%20about%20what%20the%20other%0Avehicle%20%60%60sees%27%27.%20Our%20results%20show%20that%20very%20recent%20versions%20of%20LLMs%2C%20such%20as%0AGPT-4V%20and%20GPT-4o%2C%20understand%20a%20traffic%20situation%20to%20an%20impressive%20level%20of%0Adetail%2C%20and%20hence%2C%20they%20can%20be%20used%20even%20to%20spot%20traffic%20participants.%20However%2C%0Abetter%20prompts%20are%20needed%20to%20improve%20the%20detection%20quality%20and%20future%20work%20is%0Aneeded%20towards%20a%20standardised%20message%20interchange%20format%20between%20vehicles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10794v1&entry.124074799=Read"},
{"title": "Language-Guided Self-Supervised Video Summarization Using Text Semantic\n  Matching Considering the Diversity of the Video", "author": "Tomoya Sugihara and Shuntaro Masuda and Ling Xiao and Toshihiko Yamasaki", "abstract": "  Current video summarization methods rely heavily on supervised computer\nvision techniques, which demands time-consuming and subjective manual\nannotations. To overcome these limitations, we investigated self-supervised\nvideo summarization. Inspired by the success of Large Language Models (LLMs),\nwe explored the feasibility in transforming the video summarization task into a\nNatural Language Processing (NLP) task. By leveraging the advantages of LLMs in\ncontext understanding, we aim to enhance the effectiveness of self-supervised\nvideo summarization. Our method begins by generating captions for individual\nvideo frames, which are then synthesized into text summaries by LLMs.\nSubsequently, we measure semantic distance between the captions and the text\nsummary. Notably, we propose a novel loss function to optimize our model\naccording to the diversity of the video. Finally, the summarized video can be\ngenerated by selecting the frames with captions similar to the text summary.\nOur method achieves state-of-the-art performance on the SumMe dataset in rank\ncorrelation coefficients. In addition, our method has a novel feature of being\nable to achieve personalized summarization.\n", "link": "http://arxiv.org/abs/2405.08890v2", "date": "2024-08-20", "relevancy": 2.2006, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5645}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5532}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Guided%20Self-Supervised%20Video%20Summarization%20Using%20Text%20Semantic%0A%20%20Matching%20Considering%20the%20Diversity%20of%20the%20Video&body=Title%3A%20Language-Guided%20Self-Supervised%20Video%20Summarization%20Using%20Text%20Semantic%0A%20%20Matching%20Considering%20the%20Diversity%20of%20the%20Video%0AAuthor%3A%20Tomoya%20Sugihara%20and%20Shuntaro%20Masuda%20and%20Ling%20Xiao%20and%20Toshihiko%20Yamasaki%0AAbstract%3A%20%20%20Current%20video%20summarization%20methods%20rely%20heavily%20on%20supervised%20computer%0Avision%20techniques%2C%20which%20demands%20time-consuming%20and%20subjective%20manual%0Aannotations.%20To%20overcome%20these%20limitations%2C%20we%20investigated%20self-supervised%0Avideo%20summarization.%20Inspired%20by%20the%20success%20of%20Large%20Language%20Models%20%28LLMs%29%2C%0Awe%20explored%20the%20feasibility%20in%20transforming%20the%20video%20summarization%20task%20into%20a%0ANatural%20Language%20Processing%20%28NLP%29%20task.%20By%20leveraging%20the%20advantages%20of%20LLMs%20in%0Acontext%20understanding%2C%20we%20aim%20to%20enhance%20the%20effectiveness%20of%20self-supervised%0Avideo%20summarization.%20Our%20method%20begins%20by%20generating%20captions%20for%20individual%0Avideo%20frames%2C%20which%20are%20then%20synthesized%20into%20text%20summaries%20by%20LLMs.%0ASubsequently%2C%20we%20measure%20semantic%20distance%20between%20the%20captions%20and%20the%20text%0Asummary.%20Notably%2C%20we%20propose%20a%20novel%20loss%20function%20to%20optimize%20our%20model%0Aaccording%20to%20the%20diversity%20of%20the%20video.%20Finally%2C%20the%20summarized%20video%20can%20be%0Agenerated%20by%20selecting%20the%20frames%20with%20captions%20similar%20to%20the%20text%20summary.%0AOur%20method%20achieves%20state-of-the-art%20performance%20on%20the%20SumMe%20dataset%20in%20rank%0Acorrelation%20coefficients.%20In%20addition%2C%20our%20method%20has%20a%20novel%20feature%20of%20being%0Aable%20to%20achieve%20personalized%20summarization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08890v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Guided%2520Self-Supervised%2520Video%2520Summarization%2520Using%2520Text%2520Semantic%250A%2520%2520Matching%2520Considering%2520the%2520Diversity%2520of%2520the%2520Video%26entry.906535625%3DTomoya%2520Sugihara%2520and%2520Shuntaro%2520Masuda%2520and%2520Ling%2520Xiao%2520and%2520Toshihiko%2520Yamasaki%26entry.1292438233%3D%2520%2520Current%2520video%2520summarization%2520methods%2520rely%2520heavily%2520on%2520supervised%2520computer%250Avision%2520techniques%252C%2520which%2520demands%2520time-consuming%2520and%2520subjective%2520manual%250Aannotations.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520investigated%2520self-supervised%250Avideo%2520summarization.%2520Inspired%2520by%2520the%2520success%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Awe%2520explored%2520the%2520feasibility%2520in%2520transforming%2520the%2520video%2520summarization%2520task%2520into%2520a%250ANatural%2520Language%2520Processing%2520%2528NLP%2529%2520task.%2520By%2520leveraging%2520the%2520advantages%2520of%2520LLMs%2520in%250Acontext%2520understanding%252C%2520we%2520aim%2520to%2520enhance%2520the%2520effectiveness%2520of%2520self-supervised%250Avideo%2520summarization.%2520Our%2520method%2520begins%2520by%2520generating%2520captions%2520for%2520individual%250Avideo%2520frames%252C%2520which%2520are%2520then%2520synthesized%2520into%2520text%2520summaries%2520by%2520LLMs.%250ASubsequently%252C%2520we%2520measure%2520semantic%2520distance%2520between%2520the%2520captions%2520and%2520the%2520text%250Asummary.%2520Notably%252C%2520we%2520propose%2520a%2520novel%2520loss%2520function%2520to%2520optimize%2520our%2520model%250Aaccording%2520to%2520the%2520diversity%2520of%2520the%2520video.%2520Finally%252C%2520the%2520summarized%2520video%2520can%2520be%250Agenerated%2520by%2520selecting%2520the%2520frames%2520with%2520captions%2520similar%2520to%2520the%2520text%2520summary.%250AOur%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520SumMe%2520dataset%2520in%2520rank%250Acorrelation%2520coefficients.%2520In%2520addition%252C%2520our%2520method%2520has%2520a%2520novel%2520feature%2520of%2520being%250Aable%2520to%2520achieve%2520personalized%2520summarization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08890v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Guided%20Self-Supervised%20Video%20Summarization%20Using%20Text%20Semantic%0A%20%20Matching%20Considering%20the%20Diversity%20of%20the%20Video&entry.906535625=Tomoya%20Sugihara%20and%20Shuntaro%20Masuda%20and%20Ling%20Xiao%20and%20Toshihiko%20Yamasaki&entry.1292438233=%20%20Current%20video%20summarization%20methods%20rely%20heavily%20on%20supervised%20computer%0Avision%20techniques%2C%20which%20demands%20time-consuming%20and%20subjective%20manual%0Aannotations.%20To%20overcome%20these%20limitations%2C%20we%20investigated%20self-supervised%0Avideo%20summarization.%20Inspired%20by%20the%20success%20of%20Large%20Language%20Models%20%28LLMs%29%2C%0Awe%20explored%20the%20feasibility%20in%20transforming%20the%20video%20summarization%20task%20into%20a%0ANatural%20Language%20Processing%20%28NLP%29%20task.%20By%20leveraging%20the%20advantages%20of%20LLMs%20in%0Acontext%20understanding%2C%20we%20aim%20to%20enhance%20the%20effectiveness%20of%20self-supervised%0Avideo%20summarization.%20Our%20method%20begins%20by%20generating%20captions%20for%20individual%0Avideo%20frames%2C%20which%20are%20then%20synthesized%20into%20text%20summaries%20by%20LLMs.%0ASubsequently%2C%20we%20measure%20semantic%20distance%20between%20the%20captions%20and%20the%20text%0Asummary.%20Notably%2C%20we%20propose%20a%20novel%20loss%20function%20to%20optimize%20our%20model%0Aaccording%20to%20the%20diversity%20of%20the%20video.%20Finally%2C%20the%20summarized%20video%20can%20be%0Agenerated%20by%20selecting%20the%20frames%20with%20captions%20similar%20to%20the%20text%20summary.%0AOur%20method%20achieves%20state-of-the-art%20performance%20on%20the%20SumMe%20dataset%20in%20rank%0Acorrelation%20coefficients.%20In%20addition%2C%20our%20method%20has%20a%20novel%20feature%20of%20being%0Aable%20to%20achieve%20personalized%20summarization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08890v2&entry.124074799=Read"},
{"title": "ViLReF: A Chinese Vision-Language Retinal Foundation Model", "author": "Shengzhu Yang and Jiawei Du and Jia Guo and Weihang Zhang and Hanruo Liu and Huiqi Li and Ningli Wang", "abstract": "  Subtle semantic differences in retinal image and text data present great\nchallenges for pre-training visual-language models. Moreover, false negative\nsamples, i.e., image-text pairs having the same semantics but incorrectly\nregarded as negatives, disrupt the visual-language pre-training process and\naffect the model's learning ability. This work aims to develop a retinal\nfoundation model, called ViLReF, by pre-training on a paired dataset comprising\n451,956 retinal images and corresponding diagnostic text reports. In our\nvision-language pre-training strategy, we leverage expert knowledge to\nfacilitate the extraction of labels and propose a novel constraint, the\nWeighted Similarity Coupling Loss, to adjust the speed of pushing sample pairs\nfurther apart dynamically within the feature space. Furthermore, we employ a\nbatch expansion module with dynamic memory queues, maintained by momentum\nencoders, to supply extra samples and compensate for the vacancies caused by\neliminating false negatives. Extensive experiments are conducted on multiple\ndatasets for downstream classification and segmentation tasks. The experimental\nresults demonstrate the powerful zero-shot and transfer learning capabilities\nof ViLReF, verifying the effectiveness of our pre-training strategy. Our ViLReF\nmodel is available at: https://github.com/T6Yang/ViLReF.\n", "link": "http://arxiv.org/abs/2408.10894v1", "date": "2024-08-20", "relevancy": 2.1971, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5604}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5593}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViLReF%3A%20A%20Chinese%20Vision-Language%20Retinal%20Foundation%20Model&body=Title%3A%20ViLReF%3A%20A%20Chinese%20Vision-Language%20Retinal%20Foundation%20Model%0AAuthor%3A%20Shengzhu%20Yang%20and%20Jiawei%20Du%20and%20Jia%20Guo%20and%20Weihang%20Zhang%20and%20Hanruo%20Liu%20and%20Huiqi%20Li%20and%20Ningli%20Wang%0AAbstract%3A%20%20%20Subtle%20semantic%20differences%20in%20retinal%20image%20and%20text%20data%20present%20great%0Achallenges%20for%20pre-training%20visual-language%20models.%20Moreover%2C%20false%20negative%0Asamples%2C%20i.e.%2C%20image-text%20pairs%20having%20the%20same%20semantics%20but%20incorrectly%0Aregarded%20as%20negatives%2C%20disrupt%20the%20visual-language%20pre-training%20process%20and%0Aaffect%20the%20model%27s%20learning%20ability.%20This%20work%20aims%20to%20develop%20a%20retinal%0Afoundation%20model%2C%20called%20ViLReF%2C%20by%20pre-training%20on%20a%20paired%20dataset%20comprising%0A451%2C956%20retinal%20images%20and%20corresponding%20diagnostic%20text%20reports.%20In%20our%0Avision-language%20pre-training%20strategy%2C%20we%20leverage%20expert%20knowledge%20to%0Afacilitate%20the%20extraction%20of%20labels%20and%20propose%20a%20novel%20constraint%2C%20the%0AWeighted%20Similarity%20Coupling%20Loss%2C%20to%20adjust%20the%20speed%20of%20pushing%20sample%20pairs%0Afurther%20apart%20dynamically%20within%20the%20feature%20space.%20Furthermore%2C%20we%20employ%20a%0Abatch%20expansion%20module%20with%20dynamic%20memory%20queues%2C%20maintained%20by%20momentum%0Aencoders%2C%20to%20supply%20extra%20samples%20and%20compensate%20for%20the%20vacancies%20caused%20by%0Aeliminating%20false%20negatives.%20Extensive%20experiments%20are%20conducted%20on%20multiple%0Adatasets%20for%20downstream%20classification%20and%20segmentation%20tasks.%20The%20experimental%0Aresults%20demonstrate%20the%20powerful%20zero-shot%20and%20transfer%20learning%20capabilities%0Aof%20ViLReF%2C%20verifying%20the%20effectiveness%20of%20our%20pre-training%20strategy.%20Our%20ViLReF%0Amodel%20is%20available%20at%3A%20https%3A//github.com/T6Yang/ViLReF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViLReF%253A%2520A%2520Chinese%2520Vision-Language%2520Retinal%2520Foundation%2520Model%26entry.906535625%3DShengzhu%2520Yang%2520and%2520Jiawei%2520Du%2520and%2520Jia%2520Guo%2520and%2520Weihang%2520Zhang%2520and%2520Hanruo%2520Liu%2520and%2520Huiqi%2520Li%2520and%2520Ningli%2520Wang%26entry.1292438233%3D%2520%2520Subtle%2520semantic%2520differences%2520in%2520retinal%2520image%2520and%2520text%2520data%2520present%2520great%250Achallenges%2520for%2520pre-training%2520visual-language%2520models.%2520Moreover%252C%2520false%2520negative%250Asamples%252C%2520i.e.%252C%2520image-text%2520pairs%2520having%2520the%2520same%2520semantics%2520but%2520incorrectly%250Aregarded%2520as%2520negatives%252C%2520disrupt%2520the%2520visual-language%2520pre-training%2520process%2520and%250Aaffect%2520the%2520model%2527s%2520learning%2520ability.%2520This%2520work%2520aims%2520to%2520develop%2520a%2520retinal%250Afoundation%2520model%252C%2520called%2520ViLReF%252C%2520by%2520pre-training%2520on%2520a%2520paired%2520dataset%2520comprising%250A451%252C956%2520retinal%2520images%2520and%2520corresponding%2520diagnostic%2520text%2520reports.%2520In%2520our%250Avision-language%2520pre-training%2520strategy%252C%2520we%2520leverage%2520expert%2520knowledge%2520to%250Afacilitate%2520the%2520extraction%2520of%2520labels%2520and%2520propose%2520a%2520novel%2520constraint%252C%2520the%250AWeighted%2520Similarity%2520Coupling%2520Loss%252C%2520to%2520adjust%2520the%2520speed%2520of%2520pushing%2520sample%2520pairs%250Afurther%2520apart%2520dynamically%2520within%2520the%2520feature%2520space.%2520Furthermore%252C%2520we%2520employ%2520a%250Abatch%2520expansion%2520module%2520with%2520dynamic%2520memory%2520queues%252C%2520maintained%2520by%2520momentum%250Aencoders%252C%2520to%2520supply%2520extra%2520samples%2520and%2520compensate%2520for%2520the%2520vacancies%2520caused%2520by%250Aeliminating%2520false%2520negatives.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520multiple%250Adatasets%2520for%2520downstream%2520classification%2520and%2520segmentation%2520tasks.%2520The%2520experimental%250Aresults%2520demonstrate%2520the%2520powerful%2520zero-shot%2520and%2520transfer%2520learning%2520capabilities%250Aof%2520ViLReF%252C%2520verifying%2520the%2520effectiveness%2520of%2520our%2520pre-training%2520strategy.%2520Our%2520ViLReF%250Amodel%2520is%2520available%2520at%253A%2520https%253A//github.com/T6Yang/ViLReF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViLReF%3A%20A%20Chinese%20Vision-Language%20Retinal%20Foundation%20Model&entry.906535625=Shengzhu%20Yang%20and%20Jiawei%20Du%20and%20Jia%20Guo%20and%20Weihang%20Zhang%20and%20Hanruo%20Liu%20and%20Huiqi%20Li%20and%20Ningli%20Wang&entry.1292438233=%20%20Subtle%20semantic%20differences%20in%20retinal%20image%20and%20text%20data%20present%20great%0Achallenges%20for%20pre-training%20visual-language%20models.%20Moreover%2C%20false%20negative%0Asamples%2C%20i.e.%2C%20image-text%20pairs%20having%20the%20same%20semantics%20but%20incorrectly%0Aregarded%20as%20negatives%2C%20disrupt%20the%20visual-language%20pre-training%20process%20and%0Aaffect%20the%20model%27s%20learning%20ability.%20This%20work%20aims%20to%20develop%20a%20retinal%0Afoundation%20model%2C%20called%20ViLReF%2C%20by%20pre-training%20on%20a%20paired%20dataset%20comprising%0A451%2C956%20retinal%20images%20and%20corresponding%20diagnostic%20text%20reports.%20In%20our%0Avision-language%20pre-training%20strategy%2C%20we%20leverage%20expert%20knowledge%20to%0Afacilitate%20the%20extraction%20of%20labels%20and%20propose%20a%20novel%20constraint%2C%20the%0AWeighted%20Similarity%20Coupling%20Loss%2C%20to%20adjust%20the%20speed%20of%20pushing%20sample%20pairs%0Afurther%20apart%20dynamically%20within%20the%20feature%20space.%20Furthermore%2C%20we%20employ%20a%0Abatch%20expansion%20module%20with%20dynamic%20memory%20queues%2C%20maintained%20by%20momentum%0Aencoders%2C%20to%20supply%20extra%20samples%20and%20compensate%20for%20the%20vacancies%20caused%20by%0Aeliminating%20false%20negatives.%20Extensive%20experiments%20are%20conducted%20on%20multiple%0Adatasets%20for%20downstream%20classification%20and%20segmentation%20tasks.%20The%20experimental%0Aresults%20demonstrate%20the%20powerful%20zero-shot%20and%20transfer%20learning%20capabilities%0Aof%20ViLReF%2C%20verifying%20the%20effectiveness%20of%20our%20pre-training%20strategy.%20Our%20ViLReF%0Amodel%20is%20available%20at%3A%20https%3A//github.com/T6Yang/ViLReF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10894v1&entry.124074799=Read"},
{"title": "Self-Supervised Disentanglement by Leveraging Structure in Data\n  Augmentations", "author": "Cian Eastwood and Julius von K\u00fcgelgen and Linus Ericsson and Diane Bouchacourt and Pascal Vincent and Bernhard Sch\u00f6lkopf and Mark Ibrahim", "abstract": "  Self-supervised representation learning often uses data augmentations to\ninduce some invariance to \"style\" attributes of the data. However, with\ndownstream tasks generally unknown at training time, it is difficult to deduce\na priori which attributes of the data are indeed \"style\" and can be safely\ndiscarded. To deal with this, current approaches try to retain some style\ninformation by tuning the degree of invariance to some particular task, such as\nImageNet object classification. However, prior work has shown that such\ntask-specific tuning can lead to significant performance degradation on other\ntasks that rely on the discarded style. To address this, we introduce a more\nprincipled approach that seeks to disentangle style features rather than\ndiscard them. The key idea is to add multiple style embedding spaces where: (i)\neach is invariant to all-but-one augmentation; and (ii) joint entropy is\nmaximized. We formalize our structured data-augmentation procedure from a\ncausal latent-variable-model perspective, and prove identifiability of both\ncontent and individual style variables. We empirically demonstrate the benefits\nof our approach on both synthetic and real-world data.\n", "link": "http://arxiv.org/abs/2311.08815v2", "date": "2024-08-20", "relevancy": 2.1931, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5904}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5195}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Disentanglement%20by%20Leveraging%20Structure%20in%20Data%0A%20%20Augmentations&body=Title%3A%20Self-Supervised%20Disentanglement%20by%20Leveraging%20Structure%20in%20Data%0A%20%20Augmentations%0AAuthor%3A%20Cian%20Eastwood%20and%20Julius%20von%20K%C3%BCgelgen%20and%20Linus%20Ericsson%20and%20Diane%20Bouchacourt%20and%20Pascal%20Vincent%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Mark%20Ibrahim%0AAbstract%3A%20%20%20Self-supervised%20representation%20learning%20often%20uses%20data%20augmentations%20to%0Ainduce%20some%20invariance%20to%20%22style%22%20attributes%20of%20the%20data.%20However%2C%20with%0Adownstream%20tasks%20generally%20unknown%20at%20training%20time%2C%20it%20is%20difficult%20to%20deduce%0Aa%20priori%20which%20attributes%20of%20the%20data%20are%20indeed%20%22style%22%20and%20can%20be%20safely%0Adiscarded.%20To%20deal%20with%20this%2C%20current%20approaches%20try%20to%20retain%20some%20style%0Ainformation%20by%20tuning%20the%20degree%20of%20invariance%20to%20some%20particular%20task%2C%20such%20as%0AImageNet%20object%20classification.%20However%2C%20prior%20work%20has%20shown%20that%20such%0Atask-specific%20tuning%20can%20lead%20to%20significant%20performance%20degradation%20on%20other%0Atasks%20that%20rely%20on%20the%20discarded%20style.%20To%20address%20this%2C%20we%20introduce%20a%20more%0Aprincipled%20approach%20that%20seeks%20to%20disentangle%20style%20features%20rather%20than%0Adiscard%20them.%20The%20key%20idea%20is%20to%20add%20multiple%20style%20embedding%20spaces%20where%3A%20%28i%29%0Aeach%20is%20invariant%20to%20all-but-one%20augmentation%3B%20and%20%28ii%29%20joint%20entropy%20is%0Amaximized.%20We%20formalize%20our%20structured%20data-augmentation%20procedure%20from%20a%0Acausal%20latent-variable-model%20perspective%2C%20and%20prove%20identifiability%20of%20both%0Acontent%20and%20individual%20style%20variables.%20We%20empirically%20demonstrate%20the%20benefits%0Aof%20our%20approach%20on%20both%20synthetic%20and%20real-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Disentanglement%2520by%2520Leveraging%2520Structure%2520in%2520Data%250A%2520%2520Augmentations%26entry.906535625%3DCian%2520Eastwood%2520and%2520Julius%2520von%2520K%25C3%25BCgelgen%2520and%2520Linus%2520Ericsson%2520and%2520Diane%2520Bouchacourt%2520and%2520Pascal%2520Vincent%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Mark%2520Ibrahim%26entry.1292438233%3D%2520%2520Self-supervised%2520representation%2520learning%2520often%2520uses%2520data%2520augmentations%2520to%250Ainduce%2520some%2520invariance%2520to%2520%2522style%2522%2520attributes%2520of%2520the%2520data.%2520However%252C%2520with%250Adownstream%2520tasks%2520generally%2520unknown%2520at%2520training%2520time%252C%2520it%2520is%2520difficult%2520to%2520deduce%250Aa%2520priori%2520which%2520attributes%2520of%2520the%2520data%2520are%2520indeed%2520%2522style%2522%2520and%2520can%2520be%2520safely%250Adiscarded.%2520To%2520deal%2520with%2520this%252C%2520current%2520approaches%2520try%2520to%2520retain%2520some%2520style%250Ainformation%2520by%2520tuning%2520the%2520degree%2520of%2520invariance%2520to%2520some%2520particular%2520task%252C%2520such%2520as%250AImageNet%2520object%2520classification.%2520However%252C%2520prior%2520work%2520has%2520shown%2520that%2520such%250Atask-specific%2520tuning%2520can%2520lead%2520to%2520significant%2520performance%2520degradation%2520on%2520other%250Atasks%2520that%2520rely%2520on%2520the%2520discarded%2520style.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520more%250Aprincipled%2520approach%2520that%2520seeks%2520to%2520disentangle%2520style%2520features%2520rather%2520than%250Adiscard%2520them.%2520The%2520key%2520idea%2520is%2520to%2520add%2520multiple%2520style%2520embedding%2520spaces%2520where%253A%2520%2528i%2529%250Aeach%2520is%2520invariant%2520to%2520all-but-one%2520augmentation%253B%2520and%2520%2528ii%2529%2520joint%2520entropy%2520is%250Amaximized.%2520We%2520formalize%2520our%2520structured%2520data-augmentation%2520procedure%2520from%2520a%250Acausal%2520latent-variable-model%2520perspective%252C%2520and%2520prove%2520identifiability%2520of%2520both%250Acontent%2520and%2520individual%2520style%2520variables.%2520We%2520empirically%2520demonstrate%2520the%2520benefits%250Aof%2520our%2520approach%2520on%2520both%2520synthetic%2520and%2520real-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.08815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Disentanglement%20by%20Leveraging%20Structure%20in%20Data%0A%20%20Augmentations&entry.906535625=Cian%20Eastwood%20and%20Julius%20von%20K%C3%BCgelgen%20and%20Linus%20Ericsson%20and%20Diane%20Bouchacourt%20and%20Pascal%20Vincent%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Mark%20Ibrahim&entry.1292438233=%20%20Self-supervised%20representation%20learning%20often%20uses%20data%20augmentations%20to%0Ainduce%20some%20invariance%20to%20%22style%22%20attributes%20of%20the%20data.%20However%2C%20with%0Adownstream%20tasks%20generally%20unknown%20at%20training%20time%2C%20it%20is%20difficult%20to%20deduce%0Aa%20priori%20which%20attributes%20of%20the%20data%20are%20indeed%20%22style%22%20and%20can%20be%20safely%0Adiscarded.%20To%20deal%20with%20this%2C%20current%20approaches%20try%20to%20retain%20some%20style%0Ainformation%20by%20tuning%20the%20degree%20of%20invariance%20to%20some%20particular%20task%2C%20such%20as%0AImageNet%20object%20classification.%20However%2C%20prior%20work%20has%20shown%20that%20such%0Atask-specific%20tuning%20can%20lead%20to%20significant%20performance%20degradation%20on%20other%0Atasks%20that%20rely%20on%20the%20discarded%20style.%20To%20address%20this%2C%20we%20introduce%20a%20more%0Aprincipled%20approach%20that%20seeks%20to%20disentangle%20style%20features%20rather%20than%0Adiscard%20them.%20The%20key%20idea%20is%20to%20add%20multiple%20style%20embedding%20spaces%20where%3A%20%28i%29%0Aeach%20is%20invariant%20to%20all-but-one%20augmentation%3B%20and%20%28ii%29%20joint%20entropy%20is%0Amaximized.%20We%20formalize%20our%20structured%20data-augmentation%20procedure%20from%20a%0Acausal%20latent-variable-model%20perspective%2C%20and%20prove%20identifiability%20of%20both%0Acontent%20and%20individual%20style%20variables.%20We%20empirically%20demonstrate%20the%20benefits%0Aof%20our%20approach%20on%20both%20synthetic%20and%20real-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08815v2&entry.124074799=Read"},
{"title": "Generating Synthetic Fair Syntax-agnostic Data by Learning and\n  Distilling Fair Representation", "author": "Md Fahim Sikder and Resmi Ramachandranpillai and Daniel de Leng and Fredrik Heintz", "abstract": "  Data Fairness is a crucial topic due to the recent wide usage of AI powered\napplications. Most of the real-world data is filled with human or machine\nbiases and when those data are being used to train AI models, there is a chance\nthat the model will reflect the bias in the training data. Existing\nbias-mitigating generative methods based on GANs, Diffusion models need\nin-processing fairness objectives and fail to consider computational overhead\nwhile choosing computationally-heavy architectures, which may lead to high\ncomputational demands, instability and poor optimization performance. To\nmitigate this issue, in this work, we present a fair data generation technique\nbased on knowledge distillation, where we use a small architecture to distill\nthe fair representation in the latent space. The idea of fair latent space\ndistillation enables more flexible and stable training of Fair Generative\nModels (FGMs). We first learn a syntax-agnostic (for any data type) fair\nrepresentation of the data, followed by distillation in the latent space into a\nsmaller model. After distillation, we use the distilled fair latent space to\ngenerate high-fidelity fair synthetic data. While distilling, we employ quality\nloss (for fair distillation) and utility loss (for data utility) to ensure that\nthe fairness and data utility characteristics remain in the distilled latent\nspace. Our approaches show a 5%, 5% and 10% rise in performance in fairness,\nsynthetic sample quality and data utility, respectively, than the\nstate-of-the-art fair generative model.\n", "link": "http://arxiv.org/abs/2408.10755v1", "date": "2024-08-20", "relevancy": 2.1778, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5741}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5415}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Synthetic%20Fair%20Syntax-agnostic%20Data%20by%20Learning%20and%0A%20%20Distilling%20Fair%20Representation&body=Title%3A%20Generating%20Synthetic%20Fair%20Syntax-agnostic%20Data%20by%20Learning%20and%0A%20%20Distilling%20Fair%20Representation%0AAuthor%3A%20Md%20Fahim%20Sikder%20and%20Resmi%20Ramachandranpillai%20and%20Daniel%20de%20Leng%20and%20Fredrik%20Heintz%0AAbstract%3A%20%20%20Data%20Fairness%20is%20a%20crucial%20topic%20due%20to%20the%20recent%20wide%20usage%20of%20AI%20powered%0Aapplications.%20Most%20of%20the%20real-world%20data%20is%20filled%20with%20human%20or%20machine%0Abiases%20and%20when%20those%20data%20are%20being%20used%20to%20train%20AI%20models%2C%20there%20is%20a%20chance%0Athat%20the%20model%20will%20reflect%20the%20bias%20in%20the%20training%20data.%20Existing%0Abias-mitigating%20generative%20methods%20based%20on%20GANs%2C%20Diffusion%20models%20need%0Ain-processing%20fairness%20objectives%20and%20fail%20to%20consider%20computational%20overhead%0Awhile%20choosing%20computationally-heavy%20architectures%2C%20which%20may%20lead%20to%20high%0Acomputational%20demands%2C%20instability%20and%20poor%20optimization%20performance.%20To%0Amitigate%20this%20issue%2C%20in%20this%20work%2C%20we%20present%20a%20fair%20data%20generation%20technique%0Abased%20on%20knowledge%20distillation%2C%20where%20we%20use%20a%20small%20architecture%20to%20distill%0Athe%20fair%20representation%20in%20the%20latent%20space.%20The%20idea%20of%20fair%20latent%20space%0Adistillation%20enables%20more%20flexible%20and%20stable%20training%20of%20Fair%20Generative%0AModels%20%28FGMs%29.%20We%20first%20learn%20a%20syntax-agnostic%20%28for%20any%20data%20type%29%20fair%0Arepresentation%20of%20the%20data%2C%20followed%20by%20distillation%20in%20the%20latent%20space%20into%20a%0Asmaller%20model.%20After%20distillation%2C%20we%20use%20the%20distilled%20fair%20latent%20space%20to%0Agenerate%20high-fidelity%20fair%20synthetic%20data.%20While%20distilling%2C%20we%20employ%20quality%0Aloss%20%28for%20fair%20distillation%29%20and%20utility%20loss%20%28for%20data%20utility%29%20to%20ensure%20that%0Athe%20fairness%20and%20data%20utility%20characteristics%20remain%20in%20the%20distilled%20latent%0Aspace.%20Our%20approaches%20show%20a%205%25%2C%205%25%20and%2010%25%20rise%20in%20performance%20in%20fairness%2C%0Asynthetic%20sample%20quality%20and%20data%20utility%2C%20respectively%2C%20than%20the%0Astate-of-the-art%20fair%20generative%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Synthetic%2520Fair%2520Syntax-agnostic%2520Data%2520by%2520Learning%2520and%250A%2520%2520Distilling%2520Fair%2520Representation%26entry.906535625%3DMd%2520Fahim%2520Sikder%2520and%2520Resmi%2520Ramachandranpillai%2520and%2520Daniel%2520de%2520Leng%2520and%2520Fredrik%2520Heintz%26entry.1292438233%3D%2520%2520Data%2520Fairness%2520is%2520a%2520crucial%2520topic%2520due%2520to%2520the%2520recent%2520wide%2520usage%2520of%2520AI%2520powered%250Aapplications.%2520Most%2520of%2520the%2520real-world%2520data%2520is%2520filled%2520with%2520human%2520or%2520machine%250Abiases%2520and%2520when%2520those%2520data%2520are%2520being%2520used%2520to%2520train%2520AI%2520models%252C%2520there%2520is%2520a%2520chance%250Athat%2520the%2520model%2520will%2520reflect%2520the%2520bias%2520in%2520the%2520training%2520data.%2520Existing%250Abias-mitigating%2520generative%2520methods%2520based%2520on%2520GANs%252C%2520Diffusion%2520models%2520need%250Ain-processing%2520fairness%2520objectives%2520and%2520fail%2520to%2520consider%2520computational%2520overhead%250Awhile%2520choosing%2520computationally-heavy%2520architectures%252C%2520which%2520may%2520lead%2520to%2520high%250Acomputational%2520demands%252C%2520instability%2520and%2520poor%2520optimization%2520performance.%2520To%250Amitigate%2520this%2520issue%252C%2520in%2520this%2520work%252C%2520we%2520present%2520a%2520fair%2520data%2520generation%2520technique%250Abased%2520on%2520knowledge%2520distillation%252C%2520where%2520we%2520use%2520a%2520small%2520architecture%2520to%2520distill%250Athe%2520fair%2520representation%2520in%2520the%2520latent%2520space.%2520The%2520idea%2520of%2520fair%2520latent%2520space%250Adistillation%2520enables%2520more%2520flexible%2520and%2520stable%2520training%2520of%2520Fair%2520Generative%250AModels%2520%2528FGMs%2529.%2520We%2520first%2520learn%2520a%2520syntax-agnostic%2520%2528for%2520any%2520data%2520type%2529%2520fair%250Arepresentation%2520of%2520the%2520data%252C%2520followed%2520by%2520distillation%2520in%2520the%2520latent%2520space%2520into%2520a%250Asmaller%2520model.%2520After%2520distillation%252C%2520we%2520use%2520the%2520distilled%2520fair%2520latent%2520space%2520to%250Agenerate%2520high-fidelity%2520fair%2520synthetic%2520data.%2520While%2520distilling%252C%2520we%2520employ%2520quality%250Aloss%2520%2528for%2520fair%2520distillation%2529%2520and%2520utility%2520loss%2520%2528for%2520data%2520utility%2529%2520to%2520ensure%2520that%250Athe%2520fairness%2520and%2520data%2520utility%2520characteristics%2520remain%2520in%2520the%2520distilled%2520latent%250Aspace.%2520Our%2520approaches%2520show%2520a%25205%2525%252C%25205%2525%2520and%252010%2525%2520rise%2520in%2520performance%2520in%2520fairness%252C%250Asynthetic%2520sample%2520quality%2520and%2520data%2520utility%252C%2520respectively%252C%2520than%2520the%250Astate-of-the-art%2520fair%2520generative%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Synthetic%20Fair%20Syntax-agnostic%20Data%20by%20Learning%20and%0A%20%20Distilling%20Fair%20Representation&entry.906535625=Md%20Fahim%20Sikder%20and%20Resmi%20Ramachandranpillai%20and%20Daniel%20de%20Leng%20and%20Fredrik%20Heintz&entry.1292438233=%20%20Data%20Fairness%20is%20a%20crucial%20topic%20due%20to%20the%20recent%20wide%20usage%20of%20AI%20powered%0Aapplications.%20Most%20of%20the%20real-world%20data%20is%20filled%20with%20human%20or%20machine%0Abiases%20and%20when%20those%20data%20are%20being%20used%20to%20train%20AI%20models%2C%20there%20is%20a%20chance%0Athat%20the%20model%20will%20reflect%20the%20bias%20in%20the%20training%20data.%20Existing%0Abias-mitigating%20generative%20methods%20based%20on%20GANs%2C%20Diffusion%20models%20need%0Ain-processing%20fairness%20objectives%20and%20fail%20to%20consider%20computational%20overhead%0Awhile%20choosing%20computationally-heavy%20architectures%2C%20which%20may%20lead%20to%20high%0Acomputational%20demands%2C%20instability%20and%20poor%20optimization%20performance.%20To%0Amitigate%20this%20issue%2C%20in%20this%20work%2C%20we%20present%20a%20fair%20data%20generation%20technique%0Abased%20on%20knowledge%20distillation%2C%20where%20we%20use%20a%20small%20architecture%20to%20distill%0Athe%20fair%20representation%20in%20the%20latent%20space.%20The%20idea%20of%20fair%20latent%20space%0Adistillation%20enables%20more%20flexible%20and%20stable%20training%20of%20Fair%20Generative%0AModels%20%28FGMs%29.%20We%20first%20learn%20a%20syntax-agnostic%20%28for%20any%20data%20type%29%20fair%0Arepresentation%20of%20the%20data%2C%20followed%20by%20distillation%20in%20the%20latent%20space%20into%20a%0Asmaller%20model.%20After%20distillation%2C%20we%20use%20the%20distilled%20fair%20latent%20space%20to%0Agenerate%20high-fidelity%20fair%20synthetic%20data.%20While%20distilling%2C%20we%20employ%20quality%0Aloss%20%28for%20fair%20distillation%29%20and%20utility%20loss%20%28for%20data%20utility%29%20to%20ensure%20that%0Athe%20fairness%20and%20data%20utility%20characteristics%20remain%20in%20the%20distilled%20latent%0Aspace.%20Our%20approaches%20show%20a%205%25%2C%205%25%20and%2010%25%20rise%20in%20performance%20in%20fairness%2C%0Asynthetic%20sample%20quality%20and%20data%20utility%2C%20respectively%2C%20than%20the%0Astate-of-the-art%20fair%20generative%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10755v1&entry.124074799=Read"},
{"title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos", "author": "Fuzhao Xue and Yukang Chen and Dacheng Li and Qinghao Hu and Ligeng Zhu and Xiuyu Li and Yunhao Fang and Haotian Tang and Shang Yang and Zhijian Liu and Ethan He and Hongxu Yin and Pavlo Molchanov and Jan Kautz and Linxi Fan and Yuke Zhu and Yao Lu and Song Han", "abstract": "  Long-context capability is critical for multi-modal foundation models. We\nintroduce LongVILA, a full-stack solution for long-context vision-language\nmodels, including system, model training, and dataset development. On the\nsystem side, we introduce the first long-context Multi-Modal Sequence\nParallelism (MM-SP) system that enables long training and inference, enabling\n2M context length training on 256 GPUs without any gradient checkpointing.\nMM-SP is 2.1x - 5.7x faster than ring sequence parallelism and 1.1x - 1.4x\nfaster than Megatron context parallelism + tensor parallelism in text-only\nsettings. Moreover, it seamlessly integrates with Hugging Face Transformers.\nFor model training, we propose a five-stage pipeline comprising alignment,\npre-training, short supervised fine-tuning, context extension, and long\nsupervised fine-tuning. On datasets, we construct large-scale visual language\npre-training datasets and long video instruction-following datasets to support\nour multi-stage training process. LongVILA extends the number of frames of VILA\nfrom 8 to 1024, and improves the long video captioning score from 2.00 to 3.26\n(1.6x), achieving 99.5% accuracy in 1400-frames video (274k context length)\nneedle-in-a-haystack. LongVILA-8B demonstrates consistent accuracy improvements\non long videos in the VideoMME benchmark as the number of frames increases.\n", "link": "http://arxiv.org/abs/2408.10188v2", "date": "2024-08-20", "relevancy": 2.1742, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5592}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.538}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongVILA%3A%20Scaling%20Long-Context%20Visual%20Language%20Models%20for%20Long%20Videos&body=Title%3A%20LongVILA%3A%20Scaling%20Long-Context%20Visual%20Language%20Models%20for%20Long%20Videos%0AAuthor%3A%20Fuzhao%20Xue%20and%20Yukang%20Chen%20and%20Dacheng%20Li%20and%20Qinghao%20Hu%20and%20Ligeng%20Zhu%20and%20Xiuyu%20Li%20and%20Yunhao%20Fang%20and%20Haotian%20Tang%20and%20Shang%20Yang%20and%20Zhijian%20Liu%20and%20Ethan%20He%20and%20Hongxu%20Yin%20and%20Pavlo%20Molchanov%20and%20Jan%20Kautz%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%20and%20Yao%20Lu%20and%20Song%20Han%0AAbstract%3A%20%20%20Long-context%20capability%20is%20critical%20for%20multi-modal%20foundation%20models.%20We%0Aintroduce%20LongVILA%2C%20a%20full-stack%20solution%20for%20long-context%20vision-language%0Amodels%2C%20including%20system%2C%20model%20training%2C%20and%20dataset%20development.%20On%20the%0Asystem%20side%2C%20we%20introduce%20the%20first%20long-context%20Multi-Modal%20Sequence%0AParallelism%20%28MM-SP%29%20system%20that%20enables%20long%20training%20and%20inference%2C%20enabling%0A2M%20context%20length%20training%20on%20256%20GPUs%20without%20any%20gradient%20checkpointing.%0AMM-SP%20is%202.1x%20-%205.7x%20faster%20than%20ring%20sequence%20parallelism%20and%201.1x%20-%201.4x%0Afaster%20than%20Megatron%20context%20parallelism%20%2B%20tensor%20parallelism%20in%20text-only%0Asettings.%20Moreover%2C%20it%20seamlessly%20integrates%20with%20Hugging%20Face%20Transformers.%0AFor%20model%20training%2C%20we%20propose%20a%20five-stage%20pipeline%20comprising%20alignment%2C%0Apre-training%2C%20short%20supervised%20fine-tuning%2C%20context%20extension%2C%20and%20long%0Asupervised%20fine-tuning.%20On%20datasets%2C%20we%20construct%20large-scale%20visual%20language%0Apre-training%20datasets%20and%20long%20video%20instruction-following%20datasets%20to%20support%0Aour%20multi-stage%20training%20process.%20LongVILA%20extends%20the%20number%20of%20frames%20of%20VILA%0Afrom%208%20to%201024%2C%20and%20improves%20the%20long%20video%20captioning%20score%20from%202.00%20to%203.26%0A%281.6x%29%2C%20achieving%2099.5%25%20accuracy%20in%201400-frames%20video%20%28274k%20context%20length%29%0Aneedle-in-a-haystack.%20LongVILA-8B%20demonstrates%20consistent%20accuracy%20improvements%0Aon%20long%20videos%20in%20the%20VideoMME%20benchmark%20as%20the%20number%20of%20frames%20increases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10188v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongVILA%253A%2520Scaling%2520Long-Context%2520Visual%2520Language%2520Models%2520for%2520Long%2520Videos%26entry.906535625%3DFuzhao%2520Xue%2520and%2520Yukang%2520Chen%2520and%2520Dacheng%2520Li%2520and%2520Qinghao%2520Hu%2520and%2520Ligeng%2520Zhu%2520and%2520Xiuyu%2520Li%2520and%2520Yunhao%2520Fang%2520and%2520Haotian%2520Tang%2520and%2520Shang%2520Yang%2520and%2520Zhijian%2520Liu%2520and%2520Ethan%2520He%2520and%2520Hongxu%2520Yin%2520and%2520Pavlo%2520Molchanov%2520and%2520Jan%2520Kautz%2520and%2520Linxi%2520Fan%2520and%2520Yuke%2520Zhu%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520Long-context%2520capability%2520is%2520critical%2520for%2520multi-modal%2520foundation%2520models.%2520We%250Aintroduce%2520LongVILA%252C%2520a%2520full-stack%2520solution%2520for%2520long-context%2520vision-language%250Amodels%252C%2520including%2520system%252C%2520model%2520training%252C%2520and%2520dataset%2520development.%2520On%2520the%250Asystem%2520side%252C%2520we%2520introduce%2520the%2520first%2520long-context%2520Multi-Modal%2520Sequence%250AParallelism%2520%2528MM-SP%2529%2520system%2520that%2520enables%2520long%2520training%2520and%2520inference%252C%2520enabling%250A2M%2520context%2520length%2520training%2520on%2520256%2520GPUs%2520without%2520any%2520gradient%2520checkpointing.%250AMM-SP%2520is%25202.1x%2520-%25205.7x%2520faster%2520than%2520ring%2520sequence%2520parallelism%2520and%25201.1x%2520-%25201.4x%250Afaster%2520than%2520Megatron%2520context%2520parallelism%2520%252B%2520tensor%2520parallelism%2520in%2520text-only%250Asettings.%2520Moreover%252C%2520it%2520seamlessly%2520integrates%2520with%2520Hugging%2520Face%2520Transformers.%250AFor%2520model%2520training%252C%2520we%2520propose%2520a%2520five-stage%2520pipeline%2520comprising%2520alignment%252C%250Apre-training%252C%2520short%2520supervised%2520fine-tuning%252C%2520context%2520extension%252C%2520and%2520long%250Asupervised%2520fine-tuning.%2520On%2520datasets%252C%2520we%2520construct%2520large-scale%2520visual%2520language%250Apre-training%2520datasets%2520and%2520long%2520video%2520instruction-following%2520datasets%2520to%2520support%250Aour%2520multi-stage%2520training%2520process.%2520LongVILA%2520extends%2520the%2520number%2520of%2520frames%2520of%2520VILA%250Afrom%25208%2520to%25201024%252C%2520and%2520improves%2520the%2520long%2520video%2520captioning%2520score%2520from%25202.00%2520to%25203.26%250A%25281.6x%2529%252C%2520achieving%252099.5%2525%2520accuracy%2520in%25201400-frames%2520video%2520%2528274k%2520context%2520length%2529%250Aneedle-in-a-haystack.%2520LongVILA-8B%2520demonstrates%2520consistent%2520accuracy%2520improvements%250Aon%2520long%2520videos%2520in%2520the%2520VideoMME%2520benchmark%2520as%2520the%2520number%2520of%2520frames%2520increases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10188v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongVILA%3A%20Scaling%20Long-Context%20Visual%20Language%20Models%20for%20Long%20Videos&entry.906535625=Fuzhao%20Xue%20and%20Yukang%20Chen%20and%20Dacheng%20Li%20and%20Qinghao%20Hu%20and%20Ligeng%20Zhu%20and%20Xiuyu%20Li%20and%20Yunhao%20Fang%20and%20Haotian%20Tang%20and%20Shang%20Yang%20and%20Zhijian%20Liu%20and%20Ethan%20He%20and%20Hongxu%20Yin%20and%20Pavlo%20Molchanov%20and%20Jan%20Kautz%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%20and%20Yao%20Lu%20and%20Song%20Han&entry.1292438233=%20%20Long-context%20capability%20is%20critical%20for%20multi-modal%20foundation%20models.%20We%0Aintroduce%20LongVILA%2C%20a%20full-stack%20solution%20for%20long-context%20vision-language%0Amodels%2C%20including%20system%2C%20model%20training%2C%20and%20dataset%20development.%20On%20the%0Asystem%20side%2C%20we%20introduce%20the%20first%20long-context%20Multi-Modal%20Sequence%0AParallelism%20%28MM-SP%29%20system%20that%20enables%20long%20training%20and%20inference%2C%20enabling%0A2M%20context%20length%20training%20on%20256%20GPUs%20without%20any%20gradient%20checkpointing.%0AMM-SP%20is%202.1x%20-%205.7x%20faster%20than%20ring%20sequence%20parallelism%20and%201.1x%20-%201.4x%0Afaster%20than%20Megatron%20context%20parallelism%20%2B%20tensor%20parallelism%20in%20text-only%0Asettings.%20Moreover%2C%20it%20seamlessly%20integrates%20with%20Hugging%20Face%20Transformers.%0AFor%20model%20training%2C%20we%20propose%20a%20five-stage%20pipeline%20comprising%20alignment%2C%0Apre-training%2C%20short%20supervised%20fine-tuning%2C%20context%20extension%2C%20and%20long%0Asupervised%20fine-tuning.%20On%20datasets%2C%20we%20construct%20large-scale%20visual%20language%0Apre-training%20datasets%20and%20long%20video%20instruction-following%20datasets%20to%20support%0Aour%20multi-stage%20training%20process.%20LongVILA%20extends%20the%20number%20of%20frames%20of%20VILA%0Afrom%208%20to%201024%2C%20and%20improves%20the%20long%20video%20captioning%20score%20from%202.00%20to%203.26%0A%281.6x%29%2C%20achieving%2099.5%25%20accuracy%20in%201400-frames%20video%20%28274k%20context%20length%29%0Aneedle-in-a-haystack.%20LongVILA-8B%20demonstrates%20consistent%20accuracy%20improvements%0Aon%20long%20videos%20in%20the%20VideoMME%20benchmark%20as%20the%20number%20of%20frames%20increases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10188v2&entry.124074799=Read"},
{"title": "Screen Them All: High-Throughput Pan-Cancer Genetic and Phenotypic\n  Biomarker Screening from H&E Whole Slide Images", "author": "Yi Kan Wang and Ludmila Tydlitatova and Jeremy D. Kunz and Gerard Oakley and Ran A. Godrich and Matthew C. H. Lee and Chad Vanderbilt and Razik Yousfi and Thomas Fuchs and David S. Klimstra and Siqi Liu", "abstract": "  Many molecular alterations serve as clinically prognostic or\ntherapy-predictive biomarkers, typically detected using single or multi-gene\nmolecular assays. However, these assays are expensive, tissue destructive and\noften take weeks to complete. Using AI on routine H&E WSIs offers a fast and\neconomical approach to screen for multiple molecular biomarkers. We present a\nhigh-throughput AI-based system leveraging Virchow2, a foundation model\npre-trained on 3 million slides, to interrogate genomic features previously\ndetermined by an next-generation sequencing (NGS) assay, using 47,960 scanned\nhematoxylin and eosin (H&E) whole slide images (WSIs) from 38,984 cancer\npatients. Unlike traditional methods that train individual models for each\nbiomarker or cancer type, our system employs a unified model to simultaneously\npredict a wide range of clinically relevant molecular biomarkers across cancer\ntypes. By training the network to replicate the MSK-IMPACT targeted biomarker\npanel of 505 genes, it identified 80 high performing biomarkers with a mean\nAU-ROC of 0.89 in 15 most common cancer types. In addition, 40 biomarkers\ndemonstrated strong associations with specific cancer histologic subtypes.\nFurthermore, 58 biomarkers were associated with targets frequently assayed\nclinically for therapy selection and response prediction. The model can also\npredict the activity of five canonical signaling pathways, identify defects in\nDNA repair mechanisms, and predict genomic instability measured by tumor\nmutation burden, microsatellite instability (MSI), and chromosomal instability\n(CIN). The proposed model can offer potential to guide therapy selection,\nimprove treatment efficacy, accelerate patient screening for clinical trials\nand provoke the interrogation of new therapeutic targets.\n", "link": "http://arxiv.org/abs/2408.09554v2", "date": "2024-08-20", "relevancy": 2.1491, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4395}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4395}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Screen%20Them%20All%3A%20High-Throughput%20Pan-Cancer%20Genetic%20and%20Phenotypic%0A%20%20Biomarker%20Screening%20from%20H%26E%20Whole%20Slide%20Images&body=Title%3A%20Screen%20Them%20All%3A%20High-Throughput%20Pan-Cancer%20Genetic%20and%20Phenotypic%0A%20%20Biomarker%20Screening%20from%20H%26E%20Whole%20Slide%20Images%0AAuthor%3A%20Yi%20Kan%20Wang%20and%20Ludmila%20Tydlitatova%20and%20Jeremy%20D.%20Kunz%20and%20Gerard%20Oakley%20and%20Ran%20A.%20Godrich%20and%20Matthew%20C.%20H.%20Lee%20and%20Chad%20Vanderbilt%20and%20Razik%20Yousfi%20and%20Thomas%20Fuchs%20and%20David%20S.%20Klimstra%20and%20Siqi%20Liu%0AAbstract%3A%20%20%20Many%20molecular%20alterations%20serve%20as%20clinically%20prognostic%20or%0Atherapy-predictive%20biomarkers%2C%20typically%20detected%20using%20single%20or%20multi-gene%0Amolecular%20assays.%20However%2C%20these%20assays%20are%20expensive%2C%20tissue%20destructive%20and%0Aoften%20take%20weeks%20to%20complete.%20Using%20AI%20on%20routine%20H%26E%20WSIs%20offers%20a%20fast%20and%0Aeconomical%20approach%20to%20screen%20for%20multiple%20molecular%20biomarkers.%20We%20present%20a%0Ahigh-throughput%20AI-based%20system%20leveraging%20Virchow2%2C%20a%20foundation%20model%0Apre-trained%20on%203%20million%20slides%2C%20to%20interrogate%20genomic%20features%20previously%0Adetermined%20by%20an%20next-generation%20sequencing%20%28NGS%29%20assay%2C%20using%2047%2C960%20scanned%0Ahematoxylin%20and%20eosin%20%28H%26E%29%20whole%20slide%20images%20%28WSIs%29%20from%2038%2C984%20cancer%0Apatients.%20Unlike%20traditional%20methods%20that%20train%20individual%20models%20for%20each%0Abiomarker%20or%20cancer%20type%2C%20our%20system%20employs%20a%20unified%20model%20to%20simultaneously%0Apredict%20a%20wide%20range%20of%20clinically%20relevant%20molecular%20biomarkers%20across%20cancer%0Atypes.%20By%20training%20the%20network%20to%20replicate%20the%20MSK-IMPACT%20targeted%20biomarker%0Apanel%20of%20505%20genes%2C%20it%20identified%2080%20high%20performing%20biomarkers%20with%20a%20mean%0AAU-ROC%20of%200.89%20in%2015%20most%20common%20cancer%20types.%20In%20addition%2C%2040%20biomarkers%0Ademonstrated%20strong%20associations%20with%20specific%20cancer%20histologic%20subtypes.%0AFurthermore%2C%2058%20biomarkers%20were%20associated%20with%20targets%20frequently%20assayed%0Aclinically%20for%20therapy%20selection%20and%20response%20prediction.%20The%20model%20can%20also%0Apredict%20the%20activity%20of%20five%20canonical%20signaling%20pathways%2C%20identify%20defects%20in%0ADNA%20repair%20mechanisms%2C%20and%20predict%20genomic%20instability%20measured%20by%20tumor%0Amutation%20burden%2C%20microsatellite%20instability%20%28MSI%29%2C%20and%20chromosomal%20instability%0A%28CIN%29.%20The%20proposed%20model%20can%20offer%20potential%20to%20guide%20therapy%20selection%2C%0Aimprove%20treatment%20efficacy%2C%20accelerate%20patient%20screening%20for%20clinical%20trials%0Aand%20provoke%20the%20interrogation%20of%20new%20therapeutic%20targets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09554v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScreen%2520Them%2520All%253A%2520High-Throughput%2520Pan-Cancer%2520Genetic%2520and%2520Phenotypic%250A%2520%2520Biomarker%2520Screening%2520from%2520H%2526E%2520Whole%2520Slide%2520Images%26entry.906535625%3DYi%2520Kan%2520Wang%2520and%2520Ludmila%2520Tydlitatova%2520and%2520Jeremy%2520D.%2520Kunz%2520and%2520Gerard%2520Oakley%2520and%2520Ran%2520A.%2520Godrich%2520and%2520Matthew%2520C.%2520H.%2520Lee%2520and%2520Chad%2520Vanderbilt%2520and%2520Razik%2520Yousfi%2520and%2520Thomas%2520Fuchs%2520and%2520David%2520S.%2520Klimstra%2520and%2520Siqi%2520Liu%26entry.1292438233%3D%2520%2520Many%2520molecular%2520alterations%2520serve%2520as%2520clinically%2520prognostic%2520or%250Atherapy-predictive%2520biomarkers%252C%2520typically%2520detected%2520using%2520single%2520or%2520multi-gene%250Amolecular%2520assays.%2520However%252C%2520these%2520assays%2520are%2520expensive%252C%2520tissue%2520destructive%2520and%250Aoften%2520take%2520weeks%2520to%2520complete.%2520Using%2520AI%2520on%2520routine%2520H%2526E%2520WSIs%2520offers%2520a%2520fast%2520and%250Aeconomical%2520approach%2520to%2520screen%2520for%2520multiple%2520molecular%2520biomarkers.%2520We%2520present%2520a%250Ahigh-throughput%2520AI-based%2520system%2520leveraging%2520Virchow2%252C%2520a%2520foundation%2520model%250Apre-trained%2520on%25203%2520million%2520slides%252C%2520to%2520interrogate%2520genomic%2520features%2520previously%250Adetermined%2520by%2520an%2520next-generation%2520sequencing%2520%2528NGS%2529%2520assay%252C%2520using%252047%252C960%2520scanned%250Ahematoxylin%2520and%2520eosin%2520%2528H%2526E%2529%2520whole%2520slide%2520images%2520%2528WSIs%2529%2520from%252038%252C984%2520cancer%250Apatients.%2520Unlike%2520traditional%2520methods%2520that%2520train%2520individual%2520models%2520for%2520each%250Abiomarker%2520or%2520cancer%2520type%252C%2520our%2520system%2520employs%2520a%2520unified%2520model%2520to%2520simultaneously%250Apredict%2520a%2520wide%2520range%2520of%2520clinically%2520relevant%2520molecular%2520biomarkers%2520across%2520cancer%250Atypes.%2520By%2520training%2520the%2520network%2520to%2520replicate%2520the%2520MSK-IMPACT%2520targeted%2520biomarker%250Apanel%2520of%2520505%2520genes%252C%2520it%2520identified%252080%2520high%2520performing%2520biomarkers%2520with%2520a%2520mean%250AAU-ROC%2520of%25200.89%2520in%252015%2520most%2520common%2520cancer%2520types.%2520In%2520addition%252C%252040%2520biomarkers%250Ademonstrated%2520strong%2520associations%2520with%2520specific%2520cancer%2520histologic%2520subtypes.%250AFurthermore%252C%252058%2520biomarkers%2520were%2520associated%2520with%2520targets%2520frequently%2520assayed%250Aclinically%2520for%2520therapy%2520selection%2520and%2520response%2520prediction.%2520The%2520model%2520can%2520also%250Apredict%2520the%2520activity%2520of%2520five%2520canonical%2520signaling%2520pathways%252C%2520identify%2520defects%2520in%250ADNA%2520repair%2520mechanisms%252C%2520and%2520predict%2520genomic%2520instability%2520measured%2520by%2520tumor%250Amutation%2520burden%252C%2520microsatellite%2520instability%2520%2528MSI%2529%252C%2520and%2520chromosomal%2520instability%250A%2528CIN%2529.%2520The%2520proposed%2520model%2520can%2520offer%2520potential%2520to%2520guide%2520therapy%2520selection%252C%250Aimprove%2520treatment%2520efficacy%252C%2520accelerate%2520patient%2520screening%2520for%2520clinical%2520trials%250Aand%2520provoke%2520the%2520interrogation%2520of%2520new%2520therapeutic%2520targets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09554v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Screen%20Them%20All%3A%20High-Throughput%20Pan-Cancer%20Genetic%20and%20Phenotypic%0A%20%20Biomarker%20Screening%20from%20H%26E%20Whole%20Slide%20Images&entry.906535625=Yi%20Kan%20Wang%20and%20Ludmila%20Tydlitatova%20and%20Jeremy%20D.%20Kunz%20and%20Gerard%20Oakley%20and%20Ran%20A.%20Godrich%20and%20Matthew%20C.%20H.%20Lee%20and%20Chad%20Vanderbilt%20and%20Razik%20Yousfi%20and%20Thomas%20Fuchs%20and%20David%20S.%20Klimstra%20and%20Siqi%20Liu&entry.1292438233=%20%20Many%20molecular%20alterations%20serve%20as%20clinically%20prognostic%20or%0Atherapy-predictive%20biomarkers%2C%20typically%20detected%20using%20single%20or%20multi-gene%0Amolecular%20assays.%20However%2C%20these%20assays%20are%20expensive%2C%20tissue%20destructive%20and%0Aoften%20take%20weeks%20to%20complete.%20Using%20AI%20on%20routine%20H%26E%20WSIs%20offers%20a%20fast%20and%0Aeconomical%20approach%20to%20screen%20for%20multiple%20molecular%20biomarkers.%20We%20present%20a%0Ahigh-throughput%20AI-based%20system%20leveraging%20Virchow2%2C%20a%20foundation%20model%0Apre-trained%20on%203%20million%20slides%2C%20to%20interrogate%20genomic%20features%20previously%0Adetermined%20by%20an%20next-generation%20sequencing%20%28NGS%29%20assay%2C%20using%2047%2C960%20scanned%0Ahematoxylin%20and%20eosin%20%28H%26E%29%20whole%20slide%20images%20%28WSIs%29%20from%2038%2C984%20cancer%0Apatients.%20Unlike%20traditional%20methods%20that%20train%20individual%20models%20for%20each%0Abiomarker%20or%20cancer%20type%2C%20our%20system%20employs%20a%20unified%20model%20to%20simultaneously%0Apredict%20a%20wide%20range%20of%20clinically%20relevant%20molecular%20biomarkers%20across%20cancer%0Atypes.%20By%20training%20the%20network%20to%20replicate%20the%20MSK-IMPACT%20targeted%20biomarker%0Apanel%20of%20505%20genes%2C%20it%20identified%2080%20high%20performing%20biomarkers%20with%20a%20mean%0AAU-ROC%20of%200.89%20in%2015%20most%20common%20cancer%20types.%20In%20addition%2C%2040%20biomarkers%0Ademonstrated%20strong%20associations%20with%20specific%20cancer%20histologic%20subtypes.%0AFurthermore%2C%2058%20biomarkers%20were%20associated%20with%20targets%20frequently%20assayed%0Aclinically%20for%20therapy%20selection%20and%20response%20prediction.%20The%20model%20can%20also%0Apredict%20the%20activity%20of%20five%20canonical%20signaling%20pathways%2C%20identify%20defects%20in%0ADNA%20repair%20mechanisms%2C%20and%20predict%20genomic%20instability%20measured%20by%20tumor%0Amutation%20burden%2C%20microsatellite%20instability%20%28MSI%29%2C%20and%20chromosomal%20instability%0A%28CIN%29.%20The%20proposed%20model%20can%20offer%20potential%20to%20guide%20therapy%20selection%2C%0Aimprove%20treatment%20efficacy%2C%20accelerate%20patient%20screening%20for%20clinical%20trials%0Aand%20provoke%20the%20interrogation%20of%20new%20therapeutic%20targets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09554v2&entry.124074799=Read"},
{"title": "BrainVis: Exploring the Bridge between Brain and Visual Signals via\n  Image Reconstruction", "author": "Honghao Fu and Zhiqi Shen and Jing Jih Chin and Hao Wang", "abstract": "  Analyzing and reconstructing visual stimuli from brain signals effectively\nadvances the understanding of human visual system. However, the EEG signals are\ncomplex and contain significant noise. This leads to substantial limitations in\nexisting works of visual stimuli reconstruction from EEG, such as difficulties\nin aligning EEG embeddings with the fine-grained semantic information and a\nheavy reliance on additional large self-collected dataset for training. To\naddress these challenges, we propose a novel approach called BrainVis. Firstly,\nwe divide the EEG signals into various units and apply a self-supervised\napproach on them to obtain EEG time-domain features, in an attempt to ease the\ntraining difficulty. Additionally, we also propose to utilize the\nfrequency-domain features to enhance the EEG representations. Then, we\nsimultaneously align EEG time-frequency embeddings with the interpolation of\nthe coarse and fine-grained semantics in the CLIP space, to highlight the\nprimary visual components and reduce the cross-modal alignment difficulty.\nFinally, we adopt the cascaded diffusion models to reconstruct images. Using\nonly 10\\% training data of the previous work, our proposed BrainVis outperforms\nstate of the arts in both semantic fidelity reconstruction and generation\nquality. The code is available at https://github.com/RomGai/BrainVis.\n", "link": "http://arxiv.org/abs/2312.14871v2", "date": "2024-08-20", "relevancy": 2.1452, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5377}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5377}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BrainVis%3A%20Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%0A%20%20Image%20Reconstruction&body=Title%3A%20BrainVis%3A%20Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%0A%20%20Image%20Reconstruction%0AAuthor%3A%20Honghao%20Fu%20and%20Zhiqi%20Shen%20and%20Jing%20Jih%20Chin%20and%20Hao%20Wang%0AAbstract%3A%20%20%20Analyzing%20and%20reconstructing%20visual%20stimuli%20from%20brain%20signals%20effectively%0Aadvances%20the%20understanding%20of%20human%20visual%20system.%20However%2C%20the%20EEG%20signals%20are%0Acomplex%20and%20contain%20significant%20noise.%20This%20leads%20to%20substantial%20limitations%20in%0Aexisting%20works%20of%20visual%20stimuli%20reconstruction%20from%20EEG%2C%20such%20as%20difficulties%0Ain%20aligning%20EEG%20embeddings%20with%20the%20fine-grained%20semantic%20information%20and%20a%0Aheavy%20reliance%20on%20additional%20large%20self-collected%20dataset%20for%20training.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%20called%20BrainVis.%20Firstly%2C%0Awe%20divide%20the%20EEG%20signals%20into%20various%20units%20and%20apply%20a%20self-supervised%0Aapproach%20on%20them%20to%20obtain%20EEG%20time-domain%20features%2C%20in%20an%20attempt%20to%20ease%20the%0Atraining%20difficulty.%20Additionally%2C%20we%20also%20propose%20to%20utilize%20the%0Afrequency-domain%20features%20to%20enhance%20the%20EEG%20representations.%20Then%2C%20we%0Asimultaneously%20align%20EEG%20time-frequency%20embeddings%20with%20the%20interpolation%20of%0Athe%20coarse%20and%20fine-grained%20semantics%20in%20the%20CLIP%20space%2C%20to%20highlight%20the%0Aprimary%20visual%20components%20and%20reduce%20the%20cross-modal%20alignment%20difficulty.%0AFinally%2C%20we%20adopt%20the%20cascaded%20diffusion%20models%20to%20reconstruct%20images.%20Using%0Aonly%2010%5C%25%20training%20data%20of%20the%20previous%20work%2C%20our%20proposed%20BrainVis%20outperforms%0Astate%20of%20the%20arts%20in%20both%20semantic%20fidelity%20reconstruction%20and%20generation%0Aquality.%20The%20code%20is%20available%20at%20https%3A//github.com/RomGai/BrainVis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14871v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrainVis%253A%2520Exploring%2520the%2520Bridge%2520between%2520Brain%2520and%2520Visual%2520Signals%2520via%250A%2520%2520Image%2520Reconstruction%26entry.906535625%3DHonghao%2520Fu%2520and%2520Zhiqi%2520Shen%2520and%2520Jing%2520Jih%2520Chin%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520Analyzing%2520and%2520reconstructing%2520visual%2520stimuli%2520from%2520brain%2520signals%2520effectively%250Aadvances%2520the%2520understanding%2520of%2520human%2520visual%2520system.%2520However%252C%2520the%2520EEG%2520signals%2520are%250Acomplex%2520and%2520contain%2520significant%2520noise.%2520This%2520leads%2520to%2520substantial%2520limitations%2520in%250Aexisting%2520works%2520of%2520visual%2520stimuli%2520reconstruction%2520from%2520EEG%252C%2520such%2520as%2520difficulties%250Ain%2520aligning%2520EEG%2520embeddings%2520with%2520the%2520fine-grained%2520semantic%2520information%2520and%2520a%250Aheavy%2520reliance%2520on%2520additional%2520large%2520self-collected%2520dataset%2520for%2520training.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520approach%2520called%2520BrainVis.%2520Firstly%252C%250Awe%2520divide%2520the%2520EEG%2520signals%2520into%2520various%2520units%2520and%2520apply%2520a%2520self-supervised%250Aapproach%2520on%2520them%2520to%2520obtain%2520EEG%2520time-domain%2520features%252C%2520in%2520an%2520attempt%2520to%2520ease%2520the%250Atraining%2520difficulty.%2520Additionally%252C%2520we%2520also%2520propose%2520to%2520utilize%2520the%250Afrequency-domain%2520features%2520to%2520enhance%2520the%2520EEG%2520representations.%2520Then%252C%2520we%250Asimultaneously%2520align%2520EEG%2520time-frequency%2520embeddings%2520with%2520the%2520interpolation%2520of%250Athe%2520coarse%2520and%2520fine-grained%2520semantics%2520in%2520the%2520CLIP%2520space%252C%2520to%2520highlight%2520the%250Aprimary%2520visual%2520components%2520and%2520reduce%2520the%2520cross-modal%2520alignment%2520difficulty.%250AFinally%252C%2520we%2520adopt%2520the%2520cascaded%2520diffusion%2520models%2520to%2520reconstruct%2520images.%2520Using%250Aonly%252010%255C%2525%2520training%2520data%2520of%2520the%2520previous%2520work%252C%2520our%2520proposed%2520BrainVis%2520outperforms%250Astate%2520of%2520the%2520arts%2520in%2520both%2520semantic%2520fidelity%2520reconstruction%2520and%2520generation%250Aquality.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/RomGai/BrainVis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14871v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BrainVis%3A%20Exploring%20the%20Bridge%20between%20Brain%20and%20Visual%20Signals%20via%0A%20%20Image%20Reconstruction&entry.906535625=Honghao%20Fu%20and%20Zhiqi%20Shen%20and%20Jing%20Jih%20Chin%20and%20Hao%20Wang&entry.1292438233=%20%20Analyzing%20and%20reconstructing%20visual%20stimuli%20from%20brain%20signals%20effectively%0Aadvances%20the%20understanding%20of%20human%20visual%20system.%20However%2C%20the%20EEG%20signals%20are%0Acomplex%20and%20contain%20significant%20noise.%20This%20leads%20to%20substantial%20limitations%20in%0Aexisting%20works%20of%20visual%20stimuli%20reconstruction%20from%20EEG%2C%20such%20as%20difficulties%0Ain%20aligning%20EEG%20embeddings%20with%20the%20fine-grained%20semantic%20information%20and%20a%0Aheavy%20reliance%20on%20additional%20large%20self-collected%20dataset%20for%20training.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%20called%20BrainVis.%20Firstly%2C%0Awe%20divide%20the%20EEG%20signals%20into%20various%20units%20and%20apply%20a%20self-supervised%0Aapproach%20on%20them%20to%20obtain%20EEG%20time-domain%20features%2C%20in%20an%20attempt%20to%20ease%20the%0Atraining%20difficulty.%20Additionally%2C%20we%20also%20propose%20to%20utilize%20the%0Afrequency-domain%20features%20to%20enhance%20the%20EEG%20representations.%20Then%2C%20we%0Asimultaneously%20align%20EEG%20time-frequency%20embeddings%20with%20the%20interpolation%20of%0Athe%20coarse%20and%20fine-grained%20semantics%20in%20the%20CLIP%20space%2C%20to%20highlight%20the%0Aprimary%20visual%20components%20and%20reduce%20the%20cross-modal%20alignment%20difficulty.%0AFinally%2C%20we%20adopt%20the%20cascaded%20diffusion%20models%20to%20reconstruct%20images.%20Using%0Aonly%2010%5C%25%20training%20data%20of%20the%20previous%20work%2C%20our%20proposed%20BrainVis%20outperforms%0Astate%20of%20the%20arts%20in%20both%20semantic%20fidelity%20reconstruction%20and%20generation%0Aquality.%20The%20code%20is%20available%20at%20https%3A//github.com/RomGai/BrainVis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14871v2&entry.124074799=Read"},
{"title": "Just a Hint: Point-Supervised Camouflaged Object Detection", "author": "Huafeng Chen and Dian Shao and Guangqian Guo and Shan Gao", "abstract": "  Camouflaged Object Detection (COD) demands models to expeditiously and\naccurately distinguish objects which conceal themselves seamlessly in the\nenvironment. Owing to the subtle differences and ambiguous boundaries, COD is\nnot only a remarkably challenging task for models but also for human\nannotators, requiring huge efforts to provide pixel-wise annotations. To\nalleviate the heavy annotation burden, we propose to fulfill this task with the\nhelp of only one point supervision. Specifically, by swiftly clicking on each\nobject, we first adaptively expand the original point-based annotation to a\nreasonable hint area. Then, to avoid partial localization around discriminative\nparts, we propose an attention regulator to scatter model attention to the\nwhole object through partially masking labeled regions. Moreover, to solve the\nunstable feature representation of camouflaged objects under only point-based\nannotation, we perform unsupervised contrastive learning based on differently\naugmented image pairs (e.g. changing color or doing translation). On three\nmainstream COD benchmarks, experimental results show that our model outperforms\nseveral weakly-supervised methods by a large margin across various metrics.\n", "link": "http://arxiv.org/abs/2408.10777v1", "date": "2024-08-20", "relevancy": 2.1301, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5411}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5397}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Just%20a%20Hint%3A%20Point-Supervised%20Camouflaged%20Object%20Detection&body=Title%3A%20Just%20a%20Hint%3A%20Point-Supervised%20Camouflaged%20Object%20Detection%0AAuthor%3A%20Huafeng%20Chen%20and%20Dian%20Shao%20and%20Guangqian%20Guo%20and%20Shan%20Gao%0AAbstract%3A%20%20%20Camouflaged%20Object%20Detection%20%28COD%29%20demands%20models%20to%20expeditiously%20and%0Aaccurately%20distinguish%20objects%20which%20conceal%20themselves%20seamlessly%20in%20the%0Aenvironment.%20Owing%20to%20the%20subtle%20differences%20and%20ambiguous%20boundaries%2C%20COD%20is%0Anot%20only%20a%20remarkably%20challenging%20task%20for%20models%20but%20also%20for%20human%0Aannotators%2C%20requiring%20huge%20efforts%20to%20provide%20pixel-wise%20annotations.%20To%0Aalleviate%20the%20heavy%20annotation%20burden%2C%20we%20propose%20to%20fulfill%20this%20task%20with%20the%0Ahelp%20of%20only%20one%20point%20supervision.%20Specifically%2C%20by%20swiftly%20clicking%20on%20each%0Aobject%2C%20we%20first%20adaptively%20expand%20the%20original%20point-based%20annotation%20to%20a%0Areasonable%20hint%20area.%20Then%2C%20to%20avoid%20partial%20localization%20around%20discriminative%0Aparts%2C%20we%20propose%20an%20attention%20regulator%20to%20scatter%20model%20attention%20to%20the%0Awhole%20object%20through%20partially%20masking%20labeled%20regions.%20Moreover%2C%20to%20solve%20the%0Aunstable%20feature%20representation%20of%20camouflaged%20objects%20under%20only%20point-based%0Aannotation%2C%20we%20perform%20unsupervised%20contrastive%20learning%20based%20on%20differently%0Aaugmented%20image%20pairs%20%28e.g.%20changing%20color%20or%20doing%20translation%29.%20On%20three%0Amainstream%20COD%20benchmarks%2C%20experimental%20results%20show%20that%20our%20model%20outperforms%0Aseveral%20weakly-supervised%20methods%20by%20a%20large%20margin%20across%20various%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJust%2520a%2520Hint%253A%2520Point-Supervised%2520Camouflaged%2520Object%2520Detection%26entry.906535625%3DHuafeng%2520Chen%2520and%2520Dian%2520Shao%2520and%2520Guangqian%2520Guo%2520and%2520Shan%2520Gao%26entry.1292438233%3D%2520%2520Camouflaged%2520Object%2520Detection%2520%2528COD%2529%2520demands%2520models%2520to%2520expeditiously%2520and%250Aaccurately%2520distinguish%2520objects%2520which%2520conceal%2520themselves%2520seamlessly%2520in%2520the%250Aenvironment.%2520Owing%2520to%2520the%2520subtle%2520differences%2520and%2520ambiguous%2520boundaries%252C%2520COD%2520is%250Anot%2520only%2520a%2520remarkably%2520challenging%2520task%2520for%2520models%2520but%2520also%2520for%2520human%250Aannotators%252C%2520requiring%2520huge%2520efforts%2520to%2520provide%2520pixel-wise%2520annotations.%2520To%250Aalleviate%2520the%2520heavy%2520annotation%2520burden%252C%2520we%2520propose%2520to%2520fulfill%2520this%2520task%2520with%2520the%250Ahelp%2520of%2520only%2520one%2520point%2520supervision.%2520Specifically%252C%2520by%2520swiftly%2520clicking%2520on%2520each%250Aobject%252C%2520we%2520first%2520adaptively%2520expand%2520the%2520original%2520point-based%2520annotation%2520to%2520a%250Areasonable%2520hint%2520area.%2520Then%252C%2520to%2520avoid%2520partial%2520localization%2520around%2520discriminative%250Aparts%252C%2520we%2520propose%2520an%2520attention%2520regulator%2520to%2520scatter%2520model%2520attention%2520to%2520the%250Awhole%2520object%2520through%2520partially%2520masking%2520labeled%2520regions.%2520Moreover%252C%2520to%2520solve%2520the%250Aunstable%2520feature%2520representation%2520of%2520camouflaged%2520objects%2520under%2520only%2520point-based%250Aannotation%252C%2520we%2520perform%2520unsupervised%2520contrastive%2520learning%2520based%2520on%2520differently%250Aaugmented%2520image%2520pairs%2520%2528e.g.%2520changing%2520color%2520or%2520doing%2520translation%2529.%2520On%2520three%250Amainstream%2520COD%2520benchmarks%252C%2520experimental%2520results%2520show%2520that%2520our%2520model%2520outperforms%250Aseveral%2520weakly-supervised%2520methods%2520by%2520a%2520large%2520margin%2520across%2520various%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Just%20a%20Hint%3A%20Point-Supervised%20Camouflaged%20Object%20Detection&entry.906535625=Huafeng%20Chen%20and%20Dian%20Shao%20and%20Guangqian%20Guo%20and%20Shan%20Gao&entry.1292438233=%20%20Camouflaged%20Object%20Detection%20%28COD%29%20demands%20models%20to%20expeditiously%20and%0Aaccurately%20distinguish%20objects%20which%20conceal%20themselves%20seamlessly%20in%20the%0Aenvironment.%20Owing%20to%20the%20subtle%20differences%20and%20ambiguous%20boundaries%2C%20COD%20is%0Anot%20only%20a%20remarkably%20challenging%20task%20for%20models%20but%20also%20for%20human%0Aannotators%2C%20requiring%20huge%20efforts%20to%20provide%20pixel-wise%20annotations.%20To%0Aalleviate%20the%20heavy%20annotation%20burden%2C%20we%20propose%20to%20fulfill%20this%20task%20with%20the%0Ahelp%20of%20only%20one%20point%20supervision.%20Specifically%2C%20by%20swiftly%20clicking%20on%20each%0Aobject%2C%20we%20first%20adaptively%20expand%20the%20original%20point-based%20annotation%20to%20a%0Areasonable%20hint%20area.%20Then%2C%20to%20avoid%20partial%20localization%20around%20discriminative%0Aparts%2C%20we%20propose%20an%20attention%20regulator%20to%20scatter%20model%20attention%20to%20the%0Awhole%20object%20through%20partially%20masking%20labeled%20regions.%20Moreover%2C%20to%20solve%20the%0Aunstable%20feature%20representation%20of%20camouflaged%20objects%20under%20only%20point-based%0Aannotation%2C%20we%20perform%20unsupervised%20contrastive%20learning%20based%20on%20differently%0Aaugmented%20image%20pairs%20%28e.g.%20changing%20color%20or%20doing%20translation%29.%20On%20three%0Amainstream%20COD%20benchmarks%2C%20experimental%20results%20show%20that%20our%20model%20outperforms%0Aseveral%20weakly-supervised%20methods%20by%20a%20large%20margin%20across%20various%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10777v1&entry.124074799=Read"},
{"title": "Facial Demorphing via Identity Preserving Image Decomposition", "author": "Nitish Shukla and Arun Ross", "abstract": "  A face morph is created by combining the face images usually pertaining to\ntwo distinct identities. The goal is to generate an image that can be matched\nwith two identities thereby undermining the security of a face recognition\nsystem. To deal with this problem, several morph attack detection techniques\nhave been developed. But these methods do not extract any information about the\nunderlying bonafides used to create them. Demorphing addresses this limitation.\nHowever, current demorphing techniques are mostly reference-based, i.e, they\nneed an image of one of the identities to recover the other. In this work, we\ntreat demorphing as an ill-posed decomposition problem. We propose a novel\nmethod that is reference-free and recovers the bonafides with high accuracy.\nOur method decomposes the morph into several identity-preserving feature\ncomponents. A merger network then weighs and combines these components to\nrecover the bonafides. Our method is observed to reconstruct high-quality\nbonafides in terms of definition and fidelity. Experiments on the\nCASIA-WebFace, SMDD and AMSL datasets demonstrate the effectiveness of our\nmethod.\n", "link": "http://arxiv.org/abs/2408.10993v1", "date": "2024-08-20", "relevancy": 2.1261, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5523}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5406}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Facial%20Demorphing%20via%20Identity%20Preserving%20Image%20Decomposition&body=Title%3A%20Facial%20Demorphing%20via%20Identity%20Preserving%20Image%20Decomposition%0AAuthor%3A%20Nitish%20Shukla%20and%20Arun%20Ross%0AAbstract%3A%20%20%20A%20face%20morph%20is%20created%20by%20combining%20the%20face%20images%20usually%20pertaining%20to%0Atwo%20distinct%20identities.%20The%20goal%20is%20to%20generate%20an%20image%20that%20can%20be%20matched%0Awith%20two%20identities%20thereby%20undermining%20the%20security%20of%20a%20face%20recognition%0Asystem.%20To%20deal%20with%20this%20problem%2C%20several%20morph%20attack%20detection%20techniques%0Ahave%20been%20developed.%20But%20these%20methods%20do%20not%20extract%20any%20information%20about%20the%0Aunderlying%20bonafides%20used%20to%20create%20them.%20Demorphing%20addresses%20this%20limitation.%0AHowever%2C%20current%20demorphing%20techniques%20are%20mostly%20reference-based%2C%20i.e%2C%20they%0Aneed%20an%20image%20of%20one%20of%20the%20identities%20to%20recover%20the%20other.%20In%20this%20work%2C%20we%0Atreat%20demorphing%20as%20an%20ill-posed%20decomposition%20problem.%20We%20propose%20a%20novel%0Amethod%20that%20is%20reference-free%20and%20recovers%20the%20bonafides%20with%20high%20accuracy.%0AOur%20method%20decomposes%20the%20morph%20into%20several%20identity-preserving%20feature%0Acomponents.%20A%20merger%20network%20then%20weighs%20and%20combines%20these%20components%20to%0Arecover%20the%20bonafides.%20Our%20method%20is%20observed%20to%20reconstruct%20high-quality%0Abonafides%20in%20terms%20of%20definition%20and%20fidelity.%20Experiments%20on%20the%0ACASIA-WebFace%2C%20SMDD%20and%20AMSL%20datasets%20demonstrate%20the%20effectiveness%20of%20our%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFacial%2520Demorphing%2520via%2520Identity%2520Preserving%2520Image%2520Decomposition%26entry.906535625%3DNitish%2520Shukla%2520and%2520Arun%2520Ross%26entry.1292438233%3D%2520%2520A%2520face%2520morph%2520is%2520created%2520by%2520combining%2520the%2520face%2520images%2520usually%2520pertaining%2520to%250Atwo%2520distinct%2520identities.%2520The%2520goal%2520is%2520to%2520generate%2520an%2520image%2520that%2520can%2520be%2520matched%250Awith%2520two%2520identities%2520thereby%2520undermining%2520the%2520security%2520of%2520a%2520face%2520recognition%250Asystem.%2520To%2520deal%2520with%2520this%2520problem%252C%2520several%2520morph%2520attack%2520detection%2520techniques%250Ahave%2520been%2520developed.%2520But%2520these%2520methods%2520do%2520not%2520extract%2520any%2520information%2520about%2520the%250Aunderlying%2520bonafides%2520used%2520to%2520create%2520them.%2520Demorphing%2520addresses%2520this%2520limitation.%250AHowever%252C%2520current%2520demorphing%2520techniques%2520are%2520mostly%2520reference-based%252C%2520i.e%252C%2520they%250Aneed%2520an%2520image%2520of%2520one%2520of%2520the%2520identities%2520to%2520recover%2520the%2520other.%2520In%2520this%2520work%252C%2520we%250Atreat%2520demorphing%2520as%2520an%2520ill-posed%2520decomposition%2520problem.%2520We%2520propose%2520a%2520novel%250Amethod%2520that%2520is%2520reference-free%2520and%2520recovers%2520the%2520bonafides%2520with%2520high%2520accuracy.%250AOur%2520method%2520decomposes%2520the%2520morph%2520into%2520several%2520identity-preserving%2520feature%250Acomponents.%2520A%2520merger%2520network%2520then%2520weighs%2520and%2520combines%2520these%2520components%2520to%250Arecover%2520the%2520bonafides.%2520Our%2520method%2520is%2520observed%2520to%2520reconstruct%2520high-quality%250Abonafides%2520in%2520terms%2520of%2520definition%2520and%2520fidelity.%2520Experiments%2520on%2520the%250ACASIA-WebFace%252C%2520SMDD%2520and%2520AMSL%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Facial%20Demorphing%20via%20Identity%20Preserving%20Image%20Decomposition&entry.906535625=Nitish%20Shukla%20and%20Arun%20Ross&entry.1292438233=%20%20A%20face%20morph%20is%20created%20by%20combining%20the%20face%20images%20usually%20pertaining%20to%0Atwo%20distinct%20identities.%20The%20goal%20is%20to%20generate%20an%20image%20that%20can%20be%20matched%0Awith%20two%20identities%20thereby%20undermining%20the%20security%20of%20a%20face%20recognition%0Asystem.%20To%20deal%20with%20this%20problem%2C%20several%20morph%20attack%20detection%20techniques%0Ahave%20been%20developed.%20But%20these%20methods%20do%20not%20extract%20any%20information%20about%20the%0Aunderlying%20bonafides%20used%20to%20create%20them.%20Demorphing%20addresses%20this%20limitation.%0AHowever%2C%20current%20demorphing%20techniques%20are%20mostly%20reference-based%2C%20i.e%2C%20they%0Aneed%20an%20image%20of%20one%20of%20the%20identities%20to%20recover%20the%20other.%20In%20this%20work%2C%20we%0Atreat%20demorphing%20as%20an%20ill-posed%20decomposition%20problem.%20We%20propose%20a%20novel%0Amethod%20that%20is%20reference-free%20and%20recovers%20the%20bonafides%20with%20high%20accuracy.%0AOur%20method%20decomposes%20the%20morph%20into%20several%20identity-preserving%20feature%0Acomponents.%20A%20merger%20network%20then%20weighs%20and%20combines%20these%20components%20to%0Arecover%20the%20bonafides.%20Our%20method%20is%20observed%20to%20reconstruct%20high-quality%0Abonafides%20in%20terms%20of%20definition%20and%20fidelity.%20Experiments%20on%20the%0ACASIA-WebFace%2C%20SMDD%20and%20AMSL%20datasets%20demonstrate%20the%20effectiveness%20of%20our%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10993v1&entry.124074799=Read"},
{"title": "SAM-COD: SAM-guided Unified Framework for Weakly-Supervised Camouflaged\n  Object Detection", "author": "Huafeng Chen and Pengxu Wei and Guangqian Guo and Shan Gao", "abstract": "  Most Camouflaged Object Detection (COD) methods heavily rely on mask\nannotations, which are time-consuming and labor-intensive to acquire. Existing\nweakly-supervised COD approaches exhibit significantly inferior performance\ncompared to fully-supervised methods and struggle to simultaneously support all\nthe existing types of camouflaged object labels, including scribbles, bounding\nboxes, and points. Even for Segment Anything Model (SAM), it is still\nproblematic to handle the weakly-supervised COD and it typically encounters\nchallenges of prompt compatibility of the scribble labels, extreme response,\nsemantically erroneous response, and unstable feature representations,\nproducing unsatisfactory results in camouflaged scenes. To mitigate these\nissues, we propose a unified COD framework in this paper, termed SAM-COD, which\nis capable of supporting arbitrary weakly-supervised labels. Our SAM-COD\nemploys a prompt adapter to handle scribbles as prompts based on SAM.\nMeanwhile, we introduce response filter and semantic matcher modules to improve\nthe quality of the masks obtained by SAM under COD prompts. To alleviate the\nnegative impacts of inaccurate mask predictions, a new strategy of\nprompt-adaptive knowledge distillation is utilized to ensure a reliable feature\nrepresentation. To validate the effectiveness of our approach, we have\nconducted extensive empirical experiments on three mainstream COD benchmarks.\nThe results demonstrate the superiority of our method against state-of-the-art\nweakly-supervised and even fully-supervised methods.\n", "link": "http://arxiv.org/abs/2408.10760v1", "date": "2024-08-20", "relevancy": 2.116, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5387}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5366}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM-COD%3A%20SAM-guided%20Unified%20Framework%20for%20Weakly-Supervised%20Camouflaged%0A%20%20Object%20Detection&body=Title%3A%20SAM-COD%3A%20SAM-guided%20Unified%20Framework%20for%20Weakly-Supervised%20Camouflaged%0A%20%20Object%20Detection%0AAuthor%3A%20Huafeng%20Chen%20and%20Pengxu%20Wei%20and%20Guangqian%20Guo%20and%20Shan%20Gao%0AAbstract%3A%20%20%20Most%20Camouflaged%20Object%20Detection%20%28COD%29%20methods%20heavily%20rely%20on%20mask%0Aannotations%2C%20which%20are%20time-consuming%20and%20labor-intensive%20to%20acquire.%20Existing%0Aweakly-supervised%20COD%20approaches%20exhibit%20significantly%20inferior%20performance%0Acompared%20to%20fully-supervised%20methods%20and%20struggle%20to%20simultaneously%20support%20all%0Athe%20existing%20types%20of%20camouflaged%20object%20labels%2C%20including%20scribbles%2C%20bounding%0Aboxes%2C%20and%20points.%20Even%20for%20Segment%20Anything%20Model%20%28SAM%29%2C%20it%20is%20still%0Aproblematic%20to%20handle%20the%20weakly-supervised%20COD%20and%20it%20typically%20encounters%0Achallenges%20of%20prompt%20compatibility%20of%20the%20scribble%20labels%2C%20extreme%20response%2C%0Asemantically%20erroneous%20response%2C%20and%20unstable%20feature%20representations%2C%0Aproducing%20unsatisfactory%20results%20in%20camouflaged%20scenes.%20To%20mitigate%20these%0Aissues%2C%20we%20propose%20a%20unified%20COD%20framework%20in%20this%20paper%2C%20termed%20SAM-COD%2C%20which%0Ais%20capable%20of%20supporting%20arbitrary%20weakly-supervised%20labels.%20Our%20SAM-COD%0Aemploys%20a%20prompt%20adapter%20to%20handle%20scribbles%20as%20prompts%20based%20on%20SAM.%0AMeanwhile%2C%20we%20introduce%20response%20filter%20and%20semantic%20matcher%20modules%20to%20improve%0Athe%20quality%20of%20the%20masks%20obtained%20by%20SAM%20under%20COD%20prompts.%20To%20alleviate%20the%0Anegative%20impacts%20of%20inaccurate%20mask%20predictions%2C%20a%20new%20strategy%20of%0Aprompt-adaptive%20knowledge%20distillation%20is%20utilized%20to%20ensure%20a%20reliable%20feature%0Arepresentation.%20To%20validate%20the%20effectiveness%20of%20our%20approach%2C%20we%20have%0Aconducted%20extensive%20empirical%20experiments%20on%20three%20mainstream%20COD%20benchmarks.%0AThe%20results%20demonstrate%20the%20superiority%20of%20our%20method%20against%20state-of-the-art%0Aweakly-supervised%20and%20even%20fully-supervised%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM-COD%253A%2520SAM-guided%2520Unified%2520Framework%2520for%2520Weakly-Supervised%2520Camouflaged%250A%2520%2520Object%2520Detection%26entry.906535625%3DHuafeng%2520Chen%2520and%2520Pengxu%2520Wei%2520and%2520Guangqian%2520Guo%2520and%2520Shan%2520Gao%26entry.1292438233%3D%2520%2520Most%2520Camouflaged%2520Object%2520Detection%2520%2528COD%2529%2520methods%2520heavily%2520rely%2520on%2520mask%250Aannotations%252C%2520which%2520are%2520time-consuming%2520and%2520labor-intensive%2520to%2520acquire.%2520Existing%250Aweakly-supervised%2520COD%2520approaches%2520exhibit%2520significantly%2520inferior%2520performance%250Acompared%2520to%2520fully-supervised%2520methods%2520and%2520struggle%2520to%2520simultaneously%2520support%2520all%250Athe%2520existing%2520types%2520of%2520camouflaged%2520object%2520labels%252C%2520including%2520scribbles%252C%2520bounding%250Aboxes%252C%2520and%2520points.%2520Even%2520for%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520it%2520is%2520still%250Aproblematic%2520to%2520handle%2520the%2520weakly-supervised%2520COD%2520and%2520it%2520typically%2520encounters%250Achallenges%2520of%2520prompt%2520compatibility%2520of%2520the%2520scribble%2520labels%252C%2520extreme%2520response%252C%250Asemantically%2520erroneous%2520response%252C%2520and%2520unstable%2520feature%2520representations%252C%250Aproducing%2520unsatisfactory%2520results%2520in%2520camouflaged%2520scenes.%2520To%2520mitigate%2520these%250Aissues%252C%2520we%2520propose%2520a%2520unified%2520COD%2520framework%2520in%2520this%2520paper%252C%2520termed%2520SAM-COD%252C%2520which%250Ais%2520capable%2520of%2520supporting%2520arbitrary%2520weakly-supervised%2520labels.%2520Our%2520SAM-COD%250Aemploys%2520a%2520prompt%2520adapter%2520to%2520handle%2520scribbles%2520as%2520prompts%2520based%2520on%2520SAM.%250AMeanwhile%252C%2520we%2520introduce%2520response%2520filter%2520and%2520semantic%2520matcher%2520modules%2520to%2520improve%250Athe%2520quality%2520of%2520the%2520masks%2520obtained%2520by%2520SAM%2520under%2520COD%2520prompts.%2520To%2520alleviate%2520the%250Anegative%2520impacts%2520of%2520inaccurate%2520mask%2520predictions%252C%2520a%2520new%2520strategy%2520of%250Aprompt-adaptive%2520knowledge%2520distillation%2520is%2520utilized%2520to%2520ensure%2520a%2520reliable%2520feature%250Arepresentation.%2520To%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520we%2520have%250Aconducted%2520extensive%2520empirical%2520experiments%2520on%2520three%2520mainstream%2520COD%2520benchmarks.%250AThe%2520results%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520against%2520state-of-the-art%250Aweakly-supervised%2520and%2520even%2520fully-supervised%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM-COD%3A%20SAM-guided%20Unified%20Framework%20for%20Weakly-Supervised%20Camouflaged%0A%20%20Object%20Detection&entry.906535625=Huafeng%20Chen%20and%20Pengxu%20Wei%20and%20Guangqian%20Guo%20and%20Shan%20Gao&entry.1292438233=%20%20Most%20Camouflaged%20Object%20Detection%20%28COD%29%20methods%20heavily%20rely%20on%20mask%0Aannotations%2C%20which%20are%20time-consuming%20and%20labor-intensive%20to%20acquire.%20Existing%0Aweakly-supervised%20COD%20approaches%20exhibit%20significantly%20inferior%20performance%0Acompared%20to%20fully-supervised%20methods%20and%20struggle%20to%20simultaneously%20support%20all%0Athe%20existing%20types%20of%20camouflaged%20object%20labels%2C%20including%20scribbles%2C%20bounding%0Aboxes%2C%20and%20points.%20Even%20for%20Segment%20Anything%20Model%20%28SAM%29%2C%20it%20is%20still%0Aproblematic%20to%20handle%20the%20weakly-supervised%20COD%20and%20it%20typically%20encounters%0Achallenges%20of%20prompt%20compatibility%20of%20the%20scribble%20labels%2C%20extreme%20response%2C%0Asemantically%20erroneous%20response%2C%20and%20unstable%20feature%20representations%2C%0Aproducing%20unsatisfactory%20results%20in%20camouflaged%20scenes.%20To%20mitigate%20these%0Aissues%2C%20we%20propose%20a%20unified%20COD%20framework%20in%20this%20paper%2C%20termed%20SAM-COD%2C%20which%0Ais%20capable%20of%20supporting%20arbitrary%20weakly-supervised%20labels.%20Our%20SAM-COD%0Aemploys%20a%20prompt%20adapter%20to%20handle%20scribbles%20as%20prompts%20based%20on%20SAM.%0AMeanwhile%2C%20we%20introduce%20response%20filter%20and%20semantic%20matcher%20modules%20to%20improve%0Athe%20quality%20of%20the%20masks%20obtained%20by%20SAM%20under%20COD%20prompts.%20To%20alleviate%20the%0Anegative%20impacts%20of%20inaccurate%20mask%20predictions%2C%20a%20new%20strategy%20of%0Aprompt-adaptive%20knowledge%20distillation%20is%20utilized%20to%20ensure%20a%20reliable%20feature%0Arepresentation.%20To%20validate%20the%20effectiveness%20of%20our%20approach%2C%20we%20have%0Aconducted%20extensive%20empirical%20experiments%20on%20three%20mainstream%20COD%20benchmarks.%0AThe%20results%20demonstrate%20the%20superiority%20of%20our%20method%20against%20state-of-the-art%0Aweakly-supervised%20and%20even%20fully-supervised%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10760v1&entry.124074799=Read"},
{"title": "Universal Novelty Detection Through Adaptive Contrastive Learning", "author": "Hossein Mirzaei and Mojtaba Nafez and Mohammad Jafari and Mohammad Bagher Soltani and Mohammad Azizmalayeri and Jafar Habibi and Mohammad Sabokrou and Mohammad Hossein Rohban", "abstract": "  Novelty detection is a critical task for deploying machine learning models in\nthe open world. A crucial property of novelty detection methods is\nuniversality, which can be interpreted as generalization across various\ndistributions of training or test data. More precisely, for novelty detection,\ndistribution shifts may occur in the training set or the test set. Shifts in\nthe training set refer to cases where we train a novelty detector on a new\ndataset and expect strong transferability. Conversely, distribution shifts in\nthe test set indicate the methods' performance when the trained model\nencounters a shifted test sample. We experimentally show that existing methods\nfalter in maintaining universality, which stems from their rigid inductive\nbiases. Motivated by this, we aim for more generalized techniques that have\nmore adaptable inductive biases. In this context, we leverage the fact that\ncontrastive learning provides an efficient framework to easily switch and adapt\nto new inductive biases through the proper choice of augmentations in forming\nthe negative pairs. We propose a novel probabilistic auto-negative pair\ngeneration method AutoAugOOD, along with contrastive learning, to yield a\nuniversal novelty detector method. Our experiments demonstrate the superiority\nof our method under different distribution shifts in various image benchmark\ndatasets. Notably, our method emerges universality in the lens of adaptability\nto different setups of novelty detection, including one-class, unlabeled\nmulti-class, and labeled multi-class settings. Code:\nhttps://github.com/mojtaba-nafez/UNODE\n", "link": "http://arxiv.org/abs/2408.10798v1", "date": "2024-08-20", "relevancy": 2.1103, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5548}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5248}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Novelty%20Detection%20Through%20Adaptive%20Contrastive%20Learning&body=Title%3A%20Universal%20Novelty%20Detection%20Through%20Adaptive%20Contrastive%20Learning%0AAuthor%3A%20Hossein%20Mirzaei%20and%20Mojtaba%20Nafez%20and%20Mohammad%20Jafari%20and%20Mohammad%20Bagher%20Soltani%20and%20Mohammad%20Azizmalayeri%20and%20Jafar%20Habibi%20and%20Mohammad%20Sabokrou%20and%20Mohammad%20Hossein%20Rohban%0AAbstract%3A%20%20%20Novelty%20detection%20is%20a%20critical%20task%20for%20deploying%20machine%20learning%20models%20in%0Athe%20open%20world.%20A%20crucial%20property%20of%20novelty%20detection%20methods%20is%0Auniversality%2C%20which%20can%20be%20interpreted%20as%20generalization%20across%20various%0Adistributions%20of%20training%20or%20test%20data.%20More%20precisely%2C%20for%20novelty%20detection%2C%0Adistribution%20shifts%20may%20occur%20in%20the%20training%20set%20or%20the%20test%20set.%20Shifts%20in%0Athe%20training%20set%20refer%20to%20cases%20where%20we%20train%20a%20novelty%20detector%20on%20a%20new%0Adataset%20and%20expect%20strong%20transferability.%20Conversely%2C%20distribution%20shifts%20in%0Athe%20test%20set%20indicate%20the%20methods%27%20performance%20when%20the%20trained%20model%0Aencounters%20a%20shifted%20test%20sample.%20We%20experimentally%20show%20that%20existing%20methods%0Afalter%20in%20maintaining%20universality%2C%20which%20stems%20from%20their%20rigid%20inductive%0Abiases.%20Motivated%20by%20this%2C%20we%20aim%20for%20more%20generalized%20techniques%20that%20have%0Amore%20adaptable%20inductive%20biases.%20In%20this%20context%2C%20we%20leverage%20the%20fact%20that%0Acontrastive%20learning%20provides%20an%20efficient%20framework%20to%20easily%20switch%20and%20adapt%0Ato%20new%20inductive%20biases%20through%20the%20proper%20choice%20of%20augmentations%20in%20forming%0Athe%20negative%20pairs.%20We%20propose%20a%20novel%20probabilistic%20auto-negative%20pair%0Ageneration%20method%20AutoAugOOD%2C%20along%20with%20contrastive%20learning%2C%20to%20yield%20a%0Auniversal%20novelty%20detector%20method.%20Our%20experiments%20demonstrate%20the%20superiority%0Aof%20our%20method%20under%20different%20distribution%20shifts%20in%20various%20image%20benchmark%0Adatasets.%20Notably%2C%20our%20method%20emerges%20universality%20in%20the%20lens%20of%20adaptability%0Ato%20different%20setups%20of%20novelty%20detection%2C%20including%20one-class%2C%20unlabeled%0Amulti-class%2C%20and%20labeled%20multi-class%20settings.%20Code%3A%0Ahttps%3A//github.com/mojtaba-nafez/UNODE%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Novelty%2520Detection%2520Through%2520Adaptive%2520Contrastive%2520Learning%26entry.906535625%3DHossein%2520Mirzaei%2520and%2520Mojtaba%2520Nafez%2520and%2520Mohammad%2520Jafari%2520and%2520Mohammad%2520Bagher%2520Soltani%2520and%2520Mohammad%2520Azizmalayeri%2520and%2520Jafar%2520Habibi%2520and%2520Mohammad%2520Sabokrou%2520and%2520Mohammad%2520Hossein%2520Rohban%26entry.1292438233%3D%2520%2520Novelty%2520detection%2520is%2520a%2520critical%2520task%2520for%2520deploying%2520machine%2520learning%2520models%2520in%250Athe%2520open%2520world.%2520A%2520crucial%2520property%2520of%2520novelty%2520detection%2520methods%2520is%250Auniversality%252C%2520which%2520can%2520be%2520interpreted%2520as%2520generalization%2520across%2520various%250Adistributions%2520of%2520training%2520or%2520test%2520data.%2520More%2520precisely%252C%2520for%2520novelty%2520detection%252C%250Adistribution%2520shifts%2520may%2520occur%2520in%2520the%2520training%2520set%2520or%2520the%2520test%2520set.%2520Shifts%2520in%250Athe%2520training%2520set%2520refer%2520to%2520cases%2520where%2520we%2520train%2520a%2520novelty%2520detector%2520on%2520a%2520new%250Adataset%2520and%2520expect%2520strong%2520transferability.%2520Conversely%252C%2520distribution%2520shifts%2520in%250Athe%2520test%2520set%2520indicate%2520the%2520methods%2527%2520performance%2520when%2520the%2520trained%2520model%250Aencounters%2520a%2520shifted%2520test%2520sample.%2520We%2520experimentally%2520show%2520that%2520existing%2520methods%250Afalter%2520in%2520maintaining%2520universality%252C%2520which%2520stems%2520from%2520their%2520rigid%2520inductive%250Abiases.%2520Motivated%2520by%2520this%252C%2520we%2520aim%2520for%2520more%2520generalized%2520techniques%2520that%2520have%250Amore%2520adaptable%2520inductive%2520biases.%2520In%2520this%2520context%252C%2520we%2520leverage%2520the%2520fact%2520that%250Acontrastive%2520learning%2520provides%2520an%2520efficient%2520framework%2520to%2520easily%2520switch%2520and%2520adapt%250Ato%2520new%2520inductive%2520biases%2520through%2520the%2520proper%2520choice%2520of%2520augmentations%2520in%2520forming%250Athe%2520negative%2520pairs.%2520We%2520propose%2520a%2520novel%2520probabilistic%2520auto-negative%2520pair%250Ageneration%2520method%2520AutoAugOOD%252C%2520along%2520with%2520contrastive%2520learning%252C%2520to%2520yield%2520a%250Auniversal%2520novelty%2520detector%2520method.%2520Our%2520experiments%2520demonstrate%2520the%2520superiority%250Aof%2520our%2520method%2520under%2520different%2520distribution%2520shifts%2520in%2520various%2520image%2520benchmark%250Adatasets.%2520Notably%252C%2520our%2520method%2520emerges%2520universality%2520in%2520the%2520lens%2520of%2520adaptability%250Ato%2520different%2520setups%2520of%2520novelty%2520detection%252C%2520including%2520one-class%252C%2520unlabeled%250Amulti-class%252C%2520and%2520labeled%2520multi-class%2520settings.%2520Code%253A%250Ahttps%253A//github.com/mojtaba-nafez/UNODE%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Novelty%20Detection%20Through%20Adaptive%20Contrastive%20Learning&entry.906535625=Hossein%20Mirzaei%20and%20Mojtaba%20Nafez%20and%20Mohammad%20Jafari%20and%20Mohammad%20Bagher%20Soltani%20and%20Mohammad%20Azizmalayeri%20and%20Jafar%20Habibi%20and%20Mohammad%20Sabokrou%20and%20Mohammad%20Hossein%20Rohban&entry.1292438233=%20%20Novelty%20detection%20is%20a%20critical%20task%20for%20deploying%20machine%20learning%20models%20in%0Athe%20open%20world.%20A%20crucial%20property%20of%20novelty%20detection%20methods%20is%0Auniversality%2C%20which%20can%20be%20interpreted%20as%20generalization%20across%20various%0Adistributions%20of%20training%20or%20test%20data.%20More%20precisely%2C%20for%20novelty%20detection%2C%0Adistribution%20shifts%20may%20occur%20in%20the%20training%20set%20or%20the%20test%20set.%20Shifts%20in%0Athe%20training%20set%20refer%20to%20cases%20where%20we%20train%20a%20novelty%20detector%20on%20a%20new%0Adataset%20and%20expect%20strong%20transferability.%20Conversely%2C%20distribution%20shifts%20in%0Athe%20test%20set%20indicate%20the%20methods%27%20performance%20when%20the%20trained%20model%0Aencounters%20a%20shifted%20test%20sample.%20We%20experimentally%20show%20that%20existing%20methods%0Afalter%20in%20maintaining%20universality%2C%20which%20stems%20from%20their%20rigid%20inductive%0Abiases.%20Motivated%20by%20this%2C%20we%20aim%20for%20more%20generalized%20techniques%20that%20have%0Amore%20adaptable%20inductive%20biases.%20In%20this%20context%2C%20we%20leverage%20the%20fact%20that%0Acontrastive%20learning%20provides%20an%20efficient%20framework%20to%20easily%20switch%20and%20adapt%0Ato%20new%20inductive%20biases%20through%20the%20proper%20choice%20of%20augmentations%20in%20forming%0Athe%20negative%20pairs.%20We%20propose%20a%20novel%20probabilistic%20auto-negative%20pair%0Ageneration%20method%20AutoAugOOD%2C%20along%20with%20contrastive%20learning%2C%20to%20yield%20a%0Auniversal%20novelty%20detector%20method.%20Our%20experiments%20demonstrate%20the%20superiority%0Aof%20our%20method%20under%20different%20distribution%20shifts%20in%20various%20image%20benchmark%0Adatasets.%20Notably%2C%20our%20method%20emerges%20universality%20in%20the%20lens%20of%20adaptability%0Ato%20different%20setups%20of%20novelty%20detection%2C%20including%20one-class%2C%20unlabeled%0Amulti-class%2C%20and%20labeled%20multi-class%20settings.%20Code%3A%0Ahttps%3A//github.com/mojtaba-nafez/UNODE%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10798v1&entry.124074799=Read"},
{"title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning", "author": "Bei Ouyang and Shengyuan Ye and Liekang Zeng and Tianyi Qian and Jingyi Li and Xu Chen", "abstract": "  Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.\n", "link": "http://arxiv.org/abs/2408.10746v1", "date": "2024-08-20", "relevancy": 2.1065, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5343}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5238}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pluto%20and%20Charon%3A%20A%20Time%20and%20Memory%20Efficient%20Collaborative%20Edge%20AI%0A%20%20Framework%20for%20Personal%20LLMs%20Fine-Tuning&body=Title%3A%20Pluto%20and%20Charon%3A%20A%20Time%20and%20Memory%20Efficient%20Collaborative%20Edge%20AI%0A%20%20Framework%20for%20Personal%20LLMs%20Fine-Tuning%0AAuthor%3A%20Bei%20Ouyang%20and%20Shengyuan%20Ye%20and%20Liekang%20Zeng%20and%20Tianyi%20Qian%20and%20Jingyi%20Li%20and%20Xu%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20unlocked%20a%20plethora%20of%20powerful%0Aapplications%20at%20the%20network%20edge%2C%20such%20as%20intelligent%20personal%20assistants.%20Data%0Aprivacy%20and%20security%20concerns%20have%20prompted%20a%20shift%20towards%20edge-based%0Afine-tuning%20of%20personal%20LLMs%2C%20away%20from%20cloud%20reliance.%20However%2C%20this%20raises%0Aissues%20of%20computational%20intensity%20and%20resource%20scarcity%2C%20hindering%20training%0Aefficiency%20and%20feasibility.%20While%20current%20studies%20investigate%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20techniques%20to%20mitigate%20resource%0Aconstraints%2C%20our%20analysis%20indicates%20that%20these%20techniques%20are%20not%20sufficiently%0Aresource-efficient%20for%20edge%20devices.%20To%20tackle%20these%20challenges%2C%20we%20propose%0APluto%20and%20Charon%20%28PAC%29%2C%20a%20time%20and%20memory%20efficient%20collaborative%20edge%20AI%0Aframework%20for%20personal%20LLMs%20fine-tuning.%20PAC%20breaks%20the%20resource%20wall%20of%0Apersonal%20LLMs%20fine-tuning%20with%20a%20sophisticated%20algorithm-system%20co-design.%20%281%29%0AAlgorithmically%2C%20PAC%20implements%20a%20personal%20LLMs%20fine-tuning%20technique%20that%20is%0Aefficient%20in%20terms%20of%20parameters%2C%20time%2C%20and%20memory.%20It%20utilizes%20Parallel%0AAdapters%20to%20circumvent%20the%20need%20for%20a%20full%20backward%20pass%20through%20the%20LLM%0Abackbone.%20Additionally%2C%20an%20activation%20cache%20mechanism%20further%20streamlining%20the%0Aprocess%20by%20negating%20the%20necessity%20for%20repeated%20forward%20passes%20across%20multiple%0Aepochs.%20%282%29%20Systematically%2C%20PAC%20leverages%20edge%20devices%20in%20close%20proximity%2C%0Apooling%20them%20as%20a%20collective%20resource%20for%20in-situ%20personal%20LLMs%20fine-tuning%2C%0Autilizing%20a%20hybrid%20data%20and%20pipeline%20parallelism%20to%20orchestrate%20distributed%0Atraining.%20The%20use%20of%20the%20activation%20cache%20eliminates%20the%20need%20for%20forward%20pass%0Athrough%20the%20LLM%20backbone%2Cenabling%20exclusive%20fine-tuning%20of%20the%20Parallel%0AAdapters%20using%20data%20parallelism.%20Extensive%20evaluation%20based%20on%20prototype%0Aimplementation%20demonstrates%20that%20PAC%20remarkably%20outperforms%20state-of-the-art%0Aapproaches%2C%20achieving%20up%20to%208.64x%20end-to-end%20speedup%20and%20up%20to%2088.16%25%20reduction%0Ain%20memory%20footprint.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPluto%2520and%2520Charon%253A%2520A%2520Time%2520and%2520Memory%2520Efficient%2520Collaborative%2520Edge%2520AI%250A%2520%2520Framework%2520for%2520Personal%2520LLMs%2520Fine-Tuning%26entry.906535625%3DBei%2520Ouyang%2520and%2520Shengyuan%2520Ye%2520and%2520Liekang%2520Zeng%2520and%2520Tianyi%2520Qian%2520and%2520Jingyi%2520Li%2520and%2520Xu%2520Chen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520unlocked%2520a%2520plethora%2520of%2520powerful%250Aapplications%2520at%2520the%2520network%2520edge%252C%2520such%2520as%2520intelligent%2520personal%2520assistants.%2520Data%250Aprivacy%2520and%2520security%2520concerns%2520have%2520prompted%2520a%2520shift%2520towards%2520edge-based%250Afine-tuning%2520of%2520personal%2520LLMs%252C%2520away%2520from%2520cloud%2520reliance.%2520However%252C%2520this%2520raises%250Aissues%2520of%2520computational%2520intensity%2520and%2520resource%2520scarcity%252C%2520hindering%2520training%250Aefficiency%2520and%2520feasibility.%2520While%2520current%2520studies%2520investigate%250Aparameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520techniques%2520to%2520mitigate%2520resource%250Aconstraints%252C%2520our%2520analysis%2520indicates%2520that%2520these%2520techniques%2520are%2520not%2520sufficiently%250Aresource-efficient%2520for%2520edge%2520devices.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%250APluto%2520and%2520Charon%2520%2528PAC%2529%252C%2520a%2520time%2520and%2520memory%2520efficient%2520collaborative%2520edge%2520AI%250Aframework%2520for%2520personal%2520LLMs%2520fine-tuning.%2520PAC%2520breaks%2520the%2520resource%2520wall%2520of%250Apersonal%2520LLMs%2520fine-tuning%2520with%2520a%2520sophisticated%2520algorithm-system%2520co-design.%2520%25281%2529%250AAlgorithmically%252C%2520PAC%2520implements%2520a%2520personal%2520LLMs%2520fine-tuning%2520technique%2520that%2520is%250Aefficient%2520in%2520terms%2520of%2520parameters%252C%2520time%252C%2520and%2520memory.%2520It%2520utilizes%2520Parallel%250AAdapters%2520to%2520circumvent%2520the%2520need%2520for%2520a%2520full%2520backward%2520pass%2520through%2520the%2520LLM%250Abackbone.%2520Additionally%252C%2520an%2520activation%2520cache%2520mechanism%2520further%2520streamlining%2520the%250Aprocess%2520by%2520negating%2520the%2520necessity%2520for%2520repeated%2520forward%2520passes%2520across%2520multiple%250Aepochs.%2520%25282%2529%2520Systematically%252C%2520PAC%2520leverages%2520edge%2520devices%2520in%2520close%2520proximity%252C%250Apooling%2520them%2520as%2520a%2520collective%2520resource%2520for%2520in-situ%2520personal%2520LLMs%2520fine-tuning%252C%250Autilizing%2520a%2520hybrid%2520data%2520and%2520pipeline%2520parallelism%2520to%2520orchestrate%2520distributed%250Atraining.%2520The%2520use%2520of%2520the%2520activation%2520cache%2520eliminates%2520the%2520need%2520for%2520forward%2520pass%250Athrough%2520the%2520LLM%2520backbone%252Cenabling%2520exclusive%2520fine-tuning%2520of%2520the%2520Parallel%250AAdapters%2520using%2520data%2520parallelism.%2520Extensive%2520evaluation%2520based%2520on%2520prototype%250Aimplementation%2520demonstrates%2520that%2520PAC%2520remarkably%2520outperforms%2520state-of-the-art%250Aapproaches%252C%2520achieving%2520up%2520to%25208.64x%2520end-to-end%2520speedup%2520and%2520up%2520to%252088.16%2525%2520reduction%250Ain%2520memory%2520footprint.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pluto%20and%20Charon%3A%20A%20Time%20and%20Memory%20Efficient%20Collaborative%20Edge%20AI%0A%20%20Framework%20for%20Personal%20LLMs%20Fine-Tuning&entry.906535625=Bei%20Ouyang%20and%20Shengyuan%20Ye%20and%20Liekang%20Zeng%20and%20Tianyi%20Qian%20and%20Jingyi%20Li%20and%20Xu%20Chen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20unlocked%20a%20plethora%20of%20powerful%0Aapplications%20at%20the%20network%20edge%2C%20such%20as%20intelligent%20personal%20assistants.%20Data%0Aprivacy%20and%20security%20concerns%20have%20prompted%20a%20shift%20towards%20edge-based%0Afine-tuning%20of%20personal%20LLMs%2C%20away%20from%20cloud%20reliance.%20However%2C%20this%20raises%0Aissues%20of%20computational%20intensity%20and%20resource%20scarcity%2C%20hindering%20training%0Aefficiency%20and%20feasibility.%20While%20current%20studies%20investigate%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20techniques%20to%20mitigate%20resource%0Aconstraints%2C%20our%20analysis%20indicates%20that%20these%20techniques%20are%20not%20sufficiently%0Aresource-efficient%20for%20edge%20devices.%20To%20tackle%20these%20challenges%2C%20we%20propose%0APluto%20and%20Charon%20%28PAC%29%2C%20a%20time%20and%20memory%20efficient%20collaborative%20edge%20AI%0Aframework%20for%20personal%20LLMs%20fine-tuning.%20PAC%20breaks%20the%20resource%20wall%20of%0Apersonal%20LLMs%20fine-tuning%20with%20a%20sophisticated%20algorithm-system%20co-design.%20%281%29%0AAlgorithmically%2C%20PAC%20implements%20a%20personal%20LLMs%20fine-tuning%20technique%20that%20is%0Aefficient%20in%20terms%20of%20parameters%2C%20time%2C%20and%20memory.%20It%20utilizes%20Parallel%0AAdapters%20to%20circumvent%20the%20need%20for%20a%20full%20backward%20pass%20through%20the%20LLM%0Abackbone.%20Additionally%2C%20an%20activation%20cache%20mechanism%20further%20streamlining%20the%0Aprocess%20by%20negating%20the%20necessity%20for%20repeated%20forward%20passes%20across%20multiple%0Aepochs.%20%282%29%20Systematically%2C%20PAC%20leverages%20edge%20devices%20in%20close%20proximity%2C%0Apooling%20them%20as%20a%20collective%20resource%20for%20in-situ%20personal%20LLMs%20fine-tuning%2C%0Autilizing%20a%20hybrid%20data%20and%20pipeline%20parallelism%20to%20orchestrate%20distributed%0Atraining.%20The%20use%20of%20the%20activation%20cache%20eliminates%20the%20need%20for%20forward%20pass%0Athrough%20the%20LLM%20backbone%2Cenabling%20exclusive%20fine-tuning%20of%20the%20Parallel%0AAdapters%20using%20data%20parallelism.%20Extensive%20evaluation%20based%20on%20prototype%0Aimplementation%20demonstrates%20that%20PAC%20remarkably%20outperforms%20state-of-the-art%0Aapproaches%2C%20achieving%20up%20to%208.64x%20end-to-end%20speedup%20and%20up%20to%2088.16%25%20reduction%0Ain%20memory%20footprint.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10746v1&entry.124074799=Read"},
{"title": "Enhancing End-to-End Autonomous Driving Systems Through Synchronized\n  Human Behavior Data", "author": "Yiqun Duan and Zhuoli Zhuang and Jinzhao Zhou and Yu-Cheng Chang and Yu-Kai Wang and Chin-Teng Lin", "abstract": "  This paper presents a pioneering exploration into the integration of\nfine-grained human supervision within the autonomous driving domain to enhance\nsystem performance. The current advances in End-to-End autonomous driving\nnormally are data-driven and rely on given expert trials. However, this\nreliance limits the systems' generalizability and their ability to earn human\ntrust. Addressing this gap, our research introduces a novel approach by\nsynchronously collecting data from human and machine drivers under identical\ndriving scenarios, focusing on eye-tracking and brainwave data to guide machine\nperception and decision-making processes. This paper utilizes the Carla\nsimulation to evaluate the impact brought by human behavior guidance.\nExperimental results show that using human attention to guide machine attention\ncould bring a significant improvement in driving performance. However, guidance\nby human intention still remains a challenge. This paper pioneers a promising\ndirection and potential for utilizing human behavior guidance to enhance\nautonomous systems.\n", "link": "http://arxiv.org/abs/2408.10908v1", "date": "2024-08-20", "relevancy": 2.1062, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5462}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5253}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20End-to-End%20Autonomous%20Driving%20Systems%20Through%20Synchronized%0A%20%20Human%20Behavior%20Data&body=Title%3A%20Enhancing%20End-to-End%20Autonomous%20Driving%20Systems%20Through%20Synchronized%0A%20%20Human%20Behavior%20Data%0AAuthor%3A%20Yiqun%20Duan%20and%20Zhuoli%20Zhuang%20and%20Jinzhao%20Zhou%20and%20Yu-Cheng%20Chang%20and%20Yu-Kai%20Wang%20and%20Chin-Teng%20Lin%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20pioneering%20exploration%20into%20the%20integration%20of%0Afine-grained%20human%20supervision%20within%20the%20autonomous%20driving%20domain%20to%20enhance%0Asystem%20performance.%20The%20current%20advances%20in%20End-to-End%20autonomous%20driving%0Anormally%20are%20data-driven%20and%20rely%20on%20given%20expert%20trials.%20However%2C%20this%0Areliance%20limits%20the%20systems%27%20generalizability%20and%20their%20ability%20to%20earn%20human%0Atrust.%20Addressing%20this%20gap%2C%20our%20research%20introduces%20a%20novel%20approach%20by%0Asynchronously%20collecting%20data%20from%20human%20and%20machine%20drivers%20under%20identical%0Adriving%20scenarios%2C%20focusing%20on%20eye-tracking%20and%20brainwave%20data%20to%20guide%20machine%0Aperception%20and%20decision-making%20processes.%20This%20paper%20utilizes%20the%20Carla%0Asimulation%20to%20evaluate%20the%20impact%20brought%20by%20human%20behavior%20guidance.%0AExperimental%20results%20show%20that%20using%20human%20attention%20to%20guide%20machine%20attention%0Acould%20bring%20a%20significant%20improvement%20in%20driving%20performance.%20However%2C%20guidance%0Aby%20human%20intention%20still%20remains%20a%20challenge.%20This%20paper%20pioneers%20a%20promising%0Adirection%20and%20potential%20for%20utilizing%20human%20behavior%20guidance%20to%20enhance%0Aautonomous%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520End-to-End%2520Autonomous%2520Driving%2520Systems%2520Through%2520Synchronized%250A%2520%2520Human%2520Behavior%2520Data%26entry.906535625%3DYiqun%2520Duan%2520and%2520Zhuoli%2520Zhuang%2520and%2520Jinzhao%2520Zhou%2520and%2520Yu-Cheng%2520Chang%2520and%2520Yu-Kai%2520Wang%2520and%2520Chin-Teng%2520Lin%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520pioneering%2520exploration%2520into%2520the%2520integration%2520of%250Afine-grained%2520human%2520supervision%2520within%2520the%2520autonomous%2520driving%2520domain%2520to%2520enhance%250Asystem%2520performance.%2520The%2520current%2520advances%2520in%2520End-to-End%2520autonomous%2520driving%250Anormally%2520are%2520data-driven%2520and%2520rely%2520on%2520given%2520expert%2520trials.%2520However%252C%2520this%250Areliance%2520limits%2520the%2520systems%2527%2520generalizability%2520and%2520their%2520ability%2520to%2520earn%2520human%250Atrust.%2520Addressing%2520this%2520gap%252C%2520our%2520research%2520introduces%2520a%2520novel%2520approach%2520by%250Asynchronously%2520collecting%2520data%2520from%2520human%2520and%2520machine%2520drivers%2520under%2520identical%250Adriving%2520scenarios%252C%2520focusing%2520on%2520eye-tracking%2520and%2520brainwave%2520data%2520to%2520guide%2520machine%250Aperception%2520and%2520decision-making%2520processes.%2520This%2520paper%2520utilizes%2520the%2520Carla%250Asimulation%2520to%2520evaluate%2520the%2520impact%2520brought%2520by%2520human%2520behavior%2520guidance.%250AExperimental%2520results%2520show%2520that%2520using%2520human%2520attention%2520to%2520guide%2520machine%2520attention%250Acould%2520bring%2520a%2520significant%2520improvement%2520in%2520driving%2520performance.%2520However%252C%2520guidance%250Aby%2520human%2520intention%2520still%2520remains%2520a%2520challenge.%2520This%2520paper%2520pioneers%2520a%2520promising%250Adirection%2520and%2520potential%2520for%2520utilizing%2520human%2520behavior%2520guidance%2520to%2520enhance%250Aautonomous%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20End-to-End%20Autonomous%20Driving%20Systems%20Through%20Synchronized%0A%20%20Human%20Behavior%20Data&entry.906535625=Yiqun%20Duan%20and%20Zhuoli%20Zhuang%20and%20Jinzhao%20Zhou%20and%20Yu-Cheng%20Chang%20and%20Yu-Kai%20Wang%20and%20Chin-Teng%20Lin&entry.1292438233=%20%20This%20paper%20presents%20a%20pioneering%20exploration%20into%20the%20integration%20of%0Afine-grained%20human%20supervision%20within%20the%20autonomous%20driving%20domain%20to%20enhance%0Asystem%20performance.%20The%20current%20advances%20in%20End-to-End%20autonomous%20driving%0Anormally%20are%20data-driven%20and%20rely%20on%20given%20expert%20trials.%20However%2C%20this%0Areliance%20limits%20the%20systems%27%20generalizability%20and%20their%20ability%20to%20earn%20human%0Atrust.%20Addressing%20this%20gap%2C%20our%20research%20introduces%20a%20novel%20approach%20by%0Asynchronously%20collecting%20data%20from%20human%20and%20machine%20drivers%20under%20identical%0Adriving%20scenarios%2C%20focusing%20on%20eye-tracking%20and%20brainwave%20data%20to%20guide%20machine%0Aperception%20and%20decision-making%20processes.%20This%20paper%20utilizes%20the%20Carla%0Asimulation%20to%20evaluate%20the%20impact%20brought%20by%20human%20behavior%20guidance.%0AExperimental%20results%20show%20that%20using%20human%20attention%20to%20guide%20machine%20attention%0Acould%20bring%20a%20significant%20improvement%20in%20driving%20performance.%20However%2C%20guidance%0Aby%20human%20intention%20still%20remains%20a%20challenge.%20This%20paper%20pioneers%20a%20promising%0Adirection%20and%20potential%20for%20utilizing%20human%20behavior%20guidance%20to%20enhance%0Aautonomous%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10908v1&entry.124074799=Read"},
{"title": "Efficient and Robust Quantization-aware Training via Adaptive Coreset\n  Selection", "author": "Xijie Huang and Zechun Liu and Shih-Yang Liu and Kwang-Ting Cheng", "abstract": "  Quantization-aware training (QAT) is a representative model compression\nmethod to reduce redundancy in weights and activations. However, most existing\nQAT methods require end-to-end training on the entire dataset, which suffers\nfrom long training time and high energy costs. In addition, the potential label\nnoise in the training data undermines the robustness of QAT. We propose two\nmetrics based on analysis of loss and gradient of quantized weights: error\nvector score and disagreement score, to quantify the importance of each sample\nduring training. Guided by these two metrics, we proposed a quantization-aware\nAdaptive Coreset Selection (ACS) method to select the data for the current\ntraining epoch. We evaluate our method on various networks (ResNet-18,\nMobileNetV2, RetinaNet), datasets(CIFAR-10, CIFAR-100, ImageNet-1K, COCO), and\nunder different quantization settings. Specifically, our method can achieve an\naccuracy of 68.39\\% of 4-bit quantized ResNet-18 on the ImageNet-1K dataset\nwith only a 10\\% subset, which has an absolute gain of 4.24\\% compared to the\nbaseline. Our method can also improve the robustness of QAT by removing noisy\nsamples in the training set.\n", "link": "http://arxiv.org/abs/2306.07215v3", "date": "2024-08-20", "relevancy": 2.1008, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5599}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5019}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Robust%20Quantization-aware%20Training%20via%20Adaptive%20Coreset%0A%20%20Selection&body=Title%3A%20Efficient%20and%20Robust%20Quantization-aware%20Training%20via%20Adaptive%20Coreset%0A%20%20Selection%0AAuthor%3A%20Xijie%20Huang%20and%20Zechun%20Liu%20and%20Shih-Yang%20Liu%20and%20Kwang-Ting%20Cheng%0AAbstract%3A%20%20%20Quantization-aware%20training%20%28QAT%29%20is%20a%20representative%20model%20compression%0Amethod%20to%20reduce%20redundancy%20in%20weights%20and%20activations.%20However%2C%20most%20existing%0AQAT%20methods%20require%20end-to-end%20training%20on%20the%20entire%20dataset%2C%20which%20suffers%0Afrom%20long%20training%20time%20and%20high%20energy%20costs.%20In%20addition%2C%20the%20potential%20label%0Anoise%20in%20the%20training%20data%20undermines%20the%20robustness%20of%20QAT.%20We%20propose%20two%0Ametrics%20based%20on%20analysis%20of%20loss%20and%20gradient%20of%20quantized%20weights%3A%20error%0Avector%20score%20and%20disagreement%20score%2C%20to%20quantify%20the%20importance%20of%20each%20sample%0Aduring%20training.%20Guided%20by%20these%20two%20metrics%2C%20we%20proposed%20a%20quantization-aware%0AAdaptive%20Coreset%20Selection%20%28ACS%29%20method%20to%20select%20the%20data%20for%20the%20current%0Atraining%20epoch.%20We%20evaluate%20our%20method%20on%20various%20networks%20%28ResNet-18%2C%0AMobileNetV2%2C%20RetinaNet%29%2C%20datasets%28CIFAR-10%2C%20CIFAR-100%2C%20ImageNet-1K%2C%20COCO%29%2C%20and%0Aunder%20different%20quantization%20settings.%20Specifically%2C%20our%20method%20can%20achieve%20an%0Aaccuracy%20of%2068.39%5C%25%20of%204-bit%20quantized%20ResNet-18%20on%20the%20ImageNet-1K%20dataset%0Awith%20only%20a%2010%5C%25%20subset%2C%20which%20has%20an%20absolute%20gain%20of%204.24%5C%25%20compared%20to%20the%0Abaseline.%20Our%20method%20can%20also%20improve%20the%20robustness%20of%20QAT%20by%20removing%20noisy%0Asamples%20in%20the%20training%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.07215v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Robust%2520Quantization-aware%2520Training%2520via%2520Adaptive%2520Coreset%250A%2520%2520Selection%26entry.906535625%3DXijie%2520Huang%2520and%2520Zechun%2520Liu%2520and%2520Shih-Yang%2520Liu%2520and%2520Kwang-Ting%2520Cheng%26entry.1292438233%3D%2520%2520Quantization-aware%2520training%2520%2528QAT%2529%2520is%2520a%2520representative%2520model%2520compression%250Amethod%2520to%2520reduce%2520redundancy%2520in%2520weights%2520and%2520activations.%2520However%252C%2520most%2520existing%250AQAT%2520methods%2520require%2520end-to-end%2520training%2520on%2520the%2520entire%2520dataset%252C%2520which%2520suffers%250Afrom%2520long%2520training%2520time%2520and%2520high%2520energy%2520costs.%2520In%2520addition%252C%2520the%2520potential%2520label%250Anoise%2520in%2520the%2520training%2520data%2520undermines%2520the%2520robustness%2520of%2520QAT.%2520We%2520propose%2520two%250Ametrics%2520based%2520on%2520analysis%2520of%2520loss%2520and%2520gradient%2520of%2520quantized%2520weights%253A%2520error%250Avector%2520score%2520and%2520disagreement%2520score%252C%2520to%2520quantify%2520the%2520importance%2520of%2520each%2520sample%250Aduring%2520training.%2520Guided%2520by%2520these%2520two%2520metrics%252C%2520we%2520proposed%2520a%2520quantization-aware%250AAdaptive%2520Coreset%2520Selection%2520%2528ACS%2529%2520method%2520to%2520select%2520the%2520data%2520for%2520the%2520current%250Atraining%2520epoch.%2520We%2520evaluate%2520our%2520method%2520on%2520various%2520networks%2520%2528ResNet-18%252C%250AMobileNetV2%252C%2520RetinaNet%2529%252C%2520datasets%2528CIFAR-10%252C%2520CIFAR-100%252C%2520ImageNet-1K%252C%2520COCO%2529%252C%2520and%250Aunder%2520different%2520quantization%2520settings.%2520Specifically%252C%2520our%2520method%2520can%2520achieve%2520an%250Aaccuracy%2520of%252068.39%255C%2525%2520of%25204-bit%2520quantized%2520ResNet-18%2520on%2520the%2520ImageNet-1K%2520dataset%250Awith%2520only%2520a%252010%255C%2525%2520subset%252C%2520which%2520has%2520an%2520absolute%2520gain%2520of%25204.24%255C%2525%2520compared%2520to%2520the%250Abaseline.%2520Our%2520method%2520can%2520also%2520improve%2520the%2520robustness%2520of%2520QAT%2520by%2520removing%2520noisy%250Asamples%2520in%2520the%2520training%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.07215v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Robust%20Quantization-aware%20Training%20via%20Adaptive%20Coreset%0A%20%20Selection&entry.906535625=Xijie%20Huang%20and%20Zechun%20Liu%20and%20Shih-Yang%20Liu%20and%20Kwang-Ting%20Cheng&entry.1292438233=%20%20Quantization-aware%20training%20%28QAT%29%20is%20a%20representative%20model%20compression%0Amethod%20to%20reduce%20redundancy%20in%20weights%20and%20activations.%20However%2C%20most%20existing%0AQAT%20methods%20require%20end-to-end%20training%20on%20the%20entire%20dataset%2C%20which%20suffers%0Afrom%20long%20training%20time%20and%20high%20energy%20costs.%20In%20addition%2C%20the%20potential%20label%0Anoise%20in%20the%20training%20data%20undermines%20the%20robustness%20of%20QAT.%20We%20propose%20two%0Ametrics%20based%20on%20analysis%20of%20loss%20and%20gradient%20of%20quantized%20weights%3A%20error%0Avector%20score%20and%20disagreement%20score%2C%20to%20quantify%20the%20importance%20of%20each%20sample%0Aduring%20training.%20Guided%20by%20these%20two%20metrics%2C%20we%20proposed%20a%20quantization-aware%0AAdaptive%20Coreset%20Selection%20%28ACS%29%20method%20to%20select%20the%20data%20for%20the%20current%0Atraining%20epoch.%20We%20evaluate%20our%20method%20on%20various%20networks%20%28ResNet-18%2C%0AMobileNetV2%2C%20RetinaNet%29%2C%20datasets%28CIFAR-10%2C%20CIFAR-100%2C%20ImageNet-1K%2C%20COCO%29%2C%20and%0Aunder%20different%20quantization%20settings.%20Specifically%2C%20our%20method%20can%20achieve%20an%0Aaccuracy%20of%2068.39%5C%25%20of%204-bit%20quantized%20ResNet-18%20on%20the%20ImageNet-1K%20dataset%0Awith%20only%20a%2010%5C%25%20subset%2C%20which%20has%20an%20absolute%20gain%20of%204.24%5C%25%20compared%20to%20the%0Abaseline.%20Our%20method%20can%20also%20improve%20the%20robustness%20of%20QAT%20by%20removing%20noisy%0Asamples%20in%20the%20training%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.07215v3&entry.124074799=Read"},
{"title": "Audio Match Cutting: Finding and Creating Matching Audio Transitions in\n  Movies and Videos", "author": "Dennis Fedorishin and Lie Lu and Srirangaraj Setlur and Venu Govindaraju", "abstract": "  A \"match cut\" is a common video editing technique where a pair of shots that\nhave a similar composition transition fluidly from one to another. Although\nmatch cuts are often visual, certain match cuts involve the fluid transition of\naudio, where sounds from different sources merge into one indistinguishable\ntransition between two shots. In this paper, we explore the ability to\nautomatically find and create \"audio match cuts\" within videos and movies. We\ncreate a self-supervised audio representation for audio match cutting and\ndevelop a coarse-to-fine audio match pipeline that recommends matching shots\nand creates the blended audio. We further annotate a dataset for the proposed\naudio match cut task and compare the ability of multiple audio representations\nto find audio match cut candidates. Finally, we evaluate multiple methods to\nblend two matching audio candidates with the goal of creating a smooth\ntransition. Project page and examples are available at:\nhttps://denfed.github.io/audiomatchcut/\n", "link": "http://arxiv.org/abs/2408.10998v1", "date": "2024-08-20", "relevancy": 2.0998, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5739}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5204}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio%20Match%20Cutting%3A%20Finding%20and%20Creating%20Matching%20Audio%20Transitions%20in%0A%20%20Movies%20and%20Videos&body=Title%3A%20Audio%20Match%20Cutting%3A%20Finding%20and%20Creating%20Matching%20Audio%20Transitions%20in%0A%20%20Movies%20and%20Videos%0AAuthor%3A%20Dennis%20Fedorishin%20and%20Lie%20Lu%20and%20Srirangaraj%20Setlur%20and%20Venu%20Govindaraju%0AAbstract%3A%20%20%20A%20%22match%20cut%22%20is%20a%20common%20video%20editing%20technique%20where%20a%20pair%20of%20shots%20that%0Ahave%20a%20similar%20composition%20transition%20fluidly%20from%20one%20to%20another.%20Although%0Amatch%20cuts%20are%20often%20visual%2C%20certain%20match%20cuts%20involve%20the%20fluid%20transition%20of%0Aaudio%2C%20where%20sounds%20from%20different%20sources%20merge%20into%20one%20indistinguishable%0Atransition%20between%20two%20shots.%20In%20this%20paper%2C%20we%20explore%20the%20ability%20to%0Aautomatically%20find%20and%20create%20%22audio%20match%20cuts%22%20within%20videos%20and%20movies.%20We%0Acreate%20a%20self-supervised%20audio%20representation%20for%20audio%20match%20cutting%20and%0Adevelop%20a%20coarse-to-fine%20audio%20match%20pipeline%20that%20recommends%20matching%20shots%0Aand%20creates%20the%20blended%20audio.%20We%20further%20annotate%20a%20dataset%20for%20the%20proposed%0Aaudio%20match%20cut%20task%20and%20compare%20the%20ability%20of%20multiple%20audio%20representations%0Ato%20find%20audio%20match%20cut%20candidates.%20Finally%2C%20we%20evaluate%20multiple%20methods%20to%0Ablend%20two%20matching%20audio%20candidates%20with%20the%20goal%20of%20creating%20a%20smooth%0Atransition.%20Project%20page%20and%20examples%20are%20available%20at%3A%0Ahttps%3A//denfed.github.io/audiomatchcut/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio%2520Match%2520Cutting%253A%2520Finding%2520and%2520Creating%2520Matching%2520Audio%2520Transitions%2520in%250A%2520%2520Movies%2520and%2520Videos%26entry.906535625%3DDennis%2520Fedorishin%2520and%2520Lie%2520Lu%2520and%2520Srirangaraj%2520Setlur%2520and%2520Venu%2520Govindaraju%26entry.1292438233%3D%2520%2520A%2520%2522match%2520cut%2522%2520is%2520a%2520common%2520video%2520editing%2520technique%2520where%2520a%2520pair%2520of%2520shots%2520that%250Ahave%2520a%2520similar%2520composition%2520transition%2520fluidly%2520from%2520one%2520to%2520another.%2520Although%250Amatch%2520cuts%2520are%2520often%2520visual%252C%2520certain%2520match%2520cuts%2520involve%2520the%2520fluid%2520transition%2520of%250Aaudio%252C%2520where%2520sounds%2520from%2520different%2520sources%2520merge%2520into%2520one%2520indistinguishable%250Atransition%2520between%2520two%2520shots.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520ability%2520to%250Aautomatically%2520find%2520and%2520create%2520%2522audio%2520match%2520cuts%2522%2520within%2520videos%2520and%2520movies.%2520We%250Acreate%2520a%2520self-supervised%2520audio%2520representation%2520for%2520audio%2520match%2520cutting%2520and%250Adevelop%2520a%2520coarse-to-fine%2520audio%2520match%2520pipeline%2520that%2520recommends%2520matching%2520shots%250Aand%2520creates%2520the%2520blended%2520audio.%2520We%2520further%2520annotate%2520a%2520dataset%2520for%2520the%2520proposed%250Aaudio%2520match%2520cut%2520task%2520and%2520compare%2520the%2520ability%2520of%2520multiple%2520audio%2520representations%250Ato%2520find%2520audio%2520match%2520cut%2520candidates.%2520Finally%252C%2520we%2520evaluate%2520multiple%2520methods%2520to%250Ablend%2520two%2520matching%2520audio%2520candidates%2520with%2520the%2520goal%2520of%2520creating%2520a%2520smooth%250Atransition.%2520Project%2520page%2520and%2520examples%2520are%2520available%2520at%253A%250Ahttps%253A//denfed.github.io/audiomatchcut/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio%20Match%20Cutting%3A%20Finding%20and%20Creating%20Matching%20Audio%20Transitions%20in%0A%20%20Movies%20and%20Videos&entry.906535625=Dennis%20Fedorishin%20and%20Lie%20Lu%20and%20Srirangaraj%20Setlur%20and%20Venu%20Govindaraju&entry.1292438233=%20%20A%20%22match%20cut%22%20is%20a%20common%20video%20editing%20technique%20where%20a%20pair%20of%20shots%20that%0Ahave%20a%20similar%20composition%20transition%20fluidly%20from%20one%20to%20another.%20Although%0Amatch%20cuts%20are%20often%20visual%2C%20certain%20match%20cuts%20involve%20the%20fluid%20transition%20of%0Aaudio%2C%20where%20sounds%20from%20different%20sources%20merge%20into%20one%20indistinguishable%0Atransition%20between%20two%20shots.%20In%20this%20paper%2C%20we%20explore%20the%20ability%20to%0Aautomatically%20find%20and%20create%20%22audio%20match%20cuts%22%20within%20videos%20and%20movies.%20We%0Acreate%20a%20self-supervised%20audio%20representation%20for%20audio%20match%20cutting%20and%0Adevelop%20a%20coarse-to-fine%20audio%20match%20pipeline%20that%20recommends%20matching%20shots%0Aand%20creates%20the%20blended%20audio.%20We%20further%20annotate%20a%20dataset%20for%20the%20proposed%0Aaudio%20match%20cut%20task%20and%20compare%20the%20ability%20of%20multiple%20audio%20representations%0Ato%20find%20audio%20match%20cut%20candidates.%20Finally%2C%20we%20evaluate%20multiple%20methods%20to%0Ablend%20two%20matching%20audio%20candidates%20with%20the%20goal%20of%20creating%20a%20smooth%0Atransition.%20Project%20page%20and%20examples%20are%20available%20at%3A%0Ahttps%3A//denfed.github.io/audiomatchcut/%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10998v1&entry.124074799=Read"},
{"title": "A Closer Look at Data Augmentation Strategies for Finetuning-Based\n  Low/Few-Shot Object Detection", "author": "Vladislav Li and Georgios Tsoumplekas and Ilias Siniosoglou and Vasileios Argyriou and Anastasios Lytos and Eleftherios Fountoukidis and Panagiotis Sarigiannidis", "abstract": "  Current methods for low- and few-shot object detection have primarily focused\non enhancing model performance for detecting objects. One common approach to\nachieve this is by combining model finetuning with data augmentation\nstrategies. However, little attention has been given to the energy efficiency\nof these approaches in data-scarce regimes. This paper seeks to conduct a\ncomprehensive empirical study that examines both model performance and energy\nefficiency of custom data augmentations and automated data augmentation\nselection strategies when combined with a lightweight object detector. The\nmethods are evaluated in three different benchmark datasets in terms of their\nperformance and energy consumption, and the Efficiency Factor is employed to\ngain insights into their effectiveness considering both performance and\nefficiency. Consequently, it is shown that in many cases, the performance gains\nof data augmentation strategies are overshadowed by their increased energy\nusage, necessitating the development of more energy efficient data augmentation\nstrategies to address data scarcity.\n", "link": "http://arxiv.org/abs/2408.10940v1", "date": "2024-08-20", "relevancy": 2.0977, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.559}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5017}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Closer%20Look%20at%20Data%20Augmentation%20Strategies%20for%20Finetuning-Based%0A%20%20Low/Few-Shot%20Object%20Detection&body=Title%3A%20A%20Closer%20Look%20at%20Data%20Augmentation%20Strategies%20for%20Finetuning-Based%0A%20%20Low/Few-Shot%20Object%20Detection%0AAuthor%3A%20Vladislav%20Li%20and%20Georgios%20Tsoumplekas%20and%20Ilias%20Siniosoglou%20and%20Vasileios%20Argyriou%20and%20Anastasios%20Lytos%20and%20Eleftherios%20Fountoukidis%20and%20Panagiotis%20Sarigiannidis%0AAbstract%3A%20%20%20Current%20methods%20for%20low-%20and%20few-shot%20object%20detection%20have%20primarily%20focused%0Aon%20enhancing%20model%20performance%20for%20detecting%20objects.%20One%20common%20approach%20to%0Aachieve%20this%20is%20by%20combining%20model%20finetuning%20with%20data%20augmentation%0Astrategies.%20However%2C%20little%20attention%20has%20been%20given%20to%20the%20energy%20efficiency%0Aof%20these%20approaches%20in%20data-scarce%20regimes.%20This%20paper%20seeks%20to%20conduct%20a%0Acomprehensive%20empirical%20study%20that%20examines%20both%20model%20performance%20and%20energy%0Aefficiency%20of%20custom%20data%20augmentations%20and%20automated%20data%20augmentation%0Aselection%20strategies%20when%20combined%20with%20a%20lightweight%20object%20detector.%20The%0Amethods%20are%20evaluated%20in%20three%20different%20benchmark%20datasets%20in%20terms%20of%20their%0Aperformance%20and%20energy%20consumption%2C%20and%20the%20Efficiency%20Factor%20is%20employed%20to%0Again%20insights%20into%20their%20effectiveness%20considering%20both%20performance%20and%0Aefficiency.%20Consequently%2C%20it%20is%20shown%20that%20in%20many%20cases%2C%20the%20performance%20gains%0Aof%20data%20augmentation%20strategies%20are%20overshadowed%20by%20their%20increased%20energy%0Ausage%2C%20necessitating%20the%20development%20of%20more%20energy%20efficient%20data%20augmentation%0Astrategies%20to%20address%20data%20scarcity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Closer%2520Look%2520at%2520Data%2520Augmentation%2520Strategies%2520for%2520Finetuning-Based%250A%2520%2520Low/Few-Shot%2520Object%2520Detection%26entry.906535625%3DVladislav%2520Li%2520and%2520Georgios%2520Tsoumplekas%2520and%2520Ilias%2520Siniosoglou%2520and%2520Vasileios%2520Argyriou%2520and%2520Anastasios%2520Lytos%2520and%2520Eleftherios%2520Fountoukidis%2520and%2520Panagiotis%2520Sarigiannidis%26entry.1292438233%3D%2520%2520Current%2520methods%2520for%2520low-%2520and%2520few-shot%2520object%2520detection%2520have%2520primarily%2520focused%250Aon%2520enhancing%2520model%2520performance%2520for%2520detecting%2520objects.%2520One%2520common%2520approach%2520to%250Aachieve%2520this%2520is%2520by%2520combining%2520model%2520finetuning%2520with%2520data%2520augmentation%250Astrategies.%2520However%252C%2520little%2520attention%2520has%2520been%2520given%2520to%2520the%2520energy%2520efficiency%250Aof%2520these%2520approaches%2520in%2520data-scarce%2520regimes.%2520This%2520paper%2520seeks%2520to%2520conduct%2520a%250Acomprehensive%2520empirical%2520study%2520that%2520examines%2520both%2520model%2520performance%2520and%2520energy%250Aefficiency%2520of%2520custom%2520data%2520augmentations%2520and%2520automated%2520data%2520augmentation%250Aselection%2520strategies%2520when%2520combined%2520with%2520a%2520lightweight%2520object%2520detector.%2520The%250Amethods%2520are%2520evaluated%2520in%2520three%2520different%2520benchmark%2520datasets%2520in%2520terms%2520of%2520their%250Aperformance%2520and%2520energy%2520consumption%252C%2520and%2520the%2520Efficiency%2520Factor%2520is%2520employed%2520to%250Again%2520insights%2520into%2520their%2520effectiveness%2520considering%2520both%2520performance%2520and%250Aefficiency.%2520Consequently%252C%2520it%2520is%2520shown%2520that%2520in%2520many%2520cases%252C%2520the%2520performance%2520gains%250Aof%2520data%2520augmentation%2520strategies%2520are%2520overshadowed%2520by%2520their%2520increased%2520energy%250Ausage%252C%2520necessitating%2520the%2520development%2520of%2520more%2520energy%2520efficient%2520data%2520augmentation%250Astrategies%2520to%2520address%2520data%2520scarcity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Closer%20Look%20at%20Data%20Augmentation%20Strategies%20for%20Finetuning-Based%0A%20%20Low/Few-Shot%20Object%20Detection&entry.906535625=Vladislav%20Li%20and%20Georgios%20Tsoumplekas%20and%20Ilias%20Siniosoglou%20and%20Vasileios%20Argyriou%20and%20Anastasios%20Lytos%20and%20Eleftherios%20Fountoukidis%20and%20Panagiotis%20Sarigiannidis&entry.1292438233=%20%20Current%20methods%20for%20low-%20and%20few-shot%20object%20detection%20have%20primarily%20focused%0Aon%20enhancing%20model%20performance%20for%20detecting%20objects.%20One%20common%20approach%20to%0Aachieve%20this%20is%20by%20combining%20model%20finetuning%20with%20data%20augmentation%0Astrategies.%20However%2C%20little%20attention%20has%20been%20given%20to%20the%20energy%20efficiency%0Aof%20these%20approaches%20in%20data-scarce%20regimes.%20This%20paper%20seeks%20to%20conduct%20a%0Acomprehensive%20empirical%20study%20that%20examines%20both%20model%20performance%20and%20energy%0Aefficiency%20of%20custom%20data%20augmentations%20and%20automated%20data%20augmentation%0Aselection%20strategies%20when%20combined%20with%20a%20lightweight%20object%20detector.%20The%0Amethods%20are%20evaluated%20in%20three%20different%20benchmark%20datasets%20in%20terms%20of%20their%0Aperformance%20and%20energy%20consumption%2C%20and%20the%20Efficiency%20Factor%20is%20employed%20to%0Again%20insights%20into%20their%20effectiveness%20considering%20both%20performance%20and%0Aefficiency.%20Consequently%2C%20it%20is%20shown%20that%20in%20many%20cases%2C%20the%20performance%20gains%0Aof%20data%20augmentation%20strategies%20are%20overshadowed%20by%20their%20increased%20energy%0Ausage%2C%20necessitating%20the%20development%20of%20more%20energy%20efficient%20data%20augmentation%0Astrategies%20to%20address%20data%20scarcity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10940v1&entry.124074799=Read"},
{"title": "Vision-Language Dataset Distillation", "author": "Xindi Wu and Byron Zhang and Zhiwei Deng and Olga Russakovsky", "abstract": "  Dataset distillation methods reduce large-scale datasets to smaller sets of\nsynthetic data, preserving sufficient information to quickly train a new model\nfrom scratch. However, prior work on dataset distillation has focused\nexclusively on image classification datasets, whereas modern large-scale\ndatasets are primarily vision-language datasets. In this work, we design the\nfirst vision-language dataset distillation method, building on the idea of\ntrajectory matching. A key challenge is that vision-language datasets do not\nhave a set of discrete classes. To overcome this, our proposed method jointly\ndistills image-text pairs in a contrastive formulation. Further, we leverage\nLow-Rank Adaptation (LoRA) matching to enable more efficient and effective\ntrajectory matching in complex modern vision-language models. Since there are\nno existing baselines, we compare our distillation approach with three adapted\nvision-language coreset selection methods. We demonstrate significant\nimprovements on the challenging Flickr30K and COCO retrieval benchmarks: for\nexample, on Flickr30K, the best coreset selection method selecting 1000\nimage-text pairs for training achieves only 5.6% image-to-text retrieval\naccuracy (i.e., recall@1); in contrast, our dataset distillation almost doubles\nthat to 9.9% with just 100 training pairs, an order of magnitude fewer.\n", "link": "http://arxiv.org/abs/2308.07545v4", "date": "2024-08-20", "relevancy": 2.0803, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5217}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5191}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Dataset%20Distillation&body=Title%3A%20Vision-Language%20Dataset%20Distillation%0AAuthor%3A%20Xindi%20Wu%20and%20Byron%20Zhang%20and%20Zhiwei%20Deng%20and%20Olga%20Russakovsky%0AAbstract%3A%20%20%20Dataset%20distillation%20methods%20reduce%20large-scale%20datasets%20to%20smaller%20sets%20of%0Asynthetic%20data%2C%20preserving%20sufficient%20information%20to%20quickly%20train%20a%20new%20model%0Afrom%20scratch.%20However%2C%20prior%20work%20on%20dataset%20distillation%20has%20focused%0Aexclusively%20on%20image%20classification%20datasets%2C%20whereas%20modern%20large-scale%0Adatasets%20are%20primarily%20vision-language%20datasets.%20In%20this%20work%2C%20we%20design%20the%0Afirst%20vision-language%20dataset%20distillation%20method%2C%20building%20on%20the%20idea%20of%0Atrajectory%20matching.%20A%20key%20challenge%20is%20that%20vision-language%20datasets%20do%20not%0Ahave%20a%20set%20of%20discrete%20classes.%20To%20overcome%20this%2C%20our%20proposed%20method%20jointly%0Adistills%20image-text%20pairs%20in%20a%20contrastive%20formulation.%20Further%2C%20we%20leverage%0ALow-Rank%20Adaptation%20%28LoRA%29%20matching%20to%20enable%20more%20efficient%20and%20effective%0Atrajectory%20matching%20in%20complex%20modern%20vision-language%20models.%20Since%20there%20are%0Ano%20existing%20baselines%2C%20we%20compare%20our%20distillation%20approach%20with%20three%20adapted%0Avision-language%20coreset%20selection%20methods.%20We%20demonstrate%20significant%0Aimprovements%20on%20the%20challenging%20Flickr30K%20and%20COCO%20retrieval%20benchmarks%3A%20for%0Aexample%2C%20on%20Flickr30K%2C%20the%20best%20coreset%20selection%20method%20selecting%201000%0Aimage-text%20pairs%20for%20training%20achieves%20only%205.6%25%20image-to-text%20retrieval%0Aaccuracy%20%28i.e.%2C%20recall%401%29%3B%20in%20contrast%2C%20our%20dataset%20distillation%20almost%20doubles%0Athat%20to%209.9%25%20with%20just%20100%20training%20pairs%2C%20an%20order%20of%20magnitude%20fewer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.07545v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Dataset%2520Distillation%26entry.906535625%3DXindi%2520Wu%2520and%2520Byron%2520Zhang%2520and%2520Zhiwei%2520Deng%2520and%2520Olga%2520Russakovsky%26entry.1292438233%3D%2520%2520Dataset%2520distillation%2520methods%2520reduce%2520large-scale%2520datasets%2520to%2520smaller%2520sets%2520of%250Asynthetic%2520data%252C%2520preserving%2520sufficient%2520information%2520to%2520quickly%2520train%2520a%2520new%2520model%250Afrom%2520scratch.%2520However%252C%2520prior%2520work%2520on%2520dataset%2520distillation%2520has%2520focused%250Aexclusively%2520on%2520image%2520classification%2520datasets%252C%2520whereas%2520modern%2520large-scale%250Adatasets%2520are%2520primarily%2520vision-language%2520datasets.%2520In%2520this%2520work%252C%2520we%2520design%2520the%250Afirst%2520vision-language%2520dataset%2520distillation%2520method%252C%2520building%2520on%2520the%2520idea%2520of%250Atrajectory%2520matching.%2520A%2520key%2520challenge%2520is%2520that%2520vision-language%2520datasets%2520do%2520not%250Ahave%2520a%2520set%2520of%2520discrete%2520classes.%2520To%2520overcome%2520this%252C%2520our%2520proposed%2520method%2520jointly%250Adistills%2520image-text%2520pairs%2520in%2520a%2520contrastive%2520formulation.%2520Further%252C%2520we%2520leverage%250ALow-Rank%2520Adaptation%2520%2528LoRA%2529%2520matching%2520to%2520enable%2520more%2520efficient%2520and%2520effective%250Atrajectory%2520matching%2520in%2520complex%2520modern%2520vision-language%2520models.%2520Since%2520there%2520are%250Ano%2520existing%2520baselines%252C%2520we%2520compare%2520our%2520distillation%2520approach%2520with%2520three%2520adapted%250Avision-language%2520coreset%2520selection%2520methods.%2520We%2520demonstrate%2520significant%250Aimprovements%2520on%2520the%2520challenging%2520Flickr30K%2520and%2520COCO%2520retrieval%2520benchmarks%253A%2520for%250Aexample%252C%2520on%2520Flickr30K%252C%2520the%2520best%2520coreset%2520selection%2520method%2520selecting%25201000%250Aimage-text%2520pairs%2520for%2520training%2520achieves%2520only%25205.6%2525%2520image-to-text%2520retrieval%250Aaccuracy%2520%2528i.e.%252C%2520recall%25401%2529%253B%2520in%2520contrast%252C%2520our%2520dataset%2520distillation%2520almost%2520doubles%250Athat%2520to%25209.9%2525%2520with%2520just%2520100%2520training%2520pairs%252C%2520an%2520order%2520of%2520magnitude%2520fewer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.07545v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Dataset%20Distillation&entry.906535625=Xindi%20Wu%20and%20Byron%20Zhang%20and%20Zhiwei%20Deng%20and%20Olga%20Russakovsky&entry.1292438233=%20%20Dataset%20distillation%20methods%20reduce%20large-scale%20datasets%20to%20smaller%20sets%20of%0Asynthetic%20data%2C%20preserving%20sufficient%20information%20to%20quickly%20train%20a%20new%20model%0Afrom%20scratch.%20However%2C%20prior%20work%20on%20dataset%20distillation%20has%20focused%0Aexclusively%20on%20image%20classification%20datasets%2C%20whereas%20modern%20large-scale%0Adatasets%20are%20primarily%20vision-language%20datasets.%20In%20this%20work%2C%20we%20design%20the%0Afirst%20vision-language%20dataset%20distillation%20method%2C%20building%20on%20the%20idea%20of%0Atrajectory%20matching.%20A%20key%20challenge%20is%20that%20vision-language%20datasets%20do%20not%0Ahave%20a%20set%20of%20discrete%20classes.%20To%20overcome%20this%2C%20our%20proposed%20method%20jointly%0Adistills%20image-text%20pairs%20in%20a%20contrastive%20formulation.%20Further%2C%20we%20leverage%0ALow-Rank%20Adaptation%20%28LoRA%29%20matching%20to%20enable%20more%20efficient%20and%20effective%0Atrajectory%20matching%20in%20complex%20modern%20vision-language%20models.%20Since%20there%20are%0Ano%20existing%20baselines%2C%20we%20compare%20our%20distillation%20approach%20with%20three%20adapted%0Avision-language%20coreset%20selection%20methods.%20We%20demonstrate%20significant%0Aimprovements%20on%20the%20challenging%20Flickr30K%20and%20COCO%20retrieval%20benchmarks%3A%20for%0Aexample%2C%20on%20Flickr30K%2C%20the%20best%20coreset%20selection%20method%20selecting%201000%0Aimage-text%20pairs%20for%20training%20achieves%20only%205.6%25%20image-to-text%20retrieval%0Aaccuracy%20%28i.e.%2C%20recall%401%29%3B%20in%20contrast%2C%20our%20dataset%20distillation%20almost%20doubles%0Athat%20to%209.9%25%20with%20just%20100%20training%20pairs%2C%20an%20order%20of%20magnitude%20fewer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.07545v4&entry.124074799=Read"},
{"title": "V-RoAst: A New Dataset for Visual Road Assessment", "author": "Natchapon Jongwiriyanurak and Zichao Zeng and June Moh Goo and Xinglei Wang and Ilya Ilyankou and Kerkritt Srirrongvikrai and Meihui Wang and James Haworth", "abstract": "  Road traffic crashes cause millions of deaths annually and have a significant\neconomic impact, particularly in low- and middle-income countries (LMICs). This\npaper presents an approach using Vision Language Models (VLMs) for road safety\nassessment, overcoming the limitations of traditional Convolutional Neural\nNetworks (CNNs). We introduce a new task ,V-RoAst (Visual question answering\nfor Road Assessment), with a real-world dataset. Our approach optimizes prompt\nengineering and evaluates advanced VLMs, including Gemini-1.5-flash and\nGPT-4o-mini. The models effectively examine attributes for road assessment.\nUsing crowdsourced imagery from Mapillary, our scalable solution influentially\nestimates road safety levels. In addition, this approach is designed for local\nstakeholders who lack resources, as it does not require training data. It\noffers a cost-effective and automated methods for global road safety\nassessments, potentially saving lives and reducing economic burdens.\n", "link": "http://arxiv.org/abs/2408.10872v1", "date": "2024-08-20", "relevancy": 2.0637, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5398}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-RoAst%3A%20A%20New%20Dataset%20for%20Visual%20Road%20Assessment&body=Title%3A%20V-RoAst%3A%20A%20New%20Dataset%20for%20Visual%20Road%20Assessment%0AAuthor%3A%20Natchapon%20Jongwiriyanurak%20and%20Zichao%20Zeng%20and%20June%20Moh%20Goo%20and%20Xinglei%20Wang%20and%20Ilya%20Ilyankou%20and%20Kerkritt%20Srirrongvikrai%20and%20Meihui%20Wang%20and%20James%20Haworth%0AAbstract%3A%20%20%20Road%20traffic%20crashes%20cause%20millions%20of%20deaths%20annually%20and%20have%20a%20significant%0Aeconomic%20impact%2C%20particularly%20in%20low-%20and%20middle-income%20countries%20%28LMICs%29.%20This%0Apaper%20presents%20an%20approach%20using%20Vision%20Language%20Models%20%28VLMs%29%20for%20road%20safety%0Aassessment%2C%20overcoming%20the%20limitations%20of%20traditional%20Convolutional%20Neural%0ANetworks%20%28CNNs%29.%20We%20introduce%20a%20new%20task%20%2CV-RoAst%20%28Visual%20question%20answering%0Afor%20Road%20Assessment%29%2C%20with%20a%20real-world%20dataset.%20Our%20approach%20optimizes%20prompt%0Aengineering%20and%20evaluates%20advanced%20VLMs%2C%20including%20Gemini-1.5-flash%20and%0AGPT-4o-mini.%20The%20models%20effectively%20examine%20attributes%20for%20road%20assessment.%0AUsing%20crowdsourced%20imagery%20from%20Mapillary%2C%20our%20scalable%20solution%20influentially%0Aestimates%20road%20safety%20levels.%20In%20addition%2C%20this%20approach%20is%20designed%20for%20local%0Astakeholders%20who%20lack%20resources%2C%20as%20it%20does%20not%20require%20training%20data.%20It%0Aoffers%20a%20cost-effective%20and%20automated%20methods%20for%20global%20road%20safety%0Aassessments%2C%20potentially%20saving%20lives%20and%20reducing%20economic%20burdens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-RoAst%253A%2520A%2520New%2520Dataset%2520for%2520Visual%2520Road%2520Assessment%26entry.906535625%3DNatchapon%2520Jongwiriyanurak%2520and%2520Zichao%2520Zeng%2520and%2520June%2520Moh%2520Goo%2520and%2520Xinglei%2520Wang%2520and%2520Ilya%2520Ilyankou%2520and%2520Kerkritt%2520Srirrongvikrai%2520and%2520Meihui%2520Wang%2520and%2520James%2520Haworth%26entry.1292438233%3D%2520%2520Road%2520traffic%2520crashes%2520cause%2520millions%2520of%2520deaths%2520annually%2520and%2520have%2520a%2520significant%250Aeconomic%2520impact%252C%2520particularly%2520in%2520low-%2520and%2520middle-income%2520countries%2520%2528LMICs%2529.%2520This%250Apaper%2520presents%2520an%2520approach%2520using%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520for%2520road%2520safety%250Aassessment%252C%2520overcoming%2520the%2520limitations%2520of%2520traditional%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529.%2520We%2520introduce%2520a%2520new%2520task%2520%252CV-RoAst%2520%2528Visual%2520question%2520answering%250Afor%2520Road%2520Assessment%2529%252C%2520with%2520a%2520real-world%2520dataset.%2520Our%2520approach%2520optimizes%2520prompt%250Aengineering%2520and%2520evaluates%2520advanced%2520VLMs%252C%2520including%2520Gemini-1.5-flash%2520and%250AGPT-4o-mini.%2520The%2520models%2520effectively%2520examine%2520attributes%2520for%2520road%2520assessment.%250AUsing%2520crowdsourced%2520imagery%2520from%2520Mapillary%252C%2520our%2520scalable%2520solution%2520influentially%250Aestimates%2520road%2520safety%2520levels.%2520In%2520addition%252C%2520this%2520approach%2520is%2520designed%2520for%2520local%250Astakeholders%2520who%2520lack%2520resources%252C%2520as%2520it%2520does%2520not%2520require%2520training%2520data.%2520It%250Aoffers%2520a%2520cost-effective%2520and%2520automated%2520methods%2520for%2520global%2520road%2520safety%250Aassessments%252C%2520potentially%2520saving%2520lives%2520and%2520reducing%2520economic%2520burdens.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-RoAst%3A%20A%20New%20Dataset%20for%20Visual%20Road%20Assessment&entry.906535625=Natchapon%20Jongwiriyanurak%20and%20Zichao%20Zeng%20and%20June%20Moh%20Goo%20and%20Xinglei%20Wang%20and%20Ilya%20Ilyankou%20and%20Kerkritt%20Srirrongvikrai%20and%20Meihui%20Wang%20and%20James%20Haworth&entry.1292438233=%20%20Road%20traffic%20crashes%20cause%20millions%20of%20deaths%20annually%20and%20have%20a%20significant%0Aeconomic%20impact%2C%20particularly%20in%20low-%20and%20middle-income%20countries%20%28LMICs%29.%20This%0Apaper%20presents%20an%20approach%20using%20Vision%20Language%20Models%20%28VLMs%29%20for%20road%20safety%0Aassessment%2C%20overcoming%20the%20limitations%20of%20traditional%20Convolutional%20Neural%0ANetworks%20%28CNNs%29.%20We%20introduce%20a%20new%20task%20%2CV-RoAst%20%28Visual%20question%20answering%0Afor%20Road%20Assessment%29%2C%20with%20a%20real-world%20dataset.%20Our%20approach%20optimizes%20prompt%0Aengineering%20and%20evaluates%20advanced%20VLMs%2C%20including%20Gemini-1.5-flash%20and%0AGPT-4o-mini.%20The%20models%20effectively%20examine%20attributes%20for%20road%20assessment.%0AUsing%20crowdsourced%20imagery%20from%20Mapillary%2C%20our%20scalable%20solution%20influentially%0Aestimates%20road%20safety%20levels.%20In%20addition%2C%20this%20approach%20is%20designed%20for%20local%0Astakeholders%20who%20lack%20resources%2C%20as%20it%20does%20not%20require%20training%20data.%20It%0Aoffers%20a%20cost-effective%20and%20automated%20methods%20for%20global%20road%20safety%0Aassessments%2C%20potentially%20saving%20lives%20and%20reducing%20economic%20burdens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10872v1&entry.124074799=Read"},
{"title": "ZebraPose: Zebra Detection and Pose Estimation using only Synthetic Data", "author": "Elia Bonetto and Aamir Ahmad", "abstract": "  Synthetic data is increasingly being used to address the lack of labeled\nimages in uncommon domains for deep learning tasks. A prominent example is 2D\npose estimation of animals, particularly wild species like zebras, for which\ncollecting real-world data is complex and impractical. However, many approaches\nstill require real images, consistency and style constraints, sophisticated\nanimal models, and/or powerful pre-trained networks to bridge the syn-to-real\ngap. Moreover, they often assume that the animal can be reliably detected in\nimages or videos, a hypothesis that often does not hold, e.g. in wildlife\nscenarios or aerial images. To solve this, we use synthetic data generated with\na 3D photorealistic simulator to obtain the first synthetic dataset that can be\nused for both detection and 2D pose estimation of zebras without applying any\nof the aforementioned bridging strategies. Unlike previous works, we\nextensively train and benchmark our detection and 2D pose estimation models on\nmultiple real-world and synthetic datasets using both pre-trained and\nnon-pre-trained backbones. These experiments show how the models trained from\nscratch and only with synthetic data can consistently generalize to real-world\nimages of zebras in both tasks. Moreover, we show it is possible to easily\ngeneralize those same models to 2D pose estimation of horses with a minimal\namount of real-world images to account for the domain transfer. Code, results,\ntrained models; and the synthetic, training, and validation data, including\n104K manually labeled frames, are provided as open-source at\nhttps://zebrapose.is.tue.mpg.de/\n", "link": "http://arxiv.org/abs/2408.10831v1", "date": "2024-08-20", "relevancy": 2.0573, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5446}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4945}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZebraPose%3A%20Zebra%20Detection%20and%20Pose%20Estimation%20using%20only%20Synthetic%20Data&body=Title%3A%20ZebraPose%3A%20Zebra%20Detection%20and%20Pose%20Estimation%20using%20only%20Synthetic%20Data%0AAuthor%3A%20Elia%20Bonetto%20and%20Aamir%20Ahmad%0AAbstract%3A%20%20%20Synthetic%20data%20is%20increasingly%20being%20used%20to%20address%20the%20lack%20of%20labeled%0Aimages%20in%20uncommon%20domains%20for%20deep%20learning%20tasks.%20A%20prominent%20example%20is%202D%0Apose%20estimation%20of%20animals%2C%20particularly%20wild%20species%20like%20zebras%2C%20for%20which%0Acollecting%20real-world%20data%20is%20complex%20and%20impractical.%20However%2C%20many%20approaches%0Astill%20require%20real%20images%2C%20consistency%20and%20style%20constraints%2C%20sophisticated%0Aanimal%20models%2C%20and/or%20powerful%20pre-trained%20networks%20to%20bridge%20the%20syn-to-real%0Agap.%20Moreover%2C%20they%20often%20assume%20that%20the%20animal%20can%20be%20reliably%20detected%20in%0Aimages%20or%20videos%2C%20a%20hypothesis%20that%20often%20does%20not%20hold%2C%20e.g.%20in%20wildlife%0Ascenarios%20or%20aerial%20images.%20To%20solve%20this%2C%20we%20use%20synthetic%20data%20generated%20with%0Aa%203D%20photorealistic%20simulator%20to%20obtain%20the%20first%20synthetic%20dataset%20that%20can%20be%0Aused%20for%20both%20detection%20and%202D%20pose%20estimation%20of%20zebras%20without%20applying%20any%0Aof%20the%20aforementioned%20bridging%20strategies.%20Unlike%20previous%20works%2C%20we%0Aextensively%20train%20and%20benchmark%20our%20detection%20and%202D%20pose%20estimation%20models%20on%0Amultiple%20real-world%20and%20synthetic%20datasets%20using%20both%20pre-trained%20and%0Anon-pre-trained%20backbones.%20These%20experiments%20show%20how%20the%20models%20trained%20from%0Ascratch%20and%20only%20with%20synthetic%20data%20can%20consistently%20generalize%20to%20real-world%0Aimages%20of%20zebras%20in%20both%20tasks.%20Moreover%2C%20we%20show%20it%20is%20possible%20to%20easily%0Ageneralize%20those%20same%20models%20to%202D%20pose%20estimation%20of%20horses%20with%20a%20minimal%0Aamount%20of%20real-world%20images%20to%20account%20for%20the%20domain%20transfer.%20Code%2C%20results%2C%0Atrained%20models%3B%20and%20the%20synthetic%2C%20training%2C%20and%20validation%20data%2C%20including%0A104K%20manually%20labeled%20frames%2C%20are%20provided%20as%20open-source%20at%0Ahttps%3A//zebrapose.is.tue.mpg.de/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZebraPose%253A%2520Zebra%2520Detection%2520and%2520Pose%2520Estimation%2520using%2520only%2520Synthetic%2520Data%26entry.906535625%3DElia%2520Bonetto%2520and%2520Aamir%2520Ahmad%26entry.1292438233%3D%2520%2520Synthetic%2520data%2520is%2520increasingly%2520being%2520used%2520to%2520address%2520the%2520lack%2520of%2520labeled%250Aimages%2520in%2520uncommon%2520domains%2520for%2520deep%2520learning%2520tasks.%2520A%2520prominent%2520example%2520is%25202D%250Apose%2520estimation%2520of%2520animals%252C%2520particularly%2520wild%2520species%2520like%2520zebras%252C%2520for%2520which%250Acollecting%2520real-world%2520data%2520is%2520complex%2520and%2520impractical.%2520However%252C%2520many%2520approaches%250Astill%2520require%2520real%2520images%252C%2520consistency%2520and%2520style%2520constraints%252C%2520sophisticated%250Aanimal%2520models%252C%2520and/or%2520powerful%2520pre-trained%2520networks%2520to%2520bridge%2520the%2520syn-to-real%250Agap.%2520Moreover%252C%2520they%2520often%2520assume%2520that%2520the%2520animal%2520can%2520be%2520reliably%2520detected%2520in%250Aimages%2520or%2520videos%252C%2520a%2520hypothesis%2520that%2520often%2520does%2520not%2520hold%252C%2520e.g.%2520in%2520wildlife%250Ascenarios%2520or%2520aerial%2520images.%2520To%2520solve%2520this%252C%2520we%2520use%2520synthetic%2520data%2520generated%2520with%250Aa%25203D%2520photorealistic%2520simulator%2520to%2520obtain%2520the%2520first%2520synthetic%2520dataset%2520that%2520can%2520be%250Aused%2520for%2520both%2520detection%2520and%25202D%2520pose%2520estimation%2520of%2520zebras%2520without%2520applying%2520any%250Aof%2520the%2520aforementioned%2520bridging%2520strategies.%2520Unlike%2520previous%2520works%252C%2520we%250Aextensively%2520train%2520and%2520benchmark%2520our%2520detection%2520and%25202D%2520pose%2520estimation%2520models%2520on%250Amultiple%2520real-world%2520and%2520synthetic%2520datasets%2520using%2520both%2520pre-trained%2520and%250Anon-pre-trained%2520backbones.%2520These%2520experiments%2520show%2520how%2520the%2520models%2520trained%2520from%250Ascratch%2520and%2520only%2520with%2520synthetic%2520data%2520can%2520consistently%2520generalize%2520to%2520real-world%250Aimages%2520of%2520zebras%2520in%2520both%2520tasks.%2520Moreover%252C%2520we%2520show%2520it%2520is%2520possible%2520to%2520easily%250Ageneralize%2520those%2520same%2520models%2520to%25202D%2520pose%2520estimation%2520of%2520horses%2520with%2520a%2520minimal%250Aamount%2520of%2520real-world%2520images%2520to%2520account%2520for%2520the%2520domain%2520transfer.%2520Code%252C%2520results%252C%250Atrained%2520models%253B%2520and%2520the%2520synthetic%252C%2520training%252C%2520and%2520validation%2520data%252C%2520including%250A104K%2520manually%2520labeled%2520frames%252C%2520are%2520provided%2520as%2520open-source%2520at%250Ahttps%253A//zebrapose.is.tue.mpg.de/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZebraPose%3A%20Zebra%20Detection%20and%20Pose%20Estimation%20using%20only%20Synthetic%20Data&entry.906535625=Elia%20Bonetto%20and%20Aamir%20Ahmad&entry.1292438233=%20%20Synthetic%20data%20is%20increasingly%20being%20used%20to%20address%20the%20lack%20of%20labeled%0Aimages%20in%20uncommon%20domains%20for%20deep%20learning%20tasks.%20A%20prominent%20example%20is%202D%0Apose%20estimation%20of%20animals%2C%20particularly%20wild%20species%20like%20zebras%2C%20for%20which%0Acollecting%20real-world%20data%20is%20complex%20and%20impractical.%20However%2C%20many%20approaches%0Astill%20require%20real%20images%2C%20consistency%20and%20style%20constraints%2C%20sophisticated%0Aanimal%20models%2C%20and/or%20powerful%20pre-trained%20networks%20to%20bridge%20the%20syn-to-real%0Agap.%20Moreover%2C%20they%20often%20assume%20that%20the%20animal%20can%20be%20reliably%20detected%20in%0Aimages%20or%20videos%2C%20a%20hypothesis%20that%20often%20does%20not%20hold%2C%20e.g.%20in%20wildlife%0Ascenarios%20or%20aerial%20images.%20To%20solve%20this%2C%20we%20use%20synthetic%20data%20generated%20with%0Aa%203D%20photorealistic%20simulator%20to%20obtain%20the%20first%20synthetic%20dataset%20that%20can%20be%0Aused%20for%20both%20detection%20and%202D%20pose%20estimation%20of%20zebras%20without%20applying%20any%0Aof%20the%20aforementioned%20bridging%20strategies.%20Unlike%20previous%20works%2C%20we%0Aextensively%20train%20and%20benchmark%20our%20detection%20and%202D%20pose%20estimation%20models%20on%0Amultiple%20real-world%20and%20synthetic%20datasets%20using%20both%20pre-trained%20and%0Anon-pre-trained%20backbones.%20These%20experiments%20show%20how%20the%20models%20trained%20from%0Ascratch%20and%20only%20with%20synthetic%20data%20can%20consistently%20generalize%20to%20real-world%0Aimages%20of%20zebras%20in%20both%20tasks.%20Moreover%2C%20we%20show%20it%20is%20possible%20to%20easily%0Ageneralize%20those%20same%20models%20to%202D%20pose%20estimation%20of%20horses%20with%20a%20minimal%0Aamount%20of%20real-world%20images%20to%20account%20for%20the%20domain%20transfer.%20Code%2C%20results%2C%0Atrained%20models%3B%20and%20the%20synthetic%2C%20training%2C%20and%20validation%20data%2C%20including%0A104K%20manually%20labeled%20frames%2C%20are%20provided%20as%20open-source%20at%0Ahttps%3A//zebrapose.is.tue.mpg.de/%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10831v1&entry.124074799=Read"},
{"title": "Towards reliable real-time trajectory optimization", "author": "Fatemeh Rastgar", "abstract": "  Motion planning is a key aspect of robotics. A common approach to address\nmotion planning problems is trajectory optimization. Trajectory optimization\ncan represent the high-level behaviors of robots through mathematical\nformulations. However, current trajectory optimization approaches have two main\nchallenges. Firstly, their solution heavily depends on the initial guess, and\nthey are prone to get stuck in local minima. Secondly, they face scalability\nlimitations by increasing the number of constraints. This thesis endeavors to\ntackle these challenges by introducing four innovative trajectory optimization\nalgorithms to improve reliability, scalability, and computational efficiency.\nThere are two novel aspects of the proposed algorithms. The first key\ninnovation is remodeling the kinematic constraints and collision avoidance\nconstraints. Another key innovation lies in the design of algorithms that\neffectively utilize parallel computation on GPU accelerators. By using\nreformulated constraints and leveraging the computational power of GPUs, the\nproposed algorithms of this thesis demonstrate significant improvements in\nefficiency and scalability compared to the existing methods. Parallelization\nenables faster computation times, allowing for real-time decision-making in\ndynamic environments. Moreover, the algorithms are designed to adapt to changes\nin the environment, ensuring robust performance. Extensive benchmarking for\neach proposed optimizer validates their efficacy. Overall, this thesis makes a\nsignificant contribution to the field of trajectory optimization algorithms. It\nintroduces innovative solutions that specifically address the challenges faced\nby existing methods. The proposed algorithms pave the way for more efficient\nand robust motion planning solutions in robotics by leveraging parallel\ncomputation and specific mathematical structures.\n", "link": "http://arxiv.org/abs/2408.10731v1", "date": "2024-08-20", "relevancy": 2.0292, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5318}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5082}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20reliable%20real-time%20trajectory%20optimization&body=Title%3A%20Towards%20reliable%20real-time%20trajectory%20optimization%0AAuthor%3A%20Fatemeh%20Rastgar%0AAbstract%3A%20%20%20Motion%20planning%20is%20a%20key%20aspect%20of%20robotics.%20A%20common%20approach%20to%20address%0Amotion%20planning%20problems%20is%20trajectory%20optimization.%20Trajectory%20optimization%0Acan%20represent%20the%20high-level%20behaviors%20of%20robots%20through%20mathematical%0Aformulations.%20However%2C%20current%20trajectory%20optimization%20approaches%20have%20two%20main%0Achallenges.%20Firstly%2C%20their%20solution%20heavily%20depends%20on%20the%20initial%20guess%2C%20and%0Athey%20are%20prone%20to%20get%20stuck%20in%20local%20minima.%20Secondly%2C%20they%20face%20scalability%0Alimitations%20by%20increasing%20the%20number%20of%20constraints.%20This%20thesis%20endeavors%20to%0Atackle%20these%20challenges%20by%20introducing%20four%20innovative%20trajectory%20optimization%0Aalgorithms%20to%20improve%20reliability%2C%20scalability%2C%20and%20computational%20efficiency.%0AThere%20are%20two%20novel%20aspects%20of%20the%20proposed%20algorithms.%20The%20first%20key%0Ainnovation%20is%20remodeling%20the%20kinematic%20constraints%20and%20collision%20avoidance%0Aconstraints.%20Another%20key%20innovation%20lies%20in%20the%20design%20of%20algorithms%20that%0Aeffectively%20utilize%20parallel%20computation%20on%20GPU%20accelerators.%20By%20using%0Areformulated%20constraints%20and%20leveraging%20the%20computational%20power%20of%20GPUs%2C%20the%0Aproposed%20algorithms%20of%20this%20thesis%20demonstrate%20significant%20improvements%20in%0Aefficiency%20and%20scalability%20compared%20to%20the%20existing%20methods.%20Parallelization%0Aenables%20faster%20computation%20times%2C%20allowing%20for%20real-time%20decision-making%20in%0Adynamic%20environments.%20Moreover%2C%20the%20algorithms%20are%20designed%20to%20adapt%20to%20changes%0Ain%20the%20environment%2C%20ensuring%20robust%20performance.%20Extensive%20benchmarking%20for%0Aeach%20proposed%20optimizer%20validates%20their%20efficacy.%20Overall%2C%20this%20thesis%20makes%20a%0Asignificant%20contribution%20to%20the%20field%20of%20trajectory%20optimization%20algorithms.%20It%0Aintroduces%20innovative%20solutions%20that%20specifically%20address%20the%20challenges%20faced%0Aby%20existing%20methods.%20The%20proposed%20algorithms%20pave%20the%20way%20for%20more%20efficient%0Aand%20robust%20motion%20planning%20solutions%20in%20robotics%20by%20leveraging%20parallel%0Acomputation%20and%20specific%20mathematical%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520reliable%2520real-time%2520trajectory%2520optimization%26entry.906535625%3DFatemeh%2520Rastgar%26entry.1292438233%3D%2520%2520Motion%2520planning%2520is%2520a%2520key%2520aspect%2520of%2520robotics.%2520A%2520common%2520approach%2520to%2520address%250Amotion%2520planning%2520problems%2520is%2520trajectory%2520optimization.%2520Trajectory%2520optimization%250Acan%2520represent%2520the%2520high-level%2520behaviors%2520of%2520robots%2520through%2520mathematical%250Aformulations.%2520However%252C%2520current%2520trajectory%2520optimization%2520approaches%2520have%2520two%2520main%250Achallenges.%2520Firstly%252C%2520their%2520solution%2520heavily%2520depends%2520on%2520the%2520initial%2520guess%252C%2520and%250Athey%2520are%2520prone%2520to%2520get%2520stuck%2520in%2520local%2520minima.%2520Secondly%252C%2520they%2520face%2520scalability%250Alimitations%2520by%2520increasing%2520the%2520number%2520of%2520constraints.%2520This%2520thesis%2520endeavors%2520to%250Atackle%2520these%2520challenges%2520by%2520introducing%2520four%2520innovative%2520trajectory%2520optimization%250Aalgorithms%2520to%2520improve%2520reliability%252C%2520scalability%252C%2520and%2520computational%2520efficiency.%250AThere%2520are%2520two%2520novel%2520aspects%2520of%2520the%2520proposed%2520algorithms.%2520The%2520first%2520key%250Ainnovation%2520is%2520remodeling%2520the%2520kinematic%2520constraints%2520and%2520collision%2520avoidance%250Aconstraints.%2520Another%2520key%2520innovation%2520lies%2520in%2520the%2520design%2520of%2520algorithms%2520that%250Aeffectively%2520utilize%2520parallel%2520computation%2520on%2520GPU%2520accelerators.%2520By%2520using%250Areformulated%2520constraints%2520and%2520leveraging%2520the%2520computational%2520power%2520of%2520GPUs%252C%2520the%250Aproposed%2520algorithms%2520of%2520this%2520thesis%2520demonstrate%2520significant%2520improvements%2520in%250Aefficiency%2520and%2520scalability%2520compared%2520to%2520the%2520existing%2520methods.%2520Parallelization%250Aenables%2520faster%2520computation%2520times%252C%2520allowing%2520for%2520real-time%2520decision-making%2520in%250Adynamic%2520environments.%2520Moreover%252C%2520the%2520algorithms%2520are%2520designed%2520to%2520adapt%2520to%2520changes%250Ain%2520the%2520environment%252C%2520ensuring%2520robust%2520performance.%2520Extensive%2520benchmarking%2520for%250Aeach%2520proposed%2520optimizer%2520validates%2520their%2520efficacy.%2520Overall%252C%2520this%2520thesis%2520makes%2520a%250Asignificant%2520contribution%2520to%2520the%2520field%2520of%2520trajectory%2520optimization%2520algorithms.%2520It%250Aintroduces%2520innovative%2520solutions%2520that%2520specifically%2520address%2520the%2520challenges%2520faced%250Aby%2520existing%2520methods.%2520The%2520proposed%2520algorithms%2520pave%2520the%2520way%2520for%2520more%2520efficient%250Aand%2520robust%2520motion%2520planning%2520solutions%2520in%2520robotics%2520by%2520leveraging%2520parallel%250Acomputation%2520and%2520specific%2520mathematical%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20reliable%20real-time%20trajectory%20optimization&entry.906535625=Fatemeh%20Rastgar&entry.1292438233=%20%20Motion%20planning%20is%20a%20key%20aspect%20of%20robotics.%20A%20common%20approach%20to%20address%0Amotion%20planning%20problems%20is%20trajectory%20optimization.%20Trajectory%20optimization%0Acan%20represent%20the%20high-level%20behaviors%20of%20robots%20through%20mathematical%0Aformulations.%20However%2C%20current%20trajectory%20optimization%20approaches%20have%20two%20main%0Achallenges.%20Firstly%2C%20their%20solution%20heavily%20depends%20on%20the%20initial%20guess%2C%20and%0Athey%20are%20prone%20to%20get%20stuck%20in%20local%20minima.%20Secondly%2C%20they%20face%20scalability%0Alimitations%20by%20increasing%20the%20number%20of%20constraints.%20This%20thesis%20endeavors%20to%0Atackle%20these%20challenges%20by%20introducing%20four%20innovative%20trajectory%20optimization%0Aalgorithms%20to%20improve%20reliability%2C%20scalability%2C%20and%20computational%20efficiency.%0AThere%20are%20two%20novel%20aspects%20of%20the%20proposed%20algorithms.%20The%20first%20key%0Ainnovation%20is%20remodeling%20the%20kinematic%20constraints%20and%20collision%20avoidance%0Aconstraints.%20Another%20key%20innovation%20lies%20in%20the%20design%20of%20algorithms%20that%0Aeffectively%20utilize%20parallel%20computation%20on%20GPU%20accelerators.%20By%20using%0Areformulated%20constraints%20and%20leveraging%20the%20computational%20power%20of%20GPUs%2C%20the%0Aproposed%20algorithms%20of%20this%20thesis%20demonstrate%20significant%20improvements%20in%0Aefficiency%20and%20scalability%20compared%20to%20the%20existing%20methods.%20Parallelization%0Aenables%20faster%20computation%20times%2C%20allowing%20for%20real-time%20decision-making%20in%0Adynamic%20environments.%20Moreover%2C%20the%20algorithms%20are%20designed%20to%20adapt%20to%20changes%0Ain%20the%20environment%2C%20ensuring%20robust%20performance.%20Extensive%20benchmarking%20for%0Aeach%20proposed%20optimizer%20validates%20their%20efficacy.%20Overall%2C%20this%20thesis%20makes%20a%0Asignificant%20contribution%20to%20the%20field%20of%20trajectory%20optimization%20algorithms.%20It%0Aintroduces%20innovative%20solutions%20that%20specifically%20address%20the%20challenges%20faced%0Aby%20existing%20methods.%20The%20proposed%20algorithms%20pave%20the%20way%20for%20more%20efficient%0Aand%20robust%20motion%20planning%20solutions%20in%20robotics%20by%20leveraging%20parallel%0Acomputation%20and%20specific%20mathematical%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10731v1&entry.124074799=Read"},
{"title": "Exploiting Large Language Models Capabilities for Question Answer-Driven\n  Knowledge Graph Completion Across Static and Temporal Domains", "author": "Rui Yang and Jiahao Zhu and Jianping Man and Li Fang and Yi Zhou", "abstract": "  Knowledge graph completion (KGC) aims to identify missing triples in a\nknowledge graph (KG). This is typically achieved through tasks such as link\nprediction and instance completion. However, these methods often focus on\neither static knowledge graphs (SKGs) or temporal knowledge graphs (TKGs),\naddressing only within-scope triples. This paper introduces a new generative\ncompletion framework called Generative Subgraph-based KGC (GS-KGC). GS-KGC\nemploys a question-answering format to directly generate target entities,\naddressing the challenge of questions having multiple possible answers. We\npropose a strategy that extracts subgraphs centered on entities and\nrelationships within the KG, from which negative samples and neighborhood\ninformation are separately obtained to address the one-to-many problem. Our\nmethod generates negative samples using known facts to facilitate the discovery\nof new information. Furthermore, we collect and refine neighborhood path data\nof known entities, providing contextual information to enhance reasoning in\nlarge language models (LLMs). Our experiments evaluated the proposed method on\nfour SKGs and two TKGs, achieving state-of-the-art Hits@1 metrics on five\ndatasets. Analysis of the results shows that GS-KGC can discover new triples\nwithin existing KGs and generate new facts beyond the closed KG, effectively\nbridging the gap between closed-world and open-world KGC.\n", "link": "http://arxiv.org/abs/2408.10819v1", "date": "2024-08-20", "relevancy": 2.0258, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.521}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5175}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Large%20Language%20Models%20Capabilities%20for%20Question%20Answer-Driven%0A%20%20Knowledge%20Graph%20Completion%20Across%20Static%20and%20Temporal%20Domains&body=Title%3A%20Exploiting%20Large%20Language%20Models%20Capabilities%20for%20Question%20Answer-Driven%0A%20%20Knowledge%20Graph%20Completion%20Across%20Static%20and%20Temporal%20Domains%0AAuthor%3A%20Rui%20Yang%20and%20Jiahao%20Zhu%20and%20Jianping%20Man%20and%20Li%20Fang%20and%20Yi%20Zhou%0AAbstract%3A%20%20%20Knowledge%20graph%20completion%20%28KGC%29%20aims%20to%20identify%20missing%20triples%20in%20a%0Aknowledge%20graph%20%28KG%29.%20This%20is%20typically%20achieved%20through%20tasks%20such%20as%20link%0Aprediction%20and%20instance%20completion.%20However%2C%20these%20methods%20often%20focus%20on%0Aeither%20static%20knowledge%20graphs%20%28SKGs%29%20or%20temporal%20knowledge%20graphs%20%28TKGs%29%2C%0Aaddressing%20only%20within-scope%20triples.%20This%20paper%20introduces%20a%20new%20generative%0Acompletion%20framework%20called%20Generative%20Subgraph-based%20KGC%20%28GS-KGC%29.%20GS-KGC%0Aemploys%20a%20question-answering%20format%20to%20directly%20generate%20target%20entities%2C%0Aaddressing%20the%20challenge%20of%20questions%20having%20multiple%20possible%20answers.%20We%0Apropose%20a%20strategy%20that%20extracts%20subgraphs%20centered%20on%20entities%20and%0Arelationships%20within%20the%20KG%2C%20from%20which%20negative%20samples%20and%20neighborhood%0Ainformation%20are%20separately%20obtained%20to%20address%20the%20one-to-many%20problem.%20Our%0Amethod%20generates%20negative%20samples%20using%20known%20facts%20to%20facilitate%20the%20discovery%0Aof%20new%20information.%20Furthermore%2C%20we%20collect%20and%20refine%20neighborhood%20path%20data%0Aof%20known%20entities%2C%20providing%20contextual%20information%20to%20enhance%20reasoning%20in%0Alarge%20language%20models%20%28LLMs%29.%20Our%20experiments%20evaluated%20the%20proposed%20method%20on%0Afour%20SKGs%20and%20two%20TKGs%2C%20achieving%20state-of-the-art%20Hits%401%20metrics%20on%20five%0Adatasets.%20Analysis%20of%20the%20results%20shows%20that%20GS-KGC%20can%20discover%20new%20triples%0Awithin%20existing%20KGs%20and%20generate%20new%20facts%20beyond%20the%20closed%20KG%2C%20effectively%0Abridging%20the%20gap%20between%20closed-world%20and%20open-world%20KGC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Large%2520Language%2520Models%2520Capabilities%2520for%2520Question%2520Answer-Driven%250A%2520%2520Knowledge%2520Graph%2520Completion%2520Across%2520Static%2520and%2520Temporal%2520Domains%26entry.906535625%3DRui%2520Yang%2520and%2520Jiahao%2520Zhu%2520and%2520Jianping%2520Man%2520and%2520Li%2520Fang%2520and%2520Yi%2520Zhou%26entry.1292438233%3D%2520%2520Knowledge%2520graph%2520completion%2520%2528KGC%2529%2520aims%2520to%2520identify%2520missing%2520triples%2520in%2520a%250Aknowledge%2520graph%2520%2528KG%2529.%2520This%2520is%2520typically%2520achieved%2520through%2520tasks%2520such%2520as%2520link%250Aprediction%2520and%2520instance%2520completion.%2520However%252C%2520these%2520methods%2520often%2520focus%2520on%250Aeither%2520static%2520knowledge%2520graphs%2520%2528SKGs%2529%2520or%2520temporal%2520knowledge%2520graphs%2520%2528TKGs%2529%252C%250Aaddressing%2520only%2520within-scope%2520triples.%2520This%2520paper%2520introduces%2520a%2520new%2520generative%250Acompletion%2520framework%2520called%2520Generative%2520Subgraph-based%2520KGC%2520%2528GS-KGC%2529.%2520GS-KGC%250Aemploys%2520a%2520question-answering%2520format%2520to%2520directly%2520generate%2520target%2520entities%252C%250Aaddressing%2520the%2520challenge%2520of%2520questions%2520having%2520multiple%2520possible%2520answers.%2520We%250Apropose%2520a%2520strategy%2520that%2520extracts%2520subgraphs%2520centered%2520on%2520entities%2520and%250Arelationships%2520within%2520the%2520KG%252C%2520from%2520which%2520negative%2520samples%2520and%2520neighborhood%250Ainformation%2520are%2520separately%2520obtained%2520to%2520address%2520the%2520one-to-many%2520problem.%2520Our%250Amethod%2520generates%2520negative%2520samples%2520using%2520known%2520facts%2520to%2520facilitate%2520the%2520discovery%250Aof%2520new%2520information.%2520Furthermore%252C%2520we%2520collect%2520and%2520refine%2520neighborhood%2520path%2520data%250Aof%2520known%2520entities%252C%2520providing%2520contextual%2520information%2520to%2520enhance%2520reasoning%2520in%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520Our%2520experiments%2520evaluated%2520the%2520proposed%2520method%2520on%250Afour%2520SKGs%2520and%2520two%2520TKGs%252C%2520achieving%2520state-of-the-art%2520Hits%25401%2520metrics%2520on%2520five%250Adatasets.%2520Analysis%2520of%2520the%2520results%2520shows%2520that%2520GS-KGC%2520can%2520discover%2520new%2520triples%250Awithin%2520existing%2520KGs%2520and%2520generate%2520new%2520facts%2520beyond%2520the%2520closed%2520KG%252C%2520effectively%250Abridging%2520the%2520gap%2520between%2520closed-world%2520and%2520open-world%2520KGC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Large%20Language%20Models%20Capabilities%20for%20Question%20Answer-Driven%0A%20%20Knowledge%20Graph%20Completion%20Across%20Static%20and%20Temporal%20Domains&entry.906535625=Rui%20Yang%20and%20Jiahao%20Zhu%20and%20Jianping%20Man%20and%20Li%20Fang%20and%20Yi%20Zhou&entry.1292438233=%20%20Knowledge%20graph%20completion%20%28KGC%29%20aims%20to%20identify%20missing%20triples%20in%20a%0Aknowledge%20graph%20%28KG%29.%20This%20is%20typically%20achieved%20through%20tasks%20such%20as%20link%0Aprediction%20and%20instance%20completion.%20However%2C%20these%20methods%20often%20focus%20on%0Aeither%20static%20knowledge%20graphs%20%28SKGs%29%20or%20temporal%20knowledge%20graphs%20%28TKGs%29%2C%0Aaddressing%20only%20within-scope%20triples.%20This%20paper%20introduces%20a%20new%20generative%0Acompletion%20framework%20called%20Generative%20Subgraph-based%20KGC%20%28GS-KGC%29.%20GS-KGC%0Aemploys%20a%20question-answering%20format%20to%20directly%20generate%20target%20entities%2C%0Aaddressing%20the%20challenge%20of%20questions%20having%20multiple%20possible%20answers.%20We%0Apropose%20a%20strategy%20that%20extracts%20subgraphs%20centered%20on%20entities%20and%0Arelationships%20within%20the%20KG%2C%20from%20which%20negative%20samples%20and%20neighborhood%0Ainformation%20are%20separately%20obtained%20to%20address%20the%20one-to-many%20problem.%20Our%0Amethod%20generates%20negative%20samples%20using%20known%20facts%20to%20facilitate%20the%20discovery%0Aof%20new%20information.%20Furthermore%2C%20we%20collect%20and%20refine%20neighborhood%20path%20data%0Aof%20known%20entities%2C%20providing%20contextual%20information%20to%20enhance%20reasoning%20in%0Alarge%20language%20models%20%28LLMs%29.%20Our%20experiments%20evaluated%20the%20proposed%20method%20on%0Afour%20SKGs%20and%20two%20TKGs%2C%20achieving%20state-of-the-art%20Hits%401%20metrics%20on%20five%0Adatasets.%20Analysis%20of%20the%20results%20shows%20that%20GS-KGC%20can%20discover%20new%20triples%0Awithin%20existing%20KGs%20and%20generate%20new%20facts%20beyond%20the%20closed%20KG%2C%20effectively%0Abridging%20the%20gap%20between%20closed-world%20and%20open-world%20KGC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10819v1&entry.124074799=Read"},
{"title": "DBHP: Trajectory Imputation in Multi-Agent Sports Using Derivative-Based\n  Hybrid Prediction", "author": "Hanjun Choi and Hyunsung Kim and Minho Lee and Chang-Jo Kim and Jinsung Yoon and Sang-Ki Ko", "abstract": "  Many spatiotemporal domains handle multi-agent trajectory data, but in\nreal-world scenarios, collected trajectory data are often partially missing due\nto various reasons. While existing approaches demonstrate good performance in\ntrajectory imputation, they face challenges in capturing the complex dynamics\nand interactions between agents due to a lack of physical constraints that\ngovern realistic trajectories, leading to suboptimal results. To address this\nissue, the paper proposes a Derivative-Based Hybrid Prediction (DBHP) framework\nthat can effectively impute multiple agents' missing trajectories. First, a\nneural network equipped with Set Transformers produces a naive prediction of\nmissing trajectories while satisfying the permutation-equivariance in terms of\nthe order of input agents. Then, the framework makes alternative predictions\nleveraging velocity and acceleration information and combines all the\npredictions with properly determined weights to provide final imputed\ntrajectories. In this way, our proposed framework not only accurately predicts\nposition, velocity, and acceleration values but also enforces the physical\nrelationship between them, eventually improving both the accuracy and\nnaturalness of the predicted trajectories. Accordingly, the experiment results\nabout imputing player trajectories in team sports show that our framework\nsignificantly outperforms existing imputation baselines.\n", "link": "http://arxiv.org/abs/2408.10878v1", "date": "2024-08-20", "relevancy": 2.0162, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.607}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4847}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DBHP%3A%20Trajectory%20Imputation%20in%20Multi-Agent%20Sports%20Using%20Derivative-Based%0A%20%20Hybrid%20Prediction&body=Title%3A%20DBHP%3A%20Trajectory%20Imputation%20in%20Multi-Agent%20Sports%20Using%20Derivative-Based%0A%20%20Hybrid%20Prediction%0AAuthor%3A%20Hanjun%20Choi%20and%20Hyunsung%20Kim%20and%20Minho%20Lee%20and%20Chang-Jo%20Kim%20and%20Jinsung%20Yoon%20and%20Sang-Ki%20Ko%0AAbstract%3A%20%20%20Many%20spatiotemporal%20domains%20handle%20multi-agent%20trajectory%20data%2C%20but%20in%0Areal-world%20scenarios%2C%20collected%20trajectory%20data%20are%20often%20partially%20missing%20due%0Ato%20various%20reasons.%20While%20existing%20approaches%20demonstrate%20good%20performance%20in%0Atrajectory%20imputation%2C%20they%20face%20challenges%20in%20capturing%20the%20complex%20dynamics%0Aand%20interactions%20between%20agents%20due%20to%20a%20lack%20of%20physical%20constraints%20that%0Agovern%20realistic%20trajectories%2C%20leading%20to%20suboptimal%20results.%20To%20address%20this%0Aissue%2C%20the%20paper%20proposes%20a%20Derivative-Based%20Hybrid%20Prediction%20%28DBHP%29%20framework%0Athat%20can%20effectively%20impute%20multiple%20agents%27%20missing%20trajectories.%20First%2C%20a%0Aneural%20network%20equipped%20with%20Set%20Transformers%20produces%20a%20naive%20prediction%20of%0Amissing%20trajectories%20while%20satisfying%20the%20permutation-equivariance%20in%20terms%20of%0Athe%20order%20of%20input%20agents.%20Then%2C%20the%20framework%20makes%20alternative%20predictions%0Aleveraging%20velocity%20and%20acceleration%20information%20and%20combines%20all%20the%0Apredictions%20with%20properly%20determined%20weights%20to%20provide%20final%20imputed%0Atrajectories.%20In%20this%20way%2C%20our%20proposed%20framework%20not%20only%20accurately%20predicts%0Aposition%2C%20velocity%2C%20and%20acceleration%20values%20but%20also%20enforces%20the%20physical%0Arelationship%20between%20them%2C%20eventually%20improving%20both%20the%20accuracy%20and%0Anaturalness%20of%20the%20predicted%20trajectories.%20Accordingly%2C%20the%20experiment%20results%0Aabout%20imputing%20player%20trajectories%20in%20team%20sports%20show%20that%20our%20framework%0Asignificantly%20outperforms%20existing%20imputation%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDBHP%253A%2520Trajectory%2520Imputation%2520in%2520Multi-Agent%2520Sports%2520Using%2520Derivative-Based%250A%2520%2520Hybrid%2520Prediction%26entry.906535625%3DHanjun%2520Choi%2520and%2520Hyunsung%2520Kim%2520and%2520Minho%2520Lee%2520and%2520Chang-Jo%2520Kim%2520and%2520Jinsung%2520Yoon%2520and%2520Sang-Ki%2520Ko%26entry.1292438233%3D%2520%2520Many%2520spatiotemporal%2520domains%2520handle%2520multi-agent%2520trajectory%2520data%252C%2520but%2520in%250Areal-world%2520scenarios%252C%2520collected%2520trajectory%2520data%2520are%2520often%2520partially%2520missing%2520due%250Ato%2520various%2520reasons.%2520While%2520existing%2520approaches%2520demonstrate%2520good%2520performance%2520in%250Atrajectory%2520imputation%252C%2520they%2520face%2520challenges%2520in%2520capturing%2520the%2520complex%2520dynamics%250Aand%2520interactions%2520between%2520agents%2520due%2520to%2520a%2520lack%2520of%2520physical%2520constraints%2520that%250Agovern%2520realistic%2520trajectories%252C%2520leading%2520to%2520suboptimal%2520results.%2520To%2520address%2520this%250Aissue%252C%2520the%2520paper%2520proposes%2520a%2520Derivative-Based%2520Hybrid%2520Prediction%2520%2528DBHP%2529%2520framework%250Athat%2520can%2520effectively%2520impute%2520multiple%2520agents%2527%2520missing%2520trajectories.%2520First%252C%2520a%250Aneural%2520network%2520equipped%2520with%2520Set%2520Transformers%2520produces%2520a%2520naive%2520prediction%2520of%250Amissing%2520trajectories%2520while%2520satisfying%2520the%2520permutation-equivariance%2520in%2520terms%2520of%250Athe%2520order%2520of%2520input%2520agents.%2520Then%252C%2520the%2520framework%2520makes%2520alternative%2520predictions%250Aleveraging%2520velocity%2520and%2520acceleration%2520information%2520and%2520combines%2520all%2520the%250Apredictions%2520with%2520properly%2520determined%2520weights%2520to%2520provide%2520final%2520imputed%250Atrajectories.%2520In%2520this%2520way%252C%2520our%2520proposed%2520framework%2520not%2520only%2520accurately%2520predicts%250Aposition%252C%2520velocity%252C%2520and%2520acceleration%2520values%2520but%2520also%2520enforces%2520the%2520physical%250Arelationship%2520between%2520them%252C%2520eventually%2520improving%2520both%2520the%2520accuracy%2520and%250Anaturalness%2520of%2520the%2520predicted%2520trajectories.%2520Accordingly%252C%2520the%2520experiment%2520results%250Aabout%2520imputing%2520player%2520trajectories%2520in%2520team%2520sports%2520show%2520that%2520our%2520framework%250Asignificantly%2520outperforms%2520existing%2520imputation%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DBHP%3A%20Trajectory%20Imputation%20in%20Multi-Agent%20Sports%20Using%20Derivative-Based%0A%20%20Hybrid%20Prediction&entry.906535625=Hanjun%20Choi%20and%20Hyunsung%20Kim%20and%20Minho%20Lee%20and%20Chang-Jo%20Kim%20and%20Jinsung%20Yoon%20and%20Sang-Ki%20Ko&entry.1292438233=%20%20Many%20spatiotemporal%20domains%20handle%20multi-agent%20trajectory%20data%2C%20but%20in%0Areal-world%20scenarios%2C%20collected%20trajectory%20data%20are%20often%20partially%20missing%20due%0Ato%20various%20reasons.%20While%20existing%20approaches%20demonstrate%20good%20performance%20in%0Atrajectory%20imputation%2C%20they%20face%20challenges%20in%20capturing%20the%20complex%20dynamics%0Aand%20interactions%20between%20agents%20due%20to%20a%20lack%20of%20physical%20constraints%20that%0Agovern%20realistic%20trajectories%2C%20leading%20to%20suboptimal%20results.%20To%20address%20this%0Aissue%2C%20the%20paper%20proposes%20a%20Derivative-Based%20Hybrid%20Prediction%20%28DBHP%29%20framework%0Athat%20can%20effectively%20impute%20multiple%20agents%27%20missing%20trajectories.%20First%2C%20a%0Aneural%20network%20equipped%20with%20Set%20Transformers%20produces%20a%20naive%20prediction%20of%0Amissing%20trajectories%20while%20satisfying%20the%20permutation-equivariance%20in%20terms%20of%0Athe%20order%20of%20input%20agents.%20Then%2C%20the%20framework%20makes%20alternative%20predictions%0Aleveraging%20velocity%20and%20acceleration%20information%20and%20combines%20all%20the%0Apredictions%20with%20properly%20determined%20weights%20to%20provide%20final%20imputed%0Atrajectories.%20In%20this%20way%2C%20our%20proposed%20framework%20not%20only%20accurately%20predicts%0Aposition%2C%20velocity%2C%20and%20acceleration%20values%20but%20also%20enforces%20the%20physical%0Arelationship%20between%20them%2C%20eventually%20improving%20both%20the%20accuracy%20and%0Anaturalness%20of%20the%20predicted%20trajectories.%20Accordingly%2C%20the%20experiment%20results%0Aabout%20imputing%20player%20trajectories%20in%20team%20sports%20show%20that%20our%20framework%0Asignificantly%20outperforms%20existing%20imputation%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10878v1&entry.124074799=Read"},
{"title": "Accelerating Goal-Conditioned RL Algorithms and Research", "author": "Micha\u0142 Bortkiewicz and W\u0142adek Pa\u0142ucki and Vivek Myers and Tadeusz Dziarmaga and Tomasz Arczewski and \u0141ukasz Kuci\u0144ski and Benjamin Eysenbach", "abstract": "  Self-supervision has the potential to transform reinforcement learning (RL),\nparalleling the breakthroughs it has enabled in other areas of machine\nlearning. While self-supervised learning in other domains aims to find patterns\nin a fixed dataset, self-supervised goal-conditioned reinforcement learning\n(GCRL) agents discover new behaviors by learning from the goals achieved during\nunstructured interaction with the environment. However, these methods have\nfailed to see similar success, both due to a lack of data from slow\nenvironments as well as a lack of stable algorithms. We take a step toward\naddressing both of these issues by releasing a high-performance codebase and\nbenchmark JaxGCRL for self-supervised GCRL, enabling researchers to train\nagents for millions of environment steps in minutes on a single GPU. The key to\nthis performance is a combination of GPU-accelerated environments and a stable,\nbatched version of the contrastive reinforcement learning algorithm, based on\nan infoNCE objective, that effectively makes use of this increased data\nthroughput. With this approach, we provide a foundation for future research in\nself-supervised GCRL, enabling researchers to quickly iterate on new ideas and\nevaluate them in a diverse set of challenging environments. Website + Code:\nhttps://github.com/MichalBortkiewicz/JaxGCRL\n", "link": "http://arxiv.org/abs/2408.11052v1", "date": "2024-08-20", "relevancy": 2.0158, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5239}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5034}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Goal-Conditioned%20RL%20Algorithms%20and%20Research&body=Title%3A%20Accelerating%20Goal-Conditioned%20RL%20Algorithms%20and%20Research%0AAuthor%3A%20Micha%C5%82%20Bortkiewicz%20and%20W%C5%82adek%20Pa%C5%82ucki%20and%20Vivek%20Myers%20and%20Tadeusz%20Dziarmaga%20and%20Tomasz%20Arczewski%20and%20%C5%81ukasz%20Kuci%C5%84ski%20and%20Benjamin%20Eysenbach%0AAbstract%3A%20%20%20Self-supervision%20has%20the%20potential%20to%20transform%20reinforcement%20learning%20%28RL%29%2C%0Aparalleling%20the%20breakthroughs%20it%20has%20enabled%20in%20other%20areas%20of%20machine%0Alearning.%20While%20self-supervised%20learning%20in%20other%20domains%20aims%20to%20find%20patterns%0Ain%20a%20fixed%20dataset%2C%20self-supervised%20goal-conditioned%20reinforcement%20learning%0A%28GCRL%29%20agents%20discover%20new%20behaviors%20by%20learning%20from%20the%20goals%20achieved%20during%0Aunstructured%20interaction%20with%20the%20environment.%20However%2C%20these%20methods%20have%0Afailed%20to%20see%20similar%20success%2C%20both%20due%20to%20a%20lack%20of%20data%20from%20slow%0Aenvironments%20as%20well%20as%20a%20lack%20of%20stable%20algorithms.%20We%20take%20a%20step%20toward%0Aaddressing%20both%20of%20these%20issues%20by%20releasing%20a%20high-performance%20codebase%20and%0Abenchmark%20JaxGCRL%20for%20self-supervised%20GCRL%2C%20enabling%20researchers%20to%20train%0Aagents%20for%20millions%20of%20environment%20steps%20in%20minutes%20on%20a%20single%20GPU.%20The%20key%20to%0Athis%20performance%20is%20a%20combination%20of%20GPU-accelerated%20environments%20and%20a%20stable%2C%0Abatched%20version%20of%20the%20contrastive%20reinforcement%20learning%20algorithm%2C%20based%20on%0Aan%20infoNCE%20objective%2C%20that%20effectively%20makes%20use%20of%20this%20increased%20data%0Athroughput.%20With%20this%20approach%2C%20we%20provide%20a%20foundation%20for%20future%20research%20in%0Aself-supervised%20GCRL%2C%20enabling%20researchers%20to%20quickly%20iterate%20on%20new%20ideas%20and%0Aevaluate%20them%20in%20a%20diverse%20set%20of%20challenging%20environments.%20Website%20%2B%20Code%3A%0Ahttps%3A//github.com/MichalBortkiewicz/JaxGCRL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Goal-Conditioned%2520RL%2520Algorithms%2520and%2520Research%26entry.906535625%3DMicha%25C5%2582%2520Bortkiewicz%2520and%2520W%25C5%2582adek%2520Pa%25C5%2582ucki%2520and%2520Vivek%2520Myers%2520and%2520Tadeusz%2520Dziarmaga%2520and%2520Tomasz%2520Arczewski%2520and%2520%25C5%2581ukasz%2520Kuci%25C5%2584ski%2520and%2520Benjamin%2520Eysenbach%26entry.1292438233%3D%2520%2520Self-supervision%2520has%2520the%2520potential%2520to%2520transform%2520reinforcement%2520learning%2520%2528RL%2529%252C%250Aparalleling%2520the%2520breakthroughs%2520it%2520has%2520enabled%2520in%2520other%2520areas%2520of%2520machine%250Alearning.%2520While%2520self-supervised%2520learning%2520in%2520other%2520domains%2520aims%2520to%2520find%2520patterns%250Ain%2520a%2520fixed%2520dataset%252C%2520self-supervised%2520goal-conditioned%2520reinforcement%2520learning%250A%2528GCRL%2529%2520agents%2520discover%2520new%2520behaviors%2520by%2520learning%2520from%2520the%2520goals%2520achieved%2520during%250Aunstructured%2520interaction%2520with%2520the%2520environment.%2520However%252C%2520these%2520methods%2520have%250Afailed%2520to%2520see%2520similar%2520success%252C%2520both%2520due%2520to%2520a%2520lack%2520of%2520data%2520from%2520slow%250Aenvironments%2520as%2520well%2520as%2520a%2520lack%2520of%2520stable%2520algorithms.%2520We%2520take%2520a%2520step%2520toward%250Aaddressing%2520both%2520of%2520these%2520issues%2520by%2520releasing%2520a%2520high-performance%2520codebase%2520and%250Abenchmark%2520JaxGCRL%2520for%2520self-supervised%2520GCRL%252C%2520enabling%2520researchers%2520to%2520train%250Aagents%2520for%2520millions%2520of%2520environment%2520steps%2520in%2520minutes%2520on%2520a%2520single%2520GPU.%2520The%2520key%2520to%250Athis%2520performance%2520is%2520a%2520combination%2520of%2520GPU-accelerated%2520environments%2520and%2520a%2520stable%252C%250Abatched%2520version%2520of%2520the%2520contrastive%2520reinforcement%2520learning%2520algorithm%252C%2520based%2520on%250Aan%2520infoNCE%2520objective%252C%2520that%2520effectively%2520makes%2520use%2520of%2520this%2520increased%2520data%250Athroughput.%2520With%2520this%2520approach%252C%2520we%2520provide%2520a%2520foundation%2520for%2520future%2520research%2520in%250Aself-supervised%2520GCRL%252C%2520enabling%2520researchers%2520to%2520quickly%2520iterate%2520on%2520new%2520ideas%2520and%250Aevaluate%2520them%2520in%2520a%2520diverse%2520set%2520of%2520challenging%2520environments.%2520Website%2520%252B%2520Code%253A%250Ahttps%253A//github.com/MichalBortkiewicz/JaxGCRL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Goal-Conditioned%20RL%20Algorithms%20and%20Research&entry.906535625=Micha%C5%82%20Bortkiewicz%20and%20W%C5%82adek%20Pa%C5%82ucki%20and%20Vivek%20Myers%20and%20Tadeusz%20Dziarmaga%20and%20Tomasz%20Arczewski%20and%20%C5%81ukasz%20Kuci%C5%84ski%20and%20Benjamin%20Eysenbach&entry.1292438233=%20%20Self-supervision%20has%20the%20potential%20to%20transform%20reinforcement%20learning%20%28RL%29%2C%0Aparalleling%20the%20breakthroughs%20it%20has%20enabled%20in%20other%20areas%20of%20machine%0Alearning.%20While%20self-supervised%20learning%20in%20other%20domains%20aims%20to%20find%20patterns%0Ain%20a%20fixed%20dataset%2C%20self-supervised%20goal-conditioned%20reinforcement%20learning%0A%28GCRL%29%20agents%20discover%20new%20behaviors%20by%20learning%20from%20the%20goals%20achieved%20during%0Aunstructured%20interaction%20with%20the%20environment.%20However%2C%20these%20methods%20have%0Afailed%20to%20see%20similar%20success%2C%20both%20due%20to%20a%20lack%20of%20data%20from%20slow%0Aenvironments%20as%20well%20as%20a%20lack%20of%20stable%20algorithms.%20We%20take%20a%20step%20toward%0Aaddressing%20both%20of%20these%20issues%20by%20releasing%20a%20high-performance%20codebase%20and%0Abenchmark%20JaxGCRL%20for%20self-supervised%20GCRL%2C%20enabling%20researchers%20to%20train%0Aagents%20for%20millions%20of%20environment%20steps%20in%20minutes%20on%20a%20single%20GPU.%20The%20key%20to%0Athis%20performance%20is%20a%20combination%20of%20GPU-accelerated%20environments%20and%20a%20stable%2C%0Abatched%20version%20of%20the%20contrastive%20reinforcement%20learning%20algorithm%2C%20based%20on%0Aan%20infoNCE%20objective%2C%20that%20effectively%20makes%20use%20of%20this%20increased%20data%0Athroughput.%20With%20this%20approach%2C%20we%20provide%20a%20foundation%20for%20future%20research%20in%0Aself-supervised%20GCRL%2C%20enabling%20researchers%20to%20quickly%20iterate%20on%20new%20ideas%20and%0Aevaluate%20them%20in%20a%20diverse%20set%20of%20challenging%20environments.%20Website%20%2B%20Code%3A%0Ahttps%3A//github.com/MichalBortkiewicz/JaxGCRL%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11052v1&entry.124074799=Read"},
{"title": "Tailoring Graph Neural Network-based Flow-guided Localization to\n  Individual Bloodstreams and Activities", "author": "Pablo Galv\u00e1n and Filip Lemic and Gerard Calvo Bartra and Sergi Abadal and Xavier Costa P\u00e9rez", "abstract": "  Flow-guided localization using in-body nanodevices in the bloodstream is\nexpected to be beneficial for early disease detection, continuous monitoring of\nbiological conditions, and targeted treatment. The nanodevices face size and\npower constraints that produce erroneous raw data for localization purposes.\nOn-body anchors receive this data, and use it to derive the locations of\ndiagnostic events of interest. Different Machine Learning (ML) approaches have\nbeen recently proposed for this task, yet they are currently restricted to a\nreference bloodstream of a resting patient. As such, they are unable to deal\nwith the physical diversity of patients' bloodstreams and cannot provide\ncontinuous monitoring due to changes in individual patient's activities. Toward\naddressing these issues for the current State-of-the-Art (SotA) flow-guided\nlocalization approach based on Graph Neural Networks (GNNs), we propose a\npipeline for GNN adaptation based on individual physiological indicators\nincluding height, weight, and heart rate. Our results indicate that the\nproposed adaptions are beneficial in reconciling the individual differences\nbetween bloodstreams and activities.\n", "link": "http://arxiv.org/abs/2408.01239v2", "date": "2024-08-20", "relevancy": 2.0054, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5177}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4912}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tailoring%20Graph%20Neural%20Network-based%20Flow-guided%20Localization%20to%0A%20%20Individual%20Bloodstreams%20and%20Activities&body=Title%3A%20Tailoring%20Graph%20Neural%20Network-based%20Flow-guided%20Localization%20to%0A%20%20Individual%20Bloodstreams%20and%20Activities%0AAuthor%3A%20Pablo%20Galv%C3%A1n%20and%20Filip%20Lemic%20and%20Gerard%20Calvo%20Bartra%20and%20Sergi%20Abadal%20and%20Xavier%20Costa%20P%C3%A9rez%0AAbstract%3A%20%20%20Flow-guided%20localization%20using%20in-body%20nanodevices%20in%20the%20bloodstream%20is%0Aexpected%20to%20be%20beneficial%20for%20early%20disease%20detection%2C%20continuous%20monitoring%20of%0Abiological%20conditions%2C%20and%20targeted%20treatment.%20The%20nanodevices%20face%20size%20and%0Apower%20constraints%20that%20produce%20erroneous%20raw%20data%20for%20localization%20purposes.%0AOn-body%20anchors%20receive%20this%20data%2C%20and%20use%20it%20to%20derive%20the%20locations%20of%0Adiagnostic%20events%20of%20interest.%20Different%20Machine%20Learning%20%28ML%29%20approaches%20have%0Abeen%20recently%20proposed%20for%20this%20task%2C%20yet%20they%20are%20currently%20restricted%20to%20a%0Areference%20bloodstream%20of%20a%20resting%20patient.%20As%20such%2C%20they%20are%20unable%20to%20deal%0Awith%20the%20physical%20diversity%20of%20patients%27%20bloodstreams%20and%20cannot%20provide%0Acontinuous%20monitoring%20due%20to%20changes%20in%20individual%20patient%27s%20activities.%20Toward%0Aaddressing%20these%20issues%20for%20the%20current%20State-of-the-Art%20%28SotA%29%20flow-guided%0Alocalization%20approach%20based%20on%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20we%20propose%20a%0Apipeline%20for%20GNN%20adaptation%20based%20on%20individual%20physiological%20indicators%0Aincluding%20height%2C%20weight%2C%20and%20heart%20rate.%20Our%20results%20indicate%20that%20the%0Aproposed%20adaptions%20are%20beneficial%20in%20reconciling%20the%20individual%20differences%0Abetween%20bloodstreams%20and%20activities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01239v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTailoring%2520Graph%2520Neural%2520Network-based%2520Flow-guided%2520Localization%2520to%250A%2520%2520Individual%2520Bloodstreams%2520and%2520Activities%26entry.906535625%3DPablo%2520Galv%25C3%25A1n%2520and%2520Filip%2520Lemic%2520and%2520Gerard%2520Calvo%2520Bartra%2520and%2520Sergi%2520Abadal%2520and%2520Xavier%2520Costa%2520P%25C3%25A9rez%26entry.1292438233%3D%2520%2520Flow-guided%2520localization%2520using%2520in-body%2520nanodevices%2520in%2520the%2520bloodstream%2520is%250Aexpected%2520to%2520be%2520beneficial%2520for%2520early%2520disease%2520detection%252C%2520continuous%2520monitoring%2520of%250Abiological%2520conditions%252C%2520and%2520targeted%2520treatment.%2520The%2520nanodevices%2520face%2520size%2520and%250Apower%2520constraints%2520that%2520produce%2520erroneous%2520raw%2520data%2520for%2520localization%2520purposes.%250AOn-body%2520anchors%2520receive%2520this%2520data%252C%2520and%2520use%2520it%2520to%2520derive%2520the%2520locations%2520of%250Adiagnostic%2520events%2520of%2520interest.%2520Different%2520Machine%2520Learning%2520%2528ML%2529%2520approaches%2520have%250Abeen%2520recently%2520proposed%2520for%2520this%2520task%252C%2520yet%2520they%2520are%2520currently%2520restricted%2520to%2520a%250Areference%2520bloodstream%2520of%2520a%2520resting%2520patient.%2520As%2520such%252C%2520they%2520are%2520unable%2520to%2520deal%250Awith%2520the%2520physical%2520diversity%2520of%2520patients%2527%2520bloodstreams%2520and%2520cannot%2520provide%250Acontinuous%2520monitoring%2520due%2520to%2520changes%2520in%2520individual%2520patient%2527s%2520activities.%2520Toward%250Aaddressing%2520these%2520issues%2520for%2520the%2520current%2520State-of-the-Art%2520%2528SotA%2529%2520flow-guided%250Alocalization%2520approach%2520based%2520on%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520we%2520propose%2520a%250Apipeline%2520for%2520GNN%2520adaptation%2520based%2520on%2520individual%2520physiological%2520indicators%250Aincluding%2520height%252C%2520weight%252C%2520and%2520heart%2520rate.%2520Our%2520results%2520indicate%2520that%2520the%250Aproposed%2520adaptions%2520are%2520beneficial%2520in%2520reconciling%2520the%2520individual%2520differences%250Abetween%2520bloodstreams%2520and%2520activities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01239v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tailoring%20Graph%20Neural%20Network-based%20Flow-guided%20Localization%20to%0A%20%20Individual%20Bloodstreams%20and%20Activities&entry.906535625=Pablo%20Galv%C3%A1n%20and%20Filip%20Lemic%20and%20Gerard%20Calvo%20Bartra%20and%20Sergi%20Abadal%20and%20Xavier%20Costa%20P%C3%A9rez&entry.1292438233=%20%20Flow-guided%20localization%20using%20in-body%20nanodevices%20in%20the%20bloodstream%20is%0Aexpected%20to%20be%20beneficial%20for%20early%20disease%20detection%2C%20continuous%20monitoring%20of%0Abiological%20conditions%2C%20and%20targeted%20treatment.%20The%20nanodevices%20face%20size%20and%0Apower%20constraints%20that%20produce%20erroneous%20raw%20data%20for%20localization%20purposes.%0AOn-body%20anchors%20receive%20this%20data%2C%20and%20use%20it%20to%20derive%20the%20locations%20of%0Adiagnostic%20events%20of%20interest.%20Different%20Machine%20Learning%20%28ML%29%20approaches%20have%0Abeen%20recently%20proposed%20for%20this%20task%2C%20yet%20they%20are%20currently%20restricted%20to%20a%0Areference%20bloodstream%20of%20a%20resting%20patient.%20As%20such%2C%20they%20are%20unable%20to%20deal%0Awith%20the%20physical%20diversity%20of%20patients%27%20bloodstreams%20and%20cannot%20provide%0Acontinuous%20monitoring%20due%20to%20changes%20in%20individual%20patient%27s%20activities.%20Toward%0Aaddressing%20these%20issues%20for%20the%20current%20State-of-the-Art%20%28SotA%29%20flow-guided%0Alocalization%20approach%20based%20on%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20we%20propose%20a%0Apipeline%20for%20GNN%20adaptation%20based%20on%20individual%20physiological%20indicators%0Aincluding%20height%2C%20weight%2C%20and%20heart%20rate.%20Our%20results%20indicate%20that%20the%0Aproposed%20adaptions%20are%20beneficial%20in%20reconciling%20the%20individual%20differences%0Abetween%20bloodstreams%20and%20activities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01239v2&entry.124074799=Read"},
{"title": "Knowledge Sharing and Transfer via Centralized Reward Agent for\n  Multi-Task Reinforcement Learning", "author": "Haozhe Ma and Zhengding Luo and Thanh Vinh Vo and Kuankuan Sima and Tze-Yun Leong", "abstract": "  Reward shaping is effective in addressing the sparse-reward challenge in\nreinforcement learning by providing immediate feedback through auxiliary\ninformative rewards. Based on the reward shaping strategy, we propose a novel\nmulti-task reinforcement learning framework, that integrates a centralized\nreward agent (CRA) and multiple distributed policy agents. The CRA functions as\na knowledge pool, which aims to distill knowledge from various tasks and\ndistribute it to individual policy agents to improve learning efficiency.\nSpecifically, the shaped rewards serve as a straightforward metric to encode\nknowledge. This framework not only enhances knowledge sharing across\nestablished tasks but also adapts to new tasks by transferring valuable reward\nsignals. We validate the proposed method on both discrete and continuous\ndomains, demonstrating its robustness in multi-task sparse-reward settings and\nits effective transferability to unseen tasks.\n", "link": "http://arxiv.org/abs/2408.10858v1", "date": "2024-08-20", "relevancy": 1.9955, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5293}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4779}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Sharing%20and%20Transfer%20via%20Centralized%20Reward%20Agent%20for%0A%20%20Multi-Task%20Reinforcement%20Learning&body=Title%3A%20Knowledge%20Sharing%20and%20Transfer%20via%20Centralized%20Reward%20Agent%20for%0A%20%20Multi-Task%20Reinforcement%20Learning%0AAuthor%3A%20Haozhe%20Ma%20and%20Zhengding%20Luo%20and%20Thanh%20Vinh%20Vo%20and%20Kuankuan%20Sima%20and%20Tze-Yun%20Leong%0AAbstract%3A%20%20%20Reward%20shaping%20is%20effective%20in%20addressing%20the%20sparse-reward%20challenge%20in%0Areinforcement%20learning%20by%20providing%20immediate%20feedback%20through%20auxiliary%0Ainformative%20rewards.%20Based%20on%20the%20reward%20shaping%20strategy%2C%20we%20propose%20a%20novel%0Amulti-task%20reinforcement%20learning%20framework%2C%20that%20integrates%20a%20centralized%0Areward%20agent%20%28CRA%29%20and%20multiple%20distributed%20policy%20agents.%20The%20CRA%20functions%20as%0Aa%20knowledge%20pool%2C%20which%20aims%20to%20distill%20knowledge%20from%20various%20tasks%20and%0Adistribute%20it%20to%20individual%20policy%20agents%20to%20improve%20learning%20efficiency.%0ASpecifically%2C%20the%20shaped%20rewards%20serve%20as%20a%20straightforward%20metric%20to%20encode%0Aknowledge.%20This%20framework%20not%20only%20enhances%20knowledge%20sharing%20across%0Aestablished%20tasks%20but%20also%20adapts%20to%20new%20tasks%20by%20transferring%20valuable%20reward%0Asignals.%20We%20validate%20the%20proposed%20method%20on%20both%20discrete%20and%20continuous%0Adomains%2C%20demonstrating%20its%20robustness%20in%20multi-task%20sparse-reward%20settings%20and%0Aits%20effective%20transferability%20to%20unseen%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10858v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Sharing%2520and%2520Transfer%2520via%2520Centralized%2520Reward%2520Agent%2520for%250A%2520%2520Multi-Task%2520Reinforcement%2520Learning%26entry.906535625%3DHaozhe%2520Ma%2520and%2520Zhengding%2520Luo%2520and%2520Thanh%2520Vinh%2520Vo%2520and%2520Kuankuan%2520Sima%2520and%2520Tze-Yun%2520Leong%26entry.1292438233%3D%2520%2520Reward%2520shaping%2520is%2520effective%2520in%2520addressing%2520the%2520sparse-reward%2520challenge%2520in%250Areinforcement%2520learning%2520by%2520providing%2520immediate%2520feedback%2520through%2520auxiliary%250Ainformative%2520rewards.%2520Based%2520on%2520the%2520reward%2520shaping%2520strategy%252C%2520we%2520propose%2520a%2520novel%250Amulti-task%2520reinforcement%2520learning%2520framework%252C%2520that%2520integrates%2520a%2520centralized%250Areward%2520agent%2520%2528CRA%2529%2520and%2520multiple%2520distributed%2520policy%2520agents.%2520The%2520CRA%2520functions%2520as%250Aa%2520knowledge%2520pool%252C%2520which%2520aims%2520to%2520distill%2520knowledge%2520from%2520various%2520tasks%2520and%250Adistribute%2520it%2520to%2520individual%2520policy%2520agents%2520to%2520improve%2520learning%2520efficiency.%250ASpecifically%252C%2520the%2520shaped%2520rewards%2520serve%2520as%2520a%2520straightforward%2520metric%2520to%2520encode%250Aknowledge.%2520This%2520framework%2520not%2520only%2520enhances%2520knowledge%2520sharing%2520across%250Aestablished%2520tasks%2520but%2520also%2520adapts%2520to%2520new%2520tasks%2520by%2520transferring%2520valuable%2520reward%250Asignals.%2520We%2520validate%2520the%2520proposed%2520method%2520on%2520both%2520discrete%2520and%2520continuous%250Adomains%252C%2520demonstrating%2520its%2520robustness%2520in%2520multi-task%2520sparse-reward%2520settings%2520and%250Aits%2520effective%2520transferability%2520to%2520unseen%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10858v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Sharing%20and%20Transfer%20via%20Centralized%20Reward%20Agent%20for%0A%20%20Multi-Task%20Reinforcement%20Learning&entry.906535625=Haozhe%20Ma%20and%20Zhengding%20Luo%20and%20Thanh%20Vinh%20Vo%20and%20Kuankuan%20Sima%20and%20Tze-Yun%20Leong&entry.1292438233=%20%20Reward%20shaping%20is%20effective%20in%20addressing%20the%20sparse-reward%20challenge%20in%0Areinforcement%20learning%20by%20providing%20immediate%20feedback%20through%20auxiliary%0Ainformative%20rewards.%20Based%20on%20the%20reward%20shaping%20strategy%2C%20we%20propose%20a%20novel%0Amulti-task%20reinforcement%20learning%20framework%2C%20that%20integrates%20a%20centralized%0Areward%20agent%20%28CRA%29%20and%20multiple%20distributed%20policy%20agents.%20The%20CRA%20functions%20as%0Aa%20knowledge%20pool%2C%20which%20aims%20to%20distill%20knowledge%20from%20various%20tasks%20and%0Adistribute%20it%20to%20individual%20policy%20agents%20to%20improve%20learning%20efficiency.%0ASpecifically%2C%20the%20shaped%20rewards%20serve%20as%20a%20straightforward%20metric%20to%20encode%0Aknowledge.%20This%20framework%20not%20only%20enhances%20knowledge%20sharing%20across%0Aestablished%20tasks%20but%20also%20adapts%20to%20new%20tasks%20by%20transferring%20valuable%20reward%0Asignals.%20We%20validate%20the%20proposed%20method%20on%20both%20discrete%20and%20continuous%0Adomains%2C%20demonstrating%20its%20robustness%20in%20multi-task%20sparse-reward%20settings%20and%0Aits%20effective%20transferability%20to%20unseen%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10858v1&entry.124074799=Read"},
{"title": "Navigating Spatio-Temporal Heterogeneity: A Graph Transformer Approach\n  for Traffic Forecasting", "author": "Jianxiang Zhou and Erdong Liu and Wei Chen and Siru Zhong and Yuxuan Liang", "abstract": "  Traffic forecasting has emerged as a crucial research area in the development\nof smart cities. Although various neural networks with intricate architectures\nhave been developed to address this problem, they still face two key\nchallenges: i) Recent advancements in network designs for modeling\nspatio-temporal correlations are starting to see diminishing returns in\nperformance enhancements. ii) Additionally, most models do not account for the\nspatio-temporal heterogeneity inherent in traffic data, i.e., traffic\ndistribution varies significantly across different regions and traffic flow\npatterns fluctuate across various time slots. To tackle these challenges, we\nintroduce the Spatio-Temporal Graph Transformer (STGormer), which effectively\nintegrates attribute and structure information inherent in traffic data for\nlearning spatio-temporal correlations, and a mixture-of-experts module for\ncapturing heterogeneity along spaital and temporal axes. Specifically, we\ndesign two straightforward yet effective spatial encoding methods based on the\ngraph structure and integrate time position encoding into the vanilla\ntransformer to capture spatio-temporal traffic patterns. Additionally, a\nmixture-of-experts enhanced feedforward neural network (FNN) module adaptively\nassigns suitable expert layers to distinct patterns via a spatio-temporal\ngating network, further improving overall prediction accuracy. Experiments on\nfive real-world datasets demonstrate that STGormer achieves state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2408.10822v1", "date": "2024-08-20", "relevancy": 1.9925, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5704}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4901}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20Spatio-Temporal%20Heterogeneity%3A%20A%20Graph%20Transformer%20Approach%0A%20%20for%20Traffic%20Forecasting&body=Title%3A%20Navigating%20Spatio-Temporal%20Heterogeneity%3A%20A%20Graph%20Transformer%20Approach%0A%20%20for%20Traffic%20Forecasting%0AAuthor%3A%20Jianxiang%20Zhou%20and%20Erdong%20Liu%20and%20Wei%20Chen%20and%20Siru%20Zhong%20and%20Yuxuan%20Liang%0AAbstract%3A%20%20%20Traffic%20forecasting%20has%20emerged%20as%20a%20crucial%20research%20area%20in%20the%20development%0Aof%20smart%20cities.%20Although%20various%20neural%20networks%20with%20intricate%20architectures%0Ahave%20been%20developed%20to%20address%20this%20problem%2C%20they%20still%20face%20two%20key%0Achallenges%3A%20i%29%20Recent%20advancements%20in%20network%20designs%20for%20modeling%0Aspatio-temporal%20correlations%20are%20starting%20to%20see%20diminishing%20returns%20in%0Aperformance%20enhancements.%20ii%29%20Additionally%2C%20most%20models%20do%20not%20account%20for%20the%0Aspatio-temporal%20heterogeneity%20inherent%20in%20traffic%20data%2C%20i.e.%2C%20traffic%0Adistribution%20varies%20significantly%20across%20different%20regions%20and%20traffic%20flow%0Apatterns%20fluctuate%20across%20various%20time%20slots.%20To%20tackle%20these%20challenges%2C%20we%0Aintroduce%20the%20Spatio-Temporal%20Graph%20Transformer%20%28STGormer%29%2C%20which%20effectively%0Aintegrates%20attribute%20and%20structure%20information%20inherent%20in%20traffic%20data%20for%0Alearning%20spatio-temporal%20correlations%2C%20and%20a%20mixture-of-experts%20module%20for%0Acapturing%20heterogeneity%20along%20spaital%20and%20temporal%20axes.%20Specifically%2C%20we%0Adesign%20two%20straightforward%20yet%20effective%20spatial%20encoding%20methods%20based%20on%20the%0Agraph%20structure%20and%20integrate%20time%20position%20encoding%20into%20the%20vanilla%0Atransformer%20to%20capture%20spatio-temporal%20traffic%20patterns.%20Additionally%2C%20a%0Amixture-of-experts%20enhanced%20feedforward%20neural%20network%20%28FNN%29%20module%20adaptively%0Aassigns%20suitable%20expert%20layers%20to%20distinct%20patterns%20via%20a%20spatio-temporal%0Agating%20network%2C%20further%20improving%20overall%20prediction%20accuracy.%20Experiments%20on%0Afive%20real-world%20datasets%20demonstrate%20that%20STGormer%20achieves%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520Spatio-Temporal%2520Heterogeneity%253A%2520A%2520Graph%2520Transformer%2520Approach%250A%2520%2520for%2520Traffic%2520Forecasting%26entry.906535625%3DJianxiang%2520Zhou%2520and%2520Erdong%2520Liu%2520and%2520Wei%2520Chen%2520and%2520Siru%2520Zhong%2520and%2520Yuxuan%2520Liang%26entry.1292438233%3D%2520%2520Traffic%2520forecasting%2520has%2520emerged%2520as%2520a%2520crucial%2520research%2520area%2520in%2520the%2520development%250Aof%2520smart%2520cities.%2520Although%2520various%2520neural%2520networks%2520with%2520intricate%2520architectures%250Ahave%2520been%2520developed%2520to%2520address%2520this%2520problem%252C%2520they%2520still%2520face%2520two%2520key%250Achallenges%253A%2520i%2529%2520Recent%2520advancements%2520in%2520network%2520designs%2520for%2520modeling%250Aspatio-temporal%2520correlations%2520are%2520starting%2520to%2520see%2520diminishing%2520returns%2520in%250Aperformance%2520enhancements.%2520ii%2529%2520Additionally%252C%2520most%2520models%2520do%2520not%2520account%2520for%2520the%250Aspatio-temporal%2520heterogeneity%2520inherent%2520in%2520traffic%2520data%252C%2520i.e.%252C%2520traffic%250Adistribution%2520varies%2520significantly%2520across%2520different%2520regions%2520and%2520traffic%2520flow%250Apatterns%2520fluctuate%2520across%2520various%2520time%2520slots.%2520To%2520tackle%2520these%2520challenges%252C%2520we%250Aintroduce%2520the%2520Spatio-Temporal%2520Graph%2520Transformer%2520%2528STGormer%2529%252C%2520which%2520effectively%250Aintegrates%2520attribute%2520and%2520structure%2520information%2520inherent%2520in%2520traffic%2520data%2520for%250Alearning%2520spatio-temporal%2520correlations%252C%2520and%2520a%2520mixture-of-experts%2520module%2520for%250Acapturing%2520heterogeneity%2520along%2520spaital%2520and%2520temporal%2520axes.%2520Specifically%252C%2520we%250Adesign%2520two%2520straightforward%2520yet%2520effective%2520spatial%2520encoding%2520methods%2520based%2520on%2520the%250Agraph%2520structure%2520and%2520integrate%2520time%2520position%2520encoding%2520into%2520the%2520vanilla%250Atransformer%2520to%2520capture%2520spatio-temporal%2520traffic%2520patterns.%2520Additionally%252C%2520a%250Amixture-of-experts%2520enhanced%2520feedforward%2520neural%2520network%2520%2528FNN%2529%2520module%2520adaptively%250Aassigns%2520suitable%2520expert%2520layers%2520to%2520distinct%2520patterns%2520via%2520a%2520spatio-temporal%250Agating%2520network%252C%2520further%2520improving%2520overall%2520prediction%2520accuracy.%2520Experiments%2520on%250Afive%2520real-world%2520datasets%2520demonstrate%2520that%2520STGormer%2520achieves%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20Spatio-Temporal%20Heterogeneity%3A%20A%20Graph%20Transformer%20Approach%0A%20%20for%20Traffic%20Forecasting&entry.906535625=Jianxiang%20Zhou%20and%20Erdong%20Liu%20and%20Wei%20Chen%20and%20Siru%20Zhong%20and%20Yuxuan%20Liang&entry.1292438233=%20%20Traffic%20forecasting%20has%20emerged%20as%20a%20crucial%20research%20area%20in%20the%20development%0Aof%20smart%20cities.%20Although%20various%20neural%20networks%20with%20intricate%20architectures%0Ahave%20been%20developed%20to%20address%20this%20problem%2C%20they%20still%20face%20two%20key%0Achallenges%3A%20i%29%20Recent%20advancements%20in%20network%20designs%20for%20modeling%0Aspatio-temporal%20correlations%20are%20starting%20to%20see%20diminishing%20returns%20in%0Aperformance%20enhancements.%20ii%29%20Additionally%2C%20most%20models%20do%20not%20account%20for%20the%0Aspatio-temporal%20heterogeneity%20inherent%20in%20traffic%20data%2C%20i.e.%2C%20traffic%0Adistribution%20varies%20significantly%20across%20different%20regions%20and%20traffic%20flow%0Apatterns%20fluctuate%20across%20various%20time%20slots.%20To%20tackle%20these%20challenges%2C%20we%0Aintroduce%20the%20Spatio-Temporal%20Graph%20Transformer%20%28STGormer%29%2C%20which%20effectively%0Aintegrates%20attribute%20and%20structure%20information%20inherent%20in%20traffic%20data%20for%0Alearning%20spatio-temporal%20correlations%2C%20and%20a%20mixture-of-experts%20module%20for%0Acapturing%20heterogeneity%20along%20spaital%20and%20temporal%20axes.%20Specifically%2C%20we%0Adesign%20two%20straightforward%20yet%20effective%20spatial%20encoding%20methods%20based%20on%20the%0Agraph%20structure%20and%20integrate%20time%20position%20encoding%20into%20the%20vanilla%0Atransformer%20to%20capture%20spatio-temporal%20traffic%20patterns.%20Additionally%2C%20a%0Amixture-of-experts%20enhanced%20feedforward%20neural%20network%20%28FNN%29%20module%20adaptively%0Aassigns%20suitable%20expert%20layers%20to%20distinct%20patterns%20via%20a%20spatio-temporal%0Agating%20network%2C%20further%20improving%20overall%20prediction%20accuracy.%20Experiments%20on%0Afive%20real-world%20datasets%20demonstrate%20that%20STGormer%20achieves%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10822v1&entry.124074799=Read"},
{"title": "Hibou: A Family of Foundational Vision Transformers for Pathology", "author": "Dmitry Nechaev and Alexey Pchelnikov and Ekaterina Ivanova", "abstract": "  Pathology, the microscopic examination of diseased tissue, is critical for\ndiagnosing various medical conditions, particularly cancers. Traditional\nmethods are labor-intensive and prone to human error. Digital pathology, which\nconverts glass slides into high-resolution digital images for analysis by\ncomputer algorithms, revolutionizes the field by enhancing diagnostic accuracy,\nconsistency, and efficiency through automated image analysis and large-scale\ndata processing. Foundational transformer pretraining is crucial for developing\nrobust, generalizable models as it enables learning from vast amounts of\nunannotated data.\n  This paper introduces the Hibou family of foundational vision transformers\nfor pathology, leveraging the DINOv2 framework to pretrain two model variants,\nHibou-B and Hibou-L, on a proprietary dataset of over 1 million whole slide\nimages (WSIs) representing diverse tissue types and staining techniques. Our\npretrained models demonstrate superior performance on both patch-level and\nslide-level benchmarks, surpassing existing state-of-the-art methods. Notably,\nHibou-L achieves the highest average accuracy across multiple benchmark\ndatasets. To support further research and application in the field, we have\nopen-sourced the Hibou models, which can be accessed at\nhttps://github.com/HistAI/hibou.\n", "link": "http://arxiv.org/abs/2406.05074v2", "date": "2024-08-20", "relevancy": 1.9922, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5227}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hibou%3A%20A%20Family%20of%20Foundational%20Vision%20Transformers%20for%20Pathology&body=Title%3A%20Hibou%3A%20A%20Family%20of%20Foundational%20Vision%20Transformers%20for%20Pathology%0AAuthor%3A%20Dmitry%20Nechaev%20and%20Alexey%20Pchelnikov%20and%20Ekaterina%20Ivanova%0AAbstract%3A%20%20%20Pathology%2C%20the%20microscopic%20examination%20of%20diseased%20tissue%2C%20is%20critical%20for%0Adiagnosing%20various%20medical%20conditions%2C%20particularly%20cancers.%20Traditional%0Amethods%20are%20labor-intensive%20and%20prone%20to%20human%20error.%20Digital%20pathology%2C%20which%0Aconverts%20glass%20slides%20into%20high-resolution%20digital%20images%20for%20analysis%20by%0Acomputer%20algorithms%2C%20revolutionizes%20the%20field%20by%20enhancing%20diagnostic%20accuracy%2C%0Aconsistency%2C%20and%20efficiency%20through%20automated%20image%20analysis%20and%20large-scale%0Adata%20processing.%20Foundational%20transformer%20pretraining%20is%20crucial%20for%20developing%0Arobust%2C%20generalizable%20models%20as%20it%20enables%20learning%20from%20vast%20amounts%20of%0Aunannotated%20data.%0A%20%20This%20paper%20introduces%20the%20Hibou%20family%20of%20foundational%20vision%20transformers%0Afor%20pathology%2C%20leveraging%20the%20DINOv2%20framework%20to%20pretrain%20two%20model%20variants%2C%0AHibou-B%20and%20Hibou-L%2C%20on%20a%20proprietary%20dataset%20of%20over%201%20million%20whole%20slide%0Aimages%20%28WSIs%29%20representing%20diverse%20tissue%20types%20and%20staining%20techniques.%20Our%0Apretrained%20models%20demonstrate%20superior%20performance%20on%20both%20patch-level%20and%0Aslide-level%20benchmarks%2C%20surpassing%20existing%20state-of-the-art%20methods.%20Notably%2C%0AHibou-L%20achieves%20the%20highest%20average%20accuracy%20across%20multiple%20benchmark%0Adatasets.%20To%20support%20further%20research%20and%20application%20in%20the%20field%2C%20we%20have%0Aopen-sourced%20the%20Hibou%20models%2C%20which%20can%20be%20accessed%20at%0Ahttps%3A//github.com/HistAI/hibou.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05074v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHibou%253A%2520A%2520Family%2520of%2520Foundational%2520Vision%2520Transformers%2520for%2520Pathology%26entry.906535625%3DDmitry%2520Nechaev%2520and%2520Alexey%2520Pchelnikov%2520and%2520Ekaterina%2520Ivanova%26entry.1292438233%3D%2520%2520Pathology%252C%2520the%2520microscopic%2520examination%2520of%2520diseased%2520tissue%252C%2520is%2520critical%2520for%250Adiagnosing%2520various%2520medical%2520conditions%252C%2520particularly%2520cancers.%2520Traditional%250Amethods%2520are%2520labor-intensive%2520and%2520prone%2520to%2520human%2520error.%2520Digital%2520pathology%252C%2520which%250Aconverts%2520glass%2520slides%2520into%2520high-resolution%2520digital%2520images%2520for%2520analysis%2520by%250Acomputer%2520algorithms%252C%2520revolutionizes%2520the%2520field%2520by%2520enhancing%2520diagnostic%2520accuracy%252C%250Aconsistency%252C%2520and%2520efficiency%2520through%2520automated%2520image%2520analysis%2520and%2520large-scale%250Adata%2520processing.%2520Foundational%2520transformer%2520pretraining%2520is%2520crucial%2520for%2520developing%250Arobust%252C%2520generalizable%2520models%2520as%2520it%2520enables%2520learning%2520from%2520vast%2520amounts%2520of%250Aunannotated%2520data.%250A%2520%2520This%2520paper%2520introduces%2520the%2520Hibou%2520family%2520of%2520foundational%2520vision%2520transformers%250Afor%2520pathology%252C%2520leveraging%2520the%2520DINOv2%2520framework%2520to%2520pretrain%2520two%2520model%2520variants%252C%250AHibou-B%2520and%2520Hibou-L%252C%2520on%2520a%2520proprietary%2520dataset%2520of%2520over%25201%2520million%2520whole%2520slide%250Aimages%2520%2528WSIs%2529%2520representing%2520diverse%2520tissue%2520types%2520and%2520staining%2520techniques.%2520Our%250Apretrained%2520models%2520demonstrate%2520superior%2520performance%2520on%2520both%2520patch-level%2520and%250Aslide-level%2520benchmarks%252C%2520surpassing%2520existing%2520state-of-the-art%2520methods.%2520Notably%252C%250AHibou-L%2520achieves%2520the%2520highest%2520average%2520accuracy%2520across%2520multiple%2520benchmark%250Adatasets.%2520To%2520support%2520further%2520research%2520and%2520application%2520in%2520the%2520field%252C%2520we%2520have%250Aopen-sourced%2520the%2520Hibou%2520models%252C%2520which%2520can%2520be%2520accessed%2520at%250Ahttps%253A//github.com/HistAI/hibou.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05074v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hibou%3A%20A%20Family%20of%20Foundational%20Vision%20Transformers%20for%20Pathology&entry.906535625=Dmitry%20Nechaev%20and%20Alexey%20Pchelnikov%20and%20Ekaterina%20Ivanova&entry.1292438233=%20%20Pathology%2C%20the%20microscopic%20examination%20of%20diseased%20tissue%2C%20is%20critical%20for%0Adiagnosing%20various%20medical%20conditions%2C%20particularly%20cancers.%20Traditional%0Amethods%20are%20labor-intensive%20and%20prone%20to%20human%20error.%20Digital%20pathology%2C%20which%0Aconverts%20glass%20slides%20into%20high-resolution%20digital%20images%20for%20analysis%20by%0Acomputer%20algorithms%2C%20revolutionizes%20the%20field%20by%20enhancing%20diagnostic%20accuracy%2C%0Aconsistency%2C%20and%20efficiency%20through%20automated%20image%20analysis%20and%20large-scale%0Adata%20processing.%20Foundational%20transformer%20pretraining%20is%20crucial%20for%20developing%0Arobust%2C%20generalizable%20models%20as%20it%20enables%20learning%20from%20vast%20amounts%20of%0Aunannotated%20data.%0A%20%20This%20paper%20introduces%20the%20Hibou%20family%20of%20foundational%20vision%20transformers%0Afor%20pathology%2C%20leveraging%20the%20DINOv2%20framework%20to%20pretrain%20two%20model%20variants%2C%0AHibou-B%20and%20Hibou-L%2C%20on%20a%20proprietary%20dataset%20of%20over%201%20million%20whole%20slide%0Aimages%20%28WSIs%29%20representing%20diverse%20tissue%20types%20and%20staining%20techniques.%20Our%0Apretrained%20models%20demonstrate%20superior%20performance%20on%20both%20patch-level%20and%0Aslide-level%20benchmarks%2C%20surpassing%20existing%20state-of-the-art%20methods.%20Notably%2C%0AHibou-L%20achieves%20the%20highest%20average%20accuracy%20across%20multiple%20benchmark%0Adatasets.%20To%20support%20further%20research%20and%20application%20in%20the%20field%2C%20we%20have%0Aopen-sourced%20the%20Hibou%20models%2C%20which%20can%20be%20accessed%20at%0Ahttps%3A//github.com/HistAI/hibou.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05074v2&entry.124074799=Read"},
{"title": "Human-inspired Explanations for Vision Transformers and Convolutional\n  Neural Networks", "author": "Mahadev Prasad Panda and Matteo Tiezzi and Martina Vilas and Gemma Roig and Bjoern M. Eskofier and Dario Zanca", "abstract": "  We introduce Foveation-based Explanations (FovEx), a novel human-inspired\nvisual explainability (XAI) method for Deep Neural Networks. Our method\nachieves state-of-the-art performance on both transformer (on 4 out of 5\nmetrics) and convolutional models (on 3 out of 5 metrics), demonstrating its\nversatility. Furthermore, we show the alignment between the explanation map\nproduced by FovEx and human gaze patterns (+14\\% in NSS compared to RISE,\n+203\\% in NSS compared to gradCAM), enhancing our confidence in FovEx's ability\nto close the interpretation gap between humans and machines.\n", "link": "http://arxiv.org/abs/2408.02123v2", "date": "2024-08-20", "relevancy": 1.9809, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4992}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4969}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-inspired%20Explanations%20for%20Vision%20Transformers%20and%20Convolutional%0A%20%20Neural%20Networks&body=Title%3A%20Human-inspired%20Explanations%20for%20Vision%20Transformers%20and%20Convolutional%0A%20%20Neural%20Networks%0AAuthor%3A%20Mahadev%20Prasad%20Panda%20and%20Matteo%20Tiezzi%20and%20Martina%20Vilas%20and%20Gemma%20Roig%20and%20Bjoern%20M.%20Eskofier%20and%20Dario%20Zanca%0AAbstract%3A%20%20%20We%20introduce%20Foveation-based%20Explanations%20%28FovEx%29%2C%20a%20novel%20human-inspired%0Avisual%20explainability%20%28XAI%29%20method%20for%20Deep%20Neural%20Networks.%20Our%20method%0Aachieves%20state-of-the-art%20performance%20on%20both%20transformer%20%28on%204%20out%20of%205%0Ametrics%29%20and%20convolutional%20models%20%28on%203%20out%20of%205%20metrics%29%2C%20demonstrating%20its%0Aversatility.%20Furthermore%2C%20we%20show%20the%20alignment%20between%20the%20explanation%20map%0Aproduced%20by%20FovEx%20and%20human%20gaze%20patterns%20%28%2B14%5C%25%20in%20NSS%20compared%20to%20RISE%2C%0A%2B203%5C%25%20in%20NSS%20compared%20to%20gradCAM%29%2C%20enhancing%20our%20confidence%20in%20FovEx%27s%20ability%0Ato%20close%20the%20interpretation%20gap%20between%20humans%20and%20machines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02123v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-inspired%2520Explanations%2520for%2520Vision%2520Transformers%2520and%2520Convolutional%250A%2520%2520Neural%2520Networks%26entry.906535625%3DMahadev%2520Prasad%2520Panda%2520and%2520Matteo%2520Tiezzi%2520and%2520Martina%2520Vilas%2520and%2520Gemma%2520Roig%2520and%2520Bjoern%2520M.%2520Eskofier%2520and%2520Dario%2520Zanca%26entry.1292438233%3D%2520%2520We%2520introduce%2520Foveation-based%2520Explanations%2520%2528FovEx%2529%252C%2520a%2520novel%2520human-inspired%250Avisual%2520explainability%2520%2528XAI%2529%2520method%2520for%2520Deep%2520Neural%2520Networks.%2520Our%2520method%250Aachieves%2520state-of-the-art%2520performance%2520on%2520both%2520transformer%2520%2528on%25204%2520out%2520of%25205%250Ametrics%2529%2520and%2520convolutional%2520models%2520%2528on%25203%2520out%2520of%25205%2520metrics%2529%252C%2520demonstrating%2520its%250Aversatility.%2520Furthermore%252C%2520we%2520show%2520the%2520alignment%2520between%2520the%2520explanation%2520map%250Aproduced%2520by%2520FovEx%2520and%2520human%2520gaze%2520patterns%2520%2528%252B14%255C%2525%2520in%2520NSS%2520compared%2520to%2520RISE%252C%250A%252B203%255C%2525%2520in%2520NSS%2520compared%2520to%2520gradCAM%2529%252C%2520enhancing%2520our%2520confidence%2520in%2520FovEx%2527s%2520ability%250Ato%2520close%2520the%2520interpretation%2520gap%2520between%2520humans%2520and%2520machines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02123v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-inspired%20Explanations%20for%20Vision%20Transformers%20and%20Convolutional%0A%20%20Neural%20Networks&entry.906535625=Mahadev%20Prasad%20Panda%20and%20Matteo%20Tiezzi%20and%20Martina%20Vilas%20and%20Gemma%20Roig%20and%20Bjoern%20M.%20Eskofier%20and%20Dario%20Zanca&entry.1292438233=%20%20We%20introduce%20Foveation-based%20Explanations%20%28FovEx%29%2C%20a%20novel%20human-inspired%0Avisual%20explainability%20%28XAI%29%20method%20for%20Deep%20Neural%20Networks.%20Our%20method%0Aachieves%20state-of-the-art%20performance%20on%20both%20transformer%20%28on%204%20out%20of%205%0Ametrics%29%20and%20convolutional%20models%20%28on%203%20out%20of%205%20metrics%29%2C%20demonstrating%20its%0Aversatility.%20Furthermore%2C%20we%20show%20the%20alignment%20between%20the%20explanation%20map%0Aproduced%20by%20FovEx%20and%20human%20gaze%20patterns%20%28%2B14%5C%25%20in%20NSS%20compared%20to%20RISE%2C%0A%2B203%5C%25%20in%20NSS%20compared%20to%20gradCAM%29%2C%20enhancing%20our%20confidence%20in%20FovEx%27s%20ability%0Ato%20close%20the%20interpretation%20gap%20between%20humans%20and%20machines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02123v2&entry.124074799=Read"},
{"title": "Limited Communications Distributed Optimization via Deep Unfolded\n  Distributed ADMM", "author": "Yoav Noah and Nir Shlezinger", "abstract": "  Distributed optimization is a fundamental framework for collaborative\ninference and decision making in decentralized multi-agent systems. The\noperation is modeled as the joint minimization of a shared objective which\ntypically depends on observations gathered locally by each agent. Distributed\noptimization algorithms, such as the common D-ADMM, tackle this task by\niteratively combining local computations and message exchanges. One of the main\nchallenges associated with distributed optimization, and particularly with\nD-ADMM, is that it requires a large number of communications, i.e., messages\nexchanged between the agents, to reach consensus. This can make D-ADMM costly\nin power, latency, and channel resources. In this work we propose unfolded\nD-ADMM, which follows the emerging deep unfolding methodology to enable D-ADMM\nto operate reliably with a predefined and small number of messages exchanged by\neach agent. Unfolded D-ADMM fully preserves the operation of D-ADMM, while\nleveraging data to tune the hyperparameters of each iteration of the algorithm.\nThese hyperparameters can either be agent-specific, aiming at achieving the\nbest performance within a fixed number of iterations over a given network, or\nshared among the agents, allowing to learn to distributedly optimize over\ndifferent networks. For both settings, our unfolded D-ADMM operates with\nlimited communications, while preserving the interpretability and flexibility\nof the original D-ADMM algorithm. We specialize unfolded D-ADMM for two\nrepresentative settings: a distributed estimation task, considering a sparse\nrecovery setup, and a distributed learning scenario, where multiple agents\ncollaborate in learning a machine learning model. Our numerical results\ndemonstrate that the proposed approach dramatically reduces the number of\ncommunications utilized by D-ADMM, without compromising on its performance.\n", "link": "http://arxiv.org/abs/2309.14353v2", "date": "2024-08-20", "relevancy": 1.9787, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4976}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4936}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Limited%20Communications%20Distributed%20Optimization%20via%20Deep%20Unfolded%0A%20%20Distributed%20ADMM&body=Title%3A%20Limited%20Communications%20Distributed%20Optimization%20via%20Deep%20Unfolded%0A%20%20Distributed%20ADMM%0AAuthor%3A%20Yoav%20Noah%20and%20Nir%20Shlezinger%0AAbstract%3A%20%20%20Distributed%20optimization%20is%20a%20fundamental%20framework%20for%20collaborative%0Ainference%20and%20decision%20making%20in%20decentralized%20multi-agent%20systems.%20The%0Aoperation%20is%20modeled%20as%20the%20joint%20minimization%20of%20a%20shared%20objective%20which%0Atypically%20depends%20on%20observations%20gathered%20locally%20by%20each%20agent.%20Distributed%0Aoptimization%20algorithms%2C%20such%20as%20the%20common%20D-ADMM%2C%20tackle%20this%20task%20by%0Aiteratively%20combining%20local%20computations%20and%20message%20exchanges.%20One%20of%20the%20main%0Achallenges%20associated%20with%20distributed%20optimization%2C%20and%20particularly%20with%0AD-ADMM%2C%20is%20that%20it%20requires%20a%20large%20number%20of%20communications%2C%20i.e.%2C%20messages%0Aexchanged%20between%20the%20agents%2C%20to%20reach%20consensus.%20This%20can%20make%20D-ADMM%20costly%0Ain%20power%2C%20latency%2C%20and%20channel%20resources.%20In%20this%20work%20we%20propose%20unfolded%0AD-ADMM%2C%20which%20follows%20the%20emerging%20deep%20unfolding%20methodology%20to%20enable%20D-ADMM%0Ato%20operate%20reliably%20with%20a%20predefined%20and%20small%20number%20of%20messages%20exchanged%20by%0Aeach%20agent.%20Unfolded%20D-ADMM%20fully%20preserves%20the%20operation%20of%20D-ADMM%2C%20while%0Aleveraging%20data%20to%20tune%20the%20hyperparameters%20of%20each%20iteration%20of%20the%20algorithm.%0AThese%20hyperparameters%20can%20either%20be%20agent-specific%2C%20aiming%20at%20achieving%20the%0Abest%20performance%20within%20a%20fixed%20number%20of%20iterations%20over%20a%20given%20network%2C%20or%0Ashared%20among%20the%20agents%2C%20allowing%20to%20learn%20to%20distributedly%20optimize%20over%0Adifferent%20networks.%20For%20both%20settings%2C%20our%20unfolded%20D-ADMM%20operates%20with%0Alimited%20communications%2C%20while%20preserving%20the%20interpretability%20and%20flexibility%0Aof%20the%20original%20D-ADMM%20algorithm.%20We%20specialize%20unfolded%20D-ADMM%20for%20two%0Arepresentative%20settings%3A%20a%20distributed%20estimation%20task%2C%20considering%20a%20sparse%0Arecovery%20setup%2C%20and%20a%20distributed%20learning%20scenario%2C%20where%20multiple%20agents%0Acollaborate%20in%20learning%20a%20machine%20learning%20model.%20Our%20numerical%20results%0Ademonstrate%20that%20the%20proposed%20approach%20dramatically%20reduces%20the%20number%20of%0Acommunications%20utilized%20by%20D-ADMM%2C%20without%20compromising%20on%20its%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14353v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLimited%2520Communications%2520Distributed%2520Optimization%2520via%2520Deep%2520Unfolded%250A%2520%2520Distributed%2520ADMM%26entry.906535625%3DYoav%2520Noah%2520and%2520Nir%2520Shlezinger%26entry.1292438233%3D%2520%2520Distributed%2520optimization%2520is%2520a%2520fundamental%2520framework%2520for%2520collaborative%250Ainference%2520and%2520decision%2520making%2520in%2520decentralized%2520multi-agent%2520systems.%2520The%250Aoperation%2520is%2520modeled%2520as%2520the%2520joint%2520minimization%2520of%2520a%2520shared%2520objective%2520which%250Atypically%2520depends%2520on%2520observations%2520gathered%2520locally%2520by%2520each%2520agent.%2520Distributed%250Aoptimization%2520algorithms%252C%2520such%2520as%2520the%2520common%2520D-ADMM%252C%2520tackle%2520this%2520task%2520by%250Aiteratively%2520combining%2520local%2520computations%2520and%2520message%2520exchanges.%2520One%2520of%2520the%2520main%250Achallenges%2520associated%2520with%2520distributed%2520optimization%252C%2520and%2520particularly%2520with%250AD-ADMM%252C%2520is%2520that%2520it%2520requires%2520a%2520large%2520number%2520of%2520communications%252C%2520i.e.%252C%2520messages%250Aexchanged%2520between%2520the%2520agents%252C%2520to%2520reach%2520consensus.%2520This%2520can%2520make%2520D-ADMM%2520costly%250Ain%2520power%252C%2520latency%252C%2520and%2520channel%2520resources.%2520In%2520this%2520work%2520we%2520propose%2520unfolded%250AD-ADMM%252C%2520which%2520follows%2520the%2520emerging%2520deep%2520unfolding%2520methodology%2520to%2520enable%2520D-ADMM%250Ato%2520operate%2520reliably%2520with%2520a%2520predefined%2520and%2520small%2520number%2520of%2520messages%2520exchanged%2520by%250Aeach%2520agent.%2520Unfolded%2520D-ADMM%2520fully%2520preserves%2520the%2520operation%2520of%2520D-ADMM%252C%2520while%250Aleveraging%2520data%2520to%2520tune%2520the%2520hyperparameters%2520of%2520each%2520iteration%2520of%2520the%2520algorithm.%250AThese%2520hyperparameters%2520can%2520either%2520be%2520agent-specific%252C%2520aiming%2520at%2520achieving%2520the%250Abest%2520performance%2520within%2520a%2520fixed%2520number%2520of%2520iterations%2520over%2520a%2520given%2520network%252C%2520or%250Ashared%2520among%2520the%2520agents%252C%2520allowing%2520to%2520learn%2520to%2520distributedly%2520optimize%2520over%250Adifferent%2520networks.%2520For%2520both%2520settings%252C%2520our%2520unfolded%2520D-ADMM%2520operates%2520with%250Alimited%2520communications%252C%2520while%2520preserving%2520the%2520interpretability%2520and%2520flexibility%250Aof%2520the%2520original%2520D-ADMM%2520algorithm.%2520We%2520specialize%2520unfolded%2520D-ADMM%2520for%2520two%250Arepresentative%2520settings%253A%2520a%2520distributed%2520estimation%2520task%252C%2520considering%2520a%2520sparse%250Arecovery%2520setup%252C%2520and%2520a%2520distributed%2520learning%2520scenario%252C%2520where%2520multiple%2520agents%250Acollaborate%2520in%2520learning%2520a%2520machine%2520learning%2520model.%2520Our%2520numerical%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520approach%2520dramatically%2520reduces%2520the%2520number%2520of%250Acommunications%2520utilized%2520by%2520D-ADMM%252C%2520without%2520compromising%2520on%2520its%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.14353v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Limited%20Communications%20Distributed%20Optimization%20via%20Deep%20Unfolded%0A%20%20Distributed%20ADMM&entry.906535625=Yoav%20Noah%20and%20Nir%20Shlezinger&entry.1292438233=%20%20Distributed%20optimization%20is%20a%20fundamental%20framework%20for%20collaborative%0Ainference%20and%20decision%20making%20in%20decentralized%20multi-agent%20systems.%20The%0Aoperation%20is%20modeled%20as%20the%20joint%20minimization%20of%20a%20shared%20objective%20which%0Atypically%20depends%20on%20observations%20gathered%20locally%20by%20each%20agent.%20Distributed%0Aoptimization%20algorithms%2C%20such%20as%20the%20common%20D-ADMM%2C%20tackle%20this%20task%20by%0Aiteratively%20combining%20local%20computations%20and%20message%20exchanges.%20One%20of%20the%20main%0Achallenges%20associated%20with%20distributed%20optimization%2C%20and%20particularly%20with%0AD-ADMM%2C%20is%20that%20it%20requires%20a%20large%20number%20of%20communications%2C%20i.e.%2C%20messages%0Aexchanged%20between%20the%20agents%2C%20to%20reach%20consensus.%20This%20can%20make%20D-ADMM%20costly%0Ain%20power%2C%20latency%2C%20and%20channel%20resources.%20In%20this%20work%20we%20propose%20unfolded%0AD-ADMM%2C%20which%20follows%20the%20emerging%20deep%20unfolding%20methodology%20to%20enable%20D-ADMM%0Ato%20operate%20reliably%20with%20a%20predefined%20and%20small%20number%20of%20messages%20exchanged%20by%0Aeach%20agent.%20Unfolded%20D-ADMM%20fully%20preserves%20the%20operation%20of%20D-ADMM%2C%20while%0Aleveraging%20data%20to%20tune%20the%20hyperparameters%20of%20each%20iteration%20of%20the%20algorithm.%0AThese%20hyperparameters%20can%20either%20be%20agent-specific%2C%20aiming%20at%20achieving%20the%0Abest%20performance%20within%20a%20fixed%20number%20of%20iterations%20over%20a%20given%20network%2C%20or%0Ashared%20among%20the%20agents%2C%20allowing%20to%20learn%20to%20distributedly%20optimize%20over%0Adifferent%20networks.%20For%20both%20settings%2C%20our%20unfolded%20D-ADMM%20operates%20with%0Alimited%20communications%2C%20while%20preserving%20the%20interpretability%20and%20flexibility%0Aof%20the%20original%20D-ADMM%20algorithm.%20We%20specialize%20unfolded%20D-ADMM%20for%20two%0Arepresentative%20settings%3A%20a%20distributed%20estimation%20task%2C%20considering%20a%20sparse%0Arecovery%20setup%2C%20and%20a%20distributed%20learning%20scenario%2C%20where%20multiple%20agents%0Acollaborate%20in%20learning%20a%20machine%20learning%20model.%20Our%20numerical%20results%0Ademonstrate%20that%20the%20proposed%20approach%20dramatically%20reduces%20the%20number%20of%0Acommunications%20utilized%20by%20D-ADMM%2C%20without%20compromising%20on%20its%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14353v2&entry.124074799=Read"},
{"title": "Elephants Never Forget: Memorization and Learning of Tabular Data in\n  Large Language Models", "author": "Sebastian Bordt and Harsha Nori and Vanessa Rodrigues and Besmira Nushi and Rich Caruana", "abstract": "  While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Specifically, we introduce a variety of different techniques to\nassess whether a language model has seen a tabular dataset during training.\nThis investigation reveals that LLMs have memorized many popular tabular\ndatasets verbatim. We then compare the few-shot learning performance of LLMs on\ndatasets that were seen during training to the performance on datasets released\nafter training. We find that LLMs perform better on datasets seen during\ntraining, indicating that memorization leads to overfitting. At the same time,\nLLMs show non-trivial performance on novel datasets and are surprisingly robust\nto data transformations. We then investigate the in-context statistical\nlearning abilities of LLMs. While LLMs are significantly better than random at\nsolving statistical classification problems, the sample efficiency of few-shot\nlearning lags behind traditional statistical learning algorithms, especially as\nthe dimension of the problem increases. This suggests that much of the observed\nfew-shot performance on novel real-world datasets is due to the LLM's world\nknowledge. Overall, our results highlight the importance of testing whether an\nLLM has seen an evaluation dataset during pre-training. We release the\nhttps://github.com/interpretml/LLM-Tabular-Memorization-Checker Python package\nto test LLMs for memorization of tabular datasets.\n", "link": "http://arxiv.org/abs/2404.06209v2", "date": "2024-08-20", "relevancy": 1.9728, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5092}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.484}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Elephants%20Never%20Forget%3A%20Memorization%20and%20Learning%20of%20Tabular%20Data%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20Elephants%20Never%20Forget%3A%20Memorization%20and%20Learning%20of%20Tabular%20Data%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Sebastian%20Bordt%20and%20Harsha%20Nori%20and%20Vanessa%20Rodrigues%20and%20Besmira%20Nushi%20and%20Rich%20Caruana%0AAbstract%3A%20%20%20While%20many%20have%20shown%20how%20Large%20Language%20Models%20%28LLMs%29%20can%20be%20applied%20to%20a%0Adiverse%20set%20of%20tasks%2C%20the%20critical%20issues%20of%20data%20contamination%20and%0Amemorization%20are%20often%20glossed%20over.%20In%20this%20work%2C%20we%20address%20this%20concern%20for%0Atabular%20data.%20Specifically%2C%20we%20introduce%20a%20variety%20of%20different%20techniques%20to%0Aassess%20whether%20a%20language%20model%20has%20seen%20a%20tabular%20dataset%20during%20training.%0AThis%20investigation%20reveals%20that%20LLMs%20have%20memorized%20many%20popular%20tabular%0Adatasets%20verbatim.%20We%20then%20compare%20the%20few-shot%20learning%20performance%20of%20LLMs%20on%0Adatasets%20that%20were%20seen%20during%20training%20to%20the%20performance%20on%20datasets%20released%0Aafter%20training.%20We%20find%20that%20LLMs%20perform%20better%20on%20datasets%20seen%20during%0Atraining%2C%20indicating%20that%20memorization%20leads%20to%20overfitting.%20At%20the%20same%20time%2C%0ALLMs%20show%20non-trivial%20performance%20on%20novel%20datasets%20and%20are%20surprisingly%20robust%0Ato%20data%20transformations.%20We%20then%20investigate%20the%20in-context%20statistical%0Alearning%20abilities%20of%20LLMs.%20While%20LLMs%20are%20significantly%20better%20than%20random%20at%0Asolving%20statistical%20classification%20problems%2C%20the%20sample%20efficiency%20of%20few-shot%0Alearning%20lags%20behind%20traditional%20statistical%20learning%20algorithms%2C%20especially%20as%0Athe%20dimension%20of%20the%20problem%20increases.%20This%20suggests%20that%20much%20of%20the%20observed%0Afew-shot%20performance%20on%20novel%20real-world%20datasets%20is%20due%20to%20the%20LLM%27s%20world%0Aknowledge.%20Overall%2C%20our%20results%20highlight%20the%20importance%20of%20testing%20whether%20an%0ALLM%20has%20seen%20an%20evaluation%20dataset%20during%20pre-training.%20We%20release%20the%0Ahttps%3A//github.com/interpretml/LLM-Tabular-Memorization-Checker%20Python%20package%0Ato%20test%20LLMs%20for%20memorization%20of%20tabular%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06209v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DElephants%2520Never%2520Forget%253A%2520Memorization%2520and%2520Learning%2520of%2520Tabular%2520Data%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DSebastian%2520Bordt%2520and%2520Harsha%2520Nori%2520and%2520Vanessa%2520Rodrigues%2520and%2520Besmira%2520Nushi%2520and%2520Rich%2520Caruana%26entry.1292438233%3D%2520%2520While%2520many%2520have%2520shown%2520how%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520be%2520applied%2520to%2520a%250Adiverse%2520set%2520of%2520tasks%252C%2520the%2520critical%2520issues%2520of%2520data%2520contamination%2520and%250Amemorization%2520are%2520often%2520glossed%2520over.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520concern%2520for%250Atabular%2520data.%2520Specifically%252C%2520we%2520introduce%2520a%2520variety%2520of%2520different%2520techniques%2520to%250Aassess%2520whether%2520a%2520language%2520model%2520has%2520seen%2520a%2520tabular%2520dataset%2520during%2520training.%250AThis%2520investigation%2520reveals%2520that%2520LLMs%2520have%2520memorized%2520many%2520popular%2520tabular%250Adatasets%2520verbatim.%2520We%2520then%2520compare%2520the%2520few-shot%2520learning%2520performance%2520of%2520LLMs%2520on%250Adatasets%2520that%2520were%2520seen%2520during%2520training%2520to%2520the%2520performance%2520on%2520datasets%2520released%250Aafter%2520training.%2520We%2520find%2520that%2520LLMs%2520perform%2520better%2520on%2520datasets%2520seen%2520during%250Atraining%252C%2520indicating%2520that%2520memorization%2520leads%2520to%2520overfitting.%2520At%2520the%2520same%2520time%252C%250ALLMs%2520show%2520non-trivial%2520performance%2520on%2520novel%2520datasets%2520and%2520are%2520surprisingly%2520robust%250Ato%2520data%2520transformations.%2520We%2520then%2520investigate%2520the%2520in-context%2520statistical%250Alearning%2520abilities%2520of%2520LLMs.%2520While%2520LLMs%2520are%2520significantly%2520better%2520than%2520random%2520at%250Asolving%2520statistical%2520classification%2520problems%252C%2520the%2520sample%2520efficiency%2520of%2520few-shot%250Alearning%2520lags%2520behind%2520traditional%2520statistical%2520learning%2520algorithms%252C%2520especially%2520as%250Athe%2520dimension%2520of%2520the%2520problem%2520increases.%2520This%2520suggests%2520that%2520much%2520of%2520the%2520observed%250Afew-shot%2520performance%2520on%2520novel%2520real-world%2520datasets%2520is%2520due%2520to%2520the%2520LLM%2527s%2520world%250Aknowledge.%2520Overall%252C%2520our%2520results%2520highlight%2520the%2520importance%2520of%2520testing%2520whether%2520an%250ALLM%2520has%2520seen%2520an%2520evaluation%2520dataset%2520during%2520pre-training.%2520We%2520release%2520the%250Ahttps%253A//github.com/interpretml/LLM-Tabular-Memorization-Checker%2520Python%2520package%250Ato%2520test%2520LLMs%2520for%2520memorization%2520of%2520tabular%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06209v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Elephants%20Never%20Forget%3A%20Memorization%20and%20Learning%20of%20Tabular%20Data%20in%0A%20%20Large%20Language%20Models&entry.906535625=Sebastian%20Bordt%20and%20Harsha%20Nori%20and%20Vanessa%20Rodrigues%20and%20Besmira%20Nushi%20and%20Rich%20Caruana&entry.1292438233=%20%20While%20many%20have%20shown%20how%20Large%20Language%20Models%20%28LLMs%29%20can%20be%20applied%20to%20a%0Adiverse%20set%20of%20tasks%2C%20the%20critical%20issues%20of%20data%20contamination%20and%0Amemorization%20are%20often%20glossed%20over.%20In%20this%20work%2C%20we%20address%20this%20concern%20for%0Atabular%20data.%20Specifically%2C%20we%20introduce%20a%20variety%20of%20different%20techniques%20to%0Aassess%20whether%20a%20language%20model%20has%20seen%20a%20tabular%20dataset%20during%20training.%0AThis%20investigation%20reveals%20that%20LLMs%20have%20memorized%20many%20popular%20tabular%0Adatasets%20verbatim.%20We%20then%20compare%20the%20few-shot%20learning%20performance%20of%20LLMs%20on%0Adatasets%20that%20were%20seen%20during%20training%20to%20the%20performance%20on%20datasets%20released%0Aafter%20training.%20We%20find%20that%20LLMs%20perform%20better%20on%20datasets%20seen%20during%0Atraining%2C%20indicating%20that%20memorization%20leads%20to%20overfitting.%20At%20the%20same%20time%2C%0ALLMs%20show%20non-trivial%20performance%20on%20novel%20datasets%20and%20are%20surprisingly%20robust%0Ato%20data%20transformations.%20We%20then%20investigate%20the%20in-context%20statistical%0Alearning%20abilities%20of%20LLMs.%20While%20LLMs%20are%20significantly%20better%20than%20random%20at%0Asolving%20statistical%20classification%20problems%2C%20the%20sample%20efficiency%20of%20few-shot%0Alearning%20lags%20behind%20traditional%20statistical%20learning%20algorithms%2C%20especially%20as%0Athe%20dimension%20of%20the%20problem%20increases.%20This%20suggests%20that%20much%20of%20the%20observed%0Afew-shot%20performance%20on%20novel%20real-world%20datasets%20is%20due%20to%20the%20LLM%27s%20world%0Aknowledge.%20Overall%2C%20our%20results%20highlight%20the%20importance%20of%20testing%20whether%20an%0ALLM%20has%20seen%20an%20evaluation%20dataset%20during%20pre-training.%20We%20release%20the%0Ahttps%3A//github.com/interpretml/LLM-Tabular-Memorization-Checker%20Python%20package%0Ato%20test%20LLMs%20for%20memorization%20of%20tabular%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06209v2&entry.124074799=Read"},
{"title": "ELASTIC: Efficient Linear Attention for Sequential Interest Compression", "author": "Jiaxin Deng and Shiyao Wang and Song Lu and Yinfeng Li and Xinchen Luo and Yuanjun Liu and Peixing Xu and Guorui Zhou", "abstract": "  State-of-the-art sequential recommendation models heavily rely on\ntransformer's attention mechanism. However, the quadratic computational and\nmemory complexities of self attention have limited its scalability for modeling\nusers' long range behaviour sequences. To address this problem, we propose\nELASTIC, an Efficient Linear Attention for SequenTial Interest Compression,\nrequiring only linear time complexity and decoupling model capacity from\ncomputational cost. Specifically, ELASTIC introduces a fixed length interest\nexperts with linear dispatcher attention mechanism which compresses the\nlong-term behaviour sequences to a significantly more compact representation\nwhich reduces up to 90% GPU memory usage with x2.7 inference speed up. The\nproposed linear dispatcher attention mechanism significantly reduces the\nquadratic complexity and makes the model feasible for adequately modeling\nextremely long sequences. Moreover, in order to retain the capacity for\nmodeling various user interests, ELASTIC initializes a vast learnable interest\nmemory bank and sparsely retrieves compressed user's interests from the memory\nwith a negligible computational overhead. The proposed interest memory\nretrieval technique significantly expands the cardinality of available interest\nspace while keeping the same computational cost, thereby striking a trade-off\nbetween recommendation accuracy and efficiency. To validate the effectiveness\nof our proposed ELASTIC, we conduct extensive experiments on various public\ndatasets and compare it with several strong sequential recommenders.\nExperimental results demonstrate that ELASTIC consistently outperforms\nbaselines by a significant margin and also highlight the computational\nefficiency of ELASTIC when modeling long sequences. We will make our\nimplementation code publicly available.\n", "link": "http://arxiv.org/abs/2408.09380v2", "date": "2024-08-20", "relevancy": 1.962, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.509}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4887}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELASTIC%3A%20Efficient%20Linear%20Attention%20for%20Sequential%20Interest%20Compression&body=Title%3A%20ELASTIC%3A%20Efficient%20Linear%20Attention%20for%20Sequential%20Interest%20Compression%0AAuthor%3A%20Jiaxin%20Deng%20and%20Shiyao%20Wang%20and%20Song%20Lu%20and%20Yinfeng%20Li%20and%20Xinchen%20Luo%20and%20Yuanjun%20Liu%20and%20Peixing%20Xu%20and%20Guorui%20Zhou%0AAbstract%3A%20%20%20State-of-the-art%20sequential%20recommendation%20models%20heavily%20rely%20on%0Atransformer%27s%20attention%20mechanism.%20However%2C%20the%20quadratic%20computational%20and%0Amemory%20complexities%20of%20self%20attention%20have%20limited%20its%20scalability%20for%20modeling%0Ausers%27%20long%20range%20behaviour%20sequences.%20To%20address%20this%20problem%2C%20we%20propose%0AELASTIC%2C%20an%20Efficient%20Linear%20Attention%20for%20SequenTial%20Interest%20Compression%2C%0Arequiring%20only%20linear%20time%20complexity%20and%20decoupling%20model%20capacity%20from%0Acomputational%20cost.%20Specifically%2C%20ELASTIC%20introduces%20a%20fixed%20length%20interest%0Aexperts%20with%20linear%20dispatcher%20attention%20mechanism%20which%20compresses%20the%0Along-term%20behaviour%20sequences%20to%20a%20significantly%20more%20compact%20representation%0Awhich%20reduces%20up%20to%2090%25%20GPU%20memory%20usage%20with%20x2.7%20inference%20speed%20up.%20The%0Aproposed%20linear%20dispatcher%20attention%20mechanism%20significantly%20reduces%20the%0Aquadratic%20complexity%20and%20makes%20the%20model%20feasible%20for%20adequately%20modeling%0Aextremely%20long%20sequences.%20Moreover%2C%20in%20order%20to%20retain%20the%20capacity%20for%0Amodeling%20various%20user%20interests%2C%20ELASTIC%20initializes%20a%20vast%20learnable%20interest%0Amemory%20bank%20and%20sparsely%20retrieves%20compressed%20user%27s%20interests%20from%20the%20memory%0Awith%20a%20negligible%20computational%20overhead.%20The%20proposed%20interest%20memory%0Aretrieval%20technique%20significantly%20expands%20the%20cardinality%20of%20available%20interest%0Aspace%20while%20keeping%20the%20same%20computational%20cost%2C%20thereby%20striking%20a%20trade-off%0Abetween%20recommendation%20accuracy%20and%20efficiency.%20To%20validate%20the%20effectiveness%0Aof%20our%20proposed%20ELASTIC%2C%20we%20conduct%20extensive%20experiments%20on%20various%20public%0Adatasets%20and%20compare%20it%20with%20several%20strong%20sequential%20recommenders.%0AExperimental%20results%20demonstrate%20that%20ELASTIC%20consistently%20outperforms%0Abaselines%20by%20a%20significant%20margin%20and%20also%20highlight%20the%20computational%0Aefficiency%20of%20ELASTIC%20when%20modeling%20long%20sequences.%20We%20will%20make%20our%0Aimplementation%20code%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09380v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELASTIC%253A%2520Efficient%2520Linear%2520Attention%2520for%2520Sequential%2520Interest%2520Compression%26entry.906535625%3DJiaxin%2520Deng%2520and%2520Shiyao%2520Wang%2520and%2520Song%2520Lu%2520and%2520Yinfeng%2520Li%2520and%2520Xinchen%2520Luo%2520and%2520Yuanjun%2520Liu%2520and%2520Peixing%2520Xu%2520and%2520Guorui%2520Zhou%26entry.1292438233%3D%2520%2520State-of-the-art%2520sequential%2520recommendation%2520models%2520heavily%2520rely%2520on%250Atransformer%2527s%2520attention%2520mechanism.%2520However%252C%2520the%2520quadratic%2520computational%2520and%250Amemory%2520complexities%2520of%2520self%2520attention%2520have%2520limited%2520its%2520scalability%2520for%2520modeling%250Ausers%2527%2520long%2520range%2520behaviour%2520sequences.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%250AELASTIC%252C%2520an%2520Efficient%2520Linear%2520Attention%2520for%2520SequenTial%2520Interest%2520Compression%252C%250Arequiring%2520only%2520linear%2520time%2520complexity%2520and%2520decoupling%2520model%2520capacity%2520from%250Acomputational%2520cost.%2520Specifically%252C%2520ELASTIC%2520introduces%2520a%2520fixed%2520length%2520interest%250Aexperts%2520with%2520linear%2520dispatcher%2520attention%2520mechanism%2520which%2520compresses%2520the%250Along-term%2520behaviour%2520sequences%2520to%2520a%2520significantly%2520more%2520compact%2520representation%250Awhich%2520reduces%2520up%2520to%252090%2525%2520GPU%2520memory%2520usage%2520with%2520x2.7%2520inference%2520speed%2520up.%2520The%250Aproposed%2520linear%2520dispatcher%2520attention%2520mechanism%2520significantly%2520reduces%2520the%250Aquadratic%2520complexity%2520and%2520makes%2520the%2520model%2520feasible%2520for%2520adequately%2520modeling%250Aextremely%2520long%2520sequences.%2520Moreover%252C%2520in%2520order%2520to%2520retain%2520the%2520capacity%2520for%250Amodeling%2520various%2520user%2520interests%252C%2520ELASTIC%2520initializes%2520a%2520vast%2520learnable%2520interest%250Amemory%2520bank%2520and%2520sparsely%2520retrieves%2520compressed%2520user%2527s%2520interests%2520from%2520the%2520memory%250Awith%2520a%2520negligible%2520computational%2520overhead.%2520The%2520proposed%2520interest%2520memory%250Aretrieval%2520technique%2520significantly%2520expands%2520the%2520cardinality%2520of%2520available%2520interest%250Aspace%2520while%2520keeping%2520the%2520same%2520computational%2520cost%252C%2520thereby%2520striking%2520a%2520trade-off%250Abetween%2520recommendation%2520accuracy%2520and%2520efficiency.%2520To%2520validate%2520the%2520effectiveness%250Aof%2520our%2520proposed%2520ELASTIC%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%2520various%2520public%250Adatasets%2520and%2520compare%2520it%2520with%2520several%2520strong%2520sequential%2520recommenders.%250AExperimental%2520results%2520demonstrate%2520that%2520ELASTIC%2520consistently%2520outperforms%250Abaselines%2520by%2520a%2520significant%2520margin%2520and%2520also%2520highlight%2520the%2520computational%250Aefficiency%2520of%2520ELASTIC%2520when%2520modeling%2520long%2520sequences.%2520We%2520will%2520make%2520our%250Aimplementation%2520code%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09380v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELASTIC%3A%20Efficient%20Linear%20Attention%20for%20Sequential%20Interest%20Compression&entry.906535625=Jiaxin%20Deng%20and%20Shiyao%20Wang%20and%20Song%20Lu%20and%20Yinfeng%20Li%20and%20Xinchen%20Luo%20and%20Yuanjun%20Liu%20and%20Peixing%20Xu%20and%20Guorui%20Zhou&entry.1292438233=%20%20State-of-the-art%20sequential%20recommendation%20models%20heavily%20rely%20on%0Atransformer%27s%20attention%20mechanism.%20However%2C%20the%20quadratic%20computational%20and%0Amemory%20complexities%20of%20self%20attention%20have%20limited%20its%20scalability%20for%20modeling%0Ausers%27%20long%20range%20behaviour%20sequences.%20To%20address%20this%20problem%2C%20we%20propose%0AELASTIC%2C%20an%20Efficient%20Linear%20Attention%20for%20SequenTial%20Interest%20Compression%2C%0Arequiring%20only%20linear%20time%20complexity%20and%20decoupling%20model%20capacity%20from%0Acomputational%20cost.%20Specifically%2C%20ELASTIC%20introduces%20a%20fixed%20length%20interest%0Aexperts%20with%20linear%20dispatcher%20attention%20mechanism%20which%20compresses%20the%0Along-term%20behaviour%20sequences%20to%20a%20significantly%20more%20compact%20representation%0Awhich%20reduces%20up%20to%2090%25%20GPU%20memory%20usage%20with%20x2.7%20inference%20speed%20up.%20The%0Aproposed%20linear%20dispatcher%20attention%20mechanism%20significantly%20reduces%20the%0Aquadratic%20complexity%20and%20makes%20the%20model%20feasible%20for%20adequately%20modeling%0Aextremely%20long%20sequences.%20Moreover%2C%20in%20order%20to%20retain%20the%20capacity%20for%0Amodeling%20various%20user%20interests%2C%20ELASTIC%20initializes%20a%20vast%20learnable%20interest%0Amemory%20bank%20and%20sparsely%20retrieves%20compressed%20user%27s%20interests%20from%20the%20memory%0Awith%20a%20negligible%20computational%20overhead.%20The%20proposed%20interest%20memory%0Aretrieval%20technique%20significantly%20expands%20the%20cardinality%20of%20available%20interest%0Aspace%20while%20keeping%20the%20same%20computational%20cost%2C%20thereby%20striking%20a%20trade-off%0Abetween%20recommendation%20accuracy%20and%20efficiency.%20To%20validate%20the%20effectiveness%0Aof%20our%20proposed%20ELASTIC%2C%20we%20conduct%20extensive%20experiments%20on%20various%20public%0Adatasets%20and%20compare%20it%20with%20several%20strong%20sequential%20recommenders.%0AExperimental%20results%20demonstrate%20that%20ELASTIC%20consistently%20outperforms%0Abaselines%20by%20a%20significant%20margin%20and%20also%20highlight%20the%20computational%0Aefficiency%20of%20ELASTIC%20when%20modeling%20long%20sequences.%20We%20will%20make%20our%0Aimplementation%20code%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09380v2&entry.124074799=Read"},
{"title": "Classification of Endoscopy and Video Capsule Images using\n  CNN-Transformer Model", "author": "Aliza Subedi and Smriti Regmi and Nisha Regmi and Bhumi Bhusal and Ulas Bagci and Debesh Jha", "abstract": "  Gastrointestinal cancer is a leading cause of cancer-related incidence and\ndeath, making it crucial to develop novel computer-aided diagnosis systems for\nearly detection and enhanced treatment. Traditional approaches rely on the\nexpertise of gastroenterologists to identify diseases; however, this process is\nsubjective, and interpretation can vary even among expert clinicians.\nConsidering recent advancements in classifying gastrointestinal anomalies and\nlandmarks in endoscopic and video capsule endoscopy images, this study proposes\na hybrid model that combines the advantages of Transformers and Convolutional\nNeural Networks (CNNs) to enhance classification performance. Our model\nutilizes DenseNet201 as a CNN branch to extract local features and integrates a\nSwin Transformer branch for global feature understanding, combining both to\nperform the classification task. For the GastroVision dataset, our proposed\nmodel demonstrates excellent performance with Precision, Recall, F1 score,\nAccuracy, and Matthews Correlation Coefficient (MCC) of 0.8320, 0.8386, 0.8324,\n0.8386, and 0.8191, respectively, showcasing its robustness against class\nimbalance and surpassing other CNNs as well as the Swin Transformer model.\nSimilarly, for the Kvasir-Capsule, a large video capsule endoscopy dataset, our\nmodel outperforms all others, achieving overall Precision, Recall, F1 score,\nAccuracy, and MCC of 0.7007, 0.7239, 0.6900, 0.7239, and 0.3871. Moreover, we\ngenerated saliency maps to explain our model's focus areas, demonstrating its\nreliable decision-making process. The results underscore the potential of our\nhybrid CNN-Transformer model in aiding the early and accurate detection of\ngastrointestinal (GI) anomalies.\n", "link": "http://arxiv.org/abs/2408.10733v1", "date": "2024-08-20", "relevancy": 1.9543, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5039}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5016}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20of%20Endoscopy%20and%20Video%20Capsule%20Images%20using%0A%20%20CNN-Transformer%20Model&body=Title%3A%20Classification%20of%20Endoscopy%20and%20Video%20Capsule%20Images%20using%0A%20%20CNN-Transformer%20Model%0AAuthor%3A%20Aliza%20Subedi%20and%20Smriti%20Regmi%20and%20Nisha%20Regmi%20and%20Bhumi%20Bhusal%20and%20Ulas%20Bagci%20and%20Debesh%20Jha%0AAbstract%3A%20%20%20Gastrointestinal%20cancer%20is%20a%20leading%20cause%20of%20cancer-related%20incidence%20and%0Adeath%2C%20making%20it%20crucial%20to%20develop%20novel%20computer-aided%20diagnosis%20systems%20for%0Aearly%20detection%20and%20enhanced%20treatment.%20Traditional%20approaches%20rely%20on%20the%0Aexpertise%20of%20gastroenterologists%20to%20identify%20diseases%3B%20however%2C%20this%20process%20is%0Asubjective%2C%20and%20interpretation%20can%20vary%20even%20among%20expert%20clinicians.%0AConsidering%20recent%20advancements%20in%20classifying%20gastrointestinal%20anomalies%20and%0Alandmarks%20in%20endoscopic%20and%20video%20capsule%20endoscopy%20images%2C%20this%20study%20proposes%0Aa%20hybrid%20model%20that%20combines%20the%20advantages%20of%20Transformers%20and%20Convolutional%0ANeural%20Networks%20%28CNNs%29%20to%20enhance%20classification%20performance.%20Our%20model%0Autilizes%20DenseNet201%20as%20a%20CNN%20branch%20to%20extract%20local%20features%20and%20integrates%20a%0ASwin%20Transformer%20branch%20for%20global%20feature%20understanding%2C%20combining%20both%20to%0Aperform%20the%20classification%20task.%20For%20the%20GastroVision%20dataset%2C%20our%20proposed%0Amodel%20demonstrates%20excellent%20performance%20with%20Precision%2C%20Recall%2C%20F1%20score%2C%0AAccuracy%2C%20and%20Matthews%20Correlation%20Coefficient%20%28MCC%29%20of%200.8320%2C%200.8386%2C%200.8324%2C%0A0.8386%2C%20and%200.8191%2C%20respectively%2C%20showcasing%20its%20robustness%20against%20class%0Aimbalance%20and%20surpassing%20other%20CNNs%20as%20well%20as%20the%20Swin%20Transformer%20model.%0ASimilarly%2C%20for%20the%20Kvasir-Capsule%2C%20a%20large%20video%20capsule%20endoscopy%20dataset%2C%20our%0Amodel%20outperforms%20all%20others%2C%20achieving%20overall%20Precision%2C%20Recall%2C%20F1%20score%2C%0AAccuracy%2C%20and%20MCC%20of%200.7007%2C%200.7239%2C%200.6900%2C%200.7239%2C%20and%200.3871.%20Moreover%2C%20we%0Agenerated%20saliency%20maps%20to%20explain%20our%20model%27s%20focus%20areas%2C%20demonstrating%20its%0Areliable%20decision-making%20process.%20The%20results%20underscore%20the%20potential%20of%20our%0Ahybrid%20CNN-Transformer%20model%20in%20aiding%20the%20early%20and%20accurate%20detection%20of%0Agastrointestinal%20%28GI%29%20anomalies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520of%2520Endoscopy%2520and%2520Video%2520Capsule%2520Images%2520using%250A%2520%2520CNN-Transformer%2520Model%26entry.906535625%3DAliza%2520Subedi%2520and%2520Smriti%2520Regmi%2520and%2520Nisha%2520Regmi%2520and%2520Bhumi%2520Bhusal%2520and%2520Ulas%2520Bagci%2520and%2520Debesh%2520Jha%26entry.1292438233%3D%2520%2520Gastrointestinal%2520cancer%2520is%2520a%2520leading%2520cause%2520of%2520cancer-related%2520incidence%2520and%250Adeath%252C%2520making%2520it%2520crucial%2520to%2520develop%2520novel%2520computer-aided%2520diagnosis%2520systems%2520for%250Aearly%2520detection%2520and%2520enhanced%2520treatment.%2520Traditional%2520approaches%2520rely%2520on%2520the%250Aexpertise%2520of%2520gastroenterologists%2520to%2520identify%2520diseases%253B%2520however%252C%2520this%2520process%2520is%250Asubjective%252C%2520and%2520interpretation%2520can%2520vary%2520even%2520among%2520expert%2520clinicians.%250AConsidering%2520recent%2520advancements%2520in%2520classifying%2520gastrointestinal%2520anomalies%2520and%250Alandmarks%2520in%2520endoscopic%2520and%2520video%2520capsule%2520endoscopy%2520images%252C%2520this%2520study%2520proposes%250Aa%2520hybrid%2520model%2520that%2520combines%2520the%2520advantages%2520of%2520Transformers%2520and%2520Convolutional%250ANeural%2520Networks%2520%2528CNNs%2529%2520to%2520enhance%2520classification%2520performance.%2520Our%2520model%250Autilizes%2520DenseNet201%2520as%2520a%2520CNN%2520branch%2520to%2520extract%2520local%2520features%2520and%2520integrates%2520a%250ASwin%2520Transformer%2520branch%2520for%2520global%2520feature%2520understanding%252C%2520combining%2520both%2520to%250Aperform%2520the%2520classification%2520task.%2520For%2520the%2520GastroVision%2520dataset%252C%2520our%2520proposed%250Amodel%2520demonstrates%2520excellent%2520performance%2520with%2520Precision%252C%2520Recall%252C%2520F1%2520score%252C%250AAccuracy%252C%2520and%2520Matthews%2520Correlation%2520Coefficient%2520%2528MCC%2529%2520of%25200.8320%252C%25200.8386%252C%25200.8324%252C%250A0.8386%252C%2520and%25200.8191%252C%2520respectively%252C%2520showcasing%2520its%2520robustness%2520against%2520class%250Aimbalance%2520and%2520surpassing%2520other%2520CNNs%2520as%2520well%2520as%2520the%2520Swin%2520Transformer%2520model.%250ASimilarly%252C%2520for%2520the%2520Kvasir-Capsule%252C%2520a%2520large%2520video%2520capsule%2520endoscopy%2520dataset%252C%2520our%250Amodel%2520outperforms%2520all%2520others%252C%2520achieving%2520overall%2520Precision%252C%2520Recall%252C%2520F1%2520score%252C%250AAccuracy%252C%2520and%2520MCC%2520of%25200.7007%252C%25200.7239%252C%25200.6900%252C%25200.7239%252C%2520and%25200.3871.%2520Moreover%252C%2520we%250Agenerated%2520saliency%2520maps%2520to%2520explain%2520our%2520model%2527s%2520focus%2520areas%252C%2520demonstrating%2520its%250Areliable%2520decision-making%2520process.%2520The%2520results%2520underscore%2520the%2520potential%2520of%2520our%250Ahybrid%2520CNN-Transformer%2520model%2520in%2520aiding%2520the%2520early%2520and%2520accurate%2520detection%2520of%250Agastrointestinal%2520%2528GI%2529%2520anomalies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20of%20Endoscopy%20and%20Video%20Capsule%20Images%20using%0A%20%20CNN-Transformer%20Model&entry.906535625=Aliza%20Subedi%20and%20Smriti%20Regmi%20and%20Nisha%20Regmi%20and%20Bhumi%20Bhusal%20and%20Ulas%20Bagci%20and%20Debesh%20Jha&entry.1292438233=%20%20Gastrointestinal%20cancer%20is%20a%20leading%20cause%20of%20cancer-related%20incidence%20and%0Adeath%2C%20making%20it%20crucial%20to%20develop%20novel%20computer-aided%20diagnosis%20systems%20for%0Aearly%20detection%20and%20enhanced%20treatment.%20Traditional%20approaches%20rely%20on%20the%0Aexpertise%20of%20gastroenterologists%20to%20identify%20diseases%3B%20however%2C%20this%20process%20is%0Asubjective%2C%20and%20interpretation%20can%20vary%20even%20among%20expert%20clinicians.%0AConsidering%20recent%20advancements%20in%20classifying%20gastrointestinal%20anomalies%20and%0Alandmarks%20in%20endoscopic%20and%20video%20capsule%20endoscopy%20images%2C%20this%20study%20proposes%0Aa%20hybrid%20model%20that%20combines%20the%20advantages%20of%20Transformers%20and%20Convolutional%0ANeural%20Networks%20%28CNNs%29%20to%20enhance%20classification%20performance.%20Our%20model%0Autilizes%20DenseNet201%20as%20a%20CNN%20branch%20to%20extract%20local%20features%20and%20integrates%20a%0ASwin%20Transformer%20branch%20for%20global%20feature%20understanding%2C%20combining%20both%20to%0Aperform%20the%20classification%20task.%20For%20the%20GastroVision%20dataset%2C%20our%20proposed%0Amodel%20demonstrates%20excellent%20performance%20with%20Precision%2C%20Recall%2C%20F1%20score%2C%0AAccuracy%2C%20and%20Matthews%20Correlation%20Coefficient%20%28MCC%29%20of%200.8320%2C%200.8386%2C%200.8324%2C%0A0.8386%2C%20and%200.8191%2C%20respectively%2C%20showcasing%20its%20robustness%20against%20class%0Aimbalance%20and%20surpassing%20other%20CNNs%20as%20well%20as%20the%20Swin%20Transformer%20model.%0ASimilarly%2C%20for%20the%20Kvasir-Capsule%2C%20a%20large%20video%20capsule%20endoscopy%20dataset%2C%20our%0Amodel%20outperforms%20all%20others%2C%20achieving%20overall%20Precision%2C%20Recall%2C%20F1%20score%2C%0AAccuracy%2C%20and%20MCC%20of%200.7007%2C%200.7239%2C%200.6900%2C%200.7239%2C%20and%200.3871.%20Moreover%2C%20we%0Agenerated%20saliency%20maps%20to%20explain%20our%20model%27s%20focus%20areas%2C%20demonstrating%20its%0Areliable%20decision-making%20process.%20The%20results%20underscore%20the%20potential%20of%20our%0Ahybrid%20CNN-Transformer%20model%20in%20aiding%20the%20early%20and%20accurate%20detection%20of%0Agastrointestinal%20%28GI%29%20anomalies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10733v1&entry.124074799=Read"},
{"title": "Aligning Object Detector Bounding Boxes with Human Preference", "author": "Ombretta Strafforello and Osman S. Kayhan and Oana Inel and Klamer Schutte and Jan van Gemert", "abstract": "  Previous work shows that humans tend to prefer large bounding boxes over\nsmall bounding boxes with the same IoU. However, we show here that commonly\nused object detectors predict large and small boxes equally often. In this\nwork, we investigate how to align automatically detected object boxes with\nhuman preference and study whether this improves human quality perception. We\nevaluate the performance of three commonly used object detectors through a user\nstudy (N = 123). We find that humans prefer object detections that are upscaled\nwith factors of 1.5 or 2, even if the corresponding AP is close to 0. Motivated\nby this result, we propose an asymmetric bounding box regression loss that\nencourages large over small predicted bounding boxes. Our evaluation study\nshows that object detectors fine-tuned with the asymmetric loss are better\naligned with human preference and are preferred over fixed scaling factors. A\nqualitative evaluation shows that human preference might be influenced by some\nobject characteristics, like object shape.\n", "link": "http://arxiv.org/abs/2408.10844v1", "date": "2024-08-20", "relevancy": 1.9519, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5307}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.481}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Object%20Detector%20Bounding%20Boxes%20with%20Human%20Preference&body=Title%3A%20Aligning%20Object%20Detector%20Bounding%20Boxes%20with%20Human%20Preference%0AAuthor%3A%20Ombretta%20Strafforello%20and%20Osman%20S.%20Kayhan%20and%20Oana%20Inel%20and%20Klamer%20Schutte%20and%20Jan%20van%20Gemert%0AAbstract%3A%20%20%20Previous%20work%20shows%20that%20humans%20tend%20to%20prefer%20large%20bounding%20boxes%20over%0Asmall%20bounding%20boxes%20with%20the%20same%20IoU.%20However%2C%20we%20show%20here%20that%20commonly%0Aused%20object%20detectors%20predict%20large%20and%20small%20boxes%20equally%20often.%20In%20this%0Awork%2C%20we%20investigate%20how%20to%20align%20automatically%20detected%20object%20boxes%20with%0Ahuman%20preference%20and%20study%20whether%20this%20improves%20human%20quality%20perception.%20We%0Aevaluate%20the%20performance%20of%20three%20commonly%20used%20object%20detectors%20through%20a%20user%0Astudy%20%28N%20%3D%20123%29.%20We%20find%20that%20humans%20prefer%20object%20detections%20that%20are%20upscaled%0Awith%20factors%20of%201.5%20or%202%2C%20even%20if%20the%20corresponding%20AP%20is%20close%20to%200.%20Motivated%0Aby%20this%20result%2C%20we%20propose%20an%20asymmetric%20bounding%20box%20regression%20loss%20that%0Aencourages%20large%20over%20small%20predicted%20bounding%20boxes.%20Our%20evaluation%20study%0Ashows%20that%20object%20detectors%20fine-tuned%20with%20the%20asymmetric%20loss%20are%20better%0Aaligned%20with%20human%20preference%20and%20are%20preferred%20over%20fixed%20scaling%20factors.%20A%0Aqualitative%20evaluation%20shows%20that%20human%20preference%20might%20be%20influenced%20by%20some%0Aobject%20characteristics%2C%20like%20object%20shape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Object%2520Detector%2520Bounding%2520Boxes%2520with%2520Human%2520Preference%26entry.906535625%3DOmbretta%2520Strafforello%2520and%2520Osman%2520S.%2520Kayhan%2520and%2520Oana%2520Inel%2520and%2520Klamer%2520Schutte%2520and%2520Jan%2520van%2520Gemert%26entry.1292438233%3D%2520%2520Previous%2520work%2520shows%2520that%2520humans%2520tend%2520to%2520prefer%2520large%2520bounding%2520boxes%2520over%250Asmall%2520bounding%2520boxes%2520with%2520the%2520same%2520IoU.%2520However%252C%2520we%2520show%2520here%2520that%2520commonly%250Aused%2520object%2520detectors%2520predict%2520large%2520and%2520small%2520boxes%2520equally%2520often.%2520In%2520this%250Awork%252C%2520we%2520investigate%2520how%2520to%2520align%2520automatically%2520detected%2520object%2520boxes%2520with%250Ahuman%2520preference%2520and%2520study%2520whether%2520this%2520improves%2520human%2520quality%2520perception.%2520We%250Aevaluate%2520the%2520performance%2520of%2520three%2520commonly%2520used%2520object%2520detectors%2520through%2520a%2520user%250Astudy%2520%2528N%2520%253D%2520123%2529.%2520We%2520find%2520that%2520humans%2520prefer%2520object%2520detections%2520that%2520are%2520upscaled%250Awith%2520factors%2520of%25201.5%2520or%25202%252C%2520even%2520if%2520the%2520corresponding%2520AP%2520is%2520close%2520to%25200.%2520Motivated%250Aby%2520this%2520result%252C%2520we%2520propose%2520an%2520asymmetric%2520bounding%2520box%2520regression%2520loss%2520that%250Aencourages%2520large%2520over%2520small%2520predicted%2520bounding%2520boxes.%2520Our%2520evaluation%2520study%250Ashows%2520that%2520object%2520detectors%2520fine-tuned%2520with%2520the%2520asymmetric%2520loss%2520are%2520better%250Aaligned%2520with%2520human%2520preference%2520and%2520are%2520preferred%2520over%2520fixed%2520scaling%2520factors.%2520A%250Aqualitative%2520evaluation%2520shows%2520that%2520human%2520preference%2520might%2520be%2520influenced%2520by%2520some%250Aobject%2520characteristics%252C%2520like%2520object%2520shape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Object%20Detector%20Bounding%20Boxes%20with%20Human%20Preference&entry.906535625=Ombretta%20Strafforello%20and%20Osman%20S.%20Kayhan%20and%20Oana%20Inel%20and%20Klamer%20Schutte%20and%20Jan%20van%20Gemert&entry.1292438233=%20%20Previous%20work%20shows%20that%20humans%20tend%20to%20prefer%20large%20bounding%20boxes%20over%0Asmall%20bounding%20boxes%20with%20the%20same%20IoU.%20However%2C%20we%20show%20here%20that%20commonly%0Aused%20object%20detectors%20predict%20large%20and%20small%20boxes%20equally%20often.%20In%20this%0Awork%2C%20we%20investigate%20how%20to%20align%20automatically%20detected%20object%20boxes%20with%0Ahuman%20preference%20and%20study%20whether%20this%20improves%20human%20quality%20perception.%20We%0Aevaluate%20the%20performance%20of%20three%20commonly%20used%20object%20detectors%20through%20a%20user%0Astudy%20%28N%20%3D%20123%29.%20We%20find%20that%20humans%20prefer%20object%20detections%20that%20are%20upscaled%0Awith%20factors%20of%201.5%20or%202%2C%20even%20if%20the%20corresponding%20AP%20is%20close%20to%200.%20Motivated%0Aby%20this%20result%2C%20we%20propose%20an%20asymmetric%20bounding%20box%20regression%20loss%20that%0Aencourages%20large%20over%20small%20predicted%20bounding%20boxes.%20Our%20evaluation%20study%0Ashows%20that%20object%20detectors%20fine-tuned%20with%20the%20asymmetric%20loss%20are%20better%0Aaligned%20with%20human%20preference%20and%20are%20preferred%20over%20fixed%20scaling%20factors.%20A%0Aqualitative%20evaluation%20shows%20that%20human%20preference%20might%20be%20influenced%20by%20some%0Aobject%20characteristics%2C%20like%20object%20shape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10844v1&entry.124074799=Read"},
{"title": "Detection of Intracranial Hemorrhage for Trauma Patients", "author": "Antoine P. Sanner and Nils F. Grauhan and Marc A. Brockmann and Ahmed E. Othman and Anirban Mukhopadhyay", "abstract": "  Whole-body CT is used for multi-trauma patients in the search of any and all\ninjuries. Since an initial assessment needs to be rapid and the search for\nlesions is done for the whole body, very little time can be allocated for the\ninspection of a specific anatomy. In particular, intracranial hemorrhages are\nstill missed, especially by clinical students. In this work, we present a Deep\nLearning approach for highlighting such lesions to improve the diagnostic\naccuracy. While most works on intracranial hemorrhages perform segmentation,\ndetection only requires bounding boxes for the localization of the bleeding. In\nthis paper, we propose a novel Voxel-Complete IoU (VC-IoU) loss that encourages\nthe network to learn the 3D aspect ratios of bounding boxes and leads to more\nprecise detections. We extensively experiment on brain bleeding detection using\na publicly available dataset, and validate it on a private cohort, where we\nachieve 0.877 AR30, 0.728 AP30, and 0.653 AR30, 0.514 AP30 respectively. These\nresults constitute a relative +5% improvement in Average Recall for both\ndatasets compared to other loss functions. Finally, as there is little data\ncurrently publicly available for 3D object detection and as annotation\nresources are limited in the clinical setting, we evaluate the cost of\ndifferent annotation methods, as well as the impact of imprecise bounding boxes\nin the training data on the detection performance.\n", "link": "http://arxiv.org/abs/2408.10768v1", "date": "2024-08-20", "relevancy": 1.9513, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4985}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4888}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detection%20of%20Intracranial%20Hemorrhage%20for%20Trauma%20Patients&body=Title%3A%20Detection%20of%20Intracranial%20Hemorrhage%20for%20Trauma%20Patients%0AAuthor%3A%20Antoine%20P.%20Sanner%20and%20Nils%20F.%20Grauhan%20and%20Marc%20A.%20Brockmann%20and%20Ahmed%20E.%20Othman%20and%20Anirban%20Mukhopadhyay%0AAbstract%3A%20%20%20Whole-body%20CT%20is%20used%20for%20multi-trauma%20patients%20in%20the%20search%20of%20any%20and%20all%0Ainjuries.%20Since%20an%20initial%20assessment%20needs%20to%20be%20rapid%20and%20the%20search%20for%0Alesions%20is%20done%20for%20the%20whole%20body%2C%20very%20little%20time%20can%20be%20allocated%20for%20the%0Ainspection%20of%20a%20specific%20anatomy.%20In%20particular%2C%20intracranial%20hemorrhages%20are%0Astill%20missed%2C%20especially%20by%20clinical%20students.%20In%20this%20work%2C%20we%20present%20a%20Deep%0ALearning%20approach%20for%20highlighting%20such%20lesions%20to%20improve%20the%20diagnostic%0Aaccuracy.%20While%20most%20works%20on%20intracranial%20hemorrhages%20perform%20segmentation%2C%0Adetection%20only%20requires%20bounding%20boxes%20for%20the%20localization%20of%20the%20bleeding.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20Voxel-Complete%20IoU%20%28VC-IoU%29%20loss%20that%20encourages%0Athe%20network%20to%20learn%20the%203D%20aspect%20ratios%20of%20bounding%20boxes%20and%20leads%20to%20more%0Aprecise%20detections.%20We%20extensively%20experiment%20on%20brain%20bleeding%20detection%20using%0Aa%20publicly%20available%20dataset%2C%20and%20validate%20it%20on%20a%20private%20cohort%2C%20where%20we%0Aachieve%200.877%20AR30%2C%200.728%20AP30%2C%20and%200.653%20AR30%2C%200.514%20AP30%20respectively.%20These%0Aresults%20constitute%20a%20relative%20%2B5%25%20improvement%20in%20Average%20Recall%20for%20both%0Adatasets%20compared%20to%20other%20loss%20functions.%20Finally%2C%20as%20there%20is%20little%20data%0Acurrently%20publicly%20available%20for%203D%20object%20detection%20and%20as%20annotation%0Aresources%20are%20limited%20in%20the%20clinical%20setting%2C%20we%20evaluate%20the%20cost%20of%0Adifferent%20annotation%20methods%2C%20as%20well%20as%20the%20impact%20of%20imprecise%20bounding%20boxes%0Ain%20the%20training%20data%20on%20the%20detection%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetection%2520of%2520Intracranial%2520Hemorrhage%2520for%2520Trauma%2520Patients%26entry.906535625%3DAntoine%2520P.%2520Sanner%2520and%2520Nils%2520F.%2520Grauhan%2520and%2520Marc%2520A.%2520Brockmann%2520and%2520Ahmed%2520E.%2520Othman%2520and%2520Anirban%2520Mukhopadhyay%26entry.1292438233%3D%2520%2520Whole-body%2520CT%2520is%2520used%2520for%2520multi-trauma%2520patients%2520in%2520the%2520search%2520of%2520any%2520and%2520all%250Ainjuries.%2520Since%2520an%2520initial%2520assessment%2520needs%2520to%2520be%2520rapid%2520and%2520the%2520search%2520for%250Alesions%2520is%2520done%2520for%2520the%2520whole%2520body%252C%2520very%2520little%2520time%2520can%2520be%2520allocated%2520for%2520the%250Ainspection%2520of%2520a%2520specific%2520anatomy.%2520In%2520particular%252C%2520intracranial%2520hemorrhages%2520are%250Astill%2520missed%252C%2520especially%2520by%2520clinical%2520students.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520Deep%250ALearning%2520approach%2520for%2520highlighting%2520such%2520lesions%2520to%2520improve%2520the%2520diagnostic%250Aaccuracy.%2520While%2520most%2520works%2520on%2520intracranial%2520hemorrhages%2520perform%2520segmentation%252C%250Adetection%2520only%2520requires%2520bounding%2520boxes%2520for%2520the%2520localization%2520of%2520the%2520bleeding.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Voxel-Complete%2520IoU%2520%2528VC-IoU%2529%2520loss%2520that%2520encourages%250Athe%2520network%2520to%2520learn%2520the%25203D%2520aspect%2520ratios%2520of%2520bounding%2520boxes%2520and%2520leads%2520to%2520more%250Aprecise%2520detections.%2520We%2520extensively%2520experiment%2520on%2520brain%2520bleeding%2520detection%2520using%250Aa%2520publicly%2520available%2520dataset%252C%2520and%2520validate%2520it%2520on%2520a%2520private%2520cohort%252C%2520where%2520we%250Aachieve%25200.877%2520AR30%252C%25200.728%2520AP30%252C%2520and%25200.653%2520AR30%252C%25200.514%2520AP30%2520respectively.%2520These%250Aresults%2520constitute%2520a%2520relative%2520%252B5%2525%2520improvement%2520in%2520Average%2520Recall%2520for%2520both%250Adatasets%2520compared%2520to%2520other%2520loss%2520functions.%2520Finally%252C%2520as%2520there%2520is%2520little%2520data%250Acurrently%2520publicly%2520available%2520for%25203D%2520object%2520detection%2520and%2520as%2520annotation%250Aresources%2520are%2520limited%2520in%2520the%2520clinical%2520setting%252C%2520we%2520evaluate%2520the%2520cost%2520of%250Adifferent%2520annotation%2520methods%252C%2520as%2520well%2520as%2520the%2520impact%2520of%2520imprecise%2520bounding%2520boxes%250Ain%2520the%2520training%2520data%2520on%2520the%2520detection%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detection%20of%20Intracranial%20Hemorrhage%20for%20Trauma%20Patients&entry.906535625=Antoine%20P.%20Sanner%20and%20Nils%20F.%20Grauhan%20and%20Marc%20A.%20Brockmann%20and%20Ahmed%20E.%20Othman%20and%20Anirban%20Mukhopadhyay&entry.1292438233=%20%20Whole-body%20CT%20is%20used%20for%20multi-trauma%20patients%20in%20the%20search%20of%20any%20and%20all%0Ainjuries.%20Since%20an%20initial%20assessment%20needs%20to%20be%20rapid%20and%20the%20search%20for%0Alesions%20is%20done%20for%20the%20whole%20body%2C%20very%20little%20time%20can%20be%20allocated%20for%20the%0Ainspection%20of%20a%20specific%20anatomy.%20In%20particular%2C%20intracranial%20hemorrhages%20are%0Astill%20missed%2C%20especially%20by%20clinical%20students.%20In%20this%20work%2C%20we%20present%20a%20Deep%0ALearning%20approach%20for%20highlighting%20such%20lesions%20to%20improve%20the%20diagnostic%0Aaccuracy.%20While%20most%20works%20on%20intracranial%20hemorrhages%20perform%20segmentation%2C%0Adetection%20only%20requires%20bounding%20boxes%20for%20the%20localization%20of%20the%20bleeding.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20Voxel-Complete%20IoU%20%28VC-IoU%29%20loss%20that%20encourages%0Athe%20network%20to%20learn%20the%203D%20aspect%20ratios%20of%20bounding%20boxes%20and%20leads%20to%20more%0Aprecise%20detections.%20We%20extensively%20experiment%20on%20brain%20bleeding%20detection%20using%0Aa%20publicly%20available%20dataset%2C%20and%20validate%20it%20on%20a%20private%20cohort%2C%20where%20we%0Aachieve%200.877%20AR30%2C%200.728%20AP30%2C%20and%200.653%20AR30%2C%200.514%20AP30%20respectively.%20These%0Aresults%20constitute%20a%20relative%20%2B5%25%20improvement%20in%20Average%20Recall%20for%20both%0Adatasets%20compared%20to%20other%20loss%20functions.%20Finally%2C%20as%20there%20is%20little%20data%0Acurrently%20publicly%20available%20for%203D%20object%20detection%20and%20as%20annotation%0Aresources%20are%20limited%20in%20the%20clinical%20setting%2C%20we%20evaluate%20the%20cost%20of%0Adifferent%20annotation%20methods%2C%20as%20well%20as%20the%20impact%20of%20imprecise%20bounding%20boxes%0Ain%20the%20training%20data%20on%20the%20detection%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10768v1&entry.124074799=Read"},
{"title": "Online SLA Decomposition: Enabling Real-Time Adaptation to Evolving\n  Systems", "author": "Cyril Shih-Huan Hsu and Danny De Vleeschauwer and Chrysa Papagianni", "abstract": "  When a network slice spans multiple domains, each domain must uphold the\nEnd-to-End (E2E) Service Level Agreement (SLA) associated with the slice. This\nrequires decomposing the E2E SLA into partial SLAs for each domain. In a\ntwo-level network slicing management system with an E2E orchestrator and local\ncontrollers, we propose an online learning-decomposition framework that\ndynamically updates risk models using recent feedback. This approach utilizes\nonline gradient descent and FIFO memory buffers to enhance stability and\nrobustness. Our empirical study shows the proposed framework outperforms\nstate-of-the-art static methods, offering more accurate and resilient SLA\ndecomposition under varying conditions and sparse data.\n", "link": "http://arxiv.org/abs/2408.08968v2", "date": "2024-08-20", "relevancy": 1.9512, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.496}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.484}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20SLA%20Decomposition%3A%20Enabling%20Real-Time%20Adaptation%20to%20Evolving%0A%20%20Systems&body=Title%3A%20Online%20SLA%20Decomposition%3A%20Enabling%20Real-Time%20Adaptation%20to%20Evolving%0A%20%20Systems%0AAuthor%3A%20Cyril%20Shih-Huan%20Hsu%20and%20Danny%20De%20Vleeschauwer%20and%20Chrysa%20Papagianni%0AAbstract%3A%20%20%20When%20a%20network%20slice%20spans%20multiple%20domains%2C%20each%20domain%20must%20uphold%20the%0AEnd-to-End%20%28E2E%29%20Service%20Level%20Agreement%20%28SLA%29%20associated%20with%20the%20slice.%20This%0Arequires%20decomposing%20the%20E2E%20SLA%20into%20partial%20SLAs%20for%20each%20domain.%20In%20a%0Atwo-level%20network%20slicing%20management%20system%20with%20an%20E2E%20orchestrator%20and%20local%0Acontrollers%2C%20we%20propose%20an%20online%20learning-decomposition%20framework%20that%0Adynamically%20updates%20risk%20models%20using%20recent%20feedback.%20This%20approach%20utilizes%0Aonline%20gradient%20descent%20and%20FIFO%20memory%20buffers%20to%20enhance%20stability%20and%0Arobustness.%20Our%20empirical%20study%20shows%20the%20proposed%20framework%20outperforms%0Astate-of-the-art%20static%20methods%2C%20offering%20more%20accurate%20and%20resilient%20SLA%0Adecomposition%20under%20varying%20conditions%20and%20sparse%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08968v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520SLA%2520Decomposition%253A%2520Enabling%2520Real-Time%2520Adaptation%2520to%2520Evolving%250A%2520%2520Systems%26entry.906535625%3DCyril%2520Shih-Huan%2520Hsu%2520and%2520Danny%2520De%2520Vleeschauwer%2520and%2520Chrysa%2520Papagianni%26entry.1292438233%3D%2520%2520When%2520a%2520network%2520slice%2520spans%2520multiple%2520domains%252C%2520each%2520domain%2520must%2520uphold%2520the%250AEnd-to-End%2520%2528E2E%2529%2520Service%2520Level%2520Agreement%2520%2528SLA%2529%2520associated%2520with%2520the%2520slice.%2520This%250Arequires%2520decomposing%2520the%2520E2E%2520SLA%2520into%2520partial%2520SLAs%2520for%2520each%2520domain.%2520In%2520a%250Atwo-level%2520network%2520slicing%2520management%2520system%2520with%2520an%2520E2E%2520orchestrator%2520and%2520local%250Acontrollers%252C%2520we%2520propose%2520an%2520online%2520learning-decomposition%2520framework%2520that%250Adynamically%2520updates%2520risk%2520models%2520using%2520recent%2520feedback.%2520This%2520approach%2520utilizes%250Aonline%2520gradient%2520descent%2520and%2520FIFO%2520memory%2520buffers%2520to%2520enhance%2520stability%2520and%250Arobustness.%2520Our%2520empirical%2520study%2520shows%2520the%2520proposed%2520framework%2520outperforms%250Astate-of-the-art%2520static%2520methods%252C%2520offering%2520more%2520accurate%2520and%2520resilient%2520SLA%250Adecomposition%2520under%2520varying%2520conditions%2520and%2520sparse%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08968v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20SLA%20Decomposition%3A%20Enabling%20Real-Time%20Adaptation%20to%20Evolving%0A%20%20Systems&entry.906535625=Cyril%20Shih-Huan%20Hsu%20and%20Danny%20De%20Vleeschauwer%20and%20Chrysa%20Papagianni&entry.1292438233=%20%20When%20a%20network%20slice%20spans%20multiple%20domains%2C%20each%20domain%20must%20uphold%20the%0AEnd-to-End%20%28E2E%29%20Service%20Level%20Agreement%20%28SLA%29%20associated%20with%20the%20slice.%20This%0Arequires%20decomposing%20the%20E2E%20SLA%20into%20partial%20SLAs%20for%20each%20domain.%20In%20a%0Atwo-level%20network%20slicing%20management%20system%20with%20an%20E2E%20orchestrator%20and%20local%0Acontrollers%2C%20we%20propose%20an%20online%20learning-decomposition%20framework%20that%0Adynamically%20updates%20risk%20models%20using%20recent%20feedback.%20This%20approach%20utilizes%0Aonline%20gradient%20descent%20and%20FIFO%20memory%20buffers%20to%20enhance%20stability%20and%0Arobustness.%20Our%20empirical%20study%20shows%20the%20proposed%20framework%20outperforms%0Astate-of-the-art%20static%20methods%2C%20offering%20more%20accurate%20and%20resilient%20SLA%0Adecomposition%20under%20varying%20conditions%20and%20sparse%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08968v2&entry.124074799=Read"},
{"title": "NeuralMatrix: Compute the Entire Neural Networks with Linear Matrix\n  Operations for Efficient Inference", "author": "Ruiqi Sun and Siwei Ye and Jie Zhao and Xin He and Jianzhe Lin and Yiran Li and An Zou", "abstract": "  The inherent diversity of computation types within the deep neural network\n(DNN) models often requires a variety of specialized units in hardware\nprocessors, which limits computational efficiency, increasing both inference\nlatency and power consumption, especially when the hardware processor needs to\nsupport and execute different neural networks. In this study, we introduce\nNeuralMatrix, which elastically transforms the computations of entire DNNs into\nlinear matrix operations. This transformation allows seamless execution of\nvarious DNN models all with matrix operations and paves the way for running\nversatile DNN models with a single General Matrix Multiplication (GEMM)\naccelerator.Extensive experiments with both CNN and transformer-based models\ndemonstrate the potential of NeuralMatrix to accurately and efficiently execute\na wide range of DNN models, achieving 2.17-38.72 times computation efficiency\n(i.e., throughput per power) compared to CPUs, GPUs, and SoC platforms. This\nlevel of efficiency is usually only attainable with the accelerator designed\nfor a specific neural network.\n", "link": "http://arxiv.org/abs/2305.14405v4", "date": "2024-08-20", "relevancy": 1.9415, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4968}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4945}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuralMatrix%3A%20Compute%20the%20Entire%20Neural%20Networks%20with%20Linear%20Matrix%0A%20%20Operations%20for%20Efficient%20Inference&body=Title%3A%20NeuralMatrix%3A%20Compute%20the%20Entire%20Neural%20Networks%20with%20Linear%20Matrix%0A%20%20Operations%20for%20Efficient%20Inference%0AAuthor%3A%20Ruiqi%20Sun%20and%20Siwei%20Ye%20and%20Jie%20Zhao%20and%20Xin%20He%20and%20Jianzhe%20Lin%20and%20Yiran%20Li%20and%20An%20Zou%0AAbstract%3A%20%20%20The%20inherent%20diversity%20of%20computation%20types%20within%20the%20deep%20neural%20network%0A%28DNN%29%20models%20often%20requires%20a%20variety%20of%20specialized%20units%20in%20hardware%0Aprocessors%2C%20which%20limits%20computational%20efficiency%2C%20increasing%20both%20inference%0Alatency%20and%20power%20consumption%2C%20especially%20when%20the%20hardware%20processor%20needs%20to%0Asupport%20and%20execute%20different%20neural%20networks.%20In%20this%20study%2C%20we%20introduce%0ANeuralMatrix%2C%20which%20elastically%20transforms%20the%20computations%20of%20entire%20DNNs%20into%0Alinear%20matrix%20operations.%20This%20transformation%20allows%20seamless%20execution%20of%0Avarious%20DNN%20models%20all%20with%20matrix%20operations%20and%20paves%20the%20way%20for%20running%0Aversatile%20DNN%20models%20with%20a%20single%20General%20Matrix%20Multiplication%20%28GEMM%29%0Aaccelerator.Extensive%20experiments%20with%20both%20CNN%20and%20transformer-based%20models%0Ademonstrate%20the%20potential%20of%20NeuralMatrix%20to%20accurately%20and%20efficiently%20execute%0Aa%20wide%20range%20of%20DNN%20models%2C%20achieving%202.17-38.72%20times%20computation%20efficiency%0A%28i.e.%2C%20throughput%20per%20power%29%20compared%20to%20CPUs%2C%20GPUs%2C%20and%20SoC%20platforms.%20This%0Alevel%20of%20efficiency%20is%20usually%20only%20attainable%20with%20the%20accelerator%20designed%0Afor%20a%20specific%20neural%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.14405v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuralMatrix%253A%2520Compute%2520the%2520Entire%2520Neural%2520Networks%2520with%2520Linear%2520Matrix%250A%2520%2520Operations%2520for%2520Efficient%2520Inference%26entry.906535625%3DRuiqi%2520Sun%2520and%2520Siwei%2520Ye%2520and%2520Jie%2520Zhao%2520and%2520Xin%2520He%2520and%2520Jianzhe%2520Lin%2520and%2520Yiran%2520Li%2520and%2520An%2520Zou%26entry.1292438233%3D%2520%2520The%2520inherent%2520diversity%2520of%2520computation%2520types%2520within%2520the%2520deep%2520neural%2520network%250A%2528DNN%2529%2520models%2520often%2520requires%2520a%2520variety%2520of%2520specialized%2520units%2520in%2520hardware%250Aprocessors%252C%2520which%2520limits%2520computational%2520efficiency%252C%2520increasing%2520both%2520inference%250Alatency%2520and%2520power%2520consumption%252C%2520especially%2520when%2520the%2520hardware%2520processor%2520needs%2520to%250Asupport%2520and%2520execute%2520different%2520neural%2520networks.%2520In%2520this%2520study%252C%2520we%2520introduce%250ANeuralMatrix%252C%2520which%2520elastically%2520transforms%2520the%2520computations%2520of%2520entire%2520DNNs%2520into%250Alinear%2520matrix%2520operations.%2520This%2520transformation%2520allows%2520seamless%2520execution%2520of%250Avarious%2520DNN%2520models%2520all%2520with%2520matrix%2520operations%2520and%2520paves%2520the%2520way%2520for%2520running%250Aversatile%2520DNN%2520models%2520with%2520a%2520single%2520General%2520Matrix%2520Multiplication%2520%2528GEMM%2529%250Aaccelerator.Extensive%2520experiments%2520with%2520both%2520CNN%2520and%2520transformer-based%2520models%250Ademonstrate%2520the%2520potential%2520of%2520NeuralMatrix%2520to%2520accurately%2520and%2520efficiently%2520execute%250Aa%2520wide%2520range%2520of%2520DNN%2520models%252C%2520achieving%25202.17-38.72%2520times%2520computation%2520efficiency%250A%2528i.e.%252C%2520throughput%2520per%2520power%2529%2520compared%2520to%2520CPUs%252C%2520GPUs%252C%2520and%2520SoC%2520platforms.%2520This%250Alevel%2520of%2520efficiency%2520is%2520usually%2520only%2520attainable%2520with%2520the%2520accelerator%2520designed%250Afor%2520a%2520specific%2520neural%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.14405v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuralMatrix%3A%20Compute%20the%20Entire%20Neural%20Networks%20with%20Linear%20Matrix%0A%20%20Operations%20for%20Efficient%20Inference&entry.906535625=Ruiqi%20Sun%20and%20Siwei%20Ye%20and%20Jie%20Zhao%20and%20Xin%20He%20and%20Jianzhe%20Lin%20and%20Yiran%20Li%20and%20An%20Zou&entry.1292438233=%20%20The%20inherent%20diversity%20of%20computation%20types%20within%20the%20deep%20neural%20network%0A%28DNN%29%20models%20often%20requires%20a%20variety%20of%20specialized%20units%20in%20hardware%0Aprocessors%2C%20which%20limits%20computational%20efficiency%2C%20increasing%20both%20inference%0Alatency%20and%20power%20consumption%2C%20especially%20when%20the%20hardware%20processor%20needs%20to%0Asupport%20and%20execute%20different%20neural%20networks.%20In%20this%20study%2C%20we%20introduce%0ANeuralMatrix%2C%20which%20elastically%20transforms%20the%20computations%20of%20entire%20DNNs%20into%0Alinear%20matrix%20operations.%20This%20transformation%20allows%20seamless%20execution%20of%0Avarious%20DNN%20models%20all%20with%20matrix%20operations%20and%20paves%20the%20way%20for%20running%0Aversatile%20DNN%20models%20with%20a%20single%20General%20Matrix%20Multiplication%20%28GEMM%29%0Aaccelerator.Extensive%20experiments%20with%20both%20CNN%20and%20transformer-based%20models%0Ademonstrate%20the%20potential%20of%20NeuralMatrix%20to%20accurately%20and%20efficiently%20execute%0Aa%20wide%20range%20of%20DNN%20models%2C%20achieving%202.17-38.72%20times%20computation%20efficiency%0A%28i.e.%2C%20throughput%20per%20power%29%20compared%20to%20CPUs%2C%20GPUs%2C%20and%20SoC%20platforms.%20This%0Alevel%20of%20efficiency%20is%20usually%20only%20attainable%20with%20the%20accelerator%20designed%0Afor%20a%20specific%20neural%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.14405v4&entry.124074799=Read"},
{"title": "MegaFusion: Extend Diffusion Models towards Higher-resolution Image\n  Generation without Further Tuning", "author": "Haoning Wu and Shaocheng Shen and Qiang Hu and Xiaoyun Zhang and Ya Zhang and Yanfeng Wang", "abstract": "  Diffusion models have emerged as frontrunners in text-to-image generation for\ntheir impressive capabilities. Nonetheless, their fixed image resolution during\ntraining often leads to challenges in high-resolution image generation, such as\nsemantic inaccuracies and object replication. This paper introduces MegaFusion,\na novel approach that extends existing diffusion-based text-to-image generation\nmodels towards efficient higher-resolution generation without additional\nfine-tuning or extra adaptation. Specifically, we employ an innovative truncate\nand relay strategy to bridge the denoising processes across different\nresolutions, allowing for high-resolution image generation in a coarse-to-fine\nmanner. Moreover, by integrating dilated convolutions and noise re-scheduling,\nwe further adapt the model's priors for higher resolution. The versatility and\nefficacy of MegaFusion make it universally applicable to both latent-space and\npixel-space diffusion models, along with other derivative models. Extensive\nexperiments confirm that MegaFusion significantly boosts the capability of\nexisting models to produce images of megapixels and various aspect ratios,\nwhile only requiring about 40% of the original computational cost.\n", "link": "http://arxiv.org/abs/2408.11001v1", "date": "2024-08-20", "relevancy": 1.9398, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6708}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6479}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MegaFusion%3A%20Extend%20Diffusion%20Models%20towards%20Higher-resolution%20Image%0A%20%20Generation%20without%20Further%20Tuning&body=Title%3A%20MegaFusion%3A%20Extend%20Diffusion%20Models%20towards%20Higher-resolution%20Image%0A%20%20Generation%20without%20Further%20Tuning%0AAuthor%3A%20Haoning%20Wu%20and%20Shaocheng%20Shen%20and%20Qiang%20Hu%20and%20Xiaoyun%20Zhang%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20emerged%20as%20frontrunners%20in%20text-to-image%20generation%20for%0Atheir%20impressive%20capabilities.%20Nonetheless%2C%20their%20fixed%20image%20resolution%20during%0Atraining%20often%20leads%20to%20challenges%20in%20high-resolution%20image%20generation%2C%20such%20as%0Asemantic%20inaccuracies%20and%20object%20replication.%20This%20paper%20introduces%20MegaFusion%2C%0Aa%20novel%20approach%20that%20extends%20existing%20diffusion-based%20text-to-image%20generation%0Amodels%20towards%20efficient%20higher-resolution%20generation%20without%20additional%0Afine-tuning%20or%20extra%20adaptation.%20Specifically%2C%20we%20employ%20an%20innovative%20truncate%0Aand%20relay%20strategy%20to%20bridge%20the%20denoising%20processes%20across%20different%0Aresolutions%2C%20allowing%20for%20high-resolution%20image%20generation%20in%20a%20coarse-to-fine%0Amanner.%20Moreover%2C%20by%20integrating%20dilated%20convolutions%20and%20noise%20re-scheduling%2C%0Awe%20further%20adapt%20the%20model%27s%20priors%20for%20higher%20resolution.%20The%20versatility%20and%0Aefficacy%20of%20MegaFusion%20make%20it%20universally%20applicable%20to%20both%20latent-space%20and%0Apixel-space%20diffusion%20models%2C%20along%20with%20other%20derivative%20models.%20Extensive%0Aexperiments%20confirm%20that%20MegaFusion%20significantly%20boosts%20the%20capability%20of%0Aexisting%20models%20to%20produce%20images%20of%20megapixels%20and%20various%20aspect%20ratios%2C%0Awhile%20only%20requiring%20about%2040%25%20of%20the%20original%20computational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMegaFusion%253A%2520Extend%2520Diffusion%2520Models%2520towards%2520Higher-resolution%2520Image%250A%2520%2520Generation%2520without%2520Further%2520Tuning%26entry.906535625%3DHaoning%2520Wu%2520and%2520Shaocheng%2520Shen%2520and%2520Qiang%2520Hu%2520and%2520Xiaoyun%2520Zhang%2520and%2520Ya%2520Zhang%2520and%2520Yanfeng%2520Wang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520emerged%2520as%2520frontrunners%2520in%2520text-to-image%2520generation%2520for%250Atheir%2520impressive%2520capabilities.%2520Nonetheless%252C%2520their%2520fixed%2520image%2520resolution%2520during%250Atraining%2520often%2520leads%2520to%2520challenges%2520in%2520high-resolution%2520image%2520generation%252C%2520such%2520as%250Asemantic%2520inaccuracies%2520and%2520object%2520replication.%2520This%2520paper%2520introduces%2520MegaFusion%252C%250Aa%2520novel%2520approach%2520that%2520extends%2520existing%2520diffusion-based%2520text-to-image%2520generation%250Amodels%2520towards%2520efficient%2520higher-resolution%2520generation%2520without%2520additional%250Afine-tuning%2520or%2520extra%2520adaptation.%2520Specifically%252C%2520we%2520employ%2520an%2520innovative%2520truncate%250Aand%2520relay%2520strategy%2520to%2520bridge%2520the%2520denoising%2520processes%2520across%2520different%250Aresolutions%252C%2520allowing%2520for%2520high-resolution%2520image%2520generation%2520in%2520a%2520coarse-to-fine%250Amanner.%2520Moreover%252C%2520by%2520integrating%2520dilated%2520convolutions%2520and%2520noise%2520re-scheduling%252C%250Awe%2520further%2520adapt%2520the%2520model%2527s%2520priors%2520for%2520higher%2520resolution.%2520The%2520versatility%2520and%250Aefficacy%2520of%2520MegaFusion%2520make%2520it%2520universally%2520applicable%2520to%2520both%2520latent-space%2520and%250Apixel-space%2520diffusion%2520models%252C%2520along%2520with%2520other%2520derivative%2520models.%2520Extensive%250Aexperiments%2520confirm%2520that%2520MegaFusion%2520significantly%2520boosts%2520the%2520capability%2520of%250Aexisting%2520models%2520to%2520produce%2520images%2520of%2520megapixels%2520and%2520various%2520aspect%2520ratios%252C%250Awhile%2520only%2520requiring%2520about%252040%2525%2520of%2520the%2520original%2520computational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MegaFusion%3A%20Extend%20Diffusion%20Models%20towards%20Higher-resolution%20Image%0A%20%20Generation%20without%20Further%20Tuning&entry.906535625=Haoning%20Wu%20and%20Shaocheng%20Shen%20and%20Qiang%20Hu%20and%20Xiaoyun%20Zhang%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang&entry.1292438233=%20%20Diffusion%20models%20have%20emerged%20as%20frontrunners%20in%20text-to-image%20generation%20for%0Atheir%20impressive%20capabilities.%20Nonetheless%2C%20their%20fixed%20image%20resolution%20during%0Atraining%20often%20leads%20to%20challenges%20in%20high-resolution%20image%20generation%2C%20such%20as%0Asemantic%20inaccuracies%20and%20object%20replication.%20This%20paper%20introduces%20MegaFusion%2C%0Aa%20novel%20approach%20that%20extends%20existing%20diffusion-based%20text-to-image%20generation%0Amodels%20towards%20efficient%20higher-resolution%20generation%20without%20additional%0Afine-tuning%20or%20extra%20adaptation.%20Specifically%2C%20we%20employ%20an%20innovative%20truncate%0Aand%20relay%20strategy%20to%20bridge%20the%20denoising%20processes%20across%20different%0Aresolutions%2C%20allowing%20for%20high-resolution%20image%20generation%20in%20a%20coarse-to-fine%0Amanner.%20Moreover%2C%20by%20integrating%20dilated%20convolutions%20and%20noise%20re-scheduling%2C%0Awe%20further%20adapt%20the%20model%27s%20priors%20for%20higher%20resolution.%20The%20versatility%20and%0Aefficacy%20of%20MegaFusion%20make%20it%20universally%20applicable%20to%20both%20latent-space%20and%0Apixel-space%20diffusion%20models%2C%20along%20with%20other%20derivative%20models.%20Extensive%0Aexperiments%20confirm%20that%20MegaFusion%20significantly%20boosts%20the%20capability%20of%0Aexisting%20models%20to%20produce%20images%20of%20megapixels%20and%20various%20aspect%20ratios%2C%0Awhile%20only%20requiring%20about%2040%25%20of%20the%20original%20computational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11001v1&entry.124074799=Read"},
{"title": "Which Side Are You On? A Multi-task Dataset for End-to-End Argument\n  Summarisation and Evaluation", "author": "Hao Li and Yuping Wu and Viktor Schlegel and Riza Batista-Navarro and Tharindu Madusanka and Iqra Zahid and Jiayan Zeng and Xiaochi Wang and Xinran He and Yizhi Li and Goran Nenadic", "abstract": "  With the recent advances of large language models (LLMs), it is no longer\ninfeasible to build an automated debate system that helps people to synthesise\npersuasive arguments. Previous work attempted this task by integrating multiple\ncomponents. In our work, we introduce an argument mining dataset that captures\nthe end-to-end process of preparing an argumentative essay for a debate, which\ncovers the tasks of claim and evidence identification (Task 1 ED), evidence\nconvincingness ranking (Task 2 ECR), argumentative essay summarisation and\nhuman preference ranking (Task 3 ASR) and metric learning for automated\nevaluation of resulting essays, based on human feedback along argument quality\ndimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are\nfully annotated with the various properties supporting the aforementioned\ntasks. We evaluate multiple generative baselines for each of these tasks,\nincluding representative LLMs. We find, that while they show promising results\non individual tasks in our benchmark, their end-to-end performance on all four\ntasks in succession deteriorates significantly, both in automated measures as\nwell as in human-centred evaluation. This challenge presented by our proposed\ndataset motivates future research on end-to-end argument mining and\nsummarisation. The repository of this project is available at\nhttps://github.com/HaoBytes/ArgSum-Datatset\n", "link": "http://arxiv.org/abs/2406.03151v3", "date": "2024-08-20", "relevancy": 1.9261, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4956}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4835}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Which%20Side%20Are%20You%20On%3F%20A%20Multi-task%20Dataset%20for%20End-to-End%20Argument%0A%20%20Summarisation%20and%20Evaluation&body=Title%3A%20Which%20Side%20Are%20You%20On%3F%20A%20Multi-task%20Dataset%20for%20End-to-End%20Argument%0A%20%20Summarisation%20and%20Evaluation%0AAuthor%3A%20Hao%20Li%20and%20Yuping%20Wu%20and%20Viktor%20Schlegel%20and%20Riza%20Batista-Navarro%20and%20Tharindu%20Madusanka%20and%20Iqra%20Zahid%20and%20Jiayan%20Zeng%20and%20Xiaochi%20Wang%20and%20Xinran%20He%20and%20Yizhi%20Li%20and%20Goran%20Nenadic%0AAbstract%3A%20%20%20With%20the%20recent%20advances%20of%20large%20language%20models%20%28LLMs%29%2C%20it%20is%20no%20longer%0Ainfeasible%20to%20build%20an%20automated%20debate%20system%20that%20helps%20people%20to%20synthesise%0Apersuasive%20arguments.%20Previous%20work%20attempted%20this%20task%20by%20integrating%20multiple%0Acomponents.%20In%20our%20work%2C%20we%20introduce%20an%20argument%20mining%20dataset%20that%20captures%0Athe%20end-to-end%20process%20of%20preparing%20an%20argumentative%20essay%20for%20a%20debate%2C%20which%0Acovers%20the%20tasks%20of%20claim%20and%20evidence%20identification%20%28Task%201%20ED%29%2C%20evidence%0Aconvincingness%20ranking%20%28Task%202%20ECR%29%2C%20argumentative%20essay%20summarisation%20and%0Ahuman%20preference%20ranking%20%28Task%203%20ASR%29%20and%20metric%20learning%20for%20automated%0Aevaluation%20of%20resulting%20essays%2C%20based%20on%20human%20feedback%20along%20argument%20quality%0Adimensions%20%28Task%204%20SQE%29.%20Our%20dataset%20contains%2014k%20examples%20of%20claims%20that%20are%0Afully%20annotated%20with%20the%20various%20properties%20supporting%20the%20aforementioned%0Atasks.%20We%20evaluate%20multiple%20generative%20baselines%20for%20each%20of%20these%20tasks%2C%0Aincluding%20representative%20LLMs.%20We%20find%2C%20that%20while%20they%20show%20promising%20results%0Aon%20individual%20tasks%20in%20our%20benchmark%2C%20their%20end-to-end%20performance%20on%20all%20four%0Atasks%20in%20succession%20deteriorates%20significantly%2C%20both%20in%20automated%20measures%20as%0Awell%20as%20in%20human-centred%20evaluation.%20This%20challenge%20presented%20by%20our%20proposed%0Adataset%20motivates%20future%20research%20on%20end-to-end%20argument%20mining%20and%0Asummarisation.%20The%20repository%20of%20this%20project%20is%20available%20at%0Ahttps%3A//github.com/HaoBytes/ArgSum-Datatset%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03151v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhich%2520Side%2520Are%2520You%2520On%253F%2520A%2520Multi-task%2520Dataset%2520for%2520End-to-End%2520Argument%250A%2520%2520Summarisation%2520and%2520Evaluation%26entry.906535625%3DHao%2520Li%2520and%2520Yuping%2520Wu%2520and%2520Viktor%2520Schlegel%2520and%2520Riza%2520Batista-Navarro%2520and%2520Tharindu%2520Madusanka%2520and%2520Iqra%2520Zahid%2520and%2520Jiayan%2520Zeng%2520and%2520Xiaochi%2520Wang%2520and%2520Xinran%2520He%2520and%2520Yizhi%2520Li%2520and%2520Goran%2520Nenadic%26entry.1292438233%3D%2520%2520With%2520the%2520recent%2520advances%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520it%2520is%2520no%2520longer%250Ainfeasible%2520to%2520build%2520an%2520automated%2520debate%2520system%2520that%2520helps%2520people%2520to%2520synthesise%250Apersuasive%2520arguments.%2520Previous%2520work%2520attempted%2520this%2520task%2520by%2520integrating%2520multiple%250Acomponents.%2520In%2520our%2520work%252C%2520we%2520introduce%2520an%2520argument%2520mining%2520dataset%2520that%2520captures%250Athe%2520end-to-end%2520process%2520of%2520preparing%2520an%2520argumentative%2520essay%2520for%2520a%2520debate%252C%2520which%250Acovers%2520the%2520tasks%2520of%2520claim%2520and%2520evidence%2520identification%2520%2528Task%25201%2520ED%2529%252C%2520evidence%250Aconvincingness%2520ranking%2520%2528Task%25202%2520ECR%2529%252C%2520argumentative%2520essay%2520summarisation%2520and%250Ahuman%2520preference%2520ranking%2520%2528Task%25203%2520ASR%2529%2520and%2520metric%2520learning%2520for%2520automated%250Aevaluation%2520of%2520resulting%2520essays%252C%2520based%2520on%2520human%2520feedback%2520along%2520argument%2520quality%250Adimensions%2520%2528Task%25204%2520SQE%2529.%2520Our%2520dataset%2520contains%252014k%2520examples%2520of%2520claims%2520that%2520are%250Afully%2520annotated%2520with%2520the%2520various%2520properties%2520supporting%2520the%2520aforementioned%250Atasks.%2520We%2520evaluate%2520multiple%2520generative%2520baselines%2520for%2520each%2520of%2520these%2520tasks%252C%250Aincluding%2520representative%2520LLMs.%2520We%2520find%252C%2520that%2520while%2520they%2520show%2520promising%2520results%250Aon%2520individual%2520tasks%2520in%2520our%2520benchmark%252C%2520their%2520end-to-end%2520performance%2520on%2520all%2520four%250Atasks%2520in%2520succession%2520deteriorates%2520significantly%252C%2520both%2520in%2520automated%2520measures%2520as%250Awell%2520as%2520in%2520human-centred%2520evaluation.%2520This%2520challenge%2520presented%2520by%2520our%2520proposed%250Adataset%2520motivates%2520future%2520research%2520on%2520end-to-end%2520argument%2520mining%2520and%250Asummarisation.%2520The%2520repository%2520of%2520this%2520project%2520is%2520available%2520at%250Ahttps%253A//github.com/HaoBytes/ArgSum-Datatset%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03151v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Which%20Side%20Are%20You%20On%3F%20A%20Multi-task%20Dataset%20for%20End-to-End%20Argument%0A%20%20Summarisation%20and%20Evaluation&entry.906535625=Hao%20Li%20and%20Yuping%20Wu%20and%20Viktor%20Schlegel%20and%20Riza%20Batista-Navarro%20and%20Tharindu%20Madusanka%20and%20Iqra%20Zahid%20and%20Jiayan%20Zeng%20and%20Xiaochi%20Wang%20and%20Xinran%20He%20and%20Yizhi%20Li%20and%20Goran%20Nenadic&entry.1292438233=%20%20With%20the%20recent%20advances%20of%20large%20language%20models%20%28LLMs%29%2C%20it%20is%20no%20longer%0Ainfeasible%20to%20build%20an%20automated%20debate%20system%20that%20helps%20people%20to%20synthesise%0Apersuasive%20arguments.%20Previous%20work%20attempted%20this%20task%20by%20integrating%20multiple%0Acomponents.%20In%20our%20work%2C%20we%20introduce%20an%20argument%20mining%20dataset%20that%20captures%0Athe%20end-to-end%20process%20of%20preparing%20an%20argumentative%20essay%20for%20a%20debate%2C%20which%0Acovers%20the%20tasks%20of%20claim%20and%20evidence%20identification%20%28Task%201%20ED%29%2C%20evidence%0Aconvincingness%20ranking%20%28Task%202%20ECR%29%2C%20argumentative%20essay%20summarisation%20and%0Ahuman%20preference%20ranking%20%28Task%203%20ASR%29%20and%20metric%20learning%20for%20automated%0Aevaluation%20of%20resulting%20essays%2C%20based%20on%20human%20feedback%20along%20argument%20quality%0Adimensions%20%28Task%204%20SQE%29.%20Our%20dataset%20contains%2014k%20examples%20of%20claims%20that%20are%0Afully%20annotated%20with%20the%20various%20properties%20supporting%20the%20aforementioned%0Atasks.%20We%20evaluate%20multiple%20generative%20baselines%20for%20each%20of%20these%20tasks%2C%0Aincluding%20representative%20LLMs.%20We%20find%2C%20that%20while%20they%20show%20promising%20results%0Aon%20individual%20tasks%20in%20our%20benchmark%2C%20their%20end-to-end%20performance%20on%20all%20four%0Atasks%20in%20succession%20deteriorates%20significantly%2C%20both%20in%20automated%20measures%20as%0Awell%20as%20in%20human-centred%20evaluation.%20This%20challenge%20presented%20by%20our%20proposed%0Adataset%20motivates%20future%20research%20on%20end-to-end%20argument%20mining%20and%0Asummarisation.%20The%20repository%20of%20this%20project%20is%20available%20at%0Ahttps%3A//github.com/HaoBytes/ArgSum-Datatset%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03151v3&entry.124074799=Read"},
{"title": "Conformalized Interval Arithmetic with Symmetric Calibration", "author": "Rui Luo and Zhixin Zhou", "abstract": "  Uncertainty quantification is essential in decision-making, especially when\njoint distributions of random variables are involved. While conformal\nprediction provides distribution-free prediction sets with valid coverage\nguarantees, it traditionally focuses on single predictions. This paper\nintroduces novel conformal prediction methods for estimating the sum or average\nof unknown labels over specific index sets. We develop conformal prediction\nintervals for single target to the prediction interval for sum of multiple\ntargets. Under permutation invariant assumptions, we prove the validity of our\nproposed method. We also apply our algorithms on class average estimation and\npath cost prediction tasks, and we show that our method outperforms existing\nconformalized approaches as well as non-conformal approaches.\n", "link": "http://arxiv.org/abs/2408.10939v1", "date": "2024-08-20", "relevancy": 1.9244, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5135}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.484}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformalized%20Interval%20Arithmetic%20with%20Symmetric%20Calibration&body=Title%3A%20Conformalized%20Interval%20Arithmetic%20with%20Symmetric%20Calibration%0AAuthor%3A%20Rui%20Luo%20and%20Zhixin%20Zhou%0AAbstract%3A%20%20%20Uncertainty%20quantification%20is%20essential%20in%20decision-making%2C%20especially%20when%0Ajoint%20distributions%20of%20random%20variables%20are%20involved.%20While%20conformal%0Aprediction%20provides%20distribution-free%20prediction%20sets%20with%20valid%20coverage%0Aguarantees%2C%20it%20traditionally%20focuses%20on%20single%20predictions.%20This%20paper%0Aintroduces%20novel%20conformal%20prediction%20methods%20for%20estimating%20the%20sum%20or%20average%0Aof%20unknown%20labels%20over%20specific%20index%20sets.%20We%20develop%20conformal%20prediction%0Aintervals%20for%20single%20target%20to%20the%20prediction%20interval%20for%20sum%20of%20multiple%0Atargets.%20Under%20permutation%20invariant%20assumptions%2C%20we%20prove%20the%20validity%20of%20our%0Aproposed%20method.%20We%20also%20apply%20our%20algorithms%20on%20class%20average%20estimation%20and%0Apath%20cost%20prediction%20tasks%2C%20and%20we%20show%20that%20our%20method%20outperforms%20existing%0Aconformalized%20approaches%20as%20well%20as%20non-conformal%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformalized%2520Interval%2520Arithmetic%2520with%2520Symmetric%2520Calibration%26entry.906535625%3DRui%2520Luo%2520and%2520Zhixin%2520Zhou%26entry.1292438233%3D%2520%2520Uncertainty%2520quantification%2520is%2520essential%2520in%2520decision-making%252C%2520especially%2520when%250Ajoint%2520distributions%2520of%2520random%2520variables%2520are%2520involved.%2520While%2520conformal%250Aprediction%2520provides%2520distribution-free%2520prediction%2520sets%2520with%2520valid%2520coverage%250Aguarantees%252C%2520it%2520traditionally%2520focuses%2520on%2520single%2520predictions.%2520This%2520paper%250Aintroduces%2520novel%2520conformal%2520prediction%2520methods%2520for%2520estimating%2520the%2520sum%2520or%2520average%250Aof%2520unknown%2520labels%2520over%2520specific%2520index%2520sets.%2520We%2520develop%2520conformal%2520prediction%250Aintervals%2520for%2520single%2520target%2520to%2520the%2520prediction%2520interval%2520for%2520sum%2520of%2520multiple%250Atargets.%2520Under%2520permutation%2520invariant%2520assumptions%252C%2520we%2520prove%2520the%2520validity%2520of%2520our%250Aproposed%2520method.%2520We%2520also%2520apply%2520our%2520algorithms%2520on%2520class%2520average%2520estimation%2520and%250Apath%2520cost%2520prediction%2520tasks%252C%2520and%2520we%2520show%2520that%2520our%2520method%2520outperforms%2520existing%250Aconformalized%2520approaches%2520as%2520well%2520as%2520non-conformal%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformalized%20Interval%20Arithmetic%20with%20Symmetric%20Calibration&entry.906535625=Rui%20Luo%20and%20Zhixin%20Zhou&entry.1292438233=%20%20Uncertainty%20quantification%20is%20essential%20in%20decision-making%2C%20especially%20when%0Ajoint%20distributions%20of%20random%20variables%20are%20involved.%20While%20conformal%0Aprediction%20provides%20distribution-free%20prediction%20sets%20with%20valid%20coverage%0Aguarantees%2C%20it%20traditionally%20focuses%20on%20single%20predictions.%20This%20paper%0Aintroduces%20novel%20conformal%20prediction%20methods%20for%20estimating%20the%20sum%20or%20average%0Aof%20unknown%20labels%20over%20specific%20index%20sets.%20We%20develop%20conformal%20prediction%0Aintervals%20for%20single%20target%20to%20the%20prediction%20interval%20for%20sum%20of%20multiple%0Atargets.%20Under%20permutation%20invariant%20assumptions%2C%20we%20prove%20the%20validity%20of%20our%0Aproposed%20method.%20We%20also%20apply%20our%20algorithms%20on%20class%20average%20estimation%20and%0Apath%20cost%20prediction%20tasks%2C%20and%20we%20show%20that%20our%20method%20outperforms%20existing%0Aconformalized%20approaches%20as%20well%20as%20non-conformal%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10939v1&entry.124074799=Read"},
{"title": "Flexora: Flexible Low Rank Adaptation for Large Language Models", "author": "Chenxing Wei and Yao Shu and Ying Tiffany He and Fei Richard Yu", "abstract": "  Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora.\n", "link": "http://arxiv.org/abs/2408.10774v1", "date": "2024-08-20", "relevancy": 1.9086, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4801}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4771}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flexora%3A%20Flexible%20Low%20Rank%20Adaptation%20for%20Large%20Language%20Models&body=Title%3A%20Flexora%3A%20Flexible%20Low%20Rank%20Adaptation%20for%20Large%20Language%20Models%0AAuthor%3A%20Chenxing%20Wei%20and%20Yao%20Shu%20and%20Ying%20Tiffany%20He%20and%20Fei%20Richard%20Yu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20driving%20advancements%20in%20artificial%0Aintelligence%20by%20increasing%20the%20scale%20of%20model%20parameters%2C%20which%20has%0Asignificantly%20enhanced%20generalization%20ability%20and%20unlocked%20new%20capabilities%20in%0Apractice.%20However%2C%20their%20performance%20in%20specific%20downstream%20tasks%20is%20usually%0Ahindered%20by%20their%20knowledge%20boundaries%20on%20these%20tasks.%20Thus%2C%20fine-tuning%0Atechniques%2C%20especially%20the%20widely%20used%20Low-Rank%20Adaptation%20%28LoRA%29%20method%2C%20have%0Abeen%20introduced%20to%20expand%20the%20boundaries%20on%20these%20tasks%2C%20whereas%20LoRA%20would%0Aunderperform%20on%20certain%20tasks%20owing%20to%20its%20potential%20overfitting%20on%20these%0Atasks.%20To%20overcome%20this%20overfitting%20and%20improve%20the%20performance%20of%20LoRA%2C%20we%0Apropose%20the%20flexible%20low%20rank%20adaptation%20%28Flexora%29%20method%20to%20automatically%20and%0Aflexibly%20select%20the%20most%20important%20layers%20needing%20to%20be%20fine-tuned%20to%20achieve%0Athe%20best%20performance%20on%20different%20downstream%20tasks.%20Specifically%2C%20Flexora%0Afirstly%20frames%20this%20layer%20selection%20problem%20as%20a%20well-defined%20hyperparameter%0Aoptimization%20%28HPO%29%20problem%2C%20then%20addresses%20it%20using%20the%20unrolled%0Adifferentiation%20%28UD%29%20method%2C%20and%20finally%20selects%20the%20most%20useful%20layers%20based%0Aon%20the%20optimized%20hyperparameters.%20Our%20extensive%20experiments%20on%20many%20pretrained%0Amodels%20and%20natural%20language%20tasks%20show%20that%20Flexora%20is%20able%20to%20consistently%0Aimprove%20over%20the%20existing%20baselines%2C%20indicating%20the%20effectiveness%20of%20our%0AFlexora%20in%20practice.%20We%20additionally%20provide%20insightful%20theoretical%20results%20and%0Amany%20ablation%20studies%20to%20deliver%20a%20comprehensive%20understanding%20of%20our%20Flexora.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexora%253A%2520Flexible%2520Low%2520Rank%2520Adaptation%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DChenxing%2520Wei%2520and%2520Yao%2520Shu%2520and%2520Ying%2520Tiffany%2520He%2520and%2520Fei%2520Richard%2520Yu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520driving%2520advancements%2520in%2520artificial%250Aintelligence%2520by%2520increasing%2520the%2520scale%2520of%2520model%2520parameters%252C%2520which%2520has%250Asignificantly%2520enhanced%2520generalization%2520ability%2520and%2520unlocked%2520new%2520capabilities%2520in%250Apractice.%2520However%252C%2520their%2520performance%2520in%2520specific%2520downstream%2520tasks%2520is%2520usually%250Ahindered%2520by%2520their%2520knowledge%2520boundaries%2520on%2520these%2520tasks.%2520Thus%252C%2520fine-tuning%250Atechniques%252C%2520especially%2520the%2520widely%2520used%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520method%252C%2520have%250Abeen%2520introduced%2520to%2520expand%2520the%2520boundaries%2520on%2520these%2520tasks%252C%2520whereas%2520LoRA%2520would%250Aunderperform%2520on%2520certain%2520tasks%2520owing%2520to%2520its%2520potential%2520overfitting%2520on%2520these%250Atasks.%2520To%2520overcome%2520this%2520overfitting%2520and%2520improve%2520the%2520performance%2520of%2520LoRA%252C%2520we%250Apropose%2520the%2520flexible%2520low%2520rank%2520adaptation%2520%2528Flexora%2529%2520method%2520to%2520automatically%2520and%250Aflexibly%2520select%2520the%2520most%2520important%2520layers%2520needing%2520to%2520be%2520fine-tuned%2520to%2520achieve%250Athe%2520best%2520performance%2520on%2520different%2520downstream%2520tasks.%2520Specifically%252C%2520Flexora%250Afirstly%2520frames%2520this%2520layer%2520selection%2520problem%2520as%2520a%2520well-defined%2520hyperparameter%250Aoptimization%2520%2528HPO%2529%2520problem%252C%2520then%2520addresses%2520it%2520using%2520the%2520unrolled%250Adifferentiation%2520%2528UD%2529%2520method%252C%2520and%2520finally%2520selects%2520the%2520most%2520useful%2520layers%2520based%250Aon%2520the%2520optimized%2520hyperparameters.%2520Our%2520extensive%2520experiments%2520on%2520many%2520pretrained%250Amodels%2520and%2520natural%2520language%2520tasks%2520show%2520that%2520Flexora%2520is%2520able%2520to%2520consistently%250Aimprove%2520over%2520the%2520existing%2520baselines%252C%2520indicating%2520the%2520effectiveness%2520of%2520our%250AFlexora%2520in%2520practice.%2520We%2520additionally%2520provide%2520insightful%2520theoretical%2520results%2520and%250Amany%2520ablation%2520studies%2520to%2520deliver%2520a%2520comprehensive%2520understanding%2520of%2520our%2520Flexora.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flexora%3A%20Flexible%20Low%20Rank%20Adaptation%20for%20Large%20Language%20Models&entry.906535625=Chenxing%20Wei%20and%20Yao%20Shu%20and%20Ying%20Tiffany%20He%20and%20Fei%20Richard%20Yu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20driving%20advancements%20in%20artificial%0Aintelligence%20by%20increasing%20the%20scale%20of%20model%20parameters%2C%20which%20has%0Asignificantly%20enhanced%20generalization%20ability%20and%20unlocked%20new%20capabilities%20in%0Apractice.%20However%2C%20their%20performance%20in%20specific%20downstream%20tasks%20is%20usually%0Ahindered%20by%20their%20knowledge%20boundaries%20on%20these%20tasks.%20Thus%2C%20fine-tuning%0Atechniques%2C%20especially%20the%20widely%20used%20Low-Rank%20Adaptation%20%28LoRA%29%20method%2C%20have%0Abeen%20introduced%20to%20expand%20the%20boundaries%20on%20these%20tasks%2C%20whereas%20LoRA%20would%0Aunderperform%20on%20certain%20tasks%20owing%20to%20its%20potential%20overfitting%20on%20these%0Atasks.%20To%20overcome%20this%20overfitting%20and%20improve%20the%20performance%20of%20LoRA%2C%20we%0Apropose%20the%20flexible%20low%20rank%20adaptation%20%28Flexora%29%20method%20to%20automatically%20and%0Aflexibly%20select%20the%20most%20important%20layers%20needing%20to%20be%20fine-tuned%20to%20achieve%0Athe%20best%20performance%20on%20different%20downstream%20tasks.%20Specifically%2C%20Flexora%0Afirstly%20frames%20this%20layer%20selection%20problem%20as%20a%20well-defined%20hyperparameter%0Aoptimization%20%28HPO%29%20problem%2C%20then%20addresses%20it%20using%20the%20unrolled%0Adifferentiation%20%28UD%29%20method%2C%20and%20finally%20selects%20the%20most%20useful%20layers%20based%0Aon%20the%20optimized%20hyperparameters.%20Our%20extensive%20experiments%20on%20many%20pretrained%0Amodels%20and%20natural%20language%20tasks%20show%20that%20Flexora%20is%20able%20to%20consistently%0Aimprove%20over%20the%20existing%20baselines%2C%20indicating%20the%20effectiveness%20of%20our%0AFlexora%20in%20practice.%20We%20additionally%20provide%20insightful%20theoretical%20results%20and%0Amany%20ablation%20studies%20to%20deliver%20a%20comprehensive%20understanding%20of%20our%20Flexora.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10774v1&entry.124074799=Read"},
{"title": "LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically\n  Generating Metaheuristics", "author": "Niki van Stein and Thomas B\u00e4ck", "abstract": "  Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to\nunderstand natural language and generate complex code snippets. This paper\nintroduces a novel Large Language Model Evolutionary Algorithm (LLaMEA)\nframework, leveraging GPT models for the automated generation and refinement of\nalgorithms. Given a set of criteria and a task definition (the search space),\nLLaMEA iteratively generates, mutates and selects algorithms based on\nperformance metrics and feedback from runtime evaluations. This framework\noffers a unique approach to generating optimized algorithms without requiring\nextensive prior expertise. We show how this framework can be used to generate\nnovel black-box metaheuristic optimization algorithms automatically. LLaMEA\ngenerates multiple algorithms that outperform state-of-the-art optimization\nalgorithms (Covariance Matrix Adaptation Evolution Strategy and Differential\nEvolution) on the five dimensional black box optimization benchmark (BBOB). The\nalgorithms also show competitive performance on the 10- and 20-dimensional\ninstances of the test functions, although they have not seen such instances\nduring the automated generation process. The results demonstrate the\nfeasibility of the framework and identify future directions for automated\ngeneration and optimization of algorithms via LLMs.\n", "link": "http://arxiv.org/abs/2405.20132v3", "date": "2024-08-20", "relevancy": 1.9069, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4866}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4856}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaMEA%3A%20A%20Large%20Language%20Model%20Evolutionary%20Algorithm%20for%20Automatically%0A%20%20Generating%20Metaheuristics&body=Title%3A%20LLaMEA%3A%20A%20Large%20Language%20Model%20Evolutionary%20Algorithm%20for%20Automatically%0A%20%20Generating%20Metaheuristics%0AAuthor%3A%20Niki%20van%20Stein%20and%20Thomas%20B%C3%A4ck%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20such%20as%20GPT-4%20have%20demonstrated%20their%20ability%20to%0Aunderstand%20natural%20language%20and%20generate%20complex%20code%20snippets.%20This%20paper%0Aintroduces%20a%20novel%20Large%20Language%20Model%20Evolutionary%20Algorithm%20%28LLaMEA%29%0Aframework%2C%20leveraging%20GPT%20models%20for%20the%20automated%20generation%20and%20refinement%20of%0Aalgorithms.%20Given%20a%20set%20of%20criteria%20and%20a%20task%20definition%20%28the%20search%20space%29%2C%0ALLaMEA%20iteratively%20generates%2C%20mutates%20and%20selects%20algorithms%20based%20on%0Aperformance%20metrics%20and%20feedback%20from%20runtime%20evaluations.%20This%20framework%0Aoffers%20a%20unique%20approach%20to%20generating%20optimized%20algorithms%20without%20requiring%0Aextensive%20prior%20expertise.%20We%20show%20how%20this%20framework%20can%20be%20used%20to%20generate%0Anovel%20black-box%20metaheuristic%20optimization%20algorithms%20automatically.%20LLaMEA%0Agenerates%20multiple%20algorithms%20that%20outperform%20state-of-the-art%20optimization%0Aalgorithms%20%28Covariance%20Matrix%20Adaptation%20Evolution%20Strategy%20and%20Differential%0AEvolution%29%20on%20the%20five%20dimensional%20black%20box%20optimization%20benchmark%20%28BBOB%29.%20The%0Aalgorithms%20also%20show%20competitive%20performance%20on%20the%2010-%20and%2020-dimensional%0Ainstances%20of%20the%20test%20functions%2C%20although%20they%20have%20not%20seen%20such%20instances%0Aduring%20the%20automated%20generation%20process.%20The%20results%20demonstrate%20the%0Afeasibility%20of%20the%20framework%20and%20identify%20future%20directions%20for%20automated%0Ageneration%20and%20optimization%20of%20algorithms%20via%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20132v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaMEA%253A%2520A%2520Large%2520Language%2520Model%2520Evolutionary%2520Algorithm%2520for%2520Automatically%250A%2520%2520Generating%2520Metaheuristics%26entry.906535625%3DNiki%2520van%2520Stein%2520and%2520Thomas%2520B%25C3%25A4ck%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520such%2520as%2520GPT-4%2520have%2520demonstrated%2520their%2520ability%2520to%250Aunderstand%2520natural%2520language%2520and%2520generate%2520complex%2520code%2520snippets.%2520This%2520paper%250Aintroduces%2520a%2520novel%2520Large%2520Language%2520Model%2520Evolutionary%2520Algorithm%2520%2528LLaMEA%2529%250Aframework%252C%2520leveraging%2520GPT%2520models%2520for%2520the%2520automated%2520generation%2520and%2520refinement%2520of%250Aalgorithms.%2520Given%2520a%2520set%2520of%2520criteria%2520and%2520a%2520task%2520definition%2520%2528the%2520search%2520space%2529%252C%250ALLaMEA%2520iteratively%2520generates%252C%2520mutates%2520and%2520selects%2520algorithms%2520based%2520on%250Aperformance%2520metrics%2520and%2520feedback%2520from%2520runtime%2520evaluations.%2520This%2520framework%250Aoffers%2520a%2520unique%2520approach%2520to%2520generating%2520optimized%2520algorithms%2520without%2520requiring%250Aextensive%2520prior%2520expertise.%2520We%2520show%2520how%2520this%2520framework%2520can%2520be%2520used%2520to%2520generate%250Anovel%2520black-box%2520metaheuristic%2520optimization%2520algorithms%2520automatically.%2520LLaMEA%250Agenerates%2520multiple%2520algorithms%2520that%2520outperform%2520state-of-the-art%2520optimization%250Aalgorithms%2520%2528Covariance%2520Matrix%2520Adaptation%2520Evolution%2520Strategy%2520and%2520Differential%250AEvolution%2529%2520on%2520the%2520five%2520dimensional%2520black%2520box%2520optimization%2520benchmark%2520%2528BBOB%2529.%2520The%250Aalgorithms%2520also%2520show%2520competitive%2520performance%2520on%2520the%252010-%2520and%252020-dimensional%250Ainstances%2520of%2520the%2520test%2520functions%252C%2520although%2520they%2520have%2520not%2520seen%2520such%2520instances%250Aduring%2520the%2520automated%2520generation%2520process.%2520The%2520results%2520demonstrate%2520the%250Afeasibility%2520of%2520the%2520framework%2520and%2520identify%2520future%2520directions%2520for%2520automated%250Ageneration%2520and%2520optimization%2520of%2520algorithms%2520via%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20132v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaMEA%3A%20A%20Large%20Language%20Model%20Evolutionary%20Algorithm%20for%20Automatically%0A%20%20Generating%20Metaheuristics&entry.906535625=Niki%20van%20Stein%20and%20Thomas%20B%C3%A4ck&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20such%20as%20GPT-4%20have%20demonstrated%20their%20ability%20to%0Aunderstand%20natural%20language%20and%20generate%20complex%20code%20snippets.%20This%20paper%0Aintroduces%20a%20novel%20Large%20Language%20Model%20Evolutionary%20Algorithm%20%28LLaMEA%29%0Aframework%2C%20leveraging%20GPT%20models%20for%20the%20automated%20generation%20and%20refinement%20of%0Aalgorithms.%20Given%20a%20set%20of%20criteria%20and%20a%20task%20definition%20%28the%20search%20space%29%2C%0ALLaMEA%20iteratively%20generates%2C%20mutates%20and%20selects%20algorithms%20based%20on%0Aperformance%20metrics%20and%20feedback%20from%20runtime%20evaluations.%20This%20framework%0Aoffers%20a%20unique%20approach%20to%20generating%20optimized%20algorithms%20without%20requiring%0Aextensive%20prior%20expertise.%20We%20show%20how%20this%20framework%20can%20be%20used%20to%20generate%0Anovel%20black-box%20metaheuristic%20optimization%20algorithms%20automatically.%20LLaMEA%0Agenerates%20multiple%20algorithms%20that%20outperform%20state-of-the-art%20optimization%0Aalgorithms%20%28Covariance%20Matrix%20Adaptation%20Evolution%20Strategy%20and%20Differential%0AEvolution%29%20on%20the%20five%20dimensional%20black%20box%20optimization%20benchmark%20%28BBOB%29.%20The%0Aalgorithms%20also%20show%20competitive%20performance%20on%20the%2010-%20and%2020-dimensional%0Ainstances%20of%20the%20test%20functions%2C%20although%20they%20have%20not%20seen%20such%20instances%0Aduring%20the%20automated%20generation%20process.%20The%20results%20demonstrate%20the%0Afeasibility%20of%20the%20framework%20and%20identify%20future%20directions%20for%20automated%0Ageneration%20and%20optimization%20of%20algorithms%20via%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20132v3&entry.124074799=Read"},
{"title": "DisMix: Disentangling Mixtures of Musical Instruments for Source-level\n  Pitch and Timbre Manipulation", "author": "Yin-Jyun Luo and Kin Wai Cheuk and Woosung Choi and Toshimitsu Uesaka and Keisuke Toyama and Koichi Saito and Chieh-Hsin Lai and Yuhta Takida and Wei-Hsiang Liao and Simon Dixon and Yuki Mitsufuji", "abstract": "  Existing work on pitch and timbre disentanglement has been mostly focused on\nsingle-instrument music audio, excluding the cases where multiple instruments\nare presented. To fill the gap, we propose DisMix, a generative framework in\nwhich the pitch and timbre representations act as modular building blocks for\nconstructing the melody and instrument of a source, and the collection of which\nforms a set of per-instrument latent representations underlying the observed\nmixture. By manipulating the representations, our model samples mixtures with\nnovel combinations of pitch and timbre of the constituent instruments. We can\njointly learn the disentangled pitch-timbre representations and a latent\ndiffusion transformer that reconstructs the mixture conditioned on the set of\nsource-level representations. We evaluate the model using both a simple dataset\nof isolated chords and a realistic four-part chorales in the style of J.S.\nBach, identify the key components for the success of disentanglement, and\ndemonstrate the application of mixture transformation based on source-level\nattribute manipulation.\n", "link": "http://arxiv.org/abs/2408.10807v1", "date": "2024-08-20", "relevancy": 1.8981, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4799}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4762}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisMix%3A%20Disentangling%20Mixtures%20of%20Musical%20Instruments%20for%20Source-level%0A%20%20Pitch%20and%20Timbre%20Manipulation&body=Title%3A%20DisMix%3A%20Disentangling%20Mixtures%20of%20Musical%20Instruments%20for%20Source-level%0A%20%20Pitch%20and%20Timbre%20Manipulation%0AAuthor%3A%20Yin-Jyun%20Luo%20and%20Kin%20Wai%20Cheuk%20and%20Woosung%20Choi%20and%20Toshimitsu%20Uesaka%20and%20Keisuke%20Toyama%20and%20Koichi%20Saito%20and%20Chieh-Hsin%20Lai%20and%20Yuhta%20Takida%20and%20Wei-Hsiang%20Liao%20and%20Simon%20Dixon%20and%20Yuki%20Mitsufuji%0AAbstract%3A%20%20%20Existing%20work%20on%20pitch%20and%20timbre%20disentanglement%20has%20been%20mostly%20focused%20on%0Asingle-instrument%20music%20audio%2C%20excluding%20the%20cases%20where%20multiple%20instruments%0Aare%20presented.%20To%20fill%20the%20gap%2C%20we%20propose%20DisMix%2C%20a%20generative%20framework%20in%0Awhich%20the%20pitch%20and%20timbre%20representations%20act%20as%20modular%20building%20blocks%20for%0Aconstructing%20the%20melody%20and%20instrument%20of%20a%20source%2C%20and%20the%20collection%20of%20which%0Aforms%20a%20set%20of%20per-instrument%20latent%20representations%20underlying%20the%20observed%0Amixture.%20By%20manipulating%20the%20representations%2C%20our%20model%20samples%20mixtures%20with%0Anovel%20combinations%20of%20pitch%20and%20timbre%20of%20the%20constituent%20instruments.%20We%20can%0Ajointly%20learn%20the%20disentangled%20pitch-timbre%20representations%20and%20a%20latent%0Adiffusion%20transformer%20that%20reconstructs%20the%20mixture%20conditioned%20on%20the%20set%20of%0Asource-level%20representations.%20We%20evaluate%20the%20model%20using%20both%20a%20simple%20dataset%0Aof%20isolated%20chords%20and%20a%20realistic%20four-part%20chorales%20in%20the%20style%20of%20J.S.%0ABach%2C%20identify%20the%20key%20components%20for%20the%20success%20of%20disentanglement%2C%20and%0Ademonstrate%20the%20application%20of%20mixture%20transformation%20based%20on%20source-level%0Aattribute%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisMix%253A%2520Disentangling%2520Mixtures%2520of%2520Musical%2520Instruments%2520for%2520Source-level%250A%2520%2520Pitch%2520and%2520Timbre%2520Manipulation%26entry.906535625%3DYin-Jyun%2520Luo%2520and%2520Kin%2520Wai%2520Cheuk%2520and%2520Woosung%2520Choi%2520and%2520Toshimitsu%2520Uesaka%2520and%2520Keisuke%2520Toyama%2520and%2520Koichi%2520Saito%2520and%2520Chieh-Hsin%2520Lai%2520and%2520Yuhta%2520Takida%2520and%2520Wei-Hsiang%2520Liao%2520and%2520Simon%2520Dixon%2520and%2520Yuki%2520Mitsufuji%26entry.1292438233%3D%2520%2520Existing%2520work%2520on%2520pitch%2520and%2520timbre%2520disentanglement%2520has%2520been%2520mostly%2520focused%2520on%250Asingle-instrument%2520music%2520audio%252C%2520excluding%2520the%2520cases%2520where%2520multiple%2520instruments%250Aare%2520presented.%2520To%2520fill%2520the%2520gap%252C%2520we%2520propose%2520DisMix%252C%2520a%2520generative%2520framework%2520in%250Awhich%2520the%2520pitch%2520and%2520timbre%2520representations%2520act%2520as%2520modular%2520building%2520blocks%2520for%250Aconstructing%2520the%2520melody%2520and%2520instrument%2520of%2520a%2520source%252C%2520and%2520the%2520collection%2520of%2520which%250Aforms%2520a%2520set%2520of%2520per-instrument%2520latent%2520representations%2520underlying%2520the%2520observed%250Amixture.%2520By%2520manipulating%2520the%2520representations%252C%2520our%2520model%2520samples%2520mixtures%2520with%250Anovel%2520combinations%2520of%2520pitch%2520and%2520timbre%2520of%2520the%2520constituent%2520instruments.%2520We%2520can%250Ajointly%2520learn%2520the%2520disentangled%2520pitch-timbre%2520representations%2520and%2520a%2520latent%250Adiffusion%2520transformer%2520that%2520reconstructs%2520the%2520mixture%2520conditioned%2520on%2520the%2520set%2520of%250Asource-level%2520representations.%2520We%2520evaluate%2520the%2520model%2520using%2520both%2520a%2520simple%2520dataset%250Aof%2520isolated%2520chords%2520and%2520a%2520realistic%2520four-part%2520chorales%2520in%2520the%2520style%2520of%2520J.S.%250ABach%252C%2520identify%2520the%2520key%2520components%2520for%2520the%2520success%2520of%2520disentanglement%252C%2520and%250Ademonstrate%2520the%2520application%2520of%2520mixture%2520transformation%2520based%2520on%2520source-level%250Aattribute%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisMix%3A%20Disentangling%20Mixtures%20of%20Musical%20Instruments%20for%20Source-level%0A%20%20Pitch%20and%20Timbre%20Manipulation&entry.906535625=Yin-Jyun%20Luo%20and%20Kin%20Wai%20Cheuk%20and%20Woosung%20Choi%20and%20Toshimitsu%20Uesaka%20and%20Keisuke%20Toyama%20and%20Koichi%20Saito%20and%20Chieh-Hsin%20Lai%20and%20Yuhta%20Takida%20and%20Wei-Hsiang%20Liao%20and%20Simon%20Dixon%20and%20Yuki%20Mitsufuji&entry.1292438233=%20%20Existing%20work%20on%20pitch%20and%20timbre%20disentanglement%20has%20been%20mostly%20focused%20on%0Asingle-instrument%20music%20audio%2C%20excluding%20the%20cases%20where%20multiple%20instruments%0Aare%20presented.%20To%20fill%20the%20gap%2C%20we%20propose%20DisMix%2C%20a%20generative%20framework%20in%0Awhich%20the%20pitch%20and%20timbre%20representations%20act%20as%20modular%20building%20blocks%20for%0Aconstructing%20the%20melody%20and%20instrument%20of%20a%20source%2C%20and%20the%20collection%20of%20which%0Aforms%20a%20set%20of%20per-instrument%20latent%20representations%20underlying%20the%20observed%0Amixture.%20By%20manipulating%20the%20representations%2C%20our%20model%20samples%20mixtures%20with%0Anovel%20combinations%20of%20pitch%20and%20timbre%20of%20the%20constituent%20instruments.%20We%20can%0Ajointly%20learn%20the%20disentangled%20pitch-timbre%20representations%20and%20a%20latent%0Adiffusion%20transformer%20that%20reconstructs%20the%20mixture%20conditioned%20on%20the%20set%20of%0Asource-level%20representations.%20We%20evaluate%20the%20model%20using%20both%20a%20simple%20dataset%0Aof%20isolated%20chords%20and%20a%20realistic%20four-part%20chorales%20in%20the%20style%20of%20J.S.%0ABach%2C%20identify%20the%20key%20components%20for%20the%20success%20of%20disentanglement%2C%20and%0Ademonstrate%20the%20application%20of%20mixture%20transformation%20based%20on%20source-level%0Aattribute%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10807v1&entry.124074799=Read"},
{"title": "Recurrent Neural Networks Learn to Store and Generate Sequences using\n  Non-Linear Representations", "author": "R\u00f3bert Csord\u00e1s and Christopher Potts and Christopher D. Manning and Atticus Geiger", "abstract": "  The Linear Representation Hypothesis (LRH) states that neural networks learn\nto encode concepts as directions in activation space, and a strong version of\nthe LRH states that models learn only such encodings. In this paper, we present\na counterexample to this strong LRH: when trained to repeat an input token\nsequence, gated recurrent neural networks (RNNs) learn to represent the token\nat each position with a particular order of magnitude, rather than a direction.\nThese representations have layered features that are impossible to locate in\ndistinct linear subspaces. To show this, we train interventions to predict and\nmanipulate tokens by learning the scaling factor corresponding to each sequence\nposition. These interventions indicate that the smallest RNNs find only this\nmagnitude-based solution, while larger RNNs have linear representations. These\nfindings strongly indicate that interpretability research should not be\nconfined by the LRH.\n", "link": "http://arxiv.org/abs/2408.10920v1", "date": "2024-08-20", "relevancy": 1.8891, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4893}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4695}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recurrent%20Neural%20Networks%20Learn%20to%20Store%20and%20Generate%20Sequences%20using%0A%20%20Non-Linear%20Representations&body=Title%3A%20Recurrent%20Neural%20Networks%20Learn%20to%20Store%20and%20Generate%20Sequences%20using%0A%20%20Non-Linear%20Representations%0AAuthor%3A%20R%C3%B3bert%20Csord%C3%A1s%20and%20Christopher%20Potts%20and%20Christopher%20D.%20Manning%20and%20Atticus%20Geiger%0AAbstract%3A%20%20%20The%20Linear%20Representation%20Hypothesis%20%28LRH%29%20states%20that%20neural%20networks%20learn%0Ato%20encode%20concepts%20as%20directions%20in%20activation%20space%2C%20and%20a%20strong%20version%20of%0Athe%20LRH%20states%20that%20models%20learn%20only%20such%20encodings.%20In%20this%20paper%2C%20we%20present%0Aa%20counterexample%20to%20this%20strong%20LRH%3A%20when%20trained%20to%20repeat%20an%20input%20token%0Asequence%2C%20gated%20recurrent%20neural%20networks%20%28RNNs%29%20learn%20to%20represent%20the%20token%0Aat%20each%20position%20with%20a%20particular%20order%20of%20magnitude%2C%20rather%20than%20a%20direction.%0AThese%20representations%20have%20layered%20features%20that%20are%20impossible%20to%20locate%20in%0Adistinct%20linear%20subspaces.%20To%20show%20this%2C%20we%20train%20interventions%20to%20predict%20and%0Amanipulate%20tokens%20by%20learning%20the%20scaling%20factor%20corresponding%20to%20each%20sequence%0Aposition.%20These%20interventions%20indicate%20that%20the%20smallest%20RNNs%20find%20only%20this%0Amagnitude-based%20solution%2C%20while%20larger%20RNNs%20have%20linear%20representations.%20These%0Afindings%20strongly%20indicate%20that%20interpretability%20research%20should%20not%20be%0Aconfined%20by%20the%20LRH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrent%2520Neural%2520Networks%2520Learn%2520to%2520Store%2520and%2520Generate%2520Sequences%2520using%250A%2520%2520Non-Linear%2520Representations%26entry.906535625%3DR%25C3%25B3bert%2520Csord%25C3%25A1s%2520and%2520Christopher%2520Potts%2520and%2520Christopher%2520D.%2520Manning%2520and%2520Atticus%2520Geiger%26entry.1292438233%3D%2520%2520The%2520Linear%2520Representation%2520Hypothesis%2520%2528LRH%2529%2520states%2520that%2520neural%2520networks%2520learn%250Ato%2520encode%2520concepts%2520as%2520directions%2520in%2520activation%2520space%252C%2520and%2520a%2520strong%2520version%2520of%250Athe%2520LRH%2520states%2520that%2520models%2520learn%2520only%2520such%2520encodings.%2520In%2520this%2520paper%252C%2520we%2520present%250Aa%2520counterexample%2520to%2520this%2520strong%2520LRH%253A%2520when%2520trained%2520to%2520repeat%2520an%2520input%2520token%250Asequence%252C%2520gated%2520recurrent%2520neural%2520networks%2520%2528RNNs%2529%2520learn%2520to%2520represent%2520the%2520token%250Aat%2520each%2520position%2520with%2520a%2520particular%2520order%2520of%2520magnitude%252C%2520rather%2520than%2520a%2520direction.%250AThese%2520representations%2520have%2520layered%2520features%2520that%2520are%2520impossible%2520to%2520locate%2520in%250Adistinct%2520linear%2520subspaces.%2520To%2520show%2520this%252C%2520we%2520train%2520interventions%2520to%2520predict%2520and%250Amanipulate%2520tokens%2520by%2520learning%2520the%2520scaling%2520factor%2520corresponding%2520to%2520each%2520sequence%250Aposition.%2520These%2520interventions%2520indicate%2520that%2520the%2520smallest%2520RNNs%2520find%2520only%2520this%250Amagnitude-based%2520solution%252C%2520while%2520larger%2520RNNs%2520have%2520linear%2520representations.%2520These%250Afindings%2520strongly%2520indicate%2520that%2520interpretability%2520research%2520should%2520not%2520be%250Aconfined%2520by%2520the%2520LRH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recurrent%20Neural%20Networks%20Learn%20to%20Store%20and%20Generate%20Sequences%20using%0A%20%20Non-Linear%20Representations&entry.906535625=R%C3%B3bert%20Csord%C3%A1s%20and%20Christopher%20Potts%20and%20Christopher%20D.%20Manning%20and%20Atticus%20Geiger&entry.1292438233=%20%20The%20Linear%20Representation%20Hypothesis%20%28LRH%29%20states%20that%20neural%20networks%20learn%0Ato%20encode%20concepts%20as%20directions%20in%20activation%20space%2C%20and%20a%20strong%20version%20of%0Athe%20LRH%20states%20that%20models%20learn%20only%20such%20encodings.%20In%20this%20paper%2C%20we%20present%0Aa%20counterexample%20to%20this%20strong%20LRH%3A%20when%20trained%20to%20repeat%20an%20input%20token%0Asequence%2C%20gated%20recurrent%20neural%20networks%20%28RNNs%29%20learn%20to%20represent%20the%20token%0Aat%20each%20position%20with%20a%20particular%20order%20of%20magnitude%2C%20rather%20than%20a%20direction.%0AThese%20representations%20have%20layered%20features%20that%20are%20impossible%20to%20locate%20in%0Adistinct%20linear%20subspaces.%20To%20show%20this%2C%20we%20train%20interventions%20to%20predict%20and%0Amanipulate%20tokens%20by%20learning%20the%20scaling%20factor%20corresponding%20to%20each%20sequence%0Aposition.%20These%20interventions%20indicate%20that%20the%20smallest%20RNNs%20find%20only%20this%0Amagnitude-based%20solution%2C%20while%20larger%20RNNs%20have%20linear%20representations.%20These%0Afindings%20strongly%20indicate%20that%20interpretability%20research%20should%20not%20be%0Aconfined%20by%20the%20LRH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10920v1&entry.124074799=Read"},
{"title": "Deep Learning-based Classification of Dementia using Image\n  Representation of Subcortical Signals", "author": "Shivani Ranjan and Ayush Tripathi and Harshal Shende and Robin Badal and Amit Kumar and Pramod Yadav and Deepak Joshi and Lalan Kumar", "abstract": "  Dementia is a neurological syndrome marked by cognitive decline. Alzheimer's\ndisease (AD) and Frontotemporal dementia (FTD) are the common forms of\ndementia, each with distinct progression patterns. EEG, a non-invasive tool for\nrecording brain activity, has shown potential in distinguishing AD from FTD and\nmild cognitive impairment (MCI). Previous studies have utilized various EEG\nfeatures, such as subband power and connectivity patterns to differentiate\nthese conditions. However, artifacts in EEG signals can obscure crucial\ninformation, necessitating advanced signal processing techniques. This study\naims to develop a deep learning-based classification system for dementia by\nanalyzing scout time-series signals from deep brain regions, specifically the\nhippocampus, amygdala, and thalamus. The study utilizes scout time series\nextracted via the standardized low-resolution brain electromagnetic tomography\n(sLORETA) technique. The time series is converted to image representations\nusing continuous wavelet transform (CWT) and fed as input to deep learning\nmodels. Two high-density EEG datasets are utilized to check for the efficacy of\nthe proposed method: the online BrainLat dataset (comprising AD, FTD, and\nhealthy controls (HC)) and the in-house IITD-AIIA dataset (including subjects\nwith AD, MCI, and HC). Different classification strategies and classifier\ncombinations have been utilized for the accurate mapping of classes on both\ndatasets. The best results were achieved by using a product of probabilities\nfrom classifiers for left and right subcortical regions in conjunction with the\nDenseNet model architecture. It yields accuracies of 94.17$\\%$ and 77.72$\\%$ on\nthe BrainLat and IITD-AIIA datasets, respectively. This highlights the\npotential of this approach for early and accurate differentiation of\nneurodegenerative disorders.\n", "link": "http://arxiv.org/abs/2408.10816v1", "date": "2024-08-20", "relevancy": 1.8872, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4861}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4747}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning-based%20Classification%20of%20Dementia%20using%20Image%0A%20%20Representation%20of%20Subcortical%20Signals&body=Title%3A%20Deep%20Learning-based%20Classification%20of%20Dementia%20using%20Image%0A%20%20Representation%20of%20Subcortical%20Signals%0AAuthor%3A%20Shivani%20Ranjan%20and%20Ayush%20Tripathi%20and%20Harshal%20Shende%20and%20Robin%20Badal%20and%20Amit%20Kumar%20and%20Pramod%20Yadav%20and%20Deepak%20Joshi%20and%20Lalan%20Kumar%0AAbstract%3A%20%20%20Dementia%20is%20a%20neurological%20syndrome%20marked%20by%20cognitive%20decline.%20Alzheimer%27s%0Adisease%20%28AD%29%20and%20Frontotemporal%20dementia%20%28FTD%29%20are%20the%20common%20forms%20of%0Adementia%2C%20each%20with%20distinct%20progression%20patterns.%20EEG%2C%20a%20non-invasive%20tool%20for%0Arecording%20brain%20activity%2C%20has%20shown%20potential%20in%20distinguishing%20AD%20from%20FTD%20and%0Amild%20cognitive%20impairment%20%28MCI%29.%20Previous%20studies%20have%20utilized%20various%20EEG%0Afeatures%2C%20such%20as%20subband%20power%20and%20connectivity%20patterns%20to%20differentiate%0Athese%20conditions.%20However%2C%20artifacts%20in%20EEG%20signals%20can%20obscure%20crucial%0Ainformation%2C%20necessitating%20advanced%20signal%20processing%20techniques.%20This%20study%0Aaims%20to%20develop%20a%20deep%20learning-based%20classification%20system%20for%20dementia%20by%0Aanalyzing%20scout%20time-series%20signals%20from%20deep%20brain%20regions%2C%20specifically%20the%0Ahippocampus%2C%20amygdala%2C%20and%20thalamus.%20The%20study%20utilizes%20scout%20time%20series%0Aextracted%20via%20the%20standardized%20low-resolution%20brain%20electromagnetic%20tomography%0A%28sLORETA%29%20technique.%20The%20time%20series%20is%20converted%20to%20image%20representations%0Ausing%20continuous%20wavelet%20transform%20%28CWT%29%20and%20fed%20as%20input%20to%20deep%20learning%0Amodels.%20Two%20high-density%20EEG%20datasets%20are%20utilized%20to%20check%20for%20the%20efficacy%20of%0Athe%20proposed%20method%3A%20the%20online%20BrainLat%20dataset%20%28comprising%20AD%2C%20FTD%2C%20and%0Ahealthy%20controls%20%28HC%29%29%20and%20the%20in-house%20IITD-AIIA%20dataset%20%28including%20subjects%0Awith%20AD%2C%20MCI%2C%20and%20HC%29.%20Different%20classification%20strategies%20and%20classifier%0Acombinations%20have%20been%20utilized%20for%20the%20accurate%20mapping%20of%20classes%20on%20both%0Adatasets.%20The%20best%20results%20were%20achieved%20by%20using%20a%20product%20of%20probabilities%0Afrom%20classifiers%20for%20left%20and%20right%20subcortical%20regions%20in%20conjunction%20with%20the%0ADenseNet%20model%20architecture.%20It%20yields%20accuracies%20of%2094.17%24%5C%25%24%20and%2077.72%24%5C%25%24%20on%0Athe%20BrainLat%20and%20IITD-AIIA%20datasets%2C%20respectively.%20This%20highlights%20the%0Apotential%20of%20this%20approach%20for%20early%20and%20accurate%20differentiation%20of%0Aneurodegenerative%20disorders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning-based%2520Classification%2520of%2520Dementia%2520using%2520Image%250A%2520%2520Representation%2520of%2520Subcortical%2520Signals%26entry.906535625%3DShivani%2520Ranjan%2520and%2520Ayush%2520Tripathi%2520and%2520Harshal%2520Shende%2520and%2520Robin%2520Badal%2520and%2520Amit%2520Kumar%2520and%2520Pramod%2520Yadav%2520and%2520Deepak%2520Joshi%2520and%2520Lalan%2520Kumar%26entry.1292438233%3D%2520%2520Dementia%2520is%2520a%2520neurological%2520syndrome%2520marked%2520by%2520cognitive%2520decline.%2520Alzheimer%2527s%250Adisease%2520%2528AD%2529%2520and%2520Frontotemporal%2520dementia%2520%2528FTD%2529%2520are%2520the%2520common%2520forms%2520of%250Adementia%252C%2520each%2520with%2520distinct%2520progression%2520patterns.%2520EEG%252C%2520a%2520non-invasive%2520tool%2520for%250Arecording%2520brain%2520activity%252C%2520has%2520shown%2520potential%2520in%2520distinguishing%2520AD%2520from%2520FTD%2520and%250Amild%2520cognitive%2520impairment%2520%2528MCI%2529.%2520Previous%2520studies%2520have%2520utilized%2520various%2520EEG%250Afeatures%252C%2520such%2520as%2520subband%2520power%2520and%2520connectivity%2520patterns%2520to%2520differentiate%250Athese%2520conditions.%2520However%252C%2520artifacts%2520in%2520EEG%2520signals%2520can%2520obscure%2520crucial%250Ainformation%252C%2520necessitating%2520advanced%2520signal%2520processing%2520techniques.%2520This%2520study%250Aaims%2520to%2520develop%2520a%2520deep%2520learning-based%2520classification%2520system%2520for%2520dementia%2520by%250Aanalyzing%2520scout%2520time-series%2520signals%2520from%2520deep%2520brain%2520regions%252C%2520specifically%2520the%250Ahippocampus%252C%2520amygdala%252C%2520and%2520thalamus.%2520The%2520study%2520utilizes%2520scout%2520time%2520series%250Aextracted%2520via%2520the%2520standardized%2520low-resolution%2520brain%2520electromagnetic%2520tomography%250A%2528sLORETA%2529%2520technique.%2520The%2520time%2520series%2520is%2520converted%2520to%2520image%2520representations%250Ausing%2520continuous%2520wavelet%2520transform%2520%2528CWT%2529%2520and%2520fed%2520as%2520input%2520to%2520deep%2520learning%250Amodels.%2520Two%2520high-density%2520EEG%2520datasets%2520are%2520utilized%2520to%2520check%2520for%2520the%2520efficacy%2520of%250Athe%2520proposed%2520method%253A%2520the%2520online%2520BrainLat%2520dataset%2520%2528comprising%2520AD%252C%2520FTD%252C%2520and%250Ahealthy%2520controls%2520%2528HC%2529%2529%2520and%2520the%2520in-house%2520IITD-AIIA%2520dataset%2520%2528including%2520subjects%250Awith%2520AD%252C%2520MCI%252C%2520and%2520HC%2529.%2520Different%2520classification%2520strategies%2520and%2520classifier%250Acombinations%2520have%2520been%2520utilized%2520for%2520the%2520accurate%2520mapping%2520of%2520classes%2520on%2520both%250Adatasets.%2520The%2520best%2520results%2520were%2520achieved%2520by%2520using%2520a%2520product%2520of%2520probabilities%250Afrom%2520classifiers%2520for%2520left%2520and%2520right%2520subcortical%2520regions%2520in%2520conjunction%2520with%2520the%250ADenseNet%2520model%2520architecture.%2520It%2520yields%2520accuracies%2520of%252094.17%2524%255C%2525%2524%2520and%252077.72%2524%255C%2525%2524%2520on%250Athe%2520BrainLat%2520and%2520IITD-AIIA%2520datasets%252C%2520respectively.%2520This%2520highlights%2520the%250Apotential%2520of%2520this%2520approach%2520for%2520early%2520and%2520accurate%2520differentiation%2520of%250Aneurodegenerative%2520disorders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning-based%20Classification%20of%20Dementia%20using%20Image%0A%20%20Representation%20of%20Subcortical%20Signals&entry.906535625=Shivani%20Ranjan%20and%20Ayush%20Tripathi%20and%20Harshal%20Shende%20and%20Robin%20Badal%20and%20Amit%20Kumar%20and%20Pramod%20Yadav%20and%20Deepak%20Joshi%20and%20Lalan%20Kumar&entry.1292438233=%20%20Dementia%20is%20a%20neurological%20syndrome%20marked%20by%20cognitive%20decline.%20Alzheimer%27s%0Adisease%20%28AD%29%20and%20Frontotemporal%20dementia%20%28FTD%29%20are%20the%20common%20forms%20of%0Adementia%2C%20each%20with%20distinct%20progression%20patterns.%20EEG%2C%20a%20non-invasive%20tool%20for%0Arecording%20brain%20activity%2C%20has%20shown%20potential%20in%20distinguishing%20AD%20from%20FTD%20and%0Amild%20cognitive%20impairment%20%28MCI%29.%20Previous%20studies%20have%20utilized%20various%20EEG%0Afeatures%2C%20such%20as%20subband%20power%20and%20connectivity%20patterns%20to%20differentiate%0Athese%20conditions.%20However%2C%20artifacts%20in%20EEG%20signals%20can%20obscure%20crucial%0Ainformation%2C%20necessitating%20advanced%20signal%20processing%20techniques.%20This%20study%0Aaims%20to%20develop%20a%20deep%20learning-based%20classification%20system%20for%20dementia%20by%0Aanalyzing%20scout%20time-series%20signals%20from%20deep%20brain%20regions%2C%20specifically%20the%0Ahippocampus%2C%20amygdala%2C%20and%20thalamus.%20The%20study%20utilizes%20scout%20time%20series%0Aextracted%20via%20the%20standardized%20low-resolution%20brain%20electromagnetic%20tomography%0A%28sLORETA%29%20technique.%20The%20time%20series%20is%20converted%20to%20image%20representations%0Ausing%20continuous%20wavelet%20transform%20%28CWT%29%20and%20fed%20as%20input%20to%20deep%20learning%0Amodels.%20Two%20high-density%20EEG%20datasets%20are%20utilized%20to%20check%20for%20the%20efficacy%20of%0Athe%20proposed%20method%3A%20the%20online%20BrainLat%20dataset%20%28comprising%20AD%2C%20FTD%2C%20and%0Ahealthy%20controls%20%28HC%29%29%20and%20the%20in-house%20IITD-AIIA%20dataset%20%28including%20subjects%0Awith%20AD%2C%20MCI%2C%20and%20HC%29.%20Different%20classification%20strategies%20and%20classifier%0Acombinations%20have%20been%20utilized%20for%20the%20accurate%20mapping%20of%20classes%20on%20both%0Adatasets.%20The%20best%20results%20were%20achieved%20by%20using%20a%20product%20of%20probabilities%0Afrom%20classifiers%20for%20left%20and%20right%20subcortical%20regions%20in%20conjunction%20with%20the%0ADenseNet%20model%20architecture.%20It%20yields%20accuracies%20of%2094.17%24%5C%25%24%20and%2077.72%24%5C%25%24%20on%0Athe%20BrainLat%20and%20IITD-AIIA%20datasets%2C%20respectively.%20This%20highlights%20the%0Apotential%20of%20this%20approach%20for%20early%20and%20accurate%20differentiation%20of%0Aneurodegenerative%20disorders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10816v1&entry.124074799=Read"},
{"title": "Non-autoregressive Generative Models for Reranking Recommendation", "author": "Yuxin Ren and Qiya Yang and Yichun Wu and Wei Xu and Yalong Wang and Zhiqiang Zhang", "abstract": "  Contemporary recommendation systems are designed to meet users' needs by\ndelivering tailored lists of items that align with their specific demands or\ninterests. In a multi-stage recommendation system, reranking plays a crucial\nrole by modeling the intra-list correlations among items. The key challenge of\nreranking lies in the exploration of optimal sequences within the combinatorial\nspace of permutations. Recent research proposes a generator-evaluator learning\nparadigm, where the generator generates multiple feasible sequences and the\nevaluator picks out the best sequence based on the estimated listwise score.\nThe generator is of vital importance, and generative models are well-suited for\nthe generator function. Current generative models employ an autoregressive\nstrategy for sequence generation. However, deploying autoregressive models in\nreal-time industrial systems is challenging. To address these issues, we\npropose a Non-AutoRegressive generative model for reranking Recommendation\n(NAR4Rec) designed to enhance efficiency and effectiveness. To tackle\nchallenges such as sparse training samples and dynamic candidates, we introduce\na matching model. Considering the diverse nature of user feedback, we employ a\nsequence-level unlikelihood training objective to differentiate feasible\nsequences from unfeasible ones. Additionally, to overcome the lack of\ndependency modeling in non-autoregressive models regarding target items, we\nintroduce contrastive decoding to capture correlations among these items.\nExtensive offline experiments validate the superior performance of NAR4Rec over\nstate-of-the-art reranking methods. Online A/B tests reveal that NAR4Rec\nsignificantly enhances the user experience. Furthermore, NAR4Rec has been fully\ndeployed in a popular video app Kuaishou with over 300 million daily active\nusers.\n", "link": "http://arxiv.org/abs/2402.06871v4", "date": "2024-08-20", "relevancy": 1.8833, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4931}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4556}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-autoregressive%20Generative%20Models%20for%20Reranking%20Recommendation&body=Title%3A%20Non-autoregressive%20Generative%20Models%20for%20Reranking%20Recommendation%0AAuthor%3A%20Yuxin%20Ren%20and%20Qiya%20Yang%20and%20Yichun%20Wu%20and%20Wei%20Xu%20and%20Yalong%20Wang%20and%20Zhiqiang%20Zhang%0AAbstract%3A%20%20%20Contemporary%20recommendation%20systems%20are%20designed%20to%20meet%20users%27%20needs%20by%0Adelivering%20tailored%20lists%20of%20items%20that%20align%20with%20their%20specific%20demands%20or%0Ainterests.%20In%20a%20multi-stage%20recommendation%20system%2C%20reranking%20plays%20a%20crucial%0Arole%20by%20modeling%20the%20intra-list%20correlations%20among%20items.%20The%20key%20challenge%20of%0Areranking%20lies%20in%20the%20exploration%20of%20optimal%20sequences%20within%20the%20combinatorial%0Aspace%20of%20permutations.%20Recent%20research%20proposes%20a%20generator-evaluator%20learning%0Aparadigm%2C%20where%20the%20generator%20generates%20multiple%20feasible%20sequences%20and%20the%0Aevaluator%20picks%20out%20the%20best%20sequence%20based%20on%20the%20estimated%20listwise%20score.%0AThe%20generator%20is%20of%20vital%20importance%2C%20and%20generative%20models%20are%20well-suited%20for%0Athe%20generator%20function.%20Current%20generative%20models%20employ%20an%20autoregressive%0Astrategy%20for%20sequence%20generation.%20However%2C%20deploying%20autoregressive%20models%20in%0Areal-time%20industrial%20systems%20is%20challenging.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20Non-AutoRegressive%20generative%20model%20for%20reranking%20Recommendation%0A%28NAR4Rec%29%20designed%20to%20enhance%20efficiency%20and%20effectiveness.%20To%20tackle%0Achallenges%20such%20as%20sparse%20training%20samples%20and%20dynamic%20candidates%2C%20we%20introduce%0Aa%20matching%20model.%20Considering%20the%20diverse%20nature%20of%20user%20feedback%2C%20we%20employ%20a%0Asequence-level%20unlikelihood%20training%20objective%20to%20differentiate%20feasible%0Asequences%20from%20unfeasible%20ones.%20Additionally%2C%20to%20overcome%20the%20lack%20of%0Adependency%20modeling%20in%20non-autoregressive%20models%20regarding%20target%20items%2C%20we%0Aintroduce%20contrastive%20decoding%20to%20capture%20correlations%20among%20these%20items.%0AExtensive%20offline%20experiments%20validate%20the%20superior%20performance%20of%20NAR4Rec%20over%0Astate-of-the-art%20reranking%20methods.%20Online%20A/B%20tests%20reveal%20that%20NAR4Rec%0Asignificantly%20enhances%20the%20user%20experience.%20Furthermore%2C%20NAR4Rec%20has%20been%20fully%0Adeployed%20in%20a%20popular%20video%20app%20Kuaishou%20with%20over%20300%20million%20daily%20active%0Ausers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06871v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-autoregressive%2520Generative%2520Models%2520for%2520Reranking%2520Recommendation%26entry.906535625%3DYuxin%2520Ren%2520and%2520Qiya%2520Yang%2520and%2520Yichun%2520Wu%2520and%2520Wei%2520Xu%2520and%2520Yalong%2520Wang%2520and%2520Zhiqiang%2520Zhang%26entry.1292438233%3D%2520%2520Contemporary%2520recommendation%2520systems%2520are%2520designed%2520to%2520meet%2520users%2527%2520needs%2520by%250Adelivering%2520tailored%2520lists%2520of%2520items%2520that%2520align%2520with%2520their%2520specific%2520demands%2520or%250Ainterests.%2520In%2520a%2520multi-stage%2520recommendation%2520system%252C%2520reranking%2520plays%2520a%2520crucial%250Arole%2520by%2520modeling%2520the%2520intra-list%2520correlations%2520among%2520items.%2520The%2520key%2520challenge%2520of%250Areranking%2520lies%2520in%2520the%2520exploration%2520of%2520optimal%2520sequences%2520within%2520the%2520combinatorial%250Aspace%2520of%2520permutations.%2520Recent%2520research%2520proposes%2520a%2520generator-evaluator%2520learning%250Aparadigm%252C%2520where%2520the%2520generator%2520generates%2520multiple%2520feasible%2520sequences%2520and%2520the%250Aevaluator%2520picks%2520out%2520the%2520best%2520sequence%2520based%2520on%2520the%2520estimated%2520listwise%2520score.%250AThe%2520generator%2520is%2520of%2520vital%2520importance%252C%2520and%2520generative%2520models%2520are%2520well-suited%2520for%250Athe%2520generator%2520function.%2520Current%2520generative%2520models%2520employ%2520an%2520autoregressive%250Astrategy%2520for%2520sequence%2520generation.%2520However%252C%2520deploying%2520autoregressive%2520models%2520in%250Areal-time%2520industrial%2520systems%2520is%2520challenging.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520a%2520Non-AutoRegressive%2520generative%2520model%2520for%2520reranking%2520Recommendation%250A%2528NAR4Rec%2529%2520designed%2520to%2520enhance%2520efficiency%2520and%2520effectiveness.%2520To%2520tackle%250Achallenges%2520such%2520as%2520sparse%2520training%2520samples%2520and%2520dynamic%2520candidates%252C%2520we%2520introduce%250Aa%2520matching%2520model.%2520Considering%2520the%2520diverse%2520nature%2520of%2520user%2520feedback%252C%2520we%2520employ%2520a%250Asequence-level%2520unlikelihood%2520training%2520objective%2520to%2520differentiate%2520feasible%250Asequences%2520from%2520unfeasible%2520ones.%2520Additionally%252C%2520to%2520overcome%2520the%2520lack%2520of%250Adependency%2520modeling%2520in%2520non-autoregressive%2520models%2520regarding%2520target%2520items%252C%2520we%250Aintroduce%2520contrastive%2520decoding%2520to%2520capture%2520correlations%2520among%2520these%2520items.%250AExtensive%2520offline%2520experiments%2520validate%2520the%2520superior%2520performance%2520of%2520NAR4Rec%2520over%250Astate-of-the-art%2520reranking%2520methods.%2520Online%2520A/B%2520tests%2520reveal%2520that%2520NAR4Rec%250Asignificantly%2520enhances%2520the%2520user%2520experience.%2520Furthermore%252C%2520NAR4Rec%2520has%2520been%2520fully%250Adeployed%2520in%2520a%2520popular%2520video%2520app%2520Kuaishou%2520with%2520over%2520300%2520million%2520daily%2520active%250Ausers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06871v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-autoregressive%20Generative%20Models%20for%20Reranking%20Recommendation&entry.906535625=Yuxin%20Ren%20and%20Qiya%20Yang%20and%20Yichun%20Wu%20and%20Wei%20Xu%20and%20Yalong%20Wang%20and%20Zhiqiang%20Zhang&entry.1292438233=%20%20Contemporary%20recommendation%20systems%20are%20designed%20to%20meet%20users%27%20needs%20by%0Adelivering%20tailored%20lists%20of%20items%20that%20align%20with%20their%20specific%20demands%20or%0Ainterests.%20In%20a%20multi-stage%20recommendation%20system%2C%20reranking%20plays%20a%20crucial%0Arole%20by%20modeling%20the%20intra-list%20correlations%20among%20items.%20The%20key%20challenge%20of%0Areranking%20lies%20in%20the%20exploration%20of%20optimal%20sequences%20within%20the%20combinatorial%0Aspace%20of%20permutations.%20Recent%20research%20proposes%20a%20generator-evaluator%20learning%0Aparadigm%2C%20where%20the%20generator%20generates%20multiple%20feasible%20sequences%20and%20the%0Aevaluator%20picks%20out%20the%20best%20sequence%20based%20on%20the%20estimated%20listwise%20score.%0AThe%20generator%20is%20of%20vital%20importance%2C%20and%20generative%20models%20are%20well-suited%20for%0Athe%20generator%20function.%20Current%20generative%20models%20employ%20an%20autoregressive%0Astrategy%20for%20sequence%20generation.%20However%2C%20deploying%20autoregressive%20models%20in%0Areal-time%20industrial%20systems%20is%20challenging.%20To%20address%20these%20issues%2C%20we%0Apropose%20a%20Non-AutoRegressive%20generative%20model%20for%20reranking%20Recommendation%0A%28NAR4Rec%29%20designed%20to%20enhance%20efficiency%20and%20effectiveness.%20To%20tackle%0Achallenges%20such%20as%20sparse%20training%20samples%20and%20dynamic%20candidates%2C%20we%20introduce%0Aa%20matching%20model.%20Considering%20the%20diverse%20nature%20of%20user%20feedback%2C%20we%20employ%20a%0Asequence-level%20unlikelihood%20training%20objective%20to%20differentiate%20feasible%0Asequences%20from%20unfeasible%20ones.%20Additionally%2C%20to%20overcome%20the%20lack%20of%0Adependency%20modeling%20in%20non-autoregressive%20models%20regarding%20target%20items%2C%20we%0Aintroduce%20contrastive%20decoding%20to%20capture%20correlations%20among%20these%20items.%0AExtensive%20offline%20experiments%20validate%20the%20superior%20performance%20of%20NAR4Rec%20over%0Astate-of-the-art%20reranking%20methods.%20Online%20A/B%20tests%20reveal%20that%20NAR4Rec%0Asignificantly%20enhances%20the%20user%20experience.%20Furthermore%2C%20NAR4Rec%20has%20been%20fully%0Adeployed%20in%20a%20popular%20video%20app%20Kuaishou%20with%20over%20300%20million%20daily%20active%0Ausers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06871v4&entry.124074799=Read"},
{"title": "Kernel-Based Differentiable Learning of Non-Parametric Directed Acyclic\n  Graphical Models", "author": "Yurou Liang and Oleksandr Zadorozhnyi and Mathias Drton", "abstract": "  Causal discovery amounts to learning a directed acyclic graph (DAG) that\nencodes a causal model. This model selection problem can be challenging due to\nits large combinatorial search space, particularly when dealing with\nnon-parametric causal models. Recent research has sought to bypass the\ncombinatorial search by reformulating causal discovery as a continuous\noptimization problem, employing constraints that ensure the acyclicity of the\ngraph. In non-parametric settings, existing approaches typically rely on\nfinite-dimensional approximations of the relationships between nodes, resulting\nin a score-based continuous optimization problem with a smooth acyclicity\nconstraint. In this work, we develop an alternative approximation method by\nutilizing reproducing kernel Hilbert spaces (RKHS) and applying general\nsparsity-inducing regularization terms based on partial derivatives. Within\nthis framework, we introduce an extended RKHS representer theorem. To enforce\nacyclicity, we advocate the log-determinant formulation of the acyclicity\nconstraint and show its stability. Finally, we assess the performance of our\nproposed RKHS-DAGMA procedure through simulations and illustrative data\nanalyses.\n", "link": "http://arxiv.org/abs/2408.10976v1", "date": "2024-08-20", "relevancy": 1.8825, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4772}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4675}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kernel-Based%20Differentiable%20Learning%20of%20Non-Parametric%20Directed%20Acyclic%0A%20%20Graphical%20Models&body=Title%3A%20Kernel-Based%20Differentiable%20Learning%20of%20Non-Parametric%20Directed%20Acyclic%0A%20%20Graphical%20Models%0AAuthor%3A%20Yurou%20Liang%20and%20Oleksandr%20Zadorozhnyi%20and%20Mathias%20Drton%0AAbstract%3A%20%20%20Causal%20discovery%20amounts%20to%20learning%20a%20directed%20acyclic%20graph%20%28DAG%29%20that%0Aencodes%20a%20causal%20model.%20This%20model%20selection%20problem%20can%20be%20challenging%20due%20to%0Aits%20large%20combinatorial%20search%20space%2C%20particularly%20when%20dealing%20with%0Anon-parametric%20causal%20models.%20Recent%20research%20has%20sought%20to%20bypass%20the%0Acombinatorial%20search%20by%20reformulating%20causal%20discovery%20as%20a%20continuous%0Aoptimization%20problem%2C%20employing%20constraints%20that%20ensure%20the%20acyclicity%20of%20the%0Agraph.%20In%20non-parametric%20settings%2C%20existing%20approaches%20typically%20rely%20on%0Afinite-dimensional%20approximations%20of%20the%20relationships%20between%20nodes%2C%20resulting%0Ain%20a%20score-based%20continuous%20optimization%20problem%20with%20a%20smooth%20acyclicity%0Aconstraint.%20In%20this%20work%2C%20we%20develop%20an%20alternative%20approximation%20method%20by%0Autilizing%20reproducing%20kernel%20Hilbert%20spaces%20%28RKHS%29%20and%20applying%20general%0Asparsity-inducing%20regularization%20terms%20based%20on%20partial%20derivatives.%20Within%0Athis%20framework%2C%20we%20introduce%20an%20extended%20RKHS%20representer%20theorem.%20To%20enforce%0Aacyclicity%2C%20we%20advocate%20the%20log-determinant%20formulation%20of%20the%20acyclicity%0Aconstraint%20and%20show%20its%20stability.%20Finally%2C%20we%20assess%20the%20performance%20of%20our%0Aproposed%20RKHS-DAGMA%20procedure%20through%20simulations%20and%20illustrative%20data%0Aanalyses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernel-Based%2520Differentiable%2520Learning%2520of%2520Non-Parametric%2520Directed%2520Acyclic%250A%2520%2520Graphical%2520Models%26entry.906535625%3DYurou%2520Liang%2520and%2520Oleksandr%2520Zadorozhnyi%2520and%2520Mathias%2520Drton%26entry.1292438233%3D%2520%2520Causal%2520discovery%2520amounts%2520to%2520learning%2520a%2520directed%2520acyclic%2520graph%2520%2528DAG%2529%2520that%250Aencodes%2520a%2520causal%2520model.%2520This%2520model%2520selection%2520problem%2520can%2520be%2520challenging%2520due%2520to%250Aits%2520large%2520combinatorial%2520search%2520space%252C%2520particularly%2520when%2520dealing%2520with%250Anon-parametric%2520causal%2520models.%2520Recent%2520research%2520has%2520sought%2520to%2520bypass%2520the%250Acombinatorial%2520search%2520by%2520reformulating%2520causal%2520discovery%2520as%2520a%2520continuous%250Aoptimization%2520problem%252C%2520employing%2520constraints%2520that%2520ensure%2520the%2520acyclicity%2520of%2520the%250Agraph.%2520In%2520non-parametric%2520settings%252C%2520existing%2520approaches%2520typically%2520rely%2520on%250Afinite-dimensional%2520approximations%2520of%2520the%2520relationships%2520between%2520nodes%252C%2520resulting%250Ain%2520a%2520score-based%2520continuous%2520optimization%2520problem%2520with%2520a%2520smooth%2520acyclicity%250Aconstraint.%2520In%2520this%2520work%252C%2520we%2520develop%2520an%2520alternative%2520approximation%2520method%2520by%250Autilizing%2520reproducing%2520kernel%2520Hilbert%2520spaces%2520%2528RKHS%2529%2520and%2520applying%2520general%250Asparsity-inducing%2520regularization%2520terms%2520based%2520on%2520partial%2520derivatives.%2520Within%250Athis%2520framework%252C%2520we%2520introduce%2520an%2520extended%2520RKHS%2520representer%2520theorem.%2520To%2520enforce%250Aacyclicity%252C%2520we%2520advocate%2520the%2520log-determinant%2520formulation%2520of%2520the%2520acyclicity%250Aconstraint%2520and%2520show%2520its%2520stability.%2520Finally%252C%2520we%2520assess%2520the%2520performance%2520of%2520our%250Aproposed%2520RKHS-DAGMA%2520procedure%2520through%2520simulations%2520and%2520illustrative%2520data%250Aanalyses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel-Based%20Differentiable%20Learning%20of%20Non-Parametric%20Directed%20Acyclic%0A%20%20Graphical%20Models&entry.906535625=Yurou%20Liang%20and%20Oleksandr%20Zadorozhnyi%20and%20Mathias%20Drton&entry.1292438233=%20%20Causal%20discovery%20amounts%20to%20learning%20a%20directed%20acyclic%20graph%20%28DAG%29%20that%0Aencodes%20a%20causal%20model.%20This%20model%20selection%20problem%20can%20be%20challenging%20due%20to%0Aits%20large%20combinatorial%20search%20space%2C%20particularly%20when%20dealing%20with%0Anon-parametric%20causal%20models.%20Recent%20research%20has%20sought%20to%20bypass%20the%0Acombinatorial%20search%20by%20reformulating%20causal%20discovery%20as%20a%20continuous%0Aoptimization%20problem%2C%20employing%20constraints%20that%20ensure%20the%20acyclicity%20of%20the%0Agraph.%20In%20non-parametric%20settings%2C%20existing%20approaches%20typically%20rely%20on%0Afinite-dimensional%20approximations%20of%20the%20relationships%20between%20nodes%2C%20resulting%0Ain%20a%20score-based%20continuous%20optimization%20problem%20with%20a%20smooth%20acyclicity%0Aconstraint.%20In%20this%20work%2C%20we%20develop%20an%20alternative%20approximation%20method%20by%0Autilizing%20reproducing%20kernel%20Hilbert%20spaces%20%28RKHS%29%20and%20applying%20general%0Asparsity-inducing%20regularization%20terms%20based%20on%20partial%20derivatives.%20Within%0Athis%20framework%2C%20we%20introduce%20an%20extended%20RKHS%20representer%20theorem.%20To%20enforce%0Aacyclicity%2C%20we%20advocate%20the%20log-determinant%20formulation%20of%20the%20acyclicity%0Aconstraint%20and%20show%20its%20stability.%20Finally%2C%20we%20assess%20the%20performance%20of%20our%0Aproposed%20RKHS-DAGMA%20procedure%20through%20simulations%20and%20illustrative%20data%0Aanalyses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10976v1&entry.124074799=Read"},
{"title": "Robust Regression with Ensembles Communicating over Noisy Channels", "author": "Yuval Ben-Hur and Yuval Cassuto", "abstract": "  As machine-learning models grow in size, their implementation requirements\ncannot be met by a single computer system. This observation motivates\ndistributed settings, in which intermediate computations are performed across a\nnetwork of processing units, while the central node only aggregates their\noutputs. However, distributing inference tasks across low-precision or faulty\nedge devices, operating over a network of noisy communication channels, gives\nrise to serious reliability challenges. We study the problem of an ensemble of\ndevices, implementing regression algorithms, that communicate through additive\nnoisy channels in order to collaboratively perform a joint regression task. We\ndefine the problem formally, and develop methods for optimizing the aggregation\ncoefficients for the parameters of the noise in the channels, which can\npotentially be correlated. Our results apply to the leading state-of-the-art\nensemble regression methods: bagging and gradient boosting. We demonstrate the\neffectiveness of our algorithms on both synthetic and real-world datasets.\n", "link": "http://arxiv.org/abs/2408.10942v1", "date": "2024-08-20", "relevancy": 1.8748, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4979}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4609}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Regression%20with%20Ensembles%20Communicating%20over%20Noisy%20Channels&body=Title%3A%20Robust%20Regression%20with%20Ensembles%20Communicating%20over%20Noisy%20Channels%0AAuthor%3A%20Yuval%20Ben-Hur%20and%20Yuval%20Cassuto%0AAbstract%3A%20%20%20As%20machine-learning%20models%20grow%20in%20size%2C%20their%20implementation%20requirements%0Acannot%20be%20met%20by%20a%20single%20computer%20system.%20This%20observation%20motivates%0Adistributed%20settings%2C%20in%20which%20intermediate%20computations%20are%20performed%20across%20a%0Anetwork%20of%20processing%20units%2C%20while%20the%20central%20node%20only%20aggregates%20their%0Aoutputs.%20However%2C%20distributing%20inference%20tasks%20across%20low-precision%20or%20faulty%0Aedge%20devices%2C%20operating%20over%20a%20network%20of%20noisy%20communication%20channels%2C%20gives%0Arise%20to%20serious%20reliability%20challenges.%20We%20study%20the%20problem%20of%20an%20ensemble%20of%0Adevices%2C%20implementing%20regression%20algorithms%2C%20that%20communicate%20through%20additive%0Anoisy%20channels%20in%20order%20to%20collaboratively%20perform%20a%20joint%20regression%20task.%20We%0Adefine%20the%20problem%20formally%2C%20and%20develop%20methods%20for%20optimizing%20the%20aggregation%0Acoefficients%20for%20the%20parameters%20of%20the%20noise%20in%20the%20channels%2C%20which%20can%0Apotentially%20be%20correlated.%20Our%20results%20apply%20to%20the%20leading%20state-of-the-art%0Aensemble%20regression%20methods%3A%20bagging%20and%20gradient%20boosting.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20algorithms%20on%20both%20synthetic%20and%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Regression%2520with%2520Ensembles%2520Communicating%2520over%2520Noisy%2520Channels%26entry.906535625%3DYuval%2520Ben-Hur%2520and%2520Yuval%2520Cassuto%26entry.1292438233%3D%2520%2520As%2520machine-learning%2520models%2520grow%2520in%2520size%252C%2520their%2520implementation%2520requirements%250Acannot%2520be%2520met%2520by%2520a%2520single%2520computer%2520system.%2520This%2520observation%2520motivates%250Adistributed%2520settings%252C%2520in%2520which%2520intermediate%2520computations%2520are%2520performed%2520across%2520a%250Anetwork%2520of%2520processing%2520units%252C%2520while%2520the%2520central%2520node%2520only%2520aggregates%2520their%250Aoutputs.%2520However%252C%2520distributing%2520inference%2520tasks%2520across%2520low-precision%2520or%2520faulty%250Aedge%2520devices%252C%2520operating%2520over%2520a%2520network%2520of%2520noisy%2520communication%2520channels%252C%2520gives%250Arise%2520to%2520serious%2520reliability%2520challenges.%2520We%2520study%2520the%2520problem%2520of%2520an%2520ensemble%2520of%250Adevices%252C%2520implementing%2520regression%2520algorithms%252C%2520that%2520communicate%2520through%2520additive%250Anoisy%2520channels%2520in%2520order%2520to%2520collaboratively%2520perform%2520a%2520joint%2520regression%2520task.%2520We%250Adefine%2520the%2520problem%2520formally%252C%2520and%2520develop%2520methods%2520for%2520optimizing%2520the%2520aggregation%250Acoefficients%2520for%2520the%2520parameters%2520of%2520the%2520noise%2520in%2520the%2520channels%252C%2520which%2520can%250Apotentially%2520be%2520correlated.%2520Our%2520results%2520apply%2520to%2520the%2520leading%2520state-of-the-art%250Aensemble%2520regression%2520methods%253A%2520bagging%2520and%2520gradient%2520boosting.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520algorithms%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Regression%20with%20Ensembles%20Communicating%20over%20Noisy%20Channels&entry.906535625=Yuval%20Ben-Hur%20and%20Yuval%20Cassuto&entry.1292438233=%20%20As%20machine-learning%20models%20grow%20in%20size%2C%20their%20implementation%20requirements%0Acannot%20be%20met%20by%20a%20single%20computer%20system.%20This%20observation%20motivates%0Adistributed%20settings%2C%20in%20which%20intermediate%20computations%20are%20performed%20across%20a%0Anetwork%20of%20processing%20units%2C%20while%20the%20central%20node%20only%20aggregates%20their%0Aoutputs.%20However%2C%20distributing%20inference%20tasks%20across%20low-precision%20or%20faulty%0Aedge%20devices%2C%20operating%20over%20a%20network%20of%20noisy%20communication%20channels%2C%20gives%0Arise%20to%20serious%20reliability%20challenges.%20We%20study%20the%20problem%20of%20an%20ensemble%20of%0Adevices%2C%20implementing%20regression%20algorithms%2C%20that%20communicate%20through%20additive%0Anoisy%20channels%20in%20order%20to%20collaboratively%20perform%20a%20joint%20regression%20task.%20We%0Adefine%20the%20problem%20formally%2C%20and%20develop%20methods%20for%20optimizing%20the%20aggregation%0Acoefficients%20for%20the%20parameters%20of%20the%20noise%20in%20the%20channels%2C%20which%20can%0Apotentially%20be%20correlated.%20Our%20results%20apply%20to%20the%20leading%20state-of-the-art%0Aensemble%20regression%20methods%3A%20bagging%20and%20gradient%20boosting.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20algorithms%20on%20both%20synthetic%20and%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10942v1&entry.124074799=Read"},
{"title": "CELLM: An Efficient Communication in Large Language Models Training for\n  Federated Learning", "author": "Raja Vavekanand and Kira Sam", "abstract": "  Federated Learning (FL) is a recent model training paradigm in which client\ndevices collaboratively train a model without ever aggregating their data.\nCrucially, this scheme offers users potential privacy and security benefits by\nonly ever communicating updates to the model weights to a central server as\nopposed to traditional machine learning (ML) training which directly\ncommunicates and aggregates data. However, FL training suffers from statistical\nheterogeneity as clients may have differing local data distributions. Large\nlanguage models (LLMs) offer a potential solution to this issue of\nheterogeneity given that they have consistently been shown to be able to learn\non vast amounts of noisy data. While LLMs are a promising development for\nresolving the consistent issue of non-I.I.D. Clients in federated settings\nexacerbate two other bottlenecks in FL: limited local computing and expensive\ncommunication. This thesis aims to develop efficient training methods for LLMs\nin FL. To this end, we employ two critical techniques in enabling efficient\ntraining. First, we use low-rank adaptation (LoRA) to reduce the computational\nload of local model training. Second, we communicate sparse updates throughout\ntraining to significantly cut down on communication costs. Taken together, our\nmethod reduces communication costs by up to 10x over vanilla LoRA and up to 5x\nover more complex sparse LoRA baselines while achieving greater utility. We\nemphasize the importance of carefully applying sparsity and picking effective\nrank and sparsity configurations for federated LLM training.\n", "link": "http://arxiv.org/abs/2407.20557v2", "date": "2024-08-20", "relevancy": 1.8709, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4711}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4662}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CELLM%3A%20An%20Efficient%20Communication%20in%20Large%20Language%20Models%20Training%20for%0A%20%20Federated%20Learning&body=Title%3A%20CELLM%3A%20An%20Efficient%20Communication%20in%20Large%20Language%20Models%20Training%20for%0A%20%20Federated%20Learning%0AAuthor%3A%20Raja%20Vavekanand%20and%20Kira%20Sam%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20recent%20model%20training%20paradigm%20in%20which%20client%0Adevices%20collaboratively%20train%20a%20model%20without%20ever%20aggregating%20their%20data.%0ACrucially%2C%20this%20scheme%20offers%20users%20potential%20privacy%20and%20security%20benefits%20by%0Aonly%20ever%20communicating%20updates%20to%20the%20model%20weights%20to%20a%20central%20server%20as%0Aopposed%20to%20traditional%20machine%20learning%20%28ML%29%20training%20which%20directly%0Acommunicates%20and%20aggregates%20data.%20However%2C%20FL%20training%20suffers%20from%20statistical%0Aheterogeneity%20as%20clients%20may%20have%20differing%20local%20data%20distributions.%20Large%0Alanguage%20models%20%28LLMs%29%20offer%20a%20potential%20solution%20to%20this%20issue%20of%0Aheterogeneity%20given%20that%20they%20have%20consistently%20been%20shown%20to%20be%20able%20to%20learn%0Aon%20vast%20amounts%20of%20noisy%20data.%20While%20LLMs%20are%20a%20promising%20development%20for%0Aresolving%20the%20consistent%20issue%20of%20non-I.I.D.%20Clients%20in%20federated%20settings%0Aexacerbate%20two%20other%20bottlenecks%20in%20FL%3A%20limited%20local%20computing%20and%20expensive%0Acommunication.%20This%20thesis%20aims%20to%20develop%20efficient%20training%20methods%20for%20LLMs%0Ain%20FL.%20To%20this%20end%2C%20we%20employ%20two%20critical%20techniques%20in%20enabling%20efficient%0Atraining.%20First%2C%20we%20use%20low-rank%20adaptation%20%28LoRA%29%20to%20reduce%20the%20computational%0Aload%20of%20local%20model%20training.%20Second%2C%20we%20communicate%20sparse%20updates%20throughout%0Atraining%20to%20significantly%20cut%20down%20on%20communication%20costs.%20Taken%20together%2C%20our%0Amethod%20reduces%20communication%20costs%20by%20up%20to%2010x%20over%20vanilla%20LoRA%20and%20up%20to%205x%0Aover%20more%20complex%20sparse%20LoRA%20baselines%20while%20achieving%20greater%20utility.%20We%0Aemphasize%20the%20importance%20of%20carefully%20applying%20sparsity%20and%20picking%20effective%0Arank%20and%20sparsity%20configurations%20for%20federated%20LLM%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20557v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCELLM%253A%2520An%2520Efficient%2520Communication%2520in%2520Large%2520Language%2520Models%2520Training%2520for%250A%2520%2520Federated%2520Learning%26entry.906535625%3DRaja%2520Vavekanand%2520and%2520Kira%2520Sam%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520recent%2520model%2520training%2520paradigm%2520in%2520which%2520client%250Adevices%2520collaboratively%2520train%2520a%2520model%2520without%2520ever%2520aggregating%2520their%2520data.%250ACrucially%252C%2520this%2520scheme%2520offers%2520users%2520potential%2520privacy%2520and%2520security%2520benefits%2520by%250Aonly%2520ever%2520communicating%2520updates%2520to%2520the%2520model%2520weights%2520to%2520a%2520central%2520server%2520as%250Aopposed%2520to%2520traditional%2520machine%2520learning%2520%2528ML%2529%2520training%2520which%2520directly%250Acommunicates%2520and%2520aggregates%2520data.%2520However%252C%2520FL%2520training%2520suffers%2520from%2520statistical%250Aheterogeneity%2520as%2520clients%2520may%2520have%2520differing%2520local%2520data%2520distributions.%2520Large%250Alanguage%2520models%2520%2528LLMs%2529%2520offer%2520a%2520potential%2520solution%2520to%2520this%2520issue%2520of%250Aheterogeneity%2520given%2520that%2520they%2520have%2520consistently%2520been%2520shown%2520to%2520be%2520able%2520to%2520learn%250Aon%2520vast%2520amounts%2520of%2520noisy%2520data.%2520While%2520LLMs%2520are%2520a%2520promising%2520development%2520for%250Aresolving%2520the%2520consistent%2520issue%2520of%2520non-I.I.D.%2520Clients%2520in%2520federated%2520settings%250Aexacerbate%2520two%2520other%2520bottlenecks%2520in%2520FL%253A%2520limited%2520local%2520computing%2520and%2520expensive%250Acommunication.%2520This%2520thesis%2520aims%2520to%2520develop%2520efficient%2520training%2520methods%2520for%2520LLMs%250Ain%2520FL.%2520To%2520this%2520end%252C%2520we%2520employ%2520two%2520critical%2520techniques%2520in%2520enabling%2520efficient%250Atraining.%2520First%252C%2520we%2520use%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520to%2520reduce%2520the%2520computational%250Aload%2520of%2520local%2520model%2520training.%2520Second%252C%2520we%2520communicate%2520sparse%2520updates%2520throughout%250Atraining%2520to%2520significantly%2520cut%2520down%2520on%2520communication%2520costs.%2520Taken%2520together%252C%2520our%250Amethod%2520reduces%2520communication%2520costs%2520by%2520up%2520to%252010x%2520over%2520vanilla%2520LoRA%2520and%2520up%2520to%25205x%250Aover%2520more%2520complex%2520sparse%2520LoRA%2520baselines%2520while%2520achieving%2520greater%2520utility.%2520We%250Aemphasize%2520the%2520importance%2520of%2520carefully%2520applying%2520sparsity%2520and%2520picking%2520effective%250Arank%2520and%2520sparsity%2520configurations%2520for%2520federated%2520LLM%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20557v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CELLM%3A%20An%20Efficient%20Communication%20in%20Large%20Language%20Models%20Training%20for%0A%20%20Federated%20Learning&entry.906535625=Raja%20Vavekanand%20and%20Kira%20Sam&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20recent%20model%20training%20paradigm%20in%20which%20client%0Adevices%20collaboratively%20train%20a%20model%20without%20ever%20aggregating%20their%20data.%0ACrucially%2C%20this%20scheme%20offers%20users%20potential%20privacy%20and%20security%20benefits%20by%0Aonly%20ever%20communicating%20updates%20to%20the%20model%20weights%20to%20a%20central%20server%20as%0Aopposed%20to%20traditional%20machine%20learning%20%28ML%29%20training%20which%20directly%0Acommunicates%20and%20aggregates%20data.%20However%2C%20FL%20training%20suffers%20from%20statistical%0Aheterogeneity%20as%20clients%20may%20have%20differing%20local%20data%20distributions.%20Large%0Alanguage%20models%20%28LLMs%29%20offer%20a%20potential%20solution%20to%20this%20issue%20of%0Aheterogeneity%20given%20that%20they%20have%20consistently%20been%20shown%20to%20be%20able%20to%20learn%0Aon%20vast%20amounts%20of%20noisy%20data.%20While%20LLMs%20are%20a%20promising%20development%20for%0Aresolving%20the%20consistent%20issue%20of%20non-I.I.D.%20Clients%20in%20federated%20settings%0Aexacerbate%20two%20other%20bottlenecks%20in%20FL%3A%20limited%20local%20computing%20and%20expensive%0Acommunication.%20This%20thesis%20aims%20to%20develop%20efficient%20training%20methods%20for%20LLMs%0Ain%20FL.%20To%20this%20end%2C%20we%20employ%20two%20critical%20techniques%20in%20enabling%20efficient%0Atraining.%20First%2C%20we%20use%20low-rank%20adaptation%20%28LoRA%29%20to%20reduce%20the%20computational%0Aload%20of%20local%20model%20training.%20Second%2C%20we%20communicate%20sparse%20updates%20throughout%0Atraining%20to%20significantly%20cut%20down%20on%20communication%20costs.%20Taken%20together%2C%20our%0Amethod%20reduces%20communication%20costs%20by%20up%20to%2010x%20over%20vanilla%20LoRA%20and%20up%20to%205x%0Aover%20more%20complex%20sparse%20LoRA%20baselines%20while%20achieving%20greater%20utility.%20We%0Aemphasize%20the%20importance%20of%20carefully%20applying%20sparsity%20and%20picking%20effective%0Arank%20and%20sparsity%20configurations%20for%20federated%20LLM%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20557v2&entry.124074799=Read"},
{"title": "Side-Channel Analysis of OpenVINO-based Neural Network Models", "author": "Dirmanto Jap and Jakub Breier and Zdenko Lehock\u00fd and Shivam Bhasin and Xiaolu Hou", "abstract": "  Embedded devices with neural network accelerators offer great versatility for\ntheir users, reducing the need to use cloud-based services. At the same time,\nthey introduce new security challenges in the area of hardware attacks, the\nmost prominent being side-channel analysis (SCA). It was shown that SCA can\nrecover model parameters with a high accuracy, posing a threat to entities that\nwish to keep their models confidential. In this paper, we explore the\nsusceptibility of quantized models implemented in OpenVINO, an embedded\nframework for deploying neural networks on embedded and Edge devices. We show\nthat it is possible to recover model parameters with high precision, allowing\nthe recovered model to perform very close to the original one. Our experiments\non GoogleNet v1 show only a 1% difference in the Top 1 and a 0.64% difference\nin the Top 5 accuracies.\n", "link": "http://arxiv.org/abs/2407.16467v2", "date": "2024-08-20", "relevancy": 1.8686, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5106}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.466}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Side-Channel%20Analysis%20of%20OpenVINO-based%20Neural%20Network%20Models&body=Title%3A%20Side-Channel%20Analysis%20of%20OpenVINO-based%20Neural%20Network%20Models%0AAuthor%3A%20Dirmanto%20Jap%20and%20Jakub%20Breier%20and%20Zdenko%20Lehock%C3%BD%20and%20Shivam%20Bhasin%20and%20Xiaolu%20Hou%0AAbstract%3A%20%20%20Embedded%20devices%20with%20neural%20network%20accelerators%20offer%20great%20versatility%20for%0Atheir%20users%2C%20reducing%20the%20need%20to%20use%20cloud-based%20services.%20At%20the%20same%20time%2C%0Athey%20introduce%20new%20security%20challenges%20in%20the%20area%20of%20hardware%20attacks%2C%20the%0Amost%20prominent%20being%20side-channel%20analysis%20%28SCA%29.%20It%20was%20shown%20that%20SCA%20can%0Arecover%20model%20parameters%20with%20a%20high%20accuracy%2C%20posing%20a%20threat%20to%20entities%20that%0Awish%20to%20keep%20their%20models%20confidential.%20In%20this%20paper%2C%20we%20explore%20the%0Asusceptibility%20of%20quantized%20models%20implemented%20in%20OpenVINO%2C%20an%20embedded%0Aframework%20for%20deploying%20neural%20networks%20on%20embedded%20and%20Edge%20devices.%20We%20show%0Athat%20it%20is%20possible%20to%20recover%20model%20parameters%20with%20high%20precision%2C%20allowing%0Athe%20recovered%20model%20to%20perform%20very%20close%20to%20the%20original%20one.%20Our%20experiments%0Aon%20GoogleNet%20v1%20show%20only%20a%201%25%20difference%20in%20the%20Top%201%20and%20a%200.64%25%20difference%0Ain%20the%20Top%205%20accuracies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16467v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSide-Channel%2520Analysis%2520of%2520OpenVINO-based%2520Neural%2520Network%2520Models%26entry.906535625%3DDirmanto%2520Jap%2520and%2520Jakub%2520Breier%2520and%2520Zdenko%2520Lehock%25C3%25BD%2520and%2520Shivam%2520Bhasin%2520and%2520Xiaolu%2520Hou%26entry.1292438233%3D%2520%2520Embedded%2520devices%2520with%2520neural%2520network%2520accelerators%2520offer%2520great%2520versatility%2520for%250Atheir%2520users%252C%2520reducing%2520the%2520need%2520to%2520use%2520cloud-based%2520services.%2520At%2520the%2520same%2520time%252C%250Athey%2520introduce%2520new%2520security%2520challenges%2520in%2520the%2520area%2520of%2520hardware%2520attacks%252C%2520the%250Amost%2520prominent%2520being%2520side-channel%2520analysis%2520%2528SCA%2529.%2520It%2520was%2520shown%2520that%2520SCA%2520can%250Arecover%2520model%2520parameters%2520with%2520a%2520high%2520accuracy%252C%2520posing%2520a%2520threat%2520to%2520entities%2520that%250Awish%2520to%2520keep%2520their%2520models%2520confidential.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%250Asusceptibility%2520of%2520quantized%2520models%2520implemented%2520in%2520OpenVINO%252C%2520an%2520embedded%250Aframework%2520for%2520deploying%2520neural%2520networks%2520on%2520embedded%2520and%2520Edge%2520devices.%2520We%2520show%250Athat%2520it%2520is%2520possible%2520to%2520recover%2520model%2520parameters%2520with%2520high%2520precision%252C%2520allowing%250Athe%2520recovered%2520model%2520to%2520perform%2520very%2520close%2520to%2520the%2520original%2520one.%2520Our%2520experiments%250Aon%2520GoogleNet%2520v1%2520show%2520only%2520a%25201%2525%2520difference%2520in%2520the%2520Top%25201%2520and%2520a%25200.64%2525%2520difference%250Ain%2520the%2520Top%25205%2520accuracies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16467v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Side-Channel%20Analysis%20of%20OpenVINO-based%20Neural%20Network%20Models&entry.906535625=Dirmanto%20Jap%20and%20Jakub%20Breier%20and%20Zdenko%20Lehock%C3%BD%20and%20Shivam%20Bhasin%20and%20Xiaolu%20Hou&entry.1292438233=%20%20Embedded%20devices%20with%20neural%20network%20accelerators%20offer%20great%20versatility%20for%0Atheir%20users%2C%20reducing%20the%20need%20to%20use%20cloud-based%20services.%20At%20the%20same%20time%2C%0Athey%20introduce%20new%20security%20challenges%20in%20the%20area%20of%20hardware%20attacks%2C%20the%0Amost%20prominent%20being%20side-channel%20analysis%20%28SCA%29.%20It%20was%20shown%20that%20SCA%20can%0Arecover%20model%20parameters%20with%20a%20high%20accuracy%2C%20posing%20a%20threat%20to%20entities%20that%0Awish%20to%20keep%20their%20models%20confidential.%20In%20this%20paper%2C%20we%20explore%20the%0Asusceptibility%20of%20quantized%20models%20implemented%20in%20OpenVINO%2C%20an%20embedded%0Aframework%20for%20deploying%20neural%20networks%20on%20embedded%20and%20Edge%20devices.%20We%20show%0Athat%20it%20is%20possible%20to%20recover%20model%20parameters%20with%20high%20precision%2C%20allowing%0Athe%20recovered%20model%20to%20perform%20very%20close%20to%20the%20original%20one.%20Our%20experiments%0Aon%20GoogleNet%20v1%20show%20only%20a%201%25%20difference%20in%20the%20Top%201%20and%20a%200.64%25%20difference%0Ain%20the%20Top%205%20accuracies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16467v2&entry.124074799=Read"},
{"title": "SSL-TTS: Leveraging Self-Supervised Embeddings and kNN Retrieval for\n  Zero-Shot Multi-speaker TTS", "author": "Karl El Hajal and Ajinkya Kulkarni and Enno Hermann and Mathew Magimai. -Doss", "abstract": "  While recent zero-shot multispeaker text-to-speech (TTS) models achieve\nimpressive results, they typically rely on extensive transcribed speech\ndatasets from numerous speakers and intricate training pipelines. Meanwhile,\nself-supervised learning (SSL) speech features have emerged as effective\nintermediate representations for TTS. It was also observed that SSL features\nfrom different speakers that are linearly close share phonetic information\nwhile maintaining individual speaker identity, which enables straight-forward\nand robust voice cloning. In this study, we introduce SSL-TTS, a lightweight\nand efficient zero-shot TTS framework trained on transcribed speech from a\nsingle speaker. SSL-TTS leverages SSL features and retrieval methods for simple\nand robust zero-shot multi-speaker synthesis. Objective and subjective\nevaluations show that our approach achieves performance comparable to\nstate-of-the-art models that require significantly larger training datasets.\nThe low training data requirements mean that SSL-TTS is well suited for the\ndevelopment of multi-speaker TTS systems for low-resource domains and\nlanguages. We also introduce an interpolation parameter which enables fine\ncontrol over the output speech by blending voices. Demo samples are available\nat https://idiap.github.io/ssl-tts\n", "link": "http://arxiv.org/abs/2408.10771v1", "date": "2024-08-20", "relevancy": 1.8608, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.486}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4529}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSL-TTS%3A%20Leveraging%20Self-Supervised%20Embeddings%20and%20kNN%20Retrieval%20for%0A%20%20Zero-Shot%20Multi-speaker%20TTS&body=Title%3A%20SSL-TTS%3A%20Leveraging%20Self-Supervised%20Embeddings%20and%20kNN%20Retrieval%20for%0A%20%20Zero-Shot%20Multi-speaker%20TTS%0AAuthor%3A%20Karl%20El%20Hajal%20and%20Ajinkya%20Kulkarni%20and%20Enno%20Hermann%20and%20Mathew%20Magimai.%20-Doss%0AAbstract%3A%20%20%20While%20recent%20zero-shot%20multispeaker%20text-to-speech%20%28TTS%29%20models%20achieve%0Aimpressive%20results%2C%20they%20typically%20rely%20on%20extensive%20transcribed%20speech%0Adatasets%20from%20numerous%20speakers%20and%20intricate%20training%20pipelines.%20Meanwhile%2C%0Aself-supervised%20learning%20%28SSL%29%20speech%20features%20have%20emerged%20as%20effective%0Aintermediate%20representations%20for%20TTS.%20It%20was%20also%20observed%20that%20SSL%20features%0Afrom%20different%20speakers%20that%20are%20linearly%20close%20share%20phonetic%20information%0Awhile%20maintaining%20individual%20speaker%20identity%2C%20which%20enables%20straight-forward%0Aand%20robust%20voice%20cloning.%20In%20this%20study%2C%20we%20introduce%20SSL-TTS%2C%20a%20lightweight%0Aand%20efficient%20zero-shot%20TTS%20framework%20trained%20on%20transcribed%20speech%20from%20a%0Asingle%20speaker.%20SSL-TTS%20leverages%20SSL%20features%20and%20retrieval%20methods%20for%20simple%0Aand%20robust%20zero-shot%20multi-speaker%20synthesis.%20Objective%20and%20subjective%0Aevaluations%20show%20that%20our%20approach%20achieves%20performance%20comparable%20to%0Astate-of-the-art%20models%20that%20require%20significantly%20larger%20training%20datasets.%0AThe%20low%20training%20data%20requirements%20mean%20that%20SSL-TTS%20is%20well%20suited%20for%20the%0Adevelopment%20of%20multi-speaker%20TTS%20systems%20for%20low-resource%20domains%20and%0Alanguages.%20We%20also%20introduce%20an%20interpolation%20parameter%20which%20enables%20fine%0Acontrol%20over%20the%20output%20speech%20by%20blending%20voices.%20Demo%20samples%20are%20available%0Aat%20https%3A//idiap.github.io/ssl-tts%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSL-TTS%253A%2520Leveraging%2520Self-Supervised%2520Embeddings%2520and%2520kNN%2520Retrieval%2520for%250A%2520%2520Zero-Shot%2520Multi-speaker%2520TTS%26entry.906535625%3DKarl%2520El%2520Hajal%2520and%2520Ajinkya%2520Kulkarni%2520and%2520Enno%2520Hermann%2520and%2520Mathew%2520Magimai.%2520-Doss%26entry.1292438233%3D%2520%2520While%2520recent%2520zero-shot%2520multispeaker%2520text-to-speech%2520%2528TTS%2529%2520models%2520achieve%250Aimpressive%2520results%252C%2520they%2520typically%2520rely%2520on%2520extensive%2520transcribed%2520speech%250Adatasets%2520from%2520numerous%2520speakers%2520and%2520intricate%2520training%2520pipelines.%2520Meanwhile%252C%250Aself-supervised%2520learning%2520%2528SSL%2529%2520speech%2520features%2520have%2520emerged%2520as%2520effective%250Aintermediate%2520representations%2520for%2520TTS.%2520It%2520was%2520also%2520observed%2520that%2520SSL%2520features%250Afrom%2520different%2520speakers%2520that%2520are%2520linearly%2520close%2520share%2520phonetic%2520information%250Awhile%2520maintaining%2520individual%2520speaker%2520identity%252C%2520which%2520enables%2520straight-forward%250Aand%2520robust%2520voice%2520cloning.%2520In%2520this%2520study%252C%2520we%2520introduce%2520SSL-TTS%252C%2520a%2520lightweight%250Aand%2520efficient%2520zero-shot%2520TTS%2520framework%2520trained%2520on%2520transcribed%2520speech%2520from%2520a%250Asingle%2520speaker.%2520SSL-TTS%2520leverages%2520SSL%2520features%2520and%2520retrieval%2520methods%2520for%2520simple%250Aand%2520robust%2520zero-shot%2520multi-speaker%2520synthesis.%2520Objective%2520and%2520subjective%250Aevaluations%2520show%2520that%2520our%2520approach%2520achieves%2520performance%2520comparable%2520to%250Astate-of-the-art%2520models%2520that%2520require%2520significantly%2520larger%2520training%2520datasets.%250AThe%2520low%2520training%2520data%2520requirements%2520mean%2520that%2520SSL-TTS%2520is%2520well%2520suited%2520for%2520the%250Adevelopment%2520of%2520multi-speaker%2520TTS%2520systems%2520for%2520low-resource%2520domains%2520and%250Alanguages.%2520We%2520also%2520introduce%2520an%2520interpolation%2520parameter%2520which%2520enables%2520fine%250Acontrol%2520over%2520the%2520output%2520speech%2520by%2520blending%2520voices.%2520Demo%2520samples%2520are%2520available%250Aat%2520https%253A//idiap.github.io/ssl-tts%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSL-TTS%3A%20Leveraging%20Self-Supervised%20Embeddings%20and%20kNN%20Retrieval%20for%0A%20%20Zero-Shot%20Multi-speaker%20TTS&entry.906535625=Karl%20El%20Hajal%20and%20Ajinkya%20Kulkarni%20and%20Enno%20Hermann%20and%20Mathew%20Magimai.%20-Doss&entry.1292438233=%20%20While%20recent%20zero-shot%20multispeaker%20text-to-speech%20%28TTS%29%20models%20achieve%0Aimpressive%20results%2C%20they%20typically%20rely%20on%20extensive%20transcribed%20speech%0Adatasets%20from%20numerous%20speakers%20and%20intricate%20training%20pipelines.%20Meanwhile%2C%0Aself-supervised%20learning%20%28SSL%29%20speech%20features%20have%20emerged%20as%20effective%0Aintermediate%20representations%20for%20TTS.%20It%20was%20also%20observed%20that%20SSL%20features%0Afrom%20different%20speakers%20that%20are%20linearly%20close%20share%20phonetic%20information%0Awhile%20maintaining%20individual%20speaker%20identity%2C%20which%20enables%20straight-forward%0Aand%20robust%20voice%20cloning.%20In%20this%20study%2C%20we%20introduce%20SSL-TTS%2C%20a%20lightweight%0Aand%20efficient%20zero-shot%20TTS%20framework%20trained%20on%20transcribed%20speech%20from%20a%0Asingle%20speaker.%20SSL-TTS%20leverages%20SSL%20features%20and%20retrieval%20methods%20for%20simple%0Aand%20robust%20zero-shot%20multi-speaker%20synthesis.%20Objective%20and%20subjective%0Aevaluations%20show%20that%20our%20approach%20achieves%20performance%20comparable%20to%0Astate-of-the-art%20models%20that%20require%20significantly%20larger%20training%20datasets.%0AThe%20low%20training%20data%20requirements%20mean%20that%20SSL-TTS%20is%20well%20suited%20for%20the%0Adevelopment%20of%20multi-speaker%20TTS%20systems%20for%20low-resource%20domains%20and%0Alanguages.%20We%20also%20introduce%20an%20interpolation%20parameter%20which%20enables%20fine%0Acontrol%20over%20the%20output%20speech%20by%20blending%20voices.%20Demo%20samples%20are%20available%0Aat%20https%3A//idiap.github.io/ssl-tts%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10771v1&entry.124074799=Read"},
{"title": "Using Unreliable Pseudo-Labels for Label-Efficient Semantic Segmentation", "author": "Haochen Wang and Yuchao Wang and Yujun Shen and Junsong Fan and Yuxi Wang and Zhaoxiang Zhang", "abstract": "  The crux of label-efficient semantic segmentation is to produce high-quality\npseudo-labels to leverage a large amount of unlabeled or weakly labeled data. A\ncommon practice is to select the highly confident predictions as the\npseudo-ground-truths for each pixel, but it leads to a problem that most pixels\nmay be left unused due to their unreliability. However, we argue that every\npixel matters to the model training, even those unreliable and ambiguous\npixels. Intuitively, an unreliable prediction may get confused among the top\nclasses, however, it should be confident about the pixel not belonging to the\nremaining classes. Hence, such a pixel can be convincingly treated as a\nnegative key to those most unlikely categories. Therefore, we develop an\neffective pipeline to make sufficient use of unlabeled data. Concretely, we\nseparate reliable and unreliable pixels via the entropy of predictions, push\neach unreliable pixel to a category-wise queue that consists of negative keys,\nand manage to train the model with all candidate pixels. Considering the\ntraining evolution, we adaptively adjust the threshold for the\nreliable-unreliable partition. Experimental results on various benchmarks and\ntraining settings demonstrate the superiority of our approach over the\nstate-of-the-art alternatives.\n", "link": "http://arxiv.org/abs/2306.02314v2", "date": "2024-08-20", "relevancy": 1.6129, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5735}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5332}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Unreliable%20Pseudo-Labels%20for%20Label-Efficient%20Semantic%20Segmentation&body=Title%3A%20Using%20Unreliable%20Pseudo-Labels%20for%20Label-Efficient%20Semantic%20Segmentation%0AAuthor%3A%20Haochen%20Wang%20and%20Yuchao%20Wang%20and%20Yujun%20Shen%20and%20Junsong%20Fan%20and%20Yuxi%20Wang%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20The%20crux%20of%20label-efficient%20semantic%20segmentation%20is%20to%20produce%20high-quality%0Apseudo-labels%20to%20leverage%20a%20large%20amount%20of%20unlabeled%20or%20weakly%20labeled%20data.%20A%0Acommon%20practice%20is%20to%20select%20the%20highly%20confident%20predictions%20as%20the%0Apseudo-ground-truths%20for%20each%20pixel%2C%20but%20it%20leads%20to%20a%20problem%20that%20most%20pixels%0Amay%20be%20left%20unused%20due%20to%20their%20unreliability.%20However%2C%20we%20argue%20that%20every%0Apixel%20matters%20to%20the%20model%20training%2C%20even%20those%20unreliable%20and%20ambiguous%0Apixels.%20Intuitively%2C%20an%20unreliable%20prediction%20may%20get%20confused%20among%20the%20top%0Aclasses%2C%20however%2C%20it%20should%20be%20confident%20about%20the%20pixel%20not%20belonging%20to%20the%0Aremaining%20classes.%20Hence%2C%20such%20a%20pixel%20can%20be%20convincingly%20treated%20as%20a%0Anegative%20key%20to%20those%20most%20unlikely%20categories.%20Therefore%2C%20we%20develop%20an%0Aeffective%20pipeline%20to%20make%20sufficient%20use%20of%20unlabeled%20data.%20Concretely%2C%20we%0Aseparate%20reliable%20and%20unreliable%20pixels%20via%20the%20entropy%20of%20predictions%2C%20push%0Aeach%20unreliable%20pixel%20to%20a%20category-wise%20queue%20that%20consists%20of%20negative%20keys%2C%0Aand%20manage%20to%20train%20the%20model%20with%20all%20candidate%20pixels.%20Considering%20the%0Atraining%20evolution%2C%20we%20adaptively%20adjust%20the%20threshold%20for%20the%0Areliable-unreliable%20partition.%20Experimental%20results%20on%20various%20benchmarks%20and%0Atraining%20settings%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20the%0Astate-of-the-art%20alternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02314v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Unreliable%2520Pseudo-Labels%2520for%2520Label-Efficient%2520Semantic%2520Segmentation%26entry.906535625%3DHaochen%2520Wang%2520and%2520Yuchao%2520Wang%2520and%2520Yujun%2520Shen%2520and%2520Junsong%2520Fan%2520and%2520Yuxi%2520Wang%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520The%2520crux%2520of%2520label-efficient%2520semantic%2520segmentation%2520is%2520to%2520produce%2520high-quality%250Apseudo-labels%2520to%2520leverage%2520a%2520large%2520amount%2520of%2520unlabeled%2520or%2520weakly%2520labeled%2520data.%2520A%250Acommon%2520practice%2520is%2520to%2520select%2520the%2520highly%2520confident%2520predictions%2520as%2520the%250Apseudo-ground-truths%2520for%2520each%2520pixel%252C%2520but%2520it%2520leads%2520to%2520a%2520problem%2520that%2520most%2520pixels%250Amay%2520be%2520left%2520unused%2520due%2520to%2520their%2520unreliability.%2520However%252C%2520we%2520argue%2520that%2520every%250Apixel%2520matters%2520to%2520the%2520model%2520training%252C%2520even%2520those%2520unreliable%2520and%2520ambiguous%250Apixels.%2520Intuitively%252C%2520an%2520unreliable%2520prediction%2520may%2520get%2520confused%2520among%2520the%2520top%250Aclasses%252C%2520however%252C%2520it%2520should%2520be%2520confident%2520about%2520the%2520pixel%2520not%2520belonging%2520to%2520the%250Aremaining%2520classes.%2520Hence%252C%2520such%2520a%2520pixel%2520can%2520be%2520convincingly%2520treated%2520as%2520a%250Anegative%2520key%2520to%2520those%2520most%2520unlikely%2520categories.%2520Therefore%252C%2520we%2520develop%2520an%250Aeffective%2520pipeline%2520to%2520make%2520sufficient%2520use%2520of%2520unlabeled%2520data.%2520Concretely%252C%2520we%250Aseparate%2520reliable%2520and%2520unreliable%2520pixels%2520via%2520the%2520entropy%2520of%2520predictions%252C%2520push%250Aeach%2520unreliable%2520pixel%2520to%2520a%2520category-wise%2520queue%2520that%2520consists%2520of%2520negative%2520keys%252C%250Aand%2520manage%2520to%2520train%2520the%2520model%2520with%2520all%2520candidate%2520pixels.%2520Considering%2520the%250Atraining%2520evolution%252C%2520we%2520adaptively%2520adjust%2520the%2520threshold%2520for%2520the%250Areliable-unreliable%2520partition.%2520Experimental%2520results%2520on%2520various%2520benchmarks%2520and%250Atraining%2520settings%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520over%2520the%250Astate-of-the-art%2520alternatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.02314v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Unreliable%20Pseudo-Labels%20for%20Label-Efficient%20Semantic%20Segmentation&entry.906535625=Haochen%20Wang%20and%20Yuchao%20Wang%20and%20Yujun%20Shen%20and%20Junsong%20Fan%20and%20Yuxi%20Wang%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20The%20crux%20of%20label-efficient%20semantic%20segmentation%20is%20to%20produce%20high-quality%0Apseudo-labels%20to%20leverage%20a%20large%20amount%20of%20unlabeled%20or%20weakly%20labeled%20data.%20A%0Acommon%20practice%20is%20to%20select%20the%20highly%20confident%20predictions%20as%20the%0Apseudo-ground-truths%20for%20each%20pixel%2C%20but%20it%20leads%20to%20a%20problem%20that%20most%20pixels%0Amay%20be%20left%20unused%20due%20to%20their%20unreliability.%20However%2C%20we%20argue%20that%20every%0Apixel%20matters%20to%20the%20model%20training%2C%20even%20those%20unreliable%20and%20ambiguous%0Apixels.%20Intuitively%2C%20an%20unreliable%20prediction%20may%20get%20confused%20among%20the%20top%0Aclasses%2C%20however%2C%20it%20should%20be%20confident%20about%20the%20pixel%20not%20belonging%20to%20the%0Aremaining%20classes.%20Hence%2C%20such%20a%20pixel%20can%20be%20convincingly%20treated%20as%20a%0Anegative%20key%20to%20those%20most%20unlikely%20categories.%20Therefore%2C%20we%20develop%20an%0Aeffective%20pipeline%20to%20make%20sufficient%20use%20of%20unlabeled%20data.%20Concretely%2C%20we%0Aseparate%20reliable%20and%20unreliable%20pixels%20via%20the%20entropy%20of%20predictions%2C%20push%0Aeach%20unreliable%20pixel%20to%20a%20category-wise%20queue%20that%20consists%20of%20negative%20keys%2C%0Aand%20manage%20to%20train%20the%20model%20with%20all%20candidate%20pixels.%20Considering%20the%0Atraining%20evolution%2C%20we%20adaptively%20adjust%20the%20threshold%20for%20the%0Areliable-unreliable%20partition.%20Experimental%20results%20on%20various%20benchmarks%20and%0Atraining%20settings%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20the%0Astate-of-the-art%20alternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02314v2&entry.124074799=Read"},
{"title": "PetFace: A Large-Scale Dataset and Benchmark for Animal Identification", "author": "Risa Shinoda and Kaede Shiohara", "abstract": "  Automated animal face identification plays a crucial role in the monitoring\nof behaviors, conducting of surveys, and finding of lost animals. Despite the\nadvancements in human face identification, the lack of datasets and benchmarks\nin the animal domain has impeded progress. In this paper, we introduce the\nPetFace dataset, a comprehensive resource for animal face identification\nencompassing 257,484 unique individuals across 13 animal families and 319 breed\ncategories, including both experimental and pet animals. This large-scale\ncollection of individuals facilitates the investigation of unseen animal face\nverification, an area that has not been sufficiently explored in existing\ndatasets due to the limited number of individuals. Moreover, PetFace also has\nfine-grained annotations such as sex, breed, color, and pattern. We provide\nmultiple benchmarks including re-identification for seen individuals and\nverification for unseen individuals. The models trained on our dataset\noutperform those trained on prior datasets, even for detailed breed variations\nand unseen animal families. Our result also indicates that there is some room\nto improve the performance of integrated identification on multiple animal\nfamilies. We hope the PetFace dataset will facilitate animal face\nidentification and encourage the development of non-invasive animal automatic\nidentification methods.\n", "link": "http://arxiv.org/abs/2407.13555v2", "date": "2024-08-20", "relevancy": 1.849, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4727}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.463}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PetFace%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Animal%20Identification&body=Title%3A%20PetFace%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Animal%20Identification%0AAuthor%3A%20Risa%20Shinoda%20and%20Kaede%20Shiohara%0AAbstract%3A%20%20%20Automated%20animal%20face%20identification%20plays%20a%20crucial%20role%20in%20the%20monitoring%0Aof%20behaviors%2C%20conducting%20of%20surveys%2C%20and%20finding%20of%20lost%20animals.%20Despite%20the%0Aadvancements%20in%20human%20face%20identification%2C%20the%20lack%20of%20datasets%20and%20benchmarks%0Ain%20the%20animal%20domain%20has%20impeded%20progress.%20In%20this%20paper%2C%20we%20introduce%20the%0APetFace%20dataset%2C%20a%20comprehensive%20resource%20for%20animal%20face%20identification%0Aencompassing%20257%2C484%20unique%20individuals%20across%2013%20animal%20families%20and%20319%20breed%0Acategories%2C%20including%20both%20experimental%20and%20pet%20animals.%20This%20large-scale%0Acollection%20of%20individuals%20facilitates%20the%20investigation%20of%20unseen%20animal%20face%0Averification%2C%20an%20area%20that%20has%20not%20been%20sufficiently%20explored%20in%20existing%0Adatasets%20due%20to%20the%20limited%20number%20of%20individuals.%20Moreover%2C%20PetFace%20also%20has%0Afine-grained%20annotations%20such%20as%20sex%2C%20breed%2C%20color%2C%20and%20pattern.%20We%20provide%0Amultiple%20benchmarks%20including%20re-identification%20for%20seen%20individuals%20and%0Averification%20for%20unseen%20individuals.%20The%20models%20trained%20on%20our%20dataset%0Aoutperform%20those%20trained%20on%20prior%20datasets%2C%20even%20for%20detailed%20breed%20variations%0Aand%20unseen%20animal%20families.%20Our%20result%20also%20indicates%20that%20there%20is%20some%20room%0Ato%20improve%20the%20performance%20of%20integrated%20identification%20on%20multiple%20animal%0Afamilies.%20We%20hope%20the%20PetFace%20dataset%20will%20facilitate%20animal%20face%0Aidentification%20and%20encourage%20the%20development%20of%20non-invasive%20animal%20automatic%0Aidentification%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13555v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPetFace%253A%2520A%2520Large-Scale%2520Dataset%2520and%2520Benchmark%2520for%2520Animal%2520Identification%26entry.906535625%3DRisa%2520Shinoda%2520and%2520Kaede%2520Shiohara%26entry.1292438233%3D%2520%2520Automated%2520animal%2520face%2520identification%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520monitoring%250Aof%2520behaviors%252C%2520conducting%2520of%2520surveys%252C%2520and%2520finding%2520of%2520lost%2520animals.%2520Despite%2520the%250Aadvancements%2520in%2520human%2520face%2520identification%252C%2520the%2520lack%2520of%2520datasets%2520and%2520benchmarks%250Ain%2520the%2520animal%2520domain%2520has%2520impeded%2520progress.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%250APetFace%2520dataset%252C%2520a%2520comprehensive%2520resource%2520for%2520animal%2520face%2520identification%250Aencompassing%2520257%252C484%2520unique%2520individuals%2520across%252013%2520animal%2520families%2520and%2520319%2520breed%250Acategories%252C%2520including%2520both%2520experimental%2520and%2520pet%2520animals.%2520This%2520large-scale%250Acollection%2520of%2520individuals%2520facilitates%2520the%2520investigation%2520of%2520unseen%2520animal%2520face%250Averification%252C%2520an%2520area%2520that%2520has%2520not%2520been%2520sufficiently%2520explored%2520in%2520existing%250Adatasets%2520due%2520to%2520the%2520limited%2520number%2520of%2520individuals.%2520Moreover%252C%2520PetFace%2520also%2520has%250Afine-grained%2520annotations%2520such%2520as%2520sex%252C%2520breed%252C%2520color%252C%2520and%2520pattern.%2520We%2520provide%250Amultiple%2520benchmarks%2520including%2520re-identification%2520for%2520seen%2520individuals%2520and%250Averification%2520for%2520unseen%2520individuals.%2520The%2520models%2520trained%2520on%2520our%2520dataset%250Aoutperform%2520those%2520trained%2520on%2520prior%2520datasets%252C%2520even%2520for%2520detailed%2520breed%2520variations%250Aand%2520unseen%2520animal%2520families.%2520Our%2520result%2520also%2520indicates%2520that%2520there%2520is%2520some%2520room%250Ato%2520improve%2520the%2520performance%2520of%2520integrated%2520identification%2520on%2520multiple%2520animal%250Afamilies.%2520We%2520hope%2520the%2520PetFace%2520dataset%2520will%2520facilitate%2520animal%2520face%250Aidentification%2520and%2520encourage%2520the%2520development%2520of%2520non-invasive%2520animal%2520automatic%250Aidentification%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13555v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PetFace%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Animal%20Identification&entry.906535625=Risa%20Shinoda%20and%20Kaede%20Shiohara&entry.1292438233=%20%20Automated%20animal%20face%20identification%20plays%20a%20crucial%20role%20in%20the%20monitoring%0Aof%20behaviors%2C%20conducting%20of%20surveys%2C%20and%20finding%20of%20lost%20animals.%20Despite%20the%0Aadvancements%20in%20human%20face%20identification%2C%20the%20lack%20of%20datasets%20and%20benchmarks%0Ain%20the%20animal%20domain%20has%20impeded%20progress.%20In%20this%20paper%2C%20we%20introduce%20the%0APetFace%20dataset%2C%20a%20comprehensive%20resource%20for%20animal%20face%20identification%0Aencompassing%20257%2C484%20unique%20individuals%20across%2013%20animal%20families%20and%20319%20breed%0Acategories%2C%20including%20both%20experimental%20and%20pet%20animals.%20This%20large-scale%0Acollection%20of%20individuals%20facilitates%20the%20investigation%20of%20unseen%20animal%20face%0Averification%2C%20an%20area%20that%20has%20not%20been%20sufficiently%20explored%20in%20existing%0Adatasets%20due%20to%20the%20limited%20number%20of%20individuals.%20Moreover%2C%20PetFace%20also%20has%0Afine-grained%20annotations%20such%20as%20sex%2C%20breed%2C%20color%2C%20and%20pattern.%20We%20provide%0Amultiple%20benchmarks%20including%20re-identification%20for%20seen%20individuals%20and%0Averification%20for%20unseen%20individuals.%20The%20models%20trained%20on%20our%20dataset%0Aoutperform%20those%20trained%20on%20prior%20datasets%2C%20even%20for%20detailed%20breed%20variations%0Aand%20unseen%20animal%20families.%20Our%20result%20also%20indicates%20that%20there%20is%20some%20room%0Ato%20improve%20the%20performance%20of%20integrated%20identification%20on%20multiple%20animal%0Afamilies.%20We%20hope%20the%20PetFace%20dataset%20will%20facilitate%20animal%20face%0Aidentification%20and%20encourage%20the%20development%20of%20non-invasive%20animal%20automatic%0Aidentification%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13555v2&entry.124074799=Read"},
{"title": "Recursive Model-agnostic Inverse Dynamics of Serial Soft-Rigid Robots", "author": "Pietro Pustina and Cosimo Della Santina and Alessandro De Luca", "abstract": "  Robotics is shifting from rigid, articulated systems to more sophisticated\nand heterogeneous mechanical structures. Soft robots, for example, have\ncontinuously deformable elements capable of large deformations. The flourishing\nof control techniques developed for this class of systems is fueling the need\nof efficient procedures for evaluating their inverse dynamics (ID), which is\nchallenging due to the complex and mixed nature of these systems. As of today,\nno single ID algorithm can describe the behavior of generic (combinations of)\nmodels of soft robots. We address this challenge for generic series-like\ninterconnections of possibly soft structures that may require heterogeneous\nmodeling techniques. Our proposed algorithm requires as input a purely\ngeometric description (forward-kinematics-like) of the mapping from\nconfiguration space to deformation space. With this information only, the\ncomplete equations of motion can be given an exact recursive structure which is\nessentially independent from (or `agnostic' to) the underlying reduced-order\nkinematic modeling techniques. We achieve this goal by exploiting Kane's method\nto manipulate the equations of motion, showing then their recursive structure.\nThe resulting ID algorithms have optimal computational complexity within the\nproposed setting, i.e., linear in the number of distinct modules. Further, a\nvariation of the algorithm is introduced that can evaluate the generalized mass\nmatrix without increasing computation costs. We showcase the applicability of\nthis method to robot models involving a mixture of rigid and soft elements,\ndescribed via possibly heterogeneous reduced order models (ROMs), such as\nVolumetric FEM, Cosserat strain-based, and volume-preserving deformation\nprimitives. None of these systems can be handled using existing ID techniques.\n", "link": "http://arxiv.org/abs/2402.07037v2", "date": "2024-08-20", "relevancy": 1.5242, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5419}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5216}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recursive%20Model-agnostic%20Inverse%20Dynamics%20of%20Serial%20Soft-Rigid%20Robots&body=Title%3A%20Recursive%20Model-agnostic%20Inverse%20Dynamics%20of%20Serial%20Soft-Rigid%20Robots%0AAuthor%3A%20Pietro%20Pustina%20and%20Cosimo%20Della%20Santina%20and%20Alessandro%20De%20Luca%0AAbstract%3A%20%20%20Robotics%20is%20shifting%20from%20rigid%2C%20articulated%20systems%20to%20more%20sophisticated%0Aand%20heterogeneous%20mechanical%20structures.%20Soft%20robots%2C%20for%20example%2C%20have%0Acontinuously%20deformable%20elements%20capable%20of%20large%20deformations.%20The%20flourishing%0Aof%20control%20techniques%20developed%20for%20this%20class%20of%20systems%20is%20fueling%20the%20need%0Aof%20efficient%20procedures%20for%20evaluating%20their%20inverse%20dynamics%20%28ID%29%2C%20which%20is%0Achallenging%20due%20to%20the%20complex%20and%20mixed%20nature%20of%20these%20systems.%20As%20of%20today%2C%0Ano%20single%20ID%20algorithm%20can%20describe%20the%20behavior%20of%20generic%20%28combinations%20of%29%0Amodels%20of%20soft%20robots.%20We%20address%20this%20challenge%20for%20generic%20series-like%0Ainterconnections%20of%20possibly%20soft%20structures%20that%20may%20require%20heterogeneous%0Amodeling%20techniques.%20Our%20proposed%20algorithm%20requires%20as%20input%20a%20purely%0Ageometric%20description%20%28forward-kinematics-like%29%20of%20the%20mapping%20from%0Aconfiguration%20space%20to%20deformation%20space.%20With%20this%20information%20only%2C%20the%0Acomplete%20equations%20of%20motion%20can%20be%20given%20an%20exact%20recursive%20structure%20which%20is%0Aessentially%20independent%20from%20%28or%20%60agnostic%27%20to%29%20the%20underlying%20reduced-order%0Akinematic%20modeling%20techniques.%20We%20achieve%20this%20goal%20by%20exploiting%20Kane%27s%20method%0Ato%20manipulate%20the%20equations%20of%20motion%2C%20showing%20then%20their%20recursive%20structure.%0AThe%20resulting%20ID%20algorithms%20have%20optimal%20computational%20complexity%20within%20the%0Aproposed%20setting%2C%20i.e.%2C%20linear%20in%20the%20number%20of%20distinct%20modules.%20Further%2C%20a%0Avariation%20of%20the%20algorithm%20is%20introduced%20that%20can%20evaluate%20the%20generalized%20mass%0Amatrix%20without%20increasing%20computation%20costs.%20We%20showcase%20the%20applicability%20of%0Athis%20method%20to%20robot%20models%20involving%20a%20mixture%20of%20rigid%20and%20soft%20elements%2C%0Adescribed%20via%20possibly%20heterogeneous%20reduced%20order%20models%20%28ROMs%29%2C%20such%20as%0AVolumetric%20FEM%2C%20Cosserat%20strain-based%2C%20and%20volume-preserving%20deformation%0Aprimitives.%20None%20of%20these%20systems%20can%20be%20handled%20using%20existing%20ID%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07037v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecursive%2520Model-agnostic%2520Inverse%2520Dynamics%2520of%2520Serial%2520Soft-Rigid%2520Robots%26entry.906535625%3DPietro%2520Pustina%2520and%2520Cosimo%2520Della%2520Santina%2520and%2520Alessandro%2520De%2520Luca%26entry.1292438233%3D%2520%2520Robotics%2520is%2520shifting%2520from%2520rigid%252C%2520articulated%2520systems%2520to%2520more%2520sophisticated%250Aand%2520heterogeneous%2520mechanical%2520structures.%2520Soft%2520robots%252C%2520for%2520example%252C%2520have%250Acontinuously%2520deformable%2520elements%2520capable%2520of%2520large%2520deformations.%2520The%2520flourishing%250Aof%2520control%2520techniques%2520developed%2520for%2520this%2520class%2520of%2520systems%2520is%2520fueling%2520the%2520need%250Aof%2520efficient%2520procedures%2520for%2520evaluating%2520their%2520inverse%2520dynamics%2520%2528ID%2529%252C%2520which%2520is%250Achallenging%2520due%2520to%2520the%2520complex%2520and%2520mixed%2520nature%2520of%2520these%2520systems.%2520As%2520of%2520today%252C%250Ano%2520single%2520ID%2520algorithm%2520can%2520describe%2520the%2520behavior%2520of%2520generic%2520%2528combinations%2520of%2529%250Amodels%2520of%2520soft%2520robots.%2520We%2520address%2520this%2520challenge%2520for%2520generic%2520series-like%250Ainterconnections%2520of%2520possibly%2520soft%2520structures%2520that%2520may%2520require%2520heterogeneous%250Amodeling%2520techniques.%2520Our%2520proposed%2520algorithm%2520requires%2520as%2520input%2520a%2520purely%250Ageometric%2520description%2520%2528forward-kinematics-like%2529%2520of%2520the%2520mapping%2520from%250Aconfiguration%2520space%2520to%2520deformation%2520space.%2520With%2520this%2520information%2520only%252C%2520the%250Acomplete%2520equations%2520of%2520motion%2520can%2520be%2520given%2520an%2520exact%2520recursive%2520structure%2520which%2520is%250Aessentially%2520independent%2520from%2520%2528or%2520%2560agnostic%2527%2520to%2529%2520the%2520underlying%2520reduced-order%250Akinematic%2520modeling%2520techniques.%2520We%2520achieve%2520this%2520goal%2520by%2520exploiting%2520Kane%2527s%2520method%250Ato%2520manipulate%2520the%2520equations%2520of%2520motion%252C%2520showing%2520then%2520their%2520recursive%2520structure.%250AThe%2520resulting%2520ID%2520algorithms%2520have%2520optimal%2520computational%2520complexity%2520within%2520the%250Aproposed%2520setting%252C%2520i.e.%252C%2520linear%2520in%2520the%2520number%2520of%2520distinct%2520modules.%2520Further%252C%2520a%250Avariation%2520of%2520the%2520algorithm%2520is%2520introduced%2520that%2520can%2520evaluate%2520the%2520generalized%2520mass%250Amatrix%2520without%2520increasing%2520computation%2520costs.%2520We%2520showcase%2520the%2520applicability%2520of%250Athis%2520method%2520to%2520robot%2520models%2520involving%2520a%2520mixture%2520of%2520rigid%2520and%2520soft%2520elements%252C%250Adescribed%2520via%2520possibly%2520heterogeneous%2520reduced%2520order%2520models%2520%2528ROMs%2529%252C%2520such%2520as%250AVolumetric%2520FEM%252C%2520Cosserat%2520strain-based%252C%2520and%2520volume-preserving%2520deformation%250Aprimitives.%2520None%2520of%2520these%2520systems%2520can%2520be%2520handled%2520using%2520existing%2520ID%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07037v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recursive%20Model-agnostic%20Inverse%20Dynamics%20of%20Serial%20Soft-Rigid%20Robots&entry.906535625=Pietro%20Pustina%20and%20Cosimo%20Della%20Santina%20and%20Alessandro%20De%20Luca&entry.1292438233=%20%20Robotics%20is%20shifting%20from%20rigid%2C%20articulated%20systems%20to%20more%20sophisticated%0Aand%20heterogeneous%20mechanical%20structures.%20Soft%20robots%2C%20for%20example%2C%20have%0Acontinuously%20deformable%20elements%20capable%20of%20large%20deformations.%20The%20flourishing%0Aof%20control%20techniques%20developed%20for%20this%20class%20of%20systems%20is%20fueling%20the%20need%0Aof%20efficient%20procedures%20for%20evaluating%20their%20inverse%20dynamics%20%28ID%29%2C%20which%20is%0Achallenging%20due%20to%20the%20complex%20and%20mixed%20nature%20of%20these%20systems.%20As%20of%20today%2C%0Ano%20single%20ID%20algorithm%20can%20describe%20the%20behavior%20of%20generic%20%28combinations%20of%29%0Amodels%20of%20soft%20robots.%20We%20address%20this%20challenge%20for%20generic%20series-like%0Ainterconnections%20of%20possibly%20soft%20structures%20that%20may%20require%20heterogeneous%0Amodeling%20techniques.%20Our%20proposed%20algorithm%20requires%20as%20input%20a%20purely%0Ageometric%20description%20%28forward-kinematics-like%29%20of%20the%20mapping%20from%0Aconfiguration%20space%20to%20deformation%20space.%20With%20this%20information%20only%2C%20the%0Acomplete%20equations%20of%20motion%20can%20be%20given%20an%20exact%20recursive%20structure%20which%20is%0Aessentially%20independent%20from%20%28or%20%60agnostic%27%20to%29%20the%20underlying%20reduced-order%0Akinematic%20modeling%20techniques.%20We%20achieve%20this%20goal%20by%20exploiting%20Kane%27s%20method%0Ato%20manipulate%20the%20equations%20of%20motion%2C%20showing%20then%20their%20recursive%20structure.%0AThe%20resulting%20ID%20algorithms%20have%20optimal%20computational%20complexity%20within%20the%0Aproposed%20setting%2C%20i.e.%2C%20linear%20in%20the%20number%20of%20distinct%20modules.%20Further%2C%20a%0Avariation%20of%20the%20algorithm%20is%20introduced%20that%20can%20evaluate%20the%20generalized%20mass%0Amatrix%20without%20increasing%20computation%20costs.%20We%20showcase%20the%20applicability%20of%0Athis%20method%20to%20robot%20models%20involving%20a%20mixture%20of%20rigid%20and%20soft%20elements%2C%0Adescribed%20via%20possibly%20heterogeneous%20reduced%20order%20models%20%28ROMs%29%2C%20such%20as%0AVolumetric%20FEM%2C%20Cosserat%20strain-based%2C%20and%20volume-preserving%20deformation%0Aprimitives.%20None%20of%20these%20systems%20can%20be%20handled%20using%20existing%20ID%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07037v2&entry.124074799=Read"},
{"title": "CO2Wounds-V2: Extended Chronic Wounds Dataset From Leprosy Patients", "author": "Karen Sanchez and Carlos Hinojosa and Olinto Mieles and Chen Zhao and Bernard Ghanem and Henry Arguello", "abstract": "  Chronic wounds pose an ongoing health concern globally, largely due to the\nprevalence of conditions such as diabetes and leprosy's disease. The standard\nmethod of monitoring these wounds involves visual inspection by healthcare\nprofessionals, a practice that could present challenges for patients in remote\nareas with inadequate transportation and healthcare infrastructure. This has\nled to the development of algorithms designed for the analysis and follow-up of\nwound images, which perform image-processing tasks such as classification,\ndetection, and segmentation. However, the effectiveness of these algorithms\nheavily depends on the availability of comprehensive and varied wound image\ndata, which is usually scarce. This paper introduces the CO2Wounds-V2 dataset,\nan extended collection of RGB wound images from leprosy patients with their\ncorresponding semantic segmentation annotations, aiming to enhance the\ndevelopment and testing of image-processing algorithms in the medical field.\n", "link": "http://arxiv.org/abs/2408.10827v1", "date": "2024-08-20", "relevancy": 1.6391, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4435}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.403}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CO2Wounds-V2%3A%20Extended%20Chronic%20Wounds%20Dataset%20From%20Leprosy%20Patients&body=Title%3A%20CO2Wounds-V2%3A%20Extended%20Chronic%20Wounds%20Dataset%20From%20Leprosy%20Patients%0AAuthor%3A%20Karen%20Sanchez%20and%20Carlos%20Hinojosa%20and%20Olinto%20Mieles%20and%20Chen%20Zhao%20and%20Bernard%20Ghanem%20and%20Henry%20Arguello%0AAbstract%3A%20%20%20Chronic%20wounds%20pose%20an%20ongoing%20health%20concern%20globally%2C%20largely%20due%20to%20the%0Aprevalence%20of%20conditions%20such%20as%20diabetes%20and%20leprosy%27s%20disease.%20The%20standard%0Amethod%20of%20monitoring%20these%20wounds%20involves%20visual%20inspection%20by%20healthcare%0Aprofessionals%2C%20a%20practice%20that%20could%20present%20challenges%20for%20patients%20in%20remote%0Aareas%20with%20inadequate%20transportation%20and%20healthcare%20infrastructure.%20This%20has%0Aled%20to%20the%20development%20of%20algorithms%20designed%20for%20the%20analysis%20and%20follow-up%20of%0Awound%20images%2C%20which%20perform%20image-processing%20tasks%20such%20as%20classification%2C%0Adetection%2C%20and%20segmentation.%20However%2C%20the%20effectiveness%20of%20these%20algorithms%0Aheavily%20depends%20on%20the%20availability%20of%20comprehensive%20and%20varied%20wound%20image%0Adata%2C%20which%20is%20usually%20scarce.%20This%20paper%20introduces%20the%20CO2Wounds-V2%20dataset%2C%0Aan%20extended%20collection%20of%20RGB%20wound%20images%20from%20leprosy%20patients%20with%20their%0Acorresponding%20semantic%20segmentation%20annotations%2C%20aiming%20to%20enhance%20the%0Adevelopment%20and%20testing%20of%20image-processing%20algorithms%20in%20the%20medical%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCO2Wounds-V2%253A%2520Extended%2520Chronic%2520Wounds%2520Dataset%2520From%2520Leprosy%2520Patients%26entry.906535625%3DKaren%2520Sanchez%2520and%2520Carlos%2520Hinojosa%2520and%2520Olinto%2520Mieles%2520and%2520Chen%2520Zhao%2520and%2520Bernard%2520Ghanem%2520and%2520Henry%2520Arguello%26entry.1292438233%3D%2520%2520Chronic%2520wounds%2520pose%2520an%2520ongoing%2520health%2520concern%2520globally%252C%2520largely%2520due%2520to%2520the%250Aprevalence%2520of%2520conditions%2520such%2520as%2520diabetes%2520and%2520leprosy%2527s%2520disease.%2520The%2520standard%250Amethod%2520of%2520monitoring%2520these%2520wounds%2520involves%2520visual%2520inspection%2520by%2520healthcare%250Aprofessionals%252C%2520a%2520practice%2520that%2520could%2520present%2520challenges%2520for%2520patients%2520in%2520remote%250Aareas%2520with%2520inadequate%2520transportation%2520and%2520healthcare%2520infrastructure.%2520This%2520has%250Aled%2520to%2520the%2520development%2520of%2520algorithms%2520designed%2520for%2520the%2520analysis%2520and%2520follow-up%2520of%250Awound%2520images%252C%2520which%2520perform%2520image-processing%2520tasks%2520such%2520as%2520classification%252C%250Adetection%252C%2520and%2520segmentation.%2520However%252C%2520the%2520effectiveness%2520of%2520these%2520algorithms%250Aheavily%2520depends%2520on%2520the%2520availability%2520of%2520comprehensive%2520and%2520varied%2520wound%2520image%250Adata%252C%2520which%2520is%2520usually%2520scarce.%2520This%2520paper%2520introduces%2520the%2520CO2Wounds-V2%2520dataset%252C%250Aan%2520extended%2520collection%2520of%2520RGB%2520wound%2520images%2520from%2520leprosy%2520patients%2520with%2520their%250Acorresponding%2520semantic%2520segmentation%2520annotations%252C%2520aiming%2520to%2520enhance%2520the%250Adevelopment%2520and%2520testing%2520of%2520image-processing%2520algorithms%2520in%2520the%2520medical%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CO2Wounds-V2%3A%20Extended%20Chronic%20Wounds%20Dataset%20From%20Leprosy%20Patients&entry.906535625=Karen%20Sanchez%20and%20Carlos%20Hinojosa%20and%20Olinto%20Mieles%20and%20Chen%20Zhao%20and%20Bernard%20Ghanem%20and%20Henry%20Arguello&entry.1292438233=%20%20Chronic%20wounds%20pose%20an%20ongoing%20health%20concern%20globally%2C%20largely%20due%20to%20the%0Aprevalence%20of%20conditions%20such%20as%20diabetes%20and%20leprosy%27s%20disease.%20The%20standard%0Amethod%20of%20monitoring%20these%20wounds%20involves%20visual%20inspection%20by%20healthcare%0Aprofessionals%2C%20a%20practice%20that%20could%20present%20challenges%20for%20patients%20in%20remote%0Aareas%20with%20inadequate%20transportation%20and%20healthcare%20infrastructure.%20This%20has%0Aled%20to%20the%20development%20of%20algorithms%20designed%20for%20the%20analysis%20and%20follow-up%20of%0Awound%20images%2C%20which%20perform%20image-processing%20tasks%20such%20as%20classification%2C%0Adetection%2C%20and%20segmentation.%20However%2C%20the%20effectiveness%20of%20these%20algorithms%0Aheavily%20depends%20on%20the%20availability%20of%20comprehensive%20and%20varied%20wound%20image%0Adata%2C%20which%20is%20usually%20scarce.%20This%20paper%20introduces%20the%20CO2Wounds-V2%20dataset%2C%0Aan%20extended%20collection%20of%20RGB%20wound%20images%20from%20leprosy%20patients%20with%20their%0Acorresponding%20semantic%20segmentation%20annotations%2C%20aiming%20to%20enhance%20the%0Adevelopment%20and%20testing%20of%20image-processing%20algorithms%20in%20the%20medical%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10827v1&entry.124074799=Read"},
{"title": "InstructRAG: Instructing Retrieval-Augmented Generation via\n  Self-Synthesized Rationales", "author": "Zhepei Wei and Wei-Lin Chen and Yu Meng", "abstract": "  Retrieval-augmented generation (RAG) has shown promising potential to enhance\nthe accuracy and factuality of language models (LMs). However, imperfect\nretrievers or noisy corpora can introduce misleading or even erroneous\ninformation to the retrieved contents, posing a significant challenge to the\ngeneration quality. Existing RAG methods typically address this challenge by\ndirectly predicting final answers despite potentially noisy inputs, resulting\nin an implicit denoising process that is difficult to interpret and verify. On\nthe other hand, the acquisition of explicit denoising supervision is often\ncostly, involving significant human efforts. In this work, we propose\nInstructRAG, where LMs explicitly learn the denoising process through\nself-synthesized rationales -- First, we instruct the LM to explain how the\nground-truth answer is derived from retrieved documents. Then, these rationales\ncan be used either as demonstrations for in-context learning of explicit\ndenoising or as supervised fine-tuning data to train the model. Compared to\nstandard RAG approaches, InstructRAG requires no additional supervision, allows\nfor easier verification of the predicted answers, and effectively improves\ngeneration accuracy. Experiments show InstructRAG consistently outperforms\nexisting RAG methods in both training-free and trainable scenarios, achieving a\nrelative improvement of 8.3% over the best baseline method on average across\nfive knowledge-intensive benchmarks. Extensive analysis indicates that\nInstructRAG scales well with increased numbers of retrieved documents and\nconsistently exhibits robust denoising ability even in out-of-domain datasets,\ndemonstrating strong generalizability.\n", "link": "http://arxiv.org/abs/2406.13629v2", "date": "2024-08-20", "relevancy": 1.4874, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5055}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4865}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstructRAG%3A%20Instructing%20Retrieval-Augmented%20Generation%20via%0A%20%20Self-Synthesized%20Rationales&body=Title%3A%20InstructRAG%3A%20Instructing%20Retrieval-Augmented%20Generation%20via%0A%20%20Self-Synthesized%20Rationales%0AAuthor%3A%20Zhepei%20Wei%20and%20Wei-Lin%20Chen%20and%20Yu%20Meng%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20has%20shown%20promising%20potential%20to%20enhance%0Athe%20accuracy%20and%20factuality%20of%20language%20models%20%28LMs%29.%20However%2C%20imperfect%0Aretrievers%20or%20noisy%20corpora%20can%20introduce%20misleading%20or%20even%20erroneous%0Ainformation%20to%20the%20retrieved%20contents%2C%20posing%20a%20significant%20challenge%20to%20the%0Ageneration%20quality.%20Existing%20RAG%20methods%20typically%20address%20this%20challenge%20by%0Adirectly%20predicting%20final%20answers%20despite%20potentially%20noisy%20inputs%2C%20resulting%0Ain%20an%20implicit%20denoising%20process%20that%20is%20difficult%20to%20interpret%20and%20verify.%20On%0Athe%20other%20hand%2C%20the%20acquisition%20of%20explicit%20denoising%20supervision%20is%20often%0Acostly%2C%20involving%20significant%20human%20efforts.%20In%20this%20work%2C%20we%20propose%0AInstructRAG%2C%20where%20LMs%20explicitly%20learn%20the%20denoising%20process%20through%0Aself-synthesized%20rationales%20--%20First%2C%20we%20instruct%20the%20LM%20to%20explain%20how%20the%0Aground-truth%20answer%20is%20derived%20from%20retrieved%20documents.%20Then%2C%20these%20rationales%0Acan%20be%20used%20either%20as%20demonstrations%20for%20in-context%20learning%20of%20explicit%0Adenoising%20or%20as%20supervised%20fine-tuning%20data%20to%20train%20the%20model.%20Compared%20to%0Astandard%20RAG%20approaches%2C%20InstructRAG%20requires%20no%20additional%20supervision%2C%20allows%0Afor%20easier%20verification%20of%20the%20predicted%20answers%2C%20and%20effectively%20improves%0Ageneration%20accuracy.%20Experiments%20show%20InstructRAG%20consistently%20outperforms%0Aexisting%20RAG%20methods%20in%20both%20training-free%20and%20trainable%20scenarios%2C%20achieving%20a%0Arelative%20improvement%20of%208.3%25%20over%20the%20best%20baseline%20method%20on%20average%20across%0Afive%20knowledge-intensive%20benchmarks.%20Extensive%20analysis%20indicates%20that%0AInstructRAG%20scales%20well%20with%20increased%20numbers%20of%20retrieved%20documents%20and%0Aconsistently%20exhibits%20robust%20denoising%20ability%20even%20in%20out-of-domain%20datasets%2C%0Ademonstrating%20strong%20generalizability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13629v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructRAG%253A%2520Instructing%2520Retrieval-Augmented%2520Generation%2520via%250A%2520%2520Self-Synthesized%2520Rationales%26entry.906535625%3DZhepei%2520Wei%2520and%2520Wei-Lin%2520Chen%2520and%2520Yu%2520Meng%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520has%2520shown%2520promising%2520potential%2520to%2520enhance%250Athe%2520accuracy%2520and%2520factuality%2520of%2520language%2520models%2520%2528LMs%2529.%2520However%252C%2520imperfect%250Aretrievers%2520or%2520noisy%2520corpora%2520can%2520introduce%2520misleading%2520or%2520even%2520erroneous%250Ainformation%2520to%2520the%2520retrieved%2520contents%252C%2520posing%2520a%2520significant%2520challenge%2520to%2520the%250Ageneration%2520quality.%2520Existing%2520RAG%2520methods%2520typically%2520address%2520this%2520challenge%2520by%250Adirectly%2520predicting%2520final%2520answers%2520despite%2520potentially%2520noisy%2520inputs%252C%2520resulting%250Ain%2520an%2520implicit%2520denoising%2520process%2520that%2520is%2520difficult%2520to%2520interpret%2520and%2520verify.%2520On%250Athe%2520other%2520hand%252C%2520the%2520acquisition%2520of%2520explicit%2520denoising%2520supervision%2520is%2520often%250Acostly%252C%2520involving%2520significant%2520human%2520efforts.%2520In%2520this%2520work%252C%2520we%2520propose%250AInstructRAG%252C%2520where%2520LMs%2520explicitly%2520learn%2520the%2520denoising%2520process%2520through%250Aself-synthesized%2520rationales%2520--%2520First%252C%2520we%2520instruct%2520the%2520LM%2520to%2520explain%2520how%2520the%250Aground-truth%2520answer%2520is%2520derived%2520from%2520retrieved%2520documents.%2520Then%252C%2520these%2520rationales%250Acan%2520be%2520used%2520either%2520as%2520demonstrations%2520for%2520in-context%2520learning%2520of%2520explicit%250Adenoising%2520or%2520as%2520supervised%2520fine-tuning%2520data%2520to%2520train%2520the%2520model.%2520Compared%2520to%250Astandard%2520RAG%2520approaches%252C%2520InstructRAG%2520requires%2520no%2520additional%2520supervision%252C%2520allows%250Afor%2520easier%2520verification%2520of%2520the%2520predicted%2520answers%252C%2520and%2520effectively%2520improves%250Ageneration%2520accuracy.%2520Experiments%2520show%2520InstructRAG%2520consistently%2520outperforms%250Aexisting%2520RAG%2520methods%2520in%2520both%2520training-free%2520and%2520trainable%2520scenarios%252C%2520achieving%2520a%250Arelative%2520improvement%2520of%25208.3%2525%2520over%2520the%2520best%2520baseline%2520method%2520on%2520average%2520across%250Afive%2520knowledge-intensive%2520benchmarks.%2520Extensive%2520analysis%2520indicates%2520that%250AInstructRAG%2520scales%2520well%2520with%2520increased%2520numbers%2520of%2520retrieved%2520documents%2520and%250Aconsistently%2520exhibits%2520robust%2520denoising%2520ability%2520even%2520in%2520out-of-domain%2520datasets%252C%250Ademonstrating%2520strong%2520generalizability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13629v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructRAG%3A%20Instructing%20Retrieval-Augmented%20Generation%20via%0A%20%20Self-Synthesized%20Rationales&entry.906535625=Zhepei%20Wei%20and%20Wei-Lin%20Chen%20and%20Yu%20Meng&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20has%20shown%20promising%20potential%20to%20enhance%0Athe%20accuracy%20and%20factuality%20of%20language%20models%20%28LMs%29.%20However%2C%20imperfect%0Aretrievers%20or%20noisy%20corpora%20can%20introduce%20misleading%20or%20even%20erroneous%0Ainformation%20to%20the%20retrieved%20contents%2C%20posing%20a%20significant%20challenge%20to%20the%0Ageneration%20quality.%20Existing%20RAG%20methods%20typically%20address%20this%20challenge%20by%0Adirectly%20predicting%20final%20answers%20despite%20potentially%20noisy%20inputs%2C%20resulting%0Ain%20an%20implicit%20denoising%20process%20that%20is%20difficult%20to%20interpret%20and%20verify.%20On%0Athe%20other%20hand%2C%20the%20acquisition%20of%20explicit%20denoising%20supervision%20is%20often%0Acostly%2C%20involving%20significant%20human%20efforts.%20In%20this%20work%2C%20we%20propose%0AInstructRAG%2C%20where%20LMs%20explicitly%20learn%20the%20denoising%20process%20through%0Aself-synthesized%20rationales%20--%20First%2C%20we%20instruct%20the%20LM%20to%20explain%20how%20the%0Aground-truth%20answer%20is%20derived%20from%20retrieved%20documents.%20Then%2C%20these%20rationales%0Acan%20be%20used%20either%20as%20demonstrations%20for%20in-context%20learning%20of%20explicit%0Adenoising%20or%20as%20supervised%20fine-tuning%20data%20to%20train%20the%20model.%20Compared%20to%0Astandard%20RAG%20approaches%2C%20InstructRAG%20requires%20no%20additional%20supervision%2C%20allows%0Afor%20easier%20verification%20of%20the%20predicted%20answers%2C%20and%20effectively%20improves%0Ageneration%20accuracy.%20Experiments%20show%20InstructRAG%20consistently%20outperforms%0Aexisting%20RAG%20methods%20in%20both%20training-free%20and%20trainable%20scenarios%2C%20achieving%20a%0Arelative%20improvement%20of%208.3%25%20over%20the%20best%20baseline%20method%20on%20average%20across%0Afive%20knowledge-intensive%20benchmarks.%20Extensive%20analysis%20indicates%20that%0AInstructRAG%20scales%20well%20with%20increased%20numbers%20of%20retrieved%20documents%20and%0Aconsistently%20exhibits%20robust%20denoising%20ability%20even%20in%20out-of-domain%20datasets%2C%0Ademonstrating%20strong%20generalizability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13629v2&entry.124074799=Read"},
{"title": "Variable Assignment Invariant Neural Networks for Learning Logic\n  Programs", "author": "Yin Jun Phua and Katsumi Inoue", "abstract": "  Learning from interpretation transition (LFIT) is a framework for learning\nrules from observed state transitions. LFIT has been implemented in purely\nsymbolic algorithms, but they are unable to deal with noise or generalize to\nunobserved transitions. Rule extraction based neural network methods suffer\nfrom overfitting, while more general implementation that categorize rules\nsuffer from combinatorial explosion. In this paper, we introduce a technique to\nleverage variable permutation invariance inherent in symbolic domains. Our\ntechnique ensures that the permutation and the naming of the variables would\nnot affect the results. We demonstrate the effectiveness and the scalability of\nthis method with various experiments. Our code is publicly available at\nhttps://github.com/phuayj/delta-lfit-2\n", "link": "http://arxiv.org/abs/2408.10709v1", "date": "2024-08-20", "relevancy": 1.7975, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4694}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4556}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variable%20Assignment%20Invariant%20Neural%20Networks%20for%20Learning%20Logic%0A%20%20Programs&body=Title%3A%20Variable%20Assignment%20Invariant%20Neural%20Networks%20for%20Learning%20Logic%0A%20%20Programs%0AAuthor%3A%20Yin%20Jun%20Phua%20and%20Katsumi%20Inoue%0AAbstract%3A%20%20%20Learning%20from%20interpretation%20transition%20%28LFIT%29%20is%20a%20framework%20for%20learning%0Arules%20from%20observed%20state%20transitions.%20LFIT%20has%20been%20implemented%20in%20purely%0Asymbolic%20algorithms%2C%20but%20they%20are%20unable%20to%20deal%20with%20noise%20or%20generalize%20to%0Aunobserved%20transitions.%20Rule%20extraction%20based%20neural%20network%20methods%20suffer%0Afrom%20overfitting%2C%20while%20more%20general%20implementation%20that%20categorize%20rules%0Asuffer%20from%20combinatorial%20explosion.%20In%20this%20paper%2C%20we%20introduce%20a%20technique%20to%0Aleverage%20variable%20permutation%20invariance%20inherent%20in%20symbolic%20domains.%20Our%0Atechnique%20ensures%20that%20the%20permutation%20and%20the%20naming%20of%20the%20variables%20would%0Anot%20affect%20the%20results.%20We%20demonstrate%20the%20effectiveness%20and%20the%20scalability%20of%0Athis%20method%20with%20various%20experiments.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/phuayj/delta-lfit-2%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariable%2520Assignment%2520Invariant%2520Neural%2520Networks%2520for%2520Learning%2520Logic%250A%2520%2520Programs%26entry.906535625%3DYin%2520Jun%2520Phua%2520and%2520Katsumi%2520Inoue%26entry.1292438233%3D%2520%2520Learning%2520from%2520interpretation%2520transition%2520%2528LFIT%2529%2520is%2520a%2520framework%2520for%2520learning%250Arules%2520from%2520observed%2520state%2520transitions.%2520LFIT%2520has%2520been%2520implemented%2520in%2520purely%250Asymbolic%2520algorithms%252C%2520but%2520they%2520are%2520unable%2520to%2520deal%2520with%2520noise%2520or%2520generalize%2520to%250Aunobserved%2520transitions.%2520Rule%2520extraction%2520based%2520neural%2520network%2520methods%2520suffer%250Afrom%2520overfitting%252C%2520while%2520more%2520general%2520implementation%2520that%2520categorize%2520rules%250Asuffer%2520from%2520combinatorial%2520explosion.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520technique%2520to%250Aleverage%2520variable%2520permutation%2520invariance%2520inherent%2520in%2520symbolic%2520domains.%2520Our%250Atechnique%2520ensures%2520that%2520the%2520permutation%2520and%2520the%2520naming%2520of%2520the%2520variables%2520would%250Anot%2520affect%2520the%2520results.%2520We%2520demonstrate%2520the%2520effectiveness%2520and%2520the%2520scalability%2520of%250Athis%2520method%2520with%2520various%2520experiments.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/phuayj/delta-lfit-2%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variable%20Assignment%20Invariant%20Neural%20Networks%20for%20Learning%20Logic%0A%20%20Programs&entry.906535625=Yin%20Jun%20Phua%20and%20Katsumi%20Inoue&entry.1292438233=%20%20Learning%20from%20interpretation%20transition%20%28LFIT%29%20is%20a%20framework%20for%20learning%0Arules%20from%20observed%20state%20transitions.%20LFIT%20has%20been%20implemented%20in%20purely%0Asymbolic%20algorithms%2C%20but%20they%20are%20unable%20to%20deal%20with%20noise%20or%20generalize%20to%0Aunobserved%20transitions.%20Rule%20extraction%20based%20neural%20network%20methods%20suffer%0Afrom%20overfitting%2C%20while%20more%20general%20implementation%20that%20categorize%20rules%0Asuffer%20from%20combinatorial%20explosion.%20In%20this%20paper%2C%20we%20introduce%20a%20technique%20to%0Aleverage%20variable%20permutation%20invariance%20inherent%20in%20symbolic%20domains.%20Our%0Atechnique%20ensures%20that%20the%20permutation%20and%20the%20naming%20of%20the%20variables%20would%0Anot%20affect%20the%20results.%20We%20demonstrate%20the%20effectiveness%20and%20the%20scalability%20of%0Athis%20method%20with%20various%20experiments.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/phuayj/delta-lfit-2%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10709v1&entry.124074799=Read"},
{"title": "LightMDETR: A Lightweight Approach for Low-Cost Open-Vocabulary Object\n  Detection Training", "author": "Binta Sow and Bilal Faye and Hanane Azzag and Mustapha Lebbah", "abstract": "  Object detection in computer vision traditionally involves identifying\nobjects in images. By integrating textual descriptions, we enhance this\nprocess, providing better context and accuracy. The MDETR model significantly\nadvances this by combining image and text data for more versatile object\ndetection and classification. However, MDETR's complexity and high\ncomputational demands hinder its practical use. In this paper, we introduce\nLightweight MDETR (LightMDETR), an optimized MDETR variant designed for\nimproved computational efficiency while maintaining robust multimodal\ncapabilities. Our approach involves freezing the MDETR backbone and training a\nsole component, the Deep Fusion Encoder (DFE), to represent image and text\nmodalities. A learnable context vector enables the DFE to switch between these\nmodalities. Evaluation on datasets like RefCOCO, RefCOCO+, and RefCOCOg\ndemonstrates that LightMDETR achieves superior precision and accuracy.\n", "link": "http://arxiv.org/abs/2408.10787v1", "date": "2024-08-20", "relevancy": 1.6888, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5727}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.562}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightMDETR%3A%20A%20Lightweight%20Approach%20for%20Low-Cost%20Open-Vocabulary%20Object%0A%20%20Detection%20Training&body=Title%3A%20LightMDETR%3A%20A%20Lightweight%20Approach%20for%20Low-Cost%20Open-Vocabulary%20Object%0A%20%20Detection%20Training%0AAuthor%3A%20Binta%20Sow%20and%20Bilal%20Faye%20and%20Hanane%20Azzag%20and%20Mustapha%20Lebbah%0AAbstract%3A%20%20%20Object%20detection%20in%20computer%20vision%20traditionally%20involves%20identifying%0Aobjects%20in%20images.%20By%20integrating%20textual%20descriptions%2C%20we%20enhance%20this%0Aprocess%2C%20providing%20better%20context%20and%20accuracy.%20The%20MDETR%20model%20significantly%0Aadvances%20this%20by%20combining%20image%20and%20text%20data%20for%20more%20versatile%20object%0Adetection%20and%20classification.%20However%2C%20MDETR%27s%20complexity%20and%20high%0Acomputational%20demands%20hinder%20its%20practical%20use.%20In%20this%20paper%2C%20we%20introduce%0ALightweight%20MDETR%20%28LightMDETR%29%2C%20an%20optimized%20MDETR%20variant%20designed%20for%0Aimproved%20computational%20efficiency%20while%20maintaining%20robust%20multimodal%0Acapabilities.%20Our%20approach%20involves%20freezing%20the%20MDETR%20backbone%20and%20training%20a%0Asole%20component%2C%20the%20Deep%20Fusion%20Encoder%20%28DFE%29%2C%20to%20represent%20image%20and%20text%0Amodalities.%20A%20learnable%20context%20vector%20enables%20the%20DFE%20to%20switch%20between%20these%0Amodalities.%20Evaluation%20on%20datasets%20like%20RefCOCO%2C%20RefCOCO%2B%2C%20and%20RefCOCOg%0Ademonstrates%20that%20LightMDETR%20achieves%20superior%20precision%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightMDETR%253A%2520A%2520Lightweight%2520Approach%2520for%2520Low-Cost%2520Open-Vocabulary%2520Object%250A%2520%2520Detection%2520Training%26entry.906535625%3DBinta%2520Sow%2520and%2520Bilal%2520Faye%2520and%2520Hanane%2520Azzag%2520and%2520Mustapha%2520Lebbah%26entry.1292438233%3D%2520%2520Object%2520detection%2520in%2520computer%2520vision%2520traditionally%2520involves%2520identifying%250Aobjects%2520in%2520images.%2520By%2520integrating%2520textual%2520descriptions%252C%2520we%2520enhance%2520this%250Aprocess%252C%2520providing%2520better%2520context%2520and%2520accuracy.%2520The%2520MDETR%2520model%2520significantly%250Aadvances%2520this%2520by%2520combining%2520image%2520and%2520text%2520data%2520for%2520more%2520versatile%2520object%250Adetection%2520and%2520classification.%2520However%252C%2520MDETR%2527s%2520complexity%2520and%2520high%250Acomputational%2520demands%2520hinder%2520its%2520practical%2520use.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ALightweight%2520MDETR%2520%2528LightMDETR%2529%252C%2520an%2520optimized%2520MDETR%2520variant%2520designed%2520for%250Aimproved%2520computational%2520efficiency%2520while%2520maintaining%2520robust%2520multimodal%250Acapabilities.%2520Our%2520approach%2520involves%2520freezing%2520the%2520MDETR%2520backbone%2520and%2520training%2520a%250Asole%2520component%252C%2520the%2520Deep%2520Fusion%2520Encoder%2520%2528DFE%2529%252C%2520to%2520represent%2520image%2520and%2520text%250Amodalities.%2520A%2520learnable%2520context%2520vector%2520enables%2520the%2520DFE%2520to%2520switch%2520between%2520these%250Amodalities.%2520Evaluation%2520on%2520datasets%2520like%2520RefCOCO%252C%2520RefCOCO%252B%252C%2520and%2520RefCOCOg%250Ademonstrates%2520that%2520LightMDETR%2520achieves%2520superior%2520precision%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightMDETR%3A%20A%20Lightweight%20Approach%20for%20Low-Cost%20Open-Vocabulary%20Object%0A%20%20Detection%20Training&entry.906535625=Binta%20Sow%20and%20Bilal%20Faye%20and%20Hanane%20Azzag%20and%20Mustapha%20Lebbah&entry.1292438233=%20%20Object%20detection%20in%20computer%20vision%20traditionally%20involves%20identifying%0Aobjects%20in%20images.%20By%20integrating%20textual%20descriptions%2C%20we%20enhance%20this%0Aprocess%2C%20providing%20better%20context%20and%20accuracy.%20The%20MDETR%20model%20significantly%0Aadvances%20this%20by%20combining%20image%20and%20text%20data%20for%20more%20versatile%20object%0Adetection%20and%20classification.%20However%2C%20MDETR%27s%20complexity%20and%20high%0Acomputational%20demands%20hinder%20its%20practical%20use.%20In%20this%20paper%2C%20we%20introduce%0ALightweight%20MDETR%20%28LightMDETR%29%2C%20an%20optimized%20MDETR%20variant%20designed%20for%0Aimproved%20computational%20efficiency%20while%20maintaining%20robust%20multimodal%0Acapabilities.%20Our%20approach%20involves%20freezing%20the%20MDETR%20backbone%20and%20training%20a%0Asole%20component%2C%20the%20Deep%20Fusion%20Encoder%20%28DFE%29%2C%20to%20represent%20image%20and%20text%0Amodalities.%20A%20learnable%20context%20vector%20enables%20the%20DFE%20to%20switch%20between%20these%0Amodalities.%20Evaluation%20on%20datasets%20like%20RefCOCO%2C%20RefCOCO%2B%2C%20and%20RefCOCOg%0Ademonstrates%20that%20LightMDETR%20achieves%20superior%20precision%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10787v1&entry.124074799=Read"},
{"title": "SR+Codec: a Benchmark of Super-Resolution for Video Compression Bitrate\n  Reduction", "author": "Evgeney Bogatyrev and Ivan Molodetskikh and Dmitriy Vatolin", "abstract": "  In recent years, there has been significant interest in Super-Resolution\n(SR), which focuses on generating a high-resolution image from a low-resolution\ninput. Deep learning-based methods for super-resolution have been particularly\npopular and have shown impressive results on various benchmarks. However,\nresearch indicates that these methods may not perform as well on strongly\ncompressed videos. We developed a super-resolution benchmark to analyze SR's\ncapacity to upscale compressed videos. Our dataset employed video codecs based\non five widely-used compression standards: H.264, H.265, H.266, AV1, and AVS3.\nWe assessed 19 popular SR models using our benchmark and evaluated their\nability to restore details and their susceptibility to compression artifacts.\nTo get an accurate perceptual ranking of SR models, we conducted a\ncrowd-sourced side-by-side comparison of their outputs. We found that some SR\nmodels, combined with compression, allow us to reduce the video bitrate without\nsignificant loss of quality. We also compared a range of image and video\nquality metrics with subjective scores to evaluate their accuracy on\nsuper-resolved compressed videos. The benchmark is publicly available at\nhttps://videoprocessing.ai/benchmarks/super-resolution-for-video-compression.html\n", "link": "http://arxiv.org/abs/2305.04844v2", "date": "2024-08-20", "relevancy": 1.4302, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4873}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.476}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SR%2BCodec%3A%20a%20Benchmark%20of%20Super-Resolution%20for%20Video%20Compression%20Bitrate%0A%20%20Reduction&body=Title%3A%20SR%2BCodec%3A%20a%20Benchmark%20of%20Super-Resolution%20for%20Video%20Compression%20Bitrate%0A%20%20Reduction%0AAuthor%3A%20Evgeney%20Bogatyrev%20and%20Ivan%20Molodetskikh%20and%20Dmitriy%20Vatolin%0AAbstract%3A%20%20%20In%20recent%20years%2C%20there%20has%20been%20significant%20interest%20in%20Super-Resolution%0A%28SR%29%2C%20which%20focuses%20on%20generating%20a%20high-resolution%20image%20from%20a%20low-resolution%0Ainput.%20Deep%20learning-based%20methods%20for%20super-resolution%20have%20been%20particularly%0Apopular%20and%20have%20shown%20impressive%20results%20on%20various%20benchmarks.%20However%2C%0Aresearch%20indicates%20that%20these%20methods%20may%20not%20perform%20as%20well%20on%20strongly%0Acompressed%20videos.%20We%20developed%20a%20super-resolution%20benchmark%20to%20analyze%20SR%27s%0Acapacity%20to%20upscale%20compressed%20videos.%20Our%20dataset%20employed%20video%20codecs%20based%0Aon%20five%20widely-used%20compression%20standards%3A%20H.264%2C%20H.265%2C%20H.266%2C%20AV1%2C%20and%20AVS3.%0AWe%20assessed%2019%20popular%20SR%20models%20using%20our%20benchmark%20and%20evaluated%20their%0Aability%20to%20restore%20details%20and%20their%20susceptibility%20to%20compression%20artifacts.%0ATo%20get%20an%20accurate%20perceptual%20ranking%20of%20SR%20models%2C%20we%20conducted%20a%0Acrowd-sourced%20side-by-side%20comparison%20of%20their%20outputs.%20We%20found%20that%20some%20SR%0Amodels%2C%20combined%20with%20compression%2C%20allow%20us%20to%20reduce%20the%20video%20bitrate%20without%0Asignificant%20loss%20of%20quality.%20We%20also%20compared%20a%20range%20of%20image%20and%20video%0Aquality%20metrics%20with%20subjective%20scores%20to%20evaluate%20their%20accuracy%20on%0Asuper-resolved%20compressed%20videos.%20The%20benchmark%20is%20publicly%20available%20at%0Ahttps%3A//videoprocessing.ai/benchmarks/super-resolution-for-video-compression.html%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.04844v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSR%252BCodec%253A%2520a%2520Benchmark%2520of%2520Super-Resolution%2520for%2520Video%2520Compression%2520Bitrate%250A%2520%2520Reduction%26entry.906535625%3DEvgeney%2520Bogatyrev%2520and%2520Ivan%2520Molodetskikh%2520and%2520Dmitriy%2520Vatolin%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520there%2520has%2520been%2520significant%2520interest%2520in%2520Super-Resolution%250A%2528SR%2529%252C%2520which%2520focuses%2520on%2520generating%2520a%2520high-resolution%2520image%2520from%2520a%2520low-resolution%250Ainput.%2520Deep%2520learning-based%2520methods%2520for%2520super-resolution%2520have%2520been%2520particularly%250Apopular%2520and%2520have%2520shown%2520impressive%2520results%2520on%2520various%2520benchmarks.%2520However%252C%250Aresearch%2520indicates%2520that%2520these%2520methods%2520may%2520not%2520perform%2520as%2520well%2520on%2520strongly%250Acompressed%2520videos.%2520We%2520developed%2520a%2520super-resolution%2520benchmark%2520to%2520analyze%2520SR%2527s%250Acapacity%2520to%2520upscale%2520compressed%2520videos.%2520Our%2520dataset%2520employed%2520video%2520codecs%2520based%250Aon%2520five%2520widely-used%2520compression%2520standards%253A%2520H.264%252C%2520H.265%252C%2520H.266%252C%2520AV1%252C%2520and%2520AVS3.%250AWe%2520assessed%252019%2520popular%2520SR%2520models%2520using%2520our%2520benchmark%2520and%2520evaluated%2520their%250Aability%2520to%2520restore%2520details%2520and%2520their%2520susceptibility%2520to%2520compression%2520artifacts.%250ATo%2520get%2520an%2520accurate%2520perceptual%2520ranking%2520of%2520SR%2520models%252C%2520we%2520conducted%2520a%250Acrowd-sourced%2520side-by-side%2520comparison%2520of%2520their%2520outputs.%2520We%2520found%2520that%2520some%2520SR%250Amodels%252C%2520combined%2520with%2520compression%252C%2520allow%2520us%2520to%2520reduce%2520the%2520video%2520bitrate%2520without%250Asignificant%2520loss%2520of%2520quality.%2520We%2520also%2520compared%2520a%2520range%2520of%2520image%2520and%2520video%250Aquality%2520metrics%2520with%2520subjective%2520scores%2520to%2520evaluate%2520their%2520accuracy%2520on%250Asuper-resolved%2520compressed%2520videos.%2520The%2520benchmark%2520is%2520publicly%2520available%2520at%250Ahttps%253A//videoprocessing.ai/benchmarks/super-resolution-for-video-compression.html%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.04844v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SR%2BCodec%3A%20a%20Benchmark%20of%20Super-Resolution%20for%20Video%20Compression%20Bitrate%0A%20%20Reduction&entry.906535625=Evgeney%20Bogatyrev%20and%20Ivan%20Molodetskikh%20and%20Dmitriy%20Vatolin&entry.1292438233=%20%20In%20recent%20years%2C%20there%20has%20been%20significant%20interest%20in%20Super-Resolution%0A%28SR%29%2C%20which%20focuses%20on%20generating%20a%20high-resolution%20image%20from%20a%20low-resolution%0Ainput.%20Deep%20learning-based%20methods%20for%20super-resolution%20have%20been%20particularly%0Apopular%20and%20have%20shown%20impressive%20results%20on%20various%20benchmarks.%20However%2C%0Aresearch%20indicates%20that%20these%20methods%20may%20not%20perform%20as%20well%20on%20strongly%0Acompressed%20videos.%20We%20developed%20a%20super-resolution%20benchmark%20to%20analyze%20SR%27s%0Acapacity%20to%20upscale%20compressed%20videos.%20Our%20dataset%20employed%20video%20codecs%20based%0Aon%20five%20widely-used%20compression%20standards%3A%20H.264%2C%20H.265%2C%20H.266%2C%20AV1%2C%20and%20AVS3.%0AWe%20assessed%2019%20popular%20SR%20models%20using%20our%20benchmark%20and%20evaluated%20their%0Aability%20to%20restore%20details%20and%20their%20susceptibility%20to%20compression%20artifacts.%0ATo%20get%20an%20accurate%20perceptual%20ranking%20of%20SR%20models%2C%20we%20conducted%20a%0Acrowd-sourced%20side-by-side%20comparison%20of%20their%20outputs.%20We%20found%20that%20some%20SR%0Amodels%2C%20combined%20with%20compression%2C%20allow%20us%20to%20reduce%20the%20video%20bitrate%20without%0Asignificant%20loss%20of%20quality.%20We%20also%20compared%20a%20range%20of%20image%20and%20video%0Aquality%20metrics%20with%20subjective%20scores%20to%20evaluate%20their%20accuracy%20on%0Asuper-resolved%20compressed%20videos.%20The%20benchmark%20is%20publicly%20available%20at%0Ahttps%3A//videoprocessing.ai/benchmarks/super-resolution-for-video-compression.html%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.04844v2&entry.124074799=Read"},
{"title": "Dr.Academy: A Benchmark for Evaluating Questioning Capability in\n  Education for Large Language Models", "author": "Yuyan Chen and Chenwei Wu and Songzhou Yan and Panjun Liu and Haoyu Zhou and Yanghua Xiao", "abstract": "  Teachers are important to imparting knowledge and guiding learners, and the\nrole of large language models (LLMs) as potential educators is emerging as an\nimportant area of study. Recognizing LLMs' capability to generate educational\ncontent can lead to advances in automated and personalized learning. While LLMs\nhave been tested for their comprehension and problem-solving skills, their\ncapability in teaching remains largely unexplored. In teaching, questioning is\na key skill that guides students to analyze, evaluate, and synthesize core\nconcepts and principles. Therefore, our research introduces a benchmark to\nevaluate the questioning capability in education as a teacher of LLMs through\nevaluating their generated educational questions, utilizing Anderson and\nKrathwohl's taxonomy across general, monodisciplinary, and interdisciplinary\ndomains. We shift the focus from LLMs as learners to LLMs as educators,\nassessing their teaching capability through guiding them to generate questions.\nWe apply four metrics, including relevance, coverage, representativeness, and\nconsistency, to evaluate the educational quality of LLMs' outputs. Our results\nindicate that GPT-4 demonstrates significant potential in teaching general,\nhumanities, and science courses; Claude2 appears more apt as an\ninterdisciplinary teacher. Furthermore, the automatic scores align with human\nperspectives.\n", "link": "http://arxiv.org/abs/2408.10947v1", "date": "2024-08-20", "relevancy": 1.3874, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5025}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.456}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dr.Academy%3A%20A%20Benchmark%20for%20Evaluating%20Questioning%20Capability%20in%0A%20%20Education%20for%20Large%20Language%20Models&body=Title%3A%20Dr.Academy%3A%20A%20Benchmark%20for%20Evaluating%20Questioning%20Capability%20in%0A%20%20Education%20for%20Large%20Language%20Models%0AAuthor%3A%20Yuyan%20Chen%20and%20Chenwei%20Wu%20and%20Songzhou%20Yan%20and%20Panjun%20Liu%20and%20Haoyu%20Zhou%20and%20Yanghua%20Xiao%0AAbstract%3A%20%20%20Teachers%20are%20important%20to%20imparting%20knowledge%20and%20guiding%20learners%2C%20and%20the%0Arole%20of%20large%20language%20models%20%28LLMs%29%20as%20potential%20educators%20is%20emerging%20as%20an%0Aimportant%20area%20of%20study.%20Recognizing%20LLMs%27%20capability%20to%20generate%20educational%0Acontent%20can%20lead%20to%20advances%20in%20automated%20and%20personalized%20learning.%20While%20LLMs%0Ahave%20been%20tested%20for%20their%20comprehension%20and%20problem-solving%20skills%2C%20their%0Acapability%20in%20teaching%20remains%20largely%20unexplored.%20In%20teaching%2C%20questioning%20is%0Aa%20key%20skill%20that%20guides%20students%20to%20analyze%2C%20evaluate%2C%20and%20synthesize%20core%0Aconcepts%20and%20principles.%20Therefore%2C%20our%20research%20introduces%20a%20benchmark%20to%0Aevaluate%20the%20questioning%20capability%20in%20education%20as%20a%20teacher%20of%20LLMs%20through%0Aevaluating%20their%20generated%20educational%20questions%2C%20utilizing%20Anderson%20and%0AKrathwohl%27s%20taxonomy%20across%20general%2C%20monodisciplinary%2C%20and%20interdisciplinary%0Adomains.%20We%20shift%20the%20focus%20from%20LLMs%20as%20learners%20to%20LLMs%20as%20educators%2C%0Aassessing%20their%20teaching%20capability%20through%20guiding%20them%20to%20generate%20questions.%0AWe%20apply%20four%20metrics%2C%20including%20relevance%2C%20coverage%2C%20representativeness%2C%20and%0Aconsistency%2C%20to%20evaluate%20the%20educational%20quality%20of%20LLMs%27%20outputs.%20Our%20results%0Aindicate%20that%20GPT-4%20demonstrates%20significant%20potential%20in%20teaching%20general%2C%0Ahumanities%2C%20and%20science%20courses%3B%20Claude2%20appears%20more%20apt%20as%20an%0Ainterdisciplinary%20teacher.%20Furthermore%2C%20the%20automatic%20scores%20align%20with%20human%0Aperspectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDr.Academy%253A%2520A%2520Benchmark%2520for%2520Evaluating%2520Questioning%2520Capability%2520in%250A%2520%2520Education%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DYuyan%2520Chen%2520and%2520Chenwei%2520Wu%2520and%2520Songzhou%2520Yan%2520and%2520Panjun%2520Liu%2520and%2520Haoyu%2520Zhou%2520and%2520Yanghua%2520Xiao%26entry.1292438233%3D%2520%2520Teachers%2520are%2520important%2520to%2520imparting%2520knowledge%2520and%2520guiding%2520learners%252C%2520and%2520the%250Arole%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%2520potential%2520educators%2520is%2520emerging%2520as%2520an%250Aimportant%2520area%2520of%2520study.%2520Recognizing%2520LLMs%2527%2520capability%2520to%2520generate%2520educational%250Acontent%2520can%2520lead%2520to%2520advances%2520in%2520automated%2520and%2520personalized%2520learning.%2520While%2520LLMs%250Ahave%2520been%2520tested%2520for%2520their%2520comprehension%2520and%2520problem-solving%2520skills%252C%2520their%250Acapability%2520in%2520teaching%2520remains%2520largely%2520unexplored.%2520In%2520teaching%252C%2520questioning%2520is%250Aa%2520key%2520skill%2520that%2520guides%2520students%2520to%2520analyze%252C%2520evaluate%252C%2520and%2520synthesize%2520core%250Aconcepts%2520and%2520principles.%2520Therefore%252C%2520our%2520research%2520introduces%2520a%2520benchmark%2520to%250Aevaluate%2520the%2520questioning%2520capability%2520in%2520education%2520as%2520a%2520teacher%2520of%2520LLMs%2520through%250Aevaluating%2520their%2520generated%2520educational%2520questions%252C%2520utilizing%2520Anderson%2520and%250AKrathwohl%2527s%2520taxonomy%2520across%2520general%252C%2520monodisciplinary%252C%2520and%2520interdisciplinary%250Adomains.%2520We%2520shift%2520the%2520focus%2520from%2520LLMs%2520as%2520learners%2520to%2520LLMs%2520as%2520educators%252C%250Aassessing%2520their%2520teaching%2520capability%2520through%2520guiding%2520them%2520to%2520generate%2520questions.%250AWe%2520apply%2520four%2520metrics%252C%2520including%2520relevance%252C%2520coverage%252C%2520representativeness%252C%2520and%250Aconsistency%252C%2520to%2520evaluate%2520the%2520educational%2520quality%2520of%2520LLMs%2527%2520outputs.%2520Our%2520results%250Aindicate%2520that%2520GPT-4%2520demonstrates%2520significant%2520potential%2520in%2520teaching%2520general%252C%250Ahumanities%252C%2520and%2520science%2520courses%253B%2520Claude2%2520appears%2520more%2520apt%2520as%2520an%250Ainterdisciplinary%2520teacher.%2520Furthermore%252C%2520the%2520automatic%2520scores%2520align%2520with%2520human%250Aperspectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dr.Academy%3A%20A%20Benchmark%20for%20Evaluating%20Questioning%20Capability%20in%0A%20%20Education%20for%20Large%20Language%20Models&entry.906535625=Yuyan%20Chen%20and%20Chenwei%20Wu%20and%20Songzhou%20Yan%20and%20Panjun%20Liu%20and%20Haoyu%20Zhou%20and%20Yanghua%20Xiao&entry.1292438233=%20%20Teachers%20are%20important%20to%20imparting%20knowledge%20and%20guiding%20learners%2C%20and%20the%0Arole%20of%20large%20language%20models%20%28LLMs%29%20as%20potential%20educators%20is%20emerging%20as%20an%0Aimportant%20area%20of%20study.%20Recognizing%20LLMs%27%20capability%20to%20generate%20educational%0Acontent%20can%20lead%20to%20advances%20in%20automated%20and%20personalized%20learning.%20While%20LLMs%0Ahave%20been%20tested%20for%20their%20comprehension%20and%20problem-solving%20skills%2C%20their%0Acapability%20in%20teaching%20remains%20largely%20unexplored.%20In%20teaching%2C%20questioning%20is%0Aa%20key%20skill%20that%20guides%20students%20to%20analyze%2C%20evaluate%2C%20and%20synthesize%20core%0Aconcepts%20and%20principles.%20Therefore%2C%20our%20research%20introduces%20a%20benchmark%20to%0Aevaluate%20the%20questioning%20capability%20in%20education%20as%20a%20teacher%20of%20LLMs%20through%0Aevaluating%20their%20generated%20educational%20questions%2C%20utilizing%20Anderson%20and%0AKrathwohl%27s%20taxonomy%20across%20general%2C%20monodisciplinary%2C%20and%20interdisciplinary%0Adomains.%20We%20shift%20the%20focus%20from%20LLMs%20as%20learners%20to%20LLMs%20as%20educators%2C%0Aassessing%20their%20teaching%20capability%20through%20guiding%20them%20to%20generate%20questions.%0AWe%20apply%20four%20metrics%2C%20including%20relevance%2C%20coverage%2C%20representativeness%2C%20and%0Aconsistency%2C%20to%20evaluate%20the%20educational%20quality%20of%20LLMs%27%20outputs.%20Our%20results%0Aindicate%20that%20GPT-4%20demonstrates%20significant%20potential%20in%20teaching%20general%2C%0Ahumanities%2C%20and%20science%20courses%3B%20Claude2%20appears%20more%20apt%20as%20an%0Ainterdisciplinary%20teacher.%20Furthermore%2C%20the%20automatic%20scores%20align%20with%20human%0Aperspectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10947v1&entry.124074799=Read"},
{"title": "On Learning Action Costs from Input Plans", "author": "Marianela Morales and Alberto Pozanco and Giuseppe Canonaco and Sriram Gopalakrishnan and Daniel Borrajo and Manuela Veloso", "abstract": "  Most of the work on learning action models focus on learning the actions'\ndynamics from input plans. This allows us to specify the valid plans of a\nplanning task. However, very little work focuses on learning action costs,\nwhich in turn allows us to rank the different plans. In this paper we introduce\na new problem: that of learning the costs of a set of actions such that a set\nof input plans are optimal under the resulting planning model. To solve this\nproblem we present $LACFIP^k$, an algorithm to learn action's costs from\nunlabeled input plans. We provide theoretical and empirical results showing how\n$LACFIP^k$ can successfully solve this task.\n", "link": "http://arxiv.org/abs/2408.10889v1", "date": "2024-08-20", "relevancy": 1.3706, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4702}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4596}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Learning%20Action%20Costs%20from%20Input%20Plans&body=Title%3A%20On%20Learning%20Action%20Costs%20from%20Input%20Plans%0AAuthor%3A%20Marianela%20Morales%20and%20Alberto%20Pozanco%20and%20Giuseppe%20Canonaco%20and%20Sriram%20Gopalakrishnan%20and%20Daniel%20Borrajo%20and%20Manuela%20Veloso%0AAbstract%3A%20%20%20Most%20of%20the%20work%20on%20learning%20action%20models%20focus%20on%20learning%20the%20actions%27%0Adynamics%20from%20input%20plans.%20This%20allows%20us%20to%20specify%20the%20valid%20plans%20of%20a%0Aplanning%20task.%20However%2C%20very%20little%20work%20focuses%20on%20learning%20action%20costs%2C%0Awhich%20in%20turn%20allows%20us%20to%20rank%20the%20different%20plans.%20In%20this%20paper%20we%20introduce%0Aa%20new%20problem%3A%20that%20of%20learning%20the%20costs%20of%20a%20set%20of%20actions%20such%20that%20a%20set%0Aof%20input%20plans%20are%20optimal%20under%20the%20resulting%20planning%20model.%20To%20solve%20this%0Aproblem%20we%20present%20%24LACFIP%5Ek%24%2C%20an%20algorithm%20to%20learn%20action%27s%20costs%20from%0Aunlabeled%20input%20plans.%20We%20provide%20theoretical%20and%20empirical%20results%20showing%20how%0A%24LACFIP%5Ek%24%20can%20successfully%20solve%20this%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Learning%2520Action%2520Costs%2520from%2520Input%2520Plans%26entry.906535625%3DMarianela%2520Morales%2520and%2520Alberto%2520Pozanco%2520and%2520Giuseppe%2520Canonaco%2520and%2520Sriram%2520Gopalakrishnan%2520and%2520Daniel%2520Borrajo%2520and%2520Manuela%2520Veloso%26entry.1292438233%3D%2520%2520Most%2520of%2520the%2520work%2520on%2520learning%2520action%2520models%2520focus%2520on%2520learning%2520the%2520actions%2527%250Adynamics%2520from%2520input%2520plans.%2520This%2520allows%2520us%2520to%2520specify%2520the%2520valid%2520plans%2520of%2520a%250Aplanning%2520task.%2520However%252C%2520very%2520little%2520work%2520focuses%2520on%2520learning%2520action%2520costs%252C%250Awhich%2520in%2520turn%2520allows%2520us%2520to%2520rank%2520the%2520different%2520plans.%2520In%2520this%2520paper%2520we%2520introduce%250Aa%2520new%2520problem%253A%2520that%2520of%2520learning%2520the%2520costs%2520of%2520a%2520set%2520of%2520actions%2520such%2520that%2520a%2520set%250Aof%2520input%2520plans%2520are%2520optimal%2520under%2520the%2520resulting%2520planning%2520model.%2520To%2520solve%2520this%250Aproblem%2520we%2520present%2520%2524LACFIP%255Ek%2524%252C%2520an%2520algorithm%2520to%2520learn%2520action%2527s%2520costs%2520from%250Aunlabeled%2520input%2520plans.%2520We%2520provide%2520theoretical%2520and%2520empirical%2520results%2520showing%2520how%250A%2524LACFIP%255Ek%2524%2520can%2520successfully%2520solve%2520this%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Learning%20Action%20Costs%20from%20Input%20Plans&entry.906535625=Marianela%20Morales%20and%20Alberto%20Pozanco%20and%20Giuseppe%20Canonaco%20and%20Sriram%20Gopalakrishnan%20and%20Daniel%20Borrajo%20and%20Manuela%20Veloso&entry.1292438233=%20%20Most%20of%20the%20work%20on%20learning%20action%20models%20focus%20on%20learning%20the%20actions%27%0Adynamics%20from%20input%20plans.%20This%20allows%20us%20to%20specify%20the%20valid%20plans%20of%20a%0Aplanning%20task.%20However%2C%20very%20little%20work%20focuses%20on%20learning%20action%20costs%2C%0Awhich%20in%20turn%20allows%20us%20to%20rank%20the%20different%20plans.%20In%20this%20paper%20we%20introduce%0Aa%20new%20problem%3A%20that%20of%20learning%20the%20costs%20of%20a%20set%20of%20actions%20such%20that%20a%20set%0Aof%20input%20plans%20are%20optimal%20under%20the%20resulting%20planning%20model.%20To%20solve%20this%0Aproblem%20we%20present%20%24LACFIP%5Ek%24%2C%20an%20algorithm%20to%20learn%20action%27s%20costs%20from%0Aunlabeled%20input%20plans.%20We%20provide%20theoretical%20and%20empirical%20results%20showing%20how%0A%24LACFIP%5Ek%24%20can%20successfully%20solve%20this%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10889v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


