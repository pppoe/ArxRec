<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251124.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes", "author": "Kehua Chen and Tianlu Mao and Zhuxin Ma and Hao Jiang and Zehao Li and Zihan Liu and Shuqi Gao and Honglong Zhao and Feng Dai and Yucheng Zhang and Zhaoqi Wang", "abstract": "Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction. However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge. To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments. Our method is built upon a distributed 2D Gaussian Splatting representation as the core foundation, serving as a unified backbone for subsequent modules. To handle potential sparse regions in complex scenes, we propose a structured dense enhancement scheme that utilizes SfM priors and a pointmap model to achieve a denser initialization, while incorporating a sparsity compensation mechanism to improve reconstruction completeness. Furthermore, we design a progressive hybrid geometric optimization strategy that organically integrates monocular and multi-view optimization to achieve efficient and accurate geometric refinement. Finally, to address the appearance inconsistency commonly observed in large-scale scenes, we introduce a depth-guided appearance modeling approach that learns spatial features with 3D consistency, facilitating effective decoupling between geometry and appearance and further enhancing reconstruction stability. Experiments on large-scale urban datasets demonstrate that MetroGS achieves superior geometric accuracy, rendering quality, offering a unified solution for high-fidelity large-scale scene reconstruction.", "link": "http://arxiv.org/abs/2511.19172v1", "date": "2025-11-24", "relevancy": 3.4934, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7232}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.693}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetroGS%3A%20Efficient%20and%20Stable%20Reconstruction%20of%20Geometrically%20Accurate%20High-Fidelity%20Large-Scale%20Scenes&body=Title%3A%20MetroGS%3A%20Efficient%20and%20Stable%20Reconstruction%20of%20Geometrically%20Accurate%20High-Fidelity%20Large-Scale%20Scenes%0AAuthor%3A%20Kehua%20Chen%20and%20Tianlu%20Mao%20and%20Zhuxin%20Ma%20and%20Hao%20Jiang%20and%20Zehao%20Li%20and%20Zihan%20Liu%20and%20Shuqi%20Gao%20and%20Honglong%20Zhao%20and%20Feng%20Dai%20and%20Yucheng%20Zhang%20and%20Zhaoqi%20Wang%0AAbstract%3A%20Recently%2C%203D%20Gaussian%20Splatting%20and%20its%20derivatives%20have%20achieved%20significant%20breakthroughs%20in%20large-scale%20scene%20reconstruction.%20However%2C%20how%20to%20efficiently%20and%20stably%20achieve%20high-quality%20geometric%20fidelity%20remains%20a%20core%20challenge.%20To%20address%20this%20issue%2C%20we%20introduce%20MetroGS%2C%20a%20novel%20Gaussian%20Splatting%20framework%20for%20efficient%20and%20robust%20reconstruction%20in%20complex%20urban%20environments.%20Our%20method%20is%20built%20upon%20a%20distributed%202D%20Gaussian%20Splatting%20representation%20as%20the%20core%20foundation%2C%20serving%20as%20a%20unified%20backbone%20for%20subsequent%20modules.%20To%20handle%20potential%20sparse%20regions%20in%20complex%20scenes%2C%20we%20propose%20a%20structured%20dense%20enhancement%20scheme%20that%20utilizes%20SfM%20priors%20and%20a%20pointmap%20model%20to%20achieve%20a%20denser%20initialization%2C%20while%20incorporating%20a%20sparsity%20compensation%20mechanism%20to%20improve%20reconstruction%20completeness.%20Furthermore%2C%20we%20design%20a%20progressive%20hybrid%20geometric%20optimization%20strategy%20that%20organically%20integrates%20monocular%20and%20multi-view%20optimization%20to%20achieve%20efficient%20and%20accurate%20geometric%20refinement.%20Finally%2C%20to%20address%20the%20appearance%20inconsistency%20commonly%20observed%20in%20large-scale%20scenes%2C%20we%20introduce%20a%20depth-guided%20appearance%20modeling%20approach%20that%20learns%20spatial%20features%20with%203D%20consistency%2C%20facilitating%20effective%20decoupling%20between%20geometry%20and%20appearance%20and%20further%20enhancing%20reconstruction%20stability.%20Experiments%20on%20large-scale%20urban%20datasets%20demonstrate%20that%20MetroGS%20achieves%20superior%20geometric%20accuracy%2C%20rendering%20quality%2C%20offering%20a%20unified%20solution%20for%20high-fidelity%20large-scale%20scene%20reconstruction.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetroGS%253A%2520Efficient%2520and%2520Stable%2520Reconstruction%2520of%2520Geometrically%2520Accurate%2520High-Fidelity%2520Large-Scale%2520Scenes%26entry.906535625%3DKehua%2520Chen%2520and%2520Tianlu%2520Mao%2520and%2520Zhuxin%2520Ma%2520and%2520Hao%2520Jiang%2520and%2520Zehao%2520Li%2520and%2520Zihan%2520Liu%2520and%2520Shuqi%2520Gao%2520and%2520Honglong%2520Zhao%2520and%2520Feng%2520Dai%2520and%2520Yucheng%2520Zhang%2520and%2520Zhaoqi%2520Wang%26entry.1292438233%3DRecently%252C%25203D%2520Gaussian%2520Splatting%2520and%2520its%2520derivatives%2520have%2520achieved%2520significant%2520breakthroughs%2520in%2520large-scale%2520scene%2520reconstruction.%2520However%252C%2520how%2520to%2520efficiently%2520and%2520stably%2520achieve%2520high-quality%2520geometric%2520fidelity%2520remains%2520a%2520core%2520challenge.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520MetroGS%252C%2520a%2520novel%2520Gaussian%2520Splatting%2520framework%2520for%2520efficient%2520and%2520robust%2520reconstruction%2520in%2520complex%2520urban%2520environments.%2520Our%2520method%2520is%2520built%2520upon%2520a%2520distributed%25202D%2520Gaussian%2520Splatting%2520representation%2520as%2520the%2520core%2520foundation%252C%2520serving%2520as%2520a%2520unified%2520backbone%2520for%2520subsequent%2520modules.%2520To%2520handle%2520potential%2520sparse%2520regions%2520in%2520complex%2520scenes%252C%2520we%2520propose%2520a%2520structured%2520dense%2520enhancement%2520scheme%2520that%2520utilizes%2520SfM%2520priors%2520and%2520a%2520pointmap%2520model%2520to%2520achieve%2520a%2520denser%2520initialization%252C%2520while%2520incorporating%2520a%2520sparsity%2520compensation%2520mechanism%2520to%2520improve%2520reconstruction%2520completeness.%2520Furthermore%252C%2520we%2520design%2520a%2520progressive%2520hybrid%2520geometric%2520optimization%2520strategy%2520that%2520organically%2520integrates%2520monocular%2520and%2520multi-view%2520optimization%2520to%2520achieve%2520efficient%2520and%2520accurate%2520geometric%2520refinement.%2520Finally%252C%2520to%2520address%2520the%2520appearance%2520inconsistency%2520commonly%2520observed%2520in%2520large-scale%2520scenes%252C%2520we%2520introduce%2520a%2520depth-guided%2520appearance%2520modeling%2520approach%2520that%2520learns%2520spatial%2520features%2520with%25203D%2520consistency%252C%2520facilitating%2520effective%2520decoupling%2520between%2520geometry%2520and%2520appearance%2520and%2520further%2520enhancing%2520reconstruction%2520stability.%2520Experiments%2520on%2520large-scale%2520urban%2520datasets%2520demonstrate%2520that%2520MetroGS%2520achieves%2520superior%2520geometric%2520accuracy%252C%2520rendering%2520quality%252C%2520offering%2520a%2520unified%2520solution%2520for%2520high-fidelity%2520large-scale%2520scene%2520reconstruction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetroGS%3A%20Efficient%20and%20Stable%20Reconstruction%20of%20Geometrically%20Accurate%20High-Fidelity%20Large-Scale%20Scenes&entry.906535625=Kehua%20Chen%20and%20Tianlu%20Mao%20and%20Zhuxin%20Ma%20and%20Hao%20Jiang%20and%20Zehao%20Li%20and%20Zihan%20Liu%20and%20Shuqi%20Gao%20and%20Honglong%20Zhao%20and%20Feng%20Dai%20and%20Yucheng%20Zhang%20and%20Zhaoqi%20Wang&entry.1292438233=Recently%2C%203D%20Gaussian%20Splatting%20and%20its%20derivatives%20have%20achieved%20significant%20breakthroughs%20in%20large-scale%20scene%20reconstruction.%20However%2C%20how%20to%20efficiently%20and%20stably%20achieve%20high-quality%20geometric%20fidelity%20remains%20a%20core%20challenge.%20To%20address%20this%20issue%2C%20we%20introduce%20MetroGS%2C%20a%20novel%20Gaussian%20Splatting%20framework%20for%20efficient%20and%20robust%20reconstruction%20in%20complex%20urban%20environments.%20Our%20method%20is%20built%20upon%20a%20distributed%202D%20Gaussian%20Splatting%20representation%20as%20the%20core%20foundation%2C%20serving%20as%20a%20unified%20backbone%20for%20subsequent%20modules.%20To%20handle%20potential%20sparse%20regions%20in%20complex%20scenes%2C%20we%20propose%20a%20structured%20dense%20enhancement%20scheme%20that%20utilizes%20SfM%20priors%20and%20a%20pointmap%20model%20to%20achieve%20a%20denser%20initialization%2C%20while%20incorporating%20a%20sparsity%20compensation%20mechanism%20to%20improve%20reconstruction%20completeness.%20Furthermore%2C%20we%20design%20a%20progressive%20hybrid%20geometric%20optimization%20strategy%20that%20organically%20integrates%20monocular%20and%20multi-view%20optimization%20to%20achieve%20efficient%20and%20accurate%20geometric%20refinement.%20Finally%2C%20to%20address%20the%20appearance%20inconsistency%20commonly%20observed%20in%20large-scale%20scenes%2C%20we%20introduce%20a%20depth-guided%20appearance%20modeling%20approach%20that%20learns%20spatial%20features%20with%203D%20consistency%2C%20facilitating%20effective%20decoupling%20between%20geometry%20and%20appearance%20and%20further%20enhancing%20reconstruction%20stability.%20Experiments%20on%20large-scale%20urban%20datasets%20demonstrate%20that%20MetroGS%20achieves%20superior%20geometric%20accuracy%2C%20rendering%20quality%2C%20offering%20a%20unified%20solution%20for%20high-fidelity%20large-scale%20scene%20reconstruction.&entry.1838667208=http%3A//arxiv.org/abs/2511.19172v1&entry.124074799=Read"},
{"title": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation", "author": "Duolikun Danier and Ge Gao and Steven McDonagh and Changjian Li and Hakan Bilen and Oisin Mac Aodha", "abstract": "Video generation models have made significant progress in generating realistic content, enabling applications in simulation, gaming, and film making. However, current generated videos still contain visual artifacts arising from 3D inconsistencies, e.g., objects and structures deforming under changes in camera pose, which can undermine user experience and simulation fidelity. Motivated by recent findings on representation alignment for diffusion models, we hypothesize that improving the multi-view consistency of video diffusion representations will yield more 3D-consistent video generation. Through detailed analysis on multiple recent camera-controlled video diffusion models we reveal strong correlations between 3D-consistent representations and videos. We also propose ViCoDR, a new approach for improving the 3D consistency of video models by learning multi-view consistent diffusion representations. We evaluate ViCoDR on camera controlled image-to-video, text-to-video, and multi-view generation models, demonstrating significant improvements in the 3D consistency of the generated videos. Project page: https://danier97.github.io/ViCoDR.", "link": "http://arxiv.org/abs/2511.18991v1", "date": "2025-11-24", "relevancy": 3.3755, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6802}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6802}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20View-Consistent%20Diffusion%20Representations%20for%203D-Consistent%20Video%20Generation&body=Title%3A%20View-Consistent%20Diffusion%20Representations%20for%203D-Consistent%20Video%20Generation%0AAuthor%3A%20Duolikun%20Danier%20and%20Ge%20Gao%20and%20Steven%20McDonagh%20and%20Changjian%20Li%20and%20Hakan%20Bilen%20and%20Oisin%20Mac%20Aodha%0AAbstract%3A%20Video%20generation%20models%20have%20made%20significant%20progress%20in%20generating%20realistic%20content%2C%20enabling%20applications%20in%20simulation%2C%20gaming%2C%20and%20film%20making.%20However%2C%20current%20generated%20videos%20still%20contain%20visual%20artifacts%20arising%20from%203D%20inconsistencies%2C%20e.g.%2C%20objects%20and%20structures%20deforming%20under%20changes%20in%20camera%20pose%2C%20which%20can%20undermine%20user%20experience%20and%20simulation%20fidelity.%20Motivated%20by%20recent%20findings%20on%20representation%20alignment%20for%20diffusion%20models%2C%20we%20hypothesize%20that%20improving%20the%20multi-view%20consistency%20of%20video%20diffusion%20representations%20will%20yield%20more%203D-consistent%20video%20generation.%20Through%20detailed%20analysis%20on%20multiple%20recent%20camera-controlled%20video%20diffusion%20models%20we%20reveal%20strong%20correlations%20between%203D-consistent%20representations%20and%20videos.%20We%20also%20propose%20ViCoDR%2C%20a%20new%20approach%20for%20improving%20the%203D%20consistency%20of%20video%20models%20by%20learning%20multi-view%20consistent%20diffusion%20representations.%20We%20evaluate%20ViCoDR%20on%20camera%20controlled%20image-to-video%2C%20text-to-video%2C%20and%20multi-view%20generation%20models%2C%20demonstrating%20significant%20improvements%20in%20the%203D%20consistency%20of%20the%20generated%20videos.%20Project%20page%3A%20https%3A//danier97.github.io/ViCoDR.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DView-Consistent%2520Diffusion%2520Representations%2520for%25203D-Consistent%2520Video%2520Generation%26entry.906535625%3DDuolikun%2520Danier%2520and%2520Ge%2520Gao%2520and%2520Steven%2520McDonagh%2520and%2520Changjian%2520Li%2520and%2520Hakan%2520Bilen%2520and%2520Oisin%2520Mac%2520Aodha%26entry.1292438233%3DVideo%2520generation%2520models%2520have%2520made%2520significant%2520progress%2520in%2520generating%2520realistic%2520content%252C%2520enabling%2520applications%2520in%2520simulation%252C%2520gaming%252C%2520and%2520film%2520making.%2520However%252C%2520current%2520generated%2520videos%2520still%2520contain%2520visual%2520artifacts%2520arising%2520from%25203D%2520inconsistencies%252C%2520e.g.%252C%2520objects%2520and%2520structures%2520deforming%2520under%2520changes%2520in%2520camera%2520pose%252C%2520which%2520can%2520undermine%2520user%2520experience%2520and%2520simulation%2520fidelity.%2520Motivated%2520by%2520recent%2520findings%2520on%2520representation%2520alignment%2520for%2520diffusion%2520models%252C%2520we%2520hypothesize%2520that%2520improving%2520the%2520multi-view%2520consistency%2520of%2520video%2520diffusion%2520representations%2520will%2520yield%2520more%25203D-consistent%2520video%2520generation.%2520Through%2520detailed%2520analysis%2520on%2520multiple%2520recent%2520camera-controlled%2520video%2520diffusion%2520models%2520we%2520reveal%2520strong%2520correlations%2520between%25203D-consistent%2520representations%2520and%2520videos.%2520We%2520also%2520propose%2520ViCoDR%252C%2520a%2520new%2520approach%2520for%2520improving%2520the%25203D%2520consistency%2520of%2520video%2520models%2520by%2520learning%2520multi-view%2520consistent%2520diffusion%2520representations.%2520We%2520evaluate%2520ViCoDR%2520on%2520camera%2520controlled%2520image-to-video%252C%2520text-to-video%252C%2520and%2520multi-view%2520generation%2520models%252C%2520demonstrating%2520significant%2520improvements%2520in%2520the%25203D%2520consistency%2520of%2520the%2520generated%2520videos.%2520Project%2520page%253A%2520https%253A//danier97.github.io/ViCoDR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=View-Consistent%20Diffusion%20Representations%20for%203D-Consistent%20Video%20Generation&entry.906535625=Duolikun%20Danier%20and%20Ge%20Gao%20and%20Steven%20McDonagh%20and%20Changjian%20Li%20and%20Hakan%20Bilen%20and%20Oisin%20Mac%20Aodha&entry.1292438233=Video%20generation%20models%20have%20made%20significant%20progress%20in%20generating%20realistic%20content%2C%20enabling%20applications%20in%20simulation%2C%20gaming%2C%20and%20film%20making.%20However%2C%20current%20generated%20videos%20still%20contain%20visual%20artifacts%20arising%20from%203D%20inconsistencies%2C%20e.g.%2C%20objects%20and%20structures%20deforming%20under%20changes%20in%20camera%20pose%2C%20which%20can%20undermine%20user%20experience%20and%20simulation%20fidelity.%20Motivated%20by%20recent%20findings%20on%20representation%20alignment%20for%20diffusion%20models%2C%20we%20hypothesize%20that%20improving%20the%20multi-view%20consistency%20of%20video%20diffusion%20representations%20will%20yield%20more%203D-consistent%20video%20generation.%20Through%20detailed%20analysis%20on%20multiple%20recent%20camera-controlled%20video%20diffusion%20models%20we%20reveal%20strong%20correlations%20between%203D-consistent%20representations%20and%20videos.%20We%20also%20propose%20ViCoDR%2C%20a%20new%20approach%20for%20improving%20the%203D%20consistency%20of%20video%20models%20by%20learning%20multi-view%20consistent%20diffusion%20representations.%20We%20evaluate%20ViCoDR%20on%20camera%20controlled%20image-to-video%2C%20text-to-video%2C%20and%20multi-view%20generation%20models%2C%20demonstrating%20significant%20improvements%20in%20the%203D%20consistency%20of%20the%20generated%20videos.%20Project%20page%3A%20https%3A//danier97.github.io/ViCoDR.&entry.1838667208=http%3A//arxiv.org/abs/2511.18991v1&entry.124074799=Read"},
{"title": "IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes", "author": "Carl Lindstr\u00f6m and Mahan Rafidashti and Maryam Fatemi and Lars Hammarstrand and Martin R. Oswald and Lennart Svensson", "abstract": "Reconstructing dynamic driving scenes is essential for developing autonomous systems through sensor-realistic simulation. Although recent methods achieve high-fidelity reconstructions, they either rely on costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation. We present IDSplat, a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic scenes with explicit instance decomposition and learnable motion trajectories, without requiring human annotations. Our key insight is to model dynamic objects as coherent instances undergoing rigid transformations, rather than unstructured time-varying primitives. For instance decomposition, we employ zero-shot, language-grounded video tracking anchored to 3D using lidar, and estimate consistent poses via feature correspondences. We introduce a coordinated-turn smoothing scheme to obtain temporally and physically consistent motion trajectories, mitigating pose misalignments and tracking failures, followed by joint optimization of object poses and Gaussian parameters. Experiments on the Waymo Open Dataset demonstrate that our method achieves competitive reconstruction quality while maintaining instance-level decomposition and generalizes across diverse sequences and view densities without retraining, making it practical for large-scale autonomous driving applications. Code will be released.", "link": "http://arxiv.org/abs/2511.19235v1", "date": "2025-11-24", "relevancy": 3.3513, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6839}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6715}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IDSplat%3A%20Instance-Decomposed%203D%20Gaussian%20Splatting%20for%20Driving%20Scenes&body=Title%3A%20IDSplat%3A%20Instance-Decomposed%203D%20Gaussian%20Splatting%20for%20Driving%20Scenes%0AAuthor%3A%20Carl%20Lindstr%C3%B6m%20and%20Mahan%20Rafidashti%20and%20Maryam%20Fatemi%20and%20Lars%20Hammarstrand%20and%20Martin%20R.%20Oswald%20and%20Lennart%20Svensson%0AAbstract%3A%20Reconstructing%20dynamic%20driving%20scenes%20is%20essential%20for%20developing%20autonomous%20systems%20through%20sensor-realistic%20simulation.%20Although%20recent%20methods%20achieve%20high-fidelity%20reconstructions%2C%20they%20either%20rely%20on%20costly%20human%20annotations%20for%20object%20trajectories%20or%20use%20time-varying%20representations%20without%20explicit%20object-level%20decomposition%2C%20leading%20to%20intertwined%20static%20and%20dynamic%20elements%20that%20hinder%20scene%20separation.%20We%20present%20IDSplat%2C%20a%20self-supervised%203D%20Gaussian%20Splatting%20framework%20that%20reconstructs%20dynamic%20scenes%20with%20explicit%20instance%20decomposition%20and%20learnable%20motion%20trajectories%2C%20without%20requiring%20human%20annotations.%20Our%20key%20insight%20is%20to%20model%20dynamic%20objects%20as%20coherent%20instances%20undergoing%20rigid%20transformations%2C%20rather%20than%20unstructured%20time-varying%20primitives.%20For%20instance%20decomposition%2C%20we%20employ%20zero-shot%2C%20language-grounded%20video%20tracking%20anchored%20to%203D%20using%20lidar%2C%20and%20estimate%20consistent%20poses%20via%20feature%20correspondences.%20We%20introduce%20a%20coordinated-turn%20smoothing%20scheme%20to%20obtain%20temporally%20and%20physically%20consistent%20motion%20trajectories%2C%20mitigating%20pose%20misalignments%20and%20tracking%20failures%2C%20followed%20by%20joint%20optimization%20of%20object%20poses%20and%20Gaussian%20parameters.%20Experiments%20on%20the%20Waymo%20Open%20Dataset%20demonstrate%20that%20our%20method%20achieves%20competitive%20reconstruction%20quality%20while%20maintaining%20instance-level%20decomposition%20and%20generalizes%20across%20diverse%20sequences%20and%20view%20densities%20without%20retraining%2C%20making%20it%20practical%20for%20large-scale%20autonomous%20driving%20applications.%20Code%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIDSplat%253A%2520Instance-Decomposed%25203D%2520Gaussian%2520Splatting%2520for%2520Driving%2520Scenes%26entry.906535625%3DCarl%2520Lindstr%25C3%25B6m%2520and%2520Mahan%2520Rafidashti%2520and%2520Maryam%2520Fatemi%2520and%2520Lars%2520Hammarstrand%2520and%2520Martin%2520R.%2520Oswald%2520and%2520Lennart%2520Svensson%26entry.1292438233%3DReconstructing%2520dynamic%2520driving%2520scenes%2520is%2520essential%2520for%2520developing%2520autonomous%2520systems%2520through%2520sensor-realistic%2520simulation.%2520Although%2520recent%2520methods%2520achieve%2520high-fidelity%2520reconstructions%252C%2520they%2520either%2520rely%2520on%2520costly%2520human%2520annotations%2520for%2520object%2520trajectories%2520or%2520use%2520time-varying%2520representations%2520without%2520explicit%2520object-level%2520decomposition%252C%2520leading%2520to%2520intertwined%2520static%2520and%2520dynamic%2520elements%2520that%2520hinder%2520scene%2520separation.%2520We%2520present%2520IDSplat%252C%2520a%2520self-supervised%25203D%2520Gaussian%2520Splatting%2520framework%2520that%2520reconstructs%2520dynamic%2520scenes%2520with%2520explicit%2520instance%2520decomposition%2520and%2520learnable%2520motion%2520trajectories%252C%2520without%2520requiring%2520human%2520annotations.%2520Our%2520key%2520insight%2520is%2520to%2520model%2520dynamic%2520objects%2520as%2520coherent%2520instances%2520undergoing%2520rigid%2520transformations%252C%2520rather%2520than%2520unstructured%2520time-varying%2520primitives.%2520For%2520instance%2520decomposition%252C%2520we%2520employ%2520zero-shot%252C%2520language-grounded%2520video%2520tracking%2520anchored%2520to%25203D%2520using%2520lidar%252C%2520and%2520estimate%2520consistent%2520poses%2520via%2520feature%2520correspondences.%2520We%2520introduce%2520a%2520coordinated-turn%2520smoothing%2520scheme%2520to%2520obtain%2520temporally%2520and%2520physically%2520consistent%2520motion%2520trajectories%252C%2520mitigating%2520pose%2520misalignments%2520and%2520tracking%2520failures%252C%2520followed%2520by%2520joint%2520optimization%2520of%2520object%2520poses%2520and%2520Gaussian%2520parameters.%2520Experiments%2520on%2520the%2520Waymo%2520Open%2520Dataset%2520demonstrate%2520that%2520our%2520method%2520achieves%2520competitive%2520reconstruction%2520quality%2520while%2520maintaining%2520instance-level%2520decomposition%2520and%2520generalizes%2520across%2520diverse%2520sequences%2520and%2520view%2520densities%2520without%2520retraining%252C%2520making%2520it%2520practical%2520for%2520large-scale%2520autonomous%2520driving%2520applications.%2520Code%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IDSplat%3A%20Instance-Decomposed%203D%20Gaussian%20Splatting%20for%20Driving%20Scenes&entry.906535625=Carl%20Lindstr%C3%B6m%20and%20Mahan%20Rafidashti%20and%20Maryam%20Fatemi%20and%20Lars%20Hammarstrand%20and%20Martin%20R.%20Oswald%20and%20Lennart%20Svensson&entry.1292438233=Reconstructing%20dynamic%20driving%20scenes%20is%20essential%20for%20developing%20autonomous%20systems%20through%20sensor-realistic%20simulation.%20Although%20recent%20methods%20achieve%20high-fidelity%20reconstructions%2C%20they%20either%20rely%20on%20costly%20human%20annotations%20for%20object%20trajectories%20or%20use%20time-varying%20representations%20without%20explicit%20object-level%20decomposition%2C%20leading%20to%20intertwined%20static%20and%20dynamic%20elements%20that%20hinder%20scene%20separation.%20We%20present%20IDSplat%2C%20a%20self-supervised%203D%20Gaussian%20Splatting%20framework%20that%20reconstructs%20dynamic%20scenes%20with%20explicit%20instance%20decomposition%20and%20learnable%20motion%20trajectories%2C%20without%20requiring%20human%20annotations.%20Our%20key%20insight%20is%20to%20model%20dynamic%20objects%20as%20coherent%20instances%20undergoing%20rigid%20transformations%2C%20rather%20than%20unstructured%20time-varying%20primitives.%20For%20instance%20decomposition%2C%20we%20employ%20zero-shot%2C%20language-grounded%20video%20tracking%20anchored%20to%203D%20using%20lidar%2C%20and%20estimate%20consistent%20poses%20via%20feature%20correspondences.%20We%20introduce%20a%20coordinated-turn%20smoothing%20scheme%20to%20obtain%20temporally%20and%20physically%20consistent%20motion%20trajectories%2C%20mitigating%20pose%20misalignments%20and%20tracking%20failures%2C%20followed%20by%20joint%20optimization%20of%20object%20poses%20and%20Gaussian%20parameters.%20Experiments%20on%20the%20Waymo%20Open%20Dataset%20demonstrate%20that%20our%20method%20achieves%20competitive%20reconstruction%20quality%20while%20maintaining%20instance-level%20decomposition%20and%20generalizes%20across%20diverse%20sequences%20and%20view%20densities%20without%20retraining%2C%20making%20it%20practical%20for%20large-scale%20autonomous%20driving%20applications.%20Code%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2511.19235v1&entry.124074799=Read"},
{"title": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation", "author": "Jiaming Zhang and Shengming Cao and Rui Li and Xiaotong Zhao and Yutao Cui and Xinglin Hou and Gangshan Wu and Haolan Chen and Yu Xu and Limin Wang and Kai Ma", "abstract": "Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.", "link": "http://arxiv.org/abs/2511.19320v1", "date": "2025-11-24", "relevancy": 3.2397, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6874}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6768}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SteadyDancer%3A%20Harmonized%20and%20Coherent%20Human%20Image%20Animation%20with%20First-Frame%20Preservation&body=Title%3A%20SteadyDancer%3A%20Harmonized%20and%20Coherent%20Human%20Image%20Animation%20with%20First-Frame%20Preservation%0AAuthor%3A%20Jiaming%20Zhang%20and%20Shengming%20Cao%20and%20Rui%20Li%20and%20Xiaotong%20Zhao%20and%20Yutao%20Cui%20and%20Xinglin%20Hou%20and%20Gangshan%20Wu%20and%20Haolan%20Chen%20and%20Yu%20Xu%20and%20Limin%20Wang%20and%20Kai%20Ma%0AAbstract%3A%20Preserving%20first-frame%20identity%20while%20ensuring%20precise%20motion%20control%20is%20a%20fundamental%20challenge%20in%20human%20image%20animation.%20The%20Image-to-Motion%20Binding%20process%20of%20the%20dominant%20Reference-to-Video%20%28R2V%29%20paradigm%20overlooks%20critical%20spatio-temporal%20misalignments%20common%20in%20real-world%20applications%2C%20leading%20to%20failures%20such%20as%20identity%20drift%20and%20visual%20artifacts.%20We%20introduce%20SteadyDancer%2C%20an%20Image-to-Video%20%28I2V%29%20paradigm-based%20framework%20that%20achieves%20harmonized%20and%20coherent%20animation%20and%20is%20the%20first%20to%20ensure%20first-frame%20preservation%20robustly.%20Firstly%2C%20we%20propose%20a%20Condition-Reconciliation%20Mechanism%20to%20harmonize%20the%20two%20conflicting%20conditions%2C%20enabling%20precise%20control%20without%20sacrificing%20fidelity.%20Secondly%2C%20we%20design%20Synergistic%20Pose%20Modulation%20Modules%20to%20generate%20an%20adaptive%20and%20coherent%20pose%20representation%20that%20is%20highly%20compatible%20with%20the%20reference%20image.%20Finally%2C%20we%20employ%20a%20Staged%20Decoupled-Objective%20Training%20Pipeline%20that%20hierarchically%20optimizes%20the%20model%20for%20motion%20fidelity%2C%20visual%20quality%2C%20and%20temporal%20coherence.%20Experiments%20demonstrate%20that%20SteadyDancer%20achieves%20state-of-the-art%20performance%20in%20both%20appearance%20fidelity%20and%20motion%20control%2C%20while%20requiring%20significantly%20fewer%20training%20resources%20than%20comparable%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteadyDancer%253A%2520Harmonized%2520and%2520Coherent%2520Human%2520Image%2520Animation%2520with%2520First-Frame%2520Preservation%26entry.906535625%3DJiaming%2520Zhang%2520and%2520Shengming%2520Cao%2520and%2520Rui%2520Li%2520and%2520Xiaotong%2520Zhao%2520and%2520Yutao%2520Cui%2520and%2520Xinglin%2520Hou%2520and%2520Gangshan%2520Wu%2520and%2520Haolan%2520Chen%2520and%2520Yu%2520Xu%2520and%2520Limin%2520Wang%2520and%2520Kai%2520Ma%26entry.1292438233%3DPreserving%2520first-frame%2520identity%2520while%2520ensuring%2520precise%2520motion%2520control%2520is%2520a%2520fundamental%2520challenge%2520in%2520human%2520image%2520animation.%2520The%2520Image-to-Motion%2520Binding%2520process%2520of%2520the%2520dominant%2520Reference-to-Video%2520%2528R2V%2529%2520paradigm%2520overlooks%2520critical%2520spatio-temporal%2520misalignments%2520common%2520in%2520real-world%2520applications%252C%2520leading%2520to%2520failures%2520such%2520as%2520identity%2520drift%2520and%2520visual%2520artifacts.%2520We%2520introduce%2520SteadyDancer%252C%2520an%2520Image-to-Video%2520%2528I2V%2529%2520paradigm-based%2520framework%2520that%2520achieves%2520harmonized%2520and%2520coherent%2520animation%2520and%2520is%2520the%2520first%2520to%2520ensure%2520first-frame%2520preservation%2520robustly.%2520Firstly%252C%2520we%2520propose%2520a%2520Condition-Reconciliation%2520Mechanism%2520to%2520harmonize%2520the%2520two%2520conflicting%2520conditions%252C%2520enabling%2520precise%2520control%2520without%2520sacrificing%2520fidelity.%2520Secondly%252C%2520we%2520design%2520Synergistic%2520Pose%2520Modulation%2520Modules%2520to%2520generate%2520an%2520adaptive%2520and%2520coherent%2520pose%2520representation%2520that%2520is%2520highly%2520compatible%2520with%2520the%2520reference%2520image.%2520Finally%252C%2520we%2520employ%2520a%2520Staged%2520Decoupled-Objective%2520Training%2520Pipeline%2520that%2520hierarchically%2520optimizes%2520the%2520model%2520for%2520motion%2520fidelity%252C%2520visual%2520quality%252C%2520and%2520temporal%2520coherence.%2520Experiments%2520demonstrate%2520that%2520SteadyDancer%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520appearance%2520fidelity%2520and%2520motion%2520control%252C%2520while%2520requiring%2520significantly%2520fewer%2520training%2520resources%2520than%2520comparable%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SteadyDancer%3A%20Harmonized%20and%20Coherent%20Human%20Image%20Animation%20with%20First-Frame%20Preservation&entry.906535625=Jiaming%20Zhang%20and%20Shengming%20Cao%20and%20Rui%20Li%20and%20Xiaotong%20Zhao%20and%20Yutao%20Cui%20and%20Xinglin%20Hou%20and%20Gangshan%20Wu%20and%20Haolan%20Chen%20and%20Yu%20Xu%20and%20Limin%20Wang%20and%20Kai%20Ma&entry.1292438233=Preserving%20first-frame%20identity%20while%20ensuring%20precise%20motion%20control%20is%20a%20fundamental%20challenge%20in%20human%20image%20animation.%20The%20Image-to-Motion%20Binding%20process%20of%20the%20dominant%20Reference-to-Video%20%28R2V%29%20paradigm%20overlooks%20critical%20spatio-temporal%20misalignments%20common%20in%20real-world%20applications%2C%20leading%20to%20failures%20such%20as%20identity%20drift%20and%20visual%20artifacts.%20We%20introduce%20SteadyDancer%2C%20an%20Image-to-Video%20%28I2V%29%20paradigm-based%20framework%20that%20achieves%20harmonized%20and%20coherent%20animation%20and%20is%20the%20first%20to%20ensure%20first-frame%20preservation%20robustly.%20Firstly%2C%20we%20propose%20a%20Condition-Reconciliation%20Mechanism%20to%20harmonize%20the%20two%20conflicting%20conditions%2C%20enabling%20precise%20control%20without%20sacrificing%20fidelity.%20Secondly%2C%20we%20design%20Synergistic%20Pose%20Modulation%20Modules%20to%20generate%20an%20adaptive%20and%20coherent%20pose%20representation%20that%20is%20highly%20compatible%20with%20the%20reference%20image.%20Finally%2C%20we%20employ%20a%20Staged%20Decoupled-Objective%20Training%20Pipeline%20that%20hierarchically%20optimizes%20the%20model%20for%20motion%20fidelity%2C%20visual%20quality%2C%20and%20temporal%20coherence.%20Experiments%20demonstrate%20that%20SteadyDancer%20achieves%20state-of-the-art%20performance%20in%20both%20appearance%20fidelity%20and%20motion%20control%2C%20while%20requiring%20significantly%20fewer%20training%20resources%20than%20comparable%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.19320v1&entry.124074799=Read"},
{"title": "NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting", "author": "Brent Zoomers and Florian Hahlbohm and Joni Vanherck and Lode Jorissen and Marcus Magnor and Nick Michiels", "abstract": "3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.", "link": "http://arxiv.org/abs/2511.19202v1", "date": "2025-11-24", "relevancy": 3.2395, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6686}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.652}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NVGS%3A%20Neural%20Visibility%20for%20Occlusion%20Culling%20in%203D%20Gaussian%20Splatting&body=Title%3A%20NVGS%3A%20Neural%20Visibility%20for%20Occlusion%20Culling%20in%203D%20Gaussian%20Splatting%0AAuthor%3A%20Brent%20Zoomers%20and%20Florian%20Hahlbohm%20and%20Joni%20Vanherck%20and%20Lode%20Jorissen%20and%20Marcus%20Magnor%20and%20Nick%20Michiels%0AAbstract%3A%203D%20Gaussian%20Splatting%20can%20exploit%20frustum%20culling%20and%20level-of-detail%20strategies%20to%20accelerate%20rendering%20of%20scenes%20containing%20a%20large%20number%20of%20primitives.%20However%2C%20the%20semi-transparent%20nature%20of%20Gaussians%20prevents%20the%20application%20of%20another%20highly%20effective%20technique%3A%20occlusion%20culling.%20We%20address%20this%20limitation%20by%20proposing%20a%20novel%20method%20to%20learn%20the%20viewpoint-dependent%20visibility%20function%20of%20all%20Gaussians%20in%20a%20trained%20model%20using%20a%20small%2C%20shared%20MLP%20across%20instances%20of%20an%20asset%20in%20a%20scene.%20By%20querying%20it%20for%20Gaussians%20within%20the%20viewing%20frustum%20prior%20to%20rasterization%2C%20our%20method%20can%20discard%20occluded%20primitives%20during%20rendering.%20Leveraging%20Tensor%20Cores%20for%20efficient%20computation%2C%20we%20integrate%20these%20neural%20queries%20directly%20into%20a%20novel%20instanced%20software%20rasterizer.%20Our%20approach%20outperforms%20the%20current%20state%20of%20the%20art%20for%20composed%20scenes%20in%20terms%20of%20VRAM%20usage%20and%20image%20quality%2C%20utilizing%20a%20combination%20of%20our%20instanced%20rasterizer%20and%20occlusion%20culling%20MLP%2C%20and%20exhibits%20complementary%20properties%20to%20existing%20LoD%20techniques.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNVGS%253A%2520Neural%2520Visibility%2520for%2520Occlusion%2520Culling%2520in%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DBrent%2520Zoomers%2520and%2520Florian%2520Hahlbohm%2520and%2520Joni%2520Vanherck%2520and%2520Lode%2520Jorissen%2520and%2520Marcus%2520Magnor%2520and%2520Nick%2520Michiels%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520can%2520exploit%2520frustum%2520culling%2520and%2520level-of-detail%2520strategies%2520to%2520accelerate%2520rendering%2520of%2520scenes%2520containing%2520a%2520large%2520number%2520of%2520primitives.%2520However%252C%2520the%2520semi-transparent%2520nature%2520of%2520Gaussians%2520prevents%2520the%2520application%2520of%2520another%2520highly%2520effective%2520technique%253A%2520occlusion%2520culling.%2520We%2520address%2520this%2520limitation%2520by%2520proposing%2520a%2520novel%2520method%2520to%2520learn%2520the%2520viewpoint-dependent%2520visibility%2520function%2520of%2520all%2520Gaussians%2520in%2520a%2520trained%2520model%2520using%2520a%2520small%252C%2520shared%2520MLP%2520across%2520instances%2520of%2520an%2520asset%2520in%2520a%2520scene.%2520By%2520querying%2520it%2520for%2520Gaussians%2520within%2520the%2520viewing%2520frustum%2520prior%2520to%2520rasterization%252C%2520our%2520method%2520can%2520discard%2520occluded%2520primitives%2520during%2520rendering.%2520Leveraging%2520Tensor%2520Cores%2520for%2520efficient%2520computation%252C%2520we%2520integrate%2520these%2520neural%2520queries%2520directly%2520into%2520a%2520novel%2520instanced%2520software%2520rasterizer.%2520Our%2520approach%2520outperforms%2520the%2520current%2520state%2520of%2520the%2520art%2520for%2520composed%2520scenes%2520in%2520terms%2520of%2520VRAM%2520usage%2520and%2520image%2520quality%252C%2520utilizing%2520a%2520combination%2520of%2520our%2520instanced%2520rasterizer%2520and%2520occlusion%2520culling%2520MLP%252C%2520and%2520exhibits%2520complementary%2520properties%2520to%2520existing%2520LoD%2520techniques.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NVGS%3A%20Neural%20Visibility%20for%20Occlusion%20Culling%20in%203D%20Gaussian%20Splatting&entry.906535625=Brent%20Zoomers%20and%20Florian%20Hahlbohm%20and%20Joni%20Vanherck%20and%20Lode%20Jorissen%20and%20Marcus%20Magnor%20and%20Nick%20Michiels&entry.1292438233=3D%20Gaussian%20Splatting%20can%20exploit%20frustum%20culling%20and%20level-of-detail%20strategies%20to%20accelerate%20rendering%20of%20scenes%20containing%20a%20large%20number%20of%20primitives.%20However%2C%20the%20semi-transparent%20nature%20of%20Gaussians%20prevents%20the%20application%20of%20another%20highly%20effective%20technique%3A%20occlusion%20culling.%20We%20address%20this%20limitation%20by%20proposing%20a%20novel%20method%20to%20learn%20the%20viewpoint-dependent%20visibility%20function%20of%20all%20Gaussians%20in%20a%20trained%20model%20using%20a%20small%2C%20shared%20MLP%20across%20instances%20of%20an%20asset%20in%20a%20scene.%20By%20querying%20it%20for%20Gaussians%20within%20the%20viewing%20frustum%20prior%20to%20rasterization%2C%20our%20method%20can%20discard%20occluded%20primitives%20during%20rendering.%20Leveraging%20Tensor%20Cores%20for%20efficient%20computation%2C%20we%20integrate%20these%20neural%20queries%20directly%20into%20a%20novel%20instanced%20software%20rasterizer.%20Our%20approach%20outperforms%20the%20current%20state%20of%20the%20art%20for%20composed%20scenes%20in%20terms%20of%20VRAM%20usage%20and%20image%20quality%2C%20utilizing%20a%20combination%20of%20our%20instanced%20rasterizer%20and%20occlusion%20culling%20MLP%2C%20and%20exhibits%20complementary%20properties%20to%20existing%20LoD%20techniques.&entry.1838667208=http%3A//arxiv.org/abs/2511.19202v1&entry.124074799=Read"},
{"title": "DensifyBeforehand: LiDAR-assisted Content-aware Densification for Efficient and Quality 3D Gaussian Splatting", "author": "Phurtivilai Patt and Leyang Huang and Yinqiang Zhang and Yang Lei", "abstract": "This paper addresses the limitations of existing 3D Gaussian Splatting (3DGS) methods, particularly their reliance on adaptive density control, which can lead to floating artifacts and inefficient resource usage. We propose a novel densify beforehand approach that enhances the initialization of 3D scenes by combining sparse LiDAR data with monocular depth estimation from corresponding RGB images. Our ROI-aware sampling scheme prioritizes semantically and geometrically important regions, yielding a dense point cloud that improves visual fidelity and computational efficiency. This densify beforehand approach bypasses the adaptive density control that may introduce redundant Gaussians in the original pipeline, allowing the optimization to focus on the other attributes of 3D Gaussian primitives, reducing overlap while enhancing visual quality. Our method achieves comparable results to state-of-the-art techniques while significantly lowering resource consumption and training time. We validate our approach through extensive comparisons and ablation studies on four newly collected datasets, showcasing its effectiveness in preserving regions of interest in complex scenes.", "link": "http://arxiv.org/abs/2511.19294v1", "date": "2025-11-24", "relevancy": 3.2388, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6863}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6303}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DensifyBeforehand%3A%20LiDAR-assisted%20Content-aware%20Densification%20for%20Efficient%20and%20Quality%203D%20Gaussian%20Splatting&body=Title%3A%20DensifyBeforehand%3A%20LiDAR-assisted%20Content-aware%20Densification%20for%20Efficient%20and%20Quality%203D%20Gaussian%20Splatting%0AAuthor%3A%20Phurtivilai%20Patt%20and%20Leyang%20Huang%20and%20Yinqiang%20Zhang%20and%20Yang%20Lei%0AAbstract%3A%20This%20paper%20addresses%20the%20limitations%20of%20existing%203D%20Gaussian%20Splatting%20%283DGS%29%20methods%2C%20particularly%20their%20reliance%20on%20adaptive%20density%20control%2C%20which%20can%20lead%20to%20floating%20artifacts%20and%20inefficient%20resource%20usage.%20We%20propose%20a%20novel%20densify%20beforehand%20approach%20that%20enhances%20the%20initialization%20of%203D%20scenes%20by%20combining%20sparse%20LiDAR%20data%20with%20monocular%20depth%20estimation%20from%20corresponding%20RGB%20images.%20Our%20ROI-aware%20sampling%20scheme%20prioritizes%20semantically%20and%20geometrically%20important%20regions%2C%20yielding%20a%20dense%20point%20cloud%20that%20improves%20visual%20fidelity%20and%20computational%20efficiency.%20This%20densify%20beforehand%20approach%20bypasses%20the%20adaptive%20density%20control%20that%20may%20introduce%20redundant%20Gaussians%20in%20the%20original%20pipeline%2C%20allowing%20the%20optimization%20to%20focus%20on%20the%20other%20attributes%20of%203D%20Gaussian%20primitives%2C%20reducing%20overlap%20while%20enhancing%20visual%20quality.%20Our%20method%20achieves%20comparable%20results%20to%20state-of-the-art%20techniques%20while%20significantly%20lowering%20resource%20consumption%20and%20training%20time.%20We%20validate%20our%20approach%20through%20extensive%20comparisons%20and%20ablation%20studies%20on%20four%20newly%20collected%20datasets%2C%20showcasing%20its%20effectiveness%20in%20preserving%20regions%20of%20interest%20in%20complex%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDensifyBeforehand%253A%2520LiDAR-assisted%2520Content-aware%2520Densification%2520for%2520Efficient%2520and%2520Quality%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DPhurtivilai%2520Patt%2520and%2520Leyang%2520Huang%2520and%2520Yinqiang%2520Zhang%2520and%2520Yang%2520Lei%26entry.1292438233%3DThis%2520paper%2520addresses%2520the%2520limitations%2520of%2520existing%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520methods%252C%2520particularly%2520their%2520reliance%2520on%2520adaptive%2520density%2520control%252C%2520which%2520can%2520lead%2520to%2520floating%2520artifacts%2520and%2520inefficient%2520resource%2520usage.%2520We%2520propose%2520a%2520novel%2520densify%2520beforehand%2520approach%2520that%2520enhances%2520the%2520initialization%2520of%25203D%2520scenes%2520by%2520combining%2520sparse%2520LiDAR%2520data%2520with%2520monocular%2520depth%2520estimation%2520from%2520corresponding%2520RGB%2520images.%2520Our%2520ROI-aware%2520sampling%2520scheme%2520prioritizes%2520semantically%2520and%2520geometrically%2520important%2520regions%252C%2520yielding%2520a%2520dense%2520point%2520cloud%2520that%2520improves%2520visual%2520fidelity%2520and%2520computational%2520efficiency.%2520This%2520densify%2520beforehand%2520approach%2520bypasses%2520the%2520adaptive%2520density%2520control%2520that%2520may%2520introduce%2520redundant%2520Gaussians%2520in%2520the%2520original%2520pipeline%252C%2520allowing%2520the%2520optimization%2520to%2520focus%2520on%2520the%2520other%2520attributes%2520of%25203D%2520Gaussian%2520primitives%252C%2520reducing%2520overlap%2520while%2520enhancing%2520visual%2520quality.%2520Our%2520method%2520achieves%2520comparable%2520results%2520to%2520state-of-the-art%2520techniques%2520while%2520significantly%2520lowering%2520resource%2520consumption%2520and%2520training%2520time.%2520We%2520validate%2520our%2520approach%2520through%2520extensive%2520comparisons%2520and%2520ablation%2520studies%2520on%2520four%2520newly%2520collected%2520datasets%252C%2520showcasing%2520its%2520effectiveness%2520in%2520preserving%2520regions%2520of%2520interest%2520in%2520complex%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DensifyBeforehand%3A%20LiDAR-assisted%20Content-aware%20Densification%20for%20Efficient%20and%20Quality%203D%20Gaussian%20Splatting&entry.906535625=Phurtivilai%20Patt%20and%20Leyang%20Huang%20and%20Yinqiang%20Zhang%20and%20Yang%20Lei&entry.1292438233=This%20paper%20addresses%20the%20limitations%20of%20existing%203D%20Gaussian%20Splatting%20%283DGS%29%20methods%2C%20particularly%20their%20reliance%20on%20adaptive%20density%20control%2C%20which%20can%20lead%20to%20floating%20artifacts%20and%20inefficient%20resource%20usage.%20We%20propose%20a%20novel%20densify%20beforehand%20approach%20that%20enhances%20the%20initialization%20of%203D%20scenes%20by%20combining%20sparse%20LiDAR%20data%20with%20monocular%20depth%20estimation%20from%20corresponding%20RGB%20images.%20Our%20ROI-aware%20sampling%20scheme%20prioritizes%20semantically%20and%20geometrically%20important%20regions%2C%20yielding%20a%20dense%20point%20cloud%20that%20improves%20visual%20fidelity%20and%20computational%20efficiency.%20This%20densify%20beforehand%20approach%20bypasses%20the%20adaptive%20density%20control%20that%20may%20introduce%20redundant%20Gaussians%20in%20the%20original%20pipeline%2C%20allowing%20the%20optimization%20to%20focus%20on%20the%20other%20attributes%20of%203D%20Gaussian%20primitives%2C%20reducing%20overlap%20while%20enhancing%20visual%20quality.%20Our%20method%20achieves%20comparable%20results%20to%20state-of-the-art%20techniques%20while%20significantly%20lowering%20resource%20consumption%20and%20training%20time.%20We%20validate%20our%20approach%20through%20extensive%20comparisons%20and%20ablation%20studies%20on%20four%20newly%20collected%20datasets%2C%20showcasing%20its%20effectiveness%20in%20preserving%20regions%20of%20interest%20in%20complex%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2511.19294v1&entry.124074799=Read"},
{"title": "SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis", "author": "Lingwei Dang and Zonghan Li and Juntong Li and Hongwen Zhang and Liang An and Yebin Liu and Qingyao Wu", "abstract": "Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.", "link": "http://arxiv.org/abs/2511.19319v1", "date": "2025-11-24", "relevancy": 3.2282, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7309}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.603}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SyncMV4D%3A%20Synchronized%20Multi-view%20Joint%20Diffusion%20of%20Appearance%20and%20Motion%20for%20Hand-Object%20Interaction%20Synthesis&body=Title%3A%20SyncMV4D%3A%20Synchronized%20Multi-view%20Joint%20Diffusion%20of%20Appearance%20and%20Motion%20for%20Hand-Object%20Interaction%20Synthesis%0AAuthor%3A%20Lingwei%20Dang%20and%20Zonghan%20Li%20and%20Juntong%20Li%20and%20Hongwen%20Zhang%20and%20Liang%20An%20and%20Yebin%20Liu%20and%20Qingyao%20Wu%0AAbstract%3A%20Hand-Object%20Interaction%20%28HOI%29%20generation%20plays%20a%20critical%20role%20in%20advancing%20applications%20across%20animation%20and%20robotics.%20Current%20video-based%20methods%20are%20predominantly%20single-view%2C%20which%20impedes%20comprehensive%203D%20geometry%20perception%20and%20often%20results%20in%20geometric%20distortions%20or%20unrealistic%20motion%20patterns.%20While%203D%20HOI%20approaches%20can%20generate%20dynamically%20plausible%20motions%2C%20their%20dependence%20on%20high-quality%203D%20data%20captured%20in%20controlled%20laboratory%20settings%20severely%20limits%20their%20generalization%20to%20real-world%20scenarios.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20SyncMV4D%2C%20the%20first%20model%20that%20jointly%20generates%20synchronized%20multi-view%20HOI%20videos%20and%204D%20motions%20by%20unifying%20visual%20prior%2C%20motion%20dynamics%2C%20and%20multi-view%20geometry.%20Our%20framework%20features%20two%20core%20innovations%3A%20%281%29%20a%20Multi-view%20Joint%20Diffusion%20%28MJD%29%20model%20that%20co-generates%20HOI%20videos%20and%20intermediate%20motions%2C%20and%20%282%29%20a%20Diffusion%20Points%20Aligner%20%28DPA%29%20that%20refines%20the%20coarse%20intermediate%20motion%20into%20globally%20aligned%204D%20metric%20point%20tracks.%20To%20tightly%20couple%202D%20appearance%20with%204D%20dynamics%2C%20we%20establish%20a%20closed-loop%2C%20mutually%20enhancing%20cycle.%20During%20the%20diffusion%20denoising%20process%2C%20the%20generated%20video%20conditions%20the%20refinement%20of%20the%204D%20motion%2C%20while%20the%20aligned%204D%20point%20tracks%20are%20reprojected%20to%20guide%20next-step%20joint%20generation.%20Experimentally%2C%20our%20method%20demonstrates%20superior%20performance%20to%20state-of-the-art%20alternatives%20in%20visual%20realism%2C%20motion%20plausibility%2C%20and%20multi-view%20consistency.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSyncMV4D%253A%2520Synchronized%2520Multi-view%2520Joint%2520Diffusion%2520of%2520Appearance%2520and%2520Motion%2520for%2520Hand-Object%2520Interaction%2520Synthesis%26entry.906535625%3DLingwei%2520Dang%2520and%2520Zonghan%2520Li%2520and%2520Juntong%2520Li%2520and%2520Hongwen%2520Zhang%2520and%2520Liang%2520An%2520and%2520Yebin%2520Liu%2520and%2520Qingyao%2520Wu%26entry.1292438233%3DHand-Object%2520Interaction%2520%2528HOI%2529%2520generation%2520plays%2520a%2520critical%2520role%2520in%2520advancing%2520applications%2520across%2520animation%2520and%2520robotics.%2520Current%2520video-based%2520methods%2520are%2520predominantly%2520single-view%252C%2520which%2520impedes%2520comprehensive%25203D%2520geometry%2520perception%2520and%2520often%2520results%2520in%2520geometric%2520distortions%2520or%2520unrealistic%2520motion%2520patterns.%2520While%25203D%2520HOI%2520approaches%2520can%2520generate%2520dynamically%2520plausible%2520motions%252C%2520their%2520dependence%2520on%2520high-quality%25203D%2520data%2520captured%2520in%2520controlled%2520laboratory%2520settings%2520severely%2520limits%2520their%2520generalization%2520to%2520real-world%2520scenarios.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520SyncMV4D%252C%2520the%2520first%2520model%2520that%2520jointly%2520generates%2520synchronized%2520multi-view%2520HOI%2520videos%2520and%25204D%2520motions%2520by%2520unifying%2520visual%2520prior%252C%2520motion%2520dynamics%252C%2520and%2520multi-view%2520geometry.%2520Our%2520framework%2520features%2520two%2520core%2520innovations%253A%2520%25281%2529%2520a%2520Multi-view%2520Joint%2520Diffusion%2520%2528MJD%2529%2520model%2520that%2520co-generates%2520HOI%2520videos%2520and%2520intermediate%2520motions%252C%2520and%2520%25282%2529%2520a%2520Diffusion%2520Points%2520Aligner%2520%2528DPA%2529%2520that%2520refines%2520the%2520coarse%2520intermediate%2520motion%2520into%2520globally%2520aligned%25204D%2520metric%2520point%2520tracks.%2520To%2520tightly%2520couple%25202D%2520appearance%2520with%25204D%2520dynamics%252C%2520we%2520establish%2520a%2520closed-loop%252C%2520mutually%2520enhancing%2520cycle.%2520During%2520the%2520diffusion%2520denoising%2520process%252C%2520the%2520generated%2520video%2520conditions%2520the%2520refinement%2520of%2520the%25204D%2520motion%252C%2520while%2520the%2520aligned%25204D%2520point%2520tracks%2520are%2520reprojected%2520to%2520guide%2520next-step%2520joint%2520generation.%2520Experimentally%252C%2520our%2520method%2520demonstrates%2520superior%2520performance%2520to%2520state-of-the-art%2520alternatives%2520in%2520visual%2520realism%252C%2520motion%2520plausibility%252C%2520and%2520multi-view%2520consistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SyncMV4D%3A%20Synchronized%20Multi-view%20Joint%20Diffusion%20of%20Appearance%20and%20Motion%20for%20Hand-Object%20Interaction%20Synthesis&entry.906535625=Lingwei%20Dang%20and%20Zonghan%20Li%20and%20Juntong%20Li%20and%20Hongwen%20Zhang%20and%20Liang%20An%20and%20Yebin%20Liu%20and%20Qingyao%20Wu&entry.1292438233=Hand-Object%20Interaction%20%28HOI%29%20generation%20plays%20a%20critical%20role%20in%20advancing%20applications%20across%20animation%20and%20robotics.%20Current%20video-based%20methods%20are%20predominantly%20single-view%2C%20which%20impedes%20comprehensive%203D%20geometry%20perception%20and%20often%20results%20in%20geometric%20distortions%20or%20unrealistic%20motion%20patterns.%20While%203D%20HOI%20approaches%20can%20generate%20dynamically%20plausible%20motions%2C%20their%20dependence%20on%20high-quality%203D%20data%20captured%20in%20controlled%20laboratory%20settings%20severely%20limits%20their%20generalization%20to%20real-world%20scenarios.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20SyncMV4D%2C%20the%20first%20model%20that%20jointly%20generates%20synchronized%20multi-view%20HOI%20videos%20and%204D%20motions%20by%20unifying%20visual%20prior%2C%20motion%20dynamics%2C%20and%20multi-view%20geometry.%20Our%20framework%20features%20two%20core%20innovations%3A%20%281%29%20a%20Multi-view%20Joint%20Diffusion%20%28MJD%29%20model%20that%20co-generates%20HOI%20videos%20and%20intermediate%20motions%2C%20and%20%282%29%20a%20Diffusion%20Points%20Aligner%20%28DPA%29%20that%20refines%20the%20coarse%20intermediate%20motion%20into%20globally%20aligned%204D%20metric%20point%20tracks.%20To%20tightly%20couple%202D%20appearance%20with%204D%20dynamics%2C%20we%20establish%20a%20closed-loop%2C%20mutually%20enhancing%20cycle.%20During%20the%20diffusion%20denoising%20process%2C%20the%20generated%20video%20conditions%20the%20refinement%20of%20the%204D%20motion%2C%20while%20the%20aligned%204D%20point%20tracks%20are%20reprojected%20to%20guide%20next-step%20joint%20generation.%20Experimentally%2C%20our%20method%20demonstrates%20superior%20performance%20to%20state-of-the-art%20alternatives%20in%20visual%20realism%2C%20motion%20plausibility%2C%20and%20multi-view%20consistency.&entry.1838667208=http%3A//arxiv.org/abs/2511.19319v1&entry.124074799=Read"},
{"title": "LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models", "author": "Shuai Wang and Daoan Zhang and Tianyi Bai and Shitong Shao and Jiebo Luo and Jiaheng Wei", "abstract": "Humans can perceive and understand 3D space and long videos from sequential visual observations. But do vision-language models (VLMs) can? Recent work demonstrates that even state-of-the-art VLMs still struggle to understand 3D space and long videos, although they are powerful in typical vision-language tasks. Current methods often rely on specialized architectural designs to improve performance for 3D tasks and video understanding tasks separately. In contrast, we propose LAST, short for LeArn to Think in Space and Time, to jointly improve 3D spatial and long video understanding for general VLMs with only a set of 2D images as inputs. LAST makes VLMs think in space and time rather than only with text before giving the final answer, building visual thinking trajectories in 3D space and temporal dimension. We demonstrate the effectiveness of LAST in two scenarios: 1) zero-shot, where we directly prompt proprietary models; and 2) fine-tuning general VLMs with data that include thinking trajectories in 3D space and time. We show that LAST brings substantial gains in various benchmarks, including 3 spatial understanding, 4 video understanding, and 3 image understanding tasks. Notably, 15.8% gains on EgoSchema with GPT-4o in a zero-shot manner and 8.3 gains on VSI-Bench compared with Qwen2.5-VL-7B.", "link": "http://arxiv.org/abs/2511.19261v1", "date": "2025-11-24", "relevancy": 3.1837, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.653}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.653}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAST%3A%20LeArning%20to%20Think%20in%20Space%20and%20Time%20for%20Generalist%20Vision-Language%20Models&body=Title%3A%20LAST%3A%20LeArning%20to%20Think%20in%20Space%20and%20Time%20for%20Generalist%20Vision-Language%20Models%0AAuthor%3A%20Shuai%20Wang%20and%20Daoan%20Zhang%20and%20Tianyi%20Bai%20and%20Shitong%20Shao%20and%20Jiebo%20Luo%20and%20Jiaheng%20Wei%0AAbstract%3A%20Humans%20can%20perceive%20and%20understand%203D%20space%20and%20long%20videos%20from%20sequential%20visual%20observations.%20But%20do%20vision-language%20models%20%28VLMs%29%20can%3F%20Recent%20work%20demonstrates%20that%20even%20state-of-the-art%20VLMs%20still%20struggle%20to%20understand%203D%20space%20and%20long%20videos%2C%20although%20they%20are%20powerful%20in%20typical%20vision-language%20tasks.%20Current%20methods%20often%20rely%20on%20specialized%20architectural%20designs%20to%20improve%20performance%20for%203D%20tasks%20and%20video%20understanding%20tasks%20separately.%20In%20contrast%2C%20we%20propose%20LAST%2C%20short%20for%20LeArn%20to%20Think%20in%20Space%20and%20Time%2C%20to%20jointly%20improve%203D%20spatial%20and%20long%20video%20understanding%20for%20general%20VLMs%20with%20only%20a%20set%20of%202D%20images%20as%20inputs.%20LAST%20makes%20VLMs%20think%20in%20space%20and%20time%20rather%20than%20only%20with%20text%20before%20giving%20the%20final%20answer%2C%20building%20visual%20thinking%20trajectories%20in%203D%20space%20and%20temporal%20dimension.%20We%20demonstrate%20the%20effectiveness%20of%20LAST%20in%20two%20scenarios%3A%201%29%20zero-shot%2C%20where%20we%20directly%20prompt%20proprietary%20models%3B%20and%202%29%20fine-tuning%20general%20VLMs%20with%20data%20that%20include%20thinking%20trajectories%20in%203D%20space%20and%20time.%20We%20show%20that%20LAST%20brings%20substantial%20gains%20in%20various%20benchmarks%2C%20including%203%20spatial%20understanding%2C%204%20video%20understanding%2C%20and%203%20image%20understanding%20tasks.%20Notably%2C%2015.8%25%20gains%20on%20EgoSchema%20with%20GPT-4o%20in%20a%20zero-shot%20manner%20and%208.3%20gains%20on%20VSI-Bench%20compared%20with%20Qwen2.5-VL-7B.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAST%253A%2520LeArning%2520to%2520Think%2520in%2520Space%2520and%2520Time%2520for%2520Generalist%2520Vision-Language%2520Models%26entry.906535625%3DShuai%2520Wang%2520and%2520Daoan%2520Zhang%2520and%2520Tianyi%2520Bai%2520and%2520Shitong%2520Shao%2520and%2520Jiebo%2520Luo%2520and%2520Jiaheng%2520Wei%26entry.1292438233%3DHumans%2520can%2520perceive%2520and%2520understand%25203D%2520space%2520and%2520long%2520videos%2520from%2520sequential%2520visual%2520observations.%2520But%2520do%2520vision-language%2520models%2520%2528VLMs%2529%2520can%253F%2520Recent%2520work%2520demonstrates%2520that%2520even%2520state-of-the-art%2520VLMs%2520still%2520struggle%2520to%2520understand%25203D%2520space%2520and%2520long%2520videos%252C%2520although%2520they%2520are%2520powerful%2520in%2520typical%2520vision-language%2520tasks.%2520Current%2520methods%2520often%2520rely%2520on%2520specialized%2520architectural%2520designs%2520to%2520improve%2520performance%2520for%25203D%2520tasks%2520and%2520video%2520understanding%2520tasks%2520separately.%2520In%2520contrast%252C%2520we%2520propose%2520LAST%252C%2520short%2520for%2520LeArn%2520to%2520Think%2520in%2520Space%2520and%2520Time%252C%2520to%2520jointly%2520improve%25203D%2520spatial%2520and%2520long%2520video%2520understanding%2520for%2520general%2520VLMs%2520with%2520only%2520a%2520set%2520of%25202D%2520images%2520as%2520inputs.%2520LAST%2520makes%2520VLMs%2520think%2520in%2520space%2520and%2520time%2520rather%2520than%2520only%2520with%2520text%2520before%2520giving%2520the%2520final%2520answer%252C%2520building%2520visual%2520thinking%2520trajectories%2520in%25203D%2520space%2520and%2520temporal%2520dimension.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520LAST%2520in%2520two%2520scenarios%253A%25201%2529%2520zero-shot%252C%2520where%2520we%2520directly%2520prompt%2520proprietary%2520models%253B%2520and%25202%2529%2520fine-tuning%2520general%2520VLMs%2520with%2520data%2520that%2520include%2520thinking%2520trajectories%2520in%25203D%2520space%2520and%2520time.%2520We%2520show%2520that%2520LAST%2520brings%2520substantial%2520gains%2520in%2520various%2520benchmarks%252C%2520including%25203%2520spatial%2520understanding%252C%25204%2520video%2520understanding%252C%2520and%25203%2520image%2520understanding%2520tasks.%2520Notably%252C%252015.8%2525%2520gains%2520on%2520EgoSchema%2520with%2520GPT-4o%2520in%2520a%2520zero-shot%2520manner%2520and%25208.3%2520gains%2520on%2520VSI-Bench%2520compared%2520with%2520Qwen2.5-VL-7B.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAST%3A%20LeArning%20to%20Think%20in%20Space%20and%20Time%20for%20Generalist%20Vision-Language%20Models&entry.906535625=Shuai%20Wang%20and%20Daoan%20Zhang%20and%20Tianyi%20Bai%20and%20Shitong%20Shao%20and%20Jiebo%20Luo%20and%20Jiaheng%20Wei&entry.1292438233=Humans%20can%20perceive%20and%20understand%203D%20space%20and%20long%20videos%20from%20sequential%20visual%20observations.%20But%20do%20vision-language%20models%20%28VLMs%29%20can%3F%20Recent%20work%20demonstrates%20that%20even%20state-of-the-art%20VLMs%20still%20struggle%20to%20understand%203D%20space%20and%20long%20videos%2C%20although%20they%20are%20powerful%20in%20typical%20vision-language%20tasks.%20Current%20methods%20often%20rely%20on%20specialized%20architectural%20designs%20to%20improve%20performance%20for%203D%20tasks%20and%20video%20understanding%20tasks%20separately.%20In%20contrast%2C%20we%20propose%20LAST%2C%20short%20for%20LeArn%20to%20Think%20in%20Space%20and%20Time%2C%20to%20jointly%20improve%203D%20spatial%20and%20long%20video%20understanding%20for%20general%20VLMs%20with%20only%20a%20set%20of%202D%20images%20as%20inputs.%20LAST%20makes%20VLMs%20think%20in%20space%20and%20time%20rather%20than%20only%20with%20text%20before%20giving%20the%20final%20answer%2C%20building%20visual%20thinking%20trajectories%20in%203D%20space%20and%20temporal%20dimension.%20We%20demonstrate%20the%20effectiveness%20of%20LAST%20in%20two%20scenarios%3A%201%29%20zero-shot%2C%20where%20we%20directly%20prompt%20proprietary%20models%3B%20and%202%29%20fine-tuning%20general%20VLMs%20with%20data%20that%20include%20thinking%20trajectories%20in%203D%20space%20and%20time.%20We%20show%20that%20LAST%20brings%20substantial%20gains%20in%20various%20benchmarks%2C%20including%203%20spatial%20understanding%2C%204%20video%20understanding%2C%20and%203%20image%20understanding%20tasks.%20Notably%2C%2015.8%25%20gains%20on%20EgoSchema%20with%20GPT-4o%20in%20a%20zero-shot%20manner%20and%208.3%20gains%20on%20VSI-Bench%20compared%20with%20Qwen2.5-VL-7B.&entry.1838667208=http%3A//arxiv.org/abs/2511.19261v1&entry.124074799=Read"},
{"title": "Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding", "author": "Tao Lin and Gen Li and Yilei Zhong and Yanwen Zou and Yuxin Du and Jiting Liu and Encheng Gu and Bo Zhao", "abstract": "Vision-Language-Action (VLA) models have emerged as a promising framework for enabling generalist robots capable of perceiving, reasoning, and acting in the real world. These models usually build upon pretrained Vision-Language Models (VLMs), which excel at semantic understanding due to large-scale image and text pretraining. However, existing VLMs typically lack precise spatial understanding capabilities, as they are primarily tuned on 2D image-text pairs without 3D supervision. To address this limitation, recent approaches have incorporated explicit 3D inputs such as point clouds or depth maps, but this necessitates additional depth sensors or pre-trained depth estimation models, which may yield defective results. In contrast, our work introduces a plug-and-play module that implicitly incorporates 3D geometry features into VLA models by leveraging an off-the-shelf visual geometry foundation model. This integration provides the model with depth-aware visual representations, improving its ability to understand the geometric structure of the scene and the spatial relationships among objects from RGB images alone. We evaluate our method on a set of spatially challenging tasks in both simulation and the real world. Extensive evaluations show that our method significantly improves the performance of state-of-the-art VLA models across diverse scenarios.", "link": "http://arxiv.org/abs/2507.00416v3", "date": "2025-11-24", "relevancy": 3.134, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6509}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evo-0%3A%20Vision-Language-Action%20Model%20with%20Implicit%20Spatial%20Understanding&body=Title%3A%20Evo-0%3A%20Vision-Language-Action%20Model%20with%20Implicit%20Spatial%20Understanding%0AAuthor%3A%20Tao%20Lin%20and%20Gen%20Li%20and%20Yilei%20Zhong%20and%20Yanwen%20Zou%20and%20Yuxin%20Du%20and%20Jiting%20Liu%20and%20Encheng%20Gu%20and%20Bo%20Zhao%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20a%20promising%20framework%20for%20enabling%20generalist%20robots%20capable%20of%20perceiving%2C%20reasoning%2C%20and%20acting%20in%20the%20real%20world.%20These%20models%20usually%20build%20upon%20pretrained%20Vision-Language%20Models%20%28VLMs%29%2C%20which%20excel%20at%20semantic%20understanding%20due%20to%20large-scale%20image%20and%20text%20pretraining.%20However%2C%20existing%20VLMs%20typically%20lack%20precise%20spatial%20understanding%20capabilities%2C%20as%20they%20are%20primarily%20tuned%20on%202D%20image-text%20pairs%20without%203D%20supervision.%20To%20address%20this%20limitation%2C%20recent%20approaches%20have%20incorporated%20explicit%203D%20inputs%20such%20as%20point%20clouds%20or%20depth%20maps%2C%20but%20this%20necessitates%20additional%20depth%20sensors%20or%20pre-trained%20depth%20estimation%20models%2C%20which%20may%20yield%20defective%20results.%20In%20contrast%2C%20our%20work%20introduces%20a%20plug-and-play%20module%20that%20implicitly%20incorporates%203D%20geometry%20features%20into%20VLA%20models%20by%20leveraging%20an%20off-the-shelf%20visual%20geometry%20foundation%20model.%20This%20integration%20provides%20the%20model%20with%20depth-aware%20visual%20representations%2C%20improving%20its%20ability%20to%20understand%20the%20geometric%20structure%20of%20the%20scene%20and%20the%20spatial%20relationships%20among%20objects%20from%20RGB%20images%20alone.%20We%20evaluate%20our%20method%20on%20a%20set%20of%20spatially%20challenging%20tasks%20in%20both%20simulation%20and%20the%20real%20world.%20Extensive%20evaluations%20show%20that%20our%20method%20significantly%20improves%20the%20performance%20of%20state-of-the-art%20VLA%20models%20across%20diverse%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2507.00416v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvo-0%253A%2520Vision-Language-Action%2520Model%2520with%2520Implicit%2520Spatial%2520Understanding%26entry.906535625%3DTao%2520Lin%2520and%2520Gen%2520Li%2520and%2520Yilei%2520Zhong%2520and%2520Yanwen%2520Zou%2520and%2520Yuxin%2520Du%2520and%2520Jiting%2520Liu%2520and%2520Encheng%2520Gu%2520and%2520Bo%2520Zhao%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520emerged%2520as%2520a%2520promising%2520framework%2520for%2520enabling%2520generalist%2520robots%2520capable%2520of%2520perceiving%252C%2520reasoning%252C%2520and%2520acting%2520in%2520the%2520real%2520world.%2520These%2520models%2520usually%2520build%2520upon%2520pretrained%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520which%2520excel%2520at%2520semantic%2520understanding%2520due%2520to%2520large-scale%2520image%2520and%2520text%2520pretraining.%2520However%252C%2520existing%2520VLMs%2520typically%2520lack%2520precise%2520spatial%2520understanding%2520capabilities%252C%2520as%2520they%2520are%2520primarily%2520tuned%2520on%25202D%2520image-text%2520pairs%2520without%25203D%2520supervision.%2520To%2520address%2520this%2520limitation%252C%2520recent%2520approaches%2520have%2520incorporated%2520explicit%25203D%2520inputs%2520such%2520as%2520point%2520clouds%2520or%2520depth%2520maps%252C%2520but%2520this%2520necessitates%2520additional%2520depth%2520sensors%2520or%2520pre-trained%2520depth%2520estimation%2520models%252C%2520which%2520may%2520yield%2520defective%2520results.%2520In%2520contrast%252C%2520our%2520work%2520introduces%2520a%2520plug-and-play%2520module%2520that%2520implicitly%2520incorporates%25203D%2520geometry%2520features%2520into%2520VLA%2520models%2520by%2520leveraging%2520an%2520off-the-shelf%2520visual%2520geometry%2520foundation%2520model.%2520This%2520integration%2520provides%2520the%2520model%2520with%2520depth-aware%2520visual%2520representations%252C%2520improving%2520its%2520ability%2520to%2520understand%2520the%2520geometric%2520structure%2520of%2520the%2520scene%2520and%2520the%2520spatial%2520relationships%2520among%2520objects%2520from%2520RGB%2520images%2520alone.%2520We%2520evaluate%2520our%2520method%2520on%2520a%2520set%2520of%2520spatially%2520challenging%2520tasks%2520in%2520both%2520simulation%2520and%2520the%2520real%2520world.%2520Extensive%2520evaluations%2520show%2520that%2520our%2520method%2520significantly%2520improves%2520the%2520performance%2520of%2520state-of-the-art%2520VLA%2520models%2520across%2520diverse%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00416v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evo-0%3A%20Vision-Language-Action%20Model%20with%20Implicit%20Spatial%20Understanding&entry.906535625=Tao%20Lin%20and%20Gen%20Li%20and%20Yilei%20Zhong%20and%20Yanwen%20Zou%20and%20Yuxin%20Du%20and%20Jiting%20Liu%20and%20Encheng%20Gu%20and%20Bo%20Zhao&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20a%20promising%20framework%20for%20enabling%20generalist%20robots%20capable%20of%20perceiving%2C%20reasoning%2C%20and%20acting%20in%20the%20real%20world.%20These%20models%20usually%20build%20upon%20pretrained%20Vision-Language%20Models%20%28VLMs%29%2C%20which%20excel%20at%20semantic%20understanding%20due%20to%20large-scale%20image%20and%20text%20pretraining.%20However%2C%20existing%20VLMs%20typically%20lack%20precise%20spatial%20understanding%20capabilities%2C%20as%20they%20are%20primarily%20tuned%20on%202D%20image-text%20pairs%20without%203D%20supervision.%20To%20address%20this%20limitation%2C%20recent%20approaches%20have%20incorporated%20explicit%203D%20inputs%20such%20as%20point%20clouds%20or%20depth%20maps%2C%20but%20this%20necessitates%20additional%20depth%20sensors%20or%20pre-trained%20depth%20estimation%20models%2C%20which%20may%20yield%20defective%20results.%20In%20contrast%2C%20our%20work%20introduces%20a%20plug-and-play%20module%20that%20implicitly%20incorporates%203D%20geometry%20features%20into%20VLA%20models%20by%20leveraging%20an%20off-the-shelf%20visual%20geometry%20foundation%20model.%20This%20integration%20provides%20the%20model%20with%20depth-aware%20visual%20representations%2C%20improving%20its%20ability%20to%20understand%20the%20geometric%20structure%20of%20the%20scene%20and%20the%20spatial%20relationships%20among%20objects%20from%20RGB%20images%20alone.%20We%20evaluate%20our%20method%20on%20a%20set%20of%20spatially%20challenging%20tasks%20in%20both%20simulation%20and%20the%20real%20world.%20Extensive%20evaluations%20show%20that%20our%20method%20significantly%20improves%20the%20performance%20of%20state-of-the-art%20VLA%20models%20across%20diverse%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2507.00416v3&entry.124074799=Read"},
{"title": "In-Video Instructions: Visual Signals as Generative Control", "author": "Gongfan Fang and Xinyin Ma and Xinchao Wang", "abstract": "Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.", "link": "http://arxiv.org/abs/2511.19401v1", "date": "2025-11-24", "relevancy": 3.1083, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6463}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6199}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Video%20Instructions%3A%20Visual%20Signals%20as%20Generative%20Control&body=Title%3A%20In-Video%20Instructions%3A%20Visual%20Signals%20as%20Generative%20Control%0AAuthor%3A%20Gongfan%20Fang%20and%20Xinyin%20Ma%20and%20Xinchao%20Wang%0AAbstract%3A%20Large-scale%20video%20generative%20models%20have%20recently%20demonstrated%20strong%20visual%20capabilities%2C%20enabling%20the%20prediction%20of%20future%20frames%20that%20adhere%20to%20the%20logical%20and%20physical%20cues%20in%20the%20current%20observation.%20In%20this%20work%2C%20we%20investigate%20whether%20such%20capabilities%20can%20be%20harnessed%20for%20controllable%20image-to-video%20generation%20by%20interpreting%20visual%20signals%20embedded%20within%20the%20frames%20as%20instructions%2C%20a%20paradigm%20we%20term%20In-Video%20Instruction.%20In%20contrast%20to%20prompt-based%20control%2C%20which%20provides%20textual%20descriptions%20that%20are%20inherently%20global%20and%20coarse%2C%20In-Video%20Instruction%20encodes%20user%20guidance%20directly%20into%20the%20visual%20domain%20through%20elements%20such%20as%20overlaid%20text%2C%20arrows%2C%20or%20trajectories.%20This%20enables%20explicit%2C%20spatial-aware%2C%20and%20unambiguous%20correspondences%20between%20visual%20subjects%20and%20their%20intended%20actions%20by%20assigning%20distinct%20instructions%20to%20different%20objects.%20Extensive%20experiments%20on%20three%20state-of-the-art%20generators%2C%20including%20Veo%203.1%2C%20Kling%202.5%2C%20and%20Wan%202.2%2C%20show%20that%20video%20models%20can%20reliably%20interpret%20and%20execute%20such%20visually%20embedded%20instructions%2C%20particularly%20in%20complex%20multi-object%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Video%2520Instructions%253A%2520Visual%2520Signals%2520as%2520Generative%2520Control%26entry.906535625%3DGongfan%2520Fang%2520and%2520Xinyin%2520Ma%2520and%2520Xinchao%2520Wang%26entry.1292438233%3DLarge-scale%2520video%2520generative%2520models%2520have%2520recently%2520demonstrated%2520strong%2520visual%2520capabilities%252C%2520enabling%2520the%2520prediction%2520of%2520future%2520frames%2520that%2520adhere%2520to%2520the%2520logical%2520and%2520physical%2520cues%2520in%2520the%2520current%2520observation.%2520In%2520this%2520work%252C%2520we%2520investigate%2520whether%2520such%2520capabilities%2520can%2520be%2520harnessed%2520for%2520controllable%2520image-to-video%2520generation%2520by%2520interpreting%2520visual%2520signals%2520embedded%2520within%2520the%2520frames%2520as%2520instructions%252C%2520a%2520paradigm%2520we%2520term%2520In-Video%2520Instruction.%2520In%2520contrast%2520to%2520prompt-based%2520control%252C%2520which%2520provides%2520textual%2520descriptions%2520that%2520are%2520inherently%2520global%2520and%2520coarse%252C%2520In-Video%2520Instruction%2520encodes%2520user%2520guidance%2520directly%2520into%2520the%2520visual%2520domain%2520through%2520elements%2520such%2520as%2520overlaid%2520text%252C%2520arrows%252C%2520or%2520trajectories.%2520This%2520enables%2520explicit%252C%2520spatial-aware%252C%2520and%2520unambiguous%2520correspondences%2520between%2520visual%2520subjects%2520and%2520their%2520intended%2520actions%2520by%2520assigning%2520distinct%2520instructions%2520to%2520different%2520objects.%2520Extensive%2520experiments%2520on%2520three%2520state-of-the-art%2520generators%252C%2520including%2520Veo%25203.1%252C%2520Kling%25202.5%252C%2520and%2520Wan%25202.2%252C%2520show%2520that%2520video%2520models%2520can%2520reliably%2520interpret%2520and%2520execute%2520such%2520visually%2520embedded%2520instructions%252C%2520particularly%2520in%2520complex%2520multi-object%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Video%20Instructions%3A%20Visual%20Signals%20as%20Generative%20Control&entry.906535625=Gongfan%20Fang%20and%20Xinyin%20Ma%20and%20Xinchao%20Wang&entry.1292438233=Large-scale%20video%20generative%20models%20have%20recently%20demonstrated%20strong%20visual%20capabilities%2C%20enabling%20the%20prediction%20of%20future%20frames%20that%20adhere%20to%20the%20logical%20and%20physical%20cues%20in%20the%20current%20observation.%20In%20this%20work%2C%20we%20investigate%20whether%20such%20capabilities%20can%20be%20harnessed%20for%20controllable%20image-to-video%20generation%20by%20interpreting%20visual%20signals%20embedded%20within%20the%20frames%20as%20instructions%2C%20a%20paradigm%20we%20term%20In-Video%20Instruction.%20In%20contrast%20to%20prompt-based%20control%2C%20which%20provides%20textual%20descriptions%20that%20are%20inherently%20global%20and%20coarse%2C%20In-Video%20Instruction%20encodes%20user%20guidance%20directly%20into%20the%20visual%20domain%20through%20elements%20such%20as%20overlaid%20text%2C%20arrows%2C%20or%20trajectories.%20This%20enables%20explicit%2C%20spatial-aware%2C%20and%20unambiguous%20correspondences%20between%20visual%20subjects%20and%20their%20intended%20actions%20by%20assigning%20distinct%20instructions%20to%20different%20objects.%20Extensive%20experiments%20on%20three%20state-of-the-art%20generators%2C%20including%20Veo%203.1%2C%20Kling%202.5%2C%20and%20Wan%202.2%2C%20show%20that%20video%20models%20can%20reliably%20interpret%20and%20execute%20such%20visually%20embedded%20instructions%2C%20particularly%20in%20complex%20multi-object%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2511.19401v1&entry.124074799=Read"},
{"title": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "author": "Jianhua Han and Meng Tian and Jiangtong Zhu and Fan He and Huixin Zhang and Sitong Guo and Dechang Zhu and Hao Tang and Pei Xu and Yuze Guo and Minzhe Niu and Haojie Zhu and Qichao Dong and Xuechao Yan and Siyuan Dong and Lu Hou and Qingqiu Huang and Xiaosong Jia and Hang Xu", "abstract": "Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.", "link": "http://arxiv.org/abs/2511.19221v1", "date": "2025-11-24", "relevancy": 3.0723, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6627}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5903}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Percept-WAM%3A%20Perception-Enhanced%20World-Awareness-Action%20Model%20for%20Robust%20End-to-End%20Autonomous%20Driving&body=Title%3A%20Percept-WAM%3A%20Perception-Enhanced%20World-Awareness-Action%20Model%20for%20Robust%20End-to-End%20Autonomous%20Driving%0AAuthor%3A%20Jianhua%20Han%20and%20Meng%20Tian%20and%20Jiangtong%20Zhu%20and%20Fan%20He%20and%20Huixin%20Zhang%20and%20Sitong%20Guo%20and%20Dechang%20Zhu%20and%20Hao%20Tang%20and%20Pei%20Xu%20and%20Yuze%20Guo%20and%20Minzhe%20Niu%20and%20Haojie%20Zhu%20and%20Qichao%20Dong%20and%20Xuechao%20Yan%20and%20Siyuan%20Dong%20and%20Lu%20Hou%20and%20Qingqiu%20Huang%20and%20Xiaosong%20Jia%20and%20Hang%20Xu%0AAbstract%3A%20Autonomous%20driving%20heavily%20relies%20on%20accurate%20and%20robust%20spatial%20perception.%20Many%20failures%20arise%20from%20inaccuracies%20and%20instability%2C%20especially%20in%20long-tail%20scenarios%20and%20complex%20interactions.%20However%2C%20current%20vision-language%20models%20are%20weak%20at%20spatial%20grounding%20and%20understanding%2C%20and%20VLA%20systems%20built%20on%20them%20therefore%20show%20limited%20perception%20and%20localization%20ability.%20To%20address%20these%20challenges%2C%20we%20introduce%20Percept-WAM%2C%20a%20perception-enhanced%20World-Awareness-Action%20Model%20that%20is%20the%20first%20to%20implicitly%20integrate%202D/3D%20scene%20understanding%20abilities%20within%20a%20single%20vision-language%20model%20%28VLM%29.%20Instead%20of%20relying%20on%20QA-style%20spatial%20reasoning%2C%20Percept-WAM%20unifies%202D/3D%20perception%20tasks%20into%20World-PV%20and%20World-BEV%20tokens%2C%20which%20encode%20both%20spatial%20coordinates%20and%20confidence.%20We%20propose%20a%20grid-conditioned%20prediction%20mechanism%20for%20dense%20object%20perception%2C%20incorporating%20IoU-aware%20scoring%20and%20parallel%20autoregressive%20decoding%2C%20improving%20stability%20in%20long-tail%2C%20far-range%2C%20and%20small-object%20scenarios.%20Additionally%2C%20Percept-WAM%20leverages%20pretrained%20VLM%20parameters%20to%20retain%20general%20intelligence%20%28e.g.%2C%20logical%20reasoning%29%20and%20can%20output%20perception%20results%20and%20trajectory%20control%20outputs%20directly.%20Experiments%20show%20that%20Percept-WAM%20matches%20or%20surpasses%20classical%20detectors%20and%20segmenters%20on%20downstream%20perception%20benchmarks%2C%20achieving%2051.7/58.9%20mAP%20on%20COCO%202D%20detection%20and%20nuScenes%20BEV%203D%20detection.%20When%20integrated%20with%20trajectory%20decoders%2C%20it%20further%20improves%20planning%20performance%20on%20nuScenes%20and%20NAVSIM%2C%20e.g.%2C%20surpassing%20DiffusionDrive%20by%202.1%20in%20PMDS%20on%20NAVSIM.%20Qualitative%20results%20further%20highlight%20its%20strong%20open-vocabulary%20and%20long-tail%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPercept-WAM%253A%2520Perception-Enhanced%2520World-Awareness-Action%2520Model%2520for%2520Robust%2520End-to-End%2520Autonomous%2520Driving%26entry.906535625%3DJianhua%2520Han%2520and%2520Meng%2520Tian%2520and%2520Jiangtong%2520Zhu%2520and%2520Fan%2520He%2520and%2520Huixin%2520Zhang%2520and%2520Sitong%2520Guo%2520and%2520Dechang%2520Zhu%2520and%2520Hao%2520Tang%2520and%2520Pei%2520Xu%2520and%2520Yuze%2520Guo%2520and%2520Minzhe%2520Niu%2520and%2520Haojie%2520Zhu%2520and%2520Qichao%2520Dong%2520and%2520Xuechao%2520Yan%2520and%2520Siyuan%2520Dong%2520and%2520Lu%2520Hou%2520and%2520Qingqiu%2520Huang%2520and%2520Xiaosong%2520Jia%2520and%2520Hang%2520Xu%26entry.1292438233%3DAutonomous%2520driving%2520heavily%2520relies%2520on%2520accurate%2520and%2520robust%2520spatial%2520perception.%2520Many%2520failures%2520arise%2520from%2520inaccuracies%2520and%2520instability%252C%2520especially%2520in%2520long-tail%2520scenarios%2520and%2520complex%2520interactions.%2520However%252C%2520current%2520vision-language%2520models%2520are%2520weak%2520at%2520spatial%2520grounding%2520and%2520understanding%252C%2520and%2520VLA%2520systems%2520built%2520on%2520them%2520therefore%2520show%2520limited%2520perception%2520and%2520localization%2520ability.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Percept-WAM%252C%2520a%2520perception-enhanced%2520World-Awareness-Action%2520Model%2520that%2520is%2520the%2520first%2520to%2520implicitly%2520integrate%25202D/3D%2520scene%2520understanding%2520abilities%2520within%2520a%2520single%2520vision-language%2520model%2520%2528VLM%2529.%2520Instead%2520of%2520relying%2520on%2520QA-style%2520spatial%2520reasoning%252C%2520Percept-WAM%2520unifies%25202D/3D%2520perception%2520tasks%2520into%2520World-PV%2520and%2520World-BEV%2520tokens%252C%2520which%2520encode%2520both%2520spatial%2520coordinates%2520and%2520confidence.%2520We%2520propose%2520a%2520grid-conditioned%2520prediction%2520mechanism%2520for%2520dense%2520object%2520perception%252C%2520incorporating%2520IoU-aware%2520scoring%2520and%2520parallel%2520autoregressive%2520decoding%252C%2520improving%2520stability%2520in%2520long-tail%252C%2520far-range%252C%2520and%2520small-object%2520scenarios.%2520Additionally%252C%2520Percept-WAM%2520leverages%2520pretrained%2520VLM%2520parameters%2520to%2520retain%2520general%2520intelligence%2520%2528e.g.%252C%2520logical%2520reasoning%2529%2520and%2520can%2520output%2520perception%2520results%2520and%2520trajectory%2520control%2520outputs%2520directly.%2520Experiments%2520show%2520that%2520Percept-WAM%2520matches%2520or%2520surpasses%2520classical%2520detectors%2520and%2520segmenters%2520on%2520downstream%2520perception%2520benchmarks%252C%2520achieving%252051.7/58.9%2520mAP%2520on%2520COCO%25202D%2520detection%2520and%2520nuScenes%2520BEV%25203D%2520detection.%2520When%2520integrated%2520with%2520trajectory%2520decoders%252C%2520it%2520further%2520improves%2520planning%2520performance%2520on%2520nuScenes%2520and%2520NAVSIM%252C%2520e.g.%252C%2520surpassing%2520DiffusionDrive%2520by%25202.1%2520in%2520PMDS%2520on%2520NAVSIM.%2520Qualitative%2520results%2520further%2520highlight%2520its%2520strong%2520open-vocabulary%2520and%2520long-tail%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Percept-WAM%3A%20Perception-Enhanced%20World-Awareness-Action%20Model%20for%20Robust%20End-to-End%20Autonomous%20Driving&entry.906535625=Jianhua%20Han%20and%20Meng%20Tian%20and%20Jiangtong%20Zhu%20and%20Fan%20He%20and%20Huixin%20Zhang%20and%20Sitong%20Guo%20and%20Dechang%20Zhu%20and%20Hao%20Tang%20and%20Pei%20Xu%20and%20Yuze%20Guo%20and%20Minzhe%20Niu%20and%20Haojie%20Zhu%20and%20Qichao%20Dong%20and%20Xuechao%20Yan%20and%20Siyuan%20Dong%20and%20Lu%20Hou%20and%20Qingqiu%20Huang%20and%20Xiaosong%20Jia%20and%20Hang%20Xu&entry.1292438233=Autonomous%20driving%20heavily%20relies%20on%20accurate%20and%20robust%20spatial%20perception.%20Many%20failures%20arise%20from%20inaccuracies%20and%20instability%2C%20especially%20in%20long-tail%20scenarios%20and%20complex%20interactions.%20However%2C%20current%20vision-language%20models%20are%20weak%20at%20spatial%20grounding%20and%20understanding%2C%20and%20VLA%20systems%20built%20on%20them%20therefore%20show%20limited%20perception%20and%20localization%20ability.%20To%20address%20these%20challenges%2C%20we%20introduce%20Percept-WAM%2C%20a%20perception-enhanced%20World-Awareness-Action%20Model%20that%20is%20the%20first%20to%20implicitly%20integrate%202D/3D%20scene%20understanding%20abilities%20within%20a%20single%20vision-language%20model%20%28VLM%29.%20Instead%20of%20relying%20on%20QA-style%20spatial%20reasoning%2C%20Percept-WAM%20unifies%202D/3D%20perception%20tasks%20into%20World-PV%20and%20World-BEV%20tokens%2C%20which%20encode%20both%20spatial%20coordinates%20and%20confidence.%20We%20propose%20a%20grid-conditioned%20prediction%20mechanism%20for%20dense%20object%20perception%2C%20incorporating%20IoU-aware%20scoring%20and%20parallel%20autoregressive%20decoding%2C%20improving%20stability%20in%20long-tail%2C%20far-range%2C%20and%20small-object%20scenarios.%20Additionally%2C%20Percept-WAM%20leverages%20pretrained%20VLM%20parameters%20to%20retain%20general%20intelligence%20%28e.g.%2C%20logical%20reasoning%29%20and%20can%20output%20perception%20results%20and%20trajectory%20control%20outputs%20directly.%20Experiments%20show%20that%20Percept-WAM%20matches%20or%20surpasses%20classical%20detectors%20and%20segmenters%20on%20downstream%20perception%20benchmarks%2C%20achieving%2051.7/58.9%20mAP%20on%20COCO%202D%20detection%20and%20nuScenes%20BEV%203D%20detection.%20When%20integrated%20with%20trajectory%20decoders%2C%20it%20further%20improves%20planning%20performance%20on%20nuScenes%20and%20NAVSIM%2C%20e.g.%2C%20surpassing%20DiffusionDrive%20by%202.1%20in%20PMDS%20on%20NAVSIM.%20Qualitative%20results%20further%20highlight%20its%20strong%20open-vocabulary%20and%20long-tail%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2511.19221v1&entry.124074799=Read"},
{"title": "Splats in Splats: Robust and Effective 3D Steganography towards Gaussian Splatting", "author": "Yijia Guo and Wenkai Huang and Yang Li and Gaolei Li and Hang Zhang and Liwen Hu and Jianhua Li and Tiejun Huang and Lei Ma", "abstract": "3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction performance with explicit scene representations. Given the widespread application of 3DGS in 3D reconstruction and generation tasks, there is an urgent need to protect the copyright of 3DGS assets. However, existing copyright protection techniques for 3DGS overlook the usability of 3D assets, posing challenges for practical deployment. Here we describe splats in splats, the first 3DGS steganography framework that embeds 3D content in 3DGS itself without modifying any attributes. To achieve this, we take a deep insight into spherical harmonics (SH) and devise an importance-graded SH coefficient encryption strategy to embed the hidden SH coefficients. Furthermore, we employ a convolutional autoencoder to establish a mapping between the original Gaussian primitives' opacity and the hidden Gaussian primitives' opacity. Extensive experiments indicate that our method significantly outperforms existing 3D steganography techniques, with 5.31% higher scene fidelity and 3x faster rendering speed, while ensuring security, robustness, and user experience.", "link": "http://arxiv.org/abs/2412.03121v2", "date": "2025-11-24", "relevancy": 3.0649, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6372}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6024}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Splats%20in%20Splats%3A%20Robust%20and%20Effective%203D%20Steganography%20towards%20Gaussian%20Splatting&body=Title%3A%20Splats%20in%20Splats%3A%20Robust%20and%20Effective%203D%20Steganography%20towards%20Gaussian%20Splatting%0AAuthor%3A%20Yijia%20Guo%20and%20Wenkai%20Huang%20and%20Yang%20Li%20and%20Gaolei%20Li%20and%20Hang%20Zhang%20and%20Liwen%20Hu%20and%20Jianhua%20Li%20and%20Tiejun%20Huang%20and%20Lei%20Ma%0AAbstract%3A%203D%20Gaussian%20splatting%20%283DGS%29%20has%20demonstrated%20impressive%203D%20reconstruction%20performance%20with%20explicit%20scene%20representations.%20Given%20the%20widespread%20application%20of%203DGS%20in%203D%20reconstruction%20and%20generation%20tasks%2C%20there%20is%20an%20urgent%20need%20to%20protect%20the%20copyright%20of%203DGS%20assets.%20However%2C%20existing%20copyright%20protection%20techniques%20for%203DGS%20overlook%20the%20usability%20of%203D%20assets%2C%20posing%20challenges%20for%20practical%20deployment.%20Here%20we%20describe%20splats%20in%20splats%2C%20the%20first%203DGS%20steganography%20framework%20that%20embeds%203D%20content%20in%203DGS%20itself%20without%20modifying%20any%20attributes.%20To%20achieve%20this%2C%20we%20take%20a%20deep%20insight%20into%20spherical%20harmonics%20%28SH%29%20and%20devise%20an%20importance-graded%20SH%20coefficient%20encryption%20strategy%20to%20embed%20the%20hidden%20SH%20coefficients.%20Furthermore%2C%20we%20employ%20a%20convolutional%20autoencoder%20to%20establish%20a%20mapping%20between%20the%20original%20Gaussian%20primitives%27%20opacity%20and%20the%20hidden%20Gaussian%20primitives%27%20opacity.%20Extensive%20experiments%20indicate%20that%20our%20method%20significantly%20outperforms%20existing%203D%20steganography%20techniques%2C%20with%205.31%25%20higher%20scene%20fidelity%20and%203x%20faster%20rendering%20speed%2C%20while%20ensuring%20security%2C%20robustness%2C%20and%20user%20experience.%0ALink%3A%20http%3A//arxiv.org/abs/2412.03121v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplats%2520in%2520Splats%253A%2520Robust%2520and%2520Effective%25203D%2520Steganography%2520towards%2520Gaussian%2520Splatting%26entry.906535625%3DYijia%2520Guo%2520and%2520Wenkai%2520Huang%2520and%2520Yang%2520Li%2520and%2520Gaolei%2520Li%2520and%2520Hang%2520Zhang%2520and%2520Liwen%2520Hu%2520and%2520Jianhua%2520Li%2520and%2520Tiejun%2520Huang%2520and%2520Lei%2520Ma%26entry.1292438233%3D3D%2520Gaussian%2520splatting%2520%25283DGS%2529%2520has%2520demonstrated%2520impressive%25203D%2520reconstruction%2520performance%2520with%2520explicit%2520scene%2520representations.%2520Given%2520the%2520widespread%2520application%2520of%25203DGS%2520in%25203D%2520reconstruction%2520and%2520generation%2520tasks%252C%2520there%2520is%2520an%2520urgent%2520need%2520to%2520protect%2520the%2520copyright%2520of%25203DGS%2520assets.%2520However%252C%2520existing%2520copyright%2520protection%2520techniques%2520for%25203DGS%2520overlook%2520the%2520usability%2520of%25203D%2520assets%252C%2520posing%2520challenges%2520for%2520practical%2520deployment.%2520Here%2520we%2520describe%2520splats%2520in%2520splats%252C%2520the%2520first%25203DGS%2520steganography%2520framework%2520that%2520embeds%25203D%2520content%2520in%25203DGS%2520itself%2520without%2520modifying%2520any%2520attributes.%2520To%2520achieve%2520this%252C%2520we%2520take%2520a%2520deep%2520insight%2520into%2520spherical%2520harmonics%2520%2528SH%2529%2520and%2520devise%2520an%2520importance-graded%2520SH%2520coefficient%2520encryption%2520strategy%2520to%2520embed%2520the%2520hidden%2520SH%2520coefficients.%2520Furthermore%252C%2520we%2520employ%2520a%2520convolutional%2520autoencoder%2520to%2520establish%2520a%2520mapping%2520between%2520the%2520original%2520Gaussian%2520primitives%2527%2520opacity%2520and%2520the%2520hidden%2520Gaussian%2520primitives%2527%2520opacity.%2520Extensive%2520experiments%2520indicate%2520that%2520our%2520method%2520significantly%2520outperforms%2520existing%25203D%2520steganography%2520techniques%252C%2520with%25205.31%2525%2520higher%2520scene%2520fidelity%2520and%25203x%2520faster%2520rendering%2520speed%252C%2520while%2520ensuring%2520security%252C%2520robustness%252C%2520and%2520user%2520experience.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03121v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Splats%20in%20Splats%3A%20Robust%20and%20Effective%203D%20Steganography%20towards%20Gaussian%20Splatting&entry.906535625=Yijia%20Guo%20and%20Wenkai%20Huang%20and%20Yang%20Li%20and%20Gaolei%20Li%20and%20Hang%20Zhang%20and%20Liwen%20Hu%20and%20Jianhua%20Li%20and%20Tiejun%20Huang%20and%20Lei%20Ma&entry.1292438233=3D%20Gaussian%20splatting%20%283DGS%29%20has%20demonstrated%20impressive%203D%20reconstruction%20performance%20with%20explicit%20scene%20representations.%20Given%20the%20widespread%20application%20of%203DGS%20in%203D%20reconstruction%20and%20generation%20tasks%2C%20there%20is%20an%20urgent%20need%20to%20protect%20the%20copyright%20of%203DGS%20assets.%20However%2C%20existing%20copyright%20protection%20techniques%20for%203DGS%20overlook%20the%20usability%20of%203D%20assets%2C%20posing%20challenges%20for%20practical%20deployment.%20Here%20we%20describe%20splats%20in%20splats%2C%20the%20first%203DGS%20steganography%20framework%20that%20embeds%203D%20content%20in%203DGS%20itself%20without%20modifying%20any%20attributes.%20To%20achieve%20this%2C%20we%20take%20a%20deep%20insight%20into%20spherical%20harmonics%20%28SH%29%20and%20devise%20an%20importance-graded%20SH%20coefficient%20encryption%20strategy%20to%20embed%20the%20hidden%20SH%20coefficients.%20Furthermore%2C%20we%20employ%20a%20convolutional%20autoencoder%20to%20establish%20a%20mapping%20between%20the%20original%20Gaussian%20primitives%27%20opacity%20and%20the%20hidden%20Gaussian%20primitives%27%20opacity.%20Extensive%20experiments%20indicate%20that%20our%20method%20significantly%20outperforms%20existing%203D%20steganography%20techniques%2C%20with%205.31%25%20higher%20scene%20fidelity%20and%203x%20faster%20rendering%20speed%2C%20while%20ensuring%20security%2C%20robustness%2C%20and%20user%20experience.&entry.1838667208=http%3A//arxiv.org/abs/2412.03121v2&entry.124074799=Read"},
{"title": "Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens", "author": "Yiming Qin and Bomin Wei and Jiaxin Ge and Konstantinos Kallidromitis and Stephanie Fu and Trevor Darrell and Xudong Wang", "abstract": "Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.", "link": "http://arxiv.org/abs/2511.19418v1", "date": "2025-11-24", "relevancy": 2.9873, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6165}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6165}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain-of-Visual-Thought%3A%20Teaching%20VLMs%20to%20See%20and%20Think%20Better%20with%20Continuous%20Visual%20Tokens&body=Title%3A%20Chain-of-Visual-Thought%3A%20Teaching%20VLMs%20to%20See%20and%20Think%20Better%20with%20Continuous%20Visual%20Tokens%0AAuthor%3A%20Yiming%20Qin%20and%20Bomin%20Wei%20and%20Jiaxin%20Ge%20and%20Konstantinos%20Kallidromitis%20and%20Stephanie%20Fu%20and%20Trevor%20Darrell%20and%20Xudong%20Wang%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20excel%20at%20reasoning%20in%20linguistic%20space%20but%20struggle%20with%20perceptual%20understanding%20that%20requires%20dense%20visual%20perception%2C%20e.g.%2C%20spatial%20reasoning%20and%20geometric%20awareness.%20This%20limitation%20stems%20from%20the%20fact%20that%20current%20VLMs%20have%20limited%20mechanisms%20to%20capture%20dense%20visual%20information%20across%20spatial%20dimensions.%20We%20introduce%20Chain-of-Visual-Thought%20%28COVT%29%2C%20a%20framework%20that%20enables%20VLMs%20to%20reason%20not%20only%20in%20words%20but%20also%20through%20continuous%20visual%20tokens-compact%20latent%20representations%20that%20encode%20rich%20perceptual%20cues.%20Within%20a%20small%20budget%20of%20roughly%2020%20tokens%2C%20COVT%20distills%20knowledge%20from%20lightweight%20vision%20experts%2C%20capturing%20complementary%20properties%20such%20as%202D%20appearance%2C%203D%20geometry%2C%20spatial%20layout%2C%20and%20edge%20structure.%20During%20training%2C%20the%20VLM%20with%20COVT%20autoregressively%20predicts%20these%20visual%20tokens%20to%20reconstruct%20dense%20supervision%20signals%20%28e.g.%2C%20depth%2C%20segmentation%2C%20edges%2C%20and%20DINO%20features%29.%20At%20inference%2C%20the%20model%20reasons%20directly%20in%20the%20continuous%20visual%20token%20space%2C%20preserving%20efficiency%20while%20optionally%20decoding%20dense%20predictions%20for%20interpretability.%20Evaluated%20across%20more%20than%20ten%20diverse%20perception%20benchmarks%2C%20including%20CV-Bench%2C%20MMVP%2C%20RealWorldQA%2C%20MMStar%2C%20WorldMedQA%2C%20and%20HRBench%2C%20integrating%20COVT%20into%20strong%20VLMs%20such%20as%20Qwen2.5-VL%20and%20LLaVA%20consistently%20improves%20performance%20by%203%25%20to%2016%25%20and%20demonstrates%20that%20compact%20continuous%20visual%20thinking%20enables%20more%20precise%2C%20grounded%2C%20and%20interpretable%20multimodal%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain-of-Visual-Thought%253A%2520Teaching%2520VLMs%2520to%2520See%2520and%2520Think%2520Better%2520with%2520Continuous%2520Visual%2520Tokens%26entry.906535625%3DYiming%2520Qin%2520and%2520Bomin%2520Wei%2520and%2520Jiaxin%2520Ge%2520and%2520Konstantinos%2520Kallidromitis%2520and%2520Stephanie%2520Fu%2520and%2520Trevor%2520Darrell%2520and%2520Xudong%2520Wang%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520excel%2520at%2520reasoning%2520in%2520linguistic%2520space%2520but%2520struggle%2520with%2520perceptual%2520understanding%2520that%2520requires%2520dense%2520visual%2520perception%252C%2520e.g.%252C%2520spatial%2520reasoning%2520and%2520geometric%2520awareness.%2520This%2520limitation%2520stems%2520from%2520the%2520fact%2520that%2520current%2520VLMs%2520have%2520limited%2520mechanisms%2520to%2520capture%2520dense%2520visual%2520information%2520across%2520spatial%2520dimensions.%2520We%2520introduce%2520Chain-of-Visual-Thought%2520%2528COVT%2529%252C%2520a%2520framework%2520that%2520enables%2520VLMs%2520to%2520reason%2520not%2520only%2520in%2520words%2520but%2520also%2520through%2520continuous%2520visual%2520tokens-compact%2520latent%2520representations%2520that%2520encode%2520rich%2520perceptual%2520cues.%2520Within%2520a%2520small%2520budget%2520of%2520roughly%252020%2520tokens%252C%2520COVT%2520distills%2520knowledge%2520from%2520lightweight%2520vision%2520experts%252C%2520capturing%2520complementary%2520properties%2520such%2520as%25202D%2520appearance%252C%25203D%2520geometry%252C%2520spatial%2520layout%252C%2520and%2520edge%2520structure.%2520During%2520training%252C%2520the%2520VLM%2520with%2520COVT%2520autoregressively%2520predicts%2520these%2520visual%2520tokens%2520to%2520reconstruct%2520dense%2520supervision%2520signals%2520%2528e.g.%252C%2520depth%252C%2520segmentation%252C%2520edges%252C%2520and%2520DINO%2520features%2529.%2520At%2520inference%252C%2520the%2520model%2520reasons%2520directly%2520in%2520the%2520continuous%2520visual%2520token%2520space%252C%2520preserving%2520efficiency%2520while%2520optionally%2520decoding%2520dense%2520predictions%2520for%2520interpretability.%2520Evaluated%2520across%2520more%2520than%2520ten%2520diverse%2520perception%2520benchmarks%252C%2520including%2520CV-Bench%252C%2520MMVP%252C%2520RealWorldQA%252C%2520MMStar%252C%2520WorldMedQA%252C%2520and%2520HRBench%252C%2520integrating%2520COVT%2520into%2520strong%2520VLMs%2520such%2520as%2520Qwen2.5-VL%2520and%2520LLaVA%2520consistently%2520improves%2520performance%2520by%25203%2525%2520to%252016%2525%2520and%2520demonstrates%2520that%2520compact%2520continuous%2520visual%2520thinking%2520enables%2520more%2520precise%252C%2520grounded%252C%2520and%2520interpretable%2520multimodal%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain-of-Visual-Thought%3A%20Teaching%20VLMs%20to%20See%20and%20Think%20Better%20with%20Continuous%20Visual%20Tokens&entry.906535625=Yiming%20Qin%20and%20Bomin%20Wei%20and%20Jiaxin%20Ge%20and%20Konstantinos%20Kallidromitis%20and%20Stephanie%20Fu%20and%20Trevor%20Darrell%20and%20Xudong%20Wang&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20excel%20at%20reasoning%20in%20linguistic%20space%20but%20struggle%20with%20perceptual%20understanding%20that%20requires%20dense%20visual%20perception%2C%20e.g.%2C%20spatial%20reasoning%20and%20geometric%20awareness.%20This%20limitation%20stems%20from%20the%20fact%20that%20current%20VLMs%20have%20limited%20mechanisms%20to%20capture%20dense%20visual%20information%20across%20spatial%20dimensions.%20We%20introduce%20Chain-of-Visual-Thought%20%28COVT%29%2C%20a%20framework%20that%20enables%20VLMs%20to%20reason%20not%20only%20in%20words%20but%20also%20through%20continuous%20visual%20tokens-compact%20latent%20representations%20that%20encode%20rich%20perceptual%20cues.%20Within%20a%20small%20budget%20of%20roughly%2020%20tokens%2C%20COVT%20distills%20knowledge%20from%20lightweight%20vision%20experts%2C%20capturing%20complementary%20properties%20such%20as%202D%20appearance%2C%203D%20geometry%2C%20spatial%20layout%2C%20and%20edge%20structure.%20During%20training%2C%20the%20VLM%20with%20COVT%20autoregressively%20predicts%20these%20visual%20tokens%20to%20reconstruct%20dense%20supervision%20signals%20%28e.g.%2C%20depth%2C%20segmentation%2C%20edges%2C%20and%20DINO%20features%29.%20At%20inference%2C%20the%20model%20reasons%20directly%20in%20the%20continuous%20visual%20token%20space%2C%20preserving%20efficiency%20while%20optionally%20decoding%20dense%20predictions%20for%20interpretability.%20Evaluated%20across%20more%20than%20ten%20diverse%20perception%20benchmarks%2C%20including%20CV-Bench%2C%20MMVP%2C%20RealWorldQA%2C%20MMStar%2C%20WorldMedQA%2C%20and%20HRBench%2C%20integrating%20COVT%20into%20strong%20VLMs%20such%20as%20Qwen2.5-VL%20and%20LLaVA%20consistently%20improves%20performance%20by%203%25%20to%2016%25%20and%20demonstrates%20that%20compact%20continuous%20visual%20thinking%20enables%20more%20precise%2C%20grounded%2C%20and%20interpretable%20multimodal%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2511.19418v1&entry.124074799=Read"},
{"title": "Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?", "author": "Itay Cohen and Ethan Fetaya and Amir Rosenfeld", "abstract": "Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired \"real\"/\"lookalike\" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.", "link": "http://arxiv.org/abs/2511.19200v1", "date": "2025-11-24", "relevancy": 2.9838, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6118}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6118}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Modern%20Vision%20Models%20Understand%20the%20Difference%20Between%20an%20Object%20and%20a%20Look-alike%3F&body=Title%3A%20Can%20Modern%20Vision%20Models%20Understand%20the%20Difference%20Between%20an%20Object%20and%20a%20Look-alike%3F%0AAuthor%3A%20Itay%20Cohen%20and%20Ethan%20Fetaya%20and%20Amir%20Rosenfeld%0AAbstract%3A%20Recent%20advances%20in%20computer%20vision%20have%20yielded%20models%20with%20strong%20performance%20on%20recognition%20benchmarks%3B%20however%2C%20significant%20gaps%20remain%20in%20comparison%20to%20human%20perception.%20One%20subtle%20ability%20is%20to%20judge%20whether%20an%20image%20looks%20like%20a%20given%20object%20without%20being%20an%20instance%20of%20that%20object.%20We%20study%20whether%20vision-language%20models%20such%20as%20CLIP%20capture%20this%20distinction.%20We%20curated%20a%20dataset%20named%20RoLA%20%28Real%20or%20Lookalike%29%20of%20real%20and%20lookalike%20exemplars%20%28e.g.%2C%20toys%2C%20statues%2C%20drawings%2C%20pareidolia%29%20across%20multiple%20categories%2C%20and%20first%20evaluate%20a%20prompt-based%20baseline%20with%20paired%20%22real%22/%22lookalike%22%20prompts.%20We%20then%20estimate%20a%20direction%20in%20CLIP%27s%20embedding%20space%20that%20moves%20representations%20between%20real%20and%20lookalike.%20Applying%20this%20direction%20to%20image%20and%20text%20embeddings%20improves%20discrimination%20in%20cross-modal%20retrieval%20on%20Conceptual12M%2C%20and%20also%20enhances%20captions%20produced%20by%20a%20CLIP%20prefix%20captioner.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Modern%2520Vision%2520Models%2520Understand%2520the%2520Difference%2520Between%2520an%2520Object%2520and%2520a%2520Look-alike%253F%26entry.906535625%3DItay%2520Cohen%2520and%2520Ethan%2520Fetaya%2520and%2520Amir%2520Rosenfeld%26entry.1292438233%3DRecent%2520advances%2520in%2520computer%2520vision%2520have%2520yielded%2520models%2520with%2520strong%2520performance%2520on%2520recognition%2520benchmarks%253B%2520however%252C%2520significant%2520gaps%2520remain%2520in%2520comparison%2520to%2520human%2520perception.%2520One%2520subtle%2520ability%2520is%2520to%2520judge%2520whether%2520an%2520image%2520looks%2520like%2520a%2520given%2520object%2520without%2520being%2520an%2520instance%2520of%2520that%2520object.%2520We%2520study%2520whether%2520vision-language%2520models%2520such%2520as%2520CLIP%2520capture%2520this%2520distinction.%2520We%2520curated%2520a%2520dataset%2520named%2520RoLA%2520%2528Real%2520or%2520Lookalike%2529%2520of%2520real%2520and%2520lookalike%2520exemplars%2520%2528e.g.%252C%2520toys%252C%2520statues%252C%2520drawings%252C%2520pareidolia%2529%2520across%2520multiple%2520categories%252C%2520and%2520first%2520evaluate%2520a%2520prompt-based%2520baseline%2520with%2520paired%2520%2522real%2522/%2522lookalike%2522%2520prompts.%2520We%2520then%2520estimate%2520a%2520direction%2520in%2520CLIP%2527s%2520embedding%2520space%2520that%2520moves%2520representations%2520between%2520real%2520and%2520lookalike.%2520Applying%2520this%2520direction%2520to%2520image%2520and%2520text%2520embeddings%2520improves%2520discrimination%2520in%2520cross-modal%2520retrieval%2520on%2520Conceptual12M%252C%2520and%2520also%2520enhances%2520captions%2520produced%2520by%2520a%2520CLIP%2520prefix%2520captioner.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Modern%20Vision%20Models%20Understand%20the%20Difference%20Between%20an%20Object%20and%20a%20Look-alike%3F&entry.906535625=Itay%20Cohen%20and%20Ethan%20Fetaya%20and%20Amir%20Rosenfeld&entry.1292438233=Recent%20advances%20in%20computer%20vision%20have%20yielded%20models%20with%20strong%20performance%20on%20recognition%20benchmarks%3B%20however%2C%20significant%20gaps%20remain%20in%20comparison%20to%20human%20perception.%20One%20subtle%20ability%20is%20to%20judge%20whether%20an%20image%20looks%20like%20a%20given%20object%20without%20being%20an%20instance%20of%20that%20object.%20We%20study%20whether%20vision-language%20models%20such%20as%20CLIP%20capture%20this%20distinction.%20We%20curated%20a%20dataset%20named%20RoLA%20%28Real%20or%20Lookalike%29%20of%20real%20and%20lookalike%20exemplars%20%28e.g.%2C%20toys%2C%20statues%2C%20drawings%2C%20pareidolia%29%20across%20multiple%20categories%2C%20and%20first%20evaluate%20a%20prompt-based%20baseline%20with%20paired%20%22real%22/%22lookalike%22%20prompts.%20We%20then%20estimate%20a%20direction%20in%20CLIP%27s%20embedding%20space%20that%20moves%20representations%20between%20real%20and%20lookalike.%20Applying%20this%20direction%20to%20image%20and%20text%20embeddings%20improves%20discrimination%20in%20cross-modal%20retrieval%20on%20Conceptual12M%2C%20and%20also%20enhances%20captions%20produced%20by%20a%20CLIP%20prefix%20captioner.&entry.1838667208=http%3A//arxiv.org/abs/2511.19200v1&entry.124074799=Read"},
{"title": "Optimization-Free Style Transfer for 3D Gaussian Splats", "author": "Raphael Du Sablon and David Hart", "abstract": "The task of style transfer for 3D Gaussian splats has been explored in many previous works, but these require reconstructing or fine-tuning the splat while incorporating style information or optimizing a feature extraction network on the splat representation. We propose a reconstruction- and optimization-free approach to stylizing 3D Gaussian splats, allowing for direct stylization on a .ply or .splat file without requiring the original camera views. This is done by generating a graph structure across the implicit surface of the splat representation. A feed-forward, surface-based stylization method is then used and interpolated back to the individual splats in the scene. This also allows for fast stylization of splats with no additional training, achieving speeds under 2 minutes even on CPU-based consumer hardware. We demonstrate the quality results this approach achieves and compare to other 3D Gaussian splat style transfer methods. Code is publicly available at https://github.com/davidmhart/FastSplatStyler.", "link": "http://arxiv.org/abs/2508.05813v2", "date": "2025-11-24", "relevancy": 2.9651, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6343}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.612}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimization-Free%20Style%20Transfer%20for%203D%20Gaussian%20Splats&body=Title%3A%20Optimization-Free%20Style%20Transfer%20for%203D%20Gaussian%20Splats%0AAuthor%3A%20Raphael%20Du%20Sablon%20and%20David%20Hart%0AAbstract%3A%20The%20task%20of%20style%20transfer%20for%203D%20Gaussian%20splats%20has%20been%20explored%20in%20many%20previous%20works%2C%20but%20these%20require%20reconstructing%20or%20fine-tuning%20the%20splat%20while%20incorporating%20style%20information%20or%20optimizing%20a%20feature%20extraction%20network%20on%20the%20splat%20representation.%20We%20propose%20a%20reconstruction-%20and%20optimization-free%20approach%20to%20stylizing%203D%20Gaussian%20splats%2C%20allowing%20for%20direct%20stylization%20on%20a%20.ply%20or%20.splat%20file%20without%20requiring%20the%20original%20camera%20views.%20This%20is%20done%20by%20generating%20a%20graph%20structure%20across%20the%20implicit%20surface%20of%20the%20splat%20representation.%20A%20feed-forward%2C%20surface-based%20stylization%20method%20is%20then%20used%20and%20interpolated%20back%20to%20the%20individual%20splats%20in%20the%20scene.%20This%20also%20allows%20for%20fast%20stylization%20of%20splats%20with%20no%20additional%20training%2C%20achieving%20speeds%20under%202%20minutes%20even%20on%20CPU-based%20consumer%20hardware.%20We%20demonstrate%20the%20quality%20results%20this%20approach%20achieves%20and%20compare%20to%20other%203D%20Gaussian%20splat%20style%20transfer%20methods.%20Code%20is%20publicly%20available%20at%20https%3A//github.com/davidmhart/FastSplatStyler.%0ALink%3A%20http%3A//arxiv.org/abs/2508.05813v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimization-Free%2520Style%2520Transfer%2520for%25203D%2520Gaussian%2520Splats%26entry.906535625%3DRaphael%2520Du%2520Sablon%2520and%2520David%2520Hart%26entry.1292438233%3DThe%2520task%2520of%2520style%2520transfer%2520for%25203D%2520Gaussian%2520splats%2520has%2520been%2520explored%2520in%2520many%2520previous%2520works%252C%2520but%2520these%2520require%2520reconstructing%2520or%2520fine-tuning%2520the%2520splat%2520while%2520incorporating%2520style%2520information%2520or%2520optimizing%2520a%2520feature%2520extraction%2520network%2520on%2520the%2520splat%2520representation.%2520We%2520propose%2520a%2520reconstruction-%2520and%2520optimization-free%2520approach%2520to%2520stylizing%25203D%2520Gaussian%2520splats%252C%2520allowing%2520for%2520direct%2520stylization%2520on%2520a%2520.ply%2520or%2520.splat%2520file%2520without%2520requiring%2520the%2520original%2520camera%2520views.%2520This%2520is%2520done%2520by%2520generating%2520a%2520graph%2520structure%2520across%2520the%2520implicit%2520surface%2520of%2520the%2520splat%2520representation.%2520A%2520feed-forward%252C%2520surface-based%2520stylization%2520method%2520is%2520then%2520used%2520and%2520interpolated%2520back%2520to%2520the%2520individual%2520splats%2520in%2520the%2520scene.%2520This%2520also%2520allows%2520for%2520fast%2520stylization%2520of%2520splats%2520with%2520no%2520additional%2520training%252C%2520achieving%2520speeds%2520under%25202%2520minutes%2520even%2520on%2520CPU-based%2520consumer%2520hardware.%2520We%2520demonstrate%2520the%2520quality%2520results%2520this%2520approach%2520achieves%2520and%2520compare%2520to%2520other%25203D%2520Gaussian%2520splat%2520style%2520transfer%2520methods.%2520Code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/davidmhart/FastSplatStyler.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05813v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimization-Free%20Style%20Transfer%20for%203D%20Gaussian%20Splats&entry.906535625=Raphael%20Du%20Sablon%20and%20David%20Hart&entry.1292438233=The%20task%20of%20style%20transfer%20for%203D%20Gaussian%20splats%20has%20been%20explored%20in%20many%20previous%20works%2C%20but%20these%20require%20reconstructing%20or%20fine-tuning%20the%20splat%20while%20incorporating%20style%20information%20or%20optimizing%20a%20feature%20extraction%20network%20on%20the%20splat%20representation.%20We%20propose%20a%20reconstruction-%20and%20optimization-free%20approach%20to%20stylizing%203D%20Gaussian%20splats%2C%20allowing%20for%20direct%20stylization%20on%20a%20.ply%20or%20.splat%20file%20without%20requiring%20the%20original%20camera%20views.%20This%20is%20done%20by%20generating%20a%20graph%20structure%20across%20the%20implicit%20surface%20of%20the%20splat%20representation.%20A%20feed-forward%2C%20surface-based%20stylization%20method%20is%20then%20used%20and%20interpolated%20back%20to%20the%20individual%20splats%20in%20the%20scene.%20This%20also%20allows%20for%20fast%20stylization%20of%20splats%20with%20no%20additional%20training%2C%20achieving%20speeds%20under%202%20minutes%20even%20on%20CPU-based%20consumer%20hardware.%20We%20demonstrate%20the%20quality%20results%20this%20approach%20achieves%20and%20compare%20to%20other%203D%20Gaussian%20splat%20style%20transfer%20methods.%20Code%20is%20publicly%20available%20at%20https%3A//github.com/davidmhart/FastSplatStyler.&entry.1838667208=http%3A//arxiv.org/abs/2508.05813v2&entry.124074799=Read"},
{"title": "Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration", "author": "James Y. Huang and Sheng Zhang and Qianchu Liu and Guanghui Qin and Tinghui Zhu and Tristan Naumann and Muhao Chen and Hoifung Poon", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.", "link": "http://arxiv.org/abs/2511.19417v1", "date": "2025-11-24", "relevancy": 2.9181, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5862}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Be%20My%20Eyes%3A%20Extending%20Large%20Language%20Models%20to%20New%20Modalities%20Through%20Multi-Agent%20Collaboration&body=Title%3A%20Be%20My%20Eyes%3A%20Extending%20Large%20Language%20Models%20to%20New%20Modalities%20Through%20Multi-Agent%20Collaboration%0AAuthor%3A%20James%20Y.%20Huang%20and%20Sheng%20Zhang%20and%20Qianchu%20Liu%20and%20Guanghui%20Qin%20and%20Tinghui%20Zhu%20and%20Tristan%20Naumann%20and%20Muhao%20Chen%20and%20Hoifung%20Poon%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20challenging%2C%20knowledge-intensive%20reasoning%20tasks.%20However%2C%20extending%20LLMs%20to%20perceive%20and%20reason%20over%20a%20new%20modality%20%28e.g.%2C%20vision%29%2C%20often%20requires%20costly%20development%20of%20large-scale%20vision%20language%20models%20%28VLMs%29%20with%20LLMs%20as%20backbones.%20Smaller%20VLMs%20are%20more%20efficient%20and%20adaptable%20but%20often%20lack%20the%20broad%20knowledge%20and%20reasoning%20capabilities%20of%20frontier%20LLMs.%20In%20this%20work%2C%20we%20propose%20BeMyEyes%2C%20a%20modular%2C%20multi-agent%20framework%20for%20extending%20LLMs%20to%20multimodal%20reasoning%20by%20orchestrating%20collaboration%20between%20efficient%2C%20adaptable%20VLMs%20as%20perceivers%20and%20powerful%20LLMs%20as%20reasoners%20through%20conversations.%20We%20then%20introduce%20a%20data%20synthesis%20and%20supervised%20fine-tuning%20pipeline%20to%20train%20the%20perceiver%20agent%20to%20effectively%20collaborate%20with%20the%20reasoner%20agent.%20By%20combining%20the%20complementary%20strengths%20of%20perception%20and%20reasoning%20agents%2C%20BeMyEyes%20avoids%20the%20need%20for%20training%20large-scale%20multimodal%20models%2C%20preserves%20the%20generalization%20and%20reasoning%20capabilities%20of%20LLMs%2C%20and%20allows%20flexible%20extension%20to%20new%20domains%20and%20modalities.%20Experiments%20show%20that%20our%20framework%20unlocks%20the%20multimodal%20reasoning%20capabilities%20for%20LLMs%2C%20enabling%20a%20lightweight%20and%20fully%20open-source%20solution%2C%20i.e.%20equipping%20text-only%20DeepSeek-R1%20with%20Qwen2.5-VL-7B%20perceiver%2C%20to%20outperform%20large-scale%20proprietary%20VLMs%20such%20as%20GPT-4o%20on%20a%20wide%20range%20of%20knowledge-intensive%20multimodal%20tasks.%20These%20results%20demonstrate%20the%20effectiveness%2C%20modularity%2C%20and%20scalability%20of%20our%20multi-agent%20approach%20for%20building%20future%20multimodal%20reasoning%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBe%2520My%2520Eyes%253A%2520Extending%2520Large%2520Language%2520Models%2520to%2520New%2520Modalities%2520Through%2520Multi-Agent%2520Collaboration%26entry.906535625%3DJames%2520Y.%2520Huang%2520and%2520Sheng%2520Zhang%2520and%2520Qianchu%2520Liu%2520and%2520Guanghui%2520Qin%2520and%2520Tinghui%2520Zhu%2520and%2520Tristan%2520Naumann%2520and%2520Muhao%2520Chen%2520and%2520Hoifung%2520Poon%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%2520challenging%252C%2520knowledge-intensive%2520reasoning%2520tasks.%2520However%252C%2520extending%2520LLMs%2520to%2520perceive%2520and%2520reason%2520over%2520a%2520new%2520modality%2520%2528e.g.%252C%2520vision%2529%252C%2520often%2520requires%2520costly%2520development%2520of%2520large-scale%2520vision%2520language%2520models%2520%2528VLMs%2529%2520with%2520LLMs%2520as%2520backbones.%2520Smaller%2520VLMs%2520are%2520more%2520efficient%2520and%2520adaptable%2520but%2520often%2520lack%2520the%2520broad%2520knowledge%2520and%2520reasoning%2520capabilities%2520of%2520frontier%2520LLMs.%2520In%2520this%2520work%252C%2520we%2520propose%2520BeMyEyes%252C%2520a%2520modular%252C%2520multi-agent%2520framework%2520for%2520extending%2520LLMs%2520to%2520multimodal%2520reasoning%2520by%2520orchestrating%2520collaboration%2520between%2520efficient%252C%2520adaptable%2520VLMs%2520as%2520perceivers%2520and%2520powerful%2520LLMs%2520as%2520reasoners%2520through%2520conversations.%2520We%2520then%2520introduce%2520a%2520data%2520synthesis%2520and%2520supervised%2520fine-tuning%2520pipeline%2520to%2520train%2520the%2520perceiver%2520agent%2520to%2520effectively%2520collaborate%2520with%2520the%2520reasoner%2520agent.%2520By%2520combining%2520the%2520complementary%2520strengths%2520of%2520perception%2520and%2520reasoning%2520agents%252C%2520BeMyEyes%2520avoids%2520the%2520need%2520for%2520training%2520large-scale%2520multimodal%2520models%252C%2520preserves%2520the%2520generalization%2520and%2520reasoning%2520capabilities%2520of%2520LLMs%252C%2520and%2520allows%2520flexible%2520extension%2520to%2520new%2520domains%2520and%2520modalities.%2520Experiments%2520show%2520that%2520our%2520framework%2520unlocks%2520the%2520multimodal%2520reasoning%2520capabilities%2520for%2520LLMs%252C%2520enabling%2520a%2520lightweight%2520and%2520fully%2520open-source%2520solution%252C%2520i.e.%2520equipping%2520text-only%2520DeepSeek-R1%2520with%2520Qwen2.5-VL-7B%2520perceiver%252C%2520to%2520outperform%2520large-scale%2520proprietary%2520VLMs%2520such%2520as%2520GPT-4o%2520on%2520a%2520wide%2520range%2520of%2520knowledge-intensive%2520multimodal%2520tasks.%2520These%2520results%2520demonstrate%2520the%2520effectiveness%252C%2520modularity%252C%2520and%2520scalability%2520of%2520our%2520multi-agent%2520approach%2520for%2520building%2520future%2520multimodal%2520reasoning%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Be%20My%20Eyes%3A%20Extending%20Large%20Language%20Models%20to%20New%20Modalities%20Through%20Multi-Agent%20Collaboration&entry.906535625=James%20Y.%20Huang%20and%20Sheng%20Zhang%20and%20Qianchu%20Liu%20and%20Guanghui%20Qin%20and%20Tinghui%20Zhu%20and%20Tristan%20Naumann%20and%20Muhao%20Chen%20and%20Hoifung%20Poon&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20challenging%2C%20knowledge-intensive%20reasoning%20tasks.%20However%2C%20extending%20LLMs%20to%20perceive%20and%20reason%20over%20a%20new%20modality%20%28e.g.%2C%20vision%29%2C%20often%20requires%20costly%20development%20of%20large-scale%20vision%20language%20models%20%28VLMs%29%20with%20LLMs%20as%20backbones.%20Smaller%20VLMs%20are%20more%20efficient%20and%20adaptable%20but%20often%20lack%20the%20broad%20knowledge%20and%20reasoning%20capabilities%20of%20frontier%20LLMs.%20In%20this%20work%2C%20we%20propose%20BeMyEyes%2C%20a%20modular%2C%20multi-agent%20framework%20for%20extending%20LLMs%20to%20multimodal%20reasoning%20by%20orchestrating%20collaboration%20between%20efficient%2C%20adaptable%20VLMs%20as%20perceivers%20and%20powerful%20LLMs%20as%20reasoners%20through%20conversations.%20We%20then%20introduce%20a%20data%20synthesis%20and%20supervised%20fine-tuning%20pipeline%20to%20train%20the%20perceiver%20agent%20to%20effectively%20collaborate%20with%20the%20reasoner%20agent.%20By%20combining%20the%20complementary%20strengths%20of%20perception%20and%20reasoning%20agents%2C%20BeMyEyes%20avoids%20the%20need%20for%20training%20large-scale%20multimodal%20models%2C%20preserves%20the%20generalization%20and%20reasoning%20capabilities%20of%20LLMs%2C%20and%20allows%20flexible%20extension%20to%20new%20domains%20and%20modalities.%20Experiments%20show%20that%20our%20framework%20unlocks%20the%20multimodal%20reasoning%20capabilities%20for%20LLMs%2C%20enabling%20a%20lightweight%20and%20fully%20open-source%20solution%2C%20i.e.%20equipping%20text-only%20DeepSeek-R1%20with%20Qwen2.5-VL-7B%20perceiver%2C%20to%20outperform%20large-scale%20proprietary%20VLMs%20such%20as%20GPT-4o%20on%20a%20wide%20range%20of%20knowledge-intensive%20multimodal%20tasks.%20These%20results%20demonstrate%20the%20effectiveness%2C%20modularity%2C%20and%20scalability%20of%20our%20multi-agent%20approach%20for%20building%20future%20multimodal%20reasoning%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.19417v1&entry.124074799=Read"},
{"title": "Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction", "author": "Yun Zhou and Yaoting Wang and Guangquan Jie and Jinyu Liu and Henghui Ding", "abstract": "SAM3D has garnered widespread attention for its strong 3D object reconstruction capabilities. However, a key limitation remains: SAM3D cannot reconstruct specific objects referred to by textual descriptions, a capability that is essential for practical applications such as 3D editing, game development, and virtual environments. To address this gap, we introduce Ref-SAM3D, a simple yet effective extension to SAM3D that incorporates textual descriptions as a high-level prior, enabling text-guided 3D reconstruction from a single RGB image. Through extensive qualitative experiments, we show that Ref-SAM3D, guided only by natural language and a single 2D view, delivers competitive and high-fidelity zero-shot reconstruction performance. Our results demonstrate that Ref-SAM3D effectively bridges the gap between 2D visual cues and 3D geometric understanding, offering a more flexible and accessible paradigm for reference-guided 3D reconstruction. Code is available at: https://github.com/FudanCVL/Ref-SAM3D.", "link": "http://arxiv.org/abs/2511.19426v1", "date": "2025-11-24", "relevancy": 2.9128, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5845}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5845}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ref-SAM3D%3A%20Bridging%20SAM3D%20with%20Text%20for%20Reference%203D%20Reconstruction&body=Title%3A%20Ref-SAM3D%3A%20Bridging%20SAM3D%20with%20Text%20for%20Reference%203D%20Reconstruction%0AAuthor%3A%20Yun%20Zhou%20and%20Yaoting%20Wang%20and%20Guangquan%20Jie%20and%20Jinyu%20Liu%20and%20Henghui%20Ding%0AAbstract%3A%20SAM3D%20has%20garnered%20widespread%20attention%20for%20its%20strong%203D%20object%20reconstruction%20capabilities.%20However%2C%20a%20key%20limitation%20remains%3A%20SAM3D%20cannot%20reconstruct%20specific%20objects%20referred%20to%20by%20textual%20descriptions%2C%20a%20capability%20that%20is%20essential%20for%20practical%20applications%20such%20as%203D%20editing%2C%20game%20development%2C%20and%20virtual%20environments.%20To%20address%20this%20gap%2C%20we%20introduce%20Ref-SAM3D%2C%20a%20simple%20yet%20effective%20extension%20to%20SAM3D%20that%20incorporates%20textual%20descriptions%20as%20a%20high-level%20prior%2C%20enabling%20text-guided%203D%20reconstruction%20from%20a%20single%20RGB%20image.%20Through%20extensive%20qualitative%20experiments%2C%20we%20show%20that%20Ref-SAM3D%2C%20guided%20only%20by%20natural%20language%20and%20a%20single%202D%20view%2C%20delivers%20competitive%20and%20high-fidelity%20zero-shot%20reconstruction%20performance.%20Our%20results%20demonstrate%20that%20Ref-SAM3D%20effectively%20bridges%20the%20gap%20between%202D%20visual%20cues%20and%203D%20geometric%20understanding%2C%20offering%20a%20more%20flexible%20and%20accessible%20paradigm%20for%20reference-guided%203D%20reconstruction.%20Code%20is%20available%20at%3A%20https%3A//github.com/FudanCVL/Ref-SAM3D.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRef-SAM3D%253A%2520Bridging%2520SAM3D%2520with%2520Text%2520for%2520Reference%25203D%2520Reconstruction%26entry.906535625%3DYun%2520Zhou%2520and%2520Yaoting%2520Wang%2520and%2520Guangquan%2520Jie%2520and%2520Jinyu%2520Liu%2520and%2520Henghui%2520Ding%26entry.1292438233%3DSAM3D%2520has%2520garnered%2520widespread%2520attention%2520for%2520its%2520strong%25203D%2520object%2520reconstruction%2520capabilities.%2520However%252C%2520a%2520key%2520limitation%2520remains%253A%2520SAM3D%2520cannot%2520reconstruct%2520specific%2520objects%2520referred%2520to%2520by%2520textual%2520descriptions%252C%2520a%2520capability%2520that%2520is%2520essential%2520for%2520practical%2520applications%2520such%2520as%25203D%2520editing%252C%2520game%2520development%252C%2520and%2520virtual%2520environments.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520Ref-SAM3D%252C%2520a%2520simple%2520yet%2520effective%2520extension%2520to%2520SAM3D%2520that%2520incorporates%2520textual%2520descriptions%2520as%2520a%2520high-level%2520prior%252C%2520enabling%2520text-guided%25203D%2520reconstruction%2520from%2520a%2520single%2520RGB%2520image.%2520Through%2520extensive%2520qualitative%2520experiments%252C%2520we%2520show%2520that%2520Ref-SAM3D%252C%2520guided%2520only%2520by%2520natural%2520language%2520and%2520a%2520single%25202D%2520view%252C%2520delivers%2520competitive%2520and%2520high-fidelity%2520zero-shot%2520reconstruction%2520performance.%2520Our%2520results%2520demonstrate%2520that%2520Ref-SAM3D%2520effectively%2520bridges%2520the%2520gap%2520between%25202D%2520visual%2520cues%2520and%25203D%2520geometric%2520understanding%252C%2520offering%2520a%2520more%2520flexible%2520and%2520accessible%2520paradigm%2520for%2520reference-guided%25203D%2520reconstruction.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/FudanCVL/Ref-SAM3D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ref-SAM3D%3A%20Bridging%20SAM3D%20with%20Text%20for%20Reference%203D%20Reconstruction&entry.906535625=Yun%20Zhou%20and%20Yaoting%20Wang%20and%20Guangquan%20Jie%20and%20Jinyu%20Liu%20and%20Henghui%20Ding&entry.1292438233=SAM3D%20has%20garnered%20widespread%20attention%20for%20its%20strong%203D%20object%20reconstruction%20capabilities.%20However%2C%20a%20key%20limitation%20remains%3A%20SAM3D%20cannot%20reconstruct%20specific%20objects%20referred%20to%20by%20textual%20descriptions%2C%20a%20capability%20that%20is%20essential%20for%20practical%20applications%20such%20as%203D%20editing%2C%20game%20development%2C%20and%20virtual%20environments.%20To%20address%20this%20gap%2C%20we%20introduce%20Ref-SAM3D%2C%20a%20simple%20yet%20effective%20extension%20to%20SAM3D%20that%20incorporates%20textual%20descriptions%20as%20a%20high-level%20prior%2C%20enabling%20text-guided%203D%20reconstruction%20from%20a%20single%20RGB%20image.%20Through%20extensive%20qualitative%20experiments%2C%20we%20show%20that%20Ref-SAM3D%2C%20guided%20only%20by%20natural%20language%20and%20a%20single%202D%20view%2C%20delivers%20competitive%20and%20high-fidelity%20zero-shot%20reconstruction%20performance.%20Our%20results%20demonstrate%20that%20Ref-SAM3D%20effectively%20bridges%20the%20gap%20between%202D%20visual%20cues%20and%203D%20geometric%20understanding%2C%20offering%20a%20more%20flexible%20and%20accessible%20paradigm%20for%20reference-guided%203D%20reconstruction.%20Code%20is%20available%20at%3A%20https%3A//github.com/FudanCVL/Ref-SAM3D.&entry.1838667208=http%3A//arxiv.org/abs/2511.19426v1&entry.124074799=Read"},
{"title": "Synthesizing Visual Concepts as Vision-Language Programs", "author": "Antonia W\u00fcst and Wolfgang Stammer and Hikaru Shindo and Lukas Helff and Devendra Singh Dhami and Kristian Kersting", "abstract": "Vision-Language models (VLMs) achieve strong performance on multimodal tasks but often fail at systematic visual reasoning tasks, leading to inconsistent or illogical outputs. Neuro-symbolic methods promise to address this by inducing interpretable logical rules, though they exploit rigid, domain-specific perception modules. We propose Vision-Language Programs (VLP), which combine the perceptual flexibility of VLMs with systematic reasoning of program synthesis. Rather than embedding reasoning inside the VLM, VLP leverages the model to produce structured visual descriptions that are compiled into neuro-symbolic programs. The resulting programs execute directly on images, remain consistent with task constraints, and provide human-interpretable explanations that enable easy shortcut mitigation. Experiments on synthetic and real-world datasets demonstrate that VLPs outperform direct and structured prompting, particularly on tasks requiring complex logical reasoning.", "link": "http://arxiv.org/abs/2511.18964v1", "date": "2025-11-24", "relevancy": 2.8942, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6049}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6049}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthesizing%20Visual%20Concepts%20as%20Vision-Language%20Programs&body=Title%3A%20Synthesizing%20Visual%20Concepts%20as%20Vision-Language%20Programs%0AAuthor%3A%20Antonia%20W%C3%BCst%20and%20Wolfgang%20Stammer%20and%20Hikaru%20Shindo%20and%20Lukas%20Helff%20and%20Devendra%20Singh%20Dhami%20and%20Kristian%20Kersting%0AAbstract%3A%20Vision-Language%20models%20%28VLMs%29%20achieve%20strong%20performance%20on%20multimodal%20tasks%20but%20often%20fail%20at%20systematic%20visual%20reasoning%20tasks%2C%20leading%20to%20inconsistent%20or%20illogical%20outputs.%20Neuro-symbolic%20methods%20promise%20to%20address%20this%20by%20inducing%20interpretable%20logical%20rules%2C%20though%20they%20exploit%20rigid%2C%20domain-specific%20perception%20modules.%20We%20propose%20Vision-Language%20Programs%20%28VLP%29%2C%20which%20combine%20the%20perceptual%20flexibility%20of%20VLMs%20with%20systematic%20reasoning%20of%20program%20synthesis.%20Rather%20than%20embedding%20reasoning%20inside%20the%20VLM%2C%20VLP%20leverages%20the%20model%20to%20produce%20structured%20visual%20descriptions%20that%20are%20compiled%20into%20neuro-symbolic%20programs.%20The%20resulting%20programs%20execute%20directly%20on%20images%2C%20remain%20consistent%20with%20task%20constraints%2C%20and%20provide%20human-interpretable%20explanations%20that%20enable%20easy%20shortcut%20mitigation.%20Experiments%20on%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20VLPs%20outperform%20direct%20and%20structured%20prompting%2C%20particularly%20on%20tasks%20requiring%20complex%20logical%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesizing%2520Visual%2520Concepts%2520as%2520Vision-Language%2520Programs%26entry.906535625%3DAntonia%2520W%25C3%25BCst%2520and%2520Wolfgang%2520Stammer%2520and%2520Hikaru%2520Shindo%2520and%2520Lukas%2520Helff%2520and%2520Devendra%2520Singh%2520Dhami%2520and%2520Kristian%2520Kersting%26entry.1292438233%3DVision-Language%2520models%2520%2528VLMs%2529%2520achieve%2520strong%2520performance%2520on%2520multimodal%2520tasks%2520but%2520often%2520fail%2520at%2520systematic%2520visual%2520reasoning%2520tasks%252C%2520leading%2520to%2520inconsistent%2520or%2520illogical%2520outputs.%2520Neuro-symbolic%2520methods%2520promise%2520to%2520address%2520this%2520by%2520inducing%2520interpretable%2520logical%2520rules%252C%2520though%2520they%2520exploit%2520rigid%252C%2520domain-specific%2520perception%2520modules.%2520We%2520propose%2520Vision-Language%2520Programs%2520%2528VLP%2529%252C%2520which%2520combine%2520the%2520perceptual%2520flexibility%2520of%2520VLMs%2520with%2520systematic%2520reasoning%2520of%2520program%2520synthesis.%2520Rather%2520than%2520embedding%2520reasoning%2520inside%2520the%2520VLM%252C%2520VLP%2520leverages%2520the%2520model%2520to%2520produce%2520structured%2520visual%2520descriptions%2520that%2520are%2520compiled%2520into%2520neuro-symbolic%2520programs.%2520The%2520resulting%2520programs%2520execute%2520directly%2520on%2520images%252C%2520remain%2520consistent%2520with%2520task%2520constraints%252C%2520and%2520provide%2520human-interpretable%2520explanations%2520that%2520enable%2520easy%2520shortcut%2520mitigation.%2520Experiments%2520on%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520VLPs%2520outperform%2520direct%2520and%2520structured%2520prompting%252C%2520particularly%2520on%2520tasks%2520requiring%2520complex%2520logical%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesizing%20Visual%20Concepts%20as%20Vision-Language%20Programs&entry.906535625=Antonia%20W%C3%BCst%20and%20Wolfgang%20Stammer%20and%20Hikaru%20Shindo%20and%20Lukas%20Helff%20and%20Devendra%20Singh%20Dhami%20and%20Kristian%20Kersting&entry.1292438233=Vision-Language%20models%20%28VLMs%29%20achieve%20strong%20performance%20on%20multimodal%20tasks%20but%20often%20fail%20at%20systematic%20visual%20reasoning%20tasks%2C%20leading%20to%20inconsistent%20or%20illogical%20outputs.%20Neuro-symbolic%20methods%20promise%20to%20address%20this%20by%20inducing%20interpretable%20logical%20rules%2C%20though%20they%20exploit%20rigid%2C%20domain-specific%20perception%20modules.%20We%20propose%20Vision-Language%20Programs%20%28VLP%29%2C%20which%20combine%20the%20perceptual%20flexibility%20of%20VLMs%20with%20systematic%20reasoning%20of%20program%20synthesis.%20Rather%20than%20embedding%20reasoning%20inside%20the%20VLM%2C%20VLP%20leverages%20the%20model%20to%20produce%20structured%20visual%20descriptions%20that%20are%20compiled%20into%20neuro-symbolic%20programs.%20The%20resulting%20programs%20execute%20directly%20on%20images%2C%20remain%20consistent%20with%20task%20constraints%2C%20and%20provide%20human-interpretable%20explanations%20that%20enable%20easy%20shortcut%20mitigation.%20Experiments%20on%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20VLPs%20outperform%20direct%20and%20structured%20prompting%2C%20particularly%20on%20tasks%20requiring%20complex%20logical%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2511.18964v1&entry.124074799=Read"},
{"title": "Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling", "author": "Minseok Seo and Mark Hamilton and Changick Kim", "abstract": "We present \\textbf{Upsample Anything}, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only $\\approx0.419 \\text{s}$ per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling. \\textbf{Project page:} \\href{https://seominseok0429.github.io/Upsample-Anything/}{https://seominseok0429.github.io/Upsample-Anything/}", "link": "http://arxiv.org/abs/2511.16301v2", "date": "2025-11-24", "relevancy": 2.8915, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6075}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5742}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Upsample%20Anything%3A%20A%20Simple%20and%20Hard%20to%20Beat%20Baseline%20for%20Feature%20Upsampling&body=Title%3A%20Upsample%20Anything%3A%20A%20Simple%20and%20Hard%20to%20Beat%20Baseline%20for%20Feature%20Upsampling%0AAuthor%3A%20Minseok%20Seo%20and%20Mark%20Hamilton%20and%20Changick%20Kim%0AAbstract%3A%20We%20present%20%5Ctextbf%7BUpsample%20Anything%7D%2C%20a%20lightweight%20test-time%20optimization%20%28TTO%29%20framework%20that%20restores%20low-resolution%20features%20to%20high-resolution%2C%20pixel-wise%20outputs%20without%20any%20training.%20Although%20Vision%20Foundation%20Models%20demonstrate%20strong%20generalization%20across%20diverse%20downstream%20tasks%2C%20their%20representations%20are%20typically%20downsampled%20by%2014x/16x%20%28e.g.%2C%20ViT%29%2C%20which%20limits%20their%20direct%20use%20in%20pixel-level%20applications.%20Existing%20feature%20upsampling%20approaches%20depend%20on%20dataset-specific%20retraining%20or%20heavy%20implicit%20optimization%2C%20restricting%20scalability%20and%20generalization.%20Upsample%20Anything%20addresses%20these%20issues%20through%20a%20simple%20per-image%20optimization%20that%20learns%20an%20anisotropic%20Gaussian%20kernel%20combining%20spatial%20and%20range%20cues%2C%20effectively%20bridging%20Gaussian%20Splatting%20and%20Joint%20Bilateral%20Upsampling.%20The%20learned%20kernel%20acts%20as%20a%20universal%2C%20edge-aware%20operator%20that%20transfers%20seamlessly%20across%20architectures%20and%20modalities%2C%20enabling%20precise%20high-resolution%20reconstruction%20of%20features%2C%20depth%2C%20or%20probability%20maps.%20It%20runs%20in%20only%20%24%5Capprox0.419%20%5Ctext%7Bs%7D%24%20per%20224x224%20image%20and%20achieves%20state-of-the-art%20performance%20on%20semantic%20segmentation%2C%20depth%20estimation%2C%20and%20both%20depth%20and%20probability%20map%20upsampling.%20%5Ctextbf%7BProject%20page%3A%7D%20%5Chref%7Bhttps%3A//seominseok0429.github.io/Upsample-Anything/%7D%7Bhttps%3A//seominseok0429.github.io/Upsample-Anything/%7D%0ALink%3A%20http%3A//arxiv.org/abs/2511.16301v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUpsample%2520Anything%253A%2520A%2520Simple%2520and%2520Hard%2520to%2520Beat%2520Baseline%2520for%2520Feature%2520Upsampling%26entry.906535625%3DMinseok%2520Seo%2520and%2520Mark%2520Hamilton%2520and%2520Changick%2520Kim%26entry.1292438233%3DWe%2520present%2520%255Ctextbf%257BUpsample%2520Anything%257D%252C%2520a%2520lightweight%2520test-time%2520optimization%2520%2528TTO%2529%2520framework%2520that%2520restores%2520low-resolution%2520features%2520to%2520high-resolution%252C%2520pixel-wise%2520outputs%2520without%2520any%2520training.%2520Although%2520Vision%2520Foundation%2520Models%2520demonstrate%2520strong%2520generalization%2520across%2520diverse%2520downstream%2520tasks%252C%2520their%2520representations%2520are%2520typically%2520downsampled%2520by%252014x/16x%2520%2528e.g.%252C%2520ViT%2529%252C%2520which%2520limits%2520their%2520direct%2520use%2520in%2520pixel-level%2520applications.%2520Existing%2520feature%2520upsampling%2520approaches%2520depend%2520on%2520dataset-specific%2520retraining%2520or%2520heavy%2520implicit%2520optimization%252C%2520restricting%2520scalability%2520and%2520generalization.%2520Upsample%2520Anything%2520addresses%2520these%2520issues%2520through%2520a%2520simple%2520per-image%2520optimization%2520that%2520learns%2520an%2520anisotropic%2520Gaussian%2520kernel%2520combining%2520spatial%2520and%2520range%2520cues%252C%2520effectively%2520bridging%2520Gaussian%2520Splatting%2520and%2520Joint%2520Bilateral%2520Upsampling.%2520The%2520learned%2520kernel%2520acts%2520as%2520a%2520universal%252C%2520edge-aware%2520operator%2520that%2520transfers%2520seamlessly%2520across%2520architectures%2520and%2520modalities%252C%2520enabling%2520precise%2520high-resolution%2520reconstruction%2520of%2520features%252C%2520depth%252C%2520or%2520probability%2520maps.%2520It%2520runs%2520in%2520only%2520%2524%255Capprox0.419%2520%255Ctext%257Bs%257D%2524%2520per%2520224x224%2520image%2520and%2520achieves%2520state-of-the-art%2520performance%2520on%2520semantic%2520segmentation%252C%2520depth%2520estimation%252C%2520and%2520both%2520depth%2520and%2520probability%2520map%2520upsampling.%2520%255Ctextbf%257BProject%2520page%253A%257D%2520%255Chref%257Bhttps%253A//seominseok0429.github.io/Upsample-Anything/%257D%257Bhttps%253A//seominseok0429.github.io/Upsample-Anything/%257D%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16301v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Upsample%20Anything%3A%20A%20Simple%20and%20Hard%20to%20Beat%20Baseline%20for%20Feature%20Upsampling&entry.906535625=Minseok%20Seo%20and%20Mark%20Hamilton%20and%20Changick%20Kim&entry.1292438233=We%20present%20%5Ctextbf%7BUpsample%20Anything%7D%2C%20a%20lightweight%20test-time%20optimization%20%28TTO%29%20framework%20that%20restores%20low-resolution%20features%20to%20high-resolution%2C%20pixel-wise%20outputs%20without%20any%20training.%20Although%20Vision%20Foundation%20Models%20demonstrate%20strong%20generalization%20across%20diverse%20downstream%20tasks%2C%20their%20representations%20are%20typically%20downsampled%20by%2014x/16x%20%28e.g.%2C%20ViT%29%2C%20which%20limits%20their%20direct%20use%20in%20pixel-level%20applications.%20Existing%20feature%20upsampling%20approaches%20depend%20on%20dataset-specific%20retraining%20or%20heavy%20implicit%20optimization%2C%20restricting%20scalability%20and%20generalization.%20Upsample%20Anything%20addresses%20these%20issues%20through%20a%20simple%20per-image%20optimization%20that%20learns%20an%20anisotropic%20Gaussian%20kernel%20combining%20spatial%20and%20range%20cues%2C%20effectively%20bridging%20Gaussian%20Splatting%20and%20Joint%20Bilateral%20Upsampling.%20The%20learned%20kernel%20acts%20as%20a%20universal%2C%20edge-aware%20operator%20that%20transfers%20seamlessly%20across%20architectures%20and%20modalities%2C%20enabling%20precise%20high-resolution%20reconstruction%20of%20features%2C%20depth%2C%20or%20probability%20maps.%20It%20runs%20in%20only%20%24%5Capprox0.419%20%5Ctext%7Bs%7D%24%20per%20224x224%20image%20and%20achieves%20state-of-the-art%20performance%20on%20semantic%20segmentation%2C%20depth%20estimation%2C%20and%20both%20depth%20and%20probability%20map%20upsampling.%20%5Ctextbf%7BProject%20page%3A%7D%20%5Chref%7Bhttps%3A//seominseok0429.github.io/Upsample-Anything/%7D%7Bhttps%3A//seominseok0429.github.io/Upsample-Anything/%7D&entry.1838667208=http%3A//arxiv.org/abs/2511.16301v2&entry.124074799=Read"},
{"title": "MedBridge: Bridging Foundation Vision-Language Models to Medical Image Diagnosis in Chest X-Ray", "author": "Yitong Li and Morteza Ghahremani and Christian Wachinger", "abstract": "Recent vision-language foundation models deliver state-of-the-art results in natural image classification, but falter in medical images due to pronounced domain shifts. Training a medical foundation model also requires substantial resources, including extensive annotated data and high computational capacity. To bridge this gap with minimal overhead, we introduce MedBridge, a lightweight multimodal adaptation framework that flexibly re-purposes arbitrary pre-trained foundation VLMs for medical image diagnosis. MedBridge comprises three novel core components. First, a Focal Sampling module that subsamples and extracts high-resolution local regions to capture subtle pathological features, compensating for the limited input resolution of foundation VLMs. Second, a Query-Encoder model with a small set of learnable queries to align the feature maps of frozen VLMs with medical semantics, without requiring retraining of the backbone layers. Third, a Mixture of Experts mechanism, driven by learnable queries, harnesses the complementary strength of various VLMs to maximize diagnostic performance. We evaluate MedBridge on five chest radiograph benchmarks in three key adaptation tasks, demonstrating its superior performance in both cross-domain and in-domain adaptation settings under varying levels of training data availability. MedBridge achieved an improvement of 6-15% in AUC compared to state-of-the-art VLM adaptation methods in multi-label thoracic disease diagnosis, underscoring its effectiveness in leveraging diverse foundation models for accurate and data-efficient medical diagnosis. Our project and code are available at https://github.com/ai-med/MedBridge.", "link": "http://arxiv.org/abs/2505.21698v2", "date": "2025-11-24", "relevancy": 2.8621, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.575}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedBridge%3A%20Bridging%20Foundation%20Vision-Language%20Models%20to%20Medical%20Image%20Diagnosis%20in%20Chest%20X-Ray&body=Title%3A%20MedBridge%3A%20Bridging%20Foundation%20Vision-Language%20Models%20to%20Medical%20Image%20Diagnosis%20in%20Chest%20X-Ray%0AAuthor%3A%20Yitong%20Li%20and%20Morteza%20Ghahremani%20and%20Christian%20Wachinger%0AAbstract%3A%20Recent%20vision-language%20foundation%20models%20deliver%20state-of-the-art%20results%20in%20natural%20image%20classification%2C%20but%20falter%20in%20medical%20images%20due%20to%20pronounced%20domain%20shifts.%20Training%20a%20medical%20foundation%20model%20also%20requires%20substantial%20resources%2C%20including%20extensive%20annotated%20data%20and%20high%20computational%20capacity.%20To%20bridge%20this%20gap%20with%20minimal%20overhead%2C%20we%20introduce%20MedBridge%2C%20a%20lightweight%20multimodal%20adaptation%20framework%20that%20flexibly%20re-purposes%20arbitrary%20pre-trained%20foundation%20VLMs%20for%20medical%20image%20diagnosis.%20MedBridge%20comprises%20three%20novel%20core%20components.%20First%2C%20a%20Focal%20Sampling%20module%20that%20subsamples%20and%20extracts%20high-resolution%20local%20regions%20to%20capture%20subtle%20pathological%20features%2C%20compensating%20for%20the%20limited%20input%20resolution%20of%20foundation%20VLMs.%20Second%2C%20a%20Query-Encoder%20model%20with%20a%20small%20set%20of%20learnable%20queries%20to%20align%20the%20feature%20maps%20of%20frozen%20VLMs%20with%20medical%20semantics%2C%20without%20requiring%20retraining%20of%20the%20backbone%20layers.%20Third%2C%20a%20Mixture%20of%20Experts%20mechanism%2C%20driven%20by%20learnable%20queries%2C%20harnesses%20the%20complementary%20strength%20of%20various%20VLMs%20to%20maximize%20diagnostic%20performance.%20We%20evaluate%20MedBridge%20on%20five%20chest%20radiograph%20benchmarks%20in%20three%20key%20adaptation%20tasks%2C%20demonstrating%20its%20superior%20performance%20in%20both%20cross-domain%20and%20in-domain%20adaptation%20settings%20under%20varying%20levels%20of%20training%20data%20availability.%20MedBridge%20achieved%20an%20improvement%20of%206-15%25%20in%20AUC%20compared%20to%20state-of-the-art%20VLM%20adaptation%20methods%20in%20multi-label%20thoracic%20disease%20diagnosis%2C%20underscoring%20its%20effectiveness%20in%20leveraging%20diverse%20foundation%20models%20for%20accurate%20and%20data-efficient%20medical%20diagnosis.%20Our%20project%20and%20code%20are%20available%20at%20https%3A//github.com/ai-med/MedBridge.%0ALink%3A%20http%3A//arxiv.org/abs/2505.21698v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedBridge%253A%2520Bridging%2520Foundation%2520Vision-Language%2520Models%2520to%2520Medical%2520Image%2520Diagnosis%2520in%2520Chest%2520X-Ray%26entry.906535625%3DYitong%2520Li%2520and%2520Morteza%2520Ghahremani%2520and%2520Christian%2520Wachinger%26entry.1292438233%3DRecent%2520vision-language%2520foundation%2520models%2520deliver%2520state-of-the-art%2520results%2520in%2520natural%2520image%2520classification%252C%2520but%2520falter%2520in%2520medical%2520images%2520due%2520to%2520pronounced%2520domain%2520shifts.%2520Training%2520a%2520medical%2520foundation%2520model%2520also%2520requires%2520substantial%2520resources%252C%2520including%2520extensive%2520annotated%2520data%2520and%2520high%2520computational%2520capacity.%2520To%2520bridge%2520this%2520gap%2520with%2520minimal%2520overhead%252C%2520we%2520introduce%2520MedBridge%252C%2520a%2520lightweight%2520multimodal%2520adaptation%2520framework%2520that%2520flexibly%2520re-purposes%2520arbitrary%2520pre-trained%2520foundation%2520VLMs%2520for%2520medical%2520image%2520diagnosis.%2520MedBridge%2520comprises%2520three%2520novel%2520core%2520components.%2520First%252C%2520a%2520Focal%2520Sampling%2520module%2520that%2520subsamples%2520and%2520extracts%2520high-resolution%2520local%2520regions%2520to%2520capture%2520subtle%2520pathological%2520features%252C%2520compensating%2520for%2520the%2520limited%2520input%2520resolution%2520of%2520foundation%2520VLMs.%2520Second%252C%2520a%2520Query-Encoder%2520model%2520with%2520a%2520small%2520set%2520of%2520learnable%2520queries%2520to%2520align%2520the%2520feature%2520maps%2520of%2520frozen%2520VLMs%2520with%2520medical%2520semantics%252C%2520without%2520requiring%2520retraining%2520of%2520the%2520backbone%2520layers.%2520Third%252C%2520a%2520Mixture%2520of%2520Experts%2520mechanism%252C%2520driven%2520by%2520learnable%2520queries%252C%2520harnesses%2520the%2520complementary%2520strength%2520of%2520various%2520VLMs%2520to%2520maximize%2520diagnostic%2520performance.%2520We%2520evaluate%2520MedBridge%2520on%2520five%2520chest%2520radiograph%2520benchmarks%2520in%2520three%2520key%2520adaptation%2520tasks%252C%2520demonstrating%2520its%2520superior%2520performance%2520in%2520both%2520cross-domain%2520and%2520in-domain%2520adaptation%2520settings%2520under%2520varying%2520levels%2520of%2520training%2520data%2520availability.%2520MedBridge%2520achieved%2520an%2520improvement%2520of%25206-15%2525%2520in%2520AUC%2520compared%2520to%2520state-of-the-art%2520VLM%2520adaptation%2520methods%2520in%2520multi-label%2520thoracic%2520disease%2520diagnosis%252C%2520underscoring%2520its%2520effectiveness%2520in%2520leveraging%2520diverse%2520foundation%2520models%2520for%2520accurate%2520and%2520data-efficient%2520medical%2520diagnosis.%2520Our%2520project%2520and%2520code%2520are%2520available%2520at%2520https%253A//github.com/ai-med/MedBridge.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21698v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedBridge%3A%20Bridging%20Foundation%20Vision-Language%20Models%20to%20Medical%20Image%20Diagnosis%20in%20Chest%20X-Ray&entry.906535625=Yitong%20Li%20and%20Morteza%20Ghahremani%20and%20Christian%20Wachinger&entry.1292438233=Recent%20vision-language%20foundation%20models%20deliver%20state-of-the-art%20results%20in%20natural%20image%20classification%2C%20but%20falter%20in%20medical%20images%20due%20to%20pronounced%20domain%20shifts.%20Training%20a%20medical%20foundation%20model%20also%20requires%20substantial%20resources%2C%20including%20extensive%20annotated%20data%20and%20high%20computational%20capacity.%20To%20bridge%20this%20gap%20with%20minimal%20overhead%2C%20we%20introduce%20MedBridge%2C%20a%20lightweight%20multimodal%20adaptation%20framework%20that%20flexibly%20re-purposes%20arbitrary%20pre-trained%20foundation%20VLMs%20for%20medical%20image%20diagnosis.%20MedBridge%20comprises%20three%20novel%20core%20components.%20First%2C%20a%20Focal%20Sampling%20module%20that%20subsamples%20and%20extracts%20high-resolution%20local%20regions%20to%20capture%20subtle%20pathological%20features%2C%20compensating%20for%20the%20limited%20input%20resolution%20of%20foundation%20VLMs.%20Second%2C%20a%20Query-Encoder%20model%20with%20a%20small%20set%20of%20learnable%20queries%20to%20align%20the%20feature%20maps%20of%20frozen%20VLMs%20with%20medical%20semantics%2C%20without%20requiring%20retraining%20of%20the%20backbone%20layers.%20Third%2C%20a%20Mixture%20of%20Experts%20mechanism%2C%20driven%20by%20learnable%20queries%2C%20harnesses%20the%20complementary%20strength%20of%20various%20VLMs%20to%20maximize%20diagnostic%20performance.%20We%20evaluate%20MedBridge%20on%20five%20chest%20radiograph%20benchmarks%20in%20three%20key%20adaptation%20tasks%2C%20demonstrating%20its%20superior%20performance%20in%20both%20cross-domain%20and%20in-domain%20adaptation%20settings%20under%20varying%20levels%20of%20training%20data%20availability.%20MedBridge%20achieved%20an%20improvement%20of%206-15%25%20in%20AUC%20compared%20to%20state-of-the-art%20VLM%20adaptation%20methods%20in%20multi-label%20thoracic%20disease%20diagnosis%2C%20underscoring%20its%20effectiveness%20in%20leveraging%20diverse%20foundation%20models%20for%20accurate%20and%20data-efficient%20medical%20diagnosis.%20Our%20project%20and%20code%20are%20available%20at%20https%3A//github.com/ai-med/MedBridge.&entry.1838667208=http%3A//arxiv.org/abs/2505.21698v2&entry.124074799=Read"},
{"title": "Warm Chat: Diffuse Emotion-aware Interactive Talking Head Avatar with Tree-Structured Guidance", "author": "Haijie Yang and Zhenyu Zhang and Hao Tang and Jianjun Qian and Jian Yang", "abstract": "Generative models have advanced rapidly, enabling impressive talking head generation that brings AI to life. However, most existing methods focus solely on one-way portrait animation. Even the few that support bidirectional conversational interactions lack precise emotion-adaptive capabilities, significantly limiting their practical applicability. In this paper, we propose Warm Chat, a novel emotion-aware talking head generation framework for dyadic interactions. Leveraging the dialogue generation capability of large language models (LLMs, e.g., GPT-4), our method produces temporally consistent virtual avatars with rich emotional variations that seamlessly transition between speaking and listening states. Specifically, we design a Transformer-based head mask generator that learns temporally consistent motion features in a latent mask space, capable of generating arbitrary-length, temporally consistent mask sequences to constrain head motions. Furthermore, we introduce an interactive talking tree structure to represent dialogue state transitions, where each tree node contains information such as child/parent/sibling nodes and the current character's emotional state. By performing reverse-level traversal, we extract rich historical emotional cues from the current node to guide expression synthesis. Extensive experiments demonstrate the superior performance and effectiveness of our method.", "link": "http://arxiv.org/abs/2508.18337v3", "date": "2025-11-24", "relevancy": 2.8497, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5787}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5787}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Warm%20Chat%3A%20Diffuse%20Emotion-aware%20Interactive%20Talking%20Head%20Avatar%20with%20Tree-Structured%20Guidance&body=Title%3A%20Warm%20Chat%3A%20Diffuse%20Emotion-aware%20Interactive%20Talking%20Head%20Avatar%20with%20Tree-Structured%20Guidance%0AAuthor%3A%20Haijie%20Yang%20and%20Zhenyu%20Zhang%20and%20Hao%20Tang%20and%20Jianjun%20Qian%20and%20Jian%20Yang%0AAbstract%3A%20Generative%20models%20have%20advanced%20rapidly%2C%20enabling%20impressive%20talking%20head%20generation%20that%20brings%20AI%20to%20life.%20However%2C%20most%20existing%20methods%20focus%20solely%20on%20one-way%20portrait%20animation.%20Even%20the%20few%20that%20support%20bidirectional%20conversational%20interactions%20lack%20precise%20emotion-adaptive%20capabilities%2C%20significantly%20limiting%20their%20practical%20applicability.%20In%20this%20paper%2C%20we%20propose%20Warm%20Chat%2C%20a%20novel%20emotion-aware%20talking%20head%20generation%20framework%20for%20dyadic%20interactions.%20Leveraging%20the%20dialogue%20generation%20capability%20of%20large%20language%20models%20%28LLMs%2C%20e.g.%2C%20GPT-4%29%2C%20our%20method%20produces%20temporally%20consistent%20virtual%20avatars%20with%20rich%20emotional%20variations%20that%20seamlessly%20transition%20between%20speaking%20and%20listening%20states.%20Specifically%2C%20we%20design%20a%20Transformer-based%20head%20mask%20generator%20that%20learns%20temporally%20consistent%20motion%20features%20in%20a%20latent%20mask%20space%2C%20capable%20of%20generating%20arbitrary-length%2C%20temporally%20consistent%20mask%20sequences%20to%20constrain%20head%20motions.%20Furthermore%2C%20we%20introduce%20an%20interactive%20talking%20tree%20structure%20to%20represent%20dialogue%20state%20transitions%2C%20where%20each%20tree%20node%20contains%20information%20such%20as%20child/parent/sibling%20nodes%20and%20the%20current%20character%27s%20emotional%20state.%20By%20performing%20reverse-level%20traversal%2C%20we%20extract%20rich%20historical%20emotional%20cues%20from%20the%20current%20node%20to%20guide%20expression%20synthesis.%20Extensive%20experiments%20demonstrate%20the%20superior%20performance%20and%20effectiveness%20of%20our%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2508.18337v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWarm%2520Chat%253A%2520Diffuse%2520Emotion-aware%2520Interactive%2520Talking%2520Head%2520Avatar%2520with%2520Tree-Structured%2520Guidance%26entry.906535625%3DHaijie%2520Yang%2520and%2520Zhenyu%2520Zhang%2520and%2520Hao%2520Tang%2520and%2520Jianjun%2520Qian%2520and%2520Jian%2520Yang%26entry.1292438233%3DGenerative%2520models%2520have%2520advanced%2520rapidly%252C%2520enabling%2520impressive%2520talking%2520head%2520generation%2520that%2520brings%2520AI%2520to%2520life.%2520However%252C%2520most%2520existing%2520methods%2520focus%2520solely%2520on%2520one-way%2520portrait%2520animation.%2520Even%2520the%2520few%2520that%2520support%2520bidirectional%2520conversational%2520interactions%2520lack%2520precise%2520emotion-adaptive%2520capabilities%252C%2520significantly%2520limiting%2520their%2520practical%2520applicability.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Warm%2520Chat%252C%2520a%2520novel%2520emotion-aware%2520talking%2520head%2520generation%2520framework%2520for%2520dyadic%2520interactions.%2520Leveraging%2520the%2520dialogue%2520generation%2520capability%2520of%2520large%2520language%2520models%2520%2528LLMs%252C%2520e.g.%252C%2520GPT-4%2529%252C%2520our%2520method%2520produces%2520temporally%2520consistent%2520virtual%2520avatars%2520with%2520rich%2520emotional%2520variations%2520that%2520seamlessly%2520transition%2520between%2520speaking%2520and%2520listening%2520states.%2520Specifically%252C%2520we%2520design%2520a%2520Transformer-based%2520head%2520mask%2520generator%2520that%2520learns%2520temporally%2520consistent%2520motion%2520features%2520in%2520a%2520latent%2520mask%2520space%252C%2520capable%2520of%2520generating%2520arbitrary-length%252C%2520temporally%2520consistent%2520mask%2520sequences%2520to%2520constrain%2520head%2520motions.%2520Furthermore%252C%2520we%2520introduce%2520an%2520interactive%2520talking%2520tree%2520structure%2520to%2520represent%2520dialogue%2520state%2520transitions%252C%2520where%2520each%2520tree%2520node%2520contains%2520information%2520such%2520as%2520child/parent/sibling%2520nodes%2520and%2520the%2520current%2520character%2527s%2520emotional%2520state.%2520By%2520performing%2520reverse-level%2520traversal%252C%2520we%2520extract%2520rich%2520historical%2520emotional%2520cues%2520from%2520the%2520current%2520node%2520to%2520guide%2520expression%2520synthesis.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superior%2520performance%2520and%2520effectiveness%2520of%2520our%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18337v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Warm%20Chat%3A%20Diffuse%20Emotion-aware%20Interactive%20Talking%20Head%20Avatar%20with%20Tree-Structured%20Guidance&entry.906535625=Haijie%20Yang%20and%20Zhenyu%20Zhang%20and%20Hao%20Tang%20and%20Jianjun%20Qian%20and%20Jian%20Yang&entry.1292438233=Generative%20models%20have%20advanced%20rapidly%2C%20enabling%20impressive%20talking%20head%20generation%20that%20brings%20AI%20to%20life.%20However%2C%20most%20existing%20methods%20focus%20solely%20on%20one-way%20portrait%20animation.%20Even%20the%20few%20that%20support%20bidirectional%20conversational%20interactions%20lack%20precise%20emotion-adaptive%20capabilities%2C%20significantly%20limiting%20their%20practical%20applicability.%20In%20this%20paper%2C%20we%20propose%20Warm%20Chat%2C%20a%20novel%20emotion-aware%20talking%20head%20generation%20framework%20for%20dyadic%20interactions.%20Leveraging%20the%20dialogue%20generation%20capability%20of%20large%20language%20models%20%28LLMs%2C%20e.g.%2C%20GPT-4%29%2C%20our%20method%20produces%20temporally%20consistent%20virtual%20avatars%20with%20rich%20emotional%20variations%20that%20seamlessly%20transition%20between%20speaking%20and%20listening%20states.%20Specifically%2C%20we%20design%20a%20Transformer-based%20head%20mask%20generator%20that%20learns%20temporally%20consistent%20motion%20features%20in%20a%20latent%20mask%20space%2C%20capable%20of%20generating%20arbitrary-length%2C%20temporally%20consistent%20mask%20sequences%20to%20constrain%20head%20motions.%20Furthermore%2C%20we%20introduce%20an%20interactive%20talking%20tree%20structure%20to%20represent%20dialogue%20state%20transitions%2C%20where%20each%20tree%20node%20contains%20information%20such%20as%20child/parent/sibling%20nodes%20and%20the%20current%20character%27s%20emotional%20state.%20By%20performing%20reverse-level%20traversal%2C%20we%20extract%20rich%20historical%20emotional%20cues%20from%20the%20current%20node%20to%20guide%20expression%20synthesis.%20Extensive%20experiments%20demonstrate%20the%20superior%20performance%20and%20effectiveness%20of%20our%20method.&entry.1838667208=http%3A//arxiv.org/abs/2508.18337v3&entry.124074799=Read"},
{"title": "MedSAM3: Delving into Segment Anything with Medical Concepts", "author": "Anglin Liu and Rundong Xue and Xu R. Cao and Yifan Shen and Yi Lu and Xiang Li and Qianqian Chen and Jintai Chen", "abstract": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.", "link": "http://arxiv.org/abs/2511.19046v1", "date": "2025-11-24", "relevancy": 2.8444, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5782}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5642}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedSAM3%3A%20Delving%20into%20Segment%20Anything%20with%20Medical%20Concepts&body=Title%3A%20MedSAM3%3A%20Delving%20into%20Segment%20Anything%20with%20Medical%20Concepts%0AAuthor%3A%20Anglin%20Liu%20and%20Rundong%20Xue%20and%20Xu%20R.%20Cao%20and%20Yifan%20Shen%20and%20Yi%20Lu%20and%20Xiang%20Li%20and%20Qianqian%20Chen%20and%20Jintai%20Chen%0AAbstract%3A%20Medical%20image%20segmentation%20is%20fundamental%20for%20biomedical%20discovery.%20Existing%20methods%20lack%20generalizability%20and%20demand%20extensive%2C%20time-consuming%20manual%20annotation%20for%20new%20clinical%20application.%20Here%2C%20we%20propose%20MedSAM-3%2C%20a%20text%20promptable%20medical%20segmentation%20model%20for%20medical%20image%20and%20video%20segmentation.%20By%20fine-tuning%20the%20Segment%20Anything%20Model%20%28SAM%29%203%20architecture%20on%20medical%20images%20paired%20with%20semantic%20conceptual%20labels%2C%20our%20MedSAM-3%20enables%20medical%20Promptable%20Concept%20Segmentation%20%28PCS%29%2C%20allowing%20precise%20targeting%20of%20anatomical%20structures%20via%20open-vocabulary%20text%20descriptions%20rather%20than%20solely%20geometric%20prompts.%20We%20further%20introduce%20the%20MedSAM-3%20Agent%2C%20a%20framework%20that%20integrates%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%20perform%20complex%20reasoning%20and%20iterative%20refinement%20in%20an%20agent-in-the-loop%20workflow.%20Comprehensive%20experiments%20across%20diverse%20medical%20imaging%20modalities%2C%20including%20X-ray%2C%20MRI%2C%20Ultrasound%2C%20CT%2C%20and%20video%2C%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20existing%20specialist%20and%20foundation%20models.%20We%20will%20release%20our%20code%20and%20model%20at%20https%3A//github.com/Joey-S-Liu/MedSAM3.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedSAM3%253A%2520Delving%2520into%2520Segment%2520Anything%2520with%2520Medical%2520Concepts%26entry.906535625%3DAnglin%2520Liu%2520and%2520Rundong%2520Xue%2520and%2520Xu%2520R.%2520Cao%2520and%2520Yifan%2520Shen%2520and%2520Yi%2520Lu%2520and%2520Xiang%2520Li%2520and%2520Qianqian%2520Chen%2520and%2520Jintai%2520Chen%26entry.1292438233%3DMedical%2520image%2520segmentation%2520is%2520fundamental%2520for%2520biomedical%2520discovery.%2520Existing%2520methods%2520lack%2520generalizability%2520and%2520demand%2520extensive%252C%2520time-consuming%2520manual%2520annotation%2520for%2520new%2520clinical%2520application.%2520Here%252C%2520we%2520propose%2520MedSAM-3%252C%2520a%2520text%2520promptable%2520medical%2520segmentation%2520model%2520for%2520medical%2520image%2520and%2520video%2520segmentation.%2520By%2520fine-tuning%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%25203%2520architecture%2520on%2520medical%2520images%2520paired%2520with%2520semantic%2520conceptual%2520labels%252C%2520our%2520MedSAM-3%2520enables%2520medical%2520Promptable%2520Concept%2520Segmentation%2520%2528PCS%2529%252C%2520allowing%2520precise%2520targeting%2520of%2520anatomical%2520structures%2520via%2520open-vocabulary%2520text%2520descriptions%2520rather%2520than%2520solely%2520geometric%2520prompts.%2520We%2520further%2520introduce%2520the%2520MedSAM-3%2520Agent%252C%2520a%2520framework%2520that%2520integrates%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520to%2520perform%2520complex%2520reasoning%2520and%2520iterative%2520refinement%2520in%2520an%2520agent-in-the-loop%2520workflow.%2520Comprehensive%2520experiments%2520across%2520diverse%2520medical%2520imaging%2520modalities%252C%2520including%2520X-ray%252C%2520MRI%252C%2520Ultrasound%252C%2520CT%252C%2520and%2520video%252C%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520existing%2520specialist%2520and%2520foundation%2520models.%2520We%2520will%2520release%2520our%2520code%2520and%2520model%2520at%2520https%253A//github.com/Joey-S-Liu/MedSAM3.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedSAM3%3A%20Delving%20into%20Segment%20Anything%20with%20Medical%20Concepts&entry.906535625=Anglin%20Liu%20and%20Rundong%20Xue%20and%20Xu%20R.%20Cao%20and%20Yifan%20Shen%20and%20Yi%20Lu%20and%20Xiang%20Li%20and%20Qianqian%20Chen%20and%20Jintai%20Chen&entry.1292438233=Medical%20image%20segmentation%20is%20fundamental%20for%20biomedical%20discovery.%20Existing%20methods%20lack%20generalizability%20and%20demand%20extensive%2C%20time-consuming%20manual%20annotation%20for%20new%20clinical%20application.%20Here%2C%20we%20propose%20MedSAM-3%2C%20a%20text%20promptable%20medical%20segmentation%20model%20for%20medical%20image%20and%20video%20segmentation.%20By%20fine-tuning%20the%20Segment%20Anything%20Model%20%28SAM%29%203%20architecture%20on%20medical%20images%20paired%20with%20semantic%20conceptual%20labels%2C%20our%20MedSAM-3%20enables%20medical%20Promptable%20Concept%20Segmentation%20%28PCS%29%2C%20allowing%20precise%20targeting%20of%20anatomical%20structures%20via%20open-vocabulary%20text%20descriptions%20rather%20than%20solely%20geometric%20prompts.%20We%20further%20introduce%20the%20MedSAM-3%20Agent%2C%20a%20framework%20that%20integrates%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%20perform%20complex%20reasoning%20and%20iterative%20refinement%20in%20an%20agent-in-the-loop%20workflow.%20Comprehensive%20experiments%20across%20diverse%20medical%20imaging%20modalities%2C%20including%20X-ray%2C%20MRI%2C%20Ultrasound%2C%20CT%2C%20and%20video%2C%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20existing%20specialist%20and%20foundation%20models.%20We%20will%20release%20our%20code%20and%20model%20at%20https%3A//github.com/Joey-S-Liu/MedSAM3.&entry.1838667208=http%3A//arxiv.org/abs/2511.19046v1&entry.124074799=Read"},
{"title": "SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation", "author": "Tianrun Chen and Runlong Cao and Xinda Yu and Lanyun Zhu and Chaotao Ding and Deyi Ji and Cheng Chen and Qi Zhu and Chunyan Xu and Papa Mao and Ying Zang", "abstract": "The rapid rise of large-scale foundation models has reshaped the landscape of image segmentation, with models such as Segment Anything achieving unprecedented versatility across diverse vision tasks. However, previous generations-including SAM and its successor-still struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, cell image segmentation, and shadow detection. To address these limitations, we originally proposed SAM-Adapter in 2023, demonstrating substantial gains on these difficult scenarios. With the emergence of Segment Anything 3 (SAM3)-a more efficient and higher-performing evolution with a redesigned architecture and improved training pipeline-we revisit these long-standing challenges. In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability. SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection. Built upon the modular and composable design philosophy of the original SAM-Adapter, SAM3-Adapter provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision. Extensive experiments confirm that integrating SAM3 with our adapter yields superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations. We hope SAM3-Adapter can serve as a foundation for future research and practical segmentation applications. Code, pre-trained models, and data processing pipelines are available.", "link": "http://arxiv.org/abs/2511.19425v1", "date": "2025-11-24", "relevancy": 2.8285, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6298}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5337}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM3-Adapter%3A%20Efficient%20Adaptation%20of%20Segment%20Anything%203%20for%20Camouflage%20Object%20Segmentation%2C%20Shadow%20Detection%2C%20and%20Medical%20Image%20Segmentation&body=Title%3A%20SAM3-Adapter%3A%20Efficient%20Adaptation%20of%20Segment%20Anything%203%20for%20Camouflage%20Object%20Segmentation%2C%20Shadow%20Detection%2C%20and%20Medical%20Image%20Segmentation%0AAuthor%3A%20Tianrun%20Chen%20and%20Runlong%20Cao%20and%20Xinda%20Yu%20and%20Lanyun%20Zhu%20and%20Chaotao%20Ding%20and%20Deyi%20Ji%20and%20Cheng%20Chen%20and%20Qi%20Zhu%20and%20Chunyan%20Xu%20and%20Papa%20Mao%20and%20Ying%20Zang%0AAbstract%3A%20The%20rapid%20rise%20of%20large-scale%20foundation%20models%20has%20reshaped%20the%20landscape%20of%20image%20segmentation%2C%20with%20models%20such%20as%20Segment%20Anything%20achieving%20unprecedented%20versatility%20across%20diverse%20vision%20tasks.%20However%2C%20previous%20generations-including%20SAM%20and%20its%20successor-still%20struggle%20with%20fine-grained%2C%20low-level%20segmentation%20challenges%20such%20as%20camouflaged%20object%20detection%2C%20medical%20image%20segmentation%2C%20cell%20image%20segmentation%2C%20and%20shadow%20detection.%20To%20address%20these%20limitations%2C%20we%20originally%20proposed%20SAM-Adapter%20in%202023%2C%20demonstrating%20substantial%20gains%20on%20these%20difficult%20scenarios.%20With%20the%20emergence%20of%20Segment%20Anything%203%20%28SAM3%29-a%20more%20efficient%20and%20higher-performing%20evolution%20with%20a%20redesigned%20architecture%20and%20improved%20training%20pipeline-we%20revisit%20these%20long-standing%20challenges.%20In%20this%20work%2C%20we%20present%20SAM3-Adapter%2C%20the%20first%20adapter%20framework%20tailored%20for%20SAM3%20that%20unlocks%20its%20full%20segmentation%20capability.%20SAM3-Adapter%20not%20only%20reduces%20computational%20overhead%20but%20also%20consistently%20surpasses%20both%20SAM%20and%20SAM2-based%20solutions%2C%20establishing%20new%20state-of-the-art%20results%20across%20multiple%20downstream%20tasks%2C%20including%20medical%20imaging%2C%20camouflaged%20%28concealed%29%20object%20segmentation%2C%20and%20shadow%20detection.%20Built%20upon%20the%20modular%20and%20composable%20design%20philosophy%20of%20the%20original%20SAM-Adapter%2C%20SAM3-Adapter%20provides%20stronger%20generalizability%2C%20richer%20task%20adaptability%2C%20and%20significantly%20improved%20segmentation%20precision.%20Extensive%20experiments%20confirm%20that%20integrating%20SAM3%20with%20our%20adapter%20yields%20superior%20accuracy%2C%20robustness%2C%20and%20efficiency%20compared%20to%20all%20prior%20SAM-based%20adaptations.%20We%20hope%20SAM3-Adapter%20can%20serve%20as%20a%20foundation%20for%20future%20research%20and%20practical%20segmentation%20applications.%20Code%2C%20pre-trained%20models%2C%20and%20data%20processing%20pipelines%20are%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM3-Adapter%253A%2520Efficient%2520Adaptation%2520of%2520Segment%2520Anything%25203%2520for%2520Camouflage%2520Object%2520Segmentation%252C%2520Shadow%2520Detection%252C%2520and%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DTianrun%2520Chen%2520and%2520Runlong%2520Cao%2520and%2520Xinda%2520Yu%2520and%2520Lanyun%2520Zhu%2520and%2520Chaotao%2520Ding%2520and%2520Deyi%2520Ji%2520and%2520Cheng%2520Chen%2520and%2520Qi%2520Zhu%2520and%2520Chunyan%2520Xu%2520and%2520Papa%2520Mao%2520and%2520Ying%2520Zang%26entry.1292438233%3DThe%2520rapid%2520rise%2520of%2520large-scale%2520foundation%2520models%2520has%2520reshaped%2520the%2520landscape%2520of%2520image%2520segmentation%252C%2520with%2520models%2520such%2520as%2520Segment%2520Anything%2520achieving%2520unprecedented%2520versatility%2520across%2520diverse%2520vision%2520tasks.%2520However%252C%2520previous%2520generations-including%2520SAM%2520and%2520its%2520successor-still%2520struggle%2520with%2520fine-grained%252C%2520low-level%2520segmentation%2520challenges%2520such%2520as%2520camouflaged%2520object%2520detection%252C%2520medical%2520image%2520segmentation%252C%2520cell%2520image%2520segmentation%252C%2520and%2520shadow%2520detection.%2520To%2520address%2520these%2520limitations%252C%2520we%2520originally%2520proposed%2520SAM-Adapter%2520in%25202023%252C%2520demonstrating%2520substantial%2520gains%2520on%2520these%2520difficult%2520scenarios.%2520With%2520the%2520emergence%2520of%2520Segment%2520Anything%25203%2520%2528SAM3%2529-a%2520more%2520efficient%2520and%2520higher-performing%2520evolution%2520with%2520a%2520redesigned%2520architecture%2520and%2520improved%2520training%2520pipeline-we%2520revisit%2520these%2520long-standing%2520challenges.%2520In%2520this%2520work%252C%2520we%2520present%2520SAM3-Adapter%252C%2520the%2520first%2520adapter%2520framework%2520tailored%2520for%2520SAM3%2520that%2520unlocks%2520its%2520full%2520segmentation%2520capability.%2520SAM3-Adapter%2520not%2520only%2520reduces%2520computational%2520overhead%2520but%2520also%2520consistently%2520surpasses%2520both%2520SAM%2520and%2520SAM2-based%2520solutions%252C%2520establishing%2520new%2520state-of-the-art%2520results%2520across%2520multiple%2520downstream%2520tasks%252C%2520including%2520medical%2520imaging%252C%2520camouflaged%2520%2528concealed%2529%2520object%2520segmentation%252C%2520and%2520shadow%2520detection.%2520Built%2520upon%2520the%2520modular%2520and%2520composable%2520design%2520philosophy%2520of%2520the%2520original%2520SAM-Adapter%252C%2520SAM3-Adapter%2520provides%2520stronger%2520generalizability%252C%2520richer%2520task%2520adaptability%252C%2520and%2520significantly%2520improved%2520segmentation%2520precision.%2520Extensive%2520experiments%2520confirm%2520that%2520integrating%2520SAM3%2520with%2520our%2520adapter%2520yields%2520superior%2520accuracy%252C%2520robustness%252C%2520and%2520efficiency%2520compared%2520to%2520all%2520prior%2520SAM-based%2520adaptations.%2520We%2520hope%2520SAM3-Adapter%2520can%2520serve%2520as%2520a%2520foundation%2520for%2520future%2520research%2520and%2520practical%2520segmentation%2520applications.%2520Code%252C%2520pre-trained%2520models%252C%2520and%2520data%2520processing%2520pipelines%2520are%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM3-Adapter%3A%20Efficient%20Adaptation%20of%20Segment%20Anything%203%20for%20Camouflage%20Object%20Segmentation%2C%20Shadow%20Detection%2C%20and%20Medical%20Image%20Segmentation&entry.906535625=Tianrun%20Chen%20and%20Runlong%20Cao%20and%20Xinda%20Yu%20and%20Lanyun%20Zhu%20and%20Chaotao%20Ding%20and%20Deyi%20Ji%20and%20Cheng%20Chen%20and%20Qi%20Zhu%20and%20Chunyan%20Xu%20and%20Papa%20Mao%20and%20Ying%20Zang&entry.1292438233=The%20rapid%20rise%20of%20large-scale%20foundation%20models%20has%20reshaped%20the%20landscape%20of%20image%20segmentation%2C%20with%20models%20such%20as%20Segment%20Anything%20achieving%20unprecedented%20versatility%20across%20diverse%20vision%20tasks.%20However%2C%20previous%20generations-including%20SAM%20and%20its%20successor-still%20struggle%20with%20fine-grained%2C%20low-level%20segmentation%20challenges%20such%20as%20camouflaged%20object%20detection%2C%20medical%20image%20segmentation%2C%20cell%20image%20segmentation%2C%20and%20shadow%20detection.%20To%20address%20these%20limitations%2C%20we%20originally%20proposed%20SAM-Adapter%20in%202023%2C%20demonstrating%20substantial%20gains%20on%20these%20difficult%20scenarios.%20With%20the%20emergence%20of%20Segment%20Anything%203%20%28SAM3%29-a%20more%20efficient%20and%20higher-performing%20evolution%20with%20a%20redesigned%20architecture%20and%20improved%20training%20pipeline-we%20revisit%20these%20long-standing%20challenges.%20In%20this%20work%2C%20we%20present%20SAM3-Adapter%2C%20the%20first%20adapter%20framework%20tailored%20for%20SAM3%20that%20unlocks%20its%20full%20segmentation%20capability.%20SAM3-Adapter%20not%20only%20reduces%20computational%20overhead%20but%20also%20consistently%20surpasses%20both%20SAM%20and%20SAM2-based%20solutions%2C%20establishing%20new%20state-of-the-art%20results%20across%20multiple%20downstream%20tasks%2C%20including%20medical%20imaging%2C%20camouflaged%20%28concealed%29%20object%20segmentation%2C%20and%20shadow%20detection.%20Built%20upon%20the%20modular%20and%20composable%20design%20philosophy%20of%20the%20original%20SAM-Adapter%2C%20SAM3-Adapter%20provides%20stronger%20generalizability%2C%20richer%20task%20adaptability%2C%20and%20significantly%20improved%20segmentation%20precision.%20Extensive%20experiments%20confirm%20that%20integrating%20SAM3%20with%20our%20adapter%20yields%20superior%20accuracy%2C%20robustness%2C%20and%20efficiency%20compared%20to%20all%20prior%20SAM-based%20adaptations.%20We%20hope%20SAM3-Adapter%20can%20serve%20as%20a%20foundation%20for%20future%20research%20and%20practical%20segmentation%20applications.%20Code%2C%20pre-trained%20models%2C%20and%20data%20processing%20pipelines%20are%20available.&entry.1838667208=http%3A//arxiv.org/abs/2511.19425v1&entry.124074799=Read"},
{"title": "DEAP-3DSAM: Decoder Enhanced and Auto Prompt SAM for 3D Medical Image Segmentation", "author": "Fangda Chen and Jintao Tang and Pancheng Wang and Ting Wang and Shasha Li and Ting Deng", "abstract": "The Segment Anything Model (SAM) has recently demonstrated significant potential in medical image segmentation. Although SAM is primarily trained on 2D images, attempts have been made to apply it to 3D medical image segmentation. However, the pseudo 3D processing used to adapt SAM results in spatial feature loss, limiting its performance. Additionally, most SAM-based methods still rely on manual prompts, which are challenging to implement in real-world scenarios and require extensive external expert knowledge. To address these limitations, we introduce the Decoder Enhanced and Auto Prompt SAM (DEAP-3DSAM) to tackle these limitations. Specifically, we propose a Feature Enhanced Decoder that fuses the original image features with rich and detailed spatial information to enhance spatial features. We also design a Dual Attention Prompter to automatically obtain prompt information through Spatial Attention and Channel Attention. We conduct comprehensive experiments on four public abdominal tumor segmentation datasets. The results indicate that our DEAP-3DSAM achieves state-of-the-art performance in 3D image segmentation, outperforming or matching existing manual prompt methods. Furthermore, both quantitative and qualitative ablation studies confirm the effectiveness of our proposed modules.", "link": "http://arxiv.org/abs/2511.19071v1", "date": "2025-11-24", "relevancy": 2.816, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5928}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5484}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEAP-3DSAM%3A%20Decoder%20Enhanced%20and%20Auto%20Prompt%20SAM%20for%203D%20Medical%20Image%20Segmentation&body=Title%3A%20DEAP-3DSAM%3A%20Decoder%20Enhanced%20and%20Auto%20Prompt%20SAM%20for%203D%20Medical%20Image%20Segmentation%0AAuthor%3A%20Fangda%20Chen%20and%20Jintao%20Tang%20and%20Pancheng%20Wang%20and%20Ting%20Wang%20and%20Shasha%20Li%20and%20Ting%20Deng%0AAbstract%3A%20The%20Segment%20Anything%20Model%20%28SAM%29%20has%20recently%20demonstrated%20significant%20potential%20in%20medical%20image%20segmentation.%20Although%20SAM%20is%20primarily%20trained%20on%202D%20images%2C%20attempts%20have%20been%20made%20to%20apply%20it%20to%203D%20medical%20image%20segmentation.%20However%2C%20the%20pseudo%203D%20processing%20used%20to%20adapt%20SAM%20results%20in%20spatial%20feature%20loss%2C%20limiting%20its%20performance.%20Additionally%2C%20most%20SAM-based%20methods%20still%20rely%20on%20manual%20prompts%2C%20which%20are%20challenging%20to%20implement%20in%20real-world%20scenarios%20and%20require%20extensive%20external%20expert%20knowledge.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%20Decoder%20Enhanced%20and%20Auto%20Prompt%20SAM%20%28DEAP-3DSAM%29%20to%20tackle%20these%20limitations.%20Specifically%2C%20we%20propose%20a%20Feature%20Enhanced%20Decoder%20that%20fuses%20the%20original%20image%20features%20with%20rich%20and%20detailed%20spatial%20information%20to%20enhance%20spatial%20features.%20We%20also%20design%20a%20Dual%20Attention%20Prompter%20to%20automatically%20obtain%20prompt%20information%20through%20Spatial%20Attention%20and%20Channel%20Attention.%20We%20conduct%20comprehensive%20experiments%20on%20four%20public%20abdominal%20tumor%20segmentation%20datasets.%20The%20results%20indicate%20that%20our%20DEAP-3DSAM%20achieves%20state-of-the-art%20performance%20in%203D%20image%20segmentation%2C%20outperforming%20or%20matching%20existing%20manual%20prompt%20methods.%20Furthermore%2C%20both%20quantitative%20and%20qualitative%20ablation%20studies%20confirm%20the%20effectiveness%20of%20our%20proposed%20modules.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEAP-3DSAM%253A%2520Decoder%2520Enhanced%2520and%2520Auto%2520Prompt%2520SAM%2520for%25203D%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DFangda%2520Chen%2520and%2520Jintao%2520Tang%2520and%2520Pancheng%2520Wang%2520and%2520Ting%2520Wang%2520and%2520Shasha%2520Li%2520and%2520Ting%2520Deng%26entry.1292438233%3DThe%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520recently%2520demonstrated%2520significant%2520potential%2520in%2520medical%2520image%2520segmentation.%2520Although%2520SAM%2520is%2520primarily%2520trained%2520on%25202D%2520images%252C%2520attempts%2520have%2520been%2520made%2520to%2520apply%2520it%2520to%25203D%2520medical%2520image%2520segmentation.%2520However%252C%2520the%2520pseudo%25203D%2520processing%2520used%2520to%2520adapt%2520SAM%2520results%2520in%2520spatial%2520feature%2520loss%252C%2520limiting%2520its%2520performance.%2520Additionally%252C%2520most%2520SAM-based%2520methods%2520still%2520rely%2520on%2520manual%2520prompts%252C%2520which%2520are%2520challenging%2520to%2520implement%2520in%2520real-world%2520scenarios%2520and%2520require%2520extensive%2520external%2520expert%2520knowledge.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520the%2520Decoder%2520Enhanced%2520and%2520Auto%2520Prompt%2520SAM%2520%2528DEAP-3DSAM%2529%2520to%2520tackle%2520these%2520limitations.%2520Specifically%252C%2520we%2520propose%2520a%2520Feature%2520Enhanced%2520Decoder%2520that%2520fuses%2520the%2520original%2520image%2520features%2520with%2520rich%2520and%2520detailed%2520spatial%2520information%2520to%2520enhance%2520spatial%2520features.%2520We%2520also%2520design%2520a%2520Dual%2520Attention%2520Prompter%2520to%2520automatically%2520obtain%2520prompt%2520information%2520through%2520Spatial%2520Attention%2520and%2520Channel%2520Attention.%2520We%2520conduct%2520comprehensive%2520experiments%2520on%2520four%2520public%2520abdominal%2520tumor%2520segmentation%2520datasets.%2520The%2520results%2520indicate%2520that%2520our%2520DEAP-3DSAM%2520achieves%2520state-of-the-art%2520performance%2520in%25203D%2520image%2520segmentation%252C%2520outperforming%2520or%2520matching%2520existing%2520manual%2520prompt%2520methods.%2520Furthermore%252C%2520both%2520quantitative%2520and%2520qualitative%2520ablation%2520studies%2520confirm%2520the%2520effectiveness%2520of%2520our%2520proposed%2520modules.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEAP-3DSAM%3A%20Decoder%20Enhanced%20and%20Auto%20Prompt%20SAM%20for%203D%20Medical%20Image%20Segmentation&entry.906535625=Fangda%20Chen%20and%20Jintao%20Tang%20and%20Pancheng%20Wang%20and%20Ting%20Wang%20and%20Shasha%20Li%20and%20Ting%20Deng&entry.1292438233=The%20Segment%20Anything%20Model%20%28SAM%29%20has%20recently%20demonstrated%20significant%20potential%20in%20medical%20image%20segmentation.%20Although%20SAM%20is%20primarily%20trained%20on%202D%20images%2C%20attempts%20have%20been%20made%20to%20apply%20it%20to%203D%20medical%20image%20segmentation.%20However%2C%20the%20pseudo%203D%20processing%20used%20to%20adapt%20SAM%20results%20in%20spatial%20feature%20loss%2C%20limiting%20its%20performance.%20Additionally%2C%20most%20SAM-based%20methods%20still%20rely%20on%20manual%20prompts%2C%20which%20are%20challenging%20to%20implement%20in%20real-world%20scenarios%20and%20require%20extensive%20external%20expert%20knowledge.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%20Decoder%20Enhanced%20and%20Auto%20Prompt%20SAM%20%28DEAP-3DSAM%29%20to%20tackle%20these%20limitations.%20Specifically%2C%20we%20propose%20a%20Feature%20Enhanced%20Decoder%20that%20fuses%20the%20original%20image%20features%20with%20rich%20and%20detailed%20spatial%20information%20to%20enhance%20spatial%20features.%20We%20also%20design%20a%20Dual%20Attention%20Prompter%20to%20automatically%20obtain%20prompt%20information%20through%20Spatial%20Attention%20and%20Channel%20Attention.%20We%20conduct%20comprehensive%20experiments%20on%20four%20public%20abdominal%20tumor%20segmentation%20datasets.%20The%20results%20indicate%20that%20our%20DEAP-3DSAM%20achieves%20state-of-the-art%20performance%20in%203D%20image%20segmentation%2C%20outperforming%20or%20matching%20existing%20manual%20prompt%20methods.%20Furthermore%2C%20both%20quantitative%20and%20qualitative%20ablation%20studies%20confirm%20the%20effectiveness%20of%20our%20proposed%20modules.&entry.1838667208=http%3A//arxiv.org/abs/2511.19071v1&entry.124074799=Read"},
{"title": "Growing with the Generator: Self-paced GRPO for Video Generation", "author": "Rui Li and Yuanzhi Liang and Ziqi Ni and Haibing Huang and Chi Zhang and Xuelong Li", "abstract": "Group Relative Policy Optimization (GRPO) has emerged as a powerful reinforcement learning paradigm for post-training video generation models. However, existing GRPO pipelines rely on static, fixed-capacity reward models whose evaluation behavior is frozen during training. Such rigid rewards introduce distributional bias, saturate quickly as the generator improves, and ultimately limit the stability and effectiveness of reinforcement-based alignment. We propose Self-Paced GRPO, a competence-aware GRPO framework in which reward feedback co-evolves with the generator. Our method introduces a progressive reward mechanism that automatically shifts its emphasis from coarse visual fidelity to temporal coherence and fine-grained text-video semantic alignment as generation quality increases. This self-paced curriculum alleviates reward-policy mismatch, mitigates reward exploitation, and yields more stable optimization. Experiments on VBench across multiple video generation backbones demonstrate consistent improvements in both visual quality and semantic alignment over GRPO baselines with static rewards, validating the effectiveness and generality of Self-Paced GRPO.", "link": "http://arxiv.org/abs/2511.19356v1", "date": "2025-11-24", "relevancy": 2.811, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5637}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5615}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Growing%20with%20the%20Generator%3A%20Self-paced%20GRPO%20for%20Video%20Generation&body=Title%3A%20Growing%20with%20the%20Generator%3A%20Self-paced%20GRPO%20for%20Video%20Generation%0AAuthor%3A%20Rui%20Li%20and%20Yuanzhi%20Liang%20and%20Ziqi%20Ni%20and%20Haibing%20Huang%20and%20Chi%20Zhang%20and%20Xuelong%20Li%0AAbstract%3A%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20has%20emerged%20as%20a%20powerful%20reinforcement%20learning%20paradigm%20for%20post-training%20video%20generation%20models.%20However%2C%20existing%20GRPO%20pipelines%20rely%20on%20static%2C%20fixed-capacity%20reward%20models%20whose%20evaluation%20behavior%20is%20frozen%20during%20training.%20Such%20rigid%20rewards%20introduce%20distributional%20bias%2C%20saturate%20quickly%20as%20the%20generator%20improves%2C%20and%20ultimately%20limit%20the%20stability%20and%20effectiveness%20of%20reinforcement-based%20alignment.%20We%20propose%20Self-Paced%20GRPO%2C%20a%20competence-aware%20GRPO%20framework%20in%20which%20reward%20feedback%20co-evolves%20with%20the%20generator.%20Our%20method%20introduces%20a%20progressive%20reward%20mechanism%20that%20automatically%20shifts%20its%20emphasis%20from%20coarse%20visual%20fidelity%20to%20temporal%20coherence%20and%20fine-grained%20text-video%20semantic%20alignment%20as%20generation%20quality%20increases.%20This%20self-paced%20curriculum%20alleviates%20reward-policy%20mismatch%2C%20mitigates%20reward%20exploitation%2C%20and%20yields%20more%20stable%20optimization.%20Experiments%20on%20VBench%20across%20multiple%20video%20generation%20backbones%20demonstrate%20consistent%20improvements%20in%20both%20visual%20quality%20and%20semantic%20alignment%20over%20GRPO%20baselines%20with%20static%20rewards%2C%20validating%20the%20effectiveness%20and%20generality%20of%20Self-Paced%20GRPO.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrowing%2520with%2520the%2520Generator%253A%2520Self-paced%2520GRPO%2520for%2520Video%2520Generation%26entry.906535625%3DRui%2520Li%2520and%2520Yuanzhi%2520Liang%2520and%2520Ziqi%2520Ni%2520and%2520Haibing%2520Huang%2520and%2520Chi%2520Zhang%2520and%2520Xuelong%2520Li%26entry.1292438233%3DGroup%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520reinforcement%2520learning%2520paradigm%2520for%2520post-training%2520video%2520generation%2520models.%2520However%252C%2520existing%2520GRPO%2520pipelines%2520rely%2520on%2520static%252C%2520fixed-capacity%2520reward%2520models%2520whose%2520evaluation%2520behavior%2520is%2520frozen%2520during%2520training.%2520Such%2520rigid%2520rewards%2520introduce%2520distributional%2520bias%252C%2520saturate%2520quickly%2520as%2520the%2520generator%2520improves%252C%2520and%2520ultimately%2520limit%2520the%2520stability%2520and%2520effectiveness%2520of%2520reinforcement-based%2520alignment.%2520We%2520propose%2520Self-Paced%2520GRPO%252C%2520a%2520competence-aware%2520GRPO%2520framework%2520in%2520which%2520reward%2520feedback%2520co-evolves%2520with%2520the%2520generator.%2520Our%2520method%2520introduces%2520a%2520progressive%2520reward%2520mechanism%2520that%2520automatically%2520shifts%2520its%2520emphasis%2520from%2520coarse%2520visual%2520fidelity%2520to%2520temporal%2520coherence%2520and%2520fine-grained%2520text-video%2520semantic%2520alignment%2520as%2520generation%2520quality%2520increases.%2520This%2520self-paced%2520curriculum%2520alleviates%2520reward-policy%2520mismatch%252C%2520mitigates%2520reward%2520exploitation%252C%2520and%2520yields%2520more%2520stable%2520optimization.%2520Experiments%2520on%2520VBench%2520across%2520multiple%2520video%2520generation%2520backbones%2520demonstrate%2520consistent%2520improvements%2520in%2520both%2520visual%2520quality%2520and%2520semantic%2520alignment%2520over%2520GRPO%2520baselines%2520with%2520static%2520rewards%252C%2520validating%2520the%2520effectiveness%2520and%2520generality%2520of%2520Self-Paced%2520GRPO.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Growing%20with%20the%20Generator%3A%20Self-paced%20GRPO%20for%20Video%20Generation&entry.906535625=Rui%20Li%20and%20Yuanzhi%20Liang%20and%20Ziqi%20Ni%20and%20Haibing%20Huang%20and%20Chi%20Zhang%20and%20Xuelong%20Li&entry.1292438233=Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20has%20emerged%20as%20a%20powerful%20reinforcement%20learning%20paradigm%20for%20post-training%20video%20generation%20models.%20However%2C%20existing%20GRPO%20pipelines%20rely%20on%20static%2C%20fixed-capacity%20reward%20models%20whose%20evaluation%20behavior%20is%20frozen%20during%20training.%20Such%20rigid%20rewards%20introduce%20distributional%20bias%2C%20saturate%20quickly%20as%20the%20generator%20improves%2C%20and%20ultimately%20limit%20the%20stability%20and%20effectiveness%20of%20reinforcement-based%20alignment.%20We%20propose%20Self-Paced%20GRPO%2C%20a%20competence-aware%20GRPO%20framework%20in%20which%20reward%20feedback%20co-evolves%20with%20the%20generator.%20Our%20method%20introduces%20a%20progressive%20reward%20mechanism%20that%20automatically%20shifts%20its%20emphasis%20from%20coarse%20visual%20fidelity%20to%20temporal%20coherence%20and%20fine-grained%20text-video%20semantic%20alignment%20as%20generation%20quality%20increases.%20This%20self-paced%20curriculum%20alleviates%20reward-policy%20mismatch%2C%20mitigates%20reward%20exploitation%2C%20and%20yields%20more%20stable%20optimization.%20Experiments%20on%20VBench%20across%20multiple%20video%20generation%20backbones%20demonstrate%20consistent%20improvements%20in%20both%20visual%20quality%20and%20semantic%20alignment%20over%20GRPO%20baselines%20with%20static%20rewards%2C%20validating%20the%20effectiveness%20and%20generality%20of%20Self-Paced%20GRPO.&entry.1838667208=http%3A//arxiv.org/abs/2511.19356v1&entry.124074799=Read"},
{"title": "Three-Dimensional Anatomical Data Generation Based on Artificial Neural Networks", "author": "Ann-Sophia M\u00fcller and Moonkwang Jeong and Meng Zhang and Jiyuan Tian and Arkadiusz Miernik and Stefanie Speidel and Tian Qiu", "abstract": "Surgical planning and training based on machine learning requires a large amount of 3D anatomical models reconstructed from medical imaging, which is currently one of the major bottlenecks. Obtaining these data from real patients and during surgery is very demanding, if even possible, due to legal, ethical, and technical challenges. It is especially difficult for soft tissue organs with poor imaging contrast, such as the prostate. To overcome these challenges, we present a novel workflow for automated 3D anatomical data generation using data obtained from physical organ models. We additionally use a 3D Generative Adversarial Network (GAN) to obtain a manifold of 3D models useful for other downstream machine learning tasks that rely on 3D data. We demonstrate our workflow using an artificial prostate model made of biomimetic hydrogels with imaging contrast in multiple zones. This is used to physically simulate endoscopic surgery. For evaluation and 3D data generation, we place it into a customized ultrasound scanner that records the prostate before and after the procedure. A neural network is trained to segment the recorded ultrasound images, which outperforms conventional, non-learning-based computer vision techniques in terms of intersection over union (IoU). Based on the segmentations, a 3D mesh model is reconstructed, and performance feedback is provided.", "link": "http://arxiv.org/abs/2511.19198v1", "date": "2025-11-24", "relevancy": 2.8067, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5631}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5631}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Three-Dimensional%20Anatomical%20Data%20Generation%20Based%20on%20Artificial%20Neural%20Networks&body=Title%3A%20Three-Dimensional%20Anatomical%20Data%20Generation%20Based%20on%20Artificial%20Neural%20Networks%0AAuthor%3A%20Ann-Sophia%20M%C3%BCller%20and%20Moonkwang%20Jeong%20and%20Meng%20Zhang%20and%20Jiyuan%20Tian%20and%20Arkadiusz%20Miernik%20and%20Stefanie%20Speidel%20and%20Tian%20Qiu%0AAbstract%3A%20Surgical%20planning%20and%20training%20based%20on%20machine%20learning%20requires%20a%20large%20amount%20of%203D%20anatomical%20models%20reconstructed%20from%20medical%20imaging%2C%20which%20is%20currently%20one%20of%20the%20major%20bottlenecks.%20Obtaining%20these%20data%20from%20real%20patients%20and%20during%20surgery%20is%20very%20demanding%2C%20if%20even%20possible%2C%20due%20to%20legal%2C%20ethical%2C%20and%20technical%20challenges.%20It%20is%20especially%20difficult%20for%20soft%20tissue%20organs%20with%20poor%20imaging%20contrast%2C%20such%20as%20the%20prostate.%20To%20overcome%20these%20challenges%2C%20we%20present%20a%20novel%20workflow%20for%20automated%203D%20anatomical%20data%20generation%20using%20data%20obtained%20from%20physical%20organ%20models.%20We%20additionally%20use%20a%203D%20Generative%20Adversarial%20Network%20%28GAN%29%20to%20obtain%20a%20manifold%20of%203D%20models%20useful%20for%20other%20downstream%20machine%20learning%20tasks%20that%20rely%20on%203D%20data.%20We%20demonstrate%20our%20workflow%20using%20an%20artificial%20prostate%20model%20made%20of%20biomimetic%20hydrogels%20with%20imaging%20contrast%20in%20multiple%20zones.%20This%20is%20used%20to%20physically%20simulate%20endoscopic%20surgery.%20For%20evaluation%20and%203D%20data%20generation%2C%20we%20place%20it%20into%20a%20customized%20ultrasound%20scanner%20that%20records%20the%20prostate%20before%20and%20after%20the%20procedure.%20A%20neural%20network%20is%20trained%20to%20segment%20the%20recorded%20ultrasound%20images%2C%20which%20outperforms%20conventional%2C%20non-learning-based%20computer%20vision%20techniques%20in%20terms%20of%20intersection%20over%20union%20%28IoU%29.%20Based%20on%20the%20segmentations%2C%20a%203D%20mesh%20model%20is%20reconstructed%2C%20and%20performance%20feedback%20is%20provided.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThree-Dimensional%2520Anatomical%2520Data%2520Generation%2520Based%2520on%2520Artificial%2520Neural%2520Networks%26entry.906535625%3DAnn-Sophia%2520M%25C3%25BCller%2520and%2520Moonkwang%2520Jeong%2520and%2520Meng%2520Zhang%2520and%2520Jiyuan%2520Tian%2520and%2520Arkadiusz%2520Miernik%2520and%2520Stefanie%2520Speidel%2520and%2520Tian%2520Qiu%26entry.1292438233%3DSurgical%2520planning%2520and%2520training%2520based%2520on%2520machine%2520learning%2520requires%2520a%2520large%2520amount%2520of%25203D%2520anatomical%2520models%2520reconstructed%2520from%2520medical%2520imaging%252C%2520which%2520is%2520currently%2520one%2520of%2520the%2520major%2520bottlenecks.%2520Obtaining%2520these%2520data%2520from%2520real%2520patients%2520and%2520during%2520surgery%2520is%2520very%2520demanding%252C%2520if%2520even%2520possible%252C%2520due%2520to%2520legal%252C%2520ethical%252C%2520and%2520technical%2520challenges.%2520It%2520is%2520especially%2520difficult%2520for%2520soft%2520tissue%2520organs%2520with%2520poor%2520imaging%2520contrast%252C%2520such%2520as%2520the%2520prostate.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520present%2520a%2520novel%2520workflow%2520for%2520automated%25203D%2520anatomical%2520data%2520generation%2520using%2520data%2520obtained%2520from%2520physical%2520organ%2520models.%2520We%2520additionally%2520use%2520a%25203D%2520Generative%2520Adversarial%2520Network%2520%2528GAN%2529%2520to%2520obtain%2520a%2520manifold%2520of%25203D%2520models%2520useful%2520for%2520other%2520downstream%2520machine%2520learning%2520tasks%2520that%2520rely%2520on%25203D%2520data.%2520We%2520demonstrate%2520our%2520workflow%2520using%2520an%2520artificial%2520prostate%2520model%2520made%2520of%2520biomimetic%2520hydrogels%2520with%2520imaging%2520contrast%2520in%2520multiple%2520zones.%2520This%2520is%2520used%2520to%2520physically%2520simulate%2520endoscopic%2520surgery.%2520For%2520evaluation%2520and%25203D%2520data%2520generation%252C%2520we%2520place%2520it%2520into%2520a%2520customized%2520ultrasound%2520scanner%2520that%2520records%2520the%2520prostate%2520before%2520and%2520after%2520the%2520procedure.%2520A%2520neural%2520network%2520is%2520trained%2520to%2520segment%2520the%2520recorded%2520ultrasound%2520images%252C%2520which%2520outperforms%2520conventional%252C%2520non-learning-based%2520computer%2520vision%2520techniques%2520in%2520terms%2520of%2520intersection%2520over%2520union%2520%2528IoU%2529.%2520Based%2520on%2520the%2520segmentations%252C%2520a%25203D%2520mesh%2520model%2520is%2520reconstructed%252C%2520and%2520performance%2520feedback%2520is%2520provided.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Three-Dimensional%20Anatomical%20Data%20Generation%20Based%20on%20Artificial%20Neural%20Networks&entry.906535625=Ann-Sophia%20M%C3%BCller%20and%20Moonkwang%20Jeong%20and%20Meng%20Zhang%20and%20Jiyuan%20Tian%20and%20Arkadiusz%20Miernik%20and%20Stefanie%20Speidel%20and%20Tian%20Qiu&entry.1292438233=Surgical%20planning%20and%20training%20based%20on%20machine%20learning%20requires%20a%20large%20amount%20of%203D%20anatomical%20models%20reconstructed%20from%20medical%20imaging%2C%20which%20is%20currently%20one%20of%20the%20major%20bottlenecks.%20Obtaining%20these%20data%20from%20real%20patients%20and%20during%20surgery%20is%20very%20demanding%2C%20if%20even%20possible%2C%20due%20to%20legal%2C%20ethical%2C%20and%20technical%20challenges.%20It%20is%20especially%20difficult%20for%20soft%20tissue%20organs%20with%20poor%20imaging%20contrast%2C%20such%20as%20the%20prostate.%20To%20overcome%20these%20challenges%2C%20we%20present%20a%20novel%20workflow%20for%20automated%203D%20anatomical%20data%20generation%20using%20data%20obtained%20from%20physical%20organ%20models.%20We%20additionally%20use%20a%203D%20Generative%20Adversarial%20Network%20%28GAN%29%20to%20obtain%20a%20manifold%20of%203D%20models%20useful%20for%20other%20downstream%20machine%20learning%20tasks%20that%20rely%20on%203D%20data.%20We%20demonstrate%20our%20workflow%20using%20an%20artificial%20prostate%20model%20made%20of%20biomimetic%20hydrogels%20with%20imaging%20contrast%20in%20multiple%20zones.%20This%20is%20used%20to%20physically%20simulate%20endoscopic%20surgery.%20For%20evaluation%20and%203D%20data%20generation%2C%20we%20place%20it%20into%20a%20customized%20ultrasound%20scanner%20that%20records%20the%20prostate%20before%20and%20after%20the%20procedure.%20A%20neural%20network%20is%20trained%20to%20segment%20the%20recorded%20ultrasound%20images%2C%20which%20outperforms%20conventional%2C%20non-learning-based%20computer%20vision%20techniques%20in%20terms%20of%20intersection%20over%20union%20%28IoU%29.%20Based%20on%20the%20segmentations%2C%20a%203D%20mesh%20model%20is%20reconstructed%2C%20and%20performance%20feedback%20is%20provided.&entry.1838667208=http%3A//arxiv.org/abs/2511.19198v1&entry.124074799=Read"},
{"title": "Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation", "author": "Huisoo Lee and Jisu Han and Hyunsouk Cho and Wonjun Hwang", "abstract": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data. Recent advances in Foundation Models (FMs) have introduced new opportunities for leveraging external semantic knowledge to guide SFDA. However, relying on a single FM is often insufficient, as it tends to bias adaptation toward a restricted semantic coverage, failing to capture diverse contextual cues under domain shift. To overcome this limitation, we propose a Collaborative Multi-foundation Adaptation (CoMA) framework that jointly leverages two different FMs (e.g., CLIP and BLIP) with complementary properties to capture both global semantics and local contextual cues. Specifically, we employ a bidirectional adaptation mechanism that (1) aligns different FMs with the target model for task adaptation while maintaining their semantic distinctiveness, and (2) transfers complementary knowledge from the FMs to the target model. To ensure stable adaptation under mini-batch training, we introduce Decomposed Mutual Information (DMI) that selectively enhances true dependencies while suppressing false dependencies arising from incomplete class coverage. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art SFDA methods across four benchmarks, including Office-31, Office-Home, DomainNet-126, and VisDA, under the closed-set setting, while also achieving best results on partial-set and open-set variants.", "link": "http://arxiv.org/abs/2511.19147v1", "date": "2025-11-24", "relevancy": 2.7635, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Learning%20with%20Multiple%20Foundation%20Models%20for%20Source-Free%20Domain%20Adaptation&body=Title%3A%20Collaborative%20Learning%20with%20Multiple%20Foundation%20Models%20for%20Source-Free%20Domain%20Adaptation%0AAuthor%3A%20Huisoo%20Lee%20and%20Jisu%20Han%20and%20Hyunsouk%20Cho%20and%20Wonjun%20Hwang%0AAbstract%3A%20Source-Free%20Domain%20Adaptation%20%28SFDA%29%20aims%20to%20adapt%20a%20pre-trained%20source%20model%20to%20an%20unlabeled%20target%20domain%20without%20access%20to%20source%20data.%20Recent%20advances%20in%20Foundation%20Models%20%28FMs%29%20have%20introduced%20new%20opportunities%20for%20leveraging%20external%20semantic%20knowledge%20to%20guide%20SFDA.%20However%2C%20relying%20on%20a%20single%20FM%20is%20often%20insufficient%2C%20as%20it%20tends%20to%20bias%20adaptation%20toward%20a%20restricted%20semantic%20coverage%2C%20failing%20to%20capture%20diverse%20contextual%20cues%20under%20domain%20shift.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20Collaborative%20Multi-foundation%20Adaptation%20%28CoMA%29%20framework%20that%20jointly%20leverages%20two%20different%20FMs%20%28e.g.%2C%20CLIP%20and%20BLIP%29%20with%20complementary%20properties%20to%20capture%20both%20global%20semantics%20and%20local%20contextual%20cues.%20Specifically%2C%20we%20employ%20a%20bidirectional%20adaptation%20mechanism%20that%20%281%29%20aligns%20different%20FMs%20with%20the%20target%20model%20for%20task%20adaptation%20while%20maintaining%20their%20semantic%20distinctiveness%2C%20and%20%282%29%20transfers%20complementary%20knowledge%20from%20the%20FMs%20to%20the%20target%20model.%20To%20ensure%20stable%20adaptation%20under%20mini-batch%20training%2C%20we%20introduce%20Decomposed%20Mutual%20Information%20%28DMI%29%20that%20selectively%20enhances%20true%20dependencies%20while%20suppressing%20false%20dependencies%20arising%20from%20incomplete%20class%20coverage.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20consistently%20outperforms%20existing%20state-of-the-art%20SFDA%20methods%20across%20four%20benchmarks%2C%20including%20Office-31%2C%20Office-Home%2C%20DomainNet-126%2C%20and%20VisDA%2C%20under%20the%20closed-set%20setting%2C%20while%20also%20achieving%20best%20results%20on%20partial-set%20and%20open-set%20variants.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Learning%2520with%2520Multiple%2520Foundation%2520Models%2520for%2520Source-Free%2520Domain%2520Adaptation%26entry.906535625%3DHuisoo%2520Lee%2520and%2520Jisu%2520Han%2520and%2520Hyunsouk%2520Cho%2520and%2520Wonjun%2520Hwang%26entry.1292438233%3DSource-Free%2520Domain%2520Adaptation%2520%2528SFDA%2529%2520aims%2520to%2520adapt%2520a%2520pre-trained%2520source%2520model%2520to%2520an%2520unlabeled%2520target%2520domain%2520without%2520access%2520to%2520source%2520data.%2520Recent%2520advances%2520in%2520Foundation%2520Models%2520%2528FMs%2529%2520have%2520introduced%2520new%2520opportunities%2520for%2520leveraging%2520external%2520semantic%2520knowledge%2520to%2520guide%2520SFDA.%2520However%252C%2520relying%2520on%2520a%2520single%2520FM%2520is%2520often%2520insufficient%252C%2520as%2520it%2520tends%2520to%2520bias%2520adaptation%2520toward%2520a%2520restricted%2520semantic%2520coverage%252C%2520failing%2520to%2520capture%2520diverse%2520contextual%2520cues%2520under%2520domain%2520shift.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520Collaborative%2520Multi-foundation%2520Adaptation%2520%2528CoMA%2529%2520framework%2520that%2520jointly%2520leverages%2520two%2520different%2520FMs%2520%2528e.g.%252C%2520CLIP%2520and%2520BLIP%2529%2520with%2520complementary%2520properties%2520to%2520capture%2520both%2520global%2520semantics%2520and%2520local%2520contextual%2520cues.%2520Specifically%252C%2520we%2520employ%2520a%2520bidirectional%2520adaptation%2520mechanism%2520that%2520%25281%2529%2520aligns%2520different%2520FMs%2520with%2520the%2520target%2520model%2520for%2520task%2520adaptation%2520while%2520maintaining%2520their%2520semantic%2520distinctiveness%252C%2520and%2520%25282%2529%2520transfers%2520complementary%2520knowledge%2520from%2520the%2520FMs%2520to%2520the%2520target%2520model.%2520To%2520ensure%2520stable%2520adaptation%2520under%2520mini-batch%2520training%252C%2520we%2520introduce%2520Decomposed%2520Mutual%2520Information%2520%2528DMI%2529%2520that%2520selectively%2520enhances%2520true%2520dependencies%2520while%2520suppressing%2520false%2520dependencies%2520arising%2520from%2520incomplete%2520class%2520coverage.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520consistently%2520outperforms%2520existing%2520state-of-the-art%2520SFDA%2520methods%2520across%2520four%2520benchmarks%252C%2520including%2520Office-31%252C%2520Office-Home%252C%2520DomainNet-126%252C%2520and%2520VisDA%252C%2520under%2520the%2520closed-set%2520setting%252C%2520while%2520also%2520achieving%2520best%2520results%2520on%2520partial-set%2520and%2520open-set%2520variants.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Learning%20with%20Multiple%20Foundation%20Models%20for%20Source-Free%20Domain%20Adaptation&entry.906535625=Huisoo%20Lee%20and%20Jisu%20Han%20and%20Hyunsouk%20Cho%20and%20Wonjun%20Hwang&entry.1292438233=Source-Free%20Domain%20Adaptation%20%28SFDA%29%20aims%20to%20adapt%20a%20pre-trained%20source%20model%20to%20an%20unlabeled%20target%20domain%20without%20access%20to%20source%20data.%20Recent%20advances%20in%20Foundation%20Models%20%28FMs%29%20have%20introduced%20new%20opportunities%20for%20leveraging%20external%20semantic%20knowledge%20to%20guide%20SFDA.%20However%2C%20relying%20on%20a%20single%20FM%20is%20often%20insufficient%2C%20as%20it%20tends%20to%20bias%20adaptation%20toward%20a%20restricted%20semantic%20coverage%2C%20failing%20to%20capture%20diverse%20contextual%20cues%20under%20domain%20shift.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20Collaborative%20Multi-foundation%20Adaptation%20%28CoMA%29%20framework%20that%20jointly%20leverages%20two%20different%20FMs%20%28e.g.%2C%20CLIP%20and%20BLIP%29%20with%20complementary%20properties%20to%20capture%20both%20global%20semantics%20and%20local%20contextual%20cues.%20Specifically%2C%20we%20employ%20a%20bidirectional%20adaptation%20mechanism%20that%20%281%29%20aligns%20different%20FMs%20with%20the%20target%20model%20for%20task%20adaptation%20while%20maintaining%20their%20semantic%20distinctiveness%2C%20and%20%282%29%20transfers%20complementary%20knowledge%20from%20the%20FMs%20to%20the%20target%20model.%20To%20ensure%20stable%20adaptation%20under%20mini-batch%20training%2C%20we%20introduce%20Decomposed%20Mutual%20Information%20%28DMI%29%20that%20selectively%20enhances%20true%20dependencies%20while%20suppressing%20false%20dependencies%20arising%20from%20incomplete%20class%20coverage.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20consistently%20outperforms%20existing%20state-of-the-art%20SFDA%20methods%20across%20four%20benchmarks%2C%20including%20Office-31%2C%20Office-Home%2C%20DomainNet-126%2C%20and%20VisDA%2C%20under%20the%20closed-set%20setting%2C%20while%20also%20achieving%20best%20results%20on%20partial-set%20and%20open-set%20variants.&entry.1838667208=http%3A//arxiv.org/abs/2511.19147v1&entry.124074799=Read"},
{"title": "Multiview point cloud registration with anisotropic and space-varying localization noise", "author": "Denis Fortun and Etienne Baudrier and Fabian Zwettler and Markus Sauer and Sylvain Faisan", "abstract": "In this paper, we address the problem of registering multiple point clouds corrupted with high anisotropic localization noise. Our approach follows the widely used framework of Gaussian mixture model (GMM) reconstruction with an expectation-maximization (EM) algorithm. Existing methods are based on an implicit assumption of space-invariant isotropic Gaussian noise. However, this assumption is violated in practice in applications such as single molecule localization microscopy (SMLM). To address this issue, we propose to introduce an explicit localization noise model that decouples shape modeling with the GMM from noise handling. We design a stochastic EM algorithm that considers noise-free data as a latent variable, with closed-form solutions at each EM step. The first advantage of our approach is to handle space-variant and anisotropic Gaussian noise with arbitrary covariances. The second advantage is to leverage the explicit noise model to impose prior knowledge about the noise that may be available from physical sensors. We show on various simulated data that our noise handling strategy improves significantly the robustness to high levels of anisotropic noise. We also demonstrate the performance of our method on real SMLM data.", "link": "http://arxiv.org/abs/2201.00708v2", "date": "2025-11-24", "relevancy": 2.7549, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.559}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5507}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiview%20point%20cloud%20registration%20with%20anisotropic%20and%20space-varying%20localization%20noise&body=Title%3A%20Multiview%20point%20cloud%20registration%20with%20anisotropic%20and%20space-varying%20localization%20noise%0AAuthor%3A%20Denis%20Fortun%20and%20Etienne%20Baudrier%20and%20Fabian%20Zwettler%20and%20Markus%20Sauer%20and%20Sylvain%20Faisan%0AAbstract%3A%20In%20this%20paper%2C%20we%20address%20the%20problem%20of%20registering%20multiple%20point%20clouds%20corrupted%20with%20high%20anisotropic%20localization%20noise.%20Our%20approach%20follows%20the%20widely%20used%20framework%20of%20Gaussian%20mixture%20model%20%28GMM%29%20reconstruction%20with%20an%20expectation-maximization%20%28EM%29%20algorithm.%20Existing%20methods%20are%20based%20on%20an%20implicit%20assumption%20of%20space-invariant%20isotropic%20Gaussian%20noise.%20However%2C%20this%20assumption%20is%20violated%20in%20practice%20in%20applications%20such%20as%20single%20molecule%20localization%20microscopy%20%28SMLM%29.%20To%20address%20this%20issue%2C%20we%20propose%20to%20introduce%20an%20explicit%20localization%20noise%20model%20that%20decouples%20shape%20modeling%20with%20the%20GMM%20from%20noise%20handling.%20We%20design%20a%20stochastic%20EM%20algorithm%20that%20considers%20noise-free%20data%20as%20a%20latent%20variable%2C%20with%20closed-form%20solutions%20at%20each%20EM%20step.%20The%20first%20advantage%20of%20our%20approach%20is%20to%20handle%20space-variant%20and%20anisotropic%20Gaussian%20noise%20with%20arbitrary%20covariances.%20The%20second%20advantage%20is%20to%20leverage%20the%20explicit%20noise%20model%20to%20impose%20prior%20knowledge%20about%20the%20noise%20that%20may%20be%20available%20from%20physical%20sensors.%20We%20show%20on%20various%20simulated%20data%20that%20our%20noise%20handling%20strategy%20improves%20significantly%20the%20robustness%20to%20high%20levels%20of%20anisotropic%20noise.%20We%20also%20demonstrate%20the%20performance%20of%20our%20method%20on%20real%20SMLM%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2201.00708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiview%2520point%2520cloud%2520registration%2520with%2520anisotropic%2520and%2520space-varying%2520localization%2520noise%26entry.906535625%3DDenis%2520Fortun%2520and%2520Etienne%2520Baudrier%2520and%2520Fabian%2520Zwettler%2520and%2520Markus%2520Sauer%2520and%2520Sylvain%2520Faisan%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520address%2520the%2520problem%2520of%2520registering%2520multiple%2520point%2520clouds%2520corrupted%2520with%2520high%2520anisotropic%2520localization%2520noise.%2520Our%2520approach%2520follows%2520the%2520widely%2520used%2520framework%2520of%2520Gaussian%2520mixture%2520model%2520%2528GMM%2529%2520reconstruction%2520with%2520an%2520expectation-maximization%2520%2528EM%2529%2520algorithm.%2520Existing%2520methods%2520are%2520based%2520on%2520an%2520implicit%2520assumption%2520of%2520space-invariant%2520isotropic%2520Gaussian%2520noise.%2520However%252C%2520this%2520assumption%2520is%2520violated%2520in%2520practice%2520in%2520applications%2520such%2520as%2520single%2520molecule%2520localization%2520microscopy%2520%2528SMLM%2529.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520to%2520introduce%2520an%2520explicit%2520localization%2520noise%2520model%2520that%2520decouples%2520shape%2520modeling%2520with%2520the%2520GMM%2520from%2520noise%2520handling.%2520We%2520design%2520a%2520stochastic%2520EM%2520algorithm%2520that%2520considers%2520noise-free%2520data%2520as%2520a%2520latent%2520variable%252C%2520with%2520closed-form%2520solutions%2520at%2520each%2520EM%2520step.%2520The%2520first%2520advantage%2520of%2520our%2520approach%2520is%2520to%2520handle%2520space-variant%2520and%2520anisotropic%2520Gaussian%2520noise%2520with%2520arbitrary%2520covariances.%2520The%2520second%2520advantage%2520is%2520to%2520leverage%2520the%2520explicit%2520noise%2520model%2520to%2520impose%2520prior%2520knowledge%2520about%2520the%2520noise%2520that%2520may%2520be%2520available%2520from%2520physical%2520sensors.%2520We%2520show%2520on%2520various%2520simulated%2520data%2520that%2520our%2520noise%2520handling%2520strategy%2520improves%2520significantly%2520the%2520robustness%2520to%2520high%2520levels%2520of%2520anisotropic%2520noise.%2520We%2520also%2520demonstrate%2520the%2520performance%2520of%2520our%2520method%2520on%2520real%2520SMLM%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.00708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiview%20point%20cloud%20registration%20with%20anisotropic%20and%20space-varying%20localization%20noise&entry.906535625=Denis%20Fortun%20and%20Etienne%20Baudrier%20and%20Fabian%20Zwettler%20and%20Markus%20Sauer%20and%20Sylvain%20Faisan&entry.1292438233=In%20this%20paper%2C%20we%20address%20the%20problem%20of%20registering%20multiple%20point%20clouds%20corrupted%20with%20high%20anisotropic%20localization%20noise.%20Our%20approach%20follows%20the%20widely%20used%20framework%20of%20Gaussian%20mixture%20model%20%28GMM%29%20reconstruction%20with%20an%20expectation-maximization%20%28EM%29%20algorithm.%20Existing%20methods%20are%20based%20on%20an%20implicit%20assumption%20of%20space-invariant%20isotropic%20Gaussian%20noise.%20However%2C%20this%20assumption%20is%20violated%20in%20practice%20in%20applications%20such%20as%20single%20molecule%20localization%20microscopy%20%28SMLM%29.%20To%20address%20this%20issue%2C%20we%20propose%20to%20introduce%20an%20explicit%20localization%20noise%20model%20that%20decouples%20shape%20modeling%20with%20the%20GMM%20from%20noise%20handling.%20We%20design%20a%20stochastic%20EM%20algorithm%20that%20considers%20noise-free%20data%20as%20a%20latent%20variable%2C%20with%20closed-form%20solutions%20at%20each%20EM%20step.%20The%20first%20advantage%20of%20our%20approach%20is%20to%20handle%20space-variant%20and%20anisotropic%20Gaussian%20noise%20with%20arbitrary%20covariances.%20The%20second%20advantage%20is%20to%20leverage%20the%20explicit%20noise%20model%20to%20impose%20prior%20knowledge%20about%20the%20noise%20that%20may%20be%20available%20from%20physical%20sensors.%20We%20show%20on%20various%20simulated%20data%20that%20our%20noise%20handling%20strategy%20improves%20significantly%20the%20robustness%20to%20high%20levels%20of%20anisotropic%20noise.%20We%20also%20demonstrate%20the%20performance%20of%20our%20method%20on%20real%20SMLM%20data.&entry.1838667208=http%3A//arxiv.org/abs/2201.00708v2&entry.124074799=Read"},
{"title": "Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions", "author": "Giulia Marchiori Pietrosanti and Giulio Rossolini and Alessandro Biondi and Giorgio Buttazzo", "abstract": "The robustness of deep neural networks is a crucial factor in safety-critical applications, particularly in complex and dynamic environments (e.g., medical or driving scenarios) where localized corruptions can arise. While previous studies have evaluated the robustness of semantic segmentation (SS) models under whole-image natural or adversarial corruptions, a comprehensive investigation into the spatial robustness of dense vision models under localized corruptions remains underexplored. This paper fills this gap by introducing novel, region-aware metrics for benchmarking the spatial robustness of segmentation models, along with an evaluation framework to assess the impact of natural localized corruptions. Furthermore, it uncovers the inherent complexity of evaluating worst-case spatial robustness using only a single localized adversarial attack. To address this, the work proposes a region-aware multi-attack adversarial analysis to systematically assess model robustness across specific image regions. The proposed metrics and analysis were exploited to evaluate 14 segmentation models in driving scenarios, uncovering key insights into the effects of localized corruption in both natural and adversarial forms. The results reveal that models respond to these two types of threats differently; for instance, transformer-based segmentation models demonstrate notable robustness to localized natural corruptions but are highly vulnerable to adversarial ones, and vice versa for CNN-based models. Consequently, we also address the challenge of balancing robustness to both natural and adversarial localized corruptions by means of ensemble models, thereby achieving a broader threat coverage and improved reliability for dense vision tasks.", "link": "http://arxiv.org/abs/2504.01632v3", "date": "2025-11-24", "relevancy": 2.7402, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.577}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20the%20Spatial%20Robustness%20of%20DNNs%20via%20Natural%20and%20Adversarial%20Localized%20Corruptions&body=Title%3A%20Benchmarking%20the%20Spatial%20Robustness%20of%20DNNs%20via%20Natural%20and%20Adversarial%20Localized%20Corruptions%0AAuthor%3A%20Giulia%20Marchiori%20Pietrosanti%20and%20Giulio%20Rossolini%20and%20Alessandro%20Biondi%20and%20Giorgio%20Buttazzo%0AAbstract%3A%20The%20robustness%20of%20deep%20neural%20networks%20is%20a%20crucial%20factor%20in%20safety-critical%20applications%2C%20particularly%20in%20complex%20and%20dynamic%20environments%20%28e.g.%2C%20medical%20or%20driving%20scenarios%29%20where%20localized%20corruptions%20can%20arise.%20While%20previous%20studies%20have%20evaluated%20the%20robustness%20of%20semantic%20segmentation%20%28SS%29%20models%20under%20whole-image%20natural%20or%20adversarial%20corruptions%2C%20a%20comprehensive%20investigation%20into%20the%20spatial%20robustness%20of%20dense%20vision%20models%20under%20localized%20corruptions%20remains%20underexplored.%20This%20paper%20fills%20this%20gap%20by%20introducing%20novel%2C%20region-aware%20metrics%20for%20benchmarking%20the%20spatial%20robustness%20of%20segmentation%20models%2C%20along%20with%20an%20evaluation%20framework%20to%20assess%20the%20impact%20of%20natural%20localized%20corruptions.%20Furthermore%2C%20it%20uncovers%20the%20inherent%20complexity%20of%20evaluating%20worst-case%20spatial%20robustness%20using%20only%20a%20single%20localized%20adversarial%20attack.%20To%20address%20this%2C%20the%20work%20proposes%20a%20region-aware%20multi-attack%20adversarial%20analysis%20to%20systematically%20assess%20model%20robustness%20across%20specific%20image%20regions.%20The%20proposed%20metrics%20and%20analysis%20were%20exploited%20to%20evaluate%2014%20segmentation%20models%20in%20driving%20scenarios%2C%20uncovering%20key%20insights%20into%20the%20effects%20of%20localized%20corruption%20in%20both%20natural%20and%20adversarial%20forms.%20The%20results%20reveal%20that%20models%20respond%20to%20these%20two%20types%20of%20threats%20differently%3B%20for%20instance%2C%20transformer-based%20segmentation%20models%20demonstrate%20notable%20robustness%20to%20localized%20natural%20corruptions%20but%20are%20highly%20vulnerable%20to%20adversarial%20ones%2C%20and%20vice%20versa%20for%20CNN-based%20models.%20Consequently%2C%20we%20also%20address%20the%20challenge%20of%20balancing%20robustness%20to%20both%20natural%20and%20adversarial%20localized%20corruptions%20by%20means%20of%20ensemble%20models%2C%20thereby%20achieving%20a%20broader%20threat%20coverage%20and%20improved%20reliability%20for%20dense%20vision%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2504.01632v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520the%2520Spatial%2520Robustness%2520of%2520DNNs%2520via%2520Natural%2520and%2520Adversarial%2520Localized%2520Corruptions%26entry.906535625%3DGiulia%2520Marchiori%2520Pietrosanti%2520and%2520Giulio%2520Rossolini%2520and%2520Alessandro%2520Biondi%2520and%2520Giorgio%2520Buttazzo%26entry.1292438233%3DThe%2520robustness%2520of%2520deep%2520neural%2520networks%2520is%2520a%2520crucial%2520factor%2520in%2520safety-critical%2520applications%252C%2520particularly%2520in%2520complex%2520and%2520dynamic%2520environments%2520%2528e.g.%252C%2520medical%2520or%2520driving%2520scenarios%2529%2520where%2520localized%2520corruptions%2520can%2520arise.%2520While%2520previous%2520studies%2520have%2520evaluated%2520the%2520robustness%2520of%2520semantic%2520segmentation%2520%2528SS%2529%2520models%2520under%2520whole-image%2520natural%2520or%2520adversarial%2520corruptions%252C%2520a%2520comprehensive%2520investigation%2520into%2520the%2520spatial%2520robustness%2520of%2520dense%2520vision%2520models%2520under%2520localized%2520corruptions%2520remains%2520underexplored.%2520This%2520paper%2520fills%2520this%2520gap%2520by%2520introducing%2520novel%252C%2520region-aware%2520metrics%2520for%2520benchmarking%2520the%2520spatial%2520robustness%2520of%2520segmentation%2520models%252C%2520along%2520with%2520an%2520evaluation%2520framework%2520to%2520assess%2520the%2520impact%2520of%2520natural%2520localized%2520corruptions.%2520Furthermore%252C%2520it%2520uncovers%2520the%2520inherent%2520complexity%2520of%2520evaluating%2520worst-case%2520spatial%2520robustness%2520using%2520only%2520a%2520single%2520localized%2520adversarial%2520attack.%2520To%2520address%2520this%252C%2520the%2520work%2520proposes%2520a%2520region-aware%2520multi-attack%2520adversarial%2520analysis%2520to%2520systematically%2520assess%2520model%2520robustness%2520across%2520specific%2520image%2520regions.%2520The%2520proposed%2520metrics%2520and%2520analysis%2520were%2520exploited%2520to%2520evaluate%252014%2520segmentation%2520models%2520in%2520driving%2520scenarios%252C%2520uncovering%2520key%2520insights%2520into%2520the%2520effects%2520of%2520localized%2520corruption%2520in%2520both%2520natural%2520and%2520adversarial%2520forms.%2520The%2520results%2520reveal%2520that%2520models%2520respond%2520to%2520these%2520two%2520types%2520of%2520threats%2520differently%253B%2520for%2520instance%252C%2520transformer-based%2520segmentation%2520models%2520demonstrate%2520notable%2520robustness%2520to%2520localized%2520natural%2520corruptions%2520but%2520are%2520highly%2520vulnerable%2520to%2520adversarial%2520ones%252C%2520and%2520vice%2520versa%2520for%2520CNN-based%2520models.%2520Consequently%252C%2520we%2520also%2520address%2520the%2520challenge%2520of%2520balancing%2520robustness%2520to%2520both%2520natural%2520and%2520adversarial%2520localized%2520corruptions%2520by%2520means%2520of%2520ensemble%2520models%252C%2520thereby%2520achieving%2520a%2520broader%2520threat%2520coverage%2520and%2520improved%2520reliability%2520for%2520dense%2520vision%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.01632v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20the%20Spatial%20Robustness%20of%20DNNs%20via%20Natural%20and%20Adversarial%20Localized%20Corruptions&entry.906535625=Giulia%20Marchiori%20Pietrosanti%20and%20Giulio%20Rossolini%20and%20Alessandro%20Biondi%20and%20Giorgio%20Buttazzo&entry.1292438233=The%20robustness%20of%20deep%20neural%20networks%20is%20a%20crucial%20factor%20in%20safety-critical%20applications%2C%20particularly%20in%20complex%20and%20dynamic%20environments%20%28e.g.%2C%20medical%20or%20driving%20scenarios%29%20where%20localized%20corruptions%20can%20arise.%20While%20previous%20studies%20have%20evaluated%20the%20robustness%20of%20semantic%20segmentation%20%28SS%29%20models%20under%20whole-image%20natural%20or%20adversarial%20corruptions%2C%20a%20comprehensive%20investigation%20into%20the%20spatial%20robustness%20of%20dense%20vision%20models%20under%20localized%20corruptions%20remains%20underexplored.%20This%20paper%20fills%20this%20gap%20by%20introducing%20novel%2C%20region-aware%20metrics%20for%20benchmarking%20the%20spatial%20robustness%20of%20segmentation%20models%2C%20along%20with%20an%20evaluation%20framework%20to%20assess%20the%20impact%20of%20natural%20localized%20corruptions.%20Furthermore%2C%20it%20uncovers%20the%20inherent%20complexity%20of%20evaluating%20worst-case%20spatial%20robustness%20using%20only%20a%20single%20localized%20adversarial%20attack.%20To%20address%20this%2C%20the%20work%20proposes%20a%20region-aware%20multi-attack%20adversarial%20analysis%20to%20systematically%20assess%20model%20robustness%20across%20specific%20image%20regions.%20The%20proposed%20metrics%20and%20analysis%20were%20exploited%20to%20evaluate%2014%20segmentation%20models%20in%20driving%20scenarios%2C%20uncovering%20key%20insights%20into%20the%20effects%20of%20localized%20corruption%20in%20both%20natural%20and%20adversarial%20forms.%20The%20results%20reveal%20that%20models%20respond%20to%20these%20two%20types%20of%20threats%20differently%3B%20for%20instance%2C%20transformer-based%20segmentation%20models%20demonstrate%20notable%20robustness%20to%20localized%20natural%20corruptions%20but%20are%20highly%20vulnerable%20to%20adversarial%20ones%2C%20and%20vice%20versa%20for%20CNN-based%20models.%20Consequently%2C%20we%20also%20address%20the%20challenge%20of%20balancing%20robustness%20to%20both%20natural%20and%20adversarial%20localized%20corruptions%20by%20means%20of%20ensemble%20models%2C%20thereby%20achieving%20a%20broader%20threat%20coverage%20and%20improved%20reliability%20for%20dense%20vision%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2504.01632v3&entry.124074799=Read"},
{"title": "Enhancing Domain-Specific Encoder Models with LLM-Generated Data: How to Leverage Ontologies, and How to Do Without Them", "author": "Marc Brinner and Tarek Al Mustafa and Sina Zarrie\u00df", "abstract": "We investigate the use of LLM-generated data for continual pretraining of encoder models in specialized domains with limited training data, using the scientific domain of invasion biology as a case study. To this end, we leverage domain-specific ontologies by enriching them with LLM-generated data and pretraining the encoder model as an ontology-informed embedding model for concept definitions. To evaluate the effectiveness of this method, we compile a benchmark specifically designed for assessing model performance in invasion biology. After demonstrating substantial improvements over standard LLM pretraining, we investigate the feasibility of applying the proposed approach to domains without comprehensive ontologies by substituting ontological concepts with concepts automatically extracted from a small corpus of scientific abstracts and establishing relationships between concepts through distributional statistics. Our results demonstrate that this automated approach achieves comparable performance using only a small set of scientific abstracts, resulting in a fully automated pipeline for enhancing domain-specific understanding of small encoder models that is especially suited for application in low-resource settings and achieves performance comparable to masked language modeling pretraining on much larger datasets.", "link": "http://arxiv.org/abs/2503.22006v2", "date": "2025-11-24", "relevancy": 2.7248, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5648}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Domain-Specific%20Encoder%20Models%20with%20LLM-Generated%20Data%3A%20How%20to%20Leverage%20Ontologies%2C%20and%20How%20to%20Do%20Without%20Them&body=Title%3A%20Enhancing%20Domain-Specific%20Encoder%20Models%20with%20LLM-Generated%20Data%3A%20How%20to%20Leverage%20Ontologies%2C%20and%20How%20to%20Do%20Without%20Them%0AAuthor%3A%20Marc%20Brinner%20and%20Tarek%20Al%20Mustafa%20and%20Sina%20Zarrie%C3%9F%0AAbstract%3A%20We%20investigate%20the%20use%20of%20LLM-generated%20data%20for%20continual%20pretraining%20of%20encoder%20models%20in%20specialized%20domains%20with%20limited%20training%20data%2C%20using%20the%20scientific%20domain%20of%20invasion%20biology%20as%20a%20case%20study.%20To%20this%20end%2C%20we%20leverage%20domain-specific%20ontologies%20by%20enriching%20them%20with%20LLM-generated%20data%20and%20pretraining%20the%20encoder%20model%20as%20an%20ontology-informed%20embedding%20model%20for%20concept%20definitions.%20To%20evaluate%20the%20effectiveness%20of%20this%20method%2C%20we%20compile%20a%20benchmark%20specifically%20designed%20for%20assessing%20model%20performance%20in%20invasion%20biology.%20After%20demonstrating%20substantial%20improvements%20over%20standard%20LLM%20pretraining%2C%20we%20investigate%20the%20feasibility%20of%20applying%20the%20proposed%20approach%20to%20domains%20without%20comprehensive%20ontologies%20by%20substituting%20ontological%20concepts%20with%20concepts%20automatically%20extracted%20from%20a%20small%20corpus%20of%20scientific%20abstracts%20and%20establishing%20relationships%20between%20concepts%20through%20distributional%20statistics.%20Our%20results%20demonstrate%20that%20this%20automated%20approach%20achieves%20comparable%20performance%20using%20only%20a%20small%20set%20of%20scientific%20abstracts%2C%20resulting%20in%20a%20fully%20automated%20pipeline%20for%20enhancing%20domain-specific%20understanding%20of%20small%20encoder%20models%20that%20is%20especially%20suited%20for%20application%20in%20low-resource%20settings%20and%20achieves%20performance%20comparable%20to%20masked%20language%20modeling%20pretraining%20on%20much%20larger%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2503.22006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Domain-Specific%2520Encoder%2520Models%2520with%2520LLM-Generated%2520Data%253A%2520How%2520to%2520Leverage%2520Ontologies%252C%2520and%2520How%2520to%2520Do%2520Without%2520Them%26entry.906535625%3DMarc%2520Brinner%2520and%2520Tarek%2520Al%2520Mustafa%2520and%2520Sina%2520Zarrie%25C3%259F%26entry.1292438233%3DWe%2520investigate%2520the%2520use%2520of%2520LLM-generated%2520data%2520for%2520continual%2520pretraining%2520of%2520encoder%2520models%2520in%2520specialized%2520domains%2520with%2520limited%2520training%2520data%252C%2520using%2520the%2520scientific%2520domain%2520of%2520invasion%2520biology%2520as%2520a%2520case%2520study.%2520To%2520this%2520end%252C%2520we%2520leverage%2520domain-specific%2520ontologies%2520by%2520enriching%2520them%2520with%2520LLM-generated%2520data%2520and%2520pretraining%2520the%2520encoder%2520model%2520as%2520an%2520ontology-informed%2520embedding%2520model%2520for%2520concept%2520definitions.%2520To%2520evaluate%2520the%2520effectiveness%2520of%2520this%2520method%252C%2520we%2520compile%2520a%2520benchmark%2520specifically%2520designed%2520for%2520assessing%2520model%2520performance%2520in%2520invasion%2520biology.%2520After%2520demonstrating%2520substantial%2520improvements%2520over%2520standard%2520LLM%2520pretraining%252C%2520we%2520investigate%2520the%2520feasibility%2520of%2520applying%2520the%2520proposed%2520approach%2520to%2520domains%2520without%2520comprehensive%2520ontologies%2520by%2520substituting%2520ontological%2520concepts%2520with%2520concepts%2520automatically%2520extracted%2520from%2520a%2520small%2520corpus%2520of%2520scientific%2520abstracts%2520and%2520establishing%2520relationships%2520between%2520concepts%2520through%2520distributional%2520statistics.%2520Our%2520results%2520demonstrate%2520that%2520this%2520automated%2520approach%2520achieves%2520comparable%2520performance%2520using%2520only%2520a%2520small%2520set%2520of%2520scientific%2520abstracts%252C%2520resulting%2520in%2520a%2520fully%2520automated%2520pipeline%2520for%2520enhancing%2520domain-specific%2520understanding%2520of%2520small%2520encoder%2520models%2520that%2520is%2520especially%2520suited%2520for%2520application%2520in%2520low-resource%2520settings%2520and%2520achieves%2520performance%2520comparable%2520to%2520masked%2520language%2520modeling%2520pretraining%2520on%2520much%2520larger%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Domain-Specific%20Encoder%20Models%20with%20LLM-Generated%20Data%3A%20How%20to%20Leverage%20Ontologies%2C%20and%20How%20to%20Do%20Without%20Them&entry.906535625=Marc%20Brinner%20and%20Tarek%20Al%20Mustafa%20and%20Sina%20Zarrie%C3%9F&entry.1292438233=We%20investigate%20the%20use%20of%20LLM-generated%20data%20for%20continual%20pretraining%20of%20encoder%20models%20in%20specialized%20domains%20with%20limited%20training%20data%2C%20using%20the%20scientific%20domain%20of%20invasion%20biology%20as%20a%20case%20study.%20To%20this%20end%2C%20we%20leverage%20domain-specific%20ontologies%20by%20enriching%20them%20with%20LLM-generated%20data%20and%20pretraining%20the%20encoder%20model%20as%20an%20ontology-informed%20embedding%20model%20for%20concept%20definitions.%20To%20evaluate%20the%20effectiveness%20of%20this%20method%2C%20we%20compile%20a%20benchmark%20specifically%20designed%20for%20assessing%20model%20performance%20in%20invasion%20biology.%20After%20demonstrating%20substantial%20improvements%20over%20standard%20LLM%20pretraining%2C%20we%20investigate%20the%20feasibility%20of%20applying%20the%20proposed%20approach%20to%20domains%20without%20comprehensive%20ontologies%20by%20substituting%20ontological%20concepts%20with%20concepts%20automatically%20extracted%20from%20a%20small%20corpus%20of%20scientific%20abstracts%20and%20establishing%20relationships%20between%20concepts%20through%20distributional%20statistics.%20Our%20results%20demonstrate%20that%20this%20automated%20approach%20achieves%20comparable%20performance%20using%20only%20a%20small%20set%20of%20scientific%20abstracts%2C%20resulting%20in%20a%20fully%20automated%20pipeline%20for%20enhancing%20domain-specific%20understanding%20of%20small%20encoder%20models%20that%20is%20especially%20suited%20for%20application%20in%20low-resource%20settings%20and%20achieves%20performance%20comparable%20to%20masked%20language%20modeling%20pretraining%20on%20much%20larger%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2503.22006v2&entry.124074799=Read"},
{"title": "Teacher Encoder-Student Decoder Denoising Guided Segmentation Network for Anomaly Detection", "author": "Shixuan Song and Hao Chen and Shu Hu and Xin Wang and Jinrong Hu and Xi Wu", "abstract": "Visual anomaly detection is a highly challenging task, often categorized as a one-class classification and segmentation problem. Recent studies have demonstrated that the student-teacher (S-T) framework effectively addresses this challenge. However, most S-T frameworks rely solely on pre-trained teacher networks to guide student networks in learning multi-scale similar features, overlooking the potential of the student networks to enhance learning through multi-scale feature fusion. In this study, we propose a novel model named PFADSeg, which integrates a pre-trained teacher network, a denoising student network with multi-scale feature fusion, and a guided anomaly segmentation network into a unified framework. By adopting a unique teacher-encoder and student-decoder denoising mode, the model improves the student network's ability to learn from teacher network features. Furthermore, an adaptive feature fusion mechanism is introduced to train a self-supervised segmentation network that synthesizes anomaly masks autonomously, significantly increasing detection performance. Rigorous evaluations on the widely-used MVTec AD dataset demonstrate that PFADSeg exhibits excellent performance, achieving an image-level AUC of 98.9%, a pixel-level mean precision of 76.4%, and an instance-level mean precision of 78.7%.", "link": "http://arxiv.org/abs/2501.12104v4", "date": "2025-11-24", "relevancy": 2.6784, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5563}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5342}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teacher%20Encoder-Student%20Decoder%20Denoising%20Guided%20Segmentation%20Network%20for%20Anomaly%20Detection&body=Title%3A%20Teacher%20Encoder-Student%20Decoder%20Denoising%20Guided%20Segmentation%20Network%20for%20Anomaly%20Detection%0AAuthor%3A%20Shixuan%20Song%20and%20Hao%20Chen%20and%20Shu%20Hu%20and%20Xin%20Wang%20and%20Jinrong%20Hu%20and%20Xi%20Wu%0AAbstract%3A%20Visual%20anomaly%20detection%20is%20a%20highly%20challenging%20task%2C%20often%20categorized%20as%20a%20one-class%20classification%20and%20segmentation%20problem.%20Recent%20studies%20have%20demonstrated%20that%20the%20student-teacher%20%28S-T%29%20framework%20effectively%20addresses%20this%20challenge.%20However%2C%20most%20S-T%20frameworks%20rely%20solely%20on%20pre-trained%20teacher%20networks%20to%20guide%20student%20networks%20in%20learning%20multi-scale%20similar%20features%2C%20overlooking%20the%20potential%20of%20the%20student%20networks%20to%20enhance%20learning%20through%20multi-scale%20feature%20fusion.%20In%20this%20study%2C%20we%20propose%20a%20novel%20model%20named%20PFADSeg%2C%20which%20integrates%20a%20pre-trained%20teacher%20network%2C%20a%20denoising%20student%20network%20with%20multi-scale%20feature%20fusion%2C%20and%20a%20guided%20anomaly%20segmentation%20network%20into%20a%20unified%20framework.%20By%20adopting%20a%20unique%20teacher-encoder%20and%20student-decoder%20denoising%20mode%2C%20the%20model%20improves%20the%20student%20network%27s%20ability%20to%20learn%20from%20teacher%20network%20features.%20Furthermore%2C%20an%20adaptive%20feature%20fusion%20mechanism%20is%20introduced%20to%20train%20a%20self-supervised%20segmentation%20network%20that%20synthesizes%20anomaly%20masks%20autonomously%2C%20significantly%20increasing%20detection%20performance.%20Rigorous%20evaluations%20on%20the%20widely-used%20MVTec%20AD%20dataset%20demonstrate%20that%20PFADSeg%20exhibits%20excellent%20performance%2C%20achieving%20an%20image-level%20AUC%20of%2098.9%25%2C%20a%20pixel-level%20mean%20precision%20of%2076.4%25%2C%20and%20an%20instance-level%20mean%20precision%20of%2078.7%25.%0ALink%3A%20http%3A//arxiv.org/abs/2501.12104v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeacher%2520Encoder-Student%2520Decoder%2520Denoising%2520Guided%2520Segmentation%2520Network%2520for%2520Anomaly%2520Detection%26entry.906535625%3DShixuan%2520Song%2520and%2520Hao%2520Chen%2520and%2520Shu%2520Hu%2520and%2520Xin%2520Wang%2520and%2520Jinrong%2520Hu%2520and%2520Xi%2520Wu%26entry.1292438233%3DVisual%2520anomaly%2520detection%2520is%2520a%2520highly%2520challenging%2520task%252C%2520often%2520categorized%2520as%2520a%2520one-class%2520classification%2520and%2520segmentation%2520problem.%2520Recent%2520studies%2520have%2520demonstrated%2520that%2520the%2520student-teacher%2520%2528S-T%2529%2520framework%2520effectively%2520addresses%2520this%2520challenge.%2520However%252C%2520most%2520S-T%2520frameworks%2520rely%2520solely%2520on%2520pre-trained%2520teacher%2520networks%2520to%2520guide%2520student%2520networks%2520in%2520learning%2520multi-scale%2520similar%2520features%252C%2520overlooking%2520the%2520potential%2520of%2520the%2520student%2520networks%2520to%2520enhance%2520learning%2520through%2520multi-scale%2520feature%2520fusion.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520model%2520named%2520PFADSeg%252C%2520which%2520integrates%2520a%2520pre-trained%2520teacher%2520network%252C%2520a%2520denoising%2520student%2520network%2520with%2520multi-scale%2520feature%2520fusion%252C%2520and%2520a%2520guided%2520anomaly%2520segmentation%2520network%2520into%2520a%2520unified%2520framework.%2520By%2520adopting%2520a%2520unique%2520teacher-encoder%2520and%2520student-decoder%2520denoising%2520mode%252C%2520the%2520model%2520improves%2520the%2520student%2520network%2527s%2520ability%2520to%2520learn%2520from%2520teacher%2520network%2520features.%2520Furthermore%252C%2520an%2520adaptive%2520feature%2520fusion%2520mechanism%2520is%2520introduced%2520to%2520train%2520a%2520self-supervised%2520segmentation%2520network%2520that%2520synthesizes%2520anomaly%2520masks%2520autonomously%252C%2520significantly%2520increasing%2520detection%2520performance.%2520Rigorous%2520evaluations%2520on%2520the%2520widely-used%2520MVTec%2520AD%2520dataset%2520demonstrate%2520that%2520PFADSeg%2520exhibits%2520excellent%2520performance%252C%2520achieving%2520an%2520image-level%2520AUC%2520of%252098.9%2525%252C%2520a%2520pixel-level%2520mean%2520precision%2520of%252076.4%2525%252C%2520and%2520an%2520instance-level%2520mean%2520precision%2520of%252078.7%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12104v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teacher%20Encoder-Student%20Decoder%20Denoising%20Guided%20Segmentation%20Network%20for%20Anomaly%20Detection&entry.906535625=Shixuan%20Song%20and%20Hao%20Chen%20and%20Shu%20Hu%20and%20Xin%20Wang%20and%20Jinrong%20Hu%20and%20Xi%20Wu&entry.1292438233=Visual%20anomaly%20detection%20is%20a%20highly%20challenging%20task%2C%20often%20categorized%20as%20a%20one-class%20classification%20and%20segmentation%20problem.%20Recent%20studies%20have%20demonstrated%20that%20the%20student-teacher%20%28S-T%29%20framework%20effectively%20addresses%20this%20challenge.%20However%2C%20most%20S-T%20frameworks%20rely%20solely%20on%20pre-trained%20teacher%20networks%20to%20guide%20student%20networks%20in%20learning%20multi-scale%20similar%20features%2C%20overlooking%20the%20potential%20of%20the%20student%20networks%20to%20enhance%20learning%20through%20multi-scale%20feature%20fusion.%20In%20this%20study%2C%20we%20propose%20a%20novel%20model%20named%20PFADSeg%2C%20which%20integrates%20a%20pre-trained%20teacher%20network%2C%20a%20denoising%20student%20network%20with%20multi-scale%20feature%20fusion%2C%20and%20a%20guided%20anomaly%20segmentation%20network%20into%20a%20unified%20framework.%20By%20adopting%20a%20unique%20teacher-encoder%20and%20student-decoder%20denoising%20mode%2C%20the%20model%20improves%20the%20student%20network%27s%20ability%20to%20learn%20from%20teacher%20network%20features.%20Furthermore%2C%20an%20adaptive%20feature%20fusion%20mechanism%20is%20introduced%20to%20train%20a%20self-supervised%20segmentation%20network%20that%20synthesizes%20anomaly%20masks%20autonomously%2C%20significantly%20increasing%20detection%20performance.%20Rigorous%20evaluations%20on%20the%20widely-used%20MVTec%20AD%20dataset%20demonstrate%20that%20PFADSeg%20exhibits%20excellent%20performance%2C%20achieving%20an%20image-level%20AUC%20of%2098.9%25%2C%20a%20pixel-level%20mean%20precision%20of%2076.4%25%2C%20and%20an%20instance-level%20mean%20precision%20of%2078.7%25.&entry.1838667208=http%3A//arxiv.org/abs/2501.12104v4&entry.124074799=Read"},
{"title": "POUR: A Provably Optimal Method for Unlearning Representations via Neural Collapse", "author": "Anjie Le and Can Peng and Yuyuan Liu and J. Alison Noble", "abstract": "In computer vision, machine unlearning aims to remove the influence of specific visual concepts or training images without retraining from scratch. Studies show that existing approaches often modify the classifier while leaving internal representations intact, resulting in incomplete forgetting. In this work, we extend the notion of unlearning to the representation level, deriving a three-term interplay between forgetting efficacy, retention fidelity, and class separation. Building on Neural Collapse theory, we show that the orthogonal projection of a simplex Equiangular Tight Frame (ETF) remains an ETF in a lower dimensional space, yielding a provably optimal forgetting operator. We further introduce the Representation Unlearning Score (RUS) to quantify representation-level forgetting and retention fidelity. Building on this, we introduce POUR (Provably Optimal Unlearning of Representations), a geometric projection method with closed-form (POUR-P) and a feature-level unlearning variant under a distillation scheme (POUR-D). Experiments on CIFAR-10/100 and PathMNIST demonstrate that POUR achieves effective unlearning while preserving retained knowledge, outperforming state-of-the-art unlearning methods on both classification-level and representation-level metrics.", "link": "http://arxiv.org/abs/2511.19339v1", "date": "2025-11-24", "relevancy": 2.6781, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5697}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5201}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20POUR%3A%20A%20Provably%20Optimal%20Method%20for%20Unlearning%20Representations%20via%20Neural%20Collapse&body=Title%3A%20POUR%3A%20A%20Provably%20Optimal%20Method%20for%20Unlearning%20Representations%20via%20Neural%20Collapse%0AAuthor%3A%20Anjie%20Le%20and%20Can%20Peng%20and%20Yuyuan%20Liu%20and%20J.%20Alison%20Noble%0AAbstract%3A%20In%20computer%20vision%2C%20machine%20unlearning%20aims%20to%20remove%20the%20influence%20of%20specific%20visual%20concepts%20or%20training%20images%20without%20retraining%20from%20scratch.%20Studies%20show%20that%20existing%20approaches%20often%20modify%20the%20classifier%20while%20leaving%20internal%20representations%20intact%2C%20resulting%20in%20incomplete%20forgetting.%20In%20this%20work%2C%20we%20extend%20the%20notion%20of%20unlearning%20to%20the%20representation%20level%2C%20deriving%20a%20three-term%20interplay%20between%20forgetting%20efficacy%2C%20retention%20fidelity%2C%20and%20class%20separation.%20Building%20on%20Neural%20Collapse%20theory%2C%20we%20show%20that%20the%20orthogonal%20projection%20of%20a%20simplex%20Equiangular%20Tight%20Frame%20%28ETF%29%20remains%20an%20ETF%20in%20a%20lower%20dimensional%20space%2C%20yielding%20a%20provably%20optimal%20forgetting%20operator.%20We%20further%20introduce%20the%20Representation%20Unlearning%20Score%20%28RUS%29%20to%20quantify%20representation-level%20forgetting%20and%20retention%20fidelity.%20Building%20on%20this%2C%20we%20introduce%20POUR%20%28Provably%20Optimal%20Unlearning%20of%20Representations%29%2C%20a%20geometric%20projection%20method%20with%20closed-form%20%28POUR-P%29%20and%20a%20feature-level%20unlearning%20variant%20under%20a%20distillation%20scheme%20%28POUR-D%29.%20Experiments%20on%20CIFAR-10/100%20and%20PathMNIST%20demonstrate%20that%20POUR%20achieves%20effective%20unlearning%20while%20preserving%20retained%20knowledge%2C%20outperforming%20state-of-the-art%20unlearning%20methods%20on%20both%20classification-level%20and%20representation-level%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPOUR%253A%2520A%2520Provably%2520Optimal%2520Method%2520for%2520Unlearning%2520Representations%2520via%2520Neural%2520Collapse%26entry.906535625%3DAnjie%2520Le%2520and%2520Can%2520Peng%2520and%2520Yuyuan%2520Liu%2520and%2520J.%2520Alison%2520Noble%26entry.1292438233%3DIn%2520computer%2520vision%252C%2520machine%2520unlearning%2520aims%2520to%2520remove%2520the%2520influence%2520of%2520specific%2520visual%2520concepts%2520or%2520training%2520images%2520without%2520retraining%2520from%2520scratch.%2520Studies%2520show%2520that%2520existing%2520approaches%2520often%2520modify%2520the%2520classifier%2520while%2520leaving%2520internal%2520representations%2520intact%252C%2520resulting%2520in%2520incomplete%2520forgetting.%2520In%2520this%2520work%252C%2520we%2520extend%2520the%2520notion%2520of%2520unlearning%2520to%2520the%2520representation%2520level%252C%2520deriving%2520a%2520three-term%2520interplay%2520between%2520forgetting%2520efficacy%252C%2520retention%2520fidelity%252C%2520and%2520class%2520separation.%2520Building%2520on%2520Neural%2520Collapse%2520theory%252C%2520we%2520show%2520that%2520the%2520orthogonal%2520projection%2520of%2520a%2520simplex%2520Equiangular%2520Tight%2520Frame%2520%2528ETF%2529%2520remains%2520an%2520ETF%2520in%2520a%2520lower%2520dimensional%2520space%252C%2520yielding%2520a%2520provably%2520optimal%2520forgetting%2520operator.%2520We%2520further%2520introduce%2520the%2520Representation%2520Unlearning%2520Score%2520%2528RUS%2529%2520to%2520quantify%2520representation-level%2520forgetting%2520and%2520retention%2520fidelity.%2520Building%2520on%2520this%252C%2520we%2520introduce%2520POUR%2520%2528Provably%2520Optimal%2520Unlearning%2520of%2520Representations%2529%252C%2520a%2520geometric%2520projection%2520method%2520with%2520closed-form%2520%2528POUR-P%2529%2520and%2520a%2520feature-level%2520unlearning%2520variant%2520under%2520a%2520distillation%2520scheme%2520%2528POUR-D%2529.%2520Experiments%2520on%2520CIFAR-10/100%2520and%2520PathMNIST%2520demonstrate%2520that%2520POUR%2520achieves%2520effective%2520unlearning%2520while%2520preserving%2520retained%2520knowledge%252C%2520outperforming%2520state-of-the-art%2520unlearning%2520methods%2520on%2520both%2520classification-level%2520and%2520representation-level%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=POUR%3A%20A%20Provably%20Optimal%20Method%20for%20Unlearning%20Representations%20via%20Neural%20Collapse&entry.906535625=Anjie%20Le%20and%20Can%20Peng%20and%20Yuyuan%20Liu%20and%20J.%20Alison%20Noble&entry.1292438233=In%20computer%20vision%2C%20machine%20unlearning%20aims%20to%20remove%20the%20influence%20of%20specific%20visual%20concepts%20or%20training%20images%20without%20retraining%20from%20scratch.%20Studies%20show%20that%20existing%20approaches%20often%20modify%20the%20classifier%20while%20leaving%20internal%20representations%20intact%2C%20resulting%20in%20incomplete%20forgetting.%20In%20this%20work%2C%20we%20extend%20the%20notion%20of%20unlearning%20to%20the%20representation%20level%2C%20deriving%20a%20three-term%20interplay%20between%20forgetting%20efficacy%2C%20retention%20fidelity%2C%20and%20class%20separation.%20Building%20on%20Neural%20Collapse%20theory%2C%20we%20show%20that%20the%20orthogonal%20projection%20of%20a%20simplex%20Equiangular%20Tight%20Frame%20%28ETF%29%20remains%20an%20ETF%20in%20a%20lower%20dimensional%20space%2C%20yielding%20a%20provably%20optimal%20forgetting%20operator.%20We%20further%20introduce%20the%20Representation%20Unlearning%20Score%20%28RUS%29%20to%20quantify%20representation-level%20forgetting%20and%20retention%20fidelity.%20Building%20on%20this%2C%20we%20introduce%20POUR%20%28Provably%20Optimal%20Unlearning%20of%20Representations%29%2C%20a%20geometric%20projection%20method%20with%20closed-form%20%28POUR-P%29%20and%20a%20feature-level%20unlearning%20variant%20under%20a%20distillation%20scheme%20%28POUR-D%29.%20Experiments%20on%20CIFAR-10/100%20and%20PathMNIST%20demonstrate%20that%20POUR%20achieves%20effective%20unlearning%20while%20preserving%20retained%20knowledge%2C%20outperforming%20state-of-the-art%20unlearning%20methods%20on%20both%20classification-level%20and%20representation-level%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2511.19339v1&entry.124074799=Read"},
{"title": "EEG-VLM: A Hierarchical Vision-Language Model with Multi-Level Feature Alignment and Visually Enhanced Language-Guided Reasoning for EEG Image-Based Sleep Stage Prediction", "author": "Xihe Qiu and Gengchen Ma and Haoyu Wang and Chen Zhan and Xiaoyu Tan and Shuo Li", "abstract": "Sleep stage classification based on electroencephalography (EEG) is fundamental for assessing sleep quality and diagnosing sleep-related disorders. However, most traditional machine learning methods rely heavily on prior knowledge and handcrafted features, while existing deep learning models still struggle to jointly capture fine-grained time-frequency patterns and achieve clinical interpretability. Recently, vision-language models (VLMs) have made significant progress in the medical domain, yet their performance remains constrained when applied to physiological waveform data, especially EEG signals, due to their limited visual understanding and insufficient reasoning capability. To address these challenges, we propose EEG-VLM, a hierarchical vision-language framework that integrates multi-level feature alignment with visually enhanced language-guided reasoning for interpretable EEG-based sleep stage classification. Specifically, a specialized visual enhancement module constructs high-level visual tokens from intermediate-layer features to extract rich semantic representations of EEG images. These tokens are further aligned with low-level CLIP features through a multi-level alignment mechanism, enhancing the VLM's image-processing capability. In addition, a Chain-of-Thought (CoT) reasoning strategy decomposes complex medical inference into interpretable logical steps, effectively simulating expert-like decision-making. Experimental results demonstrate that the proposed method significantly improves both the accuracy and interpretability of VLMs in EEG-based sleep stage classification, showing promising potential for automated and explainable EEG analysis in clinical settings.", "link": "http://arxiv.org/abs/2511.19155v1", "date": "2025-11-24", "relevancy": 2.6675, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5399}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG-VLM%3A%20A%20Hierarchical%20Vision-Language%20Model%20with%20Multi-Level%20Feature%20Alignment%20and%20Visually%20Enhanced%20Language-Guided%20Reasoning%20for%20EEG%20Image-Based%20Sleep%20Stage%20Prediction&body=Title%3A%20EEG-VLM%3A%20A%20Hierarchical%20Vision-Language%20Model%20with%20Multi-Level%20Feature%20Alignment%20and%20Visually%20Enhanced%20Language-Guided%20Reasoning%20for%20EEG%20Image-Based%20Sleep%20Stage%20Prediction%0AAuthor%3A%20Xihe%20Qiu%20and%20Gengchen%20Ma%20and%20Haoyu%20Wang%20and%20Chen%20Zhan%20and%20Xiaoyu%20Tan%20and%20Shuo%20Li%0AAbstract%3A%20Sleep%20stage%20classification%20based%20on%20electroencephalography%20%28EEG%29%20is%20fundamental%20for%20assessing%20sleep%20quality%20and%20diagnosing%20sleep-related%20disorders.%20However%2C%20most%20traditional%20machine%20learning%20methods%20rely%20heavily%20on%20prior%20knowledge%20and%20handcrafted%20features%2C%20while%20existing%20deep%20learning%20models%20still%20struggle%20to%20jointly%20capture%20fine-grained%20time-frequency%20patterns%20and%20achieve%20clinical%20interpretability.%20Recently%2C%20vision-language%20models%20%28VLMs%29%20have%20made%20significant%20progress%20in%20the%20medical%20domain%2C%20yet%20their%20performance%20remains%20constrained%20when%20applied%20to%20physiological%20waveform%20data%2C%20especially%20EEG%20signals%2C%20due%20to%20their%20limited%20visual%20understanding%20and%20insufficient%20reasoning%20capability.%20To%20address%20these%20challenges%2C%20we%20propose%20EEG-VLM%2C%20a%20hierarchical%20vision-language%20framework%20that%20integrates%20multi-level%20feature%20alignment%20with%20visually%20enhanced%20language-guided%20reasoning%20for%20interpretable%20EEG-based%20sleep%20stage%20classification.%20Specifically%2C%20a%20specialized%20visual%20enhancement%20module%20constructs%20high-level%20visual%20tokens%20from%20intermediate-layer%20features%20to%20extract%20rich%20semantic%20representations%20of%20EEG%20images.%20These%20tokens%20are%20further%20aligned%20with%20low-level%20CLIP%20features%20through%20a%20multi-level%20alignment%20mechanism%2C%20enhancing%20the%20VLM%27s%20image-processing%20capability.%20In%20addition%2C%20a%20Chain-of-Thought%20%28CoT%29%20reasoning%20strategy%20decomposes%20complex%20medical%20inference%20into%20interpretable%20logical%20steps%2C%20effectively%20simulating%20expert-like%20decision-making.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20method%20significantly%20improves%20both%20the%20accuracy%20and%20interpretability%20of%20VLMs%20in%20EEG-based%20sleep%20stage%20classification%2C%20showing%20promising%20potential%20for%20automated%20and%20explainable%20EEG%20analysis%20in%20clinical%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG-VLM%253A%2520A%2520Hierarchical%2520Vision-Language%2520Model%2520with%2520Multi-Level%2520Feature%2520Alignment%2520and%2520Visually%2520Enhanced%2520Language-Guided%2520Reasoning%2520for%2520EEG%2520Image-Based%2520Sleep%2520Stage%2520Prediction%26entry.906535625%3DXihe%2520Qiu%2520and%2520Gengchen%2520Ma%2520and%2520Haoyu%2520Wang%2520and%2520Chen%2520Zhan%2520and%2520Xiaoyu%2520Tan%2520and%2520Shuo%2520Li%26entry.1292438233%3DSleep%2520stage%2520classification%2520based%2520on%2520electroencephalography%2520%2528EEG%2529%2520is%2520fundamental%2520for%2520assessing%2520sleep%2520quality%2520and%2520diagnosing%2520sleep-related%2520disorders.%2520However%252C%2520most%2520traditional%2520machine%2520learning%2520methods%2520rely%2520heavily%2520on%2520prior%2520knowledge%2520and%2520handcrafted%2520features%252C%2520while%2520existing%2520deep%2520learning%2520models%2520still%2520struggle%2520to%2520jointly%2520capture%2520fine-grained%2520time-frequency%2520patterns%2520and%2520achieve%2520clinical%2520interpretability.%2520Recently%252C%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520made%2520significant%2520progress%2520in%2520the%2520medical%2520domain%252C%2520yet%2520their%2520performance%2520remains%2520constrained%2520when%2520applied%2520to%2520physiological%2520waveform%2520data%252C%2520especially%2520EEG%2520signals%252C%2520due%2520to%2520their%2520limited%2520visual%2520understanding%2520and%2520insufficient%2520reasoning%2520capability.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520EEG-VLM%252C%2520a%2520hierarchical%2520vision-language%2520framework%2520that%2520integrates%2520multi-level%2520feature%2520alignment%2520with%2520visually%2520enhanced%2520language-guided%2520reasoning%2520for%2520interpretable%2520EEG-based%2520sleep%2520stage%2520classification.%2520Specifically%252C%2520a%2520specialized%2520visual%2520enhancement%2520module%2520constructs%2520high-level%2520visual%2520tokens%2520from%2520intermediate-layer%2520features%2520to%2520extract%2520rich%2520semantic%2520representations%2520of%2520EEG%2520images.%2520These%2520tokens%2520are%2520further%2520aligned%2520with%2520low-level%2520CLIP%2520features%2520through%2520a%2520multi-level%2520alignment%2520mechanism%252C%2520enhancing%2520the%2520VLM%2527s%2520image-processing%2520capability.%2520In%2520addition%252C%2520a%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520strategy%2520decomposes%2520complex%2520medical%2520inference%2520into%2520interpretable%2520logical%2520steps%252C%2520effectively%2520simulating%2520expert-like%2520decision-making.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520method%2520significantly%2520improves%2520both%2520the%2520accuracy%2520and%2520interpretability%2520of%2520VLMs%2520in%2520EEG-based%2520sleep%2520stage%2520classification%252C%2520showing%2520promising%2520potential%2520for%2520automated%2520and%2520explainable%2520EEG%2520analysis%2520in%2520clinical%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG-VLM%3A%20A%20Hierarchical%20Vision-Language%20Model%20with%20Multi-Level%20Feature%20Alignment%20and%20Visually%20Enhanced%20Language-Guided%20Reasoning%20for%20EEG%20Image-Based%20Sleep%20Stage%20Prediction&entry.906535625=Xihe%20Qiu%20and%20Gengchen%20Ma%20and%20Haoyu%20Wang%20and%20Chen%20Zhan%20and%20Xiaoyu%20Tan%20and%20Shuo%20Li&entry.1292438233=Sleep%20stage%20classification%20based%20on%20electroencephalography%20%28EEG%29%20is%20fundamental%20for%20assessing%20sleep%20quality%20and%20diagnosing%20sleep-related%20disorders.%20However%2C%20most%20traditional%20machine%20learning%20methods%20rely%20heavily%20on%20prior%20knowledge%20and%20handcrafted%20features%2C%20while%20existing%20deep%20learning%20models%20still%20struggle%20to%20jointly%20capture%20fine-grained%20time-frequency%20patterns%20and%20achieve%20clinical%20interpretability.%20Recently%2C%20vision-language%20models%20%28VLMs%29%20have%20made%20significant%20progress%20in%20the%20medical%20domain%2C%20yet%20their%20performance%20remains%20constrained%20when%20applied%20to%20physiological%20waveform%20data%2C%20especially%20EEG%20signals%2C%20due%20to%20their%20limited%20visual%20understanding%20and%20insufficient%20reasoning%20capability.%20To%20address%20these%20challenges%2C%20we%20propose%20EEG-VLM%2C%20a%20hierarchical%20vision-language%20framework%20that%20integrates%20multi-level%20feature%20alignment%20with%20visually%20enhanced%20language-guided%20reasoning%20for%20interpretable%20EEG-based%20sleep%20stage%20classification.%20Specifically%2C%20a%20specialized%20visual%20enhancement%20module%20constructs%20high-level%20visual%20tokens%20from%20intermediate-layer%20features%20to%20extract%20rich%20semantic%20representations%20of%20EEG%20images.%20These%20tokens%20are%20further%20aligned%20with%20low-level%20CLIP%20features%20through%20a%20multi-level%20alignment%20mechanism%2C%20enhancing%20the%20VLM%27s%20image-processing%20capability.%20In%20addition%2C%20a%20Chain-of-Thought%20%28CoT%29%20reasoning%20strategy%20decomposes%20complex%20medical%20inference%20into%20interpretable%20logical%20steps%2C%20effectively%20simulating%20expert-like%20decision-making.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20method%20significantly%20improves%20both%20the%20accuracy%20and%20interpretability%20of%20VLMs%20in%20EEG-based%20sleep%20stage%20classification%2C%20showing%20promising%20potential%20for%20automated%20and%20explainable%20EEG%20analysis%20in%20clinical%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2511.19155v1&entry.124074799=Read"},
{"title": "Zero-shot segmentation of skin tumors in whole-slide images with vision-language foundation models", "author": "Santiago Moreno and Pablo Meseguer and Roc\u00edo del Amor and Valery Naranjo", "abstract": "Accurate annotation of cutaneous neoplasm biopsies represents a major challenge due to their wide morphological variability, overlapping histological patterns, and the subtle distinctions between benign and malignant lesions. Vision-language foundation models (VLMs), pre-trained on paired image-text corpora, learn joint representations that bridge visual features and diagnostic terminology, enabling zero-shot localization and classification of tissue regions without pixel-level labels. However, most existing VLM applications in histopathology remain limited to slide-level tasks or rely on coarse interactive prompts, and they struggle to produce fine-grained segmentations across gigapixel whole-slide images (WSIs). In this work, we introduce a zero-shot visual-language segmentation pipeline for whole-slide images (ZEUS), a fully automated, zero-shot segmentation framework that leverages class-specific textual prompt ensembles and frozen VLM encoders to generate high-resolution tumor masks in WSIs. By partitioning each WSI into overlapping patches, extracting visual embeddings, and computing cosine similarities against text prompts, we generate a final segmentation mask. We demonstrate competitive performance on two in-house datasets, primary spindle cell neoplasms and cutaneous metastases, highlighting the influence of prompt design, domain shifts, and institutional variability in VLMs for histopathology. ZEUS markedly reduces annotation burden while offering scalable, explainable tumor delineation for downstream diagnostic workflows.", "link": "http://arxiv.org/abs/2511.18978v1", "date": "2025-11-24", "relevancy": 2.666, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20segmentation%20of%20skin%20tumors%20in%20whole-slide%20images%20with%20vision-language%20foundation%20models&body=Title%3A%20Zero-shot%20segmentation%20of%20skin%20tumors%20in%20whole-slide%20images%20with%20vision-language%20foundation%20models%0AAuthor%3A%20Santiago%20Moreno%20and%20Pablo%20Meseguer%20and%20Roc%C3%ADo%20del%20Amor%20and%20Valery%20Naranjo%0AAbstract%3A%20Accurate%20annotation%20of%20cutaneous%20neoplasm%20biopsies%20represents%20a%20major%20challenge%20due%20to%20their%20wide%20morphological%20variability%2C%20overlapping%20histological%20patterns%2C%20and%20the%20subtle%20distinctions%20between%20benign%20and%20malignant%20lesions.%20Vision-language%20foundation%20models%20%28VLMs%29%2C%20pre-trained%20on%20paired%20image-text%20corpora%2C%20learn%20joint%20representations%20that%20bridge%20visual%20features%20and%20diagnostic%20terminology%2C%20enabling%20zero-shot%20localization%20and%20classification%20of%20tissue%20regions%20without%20pixel-level%20labels.%20However%2C%20most%20existing%20VLM%20applications%20in%20histopathology%20remain%20limited%20to%20slide-level%20tasks%20or%20rely%20on%20coarse%20interactive%20prompts%2C%20and%20they%20struggle%20to%20produce%20fine-grained%20segmentations%20across%20gigapixel%20whole-slide%20images%20%28WSIs%29.%20In%20this%20work%2C%20we%20introduce%20a%20zero-shot%20visual-language%20segmentation%20pipeline%20for%20whole-slide%20images%20%28ZEUS%29%2C%20a%20fully%20automated%2C%20zero-shot%20segmentation%20framework%20that%20leverages%20class-specific%20textual%20prompt%20ensembles%20and%20frozen%20VLM%20encoders%20to%20generate%20high-resolution%20tumor%20masks%20in%20WSIs.%20By%20partitioning%20each%20WSI%20into%20overlapping%20patches%2C%20extracting%20visual%20embeddings%2C%20and%20computing%20cosine%20similarities%20against%20text%20prompts%2C%20we%20generate%20a%20final%20segmentation%20mask.%20We%20demonstrate%20competitive%20performance%20on%20two%20in-house%20datasets%2C%20primary%20spindle%20cell%20neoplasms%20and%20cutaneous%20metastases%2C%20highlighting%20the%20influence%20of%20prompt%20design%2C%20domain%20shifts%2C%20and%20institutional%20variability%20in%20VLMs%20for%20histopathology.%20ZEUS%20markedly%20reduces%20annotation%20burden%20while%20offering%20scalable%2C%20explainable%20tumor%20delineation%20for%20downstream%20diagnostic%20workflows.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520segmentation%2520of%2520skin%2520tumors%2520in%2520whole-slide%2520images%2520with%2520vision-language%2520foundation%2520models%26entry.906535625%3DSantiago%2520Moreno%2520and%2520Pablo%2520Meseguer%2520and%2520Roc%25C3%25ADo%2520del%2520Amor%2520and%2520Valery%2520Naranjo%26entry.1292438233%3DAccurate%2520annotation%2520of%2520cutaneous%2520neoplasm%2520biopsies%2520represents%2520a%2520major%2520challenge%2520due%2520to%2520their%2520wide%2520morphological%2520variability%252C%2520overlapping%2520histological%2520patterns%252C%2520and%2520the%2520subtle%2520distinctions%2520between%2520benign%2520and%2520malignant%2520lesions.%2520Vision-language%2520foundation%2520models%2520%2528VLMs%2529%252C%2520pre-trained%2520on%2520paired%2520image-text%2520corpora%252C%2520learn%2520joint%2520representations%2520that%2520bridge%2520visual%2520features%2520and%2520diagnostic%2520terminology%252C%2520enabling%2520zero-shot%2520localization%2520and%2520classification%2520of%2520tissue%2520regions%2520without%2520pixel-level%2520labels.%2520However%252C%2520most%2520existing%2520VLM%2520applications%2520in%2520histopathology%2520remain%2520limited%2520to%2520slide-level%2520tasks%2520or%2520rely%2520on%2520coarse%2520interactive%2520prompts%252C%2520and%2520they%2520struggle%2520to%2520produce%2520fine-grained%2520segmentations%2520across%2520gigapixel%2520whole-slide%2520images%2520%2528WSIs%2529.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520zero-shot%2520visual-language%2520segmentation%2520pipeline%2520for%2520whole-slide%2520images%2520%2528ZEUS%2529%252C%2520a%2520fully%2520automated%252C%2520zero-shot%2520segmentation%2520framework%2520that%2520leverages%2520class-specific%2520textual%2520prompt%2520ensembles%2520and%2520frozen%2520VLM%2520encoders%2520to%2520generate%2520high-resolution%2520tumor%2520masks%2520in%2520WSIs.%2520By%2520partitioning%2520each%2520WSI%2520into%2520overlapping%2520patches%252C%2520extracting%2520visual%2520embeddings%252C%2520and%2520computing%2520cosine%2520similarities%2520against%2520text%2520prompts%252C%2520we%2520generate%2520a%2520final%2520segmentation%2520mask.%2520We%2520demonstrate%2520competitive%2520performance%2520on%2520two%2520in-house%2520datasets%252C%2520primary%2520spindle%2520cell%2520neoplasms%2520and%2520cutaneous%2520metastases%252C%2520highlighting%2520the%2520influence%2520of%2520prompt%2520design%252C%2520domain%2520shifts%252C%2520and%2520institutional%2520variability%2520in%2520VLMs%2520for%2520histopathology.%2520ZEUS%2520markedly%2520reduces%2520annotation%2520burden%2520while%2520offering%2520scalable%252C%2520explainable%2520tumor%2520delineation%2520for%2520downstream%2520diagnostic%2520workflows.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20segmentation%20of%20skin%20tumors%20in%20whole-slide%20images%20with%20vision-language%20foundation%20models&entry.906535625=Santiago%20Moreno%20and%20Pablo%20Meseguer%20and%20Roc%C3%ADo%20del%20Amor%20and%20Valery%20Naranjo&entry.1292438233=Accurate%20annotation%20of%20cutaneous%20neoplasm%20biopsies%20represents%20a%20major%20challenge%20due%20to%20their%20wide%20morphological%20variability%2C%20overlapping%20histological%20patterns%2C%20and%20the%20subtle%20distinctions%20between%20benign%20and%20malignant%20lesions.%20Vision-language%20foundation%20models%20%28VLMs%29%2C%20pre-trained%20on%20paired%20image-text%20corpora%2C%20learn%20joint%20representations%20that%20bridge%20visual%20features%20and%20diagnostic%20terminology%2C%20enabling%20zero-shot%20localization%20and%20classification%20of%20tissue%20regions%20without%20pixel-level%20labels.%20However%2C%20most%20existing%20VLM%20applications%20in%20histopathology%20remain%20limited%20to%20slide-level%20tasks%20or%20rely%20on%20coarse%20interactive%20prompts%2C%20and%20they%20struggle%20to%20produce%20fine-grained%20segmentations%20across%20gigapixel%20whole-slide%20images%20%28WSIs%29.%20In%20this%20work%2C%20we%20introduce%20a%20zero-shot%20visual-language%20segmentation%20pipeline%20for%20whole-slide%20images%20%28ZEUS%29%2C%20a%20fully%20automated%2C%20zero-shot%20segmentation%20framework%20that%20leverages%20class-specific%20textual%20prompt%20ensembles%20and%20frozen%20VLM%20encoders%20to%20generate%20high-resolution%20tumor%20masks%20in%20WSIs.%20By%20partitioning%20each%20WSI%20into%20overlapping%20patches%2C%20extracting%20visual%20embeddings%2C%20and%20computing%20cosine%20similarities%20against%20text%20prompts%2C%20we%20generate%20a%20final%20segmentation%20mask.%20We%20demonstrate%20competitive%20performance%20on%20two%20in-house%20datasets%2C%20primary%20spindle%20cell%20neoplasms%20and%20cutaneous%20metastases%2C%20highlighting%20the%20influence%20of%20prompt%20design%2C%20domain%20shifts%2C%20and%20institutional%20variability%20in%20VLMs%20for%20histopathology.%20ZEUS%20markedly%20reduces%20annotation%20burden%20while%20offering%20scalable%2C%20explainable%20tumor%20delineation%20for%20downstream%20diagnostic%20workflows.&entry.1838667208=http%3A//arxiv.org/abs/2511.18978v1&entry.124074799=Read"},
{"title": "DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling", "author": "Timur Mamedov and Anton Konushin and Vadim Konushin", "abstract": "Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.", "link": "http://arxiv.org/abs/2511.19067v1", "date": "2025-11-24", "relevancy": 2.6645, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5414}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5325}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaMix%3A%20Generalizable%20Person%20Re-identification%20via%20Dynamic%20Relabeling%20and%20Mixed%20Data%20Sampling&body=Title%3A%20DynaMix%3A%20Generalizable%20Person%20Re-identification%20via%20Dynamic%20Relabeling%20and%20Mixed%20Data%20Sampling%0AAuthor%3A%20Timur%20Mamedov%20and%20Anton%20Konushin%20and%20Vadim%20Konushin%0AAbstract%3A%20Generalizable%20person%20re-identification%20%28Re-ID%29%20aims%20to%20recognize%20individuals%20across%20unseen%20cameras%20and%20environments.%20While%20existing%20methods%20rely%20heavily%20on%20limited%20labeled%20multi-camera%20data%2C%20we%20propose%20DynaMix%2C%20a%20novel%20method%20that%20effectively%20combines%20manually%20labeled%20multi-camera%20and%20large-scale%20pseudo-labeled%20single-camera%20data.%20Unlike%20prior%20works%2C%20DynaMix%20dynamically%20adapts%20to%20the%20structure%20and%20noise%20of%20the%20training%20data%20through%20three%20core%20components%3A%20%281%29%20a%20Relabeling%20Module%20that%20refines%20pseudo-labels%20of%20single-camera%20identities%20on-the-fly%3B%20%282%29%20an%20Efficient%20Centroids%20Module%20that%20maintains%20robust%20identity%20representations%20under%20a%20large%20identity%20space%3B%20and%20%283%29%20a%20Data%20Sampling%20Module%20that%20carefully%20composes%20mixed%20data%20mini-batches%20to%20balance%20learning%20complexity%20and%20intra-batch%20diversity.%20All%20components%20are%20specifically%20designed%20to%20operate%20efficiently%20at%20scale%2C%20enabling%20effective%20training%20on%20millions%20of%20images%20and%20hundreds%20of%20thousands%20of%20identities.%20Extensive%20experiments%20demonstrate%20that%20DynaMix%20consistently%20outperforms%20state-of-the-art%20methods%20in%20generalizable%20person%20Re-ID.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaMix%253A%2520Generalizable%2520Person%2520Re-identification%2520via%2520Dynamic%2520Relabeling%2520and%2520Mixed%2520Data%2520Sampling%26entry.906535625%3DTimur%2520Mamedov%2520and%2520Anton%2520Konushin%2520and%2520Vadim%2520Konushin%26entry.1292438233%3DGeneralizable%2520person%2520re-identification%2520%2528Re-ID%2529%2520aims%2520to%2520recognize%2520individuals%2520across%2520unseen%2520cameras%2520and%2520environments.%2520While%2520existing%2520methods%2520rely%2520heavily%2520on%2520limited%2520labeled%2520multi-camera%2520data%252C%2520we%2520propose%2520DynaMix%252C%2520a%2520novel%2520method%2520that%2520effectively%2520combines%2520manually%2520labeled%2520multi-camera%2520and%2520large-scale%2520pseudo-labeled%2520single-camera%2520data.%2520Unlike%2520prior%2520works%252C%2520DynaMix%2520dynamically%2520adapts%2520to%2520the%2520structure%2520and%2520noise%2520of%2520the%2520training%2520data%2520through%2520three%2520core%2520components%253A%2520%25281%2529%2520a%2520Relabeling%2520Module%2520that%2520refines%2520pseudo-labels%2520of%2520single-camera%2520identities%2520on-the-fly%253B%2520%25282%2529%2520an%2520Efficient%2520Centroids%2520Module%2520that%2520maintains%2520robust%2520identity%2520representations%2520under%2520a%2520large%2520identity%2520space%253B%2520and%2520%25283%2529%2520a%2520Data%2520Sampling%2520Module%2520that%2520carefully%2520composes%2520mixed%2520data%2520mini-batches%2520to%2520balance%2520learning%2520complexity%2520and%2520intra-batch%2520diversity.%2520All%2520components%2520are%2520specifically%2520designed%2520to%2520operate%2520efficiently%2520at%2520scale%252C%2520enabling%2520effective%2520training%2520on%2520millions%2520of%2520images%2520and%2520hundreds%2520of%2520thousands%2520of%2520identities.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DynaMix%2520consistently%2520outperforms%2520state-of-the-art%2520methods%2520in%2520generalizable%2520person%2520Re-ID.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaMix%3A%20Generalizable%20Person%20Re-identification%20via%20Dynamic%20Relabeling%20and%20Mixed%20Data%20Sampling&entry.906535625=Timur%20Mamedov%20and%20Anton%20Konushin%20and%20Vadim%20Konushin&entry.1292438233=Generalizable%20person%20re-identification%20%28Re-ID%29%20aims%20to%20recognize%20individuals%20across%20unseen%20cameras%20and%20environments.%20While%20existing%20methods%20rely%20heavily%20on%20limited%20labeled%20multi-camera%20data%2C%20we%20propose%20DynaMix%2C%20a%20novel%20method%20that%20effectively%20combines%20manually%20labeled%20multi-camera%20and%20large-scale%20pseudo-labeled%20single-camera%20data.%20Unlike%20prior%20works%2C%20DynaMix%20dynamically%20adapts%20to%20the%20structure%20and%20noise%20of%20the%20training%20data%20through%20three%20core%20components%3A%20%281%29%20a%20Relabeling%20Module%20that%20refines%20pseudo-labels%20of%20single-camera%20identities%20on-the-fly%3B%20%282%29%20an%20Efficient%20Centroids%20Module%20that%20maintains%20robust%20identity%20representations%20under%20a%20large%20identity%20space%3B%20and%20%283%29%20a%20Data%20Sampling%20Module%20that%20carefully%20composes%20mixed%20data%20mini-batches%20to%20balance%20learning%20complexity%20and%20intra-batch%20diversity.%20All%20components%20are%20specifically%20designed%20to%20operate%20efficiently%20at%20scale%2C%20enabling%20effective%20training%20on%20millions%20of%20images%20and%20hundreds%20of%20thousands%20of%20identities.%20Extensive%20experiments%20demonstrate%20that%20DynaMix%20consistently%20outperforms%20state-of-the-art%20methods%20in%20generalizable%20person%20Re-ID.&entry.1838667208=http%3A//arxiv.org/abs/2511.19067v1&entry.124074799=Read"},
{"title": "Cognitive Foundations for Reasoning and Their Manifestation in LLMs", "author": "Priyanka Kargupta and Shuyue Stella Li and Haocheng Wang and Jinu Lee and Shan Chen and Orevaoghene Ahia and Dean Light and Thomas L. Griffiths and Max Kleiman-Weiner and Jiawei Han and Asli Celikyilmaz and Yulia Tsvetkov", "abstract": "Large language models (LLMs) solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. To understand this gap, we synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning reasoning invariants, meta-cognitive controls, representations for organizing reasoning & knowledge, and transformation operations. We introduce a fine-grained evaluation framework and conduct the first large-scale empirical analysis of 192K traces from 18 models across text, vision, and audio, complemented by 54 human think-aloud traces, which we make publicly available. We find that models under-utilize cognitive elements correlated with success, narrowing to rigid sequential processing on ill-structured problems where diverse representations and meta-cognitive monitoring are critical. Human traces show more abstraction and conceptual processing, while models default to surface-level enumeration. Meta-analysis of 1.6K LLM reasoning papers reveals the research community concentrates on easily quantifiable elements (sequential organization: 55%, decomposition: 60%) but neglecting meta-cognitive controls (self-awareness: 16%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 66.7% on complex problems. By establishing a shared vocabulary between cognitive science and LLM research, our framework enables systematic diagnosis of reasoning failures and principled development of models that reason through robust cognitive mechanisms rather than spurious shortcuts, while providing tools to test theories of human cognition at scale.", "link": "http://arxiv.org/abs/2511.16660v2", "date": "2025-11-24", "relevancy": 2.6623, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cognitive%20Foundations%20for%20Reasoning%20and%20Their%20Manifestation%20in%20LLMs&body=Title%3A%20Cognitive%20Foundations%20for%20Reasoning%20and%20Their%20Manifestation%20in%20LLMs%0AAuthor%3A%20Priyanka%20Kargupta%20and%20Shuyue%20Stella%20Li%20and%20Haocheng%20Wang%20and%20Jinu%20Lee%20and%20Shan%20Chen%20and%20Orevaoghene%20Ahia%20and%20Dean%20Light%20and%20Thomas%20L.%20Griffiths%20and%20Max%20Kleiman-Weiner%20and%20Jiawei%20Han%20and%20Asli%20Celikyilmaz%20and%20Yulia%20Tsvetkov%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20solve%20complex%20problems%20yet%20fail%20on%20simpler%20variants%2C%20suggesting%20they%20achieve%20correct%20outputs%20through%20mechanisms%20fundamentally%20different%20from%20human%20reasoning.%20To%20understand%20this%20gap%2C%20we%20synthesize%20cognitive%20science%20research%20into%20a%20taxonomy%20of%2028%20cognitive%20elements%20spanning%20reasoning%20invariants%2C%20meta-cognitive%20controls%2C%20representations%20for%20organizing%20reasoning%20%26%20knowledge%2C%20and%20transformation%20operations.%20We%20introduce%20a%20fine-grained%20evaluation%20framework%20and%20conduct%20the%20first%20large-scale%20empirical%20analysis%20of%20192K%20traces%20from%2018%20models%20across%20text%2C%20vision%2C%20and%20audio%2C%20complemented%20by%2054%20human%20think-aloud%20traces%2C%20which%20we%20make%20publicly%20available.%20We%20find%20that%20models%20under-utilize%20cognitive%20elements%20correlated%20with%20success%2C%20narrowing%20to%20rigid%20sequential%20processing%20on%20ill-structured%20problems%20where%20diverse%20representations%20and%20meta-cognitive%20monitoring%20are%20critical.%20Human%20traces%20show%20more%20abstraction%20and%20conceptual%20processing%2C%20while%20models%20default%20to%20surface-level%20enumeration.%20Meta-analysis%20of%201.6K%20LLM%20reasoning%20papers%20reveals%20the%20research%20community%20concentrates%20on%20easily%20quantifiable%20elements%20%28sequential%20organization%3A%2055%25%2C%20decomposition%3A%2060%25%29%20but%20neglecting%20meta-cognitive%20controls%20%28self-awareness%3A%2016%25%29%20that%20correlate%20with%20success.%20Models%20possess%20behavioral%20repertoires%20associated%20with%20success%20but%20fail%20to%20deploy%20them%20spontaneously.%20Leveraging%20these%20patterns%2C%20we%20develop%20test-time%20reasoning%20guidance%20that%20automatically%20scaffold%20successful%20structures%2C%20improving%20performance%20by%20up%20to%2066.7%25%20on%20complex%20problems.%20By%20establishing%20a%20shared%20vocabulary%20between%20cognitive%20science%20and%20LLM%20research%2C%20our%20framework%20enables%20systematic%20diagnosis%20of%20reasoning%20failures%20and%20principled%20development%20of%20models%20that%20reason%20through%20robust%20cognitive%20mechanisms%20rather%20than%20spurious%20shortcuts%2C%20while%20providing%20tools%20to%20test%20theories%20of%20human%20cognition%20at%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCognitive%2520Foundations%2520for%2520Reasoning%2520and%2520Their%2520Manifestation%2520in%2520LLMs%26entry.906535625%3DPriyanka%2520Kargupta%2520and%2520Shuyue%2520Stella%2520Li%2520and%2520Haocheng%2520Wang%2520and%2520Jinu%2520Lee%2520and%2520Shan%2520Chen%2520and%2520Orevaoghene%2520Ahia%2520and%2520Dean%2520Light%2520and%2520Thomas%2520L.%2520Griffiths%2520and%2520Max%2520Kleiman-Weiner%2520and%2520Jiawei%2520Han%2520and%2520Asli%2520Celikyilmaz%2520and%2520Yulia%2520Tsvetkov%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520solve%2520complex%2520problems%2520yet%2520fail%2520on%2520simpler%2520variants%252C%2520suggesting%2520they%2520achieve%2520correct%2520outputs%2520through%2520mechanisms%2520fundamentally%2520different%2520from%2520human%2520reasoning.%2520To%2520understand%2520this%2520gap%252C%2520we%2520synthesize%2520cognitive%2520science%2520research%2520into%2520a%2520taxonomy%2520of%252028%2520cognitive%2520elements%2520spanning%2520reasoning%2520invariants%252C%2520meta-cognitive%2520controls%252C%2520representations%2520for%2520organizing%2520reasoning%2520%2526%2520knowledge%252C%2520and%2520transformation%2520operations.%2520We%2520introduce%2520a%2520fine-grained%2520evaluation%2520framework%2520and%2520conduct%2520the%2520first%2520large-scale%2520empirical%2520analysis%2520of%2520192K%2520traces%2520from%252018%2520models%2520across%2520text%252C%2520vision%252C%2520and%2520audio%252C%2520complemented%2520by%252054%2520human%2520think-aloud%2520traces%252C%2520which%2520we%2520make%2520publicly%2520available.%2520We%2520find%2520that%2520models%2520under-utilize%2520cognitive%2520elements%2520correlated%2520with%2520success%252C%2520narrowing%2520to%2520rigid%2520sequential%2520processing%2520on%2520ill-structured%2520problems%2520where%2520diverse%2520representations%2520and%2520meta-cognitive%2520monitoring%2520are%2520critical.%2520Human%2520traces%2520show%2520more%2520abstraction%2520and%2520conceptual%2520processing%252C%2520while%2520models%2520default%2520to%2520surface-level%2520enumeration.%2520Meta-analysis%2520of%25201.6K%2520LLM%2520reasoning%2520papers%2520reveals%2520the%2520research%2520community%2520concentrates%2520on%2520easily%2520quantifiable%2520elements%2520%2528sequential%2520organization%253A%252055%2525%252C%2520decomposition%253A%252060%2525%2529%2520but%2520neglecting%2520meta-cognitive%2520controls%2520%2528self-awareness%253A%252016%2525%2529%2520that%2520correlate%2520with%2520success.%2520Models%2520possess%2520behavioral%2520repertoires%2520associated%2520with%2520success%2520but%2520fail%2520to%2520deploy%2520them%2520spontaneously.%2520Leveraging%2520these%2520patterns%252C%2520we%2520develop%2520test-time%2520reasoning%2520guidance%2520that%2520automatically%2520scaffold%2520successful%2520structures%252C%2520improving%2520performance%2520by%2520up%2520to%252066.7%2525%2520on%2520complex%2520problems.%2520By%2520establishing%2520a%2520shared%2520vocabulary%2520between%2520cognitive%2520science%2520and%2520LLM%2520research%252C%2520our%2520framework%2520enables%2520systematic%2520diagnosis%2520of%2520reasoning%2520failures%2520and%2520principled%2520development%2520of%2520models%2520that%2520reason%2520through%2520robust%2520cognitive%2520mechanisms%2520rather%2520than%2520spurious%2520shortcuts%252C%2520while%2520providing%2520tools%2520to%2520test%2520theories%2520of%2520human%2520cognition%2520at%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cognitive%20Foundations%20for%20Reasoning%20and%20Their%20Manifestation%20in%20LLMs&entry.906535625=Priyanka%20Kargupta%20and%20Shuyue%20Stella%20Li%20and%20Haocheng%20Wang%20and%20Jinu%20Lee%20and%20Shan%20Chen%20and%20Orevaoghene%20Ahia%20and%20Dean%20Light%20and%20Thomas%20L.%20Griffiths%20and%20Max%20Kleiman-Weiner%20and%20Jiawei%20Han%20and%20Asli%20Celikyilmaz%20and%20Yulia%20Tsvetkov&entry.1292438233=Large%20language%20models%20%28LLMs%29%20solve%20complex%20problems%20yet%20fail%20on%20simpler%20variants%2C%20suggesting%20they%20achieve%20correct%20outputs%20through%20mechanisms%20fundamentally%20different%20from%20human%20reasoning.%20To%20understand%20this%20gap%2C%20we%20synthesize%20cognitive%20science%20research%20into%20a%20taxonomy%20of%2028%20cognitive%20elements%20spanning%20reasoning%20invariants%2C%20meta-cognitive%20controls%2C%20representations%20for%20organizing%20reasoning%20%26%20knowledge%2C%20and%20transformation%20operations.%20We%20introduce%20a%20fine-grained%20evaluation%20framework%20and%20conduct%20the%20first%20large-scale%20empirical%20analysis%20of%20192K%20traces%20from%2018%20models%20across%20text%2C%20vision%2C%20and%20audio%2C%20complemented%20by%2054%20human%20think-aloud%20traces%2C%20which%20we%20make%20publicly%20available.%20We%20find%20that%20models%20under-utilize%20cognitive%20elements%20correlated%20with%20success%2C%20narrowing%20to%20rigid%20sequential%20processing%20on%20ill-structured%20problems%20where%20diverse%20representations%20and%20meta-cognitive%20monitoring%20are%20critical.%20Human%20traces%20show%20more%20abstraction%20and%20conceptual%20processing%2C%20while%20models%20default%20to%20surface-level%20enumeration.%20Meta-analysis%20of%201.6K%20LLM%20reasoning%20papers%20reveals%20the%20research%20community%20concentrates%20on%20easily%20quantifiable%20elements%20%28sequential%20organization%3A%2055%25%2C%20decomposition%3A%2060%25%29%20but%20neglecting%20meta-cognitive%20controls%20%28self-awareness%3A%2016%25%29%20that%20correlate%20with%20success.%20Models%20possess%20behavioral%20repertoires%20associated%20with%20success%20but%20fail%20to%20deploy%20them%20spontaneously.%20Leveraging%20these%20patterns%2C%20we%20develop%20test-time%20reasoning%20guidance%20that%20automatically%20scaffold%20successful%20structures%2C%20improving%20performance%20by%20up%20to%2066.7%25%20on%20complex%20problems.%20By%20establishing%20a%20shared%20vocabulary%20between%20cognitive%20science%20and%20LLM%20research%2C%20our%20framework%20enables%20systematic%20diagnosis%20of%20reasoning%20failures%20and%20principled%20development%20of%20models%20that%20reason%20through%20robust%20cognitive%20mechanisms%20rather%20than%20spurious%20shortcuts%2C%20while%20providing%20tools%20to%20test%20theories%20of%20human%20cognition%20at%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2511.16660v2&entry.124074799=Read"},
{"title": "MoveGPT: Scaling Mobility Foundation Models with Spatially-Aware Mixture of Experts", "author": "Chonghua Han and Yuan Yuan and Jingtao Ding and Jie Feng and Fanjin Meng and Yong Li", "abstract": "The success of foundation models in language has inspired a new wave of general-purpose models for human mobility. However, existing approaches struggle to scale effectively due to two fundamental limitations: a failure to use meaningful basic units to represent movement, and an inability to capture the vast diversity of patterns found in large-scale data. In this work, we develop MoveGPT, a large-scale foundation model specifically architected to overcome these barriers. MoveGPT is built upon two key innovations: (1) a unified location encoder that maps geographically disjoint locations into a shared semantic space, enabling pre-training on a global scale; and (2) a Spatially-Aware Mixture-of-Experts Transformer that develops specialized experts to efficiently capture diverse mobility patterns. Pre-trained on billion-scale datasets, MoveGPT establishes a new state-of-the-art across a wide range of downstream tasks, achieving performance gains of up to 35% on average. It also demonstrates strong generalization capabilities to unseen cities. Crucially, our work provides empirical evidence of scaling ability in human mobility, validating a clear path toward building increasingly capable foundation models in this domain.", "link": "http://arxiv.org/abs/2505.18670v3", "date": "2025-11-24", "relevancy": 2.6599, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5366}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5301}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoveGPT%3A%20Scaling%20Mobility%20Foundation%20Models%20with%20Spatially-Aware%20Mixture%20of%20Experts&body=Title%3A%20MoveGPT%3A%20Scaling%20Mobility%20Foundation%20Models%20with%20Spatially-Aware%20Mixture%20of%20Experts%0AAuthor%3A%20Chonghua%20Han%20and%20Yuan%20Yuan%20and%20Jingtao%20Ding%20and%20Jie%20Feng%20and%20Fanjin%20Meng%20and%20Yong%20Li%0AAbstract%3A%20The%20success%20of%20foundation%20models%20in%20language%20has%20inspired%20a%20new%20wave%20of%20general-purpose%20models%20for%20human%20mobility.%20However%2C%20existing%20approaches%20struggle%20to%20scale%20effectively%20due%20to%20two%20fundamental%20limitations%3A%20a%20failure%20to%20use%20meaningful%20basic%20units%20to%20represent%20movement%2C%20and%20an%20inability%20to%20capture%20the%20vast%20diversity%20of%20patterns%20found%20in%20large-scale%20data.%20In%20this%20work%2C%20we%20develop%20MoveGPT%2C%20a%20large-scale%20foundation%20model%20specifically%20architected%20to%20overcome%20these%20barriers.%20MoveGPT%20is%20built%20upon%20two%20key%20innovations%3A%20%281%29%20a%20unified%20location%20encoder%20that%20maps%20geographically%20disjoint%20locations%20into%20a%20shared%20semantic%20space%2C%20enabling%20pre-training%20on%20a%20global%20scale%3B%20and%20%282%29%20a%20Spatially-Aware%20Mixture-of-Experts%20Transformer%20that%20develops%20specialized%20experts%20to%20efficiently%20capture%20diverse%20mobility%20patterns.%20Pre-trained%20on%20billion-scale%20datasets%2C%20MoveGPT%20establishes%20a%20new%20state-of-the-art%20across%20a%20wide%20range%20of%20downstream%20tasks%2C%20achieving%20performance%20gains%20of%20up%20to%2035%25%20on%20average.%20It%20also%20demonstrates%20strong%20generalization%20capabilities%20to%20unseen%20cities.%20Crucially%2C%20our%20work%20provides%20empirical%20evidence%20of%20scaling%20ability%20in%20human%20mobility%2C%20validating%20a%20clear%20path%20toward%20building%20increasingly%20capable%20foundation%20models%20in%20this%20domain.%0ALink%3A%20http%3A//arxiv.org/abs/2505.18670v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoveGPT%253A%2520Scaling%2520Mobility%2520Foundation%2520Models%2520with%2520Spatially-Aware%2520Mixture%2520of%2520Experts%26entry.906535625%3DChonghua%2520Han%2520and%2520Yuan%2520Yuan%2520and%2520Jingtao%2520Ding%2520and%2520Jie%2520Feng%2520and%2520Fanjin%2520Meng%2520and%2520Yong%2520Li%26entry.1292438233%3DThe%2520success%2520of%2520foundation%2520models%2520in%2520language%2520has%2520inspired%2520a%2520new%2520wave%2520of%2520general-purpose%2520models%2520for%2520human%2520mobility.%2520However%252C%2520existing%2520approaches%2520struggle%2520to%2520scale%2520effectively%2520due%2520to%2520two%2520fundamental%2520limitations%253A%2520a%2520failure%2520to%2520use%2520meaningful%2520basic%2520units%2520to%2520represent%2520movement%252C%2520and%2520an%2520inability%2520to%2520capture%2520the%2520vast%2520diversity%2520of%2520patterns%2520found%2520in%2520large-scale%2520data.%2520In%2520this%2520work%252C%2520we%2520develop%2520MoveGPT%252C%2520a%2520large-scale%2520foundation%2520model%2520specifically%2520architected%2520to%2520overcome%2520these%2520barriers.%2520MoveGPT%2520is%2520built%2520upon%2520two%2520key%2520innovations%253A%2520%25281%2529%2520a%2520unified%2520location%2520encoder%2520that%2520maps%2520geographically%2520disjoint%2520locations%2520into%2520a%2520shared%2520semantic%2520space%252C%2520enabling%2520pre-training%2520on%2520a%2520global%2520scale%253B%2520and%2520%25282%2529%2520a%2520Spatially-Aware%2520Mixture-of-Experts%2520Transformer%2520that%2520develops%2520specialized%2520experts%2520to%2520efficiently%2520capture%2520diverse%2520mobility%2520patterns.%2520Pre-trained%2520on%2520billion-scale%2520datasets%252C%2520MoveGPT%2520establishes%2520a%2520new%2520state-of-the-art%2520across%2520a%2520wide%2520range%2520of%2520downstream%2520tasks%252C%2520achieving%2520performance%2520gains%2520of%2520up%2520to%252035%2525%2520on%2520average.%2520It%2520also%2520demonstrates%2520strong%2520generalization%2520capabilities%2520to%2520unseen%2520cities.%2520Crucially%252C%2520our%2520work%2520provides%2520empirical%2520evidence%2520of%2520scaling%2520ability%2520in%2520human%2520mobility%252C%2520validating%2520a%2520clear%2520path%2520toward%2520building%2520increasingly%2520capable%2520foundation%2520models%2520in%2520this%2520domain.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18670v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoveGPT%3A%20Scaling%20Mobility%20Foundation%20Models%20with%20Spatially-Aware%20Mixture%20of%20Experts&entry.906535625=Chonghua%20Han%20and%20Yuan%20Yuan%20and%20Jingtao%20Ding%20and%20Jie%20Feng%20and%20Fanjin%20Meng%20and%20Yong%20Li&entry.1292438233=The%20success%20of%20foundation%20models%20in%20language%20has%20inspired%20a%20new%20wave%20of%20general-purpose%20models%20for%20human%20mobility.%20However%2C%20existing%20approaches%20struggle%20to%20scale%20effectively%20due%20to%20two%20fundamental%20limitations%3A%20a%20failure%20to%20use%20meaningful%20basic%20units%20to%20represent%20movement%2C%20and%20an%20inability%20to%20capture%20the%20vast%20diversity%20of%20patterns%20found%20in%20large-scale%20data.%20In%20this%20work%2C%20we%20develop%20MoveGPT%2C%20a%20large-scale%20foundation%20model%20specifically%20architected%20to%20overcome%20these%20barriers.%20MoveGPT%20is%20built%20upon%20two%20key%20innovations%3A%20%281%29%20a%20unified%20location%20encoder%20that%20maps%20geographically%20disjoint%20locations%20into%20a%20shared%20semantic%20space%2C%20enabling%20pre-training%20on%20a%20global%20scale%3B%20and%20%282%29%20a%20Spatially-Aware%20Mixture-of-Experts%20Transformer%20that%20develops%20specialized%20experts%20to%20efficiently%20capture%20diverse%20mobility%20patterns.%20Pre-trained%20on%20billion-scale%20datasets%2C%20MoveGPT%20establishes%20a%20new%20state-of-the-art%20across%20a%20wide%20range%20of%20downstream%20tasks%2C%20achieving%20performance%20gains%20of%20up%20to%2035%25%20on%20average.%20It%20also%20demonstrates%20strong%20generalization%20capabilities%20to%20unseen%20cities.%20Crucially%2C%20our%20work%20provides%20empirical%20evidence%20of%20scaling%20ability%20in%20human%20mobility%2C%20validating%20a%20clear%20path%20toward%20building%20increasingly%20capable%20foundation%20models%20in%20this%20domain.&entry.1838667208=http%3A//arxiv.org/abs/2505.18670v3&entry.124074799=Read"},
{"title": "Graph-based 3D Human Pose Estimation using WiFi Signals", "author": "Jichao Chen and YangYang Qu and Ruibo Tang and Dirk Slock", "abstract": "WiFi-based human pose estimation (HPE) has attracted increasing attention due to its resilience to occlusion and privacy-preserving compared to camera-based methods. However, existing WiFi-based HPE approaches often employ regression networks that directly map WiFi channel state information (CSI) to 3D joint coordinates, ignoring the inherent topological relationships among human joints. In this paper, we present GraphPose-Fi, a graph-based framework that explicitly models skeletal topology for WiFi-based 3D HPE. Our framework comprises a CNN encoder shared across antennas for subcarrier-time feature extraction, a lightweight attention module that adaptively reweights features over time and across antennas, and a graph-based regression head that combines GCN layers with self-attention to capture local topology and global dependencies. Our proposed method significantly outperforms existing methods on the MM-Fi dataset in various settings. The source code is available at: https://github.com/Cirrick/GraphPose-Fi.", "link": "http://arxiv.org/abs/2511.19105v1", "date": "2025-11-24", "relevancy": 2.6501, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5543}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.523}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-based%203D%20Human%20Pose%20Estimation%20using%20WiFi%20Signals&body=Title%3A%20Graph-based%203D%20Human%20Pose%20Estimation%20using%20WiFi%20Signals%0AAuthor%3A%20Jichao%20Chen%20and%20YangYang%20Qu%20and%20Ruibo%20Tang%20and%20Dirk%20Slock%0AAbstract%3A%20WiFi-based%20human%20pose%20estimation%20%28HPE%29%20has%20attracted%20increasing%20attention%20due%20to%20its%20resilience%20to%20occlusion%20and%20privacy-preserving%20compared%20to%20camera-based%20methods.%20However%2C%20existing%20WiFi-based%20HPE%20approaches%20often%20employ%20regression%20networks%20that%20directly%20map%20WiFi%20channel%20state%20information%20%28CSI%29%20to%203D%20joint%20coordinates%2C%20ignoring%20the%20inherent%20topological%20relationships%20among%20human%20joints.%20In%20this%20paper%2C%20we%20present%20GraphPose-Fi%2C%20a%20graph-based%20framework%20that%20explicitly%20models%20skeletal%20topology%20for%20WiFi-based%203D%20HPE.%20Our%20framework%20comprises%20a%20CNN%20encoder%20shared%20across%20antennas%20for%20subcarrier-time%20feature%20extraction%2C%20a%20lightweight%20attention%20module%20that%20adaptively%20reweights%20features%20over%20time%20and%20across%20antennas%2C%20and%20a%20graph-based%20regression%20head%20that%20combines%20GCN%20layers%20with%20self-attention%20to%20capture%20local%20topology%20and%20global%20dependencies.%20Our%20proposed%20method%20significantly%20outperforms%20existing%20methods%20on%20the%20MM-Fi%20dataset%20in%20various%20settings.%20The%20source%20code%20is%20available%20at%3A%20https%3A//github.com/Cirrick/GraphPose-Fi.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-based%25203D%2520Human%2520Pose%2520Estimation%2520using%2520WiFi%2520Signals%26entry.906535625%3DJichao%2520Chen%2520and%2520YangYang%2520Qu%2520and%2520Ruibo%2520Tang%2520and%2520Dirk%2520Slock%26entry.1292438233%3DWiFi-based%2520human%2520pose%2520estimation%2520%2528HPE%2529%2520has%2520attracted%2520increasing%2520attention%2520due%2520to%2520its%2520resilience%2520to%2520occlusion%2520and%2520privacy-preserving%2520compared%2520to%2520camera-based%2520methods.%2520However%252C%2520existing%2520WiFi-based%2520HPE%2520approaches%2520often%2520employ%2520regression%2520networks%2520that%2520directly%2520map%2520WiFi%2520channel%2520state%2520information%2520%2528CSI%2529%2520to%25203D%2520joint%2520coordinates%252C%2520ignoring%2520the%2520inherent%2520topological%2520relationships%2520among%2520human%2520joints.%2520In%2520this%2520paper%252C%2520we%2520present%2520GraphPose-Fi%252C%2520a%2520graph-based%2520framework%2520that%2520explicitly%2520models%2520skeletal%2520topology%2520for%2520WiFi-based%25203D%2520HPE.%2520Our%2520framework%2520comprises%2520a%2520CNN%2520encoder%2520shared%2520across%2520antennas%2520for%2520subcarrier-time%2520feature%2520extraction%252C%2520a%2520lightweight%2520attention%2520module%2520that%2520adaptively%2520reweights%2520features%2520over%2520time%2520and%2520across%2520antennas%252C%2520and%2520a%2520graph-based%2520regression%2520head%2520that%2520combines%2520GCN%2520layers%2520with%2520self-attention%2520to%2520capture%2520local%2520topology%2520and%2520global%2520dependencies.%2520Our%2520proposed%2520method%2520significantly%2520outperforms%2520existing%2520methods%2520on%2520the%2520MM-Fi%2520dataset%2520in%2520various%2520settings.%2520The%2520source%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/Cirrick/GraphPose-Fi.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-based%203D%20Human%20Pose%20Estimation%20using%20WiFi%20Signals&entry.906535625=Jichao%20Chen%20and%20YangYang%20Qu%20and%20Ruibo%20Tang%20and%20Dirk%20Slock&entry.1292438233=WiFi-based%20human%20pose%20estimation%20%28HPE%29%20has%20attracted%20increasing%20attention%20due%20to%20its%20resilience%20to%20occlusion%20and%20privacy-preserving%20compared%20to%20camera-based%20methods.%20However%2C%20existing%20WiFi-based%20HPE%20approaches%20often%20employ%20regression%20networks%20that%20directly%20map%20WiFi%20channel%20state%20information%20%28CSI%29%20to%203D%20joint%20coordinates%2C%20ignoring%20the%20inherent%20topological%20relationships%20among%20human%20joints.%20In%20this%20paper%2C%20we%20present%20GraphPose-Fi%2C%20a%20graph-based%20framework%20that%20explicitly%20models%20skeletal%20topology%20for%20WiFi-based%203D%20HPE.%20Our%20framework%20comprises%20a%20CNN%20encoder%20shared%20across%20antennas%20for%20subcarrier-time%20feature%20extraction%2C%20a%20lightweight%20attention%20module%20that%20adaptively%20reweights%20features%20over%20time%20and%20across%20antennas%2C%20and%20a%20graph-based%20regression%20head%20that%20combines%20GCN%20layers%20with%20self-attention%20to%20capture%20local%20topology%20and%20global%20dependencies.%20Our%20proposed%20method%20significantly%20outperforms%20existing%20methods%20on%20the%20MM-Fi%20dataset%20in%20various%20settings.%20The%20source%20code%20is%20available%20at%3A%20https%3A//github.com/Cirrick/GraphPose-Fi.&entry.1838667208=http%3A//arxiv.org/abs/2511.19105v1&entry.124074799=Read"},
{"title": "Torsion-Space Diffusion for Protein Backbone Generation with Geometric Refinement", "author": "Lakshaditya Singh and Adwait Shelke and Divyansh Agrawal", "abstract": "Designing new protein structures is fundamental to computational biology, enabling advances in therapeutic molecule discovery and enzyme engineering. Existing diffusion-based generative models typically operate in Cartesian coordinate space, where adding noise disrupts strict geometric constraints such as fixed bond lengths and angles, often producing physically invalid structures. To address this limitation, we propose a Torsion-Space Diffusion Model that generates protein backbones by denoising torsion angles, ensuring perfect local geometry by construction. A differentiable forward-kinematics module reconstructs 3D coordinates with fixed 3.8 Angstrom backbone bond lengths while a constrained post-processing refinement optimizes global compactness via Radius of Gyration (Rg) correction, without violating bond constraints. Experiments on standard PDB proteins demonstrate 100% bond-length accuracy and significantly improved structural compactness, reducing Rg error from 70% to 18.6% compared to Cartesian diffusion baselines. Overall, this hybrid torsion-diffusion plus geometric-refinement framework generates physically valid and compact protein backbones, providing a promising path toward full-atom protein generation.", "link": "http://arxiv.org/abs/2511.19184v1", "date": "2025-11-24", "relevancy": 2.6321, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.529}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5251}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Torsion-Space%20Diffusion%20for%20Protein%20Backbone%20Generation%20with%20Geometric%20Refinement&body=Title%3A%20Torsion-Space%20Diffusion%20for%20Protein%20Backbone%20Generation%20with%20Geometric%20Refinement%0AAuthor%3A%20Lakshaditya%20Singh%20and%20Adwait%20Shelke%20and%20Divyansh%20Agrawal%0AAbstract%3A%20Designing%20new%20protein%20structures%20is%20fundamental%20to%20computational%20biology%2C%20enabling%20advances%20in%20therapeutic%20molecule%20discovery%20and%20enzyme%20engineering.%20Existing%20diffusion-based%20generative%20models%20typically%20operate%20in%20Cartesian%20coordinate%20space%2C%20where%20adding%20noise%20disrupts%20strict%20geometric%20constraints%20such%20as%20fixed%20bond%20lengths%20and%20angles%2C%20often%20producing%20physically%20invalid%20structures.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20Torsion-Space%20Diffusion%20Model%20that%20generates%20protein%20backbones%20by%20denoising%20torsion%20angles%2C%20ensuring%20perfect%20local%20geometry%20by%20construction.%20A%20differentiable%20forward-kinematics%20module%20reconstructs%203D%20coordinates%20with%20fixed%203.8%20Angstrom%20backbone%20bond%20lengths%20while%20a%20constrained%20post-processing%20refinement%20optimizes%20global%20compactness%20via%20Radius%20of%20Gyration%20%28Rg%29%20correction%2C%20without%20violating%20bond%20constraints.%20Experiments%20on%20standard%20PDB%20proteins%20demonstrate%20100%25%20bond-length%20accuracy%20and%20significantly%20improved%20structural%20compactness%2C%20reducing%20Rg%20error%20from%2070%25%20to%2018.6%25%20compared%20to%20Cartesian%20diffusion%20baselines.%20Overall%2C%20this%20hybrid%20torsion-diffusion%20plus%20geometric-refinement%20framework%20generates%20physically%20valid%20and%20compact%20protein%20backbones%2C%20providing%20a%20promising%20path%20toward%20full-atom%20protein%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTorsion-Space%2520Diffusion%2520for%2520Protein%2520Backbone%2520Generation%2520with%2520Geometric%2520Refinement%26entry.906535625%3DLakshaditya%2520Singh%2520and%2520Adwait%2520Shelke%2520and%2520Divyansh%2520Agrawal%26entry.1292438233%3DDesigning%2520new%2520protein%2520structures%2520is%2520fundamental%2520to%2520computational%2520biology%252C%2520enabling%2520advances%2520in%2520therapeutic%2520molecule%2520discovery%2520and%2520enzyme%2520engineering.%2520Existing%2520diffusion-based%2520generative%2520models%2520typically%2520operate%2520in%2520Cartesian%2520coordinate%2520space%252C%2520where%2520adding%2520noise%2520disrupts%2520strict%2520geometric%2520constraints%2520such%2520as%2520fixed%2520bond%2520lengths%2520and%2520angles%252C%2520often%2520producing%2520physically%2520invalid%2520structures.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520Torsion-Space%2520Diffusion%2520Model%2520that%2520generates%2520protein%2520backbones%2520by%2520denoising%2520torsion%2520angles%252C%2520ensuring%2520perfect%2520local%2520geometry%2520by%2520construction.%2520A%2520differentiable%2520forward-kinematics%2520module%2520reconstructs%25203D%2520coordinates%2520with%2520fixed%25203.8%2520Angstrom%2520backbone%2520bond%2520lengths%2520while%2520a%2520constrained%2520post-processing%2520refinement%2520optimizes%2520global%2520compactness%2520via%2520Radius%2520of%2520Gyration%2520%2528Rg%2529%2520correction%252C%2520without%2520violating%2520bond%2520constraints.%2520Experiments%2520on%2520standard%2520PDB%2520proteins%2520demonstrate%2520100%2525%2520bond-length%2520accuracy%2520and%2520significantly%2520improved%2520structural%2520compactness%252C%2520reducing%2520Rg%2520error%2520from%252070%2525%2520to%252018.6%2525%2520compared%2520to%2520Cartesian%2520diffusion%2520baselines.%2520Overall%252C%2520this%2520hybrid%2520torsion-diffusion%2520plus%2520geometric-refinement%2520framework%2520generates%2520physically%2520valid%2520and%2520compact%2520protein%2520backbones%252C%2520providing%2520a%2520promising%2520path%2520toward%2520full-atom%2520protein%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Torsion-Space%20Diffusion%20for%20Protein%20Backbone%20Generation%20with%20Geometric%20Refinement&entry.906535625=Lakshaditya%20Singh%20and%20Adwait%20Shelke%20and%20Divyansh%20Agrawal&entry.1292438233=Designing%20new%20protein%20structures%20is%20fundamental%20to%20computational%20biology%2C%20enabling%20advances%20in%20therapeutic%20molecule%20discovery%20and%20enzyme%20engineering.%20Existing%20diffusion-based%20generative%20models%20typically%20operate%20in%20Cartesian%20coordinate%20space%2C%20where%20adding%20noise%20disrupts%20strict%20geometric%20constraints%20such%20as%20fixed%20bond%20lengths%20and%20angles%2C%20often%20producing%20physically%20invalid%20structures.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20Torsion-Space%20Diffusion%20Model%20that%20generates%20protein%20backbones%20by%20denoising%20torsion%20angles%2C%20ensuring%20perfect%20local%20geometry%20by%20construction.%20A%20differentiable%20forward-kinematics%20module%20reconstructs%203D%20coordinates%20with%20fixed%203.8%20Angstrom%20backbone%20bond%20lengths%20while%20a%20constrained%20post-processing%20refinement%20optimizes%20global%20compactness%20via%20Radius%20of%20Gyration%20%28Rg%29%20correction%2C%20without%20violating%20bond%20constraints.%20Experiments%20on%20standard%20PDB%20proteins%20demonstrate%20100%25%20bond-length%20accuracy%20and%20significantly%20improved%20structural%20compactness%2C%20reducing%20Rg%20error%20from%2070%25%20to%2018.6%25%20compared%20to%20Cartesian%20diffusion%20baselines.%20Overall%2C%20this%20hybrid%20torsion-diffusion%20plus%20geometric-refinement%20framework%20generates%20physically%20valid%20and%20compact%20protein%20backbones%2C%20providing%20a%20promising%20path%20toward%20full-atom%20protein%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2511.19184v1&entry.124074799=Read"},
{"title": "What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models", "author": "Roksana Goworek and Olivia Macmillan-Scott and Eda B. \u00d6zyi\u011fit", "abstract": "Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.", "link": "http://arxiv.org/abs/2511.19324v1", "date": "2025-11-24", "relevancy": 2.6122, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Drives%20Cross-lingual%20Ranking%3F%20Retrieval%20Approaches%20with%20Multilingual%20Language%20Models&body=Title%3A%20What%20Drives%20Cross-lingual%20Ranking%3F%20Retrieval%20Approaches%20with%20Multilingual%20Language%20Models%0AAuthor%3A%20Roksana%20Goworek%20and%20Olivia%20Macmillan-Scott%20and%20Eda%20B.%20%C3%96zyi%C4%9Fit%0AAbstract%3A%20Cross-lingual%20information%20retrieval%20%28CLIR%29%20enables%20access%20to%20multilingual%20knowledge%20but%20remains%20challenging%20due%20to%20disparities%20in%20resources%2C%20scripts%2C%20and%20weak%20cross-lingual%20semantic%20alignment%20in%20embedding%20models.%20Existing%20pipelines%20often%20rely%20on%20translation%20and%20monolingual%20retrieval%20heuristics%2C%20which%20add%20computational%20overhead%20and%20noise%2C%20degrading%20performance.%20This%20work%20systematically%20evaluates%20four%20intervention%20types%2C%20namely%20document%20translation%2C%20multilingual%20dense%20retrieval%20with%20pretrained%20encoders%2C%20contrastive%20learning%20at%20word%2C%20phrase%2C%20and%20query-document%20levels%2C%20and%20cross-encoder%20re-ranking%2C%20across%20three%20benchmark%20datasets.%20We%20find%20that%20dense%20retrieval%20models%20trained%20specifically%20for%20CLIR%20consistently%20outperform%20lexical%20matching%20methods%20and%20derive%20little%20benefit%20from%20document%20translation.%20Contrastive%20learning%20mitigates%20language%20biases%20and%20yields%20substantial%20improvements%20for%20encoders%20with%20weak%20initial%20alignment%2C%20and%20re-ranking%20can%20be%20effective%2C%20but%20depends%20on%20the%20quality%20of%20the%20cross-encoder%20training%20data.%20Although%20high-resource%20languages%20still%20dominate%20overall%20performance%2C%20gains%20over%20lexical%20and%20document-translated%20baselines%20are%20most%20pronounced%20for%20low-resource%20and%20cross-script%20pairs.%20These%20findings%20indicate%20that%20cross-lingual%20search%20systems%20should%20prioritise%20semantic%20multilingual%20embeddings%20and%20targeted%20learning-based%20alignment%20over%20translation-based%20pipelines%2C%20particularly%20for%20cross-script%20and%20under-resourced%20languages.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Drives%2520Cross-lingual%2520Ranking%253F%2520Retrieval%2520Approaches%2520with%2520Multilingual%2520Language%2520Models%26entry.906535625%3DRoksana%2520Goworek%2520and%2520Olivia%2520Macmillan-Scott%2520and%2520Eda%2520B.%2520%25C3%2596zyi%25C4%259Fit%26entry.1292438233%3DCross-lingual%2520information%2520retrieval%2520%2528CLIR%2529%2520enables%2520access%2520to%2520multilingual%2520knowledge%2520but%2520remains%2520challenging%2520due%2520to%2520disparities%2520in%2520resources%252C%2520scripts%252C%2520and%2520weak%2520cross-lingual%2520semantic%2520alignment%2520in%2520embedding%2520models.%2520Existing%2520pipelines%2520often%2520rely%2520on%2520translation%2520and%2520monolingual%2520retrieval%2520heuristics%252C%2520which%2520add%2520computational%2520overhead%2520and%2520noise%252C%2520degrading%2520performance.%2520This%2520work%2520systematically%2520evaluates%2520four%2520intervention%2520types%252C%2520namely%2520document%2520translation%252C%2520multilingual%2520dense%2520retrieval%2520with%2520pretrained%2520encoders%252C%2520contrastive%2520learning%2520at%2520word%252C%2520phrase%252C%2520and%2520query-document%2520levels%252C%2520and%2520cross-encoder%2520re-ranking%252C%2520across%2520three%2520benchmark%2520datasets.%2520We%2520find%2520that%2520dense%2520retrieval%2520models%2520trained%2520specifically%2520for%2520CLIR%2520consistently%2520outperform%2520lexical%2520matching%2520methods%2520and%2520derive%2520little%2520benefit%2520from%2520document%2520translation.%2520Contrastive%2520learning%2520mitigates%2520language%2520biases%2520and%2520yields%2520substantial%2520improvements%2520for%2520encoders%2520with%2520weak%2520initial%2520alignment%252C%2520and%2520re-ranking%2520can%2520be%2520effective%252C%2520but%2520depends%2520on%2520the%2520quality%2520of%2520the%2520cross-encoder%2520training%2520data.%2520Although%2520high-resource%2520languages%2520still%2520dominate%2520overall%2520performance%252C%2520gains%2520over%2520lexical%2520and%2520document-translated%2520baselines%2520are%2520most%2520pronounced%2520for%2520low-resource%2520and%2520cross-script%2520pairs.%2520These%2520findings%2520indicate%2520that%2520cross-lingual%2520search%2520systems%2520should%2520prioritise%2520semantic%2520multilingual%2520embeddings%2520and%2520targeted%2520learning-based%2520alignment%2520over%2520translation-based%2520pipelines%252C%2520particularly%2520for%2520cross-script%2520and%2520under-resourced%2520languages.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Drives%20Cross-lingual%20Ranking%3F%20Retrieval%20Approaches%20with%20Multilingual%20Language%20Models&entry.906535625=Roksana%20Goworek%20and%20Olivia%20Macmillan-Scott%20and%20Eda%20B.%20%C3%96zyi%C4%9Fit&entry.1292438233=Cross-lingual%20information%20retrieval%20%28CLIR%29%20enables%20access%20to%20multilingual%20knowledge%20but%20remains%20challenging%20due%20to%20disparities%20in%20resources%2C%20scripts%2C%20and%20weak%20cross-lingual%20semantic%20alignment%20in%20embedding%20models.%20Existing%20pipelines%20often%20rely%20on%20translation%20and%20monolingual%20retrieval%20heuristics%2C%20which%20add%20computational%20overhead%20and%20noise%2C%20degrading%20performance.%20This%20work%20systematically%20evaluates%20four%20intervention%20types%2C%20namely%20document%20translation%2C%20multilingual%20dense%20retrieval%20with%20pretrained%20encoders%2C%20contrastive%20learning%20at%20word%2C%20phrase%2C%20and%20query-document%20levels%2C%20and%20cross-encoder%20re-ranking%2C%20across%20three%20benchmark%20datasets.%20We%20find%20that%20dense%20retrieval%20models%20trained%20specifically%20for%20CLIR%20consistently%20outperform%20lexical%20matching%20methods%20and%20derive%20little%20benefit%20from%20document%20translation.%20Contrastive%20learning%20mitigates%20language%20biases%20and%20yields%20substantial%20improvements%20for%20encoders%20with%20weak%20initial%20alignment%2C%20and%20re-ranking%20can%20be%20effective%2C%20but%20depends%20on%20the%20quality%20of%20the%20cross-encoder%20training%20data.%20Although%20high-resource%20languages%20still%20dominate%20overall%20performance%2C%20gains%20over%20lexical%20and%20document-translated%20baselines%20are%20most%20pronounced%20for%20low-resource%20and%20cross-script%20pairs.%20These%20findings%20indicate%20that%20cross-lingual%20search%20systems%20should%20prioritise%20semantic%20multilingual%20embeddings%20and%20targeted%20learning-based%20alignment%20over%20translation-based%20pipelines%2C%20particularly%20for%20cross-script%20and%20under-resourced%20languages.&entry.1838667208=http%3A//arxiv.org/abs/2511.19324v1&entry.124074799=Read"},
{"title": "Automatic Multi-View X-Ray/CT Registration Using Bone Substructure Contours", "author": "Roman Flepp and Leon Nissen and Bastian Sigrist and Arend Nieuwland and Nicola Cavalcanti and Philipp F\u00fcrnstahl and Thomas Dreher and Lilian Calvet", "abstract": "Purpose: Accurate intraoperative X-ray/CT registration is essential for surgical navigation in orthopedic procedures. However, existing methods struggle with consistently achieving sub-millimeter accuracy, robustness under broad initial pose estimates or need manual key-point annotations. This work aims to address these challenges by proposing a novel multi-view X-ray/CT registration method for intraoperative bone registration. Methods: The proposed registration method consists of a multi-view, contour-based iterative closest point (ICP) optimization. Unlike previous methods, which attempt to match bone contours across the entire silhouette in both imaging modalities, we focus on matching specific subcategories of contours corresponding to bone substructures. This leads to reduced ambiguity in the ICP matches, resulting in a more robust and accurate registration solution. This approach requires only two X-ray images and operates fully automatically. Additionally, we contribute a dataset of 5 cadaveric specimens, including real X-ray images, X-ray image poses and the corresponding CT scans. Results: The proposed registration method is evaluated on real X-ray images using mean reprojection error (mRPD). The method consistently achieves sub-millimeter accuracy with a mRPD 0.67mm compared to 5.35mm by a commercial solution requiring manual intervention. Furthermore, the method offers improved practical applicability, being fully automatic. Conclusion: Our method offers a practical, accurate, and efficient solution for multi-view X-ray/CT registration in orthopedic surgeries, which can be easily combined with tracking systems. By improving registration accuracy and minimizing manual intervention, it enhances intraoperative navigation, contributing to more accurate and effective surgical outcomes in computer-assisted surgery (CAS).", "link": "http://arxiv.org/abs/2506.13292v2", "date": "2025-11-24", "relevancy": 2.6043, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5365}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.513}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Multi-View%20X-Ray/CT%20Registration%20Using%20Bone%20Substructure%20Contours&body=Title%3A%20Automatic%20Multi-View%20X-Ray/CT%20Registration%20Using%20Bone%20Substructure%20Contours%0AAuthor%3A%20Roman%20Flepp%20and%20Leon%20Nissen%20and%20Bastian%20Sigrist%20and%20Arend%20Nieuwland%20and%20Nicola%20Cavalcanti%20and%20Philipp%20F%C3%BCrnstahl%20and%20Thomas%20Dreher%20and%20Lilian%20Calvet%0AAbstract%3A%20Purpose%3A%20Accurate%20intraoperative%20X-ray/CT%20registration%20is%20essential%20for%20surgical%20navigation%20in%20orthopedic%20procedures.%20However%2C%20existing%20methods%20struggle%20with%20consistently%20achieving%20sub-millimeter%20accuracy%2C%20robustness%20under%20broad%20initial%20pose%20estimates%20or%20need%20manual%20key-point%20annotations.%20This%20work%20aims%20to%20address%20these%20challenges%20by%20proposing%20a%20novel%20multi-view%20X-ray/CT%20registration%20method%20for%20intraoperative%20bone%20registration.%20Methods%3A%20The%20proposed%20registration%20method%20consists%20of%20a%20multi-view%2C%20contour-based%20iterative%20closest%20point%20%28ICP%29%20optimization.%20Unlike%20previous%20methods%2C%20which%20attempt%20to%20match%20bone%20contours%20across%20the%20entire%20silhouette%20in%20both%20imaging%20modalities%2C%20we%20focus%20on%20matching%20specific%20subcategories%20of%20contours%20corresponding%20to%20bone%20substructures.%20This%20leads%20to%20reduced%20ambiguity%20in%20the%20ICP%20matches%2C%20resulting%20in%20a%20more%20robust%20and%20accurate%20registration%20solution.%20This%20approach%20requires%20only%20two%20X-ray%20images%20and%20operates%20fully%20automatically.%20Additionally%2C%20we%20contribute%20a%20dataset%20of%205%20cadaveric%20specimens%2C%20including%20real%20X-ray%20images%2C%20X-ray%20image%20poses%20and%20the%20corresponding%20CT%20scans.%20Results%3A%20The%20proposed%20registration%20method%20is%20evaluated%20on%20real%20X-ray%20images%20using%20mean%20reprojection%20error%20%28mRPD%29.%20The%20method%20consistently%20achieves%20sub-millimeter%20accuracy%20with%20a%20mRPD%200.67mm%20compared%20to%205.35mm%20by%20a%20commercial%20solution%20requiring%20manual%20intervention.%20Furthermore%2C%20the%20method%20offers%20improved%20practical%20applicability%2C%20being%20fully%20automatic.%20Conclusion%3A%20Our%20method%20offers%20a%20practical%2C%20accurate%2C%20and%20efficient%20solution%20for%20multi-view%20X-ray/CT%20registration%20in%20orthopedic%20surgeries%2C%20which%20can%20be%20easily%20combined%20with%20tracking%20systems.%20By%20improving%20registration%20accuracy%20and%20minimizing%20manual%20intervention%2C%20it%20enhances%20intraoperative%20navigation%2C%20contributing%20to%20more%20accurate%20and%20effective%20surgical%20outcomes%20in%20computer-assisted%20surgery%20%28CAS%29.%0ALink%3A%20http%3A//arxiv.org/abs/2506.13292v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Multi-View%2520X-Ray/CT%2520Registration%2520Using%2520Bone%2520Substructure%2520Contours%26entry.906535625%3DRoman%2520Flepp%2520and%2520Leon%2520Nissen%2520and%2520Bastian%2520Sigrist%2520and%2520Arend%2520Nieuwland%2520and%2520Nicola%2520Cavalcanti%2520and%2520Philipp%2520F%25C3%25BCrnstahl%2520and%2520Thomas%2520Dreher%2520and%2520Lilian%2520Calvet%26entry.1292438233%3DPurpose%253A%2520Accurate%2520intraoperative%2520X-ray/CT%2520registration%2520is%2520essential%2520for%2520surgical%2520navigation%2520in%2520orthopedic%2520procedures.%2520However%252C%2520existing%2520methods%2520struggle%2520with%2520consistently%2520achieving%2520sub-millimeter%2520accuracy%252C%2520robustness%2520under%2520broad%2520initial%2520pose%2520estimates%2520or%2520need%2520manual%2520key-point%2520annotations.%2520This%2520work%2520aims%2520to%2520address%2520these%2520challenges%2520by%2520proposing%2520a%2520novel%2520multi-view%2520X-ray/CT%2520registration%2520method%2520for%2520intraoperative%2520bone%2520registration.%2520Methods%253A%2520The%2520proposed%2520registration%2520method%2520consists%2520of%2520a%2520multi-view%252C%2520contour-based%2520iterative%2520closest%2520point%2520%2528ICP%2529%2520optimization.%2520Unlike%2520previous%2520methods%252C%2520which%2520attempt%2520to%2520match%2520bone%2520contours%2520across%2520the%2520entire%2520silhouette%2520in%2520both%2520imaging%2520modalities%252C%2520we%2520focus%2520on%2520matching%2520specific%2520subcategories%2520of%2520contours%2520corresponding%2520to%2520bone%2520substructures.%2520This%2520leads%2520to%2520reduced%2520ambiguity%2520in%2520the%2520ICP%2520matches%252C%2520resulting%2520in%2520a%2520more%2520robust%2520and%2520accurate%2520registration%2520solution.%2520This%2520approach%2520requires%2520only%2520two%2520X-ray%2520images%2520and%2520operates%2520fully%2520automatically.%2520Additionally%252C%2520we%2520contribute%2520a%2520dataset%2520of%25205%2520cadaveric%2520specimens%252C%2520including%2520real%2520X-ray%2520images%252C%2520X-ray%2520image%2520poses%2520and%2520the%2520corresponding%2520CT%2520scans.%2520Results%253A%2520The%2520proposed%2520registration%2520method%2520is%2520evaluated%2520on%2520real%2520X-ray%2520images%2520using%2520mean%2520reprojection%2520error%2520%2528mRPD%2529.%2520The%2520method%2520consistently%2520achieves%2520sub-millimeter%2520accuracy%2520with%2520a%2520mRPD%25200.67mm%2520compared%2520to%25205.35mm%2520by%2520a%2520commercial%2520solution%2520requiring%2520manual%2520intervention.%2520Furthermore%252C%2520the%2520method%2520offers%2520improved%2520practical%2520applicability%252C%2520being%2520fully%2520automatic.%2520Conclusion%253A%2520Our%2520method%2520offers%2520a%2520practical%252C%2520accurate%252C%2520and%2520efficient%2520solution%2520for%2520multi-view%2520X-ray/CT%2520registration%2520in%2520orthopedic%2520surgeries%252C%2520which%2520can%2520be%2520easily%2520combined%2520with%2520tracking%2520systems.%2520By%2520improving%2520registration%2520accuracy%2520and%2520minimizing%2520manual%2520intervention%252C%2520it%2520enhances%2520intraoperative%2520navigation%252C%2520contributing%2520to%2520more%2520accurate%2520and%2520effective%2520surgical%2520outcomes%2520in%2520computer-assisted%2520surgery%2520%2528CAS%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13292v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Multi-View%20X-Ray/CT%20Registration%20Using%20Bone%20Substructure%20Contours&entry.906535625=Roman%20Flepp%20and%20Leon%20Nissen%20and%20Bastian%20Sigrist%20and%20Arend%20Nieuwland%20and%20Nicola%20Cavalcanti%20and%20Philipp%20F%C3%BCrnstahl%20and%20Thomas%20Dreher%20and%20Lilian%20Calvet&entry.1292438233=Purpose%3A%20Accurate%20intraoperative%20X-ray/CT%20registration%20is%20essential%20for%20surgical%20navigation%20in%20orthopedic%20procedures.%20However%2C%20existing%20methods%20struggle%20with%20consistently%20achieving%20sub-millimeter%20accuracy%2C%20robustness%20under%20broad%20initial%20pose%20estimates%20or%20need%20manual%20key-point%20annotations.%20This%20work%20aims%20to%20address%20these%20challenges%20by%20proposing%20a%20novel%20multi-view%20X-ray/CT%20registration%20method%20for%20intraoperative%20bone%20registration.%20Methods%3A%20The%20proposed%20registration%20method%20consists%20of%20a%20multi-view%2C%20contour-based%20iterative%20closest%20point%20%28ICP%29%20optimization.%20Unlike%20previous%20methods%2C%20which%20attempt%20to%20match%20bone%20contours%20across%20the%20entire%20silhouette%20in%20both%20imaging%20modalities%2C%20we%20focus%20on%20matching%20specific%20subcategories%20of%20contours%20corresponding%20to%20bone%20substructures.%20This%20leads%20to%20reduced%20ambiguity%20in%20the%20ICP%20matches%2C%20resulting%20in%20a%20more%20robust%20and%20accurate%20registration%20solution.%20This%20approach%20requires%20only%20two%20X-ray%20images%20and%20operates%20fully%20automatically.%20Additionally%2C%20we%20contribute%20a%20dataset%20of%205%20cadaveric%20specimens%2C%20including%20real%20X-ray%20images%2C%20X-ray%20image%20poses%20and%20the%20corresponding%20CT%20scans.%20Results%3A%20The%20proposed%20registration%20method%20is%20evaluated%20on%20real%20X-ray%20images%20using%20mean%20reprojection%20error%20%28mRPD%29.%20The%20method%20consistently%20achieves%20sub-millimeter%20accuracy%20with%20a%20mRPD%200.67mm%20compared%20to%205.35mm%20by%20a%20commercial%20solution%20requiring%20manual%20intervention.%20Furthermore%2C%20the%20method%20offers%20improved%20practical%20applicability%2C%20being%20fully%20automatic.%20Conclusion%3A%20Our%20method%20offers%20a%20practical%2C%20accurate%2C%20and%20efficient%20solution%20for%20multi-view%20X-ray/CT%20registration%20in%20orthopedic%20surgeries%2C%20which%20can%20be%20easily%20combined%20with%20tracking%20systems.%20By%20improving%20registration%20accuracy%20and%20minimizing%20manual%20intervention%2C%20it%20enhances%20intraoperative%20navigation%2C%20contributing%20to%20more%20accurate%20and%20effective%20surgical%20outcomes%20in%20computer-assisted%20surgery%20%28CAS%29.&entry.1838667208=http%3A//arxiv.org/abs/2506.13292v2&entry.124074799=Read"},
{"title": "Life-IQA: Boosting Blind Image Quality Assessment through GCN-enhanced Layer Interaction and MoE-based Feature Decoupling", "author": "Long Tang and Guoquan Zhen and Jie Hao and Jianbo Zhang and Huiyu Duan and Liang Yuan and Guangtao Zhai", "abstract": "Blind image quality assessment (BIQA) plays a crucial role in evaluating and optimizing visual experience. Most existing BIQA approaches fuse shallow and deep features extracted from backbone networks, while overlooking the unequal contributions to quality prediction. Moreover, while various vision encoder backbones are widely adopted in BIQA, the effective quality decoding architectures remain underexplored. To address these limitations, this paper investigates the contributions of shallow and deep features to BIQA, and proposes a effective quality feature decoding framework via GCN-enhanced \\underline{l}ayer\\underline{i}nteraction and MoE-based \\underline{f}eature d\\underline{e}coupling, termed \\textbf{(Life-IQA)}. Specifically, the GCN-enhanced layer interaction module utilizes the GCN-enhanced deepest-layer features as query and the penultimate-layer features as key, value, then performs cross-attention to achieve feature interaction. Moreover, a MoE-based feature decoupling module is proposed to decouple fused representations though different experts specialized for specific distortion types or quality dimensions. Extensive experiments demonstrate that Life-IQA shows more favorable balance between accuracy and cost than a vanilla Transformer decoder and achieves state-of-the-art performance on multiple BIQA benchmarks.The code is available at: \\href{https://github.com/TANGLONG2/Life-IQA/tree/main}{\\texttt{Life-IQA}}.", "link": "http://arxiv.org/abs/2511.19024v1", "date": "2025-11-24", "relevancy": 2.578, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Life-IQA%3A%20Boosting%20Blind%20Image%20Quality%20Assessment%20through%20GCN-enhanced%20Layer%20Interaction%20and%20MoE-based%20Feature%20Decoupling&body=Title%3A%20Life-IQA%3A%20Boosting%20Blind%20Image%20Quality%20Assessment%20through%20GCN-enhanced%20Layer%20Interaction%20and%20MoE-based%20Feature%20Decoupling%0AAuthor%3A%20Long%20Tang%20and%20Guoquan%20Zhen%20and%20Jie%20Hao%20and%20Jianbo%20Zhang%20and%20Huiyu%20Duan%20and%20Liang%20Yuan%20and%20Guangtao%20Zhai%0AAbstract%3A%20Blind%20image%20quality%20assessment%20%28BIQA%29%20plays%20a%20crucial%20role%20in%20evaluating%20and%20optimizing%20visual%20experience.%20Most%20existing%20BIQA%20approaches%20fuse%20shallow%20and%20deep%20features%20extracted%20from%20backbone%20networks%2C%20while%20overlooking%20the%20unequal%20contributions%20to%20quality%20prediction.%20Moreover%2C%20while%20various%20vision%20encoder%20backbones%20are%20widely%20adopted%20in%20BIQA%2C%20the%20effective%20quality%20decoding%20architectures%20remain%20underexplored.%20To%20address%20these%20limitations%2C%20this%20paper%20investigates%20the%20contributions%20of%20shallow%20and%20deep%20features%20to%20BIQA%2C%20and%20proposes%20a%20effective%20quality%20feature%20decoding%20framework%20via%20GCN-enhanced%20%5Cunderline%7Bl%7Dayer%5Cunderline%7Bi%7Dnteraction%20and%20MoE-based%20%5Cunderline%7Bf%7Deature%20d%5Cunderline%7Be%7Dcoupling%2C%20termed%20%5Ctextbf%7B%28Life-IQA%29%7D.%20Specifically%2C%20the%20GCN-enhanced%20layer%20interaction%20module%20utilizes%20the%20GCN-enhanced%20deepest-layer%20features%20as%20query%20and%20the%20penultimate-layer%20features%20as%20key%2C%20value%2C%20then%20performs%20cross-attention%20to%20achieve%20feature%20interaction.%20Moreover%2C%20a%20MoE-based%20feature%20decoupling%20module%20is%20proposed%20to%20decouple%20fused%20representations%20though%20different%20experts%20specialized%20for%20specific%20distortion%20types%20or%20quality%20dimensions.%20Extensive%20experiments%20demonstrate%20that%20Life-IQA%20shows%20more%20favorable%20balance%20between%20accuracy%20and%20cost%20than%20a%20vanilla%20Transformer%20decoder%20and%20achieves%20state-of-the-art%20performance%20on%20multiple%20BIQA%20benchmarks.The%20code%20is%20available%20at%3A%20%5Chref%7Bhttps%3A//github.com/TANGLONG2/Life-IQA/tree/main%7D%7B%5Ctexttt%7BLife-IQA%7D%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLife-IQA%253A%2520Boosting%2520Blind%2520Image%2520Quality%2520Assessment%2520through%2520GCN-enhanced%2520Layer%2520Interaction%2520and%2520MoE-based%2520Feature%2520Decoupling%26entry.906535625%3DLong%2520Tang%2520and%2520Guoquan%2520Zhen%2520and%2520Jie%2520Hao%2520and%2520Jianbo%2520Zhang%2520and%2520Huiyu%2520Duan%2520and%2520Liang%2520Yuan%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3DBlind%2520image%2520quality%2520assessment%2520%2528BIQA%2529%2520plays%2520a%2520crucial%2520role%2520in%2520evaluating%2520and%2520optimizing%2520visual%2520experience.%2520Most%2520existing%2520BIQA%2520approaches%2520fuse%2520shallow%2520and%2520deep%2520features%2520extracted%2520from%2520backbone%2520networks%252C%2520while%2520overlooking%2520the%2520unequal%2520contributions%2520to%2520quality%2520prediction.%2520Moreover%252C%2520while%2520various%2520vision%2520encoder%2520backbones%2520are%2520widely%2520adopted%2520in%2520BIQA%252C%2520the%2520effective%2520quality%2520decoding%2520architectures%2520remain%2520underexplored.%2520To%2520address%2520these%2520limitations%252C%2520this%2520paper%2520investigates%2520the%2520contributions%2520of%2520shallow%2520and%2520deep%2520features%2520to%2520BIQA%252C%2520and%2520proposes%2520a%2520effective%2520quality%2520feature%2520decoding%2520framework%2520via%2520GCN-enhanced%2520%255Cunderline%257Bl%257Dayer%255Cunderline%257Bi%257Dnteraction%2520and%2520MoE-based%2520%255Cunderline%257Bf%257Deature%2520d%255Cunderline%257Be%257Dcoupling%252C%2520termed%2520%255Ctextbf%257B%2528Life-IQA%2529%257D.%2520Specifically%252C%2520the%2520GCN-enhanced%2520layer%2520interaction%2520module%2520utilizes%2520the%2520GCN-enhanced%2520deepest-layer%2520features%2520as%2520query%2520and%2520the%2520penultimate-layer%2520features%2520as%2520key%252C%2520value%252C%2520then%2520performs%2520cross-attention%2520to%2520achieve%2520feature%2520interaction.%2520Moreover%252C%2520a%2520MoE-based%2520feature%2520decoupling%2520module%2520is%2520proposed%2520to%2520decouple%2520fused%2520representations%2520though%2520different%2520experts%2520specialized%2520for%2520specific%2520distortion%2520types%2520or%2520quality%2520dimensions.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Life-IQA%2520shows%2520more%2520favorable%2520balance%2520between%2520accuracy%2520and%2520cost%2520than%2520a%2520vanilla%2520Transformer%2520decoder%2520and%2520achieves%2520state-of-the-art%2520performance%2520on%2520multiple%2520BIQA%2520benchmarks.The%2520code%2520is%2520available%2520at%253A%2520%255Chref%257Bhttps%253A//github.com/TANGLONG2/Life-IQA/tree/main%257D%257B%255Ctexttt%257BLife-IQA%257D%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Life-IQA%3A%20Boosting%20Blind%20Image%20Quality%20Assessment%20through%20GCN-enhanced%20Layer%20Interaction%20and%20MoE-based%20Feature%20Decoupling&entry.906535625=Long%20Tang%20and%20Guoquan%20Zhen%20and%20Jie%20Hao%20and%20Jianbo%20Zhang%20and%20Huiyu%20Duan%20and%20Liang%20Yuan%20and%20Guangtao%20Zhai&entry.1292438233=Blind%20image%20quality%20assessment%20%28BIQA%29%20plays%20a%20crucial%20role%20in%20evaluating%20and%20optimizing%20visual%20experience.%20Most%20existing%20BIQA%20approaches%20fuse%20shallow%20and%20deep%20features%20extracted%20from%20backbone%20networks%2C%20while%20overlooking%20the%20unequal%20contributions%20to%20quality%20prediction.%20Moreover%2C%20while%20various%20vision%20encoder%20backbones%20are%20widely%20adopted%20in%20BIQA%2C%20the%20effective%20quality%20decoding%20architectures%20remain%20underexplored.%20To%20address%20these%20limitations%2C%20this%20paper%20investigates%20the%20contributions%20of%20shallow%20and%20deep%20features%20to%20BIQA%2C%20and%20proposes%20a%20effective%20quality%20feature%20decoding%20framework%20via%20GCN-enhanced%20%5Cunderline%7Bl%7Dayer%5Cunderline%7Bi%7Dnteraction%20and%20MoE-based%20%5Cunderline%7Bf%7Deature%20d%5Cunderline%7Be%7Dcoupling%2C%20termed%20%5Ctextbf%7B%28Life-IQA%29%7D.%20Specifically%2C%20the%20GCN-enhanced%20layer%20interaction%20module%20utilizes%20the%20GCN-enhanced%20deepest-layer%20features%20as%20query%20and%20the%20penultimate-layer%20features%20as%20key%2C%20value%2C%20then%20performs%20cross-attention%20to%20achieve%20feature%20interaction.%20Moreover%2C%20a%20MoE-based%20feature%20decoupling%20module%20is%20proposed%20to%20decouple%20fused%20representations%20though%20different%20experts%20specialized%20for%20specific%20distortion%20types%20or%20quality%20dimensions.%20Extensive%20experiments%20demonstrate%20that%20Life-IQA%20shows%20more%20favorable%20balance%20between%20accuracy%20and%20cost%20than%20a%20vanilla%20Transformer%20decoder%20and%20achieves%20state-of-the-art%20performance%20on%20multiple%20BIQA%20benchmarks.The%20code%20is%20available%20at%3A%20%5Chref%7Bhttps%3A//github.com/TANGLONG2/Life-IQA/tree/main%7D%7B%5Ctexttt%7BLife-IQA%7D%7D.&entry.1838667208=http%3A//arxiv.org/abs/2511.19024v1&entry.124074799=Read"},
{"title": "RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning", "author": "Deyi Ji and Yuekui Yang and Liqun Liu and Peng Shu and Haiyang Wu and Shaogang Tang and Xudong Chen and Shaoping Ma and Tianrun Chen and Lanyun Zhu", "abstract": "Advertising (Ad) is a cornerstone of the digital economy, yet the moderation of video advertisements remains a significant challenge due to their complexity and the need for precise violation localization. While recent advancements, such as the RAVEN model, have improved coarse-grained violation detection, critical gaps persist in fine-grained understanding, explainability, and generalization. To address these limitations, we propose RAVEN++, a novel framework that introduces three key innovations: 1) Active Reinforcement Learning (RL), which dynamically adapts training to samples of varying difficulty; 2) Fine-Grained Violation Understanding, achieved through hierarchical reward functions and reasoning distillation; and 3) Progressive Multi-Stage Training, which systematically combines knowledge injection, curriculum-based passive RL, and active RL. Extensive experiments on both public and proprietary datasets, on both offline scenarios and online deployed A/B Testing, demonstrate that RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in terms of fine-grained violation understanding, reasoning capabilities, and generalization ability.", "link": "http://arxiv.org/abs/2511.19168v1", "date": "2025-11-24", "relevancy": 2.5565, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.524}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAVEN%2B%2B%3A%20Pinpointing%20Fine-Grained%20Violations%20in%20Advertisement%20Videos%20with%20Active%20Reinforcement%20Reasoning&body=Title%3A%20RAVEN%2B%2B%3A%20Pinpointing%20Fine-Grained%20Violations%20in%20Advertisement%20Videos%20with%20Active%20Reinforcement%20Reasoning%0AAuthor%3A%20Deyi%20Ji%20and%20Yuekui%20Yang%20and%20Liqun%20Liu%20and%20Peng%20Shu%20and%20Haiyang%20Wu%20and%20Shaogang%20Tang%20and%20Xudong%20Chen%20and%20Shaoping%20Ma%20and%20Tianrun%20Chen%20and%20Lanyun%20Zhu%0AAbstract%3A%20Advertising%20%28Ad%29%20is%20a%20cornerstone%20of%20the%20digital%20economy%2C%20yet%20the%20moderation%20of%20video%20advertisements%20remains%20a%20significant%20challenge%20due%20to%20their%20complexity%20and%20the%20need%20for%20precise%20violation%20localization.%20While%20recent%20advancements%2C%20such%20as%20the%20RAVEN%20model%2C%20have%20improved%20coarse-grained%20violation%20detection%2C%20critical%20gaps%20persist%20in%20fine-grained%20understanding%2C%20explainability%2C%20and%20generalization.%20To%20address%20these%20limitations%2C%20we%20propose%20RAVEN%2B%2B%2C%20a%20novel%20framework%20that%20introduces%20three%20key%20innovations%3A%201%29%20Active%20Reinforcement%20Learning%20%28RL%29%2C%20which%20dynamically%20adapts%20training%20to%20samples%20of%20varying%20difficulty%3B%202%29%20Fine-Grained%20Violation%20Understanding%2C%20achieved%20through%20hierarchical%20reward%20functions%20and%20reasoning%20distillation%3B%20and%203%29%20Progressive%20Multi-Stage%20Training%2C%20which%20systematically%20combines%20knowledge%20injection%2C%20curriculum-based%20passive%20RL%2C%20and%20active%20RL.%20Extensive%20experiments%20on%20both%20public%20and%20proprietary%20datasets%2C%20on%20both%20offline%20scenarios%20and%20online%20deployed%20A/B%20Testing%2C%20demonstrate%20that%20RAVEN%2B%2B%20outperforms%20general-purpose%20LLMs%20and%20specialized%20models%20like%20RAVEN%20in%20terms%20of%20fine-grained%20violation%20understanding%2C%20reasoning%20capabilities%2C%20and%20generalization%20ability.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAVEN%252B%252B%253A%2520Pinpointing%2520Fine-Grained%2520Violations%2520in%2520Advertisement%2520Videos%2520with%2520Active%2520Reinforcement%2520Reasoning%26entry.906535625%3DDeyi%2520Ji%2520and%2520Yuekui%2520Yang%2520and%2520Liqun%2520Liu%2520and%2520Peng%2520Shu%2520and%2520Haiyang%2520Wu%2520and%2520Shaogang%2520Tang%2520and%2520Xudong%2520Chen%2520and%2520Shaoping%2520Ma%2520and%2520Tianrun%2520Chen%2520and%2520Lanyun%2520Zhu%26entry.1292438233%3DAdvertising%2520%2528Ad%2529%2520is%2520a%2520cornerstone%2520of%2520the%2520digital%2520economy%252C%2520yet%2520the%2520moderation%2520of%2520video%2520advertisements%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520their%2520complexity%2520and%2520the%2520need%2520for%2520precise%2520violation%2520localization.%2520While%2520recent%2520advancements%252C%2520such%2520as%2520the%2520RAVEN%2520model%252C%2520have%2520improved%2520coarse-grained%2520violation%2520detection%252C%2520critical%2520gaps%2520persist%2520in%2520fine-grained%2520understanding%252C%2520explainability%252C%2520and%2520generalization.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520RAVEN%252B%252B%252C%2520a%2520novel%2520framework%2520that%2520introduces%2520three%2520key%2520innovations%253A%25201%2529%2520Active%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520which%2520dynamically%2520adapts%2520training%2520to%2520samples%2520of%2520varying%2520difficulty%253B%25202%2529%2520Fine-Grained%2520Violation%2520Understanding%252C%2520achieved%2520through%2520hierarchical%2520reward%2520functions%2520and%2520reasoning%2520distillation%253B%2520and%25203%2529%2520Progressive%2520Multi-Stage%2520Training%252C%2520which%2520systematically%2520combines%2520knowledge%2520injection%252C%2520curriculum-based%2520passive%2520RL%252C%2520and%2520active%2520RL.%2520Extensive%2520experiments%2520on%2520both%2520public%2520and%2520proprietary%2520datasets%252C%2520on%2520both%2520offline%2520scenarios%2520and%2520online%2520deployed%2520A/B%2520Testing%252C%2520demonstrate%2520that%2520RAVEN%252B%252B%2520outperforms%2520general-purpose%2520LLMs%2520and%2520specialized%2520models%2520like%2520RAVEN%2520in%2520terms%2520of%2520fine-grained%2520violation%2520understanding%252C%2520reasoning%2520capabilities%252C%2520and%2520generalization%2520ability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAVEN%2B%2B%3A%20Pinpointing%20Fine-Grained%20Violations%20in%20Advertisement%20Videos%20with%20Active%20Reinforcement%20Reasoning&entry.906535625=Deyi%20Ji%20and%20Yuekui%20Yang%20and%20Liqun%20Liu%20and%20Peng%20Shu%20and%20Haiyang%20Wu%20and%20Shaogang%20Tang%20and%20Xudong%20Chen%20and%20Shaoping%20Ma%20and%20Tianrun%20Chen%20and%20Lanyun%20Zhu&entry.1292438233=Advertising%20%28Ad%29%20is%20a%20cornerstone%20of%20the%20digital%20economy%2C%20yet%20the%20moderation%20of%20video%20advertisements%20remains%20a%20significant%20challenge%20due%20to%20their%20complexity%20and%20the%20need%20for%20precise%20violation%20localization.%20While%20recent%20advancements%2C%20such%20as%20the%20RAVEN%20model%2C%20have%20improved%20coarse-grained%20violation%20detection%2C%20critical%20gaps%20persist%20in%20fine-grained%20understanding%2C%20explainability%2C%20and%20generalization.%20To%20address%20these%20limitations%2C%20we%20propose%20RAVEN%2B%2B%2C%20a%20novel%20framework%20that%20introduces%20three%20key%20innovations%3A%201%29%20Active%20Reinforcement%20Learning%20%28RL%29%2C%20which%20dynamically%20adapts%20training%20to%20samples%20of%20varying%20difficulty%3B%202%29%20Fine-Grained%20Violation%20Understanding%2C%20achieved%20through%20hierarchical%20reward%20functions%20and%20reasoning%20distillation%3B%20and%203%29%20Progressive%20Multi-Stage%20Training%2C%20which%20systematically%20combines%20knowledge%20injection%2C%20curriculum-based%20passive%20RL%2C%20and%20active%20RL.%20Extensive%20experiments%20on%20both%20public%20and%20proprietary%20datasets%2C%20on%20both%20offline%20scenarios%20and%20online%20deployed%20A/B%20Testing%2C%20demonstrate%20that%20RAVEN%2B%2B%20outperforms%20general-purpose%20LLMs%20and%20specialized%20models%20like%20RAVEN%20in%20terms%20of%20fine-grained%20violation%20understanding%2C%20reasoning%20capabilities%2C%20and%20generalization%20ability.&entry.1838667208=http%3A//arxiv.org/abs/2511.19168v1&entry.124074799=Read"},
{"title": "Benchmarking Corruption Robustness of LVLMs: A Discriminative Benchmark and Robustness Alignment Metric", "author": "Xiangjie Sui and Songyang Li and Hanwei Zhu and Baoliang Chen and Yuming Fang and Xin Sun", "abstract": "Despite the remarkable reasoning abilities of large vision-language models (LVLMs), their robustness under visual corruptions remains insufficiently studied. Existing evaluation paradigms exhibit two major limitations: 1) the dominance of low-discriminative samples in current datasets masks the real robustness gap between models; and 2) conventional accuracy-based metric fail to capture the degradation of the underlying prediction structure. To bridge these gaps, we introduce Bench-C, a comprehensive benchmark emphasizing discriminative samples for assessing corruption robustness, where a selection strategy is proposed to jointly consider the prediction inconsistency under corruption and the semantic diversity. Furthermore, we propose the Robustness Alignment Score (RAS), a unified metric that measures degradation in logit-level prediction structure by considering the shifts in prediction uncertainty and calibration alignment. Comprehensive experiments and analysis reveal several interesting findings: 1) model behaviors exhibit distinguish patterns under corruptions, such as erroneous confidence and hesitation; 2) despite subtle corruption may lead to a slight accuracy gain, the overall prediction structure still degrades; 3) by decomposing corruption robustness into destructive and corrective components, the distinct failure and recovery patterns across models can be revealed.", "link": "http://arxiv.org/abs/2511.19032v1", "date": "2025-11-24", "relevancy": 2.5531, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5353}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Corruption%20Robustness%20of%20LVLMs%3A%20A%20Discriminative%20Benchmark%20and%20Robustness%20Alignment%20Metric&body=Title%3A%20Benchmarking%20Corruption%20Robustness%20of%20LVLMs%3A%20A%20Discriminative%20Benchmark%20and%20Robustness%20Alignment%20Metric%0AAuthor%3A%20Xiangjie%20Sui%20and%20Songyang%20Li%20and%20Hanwei%20Zhu%20and%20Baoliang%20Chen%20and%20Yuming%20Fang%20and%20Xin%20Sun%0AAbstract%3A%20Despite%20the%20remarkable%20reasoning%20abilities%20of%20large%20vision-language%20models%20%28LVLMs%29%2C%20their%20robustness%20under%20visual%20corruptions%20remains%20insufficiently%20studied.%20Existing%20evaluation%20paradigms%20exhibit%20two%20major%20limitations%3A%201%29%20the%20dominance%20of%20low-discriminative%20samples%20in%20current%20datasets%20masks%20the%20real%20robustness%20gap%20between%20models%3B%20and%202%29%20conventional%20accuracy-based%20metric%20fail%20to%20capture%20the%20degradation%20of%20the%20underlying%20prediction%20structure.%20To%20bridge%20these%20gaps%2C%20we%20introduce%20Bench-C%2C%20a%20comprehensive%20benchmark%20emphasizing%20discriminative%20samples%20for%20assessing%20corruption%20robustness%2C%20where%20a%20selection%20strategy%20is%20proposed%20to%20jointly%20consider%20the%20prediction%20inconsistency%20under%20corruption%20and%20the%20semantic%20diversity.%20Furthermore%2C%20we%20propose%20the%20Robustness%20Alignment%20Score%20%28RAS%29%2C%20a%20unified%20metric%20that%20measures%20degradation%20in%20logit-level%20prediction%20structure%20by%20considering%20the%20shifts%20in%20prediction%20uncertainty%20and%20calibration%20alignment.%20Comprehensive%20experiments%20and%20analysis%20reveal%20several%20interesting%20findings%3A%201%29%20model%20behaviors%20exhibit%20distinguish%20patterns%20under%20corruptions%2C%20such%20as%20erroneous%20confidence%20and%20hesitation%3B%202%29%20despite%20subtle%20corruption%20may%20lead%20to%20a%20slight%20accuracy%20gain%2C%20the%20overall%20prediction%20structure%20still%20degrades%3B%203%29%20by%20decomposing%20corruption%20robustness%20into%20destructive%20and%20corrective%20components%2C%20the%20distinct%20failure%20and%20recovery%20patterns%20across%20models%20can%20be%20revealed.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Corruption%2520Robustness%2520of%2520LVLMs%253A%2520A%2520Discriminative%2520Benchmark%2520and%2520Robustness%2520Alignment%2520Metric%26entry.906535625%3DXiangjie%2520Sui%2520and%2520Songyang%2520Li%2520and%2520Hanwei%2520Zhu%2520and%2520Baoliang%2520Chen%2520and%2520Yuming%2520Fang%2520and%2520Xin%2520Sun%26entry.1292438233%3DDespite%2520the%2520remarkable%2520reasoning%2520abilities%2520of%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%252C%2520their%2520robustness%2520under%2520visual%2520corruptions%2520remains%2520insufficiently%2520studied.%2520Existing%2520evaluation%2520paradigms%2520exhibit%2520two%2520major%2520limitations%253A%25201%2529%2520the%2520dominance%2520of%2520low-discriminative%2520samples%2520in%2520current%2520datasets%2520masks%2520the%2520real%2520robustness%2520gap%2520between%2520models%253B%2520and%25202%2529%2520conventional%2520accuracy-based%2520metric%2520fail%2520to%2520capture%2520the%2520degradation%2520of%2520the%2520underlying%2520prediction%2520structure.%2520To%2520bridge%2520these%2520gaps%252C%2520we%2520introduce%2520Bench-C%252C%2520a%2520comprehensive%2520benchmark%2520emphasizing%2520discriminative%2520samples%2520for%2520assessing%2520corruption%2520robustness%252C%2520where%2520a%2520selection%2520strategy%2520is%2520proposed%2520to%2520jointly%2520consider%2520the%2520prediction%2520inconsistency%2520under%2520corruption%2520and%2520the%2520semantic%2520diversity.%2520Furthermore%252C%2520we%2520propose%2520the%2520Robustness%2520Alignment%2520Score%2520%2528RAS%2529%252C%2520a%2520unified%2520metric%2520that%2520measures%2520degradation%2520in%2520logit-level%2520prediction%2520structure%2520by%2520considering%2520the%2520shifts%2520in%2520prediction%2520uncertainty%2520and%2520calibration%2520alignment.%2520Comprehensive%2520experiments%2520and%2520analysis%2520reveal%2520several%2520interesting%2520findings%253A%25201%2529%2520model%2520behaviors%2520exhibit%2520distinguish%2520patterns%2520under%2520corruptions%252C%2520such%2520as%2520erroneous%2520confidence%2520and%2520hesitation%253B%25202%2529%2520despite%2520subtle%2520corruption%2520may%2520lead%2520to%2520a%2520slight%2520accuracy%2520gain%252C%2520the%2520overall%2520prediction%2520structure%2520still%2520degrades%253B%25203%2529%2520by%2520decomposing%2520corruption%2520robustness%2520into%2520destructive%2520and%2520corrective%2520components%252C%2520the%2520distinct%2520failure%2520and%2520recovery%2520patterns%2520across%2520models%2520can%2520be%2520revealed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Corruption%20Robustness%20of%20LVLMs%3A%20A%20Discriminative%20Benchmark%20and%20Robustness%20Alignment%20Metric&entry.906535625=Xiangjie%20Sui%20and%20Songyang%20Li%20and%20Hanwei%20Zhu%20and%20Baoliang%20Chen%20and%20Yuming%20Fang%20and%20Xin%20Sun&entry.1292438233=Despite%20the%20remarkable%20reasoning%20abilities%20of%20large%20vision-language%20models%20%28LVLMs%29%2C%20their%20robustness%20under%20visual%20corruptions%20remains%20insufficiently%20studied.%20Existing%20evaluation%20paradigms%20exhibit%20two%20major%20limitations%3A%201%29%20the%20dominance%20of%20low-discriminative%20samples%20in%20current%20datasets%20masks%20the%20real%20robustness%20gap%20between%20models%3B%20and%202%29%20conventional%20accuracy-based%20metric%20fail%20to%20capture%20the%20degradation%20of%20the%20underlying%20prediction%20structure.%20To%20bridge%20these%20gaps%2C%20we%20introduce%20Bench-C%2C%20a%20comprehensive%20benchmark%20emphasizing%20discriminative%20samples%20for%20assessing%20corruption%20robustness%2C%20where%20a%20selection%20strategy%20is%20proposed%20to%20jointly%20consider%20the%20prediction%20inconsistency%20under%20corruption%20and%20the%20semantic%20diversity.%20Furthermore%2C%20we%20propose%20the%20Robustness%20Alignment%20Score%20%28RAS%29%2C%20a%20unified%20metric%20that%20measures%20degradation%20in%20logit-level%20prediction%20structure%20by%20considering%20the%20shifts%20in%20prediction%20uncertainty%20and%20calibration%20alignment.%20Comprehensive%20experiments%20and%20analysis%20reveal%20several%20interesting%20findings%3A%201%29%20model%20behaviors%20exhibit%20distinguish%20patterns%20under%20corruptions%2C%20such%20as%20erroneous%20confidence%20and%20hesitation%3B%202%29%20despite%20subtle%20corruption%20may%20lead%20to%20a%20slight%20accuracy%20gain%2C%20the%20overall%20prediction%20structure%20still%20degrades%3B%203%29%20by%20decomposing%20corruption%20robustness%20into%20destructive%20and%20corrective%20components%2C%20the%20distinct%20failure%20and%20recovery%20patterns%20across%20models%20can%20be%20revealed.&entry.1838667208=http%3A//arxiv.org/abs/2511.19032v1&entry.124074799=Read"},
{"title": "SpectraNet: FFT-assisted Deep Learning Classifier for Deepfake Face Detection", "author": "Nithira Jayarathne and Naveen Basnayake and Keshawa Jayasundara and Pasindu Dodampegama and Praveen Wijesinghe and Hirushika Pelagewatta and Kavishka Abeywardana and Sandushan Ranaweera and Chamira Edussooriya", "abstract": "Detecting deepfake images is crucial in combating misinformation. We present a lightweight, generalizable binary classification model based on EfficientNet-B6, fine-tuned with transformation techniques to address severe class imbalances. By leveraging robust preprocessing, oversampling, and optimization strategies, our model achieves high accuracy, stability, and generalization. While incorporating Fourier transform-based phase and amplitude features showed minimal impact, our proposed framework helps non-experts to effectively identify deepfake images, making significant strides toward accessible and reliable deepfake detection.", "link": "http://arxiv.org/abs/2511.19187v1", "date": "2025-11-24", "relevancy": 2.5459, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5205}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5147}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpectraNet%3A%20FFT-assisted%20Deep%20Learning%20Classifier%20for%20Deepfake%20Face%20Detection&body=Title%3A%20SpectraNet%3A%20FFT-assisted%20Deep%20Learning%20Classifier%20for%20Deepfake%20Face%20Detection%0AAuthor%3A%20Nithira%20Jayarathne%20and%20Naveen%20Basnayake%20and%20Keshawa%20Jayasundara%20and%20Pasindu%20Dodampegama%20and%20Praveen%20Wijesinghe%20and%20Hirushika%20Pelagewatta%20and%20Kavishka%20Abeywardana%20and%20Sandushan%20Ranaweera%20and%20Chamira%20Edussooriya%0AAbstract%3A%20Detecting%20deepfake%20images%20is%20crucial%20in%20combating%20misinformation.%20We%20present%20a%20lightweight%2C%20generalizable%20binary%20classification%20model%20based%20on%20EfficientNet-B6%2C%20fine-tuned%20with%20transformation%20techniques%20to%20address%20severe%20class%20imbalances.%20By%20leveraging%20robust%20preprocessing%2C%20oversampling%2C%20and%20optimization%20strategies%2C%20our%20model%20achieves%20high%20accuracy%2C%20stability%2C%20and%20generalization.%20While%20incorporating%20Fourier%20transform-based%20phase%20and%20amplitude%20features%20showed%20minimal%20impact%2C%20our%20proposed%20framework%20helps%20non-experts%20to%20effectively%20identify%20deepfake%20images%2C%20making%20significant%20strides%20toward%20accessible%20and%20reliable%20deepfake%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectraNet%253A%2520FFT-assisted%2520Deep%2520Learning%2520Classifier%2520for%2520Deepfake%2520Face%2520Detection%26entry.906535625%3DNithira%2520Jayarathne%2520and%2520Naveen%2520Basnayake%2520and%2520Keshawa%2520Jayasundara%2520and%2520Pasindu%2520Dodampegama%2520and%2520Praveen%2520Wijesinghe%2520and%2520Hirushika%2520Pelagewatta%2520and%2520Kavishka%2520Abeywardana%2520and%2520Sandushan%2520Ranaweera%2520and%2520Chamira%2520Edussooriya%26entry.1292438233%3DDetecting%2520deepfake%2520images%2520is%2520crucial%2520in%2520combating%2520misinformation.%2520We%2520present%2520a%2520lightweight%252C%2520generalizable%2520binary%2520classification%2520model%2520based%2520on%2520EfficientNet-B6%252C%2520fine-tuned%2520with%2520transformation%2520techniques%2520to%2520address%2520severe%2520class%2520imbalances.%2520By%2520leveraging%2520robust%2520preprocessing%252C%2520oversampling%252C%2520and%2520optimization%2520strategies%252C%2520our%2520model%2520achieves%2520high%2520accuracy%252C%2520stability%252C%2520and%2520generalization.%2520While%2520incorporating%2520Fourier%2520transform-based%2520phase%2520and%2520amplitude%2520features%2520showed%2520minimal%2520impact%252C%2520our%2520proposed%2520framework%2520helps%2520non-experts%2520to%2520effectively%2520identify%2520deepfake%2520images%252C%2520making%2520significant%2520strides%2520toward%2520accessible%2520and%2520reliable%2520deepfake%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpectraNet%3A%20FFT-assisted%20Deep%20Learning%20Classifier%20for%20Deepfake%20Face%20Detection&entry.906535625=Nithira%20Jayarathne%20and%20Naveen%20Basnayake%20and%20Keshawa%20Jayasundara%20and%20Pasindu%20Dodampegama%20and%20Praveen%20Wijesinghe%20and%20Hirushika%20Pelagewatta%20and%20Kavishka%20Abeywardana%20and%20Sandushan%20Ranaweera%20and%20Chamira%20Edussooriya&entry.1292438233=Detecting%20deepfake%20images%20is%20crucial%20in%20combating%20misinformation.%20We%20present%20a%20lightweight%2C%20generalizable%20binary%20classification%20model%20based%20on%20EfficientNet-B6%2C%20fine-tuned%20with%20transformation%20techniques%20to%20address%20severe%20class%20imbalances.%20By%20leveraging%20robust%20preprocessing%2C%20oversampling%2C%20and%20optimization%20strategies%2C%20our%20model%20achieves%20high%20accuracy%2C%20stability%2C%20and%20generalization.%20While%20incorporating%20Fourier%20transform-based%20phase%20and%20amplitude%20features%20showed%20minimal%20impact%2C%20our%20proposed%20framework%20helps%20non-experts%20to%20effectively%20identify%20deepfake%20images%2C%20making%20significant%20strides%20toward%20accessible%20and%20reliable%20deepfake%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2511.19187v1&entry.124074799=Read"},
{"title": "The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification", "author": "Dante Francisco Wasmuht and Otto Brookes and Maximillian Schall and Pablo Palencia and Chris Beirne and Tilo Burghardt and Majid Mirmehdi and Hjalmar K\u00fchl and Mimi Arandjelovic and Sam Pottie and Peter Bermant and Brandon Asheim and Yi Jin Toh and Adam Elzinga and Jason Holmberg and Andrew Whitworth and Eleanor Flatt and Laura Gustafson and Chaitanya Ryali and Yuan-Ting Hu and Baishan Guo and Andrew Westbury and Kate Saenko and Didac Suris", "abstract": "Automated video analysis is critical for wildlife conservation. A foundational task in this domain is multi-animal tracking (MAT), which underpins applications such as individual re-identification and behavior recognition. However, existing datasets are limited in scale, constrained to a few species, or lack sufficient temporal and geographical diversity - leaving no suitable benchmark for training general-purpose MAT models applicable across wild animal populations. To address this, we introduce SA-FARI, the largest open-source MAT dataset for wild animals. It comprises 11,609 camera trap videos collected over approximately 10 years (2014-2024) from 741 locations across 4 continents, spanning 99 species categories. Each video is exhaustively annotated culminating in ~46 hours of densely annotated footage containing 16,224 masklet identities and 942,702 individual bounding boxes, segmentation masks, and species labels. Alongside the task-specific annotations, we publish anonymized camera trap locations for each video. Finally, we present comprehensive benchmarks on SA-FARI using state-of-the-art vision-language models for detection and tracking, including SAM 3, evaluated with both species-specific and generic animal prompts. We also compare against vision-only methods developed specifically for wildlife analysis. SA-FARI is the first large-scale dataset to combine high species diversity, multi-region coverage, and high-quality spatio-temporal annotations, offering a new foundation for advancing generalizable multianimal tracking in the wild. The dataset is available at https://www.conservationxlabs.com/sa-fari.", "link": "http://arxiv.org/abs/2511.15622v2", "date": "2025-11-24", "relevancy": 2.5083, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5043}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5003}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20SA-FARI%20Dataset%3A%20Segment%20Anything%20in%20Footage%20of%20Animals%20for%20Recognition%20and%20Identification&body=Title%3A%20The%20SA-FARI%20Dataset%3A%20Segment%20Anything%20in%20Footage%20of%20Animals%20for%20Recognition%20and%20Identification%0AAuthor%3A%20Dante%20Francisco%20Wasmuht%20and%20Otto%20Brookes%20and%20Maximillian%20Schall%20and%20Pablo%20Palencia%20and%20Chris%20Beirne%20and%20Tilo%20Burghardt%20and%20Majid%20Mirmehdi%20and%20Hjalmar%20K%C3%BChl%20and%20Mimi%20Arandjelovic%20and%20Sam%20Pottie%20and%20Peter%20Bermant%20and%20Brandon%20Asheim%20and%20Yi%20Jin%20Toh%20and%20Adam%20Elzinga%20and%20Jason%20Holmberg%20and%20Andrew%20Whitworth%20and%20Eleanor%20Flatt%20and%20Laura%20Gustafson%20and%20Chaitanya%20Ryali%20and%20Yuan-Ting%20Hu%20and%20Baishan%20Guo%20and%20Andrew%20Westbury%20and%20Kate%20Saenko%20and%20Didac%20Suris%0AAbstract%3A%20Automated%20video%20analysis%20is%20critical%20for%20wildlife%20conservation.%20A%20foundational%20task%20in%20this%20domain%20is%20multi-animal%20tracking%20%28MAT%29%2C%20which%20underpins%20applications%20such%20as%20individual%20re-identification%20and%20behavior%20recognition.%20However%2C%20existing%20datasets%20are%20limited%20in%20scale%2C%20constrained%20to%20a%20few%20species%2C%20or%20lack%20sufficient%20temporal%20and%20geographical%20diversity%20-%20leaving%20no%20suitable%20benchmark%20for%20training%20general-purpose%20MAT%20models%20applicable%20across%20wild%20animal%20populations.%20To%20address%20this%2C%20we%20introduce%20SA-FARI%2C%20the%20largest%20open-source%20MAT%20dataset%20for%20wild%20animals.%20It%20comprises%2011%2C609%20camera%20trap%20videos%20collected%20over%20approximately%2010%20years%20%282014-2024%29%20from%20741%20locations%20across%204%20continents%2C%20spanning%2099%20species%20categories.%20Each%20video%20is%20exhaustively%20annotated%20culminating%20in%20~46%20hours%20of%20densely%20annotated%20footage%20containing%2016%2C224%20masklet%20identities%20and%20942%2C702%20individual%20bounding%20boxes%2C%20segmentation%20masks%2C%20and%20species%20labels.%20Alongside%20the%20task-specific%20annotations%2C%20we%20publish%20anonymized%20camera%20trap%20locations%20for%20each%20video.%20Finally%2C%20we%20present%20comprehensive%20benchmarks%20on%20SA-FARI%20using%20state-of-the-art%20vision-language%20models%20for%20detection%20and%20tracking%2C%20including%20SAM%203%2C%20evaluated%20with%20both%20species-specific%20and%20generic%20animal%20prompts.%20We%20also%20compare%20against%20vision-only%20methods%20developed%20specifically%20for%20wildlife%20analysis.%20SA-FARI%20is%20the%20first%20large-scale%20dataset%20to%20combine%20high%20species%20diversity%2C%20multi-region%20coverage%2C%20and%20high-quality%20spatio-temporal%20annotations%2C%20offering%20a%20new%20foundation%20for%20advancing%20generalizable%20multianimal%20tracking%20in%20the%20wild.%20The%20dataset%20is%20available%20at%20https%3A//www.conservationxlabs.com/sa-fari.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15622v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520SA-FARI%2520Dataset%253A%2520Segment%2520Anything%2520in%2520Footage%2520of%2520Animals%2520for%2520Recognition%2520and%2520Identification%26entry.906535625%3DDante%2520Francisco%2520Wasmuht%2520and%2520Otto%2520Brookes%2520and%2520Maximillian%2520Schall%2520and%2520Pablo%2520Palencia%2520and%2520Chris%2520Beirne%2520and%2520Tilo%2520Burghardt%2520and%2520Majid%2520Mirmehdi%2520and%2520Hjalmar%2520K%25C3%25BChl%2520and%2520Mimi%2520Arandjelovic%2520and%2520Sam%2520Pottie%2520and%2520Peter%2520Bermant%2520and%2520Brandon%2520Asheim%2520and%2520Yi%2520Jin%2520Toh%2520and%2520Adam%2520Elzinga%2520and%2520Jason%2520Holmberg%2520and%2520Andrew%2520Whitworth%2520and%2520Eleanor%2520Flatt%2520and%2520Laura%2520Gustafson%2520and%2520Chaitanya%2520Ryali%2520and%2520Yuan-Ting%2520Hu%2520and%2520Baishan%2520Guo%2520and%2520Andrew%2520Westbury%2520and%2520Kate%2520Saenko%2520and%2520Didac%2520Suris%26entry.1292438233%3DAutomated%2520video%2520analysis%2520is%2520critical%2520for%2520wildlife%2520conservation.%2520A%2520foundational%2520task%2520in%2520this%2520domain%2520is%2520multi-animal%2520tracking%2520%2528MAT%2529%252C%2520which%2520underpins%2520applications%2520such%2520as%2520individual%2520re-identification%2520and%2520behavior%2520recognition.%2520However%252C%2520existing%2520datasets%2520are%2520limited%2520in%2520scale%252C%2520constrained%2520to%2520a%2520few%2520species%252C%2520or%2520lack%2520sufficient%2520temporal%2520and%2520geographical%2520diversity%2520-%2520leaving%2520no%2520suitable%2520benchmark%2520for%2520training%2520general-purpose%2520MAT%2520models%2520applicable%2520across%2520wild%2520animal%2520populations.%2520To%2520address%2520this%252C%2520we%2520introduce%2520SA-FARI%252C%2520the%2520largest%2520open-source%2520MAT%2520dataset%2520for%2520wild%2520animals.%2520It%2520comprises%252011%252C609%2520camera%2520trap%2520videos%2520collected%2520over%2520approximately%252010%2520years%2520%25282014-2024%2529%2520from%2520741%2520locations%2520across%25204%2520continents%252C%2520spanning%252099%2520species%2520categories.%2520Each%2520video%2520is%2520exhaustively%2520annotated%2520culminating%2520in%2520~46%2520hours%2520of%2520densely%2520annotated%2520footage%2520containing%252016%252C224%2520masklet%2520identities%2520and%2520942%252C702%2520individual%2520bounding%2520boxes%252C%2520segmentation%2520masks%252C%2520and%2520species%2520labels.%2520Alongside%2520the%2520task-specific%2520annotations%252C%2520we%2520publish%2520anonymized%2520camera%2520trap%2520locations%2520for%2520each%2520video.%2520Finally%252C%2520we%2520present%2520comprehensive%2520benchmarks%2520on%2520SA-FARI%2520using%2520state-of-the-art%2520vision-language%2520models%2520for%2520detection%2520and%2520tracking%252C%2520including%2520SAM%25203%252C%2520evaluated%2520with%2520both%2520species-specific%2520and%2520generic%2520animal%2520prompts.%2520We%2520also%2520compare%2520against%2520vision-only%2520methods%2520developed%2520specifically%2520for%2520wildlife%2520analysis.%2520SA-FARI%2520is%2520the%2520first%2520large-scale%2520dataset%2520to%2520combine%2520high%2520species%2520diversity%252C%2520multi-region%2520coverage%252C%2520and%2520high-quality%2520spatio-temporal%2520annotations%252C%2520offering%2520a%2520new%2520foundation%2520for%2520advancing%2520generalizable%2520multianimal%2520tracking%2520in%2520the%2520wild.%2520The%2520dataset%2520is%2520available%2520at%2520https%253A//www.conservationxlabs.com/sa-fari.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15622v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20SA-FARI%20Dataset%3A%20Segment%20Anything%20in%20Footage%20of%20Animals%20for%20Recognition%20and%20Identification&entry.906535625=Dante%20Francisco%20Wasmuht%20and%20Otto%20Brookes%20and%20Maximillian%20Schall%20and%20Pablo%20Palencia%20and%20Chris%20Beirne%20and%20Tilo%20Burghardt%20and%20Majid%20Mirmehdi%20and%20Hjalmar%20K%C3%BChl%20and%20Mimi%20Arandjelovic%20and%20Sam%20Pottie%20and%20Peter%20Bermant%20and%20Brandon%20Asheim%20and%20Yi%20Jin%20Toh%20and%20Adam%20Elzinga%20and%20Jason%20Holmberg%20and%20Andrew%20Whitworth%20and%20Eleanor%20Flatt%20and%20Laura%20Gustafson%20and%20Chaitanya%20Ryali%20and%20Yuan-Ting%20Hu%20and%20Baishan%20Guo%20and%20Andrew%20Westbury%20and%20Kate%20Saenko%20and%20Didac%20Suris&entry.1292438233=Automated%20video%20analysis%20is%20critical%20for%20wildlife%20conservation.%20A%20foundational%20task%20in%20this%20domain%20is%20multi-animal%20tracking%20%28MAT%29%2C%20which%20underpins%20applications%20such%20as%20individual%20re-identification%20and%20behavior%20recognition.%20However%2C%20existing%20datasets%20are%20limited%20in%20scale%2C%20constrained%20to%20a%20few%20species%2C%20or%20lack%20sufficient%20temporal%20and%20geographical%20diversity%20-%20leaving%20no%20suitable%20benchmark%20for%20training%20general-purpose%20MAT%20models%20applicable%20across%20wild%20animal%20populations.%20To%20address%20this%2C%20we%20introduce%20SA-FARI%2C%20the%20largest%20open-source%20MAT%20dataset%20for%20wild%20animals.%20It%20comprises%2011%2C609%20camera%20trap%20videos%20collected%20over%20approximately%2010%20years%20%282014-2024%29%20from%20741%20locations%20across%204%20continents%2C%20spanning%2099%20species%20categories.%20Each%20video%20is%20exhaustively%20annotated%20culminating%20in%20~46%20hours%20of%20densely%20annotated%20footage%20containing%2016%2C224%20masklet%20identities%20and%20942%2C702%20individual%20bounding%20boxes%2C%20segmentation%20masks%2C%20and%20species%20labels.%20Alongside%20the%20task-specific%20annotations%2C%20we%20publish%20anonymized%20camera%20trap%20locations%20for%20each%20video.%20Finally%2C%20we%20present%20comprehensive%20benchmarks%20on%20SA-FARI%20using%20state-of-the-art%20vision-language%20models%20for%20detection%20and%20tracking%2C%20including%20SAM%203%2C%20evaluated%20with%20both%20species-specific%20and%20generic%20animal%20prompts.%20We%20also%20compare%20against%20vision-only%20methods%20developed%20specifically%20for%20wildlife%20analysis.%20SA-FARI%20is%20the%20first%20large-scale%20dataset%20to%20combine%20high%20species%20diversity%2C%20multi-region%20coverage%2C%20and%20high-quality%20spatio-temporal%20annotations%2C%20offering%20a%20new%20foundation%20for%20advancing%20generalizable%20multianimal%20tracking%20in%20the%20wild.%20The%20dataset%20is%20available%20at%20https%3A//www.conservationxlabs.com/sa-fari.&entry.1838667208=http%3A//arxiv.org/abs/2511.15622v2&entry.124074799=Read"},
{"title": "IDEAL-M3D: Instance Diversity-Enriched Active Learning for Monocular 3D Detection", "author": "Johannes Meier and Florian G\u00fcnther and Riccardo Marin and Oussema Dhaouadi and Jacques Kaiser and Daniel Cremers", "abstract": "Monocular 3D detection relies on just a single camera and is therefore easy to deploy. Yet, achieving reliable 3D understanding from monocular images requires substantial annotation, and 3D labels are especially costly. To maximize performance under constrained labeling budgets, it is essential to prioritize annotating samples expected to deliver the largest performance gains. This prioritization is the focus of active learning. Curiously, we observed two significant limitations in active learning algorithms for 3D monocular object detection. First, previous approaches select entire images, which is inefficient, as non-informative instances contained in the same image also need to be labeled. Secondly, existing methods rely on uncertainty-based selection, which in monocular 3D object detection creates a bias toward depth ambiguity. Consequently, distant objects are selected, while nearby objects are overlooked.\n  To address these limitations, we propose IDEAL-M3D, the first instance-level pipeline for monocular 3D detection. For the first time, we demonstrate that an explicitly diverse, fast-to-train ensemble improves diversity-driven active learning for monocular 3D. We induce diversity with heterogeneous backbones and task-agnostic features, loss weight perturbation, and time-dependent bagging. IDEAL-M3D shows superior performance and significant resource savings: with just 60% of the annotations, we achieve similar or better AP3D on KITTI validation and test set results compared to training the same detector on the whole dataset.", "link": "http://arxiv.org/abs/2511.19301v1", "date": "2025-11-24", "relevancy": 2.4987, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6699}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6168}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IDEAL-M3D%3A%20Instance%20Diversity-Enriched%20Active%20Learning%20for%20Monocular%203D%20Detection&body=Title%3A%20IDEAL-M3D%3A%20Instance%20Diversity-Enriched%20Active%20Learning%20for%20Monocular%203D%20Detection%0AAuthor%3A%20Johannes%20Meier%20and%20Florian%20G%C3%BCnther%20and%20Riccardo%20Marin%20and%20Oussema%20Dhaouadi%20and%20Jacques%20Kaiser%20and%20Daniel%20Cremers%0AAbstract%3A%20Monocular%203D%20detection%20relies%20on%20just%20a%20single%20camera%20and%20is%20therefore%20easy%20to%20deploy.%20Yet%2C%20achieving%20reliable%203D%20understanding%20from%20monocular%20images%20requires%20substantial%20annotation%2C%20and%203D%20labels%20are%20especially%20costly.%20To%20maximize%20performance%20under%20constrained%20labeling%20budgets%2C%20it%20is%20essential%20to%20prioritize%20annotating%20samples%20expected%20to%20deliver%20the%20largest%20performance%20gains.%20This%20prioritization%20is%20the%20focus%20of%20active%20learning.%20Curiously%2C%20we%20observed%20two%20significant%20limitations%20in%20active%20learning%20algorithms%20for%203D%20monocular%20object%20detection.%20First%2C%20previous%20approaches%20select%20entire%20images%2C%20which%20is%20inefficient%2C%20as%20non-informative%20instances%20contained%20in%20the%20same%20image%20also%20need%20to%20be%20labeled.%20Secondly%2C%20existing%20methods%20rely%20on%20uncertainty-based%20selection%2C%20which%20in%20monocular%203D%20object%20detection%20creates%20a%20bias%20toward%20depth%20ambiguity.%20Consequently%2C%20distant%20objects%20are%20selected%2C%20while%20nearby%20objects%20are%20overlooked.%0A%20%20To%20address%20these%20limitations%2C%20we%20propose%20IDEAL-M3D%2C%20the%20first%20instance-level%20pipeline%20for%20monocular%203D%20detection.%20For%20the%20first%20time%2C%20we%20demonstrate%20that%20an%20explicitly%20diverse%2C%20fast-to-train%20ensemble%20improves%20diversity-driven%20active%20learning%20for%20monocular%203D.%20We%20induce%20diversity%20with%20heterogeneous%20backbones%20and%20task-agnostic%20features%2C%20loss%20weight%20perturbation%2C%20and%20time-dependent%20bagging.%20IDEAL-M3D%20shows%20superior%20performance%20and%20significant%20resource%20savings%3A%20with%20just%2060%25%20of%20the%20annotations%2C%20we%20achieve%20similar%20or%20better%20AP3D%20on%20KITTI%20validation%20and%20test%20set%20results%20compared%20to%20training%20the%20same%20detector%20on%20the%20whole%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19301v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIDEAL-M3D%253A%2520Instance%2520Diversity-Enriched%2520Active%2520Learning%2520for%2520Monocular%25203D%2520Detection%26entry.906535625%3DJohannes%2520Meier%2520and%2520Florian%2520G%25C3%25BCnther%2520and%2520Riccardo%2520Marin%2520and%2520Oussema%2520Dhaouadi%2520and%2520Jacques%2520Kaiser%2520and%2520Daniel%2520Cremers%26entry.1292438233%3DMonocular%25203D%2520detection%2520relies%2520on%2520just%2520a%2520single%2520camera%2520and%2520is%2520therefore%2520easy%2520to%2520deploy.%2520Yet%252C%2520achieving%2520reliable%25203D%2520understanding%2520from%2520monocular%2520images%2520requires%2520substantial%2520annotation%252C%2520and%25203D%2520labels%2520are%2520especially%2520costly.%2520To%2520maximize%2520performance%2520under%2520constrained%2520labeling%2520budgets%252C%2520it%2520is%2520essential%2520to%2520prioritize%2520annotating%2520samples%2520expected%2520to%2520deliver%2520the%2520largest%2520performance%2520gains.%2520This%2520prioritization%2520is%2520the%2520focus%2520of%2520active%2520learning.%2520Curiously%252C%2520we%2520observed%2520two%2520significant%2520limitations%2520in%2520active%2520learning%2520algorithms%2520for%25203D%2520monocular%2520object%2520detection.%2520First%252C%2520previous%2520approaches%2520select%2520entire%2520images%252C%2520which%2520is%2520inefficient%252C%2520as%2520non-informative%2520instances%2520contained%2520in%2520the%2520same%2520image%2520also%2520need%2520to%2520be%2520labeled.%2520Secondly%252C%2520existing%2520methods%2520rely%2520on%2520uncertainty-based%2520selection%252C%2520which%2520in%2520monocular%25203D%2520object%2520detection%2520creates%2520a%2520bias%2520toward%2520depth%2520ambiguity.%2520Consequently%252C%2520distant%2520objects%2520are%2520selected%252C%2520while%2520nearby%2520objects%2520are%2520overlooked.%250A%2520%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520IDEAL-M3D%252C%2520the%2520first%2520instance-level%2520pipeline%2520for%2520monocular%25203D%2520detection.%2520For%2520the%2520first%2520time%252C%2520we%2520demonstrate%2520that%2520an%2520explicitly%2520diverse%252C%2520fast-to-train%2520ensemble%2520improves%2520diversity-driven%2520active%2520learning%2520for%2520monocular%25203D.%2520We%2520induce%2520diversity%2520with%2520heterogeneous%2520backbones%2520and%2520task-agnostic%2520features%252C%2520loss%2520weight%2520perturbation%252C%2520and%2520time-dependent%2520bagging.%2520IDEAL-M3D%2520shows%2520superior%2520performance%2520and%2520significant%2520resource%2520savings%253A%2520with%2520just%252060%2525%2520of%2520the%2520annotations%252C%2520we%2520achieve%2520similar%2520or%2520better%2520AP3D%2520on%2520KITTI%2520validation%2520and%2520test%2520set%2520results%2520compared%2520to%2520training%2520the%2520same%2520detector%2520on%2520the%2520whole%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19301v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IDEAL-M3D%3A%20Instance%20Diversity-Enriched%20Active%20Learning%20for%20Monocular%203D%20Detection&entry.906535625=Johannes%20Meier%20and%20Florian%20G%C3%BCnther%20and%20Riccardo%20Marin%20and%20Oussema%20Dhaouadi%20and%20Jacques%20Kaiser%20and%20Daniel%20Cremers&entry.1292438233=Monocular%203D%20detection%20relies%20on%20just%20a%20single%20camera%20and%20is%20therefore%20easy%20to%20deploy.%20Yet%2C%20achieving%20reliable%203D%20understanding%20from%20monocular%20images%20requires%20substantial%20annotation%2C%20and%203D%20labels%20are%20especially%20costly.%20To%20maximize%20performance%20under%20constrained%20labeling%20budgets%2C%20it%20is%20essential%20to%20prioritize%20annotating%20samples%20expected%20to%20deliver%20the%20largest%20performance%20gains.%20This%20prioritization%20is%20the%20focus%20of%20active%20learning.%20Curiously%2C%20we%20observed%20two%20significant%20limitations%20in%20active%20learning%20algorithms%20for%203D%20monocular%20object%20detection.%20First%2C%20previous%20approaches%20select%20entire%20images%2C%20which%20is%20inefficient%2C%20as%20non-informative%20instances%20contained%20in%20the%20same%20image%20also%20need%20to%20be%20labeled.%20Secondly%2C%20existing%20methods%20rely%20on%20uncertainty-based%20selection%2C%20which%20in%20monocular%203D%20object%20detection%20creates%20a%20bias%20toward%20depth%20ambiguity.%20Consequently%2C%20distant%20objects%20are%20selected%2C%20while%20nearby%20objects%20are%20overlooked.%0A%20%20To%20address%20these%20limitations%2C%20we%20propose%20IDEAL-M3D%2C%20the%20first%20instance-level%20pipeline%20for%20monocular%203D%20detection.%20For%20the%20first%20time%2C%20we%20demonstrate%20that%20an%20explicitly%20diverse%2C%20fast-to-train%20ensemble%20improves%20diversity-driven%20active%20learning%20for%20monocular%203D.%20We%20induce%20diversity%20with%20heterogeneous%20backbones%20and%20task-agnostic%20features%2C%20loss%20weight%20perturbation%2C%20and%20time-dependent%20bagging.%20IDEAL-M3D%20shows%20superior%20performance%20and%20significant%20resource%20savings%3A%20with%20just%2060%25%20of%20the%20annotations%2C%20we%20achieve%20similar%20or%20better%20AP3D%20on%20KITTI%20validation%20and%20test%20set%20results%20compared%20to%20training%20the%20same%20detector%20on%20the%20whole%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2511.19301v1&entry.124074799=Read"},
{"title": "ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation", "author": "Dongha Lee and Jinhee Park and Minjun Kim and Junseok Kwon", "abstract": "We propose Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a principled initialization strategy that substantially accelerates the convergence of low-rank adapters. While LoRA offers high parameter efficiency, its random initialization restricts gradient updates to a mismatched tangent space, causing significant information loss and hindering early convergence. Our ABM-LoRA addresses this by aligning the adapter's activation boundaries with those of the pretrained model before downstream training, thereby maximizing the projection of full-parameter gradients into the adapter subspace. This alignment sharply reduces information loss at initialization, yields a lower starting loss, and accelerates convergence. We demonstrate ABM-LoRA's effectiveness across diverse architectures and tasks: language understanding (T5-Base on GLUE), dialogue generation (LLaMA2-7B on WizardLM), and vision recognition (ViT-B/16 on VTAB-1K). On VTAB-1K, it achieves the highest accuracy among all methods, with strong gains on structured reasoning tasks requiring geometric understanding.", "link": "http://arxiv.org/abs/2511.19145v1", "date": "2025-11-24", "relevancy": 2.4935, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5276}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.499}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ABM-LoRA%3A%20Activation%20Boundary%20Matching%20for%20Fast%20Convergence%20in%20Low-Rank%20Adaptation&body=Title%3A%20ABM-LoRA%3A%20Activation%20Boundary%20Matching%20for%20Fast%20Convergence%20in%20Low-Rank%20Adaptation%0AAuthor%3A%20Dongha%20Lee%20and%20Jinhee%20Park%20and%20Minjun%20Kim%20and%20Junseok%20Kwon%0AAbstract%3A%20We%20propose%20Activation%20Boundary%20Matching%20for%20Low-Rank%20Adaptation%20%28ABM-LoRA%29%2C%20a%20principled%20initialization%20strategy%20that%20substantially%20accelerates%20the%20convergence%20of%20low-rank%20adapters.%20While%20LoRA%20offers%20high%20parameter%20efficiency%2C%20its%20random%20initialization%20restricts%20gradient%20updates%20to%20a%20mismatched%20tangent%20space%2C%20causing%20significant%20information%20loss%20and%20hindering%20early%20convergence.%20Our%20ABM-LoRA%20addresses%20this%20by%20aligning%20the%20adapter%27s%20activation%20boundaries%20with%20those%20of%20the%20pretrained%20model%20before%20downstream%20training%2C%20thereby%20maximizing%20the%20projection%20of%20full-parameter%20gradients%20into%20the%20adapter%20subspace.%20This%20alignment%20sharply%20reduces%20information%20loss%20at%20initialization%2C%20yields%20a%20lower%20starting%20loss%2C%20and%20accelerates%20convergence.%20We%20demonstrate%20ABM-LoRA%27s%20effectiveness%20across%20diverse%20architectures%20and%20tasks%3A%20language%20understanding%20%28T5-Base%20on%20GLUE%29%2C%20dialogue%20generation%20%28LLaMA2-7B%20on%20WizardLM%29%2C%20and%20vision%20recognition%20%28ViT-B/16%20on%20VTAB-1K%29.%20On%20VTAB-1K%2C%20it%20achieves%20the%20highest%20accuracy%20among%20all%20methods%2C%20with%20strong%20gains%20on%20structured%20reasoning%20tasks%20requiring%20geometric%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DABM-LoRA%253A%2520Activation%2520Boundary%2520Matching%2520for%2520Fast%2520Convergence%2520in%2520Low-Rank%2520Adaptation%26entry.906535625%3DDongha%2520Lee%2520and%2520Jinhee%2520Park%2520and%2520Minjun%2520Kim%2520and%2520Junseok%2520Kwon%26entry.1292438233%3DWe%2520propose%2520Activation%2520Boundary%2520Matching%2520for%2520Low-Rank%2520Adaptation%2520%2528ABM-LoRA%2529%252C%2520a%2520principled%2520initialization%2520strategy%2520that%2520substantially%2520accelerates%2520the%2520convergence%2520of%2520low-rank%2520adapters.%2520While%2520LoRA%2520offers%2520high%2520parameter%2520efficiency%252C%2520its%2520random%2520initialization%2520restricts%2520gradient%2520updates%2520to%2520a%2520mismatched%2520tangent%2520space%252C%2520causing%2520significant%2520information%2520loss%2520and%2520hindering%2520early%2520convergence.%2520Our%2520ABM-LoRA%2520addresses%2520this%2520by%2520aligning%2520the%2520adapter%2527s%2520activation%2520boundaries%2520with%2520those%2520of%2520the%2520pretrained%2520model%2520before%2520downstream%2520training%252C%2520thereby%2520maximizing%2520the%2520projection%2520of%2520full-parameter%2520gradients%2520into%2520the%2520adapter%2520subspace.%2520This%2520alignment%2520sharply%2520reduces%2520information%2520loss%2520at%2520initialization%252C%2520yields%2520a%2520lower%2520starting%2520loss%252C%2520and%2520accelerates%2520convergence.%2520We%2520demonstrate%2520ABM-LoRA%2527s%2520effectiveness%2520across%2520diverse%2520architectures%2520and%2520tasks%253A%2520language%2520understanding%2520%2528T5-Base%2520on%2520GLUE%2529%252C%2520dialogue%2520generation%2520%2528LLaMA2-7B%2520on%2520WizardLM%2529%252C%2520and%2520vision%2520recognition%2520%2528ViT-B/16%2520on%2520VTAB-1K%2529.%2520On%2520VTAB-1K%252C%2520it%2520achieves%2520the%2520highest%2520accuracy%2520among%2520all%2520methods%252C%2520with%2520strong%2520gains%2520on%2520structured%2520reasoning%2520tasks%2520requiring%2520geometric%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ABM-LoRA%3A%20Activation%20Boundary%20Matching%20for%20Fast%20Convergence%20in%20Low-Rank%20Adaptation&entry.906535625=Dongha%20Lee%20and%20Jinhee%20Park%20and%20Minjun%20Kim%20and%20Junseok%20Kwon&entry.1292438233=We%20propose%20Activation%20Boundary%20Matching%20for%20Low-Rank%20Adaptation%20%28ABM-LoRA%29%2C%20a%20principled%20initialization%20strategy%20that%20substantially%20accelerates%20the%20convergence%20of%20low-rank%20adapters.%20While%20LoRA%20offers%20high%20parameter%20efficiency%2C%20its%20random%20initialization%20restricts%20gradient%20updates%20to%20a%20mismatched%20tangent%20space%2C%20causing%20significant%20information%20loss%20and%20hindering%20early%20convergence.%20Our%20ABM-LoRA%20addresses%20this%20by%20aligning%20the%20adapter%27s%20activation%20boundaries%20with%20those%20of%20the%20pretrained%20model%20before%20downstream%20training%2C%20thereby%20maximizing%20the%20projection%20of%20full-parameter%20gradients%20into%20the%20adapter%20subspace.%20This%20alignment%20sharply%20reduces%20information%20loss%20at%20initialization%2C%20yields%20a%20lower%20starting%20loss%2C%20and%20accelerates%20convergence.%20We%20demonstrate%20ABM-LoRA%27s%20effectiveness%20across%20diverse%20architectures%20and%20tasks%3A%20language%20understanding%20%28T5-Base%20on%20GLUE%29%2C%20dialogue%20generation%20%28LLaMA2-7B%20on%20WizardLM%29%2C%20and%20vision%20recognition%20%28ViT-B/16%20on%20VTAB-1K%29.%20On%20VTAB-1K%2C%20it%20achieves%20the%20highest%20accuracy%20among%20all%20methods%2C%20with%20strong%20gains%20on%20structured%20reasoning%20tasks%20requiring%20geometric%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2511.19145v1&entry.124074799=Read"},
{"title": "Eevee: Towards Close-up High-resolution Video-based Virtual Try-on", "author": "Jianhao Zeng and Yancheng Bai and Ruidong Chen and Xuanpu Zhang and Lei Sun and Dongyang Jin and Ryan Xu and Nannan Zhang and Dan Song and Xiangxiang Chu", "abstract": "Video virtual try-on technology provides a cost-effective solution for creating marketing videos in fashion e-commerce. However, its practical adoption is hindered by two critical limitations. First, the reliance on a single garment image as input in current virtual try-on datasets limits the accurate capture of realistic texture details. Second, most existing methods focus solely on generating full-shot virtual try-on videos, neglecting the business's demand for videos that also provide detailed close-ups. To address these challenges, we introduce a high-resolution dataset for video-based virtual try-on. This dataset offers two key features. First, it provides more detailed information on the garments, which includes high-fidelity images with detailed close-ups and textual descriptions; Second, it uniquely includes full-shot and close-up try-on videos of real human models. Furthermore, accurately assessing consistency becomes significantly more critical for the close-up videos, which demand high-fidelity preservation of garment details. To facilitate such fine-grained evaluation, we propose a new garment consistency metric VGID (Video Garment Inception Distance) that quantifies the preservation of both texture and structure. Our experiments validate these contributions. We demonstrate that by utilizing the detailed images from our dataset, existing video generation models can extract and incorporate texture features, significantly enhancing the realism and detail fidelity of virtual try-on results. Furthermore, we conduct a comprehensive benchmark of recent models. The benchmark effectively identifies the texture and structural preservation problems among current methods.", "link": "http://arxiv.org/abs/2511.18957v1", "date": "2025-11-24", "relevancy": 2.4732, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6396}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6036}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eevee%3A%20Towards%20Close-up%20High-resolution%20Video-based%20Virtual%20Try-on&body=Title%3A%20Eevee%3A%20Towards%20Close-up%20High-resolution%20Video-based%20Virtual%20Try-on%0AAuthor%3A%20Jianhao%20Zeng%20and%20Yancheng%20Bai%20and%20Ruidong%20Chen%20and%20Xuanpu%20Zhang%20and%20Lei%20Sun%20and%20Dongyang%20Jin%20and%20Ryan%20Xu%20and%20Nannan%20Zhang%20and%20Dan%20Song%20and%20Xiangxiang%20Chu%0AAbstract%3A%20Video%20virtual%20try-on%20technology%20provides%20a%20cost-effective%20solution%20for%20creating%20marketing%20videos%20in%20fashion%20e-commerce.%20However%2C%20its%20practical%20adoption%20is%20hindered%20by%20two%20critical%20limitations.%20First%2C%20the%20reliance%20on%20a%20single%20garment%20image%20as%20input%20in%20current%20virtual%20try-on%20datasets%20limits%20the%20accurate%20capture%20of%20realistic%20texture%20details.%20Second%2C%20most%20existing%20methods%20focus%20solely%20on%20generating%20full-shot%20virtual%20try-on%20videos%2C%20neglecting%20the%20business%27s%20demand%20for%20videos%20that%20also%20provide%20detailed%20close-ups.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20high-resolution%20dataset%20for%20video-based%20virtual%20try-on.%20This%20dataset%20offers%20two%20key%20features.%20First%2C%20it%20provides%20more%20detailed%20information%20on%20the%20garments%2C%20which%20includes%20high-fidelity%20images%20with%20detailed%20close-ups%20and%20textual%20descriptions%3B%20Second%2C%20it%20uniquely%20includes%20full-shot%20and%20close-up%20try-on%20videos%20of%20real%20human%20models.%20Furthermore%2C%20accurately%20assessing%20consistency%20becomes%20significantly%20more%20critical%20for%20the%20close-up%20videos%2C%20which%20demand%20high-fidelity%20preservation%20of%20garment%20details.%20To%20facilitate%20such%20fine-grained%20evaluation%2C%20we%20propose%20a%20new%20garment%20consistency%20metric%20VGID%20%28Video%20Garment%20Inception%20Distance%29%20that%20quantifies%20the%20preservation%20of%20both%20texture%20and%20structure.%20Our%20experiments%20validate%20these%20contributions.%20We%20demonstrate%20that%20by%20utilizing%20the%20detailed%20images%20from%20our%20dataset%2C%20existing%20video%20generation%20models%20can%20extract%20and%20incorporate%20texture%20features%2C%20significantly%20enhancing%20the%20realism%20and%20detail%20fidelity%20of%20virtual%20try-on%20results.%20Furthermore%2C%20we%20conduct%20a%20comprehensive%20benchmark%20of%20recent%20models.%20The%20benchmark%20effectively%20identifies%20the%20texture%20and%20structural%20preservation%20problems%20among%20current%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEevee%253A%2520Towards%2520Close-up%2520High-resolution%2520Video-based%2520Virtual%2520Try-on%26entry.906535625%3DJianhao%2520Zeng%2520and%2520Yancheng%2520Bai%2520and%2520Ruidong%2520Chen%2520and%2520Xuanpu%2520Zhang%2520and%2520Lei%2520Sun%2520and%2520Dongyang%2520Jin%2520and%2520Ryan%2520Xu%2520and%2520Nannan%2520Zhang%2520and%2520Dan%2520Song%2520and%2520Xiangxiang%2520Chu%26entry.1292438233%3DVideo%2520virtual%2520try-on%2520technology%2520provides%2520a%2520cost-effective%2520solution%2520for%2520creating%2520marketing%2520videos%2520in%2520fashion%2520e-commerce.%2520However%252C%2520its%2520practical%2520adoption%2520is%2520hindered%2520by%2520two%2520critical%2520limitations.%2520First%252C%2520the%2520reliance%2520on%2520a%2520single%2520garment%2520image%2520as%2520input%2520in%2520current%2520virtual%2520try-on%2520datasets%2520limits%2520the%2520accurate%2520capture%2520of%2520realistic%2520texture%2520details.%2520Second%252C%2520most%2520existing%2520methods%2520focus%2520solely%2520on%2520generating%2520full-shot%2520virtual%2520try-on%2520videos%252C%2520neglecting%2520the%2520business%2527s%2520demand%2520for%2520videos%2520that%2520also%2520provide%2520detailed%2520close-ups.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520high-resolution%2520dataset%2520for%2520video-based%2520virtual%2520try-on.%2520This%2520dataset%2520offers%2520two%2520key%2520features.%2520First%252C%2520it%2520provides%2520more%2520detailed%2520information%2520on%2520the%2520garments%252C%2520which%2520includes%2520high-fidelity%2520images%2520with%2520detailed%2520close-ups%2520and%2520textual%2520descriptions%253B%2520Second%252C%2520it%2520uniquely%2520includes%2520full-shot%2520and%2520close-up%2520try-on%2520videos%2520of%2520real%2520human%2520models.%2520Furthermore%252C%2520accurately%2520assessing%2520consistency%2520becomes%2520significantly%2520more%2520critical%2520for%2520the%2520close-up%2520videos%252C%2520which%2520demand%2520high-fidelity%2520preservation%2520of%2520garment%2520details.%2520To%2520facilitate%2520such%2520fine-grained%2520evaluation%252C%2520we%2520propose%2520a%2520new%2520garment%2520consistency%2520metric%2520VGID%2520%2528Video%2520Garment%2520Inception%2520Distance%2529%2520that%2520quantifies%2520the%2520preservation%2520of%2520both%2520texture%2520and%2520structure.%2520Our%2520experiments%2520validate%2520these%2520contributions.%2520We%2520demonstrate%2520that%2520by%2520utilizing%2520the%2520detailed%2520images%2520from%2520our%2520dataset%252C%2520existing%2520video%2520generation%2520models%2520can%2520extract%2520and%2520incorporate%2520texture%2520features%252C%2520significantly%2520enhancing%2520the%2520realism%2520and%2520detail%2520fidelity%2520of%2520virtual%2520try-on%2520results.%2520Furthermore%252C%2520we%2520conduct%2520a%2520comprehensive%2520benchmark%2520of%2520recent%2520models.%2520The%2520benchmark%2520effectively%2520identifies%2520the%2520texture%2520and%2520structural%2520preservation%2520problems%2520among%2520current%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eevee%3A%20Towards%20Close-up%20High-resolution%20Video-based%20Virtual%20Try-on&entry.906535625=Jianhao%20Zeng%20and%20Yancheng%20Bai%20and%20Ruidong%20Chen%20and%20Xuanpu%20Zhang%20and%20Lei%20Sun%20and%20Dongyang%20Jin%20and%20Ryan%20Xu%20and%20Nannan%20Zhang%20and%20Dan%20Song%20and%20Xiangxiang%20Chu&entry.1292438233=Video%20virtual%20try-on%20technology%20provides%20a%20cost-effective%20solution%20for%20creating%20marketing%20videos%20in%20fashion%20e-commerce.%20However%2C%20its%20practical%20adoption%20is%20hindered%20by%20two%20critical%20limitations.%20First%2C%20the%20reliance%20on%20a%20single%20garment%20image%20as%20input%20in%20current%20virtual%20try-on%20datasets%20limits%20the%20accurate%20capture%20of%20realistic%20texture%20details.%20Second%2C%20most%20existing%20methods%20focus%20solely%20on%20generating%20full-shot%20virtual%20try-on%20videos%2C%20neglecting%20the%20business%27s%20demand%20for%20videos%20that%20also%20provide%20detailed%20close-ups.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20high-resolution%20dataset%20for%20video-based%20virtual%20try-on.%20This%20dataset%20offers%20two%20key%20features.%20First%2C%20it%20provides%20more%20detailed%20information%20on%20the%20garments%2C%20which%20includes%20high-fidelity%20images%20with%20detailed%20close-ups%20and%20textual%20descriptions%3B%20Second%2C%20it%20uniquely%20includes%20full-shot%20and%20close-up%20try-on%20videos%20of%20real%20human%20models.%20Furthermore%2C%20accurately%20assessing%20consistency%20becomes%20significantly%20more%20critical%20for%20the%20close-up%20videos%2C%20which%20demand%20high-fidelity%20preservation%20of%20garment%20details.%20To%20facilitate%20such%20fine-grained%20evaluation%2C%20we%20propose%20a%20new%20garment%20consistency%20metric%20VGID%20%28Video%20Garment%20Inception%20Distance%29%20that%20quantifies%20the%20preservation%20of%20both%20texture%20and%20structure.%20Our%20experiments%20validate%20these%20contributions.%20We%20demonstrate%20that%20by%20utilizing%20the%20detailed%20images%20from%20our%20dataset%2C%20existing%20video%20generation%20models%20can%20extract%20and%20incorporate%20texture%20features%2C%20significantly%20enhancing%20the%20realism%20and%20detail%20fidelity%20of%20virtual%20try-on%20results.%20Furthermore%2C%20we%20conduct%20a%20comprehensive%20benchmark%20of%20recent%20models.%20The%20benchmark%20effectively%20identifies%20the%20texture%20and%20structural%20preservation%20problems%20among%20current%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.18957v1&entry.124074799=Read"},
{"title": "From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation", "author": "Moazzam Umer Gondal and Hamad Ul Qudous and Daniya Siddiqui and Asma Ahmad Farhan", "abstract": "This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.", "link": "http://arxiv.org/abs/2511.19149v1", "date": "2025-11-24", "relevancy": 2.463, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6513}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6403}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Pixels%20to%20Posts%3A%20Retrieval-Augmented%20Fashion%20Captioning%20and%20Hashtag%20Generation&body=Title%3A%20From%20Pixels%20to%20Posts%3A%20Retrieval-Augmented%20Fashion%20Captioning%20and%20Hashtag%20Generation%0AAuthor%3A%20Moazzam%20Umer%20Gondal%20and%20Hamad%20Ul%20Qudous%20and%20Daniya%20Siddiqui%20and%20Asma%20Ahmad%20Farhan%0AAbstract%3A%20This%20paper%20introduces%20the%20retrieval-augmented%20framework%20for%20automatic%20fashion%20caption%20and%20hashtag%20generation%2C%20combining%20multi-garment%20detection%2C%20attribute%20reasoning%2C%20and%20Large%20Language%20Model%20%28LLM%29%20prompting.%20The%20system%20aims%20to%20produce%20visually%20grounded%2C%20descriptive%2C%20and%20stylistically%20interesting%20text%20for%20fashion%20imagery%2C%20overcoming%20the%20limitations%20of%20end-to-end%20captioners%20that%20have%20problems%20with%20attribute%20fidelity%20and%20domain%20generalization.%20The%20pipeline%20combines%20a%20YOLO-based%20detector%20for%20multi-garment%20localization%2C%20k-means%20clustering%20for%20dominant%20color%20extraction%2C%20and%20a%20CLIP-FAISS%20retrieval%20module%20for%20fabric%20and%20gender%20attribute%20inference%20based%20on%20a%20structured%20product%20index.%20These%20attributes%2C%20together%20with%20retrieved%20style%20examples%2C%20create%20a%20factual%20evidence%20pack%20that%20is%20used%20to%20guide%20an%20LLM%20to%20generate%20human-like%20captions%20and%20contextually%20rich%20hashtags.%20A%20fine-tuned%20BLIP%20model%20is%20used%20as%20a%20supervised%20baseline%20model%20for%20comparison.%20Experimental%20results%20show%20that%20the%20YOLO%20detector%20is%20able%20to%20obtain%20a%20mean%20Average%20Precision%20%28mAP%400.5%29%20of%200.71%20for%20nine%20categories%20of%20garments.%20The%20RAG-LLM%20pipeline%20generates%20expressive%20attribute-aligned%20captions%20and%20achieves%20mean%20attribute%20coverage%20of%200.80%20with%20full%20coverage%20at%20the%2050%25%20threshold%20in%20hashtag%20generation%2C%20whereas%20BLIP%20gives%20higher%20lexical%20overlap%20and%20lower%20generalization.%20The%20retrieval-augmented%20approach%20exhibits%20better%20factual%20grounding%2C%20less%20hallucination%2C%20and%20great%20potential%20for%20scalable%20deployment%20in%20various%20clothing%20domains.%20These%20results%20demonstrate%20the%20use%20of%20retrieval-augmented%20generation%20as%20an%20effective%20and%20interpretable%20paradigm%20for%20automated%20and%20visually%20grounded%20fashion%20content%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Pixels%2520to%2520Posts%253A%2520Retrieval-Augmented%2520Fashion%2520Captioning%2520and%2520Hashtag%2520Generation%26entry.906535625%3DMoazzam%2520Umer%2520Gondal%2520and%2520Hamad%2520Ul%2520Qudous%2520and%2520Daniya%2520Siddiqui%2520and%2520Asma%2520Ahmad%2520Farhan%26entry.1292438233%3DThis%2520paper%2520introduces%2520the%2520retrieval-augmented%2520framework%2520for%2520automatic%2520fashion%2520caption%2520and%2520hashtag%2520generation%252C%2520combining%2520multi-garment%2520detection%252C%2520attribute%2520reasoning%252C%2520and%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520prompting.%2520The%2520system%2520aims%2520to%2520produce%2520visually%2520grounded%252C%2520descriptive%252C%2520and%2520stylistically%2520interesting%2520text%2520for%2520fashion%2520imagery%252C%2520overcoming%2520the%2520limitations%2520of%2520end-to-end%2520captioners%2520that%2520have%2520problems%2520with%2520attribute%2520fidelity%2520and%2520domain%2520generalization.%2520The%2520pipeline%2520combines%2520a%2520YOLO-based%2520detector%2520for%2520multi-garment%2520localization%252C%2520k-means%2520clustering%2520for%2520dominant%2520color%2520extraction%252C%2520and%2520a%2520CLIP-FAISS%2520retrieval%2520module%2520for%2520fabric%2520and%2520gender%2520attribute%2520inference%2520based%2520on%2520a%2520structured%2520product%2520index.%2520These%2520attributes%252C%2520together%2520with%2520retrieved%2520style%2520examples%252C%2520create%2520a%2520factual%2520evidence%2520pack%2520that%2520is%2520used%2520to%2520guide%2520an%2520LLM%2520to%2520generate%2520human-like%2520captions%2520and%2520contextually%2520rich%2520hashtags.%2520A%2520fine-tuned%2520BLIP%2520model%2520is%2520used%2520as%2520a%2520supervised%2520baseline%2520model%2520for%2520comparison.%2520Experimental%2520results%2520show%2520that%2520the%2520YOLO%2520detector%2520is%2520able%2520to%2520obtain%2520a%2520mean%2520Average%2520Precision%2520%2528mAP%25400.5%2529%2520of%25200.71%2520for%2520nine%2520categories%2520of%2520garments.%2520The%2520RAG-LLM%2520pipeline%2520generates%2520expressive%2520attribute-aligned%2520captions%2520and%2520achieves%2520mean%2520attribute%2520coverage%2520of%25200.80%2520with%2520full%2520coverage%2520at%2520the%252050%2525%2520threshold%2520in%2520hashtag%2520generation%252C%2520whereas%2520BLIP%2520gives%2520higher%2520lexical%2520overlap%2520and%2520lower%2520generalization.%2520The%2520retrieval-augmented%2520approach%2520exhibits%2520better%2520factual%2520grounding%252C%2520less%2520hallucination%252C%2520and%2520great%2520potential%2520for%2520scalable%2520deployment%2520in%2520various%2520clothing%2520domains.%2520These%2520results%2520demonstrate%2520the%2520use%2520of%2520retrieval-augmented%2520generation%2520as%2520an%2520effective%2520and%2520interpretable%2520paradigm%2520for%2520automated%2520and%2520visually%2520grounded%2520fashion%2520content%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Pixels%20to%20Posts%3A%20Retrieval-Augmented%20Fashion%20Captioning%20and%20Hashtag%20Generation&entry.906535625=Moazzam%20Umer%20Gondal%20and%20Hamad%20Ul%20Qudous%20and%20Daniya%20Siddiqui%20and%20Asma%20Ahmad%20Farhan&entry.1292438233=This%20paper%20introduces%20the%20retrieval-augmented%20framework%20for%20automatic%20fashion%20caption%20and%20hashtag%20generation%2C%20combining%20multi-garment%20detection%2C%20attribute%20reasoning%2C%20and%20Large%20Language%20Model%20%28LLM%29%20prompting.%20The%20system%20aims%20to%20produce%20visually%20grounded%2C%20descriptive%2C%20and%20stylistically%20interesting%20text%20for%20fashion%20imagery%2C%20overcoming%20the%20limitations%20of%20end-to-end%20captioners%20that%20have%20problems%20with%20attribute%20fidelity%20and%20domain%20generalization.%20The%20pipeline%20combines%20a%20YOLO-based%20detector%20for%20multi-garment%20localization%2C%20k-means%20clustering%20for%20dominant%20color%20extraction%2C%20and%20a%20CLIP-FAISS%20retrieval%20module%20for%20fabric%20and%20gender%20attribute%20inference%20based%20on%20a%20structured%20product%20index.%20These%20attributes%2C%20together%20with%20retrieved%20style%20examples%2C%20create%20a%20factual%20evidence%20pack%20that%20is%20used%20to%20guide%20an%20LLM%20to%20generate%20human-like%20captions%20and%20contextually%20rich%20hashtags.%20A%20fine-tuned%20BLIP%20model%20is%20used%20as%20a%20supervised%20baseline%20model%20for%20comparison.%20Experimental%20results%20show%20that%20the%20YOLO%20detector%20is%20able%20to%20obtain%20a%20mean%20Average%20Precision%20%28mAP%400.5%29%20of%200.71%20for%20nine%20categories%20of%20garments.%20The%20RAG-LLM%20pipeline%20generates%20expressive%20attribute-aligned%20captions%20and%20achieves%20mean%20attribute%20coverage%20of%200.80%20with%20full%20coverage%20at%20the%2050%25%20threshold%20in%20hashtag%20generation%2C%20whereas%20BLIP%20gives%20higher%20lexical%20overlap%20and%20lower%20generalization.%20The%20retrieval-augmented%20approach%20exhibits%20better%20factual%20grounding%2C%20less%20hallucination%2C%20and%20great%20potential%20for%20scalable%20deployment%20in%20various%20clothing%20domains.%20These%20results%20demonstrate%20the%20use%20of%20retrieval-augmented%20generation%20as%20an%20effective%20and%20interpretable%20paradigm%20for%20automated%20and%20visually%20grounded%20fashion%20content%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2511.19149v1&entry.124074799=Read"},
{"title": "Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors", "author": "Haihang Wu and Yuchen Zhou", "abstract": "Monocular Simultaneous Localization and Mapping (SLAM) aims to estimate a robot's pose while simultaneously reconstructing an unknown 3D scene using a single camera. While existing monocular SLAM systems generate detailed 3D geometry through dense scene representations, they are computationally expensive due to the need for iterative optimization. To address this challenge, MASt3R-SLAM utilizes learned 3D reconstruction priors, enabling more efficient and accurate estimation of both 3D structures and camera poses. However, MASt3R-SLAM is limited to single-agent operation. In this paper, we extend MASt3R-SLAM to introduce the first multi-agent monocular dense SLAM system. Each agent performs local SLAM using a 3D reconstruction prior, and their individual maps are fused into a globally consistent map through a loop-closure-based map fusion mechanism. Our approach improves computational efficiency compared to state-of-the-art methods, while maintaining similar mapping accuracy when evaluated on real-world datasets.", "link": "http://arxiv.org/abs/2511.19031v1", "date": "2025-11-24", "relevancy": 2.4618, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6328}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6036}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Monocular%20Dense%20SLAM%20With%203D%20Reconstruction%20Priors&body=Title%3A%20Multi-Agent%20Monocular%20Dense%20SLAM%20With%203D%20Reconstruction%20Priors%0AAuthor%3A%20Haihang%20Wu%20and%20Yuchen%20Zhou%0AAbstract%3A%20Monocular%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20aims%20to%20estimate%20a%20robot%27s%20pose%20while%20simultaneously%20reconstructing%20an%20unknown%203D%20scene%20using%20a%20single%20camera.%20While%20existing%20monocular%20SLAM%20systems%20generate%20detailed%203D%20geometry%20through%20dense%20scene%20representations%2C%20they%20are%20computationally%20expensive%20due%20to%20the%20need%20for%20iterative%20optimization.%20To%20address%20this%20challenge%2C%20MASt3R-SLAM%20utilizes%20learned%203D%20reconstruction%20priors%2C%20enabling%20more%20efficient%20and%20accurate%20estimation%20of%20both%203D%20structures%20and%20camera%20poses.%20However%2C%20MASt3R-SLAM%20is%20limited%20to%20single-agent%20operation.%20In%20this%20paper%2C%20we%20extend%20MASt3R-SLAM%20to%20introduce%20the%20first%20multi-agent%20monocular%20dense%20SLAM%20system.%20Each%20agent%20performs%20local%20SLAM%20using%20a%203D%20reconstruction%20prior%2C%20and%20their%20individual%20maps%20are%20fused%20into%20a%20globally%20consistent%20map%20through%20a%20loop-closure-based%20map%20fusion%20mechanism.%20Our%20approach%20improves%20computational%20efficiency%20compared%20to%20state-of-the-art%20methods%2C%20while%20maintaining%20similar%20mapping%20accuracy%20when%20evaluated%20on%20real-world%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%2520Monocular%2520Dense%2520SLAM%2520With%25203D%2520Reconstruction%2520Priors%26entry.906535625%3DHaihang%2520Wu%2520and%2520Yuchen%2520Zhou%26entry.1292438233%3DMonocular%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520aims%2520to%2520estimate%2520a%2520robot%2527s%2520pose%2520while%2520simultaneously%2520reconstructing%2520an%2520unknown%25203D%2520scene%2520using%2520a%2520single%2520camera.%2520While%2520existing%2520monocular%2520SLAM%2520systems%2520generate%2520detailed%25203D%2520geometry%2520through%2520dense%2520scene%2520representations%252C%2520they%2520are%2520computationally%2520expensive%2520due%2520to%2520the%2520need%2520for%2520iterative%2520optimization.%2520To%2520address%2520this%2520challenge%252C%2520MASt3R-SLAM%2520utilizes%2520learned%25203D%2520reconstruction%2520priors%252C%2520enabling%2520more%2520efficient%2520and%2520accurate%2520estimation%2520of%2520both%25203D%2520structures%2520and%2520camera%2520poses.%2520However%252C%2520MASt3R-SLAM%2520is%2520limited%2520to%2520single-agent%2520operation.%2520In%2520this%2520paper%252C%2520we%2520extend%2520MASt3R-SLAM%2520to%2520introduce%2520the%2520first%2520multi-agent%2520monocular%2520dense%2520SLAM%2520system.%2520Each%2520agent%2520performs%2520local%2520SLAM%2520using%2520a%25203D%2520reconstruction%2520prior%252C%2520and%2520their%2520individual%2520maps%2520are%2520fused%2520into%2520a%2520globally%2520consistent%2520map%2520through%2520a%2520loop-closure-based%2520map%2520fusion%2520mechanism.%2520Our%2520approach%2520improves%2520computational%2520efficiency%2520compared%2520to%2520state-of-the-art%2520methods%252C%2520while%2520maintaining%2520similar%2520mapping%2520accuracy%2520when%2520evaluated%2520on%2520real-world%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Monocular%20Dense%20SLAM%20With%203D%20Reconstruction%20Priors&entry.906535625=Haihang%20Wu%20and%20Yuchen%20Zhou&entry.1292438233=Monocular%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20aims%20to%20estimate%20a%20robot%27s%20pose%20while%20simultaneously%20reconstructing%20an%20unknown%203D%20scene%20using%20a%20single%20camera.%20While%20existing%20monocular%20SLAM%20systems%20generate%20detailed%203D%20geometry%20through%20dense%20scene%20representations%2C%20they%20are%20computationally%20expensive%20due%20to%20the%20need%20for%20iterative%20optimization.%20To%20address%20this%20challenge%2C%20MASt3R-SLAM%20utilizes%20learned%203D%20reconstruction%20priors%2C%20enabling%20more%20efficient%20and%20accurate%20estimation%20of%20both%203D%20structures%20and%20camera%20poses.%20However%2C%20MASt3R-SLAM%20is%20limited%20to%20single-agent%20operation.%20In%20this%20paper%2C%20we%20extend%20MASt3R-SLAM%20to%20introduce%20the%20first%20multi-agent%20monocular%20dense%20SLAM%20system.%20Each%20agent%20performs%20local%20SLAM%20using%20a%203D%20reconstruction%20prior%2C%20and%20their%20individual%20maps%20are%20fused%20into%20a%20globally%20consistent%20map%20through%20a%20loop-closure-based%20map%20fusion%20mechanism.%20Our%20approach%20improves%20computational%20efficiency%20compared%20to%20state-of-the-art%20methods%2C%20while%20maintaining%20similar%20mapping%20accuracy%20when%20evaluated%20on%20real-world%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2511.19031v1&entry.124074799=Read"},
{"title": "Don't Reach for the Stars: Rethinking Topology for Resilient Federated Learning", "author": "Mirko Konstantin and Anirban Mukhopadhyay", "abstract": "Federated learning (FL) enables collaborative model training across distributed clients while preserving data privacy by keeping data local. Traditional FL approaches rely on a centralized, star-shaped topology, where a central server aggregates model updates from clients. However, this architecture introduces several limitations, including a single point of failure, limited personalization, and poor robustness to distribution shifts or vulnerability to malfunctioning clients. Moreover, update selection in centralized FL often relies on low-level parameter differences, which can be unreliable when client data is not independent and identically distributed, and offer clients little control. In this work, we propose a decentralized, peer-to-peer (P2P) FL framework. It leverages the flexibility of the P2P topology to enable each client to identify and aggregate a personalized set of trustworthy and beneficial updates.This framework is the Local Inference Guided Aggregation for Heterogeneous Training Environments to Yield Enhancement Through Agreement and Regularization (LIGHTYEAR). Central to our method is an agreement score, computed on a local validation set, which quantifies the semantic alignment of incoming updates in the function space with respect to the clients reference model. Each client uses this score to select a tailored subset of updates and performs aggregation with a regularization term that further stabilizes the training. Our empirical evaluation across five datasets shows that the proposed approach consistently outperforms both, centralized baselines and existing P2P methods in terms of client-level performance, particularly under adversarial and heterogeneous conditions.", "link": "http://arxiv.org/abs/2508.05224v2", "date": "2025-11-24", "relevancy": 2.4616, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.505}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5048}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Reach%20for%20the%20Stars%3A%20Rethinking%20Topology%20for%20Resilient%20Federated%20Learning&body=Title%3A%20Don%27t%20Reach%20for%20the%20Stars%3A%20Rethinking%20Topology%20for%20Resilient%20Federated%20Learning%0AAuthor%3A%20Mirko%20Konstantin%20and%20Anirban%20Mukhopadhyay%0AAbstract%3A%20Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%20distributed%20clients%20while%20preserving%20data%20privacy%20by%20keeping%20data%20local.%20Traditional%20FL%20approaches%20rely%20on%20a%20centralized%2C%20star-shaped%20topology%2C%20where%20a%20central%20server%20aggregates%20model%20updates%20from%20clients.%20However%2C%20this%20architecture%20introduces%20several%20limitations%2C%20including%20a%20single%20point%20of%20failure%2C%20limited%20personalization%2C%20and%20poor%20robustness%20to%20distribution%20shifts%20or%20vulnerability%20to%20malfunctioning%20clients.%20Moreover%2C%20update%20selection%20in%20centralized%20FL%20often%20relies%20on%20low-level%20parameter%20differences%2C%20which%20can%20be%20unreliable%20when%20client%20data%20is%20not%20independent%20and%20identically%20distributed%2C%20and%20offer%20clients%20little%20control.%20In%20this%20work%2C%20we%20propose%20a%20decentralized%2C%20peer-to-peer%20%28P2P%29%20FL%20framework.%20It%20leverages%20the%20flexibility%20of%20the%20P2P%20topology%20to%20enable%20each%20client%20to%20identify%20and%20aggregate%20a%20personalized%20set%20of%20trustworthy%20and%20beneficial%20updates.This%20framework%20is%20the%20Local%20Inference%20Guided%20Aggregation%20for%20Heterogeneous%20Training%20Environments%20to%20Yield%20Enhancement%20Through%20Agreement%20and%20Regularization%20%28LIGHTYEAR%29.%20Central%20to%20our%20method%20is%20an%20agreement%20score%2C%20computed%20on%20a%20local%20validation%20set%2C%20which%20quantifies%20the%20semantic%20alignment%20of%20incoming%20updates%20in%20the%20function%20space%20with%20respect%20to%20the%20clients%20reference%20model.%20Each%20client%20uses%20this%20score%20to%20select%20a%20tailored%20subset%20of%20updates%20and%20performs%20aggregation%20with%20a%20regularization%20term%20that%20further%20stabilizes%20the%20training.%20Our%20empirical%20evaluation%20across%20five%20datasets%20shows%20that%20the%20proposed%20approach%20consistently%20outperforms%20both%2C%20centralized%20baselines%20and%20existing%20P2P%20methods%20in%20terms%20of%20client-level%20performance%2C%20particularly%20under%20adversarial%20and%20heterogeneous%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2508.05224v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Reach%2520for%2520the%2520Stars%253A%2520Rethinking%2520Topology%2520for%2520Resilient%2520Federated%2520Learning%26entry.906535625%3DMirko%2520Konstantin%2520and%2520Anirban%2520Mukhopadhyay%26entry.1292438233%3DFederated%2520learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520across%2520distributed%2520clients%2520while%2520preserving%2520data%2520privacy%2520by%2520keeping%2520data%2520local.%2520Traditional%2520FL%2520approaches%2520rely%2520on%2520a%2520centralized%252C%2520star-shaped%2520topology%252C%2520where%2520a%2520central%2520server%2520aggregates%2520model%2520updates%2520from%2520clients.%2520However%252C%2520this%2520architecture%2520introduces%2520several%2520limitations%252C%2520including%2520a%2520single%2520point%2520of%2520failure%252C%2520limited%2520personalization%252C%2520and%2520poor%2520robustness%2520to%2520distribution%2520shifts%2520or%2520vulnerability%2520to%2520malfunctioning%2520clients.%2520Moreover%252C%2520update%2520selection%2520in%2520centralized%2520FL%2520often%2520relies%2520on%2520low-level%2520parameter%2520differences%252C%2520which%2520can%2520be%2520unreliable%2520when%2520client%2520data%2520is%2520not%2520independent%2520and%2520identically%2520distributed%252C%2520and%2520offer%2520clients%2520little%2520control.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520decentralized%252C%2520peer-to-peer%2520%2528P2P%2529%2520FL%2520framework.%2520It%2520leverages%2520the%2520flexibility%2520of%2520the%2520P2P%2520topology%2520to%2520enable%2520each%2520client%2520to%2520identify%2520and%2520aggregate%2520a%2520personalized%2520set%2520of%2520trustworthy%2520and%2520beneficial%2520updates.This%2520framework%2520is%2520the%2520Local%2520Inference%2520Guided%2520Aggregation%2520for%2520Heterogeneous%2520Training%2520Environments%2520to%2520Yield%2520Enhancement%2520Through%2520Agreement%2520and%2520Regularization%2520%2528LIGHTYEAR%2529.%2520Central%2520to%2520our%2520method%2520is%2520an%2520agreement%2520score%252C%2520computed%2520on%2520a%2520local%2520validation%2520set%252C%2520which%2520quantifies%2520the%2520semantic%2520alignment%2520of%2520incoming%2520updates%2520in%2520the%2520function%2520space%2520with%2520respect%2520to%2520the%2520clients%2520reference%2520model.%2520Each%2520client%2520uses%2520this%2520score%2520to%2520select%2520a%2520tailored%2520subset%2520of%2520updates%2520and%2520performs%2520aggregation%2520with%2520a%2520regularization%2520term%2520that%2520further%2520stabilizes%2520the%2520training.%2520Our%2520empirical%2520evaluation%2520across%2520five%2520datasets%2520shows%2520that%2520the%2520proposed%2520approach%2520consistently%2520outperforms%2520both%252C%2520centralized%2520baselines%2520and%2520existing%2520P2P%2520methods%2520in%2520terms%2520of%2520client-level%2520performance%252C%2520particularly%2520under%2520adversarial%2520and%2520heterogeneous%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05224v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Reach%20for%20the%20Stars%3A%20Rethinking%20Topology%20for%20Resilient%20Federated%20Learning&entry.906535625=Mirko%20Konstantin%20and%20Anirban%20Mukhopadhyay&entry.1292438233=Federated%20learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%20distributed%20clients%20while%20preserving%20data%20privacy%20by%20keeping%20data%20local.%20Traditional%20FL%20approaches%20rely%20on%20a%20centralized%2C%20star-shaped%20topology%2C%20where%20a%20central%20server%20aggregates%20model%20updates%20from%20clients.%20However%2C%20this%20architecture%20introduces%20several%20limitations%2C%20including%20a%20single%20point%20of%20failure%2C%20limited%20personalization%2C%20and%20poor%20robustness%20to%20distribution%20shifts%20or%20vulnerability%20to%20malfunctioning%20clients.%20Moreover%2C%20update%20selection%20in%20centralized%20FL%20often%20relies%20on%20low-level%20parameter%20differences%2C%20which%20can%20be%20unreliable%20when%20client%20data%20is%20not%20independent%20and%20identically%20distributed%2C%20and%20offer%20clients%20little%20control.%20In%20this%20work%2C%20we%20propose%20a%20decentralized%2C%20peer-to-peer%20%28P2P%29%20FL%20framework.%20It%20leverages%20the%20flexibility%20of%20the%20P2P%20topology%20to%20enable%20each%20client%20to%20identify%20and%20aggregate%20a%20personalized%20set%20of%20trustworthy%20and%20beneficial%20updates.This%20framework%20is%20the%20Local%20Inference%20Guided%20Aggregation%20for%20Heterogeneous%20Training%20Environments%20to%20Yield%20Enhancement%20Through%20Agreement%20and%20Regularization%20%28LIGHTYEAR%29.%20Central%20to%20our%20method%20is%20an%20agreement%20score%2C%20computed%20on%20a%20local%20validation%20set%2C%20which%20quantifies%20the%20semantic%20alignment%20of%20incoming%20updates%20in%20the%20function%20space%20with%20respect%20to%20the%20clients%20reference%20model.%20Each%20client%20uses%20this%20score%20to%20select%20a%20tailored%20subset%20of%20updates%20and%20performs%20aggregation%20with%20a%20regularization%20term%20that%20further%20stabilizes%20the%20training.%20Our%20empirical%20evaluation%20across%20five%20datasets%20shows%20that%20the%20proposed%20approach%20consistently%20outperforms%20both%2C%20centralized%20baselines%20and%20existing%20P2P%20methods%20in%20terms%20of%20client-level%20performance%2C%20particularly%20under%20adversarial%20and%20heterogeneous%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2508.05224v2&entry.124074799=Read"},
{"title": "Introducing Visual Scenes and Reasoning: A More Realistic Benchmark for Spoken Language Understanding", "author": "Di Wu and Liting Jiang and Ruiyu Fang and  Bianjing and Hongyan Xie and Haoxiang Su and Hao Huang and Zhongjiang He and Shuangyong Song and Xuelong Li", "abstract": "Spoken Language Understanding (SLU) consists of two sub-tasks: intent detection (ID) and slot filling (SF). Given its broad range of real-world applications, enhancing SLU for practical deployment is increasingly critical. Profile-based SLU addresses ambiguous user utterances by incorporating context awareness (CA), user profiles (UP), and knowledge graphs (KG) to support disambiguation, thereby advancing SLU research toward real-world applicability. However, existing SLU datasets still fall short in representing real-world scenarios. Specifically, (1) CA uses one-hot vectors for representation, which is overly idealized, and (2) models typically focuses solely on predicting intents and slot labels, neglecting the reasoning process that could enhance performance and interpretability. To overcome these limitations, we introduce VRSLU, a novel SLU dataset that integrates both Visual images and explicit Reasoning. For over-idealized CA, we use GPT-4o and FLUX.1-dev to generate images reflecting users' environments and statuses, followed by human verification to ensure quality. For reasoning, GPT-4o is employed to generate explanations for predicted labels, which are then refined by human annotators to ensure accuracy and coherence. Additionally, we propose an instructional template, LR-Instruct, which first predicts labels and then generates corresponding reasoning. This two-step approach helps mitigate the influence of reasoning bias on label prediction. Experimental results confirm the effectiveness of incorporating visual information and highlight the promise of explicit reasoning in advancing SLU.", "link": "http://arxiv.org/abs/2511.19005v1", "date": "2025-11-24", "relevancy": 2.4613, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6281}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6281}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%20Visual%20Scenes%20and%20Reasoning%3A%20A%20More%20Realistic%20Benchmark%20for%20Spoken%20Language%20Understanding&body=Title%3A%20Introducing%20Visual%20Scenes%20and%20Reasoning%3A%20A%20More%20Realistic%20Benchmark%20for%20Spoken%20Language%20Understanding%0AAuthor%3A%20Di%20Wu%20and%20Liting%20Jiang%20and%20Ruiyu%20Fang%20and%20%20Bianjing%20and%20Hongyan%20Xie%20and%20Haoxiang%20Su%20and%20Hao%20Huang%20and%20Zhongjiang%20He%20and%20Shuangyong%20Song%20and%20Xuelong%20Li%0AAbstract%3A%20Spoken%20Language%20Understanding%20%28SLU%29%20consists%20of%20two%20sub-tasks%3A%20intent%20detection%20%28ID%29%20and%20slot%20filling%20%28SF%29.%20Given%20its%20broad%20range%20of%20real-world%20applications%2C%20enhancing%20SLU%20for%20practical%20deployment%20is%20increasingly%20critical.%20Profile-based%20SLU%20addresses%20ambiguous%20user%20utterances%20by%20incorporating%20context%20awareness%20%28CA%29%2C%20user%20profiles%20%28UP%29%2C%20and%20knowledge%20graphs%20%28KG%29%20to%20support%20disambiguation%2C%20thereby%20advancing%20SLU%20research%20toward%20real-world%20applicability.%20However%2C%20existing%20SLU%20datasets%20still%20fall%20short%20in%20representing%20real-world%20scenarios.%20Specifically%2C%20%281%29%20CA%20uses%20one-hot%20vectors%20for%20representation%2C%20which%20is%20overly%20idealized%2C%20and%20%282%29%20models%20typically%20focuses%20solely%20on%20predicting%20intents%20and%20slot%20labels%2C%20neglecting%20the%20reasoning%20process%20that%20could%20enhance%20performance%20and%20interpretability.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20VRSLU%2C%20a%20novel%20SLU%20dataset%20that%20integrates%20both%20Visual%20images%20and%20explicit%20Reasoning.%20For%20over-idealized%20CA%2C%20we%20use%20GPT-4o%20and%20FLUX.1-dev%20to%20generate%20images%20reflecting%20users%27%20environments%20and%20statuses%2C%20followed%20by%20human%20verification%20to%20ensure%20quality.%20For%20reasoning%2C%20GPT-4o%20is%20employed%20to%20generate%20explanations%20for%20predicted%20labels%2C%20which%20are%20then%20refined%20by%20human%20annotators%20to%20ensure%20accuracy%20and%20coherence.%20Additionally%2C%20we%20propose%20an%20instructional%20template%2C%20LR-Instruct%2C%20which%20first%20predicts%20labels%20and%20then%20generates%20corresponding%20reasoning.%20This%20two-step%20approach%20helps%20mitigate%20the%20influence%20of%20reasoning%20bias%20on%20label%20prediction.%20Experimental%20results%20confirm%20the%20effectiveness%20of%20incorporating%20visual%20information%20and%20highlight%20the%20promise%20of%20explicit%20reasoning%20in%20advancing%20SLU.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%2520Visual%2520Scenes%2520and%2520Reasoning%253A%2520A%2520More%2520Realistic%2520Benchmark%2520for%2520Spoken%2520Language%2520Understanding%26entry.906535625%3DDi%2520Wu%2520and%2520Liting%2520Jiang%2520and%2520Ruiyu%2520Fang%2520and%2520%2520Bianjing%2520and%2520Hongyan%2520Xie%2520and%2520Haoxiang%2520Su%2520and%2520Hao%2520Huang%2520and%2520Zhongjiang%2520He%2520and%2520Shuangyong%2520Song%2520and%2520Xuelong%2520Li%26entry.1292438233%3DSpoken%2520Language%2520Understanding%2520%2528SLU%2529%2520consists%2520of%2520two%2520sub-tasks%253A%2520intent%2520detection%2520%2528ID%2529%2520and%2520slot%2520filling%2520%2528SF%2529.%2520Given%2520its%2520broad%2520range%2520of%2520real-world%2520applications%252C%2520enhancing%2520SLU%2520for%2520practical%2520deployment%2520is%2520increasingly%2520critical.%2520Profile-based%2520SLU%2520addresses%2520ambiguous%2520user%2520utterances%2520by%2520incorporating%2520context%2520awareness%2520%2528CA%2529%252C%2520user%2520profiles%2520%2528UP%2529%252C%2520and%2520knowledge%2520graphs%2520%2528KG%2529%2520to%2520support%2520disambiguation%252C%2520thereby%2520advancing%2520SLU%2520research%2520toward%2520real-world%2520applicability.%2520However%252C%2520existing%2520SLU%2520datasets%2520still%2520fall%2520short%2520in%2520representing%2520real-world%2520scenarios.%2520Specifically%252C%2520%25281%2529%2520CA%2520uses%2520one-hot%2520vectors%2520for%2520representation%252C%2520which%2520is%2520overly%2520idealized%252C%2520and%2520%25282%2529%2520models%2520typically%2520focuses%2520solely%2520on%2520predicting%2520intents%2520and%2520slot%2520labels%252C%2520neglecting%2520the%2520reasoning%2520process%2520that%2520could%2520enhance%2520performance%2520and%2520interpretability.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520VRSLU%252C%2520a%2520novel%2520SLU%2520dataset%2520that%2520integrates%2520both%2520Visual%2520images%2520and%2520explicit%2520Reasoning.%2520For%2520over-idealized%2520CA%252C%2520we%2520use%2520GPT-4o%2520and%2520FLUX.1-dev%2520to%2520generate%2520images%2520reflecting%2520users%2527%2520environments%2520and%2520statuses%252C%2520followed%2520by%2520human%2520verification%2520to%2520ensure%2520quality.%2520For%2520reasoning%252C%2520GPT-4o%2520is%2520employed%2520to%2520generate%2520explanations%2520for%2520predicted%2520labels%252C%2520which%2520are%2520then%2520refined%2520by%2520human%2520annotators%2520to%2520ensure%2520accuracy%2520and%2520coherence.%2520Additionally%252C%2520we%2520propose%2520an%2520instructional%2520template%252C%2520LR-Instruct%252C%2520which%2520first%2520predicts%2520labels%2520and%2520then%2520generates%2520corresponding%2520reasoning.%2520This%2520two-step%2520approach%2520helps%2520mitigate%2520the%2520influence%2520of%2520reasoning%2520bias%2520on%2520label%2520prediction.%2520Experimental%2520results%2520confirm%2520the%2520effectiveness%2520of%2520incorporating%2520visual%2520information%2520and%2520highlight%2520the%2520promise%2520of%2520explicit%2520reasoning%2520in%2520advancing%2520SLU.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20Visual%20Scenes%20and%20Reasoning%3A%20A%20More%20Realistic%20Benchmark%20for%20Spoken%20Language%20Understanding&entry.906535625=Di%20Wu%20and%20Liting%20Jiang%20and%20Ruiyu%20Fang%20and%20%20Bianjing%20and%20Hongyan%20Xie%20and%20Haoxiang%20Su%20and%20Hao%20Huang%20and%20Zhongjiang%20He%20and%20Shuangyong%20Song%20and%20Xuelong%20Li&entry.1292438233=Spoken%20Language%20Understanding%20%28SLU%29%20consists%20of%20two%20sub-tasks%3A%20intent%20detection%20%28ID%29%20and%20slot%20filling%20%28SF%29.%20Given%20its%20broad%20range%20of%20real-world%20applications%2C%20enhancing%20SLU%20for%20practical%20deployment%20is%20increasingly%20critical.%20Profile-based%20SLU%20addresses%20ambiguous%20user%20utterances%20by%20incorporating%20context%20awareness%20%28CA%29%2C%20user%20profiles%20%28UP%29%2C%20and%20knowledge%20graphs%20%28KG%29%20to%20support%20disambiguation%2C%20thereby%20advancing%20SLU%20research%20toward%20real-world%20applicability.%20However%2C%20existing%20SLU%20datasets%20still%20fall%20short%20in%20representing%20real-world%20scenarios.%20Specifically%2C%20%281%29%20CA%20uses%20one-hot%20vectors%20for%20representation%2C%20which%20is%20overly%20idealized%2C%20and%20%282%29%20models%20typically%20focuses%20solely%20on%20predicting%20intents%20and%20slot%20labels%2C%20neglecting%20the%20reasoning%20process%20that%20could%20enhance%20performance%20and%20interpretability.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20VRSLU%2C%20a%20novel%20SLU%20dataset%20that%20integrates%20both%20Visual%20images%20and%20explicit%20Reasoning.%20For%20over-idealized%20CA%2C%20we%20use%20GPT-4o%20and%20FLUX.1-dev%20to%20generate%20images%20reflecting%20users%27%20environments%20and%20statuses%2C%20followed%20by%20human%20verification%20to%20ensure%20quality.%20For%20reasoning%2C%20GPT-4o%20is%20employed%20to%20generate%20explanations%20for%20predicted%20labels%2C%20which%20are%20then%20refined%20by%20human%20annotators%20to%20ensure%20accuracy%20and%20coherence.%20Additionally%2C%20we%20propose%20an%20instructional%20template%2C%20LR-Instruct%2C%20which%20first%20predicts%20labels%20and%20then%20generates%20corresponding%20reasoning.%20This%20two-step%20approach%20helps%20mitigate%20the%20influence%20of%20reasoning%20bias%20on%20label%20prediction.%20Experimental%20results%20confirm%20the%20effectiveness%20of%20incorporating%20visual%20information%20and%20highlight%20the%20promise%20of%20explicit%20reasoning%20in%20advancing%20SLU.&entry.1838667208=http%3A//arxiv.org/abs/2511.19005v1&entry.124074799=Read"},
{"title": "Leveraging Adversarial Learning for Pathological Fidelity in Virtual Staining", "author": "Jos\u00e9 Teixeira and Pascal Kl\u00f6ckner and Diana Montezuma and Melis Erdal Cesur and Jo\u00e3o Fraga and Hugo M. Horlings and Jaime S. Cardoso and Sara P. Oliveira", "abstract": "In addition to evaluating tumor morphology using H&E staining, immunohistochemistry is used to assess the presence of specific proteins within the tissue. However, this is a costly and labor-intensive technique, for which virtual staining, as an image-to-image translation task, offers a promising alternative. Although recent, this is an emerging field of research with 64% of published studies just in 2024. Most studies use publicly available datasets of H&E-IHC pairs from consecutive tissue sections. Recognizing the training challenges, many authors develop complex virtual staining models based on conditional Generative Adversarial Networks, but ignore the impact of adversarial loss on the quality of virtual staining. Furthermore, overlooking the issues of model evaluation, they claim improved performance based on metrics such as SSIM and PSNR, which are not sufficiently robust to evaluate the quality of virtually stained images. In this paper, we developed CSSP2P GAN, which we demonstrate to achieve heightened pathological fidelity through a blind pathological expert evaluation. Furthermore, while iteratively developing our model, we study the impact of the adversarial loss and demonstrate its crucial role in the quality of virtually stained images. Finally, while comparing our model with reference works in the field, we underscore the limitations of the currently used evaluation metrics and demonstrate the superior performance of CSSP2P GAN.", "link": "http://arxiv.org/abs/2511.18946v1", "date": "2025-11-24", "relevancy": 2.4613, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4925}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4922}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Adversarial%20Learning%20for%20Pathological%20Fidelity%20in%20Virtual%20Staining&body=Title%3A%20Leveraging%20Adversarial%20Learning%20for%20Pathological%20Fidelity%20in%20Virtual%20Staining%0AAuthor%3A%20Jos%C3%A9%20Teixeira%20and%20Pascal%20Kl%C3%B6ckner%20and%20Diana%20Montezuma%20and%20Melis%20Erdal%20Cesur%20and%20Jo%C3%A3o%20Fraga%20and%20Hugo%20M.%20Horlings%20and%20Jaime%20S.%20Cardoso%20and%20Sara%20P.%20Oliveira%0AAbstract%3A%20In%20addition%20to%20evaluating%20tumor%20morphology%20using%20H%26E%20staining%2C%20immunohistochemistry%20is%20used%20to%20assess%20the%20presence%20of%20specific%20proteins%20within%20the%20tissue.%20However%2C%20this%20is%20a%20costly%20and%20labor-intensive%20technique%2C%20for%20which%20virtual%20staining%2C%20as%20an%20image-to-image%20translation%20task%2C%20offers%20a%20promising%20alternative.%20Although%20recent%2C%20this%20is%20an%20emerging%20field%20of%20research%20with%2064%25%20of%20published%20studies%20just%20in%202024.%20Most%20studies%20use%20publicly%20available%20datasets%20of%20H%26E-IHC%20pairs%20from%20consecutive%20tissue%20sections.%20Recognizing%20the%20training%20challenges%2C%20many%20authors%20develop%20complex%20virtual%20staining%20models%20based%20on%20conditional%20Generative%20Adversarial%20Networks%2C%20but%20ignore%20the%20impact%20of%20adversarial%20loss%20on%20the%20quality%20of%20virtual%20staining.%20Furthermore%2C%20overlooking%20the%20issues%20of%20model%20evaluation%2C%20they%20claim%20improved%20performance%20based%20on%20metrics%20such%20as%20SSIM%20and%20PSNR%2C%20which%20are%20not%20sufficiently%20robust%20to%20evaluate%20the%20quality%20of%20virtually%20stained%20images.%20In%20this%20paper%2C%20we%20developed%20CSSP2P%20GAN%2C%20which%20we%20demonstrate%20to%20achieve%20heightened%20pathological%20fidelity%20through%20a%20blind%20pathological%20expert%20evaluation.%20Furthermore%2C%20while%20iteratively%20developing%20our%20model%2C%20we%20study%20the%20impact%20of%20the%20adversarial%20loss%20and%20demonstrate%20its%20crucial%20role%20in%20the%20quality%20of%20virtually%20stained%20images.%20Finally%2C%20while%20comparing%20our%20model%20with%20reference%20works%20in%20the%20field%2C%20we%20underscore%20the%20limitations%20of%20the%20currently%20used%20evaluation%20metrics%20and%20demonstrate%20the%20superior%20performance%20of%20CSSP2P%20GAN.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Adversarial%2520Learning%2520for%2520Pathological%2520Fidelity%2520in%2520Virtual%2520Staining%26entry.906535625%3DJos%25C3%25A9%2520Teixeira%2520and%2520Pascal%2520Kl%25C3%25B6ckner%2520and%2520Diana%2520Montezuma%2520and%2520Melis%2520Erdal%2520Cesur%2520and%2520Jo%25C3%25A3o%2520Fraga%2520and%2520Hugo%2520M.%2520Horlings%2520and%2520Jaime%2520S.%2520Cardoso%2520and%2520Sara%2520P.%2520Oliveira%26entry.1292438233%3DIn%2520addition%2520to%2520evaluating%2520tumor%2520morphology%2520using%2520H%2526E%2520staining%252C%2520immunohistochemistry%2520is%2520used%2520to%2520assess%2520the%2520presence%2520of%2520specific%2520proteins%2520within%2520the%2520tissue.%2520However%252C%2520this%2520is%2520a%2520costly%2520and%2520labor-intensive%2520technique%252C%2520for%2520which%2520virtual%2520staining%252C%2520as%2520an%2520image-to-image%2520translation%2520task%252C%2520offers%2520a%2520promising%2520alternative.%2520Although%2520recent%252C%2520this%2520is%2520an%2520emerging%2520field%2520of%2520research%2520with%252064%2525%2520of%2520published%2520studies%2520just%2520in%25202024.%2520Most%2520studies%2520use%2520publicly%2520available%2520datasets%2520of%2520H%2526E-IHC%2520pairs%2520from%2520consecutive%2520tissue%2520sections.%2520Recognizing%2520the%2520training%2520challenges%252C%2520many%2520authors%2520develop%2520complex%2520virtual%2520staining%2520models%2520based%2520on%2520conditional%2520Generative%2520Adversarial%2520Networks%252C%2520but%2520ignore%2520the%2520impact%2520of%2520adversarial%2520loss%2520on%2520the%2520quality%2520of%2520virtual%2520staining.%2520Furthermore%252C%2520overlooking%2520the%2520issues%2520of%2520model%2520evaluation%252C%2520they%2520claim%2520improved%2520performance%2520based%2520on%2520metrics%2520such%2520as%2520SSIM%2520and%2520PSNR%252C%2520which%2520are%2520not%2520sufficiently%2520robust%2520to%2520evaluate%2520the%2520quality%2520of%2520virtually%2520stained%2520images.%2520In%2520this%2520paper%252C%2520we%2520developed%2520CSSP2P%2520GAN%252C%2520which%2520we%2520demonstrate%2520to%2520achieve%2520heightened%2520pathological%2520fidelity%2520through%2520a%2520blind%2520pathological%2520expert%2520evaluation.%2520Furthermore%252C%2520while%2520iteratively%2520developing%2520our%2520model%252C%2520we%2520study%2520the%2520impact%2520of%2520the%2520adversarial%2520loss%2520and%2520demonstrate%2520its%2520crucial%2520role%2520in%2520the%2520quality%2520of%2520virtually%2520stained%2520images.%2520Finally%252C%2520while%2520comparing%2520our%2520model%2520with%2520reference%2520works%2520in%2520the%2520field%252C%2520we%2520underscore%2520the%2520limitations%2520of%2520the%2520currently%2520used%2520evaluation%2520metrics%2520and%2520demonstrate%2520the%2520superior%2520performance%2520of%2520CSSP2P%2520GAN.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Adversarial%20Learning%20for%20Pathological%20Fidelity%20in%20Virtual%20Staining&entry.906535625=Jos%C3%A9%20Teixeira%20and%20Pascal%20Kl%C3%B6ckner%20and%20Diana%20Montezuma%20and%20Melis%20Erdal%20Cesur%20and%20Jo%C3%A3o%20Fraga%20and%20Hugo%20M.%20Horlings%20and%20Jaime%20S.%20Cardoso%20and%20Sara%20P.%20Oliveira&entry.1292438233=In%20addition%20to%20evaluating%20tumor%20morphology%20using%20H%26E%20staining%2C%20immunohistochemistry%20is%20used%20to%20assess%20the%20presence%20of%20specific%20proteins%20within%20the%20tissue.%20However%2C%20this%20is%20a%20costly%20and%20labor-intensive%20technique%2C%20for%20which%20virtual%20staining%2C%20as%20an%20image-to-image%20translation%20task%2C%20offers%20a%20promising%20alternative.%20Although%20recent%2C%20this%20is%20an%20emerging%20field%20of%20research%20with%2064%25%20of%20published%20studies%20just%20in%202024.%20Most%20studies%20use%20publicly%20available%20datasets%20of%20H%26E-IHC%20pairs%20from%20consecutive%20tissue%20sections.%20Recognizing%20the%20training%20challenges%2C%20many%20authors%20develop%20complex%20virtual%20staining%20models%20based%20on%20conditional%20Generative%20Adversarial%20Networks%2C%20but%20ignore%20the%20impact%20of%20adversarial%20loss%20on%20the%20quality%20of%20virtual%20staining.%20Furthermore%2C%20overlooking%20the%20issues%20of%20model%20evaluation%2C%20they%20claim%20improved%20performance%20based%20on%20metrics%20such%20as%20SSIM%20and%20PSNR%2C%20which%20are%20not%20sufficiently%20robust%20to%20evaluate%20the%20quality%20of%20virtually%20stained%20images.%20In%20this%20paper%2C%20we%20developed%20CSSP2P%20GAN%2C%20which%20we%20demonstrate%20to%20achieve%20heightened%20pathological%20fidelity%20through%20a%20blind%20pathological%20expert%20evaluation.%20Furthermore%2C%20while%20iteratively%20developing%20our%20model%2C%20we%20study%20the%20impact%20of%20the%20adversarial%20loss%20and%20demonstrate%20its%20crucial%20role%20in%20the%20quality%20of%20virtually%20stained%20images.%20Finally%2C%20while%20comparing%20our%20model%20with%20reference%20works%20in%20the%20field%2C%20we%20underscore%20the%20limitations%20of%20the%20currently%20used%20evaluation%20metrics%20and%20demonstrate%20the%20superior%20performance%20of%20CSSP2P%20GAN.&entry.1838667208=http%3A//arxiv.org/abs/2511.18946v1&entry.124074799=Read"},
{"title": "MonoMSK: Monocular 3D Musculoskeletal Dynamics Estimation", "author": "Farnoosh Koleini and Hongfei Xue and Ahmed Helmy and Pu Wang", "abstract": "Reconstructing biomechanically realistic 3D human motion - recovering both kinematics (motion) and kinetics (forces) - is a critical challenge. While marker-based systems are lab-bound and slow, popular monocular methods use oversimplified, anatomically inaccurate models (e.g., SMPL) and ignore physics, fundamentally limiting their biomechanical fidelity. In this work, we introduce MonoMSK, a hybrid framework that bridges data-driven learning and physics-based simulation for biomechanically realistic 3D human motion estimation from monocular video. MonoMSK jointly recovers both kinematics (motions) and kinetics (forces and torques) through an anatomically accurate musculoskeletal model. By integrating transformer-based inverse dynamics with differentiable forward kinematics and dynamics layers governed by ODE-based simulation, MonoMSK establishes a physics-regulated inverse-forward loop that enforces biomechanical causality and physical plausibility. A novel forward-inverse consistency loss further aligns motion reconstruction with the underlying kinetic reasoning. Experiments on BML-MoVi, BEDLAM, and OpenCap show that MonoMSK significantly outperforms state-of-the-art methods in kinematic accuracy, while for the first time enabling precise monocular kinetics estimation.", "link": "http://arxiv.org/abs/2511.19326v1", "date": "2025-11-24", "relevancy": 2.447, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6438}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.595}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoMSK%3A%20Monocular%203D%20Musculoskeletal%20Dynamics%20Estimation&body=Title%3A%20MonoMSK%3A%20Monocular%203D%20Musculoskeletal%20Dynamics%20Estimation%0AAuthor%3A%20Farnoosh%20Koleini%20and%20Hongfei%20Xue%20and%20Ahmed%20Helmy%20and%20Pu%20Wang%0AAbstract%3A%20Reconstructing%20biomechanically%20realistic%203D%20human%20motion%20-%20recovering%20both%20kinematics%20%28motion%29%20and%20kinetics%20%28forces%29%20-%20is%20a%20critical%20challenge.%20While%20marker-based%20systems%20are%20lab-bound%20and%20slow%2C%20popular%20monocular%20methods%20use%20oversimplified%2C%20anatomically%20inaccurate%20models%20%28e.g.%2C%20SMPL%29%20and%20ignore%20physics%2C%20fundamentally%20limiting%20their%20biomechanical%20fidelity.%20In%20this%20work%2C%20we%20introduce%20MonoMSK%2C%20a%20hybrid%20framework%20that%20bridges%20data-driven%20learning%20and%20physics-based%20simulation%20for%20biomechanically%20realistic%203D%20human%20motion%20estimation%20from%20monocular%20video.%20MonoMSK%20jointly%20recovers%20both%20kinematics%20%28motions%29%20and%20kinetics%20%28forces%20and%20torques%29%20through%20an%20anatomically%20accurate%20musculoskeletal%20model.%20By%20integrating%20transformer-based%20inverse%20dynamics%20with%20differentiable%20forward%20kinematics%20and%20dynamics%20layers%20governed%20by%20ODE-based%20simulation%2C%20MonoMSK%20establishes%20a%20physics-regulated%20inverse-forward%20loop%20that%20enforces%20biomechanical%20causality%20and%20physical%20plausibility.%20A%20novel%20forward-inverse%20consistency%20loss%20further%20aligns%20motion%20reconstruction%20with%20the%20underlying%20kinetic%20reasoning.%20Experiments%20on%20BML-MoVi%2C%20BEDLAM%2C%20and%20OpenCap%20show%20that%20MonoMSK%20significantly%20outperforms%20state-of-the-art%20methods%20in%20kinematic%20accuracy%2C%20while%20for%20the%20first%20time%20enabling%20precise%20monocular%20kinetics%20estimation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoMSK%253A%2520Monocular%25203D%2520Musculoskeletal%2520Dynamics%2520Estimation%26entry.906535625%3DFarnoosh%2520Koleini%2520and%2520Hongfei%2520Xue%2520and%2520Ahmed%2520Helmy%2520and%2520Pu%2520Wang%26entry.1292438233%3DReconstructing%2520biomechanically%2520realistic%25203D%2520human%2520motion%2520-%2520recovering%2520both%2520kinematics%2520%2528motion%2529%2520and%2520kinetics%2520%2528forces%2529%2520-%2520is%2520a%2520critical%2520challenge.%2520While%2520marker-based%2520systems%2520are%2520lab-bound%2520and%2520slow%252C%2520popular%2520monocular%2520methods%2520use%2520oversimplified%252C%2520anatomically%2520inaccurate%2520models%2520%2528e.g.%252C%2520SMPL%2529%2520and%2520ignore%2520physics%252C%2520fundamentally%2520limiting%2520their%2520biomechanical%2520fidelity.%2520In%2520this%2520work%252C%2520we%2520introduce%2520MonoMSK%252C%2520a%2520hybrid%2520framework%2520that%2520bridges%2520data-driven%2520learning%2520and%2520physics-based%2520simulation%2520for%2520biomechanically%2520realistic%25203D%2520human%2520motion%2520estimation%2520from%2520monocular%2520video.%2520MonoMSK%2520jointly%2520recovers%2520both%2520kinematics%2520%2528motions%2529%2520and%2520kinetics%2520%2528forces%2520and%2520torques%2529%2520through%2520an%2520anatomically%2520accurate%2520musculoskeletal%2520model.%2520By%2520integrating%2520transformer-based%2520inverse%2520dynamics%2520with%2520differentiable%2520forward%2520kinematics%2520and%2520dynamics%2520layers%2520governed%2520by%2520ODE-based%2520simulation%252C%2520MonoMSK%2520establishes%2520a%2520physics-regulated%2520inverse-forward%2520loop%2520that%2520enforces%2520biomechanical%2520causality%2520and%2520physical%2520plausibility.%2520A%2520novel%2520forward-inverse%2520consistency%2520loss%2520further%2520aligns%2520motion%2520reconstruction%2520with%2520the%2520underlying%2520kinetic%2520reasoning.%2520Experiments%2520on%2520BML-MoVi%252C%2520BEDLAM%252C%2520and%2520OpenCap%2520show%2520that%2520MonoMSK%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520in%2520kinematic%2520accuracy%252C%2520while%2520for%2520the%2520first%2520time%2520enabling%2520precise%2520monocular%2520kinetics%2520estimation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoMSK%3A%20Monocular%203D%20Musculoskeletal%20Dynamics%20Estimation&entry.906535625=Farnoosh%20Koleini%20and%20Hongfei%20Xue%20and%20Ahmed%20Helmy%20and%20Pu%20Wang&entry.1292438233=Reconstructing%20biomechanically%20realistic%203D%20human%20motion%20-%20recovering%20both%20kinematics%20%28motion%29%20and%20kinetics%20%28forces%29%20-%20is%20a%20critical%20challenge.%20While%20marker-based%20systems%20are%20lab-bound%20and%20slow%2C%20popular%20monocular%20methods%20use%20oversimplified%2C%20anatomically%20inaccurate%20models%20%28e.g.%2C%20SMPL%29%20and%20ignore%20physics%2C%20fundamentally%20limiting%20their%20biomechanical%20fidelity.%20In%20this%20work%2C%20we%20introduce%20MonoMSK%2C%20a%20hybrid%20framework%20that%20bridges%20data-driven%20learning%20and%20physics-based%20simulation%20for%20biomechanically%20realistic%203D%20human%20motion%20estimation%20from%20monocular%20video.%20MonoMSK%20jointly%20recovers%20both%20kinematics%20%28motions%29%20and%20kinetics%20%28forces%20and%20torques%29%20through%20an%20anatomically%20accurate%20musculoskeletal%20model.%20By%20integrating%20transformer-based%20inverse%20dynamics%20with%20differentiable%20forward%20kinematics%20and%20dynamics%20layers%20governed%20by%20ODE-based%20simulation%2C%20MonoMSK%20establishes%20a%20physics-regulated%20inverse-forward%20loop%20that%20enforces%20biomechanical%20causality%20and%20physical%20plausibility.%20A%20novel%20forward-inverse%20consistency%20loss%20further%20aligns%20motion%20reconstruction%20with%20the%20underlying%20kinetic%20reasoning.%20Experiments%20on%20BML-MoVi%2C%20BEDLAM%2C%20and%20OpenCap%20show%20that%20MonoMSK%20significantly%20outperforms%20state-of-the-art%20methods%20in%20kinematic%20accuracy%2C%20while%20for%20the%20first%20time%20enabling%20precise%20monocular%20kinetics%20estimation.&entry.1838667208=http%3A//arxiv.org/abs/2511.19326v1&entry.124074799=Read"},
{"title": "Neural Architecture Search for Quantum Autoencoders", "author": "Hibah Agha and Samuel Yen-Chi Chen and Huan-Hsin Tseng and Shinjae Yoo", "abstract": "In recent years, machine learning and deep learning have driven advances in domains such as image classification, speech recognition, and anomaly detection by leveraging multi-layer neural networks to model complex data. Simultaneously, quantum computing (QC) promises to address classically intractable problems via quantum parallelism, motivating research in quantum machine learning (QML). Among QML techniques, quantum autoencoders show promise for compressing high-dimensional quantum and classical data. However, designing effective quantum circuit architectures for quantum autoencoders remains challenging due to the complexity of selecting gates, arranging circuit layers, and tuning parameters.\n  This paper proposes a neural architecture search (NAS) framework that automates the design of quantum autoencoders using a genetic algorithm (GA). By systematically evolving variational quantum circuit (VQC) configurations, our method seeks to identify high-performing hybrid quantum-classical autoencoders for data reconstruction without becoming trapped in local minima. We demonstrate effectiveness on image datasets, highlighting the potential of quantum autoencoders for efficient feature extraction within a noise-prone, near-term quantum era. Our approach lays a foundation for broader application of genetic algorithms to quantum architecture search, aiming for a robust, automated method that can adapt to varied data and hardware constraints.", "link": "http://arxiv.org/abs/2511.19246v1", "date": "2025-11-24", "relevancy": 2.4196, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5208}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Architecture%20Search%20for%20Quantum%20Autoencoders&body=Title%3A%20Neural%20Architecture%20Search%20for%20Quantum%20Autoencoders%0AAuthor%3A%20Hibah%20Agha%20and%20Samuel%20Yen-Chi%20Chen%20and%20Huan-Hsin%20Tseng%20and%20Shinjae%20Yoo%0AAbstract%3A%20In%20recent%20years%2C%20machine%20learning%20and%20deep%20learning%20have%20driven%20advances%20in%20domains%20such%20as%20image%20classification%2C%20speech%20recognition%2C%20and%20anomaly%20detection%20by%20leveraging%20multi-layer%20neural%20networks%20to%20model%20complex%20data.%20Simultaneously%2C%20quantum%20computing%20%28QC%29%20promises%20to%20address%20classically%20intractable%20problems%20via%20quantum%20parallelism%2C%20motivating%20research%20in%20quantum%20machine%20learning%20%28QML%29.%20Among%20QML%20techniques%2C%20quantum%20autoencoders%20show%20promise%20for%20compressing%20high-dimensional%20quantum%20and%20classical%20data.%20However%2C%20designing%20effective%20quantum%20circuit%20architectures%20for%20quantum%20autoencoders%20remains%20challenging%20due%20to%20the%20complexity%20of%20selecting%20gates%2C%20arranging%20circuit%20layers%2C%20and%20tuning%20parameters.%0A%20%20This%20paper%20proposes%20a%20neural%20architecture%20search%20%28NAS%29%20framework%20that%20automates%20the%20design%20of%20quantum%20autoencoders%20using%20a%20genetic%20algorithm%20%28GA%29.%20By%20systematically%20evolving%20variational%20quantum%20circuit%20%28VQC%29%20configurations%2C%20our%20method%20seeks%20to%20identify%20high-performing%20hybrid%20quantum-classical%20autoencoders%20for%20data%20reconstruction%20without%20becoming%20trapped%20in%20local%20minima.%20We%20demonstrate%20effectiveness%20on%20image%20datasets%2C%20highlighting%20the%20potential%20of%20quantum%20autoencoders%20for%20efficient%20feature%20extraction%20within%20a%20noise-prone%2C%20near-term%20quantum%20era.%20Our%20approach%20lays%20a%20foundation%20for%20broader%20application%20of%20genetic%20algorithms%20to%20quantum%20architecture%20search%2C%20aiming%20for%20a%20robust%2C%20automated%20method%20that%20can%20adapt%20to%20varied%20data%20and%20hardware%20constraints.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Architecture%2520Search%2520for%2520Quantum%2520Autoencoders%26entry.906535625%3DHibah%2520Agha%2520and%2520Samuel%2520Yen-Chi%2520Chen%2520and%2520Huan-Hsin%2520Tseng%2520and%2520Shinjae%2520Yoo%26entry.1292438233%3DIn%2520recent%2520years%252C%2520machine%2520learning%2520and%2520deep%2520learning%2520have%2520driven%2520advances%2520in%2520domains%2520such%2520as%2520image%2520classification%252C%2520speech%2520recognition%252C%2520and%2520anomaly%2520detection%2520by%2520leveraging%2520multi-layer%2520neural%2520networks%2520to%2520model%2520complex%2520data.%2520Simultaneously%252C%2520quantum%2520computing%2520%2528QC%2529%2520promises%2520to%2520address%2520classically%2520intractable%2520problems%2520via%2520quantum%2520parallelism%252C%2520motivating%2520research%2520in%2520quantum%2520machine%2520learning%2520%2528QML%2529.%2520Among%2520QML%2520techniques%252C%2520quantum%2520autoencoders%2520show%2520promise%2520for%2520compressing%2520high-dimensional%2520quantum%2520and%2520classical%2520data.%2520However%252C%2520designing%2520effective%2520quantum%2520circuit%2520architectures%2520for%2520quantum%2520autoencoders%2520remains%2520challenging%2520due%2520to%2520the%2520complexity%2520of%2520selecting%2520gates%252C%2520arranging%2520circuit%2520layers%252C%2520and%2520tuning%2520parameters.%250A%2520%2520This%2520paper%2520proposes%2520a%2520neural%2520architecture%2520search%2520%2528NAS%2529%2520framework%2520that%2520automates%2520the%2520design%2520of%2520quantum%2520autoencoders%2520using%2520a%2520genetic%2520algorithm%2520%2528GA%2529.%2520By%2520systematically%2520evolving%2520variational%2520quantum%2520circuit%2520%2528VQC%2529%2520configurations%252C%2520our%2520method%2520seeks%2520to%2520identify%2520high-performing%2520hybrid%2520quantum-classical%2520autoencoders%2520for%2520data%2520reconstruction%2520without%2520becoming%2520trapped%2520in%2520local%2520minima.%2520We%2520demonstrate%2520effectiveness%2520on%2520image%2520datasets%252C%2520highlighting%2520the%2520potential%2520of%2520quantum%2520autoencoders%2520for%2520efficient%2520feature%2520extraction%2520within%2520a%2520noise-prone%252C%2520near-term%2520quantum%2520era.%2520Our%2520approach%2520lays%2520a%2520foundation%2520for%2520broader%2520application%2520of%2520genetic%2520algorithms%2520to%2520quantum%2520architecture%2520search%252C%2520aiming%2520for%2520a%2520robust%252C%2520automated%2520method%2520that%2520can%2520adapt%2520to%2520varied%2520data%2520and%2520hardware%2520constraints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Architecture%20Search%20for%20Quantum%20Autoencoders&entry.906535625=Hibah%20Agha%20and%20Samuel%20Yen-Chi%20Chen%20and%20Huan-Hsin%20Tseng%20and%20Shinjae%20Yoo&entry.1292438233=In%20recent%20years%2C%20machine%20learning%20and%20deep%20learning%20have%20driven%20advances%20in%20domains%20such%20as%20image%20classification%2C%20speech%20recognition%2C%20and%20anomaly%20detection%20by%20leveraging%20multi-layer%20neural%20networks%20to%20model%20complex%20data.%20Simultaneously%2C%20quantum%20computing%20%28QC%29%20promises%20to%20address%20classically%20intractable%20problems%20via%20quantum%20parallelism%2C%20motivating%20research%20in%20quantum%20machine%20learning%20%28QML%29.%20Among%20QML%20techniques%2C%20quantum%20autoencoders%20show%20promise%20for%20compressing%20high-dimensional%20quantum%20and%20classical%20data.%20However%2C%20designing%20effective%20quantum%20circuit%20architectures%20for%20quantum%20autoencoders%20remains%20challenging%20due%20to%20the%20complexity%20of%20selecting%20gates%2C%20arranging%20circuit%20layers%2C%20and%20tuning%20parameters.%0A%20%20This%20paper%20proposes%20a%20neural%20architecture%20search%20%28NAS%29%20framework%20that%20automates%20the%20design%20of%20quantum%20autoencoders%20using%20a%20genetic%20algorithm%20%28GA%29.%20By%20systematically%20evolving%20variational%20quantum%20circuit%20%28VQC%29%20configurations%2C%20our%20method%20seeks%20to%20identify%20high-performing%20hybrid%20quantum-classical%20autoencoders%20for%20data%20reconstruction%20without%20becoming%20trapped%20in%20local%20minima.%20We%20demonstrate%20effectiveness%20on%20image%20datasets%2C%20highlighting%20the%20potential%20of%20quantum%20autoencoders%20for%20efficient%20feature%20extraction%20within%20a%20noise-prone%2C%20near-term%20quantum%20era.%20Our%20approach%20lays%20a%20foundation%20for%20broader%20application%20of%20genetic%20algorithms%20to%20quantum%20architecture%20search%2C%20aiming%20for%20a%20robust%2C%20automated%20method%20that%20can%20adapt%20to%20varied%20data%20and%20hardware%20constraints.&entry.1838667208=http%3A//arxiv.org/abs/2511.19246v1&entry.124074799=Read"},
{"title": "CUPID: Generative 3D Reconstruction via Joint Object and Pose Modeling", "author": "Binbin Huang and Haobin Duan and Yiqun Zhao and Zibo Zhao and Yi Ma and Shenghua Gao", "abstract": "We introduce Cupid, a generative 3D reconstruction framework that jointly models the full distribution over both canonical objects and camera poses. Our two-stage flow-based model first generates a coarse 3D structure and 2D-3D correspondences to estimate the camera pose robustly. Conditioned on this pose, a refinement stage injects pixel-aligned image features directly into the generative process, marrying the rich prior of a generative model with the geometric fidelity of reconstruction. This strategy achieves exceptional faithfulness, outperforming state-of-the-art reconstruction methods by over 3 dB PSNR and 10% in Chamfer Distance. As a unified generative model that decouples the object and camera pose, Cupid naturally extends to multi-view and scene-level reconstruction tasks without requiring post-hoc optimization or fine-tuning.", "link": "http://arxiv.org/abs/2510.20776v2", "date": "2025-11-24", "relevancy": 2.4074, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6118}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5999}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CUPID%3A%20Generative%203D%20Reconstruction%20via%20Joint%20Object%20and%20Pose%20Modeling&body=Title%3A%20CUPID%3A%20Generative%203D%20Reconstruction%20via%20Joint%20Object%20and%20Pose%20Modeling%0AAuthor%3A%20Binbin%20Huang%20and%20Haobin%20Duan%20and%20Yiqun%20Zhao%20and%20Zibo%20Zhao%20and%20Yi%20Ma%20and%20Shenghua%20Gao%0AAbstract%3A%20We%20introduce%20Cupid%2C%20a%20generative%203D%20reconstruction%20framework%20that%20jointly%20models%20the%20full%20distribution%20over%20both%20canonical%20objects%20and%20camera%20poses.%20Our%20two-stage%20flow-based%20model%20first%20generates%20a%20coarse%203D%20structure%20and%202D-3D%20correspondences%20to%20estimate%20the%20camera%20pose%20robustly.%20Conditioned%20on%20this%20pose%2C%20a%20refinement%20stage%20injects%20pixel-aligned%20image%20features%20directly%20into%20the%20generative%20process%2C%20marrying%20the%20rich%20prior%20of%20a%20generative%20model%20with%20the%20geometric%20fidelity%20of%20reconstruction.%20This%20strategy%20achieves%20exceptional%20faithfulness%2C%20outperforming%20state-of-the-art%20reconstruction%20methods%20by%20over%203%20dB%20PSNR%20and%2010%25%20in%20Chamfer%20Distance.%20As%20a%20unified%20generative%20model%20that%20decouples%20the%20object%20and%20camera%20pose%2C%20Cupid%20naturally%20extends%20to%20multi-view%20and%20scene-level%20reconstruction%20tasks%20without%20requiring%20post-hoc%20optimization%20or%20fine-tuning.%0ALink%3A%20http%3A//arxiv.org/abs/2510.20776v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCUPID%253A%2520Generative%25203D%2520Reconstruction%2520via%2520Joint%2520Object%2520and%2520Pose%2520Modeling%26entry.906535625%3DBinbin%2520Huang%2520and%2520Haobin%2520Duan%2520and%2520Yiqun%2520Zhao%2520and%2520Zibo%2520Zhao%2520and%2520Yi%2520Ma%2520and%2520Shenghua%2520Gao%26entry.1292438233%3DWe%2520introduce%2520Cupid%252C%2520a%2520generative%25203D%2520reconstruction%2520framework%2520that%2520jointly%2520models%2520the%2520full%2520distribution%2520over%2520both%2520canonical%2520objects%2520and%2520camera%2520poses.%2520Our%2520two-stage%2520flow-based%2520model%2520first%2520generates%2520a%2520coarse%25203D%2520structure%2520and%25202D-3D%2520correspondences%2520to%2520estimate%2520the%2520camera%2520pose%2520robustly.%2520Conditioned%2520on%2520this%2520pose%252C%2520a%2520refinement%2520stage%2520injects%2520pixel-aligned%2520image%2520features%2520directly%2520into%2520the%2520generative%2520process%252C%2520marrying%2520the%2520rich%2520prior%2520of%2520a%2520generative%2520model%2520with%2520the%2520geometric%2520fidelity%2520of%2520reconstruction.%2520This%2520strategy%2520achieves%2520exceptional%2520faithfulness%252C%2520outperforming%2520state-of-the-art%2520reconstruction%2520methods%2520by%2520over%25203%2520dB%2520PSNR%2520and%252010%2525%2520in%2520Chamfer%2520Distance.%2520As%2520a%2520unified%2520generative%2520model%2520that%2520decouples%2520the%2520object%2520and%2520camera%2520pose%252C%2520Cupid%2520naturally%2520extends%2520to%2520multi-view%2520and%2520scene-level%2520reconstruction%2520tasks%2520without%2520requiring%2520post-hoc%2520optimization%2520or%2520fine-tuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.20776v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CUPID%3A%20Generative%203D%20Reconstruction%20via%20Joint%20Object%20and%20Pose%20Modeling&entry.906535625=Binbin%20Huang%20and%20Haobin%20Duan%20and%20Yiqun%20Zhao%20and%20Zibo%20Zhao%20and%20Yi%20Ma%20and%20Shenghua%20Gao&entry.1292438233=We%20introduce%20Cupid%2C%20a%20generative%203D%20reconstruction%20framework%20that%20jointly%20models%20the%20full%20distribution%20over%20both%20canonical%20objects%20and%20camera%20poses.%20Our%20two-stage%20flow-based%20model%20first%20generates%20a%20coarse%203D%20structure%20and%202D-3D%20correspondences%20to%20estimate%20the%20camera%20pose%20robustly.%20Conditioned%20on%20this%20pose%2C%20a%20refinement%20stage%20injects%20pixel-aligned%20image%20features%20directly%20into%20the%20generative%20process%2C%20marrying%20the%20rich%20prior%20of%20a%20generative%20model%20with%20the%20geometric%20fidelity%20of%20reconstruction.%20This%20strategy%20achieves%20exceptional%20faithfulness%2C%20outperforming%20state-of-the-art%20reconstruction%20methods%20by%20over%203%20dB%20PSNR%20and%2010%25%20in%20Chamfer%20Distance.%20As%20a%20unified%20generative%20model%20that%20decouples%20the%20object%20and%20camera%20pose%2C%20Cupid%20naturally%20extends%20to%20multi-view%20and%20scene-level%20reconstruction%20tasks%20without%20requiring%20post-hoc%20optimization%20or%20fine-tuning.&entry.1838667208=http%3A//arxiv.org/abs/2510.20776v2&entry.124074799=Read"},
{"title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective", "author": "Shimao Zhang and Zhejian Lai and Xiang Liu and Shuaijie She and Xiao Liu and Yeyun Gong and Shujian Huang and Jiajun Chen", "abstract": "Multilingual Alignment is an effective and representative paradigm to enhance LLMs' multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some research on language-specific neurons provides a new perspective to analyze and understand LLMs' mechanisms. However, we find that there are many neurons that are shared by multiple but not all languages and cannot be correctly classified. In this work, we propose a ternary classification methodology that categorizes neurons into three types, including language-specific neurons, language-related neurons, and general neurons. And we propose a corresponding identification algorithm to distinguish these different types of neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs' internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of ''Spontaneous Multilingual Alignment''. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights to better understand multilingual alignment and multilingual capabilities of LLMs.", "link": "http://arxiv.org/abs/2505.21505v2", "date": "2025-11-24", "relevancy": 2.3997, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4805}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4805}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20does%20Alignment%20Enhance%20LLMs%27%20Multilingual%20Capabilities%3F%20A%20Language%20Neurons%20Perspective&body=Title%3A%20How%20does%20Alignment%20Enhance%20LLMs%27%20Multilingual%20Capabilities%3F%20A%20Language%20Neurons%20Perspective%0AAuthor%3A%20Shimao%20Zhang%20and%20Zhejian%20Lai%20and%20Xiang%20Liu%20and%20Shuaijie%20She%20and%20Xiao%20Liu%20and%20Yeyun%20Gong%20and%20Shujian%20Huang%20and%20Jiajun%20Chen%0AAbstract%3A%20Multilingual%20Alignment%20is%20an%20effective%20and%20representative%20paradigm%20to%20enhance%20LLMs%27%20multilingual%20capabilities%2C%20which%20transfers%20the%20capabilities%20from%20the%20high-resource%20languages%20to%20the%20low-resource%20languages.%20Meanwhile%2C%20some%20research%20on%20language-specific%20neurons%20provides%20a%20new%20perspective%20to%20analyze%20and%20understand%20LLMs%27%20mechanisms.%20However%2C%20we%20find%20that%20there%20are%20many%20neurons%20that%20are%20shared%20by%20multiple%20but%20not%20all%20languages%20and%20cannot%20be%20correctly%20classified.%20In%20this%20work%2C%20we%20propose%20a%20ternary%20classification%20methodology%20that%20categorizes%20neurons%20into%20three%20types%2C%20including%20language-specific%20neurons%2C%20language-related%20neurons%2C%20and%20general%20neurons.%20And%20we%20propose%20a%20corresponding%20identification%20algorithm%20to%20distinguish%20these%20different%20types%20of%20neurons.%20Furthermore%2C%20based%20on%20the%20distributional%20characteristics%20of%20different%20types%20of%20neurons%2C%20we%20divide%20the%20LLMs%27%20internal%20process%20for%20multilingual%20inference%20into%20four%20parts%3A%20%281%29%20multilingual%20understanding%2C%20%282%29%20shared%20semantic%20space%20reasoning%2C%20%283%29%20multilingual%20output%20space%20transformation%2C%20and%20%284%29%20vocabulary%20space%20outputting.%20Additionally%2C%20we%20systematically%20analyze%20the%20models%20before%20and%20after%20alignment%20with%20a%20focus%20on%20different%20types%20of%20neurons.%20We%20also%20analyze%20the%20phenomenon%20of%20%27%27Spontaneous%20Multilingual%20Alignment%27%27.%20Overall%2C%20our%20work%20conducts%20a%20comprehensive%20investigation%20based%20on%20different%20types%20of%20neurons%2C%20providing%20empirical%20results%20and%20valuable%20insights%20to%20better%20understand%20multilingual%20alignment%20and%20multilingual%20capabilities%20of%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2505.21505v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520does%2520Alignment%2520Enhance%2520LLMs%2527%2520Multilingual%2520Capabilities%253F%2520A%2520Language%2520Neurons%2520Perspective%26entry.906535625%3DShimao%2520Zhang%2520and%2520Zhejian%2520Lai%2520and%2520Xiang%2520Liu%2520and%2520Shuaijie%2520She%2520and%2520Xiao%2520Liu%2520and%2520Yeyun%2520Gong%2520and%2520Shujian%2520Huang%2520and%2520Jiajun%2520Chen%26entry.1292438233%3DMultilingual%2520Alignment%2520is%2520an%2520effective%2520and%2520representative%2520paradigm%2520to%2520enhance%2520LLMs%2527%2520multilingual%2520capabilities%252C%2520which%2520transfers%2520the%2520capabilities%2520from%2520the%2520high-resource%2520languages%2520to%2520the%2520low-resource%2520languages.%2520Meanwhile%252C%2520some%2520research%2520on%2520language-specific%2520neurons%2520provides%2520a%2520new%2520perspective%2520to%2520analyze%2520and%2520understand%2520LLMs%2527%2520mechanisms.%2520However%252C%2520we%2520find%2520that%2520there%2520are%2520many%2520neurons%2520that%2520are%2520shared%2520by%2520multiple%2520but%2520not%2520all%2520languages%2520and%2520cannot%2520be%2520correctly%2520classified.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520ternary%2520classification%2520methodology%2520that%2520categorizes%2520neurons%2520into%2520three%2520types%252C%2520including%2520language-specific%2520neurons%252C%2520language-related%2520neurons%252C%2520and%2520general%2520neurons.%2520And%2520we%2520propose%2520a%2520corresponding%2520identification%2520algorithm%2520to%2520distinguish%2520these%2520different%2520types%2520of%2520neurons.%2520Furthermore%252C%2520based%2520on%2520the%2520distributional%2520characteristics%2520of%2520different%2520types%2520of%2520neurons%252C%2520we%2520divide%2520the%2520LLMs%2527%2520internal%2520process%2520for%2520multilingual%2520inference%2520into%2520four%2520parts%253A%2520%25281%2529%2520multilingual%2520understanding%252C%2520%25282%2529%2520shared%2520semantic%2520space%2520reasoning%252C%2520%25283%2529%2520multilingual%2520output%2520space%2520transformation%252C%2520and%2520%25284%2529%2520vocabulary%2520space%2520outputting.%2520Additionally%252C%2520we%2520systematically%2520analyze%2520the%2520models%2520before%2520and%2520after%2520alignment%2520with%2520a%2520focus%2520on%2520different%2520types%2520of%2520neurons.%2520We%2520also%2520analyze%2520the%2520phenomenon%2520of%2520%2527%2527Spontaneous%2520Multilingual%2520Alignment%2527%2527.%2520Overall%252C%2520our%2520work%2520conducts%2520a%2520comprehensive%2520investigation%2520based%2520on%2520different%2520types%2520of%2520neurons%252C%2520providing%2520empirical%2520results%2520and%2520valuable%2520insights%2520to%2520better%2520understand%2520multilingual%2520alignment%2520and%2520multilingual%2520capabilities%2520of%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21505v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20does%20Alignment%20Enhance%20LLMs%27%20Multilingual%20Capabilities%3F%20A%20Language%20Neurons%20Perspective&entry.906535625=Shimao%20Zhang%20and%20Zhejian%20Lai%20and%20Xiang%20Liu%20and%20Shuaijie%20She%20and%20Xiao%20Liu%20and%20Yeyun%20Gong%20and%20Shujian%20Huang%20and%20Jiajun%20Chen&entry.1292438233=Multilingual%20Alignment%20is%20an%20effective%20and%20representative%20paradigm%20to%20enhance%20LLMs%27%20multilingual%20capabilities%2C%20which%20transfers%20the%20capabilities%20from%20the%20high-resource%20languages%20to%20the%20low-resource%20languages.%20Meanwhile%2C%20some%20research%20on%20language-specific%20neurons%20provides%20a%20new%20perspective%20to%20analyze%20and%20understand%20LLMs%27%20mechanisms.%20However%2C%20we%20find%20that%20there%20are%20many%20neurons%20that%20are%20shared%20by%20multiple%20but%20not%20all%20languages%20and%20cannot%20be%20correctly%20classified.%20In%20this%20work%2C%20we%20propose%20a%20ternary%20classification%20methodology%20that%20categorizes%20neurons%20into%20three%20types%2C%20including%20language-specific%20neurons%2C%20language-related%20neurons%2C%20and%20general%20neurons.%20And%20we%20propose%20a%20corresponding%20identification%20algorithm%20to%20distinguish%20these%20different%20types%20of%20neurons.%20Furthermore%2C%20based%20on%20the%20distributional%20characteristics%20of%20different%20types%20of%20neurons%2C%20we%20divide%20the%20LLMs%27%20internal%20process%20for%20multilingual%20inference%20into%20four%20parts%3A%20%281%29%20multilingual%20understanding%2C%20%282%29%20shared%20semantic%20space%20reasoning%2C%20%283%29%20multilingual%20output%20space%20transformation%2C%20and%20%284%29%20vocabulary%20space%20outputting.%20Additionally%2C%20we%20systematically%20analyze%20the%20models%20before%20and%20after%20alignment%20with%20a%20focus%20on%20different%20types%20of%20neurons.%20We%20also%20analyze%20the%20phenomenon%20of%20%27%27Spontaneous%20Multilingual%20Alignment%27%27.%20Overall%2C%20our%20work%20conducts%20a%20comprehensive%20investigation%20based%20on%20different%20types%20of%20neurons%2C%20providing%20empirical%20results%20and%20valuable%20insights%20to%20better%20understand%20multilingual%20alignment%20and%20multilingual%20capabilities%20of%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2505.21505v2&entry.124074799=Read"},
{"title": "UISearch: Graph-Based Embeddings for Multimodal Enterprise UI Screenshots Retrieval", "author": "Maroun Ayli and Youssef Bakouny and Tushar Sharma and Nader Jalloul and Hani Seifeddine and Rima Kilany", "abstract": "Enterprise software companies maintain thousands of user interface screens across products and versions, creating critical challenges for design consistency, pattern discovery, and compliance check. Existing approaches rely on visual similarity or text semantics, lacking explicit modeling of structural properties fundamental to user interface (UI) composition. We present a novel graph-based representation that converts UI screenshots into attributed graphs encoding hierarchical relationships and spatial arrangements, potentially generalizable to document layouts, architectural diagrams, and other structured visual domains. A contrastive graph autoencoder learns embeddings preserving multi-level similarity across visual, structural, and semantic properties. The comprehensive analysis demonstrates that our structural embeddings achieve better discriminative power than state-of-the-art Vision Encoders, representing a fundamental advance in the expressiveness of the UI representation. We implement this representation in UISearch, a multi-modal search framework that combines structural embeddings with semantic search through a composable query language. On 20,396 financial software UIs, UISearch achieves 0.92 Top-5 accuracy with 47.5ms median latency (P95: 124ms), scaling to 20,000+ screens. The hybrid indexing architecture enables complex queries and supports fine-grained UI distinction impossible with vision-only approaches.", "link": "http://arxiv.org/abs/2511.19380v1", "date": "2025-11-24", "relevancy": 2.392, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4834}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UISearch%3A%20Graph-Based%20Embeddings%20for%20Multimodal%20Enterprise%20UI%20Screenshots%20Retrieval&body=Title%3A%20UISearch%3A%20Graph-Based%20Embeddings%20for%20Multimodal%20Enterprise%20UI%20Screenshots%20Retrieval%0AAuthor%3A%20Maroun%20Ayli%20and%20Youssef%20Bakouny%20and%20Tushar%20Sharma%20and%20Nader%20Jalloul%20and%20Hani%20Seifeddine%20and%20Rima%20Kilany%0AAbstract%3A%20Enterprise%20software%20companies%20maintain%20thousands%20of%20user%20interface%20screens%20across%20products%20and%20versions%2C%20creating%20critical%20challenges%20for%20design%20consistency%2C%20pattern%20discovery%2C%20and%20compliance%20check.%20Existing%20approaches%20rely%20on%20visual%20similarity%20or%20text%20semantics%2C%20lacking%20explicit%20modeling%20of%20structural%20properties%20fundamental%20to%20user%20interface%20%28UI%29%20composition.%20We%20present%20a%20novel%20graph-based%20representation%20that%20converts%20UI%20screenshots%20into%20attributed%20graphs%20encoding%20hierarchical%20relationships%20and%20spatial%20arrangements%2C%20potentially%20generalizable%20to%20document%20layouts%2C%20architectural%20diagrams%2C%20and%20other%20structured%20visual%20domains.%20A%20contrastive%20graph%20autoencoder%20learns%20embeddings%20preserving%20multi-level%20similarity%20across%20visual%2C%20structural%2C%20and%20semantic%20properties.%20The%20comprehensive%20analysis%20demonstrates%20that%20our%20structural%20embeddings%20achieve%20better%20discriminative%20power%20than%20state-of-the-art%20Vision%20Encoders%2C%20representing%20a%20fundamental%20advance%20in%20the%20expressiveness%20of%20the%20UI%20representation.%20We%20implement%20this%20representation%20in%20UISearch%2C%20a%20multi-modal%20search%20framework%20that%20combines%20structural%20embeddings%20with%20semantic%20search%20through%20a%20composable%20query%20language.%20On%2020%2C396%20financial%20software%20UIs%2C%20UISearch%20achieves%200.92%20Top-5%20accuracy%20with%2047.5ms%20median%20latency%20%28P95%3A%20124ms%29%2C%20scaling%20to%2020%2C000%2B%20screens.%20The%20hybrid%20indexing%20architecture%20enables%20complex%20queries%20and%20supports%20fine-grained%20UI%20distinction%20impossible%20with%20vision-only%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUISearch%253A%2520Graph-Based%2520Embeddings%2520for%2520Multimodal%2520Enterprise%2520UI%2520Screenshots%2520Retrieval%26entry.906535625%3DMaroun%2520Ayli%2520and%2520Youssef%2520Bakouny%2520and%2520Tushar%2520Sharma%2520and%2520Nader%2520Jalloul%2520and%2520Hani%2520Seifeddine%2520and%2520Rima%2520Kilany%26entry.1292438233%3DEnterprise%2520software%2520companies%2520maintain%2520thousands%2520of%2520user%2520interface%2520screens%2520across%2520products%2520and%2520versions%252C%2520creating%2520critical%2520challenges%2520for%2520design%2520consistency%252C%2520pattern%2520discovery%252C%2520and%2520compliance%2520check.%2520Existing%2520approaches%2520rely%2520on%2520visual%2520similarity%2520or%2520text%2520semantics%252C%2520lacking%2520explicit%2520modeling%2520of%2520structural%2520properties%2520fundamental%2520to%2520user%2520interface%2520%2528UI%2529%2520composition.%2520We%2520present%2520a%2520novel%2520graph-based%2520representation%2520that%2520converts%2520UI%2520screenshots%2520into%2520attributed%2520graphs%2520encoding%2520hierarchical%2520relationships%2520and%2520spatial%2520arrangements%252C%2520potentially%2520generalizable%2520to%2520document%2520layouts%252C%2520architectural%2520diagrams%252C%2520and%2520other%2520structured%2520visual%2520domains.%2520A%2520contrastive%2520graph%2520autoencoder%2520learns%2520embeddings%2520preserving%2520multi-level%2520similarity%2520across%2520visual%252C%2520structural%252C%2520and%2520semantic%2520properties.%2520The%2520comprehensive%2520analysis%2520demonstrates%2520that%2520our%2520structural%2520embeddings%2520achieve%2520better%2520discriminative%2520power%2520than%2520state-of-the-art%2520Vision%2520Encoders%252C%2520representing%2520a%2520fundamental%2520advance%2520in%2520the%2520expressiveness%2520of%2520the%2520UI%2520representation.%2520We%2520implement%2520this%2520representation%2520in%2520UISearch%252C%2520a%2520multi-modal%2520search%2520framework%2520that%2520combines%2520structural%2520embeddings%2520with%2520semantic%2520search%2520through%2520a%2520composable%2520query%2520language.%2520On%252020%252C396%2520financial%2520software%2520UIs%252C%2520UISearch%2520achieves%25200.92%2520Top-5%2520accuracy%2520with%252047.5ms%2520median%2520latency%2520%2528P95%253A%2520124ms%2529%252C%2520scaling%2520to%252020%252C000%252B%2520screens.%2520The%2520hybrid%2520indexing%2520architecture%2520enables%2520complex%2520queries%2520and%2520supports%2520fine-grained%2520UI%2520distinction%2520impossible%2520with%2520vision-only%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UISearch%3A%20Graph-Based%20Embeddings%20for%20Multimodal%20Enterprise%20UI%20Screenshots%20Retrieval&entry.906535625=Maroun%20Ayli%20and%20Youssef%20Bakouny%20and%20Tushar%20Sharma%20and%20Nader%20Jalloul%20and%20Hani%20Seifeddine%20and%20Rima%20Kilany&entry.1292438233=Enterprise%20software%20companies%20maintain%20thousands%20of%20user%20interface%20screens%20across%20products%20and%20versions%2C%20creating%20critical%20challenges%20for%20design%20consistency%2C%20pattern%20discovery%2C%20and%20compliance%20check.%20Existing%20approaches%20rely%20on%20visual%20similarity%20or%20text%20semantics%2C%20lacking%20explicit%20modeling%20of%20structural%20properties%20fundamental%20to%20user%20interface%20%28UI%29%20composition.%20We%20present%20a%20novel%20graph-based%20representation%20that%20converts%20UI%20screenshots%20into%20attributed%20graphs%20encoding%20hierarchical%20relationships%20and%20spatial%20arrangements%2C%20potentially%20generalizable%20to%20document%20layouts%2C%20architectural%20diagrams%2C%20and%20other%20structured%20visual%20domains.%20A%20contrastive%20graph%20autoencoder%20learns%20embeddings%20preserving%20multi-level%20similarity%20across%20visual%2C%20structural%2C%20and%20semantic%20properties.%20The%20comprehensive%20analysis%20demonstrates%20that%20our%20structural%20embeddings%20achieve%20better%20discriminative%20power%20than%20state-of-the-art%20Vision%20Encoders%2C%20representing%20a%20fundamental%20advance%20in%20the%20expressiveness%20of%20the%20UI%20representation.%20We%20implement%20this%20representation%20in%20UISearch%2C%20a%20multi-modal%20search%20framework%20that%20combines%20structural%20embeddings%20with%20semantic%20search%20through%20a%20composable%20query%20language.%20On%2020%2C396%20financial%20software%20UIs%2C%20UISearch%20achieves%200.92%20Top-5%20accuracy%20with%2047.5ms%20median%20latency%20%28P95%3A%20124ms%29%2C%20scaling%20to%2020%2C000%2B%20screens.%20The%20hybrid%20indexing%20architecture%20enables%20complex%20queries%20and%20supports%20fine-grained%20UI%20distinction%20impossible%20with%20vision-only%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2511.19380v1&entry.124074799=Read"},
{"title": "Resolving Node Identifiability in Graph Neural Processes via Laplacian Spectral Encodings", "author": "Zimo Yan and Zheng Xie and Chang Liu and Yuan Wang", "abstract": "Message passing graph neural networks are widely used for learning on graphs, yet their expressive power is limited by the one-dimensional Weisfeiler-Lehman test and can fail to distinguish structurally different nodes. We provide rigorous theory for a Laplacian positional encoding that is invariant to eigenvector sign flips and to basis rotations within eigenspaces. We prove that this encoding yields node identifiability from a constant number of observations and establishes a sample-complexity separation from architectures constrained by the Weisfeiler-Lehman test. The analysis combines a monotone link between shortest-path and diffusion distance, spectral trilateration with a constant set of anchors, and quantitative spectral injectivity with logarithmic embedding size. As an instantiation, pairing this encoding with a neural-process style decoder yields significant gains on a drug-drug interaction task on chemical graphs, improving both the area under the ROC curve and the F1 score and demonstrating the practical benefits of resolving theoretical expressiveness limitations with principled positional information.", "link": "http://arxiv.org/abs/2511.19037v1", "date": "2025-11-24", "relevancy": 2.3903, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5009}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4667}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resolving%20Node%20Identifiability%20in%20Graph%20Neural%20Processes%20via%20Laplacian%20Spectral%20Encodings&body=Title%3A%20Resolving%20Node%20Identifiability%20in%20Graph%20Neural%20Processes%20via%20Laplacian%20Spectral%20Encodings%0AAuthor%3A%20Zimo%20Yan%20and%20Zheng%20Xie%20and%20Chang%20Liu%20and%20Yuan%20Wang%0AAbstract%3A%20Message%20passing%20graph%20neural%20networks%20are%20widely%20used%20for%20learning%20on%20graphs%2C%20yet%20their%20expressive%20power%20is%20limited%20by%20the%20one-dimensional%20Weisfeiler-Lehman%20test%20and%20can%20fail%20to%20distinguish%20structurally%20different%20nodes.%20We%20provide%20rigorous%20theory%20for%20a%20Laplacian%20positional%20encoding%20that%20is%20invariant%20to%20eigenvector%20sign%20flips%20and%20to%20basis%20rotations%20within%20eigenspaces.%20We%20prove%20that%20this%20encoding%20yields%20node%20identifiability%20from%20a%20constant%20number%20of%20observations%20and%20establishes%20a%20sample-complexity%20separation%20from%20architectures%20constrained%20by%20the%20Weisfeiler-Lehman%20test.%20The%20analysis%20combines%20a%20monotone%20link%20between%20shortest-path%20and%20diffusion%20distance%2C%20spectral%20trilateration%20with%20a%20constant%20set%20of%20anchors%2C%20and%20quantitative%20spectral%20injectivity%20with%20logarithmic%20embedding%20size.%20As%20an%20instantiation%2C%20pairing%20this%20encoding%20with%20a%20neural-process%20style%20decoder%20yields%20significant%20gains%20on%20a%20drug-drug%20interaction%20task%20on%20chemical%20graphs%2C%20improving%20both%20the%20area%20under%20the%20ROC%20curve%20and%20the%20F1%20score%20and%20demonstrating%20the%20practical%20benefits%20of%20resolving%20theoretical%20expressiveness%20limitations%20with%20principled%20positional%20information.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResolving%2520Node%2520Identifiability%2520in%2520Graph%2520Neural%2520Processes%2520via%2520Laplacian%2520Spectral%2520Encodings%26entry.906535625%3DZimo%2520Yan%2520and%2520Zheng%2520Xie%2520and%2520Chang%2520Liu%2520and%2520Yuan%2520Wang%26entry.1292438233%3DMessage%2520passing%2520graph%2520neural%2520networks%2520are%2520widely%2520used%2520for%2520learning%2520on%2520graphs%252C%2520yet%2520their%2520expressive%2520power%2520is%2520limited%2520by%2520the%2520one-dimensional%2520Weisfeiler-Lehman%2520test%2520and%2520can%2520fail%2520to%2520distinguish%2520structurally%2520different%2520nodes.%2520We%2520provide%2520rigorous%2520theory%2520for%2520a%2520Laplacian%2520positional%2520encoding%2520that%2520is%2520invariant%2520to%2520eigenvector%2520sign%2520flips%2520and%2520to%2520basis%2520rotations%2520within%2520eigenspaces.%2520We%2520prove%2520that%2520this%2520encoding%2520yields%2520node%2520identifiability%2520from%2520a%2520constant%2520number%2520of%2520observations%2520and%2520establishes%2520a%2520sample-complexity%2520separation%2520from%2520architectures%2520constrained%2520by%2520the%2520Weisfeiler-Lehman%2520test.%2520The%2520analysis%2520combines%2520a%2520monotone%2520link%2520between%2520shortest-path%2520and%2520diffusion%2520distance%252C%2520spectral%2520trilateration%2520with%2520a%2520constant%2520set%2520of%2520anchors%252C%2520and%2520quantitative%2520spectral%2520injectivity%2520with%2520logarithmic%2520embedding%2520size.%2520As%2520an%2520instantiation%252C%2520pairing%2520this%2520encoding%2520with%2520a%2520neural-process%2520style%2520decoder%2520yields%2520significant%2520gains%2520on%2520a%2520drug-drug%2520interaction%2520task%2520on%2520chemical%2520graphs%252C%2520improving%2520both%2520the%2520area%2520under%2520the%2520ROC%2520curve%2520and%2520the%2520F1%2520score%2520and%2520demonstrating%2520the%2520practical%2520benefits%2520of%2520resolving%2520theoretical%2520expressiveness%2520limitations%2520with%2520principled%2520positional%2520information.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resolving%20Node%20Identifiability%20in%20Graph%20Neural%20Processes%20via%20Laplacian%20Spectral%20Encodings&entry.906535625=Zimo%20Yan%20and%20Zheng%20Xie%20and%20Chang%20Liu%20and%20Yuan%20Wang&entry.1292438233=Message%20passing%20graph%20neural%20networks%20are%20widely%20used%20for%20learning%20on%20graphs%2C%20yet%20their%20expressive%20power%20is%20limited%20by%20the%20one-dimensional%20Weisfeiler-Lehman%20test%20and%20can%20fail%20to%20distinguish%20structurally%20different%20nodes.%20We%20provide%20rigorous%20theory%20for%20a%20Laplacian%20positional%20encoding%20that%20is%20invariant%20to%20eigenvector%20sign%20flips%20and%20to%20basis%20rotations%20within%20eigenspaces.%20We%20prove%20that%20this%20encoding%20yields%20node%20identifiability%20from%20a%20constant%20number%20of%20observations%20and%20establishes%20a%20sample-complexity%20separation%20from%20architectures%20constrained%20by%20the%20Weisfeiler-Lehman%20test.%20The%20analysis%20combines%20a%20monotone%20link%20between%20shortest-path%20and%20diffusion%20distance%2C%20spectral%20trilateration%20with%20a%20constant%20set%20of%20anchors%2C%20and%20quantitative%20spectral%20injectivity%20with%20logarithmic%20embedding%20size.%20As%20an%20instantiation%2C%20pairing%20this%20encoding%20with%20a%20neural-process%20style%20decoder%20yields%20significant%20gains%20on%20a%20drug-drug%20interaction%20task%20on%20chemical%20graphs%2C%20improving%20both%20the%20area%20under%20the%20ROC%20curve%20and%20the%20F1%20score%20and%20demonstrating%20the%20practical%20benefits%20of%20resolving%20theoretical%20expressiveness%20limitations%20with%20principled%20positional%20information.&entry.1838667208=http%3A//arxiv.org/abs/2511.19037v1&entry.124074799=Read"},
{"title": "HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models", "author": "Zhiguang Lu and Qianqian Xu and Peisong Wen and Siran Dai and Qingming Huang", "abstract": "Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.", "link": "http://arxiv.org/abs/2511.12547v2", "date": "2025-11-24", "relevancy": 2.384, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6135}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5961}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiGFA%3A%20Hierarchical%20Guidance%20for%20Fine-grained%20Data%20Augmentation%20with%20Diffusion%20Models&body=Title%3A%20HiGFA%3A%20Hierarchical%20Guidance%20for%20Fine-grained%20Data%20Augmentation%20with%20Diffusion%20Models%0AAuthor%3A%20Zhiguang%20Lu%20and%20Qianqian%20Xu%20and%20Peisong%20Wen%20and%20Siran%20Dai%20and%20Qingming%20Huang%0AAbstract%3A%20Generative%20diffusion%20models%20show%20promise%20for%20data%20augmentation.%20However%2C%20applying%20them%20to%20fine-grained%20tasks%20presents%20a%20significant%20challenge%3A%20ensuring%20synthetic%20images%20accurately%20capture%20the%20subtle%2C%20category-defining%20features%20critical%20for%20high%20fidelity.%20Standard%20approaches%2C%20such%20as%20text-based%20Classifier-Free%20Guidance%20%28CFG%29%2C%20often%20lack%20the%20required%20specificity%2C%20potentially%20generating%20misleading%20examples%20that%20degrade%20fine-grained%20classifier%20performance.%20To%20address%20this%2C%20we%20propose%20Hierarchically%20Guided%20Fine-grained%20Augmentation%20%28HiGFA%29.%20HiGFA%20leverages%20the%20temporal%20dynamics%20of%20the%20diffusion%20sampling%20process.%20It%20employs%20strong%20text%20and%20transformed%20contour%20guidance%20with%20fixed%20strengths%20in%20the%20early-to-mid%20sampling%20stages%20to%20establish%20overall%20scene%2C%20style%2C%20and%20structure.%20In%20the%20final%20sampling%20stages%2C%20HiGFA%20activates%20a%20specialized%20fine-grained%20classifier%20guidance%20and%20dynamically%20modulates%20the%20strength%20of%20all%20guidance%20signals%20based%20on%20prediction%20confidence.%20This%20hierarchical%2C%20confidence-driven%20orchestration%20enables%20HiGFA%20to%20generate%20diverse%20yet%20faithful%20synthetic%20images%20by%20intelligently%20balancing%20global%20structure%20formation%20with%20precise%20detail%20refinement.%20Experiments%20on%20several%20FGVC%20datasets%20demonstrate%20the%20effectiveness%20of%20HiGFA.%0ALink%3A%20http%3A//arxiv.org/abs/2511.12547v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiGFA%253A%2520Hierarchical%2520Guidance%2520for%2520Fine-grained%2520Data%2520Augmentation%2520with%2520Diffusion%2520Models%26entry.906535625%3DZhiguang%2520Lu%2520and%2520Qianqian%2520Xu%2520and%2520Peisong%2520Wen%2520and%2520Siran%2520Dai%2520and%2520Qingming%2520Huang%26entry.1292438233%3DGenerative%2520diffusion%2520models%2520show%2520promise%2520for%2520data%2520augmentation.%2520However%252C%2520applying%2520them%2520to%2520fine-grained%2520tasks%2520presents%2520a%2520significant%2520challenge%253A%2520ensuring%2520synthetic%2520images%2520accurately%2520capture%2520the%2520subtle%252C%2520category-defining%2520features%2520critical%2520for%2520high%2520fidelity.%2520Standard%2520approaches%252C%2520such%2520as%2520text-based%2520Classifier-Free%2520Guidance%2520%2528CFG%2529%252C%2520often%2520lack%2520the%2520required%2520specificity%252C%2520potentially%2520generating%2520misleading%2520examples%2520that%2520degrade%2520fine-grained%2520classifier%2520performance.%2520To%2520address%2520this%252C%2520we%2520propose%2520Hierarchically%2520Guided%2520Fine-grained%2520Augmentation%2520%2528HiGFA%2529.%2520HiGFA%2520leverages%2520the%2520temporal%2520dynamics%2520of%2520the%2520diffusion%2520sampling%2520process.%2520It%2520employs%2520strong%2520text%2520and%2520transformed%2520contour%2520guidance%2520with%2520fixed%2520strengths%2520in%2520the%2520early-to-mid%2520sampling%2520stages%2520to%2520establish%2520overall%2520scene%252C%2520style%252C%2520and%2520structure.%2520In%2520the%2520final%2520sampling%2520stages%252C%2520HiGFA%2520activates%2520a%2520specialized%2520fine-grained%2520classifier%2520guidance%2520and%2520dynamically%2520modulates%2520the%2520strength%2520of%2520all%2520guidance%2520signals%2520based%2520on%2520prediction%2520confidence.%2520This%2520hierarchical%252C%2520confidence-driven%2520orchestration%2520enables%2520HiGFA%2520to%2520generate%2520diverse%2520yet%2520faithful%2520synthetic%2520images%2520by%2520intelligently%2520balancing%2520global%2520structure%2520formation%2520with%2520precise%2520detail%2520refinement.%2520Experiments%2520on%2520several%2520FGVC%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520HiGFA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12547v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiGFA%3A%20Hierarchical%20Guidance%20for%20Fine-grained%20Data%20Augmentation%20with%20Diffusion%20Models&entry.906535625=Zhiguang%20Lu%20and%20Qianqian%20Xu%20and%20Peisong%20Wen%20and%20Siran%20Dai%20and%20Qingming%20Huang&entry.1292438233=Generative%20diffusion%20models%20show%20promise%20for%20data%20augmentation.%20However%2C%20applying%20them%20to%20fine-grained%20tasks%20presents%20a%20significant%20challenge%3A%20ensuring%20synthetic%20images%20accurately%20capture%20the%20subtle%2C%20category-defining%20features%20critical%20for%20high%20fidelity.%20Standard%20approaches%2C%20such%20as%20text-based%20Classifier-Free%20Guidance%20%28CFG%29%2C%20often%20lack%20the%20required%20specificity%2C%20potentially%20generating%20misleading%20examples%20that%20degrade%20fine-grained%20classifier%20performance.%20To%20address%20this%2C%20we%20propose%20Hierarchically%20Guided%20Fine-grained%20Augmentation%20%28HiGFA%29.%20HiGFA%20leverages%20the%20temporal%20dynamics%20of%20the%20diffusion%20sampling%20process.%20It%20employs%20strong%20text%20and%20transformed%20contour%20guidance%20with%20fixed%20strengths%20in%20the%20early-to-mid%20sampling%20stages%20to%20establish%20overall%20scene%2C%20style%2C%20and%20structure.%20In%20the%20final%20sampling%20stages%2C%20HiGFA%20activates%20a%20specialized%20fine-grained%20classifier%20guidance%20and%20dynamically%20modulates%20the%20strength%20of%20all%20guidance%20signals%20based%20on%20prediction%20confidence.%20This%20hierarchical%2C%20confidence-driven%20orchestration%20enables%20HiGFA%20to%20generate%20diverse%20yet%20faithful%20synthetic%20images%20by%20intelligently%20balancing%20global%20structure%20formation%20with%20precise%20detail%20refinement.%20Experiments%20on%20several%20FGVC%20datasets%20demonstrate%20the%20effectiveness%20of%20HiGFA.&entry.1838667208=http%3A//arxiv.org/abs/2511.12547v2&entry.124074799=Read"},
{"title": "Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data", "author": "Dominik Luszczynski", "abstract": "A common method of attacking deep learning models is through adversarial attacks, which occur when an attacker specifically modifies the input of a model to produce an incorrect result. Adversarial attacks have been deeply investigated in the image domain; however, there is less research in the time-series domain and very little for forecasting financial data. To address these concerns, this study aims to build upon previous research on adversarial attacks for time-series data by introducing two new slope-based methods aimed to alter the trends of the predicted stock forecast generated by an N-HiTS model. Compared to the normal N-HiTS predictions, the two new slope-based methods, the General Slope Attack and Least-Squares Slope Attack, can manipulate N-HiTS predictions by doubling the slope. These new slope attacks can bypass standard security mechanisms, such as a discriminator that filters real and perturbed inputs, reducing a 4-layered CNN's specificity to 28% and accuracy to 57%. Furthermore, the slope based methods were incorporated into a GAN architecture as a means of generating realistic synthetic data, while simultaneously fooling the model. Finally, this paper also proposes a sample malware designed to inject an adversarial attack in the model inference library, proving that ML-security research should not only focus on making the model safe, but also securing the entire pipeline.", "link": "http://arxiv.org/abs/2511.19330v1", "date": "2025-11-24", "relevancy": 2.3822, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4876}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4714}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Targeted%20Manipulation%3A%20Slope-Based%20Attacks%20on%20Financial%20Time-Series%20Data&body=Title%3A%20Targeted%20Manipulation%3A%20Slope-Based%20Attacks%20on%20Financial%20Time-Series%20Data%0AAuthor%3A%20Dominik%20Luszczynski%0AAbstract%3A%20A%20common%20method%20of%20attacking%20deep%20learning%20models%20is%20through%20adversarial%20attacks%2C%20which%20occur%20when%20an%20attacker%20specifically%20modifies%20the%20input%20of%20a%20model%20to%20produce%20an%20incorrect%20result.%20Adversarial%20attacks%20have%20been%20deeply%20investigated%20in%20the%20image%20domain%3B%20however%2C%20there%20is%20less%20research%20in%20the%20time-series%20domain%20and%20very%20little%20for%20forecasting%20financial%20data.%20To%20address%20these%20concerns%2C%20this%20study%20aims%20to%20build%20upon%20previous%20research%20on%20adversarial%20attacks%20for%20time-series%20data%20by%20introducing%20two%20new%20slope-based%20methods%20aimed%20to%20alter%20the%20trends%20of%20the%20predicted%20stock%20forecast%20generated%20by%20an%20N-HiTS%20model.%20Compared%20to%20the%20normal%20N-HiTS%20predictions%2C%20the%20two%20new%20slope-based%20methods%2C%20the%20General%20Slope%20Attack%20and%20Least-Squares%20Slope%20Attack%2C%20can%20manipulate%20N-HiTS%20predictions%20by%20doubling%20the%20slope.%20These%20new%20slope%20attacks%20can%20bypass%20standard%20security%20mechanisms%2C%20such%20as%20a%20discriminator%20that%20filters%20real%20and%20perturbed%20inputs%2C%20reducing%20a%204-layered%20CNN%27s%20specificity%20to%2028%25%20and%20accuracy%20to%2057%25.%20Furthermore%2C%20the%20slope%20based%20methods%20were%20incorporated%20into%20a%20GAN%20architecture%20as%20a%20means%20of%20generating%20realistic%20synthetic%20data%2C%20while%20simultaneously%20fooling%20the%20model.%20Finally%2C%20this%20paper%20also%20proposes%20a%20sample%20malware%20designed%20to%20inject%20an%20adversarial%20attack%20in%20the%20model%20inference%20library%2C%20proving%20that%20ML-security%20research%20should%20not%20only%20focus%20on%20making%20the%20model%20safe%2C%20but%20also%20securing%20the%20entire%20pipeline.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTargeted%2520Manipulation%253A%2520Slope-Based%2520Attacks%2520on%2520Financial%2520Time-Series%2520Data%26entry.906535625%3DDominik%2520Luszczynski%26entry.1292438233%3DA%2520common%2520method%2520of%2520attacking%2520deep%2520learning%2520models%2520is%2520through%2520adversarial%2520attacks%252C%2520which%2520occur%2520when%2520an%2520attacker%2520specifically%2520modifies%2520the%2520input%2520of%2520a%2520model%2520to%2520produce%2520an%2520incorrect%2520result.%2520Adversarial%2520attacks%2520have%2520been%2520deeply%2520investigated%2520in%2520the%2520image%2520domain%253B%2520however%252C%2520there%2520is%2520less%2520research%2520in%2520the%2520time-series%2520domain%2520and%2520very%2520little%2520for%2520forecasting%2520financial%2520data.%2520To%2520address%2520these%2520concerns%252C%2520this%2520study%2520aims%2520to%2520build%2520upon%2520previous%2520research%2520on%2520adversarial%2520attacks%2520for%2520time-series%2520data%2520by%2520introducing%2520two%2520new%2520slope-based%2520methods%2520aimed%2520to%2520alter%2520the%2520trends%2520of%2520the%2520predicted%2520stock%2520forecast%2520generated%2520by%2520an%2520N-HiTS%2520model.%2520Compared%2520to%2520the%2520normal%2520N-HiTS%2520predictions%252C%2520the%2520two%2520new%2520slope-based%2520methods%252C%2520the%2520General%2520Slope%2520Attack%2520and%2520Least-Squares%2520Slope%2520Attack%252C%2520can%2520manipulate%2520N-HiTS%2520predictions%2520by%2520doubling%2520the%2520slope.%2520These%2520new%2520slope%2520attacks%2520can%2520bypass%2520standard%2520security%2520mechanisms%252C%2520such%2520as%2520a%2520discriminator%2520that%2520filters%2520real%2520and%2520perturbed%2520inputs%252C%2520reducing%2520a%25204-layered%2520CNN%2527s%2520specificity%2520to%252028%2525%2520and%2520accuracy%2520to%252057%2525.%2520Furthermore%252C%2520the%2520slope%2520based%2520methods%2520were%2520incorporated%2520into%2520a%2520GAN%2520architecture%2520as%2520a%2520means%2520of%2520generating%2520realistic%2520synthetic%2520data%252C%2520while%2520simultaneously%2520fooling%2520the%2520model.%2520Finally%252C%2520this%2520paper%2520also%2520proposes%2520a%2520sample%2520malware%2520designed%2520to%2520inject%2520an%2520adversarial%2520attack%2520in%2520the%2520model%2520inference%2520library%252C%2520proving%2520that%2520ML-security%2520research%2520should%2520not%2520only%2520focus%2520on%2520making%2520the%2520model%2520safe%252C%2520but%2520also%2520securing%2520the%2520entire%2520pipeline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Targeted%20Manipulation%3A%20Slope-Based%20Attacks%20on%20Financial%20Time-Series%20Data&entry.906535625=Dominik%20Luszczynski&entry.1292438233=A%20common%20method%20of%20attacking%20deep%20learning%20models%20is%20through%20adversarial%20attacks%2C%20which%20occur%20when%20an%20attacker%20specifically%20modifies%20the%20input%20of%20a%20model%20to%20produce%20an%20incorrect%20result.%20Adversarial%20attacks%20have%20been%20deeply%20investigated%20in%20the%20image%20domain%3B%20however%2C%20there%20is%20less%20research%20in%20the%20time-series%20domain%20and%20very%20little%20for%20forecasting%20financial%20data.%20To%20address%20these%20concerns%2C%20this%20study%20aims%20to%20build%20upon%20previous%20research%20on%20adversarial%20attacks%20for%20time-series%20data%20by%20introducing%20two%20new%20slope-based%20methods%20aimed%20to%20alter%20the%20trends%20of%20the%20predicted%20stock%20forecast%20generated%20by%20an%20N-HiTS%20model.%20Compared%20to%20the%20normal%20N-HiTS%20predictions%2C%20the%20two%20new%20slope-based%20methods%2C%20the%20General%20Slope%20Attack%20and%20Least-Squares%20Slope%20Attack%2C%20can%20manipulate%20N-HiTS%20predictions%20by%20doubling%20the%20slope.%20These%20new%20slope%20attacks%20can%20bypass%20standard%20security%20mechanisms%2C%20such%20as%20a%20discriminator%20that%20filters%20real%20and%20perturbed%20inputs%2C%20reducing%20a%204-layered%20CNN%27s%20specificity%20to%2028%25%20and%20accuracy%20to%2057%25.%20Furthermore%2C%20the%20slope%20based%20methods%20were%20incorporated%20into%20a%20GAN%20architecture%20as%20a%20means%20of%20generating%20realistic%20synthetic%20data%2C%20while%20simultaneously%20fooling%20the%20model.%20Finally%2C%20this%20paper%20also%20proposes%20a%20sample%20malware%20designed%20to%20inject%20an%20adversarial%20attack%20in%20the%20model%20inference%20library%2C%20proving%20that%20ML-security%20research%20should%20not%20only%20focus%20on%20making%20the%20model%20safe%2C%20but%20also%20securing%20the%20entire%20pipeline.&entry.1838667208=http%3A//arxiv.org/abs/2511.19330v1&entry.124074799=Read"},
{"title": "Structured Matching via Cost-Regularized Unbalanced Optimal Transport", "author": "Emanuele Pardini and Katerina Papagiannouli", "abstract": "Unbalanced optimal transport (UOT) provides a flexible way to match or compare nonnegative finite Radon measures. However, UOT requires a predefined ground transport cost, which may misrepresent the data's underlying geometry. Choosing such a cost is particularly challenging when datasets live in heterogeneous spaces, often motivating practitioners to adopt Gromov-Wasserstein formulations. To address this challenge, we introduce cost-regularized unbalanced optimal transport (CR-UOT), a framework that allows the ground cost to vary while allowing mass creation and removal. We show that CR-UOT incorporates unbalanced Gromov-Wasserstein type problems through families of inner-product costs parameterized by linear transformations, enabling the matching of measures or point clouds across Euclidean spaces. We develop algorithms for such CR-UOT problems using entropic regularization and demonstrate that this approach improves the alignment of heterogeneous single-cell omics profiles, especially when many cells lack direct matches.", "link": "http://arxiv.org/abs/2511.19075v1", "date": "2025-11-24", "relevancy": 2.3637, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.481}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4707}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Matching%20via%20Cost-Regularized%20Unbalanced%20Optimal%20Transport&body=Title%3A%20Structured%20Matching%20via%20Cost-Regularized%20Unbalanced%20Optimal%20Transport%0AAuthor%3A%20Emanuele%20Pardini%20and%20Katerina%20Papagiannouli%0AAbstract%3A%20Unbalanced%20optimal%20transport%20%28UOT%29%20provides%20a%20flexible%20way%20to%20match%20or%20compare%20nonnegative%20finite%20Radon%20measures.%20However%2C%20UOT%20requires%20a%20predefined%20ground%20transport%20cost%2C%20which%20may%20misrepresent%20the%20data%27s%20underlying%20geometry.%20Choosing%20such%20a%20cost%20is%20particularly%20challenging%20when%20datasets%20live%20in%20heterogeneous%20spaces%2C%20often%20motivating%20practitioners%20to%20adopt%20Gromov-Wasserstein%20formulations.%20To%20address%20this%20challenge%2C%20we%20introduce%20cost-regularized%20unbalanced%20optimal%20transport%20%28CR-UOT%29%2C%20a%20framework%20that%20allows%20the%20ground%20cost%20to%20vary%20while%20allowing%20mass%20creation%20and%20removal.%20We%20show%20that%20CR-UOT%20incorporates%20unbalanced%20Gromov-Wasserstein%20type%20problems%20through%20families%20of%20inner-product%20costs%20parameterized%20by%20linear%20transformations%2C%20enabling%20the%20matching%20of%20measures%20or%20point%20clouds%20across%20Euclidean%20spaces.%20We%20develop%20algorithms%20for%20such%20CR-UOT%20problems%20using%20entropic%20regularization%20and%20demonstrate%20that%20this%20approach%20improves%20the%20alignment%20of%20heterogeneous%20single-cell%20omics%20profiles%2C%20especially%20when%20many%20cells%20lack%20direct%20matches.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Matching%2520via%2520Cost-Regularized%2520Unbalanced%2520Optimal%2520Transport%26entry.906535625%3DEmanuele%2520Pardini%2520and%2520Katerina%2520Papagiannouli%26entry.1292438233%3DUnbalanced%2520optimal%2520transport%2520%2528UOT%2529%2520provides%2520a%2520flexible%2520way%2520to%2520match%2520or%2520compare%2520nonnegative%2520finite%2520Radon%2520measures.%2520However%252C%2520UOT%2520requires%2520a%2520predefined%2520ground%2520transport%2520cost%252C%2520which%2520may%2520misrepresent%2520the%2520data%2527s%2520underlying%2520geometry.%2520Choosing%2520such%2520a%2520cost%2520is%2520particularly%2520challenging%2520when%2520datasets%2520live%2520in%2520heterogeneous%2520spaces%252C%2520often%2520motivating%2520practitioners%2520to%2520adopt%2520Gromov-Wasserstein%2520formulations.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520cost-regularized%2520unbalanced%2520optimal%2520transport%2520%2528CR-UOT%2529%252C%2520a%2520framework%2520that%2520allows%2520the%2520ground%2520cost%2520to%2520vary%2520while%2520allowing%2520mass%2520creation%2520and%2520removal.%2520We%2520show%2520that%2520CR-UOT%2520incorporates%2520unbalanced%2520Gromov-Wasserstein%2520type%2520problems%2520through%2520families%2520of%2520inner-product%2520costs%2520parameterized%2520by%2520linear%2520transformations%252C%2520enabling%2520the%2520matching%2520of%2520measures%2520or%2520point%2520clouds%2520across%2520Euclidean%2520spaces.%2520We%2520develop%2520algorithms%2520for%2520such%2520CR-UOT%2520problems%2520using%2520entropic%2520regularization%2520and%2520demonstrate%2520that%2520this%2520approach%2520improves%2520the%2520alignment%2520of%2520heterogeneous%2520single-cell%2520omics%2520profiles%252C%2520especially%2520when%2520many%2520cells%2520lack%2520direct%2520matches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Matching%20via%20Cost-Regularized%20Unbalanced%20Optimal%20Transport&entry.906535625=Emanuele%20Pardini%20and%20Katerina%20Papagiannouli&entry.1292438233=Unbalanced%20optimal%20transport%20%28UOT%29%20provides%20a%20flexible%20way%20to%20match%20or%20compare%20nonnegative%20finite%20Radon%20measures.%20However%2C%20UOT%20requires%20a%20predefined%20ground%20transport%20cost%2C%20which%20may%20misrepresent%20the%20data%27s%20underlying%20geometry.%20Choosing%20such%20a%20cost%20is%20particularly%20challenging%20when%20datasets%20live%20in%20heterogeneous%20spaces%2C%20often%20motivating%20practitioners%20to%20adopt%20Gromov-Wasserstein%20formulations.%20To%20address%20this%20challenge%2C%20we%20introduce%20cost-regularized%20unbalanced%20optimal%20transport%20%28CR-UOT%29%2C%20a%20framework%20that%20allows%20the%20ground%20cost%20to%20vary%20while%20allowing%20mass%20creation%20and%20removal.%20We%20show%20that%20CR-UOT%20incorporates%20unbalanced%20Gromov-Wasserstein%20type%20problems%20through%20families%20of%20inner-product%20costs%20parameterized%20by%20linear%20transformations%2C%20enabling%20the%20matching%20of%20measures%20or%20point%20clouds%20across%20Euclidean%20spaces.%20We%20develop%20algorithms%20for%20such%20CR-UOT%20problems%20using%20entropic%20regularization%20and%20demonstrate%20that%20this%20approach%20improves%20the%20alignment%20of%20heterogeneous%20single-cell%20omics%20profiles%2C%20especially%20when%20many%20cells%20lack%20direct%20matches.&entry.1838667208=http%3A//arxiv.org/abs/2511.19075v1&entry.124074799=Read"},
{"title": "OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution", "author": "Zhiqiang Wu and Zhaomang Sun and Tong Zhou and Bingtao Fu and Ji Cong and Yitong Dong and Huaqi Zhang and Xuan Tang and Mingsong Chen and Xian Wei", "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) show promising potential in one-step Real-World Image Super-Resolution (Real-ISR). Current one-step Real-ISR methods typically inject the low-quality (LQ) image latent representation at the start or end timestep of the DDPM scheduler. Recent studies have begun to note that the LQ image latent and the pre-trained noisy latent representations are intuitively closer at a mid-timestep. However, a quantitative analysis of these latent representations remains lacking. Considering these latent representations can be decomposed into signal and noise, we propose a method based on the Signal-to-Noise Ratio (SNR) to pre-compute an average optimal mid-timestep for injection. To better approximate the pre-trained noisy latent representation, we further introduce the Latent Representation Refinement (LRR) loss via a LoRA-enhanced VAE encoder. We also fine-tune the backbone of the DDPM-based generative model using LoRA to perform one-step denoising at the average optimal mid-timestep. Based on these components, we present OMGSR, a GAN-based Real-ISR framework that employs a DDPM-based generative model as the generator and a DINOv3-ConvNeXt model with multi-level discriminator heads as the discriminator. We also propose the DINOv3-ConvNeXt DISTS (Dv3CD) loss, which is enhanced for structural perception at varying resolutions. Within the OMGSR framework, we develop OMGSR-S based on SD2.1-base. An ablation study confirms that our pre-computation strategy and LRR loss significantly improve the baseline. Comparative studies demonstrate that OMGSR-S achieves state-of-the-art performance across multiple metrics. Code is available at \\hyperlink{Github}{https://github.com/wuer5/OMGSR}.", "link": "http://arxiv.org/abs/2508.08227v2", "date": "2025-11-24", "relevancy": 2.3541, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6191}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5954}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OMGSR%3A%20You%20Only%20Need%20One%20Mid-timestep%20Guidance%20for%20Real-World%20Image%20Super-Resolution&body=Title%3A%20OMGSR%3A%20You%20Only%20Need%20One%20Mid-timestep%20Guidance%20for%20Real-World%20Image%20Super-Resolution%0AAuthor%3A%20Zhiqiang%20Wu%20and%20Zhaomang%20Sun%20and%20Tong%20Zhou%20and%20Bingtao%20Fu%20and%20Ji%20Cong%20and%20Yitong%20Dong%20and%20Huaqi%20Zhang%20and%20Xuan%20Tang%20and%20Mingsong%20Chen%20and%20Xian%20Wei%0AAbstract%3A%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20show%20promising%20potential%20in%20one-step%20Real-World%20Image%20Super-Resolution%20%28Real-ISR%29.%20Current%20one-step%20Real-ISR%20methods%20typically%20inject%20the%20low-quality%20%28LQ%29%20image%20latent%20representation%20at%20the%20start%20or%20end%20timestep%20of%20the%20DDPM%20scheduler.%20Recent%20studies%20have%20begun%20to%20note%20that%20the%20LQ%20image%20latent%20and%20the%20pre-trained%20noisy%20latent%20representations%20are%20intuitively%20closer%20at%20a%20mid-timestep.%20However%2C%20a%20quantitative%20analysis%20of%20these%20latent%20representations%20remains%20lacking.%20Considering%20these%20latent%20representations%20can%20be%20decomposed%20into%20signal%20and%20noise%2C%20we%20propose%20a%20method%20based%20on%20the%20Signal-to-Noise%20Ratio%20%28SNR%29%20to%20pre-compute%20an%20average%20optimal%20mid-timestep%20for%20injection.%20To%20better%20approximate%20the%20pre-trained%20noisy%20latent%20representation%2C%20we%20further%20introduce%20the%20Latent%20Representation%20Refinement%20%28LRR%29%20loss%20via%20a%20LoRA-enhanced%20VAE%20encoder.%20We%20also%20fine-tune%20the%20backbone%20of%20the%20DDPM-based%20generative%20model%20using%20LoRA%20to%20perform%20one-step%20denoising%20at%20the%20average%20optimal%20mid-timestep.%20Based%20on%20these%20components%2C%20we%20present%20OMGSR%2C%20a%20GAN-based%20Real-ISR%20framework%20that%20employs%20a%20DDPM-based%20generative%20model%20as%20the%20generator%20and%20a%20DINOv3-ConvNeXt%20model%20with%20multi-level%20discriminator%20heads%20as%20the%20discriminator.%20We%20also%20propose%20the%20DINOv3-ConvNeXt%20DISTS%20%28Dv3CD%29%20loss%2C%20which%20is%20enhanced%20for%20structural%20perception%20at%20varying%20resolutions.%20Within%20the%20OMGSR%20framework%2C%20we%20develop%20OMGSR-S%20based%20on%20SD2.1-base.%20An%20ablation%20study%20confirms%20that%20our%20pre-computation%20strategy%20and%20LRR%20loss%20significantly%20improve%20the%20baseline.%20Comparative%20studies%20demonstrate%20that%20OMGSR-S%20achieves%20state-of-the-art%20performance%20across%20multiple%20metrics.%20Code%20is%20available%20at%20%5Chyperlink%7BGithub%7D%7Bhttps%3A//github.com/wuer5/OMGSR%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2508.08227v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOMGSR%253A%2520You%2520Only%2520Need%2520One%2520Mid-timestep%2520Guidance%2520for%2520Real-World%2520Image%2520Super-Resolution%26entry.906535625%3DZhiqiang%2520Wu%2520and%2520Zhaomang%2520Sun%2520and%2520Tong%2520Zhou%2520and%2520Bingtao%2520Fu%2520and%2520Ji%2520Cong%2520and%2520Yitong%2520Dong%2520and%2520Huaqi%2520Zhang%2520and%2520Xuan%2520Tang%2520and%2520Mingsong%2520Chen%2520and%2520Xian%2520Wei%26entry.1292438233%3DDenoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPMs%2529%2520show%2520promising%2520potential%2520in%2520one-step%2520Real-World%2520Image%2520Super-Resolution%2520%2528Real-ISR%2529.%2520Current%2520one-step%2520Real-ISR%2520methods%2520typically%2520inject%2520the%2520low-quality%2520%2528LQ%2529%2520image%2520latent%2520representation%2520at%2520the%2520start%2520or%2520end%2520timestep%2520of%2520the%2520DDPM%2520scheduler.%2520Recent%2520studies%2520have%2520begun%2520to%2520note%2520that%2520the%2520LQ%2520image%2520latent%2520and%2520the%2520pre-trained%2520noisy%2520latent%2520representations%2520are%2520intuitively%2520closer%2520at%2520a%2520mid-timestep.%2520However%252C%2520a%2520quantitative%2520analysis%2520of%2520these%2520latent%2520representations%2520remains%2520lacking.%2520Considering%2520these%2520latent%2520representations%2520can%2520be%2520decomposed%2520into%2520signal%2520and%2520noise%252C%2520we%2520propose%2520a%2520method%2520based%2520on%2520the%2520Signal-to-Noise%2520Ratio%2520%2528SNR%2529%2520to%2520pre-compute%2520an%2520average%2520optimal%2520mid-timestep%2520for%2520injection.%2520To%2520better%2520approximate%2520the%2520pre-trained%2520noisy%2520latent%2520representation%252C%2520we%2520further%2520introduce%2520the%2520Latent%2520Representation%2520Refinement%2520%2528LRR%2529%2520loss%2520via%2520a%2520LoRA-enhanced%2520VAE%2520encoder.%2520We%2520also%2520fine-tune%2520the%2520backbone%2520of%2520the%2520DDPM-based%2520generative%2520model%2520using%2520LoRA%2520to%2520perform%2520one-step%2520denoising%2520at%2520the%2520average%2520optimal%2520mid-timestep.%2520Based%2520on%2520these%2520components%252C%2520we%2520present%2520OMGSR%252C%2520a%2520GAN-based%2520Real-ISR%2520framework%2520that%2520employs%2520a%2520DDPM-based%2520generative%2520model%2520as%2520the%2520generator%2520and%2520a%2520DINOv3-ConvNeXt%2520model%2520with%2520multi-level%2520discriminator%2520heads%2520as%2520the%2520discriminator.%2520We%2520also%2520propose%2520the%2520DINOv3-ConvNeXt%2520DISTS%2520%2528Dv3CD%2529%2520loss%252C%2520which%2520is%2520enhanced%2520for%2520structural%2520perception%2520at%2520varying%2520resolutions.%2520Within%2520the%2520OMGSR%2520framework%252C%2520we%2520develop%2520OMGSR-S%2520based%2520on%2520SD2.1-base.%2520An%2520ablation%2520study%2520confirms%2520that%2520our%2520pre-computation%2520strategy%2520and%2520LRR%2520loss%2520significantly%2520improve%2520the%2520baseline.%2520Comparative%2520studies%2520demonstrate%2520that%2520OMGSR-S%2520achieves%2520state-of-the-art%2520performance%2520across%2520multiple%2520metrics.%2520Code%2520is%2520available%2520at%2520%255Chyperlink%257BGithub%257D%257Bhttps%253A//github.com/wuer5/OMGSR%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08227v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OMGSR%3A%20You%20Only%20Need%20One%20Mid-timestep%20Guidance%20for%20Real-World%20Image%20Super-Resolution&entry.906535625=Zhiqiang%20Wu%20and%20Zhaomang%20Sun%20and%20Tong%20Zhou%20and%20Bingtao%20Fu%20and%20Ji%20Cong%20and%20Yitong%20Dong%20and%20Huaqi%20Zhang%20and%20Xuan%20Tang%20and%20Mingsong%20Chen%20and%20Xian%20Wei&entry.1292438233=Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%20show%20promising%20potential%20in%20one-step%20Real-World%20Image%20Super-Resolution%20%28Real-ISR%29.%20Current%20one-step%20Real-ISR%20methods%20typically%20inject%20the%20low-quality%20%28LQ%29%20image%20latent%20representation%20at%20the%20start%20or%20end%20timestep%20of%20the%20DDPM%20scheduler.%20Recent%20studies%20have%20begun%20to%20note%20that%20the%20LQ%20image%20latent%20and%20the%20pre-trained%20noisy%20latent%20representations%20are%20intuitively%20closer%20at%20a%20mid-timestep.%20However%2C%20a%20quantitative%20analysis%20of%20these%20latent%20representations%20remains%20lacking.%20Considering%20these%20latent%20representations%20can%20be%20decomposed%20into%20signal%20and%20noise%2C%20we%20propose%20a%20method%20based%20on%20the%20Signal-to-Noise%20Ratio%20%28SNR%29%20to%20pre-compute%20an%20average%20optimal%20mid-timestep%20for%20injection.%20To%20better%20approximate%20the%20pre-trained%20noisy%20latent%20representation%2C%20we%20further%20introduce%20the%20Latent%20Representation%20Refinement%20%28LRR%29%20loss%20via%20a%20LoRA-enhanced%20VAE%20encoder.%20We%20also%20fine-tune%20the%20backbone%20of%20the%20DDPM-based%20generative%20model%20using%20LoRA%20to%20perform%20one-step%20denoising%20at%20the%20average%20optimal%20mid-timestep.%20Based%20on%20these%20components%2C%20we%20present%20OMGSR%2C%20a%20GAN-based%20Real-ISR%20framework%20that%20employs%20a%20DDPM-based%20generative%20model%20as%20the%20generator%20and%20a%20DINOv3-ConvNeXt%20model%20with%20multi-level%20discriminator%20heads%20as%20the%20discriminator.%20We%20also%20propose%20the%20DINOv3-ConvNeXt%20DISTS%20%28Dv3CD%29%20loss%2C%20which%20is%20enhanced%20for%20structural%20perception%20at%20varying%20resolutions.%20Within%20the%20OMGSR%20framework%2C%20we%20develop%20OMGSR-S%20based%20on%20SD2.1-base.%20An%20ablation%20study%20confirms%20that%20our%20pre-computation%20strategy%20and%20LRR%20loss%20significantly%20improve%20the%20baseline.%20Comparative%20studies%20demonstrate%20that%20OMGSR-S%20achieves%20state-of-the-art%20performance%20across%20multiple%20metrics.%20Code%20is%20available%20at%20%5Chyperlink%7BGithub%7D%7Bhttps%3A//github.com/wuer5/OMGSR%7D.&entry.1838667208=http%3A//arxiv.org/abs/2508.08227v2&entry.124074799=Read"},
{"title": "Cloud4D", "author": "Jacob Lin and Edward Gryspeerdt and Ronald Clark", "abstract": "There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras. Leveraging a homography-guided 2D-to-3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ($<10\\%$) against collocated radar measurements. Code and data are available on our project page https://cloud4d.jacob-lin.com/.", "link": "http://arxiv.org/abs/2511.19431v1", "date": "2025-11-24", "relevancy": 2.3012, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5777}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5777}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cloud4D&body=Title%3A%20Cloud4D%0AAuthor%3A%20Jacob%20Lin%20and%20Edward%20Gryspeerdt%20and%20Ronald%20Clark%0AAbstract%3A%20There%20has%20been%20great%20progress%20in%20improving%20numerical%20weather%20prediction%20and%20climate%20models%20using%20machine%20learning.%20However%2C%20most%20global%20models%20act%20at%20a%20kilometer-scale%2C%20making%20it%20challenging%20to%20model%20individual%20clouds%20and%20factors%20such%20as%20extreme%20precipitation%2C%20wind%20gusts%2C%20turbulence%2C%20and%20surface%20irradiance.%20Therefore%2C%20there%20is%20a%20need%20to%20move%20towards%20higher-resolution%20models%2C%20which%20in%20turn%20require%20high-resolution%20real-world%20observations%20that%20current%20instruments%20struggle%20to%20obtain.%20We%20present%20Cloud4D%2C%20the%20first%20learning-based%20framework%20that%20reconstructs%20a%20physically%20consistent%2C%20four-dimensional%20cloud%20state%20using%20only%20synchronized%20ground-based%20cameras.%20Leveraging%20a%20homography-guided%202D-to-3D%20transformer%2C%20Cloud4D%20infers%20the%20full%203D%20distribution%20of%20liquid%20water%20content%20at%2025%20m%20spatial%20and%205%20s%20temporal%20resolution.%20By%20tracking%20the%203D%20liquid%20water%20content%20retrievals%20over%20time%2C%20Cloud4D%20additionally%20estimates%20horizontal%20wind%20vectors.%20Across%20a%20two-month%20deployment%20comprising%20six%20skyward%20cameras%2C%20our%20system%20delivers%20an%20order-of-magnitude%20improvement%20in%20space-time%20resolution%20relative%20to%20state-of-the-art%20satellite%20measurements%2C%20while%20retaining%20single-digit%20relative%20error%20%28%24%3C10%5C%25%24%29%20against%20collocated%20radar%20measurements.%20Code%20and%20data%20are%20available%20on%20our%20project%20page%20https%3A//cloud4d.jacob-lin.com/.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCloud4D%26entry.906535625%3DJacob%2520Lin%2520and%2520Edward%2520Gryspeerdt%2520and%2520Ronald%2520Clark%26entry.1292438233%3DThere%2520has%2520been%2520great%2520progress%2520in%2520improving%2520numerical%2520weather%2520prediction%2520and%2520climate%2520models%2520using%2520machine%2520learning.%2520However%252C%2520most%2520global%2520models%2520act%2520at%2520a%2520kilometer-scale%252C%2520making%2520it%2520challenging%2520to%2520model%2520individual%2520clouds%2520and%2520factors%2520such%2520as%2520extreme%2520precipitation%252C%2520wind%2520gusts%252C%2520turbulence%252C%2520and%2520surface%2520irradiance.%2520Therefore%252C%2520there%2520is%2520a%2520need%2520to%2520move%2520towards%2520higher-resolution%2520models%252C%2520which%2520in%2520turn%2520require%2520high-resolution%2520real-world%2520observations%2520that%2520current%2520instruments%2520struggle%2520to%2520obtain.%2520We%2520present%2520Cloud4D%252C%2520the%2520first%2520learning-based%2520framework%2520that%2520reconstructs%2520a%2520physically%2520consistent%252C%2520four-dimensional%2520cloud%2520state%2520using%2520only%2520synchronized%2520ground-based%2520cameras.%2520Leveraging%2520a%2520homography-guided%25202D-to-3D%2520transformer%252C%2520Cloud4D%2520infers%2520the%2520full%25203D%2520distribution%2520of%2520liquid%2520water%2520content%2520at%252025%2520m%2520spatial%2520and%25205%2520s%2520temporal%2520resolution.%2520By%2520tracking%2520the%25203D%2520liquid%2520water%2520content%2520retrievals%2520over%2520time%252C%2520Cloud4D%2520additionally%2520estimates%2520horizontal%2520wind%2520vectors.%2520Across%2520a%2520two-month%2520deployment%2520comprising%2520six%2520skyward%2520cameras%252C%2520our%2520system%2520delivers%2520an%2520order-of-magnitude%2520improvement%2520in%2520space-time%2520resolution%2520relative%2520to%2520state-of-the-art%2520satellite%2520measurements%252C%2520while%2520retaining%2520single-digit%2520relative%2520error%2520%2528%2524%253C10%255C%2525%2524%2529%2520against%2520collocated%2520radar%2520measurements.%2520Code%2520and%2520data%2520are%2520available%2520on%2520our%2520project%2520page%2520https%253A//cloud4d.jacob-lin.com/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cloud4D&entry.906535625=Jacob%20Lin%20and%20Edward%20Gryspeerdt%20and%20Ronald%20Clark&entry.1292438233=There%20has%20been%20great%20progress%20in%20improving%20numerical%20weather%20prediction%20and%20climate%20models%20using%20machine%20learning.%20However%2C%20most%20global%20models%20act%20at%20a%20kilometer-scale%2C%20making%20it%20challenging%20to%20model%20individual%20clouds%20and%20factors%20such%20as%20extreme%20precipitation%2C%20wind%20gusts%2C%20turbulence%2C%20and%20surface%20irradiance.%20Therefore%2C%20there%20is%20a%20need%20to%20move%20towards%20higher-resolution%20models%2C%20which%20in%20turn%20require%20high-resolution%20real-world%20observations%20that%20current%20instruments%20struggle%20to%20obtain.%20We%20present%20Cloud4D%2C%20the%20first%20learning-based%20framework%20that%20reconstructs%20a%20physically%20consistent%2C%20four-dimensional%20cloud%20state%20using%20only%20synchronized%20ground-based%20cameras.%20Leveraging%20a%20homography-guided%202D-to-3D%20transformer%2C%20Cloud4D%20infers%20the%20full%203D%20distribution%20of%20liquid%20water%20content%20at%2025%20m%20spatial%20and%205%20s%20temporal%20resolution.%20By%20tracking%20the%203D%20liquid%20water%20content%20retrievals%20over%20time%2C%20Cloud4D%20additionally%20estimates%20horizontal%20wind%20vectors.%20Across%20a%20two-month%20deployment%20comprising%20six%20skyward%20cameras%2C%20our%20system%20delivers%20an%20order-of-magnitude%20improvement%20in%20space-time%20resolution%20relative%20to%20state-of-the-art%20satellite%20measurements%2C%20while%20retaining%20single-digit%20relative%20error%20%28%24%3C10%5C%25%24%29%20against%20collocated%20radar%20measurements.%20Code%20and%20data%20are%20available%20on%20our%20project%20page%20https%3A//cloud4d.jacob-lin.com/.&entry.1838667208=http%3A//arxiv.org/abs/2511.19431v1&entry.124074799=Read"},
{"title": "MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images", "author": "Qirui Wang and Jingyi He and Yining Pan and Si Yong Yeo and Xulei Yang and Shijie Li", "abstract": "Spatial reasoning (SR), the ability to infer 3D spatial information from 2D inputs, is essential for real-world applications such as embodied AI and autonomous driving. However, existing research primarily focuses on indoor environments and typically relies on multi-view observations, which limits their generalizability to outdoor scenarios and constrains their applicability to monocular images, the most common real-world setting. In this work, we propose MonoSR, a large-scale monocular spatial reasoning dataset that spans diverse scenarios including indoor, outdoor, and object-centric settings, and supports multiple question types. MonoSR provides a path toward open-world monocular spatial reasoning. Beyond introducing the dataset, we evaluate advanced vision-language models to reveal their limitations on this challenging task. We further analyze whether auxiliary information is crucial for monocular spatial reasoning and offer practical guidance for designing future models. These contributions collectively establish a foundation for advancing monocular spatial reasoning in real-world, open-world environments.", "link": "http://arxiv.org/abs/2511.19119v1", "date": "2025-11-24", "relevancy": 2.296, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoSR%3A%20Open-Vocabulary%20Spatial%20Reasoning%20from%20Monocular%20Images&body=Title%3A%20MonoSR%3A%20Open-Vocabulary%20Spatial%20Reasoning%20from%20Monocular%20Images%0AAuthor%3A%20Qirui%20Wang%20and%20Jingyi%20He%20and%20Yining%20Pan%20and%20Si%20Yong%20Yeo%20and%20Xulei%20Yang%20and%20Shijie%20Li%0AAbstract%3A%20Spatial%20reasoning%20%28SR%29%2C%20the%20ability%20to%20infer%203D%20spatial%20information%20from%202D%20inputs%2C%20is%20essential%20for%20real-world%20applications%20such%20as%20embodied%20AI%20and%20autonomous%20driving.%20However%2C%20existing%20research%20primarily%20focuses%20on%20indoor%20environments%20and%20typically%20relies%20on%20multi-view%20observations%2C%20which%20limits%20their%20generalizability%20to%20outdoor%20scenarios%20and%20constrains%20their%20applicability%20to%20monocular%20images%2C%20the%20most%20common%20real-world%20setting.%20In%20this%20work%2C%20we%20propose%20MonoSR%2C%20a%20large-scale%20monocular%20spatial%20reasoning%20dataset%20that%20spans%20diverse%20scenarios%20including%20indoor%2C%20outdoor%2C%20and%20object-centric%20settings%2C%20and%20supports%20multiple%20question%20types.%20MonoSR%20provides%20a%20path%20toward%20open-world%20monocular%20spatial%20reasoning.%20Beyond%20introducing%20the%20dataset%2C%20we%20evaluate%20advanced%20vision-language%20models%20to%20reveal%20their%20limitations%20on%20this%20challenging%20task.%20We%20further%20analyze%20whether%20auxiliary%20information%20is%20crucial%20for%20monocular%20spatial%20reasoning%20and%20offer%20practical%20guidance%20for%20designing%20future%20models.%20These%20contributions%20collectively%20establish%20a%20foundation%20for%20advancing%20monocular%20spatial%20reasoning%20in%20real-world%2C%20open-world%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoSR%253A%2520Open-Vocabulary%2520Spatial%2520Reasoning%2520from%2520Monocular%2520Images%26entry.906535625%3DQirui%2520Wang%2520and%2520Jingyi%2520He%2520and%2520Yining%2520Pan%2520and%2520Si%2520Yong%2520Yeo%2520and%2520Xulei%2520Yang%2520and%2520Shijie%2520Li%26entry.1292438233%3DSpatial%2520reasoning%2520%2528SR%2529%252C%2520the%2520ability%2520to%2520infer%25203D%2520spatial%2520information%2520from%25202D%2520inputs%252C%2520is%2520essential%2520for%2520real-world%2520applications%2520such%2520as%2520embodied%2520AI%2520and%2520autonomous%2520driving.%2520However%252C%2520existing%2520research%2520primarily%2520focuses%2520on%2520indoor%2520environments%2520and%2520typically%2520relies%2520on%2520multi-view%2520observations%252C%2520which%2520limits%2520their%2520generalizability%2520to%2520outdoor%2520scenarios%2520and%2520constrains%2520their%2520applicability%2520to%2520monocular%2520images%252C%2520the%2520most%2520common%2520real-world%2520setting.%2520In%2520this%2520work%252C%2520we%2520propose%2520MonoSR%252C%2520a%2520large-scale%2520monocular%2520spatial%2520reasoning%2520dataset%2520that%2520spans%2520diverse%2520scenarios%2520including%2520indoor%252C%2520outdoor%252C%2520and%2520object-centric%2520settings%252C%2520and%2520supports%2520multiple%2520question%2520types.%2520MonoSR%2520provides%2520a%2520path%2520toward%2520open-world%2520monocular%2520spatial%2520reasoning.%2520Beyond%2520introducing%2520the%2520dataset%252C%2520we%2520evaluate%2520advanced%2520vision-language%2520models%2520to%2520reveal%2520their%2520limitations%2520on%2520this%2520challenging%2520task.%2520We%2520further%2520analyze%2520whether%2520auxiliary%2520information%2520is%2520crucial%2520for%2520monocular%2520spatial%2520reasoning%2520and%2520offer%2520practical%2520guidance%2520for%2520designing%2520future%2520models.%2520These%2520contributions%2520collectively%2520establish%2520a%2520foundation%2520for%2520advancing%2520monocular%2520spatial%2520reasoning%2520in%2520real-world%252C%2520open-world%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoSR%3A%20Open-Vocabulary%20Spatial%20Reasoning%20from%20Monocular%20Images&entry.906535625=Qirui%20Wang%20and%20Jingyi%20He%20and%20Yining%20Pan%20and%20Si%20Yong%20Yeo%20and%20Xulei%20Yang%20and%20Shijie%20Li&entry.1292438233=Spatial%20reasoning%20%28SR%29%2C%20the%20ability%20to%20infer%203D%20spatial%20information%20from%202D%20inputs%2C%20is%20essential%20for%20real-world%20applications%20such%20as%20embodied%20AI%20and%20autonomous%20driving.%20However%2C%20existing%20research%20primarily%20focuses%20on%20indoor%20environments%20and%20typically%20relies%20on%20multi-view%20observations%2C%20which%20limits%20their%20generalizability%20to%20outdoor%20scenarios%20and%20constrains%20their%20applicability%20to%20monocular%20images%2C%20the%20most%20common%20real-world%20setting.%20In%20this%20work%2C%20we%20propose%20MonoSR%2C%20a%20large-scale%20monocular%20spatial%20reasoning%20dataset%20that%20spans%20diverse%20scenarios%20including%20indoor%2C%20outdoor%2C%20and%20object-centric%20settings%2C%20and%20supports%20multiple%20question%20types.%20MonoSR%20provides%20a%20path%20toward%20open-world%20monocular%20spatial%20reasoning.%20Beyond%20introducing%20the%20dataset%2C%20we%20evaluate%20advanced%20vision-language%20models%20to%20reveal%20their%20limitations%20on%20this%20challenging%20task.%20We%20further%20analyze%20whether%20auxiliary%20information%20is%20crucial%20for%20monocular%20spatial%20reasoning%20and%20offer%20practical%20guidance%20for%20designing%20future%20models.%20These%20contributions%20collectively%20establish%20a%20foundation%20for%20advancing%20monocular%20spatial%20reasoning%20in%20real-world%2C%20open-world%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2511.19119v1&entry.124074799=Read"},
{"title": "End-to-end Autonomous Vehicle Following System using Monocular Fisheye Camera", "author": "Jiale Zhang and Yeqiang Qian and Tong Qin and Mingyang Jiang and Siyuan Chen and Ming Yang", "abstract": "The increase in vehicle ownership has led to increased traffic congestion, more accidents, and higher carbon emissions. Vehicle platooning is a promising solution to address these issues by improving road capacity and reducing fuel consumption. However, existing platooning systems face challenges such as reliance on lane markings and expensive high-precision sensors, which limits their general applicability. To address these issues, we propose a vehicle following framework that expands its capability from restricted scenarios to general scenario applications using only a camera. This is achieved through our newly proposed end-to-end method, which improves overall driving performance. The method incorporates a semantic mask to address causal confusion in multi-frame data fusion. Additionally, we introduce a dynamic sampling mechanism to precisely track the trajectories of preceding vehicles. Extensive closed-loop validation in real-world vehicle experiments demonstrates the system's ability to follow vehicles in various scenarios, outperforming traditional multi-stage algorithms. This makes it a promising solution for cost-effective autonomous vehicle platooning. A complete real-world vehicle experiment is available at https://youtu.be/zL1bcVb9kqQ.", "link": "http://arxiv.org/abs/2511.19011v1", "date": "2025-11-24", "relevancy": 2.2932, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5841}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5737}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-end%20Autonomous%20Vehicle%20Following%20System%20using%20Monocular%20Fisheye%20Camera&body=Title%3A%20End-to-end%20Autonomous%20Vehicle%20Following%20System%20using%20Monocular%20Fisheye%20Camera%0AAuthor%3A%20Jiale%20Zhang%20and%20Yeqiang%20Qian%20and%20Tong%20Qin%20and%20Mingyang%20Jiang%20and%20Siyuan%20Chen%20and%20Ming%20Yang%0AAbstract%3A%20The%20increase%20in%20vehicle%20ownership%20has%20led%20to%20increased%20traffic%20congestion%2C%20more%20accidents%2C%20and%20higher%20carbon%20emissions.%20Vehicle%20platooning%20is%20a%20promising%20solution%20to%20address%20these%20issues%20by%20improving%20road%20capacity%20and%20reducing%20fuel%20consumption.%20However%2C%20existing%20platooning%20systems%20face%20challenges%20such%20as%20reliance%20on%20lane%20markings%20and%20expensive%20high-precision%20sensors%2C%20which%20limits%20their%20general%20applicability.%20To%20address%20these%20issues%2C%20we%20propose%20a%20vehicle%20following%20framework%20that%20expands%20its%20capability%20from%20restricted%20scenarios%20to%20general%20scenario%20applications%20using%20only%20a%20camera.%20This%20is%20achieved%20through%20our%20newly%20proposed%20end-to-end%20method%2C%20which%20improves%20overall%20driving%20performance.%20The%20method%20incorporates%20a%20semantic%20mask%20to%20address%20causal%20confusion%20in%20multi-frame%20data%20fusion.%20Additionally%2C%20we%20introduce%20a%20dynamic%20sampling%20mechanism%20to%20precisely%20track%20the%20trajectories%20of%20preceding%20vehicles.%20Extensive%20closed-loop%20validation%20in%20real-world%20vehicle%20experiments%20demonstrates%20the%20system%27s%20ability%20to%20follow%20vehicles%20in%20various%20scenarios%2C%20outperforming%20traditional%20multi-stage%20algorithms.%20This%20makes%20it%20a%20promising%20solution%20for%20cost-effective%20autonomous%20vehicle%20platooning.%20A%20complete%20real-world%20vehicle%20experiment%20is%20available%20at%20https%3A//youtu.be/zL1bcVb9kqQ.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-end%2520Autonomous%2520Vehicle%2520Following%2520System%2520using%2520Monocular%2520Fisheye%2520Camera%26entry.906535625%3DJiale%2520Zhang%2520and%2520Yeqiang%2520Qian%2520and%2520Tong%2520Qin%2520and%2520Mingyang%2520Jiang%2520and%2520Siyuan%2520Chen%2520and%2520Ming%2520Yang%26entry.1292438233%3DThe%2520increase%2520in%2520vehicle%2520ownership%2520has%2520led%2520to%2520increased%2520traffic%2520congestion%252C%2520more%2520accidents%252C%2520and%2520higher%2520carbon%2520emissions.%2520Vehicle%2520platooning%2520is%2520a%2520promising%2520solution%2520to%2520address%2520these%2520issues%2520by%2520improving%2520road%2520capacity%2520and%2520reducing%2520fuel%2520consumption.%2520However%252C%2520existing%2520platooning%2520systems%2520face%2520challenges%2520such%2520as%2520reliance%2520on%2520lane%2520markings%2520and%2520expensive%2520high-precision%2520sensors%252C%2520which%2520limits%2520their%2520general%2520applicability.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520vehicle%2520following%2520framework%2520that%2520expands%2520its%2520capability%2520from%2520restricted%2520scenarios%2520to%2520general%2520scenario%2520applications%2520using%2520only%2520a%2520camera.%2520This%2520is%2520achieved%2520through%2520our%2520newly%2520proposed%2520end-to-end%2520method%252C%2520which%2520improves%2520overall%2520driving%2520performance.%2520The%2520method%2520incorporates%2520a%2520semantic%2520mask%2520to%2520address%2520causal%2520confusion%2520in%2520multi-frame%2520data%2520fusion.%2520Additionally%252C%2520we%2520introduce%2520a%2520dynamic%2520sampling%2520mechanism%2520to%2520precisely%2520track%2520the%2520trajectories%2520of%2520preceding%2520vehicles.%2520Extensive%2520closed-loop%2520validation%2520in%2520real-world%2520vehicle%2520experiments%2520demonstrates%2520the%2520system%2527s%2520ability%2520to%2520follow%2520vehicles%2520in%2520various%2520scenarios%252C%2520outperforming%2520traditional%2520multi-stage%2520algorithms.%2520This%2520makes%2520it%2520a%2520promising%2520solution%2520for%2520cost-effective%2520autonomous%2520vehicle%2520platooning.%2520A%2520complete%2520real-world%2520vehicle%2520experiment%2520is%2520available%2520at%2520https%253A//youtu.be/zL1bcVb9kqQ.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-end%20Autonomous%20Vehicle%20Following%20System%20using%20Monocular%20Fisheye%20Camera&entry.906535625=Jiale%20Zhang%20and%20Yeqiang%20Qian%20and%20Tong%20Qin%20and%20Mingyang%20Jiang%20and%20Siyuan%20Chen%20and%20Ming%20Yang&entry.1292438233=The%20increase%20in%20vehicle%20ownership%20has%20led%20to%20increased%20traffic%20congestion%2C%20more%20accidents%2C%20and%20higher%20carbon%20emissions.%20Vehicle%20platooning%20is%20a%20promising%20solution%20to%20address%20these%20issues%20by%20improving%20road%20capacity%20and%20reducing%20fuel%20consumption.%20However%2C%20existing%20platooning%20systems%20face%20challenges%20such%20as%20reliance%20on%20lane%20markings%20and%20expensive%20high-precision%20sensors%2C%20which%20limits%20their%20general%20applicability.%20To%20address%20these%20issues%2C%20we%20propose%20a%20vehicle%20following%20framework%20that%20expands%20its%20capability%20from%20restricted%20scenarios%20to%20general%20scenario%20applications%20using%20only%20a%20camera.%20This%20is%20achieved%20through%20our%20newly%20proposed%20end-to-end%20method%2C%20which%20improves%20overall%20driving%20performance.%20The%20method%20incorporates%20a%20semantic%20mask%20to%20address%20causal%20confusion%20in%20multi-frame%20data%20fusion.%20Additionally%2C%20we%20introduce%20a%20dynamic%20sampling%20mechanism%20to%20precisely%20track%20the%20trajectories%20of%20preceding%20vehicles.%20Extensive%20closed-loop%20validation%20in%20real-world%20vehicle%20experiments%20demonstrates%20the%20system%27s%20ability%20to%20follow%20vehicles%20in%20various%20scenarios%2C%20outperforming%20traditional%20multi-stage%20algorithms.%20This%20makes%20it%20a%20promising%20solution%20for%20cost-effective%20autonomous%20vehicle%20platooning.%20A%20complete%20real-world%20vehicle%20experiment%20is%20available%20at%20https%3A//youtu.be/zL1bcVb9kqQ.&entry.1838667208=http%3A//arxiv.org/abs/2511.19011v1&entry.124074799=Read"},
{"title": "Interpreting Graph Inference with Skyline Explanations", "author": "Dazhuo Qiu and Haolai Che and Arijit Khan and Yinghui Wu", "abstract": "Inference queries have been routinely issued to graph machine learning models such as graph neural networks (GNNs) for various network analytical tasks. Nevertheless, GNN outputs are often hard to interpret comprehensively. Existing methods typically conform to individual pre-defined explainability measures (such as fidelity), which often leads to biased, ``one-side'' interpretations. This paper introduces skyline explanation, a new paradigm that interprets GNN outputs by simultaneously optimizing multiple explainability measures of users' interests. (1) We propose skyline explanations as a Pareto set of explanatory subgraphs that dominate others over multiple explanatory measures. We formulate skyline explanation as a multi-criteria optimization problem, and establish its hardness results. (2) We design efficient algorithms with an onion-peeling approach, which strategically prioritizes nodes and removes unpromising edges to incrementally assemble skyline explanations. (3) We also develop an algorithm to diversify the skyline explanations to enrich the comprehensive interpretation. (4) We introduce efficient parallel algorithms with load-balancing strategies to scale skyline explanation for large-scale GNN-based inference. Using real-world and synthetic graphs, we experimentally verify our algorithms' effectiveness and scalability.", "link": "http://arxiv.org/abs/2505.07635v4", "date": "2025-11-24", "relevancy": 2.2837, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.458}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.458}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Graph%20Inference%20with%20Skyline%20Explanations&body=Title%3A%20Interpreting%20Graph%20Inference%20with%20Skyline%20Explanations%0AAuthor%3A%20Dazhuo%20Qiu%20and%20Haolai%20Che%20and%20Arijit%20Khan%20and%20Yinghui%20Wu%0AAbstract%3A%20Inference%20queries%20have%20been%20routinely%20issued%20to%20graph%20machine%20learning%20models%20such%20as%20graph%20neural%20networks%20%28GNNs%29%20for%20various%20network%20analytical%20tasks.%20Nevertheless%2C%20GNN%20outputs%20are%20often%20hard%20to%20interpret%20comprehensively.%20Existing%20methods%20typically%20conform%20to%20individual%20pre-defined%20explainability%20measures%20%28such%20as%20fidelity%29%2C%20which%20often%20leads%20to%20biased%2C%20%60%60one-side%27%27%20interpretations.%20This%20paper%20introduces%20skyline%20explanation%2C%20a%20new%20paradigm%20that%20interprets%20GNN%20outputs%20by%20simultaneously%20optimizing%20multiple%20explainability%20measures%20of%20users%27%20interests.%20%281%29%20We%20propose%20skyline%20explanations%20as%20a%20Pareto%20set%20of%20explanatory%20subgraphs%20that%20dominate%20others%20over%20multiple%20explanatory%20measures.%20We%20formulate%20skyline%20explanation%20as%20a%20multi-criteria%20optimization%20problem%2C%20and%20establish%20its%20hardness%20results.%20%282%29%20We%20design%20efficient%20algorithms%20with%20an%20onion-peeling%20approach%2C%20which%20strategically%20prioritizes%20nodes%20and%20removes%20unpromising%20edges%20to%20incrementally%20assemble%20skyline%20explanations.%20%283%29%20We%20also%20develop%20an%20algorithm%20to%20diversify%20the%20skyline%20explanations%20to%20enrich%20the%20comprehensive%20interpretation.%20%284%29%20We%20introduce%20efficient%20parallel%20algorithms%20with%20load-balancing%20strategies%20to%20scale%20skyline%20explanation%20for%20large-scale%20GNN-based%20inference.%20Using%20real-world%20and%20synthetic%20graphs%2C%20we%20experimentally%20verify%20our%20algorithms%27%20effectiveness%20and%20scalability.%0ALink%3A%20http%3A//arxiv.org/abs/2505.07635v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Graph%2520Inference%2520with%2520Skyline%2520Explanations%26entry.906535625%3DDazhuo%2520Qiu%2520and%2520Haolai%2520Che%2520and%2520Arijit%2520Khan%2520and%2520Yinghui%2520Wu%26entry.1292438233%3DInference%2520queries%2520have%2520been%2520routinely%2520issued%2520to%2520graph%2520machine%2520learning%2520models%2520such%2520as%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520for%2520various%2520network%2520analytical%2520tasks.%2520Nevertheless%252C%2520GNN%2520outputs%2520are%2520often%2520hard%2520to%2520interpret%2520comprehensively.%2520Existing%2520methods%2520typically%2520conform%2520to%2520individual%2520pre-defined%2520explainability%2520measures%2520%2528such%2520as%2520fidelity%2529%252C%2520which%2520often%2520leads%2520to%2520biased%252C%2520%2560%2560one-side%2527%2527%2520interpretations.%2520This%2520paper%2520introduces%2520skyline%2520explanation%252C%2520a%2520new%2520paradigm%2520that%2520interprets%2520GNN%2520outputs%2520by%2520simultaneously%2520optimizing%2520multiple%2520explainability%2520measures%2520of%2520users%2527%2520interests.%2520%25281%2529%2520We%2520propose%2520skyline%2520explanations%2520as%2520a%2520Pareto%2520set%2520of%2520explanatory%2520subgraphs%2520that%2520dominate%2520others%2520over%2520multiple%2520explanatory%2520measures.%2520We%2520formulate%2520skyline%2520explanation%2520as%2520a%2520multi-criteria%2520optimization%2520problem%252C%2520and%2520establish%2520its%2520hardness%2520results.%2520%25282%2529%2520We%2520design%2520efficient%2520algorithms%2520with%2520an%2520onion-peeling%2520approach%252C%2520which%2520strategically%2520prioritizes%2520nodes%2520and%2520removes%2520unpromising%2520edges%2520to%2520incrementally%2520assemble%2520skyline%2520explanations.%2520%25283%2529%2520We%2520also%2520develop%2520an%2520algorithm%2520to%2520diversify%2520the%2520skyline%2520explanations%2520to%2520enrich%2520the%2520comprehensive%2520interpretation.%2520%25284%2529%2520We%2520introduce%2520efficient%2520parallel%2520algorithms%2520with%2520load-balancing%2520strategies%2520to%2520scale%2520skyline%2520explanation%2520for%2520large-scale%2520GNN-based%2520inference.%2520Using%2520real-world%2520and%2520synthetic%2520graphs%252C%2520we%2520experimentally%2520verify%2520our%2520algorithms%2527%2520effectiveness%2520and%2520scalability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07635v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Graph%20Inference%20with%20Skyline%20Explanations&entry.906535625=Dazhuo%20Qiu%20and%20Haolai%20Che%20and%20Arijit%20Khan%20and%20Yinghui%20Wu&entry.1292438233=Inference%20queries%20have%20been%20routinely%20issued%20to%20graph%20machine%20learning%20models%20such%20as%20graph%20neural%20networks%20%28GNNs%29%20for%20various%20network%20analytical%20tasks.%20Nevertheless%2C%20GNN%20outputs%20are%20often%20hard%20to%20interpret%20comprehensively.%20Existing%20methods%20typically%20conform%20to%20individual%20pre-defined%20explainability%20measures%20%28such%20as%20fidelity%29%2C%20which%20often%20leads%20to%20biased%2C%20%60%60one-side%27%27%20interpretations.%20This%20paper%20introduces%20skyline%20explanation%2C%20a%20new%20paradigm%20that%20interprets%20GNN%20outputs%20by%20simultaneously%20optimizing%20multiple%20explainability%20measures%20of%20users%27%20interests.%20%281%29%20We%20propose%20skyline%20explanations%20as%20a%20Pareto%20set%20of%20explanatory%20subgraphs%20that%20dominate%20others%20over%20multiple%20explanatory%20measures.%20We%20formulate%20skyline%20explanation%20as%20a%20multi-criteria%20optimization%20problem%2C%20and%20establish%20its%20hardness%20results.%20%282%29%20We%20design%20efficient%20algorithms%20with%20an%20onion-peeling%20approach%2C%20which%20strategically%20prioritizes%20nodes%20and%20removes%20unpromising%20edges%20to%20incrementally%20assemble%20skyline%20explanations.%20%283%29%20We%20also%20develop%20an%20algorithm%20to%20diversify%20the%20skyline%20explanations%20to%20enrich%20the%20comprehensive%20interpretation.%20%284%29%20We%20introduce%20efficient%20parallel%20algorithms%20with%20load-balancing%20strategies%20to%20scale%20skyline%20explanation%20for%20large-scale%20GNN-based%20inference.%20Using%20real-world%20and%20synthetic%20graphs%2C%20we%20experimentally%20verify%20our%20algorithms%27%20effectiveness%20and%20scalability.&entry.1838667208=http%3A//arxiv.org/abs/2505.07635v4&entry.124074799=Read"},
{"title": "VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection", "author": "Qiang Wang and Xinyuan Gao and SongLin Dong and Jizhou Han and Jiangyang Li and Yuhang He and Yihong Gong", "abstract": "We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.", "link": "http://arxiv.org/abs/2511.19436v1", "date": "2025-11-24", "relevancy": 2.281, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5894}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5612}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VDC-Agent%3A%20When%20Video%20Detailed%20Captioners%20Evolve%20Themselves%20via%20Agentic%20Self-Reflection&body=Title%3A%20VDC-Agent%3A%20When%20Video%20Detailed%20Captioners%20Evolve%20Themselves%20via%20Agentic%20Self-Reflection%0AAuthor%3A%20Qiang%20Wang%20and%20Xinyuan%20Gao%20and%20SongLin%20Dong%20and%20Jizhou%20Han%20and%20Jiangyang%20Li%20and%20Yuhang%20He%20and%20Yihong%20Gong%0AAbstract%3A%20We%20present%20VDC-Agent%2C%20a%20self-evolving%20framework%20for%20Video%20Detailed%20Captioning%20that%20requires%20neither%20human%20annotations%20nor%20larger%20teacher%20models.%20The%20agent%20forms%20a%20closed%20loop%20of%20caption%20generation%2C%20principle-guided%20scoring%20%28score%20and%20textual%20suggestions%29%2C%20and%20prompt%20refinement.%20When%20caption%20quality%20regresses%2C%20a%20self-reflection%20path%20leverages%20the%20previous%20chain-of-thought%20to%20amend%20the%20update.%20Running%20this%20process%20on%20unlabeled%20videos%20produces%20trajectories%20of%20%28caption%2C%20score%29%20pairs.%20We%20convert%20the%20trajectories%20into%20preference%20tuples%20and%20filter%20out%20samples%20with%20JSON%20parsing%20errors%2C%20resulting%20in%20VDC-Agent-19K%2C%20which%20contains%2018%2C886%20automatically%20constructed%20pairs.%20We%20then%20fine-tune%20the%20base%20MLLM%20on%20this%20dataset%20using%20an%20easy-to-hard%20curriculum%20direct%20preference%20optimization.%20Built%20on%20Qwen2.5-VL-7B-Instruct%2C%20our%20VDC-Agent-7B%20attains%20state-of-the-art%20performance%20on%20the%20VDC%20benchmark%20with%2049.08%25%20average%20accuracy%20and%202.50%20score%2C%20surpassing%20specialized%20video%20captioners%20and%20improving%20over%20the%20base%20model%20by%20%2B5.13%25%20accuracy%20and%20%2B0.27%20score%20at%20similar%20inference%20cost.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVDC-Agent%253A%2520When%2520Video%2520Detailed%2520Captioners%2520Evolve%2520Themselves%2520via%2520Agentic%2520Self-Reflection%26entry.906535625%3DQiang%2520Wang%2520and%2520Xinyuan%2520Gao%2520and%2520SongLin%2520Dong%2520and%2520Jizhou%2520Han%2520and%2520Jiangyang%2520Li%2520and%2520Yuhang%2520He%2520and%2520Yihong%2520Gong%26entry.1292438233%3DWe%2520present%2520VDC-Agent%252C%2520a%2520self-evolving%2520framework%2520for%2520Video%2520Detailed%2520Captioning%2520that%2520requires%2520neither%2520human%2520annotations%2520nor%2520larger%2520teacher%2520models.%2520The%2520agent%2520forms%2520a%2520closed%2520loop%2520of%2520caption%2520generation%252C%2520principle-guided%2520scoring%2520%2528score%2520and%2520textual%2520suggestions%2529%252C%2520and%2520prompt%2520refinement.%2520When%2520caption%2520quality%2520regresses%252C%2520a%2520self-reflection%2520path%2520leverages%2520the%2520previous%2520chain-of-thought%2520to%2520amend%2520the%2520update.%2520Running%2520this%2520process%2520on%2520unlabeled%2520videos%2520produces%2520trajectories%2520of%2520%2528caption%252C%2520score%2529%2520pairs.%2520We%2520convert%2520the%2520trajectories%2520into%2520preference%2520tuples%2520and%2520filter%2520out%2520samples%2520with%2520JSON%2520parsing%2520errors%252C%2520resulting%2520in%2520VDC-Agent-19K%252C%2520which%2520contains%252018%252C886%2520automatically%2520constructed%2520pairs.%2520We%2520then%2520fine-tune%2520the%2520base%2520MLLM%2520on%2520this%2520dataset%2520using%2520an%2520easy-to-hard%2520curriculum%2520direct%2520preference%2520optimization.%2520Built%2520on%2520Qwen2.5-VL-7B-Instruct%252C%2520our%2520VDC-Agent-7B%2520attains%2520state-of-the-art%2520performance%2520on%2520the%2520VDC%2520benchmark%2520with%252049.08%2525%2520average%2520accuracy%2520and%25202.50%2520score%252C%2520surpassing%2520specialized%2520video%2520captioners%2520and%2520improving%2520over%2520the%2520base%2520model%2520by%2520%252B5.13%2525%2520accuracy%2520and%2520%252B0.27%2520score%2520at%2520similar%2520inference%2520cost.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VDC-Agent%3A%20When%20Video%20Detailed%20Captioners%20Evolve%20Themselves%20via%20Agentic%20Self-Reflection&entry.906535625=Qiang%20Wang%20and%20Xinyuan%20Gao%20and%20SongLin%20Dong%20and%20Jizhou%20Han%20and%20Jiangyang%20Li%20and%20Yuhang%20He%20and%20Yihong%20Gong&entry.1292438233=We%20present%20VDC-Agent%2C%20a%20self-evolving%20framework%20for%20Video%20Detailed%20Captioning%20that%20requires%20neither%20human%20annotations%20nor%20larger%20teacher%20models.%20The%20agent%20forms%20a%20closed%20loop%20of%20caption%20generation%2C%20principle-guided%20scoring%20%28score%20and%20textual%20suggestions%29%2C%20and%20prompt%20refinement.%20When%20caption%20quality%20regresses%2C%20a%20self-reflection%20path%20leverages%20the%20previous%20chain-of-thought%20to%20amend%20the%20update.%20Running%20this%20process%20on%20unlabeled%20videos%20produces%20trajectories%20of%20%28caption%2C%20score%29%20pairs.%20We%20convert%20the%20trajectories%20into%20preference%20tuples%20and%20filter%20out%20samples%20with%20JSON%20parsing%20errors%2C%20resulting%20in%20VDC-Agent-19K%2C%20which%20contains%2018%2C886%20automatically%20constructed%20pairs.%20We%20then%20fine-tune%20the%20base%20MLLM%20on%20this%20dataset%20using%20an%20easy-to-hard%20curriculum%20direct%20preference%20optimization.%20Built%20on%20Qwen2.5-VL-7B-Instruct%2C%20our%20VDC-Agent-7B%20attains%20state-of-the-art%20performance%20on%20the%20VDC%20benchmark%20with%2049.08%25%20average%20accuracy%20and%202.50%20score%2C%20surpassing%20specialized%20video%20captioners%20and%20improving%20over%20the%20base%20model%20by%20%2B5.13%25%20accuracy%20and%20%2B0.27%20score%20at%20similar%20inference%20cost.&entry.1838667208=http%3A//arxiv.org/abs/2511.19436v1&entry.124074799=Read"},
{"title": "VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval", "author": "Dhiman Paul and Md Rizwan Parvez and Nabeel Mohammed and Shafin Rahman", "abstract": "Prevailing joint prediction transformers for Video Highlight Detection and Moment Retrieval (HD/MR) exhibit deficiencies in handling cross-task dynamics, achieving robust video-text alignment, and utilizing effective attention mechanisms, with the potential of Large Language/Vision-Language Models (LLMs/LVLMs) being largely untapped. This paper introduces VideoLights, a novel HD/MR framework addressing these limitations by incorporating: (i) Convolutional Projection and Feature Refinement modules with an alignment loss for enhanced video-text feature congruity; (ii) a Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware representations; (iii) a Uni-directional joint-task feedback mechanism for synergistic task improvement; (iv) hard positive/negative losses for adaptive learning; and (v) the leveraging of LVLMs (e.g., BLIP-2) for superior multimodal feature integration and intelligent pre-training with synthetic data. Comprehensive evaluations on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate that VideoLights significantly surpasses existing baselines, establishing new state-of-the-art performances. Codes and model checkpoints are available at https://github.com/dpaul06/VideoLights .", "link": "http://arxiv.org/abs/2412.01558v2", "date": "2025-11-24", "relevancy": 2.2779, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5774}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5716}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoLights%3A%20Feature%20Refinement%20and%20Cross-Task%20Alignment%20Transformer%20for%20Joint%20Video%20Highlight%20Detection%20and%20Moment%20Retrieval&body=Title%3A%20VideoLights%3A%20Feature%20Refinement%20and%20Cross-Task%20Alignment%20Transformer%20for%20Joint%20Video%20Highlight%20Detection%20and%20Moment%20Retrieval%0AAuthor%3A%20Dhiman%20Paul%20and%20Md%20Rizwan%20Parvez%20and%20Nabeel%20Mohammed%20and%20Shafin%20Rahman%0AAbstract%3A%20Prevailing%20joint%20prediction%20transformers%20for%20Video%20Highlight%20Detection%20and%20Moment%20Retrieval%20%28HD/MR%29%20exhibit%20deficiencies%20in%20handling%20cross-task%20dynamics%2C%20achieving%20robust%20video-text%20alignment%2C%20and%20utilizing%20effective%20attention%20mechanisms%2C%20with%20the%20potential%20of%20Large%20Language/Vision-Language%20Models%20%28LLMs/LVLMs%29%20being%20largely%20untapped.%20This%20paper%20introduces%20VideoLights%2C%20a%20novel%20HD/MR%20framework%20addressing%20these%20limitations%20by%20incorporating%3A%20%28i%29%20Convolutional%20Projection%20and%20Feature%20Refinement%20modules%20with%20an%20alignment%20loss%20for%20enhanced%20video-text%20feature%20congruity%3B%20%28ii%29%20a%20Bi-Directional%20Cross-Modal%20Fusion%20network%20for%20strongly%20coupled%20query-aware%20representations%3B%20%28iii%29%20a%20Uni-directional%20joint-task%20feedback%20mechanism%20for%20synergistic%20task%20improvement%3B%20%28iv%29%20hard%20positive/negative%20losses%20for%20adaptive%20learning%3B%20and%20%28v%29%20the%20leveraging%20of%20LVLMs%20%28e.g.%2C%20BLIP-2%29%20for%20superior%20multimodal%20feature%20integration%20and%20intelligent%20pre-training%20with%20synthetic%20data.%20Comprehensive%20evaluations%20on%20QVHighlights%2C%20TVSum%2C%20and%20Charades-STA%20benchmarks%20demonstrate%20that%20VideoLights%20significantly%20surpasses%20existing%20baselines%2C%20establishing%20new%20state-of-the-art%20performances.%20Codes%20and%20model%20checkpoints%20are%20available%20at%20https%3A//github.com/dpaul06/VideoLights%20.%0ALink%3A%20http%3A//arxiv.org/abs/2412.01558v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoLights%253A%2520Feature%2520Refinement%2520and%2520Cross-Task%2520Alignment%2520Transformer%2520for%2520Joint%2520Video%2520Highlight%2520Detection%2520and%2520Moment%2520Retrieval%26entry.906535625%3DDhiman%2520Paul%2520and%2520Md%2520Rizwan%2520Parvez%2520and%2520Nabeel%2520Mohammed%2520and%2520Shafin%2520Rahman%26entry.1292438233%3DPrevailing%2520joint%2520prediction%2520transformers%2520for%2520Video%2520Highlight%2520Detection%2520and%2520Moment%2520Retrieval%2520%2528HD/MR%2529%2520exhibit%2520deficiencies%2520in%2520handling%2520cross-task%2520dynamics%252C%2520achieving%2520robust%2520video-text%2520alignment%252C%2520and%2520utilizing%2520effective%2520attention%2520mechanisms%252C%2520with%2520the%2520potential%2520of%2520Large%2520Language/Vision-Language%2520Models%2520%2528LLMs/LVLMs%2529%2520being%2520largely%2520untapped.%2520This%2520paper%2520introduces%2520VideoLights%252C%2520a%2520novel%2520HD/MR%2520framework%2520addressing%2520these%2520limitations%2520by%2520incorporating%253A%2520%2528i%2529%2520Convolutional%2520Projection%2520and%2520Feature%2520Refinement%2520modules%2520with%2520an%2520alignment%2520loss%2520for%2520enhanced%2520video-text%2520feature%2520congruity%253B%2520%2528ii%2529%2520a%2520Bi-Directional%2520Cross-Modal%2520Fusion%2520network%2520for%2520strongly%2520coupled%2520query-aware%2520representations%253B%2520%2528iii%2529%2520a%2520Uni-directional%2520joint-task%2520feedback%2520mechanism%2520for%2520synergistic%2520task%2520improvement%253B%2520%2528iv%2529%2520hard%2520positive/negative%2520losses%2520for%2520adaptive%2520learning%253B%2520and%2520%2528v%2529%2520the%2520leveraging%2520of%2520LVLMs%2520%2528e.g.%252C%2520BLIP-2%2529%2520for%2520superior%2520multimodal%2520feature%2520integration%2520and%2520intelligent%2520pre-training%2520with%2520synthetic%2520data.%2520Comprehensive%2520evaluations%2520on%2520QVHighlights%252C%2520TVSum%252C%2520and%2520Charades-STA%2520benchmarks%2520demonstrate%2520that%2520VideoLights%2520significantly%2520surpasses%2520existing%2520baselines%252C%2520establishing%2520new%2520state-of-the-art%2520performances.%2520Codes%2520and%2520model%2520checkpoints%2520are%2520available%2520at%2520https%253A//github.com/dpaul06/VideoLights%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01558v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoLights%3A%20Feature%20Refinement%20and%20Cross-Task%20Alignment%20Transformer%20for%20Joint%20Video%20Highlight%20Detection%20and%20Moment%20Retrieval&entry.906535625=Dhiman%20Paul%20and%20Md%20Rizwan%20Parvez%20and%20Nabeel%20Mohammed%20and%20Shafin%20Rahman&entry.1292438233=Prevailing%20joint%20prediction%20transformers%20for%20Video%20Highlight%20Detection%20and%20Moment%20Retrieval%20%28HD/MR%29%20exhibit%20deficiencies%20in%20handling%20cross-task%20dynamics%2C%20achieving%20robust%20video-text%20alignment%2C%20and%20utilizing%20effective%20attention%20mechanisms%2C%20with%20the%20potential%20of%20Large%20Language/Vision-Language%20Models%20%28LLMs/LVLMs%29%20being%20largely%20untapped.%20This%20paper%20introduces%20VideoLights%2C%20a%20novel%20HD/MR%20framework%20addressing%20these%20limitations%20by%20incorporating%3A%20%28i%29%20Convolutional%20Projection%20and%20Feature%20Refinement%20modules%20with%20an%20alignment%20loss%20for%20enhanced%20video-text%20feature%20congruity%3B%20%28ii%29%20a%20Bi-Directional%20Cross-Modal%20Fusion%20network%20for%20strongly%20coupled%20query-aware%20representations%3B%20%28iii%29%20a%20Uni-directional%20joint-task%20feedback%20mechanism%20for%20synergistic%20task%20improvement%3B%20%28iv%29%20hard%20positive/negative%20losses%20for%20adaptive%20learning%3B%20and%20%28v%29%20the%20leveraging%20of%20LVLMs%20%28e.g.%2C%20BLIP-2%29%20for%20superior%20multimodal%20feature%20integration%20and%20intelligent%20pre-training%20with%20synthetic%20data.%20Comprehensive%20evaluations%20on%20QVHighlights%2C%20TVSum%2C%20and%20Charades-STA%20benchmarks%20demonstrate%20that%20VideoLights%20significantly%20surpasses%20existing%20baselines%2C%20establishing%20new%20state-of-the-art%20performances.%20Codes%20and%20model%20checkpoints%20are%20available%20at%20https%3A//github.com/dpaul06/VideoLights%20.&entry.1838667208=http%3A//arxiv.org/abs/2412.01558v2&entry.124074799=Read"},
{"title": "TokenCLIP: Token-wise Prompt Learning for Zero-shot Anomaly Detection", "author": "Qihang Zhou and Binbin Gao and Guansong Pang and Xin Wang and Jiming Chen and Shibo He", "abstract": "Adapting CLIP for anomaly detection on unseen objects has shown strong potential in a zero-shot manner. However, existing methods typically rely on a single textual space to align with visual semantics across diverse objects and domains. The indiscriminate alignment hinders the model from accurately capturing varied anomaly semantics. We propose TokenCLIP, a token-wise adaptation framework that enables dynamic alignment between visual and learnable textual spaces for fine-grained anomaly learning. Rather than mapping all visual tokens to a single, token-agnostic textual space, TokenCLIP aligns each token with a customized textual subspace that represents its visual characteristics. Explicitly assigning a unique learnable textual space to each token is computationally intractable and prone to insufficient optimization. We instead expand the token-agnostic textual space into a set of orthogonal subspaces, and then dynamically assign each token to a subspace combination guided by semantic affinity, which jointly supports customized and efficient token-wise adaptation. To this end, we formulate dynamic alignment as an optimal transport problem, where all visual tokens in an image are transported to textual subspaces based on semantic similarity. The transport constraints of OT ensure sufficient optimization across subspaces and encourage them to focus on different semantics. Solving the problem yields a transport plan that adaptively assigns each token to semantically relevant subspaces. A top-k masking is then applied to sparsify the plan and specialize subspaces for distinct visual regions. Extensive experiments demonstrate the superiority of TokenCLIP.", "link": "http://arxiv.org/abs/2510.21171v3", "date": "2025-11-24", "relevancy": 2.2776, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6096}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.541}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokenCLIP%3A%20Token-wise%20Prompt%20Learning%20for%20Zero-shot%20Anomaly%20Detection&body=Title%3A%20TokenCLIP%3A%20Token-wise%20Prompt%20Learning%20for%20Zero-shot%20Anomaly%20Detection%0AAuthor%3A%20Qihang%20Zhou%20and%20Binbin%20Gao%20and%20Guansong%20Pang%20and%20Xin%20Wang%20and%20Jiming%20Chen%20and%20Shibo%20He%0AAbstract%3A%20Adapting%20CLIP%20for%20anomaly%20detection%20on%20unseen%20objects%20has%20shown%20strong%20potential%20in%20a%20zero-shot%20manner.%20However%2C%20existing%20methods%20typically%20rely%20on%20a%20single%20textual%20space%20to%20align%20with%20visual%20semantics%20across%20diverse%20objects%20and%20domains.%20The%20indiscriminate%20alignment%20hinders%20the%20model%20from%20accurately%20capturing%20varied%20anomaly%20semantics.%20We%20propose%20TokenCLIP%2C%20a%20token-wise%20adaptation%20framework%20that%20enables%20dynamic%20alignment%20between%20visual%20and%20learnable%20textual%20spaces%20for%20fine-grained%20anomaly%20learning.%20Rather%20than%20mapping%20all%20visual%20tokens%20to%20a%20single%2C%20token-agnostic%20textual%20space%2C%20TokenCLIP%20aligns%20each%20token%20with%20a%20customized%20textual%20subspace%20that%20represents%20its%20visual%20characteristics.%20Explicitly%20assigning%20a%20unique%20learnable%20textual%20space%20to%20each%20token%20is%20computationally%20intractable%20and%20prone%20to%20insufficient%20optimization.%20We%20instead%20expand%20the%20token-agnostic%20textual%20space%20into%20a%20set%20of%20orthogonal%20subspaces%2C%20and%20then%20dynamically%20assign%20each%20token%20to%20a%20subspace%20combination%20guided%20by%20semantic%20affinity%2C%20which%20jointly%20supports%20customized%20and%20efficient%20token-wise%20adaptation.%20To%20this%20end%2C%20we%20formulate%20dynamic%20alignment%20as%20an%20optimal%20transport%20problem%2C%20where%20all%20visual%20tokens%20in%20an%20image%20are%20transported%20to%20textual%20subspaces%20based%20on%20semantic%20similarity.%20The%20transport%20constraints%20of%20OT%20ensure%20sufficient%20optimization%20across%20subspaces%20and%20encourage%20them%20to%20focus%20on%20different%20semantics.%20Solving%20the%20problem%20yields%20a%20transport%20plan%20that%20adaptively%20assigns%20each%20token%20to%20semantically%20relevant%20subspaces.%20A%20top-k%20masking%20is%20then%20applied%20to%20sparsify%20the%20plan%20and%20specialize%20subspaces%20for%20distinct%20visual%20regions.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20TokenCLIP.%0ALink%3A%20http%3A//arxiv.org/abs/2510.21171v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenCLIP%253A%2520Token-wise%2520Prompt%2520Learning%2520for%2520Zero-shot%2520Anomaly%2520Detection%26entry.906535625%3DQihang%2520Zhou%2520and%2520Binbin%2520Gao%2520and%2520Guansong%2520Pang%2520and%2520Xin%2520Wang%2520and%2520Jiming%2520Chen%2520and%2520Shibo%2520He%26entry.1292438233%3DAdapting%2520CLIP%2520for%2520anomaly%2520detection%2520on%2520unseen%2520objects%2520has%2520shown%2520strong%2520potential%2520in%2520a%2520zero-shot%2520manner.%2520However%252C%2520existing%2520methods%2520typically%2520rely%2520on%2520a%2520single%2520textual%2520space%2520to%2520align%2520with%2520visual%2520semantics%2520across%2520diverse%2520objects%2520and%2520domains.%2520The%2520indiscriminate%2520alignment%2520hinders%2520the%2520model%2520from%2520accurately%2520capturing%2520varied%2520anomaly%2520semantics.%2520We%2520propose%2520TokenCLIP%252C%2520a%2520token-wise%2520adaptation%2520framework%2520that%2520enables%2520dynamic%2520alignment%2520between%2520visual%2520and%2520learnable%2520textual%2520spaces%2520for%2520fine-grained%2520anomaly%2520learning.%2520Rather%2520than%2520mapping%2520all%2520visual%2520tokens%2520to%2520a%2520single%252C%2520token-agnostic%2520textual%2520space%252C%2520TokenCLIP%2520aligns%2520each%2520token%2520with%2520a%2520customized%2520textual%2520subspace%2520that%2520represents%2520its%2520visual%2520characteristics.%2520Explicitly%2520assigning%2520a%2520unique%2520learnable%2520textual%2520space%2520to%2520each%2520token%2520is%2520computationally%2520intractable%2520and%2520prone%2520to%2520insufficient%2520optimization.%2520We%2520instead%2520expand%2520the%2520token-agnostic%2520textual%2520space%2520into%2520a%2520set%2520of%2520orthogonal%2520subspaces%252C%2520and%2520then%2520dynamically%2520assign%2520each%2520token%2520to%2520a%2520subspace%2520combination%2520guided%2520by%2520semantic%2520affinity%252C%2520which%2520jointly%2520supports%2520customized%2520and%2520efficient%2520token-wise%2520adaptation.%2520To%2520this%2520end%252C%2520we%2520formulate%2520dynamic%2520alignment%2520as%2520an%2520optimal%2520transport%2520problem%252C%2520where%2520all%2520visual%2520tokens%2520in%2520an%2520image%2520are%2520transported%2520to%2520textual%2520subspaces%2520based%2520on%2520semantic%2520similarity.%2520The%2520transport%2520constraints%2520of%2520OT%2520ensure%2520sufficient%2520optimization%2520across%2520subspaces%2520and%2520encourage%2520them%2520to%2520focus%2520on%2520different%2520semantics.%2520Solving%2520the%2520problem%2520yields%2520a%2520transport%2520plan%2520that%2520adaptively%2520assigns%2520each%2520token%2520to%2520semantically%2520relevant%2520subspaces.%2520A%2520top-k%2520masking%2520is%2520then%2520applied%2520to%2520sparsify%2520the%2520plan%2520and%2520specialize%2520subspaces%2520for%2520distinct%2520visual%2520regions.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520TokenCLIP.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.21171v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokenCLIP%3A%20Token-wise%20Prompt%20Learning%20for%20Zero-shot%20Anomaly%20Detection&entry.906535625=Qihang%20Zhou%20and%20Binbin%20Gao%20and%20Guansong%20Pang%20and%20Xin%20Wang%20and%20Jiming%20Chen%20and%20Shibo%20He&entry.1292438233=Adapting%20CLIP%20for%20anomaly%20detection%20on%20unseen%20objects%20has%20shown%20strong%20potential%20in%20a%20zero-shot%20manner.%20However%2C%20existing%20methods%20typically%20rely%20on%20a%20single%20textual%20space%20to%20align%20with%20visual%20semantics%20across%20diverse%20objects%20and%20domains.%20The%20indiscriminate%20alignment%20hinders%20the%20model%20from%20accurately%20capturing%20varied%20anomaly%20semantics.%20We%20propose%20TokenCLIP%2C%20a%20token-wise%20adaptation%20framework%20that%20enables%20dynamic%20alignment%20between%20visual%20and%20learnable%20textual%20spaces%20for%20fine-grained%20anomaly%20learning.%20Rather%20than%20mapping%20all%20visual%20tokens%20to%20a%20single%2C%20token-agnostic%20textual%20space%2C%20TokenCLIP%20aligns%20each%20token%20with%20a%20customized%20textual%20subspace%20that%20represents%20its%20visual%20characteristics.%20Explicitly%20assigning%20a%20unique%20learnable%20textual%20space%20to%20each%20token%20is%20computationally%20intractable%20and%20prone%20to%20insufficient%20optimization.%20We%20instead%20expand%20the%20token-agnostic%20textual%20space%20into%20a%20set%20of%20orthogonal%20subspaces%2C%20and%20then%20dynamically%20assign%20each%20token%20to%20a%20subspace%20combination%20guided%20by%20semantic%20affinity%2C%20which%20jointly%20supports%20customized%20and%20efficient%20token-wise%20adaptation.%20To%20this%20end%2C%20we%20formulate%20dynamic%20alignment%20as%20an%20optimal%20transport%20problem%2C%20where%20all%20visual%20tokens%20in%20an%20image%20are%20transported%20to%20textual%20subspaces%20based%20on%20semantic%20similarity.%20The%20transport%20constraints%20of%20OT%20ensure%20sufficient%20optimization%20across%20subspaces%20and%20encourage%20them%20to%20focus%20on%20different%20semantics.%20Solving%20the%20problem%20yields%20a%20transport%20plan%20that%20adaptively%20assigns%20each%20token%20to%20semantically%20relevant%20subspaces.%20A%20top-k%20masking%20is%20then%20applied%20to%20sparsify%20the%20plan%20and%20specialize%20subspaces%20for%20distinct%20visual%20regions.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20TokenCLIP.&entry.1838667208=http%3A//arxiv.org/abs/2510.21171v3&entry.124074799=Read"},
{"title": "A Survey of Generative Categories and Techniques in Multimodal Generative Models", "author": "Longzhen Han and Awes Mubarak and Almas Baimagambetov and Nikolaos Polatidis and Thar Baker", "abstract": "Multimodal Generative Models (MGMs) have rapidly evolved beyond text generation, now spanning diverse output modalities including images, music, video, human motion, and 3D objects, by integrating language with other sensory modalities under unified architectures. This survey categorises six primary generative modalities and examines how foundational techniques, namely Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting, enable cross-modal capabilities. We analyze key models, architectural trends, and emergent cross-modal synergies, while highlighting transferable techniques and unresolved challenges. Building on a common taxonomy of models and training recipes, we propose a unified evaluation framework centred on faithfulness, compositionality, and robustness, and synthesise evidence from benchmarks and human studies across modalities. We further analyse trustworthiness, safety, and ethical risks, including multimodal bias, privacy leakage, and the misuse of high-fidelity media generation for deepfakes, disinformation, and copyright infringement in music and 3D assets, together with emerging mitigation strategies. Finally, we discuss how architectural trends, evaluation protocols, and governance mechanisms can be co-designed to close current capability and safety gaps, outlining critical paths toward more general-purpose, controllable, and accountable multimodal generative systems.", "link": "http://arxiv.org/abs/2506.10016v3", "date": "2025-11-24", "relevancy": 2.2745, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5763}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5706}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Generative%20Categories%20and%20Techniques%20in%20Multimodal%20Generative%20Models&body=Title%3A%20A%20Survey%20of%20Generative%20Categories%20and%20Techniques%20in%20Multimodal%20Generative%20Models%0AAuthor%3A%20Longzhen%20Han%20and%20Awes%20Mubarak%20and%20Almas%20Baimagambetov%20and%20Nikolaos%20Polatidis%20and%20Thar%20Baker%0AAbstract%3A%20Multimodal%20Generative%20Models%20%28MGMs%29%20have%20rapidly%20evolved%20beyond%20text%20generation%2C%20now%20spanning%20diverse%20output%20modalities%20including%20images%2C%20music%2C%20video%2C%20human%20motion%2C%20and%203D%20objects%2C%20by%20integrating%20language%20with%20other%20sensory%20modalities%20under%20unified%20architectures.%20This%20survey%20categorises%20six%20primary%20generative%20modalities%20and%20examines%20how%20foundational%20techniques%2C%20namely%20Self-Supervised%20Learning%20%28SSL%29%2C%20Mixture%20of%20Experts%20%28MoE%29%2C%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%2C%20and%20Chain-of-Thought%20%28CoT%29%20prompting%2C%20enable%20cross-modal%20capabilities.%20We%20analyze%20key%20models%2C%20architectural%20trends%2C%20and%20emergent%20cross-modal%20synergies%2C%20while%20highlighting%20transferable%20techniques%20and%20unresolved%20challenges.%20Building%20on%20a%20common%20taxonomy%20of%20models%20and%20training%20recipes%2C%20we%20propose%20a%20unified%20evaluation%20framework%20centred%20on%20faithfulness%2C%20compositionality%2C%20and%20robustness%2C%20and%20synthesise%20evidence%20from%20benchmarks%20and%20human%20studies%20across%20modalities.%20We%20further%20analyse%20trustworthiness%2C%20safety%2C%20and%20ethical%20risks%2C%20including%20multimodal%20bias%2C%20privacy%20leakage%2C%20and%20the%20misuse%20of%20high-fidelity%20media%20generation%20for%20deepfakes%2C%20disinformation%2C%20and%20copyright%20infringement%20in%20music%20and%203D%20assets%2C%20together%20with%20emerging%20mitigation%20strategies.%20Finally%2C%20we%20discuss%20how%20architectural%20trends%2C%20evaluation%20protocols%2C%20and%20governance%20mechanisms%20can%20be%20co-designed%20to%20close%20current%20capability%20and%20safety%20gaps%2C%20outlining%20critical%20paths%20toward%20more%20general-purpose%2C%20controllable%2C%20and%20accountable%20multimodal%20generative%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2506.10016v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Generative%2520Categories%2520and%2520Techniques%2520in%2520Multimodal%2520Generative%2520Models%26entry.906535625%3DLongzhen%2520Han%2520and%2520Awes%2520Mubarak%2520and%2520Almas%2520Baimagambetov%2520and%2520Nikolaos%2520Polatidis%2520and%2520Thar%2520Baker%26entry.1292438233%3DMultimodal%2520Generative%2520Models%2520%2528MGMs%2529%2520have%2520rapidly%2520evolved%2520beyond%2520text%2520generation%252C%2520now%2520spanning%2520diverse%2520output%2520modalities%2520including%2520images%252C%2520music%252C%2520video%252C%2520human%2520motion%252C%2520and%25203D%2520objects%252C%2520by%2520integrating%2520language%2520with%2520other%2520sensory%2520modalities%2520under%2520unified%2520architectures.%2520This%2520survey%2520categorises%2520six%2520primary%2520generative%2520modalities%2520and%2520examines%2520how%2520foundational%2520techniques%252C%2520namely%2520Self-Supervised%2520Learning%2520%2528SSL%2529%252C%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%252C%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%252C%2520and%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%252C%2520enable%2520cross-modal%2520capabilities.%2520We%2520analyze%2520key%2520models%252C%2520architectural%2520trends%252C%2520and%2520emergent%2520cross-modal%2520synergies%252C%2520while%2520highlighting%2520transferable%2520techniques%2520and%2520unresolved%2520challenges.%2520Building%2520on%2520a%2520common%2520taxonomy%2520of%2520models%2520and%2520training%2520recipes%252C%2520we%2520propose%2520a%2520unified%2520evaluation%2520framework%2520centred%2520on%2520faithfulness%252C%2520compositionality%252C%2520and%2520robustness%252C%2520and%2520synthesise%2520evidence%2520from%2520benchmarks%2520and%2520human%2520studies%2520across%2520modalities.%2520We%2520further%2520analyse%2520trustworthiness%252C%2520safety%252C%2520and%2520ethical%2520risks%252C%2520including%2520multimodal%2520bias%252C%2520privacy%2520leakage%252C%2520and%2520the%2520misuse%2520of%2520high-fidelity%2520media%2520generation%2520for%2520deepfakes%252C%2520disinformation%252C%2520and%2520copyright%2520infringement%2520in%2520music%2520and%25203D%2520assets%252C%2520together%2520with%2520emerging%2520mitigation%2520strategies.%2520Finally%252C%2520we%2520discuss%2520how%2520architectural%2520trends%252C%2520evaluation%2520protocols%252C%2520and%2520governance%2520mechanisms%2520can%2520be%2520co-designed%2520to%2520close%2520current%2520capability%2520and%2520safety%2520gaps%252C%2520outlining%2520critical%2520paths%2520toward%2520more%2520general-purpose%252C%2520controllable%252C%2520and%2520accountable%2520multimodal%2520generative%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10016v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Generative%20Categories%20and%20Techniques%20in%20Multimodal%20Generative%20Models&entry.906535625=Longzhen%20Han%20and%20Awes%20Mubarak%20and%20Almas%20Baimagambetov%20and%20Nikolaos%20Polatidis%20and%20Thar%20Baker&entry.1292438233=Multimodal%20Generative%20Models%20%28MGMs%29%20have%20rapidly%20evolved%20beyond%20text%20generation%2C%20now%20spanning%20diverse%20output%20modalities%20including%20images%2C%20music%2C%20video%2C%20human%20motion%2C%20and%203D%20objects%2C%20by%20integrating%20language%20with%20other%20sensory%20modalities%20under%20unified%20architectures.%20This%20survey%20categorises%20six%20primary%20generative%20modalities%20and%20examines%20how%20foundational%20techniques%2C%20namely%20Self-Supervised%20Learning%20%28SSL%29%2C%20Mixture%20of%20Experts%20%28MoE%29%2C%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%2C%20and%20Chain-of-Thought%20%28CoT%29%20prompting%2C%20enable%20cross-modal%20capabilities.%20We%20analyze%20key%20models%2C%20architectural%20trends%2C%20and%20emergent%20cross-modal%20synergies%2C%20while%20highlighting%20transferable%20techniques%20and%20unresolved%20challenges.%20Building%20on%20a%20common%20taxonomy%20of%20models%20and%20training%20recipes%2C%20we%20propose%20a%20unified%20evaluation%20framework%20centred%20on%20faithfulness%2C%20compositionality%2C%20and%20robustness%2C%20and%20synthesise%20evidence%20from%20benchmarks%20and%20human%20studies%20across%20modalities.%20We%20further%20analyse%20trustworthiness%2C%20safety%2C%20and%20ethical%20risks%2C%20including%20multimodal%20bias%2C%20privacy%20leakage%2C%20and%20the%20misuse%20of%20high-fidelity%20media%20generation%20for%20deepfakes%2C%20disinformation%2C%20and%20copyright%20infringement%20in%20music%20and%203D%20assets%2C%20together%20with%20emerging%20mitigation%20strategies.%20Finally%2C%20we%20discuss%20how%20architectural%20trends%2C%20evaluation%20protocols%2C%20and%20governance%20mechanisms%20can%20be%20co-designed%20to%20close%20current%20capability%20and%20safety%20gaps%2C%20outlining%20critical%20paths%20toward%20more%20general-purpose%2C%20controllable%2C%20and%20accountable%20multimodal%20generative%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2506.10016v3&entry.124074799=Read"},
{"title": "LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space", "author": "Hai Wu and Shuai Tang and Jiale Wang and Longkun Zou and Mingyue Guo and Rongqin Liang and Ke Chen and Yaowei Wang", "abstract": "Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding. However, datasets tailored for 3D LAA perception remain scarce. To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles. LAA3D contains 15,000 real images and 600,000 synthetic frames, captured across diverse scenarios, including urban and suburban environments. It covers multiple aerial object categories, including electric Vertical Take-Off and Landing (eVTOL) aircraft, Micro Aerial Vehicles (MAVs), and Helicopters. Each instance is annotated with 3D bounding box, class label, and instance identity, supporting tasks such as 3D object detection, 3D multi-object tracking (MOT), and 6-DoF pose estimation. Besides, we establish the LAA3D Benchmark, integrating multiple tasks and methods with unified evaluation protocols for comparison. Furthermore, we propose MonoLAA, a monocular 3D detection baseline, achieving robust 3D localization from zoom cameras with varying focal lengths. Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. Our LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception.", "link": "http://arxiv.org/abs/2511.19057v1", "date": "2025-11-24", "relevancy": 2.266, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5803}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.566}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAA3D%3A%20A%20Benchmark%20of%20Detecting%20and%20Tracking%20Low-Altitude%20Aircraft%20in%203D%20Space&body=Title%3A%20LAA3D%3A%20A%20Benchmark%20of%20Detecting%20and%20Tracking%20Low-Altitude%20Aircraft%20in%203D%20Space%0AAuthor%3A%20Hai%20Wu%20and%20Shuai%20Tang%20and%20Jiale%20Wang%20and%20Longkun%20Zou%20and%20Mingyue%20Guo%20and%20Rongqin%20Liang%20and%20Ke%20Chen%20and%20Yaowei%20Wang%0AAbstract%3A%20Perception%20of%20Low-Altitude%20Aircraft%20%28LAA%29%20in%203D%20space%20enables%20precise%203D%20object%20localization%20and%20behavior%20understanding.%20However%2C%20datasets%20tailored%20for%203D%20LAA%20perception%20remain%20scarce.%20To%20address%20this%20gap%2C%20we%20present%20LAA3D%2C%20a%20large-scale%20dataset%20designed%20to%20advance%203D%20detection%20and%20tracking%20of%20low-altitude%20aerial%20vehicles.%20LAA3D%20contains%2015%2C000%20real%20images%20and%20600%2C000%20synthetic%20frames%2C%20captured%20across%20diverse%20scenarios%2C%20including%20urban%20and%20suburban%20environments.%20It%20covers%20multiple%20aerial%20object%20categories%2C%20including%20electric%20Vertical%20Take-Off%20and%20Landing%20%28eVTOL%29%20aircraft%2C%20Micro%20Aerial%20Vehicles%20%28MAVs%29%2C%20and%20Helicopters.%20Each%20instance%20is%20annotated%20with%203D%20bounding%20box%2C%20class%20label%2C%20and%20instance%20identity%2C%20supporting%20tasks%20such%20as%203D%20object%20detection%2C%203D%20multi-object%20tracking%20%28MOT%29%2C%20and%206-DoF%20pose%20estimation.%20Besides%2C%20we%20establish%20the%20LAA3D%20Benchmark%2C%20integrating%20multiple%20tasks%20and%20methods%20with%20unified%20evaluation%20protocols%20for%20comparison.%20Furthermore%2C%20we%20propose%20MonoLAA%2C%20a%20monocular%203D%20detection%20baseline%2C%20achieving%20robust%203D%20localization%20from%20zoom%20cameras%20with%20varying%20focal%20lengths.%20Models%20pretrained%20on%20synthetic%20images%20transfer%20effectively%20to%20real-world%20data%20with%20fine-tuning%2C%20demonstrating%20strong%20sim-to-real%20generalization.%20Our%20LAA3D%20provides%20a%20comprehensive%20foundation%20for%20future%20research%20in%20low-altitude%203D%20object%20perception.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAA3D%253A%2520A%2520Benchmark%2520of%2520Detecting%2520and%2520Tracking%2520Low-Altitude%2520Aircraft%2520in%25203D%2520Space%26entry.906535625%3DHai%2520Wu%2520and%2520Shuai%2520Tang%2520and%2520Jiale%2520Wang%2520and%2520Longkun%2520Zou%2520and%2520Mingyue%2520Guo%2520and%2520Rongqin%2520Liang%2520and%2520Ke%2520Chen%2520and%2520Yaowei%2520Wang%26entry.1292438233%3DPerception%2520of%2520Low-Altitude%2520Aircraft%2520%2528LAA%2529%2520in%25203D%2520space%2520enables%2520precise%25203D%2520object%2520localization%2520and%2520behavior%2520understanding.%2520However%252C%2520datasets%2520tailored%2520for%25203D%2520LAA%2520perception%2520remain%2520scarce.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520LAA3D%252C%2520a%2520large-scale%2520dataset%2520designed%2520to%2520advance%25203D%2520detection%2520and%2520tracking%2520of%2520low-altitude%2520aerial%2520vehicles.%2520LAA3D%2520contains%252015%252C000%2520real%2520images%2520and%2520600%252C000%2520synthetic%2520frames%252C%2520captured%2520across%2520diverse%2520scenarios%252C%2520including%2520urban%2520and%2520suburban%2520environments.%2520It%2520covers%2520multiple%2520aerial%2520object%2520categories%252C%2520including%2520electric%2520Vertical%2520Take-Off%2520and%2520Landing%2520%2528eVTOL%2529%2520aircraft%252C%2520Micro%2520Aerial%2520Vehicles%2520%2528MAVs%2529%252C%2520and%2520Helicopters.%2520Each%2520instance%2520is%2520annotated%2520with%25203D%2520bounding%2520box%252C%2520class%2520label%252C%2520and%2520instance%2520identity%252C%2520supporting%2520tasks%2520such%2520as%25203D%2520object%2520detection%252C%25203D%2520multi-object%2520tracking%2520%2528MOT%2529%252C%2520and%25206-DoF%2520pose%2520estimation.%2520Besides%252C%2520we%2520establish%2520the%2520LAA3D%2520Benchmark%252C%2520integrating%2520multiple%2520tasks%2520and%2520methods%2520with%2520unified%2520evaluation%2520protocols%2520for%2520comparison.%2520Furthermore%252C%2520we%2520propose%2520MonoLAA%252C%2520a%2520monocular%25203D%2520detection%2520baseline%252C%2520achieving%2520robust%25203D%2520localization%2520from%2520zoom%2520cameras%2520with%2520varying%2520focal%2520lengths.%2520Models%2520pretrained%2520on%2520synthetic%2520images%2520transfer%2520effectively%2520to%2520real-world%2520data%2520with%2520fine-tuning%252C%2520demonstrating%2520strong%2520sim-to-real%2520generalization.%2520Our%2520LAA3D%2520provides%2520a%2520comprehensive%2520foundation%2520for%2520future%2520research%2520in%2520low-altitude%25203D%2520object%2520perception.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAA3D%3A%20A%20Benchmark%20of%20Detecting%20and%20Tracking%20Low-Altitude%20Aircraft%20in%203D%20Space&entry.906535625=Hai%20Wu%20and%20Shuai%20Tang%20and%20Jiale%20Wang%20and%20Longkun%20Zou%20and%20Mingyue%20Guo%20and%20Rongqin%20Liang%20and%20Ke%20Chen%20and%20Yaowei%20Wang&entry.1292438233=Perception%20of%20Low-Altitude%20Aircraft%20%28LAA%29%20in%203D%20space%20enables%20precise%203D%20object%20localization%20and%20behavior%20understanding.%20However%2C%20datasets%20tailored%20for%203D%20LAA%20perception%20remain%20scarce.%20To%20address%20this%20gap%2C%20we%20present%20LAA3D%2C%20a%20large-scale%20dataset%20designed%20to%20advance%203D%20detection%20and%20tracking%20of%20low-altitude%20aerial%20vehicles.%20LAA3D%20contains%2015%2C000%20real%20images%20and%20600%2C000%20synthetic%20frames%2C%20captured%20across%20diverse%20scenarios%2C%20including%20urban%20and%20suburban%20environments.%20It%20covers%20multiple%20aerial%20object%20categories%2C%20including%20electric%20Vertical%20Take-Off%20and%20Landing%20%28eVTOL%29%20aircraft%2C%20Micro%20Aerial%20Vehicles%20%28MAVs%29%2C%20and%20Helicopters.%20Each%20instance%20is%20annotated%20with%203D%20bounding%20box%2C%20class%20label%2C%20and%20instance%20identity%2C%20supporting%20tasks%20such%20as%203D%20object%20detection%2C%203D%20multi-object%20tracking%20%28MOT%29%2C%20and%206-DoF%20pose%20estimation.%20Besides%2C%20we%20establish%20the%20LAA3D%20Benchmark%2C%20integrating%20multiple%20tasks%20and%20methods%20with%20unified%20evaluation%20protocols%20for%20comparison.%20Furthermore%2C%20we%20propose%20MonoLAA%2C%20a%20monocular%203D%20detection%20baseline%2C%20achieving%20robust%203D%20localization%20from%20zoom%20cameras%20with%20varying%20focal%20lengths.%20Models%20pretrained%20on%20synthetic%20images%20transfer%20effectively%20to%20real-world%20data%20with%20fine-tuning%2C%20demonstrating%20strong%20sim-to-real%20generalization.%20Our%20LAA3D%20provides%20a%20comprehensive%20foundation%20for%20future%20research%20in%20low-altitude%203D%20object%20perception.&entry.1838667208=http%3A//arxiv.org/abs/2511.19057v1&entry.124074799=Read"},
{"title": "A Target-based Multi-LiDAR Multi-Camera Extrinsic Calibration System", "author": "Lorenzo Gentilini and Pierpaolo Serio and Valentina Donzella and Lorenzo Pollini", "abstract": "Extrinsic Calibration represents the cornerstone of autonomous driving. Its accuracy plays a crucial role in the perception pipeline, as any errors can have implications for the safety of the vehicle. Modern sensor systems collect different types of data from the environment, making it harder to align the data. To this end, we propose a target-based extrinsic calibration system tailored for a multi-LiDAR and multi-camera sensor suite. This system enables cross-calibration between LiDARs and cameras with limited prior knowledge using a custom ChArUco board and a tailored nonlinear optimization method. We test the system with real-world data gathered in a warehouse. Results demonstrated the effectiveness of the proposed method, highlighting the feasibility of a unique pipeline tailored for various types of sensors.", "link": "http://arxiv.org/abs/2507.16621v2", "date": "2025-11-24", "relevancy": 2.2635, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5772}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5696}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Target-based%20Multi-LiDAR%20Multi-Camera%20Extrinsic%20Calibration%20System&body=Title%3A%20A%20Target-based%20Multi-LiDAR%20Multi-Camera%20Extrinsic%20Calibration%20System%0AAuthor%3A%20Lorenzo%20Gentilini%20and%20Pierpaolo%20Serio%20and%20Valentina%20Donzella%20and%20Lorenzo%20Pollini%0AAbstract%3A%20Extrinsic%20Calibration%20represents%20the%20cornerstone%20of%20autonomous%20driving.%20Its%20accuracy%20plays%20a%20crucial%20role%20in%20the%20perception%20pipeline%2C%20as%20any%20errors%20can%20have%20implications%20for%20the%20safety%20of%20the%20vehicle.%20Modern%20sensor%20systems%20collect%20different%20types%20of%20data%20from%20the%20environment%2C%20making%20it%20harder%20to%20align%20the%20data.%20To%20this%20end%2C%20we%20propose%20a%20target-based%20extrinsic%20calibration%20system%20tailored%20for%20a%20multi-LiDAR%20and%20multi-camera%20sensor%20suite.%20This%20system%20enables%20cross-calibration%20between%20LiDARs%20and%20cameras%20with%20limited%20prior%20knowledge%20using%20a%20custom%20ChArUco%20board%20and%20a%20tailored%20nonlinear%20optimization%20method.%20We%20test%20the%20system%20with%20real-world%20data%20gathered%20in%20a%20warehouse.%20Results%20demonstrated%20the%20effectiveness%20of%20the%20proposed%20method%2C%20highlighting%20the%20feasibility%20of%20a%20unique%20pipeline%20tailored%20for%20various%20types%20of%20sensors.%0ALink%3A%20http%3A//arxiv.org/abs/2507.16621v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Target-based%2520Multi-LiDAR%2520Multi-Camera%2520Extrinsic%2520Calibration%2520System%26entry.906535625%3DLorenzo%2520Gentilini%2520and%2520Pierpaolo%2520Serio%2520and%2520Valentina%2520Donzella%2520and%2520Lorenzo%2520Pollini%26entry.1292438233%3DExtrinsic%2520Calibration%2520represents%2520the%2520cornerstone%2520of%2520autonomous%2520driving.%2520Its%2520accuracy%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520perception%2520pipeline%252C%2520as%2520any%2520errors%2520can%2520have%2520implications%2520for%2520the%2520safety%2520of%2520the%2520vehicle.%2520Modern%2520sensor%2520systems%2520collect%2520different%2520types%2520of%2520data%2520from%2520the%2520environment%252C%2520making%2520it%2520harder%2520to%2520align%2520the%2520data.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520target-based%2520extrinsic%2520calibration%2520system%2520tailored%2520for%2520a%2520multi-LiDAR%2520and%2520multi-camera%2520sensor%2520suite.%2520This%2520system%2520enables%2520cross-calibration%2520between%2520LiDARs%2520and%2520cameras%2520with%2520limited%2520prior%2520knowledge%2520using%2520a%2520custom%2520ChArUco%2520board%2520and%2520a%2520tailored%2520nonlinear%2520optimization%2520method.%2520We%2520test%2520the%2520system%2520with%2520real-world%2520data%2520gathered%2520in%2520a%2520warehouse.%2520Results%2520demonstrated%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%2520highlighting%2520the%2520feasibility%2520of%2520a%2520unique%2520pipeline%2520tailored%2520for%2520various%2520types%2520of%2520sensors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16621v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Target-based%20Multi-LiDAR%20Multi-Camera%20Extrinsic%20Calibration%20System&entry.906535625=Lorenzo%20Gentilini%20and%20Pierpaolo%20Serio%20and%20Valentina%20Donzella%20and%20Lorenzo%20Pollini&entry.1292438233=Extrinsic%20Calibration%20represents%20the%20cornerstone%20of%20autonomous%20driving.%20Its%20accuracy%20plays%20a%20crucial%20role%20in%20the%20perception%20pipeline%2C%20as%20any%20errors%20can%20have%20implications%20for%20the%20safety%20of%20the%20vehicle.%20Modern%20sensor%20systems%20collect%20different%20types%20of%20data%20from%20the%20environment%2C%20making%20it%20harder%20to%20align%20the%20data.%20To%20this%20end%2C%20we%20propose%20a%20target-based%20extrinsic%20calibration%20system%20tailored%20for%20a%20multi-LiDAR%20and%20multi-camera%20sensor%20suite.%20This%20system%20enables%20cross-calibration%20between%20LiDARs%20and%20cameras%20with%20limited%20prior%20knowledge%20using%20a%20custom%20ChArUco%20board%20and%20a%20tailored%20nonlinear%20optimization%20method.%20We%20test%20the%20system%20with%20real-world%20data%20gathered%20in%20a%20warehouse.%20Results%20demonstrated%20the%20effectiveness%20of%20the%20proposed%20method%2C%20highlighting%20the%20feasibility%20of%20a%20unique%20pipeline%20tailored%20for%20various%20types%20of%20sensors.&entry.1838667208=http%3A//arxiv.org/abs/2507.16621v2&entry.124074799=Read"},
{"title": "Node Embeddings via Neighbor Embeddings", "author": "Jan Niklas B\u00f6hm and Marius Keute and Alica Guzm\u00e1n and Sebastian Damrich and Andrew Draganov and Dmitry Kobak", "abstract": "Node embeddings are a paradigm in non-parametric graph representation learning, where graph nodes are embedded into a given vector space to enable downstream processing. State-of-the-art node-embedding algorithms, such as DeepWalk and node2vec, are based on random-walk notions of node similarity and on contrastive learning. In this work, we introduce the graph neighbor-embedding (graph NE) framework that directly pulls together embedding vectors of adjacent nodes without relying on any random walks. We show that graph NE strongly outperforms state-of-the-art node-embedding algorithms in terms of local structure preservation. Furthermore, we apply graph NE to the 2D node-embedding problem, obtaining graph t-SNE layouts that also outperform existing graph-layout algorithms.", "link": "http://arxiv.org/abs/2503.23822v2", "date": "2025-11-24", "relevancy": 2.261, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4744}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4563}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Node%20Embeddings%20via%20Neighbor%20Embeddings&body=Title%3A%20Node%20Embeddings%20via%20Neighbor%20Embeddings%0AAuthor%3A%20Jan%20Niklas%20B%C3%B6hm%20and%20Marius%20Keute%20and%20Alica%20Guzm%C3%A1n%20and%20Sebastian%20Damrich%20and%20Andrew%20Draganov%20and%20Dmitry%20Kobak%0AAbstract%3A%20Node%20embeddings%20are%20a%20paradigm%20in%20non-parametric%20graph%20representation%20learning%2C%20where%20graph%20nodes%20are%20embedded%20into%20a%20given%20vector%20space%20to%20enable%20downstream%20processing.%20State-of-the-art%20node-embedding%20algorithms%2C%20such%20as%20DeepWalk%20and%20node2vec%2C%20are%20based%20on%20random-walk%20notions%20of%20node%20similarity%20and%20on%20contrastive%20learning.%20In%20this%20work%2C%20we%20introduce%20the%20graph%20neighbor-embedding%20%28graph%20NE%29%20framework%20that%20directly%20pulls%20together%20embedding%20vectors%20of%20adjacent%20nodes%20without%20relying%20on%20any%20random%20walks.%20We%20show%20that%20graph%20NE%20strongly%20outperforms%20state-of-the-art%20node-embedding%20algorithms%20in%20terms%20of%20local%20structure%20preservation.%20Furthermore%2C%20we%20apply%20graph%20NE%20to%20the%202D%20node-embedding%20problem%2C%20obtaining%20graph%20t-SNE%20layouts%20that%20also%20outperform%20existing%20graph-layout%20algorithms.%0ALink%3A%20http%3A//arxiv.org/abs/2503.23822v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNode%2520Embeddings%2520via%2520Neighbor%2520Embeddings%26entry.906535625%3DJan%2520Niklas%2520B%25C3%25B6hm%2520and%2520Marius%2520Keute%2520and%2520Alica%2520Guzm%25C3%25A1n%2520and%2520Sebastian%2520Damrich%2520and%2520Andrew%2520Draganov%2520and%2520Dmitry%2520Kobak%26entry.1292438233%3DNode%2520embeddings%2520are%2520a%2520paradigm%2520in%2520non-parametric%2520graph%2520representation%2520learning%252C%2520where%2520graph%2520nodes%2520are%2520embedded%2520into%2520a%2520given%2520vector%2520space%2520to%2520enable%2520downstream%2520processing.%2520State-of-the-art%2520node-embedding%2520algorithms%252C%2520such%2520as%2520DeepWalk%2520and%2520node2vec%252C%2520are%2520based%2520on%2520random-walk%2520notions%2520of%2520node%2520similarity%2520and%2520on%2520contrastive%2520learning.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520graph%2520neighbor-embedding%2520%2528graph%2520NE%2529%2520framework%2520that%2520directly%2520pulls%2520together%2520embedding%2520vectors%2520of%2520adjacent%2520nodes%2520without%2520relying%2520on%2520any%2520random%2520walks.%2520We%2520show%2520that%2520graph%2520NE%2520strongly%2520outperforms%2520state-of-the-art%2520node-embedding%2520algorithms%2520in%2520terms%2520of%2520local%2520structure%2520preservation.%2520Furthermore%252C%2520we%2520apply%2520graph%2520NE%2520to%2520the%25202D%2520node-embedding%2520problem%252C%2520obtaining%2520graph%2520t-SNE%2520layouts%2520that%2520also%2520outperform%2520existing%2520graph-layout%2520algorithms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23822v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Node%20Embeddings%20via%20Neighbor%20Embeddings&entry.906535625=Jan%20Niklas%20B%C3%B6hm%20and%20Marius%20Keute%20and%20Alica%20Guzm%C3%A1n%20and%20Sebastian%20Damrich%20and%20Andrew%20Draganov%20and%20Dmitry%20Kobak&entry.1292438233=Node%20embeddings%20are%20a%20paradigm%20in%20non-parametric%20graph%20representation%20learning%2C%20where%20graph%20nodes%20are%20embedded%20into%20a%20given%20vector%20space%20to%20enable%20downstream%20processing.%20State-of-the-art%20node-embedding%20algorithms%2C%20such%20as%20DeepWalk%20and%20node2vec%2C%20are%20based%20on%20random-walk%20notions%20of%20node%20similarity%20and%20on%20contrastive%20learning.%20In%20this%20work%2C%20we%20introduce%20the%20graph%20neighbor-embedding%20%28graph%20NE%29%20framework%20that%20directly%20pulls%20together%20embedding%20vectors%20of%20adjacent%20nodes%20without%20relying%20on%20any%20random%20walks.%20We%20show%20that%20graph%20NE%20strongly%20outperforms%20state-of-the-art%20node-embedding%20algorithms%20in%20terms%20of%20local%20structure%20preservation.%20Furthermore%2C%20we%20apply%20graph%20NE%20to%20the%202D%20node-embedding%20problem%2C%20obtaining%20graph%20t-SNE%20layouts%20that%20also%20outperform%20existing%20graph-layout%20algorithms.&entry.1838667208=http%3A//arxiv.org/abs/2503.23822v2&entry.124074799=Read"},
{"title": "Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation", "author": "Juntao Gao and Feiyang Ye and Jing Zhang and Wenjing Qian", "abstract": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.", "link": "http://arxiv.org/abs/2511.18950v1", "date": "2025-11-24", "relevancy": 2.2572, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compressor-VLA%3A%20Instruction-Guided%20Visual%20Token%20Compression%20for%20Efficient%20Robotic%20Manipulation&body=Title%3A%20Compressor-VLA%3A%20Instruction-Guided%20Visual%20Token%20Compression%20for%20Efficient%20Robotic%20Manipulation%0AAuthor%3A%20Juntao%20Gao%20and%20Feiyang%20Ye%20and%20Jing%20Zhang%20and%20Wenjing%20Qian%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20a%20powerful%20paradigm%20in%20Embodied%20AI.%20However%2C%20the%20significant%20computational%20overhead%20of%20processing%20redundant%20visual%20tokens%20remains%20a%20critical%20bottleneck%20for%20real-time%20robotic%20deployment.%20While%20standard%20token%20pruning%20techniques%20can%20alleviate%20this%2C%20these%20task-agnostic%20methods%20struggle%20to%20preserve%20task-critical%20visual%20information.%20To%20address%20this%20challenge%2C%20simultaneously%20preserving%20both%20the%20holistic%20context%20and%20fine-grained%20details%20for%20precise%20action%2C%20we%20propose%20Compressor-VLA%2C%20a%20novel%20hybrid%20instruction-conditioned%20token%20compression%20framework%20designed%20for%20efficient%2C%20task-oriented%20compression%20of%20visual%20information%20in%20VLA%20models.%20The%20proposed%20Compressor-VLA%20framework%20consists%20of%20two%20token%20compression%20modules%3A%20a%20Semantic%20Task%20Compressor%20%28STC%29%20that%20distills%20holistic%2C%20task-relevant%20context%2C%20and%20a%20Spatial%20Refinement%20Compressor%20%28SRC%29%20that%20preserves%20fine-grained%20spatial%20details.%20This%20compression%20is%20dynamically%20modulated%20by%20the%20natural%20language%20instruction%2C%20allowing%20for%20the%20adaptive%20condensation%20of%20task-relevant%20visual%20information.%20Experimentally%2C%20extensive%20evaluations%20demonstrate%20that%20Compressor-VLA%20achieves%20a%20competitive%20success%20rate%20on%20the%20LIBERO%20benchmark%20while%20reducing%20FLOPs%20by%2059%25%20and%20the%20visual%20token%20count%20by%20over%203x%20compared%20to%20its%20baseline.%20The%20real-robot%20deployments%20on%20a%20dual-arm%20robot%20platform%20validate%20the%20model%27s%20sim-to-real%20transferability%20and%20practical%20applicability.%20Moreover%2C%20qualitative%20analyses%20reveal%20that%20our%20instruction%20guidance%20dynamically%20steers%20the%20model%27s%20perceptual%20focus%20toward%20task-relevant%20objects%2C%20thereby%20validating%20the%20effectiveness%20of%20our%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompressor-VLA%253A%2520Instruction-Guided%2520Visual%2520Token%2520Compression%2520for%2520Efficient%2520Robotic%2520Manipulation%26entry.906535625%3DJuntao%2520Gao%2520and%2520Feiyang%2520Ye%2520and%2520Jing%2520Zhang%2520and%2520Wenjing%2520Qian%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520in%2520Embodied%2520AI.%2520However%252C%2520the%2520significant%2520computational%2520overhead%2520of%2520processing%2520redundant%2520visual%2520tokens%2520remains%2520a%2520critical%2520bottleneck%2520for%2520real-time%2520robotic%2520deployment.%2520While%2520standard%2520token%2520pruning%2520techniques%2520can%2520alleviate%2520this%252C%2520these%2520task-agnostic%2520methods%2520struggle%2520to%2520preserve%2520task-critical%2520visual%2520information.%2520To%2520address%2520this%2520challenge%252C%2520simultaneously%2520preserving%2520both%2520the%2520holistic%2520context%2520and%2520fine-grained%2520details%2520for%2520precise%2520action%252C%2520we%2520propose%2520Compressor-VLA%252C%2520a%2520novel%2520hybrid%2520instruction-conditioned%2520token%2520compression%2520framework%2520designed%2520for%2520efficient%252C%2520task-oriented%2520compression%2520of%2520visual%2520information%2520in%2520VLA%2520models.%2520The%2520proposed%2520Compressor-VLA%2520framework%2520consists%2520of%2520two%2520token%2520compression%2520modules%253A%2520a%2520Semantic%2520Task%2520Compressor%2520%2528STC%2529%2520that%2520distills%2520holistic%252C%2520task-relevant%2520context%252C%2520and%2520a%2520Spatial%2520Refinement%2520Compressor%2520%2528SRC%2529%2520that%2520preserves%2520fine-grained%2520spatial%2520details.%2520This%2520compression%2520is%2520dynamically%2520modulated%2520by%2520the%2520natural%2520language%2520instruction%252C%2520allowing%2520for%2520the%2520adaptive%2520condensation%2520of%2520task-relevant%2520visual%2520information.%2520Experimentally%252C%2520extensive%2520evaluations%2520demonstrate%2520that%2520Compressor-VLA%2520achieves%2520a%2520competitive%2520success%2520rate%2520on%2520the%2520LIBERO%2520benchmark%2520while%2520reducing%2520FLOPs%2520by%252059%2525%2520and%2520the%2520visual%2520token%2520count%2520by%2520over%25203x%2520compared%2520to%2520its%2520baseline.%2520The%2520real-robot%2520deployments%2520on%2520a%2520dual-arm%2520robot%2520platform%2520validate%2520the%2520model%2527s%2520sim-to-real%2520transferability%2520and%2520practical%2520applicability.%2520Moreover%252C%2520qualitative%2520analyses%2520reveal%2520that%2520our%2520instruction%2520guidance%2520dynamically%2520steers%2520the%2520model%2527s%2520perceptual%2520focus%2520toward%2520task-relevant%2520objects%252C%2520thereby%2520validating%2520the%2520effectiveness%2520of%2520our%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compressor-VLA%3A%20Instruction-Guided%20Visual%20Token%20Compression%20for%20Efficient%20Robotic%20Manipulation&entry.906535625=Juntao%20Gao%20and%20Feiyang%20Ye%20and%20Jing%20Zhang%20and%20Wenjing%20Qian&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20a%20powerful%20paradigm%20in%20Embodied%20AI.%20However%2C%20the%20significant%20computational%20overhead%20of%20processing%20redundant%20visual%20tokens%20remains%20a%20critical%20bottleneck%20for%20real-time%20robotic%20deployment.%20While%20standard%20token%20pruning%20techniques%20can%20alleviate%20this%2C%20these%20task-agnostic%20methods%20struggle%20to%20preserve%20task-critical%20visual%20information.%20To%20address%20this%20challenge%2C%20simultaneously%20preserving%20both%20the%20holistic%20context%20and%20fine-grained%20details%20for%20precise%20action%2C%20we%20propose%20Compressor-VLA%2C%20a%20novel%20hybrid%20instruction-conditioned%20token%20compression%20framework%20designed%20for%20efficient%2C%20task-oriented%20compression%20of%20visual%20information%20in%20VLA%20models.%20The%20proposed%20Compressor-VLA%20framework%20consists%20of%20two%20token%20compression%20modules%3A%20a%20Semantic%20Task%20Compressor%20%28STC%29%20that%20distills%20holistic%2C%20task-relevant%20context%2C%20and%20a%20Spatial%20Refinement%20Compressor%20%28SRC%29%20that%20preserves%20fine-grained%20spatial%20details.%20This%20compression%20is%20dynamically%20modulated%20by%20the%20natural%20language%20instruction%2C%20allowing%20for%20the%20adaptive%20condensation%20of%20task-relevant%20visual%20information.%20Experimentally%2C%20extensive%20evaluations%20demonstrate%20that%20Compressor-VLA%20achieves%20a%20competitive%20success%20rate%20on%20the%20LIBERO%20benchmark%20while%20reducing%20FLOPs%20by%2059%25%20and%20the%20visual%20token%20count%20by%20over%203x%20compared%20to%20its%20baseline.%20The%20real-robot%20deployments%20on%20a%20dual-arm%20robot%20platform%20validate%20the%20model%27s%20sim-to-real%20transferability%20and%20practical%20applicability.%20Moreover%2C%20qualitative%20analyses%20reveal%20that%20our%20instruction%20guidance%20dynamically%20steers%20the%20model%27s%20perceptual%20focus%20toward%20task-relevant%20objects%2C%20thereby%20validating%20the%20effectiveness%20of%20our%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2511.18950v1&entry.124074799=Read"},
{"title": "Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering", "author": "Federico Felizzi and Olivia Riccomi and Michele Ferramola and Francesco Andrea Causio and Manuel Del Medico and Vittorio De Vita and Lorenzo De Mori and Alessandra Piscitelli Pietro Eric Risuleo and Bianca Destro Castaniti and Antonio Cristiano Alessia Longo and Luigi De Angelis and Mariapia Vassalli and Marcello Di Pumpo", "abstract": "Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.", "link": "http://arxiv.org/abs/2511.19220v1", "date": "2025-11-24", "relevancy": 2.2445, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5736}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5736}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Large%20Vision%20Language%20Models%20Truly%20Grounded%20in%20Medical%20Images%3F%20Evidence%20from%20Italian%20Clinical%20Visual%20Question%20Answering&body=Title%3A%20Are%20Large%20Vision%20Language%20Models%20Truly%20Grounded%20in%20Medical%20Images%3F%20Evidence%20from%20Italian%20Clinical%20Visual%20Question%20Answering%0AAuthor%3A%20Federico%20Felizzi%20and%20Olivia%20Riccomi%20and%20Michele%20Ferramola%20and%20Francesco%20Andrea%20Causio%20and%20Manuel%20Del%20Medico%20and%20Vittorio%20De%20Vita%20and%20Lorenzo%20De%20Mori%20and%20Alessandra%20Piscitelli%20Pietro%20Eric%20Risuleo%20and%20Bianca%20Destro%20Castaniti%20and%20Antonio%20Cristiano%20Alessia%20Longo%20and%20Luigi%20De%20Angelis%20and%20Mariapia%20Vassalli%20and%20Marcello%20Di%20Pumpo%0AAbstract%3A%20Large%20vision%20language%20models%20%28VLMs%29%20have%20achieved%20impressive%20performance%20on%20medical%20visual%20question%20answering%20benchmarks%2C%20yet%20their%20reliance%20on%20visual%20information%20remains%20unclear.%20We%20investigate%20whether%20frontier%20VLMs%20demonstrate%20genuine%20visual%20grounding%20when%20answering%20Italian%20medical%20questions%20by%20testing%20four%20state-of-the-art%20models%3A%20Claude%20Sonnet%204.5%2C%20GPT-4o%2C%20GPT-5-mini%2C%20and%20Gemini%202.0%20flash%20exp.%20Using%2060%20questions%20from%20the%20EuropeMedQA%20Italian%20dataset%20that%20explicitly%20require%20image%20interpretation%2C%20we%20substitute%20correct%20medical%20images%20with%20blank%20placeholders%20to%20test%20whether%20models%20truly%20integrate%20visual%20and%20textual%20information.%20Our%20results%20reveal%20striking%20variability%20in%20visual%20dependency%3A%20GPT-4o%20shows%20the%20strongest%20visual%20grounding%20with%20a%2027.9pp%20accuracy%20drop%20%2883.2%25%20%5B74.6%25%2C%2091.7%25%5D%20to%2055.3%25%20%5B44.1%25%2C%2066.6%25%5D%29%2C%20while%20GPT-5-mini%2C%20Gemini%2C%20and%20Claude%20maintain%20high%20accuracy%20with%20modest%20drops%20of%208.5pp%2C%202.4pp%2C%20and%205.6pp%20respectively.%20Analysis%20of%20model-generated%20reasoning%20reveals%20confident%20explanations%20for%20fabricated%20visual%20interpretations%20across%20all%20models%2C%20suggesting%20varying%20degrees%20of%20reliance%20on%20textual%20shortcuts%20versus%20genuine%20visual%20analysis.%20These%20findings%20highlight%20critical%20differences%20in%20model%20robustness%20and%20the%20need%20for%20rigorous%20evaluation%20before%20clinical%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Large%2520Vision%2520Language%2520Models%2520Truly%2520Grounded%2520in%2520Medical%2520Images%253F%2520Evidence%2520from%2520Italian%2520Clinical%2520Visual%2520Question%2520Answering%26entry.906535625%3DFederico%2520Felizzi%2520and%2520Olivia%2520Riccomi%2520and%2520Michele%2520Ferramola%2520and%2520Francesco%2520Andrea%2520Causio%2520and%2520Manuel%2520Del%2520Medico%2520and%2520Vittorio%2520De%2520Vita%2520and%2520Lorenzo%2520De%2520Mori%2520and%2520Alessandra%2520Piscitelli%2520Pietro%2520Eric%2520Risuleo%2520and%2520Bianca%2520Destro%2520Castaniti%2520and%2520Antonio%2520Cristiano%2520Alessia%2520Longo%2520and%2520Luigi%2520De%2520Angelis%2520and%2520Mariapia%2520Vassalli%2520and%2520Marcello%2520Di%2520Pumpo%26entry.1292438233%3DLarge%2520vision%2520language%2520models%2520%2528VLMs%2529%2520have%2520achieved%2520impressive%2520performance%2520on%2520medical%2520visual%2520question%2520answering%2520benchmarks%252C%2520yet%2520their%2520reliance%2520on%2520visual%2520information%2520remains%2520unclear.%2520We%2520investigate%2520whether%2520frontier%2520VLMs%2520demonstrate%2520genuine%2520visual%2520grounding%2520when%2520answering%2520Italian%2520medical%2520questions%2520by%2520testing%2520four%2520state-of-the-art%2520models%253A%2520Claude%2520Sonnet%25204.5%252C%2520GPT-4o%252C%2520GPT-5-mini%252C%2520and%2520Gemini%25202.0%2520flash%2520exp.%2520Using%252060%2520questions%2520from%2520the%2520EuropeMedQA%2520Italian%2520dataset%2520that%2520explicitly%2520require%2520image%2520interpretation%252C%2520we%2520substitute%2520correct%2520medical%2520images%2520with%2520blank%2520placeholders%2520to%2520test%2520whether%2520models%2520truly%2520integrate%2520visual%2520and%2520textual%2520information.%2520Our%2520results%2520reveal%2520striking%2520variability%2520in%2520visual%2520dependency%253A%2520GPT-4o%2520shows%2520the%2520strongest%2520visual%2520grounding%2520with%2520a%252027.9pp%2520accuracy%2520drop%2520%252883.2%2525%2520%255B74.6%2525%252C%252091.7%2525%255D%2520to%252055.3%2525%2520%255B44.1%2525%252C%252066.6%2525%255D%2529%252C%2520while%2520GPT-5-mini%252C%2520Gemini%252C%2520and%2520Claude%2520maintain%2520high%2520accuracy%2520with%2520modest%2520drops%2520of%25208.5pp%252C%25202.4pp%252C%2520and%25205.6pp%2520respectively.%2520Analysis%2520of%2520model-generated%2520reasoning%2520reveals%2520confident%2520explanations%2520for%2520fabricated%2520visual%2520interpretations%2520across%2520all%2520models%252C%2520suggesting%2520varying%2520degrees%2520of%2520reliance%2520on%2520textual%2520shortcuts%2520versus%2520genuine%2520visual%2520analysis.%2520These%2520findings%2520highlight%2520critical%2520differences%2520in%2520model%2520robustness%2520and%2520the%2520need%2520for%2520rigorous%2520evaluation%2520before%2520clinical%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Large%20Vision%20Language%20Models%20Truly%20Grounded%20in%20Medical%20Images%3F%20Evidence%20from%20Italian%20Clinical%20Visual%20Question%20Answering&entry.906535625=Federico%20Felizzi%20and%20Olivia%20Riccomi%20and%20Michele%20Ferramola%20and%20Francesco%20Andrea%20Causio%20and%20Manuel%20Del%20Medico%20and%20Vittorio%20De%20Vita%20and%20Lorenzo%20De%20Mori%20and%20Alessandra%20Piscitelli%20Pietro%20Eric%20Risuleo%20and%20Bianca%20Destro%20Castaniti%20and%20Antonio%20Cristiano%20Alessia%20Longo%20and%20Luigi%20De%20Angelis%20and%20Mariapia%20Vassalli%20and%20Marcello%20Di%20Pumpo&entry.1292438233=Large%20vision%20language%20models%20%28VLMs%29%20have%20achieved%20impressive%20performance%20on%20medical%20visual%20question%20answering%20benchmarks%2C%20yet%20their%20reliance%20on%20visual%20information%20remains%20unclear.%20We%20investigate%20whether%20frontier%20VLMs%20demonstrate%20genuine%20visual%20grounding%20when%20answering%20Italian%20medical%20questions%20by%20testing%20four%20state-of-the-art%20models%3A%20Claude%20Sonnet%204.5%2C%20GPT-4o%2C%20GPT-5-mini%2C%20and%20Gemini%202.0%20flash%20exp.%20Using%2060%20questions%20from%20the%20EuropeMedQA%20Italian%20dataset%20that%20explicitly%20require%20image%20interpretation%2C%20we%20substitute%20correct%20medical%20images%20with%20blank%20placeholders%20to%20test%20whether%20models%20truly%20integrate%20visual%20and%20textual%20information.%20Our%20results%20reveal%20striking%20variability%20in%20visual%20dependency%3A%20GPT-4o%20shows%20the%20strongest%20visual%20grounding%20with%20a%2027.9pp%20accuracy%20drop%20%2883.2%25%20%5B74.6%25%2C%2091.7%25%5D%20to%2055.3%25%20%5B44.1%25%2C%2066.6%25%5D%29%2C%20while%20GPT-5-mini%2C%20Gemini%2C%20and%20Claude%20maintain%20high%20accuracy%20with%20modest%20drops%20of%208.5pp%2C%202.4pp%2C%20and%205.6pp%20respectively.%20Analysis%20of%20model-generated%20reasoning%20reveals%20confident%20explanations%20for%20fabricated%20visual%20interpretations%20across%20all%20models%2C%20suggesting%20varying%20degrees%20of%20reliance%20on%20textual%20shortcuts%20versus%20genuine%20visual%20analysis.%20These%20findings%20highlight%20critical%20differences%20in%20model%20robustness%20and%20the%20need%20for%20rigorous%20evaluation%20before%20clinical%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2511.19220v1&entry.124074799=Read"},
{"title": "FOCUS: Efficient Keyframe Selection for Long Video Understanding", "author": "Zirui Zhu and Hailun Xu and Yang Luo and Yong Liu and Kanchan Sarkar and Zhenheng Yang and Yang You", "abstract": "Multimodal large language models (MLLMs) represent images and video frames as visual tokens. Scaling from single images to hour-long videos, however, inflates the token budget far beyond practical limits. Popular pipelines therefore either uniformly subsample or apply keyframe selection with retrieval-style scoring using smaller vision-language models. However, these keyframe selection methods still rely on pre-filtering before selection to reduce the inference cost and can miss the most informative moments. We propose FOCUS, Frame-Optimistic Confidence Upper-bound Selection, a training-free, model-agnostic keyframe selection module that selects query-relevant frames under a strict token budget. FOCUS formulates keyframe selection as a combinatorial pure-exploration (CPE) problem in multi-armed bandits: it treats short temporal clips as arms, and uses empirical means and Bernstein confidence radius to identify informative regions while preserving exploration of uncertain areas. The resulting two-stage exploration-exploitation procedure reduces from a sequential policy with theoretical guarantees, first identifying high-value temporal regions, then selecting top-scoring frames within each region. On two long-video question-answering benchmarks, FOCUS delivers substantial accuracy improvements while processing less than 2% of video frames. For videos longer than 20 minutes, it achieves an 11.9% gain in accuracy on LongVideoBench, demonstrating its effectiveness as a keyframe selection method and providing a simple and general solution for scalable long-video understanding with MLLMs. Code is available at https://github.com/NUS-HPC-AI-Lab/FOCUS.", "link": "http://arxiv.org/abs/2510.27280v2", "date": "2025-11-24", "relevancy": 2.2259, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5653}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FOCUS%3A%20Efficient%20Keyframe%20Selection%20for%20Long%20Video%20Understanding&body=Title%3A%20FOCUS%3A%20Efficient%20Keyframe%20Selection%20for%20Long%20Video%20Understanding%0AAuthor%3A%20Zirui%20Zhu%20and%20Hailun%20Xu%20and%20Yang%20Luo%20and%20Yong%20Liu%20and%20Kanchan%20Sarkar%20and%20Zhenheng%20Yang%20and%20Yang%20You%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20represent%20images%20and%20video%20frames%20as%20visual%20tokens.%20Scaling%20from%20single%20images%20to%20hour-long%20videos%2C%20however%2C%20inflates%20the%20token%20budget%20far%20beyond%20practical%20limits.%20Popular%20pipelines%20therefore%20either%20uniformly%20subsample%20or%20apply%20keyframe%20selection%20with%20retrieval-style%20scoring%20using%20smaller%20vision-language%20models.%20However%2C%20these%20keyframe%20selection%20methods%20still%20rely%20on%20pre-filtering%20before%20selection%20to%20reduce%20the%20inference%20cost%20and%20can%20miss%20the%20most%20informative%20moments.%20We%20propose%20FOCUS%2C%20Frame-Optimistic%20Confidence%20Upper-bound%20Selection%2C%20a%20training-free%2C%20model-agnostic%20keyframe%20selection%20module%20that%20selects%20query-relevant%20frames%20under%20a%20strict%20token%20budget.%20FOCUS%20formulates%20keyframe%20selection%20as%20a%20combinatorial%20pure-exploration%20%28CPE%29%20problem%20in%20multi-armed%20bandits%3A%20it%20treats%20short%20temporal%20clips%20as%20arms%2C%20and%20uses%20empirical%20means%20and%20Bernstein%20confidence%20radius%20to%20identify%20informative%20regions%20while%20preserving%20exploration%20of%20uncertain%20areas.%20The%20resulting%20two-stage%20exploration-exploitation%20procedure%20reduces%20from%20a%20sequential%20policy%20with%20theoretical%20guarantees%2C%20first%20identifying%20high-value%20temporal%20regions%2C%20then%20selecting%20top-scoring%20frames%20within%20each%20region.%20On%20two%20long-video%20question-answering%20benchmarks%2C%20FOCUS%20delivers%20substantial%20accuracy%20improvements%20while%20processing%20less%20than%202%25%20of%20video%20frames.%20For%20videos%20longer%20than%2020%20minutes%2C%20it%20achieves%20an%2011.9%25%20gain%20in%20accuracy%20on%20LongVideoBench%2C%20demonstrating%20its%20effectiveness%20as%20a%20keyframe%20selection%20method%20and%20providing%20a%20simple%20and%20general%20solution%20for%20scalable%20long-video%20understanding%20with%20MLLMs.%20Code%20is%20available%20at%20https%3A//github.com/NUS-HPC-AI-Lab/FOCUS.%0ALink%3A%20http%3A//arxiv.org/abs/2510.27280v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFOCUS%253A%2520Efficient%2520Keyframe%2520Selection%2520for%2520Long%2520Video%2520Understanding%26entry.906535625%3DZirui%2520Zhu%2520and%2520Hailun%2520Xu%2520and%2520Yang%2520Luo%2520and%2520Yong%2520Liu%2520and%2520Kanchan%2520Sarkar%2520and%2520Zhenheng%2520Yang%2520and%2520Yang%2520You%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520represent%2520images%2520and%2520video%2520frames%2520as%2520visual%2520tokens.%2520Scaling%2520from%2520single%2520images%2520to%2520hour-long%2520videos%252C%2520however%252C%2520inflates%2520the%2520token%2520budget%2520far%2520beyond%2520practical%2520limits.%2520Popular%2520pipelines%2520therefore%2520either%2520uniformly%2520subsample%2520or%2520apply%2520keyframe%2520selection%2520with%2520retrieval-style%2520scoring%2520using%2520smaller%2520vision-language%2520models.%2520However%252C%2520these%2520keyframe%2520selection%2520methods%2520still%2520rely%2520on%2520pre-filtering%2520before%2520selection%2520to%2520reduce%2520the%2520inference%2520cost%2520and%2520can%2520miss%2520the%2520most%2520informative%2520moments.%2520We%2520propose%2520FOCUS%252C%2520Frame-Optimistic%2520Confidence%2520Upper-bound%2520Selection%252C%2520a%2520training-free%252C%2520model-agnostic%2520keyframe%2520selection%2520module%2520that%2520selects%2520query-relevant%2520frames%2520under%2520a%2520strict%2520token%2520budget.%2520FOCUS%2520formulates%2520keyframe%2520selection%2520as%2520a%2520combinatorial%2520pure-exploration%2520%2528CPE%2529%2520problem%2520in%2520multi-armed%2520bandits%253A%2520it%2520treats%2520short%2520temporal%2520clips%2520as%2520arms%252C%2520and%2520uses%2520empirical%2520means%2520and%2520Bernstein%2520confidence%2520radius%2520to%2520identify%2520informative%2520regions%2520while%2520preserving%2520exploration%2520of%2520uncertain%2520areas.%2520The%2520resulting%2520two-stage%2520exploration-exploitation%2520procedure%2520reduces%2520from%2520a%2520sequential%2520policy%2520with%2520theoretical%2520guarantees%252C%2520first%2520identifying%2520high-value%2520temporal%2520regions%252C%2520then%2520selecting%2520top-scoring%2520frames%2520within%2520each%2520region.%2520On%2520two%2520long-video%2520question-answering%2520benchmarks%252C%2520FOCUS%2520delivers%2520substantial%2520accuracy%2520improvements%2520while%2520processing%2520less%2520than%25202%2525%2520of%2520video%2520frames.%2520For%2520videos%2520longer%2520than%252020%2520minutes%252C%2520it%2520achieves%2520an%252011.9%2525%2520gain%2520in%2520accuracy%2520on%2520LongVideoBench%252C%2520demonstrating%2520its%2520effectiveness%2520as%2520a%2520keyframe%2520selection%2520method%2520and%2520providing%2520a%2520simple%2520and%2520general%2520solution%2520for%2520scalable%2520long-video%2520understanding%2520with%2520MLLMs.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/NUS-HPC-AI-Lab/FOCUS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27280v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOCUS%3A%20Efficient%20Keyframe%20Selection%20for%20Long%20Video%20Understanding&entry.906535625=Zirui%20Zhu%20and%20Hailun%20Xu%20and%20Yang%20Luo%20and%20Yong%20Liu%20and%20Kanchan%20Sarkar%20and%20Zhenheng%20Yang%20and%20Yang%20You&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20represent%20images%20and%20video%20frames%20as%20visual%20tokens.%20Scaling%20from%20single%20images%20to%20hour-long%20videos%2C%20however%2C%20inflates%20the%20token%20budget%20far%20beyond%20practical%20limits.%20Popular%20pipelines%20therefore%20either%20uniformly%20subsample%20or%20apply%20keyframe%20selection%20with%20retrieval-style%20scoring%20using%20smaller%20vision-language%20models.%20However%2C%20these%20keyframe%20selection%20methods%20still%20rely%20on%20pre-filtering%20before%20selection%20to%20reduce%20the%20inference%20cost%20and%20can%20miss%20the%20most%20informative%20moments.%20We%20propose%20FOCUS%2C%20Frame-Optimistic%20Confidence%20Upper-bound%20Selection%2C%20a%20training-free%2C%20model-agnostic%20keyframe%20selection%20module%20that%20selects%20query-relevant%20frames%20under%20a%20strict%20token%20budget.%20FOCUS%20formulates%20keyframe%20selection%20as%20a%20combinatorial%20pure-exploration%20%28CPE%29%20problem%20in%20multi-armed%20bandits%3A%20it%20treats%20short%20temporal%20clips%20as%20arms%2C%20and%20uses%20empirical%20means%20and%20Bernstein%20confidence%20radius%20to%20identify%20informative%20regions%20while%20preserving%20exploration%20of%20uncertain%20areas.%20The%20resulting%20two-stage%20exploration-exploitation%20procedure%20reduces%20from%20a%20sequential%20policy%20with%20theoretical%20guarantees%2C%20first%20identifying%20high-value%20temporal%20regions%2C%20then%20selecting%20top-scoring%20frames%20within%20each%20region.%20On%20two%20long-video%20question-answering%20benchmarks%2C%20FOCUS%20delivers%20substantial%20accuracy%20improvements%20while%20processing%20less%20than%202%25%20of%20video%20frames.%20For%20videos%20longer%20than%2020%20minutes%2C%20it%20achieves%20an%2011.9%25%20gain%20in%20accuracy%20on%20LongVideoBench%2C%20demonstrating%20its%20effectiveness%20as%20a%20keyframe%20selection%20method%20and%20providing%20a%20simple%20and%20general%20solution%20for%20scalable%20long-video%20understanding%20with%20MLLMs.%20Code%20is%20available%20at%20https%3A//github.com/NUS-HPC-AI-Lab/FOCUS.&entry.1838667208=http%3A//arxiv.org/abs/2510.27280v2&entry.124074799=Read"},
{"title": "Dual-Granularity Semantic Prompting for Language Guidance Infrared Small Target Detection", "author": "Zixuan Wang and Haoran Sun and Jiaming Lu and Wenxuan Wang and Zhongling Huang and Dingwen Zhang and Xuelin Qian and Junwei Han", "abstract": "Infrared small target detection remains challenging due to limited feature representation and severe background interference, resulting in sub-optimal performance. While recent CLIP-inspired methods attempt to leverage textual guidance for detection, they are hindered by inaccurate text descriptions and reliance on manual annotations. To overcome these limitations, we propose DGSPNet, an end-to-end language prompt-driven framework. Our approach integrates dual-granularity semantic prompts: coarse-grained textual priors (e.g., 'infrared image', 'small target') and fine-grained personalized semantic descriptions derived through visual-to-textual mapping within the image space. This design not only facilitates learning fine-grained semantic information but also can inherently leverage language prompts during inference without relying on any annotation requirements. By fully leveraging the precision and conciseness of text descriptions, we further introduce a text-guide channel attention (TGCA) mechanism and text-guide spatial attention (TGSA) mechanism that enhances the model's sensitivity to potential targets across both low- and high-level feature spaces. Extensive experiments demonstrate that our method significantly improves detection accuracy and achieves state-of-the-art performance on three benchmark datasets.", "link": "http://arxiv.org/abs/2511.19306v1", "date": "2025-11-24", "relevancy": 2.22, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5609}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5557}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Granularity%20Semantic%20Prompting%20for%20Language%20Guidance%20Infrared%20Small%20Target%20Detection&body=Title%3A%20Dual-Granularity%20Semantic%20Prompting%20for%20Language%20Guidance%20Infrared%20Small%20Target%20Detection%0AAuthor%3A%20Zixuan%20Wang%20and%20Haoran%20Sun%20and%20Jiaming%20Lu%20and%20Wenxuan%20Wang%20and%20Zhongling%20Huang%20and%20Dingwen%20Zhang%20and%20Xuelin%20Qian%20and%20Junwei%20Han%0AAbstract%3A%20Infrared%20small%20target%20detection%20remains%20challenging%20due%20to%20limited%20feature%20representation%20and%20severe%20background%20interference%2C%20resulting%20in%20sub-optimal%20performance.%20While%20recent%20CLIP-inspired%20methods%20attempt%20to%20leverage%20textual%20guidance%20for%20detection%2C%20they%20are%20hindered%20by%20inaccurate%20text%20descriptions%20and%20reliance%20on%20manual%20annotations.%20To%20overcome%20these%20limitations%2C%20we%20propose%20DGSPNet%2C%20an%20end-to-end%20language%20prompt-driven%20framework.%20Our%20approach%20integrates%20dual-granularity%20semantic%20prompts%3A%20coarse-grained%20textual%20priors%20%28e.g.%2C%20%27infrared%20image%27%2C%20%27small%20target%27%29%20and%20fine-grained%20personalized%20semantic%20descriptions%20derived%20through%20visual-to-textual%20mapping%20within%20the%20image%20space.%20This%20design%20not%20only%20facilitates%20learning%20fine-grained%20semantic%20information%20but%20also%20can%20inherently%20leverage%20language%20prompts%20during%20inference%20without%20relying%20on%20any%20annotation%20requirements.%20By%20fully%20leveraging%20the%20precision%20and%20conciseness%20of%20text%20descriptions%2C%20we%20further%20introduce%20a%20text-guide%20channel%20attention%20%28TGCA%29%20mechanism%20and%20text-guide%20spatial%20attention%20%28TGSA%29%20mechanism%20that%20enhances%20the%20model%27s%20sensitivity%20to%20potential%20targets%20across%20both%20low-%20and%20high-level%20feature%20spaces.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20significantly%20improves%20detection%20accuracy%20and%20achieves%20state-of-the-art%20performance%20on%20three%20benchmark%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Granularity%2520Semantic%2520Prompting%2520for%2520Language%2520Guidance%2520Infrared%2520Small%2520Target%2520Detection%26entry.906535625%3DZixuan%2520Wang%2520and%2520Haoran%2520Sun%2520and%2520Jiaming%2520Lu%2520and%2520Wenxuan%2520Wang%2520and%2520Zhongling%2520Huang%2520and%2520Dingwen%2520Zhang%2520and%2520Xuelin%2520Qian%2520and%2520Junwei%2520Han%26entry.1292438233%3DInfrared%2520small%2520target%2520detection%2520remains%2520challenging%2520due%2520to%2520limited%2520feature%2520representation%2520and%2520severe%2520background%2520interference%252C%2520resulting%2520in%2520sub-optimal%2520performance.%2520While%2520recent%2520CLIP-inspired%2520methods%2520attempt%2520to%2520leverage%2520textual%2520guidance%2520for%2520detection%252C%2520they%2520are%2520hindered%2520by%2520inaccurate%2520text%2520descriptions%2520and%2520reliance%2520on%2520manual%2520annotations.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520DGSPNet%252C%2520an%2520end-to-end%2520language%2520prompt-driven%2520framework.%2520Our%2520approach%2520integrates%2520dual-granularity%2520semantic%2520prompts%253A%2520coarse-grained%2520textual%2520priors%2520%2528e.g.%252C%2520%2527infrared%2520image%2527%252C%2520%2527small%2520target%2527%2529%2520and%2520fine-grained%2520personalized%2520semantic%2520descriptions%2520derived%2520through%2520visual-to-textual%2520mapping%2520within%2520the%2520image%2520space.%2520This%2520design%2520not%2520only%2520facilitates%2520learning%2520fine-grained%2520semantic%2520information%2520but%2520also%2520can%2520inherently%2520leverage%2520language%2520prompts%2520during%2520inference%2520without%2520relying%2520on%2520any%2520annotation%2520requirements.%2520By%2520fully%2520leveraging%2520the%2520precision%2520and%2520conciseness%2520of%2520text%2520descriptions%252C%2520we%2520further%2520introduce%2520a%2520text-guide%2520channel%2520attention%2520%2528TGCA%2529%2520mechanism%2520and%2520text-guide%2520spatial%2520attention%2520%2528TGSA%2529%2520mechanism%2520that%2520enhances%2520the%2520model%2527s%2520sensitivity%2520to%2520potential%2520targets%2520across%2520both%2520low-%2520and%2520high-level%2520feature%2520spaces.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520significantly%2520improves%2520detection%2520accuracy%2520and%2520achieves%2520state-of-the-art%2520performance%2520on%2520three%2520benchmark%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Granularity%20Semantic%20Prompting%20for%20Language%20Guidance%20Infrared%20Small%20Target%20Detection&entry.906535625=Zixuan%20Wang%20and%20Haoran%20Sun%20and%20Jiaming%20Lu%20and%20Wenxuan%20Wang%20and%20Zhongling%20Huang%20and%20Dingwen%20Zhang%20and%20Xuelin%20Qian%20and%20Junwei%20Han&entry.1292438233=Infrared%20small%20target%20detection%20remains%20challenging%20due%20to%20limited%20feature%20representation%20and%20severe%20background%20interference%2C%20resulting%20in%20sub-optimal%20performance.%20While%20recent%20CLIP-inspired%20methods%20attempt%20to%20leverage%20textual%20guidance%20for%20detection%2C%20they%20are%20hindered%20by%20inaccurate%20text%20descriptions%20and%20reliance%20on%20manual%20annotations.%20To%20overcome%20these%20limitations%2C%20we%20propose%20DGSPNet%2C%20an%20end-to-end%20language%20prompt-driven%20framework.%20Our%20approach%20integrates%20dual-granularity%20semantic%20prompts%3A%20coarse-grained%20textual%20priors%20%28e.g.%2C%20%27infrared%20image%27%2C%20%27small%20target%27%29%20and%20fine-grained%20personalized%20semantic%20descriptions%20derived%20through%20visual-to-textual%20mapping%20within%20the%20image%20space.%20This%20design%20not%20only%20facilitates%20learning%20fine-grained%20semantic%20information%20but%20also%20can%20inherently%20leverage%20language%20prompts%20during%20inference%20without%20relying%20on%20any%20annotation%20requirements.%20By%20fully%20leveraging%20the%20precision%20and%20conciseness%20of%20text%20descriptions%2C%20we%20further%20introduce%20a%20text-guide%20channel%20attention%20%28TGCA%29%20mechanism%20and%20text-guide%20spatial%20attention%20%28TGSA%29%20mechanism%20that%20enhances%20the%20model%27s%20sensitivity%20to%20potential%20targets%20across%20both%20low-%20and%20high-level%20feature%20spaces.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20significantly%20improves%20detection%20accuracy%20and%20achieves%20state-of-the-art%20performance%20on%20three%20benchmark%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2511.19306v1&entry.124074799=Read"},
{"title": "Dynamic Granularity Matters: Rethinking Vision Transformers Beyond Fixed Patch Splitting", "author": "Qiyang Yu and Yu Fang and Tianrui Li and Xuemei Cao and Yan Chen and Jianghao Li and Fan Min", "abstract": "Vision Transformers (ViTs) have demonstrated strong capabilities in capturing global dependencies but often struggle to efficiently represent fine-grained local details. Existing multi-scale approaches alleviate this issue by integrating hierarchical or hybrid features; however, they rely on fixed patch sizes and introduce redundant computation. To address these limitations, we propose Granularity-driven Vision Transformer (Grc-ViT), a dynamic coarse-to-fine framework that adaptively adjusts visual granularity based on image complexity. It comprises two key stages: (1) Coarse Granularity Evaluation module, which assesses visual complexity using edge density, entropy, and frequency-domain cues to estimate suitable patch and window sizes; (2) Fine-grained Refinement module, which refines attention computation according to the selected granularity, enabling efficient and precise feature learning. Two learnable parameters, \u03b1 and \\b{eta}, are optimized end-to-end to balance global reasoning and local perception. Comprehensive evaluations demonstrate that Grc-ViT enhances fine-grained discrimination while achieving a superior trade-off between accuracy and computational efficiency.", "link": "http://arxiv.org/abs/2511.19021v1", "date": "2025-11-24", "relevancy": 2.2164, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5871}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5504}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Granularity%20Matters%3A%20Rethinking%20Vision%20Transformers%20Beyond%20Fixed%20Patch%20Splitting&body=Title%3A%20Dynamic%20Granularity%20Matters%3A%20Rethinking%20Vision%20Transformers%20Beyond%20Fixed%20Patch%20Splitting%0AAuthor%3A%20Qiyang%20Yu%20and%20Yu%20Fang%20and%20Tianrui%20Li%20and%20Xuemei%20Cao%20and%20Yan%20Chen%20and%20Jianghao%20Li%20and%20Fan%20Min%0AAbstract%3A%20Vision%20Transformers%20%28ViTs%29%20have%20demonstrated%20strong%20capabilities%20in%20capturing%20global%20dependencies%20but%20often%20struggle%20to%20efficiently%20represent%20fine-grained%20local%20details.%20Existing%20multi-scale%20approaches%20alleviate%20this%20issue%20by%20integrating%20hierarchical%20or%20hybrid%20features%3B%20however%2C%20they%20rely%20on%20fixed%20patch%20sizes%20and%20introduce%20redundant%20computation.%20To%20address%20these%20limitations%2C%20we%20propose%20Granularity-driven%20Vision%20Transformer%20%28Grc-ViT%29%2C%20a%20dynamic%20coarse-to-fine%20framework%20that%20adaptively%20adjusts%20visual%20granularity%20based%20on%20image%20complexity.%20It%20comprises%20two%20key%20stages%3A%20%281%29%20Coarse%20Granularity%20Evaluation%20module%2C%20which%20assesses%20visual%20complexity%20using%20edge%20density%2C%20entropy%2C%20and%20frequency-domain%20cues%20to%20estimate%20suitable%20patch%20and%20window%20sizes%3B%20%282%29%20Fine-grained%20Refinement%20module%2C%20which%20refines%20attention%20computation%20according%20to%20the%20selected%20granularity%2C%20enabling%20efficient%20and%20precise%20feature%20learning.%20Two%20learnable%20parameters%2C%20%CE%B1%20and%20%5Cb%7Beta%7D%2C%20are%20optimized%20end-to-end%20to%20balance%20global%20reasoning%20and%20local%20perception.%20Comprehensive%20evaluations%20demonstrate%20that%20Grc-ViT%20enhances%20fine-grained%20discrimination%20while%20achieving%20a%20superior%20trade-off%20between%20accuracy%20and%20computational%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Granularity%2520Matters%253A%2520Rethinking%2520Vision%2520Transformers%2520Beyond%2520Fixed%2520Patch%2520Splitting%26entry.906535625%3DQiyang%2520Yu%2520and%2520Yu%2520Fang%2520and%2520Tianrui%2520Li%2520and%2520Xuemei%2520Cao%2520and%2520Yan%2520Chen%2520and%2520Jianghao%2520Li%2520and%2520Fan%2520Min%26entry.1292438233%3DVision%2520Transformers%2520%2528ViTs%2529%2520have%2520demonstrated%2520strong%2520capabilities%2520in%2520capturing%2520global%2520dependencies%2520but%2520often%2520struggle%2520to%2520efficiently%2520represent%2520fine-grained%2520local%2520details.%2520Existing%2520multi-scale%2520approaches%2520alleviate%2520this%2520issue%2520by%2520integrating%2520hierarchical%2520or%2520hybrid%2520features%253B%2520however%252C%2520they%2520rely%2520on%2520fixed%2520patch%2520sizes%2520and%2520introduce%2520redundant%2520computation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Granularity-driven%2520Vision%2520Transformer%2520%2528Grc-ViT%2529%252C%2520a%2520dynamic%2520coarse-to-fine%2520framework%2520that%2520adaptively%2520adjusts%2520visual%2520granularity%2520based%2520on%2520image%2520complexity.%2520It%2520comprises%2520two%2520key%2520stages%253A%2520%25281%2529%2520Coarse%2520Granularity%2520Evaluation%2520module%252C%2520which%2520assesses%2520visual%2520complexity%2520using%2520edge%2520density%252C%2520entropy%252C%2520and%2520frequency-domain%2520cues%2520to%2520estimate%2520suitable%2520patch%2520and%2520window%2520sizes%253B%2520%25282%2529%2520Fine-grained%2520Refinement%2520module%252C%2520which%2520refines%2520attention%2520computation%2520according%2520to%2520the%2520selected%2520granularity%252C%2520enabling%2520efficient%2520and%2520precise%2520feature%2520learning.%2520Two%2520learnable%2520parameters%252C%2520%25CE%25B1%2520and%2520%255Cb%257Beta%257D%252C%2520are%2520optimized%2520end-to-end%2520to%2520balance%2520global%2520reasoning%2520and%2520local%2520perception.%2520Comprehensive%2520evaluations%2520demonstrate%2520that%2520Grc-ViT%2520enhances%2520fine-grained%2520discrimination%2520while%2520achieving%2520a%2520superior%2520trade-off%2520between%2520accuracy%2520and%2520computational%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Granularity%20Matters%3A%20Rethinking%20Vision%20Transformers%20Beyond%20Fixed%20Patch%20Splitting&entry.906535625=Qiyang%20Yu%20and%20Yu%20Fang%20and%20Tianrui%20Li%20and%20Xuemei%20Cao%20and%20Yan%20Chen%20and%20Jianghao%20Li%20and%20Fan%20Min&entry.1292438233=Vision%20Transformers%20%28ViTs%29%20have%20demonstrated%20strong%20capabilities%20in%20capturing%20global%20dependencies%20but%20often%20struggle%20to%20efficiently%20represent%20fine-grained%20local%20details.%20Existing%20multi-scale%20approaches%20alleviate%20this%20issue%20by%20integrating%20hierarchical%20or%20hybrid%20features%3B%20however%2C%20they%20rely%20on%20fixed%20patch%20sizes%20and%20introduce%20redundant%20computation.%20To%20address%20these%20limitations%2C%20we%20propose%20Granularity-driven%20Vision%20Transformer%20%28Grc-ViT%29%2C%20a%20dynamic%20coarse-to-fine%20framework%20that%20adaptively%20adjusts%20visual%20granularity%20based%20on%20image%20complexity.%20It%20comprises%20two%20key%20stages%3A%20%281%29%20Coarse%20Granularity%20Evaluation%20module%2C%20which%20assesses%20visual%20complexity%20using%20edge%20density%2C%20entropy%2C%20and%20frequency-domain%20cues%20to%20estimate%20suitable%20patch%20and%20window%20sizes%3B%20%282%29%20Fine-grained%20Refinement%20module%2C%20which%20refines%20attention%20computation%20according%20to%20the%20selected%20granularity%2C%20enabling%20efficient%20and%20precise%20feature%20learning.%20Two%20learnable%20parameters%2C%20%CE%B1%20and%20%5Cb%7Beta%7D%2C%20are%20optimized%20end-to-end%20to%20balance%20global%20reasoning%20and%20local%20perception.%20Comprehensive%20evaluations%20demonstrate%20that%20Grc-ViT%20enhances%20fine-grained%20discrimination%20while%20achieving%20a%20superior%20trade-off%20between%20accuracy%20and%20computational%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2511.19021v1&entry.124074799=Read"},
{"title": "nnActive: A Framework for Evaluation of Active Learning in 3D Biomedical Segmentation", "author": "Carsten T. L\u00fcth and Jeremias Traub and Kim-Celine Kahl and Till J. Bungert and Lukas Klein and Lars Kr\u00e4mer and Paul F. Jaeger and Fabian Isensee and Klaus Maier-Hein", "abstract": "Semantic segmentation is crucial for various biomedical applications, yet its reliance on large annotated datasets presents a bottleneck due to the high cost and specialized expertise required for manual labeling. Active Learning (AL) aims to mitigate this challenge by querying only the most informative samples, thereby reducing annotation effort. However, in the domain of 3D biomedical imaging, there is no consensus on whether AL consistently outperforms Random sampling. Four evaluation pitfalls hinder the current methodological assessment. These are (1) restriction to too few datasets and annotation budgets, (2) using 2D models on 3D images without partial annotations, (3) Random baseline not being adapted to the task, and (4) measuring annotation cost only in voxels. In this work, we introduce nnActive, an open-source AL framework that overcomes these pitfalls by (1) means of a large scale study spanning four biomedical imaging datasets and three label regimes, (2) extending nnU-Net by using partial annotations for training with 3D patch-based query selection, (3) proposing Foreground Aware Random sampling strategies tackling the foreground-background class imbalance of medical images and (4) propose the foreground efficiency metric, which captures the low annotation cost of background-regions. We reveal the following findings: (A) while all AL methods outperform standard Random sampling, none reliably surpasses an improved Foreground Aware Random sampling; (B) benefits of AL depend on task specific parameters; (C) Predictive Entropy is overall the best performing AL method, but likely requires the most annotation effort; (D) AL performance can be improved with more compute intensive design choices. As a holistic, open-source framework, nnActive can serve as a catalyst for research and application of AL in 3D biomedical imaging. Code is at: https://github.com/MIC-DKFZ/nnActive", "link": "http://arxiv.org/abs/2511.19183v1", "date": "2025-11-24", "relevancy": 2.2142, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5937}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20nnActive%3A%20A%20Framework%20for%20Evaluation%20of%20Active%20Learning%20in%203D%20Biomedical%20Segmentation&body=Title%3A%20nnActive%3A%20A%20Framework%20for%20Evaluation%20of%20Active%20Learning%20in%203D%20Biomedical%20Segmentation%0AAuthor%3A%20Carsten%20T.%20L%C3%BCth%20and%20Jeremias%20Traub%20and%20Kim-Celine%20Kahl%20and%20Till%20J.%20Bungert%20and%20Lukas%20Klein%20and%20Lars%20Kr%C3%A4mer%20and%20Paul%20F.%20Jaeger%20and%20Fabian%20Isensee%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20Semantic%20segmentation%20is%20crucial%20for%20various%20biomedical%20applications%2C%20yet%20its%20reliance%20on%20large%20annotated%20datasets%20presents%20a%20bottleneck%20due%20to%20the%20high%20cost%20and%20specialized%20expertise%20required%20for%20manual%20labeling.%20Active%20Learning%20%28AL%29%20aims%20to%20mitigate%20this%20challenge%20by%20querying%20only%20the%20most%20informative%20samples%2C%20thereby%20reducing%20annotation%20effort.%20However%2C%20in%20the%20domain%20of%203D%20biomedical%20imaging%2C%20there%20is%20no%20consensus%20on%20whether%20AL%20consistently%20outperforms%20Random%20sampling.%20Four%20evaluation%20pitfalls%20hinder%20the%20current%20methodological%20assessment.%20These%20are%20%281%29%20restriction%20to%20too%20few%20datasets%20and%20annotation%20budgets%2C%20%282%29%20using%202D%20models%20on%203D%20images%20without%20partial%20annotations%2C%20%283%29%20Random%20baseline%20not%20being%20adapted%20to%20the%20task%2C%20and%20%284%29%20measuring%20annotation%20cost%20only%20in%20voxels.%20In%20this%20work%2C%20we%20introduce%20nnActive%2C%20an%20open-source%20AL%20framework%20that%20overcomes%20these%20pitfalls%20by%20%281%29%20means%20of%20a%20large%20scale%20study%20spanning%20four%20biomedical%20imaging%20datasets%20and%20three%20label%20regimes%2C%20%282%29%20extending%20nnU-Net%20by%20using%20partial%20annotations%20for%20training%20with%203D%20patch-based%20query%20selection%2C%20%283%29%20proposing%20Foreground%20Aware%20Random%20sampling%20strategies%20tackling%20the%20foreground-background%20class%20imbalance%20of%20medical%20images%20and%20%284%29%20propose%20the%20foreground%20efficiency%20metric%2C%20which%20captures%20the%20low%20annotation%20cost%20of%20background-regions.%20We%20reveal%20the%20following%20findings%3A%20%28A%29%20while%20all%20AL%20methods%20outperform%20standard%20Random%20sampling%2C%20none%20reliably%20surpasses%20an%20improved%20Foreground%20Aware%20Random%20sampling%3B%20%28B%29%20benefits%20of%20AL%20depend%20on%20task%20specific%20parameters%3B%20%28C%29%20Predictive%20Entropy%20is%20overall%20the%20best%20performing%20AL%20method%2C%20but%20likely%20requires%20the%20most%20annotation%20effort%3B%20%28D%29%20AL%20performance%20can%20be%20improved%20with%20more%20compute%20intensive%20design%20choices.%20As%20a%20holistic%2C%20open-source%20framework%2C%20nnActive%20can%20serve%20as%20a%20catalyst%20for%20research%20and%20application%20of%20AL%20in%203D%20biomedical%20imaging.%20Code%20is%20at%3A%20https%3A//github.com/MIC-DKFZ/nnActive%0ALink%3A%20http%3A//arxiv.org/abs/2511.19183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DnnActive%253A%2520A%2520Framework%2520for%2520Evaluation%2520of%2520Active%2520Learning%2520in%25203D%2520Biomedical%2520Segmentation%26entry.906535625%3DCarsten%2520T.%2520L%25C3%25BCth%2520and%2520Jeremias%2520Traub%2520and%2520Kim-Celine%2520Kahl%2520and%2520Till%2520J.%2520Bungert%2520and%2520Lukas%2520Klein%2520and%2520Lars%2520Kr%25C3%25A4mer%2520and%2520Paul%2520F.%2520Jaeger%2520and%2520Fabian%2520Isensee%2520and%2520Klaus%2520Maier-Hein%26entry.1292438233%3DSemantic%2520segmentation%2520is%2520crucial%2520for%2520various%2520biomedical%2520applications%252C%2520yet%2520its%2520reliance%2520on%2520large%2520annotated%2520datasets%2520presents%2520a%2520bottleneck%2520due%2520to%2520the%2520high%2520cost%2520and%2520specialized%2520expertise%2520required%2520for%2520manual%2520labeling.%2520Active%2520Learning%2520%2528AL%2529%2520aims%2520to%2520mitigate%2520this%2520challenge%2520by%2520querying%2520only%2520the%2520most%2520informative%2520samples%252C%2520thereby%2520reducing%2520annotation%2520effort.%2520However%252C%2520in%2520the%2520domain%2520of%25203D%2520biomedical%2520imaging%252C%2520there%2520is%2520no%2520consensus%2520on%2520whether%2520AL%2520consistently%2520outperforms%2520Random%2520sampling.%2520Four%2520evaluation%2520pitfalls%2520hinder%2520the%2520current%2520methodological%2520assessment.%2520These%2520are%2520%25281%2529%2520restriction%2520to%2520too%2520few%2520datasets%2520and%2520annotation%2520budgets%252C%2520%25282%2529%2520using%25202D%2520models%2520on%25203D%2520images%2520without%2520partial%2520annotations%252C%2520%25283%2529%2520Random%2520baseline%2520not%2520being%2520adapted%2520to%2520the%2520task%252C%2520and%2520%25284%2529%2520measuring%2520annotation%2520cost%2520only%2520in%2520voxels.%2520In%2520this%2520work%252C%2520we%2520introduce%2520nnActive%252C%2520an%2520open-source%2520AL%2520framework%2520that%2520overcomes%2520these%2520pitfalls%2520by%2520%25281%2529%2520means%2520of%2520a%2520large%2520scale%2520study%2520spanning%2520four%2520biomedical%2520imaging%2520datasets%2520and%2520three%2520label%2520regimes%252C%2520%25282%2529%2520extending%2520nnU-Net%2520by%2520using%2520partial%2520annotations%2520for%2520training%2520with%25203D%2520patch-based%2520query%2520selection%252C%2520%25283%2529%2520proposing%2520Foreground%2520Aware%2520Random%2520sampling%2520strategies%2520tackling%2520the%2520foreground-background%2520class%2520imbalance%2520of%2520medical%2520images%2520and%2520%25284%2529%2520propose%2520the%2520foreground%2520efficiency%2520metric%252C%2520which%2520captures%2520the%2520low%2520annotation%2520cost%2520of%2520background-regions.%2520We%2520reveal%2520the%2520following%2520findings%253A%2520%2528A%2529%2520while%2520all%2520AL%2520methods%2520outperform%2520standard%2520Random%2520sampling%252C%2520none%2520reliably%2520surpasses%2520an%2520improved%2520Foreground%2520Aware%2520Random%2520sampling%253B%2520%2528B%2529%2520benefits%2520of%2520AL%2520depend%2520on%2520task%2520specific%2520parameters%253B%2520%2528C%2529%2520Predictive%2520Entropy%2520is%2520overall%2520the%2520best%2520performing%2520AL%2520method%252C%2520but%2520likely%2520requires%2520the%2520most%2520annotation%2520effort%253B%2520%2528D%2529%2520AL%2520performance%2520can%2520be%2520improved%2520with%2520more%2520compute%2520intensive%2520design%2520choices.%2520As%2520a%2520holistic%252C%2520open-source%2520framework%252C%2520nnActive%2520can%2520serve%2520as%2520a%2520catalyst%2520for%2520research%2520and%2520application%2520of%2520AL%2520in%25203D%2520biomedical%2520imaging.%2520Code%2520is%2520at%253A%2520https%253A//github.com/MIC-DKFZ/nnActive%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=nnActive%3A%20A%20Framework%20for%20Evaluation%20of%20Active%20Learning%20in%203D%20Biomedical%20Segmentation&entry.906535625=Carsten%20T.%20L%C3%BCth%20and%20Jeremias%20Traub%20and%20Kim-Celine%20Kahl%20and%20Till%20J.%20Bungert%20and%20Lukas%20Klein%20and%20Lars%20Kr%C3%A4mer%20and%20Paul%20F.%20Jaeger%20and%20Fabian%20Isensee%20and%20Klaus%20Maier-Hein&entry.1292438233=Semantic%20segmentation%20is%20crucial%20for%20various%20biomedical%20applications%2C%20yet%20its%20reliance%20on%20large%20annotated%20datasets%20presents%20a%20bottleneck%20due%20to%20the%20high%20cost%20and%20specialized%20expertise%20required%20for%20manual%20labeling.%20Active%20Learning%20%28AL%29%20aims%20to%20mitigate%20this%20challenge%20by%20querying%20only%20the%20most%20informative%20samples%2C%20thereby%20reducing%20annotation%20effort.%20However%2C%20in%20the%20domain%20of%203D%20biomedical%20imaging%2C%20there%20is%20no%20consensus%20on%20whether%20AL%20consistently%20outperforms%20Random%20sampling.%20Four%20evaluation%20pitfalls%20hinder%20the%20current%20methodological%20assessment.%20These%20are%20%281%29%20restriction%20to%20too%20few%20datasets%20and%20annotation%20budgets%2C%20%282%29%20using%202D%20models%20on%203D%20images%20without%20partial%20annotations%2C%20%283%29%20Random%20baseline%20not%20being%20adapted%20to%20the%20task%2C%20and%20%284%29%20measuring%20annotation%20cost%20only%20in%20voxels.%20In%20this%20work%2C%20we%20introduce%20nnActive%2C%20an%20open-source%20AL%20framework%20that%20overcomes%20these%20pitfalls%20by%20%281%29%20means%20of%20a%20large%20scale%20study%20spanning%20four%20biomedical%20imaging%20datasets%20and%20three%20label%20regimes%2C%20%282%29%20extending%20nnU-Net%20by%20using%20partial%20annotations%20for%20training%20with%203D%20patch-based%20query%20selection%2C%20%283%29%20proposing%20Foreground%20Aware%20Random%20sampling%20strategies%20tackling%20the%20foreground-background%20class%20imbalance%20of%20medical%20images%20and%20%284%29%20propose%20the%20foreground%20efficiency%20metric%2C%20which%20captures%20the%20low%20annotation%20cost%20of%20background-regions.%20We%20reveal%20the%20following%20findings%3A%20%28A%29%20while%20all%20AL%20methods%20outperform%20standard%20Random%20sampling%2C%20none%20reliably%20surpasses%20an%20improved%20Foreground%20Aware%20Random%20sampling%3B%20%28B%29%20benefits%20of%20AL%20depend%20on%20task%20specific%20parameters%3B%20%28C%29%20Predictive%20Entropy%20is%20overall%20the%20best%20performing%20AL%20method%2C%20but%20likely%20requires%20the%20most%20annotation%20effort%3B%20%28D%29%20AL%20performance%20can%20be%20improved%20with%20more%20compute%20intensive%20design%20choices.%20As%20a%20holistic%2C%20open-source%20framework%2C%20nnActive%20can%20serve%20as%20a%20catalyst%20for%20research%20and%20application%20of%20AL%20in%203D%20biomedical%20imaging.%20Code%20is%20at%3A%20https%3A//github.com/MIC-DKFZ/nnActive&entry.1838667208=http%3A//arxiv.org/abs/2511.19183v1&entry.124074799=Read"},
{"title": "Higher-Order Regularization Learning on Hypergraphs", "author": "Adrien Weihs and Andrea L. Bertozzi and Matthew Thorpe", "abstract": "Higher-Order Hypergraph Learning (HOHL) was recently introduced as a principled alternative to classical hypergraph regularization, enforcing higher-order smoothness via powers of multiscale Laplacians induced by the hypergraph structure. Prior work established the well- and ill-posedness of HOHL through an asymptotic consistency analysis in geometric settings. We extend this theoretical foundation by proving the consistency of a truncated version of HOHL and deriving explicit convergence rates when HOHL is used as a regularizer in fully supervised learning. We further demonstrate its strong empirical performance in active learning and in datasets lacking an underlying geometric structure, highlighting HOHL's versatility and robustness across diverse learning settings.", "link": "http://arxiv.org/abs/2510.26533v2", "date": "2025-11-24", "relevancy": 2.2091, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4795}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4259}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.42}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-Order%20Regularization%20Learning%20on%20Hypergraphs&body=Title%3A%20Higher-Order%20Regularization%20Learning%20on%20Hypergraphs%0AAuthor%3A%20Adrien%20Weihs%20and%20Andrea%20L.%20Bertozzi%20and%20Matthew%20Thorpe%0AAbstract%3A%20Higher-Order%20Hypergraph%20Learning%20%28HOHL%29%20was%20recently%20introduced%20as%20a%20principled%20alternative%20to%20classical%20hypergraph%20regularization%2C%20enforcing%20higher-order%20smoothness%20via%20powers%20of%20multiscale%20Laplacians%20induced%20by%20the%20hypergraph%20structure.%20Prior%20work%20established%20the%20well-%20and%20ill-posedness%20of%20HOHL%20through%20an%20asymptotic%20consistency%20analysis%20in%20geometric%20settings.%20We%20extend%20this%20theoretical%20foundation%20by%20proving%20the%20consistency%20of%20a%20truncated%20version%20of%20HOHL%20and%20deriving%20explicit%20convergence%20rates%20when%20HOHL%20is%20used%20as%20a%20regularizer%20in%20fully%20supervised%20learning.%20We%20further%20demonstrate%20its%20strong%20empirical%20performance%20in%20active%20learning%20and%20in%20datasets%20lacking%20an%20underlying%20geometric%20structure%2C%20highlighting%20HOHL%27s%20versatility%20and%20robustness%20across%20diverse%20learning%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2510.26533v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-Order%2520Regularization%2520Learning%2520on%2520Hypergraphs%26entry.906535625%3DAdrien%2520Weihs%2520and%2520Andrea%2520L.%2520Bertozzi%2520and%2520Matthew%2520Thorpe%26entry.1292438233%3DHigher-Order%2520Hypergraph%2520Learning%2520%2528HOHL%2529%2520was%2520recently%2520introduced%2520as%2520a%2520principled%2520alternative%2520to%2520classical%2520hypergraph%2520regularization%252C%2520enforcing%2520higher-order%2520smoothness%2520via%2520powers%2520of%2520multiscale%2520Laplacians%2520induced%2520by%2520the%2520hypergraph%2520structure.%2520Prior%2520work%2520established%2520the%2520well-%2520and%2520ill-posedness%2520of%2520HOHL%2520through%2520an%2520asymptotic%2520consistency%2520analysis%2520in%2520geometric%2520settings.%2520We%2520extend%2520this%2520theoretical%2520foundation%2520by%2520proving%2520the%2520consistency%2520of%2520a%2520truncated%2520version%2520of%2520HOHL%2520and%2520deriving%2520explicit%2520convergence%2520rates%2520when%2520HOHL%2520is%2520used%2520as%2520a%2520regularizer%2520in%2520fully%2520supervised%2520learning.%2520We%2520further%2520demonstrate%2520its%2520strong%2520empirical%2520performance%2520in%2520active%2520learning%2520and%2520in%2520datasets%2520lacking%2520an%2520underlying%2520geometric%2520structure%252C%2520highlighting%2520HOHL%2527s%2520versatility%2520and%2520robustness%2520across%2520diverse%2520learning%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26533v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-Order%20Regularization%20Learning%20on%20Hypergraphs&entry.906535625=Adrien%20Weihs%20and%20Andrea%20L.%20Bertozzi%20and%20Matthew%20Thorpe&entry.1292438233=Higher-Order%20Hypergraph%20Learning%20%28HOHL%29%20was%20recently%20introduced%20as%20a%20principled%20alternative%20to%20classical%20hypergraph%20regularization%2C%20enforcing%20higher-order%20smoothness%20via%20powers%20of%20multiscale%20Laplacians%20induced%20by%20the%20hypergraph%20structure.%20Prior%20work%20established%20the%20well-%20and%20ill-posedness%20of%20HOHL%20through%20an%20asymptotic%20consistency%20analysis%20in%20geometric%20settings.%20We%20extend%20this%20theoretical%20foundation%20by%20proving%20the%20consistency%20of%20a%20truncated%20version%20of%20HOHL%20and%20deriving%20explicit%20convergence%20rates%20when%20HOHL%20is%20used%20as%20a%20regularizer%20in%20fully%20supervised%20learning.%20We%20further%20demonstrate%20its%20strong%20empirical%20performance%20in%20active%20learning%20and%20in%20datasets%20lacking%20an%20underlying%20geometric%20structure%2C%20highlighting%20HOHL%27s%20versatility%20and%20robustness%20across%20diverse%20learning%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2510.26533v2&entry.124074799=Read"},
{"title": "Leveraging Spatiotemporal Graph Neural Networks for Multi-Store Sales Forecasting", "author": "Manish Singh and Arpita Dayama", "abstract": "This work evaluates the effectiveness of spatiotemporal Graph Neural Networks (GNNs) for multi-store retail sales forecasting and compares their performance against ARIMA, LSTM, and XGBoost baselines. Using weekly sales data from 45 Walmart stores, we construct a relational forecasting framework that models inter-store dependencies through a learned adaptive graph. The proposed STGNN predicts log-differenced sales and reconstructs final values through a residual path, enabling stable training and improved generalisation. Experiments show that STGNN achieves the lowest overall forecasting error, outperforming all baselines in Normalised Total Absolute Error, P90 MAPE, and variance of MAPE across stores. Analysis of the learned adjacency matrix reveals meaningful functional store clusters and high-influence nodes that emerge without geographic metadata. These results demonstrate that relational structure significantly improves forecast quality in interconnected retail environments and establishes STGNNs as a robust modelling choice for multi-store demand prediction.", "link": "http://arxiv.org/abs/2511.19267v1", "date": "2025-11-24", "relevancy": 2.2086, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.457}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4395}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Spatiotemporal%20Graph%20Neural%20Networks%20for%20Multi-Store%20Sales%20Forecasting&body=Title%3A%20Leveraging%20Spatiotemporal%20Graph%20Neural%20Networks%20for%20Multi-Store%20Sales%20Forecasting%0AAuthor%3A%20Manish%20Singh%20and%20Arpita%20Dayama%0AAbstract%3A%20This%20work%20evaluates%20the%20effectiveness%20of%20spatiotemporal%20Graph%20Neural%20Networks%20%28GNNs%29%20for%20multi-store%20retail%20sales%20forecasting%20and%20compares%20their%20performance%20against%20ARIMA%2C%20LSTM%2C%20and%20XGBoost%20baselines.%20Using%20weekly%20sales%20data%20from%2045%20Walmart%20stores%2C%20we%20construct%20a%20relational%20forecasting%20framework%20that%20models%20inter-store%20dependencies%20through%20a%20learned%20adaptive%20graph.%20The%20proposed%20STGNN%20predicts%20log-differenced%20sales%20and%20reconstructs%20final%20values%20through%20a%20residual%20path%2C%20enabling%20stable%20training%20and%20improved%20generalisation.%20Experiments%20show%20that%20STGNN%20achieves%20the%20lowest%20overall%20forecasting%20error%2C%20outperforming%20all%20baselines%20in%20Normalised%20Total%20Absolute%20Error%2C%20P90%20MAPE%2C%20and%20variance%20of%20MAPE%20across%20stores.%20Analysis%20of%20the%20learned%20adjacency%20matrix%20reveals%20meaningful%20functional%20store%20clusters%20and%20high-influence%20nodes%20that%20emerge%20without%20geographic%20metadata.%20These%20results%20demonstrate%20that%20relational%20structure%20significantly%20improves%20forecast%20quality%20in%20interconnected%20retail%20environments%20and%20establishes%20STGNNs%20as%20a%20robust%20modelling%20choice%20for%20multi-store%20demand%20prediction.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Spatiotemporal%2520Graph%2520Neural%2520Networks%2520for%2520Multi-Store%2520Sales%2520Forecasting%26entry.906535625%3DManish%2520Singh%2520and%2520Arpita%2520Dayama%26entry.1292438233%3DThis%2520work%2520evaluates%2520the%2520effectiveness%2520of%2520spatiotemporal%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520for%2520multi-store%2520retail%2520sales%2520forecasting%2520and%2520compares%2520their%2520performance%2520against%2520ARIMA%252C%2520LSTM%252C%2520and%2520XGBoost%2520baselines.%2520Using%2520weekly%2520sales%2520data%2520from%252045%2520Walmart%2520stores%252C%2520we%2520construct%2520a%2520relational%2520forecasting%2520framework%2520that%2520models%2520inter-store%2520dependencies%2520through%2520a%2520learned%2520adaptive%2520graph.%2520The%2520proposed%2520STGNN%2520predicts%2520log-differenced%2520sales%2520and%2520reconstructs%2520final%2520values%2520through%2520a%2520residual%2520path%252C%2520enabling%2520stable%2520training%2520and%2520improved%2520generalisation.%2520Experiments%2520show%2520that%2520STGNN%2520achieves%2520the%2520lowest%2520overall%2520forecasting%2520error%252C%2520outperforming%2520all%2520baselines%2520in%2520Normalised%2520Total%2520Absolute%2520Error%252C%2520P90%2520MAPE%252C%2520and%2520variance%2520of%2520MAPE%2520across%2520stores.%2520Analysis%2520of%2520the%2520learned%2520adjacency%2520matrix%2520reveals%2520meaningful%2520functional%2520store%2520clusters%2520and%2520high-influence%2520nodes%2520that%2520emerge%2520without%2520geographic%2520metadata.%2520These%2520results%2520demonstrate%2520that%2520relational%2520structure%2520significantly%2520improves%2520forecast%2520quality%2520in%2520interconnected%2520retail%2520environments%2520and%2520establishes%2520STGNNs%2520as%2520a%2520robust%2520modelling%2520choice%2520for%2520multi-store%2520demand%2520prediction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Spatiotemporal%20Graph%20Neural%20Networks%20for%20Multi-Store%20Sales%20Forecasting&entry.906535625=Manish%20Singh%20and%20Arpita%20Dayama&entry.1292438233=This%20work%20evaluates%20the%20effectiveness%20of%20spatiotemporal%20Graph%20Neural%20Networks%20%28GNNs%29%20for%20multi-store%20retail%20sales%20forecasting%20and%20compares%20their%20performance%20against%20ARIMA%2C%20LSTM%2C%20and%20XGBoost%20baselines.%20Using%20weekly%20sales%20data%20from%2045%20Walmart%20stores%2C%20we%20construct%20a%20relational%20forecasting%20framework%20that%20models%20inter-store%20dependencies%20through%20a%20learned%20adaptive%20graph.%20The%20proposed%20STGNN%20predicts%20log-differenced%20sales%20and%20reconstructs%20final%20values%20through%20a%20residual%20path%2C%20enabling%20stable%20training%20and%20improved%20generalisation.%20Experiments%20show%20that%20STGNN%20achieves%20the%20lowest%20overall%20forecasting%20error%2C%20outperforming%20all%20baselines%20in%20Normalised%20Total%20Absolute%20Error%2C%20P90%20MAPE%2C%20and%20variance%20of%20MAPE%20across%20stores.%20Analysis%20of%20the%20learned%20adjacency%20matrix%20reveals%20meaningful%20functional%20store%20clusters%20and%20high-influence%20nodes%20that%20emerge%20without%20geographic%20metadata.%20These%20results%20demonstrate%20that%20relational%20structure%20significantly%20improves%20forecast%20quality%20in%20interconnected%20retail%20environments%20and%20establishes%20STGNNs%20as%20a%20robust%20modelling%20choice%20for%20multi-store%20demand%20prediction.&entry.1838667208=http%3A//arxiv.org/abs/2511.19267v1&entry.124074799=Read"},
{"title": "When, Where and Why to Average Weights?", "author": "Niccol\u00f2 Ajroldi and Antonio Orvieto and Jonas Geiping", "abstract": "Averaging checkpoints along the training trajectory is a simple yet powerful approach to improve the generalization performance of Machine Learning models and reduce training time. Motivated by these potential gains, and in an effort to fairly and thoroughly benchmark this technique, we present an extensive evaluation of averaging techniques in modern Deep Learning, which we perform using AlgoPerf \\citep{dahl_benchmarking_2023}, a large-scale benchmark for optimization algorithms. We investigate whether weight averaging can reduce training time, improve generalization, and replace learning rate decay, as suggested by recent literature. Our evaluation across seven architectures and datasets reveals that averaging significantly accelerates training and yields considerable efficiency gains, at the price of a minimal implementation and memory cost, while mildly improving generalization across all considered workloads. Finally, we explore the relationship between averaging and learning rate annealing and show how to optimally combine the two to achieve the best performances.", "link": "http://arxiv.org/abs/2502.06761v3", "date": "2025-11-24", "relevancy": 2.207, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4647}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4374}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%2C%20Where%20and%20Why%20to%20Average%20Weights%3F&body=Title%3A%20When%2C%20Where%20and%20Why%20to%20Average%20Weights%3F%0AAuthor%3A%20Niccol%C3%B2%20Ajroldi%20and%20Antonio%20Orvieto%20and%20Jonas%20Geiping%0AAbstract%3A%20Averaging%20checkpoints%20along%20the%20training%20trajectory%20is%20a%20simple%20yet%20powerful%20approach%20to%20improve%20the%20generalization%20performance%20of%20Machine%20Learning%20models%20and%20reduce%20training%20time.%20Motivated%20by%20these%20potential%20gains%2C%20and%20in%20an%20effort%20to%20fairly%20and%20thoroughly%20benchmark%20this%20technique%2C%20we%20present%20an%20extensive%20evaluation%20of%20averaging%20techniques%20in%20modern%20Deep%20Learning%2C%20which%20we%20perform%20using%20AlgoPerf%20%5Ccitep%7Bdahl_benchmarking_2023%7D%2C%20a%20large-scale%20benchmark%20for%20optimization%20algorithms.%20We%20investigate%20whether%20weight%20averaging%20can%20reduce%20training%20time%2C%20improve%20generalization%2C%20and%20replace%20learning%20rate%20decay%2C%20as%20suggested%20by%20recent%20literature.%20Our%20evaluation%20across%20seven%20architectures%20and%20datasets%20reveals%20that%20averaging%20significantly%20accelerates%20training%20and%20yields%20considerable%20efficiency%20gains%2C%20at%20the%20price%20of%20a%20minimal%20implementation%20and%20memory%20cost%2C%20while%20mildly%20improving%20generalization%20across%20all%20considered%20workloads.%20Finally%2C%20we%20explore%20the%20relationship%20between%20averaging%20and%20learning%20rate%20annealing%20and%20show%20how%20to%20optimally%20combine%20the%20two%20to%20achieve%20the%20best%20performances.%0ALink%3A%20http%3A//arxiv.org/abs/2502.06761v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%252C%2520Where%2520and%2520Why%2520to%2520Average%2520Weights%253F%26entry.906535625%3DNiccol%25C3%25B2%2520Ajroldi%2520and%2520Antonio%2520Orvieto%2520and%2520Jonas%2520Geiping%26entry.1292438233%3DAveraging%2520checkpoints%2520along%2520the%2520training%2520trajectory%2520is%2520a%2520simple%2520yet%2520powerful%2520approach%2520to%2520improve%2520the%2520generalization%2520performance%2520of%2520Machine%2520Learning%2520models%2520and%2520reduce%2520training%2520time.%2520Motivated%2520by%2520these%2520potential%2520gains%252C%2520and%2520in%2520an%2520effort%2520to%2520fairly%2520and%2520thoroughly%2520benchmark%2520this%2520technique%252C%2520we%2520present%2520an%2520extensive%2520evaluation%2520of%2520averaging%2520techniques%2520in%2520modern%2520Deep%2520Learning%252C%2520which%2520we%2520perform%2520using%2520AlgoPerf%2520%255Ccitep%257Bdahl_benchmarking_2023%257D%252C%2520a%2520large-scale%2520benchmark%2520for%2520optimization%2520algorithms.%2520We%2520investigate%2520whether%2520weight%2520averaging%2520can%2520reduce%2520training%2520time%252C%2520improve%2520generalization%252C%2520and%2520replace%2520learning%2520rate%2520decay%252C%2520as%2520suggested%2520by%2520recent%2520literature.%2520Our%2520evaluation%2520across%2520seven%2520architectures%2520and%2520datasets%2520reveals%2520that%2520averaging%2520significantly%2520accelerates%2520training%2520and%2520yields%2520considerable%2520efficiency%2520gains%252C%2520at%2520the%2520price%2520of%2520a%2520minimal%2520implementation%2520and%2520memory%2520cost%252C%2520while%2520mildly%2520improving%2520generalization%2520across%2520all%2520considered%2520workloads.%2520Finally%252C%2520we%2520explore%2520the%2520relationship%2520between%2520averaging%2520and%2520learning%2520rate%2520annealing%2520and%2520show%2520how%2520to%2520optimally%2520combine%2520the%2520two%2520to%2520achieve%2520the%2520best%2520performances.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06761v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%2C%20Where%20and%20Why%20to%20Average%20Weights%3F&entry.906535625=Niccol%C3%B2%20Ajroldi%20and%20Antonio%20Orvieto%20and%20Jonas%20Geiping&entry.1292438233=Averaging%20checkpoints%20along%20the%20training%20trajectory%20is%20a%20simple%20yet%20powerful%20approach%20to%20improve%20the%20generalization%20performance%20of%20Machine%20Learning%20models%20and%20reduce%20training%20time.%20Motivated%20by%20these%20potential%20gains%2C%20and%20in%20an%20effort%20to%20fairly%20and%20thoroughly%20benchmark%20this%20technique%2C%20we%20present%20an%20extensive%20evaluation%20of%20averaging%20techniques%20in%20modern%20Deep%20Learning%2C%20which%20we%20perform%20using%20AlgoPerf%20%5Ccitep%7Bdahl_benchmarking_2023%7D%2C%20a%20large-scale%20benchmark%20for%20optimization%20algorithms.%20We%20investigate%20whether%20weight%20averaging%20can%20reduce%20training%20time%2C%20improve%20generalization%2C%20and%20replace%20learning%20rate%20decay%2C%20as%20suggested%20by%20recent%20literature.%20Our%20evaluation%20across%20seven%20architectures%20and%20datasets%20reveals%20that%20averaging%20significantly%20accelerates%20training%20and%20yields%20considerable%20efficiency%20gains%2C%20at%20the%20price%20of%20a%20minimal%20implementation%20and%20memory%20cost%2C%20while%20mildly%20improving%20generalization%20across%20all%20considered%20workloads.%20Finally%2C%20we%20explore%20the%20relationship%20between%20averaging%20and%20learning%20rate%20annealing%20and%20show%20how%20to%20optimally%20combine%20the%20two%20to%20achieve%20the%20best%20performances.&entry.1838667208=http%3A//arxiv.org/abs/2502.06761v3&entry.124074799=Read"},
{"title": "Reference-Free Sampling-Based Model Predictive Control", "author": "Fabian Schramm and Pierre Fabre and Nicolas Perrin-Gilbert and Justin Carpentier", "abstract": "We present a sampling-based model predictive control (MPC) framework that enables emergent locomotion without relying on handcrafted gait patterns or predefined contact sequences. Our method discovers diverse motion patterns, ranging from trotting to galloping, robust standing policies, jumping, and handstand balancing, purely through the optimization of high-level objectives. Building on model predictive path integral (MPPI), we propose a dual-space spline parameterization that operates on position and velocity control points. Our approach enables contact-making and contact-breaking strategies that adapt automatically to task requirements, requiring only a limited number of sampled trajectories. This sample efficiency allows us to achieve real-time control on standard CPU hardware, eliminating the need for GPU acceleration typically required by other state-of-the-art MPPI methods. We validate our approach on the Go2 quadrupedal robot, demonstrating various emergent gaits and basic jumping capabilities. In simulation, we further showcase more complex behaviors, such as backflips, dynamic handstand balancing and locomotion on a Humanoid, all without requiring reference tracking or offline pre-training.", "link": "http://arxiv.org/abs/2511.19204v1", "date": "2025-11-24", "relevancy": 2.2049, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6043}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5589}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reference-Free%20Sampling-Based%20Model%20Predictive%20Control&body=Title%3A%20Reference-Free%20Sampling-Based%20Model%20Predictive%20Control%0AAuthor%3A%20Fabian%20Schramm%20and%20Pierre%20Fabre%20and%20Nicolas%20Perrin-Gilbert%20and%20Justin%20Carpentier%0AAbstract%3A%20We%20present%20a%20sampling-based%20model%20predictive%20control%20%28MPC%29%20framework%20that%20enables%20emergent%20locomotion%20without%20relying%20on%20handcrafted%20gait%20patterns%20or%20predefined%20contact%20sequences.%20Our%20method%20discovers%20diverse%20motion%20patterns%2C%20ranging%20from%20trotting%20to%20galloping%2C%20robust%20standing%20policies%2C%20jumping%2C%20and%20handstand%20balancing%2C%20purely%20through%20the%20optimization%20of%20high-level%20objectives.%20Building%20on%20model%20predictive%20path%20integral%20%28MPPI%29%2C%20we%20propose%20a%20dual-space%20spline%20parameterization%20that%20operates%20on%20position%20and%20velocity%20control%20points.%20Our%20approach%20enables%20contact-making%20and%20contact-breaking%20strategies%20that%20adapt%20automatically%20to%20task%20requirements%2C%20requiring%20only%20a%20limited%20number%20of%20sampled%20trajectories.%20This%20sample%20efficiency%20allows%20us%20to%20achieve%20real-time%20control%20on%20standard%20CPU%20hardware%2C%20eliminating%20the%20need%20for%20GPU%20acceleration%20typically%20required%20by%20other%20state-of-the-art%20MPPI%20methods.%20We%20validate%20our%20approach%20on%20the%20Go2%20quadrupedal%20robot%2C%20demonstrating%20various%20emergent%20gaits%20and%20basic%20jumping%20capabilities.%20In%20simulation%2C%20we%20further%20showcase%20more%20complex%20behaviors%2C%20such%20as%20backflips%2C%20dynamic%20handstand%20balancing%20and%20locomotion%20on%20a%20Humanoid%2C%20all%20without%20requiring%20reference%20tracking%20or%20offline%20pre-training.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReference-Free%2520Sampling-Based%2520Model%2520Predictive%2520Control%26entry.906535625%3DFabian%2520Schramm%2520and%2520Pierre%2520Fabre%2520and%2520Nicolas%2520Perrin-Gilbert%2520and%2520Justin%2520Carpentier%26entry.1292438233%3DWe%2520present%2520a%2520sampling-based%2520model%2520predictive%2520control%2520%2528MPC%2529%2520framework%2520that%2520enables%2520emergent%2520locomotion%2520without%2520relying%2520on%2520handcrafted%2520gait%2520patterns%2520or%2520predefined%2520contact%2520sequences.%2520Our%2520method%2520discovers%2520diverse%2520motion%2520patterns%252C%2520ranging%2520from%2520trotting%2520to%2520galloping%252C%2520robust%2520standing%2520policies%252C%2520jumping%252C%2520and%2520handstand%2520balancing%252C%2520purely%2520through%2520the%2520optimization%2520of%2520high-level%2520objectives.%2520Building%2520on%2520model%2520predictive%2520path%2520integral%2520%2528MPPI%2529%252C%2520we%2520propose%2520a%2520dual-space%2520spline%2520parameterization%2520that%2520operates%2520on%2520position%2520and%2520velocity%2520control%2520points.%2520Our%2520approach%2520enables%2520contact-making%2520and%2520contact-breaking%2520strategies%2520that%2520adapt%2520automatically%2520to%2520task%2520requirements%252C%2520requiring%2520only%2520a%2520limited%2520number%2520of%2520sampled%2520trajectories.%2520This%2520sample%2520efficiency%2520allows%2520us%2520to%2520achieve%2520real-time%2520control%2520on%2520standard%2520CPU%2520hardware%252C%2520eliminating%2520the%2520need%2520for%2520GPU%2520acceleration%2520typically%2520required%2520by%2520other%2520state-of-the-art%2520MPPI%2520methods.%2520We%2520validate%2520our%2520approach%2520on%2520the%2520Go2%2520quadrupedal%2520robot%252C%2520demonstrating%2520various%2520emergent%2520gaits%2520and%2520basic%2520jumping%2520capabilities.%2520In%2520simulation%252C%2520we%2520further%2520showcase%2520more%2520complex%2520behaviors%252C%2520such%2520as%2520backflips%252C%2520dynamic%2520handstand%2520balancing%2520and%2520locomotion%2520on%2520a%2520Humanoid%252C%2520all%2520without%2520requiring%2520reference%2520tracking%2520or%2520offline%2520pre-training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reference-Free%20Sampling-Based%20Model%20Predictive%20Control&entry.906535625=Fabian%20Schramm%20and%20Pierre%20Fabre%20and%20Nicolas%20Perrin-Gilbert%20and%20Justin%20Carpentier&entry.1292438233=We%20present%20a%20sampling-based%20model%20predictive%20control%20%28MPC%29%20framework%20that%20enables%20emergent%20locomotion%20without%20relying%20on%20handcrafted%20gait%20patterns%20or%20predefined%20contact%20sequences.%20Our%20method%20discovers%20diverse%20motion%20patterns%2C%20ranging%20from%20trotting%20to%20galloping%2C%20robust%20standing%20policies%2C%20jumping%2C%20and%20handstand%20balancing%2C%20purely%20through%20the%20optimization%20of%20high-level%20objectives.%20Building%20on%20model%20predictive%20path%20integral%20%28MPPI%29%2C%20we%20propose%20a%20dual-space%20spline%20parameterization%20that%20operates%20on%20position%20and%20velocity%20control%20points.%20Our%20approach%20enables%20contact-making%20and%20contact-breaking%20strategies%20that%20adapt%20automatically%20to%20task%20requirements%2C%20requiring%20only%20a%20limited%20number%20of%20sampled%20trajectories.%20This%20sample%20efficiency%20allows%20us%20to%20achieve%20real-time%20control%20on%20standard%20CPU%20hardware%2C%20eliminating%20the%20need%20for%20GPU%20acceleration%20typically%20required%20by%20other%20state-of-the-art%20MPPI%20methods.%20We%20validate%20our%20approach%20on%20the%20Go2%20quadrupedal%20robot%2C%20demonstrating%20various%20emergent%20gaits%20and%20basic%20jumping%20capabilities.%20In%20simulation%2C%20we%20further%20showcase%20more%20complex%20behaviors%2C%20such%20as%20backflips%2C%20dynamic%20handstand%20balancing%20and%20locomotion%20on%20a%20Humanoid%2C%20all%20without%20requiring%20reference%20tracking%20or%20offline%20pre-training.&entry.1838667208=http%3A//arxiv.org/abs/2511.19204v1&entry.124074799=Read"},
{"title": "CDLM: Consistency Diffusion Language Models For Faster Sampling", "author": "Minseo Kim and Chenfeng Xu and Coleman Hooper and Harman Singh and Ben Athiwaratkun and Ce Zhang and Kurt Keutzer and Amir Gholami", "abstract": "Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.", "link": "http://arxiv.org/abs/2511.19269v1", "date": "2025-11-24", "relevancy": 2.1971, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6277}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5533}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CDLM%3A%20Consistency%20Diffusion%20Language%20Models%20For%20Faster%20Sampling&body=Title%3A%20CDLM%3A%20Consistency%20Diffusion%20Language%20Models%20For%20Faster%20Sampling%0AAuthor%3A%20Minseo%20Kim%20and%20Chenfeng%20Xu%20and%20Coleman%20Hooper%20and%20Harman%20Singh%20and%20Ben%20Athiwaratkun%20and%20Ce%20Zhang%20and%20Kurt%20Keutzer%20and%20Amir%20Gholami%0AAbstract%3A%20Diffusion%20Language%20Models%20%28DLMs%29%20offer%20a%20promising%20parallel%20generation%20paradigm%20but%20suffer%20from%20slow%20inference%20due%20to%20numerous%20refinement%20steps%20and%20the%20inability%20to%20use%20standard%20KV%20caching.%20We%20introduce%20CDLM%20%28Consistency%20Diffusion%20Language%20Models%29%2C%20a%20training-based%20acceleration%20method%20that%20simultaneously%20tackles%20both%20bottlenecks.%20CDLM%20integrates%20consistency%20modeling%20to%20drastically%20reduce%20the%20number%20of%20required%20sampling%20steps%20by%20enabling%20multi-token%20finalization.%20Furthermore%2C%20we%20enforce%20a%20block-wise%20causal%20attention%20mask%20during%20fine-tuning%2C%20making%20the%20model%20fully%20compatible%20with%20KV%20caching.%20Experiments%20show%20CDLM%20achieves%203.6x-14.5x%20lower%20latency%20while%20maintaining%20competitive%20accuracy%20on%20math%20and%20coding%20tasks.%20The%20full%20training%20and%20evaluation%20code%20is%20available%20at%20https%3A//github.com/SqueezeAILab/CDLM.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCDLM%253A%2520Consistency%2520Diffusion%2520Language%2520Models%2520For%2520Faster%2520Sampling%26entry.906535625%3DMinseo%2520Kim%2520and%2520Chenfeng%2520Xu%2520and%2520Coleman%2520Hooper%2520and%2520Harman%2520Singh%2520and%2520Ben%2520Athiwaratkun%2520and%2520Ce%2520Zhang%2520and%2520Kurt%2520Keutzer%2520and%2520Amir%2520Gholami%26entry.1292438233%3DDiffusion%2520Language%2520Models%2520%2528DLMs%2529%2520offer%2520a%2520promising%2520parallel%2520generation%2520paradigm%2520but%2520suffer%2520from%2520slow%2520inference%2520due%2520to%2520numerous%2520refinement%2520steps%2520and%2520the%2520inability%2520to%2520use%2520standard%2520KV%2520caching.%2520We%2520introduce%2520CDLM%2520%2528Consistency%2520Diffusion%2520Language%2520Models%2529%252C%2520a%2520training-based%2520acceleration%2520method%2520that%2520simultaneously%2520tackles%2520both%2520bottlenecks.%2520CDLM%2520integrates%2520consistency%2520modeling%2520to%2520drastically%2520reduce%2520the%2520number%2520of%2520required%2520sampling%2520steps%2520by%2520enabling%2520multi-token%2520finalization.%2520Furthermore%252C%2520we%2520enforce%2520a%2520block-wise%2520causal%2520attention%2520mask%2520during%2520fine-tuning%252C%2520making%2520the%2520model%2520fully%2520compatible%2520with%2520KV%2520caching.%2520Experiments%2520show%2520CDLM%2520achieves%25203.6x-14.5x%2520lower%2520latency%2520while%2520maintaining%2520competitive%2520accuracy%2520on%2520math%2520and%2520coding%2520tasks.%2520The%2520full%2520training%2520and%2520evaluation%2520code%2520is%2520available%2520at%2520https%253A//github.com/SqueezeAILab/CDLM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CDLM%3A%20Consistency%20Diffusion%20Language%20Models%20For%20Faster%20Sampling&entry.906535625=Minseo%20Kim%20and%20Chenfeng%20Xu%20and%20Coleman%20Hooper%20and%20Harman%20Singh%20and%20Ben%20Athiwaratkun%20and%20Ce%20Zhang%20and%20Kurt%20Keutzer%20and%20Amir%20Gholami&entry.1292438233=Diffusion%20Language%20Models%20%28DLMs%29%20offer%20a%20promising%20parallel%20generation%20paradigm%20but%20suffer%20from%20slow%20inference%20due%20to%20numerous%20refinement%20steps%20and%20the%20inability%20to%20use%20standard%20KV%20caching.%20We%20introduce%20CDLM%20%28Consistency%20Diffusion%20Language%20Models%29%2C%20a%20training-based%20acceleration%20method%20that%20simultaneously%20tackles%20both%20bottlenecks.%20CDLM%20integrates%20consistency%20modeling%20to%20drastically%20reduce%20the%20number%20of%20required%20sampling%20steps%20by%20enabling%20multi-token%20finalization.%20Furthermore%2C%20we%20enforce%20a%20block-wise%20causal%20attention%20mask%20during%20fine-tuning%2C%20making%20the%20model%20fully%20compatible%20with%20KV%20caching.%20Experiments%20show%20CDLM%20achieves%203.6x-14.5x%20lower%20latency%20while%20maintaining%20competitive%20accuracy%20on%20math%20and%20coding%20tasks.%20The%20full%20training%20and%20evaluation%20code%20is%20available%20at%20https%3A//github.com/SqueezeAILab/CDLM.&entry.1838667208=http%3A//arxiv.org/abs/2511.19269v1&entry.124074799=Read"},
{"title": "Extremum Seeking Controlled Wiggling for Tactile Insertion", "author": "Levi Burner and Pavan Mantripragada and Gabriele M. Caddeo and Lorenzo Natale and Cornelia Ferm\u00fcller and Yiannis Aloimonos", "abstract": "When humans perform complex insertion tasks such as pushing a cup into a cupboard, routing a cable, or putting a key in a lock, they wiggle the object and adapt the process through tactile feedback. A similar robotic approach has not been developed. We study an extremum seeking control law that wiggles end effector pose to maximize insertion depth while minimizing strain measured by a GelSight Mini sensor. Evaluation is conducted on four keys featuring complex geometry and five assembly tasks featuring basic geometry.\n  On keys, the algorithm achieves 71% success rate over 120 trials with 6-DOF perturbations, 84% over 240 trials with 1-DOF perturbations, and 75% over 40 trials initialized with vision. It significantly outperforms a baseline optimizer, CMA-ES, that replaces wiggling with random sampling. When tested on a state-of-the-art assembly benchmark featuring basic geometry, it achieves 98% over 50 vision-initialized trials. The benchmark's most similar baseline, which was trained on the objects, achieved 86%. These results, realized without contact modeling or learning, show that closed loop wiggling based on tactile feedback is a robust paradigm for robotic insertion.", "link": "http://arxiv.org/abs/2410.02595v2", "date": "2025-11-24", "relevancy": 2.1961, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5579}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5546}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extremum%20Seeking%20Controlled%20Wiggling%20for%20Tactile%20Insertion&body=Title%3A%20Extremum%20Seeking%20Controlled%20Wiggling%20for%20Tactile%20Insertion%0AAuthor%3A%20Levi%20Burner%20and%20Pavan%20Mantripragada%20and%20Gabriele%20M.%20Caddeo%20and%20Lorenzo%20Natale%20and%20Cornelia%20Ferm%C3%BCller%20and%20Yiannis%20Aloimonos%0AAbstract%3A%20When%20humans%20perform%20complex%20insertion%20tasks%20such%20as%20pushing%20a%20cup%20into%20a%20cupboard%2C%20routing%20a%20cable%2C%20or%20putting%20a%20key%20in%20a%20lock%2C%20they%20wiggle%20the%20object%20and%20adapt%20the%20process%20through%20tactile%20feedback.%20A%20similar%20robotic%20approach%20has%20not%20been%20developed.%20We%20study%20an%20extremum%20seeking%20control%20law%20that%20wiggles%20end%20effector%20pose%20to%20maximize%20insertion%20depth%20while%20minimizing%20strain%20measured%20by%20a%20GelSight%20Mini%20sensor.%20Evaluation%20is%20conducted%20on%20four%20keys%20featuring%20complex%20geometry%20and%20five%20assembly%20tasks%20featuring%20basic%20geometry.%0A%20%20On%20keys%2C%20the%20algorithm%20achieves%2071%25%20success%20rate%20over%20120%20trials%20with%206-DOF%20perturbations%2C%2084%25%20over%20240%20trials%20with%201-DOF%20perturbations%2C%20and%2075%25%20over%2040%20trials%20initialized%20with%20vision.%20It%20significantly%20outperforms%20a%20baseline%20optimizer%2C%20CMA-ES%2C%20that%20replaces%20wiggling%20with%20random%20sampling.%20When%20tested%20on%20a%20state-of-the-art%20assembly%20benchmark%20featuring%20basic%20geometry%2C%20it%20achieves%2098%25%20over%2050%20vision-initialized%20trials.%20The%20benchmark%27s%20most%20similar%20baseline%2C%20which%20was%20trained%20on%20the%20objects%2C%20achieved%2086%25.%20These%20results%2C%20realized%20without%20contact%20modeling%20or%20learning%2C%20show%20that%20closed%20loop%20wiggling%20based%20on%20tactile%20feedback%20is%20a%20robust%20paradigm%20for%20robotic%20insertion.%0ALink%3A%20http%3A//arxiv.org/abs/2410.02595v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtremum%2520Seeking%2520Controlled%2520Wiggling%2520for%2520Tactile%2520Insertion%26entry.906535625%3DLevi%2520Burner%2520and%2520Pavan%2520Mantripragada%2520and%2520Gabriele%2520M.%2520Caddeo%2520and%2520Lorenzo%2520Natale%2520and%2520Cornelia%2520Ferm%25C3%25BCller%2520and%2520Yiannis%2520Aloimonos%26entry.1292438233%3DWhen%2520humans%2520perform%2520complex%2520insertion%2520tasks%2520such%2520as%2520pushing%2520a%2520cup%2520into%2520a%2520cupboard%252C%2520routing%2520a%2520cable%252C%2520or%2520putting%2520a%2520key%2520in%2520a%2520lock%252C%2520they%2520wiggle%2520the%2520object%2520and%2520adapt%2520the%2520process%2520through%2520tactile%2520feedback.%2520A%2520similar%2520robotic%2520approach%2520has%2520not%2520been%2520developed.%2520We%2520study%2520an%2520extremum%2520seeking%2520control%2520law%2520that%2520wiggles%2520end%2520effector%2520pose%2520to%2520maximize%2520insertion%2520depth%2520while%2520minimizing%2520strain%2520measured%2520by%2520a%2520GelSight%2520Mini%2520sensor.%2520Evaluation%2520is%2520conducted%2520on%2520four%2520keys%2520featuring%2520complex%2520geometry%2520and%2520five%2520assembly%2520tasks%2520featuring%2520basic%2520geometry.%250A%2520%2520On%2520keys%252C%2520the%2520algorithm%2520achieves%252071%2525%2520success%2520rate%2520over%2520120%2520trials%2520with%25206-DOF%2520perturbations%252C%252084%2525%2520over%2520240%2520trials%2520with%25201-DOF%2520perturbations%252C%2520and%252075%2525%2520over%252040%2520trials%2520initialized%2520with%2520vision.%2520It%2520significantly%2520outperforms%2520a%2520baseline%2520optimizer%252C%2520CMA-ES%252C%2520that%2520replaces%2520wiggling%2520with%2520random%2520sampling.%2520When%2520tested%2520on%2520a%2520state-of-the-art%2520assembly%2520benchmark%2520featuring%2520basic%2520geometry%252C%2520it%2520achieves%252098%2525%2520over%252050%2520vision-initialized%2520trials.%2520The%2520benchmark%2527s%2520most%2520similar%2520baseline%252C%2520which%2520was%2520trained%2520on%2520the%2520objects%252C%2520achieved%252086%2525.%2520These%2520results%252C%2520realized%2520without%2520contact%2520modeling%2520or%2520learning%252C%2520show%2520that%2520closed%2520loop%2520wiggling%2520based%2520on%2520tactile%2520feedback%2520is%2520a%2520robust%2520paradigm%2520for%2520robotic%2520insertion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02595v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extremum%20Seeking%20Controlled%20Wiggling%20for%20Tactile%20Insertion&entry.906535625=Levi%20Burner%20and%20Pavan%20Mantripragada%20and%20Gabriele%20M.%20Caddeo%20and%20Lorenzo%20Natale%20and%20Cornelia%20Ferm%C3%BCller%20and%20Yiannis%20Aloimonos&entry.1292438233=When%20humans%20perform%20complex%20insertion%20tasks%20such%20as%20pushing%20a%20cup%20into%20a%20cupboard%2C%20routing%20a%20cable%2C%20or%20putting%20a%20key%20in%20a%20lock%2C%20they%20wiggle%20the%20object%20and%20adapt%20the%20process%20through%20tactile%20feedback.%20A%20similar%20robotic%20approach%20has%20not%20been%20developed.%20We%20study%20an%20extremum%20seeking%20control%20law%20that%20wiggles%20end%20effector%20pose%20to%20maximize%20insertion%20depth%20while%20minimizing%20strain%20measured%20by%20a%20GelSight%20Mini%20sensor.%20Evaluation%20is%20conducted%20on%20four%20keys%20featuring%20complex%20geometry%20and%20five%20assembly%20tasks%20featuring%20basic%20geometry.%0A%20%20On%20keys%2C%20the%20algorithm%20achieves%2071%25%20success%20rate%20over%20120%20trials%20with%206-DOF%20perturbations%2C%2084%25%20over%20240%20trials%20with%201-DOF%20perturbations%2C%20and%2075%25%20over%2040%20trials%20initialized%20with%20vision.%20It%20significantly%20outperforms%20a%20baseline%20optimizer%2C%20CMA-ES%2C%20that%20replaces%20wiggling%20with%20random%20sampling.%20When%20tested%20on%20a%20state-of-the-art%20assembly%20benchmark%20featuring%20basic%20geometry%2C%20it%20achieves%2098%25%20over%2050%20vision-initialized%20trials.%20The%20benchmark%27s%20most%20similar%20baseline%2C%20which%20was%20trained%20on%20the%20objects%2C%20achieved%2086%25.%20These%20results%2C%20realized%20without%20contact%20modeling%20or%20learning%2C%20show%20that%20closed%20loop%20wiggling%20based%20on%20tactile%20feedback%20is%20a%20robust%20paradigm%20for%20robotic%20insertion.&entry.1838667208=http%3A//arxiv.org/abs/2410.02595v2&entry.124074799=Read"},
{"title": "When Semantics Regulate: Rethinking Patch Shuffle and Internal Bias for Generated Image Detection with CLIP", "author": "Beilin Chu and Weike You and Mengtao Li and Tingting Zheng and Kehan Zhao and Xuan Xu and Zhigao Lu and Jia Song and Moxuan Xu and Linna Zhou", "abstract": "The rapid progress of GANs and Diffusion Models poses new challenges for detecting AI-generated images. Although CLIP-based detectors exhibit promising generalization, they often rely on semantic cues rather than generator artifacts, leading to brittle performance under distribution shifts. In this work, we revisit the nature of semantic bias and uncover that Patch Shuffle provides an unusually strong benefit for CLIP, that disrupts global semantic continuity while preserving local artifact cues, which reduces semantic entropy and homogenizes feature distributions between natural and synthetic images. Through a detailed layer-wise analysis, we further show that CLIP's deep semantic structure functions as a regulator that stabilizes cross-domain representations once semantic bias is suppressed. Guided by these findings, we propose SemAnti, a semantic-antagonistic fine-tuning paradigm that freezes the semantic subspace and adapts only artifact-sensitive layers under shuffled semantics. Despite its simplicity, SemAnti achieves state-of-the-art cross-domain generalization on AIGCDetectBenchmark and GenImage, demonstrating that regulating semantics is key to unlocking CLIP's full potential for robust AI-generated image detection.", "link": "http://arxiv.org/abs/2511.19126v1", "date": "2025-11-24", "relevancy": 2.193, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5494}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5491}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Semantics%20Regulate%3A%20Rethinking%20Patch%20Shuffle%20and%20Internal%20Bias%20for%20Generated%20Image%20Detection%20with%20CLIP&body=Title%3A%20When%20Semantics%20Regulate%3A%20Rethinking%20Patch%20Shuffle%20and%20Internal%20Bias%20for%20Generated%20Image%20Detection%20with%20CLIP%0AAuthor%3A%20Beilin%20Chu%20and%20Weike%20You%20and%20Mengtao%20Li%20and%20Tingting%20Zheng%20and%20Kehan%20Zhao%20and%20Xuan%20Xu%20and%20Zhigao%20Lu%20and%20Jia%20Song%20and%20Moxuan%20Xu%20and%20Linna%20Zhou%0AAbstract%3A%20The%20rapid%20progress%20of%20GANs%20and%20Diffusion%20Models%20poses%20new%20challenges%20for%20detecting%20AI-generated%20images.%20Although%20CLIP-based%20detectors%20exhibit%20promising%20generalization%2C%20they%20often%20rely%20on%20semantic%20cues%20rather%20than%20generator%20artifacts%2C%20leading%20to%20brittle%20performance%20under%20distribution%20shifts.%20In%20this%20work%2C%20we%20revisit%20the%20nature%20of%20semantic%20bias%20and%20uncover%20that%20Patch%20Shuffle%20provides%20an%20unusually%20strong%20benefit%20for%20CLIP%2C%20that%20disrupts%20global%20semantic%20continuity%20while%20preserving%20local%20artifact%20cues%2C%20which%20reduces%20semantic%20entropy%20and%20homogenizes%20feature%20distributions%20between%20natural%20and%20synthetic%20images.%20Through%20a%20detailed%20layer-wise%20analysis%2C%20we%20further%20show%20that%20CLIP%27s%20deep%20semantic%20structure%20functions%20as%20a%20regulator%20that%20stabilizes%20cross-domain%20representations%20once%20semantic%20bias%20is%20suppressed.%20Guided%20by%20these%20findings%2C%20we%20propose%20SemAnti%2C%20a%20semantic-antagonistic%20fine-tuning%20paradigm%20that%20freezes%20the%20semantic%20subspace%20and%20adapts%20only%20artifact-sensitive%20layers%20under%20shuffled%20semantics.%20Despite%20its%20simplicity%2C%20SemAnti%20achieves%20state-of-the-art%20cross-domain%20generalization%20on%20AIGCDetectBenchmark%20and%20GenImage%2C%20demonstrating%20that%20regulating%20semantics%20is%20key%20to%20unlocking%20CLIP%27s%20full%20potential%20for%20robust%20AI-generated%20image%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Semantics%2520Regulate%253A%2520Rethinking%2520Patch%2520Shuffle%2520and%2520Internal%2520Bias%2520for%2520Generated%2520Image%2520Detection%2520with%2520CLIP%26entry.906535625%3DBeilin%2520Chu%2520and%2520Weike%2520You%2520and%2520Mengtao%2520Li%2520and%2520Tingting%2520Zheng%2520and%2520Kehan%2520Zhao%2520and%2520Xuan%2520Xu%2520and%2520Zhigao%2520Lu%2520and%2520Jia%2520Song%2520and%2520Moxuan%2520Xu%2520and%2520Linna%2520Zhou%26entry.1292438233%3DThe%2520rapid%2520progress%2520of%2520GANs%2520and%2520Diffusion%2520Models%2520poses%2520new%2520challenges%2520for%2520detecting%2520AI-generated%2520images.%2520Although%2520CLIP-based%2520detectors%2520exhibit%2520promising%2520generalization%252C%2520they%2520often%2520rely%2520on%2520semantic%2520cues%2520rather%2520than%2520generator%2520artifacts%252C%2520leading%2520to%2520brittle%2520performance%2520under%2520distribution%2520shifts.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%2520nature%2520of%2520semantic%2520bias%2520and%2520uncover%2520that%2520Patch%2520Shuffle%2520provides%2520an%2520unusually%2520strong%2520benefit%2520for%2520CLIP%252C%2520that%2520disrupts%2520global%2520semantic%2520continuity%2520while%2520preserving%2520local%2520artifact%2520cues%252C%2520which%2520reduces%2520semantic%2520entropy%2520and%2520homogenizes%2520feature%2520distributions%2520between%2520natural%2520and%2520synthetic%2520images.%2520Through%2520a%2520detailed%2520layer-wise%2520analysis%252C%2520we%2520further%2520show%2520that%2520CLIP%2527s%2520deep%2520semantic%2520structure%2520functions%2520as%2520a%2520regulator%2520that%2520stabilizes%2520cross-domain%2520representations%2520once%2520semantic%2520bias%2520is%2520suppressed.%2520Guided%2520by%2520these%2520findings%252C%2520we%2520propose%2520SemAnti%252C%2520a%2520semantic-antagonistic%2520fine-tuning%2520paradigm%2520that%2520freezes%2520the%2520semantic%2520subspace%2520and%2520adapts%2520only%2520artifact-sensitive%2520layers%2520under%2520shuffled%2520semantics.%2520Despite%2520its%2520simplicity%252C%2520SemAnti%2520achieves%2520state-of-the-art%2520cross-domain%2520generalization%2520on%2520AIGCDetectBenchmark%2520and%2520GenImage%252C%2520demonstrating%2520that%2520regulating%2520semantics%2520is%2520key%2520to%2520unlocking%2520CLIP%2527s%2520full%2520potential%2520for%2520robust%2520AI-generated%2520image%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Semantics%20Regulate%3A%20Rethinking%20Patch%20Shuffle%20and%20Internal%20Bias%20for%20Generated%20Image%20Detection%20with%20CLIP&entry.906535625=Beilin%20Chu%20and%20Weike%20You%20and%20Mengtao%20Li%20and%20Tingting%20Zheng%20and%20Kehan%20Zhao%20and%20Xuan%20Xu%20and%20Zhigao%20Lu%20and%20Jia%20Song%20and%20Moxuan%20Xu%20and%20Linna%20Zhou&entry.1292438233=The%20rapid%20progress%20of%20GANs%20and%20Diffusion%20Models%20poses%20new%20challenges%20for%20detecting%20AI-generated%20images.%20Although%20CLIP-based%20detectors%20exhibit%20promising%20generalization%2C%20they%20often%20rely%20on%20semantic%20cues%20rather%20than%20generator%20artifacts%2C%20leading%20to%20brittle%20performance%20under%20distribution%20shifts.%20In%20this%20work%2C%20we%20revisit%20the%20nature%20of%20semantic%20bias%20and%20uncover%20that%20Patch%20Shuffle%20provides%20an%20unusually%20strong%20benefit%20for%20CLIP%2C%20that%20disrupts%20global%20semantic%20continuity%20while%20preserving%20local%20artifact%20cues%2C%20which%20reduces%20semantic%20entropy%20and%20homogenizes%20feature%20distributions%20between%20natural%20and%20synthetic%20images.%20Through%20a%20detailed%20layer-wise%20analysis%2C%20we%20further%20show%20that%20CLIP%27s%20deep%20semantic%20structure%20functions%20as%20a%20regulator%20that%20stabilizes%20cross-domain%20representations%20once%20semantic%20bias%20is%20suppressed.%20Guided%20by%20these%20findings%2C%20we%20propose%20SemAnti%2C%20a%20semantic-antagonistic%20fine-tuning%20paradigm%20that%20freezes%20the%20semantic%20subspace%20and%20adapts%20only%20artifact-sensitive%20layers%20under%20shuffled%20semantics.%20Despite%20its%20simplicity%2C%20SemAnti%20achieves%20state-of-the-art%20cross-domain%20generalization%20on%20AIGCDetectBenchmark%20and%20GenImage%2C%20demonstrating%20that%20regulating%20semantics%20is%20key%20to%20unlocking%20CLIP%27s%20full%20potential%20for%20robust%20AI-generated%20image%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2511.19126v1&entry.124074799=Read"},
{"title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning", "author": "Jiayi Zhang and Yiran Peng and Fanqi Kong and Yang Cheng and Yifan Wu and Zhaoyang Yu and Jinyu Xiang and Jianhao Ruan and Jinlin Wang and Maojia Song and HongZhang Liu and Xiangru Tang and Bang Liu and Chenglin Wu and Yuyu Luo", "abstract": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.", "link": "http://arxiv.org/abs/2511.19304v1", "date": "2025-11-24", "relevancy": 2.1926, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5892}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5527}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoEnv%3A%20Automated%20Environments%20for%20Measuring%20Cross-Environment%20Agent%20Learning&body=Title%3A%20AutoEnv%3A%20Automated%20Environments%20for%20Measuring%20Cross-Environment%20Agent%20Learning%0AAuthor%3A%20Jiayi%20Zhang%20and%20Yiran%20Peng%20and%20Fanqi%20Kong%20and%20Yang%20Cheng%20and%20Yifan%20Wu%20and%20Zhaoyang%20Yu%20and%20Jinyu%20Xiang%20and%20Jianhao%20Ruan%20and%20Jinlin%20Wang%20and%20Maojia%20Song%20and%20HongZhang%20Liu%20and%20Xiangru%20Tang%20and%20Bang%20Liu%20and%20Chenglin%20Wu%20and%20Yuyu%20Luo%0AAbstract%3A%20Humans%20naturally%20adapt%20to%20diverse%20environments%20by%20learning%20underlying%20rules%20across%20worlds%20with%20different%20dynamics%2C%20observations%2C%20and%20reward%20structures.%20In%20contrast%2C%20existing%20agents%20typically%20demonstrate%20improvements%20via%20self-evolving%20within%20a%20single%20domain%2C%20implicitly%20assuming%20a%20fixed%20environment%20distribution.%20Cross-environment%20learning%20has%20remained%20largely%20unmeasured%3A%20there%20is%20no%20standard%20collection%20of%20controllable%2C%20heterogeneous%20environments%2C%20nor%20a%20unified%20way%20to%20represent%20how%20agents%20learn.%20We%20address%20these%20gaps%20in%20two%20steps.%20First%2C%20we%20propose%20AutoEnv%2C%20an%20automated%20framework%20that%20treats%20environments%20as%20factorizable%20distributions%20over%20transitions%2C%20observations%2C%20and%20rewards%2C%20enabling%20low-cost%20%284.12%20USD%20on%20average%29%20generation%20of%20heterogeneous%20worlds.%20Using%20AutoEnv%2C%20we%20construct%20AutoEnv-36%2C%20a%20dataset%20of%2036%20environments%20with%20358%20validated%20levels%2C%20on%20which%20seven%20language%20models%20achieve%2012-49%25%20normalized%20reward%2C%20demonstrating%20the%20challenge%20of%20AutoEnv-36.%20Second%2C%20we%20formalize%20agent%20learning%20as%20a%20component-centric%20process%20driven%20by%20three%20stages%20of%20Selection%2C%20Optimization%2C%20and%20Evaluation%20applied%20to%20an%20improvable%20agent%20component.%20Using%20this%20formulation%2C%20we%20design%20eight%20learning%20methods%20and%20evaluate%20them%20on%20AutoEnv-36.%20Empirically%2C%20the%20gain%20of%20any%20single%20learning%20method%20quickly%20decrease%20as%20the%20number%20of%20environments%20increases%2C%20revealing%20that%20fixed%20learning%20methods%20do%20not%20scale%20across%20heterogeneous%20environments.%20Environment-adaptive%20selection%20of%20learning%20methods%20substantially%20improves%20performance%20but%20exhibits%20diminishing%20returns%20as%20the%20method%20space%20expands.%20These%20results%20highlight%20both%20the%20necessity%20and%20the%20current%20limitations%20of%20agent%20learning%20for%20scalable%20cross-environment%20generalization%2C%20and%20position%20AutoEnv%20and%20AutoEnv-36%20as%20a%20testbed%20for%20studying%20cross-environment%20agent%20learning.%20The%20code%20is%20avaiable%20at%20https%3A//github.com/FoundationAgents/AutoEnv.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19304v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoEnv%253A%2520Automated%2520Environments%2520for%2520Measuring%2520Cross-Environment%2520Agent%2520Learning%26entry.906535625%3DJiayi%2520Zhang%2520and%2520Yiran%2520Peng%2520and%2520Fanqi%2520Kong%2520and%2520Yang%2520Cheng%2520and%2520Yifan%2520Wu%2520and%2520Zhaoyang%2520Yu%2520and%2520Jinyu%2520Xiang%2520and%2520Jianhao%2520Ruan%2520and%2520Jinlin%2520Wang%2520and%2520Maojia%2520Song%2520and%2520HongZhang%2520Liu%2520and%2520Xiangru%2520Tang%2520and%2520Bang%2520Liu%2520and%2520Chenglin%2520Wu%2520and%2520Yuyu%2520Luo%26entry.1292438233%3DHumans%2520naturally%2520adapt%2520to%2520diverse%2520environments%2520by%2520learning%2520underlying%2520rules%2520across%2520worlds%2520with%2520different%2520dynamics%252C%2520observations%252C%2520and%2520reward%2520structures.%2520In%2520contrast%252C%2520existing%2520agents%2520typically%2520demonstrate%2520improvements%2520via%2520self-evolving%2520within%2520a%2520single%2520domain%252C%2520implicitly%2520assuming%2520a%2520fixed%2520environment%2520distribution.%2520Cross-environment%2520learning%2520has%2520remained%2520largely%2520unmeasured%253A%2520there%2520is%2520no%2520standard%2520collection%2520of%2520controllable%252C%2520heterogeneous%2520environments%252C%2520nor%2520a%2520unified%2520way%2520to%2520represent%2520how%2520agents%2520learn.%2520We%2520address%2520these%2520gaps%2520in%2520two%2520steps.%2520First%252C%2520we%2520propose%2520AutoEnv%252C%2520an%2520automated%2520framework%2520that%2520treats%2520environments%2520as%2520factorizable%2520distributions%2520over%2520transitions%252C%2520observations%252C%2520and%2520rewards%252C%2520enabling%2520low-cost%2520%25284.12%2520USD%2520on%2520average%2529%2520generation%2520of%2520heterogeneous%2520worlds.%2520Using%2520AutoEnv%252C%2520we%2520construct%2520AutoEnv-36%252C%2520a%2520dataset%2520of%252036%2520environments%2520with%2520358%2520validated%2520levels%252C%2520on%2520which%2520seven%2520language%2520models%2520achieve%252012-49%2525%2520normalized%2520reward%252C%2520demonstrating%2520the%2520challenge%2520of%2520AutoEnv-36.%2520Second%252C%2520we%2520formalize%2520agent%2520learning%2520as%2520a%2520component-centric%2520process%2520driven%2520by%2520three%2520stages%2520of%2520Selection%252C%2520Optimization%252C%2520and%2520Evaluation%2520applied%2520to%2520an%2520improvable%2520agent%2520component.%2520Using%2520this%2520formulation%252C%2520we%2520design%2520eight%2520learning%2520methods%2520and%2520evaluate%2520them%2520on%2520AutoEnv-36.%2520Empirically%252C%2520the%2520gain%2520of%2520any%2520single%2520learning%2520method%2520quickly%2520decrease%2520as%2520the%2520number%2520of%2520environments%2520increases%252C%2520revealing%2520that%2520fixed%2520learning%2520methods%2520do%2520not%2520scale%2520across%2520heterogeneous%2520environments.%2520Environment-adaptive%2520selection%2520of%2520learning%2520methods%2520substantially%2520improves%2520performance%2520but%2520exhibits%2520diminishing%2520returns%2520as%2520the%2520method%2520space%2520expands.%2520These%2520results%2520highlight%2520both%2520the%2520necessity%2520and%2520the%2520current%2520limitations%2520of%2520agent%2520learning%2520for%2520scalable%2520cross-environment%2520generalization%252C%2520and%2520position%2520AutoEnv%2520and%2520AutoEnv-36%2520as%2520a%2520testbed%2520for%2520studying%2520cross-environment%2520agent%2520learning.%2520The%2520code%2520is%2520avaiable%2520at%2520https%253A//github.com/FoundationAgents/AutoEnv.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19304v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoEnv%3A%20Automated%20Environments%20for%20Measuring%20Cross-Environment%20Agent%20Learning&entry.906535625=Jiayi%20Zhang%20and%20Yiran%20Peng%20and%20Fanqi%20Kong%20and%20Yang%20Cheng%20and%20Yifan%20Wu%20and%20Zhaoyang%20Yu%20and%20Jinyu%20Xiang%20and%20Jianhao%20Ruan%20and%20Jinlin%20Wang%20and%20Maojia%20Song%20and%20HongZhang%20Liu%20and%20Xiangru%20Tang%20and%20Bang%20Liu%20and%20Chenglin%20Wu%20and%20Yuyu%20Luo&entry.1292438233=Humans%20naturally%20adapt%20to%20diverse%20environments%20by%20learning%20underlying%20rules%20across%20worlds%20with%20different%20dynamics%2C%20observations%2C%20and%20reward%20structures.%20In%20contrast%2C%20existing%20agents%20typically%20demonstrate%20improvements%20via%20self-evolving%20within%20a%20single%20domain%2C%20implicitly%20assuming%20a%20fixed%20environment%20distribution.%20Cross-environment%20learning%20has%20remained%20largely%20unmeasured%3A%20there%20is%20no%20standard%20collection%20of%20controllable%2C%20heterogeneous%20environments%2C%20nor%20a%20unified%20way%20to%20represent%20how%20agents%20learn.%20We%20address%20these%20gaps%20in%20two%20steps.%20First%2C%20we%20propose%20AutoEnv%2C%20an%20automated%20framework%20that%20treats%20environments%20as%20factorizable%20distributions%20over%20transitions%2C%20observations%2C%20and%20rewards%2C%20enabling%20low-cost%20%284.12%20USD%20on%20average%29%20generation%20of%20heterogeneous%20worlds.%20Using%20AutoEnv%2C%20we%20construct%20AutoEnv-36%2C%20a%20dataset%20of%2036%20environments%20with%20358%20validated%20levels%2C%20on%20which%20seven%20language%20models%20achieve%2012-49%25%20normalized%20reward%2C%20demonstrating%20the%20challenge%20of%20AutoEnv-36.%20Second%2C%20we%20formalize%20agent%20learning%20as%20a%20component-centric%20process%20driven%20by%20three%20stages%20of%20Selection%2C%20Optimization%2C%20and%20Evaluation%20applied%20to%20an%20improvable%20agent%20component.%20Using%20this%20formulation%2C%20we%20design%20eight%20learning%20methods%20and%20evaluate%20them%20on%20AutoEnv-36.%20Empirically%2C%20the%20gain%20of%20any%20single%20learning%20method%20quickly%20decrease%20as%20the%20number%20of%20environments%20increases%2C%20revealing%20that%20fixed%20learning%20methods%20do%20not%20scale%20across%20heterogeneous%20environments.%20Environment-adaptive%20selection%20of%20learning%20methods%20substantially%20improves%20performance%20but%20exhibits%20diminishing%20returns%20as%20the%20method%20space%20expands.%20These%20results%20highlight%20both%20the%20necessity%20and%20the%20current%20limitations%20of%20agent%20learning%20for%20scalable%20cross-environment%20generalization%2C%20and%20position%20AutoEnv%20and%20AutoEnv-36%20as%20a%20testbed%20for%20studying%20cross-environment%20agent%20learning.%20The%20code%20is%20avaiable%20at%20https%3A//github.com/FoundationAgents/AutoEnv.&entry.1838667208=http%3A//arxiv.org/abs/2511.19304v1&entry.124074799=Read"},
{"title": "UniGame: Turning a Unified Multimodal Model Into Its Own Adversary", "author": "Zhaolong Su and Wang Lu and Hao Chen and Sharon Li and Jindong Wang", "abstract": "Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame", "link": "http://arxiv.org/abs/2511.19413v1", "date": "2025-11-24", "relevancy": 2.1919, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.576}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5298}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniGame%3A%20Turning%20a%20Unified%20Multimodal%20Model%20Into%20Its%20Own%20Adversary&body=Title%3A%20UniGame%3A%20Turning%20a%20Unified%20Multimodal%20Model%20Into%20Its%20Own%20Adversary%0AAuthor%3A%20Zhaolong%20Su%20and%20Wang%20Lu%20and%20Hao%20Chen%20and%20Sharon%20Li%20and%20Jindong%20Wang%0AAbstract%3A%20Unified%20Multimodal%20Models%20%28UMMs%29%20have%20shown%20impressive%20performance%20in%20both%20understanding%20and%20generation%20with%20a%20single%20architecture.%20However%2C%20UMMs%20still%20exhibit%20a%20fundamental%20inconsistency%3A%20understanding%20favors%20compact%20embeddings%2C%20whereas%20generation%20favors%20reconstruction-rich%20representations.%20This%20structural%20trade-off%20produces%20misaligned%20decision%20boundaries%2C%20degraded%20cross-modal%20coherence%2C%20and%20heightened%20vulnerability%20under%20distributional%20and%20adversarial%20shifts.%20In%20this%20paper%2C%20we%20present%20UniGame%2C%20a%20self-adversarial%20post-training%20framework%20that%20directly%20targets%20the%20inconsistencies.%20By%20applying%20a%20lightweight%20perturber%20at%20the%20shared%20token%20interface%2C%20UniGame%20enables%20the%20generation%20branch%20to%20actively%20seek%20and%20challenge%20fragile%20understanding%2C%20turning%20the%20model%20itself%20into%20its%20own%20adversary.%20Experiments%20demonstrate%20that%20UniGame%20significantly%20improves%20the%20consistency%20%28%2B4.6%25%29.%20Moreover%2C%20it%20also%20achieves%20substantial%20improvements%20in%20understanding%20%28%2B3.6%25%29%2C%20generation%20%28%2B0.02%29%2C%20out-of-distribution%20and%20adversarial%20robustness%20%28%2B4.8%25%20and%20%2B6.2%25%20on%20NaturalBench%20and%20AdVQA%29.%20The%20framework%20is%20architecture-agnostic%2C%20introduces%20less%20than%201%25%20additional%20parameters%2C%20and%20is%20complementary%20to%20existing%20post-training%20methods.%20These%20results%20position%20adversarial%20self-play%20as%20a%20general%20and%20effective%20principle%20for%20enhancing%20the%20coherence%2C%20stability%2C%20and%20unified%20competence%20of%20future%20multimodal%20foundation%20models.%20The%20official%20code%20is%20available%20at%3A%20https%3A//github.com/AIFrontierLab/UniGame%0ALink%3A%20http%3A//arxiv.org/abs/2511.19413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniGame%253A%2520Turning%2520a%2520Unified%2520Multimodal%2520Model%2520Into%2520Its%2520Own%2520Adversary%26entry.906535625%3DZhaolong%2520Su%2520and%2520Wang%2520Lu%2520and%2520Hao%2520Chen%2520and%2520Sharon%2520Li%2520and%2520Jindong%2520Wang%26entry.1292438233%3DUnified%2520Multimodal%2520Models%2520%2528UMMs%2529%2520have%2520shown%2520impressive%2520performance%2520in%2520both%2520understanding%2520and%2520generation%2520with%2520a%2520single%2520architecture.%2520However%252C%2520UMMs%2520still%2520exhibit%2520a%2520fundamental%2520inconsistency%253A%2520understanding%2520favors%2520compact%2520embeddings%252C%2520whereas%2520generation%2520favors%2520reconstruction-rich%2520representations.%2520This%2520structural%2520trade-off%2520produces%2520misaligned%2520decision%2520boundaries%252C%2520degraded%2520cross-modal%2520coherence%252C%2520and%2520heightened%2520vulnerability%2520under%2520distributional%2520and%2520adversarial%2520shifts.%2520In%2520this%2520paper%252C%2520we%2520present%2520UniGame%252C%2520a%2520self-adversarial%2520post-training%2520framework%2520that%2520directly%2520targets%2520the%2520inconsistencies.%2520By%2520applying%2520a%2520lightweight%2520perturber%2520at%2520the%2520shared%2520token%2520interface%252C%2520UniGame%2520enables%2520the%2520generation%2520branch%2520to%2520actively%2520seek%2520and%2520challenge%2520fragile%2520understanding%252C%2520turning%2520the%2520model%2520itself%2520into%2520its%2520own%2520adversary.%2520Experiments%2520demonstrate%2520that%2520UniGame%2520significantly%2520improves%2520the%2520consistency%2520%2528%252B4.6%2525%2529.%2520Moreover%252C%2520it%2520also%2520achieves%2520substantial%2520improvements%2520in%2520understanding%2520%2528%252B3.6%2525%2529%252C%2520generation%2520%2528%252B0.02%2529%252C%2520out-of-distribution%2520and%2520adversarial%2520robustness%2520%2528%252B4.8%2525%2520and%2520%252B6.2%2525%2520on%2520NaturalBench%2520and%2520AdVQA%2529.%2520The%2520framework%2520is%2520architecture-agnostic%252C%2520introduces%2520less%2520than%25201%2525%2520additional%2520parameters%252C%2520and%2520is%2520complementary%2520to%2520existing%2520post-training%2520methods.%2520These%2520results%2520position%2520adversarial%2520self-play%2520as%2520a%2520general%2520and%2520effective%2520principle%2520for%2520enhancing%2520the%2520coherence%252C%2520stability%252C%2520and%2520unified%2520competence%2520of%2520future%2520multimodal%2520foundation%2520models.%2520The%2520official%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/AIFrontierLab/UniGame%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniGame%3A%20Turning%20a%20Unified%20Multimodal%20Model%20Into%20Its%20Own%20Adversary&entry.906535625=Zhaolong%20Su%20and%20Wang%20Lu%20and%20Hao%20Chen%20and%20Sharon%20Li%20and%20Jindong%20Wang&entry.1292438233=Unified%20Multimodal%20Models%20%28UMMs%29%20have%20shown%20impressive%20performance%20in%20both%20understanding%20and%20generation%20with%20a%20single%20architecture.%20However%2C%20UMMs%20still%20exhibit%20a%20fundamental%20inconsistency%3A%20understanding%20favors%20compact%20embeddings%2C%20whereas%20generation%20favors%20reconstruction-rich%20representations.%20This%20structural%20trade-off%20produces%20misaligned%20decision%20boundaries%2C%20degraded%20cross-modal%20coherence%2C%20and%20heightened%20vulnerability%20under%20distributional%20and%20adversarial%20shifts.%20In%20this%20paper%2C%20we%20present%20UniGame%2C%20a%20self-adversarial%20post-training%20framework%20that%20directly%20targets%20the%20inconsistencies.%20By%20applying%20a%20lightweight%20perturber%20at%20the%20shared%20token%20interface%2C%20UniGame%20enables%20the%20generation%20branch%20to%20actively%20seek%20and%20challenge%20fragile%20understanding%2C%20turning%20the%20model%20itself%20into%20its%20own%20adversary.%20Experiments%20demonstrate%20that%20UniGame%20significantly%20improves%20the%20consistency%20%28%2B4.6%25%29.%20Moreover%2C%20it%20also%20achieves%20substantial%20improvements%20in%20understanding%20%28%2B3.6%25%29%2C%20generation%20%28%2B0.02%29%2C%20out-of-distribution%20and%20adversarial%20robustness%20%28%2B4.8%25%20and%20%2B6.2%25%20on%20NaturalBench%20and%20AdVQA%29.%20The%20framework%20is%20architecture-agnostic%2C%20introduces%20less%20than%201%25%20additional%20parameters%2C%20and%20is%20complementary%20to%20existing%20post-training%20methods.%20These%20results%20position%20adversarial%20self-play%20as%20a%20general%20and%20effective%20principle%20for%20enhancing%20the%20coherence%2C%20stability%2C%20and%20unified%20competence%20of%20future%20multimodal%20foundation%20models.%20The%20official%20code%20is%20available%20at%3A%20https%3A//github.com/AIFrontierLab/UniGame&entry.1838667208=http%3A//arxiv.org/abs/2511.19413v1&entry.124074799=Read"},
{"title": "Learning to Call: A Field Trial of a Collaborative Bandit Algorithm for Improved Message Delivery in Mobile Maternal Health", "author": "Arpan Dasgupta and Mizhaan Maniyar and Awadhesh Srivastava and Sanat Kumar and Amrita Mahale and Aparna Hegde and Arun Suggala and Karthikeyan Shanmugam and Aparna Taneja and Milind Tambe", "abstract": "Mobile health (mHealth) programs utilize automated voice messages to deliver health information, particularly targeting underserved communities, demonstrating the effectiveness of using mobile technology to disseminate crucial health information to these populations, improving health outcomes through increased awareness and behavioral change. India's Kilkari program delivers vital maternal health information via weekly voice calls to millions of mothers. However, the current random call scheduling often results in missed calls and reduced message delivery. This study presents a field trial of a collaborative bandit algorithm designed to optimize call timing by learning individual mothers' preferred call times. We deployed the algorithm with around $6500$ Kilkari participants as a pilot study, comparing its performance to the baseline random calling approach. Our results demonstrate a statistically significant improvement in call pick-up rates with the bandit algorithm, indicating its potential to enhance message delivery and impact millions of mothers across India. This research highlights the efficacy of personalized scheduling in mobile health interventions and underscores the potential of machine learning to improve maternal health outreach at scale.", "link": "http://arxiv.org/abs/2507.16356v2", "date": "2025-11-24", "relevancy": 1.6139, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.427}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4076}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.3899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Call%3A%20A%20Field%20Trial%20of%20a%20Collaborative%20Bandit%20Algorithm%20for%20Improved%20Message%20Delivery%20in%20Mobile%20Maternal%20Health&body=Title%3A%20Learning%20to%20Call%3A%20A%20Field%20Trial%20of%20a%20Collaborative%20Bandit%20Algorithm%20for%20Improved%20Message%20Delivery%20in%20Mobile%20Maternal%20Health%0AAuthor%3A%20Arpan%20Dasgupta%20and%20Mizhaan%20Maniyar%20and%20Awadhesh%20Srivastava%20and%20Sanat%20Kumar%20and%20Amrita%20Mahale%20and%20Aparna%20Hegde%20and%20Arun%20Suggala%20and%20Karthikeyan%20Shanmugam%20and%20Aparna%20Taneja%20and%20Milind%20Tambe%0AAbstract%3A%20Mobile%20health%20%28mHealth%29%20programs%20utilize%20automated%20voice%20messages%20to%20deliver%20health%20information%2C%20particularly%20targeting%20underserved%20communities%2C%20demonstrating%20the%20effectiveness%20of%20using%20mobile%20technology%20to%20disseminate%20crucial%20health%20information%20to%20these%20populations%2C%20improving%20health%20outcomes%20through%20increased%20awareness%20and%20behavioral%20change.%20India%27s%20Kilkari%20program%20delivers%20vital%20maternal%20health%20information%20via%20weekly%20voice%20calls%20to%20millions%20of%20mothers.%20However%2C%20the%20current%20random%20call%20scheduling%20often%20results%20in%20missed%20calls%20and%20reduced%20message%20delivery.%20This%20study%20presents%20a%20field%20trial%20of%20a%20collaborative%20bandit%20algorithm%20designed%20to%20optimize%20call%20timing%20by%20learning%20individual%20mothers%27%20preferred%20call%20times.%20We%20deployed%20the%20algorithm%20with%20around%20%246500%24%20Kilkari%20participants%20as%20a%20pilot%20study%2C%20comparing%20its%20performance%20to%20the%20baseline%20random%20calling%20approach.%20Our%20results%20demonstrate%20a%20statistically%20significant%20improvement%20in%20call%20pick-up%20rates%20with%20the%20bandit%20algorithm%2C%20indicating%20its%20potential%20to%20enhance%20message%20delivery%20and%20impact%20millions%20of%20mothers%20across%20India.%20This%20research%20highlights%20the%20efficacy%20of%20personalized%20scheduling%20in%20mobile%20health%20interventions%20and%20underscores%20the%20potential%20of%20machine%20learning%20to%20improve%20maternal%20health%20outreach%20at%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2507.16356v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Call%253A%2520A%2520Field%2520Trial%2520of%2520a%2520Collaborative%2520Bandit%2520Algorithm%2520for%2520Improved%2520Message%2520Delivery%2520in%2520Mobile%2520Maternal%2520Health%26entry.906535625%3DArpan%2520Dasgupta%2520and%2520Mizhaan%2520Maniyar%2520and%2520Awadhesh%2520Srivastava%2520and%2520Sanat%2520Kumar%2520and%2520Amrita%2520Mahale%2520and%2520Aparna%2520Hegde%2520and%2520Arun%2520Suggala%2520and%2520Karthikeyan%2520Shanmugam%2520and%2520Aparna%2520Taneja%2520and%2520Milind%2520Tambe%26entry.1292438233%3DMobile%2520health%2520%2528mHealth%2529%2520programs%2520utilize%2520automated%2520voice%2520messages%2520to%2520deliver%2520health%2520information%252C%2520particularly%2520targeting%2520underserved%2520communities%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520using%2520mobile%2520technology%2520to%2520disseminate%2520crucial%2520health%2520information%2520to%2520these%2520populations%252C%2520improving%2520health%2520outcomes%2520through%2520increased%2520awareness%2520and%2520behavioral%2520change.%2520India%2527s%2520Kilkari%2520program%2520delivers%2520vital%2520maternal%2520health%2520information%2520via%2520weekly%2520voice%2520calls%2520to%2520millions%2520of%2520mothers.%2520However%252C%2520the%2520current%2520random%2520call%2520scheduling%2520often%2520results%2520in%2520missed%2520calls%2520and%2520reduced%2520message%2520delivery.%2520This%2520study%2520presents%2520a%2520field%2520trial%2520of%2520a%2520collaborative%2520bandit%2520algorithm%2520designed%2520to%2520optimize%2520call%2520timing%2520by%2520learning%2520individual%2520mothers%2527%2520preferred%2520call%2520times.%2520We%2520deployed%2520the%2520algorithm%2520with%2520around%2520%25246500%2524%2520Kilkari%2520participants%2520as%2520a%2520pilot%2520study%252C%2520comparing%2520its%2520performance%2520to%2520the%2520baseline%2520random%2520calling%2520approach.%2520Our%2520results%2520demonstrate%2520a%2520statistically%2520significant%2520improvement%2520in%2520call%2520pick-up%2520rates%2520with%2520the%2520bandit%2520algorithm%252C%2520indicating%2520its%2520potential%2520to%2520enhance%2520message%2520delivery%2520and%2520impact%2520millions%2520of%2520mothers%2520across%2520India.%2520This%2520research%2520highlights%2520the%2520efficacy%2520of%2520personalized%2520scheduling%2520in%2520mobile%2520health%2520interventions%2520and%2520underscores%2520the%2520potential%2520of%2520machine%2520learning%2520to%2520improve%2520maternal%2520health%2520outreach%2520at%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16356v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Call%3A%20A%20Field%20Trial%20of%20a%20Collaborative%20Bandit%20Algorithm%20for%20Improved%20Message%20Delivery%20in%20Mobile%20Maternal%20Health&entry.906535625=Arpan%20Dasgupta%20and%20Mizhaan%20Maniyar%20and%20Awadhesh%20Srivastava%20and%20Sanat%20Kumar%20and%20Amrita%20Mahale%20and%20Aparna%20Hegde%20and%20Arun%20Suggala%20and%20Karthikeyan%20Shanmugam%20and%20Aparna%20Taneja%20and%20Milind%20Tambe&entry.1292438233=Mobile%20health%20%28mHealth%29%20programs%20utilize%20automated%20voice%20messages%20to%20deliver%20health%20information%2C%20particularly%20targeting%20underserved%20communities%2C%20demonstrating%20the%20effectiveness%20of%20using%20mobile%20technology%20to%20disseminate%20crucial%20health%20information%20to%20these%20populations%2C%20improving%20health%20outcomes%20through%20increased%20awareness%20and%20behavioral%20change.%20India%27s%20Kilkari%20program%20delivers%20vital%20maternal%20health%20information%20via%20weekly%20voice%20calls%20to%20millions%20of%20mothers.%20However%2C%20the%20current%20random%20call%20scheduling%20often%20results%20in%20missed%20calls%20and%20reduced%20message%20delivery.%20This%20study%20presents%20a%20field%20trial%20of%20a%20collaborative%20bandit%20algorithm%20designed%20to%20optimize%20call%20timing%20by%20learning%20individual%20mothers%27%20preferred%20call%20times.%20We%20deployed%20the%20algorithm%20with%20around%20%246500%24%20Kilkari%20participants%20as%20a%20pilot%20study%2C%20comparing%20its%20performance%20to%20the%20baseline%20random%20calling%20approach.%20Our%20results%20demonstrate%20a%20statistically%20significant%20improvement%20in%20call%20pick-up%20rates%20with%20the%20bandit%20algorithm%2C%20indicating%20its%20potential%20to%20enhance%20message%20delivery%20and%20impact%20millions%20of%20mothers%20across%20India.%20This%20research%20highlights%20the%20efficacy%20of%20personalized%20scheduling%20in%20mobile%20health%20interventions%20and%20underscores%20the%20potential%20of%20machine%20learning%20to%20improve%20maternal%20health%20outreach%20at%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2507.16356v2&entry.124074799=Read"},
{"title": "Enhancing Conformal Prediction via Class Similarity", "author": "Ariel Fargion and Lahav Dabah and Tom Tirer", "abstract": "Conformal Prediction (CP) has emerged as a powerful statistical framework for high-stakes classification applications. Instead of predicting a single class, CP generates a prediction set, guaranteed to include the true label with a pre-specified probability. The performance of different CP methods is typically assessed by their average prediction set size. In setups where the classes can be partitioned into semantic groups, e.g., diseases that require similar treatment, users can benefit from prediction sets that are not only small on average, but also contain a small number of semantically different groups. This paper begins by addressing this problem and ultimately offers a widely applicable tool for boosting any CP method on any dataset. First, given a class partition, we propose augmenting the CP score function with a term that penalizes predictions with out-of-group errors. We theoretically analyze this strategy and prove its advantages for group-related metrics. Surprisingly, we show mathematically that, for common class partitions, it can also reduce the average set size of any CP score function. Our analysis reveals the class similarity factors behind this improvement and motivates us to propose a model-specific variant, which does not require any human semantic partition and can further reduce the prediction set size. Finally, we present an extensive empirical study, encompassing prominent CP methods, multiple models, and several datasets, which demonstrates that our class-similarity-based approach consistently enhances CP methods.", "link": "http://arxiv.org/abs/2511.19359v1", "date": "2025-11-24", "relevancy": 1.4396, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4847}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4832}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Conformal%20Prediction%20via%20Class%20Similarity&body=Title%3A%20Enhancing%20Conformal%20Prediction%20via%20Class%20Similarity%0AAuthor%3A%20Ariel%20Fargion%20and%20Lahav%20Dabah%20and%20Tom%20Tirer%0AAbstract%3A%20Conformal%20Prediction%20%28CP%29%20has%20emerged%20as%20a%20powerful%20statistical%20framework%20for%20high-stakes%20classification%20applications.%20Instead%20of%20predicting%20a%20single%20class%2C%20CP%20generates%20a%20prediction%20set%2C%20guaranteed%20to%20include%20the%20true%20label%20with%20a%20pre-specified%20probability.%20The%20performance%20of%20different%20CP%20methods%20is%20typically%20assessed%20by%20their%20average%20prediction%20set%20size.%20In%20setups%20where%20the%20classes%20can%20be%20partitioned%20into%20semantic%20groups%2C%20e.g.%2C%20diseases%20that%20require%20similar%20treatment%2C%20users%20can%20benefit%20from%20prediction%20sets%20that%20are%20not%20only%20small%20on%20average%2C%20but%20also%20contain%20a%20small%20number%20of%20semantically%20different%20groups.%20This%20paper%20begins%20by%20addressing%20this%20problem%20and%20ultimately%20offers%20a%20widely%20applicable%20tool%20for%20boosting%20any%20CP%20method%20on%20any%20dataset.%20First%2C%20given%20a%20class%20partition%2C%20we%20propose%20augmenting%20the%20CP%20score%20function%20with%20a%20term%20that%20penalizes%20predictions%20with%20out-of-group%20errors.%20We%20theoretically%20analyze%20this%20strategy%20and%20prove%20its%20advantages%20for%20group-related%20metrics.%20Surprisingly%2C%20we%20show%20mathematically%20that%2C%20for%20common%20class%20partitions%2C%20it%20can%20also%20reduce%20the%20average%20set%20size%20of%20any%20CP%20score%20function.%20Our%20analysis%20reveals%20the%20class%20similarity%20factors%20behind%20this%20improvement%20and%20motivates%20us%20to%20propose%20a%20model-specific%20variant%2C%20which%20does%20not%20require%20any%20human%20semantic%20partition%20and%20can%20further%20reduce%20the%20prediction%20set%20size.%20Finally%2C%20we%20present%20an%20extensive%20empirical%20study%2C%20encompassing%20prominent%20CP%20methods%2C%20multiple%20models%2C%20and%20several%20datasets%2C%20which%20demonstrates%20that%20our%20class-similarity-based%20approach%20consistently%20enhances%20CP%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Conformal%2520Prediction%2520via%2520Class%2520Similarity%26entry.906535625%3DAriel%2520Fargion%2520and%2520Lahav%2520Dabah%2520and%2520Tom%2520Tirer%26entry.1292438233%3DConformal%2520Prediction%2520%2528CP%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520statistical%2520framework%2520for%2520high-stakes%2520classification%2520applications.%2520Instead%2520of%2520predicting%2520a%2520single%2520class%252C%2520CP%2520generates%2520a%2520prediction%2520set%252C%2520guaranteed%2520to%2520include%2520the%2520true%2520label%2520with%2520a%2520pre-specified%2520probability.%2520The%2520performance%2520of%2520different%2520CP%2520methods%2520is%2520typically%2520assessed%2520by%2520their%2520average%2520prediction%2520set%2520size.%2520In%2520setups%2520where%2520the%2520classes%2520can%2520be%2520partitioned%2520into%2520semantic%2520groups%252C%2520e.g.%252C%2520diseases%2520that%2520require%2520similar%2520treatment%252C%2520users%2520can%2520benefit%2520from%2520prediction%2520sets%2520that%2520are%2520not%2520only%2520small%2520on%2520average%252C%2520but%2520also%2520contain%2520a%2520small%2520number%2520of%2520semantically%2520different%2520groups.%2520This%2520paper%2520begins%2520by%2520addressing%2520this%2520problem%2520and%2520ultimately%2520offers%2520a%2520widely%2520applicable%2520tool%2520for%2520boosting%2520any%2520CP%2520method%2520on%2520any%2520dataset.%2520First%252C%2520given%2520a%2520class%2520partition%252C%2520we%2520propose%2520augmenting%2520the%2520CP%2520score%2520function%2520with%2520a%2520term%2520that%2520penalizes%2520predictions%2520with%2520out-of-group%2520errors.%2520We%2520theoretically%2520analyze%2520this%2520strategy%2520and%2520prove%2520its%2520advantages%2520for%2520group-related%2520metrics.%2520Surprisingly%252C%2520we%2520show%2520mathematically%2520that%252C%2520for%2520common%2520class%2520partitions%252C%2520it%2520can%2520also%2520reduce%2520the%2520average%2520set%2520size%2520of%2520any%2520CP%2520score%2520function.%2520Our%2520analysis%2520reveals%2520the%2520class%2520similarity%2520factors%2520behind%2520this%2520improvement%2520and%2520motivates%2520us%2520to%2520propose%2520a%2520model-specific%2520variant%252C%2520which%2520does%2520not%2520require%2520any%2520human%2520semantic%2520partition%2520and%2520can%2520further%2520reduce%2520the%2520prediction%2520set%2520size.%2520Finally%252C%2520we%2520present%2520an%2520extensive%2520empirical%2520study%252C%2520encompassing%2520prominent%2520CP%2520methods%252C%2520multiple%2520models%252C%2520and%2520several%2520datasets%252C%2520which%2520demonstrates%2520that%2520our%2520class-similarity-based%2520approach%2520consistently%2520enhances%2520CP%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Conformal%20Prediction%20via%20Class%20Similarity&entry.906535625=Ariel%20Fargion%20and%20Lahav%20Dabah%20and%20Tom%20Tirer&entry.1292438233=Conformal%20Prediction%20%28CP%29%20has%20emerged%20as%20a%20powerful%20statistical%20framework%20for%20high-stakes%20classification%20applications.%20Instead%20of%20predicting%20a%20single%20class%2C%20CP%20generates%20a%20prediction%20set%2C%20guaranteed%20to%20include%20the%20true%20label%20with%20a%20pre-specified%20probability.%20The%20performance%20of%20different%20CP%20methods%20is%20typically%20assessed%20by%20their%20average%20prediction%20set%20size.%20In%20setups%20where%20the%20classes%20can%20be%20partitioned%20into%20semantic%20groups%2C%20e.g.%2C%20diseases%20that%20require%20similar%20treatment%2C%20users%20can%20benefit%20from%20prediction%20sets%20that%20are%20not%20only%20small%20on%20average%2C%20but%20also%20contain%20a%20small%20number%20of%20semantically%20different%20groups.%20This%20paper%20begins%20by%20addressing%20this%20problem%20and%20ultimately%20offers%20a%20widely%20applicable%20tool%20for%20boosting%20any%20CP%20method%20on%20any%20dataset.%20First%2C%20given%20a%20class%20partition%2C%20we%20propose%20augmenting%20the%20CP%20score%20function%20with%20a%20term%20that%20penalizes%20predictions%20with%20out-of-group%20errors.%20We%20theoretically%20analyze%20this%20strategy%20and%20prove%20its%20advantages%20for%20group-related%20metrics.%20Surprisingly%2C%20we%20show%20mathematically%20that%2C%20for%20common%20class%20partitions%2C%20it%20can%20also%20reduce%20the%20average%20set%20size%20of%20any%20CP%20score%20function.%20Our%20analysis%20reveals%20the%20class%20similarity%20factors%20behind%20this%20improvement%20and%20motivates%20us%20to%20propose%20a%20model-specific%20variant%2C%20which%20does%20not%20require%20any%20human%20semantic%20partition%20and%20can%20further%20reduce%20the%20prediction%20set%20size.%20Finally%2C%20we%20present%20an%20extensive%20empirical%20study%2C%20encompassing%20prominent%20CP%20methods%2C%20multiple%20models%2C%20and%20several%20datasets%2C%20which%20demonstrates%20that%20our%20class-similarity-based%20approach%20consistently%20enhances%20CP%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.19359v1&entry.124074799=Read"},
{"title": "Analysis of Deep-Learning Methods in an ISO/TS 15066-Compliant Human-Robot Safety Framework", "author": "David Bricher and Andreas Mueller", "abstract": "Over the last years collaborative robots have gained great success in manufacturing applications where human and robot work together in close proximity. However, current ISO/TS-15066-compliant implementations often limit the efficiency of collaborative tasks due to conservative speed restrictions. For this reason, this paper introduces a deep-learning-based human-robot-safety framework (HRSF) that aims at a dynamical adaptation of robot velocities depending on the separation distance between human and robot while respecting maximum biomechanical force and pressure limits. The applicability of the framework was investigated for four different deep learning approaches that can be used for human body extraction: human body recognition, human body segmentation, human pose estimation, and human body part segmentation. Unlike conventional industrial safety systems, the proposed HRSF differentiates individual human body parts from other objects, enabling optimized robot process execution. Experiments demonstrated a quantitative reduction in cycle time of up to 15% compared to conventional safety technology.", "link": "http://arxiv.org/abs/2511.19094v1", "date": "2025-11-24", "relevancy": 1.6368, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5884}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5344}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Deep-Learning%20Methods%20in%20an%20ISO/TS%2015066-Compliant%20Human-Robot%20Safety%20Framework&body=Title%3A%20Analysis%20of%20Deep-Learning%20Methods%20in%20an%20ISO/TS%2015066-Compliant%20Human-Robot%20Safety%20Framework%0AAuthor%3A%20David%20Bricher%20and%20Andreas%20Mueller%0AAbstract%3A%20Over%20the%20last%20years%20collaborative%20robots%20have%20gained%20great%20success%20in%20manufacturing%20applications%20where%20human%20and%20robot%20work%20together%20in%20close%20proximity.%20However%2C%20current%20ISO/TS-15066-compliant%20implementations%20often%20limit%20the%20efficiency%20of%20collaborative%20tasks%20due%20to%20conservative%20speed%20restrictions.%20For%20this%20reason%2C%20this%20paper%20introduces%20a%20deep-learning-based%20human-robot-safety%20framework%20%28HRSF%29%20that%20aims%20at%20a%20dynamical%20adaptation%20of%20robot%20velocities%20depending%20on%20the%20separation%20distance%20between%20human%20and%20robot%20while%20respecting%20maximum%20biomechanical%20force%20and%20pressure%20limits.%20The%20applicability%20of%20the%20framework%20was%20investigated%20for%20four%20different%20deep%20learning%20approaches%20that%20can%20be%20used%20for%20human%20body%20extraction%3A%20human%20body%20recognition%2C%20human%20body%20segmentation%2C%20human%20pose%20estimation%2C%20and%20human%20body%20part%20segmentation.%20Unlike%20conventional%20industrial%20safety%20systems%2C%20the%20proposed%20HRSF%20differentiates%20individual%20human%20body%20parts%20from%20other%20objects%2C%20enabling%20optimized%20robot%20process%20execution.%20Experiments%20demonstrated%20a%20quantitative%20reduction%20in%20cycle%20time%20of%20up%20to%2015%25%20compared%20to%20conventional%20safety%20technology.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Deep-Learning%2520Methods%2520in%2520an%2520ISO/TS%252015066-Compliant%2520Human-Robot%2520Safety%2520Framework%26entry.906535625%3DDavid%2520Bricher%2520and%2520Andreas%2520Mueller%26entry.1292438233%3DOver%2520the%2520last%2520years%2520collaborative%2520robots%2520have%2520gained%2520great%2520success%2520in%2520manufacturing%2520applications%2520where%2520human%2520and%2520robot%2520work%2520together%2520in%2520close%2520proximity.%2520However%252C%2520current%2520ISO/TS-15066-compliant%2520implementations%2520often%2520limit%2520the%2520efficiency%2520of%2520collaborative%2520tasks%2520due%2520to%2520conservative%2520speed%2520restrictions.%2520For%2520this%2520reason%252C%2520this%2520paper%2520introduces%2520a%2520deep-learning-based%2520human-robot-safety%2520framework%2520%2528HRSF%2529%2520that%2520aims%2520at%2520a%2520dynamical%2520adaptation%2520of%2520robot%2520velocities%2520depending%2520on%2520the%2520separation%2520distance%2520between%2520human%2520and%2520robot%2520while%2520respecting%2520maximum%2520biomechanical%2520force%2520and%2520pressure%2520limits.%2520The%2520applicability%2520of%2520the%2520framework%2520was%2520investigated%2520for%2520four%2520different%2520deep%2520learning%2520approaches%2520that%2520can%2520be%2520used%2520for%2520human%2520body%2520extraction%253A%2520human%2520body%2520recognition%252C%2520human%2520body%2520segmentation%252C%2520human%2520pose%2520estimation%252C%2520and%2520human%2520body%2520part%2520segmentation.%2520Unlike%2520conventional%2520industrial%2520safety%2520systems%252C%2520the%2520proposed%2520HRSF%2520differentiates%2520individual%2520human%2520body%2520parts%2520from%2520other%2520objects%252C%2520enabling%2520optimized%2520robot%2520process%2520execution.%2520Experiments%2520demonstrated%2520a%2520quantitative%2520reduction%2520in%2520cycle%2520time%2520of%2520up%2520to%252015%2525%2520compared%2520to%2520conventional%2520safety%2520technology.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Deep-Learning%20Methods%20in%20an%20ISO/TS%2015066-Compliant%20Human-Robot%20Safety%20Framework&entry.906535625=David%20Bricher%20and%20Andreas%20Mueller&entry.1292438233=Over%20the%20last%20years%20collaborative%20robots%20have%20gained%20great%20success%20in%20manufacturing%20applications%20where%20human%20and%20robot%20work%20together%20in%20close%20proximity.%20However%2C%20current%20ISO/TS-15066-compliant%20implementations%20often%20limit%20the%20efficiency%20of%20collaborative%20tasks%20due%20to%20conservative%20speed%20restrictions.%20For%20this%20reason%2C%20this%20paper%20introduces%20a%20deep-learning-based%20human-robot-safety%20framework%20%28HRSF%29%20that%20aims%20at%20a%20dynamical%20adaptation%20of%20robot%20velocities%20depending%20on%20the%20separation%20distance%20between%20human%20and%20robot%20while%20respecting%20maximum%20biomechanical%20force%20and%20pressure%20limits.%20The%20applicability%20of%20the%20framework%20was%20investigated%20for%20four%20different%20deep%20learning%20approaches%20that%20can%20be%20used%20for%20human%20body%20extraction%3A%20human%20body%20recognition%2C%20human%20body%20segmentation%2C%20human%20pose%20estimation%2C%20and%20human%20body%20part%20segmentation.%20Unlike%20conventional%20industrial%20safety%20systems%2C%20the%20proposed%20HRSF%20differentiates%20individual%20human%20body%20parts%20from%20other%20objects%2C%20enabling%20optimized%20robot%20process%20execution.%20Experiments%20demonstrated%20a%20quantitative%20reduction%20in%20cycle%20time%20of%20up%20to%2015%25%20compared%20to%20conventional%20safety%20technology.&entry.1838667208=http%3A//arxiv.org/abs/2511.19094v1&entry.124074799=Read"},
{"title": "From Spots to Pixels: Dense Spatial Gene Expression Prediction from Histology Images", "author": "Ruikun Zhang and Yan Yang and Liyuan Pan", "abstract": "Spatial transcriptomics (ST) measures gene expression at fine-grained spatial resolution, offering insights into tissue molecular landscapes. Previous methods for spatial gene expression prediction typically crop spots of interest from histopathology slide images, and train models to map each spot to a corresponding gene expression profile. However, these methods inherently lose the spatial resolution in gene expression: 1) each spot often contains multiple cells with distinct gene expression profiles; 2) spots are typically defined at fixed spatial resolutions, limiting the ability to predict gene expression at varying scales. To address these limitations, this paper presents PixNet, a dense prediction network capable of predicting spatially resolved gene expression across spots of varying sizes and scales directly from histopathology slide images. Different from previous methods that map individual spots to gene expression values, we generate a spatially dense continuous gene expression map from the histopathology slide image, and aggregate values within spots of interest to predict the gene expression. Our PixNet outperforms state-of-the-art methods on four common ST datasets in multiple spatial scales. The source code will be publicly available.", "link": "http://arxiv.org/abs/2503.01347v3", "date": "2025-11-24", "relevancy": 2.0084, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5087}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5012}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Spots%20to%20Pixels%3A%20Dense%20Spatial%20Gene%20Expression%20Prediction%20from%20Histology%20Images&body=Title%3A%20From%20Spots%20to%20Pixels%3A%20Dense%20Spatial%20Gene%20Expression%20Prediction%20from%20Histology%20Images%0AAuthor%3A%20Ruikun%20Zhang%20and%20Yan%20Yang%20and%20Liyuan%20Pan%0AAbstract%3A%20Spatial%20transcriptomics%20%28ST%29%20measures%20gene%20expression%20at%20fine-grained%20spatial%20resolution%2C%20offering%20insights%20into%20tissue%20molecular%20landscapes.%20Previous%20methods%20for%20spatial%20gene%20expression%20prediction%20typically%20crop%20spots%20of%20interest%20from%20histopathology%20slide%20images%2C%20and%20train%20models%20to%20map%20each%20spot%20to%20a%20corresponding%20gene%20expression%20profile.%20However%2C%20these%20methods%20inherently%20lose%20the%20spatial%20resolution%20in%20gene%20expression%3A%201%29%20each%20spot%20often%20contains%20multiple%20cells%20with%20distinct%20gene%20expression%20profiles%3B%202%29%20spots%20are%20typically%20defined%20at%20fixed%20spatial%20resolutions%2C%20limiting%20the%20ability%20to%20predict%20gene%20expression%20at%20varying%20scales.%20To%20address%20these%20limitations%2C%20this%20paper%20presents%20PixNet%2C%20a%20dense%20prediction%20network%20capable%20of%20predicting%20spatially%20resolved%20gene%20expression%20across%20spots%20of%20varying%20sizes%20and%20scales%20directly%20from%20histopathology%20slide%20images.%20Different%20from%20previous%20methods%20that%20map%20individual%20spots%20to%20gene%20expression%20values%2C%20we%20generate%20a%20spatially%20dense%20continuous%20gene%20expression%20map%20from%20the%20histopathology%20slide%20image%2C%20and%20aggregate%20values%20within%20spots%20of%20interest%20to%20predict%20the%20gene%20expression.%20Our%20PixNet%20outperforms%20state-of-the-art%20methods%20on%20four%20common%20ST%20datasets%20in%20multiple%20spatial%20scales.%20The%20source%20code%20will%20be%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2503.01347v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Spots%2520to%2520Pixels%253A%2520Dense%2520Spatial%2520Gene%2520Expression%2520Prediction%2520from%2520Histology%2520Images%26entry.906535625%3DRuikun%2520Zhang%2520and%2520Yan%2520Yang%2520and%2520Liyuan%2520Pan%26entry.1292438233%3DSpatial%2520transcriptomics%2520%2528ST%2529%2520measures%2520gene%2520expression%2520at%2520fine-grained%2520spatial%2520resolution%252C%2520offering%2520insights%2520into%2520tissue%2520molecular%2520landscapes.%2520Previous%2520methods%2520for%2520spatial%2520gene%2520expression%2520prediction%2520typically%2520crop%2520spots%2520of%2520interest%2520from%2520histopathology%2520slide%2520images%252C%2520and%2520train%2520models%2520to%2520map%2520each%2520spot%2520to%2520a%2520corresponding%2520gene%2520expression%2520profile.%2520However%252C%2520these%2520methods%2520inherently%2520lose%2520the%2520spatial%2520resolution%2520in%2520gene%2520expression%253A%25201%2529%2520each%2520spot%2520often%2520contains%2520multiple%2520cells%2520with%2520distinct%2520gene%2520expression%2520profiles%253B%25202%2529%2520spots%2520are%2520typically%2520defined%2520at%2520fixed%2520spatial%2520resolutions%252C%2520limiting%2520the%2520ability%2520to%2520predict%2520gene%2520expression%2520at%2520varying%2520scales.%2520To%2520address%2520these%2520limitations%252C%2520this%2520paper%2520presents%2520PixNet%252C%2520a%2520dense%2520prediction%2520network%2520capable%2520of%2520predicting%2520spatially%2520resolved%2520gene%2520expression%2520across%2520spots%2520of%2520varying%2520sizes%2520and%2520scales%2520directly%2520from%2520histopathology%2520slide%2520images.%2520Different%2520from%2520previous%2520methods%2520that%2520map%2520individual%2520spots%2520to%2520gene%2520expression%2520values%252C%2520we%2520generate%2520a%2520spatially%2520dense%2520continuous%2520gene%2520expression%2520map%2520from%2520the%2520histopathology%2520slide%2520image%252C%2520and%2520aggregate%2520values%2520within%2520spots%2520of%2520interest%2520to%2520predict%2520the%2520gene%2520expression.%2520Our%2520PixNet%2520outperforms%2520state-of-the-art%2520methods%2520on%2520four%2520common%2520ST%2520datasets%2520in%2520multiple%2520spatial%2520scales.%2520The%2520source%2520code%2520will%2520be%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01347v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Spots%20to%20Pixels%3A%20Dense%20Spatial%20Gene%20Expression%20Prediction%20from%20Histology%20Images&entry.906535625=Ruikun%20Zhang%20and%20Yan%20Yang%20and%20Liyuan%20Pan&entry.1292438233=Spatial%20transcriptomics%20%28ST%29%20measures%20gene%20expression%20at%20fine-grained%20spatial%20resolution%2C%20offering%20insights%20into%20tissue%20molecular%20landscapes.%20Previous%20methods%20for%20spatial%20gene%20expression%20prediction%20typically%20crop%20spots%20of%20interest%20from%20histopathology%20slide%20images%2C%20and%20train%20models%20to%20map%20each%20spot%20to%20a%20corresponding%20gene%20expression%20profile.%20However%2C%20these%20methods%20inherently%20lose%20the%20spatial%20resolution%20in%20gene%20expression%3A%201%29%20each%20spot%20often%20contains%20multiple%20cells%20with%20distinct%20gene%20expression%20profiles%3B%202%29%20spots%20are%20typically%20defined%20at%20fixed%20spatial%20resolutions%2C%20limiting%20the%20ability%20to%20predict%20gene%20expression%20at%20varying%20scales.%20To%20address%20these%20limitations%2C%20this%20paper%20presents%20PixNet%2C%20a%20dense%20prediction%20network%20capable%20of%20predicting%20spatially%20resolved%20gene%20expression%20across%20spots%20of%20varying%20sizes%20and%20scales%20directly%20from%20histopathology%20slide%20images.%20Different%20from%20previous%20methods%20that%20map%20individual%20spots%20to%20gene%20expression%20values%2C%20we%20generate%20a%20spatially%20dense%20continuous%20gene%20expression%20map%20from%20the%20histopathology%20slide%20image%2C%20and%20aggregate%20values%20within%20spots%20of%20interest%20to%20predict%20the%20gene%20expression.%20Our%20PixNet%20outperforms%20state-of-the-art%20methods%20on%20four%20common%20ST%20datasets%20in%20multiple%20spatial%20scales.%20The%20source%20code%20will%20be%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2503.01347v3&entry.124074799=Read"},
{"title": "InfoScale: Unleashing Training-free Variable-scaled Image Generation via Effective Utilization of Information", "author": "Guohui Zhang and Jiangtong Tan and Linjiang Huang and Zhonghang Yuan and Mingde Yao and Jie Huang and Feng Zhao", "abstract": "Diffusion models (DMs) have become dominant in visual generation but suffer performance drop when tested on resolutions that differ from the training scale, whether lower or higher. In fact, the key challenge in generating variable-scale images lies in the differing amounts of information across resolutions, which requires information conversion procedures to be varied for generating variable-scaled images. In this paper, we investigate the issues of three critical aspects in DMs for a unified analysis in variable-scaled generation: dilated convolution, attention mechanisms, and initial noise. Specifically, 1) dilated convolution in DMs for the higher-resolution generation loses high-frequency information. 2) Attention for variable-scaled image generation struggles to adjust the information aggregation adaptively. 3) The spatial distribution of information in the initial noise is misaligned with variable-scaled image. To solve the above problems, we propose \\textbf{InfoScale}, an information-centric framework for variable-scaled image generation by effectively utilizing information from three aspects correspondingly. For information loss in 1), we introduce Progressive Frequency Compensation module to compensate for high-frequency information lost by dilated convolution in higher-resolution generation. For information aggregation inflexibility in 2), we introduce Adaptive Information Aggregation module to adaptively aggregate information in lower-resolution generation and achieve an effective balance between local and global information in higher-resolution generation. For information distribution misalignment in 3), we design Noise Adaptation module to re-distribute information in initial noise for variable-scaled generation. Our method is plug-and-play for DMs and extensive experiments demonstrate the effectiveness in variable-scaled image generation.", "link": "http://arxiv.org/abs/2509.01421v3", "date": "2025-11-24", "relevancy": 1.8738, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6614}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6164}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfoScale%3A%20Unleashing%20Training-free%20Variable-scaled%20Image%20Generation%20via%20Effective%20Utilization%20of%20Information&body=Title%3A%20InfoScale%3A%20Unleashing%20Training-free%20Variable-scaled%20Image%20Generation%20via%20Effective%20Utilization%20of%20Information%0AAuthor%3A%20Guohui%20Zhang%20and%20Jiangtong%20Tan%20and%20Linjiang%20Huang%20and%20Zhonghang%20Yuan%20and%20Mingde%20Yao%20and%20Jie%20Huang%20and%20Feng%20Zhao%0AAbstract%3A%20Diffusion%20models%20%28DMs%29%20have%20become%20dominant%20in%20visual%20generation%20but%20suffer%20performance%20drop%20when%20tested%20on%20resolutions%20that%20differ%20from%20the%20training%20scale%2C%20whether%20lower%20or%20higher.%20In%20fact%2C%20the%20key%20challenge%20in%20generating%20variable-scale%20images%20lies%20in%20the%20differing%20amounts%20of%20information%20across%20resolutions%2C%20which%20requires%20information%20conversion%20procedures%20to%20be%20varied%20for%20generating%20variable-scaled%20images.%20In%20this%20paper%2C%20we%20investigate%20the%20issues%20of%20three%20critical%20aspects%20in%20DMs%20for%20a%20unified%20analysis%20in%20variable-scaled%20generation%3A%20dilated%20convolution%2C%20attention%20mechanisms%2C%20and%20initial%20noise.%20Specifically%2C%201%29%20dilated%20convolution%20in%20DMs%20for%20the%20higher-resolution%20generation%20loses%20high-frequency%20information.%202%29%20Attention%20for%20variable-scaled%20image%20generation%20struggles%20to%20adjust%20the%20information%20aggregation%20adaptively.%203%29%20The%20spatial%20distribution%20of%20information%20in%20the%20initial%20noise%20is%20misaligned%20with%20variable-scaled%20image.%20To%20solve%20the%20above%20problems%2C%20we%20propose%20%5Ctextbf%7BInfoScale%7D%2C%20an%20information-centric%20framework%20for%20variable-scaled%20image%20generation%20by%20effectively%20utilizing%20information%20from%20three%20aspects%20correspondingly.%20For%20information%20loss%20in%201%29%2C%20we%20introduce%20Progressive%20Frequency%20Compensation%20module%20to%20compensate%20for%20high-frequency%20information%20lost%20by%20dilated%20convolution%20in%20higher-resolution%20generation.%20For%20information%20aggregation%20inflexibility%20in%202%29%2C%20we%20introduce%20Adaptive%20Information%20Aggregation%20module%20to%20adaptively%20aggregate%20information%20in%20lower-resolution%20generation%20and%20achieve%20an%20effective%20balance%20between%20local%20and%20global%20information%20in%20higher-resolution%20generation.%20For%20information%20distribution%20misalignment%20in%203%29%2C%20we%20design%20Noise%20Adaptation%20module%20to%20re-distribute%20information%20in%20initial%20noise%20for%20variable-scaled%20generation.%20Our%20method%20is%20plug-and-play%20for%20DMs%20and%20extensive%20experiments%20demonstrate%20the%20effectiveness%20in%20variable-scaled%20image%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2509.01421v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfoScale%253A%2520Unleashing%2520Training-free%2520Variable-scaled%2520Image%2520Generation%2520via%2520Effective%2520Utilization%2520of%2520Information%26entry.906535625%3DGuohui%2520Zhang%2520and%2520Jiangtong%2520Tan%2520and%2520Linjiang%2520Huang%2520and%2520Zhonghang%2520Yuan%2520and%2520Mingde%2520Yao%2520and%2520Jie%2520Huang%2520and%2520Feng%2520Zhao%26entry.1292438233%3DDiffusion%2520models%2520%2528DMs%2529%2520have%2520become%2520dominant%2520in%2520visual%2520generation%2520but%2520suffer%2520performance%2520drop%2520when%2520tested%2520on%2520resolutions%2520that%2520differ%2520from%2520the%2520training%2520scale%252C%2520whether%2520lower%2520or%2520higher.%2520In%2520fact%252C%2520the%2520key%2520challenge%2520in%2520generating%2520variable-scale%2520images%2520lies%2520in%2520the%2520differing%2520amounts%2520of%2520information%2520across%2520resolutions%252C%2520which%2520requires%2520information%2520conversion%2520procedures%2520to%2520be%2520varied%2520for%2520generating%2520variable-scaled%2520images.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520issues%2520of%2520three%2520critical%2520aspects%2520in%2520DMs%2520for%2520a%2520unified%2520analysis%2520in%2520variable-scaled%2520generation%253A%2520dilated%2520convolution%252C%2520attention%2520mechanisms%252C%2520and%2520initial%2520noise.%2520Specifically%252C%25201%2529%2520dilated%2520convolution%2520in%2520DMs%2520for%2520the%2520higher-resolution%2520generation%2520loses%2520high-frequency%2520information.%25202%2529%2520Attention%2520for%2520variable-scaled%2520image%2520generation%2520struggles%2520to%2520adjust%2520the%2520information%2520aggregation%2520adaptively.%25203%2529%2520The%2520spatial%2520distribution%2520of%2520information%2520in%2520the%2520initial%2520noise%2520is%2520misaligned%2520with%2520variable-scaled%2520image.%2520To%2520solve%2520the%2520above%2520problems%252C%2520we%2520propose%2520%255Ctextbf%257BInfoScale%257D%252C%2520an%2520information-centric%2520framework%2520for%2520variable-scaled%2520image%2520generation%2520by%2520effectively%2520utilizing%2520information%2520from%2520three%2520aspects%2520correspondingly.%2520For%2520information%2520loss%2520in%25201%2529%252C%2520we%2520introduce%2520Progressive%2520Frequency%2520Compensation%2520module%2520to%2520compensate%2520for%2520high-frequency%2520information%2520lost%2520by%2520dilated%2520convolution%2520in%2520higher-resolution%2520generation.%2520For%2520information%2520aggregation%2520inflexibility%2520in%25202%2529%252C%2520we%2520introduce%2520Adaptive%2520Information%2520Aggregation%2520module%2520to%2520adaptively%2520aggregate%2520information%2520in%2520lower-resolution%2520generation%2520and%2520achieve%2520an%2520effective%2520balance%2520between%2520local%2520and%2520global%2520information%2520in%2520higher-resolution%2520generation.%2520For%2520information%2520distribution%2520misalignment%2520in%25203%2529%252C%2520we%2520design%2520Noise%2520Adaptation%2520module%2520to%2520re-distribute%2520information%2520in%2520initial%2520noise%2520for%2520variable-scaled%2520generation.%2520Our%2520method%2520is%2520plug-and-play%2520for%2520DMs%2520and%2520extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520in%2520variable-scaled%2520image%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.01421v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfoScale%3A%20Unleashing%20Training-free%20Variable-scaled%20Image%20Generation%20via%20Effective%20Utilization%20of%20Information&entry.906535625=Guohui%20Zhang%20and%20Jiangtong%20Tan%20and%20Linjiang%20Huang%20and%20Zhonghang%20Yuan%20and%20Mingde%20Yao%20and%20Jie%20Huang%20and%20Feng%20Zhao&entry.1292438233=Diffusion%20models%20%28DMs%29%20have%20become%20dominant%20in%20visual%20generation%20but%20suffer%20performance%20drop%20when%20tested%20on%20resolutions%20that%20differ%20from%20the%20training%20scale%2C%20whether%20lower%20or%20higher.%20In%20fact%2C%20the%20key%20challenge%20in%20generating%20variable-scale%20images%20lies%20in%20the%20differing%20amounts%20of%20information%20across%20resolutions%2C%20which%20requires%20information%20conversion%20procedures%20to%20be%20varied%20for%20generating%20variable-scaled%20images.%20In%20this%20paper%2C%20we%20investigate%20the%20issues%20of%20three%20critical%20aspects%20in%20DMs%20for%20a%20unified%20analysis%20in%20variable-scaled%20generation%3A%20dilated%20convolution%2C%20attention%20mechanisms%2C%20and%20initial%20noise.%20Specifically%2C%201%29%20dilated%20convolution%20in%20DMs%20for%20the%20higher-resolution%20generation%20loses%20high-frequency%20information.%202%29%20Attention%20for%20variable-scaled%20image%20generation%20struggles%20to%20adjust%20the%20information%20aggregation%20adaptively.%203%29%20The%20spatial%20distribution%20of%20information%20in%20the%20initial%20noise%20is%20misaligned%20with%20variable-scaled%20image.%20To%20solve%20the%20above%20problems%2C%20we%20propose%20%5Ctextbf%7BInfoScale%7D%2C%20an%20information-centric%20framework%20for%20variable-scaled%20image%20generation%20by%20effectively%20utilizing%20information%20from%20three%20aspects%20correspondingly.%20For%20information%20loss%20in%201%29%2C%20we%20introduce%20Progressive%20Frequency%20Compensation%20module%20to%20compensate%20for%20high-frequency%20information%20lost%20by%20dilated%20convolution%20in%20higher-resolution%20generation.%20For%20information%20aggregation%20inflexibility%20in%202%29%2C%20we%20introduce%20Adaptive%20Information%20Aggregation%20module%20to%20adaptively%20aggregate%20information%20in%20lower-resolution%20generation%20and%20achieve%20an%20effective%20balance%20between%20local%20and%20global%20information%20in%20higher-resolution%20generation.%20For%20information%20distribution%20misalignment%20in%203%29%2C%20we%20design%20Noise%20Adaptation%20module%20to%20re-distribute%20information%20in%20initial%20noise%20for%20variable-scaled%20generation.%20Our%20method%20is%20plug-and-play%20for%20DMs%20and%20extensive%20experiments%20demonstrate%20the%20effectiveness%20in%20variable-scaled%20image%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2509.01421v3&entry.124074799=Read"},
{"title": "CSD: Change Semantic Detection with only Semantic Change Masks for Damage Assessment in Conflict Zones", "author": "Kai Zhenga and Zhenkai Wu and Fupeng Wei and Miaolan Zhou and Kai Lie and Haitao Guo and Lei Ding and Wei Zhang and Hang-Cheng Dong", "abstract": "Accurately and swiftly assessing damage from conflicts is crucial for humanitarian aid and regional stability. In conflict zones, damaged zones often share similar architectural styles, with damage typically covering small areas and exhibiting blurred boundaries. These characteristics lead to limited data, annotation difficulties, and significant recognition challenges, including high intra-class similarity and ambiguous semantic changes. To address these issues, we introduce a pre-trained DINOv3 model and propose a multi-scale cross-attention difference siamese network (MC-DiSNet). The powerful visual representation capability of the DINOv3 backbone enables robust and rich feature extraction from bi-temporal remote sensing images. We also release a new Gaza-change dataset containing high-resolution satellite image pairs from 2023-2024 with pixel-level semantic change annotations. It is worth emphasizing that our annotations only include semantic pixels of changed areas. Unlike conventional semantic change detection (SCD), our approach eliminates the need for large-scale semantic annotations of bi-temporal images, instead focusing directly on the changed regions. We term this new task change semantic detection (CSD). The CSD task represents a direct extension of binary change detection (BCD). Due to the limited spatial extent of semantic regions, it presents greater challenges than traditional SCD tasks. We evaluated our method under the CSD framework on both the Gaza-Change and SECOND datasets. Experimental results demonstrate that our proposed approach effectively addresses the CSD task, and its outstanding performance paves the way for practical applications in rapid damage assessment across conflict zones.", "link": "http://arxiv.org/abs/2511.19035v1", "date": "2025-11-24", "relevancy": 2.0482, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CSD%3A%20Change%20Semantic%20Detection%20with%20only%20Semantic%20Change%20Masks%20for%20Damage%20Assessment%20in%20Conflict%20Zones&body=Title%3A%20CSD%3A%20Change%20Semantic%20Detection%20with%20only%20Semantic%20Change%20Masks%20for%20Damage%20Assessment%20in%20Conflict%20Zones%0AAuthor%3A%20Kai%20Zhenga%20and%20Zhenkai%20Wu%20and%20Fupeng%20Wei%20and%20Miaolan%20Zhou%20and%20Kai%20Lie%20and%20Haitao%20Guo%20and%20Lei%20Ding%20and%20Wei%20Zhang%20and%20Hang-Cheng%20Dong%0AAbstract%3A%20Accurately%20and%20swiftly%20assessing%20damage%20from%20conflicts%20is%20crucial%20for%20humanitarian%20aid%20and%20regional%20stability.%20In%20conflict%20zones%2C%20damaged%20zones%20often%20share%20similar%20architectural%20styles%2C%20with%20damage%20typically%20covering%20small%20areas%20and%20exhibiting%20blurred%20boundaries.%20These%20characteristics%20lead%20to%20limited%20data%2C%20annotation%20difficulties%2C%20and%20significant%20recognition%20challenges%2C%20including%20high%20intra-class%20similarity%20and%20ambiguous%20semantic%20changes.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20pre-trained%20DINOv3%20model%20and%20propose%20a%20multi-scale%20cross-attention%20difference%20siamese%20network%20%28MC-DiSNet%29.%20The%20powerful%20visual%20representation%20capability%20of%20the%20DINOv3%20backbone%20enables%20robust%20and%20rich%20feature%20extraction%20from%20bi-temporal%20remote%20sensing%20images.%20We%20also%20release%20a%20new%20Gaza-change%20dataset%20containing%20high-resolution%20satellite%20image%20pairs%20from%202023-2024%20with%20pixel-level%20semantic%20change%20annotations.%20It%20is%20worth%20emphasizing%20that%20our%20annotations%20only%20include%20semantic%20pixels%20of%20changed%20areas.%20Unlike%20conventional%20semantic%20change%20detection%20%28SCD%29%2C%20our%20approach%20eliminates%20the%20need%20for%20large-scale%20semantic%20annotations%20of%20bi-temporal%20images%2C%20instead%20focusing%20directly%20on%20the%20changed%20regions.%20We%20term%20this%20new%20task%20change%20semantic%20detection%20%28CSD%29.%20The%20CSD%20task%20represents%20a%20direct%20extension%20of%20binary%20change%20detection%20%28BCD%29.%20Due%20to%20the%20limited%20spatial%20extent%20of%20semantic%20regions%2C%20it%20presents%20greater%20challenges%20than%20traditional%20SCD%20tasks.%20We%20evaluated%20our%20method%20under%20the%20CSD%20framework%20on%20both%20the%20Gaza-Change%20and%20SECOND%20datasets.%20Experimental%20results%20demonstrate%20that%20our%20proposed%20approach%20effectively%20addresses%20the%20CSD%20task%2C%20and%20its%20outstanding%20performance%20paves%20the%20way%20for%20practical%20applications%20in%20rapid%20damage%20assessment%20across%20conflict%20zones.%0ALink%3A%20http%3A//arxiv.org/abs/2511.19035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCSD%253A%2520Change%2520Semantic%2520Detection%2520with%2520only%2520Semantic%2520Change%2520Masks%2520for%2520Damage%2520Assessment%2520in%2520Conflict%2520Zones%26entry.906535625%3DKai%2520Zhenga%2520and%2520Zhenkai%2520Wu%2520and%2520Fupeng%2520Wei%2520and%2520Miaolan%2520Zhou%2520and%2520Kai%2520Lie%2520and%2520Haitao%2520Guo%2520and%2520Lei%2520Ding%2520and%2520Wei%2520Zhang%2520and%2520Hang-Cheng%2520Dong%26entry.1292438233%3DAccurately%2520and%2520swiftly%2520assessing%2520damage%2520from%2520conflicts%2520is%2520crucial%2520for%2520humanitarian%2520aid%2520and%2520regional%2520stability.%2520In%2520conflict%2520zones%252C%2520damaged%2520zones%2520often%2520share%2520similar%2520architectural%2520styles%252C%2520with%2520damage%2520typically%2520covering%2520small%2520areas%2520and%2520exhibiting%2520blurred%2520boundaries.%2520These%2520characteristics%2520lead%2520to%2520limited%2520data%252C%2520annotation%2520difficulties%252C%2520and%2520significant%2520recognition%2520challenges%252C%2520including%2520high%2520intra-class%2520similarity%2520and%2520ambiguous%2520semantic%2520changes.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520a%2520pre-trained%2520DINOv3%2520model%2520and%2520propose%2520a%2520multi-scale%2520cross-attention%2520difference%2520siamese%2520network%2520%2528MC-DiSNet%2529.%2520The%2520powerful%2520visual%2520representation%2520capability%2520of%2520the%2520DINOv3%2520backbone%2520enables%2520robust%2520and%2520rich%2520feature%2520extraction%2520from%2520bi-temporal%2520remote%2520sensing%2520images.%2520We%2520also%2520release%2520a%2520new%2520Gaza-change%2520dataset%2520containing%2520high-resolution%2520satellite%2520image%2520pairs%2520from%25202023-2024%2520with%2520pixel-level%2520semantic%2520change%2520annotations.%2520It%2520is%2520worth%2520emphasizing%2520that%2520our%2520annotations%2520only%2520include%2520semantic%2520pixels%2520of%2520changed%2520areas.%2520Unlike%2520conventional%2520semantic%2520change%2520detection%2520%2528SCD%2529%252C%2520our%2520approach%2520eliminates%2520the%2520need%2520for%2520large-scale%2520semantic%2520annotations%2520of%2520bi-temporal%2520images%252C%2520instead%2520focusing%2520directly%2520on%2520the%2520changed%2520regions.%2520We%2520term%2520this%2520new%2520task%2520change%2520semantic%2520detection%2520%2528CSD%2529.%2520The%2520CSD%2520task%2520represents%2520a%2520direct%2520extension%2520of%2520binary%2520change%2520detection%2520%2528BCD%2529.%2520Due%2520to%2520the%2520limited%2520spatial%2520extent%2520of%2520semantic%2520regions%252C%2520it%2520presents%2520greater%2520challenges%2520than%2520traditional%2520SCD%2520tasks.%2520We%2520evaluated%2520our%2520method%2520under%2520the%2520CSD%2520framework%2520on%2520both%2520the%2520Gaza-Change%2520and%2520SECOND%2520datasets.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520proposed%2520approach%2520effectively%2520addresses%2520the%2520CSD%2520task%252C%2520and%2520its%2520outstanding%2520performance%2520paves%2520the%2520way%2520for%2520practical%2520applications%2520in%2520rapid%2520damage%2520assessment%2520across%2520conflict%2520zones.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSD%3A%20Change%20Semantic%20Detection%20with%20only%20Semantic%20Change%20Masks%20for%20Damage%20Assessment%20in%20Conflict%20Zones&entry.906535625=Kai%20Zhenga%20and%20Zhenkai%20Wu%20and%20Fupeng%20Wei%20and%20Miaolan%20Zhou%20and%20Kai%20Lie%20and%20Haitao%20Guo%20and%20Lei%20Ding%20and%20Wei%20Zhang%20and%20Hang-Cheng%20Dong&entry.1292438233=Accurately%20and%20swiftly%20assessing%20damage%20from%20conflicts%20is%20crucial%20for%20humanitarian%20aid%20and%20regional%20stability.%20In%20conflict%20zones%2C%20damaged%20zones%20often%20share%20similar%20architectural%20styles%2C%20with%20damage%20typically%20covering%20small%20areas%20and%20exhibiting%20blurred%20boundaries.%20These%20characteristics%20lead%20to%20limited%20data%2C%20annotation%20difficulties%2C%20and%20significant%20recognition%20challenges%2C%20including%20high%20intra-class%20similarity%20and%20ambiguous%20semantic%20changes.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20pre-trained%20DINOv3%20model%20and%20propose%20a%20multi-scale%20cross-attention%20difference%20siamese%20network%20%28MC-DiSNet%29.%20The%20powerful%20visual%20representation%20capability%20of%20the%20DINOv3%20backbone%20enables%20robust%20and%20rich%20feature%20extraction%20from%20bi-temporal%20remote%20sensing%20images.%20We%20also%20release%20a%20new%20Gaza-change%20dataset%20containing%20high-resolution%20satellite%20image%20pairs%20from%202023-2024%20with%20pixel-level%20semantic%20change%20annotations.%20It%20is%20worth%20emphasizing%20that%20our%20annotations%20only%20include%20semantic%20pixels%20of%20changed%20areas.%20Unlike%20conventional%20semantic%20change%20detection%20%28SCD%29%2C%20our%20approach%20eliminates%20the%20need%20for%20large-scale%20semantic%20annotations%20of%20bi-temporal%20images%2C%20instead%20focusing%20directly%20on%20the%20changed%20regions.%20We%20term%20this%20new%20task%20change%20semantic%20detection%20%28CSD%29.%20The%20CSD%20task%20represents%20a%20direct%20extension%20of%20binary%20change%20detection%20%28BCD%29.%20Due%20to%20the%20limited%20spatial%20extent%20of%20semantic%20regions%2C%20it%20presents%20greater%20challenges%20than%20traditional%20SCD%20tasks.%20We%20evaluated%20our%20method%20under%20the%20CSD%20framework%20on%20both%20the%20Gaza-Change%20and%20SECOND%20datasets.%20Experimental%20results%20demonstrate%20that%20our%20proposed%20approach%20effectively%20addresses%20the%20CSD%20task%2C%20and%20its%20outstanding%20performance%20paves%20the%20way%20for%20practical%20applications%20in%20rapid%20damage%20assessment%20across%20conflict%20zones.&entry.1838667208=http%3A//arxiv.org/abs/2511.19035v1&entry.124074799=Read"},
{"title": "Forecasting-based Biomedical Time-series Data Synthesis for Open Data and Robust AI", "author": "Youngjoon Lee and Seongmin Cho and Yehhyun Jo and Jinu Gong and Hyunjoo Jenny Lee and Joonhyuk Kang", "abstract": "The limited data availability due to strict privacy regulations and significant resource demands severely constrains biomedical time-series AI development, which creates a critical gap between data requirements and accessibility. Synthetic data generation presents a promising solution by producing artificial datasets that maintain the statistical properties of real biomedical time-series data without compromising patient confidentiality. While GANs, VAEs, and diffusion models capture global data distributions, forecasting models offer inductive biases tailored for sequential dynamics. We propose a framework for synthetic biomedical time-series data generation based on recent forecasting models that accurately replicates complex electrophysiological signals such as EEG and EMG with high fidelity. These synthetic datasets can be freely shared for open AI development and consistently improve downstream model performance. Numerical results on sleep-stage classification show up to a 3.71\\% performance gain with augmentation and a 91.00\\% synthetic-only accuracy that surpasses the real-data-only baseline.", "link": "http://arxiv.org/abs/2510.04622v2", "date": "2025-11-24", "relevancy": 1.5596, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5371}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5046}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forecasting-based%20Biomedical%20Time-series%20Data%20Synthesis%20for%20Open%20Data%20and%20Robust%20AI&body=Title%3A%20Forecasting-based%20Biomedical%20Time-series%20Data%20Synthesis%20for%20Open%20Data%20and%20Robust%20AI%0AAuthor%3A%20Youngjoon%20Lee%20and%20Seongmin%20Cho%20and%20Yehhyun%20Jo%20and%20Jinu%20Gong%20and%20Hyunjoo%20Jenny%20Lee%20and%20Joonhyuk%20Kang%0AAbstract%3A%20The%20limited%20data%20availability%20due%20to%20strict%20privacy%20regulations%20and%20significant%20resource%20demands%20severely%20constrains%20biomedical%20time-series%20AI%20development%2C%20which%20creates%20a%20critical%20gap%20between%20data%20requirements%20and%20accessibility.%20Synthetic%20data%20generation%20presents%20a%20promising%20solution%20by%20producing%20artificial%20datasets%20that%20maintain%20the%20statistical%20properties%20of%20real%20biomedical%20time-series%20data%20without%20compromising%20patient%20confidentiality.%20While%20GANs%2C%20VAEs%2C%20and%20diffusion%20models%20capture%20global%20data%20distributions%2C%20forecasting%20models%20offer%20inductive%20biases%20tailored%20for%20sequential%20dynamics.%20We%20propose%20a%20framework%20for%20synthetic%20biomedical%20time-series%20data%20generation%20based%20on%20recent%20forecasting%20models%20that%20accurately%20replicates%20complex%20electrophysiological%20signals%20such%20as%20EEG%20and%20EMG%20with%20high%20fidelity.%20These%20synthetic%20datasets%20can%20be%20freely%20shared%20for%20open%20AI%20development%20and%20consistently%20improve%20downstream%20model%20performance.%20Numerical%20results%20on%20sleep-stage%20classification%20show%20up%20to%20a%203.71%5C%25%20performance%20gain%20with%20augmentation%20and%20a%2091.00%5C%25%20synthetic-only%20accuracy%20that%20surpasses%20the%20real-data-only%20baseline.%0ALink%3A%20http%3A//arxiv.org/abs/2510.04622v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForecasting-based%2520Biomedical%2520Time-series%2520Data%2520Synthesis%2520for%2520Open%2520Data%2520and%2520Robust%2520AI%26entry.906535625%3DYoungjoon%2520Lee%2520and%2520Seongmin%2520Cho%2520and%2520Yehhyun%2520Jo%2520and%2520Jinu%2520Gong%2520and%2520Hyunjoo%2520Jenny%2520Lee%2520and%2520Joonhyuk%2520Kang%26entry.1292438233%3DThe%2520limited%2520data%2520availability%2520due%2520to%2520strict%2520privacy%2520regulations%2520and%2520significant%2520resource%2520demands%2520severely%2520constrains%2520biomedical%2520time-series%2520AI%2520development%252C%2520which%2520creates%2520a%2520critical%2520gap%2520between%2520data%2520requirements%2520and%2520accessibility.%2520Synthetic%2520data%2520generation%2520presents%2520a%2520promising%2520solution%2520by%2520producing%2520artificial%2520datasets%2520that%2520maintain%2520the%2520statistical%2520properties%2520of%2520real%2520biomedical%2520time-series%2520data%2520without%2520compromising%2520patient%2520confidentiality.%2520While%2520GANs%252C%2520VAEs%252C%2520and%2520diffusion%2520models%2520capture%2520global%2520data%2520distributions%252C%2520forecasting%2520models%2520offer%2520inductive%2520biases%2520tailored%2520for%2520sequential%2520dynamics.%2520We%2520propose%2520a%2520framework%2520for%2520synthetic%2520biomedical%2520time-series%2520data%2520generation%2520based%2520on%2520recent%2520forecasting%2520models%2520that%2520accurately%2520replicates%2520complex%2520electrophysiological%2520signals%2520such%2520as%2520EEG%2520and%2520EMG%2520with%2520high%2520fidelity.%2520These%2520synthetic%2520datasets%2520can%2520be%2520freely%2520shared%2520for%2520open%2520AI%2520development%2520and%2520consistently%2520improve%2520downstream%2520model%2520performance.%2520Numerical%2520results%2520on%2520sleep-stage%2520classification%2520show%2520up%2520to%2520a%25203.71%255C%2525%2520performance%2520gain%2520with%2520augmentation%2520and%2520a%252091.00%255C%2525%2520synthetic-only%2520accuracy%2520that%2520surpasses%2520the%2520real-data-only%2520baseline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04622v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forecasting-based%20Biomedical%20Time-series%20Data%20Synthesis%20for%20Open%20Data%20and%20Robust%20AI&entry.906535625=Youngjoon%20Lee%20and%20Seongmin%20Cho%20and%20Yehhyun%20Jo%20and%20Jinu%20Gong%20and%20Hyunjoo%20Jenny%20Lee%20and%20Joonhyuk%20Kang&entry.1292438233=The%20limited%20data%20availability%20due%20to%20strict%20privacy%20regulations%20and%20significant%20resource%20demands%20severely%20constrains%20biomedical%20time-series%20AI%20development%2C%20which%20creates%20a%20critical%20gap%20between%20data%20requirements%20and%20accessibility.%20Synthetic%20data%20generation%20presents%20a%20promising%20solution%20by%20producing%20artificial%20datasets%20that%20maintain%20the%20statistical%20properties%20of%20real%20biomedical%20time-series%20data%20without%20compromising%20patient%20confidentiality.%20While%20GANs%2C%20VAEs%2C%20and%20diffusion%20models%20capture%20global%20data%20distributions%2C%20forecasting%20models%20offer%20inductive%20biases%20tailored%20for%20sequential%20dynamics.%20We%20propose%20a%20framework%20for%20synthetic%20biomedical%20time-series%20data%20generation%20based%20on%20recent%20forecasting%20models%20that%20accurately%20replicates%20complex%20electrophysiological%20signals%20such%20as%20EEG%20and%20EMG%20with%20high%20fidelity.%20These%20synthetic%20datasets%20can%20be%20freely%20shared%20for%20open%20AI%20development%20and%20consistently%20improve%20downstream%20model%20performance.%20Numerical%20results%20on%20sleep-stage%20classification%20show%20up%20to%20a%203.71%5C%25%20performance%20gain%20with%20augmentation%20and%20a%2091.00%5C%25%20synthetic-only%20accuracy%20that%20surpasses%20the%20real-data-only%20baseline.&entry.1838667208=http%3A//arxiv.org/abs/2510.04622v2&entry.124074799=Read"},
{"title": "DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection", "author": "Hai Ci and Ziheng Peng and Pei Yang and Yingxin Xuan and Mike Zheng Shou", "abstract": "Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k", "link": "http://arxiv.org/abs/2511.19111v1", "date": "2025-11-24", "relevancy": 1.7131, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5879}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5751}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffSeg30k%3A%20A%20Multi-Turn%20Diffusion%20Editing%20Benchmark%20for%20Localized%20AIGC%20Detection&body=Title%3A%20DiffSeg30k%3A%20A%20Multi-Turn%20Diffusion%20Editing%20Benchmark%20for%20Localized%20AIGC%20Detection%0AAuthor%3A%20Hai%20Ci%20and%20Ziheng%20Peng%20and%20Pei%20Yang%20and%20Yingxin%20Xuan%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20Diffusion-based%20editing%20enables%20realistic%20modification%20of%20local%20image%20regions%2C%20making%20AI-generated%20content%20harder%20to%20detect.%20Existing%20AIGC%20detection%20benchmarks%20focus%20on%20classifying%20entire%20images%2C%20overlooking%20the%20localization%20of%20diffusion-based%20edits.%20We%20introduce%20DiffSeg30k%2C%20a%20publicly%20available%20dataset%20of%2030k%20diffusion-edited%20images%20with%20pixel-level%20annotations%2C%20designed%20to%20support%20fine-grained%20detection.%20DiffSeg30k%20features%3A%201%29%20In-the-wild%20images--we%20collect%20images%20or%20image%20prompts%20from%20COCO%20to%20reflect%20real-world%20content%20diversity%3B%202%29%20Diverse%20diffusion%20models--local%20edits%20using%20eight%20SOTA%20diffusion%20models%3B%203%29%20Multi-turn%20editing--each%20image%20undergoes%20up%20to%20three%20sequential%20edits%20to%20mimic%20real-world%20sequential%20editing%3B%20and%204%29%20Realistic%20editing%20scenarios--a%20vision-language%20model%20%28VLM%29-based%20pipeline%20automatically%20identifies%20meaningful%20regions%20and%20generates%20context-aware%20prompts%20covering%20additions%2C%20removals%2C%20and%20attribute%20changes.%20DiffSeg30k%20shifts%20AIGC%20detection%20from%20binary%20classification%20to%20semantic%20segmentation%2C%20enabling%20simultaneous%20localization%20of%20edits%20and%20identification%20of%20the%20editing%20models.%20We%20benchmark%20three%20baseline%20segmentation%20approaches%2C%20revealing%20significant%20challenges%20in%20semantic%20segmentation%20tasks%2C%20particularly%20concerning%20robustness%20to%20image%20distortions.%20Experiments%20also%20reveal%20that%20segmentation%20models%2C%20despite%20being%20trained%20for%20pixel-level%20localization%2C%20emerge%20as%20highly%20reliable%20whole-image%20classifiers%20of%20diffusion%20edits%2C%20outperforming%20established%20forgery%20classifiers%20while%20showing%20great%20potential%20in%20cross-generator%20generalization.%20We%20believe%20DiffSeg30k%20will%20advance%20research%20in%20fine-grained%20localization%20of%20AI-generated%20content%20by%20demonstrating%20the%20promise%20and%20limitations%20of%20segmentation-based%20methods.%20DiffSeg30k%20is%20released%20at%3A%20https%3A//huggingface.co/datasets/Chaos2629/Diffseg30k%0ALink%3A%20http%3A//arxiv.org/abs/2511.19111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffSeg30k%253A%2520A%2520Multi-Turn%2520Diffusion%2520Editing%2520Benchmark%2520for%2520Localized%2520AIGC%2520Detection%26entry.906535625%3DHai%2520Ci%2520and%2520Ziheng%2520Peng%2520and%2520Pei%2520Yang%2520and%2520Yingxin%2520Xuan%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3DDiffusion-based%2520editing%2520enables%2520realistic%2520modification%2520of%2520local%2520image%2520regions%252C%2520making%2520AI-generated%2520content%2520harder%2520to%2520detect.%2520Existing%2520AIGC%2520detection%2520benchmarks%2520focus%2520on%2520classifying%2520entire%2520images%252C%2520overlooking%2520the%2520localization%2520of%2520diffusion-based%2520edits.%2520We%2520introduce%2520DiffSeg30k%252C%2520a%2520publicly%2520available%2520dataset%2520of%252030k%2520diffusion-edited%2520images%2520with%2520pixel-level%2520annotations%252C%2520designed%2520to%2520support%2520fine-grained%2520detection.%2520DiffSeg30k%2520features%253A%25201%2529%2520In-the-wild%2520images--we%2520collect%2520images%2520or%2520image%2520prompts%2520from%2520COCO%2520to%2520reflect%2520real-world%2520content%2520diversity%253B%25202%2529%2520Diverse%2520diffusion%2520models--local%2520edits%2520using%2520eight%2520SOTA%2520diffusion%2520models%253B%25203%2529%2520Multi-turn%2520editing--each%2520image%2520undergoes%2520up%2520to%2520three%2520sequential%2520edits%2520to%2520mimic%2520real-world%2520sequential%2520editing%253B%2520and%25204%2529%2520Realistic%2520editing%2520scenarios--a%2520vision-language%2520model%2520%2528VLM%2529-based%2520pipeline%2520automatically%2520identifies%2520meaningful%2520regions%2520and%2520generates%2520context-aware%2520prompts%2520covering%2520additions%252C%2520removals%252C%2520and%2520attribute%2520changes.%2520DiffSeg30k%2520shifts%2520AIGC%2520detection%2520from%2520binary%2520classification%2520to%2520semantic%2520segmentation%252C%2520enabling%2520simultaneous%2520localization%2520of%2520edits%2520and%2520identification%2520of%2520the%2520editing%2520models.%2520We%2520benchmark%2520three%2520baseline%2520segmentation%2520approaches%252C%2520revealing%2520significant%2520challenges%2520in%2520semantic%2520segmentation%2520tasks%252C%2520particularly%2520concerning%2520robustness%2520to%2520image%2520distortions.%2520Experiments%2520also%2520reveal%2520that%2520segmentation%2520models%252C%2520despite%2520being%2520trained%2520for%2520pixel-level%2520localization%252C%2520emerge%2520as%2520highly%2520reliable%2520whole-image%2520classifiers%2520of%2520diffusion%2520edits%252C%2520outperforming%2520established%2520forgery%2520classifiers%2520while%2520showing%2520great%2520potential%2520in%2520cross-generator%2520generalization.%2520We%2520believe%2520DiffSeg30k%2520will%2520advance%2520research%2520in%2520fine-grained%2520localization%2520of%2520AI-generated%2520content%2520by%2520demonstrating%2520the%2520promise%2520and%2520limitations%2520of%2520segmentation-based%2520methods.%2520DiffSeg30k%2520is%2520released%2520at%253A%2520https%253A//huggingface.co/datasets/Chaos2629/Diffseg30k%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.19111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffSeg30k%3A%20A%20Multi-Turn%20Diffusion%20Editing%20Benchmark%20for%20Localized%20AIGC%20Detection&entry.906535625=Hai%20Ci%20and%20Ziheng%20Peng%20and%20Pei%20Yang%20and%20Yingxin%20Xuan%20and%20Mike%20Zheng%20Shou&entry.1292438233=Diffusion-based%20editing%20enables%20realistic%20modification%20of%20local%20image%20regions%2C%20making%20AI-generated%20content%20harder%20to%20detect.%20Existing%20AIGC%20detection%20benchmarks%20focus%20on%20classifying%20entire%20images%2C%20overlooking%20the%20localization%20of%20diffusion-based%20edits.%20We%20introduce%20DiffSeg30k%2C%20a%20publicly%20available%20dataset%20of%2030k%20diffusion-edited%20images%20with%20pixel-level%20annotations%2C%20designed%20to%20support%20fine-grained%20detection.%20DiffSeg30k%20features%3A%201%29%20In-the-wild%20images--we%20collect%20images%20or%20image%20prompts%20from%20COCO%20to%20reflect%20real-world%20content%20diversity%3B%202%29%20Diverse%20diffusion%20models--local%20edits%20using%20eight%20SOTA%20diffusion%20models%3B%203%29%20Multi-turn%20editing--each%20image%20undergoes%20up%20to%20three%20sequential%20edits%20to%20mimic%20real-world%20sequential%20editing%3B%20and%204%29%20Realistic%20editing%20scenarios--a%20vision-language%20model%20%28VLM%29-based%20pipeline%20automatically%20identifies%20meaningful%20regions%20and%20generates%20context-aware%20prompts%20covering%20additions%2C%20removals%2C%20and%20attribute%20changes.%20DiffSeg30k%20shifts%20AIGC%20detection%20from%20binary%20classification%20to%20semantic%20segmentation%2C%20enabling%20simultaneous%20localization%20of%20edits%20and%20identification%20of%20the%20editing%20models.%20We%20benchmark%20three%20baseline%20segmentation%20approaches%2C%20revealing%20significant%20challenges%20in%20semantic%20segmentation%20tasks%2C%20particularly%20concerning%20robustness%20to%20image%20distortions.%20Experiments%20also%20reveal%20that%20segmentation%20models%2C%20despite%20being%20trained%20for%20pixel-level%20localization%2C%20emerge%20as%20highly%20reliable%20whole-image%20classifiers%20of%20diffusion%20edits%2C%20outperforming%20established%20forgery%20classifiers%20while%20showing%20great%20potential%20in%20cross-generator%20generalization.%20We%20believe%20DiffSeg30k%20will%20advance%20research%20in%20fine-grained%20localization%20of%20AI-generated%20content%20by%20demonstrating%20the%20promise%20and%20limitations%20of%20segmentation-based%20methods.%20DiffSeg30k%20is%20released%20at%3A%20https%3A//huggingface.co/datasets/Chaos2629/Diffseg30k&entry.1838667208=http%3A//arxiv.org/abs/2511.19111v1&entry.124074799=Read"},
{"title": "Word-level Annotation of GDPR Transparency Compliance in Privacy Policies using Large Language Models", "author": "Thomas Cory and Wolf Rieder and Julia Kr\u00e4mer and Philip Raschke and Patrick Herbke and Axel K\u00fcpper", "abstract": "Ensuring transparency of data practices related to personal information is a core requirement of the General Data Protection Regulation (GDPR). However, large-scale compliance assessment remains challenging due to the complexity and diversity of privacy policy language. Manual audits are labour-intensive and inconsistent, while current automated methods often lack the granularity required to capture nuanced transparency disclosures.\n  In this paper, we present a modular large language model (LLM)-based pipeline for fine-grained word-level annotation of privacy policies with respect to GDPR transparency requirements. Our approach integrates LLM-driven annotation with passage-level classification, retrieval-augmented generation, and a self-correction mechanism to deliver scalable, context-aware annotations across 21 GDPR-derived transparency requirements. To support empirical evaluation, we compile a corpus of 703,791 English-language privacy policies and generate a ground-truth sample of 200 manually annotated policies based on a comprehensive, GDPR-aligned annotation scheme.\n  We propose a two-tiered evaluation methodology capturing both passage-level classification and span-level annotation quality and conduct a comparative analysis of seven state-of-the-art LLMs on two annotation schemes, including the widely used OPP-115 dataset. The results of our evaluation show that decomposing the annotation task and integrating targeted retrieval and classification components significantly improve annotation accuracy, particularly for well-structured requirements. Our work provides new empirical resources and methodological foundations for advancing automated transparency compliance assessment at scale.", "link": "http://arxiv.org/abs/2503.10727v2", "date": "2025-11-24", "relevancy": 1.9035, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5225}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Word-level%20Annotation%20of%20GDPR%20Transparency%20Compliance%20in%20Privacy%20Policies%20using%20Large%20Language%20Models&body=Title%3A%20Word-level%20Annotation%20of%20GDPR%20Transparency%20Compliance%20in%20Privacy%20Policies%20using%20Large%20Language%20Models%0AAuthor%3A%20Thomas%20Cory%20and%20Wolf%20Rieder%20and%20Julia%20Kr%C3%A4mer%20and%20Philip%20Raschke%20and%20Patrick%20Herbke%20and%20Axel%20K%C3%BCpper%0AAbstract%3A%20Ensuring%20transparency%20of%20data%20practices%20related%20to%20personal%20information%20is%20a%20core%20requirement%20of%20the%20General%20Data%20Protection%20Regulation%20%28GDPR%29.%20However%2C%20large-scale%20compliance%20assessment%20remains%20challenging%20due%20to%20the%20complexity%20and%20diversity%20of%20privacy%20policy%20language.%20Manual%20audits%20are%20labour-intensive%20and%20inconsistent%2C%20while%20current%20automated%20methods%20often%20lack%20the%20granularity%20required%20to%20capture%20nuanced%20transparency%20disclosures.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20modular%20large%20language%20model%20%28LLM%29-based%20pipeline%20for%20fine-grained%20word-level%20annotation%20of%20privacy%20policies%20with%20respect%20to%20GDPR%20transparency%20requirements.%20Our%20approach%20integrates%20LLM-driven%20annotation%20with%20passage-level%20classification%2C%20retrieval-augmented%20generation%2C%20and%20a%20self-correction%20mechanism%20to%20deliver%20scalable%2C%20context-aware%20annotations%20across%2021%20GDPR-derived%20transparency%20requirements.%20To%20support%20empirical%20evaluation%2C%20we%20compile%20a%20corpus%20of%20703%2C791%20English-language%20privacy%20policies%20and%20generate%20a%20ground-truth%20sample%20of%20200%20manually%20annotated%20policies%20based%20on%20a%20comprehensive%2C%20GDPR-aligned%20annotation%20scheme.%0A%20%20We%20propose%20a%20two-tiered%20evaluation%20methodology%20capturing%20both%20passage-level%20classification%20and%20span-level%20annotation%20quality%20and%20conduct%20a%20comparative%20analysis%20of%20seven%20state-of-the-art%20LLMs%20on%20two%20annotation%20schemes%2C%20including%20the%20widely%20used%20OPP-115%20dataset.%20The%20results%20of%20our%20evaluation%20show%20that%20decomposing%20the%20annotation%20task%20and%20integrating%20targeted%20retrieval%20and%20classification%20components%20significantly%20improve%20annotation%20accuracy%2C%20particularly%20for%20well-structured%20requirements.%20Our%20work%20provides%20new%20empirical%20resources%20and%20methodological%20foundations%20for%20advancing%20automated%20transparency%20compliance%20assessment%20at%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2503.10727v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWord-level%2520Annotation%2520of%2520GDPR%2520Transparency%2520Compliance%2520in%2520Privacy%2520Policies%2520using%2520Large%2520Language%2520Models%26entry.906535625%3DThomas%2520Cory%2520and%2520Wolf%2520Rieder%2520and%2520Julia%2520Kr%25C3%25A4mer%2520and%2520Philip%2520Raschke%2520and%2520Patrick%2520Herbke%2520and%2520Axel%2520K%25C3%25BCpper%26entry.1292438233%3DEnsuring%2520transparency%2520of%2520data%2520practices%2520related%2520to%2520personal%2520information%2520is%2520a%2520core%2520requirement%2520of%2520the%2520General%2520Data%2520Protection%2520Regulation%2520%2528GDPR%2529.%2520However%252C%2520large-scale%2520compliance%2520assessment%2520remains%2520challenging%2520due%2520to%2520the%2520complexity%2520and%2520diversity%2520of%2520privacy%2520policy%2520language.%2520Manual%2520audits%2520are%2520labour-intensive%2520and%2520inconsistent%252C%2520while%2520current%2520automated%2520methods%2520often%2520lack%2520the%2520granularity%2520required%2520to%2520capture%2520nuanced%2520transparency%2520disclosures.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520modular%2520large%2520language%2520model%2520%2528LLM%2529-based%2520pipeline%2520for%2520fine-grained%2520word-level%2520annotation%2520of%2520privacy%2520policies%2520with%2520respect%2520to%2520GDPR%2520transparency%2520requirements.%2520Our%2520approach%2520integrates%2520LLM-driven%2520annotation%2520with%2520passage-level%2520classification%252C%2520retrieval-augmented%2520generation%252C%2520and%2520a%2520self-correction%2520mechanism%2520to%2520deliver%2520scalable%252C%2520context-aware%2520annotations%2520across%252021%2520GDPR-derived%2520transparency%2520requirements.%2520To%2520support%2520empirical%2520evaluation%252C%2520we%2520compile%2520a%2520corpus%2520of%2520703%252C791%2520English-language%2520privacy%2520policies%2520and%2520generate%2520a%2520ground-truth%2520sample%2520of%2520200%2520manually%2520annotated%2520policies%2520based%2520on%2520a%2520comprehensive%252C%2520GDPR-aligned%2520annotation%2520scheme.%250A%2520%2520We%2520propose%2520a%2520two-tiered%2520evaluation%2520methodology%2520capturing%2520both%2520passage-level%2520classification%2520and%2520span-level%2520annotation%2520quality%2520and%2520conduct%2520a%2520comparative%2520analysis%2520of%2520seven%2520state-of-the-art%2520LLMs%2520on%2520two%2520annotation%2520schemes%252C%2520including%2520the%2520widely%2520used%2520OPP-115%2520dataset.%2520The%2520results%2520of%2520our%2520evaluation%2520show%2520that%2520decomposing%2520the%2520annotation%2520task%2520and%2520integrating%2520targeted%2520retrieval%2520and%2520classification%2520components%2520significantly%2520improve%2520annotation%2520accuracy%252C%2520particularly%2520for%2520well-structured%2520requirements.%2520Our%2520work%2520provides%2520new%2520empirical%2520resources%2520and%2520methodological%2520foundations%2520for%2520advancing%2520automated%2520transparency%2520compliance%2520assessment%2520at%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10727v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Word-level%20Annotation%20of%20GDPR%20Transparency%20Compliance%20in%20Privacy%20Policies%20using%20Large%20Language%20Models&entry.906535625=Thomas%20Cory%20and%20Wolf%20Rieder%20and%20Julia%20Kr%C3%A4mer%20and%20Philip%20Raschke%20and%20Patrick%20Herbke%20and%20Axel%20K%C3%BCpper&entry.1292438233=Ensuring%20transparency%20of%20data%20practices%20related%20to%20personal%20information%20is%20a%20core%20requirement%20of%20the%20General%20Data%20Protection%20Regulation%20%28GDPR%29.%20However%2C%20large-scale%20compliance%20assessment%20remains%20challenging%20due%20to%20the%20complexity%20and%20diversity%20of%20privacy%20policy%20language.%20Manual%20audits%20are%20labour-intensive%20and%20inconsistent%2C%20while%20current%20automated%20methods%20often%20lack%20the%20granularity%20required%20to%20capture%20nuanced%20transparency%20disclosures.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20modular%20large%20language%20model%20%28LLM%29-based%20pipeline%20for%20fine-grained%20word-level%20annotation%20of%20privacy%20policies%20with%20respect%20to%20GDPR%20transparency%20requirements.%20Our%20approach%20integrates%20LLM-driven%20annotation%20with%20passage-level%20classification%2C%20retrieval-augmented%20generation%2C%20and%20a%20self-correction%20mechanism%20to%20deliver%20scalable%2C%20context-aware%20annotations%20across%2021%20GDPR-derived%20transparency%20requirements.%20To%20support%20empirical%20evaluation%2C%20we%20compile%20a%20corpus%20of%20703%2C791%20English-language%20privacy%20policies%20and%20generate%20a%20ground-truth%20sample%20of%20200%20manually%20annotated%20policies%20based%20on%20a%20comprehensive%2C%20GDPR-aligned%20annotation%20scheme.%0A%20%20We%20propose%20a%20two-tiered%20evaluation%20methodology%20capturing%20both%20passage-level%20classification%20and%20span-level%20annotation%20quality%20and%20conduct%20a%20comparative%20analysis%20of%20seven%20state-of-the-art%20LLMs%20on%20two%20annotation%20schemes%2C%20including%20the%20widely%20used%20OPP-115%20dataset.%20The%20results%20of%20our%20evaluation%20show%20that%20decomposing%20the%20annotation%20task%20and%20integrating%20targeted%20retrieval%20and%20classification%20components%20significantly%20improve%20annotation%20accuracy%2C%20particularly%20for%20well-structured%20requirements.%20Our%20work%20provides%20new%20empirical%20resources%20and%20methodological%20foundations%20for%20advancing%20automated%20transparency%20compliance%20assessment%20at%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2503.10727v2&entry.124074799=Read"},
{"title": "Fairness in Multi-modal Medical Diagnosis with Demonstration Selection", "author": "Dawei Li and Zijian Gu and Peng Wang and Chuhan Song and Zhen Tan and Mohan Zhang and Tianlong Chen and Yu Tian and Song Wang", "abstract": "Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.", "link": "http://arxiv.org/abs/2511.15986v2", "date": "2025-11-24", "relevancy": 1.5857, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5522}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5351}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness%20in%20Multi-modal%20Medical%20Diagnosis%20with%20Demonstration%20Selection&body=Title%3A%20Fairness%20in%20Multi-modal%20Medical%20Diagnosis%20with%20Demonstration%20Selection%0AAuthor%3A%20Dawei%20Li%20and%20Zijian%20Gu%20and%20Peng%20Wang%20and%20Chuhan%20Song%20and%20Zhen%20Tan%20and%20Mohan%20Zhang%20and%20Tianlong%20Chen%20and%20Yu%20Tian%20and%20Song%20Wang%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%20strong%20potential%20for%20medical%20image%20reasoning%2C%20yet%20fairness%20across%20demographic%20groups%20remains%20a%20major%20concern.%20Existing%20debiasing%20methods%20often%20rely%20on%20large%20labeled%20datasets%20or%20fine-tuning%2C%20which%20are%20impractical%20for%20foundation-scale%20models.%20We%20explore%20In-Context%20Learning%20%28ICL%29%20as%20a%20lightweight%2C%20tuning-free%20alternative%20for%20improving%20fairness.%20Through%20systematic%20analysis%2C%20we%20find%20that%20conventional%20demonstration%20selection%20%28DS%29%20strategies%20fail%20to%20ensure%20fairness%20due%20to%20demographic%20imbalance%20in%20selected%20exemplars.%20To%20address%20this%2C%20we%20propose%20Fairness-Aware%20Demonstration%20Selection%20%28FADS%29%2C%20which%20builds%20demographically%20balanced%20and%20semantically%20relevant%20demonstrations%20via%20clustering-based%20sampling.%20Experiments%20on%20multiple%20medical%20imaging%20benchmarks%20show%20that%20FADS%20consistently%20reduces%20gender-%2C%20race-%2C%20and%20ethnicity-related%20disparities%20while%20maintaining%20strong%20accuracy%2C%20offering%20an%20efficient%20and%20scalable%20path%20toward%20fair%20medical%20image%20reasoning.%20These%20results%20highlight%20the%20potential%20of%20fairness-aware%20in-context%20learning%20as%20a%20scalable%20and%20data-efficient%20solution%20for%20equitable%20medical%20image%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15986v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness%2520in%2520Multi-modal%2520Medical%2520Diagnosis%2520with%2520Demonstration%2520Selection%26entry.906535625%3DDawei%2520Li%2520and%2520Zijian%2520Gu%2520and%2520Peng%2520Wang%2520and%2520Chuhan%2520Song%2520and%2520Zhen%2520Tan%2520and%2520Mohan%2520Zhang%2520and%2520Tianlong%2520Chen%2520and%2520Yu%2520Tian%2520and%2520Song%2520Wang%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520shown%2520strong%2520potential%2520for%2520medical%2520image%2520reasoning%252C%2520yet%2520fairness%2520across%2520demographic%2520groups%2520remains%2520a%2520major%2520concern.%2520Existing%2520debiasing%2520methods%2520often%2520rely%2520on%2520large%2520labeled%2520datasets%2520or%2520fine-tuning%252C%2520which%2520are%2520impractical%2520for%2520foundation-scale%2520models.%2520We%2520explore%2520In-Context%2520Learning%2520%2528ICL%2529%2520as%2520a%2520lightweight%252C%2520tuning-free%2520alternative%2520for%2520improving%2520fairness.%2520Through%2520systematic%2520analysis%252C%2520we%2520find%2520that%2520conventional%2520demonstration%2520selection%2520%2528DS%2529%2520strategies%2520fail%2520to%2520ensure%2520fairness%2520due%2520to%2520demographic%2520imbalance%2520in%2520selected%2520exemplars.%2520To%2520address%2520this%252C%2520we%2520propose%2520Fairness-Aware%2520Demonstration%2520Selection%2520%2528FADS%2529%252C%2520which%2520builds%2520demographically%2520balanced%2520and%2520semantically%2520relevant%2520demonstrations%2520via%2520clustering-based%2520sampling.%2520Experiments%2520on%2520multiple%2520medical%2520imaging%2520benchmarks%2520show%2520that%2520FADS%2520consistently%2520reduces%2520gender-%252C%2520race-%252C%2520and%2520ethnicity-related%2520disparities%2520while%2520maintaining%2520strong%2520accuracy%252C%2520offering%2520an%2520efficient%2520and%2520scalable%2520path%2520toward%2520fair%2520medical%2520image%2520reasoning.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520fairness-aware%2520in-context%2520learning%2520as%2520a%2520scalable%2520and%2520data-efficient%2520solution%2520for%2520equitable%2520medical%2520image%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15986v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness%20in%20Multi-modal%20Medical%20Diagnosis%20with%20Demonstration%20Selection&entry.906535625=Dawei%20Li%20and%20Zijian%20Gu%20and%20Peng%20Wang%20and%20Chuhan%20Song%20and%20Zhen%20Tan%20and%20Mohan%20Zhang%20and%20Tianlong%20Chen%20and%20Yu%20Tian%20and%20Song%20Wang&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%20strong%20potential%20for%20medical%20image%20reasoning%2C%20yet%20fairness%20across%20demographic%20groups%20remains%20a%20major%20concern.%20Existing%20debiasing%20methods%20often%20rely%20on%20large%20labeled%20datasets%20or%20fine-tuning%2C%20which%20are%20impractical%20for%20foundation-scale%20models.%20We%20explore%20In-Context%20Learning%20%28ICL%29%20as%20a%20lightweight%2C%20tuning-free%20alternative%20for%20improving%20fairness.%20Through%20systematic%20analysis%2C%20we%20find%20that%20conventional%20demonstration%20selection%20%28DS%29%20strategies%20fail%20to%20ensure%20fairness%20due%20to%20demographic%20imbalance%20in%20selected%20exemplars.%20To%20address%20this%2C%20we%20propose%20Fairness-Aware%20Demonstration%20Selection%20%28FADS%29%2C%20which%20builds%20demographically%20balanced%20and%20semantically%20relevant%20demonstrations%20via%20clustering-based%20sampling.%20Experiments%20on%20multiple%20medical%20imaging%20benchmarks%20show%20that%20FADS%20consistently%20reduces%20gender-%2C%20race-%2C%20and%20ethnicity-related%20disparities%20while%20maintaining%20strong%20accuracy%2C%20offering%20an%20efficient%20and%20scalable%20path%20toward%20fair%20medical%20image%20reasoning.%20These%20results%20highlight%20the%20potential%20of%20fairness-aware%20in-context%20learning%20as%20a%20scalable%20and%20data-efficient%20solution%20for%20equitable%20medical%20image%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2511.15986v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


