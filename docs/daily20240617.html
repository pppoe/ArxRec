<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240616.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "D-NPC: Dynamic Neural Point Clouds for Non-Rigid View Synthesis from\n  Monocular Video", "author": "Moritz Kappel and Florian Hahlbohm and Timon Scholz and Susana Castillo and Christian Theobalt and Martin Eisemann and Vladislav Golyanik and Marcus Magnor", "abstract": "  Dynamic reconstruction and spatiotemporal novel-view synthesis of non-rigidly\ndeforming scenes recently gained increased attention. While existing work\nachieves impressive quality and performance on multi-view or teleporting camera\nsetups, most methods fail to efficiently and faithfully recover motion and\nappearance from casual monocular captures. This paper contributes to the field\nby introducing a new method for dynamic novel view synthesis from monocular\nvideo, such as casual smartphone captures.\n  Our approach represents the scene as a $\\textit{dynamic neural point cloud}$,\nan implicit time-conditioned point distribution that encodes local geometry and\nappearance in separate hash-encoded neural feature grids for static and dynamic\nregions. By sampling a discrete point cloud from our model, we can efficiently\nrender high-quality novel views using a fast differentiable rasterizer and\nneural rendering network. Similar to recent work, we leverage advances in\nneural scene analysis by incorporating data-driven priors like monocular depth\nestimation and object segmentation to resolve motion and depth ambiguities\noriginating from the monocular captures. In addition to guiding the\noptimization process, we show that these priors can be exploited to explicitly\ninitialize our scene representation to drastically improve optimization speed\nand final image quality. As evidenced by our experimental evaluation, our\ndynamic point cloud model not only enables fast optimization and real-time\nframe rates for interactive applications, but also achieves competitive image\nquality on monocular benchmark sequences.\n  Our project page is available at\nhttps://moritzkappel.github.io/projects/dnpc.\n", "link": "http://arxiv.org/abs/2406.10078v1", "date": "2024-06-14", "relevancy": 3.2683, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7004}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6303}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D-NPC%3A%20Dynamic%20Neural%20Point%20Clouds%20for%20Non-Rigid%20View%20Synthesis%20from%0A%20%20Monocular%20Video&body=Title%3A%20D-NPC%3A%20Dynamic%20Neural%20Point%20Clouds%20for%20Non-Rigid%20View%20Synthesis%20from%0A%20%20Monocular%20Video%0AAuthor%3A%20Moritz%20Kappel%20and%20Florian%20Hahlbohm%20and%20Timon%20Scholz%20and%20Susana%20Castillo%20and%20Christian%20Theobalt%20and%20Martin%20Eisemann%20and%20Vladislav%20Golyanik%20and%20Marcus%20Magnor%0AAbstract%3A%20%20%20Dynamic%20reconstruction%20and%20spatiotemporal%20novel-view%20synthesis%20of%20non-rigidly%0Adeforming%20scenes%20recently%20gained%20increased%20attention.%20While%20existing%20work%0Aachieves%20impressive%20quality%20and%20performance%20on%20multi-view%20or%20teleporting%20camera%0Asetups%2C%20most%20methods%20fail%20to%20efficiently%20and%20faithfully%20recover%20motion%20and%0Aappearance%20from%20casual%20monocular%20captures.%20This%20paper%20contributes%20to%20the%20field%0Aby%20introducing%20a%20new%20method%20for%20dynamic%20novel%20view%20synthesis%20from%20monocular%0Avideo%2C%20such%20as%20casual%20smartphone%20captures.%0A%20%20Our%20approach%20represents%20the%20scene%20as%20a%20%24%5Ctextit%7Bdynamic%20neural%20point%20cloud%7D%24%2C%0Aan%20implicit%20time-conditioned%20point%20distribution%20that%20encodes%20local%20geometry%20and%0Aappearance%20in%20separate%20hash-encoded%20neural%20feature%20grids%20for%20static%20and%20dynamic%0Aregions.%20By%20sampling%20a%20discrete%20point%20cloud%20from%20our%20model%2C%20we%20can%20efficiently%0Arender%20high-quality%20novel%20views%20using%20a%20fast%20differentiable%20rasterizer%20and%0Aneural%20rendering%20network.%20Similar%20to%20recent%20work%2C%20we%20leverage%20advances%20in%0Aneural%20scene%20analysis%20by%20incorporating%20data-driven%20priors%20like%20monocular%20depth%0Aestimation%20and%20object%20segmentation%20to%20resolve%20motion%20and%20depth%20ambiguities%0Aoriginating%20from%20the%20monocular%20captures.%20In%20addition%20to%20guiding%20the%0Aoptimization%20process%2C%20we%20show%20that%20these%20priors%20can%20be%20exploited%20to%20explicitly%0Ainitialize%20our%20scene%20representation%20to%20drastically%20improve%20optimization%20speed%0Aand%20final%20image%20quality.%20As%20evidenced%20by%20our%20experimental%20evaluation%2C%20our%0Adynamic%20point%20cloud%20model%20not%20only%20enables%20fast%20optimization%20and%20real-time%0Aframe%20rates%20for%20interactive%20applications%2C%20but%20also%20achieves%20competitive%20image%0Aquality%20on%20monocular%20benchmark%20sequences.%0A%20%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//moritzkappel.github.io/projects/dnpc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD-NPC%253A%2520Dynamic%2520Neural%2520Point%2520Clouds%2520for%2520Non-Rigid%2520View%2520Synthesis%2520from%250A%2520%2520Monocular%2520Video%26entry.906535625%3DMoritz%2520Kappel%2520and%2520Florian%2520Hahlbohm%2520and%2520Timon%2520Scholz%2520and%2520Susana%2520Castillo%2520and%2520Christian%2520Theobalt%2520and%2520Martin%2520Eisemann%2520and%2520Vladislav%2520Golyanik%2520and%2520Marcus%2520Magnor%26entry.1292438233%3D%2520%2520Dynamic%2520reconstruction%2520and%2520spatiotemporal%2520novel-view%2520synthesis%2520of%2520non-rigidly%250Adeforming%2520scenes%2520recently%2520gained%2520increased%2520attention.%2520While%2520existing%2520work%250Aachieves%2520impressive%2520quality%2520and%2520performance%2520on%2520multi-view%2520or%2520teleporting%2520camera%250Asetups%252C%2520most%2520methods%2520fail%2520to%2520efficiently%2520and%2520faithfully%2520recover%2520motion%2520and%250Aappearance%2520from%2520casual%2520monocular%2520captures.%2520This%2520paper%2520contributes%2520to%2520the%2520field%250Aby%2520introducing%2520a%2520new%2520method%2520for%2520dynamic%2520novel%2520view%2520synthesis%2520from%2520monocular%250Avideo%252C%2520such%2520as%2520casual%2520smartphone%2520captures.%250A%2520%2520Our%2520approach%2520represents%2520the%2520scene%2520as%2520a%2520%2524%255Ctextit%257Bdynamic%2520neural%2520point%2520cloud%257D%2524%252C%250Aan%2520implicit%2520time-conditioned%2520point%2520distribution%2520that%2520encodes%2520local%2520geometry%2520and%250Aappearance%2520in%2520separate%2520hash-encoded%2520neural%2520feature%2520grids%2520for%2520static%2520and%2520dynamic%250Aregions.%2520By%2520sampling%2520a%2520discrete%2520point%2520cloud%2520from%2520our%2520model%252C%2520we%2520can%2520efficiently%250Arender%2520high-quality%2520novel%2520views%2520using%2520a%2520fast%2520differentiable%2520rasterizer%2520and%250Aneural%2520rendering%2520network.%2520Similar%2520to%2520recent%2520work%252C%2520we%2520leverage%2520advances%2520in%250Aneural%2520scene%2520analysis%2520by%2520incorporating%2520data-driven%2520priors%2520like%2520monocular%2520depth%250Aestimation%2520and%2520object%2520segmentation%2520to%2520resolve%2520motion%2520and%2520depth%2520ambiguities%250Aoriginating%2520from%2520the%2520monocular%2520captures.%2520In%2520addition%2520to%2520guiding%2520the%250Aoptimization%2520process%252C%2520we%2520show%2520that%2520these%2520priors%2520can%2520be%2520exploited%2520to%2520explicitly%250Ainitialize%2520our%2520scene%2520representation%2520to%2520drastically%2520improve%2520optimization%2520speed%250Aand%2520final%2520image%2520quality.%2520As%2520evidenced%2520by%2520our%2520experimental%2520evaluation%252C%2520our%250Adynamic%2520point%2520cloud%2520model%2520not%2520only%2520enables%2520fast%2520optimization%2520and%2520real-time%250Aframe%2520rates%2520for%2520interactive%2520applications%252C%2520but%2520also%2520achieves%2520competitive%2520image%250Aquality%2520on%2520monocular%2520benchmark%2520sequences.%250A%2520%2520Our%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//moritzkappel.github.io/projects/dnpc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D-NPC%3A%20Dynamic%20Neural%20Point%20Clouds%20for%20Non-Rigid%20View%20Synthesis%20from%0A%20%20Monocular%20Video&entry.906535625=Moritz%20Kappel%20and%20Florian%20Hahlbohm%20and%20Timon%20Scholz%20and%20Susana%20Castillo%20and%20Christian%20Theobalt%20and%20Martin%20Eisemann%20and%20Vladislav%20Golyanik%20and%20Marcus%20Magnor&entry.1292438233=%20%20Dynamic%20reconstruction%20and%20spatiotemporal%20novel-view%20synthesis%20of%20non-rigidly%0Adeforming%20scenes%20recently%20gained%20increased%20attention.%20While%20existing%20work%0Aachieves%20impressive%20quality%20and%20performance%20on%20multi-view%20or%20teleporting%20camera%0Asetups%2C%20most%20methods%20fail%20to%20efficiently%20and%20faithfully%20recover%20motion%20and%0Aappearance%20from%20casual%20monocular%20captures.%20This%20paper%20contributes%20to%20the%20field%0Aby%20introducing%20a%20new%20method%20for%20dynamic%20novel%20view%20synthesis%20from%20monocular%0Avideo%2C%20such%20as%20casual%20smartphone%20captures.%0A%20%20Our%20approach%20represents%20the%20scene%20as%20a%20%24%5Ctextit%7Bdynamic%20neural%20point%20cloud%7D%24%2C%0Aan%20implicit%20time-conditioned%20point%20distribution%20that%20encodes%20local%20geometry%20and%0Aappearance%20in%20separate%20hash-encoded%20neural%20feature%20grids%20for%20static%20and%20dynamic%0Aregions.%20By%20sampling%20a%20discrete%20point%20cloud%20from%20our%20model%2C%20we%20can%20efficiently%0Arender%20high-quality%20novel%20views%20using%20a%20fast%20differentiable%20rasterizer%20and%0Aneural%20rendering%20network.%20Similar%20to%20recent%20work%2C%20we%20leverage%20advances%20in%0Aneural%20scene%20analysis%20by%20incorporating%20data-driven%20priors%20like%20monocular%20depth%0Aestimation%20and%20object%20segmentation%20to%20resolve%20motion%20and%20depth%20ambiguities%0Aoriginating%20from%20the%20monocular%20captures.%20In%20addition%20to%20guiding%20the%0Aoptimization%20process%2C%20we%20show%20that%20these%20priors%20can%20be%20exploited%20to%20explicitly%0Ainitialize%20our%20scene%20representation%20to%20drastically%20improve%20optimization%20speed%0Aand%20final%20image%20quality.%20As%20evidenced%20by%20our%20experimental%20evaluation%2C%20our%0Adynamic%20point%20cloud%20model%20not%20only%20enables%20fast%20optimization%20and%20real-time%0Aframe%20rates%20for%20interactive%20applications%2C%20but%20also%20achieves%20competitive%20image%0Aquality%20on%20monocular%20benchmark%20sequences.%0A%20%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//moritzkappel.github.io/projects/dnpc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10078v1&entry.124074799=Read"},
{"title": "GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors", "author": "Xiqian Yu and Hanxin Zhu and Tianyu He and Zhibo Chen", "abstract": "  Achieving high-resolution novel view synthesis (HRNVS) from low-resolution\ninput views is a challenging task due to the lack of high-resolution data.\nPrevious methods optimize high-resolution Neural Radiance Field (NeRF) from\nlow-resolution input views but suffer from slow rendering speed. In this work,\nwe base our method on 3D Gaussian Splatting (3DGS) due to its capability of\nproducing high-quality images at a faster rendering speed. To alleviate the\nshortage of data for higher-resolution synthesis, we propose to leverage\noff-the-shelf 2D diffusion priors by distilling the 2D knowledge into 3D with\nScore Distillation Sampling (SDS). Nevertheless, applying SDS directly to\nGaussian-based 3D super-resolution leads to undesirable and redundant 3D\nGaussian primitives, due to the randomness brought by generative priors. To\nmitigate this issue, we introduce two simple yet effective techniques to reduce\nstochastic disturbances introduced by SDS. Specifically, we 1) shrink the range\nof diffusion timestep in SDS with an annealing strategy; 2) randomly discard\nredundant Gaussian primitives during densification. Extensive experiments have\ndemonstrated that our proposed GaussainSR can attain high-quality results for\nHRNVS with only low-resolution inputs on both synthetic and real-world\ndatasets. Project page: https://chchnii.github.io/GaussianSR/\n", "link": "http://arxiv.org/abs/2406.10111v1", "date": "2024-06-14", "relevancy": 3.1633, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7046}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.614}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianSR%3A%203D%20Gaussian%20Super-Resolution%20with%202D%20Diffusion%20Priors&body=Title%3A%20GaussianSR%3A%203D%20Gaussian%20Super-Resolution%20with%202D%20Diffusion%20Priors%0AAuthor%3A%20Xiqian%20Yu%20and%20Hanxin%20Zhu%20and%20Tianyu%20He%20and%20Zhibo%20Chen%0AAbstract%3A%20%20%20Achieving%20high-resolution%20novel%20view%20synthesis%20%28HRNVS%29%20from%20low-resolution%0Ainput%20views%20is%20a%20challenging%20task%20due%20to%20the%20lack%20of%20high-resolution%20data.%0APrevious%20methods%20optimize%20high-resolution%20Neural%20Radiance%20Field%20%28NeRF%29%20from%0Alow-resolution%20input%20views%20but%20suffer%20from%20slow%20rendering%20speed.%20In%20this%20work%2C%0Awe%20base%20our%20method%20on%203D%20Gaussian%20Splatting%20%283DGS%29%20due%20to%20its%20capability%20of%0Aproducing%20high-quality%20images%20at%20a%20faster%20rendering%20speed.%20To%20alleviate%20the%0Ashortage%20of%20data%20for%20higher-resolution%20synthesis%2C%20we%20propose%20to%20leverage%0Aoff-the-shelf%202D%20diffusion%20priors%20by%20distilling%20the%202D%20knowledge%20into%203D%20with%0AScore%20Distillation%20Sampling%20%28SDS%29.%20Nevertheless%2C%20applying%20SDS%20directly%20to%0AGaussian-based%203D%20super-resolution%20leads%20to%20undesirable%20and%20redundant%203D%0AGaussian%20primitives%2C%20due%20to%20the%20randomness%20brought%20by%20generative%20priors.%20To%0Amitigate%20this%20issue%2C%20we%20introduce%20two%20simple%20yet%20effective%20techniques%20to%20reduce%0Astochastic%20disturbances%20introduced%20by%20SDS.%20Specifically%2C%20we%201%29%20shrink%20the%20range%0Aof%20diffusion%20timestep%20in%20SDS%20with%20an%20annealing%20strategy%3B%202%29%20randomly%20discard%0Aredundant%20Gaussian%20primitives%20during%20densification.%20Extensive%20experiments%20have%0Ademonstrated%20that%20our%20proposed%20GaussainSR%20can%20attain%20high-quality%20results%20for%0AHRNVS%20with%20only%20low-resolution%20inputs%20on%20both%20synthetic%20and%20real-world%0Adatasets.%20Project%20page%3A%20https%3A//chchnii.github.io/GaussianSR/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianSR%253A%25203D%2520Gaussian%2520Super-Resolution%2520with%25202D%2520Diffusion%2520Priors%26entry.906535625%3DXiqian%2520Yu%2520and%2520Hanxin%2520Zhu%2520and%2520Tianyu%2520He%2520and%2520Zhibo%2520Chen%26entry.1292438233%3D%2520%2520Achieving%2520high-resolution%2520novel%2520view%2520synthesis%2520%2528HRNVS%2529%2520from%2520low-resolution%250Ainput%2520views%2520is%2520a%2520challenging%2520task%2520due%2520to%2520the%2520lack%2520of%2520high-resolution%2520data.%250APrevious%2520methods%2520optimize%2520high-resolution%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%2520from%250Alow-resolution%2520input%2520views%2520but%2520suffer%2520from%2520slow%2520rendering%2520speed.%2520In%2520this%2520work%252C%250Awe%2520base%2520our%2520method%2520on%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520due%2520to%2520its%2520capability%2520of%250Aproducing%2520high-quality%2520images%2520at%2520a%2520faster%2520rendering%2520speed.%2520To%2520alleviate%2520the%250Ashortage%2520of%2520data%2520for%2520higher-resolution%2520synthesis%252C%2520we%2520propose%2520to%2520leverage%250Aoff-the-shelf%25202D%2520diffusion%2520priors%2520by%2520distilling%2520the%25202D%2520knowledge%2520into%25203D%2520with%250AScore%2520Distillation%2520Sampling%2520%2528SDS%2529.%2520Nevertheless%252C%2520applying%2520SDS%2520directly%2520to%250AGaussian-based%25203D%2520super-resolution%2520leads%2520to%2520undesirable%2520and%2520redundant%25203D%250AGaussian%2520primitives%252C%2520due%2520to%2520the%2520randomness%2520brought%2520by%2520generative%2520priors.%2520To%250Amitigate%2520this%2520issue%252C%2520we%2520introduce%2520two%2520simple%2520yet%2520effective%2520techniques%2520to%2520reduce%250Astochastic%2520disturbances%2520introduced%2520by%2520SDS.%2520Specifically%252C%2520we%25201%2529%2520shrink%2520the%2520range%250Aof%2520diffusion%2520timestep%2520in%2520SDS%2520with%2520an%2520annealing%2520strategy%253B%25202%2529%2520randomly%2520discard%250Aredundant%2520Gaussian%2520primitives%2520during%2520densification.%2520Extensive%2520experiments%2520have%250Ademonstrated%2520that%2520our%2520proposed%2520GaussainSR%2520can%2520attain%2520high-quality%2520results%2520for%250AHRNVS%2520with%2520only%2520low-resolution%2520inputs%2520on%2520both%2520synthetic%2520and%2520real-world%250Adatasets.%2520Project%2520page%253A%2520https%253A//chchnii.github.io/GaussianSR/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianSR%3A%203D%20Gaussian%20Super-Resolution%20with%202D%20Diffusion%20Priors&entry.906535625=Xiqian%20Yu%20and%20Hanxin%20Zhu%20and%20Tianyu%20He%20and%20Zhibo%20Chen&entry.1292438233=%20%20Achieving%20high-resolution%20novel%20view%20synthesis%20%28HRNVS%29%20from%20low-resolution%0Ainput%20views%20is%20a%20challenging%20task%20due%20to%20the%20lack%20of%20high-resolution%20data.%0APrevious%20methods%20optimize%20high-resolution%20Neural%20Radiance%20Field%20%28NeRF%29%20from%0Alow-resolution%20input%20views%20but%20suffer%20from%20slow%20rendering%20speed.%20In%20this%20work%2C%0Awe%20base%20our%20method%20on%203D%20Gaussian%20Splatting%20%283DGS%29%20due%20to%20its%20capability%20of%0Aproducing%20high-quality%20images%20at%20a%20faster%20rendering%20speed.%20To%20alleviate%20the%0Ashortage%20of%20data%20for%20higher-resolution%20synthesis%2C%20we%20propose%20to%20leverage%0Aoff-the-shelf%202D%20diffusion%20priors%20by%20distilling%20the%202D%20knowledge%20into%203D%20with%0AScore%20Distillation%20Sampling%20%28SDS%29.%20Nevertheless%2C%20applying%20SDS%20directly%20to%0AGaussian-based%203D%20super-resolution%20leads%20to%20undesirable%20and%20redundant%203D%0AGaussian%20primitives%2C%20due%20to%20the%20randomness%20brought%20by%20generative%20priors.%20To%0Amitigate%20this%20issue%2C%20we%20introduce%20two%20simple%20yet%20effective%20techniques%20to%20reduce%0Astochastic%20disturbances%20introduced%20by%20SDS.%20Specifically%2C%20we%201%29%20shrink%20the%20range%0Aof%20diffusion%20timestep%20in%20SDS%20with%20an%20annealing%20strategy%3B%202%29%20randomly%20discard%0Aredundant%20Gaussian%20primitives%20during%20densification.%20Extensive%20experiments%20have%0Ademonstrated%20that%20our%20proposed%20GaussainSR%20can%20attain%20high-quality%20results%20for%0AHRNVS%20with%20only%20low-resolution%20inputs%20on%20both%20synthetic%20and%20real-world%0Adatasets.%20Project%20page%3A%20https%3A//chchnii.github.io/GaussianSR/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10111v1&entry.124074799=Read"},
{"title": "4DRecons: 4D Neural Implicit Deformable Objects Reconstruction from a\n  single RGB-D Camera with Geometrical and Topological Regularizations", "author": "Xiaoyan Cong and Haitao Yang and Liyan Chen and Kaifeng Zhang and Li Yi and Chandrajit Bajaj and Qixing Huang", "abstract": "  This paper presents a novel approach 4DRecons that takes a single camera\nRGB-D sequence of a dynamic subject as input and outputs a complete textured\ndeforming 3D model over time. 4DRecons encodes the output as a 4D neural\nimplicit surface and presents an optimization procedure that combines a data\nterm and two regularization terms. The data term fits the 4D implicit surface\nto the input partial observations. We address fundamental challenges in fitting\na complete implicit surface to partial observations. The first regularization\nterm enforces that the deformation among adjacent frames is as rigid as\npossible (ARAP). To this end, we introduce a novel approach to compute\ncorrespondences between adjacent textured implicit surfaces, which are used to\ndefine the ARAP regularization term. The second regularization term enforces\nthat the topology of the underlying object remains fixed over time. This\nregularization is critical for avoiding self-intersections that are typical in\nimplicit-based reconstructions. We have evaluated the performance of 4DRecons\non a variety of datasets. Experimental results show that 4DRecons can handle\nlarge deformations and complex inter-part interactions and outperform\nstate-of-the-art approaches considerably.\n", "link": "http://arxiv.org/abs/2406.10167v1", "date": "2024-06-14", "relevancy": 3.1581, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6512}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6332}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204DRecons%3A%204D%20Neural%20Implicit%20Deformable%20Objects%20Reconstruction%20from%20a%0A%20%20single%20RGB-D%20Camera%20with%20Geometrical%20and%20Topological%20Regularizations&body=Title%3A%204DRecons%3A%204D%20Neural%20Implicit%20Deformable%20Objects%20Reconstruction%20from%20a%0A%20%20single%20RGB-D%20Camera%20with%20Geometrical%20and%20Topological%20Regularizations%0AAuthor%3A%20Xiaoyan%20Cong%20and%20Haitao%20Yang%20and%20Liyan%20Chen%20and%20Kaifeng%20Zhang%20and%20Li%20Yi%20and%20Chandrajit%20Bajaj%20and%20Qixing%20Huang%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%204DRecons%20that%20takes%20a%20single%20camera%0ARGB-D%20sequence%20of%20a%20dynamic%20subject%20as%20input%20and%20outputs%20a%20complete%20textured%0Adeforming%203D%20model%20over%20time.%204DRecons%20encodes%20the%20output%20as%20a%204D%20neural%0Aimplicit%20surface%20and%20presents%20an%20optimization%20procedure%20that%20combines%20a%20data%0Aterm%20and%20two%20regularization%20terms.%20The%20data%20term%20fits%20the%204D%20implicit%20surface%0Ato%20the%20input%20partial%20observations.%20We%20address%20fundamental%20challenges%20in%20fitting%0Aa%20complete%20implicit%20surface%20to%20partial%20observations.%20The%20first%20regularization%0Aterm%20enforces%20that%20the%20deformation%20among%20adjacent%20frames%20is%20as%20rigid%20as%0Apossible%20%28ARAP%29.%20To%20this%20end%2C%20we%20introduce%20a%20novel%20approach%20to%20compute%0Acorrespondences%20between%20adjacent%20textured%20implicit%20surfaces%2C%20which%20are%20used%20to%0Adefine%20the%20ARAP%20regularization%20term.%20The%20second%20regularization%20term%20enforces%0Athat%20the%20topology%20of%20the%20underlying%20object%20remains%20fixed%20over%20time.%20This%0Aregularization%20is%20critical%20for%20avoiding%20self-intersections%20that%20are%20typical%20in%0Aimplicit-based%20reconstructions.%20We%20have%20evaluated%20the%20performance%20of%204DRecons%0Aon%20a%20variety%20of%20datasets.%20Experimental%20results%20show%20that%204DRecons%20can%20handle%0Alarge%20deformations%20and%20complex%20inter-part%20interactions%20and%20outperform%0Astate-of-the-art%20approaches%20considerably.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4DRecons%253A%25204D%2520Neural%2520Implicit%2520Deformable%2520Objects%2520Reconstruction%2520from%2520a%250A%2520%2520single%2520RGB-D%2520Camera%2520with%2520Geometrical%2520and%2520Topological%2520Regularizations%26entry.906535625%3DXiaoyan%2520Cong%2520and%2520Haitao%2520Yang%2520and%2520Liyan%2520Chen%2520and%2520Kaifeng%2520Zhang%2520and%2520Li%2520Yi%2520and%2520Chandrajit%2520Bajaj%2520and%2520Qixing%2520Huang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%25204DRecons%2520that%2520takes%2520a%2520single%2520camera%250ARGB-D%2520sequence%2520of%2520a%2520dynamic%2520subject%2520as%2520input%2520and%2520outputs%2520a%2520complete%2520textured%250Adeforming%25203D%2520model%2520over%2520time.%25204DRecons%2520encodes%2520the%2520output%2520as%2520a%25204D%2520neural%250Aimplicit%2520surface%2520and%2520presents%2520an%2520optimization%2520procedure%2520that%2520combines%2520a%2520data%250Aterm%2520and%2520two%2520regularization%2520terms.%2520The%2520data%2520term%2520fits%2520the%25204D%2520implicit%2520surface%250Ato%2520the%2520input%2520partial%2520observations.%2520We%2520address%2520fundamental%2520challenges%2520in%2520fitting%250Aa%2520complete%2520implicit%2520surface%2520to%2520partial%2520observations.%2520The%2520first%2520regularization%250Aterm%2520enforces%2520that%2520the%2520deformation%2520among%2520adjacent%2520frames%2520is%2520as%2520rigid%2520as%250Apossible%2520%2528ARAP%2529.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520to%2520compute%250Acorrespondences%2520between%2520adjacent%2520textured%2520implicit%2520surfaces%252C%2520which%2520are%2520used%2520to%250Adefine%2520the%2520ARAP%2520regularization%2520term.%2520The%2520second%2520regularization%2520term%2520enforces%250Athat%2520the%2520topology%2520of%2520the%2520underlying%2520object%2520remains%2520fixed%2520over%2520time.%2520This%250Aregularization%2520is%2520critical%2520for%2520avoiding%2520self-intersections%2520that%2520are%2520typical%2520in%250Aimplicit-based%2520reconstructions.%2520We%2520have%2520evaluated%2520the%2520performance%2520of%25204DRecons%250Aon%2520a%2520variety%2520of%2520datasets.%2520Experimental%2520results%2520show%2520that%25204DRecons%2520can%2520handle%250Alarge%2520deformations%2520and%2520complex%2520inter-part%2520interactions%2520and%2520outperform%250Astate-of-the-art%2520approaches%2520considerably.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4DRecons%3A%204D%20Neural%20Implicit%20Deformable%20Objects%20Reconstruction%20from%20a%0A%20%20single%20RGB-D%20Camera%20with%20Geometrical%20and%20Topological%20Regularizations&entry.906535625=Xiaoyan%20Cong%20and%20Haitao%20Yang%20and%20Liyan%20Chen%20and%20Kaifeng%20Zhang%20and%20Li%20Yi%20and%20Chandrajit%20Bajaj%20and%20Qixing%20Huang&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%204DRecons%20that%20takes%20a%20single%20camera%0ARGB-D%20sequence%20of%20a%20dynamic%20subject%20as%20input%20and%20outputs%20a%20complete%20textured%0Adeforming%203D%20model%20over%20time.%204DRecons%20encodes%20the%20output%20as%20a%204D%20neural%0Aimplicit%20surface%20and%20presents%20an%20optimization%20procedure%20that%20combines%20a%20data%0Aterm%20and%20two%20regularization%20terms.%20The%20data%20term%20fits%20the%204D%20implicit%20surface%0Ato%20the%20input%20partial%20observations.%20We%20address%20fundamental%20challenges%20in%20fitting%0Aa%20complete%20implicit%20surface%20to%20partial%20observations.%20The%20first%20regularization%0Aterm%20enforces%20that%20the%20deformation%20among%20adjacent%20frames%20is%20as%20rigid%20as%0Apossible%20%28ARAP%29.%20To%20this%20end%2C%20we%20introduce%20a%20novel%20approach%20to%20compute%0Acorrespondences%20between%20adjacent%20textured%20implicit%20surfaces%2C%20which%20are%20used%20to%0Adefine%20the%20ARAP%20regularization%20term.%20The%20second%20regularization%20term%20enforces%0Athat%20the%20topology%20of%20the%20underlying%20object%20remains%20fixed%20over%20time.%20This%0Aregularization%20is%20critical%20for%20avoiding%20self-intersections%20that%20are%20typical%20in%0Aimplicit-based%20reconstructions.%20We%20have%20evaluated%20the%20performance%20of%204DRecons%0Aon%20a%20variety%20of%20datasets.%20Experimental%20results%20show%20that%204DRecons%20can%20handle%0Alarge%20deformations%20and%20complex%20inter-part%20interactions%20and%20outperform%0Astate-of-the-art%20approaches%20considerably.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10167v1&entry.124074799=Read"},
{"title": "WonderWorld: Interactive 3D Scene Generation from a Single Image", "author": "Hong-Xing Yu and Haoyi Duan and Charles Herrmann and William T. Freeman and Jiajun Wu", "abstract": "  We present WonderWorld, a novel framework for interactive 3D scene\nextrapolation that enables users to explore and shape virtual environments\nbased on a single input image and user-specified text. While significant\nimprovements have been made to the visual quality of scene generation, existing\nmethods are run offline, taking tens of minutes to hours to generate a scene.\nBy leveraging Fast Gaussian Surfels and a guided diffusion-based depth\nestimation method, WonderWorld generates geometrically consistent extrapolation\nwhile significantly reducing computational time. Our framework generates\nconnected and diverse 3D scenes in less than 10 seconds on a single A6000 GPU,\nenabling real-time user interaction and exploration. We demonstrate the\npotential of WonderWorld for applications in virtual reality, gaming, and\ncreative design, where users can quickly generate and navigate immersive,\npotentially infinite virtual worlds from a single image. Our approach\nrepresents a significant advancement in interactive 3D scene generation,\nopening up new possibilities for user-driven content creation and exploration\nin virtual environments. We will release full code and software for\nreproducibility. Project website: https://WonderWorld-2024.github.io/\n", "link": "http://arxiv.org/abs/2406.09394v2", "date": "2024-06-14", "relevancy": 3.112, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6451}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6451}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WonderWorld%3A%20Interactive%203D%20Scene%20Generation%20from%20a%20Single%20Image&body=Title%3A%20WonderWorld%3A%20Interactive%203D%20Scene%20Generation%20from%20a%20Single%20Image%0AAuthor%3A%20Hong-Xing%20Yu%20and%20Haoyi%20Duan%20and%20Charles%20Herrmann%20and%20William%20T.%20Freeman%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%20We%20present%20WonderWorld%2C%20a%20novel%20framework%20for%20interactive%203D%20scene%0Aextrapolation%20that%20enables%20users%20to%20explore%20and%20shape%20virtual%20environments%0Abased%20on%20a%20single%20input%20image%20and%20user-specified%20text.%20While%20significant%0Aimprovements%20have%20been%20made%20to%20the%20visual%20quality%20of%20scene%20generation%2C%20existing%0Amethods%20are%20run%20offline%2C%20taking%20tens%20of%20minutes%20to%20hours%20to%20generate%20a%20scene.%0ABy%20leveraging%20Fast%20Gaussian%20Surfels%20and%20a%20guided%20diffusion-based%20depth%0Aestimation%20method%2C%20WonderWorld%20generates%20geometrically%20consistent%20extrapolation%0Awhile%20significantly%20reducing%20computational%20time.%20Our%20framework%20generates%0Aconnected%20and%20diverse%203D%20scenes%20in%20less%20than%2010%20seconds%20on%20a%20single%20A6000%20GPU%2C%0Aenabling%20real-time%20user%20interaction%20and%20exploration.%20We%20demonstrate%20the%0Apotential%20of%20WonderWorld%20for%20applications%20in%20virtual%20reality%2C%20gaming%2C%20and%0Acreative%20design%2C%20where%20users%20can%20quickly%20generate%20and%20navigate%20immersive%2C%0Apotentially%20infinite%20virtual%20worlds%20from%20a%20single%20image.%20Our%20approach%0Arepresents%20a%20significant%20advancement%20in%20interactive%203D%20scene%20generation%2C%0Aopening%20up%20new%20possibilities%20for%20user-driven%20content%20creation%20and%20exploration%0Ain%20virtual%20environments.%20We%20will%20release%20full%20code%20and%20software%20for%0Areproducibility.%20Project%20website%3A%20https%3A//WonderWorld-2024.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09394v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWonderWorld%253A%2520Interactive%25203D%2520Scene%2520Generation%2520from%2520a%2520Single%2520Image%26entry.906535625%3DHong-Xing%2520Yu%2520and%2520Haoyi%2520Duan%2520and%2520Charles%2520Herrmann%2520and%2520William%2520T.%2520Freeman%2520and%2520Jiajun%2520Wu%26entry.1292438233%3D%2520%2520We%2520present%2520WonderWorld%252C%2520a%2520novel%2520framework%2520for%2520interactive%25203D%2520scene%250Aextrapolation%2520that%2520enables%2520users%2520to%2520explore%2520and%2520shape%2520virtual%2520environments%250Abased%2520on%2520a%2520single%2520input%2520image%2520and%2520user-specified%2520text.%2520While%2520significant%250Aimprovements%2520have%2520been%2520made%2520to%2520the%2520visual%2520quality%2520of%2520scene%2520generation%252C%2520existing%250Amethods%2520are%2520run%2520offline%252C%2520taking%2520tens%2520of%2520minutes%2520to%2520hours%2520to%2520generate%2520a%2520scene.%250ABy%2520leveraging%2520Fast%2520Gaussian%2520Surfels%2520and%2520a%2520guided%2520diffusion-based%2520depth%250Aestimation%2520method%252C%2520WonderWorld%2520generates%2520geometrically%2520consistent%2520extrapolation%250Awhile%2520significantly%2520reducing%2520computational%2520time.%2520Our%2520framework%2520generates%250Aconnected%2520and%2520diverse%25203D%2520scenes%2520in%2520less%2520than%252010%2520seconds%2520on%2520a%2520single%2520A6000%2520GPU%252C%250Aenabling%2520real-time%2520user%2520interaction%2520and%2520exploration.%2520We%2520demonstrate%2520the%250Apotential%2520of%2520WonderWorld%2520for%2520applications%2520in%2520virtual%2520reality%252C%2520gaming%252C%2520and%250Acreative%2520design%252C%2520where%2520users%2520can%2520quickly%2520generate%2520and%2520navigate%2520immersive%252C%250Apotentially%2520infinite%2520virtual%2520worlds%2520from%2520a%2520single%2520image.%2520Our%2520approach%250Arepresents%2520a%2520significant%2520advancement%2520in%2520interactive%25203D%2520scene%2520generation%252C%250Aopening%2520up%2520new%2520possibilities%2520for%2520user-driven%2520content%2520creation%2520and%2520exploration%250Ain%2520virtual%2520environments.%2520We%2520will%2520release%2520full%2520code%2520and%2520software%2520for%250Areproducibility.%2520Project%2520website%253A%2520https%253A//WonderWorld-2024.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09394v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WonderWorld%3A%20Interactive%203D%20Scene%20Generation%20from%20a%20Single%20Image&entry.906535625=Hong-Xing%20Yu%20and%20Haoyi%20Duan%20and%20Charles%20Herrmann%20and%20William%20T.%20Freeman%20and%20Jiajun%20Wu&entry.1292438233=%20%20We%20present%20WonderWorld%2C%20a%20novel%20framework%20for%20interactive%203D%20scene%0Aextrapolation%20that%20enables%20users%20to%20explore%20and%20shape%20virtual%20environments%0Abased%20on%20a%20single%20input%20image%20and%20user-specified%20text.%20While%20significant%0Aimprovements%20have%20been%20made%20to%20the%20visual%20quality%20of%20scene%20generation%2C%20existing%0Amethods%20are%20run%20offline%2C%20taking%20tens%20of%20minutes%20to%20hours%20to%20generate%20a%20scene.%0ABy%20leveraging%20Fast%20Gaussian%20Surfels%20and%20a%20guided%20diffusion-based%20depth%0Aestimation%20method%2C%20WonderWorld%20generates%20geometrically%20consistent%20extrapolation%0Awhile%20significantly%20reducing%20computational%20time.%20Our%20framework%20generates%0Aconnected%20and%20diverse%203D%20scenes%20in%20less%20than%2010%20seconds%20on%20a%20single%20A6000%20GPU%2C%0Aenabling%20real-time%20user%20interaction%20and%20exploration.%20We%20demonstrate%20the%0Apotential%20of%20WonderWorld%20for%20applications%20in%20virtual%20reality%2C%20gaming%2C%20and%0Acreative%20design%2C%20where%20users%20can%20quickly%20generate%20and%20navigate%20immersive%2C%0Apotentially%20infinite%20virtual%20worlds%20from%20a%20single%20image.%20Our%20approach%0Arepresents%20a%20significant%20advancement%20in%20interactive%203D%20scene%20generation%2C%0Aopening%20up%20new%20possibilities%20for%20user-driven%20content%20creation%20and%20exploration%0Ain%20virtual%20environments.%20We%20will%20release%20full%20code%20and%20software%20for%0Areproducibility.%20Project%20website%3A%20https%3A//WonderWorld-2024.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09394v2&entry.124074799=Read"},
{"title": "PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting", "author": "Alex Hanson and Allen Tu and Vasu Singla and Mayuka Jayawardhana and Matthias Zwicker and Tom Goldstein", "abstract": "  Recent advancements in novel view synthesis have enabled real-time rendering\nspeeds and high reconstruction accuracy. 3D Gaussian Splatting (3D-GS), a\nfoundational point-based parametric 3D scene representation, models scenes as\nlarge sets of 3D Gaussians. Complex scenes can comprise of millions of\nGaussians, amounting to large storage and memory requirements that limit the\nviability of 3D-GS on devices with limited resources. Current techniques for\ncompressing these pretrained models by pruning Gaussians rely on combining\nheuristics to determine which ones to remove. In this paper, we propose a\nprincipled spatial sensitivity pruning score that outperforms these approaches.\nIt is computed as a second-order approximation of the reconstruction error on\nthe training views with respect to the spatial parameters of each Gaussian.\nAdditionally, we propose a multi-round prune-refine pipeline that can be\napplied to any pretrained 3D-GS model without changing the training pipeline.\nAfter pruning 88.44% of the Gaussians, we observe that our PUP 3D-GS pipeline\nincreases the average rendering speed of 3D-GS by 2.65$\\times$ while retaining\nmore salient foreground information and achieving higher image quality metrics\nthan previous pruning techniques on scenes from the Mip-NeRF 360, Tanks &\nTemples, and Deep Blending datasets.\n", "link": "http://arxiv.org/abs/2406.10219v1", "date": "2024-06-14", "relevancy": 3.0948, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.682}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6246}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PUP%203D-GS%3A%20Principled%20Uncertainty%20Pruning%20for%203D%20Gaussian%20Splatting&body=Title%3A%20PUP%203D-GS%3A%20Principled%20Uncertainty%20Pruning%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Alex%20Hanson%20and%20Allen%20Tu%20and%20Vasu%20Singla%20and%20Mayuka%20Jayawardhana%20and%20Matthias%20Zwicker%20and%20Tom%20Goldstein%0AAbstract%3A%20%20%20Recent%20advancements%20in%20novel%20view%20synthesis%20have%20enabled%20real-time%20rendering%0Aspeeds%20and%20high%20reconstruction%20accuracy.%203D%20Gaussian%20Splatting%20%283D-GS%29%2C%20a%0Afoundational%20point-based%20parametric%203D%20scene%20representation%2C%20models%20scenes%20as%0Alarge%20sets%20of%203D%20Gaussians.%20Complex%20scenes%20can%20comprise%20of%20millions%20of%0AGaussians%2C%20amounting%20to%20large%20storage%20and%20memory%20requirements%20that%20limit%20the%0Aviability%20of%203D-GS%20on%20devices%20with%20limited%20resources.%20Current%20techniques%20for%0Acompressing%20these%20pretrained%20models%20by%20pruning%20Gaussians%20rely%20on%20combining%0Aheuristics%20to%20determine%20which%20ones%20to%20remove.%20In%20this%20paper%2C%20we%20propose%20a%0Aprincipled%20spatial%20sensitivity%20pruning%20score%20that%20outperforms%20these%20approaches.%0AIt%20is%20computed%20as%20a%20second-order%20approximation%20of%20the%20reconstruction%20error%20on%0Athe%20training%20views%20with%20respect%20to%20the%20spatial%20parameters%20of%20each%20Gaussian.%0AAdditionally%2C%20we%20propose%20a%20multi-round%20prune-refine%20pipeline%20that%20can%20be%0Aapplied%20to%20any%20pretrained%203D-GS%20model%20without%20changing%20the%20training%20pipeline.%0AAfter%20pruning%2088.44%25%20of%20the%20Gaussians%2C%20we%20observe%20that%20our%20PUP%203D-GS%20pipeline%0Aincreases%20the%20average%20rendering%20speed%20of%203D-GS%20by%202.65%24%5Ctimes%24%20while%20retaining%0Amore%20salient%20foreground%20information%20and%20achieving%20higher%20image%20quality%20metrics%0Athan%20previous%20pruning%20techniques%20on%20scenes%20from%20the%20Mip-NeRF%20360%2C%20Tanks%20%26%0ATemples%2C%20and%20Deep%20Blending%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPUP%25203D-GS%253A%2520Principled%2520Uncertainty%2520Pruning%2520for%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DAlex%2520Hanson%2520and%2520Allen%2520Tu%2520and%2520Vasu%2520Singla%2520and%2520Mayuka%2520Jayawardhana%2520and%2520Matthias%2520Zwicker%2520and%2520Tom%2520Goldstein%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520novel%2520view%2520synthesis%2520have%2520enabled%2520real-time%2520rendering%250Aspeeds%2520and%2520high%2520reconstruction%2520accuracy.%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529%252C%2520a%250Afoundational%2520point-based%2520parametric%25203D%2520scene%2520representation%252C%2520models%2520scenes%2520as%250Alarge%2520sets%2520of%25203D%2520Gaussians.%2520Complex%2520scenes%2520can%2520comprise%2520of%2520millions%2520of%250AGaussians%252C%2520amounting%2520to%2520large%2520storage%2520and%2520memory%2520requirements%2520that%2520limit%2520the%250Aviability%2520of%25203D-GS%2520on%2520devices%2520with%2520limited%2520resources.%2520Current%2520techniques%2520for%250Acompressing%2520these%2520pretrained%2520models%2520by%2520pruning%2520Gaussians%2520rely%2520on%2520combining%250Aheuristics%2520to%2520determine%2520which%2520ones%2520to%2520remove.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Aprincipled%2520spatial%2520sensitivity%2520pruning%2520score%2520that%2520outperforms%2520these%2520approaches.%250AIt%2520is%2520computed%2520as%2520a%2520second-order%2520approximation%2520of%2520the%2520reconstruction%2520error%2520on%250Athe%2520training%2520views%2520with%2520respect%2520to%2520the%2520spatial%2520parameters%2520of%2520each%2520Gaussian.%250AAdditionally%252C%2520we%2520propose%2520a%2520multi-round%2520prune-refine%2520pipeline%2520that%2520can%2520be%250Aapplied%2520to%2520any%2520pretrained%25203D-GS%2520model%2520without%2520changing%2520the%2520training%2520pipeline.%250AAfter%2520pruning%252088.44%2525%2520of%2520the%2520Gaussians%252C%2520we%2520observe%2520that%2520our%2520PUP%25203D-GS%2520pipeline%250Aincreases%2520the%2520average%2520rendering%2520speed%2520of%25203D-GS%2520by%25202.65%2524%255Ctimes%2524%2520while%2520retaining%250Amore%2520salient%2520foreground%2520information%2520and%2520achieving%2520higher%2520image%2520quality%2520metrics%250Athan%2520previous%2520pruning%2520techniques%2520on%2520scenes%2520from%2520the%2520Mip-NeRF%2520360%252C%2520Tanks%2520%2526%250ATemples%252C%2520and%2520Deep%2520Blending%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PUP%203D-GS%3A%20Principled%20Uncertainty%20Pruning%20for%203D%20Gaussian%20Splatting&entry.906535625=Alex%20Hanson%20and%20Allen%20Tu%20and%20Vasu%20Singla%20and%20Mayuka%20Jayawardhana%20and%20Matthias%20Zwicker%20and%20Tom%20Goldstein&entry.1292438233=%20%20Recent%20advancements%20in%20novel%20view%20synthesis%20have%20enabled%20real-time%20rendering%0Aspeeds%20and%20high%20reconstruction%20accuracy.%203D%20Gaussian%20Splatting%20%283D-GS%29%2C%20a%0Afoundational%20point-based%20parametric%203D%20scene%20representation%2C%20models%20scenes%20as%0Alarge%20sets%20of%203D%20Gaussians.%20Complex%20scenes%20can%20comprise%20of%20millions%20of%0AGaussians%2C%20amounting%20to%20large%20storage%20and%20memory%20requirements%20that%20limit%20the%0Aviability%20of%203D-GS%20on%20devices%20with%20limited%20resources.%20Current%20techniques%20for%0Acompressing%20these%20pretrained%20models%20by%20pruning%20Gaussians%20rely%20on%20combining%0Aheuristics%20to%20determine%20which%20ones%20to%20remove.%20In%20this%20paper%2C%20we%20propose%20a%0Aprincipled%20spatial%20sensitivity%20pruning%20score%20that%20outperforms%20these%20approaches.%0AIt%20is%20computed%20as%20a%20second-order%20approximation%20of%20the%20reconstruction%20error%20on%0Athe%20training%20views%20with%20respect%20to%20the%20spatial%20parameters%20of%20each%20Gaussian.%0AAdditionally%2C%20we%20propose%20a%20multi-round%20prune-refine%20pipeline%20that%20can%20be%0Aapplied%20to%20any%20pretrained%203D-GS%20model%20without%20changing%20the%20training%20pipeline.%0AAfter%20pruning%2088.44%25%20of%20the%20Gaussians%2C%20we%20observe%20that%20our%20PUP%203D-GS%20pipeline%0Aincreases%20the%20average%20rendering%20speed%20of%203D-GS%20by%202.65%24%5Ctimes%24%20while%20retaining%0Amore%20salient%20foreground%20information%20and%20achieving%20higher%20image%20quality%20metrics%0Athan%20previous%20pruning%20techniques%20on%20scenes%20from%20the%20Mip-NeRF%20360%2C%20Tanks%20%26%0ATemples%2C%20and%20Deep%20Blending%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10219v1&entry.124074799=Read"},
{"title": "4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities", "author": "Roman Bachmann and O\u011fuzhan Fatih Kar and David Mizrahi and Ali Garjani and Mingfei Gao and David Griffiths and Jiaming Hu and Afshin Dehghan and Amir Zamir", "abstract": "  Current multimodal and multitask foundation models like 4M or UnifiedIO show\npromising results, but in practice their out-of-the-box abilities to accept\ndiverse inputs and perform diverse tasks are limited by the (usually rather\nsmall) number of modalities and tasks they are trained on. In this paper, we\nexpand upon the capabilities of them by training a single model on tens of\nhighly diverse modalities and by performing co-training on large-scale\nmultimodal datasets and text corpora. This includes training on several\nsemantic and geometric modalities, feature maps from recent state of the art\nmodels like DINOv2 and ImageBind, pseudo labels of specialist models like SAM\nand 4DHumans, and a range of new modalities that allow for novel ways to\ninteract with the model and steer the generation, for example image metadata or\ncolor palettes. A crucial step in this process is performing discrete\ntokenization on various modalities, whether they are image-like, neural network\nfeature maps, vectors, structured data like instance segmentation or human\nposes, or data that can be represented as text. Through this, we expand on the\nout-of-the-box capabilities of multimodal models and specifically show the\npossibility of training one model to solve at least 3x more tasks/modalities\nthan existing ones and doing so without a loss in performance. This enables\nmore fine-grained and controllable multimodal generation capabilities and\nallows us to study the distillation of models trained on diverse data and\nobjectives into a unified model. We successfully scale the training to a three\nbillion parameter model using tens of modalities and different datasets. The\nresulting models and training code are open sourced at 4m.epfl.ch.\n", "link": "http://arxiv.org/abs/2406.09406v2", "date": "2024-06-14", "relevancy": 3.0182, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6413}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5848}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204M-21%3A%20An%20Any-to-Any%20Vision%20Model%20for%20Tens%20of%20Tasks%20and%20Modalities&body=Title%3A%204M-21%3A%20An%20Any-to-Any%20Vision%20Model%20for%20Tens%20of%20Tasks%20and%20Modalities%0AAuthor%3A%20Roman%20Bachmann%20and%20O%C4%9Fuzhan%20Fatih%20Kar%20and%20David%20Mizrahi%20and%20Ali%20Garjani%20and%20Mingfei%20Gao%20and%20David%20Griffiths%20and%20Jiaming%20Hu%20and%20Afshin%20Dehghan%20and%20Amir%20Zamir%0AAbstract%3A%20%20%20Current%20multimodal%20and%20multitask%20foundation%20models%20like%204M%20or%20UnifiedIO%20show%0Apromising%20results%2C%20but%20in%20practice%20their%20out-of-the-box%20abilities%20to%20accept%0Adiverse%20inputs%20and%20perform%20diverse%20tasks%20are%20limited%20by%20the%20%28usually%20rather%0Asmall%29%20number%20of%20modalities%20and%20tasks%20they%20are%20trained%20on.%20In%20this%20paper%2C%20we%0Aexpand%20upon%20the%20capabilities%20of%20them%20by%20training%20a%20single%20model%20on%20tens%20of%0Ahighly%20diverse%20modalities%20and%20by%20performing%20co-training%20on%20large-scale%0Amultimodal%20datasets%20and%20text%20corpora.%20This%20includes%20training%20on%20several%0Asemantic%20and%20geometric%20modalities%2C%20feature%20maps%20from%20recent%20state%20of%20the%20art%0Amodels%20like%20DINOv2%20and%20ImageBind%2C%20pseudo%20labels%20of%20specialist%20models%20like%20SAM%0Aand%204DHumans%2C%20and%20a%20range%20of%20new%20modalities%20that%20allow%20for%20novel%20ways%20to%0Ainteract%20with%20the%20model%20and%20steer%20the%20generation%2C%20for%20example%20image%20metadata%20or%0Acolor%20palettes.%20A%20crucial%20step%20in%20this%20process%20is%20performing%20discrete%0Atokenization%20on%20various%20modalities%2C%20whether%20they%20are%20image-like%2C%20neural%20network%0Afeature%20maps%2C%20vectors%2C%20structured%20data%20like%20instance%20segmentation%20or%20human%0Aposes%2C%20or%20data%20that%20can%20be%20represented%20as%20text.%20Through%20this%2C%20we%20expand%20on%20the%0Aout-of-the-box%20capabilities%20of%20multimodal%20models%20and%20specifically%20show%20the%0Apossibility%20of%20training%20one%20model%20to%20solve%20at%20least%203x%20more%20tasks/modalities%0Athan%20existing%20ones%20and%20doing%20so%20without%20a%20loss%20in%20performance.%20This%20enables%0Amore%20fine-grained%20and%20controllable%20multimodal%20generation%20capabilities%20and%0Aallows%20us%20to%20study%20the%20distillation%20of%20models%20trained%20on%20diverse%20data%20and%0Aobjectives%20into%20a%20unified%20model.%20We%20successfully%20scale%20the%20training%20to%20a%20three%0Abillion%20parameter%20model%20using%20tens%20of%20modalities%20and%20different%20datasets.%20The%0Aresulting%20models%20and%20training%20code%20are%20open%20sourced%20at%204m.epfl.ch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09406v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4M-21%253A%2520An%2520Any-to-Any%2520Vision%2520Model%2520for%2520Tens%2520of%2520Tasks%2520and%2520Modalities%26entry.906535625%3DRoman%2520Bachmann%2520and%2520O%25C4%259Fuzhan%2520Fatih%2520Kar%2520and%2520David%2520Mizrahi%2520and%2520Ali%2520Garjani%2520and%2520Mingfei%2520Gao%2520and%2520David%2520Griffiths%2520and%2520Jiaming%2520Hu%2520and%2520Afshin%2520Dehghan%2520and%2520Amir%2520Zamir%26entry.1292438233%3D%2520%2520Current%2520multimodal%2520and%2520multitask%2520foundation%2520models%2520like%25204M%2520or%2520UnifiedIO%2520show%250Apromising%2520results%252C%2520but%2520in%2520practice%2520their%2520out-of-the-box%2520abilities%2520to%2520accept%250Adiverse%2520inputs%2520and%2520perform%2520diverse%2520tasks%2520are%2520limited%2520by%2520the%2520%2528usually%2520rather%250Asmall%2529%2520number%2520of%2520modalities%2520and%2520tasks%2520they%2520are%2520trained%2520on.%2520In%2520this%2520paper%252C%2520we%250Aexpand%2520upon%2520the%2520capabilities%2520of%2520them%2520by%2520training%2520a%2520single%2520model%2520on%2520tens%2520of%250Ahighly%2520diverse%2520modalities%2520and%2520by%2520performing%2520co-training%2520on%2520large-scale%250Amultimodal%2520datasets%2520and%2520text%2520corpora.%2520This%2520includes%2520training%2520on%2520several%250Asemantic%2520and%2520geometric%2520modalities%252C%2520feature%2520maps%2520from%2520recent%2520state%2520of%2520the%2520art%250Amodels%2520like%2520DINOv2%2520and%2520ImageBind%252C%2520pseudo%2520labels%2520of%2520specialist%2520models%2520like%2520SAM%250Aand%25204DHumans%252C%2520and%2520a%2520range%2520of%2520new%2520modalities%2520that%2520allow%2520for%2520novel%2520ways%2520to%250Ainteract%2520with%2520the%2520model%2520and%2520steer%2520the%2520generation%252C%2520for%2520example%2520image%2520metadata%2520or%250Acolor%2520palettes.%2520A%2520crucial%2520step%2520in%2520this%2520process%2520is%2520performing%2520discrete%250Atokenization%2520on%2520various%2520modalities%252C%2520whether%2520they%2520are%2520image-like%252C%2520neural%2520network%250Afeature%2520maps%252C%2520vectors%252C%2520structured%2520data%2520like%2520instance%2520segmentation%2520or%2520human%250Aposes%252C%2520or%2520data%2520that%2520can%2520be%2520represented%2520as%2520text.%2520Through%2520this%252C%2520we%2520expand%2520on%2520the%250Aout-of-the-box%2520capabilities%2520of%2520multimodal%2520models%2520and%2520specifically%2520show%2520the%250Apossibility%2520of%2520training%2520one%2520model%2520to%2520solve%2520at%2520least%25203x%2520more%2520tasks/modalities%250Athan%2520existing%2520ones%2520and%2520doing%2520so%2520without%2520a%2520loss%2520in%2520performance.%2520This%2520enables%250Amore%2520fine-grained%2520and%2520controllable%2520multimodal%2520generation%2520capabilities%2520and%250Aallows%2520us%2520to%2520study%2520the%2520distillation%2520of%2520models%2520trained%2520on%2520diverse%2520data%2520and%250Aobjectives%2520into%2520a%2520unified%2520model.%2520We%2520successfully%2520scale%2520the%2520training%2520to%2520a%2520three%250Abillion%2520parameter%2520model%2520using%2520tens%2520of%2520modalities%2520and%2520different%2520datasets.%2520The%250Aresulting%2520models%2520and%2520training%2520code%2520are%2520open%2520sourced%2520at%25204m.epfl.ch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09406v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4M-21%3A%20An%20Any-to-Any%20Vision%20Model%20for%20Tens%20of%20Tasks%20and%20Modalities&entry.906535625=Roman%20Bachmann%20and%20O%C4%9Fuzhan%20Fatih%20Kar%20and%20David%20Mizrahi%20and%20Ali%20Garjani%20and%20Mingfei%20Gao%20and%20David%20Griffiths%20and%20Jiaming%20Hu%20and%20Afshin%20Dehghan%20and%20Amir%20Zamir&entry.1292438233=%20%20Current%20multimodal%20and%20multitask%20foundation%20models%20like%204M%20or%20UnifiedIO%20show%0Apromising%20results%2C%20but%20in%20practice%20their%20out-of-the-box%20abilities%20to%20accept%0Adiverse%20inputs%20and%20perform%20diverse%20tasks%20are%20limited%20by%20the%20%28usually%20rather%0Asmall%29%20number%20of%20modalities%20and%20tasks%20they%20are%20trained%20on.%20In%20this%20paper%2C%20we%0Aexpand%20upon%20the%20capabilities%20of%20them%20by%20training%20a%20single%20model%20on%20tens%20of%0Ahighly%20diverse%20modalities%20and%20by%20performing%20co-training%20on%20large-scale%0Amultimodal%20datasets%20and%20text%20corpora.%20This%20includes%20training%20on%20several%0Asemantic%20and%20geometric%20modalities%2C%20feature%20maps%20from%20recent%20state%20of%20the%20art%0Amodels%20like%20DINOv2%20and%20ImageBind%2C%20pseudo%20labels%20of%20specialist%20models%20like%20SAM%0Aand%204DHumans%2C%20and%20a%20range%20of%20new%20modalities%20that%20allow%20for%20novel%20ways%20to%0Ainteract%20with%20the%20model%20and%20steer%20the%20generation%2C%20for%20example%20image%20metadata%20or%0Acolor%20palettes.%20A%20crucial%20step%20in%20this%20process%20is%20performing%20discrete%0Atokenization%20on%20various%20modalities%2C%20whether%20they%20are%20image-like%2C%20neural%20network%0Afeature%20maps%2C%20vectors%2C%20structured%20data%20like%20instance%20segmentation%20or%20human%0Aposes%2C%20or%20data%20that%20can%20be%20represented%20as%20text.%20Through%20this%2C%20we%20expand%20on%20the%0Aout-of-the-box%20capabilities%20of%20multimodal%20models%20and%20specifically%20show%20the%0Apossibility%20of%20training%20one%20model%20to%20solve%20at%20least%203x%20more%20tasks/modalities%0Athan%20existing%20ones%20and%20doing%20so%20without%20a%20loss%20in%20performance.%20This%20enables%0Amore%20fine-grained%20and%20controllable%20multimodal%20generation%20capabilities%20and%0Aallows%20us%20to%20study%20the%20distillation%20of%20models%20trained%20on%20diverse%20data%20and%0Aobjectives%20into%20a%20unified%20model.%20We%20successfully%20scale%20the%20training%20to%20a%20three%0Abillion%20parameter%20model%20using%20tens%20of%20modalities%20and%20different%20datasets.%20The%0Aresulting%20models%20and%20training%20code%20are%20open%20sourced%20at%204m.epfl.ch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09406v2&entry.124074799=Read"},
{"title": "Shelf-Supervised Multi-Modal Pre-Training for 3D Object Detection", "author": "Mehar Khurana and Neehar Peri and Deva Ramanan and James Hays", "abstract": "  State-of-the-art 3D object detectors are often trained on massive labeled\ndatasets. However, annotating 3D bounding boxes remains prohibitively expensive\nand time-consuming, particularly for LiDAR. Instead, recent works demonstrate\nthat self-supervised pre-training with unlabeled data can improve detection\naccuracy with limited labels. Contemporary methods adapt best-practices for\nself-supervised learning from the image domain to point clouds (such as\ncontrastive learning). However, publicly available 3D datasets are considerably\nsmaller and less diverse than those used for image-based self-supervised\nlearning, limiting their effectiveness. We do note, however, that such data is\nnaturally collected in a multimodal fashion, often paired with images. Rather\nthan pre-training with only self-supervised objectives, we argue that it is\nbetter to bootstrap point cloud representations using image-based foundation\nmodels trained on internet-scale image data. Specifically, we propose a\nshelf-supervised approach (e.g. supervised with off-the-shelf image foundation\nmodels) for generating zero-shot 3D bounding boxes from paired RGB and LiDAR\ndata. Pre-training 3D detectors with such pseudo-labels yields significantly\nbetter semi-supervised detection accuracy than prior self-supervised pretext\ntasks. Importantly, we show that image-based shelf-supervision is helpful for\ntraining LiDAR-only and multi-modal (RGB + LiDAR) detectors. We demonstrate the\neffectiveness of our approach on nuScenes and WOD, significantly improving over\nprior work in limited data settings.\n", "link": "http://arxiv.org/abs/2406.10115v1", "date": "2024-06-14", "relevancy": 3.0173, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6199}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5954}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shelf-Supervised%20Multi-Modal%20Pre-Training%20for%203D%20Object%20Detection&body=Title%3A%20Shelf-Supervised%20Multi-Modal%20Pre-Training%20for%203D%20Object%20Detection%0AAuthor%3A%20Mehar%20Khurana%20and%20Neehar%20Peri%20and%20Deva%20Ramanan%20and%20James%20Hays%0AAbstract%3A%20%20%20State-of-the-art%203D%20object%20detectors%20are%20often%20trained%20on%20massive%20labeled%0Adatasets.%20However%2C%20annotating%203D%20bounding%20boxes%20remains%20prohibitively%20expensive%0Aand%20time-consuming%2C%20particularly%20for%20LiDAR.%20Instead%2C%20recent%20works%20demonstrate%0Athat%20self-supervised%20pre-training%20with%20unlabeled%20data%20can%20improve%20detection%0Aaccuracy%20with%20limited%20labels.%20Contemporary%20methods%20adapt%20best-practices%20for%0Aself-supervised%20learning%20from%20the%20image%20domain%20to%20point%20clouds%20%28such%20as%0Acontrastive%20learning%29.%20However%2C%20publicly%20available%203D%20datasets%20are%20considerably%0Asmaller%20and%20less%20diverse%20than%20those%20used%20for%20image-based%20self-supervised%0Alearning%2C%20limiting%20their%20effectiveness.%20We%20do%20note%2C%20however%2C%20that%20such%20data%20is%0Anaturally%20collected%20in%20a%20multimodal%20fashion%2C%20often%20paired%20with%20images.%20Rather%0Athan%20pre-training%20with%20only%20self-supervised%20objectives%2C%20we%20argue%20that%20it%20is%0Abetter%20to%20bootstrap%20point%20cloud%20representations%20using%20image-based%20foundation%0Amodels%20trained%20on%20internet-scale%20image%20data.%20Specifically%2C%20we%20propose%20a%0Ashelf-supervised%20approach%20%28e.g.%20supervised%20with%20off-the-shelf%20image%20foundation%0Amodels%29%20for%20generating%20zero-shot%203D%20bounding%20boxes%20from%20paired%20RGB%20and%20LiDAR%0Adata.%20Pre-training%203D%20detectors%20with%20such%20pseudo-labels%20yields%20significantly%0Abetter%20semi-supervised%20detection%20accuracy%20than%20prior%20self-supervised%20pretext%0Atasks.%20Importantly%2C%20we%20show%20that%20image-based%20shelf-supervision%20is%20helpful%20for%0Atraining%20LiDAR-only%20and%20multi-modal%20%28RGB%20%2B%20LiDAR%29%20detectors.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20on%20nuScenes%20and%20WOD%2C%20significantly%20improving%20over%0Aprior%20work%20in%20limited%20data%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10115v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShelf-Supervised%2520Multi-Modal%2520Pre-Training%2520for%25203D%2520Object%2520Detection%26entry.906535625%3DMehar%2520Khurana%2520and%2520Neehar%2520Peri%2520and%2520Deva%2520Ramanan%2520and%2520James%2520Hays%26entry.1292438233%3D%2520%2520State-of-the-art%25203D%2520object%2520detectors%2520are%2520often%2520trained%2520on%2520massive%2520labeled%250Adatasets.%2520However%252C%2520annotating%25203D%2520bounding%2520boxes%2520remains%2520prohibitively%2520expensive%250Aand%2520time-consuming%252C%2520particularly%2520for%2520LiDAR.%2520Instead%252C%2520recent%2520works%2520demonstrate%250Athat%2520self-supervised%2520pre-training%2520with%2520unlabeled%2520data%2520can%2520improve%2520detection%250Aaccuracy%2520with%2520limited%2520labels.%2520Contemporary%2520methods%2520adapt%2520best-practices%2520for%250Aself-supervised%2520learning%2520from%2520the%2520image%2520domain%2520to%2520point%2520clouds%2520%2528such%2520as%250Acontrastive%2520learning%2529.%2520However%252C%2520publicly%2520available%25203D%2520datasets%2520are%2520considerably%250Asmaller%2520and%2520less%2520diverse%2520than%2520those%2520used%2520for%2520image-based%2520self-supervised%250Alearning%252C%2520limiting%2520their%2520effectiveness.%2520We%2520do%2520note%252C%2520however%252C%2520that%2520such%2520data%2520is%250Anaturally%2520collected%2520in%2520a%2520multimodal%2520fashion%252C%2520often%2520paired%2520with%2520images.%2520Rather%250Athan%2520pre-training%2520with%2520only%2520self-supervised%2520objectives%252C%2520we%2520argue%2520that%2520it%2520is%250Abetter%2520to%2520bootstrap%2520point%2520cloud%2520representations%2520using%2520image-based%2520foundation%250Amodels%2520trained%2520on%2520internet-scale%2520image%2520data.%2520Specifically%252C%2520we%2520propose%2520a%250Ashelf-supervised%2520approach%2520%2528e.g.%2520supervised%2520with%2520off-the-shelf%2520image%2520foundation%250Amodels%2529%2520for%2520generating%2520zero-shot%25203D%2520bounding%2520boxes%2520from%2520paired%2520RGB%2520and%2520LiDAR%250Adata.%2520Pre-training%25203D%2520detectors%2520with%2520such%2520pseudo-labels%2520yields%2520significantly%250Abetter%2520semi-supervised%2520detection%2520accuracy%2520than%2520prior%2520self-supervised%2520pretext%250Atasks.%2520Importantly%252C%2520we%2520show%2520that%2520image-based%2520shelf-supervision%2520is%2520helpful%2520for%250Atraining%2520LiDAR-only%2520and%2520multi-modal%2520%2528RGB%2520%252B%2520LiDAR%2529%2520detectors.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520on%2520nuScenes%2520and%2520WOD%252C%2520significantly%2520improving%2520over%250Aprior%2520work%2520in%2520limited%2520data%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10115v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shelf-Supervised%20Multi-Modal%20Pre-Training%20for%203D%20Object%20Detection&entry.906535625=Mehar%20Khurana%20and%20Neehar%20Peri%20and%20Deva%20Ramanan%20and%20James%20Hays&entry.1292438233=%20%20State-of-the-art%203D%20object%20detectors%20are%20often%20trained%20on%20massive%20labeled%0Adatasets.%20However%2C%20annotating%203D%20bounding%20boxes%20remains%20prohibitively%20expensive%0Aand%20time-consuming%2C%20particularly%20for%20LiDAR.%20Instead%2C%20recent%20works%20demonstrate%0Athat%20self-supervised%20pre-training%20with%20unlabeled%20data%20can%20improve%20detection%0Aaccuracy%20with%20limited%20labels.%20Contemporary%20methods%20adapt%20best-practices%20for%0Aself-supervised%20learning%20from%20the%20image%20domain%20to%20point%20clouds%20%28such%20as%0Acontrastive%20learning%29.%20However%2C%20publicly%20available%203D%20datasets%20are%20considerably%0Asmaller%20and%20less%20diverse%20than%20those%20used%20for%20image-based%20self-supervised%0Alearning%2C%20limiting%20their%20effectiveness.%20We%20do%20note%2C%20however%2C%20that%20such%20data%20is%0Anaturally%20collected%20in%20a%20multimodal%20fashion%2C%20often%20paired%20with%20images.%20Rather%0Athan%20pre-training%20with%20only%20self-supervised%20objectives%2C%20we%20argue%20that%20it%20is%0Abetter%20to%20bootstrap%20point%20cloud%20representations%20using%20image-based%20foundation%0Amodels%20trained%20on%20internet-scale%20image%20data.%20Specifically%2C%20we%20propose%20a%0Ashelf-supervised%20approach%20%28e.g.%20supervised%20with%20off-the-shelf%20image%20foundation%0Amodels%29%20for%20generating%20zero-shot%203D%20bounding%20boxes%20from%20paired%20RGB%20and%20LiDAR%0Adata.%20Pre-training%203D%20detectors%20with%20such%20pseudo-labels%20yields%20significantly%0Abetter%20semi-supervised%20detection%20accuracy%20than%20prior%20self-supervised%20pretext%0Atasks.%20Importantly%2C%20we%20show%20that%20image-based%20shelf-supervision%20is%20helpful%20for%0Atraining%20LiDAR-only%20and%20multi-modal%20%28RGB%20%2B%20LiDAR%29%20detectors.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20on%20nuScenes%20and%20WOD%2C%20significantly%20improving%20over%0Aprior%20work%20in%20limited%20data%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10115v1&entry.124074799=Read"},
{"title": "GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions", "author": "Salvatore Esposito and Qingshan Xu and Kacper Kania and Charlie Hewitt and Octave Mariotti and Lohit Petikam and Julien Valentin and Arno Onken and Oisin Mac Aodha", "abstract": "  We introduce a new generative approach for synthesizing 3D geometry and\nimages from single-view collections. Most existing approaches predict\nvolumetric density to render multi-view consistent images. By employing\nvolumetric rendering using neural radiance fields, they inherit a key\nlimitation: the generated geometry is noisy and unconstrained, limiting the\nquality and utility of the output meshes. To address this issue, we propose\nGeoGen, a new SDF-based 3D generative model trained in an end-to-end manner.\nInitially, we reinterpret the volumetric density as a Signed Distance Function\n(SDF). This allows us to introduce useful priors to generate valid meshes.\nHowever, those priors prevent the generative model from learning details,\nlimiting the applicability of the method to real-world scenarios. To alleviate\nthat problem, we make the transformation learnable and constrain the rendered\ndepth map to be consistent with the zero-level set of the SDF. Through the lens\nof adversarial training, we encourage the network to produce higher fidelity\ndetails on the output meshes. For evaluation, we introduce a synthetic dataset\nof human avatars captured from 360-degree camera angles, to overcome the\nchallenges presented by real-world datasets, which often lack 3D consistency\nand do not cover all camera angles. Our experiments on multiple datasets show\nthat GeoGen produces visually and quantitatively better geometry than the\nprevious generative models based on neural radiance fields.\n", "link": "http://arxiv.org/abs/2406.04254v3", "date": "2024-06-14", "relevancy": 2.9382, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6019}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5882}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoGen%3A%20Geometry-Aware%20Generative%20Modeling%20via%20Signed%20Distance%20Functions&body=Title%3A%20GeoGen%3A%20Geometry-Aware%20Generative%20Modeling%20via%20Signed%20Distance%20Functions%0AAuthor%3A%20Salvatore%20Esposito%20and%20Qingshan%20Xu%20and%20Kacper%20Kania%20and%20Charlie%20Hewitt%20and%20Octave%20Mariotti%20and%20Lohit%20Petikam%20and%20Julien%20Valentin%20and%20Arno%20Onken%20and%20Oisin%20Mac%20Aodha%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20generative%20approach%20for%20synthesizing%203D%20geometry%20and%0Aimages%20from%20single-view%20collections.%20Most%20existing%20approaches%20predict%0Avolumetric%20density%20to%20render%20multi-view%20consistent%20images.%20By%20employing%0Avolumetric%20rendering%20using%20neural%20radiance%20fields%2C%20they%20inherit%20a%20key%0Alimitation%3A%20the%20generated%20geometry%20is%20noisy%20and%20unconstrained%2C%20limiting%20the%0Aquality%20and%20utility%20of%20the%20output%20meshes.%20To%20address%20this%20issue%2C%20we%20propose%0AGeoGen%2C%20a%20new%20SDF-based%203D%20generative%20model%20trained%20in%20an%20end-to-end%20manner.%0AInitially%2C%20we%20reinterpret%20the%20volumetric%20density%20as%20a%20Signed%20Distance%20Function%0A%28SDF%29.%20This%20allows%20us%20to%20introduce%20useful%20priors%20to%20generate%20valid%20meshes.%0AHowever%2C%20those%20priors%20prevent%20the%20generative%20model%20from%20learning%20details%2C%0Alimiting%20the%20applicability%20of%20the%20method%20to%20real-world%20scenarios.%20To%20alleviate%0Athat%20problem%2C%20we%20make%20the%20transformation%20learnable%20and%20constrain%20the%20rendered%0Adepth%20map%20to%20be%20consistent%20with%20the%20zero-level%20set%20of%20the%20SDF.%20Through%20the%20lens%0Aof%20adversarial%20training%2C%20we%20encourage%20the%20network%20to%20produce%20higher%20fidelity%0Adetails%20on%20the%20output%20meshes.%20For%20evaluation%2C%20we%20introduce%20a%20synthetic%20dataset%0Aof%20human%20avatars%20captured%20from%20360-degree%20camera%20angles%2C%20to%20overcome%20the%0Achallenges%20presented%20by%20real-world%20datasets%2C%20which%20often%20lack%203D%20consistency%0Aand%20do%20not%20cover%20all%20camera%20angles.%20Our%20experiments%20on%20multiple%20datasets%20show%0Athat%20GeoGen%20produces%20visually%20and%20quantitatively%20better%20geometry%20than%20the%0Aprevious%20generative%20models%20based%20on%20neural%20radiance%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04254v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoGen%253A%2520Geometry-Aware%2520Generative%2520Modeling%2520via%2520Signed%2520Distance%2520Functions%26entry.906535625%3DSalvatore%2520Esposito%2520and%2520Qingshan%2520Xu%2520and%2520Kacper%2520Kania%2520and%2520Charlie%2520Hewitt%2520and%2520Octave%2520Mariotti%2520and%2520Lohit%2520Petikam%2520and%2520Julien%2520Valentin%2520and%2520Arno%2520Onken%2520and%2520Oisin%2520Mac%2520Aodha%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520generative%2520approach%2520for%2520synthesizing%25203D%2520geometry%2520and%250Aimages%2520from%2520single-view%2520collections.%2520Most%2520existing%2520approaches%2520predict%250Avolumetric%2520density%2520to%2520render%2520multi-view%2520consistent%2520images.%2520By%2520employing%250Avolumetric%2520rendering%2520using%2520neural%2520radiance%2520fields%252C%2520they%2520inherit%2520a%2520key%250Alimitation%253A%2520the%2520generated%2520geometry%2520is%2520noisy%2520and%2520unconstrained%252C%2520limiting%2520the%250Aquality%2520and%2520utility%2520of%2520the%2520output%2520meshes.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%250AGeoGen%252C%2520a%2520new%2520SDF-based%25203D%2520generative%2520model%2520trained%2520in%2520an%2520end-to-end%2520manner.%250AInitially%252C%2520we%2520reinterpret%2520the%2520volumetric%2520density%2520as%2520a%2520Signed%2520Distance%2520Function%250A%2528SDF%2529.%2520This%2520allows%2520us%2520to%2520introduce%2520useful%2520priors%2520to%2520generate%2520valid%2520meshes.%250AHowever%252C%2520those%2520priors%2520prevent%2520the%2520generative%2520model%2520from%2520learning%2520details%252C%250Alimiting%2520the%2520applicability%2520of%2520the%2520method%2520to%2520real-world%2520scenarios.%2520To%2520alleviate%250Athat%2520problem%252C%2520we%2520make%2520the%2520transformation%2520learnable%2520and%2520constrain%2520the%2520rendered%250Adepth%2520map%2520to%2520be%2520consistent%2520with%2520the%2520zero-level%2520set%2520of%2520the%2520SDF.%2520Through%2520the%2520lens%250Aof%2520adversarial%2520training%252C%2520we%2520encourage%2520the%2520network%2520to%2520produce%2520higher%2520fidelity%250Adetails%2520on%2520the%2520output%2520meshes.%2520For%2520evaluation%252C%2520we%2520introduce%2520a%2520synthetic%2520dataset%250Aof%2520human%2520avatars%2520captured%2520from%2520360-degree%2520camera%2520angles%252C%2520to%2520overcome%2520the%250Achallenges%2520presented%2520by%2520real-world%2520datasets%252C%2520which%2520often%2520lack%25203D%2520consistency%250Aand%2520do%2520not%2520cover%2520all%2520camera%2520angles.%2520Our%2520experiments%2520on%2520multiple%2520datasets%2520show%250Athat%2520GeoGen%2520produces%2520visually%2520and%2520quantitatively%2520better%2520geometry%2520than%2520the%250Aprevious%2520generative%2520models%2520based%2520on%2520neural%2520radiance%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04254v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoGen%3A%20Geometry-Aware%20Generative%20Modeling%20via%20Signed%20Distance%20Functions&entry.906535625=Salvatore%20Esposito%20and%20Qingshan%20Xu%20and%20Kacper%20Kania%20and%20Charlie%20Hewitt%20and%20Octave%20Mariotti%20and%20Lohit%20Petikam%20and%20Julien%20Valentin%20and%20Arno%20Onken%20and%20Oisin%20Mac%20Aodha&entry.1292438233=%20%20We%20introduce%20a%20new%20generative%20approach%20for%20synthesizing%203D%20geometry%20and%0Aimages%20from%20single-view%20collections.%20Most%20existing%20approaches%20predict%0Avolumetric%20density%20to%20render%20multi-view%20consistent%20images.%20By%20employing%0Avolumetric%20rendering%20using%20neural%20radiance%20fields%2C%20they%20inherit%20a%20key%0Alimitation%3A%20the%20generated%20geometry%20is%20noisy%20and%20unconstrained%2C%20limiting%20the%0Aquality%20and%20utility%20of%20the%20output%20meshes.%20To%20address%20this%20issue%2C%20we%20propose%0AGeoGen%2C%20a%20new%20SDF-based%203D%20generative%20model%20trained%20in%20an%20end-to-end%20manner.%0AInitially%2C%20we%20reinterpret%20the%20volumetric%20density%20as%20a%20Signed%20Distance%20Function%0A%28SDF%29.%20This%20allows%20us%20to%20introduce%20useful%20priors%20to%20generate%20valid%20meshes.%0AHowever%2C%20those%20priors%20prevent%20the%20generative%20model%20from%20learning%20details%2C%0Alimiting%20the%20applicability%20of%20the%20method%20to%20real-world%20scenarios.%20To%20alleviate%0Athat%20problem%2C%20we%20make%20the%20transformation%20learnable%20and%20constrain%20the%20rendered%0Adepth%20map%20to%20be%20consistent%20with%20the%20zero-level%20set%20of%20the%20SDF.%20Through%20the%20lens%0Aof%20adversarial%20training%2C%20we%20encourage%20the%20network%20to%20produce%20higher%20fidelity%0Adetails%20on%20the%20output%20meshes.%20For%20evaluation%2C%20we%20introduce%20a%20synthetic%20dataset%0Aof%20human%20avatars%20captured%20from%20360-degree%20camera%20angles%2C%20to%20overcome%20the%0Achallenges%20presented%20by%20real-world%20datasets%2C%20which%20often%20lack%203D%20consistency%0Aand%20do%20not%20cover%20all%20camera%20angles.%20Our%20experiments%20on%20multiple%20datasets%20show%0Athat%20GeoGen%20produces%20visually%20and%20quantitatively%20better%20geometry%20than%20the%0Aprevious%20generative%20models%20based%20on%20neural%20radiance%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04254v3&entry.124074799=Read"},
{"title": "MeshAnything: Artist-Created Mesh Generation with Autoregressive\n  Transformers", "author": "Yiwen Chen and Tong He and Di Huang and Weicai Ye and Sijin Chen and Jiaxiang Tang and Xin Chen and Zhongang Cai and Lei Yang and Gang Yu and Guosheng Lin and Chi Zhang", "abstract": "  Recently, 3D assets created via reconstruction and generation have matched\nthe quality of manually crafted assets, highlighting their potential for\nreplacement. However, this potential is largely unrealized because these assets\nalways need to be converted to meshes for 3D industry applications, and the\nmeshes produced by current mesh extraction methods are significantly inferior\nto Artist-Created Meshes (AMs), i.e., meshes created by human artists.\nSpecifically, current mesh extraction methods rely on dense faces and ignore\ngeometric features, leading to inefficiencies, complicated post-processing, and\nlower representation quality. To address these issues, we introduce\nMeshAnything, a model that treats mesh extraction as a generation problem,\nproducing AMs aligned with specified shapes. By converting 3D assets in any 3D\nrepresentation into AMs, MeshAnything can be integrated with various 3D asset\nproduction methods, thereby enhancing their application across the 3D industry.\nThe architecture of MeshAnything comprises a VQ-VAE and a shape-conditioned\ndecoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE,\nthen train the shape-conditioned decoder-only transformer on this vocabulary\nfor shape-conditioned autoregressive mesh generation. Our extensive experiments\nshow that our method generates AMs with hundreds of times fewer faces,\nsignificantly improving storage, rendering, and simulation efficiencies, while\nachieving precision comparable to previous methods.\n", "link": "http://arxiv.org/abs/2406.10163v1", "date": "2024-06-14", "relevancy": 2.9245, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6082}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5733}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshAnything%3A%20Artist-Created%20Mesh%20Generation%20with%20Autoregressive%0A%20%20Transformers&body=Title%3A%20MeshAnything%3A%20Artist-Created%20Mesh%20Generation%20with%20Autoregressive%0A%20%20Transformers%0AAuthor%3A%20Yiwen%20Chen%20and%20Tong%20He%20and%20Di%20Huang%20and%20Weicai%20Ye%20and%20Sijin%20Chen%20and%20Jiaxiang%20Tang%20and%20Xin%20Chen%20and%20Zhongang%20Cai%20and%20Lei%20Yang%20and%20Gang%20Yu%20and%20Guosheng%20Lin%20and%20Chi%20Zhang%0AAbstract%3A%20%20%20Recently%2C%203D%20assets%20created%20via%20reconstruction%20and%20generation%20have%20matched%0Athe%20quality%20of%20manually%20crafted%20assets%2C%20highlighting%20their%20potential%20for%0Areplacement.%20However%2C%20this%20potential%20is%20largely%20unrealized%20because%20these%20assets%0Aalways%20need%20to%20be%20converted%20to%20meshes%20for%203D%20industry%20applications%2C%20and%20the%0Ameshes%20produced%20by%20current%20mesh%20extraction%20methods%20are%20significantly%20inferior%0Ato%20Artist-Created%20Meshes%20%28AMs%29%2C%20i.e.%2C%20meshes%20created%20by%20human%20artists.%0ASpecifically%2C%20current%20mesh%20extraction%20methods%20rely%20on%20dense%20faces%20and%20ignore%0Ageometric%20features%2C%20leading%20to%20inefficiencies%2C%20complicated%20post-processing%2C%20and%0Alower%20representation%20quality.%20To%20address%20these%20issues%2C%20we%20introduce%0AMeshAnything%2C%20a%20model%20that%20treats%20mesh%20extraction%20as%20a%20generation%20problem%2C%0Aproducing%20AMs%20aligned%20with%20specified%20shapes.%20By%20converting%203D%20assets%20in%20any%203D%0Arepresentation%20into%20AMs%2C%20MeshAnything%20can%20be%20integrated%20with%20various%203D%20asset%0Aproduction%20methods%2C%20thereby%20enhancing%20their%20application%20across%20the%203D%20industry.%0AThe%20architecture%20of%20MeshAnything%20comprises%20a%20VQ-VAE%20and%20a%20shape-conditioned%0Adecoder-only%20transformer.%20We%20first%20learn%20a%20mesh%20vocabulary%20using%20the%20VQ-VAE%2C%0Athen%20train%20the%20shape-conditioned%20decoder-only%20transformer%20on%20this%20vocabulary%0Afor%20shape-conditioned%20autoregressive%20mesh%20generation.%20Our%20extensive%20experiments%0Ashow%20that%20our%20method%20generates%20AMs%20with%20hundreds%20of%20times%20fewer%20faces%2C%0Asignificantly%20improving%20storage%2C%20rendering%2C%20and%20simulation%20efficiencies%2C%20while%0Aachieving%20precision%20comparable%20to%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshAnything%253A%2520Artist-Created%2520Mesh%2520Generation%2520with%2520Autoregressive%250A%2520%2520Transformers%26entry.906535625%3DYiwen%2520Chen%2520and%2520Tong%2520He%2520and%2520Di%2520Huang%2520and%2520Weicai%2520Ye%2520and%2520Sijin%2520Chen%2520and%2520Jiaxiang%2520Tang%2520and%2520Xin%2520Chen%2520and%2520Zhongang%2520Cai%2520and%2520Lei%2520Yang%2520and%2520Gang%2520Yu%2520and%2520Guosheng%2520Lin%2520and%2520Chi%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520assets%2520created%2520via%2520reconstruction%2520and%2520generation%2520have%2520matched%250Athe%2520quality%2520of%2520manually%2520crafted%2520assets%252C%2520highlighting%2520their%2520potential%2520for%250Areplacement.%2520However%252C%2520this%2520potential%2520is%2520largely%2520unrealized%2520because%2520these%2520assets%250Aalways%2520need%2520to%2520be%2520converted%2520to%2520meshes%2520for%25203D%2520industry%2520applications%252C%2520and%2520the%250Ameshes%2520produced%2520by%2520current%2520mesh%2520extraction%2520methods%2520are%2520significantly%2520inferior%250Ato%2520Artist-Created%2520Meshes%2520%2528AMs%2529%252C%2520i.e.%252C%2520meshes%2520created%2520by%2520human%2520artists.%250ASpecifically%252C%2520current%2520mesh%2520extraction%2520methods%2520rely%2520on%2520dense%2520faces%2520and%2520ignore%250Ageometric%2520features%252C%2520leading%2520to%2520inefficiencies%252C%2520complicated%2520post-processing%252C%2520and%250Alower%2520representation%2520quality.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%250AMeshAnything%252C%2520a%2520model%2520that%2520treats%2520mesh%2520extraction%2520as%2520a%2520generation%2520problem%252C%250Aproducing%2520AMs%2520aligned%2520with%2520specified%2520shapes.%2520By%2520converting%25203D%2520assets%2520in%2520any%25203D%250Arepresentation%2520into%2520AMs%252C%2520MeshAnything%2520can%2520be%2520integrated%2520with%2520various%25203D%2520asset%250Aproduction%2520methods%252C%2520thereby%2520enhancing%2520their%2520application%2520across%2520the%25203D%2520industry.%250AThe%2520architecture%2520of%2520MeshAnything%2520comprises%2520a%2520VQ-VAE%2520and%2520a%2520shape-conditioned%250Adecoder-only%2520transformer.%2520We%2520first%2520learn%2520a%2520mesh%2520vocabulary%2520using%2520the%2520VQ-VAE%252C%250Athen%2520train%2520the%2520shape-conditioned%2520decoder-only%2520transformer%2520on%2520this%2520vocabulary%250Afor%2520shape-conditioned%2520autoregressive%2520mesh%2520generation.%2520Our%2520extensive%2520experiments%250Ashow%2520that%2520our%2520method%2520generates%2520AMs%2520with%2520hundreds%2520of%2520times%2520fewer%2520faces%252C%250Asignificantly%2520improving%2520storage%252C%2520rendering%252C%2520and%2520simulation%2520efficiencies%252C%2520while%250Aachieving%2520precision%2520comparable%2520to%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshAnything%3A%20Artist-Created%20Mesh%20Generation%20with%20Autoregressive%0A%20%20Transformers&entry.906535625=Yiwen%20Chen%20and%20Tong%20He%20and%20Di%20Huang%20and%20Weicai%20Ye%20and%20Sijin%20Chen%20and%20Jiaxiang%20Tang%20and%20Xin%20Chen%20and%20Zhongang%20Cai%20and%20Lei%20Yang%20and%20Gang%20Yu%20and%20Guosheng%20Lin%20and%20Chi%20Zhang&entry.1292438233=%20%20Recently%2C%203D%20assets%20created%20via%20reconstruction%20and%20generation%20have%20matched%0Athe%20quality%20of%20manually%20crafted%20assets%2C%20highlighting%20their%20potential%20for%0Areplacement.%20However%2C%20this%20potential%20is%20largely%20unrealized%20because%20these%20assets%0Aalways%20need%20to%20be%20converted%20to%20meshes%20for%203D%20industry%20applications%2C%20and%20the%0Ameshes%20produced%20by%20current%20mesh%20extraction%20methods%20are%20significantly%20inferior%0Ato%20Artist-Created%20Meshes%20%28AMs%29%2C%20i.e.%2C%20meshes%20created%20by%20human%20artists.%0ASpecifically%2C%20current%20mesh%20extraction%20methods%20rely%20on%20dense%20faces%20and%20ignore%0Ageometric%20features%2C%20leading%20to%20inefficiencies%2C%20complicated%20post-processing%2C%20and%0Alower%20representation%20quality.%20To%20address%20these%20issues%2C%20we%20introduce%0AMeshAnything%2C%20a%20model%20that%20treats%20mesh%20extraction%20as%20a%20generation%20problem%2C%0Aproducing%20AMs%20aligned%20with%20specified%20shapes.%20By%20converting%203D%20assets%20in%20any%203D%0Arepresentation%20into%20AMs%2C%20MeshAnything%20can%20be%20integrated%20with%20various%203D%20asset%0Aproduction%20methods%2C%20thereby%20enhancing%20their%20application%20across%20the%203D%20industry.%0AThe%20architecture%20of%20MeshAnything%20comprises%20a%20VQ-VAE%20and%20a%20shape-conditioned%0Adecoder-only%20transformer.%20We%20first%20learn%20a%20mesh%20vocabulary%20using%20the%20VQ-VAE%2C%0Athen%20train%20the%20shape-conditioned%20decoder-only%20transformer%20on%20this%20vocabulary%0Afor%20shape-conditioned%20autoregressive%20mesh%20generation.%20Our%20extensive%20experiments%0Ashow%20that%20our%20method%20generates%20AMs%20with%20hundreds%20of%20times%20fewer%20faces%2C%0Asignificantly%20improving%20storage%2C%20rendering%2C%20and%20simulation%20efficiencies%2C%20while%0Aachieving%20precision%20comparable%20to%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10163v1&entry.124074799=Read"},
{"title": "MeshPose: Unifying DensePose and 3D Body Mesh reconstruction", "author": "Eric-Tuan L\u00ea and Antonis Kakolyris and Petros Koutras and Himmy Tam and Efstratios Skordos and George Papandreou and R\u0131za Alp G\u00fcler and Iasonas Kokkinos", "abstract": "  DensePose provides a pixel-accurate association of images with 3D mesh\ncoordinates, but does not provide a 3D mesh, while Human Mesh Reconstruction\n(HMR) systems have high 2D reprojection error, as measured by DensePose\nlocalization metrics. In this work we introduce MeshPose to jointly tackle\nDensePose and HMR. For this we first introduce new losses that allow us to use\nweak DensePose supervision to accurately localize in 2D a subset of the mesh\nvertices ('VertexPose'). We then lift these vertices to 3D, yielding a low-poly\nbody mesh ('MeshPose'). Our system is trained in an end-to-end manner and is\nthe first HMR method to attain competitive DensePose accuracy, while also being\nlightweight and amenable to efficient inference, making it suitable for\nreal-time AR applications.\n", "link": "http://arxiv.org/abs/2406.10180v1", "date": "2024-06-14", "relevancy": 2.835, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.62}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5474}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshPose%3A%20Unifying%20DensePose%20and%203D%20Body%20Mesh%20reconstruction&body=Title%3A%20MeshPose%3A%20Unifying%20DensePose%20and%203D%20Body%20Mesh%20reconstruction%0AAuthor%3A%20Eric-Tuan%20L%C3%AA%20and%20Antonis%20Kakolyris%20and%20Petros%20Koutras%20and%20Himmy%20Tam%20and%20Efstratios%20Skordos%20and%20George%20Papandreou%20and%20R%C4%B1za%20Alp%20G%C3%BCler%20and%20Iasonas%20Kokkinos%0AAbstract%3A%20%20%20DensePose%20provides%20a%20pixel-accurate%20association%20of%20images%20with%203D%20mesh%0Acoordinates%2C%20but%20does%20not%20provide%20a%203D%20mesh%2C%20while%20Human%20Mesh%20Reconstruction%0A%28HMR%29%20systems%20have%20high%202D%20reprojection%20error%2C%20as%20measured%20by%20DensePose%0Alocalization%20metrics.%20In%20this%20work%20we%20introduce%20MeshPose%20to%20jointly%20tackle%0ADensePose%20and%20HMR.%20For%20this%20we%20first%20introduce%20new%20losses%20that%20allow%20us%20to%20use%0Aweak%20DensePose%20supervision%20to%20accurately%20localize%20in%202D%20a%20subset%20of%20the%20mesh%0Avertices%20%28%27VertexPose%27%29.%20We%20then%20lift%20these%20vertices%20to%203D%2C%20yielding%20a%20low-poly%0Abody%20mesh%20%28%27MeshPose%27%29.%20Our%20system%20is%20trained%20in%20an%20end-to-end%20manner%20and%20is%0Athe%20first%20HMR%20method%20to%20attain%20competitive%20DensePose%20accuracy%2C%20while%20also%20being%0Alightweight%20and%20amenable%20to%20efficient%20inference%2C%20making%20it%20suitable%20for%0Areal-time%20AR%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshPose%253A%2520Unifying%2520DensePose%2520and%25203D%2520Body%2520Mesh%2520reconstruction%26entry.906535625%3DEric-Tuan%2520L%25C3%25AA%2520and%2520Antonis%2520Kakolyris%2520and%2520Petros%2520Koutras%2520and%2520Himmy%2520Tam%2520and%2520Efstratios%2520Skordos%2520and%2520George%2520Papandreou%2520and%2520R%25C4%25B1za%2520Alp%2520G%25C3%25BCler%2520and%2520Iasonas%2520Kokkinos%26entry.1292438233%3D%2520%2520DensePose%2520provides%2520a%2520pixel-accurate%2520association%2520of%2520images%2520with%25203D%2520mesh%250Acoordinates%252C%2520but%2520does%2520not%2520provide%2520a%25203D%2520mesh%252C%2520while%2520Human%2520Mesh%2520Reconstruction%250A%2528HMR%2529%2520systems%2520have%2520high%25202D%2520reprojection%2520error%252C%2520as%2520measured%2520by%2520DensePose%250Alocalization%2520metrics.%2520In%2520this%2520work%2520we%2520introduce%2520MeshPose%2520to%2520jointly%2520tackle%250ADensePose%2520and%2520HMR.%2520For%2520this%2520we%2520first%2520introduce%2520new%2520losses%2520that%2520allow%2520us%2520to%2520use%250Aweak%2520DensePose%2520supervision%2520to%2520accurately%2520localize%2520in%25202D%2520a%2520subset%2520of%2520the%2520mesh%250Avertices%2520%2528%2527VertexPose%2527%2529.%2520We%2520then%2520lift%2520these%2520vertices%2520to%25203D%252C%2520yielding%2520a%2520low-poly%250Abody%2520mesh%2520%2528%2527MeshPose%2527%2529.%2520Our%2520system%2520is%2520trained%2520in%2520an%2520end-to-end%2520manner%2520and%2520is%250Athe%2520first%2520HMR%2520method%2520to%2520attain%2520competitive%2520DensePose%2520accuracy%252C%2520while%2520also%2520being%250Alightweight%2520and%2520amenable%2520to%2520efficient%2520inference%252C%2520making%2520it%2520suitable%2520for%250Areal-time%2520AR%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshPose%3A%20Unifying%20DensePose%20and%203D%20Body%20Mesh%20reconstruction&entry.906535625=Eric-Tuan%20L%C3%AA%20and%20Antonis%20Kakolyris%20and%20Petros%20Koutras%20and%20Himmy%20Tam%20and%20Efstratios%20Skordos%20and%20George%20Papandreou%20and%20R%C4%B1za%20Alp%20G%C3%BCler%20and%20Iasonas%20Kokkinos&entry.1292438233=%20%20DensePose%20provides%20a%20pixel-accurate%20association%20of%20images%20with%203D%20mesh%0Acoordinates%2C%20but%20does%20not%20provide%20a%203D%20mesh%2C%20while%20Human%20Mesh%20Reconstruction%0A%28HMR%29%20systems%20have%20high%202D%20reprojection%20error%2C%20as%20measured%20by%20DensePose%0Alocalization%20metrics.%20In%20this%20work%20we%20introduce%20MeshPose%20to%20jointly%20tackle%0ADensePose%20and%20HMR.%20For%20this%20we%20first%20introduce%20new%20losses%20that%20allow%20us%20to%20use%0Aweak%20DensePose%20supervision%20to%20accurately%20localize%20in%202D%20a%20subset%20of%20the%20mesh%0Avertices%20%28%27VertexPose%27%29.%20We%20then%20lift%20these%20vertices%20to%203D%2C%20yielding%20a%20low-poly%0Abody%20mesh%20%28%27MeshPose%27%29.%20Our%20system%20is%20trained%20in%20an%20end-to-end%20manner%20and%20is%0Athe%20first%20HMR%20method%20to%20attain%20competitive%20DensePose%20accuracy%2C%20while%20also%20being%0Alightweight%20and%20amenable%20to%20efficient%20inference%2C%20making%20it%20suitable%20for%0Areal-time%20AR%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10180v1&entry.124074799=Read"},
{"title": "ALGM: Adaptive Local-then-Global Token Merging for Efficient Semantic\n  Segmentation with Plain Vision Transformers", "author": "Narges Norouzi and Svetlana Orlova and Daan de Geus and Gijs Dubbelman", "abstract": "  This work presents Adaptive Local-then-Global Merging (ALGM), a token\nreduction method for semantic segmentation networks that use plain Vision\nTransformers. ALGM merges tokens in two stages: (1) In the first network layer,\nit merges similar tokens within a small local window and (2) halfway through\nthe network, it merges similar tokens across the entire image. This is\nmotivated by an analysis in which we found that, in those situations, tokens\nwith a high cosine similarity can likely be merged without a drop in\nsegmentation quality. With extensive experiments across multiple datasets and\nnetwork configurations, we show that ALGM not only significantly improves the\nthroughput by up to 100%, but can also enhance the mean IoU by up to +1.1,\nthereby achieving a better trade-off between segmentation quality and\nefficiency than existing methods. Moreover, our approach is adaptive during\ninference, meaning that the same model can be used for optimal efficiency or\naccuracy, depending on the application. Code is available at\nhttps://tue-mps.github.io/ALGM.\n", "link": "http://arxiv.org/abs/2406.09936v1", "date": "2024-06-14", "relevancy": 2.8064, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5728}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5637}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALGM%3A%20Adaptive%20Local-then-Global%20Token%20Merging%20for%20Efficient%20Semantic%0A%20%20Segmentation%20with%20Plain%20Vision%20Transformers&body=Title%3A%20ALGM%3A%20Adaptive%20Local-then-Global%20Token%20Merging%20for%20Efficient%20Semantic%0A%20%20Segmentation%20with%20Plain%20Vision%20Transformers%0AAuthor%3A%20Narges%20Norouzi%20and%20Svetlana%20Orlova%20and%20Daan%20de%20Geus%20and%20Gijs%20Dubbelman%0AAbstract%3A%20%20%20This%20work%20presents%20Adaptive%20Local-then-Global%20Merging%20%28ALGM%29%2C%20a%20token%0Areduction%20method%20for%20semantic%20segmentation%20networks%20that%20use%20plain%20Vision%0ATransformers.%20ALGM%20merges%20tokens%20in%20two%20stages%3A%20%281%29%20In%20the%20first%20network%20layer%2C%0Ait%20merges%20similar%20tokens%20within%20a%20small%20local%20window%20and%20%282%29%20halfway%20through%0Athe%20network%2C%20it%20merges%20similar%20tokens%20across%20the%20entire%20image.%20This%20is%0Amotivated%20by%20an%20analysis%20in%20which%20we%20found%20that%2C%20in%20those%20situations%2C%20tokens%0Awith%20a%20high%20cosine%20similarity%20can%20likely%20be%20merged%20without%20a%20drop%20in%0Asegmentation%20quality.%20With%20extensive%20experiments%20across%20multiple%20datasets%20and%0Anetwork%20configurations%2C%20we%20show%20that%20ALGM%20not%20only%20significantly%20improves%20the%0Athroughput%20by%20up%20to%20100%25%2C%20but%20can%20also%20enhance%20the%20mean%20IoU%20by%20up%20to%20%2B1.1%2C%0Athereby%20achieving%20a%20better%20trade-off%20between%20segmentation%20quality%20and%0Aefficiency%20than%20existing%20methods.%20Moreover%2C%20our%20approach%20is%20adaptive%20during%0Ainference%2C%20meaning%20that%20the%20same%20model%20can%20be%20used%20for%20optimal%20efficiency%20or%0Aaccuracy%2C%20depending%20on%20the%20application.%20Code%20is%20available%20at%0Ahttps%3A//tue-mps.github.io/ALGM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALGM%253A%2520Adaptive%2520Local-then-Global%2520Token%2520Merging%2520for%2520Efficient%2520Semantic%250A%2520%2520Segmentation%2520with%2520Plain%2520Vision%2520Transformers%26entry.906535625%3DNarges%2520Norouzi%2520and%2520Svetlana%2520Orlova%2520and%2520Daan%2520de%2520Geus%2520and%2520Gijs%2520Dubbelman%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520Adaptive%2520Local-then-Global%2520Merging%2520%2528ALGM%2529%252C%2520a%2520token%250Areduction%2520method%2520for%2520semantic%2520segmentation%2520networks%2520that%2520use%2520plain%2520Vision%250ATransformers.%2520ALGM%2520merges%2520tokens%2520in%2520two%2520stages%253A%2520%25281%2529%2520In%2520the%2520first%2520network%2520layer%252C%250Ait%2520merges%2520similar%2520tokens%2520within%2520a%2520small%2520local%2520window%2520and%2520%25282%2529%2520halfway%2520through%250Athe%2520network%252C%2520it%2520merges%2520similar%2520tokens%2520across%2520the%2520entire%2520image.%2520This%2520is%250Amotivated%2520by%2520an%2520analysis%2520in%2520which%2520we%2520found%2520that%252C%2520in%2520those%2520situations%252C%2520tokens%250Awith%2520a%2520high%2520cosine%2520similarity%2520can%2520likely%2520be%2520merged%2520without%2520a%2520drop%2520in%250Asegmentation%2520quality.%2520With%2520extensive%2520experiments%2520across%2520multiple%2520datasets%2520and%250Anetwork%2520configurations%252C%2520we%2520show%2520that%2520ALGM%2520not%2520only%2520significantly%2520improves%2520the%250Athroughput%2520by%2520up%2520to%2520100%2525%252C%2520but%2520can%2520also%2520enhance%2520the%2520mean%2520IoU%2520by%2520up%2520to%2520%252B1.1%252C%250Athereby%2520achieving%2520a%2520better%2520trade-off%2520between%2520segmentation%2520quality%2520and%250Aefficiency%2520than%2520existing%2520methods.%2520Moreover%252C%2520our%2520approach%2520is%2520adaptive%2520during%250Ainference%252C%2520meaning%2520that%2520the%2520same%2520model%2520can%2520be%2520used%2520for%2520optimal%2520efficiency%2520or%250Aaccuracy%252C%2520depending%2520on%2520the%2520application.%2520Code%2520is%2520available%2520at%250Ahttps%253A//tue-mps.github.io/ALGM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALGM%3A%20Adaptive%20Local-then-Global%20Token%20Merging%20for%20Efficient%20Semantic%0A%20%20Segmentation%20with%20Plain%20Vision%20Transformers&entry.906535625=Narges%20Norouzi%20and%20Svetlana%20Orlova%20and%20Daan%20de%20Geus%20and%20Gijs%20Dubbelman&entry.1292438233=%20%20This%20work%20presents%20Adaptive%20Local-then-Global%20Merging%20%28ALGM%29%2C%20a%20token%0Areduction%20method%20for%20semantic%20segmentation%20networks%20that%20use%20plain%20Vision%0ATransformers.%20ALGM%20merges%20tokens%20in%20two%20stages%3A%20%281%29%20In%20the%20first%20network%20layer%2C%0Ait%20merges%20similar%20tokens%20within%20a%20small%20local%20window%20and%20%282%29%20halfway%20through%0Athe%20network%2C%20it%20merges%20similar%20tokens%20across%20the%20entire%20image.%20This%20is%0Amotivated%20by%20an%20analysis%20in%20which%20we%20found%20that%2C%20in%20those%20situations%2C%20tokens%0Awith%20a%20high%20cosine%20similarity%20can%20likely%20be%20merged%20without%20a%20drop%20in%0Asegmentation%20quality.%20With%20extensive%20experiments%20across%20multiple%20datasets%20and%0Anetwork%20configurations%2C%20we%20show%20that%20ALGM%20not%20only%20significantly%20improves%20the%0Athroughput%20by%20up%20to%20100%25%2C%20but%20can%20also%20enhance%20the%20mean%20IoU%20by%20up%20to%20%2B1.1%2C%0Athereby%20achieving%20a%20better%20trade-off%20between%20segmentation%20quality%20and%0Aefficiency%20than%20existing%20methods.%20Moreover%2C%20our%20approach%20is%20adaptive%20during%0Ainference%2C%20meaning%20that%20the%20same%20model%20can%20be%20used%20for%20optimal%20efficiency%20or%0Aaccuracy%2C%20depending%20on%20the%20application.%20Code%20is%20available%20at%0Ahttps%3A//tue-mps.github.io/ALGM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09936v1&entry.124074799=Read"},
{"title": "SSTFB: Leveraging self-supervised pretext learning and temporal\n  self-attention with feature branching for real-time video polyp segmentation", "author": "Ziang Xu and Jens Rittscher and Sharib Ali", "abstract": "  Polyps are early cancer indicators, so assessing occurrences of polyps and\ntheir removal is critical. They are observed through a colonoscopy screening\nprocedure that generates a stream of video frames. Segmenting polyps in their\nnatural video screening procedure has several challenges, such as the\nco-existence of imaging artefacts, motion blur, and floating debris. Most\nexisting polyp segmentation algorithms are developed on curated still image\ndatasets that do not represent real-world colonoscopy. Their performance often\ndegrades on video data. We propose a video polyp segmentation method that\nperforms self-supervised learning as an auxiliary task and a spatial-temporal\nself-attention mechanism for improved representation learning. Our end-to-end\nconfiguration and joint optimisation of losses enable the network to learn more\ndiscriminative contextual features in videos. Our experimental results\ndemonstrate an improvement with respect to several state-of-the-art (SOTA)\nmethods. Our ablation study also confirms that the choice of the proposed joint\nend-to-end training improves network accuracy by over 3% and nearly 10% on both\nthe Dice similarity coefficient and intersection-over-union compared to the\nrecently proposed method PNS+ and Polyp-PVT, respectively. Results on\npreviously unseen video data indicate that the proposed method generalises.\n", "link": "http://arxiv.org/abs/2406.10200v1", "date": "2024-06-14", "relevancy": 2.803, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5949}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5564}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSTFB%3A%20Leveraging%20self-supervised%20pretext%20learning%20and%20temporal%0A%20%20self-attention%20with%20feature%20branching%20for%20real-time%20video%20polyp%20segmentation&body=Title%3A%20SSTFB%3A%20Leveraging%20self-supervised%20pretext%20learning%20and%20temporal%0A%20%20self-attention%20with%20feature%20branching%20for%20real-time%20video%20polyp%20segmentation%0AAuthor%3A%20Ziang%20Xu%20and%20Jens%20Rittscher%20and%20Sharib%20Ali%0AAbstract%3A%20%20%20Polyps%20are%20early%20cancer%20indicators%2C%20so%20assessing%20occurrences%20of%20polyps%20and%0Atheir%20removal%20is%20critical.%20They%20are%20observed%20through%20a%20colonoscopy%20screening%0Aprocedure%20that%20generates%20a%20stream%20of%20video%20frames.%20Segmenting%20polyps%20in%20their%0Anatural%20video%20screening%20procedure%20has%20several%20challenges%2C%20such%20as%20the%0Aco-existence%20of%20imaging%20artefacts%2C%20motion%20blur%2C%20and%20floating%20debris.%20Most%0Aexisting%20polyp%20segmentation%20algorithms%20are%20developed%20on%20curated%20still%20image%0Adatasets%20that%20do%20not%20represent%20real-world%20colonoscopy.%20Their%20performance%20often%0Adegrades%20on%20video%20data.%20We%20propose%20a%20video%20polyp%20segmentation%20method%20that%0Aperforms%20self-supervised%20learning%20as%20an%20auxiliary%20task%20and%20a%20spatial-temporal%0Aself-attention%20mechanism%20for%20improved%20representation%20learning.%20Our%20end-to-end%0Aconfiguration%20and%20joint%20optimisation%20of%20losses%20enable%20the%20network%20to%20learn%20more%0Adiscriminative%20contextual%20features%20in%20videos.%20Our%20experimental%20results%0Ademonstrate%20an%20improvement%20with%20respect%20to%20several%20state-of-the-art%20%28SOTA%29%0Amethods.%20Our%20ablation%20study%20also%20confirms%20that%20the%20choice%20of%20the%20proposed%20joint%0Aend-to-end%20training%20improves%20network%20accuracy%20by%20over%203%25%20and%20nearly%2010%25%20on%20both%0Athe%20Dice%20similarity%20coefficient%20and%20intersection-over-union%20compared%20to%20the%0Arecently%20proposed%20method%20PNS%2B%20and%20Polyp-PVT%2C%20respectively.%20Results%20on%0Apreviously%20unseen%20video%20data%20indicate%20that%20the%20proposed%20method%20generalises.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSTFB%253A%2520Leveraging%2520self-supervised%2520pretext%2520learning%2520and%2520temporal%250A%2520%2520self-attention%2520with%2520feature%2520branching%2520for%2520real-time%2520video%2520polyp%2520segmentation%26entry.906535625%3DZiang%2520Xu%2520and%2520Jens%2520Rittscher%2520and%2520Sharib%2520Ali%26entry.1292438233%3D%2520%2520Polyps%2520are%2520early%2520cancer%2520indicators%252C%2520so%2520assessing%2520occurrences%2520of%2520polyps%2520and%250Atheir%2520removal%2520is%2520critical.%2520They%2520are%2520observed%2520through%2520a%2520colonoscopy%2520screening%250Aprocedure%2520that%2520generates%2520a%2520stream%2520of%2520video%2520frames.%2520Segmenting%2520polyps%2520in%2520their%250Anatural%2520video%2520screening%2520procedure%2520has%2520several%2520challenges%252C%2520such%2520as%2520the%250Aco-existence%2520of%2520imaging%2520artefacts%252C%2520motion%2520blur%252C%2520and%2520floating%2520debris.%2520Most%250Aexisting%2520polyp%2520segmentation%2520algorithms%2520are%2520developed%2520on%2520curated%2520still%2520image%250Adatasets%2520that%2520do%2520not%2520represent%2520real-world%2520colonoscopy.%2520Their%2520performance%2520often%250Adegrades%2520on%2520video%2520data.%2520We%2520propose%2520a%2520video%2520polyp%2520segmentation%2520method%2520that%250Aperforms%2520self-supervised%2520learning%2520as%2520an%2520auxiliary%2520task%2520and%2520a%2520spatial-temporal%250Aself-attention%2520mechanism%2520for%2520improved%2520representation%2520learning.%2520Our%2520end-to-end%250Aconfiguration%2520and%2520joint%2520optimisation%2520of%2520losses%2520enable%2520the%2520network%2520to%2520learn%2520more%250Adiscriminative%2520contextual%2520features%2520in%2520videos.%2520Our%2520experimental%2520results%250Ademonstrate%2520an%2520improvement%2520with%2520respect%2520to%2520several%2520state-of-the-art%2520%2528SOTA%2529%250Amethods.%2520Our%2520ablation%2520study%2520also%2520confirms%2520that%2520the%2520choice%2520of%2520the%2520proposed%2520joint%250Aend-to-end%2520training%2520improves%2520network%2520accuracy%2520by%2520over%25203%2525%2520and%2520nearly%252010%2525%2520on%2520both%250Athe%2520Dice%2520similarity%2520coefficient%2520and%2520intersection-over-union%2520compared%2520to%2520the%250Arecently%2520proposed%2520method%2520PNS%252B%2520and%2520Polyp-PVT%252C%2520respectively.%2520Results%2520on%250Apreviously%2520unseen%2520video%2520data%2520indicate%2520that%2520the%2520proposed%2520method%2520generalises.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSTFB%3A%20Leveraging%20self-supervised%20pretext%20learning%20and%20temporal%0A%20%20self-attention%20with%20feature%20branching%20for%20real-time%20video%20polyp%20segmentation&entry.906535625=Ziang%20Xu%20and%20Jens%20Rittscher%20and%20Sharib%20Ali&entry.1292438233=%20%20Polyps%20are%20early%20cancer%20indicators%2C%20so%20assessing%20occurrences%20of%20polyps%20and%0Atheir%20removal%20is%20critical.%20They%20are%20observed%20through%20a%20colonoscopy%20screening%0Aprocedure%20that%20generates%20a%20stream%20of%20video%20frames.%20Segmenting%20polyps%20in%20their%0Anatural%20video%20screening%20procedure%20has%20several%20challenges%2C%20such%20as%20the%0Aco-existence%20of%20imaging%20artefacts%2C%20motion%20blur%2C%20and%20floating%20debris.%20Most%0Aexisting%20polyp%20segmentation%20algorithms%20are%20developed%20on%20curated%20still%20image%0Adatasets%20that%20do%20not%20represent%20real-world%20colonoscopy.%20Their%20performance%20often%0Adegrades%20on%20video%20data.%20We%20propose%20a%20video%20polyp%20segmentation%20method%20that%0Aperforms%20self-supervised%20learning%20as%20an%20auxiliary%20task%20and%20a%20spatial-temporal%0Aself-attention%20mechanism%20for%20improved%20representation%20learning.%20Our%20end-to-end%0Aconfiguration%20and%20joint%20optimisation%20of%20losses%20enable%20the%20network%20to%20learn%20more%0Adiscriminative%20contextual%20features%20in%20videos.%20Our%20experimental%20results%0Ademonstrate%20an%20improvement%20with%20respect%20to%20several%20state-of-the-art%20%28SOTA%29%0Amethods.%20Our%20ablation%20study%20also%20confirms%20that%20the%20choice%20of%20the%20proposed%20joint%0Aend-to-end%20training%20improves%20network%20accuracy%20by%20over%203%25%20and%20nearly%2010%25%20on%20both%0Athe%20Dice%20similarity%20coefficient%20and%20intersection-over-union%20compared%20to%20the%0Arecently%20proposed%20method%20PNS%2B%20and%20Polyp-PVT%2C%20respectively.%20Results%20on%0Apreviously%20unseen%20video%20data%20indicate%20that%20the%20proposed%20method%20generalises.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10200v1&entry.124074799=Read"},
{"title": "Generalization Beyond Data Imbalance: A Controlled Study on CLIP for\n  Transferable Insights", "author": "Xin Wen and Bingchen Zhao and Yilun Chen and Jiangmiao Pang and Xiaojuan Qi", "abstract": "  Severe data imbalance naturally exists among web-scale vision-language\ndatasets. Despite this, we find CLIP pre-trained thereupon exhibits notable\nrobustness to the data imbalance compared to supervised learning, and\ndemonstrates significant effectiveness in learning generalizable\nrepresentations. With an aim to investigate the reasons behind this finding, we\nconduct controlled experiments to study various underlying factors, and reveal\nthat CLIP's pretext task forms a dynamic classification problem wherein only a\nsubset of classes is present in training. This isolates the bias from dominant\nclasses and implicitly balances the learning signal. Furthermore, the\nrobustness and discriminability of CLIP improve with more descriptive language\nsupervision, larger data scale, and broader open-world concepts, which are\ninaccessible to supervised learning. Our study not only uncovers the mechanisms\nbehind CLIP's generalizability beyond data imbalance but also provides\ntransferable insights for the research community. The findings are validated in\nboth supervised and self-supervised learning, enabling models trained on\nimbalanced data to achieve CLIP-level performance on diverse recognition tasks.\nCode and data are available at: https://github.com/CVMI-Lab/clip-beyond-tail.\n", "link": "http://arxiv.org/abs/2405.21070v2", "date": "2024-06-14", "relevancy": 2.7799, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6063}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5321}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20Beyond%20Data%20Imbalance%3A%20A%20Controlled%20Study%20on%20CLIP%20for%0A%20%20Transferable%20Insights&body=Title%3A%20Generalization%20Beyond%20Data%20Imbalance%3A%20A%20Controlled%20Study%20on%20CLIP%20for%0A%20%20Transferable%20Insights%0AAuthor%3A%20Xin%20Wen%20and%20Bingchen%20Zhao%20and%20Yilun%20Chen%20and%20Jiangmiao%20Pang%20and%20Xiaojuan%20Qi%0AAbstract%3A%20%20%20Severe%20data%20imbalance%20naturally%20exists%20among%20web-scale%20vision-language%0Adatasets.%20Despite%20this%2C%20we%20find%20CLIP%20pre-trained%20thereupon%20exhibits%20notable%0Arobustness%20to%20the%20data%20imbalance%20compared%20to%20supervised%20learning%2C%20and%0Ademonstrates%20significant%20effectiveness%20in%20learning%20generalizable%0Arepresentations.%20With%20an%20aim%20to%20investigate%20the%20reasons%20behind%20this%20finding%2C%20we%0Aconduct%20controlled%20experiments%20to%20study%20various%20underlying%20factors%2C%20and%20reveal%0Athat%20CLIP%27s%20pretext%20task%20forms%20a%20dynamic%20classification%20problem%20wherein%20only%20a%0Asubset%20of%20classes%20is%20present%20in%20training.%20This%20isolates%20the%20bias%20from%20dominant%0Aclasses%20and%20implicitly%20balances%20the%20learning%20signal.%20Furthermore%2C%20the%0Arobustness%20and%20discriminability%20of%20CLIP%20improve%20with%20more%20descriptive%20language%0Asupervision%2C%20larger%20data%20scale%2C%20and%20broader%20open-world%20concepts%2C%20which%20are%0Ainaccessible%20to%20supervised%20learning.%20Our%20study%20not%20only%20uncovers%20the%20mechanisms%0Abehind%20CLIP%27s%20generalizability%20beyond%20data%20imbalance%20but%20also%20provides%0Atransferable%20insights%20for%20the%20research%20community.%20The%20findings%20are%20validated%20in%0Aboth%20supervised%20and%20self-supervised%20learning%2C%20enabling%20models%20trained%20on%0Aimbalanced%20data%20to%20achieve%20CLIP-level%20performance%20on%20diverse%20recognition%20tasks.%0ACode%20and%20data%20are%20available%20at%3A%20https%3A//github.com/CVMI-Lab/clip-beyond-tail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21070v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520Beyond%2520Data%2520Imbalance%253A%2520A%2520Controlled%2520Study%2520on%2520CLIP%2520for%250A%2520%2520Transferable%2520Insights%26entry.906535625%3DXin%2520Wen%2520and%2520Bingchen%2520Zhao%2520and%2520Yilun%2520Chen%2520and%2520Jiangmiao%2520Pang%2520and%2520Xiaojuan%2520Qi%26entry.1292438233%3D%2520%2520Severe%2520data%2520imbalance%2520naturally%2520exists%2520among%2520web-scale%2520vision-language%250Adatasets.%2520Despite%2520this%252C%2520we%2520find%2520CLIP%2520pre-trained%2520thereupon%2520exhibits%2520notable%250Arobustness%2520to%2520the%2520data%2520imbalance%2520compared%2520to%2520supervised%2520learning%252C%2520and%250Ademonstrates%2520significant%2520effectiveness%2520in%2520learning%2520generalizable%250Arepresentations.%2520With%2520an%2520aim%2520to%2520investigate%2520the%2520reasons%2520behind%2520this%2520finding%252C%2520we%250Aconduct%2520controlled%2520experiments%2520to%2520study%2520various%2520underlying%2520factors%252C%2520and%2520reveal%250Athat%2520CLIP%2527s%2520pretext%2520task%2520forms%2520a%2520dynamic%2520classification%2520problem%2520wherein%2520only%2520a%250Asubset%2520of%2520classes%2520is%2520present%2520in%2520training.%2520This%2520isolates%2520the%2520bias%2520from%2520dominant%250Aclasses%2520and%2520implicitly%2520balances%2520the%2520learning%2520signal.%2520Furthermore%252C%2520the%250Arobustness%2520and%2520discriminability%2520of%2520CLIP%2520improve%2520with%2520more%2520descriptive%2520language%250Asupervision%252C%2520larger%2520data%2520scale%252C%2520and%2520broader%2520open-world%2520concepts%252C%2520which%2520are%250Ainaccessible%2520to%2520supervised%2520learning.%2520Our%2520study%2520not%2520only%2520uncovers%2520the%2520mechanisms%250Abehind%2520CLIP%2527s%2520generalizability%2520beyond%2520data%2520imbalance%2520but%2520also%2520provides%250Atransferable%2520insights%2520for%2520the%2520research%2520community.%2520The%2520findings%2520are%2520validated%2520in%250Aboth%2520supervised%2520and%2520self-supervised%2520learning%252C%2520enabling%2520models%2520trained%2520on%250Aimbalanced%2520data%2520to%2520achieve%2520CLIP-level%2520performance%2520on%2520diverse%2520recognition%2520tasks.%250ACode%2520and%2520data%2520are%2520available%2520at%253A%2520https%253A//github.com/CVMI-Lab/clip-beyond-tail.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21070v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20Beyond%20Data%20Imbalance%3A%20A%20Controlled%20Study%20on%20CLIP%20for%0A%20%20Transferable%20Insights&entry.906535625=Xin%20Wen%20and%20Bingchen%20Zhao%20and%20Yilun%20Chen%20and%20Jiangmiao%20Pang%20and%20Xiaojuan%20Qi&entry.1292438233=%20%20Severe%20data%20imbalance%20naturally%20exists%20among%20web-scale%20vision-language%0Adatasets.%20Despite%20this%2C%20we%20find%20CLIP%20pre-trained%20thereupon%20exhibits%20notable%0Arobustness%20to%20the%20data%20imbalance%20compared%20to%20supervised%20learning%2C%20and%0Ademonstrates%20significant%20effectiveness%20in%20learning%20generalizable%0Arepresentations.%20With%20an%20aim%20to%20investigate%20the%20reasons%20behind%20this%20finding%2C%20we%0Aconduct%20controlled%20experiments%20to%20study%20various%20underlying%20factors%2C%20and%20reveal%0Athat%20CLIP%27s%20pretext%20task%20forms%20a%20dynamic%20classification%20problem%20wherein%20only%20a%0Asubset%20of%20classes%20is%20present%20in%20training.%20This%20isolates%20the%20bias%20from%20dominant%0Aclasses%20and%20implicitly%20balances%20the%20learning%20signal.%20Furthermore%2C%20the%0Arobustness%20and%20discriminability%20of%20CLIP%20improve%20with%20more%20descriptive%20language%0Asupervision%2C%20larger%20data%20scale%2C%20and%20broader%20open-world%20concepts%2C%20which%20are%0Ainaccessible%20to%20supervised%20learning.%20Our%20study%20not%20only%20uncovers%20the%20mechanisms%0Abehind%20CLIP%27s%20generalizability%20beyond%20data%20imbalance%20but%20also%20provides%0Atransferable%20insights%20for%20the%20research%20community.%20The%20findings%20are%20validated%20in%0Aboth%20supervised%20and%20self-supervised%20learning%2C%20enabling%20models%20trained%20on%0Aimbalanced%20data%20to%20achieve%20CLIP-level%20performance%20on%20diverse%20recognition%20tasks.%0ACode%20and%20data%20are%20available%20at%3A%20https%3A//github.com/CVMI-Lab/clip-beyond-tail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21070v2&entry.124074799=Read"},
{"title": "OpenECAD: An Efficient Visual Language Model for Computer-Aided Design", "author": "Zhe Yuan and Jianqi Shi", "abstract": "  Computer-aided design (CAD) tools are utilized in the manufacturing industry\nfor modeling everything from cups to spacecraft. These programs are complex to\nuse and typically require years of training and experience to master.\nStructured and well-constrained 2D sketches and 3D constructions are crucial\ncomponents of CAD modeling. A well-executed CAD model can be seamlessly\nintegrated into the manufacturing process, thereby enhancing production\nefficiency. Deep generative models of 3D shapes and 3D object reconstruction\nmodels has garnered significant research interest. However, most of these\nmodels are represented in discrete forms. Moreover, the few models based on CAD\noperations often have substantial input restrictions. In this work, we\nfine-tuned pre-trained models to create OpenECAD (0.55B, 0.89B, and 4.2B),\nleveraging the visual, logical, coding, and general capabilities of visual\nlanguage models. OpenECAD can process images of 3D designs as input and\ngenerate highly structured 2D sketches and 3D construction commands. These\noutputs can be directly used with existing CAD tools' APIs to generate project\nfiles. To train our network, we created a new CAD dataset. This dataset is\nbased on existing public CAD datasets, with adjustments and augmentations to\nmeet the requirements of ~VLM training.\n", "link": "http://arxiv.org/abs/2406.09913v1", "date": "2024-06-14", "relevancy": 2.6991, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5458}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5381}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenECAD%3A%20An%20Efficient%20Visual%20Language%20Model%20for%20Computer-Aided%20Design&body=Title%3A%20OpenECAD%3A%20An%20Efficient%20Visual%20Language%20Model%20for%20Computer-Aided%20Design%0AAuthor%3A%20Zhe%20Yuan%20and%20Jianqi%20Shi%0AAbstract%3A%20%20%20Computer-aided%20design%20%28CAD%29%20tools%20are%20utilized%20in%20the%20manufacturing%20industry%0Afor%20modeling%20everything%20from%20cups%20to%20spacecraft.%20These%20programs%20are%20complex%20to%0Ause%20and%20typically%20require%20years%20of%20training%20and%20experience%20to%20master.%0AStructured%20and%20well-constrained%202D%20sketches%20and%203D%20constructions%20are%20crucial%0Acomponents%20of%20CAD%20modeling.%20A%20well-executed%20CAD%20model%20can%20be%20seamlessly%0Aintegrated%20into%20the%20manufacturing%20process%2C%20thereby%20enhancing%20production%0Aefficiency.%20Deep%20generative%20models%20of%203D%20shapes%20and%203D%20object%20reconstruction%0Amodels%20has%20garnered%20significant%20research%20interest.%20However%2C%20most%20of%20these%0Amodels%20are%20represented%20in%20discrete%20forms.%20Moreover%2C%20the%20few%20models%20based%20on%20CAD%0Aoperations%20often%20have%20substantial%20input%20restrictions.%20In%20this%20work%2C%20we%0Afine-tuned%20pre-trained%20models%20to%20create%20OpenECAD%20%280.55B%2C%200.89B%2C%20and%204.2B%29%2C%0Aleveraging%20the%20visual%2C%20logical%2C%20coding%2C%20and%20general%20capabilities%20of%20visual%0Alanguage%20models.%20OpenECAD%20can%20process%20images%20of%203D%20designs%20as%20input%20and%0Agenerate%20highly%20structured%202D%20sketches%20and%203D%20construction%20commands.%20These%0Aoutputs%20can%20be%20directly%20used%20with%20existing%20CAD%20tools%27%20APIs%20to%20generate%20project%0Afiles.%20To%20train%20our%20network%2C%20we%20created%20a%20new%20CAD%20dataset.%20This%20dataset%20is%0Abased%20on%20existing%20public%20CAD%20datasets%2C%20with%20adjustments%20and%20augmentations%20to%0Ameet%20the%20requirements%20of%20~VLM%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenECAD%253A%2520An%2520Efficient%2520Visual%2520Language%2520Model%2520for%2520Computer-Aided%2520Design%26entry.906535625%3DZhe%2520Yuan%2520and%2520Jianqi%2520Shi%26entry.1292438233%3D%2520%2520Computer-aided%2520design%2520%2528CAD%2529%2520tools%2520are%2520utilized%2520in%2520the%2520manufacturing%2520industry%250Afor%2520modeling%2520everything%2520from%2520cups%2520to%2520spacecraft.%2520These%2520programs%2520are%2520complex%2520to%250Ause%2520and%2520typically%2520require%2520years%2520of%2520training%2520and%2520experience%2520to%2520master.%250AStructured%2520and%2520well-constrained%25202D%2520sketches%2520and%25203D%2520constructions%2520are%2520crucial%250Acomponents%2520of%2520CAD%2520modeling.%2520A%2520well-executed%2520CAD%2520model%2520can%2520be%2520seamlessly%250Aintegrated%2520into%2520the%2520manufacturing%2520process%252C%2520thereby%2520enhancing%2520production%250Aefficiency.%2520Deep%2520generative%2520models%2520of%25203D%2520shapes%2520and%25203D%2520object%2520reconstruction%250Amodels%2520has%2520garnered%2520significant%2520research%2520interest.%2520However%252C%2520most%2520of%2520these%250Amodels%2520are%2520represented%2520in%2520discrete%2520forms.%2520Moreover%252C%2520the%2520few%2520models%2520based%2520on%2520CAD%250Aoperations%2520often%2520have%2520substantial%2520input%2520restrictions.%2520In%2520this%2520work%252C%2520we%250Afine-tuned%2520pre-trained%2520models%2520to%2520create%2520OpenECAD%2520%25280.55B%252C%25200.89B%252C%2520and%25204.2B%2529%252C%250Aleveraging%2520the%2520visual%252C%2520logical%252C%2520coding%252C%2520and%2520general%2520capabilities%2520of%2520visual%250Alanguage%2520models.%2520OpenECAD%2520can%2520process%2520images%2520of%25203D%2520designs%2520as%2520input%2520and%250Agenerate%2520highly%2520structured%25202D%2520sketches%2520and%25203D%2520construction%2520commands.%2520These%250Aoutputs%2520can%2520be%2520directly%2520used%2520with%2520existing%2520CAD%2520tools%2527%2520APIs%2520to%2520generate%2520project%250Afiles.%2520To%2520train%2520our%2520network%252C%2520we%2520created%2520a%2520new%2520CAD%2520dataset.%2520This%2520dataset%2520is%250Abased%2520on%2520existing%2520public%2520CAD%2520datasets%252C%2520with%2520adjustments%2520and%2520augmentations%2520to%250Ameet%2520the%2520requirements%2520of%2520~VLM%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenECAD%3A%20An%20Efficient%20Visual%20Language%20Model%20for%20Computer-Aided%20Design&entry.906535625=Zhe%20Yuan%20and%20Jianqi%20Shi&entry.1292438233=%20%20Computer-aided%20design%20%28CAD%29%20tools%20are%20utilized%20in%20the%20manufacturing%20industry%0Afor%20modeling%20everything%20from%20cups%20to%20spacecraft.%20These%20programs%20are%20complex%20to%0Ause%20and%20typically%20require%20years%20of%20training%20and%20experience%20to%20master.%0AStructured%20and%20well-constrained%202D%20sketches%20and%203D%20constructions%20are%20crucial%0Acomponents%20of%20CAD%20modeling.%20A%20well-executed%20CAD%20model%20can%20be%20seamlessly%0Aintegrated%20into%20the%20manufacturing%20process%2C%20thereby%20enhancing%20production%0Aefficiency.%20Deep%20generative%20models%20of%203D%20shapes%20and%203D%20object%20reconstruction%0Amodels%20has%20garnered%20significant%20research%20interest.%20However%2C%20most%20of%20these%0Amodels%20are%20represented%20in%20discrete%20forms.%20Moreover%2C%20the%20few%20models%20based%20on%20CAD%0Aoperations%20often%20have%20substantial%20input%20restrictions.%20In%20this%20work%2C%20we%0Afine-tuned%20pre-trained%20models%20to%20create%20OpenECAD%20%280.55B%2C%200.89B%2C%20and%204.2B%29%2C%0Aleveraging%20the%20visual%2C%20logical%2C%20coding%2C%20and%20general%20capabilities%20of%20visual%0Alanguage%20models.%20OpenECAD%20can%20process%20images%20of%203D%20designs%20as%20input%20and%0Agenerate%20highly%20structured%202D%20sketches%20and%203D%20construction%20commands.%20These%0Aoutputs%20can%20be%20directly%20used%20with%20existing%20CAD%20tools%27%20APIs%20to%20generate%20project%0Afiles.%20To%20train%20our%20network%2C%20we%20created%20a%20new%20CAD%20dataset.%20This%20dataset%20is%0Abased%20on%20existing%20public%20CAD%20datasets%2C%20with%20adjustments%20and%20augmentations%20to%0Ameet%20the%20requirements%20of%20~VLM%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09913v1&entry.124074799=Read"},
{"title": "Training-free Camera Control for Video Generation", "author": "Chen Hou and Guoqiang Wei and Yan Zeng and Zhibo Chen", "abstract": "  We propose a training-free and robust solution to offer camera movement\ncontrol for off-the-shelf video diffusion models. Unlike previous work, our\nmethod does not require any supervised finetuning on camera-annotated datasets\nor self-supervised training via data augmentation. Instead, it can be plugged\nand played with most pretrained video diffusion models and generate camera\ncontrollable videos with a single image or text prompt as input. The\ninspiration of our work comes from the layout prior that intermediate latents\nhold towards generated results, thus rearranging noisy pixels in them will make\noutput content reallocated as well. As camera move could also be seen as a kind\nof pixel rearrangement caused by perspective change, videos could be\nreorganized following specific camera motion if their noisy latents change\naccordingly. Established on this, we propose our method CamTrol, which enables\nrobust camera control for video diffusion models. It is achieved by a two-stage\nprocess. First, we model image layout rearrangement through explicit camera\nmovement in 3D point cloud space. Second, we generate videos with camera motion\nusing layout prior of noisy latents formed by a series of rearranged images.\nExtensive experiments have demonstrated the robustness our method holds in\ncontrolling camera motion of generated videos. Furthermore, we show that our\nmethod can produce impressive results in generating 3D rotation videos with\ndynamic content. Project page at https://lifedecoder.github.io/CamTrol/.\n", "link": "http://arxiv.org/abs/2406.10126v1", "date": "2024-06-14", "relevancy": 2.6518, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6857}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6651}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-free%20Camera%20Control%20for%20Video%20Generation&body=Title%3A%20Training-free%20Camera%20Control%20for%20Video%20Generation%0AAuthor%3A%20Chen%20Hou%20and%20Guoqiang%20Wei%20and%20Yan%20Zeng%20and%20Zhibo%20Chen%0AAbstract%3A%20%20%20We%20propose%20a%20training-free%20and%20robust%20solution%20to%20offer%20camera%20movement%0Acontrol%20for%20off-the-shelf%20video%20diffusion%20models.%20Unlike%20previous%20work%2C%20our%0Amethod%20does%20not%20require%20any%20supervised%20finetuning%20on%20camera-annotated%20datasets%0Aor%20self-supervised%20training%20via%20data%20augmentation.%20Instead%2C%20it%20can%20be%20plugged%0Aand%20played%20with%20most%20pretrained%20video%20diffusion%20models%20and%20generate%20camera%0Acontrollable%20videos%20with%20a%20single%20image%20or%20text%20prompt%20as%20input.%20The%0Ainspiration%20of%20our%20work%20comes%20from%20the%20layout%20prior%20that%20intermediate%20latents%0Ahold%20towards%20generated%20results%2C%20thus%20rearranging%20noisy%20pixels%20in%20them%20will%20make%0Aoutput%20content%20reallocated%20as%20well.%20As%20camera%20move%20could%20also%20be%20seen%20as%20a%20kind%0Aof%20pixel%20rearrangement%20caused%20by%20perspective%20change%2C%20videos%20could%20be%0Areorganized%20following%20specific%20camera%20motion%20if%20their%20noisy%20latents%20change%0Aaccordingly.%20Established%20on%20this%2C%20we%20propose%20our%20method%20CamTrol%2C%20which%20enables%0Arobust%20camera%20control%20for%20video%20diffusion%20models.%20It%20is%20achieved%20by%20a%20two-stage%0Aprocess.%20First%2C%20we%20model%20image%20layout%20rearrangement%20through%20explicit%20camera%0Amovement%20in%203D%20point%20cloud%20space.%20Second%2C%20we%20generate%20videos%20with%20camera%20motion%0Ausing%20layout%20prior%20of%20noisy%20latents%20formed%20by%20a%20series%20of%20rearranged%20images.%0AExtensive%20experiments%20have%20demonstrated%20the%20robustness%20our%20method%20holds%20in%0Acontrolling%20camera%20motion%20of%20generated%20videos.%20Furthermore%2C%20we%20show%20that%20our%0Amethod%20can%20produce%20impressive%20results%20in%20generating%203D%20rotation%20videos%20with%0Adynamic%20content.%20Project%20page%20at%20https%3A//lifedecoder.github.io/CamTrol/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-free%2520Camera%2520Control%2520for%2520Video%2520Generation%26entry.906535625%3DChen%2520Hou%2520and%2520Guoqiang%2520Wei%2520and%2520Yan%2520Zeng%2520and%2520Zhibo%2520Chen%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520training-free%2520and%2520robust%2520solution%2520to%2520offer%2520camera%2520movement%250Acontrol%2520for%2520off-the-shelf%2520video%2520diffusion%2520models.%2520Unlike%2520previous%2520work%252C%2520our%250Amethod%2520does%2520not%2520require%2520any%2520supervised%2520finetuning%2520on%2520camera-annotated%2520datasets%250Aor%2520self-supervised%2520training%2520via%2520data%2520augmentation.%2520Instead%252C%2520it%2520can%2520be%2520plugged%250Aand%2520played%2520with%2520most%2520pretrained%2520video%2520diffusion%2520models%2520and%2520generate%2520camera%250Acontrollable%2520videos%2520with%2520a%2520single%2520image%2520or%2520text%2520prompt%2520as%2520input.%2520The%250Ainspiration%2520of%2520our%2520work%2520comes%2520from%2520the%2520layout%2520prior%2520that%2520intermediate%2520latents%250Ahold%2520towards%2520generated%2520results%252C%2520thus%2520rearranging%2520noisy%2520pixels%2520in%2520them%2520will%2520make%250Aoutput%2520content%2520reallocated%2520as%2520well.%2520As%2520camera%2520move%2520could%2520also%2520be%2520seen%2520as%2520a%2520kind%250Aof%2520pixel%2520rearrangement%2520caused%2520by%2520perspective%2520change%252C%2520videos%2520could%2520be%250Areorganized%2520following%2520specific%2520camera%2520motion%2520if%2520their%2520noisy%2520latents%2520change%250Aaccordingly.%2520Established%2520on%2520this%252C%2520we%2520propose%2520our%2520method%2520CamTrol%252C%2520which%2520enables%250Arobust%2520camera%2520control%2520for%2520video%2520diffusion%2520models.%2520It%2520is%2520achieved%2520by%2520a%2520two-stage%250Aprocess.%2520First%252C%2520we%2520model%2520image%2520layout%2520rearrangement%2520through%2520explicit%2520camera%250Amovement%2520in%25203D%2520point%2520cloud%2520space.%2520Second%252C%2520we%2520generate%2520videos%2520with%2520camera%2520motion%250Ausing%2520layout%2520prior%2520of%2520noisy%2520latents%2520formed%2520by%2520a%2520series%2520of%2520rearranged%2520images.%250AExtensive%2520experiments%2520have%2520demonstrated%2520the%2520robustness%2520our%2520method%2520holds%2520in%250Acontrolling%2520camera%2520motion%2520of%2520generated%2520videos.%2520Furthermore%252C%2520we%2520show%2520that%2520our%250Amethod%2520can%2520produce%2520impressive%2520results%2520in%2520generating%25203D%2520rotation%2520videos%2520with%250Adynamic%2520content.%2520Project%2520page%2520at%2520https%253A//lifedecoder.github.io/CamTrol/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-free%20Camera%20Control%20for%20Video%20Generation&entry.906535625=Chen%20Hou%20and%20Guoqiang%20Wei%20and%20Yan%20Zeng%20and%20Zhibo%20Chen&entry.1292438233=%20%20We%20propose%20a%20training-free%20and%20robust%20solution%20to%20offer%20camera%20movement%0Acontrol%20for%20off-the-shelf%20video%20diffusion%20models.%20Unlike%20previous%20work%2C%20our%0Amethod%20does%20not%20require%20any%20supervised%20finetuning%20on%20camera-annotated%20datasets%0Aor%20self-supervised%20training%20via%20data%20augmentation.%20Instead%2C%20it%20can%20be%20plugged%0Aand%20played%20with%20most%20pretrained%20video%20diffusion%20models%20and%20generate%20camera%0Acontrollable%20videos%20with%20a%20single%20image%20or%20text%20prompt%20as%20input.%20The%0Ainspiration%20of%20our%20work%20comes%20from%20the%20layout%20prior%20that%20intermediate%20latents%0Ahold%20towards%20generated%20results%2C%20thus%20rearranging%20noisy%20pixels%20in%20them%20will%20make%0Aoutput%20content%20reallocated%20as%20well.%20As%20camera%20move%20could%20also%20be%20seen%20as%20a%20kind%0Aof%20pixel%20rearrangement%20caused%20by%20perspective%20change%2C%20videos%20could%20be%0Areorganized%20following%20specific%20camera%20motion%20if%20their%20noisy%20latents%20change%0Aaccordingly.%20Established%20on%20this%2C%20we%20propose%20our%20method%20CamTrol%2C%20which%20enables%0Arobust%20camera%20control%20for%20video%20diffusion%20models.%20It%20is%20achieved%20by%20a%20two-stage%0Aprocess.%20First%2C%20we%20model%20image%20layout%20rearrangement%20through%20explicit%20camera%0Amovement%20in%203D%20point%20cloud%20space.%20Second%2C%20we%20generate%20videos%20with%20camera%20motion%0Ausing%20layout%20prior%20of%20noisy%20latents%20formed%20by%20a%20series%20of%20rearranged%20images.%0AExtensive%20experiments%20have%20demonstrated%20the%20robustness%20our%20method%20holds%20in%0Acontrolling%20camera%20motion%20of%20generated%20videos.%20Furthermore%2C%20we%20show%20that%20our%0Amethod%20can%20produce%20impressive%20results%20in%20generating%203D%20rotation%20videos%20with%0Adynamic%20content.%20Project%20page%20at%20https%3A//lifedecoder.github.io/CamTrol/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10126v1&entry.124074799=Read"},
{"title": "Comparison of fine-tuning strategies for transfer learning in medical\n  image classification", "author": "Ana Davila and Jacinto Colan and Yasuhisa Hasegawa", "abstract": "  In the context of medical imaging and machine learning, one of the most\npressing challenges is the effective adaptation of pre-trained models to\nspecialized medical contexts. Despite the availability of advanced pre-trained\nmodels, their direct application to the highly specialized and diverse field of\nmedical imaging often falls short due to the unique characteristics of medical\ndata. This study provides a comprehensive analysis on the performance of\nvarious fine-tuning methods applied to pre-trained models across a spectrum of\nmedical imaging domains, including X-ray, MRI, Histology, Dermoscopy, and\nEndoscopic surgery. We evaluated eight fine-tuning strategies, including\nstandard techniques such as fine-tuning all layers or fine-tuning only the\nclassifier layers, alongside methods such as gradually unfreezing layers,\nregularization based fine-tuning and adaptive learning rates. We selected three\nwell-established CNN architectures (ResNet-50, DenseNet-121, and VGG-19) to\ncover a range of learning and feature extraction scenarios. Although our\nresults indicate that the efficacy of these fine-tuning methods significantly\nvaries depending on both the architecture and the medical imaging type,\nstrategies such as combining Linear Probing with Full Fine-tuning resulted in\nnotable improvements in over 50% of the evaluated cases, demonstrating general\neffectiveness across medical domains. Moreover, Auto-RGN, which dynamically\nadjusts learning rates, led to performance enhancements of up to 11% for\nspecific modalities. Additionally, the DenseNet architecture showed more\npronounced benefits from alternative fine-tuning approaches compared to\ntraditional full fine-tuning. This work not only provides valuable insights for\noptimizing pre-trained models in medical image analysis but also suggests the\npotential for future research into more advanced architectures and fine-tuning\nmethods.\n", "link": "http://arxiv.org/abs/2406.10050v1", "date": "2024-06-14", "relevancy": 2.6503, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5324}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5307}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparison%20of%20fine-tuning%20strategies%20for%20transfer%20learning%20in%20medical%0A%20%20image%20classification&body=Title%3A%20Comparison%20of%20fine-tuning%20strategies%20for%20transfer%20learning%20in%20medical%0A%20%20image%20classification%0AAuthor%3A%20Ana%20Davila%20and%20Jacinto%20Colan%20and%20Yasuhisa%20Hasegawa%0AAbstract%3A%20%20%20In%20the%20context%20of%20medical%20imaging%20and%20machine%20learning%2C%20one%20of%20the%20most%0Apressing%20challenges%20is%20the%20effective%20adaptation%20of%20pre-trained%20models%20to%0Aspecialized%20medical%20contexts.%20Despite%20the%20availability%20of%20advanced%20pre-trained%0Amodels%2C%20their%20direct%20application%20to%20the%20highly%20specialized%20and%20diverse%20field%20of%0Amedical%20imaging%20often%20falls%20short%20due%20to%20the%20unique%20characteristics%20of%20medical%0Adata.%20This%20study%20provides%20a%20comprehensive%20analysis%20on%20the%20performance%20of%0Avarious%20fine-tuning%20methods%20applied%20to%20pre-trained%20models%20across%20a%20spectrum%20of%0Amedical%20imaging%20domains%2C%20including%20X-ray%2C%20MRI%2C%20Histology%2C%20Dermoscopy%2C%20and%0AEndoscopic%20surgery.%20We%20evaluated%20eight%20fine-tuning%20strategies%2C%20including%0Astandard%20techniques%20such%20as%20fine-tuning%20all%20layers%20or%20fine-tuning%20only%20the%0Aclassifier%20layers%2C%20alongside%20methods%20such%20as%20gradually%20unfreezing%20layers%2C%0Aregularization%20based%20fine-tuning%20and%20adaptive%20learning%20rates.%20We%20selected%20three%0Awell-established%20CNN%20architectures%20%28ResNet-50%2C%20DenseNet-121%2C%20and%20VGG-19%29%20to%0Acover%20a%20range%20of%20learning%20and%20feature%20extraction%20scenarios.%20Although%20our%0Aresults%20indicate%20that%20the%20efficacy%20of%20these%20fine-tuning%20methods%20significantly%0Avaries%20depending%20on%20both%20the%20architecture%20and%20the%20medical%20imaging%20type%2C%0Astrategies%20such%20as%20combining%20Linear%20Probing%20with%20Full%20Fine-tuning%20resulted%20in%0Anotable%20improvements%20in%20over%2050%25%20of%20the%20evaluated%20cases%2C%20demonstrating%20general%0Aeffectiveness%20across%20medical%20domains.%20Moreover%2C%20Auto-RGN%2C%20which%20dynamically%0Aadjusts%20learning%20rates%2C%20led%20to%20performance%20enhancements%20of%20up%20to%2011%25%20for%0Aspecific%20modalities.%20Additionally%2C%20the%20DenseNet%20architecture%20showed%20more%0Apronounced%20benefits%20from%20alternative%20fine-tuning%20approaches%20compared%20to%0Atraditional%20full%20fine-tuning.%20This%20work%20not%20only%20provides%20valuable%20insights%20for%0Aoptimizing%20pre-trained%20models%20in%20medical%20image%20analysis%20but%20also%20suggests%20the%0Apotential%20for%20future%20research%20into%20more%20advanced%20architectures%20and%20fine-tuning%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparison%2520of%2520fine-tuning%2520strategies%2520for%2520transfer%2520learning%2520in%2520medical%250A%2520%2520image%2520classification%26entry.906535625%3DAna%2520Davila%2520and%2520Jacinto%2520Colan%2520and%2520Yasuhisa%2520Hasegawa%26entry.1292438233%3D%2520%2520In%2520the%2520context%2520of%2520medical%2520imaging%2520and%2520machine%2520learning%252C%2520one%2520of%2520the%2520most%250Apressing%2520challenges%2520is%2520the%2520effective%2520adaptation%2520of%2520pre-trained%2520models%2520to%250Aspecialized%2520medical%2520contexts.%2520Despite%2520the%2520availability%2520of%2520advanced%2520pre-trained%250Amodels%252C%2520their%2520direct%2520application%2520to%2520the%2520highly%2520specialized%2520and%2520diverse%2520field%2520of%250Amedical%2520imaging%2520often%2520falls%2520short%2520due%2520to%2520the%2520unique%2520characteristics%2520of%2520medical%250Adata.%2520This%2520study%2520provides%2520a%2520comprehensive%2520analysis%2520on%2520the%2520performance%2520of%250Avarious%2520fine-tuning%2520methods%2520applied%2520to%2520pre-trained%2520models%2520across%2520a%2520spectrum%2520of%250Amedical%2520imaging%2520domains%252C%2520including%2520X-ray%252C%2520MRI%252C%2520Histology%252C%2520Dermoscopy%252C%2520and%250AEndoscopic%2520surgery.%2520We%2520evaluated%2520eight%2520fine-tuning%2520strategies%252C%2520including%250Astandard%2520techniques%2520such%2520as%2520fine-tuning%2520all%2520layers%2520or%2520fine-tuning%2520only%2520the%250Aclassifier%2520layers%252C%2520alongside%2520methods%2520such%2520as%2520gradually%2520unfreezing%2520layers%252C%250Aregularization%2520based%2520fine-tuning%2520and%2520adaptive%2520learning%2520rates.%2520We%2520selected%2520three%250Awell-established%2520CNN%2520architectures%2520%2528ResNet-50%252C%2520DenseNet-121%252C%2520and%2520VGG-19%2529%2520to%250Acover%2520a%2520range%2520of%2520learning%2520and%2520feature%2520extraction%2520scenarios.%2520Although%2520our%250Aresults%2520indicate%2520that%2520the%2520efficacy%2520of%2520these%2520fine-tuning%2520methods%2520significantly%250Avaries%2520depending%2520on%2520both%2520the%2520architecture%2520and%2520the%2520medical%2520imaging%2520type%252C%250Astrategies%2520such%2520as%2520combining%2520Linear%2520Probing%2520with%2520Full%2520Fine-tuning%2520resulted%2520in%250Anotable%2520improvements%2520in%2520over%252050%2525%2520of%2520the%2520evaluated%2520cases%252C%2520demonstrating%2520general%250Aeffectiveness%2520across%2520medical%2520domains.%2520Moreover%252C%2520Auto-RGN%252C%2520which%2520dynamically%250Aadjusts%2520learning%2520rates%252C%2520led%2520to%2520performance%2520enhancements%2520of%2520up%2520to%252011%2525%2520for%250Aspecific%2520modalities.%2520Additionally%252C%2520the%2520DenseNet%2520architecture%2520showed%2520more%250Apronounced%2520benefits%2520from%2520alternative%2520fine-tuning%2520approaches%2520compared%2520to%250Atraditional%2520full%2520fine-tuning.%2520This%2520work%2520not%2520only%2520provides%2520valuable%2520insights%2520for%250Aoptimizing%2520pre-trained%2520models%2520in%2520medical%2520image%2520analysis%2520but%2520also%2520suggests%2520the%250Apotential%2520for%2520future%2520research%2520into%2520more%2520advanced%2520architectures%2520and%2520fine-tuning%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparison%20of%20fine-tuning%20strategies%20for%20transfer%20learning%20in%20medical%0A%20%20image%20classification&entry.906535625=Ana%20Davila%20and%20Jacinto%20Colan%20and%20Yasuhisa%20Hasegawa&entry.1292438233=%20%20In%20the%20context%20of%20medical%20imaging%20and%20machine%20learning%2C%20one%20of%20the%20most%0Apressing%20challenges%20is%20the%20effective%20adaptation%20of%20pre-trained%20models%20to%0Aspecialized%20medical%20contexts.%20Despite%20the%20availability%20of%20advanced%20pre-trained%0Amodels%2C%20their%20direct%20application%20to%20the%20highly%20specialized%20and%20diverse%20field%20of%0Amedical%20imaging%20often%20falls%20short%20due%20to%20the%20unique%20characteristics%20of%20medical%0Adata.%20This%20study%20provides%20a%20comprehensive%20analysis%20on%20the%20performance%20of%0Avarious%20fine-tuning%20methods%20applied%20to%20pre-trained%20models%20across%20a%20spectrum%20of%0Amedical%20imaging%20domains%2C%20including%20X-ray%2C%20MRI%2C%20Histology%2C%20Dermoscopy%2C%20and%0AEndoscopic%20surgery.%20We%20evaluated%20eight%20fine-tuning%20strategies%2C%20including%0Astandard%20techniques%20such%20as%20fine-tuning%20all%20layers%20or%20fine-tuning%20only%20the%0Aclassifier%20layers%2C%20alongside%20methods%20such%20as%20gradually%20unfreezing%20layers%2C%0Aregularization%20based%20fine-tuning%20and%20adaptive%20learning%20rates.%20We%20selected%20three%0Awell-established%20CNN%20architectures%20%28ResNet-50%2C%20DenseNet-121%2C%20and%20VGG-19%29%20to%0Acover%20a%20range%20of%20learning%20and%20feature%20extraction%20scenarios.%20Although%20our%0Aresults%20indicate%20that%20the%20efficacy%20of%20these%20fine-tuning%20methods%20significantly%0Avaries%20depending%20on%20both%20the%20architecture%20and%20the%20medical%20imaging%20type%2C%0Astrategies%20such%20as%20combining%20Linear%20Probing%20with%20Full%20Fine-tuning%20resulted%20in%0Anotable%20improvements%20in%20over%2050%25%20of%20the%20evaluated%20cases%2C%20demonstrating%20general%0Aeffectiveness%20across%20medical%20domains.%20Moreover%2C%20Auto-RGN%2C%20which%20dynamically%0Aadjusts%20learning%20rates%2C%20led%20to%20performance%20enhancements%20of%20up%20to%2011%25%20for%0Aspecific%20modalities.%20Additionally%2C%20the%20DenseNet%20architecture%20showed%20more%0Apronounced%20benefits%20from%20alternative%20fine-tuning%20approaches%20compared%20to%0Atraditional%20full%20fine-tuning.%20This%20work%20not%20only%20provides%20valuable%20insights%20for%0Aoptimizing%20pre-trained%20models%20in%20medical%20image%20analysis%20but%20also%20suggests%20the%0Apotential%20for%20future%20research%20into%20more%20advanced%20architectures%20and%20fine-tuning%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10050v1&entry.124074799=Read"},
{"title": "ProtoS-ViT: Visual foundation models for sparse self-explainable\n  classifications", "author": "Hugues Turb\u00e9 and Mina Bjelogrlic and Gianmarco Mengaldo and Christian Lovis", "abstract": "  Prototypical networks aim to build intrinsically explainable models based on\nthe linear summation of concepts. However, important challenges remain in the\ntransparency, compactness, and meaningfulness of the explanations provided by\nthese models. This work demonstrates how frozen pre-trained ViT backbones can\nbe effectively turned into prototypical models for both general and\ndomain-specific tasks, in our case biomedical image classifiers. By leveraging\nstrong spatial features combined with a novel prototypical head, ProtoS-ViT\nsurpasses existing prototypical models showing strong performance in terms of\naccuracy, compactness, and explainability. Model explainability is evaluated\nthrough an extensive set of quantitative and qualitative metrics which serve as\na general benchmark for the development of prototypical models. Code is\navailable at https://github.com/hturbe/protosvit.\n", "link": "http://arxiv.org/abs/2406.10025v1", "date": "2024-06-14", "relevancy": 2.5649, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.522}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5093}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProtoS-ViT%3A%20Visual%20foundation%20models%20for%20sparse%20self-explainable%0A%20%20classifications&body=Title%3A%20ProtoS-ViT%3A%20Visual%20foundation%20models%20for%20sparse%20self-explainable%0A%20%20classifications%0AAuthor%3A%20Hugues%20Turb%C3%A9%20and%20Mina%20Bjelogrlic%20and%20Gianmarco%20Mengaldo%20and%20Christian%20Lovis%0AAbstract%3A%20%20%20Prototypical%20networks%20aim%20to%20build%20intrinsically%20explainable%20models%20based%20on%0Athe%20linear%20summation%20of%20concepts.%20However%2C%20important%20challenges%20remain%20in%20the%0Atransparency%2C%20compactness%2C%20and%20meaningfulness%20of%20the%20explanations%20provided%20by%0Athese%20models.%20This%20work%20demonstrates%20how%20frozen%20pre-trained%20ViT%20backbones%20can%0Abe%20effectively%20turned%20into%20prototypical%20models%20for%20both%20general%20and%0Adomain-specific%20tasks%2C%20in%20our%20case%20biomedical%20image%20classifiers.%20By%20leveraging%0Astrong%20spatial%20features%20combined%20with%20a%20novel%20prototypical%20head%2C%20ProtoS-ViT%0Asurpasses%20existing%20prototypical%20models%20showing%20strong%20performance%20in%20terms%20of%0Aaccuracy%2C%20compactness%2C%20and%20explainability.%20Model%20explainability%20is%20evaluated%0Athrough%20an%20extensive%20set%20of%20quantitative%20and%20qualitative%20metrics%20which%20serve%20as%0Aa%20general%20benchmark%20for%20the%20development%20of%20prototypical%20models.%20Code%20is%0Aavailable%20at%20https%3A//github.com/hturbe/protosvit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtoS-ViT%253A%2520Visual%2520foundation%2520models%2520for%2520sparse%2520self-explainable%250A%2520%2520classifications%26entry.906535625%3DHugues%2520Turb%25C3%25A9%2520and%2520Mina%2520Bjelogrlic%2520and%2520Gianmarco%2520Mengaldo%2520and%2520Christian%2520Lovis%26entry.1292438233%3D%2520%2520Prototypical%2520networks%2520aim%2520to%2520build%2520intrinsically%2520explainable%2520models%2520based%2520on%250Athe%2520linear%2520summation%2520of%2520concepts.%2520However%252C%2520important%2520challenges%2520remain%2520in%2520the%250Atransparency%252C%2520compactness%252C%2520and%2520meaningfulness%2520of%2520the%2520explanations%2520provided%2520by%250Athese%2520models.%2520This%2520work%2520demonstrates%2520how%2520frozen%2520pre-trained%2520ViT%2520backbones%2520can%250Abe%2520effectively%2520turned%2520into%2520prototypical%2520models%2520for%2520both%2520general%2520and%250Adomain-specific%2520tasks%252C%2520in%2520our%2520case%2520biomedical%2520image%2520classifiers.%2520By%2520leveraging%250Astrong%2520spatial%2520features%2520combined%2520with%2520a%2520novel%2520prototypical%2520head%252C%2520ProtoS-ViT%250Asurpasses%2520existing%2520prototypical%2520models%2520showing%2520strong%2520performance%2520in%2520terms%2520of%250Aaccuracy%252C%2520compactness%252C%2520and%2520explainability.%2520Model%2520explainability%2520is%2520evaluated%250Athrough%2520an%2520extensive%2520set%2520of%2520quantitative%2520and%2520qualitative%2520metrics%2520which%2520serve%2520as%250Aa%2520general%2520benchmark%2520for%2520the%2520development%2520of%2520prototypical%2520models.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/hturbe/protosvit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProtoS-ViT%3A%20Visual%20foundation%20models%20for%20sparse%20self-explainable%0A%20%20classifications&entry.906535625=Hugues%20Turb%C3%A9%20and%20Mina%20Bjelogrlic%20and%20Gianmarco%20Mengaldo%20and%20Christian%20Lovis&entry.1292438233=%20%20Prototypical%20networks%20aim%20to%20build%20intrinsically%20explainable%20models%20based%20on%0Athe%20linear%20summation%20of%20concepts.%20However%2C%20important%20challenges%20remain%20in%20the%0Atransparency%2C%20compactness%2C%20and%20meaningfulness%20of%20the%20explanations%20provided%20by%0Athese%20models.%20This%20work%20demonstrates%20how%20frozen%20pre-trained%20ViT%20backbones%20can%0Abe%20effectively%20turned%20into%20prototypical%20models%20for%20both%20general%20and%0Adomain-specific%20tasks%2C%20in%20our%20case%20biomedical%20image%20classifiers.%20By%20leveraging%0Astrong%20spatial%20features%20combined%20with%20a%20novel%20prototypical%20head%2C%20ProtoS-ViT%0Asurpasses%20existing%20prototypical%20models%20showing%20strong%20performance%20in%20terms%20of%0Aaccuracy%2C%20compactness%2C%20and%20explainability.%20Model%20explainability%20is%20evaluated%0Athrough%20an%20extensive%20set%20of%20quantitative%20and%20qualitative%20metrics%20which%20serve%20as%0Aa%20general%20benchmark%20for%20the%20development%20of%20prototypical%20models.%20Code%20is%0Aavailable%20at%20https%3A//github.com/hturbe/protosvit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10025v1&entry.124074799=Read"},
{"title": "NeST: Neural Stress Tensor Tomography by leveraging 3D Photoelasticity", "author": "Akshat Dave and Tianyi Zhang and Aaron Young and Ramesh Raskar and Wolfgang Heidrich and Ashok Veeraraghavan", "abstract": "  Photoelasticity enables full-field stress analysis in transparent objects\nthrough stress-induced birefringence. Existing techniques are limited to 2D\nslices and require destructively slicing the object. Recovering the internal 3D\nstress distribution of the entire object is challenging as it involves solving\na tensor tomography problem and handling phase wrapping ambiguities. We\nintroduce NeST, an analysis-by-synthesis approach for reconstructing 3D stress\ntensor fields as neural implicit representations from polarization\nmeasurements. Our key insight is to jointly handle phase unwrapping and tensor\ntomography using a differentiable forward model based on Jones calculus. Our\nnon-linear model faithfully matches real captures, unlike prior linear\napproximations. We develop an experimental multi-axis polariscope setup to\ncapture 3D photoelasticity and experimentally demonstrate that NeST\nreconstructs the internal stress distribution for objects with varying shape\nand force conditions. Additionally, we showcase novel applications in stress\nanalysis, such as visualizing photoelastic fringes by virtually slicing the\nobject and viewing photoelastic fringes from unseen viewpoints. NeST paves the\nway for scalable non-destructive 3D photoelastic analysis.\n", "link": "http://arxiv.org/abs/2406.10212v1", "date": "2024-06-14", "relevancy": 2.5646, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5177}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5177}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeST%3A%20Neural%20Stress%20Tensor%20Tomography%20by%20leveraging%203D%20Photoelasticity&body=Title%3A%20NeST%3A%20Neural%20Stress%20Tensor%20Tomography%20by%20leveraging%203D%20Photoelasticity%0AAuthor%3A%20Akshat%20Dave%20and%20Tianyi%20Zhang%20and%20Aaron%20Young%20and%20Ramesh%20Raskar%20and%20Wolfgang%20Heidrich%20and%20Ashok%20Veeraraghavan%0AAbstract%3A%20%20%20Photoelasticity%20enables%20full-field%20stress%20analysis%20in%20transparent%20objects%0Athrough%20stress-induced%20birefringence.%20Existing%20techniques%20are%20limited%20to%202D%0Aslices%20and%20require%20destructively%20slicing%20the%20object.%20Recovering%20the%20internal%203D%0Astress%20distribution%20of%20the%20entire%20object%20is%20challenging%20as%20it%20involves%20solving%0Aa%20tensor%20tomography%20problem%20and%20handling%20phase%20wrapping%20ambiguities.%20We%0Aintroduce%20NeST%2C%20an%20analysis-by-synthesis%20approach%20for%20reconstructing%203D%20stress%0Atensor%20fields%20as%20neural%20implicit%20representations%20from%20polarization%0Ameasurements.%20Our%20key%20insight%20is%20to%20jointly%20handle%20phase%20unwrapping%20and%20tensor%0Atomography%20using%20a%20differentiable%20forward%20model%20based%20on%20Jones%20calculus.%20Our%0Anon-linear%20model%20faithfully%20matches%20real%20captures%2C%20unlike%20prior%20linear%0Aapproximations.%20We%20develop%20an%20experimental%20multi-axis%20polariscope%20setup%20to%0Acapture%203D%20photoelasticity%20and%20experimentally%20demonstrate%20that%20NeST%0Areconstructs%20the%20internal%20stress%20distribution%20for%20objects%20with%20varying%20shape%0Aand%20force%20conditions.%20Additionally%2C%20we%20showcase%20novel%20applications%20in%20stress%0Aanalysis%2C%20such%20as%20visualizing%20photoelastic%20fringes%20by%20virtually%20slicing%20the%0Aobject%20and%20viewing%20photoelastic%20fringes%20from%20unseen%20viewpoints.%20NeST%20paves%20the%0Away%20for%20scalable%20non-destructive%203D%20photoelastic%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeST%253A%2520Neural%2520Stress%2520Tensor%2520Tomography%2520by%2520leveraging%25203D%2520Photoelasticity%26entry.906535625%3DAkshat%2520Dave%2520and%2520Tianyi%2520Zhang%2520and%2520Aaron%2520Young%2520and%2520Ramesh%2520Raskar%2520and%2520Wolfgang%2520Heidrich%2520and%2520Ashok%2520Veeraraghavan%26entry.1292438233%3D%2520%2520Photoelasticity%2520enables%2520full-field%2520stress%2520analysis%2520in%2520transparent%2520objects%250Athrough%2520stress-induced%2520birefringence.%2520Existing%2520techniques%2520are%2520limited%2520to%25202D%250Aslices%2520and%2520require%2520destructively%2520slicing%2520the%2520object.%2520Recovering%2520the%2520internal%25203D%250Astress%2520distribution%2520of%2520the%2520entire%2520object%2520is%2520challenging%2520as%2520it%2520involves%2520solving%250Aa%2520tensor%2520tomography%2520problem%2520and%2520handling%2520phase%2520wrapping%2520ambiguities.%2520We%250Aintroduce%2520NeST%252C%2520an%2520analysis-by-synthesis%2520approach%2520for%2520reconstructing%25203D%2520stress%250Atensor%2520fields%2520as%2520neural%2520implicit%2520representations%2520from%2520polarization%250Ameasurements.%2520Our%2520key%2520insight%2520is%2520to%2520jointly%2520handle%2520phase%2520unwrapping%2520and%2520tensor%250Atomography%2520using%2520a%2520differentiable%2520forward%2520model%2520based%2520on%2520Jones%2520calculus.%2520Our%250Anon-linear%2520model%2520faithfully%2520matches%2520real%2520captures%252C%2520unlike%2520prior%2520linear%250Aapproximations.%2520We%2520develop%2520an%2520experimental%2520multi-axis%2520polariscope%2520setup%2520to%250Acapture%25203D%2520photoelasticity%2520and%2520experimentally%2520demonstrate%2520that%2520NeST%250Areconstructs%2520the%2520internal%2520stress%2520distribution%2520for%2520objects%2520with%2520varying%2520shape%250Aand%2520force%2520conditions.%2520Additionally%252C%2520we%2520showcase%2520novel%2520applications%2520in%2520stress%250Aanalysis%252C%2520such%2520as%2520visualizing%2520photoelastic%2520fringes%2520by%2520virtually%2520slicing%2520the%250Aobject%2520and%2520viewing%2520photoelastic%2520fringes%2520from%2520unseen%2520viewpoints.%2520NeST%2520paves%2520the%250Away%2520for%2520scalable%2520non-destructive%25203D%2520photoelastic%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeST%3A%20Neural%20Stress%20Tensor%20Tomography%20by%20leveraging%203D%20Photoelasticity&entry.906535625=Akshat%20Dave%20and%20Tianyi%20Zhang%20and%20Aaron%20Young%20and%20Ramesh%20Raskar%20and%20Wolfgang%20Heidrich%20and%20Ashok%20Veeraraghavan&entry.1292438233=%20%20Photoelasticity%20enables%20full-field%20stress%20analysis%20in%20transparent%20objects%0Athrough%20stress-induced%20birefringence.%20Existing%20techniques%20are%20limited%20to%202D%0Aslices%20and%20require%20destructively%20slicing%20the%20object.%20Recovering%20the%20internal%203D%0Astress%20distribution%20of%20the%20entire%20object%20is%20challenging%20as%20it%20involves%20solving%0Aa%20tensor%20tomography%20problem%20and%20handling%20phase%20wrapping%20ambiguities.%20We%0Aintroduce%20NeST%2C%20an%20analysis-by-synthesis%20approach%20for%20reconstructing%203D%20stress%0Atensor%20fields%20as%20neural%20implicit%20representations%20from%20polarization%0Ameasurements.%20Our%20key%20insight%20is%20to%20jointly%20handle%20phase%20unwrapping%20and%20tensor%0Atomography%20using%20a%20differentiable%20forward%20model%20based%20on%20Jones%20calculus.%20Our%0Anon-linear%20model%20faithfully%20matches%20real%20captures%2C%20unlike%20prior%20linear%0Aapproximations.%20We%20develop%20an%20experimental%20multi-axis%20polariscope%20setup%20to%0Acapture%203D%20photoelasticity%20and%20experimentally%20demonstrate%20that%20NeST%0Areconstructs%20the%20internal%20stress%20distribution%20for%20objects%20with%20varying%20shape%0Aand%20force%20conditions.%20Additionally%2C%20we%20showcase%20novel%20applications%20in%20stress%0Aanalysis%2C%20such%20as%20visualizing%20photoelastic%20fringes%20by%20virtually%20slicing%20the%0Aobject%20and%20viewing%20photoelastic%20fringes%20from%20unseen%20viewpoints.%20NeST%20paves%20the%0Away%20for%20scalable%20non-destructive%203D%20photoelastic%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10212v1&entry.124074799=Read"},
{"title": "Self-Supervised and Few-Shot Learning for Robust Bioaerosol Monitoring", "author": "Adrian Willi and Pascal Baumann and Sophie Erb and Fabian Gr\u00f6ger and Yanick Zeder and Simone Lionetti", "abstract": "  Real-time bioaerosol monitoring is improving the quality of life for people\naffected by allergies, but it often relies on deep-learning models which pose\nchallenges for widespread adoption. These models are typically trained in a\nsupervised fashion and require considerable effort to produce large amounts of\nannotated data, an effort that must be repeated for new particles, geographical\nregions, or measurement systems. In this work, we show that self-supervised\nlearning and few-shot learning can be combined to classify holographic images\nof bioaerosol particles using a large collection of unlabelled data and only a\nfew examples for each particle type. We first demonstrate that self-supervision\non pictures of unidentified particles from ambient air measurements enhances\nidentification even when labelled data is abundant. Most importantly, it\ngreatly improves few-shot classification when only a handful of labelled images\nare available. Our findings suggest that real-time bioaerosol monitoring\nworkflows can be substantially optimized, and the effort required to adapt\nmodels for different situations considerably reduced.\n", "link": "http://arxiv.org/abs/2406.09984v1", "date": "2024-06-14", "relevancy": 2.5572, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5253}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5077}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20and%20Few-Shot%20Learning%20for%20Robust%20Bioaerosol%20Monitoring&body=Title%3A%20Self-Supervised%20and%20Few-Shot%20Learning%20for%20Robust%20Bioaerosol%20Monitoring%0AAuthor%3A%20Adrian%20Willi%20and%20Pascal%20Baumann%20and%20Sophie%20Erb%20and%20Fabian%20Gr%C3%B6ger%20and%20Yanick%20Zeder%20and%20Simone%20Lionetti%0AAbstract%3A%20%20%20Real-time%20bioaerosol%20monitoring%20is%20improving%20the%20quality%20of%20life%20for%20people%0Aaffected%20by%20allergies%2C%20but%20it%20often%20relies%20on%20deep-learning%20models%20which%20pose%0Achallenges%20for%20widespread%20adoption.%20These%20models%20are%20typically%20trained%20in%20a%0Asupervised%20fashion%20and%20require%20considerable%20effort%20to%20produce%20large%20amounts%20of%0Aannotated%20data%2C%20an%20effort%20that%20must%20be%20repeated%20for%20new%20particles%2C%20geographical%0Aregions%2C%20or%20measurement%20systems.%20In%20this%20work%2C%20we%20show%20that%20self-supervised%0Alearning%20and%20few-shot%20learning%20can%20be%20combined%20to%20classify%20holographic%20images%0Aof%20bioaerosol%20particles%20using%20a%20large%20collection%20of%20unlabelled%20data%20and%20only%20a%0Afew%20examples%20for%20each%20particle%20type.%20We%20first%20demonstrate%20that%20self-supervision%0Aon%20pictures%20of%20unidentified%20particles%20from%20ambient%20air%20measurements%20enhances%0Aidentification%20even%20when%20labelled%20data%20is%20abundant.%20Most%20importantly%2C%20it%0Agreatly%20improves%20few-shot%20classification%20when%20only%20a%20handful%20of%20labelled%20images%0Aare%20available.%20Our%20findings%20suggest%20that%20real-time%20bioaerosol%20monitoring%0Aworkflows%20can%20be%20substantially%20optimized%2C%20and%20the%20effort%20required%20to%20adapt%0Amodels%20for%20different%20situations%20considerably%20reduced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520and%2520Few-Shot%2520Learning%2520for%2520Robust%2520Bioaerosol%2520Monitoring%26entry.906535625%3DAdrian%2520Willi%2520and%2520Pascal%2520Baumann%2520and%2520Sophie%2520Erb%2520and%2520Fabian%2520Gr%25C3%25B6ger%2520and%2520Yanick%2520Zeder%2520and%2520Simone%2520Lionetti%26entry.1292438233%3D%2520%2520Real-time%2520bioaerosol%2520monitoring%2520is%2520improving%2520the%2520quality%2520of%2520life%2520for%2520people%250Aaffected%2520by%2520allergies%252C%2520but%2520it%2520often%2520relies%2520on%2520deep-learning%2520models%2520which%2520pose%250Achallenges%2520for%2520widespread%2520adoption.%2520These%2520models%2520are%2520typically%2520trained%2520in%2520a%250Asupervised%2520fashion%2520and%2520require%2520considerable%2520effort%2520to%2520produce%2520large%2520amounts%2520of%250Aannotated%2520data%252C%2520an%2520effort%2520that%2520must%2520be%2520repeated%2520for%2520new%2520particles%252C%2520geographical%250Aregions%252C%2520or%2520measurement%2520systems.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520self-supervised%250Alearning%2520and%2520few-shot%2520learning%2520can%2520be%2520combined%2520to%2520classify%2520holographic%2520images%250Aof%2520bioaerosol%2520particles%2520using%2520a%2520large%2520collection%2520of%2520unlabelled%2520data%2520and%2520only%2520a%250Afew%2520examples%2520for%2520each%2520particle%2520type.%2520We%2520first%2520demonstrate%2520that%2520self-supervision%250Aon%2520pictures%2520of%2520unidentified%2520particles%2520from%2520ambient%2520air%2520measurements%2520enhances%250Aidentification%2520even%2520when%2520labelled%2520data%2520is%2520abundant.%2520Most%2520importantly%252C%2520it%250Agreatly%2520improves%2520few-shot%2520classification%2520when%2520only%2520a%2520handful%2520of%2520labelled%2520images%250Aare%2520available.%2520Our%2520findings%2520suggest%2520that%2520real-time%2520bioaerosol%2520monitoring%250Aworkflows%2520can%2520be%2520substantially%2520optimized%252C%2520and%2520the%2520effort%2520required%2520to%2520adapt%250Amodels%2520for%2520different%2520situations%2520considerably%2520reduced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20and%20Few-Shot%20Learning%20for%20Robust%20Bioaerosol%20Monitoring&entry.906535625=Adrian%20Willi%20and%20Pascal%20Baumann%20and%20Sophie%20Erb%20and%20Fabian%20Gr%C3%B6ger%20and%20Yanick%20Zeder%20and%20Simone%20Lionetti&entry.1292438233=%20%20Real-time%20bioaerosol%20monitoring%20is%20improving%20the%20quality%20of%20life%20for%20people%0Aaffected%20by%20allergies%2C%20but%20it%20often%20relies%20on%20deep-learning%20models%20which%20pose%0Achallenges%20for%20widespread%20adoption.%20These%20models%20are%20typically%20trained%20in%20a%0Asupervised%20fashion%20and%20require%20considerable%20effort%20to%20produce%20large%20amounts%20of%0Aannotated%20data%2C%20an%20effort%20that%20must%20be%20repeated%20for%20new%20particles%2C%20geographical%0Aregions%2C%20or%20measurement%20systems.%20In%20this%20work%2C%20we%20show%20that%20self-supervised%0Alearning%20and%20few-shot%20learning%20can%20be%20combined%20to%20classify%20holographic%20images%0Aof%20bioaerosol%20particles%20using%20a%20large%20collection%20of%20unlabelled%20data%20and%20only%20a%0Afew%20examples%20for%20each%20particle%20type.%20We%20first%20demonstrate%20that%20self-supervision%0Aon%20pictures%20of%20unidentified%20particles%20from%20ambient%20air%20measurements%20enhances%0Aidentification%20even%20when%20labelled%20data%20is%20abundant.%20Most%20importantly%2C%20it%0Agreatly%20improves%20few-shot%20classification%20when%20only%20a%20handful%20of%20labelled%20images%0Aare%20available.%20Our%20findings%20suggest%20that%20real-time%20bioaerosol%20monitoring%0Aworkflows%20can%20be%20substantially%20optimized%2C%20and%20the%20effort%20required%20to%20adapt%0Amodels%20for%20different%20situations%20considerably%20reduced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09984v1&entry.124074799=Read"},
{"title": "Revisiting Few-Shot Object Detection with Vision-Language Models", "author": "Anish Madan and Neehar Peri and Shu Kong and Deva Ramanan", "abstract": "  The era of vision-language models (VLMs) trained on large web-scale datasets\nchallenges conventional formulations of \"open-world\" perception. In this work,\nwe revisit the task of few-shot object detection (FSOD) in the context of\nrecent foundational VLMs. First, we point out that zero-shot VLMs such as\nGroundingDINO significantly outperform state-of-the-art few-shot detectors (48\nvs. 33 AP) on COCO. Despite their strong zero-shot performance, such\nfoundational models may still be sub-optimal. For example, trucks on the web\nmay be defined differently from trucks for a target application such as\nautonomous vehicle perception. We argue that the task of few-shot recognition\ncan be reformulated as aligning foundation models to target concepts using a\nfew examples. Interestingly, such examples can be multi-modal, using both text\nand visual cues, mimicking instructions that are often given to human\nannotators when defining a target concept of interest. Concretely, we propose\nFoundational FSOD, a new benchmark protocol that evaluates detectors\npre-trained on any external datasets and fine-tuned on multi-modal (text and\nvisual) K-shot examples per target class. We repurpose nuImages for\nFoundational FSOD, benchmark several popular open-source VLMs, and provide an\nempirical analysis of state-of-the-art methods. Lastly, we discuss our recent\nCVPR 2024 Foundational FSOD competition and share insights from the community.\nNotably, the winning team significantly outperforms our baseline by 23.9 mAP!\n", "link": "http://arxiv.org/abs/2312.14494v3", "date": "2024-06-14", "relevancy": 2.5552, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5193}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5072}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Few-Shot%20Object%20Detection%20with%20Vision-Language%20Models&body=Title%3A%20Revisiting%20Few-Shot%20Object%20Detection%20with%20Vision-Language%20Models%0AAuthor%3A%20Anish%20Madan%20and%20Neehar%20Peri%20and%20Shu%20Kong%20and%20Deva%20Ramanan%0AAbstract%3A%20%20%20The%20era%20of%20vision-language%20models%20%28VLMs%29%20trained%20on%20large%20web-scale%20datasets%0Achallenges%20conventional%20formulations%20of%20%22open-world%22%20perception.%20In%20this%20work%2C%0Awe%20revisit%20the%20task%20of%20few-shot%20object%20detection%20%28FSOD%29%20in%20the%20context%20of%0Arecent%20foundational%20VLMs.%20First%2C%20we%20point%20out%20that%20zero-shot%20VLMs%20such%20as%0AGroundingDINO%20significantly%20outperform%20state-of-the-art%20few-shot%20detectors%20%2848%0Avs.%2033%20AP%29%20on%20COCO.%20Despite%20their%20strong%20zero-shot%20performance%2C%20such%0Afoundational%20models%20may%20still%20be%20sub-optimal.%20For%20example%2C%20trucks%20on%20the%20web%0Amay%20be%20defined%20differently%20from%20trucks%20for%20a%20target%20application%20such%20as%0Aautonomous%20vehicle%20perception.%20We%20argue%20that%20the%20task%20of%20few-shot%20recognition%0Acan%20be%20reformulated%20as%20aligning%20foundation%20models%20to%20target%20concepts%20using%20a%0Afew%20examples.%20Interestingly%2C%20such%20examples%20can%20be%20multi-modal%2C%20using%20both%20text%0Aand%20visual%20cues%2C%20mimicking%20instructions%20that%20are%20often%20given%20to%20human%0Aannotators%20when%20defining%20a%20target%20concept%20of%20interest.%20Concretely%2C%20we%20propose%0AFoundational%20FSOD%2C%20a%20new%20benchmark%20protocol%20that%20evaluates%20detectors%0Apre-trained%20on%20any%20external%20datasets%20and%20fine-tuned%20on%20multi-modal%20%28text%20and%0Avisual%29%20K-shot%20examples%20per%20target%20class.%20We%20repurpose%20nuImages%20for%0AFoundational%20FSOD%2C%20benchmark%20several%20popular%20open-source%20VLMs%2C%20and%20provide%20an%0Aempirical%20analysis%20of%20state-of-the-art%20methods.%20Lastly%2C%20we%20discuss%20our%20recent%0ACVPR%202024%20Foundational%20FSOD%20competition%20and%20share%20insights%20from%20the%20community.%0ANotably%2C%20the%20winning%20team%20significantly%20outperforms%20our%20baseline%20by%2023.9%20mAP%21%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14494v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Few-Shot%2520Object%2520Detection%2520with%2520Vision-Language%2520Models%26entry.906535625%3DAnish%2520Madan%2520and%2520Neehar%2520Peri%2520and%2520Shu%2520Kong%2520and%2520Deva%2520Ramanan%26entry.1292438233%3D%2520%2520The%2520era%2520of%2520vision-language%2520models%2520%2528VLMs%2529%2520trained%2520on%2520large%2520web-scale%2520datasets%250Achallenges%2520conventional%2520formulations%2520of%2520%2522open-world%2522%2520perception.%2520In%2520this%2520work%252C%250Awe%2520revisit%2520the%2520task%2520of%2520few-shot%2520object%2520detection%2520%2528FSOD%2529%2520in%2520the%2520context%2520of%250Arecent%2520foundational%2520VLMs.%2520First%252C%2520we%2520point%2520out%2520that%2520zero-shot%2520VLMs%2520such%2520as%250AGroundingDINO%2520significantly%2520outperform%2520state-of-the-art%2520few-shot%2520detectors%2520%252848%250Avs.%252033%2520AP%2529%2520on%2520COCO.%2520Despite%2520their%2520strong%2520zero-shot%2520performance%252C%2520such%250Afoundational%2520models%2520may%2520still%2520be%2520sub-optimal.%2520For%2520example%252C%2520trucks%2520on%2520the%2520web%250Amay%2520be%2520defined%2520differently%2520from%2520trucks%2520for%2520a%2520target%2520application%2520such%2520as%250Aautonomous%2520vehicle%2520perception.%2520We%2520argue%2520that%2520the%2520task%2520of%2520few-shot%2520recognition%250Acan%2520be%2520reformulated%2520as%2520aligning%2520foundation%2520models%2520to%2520target%2520concepts%2520using%2520a%250Afew%2520examples.%2520Interestingly%252C%2520such%2520examples%2520can%2520be%2520multi-modal%252C%2520using%2520both%2520text%250Aand%2520visual%2520cues%252C%2520mimicking%2520instructions%2520that%2520are%2520often%2520given%2520to%2520human%250Aannotators%2520when%2520defining%2520a%2520target%2520concept%2520of%2520interest.%2520Concretely%252C%2520we%2520propose%250AFoundational%2520FSOD%252C%2520a%2520new%2520benchmark%2520protocol%2520that%2520evaluates%2520detectors%250Apre-trained%2520on%2520any%2520external%2520datasets%2520and%2520fine-tuned%2520on%2520multi-modal%2520%2528text%2520and%250Avisual%2529%2520K-shot%2520examples%2520per%2520target%2520class.%2520We%2520repurpose%2520nuImages%2520for%250AFoundational%2520FSOD%252C%2520benchmark%2520several%2520popular%2520open-source%2520VLMs%252C%2520and%2520provide%2520an%250Aempirical%2520analysis%2520of%2520state-of-the-art%2520methods.%2520Lastly%252C%2520we%2520discuss%2520our%2520recent%250ACVPR%25202024%2520Foundational%2520FSOD%2520competition%2520and%2520share%2520insights%2520from%2520the%2520community.%250ANotably%252C%2520the%2520winning%2520team%2520significantly%2520outperforms%2520our%2520baseline%2520by%252023.9%2520mAP%2521%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14494v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Few-Shot%20Object%20Detection%20with%20Vision-Language%20Models&entry.906535625=Anish%20Madan%20and%20Neehar%20Peri%20and%20Shu%20Kong%20and%20Deva%20Ramanan&entry.1292438233=%20%20The%20era%20of%20vision-language%20models%20%28VLMs%29%20trained%20on%20large%20web-scale%20datasets%0Achallenges%20conventional%20formulations%20of%20%22open-world%22%20perception.%20In%20this%20work%2C%0Awe%20revisit%20the%20task%20of%20few-shot%20object%20detection%20%28FSOD%29%20in%20the%20context%20of%0Arecent%20foundational%20VLMs.%20First%2C%20we%20point%20out%20that%20zero-shot%20VLMs%20such%20as%0AGroundingDINO%20significantly%20outperform%20state-of-the-art%20few-shot%20detectors%20%2848%0Avs.%2033%20AP%29%20on%20COCO.%20Despite%20their%20strong%20zero-shot%20performance%2C%20such%0Afoundational%20models%20may%20still%20be%20sub-optimal.%20For%20example%2C%20trucks%20on%20the%20web%0Amay%20be%20defined%20differently%20from%20trucks%20for%20a%20target%20application%20such%20as%0Aautonomous%20vehicle%20perception.%20We%20argue%20that%20the%20task%20of%20few-shot%20recognition%0Acan%20be%20reformulated%20as%20aligning%20foundation%20models%20to%20target%20concepts%20using%20a%0Afew%20examples.%20Interestingly%2C%20such%20examples%20can%20be%20multi-modal%2C%20using%20both%20text%0Aand%20visual%20cues%2C%20mimicking%20instructions%20that%20are%20often%20given%20to%20human%0Aannotators%20when%20defining%20a%20target%20concept%20of%20interest.%20Concretely%2C%20we%20propose%0AFoundational%20FSOD%2C%20a%20new%20benchmark%20protocol%20that%20evaluates%20detectors%0Apre-trained%20on%20any%20external%20datasets%20and%20fine-tuned%20on%20multi-modal%20%28text%20and%0Avisual%29%20K-shot%20examples%20per%20target%20class.%20We%20repurpose%20nuImages%20for%0AFoundational%20FSOD%2C%20benchmark%20several%20popular%20open-source%20VLMs%2C%20and%20provide%20an%0Aempirical%20analysis%20of%20state-of-the-art%20methods.%20Lastly%2C%20we%20discuss%20our%20recent%0ACVPR%202024%20Foundational%20FSOD%20competition%20and%20share%20insights%20from%20the%20community.%0ANotably%2C%20the%20winning%20team%20significantly%20outperforms%20our%20baseline%20by%2023.9%20mAP%21%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14494v3&entry.124074799=Read"},
{"title": "DiffusionBlend: Learning 3D Image Prior through Position-aware Diffusion\n  Score Blending for 3D Computed Tomography Reconstruction", "author": "Bowen Song and Jason Hu and Zhaoxu Luo and Jeffrey A. Fessler and Liyue Shen", "abstract": "  Diffusion models face significant challenges when employed for large-scale\nmedical image reconstruction in real practice such as 3D Computed Tomography\n(CT). Due to the demanding memory, time, and data requirements, it is difficult\nto train a diffusion model directly on the entire volume of high-dimensional\ndata to obtain an efficient 3D diffusion prior. Existing works utilizing\ndiffusion priors on single 2D image slice with hand-crafted cross-slice\nregularization would sacrifice the z-axis consistency, which results in severe\nartifacts along the z-axis. In this work, we propose a novel framework that\nenables learning the 3D image prior through position-aware 3D-patch diffusion\nscore blending for reconstructing large-scale 3D medical images. To the best of\nour knowledge, we are the first to utilize a 3D-patch diffusion prior for 3D\nmedical image reconstruction. Extensive experiments on sparse view and limited\nangle CT reconstruction show that our DiffusionBlend method significantly\noutperforms previous methods and achieves state-of-the-art performance on\nreal-world CT reconstruction problems with high-dimensional 3D image (i.e.,\n$256 \\times 256 \\times 500$). Our algorithm also comes with better or\ncomparable computational efficiency than previous state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2406.10211v1", "date": "2024-06-14", "relevancy": 2.5547, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6457}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6457}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffusionBlend%3A%20Learning%203D%20Image%20Prior%20through%20Position-aware%20Diffusion%0A%20%20Score%20Blending%20for%203D%20Computed%20Tomography%20Reconstruction&body=Title%3A%20DiffusionBlend%3A%20Learning%203D%20Image%20Prior%20through%20Position-aware%20Diffusion%0A%20%20Score%20Blending%20for%203D%20Computed%20Tomography%20Reconstruction%0AAuthor%3A%20Bowen%20Song%20and%20Jason%20Hu%20and%20Zhaoxu%20Luo%20and%20Jeffrey%20A.%20Fessler%20and%20Liyue%20Shen%0AAbstract%3A%20%20%20Diffusion%20models%20face%20significant%20challenges%20when%20employed%20for%20large-scale%0Amedical%20image%20reconstruction%20in%20real%20practice%20such%20as%203D%20Computed%20Tomography%0A%28CT%29.%20Due%20to%20the%20demanding%20memory%2C%20time%2C%20and%20data%20requirements%2C%20it%20is%20difficult%0Ato%20train%20a%20diffusion%20model%20directly%20on%20the%20entire%20volume%20of%20high-dimensional%0Adata%20to%20obtain%20an%20efficient%203D%20diffusion%20prior.%20Existing%20works%20utilizing%0Adiffusion%20priors%20on%20single%202D%20image%20slice%20with%20hand-crafted%20cross-slice%0Aregularization%20would%20sacrifice%20the%20z-axis%20consistency%2C%20which%20results%20in%20severe%0Aartifacts%20along%20the%20z-axis.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20that%0Aenables%20learning%20the%203D%20image%20prior%20through%20position-aware%203D-patch%20diffusion%0Ascore%20blending%20for%20reconstructing%20large-scale%203D%20medical%20images.%20To%20the%20best%20of%0Aour%20knowledge%2C%20we%20are%20the%20first%20to%20utilize%20a%203D-patch%20diffusion%20prior%20for%203D%0Amedical%20image%20reconstruction.%20Extensive%20experiments%20on%20sparse%20view%20and%20limited%0Aangle%20CT%20reconstruction%20show%20that%20our%20DiffusionBlend%20method%20significantly%0Aoutperforms%20previous%20methods%20and%20achieves%20state-of-the-art%20performance%20on%0Areal-world%20CT%20reconstruction%20problems%20with%20high-dimensional%203D%20image%20%28i.e.%2C%0A%24256%20%5Ctimes%20256%20%5Ctimes%20500%24%29.%20Our%20algorithm%20also%20comes%20with%20better%20or%0Acomparable%20computational%20efficiency%20than%20previous%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusionBlend%253A%2520Learning%25203D%2520Image%2520Prior%2520through%2520Position-aware%2520Diffusion%250A%2520%2520Score%2520Blending%2520for%25203D%2520Computed%2520Tomography%2520Reconstruction%26entry.906535625%3DBowen%2520Song%2520and%2520Jason%2520Hu%2520and%2520Zhaoxu%2520Luo%2520and%2520Jeffrey%2520A.%2520Fessler%2520and%2520Liyue%2520Shen%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520face%2520significant%2520challenges%2520when%2520employed%2520for%2520large-scale%250Amedical%2520image%2520reconstruction%2520in%2520real%2520practice%2520such%2520as%25203D%2520Computed%2520Tomography%250A%2528CT%2529.%2520Due%2520to%2520the%2520demanding%2520memory%252C%2520time%252C%2520and%2520data%2520requirements%252C%2520it%2520is%2520difficult%250Ato%2520train%2520a%2520diffusion%2520model%2520directly%2520on%2520the%2520entire%2520volume%2520of%2520high-dimensional%250Adata%2520to%2520obtain%2520an%2520efficient%25203D%2520diffusion%2520prior.%2520Existing%2520works%2520utilizing%250Adiffusion%2520priors%2520on%2520single%25202D%2520image%2520slice%2520with%2520hand-crafted%2520cross-slice%250Aregularization%2520would%2520sacrifice%2520the%2520z-axis%2520consistency%252C%2520which%2520results%2520in%2520severe%250Aartifacts%2520along%2520the%2520z-axis.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%250Aenables%2520learning%2520the%25203D%2520image%2520prior%2520through%2520position-aware%25203D-patch%2520diffusion%250Ascore%2520blending%2520for%2520reconstructing%2520large-scale%25203D%2520medical%2520images.%2520To%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520we%2520are%2520the%2520first%2520to%2520utilize%2520a%25203D-patch%2520diffusion%2520prior%2520for%25203D%250Amedical%2520image%2520reconstruction.%2520Extensive%2520experiments%2520on%2520sparse%2520view%2520and%2520limited%250Aangle%2520CT%2520reconstruction%2520show%2520that%2520our%2520DiffusionBlend%2520method%2520significantly%250Aoutperforms%2520previous%2520methods%2520and%2520achieves%2520state-of-the-art%2520performance%2520on%250Areal-world%2520CT%2520reconstruction%2520problems%2520with%2520high-dimensional%25203D%2520image%2520%2528i.e.%252C%250A%2524256%2520%255Ctimes%2520256%2520%255Ctimes%2520500%2524%2529.%2520Our%2520algorithm%2520also%2520comes%2520with%2520better%2520or%250Acomparable%2520computational%2520efficiency%2520than%2520previous%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffusionBlend%3A%20Learning%203D%20Image%20Prior%20through%20Position-aware%20Diffusion%0A%20%20Score%20Blending%20for%203D%20Computed%20Tomography%20Reconstruction&entry.906535625=Bowen%20Song%20and%20Jason%20Hu%20and%20Zhaoxu%20Luo%20and%20Jeffrey%20A.%20Fessler%20and%20Liyue%20Shen&entry.1292438233=%20%20Diffusion%20models%20face%20significant%20challenges%20when%20employed%20for%20large-scale%0Amedical%20image%20reconstruction%20in%20real%20practice%20such%20as%203D%20Computed%20Tomography%0A%28CT%29.%20Due%20to%20the%20demanding%20memory%2C%20time%2C%20and%20data%20requirements%2C%20it%20is%20difficult%0Ato%20train%20a%20diffusion%20model%20directly%20on%20the%20entire%20volume%20of%20high-dimensional%0Adata%20to%20obtain%20an%20efficient%203D%20diffusion%20prior.%20Existing%20works%20utilizing%0Adiffusion%20priors%20on%20single%202D%20image%20slice%20with%20hand-crafted%20cross-slice%0Aregularization%20would%20sacrifice%20the%20z-axis%20consistency%2C%20which%20results%20in%20severe%0Aartifacts%20along%20the%20z-axis.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20that%0Aenables%20learning%20the%203D%20image%20prior%20through%20position-aware%203D-patch%20diffusion%0Ascore%20blending%20for%20reconstructing%20large-scale%203D%20medical%20images.%20To%20the%20best%20of%0Aour%20knowledge%2C%20we%20are%20the%20first%20to%20utilize%20a%203D-patch%20diffusion%20prior%20for%203D%0Amedical%20image%20reconstruction.%20Extensive%20experiments%20on%20sparse%20view%20and%20limited%0Aangle%20CT%20reconstruction%20show%20that%20our%20DiffusionBlend%20method%20significantly%0Aoutperforms%20previous%20methods%20and%20achieves%20state-of-the-art%20performance%20on%0Areal-world%20CT%20reconstruction%20problems%20with%20high-dimensional%203D%20image%20%28i.e.%2C%0A%24256%20%5Ctimes%20256%20%5Ctimes%20500%24%29.%20Our%20algorithm%20also%20comes%20with%20better%20or%0Acomparable%20computational%20efficiency%20than%20previous%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10211v1&entry.124074799=Read"},
{"title": "Long-Tailed 3D Detection via 2D Late Fusion", "author": "Yechi Ma and Neehar Peri and Shuoquan Wei and Wei Hua and Deva Ramanan and Yanan Li and Shu Kong", "abstract": "  Long-Tailed 3D Object Detection (LT3D) addresses the problem of accurately\ndetecting objects from both common and rare classes. Contemporary multi-modal\ndetectors achieve low AP on rare-classes (e.g., CMT only achieves 9.4 AP on\nstroller), presumably because training detectors end-to-end with significant\nclass imbalance is challenging. To address this limitation, we delve into a\nsimple late-fusion framework that ensembles independently trained uni-modal\nLiDAR and RGB detectors. Importantly, such a late-fusion framework allows us to\nleverage large-scale uni-modal datasets (with more examples for rare classes)\nto train better uni-modal RGB detectors, unlike prevailing multimodal detectors\nthat require paired multi-modal training data. Notably, our approach\nsignificantly improves rare-class detection by 7.2% over prior work. Further,\nwe examine three critical components of our simple late-fusion approach from\nfirst principles and investigate whether to train 2D or 3D RGB detectors,\nwhether to match RGB and LiDAR detections in 3D or the projected 2D image plane\nfor fusion, and how to fuse matched detections. Extensive experiments reveal\nthat 2D RGB detectors achieve better recognition accuracy for rare classes than\n3D RGB detectors and matching on the 2D image plane mitigates depth estimation\nerrors. Our late-fusion approach achieves 51.4 mAP on the established nuScenes\nLT3D benchmark, improving over prior work by 5.9 mAP!\n", "link": "http://arxiv.org/abs/2312.10986v3", "date": "2024-06-14", "relevancy": 2.47, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6255}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6167}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Tailed%203D%20Detection%20via%202D%20Late%20Fusion&body=Title%3A%20Long-Tailed%203D%20Detection%20via%202D%20Late%20Fusion%0AAuthor%3A%20Yechi%20Ma%20and%20Neehar%20Peri%20and%20Shuoquan%20Wei%20and%20Wei%20Hua%20and%20Deva%20Ramanan%20and%20Yanan%20Li%20and%20Shu%20Kong%0AAbstract%3A%20%20%20Long-Tailed%203D%20Object%20Detection%20%28LT3D%29%20addresses%20the%20problem%20of%20accurately%0Adetecting%20objects%20from%20both%20common%20and%20rare%20classes.%20Contemporary%20multi-modal%0Adetectors%20achieve%20low%20AP%20on%20rare-classes%20%28e.g.%2C%20CMT%20only%20achieves%209.4%20AP%20on%0Astroller%29%2C%20presumably%20because%20training%20detectors%20end-to-end%20with%20significant%0Aclass%20imbalance%20is%20challenging.%20To%20address%20this%20limitation%2C%20we%20delve%20into%20a%0Asimple%20late-fusion%20framework%20that%20ensembles%20independently%20trained%20uni-modal%0ALiDAR%20and%20RGB%20detectors.%20Importantly%2C%20such%20a%20late-fusion%20framework%20allows%20us%20to%0Aleverage%20large-scale%20uni-modal%20datasets%20%28with%20more%20examples%20for%20rare%20classes%29%0Ato%20train%20better%20uni-modal%20RGB%20detectors%2C%20unlike%20prevailing%20multimodal%20detectors%0Athat%20require%20paired%20multi-modal%20training%20data.%20Notably%2C%20our%20approach%0Asignificantly%20improves%20rare-class%20detection%20by%207.2%25%20over%20prior%20work.%20Further%2C%0Awe%20examine%20three%20critical%20components%20of%20our%20simple%20late-fusion%20approach%20from%0Afirst%20principles%20and%20investigate%20whether%20to%20train%202D%20or%203D%20RGB%20detectors%2C%0Awhether%20to%20match%20RGB%20and%20LiDAR%20detections%20in%203D%20or%20the%20projected%202D%20image%20plane%0Afor%20fusion%2C%20and%20how%20to%20fuse%20matched%20detections.%20Extensive%20experiments%20reveal%0Athat%202D%20RGB%20detectors%20achieve%20better%20recognition%20accuracy%20for%20rare%20classes%20than%0A3D%20RGB%20detectors%20and%20matching%20on%20the%202D%20image%20plane%20mitigates%20depth%20estimation%0Aerrors.%20Our%20late-fusion%20approach%20achieves%2051.4%20mAP%20on%20the%20established%20nuScenes%0ALT3D%20benchmark%2C%20improving%20over%20prior%20work%20by%205.9%20mAP%21%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10986v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Tailed%25203D%2520Detection%2520via%25202D%2520Late%2520Fusion%26entry.906535625%3DYechi%2520Ma%2520and%2520Neehar%2520Peri%2520and%2520Shuoquan%2520Wei%2520and%2520Wei%2520Hua%2520and%2520Deva%2520Ramanan%2520and%2520Yanan%2520Li%2520and%2520Shu%2520Kong%26entry.1292438233%3D%2520%2520Long-Tailed%25203D%2520Object%2520Detection%2520%2528LT3D%2529%2520addresses%2520the%2520problem%2520of%2520accurately%250Adetecting%2520objects%2520from%2520both%2520common%2520and%2520rare%2520classes.%2520Contemporary%2520multi-modal%250Adetectors%2520achieve%2520low%2520AP%2520on%2520rare-classes%2520%2528e.g.%252C%2520CMT%2520only%2520achieves%25209.4%2520AP%2520on%250Astroller%2529%252C%2520presumably%2520because%2520training%2520detectors%2520end-to-end%2520with%2520significant%250Aclass%2520imbalance%2520is%2520challenging.%2520To%2520address%2520this%2520limitation%252C%2520we%2520delve%2520into%2520a%250Asimple%2520late-fusion%2520framework%2520that%2520ensembles%2520independently%2520trained%2520uni-modal%250ALiDAR%2520and%2520RGB%2520detectors.%2520Importantly%252C%2520such%2520a%2520late-fusion%2520framework%2520allows%2520us%2520to%250Aleverage%2520large-scale%2520uni-modal%2520datasets%2520%2528with%2520more%2520examples%2520for%2520rare%2520classes%2529%250Ato%2520train%2520better%2520uni-modal%2520RGB%2520detectors%252C%2520unlike%2520prevailing%2520multimodal%2520detectors%250Athat%2520require%2520paired%2520multi-modal%2520training%2520data.%2520Notably%252C%2520our%2520approach%250Asignificantly%2520improves%2520rare-class%2520detection%2520by%25207.2%2525%2520over%2520prior%2520work.%2520Further%252C%250Awe%2520examine%2520three%2520critical%2520components%2520of%2520our%2520simple%2520late-fusion%2520approach%2520from%250Afirst%2520principles%2520and%2520investigate%2520whether%2520to%2520train%25202D%2520or%25203D%2520RGB%2520detectors%252C%250Awhether%2520to%2520match%2520RGB%2520and%2520LiDAR%2520detections%2520in%25203D%2520or%2520the%2520projected%25202D%2520image%2520plane%250Afor%2520fusion%252C%2520and%2520how%2520to%2520fuse%2520matched%2520detections.%2520Extensive%2520experiments%2520reveal%250Athat%25202D%2520RGB%2520detectors%2520achieve%2520better%2520recognition%2520accuracy%2520for%2520rare%2520classes%2520than%250A3D%2520RGB%2520detectors%2520and%2520matching%2520on%2520the%25202D%2520image%2520plane%2520mitigates%2520depth%2520estimation%250Aerrors.%2520Our%2520late-fusion%2520approach%2520achieves%252051.4%2520mAP%2520on%2520the%2520established%2520nuScenes%250ALT3D%2520benchmark%252C%2520improving%2520over%2520prior%2520work%2520by%25205.9%2520mAP%2521%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10986v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Tailed%203D%20Detection%20via%202D%20Late%20Fusion&entry.906535625=Yechi%20Ma%20and%20Neehar%20Peri%20and%20Shuoquan%20Wei%20and%20Wei%20Hua%20and%20Deva%20Ramanan%20and%20Yanan%20Li%20and%20Shu%20Kong&entry.1292438233=%20%20Long-Tailed%203D%20Object%20Detection%20%28LT3D%29%20addresses%20the%20problem%20of%20accurately%0Adetecting%20objects%20from%20both%20common%20and%20rare%20classes.%20Contemporary%20multi-modal%0Adetectors%20achieve%20low%20AP%20on%20rare-classes%20%28e.g.%2C%20CMT%20only%20achieves%209.4%20AP%20on%0Astroller%29%2C%20presumably%20because%20training%20detectors%20end-to-end%20with%20significant%0Aclass%20imbalance%20is%20challenging.%20To%20address%20this%20limitation%2C%20we%20delve%20into%20a%0Asimple%20late-fusion%20framework%20that%20ensembles%20independently%20trained%20uni-modal%0ALiDAR%20and%20RGB%20detectors.%20Importantly%2C%20such%20a%20late-fusion%20framework%20allows%20us%20to%0Aleverage%20large-scale%20uni-modal%20datasets%20%28with%20more%20examples%20for%20rare%20classes%29%0Ato%20train%20better%20uni-modal%20RGB%20detectors%2C%20unlike%20prevailing%20multimodal%20detectors%0Athat%20require%20paired%20multi-modal%20training%20data.%20Notably%2C%20our%20approach%0Asignificantly%20improves%20rare-class%20detection%20by%207.2%25%20over%20prior%20work.%20Further%2C%0Awe%20examine%20three%20critical%20components%20of%20our%20simple%20late-fusion%20approach%20from%0Afirst%20principles%20and%20investigate%20whether%20to%20train%202D%20or%203D%20RGB%20detectors%2C%0Awhether%20to%20match%20RGB%20and%20LiDAR%20detections%20in%203D%20or%20the%20projected%202D%20image%20plane%0Afor%20fusion%2C%20and%20how%20to%20fuse%20matched%20detections.%20Extensive%20experiments%20reveal%0Athat%202D%20RGB%20detectors%20achieve%20better%20recognition%20accuracy%20for%20rare%20classes%20than%0A3D%20RGB%20detectors%20and%20matching%20on%20the%202D%20image%20plane%20mitigates%20depth%20estimation%0Aerrors.%20Our%20late-fusion%20approach%20achieves%2051.4%20mAP%20on%20the%20established%20nuScenes%0ALT3D%20benchmark%2C%20improving%20over%20prior%20work%20by%205.9%20mAP%21%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10986v3&entry.124074799=Read"},
{"title": "SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of\n  LLMs", "author": "Mohammad Mozaffari and Amir Yazdanbakhsh and Zhao Zhang and Maryam Mehri Dehnavi", "abstract": "  We propose SLoPe, a Double-Pruned Sparse Plus Lazy Low-rank Adapter\nPretraining method for LLMs that improves the accuracy of sparse LLMs while\naccelerating their pretraining and inference and reducing their memory\nfootprint. Sparse pretraining of LLMs reduces the accuracy of the model, to\novercome this, prior work uses dense models during fine-tuning. SLoPe improves\nthe accuracy of sparsely pretrained models by adding low-rank adapters in the\nfinal 1% iterations of pretraining without adding significant overheads to the\nmodel pretraining and inference. In addition, SLoPe uses a double-pruned\nbackward pass formulation that prunes the transposed weight matrix using N:M\nsparsity structures to enable an accelerated sparse backward pass. SLoPe\naccelerates the training and inference of models with billions of parameters up\nto $1.14\\times$ and $1.34\\times$ respectively (OPT-33B and OPT-66B) while\nreducing their memory usage by up to $0.77\\times$ and $0.51\\times$ for training\nand inference respectively.\n", "link": "http://arxiv.org/abs/2405.16325v2", "date": "2024-06-14", "relevancy": 2.4284, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5203}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4804}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLoPe%3A%20Double-Pruned%20Sparse%20Plus%20Lazy%20Low-Rank%20Adapter%20Pretraining%20of%0A%20%20LLMs&body=Title%3A%20SLoPe%3A%20Double-Pruned%20Sparse%20Plus%20Lazy%20Low-Rank%20Adapter%20Pretraining%20of%0A%20%20LLMs%0AAuthor%3A%20Mohammad%20Mozaffari%20and%20Amir%20Yazdanbakhsh%20and%20Zhao%20Zhang%20and%20Maryam%20Mehri%20Dehnavi%0AAbstract%3A%20%20%20We%20propose%20SLoPe%2C%20a%20Double-Pruned%20Sparse%20Plus%20Lazy%20Low-rank%20Adapter%0APretraining%20method%20for%20LLMs%20that%20improves%20the%20accuracy%20of%20sparse%20LLMs%20while%0Aaccelerating%20their%20pretraining%20and%20inference%20and%20reducing%20their%20memory%0Afootprint.%20Sparse%20pretraining%20of%20LLMs%20reduces%20the%20accuracy%20of%20the%20model%2C%20to%0Aovercome%20this%2C%20prior%20work%20uses%20dense%20models%20during%20fine-tuning.%20SLoPe%20improves%0Athe%20accuracy%20of%20sparsely%20pretrained%20models%20by%20adding%20low-rank%20adapters%20in%20the%0Afinal%201%25%20iterations%20of%20pretraining%20without%20adding%20significant%20overheads%20to%20the%0Amodel%20pretraining%20and%20inference.%20In%20addition%2C%20SLoPe%20uses%20a%20double-pruned%0Abackward%20pass%20formulation%20that%20prunes%20the%20transposed%20weight%20matrix%20using%20N%3AM%0Asparsity%20structures%20to%20enable%20an%20accelerated%20sparse%20backward%20pass.%20SLoPe%0Aaccelerates%20the%20training%20and%20inference%20of%20models%20with%20billions%20of%20parameters%20up%0Ato%20%241.14%5Ctimes%24%20and%20%241.34%5Ctimes%24%20respectively%20%28OPT-33B%20and%20OPT-66B%29%20while%0Areducing%20their%20memory%20usage%20by%20up%20to%20%240.77%5Ctimes%24%20and%20%240.51%5Ctimes%24%20for%20training%0Aand%20inference%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16325v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLoPe%253A%2520Double-Pruned%2520Sparse%2520Plus%2520Lazy%2520Low-Rank%2520Adapter%2520Pretraining%2520of%250A%2520%2520LLMs%26entry.906535625%3DMohammad%2520Mozaffari%2520and%2520Amir%2520Yazdanbakhsh%2520and%2520Zhao%2520Zhang%2520and%2520Maryam%2520Mehri%2520Dehnavi%26entry.1292438233%3D%2520%2520We%2520propose%2520SLoPe%252C%2520a%2520Double-Pruned%2520Sparse%2520Plus%2520Lazy%2520Low-rank%2520Adapter%250APretraining%2520method%2520for%2520LLMs%2520that%2520improves%2520the%2520accuracy%2520of%2520sparse%2520LLMs%2520while%250Aaccelerating%2520their%2520pretraining%2520and%2520inference%2520and%2520reducing%2520their%2520memory%250Afootprint.%2520Sparse%2520pretraining%2520of%2520LLMs%2520reduces%2520the%2520accuracy%2520of%2520the%2520model%252C%2520to%250Aovercome%2520this%252C%2520prior%2520work%2520uses%2520dense%2520models%2520during%2520fine-tuning.%2520SLoPe%2520improves%250Athe%2520accuracy%2520of%2520sparsely%2520pretrained%2520models%2520by%2520adding%2520low-rank%2520adapters%2520in%2520the%250Afinal%25201%2525%2520iterations%2520of%2520pretraining%2520without%2520adding%2520significant%2520overheads%2520to%2520the%250Amodel%2520pretraining%2520and%2520inference.%2520In%2520addition%252C%2520SLoPe%2520uses%2520a%2520double-pruned%250Abackward%2520pass%2520formulation%2520that%2520prunes%2520the%2520transposed%2520weight%2520matrix%2520using%2520N%253AM%250Asparsity%2520structures%2520to%2520enable%2520an%2520accelerated%2520sparse%2520backward%2520pass.%2520SLoPe%250Aaccelerates%2520the%2520training%2520and%2520inference%2520of%2520models%2520with%2520billions%2520of%2520parameters%2520up%250Ato%2520%25241.14%255Ctimes%2524%2520and%2520%25241.34%255Ctimes%2524%2520respectively%2520%2528OPT-33B%2520and%2520OPT-66B%2529%2520while%250Areducing%2520their%2520memory%2520usage%2520by%2520up%2520to%2520%25240.77%255Ctimes%2524%2520and%2520%25240.51%255Ctimes%2524%2520for%2520training%250Aand%2520inference%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16325v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLoPe%3A%20Double-Pruned%20Sparse%20Plus%20Lazy%20Low-Rank%20Adapter%20Pretraining%20of%0A%20%20LLMs&entry.906535625=Mohammad%20Mozaffari%20and%20Amir%20Yazdanbakhsh%20and%20Zhao%20Zhang%20and%20Maryam%20Mehri%20Dehnavi&entry.1292438233=%20%20We%20propose%20SLoPe%2C%20a%20Double-Pruned%20Sparse%20Plus%20Lazy%20Low-rank%20Adapter%0APretraining%20method%20for%20LLMs%20that%20improves%20the%20accuracy%20of%20sparse%20LLMs%20while%0Aaccelerating%20their%20pretraining%20and%20inference%20and%20reducing%20their%20memory%0Afootprint.%20Sparse%20pretraining%20of%20LLMs%20reduces%20the%20accuracy%20of%20the%20model%2C%20to%0Aovercome%20this%2C%20prior%20work%20uses%20dense%20models%20during%20fine-tuning.%20SLoPe%20improves%0Athe%20accuracy%20of%20sparsely%20pretrained%20models%20by%20adding%20low-rank%20adapters%20in%20the%0Afinal%201%25%20iterations%20of%20pretraining%20without%20adding%20significant%20overheads%20to%20the%0Amodel%20pretraining%20and%20inference.%20In%20addition%2C%20SLoPe%20uses%20a%20double-pruned%0Abackward%20pass%20formulation%20that%20prunes%20the%20transposed%20weight%20matrix%20using%20N%3AM%0Asparsity%20structures%20to%20enable%20an%20accelerated%20sparse%20backward%20pass.%20SLoPe%0Aaccelerates%20the%20training%20and%20inference%20of%20models%20with%20billions%20of%20parameters%20up%0Ato%20%241.14%5Ctimes%24%20and%20%241.34%5Ctimes%24%20respectively%20%28OPT-33B%20and%20OPT-66B%29%20while%0Areducing%20their%20memory%20usage%20by%20up%20to%20%240.77%5Ctimes%24%20and%20%240.51%5Ctimes%24%20for%20training%0Aand%20inference%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16325v2&entry.124074799=Read"},
{"title": "Tilt and Average : Geometric Adjustment of the Last Layer for\n  Recalibration", "author": "Gyusang Cho and Chan-Hyun Youn", "abstract": "  After the revelation that neural networks tend to produce overconfident\npredictions, the problem of calibration, which aims to align confidence with\naccuracy to enhance the reliability of predictions, has gained significant\nimportance. Several solutions based on calibration maps have been proposed to\naddress the problem of recalibrating a trained classifier using additional\ndatasets. In this paper, we offer an algorithm that transforms the weights of\nthe last layer of the classifier, distinct from the calibration-map-based\napproach. We concentrate on the geometry of the final linear layer,\nspecifically its angular aspect, and adjust the weights of the corresponding\nlayer. We name the method Tilt and Average(\\textsc{Tna}), and validate the\ncalibration effect empirically and theoretically. Through this, we demonstrate\nthat our approach, in addition to the existing calibration-map-based\ntechniques, can yield improved calibration performance. Code available :\nhttps://github.com/GYYYYYUUUUU/TNA_Angular_Scaling.\n", "link": "http://arxiv.org/abs/2406.10017v1", "date": "2024-06-14", "relevancy": 2.4211, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4885}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4873}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tilt%20and%20Average%20%3A%20Geometric%20Adjustment%20of%20the%20Last%20Layer%20for%0A%20%20Recalibration&body=Title%3A%20Tilt%20and%20Average%20%3A%20Geometric%20Adjustment%20of%20the%20Last%20Layer%20for%0A%20%20Recalibration%0AAuthor%3A%20Gyusang%20Cho%20and%20Chan-Hyun%20Youn%0AAbstract%3A%20%20%20After%20the%20revelation%20that%20neural%20networks%20tend%20to%20produce%20overconfident%0Apredictions%2C%20the%20problem%20of%20calibration%2C%20which%20aims%20to%20align%20confidence%20with%0Aaccuracy%20to%20enhance%20the%20reliability%20of%20predictions%2C%20has%20gained%20significant%0Aimportance.%20Several%20solutions%20based%20on%20calibration%20maps%20have%20been%20proposed%20to%0Aaddress%20the%20problem%20of%20recalibrating%20a%20trained%20classifier%20using%20additional%0Adatasets.%20In%20this%20paper%2C%20we%20offer%20an%20algorithm%20that%20transforms%20the%20weights%20of%0Athe%20last%20layer%20of%20the%20classifier%2C%20distinct%20from%20the%20calibration-map-based%0Aapproach.%20We%20concentrate%20on%20the%20geometry%20of%20the%20final%20linear%20layer%2C%0Aspecifically%20its%20angular%20aspect%2C%20and%20adjust%20the%20weights%20of%20the%20corresponding%0Alayer.%20We%20name%20the%20method%20Tilt%20and%20Average%28%5Ctextsc%7BTna%7D%29%2C%20and%20validate%20the%0Acalibration%20effect%20empirically%20and%20theoretically.%20Through%20this%2C%20we%20demonstrate%0Athat%20our%20approach%2C%20in%20addition%20to%20the%20existing%20calibration-map-based%0Atechniques%2C%20can%20yield%20improved%20calibration%20performance.%20Code%20available%20%3A%0Ahttps%3A//github.com/GYYYYYUUUUU/TNA_Angular_Scaling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTilt%2520and%2520Average%2520%253A%2520Geometric%2520Adjustment%2520of%2520the%2520Last%2520Layer%2520for%250A%2520%2520Recalibration%26entry.906535625%3DGyusang%2520Cho%2520and%2520Chan-Hyun%2520Youn%26entry.1292438233%3D%2520%2520After%2520the%2520revelation%2520that%2520neural%2520networks%2520tend%2520to%2520produce%2520overconfident%250Apredictions%252C%2520the%2520problem%2520of%2520calibration%252C%2520which%2520aims%2520to%2520align%2520confidence%2520with%250Aaccuracy%2520to%2520enhance%2520the%2520reliability%2520of%2520predictions%252C%2520has%2520gained%2520significant%250Aimportance.%2520Several%2520solutions%2520based%2520on%2520calibration%2520maps%2520have%2520been%2520proposed%2520to%250Aaddress%2520the%2520problem%2520of%2520recalibrating%2520a%2520trained%2520classifier%2520using%2520additional%250Adatasets.%2520In%2520this%2520paper%252C%2520we%2520offer%2520an%2520algorithm%2520that%2520transforms%2520the%2520weights%2520of%250Athe%2520last%2520layer%2520of%2520the%2520classifier%252C%2520distinct%2520from%2520the%2520calibration-map-based%250Aapproach.%2520We%2520concentrate%2520on%2520the%2520geometry%2520of%2520the%2520final%2520linear%2520layer%252C%250Aspecifically%2520its%2520angular%2520aspect%252C%2520and%2520adjust%2520the%2520weights%2520of%2520the%2520corresponding%250Alayer.%2520We%2520name%2520the%2520method%2520Tilt%2520and%2520Average%2528%255Ctextsc%257BTna%257D%2529%252C%2520and%2520validate%2520the%250Acalibration%2520effect%2520empirically%2520and%2520theoretically.%2520Through%2520this%252C%2520we%2520demonstrate%250Athat%2520our%2520approach%252C%2520in%2520addition%2520to%2520the%2520existing%2520calibration-map-based%250Atechniques%252C%2520can%2520yield%2520improved%2520calibration%2520performance.%2520Code%2520available%2520%253A%250Ahttps%253A//github.com/GYYYYYUUUUU/TNA_Angular_Scaling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tilt%20and%20Average%20%3A%20Geometric%20Adjustment%20of%20the%20Last%20Layer%20for%0A%20%20Recalibration&entry.906535625=Gyusang%20Cho%20and%20Chan-Hyun%20Youn&entry.1292438233=%20%20After%20the%20revelation%20that%20neural%20networks%20tend%20to%20produce%20overconfident%0Apredictions%2C%20the%20problem%20of%20calibration%2C%20which%20aims%20to%20align%20confidence%20with%0Aaccuracy%20to%20enhance%20the%20reliability%20of%20predictions%2C%20has%20gained%20significant%0Aimportance.%20Several%20solutions%20based%20on%20calibration%20maps%20have%20been%20proposed%20to%0Aaddress%20the%20problem%20of%20recalibrating%20a%20trained%20classifier%20using%20additional%0Adatasets.%20In%20this%20paper%2C%20we%20offer%20an%20algorithm%20that%20transforms%20the%20weights%20of%0Athe%20last%20layer%20of%20the%20classifier%2C%20distinct%20from%20the%20calibration-map-based%0Aapproach.%20We%20concentrate%20on%20the%20geometry%20of%20the%20final%20linear%20layer%2C%0Aspecifically%20its%20angular%20aspect%2C%20and%20adjust%20the%20weights%20of%20the%20corresponding%0Alayer.%20We%20name%20the%20method%20Tilt%20and%20Average%28%5Ctextsc%7BTna%7D%29%2C%20and%20validate%20the%0Acalibration%20effect%20empirically%20and%20theoretically.%20Through%20this%2C%20we%20demonstrate%0Athat%20our%20approach%2C%20in%20addition%20to%20the%20existing%20calibration-map-based%0Atechniques%2C%20can%20yield%20improved%20calibration%20performance.%20Code%20available%20%3A%0Ahttps%3A//github.com/GYYYYYUUUUU/TNA_Angular_Scaling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10017v1&entry.124074799=Read"},
{"title": "Generative AI to Generate Test Data Generators", "author": "Benoit Baudry and Khashayar Etemadi and Sen Fang and Yogya Gamage and Yi Liu and Yuxin Liu and Martin Monperrus and Javier Ron and Andr\u00e9 Silva and Deepika Tiwari", "abstract": "  Generating fake data is an essential dimension of modern software testing, as\ndemonstrated by the number and significance of data faking libraries. Yet,\ndevelopers of faking libraries cannot keep up with the wide range of data to be\ngenerated for different natural languages and domains. In this paper, we assess\nthe ability of generative AI for generating test data in different domains. We\ndesign three types of prompts for Large Language Models (LLMs), which perform\ntest data generation tasks at different levels of integrability: 1) raw test\ndata generation, 2) synthesizing programs in a specific language that generate\nuseful test data, and 3) producing programs that use state-of-the-art faker\nlibraries. We evaluate our approach by prompting LLMs to generate test data for\n11 domains. The results show that LLMs can successfully generate realistic test\ndata generators in a wide range of domains at all three levels of\nintegrability.\n", "link": "http://arxiv.org/abs/2401.17626v2", "date": "2024-06-14", "relevancy": 2.3848, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5299}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4519}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20to%20Generate%20Test%20Data%20Generators&body=Title%3A%20Generative%20AI%20to%20Generate%20Test%20Data%20Generators%0AAuthor%3A%20Benoit%20Baudry%20and%20Khashayar%20Etemadi%20and%20Sen%20Fang%20and%20Yogya%20Gamage%20and%20Yi%20Liu%20and%20Yuxin%20Liu%20and%20Martin%20Monperrus%20and%20Javier%20Ron%20and%20Andr%C3%A9%20Silva%20and%20Deepika%20Tiwari%0AAbstract%3A%20%20%20Generating%20fake%20data%20is%20an%20essential%20dimension%20of%20modern%20software%20testing%2C%20as%0Ademonstrated%20by%20the%20number%20and%20significance%20of%20data%20faking%20libraries.%20Yet%2C%0Adevelopers%20of%20faking%20libraries%20cannot%20keep%20up%20with%20the%20wide%20range%20of%20data%20to%20be%0Agenerated%20for%20different%20natural%20languages%20and%20domains.%20In%20this%20paper%2C%20we%20assess%0Athe%20ability%20of%20generative%20AI%20for%20generating%20test%20data%20in%20different%20domains.%20We%0Adesign%20three%20types%20of%20prompts%20for%20Large%20Language%20Models%20%28LLMs%29%2C%20which%20perform%0Atest%20data%20generation%20tasks%20at%20different%20levels%20of%20integrability%3A%201%29%20raw%20test%0Adata%20generation%2C%202%29%20synthesizing%20programs%20in%20a%20specific%20language%20that%20generate%0Auseful%20test%20data%2C%20and%203%29%20producing%20programs%20that%20use%20state-of-the-art%20faker%0Alibraries.%20We%20evaluate%20our%20approach%20by%20prompting%20LLMs%20to%20generate%20test%20data%20for%0A11%20domains.%20The%20results%20show%20that%20LLMs%20can%20successfully%20generate%20realistic%20test%0Adata%20generators%20in%20a%20wide%20range%20of%20domains%20at%20all%20three%20levels%20of%0Aintegrability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.17626v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520to%2520Generate%2520Test%2520Data%2520Generators%26entry.906535625%3DBenoit%2520Baudry%2520and%2520Khashayar%2520Etemadi%2520and%2520Sen%2520Fang%2520and%2520Yogya%2520Gamage%2520and%2520Yi%2520Liu%2520and%2520Yuxin%2520Liu%2520and%2520Martin%2520Monperrus%2520and%2520Javier%2520Ron%2520and%2520Andr%25C3%25A9%2520Silva%2520and%2520Deepika%2520Tiwari%26entry.1292438233%3D%2520%2520Generating%2520fake%2520data%2520is%2520an%2520essential%2520dimension%2520of%2520modern%2520software%2520testing%252C%2520as%250Ademonstrated%2520by%2520the%2520number%2520and%2520significance%2520of%2520data%2520faking%2520libraries.%2520Yet%252C%250Adevelopers%2520of%2520faking%2520libraries%2520cannot%2520keep%2520up%2520with%2520the%2520wide%2520range%2520of%2520data%2520to%2520be%250Agenerated%2520for%2520different%2520natural%2520languages%2520and%2520domains.%2520In%2520this%2520paper%252C%2520we%2520assess%250Athe%2520ability%2520of%2520generative%2520AI%2520for%2520generating%2520test%2520data%2520in%2520different%2520domains.%2520We%250Adesign%2520three%2520types%2520of%2520prompts%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520which%2520perform%250Atest%2520data%2520generation%2520tasks%2520at%2520different%2520levels%2520of%2520integrability%253A%25201%2529%2520raw%2520test%250Adata%2520generation%252C%25202%2529%2520synthesizing%2520programs%2520in%2520a%2520specific%2520language%2520that%2520generate%250Auseful%2520test%2520data%252C%2520and%25203%2529%2520producing%2520programs%2520that%2520use%2520state-of-the-art%2520faker%250Alibraries.%2520We%2520evaluate%2520our%2520approach%2520by%2520prompting%2520LLMs%2520to%2520generate%2520test%2520data%2520for%250A11%2520domains.%2520The%2520results%2520show%2520that%2520LLMs%2520can%2520successfully%2520generate%2520realistic%2520test%250Adata%2520generators%2520in%2520a%2520wide%2520range%2520of%2520domains%2520at%2520all%2520three%2520levels%2520of%250Aintegrability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.17626v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20to%20Generate%20Test%20Data%20Generators&entry.906535625=Benoit%20Baudry%20and%20Khashayar%20Etemadi%20and%20Sen%20Fang%20and%20Yogya%20Gamage%20and%20Yi%20Liu%20and%20Yuxin%20Liu%20and%20Martin%20Monperrus%20and%20Javier%20Ron%20and%20Andr%C3%A9%20Silva%20and%20Deepika%20Tiwari&entry.1292438233=%20%20Generating%20fake%20data%20is%20an%20essential%20dimension%20of%20modern%20software%20testing%2C%20as%0Ademonstrated%20by%20the%20number%20and%20significance%20of%20data%20faking%20libraries.%20Yet%2C%0Adevelopers%20of%20faking%20libraries%20cannot%20keep%20up%20with%20the%20wide%20range%20of%20data%20to%20be%0Agenerated%20for%20different%20natural%20languages%20and%20domains.%20In%20this%20paper%2C%20we%20assess%0Athe%20ability%20of%20generative%20AI%20for%20generating%20test%20data%20in%20different%20domains.%20We%0Adesign%20three%20types%20of%20prompts%20for%20Large%20Language%20Models%20%28LLMs%29%2C%20which%20perform%0Atest%20data%20generation%20tasks%20at%20different%20levels%20of%20integrability%3A%201%29%20raw%20test%0Adata%20generation%2C%202%29%20synthesizing%20programs%20in%20a%20specific%20language%20that%20generate%0Auseful%20test%20data%2C%20and%203%29%20producing%20programs%20that%20use%20state-of-the-art%20faker%0Alibraries.%20We%20evaluate%20our%20approach%20by%20prompting%20LLMs%20to%20generate%20test%20data%20for%0A11%20domains.%20The%20results%20show%20that%20LLMs%20can%20successfully%20generate%20realistic%20test%0Adata%20generators%20in%20a%20wide%20range%20of%20domains%20at%20all%20three%20levels%20of%0Aintegrability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.17626v2&entry.124074799=Read"},
{"title": "GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable\n  Simulation, Demonstration, and Imitation", "author": "Zifan Wang and Junyu Chen and Ziqing Chen and Pengwei Xie and Rui Chen and Li Yi", "abstract": "  This paper presents GenH2R, a framework for learning generalizable\nvision-based human-to-robot (H2R) handover skills. The goal is to equip robots\nwith the ability to reliably receive objects with unseen geometry handed over\nby humans in various complex trajectories. We acquire such generalizability by\nlearning H2R handover at scale with a comprehensive solution including\nprocedural simulation assets creation, automated demonstration generation, and\neffective imitation learning. We leverage large-scale 3D model repositories,\ndexterous grasp generation methods, and curve-based 3D animation to create an\nH2R handover simulation environment named \\simabbns, surpassing the number of\nscenes in existing simulators by three orders of magnitude. We further\nintroduce a distillation-friendly demonstration generation method that\nautomatically generates a million high-quality demonstrations suitable for\nlearning. Finally, we present a 4D imitation learning method augmented by a\nfuture forecasting objective to distill demonstrations into a visuo-motor\nhandover policy. Experimental evaluations in both simulators and the real world\ndemonstrate significant improvements (at least +10\\% success rate) over\nbaselines in all cases. The project page is https://GenH2R.github.io/.\n", "link": "http://arxiv.org/abs/2401.00929v2", "date": "2024-06-14", "relevancy": 2.3745, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6085}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.585}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenH2R%3A%20Learning%20Generalizable%20Human-to-Robot%20Handover%20via%20Scalable%0A%20%20Simulation%2C%20Demonstration%2C%20and%20Imitation&body=Title%3A%20GenH2R%3A%20Learning%20Generalizable%20Human-to-Robot%20Handover%20via%20Scalable%0A%20%20Simulation%2C%20Demonstration%2C%20and%20Imitation%0AAuthor%3A%20Zifan%20Wang%20and%20Junyu%20Chen%20and%20Ziqing%20Chen%20and%20Pengwei%20Xie%20and%20Rui%20Chen%20and%20Li%20Yi%0AAbstract%3A%20%20%20This%20paper%20presents%20GenH2R%2C%20a%20framework%20for%20learning%20generalizable%0Avision-based%20human-to-robot%20%28H2R%29%20handover%20skills.%20The%20goal%20is%20to%20equip%20robots%0Awith%20the%20ability%20to%20reliably%20receive%20objects%20with%20unseen%20geometry%20handed%20over%0Aby%20humans%20in%20various%20complex%20trajectories.%20We%20acquire%20such%20generalizability%20by%0Alearning%20H2R%20handover%20at%20scale%20with%20a%20comprehensive%20solution%20including%0Aprocedural%20simulation%20assets%20creation%2C%20automated%20demonstration%20generation%2C%20and%0Aeffective%20imitation%20learning.%20We%20leverage%20large-scale%203D%20model%20repositories%2C%0Adexterous%20grasp%20generation%20methods%2C%20and%20curve-based%203D%20animation%20to%20create%20an%0AH2R%20handover%20simulation%20environment%20named%20%5Csimabbns%2C%20surpassing%20the%20number%20of%0Ascenes%20in%20existing%20simulators%20by%20three%20orders%20of%20magnitude.%20We%20further%0Aintroduce%20a%20distillation-friendly%20demonstration%20generation%20method%20that%0Aautomatically%20generates%20a%20million%20high-quality%20demonstrations%20suitable%20for%0Alearning.%20Finally%2C%20we%20present%20a%204D%20imitation%20learning%20method%20augmented%20by%20a%0Afuture%20forecasting%20objective%20to%20distill%20demonstrations%20into%20a%20visuo-motor%0Ahandover%20policy.%20Experimental%20evaluations%20in%20both%20simulators%20and%20the%20real%20world%0Ademonstrate%20significant%20improvements%20%28at%20least%20%2B10%5C%25%20success%20rate%29%20over%0Abaselines%20in%20all%20cases.%20The%20project%20page%20is%20https%3A//GenH2R.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenH2R%253A%2520Learning%2520Generalizable%2520Human-to-Robot%2520Handover%2520via%2520Scalable%250A%2520%2520Simulation%252C%2520Demonstration%252C%2520and%2520Imitation%26entry.906535625%3DZifan%2520Wang%2520and%2520Junyu%2520Chen%2520and%2520Ziqing%2520Chen%2520and%2520Pengwei%2520Xie%2520and%2520Rui%2520Chen%2520and%2520Li%2520Yi%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520GenH2R%252C%2520a%2520framework%2520for%2520learning%2520generalizable%250Avision-based%2520human-to-robot%2520%2528H2R%2529%2520handover%2520skills.%2520The%2520goal%2520is%2520to%2520equip%2520robots%250Awith%2520the%2520ability%2520to%2520reliably%2520receive%2520objects%2520with%2520unseen%2520geometry%2520handed%2520over%250Aby%2520humans%2520in%2520various%2520complex%2520trajectories.%2520We%2520acquire%2520such%2520generalizability%2520by%250Alearning%2520H2R%2520handover%2520at%2520scale%2520with%2520a%2520comprehensive%2520solution%2520including%250Aprocedural%2520simulation%2520assets%2520creation%252C%2520automated%2520demonstration%2520generation%252C%2520and%250Aeffective%2520imitation%2520learning.%2520We%2520leverage%2520large-scale%25203D%2520model%2520repositories%252C%250Adexterous%2520grasp%2520generation%2520methods%252C%2520and%2520curve-based%25203D%2520animation%2520to%2520create%2520an%250AH2R%2520handover%2520simulation%2520environment%2520named%2520%255Csimabbns%252C%2520surpassing%2520the%2520number%2520of%250Ascenes%2520in%2520existing%2520simulators%2520by%2520three%2520orders%2520of%2520magnitude.%2520We%2520further%250Aintroduce%2520a%2520distillation-friendly%2520demonstration%2520generation%2520method%2520that%250Aautomatically%2520generates%2520a%2520million%2520high-quality%2520demonstrations%2520suitable%2520for%250Alearning.%2520Finally%252C%2520we%2520present%2520a%25204D%2520imitation%2520learning%2520method%2520augmented%2520by%2520a%250Afuture%2520forecasting%2520objective%2520to%2520distill%2520demonstrations%2520into%2520a%2520visuo-motor%250Ahandover%2520policy.%2520Experimental%2520evaluations%2520in%2520both%2520simulators%2520and%2520the%2520real%2520world%250Ademonstrate%2520significant%2520improvements%2520%2528at%2520least%2520%252B10%255C%2525%2520success%2520rate%2529%2520over%250Abaselines%2520in%2520all%2520cases.%2520The%2520project%2520page%2520is%2520https%253A//GenH2R.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.00929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenH2R%3A%20Learning%20Generalizable%20Human-to-Robot%20Handover%20via%20Scalable%0A%20%20Simulation%2C%20Demonstration%2C%20and%20Imitation&entry.906535625=Zifan%20Wang%20and%20Junyu%20Chen%20and%20Ziqing%20Chen%20and%20Pengwei%20Xie%20and%20Rui%20Chen%20and%20Li%20Yi&entry.1292438233=%20%20This%20paper%20presents%20GenH2R%2C%20a%20framework%20for%20learning%20generalizable%0Avision-based%20human-to-robot%20%28H2R%29%20handover%20skills.%20The%20goal%20is%20to%20equip%20robots%0Awith%20the%20ability%20to%20reliably%20receive%20objects%20with%20unseen%20geometry%20handed%20over%0Aby%20humans%20in%20various%20complex%20trajectories.%20We%20acquire%20such%20generalizability%20by%0Alearning%20H2R%20handover%20at%20scale%20with%20a%20comprehensive%20solution%20including%0Aprocedural%20simulation%20assets%20creation%2C%20automated%20demonstration%20generation%2C%20and%0Aeffective%20imitation%20learning.%20We%20leverage%20large-scale%203D%20model%20repositories%2C%0Adexterous%20grasp%20generation%20methods%2C%20and%20curve-based%203D%20animation%20to%20create%20an%0AH2R%20handover%20simulation%20environment%20named%20%5Csimabbns%2C%20surpassing%20the%20number%20of%0Ascenes%20in%20existing%20simulators%20by%20three%20orders%20of%20magnitude.%20We%20further%0Aintroduce%20a%20distillation-friendly%20demonstration%20generation%20method%20that%0Aautomatically%20generates%20a%20million%20high-quality%20demonstrations%20suitable%20for%0Alearning.%20Finally%2C%20we%20present%20a%204D%20imitation%20learning%20method%20augmented%20by%20a%0Afuture%20forecasting%20objective%20to%20distill%20demonstrations%20into%20a%20visuo-motor%0Ahandover%20policy.%20Experimental%20evaluations%20in%20both%20simulators%20and%20the%20real%20world%0Ademonstrate%20significant%20improvements%20%28at%20least%20%2B10%5C%25%20success%20rate%29%20over%0Abaselines%20in%20all%20cases.%20The%20project%20page%20is%20https%3A//GenH2R.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00929v2&entry.124074799=Read"},
{"title": "Robust Latent Representation Tuning for Image-text Classification", "author": "Hao Sun and Yu Song", "abstract": "  Large models have demonstrated exceptional generalization capabilities in\ncomputer vision and natural language processing. Recent efforts have focused on\nenhancing these models with multimodal processing abilities. However,\naddressing the challenges posed by scenarios where one modality is absent\nremains a significant hurdle. In response to this issue, we propose a robust\nlatent representation tuning method for large models. Specifically, our\napproach introduces a modality latent translation module to maximize the\ncorrelation between modalities, resulting in a robust representation. Following\nthis, a newly designed fusion module is employed to facilitate information\ninteraction between the modalities. Within this framework, common semantics are\nrefined during training, and robust performance is achieved even in the absence\nof one modality. Importantly, our method maintains the frozen state of the\nimage and text foundation models to preserve their capabilities acquired\nthrough large-scale pretraining. We conduct experiments on several public\ndatasets, and the results underscore the effectiveness of our proposed method.\n", "link": "http://arxiv.org/abs/2406.06048v2", "date": "2024-06-14", "relevancy": 2.3702, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6095}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6055}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Latent%20Representation%20Tuning%20for%20Image-text%20Classification&body=Title%3A%20Robust%20Latent%20Representation%20Tuning%20for%20Image-text%20Classification%0AAuthor%3A%20Hao%20Sun%20and%20Yu%20Song%0AAbstract%3A%20%20%20Large%20models%20have%20demonstrated%20exceptional%20generalization%20capabilities%20in%0Acomputer%20vision%20and%20natural%20language%20processing.%20Recent%20efforts%20have%20focused%20on%0Aenhancing%20these%20models%20with%20multimodal%20processing%20abilities.%20However%2C%0Aaddressing%20the%20challenges%20posed%20by%20scenarios%20where%20one%20modality%20is%20absent%0Aremains%20a%20significant%20hurdle.%20In%20response%20to%20this%20issue%2C%20we%20propose%20a%20robust%0Alatent%20representation%20tuning%20method%20for%20large%20models.%20Specifically%2C%20our%0Aapproach%20introduces%20a%20modality%20latent%20translation%20module%20to%20maximize%20the%0Acorrelation%20between%20modalities%2C%20resulting%20in%20a%20robust%20representation.%20Following%0Athis%2C%20a%20newly%20designed%20fusion%20module%20is%20employed%20to%20facilitate%20information%0Ainteraction%20between%20the%20modalities.%20Within%20this%20framework%2C%20common%20semantics%20are%0Arefined%20during%20training%2C%20and%20robust%20performance%20is%20achieved%20even%20in%20the%20absence%0Aof%20one%20modality.%20Importantly%2C%20our%20method%20maintains%20the%20frozen%20state%20of%20the%0Aimage%20and%20text%20foundation%20models%20to%20preserve%20their%20capabilities%20acquired%0Athrough%20large-scale%20pretraining.%20We%20conduct%20experiments%20on%20several%20public%0Adatasets%2C%20and%20the%20results%20underscore%20the%20effectiveness%20of%20our%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06048v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Latent%2520Representation%2520Tuning%2520for%2520Image-text%2520Classification%26entry.906535625%3DHao%2520Sun%2520and%2520Yu%2520Song%26entry.1292438233%3D%2520%2520Large%2520models%2520have%2520demonstrated%2520exceptional%2520generalization%2520capabilities%2520in%250Acomputer%2520vision%2520and%2520natural%2520language%2520processing.%2520Recent%2520efforts%2520have%2520focused%2520on%250Aenhancing%2520these%2520models%2520with%2520multimodal%2520processing%2520abilities.%2520However%252C%250Aaddressing%2520the%2520challenges%2520posed%2520by%2520scenarios%2520where%2520one%2520modality%2520is%2520absent%250Aremains%2520a%2520significant%2520hurdle.%2520In%2520response%2520to%2520this%2520issue%252C%2520we%2520propose%2520a%2520robust%250Alatent%2520representation%2520tuning%2520method%2520for%2520large%2520models.%2520Specifically%252C%2520our%250Aapproach%2520introduces%2520a%2520modality%2520latent%2520translation%2520module%2520to%2520maximize%2520the%250Acorrelation%2520between%2520modalities%252C%2520resulting%2520in%2520a%2520robust%2520representation.%2520Following%250Athis%252C%2520a%2520newly%2520designed%2520fusion%2520module%2520is%2520employed%2520to%2520facilitate%2520information%250Ainteraction%2520between%2520the%2520modalities.%2520Within%2520this%2520framework%252C%2520common%2520semantics%2520are%250Arefined%2520during%2520training%252C%2520and%2520robust%2520performance%2520is%2520achieved%2520even%2520in%2520the%2520absence%250Aof%2520one%2520modality.%2520Importantly%252C%2520our%2520method%2520maintains%2520the%2520frozen%2520state%2520of%2520the%250Aimage%2520and%2520text%2520foundation%2520models%2520to%2520preserve%2520their%2520capabilities%2520acquired%250Athrough%2520large-scale%2520pretraining.%2520We%2520conduct%2520experiments%2520on%2520several%2520public%250Adatasets%252C%2520and%2520the%2520results%2520underscore%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06048v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Latent%20Representation%20Tuning%20for%20Image-text%20Classification&entry.906535625=Hao%20Sun%20and%20Yu%20Song&entry.1292438233=%20%20Large%20models%20have%20demonstrated%20exceptional%20generalization%20capabilities%20in%0Acomputer%20vision%20and%20natural%20language%20processing.%20Recent%20efforts%20have%20focused%20on%0Aenhancing%20these%20models%20with%20multimodal%20processing%20abilities.%20However%2C%0Aaddressing%20the%20challenges%20posed%20by%20scenarios%20where%20one%20modality%20is%20absent%0Aremains%20a%20significant%20hurdle.%20In%20response%20to%20this%20issue%2C%20we%20propose%20a%20robust%0Alatent%20representation%20tuning%20method%20for%20large%20models.%20Specifically%2C%20our%0Aapproach%20introduces%20a%20modality%20latent%20translation%20module%20to%20maximize%20the%0Acorrelation%20between%20modalities%2C%20resulting%20in%20a%20robust%20representation.%20Following%0Athis%2C%20a%20newly%20designed%20fusion%20module%20is%20employed%20to%20facilitate%20information%0Ainteraction%20between%20the%20modalities.%20Within%20this%20framework%2C%20common%20semantics%20are%0Arefined%20during%20training%2C%20and%20robust%20performance%20is%20achieved%20even%20in%20the%20absence%0Aof%20one%20modality.%20Importantly%2C%20our%20method%20maintains%20the%20frozen%20state%20of%20the%0Aimage%20and%20text%20foundation%20models%20to%20preserve%20their%20capabilities%20acquired%0Athrough%20large-scale%20pretraining.%20We%20conduct%20experiments%20on%20several%20public%0Adatasets%2C%20and%20the%20results%20underscore%20the%20effectiveness%20of%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06048v2&entry.124074799=Read"},
{"title": "Make It Count: Text-to-Image Generation with an Accurate Number of\n  Objects", "author": "Lital Binyamin and Yoad Tewel and Hilit Segev and Eran Hirsch and Royi Rassin and Gal Chechik", "abstract": "  Despite the unprecedented success of text-to-image diffusion models,\ncontrolling the number of depicted objects using text is surprisingly hard.\nThis is important for various applications from technical documents, to\nchildren's books to illustrating cooking recipes. Generating object-correct\ncounts is fundamentally challenging because the generative model needs to keep\na sense of separate identity for every instance of the object, even if several\nobjects look identical or overlap, and then carry out a global computation\nimplicitly during generation. It is still unknown if such representations\nexist. To address count-correct generation, we first identify features within\nthe diffusion model that can carry the object identity information. We then use\nthem to separate and count instances of objects during the denoising process\nand detect over-generation and under-generation. We fix the latter by training\na model that predicts both the shape and location of a missing object, based on\nthe layout of existing ones, and show how it can be used to guide denoising\nwith correct object count. Our approach, CountGen, does not depend on external\nsource to determine object layout, but rather uses the prior from the diffusion\nmodel itself, creating prompt-dependent and seed-dependent layouts. Evaluated\non two benchmark datasets, we find that CountGen strongly outperforms the\ncount-accuracy of existing baselines.\n", "link": "http://arxiv.org/abs/2406.10210v1", "date": "2024-06-14", "relevancy": 2.3691, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6187}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6084}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Make%20It%20Count%3A%20Text-to-Image%20Generation%20with%20an%20Accurate%20Number%20of%0A%20%20Objects&body=Title%3A%20Make%20It%20Count%3A%20Text-to-Image%20Generation%20with%20an%20Accurate%20Number%20of%0A%20%20Objects%0AAuthor%3A%20Lital%20Binyamin%20and%20Yoad%20Tewel%20and%20Hilit%20Segev%20and%20Eran%20Hirsch%20and%20Royi%20Rassin%20and%20Gal%20Chechik%0AAbstract%3A%20%20%20Despite%20the%20unprecedented%20success%20of%20text-to-image%20diffusion%20models%2C%0Acontrolling%20the%20number%20of%20depicted%20objects%20using%20text%20is%20surprisingly%20hard.%0AThis%20is%20important%20for%20various%20applications%20from%20technical%20documents%2C%20to%0Achildren%27s%20books%20to%20illustrating%20cooking%20recipes.%20Generating%20object-correct%0Acounts%20is%20fundamentally%20challenging%20because%20the%20generative%20model%20needs%20to%20keep%0Aa%20sense%20of%20separate%20identity%20for%20every%20instance%20of%20the%20object%2C%20even%20if%20several%0Aobjects%20look%20identical%20or%20overlap%2C%20and%20then%20carry%20out%20a%20global%20computation%0Aimplicitly%20during%20generation.%20It%20is%20still%20unknown%20if%20such%20representations%0Aexist.%20To%20address%20count-correct%20generation%2C%20we%20first%20identify%20features%20within%0Athe%20diffusion%20model%20that%20can%20carry%20the%20object%20identity%20information.%20We%20then%20use%0Athem%20to%20separate%20and%20count%20instances%20of%20objects%20during%20the%20denoising%20process%0Aand%20detect%20over-generation%20and%20under-generation.%20We%20fix%20the%20latter%20by%20training%0Aa%20model%20that%20predicts%20both%20the%20shape%20and%20location%20of%20a%20missing%20object%2C%20based%20on%0Athe%20layout%20of%20existing%20ones%2C%20and%20show%20how%20it%20can%20be%20used%20to%20guide%20denoising%0Awith%20correct%20object%20count.%20Our%20approach%2C%20CountGen%2C%20does%20not%20depend%20on%20external%0Asource%20to%20determine%20object%20layout%2C%20but%20rather%20uses%20the%20prior%20from%20the%20diffusion%0Amodel%20itself%2C%20creating%20prompt-dependent%20and%20seed-dependent%20layouts.%20Evaluated%0Aon%20two%20benchmark%20datasets%2C%20we%20find%20that%20CountGen%20strongly%20outperforms%20the%0Acount-accuracy%20of%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMake%2520It%2520Count%253A%2520Text-to-Image%2520Generation%2520with%2520an%2520Accurate%2520Number%2520of%250A%2520%2520Objects%26entry.906535625%3DLital%2520Binyamin%2520and%2520Yoad%2520Tewel%2520and%2520Hilit%2520Segev%2520and%2520Eran%2520Hirsch%2520and%2520Royi%2520Rassin%2520and%2520Gal%2520Chechik%26entry.1292438233%3D%2520%2520Despite%2520the%2520unprecedented%2520success%2520of%2520text-to-image%2520diffusion%2520models%252C%250Acontrolling%2520the%2520number%2520of%2520depicted%2520objects%2520using%2520text%2520is%2520surprisingly%2520hard.%250AThis%2520is%2520important%2520for%2520various%2520applications%2520from%2520technical%2520documents%252C%2520to%250Achildren%2527s%2520books%2520to%2520illustrating%2520cooking%2520recipes.%2520Generating%2520object-correct%250Acounts%2520is%2520fundamentally%2520challenging%2520because%2520the%2520generative%2520model%2520needs%2520to%2520keep%250Aa%2520sense%2520of%2520separate%2520identity%2520for%2520every%2520instance%2520of%2520the%2520object%252C%2520even%2520if%2520several%250Aobjects%2520look%2520identical%2520or%2520overlap%252C%2520and%2520then%2520carry%2520out%2520a%2520global%2520computation%250Aimplicitly%2520during%2520generation.%2520It%2520is%2520still%2520unknown%2520if%2520such%2520representations%250Aexist.%2520To%2520address%2520count-correct%2520generation%252C%2520we%2520first%2520identify%2520features%2520within%250Athe%2520diffusion%2520model%2520that%2520can%2520carry%2520the%2520object%2520identity%2520information.%2520We%2520then%2520use%250Athem%2520to%2520separate%2520and%2520count%2520instances%2520of%2520objects%2520during%2520the%2520denoising%2520process%250Aand%2520detect%2520over-generation%2520and%2520under-generation.%2520We%2520fix%2520the%2520latter%2520by%2520training%250Aa%2520model%2520that%2520predicts%2520both%2520the%2520shape%2520and%2520location%2520of%2520a%2520missing%2520object%252C%2520based%2520on%250Athe%2520layout%2520of%2520existing%2520ones%252C%2520and%2520show%2520how%2520it%2520can%2520be%2520used%2520to%2520guide%2520denoising%250Awith%2520correct%2520object%2520count.%2520Our%2520approach%252C%2520CountGen%252C%2520does%2520not%2520depend%2520on%2520external%250Asource%2520to%2520determine%2520object%2520layout%252C%2520but%2520rather%2520uses%2520the%2520prior%2520from%2520the%2520diffusion%250Amodel%2520itself%252C%2520creating%2520prompt-dependent%2520and%2520seed-dependent%2520layouts.%2520Evaluated%250Aon%2520two%2520benchmark%2520datasets%252C%2520we%2520find%2520that%2520CountGen%2520strongly%2520outperforms%2520the%250Acount-accuracy%2520of%2520existing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Make%20It%20Count%3A%20Text-to-Image%20Generation%20with%20an%20Accurate%20Number%20of%0A%20%20Objects&entry.906535625=Lital%20Binyamin%20and%20Yoad%20Tewel%20and%20Hilit%20Segev%20and%20Eran%20Hirsch%20and%20Royi%20Rassin%20and%20Gal%20Chechik&entry.1292438233=%20%20Despite%20the%20unprecedented%20success%20of%20text-to-image%20diffusion%20models%2C%0Acontrolling%20the%20number%20of%20depicted%20objects%20using%20text%20is%20surprisingly%20hard.%0AThis%20is%20important%20for%20various%20applications%20from%20technical%20documents%2C%20to%0Achildren%27s%20books%20to%20illustrating%20cooking%20recipes.%20Generating%20object-correct%0Acounts%20is%20fundamentally%20challenging%20because%20the%20generative%20model%20needs%20to%20keep%0Aa%20sense%20of%20separate%20identity%20for%20every%20instance%20of%20the%20object%2C%20even%20if%20several%0Aobjects%20look%20identical%20or%20overlap%2C%20and%20then%20carry%20out%20a%20global%20computation%0Aimplicitly%20during%20generation.%20It%20is%20still%20unknown%20if%20such%20representations%0Aexist.%20To%20address%20count-correct%20generation%2C%20we%20first%20identify%20features%20within%0Athe%20diffusion%20model%20that%20can%20carry%20the%20object%20identity%20information.%20We%20then%20use%0Athem%20to%20separate%20and%20count%20instances%20of%20objects%20during%20the%20denoising%20process%0Aand%20detect%20over-generation%20and%20under-generation.%20We%20fix%20the%20latter%20by%20training%0Aa%20model%20that%20predicts%20both%20the%20shape%20and%20location%20of%20a%20missing%20object%2C%20based%20on%0Athe%20layout%20of%20existing%20ones%2C%20and%20show%20how%20it%20can%20be%20used%20to%20guide%20denoising%0Awith%20correct%20object%20count.%20Our%20approach%2C%20CountGen%2C%20does%20not%20depend%20on%20external%0Asource%20to%20determine%20object%20layout%2C%20but%20rather%20uses%20the%20prior%20from%20the%20diffusion%0Amodel%20itself%2C%20creating%20prompt-dependent%20and%20seed-dependent%20layouts.%20Evaluated%0Aon%20two%20benchmark%20datasets%2C%20we%20find%20that%20CountGen%20strongly%20outperforms%20the%0Acount-accuracy%20of%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10210v1&entry.124074799=Read"},
{"title": "MapVision: CVPR 2024 Autonomous Grand Challenge Mapless Driving Tech\n  Report", "author": "Zhongyu Yang and Mai Liu and Jinluo Xie and Yueming Zhang and Chen Shen and Wei Shao and Jichao Jiao and Tengfei Xing and Runbo Hu and Pengfei Xu", "abstract": "  Autonomous driving without high-definition (HD) maps demands a higher level\nof active scene understanding. In this competition, the organizers provided the\nmulti-perspective camera images and standard-definition (SD) maps to explore\nthe boundaries of scene reasoning capabilities. We found that most existing\nalgorithms construct Bird's Eye View (BEV) features from these\nmulti-perspective images and use multi-task heads to delineate road\ncenterlines, boundary lines, pedestrian crossings, and other areas. However,\nthese algorithms perform poorly at the far end of roads and struggle when the\nprimary subject in the image is occluded. Therefore, in this competition, we\nnot only used multi-perspective images as input but also incorporated SD maps\nto address this issue. We employed map encoder pre-training to enhance the\nnetwork's geometric encoding capabilities and utilized YOLOX to improve traffic\nelement detection precision. Additionally, for area detection, we innovatively\nintroduced LDTR and auxiliary tasks to achieve higher precision. As a result,\nour final OLUS score is 0.58.\n", "link": "http://arxiv.org/abs/2406.10125v1", "date": "2024-06-14", "relevancy": 2.3669, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6265}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5764}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MapVision%3A%20CVPR%202024%20Autonomous%20Grand%20Challenge%20Mapless%20Driving%20Tech%0A%20%20Report&body=Title%3A%20MapVision%3A%20CVPR%202024%20Autonomous%20Grand%20Challenge%20Mapless%20Driving%20Tech%0A%20%20Report%0AAuthor%3A%20Zhongyu%20Yang%20and%20Mai%20Liu%20and%20Jinluo%20Xie%20and%20Yueming%20Zhang%20and%20Chen%20Shen%20and%20Wei%20Shao%20and%20Jichao%20Jiao%20and%20Tengfei%20Xing%20and%20Runbo%20Hu%20and%20Pengfei%20Xu%0AAbstract%3A%20%20%20Autonomous%20driving%20without%20high-definition%20%28HD%29%20maps%20demands%20a%20higher%20level%0Aof%20active%20scene%20understanding.%20In%20this%20competition%2C%20the%20organizers%20provided%20the%0Amulti-perspective%20camera%20images%20and%20standard-definition%20%28SD%29%20maps%20to%20explore%0Athe%20boundaries%20of%20scene%20reasoning%20capabilities.%20We%20found%20that%20most%20existing%0Aalgorithms%20construct%20Bird%27s%20Eye%20View%20%28BEV%29%20features%20from%20these%0Amulti-perspective%20images%20and%20use%20multi-task%20heads%20to%20delineate%20road%0Acenterlines%2C%20boundary%20lines%2C%20pedestrian%20crossings%2C%20and%20other%20areas.%20However%2C%0Athese%20algorithms%20perform%20poorly%20at%20the%20far%20end%20of%20roads%20and%20struggle%20when%20the%0Aprimary%20subject%20in%20the%20image%20is%20occluded.%20Therefore%2C%20in%20this%20competition%2C%20we%0Anot%20only%20used%20multi-perspective%20images%20as%20input%20but%20also%20incorporated%20SD%20maps%0Ato%20address%20this%20issue.%20We%20employed%20map%20encoder%20pre-training%20to%20enhance%20the%0Anetwork%27s%20geometric%20encoding%20capabilities%20and%20utilized%20YOLOX%20to%20improve%20traffic%0Aelement%20detection%20precision.%20Additionally%2C%20for%20area%20detection%2C%20we%20innovatively%0Aintroduced%20LDTR%20and%20auxiliary%20tasks%20to%20achieve%20higher%20precision.%20As%20a%20result%2C%0Aour%20final%20OLUS%20score%20is%200.58.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapVision%253A%2520CVPR%25202024%2520Autonomous%2520Grand%2520Challenge%2520Mapless%2520Driving%2520Tech%250A%2520%2520Report%26entry.906535625%3DZhongyu%2520Yang%2520and%2520Mai%2520Liu%2520and%2520Jinluo%2520Xie%2520and%2520Yueming%2520Zhang%2520and%2520Chen%2520Shen%2520and%2520Wei%2520Shao%2520and%2520Jichao%2520Jiao%2520and%2520Tengfei%2520Xing%2520and%2520Runbo%2520Hu%2520and%2520Pengfei%2520Xu%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520without%2520high-definition%2520%2528HD%2529%2520maps%2520demands%2520a%2520higher%2520level%250Aof%2520active%2520scene%2520understanding.%2520In%2520this%2520competition%252C%2520the%2520organizers%2520provided%2520the%250Amulti-perspective%2520camera%2520images%2520and%2520standard-definition%2520%2528SD%2529%2520maps%2520to%2520explore%250Athe%2520boundaries%2520of%2520scene%2520reasoning%2520capabilities.%2520We%2520found%2520that%2520most%2520existing%250Aalgorithms%2520construct%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520features%2520from%2520these%250Amulti-perspective%2520images%2520and%2520use%2520multi-task%2520heads%2520to%2520delineate%2520road%250Acenterlines%252C%2520boundary%2520lines%252C%2520pedestrian%2520crossings%252C%2520and%2520other%2520areas.%2520However%252C%250Athese%2520algorithms%2520perform%2520poorly%2520at%2520the%2520far%2520end%2520of%2520roads%2520and%2520struggle%2520when%2520the%250Aprimary%2520subject%2520in%2520the%2520image%2520is%2520occluded.%2520Therefore%252C%2520in%2520this%2520competition%252C%2520we%250Anot%2520only%2520used%2520multi-perspective%2520images%2520as%2520input%2520but%2520also%2520incorporated%2520SD%2520maps%250Ato%2520address%2520this%2520issue.%2520We%2520employed%2520map%2520encoder%2520pre-training%2520to%2520enhance%2520the%250Anetwork%2527s%2520geometric%2520encoding%2520capabilities%2520and%2520utilized%2520YOLOX%2520to%2520improve%2520traffic%250Aelement%2520detection%2520precision.%2520Additionally%252C%2520for%2520area%2520detection%252C%2520we%2520innovatively%250Aintroduced%2520LDTR%2520and%2520auxiliary%2520tasks%2520to%2520achieve%2520higher%2520precision.%2520As%2520a%2520result%252C%250Aour%2520final%2520OLUS%2520score%2520is%25200.58.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MapVision%3A%20CVPR%202024%20Autonomous%20Grand%20Challenge%20Mapless%20Driving%20Tech%0A%20%20Report&entry.906535625=Zhongyu%20Yang%20and%20Mai%20Liu%20and%20Jinluo%20Xie%20and%20Yueming%20Zhang%20and%20Chen%20Shen%20and%20Wei%20Shao%20and%20Jichao%20Jiao%20and%20Tengfei%20Xing%20and%20Runbo%20Hu%20and%20Pengfei%20Xu&entry.1292438233=%20%20Autonomous%20driving%20without%20high-definition%20%28HD%29%20maps%20demands%20a%20higher%20level%0Aof%20active%20scene%20understanding.%20In%20this%20competition%2C%20the%20organizers%20provided%20the%0Amulti-perspective%20camera%20images%20and%20standard-definition%20%28SD%29%20maps%20to%20explore%0Athe%20boundaries%20of%20scene%20reasoning%20capabilities.%20We%20found%20that%20most%20existing%0Aalgorithms%20construct%20Bird%27s%20Eye%20View%20%28BEV%29%20features%20from%20these%0Amulti-perspective%20images%20and%20use%20multi-task%20heads%20to%20delineate%20road%0Acenterlines%2C%20boundary%20lines%2C%20pedestrian%20crossings%2C%20and%20other%20areas.%20However%2C%0Athese%20algorithms%20perform%20poorly%20at%20the%20far%20end%20of%20roads%20and%20struggle%20when%20the%0Aprimary%20subject%20in%20the%20image%20is%20occluded.%20Therefore%2C%20in%20this%20competition%2C%20we%0Anot%20only%20used%20multi-perspective%20images%20as%20input%20but%20also%20incorporated%20SD%20maps%0Ato%20address%20this%20issue.%20We%20employed%20map%20encoder%20pre-training%20to%20enhance%20the%0Anetwork%27s%20geometric%20encoding%20capabilities%20and%20utilized%20YOLOX%20to%20improve%20traffic%0Aelement%20detection%20precision.%20Additionally%2C%20for%20area%20detection%2C%20we%20innovatively%0Aintroduced%20LDTR%20and%20auxiliary%20tasks%20to%20achieve%20higher%20precision.%20As%20a%20result%2C%0Aour%20final%20OLUS%20score%20is%200.58.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10125v1&entry.124074799=Read"},
{"title": "Any2Graph: Deep End-To-End Supervised Graph Prediction With An Optimal\n  Transport Loss", "author": "Paul Krzakala and Junjie Yang and R\u00e9mi Flamary and Florence d'Alch\u00e9-Buc and Charlotte Laclau and Matthieu Labeau", "abstract": "  We propose Any2graph, a generic framework for end-to-end Supervised Graph\nPrediction (SGP) i.e. a deep learning model that predicts an entire graph for\nany kind of input. The framework is built on a novel Optimal Transport loss,\nthe Partially-Masked Fused Gromov-Wasserstein, that exhibits all necessary\nproperties (permutation invariance, differentiability and scalability) and is\ndesigned to handle any-sized graphs. Numerical experiments showcase the\nversatility of the approach that outperform existing competitors on a novel\nchallenging synthetic dataset and a variety of real-world tasks such as map\nconstruction from satellite image (Sat2Graph) or molecule prediction from\nfingerprint (Fingerprint2Graph).\n", "link": "http://arxiv.org/abs/2402.12269v3", "date": "2024-06-14", "relevancy": 2.3623, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.485}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4746}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Any2Graph%3A%20Deep%20End-To-End%20Supervised%20Graph%20Prediction%20With%20An%20Optimal%0A%20%20Transport%20Loss&body=Title%3A%20Any2Graph%3A%20Deep%20End-To-End%20Supervised%20Graph%20Prediction%20With%20An%20Optimal%0A%20%20Transport%20Loss%0AAuthor%3A%20Paul%20Krzakala%20and%20Junjie%20Yang%20and%20R%C3%A9mi%20Flamary%20and%20Florence%20d%27Alch%C3%A9-Buc%20and%20Charlotte%20Laclau%20and%20Matthieu%20Labeau%0AAbstract%3A%20%20%20We%20propose%20Any2graph%2C%20a%20generic%20framework%20for%20end-to-end%20Supervised%20Graph%0APrediction%20%28SGP%29%20i.e.%20a%20deep%20learning%20model%20that%20predicts%20an%20entire%20graph%20for%0Aany%20kind%20of%20input.%20The%20framework%20is%20built%20on%20a%20novel%20Optimal%20Transport%20loss%2C%0Athe%20Partially-Masked%20Fused%20Gromov-Wasserstein%2C%20that%20exhibits%20all%20necessary%0Aproperties%20%28permutation%20invariance%2C%20differentiability%20and%20scalability%29%20and%20is%0Adesigned%20to%20handle%20any-sized%20graphs.%20Numerical%20experiments%20showcase%20the%0Aversatility%20of%20the%20approach%20that%20outperform%20existing%20competitors%20on%20a%20novel%0Achallenging%20synthetic%20dataset%20and%20a%20variety%20of%20real-world%20tasks%20such%20as%20map%0Aconstruction%20from%20satellite%20image%20%28Sat2Graph%29%20or%20molecule%20prediction%20from%0Afingerprint%20%28Fingerprint2Graph%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12269v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAny2Graph%253A%2520Deep%2520End-To-End%2520Supervised%2520Graph%2520Prediction%2520With%2520An%2520Optimal%250A%2520%2520Transport%2520Loss%26entry.906535625%3DPaul%2520Krzakala%2520and%2520Junjie%2520Yang%2520and%2520R%25C3%25A9mi%2520Flamary%2520and%2520Florence%2520d%2527Alch%25C3%25A9-Buc%2520and%2520Charlotte%2520Laclau%2520and%2520Matthieu%2520Labeau%26entry.1292438233%3D%2520%2520We%2520propose%2520Any2graph%252C%2520a%2520generic%2520framework%2520for%2520end-to-end%2520Supervised%2520Graph%250APrediction%2520%2528SGP%2529%2520i.e.%2520a%2520deep%2520learning%2520model%2520that%2520predicts%2520an%2520entire%2520graph%2520for%250Aany%2520kind%2520of%2520input.%2520The%2520framework%2520is%2520built%2520on%2520a%2520novel%2520Optimal%2520Transport%2520loss%252C%250Athe%2520Partially-Masked%2520Fused%2520Gromov-Wasserstein%252C%2520that%2520exhibits%2520all%2520necessary%250Aproperties%2520%2528permutation%2520invariance%252C%2520differentiability%2520and%2520scalability%2529%2520and%2520is%250Adesigned%2520to%2520handle%2520any-sized%2520graphs.%2520Numerical%2520experiments%2520showcase%2520the%250Aversatility%2520of%2520the%2520approach%2520that%2520outperform%2520existing%2520competitors%2520on%2520a%2520novel%250Achallenging%2520synthetic%2520dataset%2520and%2520a%2520variety%2520of%2520real-world%2520tasks%2520such%2520as%2520map%250Aconstruction%2520from%2520satellite%2520image%2520%2528Sat2Graph%2529%2520or%2520molecule%2520prediction%2520from%250Afingerprint%2520%2528Fingerprint2Graph%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12269v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Any2Graph%3A%20Deep%20End-To-End%20Supervised%20Graph%20Prediction%20With%20An%20Optimal%0A%20%20Transport%20Loss&entry.906535625=Paul%20Krzakala%20and%20Junjie%20Yang%20and%20R%C3%A9mi%20Flamary%20and%20Florence%20d%27Alch%C3%A9-Buc%20and%20Charlotte%20Laclau%20and%20Matthieu%20Labeau&entry.1292438233=%20%20We%20propose%20Any2graph%2C%20a%20generic%20framework%20for%20end-to-end%20Supervised%20Graph%0APrediction%20%28SGP%29%20i.e.%20a%20deep%20learning%20model%20that%20predicts%20an%20entire%20graph%20for%0Aany%20kind%20of%20input.%20The%20framework%20is%20built%20on%20a%20novel%20Optimal%20Transport%20loss%2C%0Athe%20Partially-Masked%20Fused%20Gromov-Wasserstein%2C%20that%20exhibits%20all%20necessary%0Aproperties%20%28permutation%20invariance%2C%20differentiability%20and%20scalability%29%20and%20is%0Adesigned%20to%20handle%20any-sized%20graphs.%20Numerical%20experiments%20showcase%20the%0Aversatility%20of%20the%20approach%20that%20outperform%20existing%20competitors%20on%20a%20novel%0Achallenging%20synthetic%20dataset%20and%20a%20variety%20of%20real-world%20tasks%20such%20as%20map%0Aconstruction%20from%20satellite%20image%20%28Sat2Graph%29%20or%20molecule%20prediction%20from%0Afingerprint%20%28Fingerprint2Graph%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12269v3&entry.124074799=Read"},
{"title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context", "author": " Gemini Team and Petko Georgiev and Ving Ian Lei and Ryan Burnell and Libin Bai and Anmol Gulati and Garrett Tanzer and Damien Vincent and Zhufeng Pan and Shibo Wang and Soroosh Mariooryad and Yifan Ding and Xinyang Geng and Fred Alcober and Roy Frostig and Mark Omernick and Lexi Walker and Cosmin Paduraru and Christina Sorokin and Andrea Tacchetti and Colin Gaffney and Samira Daruki and Olcan Sercinoglu and Zach Gleicher and Juliette Love and Paul Voigtlaender and Rohan Jain and Gabriela Surita and Kareem Mohamed and Rory Blevins and Junwhan Ahn and Tao Zhu and Kornraphop Kawintiranon and Orhan Firat and Yiming Gu and Yujing Zhang and Matthew Rahtz and Manaal Faruqui and Natalie Clay and Justin Gilmer and JD Co-Reyes and Ivo Penchev and Rui Zhu and Nobuyuki Morioka and Kevin Hui and Krishna Haridasan and Victor Campos and Mahdis Mahdieh and Mandy Guo and Samer Hassan and Kevin Kilgour and Arpi Vezer and Heng-Tze Cheng and Raoul de Liedekerke and Siddharth Goyal and Paul Barham and DJ Strouse and Seb Noury and Jonas Adler and Mukund Sundararajan and Sharad Vikram and Dmitry Lepikhin and Michela Paganini and Xavier Garcia and Fan Yang and Dasha Valter and Maja Trebacz and Kiran Vodrahalli and Chulayuth Asawaroengchai and Roman Ring and Norbert Kalb and Livio Baldini Soares and Siddhartha Brahma and David Steiner and Tianhe Yu and Fabian Mentzer and Antoine He and Lucas Gonzalez and Bibo Xu and Raphael Lopez Kaufman and Laurent El Shafey and Junhyuk Oh and Tom Hennigan and George van den Driessche and Seth Odoom and Mario Lucic and Becca Roelofs and Sid Lall and Amit Marathe and Betty Chan and Santiago Ontanon and Luheng He and Denis Teplyashin and Jonathan Lai and Phil Crone and Bogdan Damoc and Lewis Ho and Sebastian Riedel and Karel Lenc and Chih-Kuan Yeh and Aakanksha Chowdhery and Yang Xu and Mehran Kazemi and Ehsan Amid and Anastasia Petrushkina and Kevin Swersky and Ali Khodaei and Gowoon Chen and Chris Larkin and Mario Pinto and Geng Yan and Adria Puigdomenech Badia and Piyush Patil and Steven Hansen and Dave Orr and Sebastien M. R. Arnold and Jordan Grimstad and Andrew Dai and Sholto Douglas and Rishika Sinha and Vikas Yadav and Xi Chen and Elena Gribovskaya and Jacob Austin and Jeffrey Zhao and Kaushal Patel and Paul Komarek and Sophia Austin and Sebastian Borgeaud and Linda Friso and Abhimanyu Goyal and Ben Caine and Kris Cao and Da-Woon Chung and Matthew Lamm and Gabe Barth-Maron and Thais Kagohara and Kate Olszewska and Mia Chen and Kaushik Shivakumar and Rishabh Agarwal and Harshal Godhia and Ravi Rajwar and Javier Snaider and Xerxes Dotiwalla and Yuan Liu and Aditya Barua and Victor Ungureanu and Yuan Zhang and Bat-Orgil Batsaikhan and Mateo Wirth and James Qin and Ivo Danihelka and Tulsee Doshi and Martin Chadwick and Jilin Chen and Sanil Jain and Quoc Le and Arjun Kar and Madhu Gurumurthy and Cheng Li and Ruoxin Sang and Fangyu Liu and Lampros Lamprou and Rich Munoz and Nathan Lintz and Harsh Mehta and Heidi Howard and Malcolm Reynolds and Lora Aroyo and Quan Wang and Lorenzo Blanco and Albin Cassirer and Jordan Griffith and Dipanjan Das and Stephan Lee and Jakub Sygnowski and Zach Fisher and James Besley and Richard Powell and Zafarali Ahmed and Dominik Paulus and David Reitter and Zalan Borsos and Rishabh Joshi and Aedan Pope and Steven Hand and Vittorio Selo and Vihan Jain and Nikhil Sethi and Megha Goel and Takaki Makino and Rhys May and Zhen Yang and Johan Schalkwyk and Christina Butterfield and Anja Hauth and Alex Goldin and Will Hawkins and Evan Senter and Sergey Brin and Oliver Woodman and Marvin Ritter and Eric Noland and Minh Giang and Vijay Bolina and Lisa Lee and Tim Blyth and Ian Mackinnon and Machel Reid and Obaid Sarvana and David Silver and Alexander Chen and Lily Wang and Loren Maggiore and Oscar Chang and Nithya Attaluri and Gregory Thornton and Chung-Cheng Chiu and Oskar Bunyan and Nir Levine and Timothy Chung and Evgenii Eltyshev and Xiance Si and Timothy Lillicrap and Demetra Brady and Vaibhav Aggarwal and Boxi Wu and Yuanzhong Xu and Ross McIlroy and Kartikeya Badola and Paramjit Sandhu and Erica Moreira and Wojciech Stokowiec and Ross Hemsley and Dong Li and Alex Tudor and Pranav Shyam and Elahe Rahimtoroghi and Salem Haykal and Pablo Sprechmann and Xiang Zhou and Diana Mincu and Yujia Li and Ravi Addanki and Kalpesh Krishna and Xiao Wu and Alexandre Frechette and Matan Eyal and Allan Dafoe and Dave Lacey and Jay Whang and Thi Avrahami and Ye Zhang and Emanuel Taropa and Hanzhao Lin and Daniel Toyama and Eliza Rutherford and Motoki Sano and HyunJeong Choe and Alex Tomala and Chalence Safranek-Shrader and Nora Kassner and Mantas Pajarskas and Matt Harvey and Sean Sechrist and Meire Fortunato and Christina Lyu and Gamaleldin Elsayed and Chenkai Kuang and James Lottes and Eric Chu and Chao Jia and Chih-Wei Chen and Peter Humphreys and Kate Baumli and Connie Tao and Rajkumar Samuel and Cicero Nogueira dos Santos and Anders Andreassen and Nemanja Raki\u0107evi\u0107 and Dominik Grewe and Aviral Kumar and Stephanie Winkler and Jonathan Caton and Andrew Brock and Sid Dalmia and Hannah Sheahan and Iain Barr and Yingjie Miao and Paul Natsev and Jacob Devlin and Feryal Behbahani and Flavien Prost and Yanhua Sun and Artiom Myaskovsky and Thanumalayan Sankaranarayana Pillai and Dan Hurt and Angeliki Lazaridou and Xi Xiong and Ce Zheng and Fabio Pardo and Xiaowei Li and Dan Horgan and Joe Stanton and Moran Ambar and Fei Xia and Alejandro Lince and Mingqiu Wang and Basil Mustafa and Albert Webson and Hyo Lee and Rohan Anil and Martin Wicke and Timothy Dozat and Abhishek Sinha and Enrique Piqueras and Elahe Dabir and Shyam Upadhyay and Anudhyan Boral and Lisa Anne Hendricks and Corey Fry and Josip Djolonga and Yi Su and Jake Walker and Jane Labanowski and Ronny Huang and Vedant Misra and Jeremy Chen and RJ Skerry-Ryan and Avi Singh and Shruti Rijhwani and Dian Yu and Alex Castro-Ros and Beer Changpinyo and Romina Datta and Sumit Bagri and Arnar Mar Hrafnkelsson and Marcello Maggioni and Daniel Zheng and Yury Sulsky and Shaobo Hou and Tom Le Paine and Antoine Yang and Jason Riesa and Dominika Rogozinska and Dror Marcus and Dalia El Badawy and Qiao Zhang and Luyu Wang and Helen Miller and Jeremy Greer and Lars Lowe Sjos and Azade Nova and Heiga Zen and Rahma Chaabouni and Mihaela Rosca and Jiepu Jiang and Charlie Chen and Ruibo Liu and Tara Sainath and Maxim Krikun and Alex Polozov and Jean-Baptiste Lespiau and Josh Newlan and Zeyncep Cankara and Soo Kwak and Yunhan Xu and Phil Chen and Andy Coenen and Clemens Meyer and Katerina Tsihlas and Ada Ma and Juraj Gottweis and Jinwei Xing and Chenjie Gu and Jin Miao and Christian Frank and Zeynep Cankara and Sanjay Ganapathy and Ishita Dasgupta and Steph Hughes-Fitt and Heng Chen and David Reid and Keran Rong and Hongmin Fan and Joost van Amersfoort and Vincent Zhuang and Aaron Cohen and Shixiang Shane Gu and Anhad Mohananey and Anastasija Ilic and Taylor Tobin and John Wieting and Anna Bortsova and Phoebe Thacker and Emma Wang and Emily Caveness and Justin Chiu and Eren Sezener and Alex Kaskasoli and Steven Baker and Katie Millican and Mohamed Elhawaty and Kostas Aisopos and Carl Lebsack and Nathan Byrd and Hanjun Dai and Wenhao Jia and Matthew Wiethoff and Elnaz Davoodi and Albert Weston and Lakshman Yagati and Arun Ahuja and Isabel Gao and Golan Pundak and Susan Zhang and Michael Azzam and Khe Chai Sim and Sergi Caelles and James Keeling and Abhanshu Sharma and Andy Swing and YaGuang Li and Chenxi Liu and Carrie Grimes Bostock and Yamini Bansal and Zachary Nado and Ankesh Anand and Josh Lipschultz and Abhijit Karmarkar and Lev Proleev and Abe Ittycheriah and Soheil Hassas Yeganeh and George Polovets and Aleksandra Faust and Jiao Sun and Alban Rrustemi and Pen Li and Rakesh Shivanna and Jeremiah Liu and Chris Welty and Federico Lebron and Anirudh Baddepudi and Sebastian Krause and Emilio Parisotto and Radu Soricut and Zheng Xu and Dawn Bloxwich and Melvin Johnson and Behnam Neyshabur and Justin Mao-Jones and Renshen Wang and Vinay Ramasesh and Zaheer Abbas and Arthur Guez and Constant Segal and Duc Dung Nguyen and James Svensson and Le Hou and Sarah York and Kieran Milan and Sophie Bridgers and Wiktor Gworek and Marco Tagliasacchi and James Lee-Thorp and Michael Chang and Alexey Guseynov and Ale Jakse Hartman and Michael Kwong and Ruizhe Zhao and Sheleem Kashem and Elizabeth Cole and Antoine Miech and Richard Tanburn and Mary Phuong and Filip Pavetic and Sebastien Cevey and Ramona Comanescu and Richard Ives and Sherry Yang and Cosmo Du and Bo Li and Zizhao Zhang and Mariko Iinuma and Clara Huiyi Hu and Aurko Roy and Shaan Bijwadia and Zhenkai Zhu and Danilo Martins and Rachel Saputro and Anita Gergely and Steven Zheng and Dawei Jia and Ioannis Antonoglou and Adam Sadovsky and Shane Gu and Yingying Bi and Alek Andreev and Sina Samangooei and Mina Khan and Tomas Kocisky and Angelos Filos and Chintu Kumar and Colton Bishop and Adams Yu and Sarah Hodkinson and Sid Mittal and Premal Shah and Alexandre Moufarek and Yong Cheng and Adam Bloniarz and Jaehoon Lee and Pedram Pejman and Paul Michel and Stephen Spencer and Vladimir Feinberg and Xuehan Xiong and Nikolay Savinov and Charlotte Smith and Siamak Shakeri and Dustin Tran and Mary Chesus and Bernd Bohnet and George Tucker and Tamara von Glehn and Carrie Muir and Yiran Mao and Hideto Kazawa and Ambrose Slone and Kedar Soparkar and Disha Shrivastava and James Cobon-Kerr and Michael Sharman and Jay Pavagadhi and Carlos Araya and Karolis Misiunas and Nimesh Ghelani and Michael Laskin and David Barker and Qiujia Li and Anton Briukhov and Neil Houlsby and Mia Glaese and Balaji Lakshminarayanan and Nathan Schucher and Yunhao Tang and Eli Collins and Hyeontaek Lim and Fangxiaoyu Feng and Adria Recasens and Guangda Lai and Alberto Magni and Nicola De Cao and Aditya Siddhant and Zoe Ashwood and Jordi Orbay and Mostafa Dehghani and Jenny Brennan and Yifan He and Kelvin Xu and Yang Gao and Carl Saroufim and James Molloy and Xinyi Wu and Seb Arnold and Solomon Chang and Julian Schrittwieser and Elena Buchatskaya and Soroush Radpour and Martin Polacek and Skye Giordano and Ankur Bapna and Simon Tokumine and Vincent Hellendoorn and Thibault Sottiaux and Sarah Cogan and Aliaksei Severyn and Mohammad Saleh and Shantanu Thakoor and Laurent Shefey and Siyuan Qiao and Meenu Gaba and Shuo-yiin Chang and Craig Swanson and Biao Zhang and Benjamin Lee and Paul Kishan Rubenstein and Gan Song and Tom Kwiatkowski and Anna Koop and Ajay Kannan and David Kao and Parker Schuh and Axel Stjerngren and Golnaz Ghiasi and Gena Gibson and Luke Vilnis and Ye Yuan and Felipe Tiengo Ferreira and Aishwarya Kamath and Ted Klimenko and Ken Franko and Kefan Xiao and Indro Bhattacharya and Miteyan Patel and Rui Wang and Alex Morris and Robin Strudel and Vivek Sharma and Peter Choy and Sayed Hadi Hashemi and Jessica Landon and Mara Finkelstein and Priya Jhakra and Justin Frye and Megan Barnes and Matthew Mauger and Dennis Daun and Khuslen Baatarsukh and Matthew Tung and Wael Farhan and Henryk Michalewski and Fabio Viola and Felix de Chaumont Quitry and Charline Le Lan and Tom Hudson and Qingze Wang and Felix Fischer and Ivy Zheng and Elspeth White and Anca Dragan and Jean-baptiste Alayrac and Eric Ni and Alexander Pritzel and Adam Iwanicki and Michael Isard and Anna Bulanova and Lukas Zilka and Ethan Dyer and Devendra Sachan and Srivatsan Srinivasan and Hannah Muckenhirn and Honglong Cai and Amol Mandhane and Mukarram Tariq and Jack W. Rae and Gary Wang and Kareem Ayoub and Nicholas FitzGerald and Yao Zhao and Woohyun Han and Chris Alberti and Dan Garrette and Kashyap Krishnakumar and Mai Gimenez and Anselm Levskaya and Daniel Sohn and Josip Matak and Inaki Iturrate and Michael B. Chang and Jackie Xiang and Yuan Cao and Nishant Ranka and Geoff Brown and Adrian Hutter and Vahab Mirrokni and Nanxin Chen and Kaisheng Yao and Zoltan Egyed and Francois Galilee and Tyler Liechty and Praveen Kallakuri and Evan Palmer and Sanjay Ghemawat and Jasmine Liu and David Tao and Chloe Thornton and Tim Green and Mimi Jasarevic and Sharon Lin and Victor Cotruta and Yi-Xuan Tan and Noah Fiedel and Hongkun Yu and Ed Chi and Alexander Neitz and Jens Heitkaemper and Anu Sinha and Denny Zhou and Yi Sun and Charbel Kaed and Brice Hulse and Swaroop Mishra and Maria Georgaki and Sneha Kudugunta and Clement Farabet and Izhak Shafran and Daniel Vlasic and Anton Tsitsulin and Rajagopal Ananthanarayanan and Alen Carin and Guolong Su and Pei Sun and Shashank V and Gabriel Carvajal and Josef Broder and Iulia Comsa and Alena Repina and William Wong and Warren Weilun Chen and Peter Hawkins and Egor Filonov and Lucia Loher and Christoph Hirnschall and Weiyi Wang and Jingchen Ye and Andrea Burns and Hardie Cate and Diana Gage Wright and Federico Piccinini and Lei Zhang and Chu-Cheng Lin and Ionel Gog and Yana Kulizhskaya and Ashwin Sreevatsa and Shuang Song and Luis C. Cobo and Anand Iyer and Chetan Tekur and Guillermo Garrido and Zhuyun Xiao and Rupert Kemp and Huaixiu Steven Zheng and Hui Li and Ananth Agarwal and Christel Ngani and Kati Goshvadi and Rebeca Santamaria-Fernandez and Wojciech Fica and Xinyun Chen and Chris Gorgolewski and Sean Sun and Roopal Garg and Xinyu Ye and S. M. Ali Eslami and Nan Hua and Jon Simon and Pratik Joshi and Yelin Kim and Ian Tenney and Sahitya Potluri and Lam Nguyen Thiet and Quan Yuan and Florian Luisier and Alexandra Chronopoulou and Salvatore Scellato and Praveen Srinivasan and Minmin Chen and Vinod Koverkathu and Valentin Dalibard and Yaming Xu and Brennan Saeta and Keith Anderson and Thibault Sellam and Nick Fernando and Fantine Huot and Junehyuk Jung and Mani Varadarajan and Michael Quinn and Amit Raul and Maigo Le and Ruslan Habalov and Jon Clark and Komal Jalan and Kalesha Bullard and Achintya Singhal and Thang Luong and Boyu Wang and Sujeevan Rajayogam and Julian Eisenschlos and Johnson Jia and Daniel Finchelstein and Alex Yakubovich and Daniel Balle and Michael Fink and Sameer Agarwal and Jing Li and Dj Dvijotham and Shalini Pal and Kai Kang and Jaclyn Konzelmann and Jennifer Beattie and Olivier Dousse and Diane Wu and Remi Crocker and Chen Elkind and Siddhartha Reddy Jonnalagadda and Jong Lee and Dan Holtmann-Rice and Krystal Kallarackal and Rosanne Liu and Denis Vnukov and Neera Vats and Luca Invernizzi and Mohsen Jafari and Huanjie Zhou and Lilly Taylor and Jennifer Prendki and Marcus Wu and Tom Eccles and Tianqi Liu and Kavya Kopparapu and Francoise Beaufays and Christof Angermueller and Andreea Marzoca and Shourya Sarcar and Hilal Dib and Jeff Stanway and Frank Perbet and Nejc Trdin and Rachel Sterneck and Andrey Khorlin and Dinghua Li and Xihui Wu and Sonam Goenka and David Madras and Sasha Goldshtein and Willi Gierke and Tong Zhou and Yaxin Liu and Yannie Liang and Anais White and Yunjie Li and Shreya Singh and Sanaz Bahargam and Mark Epstein and Sujoy Basu and Li Lao and Adnan Ozturel and Carl Crous and Alex Zhai and Han Lu and Zora Tung and Neeraj Gaur and Alanna Walton and Lucas Dixon and Ming Zhang and Amir Globerson and Grant Uy and Andrew Bolt and Olivia Wiles and Milad Nasr and Ilia Shumailov and Marco Selvi and Francesco Piccinno and Ricardo Aguilar and Sara McCarthy and Misha Khalman and Mrinal Shukla and Vlado Galic and John Carpenter and Kevin Villela and Haibin Zhang and Harry Richardson and James Martens and Matko Bosnjak and Shreyas Rammohan Belle and Jeff Seibert and Mahmoud Alnahlawi and Brian McWilliams and Sankalp Singh and Annie Louis and Wen Ding and Dan Popovici and Lenin Simicich and Laura Knight and Pulkit Mehta and Nishesh Gupta and Chongyang Shi and Saaber Fatehi and Jovana Mitrovic and Alex Grills and Joseph Pagadora and Dessie Petrova and Danielle Eisenbud and Zhishuai Zhang and Damion Yates and Bhavishya Mittal and Nilesh Tripuraneni and Yannis Assael and Thomas Brovelli and Prateek Jain and Mihajlo Velimirovic and Canfer Akbulut and Jiaqi Mu and Wolfgang Macherey and Ravin Kumar and Jun Xu and Haroon Qureshi and Gheorghe Comanici and Jeremy Wiesner and Zhitao Gong and Anton Ruddock and Matthias Bauer and Nick Felt and Anirudh GP and Anurag Arnab and Dustin Zelle and Jonas Rothfuss and Bill Rosgen and Ashish Shenoy and Bryan Seybold and Xinjian Li and Jayaram Mudigonda and Goker Erdogan and Jiawei Xia and Jiri Simsa and Andrea Michi and Yi Yao and Christopher Yew and Steven Kan and Isaac Caswell and Carey Radebaugh and Andre Elisseeff and Pedro Valenzuela and Kay McKinney and Kim Paterson and Albert Cui and Eri Latorre-Chimoto and Solomon Kim and William Zeng and Ken Durden and Priya Ponnapalli and Tiberiu Sosea and Christopher A. Choquette-Choo and James Manyika and Brona Robenek and Harsha Vashisht and Sebastien Pereira and Hoi Lam and Marko Velic and Denese Owusu-Afriyie and Katherine Lee and Tolga Bolukbasi and Alicia Parrish and Shawn Lu and Jane Park and Balaji Venkatraman and Alice Talbert and Lambert Rosique and Yuchung Cheng and Andrei Sozanschi and Adam Paszke and Praveen Kumar and Jessica Austin and Lu Li and Khalid Salama and Wooyeol Kim and Nandita Dukkipati and Anthony Baryshnikov and Christos Kaplanis and XiangHai Sheng and Yuri Chervonyi and Caglar Unlu and Diego de Las Casas and Harry Askham and Kathryn Tunyasuvunakool and Felix Gimeno and Siim Poder and Chester Kwak and Matt Miecnikowski and Vahab Mirrokni and Alek Dimitriev and Aaron Parisi and Dangyi Liu and Tomy Tsai and Toby Shevlane and Christina Kouridi and Drew Garmon and Adrian Goedeckemeyer and Adam R. Brown and Anitha Vijayakumar and Ali Elqursh and Sadegh Jazayeri and Jin Huang and Sara Mc Carthy and Jay Hoover and Lucy Kim and Sandeep Kumar and Wei Chen and Courtney Biles and Garrett Bingham and Evan Rosen and Lisa Wang and Qijun Tan and David Engel and Francesco Pongetti and Dario de Cesare and Dongseong Hwang and Lily Yu and Jennifer Pullman and Srini Narayanan and Kyle Levin and Siddharth Gopal and Megan Li and Asaf Aharoni and Trieu Trinh and Jessica Lo and Norman Casagrande and Roopali Vij and Loic Matthey and Bramandia Ramadhana and Austin Matthews and CJ Carey and Matthew Johnson and Kremena Goranova and Rohin Shah and Shereen Ashraf and Kingshuk Dasgupta and Rasmus Larsen and Yicheng Wang and Manish Reddy Vuyyuru and Chong Jiang and Joana Ijazi and Kazuki Osawa and Celine Smith and Ramya Sree Boppana and Taylan Bilal and Yuma Koizumi and Ying Xu and Yasemin Altun and Nir Shabat and Ben Bariach and Alex Korchemniy and Kiam Choo and Olaf Ronneberger and Chimezie Iwuanyanwu and Shubin Zhao and David Soergel and Cho-Jui Hsieh and Irene Cai and Shariq Iqbal and Martin Sundermeyer and Zhe Chen and Elie Bursztein and Chaitanya Malaviya and Fadi Biadsy and Prakash Shroff and Inderjit Dhillon and Tejasi Latkar and Chris Dyer and Hannah Forbes and Massimo Nicosia and Vitaly Nikolaev and Somer Greene and Marin Georgiev and Pidong Wang and Nina Martin and Hanie Sedghi and John Zhang and Praseem Banzal and Doug Fritz and Vikram Rao and Xuezhi Wang and Jiageng Zhang and Viorica Patraucean and Dayou Du and Igor Mordatch and Ivan Jurin and Lewis Liu and Ayush Dubey and Abhi Mohan and Janek Nowakowski and Vlad-Doru Ion and Nan Wei and Reiko Tojo and Maria Abi Raad and Drew A. Hudson and Vaishakh Keshava and Shubham Agrawal and Kevin Ramirez and Zhichun Wu and Hoang Nguyen and Ji Liu and Madhavi Sewak and Bryce Petrini and DongHyun Choi and Ivan Philips and Ziyue Wang and Ioana Bica and Ankush Garg and Jarek Wilkiewicz and Priyanka Agrawal and Xiaowei Li and Danhao Guo and Emily Xue and Naseer Shaik and Andrew Leach and Sadh MNM Khan and Julia Wiesinger and Sammy Jerome and Abhishek Chakladar and Alek Wenjiao Wang and Tina Ornduff and Folake Abu and Alireza Ghaffarkhah and Marcus Wainwright and Mario Cortes and Frederick Liu and Joshua Maynez and Slav Petrov and Yonghui Wu and Demis Hassabis and Koray Kavukcuoglu and Jeffrey Dean and Oriol Vinyals", "abstract": "  In this report, we introduce the Gemini 1.5 family of models, representing\nthe next generation of highly compute-efficient multimodal models capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio. The\nfamily includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds\nthe February version on the great majority of capabilities and benchmarks; (2)\nGemini 1.5 Flash, a more lightweight variant designed for efficiency with\nminimal regression in quality. Gemini 1.5 models achieve near-perfect recall on\nlong-context retrieval tasks across modalities, improve the state-of-the-art in\nlong-document QA, long-video QA and long-context ASR, and match or surpass\nGemini 1.0 Ultra's state-of-the-art performance across a broad set of\nbenchmarks. Studying the limits of Gemini 1.5's long-context ability, we find\ncontinued improvement in next-token prediction and near-perfect retrieval\n(>99%) up to at least 10M tokens, a generational leap over existing models such\nas Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world\nuse cases, such as Gemini 1.5 collaborating with professionals on completing\ntheir tasks achieving 26 to 75% time savings across 10 different job\ncategories, as well as surprising new capabilities of large language models at\nthe frontier; when given a grammar manual for Kalamang, a language with fewer\nthan 200 speakers worldwide, the model learns to translate English to Kalamang\nat a similar level to a person who learned from the same content.\n", "link": "http://arxiv.org/abs/2403.05530v3", "date": "2024-06-14", "relevancy": 2.3548, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4853}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4794}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gemini%201.5%3A%20Unlocking%20multimodal%20understanding%20across%20millions%20of%20tokens%0A%20%20of%20context&body=Title%3A%20Gemini%201.5%3A%20Unlocking%20multimodal%20understanding%20across%20millions%20of%20tokens%0A%20%20of%20context%0AAuthor%3A%20%20Gemini%20Team%20and%20Petko%20Georgiev%20and%20Ving%20Ian%20Lei%20and%20Ryan%20Burnell%20and%20Libin%20Bai%20and%20Anmol%20Gulati%20and%20Garrett%20Tanzer%20and%20Damien%20Vincent%20and%20Zhufeng%20Pan%20and%20Shibo%20Wang%20and%20Soroosh%20Mariooryad%20and%20Yifan%20Ding%20and%20Xinyang%20Geng%20and%20Fred%20Alcober%20and%20Roy%20Frostig%20and%20Mark%20Omernick%20and%20Lexi%20Walker%20and%20Cosmin%20Paduraru%20and%20Christina%20Sorokin%20and%20Andrea%20Tacchetti%20and%20Colin%20Gaffney%20and%20Samira%20Daruki%20and%20Olcan%20Sercinoglu%20and%20Zach%20Gleicher%20and%20Juliette%20Love%20and%20Paul%20Voigtlaender%20and%20Rohan%20Jain%20and%20Gabriela%20Surita%20and%20Kareem%20Mohamed%20and%20Rory%20Blevins%20and%20Junwhan%20Ahn%20and%20Tao%20Zhu%20and%20Kornraphop%20Kawintiranon%20and%20Orhan%20Firat%20and%20Yiming%20Gu%20and%20Yujing%20Zhang%20and%20Matthew%20Rahtz%20and%20Manaal%20Faruqui%20and%20Natalie%20Clay%20and%20Justin%20Gilmer%20and%20JD%20Co-Reyes%20and%20Ivo%20Penchev%20and%20Rui%20Zhu%20and%20Nobuyuki%20Morioka%20and%20Kevin%20Hui%20and%20Krishna%20Haridasan%20and%20Victor%20Campos%20and%20Mahdis%20Mahdieh%20and%20Mandy%20Guo%20and%20Samer%20Hassan%20and%20Kevin%20Kilgour%20and%20Arpi%20Vezer%20and%20Heng-Tze%20Cheng%20and%20Raoul%20de%20Liedekerke%20and%20Siddharth%20Goyal%20and%20Paul%20Barham%20and%20DJ%20Strouse%20and%20Seb%20Noury%20and%20Jonas%20Adler%20and%20Mukund%20Sundararajan%20and%20Sharad%20Vikram%20and%20Dmitry%20Lepikhin%20and%20Michela%20Paganini%20and%20Xavier%20Garcia%20and%20Fan%20Yang%20and%20Dasha%20Valter%20and%20Maja%20Trebacz%20and%20Kiran%20Vodrahalli%20and%20Chulayuth%20Asawaroengchai%20and%20Roman%20Ring%20and%20Norbert%20Kalb%20and%20Livio%20Baldini%20Soares%20and%20Siddhartha%20Brahma%20and%20David%20Steiner%20and%20Tianhe%20Yu%20and%20Fabian%20Mentzer%20and%20Antoine%20He%20and%20Lucas%20Gonzalez%20and%20Bibo%20Xu%20and%20Raphael%20Lopez%20Kaufman%20and%20Laurent%20El%20Shafey%20and%20Junhyuk%20Oh%20and%20Tom%20Hennigan%20and%20George%20van%20den%20Driessche%20and%20Seth%20Odoom%20and%20Mario%20Lucic%20and%20Becca%20Roelofs%20and%20Sid%20Lall%20and%20Amit%20Marathe%20and%20Betty%20Chan%20and%20Santiago%20Ontanon%20and%20Luheng%20He%20and%20Denis%20Teplyashin%20and%20Jonathan%20Lai%20and%20Phil%20Crone%20and%20Bogdan%20Damoc%20and%20Lewis%20Ho%20and%20Sebastian%20Riedel%20and%20Karel%20Lenc%20and%20Chih-Kuan%20Yeh%20and%20Aakanksha%20Chowdhery%20and%20Yang%20Xu%20and%20Mehran%20Kazemi%20and%20Ehsan%20Amid%20and%20Anastasia%20Petrushkina%20and%20Kevin%20Swersky%20and%20Ali%20Khodaei%20and%20Gowoon%20Chen%20and%20Chris%20Larkin%20and%20Mario%20Pinto%20and%20Geng%20Yan%20and%20Adria%20Puigdomenech%20Badia%20and%20Piyush%20Patil%20and%20Steven%20Hansen%20and%20Dave%20Orr%20and%20Sebastien%20M.%20R.%20Arnold%20and%20Jordan%20Grimstad%20and%20Andrew%20Dai%20and%20Sholto%20Douglas%20and%20Rishika%20Sinha%20and%20Vikas%20Yadav%20and%20Xi%20Chen%20and%20Elena%20Gribovskaya%20and%20Jacob%20Austin%20and%20Jeffrey%20Zhao%20and%20Kaushal%20Patel%20and%20Paul%20Komarek%20and%20Sophia%20Austin%20and%20Sebastian%20Borgeaud%20and%20Linda%20Friso%20and%20Abhimanyu%20Goyal%20and%20Ben%20Caine%20and%20Kris%20Cao%20and%20Da-Woon%20Chung%20and%20Matthew%20Lamm%20and%20Gabe%20Barth-Maron%20and%20Thais%20Kagohara%20and%20Kate%20Olszewska%20and%20Mia%20Chen%20and%20Kaushik%20Shivakumar%20and%20Rishabh%20Agarwal%20and%20Harshal%20Godhia%20and%20Ravi%20Rajwar%20and%20Javier%20Snaider%20and%20Xerxes%20Dotiwalla%20and%20Yuan%20Liu%20and%20Aditya%20Barua%20and%20Victor%20Ungureanu%20and%20Yuan%20Zhang%20and%20Bat-Orgil%20Batsaikhan%20and%20Mateo%20Wirth%20and%20James%20Qin%20and%20Ivo%20Danihelka%20and%20Tulsee%20Doshi%20and%20Martin%20Chadwick%20and%20Jilin%20Chen%20and%20Sanil%20Jain%20and%20Quoc%20Le%20and%20Arjun%20Kar%20and%20Madhu%20Gurumurthy%20and%20Cheng%20Li%20and%20Ruoxin%20Sang%20and%20Fangyu%20Liu%20and%20Lampros%20Lamprou%20and%20Rich%20Munoz%20and%20Nathan%20Lintz%20and%20Harsh%20Mehta%20and%20Heidi%20Howard%20and%20Malcolm%20Reynolds%20and%20Lora%20Aroyo%20and%20Quan%20Wang%20and%20Lorenzo%20Blanco%20and%20Albin%20Cassirer%20and%20Jordan%20Griffith%20and%20Dipanjan%20Das%20and%20Stephan%20Lee%20and%20Jakub%20Sygnowski%20and%20Zach%20Fisher%20and%20James%20Besley%20and%20Richard%20Powell%20and%20Zafarali%20Ahmed%20and%20Dominik%20Paulus%20and%20David%20Reitter%20and%20Zalan%20Borsos%20and%20Rishabh%20Joshi%20and%20Aedan%20Pope%20and%20Steven%20Hand%20and%20Vittorio%20Selo%20and%20Vihan%20Jain%20and%20Nikhil%20Sethi%20and%20Megha%20Goel%20and%20Takaki%20Makino%20and%20Rhys%20May%20and%20Zhen%20Yang%20and%20Johan%20Schalkwyk%20and%20Christina%20Butterfield%20and%20Anja%20Hauth%20and%20Alex%20Goldin%20and%20Will%20Hawkins%20and%20Evan%20Senter%20and%20Sergey%20Brin%20and%20Oliver%20Woodman%20and%20Marvin%20Ritter%20and%20Eric%20Noland%20and%20Minh%20Giang%20and%20Vijay%20Bolina%20and%20Lisa%20Lee%20and%20Tim%20Blyth%20and%20Ian%20Mackinnon%20and%20Machel%20Reid%20and%20Obaid%20Sarvana%20and%20David%20Silver%20and%20Alexander%20Chen%20and%20Lily%20Wang%20and%20Loren%20Maggiore%20and%20Oscar%20Chang%20and%20Nithya%20Attaluri%20and%20Gregory%20Thornton%20and%20Chung-Cheng%20Chiu%20and%20Oskar%20Bunyan%20and%20Nir%20Levine%20and%20Timothy%20Chung%20and%20Evgenii%20Eltyshev%20and%20Xiance%20Si%20and%20Timothy%20Lillicrap%20and%20Demetra%20Brady%20and%20Vaibhav%20Aggarwal%20and%20Boxi%20Wu%20and%20Yuanzhong%20Xu%20and%20Ross%20McIlroy%20and%20Kartikeya%20Badola%20and%20Paramjit%20Sandhu%20and%20Erica%20Moreira%20and%20Wojciech%20Stokowiec%20and%20Ross%20Hemsley%20and%20Dong%20Li%20and%20Alex%20Tudor%20and%20Pranav%20Shyam%20and%20Elahe%20Rahimtoroghi%20and%20Salem%20Haykal%20and%20Pablo%20Sprechmann%20and%20Xiang%20Zhou%20and%20Diana%20Mincu%20and%20Yujia%20Li%20and%20Ravi%20Addanki%20and%20Kalpesh%20Krishna%20and%20Xiao%20Wu%20and%20Alexandre%20Frechette%20and%20Matan%20Eyal%20and%20Allan%20Dafoe%20and%20Dave%20Lacey%20and%20Jay%20Whang%20and%20Thi%20Avrahami%20and%20Ye%20Zhang%20and%20Emanuel%20Taropa%20and%20Hanzhao%20Lin%20and%20Daniel%20Toyama%20and%20Eliza%20Rutherford%20and%20Motoki%20Sano%20and%20HyunJeong%20Choe%20and%20Alex%20Tomala%20and%20Chalence%20Safranek-Shrader%20and%20Nora%20Kassner%20and%20Mantas%20Pajarskas%20and%20Matt%20Harvey%20and%20Sean%20Sechrist%20and%20Meire%20Fortunato%20and%20Christina%20Lyu%20and%20Gamaleldin%20Elsayed%20and%20Chenkai%20Kuang%20and%20James%20Lottes%20and%20Eric%20Chu%20and%20Chao%20Jia%20and%20Chih-Wei%20Chen%20and%20Peter%20Humphreys%20and%20Kate%20Baumli%20and%20Connie%20Tao%20and%20Rajkumar%20Samuel%20and%20Cicero%20Nogueira%20dos%20Santos%20and%20Anders%20Andreassen%20and%20Nemanja%20Raki%C4%87evi%C4%87%20and%20Dominik%20Grewe%20and%20Aviral%20Kumar%20and%20Stephanie%20Winkler%20and%20Jonathan%20Caton%20and%20Andrew%20Brock%20and%20Sid%20Dalmia%20and%20Hannah%20Sheahan%20and%20Iain%20Barr%20and%20Yingjie%20Miao%20and%20Paul%20Natsev%20and%20Jacob%20Devlin%20and%20Feryal%20Behbahani%20and%20Flavien%20Prost%20and%20Yanhua%20Sun%20and%20Artiom%20Myaskovsky%20and%20Thanumalayan%20Sankaranarayana%20Pillai%20and%20Dan%20Hurt%20and%20Angeliki%20Lazaridou%20and%20Xi%20Xiong%20and%20Ce%20Zheng%20and%20Fabio%20Pardo%20and%20Xiaowei%20Li%20and%20Dan%20Horgan%20and%20Joe%20Stanton%20and%20Moran%20Ambar%20and%20Fei%20Xia%20and%20Alejandro%20Lince%20and%20Mingqiu%20Wang%20and%20Basil%20Mustafa%20and%20Albert%20Webson%20and%20Hyo%20Lee%20and%20Rohan%20Anil%20and%20Martin%20Wicke%20and%20Timothy%20Dozat%20and%20Abhishek%20Sinha%20and%20Enrique%20Piqueras%20and%20Elahe%20Dabir%20and%20Shyam%20Upadhyay%20and%20Anudhyan%20Boral%20and%20Lisa%20Anne%20Hendricks%20and%20Corey%20Fry%20and%20Josip%20Djolonga%20and%20Yi%20Su%20and%20Jake%20Walker%20and%20Jane%20Labanowski%20and%20Ronny%20Huang%20and%20Vedant%20Misra%20and%20Jeremy%20Chen%20and%20RJ%20Skerry-Ryan%20and%20Avi%20Singh%20and%20Shruti%20Rijhwani%20and%20Dian%20Yu%20and%20Alex%20Castro-Ros%20and%20Beer%20Changpinyo%20and%20Romina%20Datta%20and%20Sumit%20Bagri%20and%20Arnar%20Mar%20Hrafnkelsson%20and%20Marcello%20Maggioni%20and%20Daniel%20Zheng%20and%20Yury%20Sulsky%20and%20Shaobo%20Hou%20and%20Tom%20Le%20Paine%20and%20Antoine%20Yang%20and%20Jason%20Riesa%20and%20Dominika%20Rogozinska%20and%20Dror%20Marcus%20and%20Dalia%20El%20Badawy%20and%20Qiao%20Zhang%20and%20Luyu%20Wang%20and%20Helen%20Miller%20and%20Jeremy%20Greer%20and%20Lars%20Lowe%20Sjos%20and%20Azade%20Nova%20and%20Heiga%20Zen%20and%20Rahma%20Chaabouni%20and%20Mihaela%20Rosca%20and%20Jiepu%20Jiang%20and%20Charlie%20Chen%20and%20Ruibo%20Liu%20and%20Tara%20Sainath%20and%20Maxim%20Krikun%20and%20Alex%20Polozov%20and%20Jean-Baptiste%20Lespiau%20and%20Josh%20Newlan%20and%20Zeyncep%20Cankara%20and%20Soo%20Kwak%20and%20Yunhan%20Xu%20and%20Phil%20Chen%20and%20Andy%20Coenen%20and%20Clemens%20Meyer%20and%20Katerina%20Tsihlas%20and%20Ada%20Ma%20and%20Juraj%20Gottweis%20and%20Jinwei%20Xing%20and%20Chenjie%20Gu%20and%20Jin%20Miao%20and%20Christian%20Frank%20and%20Zeynep%20Cankara%20and%20Sanjay%20Ganapathy%20and%20Ishita%20Dasgupta%20and%20Steph%20Hughes-Fitt%20and%20Heng%20Chen%20and%20David%20Reid%20and%20Keran%20Rong%20and%20Hongmin%20Fan%20and%20Joost%20van%20Amersfoort%20and%20Vincent%20Zhuang%20and%20Aaron%20Cohen%20and%20Shixiang%20Shane%20Gu%20and%20Anhad%20Mohananey%20and%20Anastasija%20Ilic%20and%20Taylor%20Tobin%20and%20John%20Wieting%20and%20Anna%20Bortsova%20and%20Phoebe%20Thacker%20and%20Emma%20Wang%20and%20Emily%20Caveness%20and%20Justin%20Chiu%20and%20Eren%20Sezener%20and%20Alex%20Kaskasoli%20and%20Steven%20Baker%20and%20Katie%20Millican%20and%20Mohamed%20Elhawaty%20and%20Kostas%20Aisopos%20and%20Carl%20Lebsack%20and%20Nathan%20Byrd%20and%20Hanjun%20Dai%20and%20Wenhao%20Jia%20and%20Matthew%20Wiethoff%20and%20Elnaz%20Davoodi%20and%20Albert%20Weston%20and%20Lakshman%20Yagati%20and%20Arun%20Ahuja%20and%20Isabel%20Gao%20and%20Golan%20Pundak%20and%20Susan%20Zhang%20and%20Michael%20Azzam%20and%20Khe%20Chai%20Sim%20and%20Sergi%20Caelles%20and%20James%20Keeling%20and%20Abhanshu%20Sharma%20and%20Andy%20Swing%20and%20YaGuang%20Li%20and%20Chenxi%20Liu%20and%20Carrie%20Grimes%20Bostock%20and%20Yamini%20Bansal%20and%20Zachary%20Nado%20and%20Ankesh%20Anand%20and%20Josh%20Lipschultz%20and%20Abhijit%20Karmarkar%20and%20Lev%20Proleev%20and%20Abe%20Ittycheriah%20and%20Soheil%20Hassas%20Yeganeh%20and%20George%20Polovets%20and%20Aleksandra%20Faust%20and%20Jiao%20Sun%20and%20Alban%20Rrustemi%20and%20Pen%20Li%20and%20Rakesh%20Shivanna%20and%20Jeremiah%20Liu%20and%20Chris%20Welty%20and%20Federico%20Lebron%20and%20Anirudh%20Baddepudi%20and%20Sebastian%20Krause%20and%20Emilio%20Parisotto%20and%20Radu%20Soricut%20and%20Zheng%20Xu%20and%20Dawn%20Bloxwich%20and%20Melvin%20Johnson%20and%20Behnam%20Neyshabur%20and%20Justin%20Mao-Jones%20and%20Renshen%20Wang%20and%20Vinay%20Ramasesh%20and%20Zaheer%20Abbas%20and%20Arthur%20Guez%20and%20Constant%20Segal%20and%20Duc%20Dung%20Nguyen%20and%20James%20Svensson%20and%20Le%20Hou%20and%20Sarah%20York%20and%20Kieran%20Milan%20and%20Sophie%20Bridgers%20and%20Wiktor%20Gworek%20and%20Marco%20Tagliasacchi%20and%20James%20Lee-Thorp%20and%20Michael%20Chang%20and%20Alexey%20Guseynov%20and%20Ale%20Jakse%20Hartman%20and%20Michael%20Kwong%20and%20Ruizhe%20Zhao%20and%20Sheleem%20Kashem%20and%20Elizabeth%20Cole%20and%20Antoine%20Miech%20and%20Richard%20Tanburn%20and%20Mary%20Phuong%20and%20Filip%20Pavetic%20and%20Sebastien%20Cevey%20and%20Ramona%20Comanescu%20and%20Richard%20Ives%20and%20Sherry%20Yang%20and%20Cosmo%20Du%20and%20Bo%20Li%20and%20Zizhao%20Zhang%20and%20Mariko%20Iinuma%20and%20Clara%20Huiyi%20Hu%20and%20Aurko%20Roy%20and%20Shaan%20Bijwadia%20and%20Zhenkai%20Zhu%20and%20Danilo%20Martins%20and%20Rachel%20Saputro%20and%20Anita%20Gergely%20and%20Steven%20Zheng%20and%20Dawei%20Jia%20and%20Ioannis%20Antonoglou%20and%20Adam%20Sadovsky%20and%20Shane%20Gu%20and%20Yingying%20Bi%20and%20Alek%20Andreev%20and%20Sina%20Samangooei%20and%20Mina%20Khan%20and%20Tomas%20Kocisky%20and%20Angelos%20Filos%20and%20Chintu%20Kumar%20and%20Colton%20Bishop%20and%20Adams%20Yu%20and%20Sarah%20Hodkinson%20and%20Sid%20Mittal%20and%20Premal%20Shah%20and%20Alexandre%20Moufarek%20and%20Yong%20Cheng%20and%20Adam%20Bloniarz%20and%20Jaehoon%20Lee%20and%20Pedram%20Pejman%20and%20Paul%20Michel%20and%20Stephen%20Spencer%20and%20Vladimir%20Feinberg%20and%20Xuehan%20Xiong%20and%20Nikolay%20Savinov%20and%20Charlotte%20Smith%20and%20Siamak%20Shakeri%20and%20Dustin%20Tran%20and%20Mary%20Chesus%20and%20Bernd%20Bohnet%20and%20George%20Tucker%20and%20Tamara%20von%20Glehn%20and%20Carrie%20Muir%20and%20Yiran%20Mao%20and%20Hideto%20Kazawa%20and%20Ambrose%20Slone%20and%20Kedar%20Soparkar%20and%20Disha%20Shrivastava%20and%20James%20Cobon-Kerr%20and%20Michael%20Sharman%20and%20Jay%20Pavagadhi%20and%20Carlos%20Araya%20and%20Karolis%20Misiunas%20and%20Nimesh%20Ghelani%20and%20Michael%20Laskin%20and%20David%20Barker%20and%20Qiujia%20Li%20and%20Anton%20Briukhov%20and%20Neil%20Houlsby%20and%20Mia%20Glaese%20and%20Balaji%20Lakshminarayanan%20and%20Nathan%20Schucher%20and%20Yunhao%20Tang%20and%20Eli%20Collins%20and%20Hyeontaek%20Lim%20and%20Fangxiaoyu%20Feng%20and%20Adria%20Recasens%20and%20Guangda%20Lai%20and%20Alberto%20Magni%20and%20Nicola%20De%20Cao%20and%20Aditya%20Siddhant%20and%20Zoe%20Ashwood%20and%20Jordi%20Orbay%20and%20Mostafa%20Dehghani%20and%20Jenny%20Brennan%20and%20Yifan%20He%20and%20Kelvin%20Xu%20and%20Yang%20Gao%20and%20Carl%20Saroufim%20and%20James%20Molloy%20and%20Xinyi%20Wu%20and%20Seb%20Arnold%20and%20Solomon%20Chang%20and%20Julian%20Schrittwieser%20and%20Elena%20Buchatskaya%20and%20Soroush%20Radpour%20and%20Martin%20Polacek%20and%20Skye%20Giordano%20and%20Ankur%20Bapna%20and%20Simon%20Tokumine%20and%20Vincent%20Hellendoorn%20and%20Thibault%20Sottiaux%20and%20Sarah%20Cogan%20and%20Aliaksei%20Severyn%20and%20Mohammad%20Saleh%20and%20Shantanu%20Thakoor%20and%20Laurent%20Shefey%20and%20Siyuan%20Qiao%20and%20Meenu%20Gaba%20and%20Shuo-yiin%20Chang%20and%20Craig%20Swanson%20and%20Biao%20Zhang%20and%20Benjamin%20Lee%20and%20Paul%20Kishan%20Rubenstein%20and%20Gan%20Song%20and%20Tom%20Kwiatkowski%20and%20Anna%20Koop%20and%20Ajay%20Kannan%20and%20David%20Kao%20and%20Parker%20Schuh%20and%20Axel%20Stjerngren%20and%20Golnaz%20Ghiasi%20and%20Gena%20Gibson%20and%20Luke%20Vilnis%20and%20Ye%20Yuan%20and%20Felipe%20Tiengo%20Ferreira%20and%20Aishwarya%20Kamath%20and%20Ted%20Klimenko%20and%20Ken%20Franko%20and%20Kefan%20Xiao%20and%20Indro%20Bhattacharya%20and%20Miteyan%20Patel%20and%20Rui%20Wang%20and%20Alex%20Morris%20and%20Robin%20Strudel%20and%20Vivek%20Sharma%20and%20Peter%20Choy%20and%20Sayed%20Hadi%20Hashemi%20and%20Jessica%20Landon%20and%20Mara%20Finkelstein%20and%20Priya%20Jhakra%20and%20Justin%20Frye%20and%20Megan%20Barnes%20and%20Matthew%20Mauger%20and%20Dennis%20Daun%20and%20Khuslen%20Baatarsukh%20and%20Matthew%20Tung%20and%20Wael%20Farhan%20and%20Henryk%20Michalewski%20and%20Fabio%20Viola%20and%20Felix%20de%20Chaumont%20Quitry%20and%20Charline%20Le%20Lan%20and%20Tom%20Hudson%20and%20Qingze%20Wang%20and%20Felix%20Fischer%20and%20Ivy%20Zheng%20and%20Elspeth%20White%20and%20Anca%20Dragan%20and%20Jean-baptiste%20Alayrac%20and%20Eric%20Ni%20and%20Alexander%20Pritzel%20and%20Adam%20Iwanicki%20and%20Michael%20Isard%20and%20Anna%20Bulanova%20and%20Lukas%20Zilka%20and%20Ethan%20Dyer%20and%20Devendra%20Sachan%20and%20Srivatsan%20Srinivasan%20and%20Hannah%20Muckenhirn%20and%20Honglong%20Cai%20and%20Amol%20Mandhane%20and%20Mukarram%20Tariq%20and%20Jack%20W.%20Rae%20and%20Gary%20Wang%20and%20Kareem%20Ayoub%20and%20Nicholas%20FitzGerald%20and%20Yao%20Zhao%20and%20Woohyun%20Han%20and%20Chris%20Alberti%20and%20Dan%20Garrette%20and%20Kashyap%20Krishnakumar%20and%20Mai%20Gimenez%20and%20Anselm%20Levskaya%20and%20Daniel%20Sohn%20and%20Josip%20Matak%20and%20Inaki%20Iturrate%20and%20Michael%20B.%20Chang%20and%20Jackie%20Xiang%20and%20Yuan%20Cao%20and%20Nishant%20Ranka%20and%20Geoff%20Brown%20and%20Adrian%20Hutter%20and%20Vahab%20Mirrokni%20and%20Nanxin%20Chen%20and%20Kaisheng%20Yao%20and%20Zoltan%20Egyed%20and%20Francois%20Galilee%20and%20Tyler%20Liechty%20and%20Praveen%20Kallakuri%20and%20Evan%20Palmer%20and%20Sanjay%20Ghemawat%20and%20Jasmine%20Liu%20and%20David%20Tao%20and%20Chloe%20Thornton%20and%20Tim%20Green%20and%20Mimi%20Jasarevic%20and%20Sharon%20Lin%20and%20Victor%20Cotruta%20and%20Yi-Xuan%20Tan%20and%20Noah%20Fiedel%20and%20Hongkun%20Yu%20and%20Ed%20Chi%20and%20Alexander%20Neitz%20and%20Jens%20Heitkaemper%20and%20Anu%20Sinha%20and%20Denny%20Zhou%20and%20Yi%20Sun%20and%20Charbel%20Kaed%20and%20Brice%20Hulse%20and%20Swaroop%20Mishra%20and%20Maria%20Georgaki%20and%20Sneha%20Kudugunta%20and%20Clement%20Farabet%20and%20Izhak%20Shafran%20and%20Daniel%20Vlasic%20and%20Anton%20Tsitsulin%20and%20Rajagopal%20Ananthanarayanan%20and%20Alen%20Carin%20and%20Guolong%20Su%20and%20Pei%20Sun%20and%20Shashank%20V%20and%20Gabriel%20Carvajal%20and%20Josef%20Broder%20and%20Iulia%20Comsa%20and%20Alena%20Repina%20and%20William%20Wong%20and%20Warren%20Weilun%20Chen%20and%20Peter%20Hawkins%20and%20Egor%20Filonov%20and%20Lucia%20Loher%20and%20Christoph%20Hirnschall%20and%20Weiyi%20Wang%20and%20Jingchen%20Ye%20and%20Andrea%20Burns%20and%20Hardie%20Cate%20and%20Diana%20Gage%20Wright%20and%20Federico%20Piccinini%20and%20Lei%20Zhang%20and%20Chu-Cheng%20Lin%20and%20Ionel%20Gog%20and%20Yana%20Kulizhskaya%20and%20Ashwin%20Sreevatsa%20and%20Shuang%20Song%20and%20Luis%20C.%20Cobo%20and%20Anand%20Iyer%20and%20Chetan%20Tekur%20and%20Guillermo%20Garrido%20and%20Zhuyun%20Xiao%20and%20Rupert%20Kemp%20and%20Huaixiu%20Steven%20Zheng%20and%20Hui%20Li%20and%20Ananth%20Agarwal%20and%20Christel%20Ngani%20and%20Kati%20Goshvadi%20and%20Rebeca%20Santamaria-Fernandez%20and%20Wojciech%20Fica%20and%20Xinyun%20Chen%20and%20Chris%20Gorgolewski%20and%20Sean%20Sun%20and%20Roopal%20Garg%20and%20Xinyu%20Ye%20and%20S.%20M.%20Ali%20Eslami%20and%20Nan%20Hua%20and%20Jon%20Simon%20and%20Pratik%20Joshi%20and%20Yelin%20Kim%20and%20Ian%20Tenney%20and%20Sahitya%20Potluri%20and%20Lam%20Nguyen%20Thiet%20and%20Quan%20Yuan%20and%20Florian%20Luisier%20and%20Alexandra%20Chronopoulou%20and%20Salvatore%20Scellato%20and%20Praveen%20Srinivasan%20and%20Minmin%20Chen%20and%20Vinod%20Koverkathu%20and%20Valentin%20Dalibard%20and%20Yaming%20Xu%20and%20Brennan%20Saeta%20and%20Keith%20Anderson%20and%20Thibault%20Sellam%20and%20Nick%20Fernando%20and%20Fantine%20Huot%20and%20Junehyuk%20Jung%20and%20Mani%20Varadarajan%20and%20Michael%20Quinn%20and%20Amit%20Raul%20and%20Maigo%20Le%20and%20Ruslan%20Habalov%20and%20Jon%20Clark%20and%20Komal%20Jalan%20and%20Kalesha%20Bullard%20and%20Achintya%20Singhal%20and%20Thang%20Luong%20and%20Boyu%20Wang%20and%20Sujeevan%20Rajayogam%20and%20Julian%20Eisenschlos%20and%20Johnson%20Jia%20and%20Daniel%20Finchelstein%20and%20Alex%20Yakubovich%20and%20Daniel%20Balle%20and%20Michael%20Fink%20and%20Sameer%20Agarwal%20and%20Jing%20Li%20and%20Dj%20Dvijotham%20and%20Shalini%20Pal%20and%20Kai%20Kang%20and%20Jaclyn%20Konzelmann%20and%20Jennifer%20Beattie%20and%20Olivier%20Dousse%20and%20Diane%20Wu%20and%20Remi%20Crocker%20and%20Chen%20Elkind%20and%20Siddhartha%20Reddy%20Jonnalagadda%20and%20Jong%20Lee%20and%20Dan%20Holtmann-Rice%20and%20Krystal%20Kallarackal%20and%20Rosanne%20Liu%20and%20Denis%20Vnukov%20and%20Neera%20Vats%20and%20Luca%20Invernizzi%20and%20Mohsen%20Jafari%20and%20Huanjie%20Zhou%20and%20Lilly%20Taylor%20and%20Jennifer%20Prendki%20and%20Marcus%20Wu%20and%20Tom%20Eccles%20and%20Tianqi%20Liu%20and%20Kavya%20Kopparapu%20and%20Francoise%20Beaufays%20and%20Christof%20Angermueller%20and%20Andreea%20Marzoca%20and%20Shourya%20Sarcar%20and%20Hilal%20Dib%20and%20Jeff%20Stanway%20and%20Frank%20Perbet%20and%20Nejc%20Trdin%20and%20Rachel%20Sterneck%20and%20Andrey%20Khorlin%20and%20Dinghua%20Li%20and%20Xihui%20Wu%20and%20Sonam%20Goenka%20and%20David%20Madras%20and%20Sasha%20Goldshtein%20and%20Willi%20Gierke%20and%20Tong%20Zhou%20and%20Yaxin%20Liu%20and%20Yannie%20Liang%20and%20Anais%20White%20and%20Yunjie%20Li%20and%20Shreya%20Singh%20and%20Sanaz%20Bahargam%20and%20Mark%20Epstein%20and%20Sujoy%20Basu%20and%20Li%20Lao%20and%20Adnan%20Ozturel%20and%20Carl%20Crous%20and%20Alex%20Zhai%20and%20Han%20Lu%20and%20Zora%20Tung%20and%20Neeraj%20Gaur%20and%20Alanna%20Walton%20and%20Lucas%20Dixon%20and%20Ming%20Zhang%20and%20Amir%20Globerson%20and%20Grant%20Uy%20and%20Andrew%20Bolt%20and%20Olivia%20Wiles%20and%20Milad%20Nasr%20and%20Ilia%20Shumailov%20and%20Marco%20Selvi%20and%20Francesco%20Piccinno%20and%20Ricardo%20Aguilar%20and%20Sara%20McCarthy%20and%20Misha%20Khalman%20and%20Mrinal%20Shukla%20and%20Vlado%20Galic%20and%20John%20Carpenter%20and%20Kevin%20Villela%20and%20Haibin%20Zhang%20and%20Harry%20Richardson%20and%20James%20Martens%20and%20Matko%20Bosnjak%20and%20Shreyas%20Rammohan%20Belle%20and%20Jeff%20Seibert%20and%20Mahmoud%20Alnahlawi%20and%20Brian%20McWilliams%20and%20Sankalp%20Singh%20and%20Annie%20Louis%20and%20Wen%20Ding%20and%20Dan%20Popovici%20and%20Lenin%20Simicich%20and%20Laura%20Knight%20and%20Pulkit%20Mehta%20and%20Nishesh%20Gupta%20and%20Chongyang%20Shi%20and%20Saaber%20Fatehi%20and%20Jovana%20Mitrovic%20and%20Alex%20Grills%20and%20Joseph%20Pagadora%20and%20Dessie%20Petrova%20and%20Danielle%20Eisenbud%20and%20Zhishuai%20Zhang%20and%20Damion%20Yates%20and%20Bhavishya%20Mittal%20and%20Nilesh%20Tripuraneni%20and%20Yannis%20Assael%20and%20Thomas%20Brovelli%20and%20Prateek%20Jain%20and%20Mihajlo%20Velimirovic%20and%20Canfer%20Akbulut%20and%20Jiaqi%20Mu%20and%20Wolfgang%20Macherey%20and%20Ravin%20Kumar%20and%20Jun%20Xu%20and%20Haroon%20Qureshi%20and%20Gheorghe%20Comanici%20and%20Jeremy%20Wiesner%20and%20Zhitao%20Gong%20and%20Anton%20Ruddock%20and%20Matthias%20Bauer%20and%20Nick%20Felt%20and%20Anirudh%20GP%20and%20Anurag%20Arnab%20and%20Dustin%20Zelle%20and%20Jonas%20Rothfuss%20and%20Bill%20Rosgen%20and%20Ashish%20Shenoy%20and%20Bryan%20Seybold%20and%20Xinjian%20Li%20and%20Jayaram%20Mudigonda%20and%20Goker%20Erdogan%20and%20Jiawei%20Xia%20and%20Jiri%20Simsa%20and%20Andrea%20Michi%20and%20Yi%20Yao%20and%20Christopher%20Yew%20and%20Steven%20Kan%20and%20Isaac%20Caswell%20and%20Carey%20Radebaugh%20and%20Andre%20Elisseeff%20and%20Pedro%20Valenzuela%20and%20Kay%20McKinney%20and%20Kim%20Paterson%20and%20Albert%20Cui%20and%20Eri%20Latorre-Chimoto%20and%20Solomon%20Kim%20and%20William%20Zeng%20and%20Ken%20Durden%20and%20Priya%20Ponnapalli%20and%20Tiberiu%20Sosea%20and%20Christopher%20A.%20Choquette-Choo%20and%20James%20Manyika%20and%20Brona%20Robenek%20and%20Harsha%20Vashisht%20and%20Sebastien%20Pereira%20and%20Hoi%20Lam%20and%20Marko%20Velic%20and%20Denese%20Owusu-Afriyie%20and%20Katherine%20Lee%20and%20Tolga%20Bolukbasi%20and%20Alicia%20Parrish%20and%20Shawn%20Lu%20and%20Jane%20Park%20and%20Balaji%20Venkatraman%20and%20Alice%20Talbert%20and%20Lambert%20Rosique%20and%20Yuchung%20Cheng%20and%20Andrei%20Sozanschi%20and%20Adam%20Paszke%20and%20Praveen%20Kumar%20and%20Jessica%20Austin%20and%20Lu%20Li%20and%20Khalid%20Salama%20and%20Wooyeol%20Kim%20and%20Nandita%20Dukkipati%20and%20Anthony%20Baryshnikov%20and%20Christos%20Kaplanis%20and%20XiangHai%20Sheng%20and%20Yuri%20Chervonyi%20and%20Caglar%20Unlu%20and%20Diego%20de%20Las%20Casas%20and%20Harry%20Askham%20and%20Kathryn%20Tunyasuvunakool%20and%20Felix%20Gimeno%20and%20Siim%20Poder%20and%20Chester%20Kwak%20and%20Matt%20Miecnikowski%20and%20Vahab%20Mirrokni%20and%20Alek%20Dimitriev%20and%20Aaron%20Parisi%20and%20Dangyi%20Liu%20and%20Tomy%20Tsai%20and%20Toby%20Shevlane%20and%20Christina%20Kouridi%20and%20Drew%20Garmon%20and%20Adrian%20Goedeckemeyer%20and%20Adam%20R.%20Brown%20and%20Anitha%20Vijayakumar%20and%20Ali%20Elqursh%20and%20Sadegh%20Jazayeri%20and%20Jin%20Huang%20and%20Sara%20Mc%20Carthy%20and%20Jay%20Hoover%20and%20Lucy%20Kim%20and%20Sandeep%20Kumar%20and%20Wei%20Chen%20and%20Courtney%20Biles%20and%20Garrett%20Bingham%20and%20Evan%20Rosen%20and%20Lisa%20Wang%20and%20Qijun%20Tan%20and%20David%20Engel%20and%20Francesco%20Pongetti%20and%20Dario%20de%20Cesare%20and%20Dongseong%20Hwang%20and%20Lily%20Yu%20and%20Jennifer%20Pullman%20and%20Srini%20Narayanan%20and%20Kyle%20Levin%20and%20Siddharth%20Gopal%20and%20Megan%20Li%20and%20Asaf%20Aharoni%20and%20Trieu%20Trinh%20and%20Jessica%20Lo%20and%20Norman%20Casagrande%20and%20Roopali%20Vij%20and%20Loic%20Matthey%20and%20Bramandia%20Ramadhana%20and%20Austin%20Matthews%20and%20CJ%20Carey%20and%20Matthew%20Johnson%20and%20Kremena%20Goranova%20and%20Rohin%20Shah%20and%20Shereen%20Ashraf%20and%20Kingshuk%20Dasgupta%20and%20Rasmus%20Larsen%20and%20Yicheng%20Wang%20and%20Manish%20Reddy%20Vuyyuru%20and%20Chong%20Jiang%20and%20Joana%20Ijazi%20and%20Kazuki%20Osawa%20and%20Celine%20Smith%20and%20Ramya%20Sree%20Boppana%20and%20Taylan%20Bilal%20and%20Yuma%20Koizumi%20and%20Ying%20Xu%20and%20Yasemin%20Altun%20and%20Nir%20Shabat%20and%20Ben%20Bariach%20and%20Alex%20Korchemniy%20and%20Kiam%20Choo%20and%20Olaf%20Ronneberger%20and%20Chimezie%20Iwuanyanwu%20and%20Shubin%20Zhao%20and%20David%20Soergel%20and%20Cho-Jui%20Hsieh%20and%20Irene%20Cai%20and%20Shariq%20Iqbal%20and%20Martin%20Sundermeyer%20and%20Zhe%20Chen%20and%20Elie%20Bursztein%20and%20Chaitanya%20Malaviya%20and%20Fadi%20Biadsy%20and%20Prakash%20Shroff%20and%20Inderjit%20Dhillon%20and%20Tejasi%20Latkar%20and%20Chris%20Dyer%20and%20Hannah%20Forbes%20and%20Massimo%20Nicosia%20and%20Vitaly%20Nikolaev%20and%20Somer%20Greene%20and%20Marin%20Georgiev%20and%20Pidong%20Wang%20and%20Nina%20Martin%20and%20Hanie%20Sedghi%20and%20John%20Zhang%20and%20Praseem%20Banzal%20and%20Doug%20Fritz%20and%20Vikram%20Rao%20and%20Xuezhi%20Wang%20and%20Jiageng%20Zhang%20and%20Viorica%20Patraucean%20and%20Dayou%20Du%20and%20Igor%20Mordatch%20and%20Ivan%20Jurin%20and%20Lewis%20Liu%20and%20Ayush%20Dubey%20and%20Abhi%20Mohan%20and%20Janek%20Nowakowski%20and%20Vlad-Doru%20Ion%20and%20Nan%20Wei%20and%20Reiko%20Tojo%20and%20Maria%20Abi%20Raad%20and%20Drew%20A.%20Hudson%20and%20Vaishakh%20Keshava%20and%20Shubham%20Agrawal%20and%20Kevin%20Ramirez%20and%20Zhichun%20Wu%20and%20Hoang%20Nguyen%20and%20Ji%20Liu%20and%20Madhavi%20Sewak%20and%20Bryce%20Petrini%20and%20DongHyun%20Choi%20and%20Ivan%20Philips%20and%20Ziyue%20Wang%20and%20Ioana%20Bica%20and%20Ankush%20Garg%20and%20Jarek%20Wilkiewicz%20and%20Priyanka%20Agrawal%20and%20Xiaowei%20Li%20and%20Danhao%20Guo%20and%20Emily%20Xue%20and%20Naseer%20Shaik%20and%20Andrew%20Leach%20and%20Sadh%20MNM%20Khan%20and%20Julia%20Wiesinger%20and%20Sammy%20Jerome%20and%20Abhishek%20Chakladar%20and%20Alek%20Wenjiao%20Wang%20and%20Tina%20Ornduff%20and%20Folake%20Abu%20and%20Alireza%20Ghaffarkhah%20and%20Marcus%20Wainwright%20and%20Mario%20Cortes%20and%20Frederick%20Liu%20and%20Joshua%20Maynez%20and%20Slav%20Petrov%20and%20Yonghui%20Wu%20and%20Demis%20Hassabis%20and%20Koray%20Kavukcuoglu%20and%20Jeffrey%20Dean%20and%20Oriol%20Vinyals%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20introduce%20the%20Gemini%201.5%20family%20of%20models%2C%20representing%0Athe%20next%20generation%20of%20highly%20compute-efficient%20multimodal%20models%20capable%20of%0Arecalling%20and%20reasoning%20over%20fine-grained%20information%20from%20millions%20of%20tokens%0Aof%20context%2C%20including%20multiple%20long%20documents%20and%20hours%20of%20video%20and%20audio.%20The%0Afamily%20includes%20two%20new%20models%3A%20%281%29%20an%20updated%20Gemini%201.5%20Pro%2C%20which%20exceeds%0Athe%20February%20version%20on%20the%20great%20majority%20of%20capabilities%20and%20benchmarks%3B%20%282%29%0AGemini%201.5%20Flash%2C%20a%20more%20lightweight%20variant%20designed%20for%20efficiency%20with%0Aminimal%20regression%20in%20quality.%20Gemini%201.5%20models%20achieve%20near-perfect%20recall%20on%0Along-context%20retrieval%20tasks%20across%20modalities%2C%20improve%20the%20state-of-the-art%20in%0Along-document%20QA%2C%20long-video%20QA%20and%20long-context%20ASR%2C%20and%20match%20or%20surpass%0AGemini%201.0%20Ultra%27s%20state-of-the-art%20performance%20across%20a%20broad%20set%20of%0Abenchmarks.%20Studying%20the%20limits%20of%20Gemini%201.5%27s%20long-context%20ability%2C%20we%20find%0Acontinued%20improvement%20in%20next-token%20prediction%20and%20near-perfect%20retrieval%0A%28%3E99%25%29%20up%20to%20at%20least%2010M%20tokens%2C%20a%20generational%20leap%20over%20existing%20models%20such%0Aas%20Claude%203.0%20%28200k%29%20and%20GPT-4%20Turbo%20%28128k%29.%20Finally%2C%20we%20highlight%20real-world%0Ause%20cases%2C%20such%20as%20Gemini%201.5%20collaborating%20with%20professionals%20on%20completing%0Atheir%20tasks%20achieving%2026%20to%2075%25%20time%20savings%20across%2010%20different%20job%0Acategories%2C%20as%20well%20as%20surprising%20new%20capabilities%20of%20large%20language%20models%20at%0Athe%20frontier%3B%20when%20given%20a%20grammar%20manual%20for%20Kalamang%2C%20a%20language%20with%20fewer%0Athan%20200%20speakers%20worldwide%2C%20the%20model%20learns%20to%20translate%20English%20to%20Kalamang%0Aat%20a%20similar%20level%20to%20a%20person%20who%20learned%20from%20the%20same%20content.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05530v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGemini%25201.5%253A%2520Unlocking%2520multimodal%2520understanding%2520across%2520millions%2520of%2520tokens%250A%2520%2520of%2520context%26entry.906535625%3D%2520Gemini%2520Team%2520and%2520Petko%2520Georgiev%2520and%2520Ving%2520Ian%2520Lei%2520and%2520Ryan%2520Burnell%2520and%2520Libin%2520Bai%2520and%2520Anmol%2520Gulati%2520and%2520Garrett%2520Tanzer%2520and%2520Damien%2520Vincent%2520and%2520Zhufeng%2520Pan%2520and%2520Shibo%2520Wang%2520and%2520Soroosh%2520Mariooryad%2520and%2520Yifan%2520Ding%2520and%2520Xinyang%2520Geng%2520and%2520Fred%2520Alcober%2520and%2520Roy%2520Frostig%2520and%2520Mark%2520Omernick%2520and%2520Lexi%2520Walker%2520and%2520Cosmin%2520Paduraru%2520and%2520Christina%2520Sorokin%2520and%2520Andrea%2520Tacchetti%2520and%2520Colin%2520Gaffney%2520and%2520Samira%2520Daruki%2520and%2520Olcan%2520Sercinoglu%2520and%2520Zach%2520Gleicher%2520and%2520Juliette%2520Love%2520and%2520Paul%2520Voigtlaender%2520and%2520Rohan%2520Jain%2520and%2520Gabriela%2520Surita%2520and%2520Kareem%2520Mohamed%2520and%2520Rory%2520Blevins%2520and%2520Junwhan%2520Ahn%2520and%2520Tao%2520Zhu%2520and%2520Kornraphop%2520Kawintiranon%2520and%2520Orhan%2520Firat%2520and%2520Yiming%2520Gu%2520and%2520Yujing%2520Zhang%2520and%2520Matthew%2520Rahtz%2520and%2520Manaal%2520Faruqui%2520and%2520Natalie%2520Clay%2520and%2520Justin%2520Gilmer%2520and%2520JD%2520Co-Reyes%2520and%2520Ivo%2520Penchev%2520and%2520Rui%2520Zhu%2520and%2520Nobuyuki%2520Morioka%2520and%2520Kevin%2520Hui%2520and%2520Krishna%2520Haridasan%2520and%2520Victor%2520Campos%2520and%2520Mahdis%2520Mahdieh%2520and%2520Mandy%2520Guo%2520and%2520Samer%2520Hassan%2520and%2520Kevin%2520Kilgour%2520and%2520Arpi%2520Vezer%2520and%2520Heng-Tze%2520Cheng%2520and%2520Raoul%2520de%2520Liedekerke%2520and%2520Siddharth%2520Goyal%2520and%2520Paul%2520Barham%2520and%2520DJ%2520Strouse%2520and%2520Seb%2520Noury%2520and%2520Jonas%2520Adler%2520and%2520Mukund%2520Sundararajan%2520and%2520Sharad%2520Vikram%2520and%2520Dmitry%2520Lepikhin%2520and%2520Michela%2520Paganini%2520and%2520Xavier%2520Garcia%2520and%2520Fan%2520Yang%2520and%2520Dasha%2520Valter%2520and%2520Maja%2520Trebacz%2520and%2520Kiran%2520Vodrahalli%2520and%2520Chulayuth%2520Asawaroengchai%2520and%2520Roman%2520Ring%2520and%2520Norbert%2520Kalb%2520and%2520Livio%2520Baldini%2520Soares%2520and%2520Siddhartha%2520Brahma%2520and%2520David%2520Steiner%2520and%2520Tianhe%2520Yu%2520and%2520Fabian%2520Mentzer%2520and%2520Antoine%2520He%2520and%2520Lucas%2520Gonzalez%2520and%2520Bibo%2520Xu%2520and%2520Raphael%2520Lopez%2520Kaufman%2520and%2520Laurent%2520El%2520Shafey%2520and%2520Junhyuk%2520Oh%2520and%2520Tom%2520Hennigan%2520and%2520George%2520van%2520den%2520Driessche%2520and%2520Seth%2520Odoom%2520and%2520Mario%2520Lucic%2520and%2520Becca%2520Roelofs%2520and%2520Sid%2520Lall%2520and%2520Amit%2520Marathe%2520and%2520Betty%2520Chan%2520and%2520Santiago%2520Ontanon%2520and%2520Luheng%2520He%2520and%2520Denis%2520Teplyashin%2520and%2520Jonathan%2520Lai%2520and%2520Phil%2520Crone%2520and%2520Bogdan%2520Damoc%2520and%2520Lewis%2520Ho%2520and%2520Sebastian%2520Riedel%2520and%2520Karel%2520Lenc%2520and%2520Chih-Kuan%2520Yeh%2520and%2520Aakanksha%2520Chowdhery%2520and%2520Yang%2520Xu%2520and%2520Mehran%2520Kazemi%2520and%2520Ehsan%2520Amid%2520and%2520Anastasia%2520Petrushkina%2520and%2520Kevin%2520Swersky%2520and%2520Ali%2520Khodaei%2520and%2520Gowoon%2520Chen%2520and%2520Chris%2520Larkin%2520and%2520Mario%2520Pinto%2520and%2520Geng%2520Yan%2520and%2520Adria%2520Puigdomenech%2520Badia%2520and%2520Piyush%2520Patil%2520and%2520Steven%2520Hansen%2520and%2520Dave%2520Orr%2520and%2520Sebastien%2520M.%2520R.%2520Arnold%2520and%2520Jordan%2520Grimstad%2520and%2520Andrew%2520Dai%2520and%2520Sholto%2520Douglas%2520and%2520Rishika%2520Sinha%2520and%2520Vikas%2520Yadav%2520and%2520Xi%2520Chen%2520and%2520Elena%2520Gribovskaya%2520and%2520Jacob%2520Austin%2520and%2520Jeffrey%2520Zhao%2520and%2520Kaushal%2520Patel%2520and%2520Paul%2520Komarek%2520and%2520Sophia%2520Austin%2520and%2520Sebastian%2520Borgeaud%2520and%2520Linda%2520Friso%2520and%2520Abhimanyu%2520Goyal%2520and%2520Ben%2520Caine%2520and%2520Kris%2520Cao%2520and%2520Da-Woon%2520Chung%2520and%2520Matthew%2520Lamm%2520and%2520Gabe%2520Barth-Maron%2520and%2520Thais%2520Kagohara%2520and%2520Kate%2520Olszewska%2520and%2520Mia%2520Chen%2520and%2520Kaushik%2520Shivakumar%2520and%2520Rishabh%2520Agarwal%2520and%2520Harshal%2520Godhia%2520and%2520Ravi%2520Rajwar%2520and%2520Javier%2520Snaider%2520and%2520Xerxes%2520Dotiwalla%2520and%2520Yuan%2520Liu%2520and%2520Aditya%2520Barua%2520and%2520Victor%2520Ungureanu%2520and%2520Yuan%2520Zhang%2520and%2520Bat-Orgil%2520Batsaikhan%2520and%2520Mateo%2520Wirth%2520and%2520James%2520Qin%2520and%2520Ivo%2520Danihelka%2520and%2520Tulsee%2520Doshi%2520and%2520Martin%2520Chadwick%2520and%2520Jilin%2520Chen%2520and%2520Sanil%2520Jain%2520and%2520Quoc%2520Le%2520and%2520Arjun%2520Kar%2520and%2520Madhu%2520Gurumurthy%2520and%2520Cheng%2520Li%2520and%2520Ruoxin%2520Sang%2520and%2520Fangyu%2520Liu%2520and%2520Lampros%2520Lamprou%2520and%2520Rich%2520Munoz%2520and%2520Nathan%2520Lintz%2520and%2520Harsh%2520Mehta%2520and%2520Heidi%2520Howard%2520and%2520Malcolm%2520Reynolds%2520and%2520Lora%2520Aroyo%2520and%2520Quan%2520Wang%2520and%2520Lorenzo%2520Blanco%2520and%2520Albin%2520Cassirer%2520and%2520Jordan%2520Griffith%2520and%2520Dipanjan%2520Das%2520and%2520Stephan%2520Lee%2520and%2520Jakub%2520Sygnowski%2520and%2520Zach%2520Fisher%2520and%2520James%2520Besley%2520and%2520Richard%2520Powell%2520and%2520Zafarali%2520Ahmed%2520and%2520Dominik%2520Paulus%2520and%2520David%2520Reitter%2520and%2520Zalan%2520Borsos%2520and%2520Rishabh%2520Joshi%2520and%2520Aedan%2520Pope%2520and%2520Steven%2520Hand%2520and%2520Vittorio%2520Selo%2520and%2520Vihan%2520Jain%2520and%2520Nikhil%2520Sethi%2520and%2520Megha%2520Goel%2520and%2520Takaki%2520Makino%2520and%2520Rhys%2520May%2520and%2520Zhen%2520Yang%2520and%2520Johan%2520Schalkwyk%2520and%2520Christina%2520Butterfield%2520and%2520Anja%2520Hauth%2520and%2520Alex%2520Goldin%2520and%2520Will%2520Hawkins%2520and%2520Evan%2520Senter%2520and%2520Sergey%2520Brin%2520and%2520Oliver%2520Woodman%2520and%2520Marvin%2520Ritter%2520and%2520Eric%2520Noland%2520and%2520Minh%2520Giang%2520and%2520Vijay%2520Bolina%2520and%2520Lisa%2520Lee%2520and%2520Tim%2520Blyth%2520and%2520Ian%2520Mackinnon%2520and%2520Machel%2520Reid%2520and%2520Obaid%2520Sarvana%2520and%2520David%2520Silver%2520and%2520Alexander%2520Chen%2520and%2520Lily%2520Wang%2520and%2520Loren%2520Maggiore%2520and%2520Oscar%2520Chang%2520and%2520Nithya%2520Attaluri%2520and%2520Gregory%2520Thornton%2520and%2520Chung-Cheng%2520Chiu%2520and%2520Oskar%2520Bunyan%2520and%2520Nir%2520Levine%2520and%2520Timothy%2520Chung%2520and%2520Evgenii%2520Eltyshev%2520and%2520Xiance%2520Si%2520and%2520Timothy%2520Lillicrap%2520and%2520Demetra%2520Brady%2520and%2520Vaibhav%2520Aggarwal%2520and%2520Boxi%2520Wu%2520and%2520Yuanzhong%2520Xu%2520and%2520Ross%2520McIlroy%2520and%2520Kartikeya%2520Badola%2520and%2520Paramjit%2520Sandhu%2520and%2520Erica%2520Moreira%2520and%2520Wojciech%2520Stokowiec%2520and%2520Ross%2520Hemsley%2520and%2520Dong%2520Li%2520and%2520Alex%2520Tudor%2520and%2520Pranav%2520Shyam%2520and%2520Elahe%2520Rahimtoroghi%2520and%2520Salem%2520Haykal%2520and%2520Pablo%2520Sprechmann%2520and%2520Xiang%2520Zhou%2520and%2520Diana%2520Mincu%2520and%2520Yujia%2520Li%2520and%2520Ravi%2520Addanki%2520and%2520Kalpesh%2520Krishna%2520and%2520Xiao%2520Wu%2520and%2520Alexandre%2520Frechette%2520and%2520Matan%2520Eyal%2520and%2520Allan%2520Dafoe%2520and%2520Dave%2520Lacey%2520and%2520Jay%2520Whang%2520and%2520Thi%2520Avrahami%2520and%2520Ye%2520Zhang%2520and%2520Emanuel%2520Taropa%2520and%2520Hanzhao%2520Lin%2520and%2520Daniel%2520Toyama%2520and%2520Eliza%2520Rutherford%2520and%2520Motoki%2520Sano%2520and%2520HyunJeong%2520Choe%2520and%2520Alex%2520Tomala%2520and%2520Chalence%2520Safranek-Shrader%2520and%2520Nora%2520Kassner%2520and%2520Mantas%2520Pajarskas%2520and%2520Matt%2520Harvey%2520and%2520Sean%2520Sechrist%2520and%2520Meire%2520Fortunato%2520and%2520Christina%2520Lyu%2520and%2520Gamaleldin%2520Elsayed%2520and%2520Chenkai%2520Kuang%2520and%2520James%2520Lottes%2520and%2520Eric%2520Chu%2520and%2520Chao%2520Jia%2520and%2520Chih-Wei%2520Chen%2520and%2520Peter%2520Humphreys%2520and%2520Kate%2520Baumli%2520and%2520Connie%2520Tao%2520and%2520Rajkumar%2520Samuel%2520and%2520Cicero%2520Nogueira%2520dos%2520Santos%2520and%2520Anders%2520Andreassen%2520and%2520Nemanja%2520Raki%25C4%2587evi%25C4%2587%2520and%2520Dominik%2520Grewe%2520and%2520Aviral%2520Kumar%2520and%2520Stephanie%2520Winkler%2520and%2520Jonathan%2520Caton%2520and%2520Andrew%2520Brock%2520and%2520Sid%2520Dalmia%2520and%2520Hannah%2520Sheahan%2520and%2520Iain%2520Barr%2520and%2520Yingjie%2520Miao%2520and%2520Paul%2520Natsev%2520and%2520Jacob%2520Devlin%2520and%2520Feryal%2520Behbahani%2520and%2520Flavien%2520Prost%2520and%2520Yanhua%2520Sun%2520and%2520Artiom%2520Myaskovsky%2520and%2520Thanumalayan%2520Sankaranarayana%2520Pillai%2520and%2520Dan%2520Hurt%2520and%2520Angeliki%2520Lazaridou%2520and%2520Xi%2520Xiong%2520and%2520Ce%2520Zheng%2520and%2520Fabio%2520Pardo%2520and%2520Xiaowei%2520Li%2520and%2520Dan%2520Horgan%2520and%2520Joe%2520Stanton%2520and%2520Moran%2520Ambar%2520and%2520Fei%2520Xia%2520and%2520Alejandro%2520Lince%2520and%2520Mingqiu%2520Wang%2520and%2520Basil%2520Mustafa%2520and%2520Albert%2520Webson%2520and%2520Hyo%2520Lee%2520and%2520Rohan%2520Anil%2520and%2520Martin%2520Wicke%2520and%2520Timothy%2520Dozat%2520and%2520Abhishek%2520Sinha%2520and%2520Enrique%2520Piqueras%2520and%2520Elahe%2520Dabir%2520and%2520Shyam%2520Upadhyay%2520and%2520Anudhyan%2520Boral%2520and%2520Lisa%2520Anne%2520Hendricks%2520and%2520Corey%2520Fry%2520and%2520Josip%2520Djolonga%2520and%2520Yi%2520Su%2520and%2520Jake%2520Walker%2520and%2520Jane%2520Labanowski%2520and%2520Ronny%2520Huang%2520and%2520Vedant%2520Misra%2520and%2520Jeremy%2520Chen%2520and%2520RJ%2520Skerry-Ryan%2520and%2520Avi%2520Singh%2520and%2520Shruti%2520Rijhwani%2520and%2520Dian%2520Yu%2520and%2520Alex%2520Castro-Ros%2520and%2520Beer%2520Changpinyo%2520and%2520Romina%2520Datta%2520and%2520Sumit%2520Bagri%2520and%2520Arnar%2520Mar%2520Hrafnkelsson%2520and%2520Marcello%2520Maggioni%2520and%2520Daniel%2520Zheng%2520and%2520Yury%2520Sulsky%2520and%2520Shaobo%2520Hou%2520and%2520Tom%2520Le%2520Paine%2520and%2520Antoine%2520Yang%2520and%2520Jason%2520Riesa%2520and%2520Dominika%2520Rogozinska%2520and%2520Dror%2520Marcus%2520and%2520Dalia%2520El%2520Badawy%2520and%2520Qiao%2520Zhang%2520and%2520Luyu%2520Wang%2520and%2520Helen%2520Miller%2520and%2520Jeremy%2520Greer%2520and%2520Lars%2520Lowe%2520Sjos%2520and%2520Azade%2520Nova%2520and%2520Heiga%2520Zen%2520and%2520Rahma%2520Chaabouni%2520and%2520Mihaela%2520Rosca%2520and%2520Jiepu%2520Jiang%2520and%2520Charlie%2520Chen%2520and%2520Ruibo%2520Liu%2520and%2520Tara%2520Sainath%2520and%2520Maxim%2520Krikun%2520and%2520Alex%2520Polozov%2520and%2520Jean-Baptiste%2520Lespiau%2520and%2520Josh%2520Newlan%2520and%2520Zeyncep%2520Cankara%2520and%2520Soo%2520Kwak%2520and%2520Yunhan%2520Xu%2520and%2520Phil%2520Chen%2520and%2520Andy%2520Coenen%2520and%2520Clemens%2520Meyer%2520and%2520Katerina%2520Tsihlas%2520and%2520Ada%2520Ma%2520and%2520Juraj%2520Gottweis%2520and%2520Jinwei%2520Xing%2520and%2520Chenjie%2520Gu%2520and%2520Jin%2520Miao%2520and%2520Christian%2520Frank%2520and%2520Zeynep%2520Cankara%2520and%2520Sanjay%2520Ganapathy%2520and%2520Ishita%2520Dasgupta%2520and%2520Steph%2520Hughes-Fitt%2520and%2520Heng%2520Chen%2520and%2520David%2520Reid%2520and%2520Keran%2520Rong%2520and%2520Hongmin%2520Fan%2520and%2520Joost%2520van%2520Amersfoort%2520and%2520Vincent%2520Zhuang%2520and%2520Aaron%2520Cohen%2520and%2520Shixiang%2520Shane%2520Gu%2520and%2520Anhad%2520Mohananey%2520and%2520Anastasija%2520Ilic%2520and%2520Taylor%2520Tobin%2520and%2520John%2520Wieting%2520and%2520Anna%2520Bortsova%2520and%2520Phoebe%2520Thacker%2520and%2520Emma%2520Wang%2520and%2520Emily%2520Caveness%2520and%2520Justin%2520Chiu%2520and%2520Eren%2520Sezener%2520and%2520Alex%2520Kaskasoli%2520and%2520Steven%2520Baker%2520and%2520Katie%2520Millican%2520and%2520Mohamed%2520Elhawaty%2520and%2520Kostas%2520Aisopos%2520and%2520Carl%2520Lebsack%2520and%2520Nathan%2520Byrd%2520and%2520Hanjun%2520Dai%2520and%2520Wenhao%2520Jia%2520and%2520Matthew%2520Wiethoff%2520and%2520Elnaz%2520Davoodi%2520and%2520Albert%2520Weston%2520and%2520Lakshman%2520Yagati%2520and%2520Arun%2520Ahuja%2520and%2520Isabel%2520Gao%2520and%2520Golan%2520Pundak%2520and%2520Susan%2520Zhang%2520and%2520Michael%2520Azzam%2520and%2520Khe%2520Chai%2520Sim%2520and%2520Sergi%2520Caelles%2520and%2520James%2520Keeling%2520and%2520Abhanshu%2520Sharma%2520and%2520Andy%2520Swing%2520and%2520YaGuang%2520Li%2520and%2520Chenxi%2520Liu%2520and%2520Carrie%2520Grimes%2520Bostock%2520and%2520Yamini%2520Bansal%2520and%2520Zachary%2520Nado%2520and%2520Ankesh%2520Anand%2520and%2520Josh%2520Lipschultz%2520and%2520Abhijit%2520Karmarkar%2520and%2520Lev%2520Proleev%2520and%2520Abe%2520Ittycheriah%2520and%2520Soheil%2520Hassas%2520Yeganeh%2520and%2520George%2520Polovets%2520and%2520Aleksandra%2520Faust%2520and%2520Jiao%2520Sun%2520and%2520Alban%2520Rrustemi%2520and%2520Pen%2520Li%2520and%2520Rakesh%2520Shivanna%2520and%2520Jeremiah%2520Liu%2520and%2520Chris%2520Welty%2520and%2520Federico%2520Lebron%2520and%2520Anirudh%2520Baddepudi%2520and%2520Sebastian%2520Krause%2520and%2520Emilio%2520Parisotto%2520and%2520Radu%2520Soricut%2520and%2520Zheng%2520Xu%2520and%2520Dawn%2520Bloxwich%2520and%2520Melvin%2520Johnson%2520and%2520Behnam%2520Neyshabur%2520and%2520Justin%2520Mao-Jones%2520and%2520Renshen%2520Wang%2520and%2520Vinay%2520Ramasesh%2520and%2520Zaheer%2520Abbas%2520and%2520Arthur%2520Guez%2520and%2520Constant%2520Segal%2520and%2520Duc%2520Dung%2520Nguyen%2520and%2520James%2520Svensson%2520and%2520Le%2520Hou%2520and%2520Sarah%2520York%2520and%2520Kieran%2520Milan%2520and%2520Sophie%2520Bridgers%2520and%2520Wiktor%2520Gworek%2520and%2520Marco%2520Tagliasacchi%2520and%2520James%2520Lee-Thorp%2520and%2520Michael%2520Chang%2520and%2520Alexey%2520Guseynov%2520and%2520Ale%2520Jakse%2520Hartman%2520and%2520Michael%2520Kwong%2520and%2520Ruizhe%2520Zhao%2520and%2520Sheleem%2520Kashem%2520and%2520Elizabeth%2520Cole%2520and%2520Antoine%2520Miech%2520and%2520Richard%2520Tanburn%2520and%2520Mary%2520Phuong%2520and%2520Filip%2520Pavetic%2520and%2520Sebastien%2520Cevey%2520and%2520Ramona%2520Comanescu%2520and%2520Richard%2520Ives%2520and%2520Sherry%2520Yang%2520and%2520Cosmo%2520Du%2520and%2520Bo%2520Li%2520and%2520Zizhao%2520Zhang%2520and%2520Mariko%2520Iinuma%2520and%2520Clara%2520Huiyi%2520Hu%2520and%2520Aurko%2520Roy%2520and%2520Shaan%2520Bijwadia%2520and%2520Zhenkai%2520Zhu%2520and%2520Danilo%2520Martins%2520and%2520Rachel%2520Saputro%2520and%2520Anita%2520Gergely%2520and%2520Steven%2520Zheng%2520and%2520Dawei%2520Jia%2520and%2520Ioannis%2520Antonoglou%2520and%2520Adam%2520Sadovsky%2520and%2520Shane%2520Gu%2520and%2520Yingying%2520Bi%2520and%2520Alek%2520Andreev%2520and%2520Sina%2520Samangooei%2520and%2520Mina%2520Khan%2520and%2520Tomas%2520Kocisky%2520and%2520Angelos%2520Filos%2520and%2520Chintu%2520Kumar%2520and%2520Colton%2520Bishop%2520and%2520Adams%2520Yu%2520and%2520Sarah%2520Hodkinson%2520and%2520Sid%2520Mittal%2520and%2520Premal%2520Shah%2520and%2520Alexandre%2520Moufarek%2520and%2520Yong%2520Cheng%2520and%2520Adam%2520Bloniarz%2520and%2520Jaehoon%2520Lee%2520and%2520Pedram%2520Pejman%2520and%2520Paul%2520Michel%2520and%2520Stephen%2520Spencer%2520and%2520Vladimir%2520Feinberg%2520and%2520Xuehan%2520Xiong%2520and%2520Nikolay%2520Savinov%2520and%2520Charlotte%2520Smith%2520and%2520Siamak%2520Shakeri%2520and%2520Dustin%2520Tran%2520and%2520Mary%2520Chesus%2520and%2520Bernd%2520Bohnet%2520and%2520George%2520Tucker%2520and%2520Tamara%2520von%2520Glehn%2520and%2520Carrie%2520Muir%2520and%2520Yiran%2520Mao%2520and%2520Hideto%2520Kazawa%2520and%2520Ambrose%2520Slone%2520and%2520Kedar%2520Soparkar%2520and%2520Disha%2520Shrivastava%2520and%2520James%2520Cobon-Kerr%2520and%2520Michael%2520Sharman%2520and%2520Jay%2520Pavagadhi%2520and%2520Carlos%2520Araya%2520and%2520Karolis%2520Misiunas%2520and%2520Nimesh%2520Ghelani%2520and%2520Michael%2520Laskin%2520and%2520David%2520Barker%2520and%2520Qiujia%2520Li%2520and%2520Anton%2520Briukhov%2520and%2520Neil%2520Houlsby%2520and%2520Mia%2520Glaese%2520and%2520Balaji%2520Lakshminarayanan%2520and%2520Nathan%2520Schucher%2520and%2520Yunhao%2520Tang%2520and%2520Eli%2520Collins%2520and%2520Hyeontaek%2520Lim%2520and%2520Fangxiaoyu%2520Feng%2520and%2520Adria%2520Recasens%2520and%2520Guangda%2520Lai%2520and%2520Alberto%2520Magni%2520and%2520Nicola%2520De%2520Cao%2520and%2520Aditya%2520Siddhant%2520and%2520Zoe%2520Ashwood%2520and%2520Jordi%2520Orbay%2520and%2520Mostafa%2520Dehghani%2520and%2520Jenny%2520Brennan%2520and%2520Yifan%2520He%2520and%2520Kelvin%2520Xu%2520and%2520Yang%2520Gao%2520and%2520Carl%2520Saroufim%2520and%2520James%2520Molloy%2520and%2520Xinyi%2520Wu%2520and%2520Seb%2520Arnold%2520and%2520Solomon%2520Chang%2520and%2520Julian%2520Schrittwieser%2520and%2520Elena%2520Buchatskaya%2520and%2520Soroush%2520Radpour%2520and%2520Martin%2520Polacek%2520and%2520Skye%2520Giordano%2520and%2520Ankur%2520Bapna%2520and%2520Simon%2520Tokumine%2520and%2520Vincent%2520Hellendoorn%2520and%2520Thibault%2520Sottiaux%2520and%2520Sarah%2520Cogan%2520and%2520Aliaksei%2520Severyn%2520and%2520Mohammad%2520Saleh%2520and%2520Shantanu%2520Thakoor%2520and%2520Laurent%2520Shefey%2520and%2520Siyuan%2520Qiao%2520and%2520Meenu%2520Gaba%2520and%2520Shuo-yiin%2520Chang%2520and%2520Craig%2520Swanson%2520and%2520Biao%2520Zhang%2520and%2520Benjamin%2520Lee%2520and%2520Paul%2520Kishan%2520Rubenstein%2520and%2520Gan%2520Song%2520and%2520Tom%2520Kwiatkowski%2520and%2520Anna%2520Koop%2520and%2520Ajay%2520Kannan%2520and%2520David%2520Kao%2520and%2520Parker%2520Schuh%2520and%2520Axel%2520Stjerngren%2520and%2520Golnaz%2520Ghiasi%2520and%2520Gena%2520Gibson%2520and%2520Luke%2520Vilnis%2520and%2520Ye%2520Yuan%2520and%2520Felipe%2520Tiengo%2520Ferreira%2520and%2520Aishwarya%2520Kamath%2520and%2520Ted%2520Klimenko%2520and%2520Ken%2520Franko%2520and%2520Kefan%2520Xiao%2520and%2520Indro%2520Bhattacharya%2520and%2520Miteyan%2520Patel%2520and%2520Rui%2520Wang%2520and%2520Alex%2520Morris%2520and%2520Robin%2520Strudel%2520and%2520Vivek%2520Sharma%2520and%2520Peter%2520Choy%2520and%2520Sayed%2520Hadi%2520Hashemi%2520and%2520Jessica%2520Landon%2520and%2520Mara%2520Finkelstein%2520and%2520Priya%2520Jhakra%2520and%2520Justin%2520Frye%2520and%2520Megan%2520Barnes%2520and%2520Matthew%2520Mauger%2520and%2520Dennis%2520Daun%2520and%2520Khuslen%2520Baatarsukh%2520and%2520Matthew%2520Tung%2520and%2520Wael%2520Farhan%2520and%2520Henryk%2520Michalewski%2520and%2520Fabio%2520Viola%2520and%2520Felix%2520de%2520Chaumont%2520Quitry%2520and%2520Charline%2520Le%2520Lan%2520and%2520Tom%2520Hudson%2520and%2520Qingze%2520Wang%2520and%2520Felix%2520Fischer%2520and%2520Ivy%2520Zheng%2520and%2520Elspeth%2520White%2520and%2520Anca%2520Dragan%2520and%2520Jean-baptiste%2520Alayrac%2520and%2520Eric%2520Ni%2520and%2520Alexander%2520Pritzel%2520and%2520Adam%2520Iwanicki%2520and%2520Michael%2520Isard%2520and%2520Anna%2520Bulanova%2520and%2520Lukas%2520Zilka%2520and%2520Ethan%2520Dyer%2520and%2520Devendra%2520Sachan%2520and%2520Srivatsan%2520Srinivasan%2520and%2520Hannah%2520Muckenhirn%2520and%2520Honglong%2520Cai%2520and%2520Amol%2520Mandhane%2520and%2520Mukarram%2520Tariq%2520and%2520Jack%2520W.%2520Rae%2520and%2520Gary%2520Wang%2520and%2520Kareem%2520Ayoub%2520and%2520Nicholas%2520FitzGerald%2520and%2520Yao%2520Zhao%2520and%2520Woohyun%2520Han%2520and%2520Chris%2520Alberti%2520and%2520Dan%2520Garrette%2520and%2520Kashyap%2520Krishnakumar%2520and%2520Mai%2520Gimenez%2520and%2520Anselm%2520Levskaya%2520and%2520Daniel%2520Sohn%2520and%2520Josip%2520Matak%2520and%2520Inaki%2520Iturrate%2520and%2520Michael%2520B.%2520Chang%2520and%2520Jackie%2520Xiang%2520and%2520Yuan%2520Cao%2520and%2520Nishant%2520Ranka%2520and%2520Geoff%2520Brown%2520and%2520Adrian%2520Hutter%2520and%2520Vahab%2520Mirrokni%2520and%2520Nanxin%2520Chen%2520and%2520Kaisheng%2520Yao%2520and%2520Zoltan%2520Egyed%2520and%2520Francois%2520Galilee%2520and%2520Tyler%2520Liechty%2520and%2520Praveen%2520Kallakuri%2520and%2520Evan%2520Palmer%2520and%2520Sanjay%2520Ghemawat%2520and%2520Jasmine%2520Liu%2520and%2520David%2520Tao%2520and%2520Chloe%2520Thornton%2520and%2520Tim%2520Green%2520and%2520Mimi%2520Jasarevic%2520and%2520Sharon%2520Lin%2520and%2520Victor%2520Cotruta%2520and%2520Yi-Xuan%2520Tan%2520and%2520Noah%2520Fiedel%2520and%2520Hongkun%2520Yu%2520and%2520Ed%2520Chi%2520and%2520Alexander%2520Neitz%2520and%2520Jens%2520Heitkaemper%2520and%2520Anu%2520Sinha%2520and%2520Denny%2520Zhou%2520and%2520Yi%2520Sun%2520and%2520Charbel%2520Kaed%2520and%2520Brice%2520Hulse%2520and%2520Swaroop%2520Mishra%2520and%2520Maria%2520Georgaki%2520and%2520Sneha%2520Kudugunta%2520and%2520Clement%2520Farabet%2520and%2520Izhak%2520Shafran%2520and%2520Daniel%2520Vlasic%2520and%2520Anton%2520Tsitsulin%2520and%2520Rajagopal%2520Ananthanarayanan%2520and%2520Alen%2520Carin%2520and%2520Guolong%2520Su%2520and%2520Pei%2520Sun%2520and%2520Shashank%2520V%2520and%2520Gabriel%2520Carvajal%2520and%2520Josef%2520Broder%2520and%2520Iulia%2520Comsa%2520and%2520Alena%2520Repina%2520and%2520William%2520Wong%2520and%2520Warren%2520Weilun%2520Chen%2520and%2520Peter%2520Hawkins%2520and%2520Egor%2520Filonov%2520and%2520Lucia%2520Loher%2520and%2520Christoph%2520Hirnschall%2520and%2520Weiyi%2520Wang%2520and%2520Jingchen%2520Ye%2520and%2520Andrea%2520Burns%2520and%2520Hardie%2520Cate%2520and%2520Diana%2520Gage%2520Wright%2520and%2520Federico%2520Piccinini%2520and%2520Lei%2520Zhang%2520and%2520Chu-Cheng%2520Lin%2520and%2520Ionel%2520Gog%2520and%2520Yana%2520Kulizhskaya%2520and%2520Ashwin%2520Sreevatsa%2520and%2520Shuang%2520Song%2520and%2520Luis%2520C.%2520Cobo%2520and%2520Anand%2520Iyer%2520and%2520Chetan%2520Tekur%2520and%2520Guillermo%2520Garrido%2520and%2520Zhuyun%2520Xiao%2520and%2520Rupert%2520Kemp%2520and%2520Huaixiu%2520Steven%2520Zheng%2520and%2520Hui%2520Li%2520and%2520Ananth%2520Agarwal%2520and%2520Christel%2520Ngani%2520and%2520Kati%2520Goshvadi%2520and%2520Rebeca%2520Santamaria-Fernandez%2520and%2520Wojciech%2520Fica%2520and%2520Xinyun%2520Chen%2520and%2520Chris%2520Gorgolewski%2520and%2520Sean%2520Sun%2520and%2520Roopal%2520Garg%2520and%2520Xinyu%2520Ye%2520and%2520S.%2520M.%2520Ali%2520Eslami%2520and%2520Nan%2520Hua%2520and%2520Jon%2520Simon%2520and%2520Pratik%2520Joshi%2520and%2520Yelin%2520Kim%2520and%2520Ian%2520Tenney%2520and%2520Sahitya%2520Potluri%2520and%2520Lam%2520Nguyen%2520Thiet%2520and%2520Quan%2520Yuan%2520and%2520Florian%2520Luisier%2520and%2520Alexandra%2520Chronopoulou%2520and%2520Salvatore%2520Scellato%2520and%2520Praveen%2520Srinivasan%2520and%2520Minmin%2520Chen%2520and%2520Vinod%2520Koverkathu%2520and%2520Valentin%2520Dalibard%2520and%2520Yaming%2520Xu%2520and%2520Brennan%2520Saeta%2520and%2520Keith%2520Anderson%2520and%2520Thibault%2520Sellam%2520and%2520Nick%2520Fernando%2520and%2520Fantine%2520Huot%2520and%2520Junehyuk%2520Jung%2520and%2520Mani%2520Varadarajan%2520and%2520Michael%2520Quinn%2520and%2520Amit%2520Raul%2520and%2520Maigo%2520Le%2520and%2520Ruslan%2520Habalov%2520and%2520Jon%2520Clark%2520and%2520Komal%2520Jalan%2520and%2520Kalesha%2520Bullard%2520and%2520Achintya%2520Singhal%2520and%2520Thang%2520Luong%2520and%2520Boyu%2520Wang%2520and%2520Sujeevan%2520Rajayogam%2520and%2520Julian%2520Eisenschlos%2520and%2520Johnson%2520Jia%2520and%2520Daniel%2520Finchelstein%2520and%2520Alex%2520Yakubovich%2520and%2520Daniel%2520Balle%2520and%2520Michael%2520Fink%2520and%2520Sameer%2520Agarwal%2520and%2520Jing%2520Li%2520and%2520Dj%2520Dvijotham%2520and%2520Shalini%2520Pal%2520and%2520Kai%2520Kang%2520and%2520Jaclyn%2520Konzelmann%2520and%2520Jennifer%2520Beattie%2520and%2520Olivier%2520Dousse%2520and%2520Diane%2520Wu%2520and%2520Remi%2520Crocker%2520and%2520Chen%2520Elkind%2520and%2520Siddhartha%2520Reddy%2520Jonnalagadda%2520and%2520Jong%2520Lee%2520and%2520Dan%2520Holtmann-Rice%2520and%2520Krystal%2520Kallarackal%2520and%2520Rosanne%2520Liu%2520and%2520Denis%2520Vnukov%2520and%2520Neera%2520Vats%2520and%2520Luca%2520Invernizzi%2520and%2520Mohsen%2520Jafari%2520and%2520Huanjie%2520Zhou%2520and%2520Lilly%2520Taylor%2520and%2520Jennifer%2520Prendki%2520and%2520Marcus%2520Wu%2520and%2520Tom%2520Eccles%2520and%2520Tianqi%2520Liu%2520and%2520Kavya%2520Kopparapu%2520and%2520Francoise%2520Beaufays%2520and%2520Christof%2520Angermueller%2520and%2520Andreea%2520Marzoca%2520and%2520Shourya%2520Sarcar%2520and%2520Hilal%2520Dib%2520and%2520Jeff%2520Stanway%2520and%2520Frank%2520Perbet%2520and%2520Nejc%2520Trdin%2520and%2520Rachel%2520Sterneck%2520and%2520Andrey%2520Khorlin%2520and%2520Dinghua%2520Li%2520and%2520Xihui%2520Wu%2520and%2520Sonam%2520Goenka%2520and%2520David%2520Madras%2520and%2520Sasha%2520Goldshtein%2520and%2520Willi%2520Gierke%2520and%2520Tong%2520Zhou%2520and%2520Yaxin%2520Liu%2520and%2520Yannie%2520Liang%2520and%2520Anais%2520White%2520and%2520Yunjie%2520Li%2520and%2520Shreya%2520Singh%2520and%2520Sanaz%2520Bahargam%2520and%2520Mark%2520Epstein%2520and%2520Sujoy%2520Basu%2520and%2520Li%2520Lao%2520and%2520Adnan%2520Ozturel%2520and%2520Carl%2520Crous%2520and%2520Alex%2520Zhai%2520and%2520Han%2520Lu%2520and%2520Zora%2520Tung%2520and%2520Neeraj%2520Gaur%2520and%2520Alanna%2520Walton%2520and%2520Lucas%2520Dixon%2520and%2520Ming%2520Zhang%2520and%2520Amir%2520Globerson%2520and%2520Grant%2520Uy%2520and%2520Andrew%2520Bolt%2520and%2520Olivia%2520Wiles%2520and%2520Milad%2520Nasr%2520and%2520Ilia%2520Shumailov%2520and%2520Marco%2520Selvi%2520and%2520Francesco%2520Piccinno%2520and%2520Ricardo%2520Aguilar%2520and%2520Sara%2520McCarthy%2520and%2520Misha%2520Khalman%2520and%2520Mrinal%2520Shukla%2520and%2520Vlado%2520Galic%2520and%2520John%2520Carpenter%2520and%2520Kevin%2520Villela%2520and%2520Haibin%2520Zhang%2520and%2520Harry%2520Richardson%2520and%2520James%2520Martens%2520and%2520Matko%2520Bosnjak%2520and%2520Shreyas%2520Rammohan%2520Belle%2520and%2520Jeff%2520Seibert%2520and%2520Mahmoud%2520Alnahlawi%2520and%2520Brian%2520McWilliams%2520and%2520Sankalp%2520Singh%2520and%2520Annie%2520Louis%2520and%2520Wen%2520Ding%2520and%2520Dan%2520Popovici%2520and%2520Lenin%2520Simicich%2520and%2520Laura%2520Knight%2520and%2520Pulkit%2520Mehta%2520and%2520Nishesh%2520Gupta%2520and%2520Chongyang%2520Shi%2520and%2520Saaber%2520Fatehi%2520and%2520Jovana%2520Mitrovic%2520and%2520Alex%2520Grills%2520and%2520Joseph%2520Pagadora%2520and%2520Dessie%2520Petrova%2520and%2520Danielle%2520Eisenbud%2520and%2520Zhishuai%2520Zhang%2520and%2520Damion%2520Yates%2520and%2520Bhavishya%2520Mittal%2520and%2520Nilesh%2520Tripuraneni%2520and%2520Yannis%2520Assael%2520and%2520Thomas%2520Brovelli%2520and%2520Prateek%2520Jain%2520and%2520Mihajlo%2520Velimirovic%2520and%2520Canfer%2520Akbulut%2520and%2520Jiaqi%2520Mu%2520and%2520Wolfgang%2520Macherey%2520and%2520Ravin%2520Kumar%2520and%2520Jun%2520Xu%2520and%2520Haroon%2520Qureshi%2520and%2520Gheorghe%2520Comanici%2520and%2520Jeremy%2520Wiesner%2520and%2520Zhitao%2520Gong%2520and%2520Anton%2520Ruddock%2520and%2520Matthias%2520Bauer%2520and%2520Nick%2520Felt%2520and%2520Anirudh%2520GP%2520and%2520Anurag%2520Arnab%2520and%2520Dustin%2520Zelle%2520and%2520Jonas%2520Rothfuss%2520and%2520Bill%2520Rosgen%2520and%2520Ashish%2520Shenoy%2520and%2520Bryan%2520Seybold%2520and%2520Xinjian%2520Li%2520and%2520Jayaram%2520Mudigonda%2520and%2520Goker%2520Erdogan%2520and%2520Jiawei%2520Xia%2520and%2520Jiri%2520Simsa%2520and%2520Andrea%2520Michi%2520and%2520Yi%2520Yao%2520and%2520Christopher%2520Yew%2520and%2520Steven%2520Kan%2520and%2520Isaac%2520Caswell%2520and%2520Carey%2520Radebaugh%2520and%2520Andre%2520Elisseeff%2520and%2520Pedro%2520Valenzuela%2520and%2520Kay%2520McKinney%2520and%2520Kim%2520Paterson%2520and%2520Albert%2520Cui%2520and%2520Eri%2520Latorre-Chimoto%2520and%2520Solomon%2520Kim%2520and%2520William%2520Zeng%2520and%2520Ken%2520Durden%2520and%2520Priya%2520Ponnapalli%2520and%2520Tiberiu%2520Sosea%2520and%2520Christopher%2520A.%2520Choquette-Choo%2520and%2520James%2520Manyika%2520and%2520Brona%2520Robenek%2520and%2520Harsha%2520Vashisht%2520and%2520Sebastien%2520Pereira%2520and%2520Hoi%2520Lam%2520and%2520Marko%2520Velic%2520and%2520Denese%2520Owusu-Afriyie%2520and%2520Katherine%2520Lee%2520and%2520Tolga%2520Bolukbasi%2520and%2520Alicia%2520Parrish%2520and%2520Shawn%2520Lu%2520and%2520Jane%2520Park%2520and%2520Balaji%2520Venkatraman%2520and%2520Alice%2520Talbert%2520and%2520Lambert%2520Rosique%2520and%2520Yuchung%2520Cheng%2520and%2520Andrei%2520Sozanschi%2520and%2520Adam%2520Paszke%2520and%2520Praveen%2520Kumar%2520and%2520Jessica%2520Austin%2520and%2520Lu%2520Li%2520and%2520Khalid%2520Salama%2520and%2520Wooyeol%2520Kim%2520and%2520Nandita%2520Dukkipati%2520and%2520Anthony%2520Baryshnikov%2520and%2520Christos%2520Kaplanis%2520and%2520XiangHai%2520Sheng%2520and%2520Yuri%2520Chervonyi%2520and%2520Caglar%2520Unlu%2520and%2520Diego%2520de%2520Las%2520Casas%2520and%2520Harry%2520Askham%2520and%2520Kathryn%2520Tunyasuvunakool%2520and%2520Felix%2520Gimeno%2520and%2520Siim%2520Poder%2520and%2520Chester%2520Kwak%2520and%2520Matt%2520Miecnikowski%2520and%2520Vahab%2520Mirrokni%2520and%2520Alek%2520Dimitriev%2520and%2520Aaron%2520Parisi%2520and%2520Dangyi%2520Liu%2520and%2520Tomy%2520Tsai%2520and%2520Toby%2520Shevlane%2520and%2520Christina%2520Kouridi%2520and%2520Drew%2520Garmon%2520and%2520Adrian%2520Goedeckemeyer%2520and%2520Adam%2520R.%2520Brown%2520and%2520Anitha%2520Vijayakumar%2520and%2520Ali%2520Elqursh%2520and%2520Sadegh%2520Jazayeri%2520and%2520Jin%2520Huang%2520and%2520Sara%2520Mc%2520Carthy%2520and%2520Jay%2520Hoover%2520and%2520Lucy%2520Kim%2520and%2520Sandeep%2520Kumar%2520and%2520Wei%2520Chen%2520and%2520Courtney%2520Biles%2520and%2520Garrett%2520Bingham%2520and%2520Evan%2520Rosen%2520and%2520Lisa%2520Wang%2520and%2520Qijun%2520Tan%2520and%2520David%2520Engel%2520and%2520Francesco%2520Pongetti%2520and%2520Dario%2520de%2520Cesare%2520and%2520Dongseong%2520Hwang%2520and%2520Lily%2520Yu%2520and%2520Jennifer%2520Pullman%2520and%2520Srini%2520Narayanan%2520and%2520Kyle%2520Levin%2520and%2520Siddharth%2520Gopal%2520and%2520Megan%2520Li%2520and%2520Asaf%2520Aharoni%2520and%2520Trieu%2520Trinh%2520and%2520Jessica%2520Lo%2520and%2520Norman%2520Casagrande%2520and%2520Roopali%2520Vij%2520and%2520Loic%2520Matthey%2520and%2520Bramandia%2520Ramadhana%2520and%2520Austin%2520Matthews%2520and%2520CJ%2520Carey%2520and%2520Matthew%2520Johnson%2520and%2520Kremena%2520Goranova%2520and%2520Rohin%2520Shah%2520and%2520Shereen%2520Ashraf%2520and%2520Kingshuk%2520Dasgupta%2520and%2520Rasmus%2520Larsen%2520and%2520Yicheng%2520Wang%2520and%2520Manish%2520Reddy%2520Vuyyuru%2520and%2520Chong%2520Jiang%2520and%2520Joana%2520Ijazi%2520and%2520Kazuki%2520Osawa%2520and%2520Celine%2520Smith%2520and%2520Ramya%2520Sree%2520Boppana%2520and%2520Taylan%2520Bilal%2520and%2520Yuma%2520Koizumi%2520and%2520Ying%2520Xu%2520and%2520Yasemin%2520Altun%2520and%2520Nir%2520Shabat%2520and%2520Ben%2520Bariach%2520and%2520Alex%2520Korchemniy%2520and%2520Kiam%2520Choo%2520and%2520Olaf%2520Ronneberger%2520and%2520Chimezie%2520Iwuanyanwu%2520and%2520Shubin%2520Zhao%2520and%2520David%2520Soergel%2520and%2520Cho-Jui%2520Hsieh%2520and%2520Irene%2520Cai%2520and%2520Shariq%2520Iqbal%2520and%2520Martin%2520Sundermeyer%2520and%2520Zhe%2520Chen%2520and%2520Elie%2520Bursztein%2520and%2520Chaitanya%2520Malaviya%2520and%2520Fadi%2520Biadsy%2520and%2520Prakash%2520Shroff%2520and%2520Inderjit%2520Dhillon%2520and%2520Tejasi%2520Latkar%2520and%2520Chris%2520Dyer%2520and%2520Hannah%2520Forbes%2520and%2520Massimo%2520Nicosia%2520and%2520Vitaly%2520Nikolaev%2520and%2520Somer%2520Greene%2520and%2520Marin%2520Georgiev%2520and%2520Pidong%2520Wang%2520and%2520Nina%2520Martin%2520and%2520Hanie%2520Sedghi%2520and%2520John%2520Zhang%2520and%2520Praseem%2520Banzal%2520and%2520Doug%2520Fritz%2520and%2520Vikram%2520Rao%2520and%2520Xuezhi%2520Wang%2520and%2520Jiageng%2520Zhang%2520and%2520Viorica%2520Patraucean%2520and%2520Dayou%2520Du%2520and%2520Igor%2520Mordatch%2520and%2520Ivan%2520Jurin%2520and%2520Lewis%2520Liu%2520and%2520Ayush%2520Dubey%2520and%2520Abhi%2520Mohan%2520and%2520Janek%2520Nowakowski%2520and%2520Vlad-Doru%2520Ion%2520and%2520Nan%2520Wei%2520and%2520Reiko%2520Tojo%2520and%2520Maria%2520Abi%2520Raad%2520and%2520Drew%2520A.%2520Hudson%2520and%2520Vaishakh%2520Keshava%2520and%2520Shubham%2520Agrawal%2520and%2520Kevin%2520Ramirez%2520and%2520Zhichun%2520Wu%2520and%2520Hoang%2520Nguyen%2520and%2520Ji%2520Liu%2520and%2520Madhavi%2520Sewak%2520and%2520Bryce%2520Petrini%2520and%2520DongHyun%2520Choi%2520and%2520Ivan%2520Philips%2520and%2520Ziyue%2520Wang%2520and%2520Ioana%2520Bica%2520and%2520Ankush%2520Garg%2520and%2520Jarek%2520Wilkiewicz%2520and%2520Priyanka%2520Agrawal%2520and%2520Xiaowei%2520Li%2520and%2520Danhao%2520Guo%2520and%2520Emily%2520Xue%2520and%2520Naseer%2520Shaik%2520and%2520Andrew%2520Leach%2520and%2520Sadh%2520MNM%2520Khan%2520and%2520Julia%2520Wiesinger%2520and%2520Sammy%2520Jerome%2520and%2520Abhishek%2520Chakladar%2520and%2520Alek%2520Wenjiao%2520Wang%2520and%2520Tina%2520Ornduff%2520and%2520Folake%2520Abu%2520and%2520Alireza%2520Ghaffarkhah%2520and%2520Marcus%2520Wainwright%2520and%2520Mario%2520Cortes%2520and%2520Frederick%2520Liu%2520and%2520Joshua%2520Maynez%2520and%2520Slav%2520Petrov%2520and%2520Yonghui%2520Wu%2520and%2520Demis%2520Hassabis%2520and%2520Koray%2520Kavukcuoglu%2520and%2520Jeffrey%2520Dean%2520and%2520Oriol%2520Vinyals%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520introduce%2520the%2520Gemini%25201.5%2520family%2520of%2520models%252C%2520representing%250Athe%2520next%2520generation%2520of%2520highly%2520compute-efficient%2520multimodal%2520models%2520capable%2520of%250Arecalling%2520and%2520reasoning%2520over%2520fine-grained%2520information%2520from%2520millions%2520of%2520tokens%250Aof%2520context%252C%2520including%2520multiple%2520long%2520documents%2520and%2520hours%2520of%2520video%2520and%2520audio.%2520The%250Afamily%2520includes%2520two%2520new%2520models%253A%2520%25281%2529%2520an%2520updated%2520Gemini%25201.5%2520Pro%252C%2520which%2520exceeds%250Athe%2520February%2520version%2520on%2520the%2520great%2520majority%2520of%2520capabilities%2520and%2520benchmarks%253B%2520%25282%2529%250AGemini%25201.5%2520Flash%252C%2520a%2520more%2520lightweight%2520variant%2520designed%2520for%2520efficiency%2520with%250Aminimal%2520regression%2520in%2520quality.%2520Gemini%25201.5%2520models%2520achieve%2520near-perfect%2520recall%2520on%250Along-context%2520retrieval%2520tasks%2520across%2520modalities%252C%2520improve%2520the%2520state-of-the-art%2520in%250Along-document%2520QA%252C%2520long-video%2520QA%2520and%2520long-context%2520ASR%252C%2520and%2520match%2520or%2520surpass%250AGemini%25201.0%2520Ultra%2527s%2520state-of-the-art%2520performance%2520across%2520a%2520broad%2520set%2520of%250Abenchmarks.%2520Studying%2520the%2520limits%2520of%2520Gemini%25201.5%2527s%2520long-context%2520ability%252C%2520we%2520find%250Acontinued%2520improvement%2520in%2520next-token%2520prediction%2520and%2520near-perfect%2520retrieval%250A%2528%253E99%2525%2529%2520up%2520to%2520at%2520least%252010M%2520tokens%252C%2520a%2520generational%2520leap%2520over%2520existing%2520models%2520such%250Aas%2520Claude%25203.0%2520%2528200k%2529%2520and%2520GPT-4%2520Turbo%2520%2528128k%2529.%2520Finally%252C%2520we%2520highlight%2520real-world%250Ause%2520cases%252C%2520such%2520as%2520Gemini%25201.5%2520collaborating%2520with%2520professionals%2520on%2520completing%250Atheir%2520tasks%2520achieving%252026%2520to%252075%2525%2520time%2520savings%2520across%252010%2520different%2520job%250Acategories%252C%2520as%2520well%2520as%2520surprising%2520new%2520capabilities%2520of%2520large%2520language%2520models%2520at%250Athe%2520frontier%253B%2520when%2520given%2520a%2520grammar%2520manual%2520for%2520Kalamang%252C%2520a%2520language%2520with%2520fewer%250Athan%2520200%2520speakers%2520worldwide%252C%2520the%2520model%2520learns%2520to%2520translate%2520English%2520to%2520Kalamang%250Aat%2520a%2520similar%2520level%2520to%2520a%2520person%2520who%2520learned%2520from%2520the%2520same%2520content.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05530v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gemini%201.5%3A%20Unlocking%20multimodal%20understanding%20across%20millions%20of%20tokens%0A%20%20of%20context&entry.906535625=%20Gemini%20Team%20and%20Petko%20Georgiev%20and%20Ving%20Ian%20Lei%20and%20Ryan%20Burnell%20and%20Libin%20Bai%20and%20Anmol%20Gulati%20and%20Garrett%20Tanzer%20and%20Damien%20Vincent%20and%20Zhufeng%20Pan%20and%20Shibo%20Wang%20and%20Soroosh%20Mariooryad%20and%20Yifan%20Ding%20and%20Xinyang%20Geng%20and%20Fred%20Alcober%20and%20Roy%20Frostig%20and%20Mark%20Omernick%20and%20Lexi%20Walker%20and%20Cosmin%20Paduraru%20and%20Christina%20Sorokin%20and%20Andrea%20Tacchetti%20and%20Colin%20Gaffney%20and%20Samira%20Daruki%20and%20Olcan%20Sercinoglu%20and%20Zach%20Gleicher%20and%20Juliette%20Love%20and%20Paul%20Voigtlaender%20and%20Rohan%20Jain%20and%20Gabriela%20Surita%20and%20Kareem%20Mohamed%20and%20Rory%20Blevins%20and%20Junwhan%20Ahn%20and%20Tao%20Zhu%20and%20Kornraphop%20Kawintiranon%20and%20Orhan%20Firat%20and%20Yiming%20Gu%20and%20Yujing%20Zhang%20and%20Matthew%20Rahtz%20and%20Manaal%20Faruqui%20and%20Natalie%20Clay%20and%20Justin%20Gilmer%20and%20JD%20Co-Reyes%20and%20Ivo%20Penchev%20and%20Rui%20Zhu%20and%20Nobuyuki%20Morioka%20and%20Kevin%20Hui%20and%20Krishna%20Haridasan%20and%20Victor%20Campos%20and%20Mahdis%20Mahdieh%20and%20Mandy%20Guo%20and%20Samer%20Hassan%20and%20Kevin%20Kilgour%20and%20Arpi%20Vezer%20and%20Heng-Tze%20Cheng%20and%20Raoul%20de%20Liedekerke%20and%20Siddharth%20Goyal%20and%20Paul%20Barham%20and%20DJ%20Strouse%20and%20Seb%20Noury%20and%20Jonas%20Adler%20and%20Mukund%20Sundararajan%20and%20Sharad%20Vikram%20and%20Dmitry%20Lepikhin%20and%20Michela%20Paganini%20and%20Xavier%20Garcia%20and%20Fan%20Yang%20and%20Dasha%20Valter%20and%20Maja%20Trebacz%20and%20Kiran%20Vodrahalli%20and%20Chulayuth%20Asawaroengchai%20and%20Roman%20Ring%20and%20Norbert%20Kalb%20and%20Livio%20Baldini%20Soares%20and%20Siddhartha%20Brahma%20and%20David%20Steiner%20and%20Tianhe%20Yu%20and%20Fabian%20Mentzer%20and%20Antoine%20He%20and%20Lucas%20Gonzalez%20and%20Bibo%20Xu%20and%20Raphael%20Lopez%20Kaufman%20and%20Laurent%20El%20Shafey%20and%20Junhyuk%20Oh%20and%20Tom%20Hennigan%20and%20George%20van%20den%20Driessche%20and%20Seth%20Odoom%20and%20Mario%20Lucic%20and%20Becca%20Roelofs%20and%20Sid%20Lall%20and%20Amit%20Marathe%20and%20Betty%20Chan%20and%20Santiago%20Ontanon%20and%20Luheng%20He%20and%20Denis%20Teplyashin%20and%20Jonathan%20Lai%20and%20Phil%20Crone%20and%20Bogdan%20Damoc%20and%20Lewis%20Ho%20and%20Sebastian%20Riedel%20and%20Karel%20Lenc%20and%20Chih-Kuan%20Yeh%20and%20Aakanksha%20Chowdhery%20and%20Yang%20Xu%20and%20Mehran%20Kazemi%20and%20Ehsan%20Amid%20and%20Anastasia%20Petrushkina%20and%20Kevin%20Swersky%20and%20Ali%20Khodaei%20and%20Gowoon%20Chen%20and%20Chris%20Larkin%20and%20Mario%20Pinto%20and%20Geng%20Yan%20and%20Adria%20Puigdomenech%20Badia%20and%20Piyush%20Patil%20and%20Steven%20Hansen%20and%20Dave%20Orr%20and%20Sebastien%20M.%20R.%20Arnold%20and%20Jordan%20Grimstad%20and%20Andrew%20Dai%20and%20Sholto%20Douglas%20and%20Rishika%20Sinha%20and%20Vikas%20Yadav%20and%20Xi%20Chen%20and%20Elena%20Gribovskaya%20and%20Jacob%20Austin%20and%20Jeffrey%20Zhao%20and%20Kaushal%20Patel%20and%20Paul%20Komarek%20and%20Sophia%20Austin%20and%20Sebastian%20Borgeaud%20and%20Linda%20Friso%20and%20Abhimanyu%20Goyal%20and%20Ben%20Caine%20and%20Kris%20Cao%20and%20Da-Woon%20Chung%20and%20Matthew%20Lamm%20and%20Gabe%20Barth-Maron%20and%20Thais%20Kagohara%20and%20Kate%20Olszewska%20and%20Mia%20Chen%20and%20Kaushik%20Shivakumar%20and%20Rishabh%20Agarwal%20and%20Harshal%20Godhia%20and%20Ravi%20Rajwar%20and%20Javier%20Snaider%20and%20Xerxes%20Dotiwalla%20and%20Yuan%20Liu%20and%20Aditya%20Barua%20and%20Victor%20Ungureanu%20and%20Yuan%20Zhang%20and%20Bat-Orgil%20Batsaikhan%20and%20Mateo%20Wirth%20and%20James%20Qin%20and%20Ivo%20Danihelka%20and%20Tulsee%20Doshi%20and%20Martin%20Chadwick%20and%20Jilin%20Chen%20and%20Sanil%20Jain%20and%20Quoc%20Le%20and%20Arjun%20Kar%20and%20Madhu%20Gurumurthy%20and%20Cheng%20Li%20and%20Ruoxin%20Sang%20and%20Fangyu%20Liu%20and%20Lampros%20Lamprou%20and%20Rich%20Munoz%20and%20Nathan%20Lintz%20and%20Harsh%20Mehta%20and%20Heidi%20Howard%20and%20Malcolm%20Reynolds%20and%20Lora%20Aroyo%20and%20Quan%20Wang%20and%20Lorenzo%20Blanco%20and%20Albin%20Cassirer%20and%20Jordan%20Griffith%20and%20Dipanjan%20Das%20and%20Stephan%20Lee%20and%20Jakub%20Sygnowski%20and%20Zach%20Fisher%20and%20James%20Besley%20and%20Richard%20Powell%20and%20Zafarali%20Ahmed%20and%20Dominik%20Paulus%20and%20David%20Reitter%20and%20Zalan%20Borsos%20and%20Rishabh%20Joshi%20and%20Aedan%20Pope%20and%20Steven%20Hand%20and%20Vittorio%20Selo%20and%20Vihan%20Jain%20and%20Nikhil%20Sethi%20and%20Megha%20Goel%20and%20Takaki%20Makino%20and%20Rhys%20May%20and%20Zhen%20Yang%20and%20Johan%20Schalkwyk%20and%20Christina%20Butterfield%20and%20Anja%20Hauth%20and%20Alex%20Goldin%20and%20Will%20Hawkins%20and%20Evan%20Senter%20and%20Sergey%20Brin%20and%20Oliver%20Woodman%20and%20Marvin%20Ritter%20and%20Eric%20Noland%20and%20Minh%20Giang%20and%20Vijay%20Bolina%20and%20Lisa%20Lee%20and%20Tim%20Blyth%20and%20Ian%20Mackinnon%20and%20Machel%20Reid%20and%20Obaid%20Sarvana%20and%20David%20Silver%20and%20Alexander%20Chen%20and%20Lily%20Wang%20and%20Loren%20Maggiore%20and%20Oscar%20Chang%20and%20Nithya%20Attaluri%20and%20Gregory%20Thornton%20and%20Chung-Cheng%20Chiu%20and%20Oskar%20Bunyan%20and%20Nir%20Levine%20and%20Timothy%20Chung%20and%20Evgenii%20Eltyshev%20and%20Xiance%20Si%20and%20Timothy%20Lillicrap%20and%20Demetra%20Brady%20and%20Vaibhav%20Aggarwal%20and%20Boxi%20Wu%20and%20Yuanzhong%20Xu%20and%20Ross%20McIlroy%20and%20Kartikeya%20Badola%20and%20Paramjit%20Sandhu%20and%20Erica%20Moreira%20and%20Wojciech%20Stokowiec%20and%20Ross%20Hemsley%20and%20Dong%20Li%20and%20Alex%20Tudor%20and%20Pranav%20Shyam%20and%20Elahe%20Rahimtoroghi%20and%20Salem%20Haykal%20and%20Pablo%20Sprechmann%20and%20Xiang%20Zhou%20and%20Diana%20Mincu%20and%20Yujia%20Li%20and%20Ravi%20Addanki%20and%20Kalpesh%20Krishna%20and%20Xiao%20Wu%20and%20Alexandre%20Frechette%20and%20Matan%20Eyal%20and%20Allan%20Dafoe%20and%20Dave%20Lacey%20and%20Jay%20Whang%20and%20Thi%20Avrahami%20and%20Ye%20Zhang%20and%20Emanuel%20Taropa%20and%20Hanzhao%20Lin%20and%20Daniel%20Toyama%20and%20Eliza%20Rutherford%20and%20Motoki%20Sano%20and%20HyunJeong%20Choe%20and%20Alex%20Tomala%20and%20Chalence%20Safranek-Shrader%20and%20Nora%20Kassner%20and%20Mantas%20Pajarskas%20and%20Matt%20Harvey%20and%20Sean%20Sechrist%20and%20Meire%20Fortunato%20and%20Christina%20Lyu%20and%20Gamaleldin%20Elsayed%20and%20Chenkai%20Kuang%20and%20James%20Lottes%20and%20Eric%20Chu%20and%20Chao%20Jia%20and%20Chih-Wei%20Chen%20and%20Peter%20Humphreys%20and%20Kate%20Baumli%20and%20Connie%20Tao%20and%20Rajkumar%20Samuel%20and%20Cicero%20Nogueira%20dos%20Santos%20and%20Anders%20Andreassen%20and%20Nemanja%20Raki%C4%87evi%C4%87%20and%20Dominik%20Grewe%20and%20Aviral%20Kumar%20and%20Stephanie%20Winkler%20and%20Jonathan%20Caton%20and%20Andrew%20Brock%20and%20Sid%20Dalmia%20and%20Hannah%20Sheahan%20and%20Iain%20Barr%20and%20Yingjie%20Miao%20and%20Paul%20Natsev%20and%20Jacob%20Devlin%20and%20Feryal%20Behbahani%20and%20Flavien%20Prost%20and%20Yanhua%20Sun%20and%20Artiom%20Myaskovsky%20and%20Thanumalayan%20Sankaranarayana%20Pillai%20and%20Dan%20Hurt%20and%20Angeliki%20Lazaridou%20and%20Xi%20Xiong%20and%20Ce%20Zheng%20and%20Fabio%20Pardo%20and%20Xiaowei%20Li%20and%20Dan%20Horgan%20and%20Joe%20Stanton%20and%20Moran%20Ambar%20and%20Fei%20Xia%20and%20Alejandro%20Lince%20and%20Mingqiu%20Wang%20and%20Basil%20Mustafa%20and%20Albert%20Webson%20and%20Hyo%20Lee%20and%20Rohan%20Anil%20and%20Martin%20Wicke%20and%20Timothy%20Dozat%20and%20Abhishek%20Sinha%20and%20Enrique%20Piqueras%20and%20Elahe%20Dabir%20and%20Shyam%20Upadhyay%20and%20Anudhyan%20Boral%20and%20Lisa%20Anne%20Hendricks%20and%20Corey%20Fry%20and%20Josip%20Djolonga%20and%20Yi%20Su%20and%20Jake%20Walker%20and%20Jane%20Labanowski%20and%20Ronny%20Huang%20and%20Vedant%20Misra%20and%20Jeremy%20Chen%20and%20RJ%20Skerry-Ryan%20and%20Avi%20Singh%20and%20Shruti%20Rijhwani%20and%20Dian%20Yu%20and%20Alex%20Castro-Ros%20and%20Beer%20Changpinyo%20and%20Romina%20Datta%20and%20Sumit%20Bagri%20and%20Arnar%20Mar%20Hrafnkelsson%20and%20Marcello%20Maggioni%20and%20Daniel%20Zheng%20and%20Yury%20Sulsky%20and%20Shaobo%20Hou%20and%20Tom%20Le%20Paine%20and%20Antoine%20Yang%20and%20Jason%20Riesa%20and%20Dominika%20Rogozinska%20and%20Dror%20Marcus%20and%20Dalia%20El%20Badawy%20and%20Qiao%20Zhang%20and%20Luyu%20Wang%20and%20Helen%20Miller%20and%20Jeremy%20Greer%20and%20Lars%20Lowe%20Sjos%20and%20Azade%20Nova%20and%20Heiga%20Zen%20and%20Rahma%20Chaabouni%20and%20Mihaela%20Rosca%20and%20Jiepu%20Jiang%20and%20Charlie%20Chen%20and%20Ruibo%20Liu%20and%20Tara%20Sainath%20and%20Maxim%20Krikun%20and%20Alex%20Polozov%20and%20Jean-Baptiste%20Lespiau%20and%20Josh%20Newlan%20and%20Zeyncep%20Cankara%20and%20Soo%20Kwak%20and%20Yunhan%20Xu%20and%20Phil%20Chen%20and%20Andy%20Coenen%20and%20Clemens%20Meyer%20and%20Katerina%20Tsihlas%20and%20Ada%20Ma%20and%20Juraj%20Gottweis%20and%20Jinwei%20Xing%20and%20Chenjie%20Gu%20and%20Jin%20Miao%20and%20Christian%20Frank%20and%20Zeynep%20Cankara%20and%20Sanjay%20Ganapathy%20and%20Ishita%20Dasgupta%20and%20Steph%20Hughes-Fitt%20and%20Heng%20Chen%20and%20David%20Reid%20and%20Keran%20Rong%20and%20Hongmin%20Fan%20and%20Joost%20van%20Amersfoort%20and%20Vincent%20Zhuang%20and%20Aaron%20Cohen%20and%20Shixiang%20Shane%20Gu%20and%20Anhad%20Mohananey%20and%20Anastasija%20Ilic%20and%20Taylor%20Tobin%20and%20John%20Wieting%20and%20Anna%20Bortsova%20and%20Phoebe%20Thacker%20and%20Emma%20Wang%20and%20Emily%20Caveness%20and%20Justin%20Chiu%20and%20Eren%20Sezener%20and%20Alex%20Kaskasoli%20and%20Steven%20Baker%20and%20Katie%20Millican%20and%20Mohamed%20Elhawaty%20and%20Kostas%20Aisopos%20and%20Carl%20Lebsack%20and%20Nathan%20Byrd%20and%20Hanjun%20Dai%20and%20Wenhao%20Jia%20and%20Matthew%20Wiethoff%20and%20Elnaz%20Davoodi%20and%20Albert%20Weston%20and%20Lakshman%20Yagati%20and%20Arun%20Ahuja%20and%20Isabel%20Gao%20and%20Golan%20Pundak%20and%20Susan%20Zhang%20and%20Michael%20Azzam%20and%20Khe%20Chai%20Sim%20and%20Sergi%20Caelles%20and%20James%20Keeling%20and%20Abhanshu%20Sharma%20and%20Andy%20Swing%20and%20YaGuang%20Li%20and%20Chenxi%20Liu%20and%20Carrie%20Grimes%20Bostock%20and%20Yamini%20Bansal%20and%20Zachary%20Nado%20and%20Ankesh%20Anand%20and%20Josh%20Lipschultz%20and%20Abhijit%20Karmarkar%20and%20Lev%20Proleev%20and%20Abe%20Ittycheriah%20and%20Soheil%20Hassas%20Yeganeh%20and%20George%20Polovets%20and%20Aleksandra%20Faust%20and%20Jiao%20Sun%20and%20Alban%20Rrustemi%20and%20Pen%20Li%20and%20Rakesh%20Shivanna%20and%20Jeremiah%20Liu%20and%20Chris%20Welty%20and%20Federico%20Lebron%20and%20Anirudh%20Baddepudi%20and%20Sebastian%20Krause%20and%20Emilio%20Parisotto%20and%20Radu%20Soricut%20and%20Zheng%20Xu%20and%20Dawn%20Bloxwich%20and%20Melvin%20Johnson%20and%20Behnam%20Neyshabur%20and%20Justin%20Mao-Jones%20and%20Renshen%20Wang%20and%20Vinay%20Ramasesh%20and%20Zaheer%20Abbas%20and%20Arthur%20Guez%20and%20Constant%20Segal%20and%20Duc%20Dung%20Nguyen%20and%20James%20Svensson%20and%20Le%20Hou%20and%20Sarah%20York%20and%20Kieran%20Milan%20and%20Sophie%20Bridgers%20and%20Wiktor%20Gworek%20and%20Marco%20Tagliasacchi%20and%20James%20Lee-Thorp%20and%20Michael%20Chang%20and%20Alexey%20Guseynov%20and%20Ale%20Jakse%20Hartman%20and%20Michael%20Kwong%20and%20Ruizhe%20Zhao%20and%20Sheleem%20Kashem%20and%20Elizabeth%20Cole%20and%20Antoine%20Miech%20and%20Richard%20Tanburn%20and%20Mary%20Phuong%20and%20Filip%20Pavetic%20and%20Sebastien%20Cevey%20and%20Ramona%20Comanescu%20and%20Richard%20Ives%20and%20Sherry%20Yang%20and%20Cosmo%20Du%20and%20Bo%20Li%20and%20Zizhao%20Zhang%20and%20Mariko%20Iinuma%20and%20Clara%20Huiyi%20Hu%20and%20Aurko%20Roy%20and%20Shaan%20Bijwadia%20and%20Zhenkai%20Zhu%20and%20Danilo%20Martins%20and%20Rachel%20Saputro%20and%20Anita%20Gergely%20and%20Steven%20Zheng%20and%20Dawei%20Jia%20and%20Ioannis%20Antonoglou%20and%20Adam%20Sadovsky%20and%20Shane%20Gu%20and%20Yingying%20Bi%20and%20Alek%20Andreev%20and%20Sina%20Samangooei%20and%20Mina%20Khan%20and%20Tomas%20Kocisky%20and%20Angelos%20Filos%20and%20Chintu%20Kumar%20and%20Colton%20Bishop%20and%20Adams%20Yu%20and%20Sarah%20Hodkinson%20and%20Sid%20Mittal%20and%20Premal%20Shah%20and%20Alexandre%20Moufarek%20and%20Yong%20Cheng%20and%20Adam%20Bloniarz%20and%20Jaehoon%20Lee%20and%20Pedram%20Pejman%20and%20Paul%20Michel%20and%20Stephen%20Spencer%20and%20Vladimir%20Feinberg%20and%20Xuehan%20Xiong%20and%20Nikolay%20Savinov%20and%20Charlotte%20Smith%20and%20Siamak%20Shakeri%20and%20Dustin%20Tran%20and%20Mary%20Chesus%20and%20Bernd%20Bohnet%20and%20George%20Tucker%20and%20Tamara%20von%20Glehn%20and%20Carrie%20Muir%20and%20Yiran%20Mao%20and%20Hideto%20Kazawa%20and%20Ambrose%20Slone%20and%20Kedar%20Soparkar%20and%20Disha%20Shrivastava%20and%20James%20Cobon-Kerr%20and%20Michael%20Sharman%20and%20Jay%20Pavagadhi%20and%20Carlos%20Araya%20and%20Karolis%20Misiunas%20and%20Nimesh%20Ghelani%20and%20Michael%20Laskin%20and%20David%20Barker%20and%20Qiujia%20Li%20and%20Anton%20Briukhov%20and%20Neil%20Houlsby%20and%20Mia%20Glaese%20and%20Balaji%20Lakshminarayanan%20and%20Nathan%20Schucher%20and%20Yunhao%20Tang%20and%20Eli%20Collins%20and%20Hyeontaek%20Lim%20and%20Fangxiaoyu%20Feng%20and%20Adria%20Recasens%20and%20Guangda%20Lai%20and%20Alberto%20Magni%20and%20Nicola%20De%20Cao%20and%20Aditya%20Siddhant%20and%20Zoe%20Ashwood%20and%20Jordi%20Orbay%20and%20Mostafa%20Dehghani%20and%20Jenny%20Brennan%20and%20Yifan%20He%20and%20Kelvin%20Xu%20and%20Yang%20Gao%20and%20Carl%20Saroufim%20and%20James%20Molloy%20and%20Xinyi%20Wu%20and%20Seb%20Arnold%20and%20Solomon%20Chang%20and%20Julian%20Schrittwieser%20and%20Elena%20Buchatskaya%20and%20Soroush%20Radpour%20and%20Martin%20Polacek%20and%20Skye%20Giordano%20and%20Ankur%20Bapna%20and%20Simon%20Tokumine%20and%20Vincent%20Hellendoorn%20and%20Thibault%20Sottiaux%20and%20Sarah%20Cogan%20and%20Aliaksei%20Severyn%20and%20Mohammad%20Saleh%20and%20Shantanu%20Thakoor%20and%20Laurent%20Shefey%20and%20Siyuan%20Qiao%20and%20Meenu%20Gaba%20and%20Shuo-yiin%20Chang%20and%20Craig%20Swanson%20and%20Biao%20Zhang%20and%20Benjamin%20Lee%20and%20Paul%20Kishan%20Rubenstein%20and%20Gan%20Song%20and%20Tom%20Kwiatkowski%20and%20Anna%20Koop%20and%20Ajay%20Kannan%20and%20David%20Kao%20and%20Parker%20Schuh%20and%20Axel%20Stjerngren%20and%20Golnaz%20Ghiasi%20and%20Gena%20Gibson%20and%20Luke%20Vilnis%20and%20Ye%20Yuan%20and%20Felipe%20Tiengo%20Ferreira%20and%20Aishwarya%20Kamath%20and%20Ted%20Klimenko%20and%20Ken%20Franko%20and%20Kefan%20Xiao%20and%20Indro%20Bhattacharya%20and%20Miteyan%20Patel%20and%20Rui%20Wang%20and%20Alex%20Morris%20and%20Robin%20Strudel%20and%20Vivek%20Sharma%20and%20Peter%20Choy%20and%20Sayed%20Hadi%20Hashemi%20and%20Jessica%20Landon%20and%20Mara%20Finkelstein%20and%20Priya%20Jhakra%20and%20Justin%20Frye%20and%20Megan%20Barnes%20and%20Matthew%20Mauger%20and%20Dennis%20Daun%20and%20Khuslen%20Baatarsukh%20and%20Matthew%20Tung%20and%20Wael%20Farhan%20and%20Henryk%20Michalewski%20and%20Fabio%20Viola%20and%20Felix%20de%20Chaumont%20Quitry%20and%20Charline%20Le%20Lan%20and%20Tom%20Hudson%20and%20Qingze%20Wang%20and%20Felix%20Fischer%20and%20Ivy%20Zheng%20and%20Elspeth%20White%20and%20Anca%20Dragan%20and%20Jean-baptiste%20Alayrac%20and%20Eric%20Ni%20and%20Alexander%20Pritzel%20and%20Adam%20Iwanicki%20and%20Michael%20Isard%20and%20Anna%20Bulanova%20and%20Lukas%20Zilka%20and%20Ethan%20Dyer%20and%20Devendra%20Sachan%20and%20Srivatsan%20Srinivasan%20and%20Hannah%20Muckenhirn%20and%20Honglong%20Cai%20and%20Amol%20Mandhane%20and%20Mukarram%20Tariq%20and%20Jack%20W.%20Rae%20and%20Gary%20Wang%20and%20Kareem%20Ayoub%20and%20Nicholas%20FitzGerald%20and%20Yao%20Zhao%20and%20Woohyun%20Han%20and%20Chris%20Alberti%20and%20Dan%20Garrette%20and%20Kashyap%20Krishnakumar%20and%20Mai%20Gimenez%20and%20Anselm%20Levskaya%20and%20Daniel%20Sohn%20and%20Josip%20Matak%20and%20Inaki%20Iturrate%20and%20Michael%20B.%20Chang%20and%20Jackie%20Xiang%20and%20Yuan%20Cao%20and%20Nishant%20Ranka%20and%20Geoff%20Brown%20and%20Adrian%20Hutter%20and%20Vahab%20Mirrokni%20and%20Nanxin%20Chen%20and%20Kaisheng%20Yao%20and%20Zoltan%20Egyed%20and%20Francois%20Galilee%20and%20Tyler%20Liechty%20and%20Praveen%20Kallakuri%20and%20Evan%20Palmer%20and%20Sanjay%20Ghemawat%20and%20Jasmine%20Liu%20and%20David%20Tao%20and%20Chloe%20Thornton%20and%20Tim%20Green%20and%20Mimi%20Jasarevic%20and%20Sharon%20Lin%20and%20Victor%20Cotruta%20and%20Yi-Xuan%20Tan%20and%20Noah%20Fiedel%20and%20Hongkun%20Yu%20and%20Ed%20Chi%20and%20Alexander%20Neitz%20and%20Jens%20Heitkaemper%20and%20Anu%20Sinha%20and%20Denny%20Zhou%20and%20Yi%20Sun%20and%20Charbel%20Kaed%20and%20Brice%20Hulse%20and%20Swaroop%20Mishra%20and%20Maria%20Georgaki%20and%20Sneha%20Kudugunta%20and%20Clement%20Farabet%20and%20Izhak%20Shafran%20and%20Daniel%20Vlasic%20and%20Anton%20Tsitsulin%20and%20Rajagopal%20Ananthanarayanan%20and%20Alen%20Carin%20and%20Guolong%20Su%20and%20Pei%20Sun%20and%20Shashank%20V%20and%20Gabriel%20Carvajal%20and%20Josef%20Broder%20and%20Iulia%20Comsa%20and%20Alena%20Repina%20and%20William%20Wong%20and%20Warren%20Weilun%20Chen%20and%20Peter%20Hawkins%20and%20Egor%20Filonov%20and%20Lucia%20Loher%20and%20Christoph%20Hirnschall%20and%20Weiyi%20Wang%20and%20Jingchen%20Ye%20and%20Andrea%20Burns%20and%20Hardie%20Cate%20and%20Diana%20Gage%20Wright%20and%20Federico%20Piccinini%20and%20Lei%20Zhang%20and%20Chu-Cheng%20Lin%20and%20Ionel%20Gog%20and%20Yana%20Kulizhskaya%20and%20Ashwin%20Sreevatsa%20and%20Shuang%20Song%20and%20Luis%20C.%20Cobo%20and%20Anand%20Iyer%20and%20Chetan%20Tekur%20and%20Guillermo%20Garrido%20and%20Zhuyun%20Xiao%20and%20Rupert%20Kemp%20and%20Huaixiu%20Steven%20Zheng%20and%20Hui%20Li%20and%20Ananth%20Agarwal%20and%20Christel%20Ngani%20and%20Kati%20Goshvadi%20and%20Rebeca%20Santamaria-Fernandez%20and%20Wojciech%20Fica%20and%20Xinyun%20Chen%20and%20Chris%20Gorgolewski%20and%20Sean%20Sun%20and%20Roopal%20Garg%20and%20Xinyu%20Ye%20and%20S.%20M.%20Ali%20Eslami%20and%20Nan%20Hua%20and%20Jon%20Simon%20and%20Pratik%20Joshi%20and%20Yelin%20Kim%20and%20Ian%20Tenney%20and%20Sahitya%20Potluri%20and%20Lam%20Nguyen%20Thiet%20and%20Quan%20Yuan%20and%20Florian%20Luisier%20and%20Alexandra%20Chronopoulou%20and%20Salvatore%20Scellato%20and%20Praveen%20Srinivasan%20and%20Minmin%20Chen%20and%20Vinod%20Koverkathu%20and%20Valentin%20Dalibard%20and%20Yaming%20Xu%20and%20Brennan%20Saeta%20and%20Keith%20Anderson%20and%20Thibault%20Sellam%20and%20Nick%20Fernando%20and%20Fantine%20Huot%20and%20Junehyuk%20Jung%20and%20Mani%20Varadarajan%20and%20Michael%20Quinn%20and%20Amit%20Raul%20and%20Maigo%20Le%20and%20Ruslan%20Habalov%20and%20Jon%20Clark%20and%20Komal%20Jalan%20and%20Kalesha%20Bullard%20and%20Achintya%20Singhal%20and%20Thang%20Luong%20and%20Boyu%20Wang%20and%20Sujeevan%20Rajayogam%20and%20Julian%20Eisenschlos%20and%20Johnson%20Jia%20and%20Daniel%20Finchelstein%20and%20Alex%20Yakubovich%20and%20Daniel%20Balle%20and%20Michael%20Fink%20and%20Sameer%20Agarwal%20and%20Jing%20Li%20and%20Dj%20Dvijotham%20and%20Shalini%20Pal%20and%20Kai%20Kang%20and%20Jaclyn%20Konzelmann%20and%20Jennifer%20Beattie%20and%20Olivier%20Dousse%20and%20Diane%20Wu%20and%20Remi%20Crocker%20and%20Chen%20Elkind%20and%20Siddhartha%20Reddy%20Jonnalagadda%20and%20Jong%20Lee%20and%20Dan%20Holtmann-Rice%20and%20Krystal%20Kallarackal%20and%20Rosanne%20Liu%20and%20Denis%20Vnukov%20and%20Neera%20Vats%20and%20Luca%20Invernizzi%20and%20Mohsen%20Jafari%20and%20Huanjie%20Zhou%20and%20Lilly%20Taylor%20and%20Jennifer%20Prendki%20and%20Marcus%20Wu%20and%20Tom%20Eccles%20and%20Tianqi%20Liu%20and%20Kavya%20Kopparapu%20and%20Francoise%20Beaufays%20and%20Christof%20Angermueller%20and%20Andreea%20Marzoca%20and%20Shourya%20Sarcar%20and%20Hilal%20Dib%20and%20Jeff%20Stanway%20and%20Frank%20Perbet%20and%20Nejc%20Trdin%20and%20Rachel%20Sterneck%20and%20Andrey%20Khorlin%20and%20Dinghua%20Li%20and%20Xihui%20Wu%20and%20Sonam%20Goenka%20and%20David%20Madras%20and%20Sasha%20Goldshtein%20and%20Willi%20Gierke%20and%20Tong%20Zhou%20and%20Yaxin%20Liu%20and%20Yannie%20Liang%20and%20Anais%20White%20and%20Yunjie%20Li%20and%20Shreya%20Singh%20and%20Sanaz%20Bahargam%20and%20Mark%20Epstein%20and%20Sujoy%20Basu%20and%20Li%20Lao%20and%20Adnan%20Ozturel%20and%20Carl%20Crous%20and%20Alex%20Zhai%20and%20Han%20Lu%20and%20Zora%20Tung%20and%20Neeraj%20Gaur%20and%20Alanna%20Walton%20and%20Lucas%20Dixon%20and%20Ming%20Zhang%20and%20Amir%20Globerson%20and%20Grant%20Uy%20and%20Andrew%20Bolt%20and%20Olivia%20Wiles%20and%20Milad%20Nasr%20and%20Ilia%20Shumailov%20and%20Marco%20Selvi%20and%20Francesco%20Piccinno%20and%20Ricardo%20Aguilar%20and%20Sara%20McCarthy%20and%20Misha%20Khalman%20and%20Mrinal%20Shukla%20and%20Vlado%20Galic%20and%20John%20Carpenter%20and%20Kevin%20Villela%20and%20Haibin%20Zhang%20and%20Harry%20Richardson%20and%20James%20Martens%20and%20Matko%20Bosnjak%20and%20Shreyas%20Rammohan%20Belle%20and%20Jeff%20Seibert%20and%20Mahmoud%20Alnahlawi%20and%20Brian%20McWilliams%20and%20Sankalp%20Singh%20and%20Annie%20Louis%20and%20Wen%20Ding%20and%20Dan%20Popovici%20and%20Lenin%20Simicich%20and%20Laura%20Knight%20and%20Pulkit%20Mehta%20and%20Nishesh%20Gupta%20and%20Chongyang%20Shi%20and%20Saaber%20Fatehi%20and%20Jovana%20Mitrovic%20and%20Alex%20Grills%20and%20Joseph%20Pagadora%20and%20Dessie%20Petrova%20and%20Danielle%20Eisenbud%20and%20Zhishuai%20Zhang%20and%20Damion%20Yates%20and%20Bhavishya%20Mittal%20and%20Nilesh%20Tripuraneni%20and%20Yannis%20Assael%20and%20Thomas%20Brovelli%20and%20Prateek%20Jain%20and%20Mihajlo%20Velimirovic%20and%20Canfer%20Akbulut%20and%20Jiaqi%20Mu%20and%20Wolfgang%20Macherey%20and%20Ravin%20Kumar%20and%20Jun%20Xu%20and%20Haroon%20Qureshi%20and%20Gheorghe%20Comanici%20and%20Jeremy%20Wiesner%20and%20Zhitao%20Gong%20and%20Anton%20Ruddock%20and%20Matthias%20Bauer%20and%20Nick%20Felt%20and%20Anirudh%20GP%20and%20Anurag%20Arnab%20and%20Dustin%20Zelle%20and%20Jonas%20Rothfuss%20and%20Bill%20Rosgen%20and%20Ashish%20Shenoy%20and%20Bryan%20Seybold%20and%20Xinjian%20Li%20and%20Jayaram%20Mudigonda%20and%20Goker%20Erdogan%20and%20Jiawei%20Xia%20and%20Jiri%20Simsa%20and%20Andrea%20Michi%20and%20Yi%20Yao%20and%20Christopher%20Yew%20and%20Steven%20Kan%20and%20Isaac%20Caswell%20and%20Carey%20Radebaugh%20and%20Andre%20Elisseeff%20and%20Pedro%20Valenzuela%20and%20Kay%20McKinney%20and%20Kim%20Paterson%20and%20Albert%20Cui%20and%20Eri%20Latorre-Chimoto%20and%20Solomon%20Kim%20and%20William%20Zeng%20and%20Ken%20Durden%20and%20Priya%20Ponnapalli%20and%20Tiberiu%20Sosea%20and%20Christopher%20A.%20Choquette-Choo%20and%20James%20Manyika%20and%20Brona%20Robenek%20and%20Harsha%20Vashisht%20and%20Sebastien%20Pereira%20and%20Hoi%20Lam%20and%20Marko%20Velic%20and%20Denese%20Owusu-Afriyie%20and%20Katherine%20Lee%20and%20Tolga%20Bolukbasi%20and%20Alicia%20Parrish%20and%20Shawn%20Lu%20and%20Jane%20Park%20and%20Balaji%20Venkatraman%20and%20Alice%20Talbert%20and%20Lambert%20Rosique%20and%20Yuchung%20Cheng%20and%20Andrei%20Sozanschi%20and%20Adam%20Paszke%20and%20Praveen%20Kumar%20and%20Jessica%20Austin%20and%20Lu%20Li%20and%20Khalid%20Salama%20and%20Wooyeol%20Kim%20and%20Nandita%20Dukkipati%20and%20Anthony%20Baryshnikov%20and%20Christos%20Kaplanis%20and%20XiangHai%20Sheng%20and%20Yuri%20Chervonyi%20and%20Caglar%20Unlu%20and%20Diego%20de%20Las%20Casas%20and%20Harry%20Askham%20and%20Kathryn%20Tunyasuvunakool%20and%20Felix%20Gimeno%20and%20Siim%20Poder%20and%20Chester%20Kwak%20and%20Matt%20Miecnikowski%20and%20Vahab%20Mirrokni%20and%20Alek%20Dimitriev%20and%20Aaron%20Parisi%20and%20Dangyi%20Liu%20and%20Tomy%20Tsai%20and%20Toby%20Shevlane%20and%20Christina%20Kouridi%20and%20Drew%20Garmon%20and%20Adrian%20Goedeckemeyer%20and%20Adam%20R.%20Brown%20and%20Anitha%20Vijayakumar%20and%20Ali%20Elqursh%20and%20Sadegh%20Jazayeri%20and%20Jin%20Huang%20and%20Sara%20Mc%20Carthy%20and%20Jay%20Hoover%20and%20Lucy%20Kim%20and%20Sandeep%20Kumar%20and%20Wei%20Chen%20and%20Courtney%20Biles%20and%20Garrett%20Bingham%20and%20Evan%20Rosen%20and%20Lisa%20Wang%20and%20Qijun%20Tan%20and%20David%20Engel%20and%20Francesco%20Pongetti%20and%20Dario%20de%20Cesare%20and%20Dongseong%20Hwang%20and%20Lily%20Yu%20and%20Jennifer%20Pullman%20and%20Srini%20Narayanan%20and%20Kyle%20Levin%20and%20Siddharth%20Gopal%20and%20Megan%20Li%20and%20Asaf%20Aharoni%20and%20Trieu%20Trinh%20and%20Jessica%20Lo%20and%20Norman%20Casagrande%20and%20Roopali%20Vij%20and%20Loic%20Matthey%20and%20Bramandia%20Ramadhana%20and%20Austin%20Matthews%20and%20CJ%20Carey%20and%20Matthew%20Johnson%20and%20Kremena%20Goranova%20and%20Rohin%20Shah%20and%20Shereen%20Ashraf%20and%20Kingshuk%20Dasgupta%20and%20Rasmus%20Larsen%20and%20Yicheng%20Wang%20and%20Manish%20Reddy%20Vuyyuru%20and%20Chong%20Jiang%20and%20Joana%20Ijazi%20and%20Kazuki%20Osawa%20and%20Celine%20Smith%20and%20Ramya%20Sree%20Boppana%20and%20Taylan%20Bilal%20and%20Yuma%20Koizumi%20and%20Ying%20Xu%20and%20Yasemin%20Altun%20and%20Nir%20Shabat%20and%20Ben%20Bariach%20and%20Alex%20Korchemniy%20and%20Kiam%20Choo%20and%20Olaf%20Ronneberger%20and%20Chimezie%20Iwuanyanwu%20and%20Shubin%20Zhao%20and%20David%20Soergel%20and%20Cho-Jui%20Hsieh%20and%20Irene%20Cai%20and%20Shariq%20Iqbal%20and%20Martin%20Sundermeyer%20and%20Zhe%20Chen%20and%20Elie%20Bursztein%20and%20Chaitanya%20Malaviya%20and%20Fadi%20Biadsy%20and%20Prakash%20Shroff%20and%20Inderjit%20Dhillon%20and%20Tejasi%20Latkar%20and%20Chris%20Dyer%20and%20Hannah%20Forbes%20and%20Massimo%20Nicosia%20and%20Vitaly%20Nikolaev%20and%20Somer%20Greene%20and%20Marin%20Georgiev%20and%20Pidong%20Wang%20and%20Nina%20Martin%20and%20Hanie%20Sedghi%20and%20John%20Zhang%20and%20Praseem%20Banzal%20and%20Doug%20Fritz%20and%20Vikram%20Rao%20and%20Xuezhi%20Wang%20and%20Jiageng%20Zhang%20and%20Viorica%20Patraucean%20and%20Dayou%20Du%20and%20Igor%20Mordatch%20and%20Ivan%20Jurin%20and%20Lewis%20Liu%20and%20Ayush%20Dubey%20and%20Abhi%20Mohan%20and%20Janek%20Nowakowski%20and%20Vlad-Doru%20Ion%20and%20Nan%20Wei%20and%20Reiko%20Tojo%20and%20Maria%20Abi%20Raad%20and%20Drew%20A.%20Hudson%20and%20Vaishakh%20Keshava%20and%20Shubham%20Agrawal%20and%20Kevin%20Ramirez%20and%20Zhichun%20Wu%20and%20Hoang%20Nguyen%20and%20Ji%20Liu%20and%20Madhavi%20Sewak%20and%20Bryce%20Petrini%20and%20DongHyun%20Choi%20and%20Ivan%20Philips%20and%20Ziyue%20Wang%20and%20Ioana%20Bica%20and%20Ankush%20Garg%20and%20Jarek%20Wilkiewicz%20and%20Priyanka%20Agrawal%20and%20Xiaowei%20Li%20and%20Danhao%20Guo%20and%20Emily%20Xue%20and%20Naseer%20Shaik%20and%20Andrew%20Leach%20and%20Sadh%20MNM%20Khan%20and%20Julia%20Wiesinger%20and%20Sammy%20Jerome%20and%20Abhishek%20Chakladar%20and%20Alek%20Wenjiao%20Wang%20and%20Tina%20Ornduff%20and%20Folake%20Abu%20and%20Alireza%20Ghaffarkhah%20and%20Marcus%20Wainwright%20and%20Mario%20Cortes%20and%20Frederick%20Liu%20and%20Joshua%20Maynez%20and%20Slav%20Petrov%20and%20Yonghui%20Wu%20and%20Demis%20Hassabis%20and%20Koray%20Kavukcuoglu%20and%20Jeffrey%20Dean%20and%20Oriol%20Vinyals&entry.1292438233=%20%20In%20this%20report%2C%20we%20introduce%20the%20Gemini%201.5%20family%20of%20models%2C%20representing%0Athe%20next%20generation%20of%20highly%20compute-efficient%20multimodal%20models%20capable%20of%0Arecalling%20and%20reasoning%20over%20fine-grained%20information%20from%20millions%20of%20tokens%0Aof%20context%2C%20including%20multiple%20long%20documents%20and%20hours%20of%20video%20and%20audio.%20The%0Afamily%20includes%20two%20new%20models%3A%20%281%29%20an%20updated%20Gemini%201.5%20Pro%2C%20which%20exceeds%0Athe%20February%20version%20on%20the%20great%20majority%20of%20capabilities%20and%20benchmarks%3B%20%282%29%0AGemini%201.5%20Flash%2C%20a%20more%20lightweight%20variant%20designed%20for%20efficiency%20with%0Aminimal%20regression%20in%20quality.%20Gemini%201.5%20models%20achieve%20near-perfect%20recall%20on%0Along-context%20retrieval%20tasks%20across%20modalities%2C%20improve%20the%20state-of-the-art%20in%0Along-document%20QA%2C%20long-video%20QA%20and%20long-context%20ASR%2C%20and%20match%20or%20surpass%0AGemini%201.0%20Ultra%27s%20state-of-the-art%20performance%20across%20a%20broad%20set%20of%0Abenchmarks.%20Studying%20the%20limits%20of%20Gemini%201.5%27s%20long-context%20ability%2C%20we%20find%0Acontinued%20improvement%20in%20next-token%20prediction%20and%20near-perfect%20retrieval%0A%28%3E99%25%29%20up%20to%20at%20least%2010M%20tokens%2C%20a%20generational%20leap%20over%20existing%20models%20such%0Aas%20Claude%203.0%20%28200k%29%20and%20GPT-4%20Turbo%20%28128k%29.%20Finally%2C%20we%20highlight%20real-world%0Ause%20cases%2C%20such%20as%20Gemini%201.5%20collaborating%20with%20professionals%20on%20completing%0Atheir%20tasks%20achieving%2026%20to%2075%25%20time%20savings%20across%2010%20different%20job%0Acategories%2C%20as%20well%20as%20surprising%20new%20capabilities%20of%20large%20language%20models%20at%0Athe%20frontier%3B%20when%20given%20a%20grammar%20manual%20for%20Kalamang%2C%20a%20language%20with%20fewer%0Athan%20200%20speakers%20worldwide%2C%20the%20model%20learns%20to%20translate%20English%20to%20Kalamang%0Aat%20a%20similar%20level%20to%20a%20person%20who%20learned%20from%20the%20same%20content.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05530v3&entry.124074799=Read"},
{"title": "L2XGNN: Learning to Explain Graph Neural Networks", "author": "Giuseppe Serra and Mathias Niepert", "abstract": "  Graph Neural Networks (GNNs) are a popular class of machine learning models.\nInspired by the learning to explain (L2X) paradigm, we propose L2XGNN, a\nframework for explainable GNNs which provides faithful explanations by design.\nL2XGNN learns a mechanism for selecting explanatory subgraphs (motifs) which\nare exclusively used in the GNNs message-passing operations. L2XGNN is able to\nselect, for each input graph, a subgraph with specific properties such as being\nsparse and connected. Imposing such constraints on the motifs often leads to\nmore interpretable and effective explanations. Experiments on several datasets\nsuggest that L2XGNN achieves the same classification accuracy as baseline\nmethods using the entire input graph while ensuring that only the provided\nexplanations are used to make predictions. Moreover, we show that L2XGNN is\nable to identify motifs responsible for the graph's properties it is intended\nto predict.\n", "link": "http://arxiv.org/abs/2209.14402v4", "date": "2024-06-14", "relevancy": 2.3141, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4889}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4574}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L2XGNN%3A%20Learning%20to%20Explain%20Graph%20Neural%20Networks&body=Title%3A%20L2XGNN%3A%20Learning%20to%20Explain%20Graph%20Neural%20Networks%0AAuthor%3A%20Giuseppe%20Serra%20and%20Mathias%20Niepert%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20a%20popular%20class%20of%20machine%20learning%20models.%0AInspired%20by%20the%20learning%20to%20explain%20%28L2X%29%20paradigm%2C%20we%20propose%20L2XGNN%2C%20a%0Aframework%20for%20explainable%20GNNs%20which%20provides%20faithful%20explanations%20by%20design.%0AL2XGNN%20learns%20a%20mechanism%20for%20selecting%20explanatory%20subgraphs%20%28motifs%29%20which%0Aare%20exclusively%20used%20in%20the%20GNNs%20message-passing%20operations.%20L2XGNN%20is%20able%20to%0Aselect%2C%20for%20each%20input%20graph%2C%20a%20subgraph%20with%20specific%20properties%20such%20as%20being%0Asparse%20and%20connected.%20Imposing%20such%20constraints%20on%20the%20motifs%20often%20leads%20to%0Amore%20interpretable%20and%20effective%20explanations.%20Experiments%20on%20several%20datasets%0Asuggest%20that%20L2XGNN%20achieves%20the%20same%20classification%20accuracy%20as%20baseline%0Amethods%20using%20the%20entire%20input%20graph%20while%20ensuring%20that%20only%20the%20provided%0Aexplanations%20are%20used%20to%20make%20predictions.%20Moreover%2C%20we%20show%20that%20L2XGNN%20is%0Aable%20to%20identify%20motifs%20responsible%20for%20the%20graph%27s%20properties%20it%20is%20intended%0Ato%20predict.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.14402v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL2XGNN%253A%2520Learning%2520to%2520Explain%2520Graph%2520Neural%2520Networks%26entry.906535625%3DGiuseppe%2520Serra%2520and%2520Mathias%2520Niepert%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520a%2520popular%2520class%2520of%2520machine%2520learning%2520models.%250AInspired%2520by%2520the%2520learning%2520to%2520explain%2520%2528L2X%2529%2520paradigm%252C%2520we%2520propose%2520L2XGNN%252C%2520a%250Aframework%2520for%2520explainable%2520GNNs%2520which%2520provides%2520faithful%2520explanations%2520by%2520design.%250AL2XGNN%2520learns%2520a%2520mechanism%2520for%2520selecting%2520explanatory%2520subgraphs%2520%2528motifs%2529%2520which%250Aare%2520exclusively%2520used%2520in%2520the%2520GNNs%2520message-passing%2520operations.%2520L2XGNN%2520is%2520able%2520to%250Aselect%252C%2520for%2520each%2520input%2520graph%252C%2520a%2520subgraph%2520with%2520specific%2520properties%2520such%2520as%2520being%250Asparse%2520and%2520connected.%2520Imposing%2520such%2520constraints%2520on%2520the%2520motifs%2520often%2520leads%2520to%250Amore%2520interpretable%2520and%2520effective%2520explanations.%2520Experiments%2520on%2520several%2520datasets%250Asuggest%2520that%2520L2XGNN%2520achieves%2520the%2520same%2520classification%2520accuracy%2520as%2520baseline%250Amethods%2520using%2520the%2520entire%2520input%2520graph%2520while%2520ensuring%2520that%2520only%2520the%2520provided%250Aexplanations%2520are%2520used%2520to%2520make%2520predictions.%2520Moreover%252C%2520we%2520show%2520that%2520L2XGNN%2520is%250Aable%2520to%2520identify%2520motifs%2520responsible%2520for%2520the%2520graph%2527s%2520properties%2520it%2520is%2520intended%250Ato%2520predict.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.14402v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L2XGNN%3A%20Learning%20to%20Explain%20Graph%20Neural%20Networks&entry.906535625=Giuseppe%20Serra%20and%20Mathias%20Niepert&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20a%20popular%20class%20of%20machine%20learning%20models.%0AInspired%20by%20the%20learning%20to%20explain%20%28L2X%29%20paradigm%2C%20we%20propose%20L2XGNN%2C%20a%0Aframework%20for%20explainable%20GNNs%20which%20provides%20faithful%20explanations%20by%20design.%0AL2XGNN%20learns%20a%20mechanism%20for%20selecting%20explanatory%20subgraphs%20%28motifs%29%20which%0Aare%20exclusively%20used%20in%20the%20GNNs%20message-passing%20operations.%20L2XGNN%20is%20able%20to%0Aselect%2C%20for%20each%20input%20graph%2C%20a%20subgraph%20with%20specific%20properties%20such%20as%20being%0Asparse%20and%20connected.%20Imposing%20such%20constraints%20on%20the%20motifs%20often%20leads%20to%0Amore%20interpretable%20and%20effective%20explanations.%20Experiments%20on%20several%20datasets%0Asuggest%20that%20L2XGNN%20achieves%20the%20same%20classification%20accuracy%20as%20baseline%0Amethods%20using%20the%20entire%20input%20graph%20while%20ensuring%20that%20only%20the%20provided%0Aexplanations%20are%20used%20to%20make%20predictions.%20Moreover%2C%20we%20show%20that%20L2XGNN%20is%0Aable%20to%20identify%20motifs%20responsible%20for%20the%20graph%27s%20properties%20it%20is%20intended%0Ato%20predict.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.14402v4&entry.124074799=Read"},
{"title": "DurLAR: A High-fidelity 128-channel LiDAR Dataset with Panoramic Ambient\n  and Reflectivity Imagery for Multi-modal Autonomous Driving Applications", "author": "Li Li and Khalid N. Ismail and Hubert P. H. Shum and Toby P. Breckon", "abstract": "  We present DurLAR, a high-fidelity 128-channel 3D LiDAR dataset with\npanoramic ambient (near infrared) and reflectivity imagery, as well as a sample\nbenchmark task using depth estimation for autonomous driving applications. Our\ndriving platform is equipped with a high resolution 128 channel LiDAR, a 2MPix\nstereo camera, a lux meter and a GNSS/INS system. Ambient and reflectivity\nimages are made available along with the LiDAR point clouds to facilitate\nmulti-modal use of concurrent ambient and reflectivity scene information.\nLeveraging DurLAR, with a resolution exceeding that of prior benchmarks, we\nconsider the task of monocular depth estimation and use this increased\navailability of higher resolution, yet sparse ground truth scene depth\ninformation to propose a novel joint supervised/self-supervised loss\nformulation. We compare performance over both our new DurLAR dataset, the\nestablished KITTI benchmark and the Cityscapes dataset. Our evaluation shows\nour joint use supervised and self-supervised loss terms, enabled via the\nsuperior ground truth resolution and availability within DurLAR improves the\nquantitative and qualitative performance of leading contemporary monocular\ndepth estimation approaches (RMSE=3.639, Sq Rel=0.936).\n", "link": "http://arxiv.org/abs/2406.10068v1", "date": "2024-06-14", "relevancy": 2.3133, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6128}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5702}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DurLAR%3A%20A%20High-fidelity%20128-channel%20LiDAR%20Dataset%20with%20Panoramic%20Ambient%0A%20%20and%20Reflectivity%20Imagery%20for%20Multi-modal%20Autonomous%20Driving%20Applications&body=Title%3A%20DurLAR%3A%20A%20High-fidelity%20128-channel%20LiDAR%20Dataset%20with%20Panoramic%20Ambient%0A%20%20and%20Reflectivity%20Imagery%20for%20Multi-modal%20Autonomous%20Driving%20Applications%0AAuthor%3A%20Li%20Li%20and%20Khalid%20N.%20Ismail%20and%20Hubert%20P.%20H.%20Shum%20and%20Toby%20P.%20Breckon%0AAbstract%3A%20%20%20We%20present%20DurLAR%2C%20a%20high-fidelity%20128-channel%203D%20LiDAR%20dataset%20with%0Apanoramic%20ambient%20%28near%20infrared%29%20and%20reflectivity%20imagery%2C%20as%20well%20as%20a%20sample%0Abenchmark%20task%20using%20depth%20estimation%20for%20autonomous%20driving%20applications.%20Our%0Adriving%20platform%20is%20equipped%20with%20a%20high%20resolution%20128%20channel%20LiDAR%2C%20a%202MPix%0Astereo%20camera%2C%20a%20lux%20meter%20and%20a%20GNSS/INS%20system.%20Ambient%20and%20reflectivity%0Aimages%20are%20made%20available%20along%20with%20the%20LiDAR%20point%20clouds%20to%20facilitate%0Amulti-modal%20use%20of%20concurrent%20ambient%20and%20reflectivity%20scene%20information.%0ALeveraging%20DurLAR%2C%20with%20a%20resolution%20exceeding%20that%20of%20prior%20benchmarks%2C%20we%0Aconsider%20the%20task%20of%20monocular%20depth%20estimation%20and%20use%20this%20increased%0Aavailability%20of%20higher%20resolution%2C%20yet%20sparse%20ground%20truth%20scene%20depth%0Ainformation%20to%20propose%20a%20novel%20joint%20supervised/self-supervised%20loss%0Aformulation.%20We%20compare%20performance%20over%20both%20our%20new%20DurLAR%20dataset%2C%20the%0Aestablished%20KITTI%20benchmark%20and%20the%20Cityscapes%20dataset.%20Our%20evaluation%20shows%0Aour%20joint%20use%20supervised%20and%20self-supervised%20loss%20terms%2C%20enabled%20via%20the%0Asuperior%20ground%20truth%20resolution%20and%20availability%20within%20DurLAR%20improves%20the%0Aquantitative%20and%20qualitative%20performance%20of%20leading%20contemporary%20monocular%0Adepth%20estimation%20approaches%20%28RMSE%3D3.639%2C%20Sq%20Rel%3D0.936%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDurLAR%253A%2520A%2520High-fidelity%2520128-channel%2520LiDAR%2520Dataset%2520with%2520Panoramic%2520Ambient%250A%2520%2520and%2520Reflectivity%2520Imagery%2520for%2520Multi-modal%2520Autonomous%2520Driving%2520Applications%26entry.906535625%3DLi%2520Li%2520and%2520Khalid%2520N.%2520Ismail%2520and%2520Hubert%2520P.%2520H.%2520Shum%2520and%2520Toby%2520P.%2520Breckon%26entry.1292438233%3D%2520%2520We%2520present%2520DurLAR%252C%2520a%2520high-fidelity%2520128-channel%25203D%2520LiDAR%2520dataset%2520with%250Apanoramic%2520ambient%2520%2528near%2520infrared%2529%2520and%2520reflectivity%2520imagery%252C%2520as%2520well%2520as%2520a%2520sample%250Abenchmark%2520task%2520using%2520depth%2520estimation%2520for%2520autonomous%2520driving%2520applications.%2520Our%250Adriving%2520platform%2520is%2520equipped%2520with%2520a%2520high%2520resolution%2520128%2520channel%2520LiDAR%252C%2520a%25202MPix%250Astereo%2520camera%252C%2520a%2520lux%2520meter%2520and%2520a%2520GNSS/INS%2520system.%2520Ambient%2520and%2520reflectivity%250Aimages%2520are%2520made%2520available%2520along%2520with%2520the%2520LiDAR%2520point%2520clouds%2520to%2520facilitate%250Amulti-modal%2520use%2520of%2520concurrent%2520ambient%2520and%2520reflectivity%2520scene%2520information.%250ALeveraging%2520DurLAR%252C%2520with%2520a%2520resolution%2520exceeding%2520that%2520of%2520prior%2520benchmarks%252C%2520we%250Aconsider%2520the%2520task%2520of%2520monocular%2520depth%2520estimation%2520and%2520use%2520this%2520increased%250Aavailability%2520of%2520higher%2520resolution%252C%2520yet%2520sparse%2520ground%2520truth%2520scene%2520depth%250Ainformation%2520to%2520propose%2520a%2520novel%2520joint%2520supervised/self-supervised%2520loss%250Aformulation.%2520We%2520compare%2520performance%2520over%2520both%2520our%2520new%2520DurLAR%2520dataset%252C%2520the%250Aestablished%2520KITTI%2520benchmark%2520and%2520the%2520Cityscapes%2520dataset.%2520Our%2520evaluation%2520shows%250Aour%2520joint%2520use%2520supervised%2520and%2520self-supervised%2520loss%2520terms%252C%2520enabled%2520via%2520the%250Asuperior%2520ground%2520truth%2520resolution%2520and%2520availability%2520within%2520DurLAR%2520improves%2520the%250Aquantitative%2520and%2520qualitative%2520performance%2520of%2520leading%2520contemporary%2520monocular%250Adepth%2520estimation%2520approaches%2520%2528RMSE%253D3.639%252C%2520Sq%2520Rel%253D0.936%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DurLAR%3A%20A%20High-fidelity%20128-channel%20LiDAR%20Dataset%20with%20Panoramic%20Ambient%0A%20%20and%20Reflectivity%20Imagery%20for%20Multi-modal%20Autonomous%20Driving%20Applications&entry.906535625=Li%20Li%20and%20Khalid%20N.%20Ismail%20and%20Hubert%20P.%20H.%20Shum%20and%20Toby%20P.%20Breckon&entry.1292438233=%20%20We%20present%20DurLAR%2C%20a%20high-fidelity%20128-channel%203D%20LiDAR%20dataset%20with%0Apanoramic%20ambient%20%28near%20infrared%29%20and%20reflectivity%20imagery%2C%20as%20well%20as%20a%20sample%0Abenchmark%20task%20using%20depth%20estimation%20for%20autonomous%20driving%20applications.%20Our%0Adriving%20platform%20is%20equipped%20with%20a%20high%20resolution%20128%20channel%20LiDAR%2C%20a%202MPix%0Astereo%20camera%2C%20a%20lux%20meter%20and%20a%20GNSS/INS%20system.%20Ambient%20and%20reflectivity%0Aimages%20are%20made%20available%20along%20with%20the%20LiDAR%20point%20clouds%20to%20facilitate%0Amulti-modal%20use%20of%20concurrent%20ambient%20and%20reflectivity%20scene%20information.%0ALeveraging%20DurLAR%2C%20with%20a%20resolution%20exceeding%20that%20of%20prior%20benchmarks%2C%20we%0Aconsider%20the%20task%20of%20monocular%20depth%20estimation%20and%20use%20this%20increased%0Aavailability%20of%20higher%20resolution%2C%20yet%20sparse%20ground%20truth%20scene%20depth%0Ainformation%20to%20propose%20a%20novel%20joint%20supervised/self-supervised%20loss%0Aformulation.%20We%20compare%20performance%20over%20both%20our%20new%20DurLAR%20dataset%2C%20the%0Aestablished%20KITTI%20benchmark%20and%20the%20Cityscapes%20dataset.%20Our%20evaluation%20shows%0Aour%20joint%20use%20supervised%20and%20self-supervised%20loss%20terms%2C%20enabled%20via%20the%0Asuperior%20ground%20truth%20resolution%20and%20availability%20within%20DurLAR%20improves%20the%0Aquantitative%20and%20qualitative%20performance%20of%20leading%20contemporary%20monocular%0Adepth%20estimation%20approaches%20%28RMSE%3D3.639%2C%20Sq%20Rel%3D0.936%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10068v1&entry.124074799=Read"},
{"title": "Future Directions in the Theory of Graph Machine Learning", "author": "Christopher Morris and Fabrizio Frasca and Nadav Dym and Haggai Maron and \u0130smail \u0130lkan Ceylan and Ron Levie and Derek Lim and Michael Bronstein and Martin Grohe and Stefanie Jegelka", "abstract": "  Machine learning on graphs, especially using graph neural networks (GNNs),\nhas seen a surge in interest due to the wide availability of graph data across\na broad spectrum of disciplines, from life to social and engineering sciences.\nDespite their practical success, our theoretical understanding of the\nproperties of GNNs remains highly incomplete. Recent theoretical advancements\nprimarily focus on elucidating the coarse-grained expressive power of GNNs,\npredominantly employing combinatorial techniques. However, these studies do not\nperfectly align with practice, particularly in understanding the generalization\nbehavior of GNNs when trained with stochastic first-order optimization\ntechniques. In this position paper, we argue that the graph machine learning\ncommunity needs to shift its attention to developing a balanced theory of graph\nmachine learning, focusing on a more thorough understanding of the interplay of\nexpressive power, generalization, and optimization.\n", "link": "http://arxiv.org/abs/2402.02287v4", "date": "2024-06-14", "relevancy": 2.3082, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4802}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4619}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Future%20Directions%20in%20the%20Theory%20of%20Graph%20Machine%20Learning&body=Title%3A%20Future%20Directions%20in%20the%20Theory%20of%20Graph%20Machine%20Learning%0AAuthor%3A%20Christopher%20Morris%20and%20Fabrizio%20Frasca%20and%20Nadav%20Dym%20and%20Haggai%20Maron%20and%20%C4%B0smail%20%C4%B0lkan%20Ceylan%20and%20Ron%20Levie%20and%20Derek%20Lim%20and%20Michael%20Bronstein%20and%20Martin%20Grohe%20and%20Stefanie%20Jegelka%0AAbstract%3A%20%20%20Machine%20learning%20on%20graphs%2C%20especially%20using%20graph%20neural%20networks%20%28GNNs%29%2C%0Ahas%20seen%20a%20surge%20in%20interest%20due%20to%20the%20wide%20availability%20of%20graph%20data%20across%0Aa%20broad%20spectrum%20of%20disciplines%2C%20from%20life%20to%20social%20and%20engineering%20sciences.%0ADespite%20their%20practical%20success%2C%20our%20theoretical%20understanding%20of%20the%0Aproperties%20of%20GNNs%20remains%20highly%20incomplete.%20Recent%20theoretical%20advancements%0Aprimarily%20focus%20on%20elucidating%20the%20coarse-grained%20expressive%20power%20of%20GNNs%2C%0Apredominantly%20employing%20combinatorial%20techniques.%20However%2C%20these%20studies%20do%20not%0Aperfectly%20align%20with%20practice%2C%20particularly%20in%20understanding%20the%20generalization%0Abehavior%20of%20GNNs%20when%20trained%20with%20stochastic%20first-order%20optimization%0Atechniques.%20In%20this%20position%20paper%2C%20we%20argue%20that%20the%20graph%20machine%20learning%0Acommunity%20needs%20to%20shift%20its%20attention%20to%20developing%20a%20balanced%20theory%20of%20graph%0Amachine%20learning%2C%20focusing%20on%20a%20more%20thorough%20understanding%20of%20the%20interplay%20of%0Aexpressive%20power%2C%20generalization%2C%20and%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02287v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFuture%2520Directions%2520in%2520the%2520Theory%2520of%2520Graph%2520Machine%2520Learning%26entry.906535625%3DChristopher%2520Morris%2520and%2520Fabrizio%2520Frasca%2520and%2520Nadav%2520Dym%2520and%2520Haggai%2520Maron%2520and%2520%25C4%25B0smail%2520%25C4%25B0lkan%2520Ceylan%2520and%2520Ron%2520Levie%2520and%2520Derek%2520Lim%2520and%2520Michael%2520Bronstein%2520and%2520Martin%2520Grohe%2520and%2520Stefanie%2520Jegelka%26entry.1292438233%3D%2520%2520Machine%2520learning%2520on%2520graphs%252C%2520especially%2520using%2520graph%2520neural%2520networks%2520%2528GNNs%2529%252C%250Ahas%2520seen%2520a%2520surge%2520in%2520interest%2520due%2520to%2520the%2520wide%2520availability%2520of%2520graph%2520data%2520across%250Aa%2520broad%2520spectrum%2520of%2520disciplines%252C%2520from%2520life%2520to%2520social%2520and%2520engineering%2520sciences.%250ADespite%2520their%2520practical%2520success%252C%2520our%2520theoretical%2520understanding%2520of%2520the%250Aproperties%2520of%2520GNNs%2520remains%2520highly%2520incomplete.%2520Recent%2520theoretical%2520advancements%250Aprimarily%2520focus%2520on%2520elucidating%2520the%2520coarse-grained%2520expressive%2520power%2520of%2520GNNs%252C%250Apredominantly%2520employing%2520combinatorial%2520techniques.%2520However%252C%2520these%2520studies%2520do%2520not%250Aperfectly%2520align%2520with%2520practice%252C%2520particularly%2520in%2520understanding%2520the%2520generalization%250Abehavior%2520of%2520GNNs%2520when%2520trained%2520with%2520stochastic%2520first-order%2520optimization%250Atechniques.%2520In%2520this%2520position%2520paper%252C%2520we%2520argue%2520that%2520the%2520graph%2520machine%2520learning%250Acommunity%2520needs%2520to%2520shift%2520its%2520attention%2520to%2520developing%2520a%2520balanced%2520theory%2520of%2520graph%250Amachine%2520learning%252C%2520focusing%2520on%2520a%2520more%2520thorough%2520understanding%2520of%2520the%2520interplay%2520of%250Aexpressive%2520power%252C%2520generalization%252C%2520and%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02287v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Future%20Directions%20in%20the%20Theory%20of%20Graph%20Machine%20Learning&entry.906535625=Christopher%20Morris%20and%20Fabrizio%20Frasca%20and%20Nadav%20Dym%20and%20Haggai%20Maron%20and%20%C4%B0smail%20%C4%B0lkan%20Ceylan%20and%20Ron%20Levie%20and%20Derek%20Lim%20and%20Michael%20Bronstein%20and%20Martin%20Grohe%20and%20Stefanie%20Jegelka&entry.1292438233=%20%20Machine%20learning%20on%20graphs%2C%20especially%20using%20graph%20neural%20networks%20%28GNNs%29%2C%0Ahas%20seen%20a%20surge%20in%20interest%20due%20to%20the%20wide%20availability%20of%20graph%20data%20across%0Aa%20broad%20spectrum%20of%20disciplines%2C%20from%20life%20to%20social%20and%20engineering%20sciences.%0ADespite%20their%20practical%20success%2C%20our%20theoretical%20understanding%20of%20the%0Aproperties%20of%20GNNs%20remains%20highly%20incomplete.%20Recent%20theoretical%20advancements%0Aprimarily%20focus%20on%20elucidating%20the%20coarse-grained%20expressive%20power%20of%20GNNs%2C%0Apredominantly%20employing%20combinatorial%20techniques.%20However%2C%20these%20studies%20do%20not%0Aperfectly%20align%20with%20practice%2C%20particularly%20in%20understanding%20the%20generalization%0Abehavior%20of%20GNNs%20when%20trained%20with%20stochastic%20first-order%20optimization%0Atechniques.%20In%20this%20position%20paper%2C%20we%20argue%20that%20the%20graph%20machine%20learning%0Acommunity%20needs%20to%20shift%20its%20attention%20to%20developing%20a%20balanced%20theory%20of%20graph%0Amachine%20learning%2C%20focusing%20on%20a%20more%20thorough%20understanding%20of%20the%20interplay%20of%0Aexpressive%20power%2C%20generalization%2C%20and%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02287v4&entry.124074799=Read"},
{"title": "Compressed Sensor Caching and Collaborative Sparse Data Recovery with\n  Anchor Alignment", "author": "Yi-Jen Yang and Ming-Hsun Yang and Jwo-Yuh Wu and Y. -W. Peter Hong", "abstract": "  This work examines the compressed sensor caching problem in wireless sensor\nnetworks and devises efficient distributed sparse data recovery algorithms to\nenable collaboration among multiple caches. In this problem, each cache is only\nallowed to access measurements from a small subset of sensors within its\nvicinity to reduce both cache size and data acquisition overhead. To enable\nreliable data recovery with limited access to measurements, we propose a\ndistributed sparse data recovery method, called the collaborative sparse\nrecovery by anchor alignment (CoSR-AA) algorithm, where collaboration among\ncaches is enabled by aligning their locally recovered data at a few anchor\nnodes. The proposed algorithm is based on the consensus alternating direction\nmethod of multipliers (ADMM) algorithm but with message exchange that is\nreduced by considering the proposed anchor alignment strategy. Then, by the\ndeep unfolding of the ADMM iterations, we further propose the Deep CoSR-AA\nalgorithm that can be used to significantly reduce the number of iterations. We\nobtain a graph neural network architecture where message exchange is done more\nefficiently by an embedded autoencoder. Simulations are provided to demonstrate\nthe effectiveness of the proposed collaborative recovery algorithms in terms of\nthe improved reconstruction quality and the reduced communication overhead due\nto anchor alignment.\n", "link": "http://arxiv.org/abs/2406.10137v1", "date": "2024-06-14", "relevancy": 2.3053, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4642}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4613}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compressed%20Sensor%20Caching%20and%20Collaborative%20Sparse%20Data%20Recovery%20with%0A%20%20Anchor%20Alignment&body=Title%3A%20Compressed%20Sensor%20Caching%20and%20Collaborative%20Sparse%20Data%20Recovery%20with%0A%20%20Anchor%20Alignment%0AAuthor%3A%20Yi-Jen%20Yang%20and%20Ming-Hsun%20Yang%20and%20Jwo-Yuh%20Wu%20and%20Y.%20-W.%20Peter%20Hong%0AAbstract%3A%20%20%20This%20work%20examines%20the%20compressed%20sensor%20caching%20problem%20in%20wireless%20sensor%0Anetworks%20and%20devises%20efficient%20distributed%20sparse%20data%20recovery%20algorithms%20to%0Aenable%20collaboration%20among%20multiple%20caches.%20In%20this%20problem%2C%20each%20cache%20is%20only%0Aallowed%20to%20access%20measurements%20from%20a%20small%20subset%20of%20sensors%20within%20its%0Avicinity%20to%20reduce%20both%20cache%20size%20and%20data%20acquisition%20overhead.%20To%20enable%0Areliable%20data%20recovery%20with%20limited%20access%20to%20measurements%2C%20we%20propose%20a%0Adistributed%20sparse%20data%20recovery%20method%2C%20called%20the%20collaborative%20sparse%0Arecovery%20by%20anchor%20alignment%20%28CoSR-AA%29%20algorithm%2C%20where%20collaboration%20among%0Acaches%20is%20enabled%20by%20aligning%20their%20locally%20recovered%20data%20at%20a%20few%20anchor%0Anodes.%20The%20proposed%20algorithm%20is%20based%20on%20the%20consensus%20alternating%20direction%0Amethod%20of%20multipliers%20%28ADMM%29%20algorithm%20but%20with%20message%20exchange%20that%20is%0Areduced%20by%20considering%20the%20proposed%20anchor%20alignment%20strategy.%20Then%2C%20by%20the%0Adeep%20unfolding%20of%20the%20ADMM%20iterations%2C%20we%20further%20propose%20the%20Deep%20CoSR-AA%0Aalgorithm%20that%20can%20be%20used%20to%20significantly%20reduce%20the%20number%20of%20iterations.%20We%0Aobtain%20a%20graph%20neural%20network%20architecture%20where%20message%20exchange%20is%20done%20more%0Aefficiently%20by%20an%20embedded%20autoencoder.%20Simulations%20are%20provided%20to%20demonstrate%0Athe%20effectiveness%20of%20the%20proposed%20collaborative%20recovery%20algorithms%20in%20terms%20of%0Athe%20improved%20reconstruction%20quality%20and%20the%20reduced%20communication%20overhead%20due%0Ato%20anchor%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompressed%2520Sensor%2520Caching%2520and%2520Collaborative%2520Sparse%2520Data%2520Recovery%2520with%250A%2520%2520Anchor%2520Alignment%26entry.906535625%3DYi-Jen%2520Yang%2520and%2520Ming-Hsun%2520Yang%2520and%2520Jwo-Yuh%2520Wu%2520and%2520Y.%2520-W.%2520Peter%2520Hong%26entry.1292438233%3D%2520%2520This%2520work%2520examines%2520the%2520compressed%2520sensor%2520caching%2520problem%2520in%2520wireless%2520sensor%250Anetworks%2520and%2520devises%2520efficient%2520distributed%2520sparse%2520data%2520recovery%2520algorithms%2520to%250Aenable%2520collaboration%2520among%2520multiple%2520caches.%2520In%2520this%2520problem%252C%2520each%2520cache%2520is%2520only%250Aallowed%2520to%2520access%2520measurements%2520from%2520a%2520small%2520subset%2520of%2520sensors%2520within%2520its%250Avicinity%2520to%2520reduce%2520both%2520cache%2520size%2520and%2520data%2520acquisition%2520overhead.%2520To%2520enable%250Areliable%2520data%2520recovery%2520with%2520limited%2520access%2520to%2520measurements%252C%2520we%2520propose%2520a%250Adistributed%2520sparse%2520data%2520recovery%2520method%252C%2520called%2520the%2520collaborative%2520sparse%250Arecovery%2520by%2520anchor%2520alignment%2520%2528CoSR-AA%2529%2520algorithm%252C%2520where%2520collaboration%2520among%250Acaches%2520is%2520enabled%2520by%2520aligning%2520their%2520locally%2520recovered%2520data%2520at%2520a%2520few%2520anchor%250Anodes.%2520The%2520proposed%2520algorithm%2520is%2520based%2520on%2520the%2520consensus%2520alternating%2520direction%250Amethod%2520of%2520multipliers%2520%2528ADMM%2529%2520algorithm%2520but%2520with%2520message%2520exchange%2520that%2520is%250Areduced%2520by%2520considering%2520the%2520proposed%2520anchor%2520alignment%2520strategy.%2520Then%252C%2520by%2520the%250Adeep%2520unfolding%2520of%2520the%2520ADMM%2520iterations%252C%2520we%2520further%2520propose%2520the%2520Deep%2520CoSR-AA%250Aalgorithm%2520that%2520can%2520be%2520used%2520to%2520significantly%2520reduce%2520the%2520number%2520of%2520iterations.%2520We%250Aobtain%2520a%2520graph%2520neural%2520network%2520architecture%2520where%2520message%2520exchange%2520is%2520done%2520more%250Aefficiently%2520by%2520an%2520embedded%2520autoencoder.%2520Simulations%2520are%2520provided%2520to%2520demonstrate%250Athe%2520effectiveness%2520of%2520the%2520proposed%2520collaborative%2520recovery%2520algorithms%2520in%2520terms%2520of%250Athe%2520improved%2520reconstruction%2520quality%2520and%2520the%2520reduced%2520communication%2520overhead%2520due%250Ato%2520anchor%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compressed%20Sensor%20Caching%20and%20Collaborative%20Sparse%20Data%20Recovery%20with%0A%20%20Anchor%20Alignment&entry.906535625=Yi-Jen%20Yang%20and%20Ming-Hsun%20Yang%20and%20Jwo-Yuh%20Wu%20and%20Y.%20-W.%20Peter%20Hong&entry.1292438233=%20%20This%20work%20examines%20the%20compressed%20sensor%20caching%20problem%20in%20wireless%20sensor%0Anetworks%20and%20devises%20efficient%20distributed%20sparse%20data%20recovery%20algorithms%20to%0Aenable%20collaboration%20among%20multiple%20caches.%20In%20this%20problem%2C%20each%20cache%20is%20only%0Aallowed%20to%20access%20measurements%20from%20a%20small%20subset%20of%20sensors%20within%20its%0Avicinity%20to%20reduce%20both%20cache%20size%20and%20data%20acquisition%20overhead.%20To%20enable%0Areliable%20data%20recovery%20with%20limited%20access%20to%20measurements%2C%20we%20propose%20a%0Adistributed%20sparse%20data%20recovery%20method%2C%20called%20the%20collaborative%20sparse%0Arecovery%20by%20anchor%20alignment%20%28CoSR-AA%29%20algorithm%2C%20where%20collaboration%20among%0Acaches%20is%20enabled%20by%20aligning%20their%20locally%20recovered%20data%20at%20a%20few%20anchor%0Anodes.%20The%20proposed%20algorithm%20is%20based%20on%20the%20consensus%20alternating%20direction%0Amethod%20of%20multipliers%20%28ADMM%29%20algorithm%20but%20with%20message%20exchange%20that%20is%0Areduced%20by%20considering%20the%20proposed%20anchor%20alignment%20strategy.%20Then%2C%20by%20the%0Adeep%20unfolding%20of%20the%20ADMM%20iterations%2C%20we%20further%20propose%20the%20Deep%20CoSR-AA%0Aalgorithm%20that%20can%20be%20used%20to%20significantly%20reduce%20the%20number%20of%20iterations.%20We%0Aobtain%20a%20graph%20neural%20network%20architecture%20where%20message%20exchange%20is%20done%20more%0Aefficiently%20by%20an%20embedded%20autoencoder.%20Simulations%20are%20provided%20to%20demonstrate%0Athe%20effectiveness%20of%20the%20proposed%20collaborative%20recovery%20algorithms%20in%20terms%20of%0Athe%20improved%20reconstruction%20quality%20and%20the%20reduced%20communication%20overhead%20due%0Ato%20anchor%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10137v1&entry.124074799=Read"},
{"title": "Gradient Coding in Decentralized Learning for Evading Stragglers", "author": "Chengxi Li and Mikael Skoglund", "abstract": "  In this paper, we consider a decentralized learning problem in the presence\nof stragglers. Although gradient coding techniques have been developed for\ndistributed learning to evade stragglers, where the devices send encoded\ngradients with redundant training data, it is difficult to apply those\ntechniques directly to decentralized learning scenarios. To deal with this\nproblem, we propose a new gossip-based decentralized learning method with\ngradient coding (GOCO). In the proposed method, to avoid the negative impact of\nstragglers, the parameter vectors are updated locally using encoded gradients\nbased on the framework of stochastic gradient coding and then averaged in a\ngossip-based manner. We analyze the convergence performance of GOCO for\nstrongly convex loss functions. And we also provide simulation results to\ndemonstrate the superiority of the proposed method in terms of learning\nperformance compared with the baseline methods.\n", "link": "http://arxiv.org/abs/2402.04193v3", "date": "2024-06-14", "relevancy": 2.2876, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4716}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4567}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Coding%20in%20Decentralized%20Learning%20for%20Evading%20Stragglers&body=Title%3A%20Gradient%20Coding%20in%20Decentralized%20Learning%20for%20Evading%20Stragglers%0AAuthor%3A%20Chengxi%20Li%20and%20Mikael%20Skoglund%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20a%20decentralized%20learning%20problem%20in%20the%20presence%0Aof%20stragglers.%20Although%20gradient%20coding%20techniques%20have%20been%20developed%20for%0Adistributed%20learning%20to%20evade%20stragglers%2C%20where%20the%20devices%20send%20encoded%0Agradients%20with%20redundant%20training%20data%2C%20it%20is%20difficult%20to%20apply%20those%0Atechniques%20directly%20to%20decentralized%20learning%20scenarios.%20To%20deal%20with%20this%0Aproblem%2C%20we%20propose%20a%20new%20gossip-based%20decentralized%20learning%20method%20with%0Agradient%20coding%20%28GOCO%29.%20In%20the%20proposed%20method%2C%20to%20avoid%20the%20negative%20impact%20of%0Astragglers%2C%20the%20parameter%20vectors%20are%20updated%20locally%20using%20encoded%20gradients%0Abased%20on%20the%20framework%20of%20stochastic%20gradient%20coding%20and%20then%20averaged%20in%20a%0Agossip-based%20manner.%20We%20analyze%20the%20convergence%20performance%20of%20GOCO%20for%0Astrongly%20convex%20loss%20functions.%20And%20we%20also%20provide%20simulation%20results%20to%0Ademonstrate%20the%20superiority%20of%20the%20proposed%20method%20in%20terms%20of%20learning%0Aperformance%20compared%20with%20the%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04193v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Coding%2520in%2520Decentralized%2520Learning%2520for%2520Evading%2520Stragglers%26entry.906535625%3DChengxi%2520Li%2520and%2520Mikael%2520Skoglund%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520consider%2520a%2520decentralized%2520learning%2520problem%2520in%2520the%2520presence%250Aof%2520stragglers.%2520Although%2520gradient%2520coding%2520techniques%2520have%2520been%2520developed%2520for%250Adistributed%2520learning%2520to%2520evade%2520stragglers%252C%2520where%2520the%2520devices%2520send%2520encoded%250Agradients%2520with%2520redundant%2520training%2520data%252C%2520it%2520is%2520difficult%2520to%2520apply%2520those%250Atechniques%2520directly%2520to%2520decentralized%2520learning%2520scenarios.%2520To%2520deal%2520with%2520this%250Aproblem%252C%2520we%2520propose%2520a%2520new%2520gossip-based%2520decentralized%2520learning%2520method%2520with%250Agradient%2520coding%2520%2528GOCO%2529.%2520In%2520the%2520proposed%2520method%252C%2520to%2520avoid%2520the%2520negative%2520impact%2520of%250Astragglers%252C%2520the%2520parameter%2520vectors%2520are%2520updated%2520locally%2520using%2520encoded%2520gradients%250Abased%2520on%2520the%2520framework%2520of%2520stochastic%2520gradient%2520coding%2520and%2520then%2520averaged%2520in%2520a%250Agossip-based%2520manner.%2520We%2520analyze%2520the%2520convergence%2520performance%2520of%2520GOCO%2520for%250Astrongly%2520convex%2520loss%2520functions.%2520And%2520we%2520also%2520provide%2520simulation%2520results%2520to%250Ademonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520method%2520in%2520terms%2520of%2520learning%250Aperformance%2520compared%2520with%2520the%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04193v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Coding%20in%20Decentralized%20Learning%20for%20Evading%20Stragglers&entry.906535625=Chengxi%20Li%20and%20Mikael%20Skoglund&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20a%20decentralized%20learning%20problem%20in%20the%20presence%0Aof%20stragglers.%20Although%20gradient%20coding%20techniques%20have%20been%20developed%20for%0Adistributed%20learning%20to%20evade%20stragglers%2C%20where%20the%20devices%20send%20encoded%0Agradients%20with%20redundant%20training%20data%2C%20it%20is%20difficult%20to%20apply%20those%0Atechniques%20directly%20to%20decentralized%20learning%20scenarios.%20To%20deal%20with%20this%0Aproblem%2C%20we%20propose%20a%20new%20gossip-based%20decentralized%20learning%20method%20with%0Agradient%20coding%20%28GOCO%29.%20In%20the%20proposed%20method%2C%20to%20avoid%20the%20negative%20impact%20of%0Astragglers%2C%20the%20parameter%20vectors%20are%20updated%20locally%20using%20encoded%20gradients%0Abased%20on%20the%20framework%20of%20stochastic%20gradient%20coding%20and%20then%20averaged%20in%20a%0Agossip-based%20manner.%20We%20analyze%20the%20convergence%20performance%20of%20GOCO%20for%0Astrongly%20convex%20loss%20functions.%20And%20we%20also%20provide%20simulation%20results%20to%0Ademonstrate%20the%20superiority%20of%20the%20proposed%20method%20in%20terms%20of%20learning%0Aperformance%20compared%20with%20the%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04193v3&entry.124074799=Read"},
{"title": "Using an LLM to Turn Sign Spottings into Spoken Language Sentences", "author": "Ozge Mercanoglu Sincan and Necati Cihan Camgoz and Richard Bowden", "abstract": "  Sign Language Translation (SLT) is a challenging task that aims to generate\nspoken language sentences from sign language videos. In this paper, we\nintroduce a hybrid SLT approach, Spotter+GPT, that utilizes a sign spotter and\na powerful Large Language Model (LLM) to improve SLT performance. Spotter+GPT\nbreaks down the SLT task into two stages. The videos are first processed by the\nSpotter, which is trained on a linguistic sign language dataset, to identify\nindividual signs. These spotted signs are then passed to an LLM, which\ntransforms them into coherent and contextually appropriate spoken language\nsentences. The source code of the Spotter is available at\nhttps://gitlab.surrey.ac.uk/cogvispublic/sign-spotter.\n", "link": "http://arxiv.org/abs/2403.10434v2", "date": "2024-06-14", "relevancy": 2.2776, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4802}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4438}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20an%20LLM%20to%20Turn%20Sign%20Spottings%20into%20Spoken%20Language%20Sentences&body=Title%3A%20Using%20an%20LLM%20to%20Turn%20Sign%20Spottings%20into%20Spoken%20Language%20Sentences%0AAuthor%3A%20Ozge%20Mercanoglu%20Sincan%20and%20Necati%20Cihan%20Camgoz%20and%20Richard%20Bowden%0AAbstract%3A%20%20%20Sign%20Language%20Translation%20%28SLT%29%20is%20a%20challenging%20task%20that%20aims%20to%20generate%0Aspoken%20language%20sentences%20from%20sign%20language%20videos.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20hybrid%20SLT%20approach%2C%20Spotter%2BGPT%2C%20that%20utilizes%20a%20sign%20spotter%20and%0Aa%20powerful%20Large%20Language%20Model%20%28LLM%29%20to%20improve%20SLT%20performance.%20Spotter%2BGPT%0Abreaks%20down%20the%20SLT%20task%20into%20two%20stages.%20The%20videos%20are%20first%20processed%20by%20the%0ASpotter%2C%20which%20is%20trained%20on%20a%20linguistic%20sign%20language%20dataset%2C%20to%20identify%0Aindividual%20signs.%20These%20spotted%20signs%20are%20then%20passed%20to%20an%20LLM%2C%20which%0Atransforms%20them%20into%20coherent%20and%20contextually%20appropriate%20spoken%20language%0Asentences.%20The%20source%20code%20of%20the%20Spotter%20is%20available%20at%0Ahttps%3A//gitlab.surrey.ac.uk/cogvispublic/sign-spotter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10434v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520an%2520LLM%2520to%2520Turn%2520Sign%2520Spottings%2520into%2520Spoken%2520Language%2520Sentences%26entry.906535625%3DOzge%2520Mercanoglu%2520Sincan%2520and%2520Necati%2520Cihan%2520Camgoz%2520and%2520Richard%2520Bowden%26entry.1292438233%3D%2520%2520Sign%2520Language%2520Translation%2520%2528SLT%2529%2520is%2520a%2520challenging%2520task%2520that%2520aims%2520to%2520generate%250Aspoken%2520language%2520sentences%2520from%2520sign%2520language%2520videos.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520hybrid%2520SLT%2520approach%252C%2520Spotter%252BGPT%252C%2520that%2520utilizes%2520a%2520sign%2520spotter%2520and%250Aa%2520powerful%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520to%2520improve%2520SLT%2520performance.%2520Spotter%252BGPT%250Abreaks%2520down%2520the%2520SLT%2520task%2520into%2520two%2520stages.%2520The%2520videos%2520are%2520first%2520processed%2520by%2520the%250ASpotter%252C%2520which%2520is%2520trained%2520on%2520a%2520linguistic%2520sign%2520language%2520dataset%252C%2520to%2520identify%250Aindividual%2520signs.%2520These%2520spotted%2520signs%2520are%2520then%2520passed%2520to%2520an%2520LLM%252C%2520which%250Atransforms%2520them%2520into%2520coherent%2520and%2520contextually%2520appropriate%2520spoken%2520language%250Asentences.%2520The%2520source%2520code%2520of%2520the%2520Spotter%2520is%2520available%2520at%250Ahttps%253A//gitlab.surrey.ac.uk/cogvispublic/sign-spotter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10434v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20an%20LLM%20to%20Turn%20Sign%20Spottings%20into%20Spoken%20Language%20Sentences&entry.906535625=Ozge%20Mercanoglu%20Sincan%20and%20Necati%20Cihan%20Camgoz%20and%20Richard%20Bowden&entry.1292438233=%20%20Sign%20Language%20Translation%20%28SLT%29%20is%20a%20challenging%20task%20that%20aims%20to%20generate%0Aspoken%20language%20sentences%20from%20sign%20language%20videos.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20hybrid%20SLT%20approach%2C%20Spotter%2BGPT%2C%20that%20utilizes%20a%20sign%20spotter%20and%0Aa%20powerful%20Large%20Language%20Model%20%28LLM%29%20to%20improve%20SLT%20performance.%20Spotter%2BGPT%0Abreaks%20down%20the%20SLT%20task%20into%20two%20stages.%20The%20videos%20are%20first%20processed%20by%20the%0ASpotter%2C%20which%20is%20trained%20on%20a%20linguistic%20sign%20language%20dataset%2C%20to%20identify%0Aindividual%20signs.%20These%20spotted%20signs%20are%20then%20passed%20to%20an%20LLM%2C%20which%0Atransforms%20them%20into%20coherent%20and%20contextually%20appropriate%20spoken%20language%0Asentences.%20The%20source%20code%20of%20the%20Spotter%20is%20available%20at%0Ahttps%3A//gitlab.surrey.ac.uk/cogvispublic/sign-spotter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10434v2&entry.124074799=Read"},
{"title": "PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory\n  Planner", "author": "Kota Kondo and Claudius T. Tewari and Andrea Tagliabue and Jesus Tordesillas and Parker C. Lusk and Jonathan P. How", "abstract": "  In decentralized multiagent trajectory planners, agents need to communicate\nand exchange their positions to generate collision-free trajectories. However,\ndue to localization errors/uncertainties, trajectory deconfliction can fail\neven if trajectories are perfectly shared between agents. To address this\nissue, we first present PARM and PARM*, perception-aware, decentralized,\nasynchronous multiagent trajectory planners that enable a team of agents to\nnavigate uncertain environments while deconflicting trajectories and avoiding\nobstacles using perception information. PARM* differs from PARM as it is less\nconservative, using more computation to find closer-to-optimal solutions. While\nthese methods achieve state-of-the-art performance, they suffer from high\ncomputational costs as they need to solve large optimization problems onboard,\nmaking it difficult for agents to replan at high rates. To overcome this\nchallenge, we present our second key contribution, PRIMER, a learning-based\nplanner trained with imitation learning (IL) using PARM* as the expert\ndemonstrator. PRIMER leverages the low computational requirements at deployment\nof neural networks and achieves a computation speed up to 5500 times faster\nthan optimization-based approaches.\n", "link": "http://arxiv.org/abs/2406.10060v1", "date": "2024-06-14", "relevancy": 2.2752, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.583}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5775}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRIMER%3A%20Perception-Aware%20Robust%20Learning-based%20Multiagent%20Trajectory%0A%20%20Planner&body=Title%3A%20PRIMER%3A%20Perception-Aware%20Robust%20Learning-based%20Multiagent%20Trajectory%0A%20%20Planner%0AAuthor%3A%20Kota%20Kondo%20and%20Claudius%20T.%20Tewari%20and%20Andrea%20Tagliabue%20and%20Jesus%20Tordesillas%20and%20Parker%20C.%20Lusk%20and%20Jonathan%20P.%20How%0AAbstract%3A%20%20%20In%20decentralized%20multiagent%20trajectory%20planners%2C%20agents%20need%20to%20communicate%0Aand%20exchange%20their%20positions%20to%20generate%20collision-free%20trajectories.%20However%2C%0Adue%20to%20localization%20errors/uncertainties%2C%20trajectory%20deconfliction%20can%20fail%0Aeven%20if%20trajectories%20are%20perfectly%20shared%20between%20agents.%20To%20address%20this%0Aissue%2C%20we%20first%20present%20PARM%20and%20PARM%2A%2C%20perception-aware%2C%20decentralized%2C%0Aasynchronous%20multiagent%20trajectory%20planners%20that%20enable%20a%20team%20of%20agents%20to%0Anavigate%20uncertain%20environments%20while%20deconflicting%20trajectories%20and%20avoiding%0Aobstacles%20using%20perception%20information.%20PARM%2A%20differs%20from%20PARM%20as%20it%20is%20less%0Aconservative%2C%20using%20more%20computation%20to%20find%20closer-to-optimal%20solutions.%20While%0Athese%20methods%20achieve%20state-of-the-art%20performance%2C%20they%20suffer%20from%20high%0Acomputational%20costs%20as%20they%20need%20to%20solve%20large%20optimization%20problems%20onboard%2C%0Amaking%20it%20difficult%20for%20agents%20to%20replan%20at%20high%20rates.%20To%20overcome%20this%0Achallenge%2C%20we%20present%20our%20second%20key%20contribution%2C%20PRIMER%2C%20a%20learning-based%0Aplanner%20trained%20with%20imitation%20learning%20%28IL%29%20using%20PARM%2A%20as%20the%20expert%0Ademonstrator.%20PRIMER%20leverages%20the%20low%20computational%20requirements%20at%20deployment%0Aof%20neural%20networks%20and%20achieves%20a%20computation%20speed%20up%20to%205500%20times%20faster%0Athan%20optimization-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRIMER%253A%2520Perception-Aware%2520Robust%2520Learning-based%2520Multiagent%2520Trajectory%250A%2520%2520Planner%26entry.906535625%3DKota%2520Kondo%2520and%2520Claudius%2520T.%2520Tewari%2520and%2520Andrea%2520Tagliabue%2520and%2520Jesus%2520Tordesillas%2520and%2520Parker%2520C.%2520Lusk%2520and%2520Jonathan%2520P.%2520How%26entry.1292438233%3D%2520%2520In%2520decentralized%2520multiagent%2520trajectory%2520planners%252C%2520agents%2520need%2520to%2520communicate%250Aand%2520exchange%2520their%2520positions%2520to%2520generate%2520collision-free%2520trajectories.%2520However%252C%250Adue%2520to%2520localization%2520errors/uncertainties%252C%2520trajectory%2520deconfliction%2520can%2520fail%250Aeven%2520if%2520trajectories%2520are%2520perfectly%2520shared%2520between%2520agents.%2520To%2520address%2520this%250Aissue%252C%2520we%2520first%2520present%2520PARM%2520and%2520PARM%252A%252C%2520perception-aware%252C%2520decentralized%252C%250Aasynchronous%2520multiagent%2520trajectory%2520planners%2520that%2520enable%2520a%2520team%2520of%2520agents%2520to%250Anavigate%2520uncertain%2520environments%2520while%2520deconflicting%2520trajectories%2520and%2520avoiding%250Aobstacles%2520using%2520perception%2520information.%2520PARM%252A%2520differs%2520from%2520PARM%2520as%2520it%2520is%2520less%250Aconservative%252C%2520using%2520more%2520computation%2520to%2520find%2520closer-to-optimal%2520solutions.%2520While%250Athese%2520methods%2520achieve%2520state-of-the-art%2520performance%252C%2520they%2520suffer%2520from%2520high%250Acomputational%2520costs%2520as%2520they%2520need%2520to%2520solve%2520large%2520optimization%2520problems%2520onboard%252C%250Amaking%2520it%2520difficult%2520for%2520agents%2520to%2520replan%2520at%2520high%2520rates.%2520To%2520overcome%2520this%250Achallenge%252C%2520we%2520present%2520our%2520second%2520key%2520contribution%252C%2520PRIMER%252C%2520a%2520learning-based%250Aplanner%2520trained%2520with%2520imitation%2520learning%2520%2528IL%2529%2520using%2520PARM%252A%2520as%2520the%2520expert%250Ademonstrator.%2520PRIMER%2520leverages%2520the%2520low%2520computational%2520requirements%2520at%2520deployment%250Aof%2520neural%2520networks%2520and%2520achieves%2520a%2520computation%2520speed%2520up%2520to%25205500%2520times%2520faster%250Athan%2520optimization-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRIMER%3A%20Perception-Aware%20Robust%20Learning-based%20Multiagent%20Trajectory%0A%20%20Planner&entry.906535625=Kota%20Kondo%20and%20Claudius%20T.%20Tewari%20and%20Andrea%20Tagliabue%20and%20Jesus%20Tordesillas%20and%20Parker%20C.%20Lusk%20and%20Jonathan%20P.%20How&entry.1292438233=%20%20In%20decentralized%20multiagent%20trajectory%20planners%2C%20agents%20need%20to%20communicate%0Aand%20exchange%20their%20positions%20to%20generate%20collision-free%20trajectories.%20However%2C%0Adue%20to%20localization%20errors/uncertainties%2C%20trajectory%20deconfliction%20can%20fail%0Aeven%20if%20trajectories%20are%20perfectly%20shared%20between%20agents.%20To%20address%20this%0Aissue%2C%20we%20first%20present%20PARM%20and%20PARM%2A%2C%20perception-aware%2C%20decentralized%2C%0Aasynchronous%20multiagent%20trajectory%20planners%20that%20enable%20a%20team%20of%20agents%20to%0Anavigate%20uncertain%20environments%20while%20deconflicting%20trajectories%20and%20avoiding%0Aobstacles%20using%20perception%20information.%20PARM%2A%20differs%20from%20PARM%20as%20it%20is%20less%0Aconservative%2C%20using%20more%20computation%20to%20find%20closer-to-optimal%20solutions.%20While%0Athese%20methods%20achieve%20state-of-the-art%20performance%2C%20they%20suffer%20from%20high%0Acomputational%20costs%20as%20they%20need%20to%20solve%20large%20optimization%20problems%20onboard%2C%0Amaking%20it%20difficult%20for%20agents%20to%20replan%20at%20high%20rates.%20To%20overcome%20this%0Achallenge%2C%20we%20present%20our%20second%20key%20contribution%2C%20PRIMER%2C%20a%20learning-based%0Aplanner%20trained%20with%20imitation%20learning%20%28IL%29%20using%20PARM%2A%20as%20the%20expert%0Ademonstrator.%20PRIMER%20leverages%20the%20low%20computational%20requirements%20at%20deployment%0Aof%20neural%20networks%20and%20achieves%20a%20computation%20speed%20up%20to%205500%20times%20faster%0Athan%20optimization-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10060v1&entry.124074799=Read"},
{"title": "YOLOv1 to YOLOv10: A comprehensive review of YOLO variants and their\n  application in the agricultural domain", "author": "Mujadded Al Rabbani Alif and Muhammad Hussain", "abstract": "  This survey investigates the transformative potential of various YOLO\nvariants, from YOLOv1 to the state-of-the-art YOLOv10, in the context of\nagricultural advancements. The primary objective is to elucidate how these\ncutting-edge object detection models can re-energise and optimize diverse\naspects of agriculture, ranging from crop monitoring to livestock management.\nIt aims to achieve key objectives, including the identification of contemporary\nchallenges in agriculture, a detailed assessment of YOLO's incremental\nadvancements, and an exploration of its specific applications in agriculture.\nThis is one of the first surveys to include the latest YOLOv10, offering a\nfresh perspective on its implications for precision farming and sustainable\nagricultural practices in the era of Artificial Intelligence and automation.\nFurther, the survey undertakes a critical analysis of YOLO's performance,\nsynthesizes existing research, and projects future trends. By scrutinizing the\nunique capabilities packed in YOLO variants and their real-world applications,\nthis survey provides valuable insights into the evolving relationship between\nYOLO variants and agriculture. The findings contribute towards a nuanced\nunderstanding of the potential for precision farming and sustainable\nagricultural practices, marking a significant step forward in the integration\nof advanced object detection technologies within the agricultural sector.\n", "link": "http://arxiv.org/abs/2406.10139v1", "date": "2024-06-14", "relevancy": 2.2628, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4615}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4534}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOLOv1%20to%20YOLOv10%3A%20A%20comprehensive%20review%20of%20YOLO%20variants%20and%20their%0A%20%20application%20in%20the%20agricultural%20domain&body=Title%3A%20YOLOv1%20to%20YOLOv10%3A%20A%20comprehensive%20review%20of%20YOLO%20variants%20and%20their%0A%20%20application%20in%20the%20agricultural%20domain%0AAuthor%3A%20Mujadded%20Al%20Rabbani%20Alif%20and%20Muhammad%20Hussain%0AAbstract%3A%20%20%20This%20survey%20investigates%20the%20transformative%20potential%20of%20various%20YOLO%0Avariants%2C%20from%20YOLOv1%20to%20the%20state-of-the-art%20YOLOv10%2C%20in%20the%20context%20of%0Aagricultural%20advancements.%20The%20primary%20objective%20is%20to%20elucidate%20how%20these%0Acutting-edge%20object%20detection%20models%20can%20re-energise%20and%20optimize%20diverse%0Aaspects%20of%20agriculture%2C%20ranging%20from%20crop%20monitoring%20to%20livestock%20management.%0AIt%20aims%20to%20achieve%20key%20objectives%2C%20including%20the%20identification%20of%20contemporary%0Achallenges%20in%20agriculture%2C%20a%20detailed%20assessment%20of%20YOLO%27s%20incremental%0Aadvancements%2C%20and%20an%20exploration%20of%20its%20specific%20applications%20in%20agriculture.%0AThis%20is%20one%20of%20the%20first%20surveys%20to%20include%20the%20latest%20YOLOv10%2C%20offering%20a%0Afresh%20perspective%20on%20its%20implications%20for%20precision%20farming%20and%20sustainable%0Aagricultural%20practices%20in%20the%20era%20of%20Artificial%20Intelligence%20and%20automation.%0AFurther%2C%20the%20survey%20undertakes%20a%20critical%20analysis%20of%20YOLO%27s%20performance%2C%0Asynthesizes%20existing%20research%2C%20and%20projects%20future%20trends.%20By%20scrutinizing%20the%0Aunique%20capabilities%20packed%20in%20YOLO%20variants%20and%20their%20real-world%20applications%2C%0Athis%20survey%20provides%20valuable%20insights%20into%20the%20evolving%20relationship%20between%0AYOLO%20variants%20and%20agriculture.%20The%20findings%20contribute%20towards%20a%20nuanced%0Aunderstanding%20of%20the%20potential%20for%20precision%20farming%20and%20sustainable%0Aagricultural%20practices%2C%20marking%20a%20significant%20step%20forward%20in%20the%20integration%0Aof%20advanced%20object%20detection%20technologies%20within%20the%20agricultural%20sector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOLOv1%2520to%2520YOLOv10%253A%2520A%2520comprehensive%2520review%2520of%2520YOLO%2520variants%2520and%2520their%250A%2520%2520application%2520in%2520the%2520agricultural%2520domain%26entry.906535625%3DMujadded%2520Al%2520Rabbani%2520Alif%2520and%2520Muhammad%2520Hussain%26entry.1292438233%3D%2520%2520This%2520survey%2520investigates%2520the%2520transformative%2520potential%2520of%2520various%2520YOLO%250Avariants%252C%2520from%2520YOLOv1%2520to%2520the%2520state-of-the-art%2520YOLOv10%252C%2520in%2520the%2520context%2520of%250Aagricultural%2520advancements.%2520The%2520primary%2520objective%2520is%2520to%2520elucidate%2520how%2520these%250Acutting-edge%2520object%2520detection%2520models%2520can%2520re-energise%2520and%2520optimize%2520diverse%250Aaspects%2520of%2520agriculture%252C%2520ranging%2520from%2520crop%2520monitoring%2520to%2520livestock%2520management.%250AIt%2520aims%2520to%2520achieve%2520key%2520objectives%252C%2520including%2520the%2520identification%2520of%2520contemporary%250Achallenges%2520in%2520agriculture%252C%2520a%2520detailed%2520assessment%2520of%2520YOLO%2527s%2520incremental%250Aadvancements%252C%2520and%2520an%2520exploration%2520of%2520its%2520specific%2520applications%2520in%2520agriculture.%250AThis%2520is%2520one%2520of%2520the%2520first%2520surveys%2520to%2520include%2520the%2520latest%2520YOLOv10%252C%2520offering%2520a%250Afresh%2520perspective%2520on%2520its%2520implications%2520for%2520precision%2520farming%2520and%2520sustainable%250Aagricultural%2520practices%2520in%2520the%2520era%2520of%2520Artificial%2520Intelligence%2520and%2520automation.%250AFurther%252C%2520the%2520survey%2520undertakes%2520a%2520critical%2520analysis%2520of%2520YOLO%2527s%2520performance%252C%250Asynthesizes%2520existing%2520research%252C%2520and%2520projects%2520future%2520trends.%2520By%2520scrutinizing%2520the%250Aunique%2520capabilities%2520packed%2520in%2520YOLO%2520variants%2520and%2520their%2520real-world%2520applications%252C%250Athis%2520survey%2520provides%2520valuable%2520insights%2520into%2520the%2520evolving%2520relationship%2520between%250AYOLO%2520variants%2520and%2520agriculture.%2520The%2520findings%2520contribute%2520towards%2520a%2520nuanced%250Aunderstanding%2520of%2520the%2520potential%2520for%2520precision%2520farming%2520and%2520sustainable%250Aagricultural%2520practices%252C%2520marking%2520a%2520significant%2520step%2520forward%2520in%2520the%2520integration%250Aof%2520advanced%2520object%2520detection%2520technologies%2520within%2520the%2520agricultural%2520sector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLOv1%20to%20YOLOv10%3A%20A%20comprehensive%20review%20of%20YOLO%20variants%20and%20their%0A%20%20application%20in%20the%20agricultural%20domain&entry.906535625=Mujadded%20Al%20Rabbani%20Alif%20and%20Muhammad%20Hussain&entry.1292438233=%20%20This%20survey%20investigates%20the%20transformative%20potential%20of%20various%20YOLO%0Avariants%2C%20from%20YOLOv1%20to%20the%20state-of-the-art%20YOLOv10%2C%20in%20the%20context%20of%0Aagricultural%20advancements.%20The%20primary%20objective%20is%20to%20elucidate%20how%20these%0Acutting-edge%20object%20detection%20models%20can%20re-energise%20and%20optimize%20diverse%0Aaspects%20of%20agriculture%2C%20ranging%20from%20crop%20monitoring%20to%20livestock%20management.%0AIt%20aims%20to%20achieve%20key%20objectives%2C%20including%20the%20identification%20of%20contemporary%0Achallenges%20in%20agriculture%2C%20a%20detailed%20assessment%20of%20YOLO%27s%20incremental%0Aadvancements%2C%20and%20an%20exploration%20of%20its%20specific%20applications%20in%20agriculture.%0AThis%20is%20one%20of%20the%20first%20surveys%20to%20include%20the%20latest%20YOLOv10%2C%20offering%20a%0Afresh%20perspective%20on%20its%20implications%20for%20precision%20farming%20and%20sustainable%0Aagricultural%20practices%20in%20the%20era%20of%20Artificial%20Intelligence%20and%20automation.%0AFurther%2C%20the%20survey%20undertakes%20a%20critical%20analysis%20of%20YOLO%27s%20performance%2C%0Asynthesizes%20existing%20research%2C%20and%20projects%20future%20trends.%20By%20scrutinizing%20the%0Aunique%20capabilities%20packed%20in%20YOLO%20variants%20and%20their%20real-world%20applications%2C%0Athis%20survey%20provides%20valuable%20insights%20into%20the%20evolving%20relationship%20between%0AYOLO%20variants%20and%20agriculture.%20The%20findings%20contribute%20towards%20a%20nuanced%0Aunderstanding%20of%20the%20potential%20for%20precision%20farming%20and%20sustainable%0Aagricultural%20practices%2C%20marking%20a%20significant%20step%20forward%20in%20the%20integration%0Aof%20advanced%20object%20detection%20technologies%20within%20the%20agricultural%20sector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10139v1&entry.124074799=Read"},
{"title": "An Empirical Study Into What Matters for Calibrating Vision-Language\n  Models", "author": "Weijie Tu and Weijian Deng and Dylan Campbell and Stephen Gould and Tom Gedeon", "abstract": "  Vision-Language Models (VLMs) have emerged as the dominant approach for\nzero-shot recognition, adept at handling diverse scenarios and significant\ndistribution changes. However, their deployment in risk-sensitive areas\nrequires a deeper understanding of their uncertainty estimation capabilities, a\nrelatively uncharted area. In this study, we explore the calibration properties\nof VLMs across different architectures, datasets, and training strategies. In\nparticular, we analyze the uncertainty estimation performance of VLMs when\ncalibrated in one domain, label set or hierarchy level, and tested in a\ndifferent one. Our findings reveal that while VLMs are not inherently\ncalibrated for uncertainty, temperature scaling significantly and consistently\nimproves calibration, even across shifts in distribution and changes in label\nset. Moreover, VLMs can be calibrated with a very small set of examples.\nThrough detailed experimentation, we highlight the potential applications and\nimportance of our insights, aiming for more reliable and effective use of VLMs\nin critical, real-world scenarios.\n", "link": "http://arxiv.org/abs/2402.07417v2", "date": "2024-06-14", "relevancy": 2.2583, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5859}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5755}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirical%20Study%20Into%20What%20Matters%20for%20Calibrating%20Vision-Language%0A%20%20Models&body=Title%3A%20An%20Empirical%20Study%20Into%20What%20Matters%20for%20Calibrating%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Weijie%20Tu%20and%20Weijian%20Deng%20and%20Dylan%20Campbell%20and%20Stephen%20Gould%20and%20Tom%20Gedeon%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20emerged%20as%20the%20dominant%20approach%20for%0Azero-shot%20recognition%2C%20adept%20at%20handling%20diverse%20scenarios%20and%20significant%0Adistribution%20changes.%20However%2C%20their%20deployment%20in%20risk-sensitive%20areas%0Arequires%20a%20deeper%20understanding%20of%20their%20uncertainty%20estimation%20capabilities%2C%20a%0Arelatively%20uncharted%20area.%20In%20this%20study%2C%20we%20explore%20the%20calibration%20properties%0Aof%20VLMs%20across%20different%20architectures%2C%20datasets%2C%20and%20training%20strategies.%20In%0Aparticular%2C%20we%20analyze%20the%20uncertainty%20estimation%20performance%20of%20VLMs%20when%0Acalibrated%20in%20one%20domain%2C%20label%20set%20or%20hierarchy%20level%2C%20and%20tested%20in%20a%0Adifferent%20one.%20Our%20findings%20reveal%20that%20while%20VLMs%20are%20not%20inherently%0Acalibrated%20for%20uncertainty%2C%20temperature%20scaling%20significantly%20and%20consistently%0Aimproves%20calibration%2C%20even%20across%20shifts%20in%20distribution%20and%20changes%20in%20label%0Aset.%20Moreover%2C%20VLMs%20can%20be%20calibrated%20with%20a%20very%20small%20set%20of%20examples.%0AThrough%20detailed%20experimentation%2C%20we%20highlight%20the%20potential%20applications%20and%0Aimportance%20of%20our%20insights%2C%20aiming%20for%20more%20reliable%20and%20effective%20use%20of%20VLMs%0Ain%20critical%2C%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07417v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirical%2520Study%2520Into%2520What%2520Matters%2520for%2520Calibrating%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DWeijie%2520Tu%2520and%2520Weijian%2520Deng%2520and%2520Dylan%2520Campbell%2520and%2520Stephen%2520Gould%2520and%2520Tom%2520Gedeon%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520emerged%2520as%2520the%2520dominant%2520approach%2520for%250Azero-shot%2520recognition%252C%2520adept%2520at%2520handling%2520diverse%2520scenarios%2520and%2520significant%250Adistribution%2520changes.%2520However%252C%2520their%2520deployment%2520in%2520risk-sensitive%2520areas%250Arequires%2520a%2520deeper%2520understanding%2520of%2520their%2520uncertainty%2520estimation%2520capabilities%252C%2520a%250Arelatively%2520uncharted%2520area.%2520In%2520this%2520study%252C%2520we%2520explore%2520the%2520calibration%2520properties%250Aof%2520VLMs%2520across%2520different%2520architectures%252C%2520datasets%252C%2520and%2520training%2520strategies.%2520In%250Aparticular%252C%2520we%2520analyze%2520the%2520uncertainty%2520estimation%2520performance%2520of%2520VLMs%2520when%250Acalibrated%2520in%2520one%2520domain%252C%2520label%2520set%2520or%2520hierarchy%2520level%252C%2520and%2520tested%2520in%2520a%250Adifferent%2520one.%2520Our%2520findings%2520reveal%2520that%2520while%2520VLMs%2520are%2520not%2520inherently%250Acalibrated%2520for%2520uncertainty%252C%2520temperature%2520scaling%2520significantly%2520and%2520consistently%250Aimproves%2520calibration%252C%2520even%2520across%2520shifts%2520in%2520distribution%2520and%2520changes%2520in%2520label%250Aset.%2520Moreover%252C%2520VLMs%2520can%2520be%2520calibrated%2520with%2520a%2520very%2520small%2520set%2520of%2520examples.%250AThrough%2520detailed%2520experimentation%252C%2520we%2520highlight%2520the%2520potential%2520applications%2520and%250Aimportance%2520of%2520our%2520insights%252C%2520aiming%2520for%2520more%2520reliable%2520and%2520effective%2520use%2520of%2520VLMs%250Ain%2520critical%252C%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07417v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Study%20Into%20What%20Matters%20for%20Calibrating%20Vision-Language%0A%20%20Models&entry.906535625=Weijie%20Tu%20and%20Weijian%20Deng%20and%20Dylan%20Campbell%20and%20Stephen%20Gould%20and%20Tom%20Gedeon&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20emerged%20as%20the%20dominant%20approach%20for%0Azero-shot%20recognition%2C%20adept%20at%20handling%20diverse%20scenarios%20and%20significant%0Adistribution%20changes.%20However%2C%20their%20deployment%20in%20risk-sensitive%20areas%0Arequires%20a%20deeper%20understanding%20of%20their%20uncertainty%20estimation%20capabilities%2C%20a%0Arelatively%20uncharted%20area.%20In%20this%20study%2C%20we%20explore%20the%20calibration%20properties%0Aof%20VLMs%20across%20different%20architectures%2C%20datasets%2C%20and%20training%20strategies.%20In%0Aparticular%2C%20we%20analyze%20the%20uncertainty%20estimation%20performance%20of%20VLMs%20when%0Acalibrated%20in%20one%20domain%2C%20label%20set%20or%20hierarchy%20level%2C%20and%20tested%20in%20a%0Adifferent%20one.%20Our%20findings%20reveal%20that%20while%20VLMs%20are%20not%20inherently%0Acalibrated%20for%20uncertainty%2C%20temperature%20scaling%20significantly%20and%20consistently%0Aimproves%20calibration%2C%20even%20across%20shifts%20in%20distribution%20and%20changes%20in%20label%0Aset.%20Moreover%2C%20VLMs%20can%20be%20calibrated%20with%20a%20very%20small%20set%20of%20examples.%0AThrough%20detailed%20experimentation%2C%20we%20highlight%20the%20potential%20applications%20and%0Aimportance%20of%20our%20insights%2C%20aiming%20for%20more%20reliable%20and%20effective%20use%20of%20VLMs%0Ain%20critical%2C%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07417v2&entry.124074799=Read"},
{"title": "Real-time, accurate, and open source upper-limb musculoskeletal analysis\n  using a single RGBD camera", "author": "Amedeo Ceglia and Kael Facon and Micka\u00ebl Begon and Lama Seoud", "abstract": "  Biomechanical biofeedback may enhance rehabilitation and provide clinicians\nwith more objective task evaluation. These feedbacks often rely on expensive\nmotion capture systems, which restricts their widespread use, leading to the\ndevelopment of computer vision-based methods. These methods are subject to\nlarge joint angle errors, considering the upper limb, and exclude the scapula\nand clavicle motion in the analysis. Our open-source approach offers a\nuser-friendly solution for high-fidelity upper-limb kinematics using a single\nlow-cost RGBD camera and includes semi-automatic skin marker labeling.\nReal-time biomechanical analysis, ranging from kinematics to muscle force\nestimation, was conducted on eight participants performing a hand-cycling\nmotion to demonstrate the applicability of our approach on the upper limb.\nMarkers were recorded by the RGBD camera and an optoelectronic camera system,\nconsidered as a reference. Muscle activity and external load were recorded\nusing eight EMG and instrumented hand pedals, respectively. Bland-Altman\nanalysis revealed significant agreements in the 3D markers' positions between\nthe two motion capture methods, with errors averaging 3.3$\\pm$3.9 mm. For the\nbiomechanical analysis, the level of agreement was sensitive to whether the\nsame marker set was used. For example, joint angle differences averaging\n2.3$\\pm$2.8{\\deg} when using the same marker set, compared to 4.5$\\pm$2.9{\\deg}\notherwise. Biofeedback from the RGBD camera was provided at 63 Hz. Our study\nintroduces a novel method for using an RGBD camera as a low-cost motion capture\nsolution, emphasizing its potential for accurate kinematic reconstruction and\ncomprehensive upper-limb biomechanical studies.\n", "link": "http://arxiv.org/abs/2406.10007v1", "date": "2024-06-14", "relevancy": 2.2582, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.57}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5662}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%2C%20accurate%2C%20and%20open%20source%20upper-limb%20musculoskeletal%20analysis%0A%20%20using%20a%20single%20RGBD%20camera&body=Title%3A%20Real-time%2C%20accurate%2C%20and%20open%20source%20upper-limb%20musculoskeletal%20analysis%0A%20%20using%20a%20single%20RGBD%20camera%0AAuthor%3A%20Amedeo%20Ceglia%20and%20Kael%20Facon%20and%20Micka%C3%ABl%20Begon%20and%20Lama%20Seoud%0AAbstract%3A%20%20%20Biomechanical%20biofeedback%20may%20enhance%20rehabilitation%20and%20provide%20clinicians%0Awith%20more%20objective%20task%20evaluation.%20These%20feedbacks%20often%20rely%20on%20expensive%0Amotion%20capture%20systems%2C%20which%20restricts%20their%20widespread%20use%2C%20leading%20to%20the%0Adevelopment%20of%20computer%20vision-based%20methods.%20These%20methods%20are%20subject%20to%0Alarge%20joint%20angle%20errors%2C%20considering%20the%20upper%20limb%2C%20and%20exclude%20the%20scapula%0Aand%20clavicle%20motion%20in%20the%20analysis.%20Our%20open-source%20approach%20offers%20a%0Auser-friendly%20solution%20for%20high-fidelity%20upper-limb%20kinematics%20using%20a%20single%0Alow-cost%20RGBD%20camera%20and%20includes%20semi-automatic%20skin%20marker%20labeling.%0AReal-time%20biomechanical%20analysis%2C%20ranging%20from%20kinematics%20to%20muscle%20force%0Aestimation%2C%20was%20conducted%20on%20eight%20participants%20performing%20a%20hand-cycling%0Amotion%20to%20demonstrate%20the%20applicability%20of%20our%20approach%20on%20the%20upper%20limb.%0AMarkers%20were%20recorded%20by%20the%20RGBD%20camera%20and%20an%20optoelectronic%20camera%20system%2C%0Aconsidered%20as%20a%20reference.%20Muscle%20activity%20and%20external%20load%20were%20recorded%0Ausing%20eight%20EMG%20and%20instrumented%20hand%20pedals%2C%20respectively.%20Bland-Altman%0Aanalysis%20revealed%20significant%20agreements%20in%20the%203D%20markers%27%20positions%20between%0Athe%20two%20motion%20capture%20methods%2C%20with%20errors%20averaging%203.3%24%5Cpm%243.9%20mm.%20For%20the%0Abiomechanical%20analysis%2C%20the%20level%20of%20agreement%20was%20sensitive%20to%20whether%20the%0Asame%20marker%20set%20was%20used.%20For%20example%2C%20joint%20angle%20differences%20averaging%0A2.3%24%5Cpm%242.8%7B%5Cdeg%7D%20when%20using%20the%20same%20marker%20set%2C%20compared%20to%204.5%24%5Cpm%242.9%7B%5Cdeg%7D%0Aotherwise.%20Biofeedback%20from%20the%20RGBD%20camera%20was%20provided%20at%2063%20Hz.%20Our%20study%0Aintroduces%20a%20novel%20method%20for%20using%20an%20RGBD%20camera%20as%20a%20low-cost%20motion%20capture%0Asolution%2C%20emphasizing%20its%20potential%20for%20accurate%20kinematic%20reconstruction%20and%0Acomprehensive%20upper-limb%20biomechanical%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10007v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%252C%2520accurate%252C%2520and%2520open%2520source%2520upper-limb%2520musculoskeletal%2520analysis%250A%2520%2520using%2520a%2520single%2520RGBD%2520camera%26entry.906535625%3DAmedeo%2520Ceglia%2520and%2520Kael%2520Facon%2520and%2520Micka%25C3%25ABl%2520Begon%2520and%2520Lama%2520Seoud%26entry.1292438233%3D%2520%2520Biomechanical%2520biofeedback%2520may%2520enhance%2520rehabilitation%2520and%2520provide%2520clinicians%250Awith%2520more%2520objective%2520task%2520evaluation.%2520These%2520feedbacks%2520often%2520rely%2520on%2520expensive%250Amotion%2520capture%2520systems%252C%2520which%2520restricts%2520their%2520widespread%2520use%252C%2520leading%2520to%2520the%250Adevelopment%2520of%2520computer%2520vision-based%2520methods.%2520These%2520methods%2520are%2520subject%2520to%250Alarge%2520joint%2520angle%2520errors%252C%2520considering%2520the%2520upper%2520limb%252C%2520and%2520exclude%2520the%2520scapula%250Aand%2520clavicle%2520motion%2520in%2520the%2520analysis.%2520Our%2520open-source%2520approach%2520offers%2520a%250Auser-friendly%2520solution%2520for%2520high-fidelity%2520upper-limb%2520kinematics%2520using%2520a%2520single%250Alow-cost%2520RGBD%2520camera%2520and%2520includes%2520semi-automatic%2520skin%2520marker%2520labeling.%250AReal-time%2520biomechanical%2520analysis%252C%2520ranging%2520from%2520kinematics%2520to%2520muscle%2520force%250Aestimation%252C%2520was%2520conducted%2520on%2520eight%2520participants%2520performing%2520a%2520hand-cycling%250Amotion%2520to%2520demonstrate%2520the%2520applicability%2520of%2520our%2520approach%2520on%2520the%2520upper%2520limb.%250AMarkers%2520were%2520recorded%2520by%2520the%2520RGBD%2520camera%2520and%2520an%2520optoelectronic%2520camera%2520system%252C%250Aconsidered%2520as%2520a%2520reference.%2520Muscle%2520activity%2520and%2520external%2520load%2520were%2520recorded%250Ausing%2520eight%2520EMG%2520and%2520instrumented%2520hand%2520pedals%252C%2520respectively.%2520Bland-Altman%250Aanalysis%2520revealed%2520significant%2520agreements%2520in%2520the%25203D%2520markers%2527%2520positions%2520between%250Athe%2520two%2520motion%2520capture%2520methods%252C%2520with%2520errors%2520averaging%25203.3%2524%255Cpm%25243.9%2520mm.%2520For%2520the%250Abiomechanical%2520analysis%252C%2520the%2520level%2520of%2520agreement%2520was%2520sensitive%2520to%2520whether%2520the%250Asame%2520marker%2520set%2520was%2520used.%2520For%2520example%252C%2520joint%2520angle%2520differences%2520averaging%250A2.3%2524%255Cpm%25242.8%257B%255Cdeg%257D%2520when%2520using%2520the%2520same%2520marker%2520set%252C%2520compared%2520to%25204.5%2524%255Cpm%25242.9%257B%255Cdeg%257D%250Aotherwise.%2520Biofeedback%2520from%2520the%2520RGBD%2520camera%2520was%2520provided%2520at%252063%2520Hz.%2520Our%2520study%250Aintroduces%2520a%2520novel%2520method%2520for%2520using%2520an%2520RGBD%2520camera%2520as%2520a%2520low-cost%2520motion%2520capture%250Asolution%252C%2520emphasizing%2520its%2520potential%2520for%2520accurate%2520kinematic%2520reconstruction%2520and%250Acomprehensive%2520upper-limb%2520biomechanical%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10007v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%2C%20accurate%2C%20and%20open%20source%20upper-limb%20musculoskeletal%20analysis%0A%20%20using%20a%20single%20RGBD%20camera&entry.906535625=Amedeo%20Ceglia%20and%20Kael%20Facon%20and%20Micka%C3%ABl%20Begon%20and%20Lama%20Seoud&entry.1292438233=%20%20Biomechanical%20biofeedback%20may%20enhance%20rehabilitation%20and%20provide%20clinicians%0Awith%20more%20objective%20task%20evaluation.%20These%20feedbacks%20often%20rely%20on%20expensive%0Amotion%20capture%20systems%2C%20which%20restricts%20their%20widespread%20use%2C%20leading%20to%20the%0Adevelopment%20of%20computer%20vision-based%20methods.%20These%20methods%20are%20subject%20to%0Alarge%20joint%20angle%20errors%2C%20considering%20the%20upper%20limb%2C%20and%20exclude%20the%20scapula%0Aand%20clavicle%20motion%20in%20the%20analysis.%20Our%20open-source%20approach%20offers%20a%0Auser-friendly%20solution%20for%20high-fidelity%20upper-limb%20kinematics%20using%20a%20single%0Alow-cost%20RGBD%20camera%20and%20includes%20semi-automatic%20skin%20marker%20labeling.%0AReal-time%20biomechanical%20analysis%2C%20ranging%20from%20kinematics%20to%20muscle%20force%0Aestimation%2C%20was%20conducted%20on%20eight%20participants%20performing%20a%20hand-cycling%0Amotion%20to%20demonstrate%20the%20applicability%20of%20our%20approach%20on%20the%20upper%20limb.%0AMarkers%20were%20recorded%20by%20the%20RGBD%20camera%20and%20an%20optoelectronic%20camera%20system%2C%0Aconsidered%20as%20a%20reference.%20Muscle%20activity%20and%20external%20load%20were%20recorded%0Ausing%20eight%20EMG%20and%20instrumented%20hand%20pedals%2C%20respectively.%20Bland-Altman%0Aanalysis%20revealed%20significant%20agreements%20in%20the%203D%20markers%27%20positions%20between%0Athe%20two%20motion%20capture%20methods%2C%20with%20errors%20averaging%203.3%24%5Cpm%243.9%20mm.%20For%20the%0Abiomechanical%20analysis%2C%20the%20level%20of%20agreement%20was%20sensitive%20to%20whether%20the%0Asame%20marker%20set%20was%20used.%20For%20example%2C%20joint%20angle%20differences%20averaging%0A2.3%24%5Cpm%242.8%7B%5Cdeg%7D%20when%20using%20the%20same%20marker%20set%2C%20compared%20to%204.5%24%5Cpm%242.9%7B%5Cdeg%7D%0Aotherwise.%20Biofeedback%20from%20the%20RGBD%20camera%20was%20provided%20at%2063%20Hz.%20Our%20study%0Aintroduces%20a%20novel%20method%20for%20using%20an%20RGBD%20camera%20as%20a%20low-cost%20motion%20capture%0Asolution%2C%20emphasizing%20its%20potential%20for%20accurate%20kinematic%20reconstruction%20and%0Acomprehensive%20upper-limb%20biomechanical%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10007v1&entry.124074799=Read"},
{"title": "Enhancing Vision-Language Model with Unmasked Token Alignment", "author": "Jihao Liu and Jinliang Zheng and Boxiao Liu and Yu Liu and Hongsheng Li", "abstract": "  Contrastive pre-training on image-text pairs, exemplified by CLIP, becomes a\nstandard technique for learning multi-modal visual-language representations.\nAlthough CLIP has demonstrated remarkable performance, training it from scratch\non noisy web-scale datasets is computationally demanding. On the other hand,\nmask-then-predict pre-training approaches, like Masked Image Modeling (MIM),\noffer efficient self-supervised learning for single-modal representations. This\npaper introduces Unmasked Token Alignment (UTA), a method that leverages\nexisting CLIP models to further enhance its vision-language representations.\nUTA trains a Vision Transformer (ViT) by aligning unmasked visual tokens to the\ncorresponding image tokens from a frozen CLIP vision encoder, which\nautomatically aligns the ViT model with the CLIP text encoder. The pre-trained\nViT can be directly applied for zero-shot evaluation even without training on\nimage-text pairs. Compared to MIM approaches, UTA does not suffer from\ntraining-finetuning inconsistency and is much more training-efficient by\navoiding using the extra [MASK] tokens. Extensive experimental results\ndemonstrate that UTA can enhance CLIP models and outperform existing MIM\nmethods on various uni- and multi-modal benchmarks. Code and models are\navailable at https://github.com/jihaonew/UTA.\n", "link": "http://arxiv.org/abs/2405.19009v2", "date": "2024-06-14", "relevancy": 2.239, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6218}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.517}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Vision-Language%20Model%20with%20Unmasked%20Token%20Alignment&body=Title%3A%20Enhancing%20Vision-Language%20Model%20with%20Unmasked%20Token%20Alignment%0AAuthor%3A%20Jihao%20Liu%20and%20Jinliang%20Zheng%20and%20Boxiao%20Liu%20and%20Yu%20Liu%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20Contrastive%20pre-training%20on%20image-text%20pairs%2C%20exemplified%20by%20CLIP%2C%20becomes%20a%0Astandard%20technique%20for%20learning%20multi-modal%20visual-language%20representations.%0AAlthough%20CLIP%20has%20demonstrated%20remarkable%20performance%2C%20training%20it%20from%20scratch%0Aon%20noisy%20web-scale%20datasets%20is%20computationally%20demanding.%20On%20the%20other%20hand%2C%0Amask-then-predict%20pre-training%20approaches%2C%20like%20Masked%20Image%20Modeling%20%28MIM%29%2C%0Aoffer%20efficient%20self-supervised%20learning%20for%20single-modal%20representations.%20This%0Apaper%20introduces%20Unmasked%20Token%20Alignment%20%28UTA%29%2C%20a%20method%20that%20leverages%0Aexisting%20CLIP%20models%20to%20further%20enhance%20its%20vision-language%20representations.%0AUTA%20trains%20a%20Vision%20Transformer%20%28ViT%29%20by%20aligning%20unmasked%20visual%20tokens%20to%20the%0Acorresponding%20image%20tokens%20from%20a%20frozen%20CLIP%20vision%20encoder%2C%20which%0Aautomatically%20aligns%20the%20ViT%20model%20with%20the%20CLIP%20text%20encoder.%20The%20pre-trained%0AViT%20can%20be%20directly%20applied%20for%20zero-shot%20evaluation%20even%20without%20training%20on%0Aimage-text%20pairs.%20Compared%20to%20MIM%20approaches%2C%20UTA%20does%20not%20suffer%20from%0Atraining-finetuning%20inconsistency%20and%20is%20much%20more%20training-efficient%20by%0Aavoiding%20using%20the%20extra%20%5BMASK%5D%20tokens.%20Extensive%20experimental%20results%0Ademonstrate%20that%20UTA%20can%20enhance%20CLIP%20models%20and%20outperform%20existing%20MIM%0Amethods%20on%20various%20uni-%20and%20multi-modal%20benchmarks.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/jihaonew/UTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19009v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Vision-Language%2520Model%2520with%2520Unmasked%2520Token%2520Alignment%26entry.906535625%3DJihao%2520Liu%2520and%2520Jinliang%2520Zheng%2520and%2520Boxiao%2520Liu%2520and%2520Yu%2520Liu%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520Contrastive%2520pre-training%2520on%2520image-text%2520pairs%252C%2520exemplified%2520by%2520CLIP%252C%2520becomes%2520a%250Astandard%2520technique%2520for%2520learning%2520multi-modal%2520visual-language%2520representations.%250AAlthough%2520CLIP%2520has%2520demonstrated%2520remarkable%2520performance%252C%2520training%2520it%2520from%2520scratch%250Aon%2520noisy%2520web-scale%2520datasets%2520is%2520computationally%2520demanding.%2520On%2520the%2520other%2520hand%252C%250Amask-then-predict%2520pre-training%2520approaches%252C%2520like%2520Masked%2520Image%2520Modeling%2520%2528MIM%2529%252C%250Aoffer%2520efficient%2520self-supervised%2520learning%2520for%2520single-modal%2520representations.%2520This%250Apaper%2520introduces%2520Unmasked%2520Token%2520Alignment%2520%2528UTA%2529%252C%2520a%2520method%2520that%2520leverages%250Aexisting%2520CLIP%2520models%2520to%2520further%2520enhance%2520its%2520vision-language%2520representations.%250AUTA%2520trains%2520a%2520Vision%2520Transformer%2520%2528ViT%2529%2520by%2520aligning%2520unmasked%2520visual%2520tokens%2520to%2520the%250Acorresponding%2520image%2520tokens%2520from%2520a%2520frozen%2520CLIP%2520vision%2520encoder%252C%2520which%250Aautomatically%2520aligns%2520the%2520ViT%2520model%2520with%2520the%2520CLIP%2520text%2520encoder.%2520The%2520pre-trained%250AViT%2520can%2520be%2520directly%2520applied%2520for%2520zero-shot%2520evaluation%2520even%2520without%2520training%2520on%250Aimage-text%2520pairs.%2520Compared%2520to%2520MIM%2520approaches%252C%2520UTA%2520does%2520not%2520suffer%2520from%250Atraining-finetuning%2520inconsistency%2520and%2520is%2520much%2520more%2520training-efficient%2520by%250Aavoiding%2520using%2520the%2520extra%2520%255BMASK%255D%2520tokens.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520UTA%2520can%2520enhance%2520CLIP%2520models%2520and%2520outperform%2520existing%2520MIM%250Amethods%2520on%2520various%2520uni-%2520and%2520multi-modal%2520benchmarks.%2520Code%2520and%2520models%2520are%250Aavailable%2520at%2520https%253A//github.com/jihaonew/UTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19009v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Vision-Language%20Model%20with%20Unmasked%20Token%20Alignment&entry.906535625=Jihao%20Liu%20and%20Jinliang%20Zheng%20and%20Boxiao%20Liu%20and%20Yu%20Liu%20and%20Hongsheng%20Li&entry.1292438233=%20%20Contrastive%20pre-training%20on%20image-text%20pairs%2C%20exemplified%20by%20CLIP%2C%20becomes%20a%0Astandard%20technique%20for%20learning%20multi-modal%20visual-language%20representations.%0AAlthough%20CLIP%20has%20demonstrated%20remarkable%20performance%2C%20training%20it%20from%20scratch%0Aon%20noisy%20web-scale%20datasets%20is%20computationally%20demanding.%20On%20the%20other%20hand%2C%0Amask-then-predict%20pre-training%20approaches%2C%20like%20Masked%20Image%20Modeling%20%28MIM%29%2C%0Aoffer%20efficient%20self-supervised%20learning%20for%20single-modal%20representations.%20This%0Apaper%20introduces%20Unmasked%20Token%20Alignment%20%28UTA%29%2C%20a%20method%20that%20leverages%0Aexisting%20CLIP%20models%20to%20further%20enhance%20its%20vision-language%20representations.%0AUTA%20trains%20a%20Vision%20Transformer%20%28ViT%29%20by%20aligning%20unmasked%20visual%20tokens%20to%20the%0Acorresponding%20image%20tokens%20from%20a%20frozen%20CLIP%20vision%20encoder%2C%20which%0Aautomatically%20aligns%20the%20ViT%20model%20with%20the%20CLIP%20text%20encoder.%20The%20pre-trained%0AViT%20can%20be%20directly%20applied%20for%20zero-shot%20evaluation%20even%20without%20training%20on%0Aimage-text%20pairs.%20Compared%20to%20MIM%20approaches%2C%20UTA%20does%20not%20suffer%20from%0Atraining-finetuning%20inconsistency%20and%20is%20much%20more%20training-efficient%20by%0Aavoiding%20using%20the%20extra%20%5BMASK%5D%20tokens.%20Extensive%20experimental%20results%0Ademonstrate%20that%20UTA%20can%20enhance%20CLIP%20models%20and%20outperform%20existing%20MIM%0Amethods%20on%20various%20uni-%20and%20multi-modal%20benchmarks.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/jihaonew/UTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19009v2&entry.124074799=Read"},
{"title": "EFM3D: A Benchmark for Measuring Progress Towards 3D Egocentric\n  Foundation Models", "author": "Julian Straub and Daniel DeTone and Tianwei Shen and Nan Yang and Chris Sweeney and Richard Newcombe", "abstract": "  The advent of wearable computers enables a new source of context for AI that\nis embedded in egocentric sensor data. This new egocentric data comes equipped\nwith fine-grained 3D location information and thus presents the opportunity for\na novel class of spatial foundation models that are rooted in 3D space. To\nmeasure progress on what we term Egocentric Foundation Models (EFMs) we\nestablish EFM3D, a benchmark with two core 3D egocentric perception tasks.\nEFM3D is the first benchmark for 3D object detection and surface regression on\nhigh quality annotated egocentric data of Project Aria. We propose Egocentric\nVoxel Lifting (EVL), a baseline for 3D EFMs. EVL leverages all available\negocentric modalities and inherits foundational capabilities from 2D foundation\nmodels. This model, trained on a large simulated dataset, outperforms existing\nmethods on the EFM3D benchmark.\n", "link": "http://arxiv.org/abs/2406.10224v1", "date": "2024-06-14", "relevancy": 2.2138, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5579}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5525}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EFM3D%3A%20A%20Benchmark%20for%20Measuring%20Progress%20Towards%203D%20Egocentric%0A%20%20Foundation%20Models&body=Title%3A%20EFM3D%3A%20A%20Benchmark%20for%20Measuring%20Progress%20Towards%203D%20Egocentric%0A%20%20Foundation%20Models%0AAuthor%3A%20Julian%20Straub%20and%20Daniel%20DeTone%20and%20Tianwei%20Shen%20and%20Nan%20Yang%20and%20Chris%20Sweeney%20and%20Richard%20Newcombe%0AAbstract%3A%20%20%20The%20advent%20of%20wearable%20computers%20enables%20a%20new%20source%20of%20context%20for%20AI%20that%0Ais%20embedded%20in%20egocentric%20sensor%20data.%20This%20new%20egocentric%20data%20comes%20equipped%0Awith%20fine-grained%203D%20location%20information%20and%20thus%20presents%20the%20opportunity%20for%0Aa%20novel%20class%20of%20spatial%20foundation%20models%20that%20are%20rooted%20in%203D%20space.%20To%0Ameasure%20progress%20on%20what%20we%20term%20Egocentric%20Foundation%20Models%20%28EFMs%29%20we%0Aestablish%20EFM3D%2C%20a%20benchmark%20with%20two%20core%203D%20egocentric%20perception%20tasks.%0AEFM3D%20is%20the%20first%20benchmark%20for%203D%20object%20detection%20and%20surface%20regression%20on%0Ahigh%20quality%20annotated%20egocentric%20data%20of%20Project%20Aria.%20We%20propose%20Egocentric%0AVoxel%20Lifting%20%28EVL%29%2C%20a%20baseline%20for%203D%20EFMs.%20EVL%20leverages%20all%20available%0Aegocentric%20modalities%20and%20inherits%20foundational%20capabilities%20from%202D%20foundation%0Amodels.%20This%20model%2C%20trained%20on%20a%20large%20simulated%20dataset%2C%20outperforms%20existing%0Amethods%20on%20the%20EFM3D%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEFM3D%253A%2520A%2520Benchmark%2520for%2520Measuring%2520Progress%2520Towards%25203D%2520Egocentric%250A%2520%2520Foundation%2520Models%26entry.906535625%3DJulian%2520Straub%2520and%2520Daniel%2520DeTone%2520and%2520Tianwei%2520Shen%2520and%2520Nan%2520Yang%2520and%2520Chris%2520Sweeney%2520and%2520Richard%2520Newcombe%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520wearable%2520computers%2520enables%2520a%2520new%2520source%2520of%2520context%2520for%2520AI%2520that%250Ais%2520embedded%2520in%2520egocentric%2520sensor%2520data.%2520This%2520new%2520egocentric%2520data%2520comes%2520equipped%250Awith%2520fine-grained%25203D%2520location%2520information%2520and%2520thus%2520presents%2520the%2520opportunity%2520for%250Aa%2520novel%2520class%2520of%2520spatial%2520foundation%2520models%2520that%2520are%2520rooted%2520in%25203D%2520space.%2520To%250Ameasure%2520progress%2520on%2520what%2520we%2520term%2520Egocentric%2520Foundation%2520Models%2520%2528EFMs%2529%2520we%250Aestablish%2520EFM3D%252C%2520a%2520benchmark%2520with%2520two%2520core%25203D%2520egocentric%2520perception%2520tasks.%250AEFM3D%2520is%2520the%2520first%2520benchmark%2520for%25203D%2520object%2520detection%2520and%2520surface%2520regression%2520on%250Ahigh%2520quality%2520annotated%2520egocentric%2520data%2520of%2520Project%2520Aria.%2520We%2520propose%2520Egocentric%250AVoxel%2520Lifting%2520%2528EVL%2529%252C%2520a%2520baseline%2520for%25203D%2520EFMs.%2520EVL%2520leverages%2520all%2520available%250Aegocentric%2520modalities%2520and%2520inherits%2520foundational%2520capabilities%2520from%25202D%2520foundation%250Amodels.%2520This%2520model%252C%2520trained%2520on%2520a%2520large%2520simulated%2520dataset%252C%2520outperforms%2520existing%250Amethods%2520on%2520the%2520EFM3D%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EFM3D%3A%20A%20Benchmark%20for%20Measuring%20Progress%20Towards%203D%20Egocentric%0A%20%20Foundation%20Models&entry.906535625=Julian%20Straub%20and%20Daniel%20DeTone%20and%20Tianwei%20Shen%20and%20Nan%20Yang%20and%20Chris%20Sweeney%20and%20Richard%20Newcombe&entry.1292438233=%20%20The%20advent%20of%20wearable%20computers%20enables%20a%20new%20source%20of%20context%20for%20AI%20that%0Ais%20embedded%20in%20egocentric%20sensor%20data.%20This%20new%20egocentric%20data%20comes%20equipped%0Awith%20fine-grained%203D%20location%20information%20and%20thus%20presents%20the%20opportunity%20for%0Aa%20novel%20class%20of%20spatial%20foundation%20models%20that%20are%20rooted%20in%203D%20space.%20To%0Ameasure%20progress%20on%20what%20we%20term%20Egocentric%20Foundation%20Models%20%28EFMs%29%20we%0Aestablish%20EFM3D%2C%20a%20benchmark%20with%20two%20core%203D%20egocentric%20perception%20tasks.%0AEFM3D%20is%20the%20first%20benchmark%20for%203D%20object%20detection%20and%20surface%20regression%20on%0Ahigh%20quality%20annotated%20egocentric%20data%20of%20Project%20Aria.%20We%20propose%20Egocentric%0AVoxel%20Lifting%20%28EVL%29%2C%20a%20baseline%20for%203D%20EFMs.%20EVL%20leverages%20all%20available%0Aegocentric%20modalities%20and%20inherits%20foundational%20capabilities%20from%202D%20foundation%0Amodels.%20This%20model%2C%20trained%20on%20a%20large%20simulated%20dataset%2C%20outperforms%20existing%0Amethods%20on%20the%20EFM3D%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10224v1&entry.124074799=Read"},
{"title": "BiVLC: Extending Vision-Language Compositionality Evaluation with\n  Text-to-Image Retrieval", "author": "Imanol Miranda and Ander Salaberria and Eneko Agirre and Gorka Azkune", "abstract": "  Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe\nare formulated as image-to-text retrieval problems, where, given an image, the\nmodels need to select between the correct textual description and a synthetic\nhard negative text. In this work we present the Bidirectional Vision-Language\nCompositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic\nhard negative image generated from the synthetic text, resulting in two\nimage-to-text retrieval examples (one for each image) and, more importantly,\ntwo text-to-image retrieval examples (one for each text). Human annotators\nfilter out ill-formed examples ensuring the validity of the benchmark. The\nexperiments on BiVLC uncover a weakness of current multimodal models, as they\nperform poorly in the text-to-image direction. In fact, when considering both\nretrieval directions, the conclusions obtained in previous works change\nsignificantly. In addition to the benchmark, we show that a contrastive model\ntrained using synthetic images and texts improves the state of the art in\nSugarCrepe and in BiVLC for both retrieval directions. The gap to human\nperformance in BiVLC confirms that Vision-Language Compositionality is still a\nchallenging problem. BiVLC and code are available at\nhttps://imirandam.github.io/BiVLC_project_page.\n", "link": "http://arxiv.org/abs/2406.09952v1", "date": "2024-06-14", "relevancy": 2.2012, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5581}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.556}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiVLC%3A%20Extending%20Vision-Language%20Compositionality%20Evaluation%20with%0A%20%20Text-to-Image%20Retrieval&body=Title%3A%20BiVLC%3A%20Extending%20Vision-Language%20Compositionality%20Evaluation%20with%0A%20%20Text-to-Image%20Retrieval%0AAuthor%3A%20Imanol%20Miranda%20and%20Ander%20Salaberria%20and%20Eneko%20Agirre%20and%20Gorka%20Azkune%0AAbstract%3A%20%20%20Existing%20Vision-Language%20Compositionality%20%28VLC%29%20benchmarks%20like%20SugarCrepe%0Aare%20formulated%20as%20image-to-text%20retrieval%20problems%2C%20where%2C%20given%20an%20image%2C%20the%0Amodels%20need%20to%20select%20between%20the%20correct%20textual%20description%20and%20a%20synthetic%0Ahard%20negative%20text.%20In%20this%20work%20we%20present%20the%20Bidirectional%20Vision-Language%0ACompositionality%20%28BiVLC%29%20dataset.%20The%20novelty%20of%20BiVLC%20is%20to%20add%20a%20synthetic%0Ahard%20negative%20image%20generated%20from%20the%20synthetic%20text%2C%20resulting%20in%20two%0Aimage-to-text%20retrieval%20examples%20%28one%20for%20each%20image%29%20and%2C%20more%20importantly%2C%0Atwo%20text-to-image%20retrieval%20examples%20%28one%20for%20each%20text%29.%20Human%20annotators%0Afilter%20out%20ill-formed%20examples%20ensuring%20the%20validity%20of%20the%20benchmark.%20The%0Aexperiments%20on%20BiVLC%20uncover%20a%20weakness%20of%20current%20multimodal%20models%2C%20as%20they%0Aperform%20poorly%20in%20the%20text-to-image%20direction.%20In%20fact%2C%20when%20considering%20both%0Aretrieval%20directions%2C%20the%20conclusions%20obtained%20in%20previous%20works%20change%0Asignificantly.%20In%20addition%20to%20the%20benchmark%2C%20we%20show%20that%20a%20contrastive%20model%0Atrained%20using%20synthetic%20images%20and%20texts%20improves%20the%20state%20of%20the%20art%20in%0ASugarCrepe%20and%20in%20BiVLC%20for%20both%20retrieval%20directions.%20The%20gap%20to%20human%0Aperformance%20in%20BiVLC%20confirms%20that%20Vision-Language%20Compositionality%20is%20still%20a%0Achallenging%20problem.%20BiVLC%20and%20code%20are%20available%20at%0Ahttps%3A//imirandam.github.io/BiVLC_project_page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiVLC%253A%2520Extending%2520Vision-Language%2520Compositionality%2520Evaluation%2520with%250A%2520%2520Text-to-Image%2520Retrieval%26entry.906535625%3DImanol%2520Miranda%2520and%2520Ander%2520Salaberria%2520and%2520Eneko%2520Agirre%2520and%2520Gorka%2520Azkune%26entry.1292438233%3D%2520%2520Existing%2520Vision-Language%2520Compositionality%2520%2528VLC%2529%2520benchmarks%2520like%2520SugarCrepe%250Aare%2520formulated%2520as%2520image-to-text%2520retrieval%2520problems%252C%2520where%252C%2520given%2520an%2520image%252C%2520the%250Amodels%2520need%2520to%2520select%2520between%2520the%2520correct%2520textual%2520description%2520and%2520a%2520synthetic%250Ahard%2520negative%2520text.%2520In%2520this%2520work%2520we%2520present%2520the%2520Bidirectional%2520Vision-Language%250ACompositionality%2520%2528BiVLC%2529%2520dataset.%2520The%2520novelty%2520of%2520BiVLC%2520is%2520to%2520add%2520a%2520synthetic%250Ahard%2520negative%2520image%2520generated%2520from%2520the%2520synthetic%2520text%252C%2520resulting%2520in%2520two%250Aimage-to-text%2520retrieval%2520examples%2520%2528one%2520for%2520each%2520image%2529%2520and%252C%2520more%2520importantly%252C%250Atwo%2520text-to-image%2520retrieval%2520examples%2520%2528one%2520for%2520each%2520text%2529.%2520Human%2520annotators%250Afilter%2520out%2520ill-formed%2520examples%2520ensuring%2520the%2520validity%2520of%2520the%2520benchmark.%2520The%250Aexperiments%2520on%2520BiVLC%2520uncover%2520a%2520weakness%2520of%2520current%2520multimodal%2520models%252C%2520as%2520they%250Aperform%2520poorly%2520in%2520the%2520text-to-image%2520direction.%2520In%2520fact%252C%2520when%2520considering%2520both%250Aretrieval%2520directions%252C%2520the%2520conclusions%2520obtained%2520in%2520previous%2520works%2520change%250Asignificantly.%2520In%2520addition%2520to%2520the%2520benchmark%252C%2520we%2520show%2520that%2520a%2520contrastive%2520model%250Atrained%2520using%2520synthetic%2520images%2520and%2520texts%2520improves%2520the%2520state%2520of%2520the%2520art%2520in%250ASugarCrepe%2520and%2520in%2520BiVLC%2520for%2520both%2520retrieval%2520directions.%2520The%2520gap%2520to%2520human%250Aperformance%2520in%2520BiVLC%2520confirms%2520that%2520Vision-Language%2520Compositionality%2520is%2520still%2520a%250Achallenging%2520problem.%2520BiVLC%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//imirandam.github.io/BiVLC_project_page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiVLC%3A%20Extending%20Vision-Language%20Compositionality%20Evaluation%20with%0A%20%20Text-to-Image%20Retrieval&entry.906535625=Imanol%20Miranda%20and%20Ander%20Salaberria%20and%20Eneko%20Agirre%20and%20Gorka%20Azkune&entry.1292438233=%20%20Existing%20Vision-Language%20Compositionality%20%28VLC%29%20benchmarks%20like%20SugarCrepe%0Aare%20formulated%20as%20image-to-text%20retrieval%20problems%2C%20where%2C%20given%20an%20image%2C%20the%0Amodels%20need%20to%20select%20between%20the%20correct%20textual%20description%20and%20a%20synthetic%0Ahard%20negative%20text.%20In%20this%20work%20we%20present%20the%20Bidirectional%20Vision-Language%0ACompositionality%20%28BiVLC%29%20dataset.%20The%20novelty%20of%20BiVLC%20is%20to%20add%20a%20synthetic%0Ahard%20negative%20image%20generated%20from%20the%20synthetic%20text%2C%20resulting%20in%20two%0Aimage-to-text%20retrieval%20examples%20%28one%20for%20each%20image%29%20and%2C%20more%20importantly%2C%0Atwo%20text-to-image%20retrieval%20examples%20%28one%20for%20each%20text%29.%20Human%20annotators%0Afilter%20out%20ill-formed%20examples%20ensuring%20the%20validity%20of%20the%20benchmark.%20The%0Aexperiments%20on%20BiVLC%20uncover%20a%20weakness%20of%20current%20multimodal%20models%2C%20as%20they%0Aperform%20poorly%20in%20the%20text-to-image%20direction.%20In%20fact%2C%20when%20considering%20both%0Aretrieval%20directions%2C%20the%20conclusions%20obtained%20in%20previous%20works%20change%0Asignificantly.%20In%20addition%20to%20the%20benchmark%2C%20we%20show%20that%20a%20contrastive%20model%0Atrained%20using%20synthetic%20images%20and%20texts%20improves%20the%20state%20of%20the%20art%20in%0ASugarCrepe%20and%20in%20BiVLC%20for%20both%20retrieval%20directions.%20The%20gap%20to%20human%0Aperformance%20in%20BiVLC%20confirms%20that%20Vision-Language%20Compositionality%20is%20still%20a%0Achallenging%20problem.%20BiVLC%20and%20code%20are%20available%20at%0Ahttps%3A//imirandam.github.io/BiVLC_project_page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09952v1&entry.124074799=Read"},
{"title": "Task-aligned Part-aware Panoptic Segmentation through Joint Object-Part\n  Representations", "author": "Daan de Geus and Gijs Dubbelman", "abstract": "  Part-aware panoptic segmentation (PPS) requires (a) that each foreground\nobject and background region in an image is segmented and classified, and (b)\nthat all parts within foreground objects are segmented, classified and linked\nto their parent object. Existing methods approach PPS by separately conducting\nobject-level and part-level segmentation. However, their part-level predictions\nare not linked to individual parent objects. Therefore, their learning\nobjective is not aligned with the PPS task objective, which harms the PPS\nperformance. To solve this, and make more accurate PPS predictions, we propose\nTask-Aligned Part-aware Panoptic Segmentation (TAPPS). This method uses a set\nof shared queries to jointly predict (a) object-level segments, and (b) the\npart-level segments within those same objects. As a result, TAPPS learns to\npredict part-level segments that are linked to individual parent objects,\naligning the learning objective with the task objective, and allowing TAPPS to\nleverage joint object-part representations. With experiments, we show that\nTAPPS considerably outperforms methods that predict objects and parts\nseparately, and achieves new state-of-the-art PPS results.\n", "link": "http://arxiv.org/abs/2406.10114v1", "date": "2024-06-14", "relevancy": 2.185, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5606}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5498}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-aligned%20Part-aware%20Panoptic%20Segmentation%20through%20Joint%20Object-Part%0A%20%20Representations&body=Title%3A%20Task-aligned%20Part-aware%20Panoptic%20Segmentation%20through%20Joint%20Object-Part%0A%20%20Representations%0AAuthor%3A%20Daan%20de%20Geus%20and%20Gijs%20Dubbelman%0AAbstract%3A%20%20%20Part-aware%20panoptic%20segmentation%20%28PPS%29%20requires%20%28a%29%20that%20each%20foreground%0Aobject%20and%20background%20region%20in%20an%20image%20is%20segmented%20and%20classified%2C%20and%20%28b%29%0Athat%20all%20parts%20within%20foreground%20objects%20are%20segmented%2C%20classified%20and%20linked%0Ato%20their%20parent%20object.%20Existing%20methods%20approach%20PPS%20by%20separately%20conducting%0Aobject-level%20and%20part-level%20segmentation.%20However%2C%20their%20part-level%20predictions%0Aare%20not%20linked%20to%20individual%20parent%20objects.%20Therefore%2C%20their%20learning%0Aobjective%20is%20not%20aligned%20with%20the%20PPS%20task%20objective%2C%20which%20harms%20the%20PPS%0Aperformance.%20To%20solve%20this%2C%20and%20make%20more%20accurate%20PPS%20predictions%2C%20we%20propose%0ATask-Aligned%20Part-aware%20Panoptic%20Segmentation%20%28TAPPS%29.%20This%20method%20uses%20a%20set%0Aof%20shared%20queries%20to%20jointly%20predict%20%28a%29%20object-level%20segments%2C%20and%20%28b%29%20the%0Apart-level%20segments%20within%20those%20same%20objects.%20As%20a%20result%2C%20TAPPS%20learns%20to%0Apredict%20part-level%20segments%20that%20are%20linked%20to%20individual%20parent%20objects%2C%0Aaligning%20the%20learning%20objective%20with%20the%20task%20objective%2C%20and%20allowing%20TAPPS%20to%0Aleverage%20joint%20object-part%20representations.%20With%20experiments%2C%20we%20show%20that%0ATAPPS%20considerably%20outperforms%20methods%20that%20predict%20objects%20and%20parts%0Aseparately%2C%20and%20achieves%20new%20state-of-the-art%20PPS%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-aligned%2520Part-aware%2520Panoptic%2520Segmentation%2520through%2520Joint%2520Object-Part%250A%2520%2520Representations%26entry.906535625%3DDaan%2520de%2520Geus%2520and%2520Gijs%2520Dubbelman%26entry.1292438233%3D%2520%2520Part-aware%2520panoptic%2520segmentation%2520%2528PPS%2529%2520requires%2520%2528a%2529%2520that%2520each%2520foreground%250Aobject%2520and%2520background%2520region%2520in%2520an%2520image%2520is%2520segmented%2520and%2520classified%252C%2520and%2520%2528b%2529%250Athat%2520all%2520parts%2520within%2520foreground%2520objects%2520are%2520segmented%252C%2520classified%2520and%2520linked%250Ato%2520their%2520parent%2520object.%2520Existing%2520methods%2520approach%2520PPS%2520by%2520separately%2520conducting%250Aobject-level%2520and%2520part-level%2520segmentation.%2520However%252C%2520their%2520part-level%2520predictions%250Aare%2520not%2520linked%2520to%2520individual%2520parent%2520objects.%2520Therefore%252C%2520their%2520learning%250Aobjective%2520is%2520not%2520aligned%2520with%2520the%2520PPS%2520task%2520objective%252C%2520which%2520harms%2520the%2520PPS%250Aperformance.%2520To%2520solve%2520this%252C%2520and%2520make%2520more%2520accurate%2520PPS%2520predictions%252C%2520we%2520propose%250ATask-Aligned%2520Part-aware%2520Panoptic%2520Segmentation%2520%2528TAPPS%2529.%2520This%2520method%2520uses%2520a%2520set%250Aof%2520shared%2520queries%2520to%2520jointly%2520predict%2520%2528a%2529%2520object-level%2520segments%252C%2520and%2520%2528b%2529%2520the%250Apart-level%2520segments%2520within%2520those%2520same%2520objects.%2520As%2520a%2520result%252C%2520TAPPS%2520learns%2520to%250Apredict%2520part-level%2520segments%2520that%2520are%2520linked%2520to%2520individual%2520parent%2520objects%252C%250Aaligning%2520the%2520learning%2520objective%2520with%2520the%2520task%2520objective%252C%2520and%2520allowing%2520TAPPS%2520to%250Aleverage%2520joint%2520object-part%2520representations.%2520With%2520experiments%252C%2520we%2520show%2520that%250ATAPPS%2520considerably%2520outperforms%2520methods%2520that%2520predict%2520objects%2520and%2520parts%250Aseparately%252C%2520and%2520achieves%2520new%2520state-of-the-art%2520PPS%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-aligned%20Part-aware%20Panoptic%20Segmentation%20through%20Joint%20Object-Part%0A%20%20Representations&entry.906535625=Daan%20de%20Geus%20and%20Gijs%20Dubbelman&entry.1292438233=%20%20Part-aware%20panoptic%20segmentation%20%28PPS%29%20requires%20%28a%29%20that%20each%20foreground%0Aobject%20and%20background%20region%20in%20an%20image%20is%20segmented%20and%20classified%2C%20and%20%28b%29%0Athat%20all%20parts%20within%20foreground%20objects%20are%20segmented%2C%20classified%20and%20linked%0Ato%20their%20parent%20object.%20Existing%20methods%20approach%20PPS%20by%20separately%20conducting%0Aobject-level%20and%20part-level%20segmentation.%20However%2C%20their%20part-level%20predictions%0Aare%20not%20linked%20to%20individual%20parent%20objects.%20Therefore%2C%20their%20learning%0Aobjective%20is%20not%20aligned%20with%20the%20PPS%20task%20objective%2C%20which%20harms%20the%20PPS%0Aperformance.%20To%20solve%20this%2C%20and%20make%20more%20accurate%20PPS%20predictions%2C%20we%20propose%0ATask-Aligned%20Part-aware%20Panoptic%20Segmentation%20%28TAPPS%29.%20This%20method%20uses%20a%20set%0Aof%20shared%20queries%20to%20jointly%20predict%20%28a%29%20object-level%20segments%2C%20and%20%28b%29%20the%0Apart-level%20segments%20within%20those%20same%20objects.%20As%20a%20result%2C%20TAPPS%20learns%20to%0Apredict%20part-level%20segments%20that%20are%20linked%20to%20individual%20parent%20objects%2C%0Aaligning%20the%20learning%20objective%20with%20the%20task%20objective%2C%20and%20allowing%20TAPPS%20to%0Aleverage%20joint%20object-part%20representations.%20With%20experiments%2C%20we%20show%20that%0ATAPPS%20considerably%20outperforms%20methods%20that%20predict%20objects%20and%20parts%0Aseparately%2C%20and%20achieves%20new%20state-of-the-art%20PPS%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10114v1&entry.124074799=Read"},
{"title": "Forgetting Order of Continual Learning: Examples That are Learned First\n  are Forgotten Last", "author": "Guy Hacohen and Tinne Tuytelaars", "abstract": "  Catastrophic forgetting poses a significant challenge in continual learning,\nwhere models often forget previous tasks when trained on new data. Our\nempirical analysis reveals a strong correlation between catastrophic forgetting\nand the learning speed of examples: examples learned early are rarely\nforgotten, while those learned later are more susceptible to forgetting. We\ndemonstrate that replay-based continual learning methods can leverage this\nphenomenon by focusing on mid-learned examples for rehearsal. We introduce\nGoldilocks, a novel replay buffer sampling method that filters out examples\nlearned too quickly or too slowly, keeping those learned at an intermediate\nspeed. Goldilocks improves existing continual learning algorithms, leading to\nstate-of-the-art performance across several image classification tasks.\n", "link": "http://arxiv.org/abs/2406.09935v1", "date": "2024-06-14", "relevancy": 2.1848, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4408}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4398}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forgetting%20Order%20of%20Continual%20Learning%3A%20Examples%20That%20are%20Learned%20First%0A%20%20are%20Forgotten%20Last&body=Title%3A%20Forgetting%20Order%20of%20Continual%20Learning%3A%20Examples%20That%20are%20Learned%20First%0A%20%20are%20Forgotten%20Last%0AAuthor%3A%20Guy%20Hacohen%20and%20Tinne%20Tuytelaars%0AAbstract%3A%20%20%20Catastrophic%20forgetting%20poses%20a%20significant%20challenge%20in%20continual%20learning%2C%0Awhere%20models%20often%20forget%20previous%20tasks%20when%20trained%20on%20new%20data.%20Our%0Aempirical%20analysis%20reveals%20a%20strong%20correlation%20between%20catastrophic%20forgetting%0Aand%20the%20learning%20speed%20of%20examples%3A%20examples%20learned%20early%20are%20rarely%0Aforgotten%2C%20while%20those%20learned%20later%20are%20more%20susceptible%20to%20forgetting.%20We%0Ademonstrate%20that%20replay-based%20continual%20learning%20methods%20can%20leverage%20this%0Aphenomenon%20by%20focusing%20on%20mid-learned%20examples%20for%20rehearsal.%20We%20introduce%0AGoldilocks%2C%20a%20novel%20replay%20buffer%20sampling%20method%20that%20filters%20out%20examples%0Alearned%20too%20quickly%20or%20too%20slowly%2C%20keeping%20those%20learned%20at%20an%20intermediate%0Aspeed.%20Goldilocks%20improves%20existing%20continual%20learning%20algorithms%2C%20leading%20to%0Astate-of-the-art%20performance%20across%20several%20image%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForgetting%2520Order%2520of%2520Continual%2520Learning%253A%2520Examples%2520That%2520are%2520Learned%2520First%250A%2520%2520are%2520Forgotten%2520Last%26entry.906535625%3DGuy%2520Hacohen%2520and%2520Tinne%2520Tuytelaars%26entry.1292438233%3D%2520%2520Catastrophic%2520forgetting%2520poses%2520a%2520significant%2520challenge%2520in%2520continual%2520learning%252C%250Awhere%2520models%2520often%2520forget%2520previous%2520tasks%2520when%2520trained%2520on%2520new%2520data.%2520Our%250Aempirical%2520analysis%2520reveals%2520a%2520strong%2520correlation%2520between%2520catastrophic%2520forgetting%250Aand%2520the%2520learning%2520speed%2520of%2520examples%253A%2520examples%2520learned%2520early%2520are%2520rarely%250Aforgotten%252C%2520while%2520those%2520learned%2520later%2520are%2520more%2520susceptible%2520to%2520forgetting.%2520We%250Ademonstrate%2520that%2520replay-based%2520continual%2520learning%2520methods%2520can%2520leverage%2520this%250Aphenomenon%2520by%2520focusing%2520on%2520mid-learned%2520examples%2520for%2520rehearsal.%2520We%2520introduce%250AGoldilocks%252C%2520a%2520novel%2520replay%2520buffer%2520sampling%2520method%2520that%2520filters%2520out%2520examples%250Alearned%2520too%2520quickly%2520or%2520too%2520slowly%252C%2520keeping%2520those%2520learned%2520at%2520an%2520intermediate%250Aspeed.%2520Goldilocks%2520improves%2520existing%2520continual%2520learning%2520algorithms%252C%2520leading%2520to%250Astate-of-the-art%2520performance%2520across%2520several%2520image%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forgetting%20Order%20of%20Continual%20Learning%3A%20Examples%20That%20are%20Learned%20First%0A%20%20are%20Forgotten%20Last&entry.906535625=Guy%20Hacohen%20and%20Tinne%20Tuytelaars&entry.1292438233=%20%20Catastrophic%20forgetting%20poses%20a%20significant%20challenge%20in%20continual%20learning%2C%0Awhere%20models%20often%20forget%20previous%20tasks%20when%20trained%20on%20new%20data.%20Our%0Aempirical%20analysis%20reveals%20a%20strong%20correlation%20between%20catastrophic%20forgetting%0Aand%20the%20learning%20speed%20of%20examples%3A%20examples%20learned%20early%20are%20rarely%0Aforgotten%2C%20while%20those%20learned%20later%20are%20more%20susceptible%20to%20forgetting.%20We%0Ademonstrate%20that%20replay-based%20continual%20learning%20methods%20can%20leverage%20this%0Aphenomenon%20by%20focusing%20on%20mid-learned%20examples%20for%20rehearsal.%20We%20introduce%0AGoldilocks%2C%20a%20novel%20replay%20buffer%20sampling%20method%20that%20filters%20out%20examples%0Alearned%20too%20quickly%20or%20too%20slowly%2C%20keeping%20those%20learned%20at%20an%20intermediate%0Aspeed.%20Goldilocks%20improves%20existing%20continual%20learning%20algorithms%2C%20leading%20to%0Astate-of-the-art%20performance%20across%20several%20image%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09935v1&entry.124074799=Read"},
{"title": "Benchmarking Generative Models on Computational Thinking Tests in\n  Elementary Visual Programming", "author": "Victor-Alexandru P\u0103durean and Adish Singla", "abstract": "  Generative models have demonstrated human-level proficiency in various\nbenchmarks across domains like programming, natural sciences, and general\nknowledge. Despite these promising results on competitive benchmarks, they\nstill struggle with seemingly simple problem-solving tasks typically carried\nout by elementary-level students. How do state-of-the-art models perform on\nstandardized tests designed to assess computational thinking and\nproblem-solving skills at schools? In this paper, we curate a novel benchmark\ninvolving computational thinking tests grounded in elementary visual\nprogramming domains. Our initial results show that state-of-the-art models like\nGPT-4o and Llama3 barely match the performance of an average school student. To\nfurther boost the performance of these models, we fine-tune them using a novel\nsynthetic data generation methodology. The key idea is to develop a\ncomprehensive dataset using symbolic methods that capture different skill\nlevels, ranging from recognition of visual elements to multi-choice quizzes to\nsynthesis-style tasks. We showcase how various aspects of symbolic information\nin synthetic data help improve fine-tuned models' performance. We will release\nthe full implementation and datasets to facilitate further research on\nenhancing computational thinking in generative models.\n", "link": "http://arxiv.org/abs/2406.09891v1", "date": "2024-06-14", "relevancy": 2.1839, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.574}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.542}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Generative%20Models%20on%20Computational%20Thinking%20Tests%20in%0A%20%20Elementary%20Visual%20Programming&body=Title%3A%20Benchmarking%20Generative%20Models%20on%20Computational%20Thinking%20Tests%20in%0A%20%20Elementary%20Visual%20Programming%0AAuthor%3A%20Victor-Alexandru%20P%C4%83durean%20and%20Adish%20Singla%0AAbstract%3A%20%20%20Generative%20models%20have%20demonstrated%20human-level%20proficiency%20in%20various%0Abenchmarks%20across%20domains%20like%20programming%2C%20natural%20sciences%2C%20and%20general%0Aknowledge.%20Despite%20these%20promising%20results%20on%20competitive%20benchmarks%2C%20they%0Astill%20struggle%20with%20seemingly%20simple%20problem-solving%20tasks%20typically%20carried%0Aout%20by%20elementary-level%20students.%20How%20do%20state-of-the-art%20models%20perform%20on%0Astandardized%20tests%20designed%20to%20assess%20computational%20thinking%20and%0Aproblem-solving%20skills%20at%20schools%3F%20In%20this%20paper%2C%20we%20curate%20a%20novel%20benchmark%0Ainvolving%20computational%20thinking%20tests%20grounded%20in%20elementary%20visual%0Aprogramming%20domains.%20Our%20initial%20results%20show%20that%20state-of-the-art%20models%20like%0AGPT-4o%20and%20Llama3%20barely%20match%20the%20performance%20of%20an%20average%20school%20student.%20To%0Afurther%20boost%20the%20performance%20of%20these%20models%2C%20we%20fine-tune%20them%20using%20a%20novel%0Asynthetic%20data%20generation%20methodology.%20The%20key%20idea%20is%20to%20develop%20a%0Acomprehensive%20dataset%20using%20symbolic%20methods%20that%20capture%20different%20skill%0Alevels%2C%20ranging%20from%20recognition%20of%20visual%20elements%20to%20multi-choice%20quizzes%20to%0Asynthesis-style%20tasks.%20We%20showcase%20how%20various%20aspects%20of%20symbolic%20information%0Ain%20synthetic%20data%20help%20improve%20fine-tuned%20models%27%20performance.%20We%20will%20release%0Athe%20full%20implementation%20and%20datasets%20to%20facilitate%20further%20research%20on%0Aenhancing%20computational%20thinking%20in%20generative%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Generative%2520Models%2520on%2520Computational%2520Thinking%2520Tests%2520in%250A%2520%2520Elementary%2520Visual%2520Programming%26entry.906535625%3DVictor-Alexandru%2520P%25C4%2583durean%2520and%2520Adish%2520Singla%26entry.1292438233%3D%2520%2520Generative%2520models%2520have%2520demonstrated%2520human-level%2520proficiency%2520in%2520various%250Abenchmarks%2520across%2520domains%2520like%2520programming%252C%2520natural%2520sciences%252C%2520and%2520general%250Aknowledge.%2520Despite%2520these%2520promising%2520results%2520on%2520competitive%2520benchmarks%252C%2520they%250Astill%2520struggle%2520with%2520seemingly%2520simple%2520problem-solving%2520tasks%2520typically%2520carried%250Aout%2520by%2520elementary-level%2520students.%2520How%2520do%2520state-of-the-art%2520models%2520perform%2520on%250Astandardized%2520tests%2520designed%2520to%2520assess%2520computational%2520thinking%2520and%250Aproblem-solving%2520skills%2520at%2520schools%253F%2520In%2520this%2520paper%252C%2520we%2520curate%2520a%2520novel%2520benchmark%250Ainvolving%2520computational%2520thinking%2520tests%2520grounded%2520in%2520elementary%2520visual%250Aprogramming%2520domains.%2520Our%2520initial%2520results%2520show%2520that%2520state-of-the-art%2520models%2520like%250AGPT-4o%2520and%2520Llama3%2520barely%2520match%2520the%2520performance%2520of%2520an%2520average%2520school%2520student.%2520To%250Afurther%2520boost%2520the%2520performance%2520of%2520these%2520models%252C%2520we%2520fine-tune%2520them%2520using%2520a%2520novel%250Asynthetic%2520data%2520generation%2520methodology.%2520The%2520key%2520idea%2520is%2520to%2520develop%2520a%250Acomprehensive%2520dataset%2520using%2520symbolic%2520methods%2520that%2520capture%2520different%2520skill%250Alevels%252C%2520ranging%2520from%2520recognition%2520of%2520visual%2520elements%2520to%2520multi-choice%2520quizzes%2520to%250Asynthesis-style%2520tasks.%2520We%2520showcase%2520how%2520various%2520aspects%2520of%2520symbolic%2520information%250Ain%2520synthetic%2520data%2520help%2520improve%2520fine-tuned%2520models%2527%2520performance.%2520We%2520will%2520release%250Athe%2520full%2520implementation%2520and%2520datasets%2520to%2520facilitate%2520further%2520research%2520on%250Aenhancing%2520computational%2520thinking%2520in%2520generative%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Generative%20Models%20on%20Computational%20Thinking%20Tests%20in%0A%20%20Elementary%20Visual%20Programming&entry.906535625=Victor-Alexandru%20P%C4%83durean%20and%20Adish%20Singla&entry.1292438233=%20%20Generative%20models%20have%20demonstrated%20human-level%20proficiency%20in%20various%0Abenchmarks%20across%20domains%20like%20programming%2C%20natural%20sciences%2C%20and%20general%0Aknowledge.%20Despite%20these%20promising%20results%20on%20competitive%20benchmarks%2C%20they%0Astill%20struggle%20with%20seemingly%20simple%20problem-solving%20tasks%20typically%20carried%0Aout%20by%20elementary-level%20students.%20How%20do%20state-of-the-art%20models%20perform%20on%0Astandardized%20tests%20designed%20to%20assess%20computational%20thinking%20and%0Aproblem-solving%20skills%20at%20schools%3F%20In%20this%20paper%2C%20we%20curate%20a%20novel%20benchmark%0Ainvolving%20computational%20thinking%20tests%20grounded%20in%20elementary%20visual%0Aprogramming%20domains.%20Our%20initial%20results%20show%20that%20state-of-the-art%20models%20like%0AGPT-4o%20and%20Llama3%20barely%20match%20the%20performance%20of%20an%20average%20school%20student.%20To%0Afurther%20boost%20the%20performance%20of%20these%20models%2C%20we%20fine-tune%20them%20using%20a%20novel%0Asynthetic%20data%20generation%20methodology.%20The%20key%20idea%20is%20to%20develop%20a%0Acomprehensive%20dataset%20using%20symbolic%20methods%20that%20capture%20different%20skill%0Alevels%2C%20ranging%20from%20recognition%20of%20visual%20elements%20to%20multi-choice%20quizzes%20to%0Asynthesis-style%20tasks.%20We%20showcase%20how%20various%20aspects%20of%20symbolic%20information%0Ain%20synthetic%20data%20help%20improve%20fine-tuned%20models%27%20performance.%20We%20will%20release%0Athe%20full%20implementation%20and%20datasets%20to%20facilitate%20further%20research%20on%0Aenhancing%20computational%20thinking%20in%20generative%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09891v1&entry.124074799=Read"},
{"title": "A Simple Interpretable Transformer for Fine-Grained Image Classification\n  and Analysis", "author": "Dipanjyoti Paul and Arpita Chowdhury and Xinqi Xiong and Feng-Ju Chang and David Carlyn and Samuel Stevens and Kaiya L. Provost and Anuj Karpatne and Bryan Carstens and Daniel Rubenstein and Charles Stewart and Tanya Berger-Wolf and Yu Su and Wei-Lun Chao", "abstract": "  We present a novel usage of Transformers to make image classification\ninterpretable. Unlike mainstream classifiers that wait until the last fully\nconnected layer to incorporate class information to make predictions, we\ninvestigate a proactive approach, asking each class to search for itself in an\nimage. We realize this idea via a Transformer encoder-decoder inspired by\nDEtection TRansformer (DETR). We learn \"class-specific\" queries (one for each\nclass) as input to the decoder, enabling each class to localize its patterns in\nan image via cross-attention. We name our approach INterpretable TRansformer\n(INTR), which is fairly easy to implement and exhibits several compelling\nproperties. We show that INTR intrinsically encourages each class to attend\ndistinctively; the cross-attention weights thus provide a faithful\ninterpretation of the prediction. Interestingly, via \"multi-head\"\ncross-attention, INTR could identify different \"attributes\" of a class, making\nit particularly suitable for fine-grained classification and analysis, which we\ndemonstrate on eight datasets. Our code and pre-trained models are publicly\naccessible at the Imageomics Institute GitHub site:\nhttps://github.com/Imageomics/INTR.\n", "link": "http://arxiv.org/abs/2311.04157v3", "date": "2024-06-14", "relevancy": 2.1793, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.582}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5601}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20Interpretable%20Transformer%20for%20Fine-Grained%20Image%20Classification%0A%20%20and%20Analysis&body=Title%3A%20A%20Simple%20Interpretable%20Transformer%20for%20Fine-Grained%20Image%20Classification%0A%20%20and%20Analysis%0AAuthor%3A%20Dipanjyoti%20Paul%20and%20Arpita%20Chowdhury%20and%20Xinqi%20Xiong%20and%20Feng-Ju%20Chang%20and%20David%20Carlyn%20and%20Samuel%20Stevens%20and%20Kaiya%20L.%20Provost%20and%20Anuj%20Karpatne%20and%20Bryan%20Carstens%20and%20Daniel%20Rubenstein%20and%20Charles%20Stewart%20and%20Tanya%20Berger-Wolf%20and%20Yu%20Su%20and%20Wei-Lun%20Chao%0AAbstract%3A%20%20%20We%20present%20a%20novel%20usage%20of%20Transformers%20to%20make%20image%20classification%0Ainterpretable.%20Unlike%20mainstream%20classifiers%20that%20wait%20until%20the%20last%20fully%0Aconnected%20layer%20to%20incorporate%20class%20information%20to%20make%20predictions%2C%20we%0Ainvestigate%20a%20proactive%20approach%2C%20asking%20each%20class%20to%20search%20for%20itself%20in%20an%0Aimage.%20We%20realize%20this%20idea%20via%20a%20Transformer%20encoder-decoder%20inspired%20by%0ADEtection%20TRansformer%20%28DETR%29.%20We%20learn%20%22class-specific%22%20queries%20%28one%20for%20each%0Aclass%29%20as%20input%20to%20the%20decoder%2C%20enabling%20each%20class%20to%20localize%20its%20patterns%20in%0Aan%20image%20via%20cross-attention.%20We%20name%20our%20approach%20INterpretable%20TRansformer%0A%28INTR%29%2C%20which%20is%20fairly%20easy%20to%20implement%20and%20exhibits%20several%20compelling%0Aproperties.%20We%20show%20that%20INTR%20intrinsically%20encourages%20each%20class%20to%20attend%0Adistinctively%3B%20the%20cross-attention%20weights%20thus%20provide%20a%20faithful%0Ainterpretation%20of%20the%20prediction.%20Interestingly%2C%20via%20%22multi-head%22%0Across-attention%2C%20INTR%20could%20identify%20different%20%22attributes%22%20of%20a%20class%2C%20making%0Ait%20particularly%20suitable%20for%20fine-grained%20classification%20and%20analysis%2C%20which%20we%0Ademonstrate%20on%20eight%20datasets.%20Our%20code%20and%20pre-trained%20models%20are%20publicly%0Aaccessible%20at%20the%20Imageomics%20Institute%20GitHub%20site%3A%0Ahttps%3A//github.com/Imageomics/INTR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04157v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520Interpretable%2520Transformer%2520for%2520Fine-Grained%2520Image%2520Classification%250A%2520%2520and%2520Analysis%26entry.906535625%3DDipanjyoti%2520Paul%2520and%2520Arpita%2520Chowdhury%2520and%2520Xinqi%2520Xiong%2520and%2520Feng-Ju%2520Chang%2520and%2520David%2520Carlyn%2520and%2520Samuel%2520Stevens%2520and%2520Kaiya%2520L.%2520Provost%2520and%2520Anuj%2520Karpatne%2520and%2520Bryan%2520Carstens%2520and%2520Daniel%2520Rubenstein%2520and%2520Charles%2520Stewart%2520and%2520Tanya%2520Berger-Wolf%2520and%2520Yu%2520Su%2520and%2520Wei-Lun%2520Chao%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520usage%2520of%2520Transformers%2520to%2520make%2520image%2520classification%250Ainterpretable.%2520Unlike%2520mainstream%2520classifiers%2520that%2520wait%2520until%2520the%2520last%2520fully%250Aconnected%2520layer%2520to%2520incorporate%2520class%2520information%2520to%2520make%2520predictions%252C%2520we%250Ainvestigate%2520a%2520proactive%2520approach%252C%2520asking%2520each%2520class%2520to%2520search%2520for%2520itself%2520in%2520an%250Aimage.%2520We%2520realize%2520this%2520idea%2520via%2520a%2520Transformer%2520encoder-decoder%2520inspired%2520by%250ADEtection%2520TRansformer%2520%2528DETR%2529.%2520We%2520learn%2520%2522class-specific%2522%2520queries%2520%2528one%2520for%2520each%250Aclass%2529%2520as%2520input%2520to%2520the%2520decoder%252C%2520enabling%2520each%2520class%2520to%2520localize%2520its%2520patterns%2520in%250Aan%2520image%2520via%2520cross-attention.%2520We%2520name%2520our%2520approach%2520INterpretable%2520TRansformer%250A%2528INTR%2529%252C%2520which%2520is%2520fairly%2520easy%2520to%2520implement%2520and%2520exhibits%2520several%2520compelling%250Aproperties.%2520We%2520show%2520that%2520INTR%2520intrinsically%2520encourages%2520each%2520class%2520to%2520attend%250Adistinctively%253B%2520the%2520cross-attention%2520weights%2520thus%2520provide%2520a%2520faithful%250Ainterpretation%2520of%2520the%2520prediction.%2520Interestingly%252C%2520via%2520%2522multi-head%2522%250Across-attention%252C%2520INTR%2520could%2520identify%2520different%2520%2522attributes%2522%2520of%2520a%2520class%252C%2520making%250Ait%2520particularly%2520suitable%2520for%2520fine-grained%2520classification%2520and%2520analysis%252C%2520which%2520we%250Ademonstrate%2520on%2520eight%2520datasets.%2520Our%2520code%2520and%2520pre-trained%2520models%2520are%2520publicly%250Aaccessible%2520at%2520the%2520Imageomics%2520Institute%2520GitHub%2520site%253A%250Ahttps%253A//github.com/Imageomics/INTR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04157v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20Interpretable%20Transformer%20for%20Fine-Grained%20Image%20Classification%0A%20%20and%20Analysis&entry.906535625=Dipanjyoti%20Paul%20and%20Arpita%20Chowdhury%20and%20Xinqi%20Xiong%20and%20Feng-Ju%20Chang%20and%20David%20Carlyn%20and%20Samuel%20Stevens%20and%20Kaiya%20L.%20Provost%20and%20Anuj%20Karpatne%20and%20Bryan%20Carstens%20and%20Daniel%20Rubenstein%20and%20Charles%20Stewart%20and%20Tanya%20Berger-Wolf%20and%20Yu%20Su%20and%20Wei-Lun%20Chao&entry.1292438233=%20%20We%20present%20a%20novel%20usage%20of%20Transformers%20to%20make%20image%20classification%0Ainterpretable.%20Unlike%20mainstream%20classifiers%20that%20wait%20until%20the%20last%20fully%0Aconnected%20layer%20to%20incorporate%20class%20information%20to%20make%20predictions%2C%20we%0Ainvestigate%20a%20proactive%20approach%2C%20asking%20each%20class%20to%20search%20for%20itself%20in%20an%0Aimage.%20We%20realize%20this%20idea%20via%20a%20Transformer%20encoder-decoder%20inspired%20by%0ADEtection%20TRansformer%20%28DETR%29.%20We%20learn%20%22class-specific%22%20queries%20%28one%20for%20each%0Aclass%29%20as%20input%20to%20the%20decoder%2C%20enabling%20each%20class%20to%20localize%20its%20patterns%20in%0Aan%20image%20via%20cross-attention.%20We%20name%20our%20approach%20INterpretable%20TRansformer%0A%28INTR%29%2C%20which%20is%20fairly%20easy%20to%20implement%20and%20exhibits%20several%20compelling%0Aproperties.%20We%20show%20that%20INTR%20intrinsically%20encourages%20each%20class%20to%20attend%0Adistinctively%3B%20the%20cross-attention%20weights%20thus%20provide%20a%20faithful%0Ainterpretation%20of%20the%20prediction.%20Interestingly%2C%20via%20%22multi-head%22%0Across-attention%2C%20INTR%20could%20identify%20different%20%22attributes%22%20of%20a%20class%2C%20making%0Ait%20particularly%20suitable%20for%20fine-grained%20classification%20and%20analysis%2C%20which%20we%0Ademonstrate%20on%20eight%20datasets.%20Our%20code%20and%20pre-trained%20models%20are%20publicly%0Aaccessible%20at%20the%20Imageomics%20Institute%20GitHub%20site%3A%0Ahttps%3A//github.com/Imageomics/INTR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04157v3&entry.124074799=Read"},
{"title": "AstroCLIP: A Cross-Modal Foundation Model for Galaxies", "author": "Liam Parker and Francois Lanusse and Siavash Golkar and Leopoldo Sarra and Miles Cranmer and Alberto Bietti and Michael Eickenberg and Geraud Krawezik and Michael McCabe and Ruben Ohana and Mariel Pettee and Bruno Regaldo-Saint Blancard and Tiberiu Tesileanu and Kyunghyun Cho and Shirley Ho", "abstract": "  We present AstroCLIP, a single, versatile model that can embed both galaxy\nimages and spectra into a shared, physically meaningful latent space. These\nembeddings can then be used - without any model fine-tuning - for a variety of\ndownstream tasks including (1) accurate in-modality and cross-modality semantic\nsimilarity search, (2) photometric redshift estimation, (3) galaxy property\nestimation from both images and spectra, and (4) morphology classification. Our\napproach to implementing AstroCLIP consists of two parts. First, we embed\ngalaxy images and spectra separately by pretraining separate transformer-based\nimage and spectrum encoders in self-supervised settings. We then align the\nencoders using a contrastive loss. We apply our method to spectra from the Dark\nEnergy Spectroscopic Instrument and images from its corresponding Legacy\nImaging Survey. Overall, we find remarkable performance on all downstream\ntasks, even relative to supervised baselines. For example, for a task like\nphotometric redshift prediction, we find similar performance to a\nspecifically-trained ResNet18, and for additional tasks like physical property\nestimation (stellar mass, age, metallicity, and sSFR), we beat this supervised\nbaseline by 19\\% in terms of $R^2$. We also compare our results to a\nstate-of-the-art self-supervised single-modal model for galaxy images, and find\nthat our approach outperforms this benchmark by roughly a factor of two on\nphotometric redshift estimation and physical property prediction in terms of\n$R^2$, while remaining roughly in-line in terms of morphology classification.\nUltimately, our approach represents the first cross-modal self-supervised model\nfor galaxies, and the first self-supervised transformer-based architectures for\ngalaxy images and spectra.\n", "link": "http://arxiv.org/abs/2310.03024v2", "date": "2024-06-14", "relevancy": 2.1753, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6088}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5032}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AstroCLIP%3A%20A%20Cross-Modal%20Foundation%20Model%20for%20Galaxies&body=Title%3A%20AstroCLIP%3A%20A%20Cross-Modal%20Foundation%20Model%20for%20Galaxies%0AAuthor%3A%20Liam%20Parker%20and%20Francois%20Lanusse%20and%20Siavash%20Golkar%20and%20Leopoldo%20Sarra%20and%20Miles%20Cranmer%20and%20Alberto%20Bietti%20and%20Michael%20Eickenberg%20and%20Geraud%20Krawezik%20and%20Michael%20McCabe%20and%20Ruben%20Ohana%20and%20Mariel%20Pettee%20and%20Bruno%20Regaldo-Saint%20Blancard%20and%20Tiberiu%20Tesileanu%20and%20Kyunghyun%20Cho%20and%20Shirley%20Ho%0AAbstract%3A%20%20%20We%20present%20AstroCLIP%2C%20a%20single%2C%20versatile%20model%20that%20can%20embed%20both%20galaxy%0Aimages%20and%20spectra%20into%20a%20shared%2C%20physically%20meaningful%20latent%20space.%20These%0Aembeddings%20can%20then%20be%20used%20-%20without%20any%20model%20fine-tuning%20-%20for%20a%20variety%20of%0Adownstream%20tasks%20including%20%281%29%20accurate%20in-modality%20and%20cross-modality%20semantic%0Asimilarity%20search%2C%20%282%29%20photometric%20redshift%20estimation%2C%20%283%29%20galaxy%20property%0Aestimation%20from%20both%20images%20and%20spectra%2C%20and%20%284%29%20morphology%20classification.%20Our%0Aapproach%20to%20implementing%20AstroCLIP%20consists%20of%20two%20parts.%20First%2C%20we%20embed%0Agalaxy%20images%20and%20spectra%20separately%20by%20pretraining%20separate%20transformer-based%0Aimage%20and%20spectrum%20encoders%20in%20self-supervised%20settings.%20We%20then%20align%20the%0Aencoders%20using%20a%20contrastive%20loss.%20We%20apply%20our%20method%20to%20spectra%20from%20the%20Dark%0AEnergy%20Spectroscopic%20Instrument%20and%20images%20from%20its%20corresponding%20Legacy%0AImaging%20Survey.%20Overall%2C%20we%20find%20remarkable%20performance%20on%20all%20downstream%0Atasks%2C%20even%20relative%20to%20supervised%20baselines.%20For%20example%2C%20for%20a%20task%20like%0Aphotometric%20redshift%20prediction%2C%20we%20find%20similar%20performance%20to%20a%0Aspecifically-trained%20ResNet18%2C%20and%20for%20additional%20tasks%20like%20physical%20property%0Aestimation%20%28stellar%20mass%2C%20age%2C%20metallicity%2C%20and%20sSFR%29%2C%20we%20beat%20this%20supervised%0Abaseline%20by%2019%5C%25%20in%20terms%20of%20%24R%5E2%24.%20We%20also%20compare%20our%20results%20to%20a%0Astate-of-the-art%20self-supervised%20single-modal%20model%20for%20galaxy%20images%2C%20and%20find%0Athat%20our%20approach%20outperforms%20this%20benchmark%20by%20roughly%20a%20factor%20of%20two%20on%0Aphotometric%20redshift%20estimation%20and%20physical%20property%20prediction%20in%20terms%20of%0A%24R%5E2%24%2C%20while%20remaining%20roughly%20in-line%20in%20terms%20of%20morphology%20classification.%0AUltimately%2C%20our%20approach%20represents%20the%20first%20cross-modal%20self-supervised%20model%0Afor%20galaxies%2C%20and%20the%20first%20self-supervised%20transformer-based%20architectures%20for%0Agalaxy%20images%20and%20spectra.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03024v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAstroCLIP%253A%2520A%2520Cross-Modal%2520Foundation%2520Model%2520for%2520Galaxies%26entry.906535625%3DLiam%2520Parker%2520and%2520Francois%2520Lanusse%2520and%2520Siavash%2520Golkar%2520and%2520Leopoldo%2520Sarra%2520and%2520Miles%2520Cranmer%2520and%2520Alberto%2520Bietti%2520and%2520Michael%2520Eickenberg%2520and%2520Geraud%2520Krawezik%2520and%2520Michael%2520McCabe%2520and%2520Ruben%2520Ohana%2520and%2520Mariel%2520Pettee%2520and%2520Bruno%2520Regaldo-Saint%2520Blancard%2520and%2520Tiberiu%2520Tesileanu%2520and%2520Kyunghyun%2520Cho%2520and%2520Shirley%2520Ho%26entry.1292438233%3D%2520%2520We%2520present%2520AstroCLIP%252C%2520a%2520single%252C%2520versatile%2520model%2520that%2520can%2520embed%2520both%2520galaxy%250Aimages%2520and%2520spectra%2520into%2520a%2520shared%252C%2520physically%2520meaningful%2520latent%2520space.%2520These%250Aembeddings%2520can%2520then%2520be%2520used%2520-%2520without%2520any%2520model%2520fine-tuning%2520-%2520for%2520a%2520variety%2520of%250Adownstream%2520tasks%2520including%2520%25281%2529%2520accurate%2520in-modality%2520and%2520cross-modality%2520semantic%250Asimilarity%2520search%252C%2520%25282%2529%2520photometric%2520redshift%2520estimation%252C%2520%25283%2529%2520galaxy%2520property%250Aestimation%2520from%2520both%2520images%2520and%2520spectra%252C%2520and%2520%25284%2529%2520morphology%2520classification.%2520Our%250Aapproach%2520to%2520implementing%2520AstroCLIP%2520consists%2520of%2520two%2520parts.%2520First%252C%2520we%2520embed%250Agalaxy%2520images%2520and%2520spectra%2520separately%2520by%2520pretraining%2520separate%2520transformer-based%250Aimage%2520and%2520spectrum%2520encoders%2520in%2520self-supervised%2520settings.%2520We%2520then%2520align%2520the%250Aencoders%2520using%2520a%2520contrastive%2520loss.%2520We%2520apply%2520our%2520method%2520to%2520spectra%2520from%2520the%2520Dark%250AEnergy%2520Spectroscopic%2520Instrument%2520and%2520images%2520from%2520its%2520corresponding%2520Legacy%250AImaging%2520Survey.%2520Overall%252C%2520we%2520find%2520remarkable%2520performance%2520on%2520all%2520downstream%250Atasks%252C%2520even%2520relative%2520to%2520supervised%2520baselines.%2520For%2520example%252C%2520for%2520a%2520task%2520like%250Aphotometric%2520redshift%2520prediction%252C%2520we%2520find%2520similar%2520performance%2520to%2520a%250Aspecifically-trained%2520ResNet18%252C%2520and%2520for%2520additional%2520tasks%2520like%2520physical%2520property%250Aestimation%2520%2528stellar%2520mass%252C%2520age%252C%2520metallicity%252C%2520and%2520sSFR%2529%252C%2520we%2520beat%2520this%2520supervised%250Abaseline%2520by%252019%255C%2525%2520in%2520terms%2520of%2520%2524R%255E2%2524.%2520We%2520also%2520compare%2520our%2520results%2520to%2520a%250Astate-of-the-art%2520self-supervised%2520single-modal%2520model%2520for%2520galaxy%2520images%252C%2520and%2520find%250Athat%2520our%2520approach%2520outperforms%2520this%2520benchmark%2520by%2520roughly%2520a%2520factor%2520of%2520two%2520on%250Aphotometric%2520redshift%2520estimation%2520and%2520physical%2520property%2520prediction%2520in%2520terms%2520of%250A%2524R%255E2%2524%252C%2520while%2520remaining%2520roughly%2520in-line%2520in%2520terms%2520of%2520morphology%2520classification.%250AUltimately%252C%2520our%2520approach%2520represents%2520the%2520first%2520cross-modal%2520self-supervised%2520model%250Afor%2520galaxies%252C%2520and%2520the%2520first%2520self-supervised%2520transformer-based%2520architectures%2520for%250Agalaxy%2520images%2520and%2520spectra.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03024v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AstroCLIP%3A%20A%20Cross-Modal%20Foundation%20Model%20for%20Galaxies&entry.906535625=Liam%20Parker%20and%20Francois%20Lanusse%20and%20Siavash%20Golkar%20and%20Leopoldo%20Sarra%20and%20Miles%20Cranmer%20and%20Alberto%20Bietti%20and%20Michael%20Eickenberg%20and%20Geraud%20Krawezik%20and%20Michael%20McCabe%20and%20Ruben%20Ohana%20and%20Mariel%20Pettee%20and%20Bruno%20Regaldo-Saint%20Blancard%20and%20Tiberiu%20Tesileanu%20and%20Kyunghyun%20Cho%20and%20Shirley%20Ho&entry.1292438233=%20%20We%20present%20AstroCLIP%2C%20a%20single%2C%20versatile%20model%20that%20can%20embed%20both%20galaxy%0Aimages%20and%20spectra%20into%20a%20shared%2C%20physically%20meaningful%20latent%20space.%20These%0Aembeddings%20can%20then%20be%20used%20-%20without%20any%20model%20fine-tuning%20-%20for%20a%20variety%20of%0Adownstream%20tasks%20including%20%281%29%20accurate%20in-modality%20and%20cross-modality%20semantic%0Asimilarity%20search%2C%20%282%29%20photometric%20redshift%20estimation%2C%20%283%29%20galaxy%20property%0Aestimation%20from%20both%20images%20and%20spectra%2C%20and%20%284%29%20morphology%20classification.%20Our%0Aapproach%20to%20implementing%20AstroCLIP%20consists%20of%20two%20parts.%20First%2C%20we%20embed%0Agalaxy%20images%20and%20spectra%20separately%20by%20pretraining%20separate%20transformer-based%0Aimage%20and%20spectrum%20encoders%20in%20self-supervised%20settings.%20We%20then%20align%20the%0Aencoders%20using%20a%20contrastive%20loss.%20We%20apply%20our%20method%20to%20spectra%20from%20the%20Dark%0AEnergy%20Spectroscopic%20Instrument%20and%20images%20from%20its%20corresponding%20Legacy%0AImaging%20Survey.%20Overall%2C%20we%20find%20remarkable%20performance%20on%20all%20downstream%0Atasks%2C%20even%20relative%20to%20supervised%20baselines.%20For%20example%2C%20for%20a%20task%20like%0Aphotometric%20redshift%20prediction%2C%20we%20find%20similar%20performance%20to%20a%0Aspecifically-trained%20ResNet18%2C%20and%20for%20additional%20tasks%20like%20physical%20property%0Aestimation%20%28stellar%20mass%2C%20age%2C%20metallicity%2C%20and%20sSFR%29%2C%20we%20beat%20this%20supervised%0Abaseline%20by%2019%5C%25%20in%20terms%20of%20%24R%5E2%24.%20We%20also%20compare%20our%20results%20to%20a%0Astate-of-the-art%20self-supervised%20single-modal%20model%20for%20galaxy%20images%2C%20and%20find%0Athat%20our%20approach%20outperforms%20this%20benchmark%20by%20roughly%20a%20factor%20of%20two%20on%0Aphotometric%20redshift%20estimation%20and%20physical%20property%20prediction%20in%20terms%20of%0A%24R%5E2%24%2C%20while%20remaining%20roughly%20in-line%20in%20terms%20of%20morphology%20classification.%0AUltimately%2C%20our%20approach%20represents%20the%20first%20cross-modal%20self-supervised%20model%0Afor%20galaxies%2C%20and%20the%20first%20self-supervised%20transformer-based%20architectures%20for%0Agalaxy%20images%20and%20spectra.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03024v2&entry.124074799=Read"},
{"title": "Sparse Graphical Linear Dynamical Systems", "author": "Emilie Chouzenoux and Victor Elvira", "abstract": "  Time-series datasets are central in machine learning with applications in\nnumerous fields of science and engineering, such as biomedicine, Earth\nobservation, and network analysis. Extensive research exists on state-space\nmodels (SSMs), which are powerful mathematical tools that allow for\nprobabilistic and interpretable learning on time series. Learning the model\nparameters in SSMs is arguably one of the most complicated tasks, and the\ninclusion of prior knowledge is known to both ease the interpretation but also\nto complicate the inferential tasks. Very recent works have attempted to\nincorporate a graphical perspective on some of those model parameters, but they\npresent notable limitations that this work addresses. More generally, existing\ngraphical modeling tools are designed to incorporate either static information,\nfocusing on statistical dependencies among independent random variables (e.g.,\ngraphical Lasso approach), or dynamic information, emphasizing causal\nrelationships among time series samples (e.g., graphical Granger approaches).\nHowever, there are no joint approaches combining static and dynamic graphical\nmodeling within the context of SSMs. This work proposes a novel approach to\nfill this gap by introducing a joint graphical modeling framework that bridges\nthe graphical Lasso model and a causal-based graphical approach for the\nlinear-Gaussian SSM. We present DGLASSO (Dynamic Graphical Lasso), a new\ninference method within this framework that implements an efficient block\nalternating majorization-minimization algorithm. The algorithm's convergence is\nestablished by departing from modern tools from nonlinear analysis.\nExperimental validation on various synthetic data showcases the effectiveness\nof the proposed model and inference algorithm.\n", "link": "http://arxiv.org/abs/2307.03210v2", "date": "2024-06-14", "relevancy": 2.1733, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5773}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5255}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Graphical%20Linear%20Dynamical%20Systems&body=Title%3A%20Sparse%20Graphical%20Linear%20Dynamical%20Systems%0AAuthor%3A%20Emilie%20Chouzenoux%20and%20Victor%20Elvira%0AAbstract%3A%20%20%20Time-series%20datasets%20are%20central%20in%20machine%20learning%20with%20applications%20in%0Anumerous%20fields%20of%20science%20and%20engineering%2C%20such%20as%20biomedicine%2C%20Earth%0Aobservation%2C%20and%20network%20analysis.%20Extensive%20research%20exists%20on%20state-space%0Amodels%20%28SSMs%29%2C%20which%20are%20powerful%20mathematical%20tools%20that%20allow%20for%0Aprobabilistic%20and%20interpretable%20learning%20on%20time%20series.%20Learning%20the%20model%0Aparameters%20in%20SSMs%20is%20arguably%20one%20of%20the%20most%20complicated%20tasks%2C%20and%20the%0Ainclusion%20of%20prior%20knowledge%20is%20known%20to%20both%20ease%20the%20interpretation%20but%20also%0Ato%20complicate%20the%20inferential%20tasks.%20Very%20recent%20works%20have%20attempted%20to%0Aincorporate%20a%20graphical%20perspective%20on%20some%20of%20those%20model%20parameters%2C%20but%20they%0Apresent%20notable%20limitations%20that%20this%20work%20addresses.%20More%20generally%2C%20existing%0Agraphical%20modeling%20tools%20are%20designed%20to%20incorporate%20either%20static%20information%2C%0Afocusing%20on%20statistical%20dependencies%20among%20independent%20random%20variables%20%28e.g.%2C%0Agraphical%20Lasso%20approach%29%2C%20or%20dynamic%20information%2C%20emphasizing%20causal%0Arelationships%20among%20time%20series%20samples%20%28e.g.%2C%20graphical%20Granger%20approaches%29.%0AHowever%2C%20there%20are%20no%20joint%20approaches%20combining%20static%20and%20dynamic%20graphical%0Amodeling%20within%20the%20context%20of%20SSMs.%20This%20work%20proposes%20a%20novel%20approach%20to%0Afill%20this%20gap%20by%20introducing%20a%20joint%20graphical%20modeling%20framework%20that%20bridges%0Athe%20graphical%20Lasso%20model%20and%20a%20causal-based%20graphical%20approach%20for%20the%0Alinear-Gaussian%20SSM.%20We%20present%20DGLASSO%20%28Dynamic%20Graphical%20Lasso%29%2C%20a%20new%0Ainference%20method%20within%20this%20framework%20that%20implements%20an%20efficient%20block%0Aalternating%20majorization-minimization%20algorithm.%20The%20algorithm%27s%20convergence%20is%0Aestablished%20by%20departing%20from%20modern%20tools%20from%20nonlinear%20analysis.%0AExperimental%20validation%20on%20various%20synthetic%20data%20showcases%20the%20effectiveness%0Aof%20the%20proposed%20model%20and%20inference%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.03210v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Graphical%2520Linear%2520Dynamical%2520Systems%26entry.906535625%3DEmilie%2520Chouzenoux%2520and%2520Victor%2520Elvira%26entry.1292438233%3D%2520%2520Time-series%2520datasets%2520are%2520central%2520in%2520machine%2520learning%2520with%2520applications%2520in%250Anumerous%2520fields%2520of%2520science%2520and%2520engineering%252C%2520such%2520as%2520biomedicine%252C%2520Earth%250Aobservation%252C%2520and%2520network%2520analysis.%2520Extensive%2520research%2520exists%2520on%2520state-space%250Amodels%2520%2528SSMs%2529%252C%2520which%2520are%2520powerful%2520mathematical%2520tools%2520that%2520allow%2520for%250Aprobabilistic%2520and%2520interpretable%2520learning%2520on%2520time%2520series.%2520Learning%2520the%2520model%250Aparameters%2520in%2520SSMs%2520is%2520arguably%2520one%2520of%2520the%2520most%2520complicated%2520tasks%252C%2520and%2520the%250Ainclusion%2520of%2520prior%2520knowledge%2520is%2520known%2520to%2520both%2520ease%2520the%2520interpretation%2520but%2520also%250Ato%2520complicate%2520the%2520inferential%2520tasks.%2520Very%2520recent%2520works%2520have%2520attempted%2520to%250Aincorporate%2520a%2520graphical%2520perspective%2520on%2520some%2520of%2520those%2520model%2520parameters%252C%2520but%2520they%250Apresent%2520notable%2520limitations%2520that%2520this%2520work%2520addresses.%2520More%2520generally%252C%2520existing%250Agraphical%2520modeling%2520tools%2520are%2520designed%2520to%2520incorporate%2520either%2520static%2520information%252C%250Afocusing%2520on%2520statistical%2520dependencies%2520among%2520independent%2520random%2520variables%2520%2528e.g.%252C%250Agraphical%2520Lasso%2520approach%2529%252C%2520or%2520dynamic%2520information%252C%2520emphasizing%2520causal%250Arelationships%2520among%2520time%2520series%2520samples%2520%2528e.g.%252C%2520graphical%2520Granger%2520approaches%2529.%250AHowever%252C%2520there%2520are%2520no%2520joint%2520approaches%2520combining%2520static%2520and%2520dynamic%2520graphical%250Amodeling%2520within%2520the%2520context%2520of%2520SSMs.%2520This%2520work%2520proposes%2520a%2520novel%2520approach%2520to%250Afill%2520this%2520gap%2520by%2520introducing%2520a%2520joint%2520graphical%2520modeling%2520framework%2520that%2520bridges%250Athe%2520graphical%2520Lasso%2520model%2520and%2520a%2520causal-based%2520graphical%2520approach%2520for%2520the%250Alinear-Gaussian%2520SSM.%2520We%2520present%2520DGLASSO%2520%2528Dynamic%2520Graphical%2520Lasso%2529%252C%2520a%2520new%250Ainference%2520method%2520within%2520this%2520framework%2520that%2520implements%2520an%2520efficient%2520block%250Aalternating%2520majorization-minimization%2520algorithm.%2520The%2520algorithm%2527s%2520convergence%2520is%250Aestablished%2520by%2520departing%2520from%2520modern%2520tools%2520from%2520nonlinear%2520analysis.%250AExperimental%2520validation%2520on%2520various%2520synthetic%2520data%2520showcases%2520the%2520effectiveness%250Aof%2520the%2520proposed%2520model%2520and%2520inference%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.03210v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Graphical%20Linear%20Dynamical%20Systems&entry.906535625=Emilie%20Chouzenoux%20and%20Victor%20Elvira&entry.1292438233=%20%20Time-series%20datasets%20are%20central%20in%20machine%20learning%20with%20applications%20in%0Anumerous%20fields%20of%20science%20and%20engineering%2C%20such%20as%20biomedicine%2C%20Earth%0Aobservation%2C%20and%20network%20analysis.%20Extensive%20research%20exists%20on%20state-space%0Amodels%20%28SSMs%29%2C%20which%20are%20powerful%20mathematical%20tools%20that%20allow%20for%0Aprobabilistic%20and%20interpretable%20learning%20on%20time%20series.%20Learning%20the%20model%0Aparameters%20in%20SSMs%20is%20arguably%20one%20of%20the%20most%20complicated%20tasks%2C%20and%20the%0Ainclusion%20of%20prior%20knowledge%20is%20known%20to%20both%20ease%20the%20interpretation%20but%20also%0Ato%20complicate%20the%20inferential%20tasks.%20Very%20recent%20works%20have%20attempted%20to%0Aincorporate%20a%20graphical%20perspective%20on%20some%20of%20those%20model%20parameters%2C%20but%20they%0Apresent%20notable%20limitations%20that%20this%20work%20addresses.%20More%20generally%2C%20existing%0Agraphical%20modeling%20tools%20are%20designed%20to%20incorporate%20either%20static%20information%2C%0Afocusing%20on%20statistical%20dependencies%20among%20independent%20random%20variables%20%28e.g.%2C%0Agraphical%20Lasso%20approach%29%2C%20or%20dynamic%20information%2C%20emphasizing%20causal%0Arelationships%20among%20time%20series%20samples%20%28e.g.%2C%20graphical%20Granger%20approaches%29.%0AHowever%2C%20there%20are%20no%20joint%20approaches%20combining%20static%20and%20dynamic%20graphical%0Amodeling%20within%20the%20context%20of%20SSMs.%20This%20work%20proposes%20a%20novel%20approach%20to%0Afill%20this%20gap%20by%20introducing%20a%20joint%20graphical%20modeling%20framework%20that%20bridges%0Athe%20graphical%20Lasso%20model%20and%20a%20causal-based%20graphical%20approach%20for%20the%0Alinear-Gaussian%20SSM.%20We%20present%20DGLASSO%20%28Dynamic%20Graphical%20Lasso%29%2C%20a%20new%0Ainference%20method%20within%20this%20framework%20that%20implements%20an%20efficient%20block%0Aalternating%20majorization-minimization%20algorithm.%20The%20algorithm%27s%20convergence%20is%0Aestablished%20by%20departing%20from%20modern%20tools%20from%20nonlinear%20analysis.%0AExperimental%20validation%20on%20various%20synthetic%20data%20showcases%20the%20effectiveness%0Aof%20the%20proposed%20model%20and%20inference%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.03210v2&entry.124074799=Read"},
{"title": "Exploring the Benefits of Vision Foundation Models for Unsupervised\n  Domain Adaptation", "author": "Brun\u00f3 B. Englert and Fabrizio J. Piva and Tommie Kerssies and Daan de Geus and Gijs Dubbelman", "abstract": "  Achieving robust generalization across diverse data domains remains a\nsignificant challenge in computer vision. This challenge is important in\nsafety-critical applications, where deep-neural-network-based systems must\nperform reliably under various environmental conditions not seen during\ntraining. Our study investigates whether the generalization capabilities of\nVision Foundation Models (VFMs) and Unsupervised Domain Adaptation (UDA)\nmethods for the semantic segmentation task are complementary. Results show that\ncombining VFMs with UDA has two main benefits: (a) it allows for better UDA\nperformance while maintaining the out-of-distribution performance of VFMs, and\n(b) it makes certain time-consuming UDA components redundant, thus enabling\nsignificant inference speedups. Specifically, with equivalent model sizes, the\nresulting VFM-UDA method achieves an 8.4$\\times$ speed increase over the prior\nnon-VFM state of the art, while also improving performance by +1.2 mIoU in the\nUDA setting and by +6.1 mIoU in terms of out-of-distribution generalization.\nMoreover, when we use a VFM with 3.6$\\times$ more parameters, the VFM-UDA\napproach maintains a 3.3$\\times$ speed up, while improving the UDA performance\nby +3.1 mIoU and the out-of-distribution performance by +10.3 mIoU. These\nresults underscore the significant benefits of combining VFMs with UDA, setting\nnew standards and baselines for Unsupervised Domain Adaptation in semantic\nsegmentation.\n", "link": "http://arxiv.org/abs/2406.09896v1", "date": "2024-06-14", "relevancy": 2.1731, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5447}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5437}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Benefits%20of%20Vision%20Foundation%20Models%20for%20Unsupervised%0A%20%20Domain%20Adaptation&body=Title%3A%20Exploring%20the%20Benefits%20of%20Vision%20Foundation%20Models%20for%20Unsupervised%0A%20%20Domain%20Adaptation%0AAuthor%3A%20Brun%C3%B3%20B.%20Englert%20and%20Fabrizio%20J.%20Piva%20and%20Tommie%20Kerssies%20and%20Daan%20de%20Geus%20and%20Gijs%20Dubbelman%0AAbstract%3A%20%20%20Achieving%20robust%20generalization%20across%20diverse%20data%20domains%20remains%20a%0Asignificant%20challenge%20in%20computer%20vision.%20This%20challenge%20is%20important%20in%0Asafety-critical%20applications%2C%20where%20deep-neural-network-based%20systems%20must%0Aperform%20reliably%20under%20various%20environmental%20conditions%20not%20seen%20during%0Atraining.%20Our%20study%20investigates%20whether%20the%20generalization%20capabilities%20of%0AVision%20Foundation%20Models%20%28VFMs%29%20and%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%0Amethods%20for%20the%20semantic%20segmentation%20task%20are%20complementary.%20Results%20show%20that%0Acombining%20VFMs%20with%20UDA%20has%20two%20main%20benefits%3A%20%28a%29%20it%20allows%20for%20better%20UDA%0Aperformance%20while%20maintaining%20the%20out-of-distribution%20performance%20of%20VFMs%2C%20and%0A%28b%29%20it%20makes%20certain%20time-consuming%20UDA%20components%20redundant%2C%20thus%20enabling%0Asignificant%20inference%20speedups.%20Specifically%2C%20with%20equivalent%20model%20sizes%2C%20the%0Aresulting%20VFM-UDA%20method%20achieves%20an%208.4%24%5Ctimes%24%20speed%20increase%20over%20the%20prior%0Anon-VFM%20state%20of%20the%20art%2C%20while%20also%20improving%20performance%20by%20%2B1.2%20mIoU%20in%20the%0AUDA%20setting%20and%20by%20%2B6.1%20mIoU%20in%20terms%20of%20out-of-distribution%20generalization.%0AMoreover%2C%20when%20we%20use%20a%20VFM%20with%203.6%24%5Ctimes%24%20more%20parameters%2C%20the%20VFM-UDA%0Aapproach%20maintains%20a%203.3%24%5Ctimes%24%20speed%20up%2C%20while%20improving%20the%20UDA%20performance%0Aby%20%2B3.1%20mIoU%20and%20the%20out-of-distribution%20performance%20by%20%2B10.3%20mIoU.%20These%0Aresults%20underscore%20the%20significant%20benefits%20of%20combining%20VFMs%20with%20UDA%2C%20setting%0Anew%20standards%20and%20baselines%20for%20Unsupervised%20Domain%20Adaptation%20in%20semantic%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Benefits%2520of%2520Vision%2520Foundation%2520Models%2520for%2520Unsupervised%250A%2520%2520Domain%2520Adaptation%26entry.906535625%3DBrun%25C3%25B3%2520B.%2520Englert%2520and%2520Fabrizio%2520J.%2520Piva%2520and%2520Tommie%2520Kerssies%2520and%2520Daan%2520de%2520Geus%2520and%2520Gijs%2520Dubbelman%26entry.1292438233%3D%2520%2520Achieving%2520robust%2520generalization%2520across%2520diverse%2520data%2520domains%2520remains%2520a%250Asignificant%2520challenge%2520in%2520computer%2520vision.%2520This%2520challenge%2520is%2520important%2520in%250Asafety-critical%2520applications%252C%2520where%2520deep-neural-network-based%2520systems%2520must%250Aperform%2520reliably%2520under%2520various%2520environmental%2520conditions%2520not%2520seen%2520during%250Atraining.%2520Our%2520study%2520investigates%2520whether%2520the%2520generalization%2520capabilities%2520of%250AVision%2520Foundation%2520Models%2520%2528VFMs%2529%2520and%2520Unsupervised%2520Domain%2520Adaptation%2520%2528UDA%2529%250Amethods%2520for%2520the%2520semantic%2520segmentation%2520task%2520are%2520complementary.%2520Results%2520show%2520that%250Acombining%2520VFMs%2520with%2520UDA%2520has%2520two%2520main%2520benefits%253A%2520%2528a%2529%2520it%2520allows%2520for%2520better%2520UDA%250Aperformance%2520while%2520maintaining%2520the%2520out-of-distribution%2520performance%2520of%2520VFMs%252C%2520and%250A%2528b%2529%2520it%2520makes%2520certain%2520time-consuming%2520UDA%2520components%2520redundant%252C%2520thus%2520enabling%250Asignificant%2520inference%2520speedups.%2520Specifically%252C%2520with%2520equivalent%2520model%2520sizes%252C%2520the%250Aresulting%2520VFM-UDA%2520method%2520achieves%2520an%25208.4%2524%255Ctimes%2524%2520speed%2520increase%2520over%2520the%2520prior%250Anon-VFM%2520state%2520of%2520the%2520art%252C%2520while%2520also%2520improving%2520performance%2520by%2520%252B1.2%2520mIoU%2520in%2520the%250AUDA%2520setting%2520and%2520by%2520%252B6.1%2520mIoU%2520in%2520terms%2520of%2520out-of-distribution%2520generalization.%250AMoreover%252C%2520when%2520we%2520use%2520a%2520VFM%2520with%25203.6%2524%255Ctimes%2524%2520more%2520parameters%252C%2520the%2520VFM-UDA%250Aapproach%2520maintains%2520a%25203.3%2524%255Ctimes%2524%2520speed%2520up%252C%2520while%2520improving%2520the%2520UDA%2520performance%250Aby%2520%252B3.1%2520mIoU%2520and%2520the%2520out-of-distribution%2520performance%2520by%2520%252B10.3%2520mIoU.%2520These%250Aresults%2520underscore%2520the%2520significant%2520benefits%2520of%2520combining%2520VFMs%2520with%2520UDA%252C%2520setting%250Anew%2520standards%2520and%2520baselines%2520for%2520Unsupervised%2520Domain%2520Adaptation%2520in%2520semantic%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Benefits%20of%20Vision%20Foundation%20Models%20for%20Unsupervised%0A%20%20Domain%20Adaptation&entry.906535625=Brun%C3%B3%20B.%20Englert%20and%20Fabrizio%20J.%20Piva%20and%20Tommie%20Kerssies%20and%20Daan%20de%20Geus%20and%20Gijs%20Dubbelman&entry.1292438233=%20%20Achieving%20robust%20generalization%20across%20diverse%20data%20domains%20remains%20a%0Asignificant%20challenge%20in%20computer%20vision.%20This%20challenge%20is%20important%20in%0Asafety-critical%20applications%2C%20where%20deep-neural-network-based%20systems%20must%0Aperform%20reliably%20under%20various%20environmental%20conditions%20not%20seen%20during%0Atraining.%20Our%20study%20investigates%20whether%20the%20generalization%20capabilities%20of%0AVision%20Foundation%20Models%20%28VFMs%29%20and%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%0Amethods%20for%20the%20semantic%20segmentation%20task%20are%20complementary.%20Results%20show%20that%0Acombining%20VFMs%20with%20UDA%20has%20two%20main%20benefits%3A%20%28a%29%20it%20allows%20for%20better%20UDA%0Aperformance%20while%20maintaining%20the%20out-of-distribution%20performance%20of%20VFMs%2C%20and%0A%28b%29%20it%20makes%20certain%20time-consuming%20UDA%20components%20redundant%2C%20thus%20enabling%0Asignificant%20inference%20speedups.%20Specifically%2C%20with%20equivalent%20model%20sizes%2C%20the%0Aresulting%20VFM-UDA%20method%20achieves%20an%208.4%24%5Ctimes%24%20speed%20increase%20over%20the%20prior%0Anon-VFM%20state%20of%20the%20art%2C%20while%20also%20improving%20performance%20by%20%2B1.2%20mIoU%20in%20the%0AUDA%20setting%20and%20by%20%2B6.1%20mIoU%20in%20terms%20of%20out-of-distribution%20generalization.%0AMoreover%2C%20when%20we%20use%20a%20VFM%20with%203.6%24%5Ctimes%24%20more%20parameters%2C%20the%20VFM-UDA%0Aapproach%20maintains%20a%203.3%24%5Ctimes%24%20speed%20up%2C%20while%20improving%20the%20UDA%20performance%0Aby%20%2B3.1%20mIoU%20and%20the%20out-of-distribution%20performance%20by%20%2B10.3%20mIoU.%20These%0Aresults%20underscore%20the%20significant%20benefits%20of%20combining%20VFMs%20with%20UDA%2C%20setting%0Anew%20standards%20and%20baselines%20for%20Unsupervised%20Domain%20Adaptation%20in%20semantic%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09896v1&entry.124074799=Read"},
{"title": "Robust compressive tracking via online weighted multiple instance\n  learning", "author": "Sandeep Singh Sengar", "abstract": "  Developing a robust object tracker is a challenging task due to factors such\nas occlusion, motion blur, fast motion, illumination variations, rotation,\nbackground clutter, low resolution and deformation across the frames. In the\nliterature, lots of good approaches based on sparse representation have already\nbeen presented to tackle the above problems. However, most of the algorithms do\nnot focus on the learning of sparse representation. They only consider the\nmodeling of target appearance and therefore drift away from the target with the\nimprecise training samples. By considering all the above factors in mind, we\nhave proposed a visual object tracking algorithm by integrating a\ncoarse-to-fine search strategy based on sparse representation and the weighted\nmultiple instance learning (WMIL) algorithm. Compared with the other trackers,\nour approach has more information of the original signal with less complexity\ndue to the coarse-to-fine search method, and also has weights for important\nsamples. Thus, it can easily discriminate the background features from the\nforeground. Furthermore, we have also selected the samples from the un-occluded\nsub-regions to efficiently develop the strong classifier. As a consequence, a\nstable and robust object tracker is achieved to tackle all the aforementioned\nproblems. Experimental results with quantitative as well as qualitative\nanalysis on challenging benchmark datasets show the accuracy and efficiency of\nour method.\n", "link": "http://arxiv.org/abs/2406.09914v1", "date": "2024-06-14", "relevancy": 2.1599, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5462}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5377}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20compressive%20tracking%20via%20online%20weighted%20multiple%20instance%0A%20%20learning&body=Title%3A%20Robust%20compressive%20tracking%20via%20online%20weighted%20multiple%20instance%0A%20%20learning%0AAuthor%3A%20Sandeep%20Singh%20Sengar%0AAbstract%3A%20%20%20Developing%20a%20robust%20object%20tracker%20is%20a%20challenging%20task%20due%20to%20factors%20such%0Aas%20occlusion%2C%20motion%20blur%2C%20fast%20motion%2C%20illumination%20variations%2C%20rotation%2C%0Abackground%20clutter%2C%20low%20resolution%20and%20deformation%20across%20the%20frames.%20In%20the%0Aliterature%2C%20lots%20of%20good%20approaches%20based%20on%20sparse%20representation%20have%20already%0Abeen%20presented%20to%20tackle%20the%20above%20problems.%20However%2C%20most%20of%20the%20algorithms%20do%0Anot%20focus%20on%20the%20learning%20of%20sparse%20representation.%20They%20only%20consider%20the%0Amodeling%20of%20target%20appearance%20and%20therefore%20drift%20away%20from%20the%20target%20with%20the%0Aimprecise%20training%20samples.%20By%20considering%20all%20the%20above%20factors%20in%20mind%2C%20we%0Ahave%20proposed%20a%20visual%20object%20tracking%20algorithm%20by%20integrating%20a%0Acoarse-to-fine%20search%20strategy%20based%20on%20sparse%20representation%20and%20the%20weighted%0Amultiple%20instance%20learning%20%28WMIL%29%20algorithm.%20Compared%20with%20the%20other%20trackers%2C%0Aour%20approach%20has%20more%20information%20of%20the%20original%20signal%20with%20less%20complexity%0Adue%20to%20the%20coarse-to-fine%20search%20method%2C%20and%20also%20has%20weights%20for%20important%0Asamples.%20Thus%2C%20it%20can%20easily%20discriminate%20the%20background%20features%20from%20the%0Aforeground.%20Furthermore%2C%20we%20have%20also%20selected%20the%20samples%20from%20the%20un-occluded%0Asub-regions%20to%20efficiently%20develop%20the%20strong%20classifier.%20As%20a%20consequence%2C%20a%0Astable%20and%20robust%20object%20tracker%20is%20achieved%20to%20tackle%20all%20the%20aforementioned%0Aproblems.%20Experimental%20results%20with%20quantitative%20as%20well%20as%20qualitative%0Aanalysis%20on%20challenging%20benchmark%20datasets%20show%20the%20accuracy%20and%20efficiency%20of%0Aour%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520compressive%2520tracking%2520via%2520online%2520weighted%2520multiple%2520instance%250A%2520%2520learning%26entry.906535625%3DSandeep%2520Singh%2520Sengar%26entry.1292438233%3D%2520%2520Developing%2520a%2520robust%2520object%2520tracker%2520is%2520a%2520challenging%2520task%2520due%2520to%2520factors%2520such%250Aas%2520occlusion%252C%2520motion%2520blur%252C%2520fast%2520motion%252C%2520illumination%2520variations%252C%2520rotation%252C%250Abackground%2520clutter%252C%2520low%2520resolution%2520and%2520deformation%2520across%2520the%2520frames.%2520In%2520the%250Aliterature%252C%2520lots%2520of%2520good%2520approaches%2520based%2520on%2520sparse%2520representation%2520have%2520already%250Abeen%2520presented%2520to%2520tackle%2520the%2520above%2520problems.%2520However%252C%2520most%2520of%2520the%2520algorithms%2520do%250Anot%2520focus%2520on%2520the%2520learning%2520of%2520sparse%2520representation.%2520They%2520only%2520consider%2520the%250Amodeling%2520of%2520target%2520appearance%2520and%2520therefore%2520drift%2520away%2520from%2520the%2520target%2520with%2520the%250Aimprecise%2520training%2520samples.%2520By%2520considering%2520all%2520the%2520above%2520factors%2520in%2520mind%252C%2520we%250Ahave%2520proposed%2520a%2520visual%2520object%2520tracking%2520algorithm%2520by%2520integrating%2520a%250Acoarse-to-fine%2520search%2520strategy%2520based%2520on%2520sparse%2520representation%2520and%2520the%2520weighted%250Amultiple%2520instance%2520learning%2520%2528WMIL%2529%2520algorithm.%2520Compared%2520with%2520the%2520other%2520trackers%252C%250Aour%2520approach%2520has%2520more%2520information%2520of%2520the%2520original%2520signal%2520with%2520less%2520complexity%250Adue%2520to%2520the%2520coarse-to-fine%2520search%2520method%252C%2520and%2520also%2520has%2520weights%2520for%2520important%250Asamples.%2520Thus%252C%2520it%2520can%2520easily%2520discriminate%2520the%2520background%2520features%2520from%2520the%250Aforeground.%2520Furthermore%252C%2520we%2520have%2520also%2520selected%2520the%2520samples%2520from%2520the%2520un-occluded%250Asub-regions%2520to%2520efficiently%2520develop%2520the%2520strong%2520classifier.%2520As%2520a%2520consequence%252C%2520a%250Astable%2520and%2520robust%2520object%2520tracker%2520is%2520achieved%2520to%2520tackle%2520all%2520the%2520aforementioned%250Aproblems.%2520Experimental%2520results%2520with%2520quantitative%2520as%2520well%2520as%2520qualitative%250Aanalysis%2520on%2520challenging%2520benchmark%2520datasets%2520show%2520the%2520accuracy%2520and%2520efficiency%2520of%250Aour%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20compressive%20tracking%20via%20online%20weighted%20multiple%20instance%0A%20%20learning&entry.906535625=Sandeep%20Singh%20Sengar&entry.1292438233=%20%20Developing%20a%20robust%20object%20tracker%20is%20a%20challenging%20task%20due%20to%20factors%20such%0Aas%20occlusion%2C%20motion%20blur%2C%20fast%20motion%2C%20illumination%20variations%2C%20rotation%2C%0Abackground%20clutter%2C%20low%20resolution%20and%20deformation%20across%20the%20frames.%20In%20the%0Aliterature%2C%20lots%20of%20good%20approaches%20based%20on%20sparse%20representation%20have%20already%0Abeen%20presented%20to%20tackle%20the%20above%20problems.%20However%2C%20most%20of%20the%20algorithms%20do%0Anot%20focus%20on%20the%20learning%20of%20sparse%20representation.%20They%20only%20consider%20the%0Amodeling%20of%20target%20appearance%20and%20therefore%20drift%20away%20from%20the%20target%20with%20the%0Aimprecise%20training%20samples.%20By%20considering%20all%20the%20above%20factors%20in%20mind%2C%20we%0Ahave%20proposed%20a%20visual%20object%20tracking%20algorithm%20by%20integrating%20a%0Acoarse-to-fine%20search%20strategy%20based%20on%20sparse%20representation%20and%20the%20weighted%0Amultiple%20instance%20learning%20%28WMIL%29%20algorithm.%20Compared%20with%20the%20other%20trackers%2C%0Aour%20approach%20has%20more%20information%20of%20the%20original%20signal%20with%20less%20complexity%0Adue%20to%20the%20coarse-to-fine%20search%20method%2C%20and%20also%20has%20weights%20for%20important%0Asamples.%20Thus%2C%20it%20can%20easily%20discriminate%20the%20background%20features%20from%20the%0Aforeground.%20Furthermore%2C%20we%20have%20also%20selected%20the%20samples%20from%20the%20un-occluded%0Asub-regions%20to%20efficiently%20develop%20the%20strong%20classifier.%20As%20a%20consequence%2C%20a%0Astable%20and%20robust%20object%20tracker%20is%20achieved%20to%20tackle%20all%20the%20aforementioned%0Aproblems.%20Experimental%20results%20with%20quantitative%20as%20well%20as%20qualitative%0Aanalysis%20on%20challenging%20benchmark%20datasets%20show%20the%20accuracy%20and%20efficiency%20of%0Aour%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09914v1&entry.124074799=Read"},
{"title": "Localizing Events in Videos with Multimodal Queries", "author": "Gengyuan Zhang and Mang Ling Ada Fok and Yan Xia and Yansong Tang and Daniel Cremers and Philip Torr and Volker Tresp and Jindong Gu", "abstract": "  Video understanding is a pivotal task in the digital era, yet the dynamic and\nmultievent nature of videos makes them labor-intensive and computationally\ndemanding to process. Thus, localizing a specific event given a semantic query\nhas gained importance in both user-oriented applications like video search and\nacademic research into video foundation models. A significant limitation in\ncurrent research is that semantic queries are typically in natural language\nthat depicts the semantics of the target event. This setting overlooks the\npotential for multimodal semantic queries composed of images and texts. To\naddress this gap, we introduce a new benchmark, ICQ, for localizing events in\nvideos with multimodal queries, along with a new evaluation dataset\nICQ-Highlight. Our new benchmark aims to evaluate how well models can localize\nan event given a multimodal semantic query that consists of a reference image,\nwhich depicts the event, and a refinement text to adjust the images' semantics.\nTo systematically benchmark model performance, we include 4 styles of reference\nimages and 5 types of refinement texts, allowing us to explore model\nperformance across different domains. We propose 3 adaptation methods that\ntailor existing models to our new setting and evaluate 10 SOTA models, ranging\nfrom specialized to large-scale foundation models. We believe this benchmark is\nan initial step toward investigating multimodal queries in video event\nlocalization.\n", "link": "http://arxiv.org/abs/2406.10079v1", "date": "2024-06-14", "relevancy": 2.1472, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5546}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5408}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localizing%20Events%20in%20Videos%20with%20Multimodal%20Queries&body=Title%3A%20Localizing%20Events%20in%20Videos%20with%20Multimodal%20Queries%0AAuthor%3A%20Gengyuan%20Zhang%20and%20Mang%20Ling%20Ada%20Fok%20and%20Yan%20Xia%20and%20Yansong%20Tang%20and%20Daniel%20Cremers%20and%20Philip%20Torr%20and%20Volker%20Tresp%20and%20Jindong%20Gu%0AAbstract%3A%20%20%20Video%20understanding%20is%20a%20pivotal%20task%20in%20the%20digital%20era%2C%20yet%20the%20dynamic%20and%0Amultievent%20nature%20of%20videos%20makes%20them%20labor-intensive%20and%20computationally%0Ademanding%20to%20process.%20Thus%2C%20localizing%20a%20specific%20event%20given%20a%20semantic%20query%0Ahas%20gained%20importance%20in%20both%20user-oriented%20applications%20like%20video%20search%20and%0Aacademic%20research%20into%20video%20foundation%20models.%20A%20significant%20limitation%20in%0Acurrent%20research%20is%20that%20semantic%20queries%20are%20typically%20in%20natural%20language%0Athat%20depicts%20the%20semantics%20of%20the%20target%20event.%20This%20setting%20overlooks%20the%0Apotential%20for%20multimodal%20semantic%20queries%20composed%20of%20images%20and%20texts.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20a%20new%20benchmark%2C%20ICQ%2C%20for%20localizing%20events%20in%0Avideos%20with%20multimodal%20queries%2C%20along%20with%20a%20new%20evaluation%20dataset%0AICQ-Highlight.%20Our%20new%20benchmark%20aims%20to%20evaluate%20how%20well%20models%20can%20localize%0Aan%20event%20given%20a%20multimodal%20semantic%20query%20that%20consists%20of%20a%20reference%20image%2C%0Awhich%20depicts%20the%20event%2C%20and%20a%20refinement%20text%20to%20adjust%20the%20images%27%20semantics.%0ATo%20systematically%20benchmark%20model%20performance%2C%20we%20include%204%20styles%20of%20reference%0Aimages%20and%205%20types%20of%20refinement%20texts%2C%20allowing%20us%20to%20explore%20model%0Aperformance%20across%20different%20domains.%20We%20propose%203%20adaptation%20methods%20that%0Atailor%20existing%20models%20to%20our%20new%20setting%20and%20evaluate%2010%20SOTA%20models%2C%20ranging%0Afrom%20specialized%20to%20large-scale%20foundation%20models.%20We%20believe%20this%20benchmark%20is%0Aan%20initial%20step%20toward%20investigating%20multimodal%20queries%20in%20video%20event%0Alocalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalizing%2520Events%2520in%2520Videos%2520with%2520Multimodal%2520Queries%26entry.906535625%3DGengyuan%2520Zhang%2520and%2520Mang%2520Ling%2520Ada%2520Fok%2520and%2520Yan%2520Xia%2520and%2520Yansong%2520Tang%2520and%2520Daniel%2520Cremers%2520and%2520Philip%2520Torr%2520and%2520Volker%2520Tresp%2520and%2520Jindong%2520Gu%26entry.1292438233%3D%2520%2520Video%2520understanding%2520is%2520a%2520pivotal%2520task%2520in%2520the%2520digital%2520era%252C%2520yet%2520the%2520dynamic%2520and%250Amultievent%2520nature%2520of%2520videos%2520makes%2520them%2520labor-intensive%2520and%2520computationally%250Ademanding%2520to%2520process.%2520Thus%252C%2520localizing%2520a%2520specific%2520event%2520given%2520a%2520semantic%2520query%250Ahas%2520gained%2520importance%2520in%2520both%2520user-oriented%2520applications%2520like%2520video%2520search%2520and%250Aacademic%2520research%2520into%2520video%2520foundation%2520models.%2520A%2520significant%2520limitation%2520in%250Acurrent%2520research%2520is%2520that%2520semantic%2520queries%2520are%2520typically%2520in%2520natural%2520language%250Athat%2520depicts%2520the%2520semantics%2520of%2520the%2520target%2520event.%2520This%2520setting%2520overlooks%2520the%250Apotential%2520for%2520multimodal%2520semantic%2520queries%2520composed%2520of%2520images%2520and%2520texts.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520a%2520new%2520benchmark%252C%2520ICQ%252C%2520for%2520localizing%2520events%2520in%250Avideos%2520with%2520multimodal%2520queries%252C%2520along%2520with%2520a%2520new%2520evaluation%2520dataset%250AICQ-Highlight.%2520Our%2520new%2520benchmark%2520aims%2520to%2520evaluate%2520how%2520well%2520models%2520can%2520localize%250Aan%2520event%2520given%2520a%2520multimodal%2520semantic%2520query%2520that%2520consists%2520of%2520a%2520reference%2520image%252C%250Awhich%2520depicts%2520the%2520event%252C%2520and%2520a%2520refinement%2520text%2520to%2520adjust%2520the%2520images%2527%2520semantics.%250ATo%2520systematically%2520benchmark%2520model%2520performance%252C%2520we%2520include%25204%2520styles%2520of%2520reference%250Aimages%2520and%25205%2520types%2520of%2520refinement%2520texts%252C%2520allowing%2520us%2520to%2520explore%2520model%250Aperformance%2520across%2520different%2520domains.%2520We%2520propose%25203%2520adaptation%2520methods%2520that%250Atailor%2520existing%2520models%2520to%2520our%2520new%2520setting%2520and%2520evaluate%252010%2520SOTA%2520models%252C%2520ranging%250Afrom%2520specialized%2520to%2520large-scale%2520foundation%2520models.%2520We%2520believe%2520this%2520benchmark%2520is%250Aan%2520initial%2520step%2520toward%2520investigating%2520multimodal%2520queries%2520in%2520video%2520event%250Alocalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localizing%20Events%20in%20Videos%20with%20Multimodal%20Queries&entry.906535625=Gengyuan%20Zhang%20and%20Mang%20Ling%20Ada%20Fok%20and%20Yan%20Xia%20and%20Yansong%20Tang%20and%20Daniel%20Cremers%20and%20Philip%20Torr%20and%20Volker%20Tresp%20and%20Jindong%20Gu&entry.1292438233=%20%20Video%20understanding%20is%20a%20pivotal%20task%20in%20the%20digital%20era%2C%20yet%20the%20dynamic%20and%0Amultievent%20nature%20of%20videos%20makes%20them%20labor-intensive%20and%20computationally%0Ademanding%20to%20process.%20Thus%2C%20localizing%20a%20specific%20event%20given%20a%20semantic%20query%0Ahas%20gained%20importance%20in%20both%20user-oriented%20applications%20like%20video%20search%20and%0Aacademic%20research%20into%20video%20foundation%20models.%20A%20significant%20limitation%20in%0Acurrent%20research%20is%20that%20semantic%20queries%20are%20typically%20in%20natural%20language%0Athat%20depicts%20the%20semantics%20of%20the%20target%20event.%20This%20setting%20overlooks%20the%0Apotential%20for%20multimodal%20semantic%20queries%20composed%20of%20images%20and%20texts.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20a%20new%20benchmark%2C%20ICQ%2C%20for%20localizing%20events%20in%0Avideos%20with%20multimodal%20queries%2C%20along%20with%20a%20new%20evaluation%20dataset%0AICQ-Highlight.%20Our%20new%20benchmark%20aims%20to%20evaluate%20how%20well%20models%20can%20localize%0Aan%20event%20given%20a%20multimodal%20semantic%20query%20that%20consists%20of%20a%20reference%20image%2C%0Awhich%20depicts%20the%20event%2C%20and%20a%20refinement%20text%20to%20adjust%20the%20images%27%20semantics.%0ATo%20systematically%20benchmark%20model%20performance%2C%20we%20include%204%20styles%20of%20reference%0Aimages%20and%205%20types%20of%20refinement%20texts%2C%20allowing%20us%20to%20explore%20model%0Aperformance%20across%20different%20domains.%20We%20propose%203%20adaptation%20methods%20that%0Atailor%20existing%20models%20to%20our%20new%20setting%20and%20evaluate%2010%20SOTA%20models%2C%20ranging%0Afrom%20specialized%20to%20large-scale%20foundation%20models.%20We%20believe%20this%20benchmark%20is%0Aan%20initial%20step%20toward%20investigating%20multimodal%20queries%20in%20video%20event%0Alocalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10079v1&entry.124074799=Read"},
{"title": "FRENETIX: A High-Performance and Modular Motion Planning Framework for\n  Autonomous Driving", "author": "Rainer Trauth and Korbinian Moller and Gerald Wuersching and Johannes Betz", "abstract": "  Our research introduces a modular motion planning framework for autonomous\nvehicles using a sampling-based trajectory planning algorithm. This approach\neffectively tackles the challenges of solution space construction and\noptimization in path planning. The algorithm is applicable to both real\nvehicles and simulations, offering a robust solution for complex autonomous\nnavigation. Our method employs a multi-objective optimization strategy for\nefficient navigation in static and highly dynamic environments, focusing on\noptimizing trajectory comfort, safety, and path precision. The algorithm is\nused to analyze the algorithm performance and success rate in 1750 virtual\ncomplex urban and highway scenarios. Our results demonstrate fast calculation\ntimes (8ms for 800 trajectories), a high success rate in complex scenarios\n(88%), and easy adaptability with different modules presented. The most\nnoticeable difference exhibited was the fast trajectory sampling, feasibility\ncheck, and cost evaluation step across various trajectory counts. We\ndemonstrate the integration and execution of the framework on real vehicles by\nevaluating deviations from the controller using a test track. This evaluation\nhighlights the algorithm's robustness and reliability, ensuring it meets the\nstringent requirements of real-world autonomous driving scenarios. The code and\nthe additional modules used in this research are publicly available as\nopen-source software and can be accessed at the following link:\nhttps://github.com/TUM-AVS/Frenetix-Motion-Planner.\n", "link": "http://arxiv.org/abs/2402.01443v2", "date": "2024-06-14", "relevancy": 2.1359, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5448}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5311}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRENETIX%3A%20A%20High-Performance%20and%20Modular%20Motion%20Planning%20Framework%20for%0A%20%20Autonomous%20Driving&body=Title%3A%20FRENETIX%3A%20A%20High-Performance%20and%20Modular%20Motion%20Planning%20Framework%20for%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Rainer%20Trauth%20and%20Korbinian%20Moller%20and%20Gerald%20Wuersching%20and%20Johannes%20Betz%0AAbstract%3A%20%20%20Our%20research%20introduces%20a%20modular%20motion%20planning%20framework%20for%20autonomous%0Avehicles%20using%20a%20sampling-based%20trajectory%20planning%20algorithm.%20This%20approach%0Aeffectively%20tackles%20the%20challenges%20of%20solution%20space%20construction%20and%0Aoptimization%20in%20path%20planning.%20The%20algorithm%20is%20applicable%20to%20both%20real%0Avehicles%20and%20simulations%2C%20offering%20a%20robust%20solution%20for%20complex%20autonomous%0Anavigation.%20Our%20method%20employs%20a%20multi-objective%20optimization%20strategy%20for%0Aefficient%20navigation%20in%20static%20and%20highly%20dynamic%20environments%2C%20focusing%20on%0Aoptimizing%20trajectory%20comfort%2C%20safety%2C%20and%20path%20precision.%20The%20algorithm%20is%0Aused%20to%20analyze%20the%20algorithm%20performance%20and%20success%20rate%20in%201750%20virtual%0Acomplex%20urban%20and%20highway%20scenarios.%20Our%20results%20demonstrate%20fast%20calculation%0Atimes%20%288ms%20for%20800%20trajectories%29%2C%20a%20high%20success%20rate%20in%20complex%20scenarios%0A%2888%25%29%2C%20and%20easy%20adaptability%20with%20different%20modules%20presented.%20The%20most%0Anoticeable%20difference%20exhibited%20was%20the%20fast%20trajectory%20sampling%2C%20feasibility%0Acheck%2C%20and%20cost%20evaluation%20step%20across%20various%20trajectory%20counts.%20We%0Ademonstrate%20the%20integration%20and%20execution%20of%20the%20framework%20on%20real%20vehicles%20by%0Aevaluating%20deviations%20from%20the%20controller%20using%20a%20test%20track.%20This%20evaluation%0Ahighlights%20the%20algorithm%27s%20robustness%20and%20reliability%2C%20ensuring%20it%20meets%20the%0Astringent%20requirements%20of%20real-world%20autonomous%20driving%20scenarios.%20The%20code%20and%0Athe%20additional%20modules%20used%20in%20this%20research%20are%20publicly%20available%20as%0Aopen-source%20software%20and%20can%20be%20accessed%20at%20the%20following%20link%3A%0Ahttps%3A//github.com/TUM-AVS/Frenetix-Motion-Planner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01443v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRENETIX%253A%2520A%2520High-Performance%2520and%2520Modular%2520Motion%2520Planning%2520Framework%2520for%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DRainer%2520Trauth%2520and%2520Korbinian%2520Moller%2520and%2520Gerald%2520Wuersching%2520and%2520Johannes%2520Betz%26entry.1292438233%3D%2520%2520Our%2520research%2520introduces%2520a%2520modular%2520motion%2520planning%2520framework%2520for%2520autonomous%250Avehicles%2520using%2520a%2520sampling-based%2520trajectory%2520planning%2520algorithm.%2520This%2520approach%250Aeffectively%2520tackles%2520the%2520challenges%2520of%2520solution%2520space%2520construction%2520and%250Aoptimization%2520in%2520path%2520planning.%2520The%2520algorithm%2520is%2520applicable%2520to%2520both%2520real%250Avehicles%2520and%2520simulations%252C%2520offering%2520a%2520robust%2520solution%2520for%2520complex%2520autonomous%250Anavigation.%2520Our%2520method%2520employs%2520a%2520multi-objective%2520optimization%2520strategy%2520for%250Aefficient%2520navigation%2520in%2520static%2520and%2520highly%2520dynamic%2520environments%252C%2520focusing%2520on%250Aoptimizing%2520trajectory%2520comfort%252C%2520safety%252C%2520and%2520path%2520precision.%2520The%2520algorithm%2520is%250Aused%2520to%2520analyze%2520the%2520algorithm%2520performance%2520and%2520success%2520rate%2520in%25201750%2520virtual%250Acomplex%2520urban%2520and%2520highway%2520scenarios.%2520Our%2520results%2520demonstrate%2520fast%2520calculation%250Atimes%2520%25288ms%2520for%2520800%2520trajectories%2529%252C%2520a%2520high%2520success%2520rate%2520in%2520complex%2520scenarios%250A%252888%2525%2529%252C%2520and%2520easy%2520adaptability%2520with%2520different%2520modules%2520presented.%2520The%2520most%250Anoticeable%2520difference%2520exhibited%2520was%2520the%2520fast%2520trajectory%2520sampling%252C%2520feasibility%250Acheck%252C%2520and%2520cost%2520evaluation%2520step%2520across%2520various%2520trajectory%2520counts.%2520We%250Ademonstrate%2520the%2520integration%2520and%2520execution%2520of%2520the%2520framework%2520on%2520real%2520vehicles%2520by%250Aevaluating%2520deviations%2520from%2520the%2520controller%2520using%2520a%2520test%2520track.%2520This%2520evaluation%250Ahighlights%2520the%2520algorithm%2527s%2520robustness%2520and%2520reliability%252C%2520ensuring%2520it%2520meets%2520the%250Astringent%2520requirements%2520of%2520real-world%2520autonomous%2520driving%2520scenarios.%2520The%2520code%2520and%250Athe%2520additional%2520modules%2520used%2520in%2520this%2520research%2520are%2520publicly%2520available%2520as%250Aopen-source%2520software%2520and%2520can%2520be%2520accessed%2520at%2520the%2520following%2520link%253A%250Ahttps%253A//github.com/TUM-AVS/Frenetix-Motion-Planner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01443v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRENETIX%3A%20A%20High-Performance%20and%20Modular%20Motion%20Planning%20Framework%20for%0A%20%20Autonomous%20Driving&entry.906535625=Rainer%20Trauth%20and%20Korbinian%20Moller%20and%20Gerald%20Wuersching%20and%20Johannes%20Betz&entry.1292438233=%20%20Our%20research%20introduces%20a%20modular%20motion%20planning%20framework%20for%20autonomous%0Avehicles%20using%20a%20sampling-based%20trajectory%20planning%20algorithm.%20This%20approach%0Aeffectively%20tackles%20the%20challenges%20of%20solution%20space%20construction%20and%0Aoptimization%20in%20path%20planning.%20The%20algorithm%20is%20applicable%20to%20both%20real%0Avehicles%20and%20simulations%2C%20offering%20a%20robust%20solution%20for%20complex%20autonomous%0Anavigation.%20Our%20method%20employs%20a%20multi-objective%20optimization%20strategy%20for%0Aefficient%20navigation%20in%20static%20and%20highly%20dynamic%20environments%2C%20focusing%20on%0Aoptimizing%20trajectory%20comfort%2C%20safety%2C%20and%20path%20precision.%20The%20algorithm%20is%0Aused%20to%20analyze%20the%20algorithm%20performance%20and%20success%20rate%20in%201750%20virtual%0Acomplex%20urban%20and%20highway%20scenarios.%20Our%20results%20demonstrate%20fast%20calculation%0Atimes%20%288ms%20for%20800%20trajectories%29%2C%20a%20high%20success%20rate%20in%20complex%20scenarios%0A%2888%25%29%2C%20and%20easy%20adaptability%20with%20different%20modules%20presented.%20The%20most%0Anoticeable%20difference%20exhibited%20was%20the%20fast%20trajectory%20sampling%2C%20feasibility%0Acheck%2C%20and%20cost%20evaluation%20step%20across%20various%20trajectory%20counts.%20We%0Ademonstrate%20the%20integration%20and%20execution%20of%20the%20framework%20on%20real%20vehicles%20by%0Aevaluating%20deviations%20from%20the%20controller%20using%20a%20test%20track.%20This%20evaluation%0Ahighlights%20the%20algorithm%27s%20robustness%20and%20reliability%2C%20ensuring%20it%20meets%20the%0Astringent%20requirements%20of%20real-world%20autonomous%20driving%20scenarios.%20The%20code%20and%0Athe%20additional%20modules%20used%20in%20this%20research%20are%20publicly%20available%20as%0Aopen-source%20software%20and%20can%20be%20accessed%20at%20the%20following%20link%3A%0Ahttps%3A//github.com/TUM-AVS/Frenetix-Motion-Planner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01443v2&entry.124074799=Read"},
{"title": "VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language\n  Large Models", "author": "Chenyu Zhou and Mengdan Zhang and Peixian Chen and Chaoyou Fu and Yunhang Shen and Xiawu Zheng and Xing Sun and Rongrong Ji", "abstract": "  The swift progress of Multi-modal Large Models (MLLMs) has showcased their\nimpressive ability to tackle tasks blending vision and language. Yet, most\ncurrent models and benchmarks cater to scenarios with a narrow scope of visual\nand textual contexts. These models often fall short when faced with complex\ncomprehension tasks, which involve navigating through a plethora of irrelevant\nand potentially misleading information in both text and image forms. To bridge\nthis gap, we introduce a new, more demanding task known as Interleaved\nImage-Text Comprehension (IITC). This task challenges models to discern and\ndisregard superfluous elements in both images and text to accurately answer\nquestions and to follow intricate instructions to pinpoint the relevant image.\nIn support of this task, we further craft a new VEGA dataset, tailored for the\nIITC task on scientific content, and devised a subtask, Image-Text Association\n(ITA), to refine image-text correlation skills. Our evaluation of four leading\nclosed-source models, as well as various open-source models using VEGA,\nunderscores the rigorous nature of IITC. Even the most advanced models, such as\nGemini-1.5-pro and GPT4V, only achieved modest success. By employing a\nmulti-task, multi-scale post-training strategy, we have set a robust baseline\nfor MLLMs on the IITC task, attaining an $85.8\\%$ accuracy rate in image\nassociation and a $0.508$ Rouge score. These results validate the effectiveness\nof our dataset in improving MLLMs capabilities for nuanced image-text\ncomprehension.\n", "link": "http://arxiv.org/abs/2406.10228v1", "date": "2024-06-14", "relevancy": 2.1237, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5579}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5121}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VEGA%3A%20Learning%20Interleaved%20Image-Text%20Comprehension%20in%20Vision-Language%0A%20%20Large%20Models&body=Title%3A%20VEGA%3A%20Learning%20Interleaved%20Image-Text%20Comprehension%20in%20Vision-Language%0A%20%20Large%20Models%0AAuthor%3A%20Chenyu%20Zhou%20and%20Mengdan%20Zhang%20and%20Peixian%20Chen%20and%20Chaoyou%20Fu%20and%20Yunhang%20Shen%20and%20Xiawu%20Zheng%20and%20Xing%20Sun%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20The%20swift%20progress%20of%20Multi-modal%20Large%20Models%20%28MLLMs%29%20has%20showcased%20their%0Aimpressive%20ability%20to%20tackle%20tasks%20blending%20vision%20and%20language.%20Yet%2C%20most%0Acurrent%20models%20and%20benchmarks%20cater%20to%20scenarios%20with%20a%20narrow%20scope%20of%20visual%0Aand%20textual%20contexts.%20These%20models%20often%20fall%20short%20when%20faced%20with%20complex%0Acomprehension%20tasks%2C%20which%20involve%20navigating%20through%20a%20plethora%20of%20irrelevant%0Aand%20potentially%20misleading%20information%20in%20both%20text%20and%20image%20forms.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20a%20new%2C%20more%20demanding%20task%20known%20as%20Interleaved%0AImage-Text%20Comprehension%20%28IITC%29.%20This%20task%20challenges%20models%20to%20discern%20and%0Adisregard%20superfluous%20elements%20in%20both%20images%20and%20text%20to%20accurately%20answer%0Aquestions%20and%20to%20follow%20intricate%20instructions%20to%20pinpoint%20the%20relevant%20image.%0AIn%20support%20of%20this%20task%2C%20we%20further%20craft%20a%20new%20VEGA%20dataset%2C%20tailored%20for%20the%0AIITC%20task%20on%20scientific%20content%2C%20and%20devised%20a%20subtask%2C%20Image-Text%20Association%0A%28ITA%29%2C%20to%20refine%20image-text%20correlation%20skills.%20Our%20evaluation%20of%20four%20leading%0Aclosed-source%20models%2C%20as%20well%20as%20various%20open-source%20models%20using%20VEGA%2C%0Aunderscores%20the%20rigorous%20nature%20of%20IITC.%20Even%20the%20most%20advanced%20models%2C%20such%20as%0AGemini-1.5-pro%20and%20GPT4V%2C%20only%20achieved%20modest%20success.%20By%20employing%20a%0Amulti-task%2C%20multi-scale%20post-training%20strategy%2C%20we%20have%20set%20a%20robust%20baseline%0Afor%20MLLMs%20on%20the%20IITC%20task%2C%20attaining%20an%20%2485.8%5C%25%24%20accuracy%20rate%20in%20image%0Aassociation%20and%20a%20%240.508%24%20Rouge%20score.%20These%20results%20validate%20the%20effectiveness%0Aof%20our%20dataset%20in%20improving%20MLLMs%20capabilities%20for%20nuanced%20image-text%0Acomprehension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVEGA%253A%2520Learning%2520Interleaved%2520Image-Text%2520Comprehension%2520in%2520Vision-Language%250A%2520%2520Large%2520Models%26entry.906535625%3DChenyu%2520Zhou%2520and%2520Mengdan%2520Zhang%2520and%2520Peixian%2520Chen%2520and%2520Chaoyou%2520Fu%2520and%2520Yunhang%2520Shen%2520and%2520Xiawu%2520Zheng%2520and%2520Xing%2520Sun%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520The%2520swift%2520progress%2520of%2520Multi-modal%2520Large%2520Models%2520%2528MLLMs%2529%2520has%2520showcased%2520their%250Aimpressive%2520ability%2520to%2520tackle%2520tasks%2520blending%2520vision%2520and%2520language.%2520Yet%252C%2520most%250Acurrent%2520models%2520and%2520benchmarks%2520cater%2520to%2520scenarios%2520with%2520a%2520narrow%2520scope%2520of%2520visual%250Aand%2520textual%2520contexts.%2520These%2520models%2520often%2520fall%2520short%2520when%2520faced%2520with%2520complex%250Acomprehension%2520tasks%252C%2520which%2520involve%2520navigating%2520through%2520a%2520plethora%2520of%2520irrelevant%250Aand%2520potentially%2520misleading%2520information%2520in%2520both%2520text%2520and%2520image%2520forms.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520introduce%2520a%2520new%252C%2520more%2520demanding%2520task%2520known%2520as%2520Interleaved%250AImage-Text%2520Comprehension%2520%2528IITC%2529.%2520This%2520task%2520challenges%2520models%2520to%2520discern%2520and%250Adisregard%2520superfluous%2520elements%2520in%2520both%2520images%2520and%2520text%2520to%2520accurately%2520answer%250Aquestions%2520and%2520to%2520follow%2520intricate%2520instructions%2520to%2520pinpoint%2520the%2520relevant%2520image.%250AIn%2520support%2520of%2520this%2520task%252C%2520we%2520further%2520craft%2520a%2520new%2520VEGA%2520dataset%252C%2520tailored%2520for%2520the%250AIITC%2520task%2520on%2520scientific%2520content%252C%2520and%2520devised%2520a%2520subtask%252C%2520Image-Text%2520Association%250A%2528ITA%2529%252C%2520to%2520refine%2520image-text%2520correlation%2520skills.%2520Our%2520evaluation%2520of%2520four%2520leading%250Aclosed-source%2520models%252C%2520as%2520well%2520as%2520various%2520open-source%2520models%2520using%2520VEGA%252C%250Aunderscores%2520the%2520rigorous%2520nature%2520of%2520IITC.%2520Even%2520the%2520most%2520advanced%2520models%252C%2520such%2520as%250AGemini-1.5-pro%2520and%2520GPT4V%252C%2520only%2520achieved%2520modest%2520success.%2520By%2520employing%2520a%250Amulti-task%252C%2520multi-scale%2520post-training%2520strategy%252C%2520we%2520have%2520set%2520a%2520robust%2520baseline%250Afor%2520MLLMs%2520on%2520the%2520IITC%2520task%252C%2520attaining%2520an%2520%252485.8%255C%2525%2524%2520accuracy%2520rate%2520in%2520image%250Aassociation%2520and%2520a%2520%25240.508%2524%2520Rouge%2520score.%2520These%2520results%2520validate%2520the%2520effectiveness%250Aof%2520our%2520dataset%2520in%2520improving%2520MLLMs%2520capabilities%2520for%2520nuanced%2520image-text%250Acomprehension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VEGA%3A%20Learning%20Interleaved%20Image-Text%20Comprehension%20in%20Vision-Language%0A%20%20Large%20Models&entry.906535625=Chenyu%20Zhou%20and%20Mengdan%20Zhang%20and%20Peixian%20Chen%20and%20Chaoyou%20Fu%20and%20Yunhang%20Shen%20and%20Xiawu%20Zheng%20and%20Xing%20Sun%20and%20Rongrong%20Ji&entry.1292438233=%20%20The%20swift%20progress%20of%20Multi-modal%20Large%20Models%20%28MLLMs%29%20has%20showcased%20their%0Aimpressive%20ability%20to%20tackle%20tasks%20blending%20vision%20and%20language.%20Yet%2C%20most%0Acurrent%20models%20and%20benchmarks%20cater%20to%20scenarios%20with%20a%20narrow%20scope%20of%20visual%0Aand%20textual%20contexts.%20These%20models%20often%20fall%20short%20when%20faced%20with%20complex%0Acomprehension%20tasks%2C%20which%20involve%20navigating%20through%20a%20plethora%20of%20irrelevant%0Aand%20potentially%20misleading%20information%20in%20both%20text%20and%20image%20forms.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20a%20new%2C%20more%20demanding%20task%20known%20as%20Interleaved%0AImage-Text%20Comprehension%20%28IITC%29.%20This%20task%20challenges%20models%20to%20discern%20and%0Adisregard%20superfluous%20elements%20in%20both%20images%20and%20text%20to%20accurately%20answer%0Aquestions%20and%20to%20follow%20intricate%20instructions%20to%20pinpoint%20the%20relevant%20image.%0AIn%20support%20of%20this%20task%2C%20we%20further%20craft%20a%20new%20VEGA%20dataset%2C%20tailored%20for%20the%0AIITC%20task%20on%20scientific%20content%2C%20and%20devised%20a%20subtask%2C%20Image-Text%20Association%0A%28ITA%29%2C%20to%20refine%20image-text%20correlation%20skills.%20Our%20evaluation%20of%20four%20leading%0Aclosed-source%20models%2C%20as%20well%20as%20various%20open-source%20models%20using%20VEGA%2C%0Aunderscores%20the%20rigorous%20nature%20of%20IITC.%20Even%20the%20most%20advanced%20models%2C%20such%20as%0AGemini-1.5-pro%20and%20GPT4V%2C%20only%20achieved%20modest%20success.%20By%20employing%20a%0Amulti-task%2C%20multi-scale%20post-training%20strategy%2C%20we%20have%20set%20a%20robust%20baseline%0Afor%20MLLMs%20on%20the%20IITC%20task%2C%20attaining%20an%20%2485.8%5C%25%24%20accuracy%20rate%20in%20image%0Aassociation%20and%20a%20%240.508%24%20Rouge%20score.%20These%20results%20validate%20the%20effectiveness%0Aof%20our%20dataset%20in%20improving%20MLLMs%20capabilities%20for%20nuanced%20image-text%0Acomprehension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10228v1&entry.124074799=Read"},
{"title": "Super-resolution multi-contrast unbiased eye atlases with deep\n  probabilistic refinement", "author": "Ho Hin Lee and Adam M. Saunders and Michael E. Kim and Samuel W. Remedios and Lucas W. Remedios and Yucheng Tang and Qi Yang and Xin Yu and Shunxing Bao and Chloe Cho and Louise A. Mawn and Tonia S. Rex and Kevin L. Schey and Blake E. Dewey and Jeffrey M. Spraggins and Jerry L. Prince and Yuankai Huo and Bennett A. Landman", "abstract": "  Purpose: Eye morphology varies significantly across the population,\nespecially for the orbit and optic nerve. These variations limit the\nfeasibility and robustness of generalizing population-wise features of eye\norgans to an unbiased spatial reference.\n  Approach: To tackle these limitations, we propose a process for creating\nhigh-resolution unbiased eye atlases. First, to restore spatial details from\nscans with a low through-plane resolution compared to a high in-plane\nresolution, we apply a deep learning-based super-resolution algorithm. Then, we\ngenerate an initial unbiased reference with an iterative metric-based\nregistration using a small portion of subject scans. We register the remaining\nscans to this template and refine the template using an unsupervised deep\nprobabilistic approach that generates a more expansive deformation field to\nenhance the organ boundary alignment. We demonstrate this framework using\nmagnetic resonance images across four different tissue contrasts, generating\nfour atlases in separate spatial alignments.\n  Results: For each tissue contrast, we find a significant improvement using\nthe Wilcoxon signed-rank test in the average Dice score across four labeled\nregions compared to a standard registration framework consisting of rigid,\naffine, and deformable transformations. These results highlight the effective\nalignment of eye organs and boundaries using our proposed process.\n  Conclusions: By combining super-resolution preprocessing and deep\nprobabilistic models, we address the challenge of generating an eye atlas to\nserve as a standardized reference across a largely variable population.\n", "link": "http://arxiv.org/abs/2401.03060v2", "date": "2024-06-14", "relevancy": 2.1186, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5466}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5216}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Super-resolution%20multi-contrast%20unbiased%20eye%20atlases%20with%20deep%0A%20%20probabilistic%20refinement&body=Title%3A%20Super-resolution%20multi-contrast%20unbiased%20eye%20atlases%20with%20deep%0A%20%20probabilistic%20refinement%0AAuthor%3A%20Ho%20Hin%20Lee%20and%20Adam%20M.%20Saunders%20and%20Michael%20E.%20Kim%20and%20Samuel%20W.%20Remedios%20and%20Lucas%20W.%20Remedios%20and%20Yucheng%20Tang%20and%20Qi%20Yang%20and%20Xin%20Yu%20and%20Shunxing%20Bao%20and%20Chloe%20Cho%20and%20Louise%20A.%20Mawn%20and%20Tonia%20S.%20Rex%20and%20Kevin%20L.%20Schey%20and%20Blake%20E.%20Dewey%20and%20Jeffrey%20M.%20Spraggins%20and%20Jerry%20L.%20Prince%20and%20Yuankai%20Huo%20and%20Bennett%20A.%20Landman%0AAbstract%3A%20%20%20Purpose%3A%20Eye%20morphology%20varies%20significantly%20across%20the%20population%2C%0Aespecially%20for%20the%20orbit%20and%20optic%20nerve.%20These%20variations%20limit%20the%0Afeasibility%20and%20robustness%20of%20generalizing%20population-wise%20features%20of%20eye%0Aorgans%20to%20an%20unbiased%20spatial%20reference.%0A%20%20Approach%3A%20To%20tackle%20these%20limitations%2C%20we%20propose%20a%20process%20for%20creating%0Ahigh-resolution%20unbiased%20eye%20atlases.%20First%2C%20to%20restore%20spatial%20details%20from%0Ascans%20with%20a%20low%20through-plane%20resolution%20compared%20to%20a%20high%20in-plane%0Aresolution%2C%20we%20apply%20a%20deep%20learning-based%20super-resolution%20algorithm.%20Then%2C%20we%0Agenerate%20an%20initial%20unbiased%20reference%20with%20an%20iterative%20metric-based%0Aregistration%20using%20a%20small%20portion%20of%20subject%20scans.%20We%20register%20the%20remaining%0Ascans%20to%20this%20template%20and%20refine%20the%20template%20using%20an%20unsupervised%20deep%0Aprobabilistic%20approach%20that%20generates%20a%20more%20expansive%20deformation%20field%20to%0Aenhance%20the%20organ%20boundary%20alignment.%20We%20demonstrate%20this%20framework%20using%0Amagnetic%20resonance%20images%20across%20four%20different%20tissue%20contrasts%2C%20generating%0Afour%20atlases%20in%20separate%20spatial%20alignments.%0A%20%20Results%3A%20For%20each%20tissue%20contrast%2C%20we%20find%20a%20significant%20improvement%20using%0Athe%20Wilcoxon%20signed-rank%20test%20in%20the%20average%20Dice%20score%20across%20four%20labeled%0Aregions%20compared%20to%20a%20standard%20registration%20framework%20consisting%20of%20rigid%2C%0Aaffine%2C%20and%20deformable%20transformations.%20These%20results%20highlight%20the%20effective%0Aalignment%20of%20eye%20organs%20and%20boundaries%20using%20our%20proposed%20process.%0A%20%20Conclusions%3A%20By%20combining%20super-resolution%20preprocessing%20and%20deep%0Aprobabilistic%20models%2C%20we%20address%20the%20challenge%20of%20generating%20an%20eye%20atlas%20to%0Aserve%20as%20a%20standardized%20reference%20across%20a%20largely%20variable%20population.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuper-resolution%2520multi-contrast%2520unbiased%2520eye%2520atlases%2520with%2520deep%250A%2520%2520probabilistic%2520refinement%26entry.906535625%3DHo%2520Hin%2520Lee%2520and%2520Adam%2520M.%2520Saunders%2520and%2520Michael%2520E.%2520Kim%2520and%2520Samuel%2520W.%2520Remedios%2520and%2520Lucas%2520W.%2520Remedios%2520and%2520Yucheng%2520Tang%2520and%2520Qi%2520Yang%2520and%2520Xin%2520Yu%2520and%2520Shunxing%2520Bao%2520and%2520Chloe%2520Cho%2520and%2520Louise%2520A.%2520Mawn%2520and%2520Tonia%2520S.%2520Rex%2520and%2520Kevin%2520L.%2520Schey%2520and%2520Blake%2520E.%2520Dewey%2520and%2520Jeffrey%2520M.%2520Spraggins%2520and%2520Jerry%2520L.%2520Prince%2520and%2520Yuankai%2520Huo%2520and%2520Bennett%2520A.%2520Landman%26entry.1292438233%3D%2520%2520Purpose%253A%2520Eye%2520morphology%2520varies%2520significantly%2520across%2520the%2520population%252C%250Aespecially%2520for%2520the%2520orbit%2520and%2520optic%2520nerve.%2520These%2520variations%2520limit%2520the%250Afeasibility%2520and%2520robustness%2520of%2520generalizing%2520population-wise%2520features%2520of%2520eye%250Aorgans%2520to%2520an%2520unbiased%2520spatial%2520reference.%250A%2520%2520Approach%253A%2520To%2520tackle%2520these%2520limitations%252C%2520we%2520propose%2520a%2520process%2520for%2520creating%250Ahigh-resolution%2520unbiased%2520eye%2520atlases.%2520First%252C%2520to%2520restore%2520spatial%2520details%2520from%250Ascans%2520with%2520a%2520low%2520through-plane%2520resolution%2520compared%2520to%2520a%2520high%2520in-plane%250Aresolution%252C%2520we%2520apply%2520a%2520deep%2520learning-based%2520super-resolution%2520algorithm.%2520Then%252C%2520we%250Agenerate%2520an%2520initial%2520unbiased%2520reference%2520with%2520an%2520iterative%2520metric-based%250Aregistration%2520using%2520a%2520small%2520portion%2520of%2520subject%2520scans.%2520We%2520register%2520the%2520remaining%250Ascans%2520to%2520this%2520template%2520and%2520refine%2520the%2520template%2520using%2520an%2520unsupervised%2520deep%250Aprobabilistic%2520approach%2520that%2520generates%2520a%2520more%2520expansive%2520deformation%2520field%2520to%250Aenhance%2520the%2520organ%2520boundary%2520alignment.%2520We%2520demonstrate%2520this%2520framework%2520using%250Amagnetic%2520resonance%2520images%2520across%2520four%2520different%2520tissue%2520contrasts%252C%2520generating%250Afour%2520atlases%2520in%2520separate%2520spatial%2520alignments.%250A%2520%2520Results%253A%2520For%2520each%2520tissue%2520contrast%252C%2520we%2520find%2520a%2520significant%2520improvement%2520using%250Athe%2520Wilcoxon%2520signed-rank%2520test%2520in%2520the%2520average%2520Dice%2520score%2520across%2520four%2520labeled%250Aregions%2520compared%2520to%2520a%2520standard%2520registration%2520framework%2520consisting%2520of%2520rigid%252C%250Aaffine%252C%2520and%2520deformable%2520transformations.%2520These%2520results%2520highlight%2520the%2520effective%250Aalignment%2520of%2520eye%2520organs%2520and%2520boundaries%2520using%2520our%2520proposed%2520process.%250A%2520%2520Conclusions%253A%2520By%2520combining%2520super-resolution%2520preprocessing%2520and%2520deep%250Aprobabilistic%2520models%252C%2520we%2520address%2520the%2520challenge%2520of%2520generating%2520an%2520eye%2520atlas%2520to%250Aserve%2520as%2520a%2520standardized%2520reference%2520across%2520a%2520largely%2520variable%2520population.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Super-resolution%20multi-contrast%20unbiased%20eye%20atlases%20with%20deep%0A%20%20probabilistic%20refinement&entry.906535625=Ho%20Hin%20Lee%20and%20Adam%20M.%20Saunders%20and%20Michael%20E.%20Kim%20and%20Samuel%20W.%20Remedios%20and%20Lucas%20W.%20Remedios%20and%20Yucheng%20Tang%20and%20Qi%20Yang%20and%20Xin%20Yu%20and%20Shunxing%20Bao%20and%20Chloe%20Cho%20and%20Louise%20A.%20Mawn%20and%20Tonia%20S.%20Rex%20and%20Kevin%20L.%20Schey%20and%20Blake%20E.%20Dewey%20and%20Jeffrey%20M.%20Spraggins%20and%20Jerry%20L.%20Prince%20and%20Yuankai%20Huo%20and%20Bennett%20A.%20Landman&entry.1292438233=%20%20Purpose%3A%20Eye%20morphology%20varies%20significantly%20across%20the%20population%2C%0Aespecially%20for%20the%20orbit%20and%20optic%20nerve.%20These%20variations%20limit%20the%0Afeasibility%20and%20robustness%20of%20generalizing%20population-wise%20features%20of%20eye%0Aorgans%20to%20an%20unbiased%20spatial%20reference.%0A%20%20Approach%3A%20To%20tackle%20these%20limitations%2C%20we%20propose%20a%20process%20for%20creating%0Ahigh-resolution%20unbiased%20eye%20atlases.%20First%2C%20to%20restore%20spatial%20details%20from%0Ascans%20with%20a%20low%20through-plane%20resolution%20compared%20to%20a%20high%20in-plane%0Aresolution%2C%20we%20apply%20a%20deep%20learning-based%20super-resolution%20algorithm.%20Then%2C%20we%0Agenerate%20an%20initial%20unbiased%20reference%20with%20an%20iterative%20metric-based%0Aregistration%20using%20a%20small%20portion%20of%20subject%20scans.%20We%20register%20the%20remaining%0Ascans%20to%20this%20template%20and%20refine%20the%20template%20using%20an%20unsupervised%20deep%0Aprobabilistic%20approach%20that%20generates%20a%20more%20expansive%20deformation%20field%20to%0Aenhance%20the%20organ%20boundary%20alignment.%20We%20demonstrate%20this%20framework%20using%0Amagnetic%20resonance%20images%20across%20four%20different%20tissue%20contrasts%2C%20generating%0Afour%20atlases%20in%20separate%20spatial%20alignments.%0A%20%20Results%3A%20For%20each%20tissue%20contrast%2C%20we%20find%20a%20significant%20improvement%20using%0Athe%20Wilcoxon%20signed-rank%20test%20in%20the%20average%20Dice%20score%20across%20four%20labeled%0Aregions%20compared%20to%20a%20standard%20registration%20framework%20consisting%20of%20rigid%2C%0Aaffine%2C%20and%20deformable%20transformations.%20These%20results%20highlight%20the%20effective%0Aalignment%20of%20eye%20organs%20and%20boundaries%20using%20our%20proposed%20process.%0A%20%20Conclusions%3A%20By%20combining%20super-resolution%20preprocessing%20and%20deep%0Aprobabilistic%20models%2C%20we%20address%20the%20challenge%20of%20generating%20an%20eye%20atlas%20to%0Aserve%20as%20a%20standardized%20reference%20across%20a%20largely%20variable%20population.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03060v2&entry.124074799=Read"},
{"title": "IBoxCLA: Towards Robust Box-supervised Segmentation of Polyp via\n  Improved Box-dice and Contrastive Latent-anchors", "author": "Zhiwei Wang and Qiang Hu and Hongkuan Shi and Li He and Man He and Wenxuan Dai and Ting Li and Yitong Zhang and Dun Li and Mei Liu and Qiang Li", "abstract": "  Box-supervised polyp segmentation attracts increasing attention for its\ncost-effective potential. Existing solutions often rely on learning-free\nmethods or pretrained models to laboriously generate pseudo masks, triggering\nDice constraint subsequently. In this paper, we found that a model guided by\nthe simplest box-filled masks can accurately predict polyp locations/sizes, but\nsuffers from shape collapsing. In response, we propose two innovative learning\nfashions, Improved Box-dice (IBox) and Contrastive Latent-Anchors (CLA), and\ncombine them to train a robust box-supervised model IBoxCLA. The core idea\nbehind IBoxCLA is to decouple the learning of location/size and shape, allowing\nfor focused constraints on each of them. Specifically, IBox transforms the\nsegmentation map into a proxy map using shape decoupling and confusion-region\nswapping sequentially. Within the proxy map, shapes are disentangled, while\nlocations/sizes are encoded as box-like responses. By constraining the proxy\nmap instead of the raw prediction, the box-filled mask can well supervise\nIBoxCLA without misleading its shape learning. Furthermore, CLA contributes to\nshape learning by generating two types of latent anchors, which are learned and\nupdated using momentum and segmented polyps to steadily represent polyp and\nbackground features. The latent anchors facilitate IBoxCLA to capture\ndiscriminative features within and outside boxes in a contrastive manner,\nyielding clearer boundaries. We benchmark IBoxCLA on five public polyp\ndatasets. The experimental results demonstrate the competitive performance of\nIBoxCLA compared to recent fully-supervised polyp segmentation methods, and its\nsuperiority over other box-supervised state-of-the-arts with a relative\nincrease of overall mDice and mIoU by at least 6.5% and 7.5%, respectively.\n", "link": "http://arxiv.org/abs/2310.07248v3", "date": "2024-06-14", "relevancy": 2.1139, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5546}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5207}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IBoxCLA%3A%20Towards%20Robust%20Box-supervised%20Segmentation%20of%20Polyp%20via%0A%20%20Improved%20Box-dice%20and%20Contrastive%20Latent-anchors&body=Title%3A%20IBoxCLA%3A%20Towards%20Robust%20Box-supervised%20Segmentation%20of%20Polyp%20via%0A%20%20Improved%20Box-dice%20and%20Contrastive%20Latent-anchors%0AAuthor%3A%20Zhiwei%20Wang%20and%20Qiang%20Hu%20and%20Hongkuan%20Shi%20and%20Li%20He%20and%20Man%20He%20and%20Wenxuan%20Dai%20and%20Ting%20Li%20and%20Yitong%20Zhang%20and%20Dun%20Li%20and%20Mei%20Liu%20and%20Qiang%20Li%0AAbstract%3A%20%20%20Box-supervised%20polyp%20segmentation%20attracts%20increasing%20attention%20for%20its%0Acost-effective%20potential.%20Existing%20solutions%20often%20rely%20on%20learning-free%0Amethods%20or%20pretrained%20models%20to%20laboriously%20generate%20pseudo%20masks%2C%20triggering%0ADice%20constraint%20subsequently.%20In%20this%20paper%2C%20we%20found%20that%20a%20model%20guided%20by%0Athe%20simplest%20box-filled%20masks%20can%20accurately%20predict%20polyp%20locations/sizes%2C%20but%0Asuffers%20from%20shape%20collapsing.%20In%20response%2C%20we%20propose%20two%20innovative%20learning%0Afashions%2C%20Improved%20Box-dice%20%28IBox%29%20and%20Contrastive%20Latent-Anchors%20%28CLA%29%2C%20and%0Acombine%20them%20to%20train%20a%20robust%20box-supervised%20model%20IBoxCLA.%20The%20core%20idea%0Abehind%20IBoxCLA%20is%20to%20decouple%20the%20learning%20of%20location/size%20and%20shape%2C%20allowing%0Afor%20focused%20constraints%20on%20each%20of%20them.%20Specifically%2C%20IBox%20transforms%20the%0Asegmentation%20map%20into%20a%20proxy%20map%20using%20shape%20decoupling%20and%20confusion-region%0Aswapping%20sequentially.%20Within%20the%20proxy%20map%2C%20shapes%20are%20disentangled%2C%20while%0Alocations/sizes%20are%20encoded%20as%20box-like%20responses.%20By%20constraining%20the%20proxy%0Amap%20instead%20of%20the%20raw%20prediction%2C%20the%20box-filled%20mask%20can%20well%20supervise%0AIBoxCLA%20without%20misleading%20its%20shape%20learning.%20Furthermore%2C%20CLA%20contributes%20to%0Ashape%20learning%20by%20generating%20two%20types%20of%20latent%20anchors%2C%20which%20are%20learned%20and%0Aupdated%20using%20momentum%20and%20segmented%20polyps%20to%20steadily%20represent%20polyp%20and%0Abackground%20features.%20The%20latent%20anchors%20facilitate%20IBoxCLA%20to%20capture%0Adiscriminative%20features%20within%20and%20outside%20boxes%20in%20a%20contrastive%20manner%2C%0Ayielding%20clearer%20boundaries.%20We%20benchmark%20IBoxCLA%20on%20five%20public%20polyp%0Adatasets.%20The%20experimental%20results%20demonstrate%20the%20competitive%20performance%20of%0AIBoxCLA%20compared%20to%20recent%20fully-supervised%20polyp%20segmentation%20methods%2C%20and%20its%0Asuperiority%20over%20other%20box-supervised%20state-of-the-arts%20with%20a%20relative%0Aincrease%20of%20overall%20mDice%20and%20mIoU%20by%20at%20least%206.5%25%20and%207.5%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07248v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIBoxCLA%253A%2520Towards%2520Robust%2520Box-supervised%2520Segmentation%2520of%2520Polyp%2520via%250A%2520%2520Improved%2520Box-dice%2520and%2520Contrastive%2520Latent-anchors%26entry.906535625%3DZhiwei%2520Wang%2520and%2520Qiang%2520Hu%2520and%2520Hongkuan%2520Shi%2520and%2520Li%2520He%2520and%2520Man%2520He%2520and%2520Wenxuan%2520Dai%2520and%2520Ting%2520Li%2520and%2520Yitong%2520Zhang%2520and%2520Dun%2520Li%2520and%2520Mei%2520Liu%2520and%2520Qiang%2520Li%26entry.1292438233%3D%2520%2520Box-supervised%2520polyp%2520segmentation%2520attracts%2520increasing%2520attention%2520for%2520its%250Acost-effective%2520potential.%2520Existing%2520solutions%2520often%2520rely%2520on%2520learning-free%250Amethods%2520or%2520pretrained%2520models%2520to%2520laboriously%2520generate%2520pseudo%2520masks%252C%2520triggering%250ADice%2520constraint%2520subsequently.%2520In%2520this%2520paper%252C%2520we%2520found%2520that%2520a%2520model%2520guided%2520by%250Athe%2520simplest%2520box-filled%2520masks%2520can%2520accurately%2520predict%2520polyp%2520locations/sizes%252C%2520but%250Asuffers%2520from%2520shape%2520collapsing.%2520In%2520response%252C%2520we%2520propose%2520two%2520innovative%2520learning%250Afashions%252C%2520Improved%2520Box-dice%2520%2528IBox%2529%2520and%2520Contrastive%2520Latent-Anchors%2520%2528CLA%2529%252C%2520and%250Acombine%2520them%2520to%2520train%2520a%2520robust%2520box-supervised%2520model%2520IBoxCLA.%2520The%2520core%2520idea%250Abehind%2520IBoxCLA%2520is%2520to%2520decouple%2520the%2520learning%2520of%2520location/size%2520and%2520shape%252C%2520allowing%250Afor%2520focused%2520constraints%2520on%2520each%2520of%2520them.%2520Specifically%252C%2520IBox%2520transforms%2520the%250Asegmentation%2520map%2520into%2520a%2520proxy%2520map%2520using%2520shape%2520decoupling%2520and%2520confusion-region%250Aswapping%2520sequentially.%2520Within%2520the%2520proxy%2520map%252C%2520shapes%2520are%2520disentangled%252C%2520while%250Alocations/sizes%2520are%2520encoded%2520as%2520box-like%2520responses.%2520By%2520constraining%2520the%2520proxy%250Amap%2520instead%2520of%2520the%2520raw%2520prediction%252C%2520the%2520box-filled%2520mask%2520can%2520well%2520supervise%250AIBoxCLA%2520without%2520misleading%2520its%2520shape%2520learning.%2520Furthermore%252C%2520CLA%2520contributes%2520to%250Ashape%2520learning%2520by%2520generating%2520two%2520types%2520of%2520latent%2520anchors%252C%2520which%2520are%2520learned%2520and%250Aupdated%2520using%2520momentum%2520and%2520segmented%2520polyps%2520to%2520steadily%2520represent%2520polyp%2520and%250Abackground%2520features.%2520The%2520latent%2520anchors%2520facilitate%2520IBoxCLA%2520to%2520capture%250Adiscriminative%2520features%2520within%2520and%2520outside%2520boxes%2520in%2520a%2520contrastive%2520manner%252C%250Ayielding%2520clearer%2520boundaries.%2520We%2520benchmark%2520IBoxCLA%2520on%2520five%2520public%2520polyp%250Adatasets.%2520The%2520experimental%2520results%2520demonstrate%2520the%2520competitive%2520performance%2520of%250AIBoxCLA%2520compared%2520to%2520recent%2520fully-supervised%2520polyp%2520segmentation%2520methods%252C%2520and%2520its%250Asuperiority%2520over%2520other%2520box-supervised%2520state-of-the-arts%2520with%2520a%2520relative%250Aincrease%2520of%2520overall%2520mDice%2520and%2520mIoU%2520by%2520at%2520least%25206.5%2525%2520and%25207.5%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.07248v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IBoxCLA%3A%20Towards%20Robust%20Box-supervised%20Segmentation%20of%20Polyp%20via%0A%20%20Improved%20Box-dice%20and%20Contrastive%20Latent-anchors&entry.906535625=Zhiwei%20Wang%20and%20Qiang%20Hu%20and%20Hongkuan%20Shi%20and%20Li%20He%20and%20Man%20He%20and%20Wenxuan%20Dai%20and%20Ting%20Li%20and%20Yitong%20Zhang%20and%20Dun%20Li%20and%20Mei%20Liu%20and%20Qiang%20Li&entry.1292438233=%20%20Box-supervised%20polyp%20segmentation%20attracts%20increasing%20attention%20for%20its%0Acost-effective%20potential.%20Existing%20solutions%20often%20rely%20on%20learning-free%0Amethods%20or%20pretrained%20models%20to%20laboriously%20generate%20pseudo%20masks%2C%20triggering%0ADice%20constraint%20subsequently.%20In%20this%20paper%2C%20we%20found%20that%20a%20model%20guided%20by%0Athe%20simplest%20box-filled%20masks%20can%20accurately%20predict%20polyp%20locations/sizes%2C%20but%0Asuffers%20from%20shape%20collapsing.%20In%20response%2C%20we%20propose%20two%20innovative%20learning%0Afashions%2C%20Improved%20Box-dice%20%28IBox%29%20and%20Contrastive%20Latent-Anchors%20%28CLA%29%2C%20and%0Acombine%20them%20to%20train%20a%20robust%20box-supervised%20model%20IBoxCLA.%20The%20core%20idea%0Abehind%20IBoxCLA%20is%20to%20decouple%20the%20learning%20of%20location/size%20and%20shape%2C%20allowing%0Afor%20focused%20constraints%20on%20each%20of%20them.%20Specifically%2C%20IBox%20transforms%20the%0Asegmentation%20map%20into%20a%20proxy%20map%20using%20shape%20decoupling%20and%20confusion-region%0Aswapping%20sequentially.%20Within%20the%20proxy%20map%2C%20shapes%20are%20disentangled%2C%20while%0Alocations/sizes%20are%20encoded%20as%20box-like%20responses.%20By%20constraining%20the%20proxy%0Amap%20instead%20of%20the%20raw%20prediction%2C%20the%20box-filled%20mask%20can%20well%20supervise%0AIBoxCLA%20without%20misleading%20its%20shape%20learning.%20Furthermore%2C%20CLA%20contributes%20to%0Ashape%20learning%20by%20generating%20two%20types%20of%20latent%20anchors%2C%20which%20are%20learned%20and%0Aupdated%20using%20momentum%20and%20segmented%20polyps%20to%20steadily%20represent%20polyp%20and%0Abackground%20features.%20The%20latent%20anchors%20facilitate%20IBoxCLA%20to%20capture%0Adiscriminative%20features%20within%20and%20outside%20boxes%20in%20a%20contrastive%20manner%2C%0Ayielding%20clearer%20boundaries.%20We%20benchmark%20IBoxCLA%20on%20five%20public%20polyp%0Adatasets.%20The%20experimental%20results%20demonstrate%20the%20competitive%20performance%20of%0AIBoxCLA%20compared%20to%20recent%20fully-supervised%20polyp%20segmentation%20methods%2C%20and%20its%0Asuperiority%20over%20other%20box-supervised%20state-of-the-arts%20with%20a%20relative%0Aincrease%20of%20overall%20mDice%20and%20mIoU%20by%20at%20least%206.5%25%20and%207.5%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07248v3&entry.124074799=Read"},
{"title": "Federated Learning Across Decentralized and Unshared Archives for Remote\n  Sensing Image Classification", "author": "Bar\u0131\u015f B\u00fcy\u00fckta\u015f and Gencer Sumbul and Beg\u00fcm Demir", "abstract": "  Federated learning (FL) enables the collaboration of multiple deep learning\nmodels to learn from decentralized data archives (i.e., clients) without\naccessing data on clients. Although FL offers ample opportunities in knowledge\ndiscovery from distributed image archives, it is seldom considered in remote\nsensing (RS). In this paper, as a first time in RS, we present a comparative\nstudy of state-of-the-art FL algorithms for RS image classification problems.\nTo this end, we initially provide a systematic review of the FL algorithms\npresented in the computer vision and machine learning communities. Then, we\nselect several state-of-the-art FL algorithms based on their effectiveness with\nrespect to training data heterogeneity across clients (known as non-IID data).\nAfter presenting an extensive overview of the selected algorithms, a\ntheoretical comparison of the algorithms is conducted based on their: 1) local\ntraining complexity; 2) aggregation complexity; 3) learning efficiency; 4)\ncommunication cost; and 5) scalability in terms of number of clients. After the\ntheoretical comparison, experimental analyses are presented to compare them\nunder different decentralization scenarios. For the experimental analyses, we\nfocus our attention on multi-label image classification problems in RS. Based\non our comprehensive analyses, we finally derive a guideline for selecting\nsuitable FL algorithms in RS. The code of this work is publicly available at\nhttps://git.tu-berlin.de/rsim/FL-RS.\n", "link": "http://arxiv.org/abs/2311.06141v3", "date": "2024-06-14", "relevancy": 2.097, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5389}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5153}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20Across%20Decentralized%20and%20Unshared%20Archives%20for%20Remote%0A%20%20Sensing%20Image%20Classification&body=Title%3A%20Federated%20Learning%20Across%20Decentralized%20and%20Unshared%20Archives%20for%20Remote%0A%20%20Sensing%20Image%20Classification%0AAuthor%3A%20Bar%C4%B1%C5%9F%20B%C3%BCy%C3%BCkta%C5%9F%20and%20Gencer%20Sumbul%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20enables%20the%20collaboration%20of%20multiple%20deep%20learning%0Amodels%20to%20learn%20from%20decentralized%20data%20archives%20%28i.e.%2C%20clients%29%20without%0Aaccessing%20data%20on%20clients.%20Although%20FL%20offers%20ample%20opportunities%20in%20knowledge%0Adiscovery%20from%20distributed%20image%20archives%2C%20it%20is%20seldom%20considered%20in%20remote%0Asensing%20%28RS%29.%20In%20this%20paper%2C%20as%20a%20first%20time%20in%20RS%2C%20we%20present%20a%20comparative%0Astudy%20of%20state-of-the-art%20FL%20algorithms%20for%20RS%20image%20classification%20problems.%0ATo%20this%20end%2C%20we%20initially%20provide%20a%20systematic%20review%20of%20the%20FL%20algorithms%0Apresented%20in%20the%20computer%20vision%20and%20machine%20learning%20communities.%20Then%2C%20we%0Aselect%20several%20state-of-the-art%20FL%20algorithms%20based%20on%20their%20effectiveness%20with%0Arespect%20to%20training%20data%20heterogeneity%20across%20clients%20%28known%20as%20non-IID%20data%29.%0AAfter%20presenting%20an%20extensive%20overview%20of%20the%20selected%20algorithms%2C%20a%0Atheoretical%20comparison%20of%20the%20algorithms%20is%20conducted%20based%20on%20their%3A%201%29%20local%0Atraining%20complexity%3B%202%29%20aggregation%20complexity%3B%203%29%20learning%20efficiency%3B%204%29%0Acommunication%20cost%3B%20and%205%29%20scalability%20in%20terms%20of%20number%20of%20clients.%20After%20the%0Atheoretical%20comparison%2C%20experimental%20analyses%20are%20presented%20to%20compare%20them%0Aunder%20different%20decentralization%20scenarios.%20For%20the%20experimental%20analyses%2C%20we%0Afocus%20our%20attention%20on%20multi-label%20image%20classification%20problems%20in%20RS.%20Based%0Aon%20our%20comprehensive%20analyses%2C%20we%20finally%20derive%20a%20guideline%20for%20selecting%0Asuitable%20FL%20algorithms%20in%20RS.%20The%20code%20of%20this%20work%20is%20publicly%20available%20at%0Ahttps%3A//git.tu-berlin.de/rsim/FL-RS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.06141v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Learning%2520Across%2520Decentralized%2520and%2520Unshared%2520Archives%2520for%2520Remote%250A%2520%2520Sensing%2520Image%2520Classification%26entry.906535625%3DBar%25C4%25B1%25C5%259F%2520B%25C3%25BCy%25C3%25BCkta%25C5%259F%2520and%2520Gencer%2520Sumbul%2520and%2520Beg%25C3%25BCm%2520Demir%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520enables%2520the%2520collaboration%2520of%2520multiple%2520deep%2520learning%250Amodels%2520to%2520learn%2520from%2520decentralized%2520data%2520archives%2520%2528i.e.%252C%2520clients%2529%2520without%250Aaccessing%2520data%2520on%2520clients.%2520Although%2520FL%2520offers%2520ample%2520opportunities%2520in%2520knowledge%250Adiscovery%2520from%2520distributed%2520image%2520archives%252C%2520it%2520is%2520seldom%2520considered%2520in%2520remote%250Asensing%2520%2528RS%2529.%2520In%2520this%2520paper%252C%2520as%2520a%2520first%2520time%2520in%2520RS%252C%2520we%2520present%2520a%2520comparative%250Astudy%2520of%2520state-of-the-art%2520FL%2520algorithms%2520for%2520RS%2520image%2520classification%2520problems.%250ATo%2520this%2520end%252C%2520we%2520initially%2520provide%2520a%2520systematic%2520review%2520of%2520the%2520FL%2520algorithms%250Apresented%2520in%2520the%2520computer%2520vision%2520and%2520machine%2520learning%2520communities.%2520Then%252C%2520we%250Aselect%2520several%2520state-of-the-art%2520FL%2520algorithms%2520based%2520on%2520their%2520effectiveness%2520with%250Arespect%2520to%2520training%2520data%2520heterogeneity%2520across%2520clients%2520%2528known%2520as%2520non-IID%2520data%2529.%250AAfter%2520presenting%2520an%2520extensive%2520overview%2520of%2520the%2520selected%2520algorithms%252C%2520a%250Atheoretical%2520comparison%2520of%2520the%2520algorithms%2520is%2520conducted%2520based%2520on%2520their%253A%25201%2529%2520local%250Atraining%2520complexity%253B%25202%2529%2520aggregation%2520complexity%253B%25203%2529%2520learning%2520efficiency%253B%25204%2529%250Acommunication%2520cost%253B%2520and%25205%2529%2520scalability%2520in%2520terms%2520of%2520number%2520of%2520clients.%2520After%2520the%250Atheoretical%2520comparison%252C%2520experimental%2520analyses%2520are%2520presented%2520to%2520compare%2520them%250Aunder%2520different%2520decentralization%2520scenarios.%2520For%2520the%2520experimental%2520analyses%252C%2520we%250Afocus%2520our%2520attention%2520on%2520multi-label%2520image%2520classification%2520problems%2520in%2520RS.%2520Based%250Aon%2520our%2520comprehensive%2520analyses%252C%2520we%2520finally%2520derive%2520a%2520guideline%2520for%2520selecting%250Asuitable%2520FL%2520algorithms%2520in%2520RS.%2520The%2520code%2520of%2520this%2520work%2520is%2520publicly%2520available%2520at%250Ahttps%253A//git.tu-berlin.de/rsim/FL-RS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.06141v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20Across%20Decentralized%20and%20Unshared%20Archives%20for%20Remote%0A%20%20Sensing%20Image%20Classification&entry.906535625=Bar%C4%B1%C5%9F%20B%C3%BCy%C3%BCkta%C5%9F%20and%20Gencer%20Sumbul%20and%20Beg%C3%BCm%20Demir&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20enables%20the%20collaboration%20of%20multiple%20deep%20learning%0Amodels%20to%20learn%20from%20decentralized%20data%20archives%20%28i.e.%2C%20clients%29%20without%0Aaccessing%20data%20on%20clients.%20Although%20FL%20offers%20ample%20opportunities%20in%20knowledge%0Adiscovery%20from%20distributed%20image%20archives%2C%20it%20is%20seldom%20considered%20in%20remote%0Asensing%20%28RS%29.%20In%20this%20paper%2C%20as%20a%20first%20time%20in%20RS%2C%20we%20present%20a%20comparative%0Astudy%20of%20state-of-the-art%20FL%20algorithms%20for%20RS%20image%20classification%20problems.%0ATo%20this%20end%2C%20we%20initially%20provide%20a%20systematic%20review%20of%20the%20FL%20algorithms%0Apresented%20in%20the%20computer%20vision%20and%20machine%20learning%20communities.%20Then%2C%20we%0Aselect%20several%20state-of-the-art%20FL%20algorithms%20based%20on%20their%20effectiveness%20with%0Arespect%20to%20training%20data%20heterogeneity%20across%20clients%20%28known%20as%20non-IID%20data%29.%0AAfter%20presenting%20an%20extensive%20overview%20of%20the%20selected%20algorithms%2C%20a%0Atheoretical%20comparison%20of%20the%20algorithms%20is%20conducted%20based%20on%20their%3A%201%29%20local%0Atraining%20complexity%3B%202%29%20aggregation%20complexity%3B%203%29%20learning%20efficiency%3B%204%29%0Acommunication%20cost%3B%20and%205%29%20scalability%20in%20terms%20of%20number%20of%20clients.%20After%20the%0Atheoretical%20comparison%2C%20experimental%20analyses%20are%20presented%20to%20compare%20them%0Aunder%20different%20decentralization%20scenarios.%20For%20the%20experimental%20analyses%2C%20we%0Afocus%20our%20attention%20on%20multi-label%20image%20classification%20problems%20in%20RS.%20Based%0Aon%20our%20comprehensive%20analyses%2C%20we%20finally%20derive%20a%20guideline%20for%20selecting%0Asuitable%20FL%20algorithms%20in%20RS.%20The%20code%20of%20this%20work%20is%20publicly%20available%20at%0Ahttps%3A//git.tu-berlin.de/rsim/FL-RS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.06141v3&entry.124074799=Read"},
{"title": "Maestro: Uncovering Low-Rank Structures via Trainable Decomposition", "author": "Samuel Horvath and Stefanos Laskaridis and Shashank Rajput and Hongyi Wang", "abstract": "  Deep Neural Networks (DNNs) have been a large driver for AI breakthroughs in\nrecent years. However, these models have been getting increasingly large as\nthey become more accurate and safe. This means that their training becomes\nincreasingly costly and time-consuming and typically yields a single model to\nfit all targets. Various techniques have been proposed in the literature to\nmitigate this, including pruning, sparsification, or quantization of model\nweights and updates. While achieving high compression rates, they often incur\nsignificant computational overheads at training or lead to non-negligible\naccuracy penalty. Alternatively, factorization methods have been leveraged for\nlow-rank compression of DNNs. Similarly, such techniques (e.g., SVD) frequently\nrely on heavy iterative decompositions of layers and are potentially\nsub-optimal for non-linear models, such as DNNs. We take a further step in\ndesigning efficient low-rank models and propose Maestro, a framework for\ntrainable low-rank layers. Instead of iteratively applying a priori\ndecompositions, the low-rank structure is baked into the training process\nthrough LoD, a low-rank ordered decomposition. Not only is this the first time\nimportance ordering via sampling is applied on the decomposed DNN structure,\nbut it also allows selecting ranks at a layer granularity. Our theoretical\nanalysis demonstrates that in special cases LoD recovers the SVD decomposition\nand PCA. Applied to DNNs, Maestro enables the extraction of lower footprint\nmodels that preserve performance. Simultaneously, it enables the graceful\ntrade-off between accuracy-latency for deployment to even more constrained\ndevices without retraining.\n", "link": "http://arxiv.org/abs/2308.14929v2", "date": "2024-06-14", "relevancy": 2.092, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5341}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5268}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Maestro%3A%20Uncovering%20Low-Rank%20Structures%20via%20Trainable%20Decomposition&body=Title%3A%20Maestro%3A%20Uncovering%20Low-Rank%20Structures%20via%20Trainable%20Decomposition%0AAuthor%3A%20Samuel%20Horvath%20and%20Stefanos%20Laskaridis%20and%20Shashank%20Rajput%20and%20Hongyi%20Wang%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20been%20a%20large%20driver%20for%20AI%20breakthroughs%20in%0Arecent%20years.%20However%2C%20these%20models%20have%20been%20getting%20increasingly%20large%20as%0Athey%20become%20more%20accurate%20and%20safe.%20This%20means%20that%20their%20training%20becomes%0Aincreasingly%20costly%20and%20time-consuming%20and%20typically%20yields%20a%20single%20model%20to%0Afit%20all%20targets.%20Various%20techniques%20have%20been%20proposed%20in%20the%20literature%20to%0Amitigate%20this%2C%20including%20pruning%2C%20sparsification%2C%20or%20quantization%20of%20model%0Aweights%20and%20updates.%20While%20achieving%20high%20compression%20rates%2C%20they%20often%20incur%0Asignificant%20computational%20overheads%20at%20training%20or%20lead%20to%20non-negligible%0Aaccuracy%20penalty.%20Alternatively%2C%20factorization%20methods%20have%20been%20leveraged%20for%0Alow-rank%20compression%20of%20DNNs.%20Similarly%2C%20such%20techniques%20%28e.g.%2C%20SVD%29%20frequently%0Arely%20on%20heavy%20iterative%20decompositions%20of%20layers%20and%20are%20potentially%0Asub-optimal%20for%20non-linear%20models%2C%20such%20as%20DNNs.%20We%20take%20a%20further%20step%20in%0Adesigning%20efficient%20low-rank%20models%20and%20propose%20Maestro%2C%20a%20framework%20for%0Atrainable%20low-rank%20layers.%20Instead%20of%20iteratively%20applying%20a%20priori%0Adecompositions%2C%20the%20low-rank%20structure%20is%20baked%20into%20the%20training%20process%0Athrough%20LoD%2C%20a%20low-rank%20ordered%20decomposition.%20Not%20only%20is%20this%20the%20first%20time%0Aimportance%20ordering%20via%20sampling%20is%20applied%20on%20the%20decomposed%20DNN%20structure%2C%0Abut%20it%20also%20allows%20selecting%20ranks%20at%20a%20layer%20granularity.%20Our%20theoretical%0Aanalysis%20demonstrates%20that%20in%20special%20cases%20LoD%20recovers%20the%20SVD%20decomposition%0Aand%20PCA.%20Applied%20to%20DNNs%2C%20Maestro%20enables%20the%20extraction%20of%20lower%20footprint%0Amodels%20that%20preserve%20performance.%20Simultaneously%2C%20it%20enables%20the%20graceful%0Atrade-off%20between%20accuracy-latency%20for%20deployment%20to%20even%20more%20constrained%0Adevices%20without%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.14929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaestro%253A%2520Uncovering%2520Low-Rank%2520Structures%2520via%2520Trainable%2520Decomposition%26entry.906535625%3DSamuel%2520Horvath%2520and%2520Stefanos%2520Laskaridis%2520and%2520Shashank%2520Rajput%2520and%2520Hongyi%2520Wang%26entry.1292438233%3D%2520%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520have%2520been%2520a%2520large%2520driver%2520for%2520AI%2520breakthroughs%2520in%250Arecent%2520years.%2520However%252C%2520these%2520models%2520have%2520been%2520getting%2520increasingly%2520large%2520as%250Athey%2520become%2520more%2520accurate%2520and%2520safe.%2520This%2520means%2520that%2520their%2520training%2520becomes%250Aincreasingly%2520costly%2520and%2520time-consuming%2520and%2520typically%2520yields%2520a%2520single%2520model%2520to%250Afit%2520all%2520targets.%2520Various%2520techniques%2520have%2520been%2520proposed%2520in%2520the%2520literature%2520to%250Amitigate%2520this%252C%2520including%2520pruning%252C%2520sparsification%252C%2520or%2520quantization%2520of%2520model%250Aweights%2520and%2520updates.%2520While%2520achieving%2520high%2520compression%2520rates%252C%2520they%2520often%2520incur%250Asignificant%2520computational%2520overheads%2520at%2520training%2520or%2520lead%2520to%2520non-negligible%250Aaccuracy%2520penalty.%2520Alternatively%252C%2520factorization%2520methods%2520have%2520been%2520leveraged%2520for%250Alow-rank%2520compression%2520of%2520DNNs.%2520Similarly%252C%2520such%2520techniques%2520%2528e.g.%252C%2520SVD%2529%2520frequently%250Arely%2520on%2520heavy%2520iterative%2520decompositions%2520of%2520layers%2520and%2520are%2520potentially%250Asub-optimal%2520for%2520non-linear%2520models%252C%2520such%2520as%2520DNNs.%2520We%2520take%2520a%2520further%2520step%2520in%250Adesigning%2520efficient%2520low-rank%2520models%2520and%2520propose%2520Maestro%252C%2520a%2520framework%2520for%250Atrainable%2520low-rank%2520layers.%2520Instead%2520of%2520iteratively%2520applying%2520a%2520priori%250Adecompositions%252C%2520the%2520low-rank%2520structure%2520is%2520baked%2520into%2520the%2520training%2520process%250Athrough%2520LoD%252C%2520a%2520low-rank%2520ordered%2520decomposition.%2520Not%2520only%2520is%2520this%2520the%2520first%2520time%250Aimportance%2520ordering%2520via%2520sampling%2520is%2520applied%2520on%2520the%2520decomposed%2520DNN%2520structure%252C%250Abut%2520it%2520also%2520allows%2520selecting%2520ranks%2520at%2520a%2520layer%2520granularity.%2520Our%2520theoretical%250Aanalysis%2520demonstrates%2520that%2520in%2520special%2520cases%2520LoD%2520recovers%2520the%2520SVD%2520decomposition%250Aand%2520PCA.%2520Applied%2520to%2520DNNs%252C%2520Maestro%2520enables%2520the%2520extraction%2520of%2520lower%2520footprint%250Amodels%2520that%2520preserve%2520performance.%2520Simultaneously%252C%2520it%2520enables%2520the%2520graceful%250Atrade-off%2520between%2520accuracy-latency%2520for%2520deployment%2520to%2520even%2520more%2520constrained%250Adevices%2520without%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.14929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Maestro%3A%20Uncovering%20Low-Rank%20Structures%20via%20Trainable%20Decomposition&entry.906535625=Samuel%20Horvath%20and%20Stefanos%20Laskaridis%20and%20Shashank%20Rajput%20and%20Hongyi%20Wang&entry.1292438233=%20%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20been%20a%20large%20driver%20for%20AI%20breakthroughs%20in%0Arecent%20years.%20However%2C%20these%20models%20have%20been%20getting%20increasingly%20large%20as%0Athey%20become%20more%20accurate%20and%20safe.%20This%20means%20that%20their%20training%20becomes%0Aincreasingly%20costly%20and%20time-consuming%20and%20typically%20yields%20a%20single%20model%20to%0Afit%20all%20targets.%20Various%20techniques%20have%20been%20proposed%20in%20the%20literature%20to%0Amitigate%20this%2C%20including%20pruning%2C%20sparsification%2C%20or%20quantization%20of%20model%0Aweights%20and%20updates.%20While%20achieving%20high%20compression%20rates%2C%20they%20often%20incur%0Asignificant%20computational%20overheads%20at%20training%20or%20lead%20to%20non-negligible%0Aaccuracy%20penalty.%20Alternatively%2C%20factorization%20methods%20have%20been%20leveraged%20for%0Alow-rank%20compression%20of%20DNNs.%20Similarly%2C%20such%20techniques%20%28e.g.%2C%20SVD%29%20frequently%0Arely%20on%20heavy%20iterative%20decompositions%20of%20layers%20and%20are%20potentially%0Asub-optimal%20for%20non-linear%20models%2C%20such%20as%20DNNs.%20We%20take%20a%20further%20step%20in%0Adesigning%20efficient%20low-rank%20models%20and%20propose%20Maestro%2C%20a%20framework%20for%0Atrainable%20low-rank%20layers.%20Instead%20of%20iteratively%20applying%20a%20priori%0Adecompositions%2C%20the%20low-rank%20structure%20is%20baked%20into%20the%20training%20process%0Athrough%20LoD%2C%20a%20low-rank%20ordered%20decomposition.%20Not%20only%20is%20this%20the%20first%20time%0Aimportance%20ordering%20via%20sampling%20is%20applied%20on%20the%20decomposed%20DNN%20structure%2C%0Abut%20it%20also%20allows%20selecting%20ranks%20at%20a%20layer%20granularity.%20Our%20theoretical%0Aanalysis%20demonstrates%20that%20in%20special%20cases%20LoD%20recovers%20the%20SVD%20decomposition%0Aand%20PCA.%20Applied%20to%20DNNs%2C%20Maestro%20enables%20the%20extraction%20of%20lower%20footprint%0Amodels%20that%20preserve%20performance.%20Simultaneously%2C%20it%20enables%20the%20graceful%0Atrade-off%20between%20accuracy-latency%20for%20deployment%20to%20even%20more%20constrained%0Adevices%20without%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.14929v2&entry.124074799=Read"},
{"title": "Rule Based Learning with Dynamic (Graph) Neural Networks", "author": "Florian Seiffarth", "abstract": "  A common problem of classical neural network architectures is that additional\ninformation or expert knowledge cannot be naturally integrated into the\nlearning process. To overcome this limitation, we propose a two-step approach\nconsisting of (1) generating rule functions from knowledge and (2) using these\nrules to define rule based layers -- a new type of dynamic neural network\nlayer. The focus of this work is on the second step, i.e., rule based layers\nthat are designed to dynamically arrange learnable parameters in the weight\nmatrices and bias vectors depending on the input samples. Indeed, we prove that\nour approach generalizes classical feed-forward layers such as fully connected\nand convolutional layers by choosing appropriate rules. As a concrete\napplication we present rule based graph neural networks (RuleGNNs) that\novercome some limitations of ordinary graph neural networks. Our experiments\nshow that the predictive performance of RuleGNNs is comparable to\nstate-of-the-art graph classifiers using simple rules based on Weisfeiler-Leman\nlabeling and pattern counting. Moreover, we introduce new synthetic benchmark\ngraph datasets to show how to integrate expert knowledge into RuleGNNs making\nthem more powerful than ordinary graph neural networks.\n", "link": "http://arxiv.org/abs/2406.09954v1", "date": "2024-06-14", "relevancy": 2.088, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5643}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5018}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rule%20Based%20Learning%20with%20Dynamic%20%28Graph%29%20Neural%20Networks&body=Title%3A%20Rule%20Based%20Learning%20with%20Dynamic%20%28Graph%29%20Neural%20Networks%0AAuthor%3A%20Florian%20Seiffarth%0AAbstract%3A%20%20%20A%20common%20problem%20of%20classical%20neural%20network%20architectures%20is%20that%20additional%0Ainformation%20or%20expert%20knowledge%20cannot%20be%20naturally%20integrated%20into%20the%0Alearning%20process.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20two-step%20approach%0Aconsisting%20of%20%281%29%20generating%20rule%20functions%20from%20knowledge%20and%20%282%29%20using%20these%0Arules%20to%20define%20rule%20based%20layers%20--%20a%20new%20type%20of%20dynamic%20neural%20network%0Alayer.%20The%20focus%20of%20this%20work%20is%20on%20the%20second%20step%2C%20i.e.%2C%20rule%20based%20layers%0Athat%20are%20designed%20to%20dynamically%20arrange%20learnable%20parameters%20in%20the%20weight%0Amatrices%20and%20bias%20vectors%20depending%20on%20the%20input%20samples.%20Indeed%2C%20we%20prove%20that%0Aour%20approach%20generalizes%20classical%20feed-forward%20layers%20such%20as%20fully%20connected%0Aand%20convolutional%20layers%20by%20choosing%20appropriate%20rules.%20As%20a%20concrete%0Aapplication%20we%20present%20rule%20based%20graph%20neural%20networks%20%28RuleGNNs%29%20that%0Aovercome%20some%20limitations%20of%20ordinary%20graph%20neural%20networks.%20Our%20experiments%0Ashow%20that%20the%20predictive%20performance%20of%20RuleGNNs%20is%20comparable%20to%0Astate-of-the-art%20graph%20classifiers%20using%20simple%20rules%20based%20on%20Weisfeiler-Leman%0Alabeling%20and%20pattern%20counting.%20Moreover%2C%20we%20introduce%20new%20synthetic%20benchmark%0Agraph%20datasets%20to%20show%20how%20to%20integrate%20expert%20knowledge%20into%20RuleGNNs%20making%0Athem%20more%20powerful%20than%20ordinary%20graph%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRule%2520Based%2520Learning%2520with%2520Dynamic%2520%2528Graph%2529%2520Neural%2520Networks%26entry.906535625%3DFlorian%2520Seiffarth%26entry.1292438233%3D%2520%2520A%2520common%2520problem%2520of%2520classical%2520neural%2520network%2520architectures%2520is%2520that%2520additional%250Ainformation%2520or%2520expert%2520knowledge%2520cannot%2520be%2520naturally%2520integrated%2520into%2520the%250Alearning%2520process.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520two-step%2520approach%250Aconsisting%2520of%2520%25281%2529%2520generating%2520rule%2520functions%2520from%2520knowledge%2520and%2520%25282%2529%2520using%2520these%250Arules%2520to%2520define%2520rule%2520based%2520layers%2520--%2520a%2520new%2520type%2520of%2520dynamic%2520neural%2520network%250Alayer.%2520The%2520focus%2520of%2520this%2520work%2520is%2520on%2520the%2520second%2520step%252C%2520i.e.%252C%2520rule%2520based%2520layers%250Athat%2520are%2520designed%2520to%2520dynamically%2520arrange%2520learnable%2520parameters%2520in%2520the%2520weight%250Amatrices%2520and%2520bias%2520vectors%2520depending%2520on%2520the%2520input%2520samples.%2520Indeed%252C%2520we%2520prove%2520that%250Aour%2520approach%2520generalizes%2520classical%2520feed-forward%2520layers%2520such%2520as%2520fully%2520connected%250Aand%2520convolutional%2520layers%2520by%2520choosing%2520appropriate%2520rules.%2520As%2520a%2520concrete%250Aapplication%2520we%2520present%2520rule%2520based%2520graph%2520neural%2520networks%2520%2528RuleGNNs%2529%2520that%250Aovercome%2520some%2520limitations%2520of%2520ordinary%2520graph%2520neural%2520networks.%2520Our%2520experiments%250Ashow%2520that%2520the%2520predictive%2520performance%2520of%2520RuleGNNs%2520is%2520comparable%2520to%250Astate-of-the-art%2520graph%2520classifiers%2520using%2520simple%2520rules%2520based%2520on%2520Weisfeiler-Leman%250Alabeling%2520and%2520pattern%2520counting.%2520Moreover%252C%2520we%2520introduce%2520new%2520synthetic%2520benchmark%250Agraph%2520datasets%2520to%2520show%2520how%2520to%2520integrate%2520expert%2520knowledge%2520into%2520RuleGNNs%2520making%250Athem%2520more%2520powerful%2520than%2520ordinary%2520graph%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rule%20Based%20Learning%20with%20Dynamic%20%28Graph%29%20Neural%20Networks&entry.906535625=Florian%20Seiffarth&entry.1292438233=%20%20A%20common%20problem%20of%20classical%20neural%20network%20architectures%20is%20that%20additional%0Ainformation%20or%20expert%20knowledge%20cannot%20be%20naturally%20integrated%20into%20the%0Alearning%20process.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20two-step%20approach%0Aconsisting%20of%20%281%29%20generating%20rule%20functions%20from%20knowledge%20and%20%282%29%20using%20these%0Arules%20to%20define%20rule%20based%20layers%20--%20a%20new%20type%20of%20dynamic%20neural%20network%0Alayer.%20The%20focus%20of%20this%20work%20is%20on%20the%20second%20step%2C%20i.e.%2C%20rule%20based%20layers%0Athat%20are%20designed%20to%20dynamically%20arrange%20learnable%20parameters%20in%20the%20weight%0Amatrices%20and%20bias%20vectors%20depending%20on%20the%20input%20samples.%20Indeed%2C%20we%20prove%20that%0Aour%20approach%20generalizes%20classical%20feed-forward%20layers%20such%20as%20fully%20connected%0Aand%20convolutional%20layers%20by%20choosing%20appropriate%20rules.%20As%20a%20concrete%0Aapplication%20we%20present%20rule%20based%20graph%20neural%20networks%20%28RuleGNNs%29%20that%0Aovercome%20some%20limitations%20of%20ordinary%20graph%20neural%20networks.%20Our%20experiments%0Ashow%20that%20the%20predictive%20performance%20of%20RuleGNNs%20is%20comparable%20to%0Astate-of-the-art%20graph%20classifiers%20using%20simple%20rules%20based%20on%20Weisfeiler-Leman%0Alabeling%20and%20pattern%20counting.%20Moreover%2C%20we%20introduce%20new%20synthetic%20benchmark%0Agraph%20datasets%20to%20show%20how%20to%20integrate%20expert%20knowledge%20into%20RuleGNNs%20making%0Athem%20more%20powerful%20than%20ordinary%20graph%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09954v1&entry.124074799=Read"},
{"title": "Unobtrusive Monitoring of Physical Weakness: A Simulated Approach", "author": "Chen Long-fei and Muhammad Ahmed Raza and Craig Innes and Subramanian Ramamoorthy and Robert B. Fisher", "abstract": "  Aging and chronic conditions affect older adults' daily lives, making early\ndetection of developing health issues crucial. Weakness, common in many\nconditions, alters physical movements and daily activities subtly. However,\ndetecting such changes can be challenging due to their subtle and gradual\nnature. To address this, we employ a non-intrusive camera sensor to monitor\nindividuals' daily sitting and relaxing activities for signs of weakness. We\nsimulate weakness in healthy subjects by having them perform physical exercise\nand observing the behavioral changes in their daily activities before and after\nworkouts. The proposed system captures fine-grained features related to body\nmotion, inactivity, and environmental context in real-time while prioritizing\nprivacy. A Bayesian Network is used to model the relationships between\nfeatures, activities, and health conditions. We aim to identify specific\nfeatures and activities that indicate such changes and determine the most\nsuitable time scale for observing the change. Results show 0.97 accuracy in\ndistinguishing simulated weakness at the daily level. Fine-grained behavioral\nfeatures, including non-dominant upper body motion speed and scale, and\ninactivity distribution, along with a 300-second window, are found most\neffective. However, individual-specific models are recommended as no universal\nset of optimal features and activities was identified across all participants.\n", "link": "http://arxiv.org/abs/2406.10045v1", "date": "2024-06-14", "relevancy": 2.0827, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5563}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5287}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unobtrusive%20Monitoring%20of%20Physical%20Weakness%3A%20A%20Simulated%20Approach&body=Title%3A%20Unobtrusive%20Monitoring%20of%20Physical%20Weakness%3A%20A%20Simulated%20Approach%0AAuthor%3A%20Chen%20Long-fei%20and%20Muhammad%20Ahmed%20Raza%20and%20Craig%20Innes%20and%20Subramanian%20Ramamoorthy%20and%20Robert%20B.%20Fisher%0AAbstract%3A%20%20%20Aging%20and%20chronic%20conditions%20affect%20older%20adults%27%20daily%20lives%2C%20making%20early%0Adetection%20of%20developing%20health%20issues%20crucial.%20Weakness%2C%20common%20in%20many%0Aconditions%2C%20alters%20physical%20movements%20and%20daily%20activities%20subtly.%20However%2C%0Adetecting%20such%20changes%20can%20be%20challenging%20due%20to%20their%20subtle%20and%20gradual%0Anature.%20To%20address%20this%2C%20we%20employ%20a%20non-intrusive%20camera%20sensor%20to%20monitor%0Aindividuals%27%20daily%20sitting%20and%20relaxing%20activities%20for%20signs%20of%20weakness.%20We%0Asimulate%20weakness%20in%20healthy%20subjects%20by%20having%20them%20perform%20physical%20exercise%0Aand%20observing%20the%20behavioral%20changes%20in%20their%20daily%20activities%20before%20and%20after%0Aworkouts.%20The%20proposed%20system%20captures%20fine-grained%20features%20related%20to%20body%0Amotion%2C%20inactivity%2C%20and%20environmental%20context%20in%20real-time%20while%20prioritizing%0Aprivacy.%20A%20Bayesian%20Network%20is%20used%20to%20model%20the%20relationships%20between%0Afeatures%2C%20activities%2C%20and%20health%20conditions.%20We%20aim%20to%20identify%20specific%0Afeatures%20and%20activities%20that%20indicate%20such%20changes%20and%20determine%20the%20most%0Asuitable%20time%20scale%20for%20observing%20the%20change.%20Results%20show%200.97%20accuracy%20in%0Adistinguishing%20simulated%20weakness%20at%20the%20daily%20level.%20Fine-grained%20behavioral%0Afeatures%2C%20including%20non-dominant%20upper%20body%20motion%20speed%20and%20scale%2C%20and%0Ainactivity%20distribution%2C%20along%20with%20a%20300-second%20window%2C%20are%20found%20most%0Aeffective.%20However%2C%20individual-specific%20models%20are%20recommended%20as%20no%20universal%0Aset%20of%20optimal%20features%20and%20activities%20was%20identified%20across%20all%20participants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnobtrusive%2520Monitoring%2520of%2520Physical%2520Weakness%253A%2520A%2520Simulated%2520Approach%26entry.906535625%3DChen%2520Long-fei%2520and%2520Muhammad%2520Ahmed%2520Raza%2520and%2520Craig%2520Innes%2520and%2520Subramanian%2520Ramamoorthy%2520and%2520Robert%2520B.%2520Fisher%26entry.1292438233%3D%2520%2520Aging%2520and%2520chronic%2520conditions%2520affect%2520older%2520adults%2527%2520daily%2520lives%252C%2520making%2520early%250Adetection%2520of%2520developing%2520health%2520issues%2520crucial.%2520Weakness%252C%2520common%2520in%2520many%250Aconditions%252C%2520alters%2520physical%2520movements%2520and%2520daily%2520activities%2520subtly.%2520However%252C%250Adetecting%2520such%2520changes%2520can%2520be%2520challenging%2520due%2520to%2520their%2520subtle%2520and%2520gradual%250Anature.%2520To%2520address%2520this%252C%2520we%2520employ%2520a%2520non-intrusive%2520camera%2520sensor%2520to%2520monitor%250Aindividuals%2527%2520daily%2520sitting%2520and%2520relaxing%2520activities%2520for%2520signs%2520of%2520weakness.%2520We%250Asimulate%2520weakness%2520in%2520healthy%2520subjects%2520by%2520having%2520them%2520perform%2520physical%2520exercise%250Aand%2520observing%2520the%2520behavioral%2520changes%2520in%2520their%2520daily%2520activities%2520before%2520and%2520after%250Aworkouts.%2520The%2520proposed%2520system%2520captures%2520fine-grained%2520features%2520related%2520to%2520body%250Amotion%252C%2520inactivity%252C%2520and%2520environmental%2520context%2520in%2520real-time%2520while%2520prioritizing%250Aprivacy.%2520A%2520Bayesian%2520Network%2520is%2520used%2520to%2520model%2520the%2520relationships%2520between%250Afeatures%252C%2520activities%252C%2520and%2520health%2520conditions.%2520We%2520aim%2520to%2520identify%2520specific%250Afeatures%2520and%2520activities%2520that%2520indicate%2520such%2520changes%2520and%2520determine%2520the%2520most%250Asuitable%2520time%2520scale%2520for%2520observing%2520the%2520change.%2520Results%2520show%25200.97%2520accuracy%2520in%250Adistinguishing%2520simulated%2520weakness%2520at%2520the%2520daily%2520level.%2520Fine-grained%2520behavioral%250Afeatures%252C%2520including%2520non-dominant%2520upper%2520body%2520motion%2520speed%2520and%2520scale%252C%2520and%250Ainactivity%2520distribution%252C%2520along%2520with%2520a%2520300-second%2520window%252C%2520are%2520found%2520most%250Aeffective.%2520However%252C%2520individual-specific%2520models%2520are%2520recommended%2520as%2520no%2520universal%250Aset%2520of%2520optimal%2520features%2520and%2520activities%2520was%2520identified%2520across%2520all%2520participants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unobtrusive%20Monitoring%20of%20Physical%20Weakness%3A%20A%20Simulated%20Approach&entry.906535625=Chen%20Long-fei%20and%20Muhammad%20Ahmed%20Raza%20and%20Craig%20Innes%20and%20Subramanian%20Ramamoorthy%20and%20Robert%20B.%20Fisher&entry.1292438233=%20%20Aging%20and%20chronic%20conditions%20affect%20older%20adults%27%20daily%20lives%2C%20making%20early%0Adetection%20of%20developing%20health%20issues%20crucial.%20Weakness%2C%20common%20in%20many%0Aconditions%2C%20alters%20physical%20movements%20and%20daily%20activities%20subtly.%20However%2C%0Adetecting%20such%20changes%20can%20be%20challenging%20due%20to%20their%20subtle%20and%20gradual%0Anature.%20To%20address%20this%2C%20we%20employ%20a%20non-intrusive%20camera%20sensor%20to%20monitor%0Aindividuals%27%20daily%20sitting%20and%20relaxing%20activities%20for%20signs%20of%20weakness.%20We%0Asimulate%20weakness%20in%20healthy%20subjects%20by%20having%20them%20perform%20physical%20exercise%0Aand%20observing%20the%20behavioral%20changes%20in%20their%20daily%20activities%20before%20and%20after%0Aworkouts.%20The%20proposed%20system%20captures%20fine-grained%20features%20related%20to%20body%0Amotion%2C%20inactivity%2C%20and%20environmental%20context%20in%20real-time%20while%20prioritizing%0Aprivacy.%20A%20Bayesian%20Network%20is%20used%20to%20model%20the%20relationships%20between%0Afeatures%2C%20activities%2C%20and%20health%20conditions.%20We%20aim%20to%20identify%20specific%0Afeatures%20and%20activities%20that%20indicate%20such%20changes%20and%20determine%20the%20most%0Asuitable%20time%20scale%20for%20observing%20the%20change.%20Results%20show%200.97%20accuracy%20in%0Adistinguishing%20simulated%20weakness%20at%20the%20daily%20level.%20Fine-grained%20behavioral%0Afeatures%2C%20including%20non-dominant%20upper%20body%20motion%20speed%20and%20scale%2C%20and%0Ainactivity%20distribution%2C%20along%20with%20a%20300-second%20window%2C%20are%20found%20most%0Aeffective.%20However%2C%20individual-specific%20models%20are%20recommended%20as%20no%20universal%0Aset%20of%20optimal%20features%20and%20activities%20was%20identified%20across%20all%20participants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10045v1&entry.124074799=Read"},
{"title": "SmartRSD: An Intelligent Multimodal Approach to Real-Time Road Surface\n  Detection for Safe Driving", "author": "Adnan Md Tayeb and Mst Ayesha Khatun and Mohtasin Golam and Md Facklasur Rahaman and Ali Aouto and Oroceo Paul Angelo and Minseon Lee and Dong-Seong Kim and Jae-Min Lee and Jung-Hyeon Kim", "abstract": "  Precise and prompt identification of road surface conditions enables vehicles\nto adjust their actions, like changing speed or using specific traction control\ntechniques, to lower the chance of accidents and potential danger to drivers\nand pedestrians. However, most of the existing methods for detecting road\nsurfaces solely rely on visual data, which may be insufficient in certain\nsituations, such as when the roads are covered by debris, in low light\nconditions, or in the presence of fog. Therefore, we introduce a multimodal\napproach for the automated detection of road surface conditions by integrating\naudio and images. The robustness of the proposed method is tested on a diverse\ndataset collected under various environmental conditions and road surface\ntypes. Through extensive evaluation, we demonstrate the effectiveness and\nreliability of our multimodal approach in accurately identifying road surface\nconditions in real-time scenarios. Our findings highlight the potential of\nintegrating auditory and visual cues for enhancing road safety and minimizing\naccident risks\n", "link": "http://arxiv.org/abs/2406.10128v1", "date": "2024-06-14", "relevancy": 2.0728, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.595}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5033}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SmartRSD%3A%20An%20Intelligent%20Multimodal%20Approach%20to%20Real-Time%20Road%20Surface%0A%20%20Detection%20for%20Safe%20Driving&body=Title%3A%20SmartRSD%3A%20An%20Intelligent%20Multimodal%20Approach%20to%20Real-Time%20Road%20Surface%0A%20%20Detection%20for%20Safe%20Driving%0AAuthor%3A%20Adnan%20Md%20Tayeb%20and%20Mst%20Ayesha%20Khatun%20and%20Mohtasin%20Golam%20and%20Md%20Facklasur%20Rahaman%20and%20Ali%20Aouto%20and%20Oroceo%20Paul%20Angelo%20and%20Minseon%20Lee%20and%20Dong-Seong%20Kim%20and%20Jae-Min%20Lee%20and%20Jung-Hyeon%20Kim%0AAbstract%3A%20%20%20Precise%20and%20prompt%20identification%20of%20road%20surface%20conditions%20enables%20vehicles%0Ato%20adjust%20their%20actions%2C%20like%20changing%20speed%20or%20using%20specific%20traction%20control%0Atechniques%2C%20to%20lower%20the%20chance%20of%20accidents%20and%20potential%20danger%20to%20drivers%0Aand%20pedestrians.%20However%2C%20most%20of%20the%20existing%20methods%20for%20detecting%20road%0Asurfaces%20solely%20rely%20on%20visual%20data%2C%20which%20may%20be%20insufficient%20in%20certain%0Asituations%2C%20such%20as%20when%20the%20roads%20are%20covered%20by%20debris%2C%20in%20low%20light%0Aconditions%2C%20or%20in%20the%20presence%20of%20fog.%20Therefore%2C%20we%20introduce%20a%20multimodal%0Aapproach%20for%20the%20automated%20detection%20of%20road%20surface%20conditions%20by%20integrating%0Aaudio%20and%20images.%20The%20robustness%20of%20the%20proposed%20method%20is%20tested%20on%20a%20diverse%0Adataset%20collected%20under%20various%20environmental%20conditions%20and%20road%20surface%0Atypes.%20Through%20extensive%20evaluation%2C%20we%20demonstrate%20the%20effectiveness%20and%0Areliability%20of%20our%20multimodal%20approach%20in%20accurately%20identifying%20road%20surface%0Aconditions%20in%20real-time%20scenarios.%20Our%20findings%20highlight%20the%20potential%20of%0Aintegrating%20auditory%20and%20visual%20cues%20for%20enhancing%20road%20safety%20and%20minimizing%0Aaccident%20risks%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmartRSD%253A%2520An%2520Intelligent%2520Multimodal%2520Approach%2520to%2520Real-Time%2520Road%2520Surface%250A%2520%2520Detection%2520for%2520Safe%2520Driving%26entry.906535625%3DAdnan%2520Md%2520Tayeb%2520and%2520Mst%2520Ayesha%2520Khatun%2520and%2520Mohtasin%2520Golam%2520and%2520Md%2520Facklasur%2520Rahaman%2520and%2520Ali%2520Aouto%2520and%2520Oroceo%2520Paul%2520Angelo%2520and%2520Minseon%2520Lee%2520and%2520Dong-Seong%2520Kim%2520and%2520Jae-Min%2520Lee%2520and%2520Jung-Hyeon%2520Kim%26entry.1292438233%3D%2520%2520Precise%2520and%2520prompt%2520identification%2520of%2520road%2520surface%2520conditions%2520enables%2520vehicles%250Ato%2520adjust%2520their%2520actions%252C%2520like%2520changing%2520speed%2520or%2520using%2520specific%2520traction%2520control%250Atechniques%252C%2520to%2520lower%2520the%2520chance%2520of%2520accidents%2520and%2520potential%2520danger%2520to%2520drivers%250Aand%2520pedestrians.%2520However%252C%2520most%2520of%2520the%2520existing%2520methods%2520for%2520detecting%2520road%250Asurfaces%2520solely%2520rely%2520on%2520visual%2520data%252C%2520which%2520may%2520be%2520insufficient%2520in%2520certain%250Asituations%252C%2520such%2520as%2520when%2520the%2520roads%2520are%2520covered%2520by%2520debris%252C%2520in%2520low%2520light%250Aconditions%252C%2520or%2520in%2520the%2520presence%2520of%2520fog.%2520Therefore%252C%2520we%2520introduce%2520a%2520multimodal%250Aapproach%2520for%2520the%2520automated%2520detection%2520of%2520road%2520surface%2520conditions%2520by%2520integrating%250Aaudio%2520and%2520images.%2520The%2520robustness%2520of%2520the%2520proposed%2520method%2520is%2520tested%2520on%2520a%2520diverse%250Adataset%2520collected%2520under%2520various%2520environmental%2520conditions%2520and%2520road%2520surface%250Atypes.%2520Through%2520extensive%2520evaluation%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520and%250Areliability%2520of%2520our%2520multimodal%2520approach%2520in%2520accurately%2520identifying%2520road%2520surface%250Aconditions%2520in%2520real-time%2520scenarios.%2520Our%2520findings%2520highlight%2520the%2520potential%2520of%250Aintegrating%2520auditory%2520and%2520visual%2520cues%2520for%2520enhancing%2520road%2520safety%2520and%2520minimizing%250Aaccident%2520risks%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SmartRSD%3A%20An%20Intelligent%20Multimodal%20Approach%20to%20Real-Time%20Road%20Surface%0A%20%20Detection%20for%20Safe%20Driving&entry.906535625=Adnan%20Md%20Tayeb%20and%20Mst%20Ayesha%20Khatun%20and%20Mohtasin%20Golam%20and%20Md%20Facklasur%20Rahaman%20and%20Ali%20Aouto%20and%20Oroceo%20Paul%20Angelo%20and%20Minseon%20Lee%20and%20Dong-Seong%20Kim%20and%20Jae-Min%20Lee%20and%20Jung-Hyeon%20Kim&entry.1292438233=%20%20Precise%20and%20prompt%20identification%20of%20road%20surface%20conditions%20enables%20vehicles%0Ato%20adjust%20their%20actions%2C%20like%20changing%20speed%20or%20using%20specific%20traction%20control%0Atechniques%2C%20to%20lower%20the%20chance%20of%20accidents%20and%20potential%20danger%20to%20drivers%0Aand%20pedestrians.%20However%2C%20most%20of%20the%20existing%20methods%20for%20detecting%20road%0Asurfaces%20solely%20rely%20on%20visual%20data%2C%20which%20may%20be%20insufficient%20in%20certain%0Asituations%2C%20such%20as%20when%20the%20roads%20are%20covered%20by%20debris%2C%20in%20low%20light%0Aconditions%2C%20or%20in%20the%20presence%20of%20fog.%20Therefore%2C%20we%20introduce%20a%20multimodal%0Aapproach%20for%20the%20automated%20detection%20of%20road%20surface%20conditions%20by%20integrating%0Aaudio%20and%20images.%20The%20robustness%20of%20the%20proposed%20method%20is%20tested%20on%20a%20diverse%0Adataset%20collected%20under%20various%20environmental%20conditions%20and%20road%20surface%0Atypes.%20Through%20extensive%20evaluation%2C%20we%20demonstrate%20the%20effectiveness%20and%0Areliability%20of%20our%20multimodal%20approach%20in%20accurately%20identifying%20road%20surface%0Aconditions%20in%20real-time%20scenarios.%20Our%20findings%20highlight%20the%20potential%20of%0Aintegrating%20auditory%20and%20visual%20cues%20for%20enhancing%20road%20safety%20and%20minimizing%0Aaccident%20risks%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10128v1&entry.124074799=Read"},
{"title": "VideoGUI: A Benchmark for GUI Automation from Instructional Videos", "author": "Kevin Qinghong Lin and Linjie Li and Difei Gao and Qinchen WU and Mingyi Yan and Zhengyuan Yang and Lijuan Wang and Mike Zheng Shou", "abstract": "  Graphical User Interface (GUI) automation holds significant promise for\nenhancing human productivity by assisting with computer tasks. Existing task\nformulations primarily focus on simple tasks that can be specified by a single,\nlanguage-only instruction, such as \"Insert a new slide.\" In this work, we\nintroduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI\nassistants on visual-centric GUI tasks. Sourced from high-quality web\ninstructional videos, our benchmark focuses on tasks involving professional and\nnovel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complex\nactivities (e.g., video editing). VideoGUI evaluates GUI assistants through a\nhierarchical process, allowing for identification of the specific levels at\nwhich they may fail: (i) high-level planning: reconstruct procedural subtasks\nfrom visual conditions without language descriptions; (ii) middle-level\nplanning: generate sequences of precise action narrations based on visual state\n(i.e., screenshot) and goals; (iii) atomic action execution: perform specific\nactions such as accurately clicking designated elements. For each level, we\ndesign evaluation metrics across individual dimensions to provide clear\nsignals, such as individual performance in clicking, dragging, typing, and\nscrolling for atomic action execution. Our evaluation on VideoGUI reveals that\neven the SoTA large multimodal model GPT4o performs poorly on visual-centric\nGUI tasks, especially for high-level planning.\n", "link": "http://arxiv.org/abs/2406.10227v1", "date": "2024-06-14", "relevancy": 2.0699, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5288}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5221}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoGUI%3A%20A%20Benchmark%20for%20GUI%20Automation%20from%20Instructional%20Videos&body=Title%3A%20VideoGUI%3A%20A%20Benchmark%20for%20GUI%20Automation%20from%20Instructional%20Videos%0AAuthor%3A%20Kevin%20Qinghong%20Lin%20and%20Linjie%20Li%20and%20Difei%20Gao%20and%20Qinchen%20WU%20and%20Mingyi%20Yan%20and%20Zhengyuan%20Yang%20and%20Lijuan%20Wang%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Graphical%20User%20Interface%20%28GUI%29%20automation%20holds%20significant%20promise%20for%0Aenhancing%20human%20productivity%20by%20assisting%20with%20computer%20tasks.%20Existing%20task%0Aformulations%20primarily%20focus%20on%20simple%20tasks%20that%20can%20be%20specified%20by%20a%20single%2C%0Alanguage-only%20instruction%2C%20such%20as%20%22Insert%20a%20new%20slide.%22%20In%20this%20work%2C%20we%0Aintroduce%20VideoGUI%2C%20a%20novel%20multi-modal%20benchmark%20designed%20to%20evaluate%20GUI%0Aassistants%20on%20visual-centric%20GUI%20tasks.%20Sourced%20from%20high-quality%20web%0Ainstructional%20videos%2C%20our%20benchmark%20focuses%20on%20tasks%20involving%20professional%20and%0Anovel%20software%20%28e.g.%2C%20Adobe%20Photoshop%20or%20Stable%20Diffusion%20WebUI%29%20and%20complex%0Aactivities%20%28e.g.%2C%20video%20editing%29.%20VideoGUI%20evaluates%20GUI%20assistants%20through%20a%0Ahierarchical%20process%2C%20allowing%20for%20identification%20of%20the%20specific%20levels%20at%0Awhich%20they%20may%20fail%3A%20%28i%29%20high-level%20planning%3A%20reconstruct%20procedural%20subtasks%0Afrom%20visual%20conditions%20without%20language%20descriptions%3B%20%28ii%29%20middle-level%0Aplanning%3A%20generate%20sequences%20of%20precise%20action%20narrations%20based%20on%20visual%20state%0A%28i.e.%2C%20screenshot%29%20and%20goals%3B%20%28iii%29%20atomic%20action%20execution%3A%20perform%20specific%0Aactions%20such%20as%20accurately%20clicking%20designated%20elements.%20For%20each%20level%2C%20we%0Adesign%20evaluation%20metrics%20across%20individual%20dimensions%20to%20provide%20clear%0Asignals%2C%20such%20as%20individual%20performance%20in%20clicking%2C%20dragging%2C%20typing%2C%20and%0Ascrolling%20for%20atomic%20action%20execution.%20Our%20evaluation%20on%20VideoGUI%20reveals%20that%0Aeven%20the%20SoTA%20large%20multimodal%20model%20GPT4o%20performs%20poorly%20on%20visual-centric%0AGUI%20tasks%2C%20especially%20for%20high-level%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoGUI%253A%2520A%2520Benchmark%2520for%2520GUI%2520Automation%2520from%2520Instructional%2520Videos%26entry.906535625%3DKevin%2520Qinghong%2520Lin%2520and%2520Linjie%2520Li%2520and%2520Difei%2520Gao%2520and%2520Qinchen%2520WU%2520and%2520Mingyi%2520Yan%2520and%2520Zhengyuan%2520Yang%2520and%2520Lijuan%2520Wang%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520automation%2520holds%2520significant%2520promise%2520for%250Aenhancing%2520human%2520productivity%2520by%2520assisting%2520with%2520computer%2520tasks.%2520Existing%2520task%250Aformulations%2520primarily%2520focus%2520on%2520simple%2520tasks%2520that%2520can%2520be%2520specified%2520by%2520a%2520single%252C%250Alanguage-only%2520instruction%252C%2520such%2520as%2520%2522Insert%2520a%2520new%2520slide.%2522%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520VideoGUI%252C%2520a%2520novel%2520multi-modal%2520benchmark%2520designed%2520to%2520evaluate%2520GUI%250Aassistants%2520on%2520visual-centric%2520GUI%2520tasks.%2520Sourced%2520from%2520high-quality%2520web%250Ainstructional%2520videos%252C%2520our%2520benchmark%2520focuses%2520on%2520tasks%2520involving%2520professional%2520and%250Anovel%2520software%2520%2528e.g.%252C%2520Adobe%2520Photoshop%2520or%2520Stable%2520Diffusion%2520WebUI%2529%2520and%2520complex%250Aactivities%2520%2528e.g.%252C%2520video%2520editing%2529.%2520VideoGUI%2520evaluates%2520GUI%2520assistants%2520through%2520a%250Ahierarchical%2520process%252C%2520allowing%2520for%2520identification%2520of%2520the%2520specific%2520levels%2520at%250Awhich%2520they%2520may%2520fail%253A%2520%2528i%2529%2520high-level%2520planning%253A%2520reconstruct%2520procedural%2520subtasks%250Afrom%2520visual%2520conditions%2520without%2520language%2520descriptions%253B%2520%2528ii%2529%2520middle-level%250Aplanning%253A%2520generate%2520sequences%2520of%2520precise%2520action%2520narrations%2520based%2520on%2520visual%2520state%250A%2528i.e.%252C%2520screenshot%2529%2520and%2520goals%253B%2520%2528iii%2529%2520atomic%2520action%2520execution%253A%2520perform%2520specific%250Aactions%2520such%2520as%2520accurately%2520clicking%2520designated%2520elements.%2520For%2520each%2520level%252C%2520we%250Adesign%2520evaluation%2520metrics%2520across%2520individual%2520dimensions%2520to%2520provide%2520clear%250Asignals%252C%2520such%2520as%2520individual%2520performance%2520in%2520clicking%252C%2520dragging%252C%2520typing%252C%2520and%250Ascrolling%2520for%2520atomic%2520action%2520execution.%2520Our%2520evaluation%2520on%2520VideoGUI%2520reveals%2520that%250Aeven%2520the%2520SoTA%2520large%2520multimodal%2520model%2520GPT4o%2520performs%2520poorly%2520on%2520visual-centric%250AGUI%2520tasks%252C%2520especially%2520for%2520high-level%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoGUI%3A%20A%20Benchmark%20for%20GUI%20Automation%20from%20Instructional%20Videos&entry.906535625=Kevin%20Qinghong%20Lin%20and%20Linjie%20Li%20and%20Difei%20Gao%20and%20Qinchen%20WU%20and%20Mingyi%20Yan%20and%20Zhengyuan%20Yang%20and%20Lijuan%20Wang%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Graphical%20User%20Interface%20%28GUI%29%20automation%20holds%20significant%20promise%20for%0Aenhancing%20human%20productivity%20by%20assisting%20with%20computer%20tasks.%20Existing%20task%0Aformulations%20primarily%20focus%20on%20simple%20tasks%20that%20can%20be%20specified%20by%20a%20single%2C%0Alanguage-only%20instruction%2C%20such%20as%20%22Insert%20a%20new%20slide.%22%20In%20this%20work%2C%20we%0Aintroduce%20VideoGUI%2C%20a%20novel%20multi-modal%20benchmark%20designed%20to%20evaluate%20GUI%0Aassistants%20on%20visual-centric%20GUI%20tasks.%20Sourced%20from%20high-quality%20web%0Ainstructional%20videos%2C%20our%20benchmark%20focuses%20on%20tasks%20involving%20professional%20and%0Anovel%20software%20%28e.g.%2C%20Adobe%20Photoshop%20or%20Stable%20Diffusion%20WebUI%29%20and%20complex%0Aactivities%20%28e.g.%2C%20video%20editing%29.%20VideoGUI%20evaluates%20GUI%20assistants%20through%20a%0Ahierarchical%20process%2C%20allowing%20for%20identification%20of%20the%20specific%20levels%20at%0Awhich%20they%20may%20fail%3A%20%28i%29%20high-level%20planning%3A%20reconstruct%20procedural%20subtasks%0Afrom%20visual%20conditions%20without%20language%20descriptions%3B%20%28ii%29%20middle-level%0Aplanning%3A%20generate%20sequences%20of%20precise%20action%20narrations%20based%20on%20visual%20state%0A%28i.e.%2C%20screenshot%29%20and%20goals%3B%20%28iii%29%20atomic%20action%20execution%3A%20perform%20specific%0Aactions%20such%20as%20accurately%20clicking%20designated%20elements.%20For%20each%20level%2C%20we%0Adesign%20evaluation%20metrics%20across%20individual%20dimensions%20to%20provide%20clear%0Asignals%2C%20such%20as%20individual%20performance%20in%20clicking%2C%20dragging%2C%20typing%2C%20and%0Ascrolling%20for%20atomic%20action%20execution.%20Our%20evaluation%20on%20VideoGUI%20reveals%20that%0Aeven%20the%20SoTA%20large%20multimodal%20model%20GPT4o%20performs%20poorly%20on%20visual-centric%0AGUI%20tasks%2C%20especially%20for%20high-level%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10227v1&entry.124074799=Read"},
{"title": "CarLLaVA: Vision language models for camera-only closed-loop driving", "author": "Katrin Renz and Long Chen and Ana-Maria Marcu and Jan H\u00fcnermann and Benoit Hanotte and Alice Karnsund and Jamie Shotton and Elahe Arani and Oleg Sinavski", "abstract": "  In this technical report, we present CarLLaVA, a Vision Language Model (VLM)\nfor autonomous driving, developed for the CARLA Autonomous Driving Challenge\n2.0. CarLLaVA uses the vision encoder of the LLaVA VLM and the LLaMA\narchitecture as backbone, achieving state-of-the-art closed-loop driving\nperformance with only camera input and without the need for complex or\nexpensive labels. Additionally, we show preliminary results on predicting\nlanguage commentary alongside the driving output. CarLLaVA uses a\nsemi-disentangled output representation of both path predictions and waypoints,\ngetting the advantages of the path for better lateral control and the waypoints\nfor better longitudinal control. We propose an efficient training recipe to\ntrain on large driving datasets without wasting compute on easy, trivial data.\nCarLLaVA ranks 1st place in the sensor track of the CARLA Autonomous Driving\nChallenge 2.0 outperforming the previous state of the art by 458% and the best\nconcurrent submission by 32.6%.\n", "link": "http://arxiv.org/abs/2406.10165v1", "date": "2024-06-14", "relevancy": 2.0683, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5282}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5126}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CarLLaVA%3A%20Vision%20language%20models%20for%20camera-only%20closed-loop%20driving&body=Title%3A%20CarLLaVA%3A%20Vision%20language%20models%20for%20camera-only%20closed-loop%20driving%0AAuthor%3A%20Katrin%20Renz%20and%20Long%20Chen%20and%20Ana-Maria%20Marcu%20and%20Jan%20H%C3%BCnermann%20and%20Benoit%20Hanotte%20and%20Alice%20Karnsund%20and%20Jamie%20Shotton%20and%20Elahe%20Arani%20and%20Oleg%20Sinavski%0AAbstract%3A%20%20%20In%20this%20technical%20report%2C%20we%20present%20CarLLaVA%2C%20a%20Vision%20Language%20Model%20%28VLM%29%0Afor%20autonomous%20driving%2C%20developed%20for%20the%20CARLA%20Autonomous%20Driving%20Challenge%0A2.0.%20CarLLaVA%20uses%20the%20vision%20encoder%20of%20the%20LLaVA%20VLM%20and%20the%20LLaMA%0Aarchitecture%20as%20backbone%2C%20achieving%20state-of-the-art%20closed-loop%20driving%0Aperformance%20with%20only%20camera%20input%20and%20without%20the%20need%20for%20complex%20or%0Aexpensive%20labels.%20Additionally%2C%20we%20show%20preliminary%20results%20on%20predicting%0Alanguage%20commentary%20alongside%20the%20driving%20output.%20CarLLaVA%20uses%20a%0Asemi-disentangled%20output%20representation%20of%20both%20path%20predictions%20and%20waypoints%2C%0Agetting%20the%20advantages%20of%20the%20path%20for%20better%20lateral%20control%20and%20the%20waypoints%0Afor%20better%20longitudinal%20control.%20We%20propose%20an%20efficient%20training%20recipe%20to%0Atrain%20on%20large%20driving%20datasets%20without%20wasting%20compute%20on%20easy%2C%20trivial%20data.%0ACarLLaVA%20ranks%201st%20place%20in%20the%20sensor%20track%20of%20the%20CARLA%20Autonomous%20Driving%0AChallenge%202.0%20outperforming%20the%20previous%20state%20of%20the%20art%20by%20458%25%20and%20the%20best%0Aconcurrent%20submission%20by%2032.6%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCarLLaVA%253A%2520Vision%2520language%2520models%2520for%2520camera-only%2520closed-loop%2520driving%26entry.906535625%3DKatrin%2520Renz%2520and%2520Long%2520Chen%2520and%2520Ana-Maria%2520Marcu%2520and%2520Jan%2520H%25C3%25BCnermann%2520and%2520Benoit%2520Hanotte%2520and%2520Alice%2520Karnsund%2520and%2520Jamie%2520Shotton%2520and%2520Elahe%2520Arani%2520and%2520Oleg%2520Sinavski%26entry.1292438233%3D%2520%2520In%2520this%2520technical%2520report%252C%2520we%2520present%2520CarLLaVA%252C%2520a%2520Vision%2520Language%2520Model%2520%2528VLM%2529%250Afor%2520autonomous%2520driving%252C%2520developed%2520for%2520the%2520CARLA%2520Autonomous%2520Driving%2520Challenge%250A2.0.%2520CarLLaVA%2520uses%2520the%2520vision%2520encoder%2520of%2520the%2520LLaVA%2520VLM%2520and%2520the%2520LLaMA%250Aarchitecture%2520as%2520backbone%252C%2520achieving%2520state-of-the-art%2520closed-loop%2520driving%250Aperformance%2520with%2520only%2520camera%2520input%2520and%2520without%2520the%2520need%2520for%2520complex%2520or%250Aexpensive%2520labels.%2520Additionally%252C%2520we%2520show%2520preliminary%2520results%2520on%2520predicting%250Alanguage%2520commentary%2520alongside%2520the%2520driving%2520output.%2520CarLLaVA%2520uses%2520a%250Asemi-disentangled%2520output%2520representation%2520of%2520both%2520path%2520predictions%2520and%2520waypoints%252C%250Agetting%2520the%2520advantages%2520of%2520the%2520path%2520for%2520better%2520lateral%2520control%2520and%2520the%2520waypoints%250Afor%2520better%2520longitudinal%2520control.%2520We%2520propose%2520an%2520efficient%2520training%2520recipe%2520to%250Atrain%2520on%2520large%2520driving%2520datasets%2520without%2520wasting%2520compute%2520on%2520easy%252C%2520trivial%2520data.%250ACarLLaVA%2520ranks%25201st%2520place%2520in%2520the%2520sensor%2520track%2520of%2520the%2520CARLA%2520Autonomous%2520Driving%250AChallenge%25202.0%2520outperforming%2520the%2520previous%2520state%2520of%2520the%2520art%2520by%2520458%2525%2520and%2520the%2520best%250Aconcurrent%2520submission%2520by%252032.6%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CarLLaVA%3A%20Vision%20language%20models%20for%20camera-only%20closed-loop%20driving&entry.906535625=Katrin%20Renz%20and%20Long%20Chen%20and%20Ana-Maria%20Marcu%20and%20Jan%20H%C3%BCnermann%20and%20Benoit%20Hanotte%20and%20Alice%20Karnsund%20and%20Jamie%20Shotton%20and%20Elahe%20Arani%20and%20Oleg%20Sinavski&entry.1292438233=%20%20In%20this%20technical%20report%2C%20we%20present%20CarLLaVA%2C%20a%20Vision%20Language%20Model%20%28VLM%29%0Afor%20autonomous%20driving%2C%20developed%20for%20the%20CARLA%20Autonomous%20Driving%20Challenge%0A2.0.%20CarLLaVA%20uses%20the%20vision%20encoder%20of%20the%20LLaVA%20VLM%20and%20the%20LLaMA%0Aarchitecture%20as%20backbone%2C%20achieving%20state-of-the-art%20closed-loop%20driving%0Aperformance%20with%20only%20camera%20input%20and%20without%20the%20need%20for%20complex%20or%0Aexpensive%20labels.%20Additionally%2C%20we%20show%20preliminary%20results%20on%20predicting%0Alanguage%20commentary%20alongside%20the%20driving%20output.%20CarLLaVA%20uses%20a%0Asemi-disentangled%20output%20representation%20of%20both%20path%20predictions%20and%20waypoints%2C%0Agetting%20the%20advantages%20of%20the%20path%20for%20better%20lateral%20control%20and%20the%20waypoints%0Afor%20better%20longitudinal%20control.%20We%20propose%20an%20efficient%20training%20recipe%20to%0Atrain%20on%20large%20driving%20datasets%20without%20wasting%20compute%20on%20easy%2C%20trivial%20data.%0ACarLLaVA%20ranks%201st%20place%20in%20the%20sensor%20track%20of%20the%20CARLA%20Autonomous%20Driving%0AChallenge%202.0%20outperforming%20the%20previous%20state%20of%20the%20art%20by%20458%25%20and%20the%20best%0Aconcurrent%20submission%20by%2032.6%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10165v1&entry.124074799=Read"},
{"title": "Retraining-free Model Quantization via One-Shot Weight-Coupling Learning", "author": "Chen Tang and Yuan Meng and Jiacheng Jiang and Shuzhao Xie and Rongwei Lu and Xinzhu Ma and Zhi Wang and Wenwu Zhu", "abstract": "  Quantization is of significance for compressing the over-parameterized deep\nneural models and deploying them on resource-limited devices. Fixed-precision\nquantization suffers from performance drop due to the limited numerical\nrepresentation ability. Conversely, mixed-precision quantization (MPQ) is\nadvocated to compress the model effectively by allocating heterogeneous\nbit-width for layers. MPQ is typically organized into a searching-retraining\ntwo-stage process. In this paper, we devise a one-shot training-searching\nparadigm for mixed-precision model compression. Specifically, in the first\nstage, all potential bit-width configurations are coupled and thus optimized\nsimultaneously within a set of shared weights. However, our observations reveal\na previously unseen and severe bit-width interference phenomenon among highly\ncoupled weights during optimization, leading to considerable performance\ndegradation under a high compression ratio. To tackle this problem, we first\ndesign a bit-width scheduler to dynamically freeze the most turbulent bit-width\nof layers during training, to ensure the rest bit-widths converged properly.\nThen, taking inspiration from information theory, we present an information\ndistortion mitigation technique to align the behavior of the bad-performing\nbit-widths to the well-performing ones. In the second stage, an inference-only\ngreedy search scheme is devised to evaluate the goodness of configurations\nwithout introducing any additional training costs. Extensive experiments on\nthree representative models and three datasets demonstrate the effectiveness of\nthe proposed method. Code can be available on\n\\href{https://www.github.com/1hunters/retraining-free-quantization}{https://github.com/1hunters/retraining-free-quantization}.\n", "link": "http://arxiv.org/abs/2401.01543v2", "date": "2024-06-14", "relevancy": 2.0643, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.521}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5165}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retraining-free%20Model%20Quantization%20via%20One-Shot%20Weight-Coupling%20Learning&body=Title%3A%20Retraining-free%20Model%20Quantization%20via%20One-Shot%20Weight-Coupling%20Learning%0AAuthor%3A%20Chen%20Tang%20and%20Yuan%20Meng%20and%20Jiacheng%20Jiang%20and%20Shuzhao%20Xie%20and%20Rongwei%20Lu%20and%20Xinzhu%20Ma%20and%20Zhi%20Wang%20and%20Wenwu%20Zhu%0AAbstract%3A%20%20%20Quantization%20is%20of%20significance%20for%20compressing%20the%20over-parameterized%20deep%0Aneural%20models%20and%20deploying%20them%20on%20resource-limited%20devices.%20Fixed-precision%0Aquantization%20suffers%20from%20performance%20drop%20due%20to%20the%20limited%20numerical%0Arepresentation%20ability.%20Conversely%2C%20mixed-precision%20quantization%20%28MPQ%29%20is%0Aadvocated%20to%20compress%20the%20model%20effectively%20by%20allocating%20heterogeneous%0Abit-width%20for%20layers.%20MPQ%20is%20typically%20organized%20into%20a%20searching-retraining%0Atwo-stage%20process.%20In%20this%20paper%2C%20we%20devise%20a%20one-shot%20training-searching%0Aparadigm%20for%20mixed-precision%20model%20compression.%20Specifically%2C%20in%20the%20first%0Astage%2C%20all%20potential%20bit-width%20configurations%20are%20coupled%20and%20thus%20optimized%0Asimultaneously%20within%20a%20set%20of%20shared%20weights.%20However%2C%20our%20observations%20reveal%0Aa%20previously%20unseen%20and%20severe%20bit-width%20interference%20phenomenon%20among%20highly%0Acoupled%20weights%20during%20optimization%2C%20leading%20to%20considerable%20performance%0Adegradation%20under%20a%20high%20compression%20ratio.%20To%20tackle%20this%20problem%2C%20we%20first%0Adesign%20a%20bit-width%20scheduler%20to%20dynamically%20freeze%20the%20most%20turbulent%20bit-width%0Aof%20layers%20during%20training%2C%20to%20ensure%20the%20rest%20bit-widths%20converged%20properly.%0AThen%2C%20taking%20inspiration%20from%20information%20theory%2C%20we%20present%20an%20information%0Adistortion%20mitigation%20technique%20to%20align%20the%20behavior%20of%20the%20bad-performing%0Abit-widths%20to%20the%20well-performing%20ones.%20In%20the%20second%20stage%2C%20an%20inference-only%0Agreedy%20search%20scheme%20is%20devised%20to%20evaluate%20the%20goodness%20of%20configurations%0Awithout%20introducing%20any%20additional%20training%20costs.%20Extensive%20experiments%20on%0Athree%20representative%20models%20and%20three%20datasets%20demonstrate%20the%20effectiveness%20of%0Athe%20proposed%20method.%20Code%20can%20be%20available%20on%0A%5Chref%7Bhttps%3A//www.github.com/1hunters/retraining-free-quantization%7D%7Bhttps%3A//github.com/1hunters/retraining-free-quantization%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01543v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetraining-free%2520Model%2520Quantization%2520via%2520One-Shot%2520Weight-Coupling%2520Learning%26entry.906535625%3DChen%2520Tang%2520and%2520Yuan%2520Meng%2520and%2520Jiacheng%2520Jiang%2520and%2520Shuzhao%2520Xie%2520and%2520Rongwei%2520Lu%2520and%2520Xinzhu%2520Ma%2520and%2520Zhi%2520Wang%2520and%2520Wenwu%2520Zhu%26entry.1292438233%3D%2520%2520Quantization%2520is%2520of%2520significance%2520for%2520compressing%2520the%2520over-parameterized%2520deep%250Aneural%2520models%2520and%2520deploying%2520them%2520on%2520resource-limited%2520devices.%2520Fixed-precision%250Aquantization%2520suffers%2520from%2520performance%2520drop%2520due%2520to%2520the%2520limited%2520numerical%250Arepresentation%2520ability.%2520Conversely%252C%2520mixed-precision%2520quantization%2520%2528MPQ%2529%2520is%250Aadvocated%2520to%2520compress%2520the%2520model%2520effectively%2520by%2520allocating%2520heterogeneous%250Abit-width%2520for%2520layers.%2520MPQ%2520is%2520typically%2520organized%2520into%2520a%2520searching-retraining%250Atwo-stage%2520process.%2520In%2520this%2520paper%252C%2520we%2520devise%2520a%2520one-shot%2520training-searching%250Aparadigm%2520for%2520mixed-precision%2520model%2520compression.%2520Specifically%252C%2520in%2520the%2520first%250Astage%252C%2520all%2520potential%2520bit-width%2520configurations%2520are%2520coupled%2520and%2520thus%2520optimized%250Asimultaneously%2520within%2520a%2520set%2520of%2520shared%2520weights.%2520However%252C%2520our%2520observations%2520reveal%250Aa%2520previously%2520unseen%2520and%2520severe%2520bit-width%2520interference%2520phenomenon%2520among%2520highly%250Acoupled%2520weights%2520during%2520optimization%252C%2520leading%2520to%2520considerable%2520performance%250Adegradation%2520under%2520a%2520high%2520compression%2520ratio.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520first%250Adesign%2520a%2520bit-width%2520scheduler%2520to%2520dynamically%2520freeze%2520the%2520most%2520turbulent%2520bit-width%250Aof%2520layers%2520during%2520training%252C%2520to%2520ensure%2520the%2520rest%2520bit-widths%2520converged%2520properly.%250AThen%252C%2520taking%2520inspiration%2520from%2520information%2520theory%252C%2520we%2520present%2520an%2520information%250Adistortion%2520mitigation%2520technique%2520to%2520align%2520the%2520behavior%2520of%2520the%2520bad-performing%250Abit-widths%2520to%2520the%2520well-performing%2520ones.%2520In%2520the%2520second%2520stage%252C%2520an%2520inference-only%250Agreedy%2520search%2520scheme%2520is%2520devised%2520to%2520evaluate%2520the%2520goodness%2520of%2520configurations%250Awithout%2520introducing%2520any%2520additional%2520training%2520costs.%2520Extensive%2520experiments%2520on%250Athree%2520representative%2520models%2520and%2520three%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%250Athe%2520proposed%2520method.%2520Code%2520can%2520be%2520available%2520on%250A%255Chref%257Bhttps%253A//www.github.com/1hunters/retraining-free-quantization%257D%257Bhttps%253A//github.com/1hunters/retraining-free-quantization%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01543v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retraining-free%20Model%20Quantization%20via%20One-Shot%20Weight-Coupling%20Learning&entry.906535625=Chen%20Tang%20and%20Yuan%20Meng%20and%20Jiacheng%20Jiang%20and%20Shuzhao%20Xie%20and%20Rongwei%20Lu%20and%20Xinzhu%20Ma%20and%20Zhi%20Wang%20and%20Wenwu%20Zhu&entry.1292438233=%20%20Quantization%20is%20of%20significance%20for%20compressing%20the%20over-parameterized%20deep%0Aneural%20models%20and%20deploying%20them%20on%20resource-limited%20devices.%20Fixed-precision%0Aquantization%20suffers%20from%20performance%20drop%20due%20to%20the%20limited%20numerical%0Arepresentation%20ability.%20Conversely%2C%20mixed-precision%20quantization%20%28MPQ%29%20is%0Aadvocated%20to%20compress%20the%20model%20effectively%20by%20allocating%20heterogeneous%0Abit-width%20for%20layers.%20MPQ%20is%20typically%20organized%20into%20a%20searching-retraining%0Atwo-stage%20process.%20In%20this%20paper%2C%20we%20devise%20a%20one-shot%20training-searching%0Aparadigm%20for%20mixed-precision%20model%20compression.%20Specifically%2C%20in%20the%20first%0Astage%2C%20all%20potential%20bit-width%20configurations%20are%20coupled%20and%20thus%20optimized%0Asimultaneously%20within%20a%20set%20of%20shared%20weights.%20However%2C%20our%20observations%20reveal%0Aa%20previously%20unseen%20and%20severe%20bit-width%20interference%20phenomenon%20among%20highly%0Acoupled%20weights%20during%20optimization%2C%20leading%20to%20considerable%20performance%0Adegradation%20under%20a%20high%20compression%20ratio.%20To%20tackle%20this%20problem%2C%20we%20first%0Adesign%20a%20bit-width%20scheduler%20to%20dynamically%20freeze%20the%20most%20turbulent%20bit-width%0Aof%20layers%20during%20training%2C%20to%20ensure%20the%20rest%20bit-widths%20converged%20properly.%0AThen%2C%20taking%20inspiration%20from%20information%20theory%2C%20we%20present%20an%20information%0Adistortion%20mitigation%20technique%20to%20align%20the%20behavior%20of%20the%20bad-performing%0Abit-widths%20to%20the%20well-performing%20ones.%20In%20the%20second%20stage%2C%20an%20inference-only%0Agreedy%20search%20scheme%20is%20devised%20to%20evaluate%20the%20goodness%20of%20configurations%0Awithout%20introducing%20any%20additional%20training%20costs.%20Extensive%20experiments%20on%0Athree%20representative%20models%20and%20three%20datasets%20demonstrate%20the%20effectiveness%20of%0Athe%20proposed%20method.%20Code%20can%20be%20available%20on%0A%5Chref%7Bhttps%3A//www.github.com/1hunters/retraining-free-quantization%7D%7Bhttps%3A//github.com/1hunters/retraining-free-quantization%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01543v2&entry.124074799=Read"},
{"title": "Adaptive Robust Learning using Latent Bernoulli Variables", "author": "Aleksandr Karakulev and Dave Zachariah and Prashant Singh", "abstract": "  We present an adaptive approach for robust learning from corrupted training\nsets. We identify corrupted and non-corrupted samples with latent Bernoulli\nvariables and thus formulate the learning problem as maximization of the\nlikelihood where latent variables are marginalized. The resulting problem is\nsolved via variational inference, using an efficient Expectation-Maximization\nbased method. The proposed approach improves over the state-of-the-art by\nautomatically inferring the corruption level, while adding minimal\ncomputational overhead. We demonstrate our robust learning method and its\nparameter-free nature on a wide variety of machine learning tasks including\nonline learning and deep learning where it adapts to different levels of noise\nand maintains high prediction accuracy.\n", "link": "http://arxiv.org/abs/2312.00585v2", "date": "2024-06-14", "relevancy": 2.0546, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5173}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.514}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Robust%20Learning%20using%20Latent%20Bernoulli%20Variables&body=Title%3A%20Adaptive%20Robust%20Learning%20using%20Latent%20Bernoulli%20Variables%0AAuthor%3A%20Aleksandr%20Karakulev%20and%20Dave%20Zachariah%20and%20Prashant%20Singh%0AAbstract%3A%20%20%20We%20present%20an%20adaptive%20approach%20for%20robust%20learning%20from%20corrupted%20training%0Asets.%20We%20identify%20corrupted%20and%20non-corrupted%20samples%20with%20latent%20Bernoulli%0Avariables%20and%20thus%20formulate%20the%20learning%20problem%20as%20maximization%20of%20the%0Alikelihood%20where%20latent%20variables%20are%20marginalized.%20The%20resulting%20problem%20is%0Asolved%20via%20variational%20inference%2C%20using%20an%20efficient%20Expectation-Maximization%0Abased%20method.%20The%20proposed%20approach%20improves%20over%20the%20state-of-the-art%20by%0Aautomatically%20inferring%20the%20corruption%20level%2C%20while%20adding%20minimal%0Acomputational%20overhead.%20We%20demonstrate%20our%20robust%20learning%20method%20and%20its%0Aparameter-free%20nature%20on%20a%20wide%20variety%20of%20machine%20learning%20tasks%20including%0Aonline%20learning%20and%20deep%20learning%20where%20it%20adapts%20to%20different%20levels%20of%20noise%0Aand%20maintains%20high%20prediction%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00585v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Robust%2520Learning%2520using%2520Latent%2520Bernoulli%2520Variables%26entry.906535625%3DAleksandr%2520Karakulev%2520and%2520Dave%2520Zachariah%2520and%2520Prashant%2520Singh%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520adaptive%2520approach%2520for%2520robust%2520learning%2520from%2520corrupted%2520training%250Asets.%2520We%2520identify%2520corrupted%2520and%2520non-corrupted%2520samples%2520with%2520latent%2520Bernoulli%250Avariables%2520and%2520thus%2520formulate%2520the%2520learning%2520problem%2520as%2520maximization%2520of%2520the%250Alikelihood%2520where%2520latent%2520variables%2520are%2520marginalized.%2520The%2520resulting%2520problem%2520is%250Asolved%2520via%2520variational%2520inference%252C%2520using%2520an%2520efficient%2520Expectation-Maximization%250Abased%2520method.%2520The%2520proposed%2520approach%2520improves%2520over%2520the%2520state-of-the-art%2520by%250Aautomatically%2520inferring%2520the%2520corruption%2520level%252C%2520while%2520adding%2520minimal%250Acomputational%2520overhead.%2520We%2520demonstrate%2520our%2520robust%2520learning%2520method%2520and%2520its%250Aparameter-free%2520nature%2520on%2520a%2520wide%2520variety%2520of%2520machine%2520learning%2520tasks%2520including%250Aonline%2520learning%2520and%2520deep%2520learning%2520where%2520it%2520adapts%2520to%2520different%2520levels%2520of%2520noise%250Aand%2520maintains%2520high%2520prediction%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00585v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Robust%20Learning%20using%20Latent%20Bernoulli%20Variables&entry.906535625=Aleksandr%20Karakulev%20and%20Dave%20Zachariah%20and%20Prashant%20Singh&entry.1292438233=%20%20We%20present%20an%20adaptive%20approach%20for%20robust%20learning%20from%20corrupted%20training%0Asets.%20We%20identify%20corrupted%20and%20non-corrupted%20samples%20with%20latent%20Bernoulli%0Avariables%20and%20thus%20formulate%20the%20learning%20problem%20as%20maximization%20of%20the%0Alikelihood%20where%20latent%20variables%20are%20marginalized.%20The%20resulting%20problem%20is%0Asolved%20via%20variational%20inference%2C%20using%20an%20efficient%20Expectation-Maximization%0Abased%20method.%20The%20proposed%20approach%20improves%20over%20the%20state-of-the-art%20by%0Aautomatically%20inferring%20the%20corruption%20level%2C%20while%20adding%20minimal%0Acomputational%20overhead.%20We%20demonstrate%20our%20robust%20learning%20method%20and%20its%0Aparameter-free%20nature%20on%20a%20wide%20variety%20of%20machine%20learning%20tasks%20including%0Aonline%20learning%20and%20deep%20learning%20where%20it%20adapts%20to%20different%20levels%20of%20noise%0Aand%20maintains%20high%20prediction%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00585v2&entry.124074799=Read"},
{"title": "Heuristic Learning with Graph Neural Networks: A Unified Framework for\n  Link Prediction", "author": "Juzheng Zhang and Lanning Wei and Zhen Xu and Quanming Yao", "abstract": "  Link prediction is a fundamental task in graph learning, inherently shaped by\nthe topology of the graph. While traditional heuristics are grounded in graph\ntopology, they encounter challenges in generalizing across diverse graphs.\nRecent research efforts have aimed to leverage the potential of heuristics, yet\na unified formulation accommodating both local and global heuristics remains\nundiscovered. Drawing insights from the fact that both local and global\nheuristics can be represented by adjacency matrix multiplications, we propose a\nunified matrix formulation to accommodate and generalize various heuristics. We\nfurther propose the Heuristic Learning Graph Neural Network (HL-GNN) to\nefficiently implement the formulation. HL-GNN adopts intra-layer propagation\nand inter-layer connections, allowing it to reach a depth of around 20 layers\nwith lower time complexity than GCN. Extensive experiments on the Planetoid,\nAmazon, and OGB datasets underscore the effectiveness and efficiency of HL-GNN.\nIt outperforms existing methods by a large margin in prediction performance.\nAdditionally, HL-GNN is several orders of magnitude faster than\nheuristic-inspired methods while requiring only a few trainable parameters. The\ncase study further demonstrates that the generalized heuristics and learned\nweights are highly interpretable.\n", "link": "http://arxiv.org/abs/2406.07979v2", "date": "2024-06-14", "relevancy": 2.0467, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5283}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5012}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heuristic%20Learning%20with%20Graph%20Neural%20Networks%3A%20A%20Unified%20Framework%20for%0A%20%20Link%20Prediction&body=Title%3A%20Heuristic%20Learning%20with%20Graph%20Neural%20Networks%3A%20A%20Unified%20Framework%20for%0A%20%20Link%20Prediction%0AAuthor%3A%20Juzheng%20Zhang%20and%20Lanning%20Wei%20and%20Zhen%20Xu%20and%20Quanming%20Yao%0AAbstract%3A%20%20%20Link%20prediction%20is%20a%20fundamental%20task%20in%20graph%20learning%2C%20inherently%20shaped%20by%0Athe%20topology%20of%20the%20graph.%20While%20traditional%20heuristics%20are%20grounded%20in%20graph%0Atopology%2C%20they%20encounter%20challenges%20in%20generalizing%20across%20diverse%20graphs.%0ARecent%20research%20efforts%20have%20aimed%20to%20leverage%20the%20potential%20of%20heuristics%2C%20yet%0Aa%20unified%20formulation%20accommodating%20both%20local%20and%20global%20heuristics%20remains%0Aundiscovered.%20Drawing%20insights%20from%20the%20fact%20that%20both%20local%20and%20global%0Aheuristics%20can%20be%20represented%20by%20adjacency%20matrix%20multiplications%2C%20we%20propose%20a%0Aunified%20matrix%20formulation%20to%20accommodate%20and%20generalize%20various%20heuristics.%20We%0Afurther%20propose%20the%20Heuristic%20Learning%20Graph%20Neural%20Network%20%28HL-GNN%29%20to%0Aefficiently%20implement%20the%20formulation.%20HL-GNN%20adopts%20intra-layer%20propagation%0Aand%20inter-layer%20connections%2C%20allowing%20it%20to%20reach%20a%20depth%20of%20around%2020%20layers%0Awith%20lower%20time%20complexity%20than%20GCN.%20Extensive%20experiments%20on%20the%20Planetoid%2C%0AAmazon%2C%20and%20OGB%20datasets%20underscore%20the%20effectiveness%20and%20efficiency%20of%20HL-GNN.%0AIt%20outperforms%20existing%20methods%20by%20a%20large%20margin%20in%20prediction%20performance.%0AAdditionally%2C%20HL-GNN%20is%20several%20orders%20of%20magnitude%20faster%20than%0Aheuristic-inspired%20methods%20while%20requiring%20only%20a%20few%20trainable%20parameters.%20The%0Acase%20study%20further%20demonstrates%20that%20the%20generalized%20heuristics%20and%20learned%0Aweights%20are%20highly%20interpretable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeuristic%2520Learning%2520with%2520Graph%2520Neural%2520Networks%253A%2520A%2520Unified%2520Framework%2520for%250A%2520%2520Link%2520Prediction%26entry.906535625%3DJuzheng%2520Zhang%2520and%2520Lanning%2520Wei%2520and%2520Zhen%2520Xu%2520and%2520Quanming%2520Yao%26entry.1292438233%3D%2520%2520Link%2520prediction%2520is%2520a%2520fundamental%2520task%2520in%2520graph%2520learning%252C%2520inherently%2520shaped%2520by%250Athe%2520topology%2520of%2520the%2520graph.%2520While%2520traditional%2520heuristics%2520are%2520grounded%2520in%2520graph%250Atopology%252C%2520they%2520encounter%2520challenges%2520in%2520generalizing%2520across%2520diverse%2520graphs.%250ARecent%2520research%2520efforts%2520have%2520aimed%2520to%2520leverage%2520the%2520potential%2520of%2520heuristics%252C%2520yet%250Aa%2520unified%2520formulation%2520accommodating%2520both%2520local%2520and%2520global%2520heuristics%2520remains%250Aundiscovered.%2520Drawing%2520insights%2520from%2520the%2520fact%2520that%2520both%2520local%2520and%2520global%250Aheuristics%2520can%2520be%2520represented%2520by%2520adjacency%2520matrix%2520multiplications%252C%2520we%2520propose%2520a%250Aunified%2520matrix%2520formulation%2520to%2520accommodate%2520and%2520generalize%2520various%2520heuristics.%2520We%250Afurther%2520propose%2520the%2520Heuristic%2520Learning%2520Graph%2520Neural%2520Network%2520%2528HL-GNN%2529%2520to%250Aefficiently%2520implement%2520the%2520formulation.%2520HL-GNN%2520adopts%2520intra-layer%2520propagation%250Aand%2520inter-layer%2520connections%252C%2520allowing%2520it%2520to%2520reach%2520a%2520depth%2520of%2520around%252020%2520layers%250Awith%2520lower%2520time%2520complexity%2520than%2520GCN.%2520Extensive%2520experiments%2520on%2520the%2520Planetoid%252C%250AAmazon%252C%2520and%2520OGB%2520datasets%2520underscore%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520HL-GNN.%250AIt%2520outperforms%2520existing%2520methods%2520by%2520a%2520large%2520margin%2520in%2520prediction%2520performance.%250AAdditionally%252C%2520HL-GNN%2520is%2520several%2520orders%2520of%2520magnitude%2520faster%2520than%250Aheuristic-inspired%2520methods%2520while%2520requiring%2520only%2520a%2520few%2520trainable%2520parameters.%2520The%250Acase%2520study%2520further%2520demonstrates%2520that%2520the%2520generalized%2520heuristics%2520and%2520learned%250Aweights%2520are%2520highly%2520interpretable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heuristic%20Learning%20with%20Graph%20Neural%20Networks%3A%20A%20Unified%20Framework%20for%0A%20%20Link%20Prediction&entry.906535625=Juzheng%20Zhang%20and%20Lanning%20Wei%20and%20Zhen%20Xu%20and%20Quanming%20Yao&entry.1292438233=%20%20Link%20prediction%20is%20a%20fundamental%20task%20in%20graph%20learning%2C%20inherently%20shaped%20by%0Athe%20topology%20of%20the%20graph.%20While%20traditional%20heuristics%20are%20grounded%20in%20graph%0Atopology%2C%20they%20encounter%20challenges%20in%20generalizing%20across%20diverse%20graphs.%0ARecent%20research%20efforts%20have%20aimed%20to%20leverage%20the%20potential%20of%20heuristics%2C%20yet%0Aa%20unified%20formulation%20accommodating%20both%20local%20and%20global%20heuristics%20remains%0Aundiscovered.%20Drawing%20insights%20from%20the%20fact%20that%20both%20local%20and%20global%0Aheuristics%20can%20be%20represented%20by%20adjacency%20matrix%20multiplications%2C%20we%20propose%20a%0Aunified%20matrix%20formulation%20to%20accommodate%20and%20generalize%20various%20heuristics.%20We%0Afurther%20propose%20the%20Heuristic%20Learning%20Graph%20Neural%20Network%20%28HL-GNN%29%20to%0Aefficiently%20implement%20the%20formulation.%20HL-GNN%20adopts%20intra-layer%20propagation%0Aand%20inter-layer%20connections%2C%20allowing%20it%20to%20reach%20a%20depth%20of%20around%2020%20layers%0Awith%20lower%20time%20complexity%20than%20GCN.%20Extensive%20experiments%20on%20the%20Planetoid%2C%0AAmazon%2C%20and%20OGB%20datasets%20underscore%20the%20effectiveness%20and%20efficiency%20of%20HL-GNN.%0AIt%20outperforms%20existing%20methods%20by%20a%20large%20margin%20in%20prediction%20performance.%0AAdditionally%2C%20HL-GNN%20is%20several%20orders%20of%20magnitude%20faster%20than%0Aheuristic-inspired%20methods%20while%20requiring%20only%20a%20few%20trainable%20parameters.%20The%0Acase%20study%20further%20demonstrates%20that%20the%20generalized%20heuristics%20and%20learned%0Aweights%20are%20highly%20interpretable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07979v2&entry.124074799=Read"},
{"title": "SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for\n  Remote Sensing Vision-Language Understanding", "author": "Junwei Luo and Zhen Pang and Yongjun Zhang and Tingzhu Wang and Linlin Wang and Bo Dang and Jiangwei Lao and Jian Wang and Jingdong Chen and Yihua Tan and Yansheng Li", "abstract": "  Remote Sensing Large Multi-Modal Models (RSLMMs) are developing rapidly and\nshowcase significant capabilities in remote sensing imagery (RSI)\ncomprehension. However, due to the limitations of existing datasets, RSLMMs\nhave shortcomings in understanding the rich semantic relations among objects in\ncomplex remote sensing scenes. To unlock RSLMMs' complex comprehension ability,\nwe propose a large-scale instruction tuning dataset FIT-RS, containing\n1,800,851 instruction samples. FIT-RS covers common interpretation tasks and\ninnovatively introduces several complex comprehension tasks of escalating\ndifficulty, ranging from relation reasoning to image-level scene graph\ngeneration. Based on FIT-RS, we build the FIT-RSFG benchmark. Furthermore, we\nestablish a new benchmark to evaluate the fine-grained relation comprehension\ncapabilities of LMMs, named FIT-RSRC. Based on combined instruction data, we\npropose SkySenseGPT, which achieves outstanding performance on both public\ndatasets and FIT-RSFG, surpassing existing RSLMMs. We hope the FIT-RS dataset\ncan enhance the relation comprehension capability of RSLMMs and provide a\nlarge-scale fine-grained data source for the remote sensing community. The\ndataset will be available at https://github.com/Luo-Z13/SkySenseGPT\n", "link": "http://arxiv.org/abs/2406.10100v1", "date": "2024-06-14", "relevancy": 2.0456, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5323}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5099}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SkySenseGPT%3A%20A%20Fine-Grained%20Instruction%20Tuning%20Dataset%20and%20Model%20for%0A%20%20Remote%20Sensing%20Vision-Language%20Understanding&body=Title%3A%20SkySenseGPT%3A%20A%20Fine-Grained%20Instruction%20Tuning%20Dataset%20and%20Model%20for%0A%20%20Remote%20Sensing%20Vision-Language%20Understanding%0AAuthor%3A%20Junwei%20Luo%20and%20Zhen%20Pang%20and%20Yongjun%20Zhang%20and%20Tingzhu%20Wang%20and%20Linlin%20Wang%20and%20Bo%20Dang%20and%20Jiangwei%20Lao%20and%20Jian%20Wang%20and%20Jingdong%20Chen%20and%20Yihua%20Tan%20and%20Yansheng%20Li%0AAbstract%3A%20%20%20Remote%20Sensing%20Large%20Multi-Modal%20Models%20%28RSLMMs%29%20are%20developing%20rapidly%20and%0Ashowcase%20significant%20capabilities%20in%20remote%20sensing%20imagery%20%28RSI%29%0Acomprehension.%20However%2C%20due%20to%20the%20limitations%20of%20existing%20datasets%2C%20RSLMMs%0Ahave%20shortcomings%20in%20understanding%20the%20rich%20semantic%20relations%20among%20objects%20in%0Acomplex%20remote%20sensing%20scenes.%20To%20unlock%20RSLMMs%27%20complex%20comprehension%20ability%2C%0Awe%20propose%20a%20large-scale%20instruction%20tuning%20dataset%20FIT-RS%2C%20containing%0A1%2C800%2C851%20instruction%20samples.%20FIT-RS%20covers%20common%20interpretation%20tasks%20and%0Ainnovatively%20introduces%20several%20complex%20comprehension%20tasks%20of%20escalating%0Adifficulty%2C%20ranging%20from%20relation%20reasoning%20to%20image-level%20scene%20graph%0Ageneration.%20Based%20on%20FIT-RS%2C%20we%20build%20the%20FIT-RSFG%20benchmark.%20Furthermore%2C%20we%0Aestablish%20a%20new%20benchmark%20to%20evaluate%20the%20fine-grained%20relation%20comprehension%0Acapabilities%20of%20LMMs%2C%20named%20FIT-RSRC.%20Based%20on%20combined%20instruction%20data%2C%20we%0Apropose%20SkySenseGPT%2C%20which%20achieves%20outstanding%20performance%20on%20both%20public%0Adatasets%20and%20FIT-RSFG%2C%20surpassing%20existing%20RSLMMs.%20We%20hope%20the%20FIT-RS%20dataset%0Acan%20enhance%20the%20relation%20comprehension%20capability%20of%20RSLMMs%20and%20provide%20a%0Alarge-scale%20fine-grained%20data%20source%20for%20the%20remote%20sensing%20community.%20The%0Adataset%20will%20be%20available%20at%20https%3A//github.com/Luo-Z13/SkySenseGPT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkySenseGPT%253A%2520A%2520Fine-Grained%2520Instruction%2520Tuning%2520Dataset%2520and%2520Model%2520for%250A%2520%2520Remote%2520Sensing%2520Vision-Language%2520Understanding%26entry.906535625%3DJunwei%2520Luo%2520and%2520Zhen%2520Pang%2520and%2520Yongjun%2520Zhang%2520and%2520Tingzhu%2520Wang%2520and%2520Linlin%2520Wang%2520and%2520Bo%2520Dang%2520and%2520Jiangwei%2520Lao%2520and%2520Jian%2520Wang%2520and%2520Jingdong%2520Chen%2520and%2520Yihua%2520Tan%2520and%2520Yansheng%2520Li%26entry.1292438233%3D%2520%2520Remote%2520Sensing%2520Large%2520Multi-Modal%2520Models%2520%2528RSLMMs%2529%2520are%2520developing%2520rapidly%2520and%250Ashowcase%2520significant%2520capabilities%2520in%2520remote%2520sensing%2520imagery%2520%2528RSI%2529%250Acomprehension.%2520However%252C%2520due%2520to%2520the%2520limitations%2520of%2520existing%2520datasets%252C%2520RSLMMs%250Ahave%2520shortcomings%2520in%2520understanding%2520the%2520rich%2520semantic%2520relations%2520among%2520objects%2520in%250Acomplex%2520remote%2520sensing%2520scenes.%2520To%2520unlock%2520RSLMMs%2527%2520complex%2520comprehension%2520ability%252C%250Awe%2520propose%2520a%2520large-scale%2520instruction%2520tuning%2520dataset%2520FIT-RS%252C%2520containing%250A1%252C800%252C851%2520instruction%2520samples.%2520FIT-RS%2520covers%2520common%2520interpretation%2520tasks%2520and%250Ainnovatively%2520introduces%2520several%2520complex%2520comprehension%2520tasks%2520of%2520escalating%250Adifficulty%252C%2520ranging%2520from%2520relation%2520reasoning%2520to%2520image-level%2520scene%2520graph%250Ageneration.%2520Based%2520on%2520FIT-RS%252C%2520we%2520build%2520the%2520FIT-RSFG%2520benchmark.%2520Furthermore%252C%2520we%250Aestablish%2520a%2520new%2520benchmark%2520to%2520evaluate%2520the%2520fine-grained%2520relation%2520comprehension%250Acapabilities%2520of%2520LMMs%252C%2520named%2520FIT-RSRC.%2520Based%2520on%2520combined%2520instruction%2520data%252C%2520we%250Apropose%2520SkySenseGPT%252C%2520which%2520achieves%2520outstanding%2520performance%2520on%2520both%2520public%250Adatasets%2520and%2520FIT-RSFG%252C%2520surpassing%2520existing%2520RSLMMs.%2520We%2520hope%2520the%2520FIT-RS%2520dataset%250Acan%2520enhance%2520the%2520relation%2520comprehension%2520capability%2520of%2520RSLMMs%2520and%2520provide%2520a%250Alarge-scale%2520fine-grained%2520data%2520source%2520for%2520the%2520remote%2520sensing%2520community.%2520The%250Adataset%2520will%2520be%2520available%2520at%2520https%253A//github.com/Luo-Z13/SkySenseGPT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkySenseGPT%3A%20A%20Fine-Grained%20Instruction%20Tuning%20Dataset%20and%20Model%20for%0A%20%20Remote%20Sensing%20Vision-Language%20Understanding&entry.906535625=Junwei%20Luo%20and%20Zhen%20Pang%20and%20Yongjun%20Zhang%20and%20Tingzhu%20Wang%20and%20Linlin%20Wang%20and%20Bo%20Dang%20and%20Jiangwei%20Lao%20and%20Jian%20Wang%20and%20Jingdong%20Chen%20and%20Yihua%20Tan%20and%20Yansheng%20Li&entry.1292438233=%20%20Remote%20Sensing%20Large%20Multi-Modal%20Models%20%28RSLMMs%29%20are%20developing%20rapidly%20and%0Ashowcase%20significant%20capabilities%20in%20remote%20sensing%20imagery%20%28RSI%29%0Acomprehension.%20However%2C%20due%20to%20the%20limitations%20of%20existing%20datasets%2C%20RSLMMs%0Ahave%20shortcomings%20in%20understanding%20the%20rich%20semantic%20relations%20among%20objects%20in%0Acomplex%20remote%20sensing%20scenes.%20To%20unlock%20RSLMMs%27%20complex%20comprehension%20ability%2C%0Awe%20propose%20a%20large-scale%20instruction%20tuning%20dataset%20FIT-RS%2C%20containing%0A1%2C800%2C851%20instruction%20samples.%20FIT-RS%20covers%20common%20interpretation%20tasks%20and%0Ainnovatively%20introduces%20several%20complex%20comprehension%20tasks%20of%20escalating%0Adifficulty%2C%20ranging%20from%20relation%20reasoning%20to%20image-level%20scene%20graph%0Ageneration.%20Based%20on%20FIT-RS%2C%20we%20build%20the%20FIT-RSFG%20benchmark.%20Furthermore%2C%20we%0Aestablish%20a%20new%20benchmark%20to%20evaluate%20the%20fine-grained%20relation%20comprehension%0Acapabilities%20of%20LMMs%2C%20named%20FIT-RSRC.%20Based%20on%20combined%20instruction%20data%2C%20we%0Apropose%20SkySenseGPT%2C%20which%20achieves%20outstanding%20performance%20on%20both%20public%0Adatasets%20and%20FIT-RSFG%2C%20surpassing%20existing%20RSLMMs.%20We%20hope%20the%20FIT-RS%20dataset%0Acan%20enhance%20the%20relation%20comprehension%20capability%20of%20RSLMMs%20and%20provide%20a%0Alarge-scale%20fine-grained%20data%20source%20for%20the%20remote%20sensing%20community.%20The%0Adataset%20will%20be%20available%20at%20https%3A//github.com/Luo-Z13/SkySenseGPT%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10100v1&entry.124074799=Read"},
{"title": "Practical offloading for fine-tuning LLM on commodity GPU via learned\n  subspace projectors", "author": "Siyuan Chen and Zelong Guan and Yudong Liu and Phillip B. Gibbons", "abstract": "  Fine-tuning large language models (LLMs) requires significant memory, often\nexceeding the capacity of a single GPU. A common solution to this memory\nchallenge is offloading compute and data from the GPU to the CPU. However, this\napproach is hampered by the limited bandwidth of commodity hardware, which\nconstrains communication between the CPU and GPU.\n  In this paper, we present an offloading framework, LSP_Offload, that enables\nnear-native speed LLM fine-tuning on commodity hardware through learned\nsubspace projectors. Our data-driven approach involves learning an efficient\nsparse compressor that minimizes communication with minimal precision loss.\nAdditionally, we introduce a novel layer-wise communication schedule to\nmaximize parallelism between communication and computation. As a result, our\nframework can fine-tune a 1.3 billion parameter model on a 4GB laptop GPU and a\n7 billion parameter model on an NVIDIA RTX 4090 GPU with 24GB memory, achieving\nonly a 31% slowdown compared to fine-tuning with unlimited memory. Compared to\nstate-of-the-art offloading frameworks, our approach increases fine-tuning\nthroughput by up to 3.33 times and reduces end-to-end fine-tuning time by\n33.1%~62.5% when converging to the same accuracy.\n", "link": "http://arxiv.org/abs/2406.10181v1", "date": "2024-06-14", "relevancy": 2.0407, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5216}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5141}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Practical%20offloading%20for%20fine-tuning%20LLM%20on%20commodity%20GPU%20via%20learned%0A%20%20subspace%20projectors&body=Title%3A%20Practical%20offloading%20for%20fine-tuning%20LLM%20on%20commodity%20GPU%20via%20learned%0A%20%20subspace%20projectors%0AAuthor%3A%20Siyuan%20Chen%20and%20Zelong%20Guan%20and%20Yudong%20Liu%20and%20Phillip%20B.%20Gibbons%0AAbstract%3A%20%20%20Fine-tuning%20large%20language%20models%20%28LLMs%29%20requires%20significant%20memory%2C%20often%0Aexceeding%20the%20capacity%20of%20a%20single%20GPU.%20A%20common%20solution%20to%20this%20memory%0Achallenge%20is%20offloading%20compute%20and%20data%20from%20the%20GPU%20to%20the%20CPU.%20However%2C%20this%0Aapproach%20is%20hampered%20by%20the%20limited%20bandwidth%20of%20commodity%20hardware%2C%20which%0Aconstrains%20communication%20between%20the%20CPU%20and%20GPU.%0A%20%20In%20this%20paper%2C%20we%20present%20an%20offloading%20framework%2C%20LSP_Offload%2C%20that%20enables%0Anear-native%20speed%20LLM%20fine-tuning%20on%20commodity%20hardware%20through%20learned%0Asubspace%20projectors.%20Our%20data-driven%20approach%20involves%20learning%20an%20efficient%0Asparse%20compressor%20that%20minimizes%20communication%20with%20minimal%20precision%20loss.%0AAdditionally%2C%20we%20introduce%20a%20novel%20layer-wise%20communication%20schedule%20to%0Amaximize%20parallelism%20between%20communication%20and%20computation.%20As%20a%20result%2C%20our%0Aframework%20can%20fine-tune%20a%201.3%20billion%20parameter%20model%20on%20a%204GB%20laptop%20GPU%20and%20a%0A7%20billion%20parameter%20model%20on%20an%20NVIDIA%20RTX%204090%20GPU%20with%2024GB%20memory%2C%20achieving%0Aonly%20a%2031%25%20slowdown%20compared%20to%20fine-tuning%20with%20unlimited%20memory.%20Compared%20to%0Astate-of-the-art%20offloading%20frameworks%2C%20our%20approach%20increases%20fine-tuning%0Athroughput%20by%20up%20to%203.33%20times%20and%20reduces%20end-to-end%20fine-tuning%20time%20by%0A33.1%25~62.5%25%20when%20converging%20to%20the%20same%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPractical%2520offloading%2520for%2520fine-tuning%2520LLM%2520on%2520commodity%2520GPU%2520via%2520learned%250A%2520%2520subspace%2520projectors%26entry.906535625%3DSiyuan%2520Chen%2520and%2520Zelong%2520Guan%2520and%2520Yudong%2520Liu%2520and%2520Phillip%2520B.%2520Gibbons%26entry.1292438233%3D%2520%2520Fine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520requires%2520significant%2520memory%252C%2520often%250Aexceeding%2520the%2520capacity%2520of%2520a%2520single%2520GPU.%2520A%2520common%2520solution%2520to%2520this%2520memory%250Achallenge%2520is%2520offloading%2520compute%2520and%2520data%2520from%2520the%2520GPU%2520to%2520the%2520CPU.%2520However%252C%2520this%250Aapproach%2520is%2520hampered%2520by%2520the%2520limited%2520bandwidth%2520of%2520commodity%2520hardware%252C%2520which%250Aconstrains%2520communication%2520between%2520the%2520CPU%2520and%2520GPU.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520offloading%2520framework%252C%2520LSP_Offload%252C%2520that%2520enables%250Anear-native%2520speed%2520LLM%2520fine-tuning%2520on%2520commodity%2520hardware%2520through%2520learned%250Asubspace%2520projectors.%2520Our%2520data-driven%2520approach%2520involves%2520learning%2520an%2520efficient%250Asparse%2520compressor%2520that%2520minimizes%2520communication%2520with%2520minimal%2520precision%2520loss.%250AAdditionally%252C%2520we%2520introduce%2520a%2520novel%2520layer-wise%2520communication%2520schedule%2520to%250Amaximize%2520parallelism%2520between%2520communication%2520and%2520computation.%2520As%2520a%2520result%252C%2520our%250Aframework%2520can%2520fine-tune%2520a%25201.3%2520billion%2520parameter%2520model%2520on%2520a%25204GB%2520laptop%2520GPU%2520and%2520a%250A7%2520billion%2520parameter%2520model%2520on%2520an%2520NVIDIA%2520RTX%25204090%2520GPU%2520with%252024GB%2520memory%252C%2520achieving%250Aonly%2520a%252031%2525%2520slowdown%2520compared%2520to%2520fine-tuning%2520with%2520unlimited%2520memory.%2520Compared%2520to%250Astate-of-the-art%2520offloading%2520frameworks%252C%2520our%2520approach%2520increases%2520fine-tuning%250Athroughput%2520by%2520up%2520to%25203.33%2520times%2520and%2520reduces%2520end-to-end%2520fine-tuning%2520time%2520by%250A33.1%2525~62.5%2525%2520when%2520converging%2520to%2520the%2520same%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Practical%20offloading%20for%20fine-tuning%20LLM%20on%20commodity%20GPU%20via%20learned%0A%20%20subspace%20projectors&entry.906535625=Siyuan%20Chen%20and%20Zelong%20Guan%20and%20Yudong%20Liu%20and%20Phillip%20B.%20Gibbons&entry.1292438233=%20%20Fine-tuning%20large%20language%20models%20%28LLMs%29%20requires%20significant%20memory%2C%20often%0Aexceeding%20the%20capacity%20of%20a%20single%20GPU.%20A%20common%20solution%20to%20this%20memory%0Achallenge%20is%20offloading%20compute%20and%20data%20from%20the%20GPU%20to%20the%20CPU.%20However%2C%20this%0Aapproach%20is%20hampered%20by%20the%20limited%20bandwidth%20of%20commodity%20hardware%2C%20which%0Aconstrains%20communication%20between%20the%20CPU%20and%20GPU.%0A%20%20In%20this%20paper%2C%20we%20present%20an%20offloading%20framework%2C%20LSP_Offload%2C%20that%20enables%0Anear-native%20speed%20LLM%20fine-tuning%20on%20commodity%20hardware%20through%20learned%0Asubspace%20projectors.%20Our%20data-driven%20approach%20involves%20learning%20an%20efficient%0Asparse%20compressor%20that%20minimizes%20communication%20with%20minimal%20precision%20loss.%0AAdditionally%2C%20we%20introduce%20a%20novel%20layer-wise%20communication%20schedule%20to%0Amaximize%20parallelism%20between%20communication%20and%20computation.%20As%20a%20result%2C%20our%0Aframework%20can%20fine-tune%20a%201.3%20billion%20parameter%20model%20on%20a%204GB%20laptop%20GPU%20and%20a%0A7%20billion%20parameter%20model%20on%20an%20NVIDIA%20RTX%204090%20GPU%20with%2024GB%20memory%2C%20achieving%0Aonly%20a%2031%25%20slowdown%20compared%20to%20fine-tuning%20with%20unlimited%20memory.%20Compared%20to%0Astate-of-the-art%20offloading%20frameworks%2C%20our%20approach%20increases%20fine-tuning%0Athroughput%20by%20up%20to%203.33%20times%20and%20reduces%20end-to-end%20fine-tuning%20time%20by%0A33.1%25~62.5%25%20when%20converging%20to%20the%20same%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10181v1&entry.124074799=Read"},
{"title": "Vulnerable Road User Detection and Safety Enhancement: A Comprehensive\n  Survey", "author": "Renato M. Silva and Greg\u00f3rio F. Azevedo and Matheus V. V. Berto and Jean R. Rocha and Eduardo C. Fidelis and Matheus V. Nogueira and Pedro H. Lisboa and Tiago A. Almeida", "abstract": "  Traffic incidents involving vulnerable road users (VRUs) constitute a\nsignificant proportion of global road accidents. Advances in traffic\ncommunication ecosystems, coupled with sophisticated signal processing and\nmachine learning techniques, have facilitated the utilization of data from\ndiverse sensors. Despite these advancements and the availability of extensive\ndatasets, substantial progress is required to mitigate traffic casualties. This\npaper provides a comprehensive survey of state-of-the-art technologies and\nmethodologies to enhance the safety of VRUs. The study delves into the\ncommunication networks between vehicles and VRUs, emphasizing the integration\nof advanced sensors and the availability of relevant datasets. It explores\npreprocessing techniques and data fusion methods to enhance sensor data\nquality. Furthermore, our study assesses critical simulation environments\nessential for developing and testing VRU safety systems. Our research also\nhighlights recent advances in VRU detection and classification algorithms,\naddressing challenges such as variable environmental conditions. Additionally,\nwe cover cutting-edge research in predicting VRU intentions and behaviors,\nwhich is crucial for proactive collision avoidance strategies. Through this\nsurvey, we aim to provide a comprehensive understanding of the current\nlandscape of VRU safety technologies, identifying areas of progress and areas\nneeding further research and development.\n", "link": "http://arxiv.org/abs/2405.19202v3", "date": "2024-06-14", "relevancy": 2.0165, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5174}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4965}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vulnerable%20Road%20User%20Detection%20and%20Safety%20Enhancement%3A%20A%20Comprehensive%0A%20%20Survey&body=Title%3A%20Vulnerable%20Road%20User%20Detection%20and%20Safety%20Enhancement%3A%20A%20Comprehensive%0A%20%20Survey%0AAuthor%3A%20Renato%20M.%20Silva%20and%20Greg%C3%B3rio%20F.%20Azevedo%20and%20Matheus%20V.%20V.%20Berto%20and%20Jean%20R.%20Rocha%20and%20Eduardo%20C.%20Fidelis%20and%20Matheus%20V.%20Nogueira%20and%20Pedro%20H.%20Lisboa%20and%20Tiago%20A.%20Almeida%0AAbstract%3A%20%20%20Traffic%20incidents%20involving%20vulnerable%20road%20users%20%28VRUs%29%20constitute%20a%0Asignificant%20proportion%20of%20global%20road%20accidents.%20Advances%20in%20traffic%0Acommunication%20ecosystems%2C%20coupled%20with%20sophisticated%20signal%20processing%20and%0Amachine%20learning%20techniques%2C%20have%20facilitated%20the%20utilization%20of%20data%20from%0Adiverse%20sensors.%20Despite%20these%20advancements%20and%20the%20availability%20of%20extensive%0Adatasets%2C%20substantial%20progress%20is%20required%20to%20mitigate%20traffic%20casualties.%20This%0Apaper%20provides%20a%20comprehensive%20survey%20of%20state-of-the-art%20technologies%20and%0Amethodologies%20to%20enhance%20the%20safety%20of%20VRUs.%20The%20study%20delves%20into%20the%0Acommunication%20networks%20between%20vehicles%20and%20VRUs%2C%20emphasizing%20the%20integration%0Aof%20advanced%20sensors%20and%20the%20availability%20of%20relevant%20datasets.%20It%20explores%0Apreprocessing%20techniques%20and%20data%20fusion%20methods%20to%20enhance%20sensor%20data%0Aquality.%20Furthermore%2C%20our%20study%20assesses%20critical%20simulation%20environments%0Aessential%20for%20developing%20and%20testing%20VRU%20safety%20systems.%20Our%20research%20also%0Ahighlights%20recent%20advances%20in%20VRU%20detection%20and%20classification%20algorithms%2C%0Aaddressing%20challenges%20such%20as%20variable%20environmental%20conditions.%20Additionally%2C%0Awe%20cover%20cutting-edge%20research%20in%20predicting%20VRU%20intentions%20and%20behaviors%2C%0Awhich%20is%20crucial%20for%20proactive%20collision%20avoidance%20strategies.%20Through%20this%0Asurvey%2C%20we%20aim%20to%20provide%20a%20comprehensive%20understanding%20of%20the%20current%0Alandscape%20of%20VRU%20safety%20technologies%2C%20identifying%20areas%20of%20progress%20and%20areas%0Aneeding%20further%20research%20and%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19202v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVulnerable%2520Road%2520User%2520Detection%2520and%2520Safety%2520Enhancement%253A%2520A%2520Comprehensive%250A%2520%2520Survey%26entry.906535625%3DRenato%2520M.%2520Silva%2520and%2520Greg%25C3%25B3rio%2520F.%2520Azevedo%2520and%2520Matheus%2520V.%2520V.%2520Berto%2520and%2520Jean%2520R.%2520Rocha%2520and%2520Eduardo%2520C.%2520Fidelis%2520and%2520Matheus%2520V.%2520Nogueira%2520and%2520Pedro%2520H.%2520Lisboa%2520and%2520Tiago%2520A.%2520Almeida%26entry.1292438233%3D%2520%2520Traffic%2520incidents%2520involving%2520vulnerable%2520road%2520users%2520%2528VRUs%2529%2520constitute%2520a%250Asignificant%2520proportion%2520of%2520global%2520road%2520accidents.%2520Advances%2520in%2520traffic%250Acommunication%2520ecosystems%252C%2520coupled%2520with%2520sophisticated%2520signal%2520processing%2520and%250Amachine%2520learning%2520techniques%252C%2520have%2520facilitated%2520the%2520utilization%2520of%2520data%2520from%250Adiverse%2520sensors.%2520Despite%2520these%2520advancements%2520and%2520the%2520availability%2520of%2520extensive%250Adatasets%252C%2520substantial%2520progress%2520is%2520required%2520to%2520mitigate%2520traffic%2520casualties.%2520This%250Apaper%2520provides%2520a%2520comprehensive%2520survey%2520of%2520state-of-the-art%2520technologies%2520and%250Amethodologies%2520to%2520enhance%2520the%2520safety%2520of%2520VRUs.%2520The%2520study%2520delves%2520into%2520the%250Acommunication%2520networks%2520between%2520vehicles%2520and%2520VRUs%252C%2520emphasizing%2520the%2520integration%250Aof%2520advanced%2520sensors%2520and%2520the%2520availability%2520of%2520relevant%2520datasets.%2520It%2520explores%250Apreprocessing%2520techniques%2520and%2520data%2520fusion%2520methods%2520to%2520enhance%2520sensor%2520data%250Aquality.%2520Furthermore%252C%2520our%2520study%2520assesses%2520critical%2520simulation%2520environments%250Aessential%2520for%2520developing%2520and%2520testing%2520VRU%2520safety%2520systems.%2520Our%2520research%2520also%250Ahighlights%2520recent%2520advances%2520in%2520VRU%2520detection%2520and%2520classification%2520algorithms%252C%250Aaddressing%2520challenges%2520such%2520as%2520variable%2520environmental%2520conditions.%2520Additionally%252C%250Awe%2520cover%2520cutting-edge%2520research%2520in%2520predicting%2520VRU%2520intentions%2520and%2520behaviors%252C%250Awhich%2520is%2520crucial%2520for%2520proactive%2520collision%2520avoidance%2520strategies.%2520Through%2520this%250Asurvey%252C%2520we%2520aim%2520to%2520provide%2520a%2520comprehensive%2520understanding%2520of%2520the%2520current%250Alandscape%2520of%2520VRU%2520safety%2520technologies%252C%2520identifying%2520areas%2520of%2520progress%2520and%2520areas%250Aneeding%2520further%2520research%2520and%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19202v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vulnerable%20Road%20User%20Detection%20and%20Safety%20Enhancement%3A%20A%20Comprehensive%0A%20%20Survey&entry.906535625=Renato%20M.%20Silva%20and%20Greg%C3%B3rio%20F.%20Azevedo%20and%20Matheus%20V.%20V.%20Berto%20and%20Jean%20R.%20Rocha%20and%20Eduardo%20C.%20Fidelis%20and%20Matheus%20V.%20Nogueira%20and%20Pedro%20H.%20Lisboa%20and%20Tiago%20A.%20Almeida&entry.1292438233=%20%20Traffic%20incidents%20involving%20vulnerable%20road%20users%20%28VRUs%29%20constitute%20a%0Asignificant%20proportion%20of%20global%20road%20accidents.%20Advances%20in%20traffic%0Acommunication%20ecosystems%2C%20coupled%20with%20sophisticated%20signal%20processing%20and%0Amachine%20learning%20techniques%2C%20have%20facilitated%20the%20utilization%20of%20data%20from%0Adiverse%20sensors.%20Despite%20these%20advancements%20and%20the%20availability%20of%20extensive%0Adatasets%2C%20substantial%20progress%20is%20required%20to%20mitigate%20traffic%20casualties.%20This%0Apaper%20provides%20a%20comprehensive%20survey%20of%20state-of-the-art%20technologies%20and%0Amethodologies%20to%20enhance%20the%20safety%20of%20VRUs.%20The%20study%20delves%20into%20the%0Acommunication%20networks%20between%20vehicles%20and%20VRUs%2C%20emphasizing%20the%20integration%0Aof%20advanced%20sensors%20and%20the%20availability%20of%20relevant%20datasets.%20It%20explores%0Apreprocessing%20techniques%20and%20data%20fusion%20methods%20to%20enhance%20sensor%20data%0Aquality.%20Furthermore%2C%20our%20study%20assesses%20critical%20simulation%20environments%0Aessential%20for%20developing%20and%20testing%20VRU%20safety%20systems.%20Our%20research%20also%0Ahighlights%20recent%20advances%20in%20VRU%20detection%20and%20classification%20algorithms%2C%0Aaddressing%20challenges%20such%20as%20variable%20environmental%20conditions.%20Additionally%2C%0Awe%20cover%20cutting-edge%20research%20in%20predicting%20VRU%20intentions%20and%20behaviors%2C%0Awhich%20is%20crucial%20for%20proactive%20collision%20avoidance%20strategies.%20Through%20this%0Asurvey%2C%20we%20aim%20to%20provide%20a%20comprehensive%20understanding%20of%20the%20current%0Alandscape%20of%20VRU%20safety%20technologies%2C%20identifying%20areas%20of%20progress%20and%20areas%0Aneeding%20further%20research%20and%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19202v3&entry.124074799=Read"},
{"title": "Towards Scalable and Versatile Weight Space Learning", "author": "Konstantin Sch\u00fcrholt and Michael W. Mahoney and Damian Borth", "abstract": "  Learning representations of well-trained neural network models holds the\npromise to provide an understanding of the inner workings of those models.\nHowever, previous work has either faced limitations when processing larger\nnetworks or was task-specific to either discriminative or generative tasks.\nThis paper introduces the SANE approach to weight-space learning. SANE\novercomes previous limitations by learning task-agnostic representations of\nneural networks that are scalable to larger models of varying architectures and\nthat show capabilities beyond a single task. Our method extends the idea of\nhyper-representations towards sequential processing of subsets of neural\nnetwork weights, thus allowing one to embed larger neural networks as a set of\ntokens into the learned representation space. SANE reveals global model\ninformation from layer-wise embeddings, and it can sequentially generate unseen\nneural network models, which was unattainable with previous\nhyper-representation learning methods. Extensive empirical evaluation\ndemonstrates that SANE matches or exceeds state-of-the-art performance on\nseveral weight representation learning benchmarks, particularly in\ninitialization for new tasks and larger ResNet architectures.\n", "link": "http://arxiv.org/abs/2406.09997v1", "date": "2024-06-14", "relevancy": 2.0139, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5157}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5032}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Scalable%20and%20Versatile%20Weight%20Space%20Learning&body=Title%3A%20Towards%20Scalable%20and%20Versatile%20Weight%20Space%20Learning%0AAuthor%3A%20Konstantin%20Sch%C3%BCrholt%20and%20Michael%20W.%20Mahoney%20and%20Damian%20Borth%0AAbstract%3A%20%20%20Learning%20representations%20of%20well-trained%20neural%20network%20models%20holds%20the%0Apromise%20to%20provide%20an%20understanding%20of%20the%20inner%20workings%20of%20those%20models.%0AHowever%2C%20previous%20work%20has%20either%20faced%20limitations%20when%20processing%20larger%0Anetworks%20or%20was%20task-specific%20to%20either%20discriminative%20or%20generative%20tasks.%0AThis%20paper%20introduces%20the%20SANE%20approach%20to%20weight-space%20learning.%20SANE%0Aovercomes%20previous%20limitations%20by%20learning%20task-agnostic%20representations%20of%0Aneural%20networks%20that%20are%20scalable%20to%20larger%20models%20of%20varying%20architectures%20and%0Athat%20show%20capabilities%20beyond%20a%20single%20task.%20Our%20method%20extends%20the%20idea%20of%0Ahyper-representations%20towards%20sequential%20processing%20of%20subsets%20of%20neural%0Anetwork%20weights%2C%20thus%20allowing%20one%20to%20embed%20larger%20neural%20networks%20as%20a%20set%20of%0Atokens%20into%20the%20learned%20representation%20space.%20SANE%20reveals%20global%20model%0Ainformation%20from%20layer-wise%20embeddings%2C%20and%20it%20can%20sequentially%20generate%20unseen%0Aneural%20network%20models%2C%20which%20was%20unattainable%20with%20previous%0Ahyper-representation%20learning%20methods.%20Extensive%20empirical%20evaluation%0Ademonstrates%20that%20SANE%20matches%20or%20exceeds%20state-of-the-art%20performance%20on%0Aseveral%20weight%20representation%20learning%20benchmarks%2C%20particularly%20in%0Ainitialization%20for%20new%20tasks%20and%20larger%20ResNet%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Scalable%2520and%2520Versatile%2520Weight%2520Space%2520Learning%26entry.906535625%3DKonstantin%2520Sch%25C3%25BCrholt%2520and%2520Michael%2520W.%2520Mahoney%2520and%2520Damian%2520Borth%26entry.1292438233%3D%2520%2520Learning%2520representations%2520of%2520well-trained%2520neural%2520network%2520models%2520holds%2520the%250Apromise%2520to%2520provide%2520an%2520understanding%2520of%2520the%2520inner%2520workings%2520of%2520those%2520models.%250AHowever%252C%2520previous%2520work%2520has%2520either%2520faced%2520limitations%2520when%2520processing%2520larger%250Anetworks%2520or%2520was%2520task-specific%2520to%2520either%2520discriminative%2520or%2520generative%2520tasks.%250AThis%2520paper%2520introduces%2520the%2520SANE%2520approach%2520to%2520weight-space%2520learning.%2520SANE%250Aovercomes%2520previous%2520limitations%2520by%2520learning%2520task-agnostic%2520representations%2520of%250Aneural%2520networks%2520that%2520are%2520scalable%2520to%2520larger%2520models%2520of%2520varying%2520architectures%2520and%250Athat%2520show%2520capabilities%2520beyond%2520a%2520single%2520task.%2520Our%2520method%2520extends%2520the%2520idea%2520of%250Ahyper-representations%2520towards%2520sequential%2520processing%2520of%2520subsets%2520of%2520neural%250Anetwork%2520weights%252C%2520thus%2520allowing%2520one%2520to%2520embed%2520larger%2520neural%2520networks%2520as%2520a%2520set%2520of%250Atokens%2520into%2520the%2520learned%2520representation%2520space.%2520SANE%2520reveals%2520global%2520model%250Ainformation%2520from%2520layer-wise%2520embeddings%252C%2520and%2520it%2520can%2520sequentially%2520generate%2520unseen%250Aneural%2520network%2520models%252C%2520which%2520was%2520unattainable%2520with%2520previous%250Ahyper-representation%2520learning%2520methods.%2520Extensive%2520empirical%2520evaluation%250Ademonstrates%2520that%2520SANE%2520matches%2520or%2520exceeds%2520state-of-the-art%2520performance%2520on%250Aseveral%2520weight%2520representation%2520learning%2520benchmarks%252C%2520particularly%2520in%250Ainitialization%2520for%2520new%2520tasks%2520and%2520larger%2520ResNet%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Scalable%20and%20Versatile%20Weight%20Space%20Learning&entry.906535625=Konstantin%20Sch%C3%BCrholt%20and%20Michael%20W.%20Mahoney%20and%20Damian%20Borth&entry.1292438233=%20%20Learning%20representations%20of%20well-trained%20neural%20network%20models%20holds%20the%0Apromise%20to%20provide%20an%20understanding%20of%20the%20inner%20workings%20of%20those%20models.%0AHowever%2C%20previous%20work%20has%20either%20faced%20limitations%20when%20processing%20larger%0Anetworks%20or%20was%20task-specific%20to%20either%20discriminative%20or%20generative%20tasks.%0AThis%20paper%20introduces%20the%20SANE%20approach%20to%20weight-space%20learning.%20SANE%0Aovercomes%20previous%20limitations%20by%20learning%20task-agnostic%20representations%20of%0Aneural%20networks%20that%20are%20scalable%20to%20larger%20models%20of%20varying%20architectures%20and%0Athat%20show%20capabilities%20beyond%20a%20single%20task.%20Our%20method%20extends%20the%20idea%20of%0Ahyper-representations%20towards%20sequential%20processing%20of%20subsets%20of%20neural%0Anetwork%20weights%2C%20thus%20allowing%20one%20to%20embed%20larger%20neural%20networks%20as%20a%20set%20of%0Atokens%20into%20the%20learned%20representation%20space.%20SANE%20reveals%20global%20model%0Ainformation%20from%20layer-wise%20embeddings%2C%20and%20it%20can%20sequentially%20generate%20unseen%0Aneural%20network%20models%2C%20which%20was%20unattainable%20with%20previous%0Ahyper-representation%20learning%20methods.%20Extensive%20empirical%20evaluation%0Ademonstrates%20that%20SANE%20matches%20or%20exceeds%20state-of-the-art%20performance%20on%0Aseveral%20weight%20representation%20learning%20benchmarks%2C%20particularly%20in%0Ainitialization%20for%20new%20tasks%20and%20larger%20ResNet%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09997v1&entry.124074799=Read"},
{"title": "Over-parameterization and Adversarial Robustness in Neural Networks: An\n  Overview and Empirical Analysis", "author": "Zhang Chen and Luca Demetrio and Srishti Gupta and Xiaoyi Feng and Zhaoqiang Xia and Antonio Emanuele Cin\u00e0 and Maura Pintor and Luca Oneto and Ambra Demontis and Battista Biggio and Fabio Roli", "abstract": "  Thanks to their extensive capacity, over-parameterized neural networks\nexhibit superior predictive capabilities and generalization. However, having a\nlarge parameter space is considered one of the main suspects of the neural\nnetworks' vulnerability to adversarial example -- input samples crafted ad-hoc\nto induce a desired misclassification. Relevant literature has claimed\ncontradictory remarks in support of and against the robustness of\nover-parameterized networks. These contradictory findings might be due to the\nfailure of the attack employed to evaluate the networks' robustness. Previous\nresearch has demonstrated that depending on the considered model, the algorithm\nemployed to generate adversarial examples may not function properly, leading to\noverestimating the model's robustness. In this work, we empirically study the\nrobustness of over-parameterized networks against adversarial examples.\nHowever, unlike the previous works, we also evaluate the considered attack's\nreliability to support the results' veracity. Our results show that\nover-parameterized networks are robust against adversarial attacks as opposed\nto their under-parameterized counterparts.\n", "link": "http://arxiv.org/abs/2406.10090v1", "date": "2024-06-14", "relevancy": 2.0093, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5128}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4987}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Over-parameterization%20and%20Adversarial%20Robustness%20in%20Neural%20Networks%3A%20An%0A%20%20Overview%20and%20Empirical%20Analysis&body=Title%3A%20Over-parameterization%20and%20Adversarial%20Robustness%20in%20Neural%20Networks%3A%20An%0A%20%20Overview%20and%20Empirical%20Analysis%0AAuthor%3A%20Zhang%20Chen%20and%20Luca%20Demetrio%20and%20Srishti%20Gupta%20and%20Xiaoyi%20Feng%20and%20Zhaoqiang%20Xia%20and%20Antonio%20Emanuele%20Cin%C3%A0%20and%20Maura%20Pintor%20and%20Luca%20Oneto%20and%20Ambra%20Demontis%20and%20Battista%20Biggio%20and%20Fabio%20Roli%0AAbstract%3A%20%20%20Thanks%20to%20their%20extensive%20capacity%2C%20over-parameterized%20neural%20networks%0Aexhibit%20superior%20predictive%20capabilities%20and%20generalization.%20However%2C%20having%20a%0Alarge%20parameter%20space%20is%20considered%20one%20of%20the%20main%20suspects%20of%20the%20neural%0Anetworks%27%20vulnerability%20to%20adversarial%20example%20--%20input%20samples%20crafted%20ad-hoc%0Ato%20induce%20a%20desired%20misclassification.%20Relevant%20literature%20has%20claimed%0Acontradictory%20remarks%20in%20support%20of%20and%20against%20the%20robustness%20of%0Aover-parameterized%20networks.%20These%20contradictory%20findings%20might%20be%20due%20to%20the%0Afailure%20of%20the%20attack%20employed%20to%20evaluate%20the%20networks%27%20robustness.%20Previous%0Aresearch%20has%20demonstrated%20that%20depending%20on%20the%20considered%20model%2C%20the%20algorithm%0Aemployed%20to%20generate%20adversarial%20examples%20may%20not%20function%20properly%2C%20leading%20to%0Aoverestimating%20the%20model%27s%20robustness.%20In%20this%20work%2C%20we%20empirically%20study%20the%0Arobustness%20of%20over-parameterized%20networks%20against%20adversarial%20examples.%0AHowever%2C%20unlike%20the%20previous%20works%2C%20we%20also%20evaluate%20the%20considered%20attack%27s%0Areliability%20to%20support%20the%20results%27%20veracity.%20Our%20results%20show%20that%0Aover-parameterized%20networks%20are%20robust%20against%20adversarial%20attacks%20as%20opposed%0Ato%20their%20under-parameterized%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOver-parameterization%2520and%2520Adversarial%2520Robustness%2520in%2520Neural%2520Networks%253A%2520An%250A%2520%2520Overview%2520and%2520Empirical%2520Analysis%26entry.906535625%3DZhang%2520Chen%2520and%2520Luca%2520Demetrio%2520and%2520Srishti%2520Gupta%2520and%2520Xiaoyi%2520Feng%2520and%2520Zhaoqiang%2520Xia%2520and%2520Antonio%2520Emanuele%2520Cin%25C3%25A0%2520and%2520Maura%2520Pintor%2520and%2520Luca%2520Oneto%2520and%2520Ambra%2520Demontis%2520and%2520Battista%2520Biggio%2520and%2520Fabio%2520Roli%26entry.1292438233%3D%2520%2520Thanks%2520to%2520their%2520extensive%2520capacity%252C%2520over-parameterized%2520neural%2520networks%250Aexhibit%2520superior%2520predictive%2520capabilities%2520and%2520generalization.%2520However%252C%2520having%2520a%250Alarge%2520parameter%2520space%2520is%2520considered%2520one%2520of%2520the%2520main%2520suspects%2520of%2520the%2520neural%250Anetworks%2527%2520vulnerability%2520to%2520adversarial%2520example%2520--%2520input%2520samples%2520crafted%2520ad-hoc%250Ato%2520induce%2520a%2520desired%2520misclassification.%2520Relevant%2520literature%2520has%2520claimed%250Acontradictory%2520remarks%2520in%2520support%2520of%2520and%2520against%2520the%2520robustness%2520of%250Aover-parameterized%2520networks.%2520These%2520contradictory%2520findings%2520might%2520be%2520due%2520to%2520the%250Afailure%2520of%2520the%2520attack%2520employed%2520to%2520evaluate%2520the%2520networks%2527%2520robustness.%2520Previous%250Aresearch%2520has%2520demonstrated%2520that%2520depending%2520on%2520the%2520considered%2520model%252C%2520the%2520algorithm%250Aemployed%2520to%2520generate%2520adversarial%2520examples%2520may%2520not%2520function%2520properly%252C%2520leading%2520to%250Aoverestimating%2520the%2520model%2527s%2520robustness.%2520In%2520this%2520work%252C%2520we%2520empirically%2520study%2520the%250Arobustness%2520of%2520over-parameterized%2520networks%2520against%2520adversarial%2520examples.%250AHowever%252C%2520unlike%2520the%2520previous%2520works%252C%2520we%2520also%2520evaluate%2520the%2520considered%2520attack%2527s%250Areliability%2520to%2520support%2520the%2520results%2527%2520veracity.%2520Our%2520results%2520show%2520that%250Aover-parameterized%2520networks%2520are%2520robust%2520against%2520adversarial%2520attacks%2520as%2520opposed%250Ato%2520their%2520under-parameterized%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Over-parameterization%20and%20Adversarial%20Robustness%20in%20Neural%20Networks%3A%20An%0A%20%20Overview%20and%20Empirical%20Analysis&entry.906535625=Zhang%20Chen%20and%20Luca%20Demetrio%20and%20Srishti%20Gupta%20and%20Xiaoyi%20Feng%20and%20Zhaoqiang%20Xia%20and%20Antonio%20Emanuele%20Cin%C3%A0%20and%20Maura%20Pintor%20and%20Luca%20Oneto%20and%20Ambra%20Demontis%20and%20Battista%20Biggio%20and%20Fabio%20Roli&entry.1292438233=%20%20Thanks%20to%20their%20extensive%20capacity%2C%20over-parameterized%20neural%20networks%0Aexhibit%20superior%20predictive%20capabilities%20and%20generalization.%20However%2C%20having%20a%0Alarge%20parameter%20space%20is%20considered%20one%20of%20the%20main%20suspects%20of%20the%20neural%0Anetworks%27%20vulnerability%20to%20adversarial%20example%20--%20input%20samples%20crafted%20ad-hoc%0Ato%20induce%20a%20desired%20misclassification.%20Relevant%20literature%20has%20claimed%0Acontradictory%20remarks%20in%20support%20of%20and%20against%20the%20robustness%20of%0Aover-parameterized%20networks.%20These%20contradictory%20findings%20might%20be%20due%20to%20the%0Afailure%20of%20the%20attack%20employed%20to%20evaluate%20the%20networks%27%20robustness.%20Previous%0Aresearch%20has%20demonstrated%20that%20depending%20on%20the%20considered%20model%2C%20the%20algorithm%0Aemployed%20to%20generate%20adversarial%20examples%20may%20not%20function%20properly%2C%20leading%20to%0Aoverestimating%20the%20model%27s%20robustness.%20In%20this%20work%2C%20we%20empirically%20study%20the%0Arobustness%20of%20over-parameterized%20networks%20against%20adversarial%20examples.%0AHowever%2C%20unlike%20the%20previous%20works%2C%20we%20also%20evaluate%20the%20considered%20attack%27s%0Areliability%20to%20support%20the%20results%27%20veracity.%20Our%20results%20show%20that%0Aover-parameterized%20networks%20are%20robust%20against%20adversarial%20attacks%20as%20opposed%0Ato%20their%20under-parameterized%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10090v1&entry.124074799=Read"},
{"title": "H-Fac: Memory-Efficient Optimization with Factorized Hamiltonian Descent", "author": "Son Nguyen and Lizhang Chen and Bo Liu and Qiang Liu", "abstract": "  In this study, we introduce a novel adaptive optimizer, H-Fac, which\nincorporates a factorized approach to momentum and scaling parameters. Our\nalgorithm demonstrates competitive performances on both ResNets and Vision\nTransformers, while achieving sublinear memory costs through the use of rank-1\nparameterizations for moment estimators. We develop our algorithms based on\nprinciples derived from Hamiltonian dynamics, providing robust theoretical\nunderpinnings. These optimization algorithms are designed to be both\nstraightforward and adaptable, facilitating easy implementation in diverse\nsettings.\n", "link": "http://arxiv.org/abs/2406.09958v1", "date": "2024-06-14", "relevancy": 1.9994, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5222}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4855}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20H-Fac%3A%20Memory-Efficient%20Optimization%20with%20Factorized%20Hamiltonian%20Descent&body=Title%3A%20H-Fac%3A%20Memory-Efficient%20Optimization%20with%20Factorized%20Hamiltonian%20Descent%0AAuthor%3A%20Son%20Nguyen%20and%20Lizhang%20Chen%20and%20Bo%20Liu%20and%20Qiang%20Liu%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20introduce%20a%20novel%20adaptive%20optimizer%2C%20H-Fac%2C%20which%0Aincorporates%20a%20factorized%20approach%20to%20momentum%20and%20scaling%20parameters.%20Our%0Aalgorithm%20demonstrates%20competitive%20performances%20on%20both%20ResNets%20and%20Vision%0ATransformers%2C%20while%20achieving%20sublinear%20memory%20costs%20through%20the%20use%20of%20rank-1%0Aparameterizations%20for%20moment%20estimators.%20We%20develop%20our%20algorithms%20based%20on%0Aprinciples%20derived%20from%20Hamiltonian%20dynamics%2C%20providing%20robust%20theoretical%0Aunderpinnings.%20These%20optimization%20algorithms%20are%20designed%20to%20be%20both%0Astraightforward%20and%20adaptable%2C%20facilitating%20easy%20implementation%20in%20diverse%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DH-Fac%253A%2520Memory-Efficient%2520Optimization%2520with%2520Factorized%2520Hamiltonian%2520Descent%26entry.906535625%3DSon%2520Nguyen%2520and%2520Lizhang%2520Chen%2520and%2520Bo%2520Liu%2520and%2520Qiang%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520adaptive%2520optimizer%252C%2520H-Fac%252C%2520which%250Aincorporates%2520a%2520factorized%2520approach%2520to%2520momentum%2520and%2520scaling%2520parameters.%2520Our%250Aalgorithm%2520demonstrates%2520competitive%2520performances%2520on%2520both%2520ResNets%2520and%2520Vision%250ATransformers%252C%2520while%2520achieving%2520sublinear%2520memory%2520costs%2520through%2520the%2520use%2520of%2520rank-1%250Aparameterizations%2520for%2520moment%2520estimators.%2520We%2520develop%2520our%2520algorithms%2520based%2520on%250Aprinciples%2520derived%2520from%2520Hamiltonian%2520dynamics%252C%2520providing%2520robust%2520theoretical%250Aunderpinnings.%2520These%2520optimization%2520algorithms%2520are%2520designed%2520to%2520be%2520both%250Astraightforward%2520and%2520adaptable%252C%2520facilitating%2520easy%2520implementation%2520in%2520diverse%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H-Fac%3A%20Memory-Efficient%20Optimization%20with%20Factorized%20Hamiltonian%20Descent&entry.906535625=Son%20Nguyen%20and%20Lizhang%20Chen%20and%20Bo%20Liu%20and%20Qiang%20Liu&entry.1292438233=%20%20In%20this%20study%2C%20we%20introduce%20a%20novel%20adaptive%20optimizer%2C%20H-Fac%2C%20which%0Aincorporates%20a%20factorized%20approach%20to%20momentum%20and%20scaling%20parameters.%20Our%0Aalgorithm%20demonstrates%20competitive%20performances%20on%20both%20ResNets%20and%20Vision%0ATransformers%2C%20while%20achieving%20sublinear%20memory%20costs%20through%20the%20use%20of%20rank-1%0Aparameterizations%20for%20moment%20estimators.%20We%20develop%20our%20algorithms%20based%20on%0Aprinciples%20derived%20from%20Hamiltonian%20dynamics%2C%20providing%20robust%20theoretical%0Aunderpinnings.%20These%20optimization%20algorithms%20are%20designed%20to%20be%20both%0Astraightforward%20and%20adaptable%2C%20facilitating%20easy%20implementation%20in%20diverse%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09958v1&entry.124074799=Read"},
{"title": "Relating tSNE and UMAP to Classical Dimensionality Reduction", "author": "Andrew Draganov and Simon Dohn", "abstract": "  It has become standard to use gradient-based dimensionality reduction (DR)\nmethods like tSNE and UMAP when explaining what AI models have learned. This\nmakes sense: these methods are fast, robust, and have an uncanny ability to\nfind semantic patterns in high-dimensional data without supervision. Despite\nthis, gradient-based DR methods lack the most important quality that an\nexplainability method should possess: themselves being explainable. That is,\ngiven a UMAP output, it is currently unclear what one can say about the\ncorresponding input. We work towards closing this question by relating UMAP to\nclassical DR techniques. Specifically, we show that one can fully recover\nmethods like PCA, MDS, and ISOMAP in the modern DR paradigm: by applying\nattractions and repulsions onto a randomly initialized dataset. We also show\nthat, with a small change, Locally Linear Embeddings (LLE) can\nindistinguishably reproduce UMAP outputs. This implies that the UMAP effective\nobjective is minimized by this modified version of LLE (and vice versa). Given\nthis, we discuss what must be true of UMAP emebddings and present avenues for\nfuture work.\n", "link": "http://arxiv.org/abs/2306.11898v2", "date": "2024-06-14", "relevancy": 1.9915, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5025}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4972}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relating%20tSNE%20and%20UMAP%20to%20Classical%20Dimensionality%20Reduction&body=Title%3A%20Relating%20tSNE%20and%20UMAP%20to%20Classical%20Dimensionality%20Reduction%0AAuthor%3A%20Andrew%20Draganov%20and%20Simon%20Dohn%0AAbstract%3A%20%20%20It%20has%20become%20standard%20to%20use%20gradient-based%20dimensionality%20reduction%20%28DR%29%0Amethods%20like%20tSNE%20and%20UMAP%20when%20explaining%20what%20AI%20models%20have%20learned.%20This%0Amakes%20sense%3A%20these%20methods%20are%20fast%2C%20robust%2C%20and%20have%20an%20uncanny%20ability%20to%0Afind%20semantic%20patterns%20in%20high-dimensional%20data%20without%20supervision.%20Despite%0Athis%2C%20gradient-based%20DR%20methods%20lack%20the%20most%20important%20quality%20that%20an%0Aexplainability%20method%20should%20possess%3A%20themselves%20being%20explainable.%20That%20is%2C%0Agiven%20a%20UMAP%20output%2C%20it%20is%20currently%20unclear%20what%20one%20can%20say%20about%20the%0Acorresponding%20input.%20We%20work%20towards%20closing%20this%20question%20by%20relating%20UMAP%20to%0Aclassical%20DR%20techniques.%20Specifically%2C%20we%20show%20that%20one%20can%20fully%20recover%0Amethods%20like%20PCA%2C%20MDS%2C%20and%20ISOMAP%20in%20the%20modern%20DR%20paradigm%3A%20by%20applying%0Aattractions%20and%20repulsions%20onto%20a%20randomly%20initialized%20dataset.%20We%20also%20show%0Athat%2C%20with%20a%20small%20change%2C%20Locally%20Linear%20Embeddings%20%28LLE%29%20can%0Aindistinguishably%20reproduce%20UMAP%20outputs.%20This%20implies%20that%20the%20UMAP%20effective%0Aobjective%20is%20minimized%20by%20this%20modified%20version%20of%20LLE%20%28and%20vice%20versa%29.%20Given%0Athis%2C%20we%20discuss%20what%20must%20be%20true%20of%20UMAP%20emebddings%20and%20present%20avenues%20for%0Afuture%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.11898v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelating%2520tSNE%2520and%2520UMAP%2520to%2520Classical%2520Dimensionality%2520Reduction%26entry.906535625%3DAndrew%2520Draganov%2520and%2520Simon%2520Dohn%26entry.1292438233%3D%2520%2520It%2520has%2520become%2520standard%2520to%2520use%2520gradient-based%2520dimensionality%2520reduction%2520%2528DR%2529%250Amethods%2520like%2520tSNE%2520and%2520UMAP%2520when%2520explaining%2520what%2520AI%2520models%2520have%2520learned.%2520This%250Amakes%2520sense%253A%2520these%2520methods%2520are%2520fast%252C%2520robust%252C%2520and%2520have%2520an%2520uncanny%2520ability%2520to%250Afind%2520semantic%2520patterns%2520in%2520high-dimensional%2520data%2520without%2520supervision.%2520Despite%250Athis%252C%2520gradient-based%2520DR%2520methods%2520lack%2520the%2520most%2520important%2520quality%2520that%2520an%250Aexplainability%2520method%2520should%2520possess%253A%2520themselves%2520being%2520explainable.%2520That%2520is%252C%250Agiven%2520a%2520UMAP%2520output%252C%2520it%2520is%2520currently%2520unclear%2520what%2520one%2520can%2520say%2520about%2520the%250Acorresponding%2520input.%2520We%2520work%2520towards%2520closing%2520this%2520question%2520by%2520relating%2520UMAP%2520to%250Aclassical%2520DR%2520techniques.%2520Specifically%252C%2520we%2520show%2520that%2520one%2520can%2520fully%2520recover%250Amethods%2520like%2520PCA%252C%2520MDS%252C%2520and%2520ISOMAP%2520in%2520the%2520modern%2520DR%2520paradigm%253A%2520by%2520applying%250Aattractions%2520and%2520repulsions%2520onto%2520a%2520randomly%2520initialized%2520dataset.%2520We%2520also%2520show%250Athat%252C%2520with%2520a%2520small%2520change%252C%2520Locally%2520Linear%2520Embeddings%2520%2528LLE%2529%2520can%250Aindistinguishably%2520reproduce%2520UMAP%2520outputs.%2520This%2520implies%2520that%2520the%2520UMAP%2520effective%250Aobjective%2520is%2520minimized%2520by%2520this%2520modified%2520version%2520of%2520LLE%2520%2528and%2520vice%2520versa%2529.%2520Given%250Athis%252C%2520we%2520discuss%2520what%2520must%2520be%2520true%2520of%2520UMAP%2520emebddings%2520and%2520present%2520avenues%2520for%250Afuture%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.11898v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relating%20tSNE%20and%20UMAP%20to%20Classical%20Dimensionality%20Reduction&entry.906535625=Andrew%20Draganov%20and%20Simon%20Dohn&entry.1292438233=%20%20It%20has%20become%20standard%20to%20use%20gradient-based%20dimensionality%20reduction%20%28DR%29%0Amethods%20like%20tSNE%20and%20UMAP%20when%20explaining%20what%20AI%20models%20have%20learned.%20This%0Amakes%20sense%3A%20these%20methods%20are%20fast%2C%20robust%2C%20and%20have%20an%20uncanny%20ability%20to%0Afind%20semantic%20patterns%20in%20high-dimensional%20data%20without%20supervision.%20Despite%0Athis%2C%20gradient-based%20DR%20methods%20lack%20the%20most%20important%20quality%20that%20an%0Aexplainability%20method%20should%20possess%3A%20themselves%20being%20explainable.%20That%20is%2C%0Agiven%20a%20UMAP%20output%2C%20it%20is%20currently%20unclear%20what%20one%20can%20say%20about%20the%0Acorresponding%20input.%20We%20work%20towards%20closing%20this%20question%20by%20relating%20UMAP%20to%0Aclassical%20DR%20techniques.%20Specifically%2C%20we%20show%20that%20one%20can%20fully%20recover%0Amethods%20like%20PCA%2C%20MDS%2C%20and%20ISOMAP%20in%20the%20modern%20DR%20paradigm%3A%20by%20applying%0Aattractions%20and%20repulsions%20onto%20a%20randomly%20initialized%20dataset.%20We%20also%20show%0Athat%2C%20with%20a%20small%20change%2C%20Locally%20Linear%20Embeddings%20%28LLE%29%20can%0Aindistinguishably%20reproduce%20UMAP%20outputs.%20This%20implies%20that%20the%20UMAP%20effective%0Aobjective%20is%20minimized%20by%20this%20modified%20version%20of%20LLE%20%28and%20vice%20versa%29.%20Given%0Athis%2C%20we%20discuss%20what%20must%20be%20true%20of%20UMAP%20emebddings%20and%20present%20avenues%20for%0Afuture%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.11898v2&entry.124074799=Read"},
{"title": "Deep learning empowered sensor fusion to improve infant movement\n  classification", "author": "Tomas Kulvicius and Dajie Zhang and Luise Poustka and Sven B\u00f6lte and Lennart Jahn and Sarah Fl\u00fcgge and Marc Kraft and Markus Zweckstetter and Karin Nielsen-Saines and Florentin W\u00f6rg\u00f6tter and Peter B Marschik", "abstract": "  There is a recent boom in the development of AI solutions to facilitate and\nenhance diagnostic procedures for established clinical tools. To assess the\nintegrity of the developing nervous system, the Prechtl general movement\nassessment (GMA) is recognized for its clinical value in diagnosing\nneurological impairments in early infancy. GMA has been increasingly augmented\nthrough machine learning approaches intending to scale-up its application,\ncircumvent costs in the training of human assessors and further standardize\nclassification of spontaneous motor patterns. Available deep learning tools,\nall of which are based on single sensor modalities, are however still\nconsiderably inferior to that of well-trained human assessors. These approaches\nare hardly comparable as all models are designed, trained and evaluated on\nproprietary/silo-data sets. With this study we propose a sensor fusion approach\nfor assessing fidgety movements (FMs) comparing three different sensor\nmodalities (pressure, inertial, and visual sensors). Various combinations and\ntwo sensor fusion approaches (late and early fusion) for infant movement\nclassification were tested to evaluate whether a multi-sensor system\noutperforms single modality assessments. The performance of the three-sensor\nfusion (classification accuracy of 94.5\\%) was significantly higher than that\nof any single modality evaluated, suggesting the sensor fusion approach is a\npromising avenue for automated classification of infant motor patterns. The\ndevelopment of a robust sensor fusion system may significantly enhance AI-based\nearly recognition of neurofunctions, ultimately facilitating automated early\ndetection of neurodevelopmental conditions.\n", "link": "http://arxiv.org/abs/2406.09014v2", "date": "2024-06-14", "relevancy": 1.9892, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5441}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4929}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20learning%20empowered%20sensor%20fusion%20to%20improve%20infant%20movement%0A%20%20classification&body=Title%3A%20Deep%20learning%20empowered%20sensor%20fusion%20to%20improve%20infant%20movement%0A%20%20classification%0AAuthor%3A%20Tomas%20Kulvicius%20and%20Dajie%20Zhang%20and%20Luise%20Poustka%20and%20Sven%20B%C3%B6lte%20and%20Lennart%20Jahn%20and%20Sarah%20Fl%C3%BCgge%20and%20Marc%20Kraft%20and%20Markus%20Zweckstetter%20and%20Karin%20Nielsen-Saines%20and%20Florentin%20W%C3%B6rg%C3%B6tter%20and%20Peter%20B%20Marschik%0AAbstract%3A%20%20%20There%20is%20a%20recent%20boom%20in%20the%20development%20of%20AI%20solutions%20to%20facilitate%20and%0Aenhance%20diagnostic%20procedures%20for%20established%20clinical%20tools.%20To%20assess%20the%0Aintegrity%20of%20the%20developing%20nervous%20system%2C%20the%20Prechtl%20general%20movement%0Aassessment%20%28GMA%29%20is%20recognized%20for%20its%20clinical%20value%20in%20diagnosing%0Aneurological%20impairments%20in%20early%20infancy.%20GMA%20has%20been%20increasingly%20augmented%0Athrough%20machine%20learning%20approaches%20intending%20to%20scale-up%20its%20application%2C%0Acircumvent%20costs%20in%20the%20training%20of%20human%20assessors%20and%20further%20standardize%0Aclassification%20of%20spontaneous%20motor%20patterns.%20Available%20deep%20learning%20tools%2C%0Aall%20of%20which%20are%20based%20on%20single%20sensor%20modalities%2C%20are%20however%20still%0Aconsiderably%20inferior%20to%20that%20of%20well-trained%20human%20assessors.%20These%20approaches%0Aare%20hardly%20comparable%20as%20all%20models%20are%20designed%2C%20trained%20and%20evaluated%20on%0Aproprietary/silo-data%20sets.%20With%20this%20study%20we%20propose%20a%20sensor%20fusion%20approach%0Afor%20assessing%20fidgety%20movements%20%28FMs%29%20comparing%20three%20different%20sensor%0Amodalities%20%28pressure%2C%20inertial%2C%20and%20visual%20sensors%29.%20Various%20combinations%20and%0Atwo%20sensor%20fusion%20approaches%20%28late%20and%20early%20fusion%29%20for%20infant%20movement%0Aclassification%20were%20tested%20to%20evaluate%20whether%20a%20multi-sensor%20system%0Aoutperforms%20single%20modality%20assessments.%20The%20performance%20of%20the%20three-sensor%0Afusion%20%28classification%20accuracy%20of%2094.5%5C%25%29%20was%20significantly%20higher%20than%20that%0Aof%20any%20single%20modality%20evaluated%2C%20suggesting%20the%20sensor%20fusion%20approach%20is%20a%0Apromising%20avenue%20for%20automated%20classification%20of%20infant%20motor%20patterns.%20The%0Adevelopment%20of%20a%20robust%20sensor%20fusion%20system%20may%20significantly%20enhance%20AI-based%0Aearly%20recognition%20of%20neurofunctions%2C%20ultimately%20facilitating%20automated%20early%0Adetection%20of%20neurodevelopmental%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09014v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520learning%2520empowered%2520sensor%2520fusion%2520to%2520improve%2520infant%2520movement%250A%2520%2520classification%26entry.906535625%3DTomas%2520Kulvicius%2520and%2520Dajie%2520Zhang%2520and%2520Luise%2520Poustka%2520and%2520Sven%2520B%25C3%25B6lte%2520and%2520Lennart%2520Jahn%2520and%2520Sarah%2520Fl%25C3%25BCgge%2520and%2520Marc%2520Kraft%2520and%2520Markus%2520Zweckstetter%2520and%2520Karin%2520Nielsen-Saines%2520and%2520Florentin%2520W%25C3%25B6rg%25C3%25B6tter%2520and%2520Peter%2520B%2520Marschik%26entry.1292438233%3D%2520%2520There%2520is%2520a%2520recent%2520boom%2520in%2520the%2520development%2520of%2520AI%2520solutions%2520to%2520facilitate%2520and%250Aenhance%2520diagnostic%2520procedures%2520for%2520established%2520clinical%2520tools.%2520To%2520assess%2520the%250Aintegrity%2520of%2520the%2520developing%2520nervous%2520system%252C%2520the%2520Prechtl%2520general%2520movement%250Aassessment%2520%2528GMA%2529%2520is%2520recognized%2520for%2520its%2520clinical%2520value%2520in%2520diagnosing%250Aneurological%2520impairments%2520in%2520early%2520infancy.%2520GMA%2520has%2520been%2520increasingly%2520augmented%250Athrough%2520machine%2520learning%2520approaches%2520intending%2520to%2520scale-up%2520its%2520application%252C%250Acircumvent%2520costs%2520in%2520the%2520training%2520of%2520human%2520assessors%2520and%2520further%2520standardize%250Aclassification%2520of%2520spontaneous%2520motor%2520patterns.%2520Available%2520deep%2520learning%2520tools%252C%250Aall%2520of%2520which%2520are%2520based%2520on%2520single%2520sensor%2520modalities%252C%2520are%2520however%2520still%250Aconsiderably%2520inferior%2520to%2520that%2520of%2520well-trained%2520human%2520assessors.%2520These%2520approaches%250Aare%2520hardly%2520comparable%2520as%2520all%2520models%2520are%2520designed%252C%2520trained%2520and%2520evaluated%2520on%250Aproprietary/silo-data%2520sets.%2520With%2520this%2520study%2520we%2520propose%2520a%2520sensor%2520fusion%2520approach%250Afor%2520assessing%2520fidgety%2520movements%2520%2528FMs%2529%2520comparing%2520three%2520different%2520sensor%250Amodalities%2520%2528pressure%252C%2520inertial%252C%2520and%2520visual%2520sensors%2529.%2520Various%2520combinations%2520and%250Atwo%2520sensor%2520fusion%2520approaches%2520%2528late%2520and%2520early%2520fusion%2529%2520for%2520infant%2520movement%250Aclassification%2520were%2520tested%2520to%2520evaluate%2520whether%2520a%2520multi-sensor%2520system%250Aoutperforms%2520single%2520modality%2520assessments.%2520The%2520performance%2520of%2520the%2520three-sensor%250Afusion%2520%2528classification%2520accuracy%2520of%252094.5%255C%2525%2529%2520was%2520significantly%2520higher%2520than%2520that%250Aof%2520any%2520single%2520modality%2520evaluated%252C%2520suggesting%2520the%2520sensor%2520fusion%2520approach%2520is%2520a%250Apromising%2520avenue%2520for%2520automated%2520classification%2520of%2520infant%2520motor%2520patterns.%2520The%250Adevelopment%2520of%2520a%2520robust%2520sensor%2520fusion%2520system%2520may%2520significantly%2520enhance%2520AI-based%250Aearly%2520recognition%2520of%2520neurofunctions%252C%2520ultimately%2520facilitating%2520automated%2520early%250Adetection%2520of%2520neurodevelopmental%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09014v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20learning%20empowered%20sensor%20fusion%20to%20improve%20infant%20movement%0A%20%20classification&entry.906535625=Tomas%20Kulvicius%20and%20Dajie%20Zhang%20and%20Luise%20Poustka%20and%20Sven%20B%C3%B6lte%20and%20Lennart%20Jahn%20and%20Sarah%20Fl%C3%BCgge%20and%20Marc%20Kraft%20and%20Markus%20Zweckstetter%20and%20Karin%20Nielsen-Saines%20and%20Florentin%20W%C3%B6rg%C3%B6tter%20and%20Peter%20B%20Marschik&entry.1292438233=%20%20There%20is%20a%20recent%20boom%20in%20the%20development%20of%20AI%20solutions%20to%20facilitate%20and%0Aenhance%20diagnostic%20procedures%20for%20established%20clinical%20tools.%20To%20assess%20the%0Aintegrity%20of%20the%20developing%20nervous%20system%2C%20the%20Prechtl%20general%20movement%0Aassessment%20%28GMA%29%20is%20recognized%20for%20its%20clinical%20value%20in%20diagnosing%0Aneurological%20impairments%20in%20early%20infancy.%20GMA%20has%20been%20increasingly%20augmented%0Athrough%20machine%20learning%20approaches%20intending%20to%20scale-up%20its%20application%2C%0Acircumvent%20costs%20in%20the%20training%20of%20human%20assessors%20and%20further%20standardize%0Aclassification%20of%20spontaneous%20motor%20patterns.%20Available%20deep%20learning%20tools%2C%0Aall%20of%20which%20are%20based%20on%20single%20sensor%20modalities%2C%20are%20however%20still%0Aconsiderably%20inferior%20to%20that%20of%20well-trained%20human%20assessors.%20These%20approaches%0Aare%20hardly%20comparable%20as%20all%20models%20are%20designed%2C%20trained%20and%20evaluated%20on%0Aproprietary/silo-data%20sets.%20With%20this%20study%20we%20propose%20a%20sensor%20fusion%20approach%0Afor%20assessing%20fidgety%20movements%20%28FMs%29%20comparing%20three%20different%20sensor%0Amodalities%20%28pressure%2C%20inertial%2C%20and%20visual%20sensors%29.%20Various%20combinations%20and%0Atwo%20sensor%20fusion%20approaches%20%28late%20and%20early%20fusion%29%20for%20infant%20movement%0Aclassification%20were%20tested%20to%20evaluate%20whether%20a%20multi-sensor%20system%0Aoutperforms%20single%20modality%20assessments.%20The%20performance%20of%20the%20three-sensor%0Afusion%20%28classification%20accuracy%20of%2094.5%5C%25%29%20was%20significantly%20higher%20than%20that%0Aof%20any%20single%20modality%20evaluated%2C%20suggesting%20the%20sensor%20fusion%20approach%20is%20a%0Apromising%20avenue%20for%20automated%20classification%20of%20infant%20motor%20patterns.%20The%0Adevelopment%20of%20a%20robust%20sensor%20fusion%20system%20may%20significantly%20enhance%20AI-based%0Aearly%20recognition%20of%20neurofunctions%2C%20ultimately%20facilitating%20automated%20early%0Adetection%20of%20neurodevelopmental%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09014v2&entry.124074799=Read"},
{"title": "Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\n  Perturbations That Efficiently Fool Customized Diffusion Models", "author": "Jingyao Xu and Yuetong Lu and Yandong Li and Siyang Lu and Dongdong Wang and Xiang Wei", "abstract": "  Diffusion models (DMs) embark a new era of generative modeling and offer more\nopportunities for efficient generating high-quality and realistic data samples.\nHowever, their widespread use has also brought forth new challenges in model\nsecurity, which motivates the creation of more effective adversarial attackers\non DMs to understand its vulnerability. We propose CAAT, a simple but generic\nand efficient approach that does not require costly training to effectively\nfool latent diffusion models (LDMs). The approach is based on the observation\nthat cross-attention layers exhibits higher sensitivity to gradient change,\nallowing for leveraging subtle perturbations on published images to\nsignificantly corrupt the generated images. We show that a subtle perturbation\non an image can significantly impact the cross-attention layers, thus changing\nthe mapping between text and image during the fine-tuning of customized\ndiffusion models. Extensive experiments demonstrate that CAAT is compatible\nwith diverse diffusion models and outperforms baseline attack methods in a more\neffective (more noise) and efficient (twice as fast as Anti-DreamBooth and\nMist) manner.\n", "link": "http://arxiv.org/abs/2404.15081v2", "date": "2024-06-14", "relevancy": 1.98, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6716}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6678}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perturbing%20Attention%20Gives%20You%20More%20Bang%20for%20the%20Buck%3A%20Subtle%20Imaging%0A%20%20Perturbations%20That%20Efficiently%20Fool%20Customized%20Diffusion%20Models&body=Title%3A%20Perturbing%20Attention%20Gives%20You%20More%20Bang%20for%20the%20Buck%3A%20Subtle%20Imaging%0A%20%20Perturbations%20That%20Efficiently%20Fool%20Customized%20Diffusion%20Models%0AAuthor%3A%20Jingyao%20Xu%20and%20Yuetong%20Lu%20and%20Yandong%20Li%20and%20Siyang%20Lu%20and%20Dongdong%20Wang%20and%20Xiang%20Wei%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20embark%20a%20new%20era%20of%20generative%20modeling%20and%20offer%20more%0Aopportunities%20for%20efficient%20generating%20high-quality%20and%20realistic%20data%20samples.%0AHowever%2C%20their%20widespread%20use%20has%20also%20brought%20forth%20new%20challenges%20in%20model%0Asecurity%2C%20which%20motivates%20the%20creation%20of%20more%20effective%20adversarial%20attackers%0Aon%20DMs%20to%20understand%20its%20vulnerability.%20We%20propose%20CAAT%2C%20a%20simple%20but%20generic%0Aand%20efficient%20approach%20that%20does%20not%20require%20costly%20training%20to%20effectively%0Afool%20latent%20diffusion%20models%20%28LDMs%29.%20The%20approach%20is%20based%20on%20the%20observation%0Athat%20cross-attention%20layers%20exhibits%20higher%20sensitivity%20to%20gradient%20change%2C%0Aallowing%20for%20leveraging%20subtle%20perturbations%20on%20published%20images%20to%0Asignificantly%20corrupt%20the%20generated%20images.%20We%20show%20that%20a%20subtle%20perturbation%0Aon%20an%20image%20can%20significantly%20impact%20the%20cross-attention%20layers%2C%20thus%20changing%0Athe%20mapping%20between%20text%20and%20image%20during%20the%20fine-tuning%20of%20customized%0Adiffusion%20models.%20Extensive%20experiments%20demonstrate%20that%20CAAT%20is%20compatible%0Awith%20diverse%20diffusion%20models%20and%20outperforms%20baseline%20attack%20methods%20in%20a%20more%0Aeffective%20%28more%20noise%29%20and%20efficient%20%28twice%20as%20fast%20as%20Anti-DreamBooth%20and%0AMist%29%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15081v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerturbing%2520Attention%2520Gives%2520You%2520More%2520Bang%2520for%2520the%2520Buck%253A%2520Subtle%2520Imaging%250A%2520%2520Perturbations%2520That%2520Efficiently%2520Fool%2520Customized%2520Diffusion%2520Models%26entry.906535625%3DJingyao%2520Xu%2520and%2520Yuetong%2520Lu%2520and%2520Yandong%2520Li%2520and%2520Siyang%2520Lu%2520and%2520Dongdong%2520Wang%2520and%2520Xiang%2520Wei%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520%2528DMs%2529%2520embark%2520a%2520new%2520era%2520of%2520generative%2520modeling%2520and%2520offer%2520more%250Aopportunities%2520for%2520efficient%2520generating%2520high-quality%2520and%2520realistic%2520data%2520samples.%250AHowever%252C%2520their%2520widespread%2520use%2520has%2520also%2520brought%2520forth%2520new%2520challenges%2520in%2520model%250Asecurity%252C%2520which%2520motivates%2520the%2520creation%2520of%2520more%2520effective%2520adversarial%2520attackers%250Aon%2520DMs%2520to%2520understand%2520its%2520vulnerability.%2520We%2520propose%2520CAAT%252C%2520a%2520simple%2520but%2520generic%250Aand%2520efficient%2520approach%2520that%2520does%2520not%2520require%2520costly%2520training%2520to%2520effectively%250Afool%2520latent%2520diffusion%2520models%2520%2528LDMs%2529.%2520The%2520approach%2520is%2520based%2520on%2520the%2520observation%250Athat%2520cross-attention%2520layers%2520exhibits%2520higher%2520sensitivity%2520to%2520gradient%2520change%252C%250Aallowing%2520for%2520leveraging%2520subtle%2520perturbations%2520on%2520published%2520images%2520to%250Asignificantly%2520corrupt%2520the%2520generated%2520images.%2520We%2520show%2520that%2520a%2520subtle%2520perturbation%250Aon%2520an%2520image%2520can%2520significantly%2520impact%2520the%2520cross-attention%2520layers%252C%2520thus%2520changing%250Athe%2520mapping%2520between%2520text%2520and%2520image%2520during%2520the%2520fine-tuning%2520of%2520customized%250Adiffusion%2520models.%2520Extensive%2520experiments%2520demonstrate%2520that%2520CAAT%2520is%2520compatible%250Awith%2520diverse%2520diffusion%2520models%2520and%2520outperforms%2520baseline%2520attack%2520methods%2520in%2520a%2520more%250Aeffective%2520%2528more%2520noise%2529%2520and%2520efficient%2520%2528twice%2520as%2520fast%2520as%2520Anti-DreamBooth%2520and%250AMist%2529%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15081v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perturbing%20Attention%20Gives%20You%20More%20Bang%20for%20the%20Buck%3A%20Subtle%20Imaging%0A%20%20Perturbations%20That%20Efficiently%20Fool%20Customized%20Diffusion%20Models&entry.906535625=Jingyao%20Xu%20and%20Yuetong%20Lu%20and%20Yandong%20Li%20and%20Siyang%20Lu%20and%20Dongdong%20Wang%20and%20Xiang%20Wei&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20embark%20a%20new%20era%20of%20generative%20modeling%20and%20offer%20more%0Aopportunities%20for%20efficient%20generating%20high-quality%20and%20realistic%20data%20samples.%0AHowever%2C%20their%20widespread%20use%20has%20also%20brought%20forth%20new%20challenges%20in%20model%0Asecurity%2C%20which%20motivates%20the%20creation%20of%20more%20effective%20adversarial%20attackers%0Aon%20DMs%20to%20understand%20its%20vulnerability.%20We%20propose%20CAAT%2C%20a%20simple%20but%20generic%0Aand%20efficient%20approach%20that%20does%20not%20require%20costly%20training%20to%20effectively%0Afool%20latent%20diffusion%20models%20%28LDMs%29.%20The%20approach%20is%20based%20on%20the%20observation%0Athat%20cross-attention%20layers%20exhibits%20higher%20sensitivity%20to%20gradient%20change%2C%0Aallowing%20for%20leveraging%20subtle%20perturbations%20on%20published%20images%20to%0Asignificantly%20corrupt%20the%20generated%20images.%20We%20show%20that%20a%20subtle%20perturbation%0Aon%20an%20image%20can%20significantly%20impact%20the%20cross-attention%20layers%2C%20thus%20changing%0Athe%20mapping%20between%20text%20and%20image%20during%20the%20fine-tuning%20of%20customized%0Adiffusion%20models.%20Extensive%20experiments%20demonstrate%20that%20CAAT%20is%20compatible%0Awith%20diverse%20diffusion%20models%20and%20outperforms%20baseline%20attack%20methods%20in%20a%20more%0Aeffective%20%28more%20noise%29%20and%20efficient%20%28twice%20as%20fast%20as%20Anti-DreamBooth%20and%0AMist%29%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15081v2&entry.124074799=Read"},
{"title": "TRIP-PAL: Travel Planning with Guarantees by Combining Large Language\n  Models and Automated Planners", "author": "Tomas de la Rosa and Sriram Gopalakrishnan and Alberto Pozanco and Zhen Zeng and Daniel Borrajo", "abstract": "  Travel planning is a complex task that involves generating a sequence of\nactions related to visiting places subject to constraints and maximizing some\nuser satisfaction criteria. Traditional approaches rely on problem formulation\nin a given formal language, extracting relevant travel information from web\nsources, and use an adequate problem solver to generate a valid solution. As an\nalternative, recent Large Language Model (LLM) based approaches directly output\nplans from user requests using language. Although LLMs possess extensive travel\ndomain knowledge and provide high-level information like points of interest and\npotential routes, current state-of-the-art models often generate plans that\nlack coherence, fail to satisfy constraints fully, and do not guarantee the\ngeneration of high-quality solutions. We propose TRIP-PAL, a hybrid method that\ncombines the strengths of LLMs and automated planners, where (i) LLMs get and\ntranslate travel information and user information into data structures that can\nbe fed into planners; and (ii) automated planners generate travel plans that\nguarantee constraint satisfaction and optimize for users' utility. Our\nexperiments across various travel scenarios show that TRIP-PAL outperforms an\nLLM when generating travel plans.\n", "link": "http://arxiv.org/abs/2406.10196v1", "date": "2024-06-14", "relevancy": 1.9798, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5123}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4832}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRIP-PAL%3A%20Travel%20Planning%20with%20Guarantees%20by%20Combining%20Large%20Language%0A%20%20Models%20and%20Automated%20Planners&body=Title%3A%20TRIP-PAL%3A%20Travel%20Planning%20with%20Guarantees%20by%20Combining%20Large%20Language%0A%20%20Models%20and%20Automated%20Planners%0AAuthor%3A%20Tomas%20de%20la%20Rosa%20and%20Sriram%20Gopalakrishnan%20and%20Alberto%20Pozanco%20and%20Zhen%20Zeng%20and%20Daniel%20Borrajo%0AAbstract%3A%20%20%20Travel%20planning%20is%20a%20complex%20task%20that%20involves%20generating%20a%20sequence%20of%0Aactions%20related%20to%20visiting%20places%20subject%20to%20constraints%20and%20maximizing%20some%0Auser%20satisfaction%20criteria.%20Traditional%20approaches%20rely%20on%20problem%20formulation%0Ain%20a%20given%20formal%20language%2C%20extracting%20relevant%20travel%20information%20from%20web%0Asources%2C%20and%20use%20an%20adequate%20problem%20solver%20to%20generate%20a%20valid%20solution.%20As%20an%0Aalternative%2C%20recent%20Large%20Language%20Model%20%28LLM%29%20based%20approaches%20directly%20output%0Aplans%20from%20user%20requests%20using%20language.%20Although%20LLMs%20possess%20extensive%20travel%0Adomain%20knowledge%20and%20provide%20high-level%20information%20like%20points%20of%20interest%20and%0Apotential%20routes%2C%20current%20state-of-the-art%20models%20often%20generate%20plans%20that%0Alack%20coherence%2C%20fail%20to%20satisfy%20constraints%20fully%2C%20and%20do%20not%20guarantee%20the%0Ageneration%20of%20high-quality%20solutions.%20We%20propose%20TRIP-PAL%2C%20a%20hybrid%20method%20that%0Acombines%20the%20strengths%20of%20LLMs%20and%20automated%20planners%2C%20where%20%28i%29%20LLMs%20get%20and%0Atranslate%20travel%20information%20and%20user%20information%20into%20data%20structures%20that%20can%0Abe%20fed%20into%20planners%3B%20and%20%28ii%29%20automated%20planners%20generate%20travel%20plans%20that%0Aguarantee%20constraint%20satisfaction%20and%20optimize%20for%20users%27%20utility.%20Our%0Aexperiments%20across%20various%20travel%20scenarios%20show%20that%20TRIP-PAL%20outperforms%20an%0ALLM%20when%20generating%20travel%20plans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRIP-PAL%253A%2520Travel%2520Planning%2520with%2520Guarantees%2520by%2520Combining%2520Large%2520Language%250A%2520%2520Models%2520and%2520Automated%2520Planners%26entry.906535625%3DTomas%2520de%2520la%2520Rosa%2520and%2520Sriram%2520Gopalakrishnan%2520and%2520Alberto%2520Pozanco%2520and%2520Zhen%2520Zeng%2520and%2520Daniel%2520Borrajo%26entry.1292438233%3D%2520%2520Travel%2520planning%2520is%2520a%2520complex%2520task%2520that%2520involves%2520generating%2520a%2520sequence%2520of%250Aactions%2520related%2520to%2520visiting%2520places%2520subject%2520to%2520constraints%2520and%2520maximizing%2520some%250Auser%2520satisfaction%2520criteria.%2520Traditional%2520approaches%2520rely%2520on%2520problem%2520formulation%250Ain%2520a%2520given%2520formal%2520language%252C%2520extracting%2520relevant%2520travel%2520information%2520from%2520web%250Asources%252C%2520and%2520use%2520an%2520adequate%2520problem%2520solver%2520to%2520generate%2520a%2520valid%2520solution.%2520As%2520an%250Aalternative%252C%2520recent%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520based%2520approaches%2520directly%2520output%250Aplans%2520from%2520user%2520requests%2520using%2520language.%2520Although%2520LLMs%2520possess%2520extensive%2520travel%250Adomain%2520knowledge%2520and%2520provide%2520high-level%2520information%2520like%2520points%2520of%2520interest%2520and%250Apotential%2520routes%252C%2520current%2520state-of-the-art%2520models%2520often%2520generate%2520plans%2520that%250Alack%2520coherence%252C%2520fail%2520to%2520satisfy%2520constraints%2520fully%252C%2520and%2520do%2520not%2520guarantee%2520the%250Ageneration%2520of%2520high-quality%2520solutions.%2520We%2520propose%2520TRIP-PAL%252C%2520a%2520hybrid%2520method%2520that%250Acombines%2520the%2520strengths%2520of%2520LLMs%2520and%2520automated%2520planners%252C%2520where%2520%2528i%2529%2520LLMs%2520get%2520and%250Atranslate%2520travel%2520information%2520and%2520user%2520information%2520into%2520data%2520structures%2520that%2520can%250Abe%2520fed%2520into%2520planners%253B%2520and%2520%2528ii%2529%2520automated%2520planners%2520generate%2520travel%2520plans%2520that%250Aguarantee%2520constraint%2520satisfaction%2520and%2520optimize%2520for%2520users%2527%2520utility.%2520Our%250Aexperiments%2520across%2520various%2520travel%2520scenarios%2520show%2520that%2520TRIP-PAL%2520outperforms%2520an%250ALLM%2520when%2520generating%2520travel%2520plans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRIP-PAL%3A%20Travel%20Planning%20with%20Guarantees%20by%20Combining%20Large%20Language%0A%20%20Models%20and%20Automated%20Planners&entry.906535625=Tomas%20de%20la%20Rosa%20and%20Sriram%20Gopalakrishnan%20and%20Alberto%20Pozanco%20and%20Zhen%20Zeng%20and%20Daniel%20Borrajo&entry.1292438233=%20%20Travel%20planning%20is%20a%20complex%20task%20that%20involves%20generating%20a%20sequence%20of%0Aactions%20related%20to%20visiting%20places%20subject%20to%20constraints%20and%20maximizing%20some%0Auser%20satisfaction%20criteria.%20Traditional%20approaches%20rely%20on%20problem%20formulation%0Ain%20a%20given%20formal%20language%2C%20extracting%20relevant%20travel%20information%20from%20web%0Asources%2C%20and%20use%20an%20adequate%20problem%20solver%20to%20generate%20a%20valid%20solution.%20As%20an%0Aalternative%2C%20recent%20Large%20Language%20Model%20%28LLM%29%20based%20approaches%20directly%20output%0Aplans%20from%20user%20requests%20using%20language.%20Although%20LLMs%20possess%20extensive%20travel%0Adomain%20knowledge%20and%20provide%20high-level%20information%20like%20points%20of%20interest%20and%0Apotential%20routes%2C%20current%20state-of-the-art%20models%20often%20generate%20plans%20that%0Alack%20coherence%2C%20fail%20to%20satisfy%20constraints%20fully%2C%20and%20do%20not%20guarantee%20the%0Ageneration%20of%20high-quality%20solutions.%20We%20propose%20TRIP-PAL%2C%20a%20hybrid%20method%20that%0Acombines%20the%20strengths%20of%20LLMs%20and%20automated%20planners%2C%20where%20%28i%29%20LLMs%20get%20and%0Atranslate%20travel%20information%20and%20user%20information%20into%20data%20structures%20that%20can%0Abe%20fed%20into%20planners%3B%20and%20%28ii%29%20automated%20planners%20generate%20travel%20plans%20that%0Aguarantee%20constraint%20satisfaction%20and%20optimize%20for%20users%27%20utility.%20Our%0Aexperiments%20across%20various%20travel%20scenarios%20show%20that%20TRIP-PAL%20outperforms%20an%0ALLM%20when%20generating%20travel%20plans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10196v1&entry.124074799=Read"},
{"title": "Neural Concept Binder", "author": "Wolfgang Stammer and Antonia W\u00fcst and David Steinmann and Kristian Kersting", "abstract": "  The challenge in object-based visual reasoning lies in generating descriptive\nyet distinct concept representations. Moreover, doing this in an unsupervised\nfashion requires human users to understand a model's learned concepts and\npotentially revise false concepts. In addressing this challenge, we introduce\nthe Neural Concept Binder, a new framework for deriving discrete concept\nrepresentations resulting in what we term \"concept-slot encodings\". These\nencodings leverage both \"soft binding\" via object-centric block-slot encodings\nand \"hard binding\" via retrieval-based inference. The Neural Concept Binder\nfacilitates straightforward concept inspection and direct integration of\nexternal knowledge, such as human input or insights from other AI models like\nGPT-4. Additionally, we demonstrate that incorporating the hard binding\nmechanism does not compromise performance; instead, it enables seamless\nintegration into both neural and symbolic modules for intricate reasoning\ntasks, as evidenced by evaluations on our newly introduced CLEVR-Sudoku\ndataset.\n", "link": "http://arxiv.org/abs/2406.09949v1", "date": "2024-06-14", "relevancy": 1.9594, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5364}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4872}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Concept%20Binder&body=Title%3A%20Neural%20Concept%20Binder%0AAuthor%3A%20Wolfgang%20Stammer%20and%20Antonia%20W%C3%BCst%20and%20David%20Steinmann%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20The%20challenge%20in%20object-based%20visual%20reasoning%20lies%20in%20generating%20descriptive%0Ayet%20distinct%20concept%20representations.%20Moreover%2C%20doing%20this%20in%20an%20unsupervised%0Afashion%20requires%20human%20users%20to%20understand%20a%20model%27s%20learned%20concepts%20and%0Apotentially%20revise%20false%20concepts.%20In%20addressing%20this%20challenge%2C%20we%20introduce%0Athe%20Neural%20Concept%20Binder%2C%20a%20new%20framework%20for%20deriving%20discrete%20concept%0Arepresentations%20resulting%20in%20what%20we%20term%20%22concept-slot%20encodings%22.%20These%0Aencodings%20leverage%20both%20%22soft%20binding%22%20via%20object-centric%20block-slot%20encodings%0Aand%20%22hard%20binding%22%20via%20retrieval-based%20inference.%20The%20Neural%20Concept%20Binder%0Afacilitates%20straightforward%20concept%20inspection%20and%20direct%20integration%20of%0Aexternal%20knowledge%2C%20such%20as%20human%20input%20or%20insights%20from%20other%20AI%20models%20like%0AGPT-4.%20Additionally%2C%20we%20demonstrate%20that%20incorporating%20the%20hard%20binding%0Amechanism%20does%20not%20compromise%20performance%3B%20instead%2C%20it%20enables%20seamless%0Aintegration%20into%20both%20neural%20and%20symbolic%20modules%20for%20intricate%20reasoning%0Atasks%2C%20as%20evidenced%20by%20evaluations%20on%20our%20newly%20introduced%20CLEVR-Sudoku%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Concept%2520Binder%26entry.906535625%3DWolfgang%2520Stammer%2520and%2520Antonia%2520W%25C3%25BCst%2520and%2520David%2520Steinmann%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520The%2520challenge%2520in%2520object-based%2520visual%2520reasoning%2520lies%2520in%2520generating%2520descriptive%250Ayet%2520distinct%2520concept%2520representations.%2520Moreover%252C%2520doing%2520this%2520in%2520an%2520unsupervised%250Afashion%2520requires%2520human%2520users%2520to%2520understand%2520a%2520model%2527s%2520learned%2520concepts%2520and%250Apotentially%2520revise%2520false%2520concepts.%2520In%2520addressing%2520this%2520challenge%252C%2520we%2520introduce%250Athe%2520Neural%2520Concept%2520Binder%252C%2520a%2520new%2520framework%2520for%2520deriving%2520discrete%2520concept%250Arepresentations%2520resulting%2520in%2520what%2520we%2520term%2520%2522concept-slot%2520encodings%2522.%2520These%250Aencodings%2520leverage%2520both%2520%2522soft%2520binding%2522%2520via%2520object-centric%2520block-slot%2520encodings%250Aand%2520%2522hard%2520binding%2522%2520via%2520retrieval-based%2520inference.%2520The%2520Neural%2520Concept%2520Binder%250Afacilitates%2520straightforward%2520concept%2520inspection%2520and%2520direct%2520integration%2520of%250Aexternal%2520knowledge%252C%2520such%2520as%2520human%2520input%2520or%2520insights%2520from%2520other%2520AI%2520models%2520like%250AGPT-4.%2520Additionally%252C%2520we%2520demonstrate%2520that%2520incorporating%2520the%2520hard%2520binding%250Amechanism%2520does%2520not%2520compromise%2520performance%253B%2520instead%252C%2520it%2520enables%2520seamless%250Aintegration%2520into%2520both%2520neural%2520and%2520symbolic%2520modules%2520for%2520intricate%2520reasoning%250Atasks%252C%2520as%2520evidenced%2520by%2520evaluations%2520on%2520our%2520newly%2520introduced%2520CLEVR-Sudoku%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Concept%20Binder&entry.906535625=Wolfgang%20Stammer%20and%20Antonia%20W%C3%BCst%20and%20David%20Steinmann%20and%20Kristian%20Kersting&entry.1292438233=%20%20The%20challenge%20in%20object-based%20visual%20reasoning%20lies%20in%20generating%20descriptive%0Ayet%20distinct%20concept%20representations.%20Moreover%2C%20doing%20this%20in%20an%20unsupervised%0Afashion%20requires%20human%20users%20to%20understand%20a%20model%27s%20learned%20concepts%20and%0Apotentially%20revise%20false%20concepts.%20In%20addressing%20this%20challenge%2C%20we%20introduce%0Athe%20Neural%20Concept%20Binder%2C%20a%20new%20framework%20for%20deriving%20discrete%20concept%0Arepresentations%20resulting%20in%20what%20we%20term%20%22concept-slot%20encodings%22.%20These%0Aencodings%20leverage%20both%20%22soft%20binding%22%20via%20object-centric%20block-slot%20encodings%0Aand%20%22hard%20binding%22%20via%20retrieval-based%20inference.%20The%20Neural%20Concept%20Binder%0Afacilitates%20straightforward%20concept%20inspection%20and%20direct%20integration%20of%0Aexternal%20knowledge%2C%20such%20as%20human%20input%20or%20insights%20from%20other%20AI%20models%20like%0AGPT-4.%20Additionally%2C%20we%20demonstrate%20that%20incorporating%20the%20hard%20binding%0Amechanism%20does%20not%20compromise%20performance%3B%20instead%2C%20it%20enables%20seamless%0Aintegration%20into%20both%20neural%20and%20symbolic%20modules%20for%20intricate%20reasoning%0Atasks%2C%20as%20evidenced%20by%20evaluations%20on%20our%20newly%20introduced%20CLEVR-Sudoku%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09949v1&entry.124074799=Read"},
{"title": "Neural Operators for PDE Backstepping Control of First-Order Hyperbolic\n  PIDE with Recycle and Delay", "author": "Jie Qi and Jing Zhang and Miroslav Krstic", "abstract": "  The recently introduced DeepONet operator-learning framework for PDE control\nis extended from the results for basic hyperbolic and parabolic PDEs to an\nadvanced hyperbolic class that involves delays on both the state and the system\noutput or input. The PDE backstepping design produces gain functions that are\noutputs of a nonlinear operator, mapping functions on a spatial domain into\nfunctions on a spatial domain, and where this gain-generating operator's inputs\nare the PDE's coefficients. The operator is approximated with a DeepONet neural\nnetwork to a degree of accuracy that is provably arbitrarily tight. Once we\nproduce this approximation-theoretic result in infinite dimension, with it we\nestablish stability in closed loop under feedback that employs approximate\ngains. In addition to supplying such results under full-state feedback, we also\ndevelop DeepONet-approximated observers and output-feedback laws and prove\ntheir own stabilizing properties under neural operator approximations. With\nnumerical simulations we illustrate the theoretical results and quantify the\nnumerical effort savings, which are of two orders of magnitude, thanks to\nreplacing the numerical PDE solving with the DeepONet.\n", "link": "http://arxiv.org/abs/2307.11436v2", "date": "2024-06-14", "relevancy": 1.9586, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5088}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4805}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Operators%20for%20PDE%20Backstepping%20Control%20of%20First-Order%20Hyperbolic%0A%20%20PIDE%20with%20Recycle%20and%20Delay&body=Title%3A%20Neural%20Operators%20for%20PDE%20Backstepping%20Control%20of%20First-Order%20Hyperbolic%0A%20%20PIDE%20with%20Recycle%20and%20Delay%0AAuthor%3A%20Jie%20Qi%20and%20Jing%20Zhang%20and%20Miroslav%20Krstic%0AAbstract%3A%20%20%20The%20recently%20introduced%20DeepONet%20operator-learning%20framework%20for%20PDE%20control%0Ais%20extended%20from%20the%20results%20for%20basic%20hyperbolic%20and%20parabolic%20PDEs%20to%20an%0Aadvanced%20hyperbolic%20class%20that%20involves%20delays%20on%20both%20the%20state%20and%20the%20system%0Aoutput%20or%20input.%20The%20PDE%20backstepping%20design%20produces%20gain%20functions%20that%20are%0Aoutputs%20of%20a%20nonlinear%20operator%2C%20mapping%20functions%20on%20a%20spatial%20domain%20into%0Afunctions%20on%20a%20spatial%20domain%2C%20and%20where%20this%20gain-generating%20operator%27s%20inputs%0Aare%20the%20PDE%27s%20coefficients.%20The%20operator%20is%20approximated%20with%20a%20DeepONet%20neural%0Anetwork%20to%20a%20degree%20of%20accuracy%20that%20is%20provably%20arbitrarily%20tight.%20Once%20we%0Aproduce%20this%20approximation-theoretic%20result%20in%20infinite%20dimension%2C%20with%20it%20we%0Aestablish%20stability%20in%20closed%20loop%20under%20feedback%20that%20employs%20approximate%0Agains.%20In%20addition%20to%20supplying%20such%20results%20under%20full-state%20feedback%2C%20we%20also%0Adevelop%20DeepONet-approximated%20observers%20and%20output-feedback%20laws%20and%20prove%0Atheir%20own%20stabilizing%20properties%20under%20neural%20operator%20approximations.%20With%0Anumerical%20simulations%20we%20illustrate%20the%20theoretical%20results%20and%20quantify%20the%0Anumerical%20effort%20savings%2C%20which%20are%20of%20two%20orders%20of%20magnitude%2C%20thanks%20to%0Areplacing%20the%20numerical%20PDE%20solving%20with%20the%20DeepONet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.11436v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Operators%2520for%2520PDE%2520Backstepping%2520Control%2520of%2520First-Order%2520Hyperbolic%250A%2520%2520PIDE%2520with%2520Recycle%2520and%2520Delay%26entry.906535625%3DJie%2520Qi%2520and%2520Jing%2520Zhang%2520and%2520Miroslav%2520Krstic%26entry.1292438233%3D%2520%2520The%2520recently%2520introduced%2520DeepONet%2520operator-learning%2520framework%2520for%2520PDE%2520control%250Ais%2520extended%2520from%2520the%2520results%2520for%2520basic%2520hyperbolic%2520and%2520parabolic%2520PDEs%2520to%2520an%250Aadvanced%2520hyperbolic%2520class%2520that%2520involves%2520delays%2520on%2520both%2520the%2520state%2520and%2520the%2520system%250Aoutput%2520or%2520input.%2520The%2520PDE%2520backstepping%2520design%2520produces%2520gain%2520functions%2520that%2520are%250Aoutputs%2520of%2520a%2520nonlinear%2520operator%252C%2520mapping%2520functions%2520on%2520a%2520spatial%2520domain%2520into%250Afunctions%2520on%2520a%2520spatial%2520domain%252C%2520and%2520where%2520this%2520gain-generating%2520operator%2527s%2520inputs%250Aare%2520the%2520PDE%2527s%2520coefficients.%2520The%2520operator%2520is%2520approximated%2520with%2520a%2520DeepONet%2520neural%250Anetwork%2520to%2520a%2520degree%2520of%2520accuracy%2520that%2520is%2520provably%2520arbitrarily%2520tight.%2520Once%2520we%250Aproduce%2520this%2520approximation-theoretic%2520result%2520in%2520infinite%2520dimension%252C%2520with%2520it%2520we%250Aestablish%2520stability%2520in%2520closed%2520loop%2520under%2520feedback%2520that%2520employs%2520approximate%250Agains.%2520In%2520addition%2520to%2520supplying%2520such%2520results%2520under%2520full-state%2520feedback%252C%2520we%2520also%250Adevelop%2520DeepONet-approximated%2520observers%2520and%2520output-feedback%2520laws%2520and%2520prove%250Atheir%2520own%2520stabilizing%2520properties%2520under%2520neural%2520operator%2520approximations.%2520With%250Anumerical%2520simulations%2520we%2520illustrate%2520the%2520theoretical%2520results%2520and%2520quantify%2520the%250Anumerical%2520effort%2520savings%252C%2520which%2520are%2520of%2520two%2520orders%2520of%2520magnitude%252C%2520thanks%2520to%250Areplacing%2520the%2520numerical%2520PDE%2520solving%2520with%2520the%2520DeepONet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.11436v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Operators%20for%20PDE%20Backstepping%20Control%20of%20First-Order%20Hyperbolic%0A%20%20PIDE%20with%20Recycle%20and%20Delay&entry.906535625=Jie%20Qi%20and%20Jing%20Zhang%20and%20Miroslav%20Krstic&entry.1292438233=%20%20The%20recently%20introduced%20DeepONet%20operator-learning%20framework%20for%20PDE%20control%0Ais%20extended%20from%20the%20results%20for%20basic%20hyperbolic%20and%20parabolic%20PDEs%20to%20an%0Aadvanced%20hyperbolic%20class%20that%20involves%20delays%20on%20both%20the%20state%20and%20the%20system%0Aoutput%20or%20input.%20The%20PDE%20backstepping%20design%20produces%20gain%20functions%20that%20are%0Aoutputs%20of%20a%20nonlinear%20operator%2C%20mapping%20functions%20on%20a%20spatial%20domain%20into%0Afunctions%20on%20a%20spatial%20domain%2C%20and%20where%20this%20gain-generating%20operator%27s%20inputs%0Aare%20the%20PDE%27s%20coefficients.%20The%20operator%20is%20approximated%20with%20a%20DeepONet%20neural%0Anetwork%20to%20a%20degree%20of%20accuracy%20that%20is%20provably%20arbitrarily%20tight.%20Once%20we%0Aproduce%20this%20approximation-theoretic%20result%20in%20infinite%20dimension%2C%20with%20it%20we%0Aestablish%20stability%20in%20closed%20loop%20under%20feedback%20that%20employs%20approximate%0Agains.%20In%20addition%20to%20supplying%20such%20results%20under%20full-state%20feedback%2C%20we%20also%0Adevelop%20DeepONet-approximated%20observers%20and%20output-feedback%20laws%20and%20prove%0Atheir%20own%20stabilizing%20properties%20under%20neural%20operator%20approximations.%20With%0Anumerical%20simulations%20we%20illustrate%20the%20theoretical%20results%20and%20quantify%20the%0Anumerical%20effort%20savings%2C%20which%20are%20of%20two%20orders%20of%20magnitude%2C%20thanks%20to%0Areplacing%20the%20numerical%20PDE%20solving%20with%20the%20DeepONet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.11436v2&entry.124074799=Read"},
{"title": "NeuralClothSim: Neural Deformation Fields Meet the Thin Shell Theory", "author": "Navami Kairanda and Marc Habermann and Christian Theobalt and Vladislav Golyanik", "abstract": "  Despite existing 3D cloth simulators producing realistic results, they\npredominantly operate on discrete surface representations (e.g. points and\nmeshes) with a fixed spatial resolution, which often leads to large memory\nconsumption and resolution-dependent simulations. Moreover, back-propagating\ngradients through the existing solvers is difficult, and they cannot be easily\nintegrated into modern neural architectures. In response, this paper re-thinks\nphysically plausible cloth simulation: We propose NeuralClothSim, i.e., a new\nquasistatic cloth simulator using thin shells, in which surface deformation is\nencoded in neural network weights in the form of a neural field. Our\nmemory-efficient solver operates on a new continuous coordinate-based surface\nrepresentation called neural deformation fields (NDFs); it supervises NDF\nequilibria with the laws of the non-linear Kirchhoff-Love shell theory with a\nnon-linear anisotropic material model. NDFs are adaptive: They 1) allocate\ntheir capacity to the deformation details and 2) allow surface state queries at\narbitrary spatial resolutions without re-training. We show how to train\nNeuralClothSim while imposing hard boundary conditions and demonstrate multiple\napplications, such as material interpolation and simulation editing. The\nexperimental results highlight the effectiveness of our continuous neural\nformulation.\n", "link": "http://arxiv.org/abs/2308.12970v2", "date": "2024-06-14", "relevancy": 1.9556, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5037}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4971}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuralClothSim%3A%20Neural%20Deformation%20Fields%20Meet%20the%20Thin%20Shell%20Theory&body=Title%3A%20NeuralClothSim%3A%20Neural%20Deformation%20Fields%20Meet%20the%20Thin%20Shell%20Theory%0AAuthor%3A%20Navami%20Kairanda%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%20and%20Vladislav%20Golyanik%0AAbstract%3A%20%20%20Despite%20existing%203D%20cloth%20simulators%20producing%20realistic%20results%2C%20they%0Apredominantly%20operate%20on%20discrete%20surface%20representations%20%28e.g.%20points%20and%0Ameshes%29%20with%20a%20fixed%20spatial%20resolution%2C%20which%20often%20leads%20to%20large%20memory%0Aconsumption%20and%20resolution-dependent%20simulations.%20Moreover%2C%20back-propagating%0Agradients%20through%20the%20existing%20solvers%20is%20difficult%2C%20and%20they%20cannot%20be%20easily%0Aintegrated%20into%20modern%20neural%20architectures.%20In%20response%2C%20this%20paper%20re-thinks%0Aphysically%20plausible%20cloth%20simulation%3A%20We%20propose%20NeuralClothSim%2C%20i.e.%2C%20a%20new%0Aquasistatic%20cloth%20simulator%20using%20thin%20shells%2C%20in%20which%20surface%20deformation%20is%0Aencoded%20in%20neural%20network%20weights%20in%20the%20form%20of%20a%20neural%20field.%20Our%0Amemory-efficient%20solver%20operates%20on%20a%20new%20continuous%20coordinate-based%20surface%0Arepresentation%20called%20neural%20deformation%20fields%20%28NDFs%29%3B%20it%20supervises%20NDF%0Aequilibria%20with%20the%20laws%20of%20the%20non-linear%20Kirchhoff-Love%20shell%20theory%20with%20a%0Anon-linear%20anisotropic%20material%20model.%20NDFs%20are%20adaptive%3A%20They%201%29%20allocate%0Atheir%20capacity%20to%20the%20deformation%20details%20and%202%29%20allow%20surface%20state%20queries%20at%0Aarbitrary%20spatial%20resolutions%20without%20re-training.%20We%20show%20how%20to%20train%0ANeuralClothSim%20while%20imposing%20hard%20boundary%20conditions%20and%20demonstrate%20multiple%0Aapplications%2C%20such%20as%20material%20interpolation%20and%20simulation%20editing.%20The%0Aexperimental%20results%20highlight%20the%20effectiveness%20of%20our%20continuous%20neural%0Aformulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.12970v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuralClothSim%253A%2520Neural%2520Deformation%2520Fields%2520Meet%2520the%2520Thin%2520Shell%2520Theory%26entry.906535625%3DNavami%2520Kairanda%2520and%2520Marc%2520Habermann%2520and%2520Christian%2520Theobalt%2520and%2520Vladislav%2520Golyanik%26entry.1292438233%3D%2520%2520Despite%2520existing%25203D%2520cloth%2520simulators%2520producing%2520realistic%2520results%252C%2520they%250Apredominantly%2520operate%2520on%2520discrete%2520surface%2520representations%2520%2528e.g.%2520points%2520and%250Ameshes%2529%2520with%2520a%2520fixed%2520spatial%2520resolution%252C%2520which%2520often%2520leads%2520to%2520large%2520memory%250Aconsumption%2520and%2520resolution-dependent%2520simulations.%2520Moreover%252C%2520back-propagating%250Agradients%2520through%2520the%2520existing%2520solvers%2520is%2520difficult%252C%2520and%2520they%2520cannot%2520be%2520easily%250Aintegrated%2520into%2520modern%2520neural%2520architectures.%2520In%2520response%252C%2520this%2520paper%2520re-thinks%250Aphysically%2520plausible%2520cloth%2520simulation%253A%2520We%2520propose%2520NeuralClothSim%252C%2520i.e.%252C%2520a%2520new%250Aquasistatic%2520cloth%2520simulator%2520using%2520thin%2520shells%252C%2520in%2520which%2520surface%2520deformation%2520is%250Aencoded%2520in%2520neural%2520network%2520weights%2520in%2520the%2520form%2520of%2520a%2520neural%2520field.%2520Our%250Amemory-efficient%2520solver%2520operates%2520on%2520a%2520new%2520continuous%2520coordinate-based%2520surface%250Arepresentation%2520called%2520neural%2520deformation%2520fields%2520%2528NDFs%2529%253B%2520it%2520supervises%2520NDF%250Aequilibria%2520with%2520the%2520laws%2520of%2520the%2520non-linear%2520Kirchhoff-Love%2520shell%2520theory%2520with%2520a%250Anon-linear%2520anisotropic%2520material%2520model.%2520NDFs%2520are%2520adaptive%253A%2520They%25201%2529%2520allocate%250Atheir%2520capacity%2520to%2520the%2520deformation%2520details%2520and%25202%2529%2520allow%2520surface%2520state%2520queries%2520at%250Aarbitrary%2520spatial%2520resolutions%2520without%2520re-training.%2520We%2520show%2520how%2520to%2520train%250ANeuralClothSim%2520while%2520imposing%2520hard%2520boundary%2520conditions%2520and%2520demonstrate%2520multiple%250Aapplications%252C%2520such%2520as%2520material%2520interpolation%2520and%2520simulation%2520editing.%2520The%250Aexperimental%2520results%2520highlight%2520the%2520effectiveness%2520of%2520our%2520continuous%2520neural%250Aformulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.12970v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuralClothSim%3A%20Neural%20Deformation%20Fields%20Meet%20the%20Thin%20Shell%20Theory&entry.906535625=Navami%20Kairanda%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%20and%20Vladislav%20Golyanik&entry.1292438233=%20%20Despite%20existing%203D%20cloth%20simulators%20producing%20realistic%20results%2C%20they%0Apredominantly%20operate%20on%20discrete%20surface%20representations%20%28e.g.%20points%20and%0Ameshes%29%20with%20a%20fixed%20spatial%20resolution%2C%20which%20often%20leads%20to%20large%20memory%0Aconsumption%20and%20resolution-dependent%20simulations.%20Moreover%2C%20back-propagating%0Agradients%20through%20the%20existing%20solvers%20is%20difficult%2C%20and%20they%20cannot%20be%20easily%0Aintegrated%20into%20modern%20neural%20architectures.%20In%20response%2C%20this%20paper%20re-thinks%0Aphysically%20plausible%20cloth%20simulation%3A%20We%20propose%20NeuralClothSim%2C%20i.e.%2C%20a%20new%0Aquasistatic%20cloth%20simulator%20using%20thin%20shells%2C%20in%20which%20surface%20deformation%20is%0Aencoded%20in%20neural%20network%20weights%20in%20the%20form%20of%20a%20neural%20field.%20Our%0Amemory-efficient%20solver%20operates%20on%20a%20new%20continuous%20coordinate-based%20surface%0Arepresentation%20called%20neural%20deformation%20fields%20%28NDFs%29%3B%20it%20supervises%20NDF%0Aequilibria%20with%20the%20laws%20of%20the%20non-linear%20Kirchhoff-Love%20shell%20theory%20with%20a%0Anon-linear%20anisotropic%20material%20model.%20NDFs%20are%20adaptive%3A%20They%201%29%20allocate%0Atheir%20capacity%20to%20the%20deformation%20details%20and%202%29%20allow%20surface%20state%20queries%20at%0Aarbitrary%20spatial%20resolutions%20without%20re-training.%20We%20show%20how%20to%20train%0ANeuralClothSim%20while%20imposing%20hard%20boundary%20conditions%20and%20demonstrate%20multiple%0Aapplications%2C%20such%20as%20material%20interpolation%20and%20simulation%20editing.%20The%0Aexperimental%20results%20highlight%20the%20effectiveness%20of%20our%20continuous%20neural%0Aformulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.12970v2&entry.124074799=Read"},
{"title": "Experiments in News Bias Detection with Pre-Trained Neural Transformers", "author": "Tim Menzner and Jochen L. Leidner", "abstract": "  The World Wide Web provides unrivalled access to information globally,\nincluding factual news reporting and commentary. However, state actors and\ncommercial players increasingly spread biased (distorted) or fake (non-factual)\ninformation to promote their agendas. We compare several large, pre-trained\nlanguage models on the task of sentence-level news bias detection and sub-type\nclassification, providing quantitative and qualitative results.\n", "link": "http://arxiv.org/abs/2406.09938v1", "date": "2024-06-14", "relevancy": 1.9517, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5095}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4864}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Experiments%20in%20News%20Bias%20Detection%20with%20Pre-Trained%20Neural%20Transformers&body=Title%3A%20Experiments%20in%20News%20Bias%20Detection%20with%20Pre-Trained%20Neural%20Transformers%0AAuthor%3A%20Tim%20Menzner%20and%20Jochen%20L.%20Leidner%0AAbstract%3A%20%20%20The%20World%20Wide%20Web%20provides%20unrivalled%20access%20to%20information%20globally%2C%0Aincluding%20factual%20news%20reporting%20and%20commentary.%20However%2C%20state%20actors%20and%0Acommercial%20players%20increasingly%20spread%20biased%20%28distorted%29%20or%20fake%20%28non-factual%29%0Ainformation%20to%20promote%20their%20agendas.%20We%20compare%20several%20large%2C%20pre-trained%0Alanguage%20models%20on%20the%20task%20of%20sentence-level%20news%20bias%20detection%20and%20sub-type%0Aclassification%2C%20providing%20quantitative%20and%20qualitative%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExperiments%2520in%2520News%2520Bias%2520Detection%2520with%2520Pre-Trained%2520Neural%2520Transformers%26entry.906535625%3DTim%2520Menzner%2520and%2520Jochen%2520L.%2520Leidner%26entry.1292438233%3D%2520%2520The%2520World%2520Wide%2520Web%2520provides%2520unrivalled%2520access%2520to%2520information%2520globally%252C%250Aincluding%2520factual%2520news%2520reporting%2520and%2520commentary.%2520However%252C%2520state%2520actors%2520and%250Acommercial%2520players%2520increasingly%2520spread%2520biased%2520%2528distorted%2529%2520or%2520fake%2520%2528non-factual%2529%250Ainformation%2520to%2520promote%2520their%2520agendas.%2520We%2520compare%2520several%2520large%252C%2520pre-trained%250Alanguage%2520models%2520on%2520the%2520task%2520of%2520sentence-level%2520news%2520bias%2520detection%2520and%2520sub-type%250Aclassification%252C%2520providing%2520quantitative%2520and%2520qualitative%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Experiments%20in%20News%20Bias%20Detection%20with%20Pre-Trained%20Neural%20Transformers&entry.906535625=Tim%20Menzner%20and%20Jochen%20L.%20Leidner&entry.1292438233=%20%20The%20World%20Wide%20Web%20provides%20unrivalled%20access%20to%20information%20globally%2C%0Aincluding%20factual%20news%20reporting%20and%20commentary.%20However%2C%20state%20actors%20and%0Acommercial%20players%20increasingly%20spread%20biased%20%28distorted%29%20or%20fake%20%28non-factual%29%0Ainformation%20to%20promote%20their%20agendas.%20We%20compare%20several%20large%2C%20pre-trained%0Alanguage%20models%20on%20the%20task%20of%20sentence-level%20news%20bias%20detection%20and%20sub-type%0Aclassification%2C%20providing%20quantitative%20and%20qualitative%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09938v1&entry.124074799=Read"},
{"title": "Short Film Dataset (SFD): A Benchmark for Story-Level Video\n  Understanding", "author": "Ridouane Ghermi and Xi Wang and Vicky Kalogeiton and Ivan Laptev", "abstract": "  Recent advances in vision-language models have significantly propelled video\nunderstanding. Existing datasets and tasks, however, have notable limitations.\nMost datasets are confined to short videos with limited events and narrow\nnarratives. For example, datasets with instructional and egocentric videos\noften document the activities of one person in a single scene. Although some\nmovie datasets offer richer content, they are often limited to short-term\ntasks, lack publicly available videos and frequently encounter data leakage\ngiven the use of movie forums and other resources in LLM training. To address\nthe above limitations, we propose the Short Film Dataset (SFD) with 1,078\npublicly available amateur movies, a wide variety of genres and minimal data\nleakage issues. SFD offers long-term story-oriented video tasks in the form of\nmultiple-choice and open-ended question answering. Our extensive experiments\nemphasize the need for long-term reasoning to solve SFD tasks. Notably, we find\nstrong signals in movie transcripts leading to the on-par performance of people\nand LLMs. We also show significantly lower performance of current models\ncompared to people when using vision data alone.\n", "link": "http://arxiv.org/abs/2406.10221v1", "date": "2024-06-14", "relevancy": 1.9502, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5019}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4957}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Short%20Film%20Dataset%20%28SFD%29%3A%20A%20Benchmark%20for%20Story-Level%20Video%0A%20%20Understanding&body=Title%3A%20Short%20Film%20Dataset%20%28SFD%29%3A%20A%20Benchmark%20for%20Story-Level%20Video%0A%20%20Understanding%0AAuthor%3A%20Ridouane%20Ghermi%20and%20Xi%20Wang%20and%20Vicky%20Kalogeiton%20and%20Ivan%20Laptev%0AAbstract%3A%20%20%20Recent%20advances%20in%20vision-language%20models%20have%20significantly%20propelled%20video%0Aunderstanding.%20Existing%20datasets%20and%20tasks%2C%20however%2C%20have%20notable%20limitations.%0AMost%20datasets%20are%20confined%20to%20short%20videos%20with%20limited%20events%20and%20narrow%0Anarratives.%20For%20example%2C%20datasets%20with%20instructional%20and%20egocentric%20videos%0Aoften%20document%20the%20activities%20of%20one%20person%20in%20a%20single%20scene.%20Although%20some%0Amovie%20datasets%20offer%20richer%20content%2C%20they%20are%20often%20limited%20to%20short-term%0Atasks%2C%20lack%20publicly%20available%20videos%20and%20frequently%20encounter%20data%20leakage%0Agiven%20the%20use%20of%20movie%20forums%20and%20other%20resources%20in%20LLM%20training.%20To%20address%0Athe%20above%20limitations%2C%20we%20propose%20the%20Short%20Film%20Dataset%20%28SFD%29%20with%201%2C078%0Apublicly%20available%20amateur%20movies%2C%20a%20wide%20variety%20of%20genres%20and%20minimal%20data%0Aleakage%20issues.%20SFD%20offers%20long-term%20story-oriented%20video%20tasks%20in%20the%20form%20of%0Amultiple-choice%20and%20open-ended%20question%20answering.%20Our%20extensive%20experiments%0Aemphasize%20the%20need%20for%20long-term%20reasoning%20to%20solve%20SFD%20tasks.%20Notably%2C%20we%20find%0Astrong%20signals%20in%20movie%20transcripts%20leading%20to%20the%20on-par%20performance%20of%20people%0Aand%20LLMs.%20We%20also%20show%20significantly%20lower%20performance%20of%20current%20models%0Acompared%20to%20people%20when%20using%20vision%20data%20alone.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShort%2520Film%2520Dataset%2520%2528SFD%2529%253A%2520A%2520Benchmark%2520for%2520Story-Level%2520Video%250A%2520%2520Understanding%26entry.906535625%3DRidouane%2520Ghermi%2520and%2520Xi%2520Wang%2520and%2520Vicky%2520Kalogeiton%2520and%2520Ivan%2520Laptev%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520vision-language%2520models%2520have%2520significantly%2520propelled%2520video%250Aunderstanding.%2520Existing%2520datasets%2520and%2520tasks%252C%2520however%252C%2520have%2520notable%2520limitations.%250AMost%2520datasets%2520are%2520confined%2520to%2520short%2520videos%2520with%2520limited%2520events%2520and%2520narrow%250Anarratives.%2520For%2520example%252C%2520datasets%2520with%2520instructional%2520and%2520egocentric%2520videos%250Aoften%2520document%2520the%2520activities%2520of%2520one%2520person%2520in%2520a%2520single%2520scene.%2520Although%2520some%250Amovie%2520datasets%2520offer%2520richer%2520content%252C%2520they%2520are%2520often%2520limited%2520to%2520short-term%250Atasks%252C%2520lack%2520publicly%2520available%2520videos%2520and%2520frequently%2520encounter%2520data%2520leakage%250Agiven%2520the%2520use%2520of%2520movie%2520forums%2520and%2520other%2520resources%2520in%2520LLM%2520training.%2520To%2520address%250Athe%2520above%2520limitations%252C%2520we%2520propose%2520the%2520Short%2520Film%2520Dataset%2520%2528SFD%2529%2520with%25201%252C078%250Apublicly%2520available%2520amateur%2520movies%252C%2520a%2520wide%2520variety%2520of%2520genres%2520and%2520minimal%2520data%250Aleakage%2520issues.%2520SFD%2520offers%2520long-term%2520story-oriented%2520video%2520tasks%2520in%2520the%2520form%2520of%250Amultiple-choice%2520and%2520open-ended%2520question%2520answering.%2520Our%2520extensive%2520experiments%250Aemphasize%2520the%2520need%2520for%2520long-term%2520reasoning%2520to%2520solve%2520SFD%2520tasks.%2520Notably%252C%2520we%2520find%250Astrong%2520signals%2520in%2520movie%2520transcripts%2520leading%2520to%2520the%2520on-par%2520performance%2520of%2520people%250Aand%2520LLMs.%2520We%2520also%2520show%2520significantly%2520lower%2520performance%2520of%2520current%2520models%250Acompared%2520to%2520people%2520when%2520using%2520vision%2520data%2520alone.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Short%20Film%20Dataset%20%28SFD%29%3A%20A%20Benchmark%20for%20Story-Level%20Video%0A%20%20Understanding&entry.906535625=Ridouane%20Ghermi%20and%20Xi%20Wang%20and%20Vicky%20Kalogeiton%20and%20Ivan%20Laptev&entry.1292438233=%20%20Recent%20advances%20in%20vision-language%20models%20have%20significantly%20propelled%20video%0Aunderstanding.%20Existing%20datasets%20and%20tasks%2C%20however%2C%20have%20notable%20limitations.%0AMost%20datasets%20are%20confined%20to%20short%20videos%20with%20limited%20events%20and%20narrow%0Anarratives.%20For%20example%2C%20datasets%20with%20instructional%20and%20egocentric%20videos%0Aoften%20document%20the%20activities%20of%20one%20person%20in%20a%20single%20scene.%20Although%20some%0Amovie%20datasets%20offer%20richer%20content%2C%20they%20are%20often%20limited%20to%20short-term%0Atasks%2C%20lack%20publicly%20available%20videos%20and%20frequently%20encounter%20data%20leakage%0Agiven%20the%20use%20of%20movie%20forums%20and%20other%20resources%20in%20LLM%20training.%20To%20address%0Athe%20above%20limitations%2C%20we%20propose%20the%20Short%20Film%20Dataset%20%28SFD%29%20with%201%2C078%0Apublicly%20available%20amateur%20movies%2C%20a%20wide%20variety%20of%20genres%20and%20minimal%20data%0Aleakage%20issues.%20SFD%20offers%20long-term%20story-oriented%20video%20tasks%20in%20the%20form%20of%0Amultiple-choice%20and%20open-ended%20question%20answering.%20Our%20extensive%20experiments%0Aemphasize%20the%20need%20for%20long-term%20reasoning%20to%20solve%20SFD%20tasks.%20Notably%2C%20we%20find%0Astrong%20signals%20in%20movie%20transcripts%20leading%20to%20the%20on-par%20performance%20of%20people%0Aand%20LLMs.%20We%20also%20show%20significantly%20lower%20performance%20of%20current%20models%0Acompared%20to%20people%20when%20using%20vision%20data%20alone.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10221v1&entry.124074799=Read"},
{"title": "Learning Solution-Aware Transformers for Efficiently Solving Quadratic\n  Assignment Problem", "author": "Zhentao Tan and Yadong Mu", "abstract": "  Recently various optimization problems, such as Mixed Integer Linear\nProgramming Problems (MILPs), have undergone comprehensive investigation,\nleveraging the capabilities of machine learning. This work focuses on\nlearning-based solutions for efficiently solving the Quadratic Assignment\nProblem (QAPs), which stands as a formidable challenge in combinatorial\noptimization. While many instances of simpler problems admit fully\npolynomial-time approximate solution (FPTAS), QAP is shown to be strongly\nNP-hard. Even finding a FPTAS for QAP is difficult, in the sense that the\nexistence of a FPTAS implies $P = NP$. Current research on QAPs suffer from\nlimited scale and computational inefficiency. To attack the aforementioned\nissues, we here propose the first solution of its kind for QAP in the\nlearn-to-improve category. This work encodes facility and location nodes\nseparately, instead of forming computationally intensive association graphs\nprevalent in current approaches. This design choice enables scalability to\nlarger problem sizes. Furthermore, a \\textbf{S}olution \\textbf{AW}are\n\\textbf{T}ransformer (SAWT) architecture integrates the incumbent solution\nmatrix with the attention score to effectively capture higher-order information\nof the QAPs. Our model's effectiveness is validated through extensive\nexperiments on self-generated QAP instances of varying sizes and the QAPLIB\nbenchmark.\n", "link": "http://arxiv.org/abs/2406.09899v1", "date": "2024-06-14", "relevancy": 1.9421, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4711}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Solution-Aware%20Transformers%20for%20Efficiently%20Solving%20Quadratic%0A%20%20Assignment%20Problem&body=Title%3A%20Learning%20Solution-Aware%20Transformers%20for%20Efficiently%20Solving%20Quadratic%0A%20%20Assignment%20Problem%0AAuthor%3A%20Zhentao%20Tan%20and%20Yadong%20Mu%0AAbstract%3A%20%20%20Recently%20various%20optimization%20problems%2C%20such%20as%20Mixed%20Integer%20Linear%0AProgramming%20Problems%20%28MILPs%29%2C%20have%20undergone%20comprehensive%20investigation%2C%0Aleveraging%20the%20capabilities%20of%20machine%20learning.%20This%20work%20focuses%20on%0Alearning-based%20solutions%20for%20efficiently%20solving%20the%20Quadratic%20Assignment%0AProblem%20%28QAPs%29%2C%20which%20stands%20as%20a%20formidable%20challenge%20in%20combinatorial%0Aoptimization.%20While%20many%20instances%20of%20simpler%20problems%20admit%20fully%0Apolynomial-time%20approximate%20solution%20%28FPTAS%29%2C%20QAP%20is%20shown%20to%20be%20strongly%0ANP-hard.%20Even%20finding%20a%20FPTAS%20for%20QAP%20is%20difficult%2C%20in%20the%20sense%20that%20the%0Aexistence%20of%20a%20FPTAS%20implies%20%24P%20%3D%20NP%24.%20Current%20research%20on%20QAPs%20suffer%20from%0Alimited%20scale%20and%20computational%20inefficiency.%20To%20attack%20the%20aforementioned%0Aissues%2C%20we%20here%20propose%20the%20first%20solution%20of%20its%20kind%20for%20QAP%20in%20the%0Alearn-to-improve%20category.%20This%20work%20encodes%20facility%20and%20location%20nodes%0Aseparately%2C%20instead%20of%20forming%20computationally%20intensive%20association%20graphs%0Aprevalent%20in%20current%20approaches.%20This%20design%20choice%20enables%20scalability%20to%0Alarger%20problem%20sizes.%20Furthermore%2C%20a%20%5Ctextbf%7BS%7Dolution%20%5Ctextbf%7BAW%7Dare%0A%5Ctextbf%7BT%7Dransformer%20%28SAWT%29%20architecture%20integrates%20the%20incumbent%20solution%0Amatrix%20with%20the%20attention%20score%20to%20effectively%20capture%20higher-order%20information%0Aof%20the%20QAPs.%20Our%20model%27s%20effectiveness%20is%20validated%20through%20extensive%0Aexperiments%20on%20self-generated%20QAP%20instances%20of%20varying%20sizes%20and%20the%20QAPLIB%0Abenchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Solution-Aware%2520Transformers%2520for%2520Efficiently%2520Solving%2520Quadratic%250A%2520%2520Assignment%2520Problem%26entry.906535625%3DZhentao%2520Tan%2520and%2520Yadong%2520Mu%26entry.1292438233%3D%2520%2520Recently%2520various%2520optimization%2520problems%252C%2520such%2520as%2520Mixed%2520Integer%2520Linear%250AProgramming%2520Problems%2520%2528MILPs%2529%252C%2520have%2520undergone%2520comprehensive%2520investigation%252C%250Aleveraging%2520the%2520capabilities%2520of%2520machine%2520learning.%2520This%2520work%2520focuses%2520on%250Alearning-based%2520solutions%2520for%2520efficiently%2520solving%2520the%2520Quadratic%2520Assignment%250AProblem%2520%2528QAPs%2529%252C%2520which%2520stands%2520as%2520a%2520formidable%2520challenge%2520in%2520combinatorial%250Aoptimization.%2520While%2520many%2520instances%2520of%2520simpler%2520problems%2520admit%2520fully%250Apolynomial-time%2520approximate%2520solution%2520%2528FPTAS%2529%252C%2520QAP%2520is%2520shown%2520to%2520be%2520strongly%250ANP-hard.%2520Even%2520finding%2520a%2520FPTAS%2520for%2520QAP%2520is%2520difficult%252C%2520in%2520the%2520sense%2520that%2520the%250Aexistence%2520of%2520a%2520FPTAS%2520implies%2520%2524P%2520%253D%2520NP%2524.%2520Current%2520research%2520on%2520QAPs%2520suffer%2520from%250Alimited%2520scale%2520and%2520computational%2520inefficiency.%2520To%2520attack%2520the%2520aforementioned%250Aissues%252C%2520we%2520here%2520propose%2520the%2520first%2520solution%2520of%2520its%2520kind%2520for%2520QAP%2520in%2520the%250Alearn-to-improve%2520category.%2520This%2520work%2520encodes%2520facility%2520and%2520location%2520nodes%250Aseparately%252C%2520instead%2520of%2520forming%2520computationally%2520intensive%2520association%2520graphs%250Aprevalent%2520in%2520current%2520approaches.%2520This%2520design%2520choice%2520enables%2520scalability%2520to%250Alarger%2520problem%2520sizes.%2520Furthermore%252C%2520a%2520%255Ctextbf%257BS%257Dolution%2520%255Ctextbf%257BAW%257Dare%250A%255Ctextbf%257BT%257Dransformer%2520%2528SAWT%2529%2520architecture%2520integrates%2520the%2520incumbent%2520solution%250Amatrix%2520with%2520the%2520attention%2520score%2520to%2520effectively%2520capture%2520higher-order%2520information%250Aof%2520the%2520QAPs.%2520Our%2520model%2527s%2520effectiveness%2520is%2520validated%2520through%2520extensive%250Aexperiments%2520on%2520self-generated%2520QAP%2520instances%2520of%2520varying%2520sizes%2520and%2520the%2520QAPLIB%250Abenchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Solution-Aware%20Transformers%20for%20Efficiently%20Solving%20Quadratic%0A%20%20Assignment%20Problem&entry.906535625=Zhentao%20Tan%20and%20Yadong%20Mu&entry.1292438233=%20%20Recently%20various%20optimization%20problems%2C%20such%20as%20Mixed%20Integer%20Linear%0AProgramming%20Problems%20%28MILPs%29%2C%20have%20undergone%20comprehensive%20investigation%2C%0Aleveraging%20the%20capabilities%20of%20machine%20learning.%20This%20work%20focuses%20on%0Alearning-based%20solutions%20for%20efficiently%20solving%20the%20Quadratic%20Assignment%0AProblem%20%28QAPs%29%2C%20which%20stands%20as%20a%20formidable%20challenge%20in%20combinatorial%0Aoptimization.%20While%20many%20instances%20of%20simpler%20problems%20admit%20fully%0Apolynomial-time%20approximate%20solution%20%28FPTAS%29%2C%20QAP%20is%20shown%20to%20be%20strongly%0ANP-hard.%20Even%20finding%20a%20FPTAS%20for%20QAP%20is%20difficult%2C%20in%20the%20sense%20that%20the%0Aexistence%20of%20a%20FPTAS%20implies%20%24P%20%3D%20NP%24.%20Current%20research%20on%20QAPs%20suffer%20from%0Alimited%20scale%20and%20computational%20inefficiency.%20To%20attack%20the%20aforementioned%0Aissues%2C%20we%20here%20propose%20the%20first%20solution%20of%20its%20kind%20for%20QAP%20in%20the%0Alearn-to-improve%20category.%20This%20work%20encodes%20facility%20and%20location%20nodes%0Aseparately%2C%20instead%20of%20forming%20computationally%20intensive%20association%20graphs%0Aprevalent%20in%20current%20approaches.%20This%20design%20choice%20enables%20scalability%20to%0Alarger%20problem%20sizes.%20Furthermore%2C%20a%20%5Ctextbf%7BS%7Dolution%20%5Ctextbf%7BAW%7Dare%0A%5Ctextbf%7BT%7Dransformer%20%28SAWT%29%20architecture%20integrates%20the%20incumbent%20solution%0Amatrix%20with%20the%20attention%20score%20to%20effectively%20capture%20higher-order%20information%0Aof%20the%20QAPs.%20Our%20model%27s%20effectiveness%20is%20validated%20through%20extensive%0Aexperiments%20on%20self-generated%20QAP%20instances%20of%20varying%20sizes%20and%20the%20QAPLIB%0Abenchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09899v1&entry.124074799=Read"},
{"title": "Misam: Using ML in Dataflow Selection of Sparse-Sparse Matrix\n  Multiplication", "author": "Sanjali Yadav and Bahar Asgari", "abstract": "  Sparse matrix-matrix multiplication (SpGEMM) is a critical operation in\nnumerous fields, including scientific computing, graph analytics, and deep\nlearning. These applications exploit the sparsity of matrices to reduce storage\nand computational demands. However, the irregular structure of sparse matrices\nposes significant challenges for performance optimization. Traditional hardware\naccelerators are tailored for specific sparsity patterns with fixed dataflow\nschemes - inner, outer, and row-wise but often perform suboptimally when the\nactual sparsity deviates from these predetermined patterns. As the use of\nSpGEMM expands across various domains, each with distinct sparsity\ncharacteristics, the demand for hardware accelerators that can efficiently\nhandle a range of sparsity patterns is increasing. This paper presents a\nmachine learning based approach for adaptively selecting the most appropriate\ndataflow scheme for SpGEMM tasks with diverse sparsity patterns. By employing\ndecision trees and deep reinforcement learning, we explore the potential of\nthese techniques to surpass heuristic-based methods in identifying optimal\ndataflow schemes. We evaluate our models by comparing their performance with\nthat of a heuristic, highlighting the strengths and weaknesses of each\napproach. Our findings suggest that using machine learning for dynamic dataflow\nselection in hardware accelerators can provide upto 28 times gains.\n", "link": "http://arxiv.org/abs/2406.10166v1", "date": "2024-06-14", "relevancy": 1.9329, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4892}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4814}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Misam%3A%20Using%20ML%20in%20Dataflow%20Selection%20of%20Sparse-Sparse%20Matrix%0A%20%20Multiplication&body=Title%3A%20Misam%3A%20Using%20ML%20in%20Dataflow%20Selection%20of%20Sparse-Sparse%20Matrix%0A%20%20Multiplication%0AAuthor%3A%20Sanjali%20Yadav%20and%20Bahar%20Asgari%0AAbstract%3A%20%20%20Sparse%20matrix-matrix%20multiplication%20%28SpGEMM%29%20is%20a%20critical%20operation%20in%0Anumerous%20fields%2C%20including%20scientific%20computing%2C%20graph%20analytics%2C%20and%20deep%0Alearning.%20These%20applications%20exploit%20the%20sparsity%20of%20matrices%20to%20reduce%20storage%0Aand%20computational%20demands.%20However%2C%20the%20irregular%20structure%20of%20sparse%20matrices%0Aposes%20significant%20challenges%20for%20performance%20optimization.%20Traditional%20hardware%0Aaccelerators%20are%20tailored%20for%20specific%20sparsity%20patterns%20with%20fixed%20dataflow%0Aschemes%20-%20inner%2C%20outer%2C%20and%20row-wise%20but%20often%20perform%20suboptimally%20when%20the%0Aactual%20sparsity%20deviates%20from%20these%20predetermined%20patterns.%20As%20the%20use%20of%0ASpGEMM%20expands%20across%20various%20domains%2C%20each%20with%20distinct%20sparsity%0Acharacteristics%2C%20the%20demand%20for%20hardware%20accelerators%20that%20can%20efficiently%0Ahandle%20a%20range%20of%20sparsity%20patterns%20is%20increasing.%20This%20paper%20presents%20a%0Amachine%20learning%20based%20approach%20for%20adaptively%20selecting%20the%20most%20appropriate%0Adataflow%20scheme%20for%20SpGEMM%20tasks%20with%20diverse%20sparsity%20patterns.%20By%20employing%0Adecision%20trees%20and%20deep%20reinforcement%20learning%2C%20we%20explore%20the%20potential%20of%0Athese%20techniques%20to%20surpass%20heuristic-based%20methods%20in%20identifying%20optimal%0Adataflow%20schemes.%20We%20evaluate%20our%20models%20by%20comparing%20their%20performance%20with%0Athat%20of%20a%20heuristic%2C%20highlighting%20the%20strengths%20and%20weaknesses%20of%20each%0Aapproach.%20Our%20findings%20suggest%20that%20using%20machine%20learning%20for%20dynamic%20dataflow%0Aselection%20in%20hardware%20accelerators%20can%20provide%20upto%2028%20times%20gains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10166v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMisam%253A%2520Using%2520ML%2520in%2520Dataflow%2520Selection%2520of%2520Sparse-Sparse%2520Matrix%250A%2520%2520Multiplication%26entry.906535625%3DSanjali%2520Yadav%2520and%2520Bahar%2520Asgari%26entry.1292438233%3D%2520%2520Sparse%2520matrix-matrix%2520multiplication%2520%2528SpGEMM%2529%2520is%2520a%2520critical%2520operation%2520in%250Anumerous%2520fields%252C%2520including%2520scientific%2520computing%252C%2520graph%2520analytics%252C%2520and%2520deep%250Alearning.%2520These%2520applications%2520exploit%2520the%2520sparsity%2520of%2520matrices%2520to%2520reduce%2520storage%250Aand%2520computational%2520demands.%2520However%252C%2520the%2520irregular%2520structure%2520of%2520sparse%2520matrices%250Aposes%2520significant%2520challenges%2520for%2520performance%2520optimization.%2520Traditional%2520hardware%250Aaccelerators%2520are%2520tailored%2520for%2520specific%2520sparsity%2520patterns%2520with%2520fixed%2520dataflow%250Aschemes%2520-%2520inner%252C%2520outer%252C%2520and%2520row-wise%2520but%2520often%2520perform%2520suboptimally%2520when%2520the%250Aactual%2520sparsity%2520deviates%2520from%2520these%2520predetermined%2520patterns.%2520As%2520the%2520use%2520of%250ASpGEMM%2520expands%2520across%2520various%2520domains%252C%2520each%2520with%2520distinct%2520sparsity%250Acharacteristics%252C%2520the%2520demand%2520for%2520hardware%2520accelerators%2520that%2520can%2520efficiently%250Ahandle%2520a%2520range%2520of%2520sparsity%2520patterns%2520is%2520increasing.%2520This%2520paper%2520presents%2520a%250Amachine%2520learning%2520based%2520approach%2520for%2520adaptively%2520selecting%2520the%2520most%2520appropriate%250Adataflow%2520scheme%2520for%2520SpGEMM%2520tasks%2520with%2520diverse%2520sparsity%2520patterns.%2520By%2520employing%250Adecision%2520trees%2520and%2520deep%2520reinforcement%2520learning%252C%2520we%2520explore%2520the%2520potential%2520of%250Athese%2520techniques%2520to%2520surpass%2520heuristic-based%2520methods%2520in%2520identifying%2520optimal%250Adataflow%2520schemes.%2520We%2520evaluate%2520our%2520models%2520by%2520comparing%2520their%2520performance%2520with%250Athat%2520of%2520a%2520heuristic%252C%2520highlighting%2520the%2520strengths%2520and%2520weaknesses%2520of%2520each%250Aapproach.%2520Our%2520findings%2520suggest%2520that%2520using%2520machine%2520learning%2520for%2520dynamic%2520dataflow%250Aselection%2520in%2520hardware%2520accelerators%2520can%2520provide%2520upto%252028%2520times%2520gains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10166v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Misam%3A%20Using%20ML%20in%20Dataflow%20Selection%20of%20Sparse-Sparse%20Matrix%0A%20%20Multiplication&entry.906535625=Sanjali%20Yadav%20and%20Bahar%20Asgari&entry.1292438233=%20%20Sparse%20matrix-matrix%20multiplication%20%28SpGEMM%29%20is%20a%20critical%20operation%20in%0Anumerous%20fields%2C%20including%20scientific%20computing%2C%20graph%20analytics%2C%20and%20deep%0Alearning.%20These%20applications%20exploit%20the%20sparsity%20of%20matrices%20to%20reduce%20storage%0Aand%20computational%20demands.%20However%2C%20the%20irregular%20structure%20of%20sparse%20matrices%0Aposes%20significant%20challenges%20for%20performance%20optimization.%20Traditional%20hardware%0Aaccelerators%20are%20tailored%20for%20specific%20sparsity%20patterns%20with%20fixed%20dataflow%0Aschemes%20-%20inner%2C%20outer%2C%20and%20row-wise%20but%20often%20perform%20suboptimally%20when%20the%0Aactual%20sparsity%20deviates%20from%20these%20predetermined%20patterns.%20As%20the%20use%20of%0ASpGEMM%20expands%20across%20various%20domains%2C%20each%20with%20distinct%20sparsity%0Acharacteristics%2C%20the%20demand%20for%20hardware%20accelerators%20that%20can%20efficiently%0Ahandle%20a%20range%20of%20sparsity%20patterns%20is%20increasing.%20This%20paper%20presents%20a%0Amachine%20learning%20based%20approach%20for%20adaptively%20selecting%20the%20most%20appropriate%0Adataflow%20scheme%20for%20SpGEMM%20tasks%20with%20diverse%20sparsity%20patterns.%20By%20employing%0Adecision%20trees%20and%20deep%20reinforcement%20learning%2C%20we%20explore%20the%20potential%20of%0Athese%20techniques%20to%20surpass%20heuristic-based%20methods%20in%20identifying%20optimal%0Adataflow%20schemes.%20We%20evaluate%20our%20models%20by%20comparing%20their%20performance%20with%0Athat%20of%20a%20heuristic%2C%20highlighting%20the%20strengths%20and%20weaknesses%20of%20each%0Aapproach.%20Our%20findings%20suggest%20that%20using%20machine%20learning%20for%20dynamic%20dataflow%0Aselection%20in%20hardware%20accelerators%20can%20provide%20upto%2028%20times%20gains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10166v1&entry.124074799=Read"},
{"title": "Universal randomised signatures for generative time series modelling", "author": "Francesca Biagini and Lukas Gonon and Niklas Walter", "abstract": "  Randomised signature has been proposed as a flexible and easily implementable\nalternative to the well-established path signature. In this article, we employ\nrandomised signature to introduce a generative model for financial time series\ndata in the spirit of reservoir computing. Specifically, we propose a novel\nWasserstein-type distance based on discrete-time randomised signatures. This\nmetric on the space of probability measures captures the distance between\n(conditional) distributions. Its use is justified by our novel universal\napproximation results for randomised signatures on the space of continuous\nfunctions taking the underlying path as an input. We then use our metric as the\nloss function in a non-adversarial generator model for synthetic time series\ndata based on a reservoir neural stochastic differential equation. We compare\nthe results of our model to benchmarks from the existing literature.\n", "link": "http://arxiv.org/abs/2406.10214v1", "date": "2024-06-14", "relevancy": 1.9297, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5149}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4855}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20randomised%20signatures%20for%20generative%20time%20series%20modelling&body=Title%3A%20Universal%20randomised%20signatures%20for%20generative%20time%20series%20modelling%0AAuthor%3A%20Francesca%20Biagini%20and%20Lukas%20Gonon%20and%20Niklas%20Walter%0AAbstract%3A%20%20%20Randomised%20signature%20has%20been%20proposed%20as%20a%20flexible%20and%20easily%20implementable%0Aalternative%20to%20the%20well-established%20path%20signature.%20In%20this%20article%2C%20we%20employ%0Arandomised%20signature%20to%20introduce%20a%20generative%20model%20for%20financial%20time%20series%0Adata%20in%20the%20spirit%20of%20reservoir%20computing.%20Specifically%2C%20we%20propose%20a%20novel%0AWasserstein-type%20distance%20based%20on%20discrete-time%20randomised%20signatures.%20This%0Ametric%20on%20the%20space%20of%20probability%20measures%20captures%20the%20distance%20between%0A%28conditional%29%20distributions.%20Its%20use%20is%20justified%20by%20our%20novel%20universal%0Aapproximation%20results%20for%20randomised%20signatures%20on%20the%20space%20of%20continuous%0Afunctions%20taking%20the%20underlying%20path%20as%20an%20input.%20We%20then%20use%20our%20metric%20as%20the%0Aloss%20function%20in%20a%20non-adversarial%20generator%20model%20for%20synthetic%20time%20series%0Adata%20based%20on%20a%20reservoir%20neural%20stochastic%20differential%20equation.%20We%20compare%0Athe%20results%20of%20our%20model%20to%20benchmarks%20from%20the%20existing%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520randomised%2520signatures%2520for%2520generative%2520time%2520series%2520modelling%26entry.906535625%3DFrancesca%2520Biagini%2520and%2520Lukas%2520Gonon%2520and%2520Niklas%2520Walter%26entry.1292438233%3D%2520%2520Randomised%2520signature%2520has%2520been%2520proposed%2520as%2520a%2520flexible%2520and%2520easily%2520implementable%250Aalternative%2520to%2520the%2520well-established%2520path%2520signature.%2520In%2520this%2520article%252C%2520we%2520employ%250Arandomised%2520signature%2520to%2520introduce%2520a%2520generative%2520model%2520for%2520financial%2520time%2520series%250Adata%2520in%2520the%2520spirit%2520of%2520reservoir%2520computing.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%250AWasserstein-type%2520distance%2520based%2520on%2520discrete-time%2520randomised%2520signatures.%2520This%250Ametric%2520on%2520the%2520space%2520of%2520probability%2520measures%2520captures%2520the%2520distance%2520between%250A%2528conditional%2529%2520distributions.%2520Its%2520use%2520is%2520justified%2520by%2520our%2520novel%2520universal%250Aapproximation%2520results%2520for%2520randomised%2520signatures%2520on%2520the%2520space%2520of%2520continuous%250Afunctions%2520taking%2520the%2520underlying%2520path%2520as%2520an%2520input.%2520We%2520then%2520use%2520our%2520metric%2520as%2520the%250Aloss%2520function%2520in%2520a%2520non-adversarial%2520generator%2520model%2520for%2520synthetic%2520time%2520series%250Adata%2520based%2520on%2520a%2520reservoir%2520neural%2520stochastic%2520differential%2520equation.%2520We%2520compare%250Athe%2520results%2520of%2520our%2520model%2520to%2520benchmarks%2520from%2520the%2520existing%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20randomised%20signatures%20for%20generative%20time%20series%20modelling&entry.906535625=Francesca%20Biagini%20and%20Lukas%20Gonon%20and%20Niklas%20Walter&entry.1292438233=%20%20Randomised%20signature%20has%20been%20proposed%20as%20a%20flexible%20and%20easily%20implementable%0Aalternative%20to%20the%20well-established%20path%20signature.%20In%20this%20article%2C%20we%20employ%0Arandomised%20signature%20to%20introduce%20a%20generative%20model%20for%20financial%20time%20series%0Adata%20in%20the%20spirit%20of%20reservoir%20computing.%20Specifically%2C%20we%20propose%20a%20novel%0AWasserstein-type%20distance%20based%20on%20discrete-time%20randomised%20signatures.%20This%0Ametric%20on%20the%20space%20of%20probability%20measures%20captures%20the%20distance%20between%0A%28conditional%29%20distributions.%20Its%20use%20is%20justified%20by%20our%20novel%20universal%0Aapproximation%20results%20for%20randomised%20signatures%20on%20the%20space%20of%20continuous%0Afunctions%20taking%20the%20underlying%20path%20as%20an%20input.%20We%20then%20use%20our%20metric%20as%20the%0Aloss%20function%20in%20a%20non-adversarial%20generator%20model%20for%20synthetic%20time%20series%0Adata%20based%20on%20a%20reservoir%20neural%20stochastic%20differential%20equation.%20We%20compare%0Athe%20results%20of%20our%20model%20to%20benchmarks%20from%20the%20existing%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10214v1&entry.124074799=Read"},
{"title": "POWN: Prototypical Open-World Node Classification", "author": "Marcel Hoffmann and Lukas Galke and Ansgar Scherp", "abstract": "  We consider the problem of \\textit{true} open-world semi-supervised node\nclassification, in which nodes in a graph either belong to known or new\nclasses, with the latter not present during training. Existing methods detect\nand reject new classes but fail to distinguish between different new classes.\nWe adapt existing methods and show they do not solve the problem sufficiently.\nWe introduce a novel end-to-end approach for classification into known classes\nand new classes based on class prototypes, which we call Prototypical\nOpen-World Learning for Node Classification (POWN). Our method combines graph\nsemi-supervised learning, self-supervised learning, and pseudo-labeling to\nlearn prototype representations of new classes in a zero-shot way. In contrast\nto existing solutions from the vision domain, POWN does not require data\naugmentation techniques for node classification. Experiments on benchmark\ndatasets demonstrate the effectiveness of POWN, where it outperforms baselines\nby up to $20\\%$ accuracy on the small and up to $30\\%$ on the large datasets.\nSource code is available at https://github.com/Bobowner/POWN.\n", "link": "http://arxiv.org/abs/2406.09926v1", "date": "2024-06-14", "relevancy": 1.9283, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.478}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20POWN%3A%20Prototypical%20Open-World%20Node%20Classification&body=Title%3A%20POWN%3A%20Prototypical%20Open-World%20Node%20Classification%0AAuthor%3A%20Marcel%20Hoffmann%20and%20Lukas%20Galke%20and%20Ansgar%20Scherp%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20%5Ctextit%7Btrue%7D%20open-world%20semi-supervised%20node%0Aclassification%2C%20in%20which%20nodes%20in%20a%20graph%20either%20belong%20to%20known%20or%20new%0Aclasses%2C%20with%20the%20latter%20not%20present%20during%20training.%20Existing%20methods%20detect%0Aand%20reject%20new%20classes%20but%20fail%20to%20distinguish%20between%20different%20new%20classes.%0AWe%20adapt%20existing%20methods%20and%20show%20they%20do%20not%20solve%20the%20problem%20sufficiently.%0AWe%20introduce%20a%20novel%20end-to-end%20approach%20for%20classification%20into%20known%20classes%0Aand%20new%20classes%20based%20on%20class%20prototypes%2C%20which%20we%20call%20Prototypical%0AOpen-World%20Learning%20for%20Node%20Classification%20%28POWN%29.%20Our%20method%20combines%20graph%0Asemi-supervised%20learning%2C%20self-supervised%20learning%2C%20and%20pseudo-labeling%20to%0Alearn%20prototype%20representations%20of%20new%20classes%20in%20a%20zero-shot%20way.%20In%20contrast%0Ato%20existing%20solutions%20from%20the%20vision%20domain%2C%20POWN%20does%20not%20require%20data%0Aaugmentation%20techniques%20for%20node%20classification.%20Experiments%20on%20benchmark%0Adatasets%20demonstrate%20the%20effectiveness%20of%20POWN%2C%20where%20it%20outperforms%20baselines%0Aby%20up%20to%20%2420%5C%25%24%20accuracy%20on%20the%20small%20and%20up%20to%20%2430%5C%25%24%20on%20the%20large%20datasets.%0ASource%20code%20is%20available%20at%20https%3A//github.com/Bobowner/POWN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPOWN%253A%2520Prototypical%2520Open-World%2520Node%2520Classification%26entry.906535625%3DMarcel%2520Hoffmann%2520and%2520Lukas%2520Galke%2520and%2520Ansgar%2520Scherp%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520%255Ctextit%257Btrue%257D%2520open-world%2520semi-supervised%2520node%250Aclassification%252C%2520in%2520which%2520nodes%2520in%2520a%2520graph%2520either%2520belong%2520to%2520known%2520or%2520new%250Aclasses%252C%2520with%2520the%2520latter%2520not%2520present%2520during%2520training.%2520Existing%2520methods%2520detect%250Aand%2520reject%2520new%2520classes%2520but%2520fail%2520to%2520distinguish%2520between%2520different%2520new%2520classes.%250AWe%2520adapt%2520existing%2520methods%2520and%2520show%2520they%2520do%2520not%2520solve%2520the%2520problem%2520sufficiently.%250AWe%2520introduce%2520a%2520novel%2520end-to-end%2520approach%2520for%2520classification%2520into%2520known%2520classes%250Aand%2520new%2520classes%2520based%2520on%2520class%2520prototypes%252C%2520which%2520we%2520call%2520Prototypical%250AOpen-World%2520Learning%2520for%2520Node%2520Classification%2520%2528POWN%2529.%2520Our%2520method%2520combines%2520graph%250Asemi-supervised%2520learning%252C%2520self-supervised%2520learning%252C%2520and%2520pseudo-labeling%2520to%250Alearn%2520prototype%2520representations%2520of%2520new%2520classes%2520in%2520a%2520zero-shot%2520way.%2520In%2520contrast%250Ato%2520existing%2520solutions%2520from%2520the%2520vision%2520domain%252C%2520POWN%2520does%2520not%2520require%2520data%250Aaugmentation%2520techniques%2520for%2520node%2520classification.%2520Experiments%2520on%2520benchmark%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520of%2520POWN%252C%2520where%2520it%2520outperforms%2520baselines%250Aby%2520up%2520to%2520%252420%255C%2525%2524%2520accuracy%2520on%2520the%2520small%2520and%2520up%2520to%2520%252430%255C%2525%2524%2520on%2520the%2520large%2520datasets.%250ASource%2520code%2520is%2520available%2520at%2520https%253A//github.com/Bobowner/POWN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=POWN%3A%20Prototypical%20Open-World%20Node%20Classification&entry.906535625=Marcel%20Hoffmann%20and%20Lukas%20Galke%20and%20Ansgar%20Scherp&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20%5Ctextit%7Btrue%7D%20open-world%20semi-supervised%20node%0Aclassification%2C%20in%20which%20nodes%20in%20a%20graph%20either%20belong%20to%20known%20or%20new%0Aclasses%2C%20with%20the%20latter%20not%20present%20during%20training.%20Existing%20methods%20detect%0Aand%20reject%20new%20classes%20but%20fail%20to%20distinguish%20between%20different%20new%20classes.%0AWe%20adapt%20existing%20methods%20and%20show%20they%20do%20not%20solve%20the%20problem%20sufficiently.%0AWe%20introduce%20a%20novel%20end-to-end%20approach%20for%20classification%20into%20known%20classes%0Aand%20new%20classes%20based%20on%20class%20prototypes%2C%20which%20we%20call%20Prototypical%0AOpen-World%20Learning%20for%20Node%20Classification%20%28POWN%29.%20Our%20method%20combines%20graph%0Asemi-supervised%20learning%2C%20self-supervised%20learning%2C%20and%20pseudo-labeling%20to%0Alearn%20prototype%20representations%20of%20new%20classes%20in%20a%20zero-shot%20way.%20In%20contrast%0Ato%20existing%20solutions%20from%20the%20vision%20domain%2C%20POWN%20does%20not%20require%20data%0Aaugmentation%20techniques%20for%20node%20classification.%20Experiments%20on%20benchmark%0Adatasets%20demonstrate%20the%20effectiveness%20of%20POWN%2C%20where%20it%20outperforms%20baselines%0Aby%20up%20to%20%2420%5C%25%24%20accuracy%20on%20the%20small%20and%20up%20to%20%2430%5C%25%24%20on%20the%20large%20datasets.%0ASource%20code%20is%20available%20at%20https%3A//github.com/Bobowner/POWN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09926v1&entry.124074799=Read"},
{"title": "Architectural Blueprint For Heterogeneity-Resilient Federated Learning", "author": "Satwat Bashir and Tasos Dagiuklas and Kasra Kassai and Muddesar Iqbal", "abstract": "  This paper proposes a novel three tier architecture for federated learning to\noptimize edge computing environments. The proposed architecture addresses the\nchallenges associated with client data heterogeneity and computational\nconstraints. It introduces a scalable, privacy preserving framework that\nenhances the efficiency of distributed machine learning. Through\nexperimentation, the paper demonstrates the architecture capability to manage\nnon IID data sets more effectively than traditional federated learning models.\nAdditionally, the paper highlights the potential of this innovative approach to\nsignificantly improve model accuracy, reduce communication overhead, and\nfacilitate broader adoption of federated learning technologies.\n", "link": "http://arxiv.org/abs/2403.04546v2", "date": "2024-06-14", "relevancy": 1.9244, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4895}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.483}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Architectural%20Blueprint%20For%20Heterogeneity-Resilient%20Federated%20Learning&body=Title%3A%20Architectural%20Blueprint%20For%20Heterogeneity-Resilient%20Federated%20Learning%0AAuthor%3A%20Satwat%20Bashir%20and%20Tasos%20Dagiuklas%20and%20Kasra%20Kassai%20and%20Muddesar%20Iqbal%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20three%20tier%20architecture%20for%20federated%20learning%20to%0Aoptimize%20edge%20computing%20environments.%20The%20proposed%20architecture%20addresses%20the%0Achallenges%20associated%20with%20client%20data%20heterogeneity%20and%20computational%0Aconstraints.%20It%20introduces%20a%20scalable%2C%20privacy%20preserving%20framework%20that%0Aenhances%20the%20efficiency%20of%20distributed%20machine%20learning.%20Through%0Aexperimentation%2C%20the%20paper%20demonstrates%20the%20architecture%20capability%20to%20manage%0Anon%20IID%20data%20sets%20more%20effectively%20than%20traditional%20federated%20learning%20models.%0AAdditionally%2C%20the%20paper%20highlights%20the%20potential%20of%20this%20innovative%20approach%20to%0Asignificantly%20improve%20model%20accuracy%2C%20reduce%20communication%20overhead%2C%20and%0Afacilitate%20broader%20adoption%20of%20federated%20learning%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04546v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArchitectural%2520Blueprint%2520For%2520Heterogeneity-Resilient%2520Federated%2520Learning%26entry.906535625%3DSatwat%2520Bashir%2520and%2520Tasos%2520Dagiuklas%2520and%2520Kasra%2520Kassai%2520and%2520Muddesar%2520Iqbal%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520three%2520tier%2520architecture%2520for%2520federated%2520learning%2520to%250Aoptimize%2520edge%2520computing%2520environments.%2520The%2520proposed%2520architecture%2520addresses%2520the%250Achallenges%2520associated%2520with%2520client%2520data%2520heterogeneity%2520and%2520computational%250Aconstraints.%2520It%2520introduces%2520a%2520scalable%252C%2520privacy%2520preserving%2520framework%2520that%250Aenhances%2520the%2520efficiency%2520of%2520distributed%2520machine%2520learning.%2520Through%250Aexperimentation%252C%2520the%2520paper%2520demonstrates%2520the%2520architecture%2520capability%2520to%2520manage%250Anon%2520IID%2520data%2520sets%2520more%2520effectively%2520than%2520traditional%2520federated%2520learning%2520models.%250AAdditionally%252C%2520the%2520paper%2520highlights%2520the%2520potential%2520of%2520this%2520innovative%2520approach%2520to%250Asignificantly%2520improve%2520model%2520accuracy%252C%2520reduce%2520communication%2520overhead%252C%2520and%250Afacilitate%2520broader%2520adoption%2520of%2520federated%2520learning%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04546v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Architectural%20Blueprint%20For%20Heterogeneity-Resilient%20Federated%20Learning&entry.906535625=Satwat%20Bashir%20and%20Tasos%20Dagiuklas%20and%20Kasra%20Kassai%20and%20Muddesar%20Iqbal&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20three%20tier%20architecture%20for%20federated%20learning%20to%0Aoptimize%20edge%20computing%20environments.%20The%20proposed%20architecture%20addresses%20the%0Achallenges%20associated%20with%20client%20data%20heterogeneity%20and%20computational%0Aconstraints.%20It%20introduces%20a%20scalable%2C%20privacy%20preserving%20framework%20that%0Aenhances%20the%20efficiency%20of%20distributed%20machine%20learning.%20Through%0Aexperimentation%2C%20the%20paper%20demonstrates%20the%20architecture%20capability%20to%20manage%0Anon%20IID%20data%20sets%20more%20effectively%20than%20traditional%20federated%20learning%20models.%0AAdditionally%2C%20the%20paper%20highlights%20the%20potential%20of%20this%20innovative%20approach%20to%0Asignificantly%20improve%20model%20accuracy%2C%20reduce%20communication%20overhead%2C%20and%0Afacilitate%20broader%20adoption%20of%20federated%20learning%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04546v2&entry.124074799=Read"},
{"title": "SCKansformer: Fine-Grained Classification of Bone Marrow Cells via\n  Kansformer Backbone and Hierarchical Attention Mechanisms", "author": "Yifei Chen and Zhu Zhu and Shenghao Zhu and Linwei Qiu and Binfeng Zou and Fan Jia and Yunpeng Zhu and Chenyan Zhang and Zhaojie Fang and Feiwei Qin and Jin Fan and Changmiao Wang and Yu Gao and Gang Yu", "abstract": "  The incidence and mortality rates of malignant tumors, such as acute\nleukemia, have risen significantly. Clinically, hospitals rely on cytological\nexamination of peripheral blood and bone marrow smears to diagnose malignant\ntumors, with accurate blood cell counting being crucial. Existing automated\nmethods face challenges such as low feature expression capability, poor\ninterpretability, and redundant feature extraction when processing\nhigh-dimensional microimage data. We propose a novel fine-grained\nclassification model, SCKansformer, for bone marrow blood cells, which\naddresses these challenges and enhances classification accuracy and efficiency.\nThe model integrates the Kansformer Encoder, SCConv Encoder, and Global-Local\nAttention Encoder. The Kansformer Encoder replaces the traditional MLP layer\nwith the KAN, improving nonlinear feature representation and interpretability.\nThe SCConv Encoder, with its Spatial and Channel Reconstruction Units, enhances\nfeature representation and reduces redundancy. The Global-Local Attention\nEncoder combines Multi-head Self-Attention with a Local Part module to capture\nboth global and local features. We validated our model using the Bone Marrow\nBlood Cell Fine-Grained Classification Dataset (BMCD-FGCD), comprising over\n10,000 samples and nearly 40 classifications, developed with a partner\nhospital. Comparative experiments on our private dataset, as well as the\npublicly available PBC and ALL-IDB datasets, demonstrate that SCKansformer\noutperforms both typical and advanced microcell classification methods across\nall datasets. Our source code and private BMCD-FGCD dataset are available at\nhttps://github.com/JustlfC03/SCKansformer.\n", "link": "http://arxiv.org/abs/2406.09931v1", "date": "2024-06-14", "relevancy": 1.9208, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4837}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.479}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCKansformer%3A%20Fine-Grained%20Classification%20of%20Bone%20Marrow%20Cells%20via%0A%20%20Kansformer%20Backbone%20and%20Hierarchical%20Attention%20Mechanisms&body=Title%3A%20SCKansformer%3A%20Fine-Grained%20Classification%20of%20Bone%20Marrow%20Cells%20via%0A%20%20Kansformer%20Backbone%20and%20Hierarchical%20Attention%20Mechanisms%0AAuthor%3A%20Yifei%20Chen%20and%20Zhu%20Zhu%20and%20Shenghao%20Zhu%20and%20Linwei%20Qiu%20and%20Binfeng%20Zou%20and%20Fan%20Jia%20and%20Yunpeng%20Zhu%20and%20Chenyan%20Zhang%20and%20Zhaojie%20Fang%20and%20Feiwei%20Qin%20and%20Jin%20Fan%20and%20Changmiao%20Wang%20and%20Yu%20Gao%20and%20Gang%20Yu%0AAbstract%3A%20%20%20The%20incidence%20and%20mortality%20rates%20of%20malignant%20tumors%2C%20such%20as%20acute%0Aleukemia%2C%20have%20risen%20significantly.%20Clinically%2C%20hospitals%20rely%20on%20cytological%0Aexamination%20of%20peripheral%20blood%20and%20bone%20marrow%20smears%20to%20diagnose%20malignant%0Atumors%2C%20with%20accurate%20blood%20cell%20counting%20being%20crucial.%20Existing%20automated%0Amethods%20face%20challenges%20such%20as%20low%20feature%20expression%20capability%2C%20poor%0Ainterpretability%2C%20and%20redundant%20feature%20extraction%20when%20processing%0Ahigh-dimensional%20microimage%20data.%20We%20propose%20a%20novel%20fine-grained%0Aclassification%20model%2C%20SCKansformer%2C%20for%20bone%20marrow%20blood%20cells%2C%20which%0Aaddresses%20these%20challenges%20and%20enhances%20classification%20accuracy%20and%20efficiency.%0AThe%20model%20integrates%20the%20Kansformer%20Encoder%2C%20SCConv%20Encoder%2C%20and%20Global-Local%0AAttention%20Encoder.%20The%20Kansformer%20Encoder%20replaces%20the%20traditional%20MLP%20layer%0Awith%20the%20KAN%2C%20improving%20nonlinear%20feature%20representation%20and%20interpretability.%0AThe%20SCConv%20Encoder%2C%20with%20its%20Spatial%20and%20Channel%20Reconstruction%20Units%2C%20enhances%0Afeature%20representation%20and%20reduces%20redundancy.%20The%20Global-Local%20Attention%0AEncoder%20combines%20Multi-head%20Self-Attention%20with%20a%20Local%20Part%20module%20to%20capture%0Aboth%20global%20and%20local%20features.%20We%20validated%20our%20model%20using%20the%20Bone%20Marrow%0ABlood%20Cell%20Fine-Grained%20Classification%20Dataset%20%28BMCD-FGCD%29%2C%20comprising%20over%0A10%2C000%20samples%20and%20nearly%2040%20classifications%2C%20developed%20with%20a%20partner%0Ahospital.%20Comparative%20experiments%20on%20our%20private%20dataset%2C%20as%20well%20as%20the%0Apublicly%20available%20PBC%20and%20ALL-IDB%20datasets%2C%20demonstrate%20that%20SCKansformer%0Aoutperforms%20both%20typical%20and%20advanced%20microcell%20classification%20methods%20across%0Aall%20datasets.%20Our%20source%20code%20and%20private%20BMCD-FGCD%20dataset%20are%20available%20at%0Ahttps%3A//github.com/JustlfC03/SCKansformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCKansformer%253A%2520Fine-Grained%2520Classification%2520of%2520Bone%2520Marrow%2520Cells%2520via%250A%2520%2520Kansformer%2520Backbone%2520and%2520Hierarchical%2520Attention%2520Mechanisms%26entry.906535625%3DYifei%2520Chen%2520and%2520Zhu%2520Zhu%2520and%2520Shenghao%2520Zhu%2520and%2520Linwei%2520Qiu%2520and%2520Binfeng%2520Zou%2520and%2520Fan%2520Jia%2520and%2520Yunpeng%2520Zhu%2520and%2520Chenyan%2520Zhang%2520and%2520Zhaojie%2520Fang%2520and%2520Feiwei%2520Qin%2520and%2520Jin%2520Fan%2520and%2520Changmiao%2520Wang%2520and%2520Yu%2520Gao%2520and%2520Gang%2520Yu%26entry.1292438233%3D%2520%2520The%2520incidence%2520and%2520mortality%2520rates%2520of%2520malignant%2520tumors%252C%2520such%2520as%2520acute%250Aleukemia%252C%2520have%2520risen%2520significantly.%2520Clinically%252C%2520hospitals%2520rely%2520on%2520cytological%250Aexamination%2520of%2520peripheral%2520blood%2520and%2520bone%2520marrow%2520smears%2520to%2520diagnose%2520malignant%250Atumors%252C%2520with%2520accurate%2520blood%2520cell%2520counting%2520being%2520crucial.%2520Existing%2520automated%250Amethods%2520face%2520challenges%2520such%2520as%2520low%2520feature%2520expression%2520capability%252C%2520poor%250Ainterpretability%252C%2520and%2520redundant%2520feature%2520extraction%2520when%2520processing%250Ahigh-dimensional%2520microimage%2520data.%2520We%2520propose%2520a%2520novel%2520fine-grained%250Aclassification%2520model%252C%2520SCKansformer%252C%2520for%2520bone%2520marrow%2520blood%2520cells%252C%2520which%250Aaddresses%2520these%2520challenges%2520and%2520enhances%2520classification%2520accuracy%2520and%2520efficiency.%250AThe%2520model%2520integrates%2520the%2520Kansformer%2520Encoder%252C%2520SCConv%2520Encoder%252C%2520and%2520Global-Local%250AAttention%2520Encoder.%2520The%2520Kansformer%2520Encoder%2520replaces%2520the%2520traditional%2520MLP%2520layer%250Awith%2520the%2520KAN%252C%2520improving%2520nonlinear%2520feature%2520representation%2520and%2520interpretability.%250AThe%2520SCConv%2520Encoder%252C%2520with%2520its%2520Spatial%2520and%2520Channel%2520Reconstruction%2520Units%252C%2520enhances%250Afeature%2520representation%2520and%2520reduces%2520redundancy.%2520The%2520Global-Local%2520Attention%250AEncoder%2520combines%2520Multi-head%2520Self-Attention%2520with%2520a%2520Local%2520Part%2520module%2520to%2520capture%250Aboth%2520global%2520and%2520local%2520features.%2520We%2520validated%2520our%2520model%2520using%2520the%2520Bone%2520Marrow%250ABlood%2520Cell%2520Fine-Grained%2520Classification%2520Dataset%2520%2528BMCD-FGCD%2529%252C%2520comprising%2520over%250A10%252C000%2520samples%2520and%2520nearly%252040%2520classifications%252C%2520developed%2520with%2520a%2520partner%250Ahospital.%2520Comparative%2520experiments%2520on%2520our%2520private%2520dataset%252C%2520as%2520well%2520as%2520the%250Apublicly%2520available%2520PBC%2520and%2520ALL-IDB%2520datasets%252C%2520demonstrate%2520that%2520SCKansformer%250Aoutperforms%2520both%2520typical%2520and%2520advanced%2520microcell%2520classification%2520methods%2520across%250Aall%2520datasets.%2520Our%2520source%2520code%2520and%2520private%2520BMCD-FGCD%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/JustlfC03/SCKansformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCKansformer%3A%20Fine-Grained%20Classification%20of%20Bone%20Marrow%20Cells%20via%0A%20%20Kansformer%20Backbone%20and%20Hierarchical%20Attention%20Mechanisms&entry.906535625=Yifei%20Chen%20and%20Zhu%20Zhu%20and%20Shenghao%20Zhu%20and%20Linwei%20Qiu%20and%20Binfeng%20Zou%20and%20Fan%20Jia%20and%20Yunpeng%20Zhu%20and%20Chenyan%20Zhang%20and%20Zhaojie%20Fang%20and%20Feiwei%20Qin%20and%20Jin%20Fan%20and%20Changmiao%20Wang%20and%20Yu%20Gao%20and%20Gang%20Yu&entry.1292438233=%20%20The%20incidence%20and%20mortality%20rates%20of%20malignant%20tumors%2C%20such%20as%20acute%0Aleukemia%2C%20have%20risen%20significantly.%20Clinically%2C%20hospitals%20rely%20on%20cytological%0Aexamination%20of%20peripheral%20blood%20and%20bone%20marrow%20smears%20to%20diagnose%20malignant%0Atumors%2C%20with%20accurate%20blood%20cell%20counting%20being%20crucial.%20Existing%20automated%0Amethods%20face%20challenges%20such%20as%20low%20feature%20expression%20capability%2C%20poor%0Ainterpretability%2C%20and%20redundant%20feature%20extraction%20when%20processing%0Ahigh-dimensional%20microimage%20data.%20We%20propose%20a%20novel%20fine-grained%0Aclassification%20model%2C%20SCKansformer%2C%20for%20bone%20marrow%20blood%20cells%2C%20which%0Aaddresses%20these%20challenges%20and%20enhances%20classification%20accuracy%20and%20efficiency.%0AThe%20model%20integrates%20the%20Kansformer%20Encoder%2C%20SCConv%20Encoder%2C%20and%20Global-Local%0AAttention%20Encoder.%20The%20Kansformer%20Encoder%20replaces%20the%20traditional%20MLP%20layer%0Awith%20the%20KAN%2C%20improving%20nonlinear%20feature%20representation%20and%20interpretability.%0AThe%20SCConv%20Encoder%2C%20with%20its%20Spatial%20and%20Channel%20Reconstruction%20Units%2C%20enhances%0Afeature%20representation%20and%20reduces%20redundancy.%20The%20Global-Local%20Attention%0AEncoder%20combines%20Multi-head%20Self-Attention%20with%20a%20Local%20Part%20module%20to%20capture%0Aboth%20global%20and%20local%20features.%20We%20validated%20our%20model%20using%20the%20Bone%20Marrow%0ABlood%20Cell%20Fine-Grained%20Classification%20Dataset%20%28BMCD-FGCD%29%2C%20comprising%20over%0A10%2C000%20samples%20and%20nearly%2040%20classifications%2C%20developed%20with%20a%20partner%0Ahospital.%20Comparative%20experiments%20on%20our%20private%20dataset%2C%20as%20well%20as%20the%0Apublicly%20available%20PBC%20and%20ALL-IDB%20datasets%2C%20demonstrate%20that%20SCKansformer%0Aoutperforms%20both%20typical%20and%20advanced%20microcell%20classification%20methods%20across%0Aall%20datasets.%20Our%20source%20code%20and%20private%20BMCD-FGCD%20dataset%20are%20available%20at%0Ahttps%3A//github.com/JustlfC03/SCKansformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09931v1&entry.124074799=Read"},
{"title": "Provably Safe Neural Network Controllers via Differential Dynamic Logic", "author": "Samuel Teuber and Stefan Mitsch and Andr\u00e9 Platzer", "abstract": "  While neural networks (NNs) have potential as autonomous controllers for\nCyber-Physical Systems, verifying the safety of NN based control systems\n(NNCSs) poses significant challenges for the practical use of NNs, especially\nwhen safety is needed for unbounded time horizons. One reason is the\nintractability of analyzing NNs, ODEs and hybrid systems. To this end, we\nintroduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The\nfirst general approach that allows reusing control theory results for NNCS\nverification. By joining forces, we exploit the efficiency of NN verification\ntools while retaining the rigor of differential dynamic logic (dL). Based on\nprovably safe control envelopes in dL, we derive specifications for the NN\nwhich is proven via NN verification. We show that a proof of the NN adhering to\nthe specification is mirrored by a dL proof on the infinite-time safety of the\nNNCS.\n  The NN verification properties resulting from hybrid systems typically\ncontain nonlinear arithmetic and arbitrary logical structures while efficient\nNN verification merely supports linear constraints. To overcome this divide, we\npresent Mosaic: An efficient, sound and complete verification approach for\npolynomial real arithmetic properties on piece-wise linear NNs. Mosaic\npartitions complex verification queries into simple queries and lifts\noff-the-shelf linear constraint tools to the nonlinear setting in a\ncompleteness-preserving manner by combining approximation with exact reasoning\nfor counterexample regions. Our evaluation demonstrates the versatility of\nVerSAILLE and Mosaic: We prove infinite-time safety on the classical Vertical\nAirborne Collision Avoidance NNCS verification benchmark for two scenarios\nwhile (exhaustively) enumerating counterexample regions in unsafe scenarios. We\nalso show that our approach significantly outperforms State-of-the-Art tools in\nclosed-loop NNV.\n", "link": "http://arxiv.org/abs/2402.10998v2", "date": "2024-06-14", "relevancy": 1.9146, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4897}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4844}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provably%20Safe%20Neural%20Network%20Controllers%20via%20Differential%20Dynamic%20Logic&body=Title%3A%20Provably%20Safe%20Neural%20Network%20Controllers%20via%20Differential%20Dynamic%20Logic%0AAuthor%3A%20Samuel%20Teuber%20and%20Stefan%20Mitsch%20and%20Andr%C3%A9%20Platzer%0AAbstract%3A%20%20%20While%20neural%20networks%20%28NNs%29%20have%20potential%20as%20autonomous%20controllers%20for%0ACyber-Physical%20Systems%2C%20verifying%20the%20safety%20of%20NN%20based%20control%20systems%0A%28NNCSs%29%20poses%20significant%20challenges%20for%20the%20practical%20use%20of%20NNs%2C%20especially%0Awhen%20safety%20is%20needed%20for%20unbounded%20time%20horizons.%20One%20reason%20is%20the%0Aintractability%20of%20analyzing%20NNs%2C%20ODEs%20and%20hybrid%20systems.%20To%20this%20end%2C%20we%0Aintroduce%20VerSAILLE%20%28Verifiably%20Safe%20AI%20via%20Logically%20Linked%20Envelopes%29%3A%20The%0Afirst%20general%20approach%20that%20allows%20reusing%20control%20theory%20results%20for%20NNCS%0Averification.%20By%20joining%20forces%2C%20we%20exploit%20the%20efficiency%20of%20NN%20verification%0Atools%20while%20retaining%20the%20rigor%20of%20differential%20dynamic%20logic%20%28dL%29.%20Based%20on%0Aprovably%20safe%20control%20envelopes%20in%20dL%2C%20we%20derive%20specifications%20for%20the%20NN%0Awhich%20is%20proven%20via%20NN%20verification.%20We%20show%20that%20a%20proof%20of%20the%20NN%20adhering%20to%0Athe%20specification%20is%20mirrored%20by%20a%20dL%20proof%20on%20the%20infinite-time%20safety%20of%20the%0ANNCS.%0A%20%20The%20NN%20verification%20properties%20resulting%20from%20hybrid%20systems%20typically%0Acontain%20nonlinear%20arithmetic%20and%20arbitrary%20logical%20structures%20while%20efficient%0ANN%20verification%20merely%20supports%20linear%20constraints.%20To%20overcome%20this%20divide%2C%20we%0Apresent%20Mosaic%3A%20An%20efficient%2C%20sound%20and%20complete%20verification%20approach%20for%0Apolynomial%20real%20arithmetic%20properties%20on%20piece-wise%20linear%20NNs.%20Mosaic%0Apartitions%20complex%20verification%20queries%20into%20simple%20queries%20and%20lifts%0Aoff-the-shelf%20linear%20constraint%20tools%20to%20the%20nonlinear%20setting%20in%20a%0Acompleteness-preserving%20manner%20by%20combining%20approximation%20with%20exact%20reasoning%0Afor%20counterexample%20regions.%20Our%20evaluation%20demonstrates%20the%20versatility%20of%0AVerSAILLE%20and%20Mosaic%3A%20We%20prove%20infinite-time%20safety%20on%20the%20classical%20Vertical%0AAirborne%20Collision%20Avoidance%20NNCS%20verification%20benchmark%20for%20two%20scenarios%0Awhile%20%28exhaustively%29%20enumerating%20counterexample%20regions%20in%20unsafe%20scenarios.%20We%0Aalso%20show%20that%20our%20approach%20significantly%20outperforms%20State-of-the-Art%20tools%20in%0Aclosed-loop%20NNV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10998v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvably%2520Safe%2520Neural%2520Network%2520Controllers%2520via%2520Differential%2520Dynamic%2520Logic%26entry.906535625%3DSamuel%2520Teuber%2520and%2520Stefan%2520Mitsch%2520and%2520Andr%25C3%25A9%2520Platzer%26entry.1292438233%3D%2520%2520While%2520neural%2520networks%2520%2528NNs%2529%2520have%2520potential%2520as%2520autonomous%2520controllers%2520for%250ACyber-Physical%2520Systems%252C%2520verifying%2520the%2520safety%2520of%2520NN%2520based%2520control%2520systems%250A%2528NNCSs%2529%2520poses%2520significant%2520challenges%2520for%2520the%2520practical%2520use%2520of%2520NNs%252C%2520especially%250Awhen%2520safety%2520is%2520needed%2520for%2520unbounded%2520time%2520horizons.%2520One%2520reason%2520is%2520the%250Aintractability%2520of%2520analyzing%2520NNs%252C%2520ODEs%2520and%2520hybrid%2520systems.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520VerSAILLE%2520%2528Verifiably%2520Safe%2520AI%2520via%2520Logically%2520Linked%2520Envelopes%2529%253A%2520The%250Afirst%2520general%2520approach%2520that%2520allows%2520reusing%2520control%2520theory%2520results%2520for%2520NNCS%250Averification.%2520By%2520joining%2520forces%252C%2520we%2520exploit%2520the%2520efficiency%2520of%2520NN%2520verification%250Atools%2520while%2520retaining%2520the%2520rigor%2520of%2520differential%2520dynamic%2520logic%2520%2528dL%2529.%2520Based%2520on%250Aprovably%2520safe%2520control%2520envelopes%2520in%2520dL%252C%2520we%2520derive%2520specifications%2520for%2520the%2520NN%250Awhich%2520is%2520proven%2520via%2520NN%2520verification.%2520We%2520show%2520that%2520a%2520proof%2520of%2520the%2520NN%2520adhering%2520to%250Athe%2520specification%2520is%2520mirrored%2520by%2520a%2520dL%2520proof%2520on%2520the%2520infinite-time%2520safety%2520of%2520the%250ANNCS.%250A%2520%2520The%2520NN%2520verification%2520properties%2520resulting%2520from%2520hybrid%2520systems%2520typically%250Acontain%2520nonlinear%2520arithmetic%2520and%2520arbitrary%2520logical%2520structures%2520while%2520efficient%250ANN%2520verification%2520merely%2520supports%2520linear%2520constraints.%2520To%2520overcome%2520this%2520divide%252C%2520we%250Apresent%2520Mosaic%253A%2520An%2520efficient%252C%2520sound%2520and%2520complete%2520verification%2520approach%2520for%250Apolynomial%2520real%2520arithmetic%2520properties%2520on%2520piece-wise%2520linear%2520NNs.%2520Mosaic%250Apartitions%2520complex%2520verification%2520queries%2520into%2520simple%2520queries%2520and%2520lifts%250Aoff-the-shelf%2520linear%2520constraint%2520tools%2520to%2520the%2520nonlinear%2520setting%2520in%2520a%250Acompleteness-preserving%2520manner%2520by%2520combining%2520approximation%2520with%2520exact%2520reasoning%250Afor%2520counterexample%2520regions.%2520Our%2520evaluation%2520demonstrates%2520the%2520versatility%2520of%250AVerSAILLE%2520and%2520Mosaic%253A%2520We%2520prove%2520infinite-time%2520safety%2520on%2520the%2520classical%2520Vertical%250AAirborne%2520Collision%2520Avoidance%2520NNCS%2520verification%2520benchmark%2520for%2520two%2520scenarios%250Awhile%2520%2528exhaustively%2529%2520enumerating%2520counterexample%2520regions%2520in%2520unsafe%2520scenarios.%2520We%250Aalso%2520show%2520that%2520our%2520approach%2520significantly%2520outperforms%2520State-of-the-Art%2520tools%2520in%250Aclosed-loop%2520NNV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10998v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provably%20Safe%20Neural%20Network%20Controllers%20via%20Differential%20Dynamic%20Logic&entry.906535625=Samuel%20Teuber%20and%20Stefan%20Mitsch%20and%20Andr%C3%A9%20Platzer&entry.1292438233=%20%20While%20neural%20networks%20%28NNs%29%20have%20potential%20as%20autonomous%20controllers%20for%0ACyber-Physical%20Systems%2C%20verifying%20the%20safety%20of%20NN%20based%20control%20systems%0A%28NNCSs%29%20poses%20significant%20challenges%20for%20the%20practical%20use%20of%20NNs%2C%20especially%0Awhen%20safety%20is%20needed%20for%20unbounded%20time%20horizons.%20One%20reason%20is%20the%0Aintractability%20of%20analyzing%20NNs%2C%20ODEs%20and%20hybrid%20systems.%20To%20this%20end%2C%20we%0Aintroduce%20VerSAILLE%20%28Verifiably%20Safe%20AI%20via%20Logically%20Linked%20Envelopes%29%3A%20The%0Afirst%20general%20approach%20that%20allows%20reusing%20control%20theory%20results%20for%20NNCS%0Averification.%20By%20joining%20forces%2C%20we%20exploit%20the%20efficiency%20of%20NN%20verification%0Atools%20while%20retaining%20the%20rigor%20of%20differential%20dynamic%20logic%20%28dL%29.%20Based%20on%0Aprovably%20safe%20control%20envelopes%20in%20dL%2C%20we%20derive%20specifications%20for%20the%20NN%0Awhich%20is%20proven%20via%20NN%20verification.%20We%20show%20that%20a%20proof%20of%20the%20NN%20adhering%20to%0Athe%20specification%20is%20mirrored%20by%20a%20dL%20proof%20on%20the%20infinite-time%20safety%20of%20the%0ANNCS.%0A%20%20The%20NN%20verification%20properties%20resulting%20from%20hybrid%20systems%20typically%0Acontain%20nonlinear%20arithmetic%20and%20arbitrary%20logical%20structures%20while%20efficient%0ANN%20verification%20merely%20supports%20linear%20constraints.%20To%20overcome%20this%20divide%2C%20we%0Apresent%20Mosaic%3A%20An%20efficient%2C%20sound%20and%20complete%20verification%20approach%20for%0Apolynomial%20real%20arithmetic%20properties%20on%20piece-wise%20linear%20NNs.%20Mosaic%0Apartitions%20complex%20verification%20queries%20into%20simple%20queries%20and%20lifts%0Aoff-the-shelf%20linear%20constraint%20tools%20to%20the%20nonlinear%20setting%20in%20a%0Acompleteness-preserving%20manner%20by%20combining%20approximation%20with%20exact%20reasoning%0Afor%20counterexample%20regions.%20Our%20evaluation%20demonstrates%20the%20versatility%20of%0AVerSAILLE%20and%20Mosaic%3A%20We%20prove%20infinite-time%20safety%20on%20the%20classical%20Vertical%0AAirborne%20Collision%20Avoidance%20NNCS%20verification%20benchmark%20for%20two%20scenarios%0Awhile%20%28exhaustively%29%20enumerating%20counterexample%20regions%20in%20unsafe%20scenarios.%20We%0Aalso%20show%20that%20our%20approach%20significantly%20outperforms%20State-of-the-Art%20tools%20in%0Aclosed-loop%20NNV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10998v2&entry.124074799=Read"},
{"title": "Gradient-based Learning in State-based Potential Games for Self-Learning\n  Production Systems", "author": "Steve Yuwono and Marlon L\u00f6ppenberg and Dorothea Schwung and Andreas Schwung", "abstract": "  In this paper, we introduce novel gradient-based optimization methods for\nstate-based potential games (SbPGs) within self-learning distributed production\nsystems. SbPGs are recognised for their efficacy in enabling self-optimizing\ndistributed multi-agent systems and offer a proven convergence guarantee, which\nfacilitates collaborative player efforts towards global objectives. Our study\nstrives to replace conventional ad-hoc random exploration-based learning in\nSbPGs with contemporary gradient-based approaches, which aim for faster\nconvergence and smoother exploration dynamics, thereby shortening training\nduration while upholding the efficacy of SbPGs. Moreover, we propose three\ndistinct variants for estimating the objective function of gradient-based\nlearning, each developed to suit the unique characteristics of the systems\nunder consideration. To validate our methodology, we apply it to a laboratory\ntestbed, namely Bulk Good Laboratory Plant, which represents a smart and\nflexible distributed multi-agent production system. The incorporation of\ngradient-based learning in SbPGs reduces training times and achieves more\noptimal policies than its baseline.\n", "link": "http://arxiv.org/abs/2406.10015v1", "date": "2024-06-14", "relevancy": 1.9142, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4907}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4743}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-based%20Learning%20in%20State-based%20Potential%20Games%20for%20Self-Learning%0A%20%20Production%20Systems&body=Title%3A%20Gradient-based%20Learning%20in%20State-based%20Potential%20Games%20for%20Self-Learning%0A%20%20Production%20Systems%0AAuthor%3A%20Steve%20Yuwono%20and%20Marlon%20L%C3%B6ppenberg%20and%20Dorothea%20Schwung%20and%20Andreas%20Schwung%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20novel%20gradient-based%20optimization%20methods%20for%0Astate-based%20potential%20games%20%28SbPGs%29%20within%20self-learning%20distributed%20production%0Asystems.%20SbPGs%20are%20recognised%20for%20their%20efficacy%20in%20enabling%20self-optimizing%0Adistributed%20multi-agent%20systems%20and%20offer%20a%20proven%20convergence%20guarantee%2C%20which%0Afacilitates%20collaborative%20player%20efforts%20towards%20global%20objectives.%20Our%20study%0Astrives%20to%20replace%20conventional%20ad-hoc%20random%20exploration-based%20learning%20in%0ASbPGs%20with%20contemporary%20gradient-based%20approaches%2C%20which%20aim%20for%20faster%0Aconvergence%20and%20smoother%20exploration%20dynamics%2C%20thereby%20shortening%20training%0Aduration%20while%20upholding%20the%20efficacy%20of%20SbPGs.%20Moreover%2C%20we%20propose%20three%0Adistinct%20variants%20for%20estimating%20the%20objective%20function%20of%20gradient-based%0Alearning%2C%20each%20developed%20to%20suit%20the%20unique%20characteristics%20of%20the%20systems%0Aunder%20consideration.%20To%20validate%20our%20methodology%2C%20we%20apply%20it%20to%20a%20laboratory%0Atestbed%2C%20namely%20Bulk%20Good%20Laboratory%20Plant%2C%20which%20represents%20a%20smart%20and%0Aflexible%20distributed%20multi-agent%20production%20system.%20The%20incorporation%20of%0Agradient-based%20learning%20in%20SbPGs%20reduces%20training%20times%20and%20achieves%20more%0Aoptimal%20policies%20than%20its%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-based%2520Learning%2520in%2520State-based%2520Potential%2520Games%2520for%2520Self-Learning%250A%2520%2520Production%2520Systems%26entry.906535625%3DSteve%2520Yuwono%2520and%2520Marlon%2520L%25C3%25B6ppenberg%2520and%2520Dorothea%2520Schwung%2520and%2520Andreas%2520Schwung%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520novel%2520gradient-based%2520optimization%2520methods%2520for%250Astate-based%2520potential%2520games%2520%2528SbPGs%2529%2520within%2520self-learning%2520distributed%2520production%250Asystems.%2520SbPGs%2520are%2520recognised%2520for%2520their%2520efficacy%2520in%2520enabling%2520self-optimizing%250Adistributed%2520multi-agent%2520systems%2520and%2520offer%2520a%2520proven%2520convergence%2520guarantee%252C%2520which%250Afacilitates%2520collaborative%2520player%2520efforts%2520towards%2520global%2520objectives.%2520Our%2520study%250Astrives%2520to%2520replace%2520conventional%2520ad-hoc%2520random%2520exploration-based%2520learning%2520in%250ASbPGs%2520with%2520contemporary%2520gradient-based%2520approaches%252C%2520which%2520aim%2520for%2520faster%250Aconvergence%2520and%2520smoother%2520exploration%2520dynamics%252C%2520thereby%2520shortening%2520training%250Aduration%2520while%2520upholding%2520the%2520efficacy%2520of%2520SbPGs.%2520Moreover%252C%2520we%2520propose%2520three%250Adistinct%2520variants%2520for%2520estimating%2520the%2520objective%2520function%2520of%2520gradient-based%250Alearning%252C%2520each%2520developed%2520to%2520suit%2520the%2520unique%2520characteristics%2520of%2520the%2520systems%250Aunder%2520consideration.%2520To%2520validate%2520our%2520methodology%252C%2520we%2520apply%2520it%2520to%2520a%2520laboratory%250Atestbed%252C%2520namely%2520Bulk%2520Good%2520Laboratory%2520Plant%252C%2520which%2520represents%2520a%2520smart%2520and%250Aflexible%2520distributed%2520multi-agent%2520production%2520system.%2520The%2520incorporation%2520of%250Agradient-based%2520learning%2520in%2520SbPGs%2520reduces%2520training%2520times%2520and%2520achieves%2520more%250Aoptimal%2520policies%2520than%2520its%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-based%20Learning%20in%20State-based%20Potential%20Games%20for%20Self-Learning%0A%20%20Production%20Systems&entry.906535625=Steve%20Yuwono%20and%20Marlon%20L%C3%B6ppenberg%20and%20Dorothea%20Schwung%20and%20Andreas%20Schwung&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20novel%20gradient-based%20optimization%20methods%20for%0Astate-based%20potential%20games%20%28SbPGs%29%20within%20self-learning%20distributed%20production%0Asystems.%20SbPGs%20are%20recognised%20for%20their%20efficacy%20in%20enabling%20self-optimizing%0Adistributed%20multi-agent%20systems%20and%20offer%20a%20proven%20convergence%20guarantee%2C%20which%0Afacilitates%20collaborative%20player%20efforts%20towards%20global%20objectives.%20Our%20study%0Astrives%20to%20replace%20conventional%20ad-hoc%20random%20exploration-based%20learning%20in%0ASbPGs%20with%20contemporary%20gradient-based%20approaches%2C%20which%20aim%20for%20faster%0Aconvergence%20and%20smoother%20exploration%20dynamics%2C%20thereby%20shortening%20training%0Aduration%20while%20upholding%20the%20efficacy%20of%20SbPGs.%20Moreover%2C%20we%20propose%20three%0Adistinct%20variants%20for%20estimating%20the%20objective%20function%20of%20gradient-based%0Alearning%2C%20each%20developed%20to%20suit%20the%20unique%20characteristics%20of%20the%20systems%0Aunder%20consideration.%20To%20validate%20our%20methodology%2C%20we%20apply%20it%20to%20a%20laboratory%0Atestbed%2C%20namely%20Bulk%20Good%20Laboratory%20Plant%2C%20which%20represents%20a%20smart%20and%0Aflexible%20distributed%20multi-agent%20production%20system.%20The%20incorporation%20of%0Agradient-based%20learning%20in%20SbPGs%20reduces%20training%20times%20and%20achieves%20more%0Aoptimal%20policies%20than%20its%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10015v1&entry.124074799=Read"},
{"title": "Detecting and Evaluating Medical Hallucinations in Large Vision Language\n  Models", "author": "Jiawei Chen and Dingkang Yang and Tong Wu and Yue Jiang and Xiaolu Hou and Mingcheng Li and Shunli Wang and Dongling Xiao and Ke Li and Lihua Zhang", "abstract": "  Large Vision Language Models (LVLMs) are increasingly integral to healthcare\napplications, including medical visual question answering and imaging report\ngeneration. While these models inherit the robust capabilities of foundational\nLarge Language Models (LLMs), they also inherit susceptibility to\nhallucinations-a significant concern in high-stakes medical contexts where the\nmargin for error is minimal. However, currently, there are no dedicated methods\nor benchmarks for hallucination detection and evaluation in the medical field.\nTo bridge this gap, we introduce Med-HallMark, the first benchmark specifically\ndesigned for hallucination detection and evaluation within the medical\nmultimodal domain. This benchmark provides multi-tasking hallucination support,\nmultifaceted hallucination data, and hierarchical hallucination categorization.\nFurthermore, we propose the MediHall Score, a new medical evaluative metric\ndesigned to assess LVLMs' hallucinations through a hierarchical scoring system\nthat considers the severity and type of hallucination, thereby enabling a\ngranular assessment of potential clinical impacts. We also present\nMediHallDetector, a novel Medical LVLM engineered for precise hallucination\ndetection, which employs multitask training for hallucination detection.\nThrough extensive experimental evaluations, we establish baselines for popular\nLVLMs using our benchmark. The findings indicate that MediHall Score provides a\nmore nuanced understanding of hallucination impacts compared to traditional\nmetrics and demonstrate the enhanced performance of MediHallDetector. We hope\nthis work can significantly improve the reliability of LVLMs in medical\napplications. All resources of this work will be released soon.\n", "link": "http://arxiv.org/abs/2406.10185v1", "date": "2024-06-14", "relevancy": 1.4524, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5473}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4691}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20and%20Evaluating%20Medical%20Hallucinations%20in%20Large%20Vision%20Language%0A%20%20Models&body=Title%3A%20Detecting%20and%20Evaluating%20Medical%20Hallucinations%20in%20Large%20Vision%20Language%0A%20%20Models%0AAuthor%3A%20Jiawei%20Chen%20and%20Dingkang%20Yang%20and%20Tong%20Wu%20and%20Yue%20Jiang%20and%20Xiaolu%20Hou%20and%20Mingcheng%20Li%20and%20Shunli%20Wang%20and%20Dongling%20Xiao%20and%20Ke%20Li%20and%20Lihua%20Zhang%0AAbstract%3A%20%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20are%20increasingly%20integral%20to%20healthcare%0Aapplications%2C%20including%20medical%20visual%20question%20answering%20and%20imaging%20report%0Ageneration.%20While%20these%20models%20inherit%20the%20robust%20capabilities%20of%20foundational%0ALarge%20Language%20Models%20%28LLMs%29%2C%20they%20also%20inherit%20susceptibility%20to%0Ahallucinations-a%20significant%20concern%20in%20high-stakes%20medical%20contexts%20where%20the%0Amargin%20for%20error%20is%20minimal.%20However%2C%20currently%2C%20there%20are%20no%20dedicated%20methods%0Aor%20benchmarks%20for%20hallucination%20detection%20and%20evaluation%20in%20the%20medical%20field.%0ATo%20bridge%20this%20gap%2C%20we%20introduce%20Med-HallMark%2C%20the%20first%20benchmark%20specifically%0Adesigned%20for%20hallucination%20detection%20and%20evaluation%20within%20the%20medical%0Amultimodal%20domain.%20This%20benchmark%20provides%20multi-tasking%20hallucination%20support%2C%0Amultifaceted%20hallucination%20data%2C%20and%20hierarchical%20hallucination%20categorization.%0AFurthermore%2C%20we%20propose%20the%20MediHall%20Score%2C%20a%20new%20medical%20evaluative%20metric%0Adesigned%20to%20assess%20LVLMs%27%20hallucinations%20through%20a%20hierarchical%20scoring%20system%0Athat%20considers%20the%20severity%20and%20type%20of%20hallucination%2C%20thereby%20enabling%20a%0Agranular%20assessment%20of%20potential%20clinical%20impacts.%20We%20also%20present%0AMediHallDetector%2C%20a%20novel%20Medical%20LVLM%20engineered%20for%20precise%20hallucination%0Adetection%2C%20which%20employs%20multitask%20training%20for%20hallucination%20detection.%0AThrough%20extensive%20experimental%20evaluations%2C%20we%20establish%20baselines%20for%20popular%0ALVLMs%20using%20our%20benchmark.%20The%20findings%20indicate%20that%20MediHall%20Score%20provides%20a%0Amore%20nuanced%20understanding%20of%20hallucination%20impacts%20compared%20to%20traditional%0Ametrics%20and%20demonstrate%20the%20enhanced%20performance%20of%20MediHallDetector.%20We%20hope%0Athis%20work%20can%20significantly%20improve%20the%20reliability%20of%20LVLMs%20in%20medical%0Aapplications.%20All%20resources%20of%20this%20work%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520and%2520Evaluating%2520Medical%2520Hallucinations%2520in%2520Large%2520Vision%2520Language%250A%2520%2520Models%26entry.906535625%3DJiawei%2520Chen%2520and%2520Dingkang%2520Yang%2520and%2520Tong%2520Wu%2520and%2520Yue%2520Jiang%2520and%2520Xiaolu%2520Hou%2520and%2520Mingcheng%2520Li%2520and%2520Shunli%2520Wang%2520and%2520Dongling%2520Xiao%2520and%2520Ke%2520Li%2520and%2520Lihua%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520are%2520increasingly%2520integral%2520to%2520healthcare%250Aapplications%252C%2520including%2520medical%2520visual%2520question%2520answering%2520and%2520imaging%2520report%250Ageneration.%2520While%2520these%2520models%2520inherit%2520the%2520robust%2520capabilities%2520of%2520foundational%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%252C%2520they%2520also%2520inherit%2520susceptibility%2520to%250Ahallucinations-a%2520significant%2520concern%2520in%2520high-stakes%2520medical%2520contexts%2520where%2520the%250Amargin%2520for%2520error%2520is%2520minimal.%2520However%252C%2520currently%252C%2520there%2520are%2520no%2520dedicated%2520methods%250Aor%2520benchmarks%2520for%2520hallucination%2520detection%2520and%2520evaluation%2520in%2520the%2520medical%2520field.%250ATo%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520Med-HallMark%252C%2520the%2520first%2520benchmark%2520specifically%250Adesigned%2520for%2520hallucination%2520detection%2520and%2520evaluation%2520within%2520the%2520medical%250Amultimodal%2520domain.%2520This%2520benchmark%2520provides%2520multi-tasking%2520hallucination%2520support%252C%250Amultifaceted%2520hallucination%2520data%252C%2520and%2520hierarchical%2520hallucination%2520categorization.%250AFurthermore%252C%2520we%2520propose%2520the%2520MediHall%2520Score%252C%2520a%2520new%2520medical%2520evaluative%2520metric%250Adesigned%2520to%2520assess%2520LVLMs%2527%2520hallucinations%2520through%2520a%2520hierarchical%2520scoring%2520system%250Athat%2520considers%2520the%2520severity%2520and%2520type%2520of%2520hallucination%252C%2520thereby%2520enabling%2520a%250Agranular%2520assessment%2520of%2520potential%2520clinical%2520impacts.%2520We%2520also%2520present%250AMediHallDetector%252C%2520a%2520novel%2520Medical%2520LVLM%2520engineered%2520for%2520precise%2520hallucination%250Adetection%252C%2520which%2520employs%2520multitask%2520training%2520for%2520hallucination%2520detection.%250AThrough%2520extensive%2520experimental%2520evaluations%252C%2520we%2520establish%2520baselines%2520for%2520popular%250ALVLMs%2520using%2520our%2520benchmark.%2520The%2520findings%2520indicate%2520that%2520MediHall%2520Score%2520provides%2520a%250Amore%2520nuanced%2520understanding%2520of%2520hallucination%2520impacts%2520compared%2520to%2520traditional%250Ametrics%2520and%2520demonstrate%2520the%2520enhanced%2520performance%2520of%2520MediHallDetector.%2520We%2520hope%250Athis%2520work%2520can%2520significantly%2520improve%2520the%2520reliability%2520of%2520LVLMs%2520in%2520medical%250Aapplications.%2520All%2520resources%2520of%2520this%2520work%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20and%20Evaluating%20Medical%20Hallucinations%20in%20Large%20Vision%20Language%0A%20%20Models&entry.906535625=Jiawei%20Chen%20and%20Dingkang%20Yang%20and%20Tong%20Wu%20and%20Yue%20Jiang%20and%20Xiaolu%20Hou%20and%20Mingcheng%20Li%20and%20Shunli%20Wang%20and%20Dongling%20Xiao%20and%20Ke%20Li%20and%20Lihua%20Zhang&entry.1292438233=%20%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20are%20increasingly%20integral%20to%20healthcare%0Aapplications%2C%20including%20medical%20visual%20question%20answering%20and%20imaging%20report%0Ageneration.%20While%20these%20models%20inherit%20the%20robust%20capabilities%20of%20foundational%0ALarge%20Language%20Models%20%28LLMs%29%2C%20they%20also%20inherit%20susceptibility%20to%0Ahallucinations-a%20significant%20concern%20in%20high-stakes%20medical%20contexts%20where%20the%0Amargin%20for%20error%20is%20minimal.%20However%2C%20currently%2C%20there%20are%20no%20dedicated%20methods%0Aor%20benchmarks%20for%20hallucination%20detection%20and%20evaluation%20in%20the%20medical%20field.%0ATo%20bridge%20this%20gap%2C%20we%20introduce%20Med-HallMark%2C%20the%20first%20benchmark%20specifically%0Adesigned%20for%20hallucination%20detection%20and%20evaluation%20within%20the%20medical%0Amultimodal%20domain.%20This%20benchmark%20provides%20multi-tasking%20hallucination%20support%2C%0Amultifaceted%20hallucination%20data%2C%20and%20hierarchical%20hallucination%20categorization.%0AFurthermore%2C%20we%20propose%20the%20MediHall%20Score%2C%20a%20new%20medical%20evaluative%20metric%0Adesigned%20to%20assess%20LVLMs%27%20hallucinations%20through%20a%20hierarchical%20scoring%20system%0Athat%20considers%20the%20severity%20and%20type%20of%20hallucination%2C%20thereby%20enabling%20a%0Agranular%20assessment%20of%20potential%20clinical%20impacts.%20We%20also%20present%0AMediHallDetector%2C%20a%20novel%20Medical%20LVLM%20engineered%20for%20precise%20hallucination%0Adetection%2C%20which%20employs%20multitask%20training%20for%20hallucination%20detection.%0AThrough%20extensive%20experimental%20evaluations%2C%20we%20establish%20baselines%20for%20popular%0ALVLMs%20using%20our%20benchmark.%20The%20findings%20indicate%20that%20MediHall%20Score%20provides%20a%0Amore%20nuanced%20understanding%20of%20hallucination%20impacts%20compared%20to%20traditional%0Ametrics%20and%20demonstrate%20the%20enhanced%20performance%20of%20MediHallDetector.%20We%20hope%0Athis%20work%20can%20significantly%20improve%20the%20reliability%20of%20LVLMs%20in%20medical%0Aapplications.%20All%20resources%20of%20this%20work%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10185v1&entry.124074799=Read"},
{"title": "What Does Softmax Probability Tell Us about Classifiers Ranking Across\n  Diverse Test Conditions?", "author": "Weijie Tu and Weijian Deng and Liang Zheng and Tom Gedeon", "abstract": "  This work aims to develop a measure that can accurately rank the performance\nof various classifiers when they are tested on unlabeled data from\nout-of-distribution (OOD) distributions. We commence by demonstrating that\nconventional uncertainty metrics, notably the maximum Softmax prediction\nprobability, possess inherent utility in forecasting model generalization\nacross certain OOD contexts. Building on this insight, we introduce a new\nmeasure called Softmax Correlation (SoftmaxCorr). It calculates the cosine\nsimilarity between a class-class correlation matrix, constructed from Softmax\noutput vectors across an unlabeled test dataset, and a predefined reference\nmatrix that embodies ideal class correlations. A high resemblance of\npredictions to the reference matrix signals that the model delivers confident\nand uniform predictions across all categories, reflecting minimal uncertainty\nand confusion. Through rigorous evaluation across a suite of datasets,\nincluding ImageNet, CIFAR-10, and WILDS, we affirm the predictive validity of\nSoftmaxCorr in accurately forecasting model performance within both\nin-distribution (ID) and OOD settings. Furthermore, we discuss the limitations\nof our proposed measure and suggest avenues for future research.\n", "link": "http://arxiv.org/abs/2406.09908v1", "date": "2024-06-14", "relevancy": 1.3931, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4939}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4635}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Does%20Softmax%20Probability%20Tell%20Us%20about%20Classifiers%20Ranking%20Across%0A%20%20Diverse%20Test%20Conditions%3F&body=Title%3A%20What%20Does%20Softmax%20Probability%20Tell%20Us%20about%20Classifiers%20Ranking%20Across%0A%20%20Diverse%20Test%20Conditions%3F%0AAuthor%3A%20Weijie%20Tu%20and%20Weijian%20Deng%20and%20Liang%20Zheng%20and%20Tom%20Gedeon%0AAbstract%3A%20%20%20This%20work%20aims%20to%20develop%20a%20measure%20that%20can%20accurately%20rank%20the%20performance%0Aof%20various%20classifiers%20when%20they%20are%20tested%20on%20unlabeled%20data%20from%0Aout-of-distribution%20%28OOD%29%20distributions.%20We%20commence%20by%20demonstrating%20that%0Aconventional%20uncertainty%20metrics%2C%20notably%20the%20maximum%20Softmax%20prediction%0Aprobability%2C%20possess%20inherent%20utility%20in%20forecasting%20model%20generalization%0Aacross%20certain%20OOD%20contexts.%20Building%20on%20this%20insight%2C%20we%20introduce%20a%20new%0Ameasure%20called%20Softmax%20Correlation%20%28SoftmaxCorr%29.%20It%20calculates%20the%20cosine%0Asimilarity%20between%20a%20class-class%20correlation%20matrix%2C%20constructed%20from%20Softmax%0Aoutput%20vectors%20across%20an%20unlabeled%20test%20dataset%2C%20and%20a%20predefined%20reference%0Amatrix%20that%20embodies%20ideal%20class%20correlations.%20A%20high%20resemblance%20of%0Apredictions%20to%20the%20reference%20matrix%20signals%20that%20the%20model%20delivers%20confident%0Aand%20uniform%20predictions%20across%20all%20categories%2C%20reflecting%20minimal%20uncertainty%0Aand%20confusion.%20Through%20rigorous%20evaluation%20across%20a%20suite%20of%20datasets%2C%0Aincluding%20ImageNet%2C%20CIFAR-10%2C%20and%20WILDS%2C%20we%20affirm%20the%20predictive%20validity%20of%0ASoftmaxCorr%20in%20accurately%20forecasting%20model%20performance%20within%20both%0Ain-distribution%20%28ID%29%20and%20OOD%20settings.%20Furthermore%2C%20we%20discuss%20the%20limitations%0Aof%20our%20proposed%20measure%20and%20suggest%20avenues%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Does%2520Softmax%2520Probability%2520Tell%2520Us%2520about%2520Classifiers%2520Ranking%2520Across%250A%2520%2520Diverse%2520Test%2520Conditions%253F%26entry.906535625%3DWeijie%2520Tu%2520and%2520Weijian%2520Deng%2520and%2520Liang%2520Zheng%2520and%2520Tom%2520Gedeon%26entry.1292438233%3D%2520%2520This%2520work%2520aims%2520to%2520develop%2520a%2520measure%2520that%2520can%2520accurately%2520rank%2520the%2520performance%250Aof%2520various%2520classifiers%2520when%2520they%2520are%2520tested%2520on%2520unlabeled%2520data%2520from%250Aout-of-distribution%2520%2528OOD%2529%2520distributions.%2520We%2520commence%2520by%2520demonstrating%2520that%250Aconventional%2520uncertainty%2520metrics%252C%2520notably%2520the%2520maximum%2520Softmax%2520prediction%250Aprobability%252C%2520possess%2520inherent%2520utility%2520in%2520forecasting%2520model%2520generalization%250Aacross%2520certain%2520OOD%2520contexts.%2520Building%2520on%2520this%2520insight%252C%2520we%2520introduce%2520a%2520new%250Ameasure%2520called%2520Softmax%2520Correlation%2520%2528SoftmaxCorr%2529.%2520It%2520calculates%2520the%2520cosine%250Asimilarity%2520between%2520a%2520class-class%2520correlation%2520matrix%252C%2520constructed%2520from%2520Softmax%250Aoutput%2520vectors%2520across%2520an%2520unlabeled%2520test%2520dataset%252C%2520and%2520a%2520predefined%2520reference%250Amatrix%2520that%2520embodies%2520ideal%2520class%2520correlations.%2520A%2520high%2520resemblance%2520of%250Apredictions%2520to%2520the%2520reference%2520matrix%2520signals%2520that%2520the%2520model%2520delivers%2520confident%250Aand%2520uniform%2520predictions%2520across%2520all%2520categories%252C%2520reflecting%2520minimal%2520uncertainty%250Aand%2520confusion.%2520Through%2520rigorous%2520evaluation%2520across%2520a%2520suite%2520of%2520datasets%252C%250Aincluding%2520ImageNet%252C%2520CIFAR-10%252C%2520and%2520WILDS%252C%2520we%2520affirm%2520the%2520predictive%2520validity%2520of%250ASoftmaxCorr%2520in%2520accurately%2520forecasting%2520model%2520performance%2520within%2520both%250Ain-distribution%2520%2528ID%2529%2520and%2520OOD%2520settings.%2520Furthermore%252C%2520we%2520discuss%2520the%2520limitations%250Aof%2520our%2520proposed%2520measure%2520and%2520suggest%2520avenues%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Does%20Softmax%20Probability%20Tell%20Us%20about%20Classifiers%20Ranking%20Across%0A%20%20Diverse%20Test%20Conditions%3F&entry.906535625=Weijie%20Tu%20and%20Weijian%20Deng%20and%20Liang%20Zheng%20and%20Tom%20Gedeon&entry.1292438233=%20%20This%20work%20aims%20to%20develop%20a%20measure%20that%20can%20accurately%20rank%20the%20performance%0Aof%20various%20classifiers%20when%20they%20are%20tested%20on%20unlabeled%20data%20from%0Aout-of-distribution%20%28OOD%29%20distributions.%20We%20commence%20by%20demonstrating%20that%0Aconventional%20uncertainty%20metrics%2C%20notably%20the%20maximum%20Softmax%20prediction%0Aprobability%2C%20possess%20inherent%20utility%20in%20forecasting%20model%20generalization%0Aacross%20certain%20OOD%20contexts.%20Building%20on%20this%20insight%2C%20we%20introduce%20a%20new%0Ameasure%20called%20Softmax%20Correlation%20%28SoftmaxCorr%29.%20It%20calculates%20the%20cosine%0Asimilarity%20between%20a%20class-class%20correlation%20matrix%2C%20constructed%20from%20Softmax%0Aoutput%20vectors%20across%20an%20unlabeled%20test%20dataset%2C%20and%20a%20predefined%20reference%0Amatrix%20that%20embodies%20ideal%20class%20correlations.%20A%20high%20resemblance%20of%0Apredictions%20to%20the%20reference%20matrix%20signals%20that%20the%20model%20delivers%20confident%0Aand%20uniform%20predictions%20across%20all%20categories%2C%20reflecting%20minimal%20uncertainty%0Aand%20confusion.%20Through%20rigorous%20evaluation%20across%20a%20suite%20of%20datasets%2C%0Aincluding%20ImageNet%2C%20CIFAR-10%2C%20and%20WILDS%2C%20we%20affirm%20the%20predictive%20validity%20of%0ASoftmaxCorr%20in%20accurately%20forecasting%20model%20performance%20within%20both%0Ain-distribution%20%28ID%29%20and%20OOD%20settings.%20Furthermore%2C%20we%20discuss%20the%20limitations%0Aof%20our%20proposed%20measure%20and%20suggest%20avenues%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09908v1&entry.124074799=Read"},
{"title": "An elementary proof of a universal approximation theorem", "author": "Chris Monico", "abstract": "  In this short note, we give an elementary proof of a universal approximation\ntheorem for neural networks with three hidden layers and increasing,\ncontinuous, bounded activation function. The result is weaker than the best\nknown results, but the proof is elementary in the sense that no machinery\nbeyond undergraduate analysis is used.\n", "link": "http://arxiv.org/abs/2406.10002v1", "date": "2024-06-14", "relevancy": 1.2506, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4231}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4221}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20elementary%20proof%20of%20a%20universal%20approximation%20theorem&body=Title%3A%20An%20elementary%20proof%20of%20a%20universal%20approximation%20theorem%0AAuthor%3A%20Chris%20Monico%0AAbstract%3A%20%20%20In%20this%20short%20note%2C%20we%20give%20an%20elementary%20proof%20of%20a%20universal%20approximation%0Atheorem%20for%20neural%20networks%20with%20three%20hidden%20layers%20and%20increasing%2C%0Acontinuous%2C%20bounded%20activation%20function.%20The%20result%20is%20weaker%20than%20the%20best%0Aknown%20results%2C%20but%20the%20proof%20is%20elementary%20in%20the%20sense%20that%20no%20machinery%0Abeyond%20undergraduate%20analysis%20is%20used.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520elementary%2520proof%2520of%2520a%2520universal%2520approximation%2520theorem%26entry.906535625%3DChris%2520Monico%26entry.1292438233%3D%2520%2520In%2520this%2520short%2520note%252C%2520we%2520give%2520an%2520elementary%2520proof%2520of%2520a%2520universal%2520approximation%250Atheorem%2520for%2520neural%2520networks%2520with%2520three%2520hidden%2520layers%2520and%2520increasing%252C%250Acontinuous%252C%2520bounded%2520activation%2520function.%2520The%2520result%2520is%2520weaker%2520than%2520the%2520best%250Aknown%2520results%252C%2520but%2520the%2520proof%2520is%2520elementary%2520in%2520the%2520sense%2520that%2520no%2520machinery%250Abeyond%2520undergraduate%2520analysis%2520is%2520used.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20elementary%20proof%20of%20a%20universal%20approximation%20theorem&entry.906535625=Chris%20Monico&entry.1292438233=%20%20In%20this%20short%20note%2C%20we%20give%20an%20elementary%20proof%20of%20a%20universal%20approximation%0Atheorem%20for%20neural%20networks%20with%20three%20hidden%20layers%20and%20increasing%2C%0Acontinuous%2C%20bounded%20activation%20function.%20The%20result%20is%20weaker%20than%20the%20best%0Aknown%20results%2C%20but%20the%20proof%20is%20elementary%20in%20the%20sense%20that%20no%20machinery%0Abeyond%20undergraduate%20analysis%20is%20used.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10002v1&entry.124074799=Read"},
{"title": "Harm Mitigation in Recommender Systems under User Preference Dynamics", "author": "Jerry Chee and Shankar Kalyanaraman and Sindhu Kiranmai Ernala and Udi Weinsberg and Sarah Dean and Stratis Ioannidis", "abstract": "  We consider a recommender system that takes into account the interplay\nbetween recommendations, the evolution of user interests, and harmful content.\nWe model the impact of recommendations on user behavior, particularly the\ntendency to consume harmful content. We seek recommendation policies that\nestablish a tradeoff between maximizing click-through rate (CTR) and mitigating\nharm. We establish conditions under which the user profile dynamics have a\nstationary point, and propose algorithms for finding an optimal recommendation\npolicy at stationarity. We experiment on a semi-synthetic movie recommendation\nsetting initialized with real data and observe that our policies outperform\nbaselines at simultaneously maximizing CTR and mitigating harm.\n", "link": "http://arxiv.org/abs/2406.09882v1", "date": "2024-06-14", "relevancy": 0.8244, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4184}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4152}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harm%20Mitigation%20in%20Recommender%20Systems%20under%20User%20Preference%20Dynamics&body=Title%3A%20Harm%20Mitigation%20in%20Recommender%20Systems%20under%20User%20Preference%20Dynamics%0AAuthor%3A%20Jerry%20Chee%20and%20Shankar%20Kalyanaraman%20and%20Sindhu%20Kiranmai%20Ernala%20and%20Udi%20Weinsberg%20and%20Sarah%20Dean%20and%20Stratis%20Ioannidis%0AAbstract%3A%20%20%20We%20consider%20a%20recommender%20system%20that%20takes%20into%20account%20the%20interplay%0Abetween%20recommendations%2C%20the%20evolution%20of%20user%20interests%2C%20and%20harmful%20content.%0AWe%20model%20the%20impact%20of%20recommendations%20on%20user%20behavior%2C%20particularly%20the%0Atendency%20to%20consume%20harmful%20content.%20We%20seek%20recommendation%20policies%20that%0Aestablish%20a%20tradeoff%20between%20maximizing%20click-through%20rate%20%28CTR%29%20and%20mitigating%0Aharm.%20We%20establish%20conditions%20under%20which%20the%20user%20profile%20dynamics%20have%20a%0Astationary%20point%2C%20and%20propose%20algorithms%20for%20finding%20an%20optimal%20recommendation%0Apolicy%20at%20stationarity.%20We%20experiment%20on%20a%20semi-synthetic%20movie%20recommendation%0Asetting%20initialized%20with%20real%20data%20and%20observe%20that%20our%20policies%20outperform%0Abaselines%20at%20simultaneously%20maximizing%20CTR%20and%20mitigating%20harm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09882v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarm%2520Mitigation%2520in%2520Recommender%2520Systems%2520under%2520User%2520Preference%2520Dynamics%26entry.906535625%3DJerry%2520Chee%2520and%2520Shankar%2520Kalyanaraman%2520and%2520Sindhu%2520Kiranmai%2520Ernala%2520and%2520Udi%2520Weinsberg%2520and%2520Sarah%2520Dean%2520and%2520Stratis%2520Ioannidis%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520recommender%2520system%2520that%2520takes%2520into%2520account%2520the%2520interplay%250Abetween%2520recommendations%252C%2520the%2520evolution%2520of%2520user%2520interests%252C%2520and%2520harmful%2520content.%250AWe%2520model%2520the%2520impact%2520of%2520recommendations%2520on%2520user%2520behavior%252C%2520particularly%2520the%250Atendency%2520to%2520consume%2520harmful%2520content.%2520We%2520seek%2520recommendation%2520policies%2520that%250Aestablish%2520a%2520tradeoff%2520between%2520maximizing%2520click-through%2520rate%2520%2528CTR%2529%2520and%2520mitigating%250Aharm.%2520We%2520establish%2520conditions%2520under%2520which%2520the%2520user%2520profile%2520dynamics%2520have%2520a%250Astationary%2520point%252C%2520and%2520propose%2520algorithms%2520for%2520finding%2520an%2520optimal%2520recommendation%250Apolicy%2520at%2520stationarity.%2520We%2520experiment%2520on%2520a%2520semi-synthetic%2520movie%2520recommendation%250Asetting%2520initialized%2520with%2520real%2520data%2520and%2520observe%2520that%2520our%2520policies%2520outperform%250Abaselines%2520at%2520simultaneously%2520maximizing%2520CTR%2520and%2520mitigating%2520harm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09882v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harm%20Mitigation%20in%20Recommender%20Systems%20under%20User%20Preference%20Dynamics&entry.906535625=Jerry%20Chee%20and%20Shankar%20Kalyanaraman%20and%20Sindhu%20Kiranmai%20Ernala%20and%20Udi%20Weinsberg%20and%20Sarah%20Dean%20and%20Stratis%20Ioannidis&entry.1292438233=%20%20We%20consider%20a%20recommender%20system%20that%20takes%20into%20account%20the%20interplay%0Abetween%20recommendations%2C%20the%20evolution%20of%20user%20interests%2C%20and%20harmful%20content.%0AWe%20model%20the%20impact%20of%20recommendations%20on%20user%20behavior%2C%20particularly%20the%0Atendency%20to%20consume%20harmful%20content.%20We%20seek%20recommendation%20policies%20that%0Aestablish%20a%20tradeoff%20between%20maximizing%20click-through%20rate%20%28CTR%29%20and%20mitigating%0Aharm.%20We%20establish%20conditions%20under%20which%20the%20user%20profile%20dynamics%20have%20a%0Astationary%20point%2C%20and%20propose%20algorithms%20for%20finding%20an%20optimal%20recommendation%0Apolicy%20at%20stationarity.%20We%20experiment%20on%20a%20semi-synthetic%20movie%20recommendation%0Asetting%20initialized%20with%20real%20data%20and%20observe%20that%20our%20policies%20outperform%0Abaselines%20at%20simultaneously%20maximizing%20CTR%20and%20mitigating%20harm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09882v1&entry.124074799=Read"},
{"title": "Investigating Gender Fairness in Machine Learning-driven Personalized\n  Care for Chronic Pain", "author": "Pratik Gajane and Sean Newman and Mykola Pechenizkiy and John D. Piette", "abstract": "  Chronic pain significantly diminishes the quality of life for millions\nworldwide. While psychoeducation and therapy can improve pain outcomes, many\nindividuals experiencing pain lack access to evidence-based treatments or fail\nto complete the necessary number of sessions to achieve benefit. Reinforcement\nlearning (RL) shows potential in tailoring personalized pain management\ninterventions according to patients' individual needs while ensuring the\nefficient use of scarce clinical resources. However, clinicians, patients, and\nhealthcare decision-makers are concerned that RL solutions could exacerbate\ndisparities associated with patient characteristics like race or gender. In\nthis article, we study gender fairness in personalized pain care\nrecommendations using a real-world application of reinforcement learning\n(Piette et al., 2022a). Here, adhering to gender fairness translates to minimal\nor no disparity in the utility received by subpopulations as defined by gender.\nWe investigate whether the selection of relevant patient information (referred\nto as features) used to assist decision-making affects gender fairness. Our\nexperiments, conducted using real-world data Piette, 2022), indicate that\nincluded features can impact gender fairness. Moreover, we propose an RL\nsolution, NestedRecommendation, that demonstrates the ability: i) to adaptively\nlearn to select the features that optimize for utility and fairness, and ii) to\naccelerate feature selection and in turn, improve pain care recommendations\nfrom early on, by leveraging clinicians' domain expertise.\n", "link": "http://arxiv.org/abs/2402.19226v3", "date": "2024-06-14", "relevancy": 1.263, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4393}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4254}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Gender%20Fairness%20in%20Machine%20Learning-driven%20Personalized%0A%20%20Care%20for%20Chronic%20Pain&body=Title%3A%20Investigating%20Gender%20Fairness%20in%20Machine%20Learning-driven%20Personalized%0A%20%20Care%20for%20Chronic%20Pain%0AAuthor%3A%20Pratik%20Gajane%20and%20Sean%20Newman%20and%20Mykola%20Pechenizkiy%20and%20John%20D.%20Piette%0AAbstract%3A%20%20%20Chronic%20pain%20significantly%20diminishes%20the%20quality%20of%20life%20for%20millions%0Aworldwide.%20While%20psychoeducation%20and%20therapy%20can%20improve%20pain%20outcomes%2C%20many%0Aindividuals%20experiencing%20pain%20lack%20access%20to%20evidence-based%20treatments%20or%20fail%0Ato%20complete%20the%20necessary%20number%20of%20sessions%20to%20achieve%20benefit.%20Reinforcement%0Alearning%20%28RL%29%20shows%20potential%20in%20tailoring%20personalized%20pain%20management%0Ainterventions%20according%20to%20patients%27%20individual%20needs%20while%20ensuring%20the%0Aefficient%20use%20of%20scarce%20clinical%20resources.%20However%2C%20clinicians%2C%20patients%2C%20and%0Ahealthcare%20decision-makers%20are%20concerned%20that%20RL%20solutions%20could%20exacerbate%0Adisparities%20associated%20with%20patient%20characteristics%20like%20race%20or%20gender.%20In%0Athis%20article%2C%20we%20study%20gender%20fairness%20in%20personalized%20pain%20care%0Arecommendations%20using%20a%20real-world%20application%20of%20reinforcement%20learning%0A%28Piette%20et%20al.%2C%202022a%29.%20Here%2C%20adhering%20to%20gender%20fairness%20translates%20to%20minimal%0Aor%20no%20disparity%20in%20the%20utility%20received%20by%20subpopulations%20as%20defined%20by%20gender.%0AWe%20investigate%20whether%20the%20selection%20of%20relevant%20patient%20information%20%28referred%0Ato%20as%20features%29%20used%20to%20assist%20decision-making%20affects%20gender%20fairness.%20Our%0Aexperiments%2C%20conducted%20using%20real-world%20data%20Piette%2C%202022%29%2C%20indicate%20that%0Aincluded%20features%20can%20impact%20gender%20fairness.%20Moreover%2C%20we%20propose%20an%20RL%0Asolution%2C%20NestedRecommendation%2C%20that%20demonstrates%20the%20ability%3A%20i%29%20to%20adaptively%0Alearn%20to%20select%20the%20features%20that%20optimize%20for%20utility%20and%20fairness%2C%20and%20ii%29%20to%0Aaccelerate%20feature%20selection%20and%20in%20turn%2C%20improve%20pain%20care%20recommendations%0Afrom%20early%20on%2C%20by%20leveraging%20clinicians%27%20domain%20expertise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19226v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Gender%2520Fairness%2520in%2520Machine%2520Learning-driven%2520Personalized%250A%2520%2520Care%2520for%2520Chronic%2520Pain%26entry.906535625%3DPratik%2520Gajane%2520and%2520Sean%2520Newman%2520and%2520Mykola%2520Pechenizkiy%2520and%2520John%2520D.%2520Piette%26entry.1292438233%3D%2520%2520Chronic%2520pain%2520significantly%2520diminishes%2520the%2520quality%2520of%2520life%2520for%2520millions%250Aworldwide.%2520While%2520psychoeducation%2520and%2520therapy%2520can%2520improve%2520pain%2520outcomes%252C%2520many%250Aindividuals%2520experiencing%2520pain%2520lack%2520access%2520to%2520evidence-based%2520treatments%2520or%2520fail%250Ato%2520complete%2520the%2520necessary%2520number%2520of%2520sessions%2520to%2520achieve%2520benefit.%2520Reinforcement%250Alearning%2520%2528RL%2529%2520shows%2520potential%2520in%2520tailoring%2520personalized%2520pain%2520management%250Ainterventions%2520according%2520to%2520patients%2527%2520individual%2520needs%2520while%2520ensuring%2520the%250Aefficient%2520use%2520of%2520scarce%2520clinical%2520resources.%2520However%252C%2520clinicians%252C%2520patients%252C%2520and%250Ahealthcare%2520decision-makers%2520are%2520concerned%2520that%2520RL%2520solutions%2520could%2520exacerbate%250Adisparities%2520associated%2520with%2520patient%2520characteristics%2520like%2520race%2520or%2520gender.%2520In%250Athis%2520article%252C%2520we%2520study%2520gender%2520fairness%2520in%2520personalized%2520pain%2520care%250Arecommendations%2520using%2520a%2520real-world%2520application%2520of%2520reinforcement%2520learning%250A%2528Piette%2520et%2520al.%252C%25202022a%2529.%2520Here%252C%2520adhering%2520to%2520gender%2520fairness%2520translates%2520to%2520minimal%250Aor%2520no%2520disparity%2520in%2520the%2520utility%2520received%2520by%2520subpopulations%2520as%2520defined%2520by%2520gender.%250AWe%2520investigate%2520whether%2520the%2520selection%2520of%2520relevant%2520patient%2520information%2520%2528referred%250Ato%2520as%2520features%2529%2520used%2520to%2520assist%2520decision-making%2520affects%2520gender%2520fairness.%2520Our%250Aexperiments%252C%2520conducted%2520using%2520real-world%2520data%2520Piette%252C%25202022%2529%252C%2520indicate%2520that%250Aincluded%2520features%2520can%2520impact%2520gender%2520fairness.%2520Moreover%252C%2520we%2520propose%2520an%2520RL%250Asolution%252C%2520NestedRecommendation%252C%2520that%2520demonstrates%2520the%2520ability%253A%2520i%2529%2520to%2520adaptively%250Alearn%2520to%2520select%2520the%2520features%2520that%2520optimize%2520for%2520utility%2520and%2520fairness%252C%2520and%2520ii%2529%2520to%250Aaccelerate%2520feature%2520selection%2520and%2520in%2520turn%252C%2520improve%2520pain%2520care%2520recommendations%250Afrom%2520early%2520on%252C%2520by%2520leveraging%2520clinicians%2527%2520domain%2520expertise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19226v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Gender%20Fairness%20in%20Machine%20Learning-driven%20Personalized%0A%20%20Care%20for%20Chronic%20Pain&entry.906535625=Pratik%20Gajane%20and%20Sean%20Newman%20and%20Mykola%20Pechenizkiy%20and%20John%20D.%20Piette&entry.1292438233=%20%20Chronic%20pain%20significantly%20diminishes%20the%20quality%20of%20life%20for%20millions%0Aworldwide.%20While%20psychoeducation%20and%20therapy%20can%20improve%20pain%20outcomes%2C%20many%0Aindividuals%20experiencing%20pain%20lack%20access%20to%20evidence-based%20treatments%20or%20fail%0Ato%20complete%20the%20necessary%20number%20of%20sessions%20to%20achieve%20benefit.%20Reinforcement%0Alearning%20%28RL%29%20shows%20potential%20in%20tailoring%20personalized%20pain%20management%0Ainterventions%20according%20to%20patients%27%20individual%20needs%20while%20ensuring%20the%0Aefficient%20use%20of%20scarce%20clinical%20resources.%20However%2C%20clinicians%2C%20patients%2C%20and%0Ahealthcare%20decision-makers%20are%20concerned%20that%20RL%20solutions%20could%20exacerbate%0Adisparities%20associated%20with%20patient%20characteristics%20like%20race%20or%20gender.%20In%0Athis%20article%2C%20we%20study%20gender%20fairness%20in%20personalized%20pain%20care%0Arecommendations%20using%20a%20real-world%20application%20of%20reinforcement%20learning%0A%28Piette%20et%20al.%2C%202022a%29.%20Here%2C%20adhering%20to%20gender%20fairness%20translates%20to%20minimal%0Aor%20no%20disparity%20in%20the%20utility%20received%20by%20subpopulations%20as%20defined%20by%20gender.%0AWe%20investigate%20whether%20the%20selection%20of%20relevant%20patient%20information%20%28referred%0Ato%20as%20features%29%20used%20to%20assist%20decision-making%20affects%20gender%20fairness.%20Our%0Aexperiments%2C%20conducted%20using%20real-world%20data%20Piette%2C%202022%29%2C%20indicate%20that%0Aincluded%20features%20can%20impact%20gender%20fairness.%20Moreover%2C%20we%20propose%20an%20RL%0Asolution%2C%20NestedRecommendation%2C%20that%20demonstrates%20the%20ability%3A%20i%29%20to%20adaptively%0Alearn%20to%20select%20the%20features%20that%20optimize%20for%20utility%20and%20fairness%2C%20and%20ii%29%20to%0Aaccelerate%20feature%20selection%20and%20in%20turn%2C%20improve%20pain%20care%20recommendations%0Afrom%20early%20on%2C%20by%20leveraging%20clinicians%27%20domain%20expertise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19226v3&entry.124074799=Read"},
{"title": "Group and Shuffle: Efficient Structured Orthogonal Parametrization", "author": "Mikhail Gorbunov and Nikolay Yudin and Vera Soboleva and Aibek Alanov and Alexey Naumov and Maxim Rakhuba", "abstract": "  The increasing size of neural networks has led to a growing demand for\nmethods of efficient fine-tuning. Recently, an orthogonal fine-tuning paradigm\nwas introduced that uses orthogonal matrices for adapting the weights of a\npretrained model. In this paper, we introduce a new class of structured\nmatrices, which unifies and generalizes structured classes from previous works.\nWe examine properties of this class and build a structured orthogonal\nparametrization upon it. We then use this parametrization to modify the\northogonal fine-tuning framework, improving parameter and computational\nefficiency. We empirically validate our method on different domains, including\nadapting of text-to-image diffusion models and downstream task fine-tuning in\nlanguage modeling. Additionally, we adapt our construction for orthogonal\nconvolutions and conduct experiments with 1-Lipschitz neural networks.\n", "link": "http://arxiv.org/abs/2406.10019v1", "date": "2024-06-14", "relevancy": 1.9058, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4781}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4767}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Group%20and%20Shuffle%3A%20Efficient%20Structured%20Orthogonal%20Parametrization&body=Title%3A%20Group%20and%20Shuffle%3A%20Efficient%20Structured%20Orthogonal%20Parametrization%0AAuthor%3A%20Mikhail%20Gorbunov%20and%20Nikolay%20Yudin%20and%20Vera%20Soboleva%20and%20Aibek%20Alanov%20and%20Alexey%20Naumov%20and%20Maxim%20Rakhuba%0AAbstract%3A%20%20%20The%20increasing%20size%20of%20neural%20networks%20has%20led%20to%20a%20growing%20demand%20for%0Amethods%20of%20efficient%20fine-tuning.%20Recently%2C%20an%20orthogonal%20fine-tuning%20paradigm%0Awas%20introduced%20that%20uses%20orthogonal%20matrices%20for%20adapting%20the%20weights%20of%20a%0Apretrained%20model.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20class%20of%20structured%0Amatrices%2C%20which%20unifies%20and%20generalizes%20structured%20classes%20from%20previous%20works.%0AWe%20examine%20properties%20of%20this%20class%20and%20build%20a%20structured%20orthogonal%0Aparametrization%20upon%20it.%20We%20then%20use%20this%20parametrization%20to%20modify%20the%0Aorthogonal%20fine-tuning%20framework%2C%20improving%20parameter%20and%20computational%0Aefficiency.%20We%20empirically%20validate%20our%20method%20on%20different%20domains%2C%20including%0Aadapting%20of%20text-to-image%20diffusion%20models%20and%20downstream%20task%20fine-tuning%20in%0Alanguage%20modeling.%20Additionally%2C%20we%20adapt%20our%20construction%20for%20orthogonal%0Aconvolutions%20and%20conduct%20experiments%20with%201-Lipschitz%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroup%2520and%2520Shuffle%253A%2520Efficient%2520Structured%2520Orthogonal%2520Parametrization%26entry.906535625%3DMikhail%2520Gorbunov%2520and%2520Nikolay%2520Yudin%2520and%2520Vera%2520Soboleva%2520and%2520Aibek%2520Alanov%2520and%2520Alexey%2520Naumov%2520and%2520Maxim%2520Rakhuba%26entry.1292438233%3D%2520%2520The%2520increasing%2520size%2520of%2520neural%2520networks%2520has%2520led%2520to%2520a%2520growing%2520demand%2520for%250Amethods%2520of%2520efficient%2520fine-tuning.%2520Recently%252C%2520an%2520orthogonal%2520fine-tuning%2520paradigm%250Awas%2520introduced%2520that%2520uses%2520orthogonal%2520matrices%2520for%2520adapting%2520the%2520weights%2520of%2520a%250Apretrained%2520model.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520class%2520of%2520structured%250Amatrices%252C%2520which%2520unifies%2520and%2520generalizes%2520structured%2520classes%2520from%2520previous%2520works.%250AWe%2520examine%2520properties%2520of%2520this%2520class%2520and%2520build%2520a%2520structured%2520orthogonal%250Aparametrization%2520upon%2520it.%2520We%2520then%2520use%2520this%2520parametrization%2520to%2520modify%2520the%250Aorthogonal%2520fine-tuning%2520framework%252C%2520improving%2520parameter%2520and%2520computational%250Aefficiency.%2520We%2520empirically%2520validate%2520our%2520method%2520on%2520different%2520domains%252C%2520including%250Aadapting%2520of%2520text-to-image%2520diffusion%2520models%2520and%2520downstream%2520task%2520fine-tuning%2520in%250Alanguage%2520modeling.%2520Additionally%252C%2520we%2520adapt%2520our%2520construction%2520for%2520orthogonal%250Aconvolutions%2520and%2520conduct%2520experiments%2520with%25201-Lipschitz%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Group%20and%20Shuffle%3A%20Efficient%20Structured%20Orthogonal%20Parametrization&entry.906535625=Mikhail%20Gorbunov%20and%20Nikolay%20Yudin%20and%20Vera%20Soboleva%20and%20Aibek%20Alanov%20and%20Alexey%20Naumov%20and%20Maxim%20Rakhuba&entry.1292438233=%20%20The%20increasing%20size%20of%20neural%20networks%20has%20led%20to%20a%20growing%20demand%20for%0Amethods%20of%20efficient%20fine-tuning.%20Recently%2C%20an%20orthogonal%20fine-tuning%20paradigm%0Awas%20introduced%20that%20uses%20orthogonal%20matrices%20for%20adapting%20the%20weights%20of%20a%0Apretrained%20model.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20class%20of%20structured%0Amatrices%2C%20which%20unifies%20and%20generalizes%20structured%20classes%20from%20previous%20works.%0AWe%20examine%20properties%20of%20this%20class%20and%20build%20a%20structured%20orthogonal%0Aparametrization%20upon%20it.%20We%20then%20use%20this%20parametrization%20to%20modify%20the%0Aorthogonal%20fine-tuning%20framework%2C%20improving%20parameter%20and%20computational%0Aefficiency.%20We%20empirically%20validate%20our%20method%20on%20different%20domains%2C%20including%0Aadapting%20of%20text-to-image%20diffusion%20models%20and%20downstream%20task%20fine-tuning%20in%0Alanguage%20modeling.%20Additionally%2C%20we%20adapt%20our%20construction%20for%20orthogonal%0Aconvolutions%20and%20conduct%20experiments%20with%201-Lipschitz%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10019v1&entry.124074799=Read"},
{"title": "Understanding Pedestrian Movement Using Urban Sensing Technologies: The\n  Promise of Audio-based Sensors", "author": "Chaeyeon Han and Pavan Seshadri and Yiwei Ding and Noah Posner and Bon Woo Koo and Animesh Agrawal and Alexander Lerch and Subhrajit Guhathakurta", "abstract": "  While various sensors have been deployed to monitor vehicular flows, sensing\npedestrian movement is still nascent. Yet walking is a significant mode of\ntravel in many cities, especially those in Europe, Africa, and Asia.\nUnderstanding pedestrian volumes and flows is essential for designing safer and\nmore attractive pedestrian infrastructure and for controlling periodic\novercrowding. This study discusses a new approach to scale up urban sensing of\npeople with the help of novel audio-based technology. It assesses the benefits\nand limitations of microphone-based sensors as compared to other forms of\npedestrian sensing. A large-scale dataset called ASPED is presented, which\nincludes high-quality audio recordings along with video recordings used for\nlabeling the pedestrian count data. The baseline analyses highlight the promise\nof using audio sensors for pedestrian tracking, although algorithmic and\ntechnological improvements to make the sensors practically usable continue.\nThis study also demonstrates how the data can be leveraged to predict\npedestrian trajectories. Finally, it discusses the use cases and scenarios\nwhere audio-based pedestrian sensing can support better urban and\ntransportation planning.\n", "link": "http://arxiv.org/abs/2406.09998v1", "date": "2024-06-14", "relevancy": 1.7986, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.509}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4467}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Pedestrian%20Movement%20Using%20Urban%20Sensing%20Technologies%3A%20The%0A%20%20Promise%20of%20Audio-based%20Sensors&body=Title%3A%20Understanding%20Pedestrian%20Movement%20Using%20Urban%20Sensing%20Technologies%3A%20The%0A%20%20Promise%20of%20Audio-based%20Sensors%0AAuthor%3A%20Chaeyeon%20Han%20and%20Pavan%20Seshadri%20and%20Yiwei%20Ding%20and%20Noah%20Posner%20and%20Bon%20Woo%20Koo%20and%20Animesh%20Agrawal%20and%20Alexander%20Lerch%20and%20Subhrajit%20Guhathakurta%0AAbstract%3A%20%20%20While%20various%20sensors%20have%20been%20deployed%20to%20monitor%20vehicular%20flows%2C%20sensing%0Apedestrian%20movement%20is%20still%20nascent.%20Yet%20walking%20is%20a%20significant%20mode%20of%0Atravel%20in%20many%20cities%2C%20especially%20those%20in%20Europe%2C%20Africa%2C%20and%20Asia.%0AUnderstanding%20pedestrian%20volumes%20and%20flows%20is%20essential%20for%20designing%20safer%20and%0Amore%20attractive%20pedestrian%20infrastructure%20and%20for%20controlling%20periodic%0Aovercrowding.%20This%20study%20discusses%20a%20new%20approach%20to%20scale%20up%20urban%20sensing%20of%0Apeople%20with%20the%20help%20of%20novel%20audio-based%20technology.%20It%20assesses%20the%20benefits%0Aand%20limitations%20of%20microphone-based%20sensors%20as%20compared%20to%20other%20forms%20of%0Apedestrian%20sensing.%20A%20large-scale%20dataset%20called%20ASPED%20is%20presented%2C%20which%0Aincludes%20high-quality%20audio%20recordings%20along%20with%20video%20recordings%20used%20for%0Alabeling%20the%20pedestrian%20count%20data.%20The%20baseline%20analyses%20highlight%20the%20promise%0Aof%20using%20audio%20sensors%20for%20pedestrian%20tracking%2C%20although%20algorithmic%20and%0Atechnological%20improvements%20to%20make%20the%20sensors%20practically%20usable%20continue.%0AThis%20study%20also%20demonstrates%20how%20the%20data%20can%20be%20leveraged%20to%20predict%0Apedestrian%20trajectories.%20Finally%2C%20it%20discusses%20the%20use%20cases%20and%20scenarios%0Awhere%20audio-based%20pedestrian%20sensing%20can%20support%20better%20urban%20and%0Atransportation%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Pedestrian%2520Movement%2520Using%2520Urban%2520Sensing%2520Technologies%253A%2520The%250A%2520%2520Promise%2520of%2520Audio-based%2520Sensors%26entry.906535625%3DChaeyeon%2520Han%2520and%2520Pavan%2520Seshadri%2520and%2520Yiwei%2520Ding%2520and%2520Noah%2520Posner%2520and%2520Bon%2520Woo%2520Koo%2520and%2520Animesh%2520Agrawal%2520and%2520Alexander%2520Lerch%2520and%2520Subhrajit%2520Guhathakurta%26entry.1292438233%3D%2520%2520While%2520various%2520sensors%2520have%2520been%2520deployed%2520to%2520monitor%2520vehicular%2520flows%252C%2520sensing%250Apedestrian%2520movement%2520is%2520still%2520nascent.%2520Yet%2520walking%2520is%2520a%2520significant%2520mode%2520of%250Atravel%2520in%2520many%2520cities%252C%2520especially%2520those%2520in%2520Europe%252C%2520Africa%252C%2520and%2520Asia.%250AUnderstanding%2520pedestrian%2520volumes%2520and%2520flows%2520is%2520essential%2520for%2520designing%2520safer%2520and%250Amore%2520attractive%2520pedestrian%2520infrastructure%2520and%2520for%2520controlling%2520periodic%250Aovercrowding.%2520This%2520study%2520discusses%2520a%2520new%2520approach%2520to%2520scale%2520up%2520urban%2520sensing%2520of%250Apeople%2520with%2520the%2520help%2520of%2520novel%2520audio-based%2520technology.%2520It%2520assesses%2520the%2520benefits%250Aand%2520limitations%2520of%2520microphone-based%2520sensors%2520as%2520compared%2520to%2520other%2520forms%2520of%250Apedestrian%2520sensing.%2520A%2520large-scale%2520dataset%2520called%2520ASPED%2520is%2520presented%252C%2520which%250Aincludes%2520high-quality%2520audio%2520recordings%2520along%2520with%2520video%2520recordings%2520used%2520for%250Alabeling%2520the%2520pedestrian%2520count%2520data.%2520The%2520baseline%2520analyses%2520highlight%2520the%2520promise%250Aof%2520using%2520audio%2520sensors%2520for%2520pedestrian%2520tracking%252C%2520although%2520algorithmic%2520and%250Atechnological%2520improvements%2520to%2520make%2520the%2520sensors%2520practically%2520usable%2520continue.%250AThis%2520study%2520also%2520demonstrates%2520how%2520the%2520data%2520can%2520be%2520leveraged%2520to%2520predict%250Apedestrian%2520trajectories.%2520Finally%252C%2520it%2520discusses%2520the%2520use%2520cases%2520and%2520scenarios%250Awhere%2520audio-based%2520pedestrian%2520sensing%2520can%2520support%2520better%2520urban%2520and%250Atransportation%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Pedestrian%20Movement%20Using%20Urban%20Sensing%20Technologies%3A%20The%0A%20%20Promise%20of%20Audio-based%20Sensors&entry.906535625=Chaeyeon%20Han%20and%20Pavan%20Seshadri%20and%20Yiwei%20Ding%20and%20Noah%20Posner%20and%20Bon%20Woo%20Koo%20and%20Animesh%20Agrawal%20and%20Alexander%20Lerch%20and%20Subhrajit%20Guhathakurta&entry.1292438233=%20%20While%20various%20sensors%20have%20been%20deployed%20to%20monitor%20vehicular%20flows%2C%20sensing%0Apedestrian%20movement%20is%20still%20nascent.%20Yet%20walking%20is%20a%20significant%20mode%20of%0Atravel%20in%20many%20cities%2C%20especially%20those%20in%20Europe%2C%20Africa%2C%20and%20Asia.%0AUnderstanding%20pedestrian%20volumes%20and%20flows%20is%20essential%20for%20designing%20safer%20and%0Amore%20attractive%20pedestrian%20infrastructure%20and%20for%20controlling%20periodic%0Aovercrowding.%20This%20study%20discusses%20a%20new%20approach%20to%20scale%20up%20urban%20sensing%20of%0Apeople%20with%20the%20help%20of%20novel%20audio-based%20technology.%20It%20assesses%20the%20benefits%0Aand%20limitations%20of%20microphone-based%20sensors%20as%20compared%20to%20other%20forms%20of%0Apedestrian%20sensing.%20A%20large-scale%20dataset%20called%20ASPED%20is%20presented%2C%20which%0Aincludes%20high-quality%20audio%20recordings%20along%20with%20video%20recordings%20used%20for%0Alabeling%20the%20pedestrian%20count%20data.%20The%20baseline%20analyses%20highlight%20the%20promise%0Aof%20using%20audio%20sensors%20for%20pedestrian%20tracking%2C%20although%20algorithmic%20and%0Atechnological%20improvements%20to%20make%20the%20sensors%20practically%20usable%20continue.%0AThis%20study%20also%20demonstrates%20how%20the%20data%20can%20be%20leveraged%20to%20predict%0Apedestrian%20trajectories.%20Finally%2C%20it%20discusses%20the%20use%20cases%20and%20scenarios%0Awhere%20audio-based%20pedestrian%20sensing%20can%20support%20better%20urban%20and%0Atransportation%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09998v1&entry.124074799=Read"},
{"title": "Beyond Gut Feel: Using Time Series Transformers to Find Investment Gems", "author": "Lele Cao and Gustaf Halvardsson and Andrew McCornack and Vilhelm von Ehrenheim and Pawel Herman", "abstract": "  This paper addresses the growing application of data-driven approaches within\nthe Private Equity (PE) industry, particularly in sourcing investment targets\n(i.e., companies) for Venture Capital (VC) and Growth Capital (GC). We present\na comprehensive review of the relevant approaches and propose a novel approach\nleveraging a Transformer-based Multivariate Time Series Classifier (TMTSC) for\npredicting the success likelihood of any candidate company. The objective of\nour research is to optimize sourcing performance for VC and GC investments by\nformally defining the sourcing problem as a multivariate time series\nclassification task. We consecutively introduce the key components of our\nimplementation which collectively contribute to the successful application of\nTMTSC in VC/GC sourcing: input features, model architecture, optimization\ntarget, and investor-centric data processing. Our extensive experiments on two\nreal-world investment tasks, benchmarked towards three popular baselines,\ndemonstrate the effectiveness of our approach in improving decision making\nwithin the VC and GC industry.\n", "link": "http://arxiv.org/abs/2309.16888v3", "date": "2024-06-14", "relevancy": 1.3457, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4703}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4427}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Gut%20Feel%3A%20Using%20Time%20Series%20Transformers%20to%20Find%20Investment%20Gems&body=Title%3A%20Beyond%20Gut%20Feel%3A%20Using%20Time%20Series%20Transformers%20to%20Find%20Investment%20Gems%0AAuthor%3A%20Lele%20Cao%20and%20Gustaf%20Halvardsson%20and%20Andrew%20McCornack%20and%20Vilhelm%20von%20Ehrenheim%20and%20Pawel%20Herman%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20growing%20application%20of%20data-driven%20approaches%20within%0Athe%20Private%20Equity%20%28PE%29%20industry%2C%20particularly%20in%20sourcing%20investment%20targets%0A%28i.e.%2C%20companies%29%20for%20Venture%20Capital%20%28VC%29%20and%20Growth%20Capital%20%28GC%29.%20We%20present%0Aa%20comprehensive%20review%20of%20the%20relevant%20approaches%20and%20propose%20a%20novel%20approach%0Aleveraging%20a%20Transformer-based%20Multivariate%20Time%20Series%20Classifier%20%28TMTSC%29%20for%0Apredicting%20the%20success%20likelihood%20of%20any%20candidate%20company.%20The%20objective%20of%0Aour%20research%20is%20to%20optimize%20sourcing%20performance%20for%20VC%20and%20GC%20investments%20by%0Aformally%20defining%20the%20sourcing%20problem%20as%20a%20multivariate%20time%20series%0Aclassification%20task.%20We%20consecutively%20introduce%20the%20key%20components%20of%20our%0Aimplementation%20which%20collectively%20contribute%20to%20the%20successful%20application%20of%0ATMTSC%20in%20VC/GC%20sourcing%3A%20input%20features%2C%20model%20architecture%2C%20optimization%0Atarget%2C%20and%20investor-centric%20data%20processing.%20Our%20extensive%20experiments%20on%20two%0Areal-world%20investment%20tasks%2C%20benchmarked%20towards%20three%20popular%20baselines%2C%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20in%20improving%20decision%20making%0Awithin%20the%20VC%20and%20GC%20industry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16888v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Gut%2520Feel%253A%2520Using%2520Time%2520Series%2520Transformers%2520to%2520Find%2520Investment%2520Gems%26entry.906535625%3DLele%2520Cao%2520and%2520Gustaf%2520Halvardsson%2520and%2520Andrew%2520McCornack%2520and%2520Vilhelm%2520von%2520Ehrenheim%2520and%2520Pawel%2520Herman%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520growing%2520application%2520of%2520data-driven%2520approaches%2520within%250Athe%2520Private%2520Equity%2520%2528PE%2529%2520industry%252C%2520particularly%2520in%2520sourcing%2520investment%2520targets%250A%2528i.e.%252C%2520companies%2529%2520for%2520Venture%2520Capital%2520%2528VC%2529%2520and%2520Growth%2520Capital%2520%2528GC%2529.%2520We%2520present%250Aa%2520comprehensive%2520review%2520of%2520the%2520relevant%2520approaches%2520and%2520propose%2520a%2520novel%2520approach%250Aleveraging%2520a%2520Transformer-based%2520Multivariate%2520Time%2520Series%2520Classifier%2520%2528TMTSC%2529%2520for%250Apredicting%2520the%2520success%2520likelihood%2520of%2520any%2520candidate%2520company.%2520The%2520objective%2520of%250Aour%2520research%2520is%2520to%2520optimize%2520sourcing%2520performance%2520for%2520VC%2520and%2520GC%2520investments%2520by%250Aformally%2520defining%2520the%2520sourcing%2520problem%2520as%2520a%2520multivariate%2520time%2520series%250Aclassification%2520task.%2520We%2520consecutively%2520introduce%2520the%2520key%2520components%2520of%2520our%250Aimplementation%2520which%2520collectively%2520contribute%2520to%2520the%2520successful%2520application%2520of%250ATMTSC%2520in%2520VC/GC%2520sourcing%253A%2520input%2520features%252C%2520model%2520architecture%252C%2520optimization%250Atarget%252C%2520and%2520investor-centric%2520data%2520processing.%2520Our%2520extensive%2520experiments%2520on%2520two%250Areal-world%2520investment%2520tasks%252C%2520benchmarked%2520towards%2520three%2520popular%2520baselines%252C%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520improving%2520decision%2520making%250Awithin%2520the%2520VC%2520and%2520GC%2520industry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16888v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Gut%20Feel%3A%20Using%20Time%20Series%20Transformers%20to%20Find%20Investment%20Gems&entry.906535625=Lele%20Cao%20and%20Gustaf%20Halvardsson%20and%20Andrew%20McCornack%20and%20Vilhelm%20von%20Ehrenheim%20and%20Pawel%20Herman&entry.1292438233=%20%20This%20paper%20addresses%20the%20growing%20application%20of%20data-driven%20approaches%20within%0Athe%20Private%20Equity%20%28PE%29%20industry%2C%20particularly%20in%20sourcing%20investment%20targets%0A%28i.e.%2C%20companies%29%20for%20Venture%20Capital%20%28VC%29%20and%20Growth%20Capital%20%28GC%29.%20We%20present%0Aa%20comprehensive%20review%20of%20the%20relevant%20approaches%20and%20propose%20a%20novel%20approach%0Aleveraging%20a%20Transformer-based%20Multivariate%20Time%20Series%20Classifier%20%28TMTSC%29%20for%0Apredicting%20the%20success%20likelihood%20of%20any%20candidate%20company.%20The%20objective%20of%0Aour%20research%20is%20to%20optimize%20sourcing%20performance%20for%20VC%20and%20GC%20investments%20by%0Aformally%20defining%20the%20sourcing%20problem%20as%20a%20multivariate%20time%20series%0Aclassification%20task.%20We%20consecutively%20introduce%20the%20key%20components%20of%20our%0Aimplementation%20which%20collectively%20contribute%20to%20the%20successful%20application%20of%0ATMTSC%20in%20VC/GC%20sourcing%3A%20input%20features%2C%20model%20architecture%2C%20optimization%0Atarget%2C%20and%20investor-centric%20data%20processing.%20Our%20extensive%20experiments%20on%20two%0Areal-world%20investment%20tasks%2C%20benchmarked%20towards%20three%20popular%20baselines%2C%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20in%20improving%20decision%20making%0Awithin%20the%20VC%20and%20GC%20industry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16888v3&entry.124074799=Read"},
{"title": "To what extent can ASV systems naturally defend against spoofing\n  attacks?", "author": "Jee-weon Jung and Xin Wang and Nicholas Evans and Shinji Watanabe and Hye-jin Shim and Hemlata Tak and Sidhhant Arora and Junichi Yamagishi and Joon Son Chung", "abstract": "  The current automatic speaker verification (ASV) task involves making binary\ndecisions on two types of trials: target and non-target. However, emerging\nadvancements in speech generation technology pose significant threats to the\nreliability of ASV systems. This study investigates whether ASV effortlessly\nacquires robustness against spoofing attacks (i.e., zero-shot capability) by\nsystematically exploring diverse ASV systems and spoofing attacks, ranging from\ntraditional to cutting-edge techniques. Through extensive analyses conducted on\neight distinct ASV systems and 29 spoofing attack systems, we demonstrate that\nthe evolution of ASV inherently incorporates defense mechanisms against\nspoofing attacks. Nevertheless, our findings also underscore that the\nadvancement of spoofing attacks far outpaces that of ASV systems, hence\nnecessitating further research on spoofing-robust ASV methodologies.\n", "link": "http://arxiv.org/abs/2406.05339v2", "date": "2024-06-14", "relevancy": 1.557, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3963}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.3867}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20what%20extent%20can%20ASV%20systems%20naturally%20defend%20against%20spoofing%0A%20%20attacks%3F&body=Title%3A%20To%20what%20extent%20can%20ASV%20systems%20naturally%20defend%20against%20spoofing%0A%20%20attacks%3F%0AAuthor%3A%20Jee-weon%20Jung%20and%20Xin%20Wang%20and%20Nicholas%20Evans%20and%20Shinji%20Watanabe%20and%20Hye-jin%20Shim%20and%20Hemlata%20Tak%20and%20Sidhhant%20Arora%20and%20Junichi%20Yamagishi%20and%20Joon%20Son%20Chung%0AAbstract%3A%20%20%20The%20current%20automatic%20speaker%20verification%20%28ASV%29%20task%20involves%20making%20binary%0Adecisions%20on%20two%20types%20of%20trials%3A%20target%20and%20non-target.%20However%2C%20emerging%0Aadvancements%20in%20speech%20generation%20technology%20pose%20significant%20threats%20to%20the%0Areliability%20of%20ASV%20systems.%20This%20study%20investigates%20whether%20ASV%20effortlessly%0Aacquires%20robustness%20against%20spoofing%20attacks%20%28i.e.%2C%20zero-shot%20capability%29%20by%0Asystematically%20exploring%20diverse%20ASV%20systems%20and%20spoofing%20attacks%2C%20ranging%20from%0Atraditional%20to%20cutting-edge%20techniques.%20Through%20extensive%20analyses%20conducted%20on%0Aeight%20distinct%20ASV%20systems%20and%2029%20spoofing%20attack%20systems%2C%20we%20demonstrate%20that%0Athe%20evolution%20of%20ASV%20inherently%20incorporates%20defense%20mechanisms%20against%0Aspoofing%20attacks.%20Nevertheless%2C%20our%20findings%20also%20underscore%20that%20the%0Aadvancement%20of%20spoofing%20attacks%20far%20outpaces%20that%20of%20ASV%20systems%2C%20hence%0Anecessitating%20further%20research%20on%20spoofing-robust%20ASV%20methodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05339v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520what%2520extent%2520can%2520ASV%2520systems%2520naturally%2520defend%2520against%2520spoofing%250A%2520%2520attacks%253F%26entry.906535625%3DJee-weon%2520Jung%2520and%2520Xin%2520Wang%2520and%2520Nicholas%2520Evans%2520and%2520Shinji%2520Watanabe%2520and%2520Hye-jin%2520Shim%2520and%2520Hemlata%2520Tak%2520and%2520Sidhhant%2520Arora%2520and%2520Junichi%2520Yamagishi%2520and%2520Joon%2520Son%2520Chung%26entry.1292438233%3D%2520%2520The%2520current%2520automatic%2520speaker%2520verification%2520%2528ASV%2529%2520task%2520involves%2520making%2520binary%250Adecisions%2520on%2520two%2520types%2520of%2520trials%253A%2520target%2520and%2520non-target.%2520However%252C%2520emerging%250Aadvancements%2520in%2520speech%2520generation%2520technology%2520pose%2520significant%2520threats%2520to%2520the%250Areliability%2520of%2520ASV%2520systems.%2520This%2520study%2520investigates%2520whether%2520ASV%2520effortlessly%250Aacquires%2520robustness%2520against%2520spoofing%2520attacks%2520%2528i.e.%252C%2520zero-shot%2520capability%2529%2520by%250Asystematically%2520exploring%2520diverse%2520ASV%2520systems%2520and%2520spoofing%2520attacks%252C%2520ranging%2520from%250Atraditional%2520to%2520cutting-edge%2520techniques.%2520Through%2520extensive%2520analyses%2520conducted%2520on%250Aeight%2520distinct%2520ASV%2520systems%2520and%252029%2520spoofing%2520attack%2520systems%252C%2520we%2520demonstrate%2520that%250Athe%2520evolution%2520of%2520ASV%2520inherently%2520incorporates%2520defense%2520mechanisms%2520against%250Aspoofing%2520attacks.%2520Nevertheless%252C%2520our%2520findings%2520also%2520underscore%2520that%2520the%250Aadvancement%2520of%2520spoofing%2520attacks%2520far%2520outpaces%2520that%2520of%2520ASV%2520systems%252C%2520hence%250Anecessitating%2520further%2520research%2520on%2520spoofing-robust%2520ASV%2520methodologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05339v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20what%20extent%20can%20ASV%20systems%20naturally%20defend%20against%20spoofing%0A%20%20attacks%3F&entry.906535625=Jee-weon%20Jung%20and%20Xin%20Wang%20and%20Nicholas%20Evans%20and%20Shinji%20Watanabe%20and%20Hye-jin%20Shim%20and%20Hemlata%20Tak%20and%20Sidhhant%20Arora%20and%20Junichi%20Yamagishi%20and%20Joon%20Son%20Chung&entry.1292438233=%20%20The%20current%20automatic%20speaker%20verification%20%28ASV%29%20task%20involves%20making%20binary%0Adecisions%20on%20two%20types%20of%20trials%3A%20target%20and%20non-target.%20However%2C%20emerging%0Aadvancements%20in%20speech%20generation%20technology%20pose%20significant%20threats%20to%20the%0Areliability%20of%20ASV%20systems.%20This%20study%20investigates%20whether%20ASV%20effortlessly%0Aacquires%20robustness%20against%20spoofing%20attacks%20%28i.e.%2C%20zero-shot%20capability%29%20by%0Asystematically%20exploring%20diverse%20ASV%20systems%20and%20spoofing%20attacks%2C%20ranging%20from%0Atraditional%20to%20cutting-edge%20techniques.%20Through%20extensive%20analyses%20conducted%20on%0Aeight%20distinct%20ASV%20systems%20and%2029%20spoofing%20attack%20systems%2C%20we%20demonstrate%20that%0Athe%20evolution%20of%20ASV%20inherently%20incorporates%20defense%20mechanisms%20against%0Aspoofing%20attacks.%20Nevertheless%2C%20our%20findings%20also%20underscore%20that%20the%0Aadvancement%20of%20spoofing%20attacks%20far%20outpaces%20that%20of%20ASV%20systems%2C%20hence%0Anecessitating%20further%20research%20on%20spoofing-robust%20ASV%20methodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05339v2&entry.124074799=Read"},
{"title": "Positive-Unlabelled Learning for Identifying New Candidate Dietary\n  Restriction-related Genes among Ageing-related Genes", "author": "Jorge Paz-Ruza and Alex A. Freitas and Amparo Alonso-Betanzos and Bertha Guijarro-Berdi\u00f1as", "abstract": "  Dietary Restriction (DR) is one of the most popular anti-ageing\ninterventions, prompting exhaustive research into genes associated with its\nmechanisms. Recently, Machine Learning (ML) has been explored to identify\npotential DR-related genes among ageing-related genes, aiming to minimize\ncostly wet lab experiments needed to expand our knowledge on DR. However, to\ntrain a model from positive (DR-related) and negative (non-DR-related)\nexamples, existing ML methods naively label genes without known DR relation as\nnegative examples, assuming that lack of DR-related annotation for a gene\nrepresents evidence of absence of DR-relatedness, rather than absence of\nevidence; this hinders the reliability of the negative examples (non-DR-related\ngenes) and the method's ability to identify novel DR-related genes. This work\nintroduces a novel gene prioritization method based on the two-step\nPositive-Unlabelled (PU) Learning paradigm: using a similarity-based,\nKNN-inspired approach, our method first selects reliable negative examples\namong the genes without known DR associations. Then, these reliable negatives\nand all known positives are used to train a classifier that effectively\ndifferentiates DR-related and non-DR-related genes, which is finally employed\nto generate a more reliable ranking of promising genes for novel\nDR-relatedness. Our method significantly outperforms the existing\nstate-of-the-art non-PU approach for DR-relatedness prediction in three\nrelevant performance metrics. In addition, curation of existing literature\nfinds support for the top-ranked candidate DR-related genes identified by our\nmodel.\n", "link": "http://arxiv.org/abs/2406.09898v1", "date": "2024-06-14", "relevancy": 1.3136, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4739}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4318}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Positive-Unlabelled%20Learning%20for%20Identifying%20New%20Candidate%20Dietary%0A%20%20Restriction-related%20Genes%20among%20Ageing-related%20Genes&body=Title%3A%20Positive-Unlabelled%20Learning%20for%20Identifying%20New%20Candidate%20Dietary%0A%20%20Restriction-related%20Genes%20among%20Ageing-related%20Genes%0AAuthor%3A%20Jorge%20Paz-Ruza%20and%20Alex%20A.%20Freitas%20and%20Amparo%20Alonso-Betanzos%20and%20Bertha%20Guijarro-Berdi%C3%B1as%0AAbstract%3A%20%20%20Dietary%20Restriction%20%28DR%29%20is%20one%20of%20the%20most%20popular%20anti-ageing%0Ainterventions%2C%20prompting%20exhaustive%20research%20into%20genes%20associated%20with%20its%0Amechanisms.%20Recently%2C%20Machine%20Learning%20%28ML%29%20has%20been%20explored%20to%20identify%0Apotential%20DR-related%20genes%20among%20ageing-related%20genes%2C%20aiming%20to%20minimize%0Acostly%20wet%20lab%20experiments%20needed%20to%20expand%20our%20knowledge%20on%20DR.%20However%2C%20to%0Atrain%20a%20model%20from%20positive%20%28DR-related%29%20and%20negative%20%28non-DR-related%29%0Aexamples%2C%20existing%20ML%20methods%20naively%20label%20genes%20without%20known%20DR%20relation%20as%0Anegative%20examples%2C%20assuming%20that%20lack%20of%20DR-related%20annotation%20for%20a%20gene%0Arepresents%20evidence%20of%20absence%20of%20DR-relatedness%2C%20rather%20than%20absence%20of%0Aevidence%3B%20this%20hinders%20the%20reliability%20of%20the%20negative%20examples%20%28non-DR-related%0Agenes%29%20and%20the%20method%27s%20ability%20to%20identify%20novel%20DR-related%20genes.%20This%20work%0Aintroduces%20a%20novel%20gene%20prioritization%20method%20based%20on%20the%20two-step%0APositive-Unlabelled%20%28PU%29%20Learning%20paradigm%3A%20using%20a%20similarity-based%2C%0AKNN-inspired%20approach%2C%20our%20method%20first%20selects%20reliable%20negative%20examples%0Aamong%20the%20genes%20without%20known%20DR%20associations.%20Then%2C%20these%20reliable%20negatives%0Aand%20all%20known%20positives%20are%20used%20to%20train%20a%20classifier%20that%20effectively%0Adifferentiates%20DR-related%20and%20non-DR-related%20genes%2C%20which%20is%20finally%20employed%0Ato%20generate%20a%20more%20reliable%20ranking%20of%20promising%20genes%20for%20novel%0ADR-relatedness.%20Our%20method%20significantly%20outperforms%20the%20existing%0Astate-of-the-art%20non-PU%20approach%20for%20DR-relatedness%20prediction%20in%20three%0Arelevant%20performance%20metrics.%20In%20addition%2C%20curation%20of%20existing%20literature%0Afinds%20support%20for%20the%20top-ranked%20candidate%20DR-related%20genes%20identified%20by%20our%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09898v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPositive-Unlabelled%2520Learning%2520for%2520Identifying%2520New%2520Candidate%2520Dietary%250A%2520%2520Restriction-related%2520Genes%2520among%2520Ageing-related%2520Genes%26entry.906535625%3DJorge%2520Paz-Ruza%2520and%2520Alex%2520A.%2520Freitas%2520and%2520Amparo%2520Alonso-Betanzos%2520and%2520Bertha%2520Guijarro-Berdi%25C3%25B1as%26entry.1292438233%3D%2520%2520Dietary%2520Restriction%2520%2528DR%2529%2520is%2520one%2520of%2520the%2520most%2520popular%2520anti-ageing%250Ainterventions%252C%2520prompting%2520exhaustive%2520research%2520into%2520genes%2520associated%2520with%2520its%250Amechanisms.%2520Recently%252C%2520Machine%2520Learning%2520%2528ML%2529%2520has%2520been%2520explored%2520to%2520identify%250Apotential%2520DR-related%2520genes%2520among%2520ageing-related%2520genes%252C%2520aiming%2520to%2520minimize%250Acostly%2520wet%2520lab%2520experiments%2520needed%2520to%2520expand%2520our%2520knowledge%2520on%2520DR.%2520However%252C%2520to%250Atrain%2520a%2520model%2520from%2520positive%2520%2528DR-related%2529%2520and%2520negative%2520%2528non-DR-related%2529%250Aexamples%252C%2520existing%2520ML%2520methods%2520naively%2520label%2520genes%2520without%2520known%2520DR%2520relation%2520as%250Anegative%2520examples%252C%2520assuming%2520that%2520lack%2520of%2520DR-related%2520annotation%2520for%2520a%2520gene%250Arepresents%2520evidence%2520of%2520absence%2520of%2520DR-relatedness%252C%2520rather%2520than%2520absence%2520of%250Aevidence%253B%2520this%2520hinders%2520the%2520reliability%2520of%2520the%2520negative%2520examples%2520%2528non-DR-related%250Agenes%2529%2520and%2520the%2520method%2527s%2520ability%2520to%2520identify%2520novel%2520DR-related%2520genes.%2520This%2520work%250Aintroduces%2520a%2520novel%2520gene%2520prioritization%2520method%2520based%2520on%2520the%2520two-step%250APositive-Unlabelled%2520%2528PU%2529%2520Learning%2520paradigm%253A%2520using%2520a%2520similarity-based%252C%250AKNN-inspired%2520approach%252C%2520our%2520method%2520first%2520selects%2520reliable%2520negative%2520examples%250Aamong%2520the%2520genes%2520without%2520known%2520DR%2520associations.%2520Then%252C%2520these%2520reliable%2520negatives%250Aand%2520all%2520known%2520positives%2520are%2520used%2520to%2520train%2520a%2520classifier%2520that%2520effectively%250Adifferentiates%2520DR-related%2520and%2520non-DR-related%2520genes%252C%2520which%2520is%2520finally%2520employed%250Ato%2520generate%2520a%2520more%2520reliable%2520ranking%2520of%2520promising%2520genes%2520for%2520novel%250ADR-relatedness.%2520Our%2520method%2520significantly%2520outperforms%2520the%2520existing%250Astate-of-the-art%2520non-PU%2520approach%2520for%2520DR-relatedness%2520prediction%2520in%2520three%250Arelevant%2520performance%2520metrics.%2520In%2520addition%252C%2520curation%2520of%2520existing%2520literature%250Afinds%2520support%2520for%2520the%2520top-ranked%2520candidate%2520DR-related%2520genes%2520identified%2520by%2520our%250Amodel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09898v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Positive-Unlabelled%20Learning%20for%20Identifying%20New%20Candidate%20Dietary%0A%20%20Restriction-related%20Genes%20among%20Ageing-related%20Genes&entry.906535625=Jorge%20Paz-Ruza%20and%20Alex%20A.%20Freitas%20and%20Amparo%20Alonso-Betanzos%20and%20Bertha%20Guijarro-Berdi%C3%B1as&entry.1292438233=%20%20Dietary%20Restriction%20%28DR%29%20is%20one%20of%20the%20most%20popular%20anti-ageing%0Ainterventions%2C%20prompting%20exhaustive%20research%20into%20genes%20associated%20with%20its%0Amechanisms.%20Recently%2C%20Machine%20Learning%20%28ML%29%20has%20been%20explored%20to%20identify%0Apotential%20DR-related%20genes%20among%20ageing-related%20genes%2C%20aiming%20to%20minimize%0Acostly%20wet%20lab%20experiments%20needed%20to%20expand%20our%20knowledge%20on%20DR.%20However%2C%20to%0Atrain%20a%20model%20from%20positive%20%28DR-related%29%20and%20negative%20%28non-DR-related%29%0Aexamples%2C%20existing%20ML%20methods%20naively%20label%20genes%20without%20known%20DR%20relation%20as%0Anegative%20examples%2C%20assuming%20that%20lack%20of%20DR-related%20annotation%20for%20a%20gene%0Arepresents%20evidence%20of%20absence%20of%20DR-relatedness%2C%20rather%20than%20absence%20of%0Aevidence%3B%20this%20hinders%20the%20reliability%20of%20the%20negative%20examples%20%28non-DR-related%0Agenes%29%20and%20the%20method%27s%20ability%20to%20identify%20novel%20DR-related%20genes.%20This%20work%0Aintroduces%20a%20novel%20gene%20prioritization%20method%20based%20on%20the%20two-step%0APositive-Unlabelled%20%28PU%29%20Learning%20paradigm%3A%20using%20a%20similarity-based%2C%0AKNN-inspired%20approach%2C%20our%20method%20first%20selects%20reliable%20negative%20examples%0Aamong%20the%20genes%20without%20known%20DR%20associations.%20Then%2C%20these%20reliable%20negatives%0Aand%20all%20known%20positives%20are%20used%20to%20train%20a%20classifier%20that%20effectively%0Adifferentiates%20DR-related%20and%20non-DR-related%20genes%2C%20which%20is%20finally%20employed%0Ato%20generate%20a%20more%20reliable%20ranking%20of%20promising%20genes%20for%20novel%0ADR-relatedness.%20Our%20method%20significantly%20outperforms%20the%20existing%0Astate-of-the-art%20non-PU%20approach%20for%20DR-relatedness%20prediction%20in%20three%0Arelevant%20performance%20metrics.%20In%20addition%2C%20curation%20of%20existing%20literature%0Afinds%20support%20for%20the%20top-ranked%20candidate%20DR-related%20genes%20identified%20by%20our%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09898v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


