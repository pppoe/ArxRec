<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250525.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "DanceTogether! Identity-Preserving Multi-Person Interactive Video\n  Generation", "author": "Junhao Chen and Mingjin Chen and Jianjin Xu and Xiang Li and Junting Dong and Mingze Sun and Puhua Jiang and Hongxiang Li and Yuhang Yang and Hao Zhao and Xiaoxiao Long and Ruqi Huang", "abstract": "  Controllable video generation (CVG) has advanced rapidly, yet current systems\nfalter when more than one actor must move, interact, and exchange positions\nunder noisy control signals. We address this gap with DanceTogether, the first\nend-to-end diffusion framework that turns a single reference image plus\nindependent pose-mask streams into long, photorealistic videos while strictly\npreserving every identity. A novel MaskPoseAdapter binds \"who\" and \"how\" at\nevery denoising step by fusing robust tracking masks with semantically rich-but\nnoisy-pose heat-maps, eliminating the identity drift and appearance bleeding\nthat plague frame-wise pipelines. To train and evaluate at scale, we introduce\n(i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii)\nHumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain\ntransfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the\nDanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure\nskating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a\nsignificant margin. Moreover, we show that a one-hour fine-tune yields\nconvincing human-robot videos, underscoring broad generalization to embodied-AI\nand HRI tasks. Extensive ablations confirm that persistent identity-action\nbinding is critical to these gains. Together, our model, datasets, and\nbenchmark lift CVG from single-subject choreography to compositionally\ncontrollable, multi-actor interaction, opening new avenues for digital\nproduction, simulation, and embodied intelligence. Our video demos and code are\navailable at https://DanceTog.github.io/.\n", "link": "http://arxiv.org/abs/2505.18078v1", "date": "2025-05-23", "relevancy": 3.3634, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7111}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6749}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DanceTogether%21%20Identity-Preserving%20Multi-Person%20Interactive%20Video%0A%20%20Generation&body=Title%3A%20DanceTogether%21%20Identity-Preserving%20Multi-Person%20Interactive%20Video%0A%20%20Generation%0AAuthor%3A%20Junhao%20Chen%20and%20Mingjin%20Chen%20and%20Jianjin%20Xu%20and%20Xiang%20Li%20and%20Junting%20Dong%20and%20Mingze%20Sun%20and%20Puhua%20Jiang%20and%20Hongxiang%20Li%20and%20Yuhang%20Yang%20and%20Hao%20Zhao%20and%20Xiaoxiao%20Long%20and%20Ruqi%20Huang%0AAbstract%3A%20%20%20Controllable%20video%20generation%20%28CVG%29%20has%20advanced%20rapidly%2C%20yet%20current%20systems%0Afalter%20when%20more%20than%20one%20actor%20must%20move%2C%20interact%2C%20and%20exchange%20positions%0Aunder%20noisy%20control%20signals.%20We%20address%20this%20gap%20with%20DanceTogether%2C%20the%20first%0Aend-to-end%20diffusion%20framework%20that%20turns%20a%20single%20reference%20image%20plus%0Aindependent%20pose-mask%20streams%20into%20long%2C%20photorealistic%20videos%20while%20strictly%0Apreserving%20every%20identity.%20A%20novel%20MaskPoseAdapter%20binds%20%22who%22%20and%20%22how%22%20at%0Aevery%20denoising%20step%20by%20fusing%20robust%20tracking%20masks%20with%20semantically%20rich-but%0Anoisy-pose%20heat-maps%2C%20eliminating%20the%20identity%20drift%20and%20appearance%20bleeding%0Athat%20plague%20frame-wise%20pipelines.%20To%20train%20and%20evaluate%20at%20scale%2C%20we%20introduce%0A%28i%29%20PairFS-4K%2C%2026%20hours%20of%20dual-skater%20footage%20with%207%2C000%2B%20distinct%20IDs%2C%20%28ii%29%0AHumanRob-300%2C%20a%20one-hour%20humanoid-robot%20interaction%20set%20for%20rapid%20cross-domain%0Atransfer%2C%20and%20%28iii%29%20TogetherVideoBench%2C%20a%20three-track%20benchmark%20centered%20on%20the%0ADanceTogEval-100%20test%20suite%20covering%20dance%2C%20boxing%2C%20wrestling%2C%20yoga%2C%20and%20figure%0Askating.%20On%20TogetherVideoBench%2C%20DanceTogether%20outperforms%20the%20prior%20arts%20by%20a%0Asignificant%20margin.%20Moreover%2C%20we%20show%20that%20a%20one-hour%20fine-tune%20yields%0Aconvincing%20human-robot%20videos%2C%20underscoring%20broad%20generalization%20to%20embodied-AI%0Aand%20HRI%20tasks.%20Extensive%20ablations%20confirm%20that%20persistent%20identity-action%0Abinding%20is%20critical%20to%20these%20gains.%20Together%2C%20our%20model%2C%20datasets%2C%20and%0Abenchmark%20lift%20CVG%20from%20single-subject%20choreography%20to%20compositionally%0Acontrollable%2C%20multi-actor%20interaction%2C%20opening%20new%20avenues%20for%20digital%0Aproduction%2C%20simulation%2C%20and%20embodied%20intelligence.%20Our%20video%20demos%20and%20code%20are%0Aavailable%20at%20https%3A//DanceTog.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDanceTogether%2521%2520Identity-Preserving%2520Multi-Person%2520Interactive%2520Video%250A%2520%2520Generation%26entry.906535625%3DJunhao%2520Chen%2520and%2520Mingjin%2520Chen%2520and%2520Jianjin%2520Xu%2520and%2520Xiang%2520Li%2520and%2520Junting%2520Dong%2520and%2520Mingze%2520Sun%2520and%2520Puhua%2520Jiang%2520and%2520Hongxiang%2520Li%2520and%2520Yuhang%2520Yang%2520and%2520Hao%2520Zhao%2520and%2520Xiaoxiao%2520Long%2520and%2520Ruqi%2520Huang%26entry.1292438233%3D%2520%2520Controllable%2520video%2520generation%2520%2528CVG%2529%2520has%2520advanced%2520rapidly%252C%2520yet%2520current%2520systems%250Afalter%2520when%2520more%2520than%2520one%2520actor%2520must%2520move%252C%2520interact%252C%2520and%2520exchange%2520positions%250Aunder%2520noisy%2520control%2520signals.%2520We%2520address%2520this%2520gap%2520with%2520DanceTogether%252C%2520the%2520first%250Aend-to-end%2520diffusion%2520framework%2520that%2520turns%2520a%2520single%2520reference%2520image%2520plus%250Aindependent%2520pose-mask%2520streams%2520into%2520long%252C%2520photorealistic%2520videos%2520while%2520strictly%250Apreserving%2520every%2520identity.%2520A%2520novel%2520MaskPoseAdapter%2520binds%2520%2522who%2522%2520and%2520%2522how%2522%2520at%250Aevery%2520denoising%2520step%2520by%2520fusing%2520robust%2520tracking%2520masks%2520with%2520semantically%2520rich-but%250Anoisy-pose%2520heat-maps%252C%2520eliminating%2520the%2520identity%2520drift%2520and%2520appearance%2520bleeding%250Athat%2520plague%2520frame-wise%2520pipelines.%2520To%2520train%2520and%2520evaluate%2520at%2520scale%252C%2520we%2520introduce%250A%2528i%2529%2520PairFS-4K%252C%252026%2520hours%2520of%2520dual-skater%2520footage%2520with%25207%252C000%252B%2520distinct%2520IDs%252C%2520%2528ii%2529%250AHumanRob-300%252C%2520a%2520one-hour%2520humanoid-robot%2520interaction%2520set%2520for%2520rapid%2520cross-domain%250Atransfer%252C%2520and%2520%2528iii%2529%2520TogetherVideoBench%252C%2520a%2520three-track%2520benchmark%2520centered%2520on%2520the%250ADanceTogEval-100%2520test%2520suite%2520covering%2520dance%252C%2520boxing%252C%2520wrestling%252C%2520yoga%252C%2520and%2520figure%250Askating.%2520On%2520TogetherVideoBench%252C%2520DanceTogether%2520outperforms%2520the%2520prior%2520arts%2520by%2520a%250Asignificant%2520margin.%2520Moreover%252C%2520we%2520show%2520that%2520a%2520one-hour%2520fine-tune%2520yields%250Aconvincing%2520human-robot%2520videos%252C%2520underscoring%2520broad%2520generalization%2520to%2520embodied-AI%250Aand%2520HRI%2520tasks.%2520Extensive%2520ablations%2520confirm%2520that%2520persistent%2520identity-action%250Abinding%2520is%2520critical%2520to%2520these%2520gains.%2520Together%252C%2520our%2520model%252C%2520datasets%252C%2520and%250Abenchmark%2520lift%2520CVG%2520from%2520single-subject%2520choreography%2520to%2520compositionally%250Acontrollable%252C%2520multi-actor%2520interaction%252C%2520opening%2520new%2520avenues%2520for%2520digital%250Aproduction%252C%2520simulation%252C%2520and%2520embodied%2520intelligence.%2520Our%2520video%2520demos%2520and%2520code%2520are%250Aavailable%2520at%2520https%253A//DanceTog.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DanceTogether%21%20Identity-Preserving%20Multi-Person%20Interactive%20Video%0A%20%20Generation&entry.906535625=Junhao%20Chen%20and%20Mingjin%20Chen%20and%20Jianjin%20Xu%20and%20Xiang%20Li%20and%20Junting%20Dong%20and%20Mingze%20Sun%20and%20Puhua%20Jiang%20and%20Hongxiang%20Li%20and%20Yuhang%20Yang%20and%20Hao%20Zhao%20and%20Xiaoxiao%20Long%20and%20Ruqi%20Huang&entry.1292438233=%20%20Controllable%20video%20generation%20%28CVG%29%20has%20advanced%20rapidly%2C%20yet%20current%20systems%0Afalter%20when%20more%20than%20one%20actor%20must%20move%2C%20interact%2C%20and%20exchange%20positions%0Aunder%20noisy%20control%20signals.%20We%20address%20this%20gap%20with%20DanceTogether%2C%20the%20first%0Aend-to-end%20diffusion%20framework%20that%20turns%20a%20single%20reference%20image%20plus%0Aindependent%20pose-mask%20streams%20into%20long%2C%20photorealistic%20videos%20while%20strictly%0Apreserving%20every%20identity.%20A%20novel%20MaskPoseAdapter%20binds%20%22who%22%20and%20%22how%22%20at%0Aevery%20denoising%20step%20by%20fusing%20robust%20tracking%20masks%20with%20semantically%20rich-but%0Anoisy-pose%20heat-maps%2C%20eliminating%20the%20identity%20drift%20and%20appearance%20bleeding%0Athat%20plague%20frame-wise%20pipelines.%20To%20train%20and%20evaluate%20at%20scale%2C%20we%20introduce%0A%28i%29%20PairFS-4K%2C%2026%20hours%20of%20dual-skater%20footage%20with%207%2C000%2B%20distinct%20IDs%2C%20%28ii%29%0AHumanRob-300%2C%20a%20one-hour%20humanoid-robot%20interaction%20set%20for%20rapid%20cross-domain%0Atransfer%2C%20and%20%28iii%29%20TogetherVideoBench%2C%20a%20three-track%20benchmark%20centered%20on%20the%0ADanceTogEval-100%20test%20suite%20covering%20dance%2C%20boxing%2C%20wrestling%2C%20yoga%2C%20and%20figure%0Askating.%20On%20TogetherVideoBench%2C%20DanceTogether%20outperforms%20the%20prior%20arts%20by%20a%0Asignificant%20margin.%20Moreover%2C%20we%20show%20that%20a%20one-hour%20fine-tune%20yields%0Aconvincing%20human-robot%20videos%2C%20underscoring%20broad%20generalization%20to%20embodied-AI%0Aand%20HRI%20tasks.%20Extensive%20ablations%20confirm%20that%20persistent%20identity-action%0Abinding%20is%20critical%20to%20these%20gains.%20Together%2C%20our%20model%2C%20datasets%2C%20and%0Abenchmark%20lift%20CVG%20from%20single-subject%20choreography%20to%20compositionally%0Acontrollable%2C%20multi-actor%20interaction%2C%20opening%20new%20avenues%20for%20digital%0Aproduction%2C%20simulation%2C%20and%20embodied%20intelligence.%20Our%20video%20demos%20and%20code%20are%0Aavailable%20at%20https%3A//DanceTog.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18078v1&entry.124074799=Read"},
{"title": "VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold", "author": "Dominic Maggio and Hyungtae Lim and Luca Carlone", "abstract": "  We present VGGT-SLAM, a dense RGB SLAM system constructed by incrementally\nand globally aligning submaps created from the feed-forward scene\nreconstruction approach VGGT using only uncalibrated monocular cameras. While\nrelated works align submaps using similarity transforms (i.e., translation,\nrotation, and scale), we show that such approaches are inadequate in the case\nof uncalibrated cameras. In particular, we revisit the idea of reconstruction\nambiguity, where given a set of uncalibrated cameras with no assumption on the\ncamera motion or scene structure, the scene can only be reconstructed up to a\n15-degrees-of-freedom projective transformation of the true geometry. This\ninspires us to recover a consistent scene reconstruction across submaps by\noptimizing over the SL(4) manifold, thus estimating 15-degrees-of-freedom\nhomography transforms between sequential submaps while accounting for potential\nloop closure constraints. As verified by extensive experiments, we demonstrate\nthat VGGT-SLAM achieves improved map quality using long video sequences that\nare infeasible for VGGT due to its high GPU requirements.\n", "link": "http://arxiv.org/abs/2505.12549v2", "date": "2025-05-23", "relevancy": 3.3135, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7688}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6148}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VGGT-SLAM%3A%20Dense%20RGB%20SLAM%20Optimized%20on%20the%20SL%284%29%20Manifold&body=Title%3A%20VGGT-SLAM%3A%20Dense%20RGB%20SLAM%20Optimized%20on%20the%20SL%284%29%20Manifold%0AAuthor%3A%20Dominic%20Maggio%20and%20Hyungtae%20Lim%20and%20Luca%20Carlone%0AAbstract%3A%20%20%20We%20present%20VGGT-SLAM%2C%20a%20dense%20RGB%20SLAM%20system%20constructed%20by%20incrementally%0Aand%20globally%20aligning%20submaps%20created%20from%20the%20feed-forward%20scene%0Areconstruction%20approach%20VGGT%20using%20only%20uncalibrated%20monocular%20cameras.%20While%0Arelated%20works%20align%20submaps%20using%20similarity%20transforms%20%28i.e.%2C%20translation%2C%0Arotation%2C%20and%20scale%29%2C%20we%20show%20that%20such%20approaches%20are%20inadequate%20in%20the%20case%0Aof%20uncalibrated%20cameras.%20In%20particular%2C%20we%20revisit%20the%20idea%20of%20reconstruction%0Aambiguity%2C%20where%20given%20a%20set%20of%20uncalibrated%20cameras%20with%20no%20assumption%20on%20the%0Acamera%20motion%20or%20scene%20structure%2C%20the%20scene%20can%20only%20be%20reconstructed%20up%20to%20a%0A15-degrees-of-freedom%20projective%20transformation%20of%20the%20true%20geometry.%20This%0Ainspires%20us%20to%20recover%20a%20consistent%20scene%20reconstruction%20across%20submaps%20by%0Aoptimizing%20over%20the%20SL%284%29%20manifold%2C%20thus%20estimating%2015-degrees-of-freedom%0Ahomography%20transforms%20between%20sequential%20submaps%20while%20accounting%20for%20potential%0Aloop%20closure%20constraints.%20As%20verified%20by%20extensive%20experiments%2C%20we%20demonstrate%0Athat%20VGGT-SLAM%20achieves%20improved%20map%20quality%20using%20long%20video%20sequences%20that%0Aare%20infeasible%20for%20VGGT%20due%20to%20its%20high%20GPU%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12549v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVGGT-SLAM%253A%2520Dense%2520RGB%2520SLAM%2520Optimized%2520on%2520the%2520SL%25284%2529%2520Manifold%26entry.906535625%3DDominic%2520Maggio%2520and%2520Hyungtae%2520Lim%2520and%2520Luca%2520Carlone%26entry.1292438233%3D%2520%2520We%2520present%2520VGGT-SLAM%252C%2520a%2520dense%2520RGB%2520SLAM%2520system%2520constructed%2520by%2520incrementally%250Aand%2520globally%2520aligning%2520submaps%2520created%2520from%2520the%2520feed-forward%2520scene%250Areconstruction%2520approach%2520VGGT%2520using%2520only%2520uncalibrated%2520monocular%2520cameras.%2520While%250Arelated%2520works%2520align%2520submaps%2520using%2520similarity%2520transforms%2520%2528i.e.%252C%2520translation%252C%250Arotation%252C%2520and%2520scale%2529%252C%2520we%2520show%2520that%2520such%2520approaches%2520are%2520inadequate%2520in%2520the%2520case%250Aof%2520uncalibrated%2520cameras.%2520In%2520particular%252C%2520we%2520revisit%2520the%2520idea%2520of%2520reconstruction%250Aambiguity%252C%2520where%2520given%2520a%2520set%2520of%2520uncalibrated%2520cameras%2520with%2520no%2520assumption%2520on%2520the%250Acamera%2520motion%2520or%2520scene%2520structure%252C%2520the%2520scene%2520can%2520only%2520be%2520reconstructed%2520up%2520to%2520a%250A15-degrees-of-freedom%2520projective%2520transformation%2520of%2520the%2520true%2520geometry.%2520This%250Ainspires%2520us%2520to%2520recover%2520a%2520consistent%2520scene%2520reconstruction%2520across%2520submaps%2520by%250Aoptimizing%2520over%2520the%2520SL%25284%2529%2520manifold%252C%2520thus%2520estimating%252015-degrees-of-freedom%250Ahomography%2520transforms%2520between%2520sequential%2520submaps%2520while%2520accounting%2520for%2520potential%250Aloop%2520closure%2520constraints.%2520As%2520verified%2520by%2520extensive%2520experiments%252C%2520we%2520demonstrate%250Athat%2520VGGT-SLAM%2520achieves%2520improved%2520map%2520quality%2520using%2520long%2520video%2520sequences%2520that%250Aare%2520infeasible%2520for%2520VGGT%2520due%2520to%2520its%2520high%2520GPU%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12549v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VGGT-SLAM%3A%20Dense%20RGB%20SLAM%20Optimized%20on%20the%20SL%284%29%20Manifold&entry.906535625=Dominic%20Maggio%20and%20Hyungtae%20Lim%20and%20Luca%20Carlone&entry.1292438233=%20%20We%20present%20VGGT-SLAM%2C%20a%20dense%20RGB%20SLAM%20system%20constructed%20by%20incrementally%0Aand%20globally%20aligning%20submaps%20created%20from%20the%20feed-forward%20scene%0Areconstruction%20approach%20VGGT%20using%20only%20uncalibrated%20monocular%20cameras.%20While%0Arelated%20works%20align%20submaps%20using%20similarity%20transforms%20%28i.e.%2C%20translation%2C%0Arotation%2C%20and%20scale%29%2C%20we%20show%20that%20such%20approaches%20are%20inadequate%20in%20the%20case%0Aof%20uncalibrated%20cameras.%20In%20particular%2C%20we%20revisit%20the%20idea%20of%20reconstruction%0Aambiguity%2C%20where%20given%20a%20set%20of%20uncalibrated%20cameras%20with%20no%20assumption%20on%20the%0Acamera%20motion%20or%20scene%20structure%2C%20the%20scene%20can%20only%20be%20reconstructed%20up%20to%20a%0A15-degrees-of-freedom%20projective%20transformation%20of%20the%20true%20geometry.%20This%0Ainspires%20us%20to%20recover%20a%20consistent%20scene%20reconstruction%20across%20submaps%20by%0Aoptimizing%20over%20the%20SL%284%29%20manifold%2C%20thus%20estimating%2015-degrees-of-freedom%0Ahomography%20transforms%20between%20sequential%20submaps%20while%20accounting%20for%20potential%0Aloop%20closure%20constraints.%20As%20verified%20by%20extensive%20experiments%2C%20we%20demonstrate%0Athat%20VGGT-SLAM%20achieves%20improved%20map%20quality%20using%20long%20video%20sequences%20that%0Aare%20infeasible%20for%20VGGT%20due%20to%20its%20high%20GPU%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12549v2&entry.124074799=Read"},
{"title": "SplatCo: Structure-View Collaborative Gaussian Splatting for\n  Detail-Preserving Rendering of Large-Scale Unbounded Scenes", "author": "Haihong Xiao and Jianan Zou and Yuxin Zhou and Ying He and Wenxiong Kang", "abstract": "  We present SplatCo, a structure-view collaborative Gaussian splatting\nframework for high-fidelity rendering of complex outdoor environments. SplatCo\nbuilds upon two novel components: (1) a cross-structure collaboration module\nthat combines global tri-plane representations, which capture coarse scene\nlayouts, with local context grid features that represent fine surface details.\nThis fusion is achieved through a novel hierarchical compensation strategy,\nensuring both global consistency and local detail preservation; and (2) a\ncross-view assisted training strategy that enhances multi-view consistency by\nsynchronizing gradient updates across viewpoints, applying visibility-aware\ndensification, and pruning overfitted or inaccurate Gaussians based on\nstructural consistency. Through joint optimization of structural representation\nand multi-view coherence, SplatCo effectively reconstructs fine-grained\ngeometric structures and complex textures in large-scale scenes. Comprehensive\nevaluations on 13 diverse large-scale scenes, including Mill19, MatrixCity,\nTanks & Temples, WHU, and custom aerial captures, demonstrate that SplatCo\nconsistently achieves higher reconstruction quality than state-of-the-art\nmethods, with PSNR improvements of 1-2 dB and SSIM gains of 0.1 to 0.2. These\nresults establish a new benchmark for high-fidelity rendering of large-scale\nunbounded scenes. Code and additional information are available at\nhttps://github.com/SCUT-BIP-Lab/SplatCo.\n", "link": "http://arxiv.org/abs/2505.17951v1", "date": "2025-05-23", "relevancy": 3.285, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7126}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6515}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SplatCo%3A%20Structure-View%20Collaborative%20Gaussian%20Splatting%20for%0A%20%20Detail-Preserving%20Rendering%20of%20Large-Scale%20Unbounded%20Scenes&body=Title%3A%20SplatCo%3A%20Structure-View%20Collaborative%20Gaussian%20Splatting%20for%0A%20%20Detail-Preserving%20Rendering%20of%20Large-Scale%20Unbounded%20Scenes%0AAuthor%3A%20Haihong%20Xiao%20and%20Jianan%20Zou%20and%20Yuxin%20Zhou%20and%20Ying%20He%20and%20Wenxiong%20Kang%0AAbstract%3A%20%20%20We%20present%20SplatCo%2C%20a%20structure-view%20collaborative%20Gaussian%20splatting%0Aframework%20for%20high-fidelity%20rendering%20of%20complex%20outdoor%20environments.%20SplatCo%0Abuilds%20upon%20two%20novel%20components%3A%20%281%29%20a%20cross-structure%20collaboration%20module%0Athat%20combines%20global%20tri-plane%20representations%2C%20which%20capture%20coarse%20scene%0Alayouts%2C%20with%20local%20context%20grid%20features%20that%20represent%20fine%20surface%20details.%0AThis%20fusion%20is%20achieved%20through%20a%20novel%20hierarchical%20compensation%20strategy%2C%0Aensuring%20both%20global%20consistency%20and%20local%20detail%20preservation%3B%20and%20%282%29%20a%0Across-view%20assisted%20training%20strategy%20that%20enhances%20multi-view%20consistency%20by%0Asynchronizing%20gradient%20updates%20across%20viewpoints%2C%20applying%20visibility-aware%0Adensification%2C%20and%20pruning%20overfitted%20or%20inaccurate%20Gaussians%20based%20on%0Astructural%20consistency.%20Through%20joint%20optimization%20of%20structural%20representation%0Aand%20multi-view%20coherence%2C%20SplatCo%20effectively%20reconstructs%20fine-grained%0Ageometric%20structures%20and%20complex%20textures%20in%20large-scale%20scenes.%20Comprehensive%0Aevaluations%20on%2013%20diverse%20large-scale%20scenes%2C%20including%20Mill19%2C%20MatrixCity%2C%0ATanks%20%26%20Temples%2C%20WHU%2C%20and%20custom%20aerial%20captures%2C%20demonstrate%20that%20SplatCo%0Aconsistently%20achieves%20higher%20reconstruction%20quality%20than%20state-of-the-art%0Amethods%2C%20with%20PSNR%20improvements%20of%201-2%20dB%20and%20SSIM%20gains%20of%200.1%20to%200.2.%20These%0Aresults%20establish%20a%20new%20benchmark%20for%20high-fidelity%20rendering%20of%20large-scale%0Aunbounded%20scenes.%20Code%20and%20additional%20information%20are%20available%20at%0Ahttps%3A//github.com/SCUT-BIP-Lab/SplatCo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplatCo%253A%2520Structure-View%2520Collaborative%2520Gaussian%2520Splatting%2520for%250A%2520%2520Detail-Preserving%2520Rendering%2520of%2520Large-Scale%2520Unbounded%2520Scenes%26entry.906535625%3DHaihong%2520Xiao%2520and%2520Jianan%2520Zou%2520and%2520Yuxin%2520Zhou%2520and%2520Ying%2520He%2520and%2520Wenxiong%2520Kang%26entry.1292438233%3D%2520%2520We%2520present%2520SplatCo%252C%2520a%2520structure-view%2520collaborative%2520Gaussian%2520splatting%250Aframework%2520for%2520high-fidelity%2520rendering%2520of%2520complex%2520outdoor%2520environments.%2520SplatCo%250Abuilds%2520upon%2520two%2520novel%2520components%253A%2520%25281%2529%2520a%2520cross-structure%2520collaboration%2520module%250Athat%2520combines%2520global%2520tri-plane%2520representations%252C%2520which%2520capture%2520coarse%2520scene%250Alayouts%252C%2520with%2520local%2520context%2520grid%2520features%2520that%2520represent%2520fine%2520surface%2520details.%250AThis%2520fusion%2520is%2520achieved%2520through%2520a%2520novel%2520hierarchical%2520compensation%2520strategy%252C%250Aensuring%2520both%2520global%2520consistency%2520and%2520local%2520detail%2520preservation%253B%2520and%2520%25282%2529%2520a%250Across-view%2520assisted%2520training%2520strategy%2520that%2520enhances%2520multi-view%2520consistency%2520by%250Asynchronizing%2520gradient%2520updates%2520across%2520viewpoints%252C%2520applying%2520visibility-aware%250Adensification%252C%2520and%2520pruning%2520overfitted%2520or%2520inaccurate%2520Gaussians%2520based%2520on%250Astructural%2520consistency.%2520Through%2520joint%2520optimization%2520of%2520structural%2520representation%250Aand%2520multi-view%2520coherence%252C%2520SplatCo%2520effectively%2520reconstructs%2520fine-grained%250Ageometric%2520structures%2520and%2520complex%2520textures%2520in%2520large-scale%2520scenes.%2520Comprehensive%250Aevaluations%2520on%252013%2520diverse%2520large-scale%2520scenes%252C%2520including%2520Mill19%252C%2520MatrixCity%252C%250ATanks%2520%2526%2520Temples%252C%2520WHU%252C%2520and%2520custom%2520aerial%2520captures%252C%2520demonstrate%2520that%2520SplatCo%250Aconsistently%2520achieves%2520higher%2520reconstruction%2520quality%2520than%2520state-of-the-art%250Amethods%252C%2520with%2520PSNR%2520improvements%2520of%25201-2%2520dB%2520and%2520SSIM%2520gains%2520of%25200.1%2520to%25200.2.%2520These%250Aresults%2520establish%2520a%2520new%2520benchmark%2520for%2520high-fidelity%2520rendering%2520of%2520large-scale%250Aunbounded%2520scenes.%2520Code%2520and%2520additional%2520information%2520are%2520available%2520at%250Ahttps%253A//github.com/SCUT-BIP-Lab/SplatCo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SplatCo%3A%20Structure-View%20Collaborative%20Gaussian%20Splatting%20for%0A%20%20Detail-Preserving%20Rendering%20of%20Large-Scale%20Unbounded%20Scenes&entry.906535625=Haihong%20Xiao%20and%20Jianan%20Zou%20and%20Yuxin%20Zhou%20and%20Ying%20He%20and%20Wenxiong%20Kang&entry.1292438233=%20%20We%20present%20SplatCo%2C%20a%20structure-view%20collaborative%20Gaussian%20splatting%0Aframework%20for%20high-fidelity%20rendering%20of%20complex%20outdoor%20environments.%20SplatCo%0Abuilds%20upon%20two%20novel%20components%3A%20%281%29%20a%20cross-structure%20collaboration%20module%0Athat%20combines%20global%20tri-plane%20representations%2C%20which%20capture%20coarse%20scene%0Alayouts%2C%20with%20local%20context%20grid%20features%20that%20represent%20fine%20surface%20details.%0AThis%20fusion%20is%20achieved%20through%20a%20novel%20hierarchical%20compensation%20strategy%2C%0Aensuring%20both%20global%20consistency%20and%20local%20detail%20preservation%3B%20and%20%282%29%20a%0Across-view%20assisted%20training%20strategy%20that%20enhances%20multi-view%20consistency%20by%0Asynchronizing%20gradient%20updates%20across%20viewpoints%2C%20applying%20visibility-aware%0Adensification%2C%20and%20pruning%20overfitted%20or%20inaccurate%20Gaussians%20based%20on%0Astructural%20consistency.%20Through%20joint%20optimization%20of%20structural%20representation%0Aand%20multi-view%20coherence%2C%20SplatCo%20effectively%20reconstructs%20fine-grained%0Ageometric%20structures%20and%20complex%20textures%20in%20large-scale%20scenes.%20Comprehensive%0Aevaluations%20on%2013%20diverse%20large-scale%20scenes%2C%20including%20Mill19%2C%20MatrixCity%2C%0ATanks%20%26%20Temples%2C%20WHU%2C%20and%20custom%20aerial%20captures%2C%20demonstrate%20that%20SplatCo%0Aconsistently%20achieves%20higher%20reconstruction%20quality%20than%20state-of-the-art%0Amethods%2C%20with%20PSNR%20improvements%20of%201-2%20dB%20and%20SSIM%20gains%20of%200.1%20to%200.2.%20These%0Aresults%20establish%20a%20new%20benchmark%20for%20high-fidelity%20rendering%20of%20large-scale%0Aunbounded%20scenes.%20Code%20and%20additional%20information%20are%20available%20at%0Ahttps%3A//github.com/SCUT-BIP-Lab/SplatCo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17951v1&entry.124074799=Read"},
{"title": "WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions", "author": "Zizhang Li and Hong-Xing Yu and Wei Liu and Yin Yang and Charles Herrmann and Gordon Wetzstein and Jiajun Wu", "abstract": "  WonderPlay is a novel framework integrating physics simulation with video\ngeneration for generating action-conditioned dynamic 3D scenes from a single\nimage. While prior works are restricted to rigid body or simple elastic\ndynamics, WonderPlay features a hybrid generative simulator to synthesize a\nwide range of 3D dynamics. The hybrid generative simulator first uses a physics\nsolver to simulate coarse 3D dynamics, which subsequently conditions a video\ngenerator to produce a video with finer, more realistic motion. The generated\nvideo is then used to update the simulated dynamic 3D scene, closing the loop\nbetween the physics solver and the video generator. This approach enables\nintuitive user control to be combined with the accurate dynamics of\nphysics-based simulators and the expressivity of diffusion-based video\ngenerators. Experimental results demonstrate that WonderPlay enables users to\ninteract with various scenes of diverse content, including cloth, sand, snow,\nliquid, smoke, elastic, and rigid bodies -- all using a single image input.\nCode will be made public. Project website:\nhttps://kyleleey.github.io/WonderPlay/\n", "link": "http://arxiv.org/abs/2505.18151v1", "date": "2025-05-23", "relevancy": 3.1735, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7034}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6149}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WonderPlay%3A%20Dynamic%203D%20Scene%20Generation%20from%20a%20Single%20Image%20and%20Actions&body=Title%3A%20WonderPlay%3A%20Dynamic%203D%20Scene%20Generation%20from%20a%20Single%20Image%20and%20Actions%0AAuthor%3A%20Zizhang%20Li%20and%20Hong-Xing%20Yu%20and%20Wei%20Liu%20and%20Yin%20Yang%20and%20Charles%20Herrmann%20and%20Gordon%20Wetzstein%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%20WonderPlay%20is%20a%20novel%20framework%20integrating%20physics%20simulation%20with%20video%0Ageneration%20for%20generating%20action-conditioned%20dynamic%203D%20scenes%20from%20a%20single%0Aimage.%20While%20prior%20works%20are%20restricted%20to%20rigid%20body%20or%20simple%20elastic%0Adynamics%2C%20WonderPlay%20features%20a%20hybrid%20generative%20simulator%20to%20synthesize%20a%0Awide%20range%20of%203D%20dynamics.%20The%20hybrid%20generative%20simulator%20first%20uses%20a%20physics%0Asolver%20to%20simulate%20coarse%203D%20dynamics%2C%20which%20subsequently%20conditions%20a%20video%0Agenerator%20to%20produce%20a%20video%20with%20finer%2C%20more%20realistic%20motion.%20The%20generated%0Avideo%20is%20then%20used%20to%20update%20the%20simulated%20dynamic%203D%20scene%2C%20closing%20the%20loop%0Abetween%20the%20physics%20solver%20and%20the%20video%20generator.%20This%20approach%20enables%0Aintuitive%20user%20control%20to%20be%20combined%20with%20the%20accurate%20dynamics%20of%0Aphysics-based%20simulators%20and%20the%20expressivity%20of%20diffusion-based%20video%0Agenerators.%20Experimental%20results%20demonstrate%20that%20WonderPlay%20enables%20users%20to%0Ainteract%20with%20various%20scenes%20of%20diverse%20content%2C%20including%20cloth%2C%20sand%2C%20snow%2C%0Aliquid%2C%20smoke%2C%20elastic%2C%20and%20rigid%20bodies%20--%20all%20using%20a%20single%20image%20input.%0ACode%20will%20be%20made%20public.%20Project%20website%3A%0Ahttps%3A//kyleleey.github.io/WonderPlay/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWonderPlay%253A%2520Dynamic%25203D%2520Scene%2520Generation%2520from%2520a%2520Single%2520Image%2520and%2520Actions%26entry.906535625%3DZizhang%2520Li%2520and%2520Hong-Xing%2520Yu%2520and%2520Wei%2520Liu%2520and%2520Yin%2520Yang%2520and%2520Charles%2520Herrmann%2520and%2520Gordon%2520Wetzstein%2520and%2520Jiajun%2520Wu%26entry.1292438233%3D%2520%2520WonderPlay%2520is%2520a%2520novel%2520framework%2520integrating%2520physics%2520simulation%2520with%2520video%250Ageneration%2520for%2520generating%2520action-conditioned%2520dynamic%25203D%2520scenes%2520from%2520a%2520single%250Aimage.%2520While%2520prior%2520works%2520are%2520restricted%2520to%2520rigid%2520body%2520or%2520simple%2520elastic%250Adynamics%252C%2520WonderPlay%2520features%2520a%2520hybrid%2520generative%2520simulator%2520to%2520synthesize%2520a%250Awide%2520range%2520of%25203D%2520dynamics.%2520The%2520hybrid%2520generative%2520simulator%2520first%2520uses%2520a%2520physics%250Asolver%2520to%2520simulate%2520coarse%25203D%2520dynamics%252C%2520which%2520subsequently%2520conditions%2520a%2520video%250Agenerator%2520to%2520produce%2520a%2520video%2520with%2520finer%252C%2520more%2520realistic%2520motion.%2520The%2520generated%250Avideo%2520is%2520then%2520used%2520to%2520update%2520the%2520simulated%2520dynamic%25203D%2520scene%252C%2520closing%2520the%2520loop%250Abetween%2520the%2520physics%2520solver%2520and%2520the%2520video%2520generator.%2520This%2520approach%2520enables%250Aintuitive%2520user%2520control%2520to%2520be%2520combined%2520with%2520the%2520accurate%2520dynamics%2520of%250Aphysics-based%2520simulators%2520and%2520the%2520expressivity%2520of%2520diffusion-based%2520video%250Agenerators.%2520Experimental%2520results%2520demonstrate%2520that%2520WonderPlay%2520enables%2520users%2520to%250Ainteract%2520with%2520various%2520scenes%2520of%2520diverse%2520content%252C%2520including%2520cloth%252C%2520sand%252C%2520snow%252C%250Aliquid%252C%2520smoke%252C%2520elastic%252C%2520and%2520rigid%2520bodies%2520--%2520all%2520using%2520a%2520single%2520image%2520input.%250ACode%2520will%2520be%2520made%2520public.%2520Project%2520website%253A%250Ahttps%253A//kyleleey.github.io/WonderPlay/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WonderPlay%3A%20Dynamic%203D%20Scene%20Generation%20from%20a%20Single%20Image%20and%20Actions&entry.906535625=Zizhang%20Li%20and%20Hong-Xing%20Yu%20and%20Wei%20Liu%20and%20Yin%20Yang%20and%20Charles%20Herrmann%20and%20Gordon%20Wetzstein%20and%20Jiajun%20Wu&entry.1292438233=%20%20WonderPlay%20is%20a%20novel%20framework%20integrating%20physics%20simulation%20with%20video%0Ageneration%20for%20generating%20action-conditioned%20dynamic%203D%20scenes%20from%20a%20single%0Aimage.%20While%20prior%20works%20are%20restricted%20to%20rigid%20body%20or%20simple%20elastic%0Adynamics%2C%20WonderPlay%20features%20a%20hybrid%20generative%20simulator%20to%20synthesize%20a%0Awide%20range%20of%203D%20dynamics.%20The%20hybrid%20generative%20simulator%20first%20uses%20a%20physics%0Asolver%20to%20simulate%20coarse%203D%20dynamics%2C%20which%20subsequently%20conditions%20a%20video%0Agenerator%20to%20produce%20a%20video%20with%20finer%2C%20more%20realistic%20motion.%20The%20generated%0Avideo%20is%20then%20used%20to%20update%20the%20simulated%20dynamic%203D%20scene%2C%20closing%20the%20loop%0Abetween%20the%20physics%20solver%20and%20the%20video%20generator.%20This%20approach%20enables%0Aintuitive%20user%20control%20to%20be%20combined%20with%20the%20accurate%20dynamics%20of%0Aphysics-based%20simulators%20and%20the%20expressivity%20of%20diffusion-based%20video%0Agenerators.%20Experimental%20results%20demonstrate%20that%20WonderPlay%20enables%20users%20to%0Ainteract%20with%20various%20scenes%20of%20diverse%20content%2C%20including%20cloth%2C%20sand%2C%20snow%2C%0Aliquid%2C%20smoke%2C%20elastic%2C%20and%20rigid%20bodies%20--%20all%20using%20a%20single%20image%20input.%0ACode%20will%20be%20made%20public.%20Project%20website%3A%0Ahttps%3A//kyleleey.github.io/WonderPlay/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18151v1&entry.124074799=Read"},
{"title": "Multi-Faceted Multimodal Monosemanticity", "author": "Hanqi Yan and Xiangxiang Cui and Lu Yin and Paul Pu Liang and Yulan He and Yifei Wang", "abstract": "  Humans experience the world through multiple modalities, such as, vision,\nlanguage, and speech, making it natural to explore the commonality and\ndistinctions among them. In this work, we take a data-driven approach to\naddress this question by analyzing interpretable, monosemantic features\nextracted from deep multimodal models. Specifically, we investigate CLIP, a\nprominent visual-language representation model trained on massive image-text\npairs. Building on prior research in single-modal interpretability, we develop\na set of multi-modal interpretability tools and measures designed to\ndisentangle and analyze features learned from CLIP. Specifically, we introduce\nthe Modality Dominance Score (MDS) to attribute each CLIP feature to a specific\nmodality. We then map CLIP features into a more interpretable space, enabling\nus to categorize them into three distinct classes: vision features\n(single-modal), language features (single-modal), and visual-language features\n(cross-modal). Interestingly, this data-driven categorization closely aligns\nwith human intuitive understandings of different modalities. We further show\nthat this modality decomposition can benefit multiple downstream tasks,\nincluding reducing bias in gender detection, generating cross-modal adversarial\nexamples, and enabling modal-specific feature control in text-to-image\ngeneration. These results indicate that large-scale multimodal models, when\nequipped with task-agnostic interpretability tools, can offer valuable insights\ninto the relationships between different data modalities.\n", "link": "http://arxiv.org/abs/2502.14888v3", "date": "2025-05-23", "relevancy": 3.1151, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Faceted%20Multimodal%20Monosemanticity&body=Title%3A%20Multi-Faceted%20Multimodal%20Monosemanticity%0AAuthor%3A%20Hanqi%20Yan%20and%20Xiangxiang%20Cui%20and%20Lu%20Yin%20and%20Paul%20Pu%20Liang%20and%20Yulan%20He%20and%20Yifei%20Wang%0AAbstract%3A%20%20%20Humans%20experience%20the%20world%20through%20multiple%20modalities%2C%20such%20as%2C%20vision%2C%0Alanguage%2C%20and%20speech%2C%20making%20it%20natural%20to%20explore%20the%20commonality%20and%0Adistinctions%20among%20them.%20In%20this%20work%2C%20we%20take%20a%20data-driven%20approach%20to%0Aaddress%20this%20question%20by%20analyzing%20interpretable%2C%20monosemantic%20features%0Aextracted%20from%20deep%20multimodal%20models.%20Specifically%2C%20we%20investigate%20CLIP%2C%20a%0Aprominent%20visual-language%20representation%20model%20trained%20on%20massive%20image-text%0Apairs.%20Building%20on%20prior%20research%20in%20single-modal%20interpretability%2C%20we%20develop%0Aa%20set%20of%20multi-modal%20interpretability%20tools%20and%20measures%20designed%20to%0Adisentangle%20and%20analyze%20features%20learned%20from%20CLIP.%20Specifically%2C%20we%20introduce%0Athe%20Modality%20Dominance%20Score%20%28MDS%29%20to%20attribute%20each%20CLIP%20feature%20to%20a%20specific%0Amodality.%20We%20then%20map%20CLIP%20features%20into%20a%20more%20interpretable%20space%2C%20enabling%0Aus%20to%20categorize%20them%20into%20three%20distinct%20classes%3A%20vision%20features%0A%28single-modal%29%2C%20language%20features%20%28single-modal%29%2C%20and%20visual-language%20features%0A%28cross-modal%29.%20Interestingly%2C%20this%20data-driven%20categorization%20closely%20aligns%0Awith%20human%20intuitive%20understandings%20of%20different%20modalities.%20We%20further%20show%0Athat%20this%20modality%20decomposition%20can%20benefit%20multiple%20downstream%20tasks%2C%0Aincluding%20reducing%20bias%20in%20gender%20detection%2C%20generating%20cross-modal%20adversarial%0Aexamples%2C%20and%20enabling%20modal-specific%20feature%20control%20in%20text-to-image%0Ageneration.%20These%20results%20indicate%20that%20large-scale%20multimodal%20models%2C%20when%0Aequipped%20with%20task-agnostic%20interpretability%20tools%2C%20can%20offer%20valuable%20insights%0Ainto%20the%20relationships%20between%20different%20data%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14888v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Faceted%2520Multimodal%2520Monosemanticity%26entry.906535625%3DHanqi%2520Yan%2520and%2520Xiangxiang%2520Cui%2520and%2520Lu%2520Yin%2520and%2520Paul%2520Pu%2520Liang%2520and%2520Yulan%2520He%2520and%2520Yifei%2520Wang%26entry.1292438233%3D%2520%2520Humans%2520experience%2520the%2520world%2520through%2520multiple%2520modalities%252C%2520such%2520as%252C%2520vision%252C%250Alanguage%252C%2520and%2520speech%252C%2520making%2520it%2520natural%2520to%2520explore%2520the%2520commonality%2520and%250Adistinctions%2520among%2520them.%2520In%2520this%2520work%252C%2520we%2520take%2520a%2520data-driven%2520approach%2520to%250Aaddress%2520this%2520question%2520by%2520analyzing%2520interpretable%252C%2520monosemantic%2520features%250Aextracted%2520from%2520deep%2520multimodal%2520models.%2520Specifically%252C%2520we%2520investigate%2520CLIP%252C%2520a%250Aprominent%2520visual-language%2520representation%2520model%2520trained%2520on%2520massive%2520image-text%250Apairs.%2520Building%2520on%2520prior%2520research%2520in%2520single-modal%2520interpretability%252C%2520we%2520develop%250Aa%2520set%2520of%2520multi-modal%2520interpretability%2520tools%2520and%2520measures%2520designed%2520to%250Adisentangle%2520and%2520analyze%2520features%2520learned%2520from%2520CLIP.%2520Specifically%252C%2520we%2520introduce%250Athe%2520Modality%2520Dominance%2520Score%2520%2528MDS%2529%2520to%2520attribute%2520each%2520CLIP%2520feature%2520to%2520a%2520specific%250Amodality.%2520We%2520then%2520map%2520CLIP%2520features%2520into%2520a%2520more%2520interpretable%2520space%252C%2520enabling%250Aus%2520to%2520categorize%2520them%2520into%2520three%2520distinct%2520classes%253A%2520vision%2520features%250A%2528single-modal%2529%252C%2520language%2520features%2520%2528single-modal%2529%252C%2520and%2520visual-language%2520features%250A%2528cross-modal%2529.%2520Interestingly%252C%2520this%2520data-driven%2520categorization%2520closely%2520aligns%250Awith%2520human%2520intuitive%2520understandings%2520of%2520different%2520modalities.%2520We%2520further%2520show%250Athat%2520this%2520modality%2520decomposition%2520can%2520benefit%2520multiple%2520downstream%2520tasks%252C%250Aincluding%2520reducing%2520bias%2520in%2520gender%2520detection%252C%2520generating%2520cross-modal%2520adversarial%250Aexamples%252C%2520and%2520enabling%2520modal-specific%2520feature%2520control%2520in%2520text-to-image%250Ageneration.%2520These%2520results%2520indicate%2520that%2520large-scale%2520multimodal%2520models%252C%2520when%250Aequipped%2520with%2520task-agnostic%2520interpretability%2520tools%252C%2520can%2520offer%2520valuable%2520insights%250Ainto%2520the%2520relationships%2520between%2520different%2520data%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14888v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Faceted%20Multimodal%20Monosemanticity&entry.906535625=Hanqi%20Yan%20and%20Xiangxiang%20Cui%20and%20Lu%20Yin%20and%20Paul%20Pu%20Liang%20and%20Yulan%20He%20and%20Yifei%20Wang&entry.1292438233=%20%20Humans%20experience%20the%20world%20through%20multiple%20modalities%2C%20such%20as%2C%20vision%2C%0Alanguage%2C%20and%20speech%2C%20making%20it%20natural%20to%20explore%20the%20commonality%20and%0Adistinctions%20among%20them.%20In%20this%20work%2C%20we%20take%20a%20data-driven%20approach%20to%0Aaddress%20this%20question%20by%20analyzing%20interpretable%2C%20monosemantic%20features%0Aextracted%20from%20deep%20multimodal%20models.%20Specifically%2C%20we%20investigate%20CLIP%2C%20a%0Aprominent%20visual-language%20representation%20model%20trained%20on%20massive%20image-text%0Apairs.%20Building%20on%20prior%20research%20in%20single-modal%20interpretability%2C%20we%20develop%0Aa%20set%20of%20multi-modal%20interpretability%20tools%20and%20measures%20designed%20to%0Adisentangle%20and%20analyze%20features%20learned%20from%20CLIP.%20Specifically%2C%20we%20introduce%0Athe%20Modality%20Dominance%20Score%20%28MDS%29%20to%20attribute%20each%20CLIP%20feature%20to%20a%20specific%0Amodality.%20We%20then%20map%20CLIP%20features%20into%20a%20more%20interpretable%20space%2C%20enabling%0Aus%20to%20categorize%20them%20into%20three%20distinct%20classes%3A%20vision%20features%0A%28single-modal%29%2C%20language%20features%20%28single-modal%29%2C%20and%20visual-language%20features%0A%28cross-modal%29.%20Interestingly%2C%20this%20data-driven%20categorization%20closely%20aligns%0Awith%20human%20intuitive%20understandings%20of%20different%20modalities.%20We%20further%20show%0Athat%20this%20modality%20decomposition%20can%20benefit%20multiple%20downstream%20tasks%2C%0Aincluding%20reducing%20bias%20in%20gender%20detection%2C%20generating%20cross-modal%20adversarial%0Aexamples%2C%20and%20enabling%20modal-specific%20feature%20control%20in%20text-to-image%0Ageneration.%20These%20results%20indicate%20that%20large-scale%20multimodal%20models%2C%20when%0Aequipped%20with%20task-agnostic%20interpretability%20tools%2C%20can%20offer%20valuable%20insights%0Ainto%20the%20relationships%20between%20different%20data%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14888v3&entry.124074799=Read"},
{"title": "SpikeGen: Generative Framework for Visual Spike Stream Processing", "author": "Gaole Dai and Menghang Dong and Rongyu Zhang and Ruichuan An and Shanghang Zhang and Tiejun Huang", "abstract": "  Neuromorphic Visual Systems, such as spike cameras, have attracted\nconsiderable attention due to their ability to capture clear textures under\ndynamic conditions. This capability effectively mitigates issues related to\nmotion and aperture blur. However, in contrast to conventional RGB modalities\nthat provide dense spatial information, these systems generate binary,\nspatially sparse frames as a trade-off for temporally rich visual streams. In\nthis context, generative models emerge as a promising solution to address the\ninherent limitations of sparse data. These models not only facilitate the\nconditional fusion of existing information from both spike and RGB modalities\nbut also enable the conditional generation based on latent priors. In this\nstudy, we introduce a robust generative processing framework named SpikeGen,\ndesigned for visual spike streams captured by spike cameras. We evaluate this\nframework across multiple tasks involving mixed spike-RGB modalities, including\nconditional image/video deblurring, dense frame reconstruction from spike\nstreams, and high-speed scene novel-view synthesis. Supported by comprehensive\nexperimental results, we demonstrate that leveraging the latent space operation\nabilities of generative models allows us to effectively address the sparsity of\nspatial information while fully exploiting the temporal richness of spike\nstreams, thereby promoting a synergistic enhancement of different visual\nmodalities.\n", "link": "http://arxiv.org/abs/2505.18049v1", "date": "2025-05-23", "relevancy": 3.0645, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6187}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6156}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpikeGen%3A%20Generative%20Framework%20for%20Visual%20Spike%20Stream%20Processing&body=Title%3A%20SpikeGen%3A%20Generative%20Framework%20for%20Visual%20Spike%20Stream%20Processing%0AAuthor%3A%20Gaole%20Dai%20and%20Menghang%20Dong%20and%20Rongyu%20Zhang%20and%20Ruichuan%20An%20and%20Shanghang%20Zhang%20and%20Tiejun%20Huang%0AAbstract%3A%20%20%20Neuromorphic%20Visual%20Systems%2C%20such%20as%20spike%20cameras%2C%20have%20attracted%0Aconsiderable%20attention%20due%20to%20their%20ability%20to%20capture%20clear%20textures%20under%0Adynamic%20conditions.%20This%20capability%20effectively%20mitigates%20issues%20related%20to%0Amotion%20and%20aperture%20blur.%20However%2C%20in%20contrast%20to%20conventional%20RGB%20modalities%0Athat%20provide%20dense%20spatial%20information%2C%20these%20systems%20generate%20binary%2C%0Aspatially%20sparse%20frames%20as%20a%20trade-off%20for%20temporally%20rich%20visual%20streams.%20In%0Athis%20context%2C%20generative%20models%20emerge%20as%20a%20promising%20solution%20to%20address%20the%0Ainherent%20limitations%20of%20sparse%20data.%20These%20models%20not%20only%20facilitate%20the%0Aconditional%20fusion%20of%20existing%20information%20from%20both%20spike%20and%20RGB%20modalities%0Abut%20also%20enable%20the%20conditional%20generation%20based%20on%20latent%20priors.%20In%20this%0Astudy%2C%20we%20introduce%20a%20robust%20generative%20processing%20framework%20named%20SpikeGen%2C%0Adesigned%20for%20visual%20spike%20streams%20captured%20by%20spike%20cameras.%20We%20evaluate%20this%0Aframework%20across%20multiple%20tasks%20involving%20mixed%20spike-RGB%20modalities%2C%20including%0Aconditional%20image/video%20deblurring%2C%20dense%20frame%20reconstruction%20from%20spike%0Astreams%2C%20and%20high-speed%20scene%20novel-view%20synthesis.%20Supported%20by%20comprehensive%0Aexperimental%20results%2C%20we%20demonstrate%20that%20leveraging%20the%20latent%20space%20operation%0Aabilities%20of%20generative%20models%20allows%20us%20to%20effectively%20address%20the%20sparsity%20of%0Aspatial%20information%20while%20fully%20exploiting%20the%20temporal%20richness%20of%20spike%0Astreams%2C%20thereby%20promoting%20a%20synergistic%20enhancement%20of%20different%20visual%0Amodalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpikeGen%253A%2520Generative%2520Framework%2520for%2520Visual%2520Spike%2520Stream%2520Processing%26entry.906535625%3DGaole%2520Dai%2520and%2520Menghang%2520Dong%2520and%2520Rongyu%2520Zhang%2520and%2520Ruichuan%2520An%2520and%2520Shanghang%2520Zhang%2520and%2520Tiejun%2520Huang%26entry.1292438233%3D%2520%2520Neuromorphic%2520Visual%2520Systems%252C%2520such%2520as%2520spike%2520cameras%252C%2520have%2520attracted%250Aconsiderable%2520attention%2520due%2520to%2520their%2520ability%2520to%2520capture%2520clear%2520textures%2520under%250Adynamic%2520conditions.%2520This%2520capability%2520effectively%2520mitigates%2520issues%2520related%2520to%250Amotion%2520and%2520aperture%2520blur.%2520However%252C%2520in%2520contrast%2520to%2520conventional%2520RGB%2520modalities%250Athat%2520provide%2520dense%2520spatial%2520information%252C%2520these%2520systems%2520generate%2520binary%252C%250Aspatially%2520sparse%2520frames%2520as%2520a%2520trade-off%2520for%2520temporally%2520rich%2520visual%2520streams.%2520In%250Athis%2520context%252C%2520generative%2520models%2520emerge%2520as%2520a%2520promising%2520solution%2520to%2520address%2520the%250Ainherent%2520limitations%2520of%2520sparse%2520data.%2520These%2520models%2520not%2520only%2520facilitate%2520the%250Aconditional%2520fusion%2520of%2520existing%2520information%2520from%2520both%2520spike%2520and%2520RGB%2520modalities%250Abut%2520also%2520enable%2520the%2520conditional%2520generation%2520based%2520on%2520latent%2520priors.%2520In%2520this%250Astudy%252C%2520we%2520introduce%2520a%2520robust%2520generative%2520processing%2520framework%2520named%2520SpikeGen%252C%250Adesigned%2520for%2520visual%2520spike%2520streams%2520captured%2520by%2520spike%2520cameras.%2520We%2520evaluate%2520this%250Aframework%2520across%2520multiple%2520tasks%2520involving%2520mixed%2520spike-RGB%2520modalities%252C%2520including%250Aconditional%2520image/video%2520deblurring%252C%2520dense%2520frame%2520reconstruction%2520from%2520spike%250Astreams%252C%2520and%2520high-speed%2520scene%2520novel-view%2520synthesis.%2520Supported%2520by%2520comprehensive%250Aexperimental%2520results%252C%2520we%2520demonstrate%2520that%2520leveraging%2520the%2520latent%2520space%2520operation%250Aabilities%2520of%2520generative%2520models%2520allows%2520us%2520to%2520effectively%2520address%2520the%2520sparsity%2520of%250Aspatial%2520information%2520while%2520fully%2520exploiting%2520the%2520temporal%2520richness%2520of%2520spike%250Astreams%252C%2520thereby%2520promoting%2520a%2520synergistic%2520enhancement%2520of%2520different%2520visual%250Amodalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeGen%3A%20Generative%20Framework%20for%20Visual%20Spike%20Stream%20Processing&entry.906535625=Gaole%20Dai%20and%20Menghang%20Dong%20and%20Rongyu%20Zhang%20and%20Ruichuan%20An%20and%20Shanghang%20Zhang%20and%20Tiejun%20Huang&entry.1292438233=%20%20Neuromorphic%20Visual%20Systems%2C%20such%20as%20spike%20cameras%2C%20have%20attracted%0Aconsiderable%20attention%20due%20to%20their%20ability%20to%20capture%20clear%20textures%20under%0Adynamic%20conditions.%20This%20capability%20effectively%20mitigates%20issues%20related%20to%0Amotion%20and%20aperture%20blur.%20However%2C%20in%20contrast%20to%20conventional%20RGB%20modalities%0Athat%20provide%20dense%20spatial%20information%2C%20these%20systems%20generate%20binary%2C%0Aspatially%20sparse%20frames%20as%20a%20trade-off%20for%20temporally%20rich%20visual%20streams.%20In%0Athis%20context%2C%20generative%20models%20emerge%20as%20a%20promising%20solution%20to%20address%20the%0Ainherent%20limitations%20of%20sparse%20data.%20These%20models%20not%20only%20facilitate%20the%0Aconditional%20fusion%20of%20existing%20information%20from%20both%20spike%20and%20RGB%20modalities%0Abut%20also%20enable%20the%20conditional%20generation%20based%20on%20latent%20priors.%20In%20this%0Astudy%2C%20we%20introduce%20a%20robust%20generative%20processing%20framework%20named%20SpikeGen%2C%0Adesigned%20for%20visual%20spike%20streams%20captured%20by%20spike%20cameras.%20We%20evaluate%20this%0Aframework%20across%20multiple%20tasks%20involving%20mixed%20spike-RGB%20modalities%2C%20including%0Aconditional%20image/video%20deblurring%2C%20dense%20frame%20reconstruction%20from%20spike%0Astreams%2C%20and%20high-speed%20scene%20novel-view%20synthesis.%20Supported%20by%20comprehensive%0Aexperimental%20results%2C%20we%20demonstrate%20that%20leveraging%20the%20latent%20space%20operation%0Aabilities%20of%20generative%20models%20allows%20us%20to%20effectively%20address%20the%20sparsity%20of%0Aspatial%20information%20while%20fully%20exploiting%20the%20temporal%20richness%20of%20spike%0Astreams%2C%20thereby%20promoting%20a%20synergistic%20enhancement%20of%20different%20visual%0Amodalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18049v1&entry.124074799=Read"},
{"title": "Canonical Pose Reconstruction from Single Depth Image for 3D Non-rigid\n  Pose Recovery on Limited Datasets", "author": "Fahd Alhamazani and Yu-Kun Lai and Paul L. Rosin", "abstract": "  3D reconstruction from 2D inputs, especially for non-rigid objects like\nhumans, presents unique challenges due to the significant range of possible\ndeformations. Traditional methods often struggle with non-rigid shapes, which\nrequire extensive training data to cover the entire deformation space. This\nstudy addresses these limitations by proposing a canonical pose reconstruction\nmodel that transforms single-view depth images of deformable shapes into a\ncanonical form. This alignment facilitates shape reconstruction by enabling the\napplication of rigid object reconstruction techniques, and supports recovering\nthe input pose in voxel representation as part of the reconstruction task,\nutilizing both the original and deformed depth images. Notably, our model\nachieves effective results with only a small dataset of approximately 300\nsamples. Experimental results on animal and human datasets demonstrate that our\nmodel outperforms other state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2505.17992v1", "date": "2025-05-23", "relevancy": 3.0248, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6235}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6109}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Canonical%20Pose%20Reconstruction%20from%20Single%20Depth%20Image%20for%203D%20Non-rigid%0A%20%20Pose%20Recovery%20on%20Limited%20Datasets&body=Title%3A%20Canonical%20Pose%20Reconstruction%20from%20Single%20Depth%20Image%20for%203D%20Non-rigid%0A%20%20Pose%20Recovery%20on%20Limited%20Datasets%0AAuthor%3A%20Fahd%20Alhamazani%20and%20Yu-Kun%20Lai%20and%20Paul%20L.%20Rosin%0AAbstract%3A%20%20%203D%20reconstruction%20from%202D%20inputs%2C%20especially%20for%20non-rigid%20objects%20like%0Ahumans%2C%20presents%20unique%20challenges%20due%20to%20the%20significant%20range%20of%20possible%0Adeformations.%20Traditional%20methods%20often%20struggle%20with%20non-rigid%20shapes%2C%20which%0Arequire%20extensive%20training%20data%20to%20cover%20the%20entire%20deformation%20space.%20This%0Astudy%20addresses%20these%20limitations%20by%20proposing%20a%20canonical%20pose%20reconstruction%0Amodel%20that%20transforms%20single-view%20depth%20images%20of%20deformable%20shapes%20into%20a%0Acanonical%20form.%20This%20alignment%20facilitates%20shape%20reconstruction%20by%20enabling%20the%0Aapplication%20of%20rigid%20object%20reconstruction%20techniques%2C%20and%20supports%20recovering%0Athe%20input%20pose%20in%20voxel%20representation%20as%20part%20of%20the%20reconstruction%20task%2C%0Autilizing%20both%20the%20original%20and%20deformed%20depth%20images.%20Notably%2C%20our%20model%0Aachieves%20effective%20results%20with%20only%20a%20small%20dataset%20of%20approximately%20300%0Asamples.%20Experimental%20results%20on%20animal%20and%20human%20datasets%20demonstrate%20that%20our%0Amodel%20outperforms%20other%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCanonical%2520Pose%2520Reconstruction%2520from%2520Single%2520Depth%2520Image%2520for%25203D%2520Non-rigid%250A%2520%2520Pose%2520Recovery%2520on%2520Limited%2520Datasets%26entry.906535625%3DFahd%2520Alhamazani%2520and%2520Yu-Kun%2520Lai%2520and%2520Paul%2520L.%2520Rosin%26entry.1292438233%3D%2520%25203D%2520reconstruction%2520from%25202D%2520inputs%252C%2520especially%2520for%2520non-rigid%2520objects%2520like%250Ahumans%252C%2520presents%2520unique%2520challenges%2520due%2520to%2520the%2520significant%2520range%2520of%2520possible%250Adeformations.%2520Traditional%2520methods%2520often%2520struggle%2520with%2520non-rigid%2520shapes%252C%2520which%250Arequire%2520extensive%2520training%2520data%2520to%2520cover%2520the%2520entire%2520deformation%2520space.%2520This%250Astudy%2520addresses%2520these%2520limitations%2520by%2520proposing%2520a%2520canonical%2520pose%2520reconstruction%250Amodel%2520that%2520transforms%2520single-view%2520depth%2520images%2520of%2520deformable%2520shapes%2520into%2520a%250Acanonical%2520form.%2520This%2520alignment%2520facilitates%2520shape%2520reconstruction%2520by%2520enabling%2520the%250Aapplication%2520of%2520rigid%2520object%2520reconstruction%2520techniques%252C%2520and%2520supports%2520recovering%250Athe%2520input%2520pose%2520in%2520voxel%2520representation%2520as%2520part%2520of%2520the%2520reconstruction%2520task%252C%250Autilizing%2520both%2520the%2520original%2520and%2520deformed%2520depth%2520images.%2520Notably%252C%2520our%2520model%250Aachieves%2520effective%2520results%2520with%2520only%2520a%2520small%2520dataset%2520of%2520approximately%2520300%250Asamples.%2520Experimental%2520results%2520on%2520animal%2520and%2520human%2520datasets%2520demonstrate%2520that%2520our%250Amodel%2520outperforms%2520other%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Canonical%20Pose%20Reconstruction%20from%20Single%20Depth%20Image%20for%203D%20Non-rigid%0A%20%20Pose%20Recovery%20on%20Limited%20Datasets&entry.906535625=Fahd%20Alhamazani%20and%20Yu-Kun%20Lai%20and%20Paul%20L.%20Rosin&entry.1292438233=%20%203D%20reconstruction%20from%202D%20inputs%2C%20especially%20for%20non-rigid%20objects%20like%0Ahumans%2C%20presents%20unique%20challenges%20due%20to%20the%20significant%20range%20of%20possible%0Adeformations.%20Traditional%20methods%20often%20struggle%20with%20non-rigid%20shapes%2C%20which%0Arequire%20extensive%20training%20data%20to%20cover%20the%20entire%20deformation%20space.%20This%0Astudy%20addresses%20these%20limitations%20by%20proposing%20a%20canonical%20pose%20reconstruction%0Amodel%20that%20transforms%20single-view%20depth%20images%20of%20deformable%20shapes%20into%20a%0Acanonical%20form.%20This%20alignment%20facilitates%20shape%20reconstruction%20by%20enabling%20the%0Aapplication%20of%20rigid%20object%20reconstruction%20techniques%2C%20and%20supports%20recovering%0Athe%20input%20pose%20in%20voxel%20representation%20as%20part%20of%20the%20reconstruction%20task%2C%0Autilizing%20both%20the%20original%20and%20deformed%20depth%20images.%20Notably%2C%20our%20model%0Aachieves%20effective%20results%20with%20only%20a%20small%20dataset%20of%20approximately%20300%0Asamples.%20Experimental%20results%20on%20animal%20and%20human%20datasets%20demonstrate%20that%20our%0Amodel%20outperforms%20other%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17992v1&entry.124074799=Read"},
{"title": "Exploring Implicit Visual Misunderstandings in Multimodal Large Language\n  Models through Attention Analysis", "author": "Pengfei Wang and Guohai Xu and Weinong Wang and Junjie Yang and Jie Lou and Yunhua Xue", "abstract": "  Recent advancements have enhanced the capability of Multimodal Large Language\nModels (MLLMs) to comprehend multi-image information. However, existing\nbenchmarks primarily evaluate answer correctness, overlooking whether models\ngenuinely comprehend the visual input. To address this, we define implicit\nvisual misunderstanding (IVM), where MLLMs provide correct answers without\nfully comprehending the visual input. Through our analysis, we decouple the\nvisual and textual modalities within the causal attention module, revealing\nthat attention distribution increasingly converges on the image associated with\nthe correct answer as the network layers deepen. This insight leads to the\nintroduction of a scale-agnostic metric, \\textit{attention accuracy}, and a\nnovel benchmark for quantifying IVMs. Attention accuracy directly evaluates the\nmodel's visual understanding via internal mechanisms, remaining robust to\npositional biases for more reliable assessments. Furthermore, we extend our\napproach to finer granularities and demonstrate its effectiveness in unimodal\nscenarios, underscoring its versatility and generalizability.\n", "link": "http://arxiv.org/abs/2505.10541v2", "date": "2025-05-23", "relevancy": 2.9868, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6031}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Implicit%20Visual%20Misunderstandings%20in%20Multimodal%20Large%20Language%0A%20%20Models%20through%20Attention%20Analysis&body=Title%3A%20Exploring%20Implicit%20Visual%20Misunderstandings%20in%20Multimodal%20Large%20Language%0A%20%20Models%20through%20Attention%20Analysis%0AAuthor%3A%20Pengfei%20Wang%20and%20Guohai%20Xu%20and%20Weinong%20Wang%20and%20Junjie%20Yang%20and%20Jie%20Lou%20and%20Yunhua%20Xue%0AAbstract%3A%20%20%20Recent%20advancements%20have%20enhanced%20the%20capability%20of%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20to%20comprehend%20multi-image%20information.%20However%2C%20existing%0Abenchmarks%20primarily%20evaluate%20answer%20correctness%2C%20overlooking%20whether%20models%0Agenuinely%20comprehend%20the%20visual%20input.%20To%20address%20this%2C%20we%20define%20implicit%0Avisual%20misunderstanding%20%28IVM%29%2C%20where%20MLLMs%20provide%20correct%20answers%20without%0Afully%20comprehending%20the%20visual%20input.%20Through%20our%20analysis%2C%20we%20decouple%20the%0Avisual%20and%20textual%20modalities%20within%20the%20causal%20attention%20module%2C%20revealing%0Athat%20attention%20distribution%20increasingly%20converges%20on%20the%20image%20associated%20with%0Athe%20correct%20answer%20as%20the%20network%20layers%20deepen.%20This%20insight%20leads%20to%20the%0Aintroduction%20of%20a%20scale-agnostic%20metric%2C%20%5Ctextit%7Battention%20accuracy%7D%2C%20and%20a%0Anovel%20benchmark%20for%20quantifying%20IVMs.%20Attention%20accuracy%20directly%20evaluates%20the%0Amodel%27s%20visual%20understanding%20via%20internal%20mechanisms%2C%20remaining%20robust%20to%0Apositional%20biases%20for%20more%20reliable%20assessments.%20Furthermore%2C%20we%20extend%20our%0Aapproach%20to%20finer%20granularities%20and%20demonstrate%20its%20effectiveness%20in%20unimodal%0Ascenarios%2C%20underscoring%20its%20versatility%20and%20generalizability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10541v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Implicit%2520Visual%2520Misunderstandings%2520in%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%2520through%2520Attention%2520Analysis%26entry.906535625%3DPengfei%2520Wang%2520and%2520Guohai%2520Xu%2520and%2520Weinong%2520Wang%2520and%2520Junjie%2520Yang%2520and%2520Jie%2520Lou%2520and%2520Yunhua%2520Xue%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520have%2520enhanced%2520the%2520capability%2520of%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520to%2520comprehend%2520multi-image%2520information.%2520However%252C%2520existing%250Abenchmarks%2520primarily%2520evaluate%2520answer%2520correctness%252C%2520overlooking%2520whether%2520models%250Agenuinely%2520comprehend%2520the%2520visual%2520input.%2520To%2520address%2520this%252C%2520we%2520define%2520implicit%250Avisual%2520misunderstanding%2520%2528IVM%2529%252C%2520where%2520MLLMs%2520provide%2520correct%2520answers%2520without%250Afully%2520comprehending%2520the%2520visual%2520input.%2520Through%2520our%2520analysis%252C%2520we%2520decouple%2520the%250Avisual%2520and%2520textual%2520modalities%2520within%2520the%2520causal%2520attention%2520module%252C%2520revealing%250Athat%2520attention%2520distribution%2520increasingly%2520converges%2520on%2520the%2520image%2520associated%2520with%250Athe%2520correct%2520answer%2520as%2520the%2520network%2520layers%2520deepen.%2520This%2520insight%2520leads%2520to%2520the%250Aintroduction%2520of%2520a%2520scale-agnostic%2520metric%252C%2520%255Ctextit%257Battention%2520accuracy%257D%252C%2520and%2520a%250Anovel%2520benchmark%2520for%2520quantifying%2520IVMs.%2520Attention%2520accuracy%2520directly%2520evaluates%2520the%250Amodel%2527s%2520visual%2520understanding%2520via%2520internal%2520mechanisms%252C%2520remaining%2520robust%2520to%250Apositional%2520biases%2520for%2520more%2520reliable%2520assessments.%2520Furthermore%252C%2520we%2520extend%2520our%250Aapproach%2520to%2520finer%2520granularities%2520and%2520demonstrate%2520its%2520effectiveness%2520in%2520unimodal%250Ascenarios%252C%2520underscoring%2520its%2520versatility%2520and%2520generalizability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10541v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Implicit%20Visual%20Misunderstandings%20in%20Multimodal%20Large%20Language%0A%20%20Models%20through%20Attention%20Analysis&entry.906535625=Pengfei%20Wang%20and%20Guohai%20Xu%20and%20Weinong%20Wang%20and%20Junjie%20Yang%20and%20Jie%20Lou%20and%20Yunhua%20Xue&entry.1292438233=%20%20Recent%20advancements%20have%20enhanced%20the%20capability%20of%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20to%20comprehend%20multi-image%20information.%20However%2C%20existing%0Abenchmarks%20primarily%20evaluate%20answer%20correctness%2C%20overlooking%20whether%20models%0Agenuinely%20comprehend%20the%20visual%20input.%20To%20address%20this%2C%20we%20define%20implicit%0Avisual%20misunderstanding%20%28IVM%29%2C%20where%20MLLMs%20provide%20correct%20answers%20without%0Afully%20comprehending%20the%20visual%20input.%20Through%20our%20analysis%2C%20we%20decouple%20the%0Avisual%20and%20textual%20modalities%20within%20the%20causal%20attention%20module%2C%20revealing%0Athat%20attention%20distribution%20increasingly%20converges%20on%20the%20image%20associated%20with%0Athe%20correct%20answer%20as%20the%20network%20layers%20deepen.%20This%20insight%20leads%20to%20the%0Aintroduction%20of%20a%20scale-agnostic%20metric%2C%20%5Ctextit%7Battention%20accuracy%7D%2C%20and%20a%0Anovel%20benchmark%20for%20quantifying%20IVMs.%20Attention%20accuracy%20directly%20evaluates%20the%0Amodel%27s%20visual%20understanding%20via%20internal%20mechanisms%2C%20remaining%20robust%20to%0Apositional%20biases%20for%20more%20reliable%20assessments.%20Furthermore%2C%20we%20extend%20our%0Aapproach%20to%20finer%20granularities%20and%20demonstrate%20its%20effectiveness%20in%20unimodal%0Ascenarios%2C%20underscoring%20its%20versatility%20and%20generalizability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10541v2&entry.124074799=Read"},
{"title": "Autoregressive Sequence Modeling for 3D Medical Image Representation", "author": "Siwen Wang and Churan Wang and Fei Gao and Lixian Su and Fandong Zhang and Yizhou Wang and Yizhou Yu", "abstract": "  Three-dimensional (3D) medical images, such as Computed Tomography (CT) and\nMagnetic Resonance Imaging (MRI), are essential for clinical applications.\nHowever, the need for diverse and comprehensive representations is particularly\npronounced when considering the variability across different organs, diagnostic\ntasks, and imaging modalities. How to effectively interpret the intricate\ncontextual information and extract meaningful insights from these images\nremains an open challenge to the community. While current self-supervised\nlearning methods have shown potential, they often consider an image as a whole\nthereby overlooking the extensive, complex relationships among local regions\nfrom one or multiple images. In this work, we introduce a pioneering method for\nlearning 3D medical image representations through an autoregressive\npre-training framework. Our approach sequences various 3D medical images based\non spatial, contrast, and semantic correlations, treating them as\ninterconnected visual tokens within a token sequence. By employing an\nautoregressive sequence modeling task, we predict the next visual token in the\nsequence, which allows our model to deeply understand and integrate the\ncontextual information inherent in 3D medical images. Additionally, we\nimplement a random startup strategy to avoid overestimating token relationships\nand to enhance the robustness of learning. The effectiveness of our approach is\ndemonstrated by the superior performance over others on nine downstream tasks\nin public datasets.\n", "link": "http://arxiv.org/abs/2409.08691v2", "date": "2025-05-23", "relevancy": 2.9818, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6017}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5937}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoregressive%20Sequence%20Modeling%20for%203D%20Medical%20Image%20Representation&body=Title%3A%20Autoregressive%20Sequence%20Modeling%20for%203D%20Medical%20Image%20Representation%0AAuthor%3A%20Siwen%20Wang%20and%20Churan%20Wang%20and%20Fei%20Gao%20and%20Lixian%20Su%20and%20Fandong%20Zhang%20and%20Yizhou%20Wang%20and%20Yizhou%20Yu%0AAbstract%3A%20%20%20Three-dimensional%20%283D%29%20medical%20images%2C%20such%20as%20Computed%20Tomography%20%28CT%29%20and%0AMagnetic%20Resonance%20Imaging%20%28MRI%29%2C%20are%20essential%20for%20clinical%20applications.%0AHowever%2C%20the%20need%20for%20diverse%20and%20comprehensive%20representations%20is%20particularly%0Apronounced%20when%20considering%20the%20variability%20across%20different%20organs%2C%20diagnostic%0Atasks%2C%20and%20imaging%20modalities.%20How%20to%20effectively%20interpret%20the%20intricate%0Acontextual%20information%20and%20extract%20meaningful%20insights%20from%20these%20images%0Aremains%20an%20open%20challenge%20to%20the%20community.%20While%20current%20self-supervised%0Alearning%20methods%20have%20shown%20potential%2C%20they%20often%20consider%20an%20image%20as%20a%20whole%0Athereby%20overlooking%20the%20extensive%2C%20complex%20relationships%20among%20local%20regions%0Afrom%20one%20or%20multiple%20images.%20In%20this%20work%2C%20we%20introduce%20a%20pioneering%20method%20for%0Alearning%203D%20medical%20image%20representations%20through%20an%20autoregressive%0Apre-training%20framework.%20Our%20approach%20sequences%20various%203D%20medical%20images%20based%0Aon%20spatial%2C%20contrast%2C%20and%20semantic%20correlations%2C%20treating%20them%20as%0Ainterconnected%20visual%20tokens%20within%20a%20token%20sequence.%20By%20employing%20an%0Aautoregressive%20sequence%20modeling%20task%2C%20we%20predict%20the%20next%20visual%20token%20in%20the%0Asequence%2C%20which%20allows%20our%20model%20to%20deeply%20understand%20and%20integrate%20the%0Acontextual%20information%20inherent%20in%203D%20medical%20images.%20Additionally%2C%20we%0Aimplement%20a%20random%20startup%20strategy%20to%20avoid%20overestimating%20token%20relationships%0Aand%20to%20enhance%20the%20robustness%20of%20learning.%20The%20effectiveness%20of%20our%20approach%20is%0Ademonstrated%20by%20the%20superior%20performance%20over%20others%20on%20nine%20downstream%20tasks%0Ain%20public%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08691v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoregressive%2520Sequence%2520Modeling%2520for%25203D%2520Medical%2520Image%2520Representation%26entry.906535625%3DSiwen%2520Wang%2520and%2520Churan%2520Wang%2520and%2520Fei%2520Gao%2520and%2520Lixian%2520Su%2520and%2520Fandong%2520Zhang%2520and%2520Yizhou%2520Wang%2520and%2520Yizhou%2520Yu%26entry.1292438233%3D%2520%2520Three-dimensional%2520%25283D%2529%2520medical%2520images%252C%2520such%2520as%2520Computed%2520Tomography%2520%2528CT%2529%2520and%250AMagnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%252C%2520are%2520essential%2520for%2520clinical%2520applications.%250AHowever%252C%2520the%2520need%2520for%2520diverse%2520and%2520comprehensive%2520representations%2520is%2520particularly%250Apronounced%2520when%2520considering%2520the%2520variability%2520across%2520different%2520organs%252C%2520diagnostic%250Atasks%252C%2520and%2520imaging%2520modalities.%2520How%2520to%2520effectively%2520interpret%2520the%2520intricate%250Acontextual%2520information%2520and%2520extract%2520meaningful%2520insights%2520from%2520these%2520images%250Aremains%2520an%2520open%2520challenge%2520to%2520the%2520community.%2520While%2520current%2520self-supervised%250Alearning%2520methods%2520have%2520shown%2520potential%252C%2520they%2520often%2520consider%2520an%2520image%2520as%2520a%2520whole%250Athereby%2520overlooking%2520the%2520extensive%252C%2520complex%2520relationships%2520among%2520local%2520regions%250Afrom%2520one%2520or%2520multiple%2520images.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520pioneering%2520method%2520for%250Alearning%25203D%2520medical%2520image%2520representations%2520through%2520an%2520autoregressive%250Apre-training%2520framework.%2520Our%2520approach%2520sequences%2520various%25203D%2520medical%2520images%2520based%250Aon%2520spatial%252C%2520contrast%252C%2520and%2520semantic%2520correlations%252C%2520treating%2520them%2520as%250Ainterconnected%2520visual%2520tokens%2520within%2520a%2520token%2520sequence.%2520By%2520employing%2520an%250Aautoregressive%2520sequence%2520modeling%2520task%252C%2520we%2520predict%2520the%2520next%2520visual%2520token%2520in%2520the%250Asequence%252C%2520which%2520allows%2520our%2520model%2520to%2520deeply%2520understand%2520and%2520integrate%2520the%250Acontextual%2520information%2520inherent%2520in%25203D%2520medical%2520images.%2520Additionally%252C%2520we%250Aimplement%2520a%2520random%2520startup%2520strategy%2520to%2520avoid%2520overestimating%2520token%2520relationships%250Aand%2520to%2520enhance%2520the%2520robustness%2520of%2520learning.%2520The%2520effectiveness%2520of%2520our%2520approach%2520is%250Ademonstrated%2520by%2520the%2520superior%2520performance%2520over%2520others%2520on%2520nine%2520downstream%2520tasks%250Ain%2520public%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08691v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoregressive%20Sequence%20Modeling%20for%203D%20Medical%20Image%20Representation&entry.906535625=Siwen%20Wang%20and%20Churan%20Wang%20and%20Fei%20Gao%20and%20Lixian%20Su%20and%20Fandong%20Zhang%20and%20Yizhou%20Wang%20and%20Yizhou%20Yu&entry.1292438233=%20%20Three-dimensional%20%283D%29%20medical%20images%2C%20such%20as%20Computed%20Tomography%20%28CT%29%20and%0AMagnetic%20Resonance%20Imaging%20%28MRI%29%2C%20are%20essential%20for%20clinical%20applications.%0AHowever%2C%20the%20need%20for%20diverse%20and%20comprehensive%20representations%20is%20particularly%0Apronounced%20when%20considering%20the%20variability%20across%20different%20organs%2C%20diagnostic%0Atasks%2C%20and%20imaging%20modalities.%20How%20to%20effectively%20interpret%20the%20intricate%0Acontextual%20information%20and%20extract%20meaningful%20insights%20from%20these%20images%0Aremains%20an%20open%20challenge%20to%20the%20community.%20While%20current%20self-supervised%0Alearning%20methods%20have%20shown%20potential%2C%20they%20often%20consider%20an%20image%20as%20a%20whole%0Athereby%20overlooking%20the%20extensive%2C%20complex%20relationships%20among%20local%20regions%0Afrom%20one%20or%20multiple%20images.%20In%20this%20work%2C%20we%20introduce%20a%20pioneering%20method%20for%0Alearning%203D%20medical%20image%20representations%20through%20an%20autoregressive%0Apre-training%20framework.%20Our%20approach%20sequences%20various%203D%20medical%20images%20based%0Aon%20spatial%2C%20contrast%2C%20and%20semantic%20correlations%2C%20treating%20them%20as%0Ainterconnected%20visual%20tokens%20within%20a%20token%20sequence.%20By%20employing%20an%0Aautoregressive%20sequence%20modeling%20task%2C%20we%20predict%20the%20next%20visual%20token%20in%20the%0Asequence%2C%20which%20allows%20our%20model%20to%20deeply%20understand%20and%20integrate%20the%0Acontextual%20information%20inherent%20in%203D%20medical%20images.%20Additionally%2C%20we%0Aimplement%20a%20random%20startup%20strategy%20to%20avoid%20overestimating%20token%20relationships%0Aand%20to%20enhance%20the%20robustness%20of%20learning.%20The%20effectiveness%20of%20our%20approach%20is%0Ademonstrated%20by%20the%20superior%20performance%20over%20others%20on%20nine%20downstream%20tasks%0Ain%20public%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08691v2&entry.124074799=Read"},
{"title": "Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment\n  across Modalities", "author": "Ziwei Zhou and Rui Wang and Zuxuan Wu", "abstract": "  Recent Multimodal Large Language Models (MLLMs) achieve promising performance\non visual and audio benchmarks independently. However, the ability of these\nmodels to process cross-modal information synchronously remains largely\nunexplored. In this paper, we introduce: 1) Daily-Omni, an Audio-Visual\nQuestioning and Answering benchmark comprising 684 videos of daily life\nscenarios from diverse sources, rich in both audio and visual information, and\nfeaturing 1197 multiple-choice QA pairs across 6 major tasks; 2) Daily-Omni QA\nGeneration Pipeline, which includes automatic annotation, QA generation and QA\noptimization, significantly improves efficiency for human evaluation and\nscalability of the benchmark; 3) Daily-Omni-Agent, a training-free agent\nutilizing open-source Visual Language Model (VLM), Audio Language Model (ALM)\nand Automatic Speech Recognition (ASR) model to establish a baseline for this\nbenchmark. The results show that current MLLMs still struggle significantly\nwith tasks requiring audio-visual integration, but combining VLMs and ALMs with\nsimple temporal alignment techniques can achieve substantially better\nperformance. Codes and benchmark are available at\n\\href{https://github.com/Lliar-liar/Daily-Omni}{https://github.com/Lliar-liar/Daily-Omni}.\n", "link": "http://arxiv.org/abs/2505.17862v1", "date": "2025-05-23", "relevancy": 2.9559, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6029}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6029}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Daily-Omni%3A%20Towards%20Audio-Visual%20Reasoning%20with%20Temporal%20Alignment%0A%20%20across%20Modalities&body=Title%3A%20Daily-Omni%3A%20Towards%20Audio-Visual%20Reasoning%20with%20Temporal%20Alignment%0A%20%20across%20Modalities%0AAuthor%3A%20Ziwei%20Zhou%20and%20Rui%20Wang%20and%20Zuxuan%20Wu%0AAbstract%3A%20%20%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20achieve%20promising%20performance%0Aon%20visual%20and%20audio%20benchmarks%20independently.%20However%2C%20the%20ability%20of%20these%0Amodels%20to%20process%20cross-modal%20information%20synchronously%20remains%20largely%0Aunexplored.%20In%20this%20paper%2C%20we%20introduce%3A%201%29%20Daily-Omni%2C%20an%20Audio-Visual%0AQuestioning%20and%20Answering%20benchmark%20comprising%20684%20videos%20of%20daily%20life%0Ascenarios%20from%20diverse%20sources%2C%20rich%20in%20both%20audio%20and%20visual%20information%2C%20and%0Afeaturing%201197%20multiple-choice%20QA%20pairs%20across%206%20major%20tasks%3B%202%29%20Daily-Omni%20QA%0AGeneration%20Pipeline%2C%20which%20includes%20automatic%20annotation%2C%20QA%20generation%20and%20QA%0Aoptimization%2C%20significantly%20improves%20efficiency%20for%20human%20evaluation%20and%0Ascalability%20of%20the%20benchmark%3B%203%29%20Daily-Omni-Agent%2C%20a%20training-free%20agent%0Autilizing%20open-source%20Visual%20Language%20Model%20%28VLM%29%2C%20Audio%20Language%20Model%20%28ALM%29%0Aand%20Automatic%20Speech%20Recognition%20%28ASR%29%20model%20to%20establish%20a%20baseline%20for%20this%0Abenchmark.%20The%20results%20show%20that%20current%20MLLMs%20still%20struggle%20significantly%0Awith%20tasks%20requiring%20audio-visual%20integration%2C%20but%20combining%20VLMs%20and%20ALMs%20with%0Asimple%20temporal%20alignment%20techniques%20can%20achieve%20substantially%20better%0Aperformance.%20Codes%20and%20benchmark%20are%20available%20at%0A%5Chref%7Bhttps%3A//github.com/Lliar-liar/Daily-Omni%7D%7Bhttps%3A//github.com/Lliar-liar/Daily-Omni%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDaily-Omni%253A%2520Towards%2520Audio-Visual%2520Reasoning%2520with%2520Temporal%2520Alignment%250A%2520%2520across%2520Modalities%26entry.906535625%3DZiwei%2520Zhou%2520and%2520Rui%2520Wang%2520and%2520Zuxuan%2520Wu%26entry.1292438233%3D%2520%2520Recent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520achieve%2520promising%2520performance%250Aon%2520visual%2520and%2520audio%2520benchmarks%2520independently.%2520However%252C%2520the%2520ability%2520of%2520these%250Amodels%2520to%2520process%2520cross-modal%2520information%2520synchronously%2520remains%2520largely%250Aunexplored.%2520In%2520this%2520paper%252C%2520we%2520introduce%253A%25201%2529%2520Daily-Omni%252C%2520an%2520Audio-Visual%250AQuestioning%2520and%2520Answering%2520benchmark%2520comprising%2520684%2520videos%2520of%2520daily%2520life%250Ascenarios%2520from%2520diverse%2520sources%252C%2520rich%2520in%2520both%2520audio%2520and%2520visual%2520information%252C%2520and%250Afeaturing%25201197%2520multiple-choice%2520QA%2520pairs%2520across%25206%2520major%2520tasks%253B%25202%2529%2520Daily-Omni%2520QA%250AGeneration%2520Pipeline%252C%2520which%2520includes%2520automatic%2520annotation%252C%2520QA%2520generation%2520and%2520QA%250Aoptimization%252C%2520significantly%2520improves%2520efficiency%2520for%2520human%2520evaluation%2520and%250Ascalability%2520of%2520the%2520benchmark%253B%25203%2529%2520Daily-Omni-Agent%252C%2520a%2520training-free%2520agent%250Autilizing%2520open-source%2520Visual%2520Language%2520Model%2520%2528VLM%2529%252C%2520Audio%2520Language%2520Model%2520%2528ALM%2529%250Aand%2520Automatic%2520Speech%2520Recognition%2520%2528ASR%2529%2520model%2520to%2520establish%2520a%2520baseline%2520for%2520this%250Abenchmark.%2520The%2520results%2520show%2520that%2520current%2520MLLMs%2520still%2520struggle%2520significantly%250Awith%2520tasks%2520requiring%2520audio-visual%2520integration%252C%2520but%2520combining%2520VLMs%2520and%2520ALMs%2520with%250Asimple%2520temporal%2520alignment%2520techniques%2520can%2520achieve%2520substantially%2520better%250Aperformance.%2520Codes%2520and%2520benchmark%2520are%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/Lliar-liar/Daily-Omni%257D%257Bhttps%253A//github.com/Lliar-liar/Daily-Omni%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Daily-Omni%3A%20Towards%20Audio-Visual%20Reasoning%20with%20Temporal%20Alignment%0A%20%20across%20Modalities&entry.906535625=Ziwei%20Zhou%20and%20Rui%20Wang%20and%20Zuxuan%20Wu&entry.1292438233=%20%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20achieve%20promising%20performance%0Aon%20visual%20and%20audio%20benchmarks%20independently.%20However%2C%20the%20ability%20of%20these%0Amodels%20to%20process%20cross-modal%20information%20synchronously%20remains%20largely%0Aunexplored.%20In%20this%20paper%2C%20we%20introduce%3A%201%29%20Daily-Omni%2C%20an%20Audio-Visual%0AQuestioning%20and%20Answering%20benchmark%20comprising%20684%20videos%20of%20daily%20life%0Ascenarios%20from%20diverse%20sources%2C%20rich%20in%20both%20audio%20and%20visual%20information%2C%20and%0Afeaturing%201197%20multiple-choice%20QA%20pairs%20across%206%20major%20tasks%3B%202%29%20Daily-Omni%20QA%0AGeneration%20Pipeline%2C%20which%20includes%20automatic%20annotation%2C%20QA%20generation%20and%20QA%0Aoptimization%2C%20significantly%20improves%20efficiency%20for%20human%20evaluation%20and%0Ascalability%20of%20the%20benchmark%3B%203%29%20Daily-Omni-Agent%2C%20a%20training-free%20agent%0Autilizing%20open-source%20Visual%20Language%20Model%20%28VLM%29%2C%20Audio%20Language%20Model%20%28ALM%29%0Aand%20Automatic%20Speech%20Recognition%20%28ASR%29%20model%20to%20establish%20a%20baseline%20for%20this%0Abenchmark.%20The%20results%20show%20that%20current%20MLLMs%20still%20struggle%20significantly%0Awith%20tasks%20requiring%20audio-visual%20integration%2C%20but%20combining%20VLMs%20and%20ALMs%20with%0Asimple%20temporal%20alignment%20techniques%20can%20achieve%20substantially%20better%0Aperformance.%20Codes%20and%20benchmark%20are%20available%20at%0A%5Chref%7Bhttps%3A//github.com/Lliar-liar/Daily-Omni%7D%7Bhttps%3A//github.com/Lliar-liar/Daily-Omni%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17862v1&entry.124074799=Read"},
{"title": "Is Single-View Mesh Reconstruction Ready for Robotics?", "author": "Frederik Nolte and Bernhard Sch\u00f6lkopf and Ingmar Posner", "abstract": "  This paper evaluates single-view mesh reconstruction models for creating\ndigital twin environments in robot manipulation. Recent advances in computer\nvision for 3D reconstruction from single viewpoints present a potential\nbreakthrough for efficiently creating virtual replicas of physical environments\nfor robotics contexts. However, their suitability for physics simulations and\nrobotics applications remains unexplored. We establish benchmarking criteria\nfor 3D reconstruction in robotics contexts, including handling typical inputs,\nproducing collision-free and stable reconstructions, managing occlusions, and\nmeeting computational constraints. Our empirical evaluation using realistic\nrobotics datasets shows that despite success on computer vision benchmarks,\nexisting approaches fail to meet robotics-specific requirements. We\nquantitively examine limitations of single-view reconstruction for practical\nrobotics implementation, in contrast to prior work that focuses on multi-view\napproaches. Our findings highlight critical gaps between computer vision\nadvances and robotics needs, guiding future research at this intersection.\n", "link": "http://arxiv.org/abs/2505.17966v1", "date": "2025-05-23", "relevancy": 2.9522, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5962}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5962}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Single-View%20Mesh%20Reconstruction%20Ready%20for%20Robotics%3F&body=Title%3A%20Is%20Single-View%20Mesh%20Reconstruction%20Ready%20for%20Robotics%3F%0AAuthor%3A%20Frederik%20Nolte%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Ingmar%20Posner%0AAbstract%3A%20%20%20This%20paper%20evaluates%20single-view%20mesh%20reconstruction%20models%20for%20creating%0Adigital%20twin%20environments%20in%20robot%20manipulation.%20Recent%20advances%20in%20computer%0Avision%20for%203D%20reconstruction%20from%20single%20viewpoints%20present%20a%20potential%0Abreakthrough%20for%20efficiently%20creating%20virtual%20replicas%20of%20physical%20environments%0Afor%20robotics%20contexts.%20However%2C%20their%20suitability%20for%20physics%20simulations%20and%0Arobotics%20applications%20remains%20unexplored.%20We%20establish%20benchmarking%20criteria%0Afor%203D%20reconstruction%20in%20robotics%20contexts%2C%20including%20handling%20typical%20inputs%2C%0Aproducing%20collision-free%20and%20stable%20reconstructions%2C%20managing%20occlusions%2C%20and%0Ameeting%20computational%20constraints.%20Our%20empirical%20evaluation%20using%20realistic%0Arobotics%20datasets%20shows%20that%20despite%20success%20on%20computer%20vision%20benchmarks%2C%0Aexisting%20approaches%20fail%20to%20meet%20robotics-specific%20requirements.%20We%0Aquantitively%20examine%20limitations%20of%20single-view%20reconstruction%20for%20practical%0Arobotics%20implementation%2C%20in%20contrast%20to%20prior%20work%20that%20focuses%20on%20multi-view%0Aapproaches.%20Our%20findings%20highlight%20critical%20gaps%20between%20computer%20vision%0Aadvances%20and%20robotics%20needs%2C%20guiding%20future%20research%20at%20this%20intersection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Single-View%2520Mesh%2520Reconstruction%2520Ready%2520for%2520Robotics%253F%26entry.906535625%3DFrederik%2520Nolte%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Ingmar%2520Posner%26entry.1292438233%3D%2520%2520This%2520paper%2520evaluates%2520single-view%2520mesh%2520reconstruction%2520models%2520for%2520creating%250Adigital%2520twin%2520environments%2520in%2520robot%2520manipulation.%2520Recent%2520advances%2520in%2520computer%250Avision%2520for%25203D%2520reconstruction%2520from%2520single%2520viewpoints%2520present%2520a%2520potential%250Abreakthrough%2520for%2520efficiently%2520creating%2520virtual%2520replicas%2520of%2520physical%2520environments%250Afor%2520robotics%2520contexts.%2520However%252C%2520their%2520suitability%2520for%2520physics%2520simulations%2520and%250Arobotics%2520applications%2520remains%2520unexplored.%2520We%2520establish%2520benchmarking%2520criteria%250Afor%25203D%2520reconstruction%2520in%2520robotics%2520contexts%252C%2520including%2520handling%2520typical%2520inputs%252C%250Aproducing%2520collision-free%2520and%2520stable%2520reconstructions%252C%2520managing%2520occlusions%252C%2520and%250Ameeting%2520computational%2520constraints.%2520Our%2520empirical%2520evaluation%2520using%2520realistic%250Arobotics%2520datasets%2520shows%2520that%2520despite%2520success%2520on%2520computer%2520vision%2520benchmarks%252C%250Aexisting%2520approaches%2520fail%2520to%2520meet%2520robotics-specific%2520requirements.%2520We%250Aquantitively%2520examine%2520limitations%2520of%2520single-view%2520reconstruction%2520for%2520practical%250Arobotics%2520implementation%252C%2520in%2520contrast%2520to%2520prior%2520work%2520that%2520focuses%2520on%2520multi-view%250Aapproaches.%2520Our%2520findings%2520highlight%2520critical%2520gaps%2520between%2520computer%2520vision%250Aadvances%2520and%2520robotics%2520needs%252C%2520guiding%2520future%2520research%2520at%2520this%2520intersection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Single-View%20Mesh%20Reconstruction%20Ready%20for%20Robotics%3F&entry.906535625=Frederik%20Nolte%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Ingmar%20Posner&entry.1292438233=%20%20This%20paper%20evaluates%20single-view%20mesh%20reconstruction%20models%20for%20creating%0Adigital%20twin%20environments%20in%20robot%20manipulation.%20Recent%20advances%20in%20computer%0Avision%20for%203D%20reconstruction%20from%20single%20viewpoints%20present%20a%20potential%0Abreakthrough%20for%20efficiently%20creating%20virtual%20replicas%20of%20physical%20environments%0Afor%20robotics%20contexts.%20However%2C%20their%20suitability%20for%20physics%20simulations%20and%0Arobotics%20applications%20remains%20unexplored.%20We%20establish%20benchmarking%20criteria%0Afor%203D%20reconstruction%20in%20robotics%20contexts%2C%20including%20handling%20typical%20inputs%2C%0Aproducing%20collision-free%20and%20stable%20reconstructions%2C%20managing%20occlusions%2C%20and%0Ameeting%20computational%20constraints.%20Our%20empirical%20evaluation%20using%20realistic%0Arobotics%20datasets%20shows%20that%20despite%20success%20on%20computer%20vision%20benchmarks%2C%0Aexisting%20approaches%20fail%20to%20meet%20robotics-specific%20requirements.%20We%0Aquantitively%20examine%20limitations%20of%20single-view%20reconstruction%20for%20practical%0Arobotics%20implementation%2C%20in%20contrast%20to%20prior%20work%20that%20focuses%20on%20multi-view%0Aapproaches.%20Our%20findings%20highlight%20critical%20gaps%20between%20computer%20vision%0Aadvances%20and%20robotics%20needs%2C%20guiding%20future%20research%20at%20this%20intersection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17966v1&entry.124074799=Read"},
{"title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via\n  Modality-Aware Permutation Sparse Attention", "author": "Yucheng Li and Huiqiang Jiang and Chengruidong Zhang and Qianhui Wu and Xufang Luo and Surin Ahn and Amir H. Abdi and Dongsheng Li and Jianfeng Gao and Yuqing Yang and Lili Qiu", "abstract": "  The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference.\n", "link": "http://arxiv.org/abs/2504.16083v2", "date": "2025-05-23", "relevancy": 2.9348, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6103}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5753}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMInference%3A%20Accelerating%20Pre-filling%20for%20Long-Context%20VLMs%20via%0A%20%20Modality-Aware%20Permutation%20Sparse%20Attention&body=Title%3A%20MMInference%3A%20Accelerating%20Pre-filling%20for%20Long-Context%20VLMs%20via%0A%20%20Modality-Aware%20Permutation%20Sparse%20Attention%0AAuthor%3A%20Yucheng%20Li%20and%20Huiqiang%20Jiang%20and%20Chengruidong%20Zhang%20and%20Qianhui%20Wu%20and%20Xufang%20Luo%20and%20Surin%20Ahn%20and%20Amir%20H.%20Abdi%20and%20Dongsheng%20Li%20and%20Jianfeng%20Gao%20and%20Yuqing%20Yang%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20The%20integration%20of%20long-context%20capabilities%20with%20visual%20understanding%0Aunlocks%20unprecedented%20potential%20for%20Vision%20Language%20Models%20%28VLMs%29.%20However%2C%20the%0Aquadratic%20attention%20complexity%20during%20the%20pre-filling%20phase%20remains%20a%0Asignificant%20obstacle%20to%20real-world%20deployment.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20MMInference%20%28Multimodality%20Million%20tokens%20Inference%29%2C%20a%20dynamic%0Asparse%20attention%20method%20that%20accelerates%20the%20prefilling%20stage%20for%20long-context%0Amulti-modal%20inputs.%20First%2C%20our%20analysis%20reveals%20that%20the%20temporal%20and%20spatial%0Alocality%20of%20video%20input%20leads%20to%20a%20unique%20sparse%20pattern%2C%20the%20Grid%20pattern.%0ASimultaneously%2C%20VLMs%20exhibit%20markedly%20different%20sparse%20distributions%20across%0Adifferent%20modalities.%20We%20introduce%20a%20permutation-based%20method%20to%20leverage%20the%0Aunique%20Grid%20pattern%20and%20handle%20modality%20boundary%20issues.%20By%20offline%20search%20the%0Aoptimal%20sparse%20patterns%20for%20each%20head%2C%20MMInference%20constructs%20the%20sparse%0Adistribution%20dynamically%20based%20on%20the%20input.%20We%20also%20provide%20optimized%20GPU%0Akernels%20for%20efficient%20sparse%20computations.%20Notably%2C%20MMInference%20integrates%0Aseamlessly%20into%20existing%20VLM%20pipelines%20without%20any%20model%20modifications%20or%0Afine-tuning.%20Experiments%20on%20multi-modal%20benchmarks-including%20Video%20QA%2C%0ACaptioning%2C%20VisionNIAH%2C%20and%20Mixed-Modality%20NIAH-with%20state-of-the-art%0Along-context%20VLMs%20%28LongVila%2C%20LlavaVideo%2C%20VideoChat-Flash%2C%20Qwen2.5-VL%29%20show%20that%0AMMInference%20accelerates%20the%20pre-filling%20stage%20by%20up%20to%208.3x%20at%201M%20tokens%20while%0Amaintaining%20accuracy.%20Our%20code%20is%20available%20at%20https%3A//aka.ms/MMInference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMInference%253A%2520Accelerating%2520Pre-filling%2520for%2520Long-Context%2520VLMs%2520via%250A%2520%2520Modality-Aware%2520Permutation%2520Sparse%2520Attention%26entry.906535625%3DYucheng%2520Li%2520and%2520Huiqiang%2520Jiang%2520and%2520Chengruidong%2520Zhang%2520and%2520Qianhui%2520Wu%2520and%2520Xufang%2520Luo%2520and%2520Surin%2520Ahn%2520and%2520Amir%2520H.%2520Abdi%2520and%2520Dongsheng%2520Li%2520and%2520Jianfeng%2520Gao%2520and%2520Yuqing%2520Yang%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520long-context%2520capabilities%2520with%2520visual%2520understanding%250Aunlocks%2520unprecedented%2520potential%2520for%2520Vision%2520Language%2520Models%2520%2528VLMs%2529.%2520However%252C%2520the%250Aquadratic%2520attention%2520complexity%2520during%2520the%2520pre-filling%2520phase%2520remains%2520a%250Asignificant%2520obstacle%2520to%2520real-world%2520deployment.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Aintroduce%2520MMInference%2520%2528Multimodality%2520Million%2520tokens%2520Inference%2529%252C%2520a%2520dynamic%250Asparse%2520attention%2520method%2520that%2520accelerates%2520the%2520prefilling%2520stage%2520for%2520long-context%250Amulti-modal%2520inputs.%2520First%252C%2520our%2520analysis%2520reveals%2520that%2520the%2520temporal%2520and%2520spatial%250Alocality%2520of%2520video%2520input%2520leads%2520to%2520a%2520unique%2520sparse%2520pattern%252C%2520the%2520Grid%2520pattern.%250ASimultaneously%252C%2520VLMs%2520exhibit%2520markedly%2520different%2520sparse%2520distributions%2520across%250Adifferent%2520modalities.%2520We%2520introduce%2520a%2520permutation-based%2520method%2520to%2520leverage%2520the%250Aunique%2520Grid%2520pattern%2520and%2520handle%2520modality%2520boundary%2520issues.%2520By%2520offline%2520search%2520the%250Aoptimal%2520sparse%2520patterns%2520for%2520each%2520head%252C%2520MMInference%2520constructs%2520the%2520sparse%250Adistribution%2520dynamically%2520based%2520on%2520the%2520input.%2520We%2520also%2520provide%2520optimized%2520GPU%250Akernels%2520for%2520efficient%2520sparse%2520computations.%2520Notably%252C%2520MMInference%2520integrates%250Aseamlessly%2520into%2520existing%2520VLM%2520pipelines%2520without%2520any%2520model%2520modifications%2520or%250Afine-tuning.%2520Experiments%2520on%2520multi-modal%2520benchmarks-including%2520Video%2520QA%252C%250ACaptioning%252C%2520VisionNIAH%252C%2520and%2520Mixed-Modality%2520NIAH-with%2520state-of-the-art%250Along-context%2520VLMs%2520%2528LongVila%252C%2520LlavaVideo%252C%2520VideoChat-Flash%252C%2520Qwen2.5-VL%2529%2520show%2520that%250AMMInference%2520accelerates%2520the%2520pre-filling%2520stage%2520by%2520up%2520to%25208.3x%2520at%25201M%2520tokens%2520while%250Amaintaining%2520accuracy.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//aka.ms/MMInference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMInference%3A%20Accelerating%20Pre-filling%20for%20Long-Context%20VLMs%20via%0A%20%20Modality-Aware%20Permutation%20Sparse%20Attention&entry.906535625=Yucheng%20Li%20and%20Huiqiang%20Jiang%20and%20Chengruidong%20Zhang%20and%20Qianhui%20Wu%20and%20Xufang%20Luo%20and%20Surin%20Ahn%20and%20Amir%20H.%20Abdi%20and%20Dongsheng%20Li%20and%20Jianfeng%20Gao%20and%20Yuqing%20Yang%20and%20Lili%20Qiu&entry.1292438233=%20%20The%20integration%20of%20long-context%20capabilities%20with%20visual%20understanding%0Aunlocks%20unprecedented%20potential%20for%20Vision%20Language%20Models%20%28VLMs%29.%20However%2C%20the%0Aquadratic%20attention%20complexity%20during%20the%20pre-filling%20phase%20remains%20a%0Asignificant%20obstacle%20to%20real-world%20deployment.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20MMInference%20%28Multimodality%20Million%20tokens%20Inference%29%2C%20a%20dynamic%0Asparse%20attention%20method%20that%20accelerates%20the%20prefilling%20stage%20for%20long-context%0Amulti-modal%20inputs.%20First%2C%20our%20analysis%20reveals%20that%20the%20temporal%20and%20spatial%0Alocality%20of%20video%20input%20leads%20to%20a%20unique%20sparse%20pattern%2C%20the%20Grid%20pattern.%0ASimultaneously%2C%20VLMs%20exhibit%20markedly%20different%20sparse%20distributions%20across%0Adifferent%20modalities.%20We%20introduce%20a%20permutation-based%20method%20to%20leverage%20the%0Aunique%20Grid%20pattern%20and%20handle%20modality%20boundary%20issues.%20By%20offline%20search%20the%0Aoptimal%20sparse%20patterns%20for%20each%20head%2C%20MMInference%20constructs%20the%20sparse%0Adistribution%20dynamically%20based%20on%20the%20input.%20We%20also%20provide%20optimized%20GPU%0Akernels%20for%20efficient%20sparse%20computations.%20Notably%2C%20MMInference%20integrates%0Aseamlessly%20into%20existing%20VLM%20pipelines%20without%20any%20model%20modifications%20or%0Afine-tuning.%20Experiments%20on%20multi-modal%20benchmarks-including%20Video%20QA%2C%0ACaptioning%2C%20VisionNIAH%2C%20and%20Mixed-Modality%20NIAH-with%20state-of-the-art%0Along-context%20VLMs%20%28LongVila%2C%20LlavaVideo%2C%20VideoChat-Flash%2C%20Qwen2.5-VL%29%20show%20that%0AMMInference%20accelerates%20the%20pre-filling%20stage%20by%20up%20to%208.3x%20at%201M%20tokens%20while%0Amaintaining%20accuracy.%20Our%20code%20is%20available%20at%20https%3A//aka.ms/MMInference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16083v2&entry.124074799=Read"},
{"title": "RemoteSAM: Towards Segment Anything for Earth Observation", "author": "Liang Yao and Fan Liu and Delong Chen and Chuanyi Zhang and Yijun Wang and Ziyun Chen and Wei Xu and Shimin Di and Yuhui Zheng", "abstract": "  We aim to develop a robust yet flexible visual foundation model for Earth\nobservation. It should possess strong capabilities in recognizing and\nlocalizing diverse visual targets while providing compatibility with various\ninput-output interfaces required across different task scenarios. Current\nsystems cannot meet these requirements, as they typically utilize task-specific\narchitecture trained on narrow data domains with limited semantic coverage. Our\nstudy addresses these limitations from two aspects: data and modeling. We first\nintroduce an automatic data engine that enjoys significantly better scalability\ncompared to previous human annotation or rule-based approaches. It has enabled\nus to create the largest dataset of its kind to date, comprising 270K\nimage-text-mask triplets covering an unprecedented range of diverse semantic\ncategories and attribute specifications. Based on this data foundation, we\nfurther propose a task unification paradigm that centers around referring\nexpression segmentation. It effectively handles a wide range of vision-centric\nperception tasks, including classification, detection, segmentation, grounding,\netc, using a single model without any task-specific heads. Combining these\ninnovations on data and modeling, we present RemoteSAM, a foundation model that\nestablishes new SoTA on several earth observation perception benchmarks,\noutperforming other foundation models such as Falcon, GeoChat, and LHRS-Bot\nwith significantly higher efficiency. Models and data are publicly available at\nhttps://github.com/1e12Leon/RemoteSAM.\n", "link": "http://arxiv.org/abs/2505.18022v1", "date": "2025-05-23", "relevancy": 2.9331, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5965}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5965}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RemoteSAM%3A%20Towards%20Segment%20Anything%20for%20Earth%20Observation&body=Title%3A%20RemoteSAM%3A%20Towards%20Segment%20Anything%20for%20Earth%20Observation%0AAuthor%3A%20Liang%20Yao%20and%20Fan%20Liu%20and%20Delong%20Chen%20and%20Chuanyi%20Zhang%20and%20Yijun%20Wang%20and%20Ziyun%20Chen%20and%20Wei%20Xu%20and%20Shimin%20Di%20and%20Yuhui%20Zheng%0AAbstract%3A%20%20%20We%20aim%20to%20develop%20a%20robust%20yet%20flexible%20visual%20foundation%20model%20for%20Earth%0Aobservation.%20It%20should%20possess%20strong%20capabilities%20in%20recognizing%20and%0Alocalizing%20diverse%20visual%20targets%20while%20providing%20compatibility%20with%20various%0Ainput-output%20interfaces%20required%20across%20different%20task%20scenarios.%20Current%0Asystems%20cannot%20meet%20these%20requirements%2C%20as%20they%20typically%20utilize%20task-specific%0Aarchitecture%20trained%20on%20narrow%20data%20domains%20with%20limited%20semantic%20coverage.%20Our%0Astudy%20addresses%20these%20limitations%20from%20two%20aspects%3A%20data%20and%20modeling.%20We%20first%0Aintroduce%20an%20automatic%20data%20engine%20that%20enjoys%20significantly%20better%20scalability%0Acompared%20to%20previous%20human%20annotation%20or%20rule-based%20approaches.%20It%20has%20enabled%0Aus%20to%20create%20the%20largest%20dataset%20of%20its%20kind%20to%20date%2C%20comprising%20270K%0Aimage-text-mask%20triplets%20covering%20an%20unprecedented%20range%20of%20diverse%20semantic%0Acategories%20and%20attribute%20specifications.%20Based%20on%20this%20data%20foundation%2C%20we%0Afurther%20propose%20a%20task%20unification%20paradigm%20that%20centers%20around%20referring%0Aexpression%20segmentation.%20It%20effectively%20handles%20a%20wide%20range%20of%20vision-centric%0Aperception%20tasks%2C%20including%20classification%2C%20detection%2C%20segmentation%2C%20grounding%2C%0Aetc%2C%20using%20a%20single%20model%20without%20any%20task-specific%20heads.%20Combining%20these%0Ainnovations%20on%20data%20and%20modeling%2C%20we%20present%20RemoteSAM%2C%20a%20foundation%20model%20that%0Aestablishes%20new%20SoTA%20on%20several%20earth%20observation%20perception%20benchmarks%2C%0Aoutperforming%20other%20foundation%20models%20such%20as%20Falcon%2C%20GeoChat%2C%20and%20LHRS-Bot%0Awith%20significantly%20higher%20efficiency.%20Models%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/1e12Leon/RemoteSAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRemoteSAM%253A%2520Towards%2520Segment%2520Anything%2520for%2520Earth%2520Observation%26entry.906535625%3DLiang%2520Yao%2520and%2520Fan%2520Liu%2520and%2520Delong%2520Chen%2520and%2520Chuanyi%2520Zhang%2520and%2520Yijun%2520Wang%2520and%2520Ziyun%2520Chen%2520and%2520Wei%2520Xu%2520and%2520Shimin%2520Di%2520and%2520Yuhui%2520Zheng%26entry.1292438233%3D%2520%2520We%2520aim%2520to%2520develop%2520a%2520robust%2520yet%2520flexible%2520visual%2520foundation%2520model%2520for%2520Earth%250Aobservation.%2520It%2520should%2520possess%2520strong%2520capabilities%2520in%2520recognizing%2520and%250Alocalizing%2520diverse%2520visual%2520targets%2520while%2520providing%2520compatibility%2520with%2520various%250Ainput-output%2520interfaces%2520required%2520across%2520different%2520task%2520scenarios.%2520Current%250Asystems%2520cannot%2520meet%2520these%2520requirements%252C%2520as%2520they%2520typically%2520utilize%2520task-specific%250Aarchitecture%2520trained%2520on%2520narrow%2520data%2520domains%2520with%2520limited%2520semantic%2520coverage.%2520Our%250Astudy%2520addresses%2520these%2520limitations%2520from%2520two%2520aspects%253A%2520data%2520and%2520modeling.%2520We%2520first%250Aintroduce%2520an%2520automatic%2520data%2520engine%2520that%2520enjoys%2520significantly%2520better%2520scalability%250Acompared%2520to%2520previous%2520human%2520annotation%2520or%2520rule-based%2520approaches.%2520It%2520has%2520enabled%250Aus%2520to%2520create%2520the%2520largest%2520dataset%2520of%2520its%2520kind%2520to%2520date%252C%2520comprising%2520270K%250Aimage-text-mask%2520triplets%2520covering%2520an%2520unprecedented%2520range%2520of%2520diverse%2520semantic%250Acategories%2520and%2520attribute%2520specifications.%2520Based%2520on%2520this%2520data%2520foundation%252C%2520we%250Afurther%2520propose%2520a%2520task%2520unification%2520paradigm%2520that%2520centers%2520around%2520referring%250Aexpression%2520segmentation.%2520It%2520effectively%2520handles%2520a%2520wide%2520range%2520of%2520vision-centric%250Aperception%2520tasks%252C%2520including%2520classification%252C%2520detection%252C%2520segmentation%252C%2520grounding%252C%250Aetc%252C%2520using%2520a%2520single%2520model%2520without%2520any%2520task-specific%2520heads.%2520Combining%2520these%250Ainnovations%2520on%2520data%2520and%2520modeling%252C%2520we%2520present%2520RemoteSAM%252C%2520a%2520foundation%2520model%2520that%250Aestablishes%2520new%2520SoTA%2520on%2520several%2520earth%2520observation%2520perception%2520benchmarks%252C%250Aoutperforming%2520other%2520foundation%2520models%2520such%2520as%2520Falcon%252C%2520GeoChat%252C%2520and%2520LHRS-Bot%250Awith%2520significantly%2520higher%2520efficiency.%2520Models%2520and%2520data%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/1e12Leon/RemoteSAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RemoteSAM%3A%20Towards%20Segment%20Anything%20for%20Earth%20Observation&entry.906535625=Liang%20Yao%20and%20Fan%20Liu%20and%20Delong%20Chen%20and%20Chuanyi%20Zhang%20and%20Yijun%20Wang%20and%20Ziyun%20Chen%20and%20Wei%20Xu%20and%20Shimin%20Di%20and%20Yuhui%20Zheng&entry.1292438233=%20%20We%20aim%20to%20develop%20a%20robust%20yet%20flexible%20visual%20foundation%20model%20for%20Earth%0Aobservation.%20It%20should%20possess%20strong%20capabilities%20in%20recognizing%20and%0Alocalizing%20diverse%20visual%20targets%20while%20providing%20compatibility%20with%20various%0Ainput-output%20interfaces%20required%20across%20different%20task%20scenarios.%20Current%0Asystems%20cannot%20meet%20these%20requirements%2C%20as%20they%20typically%20utilize%20task-specific%0Aarchitecture%20trained%20on%20narrow%20data%20domains%20with%20limited%20semantic%20coverage.%20Our%0Astudy%20addresses%20these%20limitations%20from%20two%20aspects%3A%20data%20and%20modeling.%20We%20first%0Aintroduce%20an%20automatic%20data%20engine%20that%20enjoys%20significantly%20better%20scalability%0Acompared%20to%20previous%20human%20annotation%20or%20rule-based%20approaches.%20It%20has%20enabled%0Aus%20to%20create%20the%20largest%20dataset%20of%20its%20kind%20to%20date%2C%20comprising%20270K%0Aimage-text-mask%20triplets%20covering%20an%20unprecedented%20range%20of%20diverse%20semantic%0Acategories%20and%20attribute%20specifications.%20Based%20on%20this%20data%20foundation%2C%20we%0Afurther%20propose%20a%20task%20unification%20paradigm%20that%20centers%20around%20referring%0Aexpression%20segmentation.%20It%20effectively%20handles%20a%20wide%20range%20of%20vision-centric%0Aperception%20tasks%2C%20including%20classification%2C%20detection%2C%20segmentation%2C%20grounding%2C%0Aetc%2C%20using%20a%20single%20model%20without%20any%20task-specific%20heads.%20Combining%20these%0Ainnovations%20on%20data%20and%20modeling%2C%20we%20present%20RemoteSAM%2C%20a%20foundation%20model%20that%0Aestablishes%20new%20SoTA%20on%20several%20earth%20observation%20perception%20benchmarks%2C%0Aoutperforming%20other%20foundation%20models%20such%20as%20Falcon%2C%20GeoChat%2C%20and%20LHRS-Bot%0Awith%20significantly%20higher%20efficiency.%20Models%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/1e12Leon/RemoteSAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18022v1&entry.124074799=Read"},
{"title": "OneProt: Towards Multi-Modal Protein Foundation Models", "author": "Klemens Fl\u00f6ge and Srisruthi Udayakumar and Johanna Sommer and Marie Piraud and Stefan Kesselheim and Vincent Fortuin and Stephan G\u00fcnneman and Karel J van der Weg and Holger Gohlke and Erinc Merdivan and Alina Bazarova", "abstract": "  Recent advances in Artificial Intelligence have enabled multi-modal systems\nto model and translate diverse information spaces. Extending beyond text and\nvision, we introduce OneProt, a multi-modal AI for proteins that integrates\nstructural, sequence, text, and binding site data. Using the ImageBind\nframework, OneProt aligns the latent spaces of protein modality encoders in a\nlightweight fine-tuning scheme that focuses on pairwise alignment with sequence\ndata rather than requiring full matches. This novel approach comprises a mix of\nGraph Neural Networks and transformer architectures. It demonstrates strong\nperformance in retrieval tasks and showcases the efficacy of multi-modal\nsystems in Protein Machine Learning through a broad spectrum of downstream\nbaselines, including enzyme function prediction and binding site analysis.\nFurthermore, OneProt enables the transfer of representational information from\nspecialized encoders to the sequence encoder, enhancing capabilities for\ndistinguishing evolutionarily related and unrelated sequences and exhibiting\nrepresentational properties where evolutionarily related proteins align in\nsimilar directions within the latent space. In addition, we extensively\ninvestigate modality ablations to identify the encoders that contribute most to\npredictive performance, highlighting the significance of the binding site\nencoder, which has not been used in similar models previously. This work\nexpands the horizons of multi-modal protein models, paving the way for\ntransformative applications in drug discovery, biocatalytic reaction planning,\nand protein engineering.\n", "link": "http://arxiv.org/abs/2411.04863v2", "date": "2025-05-23", "relevancy": 2.9246, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5875}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5875}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneProt%3A%20Towards%20Multi-Modal%20Protein%20Foundation%20Models&body=Title%3A%20OneProt%3A%20Towards%20Multi-Modal%20Protein%20Foundation%20Models%0AAuthor%3A%20Klemens%20Fl%C3%B6ge%20and%20Srisruthi%20Udayakumar%20and%20Johanna%20Sommer%20and%20Marie%20Piraud%20and%20Stefan%20Kesselheim%20and%20Vincent%20Fortuin%20and%20Stephan%20G%C3%BCnneman%20and%20Karel%20J%20van%20der%20Weg%20and%20Holger%20Gohlke%20and%20Erinc%20Merdivan%20and%20Alina%20Bazarova%0AAbstract%3A%20%20%20Recent%20advances%20in%20Artificial%20Intelligence%20have%20enabled%20multi-modal%20systems%0Ato%20model%20and%20translate%20diverse%20information%20spaces.%20Extending%20beyond%20text%20and%0Avision%2C%20we%20introduce%20OneProt%2C%20a%20multi-modal%20AI%20for%20proteins%20that%20integrates%0Astructural%2C%20sequence%2C%20text%2C%20and%20binding%20site%20data.%20Using%20the%20ImageBind%0Aframework%2C%20OneProt%20aligns%20the%20latent%20spaces%20of%20protein%20modality%20encoders%20in%20a%0Alightweight%20fine-tuning%20scheme%20that%20focuses%20on%20pairwise%20alignment%20with%20sequence%0Adata%20rather%20than%20requiring%20full%20matches.%20This%20novel%20approach%20comprises%20a%20mix%20of%0AGraph%20Neural%20Networks%20and%20transformer%20architectures.%20It%20demonstrates%20strong%0Aperformance%20in%20retrieval%20tasks%20and%20showcases%20the%20efficacy%20of%20multi-modal%0Asystems%20in%20Protein%20Machine%20Learning%20through%20a%20broad%20spectrum%20of%20downstream%0Abaselines%2C%20including%20enzyme%20function%20prediction%20and%20binding%20site%20analysis.%0AFurthermore%2C%20OneProt%20enables%20the%20transfer%20of%20representational%20information%20from%0Aspecialized%20encoders%20to%20the%20sequence%20encoder%2C%20enhancing%20capabilities%20for%0Adistinguishing%20evolutionarily%20related%20and%20unrelated%20sequences%20and%20exhibiting%0Arepresentational%20properties%20where%20evolutionarily%20related%20proteins%20align%20in%0Asimilar%20directions%20within%20the%20latent%20space.%20In%20addition%2C%20we%20extensively%0Ainvestigate%20modality%20ablations%20to%20identify%20the%20encoders%20that%20contribute%20most%20to%0Apredictive%20performance%2C%20highlighting%20the%20significance%20of%20the%20binding%20site%0Aencoder%2C%20which%20has%20not%20been%20used%20in%20similar%20models%20previously.%20This%20work%0Aexpands%20the%20horizons%20of%20multi-modal%20protein%20models%2C%20paving%20the%20way%20for%0Atransformative%20applications%20in%20drug%20discovery%2C%20biocatalytic%20reaction%20planning%2C%0Aand%20protein%20engineering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04863v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneProt%253A%2520Towards%2520Multi-Modal%2520Protein%2520Foundation%2520Models%26entry.906535625%3DKlemens%2520Fl%25C3%25B6ge%2520and%2520Srisruthi%2520Udayakumar%2520and%2520Johanna%2520Sommer%2520and%2520Marie%2520Piraud%2520and%2520Stefan%2520Kesselheim%2520and%2520Vincent%2520Fortuin%2520and%2520Stephan%2520G%25C3%25BCnneman%2520and%2520Karel%2520J%2520van%2520der%2520Weg%2520and%2520Holger%2520Gohlke%2520and%2520Erinc%2520Merdivan%2520and%2520Alina%2520Bazarova%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Artificial%2520Intelligence%2520have%2520enabled%2520multi-modal%2520systems%250Ato%2520model%2520and%2520translate%2520diverse%2520information%2520spaces.%2520Extending%2520beyond%2520text%2520and%250Avision%252C%2520we%2520introduce%2520OneProt%252C%2520a%2520multi-modal%2520AI%2520for%2520proteins%2520that%2520integrates%250Astructural%252C%2520sequence%252C%2520text%252C%2520and%2520binding%2520site%2520data.%2520Using%2520the%2520ImageBind%250Aframework%252C%2520OneProt%2520aligns%2520the%2520latent%2520spaces%2520of%2520protein%2520modality%2520encoders%2520in%2520a%250Alightweight%2520fine-tuning%2520scheme%2520that%2520focuses%2520on%2520pairwise%2520alignment%2520with%2520sequence%250Adata%2520rather%2520than%2520requiring%2520full%2520matches.%2520This%2520novel%2520approach%2520comprises%2520a%2520mix%2520of%250AGraph%2520Neural%2520Networks%2520and%2520transformer%2520architectures.%2520It%2520demonstrates%2520strong%250Aperformance%2520in%2520retrieval%2520tasks%2520and%2520showcases%2520the%2520efficacy%2520of%2520multi-modal%250Asystems%2520in%2520Protein%2520Machine%2520Learning%2520through%2520a%2520broad%2520spectrum%2520of%2520downstream%250Abaselines%252C%2520including%2520enzyme%2520function%2520prediction%2520and%2520binding%2520site%2520analysis.%250AFurthermore%252C%2520OneProt%2520enables%2520the%2520transfer%2520of%2520representational%2520information%2520from%250Aspecialized%2520encoders%2520to%2520the%2520sequence%2520encoder%252C%2520enhancing%2520capabilities%2520for%250Adistinguishing%2520evolutionarily%2520related%2520and%2520unrelated%2520sequences%2520and%2520exhibiting%250Arepresentational%2520properties%2520where%2520evolutionarily%2520related%2520proteins%2520align%2520in%250Asimilar%2520directions%2520within%2520the%2520latent%2520space.%2520In%2520addition%252C%2520we%2520extensively%250Ainvestigate%2520modality%2520ablations%2520to%2520identify%2520the%2520encoders%2520that%2520contribute%2520most%2520to%250Apredictive%2520performance%252C%2520highlighting%2520the%2520significance%2520of%2520the%2520binding%2520site%250Aencoder%252C%2520which%2520has%2520not%2520been%2520used%2520in%2520similar%2520models%2520previously.%2520This%2520work%250Aexpands%2520the%2520horizons%2520of%2520multi-modal%2520protein%2520models%252C%2520paving%2520the%2520way%2520for%250Atransformative%2520applications%2520in%2520drug%2520discovery%252C%2520biocatalytic%2520reaction%2520planning%252C%250Aand%2520protein%2520engineering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04863v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneProt%3A%20Towards%20Multi-Modal%20Protein%20Foundation%20Models&entry.906535625=Klemens%20Fl%C3%B6ge%20and%20Srisruthi%20Udayakumar%20and%20Johanna%20Sommer%20and%20Marie%20Piraud%20and%20Stefan%20Kesselheim%20and%20Vincent%20Fortuin%20and%20Stephan%20G%C3%BCnneman%20and%20Karel%20J%20van%20der%20Weg%20and%20Holger%20Gohlke%20and%20Erinc%20Merdivan%20and%20Alina%20Bazarova&entry.1292438233=%20%20Recent%20advances%20in%20Artificial%20Intelligence%20have%20enabled%20multi-modal%20systems%0Ato%20model%20and%20translate%20diverse%20information%20spaces.%20Extending%20beyond%20text%20and%0Avision%2C%20we%20introduce%20OneProt%2C%20a%20multi-modal%20AI%20for%20proteins%20that%20integrates%0Astructural%2C%20sequence%2C%20text%2C%20and%20binding%20site%20data.%20Using%20the%20ImageBind%0Aframework%2C%20OneProt%20aligns%20the%20latent%20spaces%20of%20protein%20modality%20encoders%20in%20a%0Alightweight%20fine-tuning%20scheme%20that%20focuses%20on%20pairwise%20alignment%20with%20sequence%0Adata%20rather%20than%20requiring%20full%20matches.%20This%20novel%20approach%20comprises%20a%20mix%20of%0AGraph%20Neural%20Networks%20and%20transformer%20architectures.%20It%20demonstrates%20strong%0Aperformance%20in%20retrieval%20tasks%20and%20showcases%20the%20efficacy%20of%20multi-modal%0Asystems%20in%20Protein%20Machine%20Learning%20through%20a%20broad%20spectrum%20of%20downstream%0Abaselines%2C%20including%20enzyme%20function%20prediction%20and%20binding%20site%20analysis.%0AFurthermore%2C%20OneProt%20enables%20the%20transfer%20of%20representational%20information%20from%0Aspecialized%20encoders%20to%20the%20sequence%20encoder%2C%20enhancing%20capabilities%20for%0Adistinguishing%20evolutionarily%20related%20and%20unrelated%20sequences%20and%20exhibiting%0Arepresentational%20properties%20where%20evolutionarily%20related%20proteins%20align%20in%0Asimilar%20directions%20within%20the%20latent%20space.%20In%20addition%2C%20we%20extensively%0Ainvestigate%20modality%20ablations%20to%20identify%20the%20encoders%20that%20contribute%20most%20to%0Apredictive%20performance%2C%20highlighting%20the%20significance%20of%20the%20binding%20site%0Aencoder%2C%20which%20has%20not%20been%20used%20in%20similar%20models%20previously.%20This%20work%0Aexpands%20the%20horizons%20of%20multi-modal%20protein%20models%2C%20paving%20the%20way%20for%0Atransformative%20applications%20in%20drug%20discovery%2C%20biocatalytic%20reaction%20planning%2C%0Aand%20protein%20engineering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04863v2&entry.124074799=Read"},
{"title": "Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language\n  Alignment and Modeling", "author": "Bryan Wong and Jong Woo Kim and Huazhu Fu and Mun Yong Yi", "abstract": "  Vision-language models (VLMs) have recently been integrated into multiple\ninstance learning (MIL) frameworks to address the challenge of few-shot, weakly\nsupervised classification of whole slide images (WSIs). A key trend involves\nleveraging multi-scale information to better represent hierarchical tissue\nstructures. However, existing methods often face two key limitations: (1)\ninsufficient modeling of interactions within the same modalities across scales\n(e.g., 5x and 20x) and (2) inadequate alignment between visual and textual\nmodalities on the same scale. To address these gaps, we propose HiVE-MIL, a\nhierarchical vision-language framework that constructs a unified graph\nconsisting of (1) parent-child links between coarse (5x) and fine (20x)\nvisual/textual nodes to capture hierarchical relationships, and (2)\nheterogeneous intra-scale edges linking visual and textual nodes on the same\nscale. To further enhance semantic consistency, HiVE-MIL incorporates a\ntwo-stage, text-guided dynamic filtering mechanism that removes weakly\ncorrelated patch-text pairs, and introduces a hierarchical contrastive loss to\nalign textual semantics across scales. Extensive experiments on TCGA breast,\nlung, and kidney cancer datasets demonstrate that HiVE-MIL consistently\noutperforms both traditional MIL and recent VLM-based MIL approaches, achieving\ngains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate\nthe value of jointly modeling hierarchical structure and multimodal alignment\nfor efficient and scalable learning from limited pathology data. The code is\navailable at https://github.com/bryanwong17/HiVE-MIL\n", "link": "http://arxiv.org/abs/2505.17982v1", "date": "2025-05-23", "relevancy": 2.9236, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6009}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5766}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Learning%20from%20Gigapixel%20Images%20via%20Hierarchical%20Vision-Language%0A%20%20Alignment%20and%20Modeling&body=Title%3A%20Few-Shot%20Learning%20from%20Gigapixel%20Images%20via%20Hierarchical%20Vision-Language%0A%20%20Alignment%20and%20Modeling%0AAuthor%3A%20Bryan%20Wong%20and%20Jong%20Woo%20Kim%20and%20Huazhu%20Fu%20and%20Mun%20Yong%20Yi%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20recently%20been%20integrated%20into%20multiple%0Ainstance%20learning%20%28MIL%29%20frameworks%20to%20address%20the%20challenge%20of%20few-shot%2C%20weakly%0Asupervised%20classification%20of%20whole%20slide%20images%20%28WSIs%29.%20A%20key%20trend%20involves%0Aleveraging%20multi-scale%20information%20to%20better%20represent%20hierarchical%20tissue%0Astructures.%20However%2C%20existing%20methods%20often%20face%20two%20key%20limitations%3A%20%281%29%0Ainsufficient%20modeling%20of%20interactions%20within%20the%20same%20modalities%20across%20scales%0A%28e.g.%2C%205x%20and%2020x%29%20and%20%282%29%20inadequate%20alignment%20between%20visual%20and%20textual%0Amodalities%20on%20the%20same%20scale.%20To%20address%20these%20gaps%2C%20we%20propose%20HiVE-MIL%2C%20a%0Ahierarchical%20vision-language%20framework%20that%20constructs%20a%20unified%20graph%0Aconsisting%20of%20%281%29%20parent-child%20links%20between%20coarse%20%285x%29%20and%20fine%20%2820x%29%0Avisual/textual%20nodes%20to%20capture%20hierarchical%20relationships%2C%20and%20%282%29%0Aheterogeneous%20intra-scale%20edges%20linking%20visual%20and%20textual%20nodes%20on%20the%20same%0Ascale.%20To%20further%20enhance%20semantic%20consistency%2C%20HiVE-MIL%20incorporates%20a%0Atwo-stage%2C%20text-guided%20dynamic%20filtering%20mechanism%20that%20removes%20weakly%0Acorrelated%20patch-text%20pairs%2C%20and%20introduces%20a%20hierarchical%20contrastive%20loss%20to%0Aalign%20textual%20semantics%20across%20scales.%20Extensive%20experiments%20on%20TCGA%20breast%2C%0Alung%2C%20and%20kidney%20cancer%20datasets%20demonstrate%20that%20HiVE-MIL%20consistently%0Aoutperforms%20both%20traditional%20MIL%20and%20recent%20VLM-based%20MIL%20approaches%2C%20achieving%0Agains%20of%20up%20to%204.1%25%20in%20macro%20F1%20under%2016-shot%20settings.%20Our%20results%20demonstrate%0Athe%20value%20of%20jointly%20modeling%20hierarchical%20structure%20and%20multimodal%20alignment%0Afor%20efficient%20and%20scalable%20learning%20from%20limited%20pathology%20data.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/bryanwong17/HiVE-MIL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Learning%2520from%2520Gigapixel%2520Images%2520via%2520Hierarchical%2520Vision-Language%250A%2520%2520Alignment%2520and%2520Modeling%26entry.906535625%3DBryan%2520Wong%2520and%2520Jong%2520Woo%2520Kim%2520and%2520Huazhu%2520Fu%2520and%2520Mun%2520Yong%2520Yi%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520recently%2520been%2520integrated%2520into%2520multiple%250Ainstance%2520learning%2520%2528MIL%2529%2520frameworks%2520to%2520address%2520the%2520challenge%2520of%2520few-shot%252C%2520weakly%250Asupervised%2520classification%2520of%2520whole%2520slide%2520images%2520%2528WSIs%2529.%2520A%2520key%2520trend%2520involves%250Aleveraging%2520multi-scale%2520information%2520to%2520better%2520represent%2520hierarchical%2520tissue%250Astructures.%2520However%252C%2520existing%2520methods%2520often%2520face%2520two%2520key%2520limitations%253A%2520%25281%2529%250Ainsufficient%2520modeling%2520of%2520interactions%2520within%2520the%2520same%2520modalities%2520across%2520scales%250A%2528e.g.%252C%25205x%2520and%252020x%2529%2520and%2520%25282%2529%2520inadequate%2520alignment%2520between%2520visual%2520and%2520textual%250Amodalities%2520on%2520the%2520same%2520scale.%2520To%2520address%2520these%2520gaps%252C%2520we%2520propose%2520HiVE-MIL%252C%2520a%250Ahierarchical%2520vision-language%2520framework%2520that%2520constructs%2520a%2520unified%2520graph%250Aconsisting%2520of%2520%25281%2529%2520parent-child%2520links%2520between%2520coarse%2520%25285x%2529%2520and%2520fine%2520%252820x%2529%250Avisual/textual%2520nodes%2520to%2520capture%2520hierarchical%2520relationships%252C%2520and%2520%25282%2529%250Aheterogeneous%2520intra-scale%2520edges%2520linking%2520visual%2520and%2520textual%2520nodes%2520on%2520the%2520same%250Ascale.%2520To%2520further%2520enhance%2520semantic%2520consistency%252C%2520HiVE-MIL%2520incorporates%2520a%250Atwo-stage%252C%2520text-guided%2520dynamic%2520filtering%2520mechanism%2520that%2520removes%2520weakly%250Acorrelated%2520patch-text%2520pairs%252C%2520and%2520introduces%2520a%2520hierarchical%2520contrastive%2520loss%2520to%250Aalign%2520textual%2520semantics%2520across%2520scales.%2520Extensive%2520experiments%2520on%2520TCGA%2520breast%252C%250Alung%252C%2520and%2520kidney%2520cancer%2520datasets%2520demonstrate%2520that%2520HiVE-MIL%2520consistently%250Aoutperforms%2520both%2520traditional%2520MIL%2520and%2520recent%2520VLM-based%2520MIL%2520approaches%252C%2520achieving%250Agains%2520of%2520up%2520to%25204.1%2525%2520in%2520macro%2520F1%2520under%252016-shot%2520settings.%2520Our%2520results%2520demonstrate%250Athe%2520value%2520of%2520jointly%2520modeling%2520hierarchical%2520structure%2520and%2520multimodal%2520alignment%250Afor%2520efficient%2520and%2520scalable%2520learning%2520from%2520limited%2520pathology%2520data.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/bryanwong17/HiVE-MIL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Learning%20from%20Gigapixel%20Images%20via%20Hierarchical%20Vision-Language%0A%20%20Alignment%20and%20Modeling&entry.906535625=Bryan%20Wong%20and%20Jong%20Woo%20Kim%20and%20Huazhu%20Fu%20and%20Mun%20Yong%20Yi&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20recently%20been%20integrated%20into%20multiple%0Ainstance%20learning%20%28MIL%29%20frameworks%20to%20address%20the%20challenge%20of%20few-shot%2C%20weakly%0Asupervised%20classification%20of%20whole%20slide%20images%20%28WSIs%29.%20A%20key%20trend%20involves%0Aleveraging%20multi-scale%20information%20to%20better%20represent%20hierarchical%20tissue%0Astructures.%20However%2C%20existing%20methods%20often%20face%20two%20key%20limitations%3A%20%281%29%0Ainsufficient%20modeling%20of%20interactions%20within%20the%20same%20modalities%20across%20scales%0A%28e.g.%2C%205x%20and%2020x%29%20and%20%282%29%20inadequate%20alignment%20between%20visual%20and%20textual%0Amodalities%20on%20the%20same%20scale.%20To%20address%20these%20gaps%2C%20we%20propose%20HiVE-MIL%2C%20a%0Ahierarchical%20vision-language%20framework%20that%20constructs%20a%20unified%20graph%0Aconsisting%20of%20%281%29%20parent-child%20links%20between%20coarse%20%285x%29%20and%20fine%20%2820x%29%0Avisual/textual%20nodes%20to%20capture%20hierarchical%20relationships%2C%20and%20%282%29%0Aheterogeneous%20intra-scale%20edges%20linking%20visual%20and%20textual%20nodes%20on%20the%20same%0Ascale.%20To%20further%20enhance%20semantic%20consistency%2C%20HiVE-MIL%20incorporates%20a%0Atwo-stage%2C%20text-guided%20dynamic%20filtering%20mechanism%20that%20removes%20weakly%0Acorrelated%20patch-text%20pairs%2C%20and%20introduces%20a%20hierarchical%20contrastive%20loss%20to%0Aalign%20textual%20semantics%20across%20scales.%20Extensive%20experiments%20on%20TCGA%20breast%2C%0Alung%2C%20and%20kidney%20cancer%20datasets%20demonstrate%20that%20HiVE-MIL%20consistently%0Aoutperforms%20both%20traditional%20MIL%20and%20recent%20VLM-based%20MIL%20approaches%2C%20achieving%0Agains%20of%20up%20to%204.1%25%20in%20macro%20F1%20under%2016-shot%20settings.%20Our%20results%20demonstrate%0Athe%20value%20of%20jointly%20modeling%20hierarchical%20structure%20and%20multimodal%20alignment%0Afor%20efficient%20and%20scalable%20learning%20from%20limited%20pathology%20data.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/bryanwong17/HiVE-MIL%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17982v1&entry.124074799=Read"},
{"title": "Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using\n  Local Inference via DeepSeek", "author": "Xueyang Li and Jiahao Li and Yu Song and Yunzhong Lou and Xiangdong Zhou", "abstract": "  The advent of Computer-Aided Design (CAD) generative modeling will\nsignificantly transform the design of industrial products. The recent research\nendeavor has extended into the realm of Large Language Models (LLMs). In\ncontrast to fine-tuning methods, training-free approaches typically utilize the\nadvanced closed-source LLMs, thereby offering enhanced flexibility and\nefficiency in the development of AI agents for generating CAD parametric\nmodels. However, the substantial cost and limitations of local deployment of\nthe top-tier closed-source LLMs pose challenges in practical applications. The\nSeek-CAD is the pioneer exploration of locally deployed open-source inference\nLLM DeepSeek-R1 for CAD parametric model generation with a training-free\nmethodology. This study is the first investigation to incorporate both visual\nand Chain-of-Thought (CoT) feedback within the self-refinement mechanism for\ngenerating CAD models. Specifically, the initial generated parametric CAD model\nis rendered into a sequence of step-wise perspective images, which are\nsubsequently processed by a Vision Language Model (VLM) alongside the\ncorresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation.\nThen, the feedback is utilized by DeepSeek-R1 to refine the initial generated\nmodel for the next round of generation. Moreover, we present an innovative 3D\nCAD model dataset structured around the SSR (Sketch, Sketch-based feature, and\nRefinements) triple design paradigm. This dataset encompasses a wide range of\nCAD commands, thereby aligning effectively with industrial application\nrequirements and proving suitable for the generation of LLMs. Extensive\nexperiments validate the effectiveness of Seek-CAD under various metrics.\n", "link": "http://arxiv.org/abs/2505.17702v1", "date": "2025-05-23", "relevancy": 2.9186, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5854}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5854}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seek-CAD%3A%20A%20Self-refined%20Generative%20Modeling%20for%203D%20Parametric%20CAD%20Using%0A%20%20Local%20Inference%20via%20DeepSeek&body=Title%3A%20Seek-CAD%3A%20A%20Self-refined%20Generative%20Modeling%20for%203D%20Parametric%20CAD%20Using%0A%20%20Local%20Inference%20via%20DeepSeek%0AAuthor%3A%20Xueyang%20Li%20and%20Jiahao%20Li%20and%20Yu%20Song%20and%20Yunzhong%20Lou%20and%20Xiangdong%20Zhou%0AAbstract%3A%20%20%20The%20advent%20of%20Computer-Aided%20Design%20%28CAD%29%20generative%20modeling%20will%0Asignificantly%20transform%20the%20design%20of%20industrial%20products.%20The%20recent%20research%0Aendeavor%20has%20extended%20into%20the%20realm%20of%20Large%20Language%20Models%20%28LLMs%29.%20In%0Acontrast%20to%20fine-tuning%20methods%2C%20training-free%20approaches%20typically%20utilize%20the%0Aadvanced%20closed-source%20LLMs%2C%20thereby%20offering%20enhanced%20flexibility%20and%0Aefficiency%20in%20the%20development%20of%20AI%20agents%20for%20generating%20CAD%20parametric%0Amodels.%20However%2C%20the%20substantial%20cost%20and%20limitations%20of%20local%20deployment%20of%0Athe%20top-tier%20closed-source%20LLMs%20pose%20challenges%20in%20practical%20applications.%20The%0ASeek-CAD%20is%20the%20pioneer%20exploration%20of%20locally%20deployed%20open-source%20inference%0ALLM%20DeepSeek-R1%20for%20CAD%20parametric%20model%20generation%20with%20a%20training-free%0Amethodology.%20This%20study%20is%20the%20first%20investigation%20to%20incorporate%20both%20visual%0Aand%20Chain-of-Thought%20%28CoT%29%20feedback%20within%20the%20self-refinement%20mechanism%20for%0Agenerating%20CAD%20models.%20Specifically%2C%20the%20initial%20generated%20parametric%20CAD%20model%0Ais%20rendered%20into%20a%20sequence%20of%20step-wise%20perspective%20images%2C%20which%20are%0Asubsequently%20processed%20by%20a%20Vision%20Language%20Model%20%28VLM%29%20alongside%20the%0Acorresponding%20CoTs%20derived%20from%20DeepSeek-R1%20to%20assess%20the%20CAD%20model%20generation.%0AThen%2C%20the%20feedback%20is%20utilized%20by%20DeepSeek-R1%20to%20refine%20the%20initial%20generated%0Amodel%20for%20the%20next%20round%20of%20generation.%20Moreover%2C%20we%20present%20an%20innovative%203D%0ACAD%20model%20dataset%20structured%20around%20the%20SSR%20%28Sketch%2C%20Sketch-based%20feature%2C%20and%0ARefinements%29%20triple%20design%20paradigm.%20This%20dataset%20encompasses%20a%20wide%20range%20of%0ACAD%20commands%2C%20thereby%20aligning%20effectively%20with%20industrial%20application%0Arequirements%20and%20proving%20suitable%20for%20the%20generation%20of%20LLMs.%20Extensive%0Aexperiments%20validate%20the%20effectiveness%20of%20Seek-CAD%20under%20various%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeek-CAD%253A%2520A%2520Self-refined%2520Generative%2520Modeling%2520for%25203D%2520Parametric%2520CAD%2520Using%250A%2520%2520Local%2520Inference%2520via%2520DeepSeek%26entry.906535625%3DXueyang%2520Li%2520and%2520Jiahao%2520Li%2520and%2520Yu%2520Song%2520and%2520Yunzhong%2520Lou%2520and%2520Xiangdong%2520Zhou%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520Computer-Aided%2520Design%2520%2528CAD%2529%2520generative%2520modeling%2520will%250Asignificantly%2520transform%2520the%2520design%2520of%2520industrial%2520products.%2520The%2520recent%2520research%250Aendeavor%2520has%2520extended%2520into%2520the%2520realm%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520In%250Acontrast%2520to%2520fine-tuning%2520methods%252C%2520training-free%2520approaches%2520typically%2520utilize%2520the%250Aadvanced%2520closed-source%2520LLMs%252C%2520thereby%2520offering%2520enhanced%2520flexibility%2520and%250Aefficiency%2520in%2520the%2520development%2520of%2520AI%2520agents%2520for%2520generating%2520CAD%2520parametric%250Amodels.%2520However%252C%2520the%2520substantial%2520cost%2520and%2520limitations%2520of%2520local%2520deployment%2520of%250Athe%2520top-tier%2520closed-source%2520LLMs%2520pose%2520challenges%2520in%2520practical%2520applications.%2520The%250ASeek-CAD%2520is%2520the%2520pioneer%2520exploration%2520of%2520locally%2520deployed%2520open-source%2520inference%250ALLM%2520DeepSeek-R1%2520for%2520CAD%2520parametric%2520model%2520generation%2520with%2520a%2520training-free%250Amethodology.%2520This%2520study%2520is%2520the%2520first%2520investigation%2520to%2520incorporate%2520both%2520visual%250Aand%2520Chain-of-Thought%2520%2528CoT%2529%2520feedback%2520within%2520the%2520self-refinement%2520mechanism%2520for%250Agenerating%2520CAD%2520models.%2520Specifically%252C%2520the%2520initial%2520generated%2520parametric%2520CAD%2520model%250Ais%2520rendered%2520into%2520a%2520sequence%2520of%2520step-wise%2520perspective%2520images%252C%2520which%2520are%250Asubsequently%2520processed%2520by%2520a%2520Vision%2520Language%2520Model%2520%2528VLM%2529%2520alongside%2520the%250Acorresponding%2520CoTs%2520derived%2520from%2520DeepSeek-R1%2520to%2520assess%2520the%2520CAD%2520model%2520generation.%250AThen%252C%2520the%2520feedback%2520is%2520utilized%2520by%2520DeepSeek-R1%2520to%2520refine%2520the%2520initial%2520generated%250Amodel%2520for%2520the%2520next%2520round%2520of%2520generation.%2520Moreover%252C%2520we%2520present%2520an%2520innovative%25203D%250ACAD%2520model%2520dataset%2520structured%2520around%2520the%2520SSR%2520%2528Sketch%252C%2520Sketch-based%2520feature%252C%2520and%250ARefinements%2529%2520triple%2520design%2520paradigm.%2520This%2520dataset%2520encompasses%2520a%2520wide%2520range%2520of%250ACAD%2520commands%252C%2520thereby%2520aligning%2520effectively%2520with%2520industrial%2520application%250Arequirements%2520and%2520proving%2520suitable%2520for%2520the%2520generation%2520of%2520LLMs.%2520Extensive%250Aexperiments%2520validate%2520the%2520effectiveness%2520of%2520Seek-CAD%2520under%2520various%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seek-CAD%3A%20A%20Self-refined%20Generative%20Modeling%20for%203D%20Parametric%20CAD%20Using%0A%20%20Local%20Inference%20via%20DeepSeek&entry.906535625=Xueyang%20Li%20and%20Jiahao%20Li%20and%20Yu%20Song%20and%20Yunzhong%20Lou%20and%20Xiangdong%20Zhou&entry.1292438233=%20%20The%20advent%20of%20Computer-Aided%20Design%20%28CAD%29%20generative%20modeling%20will%0Asignificantly%20transform%20the%20design%20of%20industrial%20products.%20The%20recent%20research%0Aendeavor%20has%20extended%20into%20the%20realm%20of%20Large%20Language%20Models%20%28LLMs%29.%20In%0Acontrast%20to%20fine-tuning%20methods%2C%20training-free%20approaches%20typically%20utilize%20the%0Aadvanced%20closed-source%20LLMs%2C%20thereby%20offering%20enhanced%20flexibility%20and%0Aefficiency%20in%20the%20development%20of%20AI%20agents%20for%20generating%20CAD%20parametric%0Amodels.%20However%2C%20the%20substantial%20cost%20and%20limitations%20of%20local%20deployment%20of%0Athe%20top-tier%20closed-source%20LLMs%20pose%20challenges%20in%20practical%20applications.%20The%0ASeek-CAD%20is%20the%20pioneer%20exploration%20of%20locally%20deployed%20open-source%20inference%0ALLM%20DeepSeek-R1%20for%20CAD%20parametric%20model%20generation%20with%20a%20training-free%0Amethodology.%20This%20study%20is%20the%20first%20investigation%20to%20incorporate%20both%20visual%0Aand%20Chain-of-Thought%20%28CoT%29%20feedback%20within%20the%20self-refinement%20mechanism%20for%0Agenerating%20CAD%20models.%20Specifically%2C%20the%20initial%20generated%20parametric%20CAD%20model%0Ais%20rendered%20into%20a%20sequence%20of%20step-wise%20perspective%20images%2C%20which%20are%0Asubsequently%20processed%20by%20a%20Vision%20Language%20Model%20%28VLM%29%20alongside%20the%0Acorresponding%20CoTs%20derived%20from%20DeepSeek-R1%20to%20assess%20the%20CAD%20model%20generation.%0AThen%2C%20the%20feedback%20is%20utilized%20by%20DeepSeek-R1%20to%20refine%20the%20initial%20generated%0Amodel%20for%20the%20next%20round%20of%20generation.%20Moreover%2C%20we%20present%20an%20innovative%203D%0ACAD%20model%20dataset%20structured%20around%20the%20SSR%20%28Sketch%2C%20Sketch-based%20feature%2C%20and%0ARefinements%29%20triple%20design%20paradigm.%20This%20dataset%20encompasses%20a%20wide%20range%20of%0ACAD%20commands%2C%20thereby%20aligning%20effectively%20with%20industrial%20application%0Arequirements%20and%20proving%20suitable%20for%20the%20generation%20of%20LLMs.%20Extensive%0Aexperiments%20validate%20the%20effectiveness%20of%20Seek-CAD%20under%20various%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17702v1&entry.124074799=Read"},
{"title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning", "author": "Yan Ma and Linge Du and Xuyang Shen and Shaoxiang Chen and Pengfei Li and Qibing Ren and Lizhuang Ma and Yuchao Dai and Pengfei Liu and Junjie Yan", "abstract": "  Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.\n", "link": "http://arxiv.org/abs/2505.18129v1", "date": "2025-05-23", "relevancy": 2.9076, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5857}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5857}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20RL%20to%20See%20Them%20All%3A%20Visual%20Triple%20Unified%20Reinforcement%20Learning&body=Title%3A%20One%20RL%20to%20See%20Them%20All%3A%20Visual%20Triple%20Unified%20Reinforcement%20Learning%0AAuthor%3A%20Yan%20Ma%20and%20Linge%20Du%20and%20Xuyang%20Shen%20and%20Shaoxiang%20Chen%20and%20Pengfei%20Li%20and%20Qibing%20Ren%20and%20Lizhuang%20Ma%20and%20Yuchao%20Dai%20and%20Pengfei%20Liu%20and%20Junjie%20Yan%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20has%20significantly%20advanced%20the%20reasoning%0Acapabilities%20of%20vision-language%20models%20%28VLMs%29.%20However%2C%20the%20use%20of%20RL%20beyond%0Areasoning%20tasks%20remains%20largely%20unexplored%2C%20especially%20for%20perceptionintensive%0Atasks%20like%20object%20detection%20and%20grounding.%20We%20propose%20V-Triune%2C%20a%20Visual%20Triple%0AUnified%20Reinforcement%20Learning%20system%20that%20enables%20VLMs%20to%20jointly%20learn%20visual%0Areasoning%20and%20perception%20tasks%20within%20a%20single%20training%20pipeline.%20V-Triune%0Acomprises%20triple%20complementary%20components%3A%20Sample-Level%20Data%20Formatting%20%28to%0Aunify%20diverse%20task%20inputs%29%2C%20Verifier-Level%20Reward%20Computation%20%28to%20deliver%0Acustom%20rewards%20via%20specialized%20verifiers%29%20%2C%20and%20Source-Level%20Metric%20Monitoring%0A%28to%20diagnose%20problems%20at%20the%20data-source%20level%29.%20We%20further%20introduce%20a%20novel%0ADynamic%20IoU%20reward%2C%20which%20provides%20adaptive%2C%20progressive%2C%20and%20definite%20feedback%0Afor%20perception%20tasks%20handled%20by%20V-Triune.%20Our%20approach%20is%20instantiated%20within%0Aoff-the-shelf%20RL%20training%20framework%20using%20open-source%207B%20and%2032B%20backbone%0Amodels.%20The%20resulting%20model%2C%20dubbed%20Orsta%20%28One%20RL%20to%20See%20Them%20All%29%2C%0Ademonstrates%20consistent%20improvements%20across%20both%20reasoning%20and%20perception%0Atasks.%20This%20broad%20capability%20is%20significantly%20shaped%20by%20its%20training%20on%20a%0Adiverse%20dataset%2C%20constructed%20around%20four%20representative%20visual%20reasoning%20tasks%0A%28Math%2C%20Puzzle%2C%20Chart%2C%20and%20Science%29%20and%20four%20visual%20perception%20tasks%20%28Grounding%2C%0ADetection%2C%20Counting%2C%20and%20OCR%29.%20Subsequently%2C%20Orsta%20achieves%20substantial%20gains%0Aon%20MEGA-Bench%20Core%2C%20with%20improvements%20ranging%20from%20%2B2.1%20to%20an%20impressive%20%2B14.1%0Aacross%20its%20various%207B%20and%2032B%20model%20variants%2C%20with%20performance%20benefits%0Aextending%20to%20a%20wide%20range%20of%20downstream%20tasks.%20These%20results%20highlight%20the%0Aeffectiveness%20and%20scalability%20of%20our%20unified%20RL%20approach%20for%20VLMs.%20The%20V-Triune%0Asystem%2C%20along%20with%20the%20Orsta%20models%2C%20is%20publicly%20available%20at%0Ahttps%3A//github.com/MiniMax-AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520RL%2520to%2520See%2520Them%2520All%253A%2520Visual%2520Triple%2520Unified%2520Reinforcement%2520Learning%26entry.906535625%3DYan%2520Ma%2520and%2520Linge%2520Du%2520and%2520Xuyang%2520Shen%2520and%2520Shaoxiang%2520Chen%2520and%2520Pengfei%2520Li%2520and%2520Qibing%2520Ren%2520and%2520Lizhuang%2520Ma%2520and%2520Yuchao%2520Dai%2520and%2520Pengfei%2520Liu%2520and%2520Junjie%2520Yan%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520has%2520significantly%2520advanced%2520the%2520reasoning%250Acapabilities%2520of%2520vision-language%2520models%2520%2528VLMs%2529.%2520However%252C%2520the%2520use%2520of%2520RL%2520beyond%250Areasoning%2520tasks%2520remains%2520largely%2520unexplored%252C%2520especially%2520for%2520perceptionintensive%250Atasks%2520like%2520object%2520detection%2520and%2520grounding.%2520We%2520propose%2520V-Triune%252C%2520a%2520Visual%2520Triple%250AUnified%2520Reinforcement%2520Learning%2520system%2520that%2520enables%2520VLMs%2520to%2520jointly%2520learn%2520visual%250Areasoning%2520and%2520perception%2520tasks%2520within%2520a%2520single%2520training%2520pipeline.%2520V-Triune%250Acomprises%2520triple%2520complementary%2520components%253A%2520Sample-Level%2520Data%2520Formatting%2520%2528to%250Aunify%2520diverse%2520task%2520inputs%2529%252C%2520Verifier-Level%2520Reward%2520Computation%2520%2528to%2520deliver%250Acustom%2520rewards%2520via%2520specialized%2520verifiers%2529%2520%252C%2520and%2520Source-Level%2520Metric%2520Monitoring%250A%2528to%2520diagnose%2520problems%2520at%2520the%2520data-source%2520level%2529.%2520We%2520further%2520introduce%2520a%2520novel%250ADynamic%2520IoU%2520reward%252C%2520which%2520provides%2520adaptive%252C%2520progressive%252C%2520and%2520definite%2520feedback%250Afor%2520perception%2520tasks%2520handled%2520by%2520V-Triune.%2520Our%2520approach%2520is%2520instantiated%2520within%250Aoff-the-shelf%2520RL%2520training%2520framework%2520using%2520open-source%25207B%2520and%252032B%2520backbone%250Amodels.%2520The%2520resulting%2520model%252C%2520dubbed%2520Orsta%2520%2528One%2520RL%2520to%2520See%2520Them%2520All%2529%252C%250Ademonstrates%2520consistent%2520improvements%2520across%2520both%2520reasoning%2520and%2520perception%250Atasks.%2520This%2520broad%2520capability%2520is%2520significantly%2520shaped%2520by%2520its%2520training%2520on%2520a%250Adiverse%2520dataset%252C%2520constructed%2520around%2520four%2520representative%2520visual%2520reasoning%2520tasks%250A%2528Math%252C%2520Puzzle%252C%2520Chart%252C%2520and%2520Science%2529%2520and%2520four%2520visual%2520perception%2520tasks%2520%2528Grounding%252C%250ADetection%252C%2520Counting%252C%2520and%2520OCR%2529.%2520Subsequently%252C%2520Orsta%2520achieves%2520substantial%2520gains%250Aon%2520MEGA-Bench%2520Core%252C%2520with%2520improvements%2520ranging%2520from%2520%252B2.1%2520to%2520an%2520impressive%2520%252B14.1%250Aacross%2520its%2520various%25207B%2520and%252032B%2520model%2520variants%252C%2520with%2520performance%2520benefits%250Aextending%2520to%2520a%2520wide%2520range%2520of%2520downstream%2520tasks.%2520These%2520results%2520highlight%2520the%250Aeffectiveness%2520and%2520scalability%2520of%2520our%2520unified%2520RL%2520approach%2520for%2520VLMs.%2520The%2520V-Triune%250Asystem%252C%2520along%2520with%2520the%2520Orsta%2520models%252C%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/MiniMax-AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20RL%20to%20See%20Them%20All%3A%20Visual%20Triple%20Unified%20Reinforcement%20Learning&entry.906535625=Yan%20Ma%20and%20Linge%20Du%20and%20Xuyang%20Shen%20and%20Shaoxiang%20Chen%20and%20Pengfei%20Li%20and%20Qibing%20Ren%20and%20Lizhuang%20Ma%20and%20Yuchao%20Dai%20and%20Pengfei%20Liu%20and%20Junjie%20Yan&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20has%20significantly%20advanced%20the%20reasoning%0Acapabilities%20of%20vision-language%20models%20%28VLMs%29.%20However%2C%20the%20use%20of%20RL%20beyond%0Areasoning%20tasks%20remains%20largely%20unexplored%2C%20especially%20for%20perceptionintensive%0Atasks%20like%20object%20detection%20and%20grounding.%20We%20propose%20V-Triune%2C%20a%20Visual%20Triple%0AUnified%20Reinforcement%20Learning%20system%20that%20enables%20VLMs%20to%20jointly%20learn%20visual%0Areasoning%20and%20perception%20tasks%20within%20a%20single%20training%20pipeline.%20V-Triune%0Acomprises%20triple%20complementary%20components%3A%20Sample-Level%20Data%20Formatting%20%28to%0Aunify%20diverse%20task%20inputs%29%2C%20Verifier-Level%20Reward%20Computation%20%28to%20deliver%0Acustom%20rewards%20via%20specialized%20verifiers%29%20%2C%20and%20Source-Level%20Metric%20Monitoring%0A%28to%20diagnose%20problems%20at%20the%20data-source%20level%29.%20We%20further%20introduce%20a%20novel%0ADynamic%20IoU%20reward%2C%20which%20provides%20adaptive%2C%20progressive%2C%20and%20definite%20feedback%0Afor%20perception%20tasks%20handled%20by%20V-Triune.%20Our%20approach%20is%20instantiated%20within%0Aoff-the-shelf%20RL%20training%20framework%20using%20open-source%207B%20and%2032B%20backbone%0Amodels.%20The%20resulting%20model%2C%20dubbed%20Orsta%20%28One%20RL%20to%20See%20Them%20All%29%2C%0Ademonstrates%20consistent%20improvements%20across%20both%20reasoning%20and%20perception%0Atasks.%20This%20broad%20capability%20is%20significantly%20shaped%20by%20its%20training%20on%20a%0Adiverse%20dataset%2C%20constructed%20around%20four%20representative%20visual%20reasoning%20tasks%0A%28Math%2C%20Puzzle%2C%20Chart%2C%20and%20Science%29%20and%20four%20visual%20perception%20tasks%20%28Grounding%2C%0ADetection%2C%20Counting%2C%20and%20OCR%29.%20Subsequently%2C%20Orsta%20achieves%20substantial%20gains%0Aon%20MEGA-Bench%20Core%2C%20with%20improvements%20ranging%20from%20%2B2.1%20to%20an%20impressive%20%2B14.1%0Aacross%20its%20various%207B%20and%2032B%20model%20variants%2C%20with%20performance%20benefits%0Aextending%20to%20a%20wide%20range%20of%20downstream%20tasks.%20These%20results%20highlight%20the%0Aeffectiveness%20and%20scalability%20of%20our%20unified%20RL%20approach%20for%20VLMs.%20The%20V-Triune%0Asystem%2C%20along%20with%20the%20Orsta%20models%2C%20is%20publicly%20available%20at%0Ahttps%3A//github.com/MiniMax-AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18129v1&entry.124074799=Read"},
{"title": "LMAct: A Benchmark for In-Context Imitation Learning with Long\n  Multimodal Demonstrations", "author": "Anian Ruoss and Fabio Pardo and Harris Chan and Bonnie Li and Volodymyr Mnih and Tim Genewein", "abstract": "  In this paper, we present a benchmark to pressure-test today's frontier\nmodels' multimodal decision-making capabilities in the very long-context regime\n(up to one million tokens) and investigate whether these models can learn from\nlarge numbers of expert demonstrations in their context. We evaluate the\nperformance of Claude 3.5 Sonnet, Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 2.0\nFlash Experimental, GPT-4o, o1-mini, o1-preview, and o1 as policies across a\nbattery of simple interactive decision-making tasks: playing tic-tac-toe,\nchess, and Atari, navigating grid worlds, solving crosswords, and controlling a\nsimulated cheetah. We study increasing amounts of expert demonstrations in the\ncontext $\\unicode{x2013}$ from no demonstrations to 512 full episodes. Across\nour tasks, models rarely manage to fully reach expert performance, and often,\npresenting more demonstrations has little effect. Some models steadily improve\nwith more demonstrations on a few tasks. We investigate the effect of encoding\nobservations as text or images and the impact of chain-of-thought prompting. To\nhelp quantify the impact of other approaches and future innovations, we open\nsource our benchmark that covers the zero-, few-, and many-shot regimes in a\nunified evaluation.\n", "link": "http://arxiv.org/abs/2412.01441v3", "date": "2025-05-23", "relevancy": 2.8815, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5878}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LMAct%3A%20A%20Benchmark%20for%20In-Context%20Imitation%20Learning%20with%20Long%0A%20%20Multimodal%20Demonstrations&body=Title%3A%20LMAct%3A%20A%20Benchmark%20for%20In-Context%20Imitation%20Learning%20with%20Long%0A%20%20Multimodal%20Demonstrations%0AAuthor%3A%20Anian%20Ruoss%20and%20Fabio%20Pardo%20and%20Harris%20Chan%20and%20Bonnie%20Li%20and%20Volodymyr%20Mnih%20and%20Tim%20Genewein%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20benchmark%20to%20pressure-test%20today%27s%20frontier%0Amodels%27%20multimodal%20decision-making%20capabilities%20in%20the%20very%20long-context%20regime%0A%28up%20to%20one%20million%20tokens%29%20and%20investigate%20whether%20these%20models%20can%20learn%20from%0Alarge%20numbers%20of%20expert%20demonstrations%20in%20their%20context.%20We%20evaluate%20the%0Aperformance%20of%20Claude%203.5%20Sonnet%2C%20Gemini%201.5%20Flash%2C%20Gemini%201.5%20Pro%2C%20Gemini%202.0%0AFlash%20Experimental%2C%20GPT-4o%2C%20o1-mini%2C%20o1-preview%2C%20and%20o1%20as%20policies%20across%20a%0Abattery%20of%20simple%20interactive%20decision-making%20tasks%3A%20playing%20tic-tac-toe%2C%0Achess%2C%20and%20Atari%2C%20navigating%20grid%20worlds%2C%20solving%20crosswords%2C%20and%20controlling%20a%0Asimulated%20cheetah.%20We%20study%20increasing%20amounts%20of%20expert%20demonstrations%20in%20the%0Acontext%20%24%5Cunicode%7Bx2013%7D%24%20from%20no%20demonstrations%20to%20512%20full%20episodes.%20Across%0Aour%20tasks%2C%20models%20rarely%20manage%20to%20fully%20reach%20expert%20performance%2C%20and%20often%2C%0Apresenting%20more%20demonstrations%20has%20little%20effect.%20Some%20models%20steadily%20improve%0Awith%20more%20demonstrations%20on%20a%20few%20tasks.%20We%20investigate%20the%20effect%20of%20encoding%0Aobservations%20as%20text%20or%20images%20and%20the%20impact%20of%20chain-of-thought%20prompting.%20To%0Ahelp%20quantify%20the%20impact%20of%20other%20approaches%20and%20future%20innovations%2C%20we%20open%0Asource%20our%20benchmark%20that%20covers%20the%20zero-%2C%20few-%2C%20and%20many-shot%20regimes%20in%20a%0Aunified%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01441v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLMAct%253A%2520A%2520Benchmark%2520for%2520In-Context%2520Imitation%2520Learning%2520with%2520Long%250A%2520%2520Multimodal%2520Demonstrations%26entry.906535625%3DAnian%2520Ruoss%2520and%2520Fabio%2520Pardo%2520and%2520Harris%2520Chan%2520and%2520Bonnie%2520Li%2520and%2520Volodymyr%2520Mnih%2520and%2520Tim%2520Genewein%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520benchmark%2520to%2520pressure-test%2520today%2527s%2520frontier%250Amodels%2527%2520multimodal%2520decision-making%2520capabilities%2520in%2520the%2520very%2520long-context%2520regime%250A%2528up%2520to%2520one%2520million%2520tokens%2529%2520and%2520investigate%2520whether%2520these%2520models%2520can%2520learn%2520from%250Alarge%2520numbers%2520of%2520expert%2520demonstrations%2520in%2520their%2520context.%2520We%2520evaluate%2520the%250Aperformance%2520of%2520Claude%25203.5%2520Sonnet%252C%2520Gemini%25201.5%2520Flash%252C%2520Gemini%25201.5%2520Pro%252C%2520Gemini%25202.0%250AFlash%2520Experimental%252C%2520GPT-4o%252C%2520o1-mini%252C%2520o1-preview%252C%2520and%2520o1%2520as%2520policies%2520across%2520a%250Abattery%2520of%2520simple%2520interactive%2520decision-making%2520tasks%253A%2520playing%2520tic-tac-toe%252C%250Achess%252C%2520and%2520Atari%252C%2520navigating%2520grid%2520worlds%252C%2520solving%2520crosswords%252C%2520and%2520controlling%2520a%250Asimulated%2520cheetah.%2520We%2520study%2520increasing%2520amounts%2520of%2520expert%2520demonstrations%2520in%2520the%250Acontext%2520%2524%255Cunicode%257Bx2013%257D%2524%2520from%2520no%2520demonstrations%2520to%2520512%2520full%2520episodes.%2520Across%250Aour%2520tasks%252C%2520models%2520rarely%2520manage%2520to%2520fully%2520reach%2520expert%2520performance%252C%2520and%2520often%252C%250Apresenting%2520more%2520demonstrations%2520has%2520little%2520effect.%2520Some%2520models%2520steadily%2520improve%250Awith%2520more%2520demonstrations%2520on%2520a%2520few%2520tasks.%2520We%2520investigate%2520the%2520effect%2520of%2520encoding%250Aobservations%2520as%2520text%2520or%2520images%2520and%2520the%2520impact%2520of%2520chain-of-thought%2520prompting.%2520To%250Ahelp%2520quantify%2520the%2520impact%2520of%2520other%2520approaches%2520and%2520future%2520innovations%252C%2520we%2520open%250Asource%2520our%2520benchmark%2520that%2520covers%2520the%2520zero-%252C%2520few-%252C%2520and%2520many-shot%2520regimes%2520in%2520a%250Aunified%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01441v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LMAct%3A%20A%20Benchmark%20for%20In-Context%20Imitation%20Learning%20with%20Long%0A%20%20Multimodal%20Demonstrations&entry.906535625=Anian%20Ruoss%20and%20Fabio%20Pardo%20and%20Harris%20Chan%20and%20Bonnie%20Li%20and%20Volodymyr%20Mnih%20and%20Tim%20Genewein&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20benchmark%20to%20pressure-test%20today%27s%20frontier%0Amodels%27%20multimodal%20decision-making%20capabilities%20in%20the%20very%20long-context%20regime%0A%28up%20to%20one%20million%20tokens%29%20and%20investigate%20whether%20these%20models%20can%20learn%20from%0Alarge%20numbers%20of%20expert%20demonstrations%20in%20their%20context.%20We%20evaluate%20the%0Aperformance%20of%20Claude%203.5%20Sonnet%2C%20Gemini%201.5%20Flash%2C%20Gemini%201.5%20Pro%2C%20Gemini%202.0%0AFlash%20Experimental%2C%20GPT-4o%2C%20o1-mini%2C%20o1-preview%2C%20and%20o1%20as%20policies%20across%20a%0Abattery%20of%20simple%20interactive%20decision-making%20tasks%3A%20playing%20tic-tac-toe%2C%0Achess%2C%20and%20Atari%2C%20navigating%20grid%20worlds%2C%20solving%20crosswords%2C%20and%20controlling%20a%0Asimulated%20cheetah.%20We%20study%20increasing%20amounts%20of%20expert%20demonstrations%20in%20the%0Acontext%20%24%5Cunicode%7Bx2013%7D%24%20from%20no%20demonstrations%20to%20512%20full%20episodes.%20Across%0Aour%20tasks%2C%20models%20rarely%20manage%20to%20fully%20reach%20expert%20performance%2C%20and%20often%2C%0Apresenting%20more%20demonstrations%20has%20little%20effect.%20Some%20models%20steadily%20improve%0Awith%20more%20demonstrations%20on%20a%20few%20tasks.%20We%20investigate%20the%20effect%20of%20encoding%0Aobservations%20as%20text%20or%20images%20and%20the%20impact%20of%20chain-of-thought%20prompting.%20To%0Ahelp%20quantify%20the%20impact%20of%20other%20approaches%20and%20future%20innovations%2C%20we%20open%0Asource%20our%20benchmark%20that%20covers%20the%20zero-%2C%20few-%2C%20and%20many-shot%20regimes%20in%20a%0Aunified%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01441v3&entry.124074799=Read"},
{"title": "VideoGameBench: Can Vision-Language Models complete popular video games?", "author": "Alex L. Zhang and Thomas L. Griffiths and Karthik R. Narasimhan and Ofir Press", "abstract": "  Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions.\n", "link": "http://arxiv.org/abs/2505.18134v1", "date": "2025-05-23", "relevancy": 2.8712, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6089}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoGameBench%3A%20Can%20Vision-Language%20Models%20complete%20popular%20video%20games%3F&body=Title%3A%20VideoGameBench%3A%20Can%20Vision-Language%20Models%20complete%20popular%20video%20games%3F%0AAuthor%3A%20Alex%20L.%20Zhang%20and%20Thomas%20L.%20Griffiths%20and%20Karthik%20R.%20Narasimhan%20and%20Ofir%20Press%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20achieved%20strong%20results%20on%20coding%20and%20math%0Abenchmarks%20that%20are%20challenging%20for%20humans%2C%20yet%20their%20ability%20to%20perform%20tasks%0Athat%20come%20naturally%20to%20humans--such%20as%20perception%2C%20spatial%20navigation%2C%20and%0Amemory%20management--remains%20understudied.%20Real%20video%20games%20are%20crafted%20to%20be%0Aintuitive%20for%20humans%20to%20learn%20and%20master%20by%20leveraging%20innate%20inductive%20biases%2C%0Amaking%20them%20an%20ideal%20testbed%20for%20evaluating%20such%20capabilities%20in%20VLMs.%20To%20this%0Aend%2C%20we%20introduce%20VideoGameBench%2C%20a%20benchmark%20consisting%20of%2010%20popular%20video%0Agames%20from%20the%201990s%20that%20VLMs%20directly%20interact%20with%20in%20real-time.%0AVideoGameBench%20challenges%20models%20to%20complete%20entire%20games%20with%20access%20to%20only%0Araw%20visual%20inputs%20and%20a%20high-level%20description%20of%20objectives%20and%20controls%2C%20a%0Asignificant%20departure%20from%20existing%20setups%20that%20rely%20on%20game-specific%0Ascaffolding%20and%20auxiliary%20information.%20We%20keep%20three%20of%20the%20games%20secret%20to%0Aencourage%20solutions%20that%20generalize%20to%20unseen%20environments.%20Our%20experiments%0Ashow%20that%20frontier%20vision-language%20models%20struggle%20to%20progress%20beyond%20the%0Abeginning%20of%20each%20game.%20We%20find%20inference%20latency%20to%20be%20a%20major%20limitation%20of%0Afrontier%20models%20in%20the%20real-time%20setting%3B%20therefore%2C%20we%20introduce%0AVideoGameBench%20Lite%2C%20a%20setting%20where%20the%20game%20pauses%20while%20waiting%20for%20the%20LM%27s%0Anext%20action.%20The%20best%20performing%20model%2C%20Gemini%202.5%20Pro%2C%20completes%20only%200.48%25%20of%0AVideoGameBench%20and%201.6%25%20of%20VideoGameBench%20Lite.%20We%20hope%20that%20the%20formalization%0Aof%20the%20human%20skills%20mentioned%20above%20into%20this%20benchmark%20motivates%20progress%20in%0Athese%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoGameBench%253A%2520Can%2520Vision-Language%2520Models%2520complete%2520popular%2520video%2520games%253F%26entry.906535625%3DAlex%2520L.%2520Zhang%2520and%2520Thomas%2520L.%2520Griffiths%2520and%2520Karthik%2520R.%2520Narasimhan%2520and%2520Ofir%2520Press%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520achieved%2520strong%2520results%2520on%2520coding%2520and%2520math%250Abenchmarks%2520that%2520are%2520challenging%2520for%2520humans%252C%2520yet%2520their%2520ability%2520to%2520perform%2520tasks%250Athat%2520come%2520naturally%2520to%2520humans--such%2520as%2520perception%252C%2520spatial%2520navigation%252C%2520and%250Amemory%2520management--remains%2520understudied.%2520Real%2520video%2520games%2520are%2520crafted%2520to%2520be%250Aintuitive%2520for%2520humans%2520to%2520learn%2520and%2520master%2520by%2520leveraging%2520innate%2520inductive%2520biases%252C%250Amaking%2520them%2520an%2520ideal%2520testbed%2520for%2520evaluating%2520such%2520capabilities%2520in%2520VLMs.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520VideoGameBench%252C%2520a%2520benchmark%2520consisting%2520of%252010%2520popular%2520video%250Agames%2520from%2520the%25201990s%2520that%2520VLMs%2520directly%2520interact%2520with%2520in%2520real-time.%250AVideoGameBench%2520challenges%2520models%2520to%2520complete%2520entire%2520games%2520with%2520access%2520to%2520only%250Araw%2520visual%2520inputs%2520and%2520a%2520high-level%2520description%2520of%2520objectives%2520and%2520controls%252C%2520a%250Asignificant%2520departure%2520from%2520existing%2520setups%2520that%2520rely%2520on%2520game-specific%250Ascaffolding%2520and%2520auxiliary%2520information.%2520We%2520keep%2520three%2520of%2520the%2520games%2520secret%2520to%250Aencourage%2520solutions%2520that%2520generalize%2520to%2520unseen%2520environments.%2520Our%2520experiments%250Ashow%2520that%2520frontier%2520vision-language%2520models%2520struggle%2520to%2520progress%2520beyond%2520the%250Abeginning%2520of%2520each%2520game.%2520We%2520find%2520inference%2520latency%2520to%2520be%2520a%2520major%2520limitation%2520of%250Afrontier%2520models%2520in%2520the%2520real-time%2520setting%253B%2520therefore%252C%2520we%2520introduce%250AVideoGameBench%2520Lite%252C%2520a%2520setting%2520where%2520the%2520game%2520pauses%2520while%2520waiting%2520for%2520the%2520LM%2527s%250Anext%2520action.%2520The%2520best%2520performing%2520model%252C%2520Gemini%25202.5%2520Pro%252C%2520completes%2520only%25200.48%2525%2520of%250AVideoGameBench%2520and%25201.6%2525%2520of%2520VideoGameBench%2520Lite.%2520We%2520hope%2520that%2520the%2520formalization%250Aof%2520the%2520human%2520skills%2520mentioned%2520above%2520into%2520this%2520benchmark%2520motivates%2520progress%2520in%250Athese%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoGameBench%3A%20Can%20Vision-Language%20Models%20complete%20popular%20video%20games%3F&entry.906535625=Alex%20L.%20Zhang%20and%20Thomas%20L.%20Griffiths%20and%20Karthik%20R.%20Narasimhan%20and%20Ofir%20Press&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20achieved%20strong%20results%20on%20coding%20and%20math%0Abenchmarks%20that%20are%20challenging%20for%20humans%2C%20yet%20their%20ability%20to%20perform%20tasks%0Athat%20come%20naturally%20to%20humans--such%20as%20perception%2C%20spatial%20navigation%2C%20and%0Amemory%20management--remains%20understudied.%20Real%20video%20games%20are%20crafted%20to%20be%0Aintuitive%20for%20humans%20to%20learn%20and%20master%20by%20leveraging%20innate%20inductive%20biases%2C%0Amaking%20them%20an%20ideal%20testbed%20for%20evaluating%20such%20capabilities%20in%20VLMs.%20To%20this%0Aend%2C%20we%20introduce%20VideoGameBench%2C%20a%20benchmark%20consisting%20of%2010%20popular%20video%0Agames%20from%20the%201990s%20that%20VLMs%20directly%20interact%20with%20in%20real-time.%0AVideoGameBench%20challenges%20models%20to%20complete%20entire%20games%20with%20access%20to%20only%0Araw%20visual%20inputs%20and%20a%20high-level%20description%20of%20objectives%20and%20controls%2C%20a%0Asignificant%20departure%20from%20existing%20setups%20that%20rely%20on%20game-specific%0Ascaffolding%20and%20auxiliary%20information.%20We%20keep%20three%20of%20the%20games%20secret%20to%0Aencourage%20solutions%20that%20generalize%20to%20unseen%20environments.%20Our%20experiments%0Ashow%20that%20frontier%20vision-language%20models%20struggle%20to%20progress%20beyond%20the%0Abeginning%20of%20each%20game.%20We%20find%20inference%20latency%20to%20be%20a%20major%20limitation%20of%0Afrontier%20models%20in%20the%20real-time%20setting%3B%20therefore%2C%20we%20introduce%0AVideoGameBench%20Lite%2C%20a%20setting%20where%20the%20game%20pauses%20while%20waiting%20for%20the%20LM%27s%0Anext%20action.%20The%20best%20performing%20model%2C%20Gemini%202.5%20Pro%2C%20completes%20only%200.48%25%20of%0AVideoGameBench%20and%201.6%25%20of%20VideoGameBench%20Lite.%20We%20hope%20that%20the%20formalization%0Aof%20the%20human%20skills%20mentioned%20above%20into%20this%20benchmark%20motivates%20progress%20in%0Athese%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18134v1&entry.124074799=Read"},
{"title": "ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis", "author": "Onkar Susladkar and Gayatri Deshmukh and Yalcin Tur and Gorkhem Durak and Ulas Bagci", "abstract": "  Synthesizing medical images remains challenging due to limited annotated\npathological data, modality domain gaps, and the complexity of representing\ndiffuse pathologies such as liver cirrhosis. Existing methods often struggle to\nmaintain anatomical fidelity while accurately modeling pathological features,\nfrequently relying on priors derived from natural images or inefficient\nmulti-step sampling. In this work, we introduce ViCTr (Vital Consistency\nTransfer), a novel two-stage framework that combines a rectified flow\ntrajectory with a Tweedie-corrected diffusion process to achieve high-fidelity,\npathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k\ndataset using Elastic Weight Consolidation (EWC) to preserve critical\nanatomical structures. We then fine-tune the model adversarially with Low-Rank\nAdaptation (LoRA) modules for precise control over pathology severity. By\nreformulating Tweedie's formula within a linear trajectory framework, ViCTr\nsupports one-step sampling, reducing inference from 50 steps to just 4, without\nsacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and\nCirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art\nperformance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for\ncirrhosis synthesis 28% lower than existing approaches and improving nnUNet\nsegmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews\nindicate that ViCTr-generated liver cirrhosis MRIs are clinically\nindistinguishable from real scans. To our knowledge, ViCTr is the first method\nto provide fine-grained, pathology-aware MRI synthesis with graded severity\ncontrol, closing a critical gap in AI-driven medical imaging research.\n", "link": "http://arxiv.org/abs/2505.04963v2", "date": "2025-05-23", "relevancy": 2.793, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5674}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5542}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViCTr%3A%20Vital%20Consistency%20Transfer%20for%20Pathology%20Aware%20Image%20Synthesis&body=Title%3A%20ViCTr%3A%20Vital%20Consistency%20Transfer%20for%20Pathology%20Aware%20Image%20Synthesis%0AAuthor%3A%20Onkar%20Susladkar%20and%20Gayatri%20Deshmukh%20and%20Yalcin%20Tur%20and%20Gorkhem%20Durak%20and%20Ulas%20Bagci%0AAbstract%3A%20%20%20Synthesizing%20medical%20images%20remains%20challenging%20due%20to%20limited%20annotated%0Apathological%20data%2C%20modality%20domain%20gaps%2C%20and%20the%20complexity%20of%20representing%0Adiffuse%20pathologies%20such%20as%20liver%20cirrhosis.%20Existing%20methods%20often%20struggle%20to%0Amaintain%20anatomical%20fidelity%20while%20accurately%20modeling%20pathological%20features%2C%0Afrequently%20relying%20on%20priors%20derived%20from%20natural%20images%20or%20inefficient%0Amulti-step%20sampling.%20In%20this%20work%2C%20we%20introduce%20ViCTr%20%28Vital%20Consistency%0ATransfer%29%2C%20a%20novel%20two-stage%20framework%20that%20combines%20a%20rectified%20flow%0Atrajectory%20with%20a%20Tweedie-corrected%20diffusion%20process%20to%20achieve%20high-fidelity%2C%0Apathology-aware%20image%20synthesis.%20First%2C%20we%20pretrain%20ViCTr%20on%20the%20ATLAS-8k%0Adataset%20using%20Elastic%20Weight%20Consolidation%20%28EWC%29%20to%20preserve%20critical%0Aanatomical%20structures.%20We%20then%20fine-tune%20the%20model%20adversarially%20with%20Low-Rank%0AAdaptation%20%28LoRA%29%20modules%20for%20precise%20control%20over%20pathology%20severity.%20By%0Areformulating%20Tweedie%27s%20formula%20within%20a%20linear%20trajectory%20framework%2C%20ViCTr%0Asupports%20one-step%20sampling%2C%20reducing%20inference%20from%2050%20steps%20to%20just%204%2C%20without%0Asacrificing%20anatomical%20realism.%20We%20evaluate%20ViCTr%20on%20BTCV%20%28CT%29%2C%20AMOS%20%28MRI%29%2C%20and%0ACirrMRI600%2B%20%28cirrhosis%29%20datasets.%20Results%20demonstrate%20state-of-the-art%0Aperformance%2C%20achieving%20a%20Medical%20Frechet%20Inception%20Distance%20%28MFID%29%20of%2017.01%20for%0Acirrhosis%20synthesis%2028%25%20lower%20than%20existing%20approaches%20and%20improving%20nnUNet%0Asegmentation%20by%20%2B3.8%25%20mDSC%20when%20used%20for%20data%20augmentation.%20Radiologist%20reviews%0Aindicate%20that%20ViCTr-generated%20liver%20cirrhosis%20MRIs%20are%20clinically%0Aindistinguishable%20from%20real%20scans.%20To%20our%20knowledge%2C%20ViCTr%20is%20the%20first%20method%0Ato%20provide%20fine-grained%2C%20pathology-aware%20MRI%20synthesis%20with%20graded%20severity%0Acontrol%2C%20closing%20a%20critical%20gap%20in%20AI-driven%20medical%20imaging%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04963v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViCTr%253A%2520Vital%2520Consistency%2520Transfer%2520for%2520Pathology%2520Aware%2520Image%2520Synthesis%26entry.906535625%3DOnkar%2520Susladkar%2520and%2520Gayatri%2520Deshmukh%2520and%2520Yalcin%2520Tur%2520and%2520Gorkhem%2520Durak%2520and%2520Ulas%2520Bagci%26entry.1292438233%3D%2520%2520Synthesizing%2520medical%2520images%2520remains%2520challenging%2520due%2520to%2520limited%2520annotated%250Apathological%2520data%252C%2520modality%2520domain%2520gaps%252C%2520and%2520the%2520complexity%2520of%2520representing%250Adiffuse%2520pathologies%2520such%2520as%2520liver%2520cirrhosis.%2520Existing%2520methods%2520often%2520struggle%2520to%250Amaintain%2520anatomical%2520fidelity%2520while%2520accurately%2520modeling%2520pathological%2520features%252C%250Afrequently%2520relying%2520on%2520priors%2520derived%2520from%2520natural%2520images%2520or%2520inefficient%250Amulti-step%2520sampling.%2520In%2520this%2520work%252C%2520we%2520introduce%2520ViCTr%2520%2528Vital%2520Consistency%250ATransfer%2529%252C%2520a%2520novel%2520two-stage%2520framework%2520that%2520combines%2520a%2520rectified%2520flow%250Atrajectory%2520with%2520a%2520Tweedie-corrected%2520diffusion%2520process%2520to%2520achieve%2520high-fidelity%252C%250Apathology-aware%2520image%2520synthesis.%2520First%252C%2520we%2520pretrain%2520ViCTr%2520on%2520the%2520ATLAS-8k%250Adataset%2520using%2520Elastic%2520Weight%2520Consolidation%2520%2528EWC%2529%2520to%2520preserve%2520critical%250Aanatomical%2520structures.%2520We%2520then%2520fine-tune%2520the%2520model%2520adversarially%2520with%2520Low-Rank%250AAdaptation%2520%2528LoRA%2529%2520modules%2520for%2520precise%2520control%2520over%2520pathology%2520severity.%2520By%250Areformulating%2520Tweedie%2527s%2520formula%2520within%2520a%2520linear%2520trajectory%2520framework%252C%2520ViCTr%250Asupports%2520one-step%2520sampling%252C%2520reducing%2520inference%2520from%252050%2520steps%2520to%2520just%25204%252C%2520without%250Asacrificing%2520anatomical%2520realism.%2520We%2520evaluate%2520ViCTr%2520on%2520BTCV%2520%2528CT%2529%252C%2520AMOS%2520%2528MRI%2529%252C%2520and%250ACirrMRI600%252B%2520%2528cirrhosis%2529%2520datasets.%2520Results%2520demonstrate%2520state-of-the-art%250Aperformance%252C%2520achieving%2520a%2520Medical%2520Frechet%2520Inception%2520Distance%2520%2528MFID%2529%2520of%252017.01%2520for%250Acirrhosis%2520synthesis%252028%2525%2520lower%2520than%2520existing%2520approaches%2520and%2520improving%2520nnUNet%250Asegmentation%2520by%2520%252B3.8%2525%2520mDSC%2520when%2520used%2520for%2520data%2520augmentation.%2520Radiologist%2520reviews%250Aindicate%2520that%2520ViCTr-generated%2520liver%2520cirrhosis%2520MRIs%2520are%2520clinically%250Aindistinguishable%2520from%2520real%2520scans.%2520To%2520our%2520knowledge%252C%2520ViCTr%2520is%2520the%2520first%2520method%250Ato%2520provide%2520fine-grained%252C%2520pathology-aware%2520MRI%2520synthesis%2520with%2520graded%2520severity%250Acontrol%252C%2520closing%2520a%2520critical%2520gap%2520in%2520AI-driven%2520medical%2520imaging%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04963v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViCTr%3A%20Vital%20Consistency%20Transfer%20for%20Pathology%20Aware%20Image%20Synthesis&entry.906535625=Onkar%20Susladkar%20and%20Gayatri%20Deshmukh%20and%20Yalcin%20Tur%20and%20Gorkhem%20Durak%20and%20Ulas%20Bagci&entry.1292438233=%20%20Synthesizing%20medical%20images%20remains%20challenging%20due%20to%20limited%20annotated%0Apathological%20data%2C%20modality%20domain%20gaps%2C%20and%20the%20complexity%20of%20representing%0Adiffuse%20pathologies%20such%20as%20liver%20cirrhosis.%20Existing%20methods%20often%20struggle%20to%0Amaintain%20anatomical%20fidelity%20while%20accurately%20modeling%20pathological%20features%2C%0Afrequently%20relying%20on%20priors%20derived%20from%20natural%20images%20or%20inefficient%0Amulti-step%20sampling.%20In%20this%20work%2C%20we%20introduce%20ViCTr%20%28Vital%20Consistency%0ATransfer%29%2C%20a%20novel%20two-stage%20framework%20that%20combines%20a%20rectified%20flow%0Atrajectory%20with%20a%20Tweedie-corrected%20diffusion%20process%20to%20achieve%20high-fidelity%2C%0Apathology-aware%20image%20synthesis.%20First%2C%20we%20pretrain%20ViCTr%20on%20the%20ATLAS-8k%0Adataset%20using%20Elastic%20Weight%20Consolidation%20%28EWC%29%20to%20preserve%20critical%0Aanatomical%20structures.%20We%20then%20fine-tune%20the%20model%20adversarially%20with%20Low-Rank%0AAdaptation%20%28LoRA%29%20modules%20for%20precise%20control%20over%20pathology%20severity.%20By%0Areformulating%20Tweedie%27s%20formula%20within%20a%20linear%20trajectory%20framework%2C%20ViCTr%0Asupports%20one-step%20sampling%2C%20reducing%20inference%20from%2050%20steps%20to%20just%204%2C%20without%0Asacrificing%20anatomical%20realism.%20We%20evaluate%20ViCTr%20on%20BTCV%20%28CT%29%2C%20AMOS%20%28MRI%29%2C%20and%0ACirrMRI600%2B%20%28cirrhosis%29%20datasets.%20Results%20demonstrate%20state-of-the-art%0Aperformance%2C%20achieving%20a%20Medical%20Frechet%20Inception%20Distance%20%28MFID%29%20of%2017.01%20for%0Acirrhosis%20synthesis%2028%25%20lower%20than%20existing%20approaches%20and%20improving%20nnUNet%0Asegmentation%20by%20%2B3.8%25%20mDSC%20when%20used%20for%20data%20augmentation.%20Radiologist%20reviews%0Aindicate%20that%20ViCTr-generated%20liver%20cirrhosis%20MRIs%20are%20clinically%0Aindistinguishable%20from%20real%20scans.%20To%20our%20knowledge%2C%20ViCTr%20is%20the%20first%20method%0Ato%20provide%20fine-grained%2C%20pathology-aware%20MRI%20synthesis%20with%20graded%20severity%0Acontrol%2C%20closing%20a%20critical%20gap%20in%20AI-driven%20medical%20imaging%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04963v2&entry.124074799=Read"},
{"title": "DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations", "author": "Ziqiao Peng and Yanbo Fan and Haoyu Wu and Xuan Wang and Hongyan Liu and Jun He and Zhaoxin Fan", "abstract": "  In face-to-face conversations, individuals need to switch between speaking\nand listening roles seamlessly. Existing 3D talking head generation models\nfocus solely on speaking or listening, neglecting the natural dynamics of\ninteractive conversation, which leads to unnatural interactions and awkward\ntransitions. To address this issue, we propose a new task -- multi-round\ndual-speaker interaction for 3D talking head generation -- which requires\nmodels to handle and generate both speaking and listening behaviors in\ncontinuous conversation. To solve this task, we introduce DualTalk, a novel\nunified framework that integrates the dynamic behaviors of speakers and\nlisteners to simulate realistic and coherent dialogue interactions. This\nframework not only synthesizes lifelike talking heads when speaking but also\ngenerates continuous and vivid non-verbal feedback when listening, effectively\ncapturing the interplay between the roles. We also create a new dataset\nfeaturing 50 hours of multi-round conversations with over 1,000 characters,\nwhere participants continuously switch between speaking and listening roles.\nExtensive experiments demonstrate that our method significantly enhances the\nnaturalness and expressiveness of 3D talking heads in dual-speaker\nconversations. We recommend watching the supplementary video:\nhttps://ziqiaopeng.github.io/dualtalk.\n", "link": "http://arxiv.org/abs/2505.18096v1", "date": "2025-05-23", "relevancy": 2.788, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.587}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5493}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualTalk%3A%20Dual-Speaker%20Interaction%20for%203D%20Talking%20Head%20Conversations&body=Title%3A%20DualTalk%3A%20Dual-Speaker%20Interaction%20for%203D%20Talking%20Head%20Conversations%0AAuthor%3A%20Ziqiao%20Peng%20and%20Yanbo%20Fan%20and%20Haoyu%20Wu%20and%20Xuan%20Wang%20and%20Hongyan%20Liu%20and%20Jun%20He%20and%20Zhaoxin%20Fan%0AAbstract%3A%20%20%20In%20face-to-face%20conversations%2C%20individuals%20need%20to%20switch%20between%20speaking%0Aand%20listening%20roles%20seamlessly.%20Existing%203D%20talking%20head%20generation%20models%0Afocus%20solely%20on%20speaking%20or%20listening%2C%20neglecting%20the%20natural%20dynamics%20of%0Ainteractive%20conversation%2C%20which%20leads%20to%20unnatural%20interactions%20and%20awkward%0Atransitions.%20To%20address%20this%20issue%2C%20we%20propose%20a%20new%20task%20--%20multi-round%0Adual-speaker%20interaction%20for%203D%20talking%20head%20generation%20--%20which%20requires%0Amodels%20to%20handle%20and%20generate%20both%20speaking%20and%20listening%20behaviors%20in%0Acontinuous%20conversation.%20To%20solve%20this%20task%2C%20we%20introduce%20DualTalk%2C%20a%20novel%0Aunified%20framework%20that%20integrates%20the%20dynamic%20behaviors%20of%20speakers%20and%0Alisteners%20to%20simulate%20realistic%20and%20coherent%20dialogue%20interactions.%20This%0Aframework%20not%20only%20synthesizes%20lifelike%20talking%20heads%20when%20speaking%20but%20also%0Agenerates%20continuous%20and%20vivid%20non-verbal%20feedback%20when%20listening%2C%20effectively%0Acapturing%20the%20interplay%20between%20the%20roles.%20We%20also%20create%20a%20new%20dataset%0Afeaturing%2050%20hours%20of%20multi-round%20conversations%20with%20over%201%2C000%20characters%2C%0Awhere%20participants%20continuously%20switch%20between%20speaking%20and%20listening%20roles.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20significantly%20enhances%20the%0Anaturalness%20and%20expressiveness%20of%203D%20talking%20heads%20in%20dual-speaker%0Aconversations.%20We%20recommend%20watching%20the%20supplementary%20video%3A%0Ahttps%3A//ziqiaopeng.github.io/dualtalk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualTalk%253A%2520Dual-Speaker%2520Interaction%2520for%25203D%2520Talking%2520Head%2520Conversations%26entry.906535625%3DZiqiao%2520Peng%2520and%2520Yanbo%2520Fan%2520and%2520Haoyu%2520Wu%2520and%2520Xuan%2520Wang%2520and%2520Hongyan%2520Liu%2520and%2520Jun%2520He%2520and%2520Zhaoxin%2520Fan%26entry.1292438233%3D%2520%2520In%2520face-to-face%2520conversations%252C%2520individuals%2520need%2520to%2520switch%2520between%2520speaking%250Aand%2520listening%2520roles%2520seamlessly.%2520Existing%25203D%2520talking%2520head%2520generation%2520models%250Afocus%2520solely%2520on%2520speaking%2520or%2520listening%252C%2520neglecting%2520the%2520natural%2520dynamics%2520of%250Ainteractive%2520conversation%252C%2520which%2520leads%2520to%2520unnatural%2520interactions%2520and%2520awkward%250Atransitions.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520new%2520task%2520--%2520multi-round%250Adual-speaker%2520interaction%2520for%25203D%2520talking%2520head%2520generation%2520--%2520which%2520requires%250Amodels%2520to%2520handle%2520and%2520generate%2520both%2520speaking%2520and%2520listening%2520behaviors%2520in%250Acontinuous%2520conversation.%2520To%2520solve%2520this%2520task%252C%2520we%2520introduce%2520DualTalk%252C%2520a%2520novel%250Aunified%2520framework%2520that%2520integrates%2520the%2520dynamic%2520behaviors%2520of%2520speakers%2520and%250Alisteners%2520to%2520simulate%2520realistic%2520and%2520coherent%2520dialogue%2520interactions.%2520This%250Aframework%2520not%2520only%2520synthesizes%2520lifelike%2520talking%2520heads%2520when%2520speaking%2520but%2520also%250Agenerates%2520continuous%2520and%2520vivid%2520non-verbal%2520feedback%2520when%2520listening%252C%2520effectively%250Acapturing%2520the%2520interplay%2520between%2520the%2520roles.%2520We%2520also%2520create%2520a%2520new%2520dataset%250Afeaturing%252050%2520hours%2520of%2520multi-round%2520conversations%2520with%2520over%25201%252C000%2520characters%252C%250Awhere%2520participants%2520continuously%2520switch%2520between%2520speaking%2520and%2520listening%2520roles.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520significantly%2520enhances%2520the%250Anaturalness%2520and%2520expressiveness%2520of%25203D%2520talking%2520heads%2520in%2520dual-speaker%250Aconversations.%2520We%2520recommend%2520watching%2520the%2520supplementary%2520video%253A%250Ahttps%253A//ziqiaopeng.github.io/dualtalk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualTalk%3A%20Dual-Speaker%20Interaction%20for%203D%20Talking%20Head%20Conversations&entry.906535625=Ziqiao%20Peng%20and%20Yanbo%20Fan%20and%20Haoyu%20Wu%20and%20Xuan%20Wang%20and%20Hongyan%20Liu%20and%20Jun%20He%20and%20Zhaoxin%20Fan&entry.1292438233=%20%20In%20face-to-face%20conversations%2C%20individuals%20need%20to%20switch%20between%20speaking%0Aand%20listening%20roles%20seamlessly.%20Existing%203D%20talking%20head%20generation%20models%0Afocus%20solely%20on%20speaking%20or%20listening%2C%20neglecting%20the%20natural%20dynamics%20of%0Ainteractive%20conversation%2C%20which%20leads%20to%20unnatural%20interactions%20and%20awkward%0Atransitions.%20To%20address%20this%20issue%2C%20we%20propose%20a%20new%20task%20--%20multi-round%0Adual-speaker%20interaction%20for%203D%20talking%20head%20generation%20--%20which%20requires%0Amodels%20to%20handle%20and%20generate%20both%20speaking%20and%20listening%20behaviors%20in%0Acontinuous%20conversation.%20To%20solve%20this%20task%2C%20we%20introduce%20DualTalk%2C%20a%20novel%0Aunified%20framework%20that%20integrates%20the%20dynamic%20behaviors%20of%20speakers%20and%0Alisteners%20to%20simulate%20realistic%20and%20coherent%20dialogue%20interactions.%20This%0Aframework%20not%20only%20synthesizes%20lifelike%20talking%20heads%20when%20speaking%20but%20also%0Agenerates%20continuous%20and%20vivid%20non-verbal%20feedback%20when%20listening%2C%20effectively%0Acapturing%20the%20interplay%20between%20the%20roles.%20We%20also%20create%20a%20new%20dataset%0Afeaturing%2050%20hours%20of%20multi-round%20conversations%20with%20over%201%2C000%20characters%2C%0Awhere%20participants%20continuously%20switch%20between%20speaking%20and%20listening%20roles.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20significantly%20enhances%20the%0Anaturalness%20and%20expressiveness%20of%203D%20talking%20heads%20in%20dual-speaker%0Aconversations.%20We%20recommend%20watching%20the%20supplementary%20video%3A%0Ahttps%3A//ziqiaopeng.github.io/dualtalk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18096v1&entry.124074799=Read"},
{"title": "REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders", "author": "Savya Khosla and Sethuraman TV and Barnett Lee and Alexander Schwing and Derek Hoiem", "abstract": "  We introduce the Region Encoder Network (REN), a fast and effective model for\ngenerating region-based image representations using point prompts. Recent\nmethods combine class-agnostic segmenters (e.g., SAM) with patch-based image\nencoders (e.g., DINO) to produce compact and effective region representations,\nbut they suffer from high computational cost due to the segmentation step. REN\nbypasses this bottleneck using a lightweight module that directly generates\nregion tokens, enabling 60x faster token generation with 35x less memory, while\nalso improving token quality. It uses a few cross-attention blocks that take\npoint prompts as queries and features from a patch-based image encoder as keys\nand values to produce region tokens that correspond to the prompted objects. We\ntrain REN with three popular encoders-DINO, DINOv2, and OpenCLIP-and show that\nit can be extended to other encoders without dedicated training. We evaluate\nREN on semantic segmentation and retrieval tasks, where it consistently\noutperforms the original encoders in both performance and compactness, and\nmatches or exceeds SAM-based region methods while being significantly faster.\nNotably, REN achieves state-of-the-art results on the challenging Ego4D VQ2D\nbenchmark and outperforms proprietary LMMs on Visual Haystacks' single-needle\nchallenge. Code and models are available at: https://github.com/savya08/REN.\n", "link": "http://arxiv.org/abs/2505.18153v1", "date": "2025-05-23", "relevancy": 2.7508, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REN%3A%20Fast%20and%20Efficient%20Region%20Encodings%20from%20Patch-Based%20Image%20Encoders&body=Title%3A%20REN%3A%20Fast%20and%20Efficient%20Region%20Encodings%20from%20Patch-Based%20Image%20Encoders%0AAuthor%3A%20Savya%20Khosla%20and%20Sethuraman%20TV%20and%20Barnett%20Lee%20and%20Alexander%20Schwing%20and%20Derek%20Hoiem%0AAbstract%3A%20%20%20We%20introduce%20the%20Region%20Encoder%20Network%20%28REN%29%2C%20a%20fast%20and%20effective%20model%20for%0Agenerating%20region-based%20image%20representations%20using%20point%20prompts.%20Recent%0Amethods%20combine%20class-agnostic%20segmenters%20%28e.g.%2C%20SAM%29%20with%20patch-based%20image%0Aencoders%20%28e.g.%2C%20DINO%29%20to%20produce%20compact%20and%20effective%20region%20representations%2C%0Abut%20they%20suffer%20from%20high%20computational%20cost%20due%20to%20the%20segmentation%20step.%20REN%0Abypasses%20this%20bottleneck%20using%20a%20lightweight%20module%20that%20directly%20generates%0Aregion%20tokens%2C%20enabling%2060x%20faster%20token%20generation%20with%2035x%20less%20memory%2C%20while%0Aalso%20improving%20token%20quality.%20It%20uses%20a%20few%20cross-attention%20blocks%20that%20take%0Apoint%20prompts%20as%20queries%20and%20features%20from%20a%20patch-based%20image%20encoder%20as%20keys%0Aand%20values%20to%20produce%20region%20tokens%20that%20correspond%20to%20the%20prompted%20objects.%20We%0Atrain%20REN%20with%20three%20popular%20encoders-DINO%2C%20DINOv2%2C%20and%20OpenCLIP-and%20show%20that%0Ait%20can%20be%20extended%20to%20other%20encoders%20without%20dedicated%20training.%20We%20evaluate%0AREN%20on%20semantic%20segmentation%20and%20retrieval%20tasks%2C%20where%20it%20consistently%0Aoutperforms%20the%20original%20encoders%20in%20both%20performance%20and%20compactness%2C%20and%0Amatches%20or%20exceeds%20SAM-based%20region%20methods%20while%20being%20significantly%20faster.%0ANotably%2C%20REN%20achieves%20state-of-the-art%20results%20on%20the%20challenging%20Ego4D%20VQ2D%0Abenchmark%20and%20outperforms%20proprietary%20LMMs%20on%20Visual%20Haystacks%27%20single-needle%0Achallenge.%20Code%20and%20models%20are%20available%20at%3A%20https%3A//github.com/savya08/REN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREN%253A%2520Fast%2520and%2520Efficient%2520Region%2520Encodings%2520from%2520Patch-Based%2520Image%2520Encoders%26entry.906535625%3DSavya%2520Khosla%2520and%2520Sethuraman%2520TV%2520and%2520Barnett%2520Lee%2520and%2520Alexander%2520Schwing%2520and%2520Derek%2520Hoiem%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Region%2520Encoder%2520Network%2520%2528REN%2529%252C%2520a%2520fast%2520and%2520effective%2520model%2520for%250Agenerating%2520region-based%2520image%2520representations%2520using%2520point%2520prompts.%2520Recent%250Amethods%2520combine%2520class-agnostic%2520segmenters%2520%2528e.g.%252C%2520SAM%2529%2520with%2520patch-based%2520image%250Aencoders%2520%2528e.g.%252C%2520DINO%2529%2520to%2520produce%2520compact%2520and%2520effective%2520region%2520representations%252C%250Abut%2520they%2520suffer%2520from%2520high%2520computational%2520cost%2520due%2520to%2520the%2520segmentation%2520step.%2520REN%250Abypasses%2520this%2520bottleneck%2520using%2520a%2520lightweight%2520module%2520that%2520directly%2520generates%250Aregion%2520tokens%252C%2520enabling%252060x%2520faster%2520token%2520generation%2520with%252035x%2520less%2520memory%252C%2520while%250Aalso%2520improving%2520token%2520quality.%2520It%2520uses%2520a%2520few%2520cross-attention%2520blocks%2520that%2520take%250Apoint%2520prompts%2520as%2520queries%2520and%2520features%2520from%2520a%2520patch-based%2520image%2520encoder%2520as%2520keys%250Aand%2520values%2520to%2520produce%2520region%2520tokens%2520that%2520correspond%2520to%2520the%2520prompted%2520objects.%2520We%250Atrain%2520REN%2520with%2520three%2520popular%2520encoders-DINO%252C%2520DINOv2%252C%2520and%2520OpenCLIP-and%2520show%2520that%250Ait%2520can%2520be%2520extended%2520to%2520other%2520encoders%2520without%2520dedicated%2520training.%2520We%2520evaluate%250AREN%2520on%2520semantic%2520segmentation%2520and%2520retrieval%2520tasks%252C%2520where%2520it%2520consistently%250Aoutperforms%2520the%2520original%2520encoders%2520in%2520both%2520performance%2520and%2520compactness%252C%2520and%250Amatches%2520or%2520exceeds%2520SAM-based%2520region%2520methods%2520while%2520being%2520significantly%2520faster.%250ANotably%252C%2520REN%2520achieves%2520state-of-the-art%2520results%2520on%2520the%2520challenging%2520Ego4D%2520VQ2D%250Abenchmark%2520and%2520outperforms%2520proprietary%2520LMMs%2520on%2520Visual%2520Haystacks%2527%2520single-needle%250Achallenge.%2520Code%2520and%2520models%2520are%2520available%2520at%253A%2520https%253A//github.com/savya08/REN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REN%3A%20Fast%20and%20Efficient%20Region%20Encodings%20from%20Patch-Based%20Image%20Encoders&entry.906535625=Savya%20Khosla%20and%20Sethuraman%20TV%20and%20Barnett%20Lee%20and%20Alexander%20Schwing%20and%20Derek%20Hoiem&entry.1292438233=%20%20We%20introduce%20the%20Region%20Encoder%20Network%20%28REN%29%2C%20a%20fast%20and%20effective%20model%20for%0Agenerating%20region-based%20image%20representations%20using%20point%20prompts.%20Recent%0Amethods%20combine%20class-agnostic%20segmenters%20%28e.g.%2C%20SAM%29%20with%20patch-based%20image%0Aencoders%20%28e.g.%2C%20DINO%29%20to%20produce%20compact%20and%20effective%20region%20representations%2C%0Abut%20they%20suffer%20from%20high%20computational%20cost%20due%20to%20the%20segmentation%20step.%20REN%0Abypasses%20this%20bottleneck%20using%20a%20lightweight%20module%20that%20directly%20generates%0Aregion%20tokens%2C%20enabling%2060x%20faster%20token%20generation%20with%2035x%20less%20memory%2C%20while%0Aalso%20improving%20token%20quality.%20It%20uses%20a%20few%20cross-attention%20blocks%20that%20take%0Apoint%20prompts%20as%20queries%20and%20features%20from%20a%20patch-based%20image%20encoder%20as%20keys%0Aand%20values%20to%20produce%20region%20tokens%20that%20correspond%20to%20the%20prompted%20objects.%20We%0Atrain%20REN%20with%20three%20popular%20encoders-DINO%2C%20DINOv2%2C%20and%20OpenCLIP-and%20show%20that%0Ait%20can%20be%20extended%20to%20other%20encoders%20without%20dedicated%20training.%20We%20evaluate%0AREN%20on%20semantic%20segmentation%20and%20retrieval%20tasks%2C%20where%20it%20consistently%0Aoutperforms%20the%20original%20encoders%20in%20both%20performance%20and%20compactness%2C%20and%0Amatches%20or%20exceeds%20SAM-based%20region%20methods%20while%20being%20significantly%20faster.%0ANotably%2C%20REN%20achieves%20state-of-the-art%20results%20on%20the%20challenging%20Ego4D%20VQ2D%0Abenchmark%20and%20outperforms%20proprietary%20LMMs%20on%20Visual%20Haystacks%27%20single-needle%0Achallenge.%20Code%20and%20models%20are%20available%20at%3A%20https%3A//github.com/savya08/REN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18153v1&entry.124074799=Read"},
{"title": "ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for\n  Zero-Shot Anomaly Detection", "author": "Ziteng Yang and Jingzehua Xu and Yanshu Li and Zepeng Li and Yeqiang Wang and Xinghui Li", "abstract": "  Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any\ntarget domain training samples, relying solely on external auxiliary data.\nExisting CLIP-based methods attempt to activate the model's ZSAD potential via\nhandcrafted or static learnable prompts. The former incur high engineering\ncosts and limited semantic coverage, whereas the latter apply identical\ndescriptions across diverse anomaly types, thus fail to adapt to complex\nvariations. Furthermore, since CLIP is originally pretrained on large-scale\nclassification tasks, its anomaly segmentation quality is highly sensitive to\nthe exact wording of class names, severely constraining prompting strategies\nthat depend on class labels. To address these challenges, we introduce\nViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception\nPrompting (ViP-Prompt) mechanism, which fuses global and multi-scale local\nvisual context to adaptively generate fine-grained textual prompts, eliminating\nmanual templates and class-name priors. This design enables our model to focus\non precise abnormal regions, making it particularly valuable when category\nlabels are ambiguous or privacy-constrained. Extensive experiments on 15\nindustrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves\nstate-of-the-art performance and robust cross-domain generalization.\n", "link": "http://arxiv.org/abs/2505.17692v1", "date": "2025-05-23", "relevancy": 2.749, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViP%24%5E2%24-CLIP%3A%20Visual-Perception%20Prompting%20with%20Unified%20Alignment%20for%0A%20%20Zero-Shot%20Anomaly%20Detection&body=Title%3A%20ViP%24%5E2%24-CLIP%3A%20Visual-Perception%20Prompting%20with%20Unified%20Alignment%20for%0A%20%20Zero-Shot%20Anomaly%20Detection%0AAuthor%3A%20Ziteng%20Yang%20and%20Jingzehua%20Xu%20and%20Yanshu%20Li%20and%20Zepeng%20Li%20and%20Yeqiang%20Wang%20and%20Xinghui%20Li%0AAbstract%3A%20%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20aims%20to%20detect%20anomalies%20without%20any%0Atarget%20domain%20training%20samples%2C%20relying%20solely%20on%20external%20auxiliary%20data.%0AExisting%20CLIP-based%20methods%20attempt%20to%20activate%20the%20model%27s%20ZSAD%20potential%20via%0Ahandcrafted%20or%20static%20learnable%20prompts.%20The%20former%20incur%20high%20engineering%0Acosts%20and%20limited%20semantic%20coverage%2C%20whereas%20the%20latter%20apply%20identical%0Adescriptions%20across%20diverse%20anomaly%20types%2C%20thus%20fail%20to%20adapt%20to%20complex%0Avariations.%20Furthermore%2C%20since%20CLIP%20is%20originally%20pretrained%20on%20large-scale%0Aclassification%20tasks%2C%20its%20anomaly%20segmentation%20quality%20is%20highly%20sensitive%20to%0Athe%20exact%20wording%20of%20class%20names%2C%20severely%20constraining%20prompting%20strategies%0Athat%20depend%20on%20class%20labels.%20To%20address%20these%20challenges%2C%20we%20introduce%0AViP%24%5E%7B2%7D%24-CLIP.%20The%20key%20insight%20of%20ViP%24%5E%7B2%7D%24-CLIP%20is%20a%20Visual-Perception%0APrompting%20%28ViP-Prompt%29%20mechanism%2C%20which%20fuses%20global%20and%20multi-scale%20local%0Avisual%20context%20to%20adaptively%20generate%20fine-grained%20textual%20prompts%2C%20eliminating%0Amanual%20templates%20and%20class-name%20priors.%20This%20design%20enables%20our%20model%20to%20focus%0Aon%20precise%20abnormal%20regions%2C%20making%20it%20particularly%20valuable%20when%20category%0Alabels%20are%20ambiguous%20or%20privacy-constrained.%20Extensive%20experiments%20on%2015%0Aindustrial%20and%20medical%20benchmarks%20demonstrate%20that%20ViP%24%5E%7B2%7D%24-CLIP%20achieves%0Astate-of-the-art%20performance%20and%20robust%20cross-domain%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViP%2524%255E2%2524-CLIP%253A%2520Visual-Perception%2520Prompting%2520with%2520Unified%2520Alignment%2520for%250A%2520%2520Zero-Shot%2520Anomaly%2520Detection%26entry.906535625%3DZiteng%2520Yang%2520and%2520Jingzehua%2520Xu%2520and%2520Yanshu%2520Li%2520and%2520Zepeng%2520Li%2520and%2520Yeqiang%2520Wang%2520and%2520Xinghui%2520Li%26entry.1292438233%3D%2520%2520Zero-shot%2520anomaly%2520detection%2520%2528ZSAD%2529%2520aims%2520to%2520detect%2520anomalies%2520without%2520any%250Atarget%2520domain%2520training%2520samples%252C%2520relying%2520solely%2520on%2520external%2520auxiliary%2520data.%250AExisting%2520CLIP-based%2520methods%2520attempt%2520to%2520activate%2520the%2520model%2527s%2520ZSAD%2520potential%2520via%250Ahandcrafted%2520or%2520static%2520learnable%2520prompts.%2520The%2520former%2520incur%2520high%2520engineering%250Acosts%2520and%2520limited%2520semantic%2520coverage%252C%2520whereas%2520the%2520latter%2520apply%2520identical%250Adescriptions%2520across%2520diverse%2520anomaly%2520types%252C%2520thus%2520fail%2520to%2520adapt%2520to%2520complex%250Avariations.%2520Furthermore%252C%2520since%2520CLIP%2520is%2520originally%2520pretrained%2520on%2520large-scale%250Aclassification%2520tasks%252C%2520its%2520anomaly%2520segmentation%2520quality%2520is%2520highly%2520sensitive%2520to%250Athe%2520exact%2520wording%2520of%2520class%2520names%252C%2520severely%2520constraining%2520prompting%2520strategies%250Athat%2520depend%2520on%2520class%2520labels.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250AViP%2524%255E%257B2%257D%2524-CLIP.%2520The%2520key%2520insight%2520of%2520ViP%2524%255E%257B2%257D%2524-CLIP%2520is%2520a%2520Visual-Perception%250APrompting%2520%2528ViP-Prompt%2529%2520mechanism%252C%2520which%2520fuses%2520global%2520and%2520multi-scale%2520local%250Avisual%2520context%2520to%2520adaptively%2520generate%2520fine-grained%2520textual%2520prompts%252C%2520eliminating%250Amanual%2520templates%2520and%2520class-name%2520priors.%2520This%2520design%2520enables%2520our%2520model%2520to%2520focus%250Aon%2520precise%2520abnormal%2520regions%252C%2520making%2520it%2520particularly%2520valuable%2520when%2520category%250Alabels%2520are%2520ambiguous%2520or%2520privacy-constrained.%2520Extensive%2520experiments%2520on%252015%250Aindustrial%2520and%2520medical%2520benchmarks%2520demonstrate%2520that%2520ViP%2524%255E%257B2%257D%2524-CLIP%2520achieves%250Astate-of-the-art%2520performance%2520and%2520robust%2520cross-domain%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViP%24%5E2%24-CLIP%3A%20Visual-Perception%20Prompting%20with%20Unified%20Alignment%20for%0A%20%20Zero-Shot%20Anomaly%20Detection&entry.906535625=Ziteng%20Yang%20and%20Jingzehua%20Xu%20and%20Yanshu%20Li%20and%20Zepeng%20Li%20and%20Yeqiang%20Wang%20and%20Xinghui%20Li&entry.1292438233=%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20aims%20to%20detect%20anomalies%20without%20any%0Atarget%20domain%20training%20samples%2C%20relying%20solely%20on%20external%20auxiliary%20data.%0AExisting%20CLIP-based%20methods%20attempt%20to%20activate%20the%20model%27s%20ZSAD%20potential%20via%0Ahandcrafted%20or%20static%20learnable%20prompts.%20The%20former%20incur%20high%20engineering%0Acosts%20and%20limited%20semantic%20coverage%2C%20whereas%20the%20latter%20apply%20identical%0Adescriptions%20across%20diverse%20anomaly%20types%2C%20thus%20fail%20to%20adapt%20to%20complex%0Avariations.%20Furthermore%2C%20since%20CLIP%20is%20originally%20pretrained%20on%20large-scale%0Aclassification%20tasks%2C%20its%20anomaly%20segmentation%20quality%20is%20highly%20sensitive%20to%0Athe%20exact%20wording%20of%20class%20names%2C%20severely%20constraining%20prompting%20strategies%0Athat%20depend%20on%20class%20labels.%20To%20address%20these%20challenges%2C%20we%20introduce%0AViP%24%5E%7B2%7D%24-CLIP.%20The%20key%20insight%20of%20ViP%24%5E%7B2%7D%24-CLIP%20is%20a%20Visual-Perception%0APrompting%20%28ViP-Prompt%29%20mechanism%2C%20which%20fuses%20global%20and%20multi-scale%20local%0Avisual%20context%20to%20adaptively%20generate%20fine-grained%20textual%20prompts%2C%20eliminating%0Amanual%20templates%20and%20class-name%20priors.%20This%20design%20enables%20our%20model%20to%20focus%0Aon%20precise%20abnormal%20regions%2C%20making%20it%20particularly%20valuable%20when%20category%0Alabels%20are%20ambiguous%20or%20privacy-constrained.%20Extensive%20experiments%20on%2015%0Aindustrial%20and%20medical%20benchmarks%20demonstrate%20that%20ViP%24%5E%7B2%7D%24-CLIP%20achieves%0Astate-of-the-art%20performance%20and%20robust%20cross-domain%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17692v1&entry.124074799=Read"},
{"title": "Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM", "author": "Donghwan Chi and Hyomin Kim and Yoonjin Oh and Yongjin Kim and Donghoon Lee and Daejin Jo and Jongmin Kim and Junyeob Baek and Sungjin Ahn and Sungwoong Kim", "abstract": "  Recently, multimodal large language models (MLLMs) have emerged as a key\napproach in achieving artificial general intelligence. In particular,\nvision-language MLLMs have been developed to generate not only text but also\nvisual outputs from multimodal inputs. This advancement requires efficient\nimage tokens that LLMs can process effectively both in input and output.\nHowever, existing image tokenization methods for MLLMs typically capture only\nglobal abstract concepts or uniformly segmented image patches, restricting\nMLLMs' capability to effectively understand or generate detailed visual\ncontent, particularly at the object level. To address this limitation, we\npropose an object-centric visual tokenizer based on Slot Attention specifically\nfor MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and\nresidual vector quantization, our proposed discretized slot tokens can encode\nlocal visual details while maintaining high-level semantics, and also align\nwith textual data to be integrated seamlessly within a unified next-token\nprediction framework of LLMs. The resulting Slot-MLLM demonstrates significant\nperformance improvements over baselines with previous visual tokenizers across\nvarious vision-language tasks that entail local detailed comprehension and\ngeneration. Notably, this work is the first demonstration of the feasibility of\nobject-centric slot attention performed with MLLMs and in-the-wild natural\nimages.\n", "link": "http://arxiv.org/abs/2505.17726v1", "date": "2025-05-23", "relevancy": 2.7384, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5679}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5413}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Slot-MLLM%3A%20Object-Centric%20Visual%20Tokenization%20for%20Multimodal%20LLM&body=Title%3A%20Slot-MLLM%3A%20Object-Centric%20Visual%20Tokenization%20for%20Multimodal%20LLM%0AAuthor%3A%20Donghwan%20Chi%20and%20Hyomin%20Kim%20and%20Yoonjin%20Oh%20and%20Yongjin%20Kim%20and%20Donghoon%20Lee%20and%20Daejin%20Jo%20and%20Jongmin%20Kim%20and%20Junyeob%20Baek%20and%20Sungjin%20Ahn%20and%20Sungwoong%20Kim%0AAbstract%3A%20%20%20Recently%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20emerged%20as%20a%20key%0Aapproach%20in%20achieving%20artificial%20general%20intelligence.%20In%20particular%2C%0Avision-language%20MLLMs%20have%20been%20developed%20to%20generate%20not%20only%20text%20but%20also%0Avisual%20outputs%20from%20multimodal%20inputs.%20This%20advancement%20requires%20efficient%0Aimage%20tokens%20that%20LLMs%20can%20process%20effectively%20both%20in%20input%20and%20output.%0AHowever%2C%20existing%20image%20tokenization%20methods%20for%20MLLMs%20typically%20capture%20only%0Aglobal%20abstract%20concepts%20or%20uniformly%20segmented%20image%20patches%2C%20restricting%0AMLLMs%27%20capability%20to%20effectively%20understand%20or%20generate%20detailed%20visual%0Acontent%2C%20particularly%20at%20the%20object%20level.%20To%20address%20this%20limitation%2C%20we%0Apropose%20an%20object-centric%20visual%20tokenizer%20based%20on%20Slot%20Attention%20specifically%0Afor%20MLLMs.%20In%20particular%2C%20based%20on%20the%20Q-Former%20encoder%2C%20diffusion%20decoder%2C%20and%0Aresidual%20vector%20quantization%2C%20our%20proposed%20discretized%20slot%20tokens%20can%20encode%0Alocal%20visual%20details%20while%20maintaining%20high-level%20semantics%2C%20and%20also%20align%0Awith%20textual%20data%20to%20be%20integrated%20seamlessly%20within%20a%20unified%20next-token%0Aprediction%20framework%20of%20LLMs.%20The%20resulting%20Slot-MLLM%20demonstrates%20significant%0Aperformance%20improvements%20over%20baselines%20with%20previous%20visual%20tokenizers%20across%0Avarious%20vision-language%20tasks%20that%20entail%20local%20detailed%20comprehension%20and%0Ageneration.%20Notably%2C%20this%20work%20is%20the%20first%20demonstration%20of%20the%20feasibility%20of%0Aobject-centric%20slot%20attention%20performed%20with%20MLLMs%20and%20in-the-wild%20natural%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlot-MLLM%253A%2520Object-Centric%2520Visual%2520Tokenization%2520for%2520Multimodal%2520LLM%26entry.906535625%3DDonghwan%2520Chi%2520and%2520Hyomin%2520Kim%2520and%2520Yoonjin%2520Oh%2520and%2520Yongjin%2520Kim%2520and%2520Donghoon%2520Lee%2520and%2520Daejin%2520Jo%2520and%2520Jongmin%2520Kim%2520and%2520Junyeob%2520Baek%2520and%2520Sungjin%2520Ahn%2520and%2520Sungwoong%2520Kim%26entry.1292438233%3D%2520%2520Recently%252C%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520emerged%2520as%2520a%2520key%250Aapproach%2520in%2520achieving%2520artificial%2520general%2520intelligence.%2520In%2520particular%252C%250Avision-language%2520MLLMs%2520have%2520been%2520developed%2520to%2520generate%2520not%2520only%2520text%2520but%2520also%250Avisual%2520outputs%2520from%2520multimodal%2520inputs.%2520This%2520advancement%2520requires%2520efficient%250Aimage%2520tokens%2520that%2520LLMs%2520can%2520process%2520effectively%2520both%2520in%2520input%2520and%2520output.%250AHowever%252C%2520existing%2520image%2520tokenization%2520methods%2520for%2520MLLMs%2520typically%2520capture%2520only%250Aglobal%2520abstract%2520concepts%2520or%2520uniformly%2520segmented%2520image%2520patches%252C%2520restricting%250AMLLMs%2527%2520capability%2520to%2520effectively%2520understand%2520or%2520generate%2520detailed%2520visual%250Acontent%252C%2520particularly%2520at%2520the%2520object%2520level.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apropose%2520an%2520object-centric%2520visual%2520tokenizer%2520based%2520on%2520Slot%2520Attention%2520specifically%250Afor%2520MLLMs.%2520In%2520particular%252C%2520based%2520on%2520the%2520Q-Former%2520encoder%252C%2520diffusion%2520decoder%252C%2520and%250Aresidual%2520vector%2520quantization%252C%2520our%2520proposed%2520discretized%2520slot%2520tokens%2520can%2520encode%250Alocal%2520visual%2520details%2520while%2520maintaining%2520high-level%2520semantics%252C%2520and%2520also%2520align%250Awith%2520textual%2520data%2520to%2520be%2520integrated%2520seamlessly%2520within%2520a%2520unified%2520next-token%250Aprediction%2520framework%2520of%2520LLMs.%2520The%2520resulting%2520Slot-MLLM%2520demonstrates%2520significant%250Aperformance%2520improvements%2520over%2520baselines%2520with%2520previous%2520visual%2520tokenizers%2520across%250Avarious%2520vision-language%2520tasks%2520that%2520entail%2520local%2520detailed%2520comprehension%2520and%250Ageneration.%2520Notably%252C%2520this%2520work%2520is%2520the%2520first%2520demonstration%2520of%2520the%2520feasibility%2520of%250Aobject-centric%2520slot%2520attention%2520performed%2520with%2520MLLMs%2520and%2520in-the-wild%2520natural%250Aimages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Slot-MLLM%3A%20Object-Centric%20Visual%20Tokenization%20for%20Multimodal%20LLM&entry.906535625=Donghwan%20Chi%20and%20Hyomin%20Kim%20and%20Yoonjin%20Oh%20and%20Yongjin%20Kim%20and%20Donghoon%20Lee%20and%20Daejin%20Jo%20and%20Jongmin%20Kim%20and%20Junyeob%20Baek%20and%20Sungjin%20Ahn%20and%20Sungwoong%20Kim&entry.1292438233=%20%20Recently%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20emerged%20as%20a%20key%0Aapproach%20in%20achieving%20artificial%20general%20intelligence.%20In%20particular%2C%0Avision-language%20MLLMs%20have%20been%20developed%20to%20generate%20not%20only%20text%20but%20also%0Avisual%20outputs%20from%20multimodal%20inputs.%20This%20advancement%20requires%20efficient%0Aimage%20tokens%20that%20LLMs%20can%20process%20effectively%20both%20in%20input%20and%20output.%0AHowever%2C%20existing%20image%20tokenization%20methods%20for%20MLLMs%20typically%20capture%20only%0Aglobal%20abstract%20concepts%20or%20uniformly%20segmented%20image%20patches%2C%20restricting%0AMLLMs%27%20capability%20to%20effectively%20understand%20or%20generate%20detailed%20visual%0Acontent%2C%20particularly%20at%20the%20object%20level.%20To%20address%20this%20limitation%2C%20we%0Apropose%20an%20object-centric%20visual%20tokenizer%20based%20on%20Slot%20Attention%20specifically%0Afor%20MLLMs.%20In%20particular%2C%20based%20on%20the%20Q-Former%20encoder%2C%20diffusion%20decoder%2C%20and%0Aresidual%20vector%20quantization%2C%20our%20proposed%20discretized%20slot%20tokens%20can%20encode%0Alocal%20visual%20details%20while%20maintaining%20high-level%20semantics%2C%20and%20also%20align%0Awith%20textual%20data%20to%20be%20integrated%20seamlessly%20within%20a%20unified%20next-token%0Aprediction%20framework%20of%20LLMs.%20The%20resulting%20Slot-MLLM%20demonstrates%20significant%0Aperformance%20improvements%20over%20baselines%20with%20previous%20visual%20tokenizers%20across%0Avarious%20vision-language%20tasks%20that%20entail%20local%20detailed%20comprehension%20and%0Ageneration.%20Notably%2C%20this%20work%20is%20the%20first%20demonstration%20of%20the%20feasibility%20of%0Aobject-centric%20slot%20attention%20performed%20with%20MLLMs%20and%20in-the-wild%20natural%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17726v1&entry.124074799=Read"},
{"title": "NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective\n  Ensembling", "author": "Bram Grooten and Farid Hasanov and Chenxiang Zhang and Qiao Xiao and Boqian Wu and Zahra Atashgahi and Ghada Sokar and Shiwei Liu and Lu Yin and Elena Mocanu and Mykola Pechenizkiy and Decebal Constantin Mocanu", "abstract": "  Model ensembles have long been a cornerstone for improving generalization and\nrobustness in deep learning. However, their effectiveness often comes at the\ncost of substantial computational overhead. To address this issue,\nstate-of-the-art methods aim to replicate ensemble-class performance without\nrequiring multiple independently trained networks. Unfortunately, these\nalgorithms often still demand considerable compute at inference. In response to\nthese limitations, we introduce $\\textbf{NeuroTrails}$, a sparse multi-head\narchitecture with dynamically evolving topology. This unexplored model-agnostic\ntraining paradigm improves ensemble performance while reducing the required\nresources. We analyze the underlying reason for its effectiveness and observe\nthat the various neural trails induced by dynamic sparsity attain a\n$\\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays\nefficacy with convolutional and transformer-based architectures on computer\nvision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4,\namong many others, demonstrate increased accuracy and stronger robustness in\nzero-shot generalization, while requiring significantly fewer parameters.\n", "link": "http://arxiv.org/abs/2505.17909v1", "date": "2025-05-23", "relevancy": 2.7271, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5985}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroTrails%3A%20Training%20with%20Dynamic%20Sparse%20Heads%20as%20the%20Key%20to%20Effective%0A%20%20Ensembling&body=Title%3A%20NeuroTrails%3A%20Training%20with%20Dynamic%20Sparse%20Heads%20as%20the%20Key%20to%20Effective%0A%20%20Ensembling%0AAuthor%3A%20Bram%20Grooten%20and%20Farid%20Hasanov%20and%20Chenxiang%20Zhang%20and%20Qiao%20Xiao%20and%20Boqian%20Wu%20and%20Zahra%20Atashgahi%20and%20Ghada%20Sokar%20and%20Shiwei%20Liu%20and%20Lu%20Yin%20and%20Elena%20Mocanu%20and%20Mykola%20Pechenizkiy%20and%20Decebal%20Constantin%20Mocanu%0AAbstract%3A%20%20%20Model%20ensembles%20have%20long%20been%20a%20cornerstone%20for%20improving%20generalization%20and%0Arobustness%20in%20deep%20learning.%20However%2C%20their%20effectiveness%20often%20comes%20at%20the%0Acost%20of%20substantial%20computational%20overhead.%20To%20address%20this%20issue%2C%0Astate-of-the-art%20methods%20aim%20to%20replicate%20ensemble-class%20performance%20without%0Arequiring%20multiple%20independently%20trained%20networks.%20Unfortunately%2C%20these%0Aalgorithms%20often%20still%20demand%20considerable%20compute%20at%20inference.%20In%20response%20to%0Athese%20limitations%2C%20we%20introduce%20%24%5Ctextbf%7BNeuroTrails%7D%24%2C%20a%20sparse%20multi-head%0Aarchitecture%20with%20dynamically%20evolving%20topology.%20This%20unexplored%20model-agnostic%0Atraining%20paradigm%20improves%20ensemble%20performance%20while%20reducing%20the%20required%0Aresources.%20We%20analyze%20the%20underlying%20reason%20for%20its%20effectiveness%20and%20observe%0Athat%20the%20various%20neural%20trails%20induced%20by%20dynamic%20sparsity%20attain%20a%0A%24%5Ctextit%7BGoldilocks%20zone%7D%24%20of%20prediction%20diversity.%20NeuroTrails%20displays%0Aefficacy%20with%20convolutional%20and%20transformer-based%20architectures%20on%20computer%0Avision%20and%20language%20tasks.%20Experiments%20on%20ResNet-50/ImageNet%2C%20LLaMA-350M/C4%2C%0Aamong%20many%20others%2C%20demonstrate%20increased%20accuracy%20and%20stronger%20robustness%20in%0Azero-shot%20generalization%2C%20while%20requiring%20significantly%20fewer%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroTrails%253A%2520Training%2520with%2520Dynamic%2520Sparse%2520Heads%2520as%2520the%2520Key%2520to%2520Effective%250A%2520%2520Ensembling%26entry.906535625%3DBram%2520Grooten%2520and%2520Farid%2520Hasanov%2520and%2520Chenxiang%2520Zhang%2520and%2520Qiao%2520Xiao%2520and%2520Boqian%2520Wu%2520and%2520Zahra%2520Atashgahi%2520and%2520Ghada%2520Sokar%2520and%2520Shiwei%2520Liu%2520and%2520Lu%2520Yin%2520and%2520Elena%2520Mocanu%2520and%2520Mykola%2520Pechenizkiy%2520and%2520Decebal%2520Constantin%2520Mocanu%26entry.1292438233%3D%2520%2520Model%2520ensembles%2520have%2520long%2520been%2520a%2520cornerstone%2520for%2520improving%2520generalization%2520and%250Arobustness%2520in%2520deep%2520learning.%2520However%252C%2520their%2520effectiveness%2520often%2520comes%2520at%2520the%250Acost%2520of%2520substantial%2520computational%2520overhead.%2520To%2520address%2520this%2520issue%252C%250Astate-of-the-art%2520methods%2520aim%2520to%2520replicate%2520ensemble-class%2520performance%2520without%250Arequiring%2520multiple%2520independently%2520trained%2520networks.%2520Unfortunately%252C%2520these%250Aalgorithms%2520often%2520still%2520demand%2520considerable%2520compute%2520at%2520inference.%2520In%2520response%2520to%250Athese%2520limitations%252C%2520we%2520introduce%2520%2524%255Ctextbf%257BNeuroTrails%257D%2524%252C%2520a%2520sparse%2520multi-head%250Aarchitecture%2520with%2520dynamically%2520evolving%2520topology.%2520This%2520unexplored%2520model-agnostic%250Atraining%2520paradigm%2520improves%2520ensemble%2520performance%2520while%2520reducing%2520the%2520required%250Aresources.%2520We%2520analyze%2520the%2520underlying%2520reason%2520for%2520its%2520effectiveness%2520and%2520observe%250Athat%2520the%2520various%2520neural%2520trails%2520induced%2520by%2520dynamic%2520sparsity%2520attain%2520a%250A%2524%255Ctextit%257BGoldilocks%2520zone%257D%2524%2520of%2520prediction%2520diversity.%2520NeuroTrails%2520displays%250Aefficacy%2520with%2520convolutional%2520and%2520transformer-based%2520architectures%2520on%2520computer%250Avision%2520and%2520language%2520tasks.%2520Experiments%2520on%2520ResNet-50/ImageNet%252C%2520LLaMA-350M/C4%252C%250Aamong%2520many%2520others%252C%2520demonstrate%2520increased%2520accuracy%2520and%2520stronger%2520robustness%2520in%250Azero-shot%2520generalization%252C%2520while%2520requiring%2520significantly%2520fewer%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroTrails%3A%20Training%20with%20Dynamic%20Sparse%20Heads%20as%20the%20Key%20to%20Effective%0A%20%20Ensembling&entry.906535625=Bram%20Grooten%20and%20Farid%20Hasanov%20and%20Chenxiang%20Zhang%20and%20Qiao%20Xiao%20and%20Boqian%20Wu%20and%20Zahra%20Atashgahi%20and%20Ghada%20Sokar%20and%20Shiwei%20Liu%20and%20Lu%20Yin%20and%20Elena%20Mocanu%20and%20Mykola%20Pechenizkiy%20and%20Decebal%20Constantin%20Mocanu&entry.1292438233=%20%20Model%20ensembles%20have%20long%20been%20a%20cornerstone%20for%20improving%20generalization%20and%0Arobustness%20in%20deep%20learning.%20However%2C%20their%20effectiveness%20often%20comes%20at%20the%0Acost%20of%20substantial%20computational%20overhead.%20To%20address%20this%20issue%2C%0Astate-of-the-art%20methods%20aim%20to%20replicate%20ensemble-class%20performance%20without%0Arequiring%20multiple%20independently%20trained%20networks.%20Unfortunately%2C%20these%0Aalgorithms%20often%20still%20demand%20considerable%20compute%20at%20inference.%20In%20response%20to%0Athese%20limitations%2C%20we%20introduce%20%24%5Ctextbf%7BNeuroTrails%7D%24%2C%20a%20sparse%20multi-head%0Aarchitecture%20with%20dynamically%20evolving%20topology.%20This%20unexplored%20model-agnostic%0Atraining%20paradigm%20improves%20ensemble%20performance%20while%20reducing%20the%20required%0Aresources.%20We%20analyze%20the%20underlying%20reason%20for%20its%20effectiveness%20and%20observe%0Athat%20the%20various%20neural%20trails%20induced%20by%20dynamic%20sparsity%20attain%20a%0A%24%5Ctextit%7BGoldilocks%20zone%7D%24%20of%20prediction%20diversity.%20NeuroTrails%20displays%0Aefficacy%20with%20convolutional%20and%20transformer-based%20architectures%20on%20computer%0Avision%20and%20language%20tasks.%20Experiments%20on%20ResNet-50/ImageNet%2C%20LLaMA-350M/C4%2C%0Aamong%20many%20others%2C%20demonstrate%20increased%20accuracy%20and%20stronger%20robustness%20in%0Azero-shot%20generalization%2C%20while%20requiring%20significantly%20fewer%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17909v1&entry.124074799=Read"},
{"title": "3D Face Reconstruction Error Decomposed: A Modular Benchmark for Fair\n  and Fast Method Evaluation", "author": "Evangelos Sariyanidi and Claudio Ferrari and Federico Nocentini and Stefano Berretti and Andrea Cavallaro and Birkan Tunc", "abstract": "  Computing the standard benchmark metric for 3D face reconstruction, namely\ngeometric error, requires a number of steps, such as mesh cropping, rigid\nalignment, or point correspondence. Current benchmark tools are monolithic\n(they implement a specific combination of these steps), even though there is no\nconsensus on the best way to measure error. We present a toolkit for a\nModularized 3D Face reconstruction Benchmark (M3DFB), where the fundamental\ncomponents of error computation are segregated and interchangeable, allowing\none to quantify the effect of each. Furthermore, we propose a new component,\nnamely correction, and present a computationally efficient approach that\npenalizes for mesh topology inconsistency. Using this toolkit, we test 16 error\nestimators with 10 reconstruction methods on two real and two synthetic\ndatasets. Critically, the widely used ICP-based estimator provides the worst\nbenchmarking performance, as it significantly alters the true ranking of the\ntop-5 reconstruction methods. Notably, the correlation of ICP with the true\nerror can be as low as 0.41. Moreover, non-rigid alignment leads to significant\nimprovement (correlation larger than 0.90), highlighting the importance of\nannotating 3D landmarks on datasets. Finally, the proposed correction scheme,\ntogether with non-rigid warping, leads to an accuracy on a par with the best\nnon-rigid ICP-based estimators, but runs an order of magnitude faster. Our\nopen-source codebase is designed for researchers to easily compare alternatives\nfor each component, thus helping accelerating progress in benchmarking for 3D\nface reconstruction and, furthermore, supporting the improvement of learned\nreconstruction methods, which depend on accurate error estimation for effective\ntraining.\n", "link": "http://arxiv.org/abs/2505.18025v1", "date": "2025-05-23", "relevancy": 2.727, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5728}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.534}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Face%20Reconstruction%20Error%20Decomposed%3A%20A%20Modular%20Benchmark%20for%20Fair%0A%20%20and%20Fast%20Method%20Evaluation&body=Title%3A%203D%20Face%20Reconstruction%20Error%20Decomposed%3A%20A%20Modular%20Benchmark%20for%20Fair%0A%20%20and%20Fast%20Method%20Evaluation%0AAuthor%3A%20Evangelos%20Sariyanidi%20and%20Claudio%20Ferrari%20and%20Federico%20Nocentini%20and%20Stefano%20Berretti%20and%20Andrea%20Cavallaro%20and%20Birkan%20Tunc%0AAbstract%3A%20%20%20Computing%20the%20standard%20benchmark%20metric%20for%203D%20face%20reconstruction%2C%20namely%0Ageometric%20error%2C%20requires%20a%20number%20of%20steps%2C%20such%20as%20mesh%20cropping%2C%20rigid%0Aalignment%2C%20or%20point%20correspondence.%20Current%20benchmark%20tools%20are%20monolithic%0A%28they%20implement%20a%20specific%20combination%20of%20these%20steps%29%2C%20even%20though%20there%20is%20no%0Aconsensus%20on%20the%20best%20way%20to%20measure%20error.%20We%20present%20a%20toolkit%20for%20a%0AModularized%203D%20Face%20reconstruction%20Benchmark%20%28M3DFB%29%2C%20where%20the%20fundamental%0Acomponents%20of%20error%20computation%20are%20segregated%20and%20interchangeable%2C%20allowing%0Aone%20to%20quantify%20the%20effect%20of%20each.%20Furthermore%2C%20we%20propose%20a%20new%20component%2C%0Anamely%20correction%2C%20and%20present%20a%20computationally%20efficient%20approach%20that%0Apenalizes%20for%20mesh%20topology%20inconsistency.%20Using%20this%20toolkit%2C%20we%20test%2016%20error%0Aestimators%20with%2010%20reconstruction%20methods%20on%20two%20real%20and%20two%20synthetic%0Adatasets.%20Critically%2C%20the%20widely%20used%20ICP-based%20estimator%20provides%20the%20worst%0Abenchmarking%20performance%2C%20as%20it%20significantly%20alters%20the%20true%20ranking%20of%20the%0Atop-5%20reconstruction%20methods.%20Notably%2C%20the%20correlation%20of%20ICP%20with%20the%20true%0Aerror%20can%20be%20as%20low%20as%200.41.%20Moreover%2C%20non-rigid%20alignment%20leads%20to%20significant%0Aimprovement%20%28correlation%20larger%20than%200.90%29%2C%20highlighting%20the%20importance%20of%0Aannotating%203D%20landmarks%20on%20datasets.%20Finally%2C%20the%20proposed%20correction%20scheme%2C%0Atogether%20with%20non-rigid%20warping%2C%20leads%20to%20an%20accuracy%20on%20a%20par%20with%20the%20best%0Anon-rigid%20ICP-based%20estimators%2C%20but%20runs%20an%20order%20of%20magnitude%20faster.%20Our%0Aopen-source%20codebase%20is%20designed%20for%20researchers%20to%20easily%20compare%20alternatives%0Afor%20each%20component%2C%20thus%20helping%20accelerating%20progress%20in%20benchmarking%20for%203D%0Aface%20reconstruction%20and%2C%20furthermore%2C%20supporting%20the%20improvement%20of%20learned%0Areconstruction%20methods%2C%20which%20depend%20on%20accurate%20error%20estimation%20for%20effective%0Atraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Face%2520Reconstruction%2520Error%2520Decomposed%253A%2520A%2520Modular%2520Benchmark%2520for%2520Fair%250A%2520%2520and%2520Fast%2520Method%2520Evaluation%26entry.906535625%3DEvangelos%2520Sariyanidi%2520and%2520Claudio%2520Ferrari%2520and%2520Federico%2520Nocentini%2520and%2520Stefano%2520Berretti%2520and%2520Andrea%2520Cavallaro%2520and%2520Birkan%2520Tunc%26entry.1292438233%3D%2520%2520Computing%2520the%2520standard%2520benchmark%2520metric%2520for%25203D%2520face%2520reconstruction%252C%2520namely%250Ageometric%2520error%252C%2520requires%2520a%2520number%2520of%2520steps%252C%2520such%2520as%2520mesh%2520cropping%252C%2520rigid%250Aalignment%252C%2520or%2520point%2520correspondence.%2520Current%2520benchmark%2520tools%2520are%2520monolithic%250A%2528they%2520implement%2520a%2520specific%2520combination%2520of%2520these%2520steps%2529%252C%2520even%2520though%2520there%2520is%2520no%250Aconsensus%2520on%2520the%2520best%2520way%2520to%2520measure%2520error.%2520We%2520present%2520a%2520toolkit%2520for%2520a%250AModularized%25203D%2520Face%2520reconstruction%2520Benchmark%2520%2528M3DFB%2529%252C%2520where%2520the%2520fundamental%250Acomponents%2520of%2520error%2520computation%2520are%2520segregated%2520and%2520interchangeable%252C%2520allowing%250Aone%2520to%2520quantify%2520the%2520effect%2520of%2520each.%2520Furthermore%252C%2520we%2520propose%2520a%2520new%2520component%252C%250Anamely%2520correction%252C%2520and%2520present%2520a%2520computationally%2520efficient%2520approach%2520that%250Apenalizes%2520for%2520mesh%2520topology%2520inconsistency.%2520Using%2520this%2520toolkit%252C%2520we%2520test%252016%2520error%250Aestimators%2520with%252010%2520reconstruction%2520methods%2520on%2520two%2520real%2520and%2520two%2520synthetic%250Adatasets.%2520Critically%252C%2520the%2520widely%2520used%2520ICP-based%2520estimator%2520provides%2520the%2520worst%250Abenchmarking%2520performance%252C%2520as%2520it%2520significantly%2520alters%2520the%2520true%2520ranking%2520of%2520the%250Atop-5%2520reconstruction%2520methods.%2520Notably%252C%2520the%2520correlation%2520of%2520ICP%2520with%2520the%2520true%250Aerror%2520can%2520be%2520as%2520low%2520as%25200.41.%2520Moreover%252C%2520non-rigid%2520alignment%2520leads%2520to%2520significant%250Aimprovement%2520%2528correlation%2520larger%2520than%25200.90%2529%252C%2520highlighting%2520the%2520importance%2520of%250Aannotating%25203D%2520landmarks%2520on%2520datasets.%2520Finally%252C%2520the%2520proposed%2520correction%2520scheme%252C%250Atogether%2520with%2520non-rigid%2520warping%252C%2520leads%2520to%2520an%2520accuracy%2520on%2520a%2520par%2520with%2520the%2520best%250Anon-rigid%2520ICP-based%2520estimators%252C%2520but%2520runs%2520an%2520order%2520of%2520magnitude%2520faster.%2520Our%250Aopen-source%2520codebase%2520is%2520designed%2520for%2520researchers%2520to%2520easily%2520compare%2520alternatives%250Afor%2520each%2520component%252C%2520thus%2520helping%2520accelerating%2520progress%2520in%2520benchmarking%2520for%25203D%250Aface%2520reconstruction%2520and%252C%2520furthermore%252C%2520supporting%2520the%2520improvement%2520of%2520learned%250Areconstruction%2520methods%252C%2520which%2520depend%2520on%2520accurate%2520error%2520estimation%2520for%2520effective%250Atraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Face%20Reconstruction%20Error%20Decomposed%3A%20A%20Modular%20Benchmark%20for%20Fair%0A%20%20and%20Fast%20Method%20Evaluation&entry.906535625=Evangelos%20Sariyanidi%20and%20Claudio%20Ferrari%20and%20Federico%20Nocentini%20and%20Stefano%20Berretti%20and%20Andrea%20Cavallaro%20and%20Birkan%20Tunc&entry.1292438233=%20%20Computing%20the%20standard%20benchmark%20metric%20for%203D%20face%20reconstruction%2C%20namely%0Ageometric%20error%2C%20requires%20a%20number%20of%20steps%2C%20such%20as%20mesh%20cropping%2C%20rigid%0Aalignment%2C%20or%20point%20correspondence.%20Current%20benchmark%20tools%20are%20monolithic%0A%28they%20implement%20a%20specific%20combination%20of%20these%20steps%29%2C%20even%20though%20there%20is%20no%0Aconsensus%20on%20the%20best%20way%20to%20measure%20error.%20We%20present%20a%20toolkit%20for%20a%0AModularized%203D%20Face%20reconstruction%20Benchmark%20%28M3DFB%29%2C%20where%20the%20fundamental%0Acomponents%20of%20error%20computation%20are%20segregated%20and%20interchangeable%2C%20allowing%0Aone%20to%20quantify%20the%20effect%20of%20each.%20Furthermore%2C%20we%20propose%20a%20new%20component%2C%0Anamely%20correction%2C%20and%20present%20a%20computationally%20efficient%20approach%20that%0Apenalizes%20for%20mesh%20topology%20inconsistency.%20Using%20this%20toolkit%2C%20we%20test%2016%20error%0Aestimators%20with%2010%20reconstruction%20methods%20on%20two%20real%20and%20two%20synthetic%0Adatasets.%20Critically%2C%20the%20widely%20used%20ICP-based%20estimator%20provides%20the%20worst%0Abenchmarking%20performance%2C%20as%20it%20significantly%20alters%20the%20true%20ranking%20of%20the%0Atop-5%20reconstruction%20methods.%20Notably%2C%20the%20correlation%20of%20ICP%20with%20the%20true%0Aerror%20can%20be%20as%20low%20as%200.41.%20Moreover%2C%20non-rigid%20alignment%20leads%20to%20significant%0Aimprovement%20%28correlation%20larger%20than%200.90%29%2C%20highlighting%20the%20importance%20of%0Aannotating%203D%20landmarks%20on%20datasets.%20Finally%2C%20the%20proposed%20correction%20scheme%2C%0Atogether%20with%20non-rigid%20warping%2C%20leads%20to%20an%20accuracy%20on%20a%20par%20with%20the%20best%0Anon-rigid%20ICP-based%20estimators%2C%20but%20runs%20an%20order%20of%20magnitude%20faster.%20Our%0Aopen-source%20codebase%20is%20designed%20for%20researchers%20to%20easily%20compare%20alternatives%0Afor%20each%20component%2C%20thus%20helping%20accelerating%20progress%20in%20benchmarking%20for%203D%0Aface%20reconstruction%20and%2C%20furthermore%2C%20supporting%20the%20improvement%20of%20learned%0Areconstruction%20methods%2C%20which%20depend%20on%20accurate%20error%20estimation%20for%20effective%0Atraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18025v1&entry.124074799=Read"},
{"title": "TopoPoint: Enhance Topology Reasoning via Endpoint Detection in\n  Autonomous Driving", "author": "Yanping Fu and Xinyuan Liu and Tianyu Li and Yike Ma and Yucheng Zhang and Feng Dai", "abstract": "  Topology reasoning, which unifies perception and structured reasoning, plays\na vital role in understanding intersections for autonomous driving. However,\nits performance heavily relies on the accuracy of lane detection, particularly\nat connected lane endpoints. Existing methods often suffer from lane endpoints\ndeviation, leading to incorrect topology construction. To address this issue,\nwe propose TopoPoint, a novel framework that explicitly detects lane endpoints\nand jointly reasons over endpoints and lanes for robust topology reasoning.\nDuring training, we independently initialize point and lane query, and proposed\nPoint-Lane Merge Self-Attention to enhance global context sharing through\nincorporating geometric distances between points and lanes as an attention mask\n. We further design Point-Lane Graph Convolutional Network to enable mutual\nfeature aggregation between point and lane query. During inference, we\nintroduce Point-Lane Geometry Matching algorithm that computes distances\nbetween detected points and lanes to refine lane endpoints, effectively\nmitigating endpoint deviation. Extensive experiments on the OpenLane-V2\nbenchmark demonstrate that TopoPoint achieves state-of-the-art performance in\ntopology reasoning (48.8 on OLS). Additionally, we propose DET$_p$ to evaluate\nendpoint detection, under which our method significantly outperforms existing\napproaches (52.6 v.s. 45.2 on DET$_p$). The code is released at\nhttps://github.com/Franpin/TopoPoint.\n", "link": "http://arxiv.org/abs/2505.17771v1", "date": "2025-05-23", "relevancy": 2.7164, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5658}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5416}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopoPoint%3A%20Enhance%20Topology%20Reasoning%20via%20Endpoint%20Detection%20in%0A%20%20Autonomous%20Driving&body=Title%3A%20TopoPoint%3A%20Enhance%20Topology%20Reasoning%20via%20Endpoint%20Detection%20in%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Yanping%20Fu%20and%20Xinyuan%20Liu%20and%20Tianyu%20Li%20and%20Yike%20Ma%20and%20Yucheng%20Zhang%20and%20Feng%20Dai%0AAbstract%3A%20%20%20Topology%20reasoning%2C%20which%20unifies%20perception%20and%20structured%20reasoning%2C%20plays%0Aa%20vital%20role%20in%20understanding%20intersections%20for%20autonomous%20driving.%20However%2C%0Aits%20performance%20heavily%20relies%20on%20the%20accuracy%20of%20lane%20detection%2C%20particularly%0Aat%20connected%20lane%20endpoints.%20Existing%20methods%20often%20suffer%20from%20lane%20endpoints%0Adeviation%2C%20leading%20to%20incorrect%20topology%20construction.%20To%20address%20this%20issue%2C%0Awe%20propose%20TopoPoint%2C%20a%20novel%20framework%20that%20explicitly%20detects%20lane%20endpoints%0Aand%20jointly%20reasons%20over%20endpoints%20and%20lanes%20for%20robust%20topology%20reasoning.%0ADuring%20training%2C%20we%20independently%20initialize%20point%20and%20lane%20query%2C%20and%20proposed%0APoint-Lane%20Merge%20Self-Attention%20to%20enhance%20global%20context%20sharing%20through%0Aincorporating%20geometric%20distances%20between%20points%20and%20lanes%20as%20an%20attention%20mask%0A.%20We%20further%20design%20Point-Lane%20Graph%20Convolutional%20Network%20to%20enable%20mutual%0Afeature%20aggregation%20between%20point%20and%20lane%20query.%20During%20inference%2C%20we%0Aintroduce%20Point-Lane%20Geometry%20Matching%20algorithm%20that%20computes%20distances%0Abetween%20detected%20points%20and%20lanes%20to%20refine%20lane%20endpoints%2C%20effectively%0Amitigating%20endpoint%20deviation.%20Extensive%20experiments%20on%20the%20OpenLane-V2%0Abenchmark%20demonstrate%20that%20TopoPoint%20achieves%20state-of-the-art%20performance%20in%0Atopology%20reasoning%20%2848.8%20on%20OLS%29.%20Additionally%2C%20we%20propose%20DET%24_p%24%20to%20evaluate%0Aendpoint%20detection%2C%20under%20which%20our%20method%20significantly%20outperforms%20existing%0Aapproaches%20%2852.6%20v.s.%2045.2%20on%20DET%24_p%24%29.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/Franpin/TopoPoint.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopoPoint%253A%2520Enhance%2520Topology%2520Reasoning%2520via%2520Endpoint%2520Detection%2520in%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DYanping%2520Fu%2520and%2520Xinyuan%2520Liu%2520and%2520Tianyu%2520Li%2520and%2520Yike%2520Ma%2520and%2520Yucheng%2520Zhang%2520and%2520Feng%2520Dai%26entry.1292438233%3D%2520%2520Topology%2520reasoning%252C%2520which%2520unifies%2520perception%2520and%2520structured%2520reasoning%252C%2520plays%250Aa%2520vital%2520role%2520in%2520understanding%2520intersections%2520for%2520autonomous%2520driving.%2520However%252C%250Aits%2520performance%2520heavily%2520relies%2520on%2520the%2520accuracy%2520of%2520lane%2520detection%252C%2520particularly%250Aat%2520connected%2520lane%2520endpoints.%2520Existing%2520methods%2520often%2520suffer%2520from%2520lane%2520endpoints%250Adeviation%252C%2520leading%2520to%2520incorrect%2520topology%2520construction.%2520To%2520address%2520this%2520issue%252C%250Awe%2520propose%2520TopoPoint%252C%2520a%2520novel%2520framework%2520that%2520explicitly%2520detects%2520lane%2520endpoints%250Aand%2520jointly%2520reasons%2520over%2520endpoints%2520and%2520lanes%2520for%2520robust%2520topology%2520reasoning.%250ADuring%2520training%252C%2520we%2520independently%2520initialize%2520point%2520and%2520lane%2520query%252C%2520and%2520proposed%250APoint-Lane%2520Merge%2520Self-Attention%2520to%2520enhance%2520global%2520context%2520sharing%2520through%250Aincorporating%2520geometric%2520distances%2520between%2520points%2520and%2520lanes%2520as%2520an%2520attention%2520mask%250A.%2520We%2520further%2520design%2520Point-Lane%2520Graph%2520Convolutional%2520Network%2520to%2520enable%2520mutual%250Afeature%2520aggregation%2520between%2520point%2520and%2520lane%2520query.%2520During%2520inference%252C%2520we%250Aintroduce%2520Point-Lane%2520Geometry%2520Matching%2520algorithm%2520that%2520computes%2520distances%250Abetween%2520detected%2520points%2520and%2520lanes%2520to%2520refine%2520lane%2520endpoints%252C%2520effectively%250Amitigating%2520endpoint%2520deviation.%2520Extensive%2520experiments%2520on%2520the%2520OpenLane-V2%250Abenchmark%2520demonstrate%2520that%2520TopoPoint%2520achieves%2520state-of-the-art%2520performance%2520in%250Atopology%2520reasoning%2520%252848.8%2520on%2520OLS%2529.%2520Additionally%252C%2520we%2520propose%2520DET%2524_p%2524%2520to%2520evaluate%250Aendpoint%2520detection%252C%2520under%2520which%2520our%2520method%2520significantly%2520outperforms%2520existing%250Aapproaches%2520%252852.6%2520v.s.%252045.2%2520on%2520DET%2524_p%2524%2529.%2520The%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/Franpin/TopoPoint.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopoPoint%3A%20Enhance%20Topology%20Reasoning%20via%20Endpoint%20Detection%20in%0A%20%20Autonomous%20Driving&entry.906535625=Yanping%20Fu%20and%20Xinyuan%20Liu%20and%20Tianyu%20Li%20and%20Yike%20Ma%20and%20Yucheng%20Zhang%20and%20Feng%20Dai&entry.1292438233=%20%20Topology%20reasoning%2C%20which%20unifies%20perception%20and%20structured%20reasoning%2C%20plays%0Aa%20vital%20role%20in%20understanding%20intersections%20for%20autonomous%20driving.%20However%2C%0Aits%20performance%20heavily%20relies%20on%20the%20accuracy%20of%20lane%20detection%2C%20particularly%0Aat%20connected%20lane%20endpoints.%20Existing%20methods%20often%20suffer%20from%20lane%20endpoints%0Adeviation%2C%20leading%20to%20incorrect%20topology%20construction.%20To%20address%20this%20issue%2C%0Awe%20propose%20TopoPoint%2C%20a%20novel%20framework%20that%20explicitly%20detects%20lane%20endpoints%0Aand%20jointly%20reasons%20over%20endpoints%20and%20lanes%20for%20robust%20topology%20reasoning.%0ADuring%20training%2C%20we%20independently%20initialize%20point%20and%20lane%20query%2C%20and%20proposed%0APoint-Lane%20Merge%20Self-Attention%20to%20enhance%20global%20context%20sharing%20through%0Aincorporating%20geometric%20distances%20between%20points%20and%20lanes%20as%20an%20attention%20mask%0A.%20We%20further%20design%20Point-Lane%20Graph%20Convolutional%20Network%20to%20enable%20mutual%0Afeature%20aggregation%20between%20point%20and%20lane%20query.%20During%20inference%2C%20we%0Aintroduce%20Point-Lane%20Geometry%20Matching%20algorithm%20that%20computes%20distances%0Abetween%20detected%20points%20and%20lanes%20to%20refine%20lane%20endpoints%2C%20effectively%0Amitigating%20endpoint%20deviation.%20Extensive%20experiments%20on%20the%20OpenLane-V2%0Abenchmark%20demonstrate%20that%20TopoPoint%20achieves%20state-of-the-art%20performance%20in%0Atopology%20reasoning%20%2848.8%20on%20OLS%29.%20Additionally%2C%20we%20propose%20DET%24_p%24%20to%20evaluate%0Aendpoint%20detection%2C%20under%20which%20our%20method%20significantly%20outperforms%20existing%0Aapproaches%20%2852.6%20v.s.%2045.2%20on%20DET%24_p%24%29.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/Franpin/TopoPoint.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17771v1&entry.124074799=Read"},
{"title": "Rapid Whole Brain Motion-robust Mesoscale In-vivo MR Imaging using\n  Multi-scale Implicit Neural Representation", "author": "Jun Lyu and Lipeng Ning and William Consagra and Qiang Liu and Richard J. Rushmore and Berkin Bilgic and Yogesh Rathi", "abstract": "  High-resolution whole-brain in vivo MR imaging at mesoscale resolutions\nremains challenging due to long scan durations, motion artifacts, and limited\nsignal-to-noise ratio (SNR). This study proposes Rotating-view super-resolution\n(ROVER)-MRI, an unsupervised framework based on multi-scale implicit neural\nrepresentations (INR), enabling efficient recovery of fine anatomical details\nfrom multi-view thick-slice acquisitions. ROVER-MRI employs coordinate-based\nneural networks to implicitly and continuously encode image structures at\nmultiple spatial scales, simultaneously modeling anatomical continuity and\ncorrecting inter-view motion through an integrated registration mechanism.\nValidation on ex-vivo monkey brain data and multiple in-vivo human datasets\ndemonstrates substantially improved reconstruction performance compared to\nbicubic interpolation and state-of-the-art regularized least-squares\nsuper-resolution reconstruction (LS-SRR) with 2-fold reduction in scan time.\nNotably, ROVER-MRI achieves an unprecedented whole-brain in-vivo T2-weighted\nimaging at 180 micron isotropic resolution in only 17 minutes of scan time on a\n7T scanner with 22.4% lower relative error compared to LS-SRR. We also\ndemonstrate improved SNR using ROVER-MRI compared to a time-matched 3D GRE\nacquisition. Quantitative results on several datasets demonstrate better\nsharpness of the reconstructed images with ROVER-MRI for different\nsuper-resolution factors (5 to 11). These findings highlight ROVER-MRI's\npotential as a rapid, accurate, and motion-resilient mesoscale imaging\nsolution, promising substantial advantages for neuroimaging studies.\n", "link": "http://arxiv.org/abs/2502.08634v2", "date": "2025-05-23", "relevancy": 2.7098, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5656}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5301}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rapid%20Whole%20Brain%20Motion-robust%20Mesoscale%20In-vivo%20MR%20Imaging%20using%0A%20%20Multi-scale%20Implicit%20Neural%20Representation&body=Title%3A%20Rapid%20Whole%20Brain%20Motion-robust%20Mesoscale%20In-vivo%20MR%20Imaging%20using%0A%20%20Multi-scale%20Implicit%20Neural%20Representation%0AAuthor%3A%20Jun%20Lyu%20and%20Lipeng%20Ning%20and%20William%20Consagra%20and%20Qiang%20Liu%20and%20Richard%20J.%20Rushmore%20and%20Berkin%20Bilgic%20and%20Yogesh%20Rathi%0AAbstract%3A%20%20%20High-resolution%20whole-brain%20in%20vivo%20MR%20imaging%20at%20mesoscale%20resolutions%0Aremains%20challenging%20due%20to%20long%20scan%20durations%2C%20motion%20artifacts%2C%20and%20limited%0Asignal-to-noise%20ratio%20%28SNR%29.%20This%20study%20proposes%20Rotating-view%20super-resolution%0A%28ROVER%29-MRI%2C%20an%20unsupervised%20framework%20based%20on%20multi-scale%20implicit%20neural%0Arepresentations%20%28INR%29%2C%20enabling%20efficient%20recovery%20of%20fine%20anatomical%20details%0Afrom%20multi-view%20thick-slice%20acquisitions.%20ROVER-MRI%20employs%20coordinate-based%0Aneural%20networks%20to%20implicitly%20and%20continuously%20encode%20image%20structures%20at%0Amultiple%20spatial%20scales%2C%20simultaneously%20modeling%20anatomical%20continuity%20and%0Acorrecting%20inter-view%20motion%20through%20an%20integrated%20registration%20mechanism.%0AValidation%20on%20ex-vivo%20monkey%20brain%20data%20and%20multiple%20in-vivo%20human%20datasets%0Ademonstrates%20substantially%20improved%20reconstruction%20performance%20compared%20to%0Abicubic%20interpolation%20and%20state-of-the-art%20regularized%20least-squares%0Asuper-resolution%20reconstruction%20%28LS-SRR%29%20with%202-fold%20reduction%20in%20scan%20time.%0ANotably%2C%20ROVER-MRI%20achieves%20an%20unprecedented%20whole-brain%20in-vivo%20T2-weighted%0Aimaging%20at%20180%20micron%20isotropic%20resolution%20in%20only%2017%20minutes%20of%20scan%20time%20on%20a%0A7T%20scanner%20with%2022.4%25%20lower%20relative%20error%20compared%20to%20LS-SRR.%20We%20also%0Ademonstrate%20improved%20SNR%20using%20ROVER-MRI%20compared%20to%20a%20time-matched%203D%20GRE%0Aacquisition.%20Quantitative%20results%20on%20several%20datasets%20demonstrate%20better%0Asharpness%20of%20the%20reconstructed%20images%20with%20ROVER-MRI%20for%20different%0Asuper-resolution%20factors%20%285%20to%2011%29.%20These%20findings%20highlight%20ROVER-MRI%27s%0Apotential%20as%20a%20rapid%2C%20accurate%2C%20and%20motion-resilient%20mesoscale%20imaging%0Asolution%2C%20promising%20substantial%20advantages%20for%20neuroimaging%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapid%2520Whole%2520Brain%2520Motion-robust%2520Mesoscale%2520In-vivo%2520MR%2520Imaging%2520using%250A%2520%2520Multi-scale%2520Implicit%2520Neural%2520Representation%26entry.906535625%3DJun%2520Lyu%2520and%2520Lipeng%2520Ning%2520and%2520William%2520Consagra%2520and%2520Qiang%2520Liu%2520and%2520Richard%2520J.%2520Rushmore%2520and%2520Berkin%2520Bilgic%2520and%2520Yogesh%2520Rathi%26entry.1292438233%3D%2520%2520High-resolution%2520whole-brain%2520in%2520vivo%2520MR%2520imaging%2520at%2520mesoscale%2520resolutions%250Aremains%2520challenging%2520due%2520to%2520long%2520scan%2520durations%252C%2520motion%2520artifacts%252C%2520and%2520limited%250Asignal-to-noise%2520ratio%2520%2528SNR%2529.%2520This%2520study%2520proposes%2520Rotating-view%2520super-resolution%250A%2528ROVER%2529-MRI%252C%2520an%2520unsupervised%2520framework%2520based%2520on%2520multi-scale%2520implicit%2520neural%250Arepresentations%2520%2528INR%2529%252C%2520enabling%2520efficient%2520recovery%2520of%2520fine%2520anatomical%2520details%250Afrom%2520multi-view%2520thick-slice%2520acquisitions.%2520ROVER-MRI%2520employs%2520coordinate-based%250Aneural%2520networks%2520to%2520implicitly%2520and%2520continuously%2520encode%2520image%2520structures%2520at%250Amultiple%2520spatial%2520scales%252C%2520simultaneously%2520modeling%2520anatomical%2520continuity%2520and%250Acorrecting%2520inter-view%2520motion%2520through%2520an%2520integrated%2520registration%2520mechanism.%250AValidation%2520on%2520ex-vivo%2520monkey%2520brain%2520data%2520and%2520multiple%2520in-vivo%2520human%2520datasets%250Ademonstrates%2520substantially%2520improved%2520reconstruction%2520performance%2520compared%2520to%250Abicubic%2520interpolation%2520and%2520state-of-the-art%2520regularized%2520least-squares%250Asuper-resolution%2520reconstruction%2520%2528LS-SRR%2529%2520with%25202-fold%2520reduction%2520in%2520scan%2520time.%250ANotably%252C%2520ROVER-MRI%2520achieves%2520an%2520unprecedented%2520whole-brain%2520in-vivo%2520T2-weighted%250Aimaging%2520at%2520180%2520micron%2520isotropic%2520resolution%2520in%2520only%252017%2520minutes%2520of%2520scan%2520time%2520on%2520a%250A7T%2520scanner%2520with%252022.4%2525%2520lower%2520relative%2520error%2520compared%2520to%2520LS-SRR.%2520We%2520also%250Ademonstrate%2520improved%2520SNR%2520using%2520ROVER-MRI%2520compared%2520to%2520a%2520time-matched%25203D%2520GRE%250Aacquisition.%2520Quantitative%2520results%2520on%2520several%2520datasets%2520demonstrate%2520better%250Asharpness%2520of%2520the%2520reconstructed%2520images%2520with%2520ROVER-MRI%2520for%2520different%250Asuper-resolution%2520factors%2520%25285%2520to%252011%2529.%2520These%2520findings%2520highlight%2520ROVER-MRI%2527s%250Apotential%2520as%2520a%2520rapid%252C%2520accurate%252C%2520and%2520motion-resilient%2520mesoscale%2520imaging%250Asolution%252C%2520promising%2520substantial%2520advantages%2520for%2520neuroimaging%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rapid%20Whole%20Brain%20Motion-robust%20Mesoscale%20In-vivo%20MR%20Imaging%20using%0A%20%20Multi-scale%20Implicit%20Neural%20Representation&entry.906535625=Jun%20Lyu%20and%20Lipeng%20Ning%20and%20William%20Consagra%20and%20Qiang%20Liu%20and%20Richard%20J.%20Rushmore%20and%20Berkin%20Bilgic%20and%20Yogesh%20Rathi&entry.1292438233=%20%20High-resolution%20whole-brain%20in%20vivo%20MR%20imaging%20at%20mesoscale%20resolutions%0Aremains%20challenging%20due%20to%20long%20scan%20durations%2C%20motion%20artifacts%2C%20and%20limited%0Asignal-to-noise%20ratio%20%28SNR%29.%20This%20study%20proposes%20Rotating-view%20super-resolution%0A%28ROVER%29-MRI%2C%20an%20unsupervised%20framework%20based%20on%20multi-scale%20implicit%20neural%0Arepresentations%20%28INR%29%2C%20enabling%20efficient%20recovery%20of%20fine%20anatomical%20details%0Afrom%20multi-view%20thick-slice%20acquisitions.%20ROVER-MRI%20employs%20coordinate-based%0Aneural%20networks%20to%20implicitly%20and%20continuously%20encode%20image%20structures%20at%0Amultiple%20spatial%20scales%2C%20simultaneously%20modeling%20anatomical%20continuity%20and%0Acorrecting%20inter-view%20motion%20through%20an%20integrated%20registration%20mechanism.%0AValidation%20on%20ex-vivo%20monkey%20brain%20data%20and%20multiple%20in-vivo%20human%20datasets%0Ademonstrates%20substantially%20improved%20reconstruction%20performance%20compared%20to%0Abicubic%20interpolation%20and%20state-of-the-art%20regularized%20least-squares%0Asuper-resolution%20reconstruction%20%28LS-SRR%29%20with%202-fold%20reduction%20in%20scan%20time.%0ANotably%2C%20ROVER-MRI%20achieves%20an%20unprecedented%20whole-brain%20in-vivo%20T2-weighted%0Aimaging%20at%20180%20micron%20isotropic%20resolution%20in%20only%2017%20minutes%20of%20scan%20time%20on%20a%0A7T%20scanner%20with%2022.4%25%20lower%20relative%20error%20compared%20to%20LS-SRR.%20We%20also%0Ademonstrate%20improved%20SNR%20using%20ROVER-MRI%20compared%20to%20a%20time-matched%203D%20GRE%0Aacquisition.%20Quantitative%20results%20on%20several%20datasets%20demonstrate%20better%0Asharpness%20of%20the%20reconstructed%20images%20with%20ROVER-MRI%20for%20different%0Asuper-resolution%20factors%20%285%20to%2011%29.%20These%20findings%20highlight%20ROVER-MRI%27s%0Apotential%20as%20a%20rapid%2C%20accurate%2C%20and%20motion-resilient%20mesoscale%20imaging%0Asolution%2C%20promising%20substantial%20advantages%20for%20neuroimaging%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08634v2&entry.124074799=Read"},
{"title": "Directed Semi-Simplicial Learning with Applications to Brain Activity\n  Decoding", "author": "Manuel Lecha and Andrea Cavallo and Francesca Dominici and Ran Levi and Alessio Del Bue and Elvin Isufi and Pietro Morerio and Claudio Battiloro", "abstract": "  Graph Neural Networks (GNNs) excel at learning from pairwise interactions but\noften overlook multi-way and hierarchical relationships. Topological Deep\nLearning (TDL) addresses this limitation by leveraging combinatorial\ntopological spaces. However, existing TDL models are restricted to undirected\nsettings and fail to capture the higher-order directed patterns prevalent in\nmany complex systems, e.g., brain networks, where such interactions are both\nabundant and functionally significant. To fill this gap, we introduce\nSemi-Simplicial Neural Networks (SSNs), a principled class of TDL models that\noperate on semi-simplicial sets -- combinatorial structures that encode\ndirected higher-order motifs and their directional relationships. To enhance\nscalability, we propose Routing-SSNs, which dynamically select the most\ninformative relations in a learnable manner. We prove that SSNs are strictly\nmore expressive than standard graph and TDL models. We then introduce a new\nprincipled framework for brain dynamics representation learning, grounded in\nthe ability of SSNs to provably recover topological descriptors shown to\nsuccessfully characterize brain activity. Empirically, SSNs achieve\nstate-of-the-art performance on brain dynamics classification tasks,\noutperforming the second-best model by up to 27%, and message passing GNNs by\nup to 50% in accuracy. Our results highlight the potential of principled\ntopological models for learning from structured brain data, establishing a\nunique real-world case study for TDL. We also test SSNs on standard node\nclassification and edge regression tasks, showing competitive performance. We\nwill make the code and data publicly available.\n", "link": "http://arxiv.org/abs/2505.17939v1", "date": "2025-05-23", "relevancy": 2.7017, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5495}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5384}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Directed%20Semi-Simplicial%20Learning%20with%20Applications%20to%20Brain%20Activity%0A%20%20Decoding&body=Title%3A%20Directed%20Semi-Simplicial%20Learning%20with%20Applications%20to%20Brain%20Activity%0A%20%20Decoding%0AAuthor%3A%20Manuel%20Lecha%20and%20Andrea%20Cavallo%20and%20Francesca%20Dominici%20and%20Ran%20Levi%20and%20Alessio%20Del%20Bue%20and%20Elvin%20Isufi%20and%20Pietro%20Morerio%20and%20Claudio%20Battiloro%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20at%20learning%20from%20pairwise%20interactions%20but%0Aoften%20overlook%20multi-way%20and%20hierarchical%20relationships.%20Topological%20Deep%0ALearning%20%28TDL%29%20addresses%20this%20limitation%20by%20leveraging%20combinatorial%0Atopological%20spaces.%20However%2C%20existing%20TDL%20models%20are%20restricted%20to%20undirected%0Asettings%20and%20fail%20to%20capture%20the%20higher-order%20directed%20patterns%20prevalent%20in%0Amany%20complex%20systems%2C%20e.g.%2C%20brain%20networks%2C%20where%20such%20interactions%20are%20both%0Aabundant%20and%20functionally%20significant.%20To%20fill%20this%20gap%2C%20we%20introduce%0ASemi-Simplicial%20Neural%20Networks%20%28SSNs%29%2C%20a%20principled%20class%20of%20TDL%20models%20that%0Aoperate%20on%20semi-simplicial%20sets%20--%20combinatorial%20structures%20that%20encode%0Adirected%20higher-order%20motifs%20and%20their%20directional%20relationships.%20To%20enhance%0Ascalability%2C%20we%20propose%20Routing-SSNs%2C%20which%20dynamically%20select%20the%20most%0Ainformative%20relations%20in%20a%20learnable%20manner.%20We%20prove%20that%20SSNs%20are%20strictly%0Amore%20expressive%20than%20standard%20graph%20and%20TDL%20models.%20We%20then%20introduce%20a%20new%0Aprincipled%20framework%20for%20brain%20dynamics%20representation%20learning%2C%20grounded%20in%0Athe%20ability%20of%20SSNs%20to%20provably%20recover%20topological%20descriptors%20shown%20to%0Asuccessfully%20characterize%20brain%20activity.%20Empirically%2C%20SSNs%20achieve%0Astate-of-the-art%20performance%20on%20brain%20dynamics%20classification%20tasks%2C%0Aoutperforming%20the%20second-best%20model%20by%20up%20to%2027%25%2C%20and%20message%20passing%20GNNs%20by%0Aup%20to%2050%25%20in%20accuracy.%20Our%20results%20highlight%20the%20potential%20of%20principled%0Atopological%20models%20for%20learning%20from%20structured%20brain%20data%2C%20establishing%20a%0Aunique%20real-world%20case%20study%20for%20TDL.%20We%20also%20test%20SSNs%20on%20standard%20node%0Aclassification%20and%20edge%20regression%20tasks%2C%20showing%20competitive%20performance.%20We%0Awill%20make%20the%20code%20and%20data%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirected%2520Semi-Simplicial%2520Learning%2520with%2520Applications%2520to%2520Brain%2520Activity%250A%2520%2520Decoding%26entry.906535625%3DManuel%2520Lecha%2520and%2520Andrea%2520Cavallo%2520and%2520Francesca%2520Dominici%2520and%2520Ran%2520Levi%2520and%2520Alessio%2520Del%2520Bue%2520and%2520Elvin%2520Isufi%2520and%2520Pietro%2520Morerio%2520and%2520Claudio%2520Battiloro%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520excel%2520at%2520learning%2520from%2520pairwise%2520interactions%2520but%250Aoften%2520overlook%2520multi-way%2520and%2520hierarchical%2520relationships.%2520Topological%2520Deep%250ALearning%2520%2528TDL%2529%2520addresses%2520this%2520limitation%2520by%2520leveraging%2520combinatorial%250Atopological%2520spaces.%2520However%252C%2520existing%2520TDL%2520models%2520are%2520restricted%2520to%2520undirected%250Asettings%2520and%2520fail%2520to%2520capture%2520the%2520higher-order%2520directed%2520patterns%2520prevalent%2520in%250Amany%2520complex%2520systems%252C%2520e.g.%252C%2520brain%2520networks%252C%2520where%2520such%2520interactions%2520are%2520both%250Aabundant%2520and%2520functionally%2520significant.%2520To%2520fill%2520this%2520gap%252C%2520we%2520introduce%250ASemi-Simplicial%2520Neural%2520Networks%2520%2528SSNs%2529%252C%2520a%2520principled%2520class%2520of%2520TDL%2520models%2520that%250Aoperate%2520on%2520semi-simplicial%2520sets%2520--%2520combinatorial%2520structures%2520that%2520encode%250Adirected%2520higher-order%2520motifs%2520and%2520their%2520directional%2520relationships.%2520To%2520enhance%250Ascalability%252C%2520we%2520propose%2520Routing-SSNs%252C%2520which%2520dynamically%2520select%2520the%2520most%250Ainformative%2520relations%2520in%2520a%2520learnable%2520manner.%2520We%2520prove%2520that%2520SSNs%2520are%2520strictly%250Amore%2520expressive%2520than%2520standard%2520graph%2520and%2520TDL%2520models.%2520We%2520then%2520introduce%2520a%2520new%250Aprincipled%2520framework%2520for%2520brain%2520dynamics%2520representation%2520learning%252C%2520grounded%2520in%250Athe%2520ability%2520of%2520SSNs%2520to%2520provably%2520recover%2520topological%2520descriptors%2520shown%2520to%250Asuccessfully%2520characterize%2520brain%2520activity.%2520Empirically%252C%2520SSNs%2520achieve%250Astate-of-the-art%2520performance%2520on%2520brain%2520dynamics%2520classification%2520tasks%252C%250Aoutperforming%2520the%2520second-best%2520model%2520by%2520up%2520to%252027%2525%252C%2520and%2520message%2520passing%2520GNNs%2520by%250Aup%2520to%252050%2525%2520in%2520accuracy.%2520Our%2520results%2520highlight%2520the%2520potential%2520of%2520principled%250Atopological%2520models%2520for%2520learning%2520from%2520structured%2520brain%2520data%252C%2520establishing%2520a%250Aunique%2520real-world%2520case%2520study%2520for%2520TDL.%2520We%2520also%2520test%2520SSNs%2520on%2520standard%2520node%250Aclassification%2520and%2520edge%2520regression%2520tasks%252C%2520showing%2520competitive%2520performance.%2520We%250Awill%2520make%2520the%2520code%2520and%2520data%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Directed%20Semi-Simplicial%20Learning%20with%20Applications%20to%20Brain%20Activity%0A%20%20Decoding&entry.906535625=Manuel%20Lecha%20and%20Andrea%20Cavallo%20and%20Francesca%20Dominici%20and%20Ran%20Levi%20and%20Alessio%20Del%20Bue%20and%20Elvin%20Isufi%20and%20Pietro%20Morerio%20and%20Claudio%20Battiloro&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20at%20learning%20from%20pairwise%20interactions%20but%0Aoften%20overlook%20multi-way%20and%20hierarchical%20relationships.%20Topological%20Deep%0ALearning%20%28TDL%29%20addresses%20this%20limitation%20by%20leveraging%20combinatorial%0Atopological%20spaces.%20However%2C%20existing%20TDL%20models%20are%20restricted%20to%20undirected%0Asettings%20and%20fail%20to%20capture%20the%20higher-order%20directed%20patterns%20prevalent%20in%0Amany%20complex%20systems%2C%20e.g.%2C%20brain%20networks%2C%20where%20such%20interactions%20are%20both%0Aabundant%20and%20functionally%20significant.%20To%20fill%20this%20gap%2C%20we%20introduce%0ASemi-Simplicial%20Neural%20Networks%20%28SSNs%29%2C%20a%20principled%20class%20of%20TDL%20models%20that%0Aoperate%20on%20semi-simplicial%20sets%20--%20combinatorial%20structures%20that%20encode%0Adirected%20higher-order%20motifs%20and%20their%20directional%20relationships.%20To%20enhance%0Ascalability%2C%20we%20propose%20Routing-SSNs%2C%20which%20dynamically%20select%20the%20most%0Ainformative%20relations%20in%20a%20learnable%20manner.%20We%20prove%20that%20SSNs%20are%20strictly%0Amore%20expressive%20than%20standard%20graph%20and%20TDL%20models.%20We%20then%20introduce%20a%20new%0Aprincipled%20framework%20for%20brain%20dynamics%20representation%20learning%2C%20grounded%20in%0Athe%20ability%20of%20SSNs%20to%20provably%20recover%20topological%20descriptors%20shown%20to%0Asuccessfully%20characterize%20brain%20activity.%20Empirically%2C%20SSNs%20achieve%0Astate-of-the-art%20performance%20on%20brain%20dynamics%20classification%20tasks%2C%0Aoutperforming%20the%20second-best%20model%20by%20up%20to%2027%25%2C%20and%20message%20passing%20GNNs%20by%0Aup%20to%2050%25%20in%20accuracy.%20Our%20results%20highlight%20the%20potential%20of%20principled%0Atopological%20models%20for%20learning%20from%20structured%20brain%20data%2C%20establishing%20a%0Aunique%20real-world%20case%20study%20for%20TDL.%20We%20also%20test%20SSNs%20on%20standard%20node%0Aclassification%20and%20edge%20regression%20tasks%2C%20showing%20competitive%20performance.%20We%0Awill%20make%20the%20code%20and%20data%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17939v1&entry.124074799=Read"},
{"title": "Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor\n  Segmentation with Missing Modalities", "author": "Junze Wang and Lei Fan and Weipeng Jing and Donglin Di and Yang Song and Sidong Liu and Cong Cong", "abstract": "  Existing methods for multimodal MRI segmentation with missing modalities\ntypically assume that all MRI modalities are available during training.\nHowever, in clinical practice, some modalities may be missing due to the\nsequential nature of MRI acquisition, leading to performance degradation.\nFurthermore, retraining models to accommodate newly available modalities can be\ninefficient and may cause overfitting, potentially compromising previously\nlearned knowledge. To address these challenges, we propose Replay-based\nHypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation\nwith missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to\nenable the segmentation model to learn from newly acquired MRI modalities\nwithout forgetting previously learned information. To enhance segmentation\nperformance across diverse patient scenarios, we introduce the Cross-Patient\nHypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture\nhigh-order associations between patients. Additionally, we incorporate\nTversky-Aware Contrastive (TAC) loss to effectively mitigate information\nimbalance both across and within different modalities. Extensive experiments on\nthe BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art\nmethods, achieving an improvement of over 2% in the Dice Similarity Coefficient\nacross various tumor regions.\n", "link": "http://arxiv.org/abs/2505.16809v2", "date": "2025-05-23", "relevancy": 2.6934, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5901}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5189}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hypergraph%20Tversky-Aware%20Domain%20Incremental%20Learning%20for%20Brain%20Tumor%0A%20%20Segmentation%20with%20Missing%20Modalities&body=Title%3A%20Hypergraph%20Tversky-Aware%20Domain%20Incremental%20Learning%20for%20Brain%20Tumor%0A%20%20Segmentation%20with%20Missing%20Modalities%0AAuthor%3A%20Junze%20Wang%20and%20Lei%20Fan%20and%20Weipeng%20Jing%20and%20Donglin%20Di%20and%20Yang%20Song%20and%20Sidong%20Liu%20and%20Cong%20Cong%0AAbstract%3A%20%20%20Existing%20methods%20for%20multimodal%20MRI%20segmentation%20with%20missing%20modalities%0Atypically%20assume%20that%20all%20MRI%20modalities%20are%20available%20during%20training.%0AHowever%2C%20in%20clinical%20practice%2C%20some%20modalities%20may%20be%20missing%20due%20to%20the%0Asequential%20nature%20of%20MRI%20acquisition%2C%20leading%20to%20performance%20degradation.%0AFurthermore%2C%20retraining%20models%20to%20accommodate%20newly%20available%20modalities%20can%20be%0Ainefficient%20and%20may%20cause%20overfitting%2C%20potentially%20compromising%20previously%0Alearned%20knowledge.%20To%20address%20these%20challenges%2C%20we%20propose%20Replay-based%0AHypergraph%20Domain%20Incremental%20Learning%20%28ReHyDIL%29%20for%20brain%20tumor%20segmentation%0Awith%20missing%20modalities.%20ReHyDIL%20leverages%20Domain%20Incremental%20Learning%20%28DIL%29%20to%0Aenable%20the%20segmentation%20model%20to%20learn%20from%20newly%20acquired%20MRI%20modalities%0Awithout%20forgetting%20previously%20learned%20information.%20To%20enhance%20segmentation%0Aperformance%20across%20diverse%20patient%20scenarios%2C%20we%20introduce%20the%20Cross-Patient%0AHypergraph%20Segmentation%20Network%20%28CHSNet%29%2C%20which%20utilizes%20hypergraphs%20to%20capture%0Ahigh-order%20associations%20between%20patients.%20Additionally%2C%20we%20incorporate%0ATversky-Aware%20Contrastive%20%28TAC%29%20loss%20to%20effectively%20mitigate%20information%0Aimbalance%20both%20across%20and%20within%20different%20modalities.%20Extensive%20experiments%20on%0Athe%20BraTS2019%20dataset%20demonstrate%20that%20ReHyDIL%20outperforms%20state-of-the-art%0Amethods%2C%20achieving%20an%20improvement%20of%20over%202%25%20in%20the%20Dice%20Similarity%20Coefficient%0Aacross%20various%20tumor%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHypergraph%2520Tversky-Aware%2520Domain%2520Incremental%2520Learning%2520for%2520Brain%2520Tumor%250A%2520%2520Segmentation%2520with%2520Missing%2520Modalities%26entry.906535625%3DJunze%2520Wang%2520and%2520Lei%2520Fan%2520and%2520Weipeng%2520Jing%2520and%2520Donglin%2520Di%2520and%2520Yang%2520Song%2520and%2520Sidong%2520Liu%2520and%2520Cong%2520Cong%26entry.1292438233%3D%2520%2520Existing%2520methods%2520for%2520multimodal%2520MRI%2520segmentation%2520with%2520missing%2520modalities%250Atypically%2520assume%2520that%2520all%2520MRI%2520modalities%2520are%2520available%2520during%2520training.%250AHowever%252C%2520in%2520clinical%2520practice%252C%2520some%2520modalities%2520may%2520be%2520missing%2520due%2520to%2520the%250Asequential%2520nature%2520of%2520MRI%2520acquisition%252C%2520leading%2520to%2520performance%2520degradation.%250AFurthermore%252C%2520retraining%2520models%2520to%2520accommodate%2520newly%2520available%2520modalities%2520can%2520be%250Ainefficient%2520and%2520may%2520cause%2520overfitting%252C%2520potentially%2520compromising%2520previously%250Alearned%2520knowledge.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Replay-based%250AHypergraph%2520Domain%2520Incremental%2520Learning%2520%2528ReHyDIL%2529%2520for%2520brain%2520tumor%2520segmentation%250Awith%2520missing%2520modalities.%2520ReHyDIL%2520leverages%2520Domain%2520Incremental%2520Learning%2520%2528DIL%2529%2520to%250Aenable%2520the%2520segmentation%2520model%2520to%2520learn%2520from%2520newly%2520acquired%2520MRI%2520modalities%250Awithout%2520forgetting%2520previously%2520learned%2520information.%2520To%2520enhance%2520segmentation%250Aperformance%2520across%2520diverse%2520patient%2520scenarios%252C%2520we%2520introduce%2520the%2520Cross-Patient%250AHypergraph%2520Segmentation%2520Network%2520%2528CHSNet%2529%252C%2520which%2520utilizes%2520hypergraphs%2520to%2520capture%250Ahigh-order%2520associations%2520between%2520patients.%2520Additionally%252C%2520we%2520incorporate%250ATversky-Aware%2520Contrastive%2520%2528TAC%2529%2520loss%2520to%2520effectively%2520mitigate%2520information%250Aimbalance%2520both%2520across%2520and%2520within%2520different%2520modalities.%2520Extensive%2520experiments%2520on%250Athe%2520BraTS2019%2520dataset%2520demonstrate%2520that%2520ReHyDIL%2520outperforms%2520state-of-the-art%250Amethods%252C%2520achieving%2520an%2520improvement%2520of%2520over%25202%2525%2520in%2520the%2520Dice%2520Similarity%2520Coefficient%250Aacross%2520various%2520tumor%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypergraph%20Tversky-Aware%20Domain%20Incremental%20Learning%20for%20Brain%20Tumor%0A%20%20Segmentation%20with%20Missing%20Modalities&entry.906535625=Junze%20Wang%20and%20Lei%20Fan%20and%20Weipeng%20Jing%20and%20Donglin%20Di%20and%20Yang%20Song%20and%20Sidong%20Liu%20and%20Cong%20Cong&entry.1292438233=%20%20Existing%20methods%20for%20multimodal%20MRI%20segmentation%20with%20missing%20modalities%0Atypically%20assume%20that%20all%20MRI%20modalities%20are%20available%20during%20training.%0AHowever%2C%20in%20clinical%20practice%2C%20some%20modalities%20may%20be%20missing%20due%20to%20the%0Asequential%20nature%20of%20MRI%20acquisition%2C%20leading%20to%20performance%20degradation.%0AFurthermore%2C%20retraining%20models%20to%20accommodate%20newly%20available%20modalities%20can%20be%0Ainefficient%20and%20may%20cause%20overfitting%2C%20potentially%20compromising%20previously%0Alearned%20knowledge.%20To%20address%20these%20challenges%2C%20we%20propose%20Replay-based%0AHypergraph%20Domain%20Incremental%20Learning%20%28ReHyDIL%29%20for%20brain%20tumor%20segmentation%0Awith%20missing%20modalities.%20ReHyDIL%20leverages%20Domain%20Incremental%20Learning%20%28DIL%29%20to%0Aenable%20the%20segmentation%20model%20to%20learn%20from%20newly%20acquired%20MRI%20modalities%0Awithout%20forgetting%20previously%20learned%20information.%20To%20enhance%20segmentation%0Aperformance%20across%20diverse%20patient%20scenarios%2C%20we%20introduce%20the%20Cross-Patient%0AHypergraph%20Segmentation%20Network%20%28CHSNet%29%2C%20which%20utilizes%20hypergraphs%20to%20capture%0Ahigh-order%20associations%20between%20patients.%20Additionally%2C%20we%20incorporate%0ATversky-Aware%20Contrastive%20%28TAC%29%20loss%20to%20effectively%20mitigate%20information%0Aimbalance%20both%20across%20and%20within%20different%20modalities.%20Extensive%20experiments%20on%0Athe%20BraTS2019%20dataset%20demonstrate%20that%20ReHyDIL%20outperforms%20state-of-the-art%0Amethods%2C%20achieving%20an%20improvement%20of%20over%202%25%20in%20the%20Dice%20Similarity%20Coefficient%0Aacross%20various%20tumor%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16809v2&entry.124074799=Read"},
{"title": "Instructify: Demystifying Metadata to Visual Instruction Tuning Data\n  Conversion", "author": "Jacob Hansen and Wei Lin and Junmo Kang and Muhammad Jehanzeb Mirza and Hongyin Luo and Rogerio Feris and Alan Ritter and James Glass and Leonid Karlinsky", "abstract": "  Visual Instruction Tuning (VisIT) data, commonly available as human-assistant\nconversations with images interleaved in the human turns, are currently the\nmost widespread vehicle for aligning strong LLMs to understand visual inputs,\nconverting them to strong LMMs. While many VisIT datasets are available, most\nare constructed using ad-hoc techniques developed independently by different\ngroups. They are often poorly documented, lack reproducible code, and rely on\npaid, closed-source model APIs such as GPT-4, Gemini, or Claude to convert\nimage metadata (labels) into VisIT instructions. This leads to high costs and\nmakes it challenging to scale, enhance quality, or generate VisIT data for new\ndatasets. In this work, we address these challenges and propose an open and\nunified recipe and approach,~\\textbf{\\method}, for converting available\nmetadata to VisIT instructions using open LLMs. Our multi-stage \\method\nfeatures an efficient framework for metadata grouping, quality control, data\nand prompt organization, and conversation sampling. We show that our approach\ncan reproduce or enhance the data quality of available VisIT datasets when\napplied to the same image data and metadata sources, improving GPT-4 generated\nVisIT instructions by ~3\\% on average and up to 12\\% on individual benchmarks\nusing open models, such as Gemma 2 27B and LLaMa 3.1 70B. Additionally, our\napproach enables effective performance scaling - both in quantity and quality -\nby enhancing the resulting LMM performance across a wide range of benchmarks.\nWe also analyze the impact of various factors, including conversation format,\nbase model selection, and resampling strategies. Our code, which supports the\nreproduction of equal or higher-quality VisIT datasets and facilities future\nmetadata-to-VisIT data conversion for niche domains, is released at\nhttps://github.com/jacob-hansen/Instructify.\n", "link": "http://arxiv.org/abs/2505.18115v1", "date": "2025-05-23", "relevancy": 2.6817, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5477}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5337}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instructify%3A%20Demystifying%20Metadata%20to%20Visual%20Instruction%20Tuning%20Data%0A%20%20Conversion&body=Title%3A%20Instructify%3A%20Demystifying%20Metadata%20to%20Visual%20Instruction%20Tuning%20Data%0A%20%20Conversion%0AAuthor%3A%20Jacob%20Hansen%20and%20Wei%20Lin%20and%20Junmo%20Kang%20and%20Muhammad%20Jehanzeb%20Mirza%20and%20Hongyin%20Luo%20and%20Rogerio%20Feris%20and%20Alan%20Ritter%20and%20James%20Glass%20and%20Leonid%20Karlinsky%0AAbstract%3A%20%20%20Visual%20Instruction%20Tuning%20%28VisIT%29%20data%2C%20commonly%20available%20as%20human-assistant%0Aconversations%20with%20images%20interleaved%20in%20the%20human%20turns%2C%20are%20currently%20the%0Amost%20widespread%20vehicle%20for%20aligning%20strong%20LLMs%20to%20understand%20visual%20inputs%2C%0Aconverting%20them%20to%20strong%20LMMs.%20While%20many%20VisIT%20datasets%20are%20available%2C%20most%0Aare%20constructed%20using%20ad-hoc%20techniques%20developed%20independently%20by%20different%0Agroups.%20They%20are%20often%20poorly%20documented%2C%20lack%20reproducible%20code%2C%20and%20rely%20on%0Apaid%2C%20closed-source%20model%20APIs%20such%20as%20GPT-4%2C%20Gemini%2C%20or%20Claude%20to%20convert%0Aimage%20metadata%20%28labels%29%20into%20VisIT%20instructions.%20This%20leads%20to%20high%20costs%20and%0Amakes%20it%20challenging%20to%20scale%2C%20enhance%20quality%2C%20or%20generate%20VisIT%20data%20for%20new%0Adatasets.%20In%20this%20work%2C%20we%20address%20these%20challenges%20and%20propose%20an%20open%20and%0Aunified%20recipe%20and%20approach%2C~%5Ctextbf%7B%5Cmethod%7D%2C%20for%20converting%20available%0Ametadata%20to%20VisIT%20instructions%20using%20open%20LLMs.%20Our%20multi-stage%20%5Cmethod%0Afeatures%20an%20efficient%20framework%20for%20metadata%20grouping%2C%20quality%20control%2C%20data%0Aand%20prompt%20organization%2C%20and%20conversation%20sampling.%20We%20show%20that%20our%20approach%0Acan%20reproduce%20or%20enhance%20the%20data%20quality%20of%20available%20VisIT%20datasets%20when%0Aapplied%20to%20the%20same%20image%20data%20and%20metadata%20sources%2C%20improving%20GPT-4%20generated%0AVisIT%20instructions%20by%20~3%5C%25%20on%20average%20and%20up%20to%2012%5C%25%20on%20individual%20benchmarks%0Ausing%20open%20models%2C%20such%20as%20Gemma%202%2027B%20and%20LLaMa%203.1%2070B.%20Additionally%2C%20our%0Aapproach%20enables%20effective%20performance%20scaling%20-%20both%20in%20quantity%20and%20quality%20-%0Aby%20enhancing%20the%20resulting%20LMM%20performance%20across%20a%20wide%20range%20of%20benchmarks.%0AWe%20also%20analyze%20the%20impact%20of%20various%20factors%2C%20including%20conversation%20format%2C%0Abase%20model%20selection%2C%20and%20resampling%20strategies.%20Our%20code%2C%20which%20supports%20the%0Areproduction%20of%20equal%20or%20higher-quality%20VisIT%20datasets%20and%20facilities%20future%0Ametadata-to-VisIT%20data%20conversion%20for%20niche%20domains%2C%20is%20released%20at%0Ahttps%3A//github.com/jacob-hansen/Instructify.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18115v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructify%253A%2520Demystifying%2520Metadata%2520to%2520Visual%2520Instruction%2520Tuning%2520Data%250A%2520%2520Conversion%26entry.906535625%3DJacob%2520Hansen%2520and%2520Wei%2520Lin%2520and%2520Junmo%2520Kang%2520and%2520Muhammad%2520Jehanzeb%2520Mirza%2520and%2520Hongyin%2520Luo%2520and%2520Rogerio%2520Feris%2520and%2520Alan%2520Ritter%2520and%2520James%2520Glass%2520and%2520Leonid%2520Karlinsky%26entry.1292438233%3D%2520%2520Visual%2520Instruction%2520Tuning%2520%2528VisIT%2529%2520data%252C%2520commonly%2520available%2520as%2520human-assistant%250Aconversations%2520with%2520images%2520interleaved%2520in%2520the%2520human%2520turns%252C%2520are%2520currently%2520the%250Amost%2520widespread%2520vehicle%2520for%2520aligning%2520strong%2520LLMs%2520to%2520understand%2520visual%2520inputs%252C%250Aconverting%2520them%2520to%2520strong%2520LMMs.%2520While%2520many%2520VisIT%2520datasets%2520are%2520available%252C%2520most%250Aare%2520constructed%2520using%2520ad-hoc%2520techniques%2520developed%2520independently%2520by%2520different%250Agroups.%2520They%2520are%2520often%2520poorly%2520documented%252C%2520lack%2520reproducible%2520code%252C%2520and%2520rely%2520on%250Apaid%252C%2520closed-source%2520model%2520APIs%2520such%2520as%2520GPT-4%252C%2520Gemini%252C%2520or%2520Claude%2520to%2520convert%250Aimage%2520metadata%2520%2528labels%2529%2520into%2520VisIT%2520instructions.%2520This%2520leads%2520to%2520high%2520costs%2520and%250Amakes%2520it%2520challenging%2520to%2520scale%252C%2520enhance%2520quality%252C%2520or%2520generate%2520VisIT%2520data%2520for%2520new%250Adatasets.%2520In%2520this%2520work%252C%2520we%2520address%2520these%2520challenges%2520and%2520propose%2520an%2520open%2520and%250Aunified%2520recipe%2520and%2520approach%252C~%255Ctextbf%257B%255Cmethod%257D%252C%2520for%2520converting%2520available%250Ametadata%2520to%2520VisIT%2520instructions%2520using%2520open%2520LLMs.%2520Our%2520multi-stage%2520%255Cmethod%250Afeatures%2520an%2520efficient%2520framework%2520for%2520metadata%2520grouping%252C%2520quality%2520control%252C%2520data%250Aand%2520prompt%2520organization%252C%2520and%2520conversation%2520sampling.%2520We%2520show%2520that%2520our%2520approach%250Acan%2520reproduce%2520or%2520enhance%2520the%2520data%2520quality%2520of%2520available%2520VisIT%2520datasets%2520when%250Aapplied%2520to%2520the%2520same%2520image%2520data%2520and%2520metadata%2520sources%252C%2520improving%2520GPT-4%2520generated%250AVisIT%2520instructions%2520by%2520~3%255C%2525%2520on%2520average%2520and%2520up%2520to%252012%255C%2525%2520on%2520individual%2520benchmarks%250Ausing%2520open%2520models%252C%2520such%2520as%2520Gemma%25202%252027B%2520and%2520LLaMa%25203.1%252070B.%2520Additionally%252C%2520our%250Aapproach%2520enables%2520effective%2520performance%2520scaling%2520-%2520both%2520in%2520quantity%2520and%2520quality%2520-%250Aby%2520enhancing%2520the%2520resulting%2520LMM%2520performance%2520across%2520a%2520wide%2520range%2520of%2520benchmarks.%250AWe%2520also%2520analyze%2520the%2520impact%2520of%2520various%2520factors%252C%2520including%2520conversation%2520format%252C%250Abase%2520model%2520selection%252C%2520and%2520resampling%2520strategies.%2520Our%2520code%252C%2520which%2520supports%2520the%250Areproduction%2520of%2520equal%2520or%2520higher-quality%2520VisIT%2520datasets%2520and%2520facilities%2520future%250Ametadata-to-VisIT%2520data%2520conversion%2520for%2520niche%2520domains%252C%2520is%2520released%2520at%250Ahttps%253A//github.com/jacob-hansen/Instructify.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18115v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instructify%3A%20Demystifying%20Metadata%20to%20Visual%20Instruction%20Tuning%20Data%0A%20%20Conversion&entry.906535625=Jacob%20Hansen%20and%20Wei%20Lin%20and%20Junmo%20Kang%20and%20Muhammad%20Jehanzeb%20Mirza%20and%20Hongyin%20Luo%20and%20Rogerio%20Feris%20and%20Alan%20Ritter%20and%20James%20Glass%20and%20Leonid%20Karlinsky&entry.1292438233=%20%20Visual%20Instruction%20Tuning%20%28VisIT%29%20data%2C%20commonly%20available%20as%20human-assistant%0Aconversations%20with%20images%20interleaved%20in%20the%20human%20turns%2C%20are%20currently%20the%0Amost%20widespread%20vehicle%20for%20aligning%20strong%20LLMs%20to%20understand%20visual%20inputs%2C%0Aconverting%20them%20to%20strong%20LMMs.%20While%20many%20VisIT%20datasets%20are%20available%2C%20most%0Aare%20constructed%20using%20ad-hoc%20techniques%20developed%20independently%20by%20different%0Agroups.%20They%20are%20often%20poorly%20documented%2C%20lack%20reproducible%20code%2C%20and%20rely%20on%0Apaid%2C%20closed-source%20model%20APIs%20such%20as%20GPT-4%2C%20Gemini%2C%20or%20Claude%20to%20convert%0Aimage%20metadata%20%28labels%29%20into%20VisIT%20instructions.%20This%20leads%20to%20high%20costs%20and%0Amakes%20it%20challenging%20to%20scale%2C%20enhance%20quality%2C%20or%20generate%20VisIT%20data%20for%20new%0Adatasets.%20In%20this%20work%2C%20we%20address%20these%20challenges%20and%20propose%20an%20open%20and%0Aunified%20recipe%20and%20approach%2C~%5Ctextbf%7B%5Cmethod%7D%2C%20for%20converting%20available%0Ametadata%20to%20VisIT%20instructions%20using%20open%20LLMs.%20Our%20multi-stage%20%5Cmethod%0Afeatures%20an%20efficient%20framework%20for%20metadata%20grouping%2C%20quality%20control%2C%20data%0Aand%20prompt%20organization%2C%20and%20conversation%20sampling.%20We%20show%20that%20our%20approach%0Acan%20reproduce%20or%20enhance%20the%20data%20quality%20of%20available%20VisIT%20datasets%20when%0Aapplied%20to%20the%20same%20image%20data%20and%20metadata%20sources%2C%20improving%20GPT-4%20generated%0AVisIT%20instructions%20by%20~3%5C%25%20on%20average%20and%20up%20to%2012%5C%25%20on%20individual%20benchmarks%0Ausing%20open%20models%2C%20such%20as%20Gemma%202%2027B%20and%20LLaMa%203.1%2070B.%20Additionally%2C%20our%0Aapproach%20enables%20effective%20performance%20scaling%20-%20both%20in%20quantity%20and%20quality%20-%0Aby%20enhancing%20the%20resulting%20LMM%20performance%20across%20a%20wide%20range%20of%20benchmarks.%0AWe%20also%20analyze%20the%20impact%20of%20various%20factors%2C%20including%20conversation%20format%2C%0Abase%20model%20selection%2C%20and%20resampling%20strategies.%20Our%20code%2C%20which%20supports%20the%0Areproduction%20of%20equal%20or%20higher-quality%20VisIT%20datasets%20and%20facilities%20future%0Ametadata-to-VisIT%20data%20conversion%20for%20niche%20domains%2C%20is%20released%20at%0Ahttps%3A//github.com/jacob-hansen/Instructify.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18115v1&entry.124074799=Read"},
{"title": "Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online\n  Continual Learning", "author": "Congren Dai and Huichi Zhou and Jiahao Huang and Zhenxuan Zhang and Fanwen Wang and Guang Yang and Fei Ye", "abstract": "  Online Continual Learning (OCL) presents a complex learning environment in\nwhich new data arrives in a batch-to-batch online format, and the risk of\ncatastrophic forgetting can significantly impair model efficacy. In this study,\nwe address OCL by introducing an innovative memory framework that incorporates\na short-term memory system to retain dynamic information and a long-term memory\nsystem to archive enduring knowledge. Specifically, the long-term memory system\ncomprises a collection of sub-memory buffers, each linked to a cluster\nprototype and designed to retain data samples from distinct categories. We\npropose a novel $K$-means-based sample selection method to identify cluster\nprototypes for each encountered category. To safeguard essential and critical\nsamples, we introduce a novel memory optimisation strategy that selectively\nretains samples in the appropriate sub-memory buffer by evaluating each cluster\nprototype against incoming samples through an optimal transportation mechanism.\nThis approach specifically promotes each sub-memory buffer to retain data\nsamples that exhibit significant discrepancies from the corresponding cluster\nprototype, thereby ensuring the preservation of semantically rich information.\nIn addition, we propose a novel Divide-and-Conquer (DAC) approach that\nformulates the memory updating as an optimisation problem and divides it into\nseveral subproblems. As a result, the proposed DAC approach can solve these\nsubproblems separately and thus can significantly reduce computations of the\nproposed memory updating process. We conduct a series of experiments across\nstandard and imbalanced learning settings, and the empirical findings indicate\nthat the proposed memory framework achieves state-of-the-art performance in\nboth learning contexts.\n", "link": "http://arxiv.org/abs/2505.18101v1", "date": "2025-05-23", "relevancy": 2.6658, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5447}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5276}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Dual%20Buffer%20with%20Divide-and-Conquer%20Strategy%20for%20Online%0A%20%20Continual%20Learning&body=Title%3A%20Dynamic%20Dual%20Buffer%20with%20Divide-and-Conquer%20Strategy%20for%20Online%0A%20%20Continual%20Learning%0AAuthor%3A%20Congren%20Dai%20and%20Huichi%20Zhou%20and%20Jiahao%20Huang%20and%20Zhenxuan%20Zhang%20and%20Fanwen%20Wang%20and%20Guang%20Yang%20and%20Fei%20Ye%0AAbstract%3A%20%20%20Online%20Continual%20Learning%20%28OCL%29%20presents%20a%20complex%20learning%20environment%20in%0Awhich%20new%20data%20arrives%20in%20a%20batch-to-batch%20online%20format%2C%20and%20the%20risk%20of%0Acatastrophic%20forgetting%20can%20significantly%20impair%20model%20efficacy.%20In%20this%20study%2C%0Awe%20address%20OCL%20by%20introducing%20an%20innovative%20memory%20framework%20that%20incorporates%0Aa%20short-term%20memory%20system%20to%20retain%20dynamic%20information%20and%20a%20long-term%20memory%0Asystem%20to%20archive%20enduring%20knowledge.%20Specifically%2C%20the%20long-term%20memory%20system%0Acomprises%20a%20collection%20of%20sub-memory%20buffers%2C%20each%20linked%20to%20a%20cluster%0Aprototype%20and%20designed%20to%20retain%20data%20samples%20from%20distinct%20categories.%20We%0Apropose%20a%20novel%20%24K%24-means-based%20sample%20selection%20method%20to%20identify%20cluster%0Aprototypes%20for%20each%20encountered%20category.%20To%20safeguard%20essential%20and%20critical%0Asamples%2C%20we%20introduce%20a%20novel%20memory%20optimisation%20strategy%20that%20selectively%0Aretains%20samples%20in%20the%20appropriate%20sub-memory%20buffer%20by%20evaluating%20each%20cluster%0Aprototype%20against%20incoming%20samples%20through%20an%20optimal%20transportation%20mechanism.%0AThis%20approach%20specifically%20promotes%20each%20sub-memory%20buffer%20to%20retain%20data%0Asamples%20that%20exhibit%20significant%20discrepancies%20from%20the%20corresponding%20cluster%0Aprototype%2C%20thereby%20ensuring%20the%20preservation%20of%20semantically%20rich%20information.%0AIn%20addition%2C%20we%20propose%20a%20novel%20Divide-and-Conquer%20%28DAC%29%20approach%20that%0Aformulates%20the%20memory%20updating%20as%20an%20optimisation%20problem%20and%20divides%20it%20into%0Aseveral%20subproblems.%20As%20a%20result%2C%20the%20proposed%20DAC%20approach%20can%20solve%20these%0Asubproblems%20separately%20and%20thus%20can%20significantly%20reduce%20computations%20of%20the%0Aproposed%20memory%20updating%20process.%20We%20conduct%20a%20series%20of%20experiments%20across%0Astandard%20and%20imbalanced%20learning%20settings%2C%20and%20the%20empirical%20findings%20indicate%0Athat%20the%20proposed%20memory%20framework%20achieves%20state-of-the-art%20performance%20in%0Aboth%20learning%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Dual%2520Buffer%2520with%2520Divide-and-Conquer%2520Strategy%2520for%2520Online%250A%2520%2520Continual%2520Learning%26entry.906535625%3DCongren%2520Dai%2520and%2520Huichi%2520Zhou%2520and%2520Jiahao%2520Huang%2520and%2520Zhenxuan%2520Zhang%2520and%2520Fanwen%2520Wang%2520and%2520Guang%2520Yang%2520and%2520Fei%2520Ye%26entry.1292438233%3D%2520%2520Online%2520Continual%2520Learning%2520%2528OCL%2529%2520presents%2520a%2520complex%2520learning%2520environment%2520in%250Awhich%2520new%2520data%2520arrives%2520in%2520a%2520batch-to-batch%2520online%2520format%252C%2520and%2520the%2520risk%2520of%250Acatastrophic%2520forgetting%2520can%2520significantly%2520impair%2520model%2520efficacy.%2520In%2520this%2520study%252C%250Awe%2520address%2520OCL%2520by%2520introducing%2520an%2520innovative%2520memory%2520framework%2520that%2520incorporates%250Aa%2520short-term%2520memory%2520system%2520to%2520retain%2520dynamic%2520information%2520and%2520a%2520long-term%2520memory%250Asystem%2520to%2520archive%2520enduring%2520knowledge.%2520Specifically%252C%2520the%2520long-term%2520memory%2520system%250Acomprises%2520a%2520collection%2520of%2520sub-memory%2520buffers%252C%2520each%2520linked%2520to%2520a%2520cluster%250Aprototype%2520and%2520designed%2520to%2520retain%2520data%2520samples%2520from%2520distinct%2520categories.%2520We%250Apropose%2520a%2520novel%2520%2524K%2524-means-based%2520sample%2520selection%2520method%2520to%2520identify%2520cluster%250Aprototypes%2520for%2520each%2520encountered%2520category.%2520To%2520safeguard%2520essential%2520and%2520critical%250Asamples%252C%2520we%2520introduce%2520a%2520novel%2520memory%2520optimisation%2520strategy%2520that%2520selectively%250Aretains%2520samples%2520in%2520the%2520appropriate%2520sub-memory%2520buffer%2520by%2520evaluating%2520each%2520cluster%250Aprototype%2520against%2520incoming%2520samples%2520through%2520an%2520optimal%2520transportation%2520mechanism.%250AThis%2520approach%2520specifically%2520promotes%2520each%2520sub-memory%2520buffer%2520to%2520retain%2520data%250Asamples%2520that%2520exhibit%2520significant%2520discrepancies%2520from%2520the%2520corresponding%2520cluster%250Aprototype%252C%2520thereby%2520ensuring%2520the%2520preservation%2520of%2520semantically%2520rich%2520information.%250AIn%2520addition%252C%2520we%2520propose%2520a%2520novel%2520Divide-and-Conquer%2520%2528DAC%2529%2520approach%2520that%250Aformulates%2520the%2520memory%2520updating%2520as%2520an%2520optimisation%2520problem%2520and%2520divides%2520it%2520into%250Aseveral%2520subproblems.%2520As%2520a%2520result%252C%2520the%2520proposed%2520DAC%2520approach%2520can%2520solve%2520these%250Asubproblems%2520separately%2520and%2520thus%2520can%2520significantly%2520reduce%2520computations%2520of%2520the%250Aproposed%2520memory%2520updating%2520process.%2520We%2520conduct%2520a%2520series%2520of%2520experiments%2520across%250Astandard%2520and%2520imbalanced%2520learning%2520settings%252C%2520and%2520the%2520empirical%2520findings%2520indicate%250Athat%2520the%2520proposed%2520memory%2520framework%2520achieves%2520state-of-the-art%2520performance%2520in%250Aboth%2520learning%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Dual%20Buffer%20with%20Divide-and-Conquer%20Strategy%20for%20Online%0A%20%20Continual%20Learning&entry.906535625=Congren%20Dai%20and%20Huichi%20Zhou%20and%20Jiahao%20Huang%20and%20Zhenxuan%20Zhang%20and%20Fanwen%20Wang%20and%20Guang%20Yang%20and%20Fei%20Ye&entry.1292438233=%20%20Online%20Continual%20Learning%20%28OCL%29%20presents%20a%20complex%20learning%20environment%20in%0Awhich%20new%20data%20arrives%20in%20a%20batch-to-batch%20online%20format%2C%20and%20the%20risk%20of%0Acatastrophic%20forgetting%20can%20significantly%20impair%20model%20efficacy.%20In%20this%20study%2C%0Awe%20address%20OCL%20by%20introducing%20an%20innovative%20memory%20framework%20that%20incorporates%0Aa%20short-term%20memory%20system%20to%20retain%20dynamic%20information%20and%20a%20long-term%20memory%0Asystem%20to%20archive%20enduring%20knowledge.%20Specifically%2C%20the%20long-term%20memory%20system%0Acomprises%20a%20collection%20of%20sub-memory%20buffers%2C%20each%20linked%20to%20a%20cluster%0Aprototype%20and%20designed%20to%20retain%20data%20samples%20from%20distinct%20categories.%20We%0Apropose%20a%20novel%20%24K%24-means-based%20sample%20selection%20method%20to%20identify%20cluster%0Aprototypes%20for%20each%20encountered%20category.%20To%20safeguard%20essential%20and%20critical%0Asamples%2C%20we%20introduce%20a%20novel%20memory%20optimisation%20strategy%20that%20selectively%0Aretains%20samples%20in%20the%20appropriate%20sub-memory%20buffer%20by%20evaluating%20each%20cluster%0Aprototype%20against%20incoming%20samples%20through%20an%20optimal%20transportation%20mechanism.%0AThis%20approach%20specifically%20promotes%20each%20sub-memory%20buffer%20to%20retain%20data%0Asamples%20that%20exhibit%20significant%20discrepancies%20from%20the%20corresponding%20cluster%0Aprototype%2C%20thereby%20ensuring%20the%20preservation%20of%20semantically%20rich%20information.%0AIn%20addition%2C%20we%20propose%20a%20novel%20Divide-and-Conquer%20%28DAC%29%20approach%20that%0Aformulates%20the%20memory%20updating%20as%20an%20optimisation%20problem%20and%20divides%20it%20into%0Aseveral%20subproblems.%20As%20a%20result%2C%20the%20proposed%20DAC%20approach%20can%20solve%20these%0Asubproblems%20separately%20and%20thus%20can%20significantly%20reduce%20computations%20of%20the%0Aproposed%20memory%20updating%20process.%20We%20conduct%20a%20series%20of%20experiments%20across%0Astandard%20and%20imbalanced%20learning%20settings%2C%20and%20the%20empirical%20findings%20indicate%0Athat%20the%20proposed%20memory%20framework%20achieves%20state-of-the-art%20performance%20in%0Aboth%20learning%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18101v1&entry.124074799=Read"},
{"title": "VLM Models and Automated Grading of Atopic Dermatitis", "author": "Marc Lalonde and Hamed Ghodrati", "abstract": "  The task of grading atopic dermatitis (or AD, a form of eczema) from patient\nimages is difficult even for trained dermatologists. Research on automating\nthis task has progressed in recent years with the development of deep learning\nsolutions; however, the rapid evolution of multimodal models and more\nspecifically vision-language models (VLMs) opens the door to new possibilities\nin terms of explainable assessment of medical images, including dermatology.\nThis report describes experiments carried out to evaluate the ability of seven\nVLMs to assess the severity of AD on a set of test images.\n", "link": "http://arxiv.org/abs/2505.17835v1", "date": "2025-05-23", "relevancy": 2.6353, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLM%20Models%20and%20Automated%20Grading%20of%20Atopic%20Dermatitis&body=Title%3A%20VLM%20Models%20and%20Automated%20Grading%20of%20Atopic%20Dermatitis%0AAuthor%3A%20Marc%20Lalonde%20and%20Hamed%20Ghodrati%0AAbstract%3A%20%20%20The%20task%20of%20grading%20atopic%20dermatitis%20%28or%20AD%2C%20a%20form%20of%20eczema%29%20from%20patient%0Aimages%20is%20difficult%20even%20for%20trained%20dermatologists.%20Research%20on%20automating%0Athis%20task%20has%20progressed%20in%20recent%20years%20with%20the%20development%20of%20deep%20learning%0Asolutions%3B%20however%2C%20the%20rapid%20evolution%20of%20multimodal%20models%20and%20more%0Aspecifically%20vision-language%20models%20%28VLMs%29%20opens%20the%20door%20to%20new%20possibilities%0Ain%20terms%20of%20explainable%20assessment%20of%20medical%20images%2C%20including%20dermatology.%0AThis%20report%20describes%20experiments%20carried%20out%20to%20evaluate%20the%20ability%20of%20seven%0AVLMs%20to%20assess%20the%20severity%20of%20AD%20on%20a%20set%20of%20test%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLM%2520Models%2520and%2520Automated%2520Grading%2520of%2520Atopic%2520Dermatitis%26entry.906535625%3DMarc%2520Lalonde%2520and%2520Hamed%2520Ghodrati%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520grading%2520atopic%2520dermatitis%2520%2528or%2520AD%252C%2520a%2520form%2520of%2520eczema%2529%2520from%2520patient%250Aimages%2520is%2520difficult%2520even%2520for%2520trained%2520dermatologists.%2520Research%2520on%2520automating%250Athis%2520task%2520has%2520progressed%2520in%2520recent%2520years%2520with%2520the%2520development%2520of%2520deep%2520learning%250Asolutions%253B%2520however%252C%2520the%2520rapid%2520evolution%2520of%2520multimodal%2520models%2520and%2520more%250Aspecifically%2520vision-language%2520models%2520%2528VLMs%2529%2520opens%2520the%2520door%2520to%2520new%2520possibilities%250Ain%2520terms%2520of%2520explainable%2520assessment%2520of%2520medical%2520images%252C%2520including%2520dermatology.%250AThis%2520report%2520describes%2520experiments%2520carried%2520out%2520to%2520evaluate%2520the%2520ability%2520of%2520seven%250AVLMs%2520to%2520assess%2520the%2520severity%2520of%2520AD%2520on%2520a%2520set%2520of%2520test%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLM%20Models%20and%20Automated%20Grading%20of%20Atopic%20Dermatitis&entry.906535625=Marc%20Lalonde%20and%20Hamed%20Ghodrati&entry.1292438233=%20%20The%20task%20of%20grading%20atopic%20dermatitis%20%28or%20AD%2C%20a%20form%20of%20eczema%29%20from%20patient%0Aimages%20is%20difficult%20even%20for%20trained%20dermatologists.%20Research%20on%20automating%0Athis%20task%20has%20progressed%20in%20recent%20years%20with%20the%20development%20of%20deep%20learning%0Asolutions%3B%20however%2C%20the%20rapid%20evolution%20of%20multimodal%20models%20and%20more%0Aspecifically%20vision-language%20models%20%28VLMs%29%20opens%20the%20door%20to%20new%20possibilities%0Ain%20terms%20of%20explainable%20assessment%20of%20medical%20images%2C%20including%20dermatology.%0AThis%20report%20describes%20experiments%20carried%20out%20to%20evaluate%20the%20ability%20of%20seven%0AVLMs%20to%20assess%20the%20severity%20of%20AD%20on%20a%20set%20of%20test%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17835v1&entry.124074799=Read"},
{"title": "CLDyB: Towards Dynamic Benchmarking for Continual Learning with\n  Pre-trained Models", "author": "Shengzhuang Chen and Yikai Liao and Xiaoxiao Sun and Kede Ma and Ying Wei", "abstract": "  The advent of the foundation model era has sparked significant research\ninterest in leveraging pre-trained representations for continual learning (CL),\nyielding a series of top-performing CL methods on standard evaluation\nbenchmarks. Nonetheless, there are growing concerns regarding potential data\ncontamination during the pre-training stage. Furthermore, standard evaluation\nbenchmarks, which are typically static, fail to capture the complexities of\nreal-world CL scenarios, resulting in saturated performance. To address these\nissues, we describe CL on dynamic benchmarks (CLDyB), a general computational\nframework based on Markov decision processes for evaluating CL methods\nreliably. CLDyB dynamically identifies inherently difficult and\nalgorithm-dependent tasks for the given CL methods, and determines challenging\ntask orders using Monte Carlo tree search. Leveraging CLDyB, we first conduct a\njoint evaluation of multiple state-of-the-art CL methods, leading to a set of\ncommonly challenging and generalizable task sequences where existing CL methods\ntend to perform poorly. We then conduct separate evaluations of individual CL\nmethods using CLDyB, discovering their respective strengths and weaknesses. The\nsource code and generated task sequences are publicly accessible at\nhttps://github.com/szc12153/CLDyB.\n", "link": "http://arxiv.org/abs/2503.04655v2", "date": "2025-05-23", "relevancy": 2.6229, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5249}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLDyB%3A%20Towards%20Dynamic%20Benchmarking%20for%20Continual%20Learning%20with%0A%20%20Pre-trained%20Models&body=Title%3A%20CLDyB%3A%20Towards%20Dynamic%20Benchmarking%20for%20Continual%20Learning%20with%0A%20%20Pre-trained%20Models%0AAuthor%3A%20Shengzhuang%20Chen%20and%20Yikai%20Liao%20and%20Xiaoxiao%20Sun%20and%20Kede%20Ma%20and%20Ying%20Wei%0AAbstract%3A%20%20%20The%20advent%20of%20the%20foundation%20model%20era%20has%20sparked%20significant%20research%0Ainterest%20in%20leveraging%20pre-trained%20representations%20for%20continual%20learning%20%28CL%29%2C%0Ayielding%20a%20series%20of%20top-performing%20CL%20methods%20on%20standard%20evaluation%0Abenchmarks.%20Nonetheless%2C%20there%20are%20growing%20concerns%20regarding%20potential%20data%0Acontamination%20during%20the%20pre-training%20stage.%20Furthermore%2C%20standard%20evaluation%0Abenchmarks%2C%20which%20are%20typically%20static%2C%20fail%20to%20capture%20the%20complexities%20of%0Areal-world%20CL%20scenarios%2C%20resulting%20in%20saturated%20performance.%20To%20address%20these%0Aissues%2C%20we%20describe%20CL%20on%20dynamic%20benchmarks%20%28CLDyB%29%2C%20a%20general%20computational%0Aframework%20based%20on%20Markov%20decision%20processes%20for%20evaluating%20CL%20methods%0Areliably.%20CLDyB%20dynamically%20identifies%20inherently%20difficult%20and%0Aalgorithm-dependent%20tasks%20for%20the%20given%20CL%20methods%2C%20and%20determines%20challenging%0Atask%20orders%20using%20Monte%20Carlo%20tree%20search.%20Leveraging%20CLDyB%2C%20we%20first%20conduct%20a%0Ajoint%20evaluation%20of%20multiple%20state-of-the-art%20CL%20methods%2C%20leading%20to%20a%20set%20of%0Acommonly%20challenging%20and%20generalizable%20task%20sequences%20where%20existing%20CL%20methods%0Atend%20to%20perform%20poorly.%20We%20then%20conduct%20separate%20evaluations%20of%20individual%20CL%0Amethods%20using%20CLDyB%2C%20discovering%20their%20respective%20strengths%20and%20weaknesses.%20The%0Asource%20code%20and%20generated%20task%20sequences%20are%20publicly%20accessible%20at%0Ahttps%3A//github.com/szc12153/CLDyB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLDyB%253A%2520Towards%2520Dynamic%2520Benchmarking%2520for%2520Continual%2520Learning%2520with%250A%2520%2520Pre-trained%2520Models%26entry.906535625%3DShengzhuang%2520Chen%2520and%2520Yikai%2520Liao%2520and%2520Xiaoxiao%2520Sun%2520and%2520Kede%2520Ma%2520and%2520Ying%2520Wei%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520the%2520foundation%2520model%2520era%2520has%2520sparked%2520significant%2520research%250Ainterest%2520in%2520leveraging%2520pre-trained%2520representations%2520for%2520continual%2520learning%2520%2528CL%2529%252C%250Ayielding%2520a%2520series%2520of%2520top-performing%2520CL%2520methods%2520on%2520standard%2520evaluation%250Abenchmarks.%2520Nonetheless%252C%2520there%2520are%2520growing%2520concerns%2520regarding%2520potential%2520data%250Acontamination%2520during%2520the%2520pre-training%2520stage.%2520Furthermore%252C%2520standard%2520evaluation%250Abenchmarks%252C%2520which%2520are%2520typically%2520static%252C%2520fail%2520to%2520capture%2520the%2520complexities%2520of%250Areal-world%2520CL%2520scenarios%252C%2520resulting%2520in%2520saturated%2520performance.%2520To%2520address%2520these%250Aissues%252C%2520we%2520describe%2520CL%2520on%2520dynamic%2520benchmarks%2520%2528CLDyB%2529%252C%2520a%2520general%2520computational%250Aframework%2520based%2520on%2520Markov%2520decision%2520processes%2520for%2520evaluating%2520CL%2520methods%250Areliably.%2520CLDyB%2520dynamically%2520identifies%2520inherently%2520difficult%2520and%250Aalgorithm-dependent%2520tasks%2520for%2520the%2520given%2520CL%2520methods%252C%2520and%2520determines%2520challenging%250Atask%2520orders%2520using%2520Monte%2520Carlo%2520tree%2520search.%2520Leveraging%2520CLDyB%252C%2520we%2520first%2520conduct%2520a%250Ajoint%2520evaluation%2520of%2520multiple%2520state-of-the-art%2520CL%2520methods%252C%2520leading%2520to%2520a%2520set%2520of%250Acommonly%2520challenging%2520and%2520generalizable%2520task%2520sequences%2520where%2520existing%2520CL%2520methods%250Atend%2520to%2520perform%2520poorly.%2520We%2520then%2520conduct%2520separate%2520evaluations%2520of%2520individual%2520CL%250Amethods%2520using%2520CLDyB%252C%2520discovering%2520their%2520respective%2520strengths%2520and%2520weaknesses.%2520The%250Asource%2520code%2520and%2520generated%2520task%2520sequences%2520are%2520publicly%2520accessible%2520at%250Ahttps%253A//github.com/szc12153/CLDyB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLDyB%3A%20Towards%20Dynamic%20Benchmarking%20for%20Continual%20Learning%20with%0A%20%20Pre-trained%20Models&entry.906535625=Shengzhuang%20Chen%20and%20Yikai%20Liao%20and%20Xiaoxiao%20Sun%20and%20Kede%20Ma%20and%20Ying%20Wei&entry.1292438233=%20%20The%20advent%20of%20the%20foundation%20model%20era%20has%20sparked%20significant%20research%0Ainterest%20in%20leveraging%20pre-trained%20representations%20for%20continual%20learning%20%28CL%29%2C%0Ayielding%20a%20series%20of%20top-performing%20CL%20methods%20on%20standard%20evaluation%0Abenchmarks.%20Nonetheless%2C%20there%20are%20growing%20concerns%20regarding%20potential%20data%0Acontamination%20during%20the%20pre-training%20stage.%20Furthermore%2C%20standard%20evaluation%0Abenchmarks%2C%20which%20are%20typically%20static%2C%20fail%20to%20capture%20the%20complexities%20of%0Areal-world%20CL%20scenarios%2C%20resulting%20in%20saturated%20performance.%20To%20address%20these%0Aissues%2C%20we%20describe%20CL%20on%20dynamic%20benchmarks%20%28CLDyB%29%2C%20a%20general%20computational%0Aframework%20based%20on%20Markov%20decision%20processes%20for%20evaluating%20CL%20methods%0Areliably.%20CLDyB%20dynamically%20identifies%20inherently%20difficult%20and%0Aalgorithm-dependent%20tasks%20for%20the%20given%20CL%20methods%2C%20and%20determines%20challenging%0Atask%20orders%20using%20Monte%20Carlo%20tree%20search.%20Leveraging%20CLDyB%2C%20we%20first%20conduct%20a%0Ajoint%20evaluation%20of%20multiple%20state-of-the-art%20CL%20methods%2C%20leading%20to%20a%20set%20of%0Acommonly%20challenging%20and%20generalizable%20task%20sequences%20where%20existing%20CL%20methods%0Atend%20to%20perform%20poorly.%20We%20then%20conduct%20separate%20evaluations%20of%20individual%20CL%0Amethods%20using%20CLDyB%2C%20discovering%20their%20respective%20strengths%20and%20weaknesses.%20The%0Asource%20code%20and%20generated%20task%20sequences%20are%20publicly%20accessible%20at%0Ahttps%3A//github.com/szc12153/CLDyB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04655v2&entry.124074799=Read"},
{"title": "BOTM: Echocardiography Segmentation via Bi-directional Optimal Token\n  Matching", "author": "Zhihua Liu and Lei Tong and Xilin He and Che Liu and Rossella Arcucci and Chen Jin and Huiyu Zhou", "abstract": "  Existed echocardiography segmentation methods often suffer from anatomical\ninconsistency challenge caused by shape variation, partial observation and\nregion ambiguity with similar intensity across 2D echocardiographic sequences,\nresulting in false positive segmentation with anatomical defeated structures in\nchallenging low signal-to-noise ratio conditions. To provide a strong\nanatomical guarantee across different echocardiographic frames, we propose a\nnovel segmentation framework named BOTM (Bi-directional Optimal Token Matching)\nthat performs echocardiography segmentation and optimal anatomy transportation\nsimultaneously. Given paired echocardiographic images, BOTM learns to match two\nsets of discrete image tokens by finding optimal correspondences from a novel\nanatomical transportation perspective. We further extend the token matching\ninto a bi-directional cross-transport attention proxy to regulate the preserved\nanatomical consistency within the cardiac cyclic deformation in temporal\ndomain. Extensive experimental results show that BOTM can generate stable and\naccurate segmentation outcomes (e.g. -1.917 HD on CAMUS2H LV, +1.9% Dice on\nTED), and provide a better matching interpretation with anatomical consistency\nguarantee.\n", "link": "http://arxiv.org/abs/2505.18052v1", "date": "2025-05-23", "relevancy": 2.6211, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5446}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5324}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BOTM%3A%20Echocardiography%20Segmentation%20via%20Bi-directional%20Optimal%20Token%0A%20%20Matching&body=Title%3A%20BOTM%3A%20Echocardiography%20Segmentation%20via%20Bi-directional%20Optimal%20Token%0A%20%20Matching%0AAuthor%3A%20Zhihua%20Liu%20and%20Lei%20Tong%20and%20Xilin%20He%20and%20Che%20Liu%20and%20Rossella%20Arcucci%20and%20Chen%20Jin%20and%20Huiyu%20Zhou%0AAbstract%3A%20%20%20Existed%20echocardiography%20segmentation%20methods%20often%20suffer%20from%20anatomical%0Ainconsistency%20challenge%20caused%20by%20shape%20variation%2C%20partial%20observation%20and%0Aregion%20ambiguity%20with%20similar%20intensity%20across%202D%20echocardiographic%20sequences%2C%0Aresulting%20in%20false%20positive%20segmentation%20with%20anatomical%20defeated%20structures%20in%0Achallenging%20low%20signal-to-noise%20ratio%20conditions.%20To%20provide%20a%20strong%0Aanatomical%20guarantee%20across%20different%20echocardiographic%20frames%2C%20we%20propose%20a%0Anovel%20segmentation%20framework%20named%20BOTM%20%28Bi-directional%20Optimal%20Token%20Matching%29%0Athat%20performs%20echocardiography%20segmentation%20and%20optimal%20anatomy%20transportation%0Asimultaneously.%20Given%20paired%20echocardiographic%20images%2C%20BOTM%20learns%20to%20match%20two%0Asets%20of%20discrete%20image%20tokens%20by%20finding%20optimal%20correspondences%20from%20a%20novel%0Aanatomical%20transportation%20perspective.%20We%20further%20extend%20the%20token%20matching%0Ainto%20a%20bi-directional%20cross-transport%20attention%20proxy%20to%20regulate%20the%20preserved%0Aanatomical%20consistency%20within%20the%20cardiac%20cyclic%20deformation%20in%20temporal%0Adomain.%20Extensive%20experimental%20results%20show%20that%20BOTM%20can%20generate%20stable%20and%0Aaccurate%20segmentation%20outcomes%20%28e.g.%20-1.917%20HD%20on%20CAMUS2H%20LV%2C%20%2B1.9%25%20Dice%20on%0ATED%29%2C%20and%20provide%20a%20better%20matching%20interpretation%20with%20anatomical%20consistency%0Aguarantee.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBOTM%253A%2520Echocardiography%2520Segmentation%2520via%2520Bi-directional%2520Optimal%2520Token%250A%2520%2520Matching%26entry.906535625%3DZhihua%2520Liu%2520and%2520Lei%2520Tong%2520and%2520Xilin%2520He%2520and%2520Che%2520Liu%2520and%2520Rossella%2520Arcucci%2520and%2520Chen%2520Jin%2520and%2520Huiyu%2520Zhou%26entry.1292438233%3D%2520%2520Existed%2520echocardiography%2520segmentation%2520methods%2520often%2520suffer%2520from%2520anatomical%250Ainconsistency%2520challenge%2520caused%2520by%2520shape%2520variation%252C%2520partial%2520observation%2520and%250Aregion%2520ambiguity%2520with%2520similar%2520intensity%2520across%25202D%2520echocardiographic%2520sequences%252C%250Aresulting%2520in%2520false%2520positive%2520segmentation%2520with%2520anatomical%2520defeated%2520structures%2520in%250Achallenging%2520low%2520signal-to-noise%2520ratio%2520conditions.%2520To%2520provide%2520a%2520strong%250Aanatomical%2520guarantee%2520across%2520different%2520echocardiographic%2520frames%252C%2520we%2520propose%2520a%250Anovel%2520segmentation%2520framework%2520named%2520BOTM%2520%2528Bi-directional%2520Optimal%2520Token%2520Matching%2529%250Athat%2520performs%2520echocardiography%2520segmentation%2520and%2520optimal%2520anatomy%2520transportation%250Asimultaneously.%2520Given%2520paired%2520echocardiographic%2520images%252C%2520BOTM%2520learns%2520to%2520match%2520two%250Asets%2520of%2520discrete%2520image%2520tokens%2520by%2520finding%2520optimal%2520correspondences%2520from%2520a%2520novel%250Aanatomical%2520transportation%2520perspective.%2520We%2520further%2520extend%2520the%2520token%2520matching%250Ainto%2520a%2520bi-directional%2520cross-transport%2520attention%2520proxy%2520to%2520regulate%2520the%2520preserved%250Aanatomical%2520consistency%2520within%2520the%2520cardiac%2520cyclic%2520deformation%2520in%2520temporal%250Adomain.%2520Extensive%2520experimental%2520results%2520show%2520that%2520BOTM%2520can%2520generate%2520stable%2520and%250Aaccurate%2520segmentation%2520outcomes%2520%2528e.g.%2520-1.917%2520HD%2520on%2520CAMUS2H%2520LV%252C%2520%252B1.9%2525%2520Dice%2520on%250ATED%2529%252C%2520and%2520provide%2520a%2520better%2520matching%2520interpretation%2520with%2520anatomical%2520consistency%250Aguarantee.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BOTM%3A%20Echocardiography%20Segmentation%20via%20Bi-directional%20Optimal%20Token%0A%20%20Matching&entry.906535625=Zhihua%20Liu%20and%20Lei%20Tong%20and%20Xilin%20He%20and%20Che%20Liu%20and%20Rossella%20Arcucci%20and%20Chen%20Jin%20and%20Huiyu%20Zhou&entry.1292438233=%20%20Existed%20echocardiography%20segmentation%20methods%20often%20suffer%20from%20anatomical%0Ainconsistency%20challenge%20caused%20by%20shape%20variation%2C%20partial%20observation%20and%0Aregion%20ambiguity%20with%20similar%20intensity%20across%202D%20echocardiographic%20sequences%2C%0Aresulting%20in%20false%20positive%20segmentation%20with%20anatomical%20defeated%20structures%20in%0Achallenging%20low%20signal-to-noise%20ratio%20conditions.%20To%20provide%20a%20strong%0Aanatomical%20guarantee%20across%20different%20echocardiographic%20frames%2C%20we%20propose%20a%0Anovel%20segmentation%20framework%20named%20BOTM%20%28Bi-directional%20Optimal%20Token%20Matching%29%0Athat%20performs%20echocardiography%20segmentation%20and%20optimal%20anatomy%20transportation%0Asimultaneously.%20Given%20paired%20echocardiographic%20images%2C%20BOTM%20learns%20to%20match%20two%0Asets%20of%20discrete%20image%20tokens%20by%20finding%20optimal%20correspondences%20from%20a%20novel%0Aanatomical%20transportation%20perspective.%20We%20further%20extend%20the%20token%20matching%0Ainto%20a%20bi-directional%20cross-transport%20attention%20proxy%20to%20regulate%20the%20preserved%0Aanatomical%20consistency%20within%20the%20cardiac%20cyclic%20deformation%20in%20temporal%0Adomain.%20Extensive%20experimental%20results%20show%20that%20BOTM%20can%20generate%20stable%20and%0Aaccurate%20segmentation%20outcomes%20%28e.g.%20-1.917%20HD%20on%20CAMUS2H%20LV%2C%20%2B1.9%25%20Dice%20on%0ATED%29%2C%20and%20provide%20a%20better%20matching%20interpretation%20with%20anatomical%20consistency%0Aguarantee.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18052v1&entry.124074799=Read"},
{"title": "Locality-Sensitive Hashing for Efficient Hard Negative Sampling in\n  Contrastive Learning", "author": "Fabian Deuser and Philipp Hausenblas and Hannah Schieber and Daniel Roth and Martin Werner and Norbert Oswald", "abstract": "  Contrastive learning is a representational learning paradigm in which a\nneural network maps data elements to feature vectors. It improves the feature\nspace by forming lots with an anchor and examples that are either positive or\nnegative based on class similarity. Hard negative examples, which are close to\nthe anchor in the feature space but from a different class, improve learning\nperformance. Finding such examples of high quality efficiently in large,\nhigh-dimensional datasets is computationally challenging. In this paper, we\npropose a GPU-friendly Locality-Sensitive Hashing (LSH) scheme that quantizes\nreal-valued feature vectors into binary representations for approximate nearest\nneighbor search. We investigate its theoretical properties and evaluate it on\nseveral datasets from textual and visual domain. Our approach achieves\ncomparable or better performance while requiring significantly less computation\nthan existing hard negative mining strategies.\n", "link": "http://arxiv.org/abs/2505.17844v1", "date": "2025-05-23", "relevancy": 2.6051, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5423}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.512}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locality-Sensitive%20Hashing%20for%20Efficient%20Hard%20Negative%20Sampling%20in%0A%20%20Contrastive%20Learning&body=Title%3A%20Locality-Sensitive%20Hashing%20for%20Efficient%20Hard%20Negative%20Sampling%20in%0A%20%20Contrastive%20Learning%0AAuthor%3A%20Fabian%20Deuser%20and%20Philipp%20Hausenblas%20and%20Hannah%20Schieber%20and%20Daniel%20Roth%20and%20Martin%20Werner%20and%20Norbert%20Oswald%0AAbstract%3A%20%20%20Contrastive%20learning%20is%20a%20representational%20learning%20paradigm%20in%20which%20a%0Aneural%20network%20maps%20data%20elements%20to%20feature%20vectors.%20It%20improves%20the%20feature%0Aspace%20by%20forming%20lots%20with%20an%20anchor%20and%20examples%20that%20are%20either%20positive%20or%0Anegative%20based%20on%20class%20similarity.%20Hard%20negative%20examples%2C%20which%20are%20close%20to%0Athe%20anchor%20in%20the%20feature%20space%20but%20from%20a%20different%20class%2C%20improve%20learning%0Aperformance.%20Finding%20such%20examples%20of%20high%20quality%20efficiently%20in%20large%2C%0Ahigh-dimensional%20datasets%20is%20computationally%20challenging.%20In%20this%20paper%2C%20we%0Apropose%20a%20GPU-friendly%20Locality-Sensitive%20Hashing%20%28LSH%29%20scheme%20that%20quantizes%0Areal-valued%20feature%20vectors%20into%20binary%20representations%20for%20approximate%20nearest%0Aneighbor%20search.%20We%20investigate%20its%20theoretical%20properties%20and%20evaluate%20it%20on%0Aseveral%20datasets%20from%20textual%20and%20visual%20domain.%20Our%20approach%20achieves%0Acomparable%20or%20better%20performance%20while%20requiring%20significantly%20less%20computation%0Athan%20existing%20hard%20negative%20mining%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocality-Sensitive%2520Hashing%2520for%2520Efficient%2520Hard%2520Negative%2520Sampling%2520in%250A%2520%2520Contrastive%2520Learning%26entry.906535625%3DFabian%2520Deuser%2520and%2520Philipp%2520Hausenblas%2520and%2520Hannah%2520Schieber%2520and%2520Daniel%2520Roth%2520and%2520Martin%2520Werner%2520and%2520Norbert%2520Oswald%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520is%2520a%2520representational%2520learning%2520paradigm%2520in%2520which%2520a%250Aneural%2520network%2520maps%2520data%2520elements%2520to%2520feature%2520vectors.%2520It%2520improves%2520the%2520feature%250Aspace%2520by%2520forming%2520lots%2520with%2520an%2520anchor%2520and%2520examples%2520that%2520are%2520either%2520positive%2520or%250Anegative%2520based%2520on%2520class%2520similarity.%2520Hard%2520negative%2520examples%252C%2520which%2520are%2520close%2520to%250Athe%2520anchor%2520in%2520the%2520feature%2520space%2520but%2520from%2520a%2520different%2520class%252C%2520improve%2520learning%250Aperformance.%2520Finding%2520such%2520examples%2520of%2520high%2520quality%2520efficiently%2520in%2520large%252C%250Ahigh-dimensional%2520datasets%2520is%2520computationally%2520challenging.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520GPU-friendly%2520Locality-Sensitive%2520Hashing%2520%2528LSH%2529%2520scheme%2520that%2520quantizes%250Areal-valued%2520feature%2520vectors%2520into%2520binary%2520representations%2520for%2520approximate%2520nearest%250Aneighbor%2520search.%2520We%2520investigate%2520its%2520theoretical%2520properties%2520and%2520evaluate%2520it%2520on%250Aseveral%2520datasets%2520from%2520textual%2520and%2520visual%2520domain.%2520Our%2520approach%2520achieves%250Acomparable%2520or%2520better%2520performance%2520while%2520requiring%2520significantly%2520less%2520computation%250Athan%2520existing%2520hard%2520negative%2520mining%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locality-Sensitive%20Hashing%20for%20Efficient%20Hard%20Negative%20Sampling%20in%0A%20%20Contrastive%20Learning&entry.906535625=Fabian%20Deuser%20and%20Philipp%20Hausenblas%20and%20Hannah%20Schieber%20and%20Daniel%20Roth%20and%20Martin%20Werner%20and%20Norbert%20Oswald&entry.1292438233=%20%20Contrastive%20learning%20is%20a%20representational%20learning%20paradigm%20in%20which%20a%0Aneural%20network%20maps%20data%20elements%20to%20feature%20vectors.%20It%20improves%20the%20feature%0Aspace%20by%20forming%20lots%20with%20an%20anchor%20and%20examples%20that%20are%20either%20positive%20or%0Anegative%20based%20on%20class%20similarity.%20Hard%20negative%20examples%2C%20which%20are%20close%20to%0Athe%20anchor%20in%20the%20feature%20space%20but%20from%20a%20different%20class%2C%20improve%20learning%0Aperformance.%20Finding%20such%20examples%20of%20high%20quality%20efficiently%20in%20large%2C%0Ahigh-dimensional%20datasets%20is%20computationally%20challenging.%20In%20this%20paper%2C%20we%0Apropose%20a%20GPU-friendly%20Locality-Sensitive%20Hashing%20%28LSH%29%20scheme%20that%20quantizes%0Areal-valued%20feature%20vectors%20into%20binary%20representations%20for%20approximate%20nearest%0Aneighbor%20search.%20We%20investigate%20its%20theoretical%20properties%20and%20evaluate%20it%20on%0Aseveral%20datasets%20from%20textual%20and%20visual%20domain.%20Our%20approach%20achieves%0Acomparable%20or%20better%20performance%20while%20requiring%20significantly%20less%20computation%0Athan%20existing%20hard%20negative%20mining%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17844v1&entry.124074799=Read"},
{"title": "BiggerGait: Unlocking Gait Recognition with Layer-wise Representations\n  from Large Vision Models", "author": "Dingqing Ye and Chao Fan and Zhanbo Huang and Chengwen Luo and Jianqiang Li and Shiqi Yu and Xiaoming Liu", "abstract": "  Large vision models (LVM) based gait recognition has achieved impressive\nperformance. However, existing LVM-based approaches may overemphasize gait\npriors while neglecting the intrinsic value of LVM itself, particularly the\nrich, distinct representations across its multi-layers. To adequately unlock\nLVM's potential, this work investigates the impact of layer-wise\nrepresentations on downstream recognition tasks. Our analysis reveals that\nLVM's intermediate layers offer complementary properties across tasks,\nintegrating them yields an impressive improvement even without rich\nwell-designed gait priors. Building on this insight, we propose a simple and\nuniversal baseline for LVM-based gait recognition, termed BiggerGait.\nComprehensive evaluations on CCPG, CAISA-B*, SUSTech1K, and CCGR\\_MINI validate\nthe superiority of BiggerGait across both within- and cross-domain tasks,\nestablishing it as a simple yet practical baseline for gait representation\nlearning. All the models and code will be publicly available.\n", "link": "http://arxiv.org/abs/2505.18132v1", "date": "2025-05-23", "relevancy": 2.5943, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiggerGait%3A%20Unlocking%20Gait%20Recognition%20with%20Layer-wise%20Representations%0A%20%20from%20Large%20Vision%20Models&body=Title%3A%20BiggerGait%3A%20Unlocking%20Gait%20Recognition%20with%20Layer-wise%20Representations%0A%20%20from%20Large%20Vision%20Models%0AAuthor%3A%20Dingqing%20Ye%20and%20Chao%20Fan%20and%20Zhanbo%20Huang%20and%20Chengwen%20Luo%20and%20Jianqiang%20Li%20and%20Shiqi%20Yu%20and%20Xiaoming%20Liu%0AAbstract%3A%20%20%20Large%20vision%20models%20%28LVM%29%20based%20gait%20recognition%20has%20achieved%20impressive%0Aperformance.%20However%2C%20existing%20LVM-based%20approaches%20may%20overemphasize%20gait%0Apriors%20while%20neglecting%20the%20intrinsic%20value%20of%20LVM%20itself%2C%20particularly%20the%0Arich%2C%20distinct%20representations%20across%20its%20multi-layers.%20To%20adequately%20unlock%0ALVM%27s%20potential%2C%20this%20work%20investigates%20the%20impact%20of%20layer-wise%0Arepresentations%20on%20downstream%20recognition%20tasks.%20Our%20analysis%20reveals%20that%0ALVM%27s%20intermediate%20layers%20offer%20complementary%20properties%20across%20tasks%2C%0Aintegrating%20them%20yields%20an%20impressive%20improvement%20even%20without%20rich%0Awell-designed%20gait%20priors.%20Building%20on%20this%20insight%2C%20we%20propose%20a%20simple%20and%0Auniversal%20baseline%20for%20LVM-based%20gait%20recognition%2C%20termed%20BiggerGait.%0AComprehensive%20evaluations%20on%20CCPG%2C%20CAISA-B%2A%2C%20SUSTech1K%2C%20and%20CCGR%5C_MINI%20validate%0Athe%20superiority%20of%20BiggerGait%20across%20both%20within-%20and%20cross-domain%20tasks%2C%0Aestablishing%20it%20as%20a%20simple%20yet%20practical%20baseline%20for%20gait%20representation%0Alearning.%20All%20the%20models%20and%20code%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18132v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiggerGait%253A%2520Unlocking%2520Gait%2520Recognition%2520with%2520Layer-wise%2520Representations%250A%2520%2520from%2520Large%2520Vision%2520Models%26entry.906535625%3DDingqing%2520Ye%2520and%2520Chao%2520Fan%2520and%2520Zhanbo%2520Huang%2520and%2520Chengwen%2520Luo%2520and%2520Jianqiang%2520Li%2520and%2520Shiqi%2520Yu%2520and%2520Xiaoming%2520Liu%26entry.1292438233%3D%2520%2520Large%2520vision%2520models%2520%2528LVM%2529%2520based%2520gait%2520recognition%2520has%2520achieved%2520impressive%250Aperformance.%2520However%252C%2520existing%2520LVM-based%2520approaches%2520may%2520overemphasize%2520gait%250Apriors%2520while%2520neglecting%2520the%2520intrinsic%2520value%2520of%2520LVM%2520itself%252C%2520particularly%2520the%250Arich%252C%2520distinct%2520representations%2520across%2520its%2520multi-layers.%2520To%2520adequately%2520unlock%250ALVM%2527s%2520potential%252C%2520this%2520work%2520investigates%2520the%2520impact%2520of%2520layer-wise%250Arepresentations%2520on%2520downstream%2520recognition%2520tasks.%2520Our%2520analysis%2520reveals%2520that%250ALVM%2527s%2520intermediate%2520layers%2520offer%2520complementary%2520properties%2520across%2520tasks%252C%250Aintegrating%2520them%2520yields%2520an%2520impressive%2520improvement%2520even%2520without%2520rich%250Awell-designed%2520gait%2520priors.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520a%2520simple%2520and%250Auniversal%2520baseline%2520for%2520LVM-based%2520gait%2520recognition%252C%2520termed%2520BiggerGait.%250AComprehensive%2520evaluations%2520on%2520CCPG%252C%2520CAISA-B%252A%252C%2520SUSTech1K%252C%2520and%2520CCGR%255C_MINI%2520validate%250Athe%2520superiority%2520of%2520BiggerGait%2520across%2520both%2520within-%2520and%2520cross-domain%2520tasks%252C%250Aestablishing%2520it%2520as%2520a%2520simple%2520yet%2520practical%2520baseline%2520for%2520gait%2520representation%250Alearning.%2520All%2520the%2520models%2520and%2520code%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18132v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiggerGait%3A%20Unlocking%20Gait%20Recognition%20with%20Layer-wise%20Representations%0A%20%20from%20Large%20Vision%20Models&entry.906535625=Dingqing%20Ye%20and%20Chao%20Fan%20and%20Zhanbo%20Huang%20and%20Chengwen%20Luo%20and%20Jianqiang%20Li%20and%20Shiqi%20Yu%20and%20Xiaoming%20Liu&entry.1292438233=%20%20Large%20vision%20models%20%28LVM%29%20based%20gait%20recognition%20has%20achieved%20impressive%0Aperformance.%20However%2C%20existing%20LVM-based%20approaches%20may%20overemphasize%20gait%0Apriors%20while%20neglecting%20the%20intrinsic%20value%20of%20LVM%20itself%2C%20particularly%20the%0Arich%2C%20distinct%20representations%20across%20its%20multi-layers.%20To%20adequately%20unlock%0ALVM%27s%20potential%2C%20this%20work%20investigates%20the%20impact%20of%20layer-wise%0Arepresentations%20on%20downstream%20recognition%20tasks.%20Our%20analysis%20reveals%20that%0ALVM%27s%20intermediate%20layers%20offer%20complementary%20properties%20across%20tasks%2C%0Aintegrating%20them%20yields%20an%20impressive%20improvement%20even%20without%20rich%0Awell-designed%20gait%20priors.%20Building%20on%20this%20insight%2C%20we%20propose%20a%20simple%20and%0Auniversal%20baseline%20for%20LVM-based%20gait%20recognition%2C%20termed%20BiggerGait.%0AComprehensive%20evaluations%20on%20CCPG%2C%20CAISA-B%2A%2C%20SUSTech1K%2C%20and%20CCGR%5C_MINI%20validate%0Athe%20superiority%20of%20BiggerGait%20across%20both%20within-%20and%20cross-domain%20tasks%2C%0Aestablishing%20it%20as%20a%20simple%20yet%20practical%20baseline%20for%20gait%20representation%0Alearning.%20All%20the%20models%20and%20code%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18132v1&entry.124074799=Read"},
{"title": "A Foundation Model Framework for Multi-View MRI Classification of\n  Extramural Vascular Invasion and Mesorectal Fascia Invasion in Rectal Cancer", "author": "Yumeng Zhang and Zohaib Salahuddin and Danial Khan and Shruti Atul Mali and Henry C. Woodruff and Sina Amirrajab and Eduardo Ibor-Crespo and Ana Jimenez-Pastor and Luis Marti-Bonmati and Philippe Lambin", "abstract": "  Background: Accurate MRI-based identification of extramural vascular invasion\n(EVI) and mesorectal fascia invasion (MFI) is pivotal for risk-stratified\nmanagement of rectal cancer, yet visual assessment is subjective and vulnerable\nto inter-institutional variability. Purpose: To develop and externally evaluate\na multicenter, foundation-model-driven framework that automatically classifies\nEVI and MFI on axial and sagittal T2-weighted MRI. Methods: This retrospective\nstudy used 331 pre-treatment rectal cancer MRI examinations from three European\nhospitals. After TotalSegmentator-guided rectal patch extraction, a\nself-supervised frequency-domain harmonization pipeline was trained to minimize\nscanner-related contrast shifts. Four classifiers were compared: ResNet50,\nSeResNet, the universal biomedical pretrained transformer (UMedPT) with a\nlightweight MLP head, and a logistic-regression variant using frozen UMedPT\nfeatures (UMedPT_LR). Results: UMedPT_LR achieved the best EVI detection when\naxial and sagittal features were fused (AUC = 0.82; sensitivity = 0.75; F1\nscore = 0.73), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.74).\nThe highest MFI performance was attained by UMedPT on axial harmonized images\n(AUC = 0.77), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.75).\nFrequency-domain harmonization improved MFI classification but variably\naffected EVI performance. Conventional CNNs (ResNet50, SeResNet)\nunderperformed, especially in F1 score and balanced accuracy. Conclusion: These\nfindings demonstrate that combining foundation model features, harmonization,\nand multi-view fusion significantly enhances diagnostic performance in rectal\nMRI.\n", "link": "http://arxiv.org/abs/2505.18058v1", "date": "2025-05-23", "relevancy": 2.5842, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5236}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Foundation%20Model%20Framework%20for%20Multi-View%20MRI%20Classification%20of%0A%20%20Extramural%20Vascular%20Invasion%20and%20Mesorectal%20Fascia%20Invasion%20in%20Rectal%20Cancer&body=Title%3A%20A%20Foundation%20Model%20Framework%20for%20Multi-View%20MRI%20Classification%20of%0A%20%20Extramural%20Vascular%20Invasion%20and%20Mesorectal%20Fascia%20Invasion%20in%20Rectal%20Cancer%0AAuthor%3A%20Yumeng%20Zhang%20and%20Zohaib%20Salahuddin%20and%20Danial%20Khan%20and%20Shruti%20Atul%20Mali%20and%20Henry%20C.%20Woodruff%20and%20Sina%20Amirrajab%20and%20Eduardo%20Ibor-Crespo%20and%20Ana%20Jimenez-Pastor%20and%20Luis%20Marti-Bonmati%20and%20Philippe%20Lambin%0AAbstract%3A%20%20%20Background%3A%20Accurate%20MRI-based%20identification%20of%20extramural%20vascular%20invasion%0A%28EVI%29%20and%20mesorectal%20fascia%20invasion%20%28MFI%29%20is%20pivotal%20for%20risk-stratified%0Amanagement%20of%20rectal%20cancer%2C%20yet%20visual%20assessment%20is%20subjective%20and%20vulnerable%0Ato%20inter-institutional%20variability.%20Purpose%3A%20To%20develop%20and%20externally%20evaluate%0Aa%20multicenter%2C%20foundation-model-driven%20framework%20that%20automatically%20classifies%0AEVI%20and%20MFI%20on%20axial%20and%20sagittal%20T2-weighted%20MRI.%20Methods%3A%20This%20retrospective%0Astudy%20used%20331%20pre-treatment%20rectal%20cancer%20MRI%20examinations%20from%20three%20European%0Ahospitals.%20After%20TotalSegmentator-guided%20rectal%20patch%20extraction%2C%20a%0Aself-supervised%20frequency-domain%20harmonization%20pipeline%20was%20trained%20to%20minimize%0Ascanner-related%20contrast%20shifts.%20Four%20classifiers%20were%20compared%3A%20ResNet50%2C%0ASeResNet%2C%20the%20universal%20biomedical%20pretrained%20transformer%20%28UMedPT%29%20with%20a%0Alightweight%20MLP%20head%2C%20and%20a%20logistic-regression%20variant%20using%20frozen%20UMedPT%0Afeatures%20%28UMedPT_LR%29.%20Results%3A%20UMedPT_LR%20achieved%20the%20best%20EVI%20detection%20when%0Aaxial%20and%20sagittal%20features%20were%20fused%20%28AUC%20%3D%200.82%3B%20sensitivity%20%3D%200.75%3B%20F1%0Ascore%20%3D%200.73%29%2C%20surpassing%20the%20Chaimeleon%20Grand-Challenge%20winner%20%28AUC%20%3D%200.74%29.%0AThe%20highest%20MFI%20performance%20was%20attained%20by%20UMedPT%20on%20axial%20harmonized%20images%0A%28AUC%20%3D%200.77%29%2C%20surpassing%20the%20Chaimeleon%20Grand-Challenge%20winner%20%28AUC%20%3D%200.75%29.%0AFrequency-domain%20harmonization%20improved%20MFI%20classification%20but%20variably%0Aaffected%20EVI%20performance.%20Conventional%20CNNs%20%28ResNet50%2C%20SeResNet%29%0Aunderperformed%2C%20especially%20in%20F1%20score%20and%20balanced%20accuracy.%20Conclusion%3A%20These%0Afindings%20demonstrate%20that%20combining%20foundation%20model%20features%2C%20harmonization%2C%0Aand%20multi-view%20fusion%20significantly%20enhances%20diagnostic%20performance%20in%20rectal%0AMRI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Foundation%2520Model%2520Framework%2520for%2520Multi-View%2520MRI%2520Classification%2520of%250A%2520%2520Extramural%2520Vascular%2520Invasion%2520and%2520Mesorectal%2520Fascia%2520Invasion%2520in%2520Rectal%2520Cancer%26entry.906535625%3DYumeng%2520Zhang%2520and%2520Zohaib%2520Salahuddin%2520and%2520Danial%2520Khan%2520and%2520Shruti%2520Atul%2520Mali%2520and%2520Henry%2520C.%2520Woodruff%2520and%2520Sina%2520Amirrajab%2520and%2520Eduardo%2520Ibor-Crespo%2520and%2520Ana%2520Jimenez-Pastor%2520and%2520Luis%2520Marti-Bonmati%2520and%2520Philippe%2520Lambin%26entry.1292438233%3D%2520%2520Background%253A%2520Accurate%2520MRI-based%2520identification%2520of%2520extramural%2520vascular%2520invasion%250A%2528EVI%2529%2520and%2520mesorectal%2520fascia%2520invasion%2520%2528MFI%2529%2520is%2520pivotal%2520for%2520risk-stratified%250Amanagement%2520of%2520rectal%2520cancer%252C%2520yet%2520visual%2520assessment%2520is%2520subjective%2520and%2520vulnerable%250Ato%2520inter-institutional%2520variability.%2520Purpose%253A%2520To%2520develop%2520and%2520externally%2520evaluate%250Aa%2520multicenter%252C%2520foundation-model-driven%2520framework%2520that%2520automatically%2520classifies%250AEVI%2520and%2520MFI%2520on%2520axial%2520and%2520sagittal%2520T2-weighted%2520MRI.%2520Methods%253A%2520This%2520retrospective%250Astudy%2520used%2520331%2520pre-treatment%2520rectal%2520cancer%2520MRI%2520examinations%2520from%2520three%2520European%250Ahospitals.%2520After%2520TotalSegmentator-guided%2520rectal%2520patch%2520extraction%252C%2520a%250Aself-supervised%2520frequency-domain%2520harmonization%2520pipeline%2520was%2520trained%2520to%2520minimize%250Ascanner-related%2520contrast%2520shifts.%2520Four%2520classifiers%2520were%2520compared%253A%2520ResNet50%252C%250ASeResNet%252C%2520the%2520universal%2520biomedical%2520pretrained%2520transformer%2520%2528UMedPT%2529%2520with%2520a%250Alightweight%2520MLP%2520head%252C%2520and%2520a%2520logistic-regression%2520variant%2520using%2520frozen%2520UMedPT%250Afeatures%2520%2528UMedPT_LR%2529.%2520Results%253A%2520UMedPT_LR%2520achieved%2520the%2520best%2520EVI%2520detection%2520when%250Aaxial%2520and%2520sagittal%2520features%2520were%2520fused%2520%2528AUC%2520%253D%25200.82%253B%2520sensitivity%2520%253D%25200.75%253B%2520F1%250Ascore%2520%253D%25200.73%2529%252C%2520surpassing%2520the%2520Chaimeleon%2520Grand-Challenge%2520winner%2520%2528AUC%2520%253D%25200.74%2529.%250AThe%2520highest%2520MFI%2520performance%2520was%2520attained%2520by%2520UMedPT%2520on%2520axial%2520harmonized%2520images%250A%2528AUC%2520%253D%25200.77%2529%252C%2520surpassing%2520the%2520Chaimeleon%2520Grand-Challenge%2520winner%2520%2528AUC%2520%253D%25200.75%2529.%250AFrequency-domain%2520harmonization%2520improved%2520MFI%2520classification%2520but%2520variably%250Aaffected%2520EVI%2520performance.%2520Conventional%2520CNNs%2520%2528ResNet50%252C%2520SeResNet%2529%250Aunderperformed%252C%2520especially%2520in%2520F1%2520score%2520and%2520balanced%2520accuracy.%2520Conclusion%253A%2520These%250Afindings%2520demonstrate%2520that%2520combining%2520foundation%2520model%2520features%252C%2520harmonization%252C%250Aand%2520multi-view%2520fusion%2520significantly%2520enhances%2520diagnostic%2520performance%2520in%2520rectal%250AMRI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Foundation%20Model%20Framework%20for%20Multi-View%20MRI%20Classification%20of%0A%20%20Extramural%20Vascular%20Invasion%20and%20Mesorectal%20Fascia%20Invasion%20in%20Rectal%20Cancer&entry.906535625=Yumeng%20Zhang%20and%20Zohaib%20Salahuddin%20and%20Danial%20Khan%20and%20Shruti%20Atul%20Mali%20and%20Henry%20C.%20Woodruff%20and%20Sina%20Amirrajab%20and%20Eduardo%20Ibor-Crespo%20and%20Ana%20Jimenez-Pastor%20and%20Luis%20Marti-Bonmati%20and%20Philippe%20Lambin&entry.1292438233=%20%20Background%3A%20Accurate%20MRI-based%20identification%20of%20extramural%20vascular%20invasion%0A%28EVI%29%20and%20mesorectal%20fascia%20invasion%20%28MFI%29%20is%20pivotal%20for%20risk-stratified%0Amanagement%20of%20rectal%20cancer%2C%20yet%20visual%20assessment%20is%20subjective%20and%20vulnerable%0Ato%20inter-institutional%20variability.%20Purpose%3A%20To%20develop%20and%20externally%20evaluate%0Aa%20multicenter%2C%20foundation-model-driven%20framework%20that%20automatically%20classifies%0AEVI%20and%20MFI%20on%20axial%20and%20sagittal%20T2-weighted%20MRI.%20Methods%3A%20This%20retrospective%0Astudy%20used%20331%20pre-treatment%20rectal%20cancer%20MRI%20examinations%20from%20three%20European%0Ahospitals.%20After%20TotalSegmentator-guided%20rectal%20patch%20extraction%2C%20a%0Aself-supervised%20frequency-domain%20harmonization%20pipeline%20was%20trained%20to%20minimize%0Ascanner-related%20contrast%20shifts.%20Four%20classifiers%20were%20compared%3A%20ResNet50%2C%0ASeResNet%2C%20the%20universal%20biomedical%20pretrained%20transformer%20%28UMedPT%29%20with%20a%0Alightweight%20MLP%20head%2C%20and%20a%20logistic-regression%20variant%20using%20frozen%20UMedPT%0Afeatures%20%28UMedPT_LR%29.%20Results%3A%20UMedPT_LR%20achieved%20the%20best%20EVI%20detection%20when%0Aaxial%20and%20sagittal%20features%20were%20fused%20%28AUC%20%3D%200.82%3B%20sensitivity%20%3D%200.75%3B%20F1%0Ascore%20%3D%200.73%29%2C%20surpassing%20the%20Chaimeleon%20Grand-Challenge%20winner%20%28AUC%20%3D%200.74%29.%0AThe%20highest%20MFI%20performance%20was%20attained%20by%20UMedPT%20on%20axial%20harmonized%20images%0A%28AUC%20%3D%200.77%29%2C%20surpassing%20the%20Chaimeleon%20Grand-Challenge%20winner%20%28AUC%20%3D%200.75%29.%0AFrequency-domain%20harmonization%20improved%20MFI%20classification%20but%20variably%0Aaffected%20EVI%20performance.%20Conventional%20CNNs%20%28ResNet50%2C%20SeResNet%29%0Aunderperformed%2C%20especially%20in%20F1%20score%20and%20balanced%20accuracy.%20Conclusion%3A%20These%0Afindings%20demonstrate%20that%20combining%20foundation%20model%20features%2C%20harmonization%2C%0Aand%20multi-view%20fusion%20significantly%20enhances%20diagnostic%20performance%20in%20rectal%0AMRI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18058v1&entry.124074799=Read"},
{"title": "SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded\n  Scenarios", "author": "Simon Malzard and Nitish Mital and Richard Walters and Victoria Nockles and Raghuveer Rao and Celso M. De Melo", "abstract": "  Computer vision (CV) models for detection, prediction or classification tasks\noperate on video data-streams that are often degraded in the real world, due to\ndeployment in real-time or on resource-constrained hardware. It is therefore\ncritical that these models are robust to degraded data, but state of the art\n(SoTA) models are often insufficiently assessed with these real-world\nconstraints in mind. This is exemplified by Skeletal Human Action Recognition\n(SHAR), which is critical in many CV pipelines operating in real-time and at\nthe edge, but robustness to degraded data has previously only been shallowly\nand inconsistently assessed. Here we address this issue for SHAR by providing\nan important first data degradation benchmark on the most detailed and largest\n3D open dataset, NTU-RGB+D-120, and assess the robustness of five leading SHAR\nmodels to three forms of degradation that represent real-world issues. We\ndemonstrate the need for this benchmark by showing that the form of\ndegradation, which has not previously been considered, has a large impact on\nmodel accuracy; at the same effective frame rate, model accuracy can vary by\n>40% depending on degradation type. We also identify that temporal regularity\nof frames in degraded SHAR data is likely a major driver of differences in\nmodel performance, and harness this to improve performance of existing models\nby up to >40%, through employing a simple mitigation approach based on\ninterpolation. Finally, we highlight how our benchmark has helped identify an\nimportant degradation-resistant SHAR model based in Rough Path Theory; the\nLogSigRNN SHAR model outperforms the SoTA DeGCN model in five out of six cases\nat low frame rates by an average accuracy of 6%, despite trailing the SoTA\nmodel by 11-12% on un-degraded data at high frame rates (30 FPS).\n", "link": "http://arxiv.org/abs/2505.18048v1", "date": "2025-05-23", "relevancy": 2.5746, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5098}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHARDeg%3A%20A%20Benchmark%20for%20Skeletal%20Human%20Action%20Recognition%20in%20Degraded%0A%20%20Scenarios&body=Title%3A%20SHARDeg%3A%20A%20Benchmark%20for%20Skeletal%20Human%20Action%20Recognition%20in%20Degraded%0A%20%20Scenarios%0AAuthor%3A%20Simon%20Malzard%20and%20Nitish%20Mital%20and%20Richard%20Walters%20and%20Victoria%20Nockles%20and%20Raghuveer%20Rao%20and%20Celso%20M.%20De%20Melo%0AAbstract%3A%20%20%20Computer%20vision%20%28CV%29%20models%20for%20detection%2C%20prediction%20or%20classification%20tasks%0Aoperate%20on%20video%20data-streams%20that%20are%20often%20degraded%20in%20the%20real%20world%2C%20due%20to%0Adeployment%20in%20real-time%20or%20on%20resource-constrained%20hardware.%20It%20is%20therefore%0Acritical%20that%20these%20models%20are%20robust%20to%20degraded%20data%2C%20but%20state%20of%20the%20art%0A%28SoTA%29%20models%20are%20often%20insufficiently%20assessed%20with%20these%20real-world%0Aconstraints%20in%20mind.%20This%20is%20exemplified%20by%20Skeletal%20Human%20Action%20Recognition%0A%28SHAR%29%2C%20which%20is%20critical%20in%20many%20CV%20pipelines%20operating%20in%20real-time%20and%20at%0Athe%20edge%2C%20but%20robustness%20to%20degraded%20data%20has%20previously%20only%20been%20shallowly%0Aand%20inconsistently%20assessed.%20Here%20we%20address%20this%20issue%20for%20SHAR%20by%20providing%0Aan%20important%20first%20data%20degradation%20benchmark%20on%20the%20most%20detailed%20and%20largest%0A3D%20open%20dataset%2C%20NTU-RGB%2BD-120%2C%20and%20assess%20the%20robustness%20of%20five%20leading%20SHAR%0Amodels%20to%20three%20forms%20of%20degradation%20that%20represent%20real-world%20issues.%20We%0Ademonstrate%20the%20need%20for%20this%20benchmark%20by%20showing%20that%20the%20form%20of%0Adegradation%2C%20which%20has%20not%20previously%20been%20considered%2C%20has%20a%20large%20impact%20on%0Amodel%20accuracy%3B%20at%20the%20same%20effective%20frame%20rate%2C%20model%20accuracy%20can%20vary%20by%0A%3E40%25%20depending%20on%20degradation%20type.%20We%20also%20identify%20that%20temporal%20regularity%0Aof%20frames%20in%20degraded%20SHAR%20data%20is%20likely%20a%20major%20driver%20of%20differences%20in%0Amodel%20performance%2C%20and%20harness%20this%20to%20improve%20performance%20of%20existing%20models%0Aby%20up%20to%20%3E40%25%2C%20through%20employing%20a%20simple%20mitigation%20approach%20based%20on%0Ainterpolation.%20Finally%2C%20we%20highlight%20how%20our%20benchmark%20has%20helped%20identify%20an%0Aimportant%20degradation-resistant%20SHAR%20model%20based%20in%20Rough%20Path%20Theory%3B%20the%0ALogSigRNN%20SHAR%20model%20outperforms%20the%20SoTA%20DeGCN%20model%20in%20five%20out%20of%20six%20cases%0Aat%20low%20frame%20rates%20by%20an%20average%20accuracy%20of%206%25%2C%20despite%20trailing%20the%20SoTA%0Amodel%20by%2011-12%25%20on%20un-degraded%20data%20at%20high%20frame%20rates%20%2830%20FPS%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHARDeg%253A%2520A%2520Benchmark%2520for%2520Skeletal%2520Human%2520Action%2520Recognition%2520in%2520Degraded%250A%2520%2520Scenarios%26entry.906535625%3DSimon%2520Malzard%2520and%2520Nitish%2520Mital%2520and%2520Richard%2520Walters%2520and%2520Victoria%2520Nockles%2520and%2520Raghuveer%2520Rao%2520and%2520Celso%2520M.%2520De%2520Melo%26entry.1292438233%3D%2520%2520Computer%2520vision%2520%2528CV%2529%2520models%2520for%2520detection%252C%2520prediction%2520or%2520classification%2520tasks%250Aoperate%2520on%2520video%2520data-streams%2520that%2520are%2520often%2520degraded%2520in%2520the%2520real%2520world%252C%2520due%2520to%250Adeployment%2520in%2520real-time%2520or%2520on%2520resource-constrained%2520hardware.%2520It%2520is%2520therefore%250Acritical%2520that%2520these%2520models%2520are%2520robust%2520to%2520degraded%2520data%252C%2520but%2520state%2520of%2520the%2520art%250A%2528SoTA%2529%2520models%2520are%2520often%2520insufficiently%2520assessed%2520with%2520these%2520real-world%250Aconstraints%2520in%2520mind.%2520This%2520is%2520exemplified%2520by%2520Skeletal%2520Human%2520Action%2520Recognition%250A%2528SHAR%2529%252C%2520which%2520is%2520critical%2520in%2520many%2520CV%2520pipelines%2520operating%2520in%2520real-time%2520and%2520at%250Athe%2520edge%252C%2520but%2520robustness%2520to%2520degraded%2520data%2520has%2520previously%2520only%2520been%2520shallowly%250Aand%2520inconsistently%2520assessed.%2520Here%2520we%2520address%2520this%2520issue%2520for%2520SHAR%2520by%2520providing%250Aan%2520important%2520first%2520data%2520degradation%2520benchmark%2520on%2520the%2520most%2520detailed%2520and%2520largest%250A3D%2520open%2520dataset%252C%2520NTU-RGB%252BD-120%252C%2520and%2520assess%2520the%2520robustness%2520of%2520five%2520leading%2520SHAR%250Amodels%2520to%2520three%2520forms%2520of%2520degradation%2520that%2520represent%2520real-world%2520issues.%2520We%250Ademonstrate%2520the%2520need%2520for%2520this%2520benchmark%2520by%2520showing%2520that%2520the%2520form%2520of%250Adegradation%252C%2520which%2520has%2520not%2520previously%2520been%2520considered%252C%2520has%2520a%2520large%2520impact%2520on%250Amodel%2520accuracy%253B%2520at%2520the%2520same%2520effective%2520frame%2520rate%252C%2520model%2520accuracy%2520can%2520vary%2520by%250A%253E40%2525%2520depending%2520on%2520degradation%2520type.%2520We%2520also%2520identify%2520that%2520temporal%2520regularity%250Aof%2520frames%2520in%2520degraded%2520SHAR%2520data%2520is%2520likely%2520a%2520major%2520driver%2520of%2520differences%2520in%250Amodel%2520performance%252C%2520and%2520harness%2520this%2520to%2520improve%2520performance%2520of%2520existing%2520models%250Aby%2520up%2520to%2520%253E40%2525%252C%2520through%2520employing%2520a%2520simple%2520mitigation%2520approach%2520based%2520on%250Ainterpolation.%2520Finally%252C%2520we%2520highlight%2520how%2520our%2520benchmark%2520has%2520helped%2520identify%2520an%250Aimportant%2520degradation-resistant%2520SHAR%2520model%2520based%2520in%2520Rough%2520Path%2520Theory%253B%2520the%250ALogSigRNN%2520SHAR%2520model%2520outperforms%2520the%2520SoTA%2520DeGCN%2520model%2520in%2520five%2520out%2520of%2520six%2520cases%250Aat%2520low%2520frame%2520rates%2520by%2520an%2520average%2520accuracy%2520of%25206%2525%252C%2520despite%2520trailing%2520the%2520SoTA%250Amodel%2520by%252011-12%2525%2520on%2520un-degraded%2520data%2520at%2520high%2520frame%2520rates%2520%252830%2520FPS%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHARDeg%3A%20A%20Benchmark%20for%20Skeletal%20Human%20Action%20Recognition%20in%20Degraded%0A%20%20Scenarios&entry.906535625=Simon%20Malzard%20and%20Nitish%20Mital%20and%20Richard%20Walters%20and%20Victoria%20Nockles%20and%20Raghuveer%20Rao%20and%20Celso%20M.%20De%20Melo&entry.1292438233=%20%20Computer%20vision%20%28CV%29%20models%20for%20detection%2C%20prediction%20or%20classification%20tasks%0Aoperate%20on%20video%20data-streams%20that%20are%20often%20degraded%20in%20the%20real%20world%2C%20due%20to%0Adeployment%20in%20real-time%20or%20on%20resource-constrained%20hardware.%20It%20is%20therefore%0Acritical%20that%20these%20models%20are%20robust%20to%20degraded%20data%2C%20but%20state%20of%20the%20art%0A%28SoTA%29%20models%20are%20often%20insufficiently%20assessed%20with%20these%20real-world%0Aconstraints%20in%20mind.%20This%20is%20exemplified%20by%20Skeletal%20Human%20Action%20Recognition%0A%28SHAR%29%2C%20which%20is%20critical%20in%20many%20CV%20pipelines%20operating%20in%20real-time%20and%20at%0Athe%20edge%2C%20but%20robustness%20to%20degraded%20data%20has%20previously%20only%20been%20shallowly%0Aand%20inconsistently%20assessed.%20Here%20we%20address%20this%20issue%20for%20SHAR%20by%20providing%0Aan%20important%20first%20data%20degradation%20benchmark%20on%20the%20most%20detailed%20and%20largest%0A3D%20open%20dataset%2C%20NTU-RGB%2BD-120%2C%20and%20assess%20the%20robustness%20of%20five%20leading%20SHAR%0Amodels%20to%20three%20forms%20of%20degradation%20that%20represent%20real-world%20issues.%20We%0Ademonstrate%20the%20need%20for%20this%20benchmark%20by%20showing%20that%20the%20form%20of%0Adegradation%2C%20which%20has%20not%20previously%20been%20considered%2C%20has%20a%20large%20impact%20on%0Amodel%20accuracy%3B%20at%20the%20same%20effective%20frame%20rate%2C%20model%20accuracy%20can%20vary%20by%0A%3E40%25%20depending%20on%20degradation%20type.%20We%20also%20identify%20that%20temporal%20regularity%0Aof%20frames%20in%20degraded%20SHAR%20data%20is%20likely%20a%20major%20driver%20of%20differences%20in%0Amodel%20performance%2C%20and%20harness%20this%20to%20improve%20performance%20of%20existing%20models%0Aby%20up%20to%20%3E40%25%2C%20through%20employing%20a%20simple%20mitigation%20approach%20based%20on%0Ainterpolation.%20Finally%2C%20we%20highlight%20how%20our%20benchmark%20has%20helped%20identify%20an%0Aimportant%20degradation-resistant%20SHAR%20model%20based%20in%20Rough%20Path%20Theory%3B%20the%0ALogSigRNN%20SHAR%20model%20outperforms%20the%20SoTA%20DeGCN%20model%20in%20five%20out%20of%20six%20cases%0Aat%20low%20frame%20rates%20by%20an%20average%20accuracy%20of%206%25%2C%20despite%20trailing%20the%20SoTA%0Amodel%20by%2011-12%25%20on%20un-degraded%20data%20at%20high%20frame%20rates%20%2830%20FPS%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18048v1&entry.124074799=Read"},
{"title": "Early-Exit Graph Neural Networks", "author": "Andrea Giuseppe Di Francesco and Maria Sofia Bucarelli and Franco Maria Nardini and Raffaele Perego and Nicola Tonellotto and Fabrizio Silvestri", "abstract": "  Early-exit mechanisms allow deep neural networks to halt inference as soon as\nclassification confidence is high enough, adaptively trading depth for\nconfidence, and thereby cutting latency and energy on easy inputs while\nretaining full-depth accuracy for harder ones. Similarly, adding early exit\nmechanisms to Graph Neural Networks (GNNs), the go-to models for\ngraph-structured data, allows for dynamic trading depth for confidence on\nsimple graphs while maintaining full-depth accuracy on harder and more complex\ngraphs to capture intricate relationships. Although early exits have proven\neffective across various deep learning domains, their potential within GNNs in\nscenarios that require deep architectures while resisting over-smoothing and\nover-squashing remains largely unexplored. We unlock that potential by first\nintroducing Symmetric-Anti-Symmetric Graph Neural Networks (SAS-GNN), whose\nsymmetry-based inductive biases mitigate these issues and yield stable\nintermediate representations that can be useful to allow early exiting in GNNs.\nBuilding on this backbone, we present Early-Exit Graph Neural Networks\n(EEGNNs), which append confidence-aware exit heads that allow on-the-fly\ntermination of propagation based on each node or the entire graph. Experiments\nshow that EEGNNs preserve robust performance as depth grows and deliver\ncompetitive accuracy on heterophilic and long-range benchmarks, matching\nattention-based and asynchronous message-passing models while substantially\nreducing computation and latency. We plan to release the code to reproduce our\nexperiments.\n", "link": "http://arxiv.org/abs/2505.18088v1", "date": "2025-05-23", "relevancy": 2.5599, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6164}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.461}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early-Exit%20Graph%20Neural%20Networks&body=Title%3A%20Early-Exit%20Graph%20Neural%20Networks%0AAuthor%3A%20Andrea%20Giuseppe%20Di%20Francesco%20and%20Maria%20Sofia%20Bucarelli%20and%20Franco%20Maria%20Nardini%20and%20Raffaele%20Perego%20and%20Nicola%20Tonellotto%20and%20Fabrizio%20Silvestri%0AAbstract%3A%20%20%20Early-exit%20mechanisms%20allow%20deep%20neural%20networks%20to%20halt%20inference%20as%20soon%20as%0Aclassification%20confidence%20is%20high%20enough%2C%20adaptively%20trading%20depth%20for%0Aconfidence%2C%20and%20thereby%20cutting%20latency%20and%20energy%20on%20easy%20inputs%20while%0Aretaining%20full-depth%20accuracy%20for%20harder%20ones.%20Similarly%2C%20adding%20early%20exit%0Amechanisms%20to%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20the%20go-to%20models%20for%0Agraph-structured%20data%2C%20allows%20for%20dynamic%20trading%20depth%20for%20confidence%20on%0Asimple%20graphs%20while%20maintaining%20full-depth%20accuracy%20on%20harder%20and%20more%20complex%0Agraphs%20to%20capture%20intricate%20relationships.%20Although%20early%20exits%20have%20proven%0Aeffective%20across%20various%20deep%20learning%20domains%2C%20their%20potential%20within%20GNNs%20in%0Ascenarios%20that%20require%20deep%20architectures%20while%20resisting%20over-smoothing%20and%0Aover-squashing%20remains%20largely%20unexplored.%20We%20unlock%20that%20potential%20by%20first%0Aintroducing%20Symmetric-Anti-Symmetric%20Graph%20Neural%20Networks%20%28SAS-GNN%29%2C%20whose%0Asymmetry-based%20inductive%20biases%20mitigate%20these%20issues%20and%20yield%20stable%0Aintermediate%20representations%20that%20can%20be%20useful%20to%20allow%20early%20exiting%20in%20GNNs.%0ABuilding%20on%20this%20backbone%2C%20we%20present%20Early-Exit%20Graph%20Neural%20Networks%0A%28EEGNNs%29%2C%20which%20append%20confidence-aware%20exit%20heads%20that%20allow%20on-the-fly%0Atermination%20of%20propagation%20based%20on%20each%20node%20or%20the%20entire%20graph.%20Experiments%0Ashow%20that%20EEGNNs%20preserve%20robust%20performance%20as%20depth%20grows%20and%20deliver%0Acompetitive%20accuracy%20on%20heterophilic%20and%20long-range%20benchmarks%2C%20matching%0Aattention-based%20and%20asynchronous%20message-passing%20models%20while%20substantially%0Areducing%20computation%20and%20latency.%20We%20plan%20to%20release%20the%20code%20to%20reproduce%20our%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly-Exit%2520Graph%2520Neural%2520Networks%26entry.906535625%3DAndrea%2520Giuseppe%2520Di%2520Francesco%2520and%2520Maria%2520Sofia%2520Bucarelli%2520and%2520Franco%2520Maria%2520Nardini%2520and%2520Raffaele%2520Perego%2520and%2520Nicola%2520Tonellotto%2520and%2520Fabrizio%2520Silvestri%26entry.1292438233%3D%2520%2520Early-exit%2520mechanisms%2520allow%2520deep%2520neural%2520networks%2520to%2520halt%2520inference%2520as%2520soon%2520as%250Aclassification%2520confidence%2520is%2520high%2520enough%252C%2520adaptively%2520trading%2520depth%2520for%250Aconfidence%252C%2520and%2520thereby%2520cutting%2520latency%2520and%2520energy%2520on%2520easy%2520inputs%2520while%250Aretaining%2520full-depth%2520accuracy%2520for%2520harder%2520ones.%2520Similarly%252C%2520adding%2520early%2520exit%250Amechanisms%2520to%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520the%2520go-to%2520models%2520for%250Agraph-structured%2520data%252C%2520allows%2520for%2520dynamic%2520trading%2520depth%2520for%2520confidence%2520on%250Asimple%2520graphs%2520while%2520maintaining%2520full-depth%2520accuracy%2520on%2520harder%2520and%2520more%2520complex%250Agraphs%2520to%2520capture%2520intricate%2520relationships.%2520Although%2520early%2520exits%2520have%2520proven%250Aeffective%2520across%2520various%2520deep%2520learning%2520domains%252C%2520their%2520potential%2520within%2520GNNs%2520in%250Ascenarios%2520that%2520require%2520deep%2520architectures%2520while%2520resisting%2520over-smoothing%2520and%250Aover-squashing%2520remains%2520largely%2520unexplored.%2520We%2520unlock%2520that%2520potential%2520by%2520first%250Aintroducing%2520Symmetric-Anti-Symmetric%2520Graph%2520Neural%2520Networks%2520%2528SAS-GNN%2529%252C%2520whose%250Asymmetry-based%2520inductive%2520biases%2520mitigate%2520these%2520issues%2520and%2520yield%2520stable%250Aintermediate%2520representations%2520that%2520can%2520be%2520useful%2520to%2520allow%2520early%2520exiting%2520in%2520GNNs.%250ABuilding%2520on%2520this%2520backbone%252C%2520we%2520present%2520Early-Exit%2520Graph%2520Neural%2520Networks%250A%2528EEGNNs%2529%252C%2520which%2520append%2520confidence-aware%2520exit%2520heads%2520that%2520allow%2520on-the-fly%250Atermination%2520of%2520propagation%2520based%2520on%2520each%2520node%2520or%2520the%2520entire%2520graph.%2520Experiments%250Ashow%2520that%2520EEGNNs%2520preserve%2520robust%2520performance%2520as%2520depth%2520grows%2520and%2520deliver%250Acompetitive%2520accuracy%2520on%2520heterophilic%2520and%2520long-range%2520benchmarks%252C%2520matching%250Aattention-based%2520and%2520asynchronous%2520message-passing%2520models%2520while%2520substantially%250Areducing%2520computation%2520and%2520latency.%2520We%2520plan%2520to%2520release%2520the%2520code%2520to%2520reproduce%2520our%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early-Exit%20Graph%20Neural%20Networks&entry.906535625=Andrea%20Giuseppe%20Di%20Francesco%20and%20Maria%20Sofia%20Bucarelli%20and%20Franco%20Maria%20Nardini%20and%20Raffaele%20Perego%20and%20Nicola%20Tonellotto%20and%20Fabrizio%20Silvestri&entry.1292438233=%20%20Early-exit%20mechanisms%20allow%20deep%20neural%20networks%20to%20halt%20inference%20as%20soon%20as%0Aclassification%20confidence%20is%20high%20enough%2C%20adaptively%20trading%20depth%20for%0Aconfidence%2C%20and%20thereby%20cutting%20latency%20and%20energy%20on%20easy%20inputs%20while%0Aretaining%20full-depth%20accuracy%20for%20harder%20ones.%20Similarly%2C%20adding%20early%20exit%0Amechanisms%20to%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20the%20go-to%20models%20for%0Agraph-structured%20data%2C%20allows%20for%20dynamic%20trading%20depth%20for%20confidence%20on%0Asimple%20graphs%20while%20maintaining%20full-depth%20accuracy%20on%20harder%20and%20more%20complex%0Agraphs%20to%20capture%20intricate%20relationships.%20Although%20early%20exits%20have%20proven%0Aeffective%20across%20various%20deep%20learning%20domains%2C%20their%20potential%20within%20GNNs%20in%0Ascenarios%20that%20require%20deep%20architectures%20while%20resisting%20over-smoothing%20and%0Aover-squashing%20remains%20largely%20unexplored.%20We%20unlock%20that%20potential%20by%20first%0Aintroducing%20Symmetric-Anti-Symmetric%20Graph%20Neural%20Networks%20%28SAS-GNN%29%2C%20whose%0Asymmetry-based%20inductive%20biases%20mitigate%20these%20issues%20and%20yield%20stable%0Aintermediate%20representations%20that%20can%20be%20useful%20to%20allow%20early%20exiting%20in%20GNNs.%0ABuilding%20on%20this%20backbone%2C%20we%20present%20Early-Exit%20Graph%20Neural%20Networks%0A%28EEGNNs%29%2C%20which%20append%20confidence-aware%20exit%20heads%20that%20allow%20on-the-fly%0Atermination%20of%20propagation%20based%20on%20each%20node%20or%20the%20entire%20graph.%20Experiments%0Ashow%20that%20EEGNNs%20preserve%20robust%20performance%20as%20depth%20grows%20and%20deliver%0Acompetitive%20accuracy%20on%20heterophilic%20and%20long-range%20benchmarks%2C%20matching%0Aattention-based%20and%20asynchronous%20message-passing%20models%20while%20substantially%0Areducing%20computation%20and%20latency.%20We%20plan%20to%20release%20the%20code%20to%20reproduce%20our%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18088v1&entry.124074799=Read"},
{"title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations", "author": "Alan Arazi and Eilam Shapira and Roi Reichart", "abstract": "  While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.\n", "link": "http://arxiv.org/abs/2505.18125v1", "date": "2025-05-23", "relevancy": 2.5444, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TabSTAR%3A%20A%20Foundation%20Tabular%20Model%20With%20Semantically%20Target-Aware%0A%20%20Representations&body=Title%3A%20TabSTAR%3A%20A%20Foundation%20Tabular%20Model%20With%20Semantically%20Target-Aware%0A%20%20Representations%0AAuthor%3A%20Alan%20Arazi%20and%20Eilam%20Shapira%20and%20Roi%20Reichart%0AAbstract%3A%20%20%20While%20deep%20learning%20has%20achieved%20remarkable%20success%20across%20many%20domains%2C%20it%0Ahas%20historically%20underperformed%20on%20tabular%20learning%20tasks%2C%20which%20remain%0Adominated%20by%20gradient%20boosting%20decision%20trees%20%28GBDTs%29.%20However%2C%20recent%0Aadvancements%20are%20paving%20the%20way%20for%20Tabular%20Foundation%20Models%2C%20which%20can%0Aleverage%20real-world%20knowledge%20and%20generalize%20across%20diverse%20datasets%2C%0Aparticularly%20when%20the%20data%20contains%20free-text.%20Although%20incorporating%20language%0Amodel%20capabilities%20into%20tabular%20tasks%20has%20been%20explored%2C%20most%20existing%20methods%0Autilize%20static%2C%20target-agnostic%20textual%20representations%2C%20limiting%20their%0Aeffectiveness.%20We%20introduce%20TabSTAR%3A%20a%20Foundation%20Tabular%20Model%20with%0ASemantically%20Target-Aware%20Representations.%20TabSTAR%20is%20designed%20to%20enable%0Atransfer%20learning%20on%20tabular%20data%20with%20textual%20features%2C%20with%20an%20architecture%0Afree%20of%20dataset-specific%20parameters.%20It%20unfreezes%20a%20pretrained%20text%20encoder%20and%0Atakes%20as%20input%20target%20tokens%2C%20which%20provide%20the%20model%20with%20the%20context%20needed%0Ato%20learn%20task-specific%20embeddings.%20TabSTAR%20achieves%20state-of-the-art%0Aperformance%20for%20both%20medium-%20and%20large-sized%20datasets%20across%20known%20benchmarks%0Aof%20classification%20tasks%20with%20text%20features%2C%20and%20its%20pretraining%20phase%20exhibits%0Ascaling%20laws%20in%20the%20number%20of%20datasets%2C%20offering%20a%20pathway%20for%20further%0Aperformance%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabSTAR%253A%2520A%2520Foundation%2520Tabular%2520Model%2520With%2520Semantically%2520Target-Aware%250A%2520%2520Representations%26entry.906535625%3DAlan%2520Arazi%2520and%2520Eilam%2520Shapira%2520and%2520Roi%2520Reichart%26entry.1292438233%3D%2520%2520While%2520deep%2520learning%2520has%2520achieved%2520remarkable%2520success%2520across%2520many%2520domains%252C%2520it%250Ahas%2520historically%2520underperformed%2520on%2520tabular%2520learning%2520tasks%252C%2520which%2520remain%250Adominated%2520by%2520gradient%2520boosting%2520decision%2520trees%2520%2528GBDTs%2529.%2520However%252C%2520recent%250Aadvancements%2520are%2520paving%2520the%2520way%2520for%2520Tabular%2520Foundation%2520Models%252C%2520which%2520can%250Aleverage%2520real-world%2520knowledge%2520and%2520generalize%2520across%2520diverse%2520datasets%252C%250Aparticularly%2520when%2520the%2520data%2520contains%2520free-text.%2520Although%2520incorporating%2520language%250Amodel%2520capabilities%2520into%2520tabular%2520tasks%2520has%2520been%2520explored%252C%2520most%2520existing%2520methods%250Autilize%2520static%252C%2520target-agnostic%2520textual%2520representations%252C%2520limiting%2520their%250Aeffectiveness.%2520We%2520introduce%2520TabSTAR%253A%2520a%2520Foundation%2520Tabular%2520Model%2520with%250ASemantically%2520Target-Aware%2520Representations.%2520TabSTAR%2520is%2520designed%2520to%2520enable%250Atransfer%2520learning%2520on%2520tabular%2520data%2520with%2520textual%2520features%252C%2520with%2520an%2520architecture%250Afree%2520of%2520dataset-specific%2520parameters.%2520It%2520unfreezes%2520a%2520pretrained%2520text%2520encoder%2520and%250Atakes%2520as%2520input%2520target%2520tokens%252C%2520which%2520provide%2520the%2520model%2520with%2520the%2520context%2520needed%250Ato%2520learn%2520task-specific%2520embeddings.%2520TabSTAR%2520achieves%2520state-of-the-art%250Aperformance%2520for%2520both%2520medium-%2520and%2520large-sized%2520datasets%2520across%2520known%2520benchmarks%250Aof%2520classification%2520tasks%2520with%2520text%2520features%252C%2520and%2520its%2520pretraining%2520phase%2520exhibits%250Ascaling%2520laws%2520in%2520the%2520number%2520of%2520datasets%252C%2520offering%2520a%2520pathway%2520for%2520further%250Aperformance%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TabSTAR%3A%20A%20Foundation%20Tabular%20Model%20With%20Semantically%20Target-Aware%0A%20%20Representations&entry.906535625=Alan%20Arazi%20and%20Eilam%20Shapira%20and%20Roi%20Reichart&entry.1292438233=%20%20While%20deep%20learning%20has%20achieved%20remarkable%20success%20across%20many%20domains%2C%20it%0Ahas%20historically%20underperformed%20on%20tabular%20learning%20tasks%2C%20which%20remain%0Adominated%20by%20gradient%20boosting%20decision%20trees%20%28GBDTs%29.%20However%2C%20recent%0Aadvancements%20are%20paving%20the%20way%20for%20Tabular%20Foundation%20Models%2C%20which%20can%0Aleverage%20real-world%20knowledge%20and%20generalize%20across%20diverse%20datasets%2C%0Aparticularly%20when%20the%20data%20contains%20free-text.%20Although%20incorporating%20language%0Amodel%20capabilities%20into%20tabular%20tasks%20has%20been%20explored%2C%20most%20existing%20methods%0Autilize%20static%2C%20target-agnostic%20textual%20representations%2C%20limiting%20their%0Aeffectiveness.%20We%20introduce%20TabSTAR%3A%20a%20Foundation%20Tabular%20Model%20with%0ASemantically%20Target-Aware%20Representations.%20TabSTAR%20is%20designed%20to%20enable%0Atransfer%20learning%20on%20tabular%20data%20with%20textual%20features%2C%20with%20an%20architecture%0Afree%20of%20dataset-specific%20parameters.%20It%20unfreezes%20a%20pretrained%20text%20encoder%20and%0Atakes%20as%20input%20target%20tokens%2C%20which%20provide%20the%20model%20with%20the%20context%20needed%0Ato%20learn%20task-specific%20embeddings.%20TabSTAR%20achieves%20state-of-the-art%0Aperformance%20for%20both%20medium-%20and%20large-sized%20datasets%20across%20known%20benchmarks%0Aof%20classification%20tasks%20with%20text%20features%2C%20and%20its%20pretraining%20phase%20exhibits%0Ascaling%20laws%20in%20the%20number%20of%20datasets%2C%20offering%20a%20pathway%20for%20further%0Aperformance%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18125v1&entry.124074799=Read"},
{"title": "RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based\n  Temporal Knowledge Graph Completion", "author": "\u00d6mer Faruk Akg\u00fcl and Feiyu Zhu and Yuxin Yang and Rajgopal Kannan and Viktor Prasanna", "abstract": "  Temporal Knowledge Graphs (TKGs) represent dynamic facts as timestamped\nrelations between entities. TKG completion involves forecasting missing or\nfuture links, requiring models to reason over time-evolving structure. While\nLLMs show promise for this task, existing approaches often overemphasize\nsupervised fine-tuning and struggle particularly when historical evidence is\nlimited or missing. We introduce RECIPE-TKG, a lightweight and data-efficient\nframework designed to improve accuracy and generalization in settings with\nsparse historical context. It combines (1) rule-based multi-hop retrieval for\nstructurally diverse history, (2) contrastive fine-tuning of lightweight\nadapters to encode relational semantics, and (3) test-time semantic filtering\nto iteratively refine generations based on embedding similarity. Experiments on\nfour TKG benchmarks show that RECIPE-TKG outperforms previous LLM-based\napproaches, achieving up to 30.6\\% relative improvement in Hits@10. Moreover,\nour proposed framework produces more semantically coherent predictions, even\nfor the samples with limited historical context.\n", "link": "http://arxiv.org/abs/2505.17794v1", "date": "2025-05-23", "relevancy": 2.5158, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5171}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5025}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RECIPE-TKG%3A%20From%20Sparse%20History%20to%20Structured%20Reasoning%20for%20LLM-based%0A%20%20Temporal%20Knowledge%20Graph%20Completion&body=Title%3A%20RECIPE-TKG%3A%20From%20Sparse%20History%20to%20Structured%20Reasoning%20for%20LLM-based%0A%20%20Temporal%20Knowledge%20Graph%20Completion%0AAuthor%3A%20%C3%96mer%20Faruk%20Akg%C3%BCl%20and%20Feiyu%20Zhu%20and%20Yuxin%20Yang%20and%20Rajgopal%20Kannan%20and%20Viktor%20Prasanna%0AAbstract%3A%20%20%20Temporal%20Knowledge%20Graphs%20%28TKGs%29%20represent%20dynamic%20facts%20as%20timestamped%0Arelations%20between%20entities.%20TKG%20completion%20involves%20forecasting%20missing%20or%0Afuture%20links%2C%20requiring%20models%20to%20reason%20over%20time-evolving%20structure.%20While%0ALLMs%20show%20promise%20for%20this%20task%2C%20existing%20approaches%20often%20overemphasize%0Asupervised%20fine-tuning%20and%20struggle%20particularly%20when%20historical%20evidence%20is%0Alimited%20or%20missing.%20We%20introduce%20RECIPE-TKG%2C%20a%20lightweight%20and%20data-efficient%0Aframework%20designed%20to%20improve%20accuracy%20and%20generalization%20in%20settings%20with%0Asparse%20historical%20context.%20It%20combines%20%281%29%20rule-based%20multi-hop%20retrieval%20for%0Astructurally%20diverse%20history%2C%20%282%29%20contrastive%20fine-tuning%20of%20lightweight%0Aadapters%20to%20encode%20relational%20semantics%2C%20and%20%283%29%20test-time%20semantic%20filtering%0Ato%20iteratively%20refine%20generations%20based%20on%20embedding%20similarity.%20Experiments%20on%0Afour%20TKG%20benchmarks%20show%20that%20RECIPE-TKG%20outperforms%20previous%20LLM-based%0Aapproaches%2C%20achieving%20up%20to%2030.6%5C%25%20relative%20improvement%20in%20Hits%4010.%20Moreover%2C%0Aour%20proposed%20framework%20produces%20more%20semantically%20coherent%20predictions%2C%20even%0Afor%20the%20samples%20with%20limited%20historical%20context.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRECIPE-TKG%253A%2520From%2520Sparse%2520History%2520to%2520Structured%2520Reasoning%2520for%2520LLM-based%250A%2520%2520Temporal%2520Knowledge%2520Graph%2520Completion%26entry.906535625%3D%25C3%2596mer%2520Faruk%2520Akg%25C3%25BCl%2520and%2520Feiyu%2520Zhu%2520and%2520Yuxin%2520Yang%2520and%2520Rajgopal%2520Kannan%2520and%2520Viktor%2520Prasanna%26entry.1292438233%3D%2520%2520Temporal%2520Knowledge%2520Graphs%2520%2528TKGs%2529%2520represent%2520dynamic%2520facts%2520as%2520timestamped%250Arelations%2520between%2520entities.%2520TKG%2520completion%2520involves%2520forecasting%2520missing%2520or%250Afuture%2520links%252C%2520requiring%2520models%2520to%2520reason%2520over%2520time-evolving%2520structure.%2520While%250ALLMs%2520show%2520promise%2520for%2520this%2520task%252C%2520existing%2520approaches%2520often%2520overemphasize%250Asupervised%2520fine-tuning%2520and%2520struggle%2520particularly%2520when%2520historical%2520evidence%2520is%250Alimited%2520or%2520missing.%2520We%2520introduce%2520RECIPE-TKG%252C%2520a%2520lightweight%2520and%2520data-efficient%250Aframework%2520designed%2520to%2520improve%2520accuracy%2520and%2520generalization%2520in%2520settings%2520with%250Asparse%2520historical%2520context.%2520It%2520combines%2520%25281%2529%2520rule-based%2520multi-hop%2520retrieval%2520for%250Astructurally%2520diverse%2520history%252C%2520%25282%2529%2520contrastive%2520fine-tuning%2520of%2520lightweight%250Aadapters%2520to%2520encode%2520relational%2520semantics%252C%2520and%2520%25283%2529%2520test-time%2520semantic%2520filtering%250Ato%2520iteratively%2520refine%2520generations%2520based%2520on%2520embedding%2520similarity.%2520Experiments%2520on%250Afour%2520TKG%2520benchmarks%2520show%2520that%2520RECIPE-TKG%2520outperforms%2520previous%2520LLM-based%250Aapproaches%252C%2520achieving%2520up%2520to%252030.6%255C%2525%2520relative%2520improvement%2520in%2520Hits%254010.%2520Moreover%252C%250Aour%2520proposed%2520framework%2520produces%2520more%2520semantically%2520coherent%2520predictions%252C%2520even%250Afor%2520the%2520samples%2520with%2520limited%2520historical%2520context.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RECIPE-TKG%3A%20From%20Sparse%20History%20to%20Structured%20Reasoning%20for%20LLM-based%0A%20%20Temporal%20Knowledge%20Graph%20Completion&entry.906535625=%C3%96mer%20Faruk%20Akg%C3%BCl%20and%20Feiyu%20Zhu%20and%20Yuxin%20Yang%20and%20Rajgopal%20Kannan%20and%20Viktor%20Prasanna&entry.1292438233=%20%20Temporal%20Knowledge%20Graphs%20%28TKGs%29%20represent%20dynamic%20facts%20as%20timestamped%0Arelations%20between%20entities.%20TKG%20completion%20involves%20forecasting%20missing%20or%0Afuture%20links%2C%20requiring%20models%20to%20reason%20over%20time-evolving%20structure.%20While%0ALLMs%20show%20promise%20for%20this%20task%2C%20existing%20approaches%20often%20overemphasize%0Asupervised%20fine-tuning%20and%20struggle%20particularly%20when%20historical%20evidence%20is%0Alimited%20or%20missing.%20We%20introduce%20RECIPE-TKG%2C%20a%20lightweight%20and%20data-efficient%0Aframework%20designed%20to%20improve%20accuracy%20and%20generalization%20in%20settings%20with%0Asparse%20historical%20context.%20It%20combines%20%281%29%20rule-based%20multi-hop%20retrieval%20for%0Astructurally%20diverse%20history%2C%20%282%29%20contrastive%20fine-tuning%20of%20lightweight%0Aadapters%20to%20encode%20relational%20semantics%2C%20and%20%283%29%20test-time%20semantic%20filtering%0Ato%20iteratively%20refine%20generations%20based%20on%20embedding%20similarity.%20Experiments%20on%0Afour%20TKG%20benchmarks%20show%20that%20RECIPE-TKG%20outperforms%20previous%20LLM-based%0Aapproaches%2C%20achieving%20up%20to%2030.6%5C%25%20relative%20improvement%20in%20Hits%4010.%20Moreover%2C%0Aour%20proposed%20framework%20produces%20more%20semantically%20coherent%20predictions%2C%20even%0Afor%20the%20samples%20with%20limited%20historical%20context.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17794v1&entry.124074799=Read"},
{"title": "R-Genie: Reasoning-Guided Generative Image Editing", "author": "Dong Zhang and Lingfeng He and Rui Yan and Fei Shen and Jinhui Tang", "abstract": "  While recent advances in image editing have enabled impressive visual\nsynthesis capabilities, current methods remain constrained by explicit textual\ninstructions and limited editing operations, lacking deep comprehension of\nimplicit user intentions and contextual reasoning. In this work, we introduce a\nnew image editing paradigm: reasoning-guided generative editing, which\nsynthesizes images based on complex, multi-faceted textual queries accepting\nworld knowledge and intention inference. To facilitate this task, we first\nconstruct a comprehensive dataset featuring over 1,000 image-instruction-edit\ntriples that incorporate rich reasoning contexts and real-world knowledge. We\nthen propose R-Genie: a reasoning-guided generative image editor, which\nsynergizes the generation power of diffusion models with advanced reasoning\ncapabilities of multimodal large language models. R-Genie incorporates a\nreasoning-attention mechanism to bridge linguistic understanding with visual\nsynthesis, enabling it to handle intricate editing requests involving abstract\nuser intentions and contextual reasoning relations. Extensive experimental\nresults validate that R-Genie can equip diffusion models with advanced\nreasoning-based editing capabilities, unlocking new potentials for intelligent\nimage synthesis.\n", "link": "http://arxiv.org/abs/2505.17768v1", "date": "2025-05-23", "relevancy": 2.5157, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6573}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6131}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R-Genie%3A%20Reasoning-Guided%20Generative%20Image%20Editing&body=Title%3A%20R-Genie%3A%20Reasoning-Guided%20Generative%20Image%20Editing%0AAuthor%3A%20Dong%20Zhang%20and%20Lingfeng%20He%20and%20Rui%20Yan%20and%20Fei%20Shen%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20While%20recent%20advances%20in%20image%20editing%20have%20enabled%20impressive%20visual%0Asynthesis%20capabilities%2C%20current%20methods%20remain%20constrained%20by%20explicit%20textual%0Ainstructions%20and%20limited%20editing%20operations%2C%20lacking%20deep%20comprehension%20of%0Aimplicit%20user%20intentions%20and%20contextual%20reasoning.%20In%20this%20work%2C%20we%20introduce%20a%0Anew%20image%20editing%20paradigm%3A%20reasoning-guided%20generative%20editing%2C%20which%0Asynthesizes%20images%20based%20on%20complex%2C%20multi-faceted%20textual%20queries%20accepting%0Aworld%20knowledge%20and%20intention%20inference.%20To%20facilitate%20this%20task%2C%20we%20first%0Aconstruct%20a%20comprehensive%20dataset%20featuring%20over%201%2C000%20image-instruction-edit%0Atriples%20that%20incorporate%20rich%20reasoning%20contexts%20and%20real-world%20knowledge.%20We%0Athen%20propose%20R-Genie%3A%20a%20reasoning-guided%20generative%20image%20editor%2C%20which%0Asynergizes%20the%20generation%20power%20of%20diffusion%20models%20with%20advanced%20reasoning%0Acapabilities%20of%20multimodal%20large%20language%20models.%20R-Genie%20incorporates%20a%0Areasoning-attention%20mechanism%20to%20bridge%20linguistic%20understanding%20with%20visual%0Asynthesis%2C%20enabling%20it%20to%20handle%20intricate%20editing%20requests%20involving%20abstract%0Auser%20intentions%20and%20contextual%20reasoning%20relations.%20Extensive%20experimental%0Aresults%20validate%20that%20R-Genie%20can%20equip%20diffusion%20models%20with%20advanced%0Areasoning-based%20editing%20capabilities%2C%20unlocking%20new%20potentials%20for%20intelligent%0Aimage%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR-Genie%253A%2520Reasoning-Guided%2520Generative%2520Image%2520Editing%26entry.906535625%3DDong%2520Zhang%2520and%2520Lingfeng%2520He%2520and%2520Rui%2520Yan%2520and%2520Fei%2520Shen%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520While%2520recent%2520advances%2520in%2520image%2520editing%2520have%2520enabled%2520impressive%2520visual%250Asynthesis%2520capabilities%252C%2520current%2520methods%2520remain%2520constrained%2520by%2520explicit%2520textual%250Ainstructions%2520and%2520limited%2520editing%2520operations%252C%2520lacking%2520deep%2520comprehension%2520of%250Aimplicit%2520user%2520intentions%2520and%2520contextual%2520reasoning.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%250Anew%2520image%2520editing%2520paradigm%253A%2520reasoning-guided%2520generative%2520editing%252C%2520which%250Asynthesizes%2520images%2520based%2520on%2520complex%252C%2520multi-faceted%2520textual%2520queries%2520accepting%250Aworld%2520knowledge%2520and%2520intention%2520inference.%2520To%2520facilitate%2520this%2520task%252C%2520we%2520first%250Aconstruct%2520a%2520comprehensive%2520dataset%2520featuring%2520over%25201%252C000%2520image-instruction-edit%250Atriples%2520that%2520incorporate%2520rich%2520reasoning%2520contexts%2520and%2520real-world%2520knowledge.%2520We%250Athen%2520propose%2520R-Genie%253A%2520a%2520reasoning-guided%2520generative%2520image%2520editor%252C%2520which%250Asynergizes%2520the%2520generation%2520power%2520of%2520diffusion%2520models%2520with%2520advanced%2520reasoning%250Acapabilities%2520of%2520multimodal%2520large%2520language%2520models.%2520R-Genie%2520incorporates%2520a%250Areasoning-attention%2520mechanism%2520to%2520bridge%2520linguistic%2520understanding%2520with%2520visual%250Asynthesis%252C%2520enabling%2520it%2520to%2520handle%2520intricate%2520editing%2520requests%2520involving%2520abstract%250Auser%2520intentions%2520and%2520contextual%2520reasoning%2520relations.%2520Extensive%2520experimental%250Aresults%2520validate%2520that%2520R-Genie%2520can%2520equip%2520diffusion%2520models%2520with%2520advanced%250Areasoning-based%2520editing%2520capabilities%252C%2520unlocking%2520new%2520potentials%2520for%2520intelligent%250Aimage%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R-Genie%3A%20Reasoning-Guided%20Generative%20Image%20Editing&entry.906535625=Dong%20Zhang%20and%20Lingfeng%20He%20and%20Rui%20Yan%20and%20Fei%20Shen%20and%20Jinhui%20Tang&entry.1292438233=%20%20While%20recent%20advances%20in%20image%20editing%20have%20enabled%20impressive%20visual%0Asynthesis%20capabilities%2C%20current%20methods%20remain%20constrained%20by%20explicit%20textual%0Ainstructions%20and%20limited%20editing%20operations%2C%20lacking%20deep%20comprehension%20of%0Aimplicit%20user%20intentions%20and%20contextual%20reasoning.%20In%20this%20work%2C%20we%20introduce%20a%0Anew%20image%20editing%20paradigm%3A%20reasoning-guided%20generative%20editing%2C%20which%0Asynthesizes%20images%20based%20on%20complex%2C%20multi-faceted%20textual%20queries%20accepting%0Aworld%20knowledge%20and%20intention%20inference.%20To%20facilitate%20this%20task%2C%20we%20first%0Aconstruct%20a%20comprehensive%20dataset%20featuring%20over%201%2C000%20image-instruction-edit%0Atriples%20that%20incorporate%20rich%20reasoning%20contexts%20and%20real-world%20knowledge.%20We%0Athen%20propose%20R-Genie%3A%20a%20reasoning-guided%20generative%20image%20editor%2C%20which%0Asynergizes%20the%20generation%20power%20of%20diffusion%20models%20with%20advanced%20reasoning%0Acapabilities%20of%20multimodal%20large%20language%20models.%20R-Genie%20incorporates%20a%0Areasoning-attention%20mechanism%20to%20bridge%20linguistic%20understanding%20with%20visual%0Asynthesis%2C%20enabling%20it%20to%20handle%20intricate%20editing%20requests%20involving%20abstract%0Auser%20intentions%20and%20contextual%20reasoning%20relations.%20Extensive%20experimental%0Aresults%20validate%20that%20R-Genie%20can%20equip%20diffusion%20models%20with%20advanced%0Areasoning-based%20editing%20capabilities%2C%20unlocking%20new%20potentials%20for%20intelligent%0Aimage%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17768v1&entry.124074799=Read"},
{"title": "To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile\n  Mapping Cameras to Textured Semantic 3D Building Models", "author": "Simone Gaisbauer and Prabin Gyawali and Qilin Zhang and Olaf Wysocki and Boris Jutzi", "abstract": "  Feature matching is a necessary step for many computer vision and\nphotogrammetry applications such as image registration, structure-from-motion,\nand visual localization. Classical handcrafted methods such as SIFT feature\ndetection and description combined with nearest neighbour matching and RANSAC\noutlier removal have been state-of-the-art for mobile mapping cameras. With\nrecent advances in deep learning, learnable methods have been introduced and\nproven to have better robustness and performance under complex conditions.\nDespite their growing adoption, a comprehensive comparison between classical\nand learnable feature matching methods for the specific task of semantic 3D\nbuilding camera-to-model matching is still missing. This submission\nsystematically evaluates the effectiveness of different feature-matching\ntechniques in visual localization using textured CityGML LoD2 models. We use\nstandard benchmark datasets (HPatches, MegaDepth-1500) and custom datasets\nconsisting of facade textures and corresponding camera images (terrestrial and\ndrone). For the latter, we evaluate the achievable accuracy of the absolute\npose estimated using a Perspective-n-Point (PnP) algorithm, with geometric\nground truth derived from geo-referenced trajectory data. The results indicate\nthat the learnable feature matching methods vastly outperform traditional\napproaches regarding accuracy and robustness on our challenging custom datasets\nwith zero to 12 RANSAC-inliers and zero to 0.16 area under the curve. We\nbelieve that this work will foster the development of model-based visual\nlocalization methods. Link to the code:\nhttps://github.com/simBauer/To\\_Glue\\_or\\_not\\_to\\_Glue\n", "link": "http://arxiv.org/abs/2505.17973v1", "date": "2025-05-23", "relevancy": 2.5128, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6338}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6318}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20Glue%20or%20Not%20to%20Glue%3F%20Classical%20vs%20Learned%20Image%20Matching%20for%20Mobile%0A%20%20Mapping%20Cameras%20to%20Textured%20Semantic%203D%20Building%20Models&body=Title%3A%20To%20Glue%20or%20Not%20to%20Glue%3F%20Classical%20vs%20Learned%20Image%20Matching%20for%20Mobile%0A%20%20Mapping%20Cameras%20to%20Textured%20Semantic%203D%20Building%20Models%0AAuthor%3A%20Simone%20Gaisbauer%20and%20Prabin%20Gyawali%20and%20Qilin%20Zhang%20and%20Olaf%20Wysocki%20and%20Boris%20Jutzi%0AAbstract%3A%20%20%20Feature%20matching%20is%20a%20necessary%20step%20for%20many%20computer%20vision%20and%0Aphotogrammetry%20applications%20such%20as%20image%20registration%2C%20structure-from-motion%2C%0Aand%20visual%20localization.%20Classical%20handcrafted%20methods%20such%20as%20SIFT%20feature%0Adetection%20and%20description%20combined%20with%20nearest%20neighbour%20matching%20and%20RANSAC%0Aoutlier%20removal%20have%20been%20state-of-the-art%20for%20mobile%20mapping%20cameras.%20With%0Arecent%20advances%20in%20deep%20learning%2C%20learnable%20methods%20have%20been%20introduced%20and%0Aproven%20to%20have%20better%20robustness%20and%20performance%20under%20complex%20conditions.%0ADespite%20their%20growing%20adoption%2C%20a%20comprehensive%20comparison%20between%20classical%0Aand%20learnable%20feature%20matching%20methods%20for%20the%20specific%20task%20of%20semantic%203D%0Abuilding%20camera-to-model%20matching%20is%20still%20missing.%20This%20submission%0Asystematically%20evaluates%20the%20effectiveness%20of%20different%20feature-matching%0Atechniques%20in%20visual%20localization%20using%20textured%20CityGML%20LoD2%20models.%20We%20use%0Astandard%20benchmark%20datasets%20%28HPatches%2C%20MegaDepth-1500%29%20and%20custom%20datasets%0Aconsisting%20of%20facade%20textures%20and%20corresponding%20camera%20images%20%28terrestrial%20and%0Adrone%29.%20For%20the%20latter%2C%20we%20evaluate%20the%20achievable%20accuracy%20of%20the%20absolute%0Apose%20estimated%20using%20a%20Perspective-n-Point%20%28PnP%29%20algorithm%2C%20with%20geometric%0Aground%20truth%20derived%20from%20geo-referenced%20trajectory%20data.%20The%20results%20indicate%0Athat%20the%20learnable%20feature%20matching%20methods%20vastly%20outperform%20traditional%0Aapproaches%20regarding%20accuracy%20and%20robustness%20on%20our%20challenging%20custom%20datasets%0Awith%20zero%20to%2012%20RANSAC-inliers%20and%20zero%20to%200.16%20area%20under%20the%20curve.%20We%0Abelieve%20that%20this%20work%20will%20foster%20the%20development%20of%20model-based%20visual%0Alocalization%20methods.%20Link%20to%20the%20code%3A%0Ahttps%3A//github.com/simBauer/To%5C_Glue%5C_or%5C_not%5C_to%5C_Glue%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520Glue%2520or%2520Not%2520to%2520Glue%253F%2520Classical%2520vs%2520Learned%2520Image%2520Matching%2520for%2520Mobile%250A%2520%2520Mapping%2520Cameras%2520to%2520Textured%2520Semantic%25203D%2520Building%2520Models%26entry.906535625%3DSimone%2520Gaisbauer%2520and%2520Prabin%2520Gyawali%2520and%2520Qilin%2520Zhang%2520and%2520Olaf%2520Wysocki%2520and%2520Boris%2520Jutzi%26entry.1292438233%3D%2520%2520Feature%2520matching%2520is%2520a%2520necessary%2520step%2520for%2520many%2520computer%2520vision%2520and%250Aphotogrammetry%2520applications%2520such%2520as%2520image%2520registration%252C%2520structure-from-motion%252C%250Aand%2520visual%2520localization.%2520Classical%2520handcrafted%2520methods%2520such%2520as%2520SIFT%2520feature%250Adetection%2520and%2520description%2520combined%2520with%2520nearest%2520neighbour%2520matching%2520and%2520RANSAC%250Aoutlier%2520removal%2520have%2520been%2520state-of-the-art%2520for%2520mobile%2520mapping%2520cameras.%2520With%250Arecent%2520advances%2520in%2520deep%2520learning%252C%2520learnable%2520methods%2520have%2520been%2520introduced%2520and%250Aproven%2520to%2520have%2520better%2520robustness%2520and%2520performance%2520under%2520complex%2520conditions.%250ADespite%2520their%2520growing%2520adoption%252C%2520a%2520comprehensive%2520comparison%2520between%2520classical%250Aand%2520learnable%2520feature%2520matching%2520methods%2520for%2520the%2520specific%2520task%2520of%2520semantic%25203D%250Abuilding%2520camera-to-model%2520matching%2520is%2520still%2520missing.%2520This%2520submission%250Asystematically%2520evaluates%2520the%2520effectiveness%2520of%2520different%2520feature-matching%250Atechniques%2520in%2520visual%2520localization%2520using%2520textured%2520CityGML%2520LoD2%2520models.%2520We%2520use%250Astandard%2520benchmark%2520datasets%2520%2528HPatches%252C%2520MegaDepth-1500%2529%2520and%2520custom%2520datasets%250Aconsisting%2520of%2520facade%2520textures%2520and%2520corresponding%2520camera%2520images%2520%2528terrestrial%2520and%250Adrone%2529.%2520For%2520the%2520latter%252C%2520we%2520evaluate%2520the%2520achievable%2520accuracy%2520of%2520the%2520absolute%250Apose%2520estimated%2520using%2520a%2520Perspective-n-Point%2520%2528PnP%2529%2520algorithm%252C%2520with%2520geometric%250Aground%2520truth%2520derived%2520from%2520geo-referenced%2520trajectory%2520data.%2520The%2520results%2520indicate%250Athat%2520the%2520learnable%2520feature%2520matching%2520methods%2520vastly%2520outperform%2520traditional%250Aapproaches%2520regarding%2520accuracy%2520and%2520robustness%2520on%2520our%2520challenging%2520custom%2520datasets%250Awith%2520zero%2520to%252012%2520RANSAC-inliers%2520and%2520zero%2520to%25200.16%2520area%2520under%2520the%2520curve.%2520We%250Abelieve%2520that%2520this%2520work%2520will%2520foster%2520the%2520development%2520of%2520model-based%2520visual%250Alocalization%2520methods.%2520Link%2520to%2520the%2520code%253A%250Ahttps%253A//github.com/simBauer/To%255C_Glue%255C_or%255C_not%255C_to%255C_Glue%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Glue%20or%20Not%20to%20Glue%3F%20Classical%20vs%20Learned%20Image%20Matching%20for%20Mobile%0A%20%20Mapping%20Cameras%20to%20Textured%20Semantic%203D%20Building%20Models&entry.906535625=Simone%20Gaisbauer%20and%20Prabin%20Gyawali%20and%20Qilin%20Zhang%20and%20Olaf%20Wysocki%20and%20Boris%20Jutzi&entry.1292438233=%20%20Feature%20matching%20is%20a%20necessary%20step%20for%20many%20computer%20vision%20and%0Aphotogrammetry%20applications%20such%20as%20image%20registration%2C%20structure-from-motion%2C%0Aand%20visual%20localization.%20Classical%20handcrafted%20methods%20such%20as%20SIFT%20feature%0Adetection%20and%20description%20combined%20with%20nearest%20neighbour%20matching%20and%20RANSAC%0Aoutlier%20removal%20have%20been%20state-of-the-art%20for%20mobile%20mapping%20cameras.%20With%0Arecent%20advances%20in%20deep%20learning%2C%20learnable%20methods%20have%20been%20introduced%20and%0Aproven%20to%20have%20better%20robustness%20and%20performance%20under%20complex%20conditions.%0ADespite%20their%20growing%20adoption%2C%20a%20comprehensive%20comparison%20between%20classical%0Aand%20learnable%20feature%20matching%20methods%20for%20the%20specific%20task%20of%20semantic%203D%0Abuilding%20camera-to-model%20matching%20is%20still%20missing.%20This%20submission%0Asystematically%20evaluates%20the%20effectiveness%20of%20different%20feature-matching%0Atechniques%20in%20visual%20localization%20using%20textured%20CityGML%20LoD2%20models.%20We%20use%0Astandard%20benchmark%20datasets%20%28HPatches%2C%20MegaDepth-1500%29%20and%20custom%20datasets%0Aconsisting%20of%20facade%20textures%20and%20corresponding%20camera%20images%20%28terrestrial%20and%0Adrone%29.%20For%20the%20latter%2C%20we%20evaluate%20the%20achievable%20accuracy%20of%20the%20absolute%0Apose%20estimated%20using%20a%20Perspective-n-Point%20%28PnP%29%20algorithm%2C%20with%20geometric%0Aground%20truth%20derived%20from%20geo-referenced%20trajectory%20data.%20The%20results%20indicate%0Athat%20the%20learnable%20feature%20matching%20methods%20vastly%20outperform%20traditional%0Aapproaches%20regarding%20accuracy%20and%20robustness%20on%20our%20challenging%20custom%20datasets%0Awith%20zero%20to%2012%20RANSAC-inliers%20and%20zero%20to%200.16%20area%20under%20the%20curve.%20We%0Abelieve%20that%20this%20work%20will%20foster%20the%20development%20of%20model-based%20visual%0Alocalization%20methods.%20Link%20to%20the%20code%3A%0Ahttps%3A//github.com/simBauer/To%5C_Glue%5C_or%5C_not%5C_to%5C_Glue%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17973v1&entry.124074799=Read"},
{"title": "Semi-Supervised Multi-Label Feature Selection with Consistent Sparse\n  Graph Learning", "author": "Yan Zhong and Xingyu Wu and Xinping Zhao and Li Zhang and Xinyuan Song and Lei Shi and Bingbing Jiang", "abstract": "  In practical domains, high-dimensional data are usually associated with\ndiverse semantic labels, whereas traditional feature selection methods are\ndesigned for single-label data. Moreover, existing multi-label methods\nencounter two main challenges in semi-supervised scenarios: (1). Most\nsemi-supervised methods fail to evaluate the label correlations without enough\nlabeled samples, which are the critical information of multi-label feature\nselection, making label-specific features discarded. (2). The similarity graph\nstructure directly derived from the original feature space is suboptimal for\nmulti-label problems in existing graph-based methods, leading to unreliable\nsoft labels and degraded feature selection performance. To overcome them, we\npropose a consistent sparse graph learning method for multi-label\nsemi-supervised feature selection (SGMFS), which can enhance the feature\nselection performance by maintaining space consistency and learning label\ncorrelations in semi-supervised scenarios. Specifically, for Challenge (1),\nSGMFS learns a low-dimensional and independent label subspace from the\nprojected features, which can compatibly cross multiple labels and effectively\nachieve the label correlations. For Challenge (2), instead of constructing a\nfixed similarity graph for semi-supervised learning, SGMFS thoroughly explores\nthe intrinsic structure of the data by performing sparse reconstruction of\nsamples in both the label space and the learned subspace simultaneously. In\nthis way, the similarity graph can be adaptively learned to maintain the\nconsistency between label space and the learned subspace, which can promote\npropagating proper soft labels for unlabeled samples, facilitating the ultimate\nfeature selection. An effective solution with fast convergence is designed to\noptimize the objective function. Extensive experiments validate the superiority\nof SGMFS.\n", "link": "http://arxiv.org/abs/2505.17875v1", "date": "2025-05-23", "relevancy": 2.5073, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5489}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4833}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Multi-Label%20Feature%20Selection%20with%20Consistent%20Sparse%0A%20%20Graph%20Learning&body=Title%3A%20Semi-Supervised%20Multi-Label%20Feature%20Selection%20with%20Consistent%20Sparse%0A%20%20Graph%20Learning%0AAuthor%3A%20Yan%20Zhong%20and%20Xingyu%20Wu%20and%20Xinping%20Zhao%20and%20Li%20Zhang%20and%20Xinyuan%20Song%20and%20Lei%20Shi%20and%20Bingbing%20Jiang%0AAbstract%3A%20%20%20In%20practical%20domains%2C%20high-dimensional%20data%20are%20usually%20associated%20with%0Adiverse%20semantic%20labels%2C%20whereas%20traditional%20feature%20selection%20methods%20are%0Adesigned%20for%20single-label%20data.%20Moreover%2C%20existing%20multi-label%20methods%0Aencounter%20two%20main%20challenges%20in%20semi-supervised%20scenarios%3A%20%281%29.%20Most%0Asemi-supervised%20methods%20fail%20to%20evaluate%20the%20label%20correlations%20without%20enough%0Alabeled%20samples%2C%20which%20are%20the%20critical%20information%20of%20multi-label%20feature%0Aselection%2C%20making%20label-specific%20features%20discarded.%20%282%29.%20The%20similarity%20graph%0Astructure%20directly%20derived%20from%20the%20original%20feature%20space%20is%20suboptimal%20for%0Amulti-label%20problems%20in%20existing%20graph-based%20methods%2C%20leading%20to%20unreliable%0Asoft%20labels%20and%20degraded%20feature%20selection%20performance.%20To%20overcome%20them%2C%20we%0Apropose%20a%20consistent%20sparse%20graph%20learning%20method%20for%20multi-label%0Asemi-supervised%20feature%20selection%20%28SGMFS%29%2C%20which%20can%20enhance%20the%20feature%0Aselection%20performance%20by%20maintaining%20space%20consistency%20and%20learning%20label%0Acorrelations%20in%20semi-supervised%20scenarios.%20Specifically%2C%20for%20Challenge%20%281%29%2C%0ASGMFS%20learns%20a%20low-dimensional%20and%20independent%20label%20subspace%20from%20the%0Aprojected%20features%2C%20which%20can%20compatibly%20cross%20multiple%20labels%20and%20effectively%0Aachieve%20the%20label%20correlations.%20For%20Challenge%20%282%29%2C%20instead%20of%20constructing%20a%0Afixed%20similarity%20graph%20for%20semi-supervised%20learning%2C%20SGMFS%20thoroughly%20explores%0Athe%20intrinsic%20structure%20of%20the%20data%20by%20performing%20sparse%20reconstruction%20of%0Asamples%20in%20both%20the%20label%20space%20and%20the%20learned%20subspace%20simultaneously.%20In%0Athis%20way%2C%20the%20similarity%20graph%20can%20be%20adaptively%20learned%20to%20maintain%20the%0Aconsistency%20between%20label%20space%20and%20the%20learned%20subspace%2C%20which%20can%20promote%0Apropagating%20proper%20soft%20labels%20for%20unlabeled%20samples%2C%20facilitating%20the%20ultimate%0Afeature%20selection.%20An%20effective%20solution%20with%20fast%20convergence%20is%20designed%20to%0Aoptimize%20the%20objective%20function.%20Extensive%20experiments%20validate%20the%20superiority%0Aof%20SGMFS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%2520Multi-Label%2520Feature%2520Selection%2520with%2520Consistent%2520Sparse%250A%2520%2520Graph%2520Learning%26entry.906535625%3DYan%2520Zhong%2520and%2520Xingyu%2520Wu%2520and%2520Xinping%2520Zhao%2520and%2520Li%2520Zhang%2520and%2520Xinyuan%2520Song%2520and%2520Lei%2520Shi%2520and%2520Bingbing%2520Jiang%26entry.1292438233%3D%2520%2520In%2520practical%2520domains%252C%2520high-dimensional%2520data%2520are%2520usually%2520associated%2520with%250Adiverse%2520semantic%2520labels%252C%2520whereas%2520traditional%2520feature%2520selection%2520methods%2520are%250Adesigned%2520for%2520single-label%2520data.%2520Moreover%252C%2520existing%2520multi-label%2520methods%250Aencounter%2520two%2520main%2520challenges%2520in%2520semi-supervised%2520scenarios%253A%2520%25281%2529.%2520Most%250Asemi-supervised%2520methods%2520fail%2520to%2520evaluate%2520the%2520label%2520correlations%2520without%2520enough%250Alabeled%2520samples%252C%2520which%2520are%2520the%2520critical%2520information%2520of%2520multi-label%2520feature%250Aselection%252C%2520making%2520label-specific%2520features%2520discarded.%2520%25282%2529.%2520The%2520similarity%2520graph%250Astructure%2520directly%2520derived%2520from%2520the%2520original%2520feature%2520space%2520is%2520suboptimal%2520for%250Amulti-label%2520problems%2520in%2520existing%2520graph-based%2520methods%252C%2520leading%2520to%2520unreliable%250Asoft%2520labels%2520and%2520degraded%2520feature%2520selection%2520performance.%2520To%2520overcome%2520them%252C%2520we%250Apropose%2520a%2520consistent%2520sparse%2520graph%2520learning%2520method%2520for%2520multi-label%250Asemi-supervised%2520feature%2520selection%2520%2528SGMFS%2529%252C%2520which%2520can%2520enhance%2520the%2520feature%250Aselection%2520performance%2520by%2520maintaining%2520space%2520consistency%2520and%2520learning%2520label%250Acorrelations%2520in%2520semi-supervised%2520scenarios.%2520Specifically%252C%2520for%2520Challenge%2520%25281%2529%252C%250ASGMFS%2520learns%2520a%2520low-dimensional%2520and%2520independent%2520label%2520subspace%2520from%2520the%250Aprojected%2520features%252C%2520which%2520can%2520compatibly%2520cross%2520multiple%2520labels%2520and%2520effectively%250Aachieve%2520the%2520label%2520correlations.%2520For%2520Challenge%2520%25282%2529%252C%2520instead%2520of%2520constructing%2520a%250Afixed%2520similarity%2520graph%2520for%2520semi-supervised%2520learning%252C%2520SGMFS%2520thoroughly%2520explores%250Athe%2520intrinsic%2520structure%2520of%2520the%2520data%2520by%2520performing%2520sparse%2520reconstruction%2520of%250Asamples%2520in%2520both%2520the%2520label%2520space%2520and%2520the%2520learned%2520subspace%2520simultaneously.%2520In%250Athis%2520way%252C%2520the%2520similarity%2520graph%2520can%2520be%2520adaptively%2520learned%2520to%2520maintain%2520the%250Aconsistency%2520between%2520label%2520space%2520and%2520the%2520learned%2520subspace%252C%2520which%2520can%2520promote%250Apropagating%2520proper%2520soft%2520labels%2520for%2520unlabeled%2520samples%252C%2520facilitating%2520the%2520ultimate%250Afeature%2520selection.%2520An%2520effective%2520solution%2520with%2520fast%2520convergence%2520is%2520designed%2520to%250Aoptimize%2520the%2520objective%2520function.%2520Extensive%2520experiments%2520validate%2520the%2520superiority%250Aof%2520SGMFS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Multi-Label%20Feature%20Selection%20with%20Consistent%20Sparse%0A%20%20Graph%20Learning&entry.906535625=Yan%20Zhong%20and%20Xingyu%20Wu%20and%20Xinping%20Zhao%20and%20Li%20Zhang%20and%20Xinyuan%20Song%20and%20Lei%20Shi%20and%20Bingbing%20Jiang&entry.1292438233=%20%20In%20practical%20domains%2C%20high-dimensional%20data%20are%20usually%20associated%20with%0Adiverse%20semantic%20labels%2C%20whereas%20traditional%20feature%20selection%20methods%20are%0Adesigned%20for%20single-label%20data.%20Moreover%2C%20existing%20multi-label%20methods%0Aencounter%20two%20main%20challenges%20in%20semi-supervised%20scenarios%3A%20%281%29.%20Most%0Asemi-supervised%20methods%20fail%20to%20evaluate%20the%20label%20correlations%20without%20enough%0Alabeled%20samples%2C%20which%20are%20the%20critical%20information%20of%20multi-label%20feature%0Aselection%2C%20making%20label-specific%20features%20discarded.%20%282%29.%20The%20similarity%20graph%0Astructure%20directly%20derived%20from%20the%20original%20feature%20space%20is%20suboptimal%20for%0Amulti-label%20problems%20in%20existing%20graph-based%20methods%2C%20leading%20to%20unreliable%0Asoft%20labels%20and%20degraded%20feature%20selection%20performance.%20To%20overcome%20them%2C%20we%0Apropose%20a%20consistent%20sparse%20graph%20learning%20method%20for%20multi-label%0Asemi-supervised%20feature%20selection%20%28SGMFS%29%2C%20which%20can%20enhance%20the%20feature%0Aselection%20performance%20by%20maintaining%20space%20consistency%20and%20learning%20label%0Acorrelations%20in%20semi-supervised%20scenarios.%20Specifically%2C%20for%20Challenge%20%281%29%2C%0ASGMFS%20learns%20a%20low-dimensional%20and%20independent%20label%20subspace%20from%20the%0Aprojected%20features%2C%20which%20can%20compatibly%20cross%20multiple%20labels%20and%20effectively%0Aachieve%20the%20label%20correlations.%20For%20Challenge%20%282%29%2C%20instead%20of%20constructing%20a%0Afixed%20similarity%20graph%20for%20semi-supervised%20learning%2C%20SGMFS%20thoroughly%20explores%0Athe%20intrinsic%20structure%20of%20the%20data%20by%20performing%20sparse%20reconstruction%20of%0Asamples%20in%20both%20the%20label%20space%20and%20the%20learned%20subspace%20simultaneously.%20In%0Athis%20way%2C%20the%20similarity%20graph%20can%20be%20adaptively%20learned%20to%20maintain%20the%0Aconsistency%20between%20label%20space%20and%20the%20learned%20subspace%2C%20which%20can%20promote%0Apropagating%20proper%20soft%20labels%20for%20unlabeled%20samples%2C%20facilitating%20the%20ultimate%0Afeature%20selection.%20An%20effective%20solution%20with%20fast%20convergence%20is%20designed%20to%0Aoptimize%20the%20objective%20function.%20Extensive%20experiments%20validate%20the%20superiority%0Aof%20SGMFS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17875v1&entry.124074799=Read"},
{"title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient", "author": "Zigeng Chen and Xinyin Ma and Gongfan Fang and Ruonan Yu and Xinchao Wang", "abstract": "  Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought\n(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily\nlengthy reasoning chains, dramatically increasing inference costs. To mitigate\nthis issue, we introduce VeriThinker, a novel approach for CoT compression.\nUnlike conventional methods that fine-tune LRMs directly on the original\nreasoning task using synthetic concise CoT data, we innovatively fine-tune the\nmodel solely through an auxiliary verification task. By training LRMs to\naccurately verify the correctness of CoT solutions, the LRMs inherently become\nmore discerning about the necessity of subsequent self-reflection steps,\nthereby effectively suppressing overthinking. Extensive experiments validate\nthat VeriThinker substantially reduces reasoning chain lengths while\nmaintaining or even slightly improving accuracy. When applied to\nDeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500\nfrom 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on\nAIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to\n40.8%). Additionally, our experiments demonstrate that VeriThinker can also be\nzero-shot generalized to speculative reasoning. Code is available at\nhttps://github.com/czg1225/VeriThinker\n", "link": "http://arxiv.org/abs/2505.17941v1", "date": "2025-05-23", "relevancy": 2.5056, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VeriThinker%3A%20Learning%20to%20Verify%20Makes%20Reasoning%20Model%20Efficient&body=Title%3A%20VeriThinker%3A%20Learning%20to%20Verify%20Makes%20Reasoning%20Model%20Efficient%0AAuthor%3A%20Zigeng%20Chen%20and%20Xinyin%20Ma%20and%20Gongfan%20Fang%20and%20Ruonan%20Yu%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Large%20Reasoning%20Models%20%28LRMs%29%20excel%20at%20complex%20tasks%20using%20Chain-of-Thought%0A%28CoT%29%20reasoning.%20However%2C%20their%20tendency%20to%20overthinking%20leads%20to%20unnecessarily%0Alengthy%20reasoning%20chains%2C%20dramatically%20increasing%20inference%20costs.%20To%20mitigate%0Athis%20issue%2C%20we%20introduce%20VeriThinker%2C%20a%20novel%20approach%20for%20CoT%20compression.%0AUnlike%20conventional%20methods%20that%20fine-tune%20LRMs%20directly%20on%20the%20original%0Areasoning%20task%20using%20synthetic%20concise%20CoT%20data%2C%20we%20innovatively%20fine-tune%20the%0Amodel%20solely%20through%20an%20auxiliary%20verification%20task.%20By%20training%20LRMs%20to%0Aaccurately%20verify%20the%20correctness%20of%20CoT%20solutions%2C%20the%20LRMs%20inherently%20become%0Amore%20discerning%20about%20the%20necessity%20of%20subsequent%20self-reflection%20steps%2C%0Athereby%20effectively%20suppressing%20overthinking.%20Extensive%20experiments%20validate%0Athat%20VeriThinker%20substantially%20reduces%20reasoning%20chain%20lengths%20while%0Amaintaining%20or%20even%20slightly%20improving%20accuracy.%20When%20applied%20to%0ADeepSeek-R1-Distill-Qwen-7B%2C%20our%20approach%20reduces%20reasoning%20tokens%20on%20MATH500%0Afrom%203790%20to%202125%20while%20improving%20accuracy%20by%200.8%25%20%2894.0%25%20to%2094.8%25%29%2C%20and%20on%0AAIME25%2C%20tokens%20decrease%20from%2014321%20to%2010287%20with%20a%202.1%25%20accuracy%20gain%20%2838.7%25%20to%0A40.8%25%29.%20Additionally%2C%20our%20experiments%20demonstrate%20that%20VeriThinker%20can%20also%20be%0Azero-shot%20generalized%20to%20speculative%20reasoning.%20Code%20is%20available%20at%0Ahttps%3A//github.com/czg1225/VeriThinker%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17941v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVeriThinker%253A%2520Learning%2520to%2520Verify%2520Makes%2520Reasoning%2520Model%2520Efficient%26entry.906535625%3DZigeng%2520Chen%2520and%2520Xinyin%2520Ma%2520and%2520Gongfan%2520Fang%2520and%2520Ruonan%2520Yu%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%2520excel%2520at%2520complex%2520tasks%2520using%2520Chain-of-Thought%250A%2528CoT%2529%2520reasoning.%2520However%252C%2520their%2520tendency%2520to%2520overthinking%2520leads%2520to%2520unnecessarily%250Alengthy%2520reasoning%2520chains%252C%2520dramatically%2520increasing%2520inference%2520costs.%2520To%2520mitigate%250Athis%2520issue%252C%2520we%2520introduce%2520VeriThinker%252C%2520a%2520novel%2520approach%2520for%2520CoT%2520compression.%250AUnlike%2520conventional%2520methods%2520that%2520fine-tune%2520LRMs%2520directly%2520on%2520the%2520original%250Areasoning%2520task%2520using%2520synthetic%2520concise%2520CoT%2520data%252C%2520we%2520innovatively%2520fine-tune%2520the%250Amodel%2520solely%2520through%2520an%2520auxiliary%2520verification%2520task.%2520By%2520training%2520LRMs%2520to%250Aaccurately%2520verify%2520the%2520correctness%2520of%2520CoT%2520solutions%252C%2520the%2520LRMs%2520inherently%2520become%250Amore%2520discerning%2520about%2520the%2520necessity%2520of%2520subsequent%2520self-reflection%2520steps%252C%250Athereby%2520effectively%2520suppressing%2520overthinking.%2520Extensive%2520experiments%2520validate%250Athat%2520VeriThinker%2520substantially%2520reduces%2520reasoning%2520chain%2520lengths%2520while%250Amaintaining%2520or%2520even%2520slightly%2520improving%2520accuracy.%2520When%2520applied%2520to%250ADeepSeek-R1-Distill-Qwen-7B%252C%2520our%2520approach%2520reduces%2520reasoning%2520tokens%2520on%2520MATH500%250Afrom%25203790%2520to%25202125%2520while%2520improving%2520accuracy%2520by%25200.8%2525%2520%252894.0%2525%2520to%252094.8%2525%2529%252C%2520and%2520on%250AAIME25%252C%2520tokens%2520decrease%2520from%252014321%2520to%252010287%2520with%2520a%25202.1%2525%2520accuracy%2520gain%2520%252838.7%2525%2520to%250A40.8%2525%2529.%2520Additionally%252C%2520our%2520experiments%2520demonstrate%2520that%2520VeriThinker%2520can%2520also%2520be%250Azero-shot%2520generalized%2520to%2520speculative%2520reasoning.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/czg1225/VeriThinker%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17941v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VeriThinker%3A%20Learning%20to%20Verify%20Makes%20Reasoning%20Model%20Efficient&entry.906535625=Zigeng%20Chen%20and%20Xinyin%20Ma%20and%20Gongfan%20Fang%20and%20Ruonan%20Yu%20and%20Xinchao%20Wang&entry.1292438233=%20%20Large%20Reasoning%20Models%20%28LRMs%29%20excel%20at%20complex%20tasks%20using%20Chain-of-Thought%0A%28CoT%29%20reasoning.%20However%2C%20their%20tendency%20to%20overthinking%20leads%20to%20unnecessarily%0Alengthy%20reasoning%20chains%2C%20dramatically%20increasing%20inference%20costs.%20To%20mitigate%0Athis%20issue%2C%20we%20introduce%20VeriThinker%2C%20a%20novel%20approach%20for%20CoT%20compression.%0AUnlike%20conventional%20methods%20that%20fine-tune%20LRMs%20directly%20on%20the%20original%0Areasoning%20task%20using%20synthetic%20concise%20CoT%20data%2C%20we%20innovatively%20fine-tune%20the%0Amodel%20solely%20through%20an%20auxiliary%20verification%20task.%20By%20training%20LRMs%20to%0Aaccurately%20verify%20the%20correctness%20of%20CoT%20solutions%2C%20the%20LRMs%20inherently%20become%0Amore%20discerning%20about%20the%20necessity%20of%20subsequent%20self-reflection%20steps%2C%0Athereby%20effectively%20suppressing%20overthinking.%20Extensive%20experiments%20validate%0Athat%20VeriThinker%20substantially%20reduces%20reasoning%20chain%20lengths%20while%0Amaintaining%20or%20even%20slightly%20improving%20accuracy.%20When%20applied%20to%0ADeepSeek-R1-Distill-Qwen-7B%2C%20our%20approach%20reduces%20reasoning%20tokens%20on%20MATH500%0Afrom%203790%20to%202125%20while%20improving%20accuracy%20by%200.8%25%20%2894.0%25%20to%2094.8%25%29%2C%20and%20on%0AAIME25%2C%20tokens%20decrease%20from%2014321%20to%2010287%20with%20a%202.1%25%20accuracy%20gain%20%2838.7%25%20to%0A40.8%25%29.%20Additionally%2C%20our%20experiments%20demonstrate%20that%20VeriThinker%20can%20also%20be%0Azero-shot%20generalized%20to%20speculative%20reasoning.%20Code%20is%20available%20at%0Ahttps%3A//github.com/czg1225/VeriThinker%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17941v1&entry.124074799=Read"},
{"title": "Multi-Person Interaction Generation from Two-Person Motion Priors", "author": "Wenning Xu and Shiyu Fan and Paul Henderson and Edmond S. L. Ho", "abstract": "  Generating realistic human motion with high-level controls is a crucial task\nfor social understanding, robotics, and animation. With high-quality MOCAP data\nbecoming more available recently, a wide range of data-driven approaches have\nbeen presented. However, modelling multi-person interactions still remains a\nless explored area. In this paper, we present Graph-driven Interaction\nSampling, a method that can generate realistic and diverse multi-person\ninteractions by leveraging existing two-person motion diffusion models as\nmotion priors. Instead of training a new model specific to multi-person\ninteraction synthesis, our key insight is to spatially and temporally separate\ncomplex multi-person interactions into a graph structure of two-person\ninteractions, which we name the Pairwise Interaction Graph. We thus decompose\nthe generation task into simultaneous single-person motion generation\nconditioned on one other's motion. In addition, to reduce artifacts such as\ninterpenetrations of body parts in generated multi-person interactions, we\nintroduce two graph-dependent guidance terms into the diffusion sampling\nscheme. Unlike previous work, our method can produce various high-quality\nmulti-person interactions without having repetitive individual motions.\nExtensive experiments demonstrate that our approach consistently outperforms\nexisting methods in reducing artifacts when generating a wide range of\ntwo-person and multi-person interactions.\n", "link": "http://arxiv.org/abs/2505.17860v1", "date": "2025-05-23", "relevancy": 2.4836, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7008}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5673}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Person%20Interaction%20Generation%20from%20Two-Person%20Motion%20Priors&body=Title%3A%20Multi-Person%20Interaction%20Generation%20from%20Two-Person%20Motion%20Priors%0AAuthor%3A%20Wenning%20Xu%20and%20Shiyu%20Fan%20and%20Paul%20Henderson%20and%20Edmond%20S.%20L.%20Ho%0AAbstract%3A%20%20%20Generating%20realistic%20human%20motion%20with%20high-level%20controls%20is%20a%20crucial%20task%0Afor%20social%20understanding%2C%20robotics%2C%20and%20animation.%20With%20high-quality%20MOCAP%20data%0Abecoming%20more%20available%20recently%2C%20a%20wide%20range%20of%20data-driven%20approaches%20have%0Abeen%20presented.%20However%2C%20modelling%20multi-person%20interactions%20still%20remains%20a%0Aless%20explored%20area.%20In%20this%20paper%2C%20we%20present%20Graph-driven%20Interaction%0ASampling%2C%20a%20method%20that%20can%20generate%20realistic%20and%20diverse%20multi-person%0Ainteractions%20by%20leveraging%20existing%20two-person%20motion%20diffusion%20models%20as%0Amotion%20priors.%20Instead%20of%20training%20a%20new%20model%20specific%20to%20multi-person%0Ainteraction%20synthesis%2C%20our%20key%20insight%20is%20to%20spatially%20and%20temporally%20separate%0Acomplex%20multi-person%20interactions%20into%20a%20graph%20structure%20of%20two-person%0Ainteractions%2C%20which%20we%20name%20the%20Pairwise%20Interaction%20Graph.%20We%20thus%20decompose%0Athe%20generation%20task%20into%20simultaneous%20single-person%20motion%20generation%0Aconditioned%20on%20one%20other%27s%20motion.%20In%20addition%2C%20to%20reduce%20artifacts%20such%20as%0Ainterpenetrations%20of%20body%20parts%20in%20generated%20multi-person%20interactions%2C%20we%0Aintroduce%20two%20graph-dependent%20guidance%20terms%20into%20the%20diffusion%20sampling%0Ascheme.%20Unlike%20previous%20work%2C%20our%20method%20can%20produce%20various%20high-quality%0Amulti-person%20interactions%20without%20having%20repetitive%20individual%20motions.%0AExtensive%20experiments%20demonstrate%20that%20our%20approach%20consistently%20outperforms%0Aexisting%20methods%20in%20reducing%20artifacts%20when%20generating%20a%20wide%20range%20of%0Atwo-person%20and%20multi-person%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Person%2520Interaction%2520Generation%2520from%2520Two-Person%2520Motion%2520Priors%26entry.906535625%3DWenning%2520Xu%2520and%2520Shiyu%2520Fan%2520and%2520Paul%2520Henderson%2520and%2520Edmond%2520S.%2520L.%2520Ho%26entry.1292438233%3D%2520%2520Generating%2520realistic%2520human%2520motion%2520with%2520high-level%2520controls%2520is%2520a%2520crucial%2520task%250Afor%2520social%2520understanding%252C%2520robotics%252C%2520and%2520animation.%2520With%2520high-quality%2520MOCAP%2520data%250Abecoming%2520more%2520available%2520recently%252C%2520a%2520wide%2520range%2520of%2520data-driven%2520approaches%2520have%250Abeen%2520presented.%2520However%252C%2520modelling%2520multi-person%2520interactions%2520still%2520remains%2520a%250Aless%2520explored%2520area.%2520In%2520this%2520paper%252C%2520we%2520present%2520Graph-driven%2520Interaction%250ASampling%252C%2520a%2520method%2520that%2520can%2520generate%2520realistic%2520and%2520diverse%2520multi-person%250Ainteractions%2520by%2520leveraging%2520existing%2520two-person%2520motion%2520diffusion%2520models%2520as%250Amotion%2520priors.%2520Instead%2520of%2520training%2520a%2520new%2520model%2520specific%2520to%2520multi-person%250Ainteraction%2520synthesis%252C%2520our%2520key%2520insight%2520is%2520to%2520spatially%2520and%2520temporally%2520separate%250Acomplex%2520multi-person%2520interactions%2520into%2520a%2520graph%2520structure%2520of%2520two-person%250Ainteractions%252C%2520which%2520we%2520name%2520the%2520Pairwise%2520Interaction%2520Graph.%2520We%2520thus%2520decompose%250Athe%2520generation%2520task%2520into%2520simultaneous%2520single-person%2520motion%2520generation%250Aconditioned%2520on%2520one%2520other%2527s%2520motion.%2520In%2520addition%252C%2520to%2520reduce%2520artifacts%2520such%2520as%250Ainterpenetrations%2520of%2520body%2520parts%2520in%2520generated%2520multi-person%2520interactions%252C%2520we%250Aintroduce%2520two%2520graph-dependent%2520guidance%2520terms%2520into%2520the%2520diffusion%2520sampling%250Ascheme.%2520Unlike%2520previous%2520work%252C%2520our%2520method%2520can%2520produce%2520various%2520high-quality%250Amulti-person%2520interactions%2520without%2520having%2520repetitive%2520individual%2520motions.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520consistently%2520outperforms%250Aexisting%2520methods%2520in%2520reducing%2520artifacts%2520when%2520generating%2520a%2520wide%2520range%2520of%250Atwo-person%2520and%2520multi-person%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Person%20Interaction%20Generation%20from%20Two-Person%20Motion%20Priors&entry.906535625=Wenning%20Xu%20and%20Shiyu%20Fan%20and%20Paul%20Henderson%20and%20Edmond%20S.%20L.%20Ho&entry.1292438233=%20%20Generating%20realistic%20human%20motion%20with%20high-level%20controls%20is%20a%20crucial%20task%0Afor%20social%20understanding%2C%20robotics%2C%20and%20animation.%20With%20high-quality%20MOCAP%20data%0Abecoming%20more%20available%20recently%2C%20a%20wide%20range%20of%20data-driven%20approaches%20have%0Abeen%20presented.%20However%2C%20modelling%20multi-person%20interactions%20still%20remains%20a%0Aless%20explored%20area.%20In%20this%20paper%2C%20we%20present%20Graph-driven%20Interaction%0ASampling%2C%20a%20method%20that%20can%20generate%20realistic%20and%20diverse%20multi-person%0Ainteractions%20by%20leveraging%20existing%20two-person%20motion%20diffusion%20models%20as%0Amotion%20priors.%20Instead%20of%20training%20a%20new%20model%20specific%20to%20multi-person%0Ainteraction%20synthesis%2C%20our%20key%20insight%20is%20to%20spatially%20and%20temporally%20separate%0Acomplex%20multi-person%20interactions%20into%20a%20graph%20structure%20of%20two-person%0Ainteractions%2C%20which%20we%20name%20the%20Pairwise%20Interaction%20Graph.%20We%20thus%20decompose%0Athe%20generation%20task%20into%20simultaneous%20single-person%20motion%20generation%0Aconditioned%20on%20one%20other%27s%20motion.%20In%20addition%2C%20to%20reduce%20artifacts%20such%20as%0Ainterpenetrations%20of%20body%20parts%20in%20generated%20multi-person%20interactions%2C%20we%0Aintroduce%20two%20graph-dependent%20guidance%20terms%20into%20the%20diffusion%20sampling%0Ascheme.%20Unlike%20previous%20work%2C%20our%20method%20can%20produce%20various%20high-quality%0Amulti-person%20interactions%20without%20having%20repetitive%20individual%20motions.%0AExtensive%20experiments%20demonstrate%20that%20our%20approach%20consistently%20outperforms%0Aexisting%20methods%20in%20reducing%20artifacts%20when%20generating%20a%20wide%20range%20of%0Atwo-person%20and%20multi-person%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17860v1&entry.124074799=Read"},
{"title": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual\n  Knowledge Synchronization in LLMs", "author": "Yuchen Wu and Liang Ding and Li Shen and Dacheng Tao", "abstract": "  Knowledge editing allows for efficient adaptation of large language models\n(LLMs) to new information or corrections without requiring full retraining.\nHowever, prior methods typically focus on either single-language editing or\nbasic multilingual editing, failing to achieve true cross-linguistic knowledge\nsynchronization. To address this, we present a simple and practical\nstate-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE),\ndesigned to propagate knowledge from a dominant language to other languages\neffectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition\nInstruction Tuning (XE-IT), which fine-tunes the model on a curated parallel\ndataset to modify in-scope knowledge while preserving unrelated information,\nand (ii) Target-language Preference Optimization (TL-PO), which applies\nadvanced optimization techniques to ensure consistency across languages,\nfostering the transfer of updates. Additionally, we contribute a high-quality,\ncross-lingual dataset, specifically designed to enhance knowledge transfer\nacross languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks\nshow that X-KDE significantly enhances cross-lingual performance, achieving an\naverage improvement of +8.19%, while maintaining high accuracy in monolingual\nsettings.\n", "link": "http://arxiv.org/abs/2502.14645v2", "date": "2025-05-23", "relevancy": 2.4732, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4978}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edit%20Once%2C%20Update%20Everywhere%3A%20A%20Simple%20Framework%20for%20Cross-Lingual%0A%20%20Knowledge%20Synchronization%20in%20LLMs&body=Title%3A%20Edit%20Once%2C%20Update%20Everywhere%3A%20A%20Simple%20Framework%20for%20Cross-Lingual%0A%20%20Knowledge%20Synchronization%20in%20LLMs%0AAuthor%3A%20Yuchen%20Wu%20and%20Liang%20Ding%20and%20Li%20Shen%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Knowledge%20editing%20allows%20for%20efficient%20adaptation%20of%20large%20language%20models%0A%28LLMs%29%20to%20new%20information%20or%20corrections%20without%20requiring%20full%20retraining.%0AHowever%2C%20prior%20methods%20typically%20focus%20on%20either%20single-language%20editing%20or%0Abasic%20multilingual%20editing%2C%20failing%20to%20achieve%20true%20cross-linguistic%20knowledge%0Asynchronization.%20To%20address%20this%2C%20we%20present%20a%20simple%20and%20practical%0Astate-of-the-art%20%28SOTA%29%20recipe%20Cross-Lingual%20Knowledge%20Democracy%20Edit%20%28X-KDE%29%2C%0Adesigned%20to%20propagate%20knowledge%20from%20a%20dominant%20language%20to%20other%20languages%0Aeffectively.%20Our%20X-KDE%20comprises%20two%20stages%3A%20%28i%29%20Cross-lingual%20Edition%0AInstruction%20Tuning%20%28XE-IT%29%2C%20which%20fine-tunes%20the%20model%20on%20a%20curated%20parallel%0Adataset%20to%20modify%20in-scope%20knowledge%20while%20preserving%20unrelated%20information%2C%0Aand%20%28ii%29%20Target-language%20Preference%20Optimization%20%28TL-PO%29%2C%20which%20applies%0Aadvanced%20optimization%20techniques%20to%20ensure%20consistency%20across%20languages%2C%0Afostering%20the%20transfer%20of%20updates.%20Additionally%2C%20we%20contribute%20a%20high-quality%2C%0Across-lingual%20dataset%2C%20specifically%20designed%20to%20enhance%20knowledge%20transfer%0Aacross%20languages.%20Extensive%20experiments%20on%20the%20Bi-ZsRE%20and%20MzsRE%20benchmarks%0Ashow%20that%20X-KDE%20significantly%20enhances%20cross-lingual%20performance%2C%20achieving%20an%0Aaverage%20improvement%20of%20%2B8.19%25%2C%20while%20maintaining%20high%20accuracy%20in%20monolingual%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14645v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdit%2520Once%252C%2520Update%2520Everywhere%253A%2520A%2520Simple%2520Framework%2520for%2520Cross-Lingual%250A%2520%2520Knowledge%2520Synchronization%2520in%2520LLMs%26entry.906535625%3DYuchen%2520Wu%2520and%2520Liang%2520Ding%2520and%2520Li%2520Shen%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Knowledge%2520editing%2520allows%2520for%2520efficient%2520adaptation%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520new%2520information%2520or%2520corrections%2520without%2520requiring%2520full%2520retraining.%250AHowever%252C%2520prior%2520methods%2520typically%2520focus%2520on%2520either%2520single-language%2520editing%2520or%250Abasic%2520multilingual%2520editing%252C%2520failing%2520to%2520achieve%2520true%2520cross-linguistic%2520knowledge%250Asynchronization.%2520To%2520address%2520this%252C%2520we%2520present%2520a%2520simple%2520and%2520practical%250Astate-of-the-art%2520%2528SOTA%2529%2520recipe%2520Cross-Lingual%2520Knowledge%2520Democracy%2520Edit%2520%2528X-KDE%2529%252C%250Adesigned%2520to%2520propagate%2520knowledge%2520from%2520a%2520dominant%2520language%2520to%2520other%2520languages%250Aeffectively.%2520Our%2520X-KDE%2520comprises%2520two%2520stages%253A%2520%2528i%2529%2520Cross-lingual%2520Edition%250AInstruction%2520Tuning%2520%2528XE-IT%2529%252C%2520which%2520fine-tunes%2520the%2520model%2520on%2520a%2520curated%2520parallel%250Adataset%2520to%2520modify%2520in-scope%2520knowledge%2520while%2520preserving%2520unrelated%2520information%252C%250Aand%2520%2528ii%2529%2520Target-language%2520Preference%2520Optimization%2520%2528TL-PO%2529%252C%2520which%2520applies%250Aadvanced%2520optimization%2520techniques%2520to%2520ensure%2520consistency%2520across%2520languages%252C%250Afostering%2520the%2520transfer%2520of%2520updates.%2520Additionally%252C%2520we%2520contribute%2520a%2520high-quality%252C%250Across-lingual%2520dataset%252C%2520specifically%2520designed%2520to%2520enhance%2520knowledge%2520transfer%250Aacross%2520languages.%2520Extensive%2520experiments%2520on%2520the%2520Bi-ZsRE%2520and%2520MzsRE%2520benchmarks%250Ashow%2520that%2520X-KDE%2520significantly%2520enhances%2520cross-lingual%2520performance%252C%2520achieving%2520an%250Aaverage%2520improvement%2520of%2520%252B8.19%2525%252C%2520while%2520maintaining%2520high%2520accuracy%2520in%2520monolingual%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14645v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edit%20Once%2C%20Update%20Everywhere%3A%20A%20Simple%20Framework%20for%20Cross-Lingual%0A%20%20Knowledge%20Synchronization%20in%20LLMs&entry.906535625=Yuchen%20Wu%20and%20Liang%20Ding%20and%20Li%20Shen%20and%20Dacheng%20Tao&entry.1292438233=%20%20Knowledge%20editing%20allows%20for%20efficient%20adaptation%20of%20large%20language%20models%0A%28LLMs%29%20to%20new%20information%20or%20corrections%20without%20requiring%20full%20retraining.%0AHowever%2C%20prior%20methods%20typically%20focus%20on%20either%20single-language%20editing%20or%0Abasic%20multilingual%20editing%2C%20failing%20to%20achieve%20true%20cross-linguistic%20knowledge%0Asynchronization.%20To%20address%20this%2C%20we%20present%20a%20simple%20and%20practical%0Astate-of-the-art%20%28SOTA%29%20recipe%20Cross-Lingual%20Knowledge%20Democracy%20Edit%20%28X-KDE%29%2C%0Adesigned%20to%20propagate%20knowledge%20from%20a%20dominant%20language%20to%20other%20languages%0Aeffectively.%20Our%20X-KDE%20comprises%20two%20stages%3A%20%28i%29%20Cross-lingual%20Edition%0AInstruction%20Tuning%20%28XE-IT%29%2C%20which%20fine-tunes%20the%20model%20on%20a%20curated%20parallel%0Adataset%20to%20modify%20in-scope%20knowledge%20while%20preserving%20unrelated%20information%2C%0Aand%20%28ii%29%20Target-language%20Preference%20Optimization%20%28TL-PO%29%2C%20which%20applies%0Aadvanced%20optimization%20techniques%20to%20ensure%20consistency%20across%20languages%2C%0Afostering%20the%20transfer%20of%20updates.%20Additionally%2C%20we%20contribute%20a%20high-quality%2C%0Across-lingual%20dataset%2C%20specifically%20designed%20to%20enhance%20knowledge%20transfer%0Aacross%20languages.%20Extensive%20experiments%20on%20the%20Bi-ZsRE%20and%20MzsRE%20benchmarks%0Ashow%20that%20X-KDE%20significantly%20enhances%20cross-lingual%20performance%2C%20achieving%20an%0Aaverage%20improvement%20of%20%2B8.19%25%2C%20while%20maintaining%20high%20accuracy%20in%20monolingual%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14645v2&entry.124074799=Read"},
{"title": "Improving Multi-task Learning via Seeking Task-based Flat Regions", "author": "Hoang Phan and Lam Tran and Quyen Tran and Ngoc N. Tran and Tuan Truong and Qi Lei and Nhat Ho and Dinh Phung and Trung Le", "abstract": "  Multi-Task Learning (MTL) is a widely-used and powerful learning paradigm for\ntraining deep neural networks that allows learning more than one objective by a\nsingle backbone. Compared to training tasks separately, MTL significantly\nreduces computational costs, improves data efficiency, and potentially enhances\nmodel performance by leveraging knowledge across tasks. Hence, it has been\nadopted in a variety of applications, ranging from computer vision to natural\nlanguage processing and speech recognition. Among them, there is an emerging\nline of work in MTL that focuses on manipulating the task gradient to derive an\nultimate gradient descent direction to benefit all tasks. Despite achieving\nimpressive results on many benchmarks, directly applying these approaches\nwithout using appropriate regularization techniques might lead to suboptimal\nsolutions on real-world problems. In particular, standard training that\nminimizes the empirical loss on the training data can easily suffer from\noverfitting to low-resource tasks or be spoiled by noisy-labeled ones, which\ncan cause negative transfer between tasks and overall performance drop. To\nalleviate such problems, we propose to leverage a recently introduced training\nmethod, named Sharpness-aware Minimization, which can enhance model\ngeneralization ability on single-task learning. Accordingly, we present a novel\nMTL training methodology, encouraging the model to find task-based flat minima\nfor coherently improving its generalization capability on all tasks. Finally,\nwe conduct comprehensive experiments on a variety of applications to\ndemonstrate the merit of our proposed approach to existing gradient-based MTL\nmethods, as suggested by our developed theory.\n", "link": "http://arxiv.org/abs/2211.13723v4", "date": "2025-05-23", "relevancy": 2.4678, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.526}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4775}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Multi-task%20Learning%20via%20Seeking%20Task-based%20Flat%20Regions&body=Title%3A%20Improving%20Multi-task%20Learning%20via%20Seeking%20Task-based%20Flat%20Regions%0AAuthor%3A%20Hoang%20Phan%20and%20Lam%20Tran%20and%20Quyen%20Tran%20and%20Ngoc%20N.%20Tran%20and%20Tuan%20Truong%20and%20Qi%20Lei%20and%20Nhat%20Ho%20and%20Dinh%20Phung%20and%20Trung%20Le%0AAbstract%3A%20%20%20Multi-Task%20Learning%20%28MTL%29%20is%20a%20widely-used%20and%20powerful%20learning%20paradigm%20for%0Atraining%20deep%20neural%20networks%20that%20allows%20learning%20more%20than%20one%20objective%20by%20a%0Asingle%20backbone.%20Compared%20to%20training%20tasks%20separately%2C%20MTL%20significantly%0Areduces%20computational%20costs%2C%20improves%20data%20efficiency%2C%20and%20potentially%20enhances%0Amodel%20performance%20by%20leveraging%20knowledge%20across%20tasks.%20Hence%2C%20it%20has%20been%0Aadopted%20in%20a%20variety%20of%20applications%2C%20ranging%20from%20computer%20vision%20to%20natural%0Alanguage%20processing%20and%20speech%20recognition.%20Among%20them%2C%20there%20is%20an%20emerging%0Aline%20of%20work%20in%20MTL%20that%20focuses%20on%20manipulating%20the%20task%20gradient%20to%20derive%20an%0Aultimate%20gradient%20descent%20direction%20to%20benefit%20all%20tasks.%20Despite%20achieving%0Aimpressive%20results%20on%20many%20benchmarks%2C%20directly%20applying%20these%20approaches%0Awithout%20using%20appropriate%20regularization%20techniques%20might%20lead%20to%20suboptimal%0Asolutions%20on%20real-world%20problems.%20In%20particular%2C%20standard%20training%20that%0Aminimizes%20the%20empirical%20loss%20on%20the%20training%20data%20can%20easily%20suffer%20from%0Aoverfitting%20to%20low-resource%20tasks%20or%20be%20spoiled%20by%20noisy-labeled%20ones%2C%20which%0Acan%20cause%20negative%20transfer%20between%20tasks%20and%20overall%20performance%20drop.%20To%0Aalleviate%20such%20problems%2C%20we%20propose%20to%20leverage%20a%20recently%20introduced%20training%0Amethod%2C%20named%20Sharpness-aware%20Minimization%2C%20which%20can%20enhance%20model%0Ageneralization%20ability%20on%20single-task%20learning.%20Accordingly%2C%20we%20present%20a%20novel%0AMTL%20training%20methodology%2C%20encouraging%20the%20model%20to%20find%20task-based%20flat%20minima%0Afor%20coherently%20improving%20its%20generalization%20capability%20on%20all%20tasks.%20Finally%2C%0Awe%20conduct%20comprehensive%20experiments%20on%20a%20variety%20of%20applications%20to%0Ademonstrate%20the%20merit%20of%20our%20proposed%20approach%20to%20existing%20gradient-based%20MTL%0Amethods%2C%20as%20suggested%20by%20our%20developed%20theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.13723v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Multi-task%2520Learning%2520via%2520Seeking%2520Task-based%2520Flat%2520Regions%26entry.906535625%3DHoang%2520Phan%2520and%2520Lam%2520Tran%2520and%2520Quyen%2520Tran%2520and%2520Ngoc%2520N.%2520Tran%2520and%2520Tuan%2520Truong%2520and%2520Qi%2520Lei%2520and%2520Nhat%2520Ho%2520and%2520Dinh%2520Phung%2520and%2520Trung%2520Le%26entry.1292438233%3D%2520%2520Multi-Task%2520Learning%2520%2528MTL%2529%2520is%2520a%2520widely-used%2520and%2520powerful%2520learning%2520paradigm%2520for%250Atraining%2520deep%2520neural%2520networks%2520that%2520allows%2520learning%2520more%2520than%2520one%2520objective%2520by%2520a%250Asingle%2520backbone.%2520Compared%2520to%2520training%2520tasks%2520separately%252C%2520MTL%2520significantly%250Areduces%2520computational%2520costs%252C%2520improves%2520data%2520efficiency%252C%2520and%2520potentially%2520enhances%250Amodel%2520performance%2520by%2520leveraging%2520knowledge%2520across%2520tasks.%2520Hence%252C%2520it%2520has%2520been%250Aadopted%2520in%2520a%2520variety%2520of%2520applications%252C%2520ranging%2520from%2520computer%2520vision%2520to%2520natural%250Alanguage%2520processing%2520and%2520speech%2520recognition.%2520Among%2520them%252C%2520there%2520is%2520an%2520emerging%250Aline%2520of%2520work%2520in%2520MTL%2520that%2520focuses%2520on%2520manipulating%2520the%2520task%2520gradient%2520to%2520derive%2520an%250Aultimate%2520gradient%2520descent%2520direction%2520to%2520benefit%2520all%2520tasks.%2520Despite%2520achieving%250Aimpressive%2520results%2520on%2520many%2520benchmarks%252C%2520directly%2520applying%2520these%2520approaches%250Awithout%2520using%2520appropriate%2520regularization%2520techniques%2520might%2520lead%2520to%2520suboptimal%250Asolutions%2520on%2520real-world%2520problems.%2520In%2520particular%252C%2520standard%2520training%2520that%250Aminimizes%2520the%2520empirical%2520loss%2520on%2520the%2520training%2520data%2520can%2520easily%2520suffer%2520from%250Aoverfitting%2520to%2520low-resource%2520tasks%2520or%2520be%2520spoiled%2520by%2520noisy-labeled%2520ones%252C%2520which%250Acan%2520cause%2520negative%2520transfer%2520between%2520tasks%2520and%2520overall%2520performance%2520drop.%2520To%250Aalleviate%2520such%2520problems%252C%2520we%2520propose%2520to%2520leverage%2520a%2520recently%2520introduced%2520training%250Amethod%252C%2520named%2520Sharpness-aware%2520Minimization%252C%2520which%2520can%2520enhance%2520model%250Ageneralization%2520ability%2520on%2520single-task%2520learning.%2520Accordingly%252C%2520we%2520present%2520a%2520novel%250AMTL%2520training%2520methodology%252C%2520encouraging%2520the%2520model%2520to%2520find%2520task-based%2520flat%2520minima%250Afor%2520coherently%2520improving%2520its%2520generalization%2520capability%2520on%2520all%2520tasks.%2520Finally%252C%250Awe%2520conduct%2520comprehensive%2520experiments%2520on%2520a%2520variety%2520of%2520applications%2520to%250Ademonstrate%2520the%2520merit%2520of%2520our%2520proposed%2520approach%2520to%2520existing%2520gradient-based%2520MTL%250Amethods%252C%2520as%2520suggested%2520by%2520our%2520developed%2520theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.13723v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Multi-task%20Learning%20via%20Seeking%20Task-based%20Flat%20Regions&entry.906535625=Hoang%20Phan%20and%20Lam%20Tran%20and%20Quyen%20Tran%20and%20Ngoc%20N.%20Tran%20and%20Tuan%20Truong%20and%20Qi%20Lei%20and%20Nhat%20Ho%20and%20Dinh%20Phung%20and%20Trung%20Le&entry.1292438233=%20%20Multi-Task%20Learning%20%28MTL%29%20is%20a%20widely-used%20and%20powerful%20learning%20paradigm%20for%0Atraining%20deep%20neural%20networks%20that%20allows%20learning%20more%20than%20one%20objective%20by%20a%0Asingle%20backbone.%20Compared%20to%20training%20tasks%20separately%2C%20MTL%20significantly%0Areduces%20computational%20costs%2C%20improves%20data%20efficiency%2C%20and%20potentially%20enhances%0Amodel%20performance%20by%20leveraging%20knowledge%20across%20tasks.%20Hence%2C%20it%20has%20been%0Aadopted%20in%20a%20variety%20of%20applications%2C%20ranging%20from%20computer%20vision%20to%20natural%0Alanguage%20processing%20and%20speech%20recognition.%20Among%20them%2C%20there%20is%20an%20emerging%0Aline%20of%20work%20in%20MTL%20that%20focuses%20on%20manipulating%20the%20task%20gradient%20to%20derive%20an%0Aultimate%20gradient%20descent%20direction%20to%20benefit%20all%20tasks.%20Despite%20achieving%0Aimpressive%20results%20on%20many%20benchmarks%2C%20directly%20applying%20these%20approaches%0Awithout%20using%20appropriate%20regularization%20techniques%20might%20lead%20to%20suboptimal%0Asolutions%20on%20real-world%20problems.%20In%20particular%2C%20standard%20training%20that%0Aminimizes%20the%20empirical%20loss%20on%20the%20training%20data%20can%20easily%20suffer%20from%0Aoverfitting%20to%20low-resource%20tasks%20or%20be%20spoiled%20by%20noisy-labeled%20ones%2C%20which%0Acan%20cause%20negative%20transfer%20between%20tasks%20and%20overall%20performance%20drop.%20To%0Aalleviate%20such%20problems%2C%20we%20propose%20to%20leverage%20a%20recently%20introduced%20training%0Amethod%2C%20named%20Sharpness-aware%20Minimization%2C%20which%20can%20enhance%20model%0Ageneralization%20ability%20on%20single-task%20learning.%20Accordingly%2C%20we%20present%20a%20novel%0AMTL%20training%20methodology%2C%20encouraging%20the%20model%20to%20find%20task-based%20flat%20minima%0Afor%20coherently%20improving%20its%20generalization%20capability%20on%20all%20tasks.%20Finally%2C%0Awe%20conduct%20comprehensive%20experiments%20on%20a%20variety%20of%20applications%20to%0Ademonstrate%20the%20merit%20of%20our%20proposed%20approach%20to%20existing%20gradient-based%20MTL%0Amethods%2C%20as%20suggested%20by%20our%20developed%20theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.13723v4&entry.124074799=Read"},
{"title": "Training with Pseudo-Code for Instruction Following", "author": "Prince Kumar and Rudra Murthy and Riyaz Bhat and Danish Contractor", "abstract": "  Despite the rapid progress in the capabilities of Large Language Models\n(LLMs), they continue to have difficulty following relatively simple,\nunambiguous instructions, especially when compositions are involved. In this\npaper, we take inspiration from recent work that suggests that models may\nfollow instructions better when they are expressed in pseudo-code. However,\nwriting pseudo-code programs can be tedious and using few-shot demonstrations\nto craft code representations for use in inference can be unnatural for\nnon-expert users of LLMs. To overcome these limitations, we propose fine-tuning\nLLMs with instruction-tuning data that additionally includes instructions\nre-expressed in pseudo-code along with the final response. We evaluate models\ntrained using our method on $11$ publicly available benchmarks comprising of\ntasks related to instruction-following, mathematics, and common-sense\nreasoning. We conduct rigorous experiments with $5$ different models and find\nthat not only do models follow instructions better when trained with\npseudo-code, they also retain their capabilities on the other tasks related to\nmathematical and common sense reasoning. Specifically, we observe a relative\ngain of $3$--$19$% on instruction-following benchmark, and an average gain of\nupto 14% across all tasks.\n", "link": "http://arxiv.org/abs/2505.18011v1", "date": "2025-05-23", "relevancy": 2.4652, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4965}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4965}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20with%20Pseudo-Code%20for%20Instruction%20Following&body=Title%3A%20Training%20with%20Pseudo-Code%20for%20Instruction%20Following%0AAuthor%3A%20Prince%20Kumar%20and%20Rudra%20Murthy%20and%20Riyaz%20Bhat%20and%20Danish%20Contractor%0AAbstract%3A%20%20%20Despite%20the%20rapid%20progress%20in%20the%20capabilities%20of%20Large%20Language%20Models%0A%28LLMs%29%2C%20they%20continue%20to%20have%20difficulty%20following%20relatively%20simple%2C%0Aunambiguous%20instructions%2C%20especially%20when%20compositions%20are%20involved.%20In%20this%0Apaper%2C%20we%20take%20inspiration%20from%20recent%20work%20that%20suggests%20that%20models%20may%0Afollow%20instructions%20better%20when%20they%20are%20expressed%20in%20pseudo-code.%20However%2C%0Awriting%20pseudo-code%20programs%20can%20be%20tedious%20and%20using%20few-shot%20demonstrations%0Ato%20craft%20code%20representations%20for%20use%20in%20inference%20can%20be%20unnatural%20for%0Anon-expert%20users%20of%20LLMs.%20To%20overcome%20these%20limitations%2C%20we%20propose%20fine-tuning%0ALLMs%20with%20instruction-tuning%20data%20that%20additionally%20includes%20instructions%0Are-expressed%20in%20pseudo-code%20along%20with%20the%20final%20response.%20We%20evaluate%20models%0Atrained%20using%20our%20method%20on%20%2411%24%20publicly%20available%20benchmarks%20comprising%20of%0Atasks%20related%20to%20instruction-following%2C%20mathematics%2C%20and%20common-sense%0Areasoning.%20We%20conduct%20rigorous%20experiments%20with%20%245%24%20different%20models%20and%20find%0Athat%20not%20only%20do%20models%20follow%20instructions%20better%20when%20trained%20with%0Apseudo-code%2C%20they%20also%20retain%20their%20capabilities%20on%20the%20other%20tasks%20related%20to%0Amathematical%20and%20common%20sense%20reasoning.%20Specifically%2C%20we%20observe%20a%20relative%0Again%20of%20%243%24--%2419%24%25%20on%20instruction-following%20benchmark%2C%20and%20an%20average%20gain%20of%0Aupto%2014%25%20across%20all%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520with%2520Pseudo-Code%2520for%2520Instruction%2520Following%26entry.906535625%3DPrince%2520Kumar%2520and%2520Rudra%2520Murthy%2520and%2520Riyaz%2520Bhat%2520and%2520Danish%2520Contractor%26entry.1292438233%3D%2520%2520Despite%2520the%2520rapid%2520progress%2520in%2520the%2520capabilities%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529%252C%2520they%2520continue%2520to%2520have%2520difficulty%2520following%2520relatively%2520simple%252C%250Aunambiguous%2520instructions%252C%2520especially%2520when%2520compositions%2520are%2520involved.%2520In%2520this%250Apaper%252C%2520we%2520take%2520inspiration%2520from%2520recent%2520work%2520that%2520suggests%2520that%2520models%2520may%250Afollow%2520instructions%2520better%2520when%2520they%2520are%2520expressed%2520in%2520pseudo-code.%2520However%252C%250Awriting%2520pseudo-code%2520programs%2520can%2520be%2520tedious%2520and%2520using%2520few-shot%2520demonstrations%250Ato%2520craft%2520code%2520representations%2520for%2520use%2520in%2520inference%2520can%2520be%2520unnatural%2520for%250Anon-expert%2520users%2520of%2520LLMs.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520fine-tuning%250ALLMs%2520with%2520instruction-tuning%2520data%2520that%2520additionally%2520includes%2520instructions%250Are-expressed%2520in%2520pseudo-code%2520along%2520with%2520the%2520final%2520response.%2520We%2520evaluate%2520models%250Atrained%2520using%2520our%2520method%2520on%2520%252411%2524%2520publicly%2520available%2520benchmarks%2520comprising%2520of%250Atasks%2520related%2520to%2520instruction-following%252C%2520mathematics%252C%2520and%2520common-sense%250Areasoning.%2520We%2520conduct%2520rigorous%2520experiments%2520with%2520%25245%2524%2520different%2520models%2520and%2520find%250Athat%2520not%2520only%2520do%2520models%2520follow%2520instructions%2520better%2520when%2520trained%2520with%250Apseudo-code%252C%2520they%2520also%2520retain%2520their%2520capabilities%2520on%2520the%2520other%2520tasks%2520related%2520to%250Amathematical%2520and%2520common%2520sense%2520reasoning.%2520Specifically%252C%2520we%2520observe%2520a%2520relative%250Again%2520of%2520%25243%2524--%252419%2524%2525%2520on%2520instruction-following%2520benchmark%252C%2520and%2520an%2520average%2520gain%2520of%250Aupto%252014%2525%2520across%2520all%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20with%20Pseudo-Code%20for%20Instruction%20Following&entry.906535625=Prince%20Kumar%20and%20Rudra%20Murthy%20and%20Riyaz%20Bhat%20and%20Danish%20Contractor&entry.1292438233=%20%20Despite%20the%20rapid%20progress%20in%20the%20capabilities%20of%20Large%20Language%20Models%0A%28LLMs%29%2C%20they%20continue%20to%20have%20difficulty%20following%20relatively%20simple%2C%0Aunambiguous%20instructions%2C%20especially%20when%20compositions%20are%20involved.%20In%20this%0Apaper%2C%20we%20take%20inspiration%20from%20recent%20work%20that%20suggests%20that%20models%20may%0Afollow%20instructions%20better%20when%20they%20are%20expressed%20in%20pseudo-code.%20However%2C%0Awriting%20pseudo-code%20programs%20can%20be%20tedious%20and%20using%20few-shot%20demonstrations%0Ato%20craft%20code%20representations%20for%20use%20in%20inference%20can%20be%20unnatural%20for%0Anon-expert%20users%20of%20LLMs.%20To%20overcome%20these%20limitations%2C%20we%20propose%20fine-tuning%0ALLMs%20with%20instruction-tuning%20data%20that%20additionally%20includes%20instructions%0Are-expressed%20in%20pseudo-code%20along%20with%20the%20final%20response.%20We%20evaluate%20models%0Atrained%20using%20our%20method%20on%20%2411%24%20publicly%20available%20benchmarks%20comprising%20of%0Atasks%20related%20to%20instruction-following%2C%20mathematics%2C%20and%20common-sense%0Areasoning.%20We%20conduct%20rigorous%20experiments%20with%20%245%24%20different%20models%20and%20find%0Athat%20not%20only%20do%20models%20follow%20instructions%20better%20when%20trained%20with%0Apseudo-code%2C%20they%20also%20retain%20their%20capabilities%20on%20the%20other%20tasks%20related%20to%0Amathematical%20and%20common%20sense%20reasoning.%20Specifically%2C%20we%20observe%20a%20relative%0Again%20of%20%243%24--%2419%24%25%20on%20instruction-following%20benchmark%2C%20and%20an%20average%20gain%20of%0Aupto%2014%25%20across%20all%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18011v1&entry.124074799=Read"},
{"title": "SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment\n  for LLMs' Mathematical Problem Solving", "author": "Yujie Hou and Ting Zhang and Mei Wang and Xuetao Ma and Hua Huang", "abstract": "  Large Language Models have achieved remarkable results on a variety of\nmathematical benchmarks. However, concerns remain as to whether these successes\nreflect genuine mathematical reasoning or superficial pattern recognition.\nCommon evaluation metrics, such as final answer accuracy, fail to disentangle\nthe underlying competencies involved, offering limited diagnostic value. To\naddress these limitations, we introduce SMART: a Self-Generating and\nSelf-Validating Multi-Dimensional Assessment Framework. SMART decomposes\nmathematical problem solving into four distinct dimensions: understanding,\nreasoning, arithmetic, and reflection \\& refinement. Each dimension is\nevaluated independently through tailored tasks, enabling interpretable and\nfine-grained analysis of LLM behavior. Crucially, SMART integrates an automated\nself-generating and self-validating mechanism to produce and verify benchmark\ndata, ensuring both scalability and reliability. We apply SMART to 21\nstate-of-the-art open- and closed-source LLMs, uncovering significant\ndiscrepancies in their abilities across different dimensions. Our findings\ndemonstrate the inadequacy of final answer accuracy as a sole metric and\nmotivate a new holistic metric to better capture true problem-solving\ncapabilities. Code and benchmarks will be released upon acceptance.\n", "link": "http://arxiv.org/abs/2505.16646v2", "date": "2025-05-23", "relevancy": 2.4646, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMART%3A%20Self-Generating%20and%20Self-Validating%20Multi-Dimensional%20Assessment%0A%20%20for%20LLMs%27%20Mathematical%20Problem%20Solving&body=Title%3A%20SMART%3A%20Self-Generating%20and%20Self-Validating%20Multi-Dimensional%20Assessment%0A%20%20for%20LLMs%27%20Mathematical%20Problem%20Solving%0AAuthor%3A%20Yujie%20Hou%20and%20Ting%20Zhang%20and%20Mei%20Wang%20and%20Xuetao%20Ma%20and%20Hua%20Huang%0AAbstract%3A%20%20%20Large%20Language%20Models%20have%20achieved%20remarkable%20results%20on%20a%20variety%20of%0Amathematical%20benchmarks.%20However%2C%20concerns%20remain%20as%20to%20whether%20these%20successes%0Areflect%20genuine%20mathematical%20reasoning%20or%20superficial%20pattern%20recognition.%0ACommon%20evaluation%20metrics%2C%20such%20as%20final%20answer%20accuracy%2C%20fail%20to%20disentangle%0Athe%20underlying%20competencies%20involved%2C%20offering%20limited%20diagnostic%20value.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20SMART%3A%20a%20Self-Generating%20and%0ASelf-Validating%20Multi-Dimensional%20Assessment%20Framework.%20SMART%20decomposes%0Amathematical%20problem%20solving%20into%20four%20distinct%20dimensions%3A%20understanding%2C%0Areasoning%2C%20arithmetic%2C%20and%20reflection%20%5C%26%20refinement.%20Each%20dimension%20is%0Aevaluated%20independently%20through%20tailored%20tasks%2C%20enabling%20interpretable%20and%0Afine-grained%20analysis%20of%20LLM%20behavior.%20Crucially%2C%20SMART%20integrates%20an%20automated%0Aself-generating%20and%20self-validating%20mechanism%20to%20produce%20and%20verify%20benchmark%0Adata%2C%20ensuring%20both%20scalability%20and%20reliability.%20We%20apply%20SMART%20to%2021%0Astate-of-the-art%20open-%20and%20closed-source%20LLMs%2C%20uncovering%20significant%0Adiscrepancies%20in%20their%20abilities%20across%20different%20dimensions.%20Our%20findings%0Ademonstrate%20the%20inadequacy%20of%20final%20answer%20accuracy%20as%20a%20sole%20metric%20and%0Amotivate%20a%20new%20holistic%20metric%20to%20better%20capture%20true%20problem-solving%0Acapabilities.%20Code%20and%20benchmarks%20will%20be%20released%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16646v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMART%253A%2520Self-Generating%2520and%2520Self-Validating%2520Multi-Dimensional%2520Assessment%250A%2520%2520for%2520LLMs%2527%2520Mathematical%2520Problem%2520Solving%26entry.906535625%3DYujie%2520Hou%2520and%2520Ting%2520Zhang%2520and%2520Mei%2520Wang%2520and%2520Xuetao%2520Ma%2520and%2520Hua%2520Huang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520have%2520achieved%2520remarkable%2520results%2520on%2520a%2520variety%2520of%250Amathematical%2520benchmarks.%2520However%252C%2520concerns%2520remain%2520as%2520to%2520whether%2520these%2520successes%250Areflect%2520genuine%2520mathematical%2520reasoning%2520or%2520superficial%2520pattern%2520recognition.%250ACommon%2520evaluation%2520metrics%252C%2520such%2520as%2520final%2520answer%2520accuracy%252C%2520fail%2520to%2520disentangle%250Athe%2520underlying%2520competencies%2520involved%252C%2520offering%2520limited%2520diagnostic%2520value.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520introduce%2520SMART%253A%2520a%2520Self-Generating%2520and%250ASelf-Validating%2520Multi-Dimensional%2520Assessment%2520Framework.%2520SMART%2520decomposes%250Amathematical%2520problem%2520solving%2520into%2520four%2520distinct%2520dimensions%253A%2520understanding%252C%250Areasoning%252C%2520arithmetic%252C%2520and%2520reflection%2520%255C%2526%2520refinement.%2520Each%2520dimension%2520is%250Aevaluated%2520independently%2520through%2520tailored%2520tasks%252C%2520enabling%2520interpretable%2520and%250Afine-grained%2520analysis%2520of%2520LLM%2520behavior.%2520Crucially%252C%2520SMART%2520integrates%2520an%2520automated%250Aself-generating%2520and%2520self-validating%2520mechanism%2520to%2520produce%2520and%2520verify%2520benchmark%250Adata%252C%2520ensuring%2520both%2520scalability%2520and%2520reliability.%2520We%2520apply%2520SMART%2520to%252021%250Astate-of-the-art%2520open-%2520and%2520closed-source%2520LLMs%252C%2520uncovering%2520significant%250Adiscrepancies%2520in%2520their%2520abilities%2520across%2520different%2520dimensions.%2520Our%2520findings%250Ademonstrate%2520the%2520inadequacy%2520of%2520final%2520answer%2520accuracy%2520as%2520a%2520sole%2520metric%2520and%250Amotivate%2520a%2520new%2520holistic%2520metric%2520to%2520better%2520capture%2520true%2520problem-solving%250Acapabilities.%2520Code%2520and%2520benchmarks%2520will%2520be%2520released%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16646v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMART%3A%20Self-Generating%20and%20Self-Validating%20Multi-Dimensional%20Assessment%0A%20%20for%20LLMs%27%20Mathematical%20Problem%20Solving&entry.906535625=Yujie%20Hou%20and%20Ting%20Zhang%20and%20Mei%20Wang%20and%20Xuetao%20Ma%20and%20Hua%20Huang&entry.1292438233=%20%20Large%20Language%20Models%20have%20achieved%20remarkable%20results%20on%20a%20variety%20of%0Amathematical%20benchmarks.%20However%2C%20concerns%20remain%20as%20to%20whether%20these%20successes%0Areflect%20genuine%20mathematical%20reasoning%20or%20superficial%20pattern%20recognition.%0ACommon%20evaluation%20metrics%2C%20such%20as%20final%20answer%20accuracy%2C%20fail%20to%20disentangle%0Athe%20underlying%20competencies%20involved%2C%20offering%20limited%20diagnostic%20value.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20SMART%3A%20a%20Self-Generating%20and%0ASelf-Validating%20Multi-Dimensional%20Assessment%20Framework.%20SMART%20decomposes%0Amathematical%20problem%20solving%20into%20four%20distinct%20dimensions%3A%20understanding%2C%0Areasoning%2C%20arithmetic%2C%20and%20reflection%20%5C%26%20refinement.%20Each%20dimension%20is%0Aevaluated%20independently%20through%20tailored%20tasks%2C%20enabling%20interpretable%20and%0Afine-grained%20analysis%20of%20LLM%20behavior.%20Crucially%2C%20SMART%20integrates%20an%20automated%0Aself-generating%20and%20self-validating%20mechanism%20to%20produce%20and%20verify%20benchmark%0Adata%2C%20ensuring%20both%20scalability%20and%20reliability.%20We%20apply%20SMART%20to%2021%0Astate-of-the-art%20open-%20and%20closed-source%20LLMs%2C%20uncovering%20significant%0Adiscrepancies%20in%20their%20abilities%20across%20different%20dimensions.%20Our%20findings%0Ademonstrate%20the%20inadequacy%20of%20final%20answer%20accuracy%20as%20a%20sole%20metric%20and%0Amotivate%20a%20new%20holistic%20metric%20to%20better%20capture%20true%20problem-solving%0Acapabilities.%20Code%20and%20benchmarks%20will%20be%20released%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16646v2&entry.124074799=Read"},
{"title": "Structured Thinking Matters: Improving LLMs Generalization in Causal\n  Inference Tasks", "author": "Wentao Sun and Joao Paulo Nogueira and Alonso Silva", "abstract": "  Despite remarkable advances in the field, LLMs remain unreliable in\ndistinguishing causation from correlation. Recent results from the Corr2Cause\ndataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score:\n29.08) -- only marginally outperform random baselines (Random Uniform, F1\nscore: 20.38), indicating limited capacity of generalization. To tackle this\nlimitation, we propose a novel structured approach: rather than directly\nanswering causal queries, we provide the model with the capability to structure\nits thinking by guiding the model to build a structured knowledge graph,\nsystematically encoding the provided correlational premises, to answer the\ncausal queries. This intermediate representation significantly enhances the\nmodel's causal capabilities. Experiments on the test subset of the Corr2Cause\ndataset benchmark with Qwen3-32B model (reasoning model) show substantial gains\nover standard direct prompting methods, improving F1 scores from 32.71 to 48.26\n(over 47.5% relative increase), along with notable improvements in precision\nand recall. These results underscore the effectiveness of providing the model\nwith the capability to structure its thinking and highlight its promising\npotential for broader generalization across diverse causal inference tasks.\n", "link": "http://arxiv.org/abs/2505.18034v1", "date": "2025-05-23", "relevancy": 2.4606, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.496}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Thinking%20Matters%3A%20Improving%20LLMs%20Generalization%20in%20Causal%0A%20%20Inference%20Tasks&body=Title%3A%20Structured%20Thinking%20Matters%3A%20Improving%20LLMs%20Generalization%20in%20Causal%0A%20%20Inference%20Tasks%0AAuthor%3A%20Wentao%20Sun%20and%20Joao%20Paulo%20Nogueira%20and%20Alonso%20Silva%0AAbstract%3A%20%20%20Despite%20remarkable%20advances%20in%20the%20field%2C%20LLMs%20remain%20unreliable%20in%0Adistinguishing%20causation%20from%20correlation.%20Recent%20results%20from%20the%20Corr2Cause%0Adataset%20benchmark%20reveal%20that%20state-of-the-art%20LLMs%20--%20such%20as%20GPT-4%20%28F1%20score%3A%0A29.08%29%20--%20only%20marginally%20outperform%20random%20baselines%20%28Random%20Uniform%2C%20F1%0Ascore%3A%2020.38%29%2C%20indicating%20limited%20capacity%20of%20generalization.%20To%20tackle%20this%0Alimitation%2C%20we%20propose%20a%20novel%20structured%20approach%3A%20rather%20than%20directly%0Aanswering%20causal%20queries%2C%20we%20provide%20the%20model%20with%20the%20capability%20to%20structure%0Aits%20thinking%20by%20guiding%20the%20model%20to%20build%20a%20structured%20knowledge%20graph%2C%0Asystematically%20encoding%20the%20provided%20correlational%20premises%2C%20to%20answer%20the%0Acausal%20queries.%20This%20intermediate%20representation%20significantly%20enhances%20the%0Amodel%27s%20causal%20capabilities.%20Experiments%20on%20the%20test%20subset%20of%20the%20Corr2Cause%0Adataset%20benchmark%20with%20Qwen3-32B%20model%20%28reasoning%20model%29%20show%20substantial%20gains%0Aover%20standard%20direct%20prompting%20methods%2C%20improving%20F1%20scores%20from%2032.71%20to%2048.26%0A%28over%2047.5%25%20relative%20increase%29%2C%20along%20with%20notable%20improvements%20in%20precision%0Aand%20recall.%20These%20results%20underscore%20the%20effectiveness%20of%20providing%20the%20model%0Awith%20the%20capability%20to%20structure%20its%20thinking%20and%20highlight%20its%20promising%0Apotential%20for%20broader%20generalization%20across%20diverse%20causal%20inference%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Thinking%2520Matters%253A%2520Improving%2520LLMs%2520Generalization%2520in%2520Causal%250A%2520%2520Inference%2520Tasks%26entry.906535625%3DWentao%2520Sun%2520and%2520Joao%2520Paulo%2520Nogueira%2520and%2520Alonso%2520Silva%26entry.1292438233%3D%2520%2520Despite%2520remarkable%2520advances%2520in%2520the%2520field%252C%2520LLMs%2520remain%2520unreliable%2520in%250Adistinguishing%2520causation%2520from%2520correlation.%2520Recent%2520results%2520from%2520the%2520Corr2Cause%250Adataset%2520benchmark%2520reveal%2520that%2520state-of-the-art%2520LLMs%2520--%2520such%2520as%2520GPT-4%2520%2528F1%2520score%253A%250A29.08%2529%2520--%2520only%2520marginally%2520outperform%2520random%2520baselines%2520%2528Random%2520Uniform%252C%2520F1%250Ascore%253A%252020.38%2529%252C%2520indicating%2520limited%2520capacity%2520of%2520generalization.%2520To%2520tackle%2520this%250Alimitation%252C%2520we%2520propose%2520a%2520novel%2520structured%2520approach%253A%2520rather%2520than%2520directly%250Aanswering%2520causal%2520queries%252C%2520we%2520provide%2520the%2520model%2520with%2520the%2520capability%2520to%2520structure%250Aits%2520thinking%2520by%2520guiding%2520the%2520model%2520to%2520build%2520a%2520structured%2520knowledge%2520graph%252C%250Asystematically%2520encoding%2520the%2520provided%2520correlational%2520premises%252C%2520to%2520answer%2520the%250Acausal%2520queries.%2520This%2520intermediate%2520representation%2520significantly%2520enhances%2520the%250Amodel%2527s%2520causal%2520capabilities.%2520Experiments%2520on%2520the%2520test%2520subset%2520of%2520the%2520Corr2Cause%250Adataset%2520benchmark%2520with%2520Qwen3-32B%2520model%2520%2528reasoning%2520model%2529%2520show%2520substantial%2520gains%250Aover%2520standard%2520direct%2520prompting%2520methods%252C%2520improving%2520F1%2520scores%2520from%252032.71%2520to%252048.26%250A%2528over%252047.5%2525%2520relative%2520increase%2529%252C%2520along%2520with%2520notable%2520improvements%2520in%2520precision%250Aand%2520recall.%2520These%2520results%2520underscore%2520the%2520effectiveness%2520of%2520providing%2520the%2520model%250Awith%2520the%2520capability%2520to%2520structure%2520its%2520thinking%2520and%2520highlight%2520its%2520promising%250Apotential%2520for%2520broader%2520generalization%2520across%2520diverse%2520causal%2520inference%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Thinking%20Matters%3A%20Improving%20LLMs%20Generalization%20in%20Causal%0A%20%20Inference%20Tasks&entry.906535625=Wentao%20Sun%20and%20Joao%20Paulo%20Nogueira%20and%20Alonso%20Silva&entry.1292438233=%20%20Despite%20remarkable%20advances%20in%20the%20field%2C%20LLMs%20remain%20unreliable%20in%0Adistinguishing%20causation%20from%20correlation.%20Recent%20results%20from%20the%20Corr2Cause%0Adataset%20benchmark%20reveal%20that%20state-of-the-art%20LLMs%20--%20such%20as%20GPT-4%20%28F1%20score%3A%0A29.08%29%20--%20only%20marginally%20outperform%20random%20baselines%20%28Random%20Uniform%2C%20F1%0Ascore%3A%2020.38%29%2C%20indicating%20limited%20capacity%20of%20generalization.%20To%20tackle%20this%0Alimitation%2C%20we%20propose%20a%20novel%20structured%20approach%3A%20rather%20than%20directly%0Aanswering%20causal%20queries%2C%20we%20provide%20the%20model%20with%20the%20capability%20to%20structure%0Aits%20thinking%20by%20guiding%20the%20model%20to%20build%20a%20structured%20knowledge%20graph%2C%0Asystematically%20encoding%20the%20provided%20correlational%20premises%2C%20to%20answer%20the%0Acausal%20queries.%20This%20intermediate%20representation%20significantly%20enhances%20the%0Amodel%27s%20causal%20capabilities.%20Experiments%20on%20the%20test%20subset%20of%20the%20Corr2Cause%0Adataset%20benchmark%20with%20Qwen3-32B%20model%20%28reasoning%20model%29%20show%20substantial%20gains%0Aover%20standard%20direct%20prompting%20methods%2C%20improving%20F1%20scores%20from%2032.71%20to%2048.26%0A%28over%2047.5%25%20relative%20increase%29%2C%20along%20with%20notable%20improvements%20in%20precision%0Aand%20recall.%20These%20results%20underscore%20the%20effectiveness%20of%20providing%20the%20model%0Awith%20the%20capability%20to%20structure%20its%20thinking%20and%20highlight%20its%20promising%0Apotential%20for%20broader%20generalization%20across%20diverse%20causal%20inference%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18034v1&entry.124074799=Read"},
{"title": "RCR: Robust Crowd Reconstruction with Upright Space from a Single\n  Large-scene Image", "author": "Jing Huang and Hao Wen and Tianyi Zhou and Haozhe Lin and Yu-kun Lai and Kun Li", "abstract": "  This paper focuses on spatially consistent hundreds of human pose and shape\nreconstruction from a single large-scene image with various human scales under\narbitrary camera FoVs (Fields of View). Due to the small and highly varying 2D\nhuman scales, depth ambiguity, and perspective distortion, no existing methods\ncan achieve globally consistent reconstruction with correct reprojection. To\naddress these challenges, we first propose a new concept, Human-scene Virtual\nInteraction Point (HVIP), to convert the complex 3D human localization into\n2D-pixel localization. We then extend it to RCR (Robust Crowd Reconstruction),\nwhich achieves globally consistent reconstruction and stable generalization on\ndifferent camera FoVs without test-time optimization. To perceive humans in\nvarying pixel sizes, we propose an Iterative Ground-aware Cropping to\nautomatically crop the image and then merge the results. To eliminate the\ninfluence of the camera and cropping process during the reconstruction, we\nintroduce a canonical Upright 3D Space and the corresponding Upright 2D Space.\nTo link the canonical space and the camera space, we propose the Upright\nNormalization, which transforms the local crop input into the Upright 2D Space,\nand transforms the output from the Upright 3D Space into the unified camera\nspace. Besides, we contribute two benchmark datasets, LargeCrowd and SynCrowd,\nfor evaluating crowd reconstruction in large scenes. Experimental results\ndemonstrate the effectiveness of the proposed method. The source code and data\nwill be publicly available for research purposes.\n", "link": "http://arxiv.org/abs/2411.06232v2", "date": "2025-05-23", "relevancy": 2.4575, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6311}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6036}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RCR%3A%20Robust%20Crowd%20Reconstruction%20with%20Upright%20Space%20from%20a%20Single%0A%20%20Large-scene%20Image&body=Title%3A%20RCR%3A%20Robust%20Crowd%20Reconstruction%20with%20Upright%20Space%20from%20a%20Single%0A%20%20Large-scene%20Image%0AAuthor%3A%20Jing%20Huang%20and%20Hao%20Wen%20and%20Tianyi%20Zhou%20and%20Haozhe%20Lin%20and%20Yu-kun%20Lai%20and%20Kun%20Li%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20spatially%20consistent%20hundreds%20of%20human%20pose%20and%20shape%0Areconstruction%20from%20a%20single%20large-scene%20image%20with%20various%20human%20scales%20under%0Aarbitrary%20camera%20FoVs%20%28Fields%20of%20View%29.%20Due%20to%20the%20small%20and%20highly%20varying%202D%0Ahuman%20scales%2C%20depth%20ambiguity%2C%20and%20perspective%20distortion%2C%20no%20existing%20methods%0Acan%20achieve%20globally%20consistent%20reconstruction%20with%20correct%20reprojection.%20To%0Aaddress%20these%20challenges%2C%20we%20first%20propose%20a%20new%20concept%2C%20Human-scene%20Virtual%0AInteraction%20Point%20%28HVIP%29%2C%20to%20convert%20the%20complex%203D%20human%20localization%20into%0A2D-pixel%20localization.%20We%20then%20extend%20it%20to%20RCR%20%28Robust%20Crowd%20Reconstruction%29%2C%0Awhich%20achieves%20globally%20consistent%20reconstruction%20and%20stable%20generalization%20on%0Adifferent%20camera%20FoVs%20without%20test-time%20optimization.%20To%20perceive%20humans%20in%0Avarying%20pixel%20sizes%2C%20we%20propose%20an%20Iterative%20Ground-aware%20Cropping%20to%0Aautomatically%20crop%20the%20image%20and%20then%20merge%20the%20results.%20To%20eliminate%20the%0Ainfluence%20of%20the%20camera%20and%20cropping%20process%20during%20the%20reconstruction%2C%20we%0Aintroduce%20a%20canonical%20Upright%203D%20Space%20and%20the%20corresponding%20Upright%202D%20Space.%0ATo%20link%20the%20canonical%20space%20and%20the%20camera%20space%2C%20we%20propose%20the%20Upright%0ANormalization%2C%20which%20transforms%20the%20local%20crop%20input%20into%20the%20Upright%202D%20Space%2C%0Aand%20transforms%20the%20output%20from%20the%20Upright%203D%20Space%20into%20the%20unified%20camera%0Aspace.%20Besides%2C%20we%20contribute%20two%20benchmark%20datasets%2C%20LargeCrowd%20and%20SynCrowd%2C%0Afor%20evaluating%20crowd%20reconstruction%20in%20large%20scenes.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%20The%20source%20code%20and%20data%0Awill%20be%20publicly%20available%20for%20research%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06232v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRCR%253A%2520Robust%2520Crowd%2520Reconstruction%2520with%2520Upright%2520Space%2520from%2520a%2520Single%250A%2520%2520Large-scene%2520Image%26entry.906535625%3DJing%2520Huang%2520and%2520Hao%2520Wen%2520and%2520Tianyi%2520Zhou%2520and%2520Haozhe%2520Lin%2520and%2520Yu-kun%2520Lai%2520and%2520Kun%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520spatially%2520consistent%2520hundreds%2520of%2520human%2520pose%2520and%2520shape%250Areconstruction%2520from%2520a%2520single%2520large-scene%2520image%2520with%2520various%2520human%2520scales%2520under%250Aarbitrary%2520camera%2520FoVs%2520%2528Fields%2520of%2520View%2529.%2520Due%2520to%2520the%2520small%2520and%2520highly%2520varying%25202D%250Ahuman%2520scales%252C%2520depth%2520ambiguity%252C%2520and%2520perspective%2520distortion%252C%2520no%2520existing%2520methods%250Acan%2520achieve%2520globally%2520consistent%2520reconstruction%2520with%2520correct%2520reprojection.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520first%2520propose%2520a%2520new%2520concept%252C%2520Human-scene%2520Virtual%250AInteraction%2520Point%2520%2528HVIP%2529%252C%2520to%2520convert%2520the%2520complex%25203D%2520human%2520localization%2520into%250A2D-pixel%2520localization.%2520We%2520then%2520extend%2520it%2520to%2520RCR%2520%2528Robust%2520Crowd%2520Reconstruction%2529%252C%250Awhich%2520achieves%2520globally%2520consistent%2520reconstruction%2520and%2520stable%2520generalization%2520on%250Adifferent%2520camera%2520FoVs%2520without%2520test-time%2520optimization.%2520To%2520perceive%2520humans%2520in%250Avarying%2520pixel%2520sizes%252C%2520we%2520propose%2520an%2520Iterative%2520Ground-aware%2520Cropping%2520to%250Aautomatically%2520crop%2520the%2520image%2520and%2520then%2520merge%2520the%2520results.%2520To%2520eliminate%2520the%250Ainfluence%2520of%2520the%2520camera%2520and%2520cropping%2520process%2520during%2520the%2520reconstruction%252C%2520we%250Aintroduce%2520a%2520canonical%2520Upright%25203D%2520Space%2520and%2520the%2520corresponding%2520Upright%25202D%2520Space.%250ATo%2520link%2520the%2520canonical%2520space%2520and%2520the%2520camera%2520space%252C%2520we%2520propose%2520the%2520Upright%250ANormalization%252C%2520which%2520transforms%2520the%2520local%2520crop%2520input%2520into%2520the%2520Upright%25202D%2520Space%252C%250Aand%2520transforms%2520the%2520output%2520from%2520the%2520Upright%25203D%2520Space%2520into%2520the%2520unified%2520camera%250Aspace.%2520Besides%252C%2520we%2520contribute%2520two%2520benchmark%2520datasets%252C%2520LargeCrowd%2520and%2520SynCrowd%252C%250Afor%2520evaluating%2520crowd%2520reconstruction%2520in%2520large%2520scenes.%2520Experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%2520The%2520source%2520code%2520and%2520data%250Awill%2520be%2520publicly%2520available%2520for%2520research%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06232v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RCR%3A%20Robust%20Crowd%20Reconstruction%20with%20Upright%20Space%20from%20a%20Single%0A%20%20Large-scene%20Image&entry.906535625=Jing%20Huang%20and%20Hao%20Wen%20and%20Tianyi%20Zhou%20and%20Haozhe%20Lin%20and%20Yu-kun%20Lai%20and%20Kun%20Li&entry.1292438233=%20%20This%20paper%20focuses%20on%20spatially%20consistent%20hundreds%20of%20human%20pose%20and%20shape%0Areconstruction%20from%20a%20single%20large-scene%20image%20with%20various%20human%20scales%20under%0Aarbitrary%20camera%20FoVs%20%28Fields%20of%20View%29.%20Due%20to%20the%20small%20and%20highly%20varying%202D%0Ahuman%20scales%2C%20depth%20ambiguity%2C%20and%20perspective%20distortion%2C%20no%20existing%20methods%0Acan%20achieve%20globally%20consistent%20reconstruction%20with%20correct%20reprojection.%20To%0Aaddress%20these%20challenges%2C%20we%20first%20propose%20a%20new%20concept%2C%20Human-scene%20Virtual%0AInteraction%20Point%20%28HVIP%29%2C%20to%20convert%20the%20complex%203D%20human%20localization%20into%0A2D-pixel%20localization.%20We%20then%20extend%20it%20to%20RCR%20%28Robust%20Crowd%20Reconstruction%29%2C%0Awhich%20achieves%20globally%20consistent%20reconstruction%20and%20stable%20generalization%20on%0Adifferent%20camera%20FoVs%20without%20test-time%20optimization.%20To%20perceive%20humans%20in%0Avarying%20pixel%20sizes%2C%20we%20propose%20an%20Iterative%20Ground-aware%20Cropping%20to%0Aautomatically%20crop%20the%20image%20and%20then%20merge%20the%20results.%20To%20eliminate%20the%0Ainfluence%20of%20the%20camera%20and%20cropping%20process%20during%20the%20reconstruction%2C%20we%0Aintroduce%20a%20canonical%20Upright%203D%20Space%20and%20the%20corresponding%20Upright%202D%20Space.%0ATo%20link%20the%20canonical%20space%20and%20the%20camera%20space%2C%20we%20propose%20the%20Upright%0ANormalization%2C%20which%20transforms%20the%20local%20crop%20input%20into%20the%20Upright%202D%20Space%2C%0Aand%20transforms%20the%20output%20from%20the%20Upright%203D%20Space%20into%20the%20unified%20camera%0Aspace.%20Besides%2C%20we%20contribute%20two%20benchmark%20datasets%2C%20LargeCrowd%20and%20SynCrowd%2C%0Afor%20evaluating%20crowd%20reconstruction%20in%20large%20scenes.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%20The%20source%20code%20and%20data%0Awill%20be%20publicly%20available%20for%20research%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06232v2&entry.124074799=Read"},
{"title": "QVGen: Pushing the Limit of Quantized Video Generative Models", "author": "Yushi Huang and Ruihao Gong and Jing Liu and Yifu Ding and Chengtao Lv and Haotong Qin and Jun Zhang", "abstract": "  Video diffusion models (DMs) have enabled high-quality video synthesis. Yet,\ntheir substantial computational and memory demands pose serious challenges to\nreal-world deployment, even on high-end GPUs. As a commonly adopted solution,\nquantization has proven notable success in reducing cost for image DMs, while\nits direct application to video DMs remains ineffective. In this paper, we\npresent QVGen, a novel quantization-aware training (QAT) framework tailored for\nhigh-performance and inference-efficient video DMs under extremely low-bit\nquantization (e.g., 4-bit or below). We begin with a theoretical analysis\ndemonstrating that reducing the gradient norm is essential to facilitate\nconvergence for QAT. To this end, we introduce auxiliary modules ($\\Phi$) to\nmitigate large quantization errors, leading to significantly enhanced\nconvergence. To eliminate the inference overhead of $\\Phi$, we propose a\nrank-decay strategy that progressively eliminates $\\Phi$. Specifically, we\nrepeatedly employ singular value decomposition (SVD) and a proposed rank-based\nregularization $\\mathbf{\\gamma}$ to identify and decay low-contributing\ncomponents. This strategy retains performance while zeroing out inference\noverhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs,\nwith parameter sizes ranging from $1.3$B $\\sim14$B, show that QVGen is the\nfirst to reach full-precision comparable quality under 4-bit settings.\nMoreover, it significantly outperforms existing methods. For instance, our\n3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and\n$+8.43$ in Scene Consistency on VBench.\n", "link": "http://arxiv.org/abs/2505.11497v2", "date": "2025-05-23", "relevancy": 2.4545, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6325}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6044}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QVGen%3A%20Pushing%20the%20Limit%20of%20Quantized%20Video%20Generative%20Models&body=Title%3A%20QVGen%3A%20Pushing%20the%20Limit%20of%20Quantized%20Video%20Generative%20Models%0AAuthor%3A%20Yushi%20Huang%20and%20Ruihao%20Gong%20and%20Jing%20Liu%20and%20Yifu%20Ding%20and%20Chengtao%20Lv%20and%20Haotong%20Qin%20and%20Jun%20Zhang%0AAbstract%3A%20%20%20Video%20diffusion%20models%20%28DMs%29%20have%20enabled%20high-quality%20video%20synthesis.%20Yet%2C%0Atheir%20substantial%20computational%20and%20memory%20demands%20pose%20serious%20challenges%20to%0Areal-world%20deployment%2C%20even%20on%20high-end%20GPUs.%20As%20a%20commonly%20adopted%20solution%2C%0Aquantization%20has%20proven%20notable%20success%20in%20reducing%20cost%20for%20image%20DMs%2C%20while%0Aits%20direct%20application%20to%20video%20DMs%20remains%20ineffective.%20In%20this%20paper%2C%20we%0Apresent%20QVGen%2C%20a%20novel%20quantization-aware%20training%20%28QAT%29%20framework%20tailored%20for%0Ahigh-performance%20and%20inference-efficient%20video%20DMs%20under%20extremely%20low-bit%0Aquantization%20%28e.g.%2C%204-bit%20or%20below%29.%20We%20begin%20with%20a%20theoretical%20analysis%0Ademonstrating%20that%20reducing%20the%20gradient%20norm%20is%20essential%20to%20facilitate%0Aconvergence%20for%20QAT.%20To%20this%20end%2C%20we%20introduce%20auxiliary%20modules%20%28%24%5CPhi%24%29%20to%0Amitigate%20large%20quantization%20errors%2C%20leading%20to%20significantly%20enhanced%0Aconvergence.%20To%20eliminate%20the%20inference%20overhead%20of%20%24%5CPhi%24%2C%20we%20propose%20a%0Arank-decay%20strategy%20that%20progressively%20eliminates%20%24%5CPhi%24.%20Specifically%2C%20we%0Arepeatedly%20employ%20singular%20value%20decomposition%20%28SVD%29%20and%20a%20proposed%20rank-based%0Aregularization%20%24%5Cmathbf%7B%5Cgamma%7D%24%20to%20identify%20and%20decay%20low-contributing%0Acomponents.%20This%20strategy%20retains%20performance%20while%20zeroing%20out%20inference%0Aoverhead.%20Extensive%20experiments%20across%20%244%24%20state-of-the-art%20%28SOTA%29%20video%20DMs%2C%0Awith%20parameter%20sizes%20ranging%20from%20%241.3%24B%20%24%5Csim14%24B%2C%20show%20that%20QVGen%20is%20the%0Afirst%20to%20reach%20full-precision%20comparable%20quality%20under%204-bit%20settings.%0AMoreover%2C%20it%20significantly%20outperforms%20existing%20methods.%20For%20instance%2C%20our%0A3-bit%20CogVideoX-2B%20achieves%20improvements%20of%20%24%2B25.28%24%20in%20Dynamic%20Degree%20and%0A%24%2B8.43%24%20in%20Scene%20Consistency%20on%20VBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11497v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQVGen%253A%2520Pushing%2520the%2520Limit%2520of%2520Quantized%2520Video%2520Generative%2520Models%26entry.906535625%3DYushi%2520Huang%2520and%2520Ruihao%2520Gong%2520and%2520Jing%2520Liu%2520and%2520Yifu%2520Ding%2520and%2520Chengtao%2520Lv%2520and%2520Haotong%2520Qin%2520and%2520Jun%2520Zhang%26entry.1292438233%3D%2520%2520Video%2520diffusion%2520models%2520%2528DMs%2529%2520have%2520enabled%2520high-quality%2520video%2520synthesis.%2520Yet%252C%250Atheir%2520substantial%2520computational%2520and%2520memory%2520demands%2520pose%2520serious%2520challenges%2520to%250Areal-world%2520deployment%252C%2520even%2520on%2520high-end%2520GPUs.%2520As%2520a%2520commonly%2520adopted%2520solution%252C%250Aquantization%2520has%2520proven%2520notable%2520success%2520in%2520reducing%2520cost%2520for%2520image%2520DMs%252C%2520while%250Aits%2520direct%2520application%2520to%2520video%2520DMs%2520remains%2520ineffective.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520QVGen%252C%2520a%2520novel%2520quantization-aware%2520training%2520%2528QAT%2529%2520framework%2520tailored%2520for%250Ahigh-performance%2520and%2520inference-efficient%2520video%2520DMs%2520under%2520extremely%2520low-bit%250Aquantization%2520%2528e.g.%252C%25204-bit%2520or%2520below%2529.%2520We%2520begin%2520with%2520a%2520theoretical%2520analysis%250Ademonstrating%2520that%2520reducing%2520the%2520gradient%2520norm%2520is%2520essential%2520to%2520facilitate%250Aconvergence%2520for%2520QAT.%2520To%2520this%2520end%252C%2520we%2520introduce%2520auxiliary%2520modules%2520%2528%2524%255CPhi%2524%2529%2520to%250Amitigate%2520large%2520quantization%2520errors%252C%2520leading%2520to%2520significantly%2520enhanced%250Aconvergence.%2520To%2520eliminate%2520the%2520inference%2520overhead%2520of%2520%2524%255CPhi%2524%252C%2520we%2520propose%2520a%250Arank-decay%2520strategy%2520that%2520progressively%2520eliminates%2520%2524%255CPhi%2524.%2520Specifically%252C%2520we%250Arepeatedly%2520employ%2520singular%2520value%2520decomposition%2520%2528SVD%2529%2520and%2520a%2520proposed%2520rank-based%250Aregularization%2520%2524%255Cmathbf%257B%255Cgamma%257D%2524%2520to%2520identify%2520and%2520decay%2520low-contributing%250Acomponents.%2520This%2520strategy%2520retains%2520performance%2520while%2520zeroing%2520out%2520inference%250Aoverhead.%2520Extensive%2520experiments%2520across%2520%25244%2524%2520state-of-the-art%2520%2528SOTA%2529%2520video%2520DMs%252C%250Awith%2520parameter%2520sizes%2520ranging%2520from%2520%25241.3%2524B%2520%2524%255Csim14%2524B%252C%2520show%2520that%2520QVGen%2520is%2520the%250Afirst%2520to%2520reach%2520full-precision%2520comparable%2520quality%2520under%25204-bit%2520settings.%250AMoreover%252C%2520it%2520significantly%2520outperforms%2520existing%2520methods.%2520For%2520instance%252C%2520our%250A3-bit%2520CogVideoX-2B%2520achieves%2520improvements%2520of%2520%2524%252B25.28%2524%2520in%2520Dynamic%2520Degree%2520and%250A%2524%252B8.43%2524%2520in%2520Scene%2520Consistency%2520on%2520VBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11497v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QVGen%3A%20Pushing%20the%20Limit%20of%20Quantized%20Video%20Generative%20Models&entry.906535625=Yushi%20Huang%20and%20Ruihao%20Gong%20and%20Jing%20Liu%20and%20Yifu%20Ding%20and%20Chengtao%20Lv%20and%20Haotong%20Qin%20and%20Jun%20Zhang&entry.1292438233=%20%20Video%20diffusion%20models%20%28DMs%29%20have%20enabled%20high-quality%20video%20synthesis.%20Yet%2C%0Atheir%20substantial%20computational%20and%20memory%20demands%20pose%20serious%20challenges%20to%0Areal-world%20deployment%2C%20even%20on%20high-end%20GPUs.%20As%20a%20commonly%20adopted%20solution%2C%0Aquantization%20has%20proven%20notable%20success%20in%20reducing%20cost%20for%20image%20DMs%2C%20while%0Aits%20direct%20application%20to%20video%20DMs%20remains%20ineffective.%20In%20this%20paper%2C%20we%0Apresent%20QVGen%2C%20a%20novel%20quantization-aware%20training%20%28QAT%29%20framework%20tailored%20for%0Ahigh-performance%20and%20inference-efficient%20video%20DMs%20under%20extremely%20low-bit%0Aquantization%20%28e.g.%2C%204-bit%20or%20below%29.%20We%20begin%20with%20a%20theoretical%20analysis%0Ademonstrating%20that%20reducing%20the%20gradient%20norm%20is%20essential%20to%20facilitate%0Aconvergence%20for%20QAT.%20To%20this%20end%2C%20we%20introduce%20auxiliary%20modules%20%28%24%5CPhi%24%29%20to%0Amitigate%20large%20quantization%20errors%2C%20leading%20to%20significantly%20enhanced%0Aconvergence.%20To%20eliminate%20the%20inference%20overhead%20of%20%24%5CPhi%24%2C%20we%20propose%20a%0Arank-decay%20strategy%20that%20progressively%20eliminates%20%24%5CPhi%24.%20Specifically%2C%20we%0Arepeatedly%20employ%20singular%20value%20decomposition%20%28SVD%29%20and%20a%20proposed%20rank-based%0Aregularization%20%24%5Cmathbf%7B%5Cgamma%7D%24%20to%20identify%20and%20decay%20low-contributing%0Acomponents.%20This%20strategy%20retains%20performance%20while%20zeroing%20out%20inference%0Aoverhead.%20Extensive%20experiments%20across%20%244%24%20state-of-the-art%20%28SOTA%29%20video%20DMs%2C%0Awith%20parameter%20sizes%20ranging%20from%20%241.3%24B%20%24%5Csim14%24B%2C%20show%20that%20QVGen%20is%20the%0Afirst%20to%20reach%20full-precision%20comparable%20quality%20under%204-bit%20settings.%0AMoreover%2C%20it%20significantly%20outperforms%20existing%20methods.%20For%20instance%2C%20our%0A3-bit%20CogVideoX-2B%20achieves%20improvements%20of%20%24%2B25.28%24%20in%20Dynamic%20Degree%20and%0A%24%2B8.43%24%20in%20Scene%20Consistency%20on%20VBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11497v2&entry.124074799=Read"},
{"title": "Object-level Cross-view Geo-localization with Location Enhancement and\n  Multi-Head Cross Attention", "author": "Zheyang Huang and Jagannath Aryal and Saeid Nahavandi and Xuequan Lu and Chee Peng Lim and Lei Wei and Hailing Zhou", "abstract": "  Cross-view geo-localization determines the location of a query image,\ncaptured by a drone or ground-based camera, by matching it to a geo-referenced\nsatellite image. While traditional approaches focus on image-level\nlocalization, many applications, such as search-and-rescue, infrastructure\ninspection, and precision delivery, demand object-level accuracy. This enables\nusers to prompt a specific object with a single click on a drone image to\nretrieve precise geo-tagged information of the object. However, variations in\nviewpoints, timing, and imaging conditions pose significant challenges,\nespecially when identifying visually similar objects in extensive satellite\nimagery. To address these challenges, we propose an Object-level Cross-view\nGeo-localization Network (OCGNet). It integrates user-specified click locations\nusing Gaussian Kernel Transfer (GKT) to preserve location information\nthroughout the network. This cue is dually embedded into the feature encoder\nand feature matching blocks, ensuring robust object-specific localization.\nAdditionally, OCGNet incorporates a Location Enhancement (LE) module and a\nMulti-Head Cross Attention (MHCA) module to adaptively emphasize\nobject-specific features or expand focus to relevant contextual regions when\nnecessary. OCGNet achieves state-of-the-art performance on a public dataset,\nCVOGL. It also demonstrates few-shot learning capabilities, effectively\ngeneralizing from limited examples, making it suitable for diverse applications\n(https://github.com/ZheyangH/OCGNet).\n", "link": "http://arxiv.org/abs/2505.17911v1", "date": "2025-05-23", "relevancy": 2.4309, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6376}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.591}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-level%20Cross-view%20Geo-localization%20with%20Location%20Enhancement%20and%0A%20%20Multi-Head%20Cross%20Attention&body=Title%3A%20Object-level%20Cross-view%20Geo-localization%20with%20Location%20Enhancement%20and%0A%20%20Multi-Head%20Cross%20Attention%0AAuthor%3A%20Zheyang%20Huang%20and%20Jagannath%20Aryal%20and%20Saeid%20Nahavandi%20and%20Xuequan%20Lu%20and%20Chee%20Peng%20Lim%20and%20Lei%20Wei%20and%20Hailing%20Zhou%0AAbstract%3A%20%20%20Cross-view%20geo-localization%20determines%20the%20location%20of%20a%20query%20image%2C%0Acaptured%20by%20a%20drone%20or%20ground-based%20camera%2C%20by%20matching%20it%20to%20a%20geo-referenced%0Asatellite%20image.%20While%20traditional%20approaches%20focus%20on%20image-level%0Alocalization%2C%20many%20applications%2C%20such%20as%20search-and-rescue%2C%20infrastructure%0Ainspection%2C%20and%20precision%20delivery%2C%20demand%20object-level%20accuracy.%20This%20enables%0Ausers%20to%20prompt%20a%20specific%20object%20with%20a%20single%20click%20on%20a%20drone%20image%20to%0Aretrieve%20precise%20geo-tagged%20information%20of%20the%20object.%20However%2C%20variations%20in%0Aviewpoints%2C%20timing%2C%20and%20imaging%20conditions%20pose%20significant%20challenges%2C%0Aespecially%20when%20identifying%20visually%20similar%20objects%20in%20extensive%20satellite%0Aimagery.%20To%20address%20these%20challenges%2C%20we%20propose%20an%20Object-level%20Cross-view%0AGeo-localization%20Network%20%28OCGNet%29.%20It%20integrates%20user-specified%20click%20locations%0Ausing%20Gaussian%20Kernel%20Transfer%20%28GKT%29%20to%20preserve%20location%20information%0Athroughout%20the%20network.%20This%20cue%20is%20dually%20embedded%20into%20the%20feature%20encoder%0Aand%20feature%20matching%20blocks%2C%20ensuring%20robust%20object-specific%20localization.%0AAdditionally%2C%20OCGNet%20incorporates%20a%20Location%20Enhancement%20%28LE%29%20module%20and%20a%0AMulti-Head%20Cross%20Attention%20%28MHCA%29%20module%20to%20adaptively%20emphasize%0Aobject-specific%20features%20or%20expand%20focus%20to%20relevant%20contextual%20regions%20when%0Anecessary.%20OCGNet%20achieves%20state-of-the-art%20performance%20on%20a%20public%20dataset%2C%0ACVOGL.%20It%20also%20demonstrates%20few-shot%20learning%20capabilities%2C%20effectively%0Ageneralizing%20from%20limited%20examples%2C%20making%20it%20suitable%20for%20diverse%20applications%0A%28https%3A//github.com/ZheyangH/OCGNet%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17911v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-level%2520Cross-view%2520Geo-localization%2520with%2520Location%2520Enhancement%2520and%250A%2520%2520Multi-Head%2520Cross%2520Attention%26entry.906535625%3DZheyang%2520Huang%2520and%2520Jagannath%2520Aryal%2520and%2520Saeid%2520Nahavandi%2520and%2520Xuequan%2520Lu%2520and%2520Chee%2520Peng%2520Lim%2520and%2520Lei%2520Wei%2520and%2520Hailing%2520Zhou%26entry.1292438233%3D%2520%2520Cross-view%2520geo-localization%2520determines%2520the%2520location%2520of%2520a%2520query%2520image%252C%250Acaptured%2520by%2520a%2520drone%2520or%2520ground-based%2520camera%252C%2520by%2520matching%2520it%2520to%2520a%2520geo-referenced%250Asatellite%2520image.%2520While%2520traditional%2520approaches%2520focus%2520on%2520image-level%250Alocalization%252C%2520many%2520applications%252C%2520such%2520as%2520search-and-rescue%252C%2520infrastructure%250Ainspection%252C%2520and%2520precision%2520delivery%252C%2520demand%2520object-level%2520accuracy.%2520This%2520enables%250Ausers%2520to%2520prompt%2520a%2520specific%2520object%2520with%2520a%2520single%2520click%2520on%2520a%2520drone%2520image%2520to%250Aretrieve%2520precise%2520geo-tagged%2520information%2520of%2520the%2520object.%2520However%252C%2520variations%2520in%250Aviewpoints%252C%2520timing%252C%2520and%2520imaging%2520conditions%2520pose%2520significant%2520challenges%252C%250Aespecially%2520when%2520identifying%2520visually%2520similar%2520objects%2520in%2520extensive%2520satellite%250Aimagery.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520an%2520Object-level%2520Cross-view%250AGeo-localization%2520Network%2520%2528OCGNet%2529.%2520It%2520integrates%2520user-specified%2520click%2520locations%250Ausing%2520Gaussian%2520Kernel%2520Transfer%2520%2528GKT%2529%2520to%2520preserve%2520location%2520information%250Athroughout%2520the%2520network.%2520This%2520cue%2520is%2520dually%2520embedded%2520into%2520the%2520feature%2520encoder%250Aand%2520feature%2520matching%2520blocks%252C%2520ensuring%2520robust%2520object-specific%2520localization.%250AAdditionally%252C%2520OCGNet%2520incorporates%2520a%2520Location%2520Enhancement%2520%2528LE%2529%2520module%2520and%2520a%250AMulti-Head%2520Cross%2520Attention%2520%2528MHCA%2529%2520module%2520to%2520adaptively%2520emphasize%250Aobject-specific%2520features%2520or%2520expand%2520focus%2520to%2520relevant%2520contextual%2520regions%2520when%250Anecessary.%2520OCGNet%2520achieves%2520state-of-the-art%2520performance%2520on%2520a%2520public%2520dataset%252C%250ACVOGL.%2520It%2520also%2520demonstrates%2520few-shot%2520learning%2520capabilities%252C%2520effectively%250Ageneralizing%2520from%2520limited%2520examples%252C%2520making%2520it%2520suitable%2520for%2520diverse%2520applications%250A%2528https%253A//github.com/ZheyangH/OCGNet%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17911v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-level%20Cross-view%20Geo-localization%20with%20Location%20Enhancement%20and%0A%20%20Multi-Head%20Cross%20Attention&entry.906535625=Zheyang%20Huang%20and%20Jagannath%20Aryal%20and%20Saeid%20Nahavandi%20and%20Xuequan%20Lu%20and%20Chee%20Peng%20Lim%20and%20Lei%20Wei%20and%20Hailing%20Zhou&entry.1292438233=%20%20Cross-view%20geo-localization%20determines%20the%20location%20of%20a%20query%20image%2C%0Acaptured%20by%20a%20drone%20or%20ground-based%20camera%2C%20by%20matching%20it%20to%20a%20geo-referenced%0Asatellite%20image.%20While%20traditional%20approaches%20focus%20on%20image-level%0Alocalization%2C%20many%20applications%2C%20such%20as%20search-and-rescue%2C%20infrastructure%0Ainspection%2C%20and%20precision%20delivery%2C%20demand%20object-level%20accuracy.%20This%20enables%0Ausers%20to%20prompt%20a%20specific%20object%20with%20a%20single%20click%20on%20a%20drone%20image%20to%0Aretrieve%20precise%20geo-tagged%20information%20of%20the%20object.%20However%2C%20variations%20in%0Aviewpoints%2C%20timing%2C%20and%20imaging%20conditions%20pose%20significant%20challenges%2C%0Aespecially%20when%20identifying%20visually%20similar%20objects%20in%20extensive%20satellite%0Aimagery.%20To%20address%20these%20challenges%2C%20we%20propose%20an%20Object-level%20Cross-view%0AGeo-localization%20Network%20%28OCGNet%29.%20It%20integrates%20user-specified%20click%20locations%0Ausing%20Gaussian%20Kernel%20Transfer%20%28GKT%29%20to%20preserve%20location%20information%0Athroughout%20the%20network.%20This%20cue%20is%20dually%20embedded%20into%20the%20feature%20encoder%0Aand%20feature%20matching%20blocks%2C%20ensuring%20robust%20object-specific%20localization.%0AAdditionally%2C%20OCGNet%20incorporates%20a%20Location%20Enhancement%20%28LE%29%20module%20and%20a%0AMulti-Head%20Cross%20Attention%20%28MHCA%29%20module%20to%20adaptively%20emphasize%0Aobject-specific%20features%20or%20expand%20focus%20to%20relevant%20contextual%20regions%20when%0Anecessary.%20OCGNet%20achieves%20state-of-the-art%20performance%20on%20a%20public%20dataset%2C%0ACVOGL.%20It%20also%20demonstrates%20few-shot%20learning%20capabilities%2C%20effectively%0Ageneralizing%20from%20limited%20examples%2C%20making%20it%20suitable%20for%20diverse%20applications%0A%28https%3A//github.com/ZheyangH/OCGNet%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17911v1&entry.124074799=Read"},
{"title": "Enhancing Low-Resource Language and Instruction Following Capabilities\n  of Audio Language Models", "author": "Potsawee Manakul and Guangzhi Sun and Warit Sirichotedumrong and Kasima Tharnpipitchai and Kunat Pipatanakul", "abstract": "  Audio language models process audio inputs using textual prompts for tasks\nlike speech recognition and audio captioning. Although built on multilingual\npre-trained components, most are trained primarily on English, limiting their\nusability for other languages. This paper evaluates audio language models on\nThai, a low-resource language, and finds that they lack emergent cross-lingual\nabilities despite their multilingual foundations. To address this, we explore\ndata mixtures that optimize audio language models for both a target language\nand English while integrating audio comprehension and speech\ninstruction-following into a unified model. Our experiments provide insights\ninto improving instruction-following in low-resource languages by balancing\nlanguage-specific and multilingual training data. The proposed model,\nTyphoon-Audio, significantly outperforms existing open-source models and\nachieves performance comparable to state-of-the-art Gemini-1.5-Pro in both\nEnglish and Thai.\n", "link": "http://arxiv.org/abs/2409.10999v2", "date": "2025-05-23", "relevancy": 2.43, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Low-Resource%20Language%20and%20Instruction%20Following%20Capabilities%0A%20%20of%20Audio%20Language%20Models&body=Title%3A%20Enhancing%20Low-Resource%20Language%20and%20Instruction%20Following%20Capabilities%0A%20%20of%20Audio%20Language%20Models%0AAuthor%3A%20Potsawee%20Manakul%20and%20Guangzhi%20Sun%20and%20Warit%20Sirichotedumrong%20and%20Kasima%20Tharnpipitchai%20and%20Kunat%20Pipatanakul%0AAbstract%3A%20%20%20Audio%20language%20models%20process%20audio%20inputs%20using%20textual%20prompts%20for%20tasks%0Alike%20speech%20recognition%20and%20audio%20captioning.%20Although%20built%20on%20multilingual%0Apre-trained%20components%2C%20most%20are%20trained%20primarily%20on%20English%2C%20limiting%20their%0Ausability%20for%20other%20languages.%20This%20paper%20evaluates%20audio%20language%20models%20on%0AThai%2C%20a%20low-resource%20language%2C%20and%20finds%20that%20they%20lack%20emergent%20cross-lingual%0Aabilities%20despite%20their%20multilingual%20foundations.%20To%20address%20this%2C%20we%20explore%0Adata%20mixtures%20that%20optimize%20audio%20language%20models%20for%20both%20a%20target%20language%0Aand%20English%20while%20integrating%20audio%20comprehension%20and%20speech%0Ainstruction-following%20into%20a%20unified%20model.%20Our%20experiments%20provide%20insights%0Ainto%20improving%20instruction-following%20in%20low-resource%20languages%20by%20balancing%0Alanguage-specific%20and%20multilingual%20training%20data.%20The%20proposed%20model%2C%0ATyphoon-Audio%2C%20significantly%20outperforms%20existing%20open-source%20models%20and%0Aachieves%20performance%20comparable%20to%20state-of-the-art%20Gemini-1.5-Pro%20in%20both%0AEnglish%20and%20Thai.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10999v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Low-Resource%2520Language%2520and%2520Instruction%2520Following%2520Capabilities%250A%2520%2520of%2520Audio%2520Language%2520Models%26entry.906535625%3DPotsawee%2520Manakul%2520and%2520Guangzhi%2520Sun%2520and%2520Warit%2520Sirichotedumrong%2520and%2520Kasima%2520Tharnpipitchai%2520and%2520Kunat%2520Pipatanakul%26entry.1292438233%3D%2520%2520Audio%2520language%2520models%2520process%2520audio%2520inputs%2520using%2520textual%2520prompts%2520for%2520tasks%250Alike%2520speech%2520recognition%2520and%2520audio%2520captioning.%2520Although%2520built%2520on%2520multilingual%250Apre-trained%2520components%252C%2520most%2520are%2520trained%2520primarily%2520on%2520English%252C%2520limiting%2520their%250Ausability%2520for%2520other%2520languages.%2520This%2520paper%2520evaluates%2520audio%2520language%2520models%2520on%250AThai%252C%2520a%2520low-resource%2520language%252C%2520and%2520finds%2520that%2520they%2520lack%2520emergent%2520cross-lingual%250Aabilities%2520despite%2520their%2520multilingual%2520foundations.%2520To%2520address%2520this%252C%2520we%2520explore%250Adata%2520mixtures%2520that%2520optimize%2520audio%2520language%2520models%2520for%2520both%2520a%2520target%2520language%250Aand%2520English%2520while%2520integrating%2520audio%2520comprehension%2520and%2520speech%250Ainstruction-following%2520into%2520a%2520unified%2520model.%2520Our%2520experiments%2520provide%2520insights%250Ainto%2520improving%2520instruction-following%2520in%2520low-resource%2520languages%2520by%2520balancing%250Alanguage-specific%2520and%2520multilingual%2520training%2520data.%2520The%2520proposed%2520model%252C%250ATyphoon-Audio%252C%2520significantly%2520outperforms%2520existing%2520open-source%2520models%2520and%250Aachieves%2520performance%2520comparable%2520to%2520state-of-the-art%2520Gemini-1.5-Pro%2520in%2520both%250AEnglish%2520and%2520Thai.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10999v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Low-Resource%20Language%20and%20Instruction%20Following%20Capabilities%0A%20%20of%20Audio%20Language%20Models&entry.906535625=Potsawee%20Manakul%20and%20Guangzhi%20Sun%20and%20Warit%20Sirichotedumrong%20and%20Kasima%20Tharnpipitchai%20and%20Kunat%20Pipatanakul&entry.1292438233=%20%20Audio%20language%20models%20process%20audio%20inputs%20using%20textual%20prompts%20for%20tasks%0Alike%20speech%20recognition%20and%20audio%20captioning.%20Although%20built%20on%20multilingual%0Apre-trained%20components%2C%20most%20are%20trained%20primarily%20on%20English%2C%20limiting%20their%0Ausability%20for%20other%20languages.%20This%20paper%20evaluates%20audio%20language%20models%20on%0AThai%2C%20a%20low-resource%20language%2C%20and%20finds%20that%20they%20lack%20emergent%20cross-lingual%0Aabilities%20despite%20their%20multilingual%20foundations.%20To%20address%20this%2C%20we%20explore%0Adata%20mixtures%20that%20optimize%20audio%20language%20models%20for%20both%20a%20target%20language%0Aand%20English%20while%20integrating%20audio%20comprehension%20and%20speech%0Ainstruction-following%20into%20a%20unified%20model.%20Our%20experiments%20provide%20insights%0Ainto%20improving%20instruction-following%20in%20low-resource%20languages%20by%20balancing%0Alanguage-specific%20and%20multilingual%20training%20data.%20The%20proposed%20model%2C%0ATyphoon-Audio%2C%20significantly%20outperforms%20existing%20open-source%20models%20and%0Aachieves%20performance%20comparable%20to%20state-of-the-art%20Gemini-1.5-Pro%20in%20both%0AEnglish%20and%20Thai.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10999v2&entry.124074799=Read"},
{"title": "Activation Control for Efficiently Eliciting Long Chain-of-thought\n  Ability of Language Models", "author": "Zekai Zhao and Qi Liu and Kun Zhou and Zihan Liu and Yifei Shao and Zhiting Hu and Biwei Huang", "abstract": "  Despite the remarkable reasoning performance, eliciting the long\nchain-of-thought (CoT) ability in large language models (LLMs) typically\nrequires costly reinforcement learning or supervised fine-tuning on\nhigh-quality distilled data. We investigate the internal mechanisms behind this\ncapability and show that a small set of high-impact activations in the last few\nlayers largely governs long-form reasoning attributes, such as output length\nand self-reflection. By simply amplifying these activations and inserting\n\"wait\" tokens, we can invoke the long CoT ability without any training,\nresulting in significantly increased self-reflection rates and accuracy.\nMoreover, we find that the activation dynamics follow predictable trajectories,\nwith a sharp rise after special tokens and a subsequent exponential decay.\nBuilding on these insights, we introduce a general training-free activation\ncontrol technique. It leverages a few contrastive examples to identify key\nactivations, and employs simple analytic functions to modulate their values at\ninference time to elicit long CoTs. Extensive experiments confirm the\neffectiveness of our method in efficiently eliciting long CoT reasoning in LLMs\nand improving their performance. Additionally, we propose a parameter-efficient\nfine-tuning method that trains only a last-layer activation amplification\nmodule and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning\nbenchmarks with significantly fewer parameters. Our code and data are publicly\nreleased.\n", "link": "http://arxiv.org/abs/2505.17697v1", "date": "2025-05-23", "relevancy": 2.4239, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4869}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4865}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Activation%20Control%20for%20Efficiently%20Eliciting%20Long%20Chain-of-thought%0A%20%20Ability%20of%20Language%20Models&body=Title%3A%20Activation%20Control%20for%20Efficiently%20Eliciting%20Long%20Chain-of-thought%0A%20%20Ability%20of%20Language%20Models%0AAuthor%3A%20Zekai%20Zhao%20and%20Qi%20Liu%20and%20Kun%20Zhou%20and%20Zihan%20Liu%20and%20Yifei%20Shao%20and%20Zhiting%20Hu%20and%20Biwei%20Huang%0AAbstract%3A%20%20%20Despite%20the%20remarkable%20reasoning%20performance%2C%20eliciting%20the%20long%0Achain-of-thought%20%28CoT%29%20ability%20in%20large%20language%20models%20%28LLMs%29%20typically%0Arequires%20costly%20reinforcement%20learning%20or%20supervised%20fine-tuning%20on%0Ahigh-quality%20distilled%20data.%20We%20investigate%20the%20internal%20mechanisms%20behind%20this%0Acapability%20and%20show%20that%20a%20small%20set%20of%20high-impact%20activations%20in%20the%20last%20few%0Alayers%20largely%20governs%20long-form%20reasoning%20attributes%2C%20such%20as%20output%20length%0Aand%20self-reflection.%20By%20simply%20amplifying%20these%20activations%20and%20inserting%0A%22wait%22%20tokens%2C%20we%20can%20invoke%20the%20long%20CoT%20ability%20without%20any%20training%2C%0Aresulting%20in%20significantly%20increased%20self-reflection%20rates%20and%20accuracy.%0AMoreover%2C%20we%20find%20that%20the%20activation%20dynamics%20follow%20predictable%20trajectories%2C%0Awith%20a%20sharp%20rise%20after%20special%20tokens%20and%20a%20subsequent%20exponential%20decay.%0ABuilding%20on%20these%20insights%2C%20we%20introduce%20a%20general%20training-free%20activation%0Acontrol%20technique.%20It%20leverages%20a%20few%20contrastive%20examples%20to%20identify%20key%0Aactivations%2C%20and%20employs%20simple%20analytic%20functions%20to%20modulate%20their%20values%20at%0Ainference%20time%20to%20elicit%20long%20CoTs.%20Extensive%20experiments%20confirm%20the%0Aeffectiveness%20of%20our%20method%20in%20efficiently%20eliciting%20long%20CoT%20reasoning%20in%20LLMs%0Aand%20improving%20their%20performance.%20Additionally%2C%20we%20propose%20a%20parameter-efficient%0Afine-tuning%20method%20that%20trains%20only%20a%20last-layer%20activation%20amplification%0Amodule%20and%20a%20few%20LoRA%20layers%2C%20outperforming%20full%20LoRA%20fine-tuning%20on%20reasoning%0Abenchmarks%20with%20significantly%20fewer%20parameters.%20Our%20code%20and%20data%20are%20publicly%0Areleased.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActivation%2520Control%2520for%2520Efficiently%2520Eliciting%2520Long%2520Chain-of-thought%250A%2520%2520Ability%2520of%2520Language%2520Models%26entry.906535625%3DZekai%2520Zhao%2520and%2520Qi%2520Liu%2520and%2520Kun%2520Zhou%2520and%2520Zihan%2520Liu%2520and%2520Yifei%2520Shao%2520and%2520Zhiting%2520Hu%2520and%2520Biwei%2520Huang%26entry.1292438233%3D%2520%2520Despite%2520the%2520remarkable%2520reasoning%2520performance%252C%2520eliciting%2520the%2520long%250Achain-of-thought%2520%2528CoT%2529%2520ability%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520typically%250Arequires%2520costly%2520reinforcement%2520learning%2520or%2520supervised%2520fine-tuning%2520on%250Ahigh-quality%2520distilled%2520data.%2520We%2520investigate%2520the%2520internal%2520mechanisms%2520behind%2520this%250Acapability%2520and%2520show%2520that%2520a%2520small%2520set%2520of%2520high-impact%2520activations%2520in%2520the%2520last%2520few%250Alayers%2520largely%2520governs%2520long-form%2520reasoning%2520attributes%252C%2520such%2520as%2520output%2520length%250Aand%2520self-reflection.%2520By%2520simply%2520amplifying%2520these%2520activations%2520and%2520inserting%250A%2522wait%2522%2520tokens%252C%2520we%2520can%2520invoke%2520the%2520long%2520CoT%2520ability%2520without%2520any%2520training%252C%250Aresulting%2520in%2520significantly%2520increased%2520self-reflection%2520rates%2520and%2520accuracy.%250AMoreover%252C%2520we%2520find%2520that%2520the%2520activation%2520dynamics%2520follow%2520predictable%2520trajectories%252C%250Awith%2520a%2520sharp%2520rise%2520after%2520special%2520tokens%2520and%2520a%2520subsequent%2520exponential%2520decay.%250ABuilding%2520on%2520these%2520insights%252C%2520we%2520introduce%2520a%2520general%2520training-free%2520activation%250Acontrol%2520technique.%2520It%2520leverages%2520a%2520few%2520contrastive%2520examples%2520to%2520identify%2520key%250Aactivations%252C%2520and%2520employs%2520simple%2520analytic%2520functions%2520to%2520modulate%2520their%2520values%2520at%250Ainference%2520time%2520to%2520elicit%2520long%2520CoTs.%2520Extensive%2520experiments%2520confirm%2520the%250Aeffectiveness%2520of%2520our%2520method%2520in%2520efficiently%2520eliciting%2520long%2520CoT%2520reasoning%2520in%2520LLMs%250Aand%2520improving%2520their%2520performance.%2520Additionally%252C%2520we%2520propose%2520a%2520parameter-efficient%250Afine-tuning%2520method%2520that%2520trains%2520only%2520a%2520last-layer%2520activation%2520amplification%250Amodule%2520and%2520a%2520few%2520LoRA%2520layers%252C%2520outperforming%2520full%2520LoRA%2520fine-tuning%2520on%2520reasoning%250Abenchmarks%2520with%2520significantly%2520fewer%2520parameters.%2520Our%2520code%2520and%2520data%2520are%2520publicly%250Areleased.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Activation%20Control%20for%20Efficiently%20Eliciting%20Long%20Chain-of-thought%0A%20%20Ability%20of%20Language%20Models&entry.906535625=Zekai%20Zhao%20and%20Qi%20Liu%20and%20Kun%20Zhou%20and%20Zihan%20Liu%20and%20Yifei%20Shao%20and%20Zhiting%20Hu%20and%20Biwei%20Huang&entry.1292438233=%20%20Despite%20the%20remarkable%20reasoning%20performance%2C%20eliciting%20the%20long%0Achain-of-thought%20%28CoT%29%20ability%20in%20large%20language%20models%20%28LLMs%29%20typically%0Arequires%20costly%20reinforcement%20learning%20or%20supervised%20fine-tuning%20on%0Ahigh-quality%20distilled%20data.%20We%20investigate%20the%20internal%20mechanisms%20behind%20this%0Acapability%20and%20show%20that%20a%20small%20set%20of%20high-impact%20activations%20in%20the%20last%20few%0Alayers%20largely%20governs%20long-form%20reasoning%20attributes%2C%20such%20as%20output%20length%0Aand%20self-reflection.%20By%20simply%20amplifying%20these%20activations%20and%20inserting%0A%22wait%22%20tokens%2C%20we%20can%20invoke%20the%20long%20CoT%20ability%20without%20any%20training%2C%0Aresulting%20in%20significantly%20increased%20self-reflection%20rates%20and%20accuracy.%0AMoreover%2C%20we%20find%20that%20the%20activation%20dynamics%20follow%20predictable%20trajectories%2C%0Awith%20a%20sharp%20rise%20after%20special%20tokens%20and%20a%20subsequent%20exponential%20decay.%0ABuilding%20on%20these%20insights%2C%20we%20introduce%20a%20general%20training-free%20activation%0Acontrol%20technique.%20It%20leverages%20a%20few%20contrastive%20examples%20to%20identify%20key%0Aactivations%2C%20and%20employs%20simple%20analytic%20functions%20to%20modulate%20their%20values%20at%0Ainference%20time%20to%20elicit%20long%20CoTs.%20Extensive%20experiments%20confirm%20the%0Aeffectiveness%20of%20our%20method%20in%20efficiently%20eliciting%20long%20CoT%20reasoning%20in%20LLMs%0Aand%20improving%20their%20performance.%20Additionally%2C%20we%20propose%20a%20parameter-efficient%0Afine-tuning%20method%20that%20trains%20only%20a%20last-layer%20activation%20amplification%0Amodule%20and%20a%20few%20LoRA%20layers%2C%20outperforming%20full%20LoRA%20fine-tuning%20on%20reasoning%0Abenchmarks%20with%20significantly%20fewer%20parameters.%20Our%20code%20and%20data%20are%20publicly%0Areleased.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17697v1&entry.124074799=Read"},
{"title": "U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound\n  Understanding", "author": "Anjie Le and Henan Liu and Yue Wang and Zhenyu Liu and Rongkun Zhu and Taohan Weng and Jinze Yu and Boyang Wang and Yalun Wu and Kaiwen Yan and Quanlin Sun and Meirui Jiang and Jialun Pei and Siya Liu and Haoyun Zheng and Zhoujun Li and Alison Noble and Jacques Souquet and Xiaoqing Guo and Manxi Lin and Hongcheng Guo", "abstract": "  Ultrasound is a widely-used imaging modality critical to global healthcare,\nyet its interpretation remains challenging due to its varying image quality on\noperators, noises, and anatomical structures. Although large vision-language\nmodels (LVLMs) have demonstrated impressive multimodal capabilities across\nnatural and medical domains, their performance on ultrasound remains largely\nunexplored. We introduce U2-BENCH, the first comprehensive benchmark to\nevaluate LVLMs on ultrasound understanding across classification, detection,\nregression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning\n15 anatomical regions and defines 8 clinically inspired tasks, such as\ndiagnosis, view recognition, lesion localization, clinical value estimation,\nand report generation, across 50 ultrasound application scenarios. We evaluate\n20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and\nmedical-specific. Our results reveal strong performance on image-level\nclassification, but persistent challenges in spatial reasoning and clinical\nlanguage generation. U2-BENCH establishes a rigorous and unified testbed to\nassess and accelerate LVLM research in the uniquely multimodal domain of\nmedical ultrasound imaging.\n", "link": "http://arxiv.org/abs/2505.17779v1", "date": "2025-05-23", "relevancy": 2.398, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6076}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6076}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20U2-BENCH%3A%20Benchmarking%20Large%20Vision-Language%20Models%20on%20Ultrasound%0A%20%20Understanding&body=Title%3A%20U2-BENCH%3A%20Benchmarking%20Large%20Vision-Language%20Models%20on%20Ultrasound%0A%20%20Understanding%0AAuthor%3A%20Anjie%20Le%20and%20Henan%20Liu%20and%20Yue%20Wang%20and%20Zhenyu%20Liu%20and%20Rongkun%20Zhu%20and%20Taohan%20Weng%20and%20Jinze%20Yu%20and%20Boyang%20Wang%20and%20Yalun%20Wu%20and%20Kaiwen%20Yan%20and%20Quanlin%20Sun%20and%20Meirui%20Jiang%20and%20Jialun%20Pei%20and%20Siya%20Liu%20and%20Haoyun%20Zheng%20and%20Zhoujun%20Li%20and%20Alison%20Noble%20and%20Jacques%20Souquet%20and%20Xiaoqing%20Guo%20and%20Manxi%20Lin%20and%20Hongcheng%20Guo%0AAbstract%3A%20%20%20Ultrasound%20is%20a%20widely-used%20imaging%20modality%20critical%20to%20global%20healthcare%2C%0Ayet%20its%20interpretation%20remains%20challenging%20due%20to%20its%20varying%20image%20quality%20on%0Aoperators%2C%20noises%2C%20and%20anatomical%20structures.%20Although%20large%20vision-language%0Amodels%20%28LVLMs%29%20have%20demonstrated%20impressive%20multimodal%20capabilities%20across%0Anatural%20and%20medical%20domains%2C%20their%20performance%20on%20ultrasound%20remains%20largely%0Aunexplored.%20We%20introduce%20U2-BENCH%2C%20the%20first%20comprehensive%20benchmark%20to%0Aevaluate%20LVLMs%20on%20ultrasound%20understanding%20across%20classification%2C%20detection%2C%0Aregression%2C%20and%20text%20generation%20tasks.%20U2-BENCH%20aggregates%207%2C241%20cases%20spanning%0A15%20anatomical%20regions%20and%20defines%208%20clinically%20inspired%20tasks%2C%20such%20as%0Adiagnosis%2C%20view%20recognition%2C%20lesion%20localization%2C%20clinical%20value%20estimation%2C%0Aand%20report%20generation%2C%20across%2050%20ultrasound%20application%20scenarios.%20We%20evaluate%0A20%20state-of-the-art%20LVLMs%2C%20both%20open-%20and%20closed-source%2C%20general-purpose%20and%0Amedical-specific.%20Our%20results%20reveal%20strong%20performance%20on%20image-level%0Aclassification%2C%20but%20persistent%20challenges%20in%20spatial%20reasoning%20and%20clinical%0Alanguage%20generation.%20U2-BENCH%20establishes%20a%20rigorous%20and%20unified%20testbed%20to%0Aassess%20and%20accelerate%20LVLM%20research%20in%20the%20uniquely%20multimodal%20domain%20of%0Amedical%20ultrasound%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DU2-BENCH%253A%2520Benchmarking%2520Large%2520Vision-Language%2520Models%2520on%2520Ultrasound%250A%2520%2520Understanding%26entry.906535625%3DAnjie%2520Le%2520and%2520Henan%2520Liu%2520and%2520Yue%2520Wang%2520and%2520Zhenyu%2520Liu%2520and%2520Rongkun%2520Zhu%2520and%2520Taohan%2520Weng%2520and%2520Jinze%2520Yu%2520and%2520Boyang%2520Wang%2520and%2520Yalun%2520Wu%2520and%2520Kaiwen%2520Yan%2520and%2520Quanlin%2520Sun%2520and%2520Meirui%2520Jiang%2520and%2520Jialun%2520Pei%2520and%2520Siya%2520Liu%2520and%2520Haoyun%2520Zheng%2520and%2520Zhoujun%2520Li%2520and%2520Alison%2520Noble%2520and%2520Jacques%2520Souquet%2520and%2520Xiaoqing%2520Guo%2520and%2520Manxi%2520Lin%2520and%2520Hongcheng%2520Guo%26entry.1292438233%3D%2520%2520Ultrasound%2520is%2520a%2520widely-used%2520imaging%2520modality%2520critical%2520to%2520global%2520healthcare%252C%250Ayet%2520its%2520interpretation%2520remains%2520challenging%2520due%2520to%2520its%2520varying%2520image%2520quality%2520on%250Aoperators%252C%2520noises%252C%2520and%2520anatomical%2520structures.%2520Although%2520large%2520vision-language%250Amodels%2520%2528LVLMs%2529%2520have%2520demonstrated%2520impressive%2520multimodal%2520capabilities%2520across%250Anatural%2520and%2520medical%2520domains%252C%2520their%2520performance%2520on%2520ultrasound%2520remains%2520largely%250Aunexplored.%2520We%2520introduce%2520U2-BENCH%252C%2520the%2520first%2520comprehensive%2520benchmark%2520to%250Aevaluate%2520LVLMs%2520on%2520ultrasound%2520understanding%2520across%2520classification%252C%2520detection%252C%250Aregression%252C%2520and%2520text%2520generation%2520tasks.%2520U2-BENCH%2520aggregates%25207%252C241%2520cases%2520spanning%250A15%2520anatomical%2520regions%2520and%2520defines%25208%2520clinically%2520inspired%2520tasks%252C%2520such%2520as%250Adiagnosis%252C%2520view%2520recognition%252C%2520lesion%2520localization%252C%2520clinical%2520value%2520estimation%252C%250Aand%2520report%2520generation%252C%2520across%252050%2520ultrasound%2520application%2520scenarios.%2520We%2520evaluate%250A20%2520state-of-the-art%2520LVLMs%252C%2520both%2520open-%2520and%2520closed-source%252C%2520general-purpose%2520and%250Amedical-specific.%2520Our%2520results%2520reveal%2520strong%2520performance%2520on%2520image-level%250Aclassification%252C%2520but%2520persistent%2520challenges%2520in%2520spatial%2520reasoning%2520and%2520clinical%250Alanguage%2520generation.%2520U2-BENCH%2520establishes%2520a%2520rigorous%2520and%2520unified%2520testbed%2520to%250Aassess%2520and%2520accelerate%2520LVLM%2520research%2520in%2520the%2520uniquely%2520multimodal%2520domain%2520of%250Amedical%2520ultrasound%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=U2-BENCH%3A%20Benchmarking%20Large%20Vision-Language%20Models%20on%20Ultrasound%0A%20%20Understanding&entry.906535625=Anjie%20Le%20and%20Henan%20Liu%20and%20Yue%20Wang%20and%20Zhenyu%20Liu%20and%20Rongkun%20Zhu%20and%20Taohan%20Weng%20and%20Jinze%20Yu%20and%20Boyang%20Wang%20and%20Yalun%20Wu%20and%20Kaiwen%20Yan%20and%20Quanlin%20Sun%20and%20Meirui%20Jiang%20and%20Jialun%20Pei%20and%20Siya%20Liu%20and%20Haoyun%20Zheng%20and%20Zhoujun%20Li%20and%20Alison%20Noble%20and%20Jacques%20Souquet%20and%20Xiaoqing%20Guo%20and%20Manxi%20Lin%20and%20Hongcheng%20Guo&entry.1292438233=%20%20Ultrasound%20is%20a%20widely-used%20imaging%20modality%20critical%20to%20global%20healthcare%2C%0Ayet%20its%20interpretation%20remains%20challenging%20due%20to%20its%20varying%20image%20quality%20on%0Aoperators%2C%20noises%2C%20and%20anatomical%20structures.%20Although%20large%20vision-language%0Amodels%20%28LVLMs%29%20have%20demonstrated%20impressive%20multimodal%20capabilities%20across%0Anatural%20and%20medical%20domains%2C%20their%20performance%20on%20ultrasound%20remains%20largely%0Aunexplored.%20We%20introduce%20U2-BENCH%2C%20the%20first%20comprehensive%20benchmark%20to%0Aevaluate%20LVLMs%20on%20ultrasound%20understanding%20across%20classification%2C%20detection%2C%0Aregression%2C%20and%20text%20generation%20tasks.%20U2-BENCH%20aggregates%207%2C241%20cases%20spanning%0A15%20anatomical%20regions%20and%20defines%208%20clinically%20inspired%20tasks%2C%20such%20as%0Adiagnosis%2C%20view%20recognition%2C%20lesion%20localization%2C%20clinical%20value%20estimation%2C%0Aand%20report%20generation%2C%20across%2050%20ultrasound%20application%20scenarios.%20We%20evaluate%0A20%20state-of-the-art%20LVLMs%2C%20both%20open-%20and%20closed-source%2C%20general-purpose%20and%0Amedical-specific.%20Our%20results%20reveal%20strong%20performance%20on%20image-level%0Aclassification%2C%20but%20persistent%20challenges%20in%20spatial%20reasoning%20and%20clinical%0Alanguage%20generation.%20U2-BENCH%20establishes%20a%20rigorous%20and%20unified%20testbed%20to%0Aassess%20and%20accelerate%20LVLM%20research%20in%20the%20uniquely%20multimodal%20domain%20of%0Amedical%20ultrasound%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17779v1&entry.124074799=Read"},
{"title": "Generative Data Augmentation for Object Point Cloud Segmentation", "author": "Dekai Zhu and Stefan Gavranovic and Flavien Boussuge and Benjamin Busam and Slobodan Ilic", "abstract": "  Data augmentation is widely used to train deep learning models to address\ndata scarcity. However, traditional data augmentation (TDA) typically relies on\nsimple geometric transformation, such as random rotation and rescaling,\nresulting in minimal data diversity enrichment and limited model performance\nimprovement. State-of-the-art generative models for 3D shape generation rely on\nthe denoising diffusion probabilistic models and manage to generate realistic\nnovel point clouds for 3D content creation and manipulation. Nevertheless, the\ngenerated 3D shapes lack associated point-wise semantic labels, restricting\ntheir usage in enlarging the training data for point cloud segmentation tasks.\nTo bridge the gap between data augmentation techniques and the advanced\ndiffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to a\npart-aware generative model that can generate high-quality point clouds\nconditioned on given segmentation masks. Leveraging the novel generative model,\nwe introduce a 3-step generative data augmentation (GDA) pipeline for point\ncloud segmentation training. Our GDA approach requires only a small amount of\nlabeled samples but enriches the training data with generated variants and\npseudo-labeled samples, which are validated by a novel diffusion-based\npseudo-label filtering method. Extensive experiments on two large-scale\nsynthetic datasets and a real-world medical dataset demonstrate that our GDA\nmethod outperforms TDA approach and related semi-supervised and self-supervised\nmethods.\n", "link": "http://arxiv.org/abs/2505.17783v1", "date": "2025-05-23", "relevancy": 2.3852, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.634}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5889}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Data%20Augmentation%20for%20Object%20Point%20Cloud%20Segmentation&body=Title%3A%20Generative%20Data%20Augmentation%20for%20Object%20Point%20Cloud%20Segmentation%0AAuthor%3A%20Dekai%20Zhu%20and%20Stefan%20Gavranovic%20and%20Flavien%20Boussuge%20and%20Benjamin%20Busam%20and%20Slobodan%20Ilic%0AAbstract%3A%20%20%20Data%20augmentation%20is%20widely%20used%20to%20train%20deep%20learning%20models%20to%20address%0Adata%20scarcity.%20However%2C%20traditional%20data%20augmentation%20%28TDA%29%20typically%20relies%20on%0Asimple%20geometric%20transformation%2C%20such%20as%20random%20rotation%20and%20rescaling%2C%0Aresulting%20in%20minimal%20data%20diversity%20enrichment%20and%20limited%20model%20performance%0Aimprovement.%20State-of-the-art%20generative%20models%20for%203D%20shape%20generation%20rely%20on%0Athe%20denoising%20diffusion%20probabilistic%20models%20and%20manage%20to%20generate%20realistic%0Anovel%20point%20clouds%20for%203D%20content%20creation%20and%20manipulation.%20Nevertheless%2C%20the%0Agenerated%203D%20shapes%20lack%20associated%20point-wise%20semantic%20labels%2C%20restricting%0Atheir%20usage%20in%20enlarging%20the%20training%20data%20for%20point%20cloud%20segmentation%20tasks.%0ATo%20bridge%20the%20gap%20between%20data%20augmentation%20techniques%20and%20the%20advanced%0Adiffusion%20models%2C%20we%20extend%20the%20state-of-the-art%203D%20diffusion%20model%2C%20Lion%2C%20to%20a%0Apart-aware%20generative%20model%20that%20can%20generate%20high-quality%20point%20clouds%0Aconditioned%20on%20given%20segmentation%20masks.%20Leveraging%20the%20novel%20generative%20model%2C%0Awe%20introduce%20a%203-step%20generative%20data%20augmentation%20%28GDA%29%20pipeline%20for%20point%0Acloud%20segmentation%20training.%20Our%20GDA%20approach%20requires%20only%20a%20small%20amount%20of%0Alabeled%20samples%20but%20enriches%20the%20training%20data%20with%20generated%20variants%20and%0Apseudo-labeled%20samples%2C%20which%20are%20validated%20by%20a%20novel%20diffusion-based%0Apseudo-label%20filtering%20method.%20Extensive%20experiments%20on%20two%20large-scale%0Asynthetic%20datasets%20and%20a%20real-world%20medical%20dataset%20demonstrate%20that%20our%20GDA%0Amethod%20outperforms%20TDA%20approach%20and%20related%20semi-supervised%20and%20self-supervised%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Data%2520Augmentation%2520for%2520Object%2520Point%2520Cloud%2520Segmentation%26entry.906535625%3DDekai%2520Zhu%2520and%2520Stefan%2520Gavranovic%2520and%2520Flavien%2520Boussuge%2520and%2520Benjamin%2520Busam%2520and%2520Slobodan%2520Ilic%26entry.1292438233%3D%2520%2520Data%2520augmentation%2520is%2520widely%2520used%2520to%2520train%2520deep%2520learning%2520models%2520to%2520address%250Adata%2520scarcity.%2520However%252C%2520traditional%2520data%2520augmentation%2520%2528TDA%2529%2520typically%2520relies%2520on%250Asimple%2520geometric%2520transformation%252C%2520such%2520as%2520random%2520rotation%2520and%2520rescaling%252C%250Aresulting%2520in%2520minimal%2520data%2520diversity%2520enrichment%2520and%2520limited%2520model%2520performance%250Aimprovement.%2520State-of-the-art%2520generative%2520models%2520for%25203D%2520shape%2520generation%2520rely%2520on%250Athe%2520denoising%2520diffusion%2520probabilistic%2520models%2520and%2520manage%2520to%2520generate%2520realistic%250Anovel%2520point%2520clouds%2520for%25203D%2520content%2520creation%2520and%2520manipulation.%2520Nevertheless%252C%2520the%250Agenerated%25203D%2520shapes%2520lack%2520associated%2520point-wise%2520semantic%2520labels%252C%2520restricting%250Atheir%2520usage%2520in%2520enlarging%2520the%2520training%2520data%2520for%2520point%2520cloud%2520segmentation%2520tasks.%250ATo%2520bridge%2520the%2520gap%2520between%2520data%2520augmentation%2520techniques%2520and%2520the%2520advanced%250Adiffusion%2520models%252C%2520we%2520extend%2520the%2520state-of-the-art%25203D%2520diffusion%2520model%252C%2520Lion%252C%2520to%2520a%250Apart-aware%2520generative%2520model%2520that%2520can%2520generate%2520high-quality%2520point%2520clouds%250Aconditioned%2520on%2520given%2520segmentation%2520masks.%2520Leveraging%2520the%2520novel%2520generative%2520model%252C%250Awe%2520introduce%2520a%25203-step%2520generative%2520data%2520augmentation%2520%2528GDA%2529%2520pipeline%2520for%2520point%250Acloud%2520segmentation%2520training.%2520Our%2520GDA%2520approach%2520requires%2520only%2520a%2520small%2520amount%2520of%250Alabeled%2520samples%2520but%2520enriches%2520the%2520training%2520data%2520with%2520generated%2520variants%2520and%250Apseudo-labeled%2520samples%252C%2520which%2520are%2520validated%2520by%2520a%2520novel%2520diffusion-based%250Apseudo-label%2520filtering%2520method.%2520Extensive%2520experiments%2520on%2520two%2520large-scale%250Asynthetic%2520datasets%2520and%2520a%2520real-world%2520medical%2520dataset%2520demonstrate%2520that%2520our%2520GDA%250Amethod%2520outperforms%2520TDA%2520approach%2520and%2520related%2520semi-supervised%2520and%2520self-supervised%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Data%20Augmentation%20for%20Object%20Point%20Cloud%20Segmentation&entry.906535625=Dekai%20Zhu%20and%20Stefan%20Gavranovic%20and%20Flavien%20Boussuge%20and%20Benjamin%20Busam%20and%20Slobodan%20Ilic&entry.1292438233=%20%20Data%20augmentation%20is%20widely%20used%20to%20train%20deep%20learning%20models%20to%20address%0Adata%20scarcity.%20However%2C%20traditional%20data%20augmentation%20%28TDA%29%20typically%20relies%20on%0Asimple%20geometric%20transformation%2C%20such%20as%20random%20rotation%20and%20rescaling%2C%0Aresulting%20in%20minimal%20data%20diversity%20enrichment%20and%20limited%20model%20performance%0Aimprovement.%20State-of-the-art%20generative%20models%20for%203D%20shape%20generation%20rely%20on%0Athe%20denoising%20diffusion%20probabilistic%20models%20and%20manage%20to%20generate%20realistic%0Anovel%20point%20clouds%20for%203D%20content%20creation%20and%20manipulation.%20Nevertheless%2C%20the%0Agenerated%203D%20shapes%20lack%20associated%20point-wise%20semantic%20labels%2C%20restricting%0Atheir%20usage%20in%20enlarging%20the%20training%20data%20for%20point%20cloud%20segmentation%20tasks.%0ATo%20bridge%20the%20gap%20between%20data%20augmentation%20techniques%20and%20the%20advanced%0Adiffusion%20models%2C%20we%20extend%20the%20state-of-the-art%203D%20diffusion%20model%2C%20Lion%2C%20to%20a%0Apart-aware%20generative%20model%20that%20can%20generate%20high-quality%20point%20clouds%0Aconditioned%20on%20given%20segmentation%20masks.%20Leveraging%20the%20novel%20generative%20model%2C%0Awe%20introduce%20a%203-step%20generative%20data%20augmentation%20%28GDA%29%20pipeline%20for%20point%0Acloud%20segmentation%20training.%20Our%20GDA%20approach%20requires%20only%20a%20small%20amount%20of%0Alabeled%20samples%20but%20enriches%20the%20training%20data%20with%20generated%20variants%20and%0Apseudo-labeled%20samples%2C%20which%20are%20validated%20by%20a%20novel%20diffusion-based%0Apseudo-label%20filtering%20method.%20Extensive%20experiments%20on%20two%20large-scale%0Asynthetic%20datasets%20and%20a%20real-world%20medical%20dataset%20demonstrate%20that%20our%20GDA%0Amethod%20outperforms%20TDA%20approach%20and%20related%20semi-supervised%20and%20self-supervised%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17783v1&entry.124074799=Read"},
{"title": "Task Arithmetic for Language Expansion in Speech Translation", "author": "Yao-Fei Cheng and Hayato Futami and Yosuke Kashiwagi and Emiru Tsunoo and Wen Shen Teo and Siddhant Arora and Shinji Watanabe", "abstract": "  Recent progress in large language models (LLMs) has gained interest in\nspeech-text multimodal foundation models, achieving strong performance on\ninstruction-tuned speech translation (ST). However, expanding language pairs is\ncostly due to re-training on combined new and previous datasets. To address\nthis, we aim to build a one-to-many ST system from existing one-to-one ST\nsystems using task arithmetic without re-training. Direct application of task\narithmetic in ST leads to language confusion; therefore, we introduce an\naugmented task arithmetic method incorporating a language control model to\nensure correct target language generation. Our experiments on MuST-C and\nCoVoST-2 show BLEU score improvements of up to 4.66 and 4.92, with COMET gains\nof 8.87 and 11.83. In addition, we demonstrate our framework can extend to\nlanguage pairs lacking paired ST training data or pre-trained ST models by\nsynthesizing ST models based on existing machine translation (MT) and ST models\nvia task analogies.\n", "link": "http://arxiv.org/abs/2409.11274v2", "date": "2025-05-23", "relevancy": 2.3842, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4906}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4699}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task%20Arithmetic%20for%20Language%20Expansion%20in%20Speech%20Translation&body=Title%3A%20Task%20Arithmetic%20for%20Language%20Expansion%20in%20Speech%20Translation%0AAuthor%3A%20Yao-Fei%20Cheng%20and%20Hayato%20Futami%20and%20Yosuke%20Kashiwagi%20and%20Emiru%20Tsunoo%20and%20Wen%20Shen%20Teo%20and%20Siddhant%20Arora%20and%20Shinji%20Watanabe%0AAbstract%3A%20%20%20Recent%20progress%20in%20large%20language%20models%20%28LLMs%29%20has%20gained%20interest%20in%0Aspeech-text%20multimodal%20foundation%20models%2C%20achieving%20strong%20performance%20on%0Ainstruction-tuned%20speech%20translation%20%28ST%29.%20However%2C%20expanding%20language%20pairs%20is%0Acostly%20due%20to%20re-training%20on%20combined%20new%20and%20previous%20datasets.%20To%20address%0Athis%2C%20we%20aim%20to%20build%20a%20one-to-many%20ST%20system%20from%20existing%20one-to-one%20ST%0Asystems%20using%20task%20arithmetic%20without%20re-training.%20Direct%20application%20of%20task%0Aarithmetic%20in%20ST%20leads%20to%20language%20confusion%3B%20therefore%2C%20we%20introduce%20an%0Aaugmented%20task%20arithmetic%20method%20incorporating%20a%20language%20control%20model%20to%0Aensure%20correct%20target%20language%20generation.%20Our%20experiments%20on%20MuST-C%20and%0ACoVoST-2%20show%20BLEU%20score%20improvements%20of%20up%20to%204.66%20and%204.92%2C%20with%20COMET%20gains%0Aof%208.87%20and%2011.83.%20In%20addition%2C%20we%20demonstrate%20our%20framework%20can%20extend%20to%0Alanguage%20pairs%20lacking%20paired%20ST%20training%20data%20or%20pre-trained%20ST%20models%20by%0Asynthesizing%20ST%20models%20based%20on%20existing%20machine%20translation%20%28MT%29%20and%20ST%20models%0Avia%20task%20analogies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11274v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask%2520Arithmetic%2520for%2520Language%2520Expansion%2520in%2520Speech%2520Translation%26entry.906535625%3DYao-Fei%2520Cheng%2520and%2520Hayato%2520Futami%2520and%2520Yosuke%2520Kashiwagi%2520and%2520Emiru%2520Tsunoo%2520and%2520Wen%2520Shen%2520Teo%2520and%2520Siddhant%2520Arora%2520and%2520Shinji%2520Watanabe%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520gained%2520interest%2520in%250Aspeech-text%2520multimodal%2520foundation%2520models%252C%2520achieving%2520strong%2520performance%2520on%250Ainstruction-tuned%2520speech%2520translation%2520%2528ST%2529.%2520However%252C%2520expanding%2520language%2520pairs%2520is%250Acostly%2520due%2520to%2520re-training%2520on%2520combined%2520new%2520and%2520previous%2520datasets.%2520To%2520address%250Athis%252C%2520we%2520aim%2520to%2520build%2520a%2520one-to-many%2520ST%2520system%2520from%2520existing%2520one-to-one%2520ST%250Asystems%2520using%2520task%2520arithmetic%2520without%2520re-training.%2520Direct%2520application%2520of%2520task%250Aarithmetic%2520in%2520ST%2520leads%2520to%2520language%2520confusion%253B%2520therefore%252C%2520we%2520introduce%2520an%250Aaugmented%2520task%2520arithmetic%2520method%2520incorporating%2520a%2520language%2520control%2520model%2520to%250Aensure%2520correct%2520target%2520language%2520generation.%2520Our%2520experiments%2520on%2520MuST-C%2520and%250ACoVoST-2%2520show%2520BLEU%2520score%2520improvements%2520of%2520up%2520to%25204.66%2520and%25204.92%252C%2520with%2520COMET%2520gains%250Aof%25208.87%2520and%252011.83.%2520In%2520addition%252C%2520we%2520demonstrate%2520our%2520framework%2520can%2520extend%2520to%250Alanguage%2520pairs%2520lacking%2520paired%2520ST%2520training%2520data%2520or%2520pre-trained%2520ST%2520models%2520by%250Asynthesizing%2520ST%2520models%2520based%2520on%2520existing%2520machine%2520translation%2520%2528MT%2529%2520and%2520ST%2520models%250Avia%2520task%2520analogies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11274v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task%20Arithmetic%20for%20Language%20Expansion%20in%20Speech%20Translation&entry.906535625=Yao-Fei%20Cheng%20and%20Hayato%20Futami%20and%20Yosuke%20Kashiwagi%20and%20Emiru%20Tsunoo%20and%20Wen%20Shen%20Teo%20and%20Siddhant%20Arora%20and%20Shinji%20Watanabe&entry.1292438233=%20%20Recent%20progress%20in%20large%20language%20models%20%28LLMs%29%20has%20gained%20interest%20in%0Aspeech-text%20multimodal%20foundation%20models%2C%20achieving%20strong%20performance%20on%0Ainstruction-tuned%20speech%20translation%20%28ST%29.%20However%2C%20expanding%20language%20pairs%20is%0Acostly%20due%20to%20re-training%20on%20combined%20new%20and%20previous%20datasets.%20To%20address%0Athis%2C%20we%20aim%20to%20build%20a%20one-to-many%20ST%20system%20from%20existing%20one-to-one%20ST%0Asystems%20using%20task%20arithmetic%20without%20re-training.%20Direct%20application%20of%20task%0Aarithmetic%20in%20ST%20leads%20to%20language%20confusion%3B%20therefore%2C%20we%20introduce%20an%0Aaugmented%20task%20arithmetic%20method%20incorporating%20a%20language%20control%20model%20to%0Aensure%20correct%20target%20language%20generation.%20Our%20experiments%20on%20MuST-C%20and%0ACoVoST-2%20show%20BLEU%20score%20improvements%20of%20up%20to%204.66%20and%204.92%2C%20with%20COMET%20gains%0Aof%208.87%20and%2011.83.%20In%20addition%2C%20we%20demonstrate%20our%20framework%20can%20extend%20to%0Alanguage%20pairs%20lacking%20paired%20ST%20training%20data%20or%20pre-trained%20ST%20models%20by%0Asynthesizing%20ST%20models%20based%20on%20existing%20machine%20translation%20%28MT%29%20and%20ST%20models%0Avia%20task%20analogies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11274v2&entry.124074799=Read"},
{"title": "RBench-V: A Primary Assessment for Visual Reasoning Models with\n  Multi-modal Outputs", "author": "Meng-Hao Guo and Xuanyu Chu and Qianrui Yang and Zhe-Han Mo and Yiqing Shen and Pei-lin Li and Xinjie Lin and Jinnian Zhang and Xin-Sheng Chen and Yi Zhang and Kiyohiro Nakayama and Zhengyang Geng and Houwen Peng and Han Hu and Shi-Min Hu", "abstract": "  The rapid advancement of native multi-modal models and omni-models,\nexemplified by GPT-4o, Gemini, and o3, with their capability to process and\ngenerate content across modalities such as text and images, marks a significant\nmilestone in the evolution of intelligence. Systematic evaluation of their\nmulti-modal output capabilities in visual thinking processes (also known as\nmulti-modal chain of thought, M-CoT) becomes critically important. However,\nexisting benchmarks for evaluating multi-modal models primarily focus on\nassessing multi-modal inputs and text-only reasoning while neglecting the\nimportance of reasoning through multi-modal outputs. In this paper, we present\na benchmark, dubbed RBench-V, designed to assess models' vision-indispensable\nreasoning abilities. To construct RBench-V, we carefully hand-pick 803\nquestions covering math, physics, counting, and games. Unlike previous\nbenchmarks that typically specify certain input modalities, RBench-V presents\nproblems centered on multi-modal outputs, which require image manipulation such\nas generating novel images and constructing auxiliary lines to support the\nreasoning process. We evaluate numerous open- and closed-source models on\nRBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the\nbest-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below\nthe human score of 82.3%, highlighting that current models struggle to leverage\nmulti-modal reasoning. Data and code are available at\nhttps://evalmodels.github.io/rbenchv\n", "link": "http://arxiv.org/abs/2505.16770v2", "date": "2025-05-23", "relevancy": 2.3678, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5934}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RBench-V%3A%20A%20Primary%20Assessment%20for%20Visual%20Reasoning%20Models%20with%0A%20%20Multi-modal%20Outputs&body=Title%3A%20RBench-V%3A%20A%20Primary%20Assessment%20for%20Visual%20Reasoning%20Models%20with%0A%20%20Multi-modal%20Outputs%0AAuthor%3A%20Meng-Hao%20Guo%20and%20Xuanyu%20Chu%20and%20Qianrui%20Yang%20and%20Zhe-Han%20Mo%20and%20Yiqing%20Shen%20and%20Pei-lin%20Li%20and%20Xinjie%20Lin%20and%20Jinnian%20Zhang%20and%20Xin-Sheng%20Chen%20and%20Yi%20Zhang%20and%20Kiyohiro%20Nakayama%20and%20Zhengyang%20Geng%20and%20Houwen%20Peng%20and%20Han%20Hu%20and%20Shi-Min%20Hu%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20native%20multi-modal%20models%20and%20omni-models%2C%0Aexemplified%20by%20GPT-4o%2C%20Gemini%2C%20and%20o3%2C%20with%20their%20capability%20to%20process%20and%0Agenerate%20content%20across%20modalities%20such%20as%20text%20and%20images%2C%20marks%20a%20significant%0Amilestone%20in%20the%20evolution%20of%20intelligence.%20Systematic%20evaluation%20of%20their%0Amulti-modal%20output%20capabilities%20in%20visual%20thinking%20processes%20%28also%20known%20as%0Amulti-modal%20chain%20of%20thought%2C%20M-CoT%29%20becomes%20critically%20important.%20However%2C%0Aexisting%20benchmarks%20for%20evaluating%20multi-modal%20models%20primarily%20focus%20on%0Aassessing%20multi-modal%20inputs%20and%20text-only%20reasoning%20while%20neglecting%20the%0Aimportance%20of%20reasoning%20through%20multi-modal%20outputs.%20In%20this%20paper%2C%20we%20present%0Aa%20benchmark%2C%20dubbed%20RBench-V%2C%20designed%20to%20assess%20models%27%20vision-indispensable%0Areasoning%20abilities.%20To%20construct%20RBench-V%2C%20we%20carefully%20hand-pick%20803%0Aquestions%20covering%20math%2C%20physics%2C%20counting%2C%20and%20games.%20Unlike%20previous%0Abenchmarks%20that%20typically%20specify%20certain%20input%20modalities%2C%20RBench-V%20presents%0Aproblems%20centered%20on%20multi-modal%20outputs%2C%20which%20require%20image%20manipulation%20such%0Aas%20generating%20novel%20images%20and%20constructing%20auxiliary%20lines%20to%20support%20the%0Areasoning%20process.%20We%20evaluate%20numerous%20open-%20and%20closed-source%20models%20on%0ARBench-V%2C%20including%20o3%2C%20Gemini%202.5%20Pro%2C%20Qwen2.5-VL%2C%20etc.%20Even%20the%0Abest-performing%20model%2C%20o3%2C%20achieves%20only%2025.8%25%20accuracy%20on%20RBench-V%2C%20far%20below%0Athe%20human%20score%20of%2082.3%25%2C%20highlighting%20that%20current%20models%20struggle%20to%20leverage%0Amulti-modal%20reasoning.%20Data%20and%20code%20are%20available%20at%0Ahttps%3A//evalmodels.github.io/rbenchv%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16770v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRBench-V%253A%2520A%2520Primary%2520Assessment%2520for%2520Visual%2520Reasoning%2520Models%2520with%250A%2520%2520Multi-modal%2520Outputs%26entry.906535625%3DMeng-Hao%2520Guo%2520and%2520Xuanyu%2520Chu%2520and%2520Qianrui%2520Yang%2520and%2520Zhe-Han%2520Mo%2520and%2520Yiqing%2520Shen%2520and%2520Pei-lin%2520Li%2520and%2520Xinjie%2520Lin%2520and%2520Jinnian%2520Zhang%2520and%2520Xin-Sheng%2520Chen%2520and%2520Yi%2520Zhang%2520and%2520Kiyohiro%2520Nakayama%2520and%2520Zhengyang%2520Geng%2520and%2520Houwen%2520Peng%2520and%2520Han%2520Hu%2520and%2520Shi-Min%2520Hu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520native%2520multi-modal%2520models%2520and%2520omni-models%252C%250Aexemplified%2520by%2520GPT-4o%252C%2520Gemini%252C%2520and%2520o3%252C%2520with%2520their%2520capability%2520to%2520process%2520and%250Agenerate%2520content%2520across%2520modalities%2520such%2520as%2520text%2520and%2520images%252C%2520marks%2520a%2520significant%250Amilestone%2520in%2520the%2520evolution%2520of%2520intelligence.%2520Systematic%2520evaluation%2520of%2520their%250Amulti-modal%2520output%2520capabilities%2520in%2520visual%2520thinking%2520processes%2520%2528also%2520known%2520as%250Amulti-modal%2520chain%2520of%2520thought%252C%2520M-CoT%2529%2520becomes%2520critically%2520important.%2520However%252C%250Aexisting%2520benchmarks%2520for%2520evaluating%2520multi-modal%2520models%2520primarily%2520focus%2520on%250Aassessing%2520multi-modal%2520inputs%2520and%2520text-only%2520reasoning%2520while%2520neglecting%2520the%250Aimportance%2520of%2520reasoning%2520through%2520multi-modal%2520outputs.%2520In%2520this%2520paper%252C%2520we%2520present%250Aa%2520benchmark%252C%2520dubbed%2520RBench-V%252C%2520designed%2520to%2520assess%2520models%2527%2520vision-indispensable%250Areasoning%2520abilities.%2520To%2520construct%2520RBench-V%252C%2520we%2520carefully%2520hand-pick%2520803%250Aquestions%2520covering%2520math%252C%2520physics%252C%2520counting%252C%2520and%2520games.%2520Unlike%2520previous%250Abenchmarks%2520that%2520typically%2520specify%2520certain%2520input%2520modalities%252C%2520RBench-V%2520presents%250Aproblems%2520centered%2520on%2520multi-modal%2520outputs%252C%2520which%2520require%2520image%2520manipulation%2520such%250Aas%2520generating%2520novel%2520images%2520and%2520constructing%2520auxiliary%2520lines%2520to%2520support%2520the%250Areasoning%2520process.%2520We%2520evaluate%2520numerous%2520open-%2520and%2520closed-source%2520models%2520on%250ARBench-V%252C%2520including%2520o3%252C%2520Gemini%25202.5%2520Pro%252C%2520Qwen2.5-VL%252C%2520etc.%2520Even%2520the%250Abest-performing%2520model%252C%2520o3%252C%2520achieves%2520only%252025.8%2525%2520accuracy%2520on%2520RBench-V%252C%2520far%2520below%250Athe%2520human%2520score%2520of%252082.3%2525%252C%2520highlighting%2520that%2520current%2520models%2520struggle%2520to%2520leverage%250Amulti-modal%2520reasoning.%2520Data%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//evalmodels.github.io/rbenchv%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16770v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RBench-V%3A%20A%20Primary%20Assessment%20for%20Visual%20Reasoning%20Models%20with%0A%20%20Multi-modal%20Outputs&entry.906535625=Meng-Hao%20Guo%20and%20Xuanyu%20Chu%20and%20Qianrui%20Yang%20and%20Zhe-Han%20Mo%20and%20Yiqing%20Shen%20and%20Pei-lin%20Li%20and%20Xinjie%20Lin%20and%20Jinnian%20Zhang%20and%20Xin-Sheng%20Chen%20and%20Yi%20Zhang%20and%20Kiyohiro%20Nakayama%20and%20Zhengyang%20Geng%20and%20Houwen%20Peng%20and%20Han%20Hu%20and%20Shi-Min%20Hu&entry.1292438233=%20%20The%20rapid%20advancement%20of%20native%20multi-modal%20models%20and%20omni-models%2C%0Aexemplified%20by%20GPT-4o%2C%20Gemini%2C%20and%20o3%2C%20with%20their%20capability%20to%20process%20and%0Agenerate%20content%20across%20modalities%20such%20as%20text%20and%20images%2C%20marks%20a%20significant%0Amilestone%20in%20the%20evolution%20of%20intelligence.%20Systematic%20evaluation%20of%20their%0Amulti-modal%20output%20capabilities%20in%20visual%20thinking%20processes%20%28also%20known%20as%0Amulti-modal%20chain%20of%20thought%2C%20M-CoT%29%20becomes%20critically%20important.%20However%2C%0Aexisting%20benchmarks%20for%20evaluating%20multi-modal%20models%20primarily%20focus%20on%0Aassessing%20multi-modal%20inputs%20and%20text-only%20reasoning%20while%20neglecting%20the%0Aimportance%20of%20reasoning%20through%20multi-modal%20outputs.%20In%20this%20paper%2C%20we%20present%0Aa%20benchmark%2C%20dubbed%20RBench-V%2C%20designed%20to%20assess%20models%27%20vision-indispensable%0Areasoning%20abilities.%20To%20construct%20RBench-V%2C%20we%20carefully%20hand-pick%20803%0Aquestions%20covering%20math%2C%20physics%2C%20counting%2C%20and%20games.%20Unlike%20previous%0Abenchmarks%20that%20typically%20specify%20certain%20input%20modalities%2C%20RBench-V%20presents%0Aproblems%20centered%20on%20multi-modal%20outputs%2C%20which%20require%20image%20manipulation%20such%0Aas%20generating%20novel%20images%20and%20constructing%20auxiliary%20lines%20to%20support%20the%0Areasoning%20process.%20We%20evaluate%20numerous%20open-%20and%20closed-source%20models%20on%0ARBench-V%2C%20including%20o3%2C%20Gemini%202.5%20Pro%2C%20Qwen2.5-VL%2C%20etc.%20Even%20the%0Abest-performing%20model%2C%20o3%2C%20achieves%20only%2025.8%25%20accuracy%20on%20RBench-V%2C%20far%20below%0Athe%20human%20score%20of%2082.3%25%2C%20highlighting%20that%20current%20models%20struggle%20to%20leverage%0Amulti-modal%20reasoning.%20Data%20and%20code%20are%20available%20at%0Ahttps%3A//evalmodels.github.io/rbenchv%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16770v2&entry.124074799=Read"},
{"title": "TokBench: Evaluating Your Visual Tokenizer before Visual Generation", "author": "Junfeng Wu and Dongliang Luo and Weizhi Zhao and Zhihao Xie and Yuanhao Wang and Junyi Li and Xudong Xie and Yuliang Liu and Xiang Bai", "abstract": "  In this work, we reveal the limitations of visual tokenizers and VAEs in\npreserving fine-grained features, and propose a benchmark to evaluate\nreconstruction performance for two challenging visual contents: text and face.\nImage tokenization has significantly advanced visual generation and multimodal\nmodeling, particularly with autoregressive models due to the modeling\nsimplicity of discrete tokens. Autoregressive models typically rely on image\ntokenizers to compress images into discrete tokens for sequential prediction,\nwhereas diffusion models often operate on continuous latent space to reduce\ncomputational costs. However, both visual compression approaches inevitably\nlose visual information, thereby limiting the upper bound of visual generation\nquality. To evaluate how these compression losses affect text and faces, the\nmost human-sensitive visual elements, we first collect and curate a collection\nof text and faces images from existing datasets, ensuring clarity and\ndiversity. For text reconstruction, we employ OCR models to assess the\nrecognition accuracy of the reconstructed text, and then we measure feature\nsimilarity between original and reconstructed faces thereby quantifying faces\nreconstruction fidelity. Our method is highly lightweight, requiring just 2GB\nmemory and 4 minutes to complete evaluations. With our benchmark, we analyze\nthe reconstruction quality of text and faces at various scales across different\nimage tokenizers and VAEs. Our results demonstrate that modern visual\ntokenizers still struggle to preserve fine-grained features, particularly at\nsmaller scales. Furthermore, we extend this evaluation framework to the video,\nconducting a comprehensive analysis of video tokenizers. Additionally, we find\nthat traditional metrics fail to accurately reflect the reconstruction\nperformance for faces and text, while our proposed metrics serve as an\neffective complement.\n", "link": "http://arxiv.org/abs/2505.18142v1", "date": "2025-05-23", "relevancy": 2.3622, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6245}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5716}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokBench%3A%20Evaluating%20Your%20Visual%20Tokenizer%20before%20Visual%20Generation&body=Title%3A%20TokBench%3A%20Evaluating%20Your%20Visual%20Tokenizer%20before%20Visual%20Generation%0AAuthor%3A%20Junfeng%20Wu%20and%20Dongliang%20Luo%20and%20Weizhi%20Zhao%20and%20Zhihao%20Xie%20and%20Yuanhao%20Wang%20and%20Junyi%20Li%20and%20Xudong%20Xie%20and%20Yuliang%20Liu%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20reveal%20the%20limitations%20of%20visual%20tokenizers%20and%20VAEs%20in%0Apreserving%20fine-grained%20features%2C%20and%20propose%20a%20benchmark%20to%20evaluate%0Areconstruction%20performance%20for%20two%20challenging%20visual%20contents%3A%20text%20and%20face.%0AImage%20tokenization%20has%20significantly%20advanced%20visual%20generation%20and%20multimodal%0Amodeling%2C%20particularly%20with%20autoregressive%20models%20due%20to%20the%20modeling%0Asimplicity%20of%20discrete%20tokens.%20Autoregressive%20models%20typically%20rely%20on%20image%0Atokenizers%20to%20compress%20images%20into%20discrete%20tokens%20for%20sequential%20prediction%2C%0Awhereas%20diffusion%20models%20often%20operate%20on%20continuous%20latent%20space%20to%20reduce%0Acomputational%20costs.%20However%2C%20both%20visual%20compression%20approaches%20inevitably%0Alose%20visual%20information%2C%20thereby%20limiting%20the%20upper%20bound%20of%20visual%20generation%0Aquality.%20To%20evaluate%20how%20these%20compression%20losses%20affect%20text%20and%20faces%2C%20the%0Amost%20human-sensitive%20visual%20elements%2C%20we%20first%20collect%20and%20curate%20a%20collection%0Aof%20text%20and%20faces%20images%20from%20existing%20datasets%2C%20ensuring%20clarity%20and%0Adiversity.%20For%20text%20reconstruction%2C%20we%20employ%20OCR%20models%20to%20assess%20the%0Arecognition%20accuracy%20of%20the%20reconstructed%20text%2C%20and%20then%20we%20measure%20feature%0Asimilarity%20between%20original%20and%20reconstructed%20faces%20thereby%20quantifying%20faces%0Areconstruction%20fidelity.%20Our%20method%20is%20highly%20lightweight%2C%20requiring%20just%202GB%0Amemory%20and%204%20minutes%20to%20complete%20evaluations.%20With%20our%20benchmark%2C%20we%20analyze%0Athe%20reconstruction%20quality%20of%20text%20and%20faces%20at%20various%20scales%20across%20different%0Aimage%20tokenizers%20and%20VAEs.%20Our%20results%20demonstrate%20that%20modern%20visual%0Atokenizers%20still%20struggle%20to%20preserve%20fine-grained%20features%2C%20particularly%20at%0Asmaller%20scales.%20Furthermore%2C%20we%20extend%20this%20evaluation%20framework%20to%20the%20video%2C%0Aconducting%20a%20comprehensive%20analysis%20of%20video%20tokenizers.%20Additionally%2C%20we%20find%0Athat%20traditional%20metrics%20fail%20to%20accurately%20reflect%20the%20reconstruction%0Aperformance%20for%20faces%20and%20text%2C%20while%20our%20proposed%20metrics%20serve%20as%20an%0Aeffective%20complement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokBench%253A%2520Evaluating%2520Your%2520Visual%2520Tokenizer%2520before%2520Visual%2520Generation%26entry.906535625%3DJunfeng%2520Wu%2520and%2520Dongliang%2520Luo%2520and%2520Weizhi%2520Zhao%2520and%2520Zhihao%2520Xie%2520and%2520Yuanhao%2520Wang%2520and%2520Junyi%2520Li%2520and%2520Xudong%2520Xie%2520and%2520Yuliang%2520Liu%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520reveal%2520the%2520limitations%2520of%2520visual%2520tokenizers%2520and%2520VAEs%2520in%250Apreserving%2520fine-grained%2520features%252C%2520and%2520propose%2520a%2520benchmark%2520to%2520evaluate%250Areconstruction%2520performance%2520for%2520two%2520challenging%2520visual%2520contents%253A%2520text%2520and%2520face.%250AImage%2520tokenization%2520has%2520significantly%2520advanced%2520visual%2520generation%2520and%2520multimodal%250Amodeling%252C%2520particularly%2520with%2520autoregressive%2520models%2520due%2520to%2520the%2520modeling%250Asimplicity%2520of%2520discrete%2520tokens.%2520Autoregressive%2520models%2520typically%2520rely%2520on%2520image%250Atokenizers%2520to%2520compress%2520images%2520into%2520discrete%2520tokens%2520for%2520sequential%2520prediction%252C%250Awhereas%2520diffusion%2520models%2520often%2520operate%2520on%2520continuous%2520latent%2520space%2520to%2520reduce%250Acomputational%2520costs.%2520However%252C%2520both%2520visual%2520compression%2520approaches%2520inevitably%250Alose%2520visual%2520information%252C%2520thereby%2520limiting%2520the%2520upper%2520bound%2520of%2520visual%2520generation%250Aquality.%2520To%2520evaluate%2520how%2520these%2520compression%2520losses%2520affect%2520text%2520and%2520faces%252C%2520the%250Amost%2520human-sensitive%2520visual%2520elements%252C%2520we%2520first%2520collect%2520and%2520curate%2520a%2520collection%250Aof%2520text%2520and%2520faces%2520images%2520from%2520existing%2520datasets%252C%2520ensuring%2520clarity%2520and%250Adiversity.%2520For%2520text%2520reconstruction%252C%2520we%2520employ%2520OCR%2520models%2520to%2520assess%2520the%250Arecognition%2520accuracy%2520of%2520the%2520reconstructed%2520text%252C%2520and%2520then%2520we%2520measure%2520feature%250Asimilarity%2520between%2520original%2520and%2520reconstructed%2520faces%2520thereby%2520quantifying%2520faces%250Areconstruction%2520fidelity.%2520Our%2520method%2520is%2520highly%2520lightweight%252C%2520requiring%2520just%25202GB%250Amemory%2520and%25204%2520minutes%2520to%2520complete%2520evaluations.%2520With%2520our%2520benchmark%252C%2520we%2520analyze%250Athe%2520reconstruction%2520quality%2520of%2520text%2520and%2520faces%2520at%2520various%2520scales%2520across%2520different%250Aimage%2520tokenizers%2520and%2520VAEs.%2520Our%2520results%2520demonstrate%2520that%2520modern%2520visual%250Atokenizers%2520still%2520struggle%2520to%2520preserve%2520fine-grained%2520features%252C%2520particularly%2520at%250Asmaller%2520scales.%2520Furthermore%252C%2520we%2520extend%2520this%2520evaluation%2520framework%2520to%2520the%2520video%252C%250Aconducting%2520a%2520comprehensive%2520analysis%2520of%2520video%2520tokenizers.%2520Additionally%252C%2520we%2520find%250Athat%2520traditional%2520metrics%2520fail%2520to%2520accurately%2520reflect%2520the%2520reconstruction%250Aperformance%2520for%2520faces%2520and%2520text%252C%2520while%2520our%2520proposed%2520metrics%2520serve%2520as%2520an%250Aeffective%2520complement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokBench%3A%20Evaluating%20Your%20Visual%20Tokenizer%20before%20Visual%20Generation&entry.906535625=Junfeng%20Wu%20and%20Dongliang%20Luo%20and%20Weizhi%20Zhao%20and%20Zhihao%20Xie%20and%20Yuanhao%20Wang%20and%20Junyi%20Li%20and%20Xudong%20Xie%20and%20Yuliang%20Liu%20and%20Xiang%20Bai&entry.1292438233=%20%20In%20this%20work%2C%20we%20reveal%20the%20limitations%20of%20visual%20tokenizers%20and%20VAEs%20in%0Apreserving%20fine-grained%20features%2C%20and%20propose%20a%20benchmark%20to%20evaluate%0Areconstruction%20performance%20for%20two%20challenging%20visual%20contents%3A%20text%20and%20face.%0AImage%20tokenization%20has%20significantly%20advanced%20visual%20generation%20and%20multimodal%0Amodeling%2C%20particularly%20with%20autoregressive%20models%20due%20to%20the%20modeling%0Asimplicity%20of%20discrete%20tokens.%20Autoregressive%20models%20typically%20rely%20on%20image%0Atokenizers%20to%20compress%20images%20into%20discrete%20tokens%20for%20sequential%20prediction%2C%0Awhereas%20diffusion%20models%20often%20operate%20on%20continuous%20latent%20space%20to%20reduce%0Acomputational%20costs.%20However%2C%20both%20visual%20compression%20approaches%20inevitably%0Alose%20visual%20information%2C%20thereby%20limiting%20the%20upper%20bound%20of%20visual%20generation%0Aquality.%20To%20evaluate%20how%20these%20compression%20losses%20affect%20text%20and%20faces%2C%20the%0Amost%20human-sensitive%20visual%20elements%2C%20we%20first%20collect%20and%20curate%20a%20collection%0Aof%20text%20and%20faces%20images%20from%20existing%20datasets%2C%20ensuring%20clarity%20and%0Adiversity.%20For%20text%20reconstruction%2C%20we%20employ%20OCR%20models%20to%20assess%20the%0Arecognition%20accuracy%20of%20the%20reconstructed%20text%2C%20and%20then%20we%20measure%20feature%0Asimilarity%20between%20original%20and%20reconstructed%20faces%20thereby%20quantifying%20faces%0Areconstruction%20fidelity.%20Our%20method%20is%20highly%20lightweight%2C%20requiring%20just%202GB%0Amemory%20and%204%20minutes%20to%20complete%20evaluations.%20With%20our%20benchmark%2C%20we%20analyze%0Athe%20reconstruction%20quality%20of%20text%20and%20faces%20at%20various%20scales%20across%20different%0Aimage%20tokenizers%20and%20VAEs.%20Our%20results%20demonstrate%20that%20modern%20visual%0Atokenizers%20still%20struggle%20to%20preserve%20fine-grained%20features%2C%20particularly%20at%0Asmaller%20scales.%20Furthermore%2C%20we%20extend%20this%20evaluation%20framework%20to%20the%20video%2C%0Aconducting%20a%20comprehensive%20analysis%20of%20video%20tokenizers.%20Additionally%2C%20we%20find%0Athat%20traditional%20metrics%20fail%20to%20accurately%20reflect%20the%20reconstruction%0Aperformance%20for%20faces%20and%20text%2C%20while%20our%20proposed%20metrics%20serve%20as%20an%0Aeffective%20complement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18142v1&entry.124074799=Read"},
{"title": "A Coreset Selection of Coreset Selection Literature: Introduction and\n  Recent Advances", "author": "Brian B. Moser and Arundhati S. Shanbhag and Stanislav Frolov and Federico Raue and Joachim Folz and Andreas Dengel", "abstract": "  Coreset selection targets the challenge of finding a small, representative\nsubset of a large dataset that preserves essential patterns for effective\nmachine learning. Although several surveys have examined data reduction\nstrategies before, most focus narrowly on either classical geometry-based\nmethods or active learning techniques. In contrast, this survey presents a more\ncomprehensive view by unifying three major lines of coreset research, namely,\ntraining-free, training-oriented, and label-free approaches, into a single\ntaxonomy. We present subfields often overlooked by existing work, including\nsubmodular formulations, bilevel optimization, and recent progress in\npseudo-labeling for unlabeled datasets. Additionally, we examine how pruning\nstrategies influence generalization and neural scaling laws, offering new\ninsights that are absent from prior reviews. Finally, we compare these methods\nunder varying computational, robustness, and performance demands and highlight\nopen challenges, such as robustness, outlier filtering, and adapting coreset\nselection to foundation models, for future research.\n", "link": "http://arxiv.org/abs/2505.17799v1", "date": "2025-05-23", "relevancy": 2.3538, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Coreset%20Selection%20of%20Coreset%20Selection%20Literature%3A%20Introduction%20and%0A%20%20Recent%20Advances&body=Title%3A%20A%20Coreset%20Selection%20of%20Coreset%20Selection%20Literature%3A%20Introduction%20and%0A%20%20Recent%20Advances%0AAuthor%3A%20Brian%20B.%20Moser%20and%20Arundhati%20S.%20Shanbhag%20and%20Stanislav%20Frolov%20and%20Federico%20Raue%20and%20Joachim%20Folz%20and%20Andreas%20Dengel%0AAbstract%3A%20%20%20Coreset%20selection%20targets%20the%20challenge%20of%20finding%20a%20small%2C%20representative%0Asubset%20of%20a%20large%20dataset%20that%20preserves%20essential%20patterns%20for%20effective%0Amachine%20learning.%20Although%20several%20surveys%20have%20examined%20data%20reduction%0Astrategies%20before%2C%20most%20focus%20narrowly%20on%20either%20classical%20geometry-based%0Amethods%20or%20active%20learning%20techniques.%20In%20contrast%2C%20this%20survey%20presents%20a%20more%0Acomprehensive%20view%20by%20unifying%20three%20major%20lines%20of%20coreset%20research%2C%20namely%2C%0Atraining-free%2C%20training-oriented%2C%20and%20label-free%20approaches%2C%20into%20a%20single%0Ataxonomy.%20We%20present%20subfields%20often%20overlooked%20by%20existing%20work%2C%20including%0Asubmodular%20formulations%2C%20bilevel%20optimization%2C%20and%20recent%20progress%20in%0Apseudo-labeling%20for%20unlabeled%20datasets.%20Additionally%2C%20we%20examine%20how%20pruning%0Astrategies%20influence%20generalization%20and%20neural%20scaling%20laws%2C%20offering%20new%0Ainsights%20that%20are%20absent%20from%20prior%20reviews.%20Finally%2C%20we%20compare%20these%20methods%0Aunder%20varying%20computational%2C%20robustness%2C%20and%20performance%20demands%20and%20highlight%0Aopen%20challenges%2C%20such%20as%20robustness%2C%20outlier%20filtering%2C%20and%20adapting%20coreset%0Aselection%20to%20foundation%20models%2C%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Coreset%2520Selection%2520of%2520Coreset%2520Selection%2520Literature%253A%2520Introduction%2520and%250A%2520%2520Recent%2520Advances%26entry.906535625%3DBrian%2520B.%2520Moser%2520and%2520Arundhati%2520S.%2520Shanbhag%2520and%2520Stanislav%2520Frolov%2520and%2520Federico%2520Raue%2520and%2520Joachim%2520Folz%2520and%2520Andreas%2520Dengel%26entry.1292438233%3D%2520%2520Coreset%2520selection%2520targets%2520the%2520challenge%2520of%2520finding%2520a%2520small%252C%2520representative%250Asubset%2520of%2520a%2520large%2520dataset%2520that%2520preserves%2520essential%2520patterns%2520for%2520effective%250Amachine%2520learning.%2520Although%2520several%2520surveys%2520have%2520examined%2520data%2520reduction%250Astrategies%2520before%252C%2520most%2520focus%2520narrowly%2520on%2520either%2520classical%2520geometry-based%250Amethods%2520or%2520active%2520learning%2520techniques.%2520In%2520contrast%252C%2520this%2520survey%2520presents%2520a%2520more%250Acomprehensive%2520view%2520by%2520unifying%2520three%2520major%2520lines%2520of%2520coreset%2520research%252C%2520namely%252C%250Atraining-free%252C%2520training-oriented%252C%2520and%2520label-free%2520approaches%252C%2520into%2520a%2520single%250Ataxonomy.%2520We%2520present%2520subfields%2520often%2520overlooked%2520by%2520existing%2520work%252C%2520including%250Asubmodular%2520formulations%252C%2520bilevel%2520optimization%252C%2520and%2520recent%2520progress%2520in%250Apseudo-labeling%2520for%2520unlabeled%2520datasets.%2520Additionally%252C%2520we%2520examine%2520how%2520pruning%250Astrategies%2520influence%2520generalization%2520and%2520neural%2520scaling%2520laws%252C%2520offering%2520new%250Ainsights%2520that%2520are%2520absent%2520from%2520prior%2520reviews.%2520Finally%252C%2520we%2520compare%2520these%2520methods%250Aunder%2520varying%2520computational%252C%2520robustness%252C%2520and%2520performance%2520demands%2520and%2520highlight%250Aopen%2520challenges%252C%2520such%2520as%2520robustness%252C%2520outlier%2520filtering%252C%2520and%2520adapting%2520coreset%250Aselection%2520to%2520foundation%2520models%252C%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Coreset%20Selection%20of%20Coreset%20Selection%20Literature%3A%20Introduction%20and%0A%20%20Recent%20Advances&entry.906535625=Brian%20B.%20Moser%20and%20Arundhati%20S.%20Shanbhag%20and%20Stanislav%20Frolov%20and%20Federico%20Raue%20and%20Joachim%20Folz%20and%20Andreas%20Dengel&entry.1292438233=%20%20Coreset%20selection%20targets%20the%20challenge%20of%20finding%20a%20small%2C%20representative%0Asubset%20of%20a%20large%20dataset%20that%20preserves%20essential%20patterns%20for%20effective%0Amachine%20learning.%20Although%20several%20surveys%20have%20examined%20data%20reduction%0Astrategies%20before%2C%20most%20focus%20narrowly%20on%20either%20classical%20geometry-based%0Amethods%20or%20active%20learning%20techniques.%20In%20contrast%2C%20this%20survey%20presents%20a%20more%0Acomprehensive%20view%20by%20unifying%20three%20major%20lines%20of%20coreset%20research%2C%20namely%2C%0Atraining-free%2C%20training-oriented%2C%20and%20label-free%20approaches%2C%20into%20a%20single%0Ataxonomy.%20We%20present%20subfields%20often%20overlooked%20by%20existing%20work%2C%20including%0Asubmodular%20formulations%2C%20bilevel%20optimization%2C%20and%20recent%20progress%20in%0Apseudo-labeling%20for%20unlabeled%20datasets.%20Additionally%2C%20we%20examine%20how%20pruning%0Astrategies%20influence%20generalization%20and%20neural%20scaling%20laws%2C%20offering%20new%0Ainsights%20that%20are%20absent%20from%20prior%20reviews.%20Finally%2C%20we%20compare%20these%20methods%0Aunder%20varying%20computational%2C%20robustness%2C%20and%20performance%20demands%20and%20highlight%0Aopen%20challenges%2C%20such%20as%20robustness%2C%20outlier%20filtering%2C%20and%20adapting%20coreset%0Aselection%20to%20foundation%20models%2C%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17799v1&entry.124074799=Read"},
{"title": "ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for\n  Human Activity Modeling", "author": "Weihang You and Hanqi Jiang and Zishuai Liu and Zihang Xie and Tianming Liu and Jin Lu and Fei Dou", "abstract": "  Real world collection of Activities of Daily Living data is challenging due\nto privacy concerns, costly deployment and labeling, and the inherent sparsity\nand imbalance of human behavior. We present ADLGen, a generative framework\nspecifically designed to synthesize realistic, event triggered, and symbolic\nsensor sequences for ambient assistive environments. ADLGen integrates a\ndecoder only Transformer with sign based symbolic temporal encoding, and a\ncontext and layout aware sampling mechanism to guide generation toward\nsemantically rich and physically plausible sensor event sequences. To enhance\nsemantic fidelity and correct structural inconsistencies, we further\nincorporate a large language model into an automatic generate evaluate refine\nloop, which verifies logical, behavioral, and temporal coherence and generates\ncorrection rules without manual intervention or environment specific tuning.\nThrough comprehensive experiments with novel evaluation metrics, ADLGen is\nshown to outperform baseline generators in statistical fidelity, semantic\nrichness, and downstream activity recognition, offering a scalable and\nprivacy-preserving solution for ADL data synthesis.\n", "link": "http://arxiv.org/abs/2505.17987v1", "date": "2025-05-23", "relevancy": 2.3514, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6321}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5922}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ADLGen%3A%20Synthesizing%20Symbolic%2C%20Event-Triggered%20Sensor%20Sequences%20for%0A%20%20Human%20Activity%20Modeling&body=Title%3A%20ADLGen%3A%20Synthesizing%20Symbolic%2C%20Event-Triggered%20Sensor%20Sequences%20for%0A%20%20Human%20Activity%20Modeling%0AAuthor%3A%20Weihang%20You%20and%20Hanqi%20Jiang%20and%20Zishuai%20Liu%20and%20Zihang%20Xie%20and%20Tianming%20Liu%20and%20Jin%20Lu%20and%20Fei%20Dou%0AAbstract%3A%20%20%20Real%20world%20collection%20of%20Activities%20of%20Daily%20Living%20data%20is%20challenging%20due%0Ato%20privacy%20concerns%2C%20costly%20deployment%20and%20labeling%2C%20and%20the%20inherent%20sparsity%0Aand%20imbalance%20of%20human%20behavior.%20We%20present%20ADLGen%2C%20a%20generative%20framework%0Aspecifically%20designed%20to%20synthesize%20realistic%2C%20event%20triggered%2C%20and%20symbolic%0Asensor%20sequences%20for%20ambient%20assistive%20environments.%20ADLGen%20integrates%20a%0Adecoder%20only%20Transformer%20with%20sign%20based%20symbolic%20temporal%20encoding%2C%20and%20a%0Acontext%20and%20layout%20aware%20sampling%20mechanism%20to%20guide%20generation%20toward%0Asemantically%20rich%20and%20physically%20plausible%20sensor%20event%20sequences.%20To%20enhance%0Asemantic%20fidelity%20and%20correct%20structural%20inconsistencies%2C%20we%20further%0Aincorporate%20a%20large%20language%20model%20into%20an%20automatic%20generate%20evaluate%20refine%0Aloop%2C%20which%20verifies%20logical%2C%20behavioral%2C%20and%20temporal%20coherence%20and%20generates%0Acorrection%20rules%20without%20manual%20intervention%20or%20environment%20specific%20tuning.%0AThrough%20comprehensive%20experiments%20with%20novel%20evaluation%20metrics%2C%20ADLGen%20is%0Ashown%20to%20outperform%20baseline%20generators%20in%20statistical%20fidelity%2C%20semantic%0Arichness%2C%20and%20downstream%20activity%20recognition%2C%20offering%20a%20scalable%20and%0Aprivacy-preserving%20solution%20for%20ADL%20data%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DADLGen%253A%2520Synthesizing%2520Symbolic%252C%2520Event-Triggered%2520Sensor%2520Sequences%2520for%250A%2520%2520Human%2520Activity%2520Modeling%26entry.906535625%3DWeihang%2520You%2520and%2520Hanqi%2520Jiang%2520and%2520Zishuai%2520Liu%2520and%2520Zihang%2520Xie%2520and%2520Tianming%2520Liu%2520and%2520Jin%2520Lu%2520and%2520Fei%2520Dou%26entry.1292438233%3D%2520%2520Real%2520world%2520collection%2520of%2520Activities%2520of%2520Daily%2520Living%2520data%2520is%2520challenging%2520due%250Ato%2520privacy%2520concerns%252C%2520costly%2520deployment%2520and%2520labeling%252C%2520and%2520the%2520inherent%2520sparsity%250Aand%2520imbalance%2520of%2520human%2520behavior.%2520We%2520present%2520ADLGen%252C%2520a%2520generative%2520framework%250Aspecifically%2520designed%2520to%2520synthesize%2520realistic%252C%2520event%2520triggered%252C%2520and%2520symbolic%250Asensor%2520sequences%2520for%2520ambient%2520assistive%2520environments.%2520ADLGen%2520integrates%2520a%250Adecoder%2520only%2520Transformer%2520with%2520sign%2520based%2520symbolic%2520temporal%2520encoding%252C%2520and%2520a%250Acontext%2520and%2520layout%2520aware%2520sampling%2520mechanism%2520to%2520guide%2520generation%2520toward%250Asemantically%2520rich%2520and%2520physically%2520plausible%2520sensor%2520event%2520sequences.%2520To%2520enhance%250Asemantic%2520fidelity%2520and%2520correct%2520structural%2520inconsistencies%252C%2520we%2520further%250Aincorporate%2520a%2520large%2520language%2520model%2520into%2520an%2520automatic%2520generate%2520evaluate%2520refine%250Aloop%252C%2520which%2520verifies%2520logical%252C%2520behavioral%252C%2520and%2520temporal%2520coherence%2520and%2520generates%250Acorrection%2520rules%2520without%2520manual%2520intervention%2520or%2520environment%2520specific%2520tuning.%250AThrough%2520comprehensive%2520experiments%2520with%2520novel%2520evaluation%2520metrics%252C%2520ADLGen%2520is%250Ashown%2520to%2520outperform%2520baseline%2520generators%2520in%2520statistical%2520fidelity%252C%2520semantic%250Arichness%252C%2520and%2520downstream%2520activity%2520recognition%252C%2520offering%2520a%2520scalable%2520and%250Aprivacy-preserving%2520solution%2520for%2520ADL%2520data%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ADLGen%3A%20Synthesizing%20Symbolic%2C%20Event-Triggered%20Sensor%20Sequences%20for%0A%20%20Human%20Activity%20Modeling&entry.906535625=Weihang%20You%20and%20Hanqi%20Jiang%20and%20Zishuai%20Liu%20and%20Zihang%20Xie%20and%20Tianming%20Liu%20and%20Jin%20Lu%20and%20Fei%20Dou&entry.1292438233=%20%20Real%20world%20collection%20of%20Activities%20of%20Daily%20Living%20data%20is%20challenging%20due%0Ato%20privacy%20concerns%2C%20costly%20deployment%20and%20labeling%2C%20and%20the%20inherent%20sparsity%0Aand%20imbalance%20of%20human%20behavior.%20We%20present%20ADLGen%2C%20a%20generative%20framework%0Aspecifically%20designed%20to%20synthesize%20realistic%2C%20event%20triggered%2C%20and%20symbolic%0Asensor%20sequences%20for%20ambient%20assistive%20environments.%20ADLGen%20integrates%20a%0Adecoder%20only%20Transformer%20with%20sign%20based%20symbolic%20temporal%20encoding%2C%20and%20a%0Acontext%20and%20layout%20aware%20sampling%20mechanism%20to%20guide%20generation%20toward%0Asemantically%20rich%20and%20physically%20plausible%20sensor%20event%20sequences.%20To%20enhance%0Asemantic%20fidelity%20and%20correct%20structural%20inconsistencies%2C%20we%20further%0Aincorporate%20a%20large%20language%20model%20into%20an%20automatic%20generate%20evaluate%20refine%0Aloop%2C%20which%20verifies%20logical%2C%20behavioral%2C%20and%20temporal%20coherence%20and%20generates%0Acorrection%20rules%20without%20manual%20intervention%20or%20environment%20specific%20tuning.%0AThrough%20comprehensive%20experiments%20with%20novel%20evaluation%20metrics%2C%20ADLGen%20is%0Ashown%20to%20outperform%20baseline%20generators%20in%20statistical%20fidelity%2C%20semantic%0Arichness%2C%20and%20downstream%20activity%20recognition%2C%20offering%20a%20scalable%20and%0Aprivacy-preserving%20solution%20for%20ADL%20data%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17987v1&entry.124074799=Read"},
{"title": "CAMME: Adaptive Deepfake Image Detection with Multi-Modal\n  Cross-Attention", "author": "Naseem Khan and Tuan Nguyen and Amine Bermak and Issa Khalil", "abstract": "  The proliferation of sophisticated AI-generated deepfakes poses critical\nchallenges for digital media authentication and societal security. While\nexisting detection methods perform well within specific generative domains,\nthey exhibit significant performance degradation when applied to manipulations\nproduced by unseen architectures--a fundamental limitation as generative\ntechnologies rapidly evolve. We propose CAMME (Cross-Attention Multi-Modal\nEmbeddings), a framework that dynamically integrates visual, textual, and\nfrequency-domain features through a multi-head cross-attention mechanism to\nestablish robust cross-domain generalization. Extensive experiments demonstrate\nCAMME's superiority over state-of-the-art methods, yielding improvements of\n12.56% on natural scenes and 13.25% on facial deepfakes. The framework\ndemonstrates exceptional resilience, maintaining (over 91%) accuracy under\nnatural image perturbations and achieving 89.01% and 96.14% accuracy against\nPGD and FGSM adversarial attacks, respectively. Our findings validate that\nintegrating complementary modalities through cross-attention enables more\neffective decision boundary realignment for reliable deepfake detection across\nheterogeneous generative architectures.\n", "link": "http://arxiv.org/abs/2505.18035v1", "date": "2025-05-23", "relevancy": 2.3465, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.597}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5816}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMME%3A%20Adaptive%20Deepfake%20Image%20Detection%20with%20Multi-Modal%0A%20%20Cross-Attention&body=Title%3A%20CAMME%3A%20Adaptive%20Deepfake%20Image%20Detection%20with%20Multi-Modal%0A%20%20Cross-Attention%0AAuthor%3A%20Naseem%20Khan%20and%20Tuan%20Nguyen%20and%20Amine%20Bermak%20and%20Issa%20Khalil%0AAbstract%3A%20%20%20The%20proliferation%20of%20sophisticated%20AI-generated%20deepfakes%20poses%20critical%0Achallenges%20for%20digital%20media%20authentication%20and%20societal%20security.%20While%0Aexisting%20detection%20methods%20perform%20well%20within%20specific%20generative%20domains%2C%0Athey%20exhibit%20significant%20performance%20degradation%20when%20applied%20to%20manipulations%0Aproduced%20by%20unseen%20architectures--a%20fundamental%20limitation%20as%20generative%0Atechnologies%20rapidly%20evolve.%20We%20propose%20CAMME%20%28Cross-Attention%20Multi-Modal%0AEmbeddings%29%2C%20a%20framework%20that%20dynamically%20integrates%20visual%2C%20textual%2C%20and%0Afrequency-domain%20features%20through%20a%20multi-head%20cross-attention%20mechanism%20to%0Aestablish%20robust%20cross-domain%20generalization.%20Extensive%20experiments%20demonstrate%0ACAMME%27s%20superiority%20over%20state-of-the-art%20methods%2C%20yielding%20improvements%20of%0A12.56%25%20on%20natural%20scenes%20and%2013.25%25%20on%20facial%20deepfakes.%20The%20framework%0Ademonstrates%20exceptional%20resilience%2C%20maintaining%20%28over%2091%25%29%20accuracy%20under%0Anatural%20image%20perturbations%20and%20achieving%2089.01%25%20and%2096.14%25%20accuracy%20against%0APGD%20and%20FGSM%20adversarial%20attacks%2C%20respectively.%20Our%20findings%20validate%20that%0Aintegrating%20complementary%20modalities%20through%20cross-attention%20enables%20more%0Aeffective%20decision%20boundary%20realignment%20for%20reliable%20deepfake%20detection%20across%0Aheterogeneous%20generative%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMME%253A%2520Adaptive%2520Deepfake%2520Image%2520Detection%2520with%2520Multi-Modal%250A%2520%2520Cross-Attention%26entry.906535625%3DNaseem%2520Khan%2520and%2520Tuan%2520Nguyen%2520and%2520Amine%2520Bermak%2520and%2520Issa%2520Khalil%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520sophisticated%2520AI-generated%2520deepfakes%2520poses%2520critical%250Achallenges%2520for%2520digital%2520media%2520authentication%2520and%2520societal%2520security.%2520While%250Aexisting%2520detection%2520methods%2520perform%2520well%2520within%2520specific%2520generative%2520domains%252C%250Athey%2520exhibit%2520significant%2520performance%2520degradation%2520when%2520applied%2520to%2520manipulations%250Aproduced%2520by%2520unseen%2520architectures--a%2520fundamental%2520limitation%2520as%2520generative%250Atechnologies%2520rapidly%2520evolve.%2520We%2520propose%2520CAMME%2520%2528Cross-Attention%2520Multi-Modal%250AEmbeddings%2529%252C%2520a%2520framework%2520that%2520dynamically%2520integrates%2520visual%252C%2520textual%252C%2520and%250Afrequency-domain%2520features%2520through%2520a%2520multi-head%2520cross-attention%2520mechanism%2520to%250Aestablish%2520robust%2520cross-domain%2520generalization.%2520Extensive%2520experiments%2520demonstrate%250ACAMME%2527s%2520superiority%2520over%2520state-of-the-art%2520methods%252C%2520yielding%2520improvements%2520of%250A12.56%2525%2520on%2520natural%2520scenes%2520and%252013.25%2525%2520on%2520facial%2520deepfakes.%2520The%2520framework%250Ademonstrates%2520exceptional%2520resilience%252C%2520maintaining%2520%2528over%252091%2525%2529%2520accuracy%2520under%250Anatural%2520image%2520perturbations%2520and%2520achieving%252089.01%2525%2520and%252096.14%2525%2520accuracy%2520against%250APGD%2520and%2520FGSM%2520adversarial%2520attacks%252C%2520respectively.%2520Our%2520findings%2520validate%2520that%250Aintegrating%2520complementary%2520modalities%2520through%2520cross-attention%2520enables%2520more%250Aeffective%2520decision%2520boundary%2520realignment%2520for%2520reliable%2520deepfake%2520detection%2520across%250Aheterogeneous%2520generative%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMME%3A%20Adaptive%20Deepfake%20Image%20Detection%20with%20Multi-Modal%0A%20%20Cross-Attention&entry.906535625=Naseem%20Khan%20and%20Tuan%20Nguyen%20and%20Amine%20Bermak%20and%20Issa%20Khalil&entry.1292438233=%20%20The%20proliferation%20of%20sophisticated%20AI-generated%20deepfakes%20poses%20critical%0Achallenges%20for%20digital%20media%20authentication%20and%20societal%20security.%20While%0Aexisting%20detection%20methods%20perform%20well%20within%20specific%20generative%20domains%2C%0Athey%20exhibit%20significant%20performance%20degradation%20when%20applied%20to%20manipulations%0Aproduced%20by%20unseen%20architectures--a%20fundamental%20limitation%20as%20generative%0Atechnologies%20rapidly%20evolve.%20We%20propose%20CAMME%20%28Cross-Attention%20Multi-Modal%0AEmbeddings%29%2C%20a%20framework%20that%20dynamically%20integrates%20visual%2C%20textual%2C%20and%0Afrequency-domain%20features%20through%20a%20multi-head%20cross-attention%20mechanism%20to%0Aestablish%20robust%20cross-domain%20generalization.%20Extensive%20experiments%20demonstrate%0ACAMME%27s%20superiority%20over%20state-of-the-art%20methods%2C%20yielding%20improvements%20of%0A12.56%25%20on%20natural%20scenes%20and%2013.25%25%20on%20facial%20deepfakes.%20The%20framework%0Ademonstrates%20exceptional%20resilience%2C%20maintaining%20%28over%2091%25%29%20accuracy%20under%0Anatural%20image%20perturbations%20and%20achieving%2089.01%25%20and%2096.14%25%20accuracy%20against%0APGD%20and%20FGSM%20adversarial%20attacks%2C%20respectively.%20Our%20findings%20validate%20that%0Aintegrating%20complementary%20modalities%20through%20cross-attention%20enables%20more%0Aeffective%20decision%20boundary%20realignment%20for%20reliable%20deepfake%20detection%20across%0Aheterogeneous%20generative%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18035v1&entry.124074799=Read"},
{"title": "Random Feature Representation Boosting", "author": "Nikita Zozoulenko and Thomas Cass and Lukas Gonon", "abstract": "  We introduce Random Feature Representation Boosting (RFRBoost), a novel\nmethod for constructing deep residual random feature neural networks (RFNNs)\nusing boosting theory. RFRBoost uses random features at each layer to learn the\nfunctional gradient of the network representation, enhancing performance while\npreserving the convex optimization benefits of RFNNs. In the case of MSE loss,\nwe obtain closed-form solutions to greedy layer-wise boosting with random\nfeatures. For general loss functions, we show that fitting random feature\nresidual blocks reduces to solving a quadratically constrained least squares\nproblem. Through extensive numerical experiments on tabular datasets for both\nregression and classification, we show that RFRBoost significantly outperforms\nRFNNs and end-to-end trained MLP ResNets in the small- to medium-scale regime\nwhere RFNNs are typically applied. Moreover, RFRBoost offers substantial\ncomputational benefits, and theoretical guarantees stemming from boosting\ntheory.\n", "link": "http://arxiv.org/abs/2501.18283v2", "date": "2025-05-23", "relevancy": 2.3343, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5221}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4431}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20Feature%20Representation%20Boosting&body=Title%3A%20Random%20Feature%20Representation%20Boosting%0AAuthor%3A%20Nikita%20Zozoulenko%20and%20Thomas%20Cass%20and%20Lukas%20Gonon%0AAbstract%3A%20%20%20We%20introduce%20Random%20Feature%20Representation%20Boosting%20%28RFRBoost%29%2C%20a%20novel%0Amethod%20for%20constructing%20deep%20residual%20random%20feature%20neural%20networks%20%28RFNNs%29%0Ausing%20boosting%20theory.%20RFRBoost%20uses%20random%20features%20at%20each%20layer%20to%20learn%20the%0Afunctional%20gradient%20of%20the%20network%20representation%2C%20enhancing%20performance%20while%0Apreserving%20the%20convex%20optimization%20benefits%20of%20RFNNs.%20In%20the%20case%20of%20MSE%20loss%2C%0Awe%20obtain%20closed-form%20solutions%20to%20greedy%20layer-wise%20boosting%20with%20random%0Afeatures.%20For%20general%20loss%20functions%2C%20we%20show%20that%20fitting%20random%20feature%0Aresidual%20blocks%20reduces%20to%20solving%20a%20quadratically%20constrained%20least%20squares%0Aproblem.%20Through%20extensive%20numerical%20experiments%20on%20tabular%20datasets%20for%20both%0Aregression%20and%20classification%2C%20we%20show%20that%20RFRBoost%20significantly%20outperforms%0ARFNNs%20and%20end-to-end%20trained%20MLP%20ResNets%20in%20the%20small-%20to%20medium-scale%20regime%0Awhere%20RFNNs%20are%20typically%20applied.%20Moreover%2C%20RFRBoost%20offers%20substantial%0Acomputational%20benefits%2C%20and%20theoretical%20guarantees%20stemming%20from%20boosting%0Atheory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520Feature%2520Representation%2520Boosting%26entry.906535625%3DNikita%2520Zozoulenko%2520and%2520Thomas%2520Cass%2520and%2520Lukas%2520Gonon%26entry.1292438233%3D%2520%2520We%2520introduce%2520Random%2520Feature%2520Representation%2520Boosting%2520%2528RFRBoost%2529%252C%2520a%2520novel%250Amethod%2520for%2520constructing%2520deep%2520residual%2520random%2520feature%2520neural%2520networks%2520%2528RFNNs%2529%250Ausing%2520boosting%2520theory.%2520RFRBoost%2520uses%2520random%2520features%2520at%2520each%2520layer%2520to%2520learn%2520the%250Afunctional%2520gradient%2520of%2520the%2520network%2520representation%252C%2520enhancing%2520performance%2520while%250Apreserving%2520the%2520convex%2520optimization%2520benefits%2520of%2520RFNNs.%2520In%2520the%2520case%2520of%2520MSE%2520loss%252C%250Awe%2520obtain%2520closed-form%2520solutions%2520to%2520greedy%2520layer-wise%2520boosting%2520with%2520random%250Afeatures.%2520For%2520general%2520loss%2520functions%252C%2520we%2520show%2520that%2520fitting%2520random%2520feature%250Aresidual%2520blocks%2520reduces%2520to%2520solving%2520a%2520quadratically%2520constrained%2520least%2520squares%250Aproblem.%2520Through%2520extensive%2520numerical%2520experiments%2520on%2520tabular%2520datasets%2520for%2520both%250Aregression%2520and%2520classification%252C%2520we%2520show%2520that%2520RFRBoost%2520significantly%2520outperforms%250ARFNNs%2520and%2520end-to-end%2520trained%2520MLP%2520ResNets%2520in%2520the%2520small-%2520to%2520medium-scale%2520regime%250Awhere%2520RFNNs%2520are%2520typically%2520applied.%2520Moreover%252C%2520RFRBoost%2520offers%2520substantial%250Acomputational%2520benefits%252C%2520and%2520theoretical%2520guarantees%2520stemming%2520from%2520boosting%250Atheory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20Feature%20Representation%20Boosting&entry.906535625=Nikita%20Zozoulenko%20and%20Thomas%20Cass%20and%20Lukas%20Gonon&entry.1292438233=%20%20We%20introduce%20Random%20Feature%20Representation%20Boosting%20%28RFRBoost%29%2C%20a%20novel%0Amethod%20for%20constructing%20deep%20residual%20random%20feature%20neural%20networks%20%28RFNNs%29%0Ausing%20boosting%20theory.%20RFRBoost%20uses%20random%20features%20at%20each%20layer%20to%20learn%20the%0Afunctional%20gradient%20of%20the%20network%20representation%2C%20enhancing%20performance%20while%0Apreserving%20the%20convex%20optimization%20benefits%20of%20RFNNs.%20In%20the%20case%20of%20MSE%20loss%2C%0Awe%20obtain%20closed-form%20solutions%20to%20greedy%20layer-wise%20boosting%20with%20random%0Afeatures.%20For%20general%20loss%20functions%2C%20we%20show%20that%20fitting%20random%20feature%0Aresidual%20blocks%20reduces%20to%20solving%20a%20quadratically%20constrained%20least%20squares%0Aproblem.%20Through%20extensive%20numerical%20experiments%20on%20tabular%20datasets%20for%20both%0Aregression%20and%20classification%2C%20we%20show%20that%20RFRBoost%20significantly%20outperforms%0ARFNNs%20and%20end-to-end%20trained%20MLP%20ResNets%20in%20the%20small-%20to%20medium-scale%20regime%0Awhere%20RFNNs%20are%20typically%20applied.%20Moreover%2C%20RFRBoost%20offers%20substantial%0Acomputational%20benefits%2C%20and%20theoretical%20guarantees%20stemming%20from%20boosting%0Atheory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18283v2&entry.124074799=Read"},
{"title": "Camera Movement Estimation and Path Correction using the Combination of\n  Modified A-SIFT and Stereo System for 3D Modelling", "author": "Usha Kumari and Shuvendu Rana", "abstract": "  Creating accurate and efficient 3D models poses significant challenges,\nparticularly in addressing large viewpoint variations, computational\ncomplexity, and alignment discrepancies. Efficient camera path generation can\nhelp resolve these issues. In this context, a modified version of the Affine\nScale-Invariant Feature Transform (ASIFT) is proposed to extract more matching\npoints with reduced computational overhead, ensuring an adequate number of\ninliers for precise camera rotation angle estimation. Additionally, a novel\ntwo-camera-based rotation correction model is introduced to mitigate small\nrotational errors, further enhancing accuracy. Furthermore, a stereo\ncamera-based translation estimation and correction model is implemented to\ndetermine camera movement in 3D space by altering the Structure From Motion\n(SFM) model. Finally, the novel combination of ASIFT and two camera-based SFM\nmodels provides an accurate camera movement trajectory in 3D space.\nExperimental results show that the proposed camera movement approach achieves\n99.9% accuracy compared to the actual camera movement path and outperforms\nstate-of-the-art camera path estimation methods. By leveraging this accurate\ncamera path, the system facilitates the creation of precise 3D models, making\nit a robust solution for applications requiring high fidelity and efficiency in\n3D reconstruction.\n", "link": "http://arxiv.org/abs/2503.17668v2", "date": "2025-05-23", "relevancy": 2.3264, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6188}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.586}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Camera%20Movement%20Estimation%20and%20Path%20Correction%20using%20the%20Combination%20of%0A%20%20Modified%20A-SIFT%20and%20Stereo%20System%20for%203D%20Modelling&body=Title%3A%20Camera%20Movement%20Estimation%20and%20Path%20Correction%20using%20the%20Combination%20of%0A%20%20Modified%20A-SIFT%20and%20Stereo%20System%20for%203D%20Modelling%0AAuthor%3A%20Usha%20Kumari%20and%20Shuvendu%20Rana%0AAbstract%3A%20%20%20Creating%20accurate%20and%20efficient%203D%20models%20poses%20significant%20challenges%2C%0Aparticularly%20in%20addressing%20large%20viewpoint%20variations%2C%20computational%0Acomplexity%2C%20and%20alignment%20discrepancies.%20Efficient%20camera%20path%20generation%20can%0Ahelp%20resolve%20these%20issues.%20In%20this%20context%2C%20a%20modified%20version%20of%20the%20Affine%0AScale-Invariant%20Feature%20Transform%20%28ASIFT%29%20is%20proposed%20to%20extract%20more%20matching%0Apoints%20with%20reduced%20computational%20overhead%2C%20ensuring%20an%20adequate%20number%20of%0Ainliers%20for%20precise%20camera%20rotation%20angle%20estimation.%20Additionally%2C%20a%20novel%0Atwo-camera-based%20rotation%20correction%20model%20is%20introduced%20to%20mitigate%20small%0Arotational%20errors%2C%20further%20enhancing%20accuracy.%20Furthermore%2C%20a%20stereo%0Acamera-based%20translation%20estimation%20and%20correction%20model%20is%20implemented%20to%0Adetermine%20camera%20movement%20in%203D%20space%20by%20altering%20the%20Structure%20From%20Motion%0A%28SFM%29%20model.%20Finally%2C%20the%20novel%20combination%20of%20ASIFT%20and%20two%20camera-based%20SFM%0Amodels%20provides%20an%20accurate%20camera%20movement%20trajectory%20in%203D%20space.%0AExperimental%20results%20show%20that%20the%20proposed%20camera%20movement%20approach%20achieves%0A99.9%25%20accuracy%20compared%20to%20the%20actual%20camera%20movement%20path%20and%20outperforms%0Astate-of-the-art%20camera%20path%20estimation%20methods.%20By%20leveraging%20this%20accurate%0Acamera%20path%2C%20the%20system%20facilitates%20the%20creation%20of%20precise%203D%20models%2C%20making%0Ait%20a%20robust%20solution%20for%20applications%20requiring%20high%20fidelity%20and%20efficiency%20in%0A3D%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17668v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCamera%2520Movement%2520Estimation%2520and%2520Path%2520Correction%2520using%2520the%2520Combination%2520of%250A%2520%2520Modified%2520A-SIFT%2520and%2520Stereo%2520System%2520for%25203D%2520Modelling%26entry.906535625%3DUsha%2520Kumari%2520and%2520Shuvendu%2520Rana%26entry.1292438233%3D%2520%2520Creating%2520accurate%2520and%2520efficient%25203D%2520models%2520poses%2520significant%2520challenges%252C%250Aparticularly%2520in%2520addressing%2520large%2520viewpoint%2520variations%252C%2520computational%250Acomplexity%252C%2520and%2520alignment%2520discrepancies.%2520Efficient%2520camera%2520path%2520generation%2520can%250Ahelp%2520resolve%2520these%2520issues.%2520In%2520this%2520context%252C%2520a%2520modified%2520version%2520of%2520the%2520Affine%250AScale-Invariant%2520Feature%2520Transform%2520%2528ASIFT%2529%2520is%2520proposed%2520to%2520extract%2520more%2520matching%250Apoints%2520with%2520reduced%2520computational%2520overhead%252C%2520ensuring%2520an%2520adequate%2520number%2520of%250Ainliers%2520for%2520precise%2520camera%2520rotation%2520angle%2520estimation.%2520Additionally%252C%2520a%2520novel%250Atwo-camera-based%2520rotation%2520correction%2520model%2520is%2520introduced%2520to%2520mitigate%2520small%250Arotational%2520errors%252C%2520further%2520enhancing%2520accuracy.%2520Furthermore%252C%2520a%2520stereo%250Acamera-based%2520translation%2520estimation%2520and%2520correction%2520model%2520is%2520implemented%2520to%250Adetermine%2520camera%2520movement%2520in%25203D%2520space%2520by%2520altering%2520the%2520Structure%2520From%2520Motion%250A%2528SFM%2529%2520model.%2520Finally%252C%2520the%2520novel%2520combination%2520of%2520ASIFT%2520and%2520two%2520camera-based%2520SFM%250Amodels%2520provides%2520an%2520accurate%2520camera%2520movement%2520trajectory%2520in%25203D%2520space.%250AExperimental%2520results%2520show%2520that%2520the%2520proposed%2520camera%2520movement%2520approach%2520achieves%250A99.9%2525%2520accuracy%2520compared%2520to%2520the%2520actual%2520camera%2520movement%2520path%2520and%2520outperforms%250Astate-of-the-art%2520camera%2520path%2520estimation%2520methods.%2520By%2520leveraging%2520this%2520accurate%250Acamera%2520path%252C%2520the%2520system%2520facilitates%2520the%2520creation%2520of%2520precise%25203D%2520models%252C%2520making%250Ait%2520a%2520robust%2520solution%2520for%2520applications%2520requiring%2520high%2520fidelity%2520and%2520efficiency%2520in%250A3D%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17668v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Camera%20Movement%20Estimation%20and%20Path%20Correction%20using%20the%20Combination%20of%0A%20%20Modified%20A-SIFT%20and%20Stereo%20System%20for%203D%20Modelling&entry.906535625=Usha%20Kumari%20and%20Shuvendu%20Rana&entry.1292438233=%20%20Creating%20accurate%20and%20efficient%203D%20models%20poses%20significant%20challenges%2C%0Aparticularly%20in%20addressing%20large%20viewpoint%20variations%2C%20computational%0Acomplexity%2C%20and%20alignment%20discrepancies.%20Efficient%20camera%20path%20generation%20can%0Ahelp%20resolve%20these%20issues.%20In%20this%20context%2C%20a%20modified%20version%20of%20the%20Affine%0AScale-Invariant%20Feature%20Transform%20%28ASIFT%29%20is%20proposed%20to%20extract%20more%20matching%0Apoints%20with%20reduced%20computational%20overhead%2C%20ensuring%20an%20adequate%20number%20of%0Ainliers%20for%20precise%20camera%20rotation%20angle%20estimation.%20Additionally%2C%20a%20novel%0Atwo-camera-based%20rotation%20correction%20model%20is%20introduced%20to%20mitigate%20small%0Arotational%20errors%2C%20further%20enhancing%20accuracy.%20Furthermore%2C%20a%20stereo%0Acamera-based%20translation%20estimation%20and%20correction%20model%20is%20implemented%20to%0Adetermine%20camera%20movement%20in%203D%20space%20by%20altering%20the%20Structure%20From%20Motion%0A%28SFM%29%20model.%20Finally%2C%20the%20novel%20combination%20of%20ASIFT%20and%20two%20camera-based%20SFM%0Amodels%20provides%20an%20accurate%20camera%20movement%20trajectory%20in%203D%20space.%0AExperimental%20results%20show%20that%20the%20proposed%20camera%20movement%20approach%20achieves%0A99.9%25%20accuracy%20compared%20to%20the%20actual%20camera%20movement%20path%20and%20outperforms%0Astate-of-the-art%20camera%20path%20estimation%20methods.%20By%20leveraging%20this%20accurate%0Acamera%20path%2C%20the%20system%20facilitates%20the%20creation%20of%20precise%203D%20models%2C%20making%0Ait%20a%20robust%20solution%20for%20applications%20requiring%20high%20fidelity%20and%20efficiency%20in%0A3D%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17668v2&entry.124074799=Read"},
{"title": "Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate\n  Object Hallucinations", "author": "Boxu Chen and Ziwei Zheng and Le Yang and Zeyu Geng and Zhengyu Zhao and Chenhao Lin and Chao Shen", "abstract": "  Large Vision-Language Models (LVLMs) have achieved remarkable success but\ncontinue to struggle with object hallucination (OH), generating outputs\ninconsistent with visual inputs. While previous work has proposed methods to\nreduce OH, the visual decision-making mechanisms that lead to hallucinations\nremain poorly understood. In this paper, we propose VaLSe, a Vision-aware\nLatent Steering framework that adopts an interpretation-then-mitigation\nstrategy to address OH in LVLMs. By tackling dual challenges of modeling\ncomplex vision-language interactions and eliminating spurious activation\nartifacts, VaLSe can generate visual contribution maps that trace how specific\nvisual inputs influence individual output tokens. These maps reveal the model's\nvision-aware focus regions, which are then used to perform latent space\nsteering, realigning internal representations toward semantically relevant\ncontent and reducing hallucinated outputs. Extensive experiments demonstrate\nthat VaLSe is a powerful interpretability tool and an effective method for\nenhancing model robustness against OH across multiple benchmarks. Furthermore,\nour analysis uncovers limitations in existing OH evaluation metrics,\nunderscoring the need for more nuanced, interpretable, and visually grounded OH\nbenchmarks in future work. Code is available at:\nhttps://github.com/Ziwei-Zheng/VaLSe.\n", "link": "http://arxiv.org/abs/2505.17812v1", "date": "2025-05-23", "relevancy": 2.3254, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5894}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20It%20or%20Not%3F%20Interpretable%20Vision-aware%20Latent%20Steering%20to%20Mitigate%0A%20%20Object%20Hallucinations&body=Title%3A%20Seeing%20It%20or%20Not%3F%20Interpretable%20Vision-aware%20Latent%20Steering%20to%20Mitigate%0A%20%20Object%20Hallucinations%0AAuthor%3A%20Boxu%20Chen%20and%20Ziwei%20Zheng%20and%20Le%20Yang%20and%20Zeyu%20Geng%20and%20Zhengyu%20Zhao%20and%20Chenhao%20Lin%20and%20Chao%20Shen%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20achieved%20remarkable%20success%20but%0Acontinue%20to%20struggle%20with%20object%20hallucination%20%28OH%29%2C%20generating%20outputs%0Ainconsistent%20with%20visual%20inputs.%20While%20previous%20work%20has%20proposed%20methods%20to%0Areduce%20OH%2C%20the%20visual%20decision-making%20mechanisms%20that%20lead%20to%20hallucinations%0Aremain%20poorly%20understood.%20In%20this%20paper%2C%20we%20propose%20VaLSe%2C%20a%20Vision-aware%0ALatent%20Steering%20framework%20that%20adopts%20an%20interpretation-then-mitigation%0Astrategy%20to%20address%20OH%20in%20LVLMs.%20By%20tackling%20dual%20challenges%20of%20modeling%0Acomplex%20vision-language%20interactions%20and%20eliminating%20spurious%20activation%0Aartifacts%2C%20VaLSe%20can%20generate%20visual%20contribution%20maps%20that%20trace%20how%20specific%0Avisual%20inputs%20influence%20individual%20output%20tokens.%20These%20maps%20reveal%20the%20model%27s%0Avision-aware%20focus%20regions%2C%20which%20are%20then%20used%20to%20perform%20latent%20space%0Asteering%2C%20realigning%20internal%20representations%20toward%20semantically%20relevant%0Acontent%20and%20reducing%20hallucinated%20outputs.%20Extensive%20experiments%20demonstrate%0Athat%20VaLSe%20is%20a%20powerful%20interpretability%20tool%20and%20an%20effective%20method%20for%0Aenhancing%20model%20robustness%20against%20OH%20across%20multiple%20benchmarks.%20Furthermore%2C%0Aour%20analysis%20uncovers%20limitations%20in%20existing%20OH%20evaluation%20metrics%2C%0Aunderscoring%20the%20need%20for%20more%20nuanced%2C%20interpretable%2C%20and%20visually%20grounded%20OH%0Abenchmarks%20in%20future%20work.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/Ziwei-Zheng/VaLSe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520It%2520or%2520Not%253F%2520Interpretable%2520Vision-aware%2520Latent%2520Steering%2520to%2520Mitigate%250A%2520%2520Object%2520Hallucinations%26entry.906535625%3DBoxu%2520Chen%2520and%2520Ziwei%2520Zheng%2520and%2520Le%2520Yang%2520and%2520Zeyu%2520Geng%2520and%2520Zhengyu%2520Zhao%2520and%2520Chenhao%2520Lin%2520and%2520Chao%2520Shen%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520but%250Acontinue%2520to%2520struggle%2520with%2520object%2520hallucination%2520%2528OH%2529%252C%2520generating%2520outputs%250Ainconsistent%2520with%2520visual%2520inputs.%2520While%2520previous%2520work%2520has%2520proposed%2520methods%2520to%250Areduce%2520OH%252C%2520the%2520visual%2520decision-making%2520mechanisms%2520that%2520lead%2520to%2520hallucinations%250Aremain%2520poorly%2520understood.%2520In%2520this%2520paper%252C%2520we%2520propose%2520VaLSe%252C%2520a%2520Vision-aware%250ALatent%2520Steering%2520framework%2520that%2520adopts%2520an%2520interpretation-then-mitigation%250Astrategy%2520to%2520address%2520OH%2520in%2520LVLMs.%2520By%2520tackling%2520dual%2520challenges%2520of%2520modeling%250Acomplex%2520vision-language%2520interactions%2520and%2520eliminating%2520spurious%2520activation%250Aartifacts%252C%2520VaLSe%2520can%2520generate%2520visual%2520contribution%2520maps%2520that%2520trace%2520how%2520specific%250Avisual%2520inputs%2520influence%2520individual%2520output%2520tokens.%2520These%2520maps%2520reveal%2520the%2520model%2527s%250Avision-aware%2520focus%2520regions%252C%2520which%2520are%2520then%2520used%2520to%2520perform%2520latent%2520space%250Asteering%252C%2520realigning%2520internal%2520representations%2520toward%2520semantically%2520relevant%250Acontent%2520and%2520reducing%2520hallucinated%2520outputs.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520VaLSe%2520is%2520a%2520powerful%2520interpretability%2520tool%2520and%2520an%2520effective%2520method%2520for%250Aenhancing%2520model%2520robustness%2520against%2520OH%2520across%2520multiple%2520benchmarks.%2520Furthermore%252C%250Aour%2520analysis%2520uncovers%2520limitations%2520in%2520existing%2520OH%2520evaluation%2520metrics%252C%250Aunderscoring%2520the%2520need%2520for%2520more%2520nuanced%252C%2520interpretable%252C%2520and%2520visually%2520grounded%2520OH%250Abenchmarks%2520in%2520future%2520work.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/Ziwei-Zheng/VaLSe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20It%20or%20Not%3F%20Interpretable%20Vision-aware%20Latent%20Steering%20to%20Mitigate%0A%20%20Object%20Hallucinations&entry.906535625=Boxu%20Chen%20and%20Ziwei%20Zheng%20and%20Le%20Yang%20and%20Zeyu%20Geng%20and%20Zhengyu%20Zhao%20and%20Chenhao%20Lin%20and%20Chao%20Shen&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20achieved%20remarkable%20success%20but%0Acontinue%20to%20struggle%20with%20object%20hallucination%20%28OH%29%2C%20generating%20outputs%0Ainconsistent%20with%20visual%20inputs.%20While%20previous%20work%20has%20proposed%20methods%20to%0Areduce%20OH%2C%20the%20visual%20decision-making%20mechanisms%20that%20lead%20to%20hallucinations%0Aremain%20poorly%20understood.%20In%20this%20paper%2C%20we%20propose%20VaLSe%2C%20a%20Vision-aware%0ALatent%20Steering%20framework%20that%20adopts%20an%20interpretation-then-mitigation%0Astrategy%20to%20address%20OH%20in%20LVLMs.%20By%20tackling%20dual%20challenges%20of%20modeling%0Acomplex%20vision-language%20interactions%20and%20eliminating%20spurious%20activation%0Aartifacts%2C%20VaLSe%20can%20generate%20visual%20contribution%20maps%20that%20trace%20how%20specific%0Avisual%20inputs%20influence%20individual%20output%20tokens.%20These%20maps%20reveal%20the%20model%27s%0Avision-aware%20focus%20regions%2C%20which%20are%20then%20used%20to%20perform%20latent%20space%0Asteering%2C%20realigning%20internal%20representations%20toward%20semantically%20relevant%0Acontent%20and%20reducing%20hallucinated%20outputs.%20Extensive%20experiments%20demonstrate%0Athat%20VaLSe%20is%20a%20powerful%20interpretability%20tool%20and%20an%20effective%20method%20for%0Aenhancing%20model%20robustness%20against%20OH%20across%20multiple%20benchmarks.%20Furthermore%2C%0Aour%20analysis%20uncovers%20limitations%20in%20existing%20OH%20evaluation%20metrics%2C%0Aunderscoring%20the%20need%20for%20more%20nuanced%2C%20interpretable%2C%20and%20visually%20grounded%20OH%0Abenchmarks%20in%20future%20work.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/Ziwei-Zheng/VaLSe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17812v1&entry.124074799=Read"},
{"title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding", "author": "Xiaoyi Zhang and Zhaoyang Jia and Zongyu Guo and Jiahao Li and Bin Li and Houqiang Li and Yan Lu", "abstract": "  Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code will be released later.\n", "link": "http://arxiv.org/abs/2505.18079v1", "date": "2025-05-23", "relevancy": 2.3185, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5827}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Video%20Discovery%3A%20Agentic%20Search%20with%20Tool%20Use%20for%20Long-form%20Video%0A%20%20Understanding&body=Title%3A%20Deep%20Video%20Discovery%3A%20Agentic%20Search%20with%20Tool%20Use%20for%20Long-form%20Video%0A%20%20Understanding%0AAuthor%3A%20Xiaoyi%20Zhang%20and%20Zhaoyang%20Jia%20and%20Zongyu%20Guo%20and%20Jiahao%20Li%20and%20Bin%20Li%20and%20Houqiang%20Li%20and%20Yan%20Lu%0AAbstract%3A%20%20%20Long-form%20video%20understanding%20presents%20significant%20challenges%20due%20to%0Aextensive%20temporal-spatial%20complexity%20and%20the%20difficulty%20of%20question%20answering%0Aunder%20such%20extended%20contexts.%20While%20Large%20Language%20Models%20%28LLMs%29%20have%0Ademonstrated%20considerable%20advancements%20in%20video%20analysis%20capabilities%20and%20long%0Acontext%20handling%2C%20they%20continue%20to%20exhibit%20limitations%20when%20processing%0Ainformation-dense%20hour-long%20videos.%20To%20overcome%20such%20limitations%2C%20we%20propose%0Athe%20Deep%20Video%20Discovery%20agent%20to%20leverage%20an%20agentic%20search%20strategy%20over%0Asegmented%20video%20clips.%20Different%20from%20previous%20video%20agents%20manually%20designing%0Aa%20rigid%20workflow%2C%20our%20approach%20emphasizes%20the%20autonomous%20nature%20of%20agents.%20By%0Aproviding%20a%20set%20of%20search-centric%20tools%20on%20multi-granular%20video%20database%2C%20our%0ADVD%20agent%20leverages%20the%20advanced%20reasoning%20capability%20of%20LLM%20to%20plan%20on%20its%0Acurrent%20observation%20state%2C%20strategically%20selects%20tools%2C%20formulates%20appropriate%0Aparameters%20for%20actions%2C%20and%20iteratively%20refines%20its%20internal%20reasoning%20in%20light%0Aof%20the%20gathered%20information.%20We%20perform%20comprehensive%20evaluation%20on%20multiple%0Along%20video%20understanding%20benchmarks%20that%20demonstrates%20the%20advantage%20of%20the%0Aentire%20system%20design.%20Our%20DVD%20agent%20achieves%20SOTA%20performance%2C%20significantly%0Asurpassing%20prior%20works%20by%20a%20large%20margin%20on%20the%20challenging%20LVBench%20dataset.%0AComprehensive%20ablation%20studies%20and%20in-depth%20tool%20analyses%20are%20also%20provided%2C%0Ayielding%20insights%20to%20further%20advance%20intelligent%20agents%20tailored%20for%20long-form%0Avideo%20understanding%20tasks.%20The%20code%20will%20be%20released%20later.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Video%2520Discovery%253A%2520Agentic%2520Search%2520with%2520Tool%2520Use%2520for%2520Long-form%2520Video%250A%2520%2520Understanding%26entry.906535625%3DXiaoyi%2520Zhang%2520and%2520Zhaoyang%2520Jia%2520and%2520Zongyu%2520Guo%2520and%2520Jiahao%2520Li%2520and%2520Bin%2520Li%2520and%2520Houqiang%2520Li%2520and%2520Yan%2520Lu%26entry.1292438233%3D%2520%2520Long-form%2520video%2520understanding%2520presents%2520significant%2520challenges%2520due%2520to%250Aextensive%2520temporal-spatial%2520complexity%2520and%2520the%2520difficulty%2520of%2520question%2520answering%250Aunder%2520such%2520extended%2520contexts.%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%250Ademonstrated%2520considerable%2520advancements%2520in%2520video%2520analysis%2520capabilities%2520and%2520long%250Acontext%2520handling%252C%2520they%2520continue%2520to%2520exhibit%2520limitations%2520when%2520processing%250Ainformation-dense%2520hour-long%2520videos.%2520To%2520overcome%2520such%2520limitations%252C%2520we%2520propose%250Athe%2520Deep%2520Video%2520Discovery%2520agent%2520to%2520leverage%2520an%2520agentic%2520search%2520strategy%2520over%250Asegmented%2520video%2520clips.%2520Different%2520from%2520previous%2520video%2520agents%2520manually%2520designing%250Aa%2520rigid%2520workflow%252C%2520our%2520approach%2520emphasizes%2520the%2520autonomous%2520nature%2520of%2520agents.%2520By%250Aproviding%2520a%2520set%2520of%2520search-centric%2520tools%2520on%2520multi-granular%2520video%2520database%252C%2520our%250ADVD%2520agent%2520leverages%2520the%2520advanced%2520reasoning%2520capability%2520of%2520LLM%2520to%2520plan%2520on%2520its%250Acurrent%2520observation%2520state%252C%2520strategically%2520selects%2520tools%252C%2520formulates%2520appropriate%250Aparameters%2520for%2520actions%252C%2520and%2520iteratively%2520refines%2520its%2520internal%2520reasoning%2520in%2520light%250Aof%2520the%2520gathered%2520information.%2520We%2520perform%2520comprehensive%2520evaluation%2520on%2520multiple%250Along%2520video%2520understanding%2520benchmarks%2520that%2520demonstrates%2520the%2520advantage%2520of%2520the%250Aentire%2520system%2520design.%2520Our%2520DVD%2520agent%2520achieves%2520SOTA%2520performance%252C%2520significantly%250Asurpassing%2520prior%2520works%2520by%2520a%2520large%2520margin%2520on%2520the%2520challenging%2520LVBench%2520dataset.%250AComprehensive%2520ablation%2520studies%2520and%2520in-depth%2520tool%2520analyses%2520are%2520also%2520provided%252C%250Ayielding%2520insights%2520to%2520further%2520advance%2520intelligent%2520agents%2520tailored%2520for%2520long-form%250Avideo%2520understanding%2520tasks.%2520The%2520code%2520will%2520be%2520released%2520later.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Video%20Discovery%3A%20Agentic%20Search%20with%20Tool%20Use%20for%20Long-form%20Video%0A%20%20Understanding&entry.906535625=Xiaoyi%20Zhang%20and%20Zhaoyang%20Jia%20and%20Zongyu%20Guo%20and%20Jiahao%20Li%20and%20Bin%20Li%20and%20Houqiang%20Li%20and%20Yan%20Lu&entry.1292438233=%20%20Long-form%20video%20understanding%20presents%20significant%20challenges%20due%20to%0Aextensive%20temporal-spatial%20complexity%20and%20the%20difficulty%20of%20question%20answering%0Aunder%20such%20extended%20contexts.%20While%20Large%20Language%20Models%20%28LLMs%29%20have%0Ademonstrated%20considerable%20advancements%20in%20video%20analysis%20capabilities%20and%20long%0Acontext%20handling%2C%20they%20continue%20to%20exhibit%20limitations%20when%20processing%0Ainformation-dense%20hour-long%20videos.%20To%20overcome%20such%20limitations%2C%20we%20propose%0Athe%20Deep%20Video%20Discovery%20agent%20to%20leverage%20an%20agentic%20search%20strategy%20over%0Asegmented%20video%20clips.%20Different%20from%20previous%20video%20agents%20manually%20designing%0Aa%20rigid%20workflow%2C%20our%20approach%20emphasizes%20the%20autonomous%20nature%20of%20agents.%20By%0Aproviding%20a%20set%20of%20search-centric%20tools%20on%20multi-granular%20video%20database%2C%20our%0ADVD%20agent%20leverages%20the%20advanced%20reasoning%20capability%20of%20LLM%20to%20plan%20on%20its%0Acurrent%20observation%20state%2C%20strategically%20selects%20tools%2C%20formulates%20appropriate%0Aparameters%20for%20actions%2C%20and%20iteratively%20refines%20its%20internal%20reasoning%20in%20light%0Aof%20the%20gathered%20information.%20We%20perform%20comprehensive%20evaluation%20on%20multiple%0Along%20video%20understanding%20benchmarks%20that%20demonstrates%20the%20advantage%20of%20the%0Aentire%20system%20design.%20Our%20DVD%20agent%20achieves%20SOTA%20performance%2C%20significantly%0Asurpassing%20prior%20works%20by%20a%20large%20margin%20on%20the%20challenging%20LVBench%20dataset.%0AComprehensive%20ablation%20studies%20and%20in-depth%20tool%20analyses%20are%20also%20provided%2C%0Ayielding%20insights%20to%20further%20advance%20intelligent%20agents%20tailored%20for%20long-form%0Avideo%20understanding%20tasks.%20The%20code%20will%20be%20released%20later.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18079v1&entry.124074799=Read"},
{"title": "LookWhere? Efficient Visual Recognition by Learning Where to Look and\n  What to See from Self-Supervision", "author": "Anthony Fuller and Yousef Yassin and Junfeng Wen and Daniel G. Kyrollos and Tarek Ibrahim and James R. Green and Evan Shelhamer", "abstract": "  Vision transformers are ever larger, more accurate, and more expensive to\ncompute. The expense is even more extreme at high resolution as the number of\ntokens grows quadratically with the image size. We turn to adaptive computation\nto cope with this cost by learning to predict where to compute. Our LookWhere\nmethod divides the computation between a low-resolution selector and a\nhigh-resolution extractor without ever processing the full high-resolution\ninput. We jointly pretrain the selector and extractor without task supervision\nby distillation from a self-supervised teacher, in effect, learning where and\nwhat to compute simultaneously. Unlike prior token reduction methods, which pay\nto save by pruning already-computed tokens, and prior token selection methods,\nwhich require complex and expensive per-task optimization, LookWhere\neconomically and accurately selects and extracts transferrable representations\nof images. We show that LookWhere excels at sparse recognition on\nhigh-resolution inputs (Traffic Signs), maintaining accuracy while reducing\nFLOPs by up to 34x and time by 6x. It also excels at standard recognition tasks\nthat are global (ImageNet classification) or local (ADE20K segmentation),\nimproving accuracy while reducing time by 1.36x.\n", "link": "http://arxiv.org/abs/2505.18051v1", "date": "2025-05-23", "relevancy": 2.3078, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6076}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5606}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LookWhere%3F%20Efficient%20Visual%20Recognition%20by%20Learning%20Where%20to%20Look%20and%0A%20%20What%20to%20See%20from%20Self-Supervision&body=Title%3A%20LookWhere%3F%20Efficient%20Visual%20Recognition%20by%20Learning%20Where%20to%20Look%20and%0A%20%20What%20to%20See%20from%20Self-Supervision%0AAuthor%3A%20Anthony%20Fuller%20and%20Yousef%20Yassin%20and%20Junfeng%20Wen%20and%20Daniel%20G.%20Kyrollos%20and%20Tarek%20Ibrahim%20and%20James%20R.%20Green%20and%20Evan%20Shelhamer%0AAbstract%3A%20%20%20Vision%20transformers%20are%20ever%20larger%2C%20more%20accurate%2C%20and%20more%20expensive%20to%0Acompute.%20The%20expense%20is%20even%20more%20extreme%20at%20high%20resolution%20as%20the%20number%20of%0Atokens%20grows%20quadratically%20with%20the%20image%20size.%20We%20turn%20to%20adaptive%20computation%0Ato%20cope%20with%20this%20cost%20by%20learning%20to%20predict%20where%20to%20compute.%20Our%20LookWhere%0Amethod%20divides%20the%20computation%20between%20a%20low-resolution%20selector%20and%20a%0Ahigh-resolution%20extractor%20without%20ever%20processing%20the%20full%20high-resolution%0Ainput.%20We%20jointly%20pretrain%20the%20selector%20and%20extractor%20without%20task%20supervision%0Aby%20distillation%20from%20a%20self-supervised%20teacher%2C%20in%20effect%2C%20learning%20where%20and%0Awhat%20to%20compute%20simultaneously.%20Unlike%20prior%20token%20reduction%20methods%2C%20which%20pay%0Ato%20save%20by%20pruning%20already-computed%20tokens%2C%20and%20prior%20token%20selection%20methods%2C%0Awhich%20require%20complex%20and%20expensive%20per-task%20optimization%2C%20LookWhere%0Aeconomically%20and%20accurately%20selects%20and%20extracts%20transferrable%20representations%0Aof%20images.%20We%20show%20that%20LookWhere%20excels%20at%20sparse%20recognition%20on%0Ahigh-resolution%20inputs%20%28Traffic%20Signs%29%2C%20maintaining%20accuracy%20while%20reducing%0AFLOPs%20by%20up%20to%2034x%20and%20time%20by%206x.%20It%20also%20excels%20at%20standard%20recognition%20tasks%0Athat%20are%20global%20%28ImageNet%20classification%29%20or%20local%20%28ADE20K%20segmentation%29%2C%0Aimproving%20accuracy%20while%20reducing%20time%20by%201.36x.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLookWhere%253F%2520Efficient%2520Visual%2520Recognition%2520by%2520Learning%2520Where%2520to%2520Look%2520and%250A%2520%2520What%2520to%2520See%2520from%2520Self-Supervision%26entry.906535625%3DAnthony%2520Fuller%2520and%2520Yousef%2520Yassin%2520and%2520Junfeng%2520Wen%2520and%2520Daniel%2520G.%2520Kyrollos%2520and%2520Tarek%2520Ibrahim%2520and%2520James%2520R.%2520Green%2520and%2520Evan%2520Shelhamer%26entry.1292438233%3D%2520%2520Vision%2520transformers%2520are%2520ever%2520larger%252C%2520more%2520accurate%252C%2520and%2520more%2520expensive%2520to%250Acompute.%2520The%2520expense%2520is%2520even%2520more%2520extreme%2520at%2520high%2520resolution%2520as%2520the%2520number%2520of%250Atokens%2520grows%2520quadratically%2520with%2520the%2520image%2520size.%2520We%2520turn%2520to%2520adaptive%2520computation%250Ato%2520cope%2520with%2520this%2520cost%2520by%2520learning%2520to%2520predict%2520where%2520to%2520compute.%2520Our%2520LookWhere%250Amethod%2520divides%2520the%2520computation%2520between%2520a%2520low-resolution%2520selector%2520and%2520a%250Ahigh-resolution%2520extractor%2520without%2520ever%2520processing%2520the%2520full%2520high-resolution%250Ainput.%2520We%2520jointly%2520pretrain%2520the%2520selector%2520and%2520extractor%2520without%2520task%2520supervision%250Aby%2520distillation%2520from%2520a%2520self-supervised%2520teacher%252C%2520in%2520effect%252C%2520learning%2520where%2520and%250Awhat%2520to%2520compute%2520simultaneously.%2520Unlike%2520prior%2520token%2520reduction%2520methods%252C%2520which%2520pay%250Ato%2520save%2520by%2520pruning%2520already-computed%2520tokens%252C%2520and%2520prior%2520token%2520selection%2520methods%252C%250Awhich%2520require%2520complex%2520and%2520expensive%2520per-task%2520optimization%252C%2520LookWhere%250Aeconomically%2520and%2520accurately%2520selects%2520and%2520extracts%2520transferrable%2520representations%250Aof%2520images.%2520We%2520show%2520that%2520LookWhere%2520excels%2520at%2520sparse%2520recognition%2520on%250Ahigh-resolution%2520inputs%2520%2528Traffic%2520Signs%2529%252C%2520maintaining%2520accuracy%2520while%2520reducing%250AFLOPs%2520by%2520up%2520to%252034x%2520and%2520time%2520by%25206x.%2520It%2520also%2520excels%2520at%2520standard%2520recognition%2520tasks%250Athat%2520are%2520global%2520%2528ImageNet%2520classification%2529%2520or%2520local%2520%2528ADE20K%2520segmentation%2529%252C%250Aimproving%2520accuracy%2520while%2520reducing%2520time%2520by%25201.36x.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LookWhere%3F%20Efficient%20Visual%20Recognition%20by%20Learning%20Where%20to%20Look%20and%0A%20%20What%20to%20See%20from%20Self-Supervision&entry.906535625=Anthony%20Fuller%20and%20Yousef%20Yassin%20and%20Junfeng%20Wen%20and%20Daniel%20G.%20Kyrollos%20and%20Tarek%20Ibrahim%20and%20James%20R.%20Green%20and%20Evan%20Shelhamer&entry.1292438233=%20%20Vision%20transformers%20are%20ever%20larger%2C%20more%20accurate%2C%20and%20more%20expensive%20to%0Acompute.%20The%20expense%20is%20even%20more%20extreme%20at%20high%20resolution%20as%20the%20number%20of%0Atokens%20grows%20quadratically%20with%20the%20image%20size.%20We%20turn%20to%20adaptive%20computation%0Ato%20cope%20with%20this%20cost%20by%20learning%20to%20predict%20where%20to%20compute.%20Our%20LookWhere%0Amethod%20divides%20the%20computation%20between%20a%20low-resolution%20selector%20and%20a%0Ahigh-resolution%20extractor%20without%20ever%20processing%20the%20full%20high-resolution%0Ainput.%20We%20jointly%20pretrain%20the%20selector%20and%20extractor%20without%20task%20supervision%0Aby%20distillation%20from%20a%20self-supervised%20teacher%2C%20in%20effect%2C%20learning%20where%20and%0Awhat%20to%20compute%20simultaneously.%20Unlike%20prior%20token%20reduction%20methods%2C%20which%20pay%0Ato%20save%20by%20pruning%20already-computed%20tokens%2C%20and%20prior%20token%20selection%20methods%2C%0Awhich%20require%20complex%20and%20expensive%20per-task%20optimization%2C%20LookWhere%0Aeconomically%20and%20accurately%20selects%20and%20extracts%20transferrable%20representations%0Aof%20images.%20We%20show%20that%20LookWhere%20excels%20at%20sparse%20recognition%20on%0Ahigh-resolution%20inputs%20%28Traffic%20Signs%29%2C%20maintaining%20accuracy%20while%20reducing%0AFLOPs%20by%20up%20to%2034x%20and%20time%20by%206x.%20It%20also%20excels%20at%20standard%20recognition%20tasks%0Athat%20are%20global%20%28ImageNet%20classification%29%20or%20local%20%28ADE20K%20segmentation%29%2C%0Aimproving%20accuracy%20while%20reducing%20time%20by%201.36x.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18051v1&entry.124074799=Read"},
{"title": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for\n  Autonomous Driving", "author": "Shuang Zeng and Xinyuan Chang and Mengwei Xie and Xinran Liu and Yifan Bai and Zheng Pan and Mu Xu and Xing Wei", "abstract": "  Visual language models (VLMs) have attracted increasing interest in\nautonomous driving due to their powerful reasoning capabilities. However,\nexisting VLMs typically utilize discrete text Chain-of-Thought (CoT) tailored\nto the current scenario, which essentially represents highly abstract and\nsymbolic compression of visual information, potentially leading to\nspatio-temporal relationship ambiguity and fine-grained information loss. Is\nautonomous driving better modeled on real-world simulation and imagination than\non pure symbolic logic? In this paper, we propose a spatio-temporal CoT\nreasoning method that enables models to think visually. First, VLM serves as a\nworld model to generate unified image frame for predicting future world states:\nwhere perception results (e.g., lane divider and 3D detection) represent the\nfuture spatial relationships, and ordinary future frame represent the temporal\nevolution relationships. This spatio-temporal CoT then serves as intermediate\nreasoning steps, enabling the VLM to function as an inverse dynamics model for\ntrajectory planning based on current observations and future predictions. To\nimplement visual generation in VLMs, we propose a unified pretraining paradigm\nintegrating visual generation and understanding, along with a progressive\nvisual CoT enhancing autoregressive image generation. Extensive experimental\nresults demonstrate the effectiveness of the proposed method, advancing\nautonomous driving towards visual reasoning.\n", "link": "http://arxiv.org/abs/2505.17685v1", "date": "2025-05-23", "relevancy": 2.3018, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5764}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5764}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FutureSightDrive%3A%20Thinking%20Visually%20with%20Spatio-Temporal%20CoT%20for%0A%20%20Autonomous%20Driving&body=Title%3A%20FutureSightDrive%3A%20Thinking%20Visually%20with%20Spatio-Temporal%20CoT%20for%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Shuang%20Zeng%20and%20Xinyuan%20Chang%20and%20Mengwei%20Xie%20and%20Xinran%20Liu%20and%20Yifan%20Bai%20and%20Zheng%20Pan%20and%20Mu%20Xu%20and%20Xing%20Wei%0AAbstract%3A%20%20%20Visual%20language%20models%20%28VLMs%29%20have%20attracted%20increasing%20interest%20in%0Aautonomous%20driving%20due%20to%20their%20powerful%20reasoning%20capabilities.%20However%2C%0Aexisting%20VLMs%20typically%20utilize%20discrete%20text%20Chain-of-Thought%20%28CoT%29%20tailored%0Ato%20the%20current%20scenario%2C%20which%20essentially%20represents%20highly%20abstract%20and%0Asymbolic%20compression%20of%20visual%20information%2C%20potentially%20leading%20to%0Aspatio-temporal%20relationship%20ambiguity%20and%20fine-grained%20information%20loss.%20Is%0Aautonomous%20driving%20better%20modeled%20on%20real-world%20simulation%20and%20imagination%20than%0Aon%20pure%20symbolic%20logic%3F%20In%20this%20paper%2C%20we%20propose%20a%20spatio-temporal%20CoT%0Areasoning%20method%20that%20enables%20models%20to%20think%20visually.%20First%2C%20VLM%20serves%20as%20a%0Aworld%20model%20to%20generate%20unified%20image%20frame%20for%20predicting%20future%20world%20states%3A%0Awhere%20perception%20results%20%28e.g.%2C%20lane%20divider%20and%203D%20detection%29%20represent%20the%0Afuture%20spatial%20relationships%2C%20and%20ordinary%20future%20frame%20represent%20the%20temporal%0Aevolution%20relationships.%20This%20spatio-temporal%20CoT%20then%20serves%20as%20intermediate%0Areasoning%20steps%2C%20enabling%20the%20VLM%20to%20function%20as%20an%20inverse%20dynamics%20model%20for%0Atrajectory%20planning%20based%20on%20current%20observations%20and%20future%20predictions.%20To%0Aimplement%20visual%20generation%20in%20VLMs%2C%20we%20propose%20a%20unified%20pretraining%20paradigm%0Aintegrating%20visual%20generation%20and%20understanding%2C%20along%20with%20a%20progressive%0Avisual%20CoT%20enhancing%20autoregressive%20image%20generation.%20Extensive%20experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%2C%20advancing%0Aautonomous%20driving%20towards%20visual%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFutureSightDrive%253A%2520Thinking%2520Visually%2520with%2520Spatio-Temporal%2520CoT%2520for%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DShuang%2520Zeng%2520and%2520Xinyuan%2520Chang%2520and%2520Mengwei%2520Xie%2520and%2520Xinran%2520Liu%2520and%2520Yifan%2520Bai%2520and%2520Zheng%2520Pan%2520and%2520Mu%2520Xu%2520and%2520Xing%2520Wei%26entry.1292438233%3D%2520%2520Visual%2520language%2520models%2520%2528VLMs%2529%2520have%2520attracted%2520increasing%2520interest%2520in%250Aautonomous%2520driving%2520due%2520to%2520their%2520powerful%2520reasoning%2520capabilities.%2520However%252C%250Aexisting%2520VLMs%2520typically%2520utilize%2520discrete%2520text%2520Chain-of-Thought%2520%2528CoT%2529%2520tailored%250Ato%2520the%2520current%2520scenario%252C%2520which%2520essentially%2520represents%2520highly%2520abstract%2520and%250Asymbolic%2520compression%2520of%2520visual%2520information%252C%2520potentially%2520leading%2520to%250Aspatio-temporal%2520relationship%2520ambiguity%2520and%2520fine-grained%2520information%2520loss.%2520Is%250Aautonomous%2520driving%2520better%2520modeled%2520on%2520real-world%2520simulation%2520and%2520imagination%2520than%250Aon%2520pure%2520symbolic%2520logic%253F%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520spatio-temporal%2520CoT%250Areasoning%2520method%2520that%2520enables%2520models%2520to%2520think%2520visually.%2520First%252C%2520VLM%2520serves%2520as%2520a%250Aworld%2520model%2520to%2520generate%2520unified%2520image%2520frame%2520for%2520predicting%2520future%2520world%2520states%253A%250Awhere%2520perception%2520results%2520%2528e.g.%252C%2520lane%2520divider%2520and%25203D%2520detection%2529%2520represent%2520the%250Afuture%2520spatial%2520relationships%252C%2520and%2520ordinary%2520future%2520frame%2520represent%2520the%2520temporal%250Aevolution%2520relationships.%2520This%2520spatio-temporal%2520CoT%2520then%2520serves%2520as%2520intermediate%250Areasoning%2520steps%252C%2520enabling%2520the%2520VLM%2520to%2520function%2520as%2520an%2520inverse%2520dynamics%2520model%2520for%250Atrajectory%2520planning%2520based%2520on%2520current%2520observations%2520and%2520future%2520predictions.%2520To%250Aimplement%2520visual%2520generation%2520in%2520VLMs%252C%2520we%2520propose%2520a%2520unified%2520pretraining%2520paradigm%250Aintegrating%2520visual%2520generation%2520and%2520understanding%252C%2520along%2520with%2520a%2520progressive%250Avisual%2520CoT%2520enhancing%2520autoregressive%2520image%2520generation.%2520Extensive%2520experimental%250Aresults%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%2520advancing%250Aautonomous%2520driving%2520towards%2520visual%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FutureSightDrive%3A%20Thinking%20Visually%20with%20Spatio-Temporal%20CoT%20for%0A%20%20Autonomous%20Driving&entry.906535625=Shuang%20Zeng%20and%20Xinyuan%20Chang%20and%20Mengwei%20Xie%20and%20Xinran%20Liu%20and%20Yifan%20Bai%20and%20Zheng%20Pan%20and%20Mu%20Xu%20and%20Xing%20Wei&entry.1292438233=%20%20Visual%20language%20models%20%28VLMs%29%20have%20attracted%20increasing%20interest%20in%0Aautonomous%20driving%20due%20to%20their%20powerful%20reasoning%20capabilities.%20However%2C%0Aexisting%20VLMs%20typically%20utilize%20discrete%20text%20Chain-of-Thought%20%28CoT%29%20tailored%0Ato%20the%20current%20scenario%2C%20which%20essentially%20represents%20highly%20abstract%20and%0Asymbolic%20compression%20of%20visual%20information%2C%20potentially%20leading%20to%0Aspatio-temporal%20relationship%20ambiguity%20and%20fine-grained%20information%20loss.%20Is%0Aautonomous%20driving%20better%20modeled%20on%20real-world%20simulation%20and%20imagination%20than%0Aon%20pure%20symbolic%20logic%3F%20In%20this%20paper%2C%20we%20propose%20a%20spatio-temporal%20CoT%0Areasoning%20method%20that%20enables%20models%20to%20think%20visually.%20First%2C%20VLM%20serves%20as%20a%0Aworld%20model%20to%20generate%20unified%20image%20frame%20for%20predicting%20future%20world%20states%3A%0Awhere%20perception%20results%20%28e.g.%2C%20lane%20divider%20and%203D%20detection%29%20represent%20the%0Afuture%20spatial%20relationships%2C%20and%20ordinary%20future%20frame%20represent%20the%20temporal%0Aevolution%20relationships.%20This%20spatio-temporal%20CoT%20then%20serves%20as%20intermediate%0Areasoning%20steps%2C%20enabling%20the%20VLM%20to%20function%20as%20an%20inverse%20dynamics%20model%20for%0Atrajectory%20planning%20based%20on%20current%20observations%20and%20future%20predictions.%20To%0Aimplement%20visual%20generation%20in%20VLMs%2C%20we%20propose%20a%20unified%20pretraining%20paradigm%0Aintegrating%20visual%20generation%20and%20understanding%2C%20along%20with%20a%20progressive%0Avisual%20CoT%20enhancing%20autoregressive%20image%20generation.%20Extensive%20experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%2C%20advancing%0Aautonomous%20driving%20towards%20visual%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17685v1&entry.124074799=Read"},
{"title": "ICPL-ReID: Identity-Conditional Prompt Learning for Multi-Spectral\n  Object Re-Identification", "author": "Shihao Li and Chenglong Li and Aihua Zheng and Jin Tang and Bin Luo", "abstract": "  Multi-spectral object re-identification (ReID) brings a new perception\nperspective for smart city and intelligent transportation applications,\neffectively addressing challenges from complex illumination and adverse\nweather. However, complex modal differences between heterogeneous spectra pose\nchallenges to efficiently utilizing complementary and discrepancy of spectra\ninformation. Most existing methods fuse spectral data through intricate modal\ninteraction modules, lacking fine-grained semantic understanding of spectral\ninformation (\\textit{e.g.}, text descriptions, part masks, and object\nkeypoints). To solve this challenge, we propose a novel Identity-Conditional\ntext Prompt Learning framework (ICPL), which exploits the powerful cross-modal\nalignment capability of CLIP, to unify different spectral visual features from\ntext semantics. Specifically, we first propose the online prompt learning using\nlearnable text prompt as the identity-level semantic center to bridge the\nidentity semantics of different spectra in online manner. Then, in lack of\nconcrete text descriptions, we propose the multi-spectral identity-condition\nmodule to use identity prototype as spectral identity condition to constraint\nprompt learning. Meanwhile, we construct the alignment loop mutually optimizing\nthe learnable text prompt and spectral visual encoder to avoid online prompt\nlearning disrupting the pre-trained text-image alignment distribution. In\naddition, to adapt to small-scale multi-spectral data and mitigate style\ndifferences between spectra, we propose multi-spectral adapter that employs a\nlow-rank adaption method to learn spectra-specific features. Comprehensive\nexperiments on 5 benchmarks, including RGBNT201, Market-MM, MSVR310, RGBN300,\nand RGBNT100, demonstrate that the proposed method outperforms the\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2505.17821v1", "date": "2025-05-23", "relevancy": 2.3007, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5814}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5759}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ICPL-ReID%3A%20Identity-Conditional%20Prompt%20Learning%20for%20Multi-Spectral%0A%20%20Object%20Re-Identification&body=Title%3A%20ICPL-ReID%3A%20Identity-Conditional%20Prompt%20Learning%20for%20Multi-Spectral%0A%20%20Object%20Re-Identification%0AAuthor%3A%20Shihao%20Li%20and%20Chenglong%20Li%20and%20Aihua%20Zheng%20and%20Jin%20Tang%20and%20Bin%20Luo%0AAbstract%3A%20%20%20Multi-spectral%20object%20re-identification%20%28ReID%29%20brings%20a%20new%20perception%0Aperspective%20for%20smart%20city%20and%20intelligent%20transportation%20applications%2C%0Aeffectively%20addressing%20challenges%20from%20complex%20illumination%20and%20adverse%0Aweather.%20However%2C%20complex%20modal%20differences%20between%20heterogeneous%20spectra%20pose%0Achallenges%20to%20efficiently%20utilizing%20complementary%20and%20discrepancy%20of%20spectra%0Ainformation.%20Most%20existing%20methods%20fuse%20spectral%20data%20through%20intricate%20modal%0Ainteraction%20modules%2C%20lacking%20fine-grained%20semantic%20understanding%20of%20spectral%0Ainformation%20%28%5Ctextit%7Be.g.%7D%2C%20text%20descriptions%2C%20part%20masks%2C%20and%20object%0Akeypoints%29.%20To%20solve%20this%20challenge%2C%20we%20propose%20a%20novel%20Identity-Conditional%0Atext%20Prompt%20Learning%20framework%20%28ICPL%29%2C%20which%20exploits%20the%20powerful%20cross-modal%0Aalignment%20capability%20of%20CLIP%2C%20to%20unify%20different%20spectral%20visual%20features%20from%0Atext%20semantics.%20Specifically%2C%20we%20first%20propose%20the%20online%20prompt%20learning%20using%0Alearnable%20text%20prompt%20as%20the%20identity-level%20semantic%20center%20to%20bridge%20the%0Aidentity%20semantics%20of%20different%20spectra%20in%20online%20manner.%20Then%2C%20in%20lack%20of%0Aconcrete%20text%20descriptions%2C%20we%20propose%20the%20multi-spectral%20identity-condition%0Amodule%20to%20use%20identity%20prototype%20as%20spectral%20identity%20condition%20to%20constraint%0Aprompt%20learning.%20Meanwhile%2C%20we%20construct%20the%20alignment%20loop%20mutually%20optimizing%0Athe%20learnable%20text%20prompt%20and%20spectral%20visual%20encoder%20to%20avoid%20online%20prompt%0Alearning%20disrupting%20the%20pre-trained%20text-image%20alignment%20distribution.%20In%0Aaddition%2C%20to%20adapt%20to%20small-scale%20multi-spectral%20data%20and%20mitigate%20style%0Adifferences%20between%20spectra%2C%20we%20propose%20multi-spectral%20adapter%20that%20employs%20a%0Alow-rank%20adaption%20method%20to%20learn%20spectra-specific%20features.%20Comprehensive%0Aexperiments%20on%205%20benchmarks%2C%20including%20RGBNT201%2C%20Market-MM%2C%20MSVR310%2C%20RGBN300%2C%0Aand%20RGBNT100%2C%20demonstrate%20that%20the%20proposed%20method%20outperforms%20the%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DICPL-ReID%253A%2520Identity-Conditional%2520Prompt%2520Learning%2520for%2520Multi-Spectral%250A%2520%2520Object%2520Re-Identification%26entry.906535625%3DShihao%2520Li%2520and%2520Chenglong%2520Li%2520and%2520Aihua%2520Zheng%2520and%2520Jin%2520Tang%2520and%2520Bin%2520Luo%26entry.1292438233%3D%2520%2520Multi-spectral%2520object%2520re-identification%2520%2528ReID%2529%2520brings%2520a%2520new%2520perception%250Aperspective%2520for%2520smart%2520city%2520and%2520intelligent%2520transportation%2520applications%252C%250Aeffectively%2520addressing%2520challenges%2520from%2520complex%2520illumination%2520and%2520adverse%250Aweather.%2520However%252C%2520complex%2520modal%2520differences%2520between%2520heterogeneous%2520spectra%2520pose%250Achallenges%2520to%2520efficiently%2520utilizing%2520complementary%2520and%2520discrepancy%2520of%2520spectra%250Ainformation.%2520Most%2520existing%2520methods%2520fuse%2520spectral%2520data%2520through%2520intricate%2520modal%250Ainteraction%2520modules%252C%2520lacking%2520fine-grained%2520semantic%2520understanding%2520of%2520spectral%250Ainformation%2520%2528%255Ctextit%257Be.g.%257D%252C%2520text%2520descriptions%252C%2520part%2520masks%252C%2520and%2520object%250Akeypoints%2529.%2520To%2520solve%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520Identity-Conditional%250Atext%2520Prompt%2520Learning%2520framework%2520%2528ICPL%2529%252C%2520which%2520exploits%2520the%2520powerful%2520cross-modal%250Aalignment%2520capability%2520of%2520CLIP%252C%2520to%2520unify%2520different%2520spectral%2520visual%2520features%2520from%250Atext%2520semantics.%2520Specifically%252C%2520we%2520first%2520propose%2520the%2520online%2520prompt%2520learning%2520using%250Alearnable%2520text%2520prompt%2520as%2520the%2520identity-level%2520semantic%2520center%2520to%2520bridge%2520the%250Aidentity%2520semantics%2520of%2520different%2520spectra%2520in%2520online%2520manner.%2520Then%252C%2520in%2520lack%2520of%250Aconcrete%2520text%2520descriptions%252C%2520we%2520propose%2520the%2520multi-spectral%2520identity-condition%250Amodule%2520to%2520use%2520identity%2520prototype%2520as%2520spectral%2520identity%2520condition%2520to%2520constraint%250Aprompt%2520learning.%2520Meanwhile%252C%2520we%2520construct%2520the%2520alignment%2520loop%2520mutually%2520optimizing%250Athe%2520learnable%2520text%2520prompt%2520and%2520spectral%2520visual%2520encoder%2520to%2520avoid%2520online%2520prompt%250Alearning%2520disrupting%2520the%2520pre-trained%2520text-image%2520alignment%2520distribution.%2520In%250Aaddition%252C%2520to%2520adapt%2520to%2520small-scale%2520multi-spectral%2520data%2520and%2520mitigate%2520style%250Adifferences%2520between%2520spectra%252C%2520we%2520propose%2520multi-spectral%2520adapter%2520that%2520employs%2520a%250Alow-rank%2520adaption%2520method%2520to%2520learn%2520spectra-specific%2520features.%2520Comprehensive%250Aexperiments%2520on%25205%2520benchmarks%252C%2520including%2520RGBNT201%252C%2520Market-MM%252C%2520MSVR310%252C%2520RGBN300%252C%250Aand%2520RGBNT100%252C%2520demonstrate%2520that%2520the%2520proposed%2520method%2520outperforms%2520the%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ICPL-ReID%3A%20Identity-Conditional%20Prompt%20Learning%20for%20Multi-Spectral%0A%20%20Object%20Re-Identification&entry.906535625=Shihao%20Li%20and%20Chenglong%20Li%20and%20Aihua%20Zheng%20and%20Jin%20Tang%20and%20Bin%20Luo&entry.1292438233=%20%20Multi-spectral%20object%20re-identification%20%28ReID%29%20brings%20a%20new%20perception%0Aperspective%20for%20smart%20city%20and%20intelligent%20transportation%20applications%2C%0Aeffectively%20addressing%20challenges%20from%20complex%20illumination%20and%20adverse%0Aweather.%20However%2C%20complex%20modal%20differences%20between%20heterogeneous%20spectra%20pose%0Achallenges%20to%20efficiently%20utilizing%20complementary%20and%20discrepancy%20of%20spectra%0Ainformation.%20Most%20existing%20methods%20fuse%20spectral%20data%20through%20intricate%20modal%0Ainteraction%20modules%2C%20lacking%20fine-grained%20semantic%20understanding%20of%20spectral%0Ainformation%20%28%5Ctextit%7Be.g.%7D%2C%20text%20descriptions%2C%20part%20masks%2C%20and%20object%0Akeypoints%29.%20To%20solve%20this%20challenge%2C%20we%20propose%20a%20novel%20Identity-Conditional%0Atext%20Prompt%20Learning%20framework%20%28ICPL%29%2C%20which%20exploits%20the%20powerful%20cross-modal%0Aalignment%20capability%20of%20CLIP%2C%20to%20unify%20different%20spectral%20visual%20features%20from%0Atext%20semantics.%20Specifically%2C%20we%20first%20propose%20the%20online%20prompt%20learning%20using%0Alearnable%20text%20prompt%20as%20the%20identity-level%20semantic%20center%20to%20bridge%20the%0Aidentity%20semantics%20of%20different%20spectra%20in%20online%20manner.%20Then%2C%20in%20lack%20of%0Aconcrete%20text%20descriptions%2C%20we%20propose%20the%20multi-spectral%20identity-condition%0Amodule%20to%20use%20identity%20prototype%20as%20spectral%20identity%20condition%20to%20constraint%0Aprompt%20learning.%20Meanwhile%2C%20we%20construct%20the%20alignment%20loop%20mutually%20optimizing%0Athe%20learnable%20text%20prompt%20and%20spectral%20visual%20encoder%20to%20avoid%20online%20prompt%0Alearning%20disrupting%20the%20pre-trained%20text-image%20alignment%20distribution.%20In%0Aaddition%2C%20to%20adapt%20to%20small-scale%20multi-spectral%20data%20and%20mitigate%20style%0Adifferences%20between%20spectra%2C%20we%20propose%20multi-spectral%20adapter%20that%20employs%20a%0Alow-rank%20adaption%20method%20to%20learn%20spectra-specific%20features.%20Comprehensive%0Aexperiments%20on%205%20benchmarks%2C%20including%20RGBNT201%2C%20Market-MM%2C%20MSVR310%2C%20RGBN300%2C%0Aand%20RGBNT100%2C%20demonstrate%20that%20the%20proposed%20method%20outperforms%20the%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17821v1&entry.124074799=Read"},
{"title": "Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and\n  Foundation-Model Signatures Across Multicentre NSCLC Data", "author": "Shruti Atul Mali and Zohaib Salahuddin and Danial Khan and Yumeng Zhang and Henry C. Woodruff and Eduardo Ibor-Crespo and Ana Jimenez-Pastor and Luis Marti-Bonmati and Philippe Lambin", "abstract": "  Purpose: To evaluate the impact of harmonization and multi-region CT image\nfeature integration on survival prediction in non-small cell lung cancer\n(NSCLC) patients, using handcrafted radiomics, pretrained foundation model (FM)\nfeatures, and clinical data from a multicenter dataset.\n  Methods: We analyzed CT scans and clinical data from 876 NSCLC patients (604\ntraining, 272 test) across five centers. Features were extracted from the whole\nlung, tumor, mediastinal nodes, coronary arteries, and coronary artery calcium\n(CAC). Handcrafted radiomics and FM deep features were harmonized using ComBat,\nreconstruction kernel normalization (RKN), and RKN+ComBat. Regularized Cox\nmodels predicted overall survival; performance was assessed using the\nconcordance index (C-index), 5-year time-dependent area under the curve\n(t-AUC), and hazard ratio (HR). SHapley Additive exPlanations (SHAP) values\nexplained feature contributions. A consensus model used agreement across top\nregion of interest (ROI) models to stratify patient risk.\n  Results: TNM staging showed prognostic utility (C-index = 0.67; HR = 2.70;\nt-AUC = 0.85). The clinical + tumor radiomics model with ComBat achieved a\nC-index of 0.7552 and t-AUC of 0.8820. FM features (50-voxel cubes) combined\nwith clinical data yielded the highest performance (C-index = 0.7616; t-AUC =\n0.8866). An ensemble of all ROIs and FM features reached a C-index of 0.7142\nand t-AUC of 0.7885. The consensus model, covering 78% of valid test cases,\nachieved a t-AUC of 0.92, sensitivity of 97.6%, and specificity of 66.7%.\n  Conclusion: Harmonization and multi-region feature integration improve\nsurvival prediction in multicenter NSCLC data. Combining interpretable\nradiomics, FM features, and consensus modeling enables robust risk\nstratification across imaging centers.\n", "link": "http://arxiv.org/abs/2505.17893v1", "date": "2025-05-23", "relevancy": 2.3006, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4786}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixels%20to%20Prognosis%3A%20Harmonized%20Multi-Region%20CT-Radiomics%20and%0A%20%20Foundation-Model%20Signatures%20Across%20Multicentre%20NSCLC%20Data&body=Title%3A%20Pixels%20to%20Prognosis%3A%20Harmonized%20Multi-Region%20CT-Radiomics%20and%0A%20%20Foundation-Model%20Signatures%20Across%20Multicentre%20NSCLC%20Data%0AAuthor%3A%20Shruti%20Atul%20Mali%20and%20Zohaib%20Salahuddin%20and%20Danial%20Khan%20and%20Yumeng%20Zhang%20and%20Henry%20C.%20Woodruff%20and%20Eduardo%20Ibor-Crespo%20and%20Ana%20Jimenez-Pastor%20and%20Luis%20Marti-Bonmati%20and%20Philippe%20Lambin%0AAbstract%3A%20%20%20Purpose%3A%20To%20evaluate%20the%20impact%20of%20harmonization%20and%20multi-region%20CT%20image%0Afeature%20integration%20on%20survival%20prediction%20in%20non-small%20cell%20lung%20cancer%0A%28NSCLC%29%20patients%2C%20using%20handcrafted%20radiomics%2C%20pretrained%20foundation%20model%20%28FM%29%0Afeatures%2C%20and%20clinical%20data%20from%20a%20multicenter%20dataset.%0A%20%20Methods%3A%20We%20analyzed%20CT%20scans%20and%20clinical%20data%20from%20876%20NSCLC%20patients%20%28604%0Atraining%2C%20272%20test%29%20across%20five%20centers.%20Features%20were%20extracted%20from%20the%20whole%0Alung%2C%20tumor%2C%20mediastinal%20nodes%2C%20coronary%20arteries%2C%20and%20coronary%20artery%20calcium%0A%28CAC%29.%20Handcrafted%20radiomics%20and%20FM%20deep%20features%20were%20harmonized%20using%20ComBat%2C%0Areconstruction%20kernel%20normalization%20%28RKN%29%2C%20and%20RKN%2BComBat.%20Regularized%20Cox%0Amodels%20predicted%20overall%20survival%3B%20performance%20was%20assessed%20using%20the%0Aconcordance%20index%20%28C-index%29%2C%205-year%20time-dependent%20area%20under%20the%20curve%0A%28t-AUC%29%2C%20and%20hazard%20ratio%20%28HR%29.%20SHapley%20Additive%20exPlanations%20%28SHAP%29%20values%0Aexplained%20feature%20contributions.%20A%20consensus%20model%20used%20agreement%20across%20top%0Aregion%20of%20interest%20%28ROI%29%20models%20to%20stratify%20patient%20risk.%0A%20%20Results%3A%20TNM%20staging%20showed%20prognostic%20utility%20%28C-index%20%3D%200.67%3B%20HR%20%3D%202.70%3B%0At-AUC%20%3D%200.85%29.%20The%20clinical%20%2B%20tumor%20radiomics%20model%20with%20ComBat%20achieved%20a%0AC-index%20of%200.7552%20and%20t-AUC%20of%200.8820.%20FM%20features%20%2850-voxel%20cubes%29%20combined%0Awith%20clinical%20data%20yielded%20the%20highest%20performance%20%28C-index%20%3D%200.7616%3B%20t-AUC%20%3D%0A0.8866%29.%20An%20ensemble%20of%20all%20ROIs%20and%20FM%20features%20reached%20a%20C-index%20of%200.7142%0Aand%20t-AUC%20of%200.7885.%20The%20consensus%20model%2C%20covering%2078%25%20of%20valid%20test%20cases%2C%0Aachieved%20a%20t-AUC%20of%200.92%2C%20sensitivity%20of%2097.6%25%2C%20and%20specificity%20of%2066.7%25.%0A%20%20Conclusion%3A%20Harmonization%20and%20multi-region%20feature%20integration%20improve%0Asurvival%20prediction%20in%20multicenter%20NSCLC%20data.%20Combining%20interpretable%0Aradiomics%2C%20FM%20features%2C%20and%20consensus%20modeling%20enables%20robust%20risk%0Astratification%20across%20imaging%20centers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17893v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixels%2520to%2520Prognosis%253A%2520Harmonized%2520Multi-Region%2520CT-Radiomics%2520and%250A%2520%2520Foundation-Model%2520Signatures%2520Across%2520Multicentre%2520NSCLC%2520Data%26entry.906535625%3DShruti%2520Atul%2520Mali%2520and%2520Zohaib%2520Salahuddin%2520and%2520Danial%2520Khan%2520and%2520Yumeng%2520Zhang%2520and%2520Henry%2520C.%2520Woodruff%2520and%2520Eduardo%2520Ibor-Crespo%2520and%2520Ana%2520Jimenez-Pastor%2520and%2520Luis%2520Marti-Bonmati%2520and%2520Philippe%2520Lambin%26entry.1292438233%3D%2520%2520Purpose%253A%2520To%2520evaluate%2520the%2520impact%2520of%2520harmonization%2520and%2520multi-region%2520CT%2520image%250Afeature%2520integration%2520on%2520survival%2520prediction%2520in%2520non-small%2520cell%2520lung%2520cancer%250A%2528NSCLC%2529%2520patients%252C%2520using%2520handcrafted%2520radiomics%252C%2520pretrained%2520foundation%2520model%2520%2528FM%2529%250Afeatures%252C%2520and%2520clinical%2520data%2520from%2520a%2520multicenter%2520dataset.%250A%2520%2520Methods%253A%2520We%2520analyzed%2520CT%2520scans%2520and%2520clinical%2520data%2520from%2520876%2520NSCLC%2520patients%2520%2528604%250Atraining%252C%2520272%2520test%2529%2520across%2520five%2520centers.%2520Features%2520were%2520extracted%2520from%2520the%2520whole%250Alung%252C%2520tumor%252C%2520mediastinal%2520nodes%252C%2520coronary%2520arteries%252C%2520and%2520coronary%2520artery%2520calcium%250A%2528CAC%2529.%2520Handcrafted%2520radiomics%2520and%2520FM%2520deep%2520features%2520were%2520harmonized%2520using%2520ComBat%252C%250Areconstruction%2520kernel%2520normalization%2520%2528RKN%2529%252C%2520and%2520RKN%252BComBat.%2520Regularized%2520Cox%250Amodels%2520predicted%2520overall%2520survival%253B%2520performance%2520was%2520assessed%2520using%2520the%250Aconcordance%2520index%2520%2528C-index%2529%252C%25205-year%2520time-dependent%2520area%2520under%2520the%2520curve%250A%2528t-AUC%2529%252C%2520and%2520hazard%2520ratio%2520%2528HR%2529.%2520SHapley%2520Additive%2520exPlanations%2520%2528SHAP%2529%2520values%250Aexplained%2520feature%2520contributions.%2520A%2520consensus%2520model%2520used%2520agreement%2520across%2520top%250Aregion%2520of%2520interest%2520%2528ROI%2529%2520models%2520to%2520stratify%2520patient%2520risk.%250A%2520%2520Results%253A%2520TNM%2520staging%2520showed%2520prognostic%2520utility%2520%2528C-index%2520%253D%25200.67%253B%2520HR%2520%253D%25202.70%253B%250At-AUC%2520%253D%25200.85%2529.%2520The%2520clinical%2520%252B%2520tumor%2520radiomics%2520model%2520with%2520ComBat%2520achieved%2520a%250AC-index%2520of%25200.7552%2520and%2520t-AUC%2520of%25200.8820.%2520FM%2520features%2520%252850-voxel%2520cubes%2529%2520combined%250Awith%2520clinical%2520data%2520yielded%2520the%2520highest%2520performance%2520%2528C-index%2520%253D%25200.7616%253B%2520t-AUC%2520%253D%250A0.8866%2529.%2520An%2520ensemble%2520of%2520all%2520ROIs%2520and%2520FM%2520features%2520reached%2520a%2520C-index%2520of%25200.7142%250Aand%2520t-AUC%2520of%25200.7885.%2520The%2520consensus%2520model%252C%2520covering%252078%2525%2520of%2520valid%2520test%2520cases%252C%250Aachieved%2520a%2520t-AUC%2520of%25200.92%252C%2520sensitivity%2520of%252097.6%2525%252C%2520and%2520specificity%2520of%252066.7%2525.%250A%2520%2520Conclusion%253A%2520Harmonization%2520and%2520multi-region%2520feature%2520integration%2520improve%250Asurvival%2520prediction%2520in%2520multicenter%2520NSCLC%2520data.%2520Combining%2520interpretable%250Aradiomics%252C%2520FM%2520features%252C%2520and%2520consensus%2520modeling%2520enables%2520robust%2520risk%250Astratification%2520across%2520imaging%2520centers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17893v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixels%20to%20Prognosis%3A%20Harmonized%20Multi-Region%20CT-Radiomics%20and%0A%20%20Foundation-Model%20Signatures%20Across%20Multicentre%20NSCLC%20Data&entry.906535625=Shruti%20Atul%20Mali%20and%20Zohaib%20Salahuddin%20and%20Danial%20Khan%20and%20Yumeng%20Zhang%20and%20Henry%20C.%20Woodruff%20and%20Eduardo%20Ibor-Crespo%20and%20Ana%20Jimenez-Pastor%20and%20Luis%20Marti-Bonmati%20and%20Philippe%20Lambin&entry.1292438233=%20%20Purpose%3A%20To%20evaluate%20the%20impact%20of%20harmonization%20and%20multi-region%20CT%20image%0Afeature%20integration%20on%20survival%20prediction%20in%20non-small%20cell%20lung%20cancer%0A%28NSCLC%29%20patients%2C%20using%20handcrafted%20radiomics%2C%20pretrained%20foundation%20model%20%28FM%29%0Afeatures%2C%20and%20clinical%20data%20from%20a%20multicenter%20dataset.%0A%20%20Methods%3A%20We%20analyzed%20CT%20scans%20and%20clinical%20data%20from%20876%20NSCLC%20patients%20%28604%0Atraining%2C%20272%20test%29%20across%20five%20centers.%20Features%20were%20extracted%20from%20the%20whole%0Alung%2C%20tumor%2C%20mediastinal%20nodes%2C%20coronary%20arteries%2C%20and%20coronary%20artery%20calcium%0A%28CAC%29.%20Handcrafted%20radiomics%20and%20FM%20deep%20features%20were%20harmonized%20using%20ComBat%2C%0Areconstruction%20kernel%20normalization%20%28RKN%29%2C%20and%20RKN%2BComBat.%20Regularized%20Cox%0Amodels%20predicted%20overall%20survival%3B%20performance%20was%20assessed%20using%20the%0Aconcordance%20index%20%28C-index%29%2C%205-year%20time-dependent%20area%20under%20the%20curve%0A%28t-AUC%29%2C%20and%20hazard%20ratio%20%28HR%29.%20SHapley%20Additive%20exPlanations%20%28SHAP%29%20values%0Aexplained%20feature%20contributions.%20A%20consensus%20model%20used%20agreement%20across%20top%0Aregion%20of%20interest%20%28ROI%29%20models%20to%20stratify%20patient%20risk.%0A%20%20Results%3A%20TNM%20staging%20showed%20prognostic%20utility%20%28C-index%20%3D%200.67%3B%20HR%20%3D%202.70%3B%0At-AUC%20%3D%200.85%29.%20The%20clinical%20%2B%20tumor%20radiomics%20model%20with%20ComBat%20achieved%20a%0AC-index%20of%200.7552%20and%20t-AUC%20of%200.8820.%20FM%20features%20%2850-voxel%20cubes%29%20combined%0Awith%20clinical%20data%20yielded%20the%20highest%20performance%20%28C-index%20%3D%200.7616%3B%20t-AUC%20%3D%0A0.8866%29.%20An%20ensemble%20of%20all%20ROIs%20and%20FM%20features%20reached%20a%20C-index%20of%200.7142%0Aand%20t-AUC%20of%200.7885.%20The%20consensus%20model%2C%20covering%2078%25%20of%20valid%20test%20cases%2C%0Aachieved%20a%20t-AUC%20of%200.92%2C%20sensitivity%20of%2097.6%25%2C%20and%20specificity%20of%2066.7%25.%0A%20%20Conclusion%3A%20Harmonization%20and%20multi-region%20feature%20integration%20improve%0Asurvival%20prediction%20in%20multicenter%20NSCLC%20data.%20Combining%20interpretable%0Aradiomics%2C%20FM%20features%2C%20and%20consensus%20modeling%20enables%20robust%20risk%0Astratification%20across%20imaging%20centers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17893v1&entry.124074799=Read"},
{"title": "Accelerating Learned Image Compression Through Modeling Neural Training\n  Dynamics", "author": "Yichi Zhang and Zhihao Duan and Yuning Huang and Fengqing Zhu", "abstract": "  As learned image compression (LIC) methods become increasingly\ncomputationally demanding, enhancing their training efficiency is crucial. This\npaper takes a step forward in accelerating the training of LIC methods by\nmodeling the neural training dynamics. We first propose a Sensitivity-aware\nTrue and Dummy Embedding Training mechanism (STDET) that clusters LIC model\nparameters into few separate modes where parameters are expressed as affine\ntransformations of reference parameters within the same mode. By further\nutilizing the stable intra-mode correlations throughout training and parameter\nsensitivities, we gradually embed non-reference parameters, reducing the number\nof trainable parameters. Additionally, we incorporate a Sampling-then-Moving\nAverage (SMA) technique, interpolating sampled weights from stochastic gradient\ndescent (SGD) training to obtain the moving average weights, ensuring smooth\ntemporal behavior and minimizing training state variances. Overall, our method\nsignificantly reduces training space dimensions and the number of trainable\nparameters without sacrificing model performance, thus accelerating model\nconvergence. We also provide a theoretical analysis on the Noisy quadratic\nmodel, showing that the proposed method achieves a lower training variance than\nstandard SGD. Our approach offers valuable insights for further developing\nefficient training methods for LICs.\n", "link": "http://arxiv.org/abs/2505.18107v1", "date": "2025-05-23", "relevancy": 2.2967, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.629}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5639}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Learned%20Image%20Compression%20Through%20Modeling%20Neural%20Training%0A%20%20Dynamics&body=Title%3A%20Accelerating%20Learned%20Image%20Compression%20Through%20Modeling%20Neural%20Training%0A%20%20Dynamics%0AAuthor%3A%20Yichi%20Zhang%20and%20Zhihao%20Duan%20and%20Yuning%20Huang%20and%20Fengqing%20Zhu%0AAbstract%3A%20%20%20As%20learned%20image%20compression%20%28LIC%29%20methods%20become%20increasingly%0Acomputationally%20demanding%2C%20enhancing%20their%20training%20efficiency%20is%20crucial.%20This%0Apaper%20takes%20a%20step%20forward%20in%20accelerating%20the%20training%20of%20LIC%20methods%20by%0Amodeling%20the%20neural%20training%20dynamics.%20We%20first%20propose%20a%20Sensitivity-aware%0ATrue%20and%20Dummy%20Embedding%20Training%20mechanism%20%28STDET%29%20that%20clusters%20LIC%20model%0Aparameters%20into%20few%20separate%20modes%20where%20parameters%20are%20expressed%20as%20affine%0Atransformations%20of%20reference%20parameters%20within%20the%20same%20mode.%20By%20further%0Autilizing%20the%20stable%20intra-mode%20correlations%20throughout%20training%20and%20parameter%0Asensitivities%2C%20we%20gradually%20embed%20non-reference%20parameters%2C%20reducing%20the%20number%0Aof%20trainable%20parameters.%20Additionally%2C%20we%20incorporate%20a%20Sampling-then-Moving%0AAverage%20%28SMA%29%20technique%2C%20interpolating%20sampled%20weights%20from%20stochastic%20gradient%0Adescent%20%28SGD%29%20training%20to%20obtain%20the%20moving%20average%20weights%2C%20ensuring%20smooth%0Atemporal%20behavior%20and%20minimizing%20training%20state%20variances.%20Overall%2C%20our%20method%0Asignificantly%20reduces%20training%20space%20dimensions%20and%20the%20number%20of%20trainable%0Aparameters%20without%20sacrificing%20model%20performance%2C%20thus%20accelerating%20model%0Aconvergence.%20We%20also%20provide%20a%20theoretical%20analysis%20on%20the%20Noisy%20quadratic%0Amodel%2C%20showing%20that%20the%20proposed%20method%20achieves%20a%20lower%20training%20variance%20than%0Astandard%20SGD.%20Our%20approach%20offers%20valuable%20insights%20for%20further%20developing%0Aefficient%20training%20methods%20for%20LICs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Learned%2520Image%2520Compression%2520Through%2520Modeling%2520Neural%2520Training%250A%2520%2520Dynamics%26entry.906535625%3DYichi%2520Zhang%2520and%2520Zhihao%2520Duan%2520and%2520Yuning%2520Huang%2520and%2520Fengqing%2520Zhu%26entry.1292438233%3D%2520%2520As%2520learned%2520image%2520compression%2520%2528LIC%2529%2520methods%2520become%2520increasingly%250Acomputationally%2520demanding%252C%2520enhancing%2520their%2520training%2520efficiency%2520is%2520crucial.%2520This%250Apaper%2520takes%2520a%2520step%2520forward%2520in%2520accelerating%2520the%2520training%2520of%2520LIC%2520methods%2520by%250Amodeling%2520the%2520neural%2520training%2520dynamics.%2520We%2520first%2520propose%2520a%2520Sensitivity-aware%250ATrue%2520and%2520Dummy%2520Embedding%2520Training%2520mechanism%2520%2528STDET%2529%2520that%2520clusters%2520LIC%2520model%250Aparameters%2520into%2520few%2520separate%2520modes%2520where%2520parameters%2520are%2520expressed%2520as%2520affine%250Atransformations%2520of%2520reference%2520parameters%2520within%2520the%2520same%2520mode.%2520By%2520further%250Autilizing%2520the%2520stable%2520intra-mode%2520correlations%2520throughout%2520training%2520and%2520parameter%250Asensitivities%252C%2520we%2520gradually%2520embed%2520non-reference%2520parameters%252C%2520reducing%2520the%2520number%250Aof%2520trainable%2520parameters.%2520Additionally%252C%2520we%2520incorporate%2520a%2520Sampling-then-Moving%250AAverage%2520%2528SMA%2529%2520technique%252C%2520interpolating%2520sampled%2520weights%2520from%2520stochastic%2520gradient%250Adescent%2520%2528SGD%2529%2520training%2520to%2520obtain%2520the%2520moving%2520average%2520weights%252C%2520ensuring%2520smooth%250Atemporal%2520behavior%2520and%2520minimizing%2520training%2520state%2520variances.%2520Overall%252C%2520our%2520method%250Asignificantly%2520reduces%2520training%2520space%2520dimensions%2520and%2520the%2520number%2520of%2520trainable%250Aparameters%2520without%2520sacrificing%2520model%2520performance%252C%2520thus%2520accelerating%2520model%250Aconvergence.%2520We%2520also%2520provide%2520a%2520theoretical%2520analysis%2520on%2520the%2520Noisy%2520quadratic%250Amodel%252C%2520showing%2520that%2520the%2520proposed%2520method%2520achieves%2520a%2520lower%2520training%2520variance%2520than%250Astandard%2520SGD.%2520Our%2520approach%2520offers%2520valuable%2520insights%2520for%2520further%2520developing%250Aefficient%2520training%2520methods%2520for%2520LICs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Learned%20Image%20Compression%20Through%20Modeling%20Neural%20Training%0A%20%20Dynamics&entry.906535625=Yichi%20Zhang%20and%20Zhihao%20Duan%20and%20Yuning%20Huang%20and%20Fengqing%20Zhu&entry.1292438233=%20%20As%20learned%20image%20compression%20%28LIC%29%20methods%20become%20increasingly%0Acomputationally%20demanding%2C%20enhancing%20their%20training%20efficiency%20is%20crucial.%20This%0Apaper%20takes%20a%20step%20forward%20in%20accelerating%20the%20training%20of%20LIC%20methods%20by%0Amodeling%20the%20neural%20training%20dynamics.%20We%20first%20propose%20a%20Sensitivity-aware%0ATrue%20and%20Dummy%20Embedding%20Training%20mechanism%20%28STDET%29%20that%20clusters%20LIC%20model%0Aparameters%20into%20few%20separate%20modes%20where%20parameters%20are%20expressed%20as%20affine%0Atransformations%20of%20reference%20parameters%20within%20the%20same%20mode.%20By%20further%0Autilizing%20the%20stable%20intra-mode%20correlations%20throughout%20training%20and%20parameter%0Asensitivities%2C%20we%20gradually%20embed%20non-reference%20parameters%2C%20reducing%20the%20number%0Aof%20trainable%20parameters.%20Additionally%2C%20we%20incorporate%20a%20Sampling-then-Moving%0AAverage%20%28SMA%29%20technique%2C%20interpolating%20sampled%20weights%20from%20stochastic%20gradient%0Adescent%20%28SGD%29%20training%20to%20obtain%20the%20moving%20average%20weights%2C%20ensuring%20smooth%0Atemporal%20behavior%20and%20minimizing%20training%20state%20variances.%20Overall%2C%20our%20method%0Asignificantly%20reduces%20training%20space%20dimensions%20and%20the%20number%20of%20trainable%0Aparameters%20without%20sacrificing%20model%20performance%2C%20thus%20accelerating%20model%0Aconvergence.%20We%20also%20provide%20a%20theoretical%20analysis%20on%20the%20Noisy%20quadratic%0Amodel%2C%20showing%20that%20the%20proposed%20method%20achieves%20a%20lower%20training%20variance%20than%0Astandard%20SGD.%20Our%20approach%20offers%20valuable%20insights%20for%20further%20developing%0Aefficient%20training%20methods%20for%20LICs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18107v1&entry.124074799=Read"},
{"title": "Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM\n  Reasoning", "author": "Michael Hassid and Gabriel Synnaeve and Yossi Adi and Roy Schwartz", "abstract": "  Reasoning large language models (LLMs) heavily rely on scaling test-time\ncompute to perform complex reasoning tasks by generating extensive \"thinking\"\nchains. While demonstrating impressive results, this approach incurs\nsignificant computational costs and inference time. In this work, we challenge\nthe assumption that long thinking chains results in better reasoning\ncapabilities. We first demonstrate that shorter reasoning chains within\nindividual questions are significantly more likely to yield correct answers -\nup to 34.5% more accurate than the longest chain sampled for the same question.\nBased on these results, we suggest short-m@k, a novel reasoning LLM inference\nmethod. Our method executes k independent generations in parallel and halts\ncomputation once the first m thinking processes are done. The final answer is\nchosen using majority voting among these m chains. Basic short-1@k demonstrates\nsimilar or even superior performance over standard majority voting in\nlow-compute settings - using up to 40% fewer thinking tokens. short-3@k, while\nslightly less efficient than short-1@k, consistently surpasses majority voting\nacross all compute budgets, while still being substantially faster (up to 33%\nwall time reduction). Inspired by our results, we finetune an LLM using short,\nlong, and randomly selected reasoning chains. We then observe that training on\nthe shorter ones leads to better performance. Our findings suggest rethinking\ncurrent methods of test-time compute in reasoning LLMs, emphasizing that longer\n\"thinking\" does not necessarily translate to improved performance and can,\ncounter-intuitively, lead to degraded results.\n", "link": "http://arxiv.org/abs/2505.17813v1", "date": "2025-05-23", "relevancy": 2.2964, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4648}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Overthink%20it.%20Preferring%20Shorter%20Thinking%20Chains%20for%20Improved%20LLM%0A%20%20Reasoning&body=Title%3A%20Don%27t%20Overthink%20it.%20Preferring%20Shorter%20Thinking%20Chains%20for%20Improved%20LLM%0A%20%20Reasoning%0AAuthor%3A%20Michael%20Hassid%20and%20Gabriel%20Synnaeve%20and%20Yossi%20Adi%20and%20Roy%20Schwartz%0AAbstract%3A%20%20%20Reasoning%20large%20language%20models%20%28LLMs%29%20heavily%20rely%20on%20scaling%20test-time%0Acompute%20to%20perform%20complex%20reasoning%20tasks%20by%20generating%20extensive%20%22thinking%22%0Achains.%20While%20demonstrating%20impressive%20results%2C%20this%20approach%20incurs%0Asignificant%20computational%20costs%20and%20inference%20time.%20In%20this%20work%2C%20we%20challenge%0Athe%20assumption%20that%20long%20thinking%20chains%20results%20in%20better%20reasoning%0Acapabilities.%20We%20first%20demonstrate%20that%20shorter%20reasoning%20chains%20within%0Aindividual%20questions%20are%20significantly%20more%20likely%20to%20yield%20correct%20answers%20-%0Aup%20to%2034.5%25%20more%20accurate%20than%20the%20longest%20chain%20sampled%20for%20the%20same%20question.%0ABased%20on%20these%20results%2C%20we%20suggest%20short-m%40k%2C%20a%20novel%20reasoning%20LLM%20inference%0Amethod.%20Our%20method%20executes%20k%20independent%20generations%20in%20parallel%20and%20halts%0Acomputation%20once%20the%20first%20m%20thinking%20processes%20are%20done.%20The%20final%20answer%20is%0Achosen%20using%20majority%20voting%20among%20these%20m%20chains.%20Basic%20short-1%40k%20demonstrates%0Asimilar%20or%20even%20superior%20performance%20over%20standard%20majority%20voting%20in%0Alow-compute%20settings%20-%20using%20up%20to%2040%25%20fewer%20thinking%20tokens.%20short-3%40k%2C%20while%0Aslightly%20less%20efficient%20than%20short-1%40k%2C%20consistently%20surpasses%20majority%20voting%0Aacross%20all%20compute%20budgets%2C%20while%20still%20being%20substantially%20faster%20%28up%20to%2033%25%0Awall%20time%20reduction%29.%20Inspired%20by%20our%20results%2C%20we%20finetune%20an%20LLM%20using%20short%2C%0Along%2C%20and%20randomly%20selected%20reasoning%20chains.%20We%20then%20observe%20that%20training%20on%0Athe%20shorter%20ones%20leads%20to%20better%20performance.%20Our%20findings%20suggest%20rethinking%0Acurrent%20methods%20of%20test-time%20compute%20in%20reasoning%20LLMs%2C%20emphasizing%20that%20longer%0A%22thinking%22%20does%20not%20necessarily%20translate%20to%20improved%20performance%20and%20can%2C%0Acounter-intuitively%2C%20lead%20to%20degraded%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Overthink%2520it.%2520Preferring%2520Shorter%2520Thinking%2520Chains%2520for%2520Improved%2520LLM%250A%2520%2520Reasoning%26entry.906535625%3DMichael%2520Hassid%2520and%2520Gabriel%2520Synnaeve%2520and%2520Yossi%2520Adi%2520and%2520Roy%2520Schwartz%26entry.1292438233%3D%2520%2520Reasoning%2520large%2520language%2520models%2520%2528LLMs%2529%2520heavily%2520rely%2520on%2520scaling%2520test-time%250Acompute%2520to%2520perform%2520complex%2520reasoning%2520tasks%2520by%2520generating%2520extensive%2520%2522thinking%2522%250Achains.%2520While%2520demonstrating%2520impressive%2520results%252C%2520this%2520approach%2520incurs%250Asignificant%2520computational%2520costs%2520and%2520inference%2520time.%2520In%2520this%2520work%252C%2520we%2520challenge%250Athe%2520assumption%2520that%2520long%2520thinking%2520chains%2520results%2520in%2520better%2520reasoning%250Acapabilities.%2520We%2520first%2520demonstrate%2520that%2520shorter%2520reasoning%2520chains%2520within%250Aindividual%2520questions%2520are%2520significantly%2520more%2520likely%2520to%2520yield%2520correct%2520answers%2520-%250Aup%2520to%252034.5%2525%2520more%2520accurate%2520than%2520the%2520longest%2520chain%2520sampled%2520for%2520the%2520same%2520question.%250ABased%2520on%2520these%2520results%252C%2520we%2520suggest%2520short-m%2540k%252C%2520a%2520novel%2520reasoning%2520LLM%2520inference%250Amethod.%2520Our%2520method%2520executes%2520k%2520independent%2520generations%2520in%2520parallel%2520and%2520halts%250Acomputation%2520once%2520the%2520first%2520m%2520thinking%2520processes%2520are%2520done.%2520The%2520final%2520answer%2520is%250Achosen%2520using%2520majority%2520voting%2520among%2520these%2520m%2520chains.%2520Basic%2520short-1%2540k%2520demonstrates%250Asimilar%2520or%2520even%2520superior%2520performance%2520over%2520standard%2520majority%2520voting%2520in%250Alow-compute%2520settings%2520-%2520using%2520up%2520to%252040%2525%2520fewer%2520thinking%2520tokens.%2520short-3%2540k%252C%2520while%250Aslightly%2520less%2520efficient%2520than%2520short-1%2540k%252C%2520consistently%2520surpasses%2520majority%2520voting%250Aacross%2520all%2520compute%2520budgets%252C%2520while%2520still%2520being%2520substantially%2520faster%2520%2528up%2520to%252033%2525%250Awall%2520time%2520reduction%2529.%2520Inspired%2520by%2520our%2520results%252C%2520we%2520finetune%2520an%2520LLM%2520using%2520short%252C%250Along%252C%2520and%2520randomly%2520selected%2520reasoning%2520chains.%2520We%2520then%2520observe%2520that%2520training%2520on%250Athe%2520shorter%2520ones%2520leads%2520to%2520better%2520performance.%2520Our%2520findings%2520suggest%2520rethinking%250Acurrent%2520methods%2520of%2520test-time%2520compute%2520in%2520reasoning%2520LLMs%252C%2520emphasizing%2520that%2520longer%250A%2522thinking%2522%2520does%2520not%2520necessarily%2520translate%2520to%2520improved%2520performance%2520and%2520can%252C%250Acounter-intuitively%252C%2520lead%2520to%2520degraded%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Overthink%20it.%20Preferring%20Shorter%20Thinking%20Chains%20for%20Improved%20LLM%0A%20%20Reasoning&entry.906535625=Michael%20Hassid%20and%20Gabriel%20Synnaeve%20and%20Yossi%20Adi%20and%20Roy%20Schwartz&entry.1292438233=%20%20Reasoning%20large%20language%20models%20%28LLMs%29%20heavily%20rely%20on%20scaling%20test-time%0Acompute%20to%20perform%20complex%20reasoning%20tasks%20by%20generating%20extensive%20%22thinking%22%0Achains.%20While%20demonstrating%20impressive%20results%2C%20this%20approach%20incurs%0Asignificant%20computational%20costs%20and%20inference%20time.%20In%20this%20work%2C%20we%20challenge%0Athe%20assumption%20that%20long%20thinking%20chains%20results%20in%20better%20reasoning%0Acapabilities.%20We%20first%20demonstrate%20that%20shorter%20reasoning%20chains%20within%0Aindividual%20questions%20are%20significantly%20more%20likely%20to%20yield%20correct%20answers%20-%0Aup%20to%2034.5%25%20more%20accurate%20than%20the%20longest%20chain%20sampled%20for%20the%20same%20question.%0ABased%20on%20these%20results%2C%20we%20suggest%20short-m%40k%2C%20a%20novel%20reasoning%20LLM%20inference%0Amethod.%20Our%20method%20executes%20k%20independent%20generations%20in%20parallel%20and%20halts%0Acomputation%20once%20the%20first%20m%20thinking%20processes%20are%20done.%20The%20final%20answer%20is%0Achosen%20using%20majority%20voting%20among%20these%20m%20chains.%20Basic%20short-1%40k%20demonstrates%0Asimilar%20or%20even%20superior%20performance%20over%20standard%20majority%20voting%20in%0Alow-compute%20settings%20-%20using%20up%20to%2040%25%20fewer%20thinking%20tokens.%20short-3%40k%2C%20while%0Aslightly%20less%20efficient%20than%20short-1%40k%2C%20consistently%20surpasses%20majority%20voting%0Aacross%20all%20compute%20budgets%2C%20while%20still%20being%20substantially%20faster%20%28up%20to%2033%25%0Awall%20time%20reduction%29.%20Inspired%20by%20our%20results%2C%20we%20finetune%20an%20LLM%20using%20short%2C%0Along%2C%20and%20randomly%20selected%20reasoning%20chains.%20We%20then%20observe%20that%20training%20on%0Athe%20shorter%20ones%20leads%20to%20better%20performance.%20Our%20findings%20suggest%20rethinking%0Acurrent%20methods%20of%20test-time%20compute%20in%20reasoning%20LLMs%2C%20emphasizing%20that%20longer%0A%22thinking%22%20does%20not%20necessarily%20translate%20to%20improved%20performance%20and%20can%2C%0Acounter-intuitively%2C%20lead%20to%20degraded%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17813v1&entry.124074799=Read"},
{"title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation", "author": "Yaoxiang Wang and Haoling Li and Xin Zhang and Jie Wu and Xiao Liu and Wenxiang Hu and Zhongxin Guo and Yangyu Huang and Ying Xin and Yujiu Yang and Jinsong Su and Qi Chen and Scarlett Li", "abstract": "  Existing methods for code generation use code snippets as seed data,\nrestricting the complexity and diversity of the synthesized data. In this\npaper, we introduce a novel feature tree-based synthesis framework, which\nrevolves around hierarchical code features derived from high-level abstractions\nof code. The feature tree is constructed from raw data and refined iteratively\nto increase the quantity and diversity of the extracted features, which\ncaptures and recognizes more complex patterns and relationships within the\ncode. By adjusting the depth and breadth of the sampled subtrees, our framework\nprovides precise control over the complexity of the generated code, enabling\nfunctionalities that range from function-level operations to multi-file\nscenarios. We fine-tuned widely-used base models to obtain EpiCoder series,\nachieving state-of-the-art performance on multiple benchmarks at both the\nfunction and file levels. In particular, empirical evidence indicates that our\napproach shows significant potential in the synthesizing of repository-level\ncode data. Our code and data are publicly available at\nhttps://github.com/microsoft/EpiCoder.\n", "link": "http://arxiv.org/abs/2501.04694v2", "date": "2025-05-23", "relevancy": 2.2952, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4778}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EpiCoder%3A%20Encompassing%20Diversity%20and%20Complexity%20in%20Code%20Generation&body=Title%3A%20EpiCoder%3A%20Encompassing%20Diversity%20and%20Complexity%20in%20Code%20Generation%0AAuthor%3A%20Yaoxiang%20Wang%20and%20Haoling%20Li%20and%20Xin%20Zhang%20and%20Jie%20Wu%20and%20Xiao%20Liu%20and%20Wenxiang%20Hu%20and%20Zhongxin%20Guo%20and%20Yangyu%20Huang%20and%20Ying%20Xin%20and%20Yujiu%20Yang%20and%20Jinsong%20Su%20and%20Qi%20Chen%20and%20Scarlett%20Li%0AAbstract%3A%20%20%20Existing%20methods%20for%20code%20generation%20use%20code%20snippets%20as%20seed%20data%2C%0Arestricting%20the%20complexity%20and%20diversity%20of%20the%20synthesized%20data.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20feature%20tree-based%20synthesis%20framework%2C%20which%0Arevolves%20around%20hierarchical%20code%20features%20derived%20from%20high-level%20abstractions%0Aof%20code.%20The%20feature%20tree%20is%20constructed%20from%20raw%20data%20and%20refined%20iteratively%0Ato%20increase%20the%20quantity%20and%20diversity%20of%20the%20extracted%20features%2C%20which%0Acaptures%20and%20recognizes%20more%20complex%20patterns%20and%20relationships%20within%20the%0Acode.%20By%20adjusting%20the%20depth%20and%20breadth%20of%20the%20sampled%20subtrees%2C%20our%20framework%0Aprovides%20precise%20control%20over%20the%20complexity%20of%20the%20generated%20code%2C%20enabling%0Afunctionalities%20that%20range%20from%20function-level%20operations%20to%20multi-file%0Ascenarios.%20We%20fine-tuned%20widely-used%20base%20models%20to%20obtain%20EpiCoder%20series%2C%0Aachieving%20state-of-the-art%20performance%20on%20multiple%20benchmarks%20at%20both%20the%0Afunction%20and%20file%20levels.%20In%20particular%2C%20empirical%20evidence%20indicates%20that%20our%0Aapproach%20shows%20significant%20potential%20in%20the%20synthesizing%20of%20repository-level%0Acode%20data.%20Our%20code%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/microsoft/EpiCoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04694v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEpiCoder%253A%2520Encompassing%2520Diversity%2520and%2520Complexity%2520in%2520Code%2520Generation%26entry.906535625%3DYaoxiang%2520Wang%2520and%2520Haoling%2520Li%2520and%2520Xin%2520Zhang%2520and%2520Jie%2520Wu%2520and%2520Xiao%2520Liu%2520and%2520Wenxiang%2520Hu%2520and%2520Zhongxin%2520Guo%2520and%2520Yangyu%2520Huang%2520and%2520Ying%2520Xin%2520and%2520Yujiu%2520Yang%2520and%2520Jinsong%2520Su%2520and%2520Qi%2520Chen%2520and%2520Scarlett%2520Li%26entry.1292438233%3D%2520%2520Existing%2520methods%2520for%2520code%2520generation%2520use%2520code%2520snippets%2520as%2520seed%2520data%252C%250Arestricting%2520the%2520complexity%2520and%2520diversity%2520of%2520the%2520synthesized%2520data.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520feature%2520tree-based%2520synthesis%2520framework%252C%2520which%250Arevolves%2520around%2520hierarchical%2520code%2520features%2520derived%2520from%2520high-level%2520abstractions%250Aof%2520code.%2520The%2520feature%2520tree%2520is%2520constructed%2520from%2520raw%2520data%2520and%2520refined%2520iteratively%250Ato%2520increase%2520the%2520quantity%2520and%2520diversity%2520of%2520the%2520extracted%2520features%252C%2520which%250Acaptures%2520and%2520recognizes%2520more%2520complex%2520patterns%2520and%2520relationships%2520within%2520the%250Acode.%2520By%2520adjusting%2520the%2520depth%2520and%2520breadth%2520of%2520the%2520sampled%2520subtrees%252C%2520our%2520framework%250Aprovides%2520precise%2520control%2520over%2520the%2520complexity%2520of%2520the%2520generated%2520code%252C%2520enabling%250Afunctionalities%2520that%2520range%2520from%2520function-level%2520operations%2520to%2520multi-file%250Ascenarios.%2520We%2520fine-tuned%2520widely-used%2520base%2520models%2520to%2520obtain%2520EpiCoder%2520series%252C%250Aachieving%2520state-of-the-art%2520performance%2520on%2520multiple%2520benchmarks%2520at%2520both%2520the%250Afunction%2520and%2520file%2520levels.%2520In%2520particular%252C%2520empirical%2520evidence%2520indicates%2520that%2520our%250Aapproach%2520shows%2520significant%2520potential%2520in%2520the%2520synthesizing%2520of%2520repository-level%250Acode%2520data.%2520Our%2520code%2520and%2520data%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/microsoft/EpiCoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04694v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EpiCoder%3A%20Encompassing%20Diversity%20and%20Complexity%20in%20Code%20Generation&entry.906535625=Yaoxiang%20Wang%20and%20Haoling%20Li%20and%20Xin%20Zhang%20and%20Jie%20Wu%20and%20Xiao%20Liu%20and%20Wenxiang%20Hu%20and%20Zhongxin%20Guo%20and%20Yangyu%20Huang%20and%20Ying%20Xin%20and%20Yujiu%20Yang%20and%20Jinsong%20Su%20and%20Qi%20Chen%20and%20Scarlett%20Li&entry.1292438233=%20%20Existing%20methods%20for%20code%20generation%20use%20code%20snippets%20as%20seed%20data%2C%0Arestricting%20the%20complexity%20and%20diversity%20of%20the%20synthesized%20data.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20feature%20tree-based%20synthesis%20framework%2C%20which%0Arevolves%20around%20hierarchical%20code%20features%20derived%20from%20high-level%20abstractions%0Aof%20code.%20The%20feature%20tree%20is%20constructed%20from%20raw%20data%20and%20refined%20iteratively%0Ato%20increase%20the%20quantity%20and%20diversity%20of%20the%20extracted%20features%2C%20which%0Acaptures%20and%20recognizes%20more%20complex%20patterns%20and%20relationships%20within%20the%0Acode.%20By%20adjusting%20the%20depth%20and%20breadth%20of%20the%20sampled%20subtrees%2C%20our%20framework%0Aprovides%20precise%20control%20over%20the%20complexity%20of%20the%20generated%20code%2C%20enabling%0Afunctionalities%20that%20range%20from%20function-level%20operations%20to%20multi-file%0Ascenarios.%20We%20fine-tuned%20widely-used%20base%20models%20to%20obtain%20EpiCoder%20series%2C%0Aachieving%20state-of-the-art%20performance%20on%20multiple%20benchmarks%20at%20both%20the%0Afunction%20and%20file%20levels.%20In%20particular%2C%20empirical%20evidence%20indicates%20that%20our%0Aapproach%20shows%20significant%20potential%20in%20the%20synthesizing%20of%20repository-level%0Acode%20data.%20Our%20code%20and%20data%20are%20publicly%20available%20at%0Ahttps%3A//github.com/microsoft/EpiCoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04694v2&entry.124074799=Read"},
{"title": "Tuning for Trustworthiness -- Balancing Performance and Explanation\n  Consistency in Neural Network Optimization", "author": "Alexander Hinterleitner and Thomas Bartz-Beielstein", "abstract": "  Despite the growing interest in Explainable Artificial Intelligence (XAI),\nexplainability is rarely considered during hyperparameter tuning or neural\narchitecture optimization, where the focus remains primarily on minimizing\npredictive loss. In this work, we introduce the novel concept of XAI\nconsistency, defined as the agreement among different feature attribution\nmethods, and propose new metrics to quantify it. For the first time, we\nintegrate XAI consistency directly into the hyperparameter tuning objective,\ncreating a multi-objective optimization framework that balances predictive\nperformance with explanation robustness. Implemented within the Sequential\nParameter Optimization Toolbox (SPOT), our approach uses both weighted\naggregation and desirability-based strategies to guide model selection. Through\nour proposed framework and supporting tools, we explore the impact of\nincorporating XAI consistency into the optimization process. This enables us to\ncharacterize distinct regions in the architecture configuration space: one\nregion with poor performance and comparatively low interpretability, another\nwith strong predictive performance but weak interpretability due to low\n\\gls{xai} consistency, and a trade-off region that balances both objectives by\noffering high interpretability alongside competitive performance. Beyond\nintroducing this novel approach, our research provides a foundation for future\ninvestigations into whether models from the trade-off zone-balancing\nperformance loss and XAI consistency-exhibit greater robustness by avoiding\noverfitting to training performance, thereby leading to more reliable\npredictions on out-of-distribution data.\n", "link": "http://arxiv.org/abs/2505.07910v2", "date": "2025-05-23", "relevancy": 2.2932, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4631}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tuning%20for%20Trustworthiness%20--%20Balancing%20Performance%20and%20Explanation%0A%20%20Consistency%20in%20Neural%20Network%20Optimization&body=Title%3A%20Tuning%20for%20Trustworthiness%20--%20Balancing%20Performance%20and%20Explanation%0A%20%20Consistency%20in%20Neural%20Network%20Optimization%0AAuthor%3A%20Alexander%20Hinterleitner%20and%20Thomas%20Bartz-Beielstein%0AAbstract%3A%20%20%20Despite%20the%20growing%20interest%20in%20Explainable%20Artificial%20Intelligence%20%28XAI%29%2C%0Aexplainability%20is%20rarely%20considered%20during%20hyperparameter%20tuning%20or%20neural%0Aarchitecture%20optimization%2C%20where%20the%20focus%20remains%20primarily%20on%20minimizing%0Apredictive%20loss.%20In%20this%20work%2C%20we%20introduce%20the%20novel%20concept%20of%20XAI%0Aconsistency%2C%20defined%20as%20the%20agreement%20among%20different%20feature%20attribution%0Amethods%2C%20and%20propose%20new%20metrics%20to%20quantify%20it.%20For%20the%20first%20time%2C%20we%0Aintegrate%20XAI%20consistency%20directly%20into%20the%20hyperparameter%20tuning%20objective%2C%0Acreating%20a%20multi-objective%20optimization%20framework%20that%20balances%20predictive%0Aperformance%20with%20explanation%20robustness.%20Implemented%20within%20the%20Sequential%0AParameter%20Optimization%20Toolbox%20%28SPOT%29%2C%20our%20approach%20uses%20both%20weighted%0Aaggregation%20and%20desirability-based%20strategies%20to%20guide%20model%20selection.%20Through%0Aour%20proposed%20framework%20and%20supporting%20tools%2C%20we%20explore%20the%20impact%20of%0Aincorporating%20XAI%20consistency%20into%20the%20optimization%20process.%20This%20enables%20us%20to%0Acharacterize%20distinct%20regions%20in%20the%20architecture%20configuration%20space%3A%20one%0Aregion%20with%20poor%20performance%20and%20comparatively%20low%20interpretability%2C%20another%0Awith%20strong%20predictive%20performance%20but%20weak%20interpretability%20due%20to%20low%0A%5Cgls%7Bxai%7D%20consistency%2C%20and%20a%20trade-off%20region%20that%20balances%20both%20objectives%20by%0Aoffering%20high%20interpretability%20alongside%20competitive%20performance.%20Beyond%0Aintroducing%20this%20novel%20approach%2C%20our%20research%20provides%20a%20foundation%20for%20future%0Ainvestigations%20into%20whether%20models%20from%20the%20trade-off%20zone-balancing%0Aperformance%20loss%20and%20XAI%20consistency-exhibit%20greater%20robustness%20by%20avoiding%0Aoverfitting%20to%20training%20performance%2C%20thereby%20leading%20to%20more%20reliable%0Apredictions%20on%20out-of-distribution%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07910v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTuning%2520for%2520Trustworthiness%2520--%2520Balancing%2520Performance%2520and%2520Explanation%250A%2520%2520Consistency%2520in%2520Neural%2520Network%2520Optimization%26entry.906535625%3DAlexander%2520Hinterleitner%2520and%2520Thomas%2520Bartz-Beielstein%26entry.1292438233%3D%2520%2520Despite%2520the%2520growing%2520interest%2520in%2520Explainable%2520Artificial%2520Intelligence%2520%2528XAI%2529%252C%250Aexplainability%2520is%2520rarely%2520considered%2520during%2520hyperparameter%2520tuning%2520or%2520neural%250Aarchitecture%2520optimization%252C%2520where%2520the%2520focus%2520remains%2520primarily%2520on%2520minimizing%250Apredictive%2520loss.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520novel%2520concept%2520of%2520XAI%250Aconsistency%252C%2520defined%2520as%2520the%2520agreement%2520among%2520different%2520feature%2520attribution%250Amethods%252C%2520and%2520propose%2520new%2520metrics%2520to%2520quantify%2520it.%2520For%2520the%2520first%2520time%252C%2520we%250Aintegrate%2520XAI%2520consistency%2520directly%2520into%2520the%2520hyperparameter%2520tuning%2520objective%252C%250Acreating%2520a%2520multi-objective%2520optimization%2520framework%2520that%2520balances%2520predictive%250Aperformance%2520with%2520explanation%2520robustness.%2520Implemented%2520within%2520the%2520Sequential%250AParameter%2520Optimization%2520Toolbox%2520%2528SPOT%2529%252C%2520our%2520approach%2520uses%2520both%2520weighted%250Aaggregation%2520and%2520desirability-based%2520strategies%2520to%2520guide%2520model%2520selection.%2520Through%250Aour%2520proposed%2520framework%2520and%2520supporting%2520tools%252C%2520we%2520explore%2520the%2520impact%2520of%250Aincorporating%2520XAI%2520consistency%2520into%2520the%2520optimization%2520process.%2520This%2520enables%2520us%2520to%250Acharacterize%2520distinct%2520regions%2520in%2520the%2520architecture%2520configuration%2520space%253A%2520one%250Aregion%2520with%2520poor%2520performance%2520and%2520comparatively%2520low%2520interpretability%252C%2520another%250Awith%2520strong%2520predictive%2520performance%2520but%2520weak%2520interpretability%2520due%2520to%2520low%250A%255Cgls%257Bxai%257D%2520consistency%252C%2520and%2520a%2520trade-off%2520region%2520that%2520balances%2520both%2520objectives%2520by%250Aoffering%2520high%2520interpretability%2520alongside%2520competitive%2520performance.%2520Beyond%250Aintroducing%2520this%2520novel%2520approach%252C%2520our%2520research%2520provides%2520a%2520foundation%2520for%2520future%250Ainvestigations%2520into%2520whether%2520models%2520from%2520the%2520trade-off%2520zone-balancing%250Aperformance%2520loss%2520and%2520XAI%2520consistency-exhibit%2520greater%2520robustness%2520by%2520avoiding%250Aoverfitting%2520to%2520training%2520performance%252C%2520thereby%2520leading%2520to%2520more%2520reliable%250Apredictions%2520on%2520out-of-distribution%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07910v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tuning%20for%20Trustworthiness%20--%20Balancing%20Performance%20and%20Explanation%0A%20%20Consistency%20in%20Neural%20Network%20Optimization&entry.906535625=Alexander%20Hinterleitner%20and%20Thomas%20Bartz-Beielstein&entry.1292438233=%20%20Despite%20the%20growing%20interest%20in%20Explainable%20Artificial%20Intelligence%20%28XAI%29%2C%0Aexplainability%20is%20rarely%20considered%20during%20hyperparameter%20tuning%20or%20neural%0Aarchitecture%20optimization%2C%20where%20the%20focus%20remains%20primarily%20on%20minimizing%0Apredictive%20loss.%20In%20this%20work%2C%20we%20introduce%20the%20novel%20concept%20of%20XAI%0Aconsistency%2C%20defined%20as%20the%20agreement%20among%20different%20feature%20attribution%0Amethods%2C%20and%20propose%20new%20metrics%20to%20quantify%20it.%20For%20the%20first%20time%2C%20we%0Aintegrate%20XAI%20consistency%20directly%20into%20the%20hyperparameter%20tuning%20objective%2C%0Acreating%20a%20multi-objective%20optimization%20framework%20that%20balances%20predictive%0Aperformance%20with%20explanation%20robustness.%20Implemented%20within%20the%20Sequential%0AParameter%20Optimization%20Toolbox%20%28SPOT%29%2C%20our%20approach%20uses%20both%20weighted%0Aaggregation%20and%20desirability-based%20strategies%20to%20guide%20model%20selection.%20Through%0Aour%20proposed%20framework%20and%20supporting%20tools%2C%20we%20explore%20the%20impact%20of%0Aincorporating%20XAI%20consistency%20into%20the%20optimization%20process.%20This%20enables%20us%20to%0Acharacterize%20distinct%20regions%20in%20the%20architecture%20configuration%20space%3A%20one%0Aregion%20with%20poor%20performance%20and%20comparatively%20low%20interpretability%2C%20another%0Awith%20strong%20predictive%20performance%20but%20weak%20interpretability%20due%20to%20low%0A%5Cgls%7Bxai%7D%20consistency%2C%20and%20a%20trade-off%20region%20that%20balances%20both%20objectives%20by%0Aoffering%20high%20interpretability%20alongside%20competitive%20performance.%20Beyond%0Aintroducing%20this%20novel%20approach%2C%20our%20research%20provides%20a%20foundation%20for%20future%0Ainvestigations%20into%20whether%20models%20from%20the%20trade-off%20zone-balancing%0Aperformance%20loss%20and%20XAI%20consistency-exhibit%20greater%20robustness%20by%20avoiding%0Aoverfitting%20to%20training%20performance%2C%20thereby%20leading%20to%20more%20reliable%0Apredictions%20on%20out-of-distribution%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07910v2&entry.124074799=Read"},
{"title": "Fourier-Based 3D Multistage Transformer for Aberration Correction in\n  Multicellular Specimens", "author": "Thayer Alshaabi and Daniel E. Milkie and Gaoxiang Liu and Cyna Shirazinejad and Jason L. Hong and Kemal Achour and Frederik G\u00f6rlitz and Ana Milunovic-Jevtic and Cat Simmons and Ibrahim S. Abuzahriyeh and Erin Hong and Samara Erin Williams and Nathanael Harrison and Evan Huang and Eun Seok Bae and Alison N. Killilea and David G. Drubin and Ian A. Swinburne and Srigokul Upadhyayula and Eric Betzig", "abstract": "  High-resolution tissue imaging is often compromised by sample-induced optical\naberrations that degrade resolution and contrast. While wavefront sensor-based\nadaptive optics (AO) can measure these aberrations, such hardware solutions are\ntypically complex, expensive to implement, and slow when serially mapping\nspatially varying aberrations across large fields of view. Here, we introduce\nAOViFT (Adaptive Optical Vision Fourier Transformer) -- a machine\nlearning-based aberration sensing framework built around a 3D multistage Vision\nTransformer that operates on Fourier domain embeddings. AOViFT infers\naberrations and restores diffraction-limited performance in puncta-labeled\nspecimens with substantially reduced computational cost, training time, and\nmemory footprint compared to conventional architectures or real-space networks.\nWe validated AOViFT on live gene-edited zebrafish embryos, demonstrating its\nability to correct spatially varying aberrations using either a deformable\nmirror or post-acquisition deconvolution. By eliminating the need for the guide\nstar and wavefront sensing hardware and simplifying the experimental workflow,\nAOViFT lowers technical barriers for high-resolution volumetric microscopy\nacross diverse biological samples.\n", "link": "http://arxiv.org/abs/2503.12593v2", "date": "2025-05-23", "relevancy": 2.2913, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5792}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5792}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fourier-Based%203D%20Multistage%20Transformer%20for%20Aberration%20Correction%20in%0A%20%20Multicellular%20Specimens&body=Title%3A%20Fourier-Based%203D%20Multistage%20Transformer%20for%20Aberration%20Correction%20in%0A%20%20Multicellular%20Specimens%0AAuthor%3A%20Thayer%20Alshaabi%20and%20Daniel%20E.%20Milkie%20and%20Gaoxiang%20Liu%20and%20Cyna%20Shirazinejad%20and%20Jason%20L.%20Hong%20and%20Kemal%20Achour%20and%20Frederik%20G%C3%B6rlitz%20and%20Ana%20Milunovic-Jevtic%20and%20Cat%20Simmons%20and%20Ibrahim%20S.%20Abuzahriyeh%20and%20Erin%20Hong%20and%20Samara%20Erin%20Williams%20and%20Nathanael%20Harrison%20and%20Evan%20Huang%20and%20Eun%20Seok%20Bae%20and%20Alison%20N.%20Killilea%20and%20David%20G.%20Drubin%20and%20Ian%20A.%20Swinburne%20and%20Srigokul%20Upadhyayula%20and%20Eric%20Betzig%0AAbstract%3A%20%20%20High-resolution%20tissue%20imaging%20is%20often%20compromised%20by%20sample-induced%20optical%0Aaberrations%20that%20degrade%20resolution%20and%20contrast.%20While%20wavefront%20sensor-based%0Aadaptive%20optics%20%28AO%29%20can%20measure%20these%20aberrations%2C%20such%20hardware%20solutions%20are%0Atypically%20complex%2C%20expensive%20to%20implement%2C%20and%20slow%20when%20serially%20mapping%0Aspatially%20varying%20aberrations%20across%20large%20fields%20of%20view.%20Here%2C%20we%20introduce%0AAOViFT%20%28Adaptive%20Optical%20Vision%20Fourier%20Transformer%29%20--%20a%20machine%0Alearning-based%20aberration%20sensing%20framework%20built%20around%20a%203D%20multistage%20Vision%0ATransformer%20that%20operates%20on%20Fourier%20domain%20embeddings.%20AOViFT%20infers%0Aaberrations%20and%20restores%20diffraction-limited%20performance%20in%20puncta-labeled%0Aspecimens%20with%20substantially%20reduced%20computational%20cost%2C%20training%20time%2C%20and%0Amemory%20footprint%20compared%20to%20conventional%20architectures%20or%20real-space%20networks.%0AWe%20validated%20AOViFT%20on%20live%20gene-edited%20zebrafish%20embryos%2C%20demonstrating%20its%0Aability%20to%20correct%20spatially%20varying%20aberrations%20using%20either%20a%20deformable%0Amirror%20or%20post-acquisition%20deconvolution.%20By%20eliminating%20the%20need%20for%20the%20guide%0Astar%20and%20wavefront%20sensing%20hardware%20and%20simplifying%20the%20experimental%20workflow%2C%0AAOViFT%20lowers%20technical%20barriers%20for%20high-resolution%20volumetric%20microscopy%0Aacross%20diverse%20biological%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.12593v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFourier-Based%25203D%2520Multistage%2520Transformer%2520for%2520Aberration%2520Correction%2520in%250A%2520%2520Multicellular%2520Specimens%26entry.906535625%3DThayer%2520Alshaabi%2520and%2520Daniel%2520E.%2520Milkie%2520and%2520Gaoxiang%2520Liu%2520and%2520Cyna%2520Shirazinejad%2520and%2520Jason%2520L.%2520Hong%2520and%2520Kemal%2520Achour%2520and%2520Frederik%2520G%25C3%25B6rlitz%2520and%2520Ana%2520Milunovic-Jevtic%2520and%2520Cat%2520Simmons%2520and%2520Ibrahim%2520S.%2520Abuzahriyeh%2520and%2520Erin%2520Hong%2520and%2520Samara%2520Erin%2520Williams%2520and%2520Nathanael%2520Harrison%2520and%2520Evan%2520Huang%2520and%2520Eun%2520Seok%2520Bae%2520and%2520Alison%2520N.%2520Killilea%2520and%2520David%2520G.%2520Drubin%2520and%2520Ian%2520A.%2520Swinburne%2520and%2520Srigokul%2520Upadhyayula%2520and%2520Eric%2520Betzig%26entry.1292438233%3D%2520%2520High-resolution%2520tissue%2520imaging%2520is%2520often%2520compromised%2520by%2520sample-induced%2520optical%250Aaberrations%2520that%2520degrade%2520resolution%2520and%2520contrast.%2520While%2520wavefront%2520sensor-based%250Aadaptive%2520optics%2520%2528AO%2529%2520can%2520measure%2520these%2520aberrations%252C%2520such%2520hardware%2520solutions%2520are%250Atypically%2520complex%252C%2520expensive%2520to%2520implement%252C%2520and%2520slow%2520when%2520serially%2520mapping%250Aspatially%2520varying%2520aberrations%2520across%2520large%2520fields%2520of%2520view.%2520Here%252C%2520we%2520introduce%250AAOViFT%2520%2528Adaptive%2520Optical%2520Vision%2520Fourier%2520Transformer%2529%2520--%2520a%2520machine%250Alearning-based%2520aberration%2520sensing%2520framework%2520built%2520around%2520a%25203D%2520multistage%2520Vision%250ATransformer%2520that%2520operates%2520on%2520Fourier%2520domain%2520embeddings.%2520AOViFT%2520infers%250Aaberrations%2520and%2520restores%2520diffraction-limited%2520performance%2520in%2520puncta-labeled%250Aspecimens%2520with%2520substantially%2520reduced%2520computational%2520cost%252C%2520training%2520time%252C%2520and%250Amemory%2520footprint%2520compared%2520to%2520conventional%2520architectures%2520or%2520real-space%2520networks.%250AWe%2520validated%2520AOViFT%2520on%2520live%2520gene-edited%2520zebrafish%2520embryos%252C%2520demonstrating%2520its%250Aability%2520to%2520correct%2520spatially%2520varying%2520aberrations%2520using%2520either%2520a%2520deformable%250Amirror%2520or%2520post-acquisition%2520deconvolution.%2520By%2520eliminating%2520the%2520need%2520for%2520the%2520guide%250Astar%2520and%2520wavefront%2520sensing%2520hardware%2520and%2520simplifying%2520the%2520experimental%2520workflow%252C%250AAOViFT%2520lowers%2520technical%2520barriers%2520for%2520high-resolution%2520volumetric%2520microscopy%250Aacross%2520diverse%2520biological%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.12593v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fourier-Based%203D%20Multistage%20Transformer%20for%20Aberration%20Correction%20in%0A%20%20Multicellular%20Specimens&entry.906535625=Thayer%20Alshaabi%20and%20Daniel%20E.%20Milkie%20and%20Gaoxiang%20Liu%20and%20Cyna%20Shirazinejad%20and%20Jason%20L.%20Hong%20and%20Kemal%20Achour%20and%20Frederik%20G%C3%B6rlitz%20and%20Ana%20Milunovic-Jevtic%20and%20Cat%20Simmons%20and%20Ibrahim%20S.%20Abuzahriyeh%20and%20Erin%20Hong%20and%20Samara%20Erin%20Williams%20and%20Nathanael%20Harrison%20and%20Evan%20Huang%20and%20Eun%20Seok%20Bae%20and%20Alison%20N.%20Killilea%20and%20David%20G.%20Drubin%20and%20Ian%20A.%20Swinburne%20and%20Srigokul%20Upadhyayula%20and%20Eric%20Betzig&entry.1292438233=%20%20High-resolution%20tissue%20imaging%20is%20often%20compromised%20by%20sample-induced%20optical%0Aaberrations%20that%20degrade%20resolution%20and%20contrast.%20While%20wavefront%20sensor-based%0Aadaptive%20optics%20%28AO%29%20can%20measure%20these%20aberrations%2C%20such%20hardware%20solutions%20are%0Atypically%20complex%2C%20expensive%20to%20implement%2C%20and%20slow%20when%20serially%20mapping%0Aspatially%20varying%20aberrations%20across%20large%20fields%20of%20view.%20Here%2C%20we%20introduce%0AAOViFT%20%28Adaptive%20Optical%20Vision%20Fourier%20Transformer%29%20--%20a%20machine%0Alearning-based%20aberration%20sensing%20framework%20built%20around%20a%203D%20multistage%20Vision%0ATransformer%20that%20operates%20on%20Fourier%20domain%20embeddings.%20AOViFT%20infers%0Aaberrations%20and%20restores%20diffraction-limited%20performance%20in%20puncta-labeled%0Aspecimens%20with%20substantially%20reduced%20computational%20cost%2C%20training%20time%2C%20and%0Amemory%20footprint%20compared%20to%20conventional%20architectures%20or%20real-space%20networks.%0AWe%20validated%20AOViFT%20on%20live%20gene-edited%20zebrafish%20embryos%2C%20demonstrating%20its%0Aability%20to%20correct%20spatially%20varying%20aberrations%20using%20either%20a%20deformable%0Amirror%20or%20post-acquisition%20deconvolution.%20By%20eliminating%20the%20need%20for%20the%20guide%0Astar%20and%20wavefront%20sensing%20hardware%20and%20simplifying%20the%20experimental%20workflow%2C%0AAOViFT%20lowers%20technical%20barriers%20for%20high-resolution%20volumetric%20microscopy%0Aacross%20diverse%20biological%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.12593v2&entry.124074799=Read"},
{"title": "The Third Pillar of Causal Analysis? A Measurement Perspective on Causal\n  Representations", "author": "Dingling Yao and Shimeng Huang and Riccardo Cadei and Kun Zhang and Francesco Locatello", "abstract": "  Causal reasoning and discovery, two fundamental tasks of causal analysis,\noften face challenges in applications due to the complexity, noisiness, and\nhigh-dimensionality of real-world data. Despite recent progress in identifying\nlatent causal structures using causal representation learning (CRL), what makes\nlearned representations useful for causal downstream tasks and how to evaluate\nthem are still not well understood. In this paper, we reinterpret CRL using a\nmeasurement model framework, where the learned representations are viewed as\nproxy measurements of the latent causal variables. Our approach clarifies the\nconditions under which learned representations support downstream causal\nreasoning and provides a principled basis for quantitatively assessing the\nquality of representations using a new Test-based Measurement EXclusivity\n(T-MEX) score. We validate T-MEX across diverse causal inference scenarios,\nincluding numerical simulations and real-world ecological video analysis,\ndemonstrating that the proposed framework and corresponding score effectively\nassess the identification of learned representations and their usefulness for\ncausal downstream tasks.\n", "link": "http://arxiv.org/abs/2505.17708v1", "date": "2025-05-23", "relevancy": 2.2911, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4667}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4667}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Third%20Pillar%20of%20Causal%20Analysis%3F%20A%20Measurement%20Perspective%20on%20Causal%0A%20%20Representations&body=Title%3A%20The%20Third%20Pillar%20of%20Causal%20Analysis%3F%20A%20Measurement%20Perspective%20on%20Causal%0A%20%20Representations%0AAuthor%3A%20Dingling%20Yao%20and%20Shimeng%20Huang%20and%20Riccardo%20Cadei%20and%20Kun%20Zhang%20and%20Francesco%20Locatello%0AAbstract%3A%20%20%20Causal%20reasoning%20and%20discovery%2C%20two%20fundamental%20tasks%20of%20causal%20analysis%2C%0Aoften%20face%20challenges%20in%20applications%20due%20to%20the%20complexity%2C%20noisiness%2C%20and%0Ahigh-dimensionality%20of%20real-world%20data.%20Despite%20recent%20progress%20in%20identifying%0Alatent%20causal%20structures%20using%20causal%20representation%20learning%20%28CRL%29%2C%20what%20makes%0Alearned%20representations%20useful%20for%20causal%20downstream%20tasks%20and%20how%20to%20evaluate%0Athem%20are%20still%20not%20well%20understood.%20In%20this%20paper%2C%20we%20reinterpret%20CRL%20using%20a%0Ameasurement%20model%20framework%2C%20where%20the%20learned%20representations%20are%20viewed%20as%0Aproxy%20measurements%20of%20the%20latent%20causal%20variables.%20Our%20approach%20clarifies%20the%0Aconditions%20under%20which%20learned%20representations%20support%20downstream%20causal%0Areasoning%20and%20provides%20a%20principled%20basis%20for%20quantitatively%20assessing%20the%0Aquality%20of%20representations%20using%20a%20new%20Test-based%20Measurement%20EXclusivity%0A%28T-MEX%29%20score.%20We%20validate%20T-MEX%20across%20diverse%20causal%20inference%20scenarios%2C%0Aincluding%20numerical%20simulations%20and%20real-world%20ecological%20video%20analysis%2C%0Ademonstrating%20that%20the%20proposed%20framework%20and%20corresponding%20score%20effectively%0Aassess%20the%20identification%20of%20learned%20representations%20and%20their%20usefulness%20for%0Acausal%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Third%2520Pillar%2520of%2520Causal%2520Analysis%253F%2520A%2520Measurement%2520Perspective%2520on%2520Causal%250A%2520%2520Representations%26entry.906535625%3DDingling%2520Yao%2520and%2520Shimeng%2520Huang%2520and%2520Riccardo%2520Cadei%2520and%2520Kun%2520Zhang%2520and%2520Francesco%2520Locatello%26entry.1292438233%3D%2520%2520Causal%2520reasoning%2520and%2520discovery%252C%2520two%2520fundamental%2520tasks%2520of%2520causal%2520analysis%252C%250Aoften%2520face%2520challenges%2520in%2520applications%2520due%2520to%2520the%2520complexity%252C%2520noisiness%252C%2520and%250Ahigh-dimensionality%2520of%2520real-world%2520data.%2520Despite%2520recent%2520progress%2520in%2520identifying%250Alatent%2520causal%2520structures%2520using%2520causal%2520representation%2520learning%2520%2528CRL%2529%252C%2520what%2520makes%250Alearned%2520representations%2520useful%2520for%2520causal%2520downstream%2520tasks%2520and%2520how%2520to%2520evaluate%250Athem%2520are%2520still%2520not%2520well%2520understood.%2520In%2520this%2520paper%252C%2520we%2520reinterpret%2520CRL%2520using%2520a%250Ameasurement%2520model%2520framework%252C%2520where%2520the%2520learned%2520representations%2520are%2520viewed%2520as%250Aproxy%2520measurements%2520of%2520the%2520latent%2520causal%2520variables.%2520Our%2520approach%2520clarifies%2520the%250Aconditions%2520under%2520which%2520learned%2520representations%2520support%2520downstream%2520causal%250Areasoning%2520and%2520provides%2520a%2520principled%2520basis%2520for%2520quantitatively%2520assessing%2520the%250Aquality%2520of%2520representations%2520using%2520a%2520new%2520Test-based%2520Measurement%2520EXclusivity%250A%2528T-MEX%2529%2520score.%2520We%2520validate%2520T-MEX%2520across%2520diverse%2520causal%2520inference%2520scenarios%252C%250Aincluding%2520numerical%2520simulations%2520and%2520real-world%2520ecological%2520video%2520analysis%252C%250Ademonstrating%2520that%2520the%2520proposed%2520framework%2520and%2520corresponding%2520score%2520effectively%250Aassess%2520the%2520identification%2520of%2520learned%2520representations%2520and%2520their%2520usefulness%2520for%250Acausal%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Third%20Pillar%20of%20Causal%20Analysis%3F%20A%20Measurement%20Perspective%20on%20Causal%0A%20%20Representations&entry.906535625=Dingling%20Yao%20and%20Shimeng%20Huang%20and%20Riccardo%20Cadei%20and%20Kun%20Zhang%20and%20Francesco%20Locatello&entry.1292438233=%20%20Causal%20reasoning%20and%20discovery%2C%20two%20fundamental%20tasks%20of%20causal%20analysis%2C%0Aoften%20face%20challenges%20in%20applications%20due%20to%20the%20complexity%2C%20noisiness%2C%20and%0Ahigh-dimensionality%20of%20real-world%20data.%20Despite%20recent%20progress%20in%20identifying%0Alatent%20causal%20structures%20using%20causal%20representation%20learning%20%28CRL%29%2C%20what%20makes%0Alearned%20representations%20useful%20for%20causal%20downstream%20tasks%20and%20how%20to%20evaluate%0Athem%20are%20still%20not%20well%20understood.%20In%20this%20paper%2C%20we%20reinterpret%20CRL%20using%20a%0Ameasurement%20model%20framework%2C%20where%20the%20learned%20representations%20are%20viewed%20as%0Aproxy%20measurements%20of%20the%20latent%20causal%20variables.%20Our%20approach%20clarifies%20the%0Aconditions%20under%20which%20learned%20representations%20support%20downstream%20causal%0Areasoning%20and%20provides%20a%20principled%20basis%20for%20quantitatively%20assessing%20the%0Aquality%20of%20representations%20using%20a%20new%20Test-based%20Measurement%20EXclusivity%0A%28T-MEX%29%20score.%20We%20validate%20T-MEX%20across%20diverse%20causal%20inference%20scenarios%2C%0Aincluding%20numerical%20simulations%20and%20real-world%20ecological%20video%20analysis%2C%0Ademonstrating%20that%20the%20proposed%20framework%20and%20corresponding%20score%20effectively%0Aassess%20the%20identification%20of%20learned%20representations%20and%20their%20usefulness%20for%0Acausal%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17708v1&entry.124074799=Read"},
{"title": "ERPoT: Effective and Reliable Pose Tracking for Mobile Robots Using\n  Lightweight Polygon Maps", "author": "Haiming Gao and Qibo Qiu and Hongyan Liu and Dingkun Liang and Chaoqun Wang and Xuebo Zhang", "abstract": "  This paper presents an effective and reliable pose tracking solution, termed\nERPoT, for mobile robots operating in large-scale outdoor and challenging\nindoor environments, underpinned by an innovative prior polygon map.\nEspecially, to overcome the challenge that arises as the map size grows with\nthe expansion of the environment, the novel form of a prior map composed of\nmultiple polygons is proposed. Benefiting from the use of polygons to concisely\nand accurately depict environmental occupancy, the prior polygon map achieves\nlong-term reliable pose tracking while ensuring a compact form. More\nimportantly, pose tracking is carried out under pure LiDAR mode, and the dense\n3D point cloud is transformed into a sparse 2D scan through ground removal and\nobstacle selection. On this basis, a novel cost function for pose estimation\nthrough point-polygon matching is introduced, encompassing two distinct\nconstraint forms: point-to-vertex and point-to-edge. In this study, our primary\nfocus lies on two crucial aspects: lightweight and compact prior map\nconstruction, as well as effective and reliable robot pose tracking. Both\naspects serve as the foundational pillars for future navigation across diverse\nmobile platforms equipped with different LiDAR sensors in varied environments.\nComparative experiments based on the publicly available datasets and our\nself-recorded datasets are conducted, and evaluation results show the superior\nperformance of ERPoT on reliability, prior map size, pose estimation error, and\nruntime over the other six approaches. The corresponding code can be accessed\nat https://github.com/ghm0819/ERPoT, and the supplementary video is at\nhttps://youtu.be/cseml5FrW1Q.\n", "link": "http://arxiv.org/abs/2409.14723v3", "date": "2025-05-23", "relevancy": 2.2897, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6086}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5563}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ERPoT%3A%20Effective%20and%20Reliable%20Pose%20Tracking%20for%20Mobile%20Robots%20Using%0A%20%20Lightweight%20Polygon%20Maps&body=Title%3A%20ERPoT%3A%20Effective%20and%20Reliable%20Pose%20Tracking%20for%20Mobile%20Robots%20Using%0A%20%20Lightweight%20Polygon%20Maps%0AAuthor%3A%20Haiming%20Gao%20and%20Qibo%20Qiu%20and%20Hongyan%20Liu%20and%20Dingkun%20Liang%20and%20Chaoqun%20Wang%20and%20Xuebo%20Zhang%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20effective%20and%20reliable%20pose%20tracking%20solution%2C%20termed%0AERPoT%2C%20for%20mobile%20robots%20operating%20in%20large-scale%20outdoor%20and%20challenging%0Aindoor%20environments%2C%20underpinned%20by%20an%20innovative%20prior%20polygon%20map.%0AEspecially%2C%20to%20overcome%20the%20challenge%20that%20arises%20as%20the%20map%20size%20grows%20with%0Athe%20expansion%20of%20the%20environment%2C%20the%20novel%20form%20of%20a%20prior%20map%20composed%20of%0Amultiple%20polygons%20is%20proposed.%20Benefiting%20from%20the%20use%20of%20polygons%20to%20concisely%0Aand%20accurately%20depict%20environmental%20occupancy%2C%20the%20prior%20polygon%20map%20achieves%0Along-term%20reliable%20pose%20tracking%20while%20ensuring%20a%20compact%20form.%20More%0Aimportantly%2C%20pose%20tracking%20is%20carried%20out%20under%20pure%20LiDAR%20mode%2C%20and%20the%20dense%0A3D%20point%20cloud%20is%20transformed%20into%20a%20sparse%202D%20scan%20through%20ground%20removal%20and%0Aobstacle%20selection.%20On%20this%20basis%2C%20a%20novel%20cost%20function%20for%20pose%20estimation%0Athrough%20point-polygon%20matching%20is%20introduced%2C%20encompassing%20two%20distinct%0Aconstraint%20forms%3A%20point-to-vertex%20and%20point-to-edge.%20In%20this%20study%2C%20our%20primary%0Afocus%20lies%20on%20two%20crucial%20aspects%3A%20lightweight%20and%20compact%20prior%20map%0Aconstruction%2C%20as%20well%20as%20effective%20and%20reliable%20robot%20pose%20tracking.%20Both%0Aaspects%20serve%20as%20the%20foundational%20pillars%20for%20future%20navigation%20across%20diverse%0Amobile%20platforms%20equipped%20with%20different%20LiDAR%20sensors%20in%20varied%20environments.%0AComparative%20experiments%20based%20on%20the%20publicly%20available%20datasets%20and%20our%0Aself-recorded%20datasets%20are%20conducted%2C%20and%20evaluation%20results%20show%20the%20superior%0Aperformance%20of%20ERPoT%20on%20reliability%2C%20prior%20map%20size%2C%20pose%20estimation%20error%2C%20and%0Aruntime%20over%20the%20other%20six%20approaches.%20The%20corresponding%20code%20can%20be%20accessed%0Aat%20https%3A//github.com/ghm0819/ERPoT%2C%20and%20the%20supplementary%20video%20is%20at%0Ahttps%3A//youtu.be/cseml5FrW1Q.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14723v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DERPoT%253A%2520Effective%2520and%2520Reliable%2520Pose%2520Tracking%2520for%2520Mobile%2520Robots%2520Using%250A%2520%2520Lightweight%2520Polygon%2520Maps%26entry.906535625%3DHaiming%2520Gao%2520and%2520Qibo%2520Qiu%2520and%2520Hongyan%2520Liu%2520and%2520Dingkun%2520Liang%2520and%2520Chaoqun%2520Wang%2520and%2520Xuebo%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520effective%2520and%2520reliable%2520pose%2520tracking%2520solution%252C%2520termed%250AERPoT%252C%2520for%2520mobile%2520robots%2520operating%2520in%2520large-scale%2520outdoor%2520and%2520challenging%250Aindoor%2520environments%252C%2520underpinned%2520by%2520an%2520innovative%2520prior%2520polygon%2520map.%250AEspecially%252C%2520to%2520overcome%2520the%2520challenge%2520that%2520arises%2520as%2520the%2520map%2520size%2520grows%2520with%250Athe%2520expansion%2520of%2520the%2520environment%252C%2520the%2520novel%2520form%2520of%2520a%2520prior%2520map%2520composed%2520of%250Amultiple%2520polygons%2520is%2520proposed.%2520Benefiting%2520from%2520the%2520use%2520of%2520polygons%2520to%2520concisely%250Aand%2520accurately%2520depict%2520environmental%2520occupancy%252C%2520the%2520prior%2520polygon%2520map%2520achieves%250Along-term%2520reliable%2520pose%2520tracking%2520while%2520ensuring%2520a%2520compact%2520form.%2520More%250Aimportantly%252C%2520pose%2520tracking%2520is%2520carried%2520out%2520under%2520pure%2520LiDAR%2520mode%252C%2520and%2520the%2520dense%250A3D%2520point%2520cloud%2520is%2520transformed%2520into%2520a%2520sparse%25202D%2520scan%2520through%2520ground%2520removal%2520and%250Aobstacle%2520selection.%2520On%2520this%2520basis%252C%2520a%2520novel%2520cost%2520function%2520for%2520pose%2520estimation%250Athrough%2520point-polygon%2520matching%2520is%2520introduced%252C%2520encompassing%2520two%2520distinct%250Aconstraint%2520forms%253A%2520point-to-vertex%2520and%2520point-to-edge.%2520In%2520this%2520study%252C%2520our%2520primary%250Afocus%2520lies%2520on%2520two%2520crucial%2520aspects%253A%2520lightweight%2520and%2520compact%2520prior%2520map%250Aconstruction%252C%2520as%2520well%2520as%2520effective%2520and%2520reliable%2520robot%2520pose%2520tracking.%2520Both%250Aaspects%2520serve%2520as%2520the%2520foundational%2520pillars%2520for%2520future%2520navigation%2520across%2520diverse%250Amobile%2520platforms%2520equipped%2520with%2520different%2520LiDAR%2520sensors%2520in%2520varied%2520environments.%250AComparative%2520experiments%2520based%2520on%2520the%2520publicly%2520available%2520datasets%2520and%2520our%250Aself-recorded%2520datasets%2520are%2520conducted%252C%2520and%2520evaluation%2520results%2520show%2520the%2520superior%250Aperformance%2520of%2520ERPoT%2520on%2520reliability%252C%2520prior%2520map%2520size%252C%2520pose%2520estimation%2520error%252C%2520and%250Aruntime%2520over%2520the%2520other%2520six%2520approaches.%2520The%2520corresponding%2520code%2520can%2520be%2520accessed%250Aat%2520https%253A//github.com/ghm0819/ERPoT%252C%2520and%2520the%2520supplementary%2520video%2520is%2520at%250Ahttps%253A//youtu.be/cseml5FrW1Q.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14723v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ERPoT%3A%20Effective%20and%20Reliable%20Pose%20Tracking%20for%20Mobile%20Robots%20Using%0A%20%20Lightweight%20Polygon%20Maps&entry.906535625=Haiming%20Gao%20and%20Qibo%20Qiu%20and%20Hongyan%20Liu%20and%20Dingkun%20Liang%20and%20Chaoqun%20Wang%20and%20Xuebo%20Zhang&entry.1292438233=%20%20This%20paper%20presents%20an%20effective%20and%20reliable%20pose%20tracking%20solution%2C%20termed%0AERPoT%2C%20for%20mobile%20robots%20operating%20in%20large-scale%20outdoor%20and%20challenging%0Aindoor%20environments%2C%20underpinned%20by%20an%20innovative%20prior%20polygon%20map.%0AEspecially%2C%20to%20overcome%20the%20challenge%20that%20arises%20as%20the%20map%20size%20grows%20with%0Athe%20expansion%20of%20the%20environment%2C%20the%20novel%20form%20of%20a%20prior%20map%20composed%20of%0Amultiple%20polygons%20is%20proposed.%20Benefiting%20from%20the%20use%20of%20polygons%20to%20concisely%0Aand%20accurately%20depict%20environmental%20occupancy%2C%20the%20prior%20polygon%20map%20achieves%0Along-term%20reliable%20pose%20tracking%20while%20ensuring%20a%20compact%20form.%20More%0Aimportantly%2C%20pose%20tracking%20is%20carried%20out%20under%20pure%20LiDAR%20mode%2C%20and%20the%20dense%0A3D%20point%20cloud%20is%20transformed%20into%20a%20sparse%202D%20scan%20through%20ground%20removal%20and%0Aobstacle%20selection.%20On%20this%20basis%2C%20a%20novel%20cost%20function%20for%20pose%20estimation%0Athrough%20point-polygon%20matching%20is%20introduced%2C%20encompassing%20two%20distinct%0Aconstraint%20forms%3A%20point-to-vertex%20and%20point-to-edge.%20In%20this%20study%2C%20our%20primary%0Afocus%20lies%20on%20two%20crucial%20aspects%3A%20lightweight%20and%20compact%20prior%20map%0Aconstruction%2C%20as%20well%20as%20effective%20and%20reliable%20robot%20pose%20tracking.%20Both%0Aaspects%20serve%20as%20the%20foundational%20pillars%20for%20future%20navigation%20across%20diverse%0Amobile%20platforms%20equipped%20with%20different%20LiDAR%20sensors%20in%20varied%20environments.%0AComparative%20experiments%20based%20on%20the%20publicly%20available%20datasets%20and%20our%0Aself-recorded%20datasets%20are%20conducted%2C%20and%20evaluation%20results%20show%20the%20superior%0Aperformance%20of%20ERPoT%20on%20reliability%2C%20prior%20map%20size%2C%20pose%20estimation%20error%2C%20and%0Aruntime%20over%20the%20other%20six%20approaches.%20The%20corresponding%20code%20can%20be%20accessed%0Aat%20https%3A//github.com/ghm0819/ERPoT%2C%20and%20the%20supplementary%20video%20is%20at%0Ahttps%3A//youtu.be/cseml5FrW1Q.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14723v3&entry.124074799=Read"},
{"title": "Temporal Consistency Constrained Transferable Adversarial Attacks with\n  Background Mixup for Action Recognition", "author": "Ping Li and Jianan Ni and Bo Pang", "abstract": "  Action recognition models using deep learning are vulnerable to adversarial\nexamples, which are transferable across other models trained on the same data\nmodality. Existing transferable attack methods face two major challenges: 1)\nthey heavily rely on the assumption that the decision boundaries of the\nsurrogate (a.k.a., source) model and the target model are similar, which limits\nthe adversarial transferability; and 2) their decision boundary difference\nmakes the attack direction uncertain, which may result in the gradient\noscillation, weakening the adversarial attack. This motivates us to propose a\nBackground Mixup-induced Temporal Consistency (BMTC) attack method for action\nrecognition. From the input transformation perspective, we design a\nmodel-agnostic background adversarial mixup module to reduce the\nsurrogate-target model dependency. In particular, we randomly sample one video\nfrom each category and make its background frame, while selecting the\nbackground frame with the top attack ability for mixup with the clean frame by\nreinforcement learning. Moreover, to ensure an explicit attack direction, we\nleverage the background category as guidance for updating the gradient of\nadversarial example, and design a temporal gradient consistency loss, which\nstrengthens the stability of the attack direction on subsequent frames.\nEmpirical studies on two video datasets, i.e., UCF101 and Kinetics-400, and one\nimage dataset, i.e., ImageNet, demonstrate that our method significantly boosts\nthe transferability of adversarial examples across several action/image\nrecognition models. Our code is available at\nhttps://github.com/mlvccn/BMTC_TransferAttackVid.\n", "link": "http://arxiv.org/abs/2505.17807v1", "date": "2025-05-23", "relevancy": 2.2845, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.577}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5722}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Consistency%20Constrained%20Transferable%20Adversarial%20Attacks%20with%0A%20%20Background%20Mixup%20for%20Action%20Recognition&body=Title%3A%20Temporal%20Consistency%20Constrained%20Transferable%20Adversarial%20Attacks%20with%0A%20%20Background%20Mixup%20for%20Action%20Recognition%0AAuthor%3A%20Ping%20Li%20and%20Jianan%20Ni%20and%20Bo%20Pang%0AAbstract%3A%20%20%20Action%20recognition%20models%20using%20deep%20learning%20are%20vulnerable%20to%20adversarial%0Aexamples%2C%20which%20are%20transferable%20across%20other%20models%20trained%20on%20the%20same%20data%0Amodality.%20Existing%20transferable%20attack%20methods%20face%20two%20major%20challenges%3A%201%29%0Athey%20heavily%20rely%20on%20the%20assumption%20that%20the%20decision%20boundaries%20of%20the%0Asurrogate%20%28a.k.a.%2C%20source%29%20model%20and%20the%20target%20model%20are%20similar%2C%20which%20limits%0Athe%20adversarial%20transferability%3B%20and%202%29%20their%20decision%20boundary%20difference%0Amakes%20the%20attack%20direction%20uncertain%2C%20which%20may%20result%20in%20the%20gradient%0Aoscillation%2C%20weakening%20the%20adversarial%20attack.%20This%20motivates%20us%20to%20propose%20a%0ABackground%20Mixup-induced%20Temporal%20Consistency%20%28BMTC%29%20attack%20method%20for%20action%0Arecognition.%20From%20the%20input%20transformation%20perspective%2C%20we%20design%20a%0Amodel-agnostic%20background%20adversarial%20mixup%20module%20to%20reduce%20the%0Asurrogate-target%20model%20dependency.%20In%20particular%2C%20we%20randomly%20sample%20one%20video%0Afrom%20each%20category%20and%20make%20its%20background%20frame%2C%20while%20selecting%20the%0Abackground%20frame%20with%20the%20top%20attack%20ability%20for%20mixup%20with%20the%20clean%20frame%20by%0Areinforcement%20learning.%20Moreover%2C%20to%20ensure%20an%20explicit%20attack%20direction%2C%20we%0Aleverage%20the%20background%20category%20as%20guidance%20for%20updating%20the%20gradient%20of%0Aadversarial%20example%2C%20and%20design%20a%20temporal%20gradient%20consistency%20loss%2C%20which%0Astrengthens%20the%20stability%20of%20the%20attack%20direction%20on%20subsequent%20frames.%0AEmpirical%20studies%20on%20two%20video%20datasets%2C%20i.e.%2C%20UCF101%20and%20Kinetics-400%2C%20and%20one%0Aimage%20dataset%2C%20i.e.%2C%20ImageNet%2C%20demonstrate%20that%20our%20method%20significantly%20boosts%0Athe%20transferability%20of%20adversarial%20examples%20across%20several%20action/image%0Arecognition%20models.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mlvccn/BMTC_TransferAttackVid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Consistency%2520Constrained%2520Transferable%2520Adversarial%2520Attacks%2520with%250A%2520%2520Background%2520Mixup%2520for%2520Action%2520Recognition%26entry.906535625%3DPing%2520Li%2520and%2520Jianan%2520Ni%2520and%2520Bo%2520Pang%26entry.1292438233%3D%2520%2520Action%2520recognition%2520models%2520using%2520deep%2520learning%2520are%2520vulnerable%2520to%2520adversarial%250Aexamples%252C%2520which%2520are%2520transferable%2520across%2520other%2520models%2520trained%2520on%2520the%2520same%2520data%250Amodality.%2520Existing%2520transferable%2520attack%2520methods%2520face%2520two%2520major%2520challenges%253A%25201%2529%250Athey%2520heavily%2520rely%2520on%2520the%2520assumption%2520that%2520the%2520decision%2520boundaries%2520of%2520the%250Asurrogate%2520%2528a.k.a.%252C%2520source%2529%2520model%2520and%2520the%2520target%2520model%2520are%2520similar%252C%2520which%2520limits%250Athe%2520adversarial%2520transferability%253B%2520and%25202%2529%2520their%2520decision%2520boundary%2520difference%250Amakes%2520the%2520attack%2520direction%2520uncertain%252C%2520which%2520may%2520result%2520in%2520the%2520gradient%250Aoscillation%252C%2520weakening%2520the%2520adversarial%2520attack.%2520This%2520motivates%2520us%2520to%2520propose%2520a%250ABackground%2520Mixup-induced%2520Temporal%2520Consistency%2520%2528BMTC%2529%2520attack%2520method%2520for%2520action%250Arecognition.%2520From%2520the%2520input%2520transformation%2520perspective%252C%2520we%2520design%2520a%250Amodel-agnostic%2520background%2520adversarial%2520mixup%2520module%2520to%2520reduce%2520the%250Asurrogate-target%2520model%2520dependency.%2520In%2520particular%252C%2520we%2520randomly%2520sample%2520one%2520video%250Afrom%2520each%2520category%2520and%2520make%2520its%2520background%2520frame%252C%2520while%2520selecting%2520the%250Abackground%2520frame%2520with%2520the%2520top%2520attack%2520ability%2520for%2520mixup%2520with%2520the%2520clean%2520frame%2520by%250Areinforcement%2520learning.%2520Moreover%252C%2520to%2520ensure%2520an%2520explicit%2520attack%2520direction%252C%2520we%250Aleverage%2520the%2520background%2520category%2520as%2520guidance%2520for%2520updating%2520the%2520gradient%2520of%250Aadversarial%2520example%252C%2520and%2520design%2520a%2520temporal%2520gradient%2520consistency%2520loss%252C%2520which%250Astrengthens%2520the%2520stability%2520of%2520the%2520attack%2520direction%2520on%2520subsequent%2520frames.%250AEmpirical%2520studies%2520on%2520two%2520video%2520datasets%252C%2520i.e.%252C%2520UCF101%2520and%2520Kinetics-400%252C%2520and%2520one%250Aimage%2520dataset%252C%2520i.e.%252C%2520ImageNet%252C%2520demonstrate%2520that%2520our%2520method%2520significantly%2520boosts%250Athe%2520transferability%2520of%2520adversarial%2520examples%2520across%2520several%2520action/image%250Arecognition%2520models.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/mlvccn/BMTC_TransferAttackVid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Consistency%20Constrained%20Transferable%20Adversarial%20Attacks%20with%0A%20%20Background%20Mixup%20for%20Action%20Recognition&entry.906535625=Ping%20Li%20and%20Jianan%20Ni%20and%20Bo%20Pang&entry.1292438233=%20%20Action%20recognition%20models%20using%20deep%20learning%20are%20vulnerable%20to%20adversarial%0Aexamples%2C%20which%20are%20transferable%20across%20other%20models%20trained%20on%20the%20same%20data%0Amodality.%20Existing%20transferable%20attack%20methods%20face%20two%20major%20challenges%3A%201%29%0Athey%20heavily%20rely%20on%20the%20assumption%20that%20the%20decision%20boundaries%20of%20the%0Asurrogate%20%28a.k.a.%2C%20source%29%20model%20and%20the%20target%20model%20are%20similar%2C%20which%20limits%0Athe%20adversarial%20transferability%3B%20and%202%29%20their%20decision%20boundary%20difference%0Amakes%20the%20attack%20direction%20uncertain%2C%20which%20may%20result%20in%20the%20gradient%0Aoscillation%2C%20weakening%20the%20adversarial%20attack.%20This%20motivates%20us%20to%20propose%20a%0ABackground%20Mixup-induced%20Temporal%20Consistency%20%28BMTC%29%20attack%20method%20for%20action%0Arecognition.%20From%20the%20input%20transformation%20perspective%2C%20we%20design%20a%0Amodel-agnostic%20background%20adversarial%20mixup%20module%20to%20reduce%20the%0Asurrogate-target%20model%20dependency.%20In%20particular%2C%20we%20randomly%20sample%20one%20video%0Afrom%20each%20category%20and%20make%20its%20background%20frame%2C%20while%20selecting%20the%0Abackground%20frame%20with%20the%20top%20attack%20ability%20for%20mixup%20with%20the%20clean%20frame%20by%0Areinforcement%20learning.%20Moreover%2C%20to%20ensure%20an%20explicit%20attack%20direction%2C%20we%0Aleverage%20the%20background%20category%20as%20guidance%20for%20updating%20the%20gradient%20of%0Aadversarial%20example%2C%20and%20design%20a%20temporal%20gradient%20consistency%20loss%2C%20which%0Astrengthens%20the%20stability%20of%20the%20attack%20direction%20on%20subsequent%20frames.%0AEmpirical%20studies%20on%20two%20video%20datasets%2C%20i.e.%2C%20UCF101%20and%20Kinetics-400%2C%20and%20one%0Aimage%20dataset%2C%20i.e.%2C%20ImageNet%2C%20demonstrate%20that%20our%20method%20significantly%20boosts%0Athe%20transferability%20of%20adversarial%20examples%20across%20several%20action/image%0Arecognition%20models.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mlvccn/BMTC_TransferAttackVid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17807v1&entry.124074799=Read"},
{"title": "SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the\n  Real World Domain", "author": "Jiawei Zhou and Linye Lyu and Zhuotao Tian and Cheng Zhuo and Yu Li", "abstract": "  Safety-critical scenarios are rare yet pivotal for evaluating and enhancing\nthe robustness of autonomous driving systems. While existing methods generate\nsafety-critical driving trajectories, simulations, or single-view videos, they\nfall short of meeting the demands of advanced end-to-end autonomous systems\n(E2E AD), which require real-world, multi-view video data. To bridge this gap,\nwe introduce SafeMVDrive, the first framework designed to generate\nhigh-quality, safety-critical, multi-view driving videos grounded in real-world\ndomains. SafeMVDrive strategically integrates a safety-critical trajectory\ngenerator with an advanced multi-view video generator. To tackle the challenges\ninherent in this integration, we first enhance scene understanding ability of\nthe trajectory generator by incorporating visual context -- which is previously\nunavailable to such generator -- and leveraging a GRPO-finetuned\nvision-language model to achieve more realistic and context-aware trajectory\ngeneration. Second, recognizing that existing multi-view video generators\nstruggle to render realistic collision events, we introduce a two-stage,\ncontrollable trajectory generation mechanism that produces collision-evasion\ntrajectories, ensuring both video quality and safety-critical fidelity.\nFinally, we employ a diffusion-based multi-view video generator to synthesize\nhigh-quality safety-critical driving videos from the generated trajectories.\nExperiments conducted on an E2E AD planner demonstrate a significant increase\nin collision rate when tested with our generated data, validating the\neffectiveness of SafeMVDrive in stress-testing planning modules. Our code,\nexamples, and datasets are publicly available at:\nhttps://zhoujiawei3.github.io/SafeMVDrive/.\n", "link": "http://arxiv.org/abs/2505.17727v1", "date": "2025-05-23", "relevancy": 2.2804, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.598}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5684}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafeMVDrive%3A%20Multi-view%20Safety-Critical%20Driving%20Video%20Synthesis%20in%20the%0A%20%20Real%20World%20Domain&body=Title%3A%20SafeMVDrive%3A%20Multi-view%20Safety-Critical%20Driving%20Video%20Synthesis%20in%20the%0A%20%20Real%20World%20Domain%0AAuthor%3A%20Jiawei%20Zhou%20and%20Linye%20Lyu%20and%20Zhuotao%20Tian%20and%20Cheng%20Zhuo%20and%20Yu%20Li%0AAbstract%3A%20%20%20Safety-critical%20scenarios%20are%20rare%20yet%20pivotal%20for%20evaluating%20and%20enhancing%0Athe%20robustness%20of%20autonomous%20driving%20systems.%20While%20existing%20methods%20generate%0Asafety-critical%20driving%20trajectories%2C%20simulations%2C%20or%20single-view%20videos%2C%20they%0Afall%20short%20of%20meeting%20the%20demands%20of%20advanced%20end-to-end%20autonomous%20systems%0A%28E2E%20AD%29%2C%20which%20require%20real-world%2C%20multi-view%20video%20data.%20To%20bridge%20this%20gap%2C%0Awe%20introduce%20SafeMVDrive%2C%20the%20first%20framework%20designed%20to%20generate%0Ahigh-quality%2C%20safety-critical%2C%20multi-view%20driving%20videos%20grounded%20in%20real-world%0Adomains.%20SafeMVDrive%20strategically%20integrates%20a%20safety-critical%20trajectory%0Agenerator%20with%20an%20advanced%20multi-view%20video%20generator.%20To%20tackle%20the%20challenges%0Ainherent%20in%20this%20integration%2C%20we%20first%20enhance%20scene%20understanding%20ability%20of%0Athe%20trajectory%20generator%20by%20incorporating%20visual%20context%20--%20which%20is%20previously%0Aunavailable%20to%20such%20generator%20--%20and%20leveraging%20a%20GRPO-finetuned%0Avision-language%20model%20to%20achieve%20more%20realistic%20and%20context-aware%20trajectory%0Ageneration.%20Second%2C%20recognizing%20that%20existing%20multi-view%20video%20generators%0Astruggle%20to%20render%20realistic%20collision%20events%2C%20we%20introduce%20a%20two-stage%2C%0Acontrollable%20trajectory%20generation%20mechanism%20that%20produces%20collision-evasion%0Atrajectories%2C%20ensuring%20both%20video%20quality%20and%20safety-critical%20fidelity.%0AFinally%2C%20we%20employ%20a%20diffusion-based%20multi-view%20video%20generator%20to%20synthesize%0Ahigh-quality%20safety-critical%20driving%20videos%20from%20the%20generated%20trajectories.%0AExperiments%20conducted%20on%20an%20E2E%20AD%20planner%20demonstrate%20a%20significant%20increase%0Ain%20collision%20rate%20when%20tested%20with%20our%20generated%20data%2C%20validating%20the%0Aeffectiveness%20of%20SafeMVDrive%20in%20stress-testing%20planning%20modules.%20Our%20code%2C%0Aexamples%2C%20and%20datasets%20are%20publicly%20available%20at%3A%0Ahttps%3A//zhoujiawei3.github.io/SafeMVDrive/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafeMVDrive%253A%2520Multi-view%2520Safety-Critical%2520Driving%2520Video%2520Synthesis%2520in%2520the%250A%2520%2520Real%2520World%2520Domain%26entry.906535625%3DJiawei%2520Zhou%2520and%2520Linye%2520Lyu%2520and%2520Zhuotao%2520Tian%2520and%2520Cheng%2520Zhuo%2520and%2520Yu%2520Li%26entry.1292438233%3D%2520%2520Safety-critical%2520scenarios%2520are%2520rare%2520yet%2520pivotal%2520for%2520evaluating%2520and%2520enhancing%250Athe%2520robustness%2520of%2520autonomous%2520driving%2520systems.%2520While%2520existing%2520methods%2520generate%250Asafety-critical%2520driving%2520trajectories%252C%2520simulations%252C%2520or%2520single-view%2520videos%252C%2520they%250Afall%2520short%2520of%2520meeting%2520the%2520demands%2520of%2520advanced%2520end-to-end%2520autonomous%2520systems%250A%2528E2E%2520AD%2529%252C%2520which%2520require%2520real-world%252C%2520multi-view%2520video%2520data.%2520To%2520bridge%2520this%2520gap%252C%250Awe%2520introduce%2520SafeMVDrive%252C%2520the%2520first%2520framework%2520designed%2520to%2520generate%250Ahigh-quality%252C%2520safety-critical%252C%2520multi-view%2520driving%2520videos%2520grounded%2520in%2520real-world%250Adomains.%2520SafeMVDrive%2520strategically%2520integrates%2520a%2520safety-critical%2520trajectory%250Agenerator%2520with%2520an%2520advanced%2520multi-view%2520video%2520generator.%2520To%2520tackle%2520the%2520challenges%250Ainherent%2520in%2520this%2520integration%252C%2520we%2520first%2520enhance%2520scene%2520understanding%2520ability%2520of%250Athe%2520trajectory%2520generator%2520by%2520incorporating%2520visual%2520context%2520--%2520which%2520is%2520previously%250Aunavailable%2520to%2520such%2520generator%2520--%2520and%2520leveraging%2520a%2520GRPO-finetuned%250Avision-language%2520model%2520to%2520achieve%2520more%2520realistic%2520and%2520context-aware%2520trajectory%250Ageneration.%2520Second%252C%2520recognizing%2520that%2520existing%2520multi-view%2520video%2520generators%250Astruggle%2520to%2520render%2520realistic%2520collision%2520events%252C%2520we%2520introduce%2520a%2520two-stage%252C%250Acontrollable%2520trajectory%2520generation%2520mechanism%2520that%2520produces%2520collision-evasion%250Atrajectories%252C%2520ensuring%2520both%2520video%2520quality%2520and%2520safety-critical%2520fidelity.%250AFinally%252C%2520we%2520employ%2520a%2520diffusion-based%2520multi-view%2520video%2520generator%2520to%2520synthesize%250Ahigh-quality%2520safety-critical%2520driving%2520videos%2520from%2520the%2520generated%2520trajectories.%250AExperiments%2520conducted%2520on%2520an%2520E2E%2520AD%2520planner%2520demonstrate%2520a%2520significant%2520increase%250Ain%2520collision%2520rate%2520when%2520tested%2520with%2520our%2520generated%2520data%252C%2520validating%2520the%250Aeffectiveness%2520of%2520SafeMVDrive%2520in%2520stress-testing%2520planning%2520modules.%2520Our%2520code%252C%250Aexamples%252C%2520and%2520datasets%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//zhoujiawei3.github.io/SafeMVDrive/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafeMVDrive%3A%20Multi-view%20Safety-Critical%20Driving%20Video%20Synthesis%20in%20the%0A%20%20Real%20World%20Domain&entry.906535625=Jiawei%20Zhou%20and%20Linye%20Lyu%20and%20Zhuotao%20Tian%20and%20Cheng%20Zhuo%20and%20Yu%20Li&entry.1292438233=%20%20Safety-critical%20scenarios%20are%20rare%20yet%20pivotal%20for%20evaluating%20and%20enhancing%0Athe%20robustness%20of%20autonomous%20driving%20systems.%20While%20existing%20methods%20generate%0Asafety-critical%20driving%20trajectories%2C%20simulations%2C%20or%20single-view%20videos%2C%20they%0Afall%20short%20of%20meeting%20the%20demands%20of%20advanced%20end-to-end%20autonomous%20systems%0A%28E2E%20AD%29%2C%20which%20require%20real-world%2C%20multi-view%20video%20data.%20To%20bridge%20this%20gap%2C%0Awe%20introduce%20SafeMVDrive%2C%20the%20first%20framework%20designed%20to%20generate%0Ahigh-quality%2C%20safety-critical%2C%20multi-view%20driving%20videos%20grounded%20in%20real-world%0Adomains.%20SafeMVDrive%20strategically%20integrates%20a%20safety-critical%20trajectory%0Agenerator%20with%20an%20advanced%20multi-view%20video%20generator.%20To%20tackle%20the%20challenges%0Ainherent%20in%20this%20integration%2C%20we%20first%20enhance%20scene%20understanding%20ability%20of%0Athe%20trajectory%20generator%20by%20incorporating%20visual%20context%20--%20which%20is%20previously%0Aunavailable%20to%20such%20generator%20--%20and%20leveraging%20a%20GRPO-finetuned%0Avision-language%20model%20to%20achieve%20more%20realistic%20and%20context-aware%20trajectory%0Ageneration.%20Second%2C%20recognizing%20that%20existing%20multi-view%20video%20generators%0Astruggle%20to%20render%20realistic%20collision%20events%2C%20we%20introduce%20a%20two-stage%2C%0Acontrollable%20trajectory%20generation%20mechanism%20that%20produces%20collision-evasion%0Atrajectories%2C%20ensuring%20both%20video%20quality%20and%20safety-critical%20fidelity.%0AFinally%2C%20we%20employ%20a%20diffusion-based%20multi-view%20video%20generator%20to%20synthesize%0Ahigh-quality%20safety-critical%20driving%20videos%20from%20the%20generated%20trajectories.%0AExperiments%20conducted%20on%20an%20E2E%20AD%20planner%20demonstrate%20a%20significant%20increase%0Ain%20collision%20rate%20when%20tested%20with%20our%20generated%20data%2C%20validating%20the%0Aeffectiveness%20of%20SafeMVDrive%20in%20stress-testing%20planning%20modules.%20Our%20code%2C%0Aexamples%2C%20and%20datasets%20are%20publicly%20available%20at%3A%0Ahttps%3A//zhoujiawei3.github.io/SafeMVDrive/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17727v1&entry.124074799=Read"},
{"title": "Compression via Pre-trained Transformers: A Study on Byte-Level\n  Multimodal Data", "author": "David Heurtel-Depeiges and Anian Ruoss and Joel Veness and Tim Genewein", "abstract": "  Foundation models are strong data compressors, but when accounting for their\nparameter size, their compression ratios are inferior to standard compression\nalgorithms. Naively reducing the parameter count does not necessarily help as\nit deteriorates predictions and, accordingly, compression. We conduct a\nlarge-scale empirical study to find a sweet spot where pre-trained vanilla\ntransformers can achieve competitive compression ratios. To this end, we train\nmodels on 165GB of raw byte sequences of either text, image, or audio data (and\nall possible combinations of the three) and then compress 1GB of\nout-of-distribution (OOD) data from each modality. We find that relatively\nsmall models (millions of parameters) can outperform standard general-purpose\ncompression algorithms (gzip, LZMA2) and even domain-specific compressors (PNG,\nJPEG-XL, FLAC) $\\unicode{x2013}$ even when accounting for parameter size. We\nachieve, e.g., the lowest compression ratio of 0.49 on OOD audio data (vs. 0.54\nfor FLAC). We conduct extensive ablations and hyperparameter sweeps to study\nthe impact of model- and dataset scale, and we investigate the effect of\nunimodal versus multimodal training. We find that even small models can be\ntrained to perform well on multiple modalities, but unlike large-scale\nfoundation models, transfer to unseen modalities is generally weak.\n", "link": "http://arxiv.org/abs/2410.05078v2", "date": "2025-05-23", "relevancy": 2.2655, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5673}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5673}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compression%20via%20Pre-trained%20Transformers%3A%20A%20Study%20on%20Byte-Level%0A%20%20Multimodal%20Data&body=Title%3A%20Compression%20via%20Pre-trained%20Transformers%3A%20A%20Study%20on%20Byte-Level%0A%20%20Multimodal%20Data%0AAuthor%3A%20David%20Heurtel-Depeiges%20and%20Anian%20Ruoss%20and%20Joel%20Veness%20and%20Tim%20Genewein%0AAbstract%3A%20%20%20Foundation%20models%20are%20strong%20data%20compressors%2C%20but%20when%20accounting%20for%20their%0Aparameter%20size%2C%20their%20compression%20ratios%20are%20inferior%20to%20standard%20compression%0Aalgorithms.%20Naively%20reducing%20the%20parameter%20count%20does%20not%20necessarily%20help%20as%0Ait%20deteriorates%20predictions%20and%2C%20accordingly%2C%20compression.%20We%20conduct%20a%0Alarge-scale%20empirical%20study%20to%20find%20a%20sweet%20spot%20where%20pre-trained%20vanilla%0Atransformers%20can%20achieve%20competitive%20compression%20ratios.%20To%20this%20end%2C%20we%20train%0Amodels%20on%20165GB%20of%20raw%20byte%20sequences%20of%20either%20text%2C%20image%2C%20or%20audio%20data%20%28and%0Aall%20possible%20combinations%20of%20the%20three%29%20and%20then%20compress%201GB%20of%0Aout-of-distribution%20%28OOD%29%20data%20from%20each%20modality.%20We%20find%20that%20relatively%0Asmall%20models%20%28millions%20of%20parameters%29%20can%20outperform%20standard%20general-purpose%0Acompression%20algorithms%20%28gzip%2C%20LZMA2%29%20and%20even%20domain-specific%20compressors%20%28PNG%2C%0AJPEG-XL%2C%20FLAC%29%20%24%5Cunicode%7Bx2013%7D%24%20even%20when%20accounting%20for%20parameter%20size.%20We%0Aachieve%2C%20e.g.%2C%20the%20lowest%20compression%20ratio%20of%200.49%20on%20OOD%20audio%20data%20%28vs.%200.54%0Afor%20FLAC%29.%20We%20conduct%20extensive%20ablations%20and%20hyperparameter%20sweeps%20to%20study%0Athe%20impact%20of%20model-%20and%20dataset%20scale%2C%20and%20we%20investigate%20the%20effect%20of%0Aunimodal%20versus%20multimodal%20training.%20We%20find%20that%20even%20small%20models%20can%20be%0Atrained%20to%20perform%20well%20on%20multiple%20modalities%2C%20but%20unlike%20large-scale%0Afoundation%20models%2C%20transfer%20to%20unseen%20modalities%20is%20generally%20weak.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05078v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompression%2520via%2520Pre-trained%2520Transformers%253A%2520A%2520Study%2520on%2520Byte-Level%250A%2520%2520Multimodal%2520Data%26entry.906535625%3DDavid%2520Heurtel-Depeiges%2520and%2520Anian%2520Ruoss%2520and%2520Joel%2520Veness%2520and%2520Tim%2520Genewein%26entry.1292438233%3D%2520%2520Foundation%2520models%2520are%2520strong%2520data%2520compressors%252C%2520but%2520when%2520accounting%2520for%2520their%250Aparameter%2520size%252C%2520their%2520compression%2520ratios%2520are%2520inferior%2520to%2520standard%2520compression%250Aalgorithms.%2520Naively%2520reducing%2520the%2520parameter%2520count%2520does%2520not%2520necessarily%2520help%2520as%250Ait%2520deteriorates%2520predictions%2520and%252C%2520accordingly%252C%2520compression.%2520We%2520conduct%2520a%250Alarge-scale%2520empirical%2520study%2520to%2520find%2520a%2520sweet%2520spot%2520where%2520pre-trained%2520vanilla%250Atransformers%2520can%2520achieve%2520competitive%2520compression%2520ratios.%2520To%2520this%2520end%252C%2520we%2520train%250Amodels%2520on%2520165GB%2520of%2520raw%2520byte%2520sequences%2520of%2520either%2520text%252C%2520image%252C%2520or%2520audio%2520data%2520%2528and%250Aall%2520possible%2520combinations%2520of%2520the%2520three%2529%2520and%2520then%2520compress%25201GB%2520of%250Aout-of-distribution%2520%2528OOD%2529%2520data%2520from%2520each%2520modality.%2520We%2520find%2520that%2520relatively%250Asmall%2520models%2520%2528millions%2520of%2520parameters%2529%2520can%2520outperform%2520standard%2520general-purpose%250Acompression%2520algorithms%2520%2528gzip%252C%2520LZMA2%2529%2520and%2520even%2520domain-specific%2520compressors%2520%2528PNG%252C%250AJPEG-XL%252C%2520FLAC%2529%2520%2524%255Cunicode%257Bx2013%257D%2524%2520even%2520when%2520accounting%2520for%2520parameter%2520size.%2520We%250Aachieve%252C%2520e.g.%252C%2520the%2520lowest%2520compression%2520ratio%2520of%25200.49%2520on%2520OOD%2520audio%2520data%2520%2528vs.%25200.54%250Afor%2520FLAC%2529.%2520We%2520conduct%2520extensive%2520ablations%2520and%2520hyperparameter%2520sweeps%2520to%2520study%250Athe%2520impact%2520of%2520model-%2520and%2520dataset%2520scale%252C%2520and%2520we%2520investigate%2520the%2520effect%2520of%250Aunimodal%2520versus%2520multimodal%2520training.%2520We%2520find%2520that%2520even%2520small%2520models%2520can%2520be%250Atrained%2520to%2520perform%2520well%2520on%2520multiple%2520modalities%252C%2520but%2520unlike%2520large-scale%250Afoundation%2520models%252C%2520transfer%2520to%2520unseen%2520modalities%2520is%2520generally%2520weak.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05078v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compression%20via%20Pre-trained%20Transformers%3A%20A%20Study%20on%20Byte-Level%0A%20%20Multimodal%20Data&entry.906535625=David%20Heurtel-Depeiges%20and%20Anian%20Ruoss%20and%20Joel%20Veness%20and%20Tim%20Genewein&entry.1292438233=%20%20Foundation%20models%20are%20strong%20data%20compressors%2C%20but%20when%20accounting%20for%20their%0Aparameter%20size%2C%20their%20compression%20ratios%20are%20inferior%20to%20standard%20compression%0Aalgorithms.%20Naively%20reducing%20the%20parameter%20count%20does%20not%20necessarily%20help%20as%0Ait%20deteriorates%20predictions%20and%2C%20accordingly%2C%20compression.%20We%20conduct%20a%0Alarge-scale%20empirical%20study%20to%20find%20a%20sweet%20spot%20where%20pre-trained%20vanilla%0Atransformers%20can%20achieve%20competitive%20compression%20ratios.%20To%20this%20end%2C%20we%20train%0Amodels%20on%20165GB%20of%20raw%20byte%20sequences%20of%20either%20text%2C%20image%2C%20or%20audio%20data%20%28and%0Aall%20possible%20combinations%20of%20the%20three%29%20and%20then%20compress%201GB%20of%0Aout-of-distribution%20%28OOD%29%20data%20from%20each%20modality.%20We%20find%20that%20relatively%0Asmall%20models%20%28millions%20of%20parameters%29%20can%20outperform%20standard%20general-purpose%0Acompression%20algorithms%20%28gzip%2C%20LZMA2%29%20and%20even%20domain-specific%20compressors%20%28PNG%2C%0AJPEG-XL%2C%20FLAC%29%20%24%5Cunicode%7Bx2013%7D%24%20even%20when%20accounting%20for%20parameter%20size.%20We%0Aachieve%2C%20e.g.%2C%20the%20lowest%20compression%20ratio%20of%200.49%20on%20OOD%20audio%20data%20%28vs.%200.54%0Afor%20FLAC%29.%20We%20conduct%20extensive%20ablations%20and%20hyperparameter%20sweeps%20to%20study%0Athe%20impact%20of%20model-%20and%20dataset%20scale%2C%20and%20we%20investigate%20the%20effect%20of%0Aunimodal%20versus%20multimodal%20training.%20We%20find%20that%20even%20small%20models%20can%20be%0Atrained%20to%20perform%20well%20on%20multiple%20modalities%2C%20but%20unlike%20large-scale%0Afoundation%20models%2C%20transfer%20to%20unseen%20modalities%20is%20generally%20weak.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05078v2&entry.124074799=Read"},
{"title": "RQR3D: Reparametrizing the regression targets for BEV-based 3D object\n  detection", "author": "Ozsel Kilinc and Cem Tarhan", "abstract": "  Accurate, fast, and reliable 3D perception is essential for autonomous\ndriving. Recently, bird's-eye view (BEV)-based perception approaches have\nemerged as superior alternatives to perspective-based solutions, offering\nenhanced spatial understanding and more natural outputs for planning. Existing\nBEV-based 3D object detection methods, typically adhering to angle-based\nrepresentation, directly estimate the size and orientation of rotated bounding\nboxes. We observe that BEV-based 3D object detection is analogous to aerial\noriented object detection, where angle-based methods are recognized for being\naffected by discontinuities in their loss functions. Drawing inspiration from\nthis domain, we propose Restricted Quadrilateral Representation to define 3D\nregression targets. RQR3D regresses the smallest horizontal bounding box\nencapsulating the oriented box, along with the offsets between the corners of\nthese two boxes, thereby transforming the oriented object detection problem\ninto a keypoint regression task. RQR3D is compatible with any 3D object\ndetection approach. We employ RQR3D within an anchor-free single-stage object\ndetection method and introduce an objectness head to address class imbalance\nproblem. Furthermore, we introduce a simplified radar fusion backbone that\neliminates the need for voxel grouping and processes the BEV-mapped point cloud\nwith standard 2D convolutions, rather than sparse convolutions. Extensive\nevaluations on the nuScenes dataset demonstrate that RQR3D achieves\nstate-of-the-art performance in camera-radar 3D object detection, outperforming\nthe previous best method by +4% in NDS and +2.4% in mAP, and significantly\nreducing the translation and orientation errors, which are crucial for safe\nautonomous driving. These consistent gains highlight the robustness, precision,\nand real-world readiness of our approach.\n", "link": "http://arxiv.org/abs/2505.17732v1", "date": "2025-05-23", "relevancy": 2.2639, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5713}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5635}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RQR3D%3A%20Reparametrizing%20the%20regression%20targets%20for%20BEV-based%203D%20object%0A%20%20detection&body=Title%3A%20RQR3D%3A%20Reparametrizing%20the%20regression%20targets%20for%20BEV-based%203D%20object%0A%20%20detection%0AAuthor%3A%20Ozsel%20Kilinc%20and%20Cem%20Tarhan%0AAbstract%3A%20%20%20Accurate%2C%20fast%2C%20and%20reliable%203D%20perception%20is%20essential%20for%20autonomous%0Adriving.%20Recently%2C%20bird%27s-eye%20view%20%28BEV%29-based%20perception%20approaches%20have%0Aemerged%20as%20superior%20alternatives%20to%20perspective-based%20solutions%2C%20offering%0Aenhanced%20spatial%20understanding%20and%20more%20natural%20outputs%20for%20planning.%20Existing%0ABEV-based%203D%20object%20detection%20methods%2C%20typically%20adhering%20to%20angle-based%0Arepresentation%2C%20directly%20estimate%20the%20size%20and%20orientation%20of%20rotated%20bounding%0Aboxes.%20We%20observe%20that%20BEV-based%203D%20object%20detection%20is%20analogous%20to%20aerial%0Aoriented%20object%20detection%2C%20where%20angle-based%20methods%20are%20recognized%20for%20being%0Aaffected%20by%20discontinuities%20in%20their%20loss%20functions.%20Drawing%20inspiration%20from%0Athis%20domain%2C%20we%20propose%20Restricted%20Quadrilateral%20Representation%20to%20define%203D%0Aregression%20targets.%20RQR3D%20regresses%20the%20smallest%20horizontal%20bounding%20box%0Aencapsulating%20the%20oriented%20box%2C%20along%20with%20the%20offsets%20between%20the%20corners%20of%0Athese%20two%20boxes%2C%20thereby%20transforming%20the%20oriented%20object%20detection%20problem%0Ainto%20a%20keypoint%20regression%20task.%20RQR3D%20is%20compatible%20with%20any%203D%20object%0Adetection%20approach.%20We%20employ%20RQR3D%20within%20an%20anchor-free%20single-stage%20object%0Adetection%20method%20and%20introduce%20an%20objectness%20head%20to%20address%20class%20imbalance%0Aproblem.%20Furthermore%2C%20we%20introduce%20a%20simplified%20radar%20fusion%20backbone%20that%0Aeliminates%20the%20need%20for%20voxel%20grouping%20and%20processes%20the%20BEV-mapped%20point%20cloud%0Awith%20standard%202D%20convolutions%2C%20rather%20than%20sparse%20convolutions.%20Extensive%0Aevaluations%20on%20the%20nuScenes%20dataset%20demonstrate%20that%20RQR3D%20achieves%0Astate-of-the-art%20performance%20in%20camera-radar%203D%20object%20detection%2C%20outperforming%0Athe%20previous%20best%20method%20by%20%2B4%25%20in%20NDS%20and%20%2B2.4%25%20in%20mAP%2C%20and%20significantly%0Areducing%20the%20translation%20and%20orientation%20errors%2C%20which%20are%20crucial%20for%20safe%0Aautonomous%20driving.%20These%20consistent%20gains%20highlight%20the%20robustness%2C%20precision%2C%0Aand%20real-world%20readiness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRQR3D%253A%2520Reparametrizing%2520the%2520regression%2520targets%2520for%2520BEV-based%25203D%2520object%250A%2520%2520detection%26entry.906535625%3DOzsel%2520Kilinc%2520and%2520Cem%2520Tarhan%26entry.1292438233%3D%2520%2520Accurate%252C%2520fast%252C%2520and%2520reliable%25203D%2520perception%2520is%2520essential%2520for%2520autonomous%250Adriving.%2520Recently%252C%2520bird%2527s-eye%2520view%2520%2528BEV%2529-based%2520perception%2520approaches%2520have%250Aemerged%2520as%2520superior%2520alternatives%2520to%2520perspective-based%2520solutions%252C%2520offering%250Aenhanced%2520spatial%2520understanding%2520and%2520more%2520natural%2520outputs%2520for%2520planning.%2520Existing%250ABEV-based%25203D%2520object%2520detection%2520methods%252C%2520typically%2520adhering%2520to%2520angle-based%250Arepresentation%252C%2520directly%2520estimate%2520the%2520size%2520and%2520orientation%2520of%2520rotated%2520bounding%250Aboxes.%2520We%2520observe%2520that%2520BEV-based%25203D%2520object%2520detection%2520is%2520analogous%2520to%2520aerial%250Aoriented%2520object%2520detection%252C%2520where%2520angle-based%2520methods%2520are%2520recognized%2520for%2520being%250Aaffected%2520by%2520discontinuities%2520in%2520their%2520loss%2520functions.%2520Drawing%2520inspiration%2520from%250Athis%2520domain%252C%2520we%2520propose%2520Restricted%2520Quadrilateral%2520Representation%2520to%2520define%25203D%250Aregression%2520targets.%2520RQR3D%2520regresses%2520the%2520smallest%2520horizontal%2520bounding%2520box%250Aencapsulating%2520the%2520oriented%2520box%252C%2520along%2520with%2520the%2520offsets%2520between%2520the%2520corners%2520of%250Athese%2520two%2520boxes%252C%2520thereby%2520transforming%2520the%2520oriented%2520object%2520detection%2520problem%250Ainto%2520a%2520keypoint%2520regression%2520task.%2520RQR3D%2520is%2520compatible%2520with%2520any%25203D%2520object%250Adetection%2520approach.%2520We%2520employ%2520RQR3D%2520within%2520an%2520anchor-free%2520single-stage%2520object%250Adetection%2520method%2520and%2520introduce%2520an%2520objectness%2520head%2520to%2520address%2520class%2520imbalance%250Aproblem.%2520Furthermore%252C%2520we%2520introduce%2520a%2520simplified%2520radar%2520fusion%2520backbone%2520that%250Aeliminates%2520the%2520need%2520for%2520voxel%2520grouping%2520and%2520processes%2520the%2520BEV-mapped%2520point%2520cloud%250Awith%2520standard%25202D%2520convolutions%252C%2520rather%2520than%2520sparse%2520convolutions.%2520Extensive%250Aevaluations%2520on%2520the%2520nuScenes%2520dataset%2520demonstrate%2520that%2520RQR3D%2520achieves%250Astate-of-the-art%2520performance%2520in%2520camera-radar%25203D%2520object%2520detection%252C%2520outperforming%250Athe%2520previous%2520best%2520method%2520by%2520%252B4%2525%2520in%2520NDS%2520and%2520%252B2.4%2525%2520in%2520mAP%252C%2520and%2520significantly%250Areducing%2520the%2520translation%2520and%2520orientation%2520errors%252C%2520which%2520are%2520crucial%2520for%2520safe%250Aautonomous%2520driving.%2520These%2520consistent%2520gains%2520highlight%2520the%2520robustness%252C%2520precision%252C%250Aand%2520real-world%2520readiness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RQR3D%3A%20Reparametrizing%20the%20regression%20targets%20for%20BEV-based%203D%20object%0A%20%20detection&entry.906535625=Ozsel%20Kilinc%20and%20Cem%20Tarhan&entry.1292438233=%20%20Accurate%2C%20fast%2C%20and%20reliable%203D%20perception%20is%20essential%20for%20autonomous%0Adriving.%20Recently%2C%20bird%27s-eye%20view%20%28BEV%29-based%20perception%20approaches%20have%0Aemerged%20as%20superior%20alternatives%20to%20perspective-based%20solutions%2C%20offering%0Aenhanced%20spatial%20understanding%20and%20more%20natural%20outputs%20for%20planning.%20Existing%0ABEV-based%203D%20object%20detection%20methods%2C%20typically%20adhering%20to%20angle-based%0Arepresentation%2C%20directly%20estimate%20the%20size%20and%20orientation%20of%20rotated%20bounding%0Aboxes.%20We%20observe%20that%20BEV-based%203D%20object%20detection%20is%20analogous%20to%20aerial%0Aoriented%20object%20detection%2C%20where%20angle-based%20methods%20are%20recognized%20for%20being%0Aaffected%20by%20discontinuities%20in%20their%20loss%20functions.%20Drawing%20inspiration%20from%0Athis%20domain%2C%20we%20propose%20Restricted%20Quadrilateral%20Representation%20to%20define%203D%0Aregression%20targets.%20RQR3D%20regresses%20the%20smallest%20horizontal%20bounding%20box%0Aencapsulating%20the%20oriented%20box%2C%20along%20with%20the%20offsets%20between%20the%20corners%20of%0Athese%20two%20boxes%2C%20thereby%20transforming%20the%20oriented%20object%20detection%20problem%0Ainto%20a%20keypoint%20regression%20task.%20RQR3D%20is%20compatible%20with%20any%203D%20object%0Adetection%20approach.%20We%20employ%20RQR3D%20within%20an%20anchor-free%20single-stage%20object%0Adetection%20method%20and%20introduce%20an%20objectness%20head%20to%20address%20class%20imbalance%0Aproblem.%20Furthermore%2C%20we%20introduce%20a%20simplified%20radar%20fusion%20backbone%20that%0Aeliminates%20the%20need%20for%20voxel%20grouping%20and%20processes%20the%20BEV-mapped%20point%20cloud%0Awith%20standard%202D%20convolutions%2C%20rather%20than%20sparse%20convolutions.%20Extensive%0Aevaluations%20on%20the%20nuScenes%20dataset%20demonstrate%20that%20RQR3D%20achieves%0Astate-of-the-art%20performance%20in%20camera-radar%203D%20object%20detection%2C%20outperforming%0Athe%20previous%20best%20method%20by%20%2B4%25%20in%20NDS%20and%20%2B2.4%25%20in%20mAP%2C%20and%20significantly%0Areducing%20the%20translation%20and%20orientation%20errors%2C%20which%20are%20crucial%20for%20safe%0Aautonomous%20driving.%20These%20consistent%20gains%20highlight%20the%20robustness%2C%20precision%2C%0Aand%20real-world%20readiness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17732v1&entry.124074799=Read"},
{"title": "Active Speech Enhancement: Active Speech Denoising Decliping and\n  Deveraberation", "author": "Ofir Yaish and Yehuda Mishaly and Eliya Nachmani", "abstract": "  We introduce a new paradigm for active sound modification: Active Speech\nEnhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on\nsuppressing external interference, ASE goes further by actively shaping the\nspeech signal -- both attenuating unwanted noise components and amplifying\nspeech-relevant frequencies -- to improve intelligibility and perceptual\nquality. To enable this, we propose a novel Transformer-Mamba-based\narchitecture, along with a task-specific loss function designed to jointly\noptimize interference suppression and signal enrichment. Our method outperforms\nexisting baselines across multiple speech processing tasks -- including\ndenoising, dereverberation, and declipping -- demonstrating the effectiveness\nof active, targeted modulation in challenging acoustic environments.\n", "link": "http://arxiv.org/abs/2505.16911v2", "date": "2025-05-23", "relevancy": 2.2592, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4706}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Speech%20Enhancement%3A%20Active%20Speech%20Denoising%20Decliping%20and%0A%20%20Deveraberation&body=Title%3A%20Active%20Speech%20Enhancement%3A%20Active%20Speech%20Denoising%20Decliping%20and%0A%20%20Deveraberation%0AAuthor%3A%20Ofir%20Yaish%20and%20Yehuda%20Mishaly%20and%20Eliya%20Nachmani%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20paradigm%20for%20active%20sound%20modification%3A%20Active%20Speech%0AEnhancement%20%28ASE%29.%20While%20Active%20Noise%20Cancellation%20%28ANC%29%20algorithms%20focus%20on%0Asuppressing%20external%20interference%2C%20ASE%20goes%20further%20by%20actively%20shaping%20the%0Aspeech%20signal%20--%20both%20attenuating%20unwanted%20noise%20components%20and%20amplifying%0Aspeech-relevant%20frequencies%20--%20to%20improve%20intelligibility%20and%20perceptual%0Aquality.%20To%20enable%20this%2C%20we%20propose%20a%20novel%20Transformer-Mamba-based%0Aarchitecture%2C%20along%20with%20a%20task-specific%20loss%20function%20designed%20to%20jointly%0Aoptimize%20interference%20suppression%20and%20signal%20enrichment.%20Our%20method%20outperforms%0Aexisting%20baselines%20across%20multiple%20speech%20processing%20tasks%20--%20including%0Adenoising%2C%20dereverberation%2C%20and%20declipping%20--%20demonstrating%20the%20effectiveness%0Aof%20active%2C%20targeted%20modulation%20in%20challenging%20acoustic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16911v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Speech%2520Enhancement%253A%2520Active%2520Speech%2520Denoising%2520Decliping%2520and%250A%2520%2520Deveraberation%26entry.906535625%3DOfir%2520Yaish%2520and%2520Yehuda%2520Mishaly%2520and%2520Eliya%2520Nachmani%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520paradigm%2520for%2520active%2520sound%2520modification%253A%2520Active%2520Speech%250AEnhancement%2520%2528ASE%2529.%2520While%2520Active%2520Noise%2520Cancellation%2520%2528ANC%2529%2520algorithms%2520focus%2520on%250Asuppressing%2520external%2520interference%252C%2520ASE%2520goes%2520further%2520by%2520actively%2520shaping%2520the%250Aspeech%2520signal%2520--%2520both%2520attenuating%2520unwanted%2520noise%2520components%2520and%2520amplifying%250Aspeech-relevant%2520frequencies%2520--%2520to%2520improve%2520intelligibility%2520and%2520perceptual%250Aquality.%2520To%2520enable%2520this%252C%2520we%2520propose%2520a%2520novel%2520Transformer-Mamba-based%250Aarchitecture%252C%2520along%2520with%2520a%2520task-specific%2520loss%2520function%2520designed%2520to%2520jointly%250Aoptimize%2520interference%2520suppression%2520and%2520signal%2520enrichment.%2520Our%2520method%2520outperforms%250Aexisting%2520baselines%2520across%2520multiple%2520speech%2520processing%2520tasks%2520--%2520including%250Adenoising%252C%2520dereverberation%252C%2520and%2520declipping%2520--%2520demonstrating%2520the%2520effectiveness%250Aof%2520active%252C%2520targeted%2520modulation%2520in%2520challenging%2520acoustic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16911v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Speech%20Enhancement%3A%20Active%20Speech%20Denoising%20Decliping%20and%0A%20%20Deveraberation&entry.906535625=Ofir%20Yaish%20and%20Yehuda%20Mishaly%20and%20Eliya%20Nachmani&entry.1292438233=%20%20We%20introduce%20a%20new%20paradigm%20for%20active%20sound%20modification%3A%20Active%20Speech%0AEnhancement%20%28ASE%29.%20While%20Active%20Noise%20Cancellation%20%28ANC%29%20algorithms%20focus%20on%0Asuppressing%20external%20interference%2C%20ASE%20goes%20further%20by%20actively%20shaping%20the%0Aspeech%20signal%20--%20both%20attenuating%20unwanted%20noise%20components%20and%20amplifying%0Aspeech-relevant%20frequencies%20--%20to%20improve%20intelligibility%20and%20perceptual%0Aquality.%20To%20enable%20this%2C%20we%20propose%20a%20novel%20Transformer-Mamba-based%0Aarchitecture%2C%20along%20with%20a%20task-specific%20loss%20function%20designed%20to%20jointly%0Aoptimize%20interference%20suppression%20and%20signal%20enrichment.%20Our%20method%20outperforms%0Aexisting%20baselines%20across%20multiple%20speech%20processing%20tasks%20--%20including%0Adenoising%2C%20dereverberation%2C%20and%20declipping%20--%20demonstrating%20the%20effectiveness%0Aof%20active%2C%20targeted%20modulation%20in%20challenging%20acoustic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16911v2&entry.124074799=Read"},
{"title": "Semi-Supervised Medical Image Segmentation via Dual Networks", "author": "Yunyao Lu and Yihang Wu and Reem Kateb and Ahmad Chaddad", "abstract": "  Traditional supervised medical image segmentation models require large\namounts of labeled data for training; however, obtaining such large-scale\nlabeled datasets in the real world is extremely challenging. Recent\nsemi-supervised segmentation models also suffer from noisy pseudo-label issue\nand limited supervision in feature space. To solve these challenges, we propose\nan innovative semi-supervised 3D medical image segmentation method to reduce\nthe dependency on large, expert-labeled datasets. Furthermore, we introduce a\ndual-network architecture to address the limitations of existing methods in\nusing contextual information and generating reliable pseudo-labels. In\naddition, a self-supervised contrastive learning strategy is used to enhance\nthe representation of the network and reduce prediction uncertainty by\ndistinguishing between reliable and unreliable predictions. Experiments on\nclinical magnetic resonance imaging demonstrate that our approach outperforms\nstate-of-the-art techniques. Our code is available at\nhttps://github.com/AIPMLab/Semi-supervised-Segmentation.\n", "link": "http://arxiv.org/abs/2505.17690v1", "date": "2025-05-23", "relevancy": 2.2579, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5805}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5685}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Medical%20Image%20Segmentation%20via%20Dual%20Networks&body=Title%3A%20Semi-Supervised%20Medical%20Image%20Segmentation%20via%20Dual%20Networks%0AAuthor%3A%20Yunyao%20Lu%20and%20Yihang%20Wu%20and%20Reem%20Kateb%20and%20Ahmad%20Chaddad%0AAbstract%3A%20%20%20Traditional%20supervised%20medical%20image%20segmentation%20models%20require%20large%0Aamounts%20of%20labeled%20data%20for%20training%3B%20however%2C%20obtaining%20such%20large-scale%0Alabeled%20datasets%20in%20the%20real%20world%20is%20extremely%20challenging.%20Recent%0Asemi-supervised%20segmentation%20models%20also%20suffer%20from%20noisy%20pseudo-label%20issue%0Aand%20limited%20supervision%20in%20feature%20space.%20To%20solve%20these%20challenges%2C%20we%20propose%0Aan%20innovative%20semi-supervised%203D%20medical%20image%20segmentation%20method%20to%20reduce%0Athe%20dependency%20on%20large%2C%20expert-labeled%20datasets.%20Furthermore%2C%20we%20introduce%20a%0Adual-network%20architecture%20to%20address%20the%20limitations%20of%20existing%20methods%20in%0Ausing%20contextual%20information%20and%20generating%20reliable%20pseudo-labels.%20In%0Aaddition%2C%20a%20self-supervised%20contrastive%20learning%20strategy%20is%20used%20to%20enhance%0Athe%20representation%20of%20the%20network%20and%20reduce%20prediction%20uncertainty%20by%0Adistinguishing%20between%20reliable%20and%20unreliable%20predictions.%20Experiments%20on%0Aclinical%20magnetic%20resonance%20imaging%20demonstrate%20that%20our%20approach%20outperforms%0Astate-of-the-art%20techniques.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/AIPMLab/Semi-supervised-Segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%2520Medical%2520Image%2520Segmentation%2520via%2520Dual%2520Networks%26entry.906535625%3DYunyao%2520Lu%2520and%2520Yihang%2520Wu%2520and%2520Reem%2520Kateb%2520and%2520Ahmad%2520Chaddad%26entry.1292438233%3D%2520%2520Traditional%2520supervised%2520medical%2520image%2520segmentation%2520models%2520require%2520large%250Aamounts%2520of%2520labeled%2520data%2520for%2520training%253B%2520however%252C%2520obtaining%2520such%2520large-scale%250Alabeled%2520datasets%2520in%2520the%2520real%2520world%2520is%2520extremely%2520challenging.%2520Recent%250Asemi-supervised%2520segmentation%2520models%2520also%2520suffer%2520from%2520noisy%2520pseudo-label%2520issue%250Aand%2520limited%2520supervision%2520in%2520feature%2520space.%2520To%2520solve%2520these%2520challenges%252C%2520we%2520propose%250Aan%2520innovative%2520semi-supervised%25203D%2520medical%2520image%2520segmentation%2520method%2520to%2520reduce%250Athe%2520dependency%2520on%2520large%252C%2520expert-labeled%2520datasets.%2520Furthermore%252C%2520we%2520introduce%2520a%250Adual-network%2520architecture%2520to%2520address%2520the%2520limitations%2520of%2520existing%2520methods%2520in%250Ausing%2520contextual%2520information%2520and%2520generating%2520reliable%2520pseudo-labels.%2520In%250Aaddition%252C%2520a%2520self-supervised%2520contrastive%2520learning%2520strategy%2520is%2520used%2520to%2520enhance%250Athe%2520representation%2520of%2520the%2520network%2520and%2520reduce%2520prediction%2520uncertainty%2520by%250Adistinguishing%2520between%2520reliable%2520and%2520unreliable%2520predictions.%2520Experiments%2520on%250Aclinical%2520magnetic%2520resonance%2520imaging%2520demonstrate%2520that%2520our%2520approach%2520outperforms%250Astate-of-the-art%2520techniques.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/AIPMLab/Semi-supervised-Segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Medical%20Image%20Segmentation%20via%20Dual%20Networks&entry.906535625=Yunyao%20Lu%20and%20Yihang%20Wu%20and%20Reem%20Kateb%20and%20Ahmad%20Chaddad&entry.1292438233=%20%20Traditional%20supervised%20medical%20image%20segmentation%20models%20require%20large%0Aamounts%20of%20labeled%20data%20for%20training%3B%20however%2C%20obtaining%20such%20large-scale%0Alabeled%20datasets%20in%20the%20real%20world%20is%20extremely%20challenging.%20Recent%0Asemi-supervised%20segmentation%20models%20also%20suffer%20from%20noisy%20pseudo-label%20issue%0Aand%20limited%20supervision%20in%20feature%20space.%20To%20solve%20these%20challenges%2C%20we%20propose%0Aan%20innovative%20semi-supervised%203D%20medical%20image%20segmentation%20method%20to%20reduce%0Athe%20dependency%20on%20large%2C%20expert-labeled%20datasets.%20Furthermore%2C%20we%20introduce%20a%0Adual-network%20architecture%20to%20address%20the%20limitations%20of%20existing%20methods%20in%0Ausing%20contextual%20information%20and%20generating%20reliable%20pseudo-labels.%20In%0Aaddition%2C%20a%20self-supervised%20contrastive%20learning%20strategy%20is%20used%20to%20enhance%0Athe%20representation%20of%20the%20network%20and%20reduce%20prediction%20uncertainty%20by%0Adistinguishing%20between%20reliable%20and%20unreliable%20predictions.%20Experiments%20on%0Aclinical%20magnetic%20resonance%20imaging%20demonstrate%20that%20our%20approach%20outperforms%0Astate-of-the-art%20techniques.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/AIPMLab/Semi-supervised-Segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17690v1&entry.124074799=Read"},
{"title": "Generative Distribution Embeddings", "author": "Nic Fishman and Gokul Gowri and Peng Yin and Jonathan Gootenberg and Omar Abudayyeh", "abstract": "  Many real-world problems require reasoning across multiple scales, demanding\nmodels which operate not on single data points, but on entire distributions. We\nintroduce generative distribution embeddings (GDE), a framework that lifts\nautoencoders to the space of distributions. In GDEs, an encoder acts on sets of\nsamples, and the decoder is replaced by a generator which aims to match the\ninput distribution. This framework enables learning representations of\ndistributions by coupling conditional generative models with encoder networks\nwhich satisfy a criterion we call distributional invariance. We show that GDEs\nlearn predictive sufficient statistics embedded in the Wasserstein space, such\nthat latent GDE distances approximately recover the $W_2$ distance, and latent\ninterpolation approximately recovers optimal transport trajectories for\nGaussian and Gaussian mixture distributions. We systematically benchmark GDEs\nagainst existing approaches on synthetic datasets, demonstrating consistently\nstronger performance. We then apply GDEs to six key problems in computational\nbiology: learning representations of cell populations from lineage-tracing data\n(150K cells), predicting perturbation effects on single-cell transcriptomes (1M\ncells), predicting perturbation effects on cellular phenotypes (20M single-cell\nimages), modeling tissue-specific DNA methylation patterns (253M sequences),\ndesigning synthetic yeast promoters (34M sequences), and spatiotemporal\nmodeling of viral protein sequences (1M sequences).\n", "link": "http://arxiv.org/abs/2505.18150v1", "date": "2025-05-23", "relevancy": 2.2558, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5953}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5417}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Distribution%20Embeddings&body=Title%3A%20Generative%20Distribution%20Embeddings%0AAuthor%3A%20Nic%20Fishman%20and%20Gokul%20Gowri%20and%20Peng%20Yin%20and%20Jonathan%20Gootenberg%20and%20Omar%20Abudayyeh%0AAbstract%3A%20%20%20Many%20real-world%20problems%20require%20reasoning%20across%20multiple%20scales%2C%20demanding%0Amodels%20which%20operate%20not%20on%20single%20data%20points%2C%20but%20on%20entire%20distributions.%20We%0Aintroduce%20generative%20distribution%20embeddings%20%28GDE%29%2C%20a%20framework%20that%20lifts%0Aautoencoders%20to%20the%20space%20of%20distributions.%20In%20GDEs%2C%20an%20encoder%20acts%20on%20sets%20of%0Asamples%2C%20and%20the%20decoder%20is%20replaced%20by%20a%20generator%20which%20aims%20to%20match%20the%0Ainput%20distribution.%20This%20framework%20enables%20learning%20representations%20of%0Adistributions%20by%20coupling%20conditional%20generative%20models%20with%20encoder%20networks%0Awhich%20satisfy%20a%20criterion%20we%20call%20distributional%20invariance.%20We%20show%20that%20GDEs%0Alearn%20predictive%20sufficient%20statistics%20embedded%20in%20the%20Wasserstein%20space%2C%20such%0Athat%20latent%20GDE%20distances%20approximately%20recover%20the%20%24W_2%24%20distance%2C%20and%20latent%0Ainterpolation%20approximately%20recovers%20optimal%20transport%20trajectories%20for%0AGaussian%20and%20Gaussian%20mixture%20distributions.%20We%20systematically%20benchmark%20GDEs%0Aagainst%20existing%20approaches%20on%20synthetic%20datasets%2C%20demonstrating%20consistently%0Astronger%20performance.%20We%20then%20apply%20GDEs%20to%20six%20key%20problems%20in%20computational%0Abiology%3A%20learning%20representations%20of%20cell%20populations%20from%20lineage-tracing%20data%0A%28150K%20cells%29%2C%20predicting%20perturbation%20effects%20on%20single-cell%20transcriptomes%20%281M%0Acells%29%2C%20predicting%20perturbation%20effects%20on%20cellular%20phenotypes%20%2820M%20single-cell%0Aimages%29%2C%20modeling%20tissue-specific%20DNA%20methylation%20patterns%20%28253M%20sequences%29%2C%0Adesigning%20synthetic%20yeast%20promoters%20%2834M%20sequences%29%2C%20and%20spatiotemporal%0Amodeling%20of%20viral%20protein%20sequences%20%281M%20sequences%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Distribution%2520Embeddings%26entry.906535625%3DNic%2520Fishman%2520and%2520Gokul%2520Gowri%2520and%2520Peng%2520Yin%2520and%2520Jonathan%2520Gootenberg%2520and%2520Omar%2520Abudayyeh%26entry.1292438233%3D%2520%2520Many%2520real-world%2520problems%2520require%2520reasoning%2520across%2520multiple%2520scales%252C%2520demanding%250Amodels%2520which%2520operate%2520not%2520on%2520single%2520data%2520points%252C%2520but%2520on%2520entire%2520distributions.%2520We%250Aintroduce%2520generative%2520distribution%2520embeddings%2520%2528GDE%2529%252C%2520a%2520framework%2520that%2520lifts%250Aautoencoders%2520to%2520the%2520space%2520of%2520distributions.%2520In%2520GDEs%252C%2520an%2520encoder%2520acts%2520on%2520sets%2520of%250Asamples%252C%2520and%2520the%2520decoder%2520is%2520replaced%2520by%2520a%2520generator%2520which%2520aims%2520to%2520match%2520the%250Ainput%2520distribution.%2520This%2520framework%2520enables%2520learning%2520representations%2520of%250Adistributions%2520by%2520coupling%2520conditional%2520generative%2520models%2520with%2520encoder%2520networks%250Awhich%2520satisfy%2520a%2520criterion%2520we%2520call%2520distributional%2520invariance.%2520We%2520show%2520that%2520GDEs%250Alearn%2520predictive%2520sufficient%2520statistics%2520embedded%2520in%2520the%2520Wasserstein%2520space%252C%2520such%250Athat%2520latent%2520GDE%2520distances%2520approximately%2520recover%2520the%2520%2524W_2%2524%2520distance%252C%2520and%2520latent%250Ainterpolation%2520approximately%2520recovers%2520optimal%2520transport%2520trajectories%2520for%250AGaussian%2520and%2520Gaussian%2520mixture%2520distributions.%2520We%2520systematically%2520benchmark%2520GDEs%250Aagainst%2520existing%2520approaches%2520on%2520synthetic%2520datasets%252C%2520demonstrating%2520consistently%250Astronger%2520performance.%2520We%2520then%2520apply%2520GDEs%2520to%2520six%2520key%2520problems%2520in%2520computational%250Abiology%253A%2520learning%2520representations%2520of%2520cell%2520populations%2520from%2520lineage-tracing%2520data%250A%2528150K%2520cells%2529%252C%2520predicting%2520perturbation%2520effects%2520on%2520single-cell%2520transcriptomes%2520%25281M%250Acells%2529%252C%2520predicting%2520perturbation%2520effects%2520on%2520cellular%2520phenotypes%2520%252820M%2520single-cell%250Aimages%2529%252C%2520modeling%2520tissue-specific%2520DNA%2520methylation%2520patterns%2520%2528253M%2520sequences%2529%252C%250Adesigning%2520synthetic%2520yeast%2520promoters%2520%252834M%2520sequences%2529%252C%2520and%2520spatiotemporal%250Amodeling%2520of%2520viral%2520protein%2520sequences%2520%25281M%2520sequences%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Distribution%20Embeddings&entry.906535625=Nic%20Fishman%20and%20Gokul%20Gowri%20and%20Peng%20Yin%20and%20Jonathan%20Gootenberg%20and%20Omar%20Abudayyeh&entry.1292438233=%20%20Many%20real-world%20problems%20require%20reasoning%20across%20multiple%20scales%2C%20demanding%0Amodels%20which%20operate%20not%20on%20single%20data%20points%2C%20but%20on%20entire%20distributions.%20We%0Aintroduce%20generative%20distribution%20embeddings%20%28GDE%29%2C%20a%20framework%20that%20lifts%0Aautoencoders%20to%20the%20space%20of%20distributions.%20In%20GDEs%2C%20an%20encoder%20acts%20on%20sets%20of%0Asamples%2C%20and%20the%20decoder%20is%20replaced%20by%20a%20generator%20which%20aims%20to%20match%20the%0Ainput%20distribution.%20This%20framework%20enables%20learning%20representations%20of%0Adistributions%20by%20coupling%20conditional%20generative%20models%20with%20encoder%20networks%0Awhich%20satisfy%20a%20criterion%20we%20call%20distributional%20invariance.%20We%20show%20that%20GDEs%0Alearn%20predictive%20sufficient%20statistics%20embedded%20in%20the%20Wasserstein%20space%2C%20such%0Athat%20latent%20GDE%20distances%20approximately%20recover%20the%20%24W_2%24%20distance%2C%20and%20latent%0Ainterpolation%20approximately%20recovers%20optimal%20transport%20trajectories%20for%0AGaussian%20and%20Gaussian%20mixture%20distributions.%20We%20systematically%20benchmark%20GDEs%0Aagainst%20existing%20approaches%20on%20synthetic%20datasets%2C%20demonstrating%20consistently%0Astronger%20performance.%20We%20then%20apply%20GDEs%20to%20six%20key%20problems%20in%20computational%0Abiology%3A%20learning%20representations%20of%20cell%20populations%20from%20lineage-tracing%20data%0A%28150K%20cells%29%2C%20predicting%20perturbation%20effects%20on%20single-cell%20transcriptomes%20%281M%0Acells%29%2C%20predicting%20perturbation%20effects%20on%20cellular%20phenotypes%20%2820M%20single-cell%0Aimages%29%2C%20modeling%20tissue-specific%20DNA%20methylation%20patterns%20%28253M%20sequences%29%2C%0Adesigning%20synthetic%20yeast%20promoters%20%2834M%20sequences%29%2C%20and%20spatiotemporal%0Amodeling%20of%20viral%20protein%20sequences%20%281M%20sequences%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18150v1&entry.124074799=Read"},
{"title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and\n  Reactive Feedback", "author": "Litao Guo and Xinli Xu and Luozhou Wang and Jiantao Lin and Jinsong Zhou and Zixin Zhang and Bolan Su and Ying-Cong Chen", "abstract": "  With the rapid advancement of generative models, general-purpose generation\nhas gained increasing attention as a promising approach to unify diverse tasks\nacross modalities within a single system. Despite this progress, existing\nopen-source frameworks often remain fragile and struggle to support complex\nreal-world applications due to the lack of structured workflow planning and\nexecution-level feedback. To address these limitations, we present ComfyMind, a\ncollaborative AI system designed to enable robust and scalable general-purpose\ngeneration, built on the ComfyUI platform. ComfyMind introduces two core\ninnovations: Semantic Workflow Interface (SWI) that abstracts low-level node\ngraphs into callable functional modules described in natural language, enabling\nhigh-level composition and reducing structural errors; Search Tree Planning\nmechanism with localized feedback execution, which models generation as a\nhierarchical decision process and allows adaptive correction at each stage.\nTogether, these components improve the stability and flexibility of complex\ngenerative workflows. We evaluate ComfyMind on three public benchmarks:\nComfyBench, GenEval, and Reason-Edit, which span generation, editing, and\nreasoning tasks. Results show that ComfyMind consistently outperforms existing\nopen-source baselines and achieves performance comparable to GPT-Image-1.\nComfyMind paves a promising path for the development of open-source\ngeneral-purpose generative AI systems. Project page:\nhttps://github.com/LitaoGuo/ComfyMind\n", "link": "http://arxiv.org/abs/2505.17908v1", "date": "2025-05-23", "relevancy": 2.2544, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5839}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.58}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ComfyMind%3A%20Toward%20General-Purpose%20Generation%20via%20Tree-Based%20Planning%20and%0A%20%20Reactive%20Feedback&body=Title%3A%20ComfyMind%3A%20Toward%20General-Purpose%20Generation%20via%20Tree-Based%20Planning%20and%0A%20%20Reactive%20Feedback%0AAuthor%3A%20Litao%20Guo%20and%20Xinli%20Xu%20and%20Luozhou%20Wang%20and%20Jiantao%20Lin%20and%20Jinsong%20Zhou%20and%20Zixin%20Zhang%20and%20Bolan%20Su%20and%20Ying-Cong%20Chen%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20generative%20models%2C%20general-purpose%20generation%0Ahas%20gained%20increasing%20attention%20as%20a%20promising%20approach%20to%20unify%20diverse%20tasks%0Aacross%20modalities%20within%20a%20single%20system.%20Despite%20this%20progress%2C%20existing%0Aopen-source%20frameworks%20often%20remain%20fragile%20and%20struggle%20to%20support%20complex%0Areal-world%20applications%20due%20to%20the%20lack%20of%20structured%20workflow%20planning%20and%0Aexecution-level%20feedback.%20To%20address%20these%20limitations%2C%20we%20present%20ComfyMind%2C%20a%0Acollaborative%20AI%20system%20designed%20to%20enable%20robust%20and%20scalable%20general-purpose%0Ageneration%2C%20built%20on%20the%20ComfyUI%20platform.%20ComfyMind%20introduces%20two%20core%0Ainnovations%3A%20Semantic%20Workflow%20Interface%20%28SWI%29%20that%20abstracts%20low-level%20node%0Agraphs%20into%20callable%20functional%20modules%20described%20in%20natural%20language%2C%20enabling%0Ahigh-level%20composition%20and%20reducing%20structural%20errors%3B%20Search%20Tree%20Planning%0Amechanism%20with%20localized%20feedback%20execution%2C%20which%20models%20generation%20as%20a%0Ahierarchical%20decision%20process%20and%20allows%20adaptive%20correction%20at%20each%20stage.%0ATogether%2C%20these%20components%20improve%20the%20stability%20and%20flexibility%20of%20complex%0Agenerative%20workflows.%20We%20evaluate%20ComfyMind%20on%20three%20public%20benchmarks%3A%0AComfyBench%2C%20GenEval%2C%20and%20Reason-Edit%2C%20which%20span%20generation%2C%20editing%2C%20and%0Areasoning%20tasks.%20Results%20show%20that%20ComfyMind%20consistently%20outperforms%20existing%0Aopen-source%20baselines%20and%20achieves%20performance%20comparable%20to%20GPT-Image-1.%0AComfyMind%20paves%20a%20promising%20path%20for%20the%20development%20of%20open-source%0Ageneral-purpose%20generative%20AI%20systems.%20Project%20page%3A%0Ahttps%3A//github.com/LitaoGuo/ComfyMind%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComfyMind%253A%2520Toward%2520General-Purpose%2520Generation%2520via%2520Tree-Based%2520Planning%2520and%250A%2520%2520Reactive%2520Feedback%26entry.906535625%3DLitao%2520Guo%2520and%2520Xinli%2520Xu%2520and%2520Luozhou%2520Wang%2520and%2520Jiantao%2520Lin%2520and%2520Jinsong%2520Zhou%2520and%2520Zixin%2520Zhang%2520and%2520Bolan%2520Su%2520and%2520Ying-Cong%2520Chen%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520generative%2520models%252C%2520general-purpose%2520generation%250Ahas%2520gained%2520increasing%2520attention%2520as%2520a%2520promising%2520approach%2520to%2520unify%2520diverse%2520tasks%250Aacross%2520modalities%2520within%2520a%2520single%2520system.%2520Despite%2520this%2520progress%252C%2520existing%250Aopen-source%2520frameworks%2520often%2520remain%2520fragile%2520and%2520struggle%2520to%2520support%2520complex%250Areal-world%2520applications%2520due%2520to%2520the%2520lack%2520of%2520structured%2520workflow%2520planning%2520and%250Aexecution-level%2520feedback.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520ComfyMind%252C%2520a%250Acollaborative%2520AI%2520system%2520designed%2520to%2520enable%2520robust%2520and%2520scalable%2520general-purpose%250Ageneration%252C%2520built%2520on%2520the%2520ComfyUI%2520platform.%2520ComfyMind%2520introduces%2520two%2520core%250Ainnovations%253A%2520Semantic%2520Workflow%2520Interface%2520%2528SWI%2529%2520that%2520abstracts%2520low-level%2520node%250Agraphs%2520into%2520callable%2520functional%2520modules%2520described%2520in%2520natural%2520language%252C%2520enabling%250Ahigh-level%2520composition%2520and%2520reducing%2520structural%2520errors%253B%2520Search%2520Tree%2520Planning%250Amechanism%2520with%2520localized%2520feedback%2520execution%252C%2520which%2520models%2520generation%2520as%2520a%250Ahierarchical%2520decision%2520process%2520and%2520allows%2520adaptive%2520correction%2520at%2520each%2520stage.%250ATogether%252C%2520these%2520components%2520improve%2520the%2520stability%2520and%2520flexibility%2520of%2520complex%250Agenerative%2520workflows.%2520We%2520evaluate%2520ComfyMind%2520on%2520three%2520public%2520benchmarks%253A%250AComfyBench%252C%2520GenEval%252C%2520and%2520Reason-Edit%252C%2520which%2520span%2520generation%252C%2520editing%252C%2520and%250Areasoning%2520tasks.%2520Results%2520show%2520that%2520ComfyMind%2520consistently%2520outperforms%2520existing%250Aopen-source%2520baselines%2520and%2520achieves%2520performance%2520comparable%2520to%2520GPT-Image-1.%250AComfyMind%2520paves%2520a%2520promising%2520path%2520for%2520the%2520development%2520of%2520open-source%250Ageneral-purpose%2520generative%2520AI%2520systems.%2520Project%2520page%253A%250Ahttps%253A//github.com/LitaoGuo/ComfyMind%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ComfyMind%3A%20Toward%20General-Purpose%20Generation%20via%20Tree-Based%20Planning%20and%0A%20%20Reactive%20Feedback&entry.906535625=Litao%20Guo%20and%20Xinli%20Xu%20and%20Luozhou%20Wang%20and%20Jiantao%20Lin%20and%20Jinsong%20Zhou%20and%20Zixin%20Zhang%20and%20Bolan%20Su%20and%20Ying-Cong%20Chen&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20generative%20models%2C%20general-purpose%20generation%0Ahas%20gained%20increasing%20attention%20as%20a%20promising%20approach%20to%20unify%20diverse%20tasks%0Aacross%20modalities%20within%20a%20single%20system.%20Despite%20this%20progress%2C%20existing%0Aopen-source%20frameworks%20often%20remain%20fragile%20and%20struggle%20to%20support%20complex%0Areal-world%20applications%20due%20to%20the%20lack%20of%20structured%20workflow%20planning%20and%0Aexecution-level%20feedback.%20To%20address%20these%20limitations%2C%20we%20present%20ComfyMind%2C%20a%0Acollaborative%20AI%20system%20designed%20to%20enable%20robust%20and%20scalable%20general-purpose%0Ageneration%2C%20built%20on%20the%20ComfyUI%20platform.%20ComfyMind%20introduces%20two%20core%0Ainnovations%3A%20Semantic%20Workflow%20Interface%20%28SWI%29%20that%20abstracts%20low-level%20node%0Agraphs%20into%20callable%20functional%20modules%20described%20in%20natural%20language%2C%20enabling%0Ahigh-level%20composition%20and%20reducing%20structural%20errors%3B%20Search%20Tree%20Planning%0Amechanism%20with%20localized%20feedback%20execution%2C%20which%20models%20generation%20as%20a%0Ahierarchical%20decision%20process%20and%20allows%20adaptive%20correction%20at%20each%20stage.%0ATogether%2C%20these%20components%20improve%20the%20stability%20and%20flexibility%20of%20complex%0Agenerative%20workflows.%20We%20evaluate%20ComfyMind%20on%20three%20public%20benchmarks%3A%0AComfyBench%2C%20GenEval%2C%20and%20Reason-Edit%2C%20which%20span%20generation%2C%20editing%2C%20and%0Areasoning%20tasks.%20Results%20show%20that%20ComfyMind%20consistently%20outperforms%20existing%0Aopen-source%20baselines%20and%20achieves%20performance%20comparable%20to%20GPT-Image-1.%0AComfyMind%20paves%20a%20promising%20path%20for%20the%20development%20of%20open-source%0Ageneral-purpose%20generative%20AI%20systems.%20Project%20page%3A%0Ahttps%3A//github.com/LitaoGuo/ComfyMind%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17908v1&entry.124074799=Read"},
{"title": "Semantic Correspondence: Unified Benchmarking and a Strong Baseline", "author": "Kaiyan Zhang and Xinghui Li and Jingyi Lu and Kai Han", "abstract": "  Establishing semantic correspondence is a challenging task in computer\nvision, aiming to match keypoints with the same semantic information across\ndifferent images. Benefiting from the rapid development of deep learning,\nremarkable progress has been made over the past decade. However, a\ncomprehensive review and analysis of this task remains absent. In this paper,\nwe present the first extensive survey of semantic correspondence methods. We\nfirst propose a taxonomy to classify existing methods based on the type of\ntheir method designs. These methods are then categorized accordingly, and we\nprovide a detailed analysis of each approach. Furthermore, we aggregate and\nsummarize the results of methods in literature across various benchmarks into a\nunified comparative table, with detailed configurations to highlight\nperformance variations. Additionally, to provide a detailed understanding on\nexisting methods for semantic matching, we thoroughly conduct controlled\nexperiments to analyse the effectiveness of the components of different\nmethods. Finally, we propose a simple yet effective baseline that achieves\nstate-of-the-art performance on multiple benchmarks, providing a solid\nfoundation for future research in this field. We hope this survey serves as a\ncomprehensive reference and consolidated baseline for future development. Code\nis publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.\n", "link": "http://arxiv.org/abs/2505.18060v1", "date": "2025-05-23", "relevancy": 2.2488, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Correspondence%3A%20Unified%20Benchmarking%20and%20a%20Strong%20Baseline&body=Title%3A%20Semantic%20Correspondence%3A%20Unified%20Benchmarking%20and%20a%20Strong%20Baseline%0AAuthor%3A%20Kaiyan%20Zhang%20and%20Xinghui%20Li%20and%20Jingyi%20Lu%20and%20Kai%20Han%0AAbstract%3A%20%20%20Establishing%20semantic%20correspondence%20is%20a%20challenging%20task%20in%20computer%0Avision%2C%20aiming%20to%20match%20keypoints%20with%20the%20same%20semantic%20information%20across%0Adifferent%20images.%20Benefiting%20from%20the%20rapid%20development%20of%20deep%20learning%2C%0Aremarkable%20progress%20has%20been%20made%20over%20the%20past%20decade.%20However%2C%20a%0Acomprehensive%20review%20and%20analysis%20of%20this%20task%20remains%20absent.%20In%20this%20paper%2C%0Awe%20present%20the%20first%20extensive%20survey%20of%20semantic%20correspondence%20methods.%20We%0Afirst%20propose%20a%20taxonomy%20to%20classify%20existing%20methods%20based%20on%20the%20type%20of%0Atheir%20method%20designs.%20These%20methods%20are%20then%20categorized%20accordingly%2C%20and%20we%0Aprovide%20a%20detailed%20analysis%20of%20each%20approach.%20Furthermore%2C%20we%20aggregate%20and%0Asummarize%20the%20results%20of%20methods%20in%20literature%20across%20various%20benchmarks%20into%20a%0Aunified%20comparative%20table%2C%20with%20detailed%20configurations%20to%20highlight%0Aperformance%20variations.%20Additionally%2C%20to%20provide%20a%20detailed%20understanding%20on%0Aexisting%20methods%20for%20semantic%20matching%2C%20we%20thoroughly%20conduct%20controlled%0Aexperiments%20to%20analyse%20the%20effectiveness%20of%20the%20components%20of%20different%0Amethods.%20Finally%2C%20we%20propose%20a%20simple%20yet%20effective%20baseline%20that%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%2C%20providing%20a%20solid%0Afoundation%20for%20future%20research%20in%20this%20field.%20We%20hope%20this%20survey%20serves%20as%20a%0Acomprehensive%20reference%20and%20consolidated%20baseline%20for%20future%20development.%20Code%0Ais%20publicly%20available%20at%3A%20https%3A//github.com/Visual-AI/Semantic-Correspondence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Correspondence%253A%2520Unified%2520Benchmarking%2520and%2520a%2520Strong%2520Baseline%26entry.906535625%3DKaiyan%2520Zhang%2520and%2520Xinghui%2520Li%2520and%2520Jingyi%2520Lu%2520and%2520Kai%2520Han%26entry.1292438233%3D%2520%2520Establishing%2520semantic%2520correspondence%2520is%2520a%2520challenging%2520task%2520in%2520computer%250Avision%252C%2520aiming%2520to%2520match%2520keypoints%2520with%2520the%2520same%2520semantic%2520information%2520across%250Adifferent%2520images.%2520Benefiting%2520from%2520the%2520rapid%2520development%2520of%2520deep%2520learning%252C%250Aremarkable%2520progress%2520has%2520been%2520made%2520over%2520the%2520past%2520decade.%2520However%252C%2520a%250Acomprehensive%2520review%2520and%2520analysis%2520of%2520this%2520task%2520remains%2520absent.%2520In%2520this%2520paper%252C%250Awe%2520present%2520the%2520first%2520extensive%2520survey%2520of%2520semantic%2520correspondence%2520methods.%2520We%250Afirst%2520propose%2520a%2520taxonomy%2520to%2520classify%2520existing%2520methods%2520based%2520on%2520the%2520type%2520of%250Atheir%2520method%2520designs.%2520These%2520methods%2520are%2520then%2520categorized%2520accordingly%252C%2520and%2520we%250Aprovide%2520a%2520detailed%2520analysis%2520of%2520each%2520approach.%2520Furthermore%252C%2520we%2520aggregate%2520and%250Asummarize%2520the%2520results%2520of%2520methods%2520in%2520literature%2520across%2520various%2520benchmarks%2520into%2520a%250Aunified%2520comparative%2520table%252C%2520with%2520detailed%2520configurations%2520to%2520highlight%250Aperformance%2520variations.%2520Additionally%252C%2520to%2520provide%2520a%2520detailed%2520understanding%2520on%250Aexisting%2520methods%2520for%2520semantic%2520matching%252C%2520we%2520thoroughly%2520conduct%2520controlled%250Aexperiments%2520to%2520analyse%2520the%2520effectiveness%2520of%2520the%2520components%2520of%2520different%250Amethods.%2520Finally%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520baseline%2520that%2520achieves%250Astate-of-the-art%2520performance%2520on%2520multiple%2520benchmarks%252C%2520providing%2520a%2520solid%250Afoundation%2520for%2520future%2520research%2520in%2520this%2520field.%2520We%2520hope%2520this%2520survey%2520serves%2520as%2520a%250Acomprehensive%2520reference%2520and%2520consolidated%2520baseline%2520for%2520future%2520development.%2520Code%250Ais%2520publicly%2520available%2520at%253A%2520https%253A//github.com/Visual-AI/Semantic-Correspondence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Correspondence%3A%20Unified%20Benchmarking%20and%20a%20Strong%20Baseline&entry.906535625=Kaiyan%20Zhang%20and%20Xinghui%20Li%20and%20Jingyi%20Lu%20and%20Kai%20Han&entry.1292438233=%20%20Establishing%20semantic%20correspondence%20is%20a%20challenging%20task%20in%20computer%0Avision%2C%20aiming%20to%20match%20keypoints%20with%20the%20same%20semantic%20information%20across%0Adifferent%20images.%20Benefiting%20from%20the%20rapid%20development%20of%20deep%20learning%2C%0Aremarkable%20progress%20has%20been%20made%20over%20the%20past%20decade.%20However%2C%20a%0Acomprehensive%20review%20and%20analysis%20of%20this%20task%20remains%20absent.%20In%20this%20paper%2C%0Awe%20present%20the%20first%20extensive%20survey%20of%20semantic%20correspondence%20methods.%20We%0Afirst%20propose%20a%20taxonomy%20to%20classify%20existing%20methods%20based%20on%20the%20type%20of%0Atheir%20method%20designs.%20These%20methods%20are%20then%20categorized%20accordingly%2C%20and%20we%0Aprovide%20a%20detailed%20analysis%20of%20each%20approach.%20Furthermore%2C%20we%20aggregate%20and%0Asummarize%20the%20results%20of%20methods%20in%20literature%20across%20various%20benchmarks%20into%20a%0Aunified%20comparative%20table%2C%20with%20detailed%20configurations%20to%20highlight%0Aperformance%20variations.%20Additionally%2C%20to%20provide%20a%20detailed%20understanding%20on%0Aexisting%20methods%20for%20semantic%20matching%2C%20we%20thoroughly%20conduct%20controlled%0Aexperiments%20to%20analyse%20the%20effectiveness%20of%20the%20components%20of%20different%0Amethods.%20Finally%2C%20we%20propose%20a%20simple%20yet%20effective%20baseline%20that%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%2C%20providing%20a%20solid%0Afoundation%20for%20future%20research%20in%20this%20field.%20We%20hope%20this%20survey%20serves%20as%20a%0Acomprehensive%20reference%20and%20consolidated%20baseline%20for%20future%20development.%20Code%0Ais%20publicly%20available%20at%3A%20https%3A//github.com/Visual-AI/Semantic-Correspondence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18060v1&entry.124074799=Read"},
{"title": "On the Robustness of Medical Vision-Language Models: Are they Truly\n  Generalizable?", "author": "Raza Imam and Rufael Marew and Mohammad Yaqub", "abstract": "  Medical Vision-Language Models (MVLMs) have achieved par excellence\ngeneralization in medical image analysis, yet their performance under noisy,\ncorrupted conditions remains largely untested. Clinical imaging is inherently\nsusceptible to acquisition artifacts and noise; however, existing evaluations\npredominantly assess generally clean datasets, overlooking robustness -- i.e.,\nthe model's ability to perform under real-world distortions. To address this\ngap, we first introduce MediMeta-C, a corruption benchmark that systematically\napplies several perturbations across multiple medical imaging datasets.\nCombined with MedMNIST-C, this establishes a comprehensive robustness\nevaluation framework for MVLMs. We further propose RobustMedCLIP, a visual\nencoder adaptation of a pretrained MVLM that incorporates few-shot tuning to\nenhance resilience against corruptions. Through extensive experiments, we\nbenchmark 5 major MVLMs across 5 medical imaging modalities, revealing that\nexisting models exhibit severe degradation under corruption and struggle with\ndomain-modality tradeoffs. Our findings highlight the necessity of diverse\ntraining and robust adaptation strategies, demonstrating that efficient\nlow-rank adaptation when paired with few-shot tuning, improves robustness while\npreserving generalization across modalities.\n", "link": "http://arxiv.org/abs/2505.15425v2", "date": "2025-05-23", "relevancy": 2.2484, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.564}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Robustness%20of%20Medical%20Vision-Language%20Models%3A%20Are%20they%20Truly%0A%20%20Generalizable%3F&body=Title%3A%20On%20the%20Robustness%20of%20Medical%20Vision-Language%20Models%3A%20Are%20they%20Truly%0A%20%20Generalizable%3F%0AAuthor%3A%20Raza%20Imam%20and%20Rufael%20Marew%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Medical%20Vision-Language%20Models%20%28MVLMs%29%20have%20achieved%20par%20excellence%0Ageneralization%20in%20medical%20image%20analysis%2C%20yet%20their%20performance%20under%20noisy%2C%0Acorrupted%20conditions%20remains%20largely%20untested.%20Clinical%20imaging%20is%20inherently%0Asusceptible%20to%20acquisition%20artifacts%20and%20noise%3B%20however%2C%20existing%20evaluations%0Apredominantly%20assess%20generally%20clean%20datasets%2C%20overlooking%20robustness%20--%20i.e.%2C%0Athe%20model%27s%20ability%20to%20perform%20under%20real-world%20distortions.%20To%20address%20this%0Agap%2C%20we%20first%20introduce%20MediMeta-C%2C%20a%20corruption%20benchmark%20that%20systematically%0Aapplies%20several%20perturbations%20across%20multiple%20medical%20imaging%20datasets.%0ACombined%20with%20MedMNIST-C%2C%20this%20establishes%20a%20comprehensive%20robustness%0Aevaluation%20framework%20for%20MVLMs.%20We%20further%20propose%20RobustMedCLIP%2C%20a%20visual%0Aencoder%20adaptation%20of%20a%20pretrained%20MVLM%20that%20incorporates%20few-shot%20tuning%20to%0Aenhance%20resilience%20against%20corruptions.%20Through%20extensive%20experiments%2C%20we%0Abenchmark%205%20major%20MVLMs%20across%205%20medical%20imaging%20modalities%2C%20revealing%20that%0Aexisting%20models%20exhibit%20severe%20degradation%20under%20corruption%20and%20struggle%20with%0Adomain-modality%20tradeoffs.%20Our%20findings%20highlight%20the%20necessity%20of%20diverse%0Atraining%20and%20robust%20adaptation%20strategies%2C%20demonstrating%20that%20efficient%0Alow-rank%20adaptation%20when%20paired%20with%20few-shot%20tuning%2C%20improves%20robustness%20while%0Apreserving%20generalization%20across%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15425v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Robustness%2520of%2520Medical%2520Vision-Language%2520Models%253A%2520Are%2520they%2520Truly%250A%2520%2520Generalizable%253F%26entry.906535625%3DRaza%2520Imam%2520and%2520Rufael%2520Marew%2520and%2520Mohammad%2520Yaqub%26entry.1292438233%3D%2520%2520Medical%2520Vision-Language%2520Models%2520%2528MVLMs%2529%2520have%2520achieved%2520par%2520excellence%250Ageneralization%2520in%2520medical%2520image%2520analysis%252C%2520yet%2520their%2520performance%2520under%2520noisy%252C%250Acorrupted%2520conditions%2520remains%2520largely%2520untested.%2520Clinical%2520imaging%2520is%2520inherently%250Asusceptible%2520to%2520acquisition%2520artifacts%2520and%2520noise%253B%2520however%252C%2520existing%2520evaluations%250Apredominantly%2520assess%2520generally%2520clean%2520datasets%252C%2520overlooking%2520robustness%2520--%2520i.e.%252C%250Athe%2520model%2527s%2520ability%2520to%2520perform%2520under%2520real-world%2520distortions.%2520To%2520address%2520this%250Agap%252C%2520we%2520first%2520introduce%2520MediMeta-C%252C%2520a%2520corruption%2520benchmark%2520that%2520systematically%250Aapplies%2520several%2520perturbations%2520across%2520multiple%2520medical%2520imaging%2520datasets.%250ACombined%2520with%2520MedMNIST-C%252C%2520this%2520establishes%2520a%2520comprehensive%2520robustness%250Aevaluation%2520framework%2520for%2520MVLMs.%2520We%2520further%2520propose%2520RobustMedCLIP%252C%2520a%2520visual%250Aencoder%2520adaptation%2520of%2520a%2520pretrained%2520MVLM%2520that%2520incorporates%2520few-shot%2520tuning%2520to%250Aenhance%2520resilience%2520against%2520corruptions.%2520Through%2520extensive%2520experiments%252C%2520we%250Abenchmark%25205%2520major%2520MVLMs%2520across%25205%2520medical%2520imaging%2520modalities%252C%2520revealing%2520that%250Aexisting%2520models%2520exhibit%2520severe%2520degradation%2520under%2520corruption%2520and%2520struggle%2520with%250Adomain-modality%2520tradeoffs.%2520Our%2520findings%2520highlight%2520the%2520necessity%2520of%2520diverse%250Atraining%2520and%2520robust%2520adaptation%2520strategies%252C%2520demonstrating%2520that%2520efficient%250Alow-rank%2520adaptation%2520when%2520paired%2520with%2520few-shot%2520tuning%252C%2520improves%2520robustness%2520while%250Apreserving%2520generalization%2520across%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15425v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Robustness%20of%20Medical%20Vision-Language%20Models%3A%20Are%20they%20Truly%0A%20%20Generalizable%3F&entry.906535625=Raza%20Imam%20and%20Rufael%20Marew%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Medical%20Vision-Language%20Models%20%28MVLMs%29%20have%20achieved%20par%20excellence%0Ageneralization%20in%20medical%20image%20analysis%2C%20yet%20their%20performance%20under%20noisy%2C%0Acorrupted%20conditions%20remains%20largely%20untested.%20Clinical%20imaging%20is%20inherently%0Asusceptible%20to%20acquisition%20artifacts%20and%20noise%3B%20however%2C%20existing%20evaluations%0Apredominantly%20assess%20generally%20clean%20datasets%2C%20overlooking%20robustness%20--%20i.e.%2C%0Athe%20model%27s%20ability%20to%20perform%20under%20real-world%20distortions.%20To%20address%20this%0Agap%2C%20we%20first%20introduce%20MediMeta-C%2C%20a%20corruption%20benchmark%20that%20systematically%0Aapplies%20several%20perturbations%20across%20multiple%20medical%20imaging%20datasets.%0ACombined%20with%20MedMNIST-C%2C%20this%20establishes%20a%20comprehensive%20robustness%0Aevaluation%20framework%20for%20MVLMs.%20We%20further%20propose%20RobustMedCLIP%2C%20a%20visual%0Aencoder%20adaptation%20of%20a%20pretrained%20MVLM%20that%20incorporates%20few-shot%20tuning%20to%0Aenhance%20resilience%20against%20corruptions.%20Through%20extensive%20experiments%2C%20we%0Abenchmark%205%20major%20MVLMs%20across%205%20medical%20imaging%20modalities%2C%20revealing%20that%0Aexisting%20models%20exhibit%20severe%20degradation%20under%20corruption%20and%20struggle%20with%0Adomain-modality%20tradeoffs.%20Our%20findings%20highlight%20the%20necessity%20of%20diverse%0Atraining%20and%20robust%20adaptation%20strategies%2C%20demonstrating%20that%20efficient%0Alow-rank%20adaptation%20when%20paired%20with%20few-shot%20tuning%2C%20improves%20robustness%20while%0Apreserving%20generalization%20across%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15425v2&entry.124074799=Read"},
{"title": "Forensics Adapter: Unleashing CLIP for Generalizable Face Forgery\n  Detection", "author": "Xinjie Cui and Yuezun Li and Delong Zhu and Jiaran Zhou and Junyu Dong and Siwei Lyu", "abstract": "  We describe Forensics Adapter, an adapter network designed to transform CLIP\ninto an effective and generalizable face forgery detector. Although CLIP is\nhighly versatile, adapting it for face forgery detection is non-trivial as\nforgery-related knowledge is entangled with a wide range of unrelated\nknowledge. Existing methods treat CLIP merely as a feature extractor, lacking\ntask-specific adaptation, which limits their effectiveness. To address this, we\nintroduce an adapter to learn face forgery traces -- the blending boundaries\nunique to forged faces, guided by task-specific objectives. Then we enhance the\nCLIP visual tokens with a dedicated interaction strategy that communicates\nknowledge across CLIP and the adapter. Since the adapter is alongside CLIP, its\nversatility is highly retained, naturally ensuring strong generalizability in\nface forgery detection. With only 5.7M trainable parameters, our method\nachieves a significant performance boost, improving by approximately 7% on\naverage across five standard datasets. Additionally, we describe Forensics\nAdapter++, an extended method that incorporates textual modality via a newly\nproposed forgery-aware prompt learning strategy. This extension leads to a\nfurther 1.3% performance boost over the original Forensics Adapter. We believe\nthe proposed methods can serve as a baseline for future CLIP-based face forgery\ndetection methods. The codes have been released at\nhttps://github.com/OUC-VAS/ForensicsAdapter.\n", "link": "http://arxiv.org/abs/2411.19715v3", "date": "2025-05-23", "relevancy": 2.1436, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5867}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5045}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forensics%20Adapter%3A%20Unleashing%20CLIP%20for%20Generalizable%20Face%20Forgery%0A%20%20Detection&body=Title%3A%20Forensics%20Adapter%3A%20Unleashing%20CLIP%20for%20Generalizable%20Face%20Forgery%0A%20%20Detection%0AAuthor%3A%20Xinjie%20Cui%20and%20Yuezun%20Li%20and%20Delong%20Zhu%20and%20Jiaran%20Zhou%20and%20Junyu%20Dong%20and%20Siwei%20Lyu%0AAbstract%3A%20%20%20We%20describe%20Forensics%20Adapter%2C%20an%20adapter%20network%20designed%20to%20transform%20CLIP%0Ainto%20an%20effective%20and%20generalizable%20face%20forgery%20detector.%20Although%20CLIP%20is%0Ahighly%20versatile%2C%20adapting%20it%20for%20face%20forgery%20detection%20is%20non-trivial%20as%0Aforgery-related%20knowledge%20is%20entangled%20with%20a%20wide%20range%20of%20unrelated%0Aknowledge.%20Existing%20methods%20treat%20CLIP%20merely%20as%20a%20feature%20extractor%2C%20lacking%0Atask-specific%20adaptation%2C%20which%20limits%20their%20effectiveness.%20To%20address%20this%2C%20we%0Aintroduce%20an%20adapter%20to%20learn%20face%20forgery%20traces%20--%20the%20blending%20boundaries%0Aunique%20to%20forged%20faces%2C%20guided%20by%20task-specific%20objectives.%20Then%20we%20enhance%20the%0ACLIP%20visual%20tokens%20with%20a%20dedicated%20interaction%20strategy%20that%20communicates%0Aknowledge%20across%20CLIP%20and%20the%20adapter.%20Since%20the%20adapter%20is%20alongside%20CLIP%2C%20its%0Aversatility%20is%20highly%20retained%2C%20naturally%20ensuring%20strong%20generalizability%20in%0Aface%20forgery%20detection.%20With%20only%205.7M%20trainable%20parameters%2C%20our%20method%0Aachieves%20a%20significant%20performance%20boost%2C%20improving%20by%20approximately%207%25%20on%0Aaverage%20across%20five%20standard%20datasets.%20Additionally%2C%20we%20describe%20Forensics%0AAdapter%2B%2B%2C%20an%20extended%20method%20that%20incorporates%20textual%20modality%20via%20a%20newly%0Aproposed%20forgery-aware%20prompt%20learning%20strategy.%20This%20extension%20leads%20to%20a%0Afurther%201.3%25%20performance%20boost%20over%20the%20original%20Forensics%20Adapter.%20We%20believe%0Athe%20proposed%20methods%20can%20serve%20as%20a%20baseline%20for%20future%20CLIP-based%20face%20forgery%0Adetection%20methods.%20The%20codes%20have%20been%20released%20at%0Ahttps%3A//github.com/OUC-VAS/ForensicsAdapter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19715v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForensics%2520Adapter%253A%2520Unleashing%2520CLIP%2520for%2520Generalizable%2520Face%2520Forgery%250A%2520%2520Detection%26entry.906535625%3DXinjie%2520Cui%2520and%2520Yuezun%2520Li%2520and%2520Delong%2520Zhu%2520and%2520Jiaran%2520Zhou%2520and%2520Junyu%2520Dong%2520and%2520Siwei%2520Lyu%26entry.1292438233%3D%2520%2520We%2520describe%2520Forensics%2520Adapter%252C%2520an%2520adapter%2520network%2520designed%2520to%2520transform%2520CLIP%250Ainto%2520an%2520effective%2520and%2520generalizable%2520face%2520forgery%2520detector.%2520Although%2520CLIP%2520is%250Ahighly%2520versatile%252C%2520adapting%2520it%2520for%2520face%2520forgery%2520detection%2520is%2520non-trivial%2520as%250Aforgery-related%2520knowledge%2520is%2520entangled%2520with%2520a%2520wide%2520range%2520of%2520unrelated%250Aknowledge.%2520Existing%2520methods%2520treat%2520CLIP%2520merely%2520as%2520a%2520feature%2520extractor%252C%2520lacking%250Atask-specific%2520adaptation%252C%2520which%2520limits%2520their%2520effectiveness.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520an%2520adapter%2520to%2520learn%2520face%2520forgery%2520traces%2520--%2520the%2520blending%2520boundaries%250Aunique%2520to%2520forged%2520faces%252C%2520guided%2520by%2520task-specific%2520objectives.%2520Then%2520we%2520enhance%2520the%250ACLIP%2520visual%2520tokens%2520with%2520a%2520dedicated%2520interaction%2520strategy%2520that%2520communicates%250Aknowledge%2520across%2520CLIP%2520and%2520the%2520adapter.%2520Since%2520the%2520adapter%2520is%2520alongside%2520CLIP%252C%2520its%250Aversatility%2520is%2520highly%2520retained%252C%2520naturally%2520ensuring%2520strong%2520generalizability%2520in%250Aface%2520forgery%2520detection.%2520With%2520only%25205.7M%2520trainable%2520parameters%252C%2520our%2520method%250Aachieves%2520a%2520significant%2520performance%2520boost%252C%2520improving%2520by%2520approximately%25207%2525%2520on%250Aaverage%2520across%2520five%2520standard%2520datasets.%2520Additionally%252C%2520we%2520describe%2520Forensics%250AAdapter%252B%252B%252C%2520an%2520extended%2520method%2520that%2520incorporates%2520textual%2520modality%2520via%2520a%2520newly%250Aproposed%2520forgery-aware%2520prompt%2520learning%2520strategy.%2520This%2520extension%2520leads%2520to%2520a%250Afurther%25201.3%2525%2520performance%2520boost%2520over%2520the%2520original%2520Forensics%2520Adapter.%2520We%2520believe%250Athe%2520proposed%2520methods%2520can%2520serve%2520as%2520a%2520baseline%2520for%2520future%2520CLIP-based%2520face%2520forgery%250Adetection%2520methods.%2520The%2520codes%2520have%2520been%2520released%2520at%250Ahttps%253A//github.com/OUC-VAS/ForensicsAdapter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19715v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forensics%20Adapter%3A%20Unleashing%20CLIP%20for%20Generalizable%20Face%20Forgery%0A%20%20Detection&entry.906535625=Xinjie%20Cui%20and%20Yuezun%20Li%20and%20Delong%20Zhu%20and%20Jiaran%20Zhou%20and%20Junyu%20Dong%20and%20Siwei%20Lyu&entry.1292438233=%20%20We%20describe%20Forensics%20Adapter%2C%20an%20adapter%20network%20designed%20to%20transform%20CLIP%0Ainto%20an%20effective%20and%20generalizable%20face%20forgery%20detector.%20Although%20CLIP%20is%0Ahighly%20versatile%2C%20adapting%20it%20for%20face%20forgery%20detection%20is%20non-trivial%20as%0Aforgery-related%20knowledge%20is%20entangled%20with%20a%20wide%20range%20of%20unrelated%0Aknowledge.%20Existing%20methods%20treat%20CLIP%20merely%20as%20a%20feature%20extractor%2C%20lacking%0Atask-specific%20adaptation%2C%20which%20limits%20their%20effectiveness.%20To%20address%20this%2C%20we%0Aintroduce%20an%20adapter%20to%20learn%20face%20forgery%20traces%20--%20the%20blending%20boundaries%0Aunique%20to%20forged%20faces%2C%20guided%20by%20task-specific%20objectives.%20Then%20we%20enhance%20the%0ACLIP%20visual%20tokens%20with%20a%20dedicated%20interaction%20strategy%20that%20communicates%0Aknowledge%20across%20CLIP%20and%20the%20adapter.%20Since%20the%20adapter%20is%20alongside%20CLIP%2C%20its%0Aversatility%20is%20highly%20retained%2C%20naturally%20ensuring%20strong%20generalizability%20in%0Aface%20forgery%20detection.%20With%20only%205.7M%20trainable%20parameters%2C%20our%20method%0Aachieves%20a%20significant%20performance%20boost%2C%20improving%20by%20approximately%207%25%20on%0Aaverage%20across%20five%20standard%20datasets.%20Additionally%2C%20we%20describe%20Forensics%0AAdapter%2B%2B%2C%20an%20extended%20method%20that%20incorporates%20textual%20modality%20via%20a%20newly%0Aproposed%20forgery-aware%20prompt%20learning%20strategy.%20This%20extension%20leads%20to%20a%0Afurther%201.3%25%20performance%20boost%20over%20the%20original%20Forensics%20Adapter.%20We%20believe%0Athe%20proposed%20methods%20can%20serve%20as%20a%20baseline%20for%20future%20CLIP-based%20face%20forgery%0Adetection%20methods.%20The%20codes%20have%20been%20released%20at%0Ahttps%3A//github.com/OUC-VAS/ForensicsAdapter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19715v3&entry.124074799=Read"},
{"title": "CP-Guard: Malicious Agent Detection and Defense in Collaborative Bird's\n  Eye View Perception", "author": "Senkang Hu and Yihang Tao and Guowen Xu and Yiqin Deng and Xianhao Chen and Yuguang Fang and Sam Kwong", "abstract": "  Collaborative Perception (CP) has shown a promising technique for autonomous\ndriving, where multiple connected and autonomous vehicles (CAVs) share their\nperception information to enhance the overall perception performance and expand\nthe perception range. However, in CP, ego CAV needs to receive messages from\nits collaborators, which makes it easy to be attacked by malicious agents. For\nexample, a malicious agent can send harmful information to the ego CAV to\nmislead it. To address this critical issue, we propose a novel method,\nCP-Guard, a tailored defense mechanism for CP that can be deployed by each\nagent to accurately detect and eliminate malicious agents in its collaboration\nnetwork. Our key idea is to enable CP to reach a consensus rather than a\nconflict against the ego CAV's perception results. Based on this idea, we first\ndevelop a probability-agnostic sample consensus (PASAC) method to effectively\nsample a subset of the collaborators and verify the consensus without prior\nprobabilities of malicious agents. Furthermore, we define a collaborative\nconsistency loss (CCLoss) to capture the discrepancy between the ego CAV and\nits collaborators, which is used as a verification criterion for consensus.\nFinally, we conduct extensive experiments in collaborative bird's eye view\n(BEV) tasks and our results demonstrate the effectiveness of our CP-Guard. Code\nis available at https://github.com/CP-Security/CP-Guard\n", "link": "http://arxiv.org/abs/2412.12000v2", "date": "2025-05-23", "relevancy": 2.1205, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5643}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5105}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CP-Guard%3A%20Malicious%20Agent%20Detection%20and%20Defense%20in%20Collaborative%20Bird%27s%0A%20%20Eye%20View%20Perception&body=Title%3A%20CP-Guard%3A%20Malicious%20Agent%20Detection%20and%20Defense%20in%20Collaborative%20Bird%27s%0A%20%20Eye%20View%20Perception%0AAuthor%3A%20Senkang%20Hu%20and%20Yihang%20Tao%20and%20Guowen%20Xu%20and%20Yiqin%20Deng%20and%20Xianhao%20Chen%20and%20Yuguang%20Fang%20and%20Sam%20Kwong%0AAbstract%3A%20%20%20Collaborative%20Perception%20%28CP%29%20has%20shown%20a%20promising%20technique%20for%20autonomous%0Adriving%2C%20where%20multiple%20connected%20and%20autonomous%20vehicles%20%28CAVs%29%20share%20their%0Aperception%20information%20to%20enhance%20the%20overall%20perception%20performance%20and%20expand%0Athe%20perception%20range.%20However%2C%20in%20CP%2C%20ego%20CAV%20needs%20to%20receive%20messages%20from%0Aits%20collaborators%2C%20which%20makes%20it%20easy%20to%20be%20attacked%20by%20malicious%20agents.%20For%0Aexample%2C%20a%20malicious%20agent%20can%20send%20harmful%20information%20to%20the%20ego%20CAV%20to%0Amislead%20it.%20To%20address%20this%20critical%20issue%2C%20we%20propose%20a%20novel%20method%2C%0ACP-Guard%2C%20a%20tailored%20defense%20mechanism%20for%20CP%20that%20can%20be%20deployed%20by%20each%0Aagent%20to%20accurately%20detect%20and%20eliminate%20malicious%20agents%20in%20its%20collaboration%0Anetwork.%20Our%20key%20idea%20is%20to%20enable%20CP%20to%20reach%20a%20consensus%20rather%20than%20a%0Aconflict%20against%20the%20ego%20CAV%27s%20perception%20results.%20Based%20on%20this%20idea%2C%20we%20first%0Adevelop%20a%20probability-agnostic%20sample%20consensus%20%28PASAC%29%20method%20to%20effectively%0Asample%20a%20subset%20of%20the%20collaborators%20and%20verify%20the%20consensus%20without%20prior%0Aprobabilities%20of%20malicious%20agents.%20Furthermore%2C%20we%20define%20a%20collaborative%0Aconsistency%20loss%20%28CCLoss%29%20to%20capture%20the%20discrepancy%20between%20the%20ego%20CAV%20and%0Aits%20collaborators%2C%20which%20is%20used%20as%20a%20verification%20criterion%20for%20consensus.%0AFinally%2C%20we%20conduct%20extensive%20experiments%20in%20collaborative%20bird%27s%20eye%20view%0A%28BEV%29%20tasks%20and%20our%20results%20demonstrate%20the%20effectiveness%20of%20our%20CP-Guard.%20Code%0Ais%20available%20at%20https%3A//github.com/CP-Security/CP-Guard%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12000v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCP-Guard%253A%2520Malicious%2520Agent%2520Detection%2520and%2520Defense%2520in%2520Collaborative%2520Bird%2527s%250A%2520%2520Eye%2520View%2520Perception%26entry.906535625%3DSenkang%2520Hu%2520and%2520Yihang%2520Tao%2520and%2520Guowen%2520Xu%2520and%2520Yiqin%2520Deng%2520and%2520Xianhao%2520Chen%2520and%2520Yuguang%2520Fang%2520and%2520Sam%2520Kwong%26entry.1292438233%3D%2520%2520Collaborative%2520Perception%2520%2528CP%2529%2520has%2520shown%2520a%2520promising%2520technique%2520for%2520autonomous%250Adriving%252C%2520where%2520multiple%2520connected%2520and%2520autonomous%2520vehicles%2520%2528CAVs%2529%2520share%2520their%250Aperception%2520information%2520to%2520enhance%2520the%2520overall%2520perception%2520performance%2520and%2520expand%250Athe%2520perception%2520range.%2520However%252C%2520in%2520CP%252C%2520ego%2520CAV%2520needs%2520to%2520receive%2520messages%2520from%250Aits%2520collaborators%252C%2520which%2520makes%2520it%2520easy%2520to%2520be%2520attacked%2520by%2520malicious%2520agents.%2520For%250Aexample%252C%2520a%2520malicious%2520agent%2520can%2520send%2520harmful%2520information%2520to%2520the%2520ego%2520CAV%2520to%250Amislead%2520it.%2520To%2520address%2520this%2520critical%2520issue%252C%2520we%2520propose%2520a%2520novel%2520method%252C%250ACP-Guard%252C%2520a%2520tailored%2520defense%2520mechanism%2520for%2520CP%2520that%2520can%2520be%2520deployed%2520by%2520each%250Aagent%2520to%2520accurately%2520detect%2520and%2520eliminate%2520malicious%2520agents%2520in%2520its%2520collaboration%250Anetwork.%2520Our%2520key%2520idea%2520is%2520to%2520enable%2520CP%2520to%2520reach%2520a%2520consensus%2520rather%2520than%2520a%250Aconflict%2520against%2520the%2520ego%2520CAV%2527s%2520perception%2520results.%2520Based%2520on%2520this%2520idea%252C%2520we%2520first%250Adevelop%2520a%2520probability-agnostic%2520sample%2520consensus%2520%2528PASAC%2529%2520method%2520to%2520effectively%250Asample%2520a%2520subset%2520of%2520the%2520collaborators%2520and%2520verify%2520the%2520consensus%2520without%2520prior%250Aprobabilities%2520of%2520malicious%2520agents.%2520Furthermore%252C%2520we%2520define%2520a%2520collaborative%250Aconsistency%2520loss%2520%2528CCLoss%2529%2520to%2520capture%2520the%2520discrepancy%2520between%2520the%2520ego%2520CAV%2520and%250Aits%2520collaborators%252C%2520which%2520is%2520used%2520as%2520a%2520verification%2520criterion%2520for%2520consensus.%250AFinally%252C%2520we%2520conduct%2520extensive%2520experiments%2520in%2520collaborative%2520bird%2527s%2520eye%2520view%250A%2528BEV%2529%2520tasks%2520and%2520our%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520CP-Guard.%2520Code%250Ais%2520available%2520at%2520https%253A//github.com/CP-Security/CP-Guard%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12000v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CP-Guard%3A%20Malicious%20Agent%20Detection%20and%20Defense%20in%20Collaborative%20Bird%27s%0A%20%20Eye%20View%20Perception&entry.906535625=Senkang%20Hu%20and%20Yihang%20Tao%20and%20Guowen%20Xu%20and%20Yiqin%20Deng%20and%20Xianhao%20Chen%20and%20Yuguang%20Fang%20and%20Sam%20Kwong&entry.1292438233=%20%20Collaborative%20Perception%20%28CP%29%20has%20shown%20a%20promising%20technique%20for%20autonomous%0Adriving%2C%20where%20multiple%20connected%20and%20autonomous%20vehicles%20%28CAVs%29%20share%20their%0Aperception%20information%20to%20enhance%20the%20overall%20perception%20performance%20and%20expand%0Athe%20perception%20range.%20However%2C%20in%20CP%2C%20ego%20CAV%20needs%20to%20receive%20messages%20from%0Aits%20collaborators%2C%20which%20makes%20it%20easy%20to%20be%20attacked%20by%20malicious%20agents.%20For%0Aexample%2C%20a%20malicious%20agent%20can%20send%20harmful%20information%20to%20the%20ego%20CAV%20to%0Amislead%20it.%20To%20address%20this%20critical%20issue%2C%20we%20propose%20a%20novel%20method%2C%0ACP-Guard%2C%20a%20tailored%20defense%20mechanism%20for%20CP%20that%20can%20be%20deployed%20by%20each%0Aagent%20to%20accurately%20detect%20and%20eliminate%20malicious%20agents%20in%20its%20collaboration%0Anetwork.%20Our%20key%20idea%20is%20to%20enable%20CP%20to%20reach%20a%20consensus%20rather%20than%20a%0Aconflict%20against%20the%20ego%20CAV%27s%20perception%20results.%20Based%20on%20this%20idea%2C%20we%20first%0Adevelop%20a%20probability-agnostic%20sample%20consensus%20%28PASAC%29%20method%20to%20effectively%0Asample%20a%20subset%20of%20the%20collaborators%20and%20verify%20the%20consensus%20without%20prior%0Aprobabilities%20of%20malicious%20agents.%20Furthermore%2C%20we%20define%20a%20collaborative%0Aconsistency%20loss%20%28CCLoss%29%20to%20capture%20the%20discrepancy%20between%20the%20ego%20CAV%20and%0Aits%20collaborators%2C%20which%20is%20used%20as%20a%20verification%20criterion%20for%20consensus.%0AFinally%2C%20we%20conduct%20extensive%20experiments%20in%20collaborative%20bird%27s%20eye%20view%0A%28BEV%29%20tasks%20and%20our%20results%20demonstrate%20the%20effectiveness%20of%20our%20CP-Guard.%20Code%0Ais%20available%20at%20https%3A//github.com/CP-Security/CP-Guard%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12000v2&entry.124074799=Read"},
{"title": "Overcoming Non-stationary Dynamics with Evidential Proximal Policy\n  Optimization", "author": "Abdullah Akg\u00fcl and Gulcin Baykal and Manuel Hau\u00dfmann and Melih Kandemir", "abstract": "  Continuous control of non-stationary environments is a major challenge for\ndeep reinforcement learning algorithms. The time-dependency of the state\ntransition dynamics aggravates the notorious stability problems of model-free\ndeep actor-critic architectures. We posit that two properties will play a key\nrole in overcoming non-stationarity in transition dynamics: (i) preserving the\nplasticity of the critic network, (ii) directed exploration for rapid\nadaptation to the changing dynamics. We show that performing on-policy\nreinforcement learning with an evidential critic provides both of these\nproperties. The evidential design ensures a fast and sufficiently accurate\napproximation to the uncertainty around the state-value, which maintains the\nplasticity of the critic network by detecting the distributional shifts caused\nby the change in dynamics. The probabilistic critic also makes the actor\ntraining objective a random variable, enabling the use of directed exploration\napproaches as a by-product. We name the resulting algorithm as $\\textit{\nEvidential Proximal Policy Optimization (EPPO)}$ due to the integral role of\nevidential uncertainty quantification in both policy evaluation and policy\nimprovement stages. Through experiments on non-stationary continuous control\ntasks, where the environment dynamics change at regular intervals, we\ndemonstrate that our algorithm outperforms state-of-the-art on-policy\nreinforcement learning variants in both task-specific and overall return.\n", "link": "http://arxiv.org/abs/2503.01468v2", "date": "2025-05-23", "relevancy": 1.5789, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5304}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5217}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20Non-stationary%20Dynamics%20with%20Evidential%20Proximal%20Policy%0A%20%20Optimization&body=Title%3A%20Overcoming%20Non-stationary%20Dynamics%20with%20Evidential%20Proximal%20Policy%0A%20%20Optimization%0AAuthor%3A%20Abdullah%20Akg%C3%BCl%20and%20Gulcin%20Baykal%20and%20Manuel%20Hau%C3%9Fmann%20and%20Melih%20Kandemir%0AAbstract%3A%20%20%20Continuous%20control%20of%20non-stationary%20environments%20is%20a%20major%20challenge%20for%0Adeep%20reinforcement%20learning%20algorithms.%20The%20time-dependency%20of%20the%20state%0Atransition%20dynamics%20aggravates%20the%20notorious%20stability%20problems%20of%20model-free%0Adeep%20actor-critic%20architectures.%20We%20posit%20that%20two%20properties%20will%20play%20a%20key%0Arole%20in%20overcoming%20non-stationarity%20in%20transition%20dynamics%3A%20%28i%29%20preserving%20the%0Aplasticity%20of%20the%20critic%20network%2C%20%28ii%29%20directed%20exploration%20for%20rapid%0Aadaptation%20to%20the%20changing%20dynamics.%20We%20show%20that%20performing%20on-policy%0Areinforcement%20learning%20with%20an%20evidential%20critic%20provides%20both%20of%20these%0Aproperties.%20The%20evidential%20design%20ensures%20a%20fast%20and%20sufficiently%20accurate%0Aapproximation%20to%20the%20uncertainty%20around%20the%20state-value%2C%20which%20maintains%20the%0Aplasticity%20of%20the%20critic%20network%20by%20detecting%20the%20distributional%20shifts%20caused%0Aby%20the%20change%20in%20dynamics.%20The%20probabilistic%20critic%20also%20makes%20the%20actor%0Atraining%20objective%20a%20random%20variable%2C%20enabling%20the%20use%20of%20directed%20exploration%0Aapproaches%20as%20a%20by-product.%20We%20name%20the%20resulting%20algorithm%20as%20%24%5Ctextit%7B%0AEvidential%20Proximal%20Policy%20Optimization%20%28EPPO%29%7D%24%20due%20to%20the%20integral%20role%20of%0Aevidential%20uncertainty%20quantification%20in%20both%20policy%20evaluation%20and%20policy%0Aimprovement%20stages.%20Through%20experiments%20on%20non-stationary%20continuous%20control%0Atasks%2C%20where%20the%20environment%20dynamics%20change%20at%20regular%20intervals%2C%20we%0Ademonstrate%20that%20our%20algorithm%20outperforms%20state-of-the-art%20on-policy%0Areinforcement%20learning%20variants%20in%20both%20task-specific%20and%20overall%20return.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01468v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520Non-stationary%2520Dynamics%2520with%2520Evidential%2520Proximal%2520Policy%250A%2520%2520Optimization%26entry.906535625%3DAbdullah%2520Akg%25C3%25BCl%2520and%2520Gulcin%2520Baykal%2520and%2520Manuel%2520Hau%25C3%259Fmann%2520and%2520Melih%2520Kandemir%26entry.1292438233%3D%2520%2520Continuous%2520control%2520of%2520non-stationary%2520environments%2520is%2520a%2520major%2520challenge%2520for%250Adeep%2520reinforcement%2520learning%2520algorithms.%2520The%2520time-dependency%2520of%2520the%2520state%250Atransition%2520dynamics%2520aggravates%2520the%2520notorious%2520stability%2520problems%2520of%2520model-free%250Adeep%2520actor-critic%2520architectures.%2520We%2520posit%2520that%2520two%2520properties%2520will%2520play%2520a%2520key%250Arole%2520in%2520overcoming%2520non-stationarity%2520in%2520transition%2520dynamics%253A%2520%2528i%2529%2520preserving%2520the%250Aplasticity%2520of%2520the%2520critic%2520network%252C%2520%2528ii%2529%2520directed%2520exploration%2520for%2520rapid%250Aadaptation%2520to%2520the%2520changing%2520dynamics.%2520We%2520show%2520that%2520performing%2520on-policy%250Areinforcement%2520learning%2520with%2520an%2520evidential%2520critic%2520provides%2520both%2520of%2520these%250Aproperties.%2520The%2520evidential%2520design%2520ensures%2520a%2520fast%2520and%2520sufficiently%2520accurate%250Aapproximation%2520to%2520the%2520uncertainty%2520around%2520the%2520state-value%252C%2520which%2520maintains%2520the%250Aplasticity%2520of%2520the%2520critic%2520network%2520by%2520detecting%2520the%2520distributional%2520shifts%2520caused%250Aby%2520the%2520change%2520in%2520dynamics.%2520The%2520probabilistic%2520critic%2520also%2520makes%2520the%2520actor%250Atraining%2520objective%2520a%2520random%2520variable%252C%2520enabling%2520the%2520use%2520of%2520directed%2520exploration%250Aapproaches%2520as%2520a%2520by-product.%2520We%2520name%2520the%2520resulting%2520algorithm%2520as%2520%2524%255Ctextit%257B%250AEvidential%2520Proximal%2520Policy%2520Optimization%2520%2528EPPO%2529%257D%2524%2520due%2520to%2520the%2520integral%2520role%2520of%250Aevidential%2520uncertainty%2520quantification%2520in%2520both%2520policy%2520evaluation%2520and%2520policy%250Aimprovement%2520stages.%2520Through%2520experiments%2520on%2520non-stationary%2520continuous%2520control%250Atasks%252C%2520where%2520the%2520environment%2520dynamics%2520change%2520at%2520regular%2520intervals%252C%2520we%250Ademonstrate%2520that%2520our%2520algorithm%2520outperforms%2520state-of-the-art%2520on-policy%250Areinforcement%2520learning%2520variants%2520in%2520both%2520task-specific%2520and%2520overall%2520return.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01468v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20Non-stationary%20Dynamics%20with%20Evidential%20Proximal%20Policy%0A%20%20Optimization&entry.906535625=Abdullah%20Akg%C3%BCl%20and%20Gulcin%20Baykal%20and%20Manuel%20Hau%C3%9Fmann%20and%20Melih%20Kandemir&entry.1292438233=%20%20Continuous%20control%20of%20non-stationary%20environments%20is%20a%20major%20challenge%20for%0Adeep%20reinforcement%20learning%20algorithms.%20The%20time-dependency%20of%20the%20state%0Atransition%20dynamics%20aggravates%20the%20notorious%20stability%20problems%20of%20model-free%0Adeep%20actor-critic%20architectures.%20We%20posit%20that%20two%20properties%20will%20play%20a%20key%0Arole%20in%20overcoming%20non-stationarity%20in%20transition%20dynamics%3A%20%28i%29%20preserving%20the%0Aplasticity%20of%20the%20critic%20network%2C%20%28ii%29%20directed%20exploration%20for%20rapid%0Aadaptation%20to%20the%20changing%20dynamics.%20We%20show%20that%20performing%20on-policy%0Areinforcement%20learning%20with%20an%20evidential%20critic%20provides%20both%20of%20these%0Aproperties.%20The%20evidential%20design%20ensures%20a%20fast%20and%20sufficiently%20accurate%0Aapproximation%20to%20the%20uncertainty%20around%20the%20state-value%2C%20which%20maintains%20the%0Aplasticity%20of%20the%20critic%20network%20by%20detecting%20the%20distributional%20shifts%20caused%0Aby%20the%20change%20in%20dynamics.%20The%20probabilistic%20critic%20also%20makes%20the%20actor%0Atraining%20objective%20a%20random%20variable%2C%20enabling%20the%20use%20of%20directed%20exploration%0Aapproaches%20as%20a%20by-product.%20We%20name%20the%20resulting%20algorithm%20as%20%24%5Ctextit%7B%0AEvidential%20Proximal%20Policy%20Optimization%20%28EPPO%29%7D%24%20due%20to%20the%20integral%20role%20of%0Aevidential%20uncertainty%20quantification%20in%20both%20policy%20evaluation%20and%20policy%0Aimprovement%20stages.%20Through%20experiments%20on%20non-stationary%20continuous%20control%0Atasks%2C%20where%20the%20environment%20dynamics%20change%20at%20regular%20intervals%2C%20we%0Ademonstrate%20that%20our%20algorithm%20outperforms%20state-of-the-art%20on-policy%0Areinforcement%20learning%20variants%20in%20both%20task-specific%20and%20overall%20return.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01468v2&entry.124074799=Read"},
{"title": "URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous\n  Vehicles", "author": "Ahmet Onur Akman and Anastasia Psarou and Micha\u0142 Hoffmann and \u0141ukasz Gorczyca and \u0141ukasz Kowalski and Pawe\u0142 Gora and Grzegorz Jamr\u00f3z and Rafa\u0142 Kucharski", "abstract": "  Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future\nurban networks, potentially by optimizing their routing decisions. Unlike for\nhuman drivers, these decisions can be made with collective, data-driven\npolicies, developed by machine learning algorithms. Reinforcement learning (RL)\ncan facilitate the development of such collective routing strategies, yet\nstandardized and realistic benchmarks are missing. To that end, we present\n\\our{}: Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles.\n\\our{} is a comprehensive benchmarking environment that unifies evaluation\nacross 29 real-world traffic networks paired with realistic demand patterns.\n\\our{} comes with a catalog of predefined tasks, four state-of-the-art\nmulti-agent RL (MARL) algorithm implementations, three baseline methods,\ndomain-specific performance metrics, and a modular configuration scheme. Our\nresults suggest that, despite the lengthy and costly training, state-of-the-art\nMARL algorithms rarely outperformed humans. Experimental results reported in\nthis paper initiate the first leaderboard for MARL in large-scale urban routing\noptimization and reveal that current approaches struggle to scale, emphasizing\nthe urgent need for advancements in this domain.\n", "link": "http://arxiv.org/abs/2505.17734v1", "date": "2025-05-23", "relevancy": 1.5844, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5434}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5133}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20URB%20--%20Urban%20Routing%20Benchmark%20for%20RL-equipped%20Connected%20Autonomous%0A%20%20Vehicles&body=Title%3A%20URB%20--%20Urban%20Routing%20Benchmark%20for%20RL-equipped%20Connected%20Autonomous%0A%20%20Vehicles%0AAuthor%3A%20Ahmet%20Onur%20Akman%20and%20Anastasia%20Psarou%20and%20Micha%C5%82%20Hoffmann%20and%20%C5%81ukasz%20Gorczyca%20and%20%C5%81ukasz%20Kowalski%20and%20Pawe%C5%82%20Gora%20and%20Grzegorz%20Jamr%C3%B3z%20and%20Rafa%C5%82%20Kucharski%0AAbstract%3A%20%20%20Connected%20Autonomous%20Vehicles%20%28CAVs%29%20promise%20to%20reduce%20congestion%20in%20future%0Aurban%20networks%2C%20potentially%20by%20optimizing%20their%20routing%20decisions.%20Unlike%20for%0Ahuman%20drivers%2C%20these%20decisions%20can%20be%20made%20with%20collective%2C%20data-driven%0Apolicies%2C%20developed%20by%20machine%20learning%20algorithms.%20Reinforcement%20learning%20%28RL%29%0Acan%20facilitate%20the%20development%20of%20such%20collective%20routing%20strategies%2C%20yet%0Astandardized%20and%20realistic%20benchmarks%20are%20missing.%20To%20that%20end%2C%20we%20present%0A%5Cour%7B%7D%3A%20Urban%20Routing%20Benchmark%20for%20RL-equipped%20Connected%20Autonomous%20Vehicles.%0A%5Cour%7B%7D%20is%20a%20comprehensive%20benchmarking%20environment%20that%20unifies%20evaluation%0Aacross%2029%20real-world%20traffic%20networks%20paired%20with%20realistic%20demand%20patterns.%0A%5Cour%7B%7D%20comes%20with%20a%20catalog%20of%20predefined%20tasks%2C%20four%20state-of-the-art%0Amulti-agent%20RL%20%28MARL%29%20algorithm%20implementations%2C%20three%20baseline%20methods%2C%0Adomain-specific%20performance%20metrics%2C%20and%20a%20modular%20configuration%20scheme.%20Our%0Aresults%20suggest%20that%2C%20despite%20the%20lengthy%20and%20costly%20training%2C%20state-of-the-art%0AMARL%20algorithms%20rarely%20outperformed%20humans.%20Experimental%20results%20reported%20in%0Athis%20paper%20initiate%20the%20first%20leaderboard%20for%20MARL%20in%20large-scale%20urban%20routing%0Aoptimization%20and%20reveal%20that%20current%20approaches%20struggle%20to%20scale%2C%20emphasizing%0Athe%20urgent%20need%20for%20advancements%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DURB%2520--%2520Urban%2520Routing%2520Benchmark%2520for%2520RL-equipped%2520Connected%2520Autonomous%250A%2520%2520Vehicles%26entry.906535625%3DAhmet%2520Onur%2520Akman%2520and%2520Anastasia%2520Psarou%2520and%2520Micha%25C5%2582%2520Hoffmann%2520and%2520%25C5%2581ukasz%2520Gorczyca%2520and%2520%25C5%2581ukasz%2520Kowalski%2520and%2520Pawe%25C5%2582%2520Gora%2520and%2520Grzegorz%2520Jamr%25C3%25B3z%2520and%2520Rafa%25C5%2582%2520Kucharski%26entry.1292438233%3D%2520%2520Connected%2520Autonomous%2520Vehicles%2520%2528CAVs%2529%2520promise%2520to%2520reduce%2520congestion%2520in%2520future%250Aurban%2520networks%252C%2520potentially%2520by%2520optimizing%2520their%2520routing%2520decisions.%2520Unlike%2520for%250Ahuman%2520drivers%252C%2520these%2520decisions%2520can%2520be%2520made%2520with%2520collective%252C%2520data-driven%250Apolicies%252C%2520developed%2520by%2520machine%2520learning%2520algorithms.%2520Reinforcement%2520learning%2520%2528RL%2529%250Acan%2520facilitate%2520the%2520development%2520of%2520such%2520collective%2520routing%2520strategies%252C%2520yet%250Astandardized%2520and%2520realistic%2520benchmarks%2520are%2520missing.%2520To%2520that%2520end%252C%2520we%2520present%250A%255Cour%257B%257D%253A%2520Urban%2520Routing%2520Benchmark%2520for%2520RL-equipped%2520Connected%2520Autonomous%2520Vehicles.%250A%255Cour%257B%257D%2520is%2520a%2520comprehensive%2520benchmarking%2520environment%2520that%2520unifies%2520evaluation%250Aacross%252029%2520real-world%2520traffic%2520networks%2520paired%2520with%2520realistic%2520demand%2520patterns.%250A%255Cour%257B%257D%2520comes%2520with%2520a%2520catalog%2520of%2520predefined%2520tasks%252C%2520four%2520state-of-the-art%250Amulti-agent%2520RL%2520%2528MARL%2529%2520algorithm%2520implementations%252C%2520three%2520baseline%2520methods%252C%250Adomain-specific%2520performance%2520metrics%252C%2520and%2520a%2520modular%2520configuration%2520scheme.%2520Our%250Aresults%2520suggest%2520that%252C%2520despite%2520the%2520lengthy%2520and%2520costly%2520training%252C%2520state-of-the-art%250AMARL%2520algorithms%2520rarely%2520outperformed%2520humans.%2520Experimental%2520results%2520reported%2520in%250Athis%2520paper%2520initiate%2520the%2520first%2520leaderboard%2520for%2520MARL%2520in%2520large-scale%2520urban%2520routing%250Aoptimization%2520and%2520reveal%2520that%2520current%2520approaches%2520struggle%2520to%2520scale%252C%2520emphasizing%250Athe%2520urgent%2520need%2520for%2520advancements%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=URB%20--%20Urban%20Routing%20Benchmark%20for%20RL-equipped%20Connected%20Autonomous%0A%20%20Vehicles&entry.906535625=Ahmet%20Onur%20Akman%20and%20Anastasia%20Psarou%20and%20Micha%C5%82%20Hoffmann%20and%20%C5%81ukasz%20Gorczyca%20and%20%C5%81ukasz%20Kowalski%20and%20Pawe%C5%82%20Gora%20and%20Grzegorz%20Jamr%C3%B3z%20and%20Rafa%C5%82%20Kucharski&entry.1292438233=%20%20Connected%20Autonomous%20Vehicles%20%28CAVs%29%20promise%20to%20reduce%20congestion%20in%20future%0Aurban%20networks%2C%20potentially%20by%20optimizing%20their%20routing%20decisions.%20Unlike%20for%0Ahuman%20drivers%2C%20these%20decisions%20can%20be%20made%20with%20collective%2C%20data-driven%0Apolicies%2C%20developed%20by%20machine%20learning%20algorithms.%20Reinforcement%20learning%20%28RL%29%0Acan%20facilitate%20the%20development%20of%20such%20collective%20routing%20strategies%2C%20yet%0Astandardized%20and%20realistic%20benchmarks%20are%20missing.%20To%20that%20end%2C%20we%20present%0A%5Cour%7B%7D%3A%20Urban%20Routing%20Benchmark%20for%20RL-equipped%20Connected%20Autonomous%20Vehicles.%0A%5Cour%7B%7D%20is%20a%20comprehensive%20benchmarking%20environment%20that%20unifies%20evaluation%0Aacross%2029%20real-world%20traffic%20networks%20paired%20with%20realistic%20demand%20patterns.%0A%5Cour%7B%7D%20comes%20with%20a%20catalog%20of%20predefined%20tasks%2C%20four%20state-of-the-art%0Amulti-agent%20RL%20%28MARL%29%20algorithm%20implementations%2C%20three%20baseline%20methods%2C%0Adomain-specific%20performance%20metrics%2C%20and%20a%20modular%20configuration%20scheme.%20Our%0Aresults%20suggest%20that%2C%20despite%20the%20lengthy%20and%20costly%20training%2C%20state-of-the-art%0AMARL%20algorithms%20rarely%20outperformed%20humans.%20Experimental%20results%20reported%20in%0Athis%20paper%20initiate%20the%20first%20leaderboard%20for%20MARL%20in%20large-scale%20urban%20routing%0Aoptimization%20and%20reveal%20that%20current%20approaches%20struggle%20to%20scale%2C%20emphasizing%0Athe%20urgent%20need%20for%20advancements%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17734v1&entry.124074799=Read"},
{"title": "Joker: Joint Optimization Framework for Lightweight Kernel Machines", "author": "Junhong Zhang and Zhihui Lai", "abstract": "  Kernel methods are powerful tools for nonlinear learning with\nwell-established theory. The scalability issue has been their long-standing\nchallenge. Despite the existing success, there are two limitations in\nlarge-scale kernel methods: (i) The memory overhead is too high for users to\nafford; (ii) existing efforts mainly focus on kernel ridge regression (KRR),\nwhile other models lack study. In this paper, we propose Joker, a joint\noptimization framework for diverse kernel models, including KRR, logistic\nregression, and support vector machines. We design a dual block coordinate\ndescent method with trust region (DBCD-TR) and adopt kernel approximation with\nrandomized features, leading to low memory costs and high efficiency in\nlarge-scale learning. Experiments show that Joker saves up to 90\\% memory but\nachieves comparable training time and performance (or even better) than the\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2505.17765v1", "date": "2025-05-23", "relevancy": 1.426, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4907}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4617}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joker%3A%20Joint%20Optimization%20Framework%20for%20Lightweight%20Kernel%20Machines&body=Title%3A%20Joker%3A%20Joint%20Optimization%20Framework%20for%20Lightweight%20Kernel%20Machines%0AAuthor%3A%20Junhong%20Zhang%20and%20Zhihui%20Lai%0AAbstract%3A%20%20%20Kernel%20methods%20are%20powerful%20tools%20for%20nonlinear%20learning%20with%0Awell-established%20theory.%20The%20scalability%20issue%20has%20been%20their%20long-standing%0Achallenge.%20Despite%20the%20existing%20success%2C%20there%20are%20two%20limitations%20in%0Alarge-scale%20kernel%20methods%3A%20%28i%29%20The%20memory%20overhead%20is%20too%20high%20for%20users%20to%0Aafford%3B%20%28ii%29%20existing%20efforts%20mainly%20focus%20on%20kernel%20ridge%20regression%20%28KRR%29%2C%0Awhile%20other%20models%20lack%20study.%20In%20this%20paper%2C%20we%20propose%20Joker%2C%20a%20joint%0Aoptimization%20framework%20for%20diverse%20kernel%20models%2C%20including%20KRR%2C%20logistic%0Aregression%2C%20and%20support%20vector%20machines.%20We%20design%20a%20dual%20block%20coordinate%0Adescent%20method%20with%20trust%20region%20%28DBCD-TR%29%20and%20adopt%20kernel%20approximation%20with%0Arandomized%20features%2C%20leading%20to%20low%20memory%20costs%20and%20high%20efficiency%20in%0Alarge-scale%20learning.%20Experiments%20show%20that%20Joker%20saves%20up%20to%2090%5C%25%20memory%20but%0Aachieves%20comparable%20training%20time%20and%20performance%20%28or%20even%20better%29%20than%20the%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoker%253A%2520Joint%2520Optimization%2520Framework%2520for%2520Lightweight%2520Kernel%2520Machines%26entry.906535625%3DJunhong%2520Zhang%2520and%2520Zhihui%2520Lai%26entry.1292438233%3D%2520%2520Kernel%2520methods%2520are%2520powerful%2520tools%2520for%2520nonlinear%2520learning%2520with%250Awell-established%2520theory.%2520The%2520scalability%2520issue%2520has%2520been%2520their%2520long-standing%250Achallenge.%2520Despite%2520the%2520existing%2520success%252C%2520there%2520are%2520two%2520limitations%2520in%250Alarge-scale%2520kernel%2520methods%253A%2520%2528i%2529%2520The%2520memory%2520overhead%2520is%2520too%2520high%2520for%2520users%2520to%250Aafford%253B%2520%2528ii%2529%2520existing%2520efforts%2520mainly%2520focus%2520on%2520kernel%2520ridge%2520regression%2520%2528KRR%2529%252C%250Awhile%2520other%2520models%2520lack%2520study.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Joker%252C%2520a%2520joint%250Aoptimization%2520framework%2520for%2520diverse%2520kernel%2520models%252C%2520including%2520KRR%252C%2520logistic%250Aregression%252C%2520and%2520support%2520vector%2520machines.%2520We%2520design%2520a%2520dual%2520block%2520coordinate%250Adescent%2520method%2520with%2520trust%2520region%2520%2528DBCD-TR%2529%2520and%2520adopt%2520kernel%2520approximation%2520with%250Arandomized%2520features%252C%2520leading%2520to%2520low%2520memory%2520costs%2520and%2520high%2520efficiency%2520in%250Alarge-scale%2520learning.%2520Experiments%2520show%2520that%2520Joker%2520saves%2520up%2520to%252090%255C%2525%2520memory%2520but%250Aachieves%2520comparable%2520training%2520time%2520and%2520performance%2520%2528or%2520even%2520better%2529%2520than%2520the%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joker%3A%20Joint%20Optimization%20Framework%20for%20Lightweight%20Kernel%20Machines&entry.906535625=Junhong%20Zhang%20and%20Zhihui%20Lai&entry.1292438233=%20%20Kernel%20methods%20are%20powerful%20tools%20for%20nonlinear%20learning%20with%0Awell-established%20theory.%20The%20scalability%20issue%20has%20been%20their%20long-standing%0Achallenge.%20Despite%20the%20existing%20success%2C%20there%20are%20two%20limitations%20in%0Alarge-scale%20kernel%20methods%3A%20%28i%29%20The%20memory%20overhead%20is%20too%20high%20for%20users%20to%0Aafford%3B%20%28ii%29%20existing%20efforts%20mainly%20focus%20on%20kernel%20ridge%20regression%20%28KRR%29%2C%0Awhile%20other%20models%20lack%20study.%20In%20this%20paper%2C%20we%20propose%20Joker%2C%20a%20joint%0Aoptimization%20framework%20for%20diverse%20kernel%20models%2C%20including%20KRR%2C%20logistic%0Aregression%2C%20and%20support%20vector%20machines.%20We%20design%20a%20dual%20block%20coordinate%0Adescent%20method%20with%20trust%20region%20%28DBCD-TR%29%20and%20adopt%20kernel%20approximation%20with%0Arandomized%20features%2C%20leading%20to%20low%20memory%20costs%20and%20high%20efficiency%20in%0Alarge-scale%20learning.%20Experiments%20show%20that%20Joker%20saves%20up%20to%2090%5C%25%20memory%20but%0Aachieves%20comparable%20training%20time%20and%20performance%20%28or%20even%20better%29%20than%20the%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17765v1&entry.124074799=Read"},
{"title": "WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in\n  Post-Training", "author": "Benjamin Feuer and Chinmay Hegde", "abstract": "  Language model (LLM) post-training, from DPO to distillation, can refine\nbehaviors and unlock new skills, but the open science supporting these\npost-training techniques is still in its infancy. One limiting factor has been\nthe difficulty of conducting large-scale comparative analyses of synthetic data\ngenerating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,\nthe largest public chat dataset to date. We extend the existing WildChat\ndataset to include responses not only from GPT, but from over 50 different\nopen-weight models, ranging in size from 0.5B to 104B parameters. We conduct an\nextensive comparative analysis and demonstrate the potential of this dataset by\ncreating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3\nSFT mixture from Allen AI with only 40% as many samples. Our dataset, samples\nand code are available at https://github.com/penfever/wildchat-50m.\n", "link": "http://arxiv.org/abs/2501.18511v2", "date": "2025-05-23", "relevancy": 2.0743, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5341}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5161}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WILDCHAT-50M%3A%20A%20Deep%20Dive%20Into%20the%20Role%20of%20Synthetic%20Data%20in%0A%20%20Post-Training&body=Title%3A%20WILDCHAT-50M%3A%20A%20Deep%20Dive%20Into%20the%20Role%20of%20Synthetic%20Data%20in%0A%20%20Post-Training%0AAuthor%3A%20Benjamin%20Feuer%20and%20Chinmay%20Hegde%0AAbstract%3A%20%20%20Language%20model%20%28LLM%29%20post-training%2C%20from%20DPO%20to%20distillation%2C%20can%20refine%0Abehaviors%20and%20unlock%20new%20skills%2C%20but%20the%20open%20science%20supporting%20these%0Apost-training%20techniques%20is%20still%20in%20its%20infancy.%20One%20limiting%20factor%20has%20been%0Athe%20difficulty%20of%20conducting%20large-scale%20comparative%20analyses%20of%20synthetic%20data%0Agenerating%20models%20and%20LLM%20judges.%20To%20close%20this%20gap%2C%20we%20introduce%20WILDCHAT-50M%2C%0Athe%20largest%20public%20chat%20dataset%20to%20date.%20We%20extend%20the%20existing%20WildChat%0Adataset%20to%20include%20responses%20not%20only%20from%20GPT%2C%20but%20from%20over%2050%20different%0Aopen-weight%20models%2C%20ranging%20in%20size%20from%200.5B%20to%20104B%20parameters.%20We%20conduct%20an%0Aextensive%20comparative%20analysis%20and%20demonstrate%20the%20potential%20of%20this%20dataset%20by%0Acreating%20RE-WILD%2C%20our%20own%20public%20SFT%20mix%2C%20which%20outperforms%20the%20recent%20Tulu-3%0ASFT%20mixture%20from%20Allen%20AI%20with%20only%2040%25%20as%20many%20samples.%20Our%20dataset%2C%20samples%0Aand%20code%20are%20available%20at%20https%3A//github.com/penfever/wildchat-50m.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18511v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWILDCHAT-50M%253A%2520A%2520Deep%2520Dive%2520Into%2520the%2520Role%2520of%2520Synthetic%2520Data%2520in%250A%2520%2520Post-Training%26entry.906535625%3DBenjamin%2520Feuer%2520and%2520Chinmay%2520Hegde%26entry.1292438233%3D%2520%2520Language%2520model%2520%2528LLM%2529%2520post-training%252C%2520from%2520DPO%2520to%2520distillation%252C%2520can%2520refine%250Abehaviors%2520and%2520unlock%2520new%2520skills%252C%2520but%2520the%2520open%2520science%2520supporting%2520these%250Apost-training%2520techniques%2520is%2520still%2520in%2520its%2520infancy.%2520One%2520limiting%2520factor%2520has%2520been%250Athe%2520difficulty%2520of%2520conducting%2520large-scale%2520comparative%2520analyses%2520of%2520synthetic%2520data%250Agenerating%2520models%2520and%2520LLM%2520judges.%2520To%2520close%2520this%2520gap%252C%2520we%2520introduce%2520WILDCHAT-50M%252C%250Athe%2520largest%2520public%2520chat%2520dataset%2520to%2520date.%2520We%2520extend%2520the%2520existing%2520WildChat%250Adataset%2520to%2520include%2520responses%2520not%2520only%2520from%2520GPT%252C%2520but%2520from%2520over%252050%2520different%250Aopen-weight%2520models%252C%2520ranging%2520in%2520size%2520from%25200.5B%2520to%2520104B%2520parameters.%2520We%2520conduct%2520an%250Aextensive%2520comparative%2520analysis%2520and%2520demonstrate%2520the%2520potential%2520of%2520this%2520dataset%2520by%250Acreating%2520RE-WILD%252C%2520our%2520own%2520public%2520SFT%2520mix%252C%2520which%2520outperforms%2520the%2520recent%2520Tulu-3%250ASFT%2520mixture%2520from%2520Allen%2520AI%2520with%2520only%252040%2525%2520as%2520many%2520samples.%2520Our%2520dataset%252C%2520samples%250Aand%2520code%2520are%2520available%2520at%2520https%253A//github.com/penfever/wildchat-50m.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18511v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WILDCHAT-50M%3A%20A%20Deep%20Dive%20Into%20the%20Role%20of%20Synthetic%20Data%20in%0A%20%20Post-Training&entry.906535625=Benjamin%20Feuer%20and%20Chinmay%20Hegde&entry.1292438233=%20%20Language%20model%20%28LLM%29%20post-training%2C%20from%20DPO%20to%20distillation%2C%20can%20refine%0Abehaviors%20and%20unlock%20new%20skills%2C%20but%20the%20open%20science%20supporting%20these%0Apost-training%20techniques%20is%20still%20in%20its%20infancy.%20One%20limiting%20factor%20has%20been%0Athe%20difficulty%20of%20conducting%20large-scale%20comparative%20analyses%20of%20synthetic%20data%0Agenerating%20models%20and%20LLM%20judges.%20To%20close%20this%20gap%2C%20we%20introduce%20WILDCHAT-50M%2C%0Athe%20largest%20public%20chat%20dataset%20to%20date.%20We%20extend%20the%20existing%20WildChat%0Adataset%20to%20include%20responses%20not%20only%20from%20GPT%2C%20but%20from%20over%2050%20different%0Aopen-weight%20models%2C%20ranging%20in%20size%20from%200.5B%20to%20104B%20parameters.%20We%20conduct%20an%0Aextensive%20comparative%20analysis%20and%20demonstrate%20the%20potential%20of%20this%20dataset%20by%0Acreating%20RE-WILD%2C%20our%20own%20public%20SFT%20mix%2C%20which%20outperforms%20the%20recent%20Tulu-3%0ASFT%20mixture%20from%20Allen%20AI%20with%20only%2040%25%20as%20many%20samples.%20Our%20dataset%2C%20samples%0Aand%20code%20are%20available%20at%20https%3A//github.com/penfever/wildchat-50m.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18511v2&entry.124074799=Read"},
{"title": "Reward Model Generalization for Compute-Aware Test-Time Reasoning", "author": "Zeen Song and Wenwen Qiang and Siyu Zhao and Changwen Zheng and Gang Hua", "abstract": "  External test-time reasoning enhances large language models (LLMs) by\ndecoupling generation and selection. At inference time, the model generates\nmultiple reasoning paths, and an auxiliary process reward model (PRM) is used\nto score and select the best one. A central challenge in this setting is\ntest-time compute optimality (TCO), i.e., how to maximize answer accuracy under\na fixed inference budget. In this work, we establish a theoretical framework to\nanalyze how the generalization error of the PRM affects compute efficiency and\nreasoning performance. Leveraging PAC-Bayes theory, we derive generalization\nbounds and show that a lower generalization error of PRM leads to fewer samples\nrequired to find correct answers. Motivated by this analysis, we propose\nCompute-Aware Tree Search (CATS), an actor-critic framework that dynamically\ncontrols search behavior. The actor outputs sampling hyperparameters based on\nreward distributions and sparsity statistics, while the critic estimates their\nutility to guide budget allocation. Experiments on the MATH and AIME benchmarks\nwith various LLMs and PRMs demonstrate that CATS consistently outperforms other\nexternal TTS methods, validating our theoretical predictions.\n", "link": "http://arxiv.org/abs/2505.18065v1", "date": "2025-05-23", "relevancy": 1.8625, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4642}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reward%20Model%20Generalization%20for%20Compute-Aware%20Test-Time%20Reasoning&body=Title%3A%20Reward%20Model%20Generalization%20for%20Compute-Aware%20Test-Time%20Reasoning%0AAuthor%3A%20Zeen%20Song%20and%20Wenwen%20Qiang%20and%20Siyu%20Zhao%20and%20Changwen%20Zheng%20and%20Gang%20Hua%0AAbstract%3A%20%20%20External%20test-time%20reasoning%20enhances%20large%20language%20models%20%28LLMs%29%20by%0Adecoupling%20generation%20and%20selection.%20At%20inference%20time%2C%20the%20model%20generates%0Amultiple%20reasoning%20paths%2C%20and%20an%20auxiliary%20process%20reward%20model%20%28PRM%29%20is%20used%0Ato%20score%20and%20select%20the%20best%20one.%20A%20central%20challenge%20in%20this%20setting%20is%0Atest-time%20compute%20optimality%20%28TCO%29%2C%20i.e.%2C%20how%20to%20maximize%20answer%20accuracy%20under%0Aa%20fixed%20inference%20budget.%20In%20this%20work%2C%20we%20establish%20a%20theoretical%20framework%20to%0Aanalyze%20how%20the%20generalization%20error%20of%20the%20PRM%20affects%20compute%20efficiency%20and%0Areasoning%20performance.%20Leveraging%20PAC-Bayes%20theory%2C%20we%20derive%20generalization%0Abounds%20and%20show%20that%20a%20lower%20generalization%20error%20of%20PRM%20leads%20to%20fewer%20samples%0Arequired%20to%20find%20correct%20answers.%20Motivated%20by%20this%20analysis%2C%20we%20propose%0ACompute-Aware%20Tree%20Search%20%28CATS%29%2C%20an%20actor-critic%20framework%20that%20dynamically%0Acontrols%20search%20behavior.%20The%20actor%20outputs%20sampling%20hyperparameters%20based%20on%0Areward%20distributions%20and%20sparsity%20statistics%2C%20while%20the%20critic%20estimates%20their%0Autility%20to%20guide%20budget%20allocation.%20Experiments%20on%20the%20MATH%20and%20AIME%20benchmarks%0Awith%20various%20LLMs%20and%20PRMs%20demonstrate%20that%20CATS%20consistently%20outperforms%20other%0Aexternal%20TTS%20methods%2C%20validating%20our%20theoretical%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReward%2520Model%2520Generalization%2520for%2520Compute-Aware%2520Test-Time%2520Reasoning%26entry.906535625%3DZeen%2520Song%2520and%2520Wenwen%2520Qiang%2520and%2520Siyu%2520Zhao%2520and%2520Changwen%2520Zheng%2520and%2520Gang%2520Hua%26entry.1292438233%3D%2520%2520External%2520test-time%2520reasoning%2520enhances%2520large%2520language%2520models%2520%2528LLMs%2529%2520by%250Adecoupling%2520generation%2520and%2520selection.%2520At%2520inference%2520time%252C%2520the%2520model%2520generates%250Amultiple%2520reasoning%2520paths%252C%2520and%2520an%2520auxiliary%2520process%2520reward%2520model%2520%2528PRM%2529%2520is%2520used%250Ato%2520score%2520and%2520select%2520the%2520best%2520one.%2520A%2520central%2520challenge%2520in%2520this%2520setting%2520is%250Atest-time%2520compute%2520optimality%2520%2528TCO%2529%252C%2520i.e.%252C%2520how%2520to%2520maximize%2520answer%2520accuracy%2520under%250Aa%2520fixed%2520inference%2520budget.%2520In%2520this%2520work%252C%2520we%2520establish%2520a%2520theoretical%2520framework%2520to%250Aanalyze%2520how%2520the%2520generalization%2520error%2520of%2520the%2520PRM%2520affects%2520compute%2520efficiency%2520and%250Areasoning%2520performance.%2520Leveraging%2520PAC-Bayes%2520theory%252C%2520we%2520derive%2520generalization%250Abounds%2520and%2520show%2520that%2520a%2520lower%2520generalization%2520error%2520of%2520PRM%2520leads%2520to%2520fewer%2520samples%250Arequired%2520to%2520find%2520correct%2520answers.%2520Motivated%2520by%2520this%2520analysis%252C%2520we%2520propose%250ACompute-Aware%2520Tree%2520Search%2520%2528CATS%2529%252C%2520an%2520actor-critic%2520framework%2520that%2520dynamically%250Acontrols%2520search%2520behavior.%2520The%2520actor%2520outputs%2520sampling%2520hyperparameters%2520based%2520on%250Areward%2520distributions%2520and%2520sparsity%2520statistics%252C%2520while%2520the%2520critic%2520estimates%2520their%250Autility%2520to%2520guide%2520budget%2520allocation.%2520Experiments%2520on%2520the%2520MATH%2520and%2520AIME%2520benchmarks%250Awith%2520various%2520LLMs%2520and%2520PRMs%2520demonstrate%2520that%2520CATS%2520consistently%2520outperforms%2520other%250Aexternal%2520TTS%2520methods%252C%2520validating%2520our%2520theoretical%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward%20Model%20Generalization%20for%20Compute-Aware%20Test-Time%20Reasoning&entry.906535625=Zeen%20Song%20and%20Wenwen%20Qiang%20and%20Siyu%20Zhao%20and%20Changwen%20Zheng%20and%20Gang%20Hua&entry.1292438233=%20%20External%20test-time%20reasoning%20enhances%20large%20language%20models%20%28LLMs%29%20by%0Adecoupling%20generation%20and%20selection.%20At%20inference%20time%2C%20the%20model%20generates%0Amultiple%20reasoning%20paths%2C%20and%20an%20auxiliary%20process%20reward%20model%20%28PRM%29%20is%20used%0Ato%20score%20and%20select%20the%20best%20one.%20A%20central%20challenge%20in%20this%20setting%20is%0Atest-time%20compute%20optimality%20%28TCO%29%2C%20i.e.%2C%20how%20to%20maximize%20answer%20accuracy%20under%0Aa%20fixed%20inference%20budget.%20In%20this%20work%2C%20we%20establish%20a%20theoretical%20framework%20to%0Aanalyze%20how%20the%20generalization%20error%20of%20the%20PRM%20affects%20compute%20efficiency%20and%0Areasoning%20performance.%20Leveraging%20PAC-Bayes%20theory%2C%20we%20derive%20generalization%0Abounds%20and%20show%20that%20a%20lower%20generalization%20error%20of%20PRM%20leads%20to%20fewer%20samples%0Arequired%20to%20find%20correct%20answers.%20Motivated%20by%20this%20analysis%2C%20we%20propose%0ACompute-Aware%20Tree%20Search%20%28CATS%29%2C%20an%20actor-critic%20framework%20that%20dynamically%0Acontrols%20search%20behavior.%20The%20actor%20outputs%20sampling%20hyperparameters%20based%20on%0Areward%20distributions%20and%20sparsity%20statistics%2C%20while%20the%20critic%20estimates%20their%0Autility%20to%20guide%20budget%20allocation.%20Experiments%20on%20the%20MATH%20and%20AIME%20benchmarks%0Awith%20various%20LLMs%20and%20PRMs%20demonstrate%20that%20CATS%20consistently%20outperforms%20other%0Aexternal%20TTS%20methods%2C%20validating%20our%20theoretical%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18065v1&entry.124074799=Read"},
{"title": "Emergence of Hebbian Dynamics in Regularized Non-Local Learners", "author": "David Koplow and Tomaso Poggio and Liu Ziyin", "abstract": "  Stochastic Gradient Descent (SGD) has emerged as a remarkably effective\nlearning algorithm, underpinning nearly all state-of-the-art machine learning\nmodels, from large language models to autonomous vehicles. Despite its\npractical success, SGD appears fundamentally distinct from biological learning\nmechanisms. It is widely believed that the biological brain can not implement\ngradient descent because it is nonlocal, and we have found little (if any)\nexperimental evidence for it. In contrast, the brain is widely thought to learn\nvia local Hebbian learning principles, which have been seen as incompatible\nwith gradient descent. In this paper, we establish a theoretical and empirical\nconnection between the learning signals of neural networks trained using SGD\nwith weight decay and those trained with Hebbian learning near convergence. We\nshow that SGD with regularization can appear to learn according to a Hebbian\nrule, and SGD with injected noise according to an anti-Hebbian rule. We also\nprovide empirical evidence that Hebbian learning properties can emerge in a\nnetwork with weight decay from virtually any learning rule--even random ones.\nThese results may bridge a long-standing gap between artificial and biological\nlearning, revealing Hebbian properties as an epiphenomenon of deeper\noptimization principles and cautioning against interpreting their presence in\nneural data as evidence against more complex hetero-synaptic mechanisms.\n", "link": "http://arxiv.org/abs/2505.18069v1", "date": "2025-05-23", "relevancy": 2.0493, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5491}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5137}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergence%20of%20Hebbian%20Dynamics%20in%20Regularized%20Non-Local%20Learners&body=Title%3A%20Emergence%20of%20Hebbian%20Dynamics%20in%20Regularized%20Non-Local%20Learners%0AAuthor%3A%20David%20Koplow%20and%20Tomaso%20Poggio%20and%20Liu%20Ziyin%0AAbstract%3A%20%20%20Stochastic%20Gradient%20Descent%20%28SGD%29%20has%20emerged%20as%20a%20remarkably%20effective%0Alearning%20algorithm%2C%20underpinning%20nearly%20all%20state-of-the-art%20machine%20learning%0Amodels%2C%20from%20large%20language%20models%20to%20autonomous%20vehicles.%20Despite%20its%0Apractical%20success%2C%20SGD%20appears%20fundamentally%20distinct%20from%20biological%20learning%0Amechanisms.%20It%20is%20widely%20believed%20that%20the%20biological%20brain%20can%20not%20implement%0Agradient%20descent%20because%20it%20is%20nonlocal%2C%20and%20we%20have%20found%20little%20%28if%20any%29%0Aexperimental%20evidence%20for%20it.%20In%20contrast%2C%20the%20brain%20is%20widely%20thought%20to%20learn%0Avia%20local%20Hebbian%20learning%20principles%2C%20which%20have%20been%20seen%20as%20incompatible%0Awith%20gradient%20descent.%20In%20this%20paper%2C%20we%20establish%20a%20theoretical%20and%20empirical%0Aconnection%20between%20the%20learning%20signals%20of%20neural%20networks%20trained%20using%20SGD%0Awith%20weight%20decay%20and%20those%20trained%20with%20Hebbian%20learning%20near%20convergence.%20We%0Ashow%20that%20SGD%20with%20regularization%20can%20appear%20to%20learn%20according%20to%20a%20Hebbian%0Arule%2C%20and%20SGD%20with%20injected%20noise%20according%20to%20an%20anti-Hebbian%20rule.%20We%20also%0Aprovide%20empirical%20evidence%20that%20Hebbian%20learning%20properties%20can%20emerge%20in%20a%0Anetwork%20with%20weight%20decay%20from%20virtually%20any%20learning%20rule--even%20random%20ones.%0AThese%20results%20may%20bridge%20a%20long-standing%20gap%20between%20artificial%20and%20biological%0Alearning%2C%20revealing%20Hebbian%20properties%20as%20an%20epiphenomenon%20of%20deeper%0Aoptimization%20principles%20and%20cautioning%20against%20interpreting%20their%20presence%20in%0Aneural%20data%20as%20evidence%20against%20more%20complex%20hetero-synaptic%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18069v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergence%2520of%2520Hebbian%2520Dynamics%2520in%2520Regularized%2520Non-Local%2520Learners%26entry.906535625%3DDavid%2520Koplow%2520and%2520Tomaso%2520Poggio%2520and%2520Liu%2520Ziyin%26entry.1292438233%3D%2520%2520Stochastic%2520Gradient%2520Descent%2520%2528SGD%2529%2520has%2520emerged%2520as%2520a%2520remarkably%2520effective%250Alearning%2520algorithm%252C%2520underpinning%2520nearly%2520all%2520state-of-the-art%2520machine%2520learning%250Amodels%252C%2520from%2520large%2520language%2520models%2520to%2520autonomous%2520vehicles.%2520Despite%2520its%250Apractical%2520success%252C%2520SGD%2520appears%2520fundamentally%2520distinct%2520from%2520biological%2520learning%250Amechanisms.%2520It%2520is%2520widely%2520believed%2520that%2520the%2520biological%2520brain%2520can%2520not%2520implement%250Agradient%2520descent%2520because%2520it%2520is%2520nonlocal%252C%2520and%2520we%2520have%2520found%2520little%2520%2528if%2520any%2529%250Aexperimental%2520evidence%2520for%2520it.%2520In%2520contrast%252C%2520the%2520brain%2520is%2520widely%2520thought%2520to%2520learn%250Avia%2520local%2520Hebbian%2520learning%2520principles%252C%2520which%2520have%2520been%2520seen%2520as%2520incompatible%250Awith%2520gradient%2520descent.%2520In%2520this%2520paper%252C%2520we%2520establish%2520a%2520theoretical%2520and%2520empirical%250Aconnection%2520between%2520the%2520learning%2520signals%2520of%2520neural%2520networks%2520trained%2520using%2520SGD%250Awith%2520weight%2520decay%2520and%2520those%2520trained%2520with%2520Hebbian%2520learning%2520near%2520convergence.%2520We%250Ashow%2520that%2520SGD%2520with%2520regularization%2520can%2520appear%2520to%2520learn%2520according%2520to%2520a%2520Hebbian%250Arule%252C%2520and%2520SGD%2520with%2520injected%2520noise%2520according%2520to%2520an%2520anti-Hebbian%2520rule.%2520We%2520also%250Aprovide%2520empirical%2520evidence%2520that%2520Hebbian%2520learning%2520properties%2520can%2520emerge%2520in%2520a%250Anetwork%2520with%2520weight%2520decay%2520from%2520virtually%2520any%2520learning%2520rule--even%2520random%2520ones.%250AThese%2520results%2520may%2520bridge%2520a%2520long-standing%2520gap%2520between%2520artificial%2520and%2520biological%250Alearning%252C%2520revealing%2520Hebbian%2520properties%2520as%2520an%2520epiphenomenon%2520of%2520deeper%250Aoptimization%2520principles%2520and%2520cautioning%2520against%2520interpreting%2520their%2520presence%2520in%250Aneural%2520data%2520as%2520evidence%2520against%2520more%2520complex%2520hetero-synaptic%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18069v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergence%20of%20Hebbian%20Dynamics%20in%20Regularized%20Non-Local%20Learners&entry.906535625=David%20Koplow%20and%20Tomaso%20Poggio%20and%20Liu%20Ziyin&entry.1292438233=%20%20Stochastic%20Gradient%20Descent%20%28SGD%29%20has%20emerged%20as%20a%20remarkably%20effective%0Alearning%20algorithm%2C%20underpinning%20nearly%20all%20state-of-the-art%20machine%20learning%0Amodels%2C%20from%20large%20language%20models%20to%20autonomous%20vehicles.%20Despite%20its%0Apractical%20success%2C%20SGD%20appears%20fundamentally%20distinct%20from%20biological%20learning%0Amechanisms.%20It%20is%20widely%20believed%20that%20the%20biological%20brain%20can%20not%20implement%0Agradient%20descent%20because%20it%20is%20nonlocal%2C%20and%20we%20have%20found%20little%20%28if%20any%29%0Aexperimental%20evidence%20for%20it.%20In%20contrast%2C%20the%20brain%20is%20widely%20thought%20to%20learn%0Avia%20local%20Hebbian%20learning%20principles%2C%20which%20have%20been%20seen%20as%20incompatible%0Awith%20gradient%20descent.%20In%20this%20paper%2C%20we%20establish%20a%20theoretical%20and%20empirical%0Aconnection%20between%20the%20learning%20signals%20of%20neural%20networks%20trained%20using%20SGD%0Awith%20weight%20decay%20and%20those%20trained%20with%20Hebbian%20learning%20near%20convergence.%20We%0Ashow%20that%20SGD%20with%20regularization%20can%20appear%20to%20learn%20according%20to%20a%20Hebbian%0Arule%2C%20and%20SGD%20with%20injected%20noise%20according%20to%20an%20anti-Hebbian%20rule.%20We%20also%0Aprovide%20empirical%20evidence%20that%20Hebbian%20learning%20properties%20can%20emerge%20in%20a%0Anetwork%20with%20weight%20decay%20from%20virtually%20any%20learning%20rule--even%20random%20ones.%0AThese%20results%20may%20bridge%20a%20long-standing%20gap%20between%20artificial%20and%20biological%0Alearning%2C%20revealing%20Hebbian%20properties%20as%20an%20epiphenomenon%20of%20deeper%0Aoptimization%20principles%20and%20cautioning%20against%20interpreting%20their%20presence%20in%0Aneural%20data%20as%20evidence%20against%20more%20complex%20hetero-synaptic%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18069v1&entry.124074799=Read"},
{"title": "But what is your honest answer? Aiding LLM-judges with honest\n  alternatives using steering vectors", "author": "Leon Eshuijs and Archie Chaudhury and Alan McBeth and Ethan Nguyen", "abstract": "  Recent safety evaluations of Large Language Models (LLMs) show that many\nmodels exhibit dishonest behavior, such as sycophancy. However, most honesty\nbenchmarks focus exclusively on factual knowledge or explicitly harmful\nbehavior and rely on external judges, which are often unable to detect less\nobvious forms of dishonesty. In this work, we introduce a new framework, Judge\nUsing Safety-Steered Alternatives (JUSSA), which utilizes steering vectors\ntrained on a single sample to elicit more honest responses from models, helping\nLLM-judges in the detection of dishonest behavior. To test our framework, we\nintroduce a new manipulation dataset with prompts specifically designed to\nelicit deceptive responses. We find that JUSSA enables LLM judges to better\ndifferentiate between dishonest and benign responses, and helps them identify\nsubtle instances of manipulative behavior.\n", "link": "http://arxiv.org/abs/2505.17760v1", "date": "2025-05-23", "relevancy": 1.4126, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5111}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4623}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20But%20what%20is%20your%20honest%20answer%3F%20Aiding%20LLM-judges%20with%20honest%0A%20%20alternatives%20using%20steering%20vectors&body=Title%3A%20But%20what%20is%20your%20honest%20answer%3F%20Aiding%20LLM-judges%20with%20honest%0A%20%20alternatives%20using%20steering%20vectors%0AAuthor%3A%20Leon%20Eshuijs%20and%20Archie%20Chaudhury%20and%20Alan%20McBeth%20and%20Ethan%20Nguyen%0AAbstract%3A%20%20%20Recent%20safety%20evaluations%20of%20Large%20Language%20Models%20%28LLMs%29%20show%20that%20many%0Amodels%20exhibit%20dishonest%20behavior%2C%20such%20as%20sycophancy.%20However%2C%20most%20honesty%0Abenchmarks%20focus%20exclusively%20on%20factual%20knowledge%20or%20explicitly%20harmful%0Abehavior%20and%20rely%20on%20external%20judges%2C%20which%20are%20often%20unable%20to%20detect%20less%0Aobvious%20forms%20of%20dishonesty.%20In%20this%20work%2C%20we%20introduce%20a%20new%20framework%2C%20Judge%0AUsing%20Safety-Steered%20Alternatives%20%28JUSSA%29%2C%20which%20utilizes%20steering%20vectors%0Atrained%20on%20a%20single%20sample%20to%20elicit%20more%20honest%20responses%20from%20models%2C%20helping%0ALLM-judges%20in%20the%20detection%20of%20dishonest%20behavior.%20To%20test%20our%20framework%2C%20we%0Aintroduce%20a%20new%20manipulation%20dataset%20with%20prompts%20specifically%20designed%20to%0Aelicit%20deceptive%20responses.%20We%20find%20that%20JUSSA%20enables%20LLM%20judges%20to%20better%0Adifferentiate%20between%20dishonest%20and%20benign%20responses%2C%20and%20helps%20them%20identify%0Asubtle%20instances%20of%20manipulative%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBut%2520what%2520is%2520your%2520honest%2520answer%253F%2520Aiding%2520LLM-judges%2520with%2520honest%250A%2520%2520alternatives%2520using%2520steering%2520vectors%26entry.906535625%3DLeon%2520Eshuijs%2520and%2520Archie%2520Chaudhury%2520and%2520Alan%2520McBeth%2520and%2520Ethan%2520Nguyen%26entry.1292438233%3D%2520%2520Recent%2520safety%2520evaluations%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520show%2520that%2520many%250Amodels%2520exhibit%2520dishonest%2520behavior%252C%2520such%2520as%2520sycophancy.%2520However%252C%2520most%2520honesty%250Abenchmarks%2520focus%2520exclusively%2520on%2520factual%2520knowledge%2520or%2520explicitly%2520harmful%250Abehavior%2520and%2520rely%2520on%2520external%2520judges%252C%2520which%2520are%2520often%2520unable%2520to%2520detect%2520less%250Aobvious%2520forms%2520of%2520dishonesty.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%2520framework%252C%2520Judge%250AUsing%2520Safety-Steered%2520Alternatives%2520%2528JUSSA%2529%252C%2520which%2520utilizes%2520steering%2520vectors%250Atrained%2520on%2520a%2520single%2520sample%2520to%2520elicit%2520more%2520honest%2520responses%2520from%2520models%252C%2520helping%250ALLM-judges%2520in%2520the%2520detection%2520of%2520dishonest%2520behavior.%2520To%2520test%2520our%2520framework%252C%2520we%250Aintroduce%2520a%2520new%2520manipulation%2520dataset%2520with%2520prompts%2520specifically%2520designed%2520to%250Aelicit%2520deceptive%2520responses.%2520We%2520find%2520that%2520JUSSA%2520enables%2520LLM%2520judges%2520to%2520better%250Adifferentiate%2520between%2520dishonest%2520and%2520benign%2520responses%252C%2520and%2520helps%2520them%2520identify%250Asubtle%2520instances%2520of%2520manipulative%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=But%20what%20is%20your%20honest%20answer%3F%20Aiding%20LLM-judges%20with%20honest%0A%20%20alternatives%20using%20steering%20vectors&entry.906535625=Leon%20Eshuijs%20and%20Archie%20Chaudhury%20and%20Alan%20McBeth%20and%20Ethan%20Nguyen&entry.1292438233=%20%20Recent%20safety%20evaluations%20of%20Large%20Language%20Models%20%28LLMs%29%20show%20that%20many%0Amodels%20exhibit%20dishonest%20behavior%2C%20such%20as%20sycophancy.%20However%2C%20most%20honesty%0Abenchmarks%20focus%20exclusively%20on%20factual%20knowledge%20or%20explicitly%20harmful%0Abehavior%20and%20rely%20on%20external%20judges%2C%20which%20are%20often%20unable%20to%20detect%20less%0Aobvious%20forms%20of%20dishonesty.%20In%20this%20work%2C%20we%20introduce%20a%20new%20framework%2C%20Judge%0AUsing%20Safety-Steered%20Alternatives%20%28JUSSA%29%2C%20which%20utilizes%20steering%20vectors%0Atrained%20on%20a%20single%20sample%20to%20elicit%20more%20honest%20responses%20from%20models%2C%20helping%0ALLM-judges%20in%20the%20detection%20of%20dishonest%20behavior.%20To%20test%20our%20framework%2C%20we%0Aintroduce%20a%20new%20manipulation%20dataset%20with%20prompts%20specifically%20designed%20to%0Aelicit%20deceptive%20responses.%20We%20find%20that%20JUSSA%20enables%20LLM%20judges%20to%20better%0Adifferentiate%20between%20dishonest%20and%20benign%20responses%2C%20and%20helps%20them%20identify%0Asubtle%20instances%20of%20manipulative%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17760v1&entry.124074799=Read"},
{"title": "RestoreVAR: Visual Autoregressive Generation for All-in-One Image\n  Restoration", "author": "Sudarshan Rajagopalan and Kartik Narayan and Vishal M. Patel", "abstract": "  The use of latent diffusion models (LDMs) such as Stable Diffusion has\nsignificantly improved the perceptual quality of All-in-One image Restoration\n(AiOR) methods, while also enhancing their generalization capabilities.\nHowever, these LDM-based frameworks suffer from slow inference due to their\niterative denoising process, rendering them impractical for time-sensitive\napplications. To address this, we propose RestoreVAR, a novel generative\napproach for AiOR that significantly outperforms LDM-based models in\nrestoration performance while achieving over $\\mathbf{10\\times}$ faster\ninference. RestoreVAR leverages visual autoregressive modeling (VAR), a\nrecently introduced approach which performs scale-space autoregression for\nimage generation. VAR achieves comparable performance to that of\nstate-of-the-art diffusion transformers with drastically reduced computational\ncosts. To optimally exploit these advantages of VAR for AiOR, we propose\narchitectural modifications and improvements, including intricately designed\ncross-attention mechanisms and a latent-space refinement module, tailored for\nthe AiOR task. Extensive experiments show that RestoreVAR achieves\nstate-of-the-art performance among generative AiOR methods, while also\nexhibiting strong generalization capabilities.\n", "link": "http://arxiv.org/abs/2505.18047v1", "date": "2025-05-23", "relevancy": 1.7774, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5935}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5922}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RestoreVAR%3A%20Visual%20Autoregressive%20Generation%20for%20All-in-One%20Image%0A%20%20Restoration&body=Title%3A%20RestoreVAR%3A%20Visual%20Autoregressive%20Generation%20for%20All-in-One%20Image%0A%20%20Restoration%0AAuthor%3A%20Sudarshan%20Rajagopalan%20and%20Kartik%20Narayan%20and%20Vishal%20M.%20Patel%0AAbstract%3A%20%20%20The%20use%20of%20latent%20diffusion%20models%20%28LDMs%29%20such%20as%20Stable%20Diffusion%20has%0Asignificantly%20improved%20the%20perceptual%20quality%20of%20All-in-One%20image%20Restoration%0A%28AiOR%29%20methods%2C%20while%20also%20enhancing%20their%20generalization%20capabilities.%0AHowever%2C%20these%20LDM-based%20frameworks%20suffer%20from%20slow%20inference%20due%20to%20their%0Aiterative%20denoising%20process%2C%20rendering%20them%20impractical%20for%20time-sensitive%0Aapplications.%20To%20address%20this%2C%20we%20propose%20RestoreVAR%2C%20a%20novel%20generative%0Aapproach%20for%20AiOR%20that%20significantly%20outperforms%20LDM-based%20models%20in%0Arestoration%20performance%20while%20achieving%20over%20%24%5Cmathbf%7B10%5Ctimes%7D%24%20faster%0Ainference.%20RestoreVAR%20leverages%20visual%20autoregressive%20modeling%20%28VAR%29%2C%20a%0Arecently%20introduced%20approach%20which%20performs%20scale-space%20autoregression%20for%0Aimage%20generation.%20VAR%20achieves%20comparable%20performance%20to%20that%20of%0Astate-of-the-art%20diffusion%20transformers%20with%20drastically%20reduced%20computational%0Acosts.%20To%20optimally%20exploit%20these%20advantages%20of%20VAR%20for%20AiOR%2C%20we%20propose%0Aarchitectural%20modifications%20and%20improvements%2C%20including%20intricately%20designed%0Across-attention%20mechanisms%20and%20a%20latent-space%20refinement%20module%2C%20tailored%20for%0Athe%20AiOR%20task.%20Extensive%20experiments%20show%20that%20RestoreVAR%20achieves%0Astate-of-the-art%20performance%20among%20generative%20AiOR%20methods%2C%20while%20also%0Aexhibiting%20strong%20generalization%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRestoreVAR%253A%2520Visual%2520Autoregressive%2520Generation%2520for%2520All-in-One%2520Image%250A%2520%2520Restoration%26entry.906535625%3DSudarshan%2520Rajagopalan%2520and%2520Kartik%2520Narayan%2520and%2520Vishal%2520M.%2520Patel%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520latent%2520diffusion%2520models%2520%2528LDMs%2529%2520such%2520as%2520Stable%2520Diffusion%2520has%250Asignificantly%2520improved%2520the%2520perceptual%2520quality%2520of%2520All-in-One%2520image%2520Restoration%250A%2528AiOR%2529%2520methods%252C%2520while%2520also%2520enhancing%2520their%2520generalization%2520capabilities.%250AHowever%252C%2520these%2520LDM-based%2520frameworks%2520suffer%2520from%2520slow%2520inference%2520due%2520to%2520their%250Aiterative%2520denoising%2520process%252C%2520rendering%2520them%2520impractical%2520for%2520time-sensitive%250Aapplications.%2520To%2520address%2520this%252C%2520we%2520propose%2520RestoreVAR%252C%2520a%2520novel%2520generative%250Aapproach%2520for%2520AiOR%2520that%2520significantly%2520outperforms%2520LDM-based%2520models%2520in%250Arestoration%2520performance%2520while%2520achieving%2520over%2520%2524%255Cmathbf%257B10%255Ctimes%257D%2524%2520faster%250Ainference.%2520RestoreVAR%2520leverages%2520visual%2520autoregressive%2520modeling%2520%2528VAR%2529%252C%2520a%250Arecently%2520introduced%2520approach%2520which%2520performs%2520scale-space%2520autoregression%2520for%250Aimage%2520generation.%2520VAR%2520achieves%2520comparable%2520performance%2520to%2520that%2520of%250Astate-of-the-art%2520diffusion%2520transformers%2520with%2520drastically%2520reduced%2520computational%250Acosts.%2520To%2520optimally%2520exploit%2520these%2520advantages%2520of%2520VAR%2520for%2520AiOR%252C%2520we%2520propose%250Aarchitectural%2520modifications%2520and%2520improvements%252C%2520including%2520intricately%2520designed%250Across-attention%2520mechanisms%2520and%2520a%2520latent-space%2520refinement%2520module%252C%2520tailored%2520for%250Athe%2520AiOR%2520task.%2520Extensive%2520experiments%2520show%2520that%2520RestoreVAR%2520achieves%250Astate-of-the-art%2520performance%2520among%2520generative%2520AiOR%2520methods%252C%2520while%2520also%250Aexhibiting%2520strong%2520generalization%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RestoreVAR%3A%20Visual%20Autoregressive%20Generation%20for%20All-in-One%20Image%0A%20%20Restoration&entry.906535625=Sudarshan%20Rajagopalan%20and%20Kartik%20Narayan%20and%20Vishal%20M.%20Patel&entry.1292438233=%20%20The%20use%20of%20latent%20diffusion%20models%20%28LDMs%29%20such%20as%20Stable%20Diffusion%20has%0Asignificantly%20improved%20the%20perceptual%20quality%20of%20All-in-One%20image%20Restoration%0A%28AiOR%29%20methods%2C%20while%20also%20enhancing%20their%20generalization%20capabilities.%0AHowever%2C%20these%20LDM-based%20frameworks%20suffer%20from%20slow%20inference%20due%20to%20their%0Aiterative%20denoising%20process%2C%20rendering%20them%20impractical%20for%20time-sensitive%0Aapplications.%20To%20address%20this%2C%20we%20propose%20RestoreVAR%2C%20a%20novel%20generative%0Aapproach%20for%20AiOR%20that%20significantly%20outperforms%20LDM-based%20models%20in%0Arestoration%20performance%20while%20achieving%20over%20%24%5Cmathbf%7B10%5Ctimes%7D%24%20faster%0Ainference.%20RestoreVAR%20leverages%20visual%20autoregressive%20modeling%20%28VAR%29%2C%20a%0Arecently%20introduced%20approach%20which%20performs%20scale-space%20autoregression%20for%0Aimage%20generation.%20VAR%20achieves%20comparable%20performance%20to%20that%20of%0Astate-of-the-art%20diffusion%20transformers%20with%20drastically%20reduced%20computational%0Acosts.%20To%20optimally%20exploit%20these%20advantages%20of%20VAR%20for%20AiOR%2C%20we%20propose%0Aarchitectural%20modifications%20and%20improvements%2C%20including%20intricately%20designed%0Across-attention%20mechanisms%20and%20a%20latent-space%20refinement%20module%2C%20tailored%20for%0Athe%20AiOR%20task.%20Extensive%20experiments%20show%20that%20RestoreVAR%20achieves%0Astate-of-the-art%20performance%20among%20generative%20AiOR%20methods%2C%20while%20also%0Aexhibiting%20strong%20generalization%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18047v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


