<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240901.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "OG-Mapping: Octree-based Structured 3D Gaussians for Online Dense\n  Mapping", "author": "Meng Wang and Junyi Wang and Changqun Xia and Chen Wang and Yue Qi", "abstract": "  3D Gaussian splatting (3DGS) has recently demonstrated promising advancements\nin RGB-D online dense mapping. Nevertheless, existing methods excessively rely\non per-pixel depth cues to perform map densification, which leads to\nsignificant redundancy and increased sensitivity to depth noise. Additionally,\nexplicitly storing 3D Gaussian parameters of room-scale scene poses a\nsignificant storage challenge. In this paper, we introduce OG-Mapping, which\nleverages the robust scene structural representation capability of sparse\noctrees, combined with structured 3D Gaussian representations, to achieve\nefficient and robust online dense mapping. Moreover, OG-Mapping employs an\nanchor-based progressive map refinement strategy to recover the scene\nstructures at multiple levels of detail. Instead of maintaining a small number\nof active keyframes with a fixed keyframe window as previous approaches do, a\ndynamic keyframe window is employed to allow OG-Mapping to better tackle false\nlocal minima and forgetting issues. Experimental results demonstrate that\nOG-Mapping delivers more robust and superior realism mapping results than\nexisting Gaussian-based RGB-D online mapping methods with a compact model, and\nno additional post-processing is required.\n", "link": "http://arxiv.org/abs/2408.17223v1", "date": "2024-08-30", "relevancy": 3.2086, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7147}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6558}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OG-Mapping%3A%20Octree-based%20Structured%203D%20Gaussians%20for%20Online%20Dense%0A%20%20Mapping&body=Title%3A%20OG-Mapping%3A%20Octree-based%20Structured%203D%20Gaussians%20for%20Online%20Dense%0A%20%20Mapping%0AAuthor%3A%20Meng%20Wang%20and%20Junyi%20Wang%20and%20Changqun%20Xia%20and%20Chen%20Wang%20and%20Yue%20Qi%0AAbstract%3A%20%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20recently%20demonstrated%20promising%20advancements%0Ain%20RGB-D%20online%20dense%20mapping.%20Nevertheless%2C%20existing%20methods%20excessively%20rely%0Aon%20per-pixel%20depth%20cues%20to%20perform%20map%20densification%2C%20which%20leads%20to%0Asignificant%20redundancy%20and%20increased%20sensitivity%20to%20depth%20noise.%20Additionally%2C%0Aexplicitly%20storing%203D%20Gaussian%20parameters%20of%20room-scale%20scene%20poses%20a%0Asignificant%20storage%20challenge.%20In%20this%20paper%2C%20we%20introduce%20OG-Mapping%2C%20which%0Aleverages%20the%20robust%20scene%20structural%20representation%20capability%20of%20sparse%0Aoctrees%2C%20combined%20with%20structured%203D%20Gaussian%20representations%2C%20to%20achieve%0Aefficient%20and%20robust%20online%20dense%20mapping.%20Moreover%2C%20OG-Mapping%20employs%20an%0Aanchor-based%20progressive%20map%20refinement%20strategy%20to%20recover%20the%20scene%0Astructures%20at%20multiple%20levels%20of%20detail.%20Instead%20of%20maintaining%20a%20small%20number%0Aof%20active%20keyframes%20with%20a%20fixed%20keyframe%20window%20as%20previous%20approaches%20do%2C%20a%0Adynamic%20keyframe%20window%20is%20employed%20to%20allow%20OG-Mapping%20to%20better%20tackle%20false%0Alocal%20minima%20and%20forgetting%20issues.%20Experimental%20results%20demonstrate%20that%0AOG-Mapping%20delivers%20more%20robust%20and%20superior%20realism%20mapping%20results%20than%0Aexisting%20Gaussian-based%20RGB-D%20online%20mapping%20methods%20with%20a%20compact%20model%2C%20and%0Ano%20additional%20post-processing%20is%20required.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOG-Mapping%253A%2520Octree-based%2520Structured%25203D%2520Gaussians%2520for%2520Online%2520Dense%250A%2520%2520Mapping%26entry.906535625%3DMeng%2520Wang%2520and%2520Junyi%2520Wang%2520and%2520Changqun%2520Xia%2520and%2520Chen%2520Wang%2520and%2520Yue%2520Qi%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%2520has%2520recently%2520demonstrated%2520promising%2520advancements%250Ain%2520RGB-D%2520online%2520dense%2520mapping.%2520Nevertheless%252C%2520existing%2520methods%2520excessively%2520rely%250Aon%2520per-pixel%2520depth%2520cues%2520to%2520perform%2520map%2520densification%252C%2520which%2520leads%2520to%250Asignificant%2520redundancy%2520and%2520increased%2520sensitivity%2520to%2520depth%2520noise.%2520Additionally%252C%250Aexplicitly%2520storing%25203D%2520Gaussian%2520parameters%2520of%2520room-scale%2520scene%2520poses%2520a%250Asignificant%2520storage%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520OG-Mapping%252C%2520which%250Aleverages%2520the%2520robust%2520scene%2520structural%2520representation%2520capability%2520of%2520sparse%250Aoctrees%252C%2520combined%2520with%2520structured%25203D%2520Gaussian%2520representations%252C%2520to%2520achieve%250Aefficient%2520and%2520robust%2520online%2520dense%2520mapping.%2520Moreover%252C%2520OG-Mapping%2520employs%2520an%250Aanchor-based%2520progressive%2520map%2520refinement%2520strategy%2520to%2520recover%2520the%2520scene%250Astructures%2520at%2520multiple%2520levels%2520of%2520detail.%2520Instead%2520of%2520maintaining%2520a%2520small%2520number%250Aof%2520active%2520keyframes%2520with%2520a%2520fixed%2520keyframe%2520window%2520as%2520previous%2520approaches%2520do%252C%2520a%250Adynamic%2520keyframe%2520window%2520is%2520employed%2520to%2520allow%2520OG-Mapping%2520to%2520better%2520tackle%2520false%250Alocal%2520minima%2520and%2520forgetting%2520issues.%2520Experimental%2520results%2520demonstrate%2520that%250AOG-Mapping%2520delivers%2520more%2520robust%2520and%2520superior%2520realism%2520mapping%2520results%2520than%250Aexisting%2520Gaussian-based%2520RGB-D%2520online%2520mapping%2520methods%2520with%2520a%2520compact%2520model%252C%2520and%250Ano%2520additional%2520post-processing%2520is%2520required.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OG-Mapping%3A%20Octree-based%20Structured%203D%20Gaussians%20for%20Online%20Dense%0A%20%20Mapping&entry.906535625=Meng%20Wang%20and%20Junyi%20Wang%20and%20Changqun%20Xia%20and%20Chen%20Wang%20and%20Yue%20Qi&entry.1292438233=%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20recently%20demonstrated%20promising%20advancements%0Ain%20RGB-D%20online%20dense%20mapping.%20Nevertheless%2C%20existing%20methods%20excessively%20rely%0Aon%20per-pixel%20depth%20cues%20to%20perform%20map%20densification%2C%20which%20leads%20to%0Asignificant%20redundancy%20and%20increased%20sensitivity%20to%20depth%20noise.%20Additionally%2C%0Aexplicitly%20storing%203D%20Gaussian%20parameters%20of%20room-scale%20scene%20poses%20a%0Asignificant%20storage%20challenge.%20In%20this%20paper%2C%20we%20introduce%20OG-Mapping%2C%20which%0Aleverages%20the%20robust%20scene%20structural%20representation%20capability%20of%20sparse%0Aoctrees%2C%20combined%20with%20structured%203D%20Gaussian%20representations%2C%20to%20achieve%0Aefficient%20and%20robust%20online%20dense%20mapping.%20Moreover%2C%20OG-Mapping%20employs%20an%0Aanchor-based%20progressive%20map%20refinement%20strategy%20to%20recover%20the%20scene%0Astructures%20at%20multiple%20levels%20of%20detail.%20Instead%20of%20maintaining%20a%20small%20number%0Aof%20active%20keyframes%20with%20a%20fixed%20keyframe%20window%20as%20previous%20approaches%20do%2C%20a%0Adynamic%20keyframe%20window%20is%20employed%20to%20allow%20OG-Mapping%20to%20better%20tackle%20false%0Alocal%20minima%20and%20forgetting%20issues.%20Experimental%20results%20demonstrate%20that%0AOG-Mapping%20delivers%20more%20robust%20and%20superior%20realism%20mapping%20results%20than%0Aexisting%20Gaussian-based%20RGB-D%20online%20mapping%20methods%20with%20a%20compact%20model%2C%20and%0Ano%20additional%20post-processing%20is%20required.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17223v1&entry.124074799=Read"},
{"title": "RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian\n  Representations of Radiance Fields", "author": "Mihnea-Bogdan Jurca and Remco Royen and Ion Giosan and Adrian Munteanu", "abstract": "  Gaussian Splatting has revolutionized the world of novel view synthesis by\nachieving high rendering performance in real-time. Recently, studies have\nfocused on enriching these 3D representations with semantic information for\ndownstream tasks. In this paper, we introduce RT-GS2, the first generalizable\nsemantic segmentation method employing Gaussian Splatting. While existing\nGaussian Splatting-based approaches rely on scene-specific training, RT-GS2\ndemonstrates the ability to generalize to unseen scenes. Our method adopts a\nnew approach by first extracting view-independent 3D Gaussian features in a\nself-supervised manner, followed by a novel View-Dependent / View-Independent\n(VDVI) feature fusion to enhance semantic consistency over different views.\nExtensive experimentation on three different datasets showcases RT-GS2's\nsuperiority over the state-of-the-art methods in semantic segmentation quality,\nexemplified by a 8.01% increase in mIoU on the Replica dataset. Moreover, our\nmethod achieves real-time performance of 27.03 FPS, marking an astonishing 901\ntimes speedup compared to existing approaches. This work represents a\nsignificant advancement in the field by introducing, to the best of our\nknowledge, the first real-time generalizable semantic segmentation method for\n3D Gaussian representations of radiance fields.\n", "link": "http://arxiv.org/abs/2405.18033v2", "date": "2024-08-30", "relevancy": 3.2027, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7848}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5911}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RT-GS2%3A%20Real-Time%20Generalizable%20Semantic%20Segmentation%20for%203D%20Gaussian%0A%20%20Representations%20of%20Radiance%20Fields&body=Title%3A%20RT-GS2%3A%20Real-Time%20Generalizable%20Semantic%20Segmentation%20for%203D%20Gaussian%0A%20%20Representations%20of%20Radiance%20Fields%0AAuthor%3A%20Mihnea-Bogdan%20Jurca%20and%20Remco%20Royen%20and%20Ion%20Giosan%20and%20Adrian%20Munteanu%0AAbstract%3A%20%20%20Gaussian%20Splatting%20has%20revolutionized%20the%20world%20of%20novel%20view%20synthesis%20by%0Aachieving%20high%20rendering%20performance%20in%20real-time.%20Recently%2C%20studies%20have%0Afocused%20on%20enriching%20these%203D%20representations%20with%20semantic%20information%20for%0Adownstream%20tasks.%20In%20this%20paper%2C%20we%20introduce%20RT-GS2%2C%20the%20first%20generalizable%0Asemantic%20segmentation%20method%20employing%20Gaussian%20Splatting.%20While%20existing%0AGaussian%20Splatting-based%20approaches%20rely%20on%20scene-specific%20training%2C%20RT-GS2%0Ademonstrates%20the%20ability%20to%20generalize%20to%20unseen%20scenes.%20Our%20method%20adopts%20a%0Anew%20approach%20by%20first%20extracting%20view-independent%203D%20Gaussian%20features%20in%20a%0Aself-supervised%20manner%2C%20followed%20by%20a%20novel%20View-Dependent%20/%20View-Independent%0A%28VDVI%29%20feature%20fusion%20to%20enhance%20semantic%20consistency%20over%20different%20views.%0AExtensive%20experimentation%20on%20three%20different%20datasets%20showcases%20RT-GS2%27s%0Asuperiority%20over%20the%20state-of-the-art%20methods%20in%20semantic%20segmentation%20quality%2C%0Aexemplified%20by%20a%208.01%25%20increase%20in%20mIoU%20on%20the%20Replica%20dataset.%20Moreover%2C%20our%0Amethod%20achieves%20real-time%20performance%20of%2027.03%20FPS%2C%20marking%20an%20astonishing%20901%0Atimes%20speedup%20compared%20to%20existing%20approaches.%20This%20work%20represents%20a%0Asignificant%20advancement%20in%20the%20field%20by%20introducing%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20first%20real-time%20generalizable%20semantic%20segmentation%20method%20for%0A3D%20Gaussian%20representations%20of%20radiance%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18033v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRT-GS2%253A%2520Real-Time%2520Generalizable%2520Semantic%2520Segmentation%2520for%25203D%2520Gaussian%250A%2520%2520Representations%2520of%2520Radiance%2520Fields%26entry.906535625%3DMihnea-Bogdan%2520Jurca%2520and%2520Remco%2520Royen%2520and%2520Ion%2520Giosan%2520and%2520Adrian%2520Munteanu%26entry.1292438233%3D%2520%2520Gaussian%2520Splatting%2520has%2520revolutionized%2520the%2520world%2520of%2520novel%2520view%2520synthesis%2520by%250Aachieving%2520high%2520rendering%2520performance%2520in%2520real-time.%2520Recently%252C%2520studies%2520have%250Afocused%2520on%2520enriching%2520these%25203D%2520representations%2520with%2520semantic%2520information%2520for%250Adownstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520RT-GS2%252C%2520the%2520first%2520generalizable%250Asemantic%2520segmentation%2520method%2520employing%2520Gaussian%2520Splatting.%2520While%2520existing%250AGaussian%2520Splatting-based%2520approaches%2520rely%2520on%2520scene-specific%2520training%252C%2520RT-GS2%250Ademonstrates%2520the%2520ability%2520to%2520generalize%2520to%2520unseen%2520scenes.%2520Our%2520method%2520adopts%2520a%250Anew%2520approach%2520by%2520first%2520extracting%2520view-independent%25203D%2520Gaussian%2520features%2520in%2520a%250Aself-supervised%2520manner%252C%2520followed%2520by%2520a%2520novel%2520View-Dependent%2520/%2520View-Independent%250A%2528VDVI%2529%2520feature%2520fusion%2520to%2520enhance%2520semantic%2520consistency%2520over%2520different%2520views.%250AExtensive%2520experimentation%2520on%2520three%2520different%2520datasets%2520showcases%2520RT-GS2%2527s%250Asuperiority%2520over%2520the%2520state-of-the-art%2520methods%2520in%2520semantic%2520segmentation%2520quality%252C%250Aexemplified%2520by%2520a%25208.01%2525%2520increase%2520in%2520mIoU%2520on%2520the%2520Replica%2520dataset.%2520Moreover%252C%2520our%250Amethod%2520achieves%2520real-time%2520performance%2520of%252027.03%2520FPS%252C%2520marking%2520an%2520astonishing%2520901%250Atimes%2520speedup%2520compared%2520to%2520existing%2520approaches.%2520This%2520work%2520represents%2520a%250Asignificant%2520advancement%2520in%2520the%2520field%2520by%2520introducing%252C%2520to%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520the%2520first%2520real-time%2520generalizable%2520semantic%2520segmentation%2520method%2520for%250A3D%2520Gaussian%2520representations%2520of%2520radiance%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18033v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RT-GS2%3A%20Real-Time%20Generalizable%20Semantic%20Segmentation%20for%203D%20Gaussian%0A%20%20Representations%20of%20Radiance%20Fields&entry.906535625=Mihnea-Bogdan%20Jurca%20and%20Remco%20Royen%20and%20Ion%20Giosan%20and%20Adrian%20Munteanu&entry.1292438233=%20%20Gaussian%20Splatting%20has%20revolutionized%20the%20world%20of%20novel%20view%20synthesis%20by%0Aachieving%20high%20rendering%20performance%20in%20real-time.%20Recently%2C%20studies%20have%0Afocused%20on%20enriching%20these%203D%20representations%20with%20semantic%20information%20for%0Adownstream%20tasks.%20In%20this%20paper%2C%20we%20introduce%20RT-GS2%2C%20the%20first%20generalizable%0Asemantic%20segmentation%20method%20employing%20Gaussian%20Splatting.%20While%20existing%0AGaussian%20Splatting-based%20approaches%20rely%20on%20scene-specific%20training%2C%20RT-GS2%0Ademonstrates%20the%20ability%20to%20generalize%20to%20unseen%20scenes.%20Our%20method%20adopts%20a%0Anew%20approach%20by%20first%20extracting%20view-independent%203D%20Gaussian%20features%20in%20a%0Aself-supervised%20manner%2C%20followed%20by%20a%20novel%20View-Dependent%20/%20View-Independent%0A%28VDVI%29%20feature%20fusion%20to%20enhance%20semantic%20consistency%20over%20different%20views.%0AExtensive%20experimentation%20on%20three%20different%20datasets%20showcases%20RT-GS2%27s%0Asuperiority%20over%20the%20state-of-the-art%20methods%20in%20semantic%20segmentation%20quality%2C%0Aexemplified%20by%20a%208.01%25%20increase%20in%20mIoU%20on%20the%20Replica%20dataset.%20Moreover%2C%20our%0Amethod%20achieves%20real-time%20performance%20of%2027.03%20FPS%2C%20marking%20an%20astonishing%20901%0Atimes%20speedup%20compared%20to%20existing%20approaches.%20This%20work%20represents%20a%0Asignificant%20advancement%20in%20the%20field%20by%20introducing%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20first%20real-time%20generalizable%20semantic%20segmentation%20method%20for%0A3D%20Gaussian%20representations%20of%20radiance%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18033v2&entry.124074799=Read"},
{"title": "Frankenstein: Generating Semantic-Compositional 3D Scenes in One\n  Tri-Plane", "author": "Han Yan and Yang Li and Zhennan Wu and Shenzhou Chen and Weixuan Sun and Taizhang Shang and Weizhe Liu and Tian Chen and Xiaqiang Dai and Chao Ma and Hongdong Li and Pan Ji", "abstract": "  We present Frankenstein, a diffusion-based framework that can generate\nsemantic-compositional 3D scenes in a single pass. Unlike existing methods that\noutput a single, unified 3D shape, Frankenstein simultaneously generates\nmultiple separated shapes, each corresponding to a semantically meaningful\npart. The 3D scene information is encoded in one single tri-plane tensor, from\nwhich multiple Singed Distance Function (SDF) fields can be decoded to\nrepresent the compositional shapes. During training, an auto-encoder compresses\ntri-planes into a latent space, and then the denoising diffusion process is\nemployed to approximate the distribution of the compositional scenes.\nFrankenstein demonstrates promising results in generating room interiors as\nwell as human avatars with automatically separated parts. The generated scenes\nfacilitate many downstream applications, such as part-wise re-texturing, object\nrearrangement in the room or avatar cloth re-targeting. Our project page is\navailable at: https://wolfball.github.io/frankenstein/.\n", "link": "http://arxiv.org/abs/2403.16210v2", "date": "2024-08-30", "relevancy": 3.068, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6428}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6428}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frankenstein%3A%20Generating%20Semantic-Compositional%203D%20Scenes%20in%20One%0A%20%20Tri-Plane&body=Title%3A%20Frankenstein%3A%20Generating%20Semantic-Compositional%203D%20Scenes%20in%20One%0A%20%20Tri-Plane%0AAuthor%3A%20Han%20Yan%20and%20Yang%20Li%20and%20Zhennan%20Wu%20and%20Shenzhou%20Chen%20and%20Weixuan%20Sun%20and%20Taizhang%20Shang%20and%20Weizhe%20Liu%20and%20Tian%20Chen%20and%20Xiaqiang%20Dai%20and%20Chao%20Ma%20and%20Hongdong%20Li%20and%20Pan%20Ji%0AAbstract%3A%20%20%20We%20present%20Frankenstein%2C%20a%20diffusion-based%20framework%20that%20can%20generate%0Asemantic-compositional%203D%20scenes%20in%20a%20single%20pass.%20Unlike%20existing%20methods%20that%0Aoutput%20a%20single%2C%20unified%203D%20shape%2C%20Frankenstein%20simultaneously%20generates%0Amultiple%20separated%20shapes%2C%20each%20corresponding%20to%20a%20semantically%20meaningful%0Apart.%20The%203D%20scene%20information%20is%20encoded%20in%20one%20single%20tri-plane%20tensor%2C%20from%0Awhich%20multiple%20Singed%20Distance%20Function%20%28SDF%29%20fields%20can%20be%20decoded%20to%0Arepresent%20the%20compositional%20shapes.%20During%20training%2C%20an%20auto-encoder%20compresses%0Atri-planes%20into%20a%20latent%20space%2C%20and%20then%20the%20denoising%20diffusion%20process%20is%0Aemployed%20to%20approximate%20the%20distribution%20of%20the%20compositional%20scenes.%0AFrankenstein%20demonstrates%20promising%20results%20in%20generating%20room%20interiors%20as%0Awell%20as%20human%20avatars%20with%20automatically%20separated%20parts.%20The%20generated%20scenes%0Afacilitate%20many%20downstream%20applications%2C%20such%20as%20part-wise%20re-texturing%2C%20object%0Arearrangement%20in%20the%20room%20or%20avatar%20cloth%20re-targeting.%20Our%20project%20page%20is%0Aavailable%20at%3A%20https%3A//wolfball.github.io/frankenstein/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16210v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrankenstein%253A%2520Generating%2520Semantic-Compositional%25203D%2520Scenes%2520in%2520One%250A%2520%2520Tri-Plane%26entry.906535625%3DHan%2520Yan%2520and%2520Yang%2520Li%2520and%2520Zhennan%2520Wu%2520and%2520Shenzhou%2520Chen%2520and%2520Weixuan%2520Sun%2520and%2520Taizhang%2520Shang%2520and%2520Weizhe%2520Liu%2520and%2520Tian%2520Chen%2520and%2520Xiaqiang%2520Dai%2520and%2520Chao%2520Ma%2520and%2520Hongdong%2520Li%2520and%2520Pan%2520Ji%26entry.1292438233%3D%2520%2520We%2520present%2520Frankenstein%252C%2520a%2520diffusion-based%2520framework%2520that%2520can%2520generate%250Asemantic-compositional%25203D%2520scenes%2520in%2520a%2520single%2520pass.%2520Unlike%2520existing%2520methods%2520that%250Aoutput%2520a%2520single%252C%2520unified%25203D%2520shape%252C%2520Frankenstein%2520simultaneously%2520generates%250Amultiple%2520separated%2520shapes%252C%2520each%2520corresponding%2520to%2520a%2520semantically%2520meaningful%250Apart.%2520The%25203D%2520scene%2520information%2520is%2520encoded%2520in%2520one%2520single%2520tri-plane%2520tensor%252C%2520from%250Awhich%2520multiple%2520Singed%2520Distance%2520Function%2520%2528SDF%2529%2520fields%2520can%2520be%2520decoded%2520to%250Arepresent%2520the%2520compositional%2520shapes.%2520During%2520training%252C%2520an%2520auto-encoder%2520compresses%250Atri-planes%2520into%2520a%2520latent%2520space%252C%2520and%2520then%2520the%2520denoising%2520diffusion%2520process%2520is%250Aemployed%2520to%2520approximate%2520the%2520distribution%2520of%2520the%2520compositional%2520scenes.%250AFrankenstein%2520demonstrates%2520promising%2520results%2520in%2520generating%2520room%2520interiors%2520as%250Awell%2520as%2520human%2520avatars%2520with%2520automatically%2520separated%2520parts.%2520The%2520generated%2520scenes%250Afacilitate%2520many%2520downstream%2520applications%252C%2520such%2520as%2520part-wise%2520re-texturing%252C%2520object%250Arearrangement%2520in%2520the%2520room%2520or%2520avatar%2520cloth%2520re-targeting.%2520Our%2520project%2520page%2520is%250Aavailable%2520at%253A%2520https%253A//wolfball.github.io/frankenstein/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16210v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frankenstein%3A%20Generating%20Semantic-Compositional%203D%20Scenes%20in%20One%0A%20%20Tri-Plane&entry.906535625=Han%20Yan%20and%20Yang%20Li%20and%20Zhennan%20Wu%20and%20Shenzhou%20Chen%20and%20Weixuan%20Sun%20and%20Taizhang%20Shang%20and%20Weizhe%20Liu%20and%20Tian%20Chen%20and%20Xiaqiang%20Dai%20and%20Chao%20Ma%20and%20Hongdong%20Li%20and%20Pan%20Ji&entry.1292438233=%20%20We%20present%20Frankenstein%2C%20a%20diffusion-based%20framework%20that%20can%20generate%0Asemantic-compositional%203D%20scenes%20in%20a%20single%20pass.%20Unlike%20existing%20methods%20that%0Aoutput%20a%20single%2C%20unified%203D%20shape%2C%20Frankenstein%20simultaneously%20generates%0Amultiple%20separated%20shapes%2C%20each%20corresponding%20to%20a%20semantically%20meaningful%0Apart.%20The%203D%20scene%20information%20is%20encoded%20in%20one%20single%20tri-plane%20tensor%2C%20from%0Awhich%20multiple%20Singed%20Distance%20Function%20%28SDF%29%20fields%20can%20be%20decoded%20to%0Arepresent%20the%20compositional%20shapes.%20During%20training%2C%20an%20auto-encoder%20compresses%0Atri-planes%20into%20a%20latent%20space%2C%20and%20then%20the%20denoising%20diffusion%20process%20is%0Aemployed%20to%20approximate%20the%20distribution%20of%20the%20compositional%20scenes.%0AFrankenstein%20demonstrates%20promising%20results%20in%20generating%20room%20interiors%20as%0Awell%20as%20human%20avatars%20with%20automatically%20separated%20parts.%20The%20generated%20scenes%0Afacilitate%20many%20downstream%20applications%2C%20such%20as%20part-wise%20re-texturing%2C%20object%0Arearrangement%20in%20the%20room%20or%20avatar%20cloth%20re-targeting.%20Our%20project%20page%20is%0Aavailable%20at%3A%20https%3A//wolfball.github.io/frankenstein/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16210v2&entry.124074799=Read"},
{"title": "CinePreGen: Camera Controllable Video Previsualization via\n  Engine-powered Diffusion", "author": "Yiran Chen and Anyi Rao and Xuekun Jiang and Shishi Xiao and Ruiqing Ma and Zeyu Wang and Hui Xiong and Bo Dai", "abstract": "  With advancements in video generative AI models (e.g., SORA), creators are\nincreasingly using these techniques to enhance video previsualization. However,\nthey face challenges with incomplete and mismatched AI workflows. Existing\nmethods mainly rely on text descriptions and struggle with camera placement, a\nkey component of previsualization. To address these issues, we introduce\nCinePreGen, a visual previsualization system enhanced with engine-powered\ndiffusion. It features a novel camera and storyboard interface that offers\ndynamic control, from global to local camera adjustments. This is combined with\na user-friendly AI rendering workflow, which aims to achieve consistent results\nthrough multi-masked IP-Adapter and engine simulation guidelines. In our\ncomprehensive evaluation study, we demonstrate that our system reduces\ndevelopment viscosity (i.e., the complexity and challenges in the development\nprocess), meets users' needs for extensive control and iteration in the design\nprocess, and outperforms other AI video production workflows in cinematic\ncamera movement, as shown by our experiments and a within-subjects user study.\nWith its intuitive camera controls and realistic rendering of camera motion,\nCinePreGen shows great potential for improving video production for both\nindividual creators and industry professionals.\n", "link": "http://arxiv.org/abs/2408.17424v1", "date": "2024-08-30", "relevancy": 2.9768, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.596}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.596}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CinePreGen%3A%20Camera%20Controllable%20Video%20Previsualization%20via%0A%20%20Engine-powered%20Diffusion&body=Title%3A%20CinePreGen%3A%20Camera%20Controllable%20Video%20Previsualization%20via%0A%20%20Engine-powered%20Diffusion%0AAuthor%3A%20Yiran%20Chen%20and%20Anyi%20Rao%20and%20Xuekun%20Jiang%20and%20Shishi%20Xiao%20and%20Ruiqing%20Ma%20and%20Zeyu%20Wang%20and%20Hui%20Xiong%20and%20Bo%20Dai%0AAbstract%3A%20%20%20With%20advancements%20in%20video%20generative%20AI%20models%20%28e.g.%2C%20SORA%29%2C%20creators%20are%0Aincreasingly%20using%20these%20techniques%20to%20enhance%20video%20previsualization.%20However%2C%0Athey%20face%20challenges%20with%20incomplete%20and%20mismatched%20AI%20workflows.%20Existing%0Amethods%20mainly%20rely%20on%20text%20descriptions%20and%20struggle%20with%20camera%20placement%2C%20a%0Akey%20component%20of%20previsualization.%20To%20address%20these%20issues%2C%20we%20introduce%0ACinePreGen%2C%20a%20visual%20previsualization%20system%20enhanced%20with%20engine-powered%0Adiffusion.%20It%20features%20a%20novel%20camera%20and%20storyboard%20interface%20that%20offers%0Adynamic%20control%2C%20from%20global%20to%20local%20camera%20adjustments.%20This%20is%20combined%20with%0Aa%20user-friendly%20AI%20rendering%20workflow%2C%20which%20aims%20to%20achieve%20consistent%20results%0Athrough%20multi-masked%20IP-Adapter%20and%20engine%20simulation%20guidelines.%20In%20our%0Acomprehensive%20evaluation%20study%2C%20we%20demonstrate%20that%20our%20system%20reduces%0Adevelopment%20viscosity%20%28i.e.%2C%20the%20complexity%20and%20challenges%20in%20the%20development%0Aprocess%29%2C%20meets%20users%27%20needs%20for%20extensive%20control%20and%20iteration%20in%20the%20design%0Aprocess%2C%20and%20outperforms%20other%20AI%20video%20production%20workflows%20in%20cinematic%0Acamera%20movement%2C%20as%20shown%20by%20our%20experiments%20and%20a%20within-subjects%20user%20study.%0AWith%20its%20intuitive%20camera%20controls%20and%20realistic%20rendering%20of%20camera%20motion%2C%0ACinePreGen%20shows%20great%20potential%20for%20improving%20video%20production%20for%20both%0Aindividual%20creators%20and%20industry%20professionals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCinePreGen%253A%2520Camera%2520Controllable%2520Video%2520Previsualization%2520via%250A%2520%2520Engine-powered%2520Diffusion%26entry.906535625%3DYiran%2520Chen%2520and%2520Anyi%2520Rao%2520and%2520Xuekun%2520Jiang%2520and%2520Shishi%2520Xiao%2520and%2520Ruiqing%2520Ma%2520and%2520Zeyu%2520Wang%2520and%2520Hui%2520Xiong%2520and%2520Bo%2520Dai%26entry.1292438233%3D%2520%2520With%2520advancements%2520in%2520video%2520generative%2520AI%2520models%2520%2528e.g.%252C%2520SORA%2529%252C%2520creators%2520are%250Aincreasingly%2520using%2520these%2520techniques%2520to%2520enhance%2520video%2520previsualization.%2520However%252C%250Athey%2520face%2520challenges%2520with%2520incomplete%2520and%2520mismatched%2520AI%2520workflows.%2520Existing%250Amethods%2520mainly%2520rely%2520on%2520text%2520descriptions%2520and%2520struggle%2520with%2520camera%2520placement%252C%2520a%250Akey%2520component%2520of%2520previsualization.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%250ACinePreGen%252C%2520a%2520visual%2520previsualization%2520system%2520enhanced%2520with%2520engine-powered%250Adiffusion.%2520It%2520features%2520a%2520novel%2520camera%2520and%2520storyboard%2520interface%2520that%2520offers%250Adynamic%2520control%252C%2520from%2520global%2520to%2520local%2520camera%2520adjustments.%2520This%2520is%2520combined%2520with%250Aa%2520user-friendly%2520AI%2520rendering%2520workflow%252C%2520which%2520aims%2520to%2520achieve%2520consistent%2520results%250Athrough%2520multi-masked%2520IP-Adapter%2520and%2520engine%2520simulation%2520guidelines.%2520In%2520our%250Acomprehensive%2520evaluation%2520study%252C%2520we%2520demonstrate%2520that%2520our%2520system%2520reduces%250Adevelopment%2520viscosity%2520%2528i.e.%252C%2520the%2520complexity%2520and%2520challenges%2520in%2520the%2520development%250Aprocess%2529%252C%2520meets%2520users%2527%2520needs%2520for%2520extensive%2520control%2520and%2520iteration%2520in%2520the%2520design%250Aprocess%252C%2520and%2520outperforms%2520other%2520AI%2520video%2520production%2520workflows%2520in%2520cinematic%250Acamera%2520movement%252C%2520as%2520shown%2520by%2520our%2520experiments%2520and%2520a%2520within-subjects%2520user%2520study.%250AWith%2520its%2520intuitive%2520camera%2520controls%2520and%2520realistic%2520rendering%2520of%2520camera%2520motion%252C%250ACinePreGen%2520shows%2520great%2520potential%2520for%2520improving%2520video%2520production%2520for%2520both%250Aindividual%2520creators%2520and%2520industry%2520professionals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CinePreGen%3A%20Camera%20Controllable%20Video%20Previsualization%20via%0A%20%20Engine-powered%20Diffusion&entry.906535625=Yiran%20Chen%20and%20Anyi%20Rao%20and%20Xuekun%20Jiang%20and%20Shishi%20Xiao%20and%20Ruiqing%20Ma%20and%20Zeyu%20Wang%20and%20Hui%20Xiong%20and%20Bo%20Dai&entry.1292438233=%20%20With%20advancements%20in%20video%20generative%20AI%20models%20%28e.g.%2C%20SORA%29%2C%20creators%20are%0Aincreasingly%20using%20these%20techniques%20to%20enhance%20video%20previsualization.%20However%2C%0Athey%20face%20challenges%20with%20incomplete%20and%20mismatched%20AI%20workflows.%20Existing%0Amethods%20mainly%20rely%20on%20text%20descriptions%20and%20struggle%20with%20camera%20placement%2C%20a%0Akey%20component%20of%20previsualization.%20To%20address%20these%20issues%2C%20we%20introduce%0ACinePreGen%2C%20a%20visual%20previsualization%20system%20enhanced%20with%20engine-powered%0Adiffusion.%20It%20features%20a%20novel%20camera%20and%20storyboard%20interface%20that%20offers%0Adynamic%20control%2C%20from%20global%20to%20local%20camera%20adjustments.%20This%20is%20combined%20with%0Aa%20user-friendly%20AI%20rendering%20workflow%2C%20which%20aims%20to%20achieve%20consistent%20results%0Athrough%20multi-masked%20IP-Adapter%20and%20engine%20simulation%20guidelines.%20In%20our%0Acomprehensive%20evaluation%20study%2C%20we%20demonstrate%20that%20our%20system%20reduces%0Adevelopment%20viscosity%20%28i.e.%2C%20the%20complexity%20and%20challenges%20in%20the%20development%0Aprocess%29%2C%20meets%20users%27%20needs%20for%20extensive%20control%20and%20iteration%20in%20the%20design%0Aprocess%2C%20and%20outperforms%20other%20AI%20video%20production%20workflows%20in%20cinematic%0Acamera%20movement%2C%20as%20shown%20by%20our%20experiments%20and%20a%20within-subjects%20user%20study.%0AWith%20its%20intuitive%20camera%20controls%20and%20realistic%20rendering%20of%20camera%20motion%2C%0ACinePreGen%20shows%20great%20potential%20for%20improving%20video%20production%20for%20both%0Aindividual%20creators%20and%20industry%20professionals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17424v1&entry.124074799=Read"},
{"title": "DreamPhysics: Learning Physical Properties of Dynamic 3D Gaussians with\n  Video Diffusion Priors", "author": "Tianyu Huang and Haoze Zhang and Yihan Zeng and Zhilu Zhang and Hui Li and Wangmeng Zuo and Rynson W. H. Lau", "abstract": "  Dynamic 3D interaction has been attracting a lot of attention recently.\nHowever, creating such 4D content remains challenging. One solution is to\nanimate 3D scenes with physics-based simulation, which requires manually\nassigning precise physical properties to the object or the simulated results\nwould become unnatural. Another solution is to learn the deformation of 3D\nobjects with the distillation of video generative models, which, however, tends\nto produce 3D videos with small and discontinuous motions due to the\ninappropriate extraction and application of physical prior. In this work,\ncombining the strengths and complementing shortcomings of the above two\nsolutions, we propose to learn the physical properties of a material field with\nvideo diffusion priors, and then utilize a physics-based Material-Point-Method\n(MPM) simulator to generate 4D content with realistic motions. In particular,\nwe propose motion distillation sampling to emphasize video motion information\nduring distillation. Moreover, to facilitate the optimization, we further\npropose a KAN-based material field with frame boosting. Experimental results\ndemonstrate that our method enjoys more realistic motion than\nstate-of-the-arts. Codes are released at:\nhttps://github.com/tyhuang0428/DreamPhysics.\n", "link": "http://arxiv.org/abs/2406.01476v2", "date": "2024-08-30", "relevancy": 2.9488, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6252}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.572}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamPhysics%3A%20Learning%20Physical%20Properties%20of%20Dynamic%203D%20Gaussians%20with%0A%20%20Video%20Diffusion%20Priors&body=Title%3A%20DreamPhysics%3A%20Learning%20Physical%20Properties%20of%20Dynamic%203D%20Gaussians%20with%0A%20%20Video%20Diffusion%20Priors%0AAuthor%3A%20Tianyu%20Huang%20and%20Haoze%20Zhang%20and%20Yihan%20Zeng%20and%20Zhilu%20Zhang%20and%20Hui%20Li%20and%20Wangmeng%20Zuo%20and%20Rynson%20W.%20H.%20Lau%0AAbstract%3A%20%20%20Dynamic%203D%20interaction%20has%20been%20attracting%20a%20lot%20of%20attention%20recently.%0AHowever%2C%20creating%20such%204D%20content%20remains%20challenging.%20One%20solution%20is%20to%0Aanimate%203D%20scenes%20with%20physics-based%20simulation%2C%20which%20requires%20manually%0Aassigning%20precise%20physical%20properties%20to%20the%20object%20or%20the%20simulated%20results%0Awould%20become%20unnatural.%20Another%20solution%20is%20to%20learn%20the%20deformation%20of%203D%0Aobjects%20with%20the%20distillation%20of%20video%20generative%20models%2C%20which%2C%20however%2C%20tends%0Ato%20produce%203D%20videos%20with%20small%20and%20discontinuous%20motions%20due%20to%20the%0Ainappropriate%20extraction%20and%20application%20of%20physical%20prior.%20In%20this%20work%2C%0Acombining%20the%20strengths%20and%20complementing%20shortcomings%20of%20the%20above%20two%0Asolutions%2C%20we%20propose%20to%20learn%20the%20physical%20properties%20of%20a%20material%20field%20with%0Avideo%20diffusion%20priors%2C%20and%20then%20utilize%20a%20physics-based%20Material-Point-Method%0A%28MPM%29%20simulator%20to%20generate%204D%20content%20with%20realistic%20motions.%20In%20particular%2C%0Awe%20propose%20motion%20distillation%20sampling%20to%20emphasize%20video%20motion%20information%0Aduring%20distillation.%20Moreover%2C%20to%20facilitate%20the%20optimization%2C%20we%20further%0Apropose%20a%20KAN-based%20material%20field%20with%20frame%20boosting.%20Experimental%20results%0Ademonstrate%20that%20our%20method%20enjoys%20more%20realistic%20motion%20than%0Astate-of-the-arts.%20Codes%20are%20released%20at%3A%0Ahttps%3A//github.com/tyhuang0428/DreamPhysics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01476v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamPhysics%253A%2520Learning%2520Physical%2520Properties%2520of%2520Dynamic%25203D%2520Gaussians%2520with%250A%2520%2520Video%2520Diffusion%2520Priors%26entry.906535625%3DTianyu%2520Huang%2520and%2520Haoze%2520Zhang%2520and%2520Yihan%2520Zeng%2520and%2520Zhilu%2520Zhang%2520and%2520Hui%2520Li%2520and%2520Wangmeng%2520Zuo%2520and%2520Rynson%2520W.%2520H.%2520Lau%26entry.1292438233%3D%2520%2520Dynamic%25203D%2520interaction%2520has%2520been%2520attracting%2520a%2520lot%2520of%2520attention%2520recently.%250AHowever%252C%2520creating%2520such%25204D%2520content%2520remains%2520challenging.%2520One%2520solution%2520is%2520to%250Aanimate%25203D%2520scenes%2520with%2520physics-based%2520simulation%252C%2520which%2520requires%2520manually%250Aassigning%2520precise%2520physical%2520properties%2520to%2520the%2520object%2520or%2520the%2520simulated%2520results%250Awould%2520become%2520unnatural.%2520Another%2520solution%2520is%2520to%2520learn%2520the%2520deformation%2520of%25203D%250Aobjects%2520with%2520the%2520distillation%2520of%2520video%2520generative%2520models%252C%2520which%252C%2520however%252C%2520tends%250Ato%2520produce%25203D%2520videos%2520with%2520small%2520and%2520discontinuous%2520motions%2520due%2520to%2520the%250Ainappropriate%2520extraction%2520and%2520application%2520of%2520physical%2520prior.%2520In%2520this%2520work%252C%250Acombining%2520the%2520strengths%2520and%2520complementing%2520shortcomings%2520of%2520the%2520above%2520two%250Asolutions%252C%2520we%2520propose%2520to%2520learn%2520the%2520physical%2520properties%2520of%2520a%2520material%2520field%2520with%250Avideo%2520diffusion%2520priors%252C%2520and%2520then%2520utilize%2520a%2520physics-based%2520Material-Point-Method%250A%2528MPM%2529%2520simulator%2520to%2520generate%25204D%2520content%2520with%2520realistic%2520motions.%2520In%2520particular%252C%250Awe%2520propose%2520motion%2520distillation%2520sampling%2520to%2520emphasize%2520video%2520motion%2520information%250Aduring%2520distillation.%2520Moreover%252C%2520to%2520facilitate%2520the%2520optimization%252C%2520we%2520further%250Apropose%2520a%2520KAN-based%2520material%2520field%2520with%2520frame%2520boosting.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520method%2520enjoys%2520more%2520realistic%2520motion%2520than%250Astate-of-the-arts.%2520Codes%2520are%2520released%2520at%253A%250Ahttps%253A//github.com/tyhuang0428/DreamPhysics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01476v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamPhysics%3A%20Learning%20Physical%20Properties%20of%20Dynamic%203D%20Gaussians%20with%0A%20%20Video%20Diffusion%20Priors&entry.906535625=Tianyu%20Huang%20and%20Haoze%20Zhang%20and%20Yihan%20Zeng%20and%20Zhilu%20Zhang%20and%20Hui%20Li%20and%20Wangmeng%20Zuo%20and%20Rynson%20W.%20H.%20Lau&entry.1292438233=%20%20Dynamic%203D%20interaction%20has%20been%20attracting%20a%20lot%20of%20attention%20recently.%0AHowever%2C%20creating%20such%204D%20content%20remains%20challenging.%20One%20solution%20is%20to%0Aanimate%203D%20scenes%20with%20physics-based%20simulation%2C%20which%20requires%20manually%0Aassigning%20precise%20physical%20properties%20to%20the%20object%20or%20the%20simulated%20results%0Awould%20become%20unnatural.%20Another%20solution%20is%20to%20learn%20the%20deformation%20of%203D%0Aobjects%20with%20the%20distillation%20of%20video%20generative%20models%2C%20which%2C%20however%2C%20tends%0Ato%20produce%203D%20videos%20with%20small%20and%20discontinuous%20motions%20due%20to%20the%0Ainappropriate%20extraction%20and%20application%20of%20physical%20prior.%20In%20this%20work%2C%0Acombining%20the%20strengths%20and%20complementing%20shortcomings%20of%20the%20above%20two%0Asolutions%2C%20we%20propose%20to%20learn%20the%20physical%20properties%20of%20a%20material%20field%20with%0Avideo%20diffusion%20priors%2C%20and%20then%20utilize%20a%20physics-based%20Material-Point-Method%0A%28MPM%29%20simulator%20to%20generate%204D%20content%20with%20realistic%20motions.%20In%20particular%2C%0Awe%20propose%20motion%20distillation%20sampling%20to%20emphasize%20video%20motion%20information%0Aduring%20distillation.%20Moreover%2C%20to%20facilitate%20the%20optimization%2C%20we%20further%0Apropose%20a%20KAN-based%20material%20field%20with%20frame%20boosting.%20Experimental%20results%0Ademonstrate%20that%20our%20method%20enjoys%20more%20realistic%20motion%20than%0Astate-of-the-arts.%20Codes%20are%20released%20at%3A%0Ahttps%3A//github.com/tyhuang0428/DreamPhysics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01476v2&entry.124074799=Read"},
{"title": "Augmented Reality without Borders: Achieving Precise Localization\n  Without Maps", "author": "Albert Gassol Puigjaner and Irvin Aloise and Patrik Schmuck", "abstract": "  Visual localization is crucial for Computer Vision and Augmented Reality (AR)\napplications, where determining the camera or device's position and orientation\nis essential to accurately interact with the physical environment. Traditional\nmethods rely on detailed 3D maps constructed using Structure from Motion (SfM)\nor Simultaneous Localization and Mapping (SLAM), which is computationally\nexpensive and impractical for dynamic or large-scale environments. We introduce\nMARLOC, a novel localization framework for AR applications that uses known\nrelative transformations within image sequences to perform intra-sequence\ntriangulation, generating 3D-2D correspondences for pose estimation and\nrefinement. MARLOC eliminates the need for pre-built SfM maps, providing\naccurate and efficient localization suitable for dynamic outdoor environments.\nEvaluation with benchmark datasets and real-world experiments demonstrates\nMARLOC's state-of-the-art performance and robustness. By integrating MARLOC\ninto an AR device, we highlight its capability to achieve precise localization\nin real-world outdoor scenarios, showcasing its practical effectiveness and\npotential to enhance visual localization in AR applications.\n", "link": "http://arxiv.org/abs/2408.17373v1", "date": "2024-08-30", "relevancy": 2.9229, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6067}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5771}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Augmented%20Reality%20without%20Borders%3A%20Achieving%20Precise%20Localization%0A%20%20Without%20Maps&body=Title%3A%20Augmented%20Reality%20without%20Borders%3A%20Achieving%20Precise%20Localization%0A%20%20Without%20Maps%0AAuthor%3A%20Albert%20Gassol%20Puigjaner%20and%20Irvin%20Aloise%20and%20Patrik%20Schmuck%0AAbstract%3A%20%20%20Visual%20localization%20is%20crucial%20for%20Computer%20Vision%20and%20Augmented%20Reality%20%28AR%29%0Aapplications%2C%20where%20determining%20the%20camera%20or%20device%27s%20position%20and%20orientation%0Ais%20essential%20to%20accurately%20interact%20with%20the%20physical%20environment.%20Traditional%0Amethods%20rely%20on%20detailed%203D%20maps%20constructed%20using%20Structure%20from%20Motion%20%28SfM%29%0Aor%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%2C%20which%20is%20computationally%0Aexpensive%20and%20impractical%20for%20dynamic%20or%20large-scale%20environments.%20We%20introduce%0AMARLOC%2C%20a%20novel%20localization%20framework%20for%20AR%20applications%20that%20uses%20known%0Arelative%20transformations%20within%20image%20sequences%20to%20perform%20intra-sequence%0Atriangulation%2C%20generating%203D-2D%20correspondences%20for%20pose%20estimation%20and%0Arefinement.%20MARLOC%20eliminates%20the%20need%20for%20pre-built%20SfM%20maps%2C%20providing%0Aaccurate%20and%20efficient%20localization%20suitable%20for%20dynamic%20outdoor%20environments.%0AEvaluation%20with%20benchmark%20datasets%20and%20real-world%20experiments%20demonstrates%0AMARLOC%27s%20state-of-the-art%20performance%20and%20robustness.%20By%20integrating%20MARLOC%0Ainto%20an%20AR%20device%2C%20we%20highlight%20its%20capability%20to%20achieve%20precise%20localization%0Ain%20real-world%20outdoor%20scenarios%2C%20showcasing%20its%20practical%20effectiveness%20and%0Apotential%20to%20enhance%20visual%20localization%20in%20AR%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmented%2520Reality%2520without%2520Borders%253A%2520Achieving%2520Precise%2520Localization%250A%2520%2520Without%2520Maps%26entry.906535625%3DAlbert%2520Gassol%2520Puigjaner%2520and%2520Irvin%2520Aloise%2520and%2520Patrik%2520Schmuck%26entry.1292438233%3D%2520%2520Visual%2520localization%2520is%2520crucial%2520for%2520Computer%2520Vision%2520and%2520Augmented%2520Reality%2520%2528AR%2529%250Aapplications%252C%2520where%2520determining%2520the%2520camera%2520or%2520device%2527s%2520position%2520and%2520orientation%250Ais%2520essential%2520to%2520accurately%2520interact%2520with%2520the%2520physical%2520environment.%2520Traditional%250Amethods%2520rely%2520on%2520detailed%25203D%2520maps%2520constructed%2520using%2520Structure%2520from%2520Motion%2520%2528SfM%2529%250Aor%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%252C%2520which%2520is%2520computationally%250Aexpensive%2520and%2520impractical%2520for%2520dynamic%2520or%2520large-scale%2520environments.%2520We%2520introduce%250AMARLOC%252C%2520a%2520novel%2520localization%2520framework%2520for%2520AR%2520applications%2520that%2520uses%2520known%250Arelative%2520transformations%2520within%2520image%2520sequences%2520to%2520perform%2520intra-sequence%250Atriangulation%252C%2520generating%25203D-2D%2520correspondences%2520for%2520pose%2520estimation%2520and%250Arefinement.%2520MARLOC%2520eliminates%2520the%2520need%2520for%2520pre-built%2520SfM%2520maps%252C%2520providing%250Aaccurate%2520and%2520efficient%2520localization%2520suitable%2520for%2520dynamic%2520outdoor%2520environments.%250AEvaluation%2520with%2520benchmark%2520datasets%2520and%2520real-world%2520experiments%2520demonstrates%250AMARLOC%2527s%2520state-of-the-art%2520performance%2520and%2520robustness.%2520By%2520integrating%2520MARLOC%250Ainto%2520an%2520AR%2520device%252C%2520we%2520highlight%2520its%2520capability%2520to%2520achieve%2520precise%2520localization%250Ain%2520real-world%2520outdoor%2520scenarios%252C%2520showcasing%2520its%2520practical%2520effectiveness%2520and%250Apotential%2520to%2520enhance%2520visual%2520localization%2520in%2520AR%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmented%20Reality%20without%20Borders%3A%20Achieving%20Precise%20Localization%0A%20%20Without%20Maps&entry.906535625=Albert%20Gassol%20Puigjaner%20and%20Irvin%20Aloise%20and%20Patrik%20Schmuck&entry.1292438233=%20%20Visual%20localization%20is%20crucial%20for%20Computer%20Vision%20and%20Augmented%20Reality%20%28AR%29%0Aapplications%2C%20where%20determining%20the%20camera%20or%20device%27s%20position%20and%20orientation%0Ais%20essential%20to%20accurately%20interact%20with%20the%20physical%20environment.%20Traditional%0Amethods%20rely%20on%20detailed%203D%20maps%20constructed%20using%20Structure%20from%20Motion%20%28SfM%29%0Aor%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%2C%20which%20is%20computationally%0Aexpensive%20and%20impractical%20for%20dynamic%20or%20large-scale%20environments.%20We%20introduce%0AMARLOC%2C%20a%20novel%20localization%20framework%20for%20AR%20applications%20that%20uses%20known%0Arelative%20transformations%20within%20image%20sequences%20to%20perform%20intra-sequence%0Atriangulation%2C%20generating%203D-2D%20correspondences%20for%20pose%20estimation%20and%0Arefinement.%20MARLOC%20eliminates%20the%20need%20for%20pre-built%20SfM%20maps%2C%20providing%0Aaccurate%20and%20efficient%20localization%20suitable%20for%20dynamic%20outdoor%20environments.%0AEvaluation%20with%20benchmark%20datasets%20and%20real-world%20experiments%20demonstrates%0AMARLOC%27s%20state-of-the-art%20performance%20and%20robustness.%20By%20integrating%20MARLOC%0Ainto%20an%20AR%20device%2C%20we%20highlight%20its%20capability%20to%20achieve%20precise%20localization%0Ain%20real-world%20outdoor%20scenarios%2C%20showcasing%20its%20practical%20effectiveness%20and%0Apotential%20to%20enhance%20visual%20localization%20in%20AR%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17373v1&entry.124074799=Read"},
{"title": "Revisiting 360 Depth Estimation with PanoGabor: A New Fusion Perspective", "author": "Zhijie Shen and Chunyu Lin and Lang Nie and Kang Liao", "abstract": "  Depth estimation from a monocular 360 image is important to the perception of\nthe entire 3D environment. However, the inherent distortion and large field of\nview (FoV) in 360 images pose great challenges for this task. To this end,\nexisting mainstream solutions typically introduce additional perspective-based\n360 representations (\\textit{e.g.}, Cubemap) to achieve effective feature\nextraction. Nevertheless, regardless of the introduced representations, they\neventually need to be unified into the equirectangular projection (ERP) format\nfor the subsequent depth estimation, which inevitably reintroduces the\ntroublesome distortions. In this work, we propose an oriented distortion-aware\nGabor Fusion framework (PGFuse) to address the above challenges. First, we\nintroduce Gabor filters that analyze texture in the frequency domain, thereby\nextending the receptive fields and enhancing depth cues. To address the\nreintroduced distortions, we design a linear latitude-aware distortion\nrepresentation method to generate customized, distortion-aware Gabor filters\n(PanoGabor filters). Furthermore, we design a channel-wise and spatial-wise\nunidirectional fusion module (CS-UFM) that integrates the proposed PanoGabor\nfilters to unify other representations into the ERP format, delivering\neffective and distortion-free features. Considering the orientation sensitivity\nof the Gabor transform, we introduce a spherical gradient constraint to\nstabilize this sensitivity. Experimental results on three popular indoor 360\nbenchmarks demonstrate the superiority of the proposed PGFuse to existing\nstate-of-the-art solutions. Code can be available upon acceptance.\n", "link": "http://arxiv.org/abs/2408.16227v2", "date": "2024-08-30", "relevancy": 2.7812, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5604}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5542}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20360%20Depth%20Estimation%20with%20PanoGabor%3A%20A%20New%20Fusion%20Perspective&body=Title%3A%20Revisiting%20360%20Depth%20Estimation%20with%20PanoGabor%3A%20A%20New%20Fusion%20Perspective%0AAuthor%3A%20Zhijie%20Shen%20and%20Chunyu%20Lin%20and%20Lang%20Nie%20and%20Kang%20Liao%0AAbstract%3A%20%20%20Depth%20estimation%20from%20a%20monocular%20360%20image%20is%20important%20to%20the%20perception%20of%0Athe%20entire%203D%20environment.%20However%2C%20the%20inherent%20distortion%20and%20large%20field%20of%0Aview%20%28FoV%29%20in%20360%20images%20pose%20great%20challenges%20for%20this%20task.%20To%20this%20end%2C%0Aexisting%20mainstream%20solutions%20typically%20introduce%20additional%20perspective-based%0A360%20representations%20%28%5Ctextit%7Be.g.%7D%2C%20Cubemap%29%20to%20achieve%20effective%20feature%0Aextraction.%20Nevertheless%2C%20regardless%20of%20the%20introduced%20representations%2C%20they%0Aeventually%20need%20to%20be%20unified%20into%20the%20equirectangular%20projection%20%28ERP%29%20format%0Afor%20the%20subsequent%20depth%20estimation%2C%20which%20inevitably%20reintroduces%20the%0Atroublesome%20distortions.%20In%20this%20work%2C%20we%20propose%20an%20oriented%20distortion-aware%0AGabor%20Fusion%20framework%20%28PGFuse%29%20to%20address%20the%20above%20challenges.%20First%2C%20we%0Aintroduce%20Gabor%20filters%20that%20analyze%20texture%20in%20the%20frequency%20domain%2C%20thereby%0Aextending%20the%20receptive%20fields%20and%20enhancing%20depth%20cues.%20To%20address%20the%0Areintroduced%20distortions%2C%20we%20design%20a%20linear%20latitude-aware%20distortion%0Arepresentation%20method%20to%20generate%20customized%2C%20distortion-aware%20Gabor%20filters%0A%28PanoGabor%20filters%29.%20Furthermore%2C%20we%20design%20a%20channel-wise%20and%20spatial-wise%0Aunidirectional%20fusion%20module%20%28CS-UFM%29%20that%20integrates%20the%20proposed%20PanoGabor%0Afilters%20to%20unify%20other%20representations%20into%20the%20ERP%20format%2C%20delivering%0Aeffective%20and%20distortion-free%20features.%20Considering%20the%20orientation%20sensitivity%0Aof%20the%20Gabor%20transform%2C%20we%20introduce%20a%20spherical%20gradient%20constraint%20to%0Astabilize%20this%20sensitivity.%20Experimental%20results%20on%20three%20popular%20indoor%20360%0Abenchmarks%20demonstrate%20the%20superiority%20of%20the%20proposed%20PGFuse%20to%20existing%0Astate-of-the-art%20solutions.%20Code%20can%20be%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16227v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520360%2520Depth%2520Estimation%2520with%2520PanoGabor%253A%2520A%2520New%2520Fusion%2520Perspective%26entry.906535625%3DZhijie%2520Shen%2520and%2520Chunyu%2520Lin%2520and%2520Lang%2520Nie%2520and%2520Kang%2520Liao%26entry.1292438233%3D%2520%2520Depth%2520estimation%2520from%2520a%2520monocular%2520360%2520image%2520is%2520important%2520to%2520the%2520perception%2520of%250Athe%2520entire%25203D%2520environment.%2520However%252C%2520the%2520inherent%2520distortion%2520and%2520large%2520field%2520of%250Aview%2520%2528FoV%2529%2520in%2520360%2520images%2520pose%2520great%2520challenges%2520for%2520this%2520task.%2520To%2520this%2520end%252C%250Aexisting%2520mainstream%2520solutions%2520typically%2520introduce%2520additional%2520perspective-based%250A360%2520representations%2520%2528%255Ctextit%257Be.g.%257D%252C%2520Cubemap%2529%2520to%2520achieve%2520effective%2520feature%250Aextraction.%2520Nevertheless%252C%2520regardless%2520of%2520the%2520introduced%2520representations%252C%2520they%250Aeventually%2520need%2520to%2520be%2520unified%2520into%2520the%2520equirectangular%2520projection%2520%2528ERP%2529%2520format%250Afor%2520the%2520subsequent%2520depth%2520estimation%252C%2520which%2520inevitably%2520reintroduces%2520the%250Atroublesome%2520distortions.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520oriented%2520distortion-aware%250AGabor%2520Fusion%2520framework%2520%2528PGFuse%2529%2520to%2520address%2520the%2520above%2520challenges.%2520First%252C%2520we%250Aintroduce%2520Gabor%2520filters%2520that%2520analyze%2520texture%2520in%2520the%2520frequency%2520domain%252C%2520thereby%250Aextending%2520the%2520receptive%2520fields%2520and%2520enhancing%2520depth%2520cues.%2520To%2520address%2520the%250Areintroduced%2520distortions%252C%2520we%2520design%2520a%2520linear%2520latitude-aware%2520distortion%250Arepresentation%2520method%2520to%2520generate%2520customized%252C%2520distortion-aware%2520Gabor%2520filters%250A%2528PanoGabor%2520filters%2529.%2520Furthermore%252C%2520we%2520design%2520a%2520channel-wise%2520and%2520spatial-wise%250Aunidirectional%2520fusion%2520module%2520%2528CS-UFM%2529%2520that%2520integrates%2520the%2520proposed%2520PanoGabor%250Afilters%2520to%2520unify%2520other%2520representations%2520into%2520the%2520ERP%2520format%252C%2520delivering%250Aeffective%2520and%2520distortion-free%2520features.%2520Considering%2520the%2520orientation%2520sensitivity%250Aof%2520the%2520Gabor%2520transform%252C%2520we%2520introduce%2520a%2520spherical%2520gradient%2520constraint%2520to%250Astabilize%2520this%2520sensitivity.%2520Experimental%2520results%2520on%2520three%2520popular%2520indoor%2520360%250Abenchmarks%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520PGFuse%2520to%2520existing%250Astate-of-the-art%2520solutions.%2520Code%2520can%2520be%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16227v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20360%20Depth%20Estimation%20with%20PanoGabor%3A%20A%20New%20Fusion%20Perspective&entry.906535625=Zhijie%20Shen%20and%20Chunyu%20Lin%20and%20Lang%20Nie%20and%20Kang%20Liao&entry.1292438233=%20%20Depth%20estimation%20from%20a%20monocular%20360%20image%20is%20important%20to%20the%20perception%20of%0Athe%20entire%203D%20environment.%20However%2C%20the%20inherent%20distortion%20and%20large%20field%20of%0Aview%20%28FoV%29%20in%20360%20images%20pose%20great%20challenges%20for%20this%20task.%20To%20this%20end%2C%0Aexisting%20mainstream%20solutions%20typically%20introduce%20additional%20perspective-based%0A360%20representations%20%28%5Ctextit%7Be.g.%7D%2C%20Cubemap%29%20to%20achieve%20effective%20feature%0Aextraction.%20Nevertheless%2C%20regardless%20of%20the%20introduced%20representations%2C%20they%0Aeventually%20need%20to%20be%20unified%20into%20the%20equirectangular%20projection%20%28ERP%29%20format%0Afor%20the%20subsequent%20depth%20estimation%2C%20which%20inevitably%20reintroduces%20the%0Atroublesome%20distortions.%20In%20this%20work%2C%20we%20propose%20an%20oriented%20distortion-aware%0AGabor%20Fusion%20framework%20%28PGFuse%29%20to%20address%20the%20above%20challenges.%20First%2C%20we%0Aintroduce%20Gabor%20filters%20that%20analyze%20texture%20in%20the%20frequency%20domain%2C%20thereby%0Aextending%20the%20receptive%20fields%20and%20enhancing%20depth%20cues.%20To%20address%20the%0Areintroduced%20distortions%2C%20we%20design%20a%20linear%20latitude-aware%20distortion%0Arepresentation%20method%20to%20generate%20customized%2C%20distortion-aware%20Gabor%20filters%0A%28PanoGabor%20filters%29.%20Furthermore%2C%20we%20design%20a%20channel-wise%20and%20spatial-wise%0Aunidirectional%20fusion%20module%20%28CS-UFM%29%20that%20integrates%20the%20proposed%20PanoGabor%0Afilters%20to%20unify%20other%20representations%20into%20the%20ERP%20format%2C%20delivering%0Aeffective%20and%20distortion-free%20features.%20Considering%20the%20orientation%20sensitivity%0Aof%20the%20Gabor%20transform%2C%20we%20introduce%20a%20spherical%20gradient%20constraint%20to%0Astabilize%20this%20sensitivity.%20Experimental%20results%20on%20three%20popular%20indoor%20360%0Abenchmarks%20demonstrate%20the%20superiority%20of%20the%20proposed%20PGFuse%20to%20existing%0Astate-of-the-art%20solutions.%20Code%20can%20be%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16227v2&entry.124074799=Read"},
{"title": "Abstracted Gaussian Prototypes for One-Shot Concept Learning", "author": "Chelsea Zou and Kenneth J. Kurtz", "abstract": "  We introduce a cluster-based generative image segmentation framework to\nencode higher-level representations of visual concepts based on one-shot\nlearning inspired by the Omniglot Challenge. The inferred parameters of each\ncomponent of a Gaussian Mixture Model (GMM) represent a distinct topological\nsubpart of a visual concept. Sampling new data from these parameters generates\naugmented subparts to build a more robust prototype for each concept, i.e., the\nAbstracted Gaussian Prototype (AGP). This framework addresses one-shot\nclassification tasks using a cognitively-inspired similarity metric and\naddresses one-shot generative tasks through a novel AGP-VAE pipeline employing\nvariational autoencoders (VAEs) to generate new class variants. Results from\nhuman judges reveal that the generative pipeline produces novel examples and\nclasses of visual concepts that are broadly indistinguishable from those made\nby humans. The proposed framework leads to impressive but not state-of-the-art\nclassification accuracy; thus, the contribution is two-fold: 1) the system is\nuniquely low in theoretical and computational complexity and operates in a\ncompletely standalone manner compared while existing approaches draw heavily on\npre-training or knowledge engineering; and 2) in contrast with competing neural\nnetwork models, the AGP approach addresses the importance of breadth of task\ncapability emphasized in the Omniglot challenge (i.e., successful performance\non generative tasks). These two points are critical as we advance toward an\nunderstanding of how learning/reasoning systems can produce viable, robust, and\nflexible concepts based on literally nothing more than a single example.\n", "link": "http://arxiv.org/abs/2408.17251v1", "date": "2024-08-30", "relevancy": 2.7646, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5543}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5538}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Abstracted%20Gaussian%20Prototypes%20for%20One-Shot%20Concept%20Learning&body=Title%3A%20Abstracted%20Gaussian%20Prototypes%20for%20One-Shot%20Concept%20Learning%0AAuthor%3A%20Chelsea%20Zou%20and%20Kenneth%20J.%20Kurtz%0AAbstract%3A%20%20%20We%20introduce%20a%20cluster-based%20generative%20image%20segmentation%20framework%20to%0Aencode%20higher-level%20representations%20of%20visual%20concepts%20based%20on%20one-shot%0Alearning%20inspired%20by%20the%20Omniglot%20Challenge.%20The%20inferred%20parameters%20of%20each%0Acomponent%20of%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20represent%20a%20distinct%20topological%0Asubpart%20of%20a%20visual%20concept.%20Sampling%20new%20data%20from%20these%20parameters%20generates%0Aaugmented%20subparts%20to%20build%20a%20more%20robust%20prototype%20for%20each%20concept%2C%20i.e.%2C%20the%0AAbstracted%20Gaussian%20Prototype%20%28AGP%29.%20This%20framework%20addresses%20one-shot%0Aclassification%20tasks%20using%20a%20cognitively-inspired%20similarity%20metric%20and%0Aaddresses%20one-shot%20generative%20tasks%20through%20a%20novel%20AGP-VAE%20pipeline%20employing%0Avariational%20autoencoders%20%28VAEs%29%20to%20generate%20new%20class%20variants.%20Results%20from%0Ahuman%20judges%20reveal%20that%20the%20generative%20pipeline%20produces%20novel%20examples%20and%0Aclasses%20of%20visual%20concepts%20that%20are%20broadly%20indistinguishable%20from%20those%20made%0Aby%20humans.%20The%20proposed%20framework%20leads%20to%20impressive%20but%20not%20state-of-the-art%0Aclassification%20accuracy%3B%20thus%2C%20the%20contribution%20is%20two-fold%3A%201%29%20the%20system%20is%0Auniquely%20low%20in%20theoretical%20and%20computational%20complexity%20and%20operates%20in%20a%0Acompletely%20standalone%20manner%20compared%20while%20existing%20approaches%20draw%20heavily%20on%0Apre-training%20or%20knowledge%20engineering%3B%20and%202%29%20in%20contrast%20with%20competing%20neural%0Anetwork%20models%2C%20the%20AGP%20approach%20addresses%20the%20importance%20of%20breadth%20of%20task%0Acapability%20emphasized%20in%20the%20Omniglot%20challenge%20%28i.e.%2C%20successful%20performance%0Aon%20generative%20tasks%29.%20These%20two%20points%20are%20critical%20as%20we%20advance%20toward%20an%0Aunderstanding%20of%20how%20learning/reasoning%20systems%20can%20produce%20viable%2C%20robust%2C%20and%0Aflexible%20concepts%20based%20on%20literally%20nothing%20more%20than%20a%20single%20example.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbstracted%2520Gaussian%2520Prototypes%2520for%2520One-Shot%2520Concept%2520Learning%26entry.906535625%3DChelsea%2520Zou%2520and%2520Kenneth%2520J.%2520Kurtz%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520cluster-based%2520generative%2520image%2520segmentation%2520framework%2520to%250Aencode%2520higher-level%2520representations%2520of%2520visual%2520concepts%2520based%2520on%2520one-shot%250Alearning%2520inspired%2520by%2520the%2520Omniglot%2520Challenge.%2520The%2520inferred%2520parameters%2520of%2520each%250Acomponent%2520of%2520a%2520Gaussian%2520Mixture%2520Model%2520%2528GMM%2529%2520represent%2520a%2520distinct%2520topological%250Asubpart%2520of%2520a%2520visual%2520concept.%2520Sampling%2520new%2520data%2520from%2520these%2520parameters%2520generates%250Aaugmented%2520subparts%2520to%2520build%2520a%2520more%2520robust%2520prototype%2520for%2520each%2520concept%252C%2520i.e.%252C%2520the%250AAbstracted%2520Gaussian%2520Prototype%2520%2528AGP%2529.%2520This%2520framework%2520addresses%2520one-shot%250Aclassification%2520tasks%2520using%2520a%2520cognitively-inspired%2520similarity%2520metric%2520and%250Aaddresses%2520one-shot%2520generative%2520tasks%2520through%2520a%2520novel%2520AGP-VAE%2520pipeline%2520employing%250Avariational%2520autoencoders%2520%2528VAEs%2529%2520to%2520generate%2520new%2520class%2520variants.%2520Results%2520from%250Ahuman%2520judges%2520reveal%2520that%2520the%2520generative%2520pipeline%2520produces%2520novel%2520examples%2520and%250Aclasses%2520of%2520visual%2520concepts%2520that%2520are%2520broadly%2520indistinguishable%2520from%2520those%2520made%250Aby%2520humans.%2520The%2520proposed%2520framework%2520leads%2520to%2520impressive%2520but%2520not%2520state-of-the-art%250Aclassification%2520accuracy%253B%2520thus%252C%2520the%2520contribution%2520is%2520two-fold%253A%25201%2529%2520the%2520system%2520is%250Auniquely%2520low%2520in%2520theoretical%2520and%2520computational%2520complexity%2520and%2520operates%2520in%2520a%250Acompletely%2520standalone%2520manner%2520compared%2520while%2520existing%2520approaches%2520draw%2520heavily%2520on%250Apre-training%2520or%2520knowledge%2520engineering%253B%2520and%25202%2529%2520in%2520contrast%2520with%2520competing%2520neural%250Anetwork%2520models%252C%2520the%2520AGP%2520approach%2520addresses%2520the%2520importance%2520of%2520breadth%2520of%2520task%250Acapability%2520emphasized%2520in%2520the%2520Omniglot%2520challenge%2520%2528i.e.%252C%2520successful%2520performance%250Aon%2520generative%2520tasks%2529.%2520These%2520two%2520points%2520are%2520critical%2520as%2520we%2520advance%2520toward%2520an%250Aunderstanding%2520of%2520how%2520learning/reasoning%2520systems%2520can%2520produce%2520viable%252C%2520robust%252C%2520and%250Aflexible%2520concepts%2520based%2520on%2520literally%2520nothing%2520more%2520than%2520a%2520single%2520example.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Abstracted%20Gaussian%20Prototypes%20for%20One-Shot%20Concept%20Learning&entry.906535625=Chelsea%20Zou%20and%20Kenneth%20J.%20Kurtz&entry.1292438233=%20%20We%20introduce%20a%20cluster-based%20generative%20image%20segmentation%20framework%20to%0Aencode%20higher-level%20representations%20of%20visual%20concepts%20based%20on%20one-shot%0Alearning%20inspired%20by%20the%20Omniglot%20Challenge.%20The%20inferred%20parameters%20of%20each%0Acomponent%20of%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20represent%20a%20distinct%20topological%0Asubpart%20of%20a%20visual%20concept.%20Sampling%20new%20data%20from%20these%20parameters%20generates%0Aaugmented%20subparts%20to%20build%20a%20more%20robust%20prototype%20for%20each%20concept%2C%20i.e.%2C%20the%0AAbstracted%20Gaussian%20Prototype%20%28AGP%29.%20This%20framework%20addresses%20one-shot%0Aclassification%20tasks%20using%20a%20cognitively-inspired%20similarity%20metric%20and%0Aaddresses%20one-shot%20generative%20tasks%20through%20a%20novel%20AGP-VAE%20pipeline%20employing%0Avariational%20autoencoders%20%28VAEs%29%20to%20generate%20new%20class%20variants.%20Results%20from%0Ahuman%20judges%20reveal%20that%20the%20generative%20pipeline%20produces%20novel%20examples%20and%0Aclasses%20of%20visual%20concepts%20that%20are%20broadly%20indistinguishable%20from%20those%20made%0Aby%20humans.%20The%20proposed%20framework%20leads%20to%20impressive%20but%20not%20state-of-the-art%0Aclassification%20accuracy%3B%20thus%2C%20the%20contribution%20is%20two-fold%3A%201%29%20the%20system%20is%0Auniquely%20low%20in%20theoretical%20and%20computational%20complexity%20and%20operates%20in%20a%0Acompletely%20standalone%20manner%20compared%20while%20existing%20approaches%20draw%20heavily%20on%0Apre-training%20or%20knowledge%20engineering%3B%20and%202%29%20in%20contrast%20with%20competing%20neural%0Anetwork%20models%2C%20the%20AGP%20approach%20addresses%20the%20importance%20of%20breadth%20of%20task%0Acapability%20emphasized%20in%20the%20Omniglot%20challenge%20%28i.e.%2C%20successful%20performance%0Aon%20generative%20tasks%29.%20These%20two%20points%20are%20critical%20as%20we%20advance%20toward%20an%0Aunderstanding%20of%20how%20learning/reasoning%20systems%20can%20produce%20viable%2C%20robust%2C%20and%0Aflexible%20concepts%20based%20on%20literally%20nothing%20more%20than%20a%20single%20example.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17251v1&entry.124074799=Read"},
{"title": "Large coordinate kernel attention network for lightweight image\n  super-resolution", "author": "Fangwei Hao and Jiesheng Wu and Haotian Lu and Ji Du and Jing Xu and Xiaoxuan Xu", "abstract": "  The multi-scale receptive field and large kernel attention (LKA) module have\nbeen shown to significantly improve performance in the lightweight image\nsuper-resolution task. However, existing lightweight super-resolution (SR)\nmethods seldom pay attention to designing efficient building block with\nmulti-scale receptive field for local modeling, and their LKA modules face a\nquadratic increase in computational and memory footprints as the convolutional\nkernel size increases. To address the first issue, we propose the multi-scale\nblueprint separable convolutions (MBSConv) as highly efficient building block\nwith multi-scale receptive field, it can focus on the learning for the\nmulti-scale information which is a vital component of discriminative\nrepresentation. As for the second issue, we revisit the key properties of LKA\nin which we find that the adjacent direct interaction of local information and\nlong-distance dependencies is crucial to provide remarkable performance. Thus,\ntaking this into account and in order to mitigate the complexity of LKA, we\npropose a large coordinate kernel attention (LCKA) module which decomposes the\n2D convolutional kernels of the depth-wise convolutional layers in LKA into\nhorizontal and vertical 1-D kernels. LCKA enables the adjacent direct\ninteraction of local information and long-distance dependencies not only in the\nhorizontal direction but also in the vertical. Besides, LCKA allows for the\ndirect use of extremely large kernels in the depth-wise convolutional layers to\ncapture more contextual information, which helps to significantly improve the\nreconstruction performance, and it incurs lower computational complexity and\nmemory footprints. Integrating MBSConv and LCKA, we propose a large coordinate\nkernel attention network (LCAN).\n", "link": "http://arxiv.org/abs/2405.09353v2", "date": "2024-08-30", "relevancy": 2.7541, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5818}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5366}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20coordinate%20kernel%20attention%20network%20for%20lightweight%20image%0A%20%20super-resolution&body=Title%3A%20Large%20coordinate%20kernel%20attention%20network%20for%20lightweight%20image%0A%20%20super-resolution%0AAuthor%3A%20Fangwei%20Hao%20and%20Jiesheng%20Wu%20and%20Haotian%20Lu%20and%20Ji%20Du%20and%20Jing%20Xu%20and%20Xiaoxuan%20Xu%0AAbstract%3A%20%20%20The%20multi-scale%20receptive%20field%20and%20large%20kernel%20attention%20%28LKA%29%20module%20have%0Abeen%20shown%20to%20significantly%20improve%20performance%20in%20the%20lightweight%20image%0Asuper-resolution%20task.%20However%2C%20existing%20lightweight%20super-resolution%20%28SR%29%0Amethods%20seldom%20pay%20attention%20to%20designing%20efficient%20building%20block%20with%0Amulti-scale%20receptive%20field%20for%20local%20modeling%2C%20and%20their%20LKA%20modules%20face%20a%0Aquadratic%20increase%20in%20computational%20and%20memory%20footprints%20as%20the%20convolutional%0Akernel%20size%20increases.%20To%20address%20the%20first%20issue%2C%20we%20propose%20the%20multi-scale%0Ablueprint%20separable%20convolutions%20%28MBSConv%29%20as%20highly%20efficient%20building%20block%0Awith%20multi-scale%20receptive%20field%2C%20it%20can%20focus%20on%20the%20learning%20for%20the%0Amulti-scale%20information%20which%20is%20a%20vital%20component%20of%20discriminative%0Arepresentation.%20As%20for%20the%20second%20issue%2C%20we%20revisit%20the%20key%20properties%20of%20LKA%0Ain%20which%20we%20find%20that%20the%20adjacent%20direct%20interaction%20of%20local%20information%20and%0Along-distance%20dependencies%20is%20crucial%20to%20provide%20remarkable%20performance.%20Thus%2C%0Ataking%20this%20into%20account%20and%20in%20order%20to%20mitigate%20the%20complexity%20of%20LKA%2C%20we%0Apropose%20a%20large%20coordinate%20kernel%20attention%20%28LCKA%29%20module%20which%20decomposes%20the%0A2D%20convolutional%20kernels%20of%20the%20depth-wise%20convolutional%20layers%20in%20LKA%20into%0Ahorizontal%20and%20vertical%201-D%20kernels.%20LCKA%20enables%20the%20adjacent%20direct%0Ainteraction%20of%20local%20information%20and%20long-distance%20dependencies%20not%20only%20in%20the%0Ahorizontal%20direction%20but%20also%20in%20the%20vertical.%20Besides%2C%20LCKA%20allows%20for%20the%0Adirect%20use%20of%20extremely%20large%20kernels%20in%20the%20depth-wise%20convolutional%20layers%20to%0Acapture%20more%20contextual%20information%2C%20which%20helps%20to%20significantly%20improve%20the%0Areconstruction%20performance%2C%20and%20it%20incurs%20lower%20computational%20complexity%20and%0Amemory%20footprints.%20Integrating%20MBSConv%20and%20LCKA%2C%20we%20propose%20a%20large%20coordinate%0Akernel%20attention%20network%20%28LCAN%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09353v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520coordinate%2520kernel%2520attention%2520network%2520for%2520lightweight%2520image%250A%2520%2520super-resolution%26entry.906535625%3DFangwei%2520Hao%2520and%2520Jiesheng%2520Wu%2520and%2520Haotian%2520Lu%2520and%2520Ji%2520Du%2520and%2520Jing%2520Xu%2520and%2520Xiaoxuan%2520Xu%26entry.1292438233%3D%2520%2520The%2520multi-scale%2520receptive%2520field%2520and%2520large%2520kernel%2520attention%2520%2528LKA%2529%2520module%2520have%250Abeen%2520shown%2520to%2520significantly%2520improve%2520performance%2520in%2520the%2520lightweight%2520image%250Asuper-resolution%2520task.%2520However%252C%2520existing%2520lightweight%2520super-resolution%2520%2528SR%2529%250Amethods%2520seldom%2520pay%2520attention%2520to%2520designing%2520efficient%2520building%2520block%2520with%250Amulti-scale%2520receptive%2520field%2520for%2520local%2520modeling%252C%2520and%2520their%2520LKA%2520modules%2520face%2520a%250Aquadratic%2520increase%2520in%2520computational%2520and%2520memory%2520footprints%2520as%2520the%2520convolutional%250Akernel%2520size%2520increases.%2520To%2520address%2520the%2520first%2520issue%252C%2520we%2520propose%2520the%2520multi-scale%250Ablueprint%2520separable%2520convolutions%2520%2528MBSConv%2529%2520as%2520highly%2520efficient%2520building%2520block%250Awith%2520multi-scale%2520receptive%2520field%252C%2520it%2520can%2520focus%2520on%2520the%2520learning%2520for%2520the%250Amulti-scale%2520information%2520which%2520is%2520a%2520vital%2520component%2520of%2520discriminative%250Arepresentation.%2520As%2520for%2520the%2520second%2520issue%252C%2520we%2520revisit%2520the%2520key%2520properties%2520of%2520LKA%250Ain%2520which%2520we%2520find%2520that%2520the%2520adjacent%2520direct%2520interaction%2520of%2520local%2520information%2520and%250Along-distance%2520dependencies%2520is%2520crucial%2520to%2520provide%2520remarkable%2520performance.%2520Thus%252C%250Ataking%2520this%2520into%2520account%2520and%2520in%2520order%2520to%2520mitigate%2520the%2520complexity%2520of%2520LKA%252C%2520we%250Apropose%2520a%2520large%2520coordinate%2520kernel%2520attention%2520%2528LCKA%2529%2520module%2520which%2520decomposes%2520the%250A2D%2520convolutional%2520kernels%2520of%2520the%2520depth-wise%2520convolutional%2520layers%2520in%2520LKA%2520into%250Ahorizontal%2520and%2520vertical%25201-D%2520kernels.%2520LCKA%2520enables%2520the%2520adjacent%2520direct%250Ainteraction%2520of%2520local%2520information%2520and%2520long-distance%2520dependencies%2520not%2520only%2520in%2520the%250Ahorizontal%2520direction%2520but%2520also%2520in%2520the%2520vertical.%2520Besides%252C%2520LCKA%2520allows%2520for%2520the%250Adirect%2520use%2520of%2520extremely%2520large%2520kernels%2520in%2520the%2520depth-wise%2520convolutional%2520layers%2520to%250Acapture%2520more%2520contextual%2520information%252C%2520which%2520helps%2520to%2520significantly%2520improve%2520the%250Areconstruction%2520performance%252C%2520and%2520it%2520incurs%2520lower%2520computational%2520complexity%2520and%250Amemory%2520footprints.%2520Integrating%2520MBSConv%2520and%2520LCKA%252C%2520we%2520propose%2520a%2520large%2520coordinate%250Akernel%2520attention%2520network%2520%2528LCAN%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09353v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20coordinate%20kernel%20attention%20network%20for%20lightweight%20image%0A%20%20super-resolution&entry.906535625=Fangwei%20Hao%20and%20Jiesheng%20Wu%20and%20Haotian%20Lu%20and%20Ji%20Du%20and%20Jing%20Xu%20and%20Xiaoxuan%20Xu&entry.1292438233=%20%20The%20multi-scale%20receptive%20field%20and%20large%20kernel%20attention%20%28LKA%29%20module%20have%0Abeen%20shown%20to%20significantly%20improve%20performance%20in%20the%20lightweight%20image%0Asuper-resolution%20task.%20However%2C%20existing%20lightweight%20super-resolution%20%28SR%29%0Amethods%20seldom%20pay%20attention%20to%20designing%20efficient%20building%20block%20with%0Amulti-scale%20receptive%20field%20for%20local%20modeling%2C%20and%20their%20LKA%20modules%20face%20a%0Aquadratic%20increase%20in%20computational%20and%20memory%20footprints%20as%20the%20convolutional%0Akernel%20size%20increases.%20To%20address%20the%20first%20issue%2C%20we%20propose%20the%20multi-scale%0Ablueprint%20separable%20convolutions%20%28MBSConv%29%20as%20highly%20efficient%20building%20block%0Awith%20multi-scale%20receptive%20field%2C%20it%20can%20focus%20on%20the%20learning%20for%20the%0Amulti-scale%20information%20which%20is%20a%20vital%20component%20of%20discriminative%0Arepresentation.%20As%20for%20the%20second%20issue%2C%20we%20revisit%20the%20key%20properties%20of%20LKA%0Ain%20which%20we%20find%20that%20the%20adjacent%20direct%20interaction%20of%20local%20information%20and%0Along-distance%20dependencies%20is%20crucial%20to%20provide%20remarkable%20performance.%20Thus%2C%0Ataking%20this%20into%20account%20and%20in%20order%20to%20mitigate%20the%20complexity%20of%20LKA%2C%20we%0Apropose%20a%20large%20coordinate%20kernel%20attention%20%28LCKA%29%20module%20which%20decomposes%20the%0A2D%20convolutional%20kernels%20of%20the%20depth-wise%20convolutional%20layers%20in%20LKA%20into%0Ahorizontal%20and%20vertical%201-D%20kernels.%20LCKA%20enables%20the%20adjacent%20direct%0Ainteraction%20of%20local%20information%20and%20long-distance%20dependencies%20not%20only%20in%20the%0Ahorizontal%20direction%20but%20also%20in%20the%20vertical.%20Besides%2C%20LCKA%20allows%20for%20the%0Adirect%20use%20of%20extremely%20large%20kernels%20in%20the%20depth-wise%20convolutional%20layers%20to%0Acapture%20more%20contextual%20information%2C%20which%20helps%20to%20significantly%20improve%20the%0Areconstruction%20performance%2C%20and%20it%20incurs%20lower%20computational%20complexity%20and%0Amemory%20footprints.%20Integrating%20MBSConv%20and%20LCKA%2C%20we%20propose%20a%20large%20coordinate%0Akernel%20attention%20network%20%28LCAN%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09353v2&entry.124074799=Read"},
{"title": "Addressing the challenges of loop detection in agricultural environments", "author": "Nicol\u00e1s Soncini and Javier Civera and Taih\u00fa Pire", "abstract": "  While visual SLAM systems are well studied and achieve impressive results in\nindoor and urban settings, natural, outdoor and open-field environments are\nmuch less explored and still present relevant research challenges. Visual\nnavigation and local mapping have shown a relatively good performance in\nopen-field environments. However, globally consistent mapping and long-term\nlocalization still depend on the robustness of loop detection and closure, for\nwhich the literature is scarce. In this work we propose a novel method to pave\nthe way towards robust loop detection in open fields, particularly in\nagricultural settings, based on local feature search and stereo geometric\nrefinement, with a final stage of relative pose estimation. Our method\nconsistently achieves good loop detections, with a median error of 15cm. We aim\nto characterize open fields as a novel environment for loop detection,\nunderstanding the limitations and problems that arise when dealing with them.\n", "link": "http://arxiv.org/abs/2408.15761v2", "date": "2024-08-30", "relevancy": 2.7045, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5777}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.525}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20the%20challenges%20of%20loop%20detection%20in%20agricultural%20environments&body=Title%3A%20Addressing%20the%20challenges%20of%20loop%20detection%20in%20agricultural%20environments%0AAuthor%3A%20Nicol%C3%A1s%20Soncini%20and%20Javier%20Civera%20and%20Taih%C3%BA%20Pire%0AAbstract%3A%20%20%20While%20visual%20SLAM%20systems%20are%20well%20studied%20and%20achieve%20impressive%20results%20in%0Aindoor%20and%20urban%20settings%2C%20natural%2C%20outdoor%20and%20open-field%20environments%20are%0Amuch%20less%20explored%20and%20still%20present%20relevant%20research%20challenges.%20Visual%0Anavigation%20and%20local%20mapping%20have%20shown%20a%20relatively%20good%20performance%20in%0Aopen-field%20environments.%20However%2C%20globally%20consistent%20mapping%20and%20long-term%0Alocalization%20still%20depend%20on%20the%20robustness%20of%20loop%20detection%20and%20closure%2C%20for%0Awhich%20the%20literature%20is%20scarce.%20In%20this%20work%20we%20propose%20a%20novel%20method%20to%20pave%0Athe%20way%20towards%20robust%20loop%20detection%20in%20open%20fields%2C%20particularly%20in%0Aagricultural%20settings%2C%20based%20on%20local%20feature%20search%20and%20stereo%20geometric%0Arefinement%2C%20with%20a%20final%20stage%20of%20relative%20pose%20estimation.%20Our%20method%0Aconsistently%20achieves%20good%20loop%20detections%2C%20with%20a%20median%20error%20of%2015cm.%20We%20aim%0Ato%20characterize%20open%20fields%20as%20a%20novel%20environment%20for%20loop%20detection%2C%0Aunderstanding%20the%20limitations%20and%20problems%20that%20arise%20when%20dealing%20with%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15761v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520the%2520challenges%2520of%2520loop%2520detection%2520in%2520agricultural%2520environments%26entry.906535625%3DNicol%25C3%25A1s%2520Soncini%2520and%2520Javier%2520Civera%2520and%2520Taih%25C3%25BA%2520Pire%26entry.1292438233%3D%2520%2520While%2520visual%2520SLAM%2520systems%2520are%2520well%2520studied%2520and%2520achieve%2520impressive%2520results%2520in%250Aindoor%2520and%2520urban%2520settings%252C%2520natural%252C%2520outdoor%2520and%2520open-field%2520environments%2520are%250Amuch%2520less%2520explored%2520and%2520still%2520present%2520relevant%2520research%2520challenges.%2520Visual%250Anavigation%2520and%2520local%2520mapping%2520have%2520shown%2520a%2520relatively%2520good%2520performance%2520in%250Aopen-field%2520environments.%2520However%252C%2520globally%2520consistent%2520mapping%2520and%2520long-term%250Alocalization%2520still%2520depend%2520on%2520the%2520robustness%2520of%2520loop%2520detection%2520and%2520closure%252C%2520for%250Awhich%2520the%2520literature%2520is%2520scarce.%2520In%2520this%2520work%2520we%2520propose%2520a%2520novel%2520method%2520to%2520pave%250Athe%2520way%2520towards%2520robust%2520loop%2520detection%2520in%2520open%2520fields%252C%2520particularly%2520in%250Aagricultural%2520settings%252C%2520based%2520on%2520local%2520feature%2520search%2520and%2520stereo%2520geometric%250Arefinement%252C%2520with%2520a%2520final%2520stage%2520of%2520relative%2520pose%2520estimation.%2520Our%2520method%250Aconsistently%2520achieves%2520good%2520loop%2520detections%252C%2520with%2520a%2520median%2520error%2520of%252015cm.%2520We%2520aim%250Ato%2520characterize%2520open%2520fields%2520as%2520a%2520novel%2520environment%2520for%2520loop%2520detection%252C%250Aunderstanding%2520the%2520limitations%2520and%2520problems%2520that%2520arise%2520when%2520dealing%2520with%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15761v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20the%20challenges%20of%20loop%20detection%20in%20agricultural%20environments&entry.906535625=Nicol%C3%A1s%20Soncini%20and%20Javier%20Civera%20and%20Taih%C3%BA%20Pire&entry.1292438233=%20%20While%20visual%20SLAM%20systems%20are%20well%20studied%20and%20achieve%20impressive%20results%20in%0Aindoor%20and%20urban%20settings%2C%20natural%2C%20outdoor%20and%20open-field%20environments%20are%0Amuch%20less%20explored%20and%20still%20present%20relevant%20research%20challenges.%20Visual%0Anavigation%20and%20local%20mapping%20have%20shown%20a%20relatively%20good%20performance%20in%0Aopen-field%20environments.%20However%2C%20globally%20consistent%20mapping%20and%20long-term%0Alocalization%20still%20depend%20on%20the%20robustness%20of%20loop%20detection%20and%20closure%2C%20for%0Awhich%20the%20literature%20is%20scarce.%20In%20this%20work%20we%20propose%20a%20novel%20method%20to%20pave%0Athe%20way%20towards%20robust%20loop%20detection%20in%20open%20fields%2C%20particularly%20in%0Aagricultural%20settings%2C%20based%20on%20local%20feature%20search%20and%20stereo%20geometric%0Arefinement%2C%20with%20a%20final%20stage%20of%20relative%20pose%20estimation.%20Our%20method%0Aconsistently%20achieves%20good%20loop%20detections%2C%20with%20a%20median%20error%20of%2015cm.%20We%20aim%0Ato%20characterize%20open%20fields%20as%20a%20novel%20environment%20for%20loop%20detection%2C%0Aunderstanding%20the%20limitations%20and%20problems%20that%20arise%20when%20dealing%20with%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15761v2&entry.124074799=Read"},
{"title": "Hybrid Classification-Regression Adaptive Loss for Dense Object\n  Detection", "author": "Yanquan Huang and Liu Wei Zhen and Yun Hao and Mengyuan Zhang and Qingyao Wu and Zikun Deng and Xueming Liu and Hong Deng", "abstract": "  For object detection detectors, enhancing model performance hinges on the\nability to simultaneously consider inconsistencies across tasks and focus on\ndifficult-to-train samples. Achieving this necessitates incorporating\ninformation from both the classification and regression tasks. However, prior\nwork tends to either emphasize difficult-to-train samples within their\nrespective tasks or simply compute classification scores with IoU, often\nleading to suboptimal model performance. In this paper, we propose a Hybrid\nClassification-Regression Adaptive Loss, termed as HCRAL. Specifically, we\nintroduce the Residual of Classification and IoU (RCI) module for cross-task\nsupervision, addressing task inconsistencies, and the Conditioning Factor (CF)\nto focus on difficult-to-train samples within each task. Furthermore, we\nintroduce a new strategy named Expanded Adaptive Training Sample Selection\n(EATSS) to provide additional samples that exhibit classification and\nregression inconsistencies. To validate the effectiveness of the proposed\nmethod, we conduct extensive experiments on COCO test-dev. Experimental\nevaluations demonstrate the superiority of our approachs. Additionally, we\ndesigned experiments by separately combining the classification and regression\nloss with regular loss functions in popular one-stage models, demonstrating\nimproved performance.\n", "link": "http://arxiv.org/abs/2408.17182v1", "date": "2024-08-30", "relevancy": 2.6264, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.531}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5231}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Classification-Regression%20Adaptive%20Loss%20for%20Dense%20Object%0A%20%20Detection&body=Title%3A%20Hybrid%20Classification-Regression%20Adaptive%20Loss%20for%20Dense%20Object%0A%20%20Detection%0AAuthor%3A%20Yanquan%20Huang%20and%20Liu%20Wei%20Zhen%20and%20Yun%20Hao%20and%20Mengyuan%20Zhang%20and%20Qingyao%20Wu%20and%20Zikun%20Deng%20and%20Xueming%20Liu%20and%20Hong%20Deng%0AAbstract%3A%20%20%20For%20object%20detection%20detectors%2C%20enhancing%20model%20performance%20hinges%20on%20the%0Aability%20to%20simultaneously%20consider%20inconsistencies%20across%20tasks%20and%20focus%20on%0Adifficult-to-train%20samples.%20Achieving%20this%20necessitates%20incorporating%0Ainformation%20from%20both%20the%20classification%20and%20regression%20tasks.%20However%2C%20prior%0Awork%20tends%20to%20either%20emphasize%20difficult-to-train%20samples%20within%20their%0Arespective%20tasks%20or%20simply%20compute%20classification%20scores%20with%20IoU%2C%20often%0Aleading%20to%20suboptimal%20model%20performance.%20In%20this%20paper%2C%20we%20propose%20a%20Hybrid%0AClassification-Regression%20Adaptive%20Loss%2C%20termed%20as%20HCRAL.%20Specifically%2C%20we%0Aintroduce%20the%20Residual%20of%20Classification%20and%20IoU%20%28RCI%29%20module%20for%20cross-task%0Asupervision%2C%20addressing%20task%20inconsistencies%2C%20and%20the%20Conditioning%20Factor%20%28CF%29%0Ato%20focus%20on%20difficult-to-train%20samples%20within%20each%20task.%20Furthermore%2C%20we%0Aintroduce%20a%20new%20strategy%20named%20Expanded%20Adaptive%20Training%20Sample%20Selection%0A%28EATSS%29%20to%20provide%20additional%20samples%20that%20exhibit%20classification%20and%0Aregression%20inconsistencies.%20To%20validate%20the%20effectiveness%20of%20the%20proposed%0Amethod%2C%20we%20conduct%20extensive%20experiments%20on%20COCO%20test-dev.%20Experimental%0Aevaluations%20demonstrate%20the%20superiority%20of%20our%20approachs.%20Additionally%2C%20we%0Adesigned%20experiments%20by%20separately%20combining%20the%20classification%20and%20regression%0Aloss%20with%20regular%20loss%20functions%20in%20popular%20one-stage%20models%2C%20demonstrating%0Aimproved%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Classification-Regression%2520Adaptive%2520Loss%2520for%2520Dense%2520Object%250A%2520%2520Detection%26entry.906535625%3DYanquan%2520Huang%2520and%2520Liu%2520Wei%2520Zhen%2520and%2520Yun%2520Hao%2520and%2520Mengyuan%2520Zhang%2520and%2520Qingyao%2520Wu%2520and%2520Zikun%2520Deng%2520and%2520Xueming%2520Liu%2520and%2520Hong%2520Deng%26entry.1292438233%3D%2520%2520For%2520object%2520detection%2520detectors%252C%2520enhancing%2520model%2520performance%2520hinges%2520on%2520the%250Aability%2520to%2520simultaneously%2520consider%2520inconsistencies%2520across%2520tasks%2520and%2520focus%2520on%250Adifficult-to-train%2520samples.%2520Achieving%2520this%2520necessitates%2520incorporating%250Ainformation%2520from%2520both%2520the%2520classification%2520and%2520regression%2520tasks.%2520However%252C%2520prior%250Awork%2520tends%2520to%2520either%2520emphasize%2520difficult-to-train%2520samples%2520within%2520their%250Arespective%2520tasks%2520or%2520simply%2520compute%2520classification%2520scores%2520with%2520IoU%252C%2520often%250Aleading%2520to%2520suboptimal%2520model%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Hybrid%250AClassification-Regression%2520Adaptive%2520Loss%252C%2520termed%2520as%2520HCRAL.%2520Specifically%252C%2520we%250Aintroduce%2520the%2520Residual%2520of%2520Classification%2520and%2520IoU%2520%2528RCI%2529%2520module%2520for%2520cross-task%250Asupervision%252C%2520addressing%2520task%2520inconsistencies%252C%2520and%2520the%2520Conditioning%2520Factor%2520%2528CF%2529%250Ato%2520focus%2520on%2520difficult-to-train%2520samples%2520within%2520each%2520task.%2520Furthermore%252C%2520we%250Aintroduce%2520a%2520new%2520strategy%2520named%2520Expanded%2520Adaptive%2520Training%2520Sample%2520Selection%250A%2528EATSS%2529%2520to%2520provide%2520additional%2520samples%2520that%2520exhibit%2520classification%2520and%250Aregression%2520inconsistencies.%2520To%2520validate%2520the%2520effectiveness%2520of%2520the%2520proposed%250Amethod%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%2520COCO%2520test-dev.%2520Experimental%250Aevaluations%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approachs.%2520Additionally%252C%2520we%250Adesigned%2520experiments%2520by%2520separately%2520combining%2520the%2520classification%2520and%2520regression%250Aloss%2520with%2520regular%2520loss%2520functions%2520in%2520popular%2520one-stage%2520models%252C%2520demonstrating%250Aimproved%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Classification-Regression%20Adaptive%20Loss%20for%20Dense%20Object%0A%20%20Detection&entry.906535625=Yanquan%20Huang%20and%20Liu%20Wei%20Zhen%20and%20Yun%20Hao%20and%20Mengyuan%20Zhang%20and%20Qingyao%20Wu%20and%20Zikun%20Deng%20and%20Xueming%20Liu%20and%20Hong%20Deng&entry.1292438233=%20%20For%20object%20detection%20detectors%2C%20enhancing%20model%20performance%20hinges%20on%20the%0Aability%20to%20simultaneously%20consider%20inconsistencies%20across%20tasks%20and%20focus%20on%0Adifficult-to-train%20samples.%20Achieving%20this%20necessitates%20incorporating%0Ainformation%20from%20both%20the%20classification%20and%20regression%20tasks.%20However%2C%20prior%0Awork%20tends%20to%20either%20emphasize%20difficult-to-train%20samples%20within%20their%0Arespective%20tasks%20or%20simply%20compute%20classification%20scores%20with%20IoU%2C%20often%0Aleading%20to%20suboptimal%20model%20performance.%20In%20this%20paper%2C%20we%20propose%20a%20Hybrid%0AClassification-Regression%20Adaptive%20Loss%2C%20termed%20as%20HCRAL.%20Specifically%2C%20we%0Aintroduce%20the%20Residual%20of%20Classification%20and%20IoU%20%28RCI%29%20module%20for%20cross-task%0Asupervision%2C%20addressing%20task%20inconsistencies%2C%20and%20the%20Conditioning%20Factor%20%28CF%29%0Ato%20focus%20on%20difficult-to-train%20samples%20within%20each%20task.%20Furthermore%2C%20we%0Aintroduce%20a%20new%20strategy%20named%20Expanded%20Adaptive%20Training%20Sample%20Selection%0A%28EATSS%29%20to%20provide%20additional%20samples%20that%20exhibit%20classification%20and%0Aregression%20inconsistencies.%20To%20validate%20the%20effectiveness%20of%20the%20proposed%0Amethod%2C%20we%20conduct%20extensive%20experiments%20on%20COCO%20test-dev.%20Experimental%0Aevaluations%20demonstrate%20the%20superiority%20of%20our%20approachs.%20Additionally%2C%20we%0Adesigned%20experiments%20by%20separately%20combining%20the%20classification%20and%20regression%0Aloss%20with%20regular%20loss%20functions%20in%20popular%20one-stage%20models%2C%20demonstrating%0Aimproved%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17182v1&entry.124074799=Read"},
{"title": "DeformGS: Scene Flow in Highly Deformable Scenes for Deformable Object\n  Manipulation", "author": "Bardienus P. Duisterhof and Zhao Mandi and Yunchao Yao and Jia-Wei Liu and Jenny Seidenschwarz and Mike Zheng Shou and Deva Ramanan and Shuran Song and Stan Birchfield and Bowen Wen and Jeffrey Ichnowski", "abstract": "  Teaching robots to fold, drape, or reposition deformable objects such as\ncloth will unlock a variety of automation applications. While remarkable\nprogress has been made for rigid object manipulation, manipulating deformable\nobjects poses unique challenges, including frequent occlusions,\ninfinite-dimensional state spaces and complex dynamics. Just as object pose\nestimation and tracking have aided robots for rigid manipulation, dense 3D\ntracking (scene flow) of highly deformable objects will enable new applications\nin robotics while aiding existing approaches, such as imitation learning or\ncreating digital twins with real2sim transfer. We propose DeformGS, an approach\nto recover scene flow in highly deformable scenes, using simultaneous video\ncaptures of a dynamic scene from multiple cameras. DeformGS builds on recent\nadvances in Gaussian splatting, a method that learns the properties of a large\nnumber of Gaussians for state-of-the-art and fast novel-view synthesis.\nDeformGS learns a deformation function to project a set of Gaussians with\ncanonical properties into world space. The deformation function uses a\nneural-voxel encoding and a multilayer perceptron (MLP) to infer Gaussian\nposition, rotation, and a shadow scalar. We enforce physics-inspired\nregularization terms based on conservation of momentum and isometry, which\nleads to trajectories with smaller trajectory errors. We also leverage existing\nfoundation models SAM and XMEM to produce noisy masks, and learn a per-Gaussian\nmask for better physics-inspired regularization. DeformGS achieves high-quality\n3D tracking on highly deformable scenes with shadows and occlusions. In\nexperiments, DeformGS improves 3D tracking by an average of 55.8% compared to\nthe state-of-the-art. With sufficient texture, DeformGS achieves a median\ntracking error of 3.3 mm on a cloth of 1.5 x 1.5 m in area. Website:\nhttps://deformgs.github.io\n", "link": "http://arxiv.org/abs/2312.00583v2", "date": "2024-08-30", "relevancy": 2.5589, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6682}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6341}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeformGS%3A%20Scene%20Flow%20in%20Highly%20Deformable%20Scenes%20for%20Deformable%20Object%0A%20%20Manipulation&body=Title%3A%20DeformGS%3A%20Scene%20Flow%20in%20Highly%20Deformable%20Scenes%20for%20Deformable%20Object%0A%20%20Manipulation%0AAuthor%3A%20Bardienus%20P.%20Duisterhof%20and%20Zhao%20Mandi%20and%20Yunchao%20Yao%20and%20Jia-Wei%20Liu%20and%20Jenny%20Seidenschwarz%20and%20Mike%20Zheng%20Shou%20and%20Deva%20Ramanan%20and%20Shuran%20Song%20and%20Stan%20Birchfield%20and%20Bowen%20Wen%20and%20Jeffrey%20Ichnowski%0AAbstract%3A%20%20%20Teaching%20robots%20to%20fold%2C%20drape%2C%20or%20reposition%20deformable%20objects%20such%20as%0Acloth%20will%20unlock%20a%20variety%20of%20automation%20applications.%20While%20remarkable%0Aprogress%20has%20been%20made%20for%20rigid%20object%20manipulation%2C%20manipulating%20deformable%0Aobjects%20poses%20unique%20challenges%2C%20including%20frequent%20occlusions%2C%0Ainfinite-dimensional%20state%20spaces%20and%20complex%20dynamics.%20Just%20as%20object%20pose%0Aestimation%20and%20tracking%20have%20aided%20robots%20for%20rigid%20manipulation%2C%20dense%203D%0Atracking%20%28scene%20flow%29%20of%20highly%20deformable%20objects%20will%20enable%20new%20applications%0Ain%20robotics%20while%20aiding%20existing%20approaches%2C%20such%20as%20imitation%20learning%20or%0Acreating%20digital%20twins%20with%20real2sim%20transfer.%20We%20propose%20DeformGS%2C%20an%20approach%0Ato%20recover%20scene%20flow%20in%20highly%20deformable%20scenes%2C%20using%20simultaneous%20video%0Acaptures%20of%20a%20dynamic%20scene%20from%20multiple%20cameras.%20DeformGS%20builds%20on%20recent%0Aadvances%20in%20Gaussian%20splatting%2C%20a%20method%20that%20learns%20the%20properties%20of%20a%20large%0Anumber%20of%20Gaussians%20for%20state-of-the-art%20and%20fast%20novel-view%20synthesis.%0ADeformGS%20learns%20a%20deformation%20function%20to%20project%20a%20set%20of%20Gaussians%20with%0Acanonical%20properties%20into%20world%20space.%20The%20deformation%20function%20uses%20a%0Aneural-voxel%20encoding%20and%20a%20multilayer%20perceptron%20%28MLP%29%20to%20infer%20Gaussian%0Aposition%2C%20rotation%2C%20and%20a%20shadow%20scalar.%20We%20enforce%20physics-inspired%0Aregularization%20terms%20based%20on%20conservation%20of%20momentum%20and%20isometry%2C%20which%0Aleads%20to%20trajectories%20with%20smaller%20trajectory%20errors.%20We%20also%20leverage%20existing%0Afoundation%20models%20SAM%20and%20XMEM%20to%20produce%20noisy%20masks%2C%20and%20learn%20a%20per-Gaussian%0Amask%20for%20better%20physics-inspired%20regularization.%20DeformGS%20achieves%20high-quality%0A3D%20tracking%20on%20highly%20deformable%20scenes%20with%20shadows%20and%20occlusions.%20In%0Aexperiments%2C%20DeformGS%20improves%203D%20tracking%20by%20an%20average%20of%2055.8%25%20compared%20to%0Athe%20state-of-the-art.%20With%20sufficient%20texture%2C%20DeformGS%20achieves%20a%20median%0Atracking%20error%20of%203.3%20mm%20on%20a%20cloth%20of%201.5%20x%201.5%20m%20in%20area.%20Website%3A%0Ahttps%3A//deformgs.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00583v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeformGS%253A%2520Scene%2520Flow%2520in%2520Highly%2520Deformable%2520Scenes%2520for%2520Deformable%2520Object%250A%2520%2520Manipulation%26entry.906535625%3DBardienus%2520P.%2520Duisterhof%2520and%2520Zhao%2520Mandi%2520and%2520Yunchao%2520Yao%2520and%2520Jia-Wei%2520Liu%2520and%2520Jenny%2520Seidenschwarz%2520and%2520Mike%2520Zheng%2520Shou%2520and%2520Deva%2520Ramanan%2520and%2520Shuran%2520Song%2520and%2520Stan%2520Birchfield%2520and%2520Bowen%2520Wen%2520and%2520Jeffrey%2520Ichnowski%26entry.1292438233%3D%2520%2520Teaching%2520robots%2520to%2520fold%252C%2520drape%252C%2520or%2520reposition%2520deformable%2520objects%2520such%2520as%250Acloth%2520will%2520unlock%2520a%2520variety%2520of%2520automation%2520applications.%2520While%2520remarkable%250Aprogress%2520has%2520been%2520made%2520for%2520rigid%2520object%2520manipulation%252C%2520manipulating%2520deformable%250Aobjects%2520poses%2520unique%2520challenges%252C%2520including%2520frequent%2520occlusions%252C%250Ainfinite-dimensional%2520state%2520spaces%2520and%2520complex%2520dynamics.%2520Just%2520as%2520object%2520pose%250Aestimation%2520and%2520tracking%2520have%2520aided%2520robots%2520for%2520rigid%2520manipulation%252C%2520dense%25203D%250Atracking%2520%2528scene%2520flow%2529%2520of%2520highly%2520deformable%2520objects%2520will%2520enable%2520new%2520applications%250Ain%2520robotics%2520while%2520aiding%2520existing%2520approaches%252C%2520such%2520as%2520imitation%2520learning%2520or%250Acreating%2520digital%2520twins%2520with%2520real2sim%2520transfer.%2520We%2520propose%2520DeformGS%252C%2520an%2520approach%250Ato%2520recover%2520scene%2520flow%2520in%2520highly%2520deformable%2520scenes%252C%2520using%2520simultaneous%2520video%250Acaptures%2520of%2520a%2520dynamic%2520scene%2520from%2520multiple%2520cameras.%2520DeformGS%2520builds%2520on%2520recent%250Aadvances%2520in%2520Gaussian%2520splatting%252C%2520a%2520method%2520that%2520learns%2520the%2520properties%2520of%2520a%2520large%250Anumber%2520of%2520Gaussians%2520for%2520state-of-the-art%2520and%2520fast%2520novel-view%2520synthesis.%250ADeformGS%2520learns%2520a%2520deformation%2520function%2520to%2520project%2520a%2520set%2520of%2520Gaussians%2520with%250Acanonical%2520properties%2520into%2520world%2520space.%2520The%2520deformation%2520function%2520uses%2520a%250Aneural-voxel%2520encoding%2520and%2520a%2520multilayer%2520perceptron%2520%2528MLP%2529%2520to%2520infer%2520Gaussian%250Aposition%252C%2520rotation%252C%2520and%2520a%2520shadow%2520scalar.%2520We%2520enforce%2520physics-inspired%250Aregularization%2520terms%2520based%2520on%2520conservation%2520of%2520momentum%2520and%2520isometry%252C%2520which%250Aleads%2520to%2520trajectories%2520with%2520smaller%2520trajectory%2520errors.%2520We%2520also%2520leverage%2520existing%250Afoundation%2520models%2520SAM%2520and%2520XMEM%2520to%2520produce%2520noisy%2520masks%252C%2520and%2520learn%2520a%2520per-Gaussian%250Amask%2520for%2520better%2520physics-inspired%2520regularization.%2520DeformGS%2520achieves%2520high-quality%250A3D%2520tracking%2520on%2520highly%2520deformable%2520scenes%2520with%2520shadows%2520and%2520occlusions.%2520In%250Aexperiments%252C%2520DeformGS%2520improves%25203D%2520tracking%2520by%2520an%2520average%2520of%252055.8%2525%2520compared%2520to%250Athe%2520state-of-the-art.%2520With%2520sufficient%2520texture%252C%2520DeformGS%2520achieves%2520a%2520median%250Atracking%2520error%2520of%25203.3%2520mm%2520on%2520a%2520cloth%2520of%25201.5%2520x%25201.5%2520m%2520in%2520area.%2520Website%253A%250Ahttps%253A//deformgs.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00583v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeformGS%3A%20Scene%20Flow%20in%20Highly%20Deformable%20Scenes%20for%20Deformable%20Object%0A%20%20Manipulation&entry.906535625=Bardienus%20P.%20Duisterhof%20and%20Zhao%20Mandi%20and%20Yunchao%20Yao%20and%20Jia-Wei%20Liu%20and%20Jenny%20Seidenschwarz%20and%20Mike%20Zheng%20Shou%20and%20Deva%20Ramanan%20and%20Shuran%20Song%20and%20Stan%20Birchfield%20and%20Bowen%20Wen%20and%20Jeffrey%20Ichnowski&entry.1292438233=%20%20Teaching%20robots%20to%20fold%2C%20drape%2C%20or%20reposition%20deformable%20objects%20such%20as%0Acloth%20will%20unlock%20a%20variety%20of%20automation%20applications.%20While%20remarkable%0Aprogress%20has%20been%20made%20for%20rigid%20object%20manipulation%2C%20manipulating%20deformable%0Aobjects%20poses%20unique%20challenges%2C%20including%20frequent%20occlusions%2C%0Ainfinite-dimensional%20state%20spaces%20and%20complex%20dynamics.%20Just%20as%20object%20pose%0Aestimation%20and%20tracking%20have%20aided%20robots%20for%20rigid%20manipulation%2C%20dense%203D%0Atracking%20%28scene%20flow%29%20of%20highly%20deformable%20objects%20will%20enable%20new%20applications%0Ain%20robotics%20while%20aiding%20existing%20approaches%2C%20such%20as%20imitation%20learning%20or%0Acreating%20digital%20twins%20with%20real2sim%20transfer.%20We%20propose%20DeformGS%2C%20an%20approach%0Ato%20recover%20scene%20flow%20in%20highly%20deformable%20scenes%2C%20using%20simultaneous%20video%0Acaptures%20of%20a%20dynamic%20scene%20from%20multiple%20cameras.%20DeformGS%20builds%20on%20recent%0Aadvances%20in%20Gaussian%20splatting%2C%20a%20method%20that%20learns%20the%20properties%20of%20a%20large%0Anumber%20of%20Gaussians%20for%20state-of-the-art%20and%20fast%20novel-view%20synthesis.%0ADeformGS%20learns%20a%20deformation%20function%20to%20project%20a%20set%20of%20Gaussians%20with%0Acanonical%20properties%20into%20world%20space.%20The%20deformation%20function%20uses%20a%0Aneural-voxel%20encoding%20and%20a%20multilayer%20perceptron%20%28MLP%29%20to%20infer%20Gaussian%0Aposition%2C%20rotation%2C%20and%20a%20shadow%20scalar.%20We%20enforce%20physics-inspired%0Aregularization%20terms%20based%20on%20conservation%20of%20momentum%20and%20isometry%2C%20which%0Aleads%20to%20trajectories%20with%20smaller%20trajectory%20errors.%20We%20also%20leverage%20existing%0Afoundation%20models%20SAM%20and%20XMEM%20to%20produce%20noisy%20masks%2C%20and%20learn%20a%20per-Gaussian%0Amask%20for%20better%20physics-inspired%20regularization.%20DeformGS%20achieves%20high-quality%0A3D%20tracking%20on%20highly%20deformable%20scenes%20with%20shadows%20and%20occlusions.%20In%0Aexperiments%2C%20DeformGS%20improves%203D%20tracking%20by%20an%20average%20of%2055.8%25%20compared%20to%0Athe%20state-of-the-art.%20With%20sufficient%20texture%2C%20DeformGS%20achieves%20a%20median%0Atracking%20error%20of%203.3%20mm%20on%20a%20cloth%20of%201.5%20x%201.5%20m%20in%20area.%20Website%3A%0Ahttps%3A//deformgs.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00583v2&entry.124074799=Read"},
{"title": "Learning Multi-Target TDOA Features for Sound Event Localization and\n  Detection", "author": "Axel Berg and Johanna Engman and Jens Gulin and Karl \u00c5str\u00f6m and Magnus Oskarsson", "abstract": "  Sound event localization and detection (SELD) systems using audio recordings\nfrom a microphone array rely on spatial cues for determining the location of\nsound events. As a consequence, the localization performance of such systems is\nto a large extent determined by the quality of the audio features that are used\nas inputs to the system. We propose a new feature, based on neural generalized\ncross-correlations with phase-transform (NGCC-PHAT), that learns audio\nrepresentations suitable for localization. Using permutation invariant training\nfor the time-difference of arrival (TDOA) estimation problem enables NGCC-PHAT\nto learn TDOA features for multiple overlapping sound events. These features\ncan be used as a drop-in replacement for GCC-PHAT inputs to a SELD-network. We\ntest our method on the STARSS23 dataset and demonstrate improved localization\nperformance compared to using standard GCC-PHAT or SALSA-Lite input features.\n", "link": "http://arxiv.org/abs/2408.17166v1", "date": "2024-08-30", "relevancy": 2.5151, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5428}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4885}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Multi-Target%20TDOA%20Features%20for%20Sound%20Event%20Localization%20and%0A%20%20Detection&body=Title%3A%20Learning%20Multi-Target%20TDOA%20Features%20for%20Sound%20Event%20Localization%20and%0A%20%20Detection%0AAuthor%3A%20Axel%20Berg%20and%20Johanna%20Engman%20and%20Jens%20Gulin%20and%20Karl%20%C3%85str%C3%B6m%20and%20Magnus%20Oskarsson%0AAbstract%3A%20%20%20Sound%20event%20localization%20and%20detection%20%28SELD%29%20systems%20using%20audio%20recordings%0Afrom%20a%20microphone%20array%20rely%20on%20spatial%20cues%20for%20determining%20the%20location%20of%0Asound%20events.%20As%20a%20consequence%2C%20the%20localization%20performance%20of%20such%20systems%20is%0Ato%20a%20large%20extent%20determined%20by%20the%20quality%20of%20the%20audio%20features%20that%20are%20used%0Aas%20inputs%20to%20the%20system.%20We%20propose%20a%20new%20feature%2C%20based%20on%20neural%20generalized%0Across-correlations%20with%20phase-transform%20%28NGCC-PHAT%29%2C%20that%20learns%20audio%0Arepresentations%20suitable%20for%20localization.%20Using%20permutation%20invariant%20training%0Afor%20the%20time-difference%20of%20arrival%20%28TDOA%29%20estimation%20problem%20enables%20NGCC-PHAT%0Ato%20learn%20TDOA%20features%20for%20multiple%20overlapping%20sound%20events.%20These%20features%0Acan%20be%20used%20as%20a%20drop-in%20replacement%20for%20GCC-PHAT%20inputs%20to%20a%20SELD-network.%20We%0Atest%20our%20method%20on%20the%20STARSS23%20dataset%20and%20demonstrate%20improved%20localization%0Aperformance%20compared%20to%20using%20standard%20GCC-PHAT%20or%20SALSA-Lite%20input%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17166v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Multi-Target%2520TDOA%2520Features%2520for%2520Sound%2520Event%2520Localization%2520and%250A%2520%2520Detection%26entry.906535625%3DAxel%2520Berg%2520and%2520Johanna%2520Engman%2520and%2520Jens%2520Gulin%2520and%2520Karl%2520%25C3%2585str%25C3%25B6m%2520and%2520Magnus%2520Oskarsson%26entry.1292438233%3D%2520%2520Sound%2520event%2520localization%2520and%2520detection%2520%2528SELD%2529%2520systems%2520using%2520audio%2520recordings%250Afrom%2520a%2520microphone%2520array%2520rely%2520on%2520spatial%2520cues%2520for%2520determining%2520the%2520location%2520of%250Asound%2520events.%2520As%2520a%2520consequence%252C%2520the%2520localization%2520performance%2520of%2520such%2520systems%2520is%250Ato%2520a%2520large%2520extent%2520determined%2520by%2520the%2520quality%2520of%2520the%2520audio%2520features%2520that%2520are%2520used%250Aas%2520inputs%2520to%2520the%2520system.%2520We%2520propose%2520a%2520new%2520feature%252C%2520based%2520on%2520neural%2520generalized%250Across-correlations%2520with%2520phase-transform%2520%2528NGCC-PHAT%2529%252C%2520that%2520learns%2520audio%250Arepresentations%2520suitable%2520for%2520localization.%2520Using%2520permutation%2520invariant%2520training%250Afor%2520the%2520time-difference%2520of%2520arrival%2520%2528TDOA%2529%2520estimation%2520problem%2520enables%2520NGCC-PHAT%250Ato%2520learn%2520TDOA%2520features%2520for%2520multiple%2520overlapping%2520sound%2520events.%2520These%2520features%250Acan%2520be%2520used%2520as%2520a%2520drop-in%2520replacement%2520for%2520GCC-PHAT%2520inputs%2520to%2520a%2520SELD-network.%2520We%250Atest%2520our%2520method%2520on%2520the%2520STARSS23%2520dataset%2520and%2520demonstrate%2520improved%2520localization%250Aperformance%2520compared%2520to%2520using%2520standard%2520GCC-PHAT%2520or%2520SALSA-Lite%2520input%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17166v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Multi-Target%20TDOA%20Features%20for%20Sound%20Event%20Localization%20and%0A%20%20Detection&entry.906535625=Axel%20Berg%20and%20Johanna%20Engman%20and%20Jens%20Gulin%20and%20Karl%20%C3%85str%C3%B6m%20and%20Magnus%20Oskarsson&entry.1292438233=%20%20Sound%20event%20localization%20and%20detection%20%28SELD%29%20systems%20using%20audio%20recordings%0Afrom%20a%20microphone%20array%20rely%20on%20spatial%20cues%20for%20determining%20the%20location%20of%0Asound%20events.%20As%20a%20consequence%2C%20the%20localization%20performance%20of%20such%20systems%20is%0Ato%20a%20large%20extent%20determined%20by%20the%20quality%20of%20the%20audio%20features%20that%20are%20used%0Aas%20inputs%20to%20the%20system.%20We%20propose%20a%20new%20feature%2C%20based%20on%20neural%20generalized%0Across-correlations%20with%20phase-transform%20%28NGCC-PHAT%29%2C%20that%20learns%20audio%0Arepresentations%20suitable%20for%20localization.%20Using%20permutation%20invariant%20training%0Afor%20the%20time-difference%20of%20arrival%20%28TDOA%29%20estimation%20problem%20enables%20NGCC-PHAT%0Ato%20learn%20TDOA%20features%20for%20multiple%20overlapping%20sound%20events.%20These%20features%0Acan%20be%20used%20as%20a%20drop-in%20replacement%20for%20GCC-PHAT%20inputs%20to%20a%20SELD-network.%20We%0Atest%20our%20method%20on%20the%20STARSS23%20dataset%20and%20demonstrate%20improved%20localization%0Aperformance%20compared%20to%20using%20standard%20GCC-PHAT%20or%20SALSA-Lite%20input%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17166v1&entry.124074799=Read"},
{"title": "CathAction: A Benchmark for Endovascular Intervention Understanding", "author": "Baoru Huang and Tuan Vo and Chayun Kongtongvattana and Giulio Dagnino and Dennis Kundrat and Wenqiang Chi and Mohamed Abdelaziz and Trevor Kwok and Tudor Jianu and Tuong Do and Hieu Le and Minh Nguyen and Hoan Nguyen and Erman Tjiputra and Quang Tran and Jianyang Xie and Yanda Meng and Binod Bhattarai and Zhaorui Tan and Hongbin Liu and Hong Seng Gan and Wei Wang and Xi Yang and Qiufeng Wang and Jionglong Su and Kaizhu Huang and Angelos Stefanidis and Min Guo and Bo Du and Rong Tao and Minh Vu and Guoyan Zheng and Yalin Zheng and Francisco Vasconcelos and Danail Stoyanov and Daniel Elson and Ferdinando Rodriguez y Baena and Anh Nguyen", "abstract": "  Real-time visual feedback from catheterization analysis is crucial for\nenhancing surgical safety and efficiency during endovascular interventions.\nHowever, existing datasets are often limited to specific tasks, small scale,\nand lack the comprehensive annotations necessary for broader endovascular\nintervention understanding. To tackle these limitations, we introduce\nCathAction, a large-scale dataset for catheterization understanding. Our\nCathAction dataset encompasses approximately 500,000 annotated frames for\ncatheterization action understanding and collision detection, and 25,000 ground\ntruth masks for catheter and guidewire segmentation. For each task, we\nbenchmark recent related works in the field. We further discuss the challenges\nof endovascular intentions compared to traditional computer vision tasks and\npoint out open research questions. We hope that CathAction will facilitate the\ndevelopment of endovascular intervention understanding methods that can be\napplied to real-world applications. The dataset is available at\nhttps://airvlab.github.io/cathaction/.\n", "link": "http://arxiv.org/abs/2408.13126v2", "date": "2024-08-30", "relevancy": 2.512, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5148}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5148}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CathAction%3A%20A%20Benchmark%20for%20Endovascular%20Intervention%20Understanding&body=Title%3A%20CathAction%3A%20A%20Benchmark%20for%20Endovascular%20Intervention%20Understanding%0AAuthor%3A%20Baoru%20Huang%20and%20Tuan%20Vo%20and%20Chayun%20Kongtongvattana%20and%20Giulio%20Dagnino%20and%20Dennis%20Kundrat%20and%20Wenqiang%20Chi%20and%20Mohamed%20Abdelaziz%20and%20Trevor%20Kwok%20and%20Tudor%20Jianu%20and%20Tuong%20Do%20and%20Hieu%20Le%20and%20Minh%20Nguyen%20and%20Hoan%20Nguyen%20and%20Erman%20Tjiputra%20and%20Quang%20Tran%20and%20Jianyang%20Xie%20and%20Yanda%20Meng%20and%20Binod%20Bhattarai%20and%20Zhaorui%20Tan%20and%20Hongbin%20Liu%20and%20Hong%20Seng%20Gan%20and%20Wei%20Wang%20and%20Xi%20Yang%20and%20Qiufeng%20Wang%20and%20Jionglong%20Su%20and%20Kaizhu%20Huang%20and%20Angelos%20Stefanidis%20and%20Min%20Guo%20and%20Bo%20Du%20and%20Rong%20Tao%20and%20Minh%20Vu%20and%20Guoyan%20Zheng%20and%20Yalin%20Zheng%20and%20Francisco%20Vasconcelos%20and%20Danail%20Stoyanov%20and%20Daniel%20Elson%20and%20Ferdinando%20Rodriguez%20y%20Baena%20and%20Anh%20Nguyen%0AAbstract%3A%20%20%20Real-time%20visual%20feedback%20from%20catheterization%20analysis%20is%20crucial%20for%0Aenhancing%20surgical%20safety%20and%20efficiency%20during%20endovascular%20interventions.%0AHowever%2C%20existing%20datasets%20are%20often%20limited%20to%20specific%20tasks%2C%20small%20scale%2C%0Aand%20lack%20the%20comprehensive%20annotations%20necessary%20for%20broader%20endovascular%0Aintervention%20understanding.%20To%20tackle%20these%20limitations%2C%20we%20introduce%0ACathAction%2C%20a%20large-scale%20dataset%20for%20catheterization%20understanding.%20Our%0ACathAction%20dataset%20encompasses%20approximately%20500%2C000%20annotated%20frames%20for%0Acatheterization%20action%20understanding%20and%20collision%20detection%2C%20and%2025%2C000%20ground%0Atruth%20masks%20for%20catheter%20and%20guidewire%20segmentation.%20For%20each%20task%2C%20we%0Abenchmark%20recent%20related%20works%20in%20the%20field.%20We%20further%20discuss%20the%20challenges%0Aof%20endovascular%20intentions%20compared%20to%20traditional%20computer%20vision%20tasks%20and%0Apoint%20out%20open%20research%20questions.%20We%20hope%20that%20CathAction%20will%20facilitate%20the%0Adevelopment%20of%20endovascular%20intervention%20understanding%20methods%20that%20can%20be%0Aapplied%20to%20real-world%20applications.%20The%20dataset%20is%20available%20at%0Ahttps%3A//airvlab.github.io/cathaction/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13126v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCathAction%253A%2520A%2520Benchmark%2520for%2520Endovascular%2520Intervention%2520Understanding%26entry.906535625%3DBaoru%2520Huang%2520and%2520Tuan%2520Vo%2520and%2520Chayun%2520Kongtongvattana%2520and%2520Giulio%2520Dagnino%2520and%2520Dennis%2520Kundrat%2520and%2520Wenqiang%2520Chi%2520and%2520Mohamed%2520Abdelaziz%2520and%2520Trevor%2520Kwok%2520and%2520Tudor%2520Jianu%2520and%2520Tuong%2520Do%2520and%2520Hieu%2520Le%2520and%2520Minh%2520Nguyen%2520and%2520Hoan%2520Nguyen%2520and%2520Erman%2520Tjiputra%2520and%2520Quang%2520Tran%2520and%2520Jianyang%2520Xie%2520and%2520Yanda%2520Meng%2520and%2520Binod%2520Bhattarai%2520and%2520Zhaorui%2520Tan%2520and%2520Hongbin%2520Liu%2520and%2520Hong%2520Seng%2520Gan%2520and%2520Wei%2520Wang%2520and%2520Xi%2520Yang%2520and%2520Qiufeng%2520Wang%2520and%2520Jionglong%2520Su%2520and%2520Kaizhu%2520Huang%2520and%2520Angelos%2520Stefanidis%2520and%2520Min%2520Guo%2520and%2520Bo%2520Du%2520and%2520Rong%2520Tao%2520and%2520Minh%2520Vu%2520and%2520Guoyan%2520Zheng%2520and%2520Yalin%2520Zheng%2520and%2520Francisco%2520Vasconcelos%2520and%2520Danail%2520Stoyanov%2520and%2520Daniel%2520Elson%2520and%2520Ferdinando%2520Rodriguez%2520y%2520Baena%2520and%2520Anh%2520Nguyen%26entry.1292438233%3D%2520%2520Real-time%2520visual%2520feedback%2520from%2520catheterization%2520analysis%2520is%2520crucial%2520for%250Aenhancing%2520surgical%2520safety%2520and%2520efficiency%2520during%2520endovascular%2520interventions.%250AHowever%252C%2520existing%2520datasets%2520are%2520often%2520limited%2520to%2520specific%2520tasks%252C%2520small%2520scale%252C%250Aand%2520lack%2520the%2520comprehensive%2520annotations%2520necessary%2520for%2520broader%2520endovascular%250Aintervention%2520understanding.%2520To%2520tackle%2520these%2520limitations%252C%2520we%2520introduce%250ACathAction%252C%2520a%2520large-scale%2520dataset%2520for%2520catheterization%2520understanding.%2520Our%250ACathAction%2520dataset%2520encompasses%2520approximately%2520500%252C000%2520annotated%2520frames%2520for%250Acatheterization%2520action%2520understanding%2520and%2520collision%2520detection%252C%2520and%252025%252C000%2520ground%250Atruth%2520masks%2520for%2520catheter%2520and%2520guidewire%2520segmentation.%2520For%2520each%2520task%252C%2520we%250Abenchmark%2520recent%2520related%2520works%2520in%2520the%2520field.%2520We%2520further%2520discuss%2520the%2520challenges%250Aof%2520endovascular%2520intentions%2520compared%2520to%2520traditional%2520computer%2520vision%2520tasks%2520and%250Apoint%2520out%2520open%2520research%2520questions.%2520We%2520hope%2520that%2520CathAction%2520will%2520facilitate%2520the%250Adevelopment%2520of%2520endovascular%2520intervention%2520understanding%2520methods%2520that%2520can%2520be%250Aapplied%2520to%2520real-world%2520applications.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//airvlab.github.io/cathaction/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13126v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CathAction%3A%20A%20Benchmark%20for%20Endovascular%20Intervention%20Understanding&entry.906535625=Baoru%20Huang%20and%20Tuan%20Vo%20and%20Chayun%20Kongtongvattana%20and%20Giulio%20Dagnino%20and%20Dennis%20Kundrat%20and%20Wenqiang%20Chi%20and%20Mohamed%20Abdelaziz%20and%20Trevor%20Kwok%20and%20Tudor%20Jianu%20and%20Tuong%20Do%20and%20Hieu%20Le%20and%20Minh%20Nguyen%20and%20Hoan%20Nguyen%20and%20Erman%20Tjiputra%20and%20Quang%20Tran%20and%20Jianyang%20Xie%20and%20Yanda%20Meng%20and%20Binod%20Bhattarai%20and%20Zhaorui%20Tan%20and%20Hongbin%20Liu%20and%20Hong%20Seng%20Gan%20and%20Wei%20Wang%20and%20Xi%20Yang%20and%20Qiufeng%20Wang%20and%20Jionglong%20Su%20and%20Kaizhu%20Huang%20and%20Angelos%20Stefanidis%20and%20Min%20Guo%20and%20Bo%20Du%20and%20Rong%20Tao%20and%20Minh%20Vu%20and%20Guoyan%20Zheng%20and%20Yalin%20Zheng%20and%20Francisco%20Vasconcelos%20and%20Danail%20Stoyanov%20and%20Daniel%20Elson%20and%20Ferdinando%20Rodriguez%20y%20Baena%20and%20Anh%20Nguyen&entry.1292438233=%20%20Real-time%20visual%20feedback%20from%20catheterization%20analysis%20is%20crucial%20for%0Aenhancing%20surgical%20safety%20and%20efficiency%20during%20endovascular%20interventions.%0AHowever%2C%20existing%20datasets%20are%20often%20limited%20to%20specific%20tasks%2C%20small%20scale%2C%0Aand%20lack%20the%20comprehensive%20annotations%20necessary%20for%20broader%20endovascular%0Aintervention%20understanding.%20To%20tackle%20these%20limitations%2C%20we%20introduce%0ACathAction%2C%20a%20large-scale%20dataset%20for%20catheterization%20understanding.%20Our%0ACathAction%20dataset%20encompasses%20approximately%20500%2C000%20annotated%20frames%20for%0Acatheterization%20action%20understanding%20and%20collision%20detection%2C%20and%2025%2C000%20ground%0Atruth%20masks%20for%20catheter%20and%20guidewire%20segmentation.%20For%20each%20task%2C%20we%0Abenchmark%20recent%20related%20works%20in%20the%20field.%20We%20further%20discuss%20the%20challenges%0Aof%20endovascular%20intentions%20compared%20to%20traditional%20computer%20vision%20tasks%20and%0Apoint%20out%20open%20research%20questions.%20We%20hope%20that%20CathAction%20will%20facilitate%20the%0Adevelopment%20of%20endovascular%20intervention%20understanding%20methods%20that%20can%20be%0Aapplied%20to%20real-world%20applications.%20The%20dataset%20is%20available%20at%0Ahttps%3A//airvlab.github.io/cathaction/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13126v2&entry.124074799=Read"},
{"title": "Covariance-corrected Whitening Alleviates Network Degeneration on\n  Imbalanced Classification", "author": "Zhiwei Zhang", "abstract": "  Class imbalance is a critical issue in image classification that\nsignificantly affects the performance of deep recognition models. In this work,\nwe first identify a network degeneration dilemma that hinders the model\nlearning by introducing a high linear dependence among the features inputted\ninto the classifier. To overcome this challenge, we propose a novel framework\ncalled Whitening-Net to mitigate the degenerate solutions, in which ZCA\nwhitening is integrated before the linear classifier to normalize and\ndecorrelate the batch samples. However, in scenarios with extreme class\nimbalance, the batch covariance statistic exhibits significant fluctuations,\nimpeding the convergence of the whitening operation. Therefore, we propose two\ncovariance-corrected modules, the Group-based Relatively Balanced Batch Sampler\n(GRBS) and the Batch Embedded Training (BET), to get more accurate and stable\nbatch covariance, thereby reinforcing the capability of whitening. Our modules\ncan be trained end-to-end without incurring substantial computational costs.\nComprehensive empirical evaluations conducted on benchmark datasets, including\nCIFAR-LT-10/100, ImageNet-LT, and iNaturalist-LT, validate the effectiveness of\nour proposed approaches.\n", "link": "http://arxiv.org/abs/2408.17197v1", "date": "2024-08-30", "relevancy": 2.5119, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5103}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4996}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Covariance-corrected%20Whitening%20Alleviates%20Network%20Degeneration%20on%0A%20%20Imbalanced%20Classification&body=Title%3A%20Covariance-corrected%20Whitening%20Alleviates%20Network%20Degeneration%20on%0A%20%20Imbalanced%20Classification%0AAuthor%3A%20Zhiwei%20Zhang%0AAbstract%3A%20%20%20Class%20imbalance%20is%20a%20critical%20issue%20in%20image%20classification%20that%0Asignificantly%20affects%20the%20performance%20of%20deep%20recognition%20models.%20In%20this%20work%2C%0Awe%20first%20identify%20a%20network%20degeneration%20dilemma%20that%20hinders%20the%20model%0Alearning%20by%20introducing%20a%20high%20linear%20dependence%20among%20the%20features%20inputted%0Ainto%20the%20classifier.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%20novel%20framework%0Acalled%20Whitening-Net%20to%20mitigate%20the%20degenerate%20solutions%2C%20in%20which%20ZCA%0Awhitening%20is%20integrated%20before%20the%20linear%20classifier%20to%20normalize%20and%0Adecorrelate%20the%20batch%20samples.%20However%2C%20in%20scenarios%20with%20extreme%20class%0Aimbalance%2C%20the%20batch%20covariance%20statistic%20exhibits%20significant%20fluctuations%2C%0Aimpeding%20the%20convergence%20of%20the%20whitening%20operation.%20Therefore%2C%20we%20propose%20two%0Acovariance-corrected%20modules%2C%20the%20Group-based%20Relatively%20Balanced%20Batch%20Sampler%0A%28GRBS%29%20and%20the%20Batch%20Embedded%20Training%20%28BET%29%2C%20to%20get%20more%20accurate%20and%20stable%0Abatch%20covariance%2C%20thereby%20reinforcing%20the%20capability%20of%20whitening.%20Our%20modules%0Acan%20be%20trained%20end-to-end%20without%20incurring%20substantial%20computational%20costs.%0AComprehensive%20empirical%20evaluations%20conducted%20on%20benchmark%20datasets%2C%20including%0ACIFAR-LT-10/100%2C%20ImageNet-LT%2C%20and%20iNaturalist-LT%2C%20validate%20the%20effectiveness%20of%0Aour%20proposed%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCovariance-corrected%2520Whitening%2520Alleviates%2520Network%2520Degeneration%2520on%250A%2520%2520Imbalanced%2520Classification%26entry.906535625%3DZhiwei%2520Zhang%26entry.1292438233%3D%2520%2520Class%2520imbalance%2520is%2520a%2520critical%2520issue%2520in%2520image%2520classification%2520that%250Asignificantly%2520affects%2520the%2520performance%2520of%2520deep%2520recognition%2520models.%2520In%2520this%2520work%252C%250Awe%2520first%2520identify%2520a%2520network%2520degeneration%2520dilemma%2520that%2520hinders%2520the%2520model%250Alearning%2520by%2520introducing%2520a%2520high%2520linear%2520dependence%2520among%2520the%2520features%2520inputted%250Ainto%2520the%2520classifier.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520framework%250Acalled%2520Whitening-Net%2520to%2520mitigate%2520the%2520degenerate%2520solutions%252C%2520in%2520which%2520ZCA%250Awhitening%2520is%2520integrated%2520before%2520the%2520linear%2520classifier%2520to%2520normalize%2520and%250Adecorrelate%2520the%2520batch%2520samples.%2520However%252C%2520in%2520scenarios%2520with%2520extreme%2520class%250Aimbalance%252C%2520the%2520batch%2520covariance%2520statistic%2520exhibits%2520significant%2520fluctuations%252C%250Aimpeding%2520the%2520convergence%2520of%2520the%2520whitening%2520operation.%2520Therefore%252C%2520we%2520propose%2520two%250Acovariance-corrected%2520modules%252C%2520the%2520Group-based%2520Relatively%2520Balanced%2520Batch%2520Sampler%250A%2528GRBS%2529%2520and%2520the%2520Batch%2520Embedded%2520Training%2520%2528BET%2529%252C%2520to%2520get%2520more%2520accurate%2520and%2520stable%250Abatch%2520covariance%252C%2520thereby%2520reinforcing%2520the%2520capability%2520of%2520whitening.%2520Our%2520modules%250Acan%2520be%2520trained%2520end-to-end%2520without%2520incurring%2520substantial%2520computational%2520costs.%250AComprehensive%2520empirical%2520evaluations%2520conducted%2520on%2520benchmark%2520datasets%252C%2520including%250ACIFAR-LT-10/100%252C%2520ImageNet-LT%252C%2520and%2520iNaturalist-LT%252C%2520validate%2520the%2520effectiveness%2520of%250Aour%2520proposed%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Covariance-corrected%20Whitening%20Alleviates%20Network%20Degeneration%20on%0A%20%20Imbalanced%20Classification&entry.906535625=Zhiwei%20Zhang&entry.1292438233=%20%20Class%20imbalance%20is%20a%20critical%20issue%20in%20image%20classification%20that%0Asignificantly%20affects%20the%20performance%20of%20deep%20recognition%20models.%20In%20this%20work%2C%0Awe%20first%20identify%20a%20network%20degeneration%20dilemma%20that%20hinders%20the%20model%0Alearning%20by%20introducing%20a%20high%20linear%20dependence%20among%20the%20features%20inputted%0Ainto%20the%20classifier.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%20novel%20framework%0Acalled%20Whitening-Net%20to%20mitigate%20the%20degenerate%20solutions%2C%20in%20which%20ZCA%0Awhitening%20is%20integrated%20before%20the%20linear%20classifier%20to%20normalize%20and%0Adecorrelate%20the%20batch%20samples.%20However%2C%20in%20scenarios%20with%20extreme%20class%0Aimbalance%2C%20the%20batch%20covariance%20statistic%20exhibits%20significant%20fluctuations%2C%0Aimpeding%20the%20convergence%20of%20the%20whitening%20operation.%20Therefore%2C%20we%20propose%20two%0Acovariance-corrected%20modules%2C%20the%20Group-based%20Relatively%20Balanced%20Batch%20Sampler%0A%28GRBS%29%20and%20the%20Batch%20Embedded%20Training%20%28BET%29%2C%20to%20get%20more%20accurate%20and%20stable%0Abatch%20covariance%2C%20thereby%20reinforcing%20the%20capability%20of%20whitening.%20Our%20modules%0Acan%20be%20trained%20end-to-end%20without%20incurring%20substantial%20computational%20costs.%0AComprehensive%20empirical%20evaluations%20conducted%20on%20benchmark%20datasets%2C%20including%0ACIFAR-LT-10/100%2C%20ImageNet-LT%2C%20and%20iNaturalist-LT%2C%20validate%20the%20effectiveness%20of%0Aour%20proposed%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17197v1&entry.124074799=Read"},
{"title": "TaSL: Task Skill Localization and Consolidation for Language Model\n  Continual Learning", "author": "Yujie Feng and Xu Chu and Yongxin Xu and Zexin Lu and Bo Liu and Philip S. Yu and Xiao-Ming Wu", "abstract": "  Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Task Skill Localization and Consolidation\n(TaSL), which boosts knowledge transfer without depending on memory replay.\nTaSL initially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise skill localization technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained skill\nconsolidation strategy that retains task-specific knowledge, thereby preventing\nforgetting, and updates task-shared knowledge, which facilitates bi-directional\nknowledge transfer. As a result, TaSL achieves an optimal balance between\nretaining prior knowledge and excelling in new tasks. TaSL also demonstrates\nstrong generalizability, making it suitable for various base models and\nadaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of TaSL and\nits variants across different settings.\n", "link": "http://arxiv.org/abs/2408.05200v2", "date": "2024-08-30", "relevancy": 2.5117, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5808}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.469}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TaSL%3A%20Task%20Skill%20Localization%20and%20Consolidation%20for%20Language%20Model%0A%20%20Continual%20Learning&body=Title%3A%20TaSL%3A%20Task%20Skill%20Localization%20and%20Consolidation%20for%20Language%20Model%0A%20%20Continual%20Learning%0AAuthor%3A%20Yujie%20Feng%20and%20Xu%20Chu%20and%20Yongxin%20Xu%20and%20Zexin%20Lu%20and%20Bo%20Liu%20and%20Philip%20S.%20Yu%20and%20Xiao-Ming%20Wu%0AAbstract%3A%20%20%20Language%20model%20continual%20learning%20%28CL%29%20has%20recently%20attracted%20significant%0Ainterest%20for%20its%20ability%20to%20adapt%20large%20language%20models%20%28LLMs%29%20to%20dynamic%0Areal-world%20scenarios%20without%20retraining.%20A%20major%20challenge%20in%20this%20domain%20is%0Acatastrophic%20forgetting%2C%20where%20models%20lose%20previously%20acquired%20knowledge%20upon%0Alearning%20new%20tasks.%20Existing%20approaches%20commonly%20utilize%20multiple%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20blocks%20to%20acquire%20task-specific%0Aknowledge%2C%20yet%20these%20methods%20are%20inefficient%20and%20fail%20to%20leverage%20potential%0Aknowledge%20transfer%20across%20tasks.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20CL%0Aframework%20for%20language%20models%2C%20named%20Task%20Skill%20Localization%20and%20Consolidation%0A%28TaSL%29%2C%20which%20boosts%20knowledge%20transfer%20without%20depending%20on%20memory%20replay.%0ATaSL%20initially%20segregates%20the%20model%20into%20%27skill%20units%27%20based%20on%20parameter%0Adependencies%2C%20allowing%20for%20more%20precise%20control.%20Subsequently%2C%20it%20employs%20a%0Anovel%20group-wise%20skill%20localization%20technique%20to%20ascertain%20the%20importance%0Adistribution%20of%20skill%20units%20for%20a%20new%20task.%20By%20comparing%20this%20importance%0Adistribution%20with%20those%20from%20previous%20tasks%2C%20we%20implement%20a%20fine-grained%20skill%0Aconsolidation%20strategy%20that%20retains%20task-specific%20knowledge%2C%20thereby%20preventing%0Aforgetting%2C%20and%20updates%20task-shared%20knowledge%2C%20which%20facilitates%20bi-directional%0Aknowledge%20transfer.%20As%20a%20result%2C%20TaSL%20achieves%20an%20optimal%20balance%20between%0Aretaining%20prior%20knowledge%20and%20excelling%20in%20new%20tasks.%20TaSL%20also%20demonstrates%0Astrong%20generalizability%2C%20making%20it%20suitable%20for%20various%20base%20models%20and%0Aadaptable%20to%20PEFT%20methods%20like%20LoRA.%20Furthermore%2C%20it%20offers%20notable%0Aextensibility%2C%20supporting%20enhancements%20through%20integration%20with%20memory%20replay%0Atechniques.%20Comprehensive%20experiments%20conducted%20on%20two%20CL%20benchmarks%2C%20involving%0Amodels%20ranging%20from%20220M%20to%207B%20parameters%2C%20affirm%20the%20effectiveness%20of%20TaSL%20and%0Aits%20variants%20across%20different%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05200v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaSL%253A%2520Task%2520Skill%2520Localization%2520and%2520Consolidation%2520for%2520Language%2520Model%250A%2520%2520Continual%2520Learning%26entry.906535625%3DYujie%2520Feng%2520and%2520Xu%2520Chu%2520and%2520Yongxin%2520Xu%2520and%2520Zexin%2520Lu%2520and%2520Bo%2520Liu%2520and%2520Philip%2520S.%2520Yu%2520and%2520Xiao-Ming%2520Wu%26entry.1292438233%3D%2520%2520Language%2520model%2520continual%2520learning%2520%2528CL%2529%2520has%2520recently%2520attracted%2520significant%250Ainterest%2520for%2520its%2520ability%2520to%2520adapt%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520dynamic%250Areal-world%2520scenarios%2520without%2520retraining.%2520A%2520major%2520challenge%2520in%2520this%2520domain%2520is%250Acatastrophic%2520forgetting%252C%2520where%2520models%2520lose%2520previously%2520acquired%2520knowledge%2520upon%250Alearning%2520new%2520tasks.%2520Existing%2520approaches%2520commonly%2520utilize%2520multiple%250Aparameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520blocks%2520to%2520acquire%2520task-specific%250Aknowledge%252C%2520yet%2520these%2520methods%2520are%2520inefficient%2520and%2520fail%2520to%2520leverage%2520potential%250Aknowledge%2520transfer%2520across%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520CL%250Aframework%2520for%2520language%2520models%252C%2520named%2520Task%2520Skill%2520Localization%2520and%2520Consolidation%250A%2528TaSL%2529%252C%2520which%2520boosts%2520knowledge%2520transfer%2520without%2520depending%2520on%2520memory%2520replay.%250ATaSL%2520initially%2520segregates%2520the%2520model%2520into%2520%2527skill%2520units%2527%2520based%2520on%2520parameter%250Adependencies%252C%2520allowing%2520for%2520more%2520precise%2520control.%2520Subsequently%252C%2520it%2520employs%2520a%250Anovel%2520group-wise%2520skill%2520localization%2520technique%2520to%2520ascertain%2520the%2520importance%250Adistribution%2520of%2520skill%2520units%2520for%2520a%2520new%2520task.%2520By%2520comparing%2520this%2520importance%250Adistribution%2520with%2520those%2520from%2520previous%2520tasks%252C%2520we%2520implement%2520a%2520fine-grained%2520skill%250Aconsolidation%2520strategy%2520that%2520retains%2520task-specific%2520knowledge%252C%2520thereby%2520preventing%250Aforgetting%252C%2520and%2520updates%2520task-shared%2520knowledge%252C%2520which%2520facilitates%2520bi-directional%250Aknowledge%2520transfer.%2520As%2520a%2520result%252C%2520TaSL%2520achieves%2520an%2520optimal%2520balance%2520between%250Aretaining%2520prior%2520knowledge%2520and%2520excelling%2520in%2520new%2520tasks.%2520TaSL%2520also%2520demonstrates%250Astrong%2520generalizability%252C%2520making%2520it%2520suitable%2520for%2520various%2520base%2520models%2520and%250Aadaptable%2520to%2520PEFT%2520methods%2520like%2520LoRA.%2520Furthermore%252C%2520it%2520offers%2520notable%250Aextensibility%252C%2520supporting%2520enhancements%2520through%2520integration%2520with%2520memory%2520replay%250Atechniques.%2520Comprehensive%2520experiments%2520conducted%2520on%2520two%2520CL%2520benchmarks%252C%2520involving%250Amodels%2520ranging%2520from%2520220M%2520to%25207B%2520parameters%252C%2520affirm%2520the%2520effectiveness%2520of%2520TaSL%2520and%250Aits%2520variants%2520across%2520different%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05200v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TaSL%3A%20Task%20Skill%20Localization%20and%20Consolidation%20for%20Language%20Model%0A%20%20Continual%20Learning&entry.906535625=Yujie%20Feng%20and%20Xu%20Chu%20and%20Yongxin%20Xu%20and%20Zexin%20Lu%20and%20Bo%20Liu%20and%20Philip%20S.%20Yu%20and%20Xiao-Ming%20Wu&entry.1292438233=%20%20Language%20model%20continual%20learning%20%28CL%29%20has%20recently%20attracted%20significant%0Ainterest%20for%20its%20ability%20to%20adapt%20large%20language%20models%20%28LLMs%29%20to%20dynamic%0Areal-world%20scenarios%20without%20retraining.%20A%20major%20challenge%20in%20this%20domain%20is%0Acatastrophic%20forgetting%2C%20where%20models%20lose%20previously%20acquired%20knowledge%20upon%0Alearning%20new%20tasks.%20Existing%20approaches%20commonly%20utilize%20multiple%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20blocks%20to%20acquire%20task-specific%0Aknowledge%2C%20yet%20these%20methods%20are%20inefficient%20and%20fail%20to%20leverage%20potential%0Aknowledge%20transfer%20across%20tasks.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20CL%0Aframework%20for%20language%20models%2C%20named%20Task%20Skill%20Localization%20and%20Consolidation%0A%28TaSL%29%2C%20which%20boosts%20knowledge%20transfer%20without%20depending%20on%20memory%20replay.%0ATaSL%20initially%20segregates%20the%20model%20into%20%27skill%20units%27%20based%20on%20parameter%0Adependencies%2C%20allowing%20for%20more%20precise%20control.%20Subsequently%2C%20it%20employs%20a%0Anovel%20group-wise%20skill%20localization%20technique%20to%20ascertain%20the%20importance%0Adistribution%20of%20skill%20units%20for%20a%20new%20task.%20By%20comparing%20this%20importance%0Adistribution%20with%20those%20from%20previous%20tasks%2C%20we%20implement%20a%20fine-grained%20skill%0Aconsolidation%20strategy%20that%20retains%20task-specific%20knowledge%2C%20thereby%20preventing%0Aforgetting%2C%20and%20updates%20task-shared%20knowledge%2C%20which%20facilitates%20bi-directional%0Aknowledge%20transfer.%20As%20a%20result%2C%20TaSL%20achieves%20an%20optimal%20balance%20between%0Aretaining%20prior%20knowledge%20and%20excelling%20in%20new%20tasks.%20TaSL%20also%20demonstrates%0Astrong%20generalizability%2C%20making%20it%20suitable%20for%20various%20base%20models%20and%0Aadaptable%20to%20PEFT%20methods%20like%20LoRA.%20Furthermore%2C%20it%20offers%20notable%0Aextensibility%2C%20supporting%20enhancements%20through%20integration%20with%20memory%20replay%0Atechniques.%20Comprehensive%20experiments%20conducted%20on%20two%20CL%20benchmarks%2C%20involving%0Amodels%20ranging%20from%20220M%20to%207B%20parameters%2C%20affirm%20the%20effectiveness%20of%20TaSL%20and%0Aits%20variants%20across%20different%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05200v2&entry.124074799=Read"},
{"title": "How Knowledge Distillation Mitigates the Synthetic Gap in Fair Face\n  Recognition", "author": "Pedro C. Neto and Ivona Colakovic and Sa\u0161o Karakati\u010d and Ana F. Sequeira", "abstract": "  Leveraging the capabilities of Knowledge Distillation (KD) strategies, we\ndevise a strategy to fight the recent retraction of face recognition datasets.\nGiven a pretrained Teacher model trained on a real dataset, we show that\ncarefully utilising synthetic datasets, or a mix between real and synthetic\ndatasets to distil knowledge from this teacher to smaller students can yield\nsurprising results. In this sense, we trained 33 different models with and\nwithout KD, on different datasets, with different architectures and losses. And\nour findings are consistent, using KD leads to performance gains across all\nethnicities and decreased bias. In addition, it helps to mitigate the\nperformance gap between real and synthetic datasets. This approach addresses\nthe limitations of synthetic data training, improving both the accuracy and\nfairness of face recognition models.\n", "link": "http://arxiv.org/abs/2408.17399v1", "date": "2024-08-30", "relevancy": 2.5051, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.508}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5012}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Knowledge%20Distillation%20Mitigates%20the%20Synthetic%20Gap%20in%20Fair%20Face%0A%20%20Recognition&body=Title%3A%20How%20Knowledge%20Distillation%20Mitigates%20the%20Synthetic%20Gap%20in%20Fair%20Face%0A%20%20Recognition%0AAuthor%3A%20Pedro%20C.%20Neto%20and%20Ivona%20Colakovic%20and%20Sa%C5%A1o%20Karakati%C4%8D%20and%20Ana%20F.%20Sequeira%0AAbstract%3A%20%20%20Leveraging%20the%20capabilities%20of%20Knowledge%20Distillation%20%28KD%29%20strategies%2C%20we%0Adevise%20a%20strategy%20to%20fight%20the%20recent%20retraction%20of%20face%20recognition%20datasets.%0AGiven%20a%20pretrained%20Teacher%20model%20trained%20on%20a%20real%20dataset%2C%20we%20show%20that%0Acarefully%20utilising%20synthetic%20datasets%2C%20or%20a%20mix%20between%20real%20and%20synthetic%0Adatasets%20to%20distil%20knowledge%20from%20this%20teacher%20to%20smaller%20students%20can%20yield%0Asurprising%20results.%20In%20this%20sense%2C%20we%20trained%2033%20different%20models%20with%20and%0Awithout%20KD%2C%20on%20different%20datasets%2C%20with%20different%20architectures%20and%20losses.%20And%0Aour%20findings%20are%20consistent%2C%20using%20KD%20leads%20to%20performance%20gains%20across%20all%0Aethnicities%20and%20decreased%20bias.%20In%20addition%2C%20it%20helps%20to%20mitigate%20the%0Aperformance%20gap%20between%20real%20and%20synthetic%20datasets.%20This%20approach%20addresses%0Athe%20limitations%20of%20synthetic%20data%20training%2C%20improving%20both%20the%20accuracy%20and%0Afairness%20of%20face%20recognition%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Knowledge%2520Distillation%2520Mitigates%2520the%2520Synthetic%2520Gap%2520in%2520Fair%2520Face%250A%2520%2520Recognition%26entry.906535625%3DPedro%2520C.%2520Neto%2520and%2520Ivona%2520Colakovic%2520and%2520Sa%25C5%25A1o%2520Karakati%25C4%258D%2520and%2520Ana%2520F.%2520Sequeira%26entry.1292438233%3D%2520%2520Leveraging%2520the%2520capabilities%2520of%2520Knowledge%2520Distillation%2520%2528KD%2529%2520strategies%252C%2520we%250Adevise%2520a%2520strategy%2520to%2520fight%2520the%2520recent%2520retraction%2520of%2520face%2520recognition%2520datasets.%250AGiven%2520a%2520pretrained%2520Teacher%2520model%2520trained%2520on%2520a%2520real%2520dataset%252C%2520we%2520show%2520that%250Acarefully%2520utilising%2520synthetic%2520datasets%252C%2520or%2520a%2520mix%2520between%2520real%2520and%2520synthetic%250Adatasets%2520to%2520distil%2520knowledge%2520from%2520this%2520teacher%2520to%2520smaller%2520students%2520can%2520yield%250Asurprising%2520results.%2520In%2520this%2520sense%252C%2520we%2520trained%252033%2520different%2520models%2520with%2520and%250Awithout%2520KD%252C%2520on%2520different%2520datasets%252C%2520with%2520different%2520architectures%2520and%2520losses.%2520And%250Aour%2520findings%2520are%2520consistent%252C%2520using%2520KD%2520leads%2520to%2520performance%2520gains%2520across%2520all%250Aethnicities%2520and%2520decreased%2520bias.%2520In%2520addition%252C%2520it%2520helps%2520to%2520mitigate%2520the%250Aperformance%2520gap%2520between%2520real%2520and%2520synthetic%2520datasets.%2520This%2520approach%2520addresses%250Athe%2520limitations%2520of%2520synthetic%2520data%2520training%252C%2520improving%2520both%2520the%2520accuracy%2520and%250Afairness%2520of%2520face%2520recognition%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Knowledge%20Distillation%20Mitigates%20the%20Synthetic%20Gap%20in%20Fair%20Face%0A%20%20Recognition&entry.906535625=Pedro%20C.%20Neto%20and%20Ivona%20Colakovic%20and%20Sa%C5%A1o%20Karakati%C4%8D%20and%20Ana%20F.%20Sequeira&entry.1292438233=%20%20Leveraging%20the%20capabilities%20of%20Knowledge%20Distillation%20%28KD%29%20strategies%2C%20we%0Adevise%20a%20strategy%20to%20fight%20the%20recent%20retraction%20of%20face%20recognition%20datasets.%0AGiven%20a%20pretrained%20Teacher%20model%20trained%20on%20a%20real%20dataset%2C%20we%20show%20that%0Acarefully%20utilising%20synthetic%20datasets%2C%20or%20a%20mix%20between%20real%20and%20synthetic%0Adatasets%20to%20distil%20knowledge%20from%20this%20teacher%20to%20smaller%20students%20can%20yield%0Asurprising%20results.%20In%20this%20sense%2C%20we%20trained%2033%20different%20models%20with%20and%0Awithout%20KD%2C%20on%20different%20datasets%2C%20with%20different%20architectures%20and%20losses.%20And%0Aour%20findings%20are%20consistent%2C%20using%20KD%20leads%20to%20performance%20gains%20across%20all%0Aethnicities%20and%20decreased%20bias.%20In%20addition%2C%20it%20helps%20to%20mitigate%20the%0Aperformance%20gap%20between%20real%20and%20synthetic%20datasets.%20This%20approach%20addresses%0Athe%20limitations%20of%20synthetic%20data%20training%2C%20improving%20both%20the%20accuracy%20and%0Afairness%20of%20face%20recognition%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17399v1&entry.124074799=Read"},
{"title": "Non-Homophilic Graph Pre-Training and Prompt Learning", "author": "Xingtong Yu and Jie Zhang and Yuan Fang and Renhe Jiang", "abstract": "  Graphs are ubiquitous for modeling complex relationships between objects\nacross various fields. Graph neural networks (GNNs) have become a mainstream\ntechnique for graph-based applications, but their performance heavily relies on\nabundant labeled data. To reduce labeling requirement, pre-training and prompt\nlearning has become a popular alternative. However, most existing prompt\nmethods do not differentiate homophilic and heterophilic characteristics of\nreal-world graphs. In particular, many real-world graphs are non-homophilic,\nnot strictly or uniformly homophilic with mixing homophilic and heterophilic\npatterns, exhibiting varying non-homophilic characteristics across graphs and\nnodes. In this paper, we propose ProNoG, a novel pre-training and prompt\nlearning framework for such non-homophilic graphs. First, we analyze existing\ngraph pre-training methods, providing theoretical insights into the choice of\npre-training tasks. Second, recognizing that each node exhibits unique\nnon-homophilic characteristics, we propose a conditional network to\ncharacterize the node-specific patterns in downstream tasks. Finally, we\nthoroughly evaluate and analyze ProNoG through extensive experiments on ten\npublic datasets.\n", "link": "http://arxiv.org/abs/2408.12594v3", "date": "2024-08-30", "relevancy": 2.4702, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5198}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4949}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Homophilic%20Graph%20Pre-Training%20and%20Prompt%20Learning&body=Title%3A%20Non-Homophilic%20Graph%20Pre-Training%20and%20Prompt%20Learning%0AAuthor%3A%20Xingtong%20Yu%20and%20Jie%20Zhang%20and%20Yuan%20Fang%20and%20Renhe%20Jiang%0AAbstract%3A%20%20%20Graphs%20are%20ubiquitous%20for%20modeling%20complex%20relationships%20between%20objects%0Aacross%20various%20fields.%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20a%20mainstream%0Atechnique%20for%20graph-based%20applications%2C%20but%20their%20performance%20heavily%20relies%20on%0Aabundant%20labeled%20data.%20To%20reduce%20labeling%20requirement%2C%20pre-training%20and%20prompt%0Alearning%20has%20become%20a%20popular%20alternative.%20However%2C%20most%20existing%20prompt%0Amethods%20do%20not%20differentiate%20homophilic%20and%20heterophilic%20characteristics%20of%0Areal-world%20graphs.%20In%20particular%2C%20many%20real-world%20graphs%20are%20non-homophilic%2C%0Anot%20strictly%20or%20uniformly%20homophilic%20with%20mixing%20homophilic%20and%20heterophilic%0Apatterns%2C%20exhibiting%20varying%20non-homophilic%20characteristics%20across%20graphs%20and%0Anodes.%20In%20this%20paper%2C%20we%20propose%20ProNoG%2C%20a%20novel%20pre-training%20and%20prompt%0Alearning%20framework%20for%20such%20non-homophilic%20graphs.%20First%2C%20we%20analyze%20existing%0Agraph%20pre-training%20methods%2C%20providing%20theoretical%20insights%20into%20the%20choice%20of%0Apre-training%20tasks.%20Second%2C%20recognizing%20that%20each%20node%20exhibits%20unique%0Anon-homophilic%20characteristics%2C%20we%20propose%20a%20conditional%20network%20to%0Acharacterize%20the%20node-specific%20patterns%20in%20downstream%20tasks.%20Finally%2C%20we%0Athoroughly%20evaluate%20and%20analyze%20ProNoG%20through%20extensive%20experiments%20on%20ten%0Apublic%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12594v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Homophilic%2520Graph%2520Pre-Training%2520and%2520Prompt%2520Learning%26entry.906535625%3DXingtong%2520Yu%2520and%2520Jie%2520Zhang%2520and%2520Yuan%2520Fang%2520and%2520Renhe%2520Jiang%26entry.1292438233%3D%2520%2520Graphs%2520are%2520ubiquitous%2520for%2520modeling%2520complex%2520relationships%2520between%2520objects%250Aacross%2520various%2520fields.%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520become%2520a%2520mainstream%250Atechnique%2520for%2520graph-based%2520applications%252C%2520but%2520their%2520performance%2520heavily%2520relies%2520on%250Aabundant%2520labeled%2520data.%2520To%2520reduce%2520labeling%2520requirement%252C%2520pre-training%2520and%2520prompt%250Alearning%2520has%2520become%2520a%2520popular%2520alternative.%2520However%252C%2520most%2520existing%2520prompt%250Amethods%2520do%2520not%2520differentiate%2520homophilic%2520and%2520heterophilic%2520characteristics%2520of%250Areal-world%2520graphs.%2520In%2520particular%252C%2520many%2520real-world%2520graphs%2520are%2520non-homophilic%252C%250Anot%2520strictly%2520or%2520uniformly%2520homophilic%2520with%2520mixing%2520homophilic%2520and%2520heterophilic%250Apatterns%252C%2520exhibiting%2520varying%2520non-homophilic%2520characteristics%2520across%2520graphs%2520and%250Anodes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ProNoG%252C%2520a%2520novel%2520pre-training%2520and%2520prompt%250Alearning%2520framework%2520for%2520such%2520non-homophilic%2520graphs.%2520First%252C%2520we%2520analyze%2520existing%250Agraph%2520pre-training%2520methods%252C%2520providing%2520theoretical%2520insights%2520into%2520the%2520choice%2520of%250Apre-training%2520tasks.%2520Second%252C%2520recognizing%2520that%2520each%2520node%2520exhibits%2520unique%250Anon-homophilic%2520characteristics%252C%2520we%2520propose%2520a%2520conditional%2520network%2520to%250Acharacterize%2520the%2520node-specific%2520patterns%2520in%2520downstream%2520tasks.%2520Finally%252C%2520we%250Athoroughly%2520evaluate%2520and%2520analyze%2520ProNoG%2520through%2520extensive%2520experiments%2520on%2520ten%250Apublic%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12594v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Homophilic%20Graph%20Pre-Training%20and%20Prompt%20Learning&entry.906535625=Xingtong%20Yu%20and%20Jie%20Zhang%20and%20Yuan%20Fang%20and%20Renhe%20Jiang&entry.1292438233=%20%20Graphs%20are%20ubiquitous%20for%20modeling%20complex%20relationships%20between%20objects%0Aacross%20various%20fields.%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20a%20mainstream%0Atechnique%20for%20graph-based%20applications%2C%20but%20their%20performance%20heavily%20relies%20on%0Aabundant%20labeled%20data.%20To%20reduce%20labeling%20requirement%2C%20pre-training%20and%20prompt%0Alearning%20has%20become%20a%20popular%20alternative.%20However%2C%20most%20existing%20prompt%0Amethods%20do%20not%20differentiate%20homophilic%20and%20heterophilic%20characteristics%20of%0Areal-world%20graphs.%20In%20particular%2C%20many%20real-world%20graphs%20are%20non-homophilic%2C%0Anot%20strictly%20or%20uniformly%20homophilic%20with%20mixing%20homophilic%20and%20heterophilic%0Apatterns%2C%20exhibiting%20varying%20non-homophilic%20characteristics%20across%20graphs%20and%0Anodes.%20In%20this%20paper%2C%20we%20propose%20ProNoG%2C%20a%20novel%20pre-training%20and%20prompt%0Alearning%20framework%20for%20such%20non-homophilic%20graphs.%20First%2C%20we%20analyze%20existing%0Agraph%20pre-training%20methods%2C%20providing%20theoretical%20insights%20into%20the%20choice%20of%0Apre-training%20tasks.%20Second%2C%20recognizing%20that%20each%20node%20exhibits%20unique%0Anon-homophilic%20characteristics%2C%20we%20propose%20a%20conditional%20network%20to%0Acharacterize%20the%20node-specific%20patterns%20in%20downstream%20tasks.%20Finally%2C%20we%0Athoroughly%20evaluate%20and%20analyze%20ProNoG%20through%20extensive%20experiments%20on%20ten%0Apublic%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12594v3&entry.124074799=Read"},
{"title": "How Could Generative AI Support Compliance with the EU AI Act? A Review\n  for Safe Automated Driving Perception", "author": "Mert Keser and Youssef Shoeb and Alois Knoll", "abstract": "  Deep Neural Networks (DNNs) have become central for the perception functions\nof autonomous vehicles, substantially enhancing their ability to understand and\ninterpret the environment. However, these systems exhibit inherent limitations\nsuch as brittleness, opacity, and unpredictable behavior in out-of-distribution\nscenarios. The European Union (EU) Artificial Intelligence (AI) Act, as a\npioneering legislative framework, aims to address these challenges by\nestablishing stringent norms and standards for AI systems, including those used\nin autonomous driving (AD), which are categorized as high-risk AI. In this\nwork, we explore how the newly available generative AI models can potentially\nsupport addressing upcoming regulatory requirements in AD perception,\nparticularly with respect to safety. This short review paper summarizes the\nrequirements arising from the EU AI Act regarding DNN-based perception systems\nand systematically categorizes existing generative AI applications in AD. While\ngenerative AI models show promise in addressing some of the EU AI Acts\nrequirements, such as transparency and robustness, this review examines their\npotential benefits and discusses how developers could leverage these methods to\nenhance compliance with the Act. The paper also highlights areas where further\nresearch is needed to ensure reliable and safe integration of these\ntechnologies.\n", "link": "http://arxiv.org/abs/2408.17222v1", "date": "2024-08-30", "relevancy": 2.4595, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5169}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4804}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Could%20Generative%20AI%20Support%20Compliance%20with%20the%20EU%20AI%20Act%3F%20A%20Review%0A%20%20for%20Safe%20Automated%20Driving%20Perception&body=Title%3A%20How%20Could%20Generative%20AI%20Support%20Compliance%20with%20the%20EU%20AI%20Act%3F%20A%20Review%0A%20%20for%20Safe%20Automated%20Driving%20Perception%0AAuthor%3A%20Mert%20Keser%20and%20Youssef%20Shoeb%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20become%20central%20for%20the%20perception%20functions%0Aof%20autonomous%20vehicles%2C%20substantially%20enhancing%20their%20ability%20to%20understand%20and%0Ainterpret%20the%20environment.%20However%2C%20these%20systems%20exhibit%20inherent%20limitations%0Asuch%20as%20brittleness%2C%20opacity%2C%20and%20unpredictable%20behavior%20in%20out-of-distribution%0Ascenarios.%20The%20European%20Union%20%28EU%29%20Artificial%20Intelligence%20%28AI%29%20Act%2C%20as%20a%0Apioneering%20legislative%20framework%2C%20aims%20to%20address%20these%20challenges%20by%0Aestablishing%20stringent%20norms%20and%20standards%20for%20AI%20systems%2C%20including%20those%20used%0Ain%20autonomous%20driving%20%28AD%29%2C%20which%20are%20categorized%20as%20high-risk%20AI.%20In%20this%0Awork%2C%20we%20explore%20how%20the%20newly%20available%20generative%20AI%20models%20can%20potentially%0Asupport%20addressing%20upcoming%20regulatory%20requirements%20in%20AD%20perception%2C%0Aparticularly%20with%20respect%20to%20safety.%20This%20short%20review%20paper%20summarizes%20the%0Arequirements%20arising%20from%20the%20EU%20AI%20Act%20regarding%20DNN-based%20perception%20systems%0Aand%20systematically%20categorizes%20existing%20generative%20AI%20applications%20in%20AD.%20While%0Agenerative%20AI%20models%20show%20promise%20in%20addressing%20some%20of%20the%20EU%20AI%20Acts%0Arequirements%2C%20such%20as%20transparency%20and%20robustness%2C%20this%20review%20examines%20their%0Apotential%20benefits%20and%20discusses%20how%20developers%20could%20leverage%20these%20methods%20to%0Aenhance%20compliance%20with%20the%20Act.%20The%20paper%20also%20highlights%20areas%20where%20further%0Aresearch%20is%20needed%20to%20ensure%20reliable%20and%20safe%20integration%20of%20these%0Atechnologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Could%2520Generative%2520AI%2520Support%2520Compliance%2520with%2520the%2520EU%2520AI%2520Act%253F%2520A%2520Review%250A%2520%2520for%2520Safe%2520Automated%2520Driving%2520Perception%26entry.906535625%3DMert%2520Keser%2520and%2520Youssef%2520Shoeb%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520have%2520become%2520central%2520for%2520the%2520perception%2520functions%250Aof%2520autonomous%2520vehicles%252C%2520substantially%2520enhancing%2520their%2520ability%2520to%2520understand%2520and%250Ainterpret%2520the%2520environment.%2520However%252C%2520these%2520systems%2520exhibit%2520inherent%2520limitations%250Asuch%2520as%2520brittleness%252C%2520opacity%252C%2520and%2520unpredictable%2520behavior%2520in%2520out-of-distribution%250Ascenarios.%2520The%2520European%2520Union%2520%2528EU%2529%2520Artificial%2520Intelligence%2520%2528AI%2529%2520Act%252C%2520as%2520a%250Apioneering%2520legislative%2520framework%252C%2520aims%2520to%2520address%2520these%2520challenges%2520by%250Aestablishing%2520stringent%2520norms%2520and%2520standards%2520for%2520AI%2520systems%252C%2520including%2520those%2520used%250Ain%2520autonomous%2520driving%2520%2528AD%2529%252C%2520which%2520are%2520categorized%2520as%2520high-risk%2520AI.%2520In%2520this%250Awork%252C%2520we%2520explore%2520how%2520the%2520newly%2520available%2520generative%2520AI%2520models%2520can%2520potentially%250Asupport%2520addressing%2520upcoming%2520regulatory%2520requirements%2520in%2520AD%2520perception%252C%250Aparticularly%2520with%2520respect%2520to%2520safety.%2520This%2520short%2520review%2520paper%2520summarizes%2520the%250Arequirements%2520arising%2520from%2520the%2520EU%2520AI%2520Act%2520regarding%2520DNN-based%2520perception%2520systems%250Aand%2520systematically%2520categorizes%2520existing%2520generative%2520AI%2520applications%2520in%2520AD.%2520While%250Agenerative%2520AI%2520models%2520show%2520promise%2520in%2520addressing%2520some%2520of%2520the%2520EU%2520AI%2520Acts%250Arequirements%252C%2520such%2520as%2520transparency%2520and%2520robustness%252C%2520this%2520review%2520examines%2520their%250Apotential%2520benefits%2520and%2520discusses%2520how%2520developers%2520could%2520leverage%2520these%2520methods%2520to%250Aenhance%2520compliance%2520with%2520the%2520Act.%2520The%2520paper%2520also%2520highlights%2520areas%2520where%2520further%250Aresearch%2520is%2520needed%2520to%2520ensure%2520reliable%2520and%2520safe%2520integration%2520of%2520these%250Atechnologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Could%20Generative%20AI%20Support%20Compliance%20with%20the%20EU%20AI%20Act%3F%20A%20Review%0A%20%20for%20Safe%20Automated%20Driving%20Perception&entry.906535625=Mert%20Keser%20and%20Youssef%20Shoeb%20and%20Alois%20Knoll&entry.1292438233=%20%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20become%20central%20for%20the%20perception%20functions%0Aof%20autonomous%20vehicles%2C%20substantially%20enhancing%20their%20ability%20to%20understand%20and%0Ainterpret%20the%20environment.%20However%2C%20these%20systems%20exhibit%20inherent%20limitations%0Asuch%20as%20brittleness%2C%20opacity%2C%20and%20unpredictable%20behavior%20in%20out-of-distribution%0Ascenarios.%20The%20European%20Union%20%28EU%29%20Artificial%20Intelligence%20%28AI%29%20Act%2C%20as%20a%0Apioneering%20legislative%20framework%2C%20aims%20to%20address%20these%20challenges%20by%0Aestablishing%20stringent%20norms%20and%20standards%20for%20AI%20systems%2C%20including%20those%20used%0Ain%20autonomous%20driving%20%28AD%29%2C%20which%20are%20categorized%20as%20high-risk%20AI.%20In%20this%0Awork%2C%20we%20explore%20how%20the%20newly%20available%20generative%20AI%20models%20can%20potentially%0Asupport%20addressing%20upcoming%20regulatory%20requirements%20in%20AD%20perception%2C%0Aparticularly%20with%20respect%20to%20safety.%20This%20short%20review%20paper%20summarizes%20the%0Arequirements%20arising%20from%20the%20EU%20AI%20Act%20regarding%20DNN-based%20perception%20systems%0Aand%20systematically%20categorizes%20existing%20generative%20AI%20applications%20in%20AD.%20While%0Agenerative%20AI%20models%20show%20promise%20in%20addressing%20some%20of%20the%20EU%20AI%20Acts%0Arequirements%2C%20such%20as%20transparency%20and%20robustness%2C%20this%20review%20examines%20their%0Apotential%20benefits%20and%20discusses%20how%20developers%20could%20leverage%20these%20methods%20to%0Aenhance%20compliance%20with%20the%20Act.%20The%20paper%20also%20highlights%20areas%20where%20further%0Aresearch%20is%20needed%20to%20ensure%20reliable%20and%20safe%20integration%20of%20these%0Atechnologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17222v1&entry.124074799=Read"},
{"title": "Localization Under Consistent Assumptions Over Dynamics", "author": "Matti Pekkanen and Francesco Verdoja and Ville Kyrki", "abstract": "  Accurate maps are a prerequisite for virtually all mobile robot tasks. Most\nstate-of-the-art maps assume a static world; therefore, dynamic objects are\nfiltered out of the measurements. However, this division ignores movable but\nnon-moving -- i.e., semi-static -- objects, which are usually recorded in the\nmap and treated as static objects, violating the static world assumption and\ncausing errors in the localization. This paper presents a method for\nconsistently modeling moving and movable objects to match the map and\nmeasurements. This reduces the error resulting from inconsistent categorization\nand treatment of non-static measurements. A semantic segmentation network is\nused to categorize the measurements into static and semi-static classes, and a\nbackground subtraction filter is used to remove dynamic measurements. Finally,\nwe show that consistent assumptions over dynamics improve localization accuracy\nwhen compared against a state-of-the-art baseline solution using real-world\ndata from the Oxford Radar RobotCar data set.\n", "link": "http://arxiv.org/abs/2305.16702v3", "date": "2024-08-30", "relevancy": 2.4135, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6376}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5915}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localization%20Under%20Consistent%20Assumptions%20Over%20Dynamics&body=Title%3A%20Localization%20Under%20Consistent%20Assumptions%20Over%20Dynamics%0AAuthor%3A%20Matti%20Pekkanen%20and%20Francesco%20Verdoja%20and%20Ville%20Kyrki%0AAbstract%3A%20%20%20Accurate%20maps%20are%20a%20prerequisite%20for%20virtually%20all%20mobile%20robot%20tasks.%20Most%0Astate-of-the-art%20maps%20assume%20a%20static%20world%3B%20therefore%2C%20dynamic%20objects%20are%0Afiltered%20out%20of%20the%20measurements.%20However%2C%20this%20division%20ignores%20movable%20but%0Anon-moving%20--%20i.e.%2C%20semi-static%20--%20objects%2C%20which%20are%20usually%20recorded%20in%20the%0Amap%20and%20treated%20as%20static%20objects%2C%20violating%20the%20static%20world%20assumption%20and%0Acausing%20errors%20in%20the%20localization.%20This%20paper%20presents%20a%20method%20for%0Aconsistently%20modeling%20moving%20and%20movable%20objects%20to%20match%20the%20map%20and%0Ameasurements.%20This%20reduces%20the%20error%20resulting%20from%20inconsistent%20categorization%0Aand%20treatment%20of%20non-static%20measurements.%20A%20semantic%20segmentation%20network%20is%0Aused%20to%20categorize%20the%20measurements%20into%20static%20and%20semi-static%20classes%2C%20and%20a%0Abackground%20subtraction%20filter%20is%20used%20to%20remove%20dynamic%20measurements.%20Finally%2C%0Awe%20show%20that%20consistent%20assumptions%20over%20dynamics%20improve%20localization%20accuracy%0Awhen%20compared%20against%20a%20state-of-the-art%20baseline%20solution%20using%20real-world%0Adata%20from%20the%20Oxford%20Radar%20RobotCar%20data%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.16702v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalization%2520Under%2520Consistent%2520Assumptions%2520Over%2520Dynamics%26entry.906535625%3DMatti%2520Pekkanen%2520and%2520Francesco%2520Verdoja%2520and%2520Ville%2520Kyrki%26entry.1292438233%3D%2520%2520Accurate%2520maps%2520are%2520a%2520prerequisite%2520for%2520virtually%2520all%2520mobile%2520robot%2520tasks.%2520Most%250Astate-of-the-art%2520maps%2520assume%2520a%2520static%2520world%253B%2520therefore%252C%2520dynamic%2520objects%2520are%250Afiltered%2520out%2520of%2520the%2520measurements.%2520However%252C%2520this%2520division%2520ignores%2520movable%2520but%250Anon-moving%2520--%2520i.e.%252C%2520semi-static%2520--%2520objects%252C%2520which%2520are%2520usually%2520recorded%2520in%2520the%250Amap%2520and%2520treated%2520as%2520static%2520objects%252C%2520violating%2520the%2520static%2520world%2520assumption%2520and%250Acausing%2520errors%2520in%2520the%2520localization.%2520This%2520paper%2520presents%2520a%2520method%2520for%250Aconsistently%2520modeling%2520moving%2520and%2520movable%2520objects%2520to%2520match%2520the%2520map%2520and%250Ameasurements.%2520This%2520reduces%2520the%2520error%2520resulting%2520from%2520inconsistent%2520categorization%250Aand%2520treatment%2520of%2520non-static%2520measurements.%2520A%2520semantic%2520segmentation%2520network%2520is%250Aused%2520to%2520categorize%2520the%2520measurements%2520into%2520static%2520and%2520semi-static%2520classes%252C%2520and%2520a%250Abackground%2520subtraction%2520filter%2520is%2520used%2520to%2520remove%2520dynamic%2520measurements.%2520Finally%252C%250Awe%2520show%2520that%2520consistent%2520assumptions%2520over%2520dynamics%2520improve%2520localization%2520accuracy%250Awhen%2520compared%2520against%2520a%2520state-of-the-art%2520baseline%2520solution%2520using%2520real-world%250Adata%2520from%2520the%2520Oxford%2520Radar%2520RobotCar%2520data%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.16702v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localization%20Under%20Consistent%20Assumptions%20Over%20Dynamics&entry.906535625=Matti%20Pekkanen%20and%20Francesco%20Verdoja%20and%20Ville%20Kyrki&entry.1292438233=%20%20Accurate%20maps%20are%20a%20prerequisite%20for%20virtually%20all%20mobile%20robot%20tasks.%20Most%0Astate-of-the-art%20maps%20assume%20a%20static%20world%3B%20therefore%2C%20dynamic%20objects%20are%0Afiltered%20out%20of%20the%20measurements.%20However%2C%20this%20division%20ignores%20movable%20but%0Anon-moving%20--%20i.e.%2C%20semi-static%20--%20objects%2C%20which%20are%20usually%20recorded%20in%20the%0Amap%20and%20treated%20as%20static%20objects%2C%20violating%20the%20static%20world%20assumption%20and%0Acausing%20errors%20in%20the%20localization.%20This%20paper%20presents%20a%20method%20for%0Aconsistently%20modeling%20moving%20and%20movable%20objects%20to%20match%20the%20map%20and%0Ameasurements.%20This%20reduces%20the%20error%20resulting%20from%20inconsistent%20categorization%0Aand%20treatment%20of%20non-static%20measurements.%20A%20semantic%20segmentation%20network%20is%0Aused%20to%20categorize%20the%20measurements%20into%20static%20and%20semi-static%20classes%2C%20and%20a%0Abackground%20subtraction%20filter%20is%20used%20to%20remove%20dynamic%20measurements.%20Finally%2C%0Awe%20show%20that%20consistent%20assumptions%20over%20dynamics%20improve%20localization%20accuracy%0Awhen%20compared%20against%20a%20state-of-the-art%20baseline%20solution%20using%20real-world%0Adata%20from%20the%20Oxford%20Radar%20RobotCar%20data%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.16702v3&entry.124074799=Read"},
{"title": "Hybridizing Base-Line 2D-CNN Model with Cat Swarm Optimization for\n  Enhanced Advanced Persistent Threat Detection", "author": "Ali M. Bakhiet and Salah A. Aly", "abstract": "  In the realm of cyber-security, detecting Advanced Persistent Threats (APTs)\nremains a formidable challenge due to their stealthy and sophisticated nature.\nThis research paper presents an innovative approach that leverages\nConvolutional Neural Networks (CNNs) with a 2D baseline model, enhanced by the\ncutting-edge Cat Swarm Optimization (CSO) algorithm, to significantly improve\nAPT detection accuracy. By seamlessly integrating the 2D-CNN baseline model\nwith CSO, we unlock the potential for unprecedented accuracy and efficiency in\nAPT detection. The results unveil an impressive accuracy score of $98.4\\%$,\nmarking a significant enhancement in APT detection across various attack\nstages, illuminating a path forward in combating these relentless and\nsophisticated threats.\n", "link": "http://arxiv.org/abs/2408.17307v1", "date": "2024-08-30", "relevancy": 2.4011, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4924}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4799}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybridizing%20Base-Line%202D-CNN%20Model%20with%20Cat%20Swarm%20Optimization%20for%0A%20%20Enhanced%20Advanced%20Persistent%20Threat%20Detection&body=Title%3A%20Hybridizing%20Base-Line%202D-CNN%20Model%20with%20Cat%20Swarm%20Optimization%20for%0A%20%20Enhanced%20Advanced%20Persistent%20Threat%20Detection%0AAuthor%3A%20Ali%20M.%20Bakhiet%20and%20Salah%20A.%20Aly%0AAbstract%3A%20%20%20In%20the%20realm%20of%20cyber-security%2C%20detecting%20Advanced%20Persistent%20Threats%20%28APTs%29%0Aremains%20a%20formidable%20challenge%20due%20to%20their%20stealthy%20and%20sophisticated%20nature.%0AThis%20research%20paper%20presents%20an%20innovative%20approach%20that%20leverages%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20with%20a%202D%20baseline%20model%2C%20enhanced%20by%20the%0Acutting-edge%20Cat%20Swarm%20Optimization%20%28CSO%29%20algorithm%2C%20to%20significantly%20improve%0AAPT%20detection%20accuracy.%20By%20seamlessly%20integrating%20the%202D-CNN%20baseline%20model%0Awith%20CSO%2C%20we%20unlock%20the%20potential%20for%20unprecedented%20accuracy%20and%20efficiency%20in%0AAPT%20detection.%20The%20results%20unveil%20an%20impressive%20accuracy%20score%20of%20%2498.4%5C%25%24%2C%0Amarking%20a%20significant%20enhancement%20in%20APT%20detection%20across%20various%20attack%0Astages%2C%20illuminating%20a%20path%20forward%20in%20combating%20these%20relentless%20and%0Asophisticated%20threats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybridizing%2520Base-Line%25202D-CNN%2520Model%2520with%2520Cat%2520Swarm%2520Optimization%2520for%250A%2520%2520Enhanced%2520Advanced%2520Persistent%2520Threat%2520Detection%26entry.906535625%3DAli%2520M.%2520Bakhiet%2520and%2520Salah%2520A.%2520Aly%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520cyber-security%252C%2520detecting%2520Advanced%2520Persistent%2520Threats%2520%2528APTs%2529%250Aremains%2520a%2520formidable%2520challenge%2520due%2520to%2520their%2520stealthy%2520and%2520sophisticated%2520nature.%250AThis%2520research%2520paper%2520presents%2520an%2520innovative%2520approach%2520that%2520leverages%250AConvolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520with%2520a%25202D%2520baseline%2520model%252C%2520enhanced%2520by%2520the%250Acutting-edge%2520Cat%2520Swarm%2520Optimization%2520%2528CSO%2529%2520algorithm%252C%2520to%2520significantly%2520improve%250AAPT%2520detection%2520accuracy.%2520By%2520seamlessly%2520integrating%2520the%25202D-CNN%2520baseline%2520model%250Awith%2520CSO%252C%2520we%2520unlock%2520the%2520potential%2520for%2520unprecedented%2520accuracy%2520and%2520efficiency%2520in%250AAPT%2520detection.%2520The%2520results%2520unveil%2520an%2520impressive%2520accuracy%2520score%2520of%2520%252498.4%255C%2525%2524%252C%250Amarking%2520a%2520significant%2520enhancement%2520in%2520APT%2520detection%2520across%2520various%2520attack%250Astages%252C%2520illuminating%2520a%2520path%2520forward%2520in%2520combating%2520these%2520relentless%2520and%250Asophisticated%2520threats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybridizing%20Base-Line%202D-CNN%20Model%20with%20Cat%20Swarm%20Optimization%20for%0A%20%20Enhanced%20Advanced%20Persistent%20Threat%20Detection&entry.906535625=Ali%20M.%20Bakhiet%20and%20Salah%20A.%20Aly&entry.1292438233=%20%20In%20the%20realm%20of%20cyber-security%2C%20detecting%20Advanced%20Persistent%20Threats%20%28APTs%29%0Aremains%20a%20formidable%20challenge%20due%20to%20their%20stealthy%20and%20sophisticated%20nature.%0AThis%20research%20paper%20presents%20an%20innovative%20approach%20that%20leverages%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20with%20a%202D%20baseline%20model%2C%20enhanced%20by%20the%0Acutting-edge%20Cat%20Swarm%20Optimization%20%28CSO%29%20algorithm%2C%20to%20significantly%20improve%0AAPT%20detection%20accuracy.%20By%20seamlessly%20integrating%20the%202D-CNN%20baseline%20model%0Awith%20CSO%2C%20we%20unlock%20the%20potential%20for%20unprecedented%20accuracy%20and%20efficiency%20in%0AAPT%20detection.%20The%20results%20unveil%20an%20impressive%20accuracy%20score%20of%20%2498.4%5C%25%24%2C%0Amarking%20a%20significant%20enhancement%20in%20APT%20detection%20across%20various%20attack%0Astages%2C%20illuminating%20a%20path%20forward%20in%20combating%20these%20relentless%20and%0Asophisticated%20threats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17307v1&entry.124074799=Read"},
{"title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio\n  Language Model", "author": "Zhen Ye and Peiwen Sun and Jiahe Lei and Hongzhan Lin and Xu Tan and Zheqi Dai and Qiuqiang Kong and Jianyi Chen and Jiahao Pan and Qifeng Liu and Yike Guo and Wei Xue", "abstract": "  Recent advancements in audio generation have been significantly propelled by\nthe capabilities of Large Language Models (LLMs). The existing research on\naudio LLM has primarily focused on enhancing the architecture and scale of\naudio language models, as well as leveraging larger datasets, and generally,\nacoustic codecs, such as EnCodec, are used for audio tokenization. However,\nthese codecs were originally designed for audio compression, which may lead to\nsuboptimal performance in the context of audio LLM. Our research aims to\naddress the shortcomings of current audio LLM codecs, particularly their\nchallenges in maintaining semantic integrity in generated audio. For instance,\nexisting methods like VALL-E, which condition acoustic token generation on text\ntranscriptions, often suffer from content inaccuracies and elevated word error\nrates (WER) due to semantic misinterpretations of acoustic tokens, resulting in\nword skipping and errors. To overcome these issues, we propose a\nstraightforward yet effective approach called X-Codec. X-Codec incorporates\nsemantic features from a pre-trained semantic encoder before the Residual\nVector Quantization (RVQ) stage and introduces a semantic reconstruction loss\nafter RVQ. By enhancing the semantic ability of the codec, X-Codec\nsignificantly reduces WER in speech synthesis tasks and extends these benefits\nto non-speech applications, including music and sound generation. Our\nexperiments in text-to-speech, music continuation, and text-to-sound tasks\ndemonstrate that integrating semantic information substantially improves the\noverall performance of language models in audio generation. Our code and demo\nare available (Demo: https://x-codec-audio.github.io Code:\nhttps://github.com/zhenye234/xcodec)\n", "link": "http://arxiv.org/abs/2408.17175v1", "date": "2024-08-30", "relevancy": 2.3874, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5006}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4715}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Codec%20Does%20Matter%3A%20Exploring%20the%20Semantic%20Shortcoming%20of%20Codec%20for%20Audio%0A%20%20Language%20Model&body=Title%3A%20Codec%20Does%20Matter%3A%20Exploring%20the%20Semantic%20Shortcoming%20of%20Codec%20for%20Audio%0A%20%20Language%20Model%0AAuthor%3A%20Zhen%20Ye%20and%20Peiwen%20Sun%20and%20Jiahe%20Lei%20and%20Hongzhan%20Lin%20and%20Xu%20Tan%20and%20Zheqi%20Dai%20and%20Qiuqiang%20Kong%20and%20Jianyi%20Chen%20and%20Jiahao%20Pan%20and%20Qifeng%20Liu%20and%20Yike%20Guo%20and%20Wei%20Xue%0AAbstract%3A%20%20%20Recent%20advancements%20in%20audio%20generation%20have%20been%20significantly%20propelled%20by%0Athe%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20The%20existing%20research%20on%0Aaudio%20LLM%20has%20primarily%20focused%20on%20enhancing%20the%20architecture%20and%20scale%20of%0Aaudio%20language%20models%2C%20as%20well%20as%20leveraging%20larger%20datasets%2C%20and%20generally%2C%0Aacoustic%20codecs%2C%20such%20as%20EnCodec%2C%20are%20used%20for%20audio%20tokenization.%20However%2C%0Athese%20codecs%20were%20originally%20designed%20for%20audio%20compression%2C%20which%20may%20lead%20to%0Asuboptimal%20performance%20in%20the%20context%20of%20audio%20LLM.%20Our%20research%20aims%20to%0Aaddress%20the%20shortcomings%20of%20current%20audio%20LLM%20codecs%2C%20particularly%20their%0Achallenges%20in%20maintaining%20semantic%20integrity%20in%20generated%20audio.%20For%20instance%2C%0Aexisting%20methods%20like%20VALL-E%2C%20which%20condition%20acoustic%20token%20generation%20on%20text%0Atranscriptions%2C%20often%20suffer%20from%20content%20inaccuracies%20and%20elevated%20word%20error%0Arates%20%28WER%29%20due%20to%20semantic%20misinterpretations%20of%20acoustic%20tokens%2C%20resulting%20in%0Aword%20skipping%20and%20errors.%20To%20overcome%20these%20issues%2C%20we%20propose%20a%0Astraightforward%20yet%20effective%20approach%20called%20X-Codec.%20X-Codec%20incorporates%0Asemantic%20features%20from%20a%20pre-trained%20semantic%20encoder%20before%20the%20Residual%0AVector%20Quantization%20%28RVQ%29%20stage%20and%20introduces%20a%20semantic%20reconstruction%20loss%0Aafter%20RVQ.%20By%20enhancing%20the%20semantic%20ability%20of%20the%20codec%2C%20X-Codec%0Asignificantly%20reduces%20WER%20in%20speech%20synthesis%20tasks%20and%20extends%20these%20benefits%0Ato%20non-speech%20applications%2C%20including%20music%20and%20sound%20generation.%20Our%0Aexperiments%20in%20text-to-speech%2C%20music%20continuation%2C%20and%20text-to-sound%20tasks%0Ademonstrate%20that%20integrating%20semantic%20information%20substantially%20improves%20the%0Aoverall%20performance%20of%20language%20models%20in%20audio%20generation.%20Our%20code%20and%20demo%0Aare%20available%20%28Demo%3A%20https%3A//x-codec-audio.github.io%20Code%3A%0Ahttps%3A//github.com/zhenye234/xcodec%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodec%2520Does%2520Matter%253A%2520Exploring%2520the%2520Semantic%2520Shortcoming%2520of%2520Codec%2520for%2520Audio%250A%2520%2520Language%2520Model%26entry.906535625%3DZhen%2520Ye%2520and%2520Peiwen%2520Sun%2520and%2520Jiahe%2520Lei%2520and%2520Hongzhan%2520Lin%2520and%2520Xu%2520Tan%2520and%2520Zheqi%2520Dai%2520and%2520Qiuqiang%2520Kong%2520and%2520Jianyi%2520Chen%2520and%2520Jiahao%2520Pan%2520and%2520Qifeng%2520Liu%2520and%2520Yike%2520Guo%2520and%2520Wei%2520Xue%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520audio%2520generation%2520have%2520been%2520significantly%2520propelled%2520by%250Athe%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520The%2520existing%2520research%2520on%250Aaudio%2520LLM%2520has%2520primarily%2520focused%2520on%2520enhancing%2520the%2520architecture%2520and%2520scale%2520of%250Aaudio%2520language%2520models%252C%2520as%2520well%2520as%2520leveraging%2520larger%2520datasets%252C%2520and%2520generally%252C%250Aacoustic%2520codecs%252C%2520such%2520as%2520EnCodec%252C%2520are%2520used%2520for%2520audio%2520tokenization.%2520However%252C%250Athese%2520codecs%2520were%2520originally%2520designed%2520for%2520audio%2520compression%252C%2520which%2520may%2520lead%2520to%250Asuboptimal%2520performance%2520in%2520the%2520context%2520of%2520audio%2520LLM.%2520Our%2520research%2520aims%2520to%250Aaddress%2520the%2520shortcomings%2520of%2520current%2520audio%2520LLM%2520codecs%252C%2520particularly%2520their%250Achallenges%2520in%2520maintaining%2520semantic%2520integrity%2520in%2520generated%2520audio.%2520For%2520instance%252C%250Aexisting%2520methods%2520like%2520VALL-E%252C%2520which%2520condition%2520acoustic%2520token%2520generation%2520on%2520text%250Atranscriptions%252C%2520often%2520suffer%2520from%2520content%2520inaccuracies%2520and%2520elevated%2520word%2520error%250Arates%2520%2528WER%2529%2520due%2520to%2520semantic%2520misinterpretations%2520of%2520acoustic%2520tokens%252C%2520resulting%2520in%250Aword%2520skipping%2520and%2520errors.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520propose%2520a%250Astraightforward%2520yet%2520effective%2520approach%2520called%2520X-Codec.%2520X-Codec%2520incorporates%250Asemantic%2520features%2520from%2520a%2520pre-trained%2520semantic%2520encoder%2520before%2520the%2520Residual%250AVector%2520Quantization%2520%2528RVQ%2529%2520stage%2520and%2520introduces%2520a%2520semantic%2520reconstruction%2520loss%250Aafter%2520RVQ.%2520By%2520enhancing%2520the%2520semantic%2520ability%2520of%2520the%2520codec%252C%2520X-Codec%250Asignificantly%2520reduces%2520WER%2520in%2520speech%2520synthesis%2520tasks%2520and%2520extends%2520these%2520benefits%250Ato%2520non-speech%2520applications%252C%2520including%2520music%2520and%2520sound%2520generation.%2520Our%250Aexperiments%2520in%2520text-to-speech%252C%2520music%2520continuation%252C%2520and%2520text-to-sound%2520tasks%250Ademonstrate%2520that%2520integrating%2520semantic%2520information%2520substantially%2520improves%2520the%250Aoverall%2520performance%2520of%2520language%2520models%2520in%2520audio%2520generation.%2520Our%2520code%2520and%2520demo%250Aare%2520available%2520%2528Demo%253A%2520https%253A//x-codec-audio.github.io%2520Code%253A%250Ahttps%253A//github.com/zhenye234/xcodec%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Codec%20Does%20Matter%3A%20Exploring%20the%20Semantic%20Shortcoming%20of%20Codec%20for%20Audio%0A%20%20Language%20Model&entry.906535625=Zhen%20Ye%20and%20Peiwen%20Sun%20and%20Jiahe%20Lei%20and%20Hongzhan%20Lin%20and%20Xu%20Tan%20and%20Zheqi%20Dai%20and%20Qiuqiang%20Kong%20and%20Jianyi%20Chen%20and%20Jiahao%20Pan%20and%20Qifeng%20Liu%20and%20Yike%20Guo%20and%20Wei%20Xue&entry.1292438233=%20%20Recent%20advancements%20in%20audio%20generation%20have%20been%20significantly%20propelled%20by%0Athe%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20The%20existing%20research%20on%0Aaudio%20LLM%20has%20primarily%20focused%20on%20enhancing%20the%20architecture%20and%20scale%20of%0Aaudio%20language%20models%2C%20as%20well%20as%20leveraging%20larger%20datasets%2C%20and%20generally%2C%0Aacoustic%20codecs%2C%20such%20as%20EnCodec%2C%20are%20used%20for%20audio%20tokenization.%20However%2C%0Athese%20codecs%20were%20originally%20designed%20for%20audio%20compression%2C%20which%20may%20lead%20to%0Asuboptimal%20performance%20in%20the%20context%20of%20audio%20LLM.%20Our%20research%20aims%20to%0Aaddress%20the%20shortcomings%20of%20current%20audio%20LLM%20codecs%2C%20particularly%20their%0Achallenges%20in%20maintaining%20semantic%20integrity%20in%20generated%20audio.%20For%20instance%2C%0Aexisting%20methods%20like%20VALL-E%2C%20which%20condition%20acoustic%20token%20generation%20on%20text%0Atranscriptions%2C%20often%20suffer%20from%20content%20inaccuracies%20and%20elevated%20word%20error%0Arates%20%28WER%29%20due%20to%20semantic%20misinterpretations%20of%20acoustic%20tokens%2C%20resulting%20in%0Aword%20skipping%20and%20errors.%20To%20overcome%20these%20issues%2C%20we%20propose%20a%0Astraightforward%20yet%20effective%20approach%20called%20X-Codec.%20X-Codec%20incorporates%0Asemantic%20features%20from%20a%20pre-trained%20semantic%20encoder%20before%20the%20Residual%0AVector%20Quantization%20%28RVQ%29%20stage%20and%20introduces%20a%20semantic%20reconstruction%20loss%0Aafter%20RVQ.%20By%20enhancing%20the%20semantic%20ability%20of%20the%20codec%2C%20X-Codec%0Asignificantly%20reduces%20WER%20in%20speech%20synthesis%20tasks%20and%20extends%20these%20benefits%0Ato%20non-speech%20applications%2C%20including%20music%20and%20sound%20generation.%20Our%0Aexperiments%20in%20text-to-speech%2C%20music%20continuation%2C%20and%20text-to-sound%20tasks%0Ademonstrate%20that%20integrating%20semantic%20information%20substantially%20improves%20the%0Aoverall%20performance%20of%20language%20models%20in%20audio%20generation.%20Our%20code%20and%20demo%0Aare%20available%20%28Demo%3A%20https%3A//x-codec-audio.github.io%20Code%3A%0Ahttps%3A//github.com/zhenye234/xcodec%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17175v1&entry.124074799=Read"},
{"title": "Look, Learn and Leverage (L$^3$): Mitigating Visual-Domain Shift and\n  Discovering Intrinsic Relations via Symbolic Alignment", "author": "Hanchen Xie and Jiageng Zhu and Mahyar Khayatkhoei and Jiazhi Li and Wael AbdAlmageed", "abstract": "  Modern deep learning models have demonstrated outstanding performance on\ndiscovering the underlying mechanisms when both visual appearance and intrinsic\nrelations (e.g., causal structure) data are sufficient, such as Disentangled\nRepresentation Learning (DRL), Causal Representation Learning (CRL) and Visual\nQuestion Answering (VQA) methods. However, generalization ability of these\nmodels is challenged when the visual domain shifts and the relations data is\nabsent during finetuning. To address this challenge, we propose a novel\nlearning framework, Look, Learn and Leverage (L$^3$), which decomposes the\nlearning process into three distinct phases and systematically utilize the\nclass-agnostic segmentation masks as the common symbolic space to align visual\ndomains. Thus, a relations discovery model can be trained on the source domain,\nand when the visual domain shifts and the intrinsic relations are absent, the\npretrained relations discovery model can be directly reused and maintain a\nsatisfactory performance. Extensive performance evaluations are conducted on\nthree different tasks: DRL, CRL and VQA, and show outstanding results on all\nthree tasks, which reveals the advantages of L$^3$.\n", "link": "http://arxiv.org/abs/2408.17363v1", "date": "2024-08-30", "relevancy": 2.3747, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.621}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5772}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look%2C%20Learn%20and%20Leverage%20%28L%24%5E3%24%29%3A%20Mitigating%20Visual-Domain%20Shift%20and%0A%20%20Discovering%20Intrinsic%20Relations%20via%20Symbolic%20Alignment&body=Title%3A%20Look%2C%20Learn%20and%20Leverage%20%28L%24%5E3%24%29%3A%20Mitigating%20Visual-Domain%20Shift%20and%0A%20%20Discovering%20Intrinsic%20Relations%20via%20Symbolic%20Alignment%0AAuthor%3A%20Hanchen%20Xie%20and%20Jiageng%20Zhu%20and%20Mahyar%20Khayatkhoei%20and%20Jiazhi%20Li%20and%20Wael%20AbdAlmageed%0AAbstract%3A%20%20%20Modern%20deep%20learning%20models%20have%20demonstrated%20outstanding%20performance%20on%0Adiscovering%20the%20underlying%20mechanisms%20when%20both%20visual%20appearance%20and%20intrinsic%0Arelations%20%28e.g.%2C%20causal%20structure%29%20data%20are%20sufficient%2C%20such%20as%20Disentangled%0ARepresentation%20Learning%20%28DRL%29%2C%20Causal%20Representation%20Learning%20%28CRL%29%20and%20Visual%0AQuestion%20Answering%20%28VQA%29%20methods.%20However%2C%20generalization%20ability%20of%20these%0Amodels%20is%20challenged%20when%20the%20visual%20domain%20shifts%20and%20the%20relations%20data%20is%0Aabsent%20during%20finetuning.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%0Alearning%20framework%2C%20Look%2C%20Learn%20and%20Leverage%20%28L%24%5E3%24%29%2C%20which%20decomposes%20the%0Alearning%20process%20into%20three%20distinct%20phases%20and%20systematically%20utilize%20the%0Aclass-agnostic%20segmentation%20masks%20as%20the%20common%20symbolic%20space%20to%20align%20visual%0Adomains.%20Thus%2C%20a%20relations%20discovery%20model%20can%20be%20trained%20on%20the%20source%20domain%2C%0Aand%20when%20the%20visual%20domain%20shifts%20and%20the%20intrinsic%20relations%20are%20absent%2C%20the%0Apretrained%20relations%20discovery%20model%20can%20be%20directly%20reused%20and%20maintain%20a%0Asatisfactory%20performance.%20Extensive%20performance%20evaluations%20are%20conducted%20on%0Athree%20different%20tasks%3A%20DRL%2C%20CRL%20and%20VQA%2C%20and%20show%20outstanding%20results%20on%20all%0Athree%20tasks%2C%20which%20reveals%20the%20advantages%20of%20L%24%5E3%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17363v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook%252C%2520Learn%2520and%2520Leverage%2520%2528L%2524%255E3%2524%2529%253A%2520Mitigating%2520Visual-Domain%2520Shift%2520and%250A%2520%2520Discovering%2520Intrinsic%2520Relations%2520via%2520Symbolic%2520Alignment%26entry.906535625%3DHanchen%2520Xie%2520and%2520Jiageng%2520Zhu%2520and%2520Mahyar%2520Khayatkhoei%2520and%2520Jiazhi%2520Li%2520and%2520Wael%2520AbdAlmageed%26entry.1292438233%3D%2520%2520Modern%2520deep%2520learning%2520models%2520have%2520demonstrated%2520outstanding%2520performance%2520on%250Adiscovering%2520the%2520underlying%2520mechanisms%2520when%2520both%2520visual%2520appearance%2520and%2520intrinsic%250Arelations%2520%2528e.g.%252C%2520causal%2520structure%2529%2520data%2520are%2520sufficient%252C%2520such%2520as%2520Disentangled%250ARepresentation%2520Learning%2520%2528DRL%2529%252C%2520Causal%2520Representation%2520Learning%2520%2528CRL%2529%2520and%2520Visual%250AQuestion%2520Answering%2520%2528VQA%2529%2520methods.%2520However%252C%2520generalization%2520ability%2520of%2520these%250Amodels%2520is%2520challenged%2520when%2520the%2520visual%2520domain%2520shifts%2520and%2520the%2520relations%2520data%2520is%250Aabsent%2520during%2520finetuning.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%250Alearning%2520framework%252C%2520Look%252C%2520Learn%2520and%2520Leverage%2520%2528L%2524%255E3%2524%2529%252C%2520which%2520decomposes%2520the%250Alearning%2520process%2520into%2520three%2520distinct%2520phases%2520and%2520systematically%2520utilize%2520the%250Aclass-agnostic%2520segmentation%2520masks%2520as%2520the%2520common%2520symbolic%2520space%2520to%2520align%2520visual%250Adomains.%2520Thus%252C%2520a%2520relations%2520discovery%2520model%2520can%2520be%2520trained%2520on%2520the%2520source%2520domain%252C%250Aand%2520when%2520the%2520visual%2520domain%2520shifts%2520and%2520the%2520intrinsic%2520relations%2520are%2520absent%252C%2520the%250Apretrained%2520relations%2520discovery%2520model%2520can%2520be%2520directly%2520reused%2520and%2520maintain%2520a%250Asatisfactory%2520performance.%2520Extensive%2520performance%2520evaluations%2520are%2520conducted%2520on%250Athree%2520different%2520tasks%253A%2520DRL%252C%2520CRL%2520and%2520VQA%252C%2520and%2520show%2520outstanding%2520results%2520on%2520all%250Athree%2520tasks%252C%2520which%2520reveals%2520the%2520advantages%2520of%2520L%2524%255E3%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17363v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%2C%20Learn%20and%20Leverage%20%28L%24%5E3%24%29%3A%20Mitigating%20Visual-Domain%20Shift%20and%0A%20%20Discovering%20Intrinsic%20Relations%20via%20Symbolic%20Alignment&entry.906535625=Hanchen%20Xie%20and%20Jiageng%20Zhu%20and%20Mahyar%20Khayatkhoei%20and%20Jiazhi%20Li%20and%20Wael%20AbdAlmageed&entry.1292438233=%20%20Modern%20deep%20learning%20models%20have%20demonstrated%20outstanding%20performance%20on%0Adiscovering%20the%20underlying%20mechanisms%20when%20both%20visual%20appearance%20and%20intrinsic%0Arelations%20%28e.g.%2C%20causal%20structure%29%20data%20are%20sufficient%2C%20such%20as%20Disentangled%0ARepresentation%20Learning%20%28DRL%29%2C%20Causal%20Representation%20Learning%20%28CRL%29%20and%20Visual%0AQuestion%20Answering%20%28VQA%29%20methods.%20However%2C%20generalization%20ability%20of%20these%0Amodels%20is%20challenged%20when%20the%20visual%20domain%20shifts%20and%20the%20relations%20data%20is%0Aabsent%20during%20finetuning.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%0Alearning%20framework%2C%20Look%2C%20Learn%20and%20Leverage%20%28L%24%5E3%24%29%2C%20which%20decomposes%20the%0Alearning%20process%20into%20three%20distinct%20phases%20and%20systematically%20utilize%20the%0Aclass-agnostic%20segmentation%20masks%20as%20the%20common%20symbolic%20space%20to%20align%20visual%0Adomains.%20Thus%2C%20a%20relations%20discovery%20model%20can%20be%20trained%20on%20the%20source%20domain%2C%0Aand%20when%20the%20visual%20domain%20shifts%20and%20the%20intrinsic%20relations%20are%20absent%2C%20the%0Apretrained%20relations%20discovery%20model%20can%20be%20directly%20reused%20and%20maintain%20a%0Asatisfactory%20performance.%20Extensive%20performance%20evaluations%20are%20conducted%20on%0Athree%20different%20tasks%3A%20DRL%2C%20CRL%20and%20VQA%2C%20and%20show%20outstanding%20results%20on%20all%0Athree%20tasks%2C%20which%20reveals%20the%20advantages%20of%20L%24%5E3%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17363v1&entry.124074799=Read"},
{"title": "Evaluation and Deployment of LiDAR-based Place Recognition in Dense\n  Forests", "author": "Haedam Oh and Nived Chebrolu and Matias Mattamala and Leonard Frei\u00dfmuth and Maurice Fallon", "abstract": "  Many LiDAR place recognition systems have been developed and tested\nspecifically for urban driving scenarios. Their performance in natural\nenvironments such as forests and woodlands have been studied less closely. In\nthis paper, we analyzed the capabilities of four different LiDAR place\nrecognition systems, both handcrafted and learning-based methods, using LiDAR\ndata collected with a handheld device and legged robot within dense forest\nenvironments. In particular, we focused on evaluating localization where there\nis significant translational and orientation difference between corresponding\nLiDAR scan pairs. This is particularly important for forest survey systems\nwhere the sensor or robot does not follow a defined road or path. Extending our\nanalysis we then incorporated the best performing approach, Logg3dNet, into a\nfull 6-DoF pose estimation system -- introducing several verification layers\nfor precise registration. We demonstrated the performance of our methods in\nthree operational modes: online SLAM, offline multi-mission SLAM map merging,\nand relocalization into a prior map. We evaluated these modes using data\ncaptured in forests from three different countries, achieving 80% of correct\nloop closures candidates with baseline distances up to 5m, and 60% up to 10m.\nVideo at: https://youtu.be/86l-oxjwmjY\n", "link": "http://arxiv.org/abs/2403.14326v2", "date": "2024-08-30", "relevancy": 2.2631, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5958}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5601}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20and%20Deployment%20of%20LiDAR-based%20Place%20Recognition%20in%20Dense%0A%20%20Forests&body=Title%3A%20Evaluation%20and%20Deployment%20of%20LiDAR-based%20Place%20Recognition%20in%20Dense%0A%20%20Forests%0AAuthor%3A%20Haedam%20Oh%20and%20Nived%20Chebrolu%20and%20Matias%20Mattamala%20and%20Leonard%20Frei%C3%9Fmuth%20and%20Maurice%20Fallon%0AAbstract%3A%20%20%20Many%20LiDAR%20place%20recognition%20systems%20have%20been%20developed%20and%20tested%0Aspecifically%20for%20urban%20driving%20scenarios.%20Their%20performance%20in%20natural%0Aenvironments%20such%20as%20forests%20and%20woodlands%20have%20been%20studied%20less%20closely.%20In%0Athis%20paper%2C%20we%20analyzed%20the%20capabilities%20of%20four%20different%20LiDAR%20place%0Arecognition%20systems%2C%20both%20handcrafted%20and%20learning-based%20methods%2C%20using%20LiDAR%0Adata%20collected%20with%20a%20handheld%20device%20and%20legged%20robot%20within%20dense%20forest%0Aenvironments.%20In%20particular%2C%20we%20focused%20on%20evaluating%20localization%20where%20there%0Ais%20significant%20translational%20and%20orientation%20difference%20between%20corresponding%0ALiDAR%20scan%20pairs.%20This%20is%20particularly%20important%20for%20forest%20survey%20systems%0Awhere%20the%20sensor%20or%20robot%20does%20not%20follow%20a%20defined%20road%20or%20path.%20Extending%20our%0Aanalysis%20we%20then%20incorporated%20the%20best%20performing%20approach%2C%20Logg3dNet%2C%20into%20a%0Afull%206-DoF%20pose%20estimation%20system%20--%20introducing%20several%20verification%20layers%0Afor%20precise%20registration.%20We%20demonstrated%20the%20performance%20of%20our%20methods%20in%0Athree%20operational%20modes%3A%20online%20SLAM%2C%20offline%20multi-mission%20SLAM%20map%20merging%2C%0Aand%20relocalization%20into%20a%20prior%20map.%20We%20evaluated%20these%20modes%20using%20data%0Acaptured%20in%20forests%20from%20three%20different%20countries%2C%20achieving%2080%25%20of%20correct%0Aloop%20closures%20candidates%20with%20baseline%20distances%20up%20to%205m%2C%20and%2060%25%20up%20to%2010m.%0AVideo%20at%3A%20https%3A//youtu.be/86l-oxjwmjY%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14326v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520and%2520Deployment%2520of%2520LiDAR-based%2520Place%2520Recognition%2520in%2520Dense%250A%2520%2520Forests%26entry.906535625%3DHaedam%2520Oh%2520and%2520Nived%2520Chebrolu%2520and%2520Matias%2520Mattamala%2520and%2520Leonard%2520Frei%25C3%259Fmuth%2520and%2520Maurice%2520Fallon%26entry.1292438233%3D%2520%2520Many%2520LiDAR%2520place%2520recognition%2520systems%2520have%2520been%2520developed%2520and%2520tested%250Aspecifically%2520for%2520urban%2520driving%2520scenarios.%2520Their%2520performance%2520in%2520natural%250Aenvironments%2520such%2520as%2520forests%2520and%2520woodlands%2520have%2520been%2520studied%2520less%2520closely.%2520In%250Athis%2520paper%252C%2520we%2520analyzed%2520the%2520capabilities%2520of%2520four%2520different%2520LiDAR%2520place%250Arecognition%2520systems%252C%2520both%2520handcrafted%2520and%2520learning-based%2520methods%252C%2520using%2520LiDAR%250Adata%2520collected%2520with%2520a%2520handheld%2520device%2520and%2520legged%2520robot%2520within%2520dense%2520forest%250Aenvironments.%2520In%2520particular%252C%2520we%2520focused%2520on%2520evaluating%2520localization%2520where%2520there%250Ais%2520significant%2520translational%2520and%2520orientation%2520difference%2520between%2520corresponding%250ALiDAR%2520scan%2520pairs.%2520This%2520is%2520particularly%2520important%2520for%2520forest%2520survey%2520systems%250Awhere%2520the%2520sensor%2520or%2520robot%2520does%2520not%2520follow%2520a%2520defined%2520road%2520or%2520path.%2520Extending%2520our%250Aanalysis%2520we%2520then%2520incorporated%2520the%2520best%2520performing%2520approach%252C%2520Logg3dNet%252C%2520into%2520a%250Afull%25206-DoF%2520pose%2520estimation%2520system%2520--%2520introducing%2520several%2520verification%2520layers%250Afor%2520precise%2520registration.%2520We%2520demonstrated%2520the%2520performance%2520of%2520our%2520methods%2520in%250Athree%2520operational%2520modes%253A%2520online%2520SLAM%252C%2520offline%2520multi-mission%2520SLAM%2520map%2520merging%252C%250Aand%2520relocalization%2520into%2520a%2520prior%2520map.%2520We%2520evaluated%2520these%2520modes%2520using%2520data%250Acaptured%2520in%2520forests%2520from%2520three%2520different%2520countries%252C%2520achieving%252080%2525%2520of%2520correct%250Aloop%2520closures%2520candidates%2520with%2520baseline%2520distances%2520up%2520to%25205m%252C%2520and%252060%2525%2520up%2520to%252010m.%250AVideo%2520at%253A%2520https%253A//youtu.be/86l-oxjwmjY%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14326v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20and%20Deployment%20of%20LiDAR-based%20Place%20Recognition%20in%20Dense%0A%20%20Forests&entry.906535625=Haedam%20Oh%20and%20Nived%20Chebrolu%20and%20Matias%20Mattamala%20and%20Leonard%20Frei%C3%9Fmuth%20and%20Maurice%20Fallon&entry.1292438233=%20%20Many%20LiDAR%20place%20recognition%20systems%20have%20been%20developed%20and%20tested%0Aspecifically%20for%20urban%20driving%20scenarios.%20Their%20performance%20in%20natural%0Aenvironments%20such%20as%20forests%20and%20woodlands%20have%20been%20studied%20less%20closely.%20In%0Athis%20paper%2C%20we%20analyzed%20the%20capabilities%20of%20four%20different%20LiDAR%20place%0Arecognition%20systems%2C%20both%20handcrafted%20and%20learning-based%20methods%2C%20using%20LiDAR%0Adata%20collected%20with%20a%20handheld%20device%20and%20legged%20robot%20within%20dense%20forest%0Aenvironments.%20In%20particular%2C%20we%20focused%20on%20evaluating%20localization%20where%20there%0Ais%20significant%20translational%20and%20orientation%20difference%20between%20corresponding%0ALiDAR%20scan%20pairs.%20This%20is%20particularly%20important%20for%20forest%20survey%20systems%0Awhere%20the%20sensor%20or%20robot%20does%20not%20follow%20a%20defined%20road%20or%20path.%20Extending%20our%0Aanalysis%20we%20then%20incorporated%20the%20best%20performing%20approach%2C%20Logg3dNet%2C%20into%20a%0Afull%206-DoF%20pose%20estimation%20system%20--%20introducing%20several%20verification%20layers%0Afor%20precise%20registration.%20We%20demonstrated%20the%20performance%20of%20our%20methods%20in%0Athree%20operational%20modes%3A%20online%20SLAM%2C%20offline%20multi-mission%20SLAM%20map%20merging%2C%0Aand%20relocalization%20into%20a%20prior%20map.%20We%20evaluated%20these%20modes%20using%20data%0Acaptured%20in%20forests%20from%20three%20different%20countries%2C%20achieving%2080%25%20of%20correct%0Aloop%20closures%20candidates%20with%20baseline%20distances%20up%20to%205m%2C%20and%2060%25%20up%20to%2010m.%0AVideo%20at%3A%20https%3A//youtu.be/86l-oxjwmjY%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14326v2&entry.124074799=Read"},
{"title": "Open-vocabulary Temporal Action Localization using VLMs", "author": "Naoki Wake and Atsushi Kanehira and Kazuhiro Sasabuchi and Jun Takamatsu and Katsushi Ikeuchi", "abstract": "  Video action localization aims to find timings of a specific action from a\nlong video. Although existing learning-based approaches have been successful,\nthose require annotating videos that come with a considerable labor cost. This\npaper proposes a learning-free, open-vocabulary approach based on emerging\nvision-language models (VLM). The challenge stems from the fact that VLMs are\nneither designed to process long videos nor tailored for finding actions. We\novercome these problems by extending an iterative visual prompting technique.\nSpecifically, we sample video frames into a concatenated image with frame index\nlabels, making a VLM guess a frame that is considered to be closest to the\nstart/end of the action. Iterating this process by narrowing a sampling time\nwindow results in finding a specific frame of start and end of an action. We\ndemonstrate that this sampling technique yields reasonable results,\nillustrating a practical extension of VLMs for understanding videos.\n", "link": "http://arxiv.org/abs/2408.17422v1", "date": "2024-08-30", "relevancy": 2.2049, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5669}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.564}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-vocabulary%20Temporal%20Action%20Localization%20using%20VLMs&body=Title%3A%20Open-vocabulary%20Temporal%20Action%20Localization%20using%20VLMs%0AAuthor%3A%20Naoki%20Wake%20and%20Atsushi%20Kanehira%20and%20Kazuhiro%20Sasabuchi%20and%20Jun%20Takamatsu%20and%20Katsushi%20Ikeuchi%0AAbstract%3A%20%20%20Video%20action%20localization%20aims%20to%20find%20timings%20of%20a%20specific%20action%20from%20a%0Along%20video.%20Although%20existing%20learning-based%20approaches%20have%20been%20successful%2C%0Athose%20require%20annotating%20videos%20that%20come%20with%20a%20considerable%20labor%20cost.%20This%0Apaper%20proposes%20a%20learning-free%2C%20open-vocabulary%20approach%20based%20on%20emerging%0Avision-language%20models%20%28VLM%29.%20The%20challenge%20stems%20from%20the%20fact%20that%20VLMs%20are%0Aneither%20designed%20to%20process%20long%20videos%20nor%20tailored%20for%20finding%20actions.%20We%0Aovercome%20these%20problems%20by%20extending%20an%20iterative%20visual%20prompting%20technique.%0ASpecifically%2C%20we%20sample%20video%20frames%20into%20a%20concatenated%20image%20with%20frame%20index%0Alabels%2C%20making%20a%20VLM%20guess%20a%20frame%20that%20is%20considered%20to%20be%20closest%20to%20the%0Astart/end%20of%20the%20action.%20Iterating%20this%20process%20by%20narrowing%20a%20sampling%20time%0Awindow%20results%20in%20finding%20a%20specific%20frame%20of%20start%20and%20end%20of%20an%20action.%20We%0Ademonstrate%20that%20this%20sampling%20technique%20yields%20reasonable%20results%2C%0Aillustrating%20a%20practical%20extension%20of%20VLMs%20for%20understanding%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-vocabulary%2520Temporal%2520Action%2520Localization%2520using%2520VLMs%26entry.906535625%3DNaoki%2520Wake%2520and%2520Atsushi%2520Kanehira%2520and%2520Kazuhiro%2520Sasabuchi%2520and%2520Jun%2520Takamatsu%2520and%2520Katsushi%2520Ikeuchi%26entry.1292438233%3D%2520%2520Video%2520action%2520localization%2520aims%2520to%2520find%2520timings%2520of%2520a%2520specific%2520action%2520from%2520a%250Along%2520video.%2520Although%2520existing%2520learning-based%2520approaches%2520have%2520been%2520successful%252C%250Athose%2520require%2520annotating%2520videos%2520that%2520come%2520with%2520a%2520considerable%2520labor%2520cost.%2520This%250Apaper%2520proposes%2520a%2520learning-free%252C%2520open-vocabulary%2520approach%2520based%2520on%2520emerging%250Avision-language%2520models%2520%2528VLM%2529.%2520The%2520challenge%2520stems%2520from%2520the%2520fact%2520that%2520VLMs%2520are%250Aneither%2520designed%2520to%2520process%2520long%2520videos%2520nor%2520tailored%2520for%2520finding%2520actions.%2520We%250Aovercome%2520these%2520problems%2520by%2520extending%2520an%2520iterative%2520visual%2520prompting%2520technique.%250ASpecifically%252C%2520we%2520sample%2520video%2520frames%2520into%2520a%2520concatenated%2520image%2520with%2520frame%2520index%250Alabels%252C%2520making%2520a%2520VLM%2520guess%2520a%2520frame%2520that%2520is%2520considered%2520to%2520be%2520closest%2520to%2520the%250Astart/end%2520of%2520the%2520action.%2520Iterating%2520this%2520process%2520by%2520narrowing%2520a%2520sampling%2520time%250Awindow%2520results%2520in%2520finding%2520a%2520specific%2520frame%2520of%2520start%2520and%2520end%2520of%2520an%2520action.%2520We%250Ademonstrate%2520that%2520this%2520sampling%2520technique%2520yields%2520reasonable%2520results%252C%250Aillustrating%2520a%2520practical%2520extension%2520of%2520VLMs%2520for%2520understanding%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-vocabulary%20Temporal%20Action%20Localization%20using%20VLMs&entry.906535625=Naoki%20Wake%20and%20Atsushi%20Kanehira%20and%20Kazuhiro%20Sasabuchi%20and%20Jun%20Takamatsu%20and%20Katsushi%20Ikeuchi&entry.1292438233=%20%20Video%20action%20localization%20aims%20to%20find%20timings%20of%20a%20specific%20action%20from%20a%0Along%20video.%20Although%20existing%20learning-based%20approaches%20have%20been%20successful%2C%0Athose%20require%20annotating%20videos%20that%20come%20with%20a%20considerable%20labor%20cost.%20This%0Apaper%20proposes%20a%20learning-free%2C%20open-vocabulary%20approach%20based%20on%20emerging%0Avision-language%20models%20%28VLM%29.%20The%20challenge%20stems%20from%20the%20fact%20that%20VLMs%20are%0Aneither%20designed%20to%20process%20long%20videos%20nor%20tailored%20for%20finding%20actions.%20We%0Aovercome%20these%20problems%20by%20extending%20an%20iterative%20visual%20prompting%20technique.%0ASpecifically%2C%20we%20sample%20video%20frames%20into%20a%20concatenated%20image%20with%20frame%20index%0Alabels%2C%20making%20a%20VLM%20guess%20a%20frame%20that%20is%20considered%20to%20be%20closest%20to%20the%0Astart/end%20of%20the%20action.%20Iterating%20this%20process%20by%20narrowing%20a%20sampling%20time%0Awindow%20results%20in%20finding%20a%20specific%20frame%20of%20start%20and%20end%20of%20an%20action.%20We%0Ademonstrate%20that%20this%20sampling%20technique%20yields%20reasonable%20results%2C%0Aillustrating%20a%20practical%20extension%20of%20VLMs%20for%20understanding%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17422v1&entry.124074799=Read"},
{"title": "Deep Feature Embedding for Tabular Data", "author": "Yuqian Wu and Hengyi Luo and Raymond S. T. Lee", "abstract": "  Tabular data learning has extensive applications in deep learning but its\nexisting embedding techniques are limited in numerical and categorical features\nsuch as the inability to capture complex relationships and engineering. This\npaper proposes a novel deep embedding framework with leverages lightweight deep\nneural networks to generate effective feature embeddings for tabular data in\nmachine learning research. For numerical features, a two-step feature expansion\nand deep transformation technique is used to capture copious semantic\ninformation. For categorical features, a unique identification vector for each\nentity is referred by a compact lookup table with a parameterized deep\nembedding function to uniform the embedding size dimensions, and transformed\ninto a embedding vector using deep neural network. Experiments are conducted on\nreal-world datasets for performance evaluation.\n", "link": "http://arxiv.org/abs/2408.17162v1", "date": "2024-08-30", "relevancy": 2.2047, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4436}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4411}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Feature%20Embedding%20for%20Tabular%20Data&body=Title%3A%20Deep%20Feature%20Embedding%20for%20Tabular%20Data%0AAuthor%3A%20Yuqian%20Wu%20and%20Hengyi%20Luo%20and%20Raymond%20S.%20T.%20Lee%0AAbstract%3A%20%20%20Tabular%20data%20learning%20has%20extensive%20applications%20in%20deep%20learning%20but%20its%0Aexisting%20embedding%20techniques%20are%20limited%20in%20numerical%20and%20categorical%20features%0Asuch%20as%20the%20inability%20to%20capture%20complex%20relationships%20and%20engineering.%20This%0Apaper%20proposes%20a%20novel%20deep%20embedding%20framework%20with%20leverages%20lightweight%20deep%0Aneural%20networks%20to%20generate%20effective%20feature%20embeddings%20for%20tabular%20data%20in%0Amachine%20learning%20research.%20For%20numerical%20features%2C%20a%20two-step%20feature%20expansion%0Aand%20deep%20transformation%20technique%20is%20used%20to%20capture%20copious%20semantic%0Ainformation.%20For%20categorical%20features%2C%20a%20unique%20identification%20vector%20for%20each%0Aentity%20is%20referred%20by%20a%20compact%20lookup%20table%20with%20a%20parameterized%20deep%0Aembedding%20function%20to%20uniform%20the%20embedding%20size%20dimensions%2C%20and%20transformed%0Ainto%20a%20embedding%20vector%20using%20deep%20neural%20network.%20Experiments%20are%20conducted%20on%0Areal-world%20datasets%20for%20performance%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Feature%2520Embedding%2520for%2520Tabular%2520Data%26entry.906535625%3DYuqian%2520Wu%2520and%2520Hengyi%2520Luo%2520and%2520Raymond%2520S.%2520T.%2520Lee%26entry.1292438233%3D%2520%2520Tabular%2520data%2520learning%2520has%2520extensive%2520applications%2520in%2520deep%2520learning%2520but%2520its%250Aexisting%2520embedding%2520techniques%2520are%2520limited%2520in%2520numerical%2520and%2520categorical%2520features%250Asuch%2520as%2520the%2520inability%2520to%2520capture%2520complex%2520relationships%2520and%2520engineering.%2520This%250Apaper%2520proposes%2520a%2520novel%2520deep%2520embedding%2520framework%2520with%2520leverages%2520lightweight%2520deep%250Aneural%2520networks%2520to%2520generate%2520effective%2520feature%2520embeddings%2520for%2520tabular%2520data%2520in%250Amachine%2520learning%2520research.%2520For%2520numerical%2520features%252C%2520a%2520two-step%2520feature%2520expansion%250Aand%2520deep%2520transformation%2520technique%2520is%2520used%2520to%2520capture%2520copious%2520semantic%250Ainformation.%2520For%2520categorical%2520features%252C%2520a%2520unique%2520identification%2520vector%2520for%2520each%250Aentity%2520is%2520referred%2520by%2520a%2520compact%2520lookup%2520table%2520with%2520a%2520parameterized%2520deep%250Aembedding%2520function%2520to%2520uniform%2520the%2520embedding%2520size%2520dimensions%252C%2520and%2520transformed%250Ainto%2520a%2520embedding%2520vector%2520using%2520deep%2520neural%2520network.%2520Experiments%2520are%2520conducted%2520on%250Areal-world%2520datasets%2520for%2520performance%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Feature%20Embedding%20for%20Tabular%20Data&entry.906535625=Yuqian%20Wu%20and%20Hengyi%20Luo%20and%20Raymond%20S.%20T.%20Lee&entry.1292438233=%20%20Tabular%20data%20learning%20has%20extensive%20applications%20in%20deep%20learning%20but%20its%0Aexisting%20embedding%20techniques%20are%20limited%20in%20numerical%20and%20categorical%20features%0Asuch%20as%20the%20inability%20to%20capture%20complex%20relationships%20and%20engineering.%20This%0Apaper%20proposes%20a%20novel%20deep%20embedding%20framework%20with%20leverages%20lightweight%20deep%0Aneural%20networks%20to%20generate%20effective%20feature%20embeddings%20for%20tabular%20data%20in%0Amachine%20learning%20research.%20For%20numerical%20features%2C%20a%20two-step%20feature%20expansion%0Aand%20deep%20transformation%20technique%20is%20used%20to%20capture%20copious%20semantic%0Ainformation.%20For%20categorical%20features%2C%20a%20unique%20identification%20vector%20for%20each%0Aentity%20is%20referred%20by%20a%20compact%20lookup%20table%20with%20a%20parameterized%20deep%0Aembedding%20function%20to%20uniform%20the%20embedding%20size%20dimensions%2C%20and%20transformed%0Ainto%20a%20embedding%20vector%20using%20deep%20neural%20network.%20Experiments%20are%20conducted%20on%0Areal-world%20datasets%20for%20performance%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17162v1&entry.124074799=Read"},
{"title": "GeoMeter: Probing Depth and Height Perception of Large Visual-Language\n  Models", "author": "Shehreen Azad and Yash Jain and Rishit Garg and Yogesh S Rawat and Vibhav Vineet", "abstract": "  Geometric understanding is crucial for navigating and interacting with our\nenvironment. While large Vision Language Models (VLMs) demonstrate impressive\ncapabilities, deploying them in real-world scenarios necessitates a comparable\ngeometric understanding in visual perception. In this work, we focus on the\ngeometric comprehension of these models; specifically targeting the depths and\nheights of objects within a scene. Our observations reveal that, although VLMs\nexcel in basic geometric properties perception such as shape and size, they\nencounter significant challenges in reasoning about the depth and height of\nobjects. To address this, we introduce GeoMeter, a suite of benchmark datasets\nencompassing Synthetic 2D, Synthetic 3D, and Real-World scenarios to rigorously\nevaluate these aspects. We benchmark 17 state-of-the-art VLMs using these\ndatasets and find that they consistently struggle with both depth and height\nperception. Our key insights include detailed analyses of the shortcomings in\ndepth and height reasoning capabilities of VLMs and the inherent bias present\nin these models. This study aims to pave the way for the development of VLMs\nwith enhanced geometric understanding, crucial for real-world applications.\n", "link": "http://arxiv.org/abs/2408.11748v3", "date": "2024-08-30", "relevancy": 2.2044, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5566}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5558}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoMeter%3A%20Probing%20Depth%20and%20Height%20Perception%20of%20Large%20Visual-Language%0A%20%20Models&body=Title%3A%20GeoMeter%3A%20Probing%20Depth%20and%20Height%20Perception%20of%20Large%20Visual-Language%0A%20%20Models%0AAuthor%3A%20Shehreen%20Azad%20and%20Yash%20Jain%20and%20Rishit%20Garg%20and%20Yogesh%20S%20Rawat%20and%20Vibhav%20Vineet%0AAbstract%3A%20%20%20Geometric%20understanding%20is%20crucial%20for%20navigating%20and%20interacting%20with%20our%0Aenvironment.%20While%20large%20Vision%20Language%20Models%20%28VLMs%29%20demonstrate%20impressive%0Acapabilities%2C%20deploying%20them%20in%20real-world%20scenarios%20necessitates%20a%20comparable%0Ageometric%20understanding%20in%20visual%20perception.%20In%20this%20work%2C%20we%20focus%20on%20the%0Ageometric%20comprehension%20of%20these%20models%3B%20specifically%20targeting%20the%20depths%20and%0Aheights%20of%20objects%20within%20a%20scene.%20Our%20observations%20reveal%20that%2C%20although%20VLMs%0Aexcel%20in%20basic%20geometric%20properties%20perception%20such%20as%20shape%20and%20size%2C%20they%0Aencounter%20significant%20challenges%20in%20reasoning%20about%20the%20depth%20and%20height%20of%0Aobjects.%20To%20address%20this%2C%20we%20introduce%20GeoMeter%2C%20a%20suite%20of%20benchmark%20datasets%0Aencompassing%20Synthetic%202D%2C%20Synthetic%203D%2C%20and%20Real-World%20scenarios%20to%20rigorously%0Aevaluate%20these%20aspects.%20We%20benchmark%2017%20state-of-the-art%20VLMs%20using%20these%0Adatasets%20and%20find%20that%20they%20consistently%20struggle%20with%20both%20depth%20and%20height%0Aperception.%20Our%20key%20insights%20include%20detailed%20analyses%20of%20the%20shortcomings%20in%0Adepth%20and%20height%20reasoning%20capabilities%20of%20VLMs%20and%20the%20inherent%20bias%20present%0Ain%20these%20models.%20This%20study%20aims%20to%20pave%20the%20way%20for%20the%20development%20of%20VLMs%0Awith%20enhanced%20geometric%20understanding%2C%20crucial%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11748v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoMeter%253A%2520Probing%2520Depth%2520and%2520Height%2520Perception%2520of%2520Large%2520Visual-Language%250A%2520%2520Models%26entry.906535625%3DShehreen%2520Azad%2520and%2520Yash%2520Jain%2520and%2520Rishit%2520Garg%2520and%2520Yogesh%2520S%2520Rawat%2520and%2520Vibhav%2520Vineet%26entry.1292438233%3D%2520%2520Geometric%2520understanding%2520is%2520crucial%2520for%2520navigating%2520and%2520interacting%2520with%2520our%250Aenvironment.%2520While%2520large%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520demonstrate%2520impressive%250Acapabilities%252C%2520deploying%2520them%2520in%2520real-world%2520scenarios%2520necessitates%2520a%2520comparable%250Ageometric%2520understanding%2520in%2520visual%2520perception.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520the%250Ageometric%2520comprehension%2520of%2520these%2520models%253B%2520specifically%2520targeting%2520the%2520depths%2520and%250Aheights%2520of%2520objects%2520within%2520a%2520scene.%2520Our%2520observations%2520reveal%2520that%252C%2520although%2520VLMs%250Aexcel%2520in%2520basic%2520geometric%2520properties%2520perception%2520such%2520as%2520shape%2520and%2520size%252C%2520they%250Aencounter%2520significant%2520challenges%2520in%2520reasoning%2520about%2520the%2520depth%2520and%2520height%2520of%250Aobjects.%2520To%2520address%2520this%252C%2520we%2520introduce%2520GeoMeter%252C%2520a%2520suite%2520of%2520benchmark%2520datasets%250Aencompassing%2520Synthetic%25202D%252C%2520Synthetic%25203D%252C%2520and%2520Real-World%2520scenarios%2520to%2520rigorously%250Aevaluate%2520these%2520aspects.%2520We%2520benchmark%252017%2520state-of-the-art%2520VLMs%2520using%2520these%250Adatasets%2520and%2520find%2520that%2520they%2520consistently%2520struggle%2520with%2520both%2520depth%2520and%2520height%250Aperception.%2520Our%2520key%2520insights%2520include%2520detailed%2520analyses%2520of%2520the%2520shortcomings%2520in%250Adepth%2520and%2520height%2520reasoning%2520capabilities%2520of%2520VLMs%2520and%2520the%2520inherent%2520bias%2520present%250Ain%2520these%2520models.%2520This%2520study%2520aims%2520to%2520pave%2520the%2520way%2520for%2520the%2520development%2520of%2520VLMs%250Awith%2520enhanced%2520geometric%2520understanding%252C%2520crucial%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11748v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoMeter%3A%20Probing%20Depth%20and%20Height%20Perception%20of%20Large%20Visual-Language%0A%20%20Models&entry.906535625=Shehreen%20Azad%20and%20Yash%20Jain%20and%20Rishit%20Garg%20and%20Yogesh%20S%20Rawat%20and%20Vibhav%20Vineet&entry.1292438233=%20%20Geometric%20understanding%20is%20crucial%20for%20navigating%20and%20interacting%20with%20our%0Aenvironment.%20While%20large%20Vision%20Language%20Models%20%28VLMs%29%20demonstrate%20impressive%0Acapabilities%2C%20deploying%20them%20in%20real-world%20scenarios%20necessitates%20a%20comparable%0Ageometric%20understanding%20in%20visual%20perception.%20In%20this%20work%2C%20we%20focus%20on%20the%0Ageometric%20comprehension%20of%20these%20models%3B%20specifically%20targeting%20the%20depths%20and%0Aheights%20of%20objects%20within%20a%20scene.%20Our%20observations%20reveal%20that%2C%20although%20VLMs%0Aexcel%20in%20basic%20geometric%20properties%20perception%20such%20as%20shape%20and%20size%2C%20they%0Aencounter%20significant%20challenges%20in%20reasoning%20about%20the%20depth%20and%20height%20of%0Aobjects.%20To%20address%20this%2C%20we%20introduce%20GeoMeter%2C%20a%20suite%20of%20benchmark%20datasets%0Aencompassing%20Synthetic%202D%2C%20Synthetic%203D%2C%20and%20Real-World%20scenarios%20to%20rigorously%0Aevaluate%20these%20aspects.%20We%20benchmark%2017%20state-of-the-art%20VLMs%20using%20these%0Adatasets%20and%20find%20that%20they%20consistently%20struggle%20with%20both%20depth%20and%20height%0Aperception.%20Our%20key%20insights%20include%20detailed%20analyses%20of%20the%20shortcomings%20in%0Adepth%20and%20height%20reasoning%20capabilities%20of%20VLMs%20and%20the%20inherent%20bias%20present%0Ain%20these%20models.%20This%20study%20aims%20to%20pave%20the%20way%20for%20the%20development%20of%20VLMs%0Awith%20enhanced%20geometric%20understanding%2C%20crucial%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11748v3&entry.124074799=Read"},
{"title": "CaFNet: A Confidence-Driven Framework for Radar Camera Depth Estimation", "author": "Huawei Sun and Hao Feng and Julius Ott and Lorenzo Servadei and Robert Wille", "abstract": "  Depth estimation is critical in autonomous driving for interpreting 3D scenes\naccurately. Recently, radar-camera depth estimation has become of sufficient\ninterest due to the robustness and low-cost properties of radar. Thus, this\npaper introduces a two-stage, end-to-end trainable Confidence-aware Fusion Net\n(CaFNet) for dense depth estimation, combining RGB imagery with sparse and\nnoisy radar point cloud data. The first stage addresses radar-specific\nchallenges, such as ambiguous elevation and noisy measurements, by predicting a\nradar confidence map and a preliminary coarse depth map. A novel approach is\npresented for generating the ground truth for the confidence map, which\ninvolves associating each radar point with its corresponding object to identify\npotential projection surfaces. These maps, together with the initial radar\ninput, are processed by a second encoder. For the final depth estimation, we\ninnovate a confidence-aware gated fusion mechanism to integrate radar and image\nfeatures effectively, thereby enhancing the reliability of the depth map by\nfiltering out radar noise. Our methodology, evaluated on the nuScenes dataset,\ndemonstrates superior performance, improving upon the current leading model by\n3.2% in Mean Absolute Error (MAE) and 2.7% in Root Mean Square Error (RMSE).\nCode: https://github.com/harborsarah/CaFNet\n", "link": "http://arxiv.org/abs/2407.00697v3", "date": "2024-08-30", "relevancy": 2.1857, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5778}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5252}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaFNet%3A%20A%20Confidence-Driven%20Framework%20for%20Radar%20Camera%20Depth%20Estimation&body=Title%3A%20CaFNet%3A%20A%20Confidence-Driven%20Framework%20for%20Radar%20Camera%20Depth%20Estimation%0AAuthor%3A%20Huawei%20Sun%20and%20Hao%20Feng%20and%20Julius%20Ott%20and%20Lorenzo%20Servadei%20and%20Robert%20Wille%0AAbstract%3A%20%20%20Depth%20estimation%20is%20critical%20in%20autonomous%20driving%20for%20interpreting%203D%20scenes%0Aaccurately.%20Recently%2C%20radar-camera%20depth%20estimation%20has%20become%20of%20sufficient%0Ainterest%20due%20to%20the%20robustness%20and%20low-cost%20properties%20of%20radar.%20Thus%2C%20this%0Apaper%20introduces%20a%20two-stage%2C%20end-to-end%20trainable%20Confidence-aware%20Fusion%20Net%0A%28CaFNet%29%20for%20dense%20depth%20estimation%2C%20combining%20RGB%20imagery%20with%20sparse%20and%0Anoisy%20radar%20point%20cloud%20data.%20The%20first%20stage%20addresses%20radar-specific%0Achallenges%2C%20such%20as%20ambiguous%20elevation%20and%20noisy%20measurements%2C%20by%20predicting%20a%0Aradar%20confidence%20map%20and%20a%20preliminary%20coarse%20depth%20map.%20A%20novel%20approach%20is%0Apresented%20for%20generating%20the%20ground%20truth%20for%20the%20confidence%20map%2C%20which%0Ainvolves%20associating%20each%20radar%20point%20with%20its%20corresponding%20object%20to%20identify%0Apotential%20projection%20surfaces.%20These%20maps%2C%20together%20with%20the%20initial%20radar%0Ainput%2C%20are%20processed%20by%20a%20second%20encoder.%20For%20the%20final%20depth%20estimation%2C%20we%0Ainnovate%20a%20confidence-aware%20gated%20fusion%20mechanism%20to%20integrate%20radar%20and%20image%0Afeatures%20effectively%2C%20thereby%20enhancing%20the%20reliability%20of%20the%20depth%20map%20by%0Afiltering%20out%20radar%20noise.%20Our%20methodology%2C%20evaluated%20on%20the%20nuScenes%20dataset%2C%0Ademonstrates%20superior%20performance%2C%20improving%20upon%20the%20current%20leading%20model%20by%0A3.2%25%20in%20Mean%20Absolute%20Error%20%28MAE%29%20and%202.7%25%20in%20Root%20Mean%20Square%20Error%20%28RMSE%29.%0ACode%3A%20https%3A//github.com/harborsarah/CaFNet%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00697v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaFNet%253A%2520A%2520Confidence-Driven%2520Framework%2520for%2520Radar%2520Camera%2520Depth%2520Estimation%26entry.906535625%3DHuawei%2520Sun%2520and%2520Hao%2520Feng%2520and%2520Julius%2520Ott%2520and%2520Lorenzo%2520Servadei%2520and%2520Robert%2520Wille%26entry.1292438233%3D%2520%2520Depth%2520estimation%2520is%2520critical%2520in%2520autonomous%2520driving%2520for%2520interpreting%25203D%2520scenes%250Aaccurately.%2520Recently%252C%2520radar-camera%2520depth%2520estimation%2520has%2520become%2520of%2520sufficient%250Ainterest%2520due%2520to%2520the%2520robustness%2520and%2520low-cost%2520properties%2520of%2520radar.%2520Thus%252C%2520this%250Apaper%2520introduces%2520a%2520two-stage%252C%2520end-to-end%2520trainable%2520Confidence-aware%2520Fusion%2520Net%250A%2528CaFNet%2529%2520for%2520dense%2520depth%2520estimation%252C%2520combining%2520RGB%2520imagery%2520with%2520sparse%2520and%250Anoisy%2520radar%2520point%2520cloud%2520data.%2520The%2520first%2520stage%2520addresses%2520radar-specific%250Achallenges%252C%2520such%2520as%2520ambiguous%2520elevation%2520and%2520noisy%2520measurements%252C%2520by%2520predicting%2520a%250Aradar%2520confidence%2520map%2520and%2520a%2520preliminary%2520coarse%2520depth%2520map.%2520A%2520novel%2520approach%2520is%250Apresented%2520for%2520generating%2520the%2520ground%2520truth%2520for%2520the%2520confidence%2520map%252C%2520which%250Ainvolves%2520associating%2520each%2520radar%2520point%2520with%2520its%2520corresponding%2520object%2520to%2520identify%250Apotential%2520projection%2520surfaces.%2520These%2520maps%252C%2520together%2520with%2520the%2520initial%2520radar%250Ainput%252C%2520are%2520processed%2520by%2520a%2520second%2520encoder.%2520For%2520the%2520final%2520depth%2520estimation%252C%2520we%250Ainnovate%2520a%2520confidence-aware%2520gated%2520fusion%2520mechanism%2520to%2520integrate%2520radar%2520and%2520image%250Afeatures%2520effectively%252C%2520thereby%2520enhancing%2520the%2520reliability%2520of%2520the%2520depth%2520map%2520by%250Afiltering%2520out%2520radar%2520noise.%2520Our%2520methodology%252C%2520evaluated%2520on%2520the%2520nuScenes%2520dataset%252C%250Ademonstrates%2520superior%2520performance%252C%2520improving%2520upon%2520the%2520current%2520leading%2520model%2520by%250A3.2%2525%2520in%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%2520and%25202.7%2525%2520in%2520Root%2520Mean%2520Square%2520Error%2520%2528RMSE%2529.%250ACode%253A%2520https%253A//github.com/harborsarah/CaFNet%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00697v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaFNet%3A%20A%20Confidence-Driven%20Framework%20for%20Radar%20Camera%20Depth%20Estimation&entry.906535625=Huawei%20Sun%20and%20Hao%20Feng%20and%20Julius%20Ott%20and%20Lorenzo%20Servadei%20and%20Robert%20Wille&entry.1292438233=%20%20Depth%20estimation%20is%20critical%20in%20autonomous%20driving%20for%20interpreting%203D%20scenes%0Aaccurately.%20Recently%2C%20radar-camera%20depth%20estimation%20has%20become%20of%20sufficient%0Ainterest%20due%20to%20the%20robustness%20and%20low-cost%20properties%20of%20radar.%20Thus%2C%20this%0Apaper%20introduces%20a%20two-stage%2C%20end-to-end%20trainable%20Confidence-aware%20Fusion%20Net%0A%28CaFNet%29%20for%20dense%20depth%20estimation%2C%20combining%20RGB%20imagery%20with%20sparse%20and%0Anoisy%20radar%20point%20cloud%20data.%20The%20first%20stage%20addresses%20radar-specific%0Achallenges%2C%20such%20as%20ambiguous%20elevation%20and%20noisy%20measurements%2C%20by%20predicting%20a%0Aradar%20confidence%20map%20and%20a%20preliminary%20coarse%20depth%20map.%20A%20novel%20approach%20is%0Apresented%20for%20generating%20the%20ground%20truth%20for%20the%20confidence%20map%2C%20which%0Ainvolves%20associating%20each%20radar%20point%20with%20its%20corresponding%20object%20to%20identify%0Apotential%20projection%20surfaces.%20These%20maps%2C%20together%20with%20the%20initial%20radar%0Ainput%2C%20are%20processed%20by%20a%20second%20encoder.%20For%20the%20final%20depth%20estimation%2C%20we%0Ainnovate%20a%20confidence-aware%20gated%20fusion%20mechanism%20to%20integrate%20radar%20and%20image%0Afeatures%20effectively%2C%20thereby%20enhancing%20the%20reliability%20of%20the%20depth%20map%20by%0Afiltering%20out%20radar%20noise.%20Our%20methodology%2C%20evaluated%20on%20the%20nuScenes%20dataset%2C%0Ademonstrates%20superior%20performance%2C%20improving%20upon%20the%20current%20leading%20model%20by%0A3.2%25%20in%20Mean%20Absolute%20Error%20%28MAE%29%20and%202.7%25%20in%20Root%20Mean%20Square%20Error%20%28RMSE%29.%0ACode%3A%20https%3A//github.com/harborsarah/CaFNet%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00697v3&entry.124074799=Read"},
{"title": "DARES: Depth Anything in Robotic Endoscopic Surgery with Self-supervised\n  Vector-LoRA of the Foundation Model", "author": "Mona Sheikh Zeinoddin and Chiara Lena and Jiongqi Qu and Luca Carlini and Mattia Magro and Seunghoi Kim and Elena De Momi and Sophia Bano and Matthew Grech-Sollars and Evangelos Mazomenos and Daniel C. Alexander and Danail Stoyanov and Matthew J. Clarkson and Mobarakol Islam", "abstract": "  Robotic-assisted surgery (RAS) relies on accurate depth estimation for 3D\nreconstruction and visualization. While foundation models like Depth Anything\nModels (DAM) show promise, directly applying them to surgery often yields\nsuboptimal results. Fully fine-tuning on limited surgical data can cause\noverfitting and catastrophic forgetting, compromising model robustness and\ngeneralization. Although Low-Rank Adaptation (LoRA) addresses some adaptation\nissues, its uniform parameter distribution neglects the inherent feature\nhierarchy, where earlier layers, learning more general features, require more\nparameters than later ones. To tackle this issue, we introduce Depth Anything\nin Robotic Endoscopic Surgery (DARES), a novel approach that employs a new\nadaptation technique, Vector Low-Rank Adaptation (Vector-LoRA) on the DAM V2 to\nperform self-supervised monocular depth estimation in RAS scenes. To enhance\nlearning efficiency, we introduce Vector-LoRA by integrating more parameters in\nearlier layers and gradually decreasing parameters in later layers. We also\ndesign a reprojection loss based on the multi-scale SSIM error to enhance depth\nperception by better tailoring the foundation model to the specific\nrequirements of the surgical environment. The proposed method is validated on\nthe SCARED dataset and demonstrates superior performance over recent\nstate-of-the-art self-supervised monocular depth estimation techniques,\nachieving an improvement of 13.3% in the absolute relative error metric. The\ncode and pre-trained weights are available at\nhttps://github.com/mobarakol/DARES.\n", "link": "http://arxiv.org/abs/2408.17433v1", "date": "2024-08-30", "relevancy": 2.1788, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5643}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5409}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DARES%3A%20Depth%20Anything%20in%20Robotic%20Endoscopic%20Surgery%20with%20Self-supervised%0A%20%20Vector-LoRA%20of%20the%20Foundation%20Model&body=Title%3A%20DARES%3A%20Depth%20Anything%20in%20Robotic%20Endoscopic%20Surgery%20with%20Self-supervised%0A%20%20Vector-LoRA%20of%20the%20Foundation%20Model%0AAuthor%3A%20Mona%20Sheikh%20Zeinoddin%20and%20Chiara%20Lena%20and%20Jiongqi%20Qu%20and%20Luca%20Carlini%20and%20Mattia%20Magro%20and%20Seunghoi%20Kim%20and%20Elena%20De%20Momi%20and%20Sophia%20Bano%20and%20Matthew%20Grech-Sollars%20and%20Evangelos%20Mazomenos%20and%20Daniel%20C.%20Alexander%20and%20Danail%20Stoyanov%20and%20Matthew%20J.%20Clarkson%20and%20Mobarakol%20Islam%0AAbstract%3A%20%20%20Robotic-assisted%20surgery%20%28RAS%29%20relies%20on%20accurate%20depth%20estimation%20for%203D%0Areconstruction%20and%20visualization.%20While%20foundation%20models%20like%20Depth%20Anything%0AModels%20%28DAM%29%20show%20promise%2C%20directly%20applying%20them%20to%20surgery%20often%20yields%0Asuboptimal%20results.%20Fully%20fine-tuning%20on%20limited%20surgical%20data%20can%20cause%0Aoverfitting%20and%20catastrophic%20forgetting%2C%20compromising%20model%20robustness%20and%0Ageneralization.%20Although%20Low-Rank%20Adaptation%20%28LoRA%29%20addresses%20some%20adaptation%0Aissues%2C%20its%20uniform%20parameter%20distribution%20neglects%20the%20inherent%20feature%0Ahierarchy%2C%20where%20earlier%20layers%2C%20learning%20more%20general%20features%2C%20require%20more%0Aparameters%20than%20later%20ones.%20To%20tackle%20this%20issue%2C%20we%20introduce%20Depth%20Anything%0Ain%20Robotic%20Endoscopic%20Surgery%20%28DARES%29%2C%20a%20novel%20approach%20that%20employs%20a%20new%0Aadaptation%20technique%2C%20Vector%20Low-Rank%20Adaptation%20%28Vector-LoRA%29%20on%20the%20DAM%20V2%20to%0Aperform%20self-supervised%20monocular%20depth%20estimation%20in%20RAS%20scenes.%20To%20enhance%0Alearning%20efficiency%2C%20we%20introduce%20Vector-LoRA%20by%20integrating%20more%20parameters%20in%0Aearlier%20layers%20and%20gradually%20decreasing%20parameters%20in%20later%20layers.%20We%20also%0Adesign%20a%20reprojection%20loss%20based%20on%20the%20multi-scale%20SSIM%20error%20to%20enhance%20depth%0Aperception%20by%20better%20tailoring%20the%20foundation%20model%20to%20the%20specific%0Arequirements%20of%20the%20surgical%20environment.%20The%20proposed%20method%20is%20validated%20on%0Athe%20SCARED%20dataset%20and%20demonstrates%20superior%20performance%20over%20recent%0Astate-of-the-art%20self-supervised%20monocular%20depth%20estimation%20techniques%2C%0Aachieving%20an%20improvement%20of%2013.3%25%20in%20the%20absolute%20relative%20error%20metric.%20The%0Acode%20and%20pre-trained%20weights%20are%20available%20at%0Ahttps%3A//github.com/mobarakol/DARES.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDARES%253A%2520Depth%2520Anything%2520in%2520Robotic%2520Endoscopic%2520Surgery%2520with%2520Self-supervised%250A%2520%2520Vector-LoRA%2520of%2520the%2520Foundation%2520Model%26entry.906535625%3DMona%2520Sheikh%2520Zeinoddin%2520and%2520Chiara%2520Lena%2520and%2520Jiongqi%2520Qu%2520and%2520Luca%2520Carlini%2520and%2520Mattia%2520Magro%2520and%2520Seunghoi%2520Kim%2520and%2520Elena%2520De%2520Momi%2520and%2520Sophia%2520Bano%2520and%2520Matthew%2520Grech-Sollars%2520and%2520Evangelos%2520Mazomenos%2520and%2520Daniel%2520C.%2520Alexander%2520and%2520Danail%2520Stoyanov%2520and%2520Matthew%2520J.%2520Clarkson%2520and%2520Mobarakol%2520Islam%26entry.1292438233%3D%2520%2520Robotic-assisted%2520surgery%2520%2528RAS%2529%2520relies%2520on%2520accurate%2520depth%2520estimation%2520for%25203D%250Areconstruction%2520and%2520visualization.%2520While%2520foundation%2520models%2520like%2520Depth%2520Anything%250AModels%2520%2528DAM%2529%2520show%2520promise%252C%2520directly%2520applying%2520them%2520to%2520surgery%2520often%2520yields%250Asuboptimal%2520results.%2520Fully%2520fine-tuning%2520on%2520limited%2520surgical%2520data%2520can%2520cause%250Aoverfitting%2520and%2520catastrophic%2520forgetting%252C%2520compromising%2520model%2520robustness%2520and%250Ageneralization.%2520Although%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520addresses%2520some%2520adaptation%250Aissues%252C%2520its%2520uniform%2520parameter%2520distribution%2520neglects%2520the%2520inherent%2520feature%250Ahierarchy%252C%2520where%2520earlier%2520layers%252C%2520learning%2520more%2520general%2520features%252C%2520require%2520more%250Aparameters%2520than%2520later%2520ones.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520introduce%2520Depth%2520Anything%250Ain%2520Robotic%2520Endoscopic%2520Surgery%2520%2528DARES%2529%252C%2520a%2520novel%2520approach%2520that%2520employs%2520a%2520new%250Aadaptation%2520technique%252C%2520Vector%2520Low-Rank%2520Adaptation%2520%2528Vector-LoRA%2529%2520on%2520the%2520DAM%2520V2%2520to%250Aperform%2520self-supervised%2520monocular%2520depth%2520estimation%2520in%2520RAS%2520scenes.%2520To%2520enhance%250Alearning%2520efficiency%252C%2520we%2520introduce%2520Vector-LoRA%2520by%2520integrating%2520more%2520parameters%2520in%250Aearlier%2520layers%2520and%2520gradually%2520decreasing%2520parameters%2520in%2520later%2520layers.%2520We%2520also%250Adesign%2520a%2520reprojection%2520loss%2520based%2520on%2520the%2520multi-scale%2520SSIM%2520error%2520to%2520enhance%2520depth%250Aperception%2520by%2520better%2520tailoring%2520the%2520foundation%2520model%2520to%2520the%2520specific%250Arequirements%2520of%2520the%2520surgical%2520environment.%2520The%2520proposed%2520method%2520is%2520validated%2520on%250Athe%2520SCARED%2520dataset%2520and%2520demonstrates%2520superior%2520performance%2520over%2520recent%250Astate-of-the-art%2520self-supervised%2520monocular%2520depth%2520estimation%2520techniques%252C%250Aachieving%2520an%2520improvement%2520of%252013.3%2525%2520in%2520the%2520absolute%2520relative%2520error%2520metric.%2520The%250Acode%2520and%2520pre-trained%2520weights%2520are%2520available%2520at%250Ahttps%253A//github.com/mobarakol/DARES.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DARES%3A%20Depth%20Anything%20in%20Robotic%20Endoscopic%20Surgery%20with%20Self-supervised%0A%20%20Vector-LoRA%20of%20the%20Foundation%20Model&entry.906535625=Mona%20Sheikh%20Zeinoddin%20and%20Chiara%20Lena%20and%20Jiongqi%20Qu%20and%20Luca%20Carlini%20and%20Mattia%20Magro%20and%20Seunghoi%20Kim%20and%20Elena%20De%20Momi%20and%20Sophia%20Bano%20and%20Matthew%20Grech-Sollars%20and%20Evangelos%20Mazomenos%20and%20Daniel%20C.%20Alexander%20and%20Danail%20Stoyanov%20and%20Matthew%20J.%20Clarkson%20and%20Mobarakol%20Islam&entry.1292438233=%20%20Robotic-assisted%20surgery%20%28RAS%29%20relies%20on%20accurate%20depth%20estimation%20for%203D%0Areconstruction%20and%20visualization.%20While%20foundation%20models%20like%20Depth%20Anything%0AModels%20%28DAM%29%20show%20promise%2C%20directly%20applying%20them%20to%20surgery%20often%20yields%0Asuboptimal%20results.%20Fully%20fine-tuning%20on%20limited%20surgical%20data%20can%20cause%0Aoverfitting%20and%20catastrophic%20forgetting%2C%20compromising%20model%20robustness%20and%0Ageneralization.%20Although%20Low-Rank%20Adaptation%20%28LoRA%29%20addresses%20some%20adaptation%0Aissues%2C%20its%20uniform%20parameter%20distribution%20neglects%20the%20inherent%20feature%0Ahierarchy%2C%20where%20earlier%20layers%2C%20learning%20more%20general%20features%2C%20require%20more%0Aparameters%20than%20later%20ones.%20To%20tackle%20this%20issue%2C%20we%20introduce%20Depth%20Anything%0Ain%20Robotic%20Endoscopic%20Surgery%20%28DARES%29%2C%20a%20novel%20approach%20that%20employs%20a%20new%0Aadaptation%20technique%2C%20Vector%20Low-Rank%20Adaptation%20%28Vector-LoRA%29%20on%20the%20DAM%20V2%20to%0Aperform%20self-supervised%20monocular%20depth%20estimation%20in%20RAS%20scenes.%20To%20enhance%0Alearning%20efficiency%2C%20we%20introduce%20Vector-LoRA%20by%20integrating%20more%20parameters%20in%0Aearlier%20layers%20and%20gradually%20decreasing%20parameters%20in%20later%20layers.%20We%20also%0Adesign%20a%20reprojection%20loss%20based%20on%20the%20multi-scale%20SSIM%20error%20to%20enhance%20depth%0Aperception%20by%20better%20tailoring%20the%20foundation%20model%20to%20the%20specific%0Arequirements%20of%20the%20surgical%20environment.%20The%20proposed%20method%20is%20validated%20on%0Athe%20SCARED%20dataset%20and%20demonstrates%20superior%20performance%20over%20recent%0Astate-of-the-art%20self-supervised%20monocular%20depth%20estimation%20techniques%2C%0Aachieving%20an%20improvement%20of%2013.3%25%20in%20the%20absolute%20relative%20error%20metric.%20The%0Acode%20and%20pre-trained%20weights%20are%20available%20at%0Ahttps%3A//github.com/mobarakol/DARES.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17433v1&entry.124074799=Read"},
{"title": "Improving Online Source-free Domain Adaptation for Object Detection by\n  Unsupervised Data Acquisition", "author": "Xiangyu Shi and Yanyuan Qiao and Qi Wu and Lingqiao Liu and Feras Dayoub", "abstract": "  Effective object detection in autonomous vehicles is challenged by deployment\nin diverse and unfamiliar environments. Online Source-Free Domain Adaptation\n(O-SFDA) offers model adaptation using a stream of unlabeled data from a target\ndomain in an online manner. However, not all captured frames contain\ninformation beneficial for adaptation, especially in the presence of redundant\ndata and class imbalance issues. This paper introduces a novel approach to\nenhance O-SFDA for adaptive object detection through unsupervised data\nacquisition. Our methodology prioritizes the most informative unlabeled frames\nfor inclusion in the online training process. Empirical evaluation on a\nreal-world dataset reveals that our method outperforms existing\nstate-of-the-art O-SFDA techniques, demonstrating the viability of unsupervised\ndata acquisition for improving the adaptive object detector.\n", "link": "http://arxiv.org/abs/2310.19258v3", "date": "2024-08-30", "relevancy": 2.1755, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5551}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5375}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Online%20Source-free%20Domain%20Adaptation%20for%20Object%20Detection%20by%0A%20%20Unsupervised%20Data%20Acquisition&body=Title%3A%20Improving%20Online%20Source-free%20Domain%20Adaptation%20for%20Object%20Detection%20by%0A%20%20Unsupervised%20Data%20Acquisition%0AAuthor%3A%20Xiangyu%20Shi%20and%20Yanyuan%20Qiao%20and%20Qi%20Wu%20and%20Lingqiao%20Liu%20and%20Feras%20Dayoub%0AAbstract%3A%20%20%20Effective%20object%20detection%20in%20autonomous%20vehicles%20is%20challenged%20by%20deployment%0Ain%20diverse%20and%20unfamiliar%20environments.%20Online%20Source-Free%20Domain%20Adaptation%0A%28O-SFDA%29%20offers%20model%20adaptation%20using%20a%20stream%20of%20unlabeled%20data%20from%20a%20target%0Adomain%20in%20an%20online%20manner.%20However%2C%20not%20all%20captured%20frames%20contain%0Ainformation%20beneficial%20for%20adaptation%2C%20especially%20in%20the%20presence%20of%20redundant%0Adata%20and%20class%20imbalance%20issues.%20This%20paper%20introduces%20a%20novel%20approach%20to%0Aenhance%20O-SFDA%20for%20adaptive%20object%20detection%20through%20unsupervised%20data%0Aacquisition.%20Our%20methodology%20prioritizes%20the%20most%20informative%20unlabeled%20frames%0Afor%20inclusion%20in%20the%20online%20training%20process.%20Empirical%20evaluation%20on%20a%0Areal-world%20dataset%20reveals%20that%20our%20method%20outperforms%20existing%0Astate-of-the-art%20O-SFDA%20techniques%2C%20demonstrating%20the%20viability%20of%20unsupervised%0Adata%20acquisition%20for%20improving%20the%20adaptive%20object%20detector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19258v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Online%2520Source-free%2520Domain%2520Adaptation%2520for%2520Object%2520Detection%2520by%250A%2520%2520Unsupervised%2520Data%2520Acquisition%26entry.906535625%3DXiangyu%2520Shi%2520and%2520Yanyuan%2520Qiao%2520and%2520Qi%2520Wu%2520and%2520Lingqiao%2520Liu%2520and%2520Feras%2520Dayoub%26entry.1292438233%3D%2520%2520Effective%2520object%2520detection%2520in%2520autonomous%2520vehicles%2520is%2520challenged%2520by%2520deployment%250Ain%2520diverse%2520and%2520unfamiliar%2520environments.%2520Online%2520Source-Free%2520Domain%2520Adaptation%250A%2528O-SFDA%2529%2520offers%2520model%2520adaptation%2520using%2520a%2520stream%2520of%2520unlabeled%2520data%2520from%2520a%2520target%250Adomain%2520in%2520an%2520online%2520manner.%2520However%252C%2520not%2520all%2520captured%2520frames%2520contain%250Ainformation%2520beneficial%2520for%2520adaptation%252C%2520especially%2520in%2520the%2520presence%2520of%2520redundant%250Adata%2520and%2520class%2520imbalance%2520issues.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%250Aenhance%2520O-SFDA%2520for%2520adaptive%2520object%2520detection%2520through%2520unsupervised%2520data%250Aacquisition.%2520Our%2520methodology%2520prioritizes%2520the%2520most%2520informative%2520unlabeled%2520frames%250Afor%2520inclusion%2520in%2520the%2520online%2520training%2520process.%2520Empirical%2520evaluation%2520on%2520a%250Areal-world%2520dataset%2520reveals%2520that%2520our%2520method%2520outperforms%2520existing%250Astate-of-the-art%2520O-SFDA%2520techniques%252C%2520demonstrating%2520the%2520viability%2520of%2520unsupervised%250Adata%2520acquisition%2520for%2520improving%2520the%2520adaptive%2520object%2520detector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19258v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Online%20Source-free%20Domain%20Adaptation%20for%20Object%20Detection%20by%0A%20%20Unsupervised%20Data%20Acquisition&entry.906535625=Xiangyu%20Shi%20and%20Yanyuan%20Qiao%20and%20Qi%20Wu%20and%20Lingqiao%20Liu%20and%20Feras%20Dayoub&entry.1292438233=%20%20Effective%20object%20detection%20in%20autonomous%20vehicles%20is%20challenged%20by%20deployment%0Ain%20diverse%20and%20unfamiliar%20environments.%20Online%20Source-Free%20Domain%20Adaptation%0A%28O-SFDA%29%20offers%20model%20adaptation%20using%20a%20stream%20of%20unlabeled%20data%20from%20a%20target%0Adomain%20in%20an%20online%20manner.%20However%2C%20not%20all%20captured%20frames%20contain%0Ainformation%20beneficial%20for%20adaptation%2C%20especially%20in%20the%20presence%20of%20redundant%0Adata%20and%20class%20imbalance%20issues.%20This%20paper%20introduces%20a%20novel%20approach%20to%0Aenhance%20O-SFDA%20for%20adaptive%20object%20detection%20through%20unsupervised%20data%0Aacquisition.%20Our%20methodology%20prioritizes%20the%20most%20informative%20unlabeled%20frames%0Afor%20inclusion%20in%20the%20online%20training%20process.%20Empirical%20evaluation%20on%20a%0Areal-world%20dataset%20reveals%20that%20our%20method%20outperforms%20existing%0Astate-of-the-art%20O-SFDA%20techniques%2C%20demonstrating%20the%20viability%20of%20unsupervised%0Adata%20acquisition%20for%20improving%20the%20adaptive%20object%20detector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19258v3&entry.124074799=Read"},
{"title": "OpticalRS-4M: Scaling Efficient Masked Autoencoder Learning on Large\n  Remote Sensing Dataset", "author": "Fengxiang Wang and Hongzhen Wang and Di Wang and Zonghao Guo and Zhenyu Zhong and Long Lan and Jing Zhang and Zhiyuan Liu and Maosong Sun", "abstract": "  Masked Image Modeling (MIM) has become an essential method for building\nfoundational visual models in remote sensing (RS). However, the limitations in\nsize and diversity of existing RS datasets restrict the ability of MIM methods\nto learn generalizable representations. Additionally, conventional MIM\ntechniques, which require reconstructing all tokens, introduce unnecessary\ncomputational overhead. To address these issues, we present a new pre-training\npipeline for RS models, featuring the creation of a large-scale RS dataset and\nan efficient MIM approach. We curated a high-quality dataset named OpticalRS-4M\nby collecting publicly available RS datasets and processing them through\nexclusion, slicing, and deduplication. OpticalRS-4M comprises 4 million optical\nimages covering various RS tasks, such as object detection and pixel\nsegmentation. To enhance efficiency, we propose SelectiveMAE, a pre-training\nmethod that dynamically encodes and reconstructs semantically rich patch\ntokens, thereby reducing the inefficiencies of traditional MIM models caused by\nredundant background pixels in RS images. Extensive experiments demonstrate\nthat OpticalRS-4M significantly improves classification, detection, and\nsegmentation performance, while SelectiveMAE increases training efficiency over\n2 times. This highlights the effectiveness and scalability of our pipeline in\ndeveloping RS foundational models.\n", "link": "http://arxiv.org/abs/2406.11933v3", "date": "2024-08-30", "relevancy": 2.1498, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5387}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5381}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpticalRS-4M%3A%20Scaling%20Efficient%20Masked%20Autoencoder%20Learning%20on%20Large%0A%20%20Remote%20Sensing%20Dataset&body=Title%3A%20OpticalRS-4M%3A%20Scaling%20Efficient%20Masked%20Autoencoder%20Learning%20on%20Large%0A%20%20Remote%20Sensing%20Dataset%0AAuthor%3A%20Fengxiang%20Wang%20and%20Hongzhen%20Wang%20and%20Di%20Wang%20and%20Zonghao%20Guo%20and%20Zhenyu%20Zhong%20and%20Long%20Lan%20and%20Jing%20Zhang%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Masked%20Image%20Modeling%20%28MIM%29%20has%20become%20an%20essential%20method%20for%20building%0Afoundational%20visual%20models%20in%20remote%20sensing%20%28RS%29.%20However%2C%20the%20limitations%20in%0Asize%20and%20diversity%20of%20existing%20RS%20datasets%20restrict%20the%20ability%20of%20MIM%20methods%0Ato%20learn%20generalizable%20representations.%20Additionally%2C%20conventional%20MIM%0Atechniques%2C%20which%20require%20reconstructing%20all%20tokens%2C%20introduce%20unnecessary%0Acomputational%20overhead.%20To%20address%20these%20issues%2C%20we%20present%20a%20new%20pre-training%0Apipeline%20for%20RS%20models%2C%20featuring%20the%20creation%20of%20a%20large-scale%20RS%20dataset%20and%0Aan%20efficient%20MIM%20approach.%20We%20curated%20a%20high-quality%20dataset%20named%20OpticalRS-4M%0Aby%20collecting%20publicly%20available%20RS%20datasets%20and%20processing%20them%20through%0Aexclusion%2C%20slicing%2C%20and%20deduplication.%20OpticalRS-4M%20comprises%204%20million%20optical%0Aimages%20covering%20various%20RS%20tasks%2C%20such%20as%20object%20detection%20and%20pixel%0Asegmentation.%20To%20enhance%20efficiency%2C%20we%20propose%20SelectiveMAE%2C%20a%20pre-training%0Amethod%20that%20dynamically%20encodes%20and%20reconstructs%20semantically%20rich%20patch%0Atokens%2C%20thereby%20reducing%20the%20inefficiencies%20of%20traditional%20MIM%20models%20caused%20by%0Aredundant%20background%20pixels%20in%20RS%20images.%20Extensive%20experiments%20demonstrate%0Athat%20OpticalRS-4M%20significantly%20improves%20classification%2C%20detection%2C%20and%0Asegmentation%20performance%2C%20while%20SelectiveMAE%20increases%20training%20efficiency%20over%0A2%20times.%20This%20highlights%20the%20effectiveness%20and%20scalability%20of%20our%20pipeline%20in%0Adeveloping%20RS%20foundational%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11933v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpticalRS-4M%253A%2520Scaling%2520Efficient%2520Masked%2520Autoencoder%2520Learning%2520on%2520Large%250A%2520%2520Remote%2520Sensing%2520Dataset%26entry.906535625%3DFengxiang%2520Wang%2520and%2520Hongzhen%2520Wang%2520and%2520Di%2520Wang%2520and%2520Zonghao%2520Guo%2520and%2520Zhenyu%2520Zhong%2520and%2520Long%2520Lan%2520and%2520Jing%2520Zhang%2520and%2520Zhiyuan%2520Liu%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Masked%2520Image%2520Modeling%2520%2528MIM%2529%2520has%2520become%2520an%2520essential%2520method%2520for%2520building%250Afoundational%2520visual%2520models%2520in%2520remote%2520sensing%2520%2528RS%2529.%2520However%252C%2520the%2520limitations%2520in%250Asize%2520and%2520diversity%2520of%2520existing%2520RS%2520datasets%2520restrict%2520the%2520ability%2520of%2520MIM%2520methods%250Ato%2520learn%2520generalizable%2520representations.%2520Additionally%252C%2520conventional%2520MIM%250Atechniques%252C%2520which%2520require%2520reconstructing%2520all%2520tokens%252C%2520introduce%2520unnecessary%250Acomputational%2520overhead.%2520To%2520address%2520these%2520issues%252C%2520we%2520present%2520a%2520new%2520pre-training%250Apipeline%2520for%2520RS%2520models%252C%2520featuring%2520the%2520creation%2520of%2520a%2520large-scale%2520RS%2520dataset%2520and%250Aan%2520efficient%2520MIM%2520approach.%2520We%2520curated%2520a%2520high-quality%2520dataset%2520named%2520OpticalRS-4M%250Aby%2520collecting%2520publicly%2520available%2520RS%2520datasets%2520and%2520processing%2520them%2520through%250Aexclusion%252C%2520slicing%252C%2520and%2520deduplication.%2520OpticalRS-4M%2520comprises%25204%2520million%2520optical%250Aimages%2520covering%2520various%2520RS%2520tasks%252C%2520such%2520as%2520object%2520detection%2520and%2520pixel%250Asegmentation.%2520To%2520enhance%2520efficiency%252C%2520we%2520propose%2520SelectiveMAE%252C%2520a%2520pre-training%250Amethod%2520that%2520dynamically%2520encodes%2520and%2520reconstructs%2520semantically%2520rich%2520patch%250Atokens%252C%2520thereby%2520reducing%2520the%2520inefficiencies%2520of%2520traditional%2520MIM%2520models%2520caused%2520by%250Aredundant%2520background%2520pixels%2520in%2520RS%2520images.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520OpticalRS-4M%2520significantly%2520improves%2520classification%252C%2520detection%252C%2520and%250Asegmentation%2520performance%252C%2520while%2520SelectiveMAE%2520increases%2520training%2520efficiency%2520over%250A2%2520times.%2520This%2520highlights%2520the%2520effectiveness%2520and%2520scalability%2520of%2520our%2520pipeline%2520in%250Adeveloping%2520RS%2520foundational%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11933v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpticalRS-4M%3A%20Scaling%20Efficient%20Masked%20Autoencoder%20Learning%20on%20Large%0A%20%20Remote%20Sensing%20Dataset&entry.906535625=Fengxiang%20Wang%20and%20Hongzhen%20Wang%20and%20Di%20Wang%20and%20Zonghao%20Guo%20and%20Zhenyu%20Zhong%20and%20Long%20Lan%20and%20Jing%20Zhang%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun&entry.1292438233=%20%20Masked%20Image%20Modeling%20%28MIM%29%20has%20become%20an%20essential%20method%20for%20building%0Afoundational%20visual%20models%20in%20remote%20sensing%20%28RS%29.%20However%2C%20the%20limitations%20in%0Asize%20and%20diversity%20of%20existing%20RS%20datasets%20restrict%20the%20ability%20of%20MIM%20methods%0Ato%20learn%20generalizable%20representations.%20Additionally%2C%20conventional%20MIM%0Atechniques%2C%20which%20require%20reconstructing%20all%20tokens%2C%20introduce%20unnecessary%0Acomputational%20overhead.%20To%20address%20these%20issues%2C%20we%20present%20a%20new%20pre-training%0Apipeline%20for%20RS%20models%2C%20featuring%20the%20creation%20of%20a%20large-scale%20RS%20dataset%20and%0Aan%20efficient%20MIM%20approach.%20We%20curated%20a%20high-quality%20dataset%20named%20OpticalRS-4M%0Aby%20collecting%20publicly%20available%20RS%20datasets%20and%20processing%20them%20through%0Aexclusion%2C%20slicing%2C%20and%20deduplication.%20OpticalRS-4M%20comprises%204%20million%20optical%0Aimages%20covering%20various%20RS%20tasks%2C%20such%20as%20object%20detection%20and%20pixel%0Asegmentation.%20To%20enhance%20efficiency%2C%20we%20propose%20SelectiveMAE%2C%20a%20pre-training%0Amethod%20that%20dynamically%20encodes%20and%20reconstructs%20semantically%20rich%20patch%0Atokens%2C%20thereby%20reducing%20the%20inefficiencies%20of%20traditional%20MIM%20models%20caused%20by%0Aredundant%20background%20pixels%20in%20RS%20images.%20Extensive%20experiments%20demonstrate%0Athat%20OpticalRS-4M%20significantly%20improves%20classification%2C%20detection%2C%20and%0Asegmentation%20performance%2C%20while%20SelectiveMAE%20increases%20training%20efficiency%20over%0A2%20times.%20This%20highlights%20the%20effectiveness%20and%20scalability%20of%20our%20pipeline%20in%0Adeveloping%20RS%20foundational%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11933v3&entry.124074799=Read"},
{"title": "Docling Technical Report", "author": "Christoph Auer and Maksym Lysak and Ahmed Nassar and Michele Dolfi and Nikolaos Livathinos and Panos Vagenas and Cesar Berrospi Ramis and Matteo Omenetti and Fabian Lindlbauer and Kasper Dinkla and Lokesh Mishra and Yusik Kim and Shubham Gupta and Rafael Teixeira de Lima and Valery Weber and Lucas Morin and Ingmar Meijer and Viktor Kuropiatnyk and Peter W. J. Staar", "abstract": "  This technical report introduces Docling, an easy to use, self-contained,\nMIT-licensed open-source package for PDF document conversion. It is powered by\nstate-of-the-art specialized AI models for layout analysis (DocLayNet) and\ntable structure recognition (TableFormer), and runs efficiently on commodity\nhardware in a small resource budget. The code interface allows for easy\nextensibility and addition of new features and models.\n", "link": "http://arxiv.org/abs/2408.09869v3", "date": "2024-08-30", "relevancy": 2.1317, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4557}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4117}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Docling%20Technical%20Report&body=Title%3A%20Docling%20Technical%20Report%0AAuthor%3A%20Christoph%20Auer%20and%20Maksym%20Lysak%20and%20Ahmed%20Nassar%20and%20Michele%20Dolfi%20and%20Nikolaos%20Livathinos%20and%20Panos%20Vagenas%20and%20Cesar%20Berrospi%20Ramis%20and%20Matteo%20Omenetti%20and%20Fabian%20Lindlbauer%20and%20Kasper%20Dinkla%20and%20Lokesh%20Mishra%20and%20Yusik%20Kim%20and%20Shubham%20Gupta%20and%20Rafael%20Teixeira%20de%20Lima%20and%20Valery%20Weber%20and%20Lucas%20Morin%20and%20Ingmar%20Meijer%20and%20Viktor%20Kuropiatnyk%20and%20Peter%20W.%20J.%20Staar%0AAbstract%3A%20%20%20This%20technical%20report%20introduces%20Docling%2C%20an%20easy%20to%20use%2C%20self-contained%2C%0AMIT-licensed%20open-source%20package%20for%20PDF%20document%20conversion.%20It%20is%20powered%20by%0Astate-of-the-art%20specialized%20AI%20models%20for%20layout%20analysis%20%28DocLayNet%29%20and%0Atable%20structure%20recognition%20%28TableFormer%29%2C%20and%20runs%20efficiently%20on%20commodity%0Ahardware%20in%20a%20small%20resource%20budget.%20The%20code%20interface%20allows%20for%20easy%0Aextensibility%20and%20addition%20of%20new%20features%20and%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09869v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocling%2520Technical%2520Report%26entry.906535625%3DChristoph%2520Auer%2520and%2520Maksym%2520Lysak%2520and%2520Ahmed%2520Nassar%2520and%2520Michele%2520Dolfi%2520and%2520Nikolaos%2520Livathinos%2520and%2520Panos%2520Vagenas%2520and%2520Cesar%2520Berrospi%2520Ramis%2520and%2520Matteo%2520Omenetti%2520and%2520Fabian%2520Lindlbauer%2520and%2520Kasper%2520Dinkla%2520and%2520Lokesh%2520Mishra%2520and%2520Yusik%2520Kim%2520and%2520Shubham%2520Gupta%2520and%2520Rafael%2520Teixeira%2520de%2520Lima%2520and%2520Valery%2520Weber%2520and%2520Lucas%2520Morin%2520and%2520Ingmar%2520Meijer%2520and%2520Viktor%2520Kuropiatnyk%2520and%2520Peter%2520W.%2520J.%2520Staar%26entry.1292438233%3D%2520%2520This%2520technical%2520report%2520introduces%2520Docling%252C%2520an%2520easy%2520to%2520use%252C%2520self-contained%252C%250AMIT-licensed%2520open-source%2520package%2520for%2520PDF%2520document%2520conversion.%2520It%2520is%2520powered%2520by%250Astate-of-the-art%2520specialized%2520AI%2520models%2520for%2520layout%2520analysis%2520%2528DocLayNet%2529%2520and%250Atable%2520structure%2520recognition%2520%2528TableFormer%2529%252C%2520and%2520runs%2520efficiently%2520on%2520commodity%250Ahardware%2520in%2520a%2520small%2520resource%2520budget.%2520The%2520code%2520interface%2520allows%2520for%2520easy%250Aextensibility%2520and%2520addition%2520of%2520new%2520features%2520and%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09869v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Docling%20Technical%20Report&entry.906535625=Christoph%20Auer%20and%20Maksym%20Lysak%20and%20Ahmed%20Nassar%20and%20Michele%20Dolfi%20and%20Nikolaos%20Livathinos%20and%20Panos%20Vagenas%20and%20Cesar%20Berrospi%20Ramis%20and%20Matteo%20Omenetti%20and%20Fabian%20Lindlbauer%20and%20Kasper%20Dinkla%20and%20Lokesh%20Mishra%20and%20Yusik%20Kim%20and%20Shubham%20Gupta%20and%20Rafael%20Teixeira%20de%20Lima%20and%20Valery%20Weber%20and%20Lucas%20Morin%20and%20Ingmar%20Meijer%20and%20Viktor%20Kuropiatnyk%20and%20Peter%20W.%20J.%20Staar&entry.1292438233=%20%20This%20technical%20report%20introduces%20Docling%2C%20an%20easy%20to%20use%2C%20self-contained%2C%0AMIT-licensed%20open-source%20package%20for%20PDF%20document%20conversion.%20It%20is%20powered%20by%0Astate-of-the-art%20specialized%20AI%20models%20for%20layout%20analysis%20%28DocLayNet%29%20and%0Atable%20structure%20recognition%20%28TableFormer%29%2C%20and%20runs%20efficiently%20on%20commodity%0Ahardware%20in%20a%20small%20resource%20budget.%20The%20code%20interface%20allows%20for%20easy%0Aextensibility%20and%20addition%20of%20new%20features%20and%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09869v3&entry.124074799=Read"},
{"title": "NanoMVG: USV-Centric Low-Power Multi-Task Visual Grounding based on\n  Prompt-Guided Camera and 4D mmWave Radar", "author": "Runwei Guan and Jianan Liu and Liye Jia and Haocheng Zhao and Shanliang Yao and Xiaohui Zhu and Ka Lok Man and Eng Gee Lim and Jeremy Smith and Yutao Yue", "abstract": "  Recently, visual grounding and multi-sensors setting have been incorporated\ninto perception system for terrestrial autonomous driving systems and Unmanned\nSurface Vehicles (USVs), yet the high complexity of modern learning-based\nvisual grounding model using multi-sensors prevents such model to be deployed\non USVs in the real-life. To this end, we design a low-power multi-task model\nnamed NanoMVG for waterway embodied perception, guiding both camera and 4D\nmillimeter-wave radar to locate specific object(s) through natural language.\nNanoMVG can perform both box-level and mask-level visual grounding tasks\nsimultaneously. Compared to other visual grounding models, NanoMVG achieves\nhighly competitive performance on the WaterVG dataset, particularly in harsh\nenvironments and boasts ultra-low power consumption for long endurance.\n", "link": "http://arxiv.org/abs/2408.17207v1", "date": "2024-08-30", "relevancy": 2.1276, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5522}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5352}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NanoMVG%3A%20USV-Centric%20Low-Power%20Multi-Task%20Visual%20Grounding%20based%20on%0A%20%20Prompt-Guided%20Camera%20and%204D%20mmWave%20Radar&body=Title%3A%20NanoMVG%3A%20USV-Centric%20Low-Power%20Multi-Task%20Visual%20Grounding%20based%20on%0A%20%20Prompt-Guided%20Camera%20and%204D%20mmWave%20Radar%0AAuthor%3A%20Runwei%20Guan%20and%20Jianan%20Liu%20and%20Liye%20Jia%20and%20Haocheng%20Zhao%20and%20Shanliang%20Yao%20and%20Xiaohui%20Zhu%20and%20Ka%20Lok%20Man%20and%20Eng%20Gee%20Lim%20and%20Jeremy%20Smith%20and%20Yutao%20Yue%0AAbstract%3A%20%20%20Recently%2C%20visual%20grounding%20and%20multi-sensors%20setting%20have%20been%20incorporated%0Ainto%20perception%20system%20for%20terrestrial%20autonomous%20driving%20systems%20and%20Unmanned%0ASurface%20Vehicles%20%28USVs%29%2C%20yet%20the%20high%20complexity%20of%20modern%20learning-based%0Avisual%20grounding%20model%20using%20multi-sensors%20prevents%20such%20model%20to%20be%20deployed%0Aon%20USVs%20in%20the%20real-life.%20To%20this%20end%2C%20we%20design%20a%20low-power%20multi-task%20model%0Anamed%20NanoMVG%20for%20waterway%20embodied%20perception%2C%20guiding%20both%20camera%20and%204D%0Amillimeter-wave%20radar%20to%20locate%20specific%20object%28s%29%20through%20natural%20language.%0ANanoMVG%20can%20perform%20both%20box-level%20and%20mask-level%20visual%20grounding%20tasks%0Asimultaneously.%20Compared%20to%20other%20visual%20grounding%20models%2C%20NanoMVG%20achieves%0Ahighly%20competitive%20performance%20on%20the%20WaterVG%20dataset%2C%20particularly%20in%20harsh%0Aenvironments%20and%20boasts%20ultra-low%20power%20consumption%20for%20long%20endurance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNanoMVG%253A%2520USV-Centric%2520Low-Power%2520Multi-Task%2520Visual%2520Grounding%2520based%2520on%250A%2520%2520Prompt-Guided%2520Camera%2520and%25204D%2520mmWave%2520Radar%26entry.906535625%3DRunwei%2520Guan%2520and%2520Jianan%2520Liu%2520and%2520Liye%2520Jia%2520and%2520Haocheng%2520Zhao%2520and%2520Shanliang%2520Yao%2520and%2520Xiaohui%2520Zhu%2520and%2520Ka%2520Lok%2520Man%2520and%2520Eng%2520Gee%2520Lim%2520and%2520Jeremy%2520Smith%2520and%2520Yutao%2520Yue%26entry.1292438233%3D%2520%2520Recently%252C%2520visual%2520grounding%2520and%2520multi-sensors%2520setting%2520have%2520been%2520incorporated%250Ainto%2520perception%2520system%2520for%2520terrestrial%2520autonomous%2520driving%2520systems%2520and%2520Unmanned%250ASurface%2520Vehicles%2520%2528USVs%2529%252C%2520yet%2520the%2520high%2520complexity%2520of%2520modern%2520learning-based%250Avisual%2520grounding%2520model%2520using%2520multi-sensors%2520prevents%2520such%2520model%2520to%2520be%2520deployed%250Aon%2520USVs%2520in%2520the%2520real-life.%2520To%2520this%2520end%252C%2520we%2520design%2520a%2520low-power%2520multi-task%2520model%250Anamed%2520NanoMVG%2520for%2520waterway%2520embodied%2520perception%252C%2520guiding%2520both%2520camera%2520and%25204D%250Amillimeter-wave%2520radar%2520to%2520locate%2520specific%2520object%2528s%2529%2520through%2520natural%2520language.%250ANanoMVG%2520can%2520perform%2520both%2520box-level%2520and%2520mask-level%2520visual%2520grounding%2520tasks%250Asimultaneously.%2520Compared%2520to%2520other%2520visual%2520grounding%2520models%252C%2520NanoMVG%2520achieves%250Ahighly%2520competitive%2520performance%2520on%2520the%2520WaterVG%2520dataset%252C%2520particularly%2520in%2520harsh%250Aenvironments%2520and%2520boasts%2520ultra-low%2520power%2520consumption%2520for%2520long%2520endurance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NanoMVG%3A%20USV-Centric%20Low-Power%20Multi-Task%20Visual%20Grounding%20based%20on%0A%20%20Prompt-Guided%20Camera%20and%204D%20mmWave%20Radar&entry.906535625=Runwei%20Guan%20and%20Jianan%20Liu%20and%20Liye%20Jia%20and%20Haocheng%20Zhao%20and%20Shanliang%20Yao%20and%20Xiaohui%20Zhu%20and%20Ka%20Lok%20Man%20and%20Eng%20Gee%20Lim%20and%20Jeremy%20Smith%20and%20Yutao%20Yue&entry.1292438233=%20%20Recently%2C%20visual%20grounding%20and%20multi-sensors%20setting%20have%20been%20incorporated%0Ainto%20perception%20system%20for%20terrestrial%20autonomous%20driving%20systems%20and%20Unmanned%0ASurface%20Vehicles%20%28USVs%29%2C%20yet%20the%20high%20complexity%20of%20modern%20learning-based%0Avisual%20grounding%20model%20using%20multi-sensors%20prevents%20such%20model%20to%20be%20deployed%0Aon%20USVs%20in%20the%20real-life.%20To%20this%20end%2C%20we%20design%20a%20low-power%20multi-task%20model%0Anamed%20NanoMVG%20for%20waterway%20embodied%20perception%2C%20guiding%20both%20camera%20and%204D%0Amillimeter-wave%20radar%20to%20locate%20specific%20object%28s%29%20through%20natural%20language.%0ANanoMVG%20can%20perform%20both%20box-level%20and%20mask-level%20visual%20grounding%20tasks%0Asimultaneously.%20Compared%20to%20other%20visual%20grounding%20models%2C%20NanoMVG%20achieves%0Ahighly%20competitive%20performance%20on%20the%20WaterVG%20dataset%2C%20particularly%20in%20harsh%0Aenvironments%20and%20boasts%20ultra-low%20power%20consumption%20for%20long%20endurance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17207v1&entry.124074799=Read"},
{"title": "Enhancing Underwater Imaging with 4-D Light Fields: Dataset and Method", "author": "Yuji Lin and Xianqiang Lyu and Junhui Hou and Qian Zhao and Deyu Meng", "abstract": "  In this paper, we delve into the realm of 4-D light fields (LFs) to enhance\nunderwater imaging plagued by light absorption, scattering, and other\nchallenges. Contrasting with conventional 2-D RGB imaging, 4-D LF imaging\nexcels in capturing scenes from multiple perspectives, thereby indirectly\nembedding geometric information. This intrinsic property is anticipated to\neffectively address the challenges associated with underwater imaging. By\nleveraging both explicit and implicit depth cues present in 4-D LF images, we\npropose a progressive, mutually reinforcing framework for underwater 4-D LF\nimage enhancement and depth estimation. Specifically, our framework explicitly\nutilizes estimated depth information alongside implicit depth-related dynamic\nconvolutional kernels to modulate output features. The entire framework\ndecomposes this complex task, iteratively optimizing the enhanced image and\ndepth information to progressively achieve optimal enhancement results. More\nimportantly, we construct the first 4-D LF-based underwater image dataset for\nquantitative evaluation and supervised training of learning-based methods,\ncomprising 75 underwater scenes and 3675 high-resolution 2K pairs. To craft\nvibrant and varied underwater scenes, we build underwater environments with\nvarious objects and adopt several types of degradation. Through extensive\nexperimentation, we showcase the potential and superiority of 4-D LF-based\nunderwater imaging vis-a-vis traditional 2-D RGB-based approaches. Moreover,\nour method effectively corrects color bias and achieves state-of-the-art\nperformance. The dataset and code will be publicly available at\nhttps://github.com/linlos1234/LFUIE.\n", "link": "http://arxiv.org/abs/2408.17339v1", "date": "2024-08-30", "relevancy": 2.1107, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5475}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5289}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Underwater%20Imaging%20with%204-D%20Light%20Fields%3A%20Dataset%20and%20Method&body=Title%3A%20Enhancing%20Underwater%20Imaging%20with%204-D%20Light%20Fields%3A%20Dataset%20and%20Method%0AAuthor%3A%20Yuji%20Lin%20and%20Xianqiang%20Lyu%20and%20Junhui%20Hou%20and%20Qian%20Zhao%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20delve%20into%20the%20realm%20of%204-D%20light%20fields%20%28LFs%29%20to%20enhance%0Aunderwater%20imaging%20plagued%20by%20light%20absorption%2C%20scattering%2C%20and%20other%0Achallenges.%20Contrasting%20with%20conventional%202-D%20RGB%20imaging%2C%204-D%20LF%20imaging%0Aexcels%20in%20capturing%20scenes%20from%20multiple%20perspectives%2C%20thereby%20indirectly%0Aembedding%20geometric%20information.%20This%20intrinsic%20property%20is%20anticipated%20to%0Aeffectively%20address%20the%20challenges%20associated%20with%20underwater%20imaging.%20By%0Aleveraging%20both%20explicit%20and%20implicit%20depth%20cues%20present%20in%204-D%20LF%20images%2C%20we%0Apropose%20a%20progressive%2C%20mutually%20reinforcing%20framework%20for%20underwater%204-D%20LF%0Aimage%20enhancement%20and%20depth%20estimation.%20Specifically%2C%20our%20framework%20explicitly%0Autilizes%20estimated%20depth%20information%20alongside%20implicit%20depth-related%20dynamic%0Aconvolutional%20kernels%20to%20modulate%20output%20features.%20The%20entire%20framework%0Adecomposes%20this%20complex%20task%2C%20iteratively%20optimizing%20the%20enhanced%20image%20and%0Adepth%20information%20to%20progressively%20achieve%20optimal%20enhancement%20results.%20More%0Aimportantly%2C%20we%20construct%20the%20first%204-D%20LF-based%20underwater%20image%20dataset%20for%0Aquantitative%20evaluation%20and%20supervised%20training%20of%20learning-based%20methods%2C%0Acomprising%2075%20underwater%20scenes%20and%203675%20high-resolution%202K%20pairs.%20To%20craft%0Avibrant%20and%20varied%20underwater%20scenes%2C%20we%20build%20underwater%20environments%20with%0Avarious%20objects%20and%20adopt%20several%20types%20of%20degradation.%20Through%20extensive%0Aexperimentation%2C%20we%20showcase%20the%20potential%20and%20superiority%20of%204-D%20LF-based%0Aunderwater%20imaging%20vis-a-vis%20traditional%202-D%20RGB-based%20approaches.%20Moreover%2C%0Aour%20method%20effectively%20corrects%20color%20bias%20and%20achieves%20state-of-the-art%0Aperformance.%20The%20dataset%20and%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/linlos1234/LFUIE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Underwater%2520Imaging%2520with%25204-D%2520Light%2520Fields%253A%2520Dataset%2520and%2520Method%26entry.906535625%3DYuji%2520Lin%2520and%2520Xianqiang%2520Lyu%2520and%2520Junhui%2520Hou%2520and%2520Qian%2520Zhao%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520delve%2520into%2520the%2520realm%2520of%25204-D%2520light%2520fields%2520%2528LFs%2529%2520to%2520enhance%250Aunderwater%2520imaging%2520plagued%2520by%2520light%2520absorption%252C%2520scattering%252C%2520and%2520other%250Achallenges.%2520Contrasting%2520with%2520conventional%25202-D%2520RGB%2520imaging%252C%25204-D%2520LF%2520imaging%250Aexcels%2520in%2520capturing%2520scenes%2520from%2520multiple%2520perspectives%252C%2520thereby%2520indirectly%250Aembedding%2520geometric%2520information.%2520This%2520intrinsic%2520property%2520is%2520anticipated%2520to%250Aeffectively%2520address%2520the%2520challenges%2520associated%2520with%2520underwater%2520imaging.%2520By%250Aleveraging%2520both%2520explicit%2520and%2520implicit%2520depth%2520cues%2520present%2520in%25204-D%2520LF%2520images%252C%2520we%250Apropose%2520a%2520progressive%252C%2520mutually%2520reinforcing%2520framework%2520for%2520underwater%25204-D%2520LF%250Aimage%2520enhancement%2520and%2520depth%2520estimation.%2520Specifically%252C%2520our%2520framework%2520explicitly%250Autilizes%2520estimated%2520depth%2520information%2520alongside%2520implicit%2520depth-related%2520dynamic%250Aconvolutional%2520kernels%2520to%2520modulate%2520output%2520features.%2520The%2520entire%2520framework%250Adecomposes%2520this%2520complex%2520task%252C%2520iteratively%2520optimizing%2520the%2520enhanced%2520image%2520and%250Adepth%2520information%2520to%2520progressively%2520achieve%2520optimal%2520enhancement%2520results.%2520More%250Aimportantly%252C%2520we%2520construct%2520the%2520first%25204-D%2520LF-based%2520underwater%2520image%2520dataset%2520for%250Aquantitative%2520evaluation%2520and%2520supervised%2520training%2520of%2520learning-based%2520methods%252C%250Acomprising%252075%2520underwater%2520scenes%2520and%25203675%2520high-resolution%25202K%2520pairs.%2520To%2520craft%250Avibrant%2520and%2520varied%2520underwater%2520scenes%252C%2520we%2520build%2520underwater%2520environments%2520with%250Avarious%2520objects%2520and%2520adopt%2520several%2520types%2520of%2520degradation.%2520Through%2520extensive%250Aexperimentation%252C%2520we%2520showcase%2520the%2520potential%2520and%2520superiority%2520of%25204-D%2520LF-based%250Aunderwater%2520imaging%2520vis-a-vis%2520traditional%25202-D%2520RGB-based%2520approaches.%2520Moreover%252C%250Aour%2520method%2520effectively%2520corrects%2520color%2520bias%2520and%2520achieves%2520state-of-the-art%250Aperformance.%2520The%2520dataset%2520and%2520code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/linlos1234/LFUIE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Underwater%20Imaging%20with%204-D%20Light%20Fields%3A%20Dataset%20and%20Method&entry.906535625=Yuji%20Lin%20and%20Xianqiang%20Lyu%20and%20Junhui%20Hou%20and%20Qian%20Zhao%20and%20Deyu%20Meng&entry.1292438233=%20%20In%20this%20paper%2C%20we%20delve%20into%20the%20realm%20of%204-D%20light%20fields%20%28LFs%29%20to%20enhance%0Aunderwater%20imaging%20plagued%20by%20light%20absorption%2C%20scattering%2C%20and%20other%0Achallenges.%20Contrasting%20with%20conventional%202-D%20RGB%20imaging%2C%204-D%20LF%20imaging%0Aexcels%20in%20capturing%20scenes%20from%20multiple%20perspectives%2C%20thereby%20indirectly%0Aembedding%20geometric%20information.%20This%20intrinsic%20property%20is%20anticipated%20to%0Aeffectively%20address%20the%20challenges%20associated%20with%20underwater%20imaging.%20By%0Aleveraging%20both%20explicit%20and%20implicit%20depth%20cues%20present%20in%204-D%20LF%20images%2C%20we%0Apropose%20a%20progressive%2C%20mutually%20reinforcing%20framework%20for%20underwater%204-D%20LF%0Aimage%20enhancement%20and%20depth%20estimation.%20Specifically%2C%20our%20framework%20explicitly%0Autilizes%20estimated%20depth%20information%20alongside%20implicit%20depth-related%20dynamic%0Aconvolutional%20kernels%20to%20modulate%20output%20features.%20The%20entire%20framework%0Adecomposes%20this%20complex%20task%2C%20iteratively%20optimizing%20the%20enhanced%20image%20and%0Adepth%20information%20to%20progressively%20achieve%20optimal%20enhancement%20results.%20More%0Aimportantly%2C%20we%20construct%20the%20first%204-D%20LF-based%20underwater%20image%20dataset%20for%0Aquantitative%20evaluation%20and%20supervised%20training%20of%20learning-based%20methods%2C%0Acomprising%2075%20underwater%20scenes%20and%203675%20high-resolution%202K%20pairs.%20To%20craft%0Avibrant%20and%20varied%20underwater%20scenes%2C%20we%20build%20underwater%20environments%20with%0Avarious%20objects%20and%20adopt%20several%20types%20of%20degradation.%20Through%20extensive%0Aexperimentation%2C%20we%20showcase%20the%20potential%20and%20superiority%20of%204-D%20LF-based%0Aunderwater%20imaging%20vis-a-vis%20traditional%202-D%20RGB-based%20approaches.%20Moreover%2C%0Aour%20method%20effectively%20corrects%20color%20bias%20and%20achieves%20state-of-the-art%0Aperformance.%20The%20dataset%20and%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/linlos1234/LFUIE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17339v1&entry.124074799=Read"},
{"title": "VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time\n  Series Forecasters", "author": "Mouxiang Chen and Lefei Shen and Zhuo Li and Xiaoyun Joy Wang and Jianling Sun and Chenghao Liu", "abstract": "  Foundation models have emerged as a promising approach in time series\nforecasting (TSF). Existing approaches either fine-tune large language models\n(LLMs) or build large-scale time-series datasets to develop TSF foundation\nmodels. However, these methods face challenges due to the severe cross-domain\ngap or in-domain heterogeneity. In this paper, we explore a new road to\nbuilding a TSF foundation model from rich and high-quality natural images,\nbased on the intrinsic similarities between images and time series. To bridge\nthe gap between the two domains, we reformulate the TSF task as an image\nreconstruction task, which is further processed by a visual masked autoencoder\n(MAE) self-supervised pre-trained on the ImageNet dataset. Surprisingly,\nwithout further adaptation in the time-series domain, the proposed VisionTS\ncould achieve superior zero-shot forecasting performance compared to existing\nTSF foundation models. With minimal fine-tuning, VisionTS could further improve\nthe forecasting and achieve state-of-the-art performance in most cases. These\nfindings suggest that visual models could be a free lunch for TSF and highlight\nthe potential for future cross-domain research between computer vision and TSF.\nOur code is publicly available at https://github.com/Keytoyze/VisionTS.\n", "link": "http://arxiv.org/abs/2408.17253v1", "date": "2024-08-30", "relevancy": 2.1085, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5424}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5249}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisionTS%3A%20Visual%20Masked%20Autoencoders%20Are%20Free-Lunch%20Zero-Shot%20Time%0A%20%20Series%20Forecasters&body=Title%3A%20VisionTS%3A%20Visual%20Masked%20Autoencoders%20Are%20Free-Lunch%20Zero-Shot%20Time%0A%20%20Series%20Forecasters%0AAuthor%3A%20Mouxiang%20Chen%20and%20Lefei%20Shen%20and%20Zhuo%20Li%20and%20Xiaoyun%20Joy%20Wang%20and%20Jianling%20Sun%20and%20Chenghao%20Liu%0AAbstract%3A%20%20%20Foundation%20models%20have%20emerged%20as%20a%20promising%20approach%20in%20time%20series%0Aforecasting%20%28TSF%29.%20Existing%20approaches%20either%20fine-tune%20large%20language%20models%0A%28LLMs%29%20or%20build%20large-scale%20time-series%20datasets%20to%20develop%20TSF%20foundation%0Amodels.%20However%2C%20these%20methods%20face%20challenges%20due%20to%20the%20severe%20cross-domain%0Agap%20or%20in-domain%20heterogeneity.%20In%20this%20paper%2C%20we%20explore%20a%20new%20road%20to%0Abuilding%20a%20TSF%20foundation%20model%20from%20rich%20and%20high-quality%20natural%20images%2C%0Abased%20on%20the%20intrinsic%20similarities%20between%20images%20and%20time%20series.%20To%20bridge%0Athe%20gap%20between%20the%20two%20domains%2C%20we%20reformulate%20the%20TSF%20task%20as%20an%20image%0Areconstruction%20task%2C%20which%20is%20further%20processed%20by%20a%20visual%20masked%20autoencoder%0A%28MAE%29%20self-supervised%20pre-trained%20on%20the%20ImageNet%20dataset.%20Surprisingly%2C%0Awithout%20further%20adaptation%20in%20the%20time-series%20domain%2C%20the%20proposed%20VisionTS%0Acould%20achieve%20superior%20zero-shot%20forecasting%20performance%20compared%20to%20existing%0ATSF%20foundation%20models.%20With%20minimal%20fine-tuning%2C%20VisionTS%20could%20further%20improve%0Athe%20forecasting%20and%20achieve%20state-of-the-art%20performance%20in%20most%20cases.%20These%0Afindings%20suggest%20that%20visual%20models%20could%20be%20a%20free%20lunch%20for%20TSF%20and%20highlight%0Athe%20potential%20for%20future%20cross-domain%20research%20between%20computer%20vision%20and%20TSF.%0AOur%20code%20is%20publicly%20available%20at%20https%3A//github.com/Keytoyze/VisionTS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisionTS%253A%2520Visual%2520Masked%2520Autoencoders%2520Are%2520Free-Lunch%2520Zero-Shot%2520Time%250A%2520%2520Series%2520Forecasters%26entry.906535625%3DMouxiang%2520Chen%2520and%2520Lefei%2520Shen%2520and%2520Zhuo%2520Li%2520and%2520Xiaoyun%2520Joy%2520Wang%2520and%2520Jianling%2520Sun%2520and%2520Chenghao%2520Liu%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520in%2520time%2520series%250Aforecasting%2520%2528TSF%2529.%2520Existing%2520approaches%2520either%2520fine-tune%2520large%2520language%2520models%250A%2528LLMs%2529%2520or%2520build%2520large-scale%2520time-series%2520datasets%2520to%2520develop%2520TSF%2520foundation%250Amodels.%2520However%252C%2520these%2520methods%2520face%2520challenges%2520due%2520to%2520the%2520severe%2520cross-domain%250Agap%2520or%2520in-domain%2520heterogeneity.%2520In%2520this%2520paper%252C%2520we%2520explore%2520a%2520new%2520road%2520to%250Abuilding%2520a%2520TSF%2520foundation%2520model%2520from%2520rich%2520and%2520high-quality%2520natural%2520images%252C%250Abased%2520on%2520the%2520intrinsic%2520similarities%2520between%2520images%2520and%2520time%2520series.%2520To%2520bridge%250Athe%2520gap%2520between%2520the%2520two%2520domains%252C%2520we%2520reformulate%2520the%2520TSF%2520task%2520as%2520an%2520image%250Areconstruction%2520task%252C%2520which%2520is%2520further%2520processed%2520by%2520a%2520visual%2520masked%2520autoencoder%250A%2528MAE%2529%2520self-supervised%2520pre-trained%2520on%2520the%2520ImageNet%2520dataset.%2520Surprisingly%252C%250Awithout%2520further%2520adaptation%2520in%2520the%2520time-series%2520domain%252C%2520the%2520proposed%2520VisionTS%250Acould%2520achieve%2520superior%2520zero-shot%2520forecasting%2520performance%2520compared%2520to%2520existing%250ATSF%2520foundation%2520models.%2520With%2520minimal%2520fine-tuning%252C%2520VisionTS%2520could%2520further%2520improve%250Athe%2520forecasting%2520and%2520achieve%2520state-of-the-art%2520performance%2520in%2520most%2520cases.%2520These%250Afindings%2520suggest%2520that%2520visual%2520models%2520could%2520be%2520a%2520free%2520lunch%2520for%2520TSF%2520and%2520highlight%250Athe%2520potential%2520for%2520future%2520cross-domain%2520research%2520between%2520computer%2520vision%2520and%2520TSF.%250AOur%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/Keytoyze/VisionTS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisionTS%3A%20Visual%20Masked%20Autoencoders%20Are%20Free-Lunch%20Zero-Shot%20Time%0A%20%20Series%20Forecasters&entry.906535625=Mouxiang%20Chen%20and%20Lefei%20Shen%20and%20Zhuo%20Li%20and%20Xiaoyun%20Joy%20Wang%20and%20Jianling%20Sun%20and%20Chenghao%20Liu&entry.1292438233=%20%20Foundation%20models%20have%20emerged%20as%20a%20promising%20approach%20in%20time%20series%0Aforecasting%20%28TSF%29.%20Existing%20approaches%20either%20fine-tune%20large%20language%20models%0A%28LLMs%29%20or%20build%20large-scale%20time-series%20datasets%20to%20develop%20TSF%20foundation%0Amodels.%20However%2C%20these%20methods%20face%20challenges%20due%20to%20the%20severe%20cross-domain%0Agap%20or%20in-domain%20heterogeneity.%20In%20this%20paper%2C%20we%20explore%20a%20new%20road%20to%0Abuilding%20a%20TSF%20foundation%20model%20from%20rich%20and%20high-quality%20natural%20images%2C%0Abased%20on%20the%20intrinsic%20similarities%20between%20images%20and%20time%20series.%20To%20bridge%0Athe%20gap%20between%20the%20two%20domains%2C%20we%20reformulate%20the%20TSF%20task%20as%20an%20image%0Areconstruction%20task%2C%20which%20is%20further%20processed%20by%20a%20visual%20masked%20autoencoder%0A%28MAE%29%20self-supervised%20pre-trained%20on%20the%20ImageNet%20dataset.%20Surprisingly%2C%0Awithout%20further%20adaptation%20in%20the%20time-series%20domain%2C%20the%20proposed%20VisionTS%0Acould%20achieve%20superior%20zero-shot%20forecasting%20performance%20compared%20to%20existing%0ATSF%20foundation%20models.%20With%20minimal%20fine-tuning%2C%20VisionTS%20could%20further%20improve%0Athe%20forecasting%20and%20achieve%20state-of-the-art%20performance%20in%20most%20cases.%20These%0Afindings%20suggest%20that%20visual%20models%20could%20be%20a%20free%20lunch%20for%20TSF%20and%20highlight%0Athe%20potential%20for%20future%20cross-domain%20research%20between%20computer%20vision%20and%20TSF.%0AOur%20code%20is%20publicly%20available%20at%20https%3A//github.com/Keytoyze/VisionTS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17253v1&entry.124074799=Read"},
{"title": "L4DR: LiDAR-4DRadar Fusion for Weather-Robust 3D Object Detection", "author": "Xun Huang and Ziyu Xu and Hai Wu and Jinlong Wang and Qiming Xia and Yan Xia and Jonathan Li and Kyle Gao and Chenglu Wen and Cheng Wang", "abstract": "  LiDAR-based vision systems are integral for 3D object detection, which is\ncrucial for autonomous navigation. However, they suffer from performance\ndegradation in adverse weather conditions due to the quality deterioration of\nLiDAR point clouds. Fusing LiDAR with the weather-robust 4D radar sensor is\nexpected to solve this problem. However, the fusion of LiDAR and 4D radar is\nchallenging because they differ significantly in terms of data quality and the\ndegree of degradation in adverse weather. To address these issues, we introduce\nL4DR, a weather-robust 3D object detection method that effectively achieves\nLiDAR and 4D Radar fusion. Our L4DR includes Multi-Modal Encoding (MME) and\nForeground-Aware Denoising (FAD) technique to reconcile sensor gaps, which is\nthe first exploration of the complementarity of early fusion between LiDAR and\n4D radar. Additionally, we design an Inter-Modal and Intra-Modal ({IM}2 )\nparallel feature extraction backbone coupled with a Multi-Scale Gated Fusion\n(MSGF) module to counteract the varying degrees of sensor degradation under\nadverse weather conditions. Experimental evaluation on a VoD dataset with\nsimulated fog proves that L4DR is more adaptable to changing weather\nconditions. It delivers a significant performance increase under different fog\nlevels, improving the 3D mAP by up to 20.0% over the traditional LiDAR-only\napproach. Moreover, the results on the K-Radar dataset validate the consistent\nperformance improvement of L4DR in real-world adverse weather conditions.\n", "link": "http://arxiv.org/abs/2408.03677v3", "date": "2024-08-30", "relevancy": 2.0988, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.527}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.526}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L4DR%3A%20LiDAR-4DRadar%20Fusion%20for%20Weather-Robust%203D%20Object%20Detection&body=Title%3A%20L4DR%3A%20LiDAR-4DRadar%20Fusion%20for%20Weather-Robust%203D%20Object%20Detection%0AAuthor%3A%20Xun%20Huang%20and%20Ziyu%20Xu%20and%20Hai%20Wu%20and%20Jinlong%20Wang%20and%20Qiming%20Xia%20and%20Yan%20Xia%20and%20Jonathan%20Li%20and%20Kyle%20Gao%20and%20Chenglu%20Wen%20and%20Cheng%20Wang%0AAbstract%3A%20%20%20LiDAR-based%20vision%20systems%20are%20integral%20for%203D%20object%20detection%2C%20which%20is%0Acrucial%20for%20autonomous%20navigation.%20However%2C%20they%20suffer%20from%20performance%0Adegradation%20in%20adverse%20weather%20conditions%20due%20to%20the%20quality%20deterioration%20of%0ALiDAR%20point%20clouds.%20Fusing%20LiDAR%20with%20the%20weather-robust%204D%20radar%20sensor%20is%0Aexpected%20to%20solve%20this%20problem.%20However%2C%20the%20fusion%20of%20LiDAR%20and%204D%20radar%20is%0Achallenging%20because%20they%20differ%20significantly%20in%20terms%20of%20data%20quality%20and%20the%0Adegree%20of%20degradation%20in%20adverse%20weather.%20To%20address%20these%20issues%2C%20we%20introduce%0AL4DR%2C%20a%20weather-robust%203D%20object%20detection%20method%20that%20effectively%20achieves%0ALiDAR%20and%204D%20Radar%20fusion.%20Our%20L4DR%20includes%20Multi-Modal%20Encoding%20%28MME%29%20and%0AForeground-Aware%20Denoising%20%28FAD%29%20technique%20to%20reconcile%20sensor%20gaps%2C%20which%20is%0Athe%20first%20exploration%20of%20the%20complementarity%20of%20early%20fusion%20between%20LiDAR%20and%0A4D%20radar.%20Additionally%2C%20we%20design%20an%20Inter-Modal%20and%20Intra-Modal%20%28%7BIM%7D2%20%29%0Aparallel%20feature%20extraction%20backbone%20coupled%20with%20a%20Multi-Scale%20Gated%20Fusion%0A%28MSGF%29%20module%20to%20counteract%20the%20varying%20degrees%20of%20sensor%20degradation%20under%0Aadverse%20weather%20conditions.%20Experimental%20evaluation%20on%20a%20VoD%20dataset%20with%0Asimulated%20fog%20proves%20that%20L4DR%20is%20more%20adaptable%20to%20changing%20weather%0Aconditions.%20It%20delivers%20a%20significant%20performance%20increase%20under%20different%20fog%0Alevels%2C%20improving%20the%203D%20mAP%20by%20up%20to%2020.0%25%20over%20the%20traditional%20LiDAR-only%0Aapproach.%20Moreover%2C%20the%20results%20on%20the%20K-Radar%20dataset%20validate%20the%20consistent%0Aperformance%20improvement%20of%20L4DR%20in%20real-world%20adverse%20weather%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03677v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL4DR%253A%2520LiDAR-4DRadar%2520Fusion%2520for%2520Weather-Robust%25203D%2520Object%2520Detection%26entry.906535625%3DXun%2520Huang%2520and%2520Ziyu%2520Xu%2520and%2520Hai%2520Wu%2520and%2520Jinlong%2520Wang%2520and%2520Qiming%2520Xia%2520and%2520Yan%2520Xia%2520and%2520Jonathan%2520Li%2520and%2520Kyle%2520Gao%2520and%2520Chenglu%2520Wen%2520and%2520Cheng%2520Wang%26entry.1292438233%3D%2520%2520LiDAR-based%2520vision%2520systems%2520are%2520integral%2520for%25203D%2520object%2520detection%252C%2520which%2520is%250Acrucial%2520for%2520autonomous%2520navigation.%2520However%252C%2520they%2520suffer%2520from%2520performance%250Adegradation%2520in%2520adverse%2520weather%2520conditions%2520due%2520to%2520the%2520quality%2520deterioration%2520of%250ALiDAR%2520point%2520clouds.%2520Fusing%2520LiDAR%2520with%2520the%2520weather-robust%25204D%2520radar%2520sensor%2520is%250Aexpected%2520to%2520solve%2520this%2520problem.%2520However%252C%2520the%2520fusion%2520of%2520LiDAR%2520and%25204D%2520radar%2520is%250Achallenging%2520because%2520they%2520differ%2520significantly%2520in%2520terms%2520of%2520data%2520quality%2520and%2520the%250Adegree%2520of%2520degradation%2520in%2520adverse%2520weather.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%250AL4DR%252C%2520a%2520weather-robust%25203D%2520object%2520detection%2520method%2520that%2520effectively%2520achieves%250ALiDAR%2520and%25204D%2520Radar%2520fusion.%2520Our%2520L4DR%2520includes%2520Multi-Modal%2520Encoding%2520%2528MME%2529%2520and%250AForeground-Aware%2520Denoising%2520%2528FAD%2529%2520technique%2520to%2520reconcile%2520sensor%2520gaps%252C%2520which%2520is%250Athe%2520first%2520exploration%2520of%2520the%2520complementarity%2520of%2520early%2520fusion%2520between%2520LiDAR%2520and%250A4D%2520radar.%2520Additionally%252C%2520we%2520design%2520an%2520Inter-Modal%2520and%2520Intra-Modal%2520%2528%257BIM%257D2%2520%2529%250Aparallel%2520feature%2520extraction%2520backbone%2520coupled%2520with%2520a%2520Multi-Scale%2520Gated%2520Fusion%250A%2528MSGF%2529%2520module%2520to%2520counteract%2520the%2520varying%2520degrees%2520of%2520sensor%2520degradation%2520under%250Aadverse%2520weather%2520conditions.%2520Experimental%2520evaluation%2520on%2520a%2520VoD%2520dataset%2520with%250Asimulated%2520fog%2520proves%2520that%2520L4DR%2520is%2520more%2520adaptable%2520to%2520changing%2520weather%250Aconditions.%2520It%2520delivers%2520a%2520significant%2520performance%2520increase%2520under%2520different%2520fog%250Alevels%252C%2520improving%2520the%25203D%2520mAP%2520by%2520up%2520to%252020.0%2525%2520over%2520the%2520traditional%2520LiDAR-only%250Aapproach.%2520Moreover%252C%2520the%2520results%2520on%2520the%2520K-Radar%2520dataset%2520validate%2520the%2520consistent%250Aperformance%2520improvement%2520of%2520L4DR%2520in%2520real-world%2520adverse%2520weather%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03677v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L4DR%3A%20LiDAR-4DRadar%20Fusion%20for%20Weather-Robust%203D%20Object%20Detection&entry.906535625=Xun%20Huang%20and%20Ziyu%20Xu%20and%20Hai%20Wu%20and%20Jinlong%20Wang%20and%20Qiming%20Xia%20and%20Yan%20Xia%20and%20Jonathan%20Li%20and%20Kyle%20Gao%20and%20Chenglu%20Wen%20and%20Cheng%20Wang&entry.1292438233=%20%20LiDAR-based%20vision%20systems%20are%20integral%20for%203D%20object%20detection%2C%20which%20is%0Acrucial%20for%20autonomous%20navigation.%20However%2C%20they%20suffer%20from%20performance%0Adegradation%20in%20adverse%20weather%20conditions%20due%20to%20the%20quality%20deterioration%20of%0ALiDAR%20point%20clouds.%20Fusing%20LiDAR%20with%20the%20weather-robust%204D%20radar%20sensor%20is%0Aexpected%20to%20solve%20this%20problem.%20However%2C%20the%20fusion%20of%20LiDAR%20and%204D%20radar%20is%0Achallenging%20because%20they%20differ%20significantly%20in%20terms%20of%20data%20quality%20and%20the%0Adegree%20of%20degradation%20in%20adverse%20weather.%20To%20address%20these%20issues%2C%20we%20introduce%0AL4DR%2C%20a%20weather-robust%203D%20object%20detection%20method%20that%20effectively%20achieves%0ALiDAR%20and%204D%20Radar%20fusion.%20Our%20L4DR%20includes%20Multi-Modal%20Encoding%20%28MME%29%20and%0AForeground-Aware%20Denoising%20%28FAD%29%20technique%20to%20reconcile%20sensor%20gaps%2C%20which%20is%0Athe%20first%20exploration%20of%20the%20complementarity%20of%20early%20fusion%20between%20LiDAR%20and%0A4D%20radar.%20Additionally%2C%20we%20design%20an%20Inter-Modal%20and%20Intra-Modal%20%28%7BIM%7D2%20%29%0Aparallel%20feature%20extraction%20backbone%20coupled%20with%20a%20Multi-Scale%20Gated%20Fusion%0A%28MSGF%29%20module%20to%20counteract%20the%20varying%20degrees%20of%20sensor%20degradation%20under%0Aadverse%20weather%20conditions.%20Experimental%20evaluation%20on%20a%20VoD%20dataset%20with%0Asimulated%20fog%20proves%20that%20L4DR%20is%20more%20adaptable%20to%20changing%20weather%0Aconditions.%20It%20delivers%20a%20significant%20performance%20increase%20under%20different%20fog%0Alevels%2C%20improving%20the%203D%20mAP%20by%20up%20to%2020.0%25%20over%20the%20traditional%20LiDAR-only%0Aapproach.%20Moreover%2C%20the%20results%20on%20the%20K-Radar%20dataset%20validate%20the%20consistent%0Aperformance%20improvement%20of%20L4DR%20in%20real-world%20adverse%20weather%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03677v3&entry.124074799=Read"},
{"title": "The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by\n  Leveraging Second-Order Information", "author": "Diyuan Wu and Ionut-Vlad Modoranu and Mher Safaryan and Denis Kuznedelev and Dan Alistarh", "abstract": "  The rising footprint of machine learning has led to a focus on imposing\n\\emph{model sparsity} as a means of reducing computational and memory costs.\nFor deep neural networks (DNNs), the state-of-the-art accuracy-vs-sparsity is\nachieved by heuristics inspired by the classical Optimal Brain Surgeon (OBS)\nframework~\\citep{lecun90brain, hassibi1992second, hassibi1993optimal}, which\nleverages loss curvature information to make better pruning decisions. Yet,\nthese results still lack a solid theoretical understanding, and it is unclear\nwhether they can be improved by leveraging connections to the wealth of work on\nsparse recovery algorithms. In this paper, we draw new connections between\nthese two areas and present new sparse recovery algorithms inspired by the OBS\nframework that comes with theoretical guarantees under reasonable assumptions\nand have strong practical performance. Specifically, our work starts from the\nobservation that we can leverage curvature information in OBS-like fashion upon\nthe projection step of classic iterative sparse recovery algorithms such as\nIHT. We show for the first time that this leads both to improved convergence\nbounds under standard assumptions. Furthermore, we present extensions of this\napproach to the practical task of obtaining accurate sparse DNNs, and validate\nit experimentally at scale for Transformer-based models on vision and language\ntasks.\n", "link": "http://arxiv.org/abs/2408.17163v1", "date": "2024-08-30", "relevancy": 2.0937, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5374}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5172}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Iterative%20Optimal%20Brain%20Surgeon%3A%20Faster%20Sparse%20Recovery%20by%0A%20%20Leveraging%20Second-Order%20Information&body=Title%3A%20The%20Iterative%20Optimal%20Brain%20Surgeon%3A%20Faster%20Sparse%20Recovery%20by%0A%20%20Leveraging%20Second-Order%20Information%0AAuthor%3A%20Diyuan%20Wu%20and%20Ionut-Vlad%20Modoranu%20and%20Mher%20Safaryan%20and%20Denis%20Kuznedelev%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20The%20rising%20footprint%20of%20machine%20learning%20has%20led%20to%20a%20focus%20on%20imposing%0A%5Cemph%7Bmodel%20sparsity%7D%20as%20a%20means%20of%20reducing%20computational%20and%20memory%20costs.%0AFor%20deep%20neural%20networks%20%28DNNs%29%2C%20the%20state-of-the-art%20accuracy-vs-sparsity%20is%0Aachieved%20by%20heuristics%20inspired%20by%20the%20classical%20Optimal%20Brain%20Surgeon%20%28OBS%29%0Aframework~%5Ccitep%7Blecun90brain%2C%20hassibi1992second%2C%20hassibi1993optimal%7D%2C%20which%0Aleverages%20loss%20curvature%20information%20to%20make%20better%20pruning%20decisions.%20Yet%2C%0Athese%20results%20still%20lack%20a%20solid%20theoretical%20understanding%2C%20and%20it%20is%20unclear%0Awhether%20they%20can%20be%20improved%20by%20leveraging%20connections%20to%20the%20wealth%20of%20work%20on%0Asparse%20recovery%20algorithms.%20In%20this%20paper%2C%20we%20draw%20new%20connections%20between%0Athese%20two%20areas%20and%20present%20new%20sparse%20recovery%20algorithms%20inspired%20by%20the%20OBS%0Aframework%20that%20comes%20with%20theoretical%20guarantees%20under%20reasonable%20assumptions%0Aand%20have%20strong%20practical%20performance.%20Specifically%2C%20our%20work%20starts%20from%20the%0Aobservation%20that%20we%20can%20leverage%20curvature%20information%20in%20OBS-like%20fashion%20upon%0Athe%20projection%20step%20of%20classic%20iterative%20sparse%20recovery%20algorithms%20such%20as%0AIHT.%20We%20show%20for%20the%20first%20time%20that%20this%20leads%20both%20to%20improved%20convergence%0Abounds%20under%20standard%20assumptions.%20Furthermore%2C%20we%20present%20extensions%20of%20this%0Aapproach%20to%20the%20practical%20task%20of%20obtaining%20accurate%20sparse%20DNNs%2C%20and%20validate%0Ait%20experimentally%20at%20scale%20for%20Transformer-based%20models%20on%20vision%20and%20language%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Iterative%2520Optimal%2520Brain%2520Surgeon%253A%2520Faster%2520Sparse%2520Recovery%2520by%250A%2520%2520Leveraging%2520Second-Order%2520Information%26entry.906535625%3DDiyuan%2520Wu%2520and%2520Ionut-Vlad%2520Modoranu%2520and%2520Mher%2520Safaryan%2520and%2520Denis%2520Kuznedelev%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520The%2520rising%2520footprint%2520of%2520machine%2520learning%2520has%2520led%2520to%2520a%2520focus%2520on%2520imposing%250A%255Cemph%257Bmodel%2520sparsity%257D%2520as%2520a%2520means%2520of%2520reducing%2520computational%2520and%2520memory%2520costs.%250AFor%2520deep%2520neural%2520networks%2520%2528DNNs%2529%252C%2520the%2520state-of-the-art%2520accuracy-vs-sparsity%2520is%250Aachieved%2520by%2520heuristics%2520inspired%2520by%2520the%2520classical%2520Optimal%2520Brain%2520Surgeon%2520%2528OBS%2529%250Aframework~%255Ccitep%257Blecun90brain%252C%2520hassibi1992second%252C%2520hassibi1993optimal%257D%252C%2520which%250Aleverages%2520loss%2520curvature%2520information%2520to%2520make%2520better%2520pruning%2520decisions.%2520Yet%252C%250Athese%2520results%2520still%2520lack%2520a%2520solid%2520theoretical%2520understanding%252C%2520and%2520it%2520is%2520unclear%250Awhether%2520they%2520can%2520be%2520improved%2520by%2520leveraging%2520connections%2520to%2520the%2520wealth%2520of%2520work%2520on%250Asparse%2520recovery%2520algorithms.%2520In%2520this%2520paper%252C%2520we%2520draw%2520new%2520connections%2520between%250Athese%2520two%2520areas%2520and%2520present%2520new%2520sparse%2520recovery%2520algorithms%2520inspired%2520by%2520the%2520OBS%250Aframework%2520that%2520comes%2520with%2520theoretical%2520guarantees%2520under%2520reasonable%2520assumptions%250Aand%2520have%2520strong%2520practical%2520performance.%2520Specifically%252C%2520our%2520work%2520starts%2520from%2520the%250Aobservation%2520that%2520we%2520can%2520leverage%2520curvature%2520information%2520in%2520OBS-like%2520fashion%2520upon%250Athe%2520projection%2520step%2520of%2520classic%2520iterative%2520sparse%2520recovery%2520algorithms%2520such%2520as%250AIHT.%2520We%2520show%2520for%2520the%2520first%2520time%2520that%2520this%2520leads%2520both%2520to%2520improved%2520convergence%250Abounds%2520under%2520standard%2520assumptions.%2520Furthermore%252C%2520we%2520present%2520extensions%2520of%2520this%250Aapproach%2520to%2520the%2520practical%2520task%2520of%2520obtaining%2520accurate%2520sparse%2520DNNs%252C%2520and%2520validate%250Ait%2520experimentally%2520at%2520scale%2520for%2520Transformer-based%2520models%2520on%2520vision%2520and%2520language%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Iterative%20Optimal%20Brain%20Surgeon%3A%20Faster%20Sparse%20Recovery%20by%0A%20%20Leveraging%20Second-Order%20Information&entry.906535625=Diyuan%20Wu%20and%20Ionut-Vlad%20Modoranu%20and%20Mher%20Safaryan%20and%20Denis%20Kuznedelev%20and%20Dan%20Alistarh&entry.1292438233=%20%20The%20rising%20footprint%20of%20machine%20learning%20has%20led%20to%20a%20focus%20on%20imposing%0A%5Cemph%7Bmodel%20sparsity%7D%20as%20a%20means%20of%20reducing%20computational%20and%20memory%20costs.%0AFor%20deep%20neural%20networks%20%28DNNs%29%2C%20the%20state-of-the-art%20accuracy-vs-sparsity%20is%0Aachieved%20by%20heuristics%20inspired%20by%20the%20classical%20Optimal%20Brain%20Surgeon%20%28OBS%29%0Aframework~%5Ccitep%7Blecun90brain%2C%20hassibi1992second%2C%20hassibi1993optimal%7D%2C%20which%0Aleverages%20loss%20curvature%20information%20to%20make%20better%20pruning%20decisions.%20Yet%2C%0Athese%20results%20still%20lack%20a%20solid%20theoretical%20understanding%2C%20and%20it%20is%20unclear%0Awhether%20they%20can%20be%20improved%20by%20leveraging%20connections%20to%20the%20wealth%20of%20work%20on%0Asparse%20recovery%20algorithms.%20In%20this%20paper%2C%20we%20draw%20new%20connections%20between%0Athese%20two%20areas%20and%20present%20new%20sparse%20recovery%20algorithms%20inspired%20by%20the%20OBS%0Aframework%20that%20comes%20with%20theoretical%20guarantees%20under%20reasonable%20assumptions%0Aand%20have%20strong%20practical%20performance.%20Specifically%2C%20our%20work%20starts%20from%20the%0Aobservation%20that%20we%20can%20leverage%20curvature%20information%20in%20OBS-like%20fashion%20upon%0Athe%20projection%20step%20of%20classic%20iterative%20sparse%20recovery%20algorithms%20such%20as%0AIHT.%20We%20show%20for%20the%20first%20time%20that%20this%20leads%20both%20to%20improved%20convergence%0Abounds%20under%20standard%20assumptions.%20Furthermore%2C%20we%20present%20extensions%20of%20this%0Aapproach%20to%20the%20practical%20task%20of%20obtaining%20accurate%20sparse%20DNNs%2C%20and%20validate%0Ait%20experimentally%20at%20scale%20for%20Transformer-based%20models%20on%20vision%20and%20language%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17163v1&entry.124074799=Read"},
{"title": "Traffic expertise meets residual RL: Knowledge-informed model-based\n  residual reinforcement learning for CAV trajectory control", "author": "Zihao Sheng and Zilin Huang and Sikai Chen", "abstract": "  Model-based reinforcement learning (RL) is anticipated to exhibit higher\nsample efficiency compared to model-free RL by utilizing a virtual environment\nmodel. However, it is challenging to obtain sufficiently accurate\nrepresentations of the environmental dynamics due to uncertainties in complex\nsystems and environments. An inaccurate environment model may degrade the\nsample efficiency and performance of model-based RL. Furthermore, while\nmodel-based RL can improve sample efficiency, it often still requires\nsubstantial training time to learn from scratch, potentially limiting its\nadvantages over model-free approaches. To address these challenges, this paper\nintroduces a knowledge-informed model-based residual reinforcement learning\nframework aimed at enhancing learning efficiency by infusing established expert\nknowledge into the learning process and avoiding the issue of beginning from\nzero. Our approach integrates traffic expert knowledge into a virtual\nenvironment model, employing the Intelligent Driver Model (IDM) for basic\ndynamics and neural networks for residual dynamics, thus ensuring adaptability\nto complex scenarios. We propose a novel strategy that combines traditional\ncontrol methods with residual RL, facilitating efficient learning and policy\noptimization without the need to learn from scratch. The proposed approach is\napplied to CAV trajectory control tasks for the dissipation of stop-and-go\nwaves in mixed traffic flow. Experimental results demonstrate that our proposed\napproach enables the CAV agent to achieve superior performance in trajectory\ncontrol compared to the baseline agents in terms of sample efficiency, traffic\nflow smoothness and traffic mobility. The source code and supplementary\nmaterials are available at https://github.com/zihaosheng/traffic-expertise-RL/.\n", "link": "http://arxiv.org/abs/2408.17380v1", "date": "2024-08-30", "relevancy": 2.0917, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5418}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5328}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Traffic%20expertise%20meets%20residual%20RL%3A%20Knowledge-informed%20model-based%0A%20%20residual%20reinforcement%20learning%20for%20CAV%20trajectory%20control&body=Title%3A%20Traffic%20expertise%20meets%20residual%20RL%3A%20Knowledge-informed%20model-based%0A%20%20residual%20reinforcement%20learning%20for%20CAV%20trajectory%20control%0AAuthor%3A%20Zihao%20Sheng%20and%20Zilin%20Huang%20and%20Sikai%20Chen%0AAbstract%3A%20%20%20Model-based%20reinforcement%20learning%20%28RL%29%20is%20anticipated%20to%20exhibit%20higher%0Asample%20efficiency%20compared%20to%20model-free%20RL%20by%20utilizing%20a%20virtual%20environment%0Amodel.%20However%2C%20it%20is%20challenging%20to%20obtain%20sufficiently%20accurate%0Arepresentations%20of%20the%20environmental%20dynamics%20due%20to%20uncertainties%20in%20complex%0Asystems%20and%20environments.%20An%20inaccurate%20environment%20model%20may%20degrade%20the%0Asample%20efficiency%20and%20performance%20of%20model-based%20RL.%20Furthermore%2C%20while%0Amodel-based%20RL%20can%20improve%20sample%20efficiency%2C%20it%20often%20still%20requires%0Asubstantial%20training%20time%20to%20learn%20from%20scratch%2C%20potentially%20limiting%20its%0Aadvantages%20over%20model-free%20approaches.%20To%20address%20these%20challenges%2C%20this%20paper%0Aintroduces%20a%20knowledge-informed%20model-based%20residual%20reinforcement%20learning%0Aframework%20aimed%20at%20enhancing%20learning%20efficiency%20by%20infusing%20established%20expert%0Aknowledge%20into%20the%20learning%20process%20and%20avoiding%20the%20issue%20of%20beginning%20from%0Azero.%20Our%20approach%20integrates%20traffic%20expert%20knowledge%20into%20a%20virtual%0Aenvironment%20model%2C%20employing%20the%20Intelligent%20Driver%20Model%20%28IDM%29%20for%20basic%0Adynamics%20and%20neural%20networks%20for%20residual%20dynamics%2C%20thus%20ensuring%20adaptability%0Ato%20complex%20scenarios.%20We%20propose%20a%20novel%20strategy%20that%20combines%20traditional%0Acontrol%20methods%20with%20residual%20RL%2C%20facilitating%20efficient%20learning%20and%20policy%0Aoptimization%20without%20the%20need%20to%20learn%20from%20scratch.%20The%20proposed%20approach%20is%0Aapplied%20to%20CAV%20trajectory%20control%20tasks%20for%20the%20dissipation%20of%20stop-and-go%0Awaves%20in%20mixed%20traffic%20flow.%20Experimental%20results%20demonstrate%20that%20our%20proposed%0Aapproach%20enables%20the%20CAV%20agent%20to%20achieve%20superior%20performance%20in%20trajectory%0Acontrol%20compared%20to%20the%20baseline%20agents%20in%20terms%20of%20sample%20efficiency%2C%20traffic%0Aflow%20smoothness%20and%20traffic%20mobility.%20The%20source%20code%20and%20supplementary%0Amaterials%20are%20available%20at%20https%3A//github.com/zihaosheng/traffic-expertise-RL/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraffic%2520expertise%2520meets%2520residual%2520RL%253A%2520Knowledge-informed%2520model-based%250A%2520%2520residual%2520reinforcement%2520learning%2520for%2520CAV%2520trajectory%2520control%26entry.906535625%3DZihao%2520Sheng%2520and%2520Zilin%2520Huang%2520and%2520Sikai%2520Chen%26entry.1292438233%3D%2520%2520Model-based%2520reinforcement%2520learning%2520%2528RL%2529%2520is%2520anticipated%2520to%2520exhibit%2520higher%250Asample%2520efficiency%2520compared%2520to%2520model-free%2520RL%2520by%2520utilizing%2520a%2520virtual%2520environment%250Amodel.%2520However%252C%2520it%2520is%2520challenging%2520to%2520obtain%2520sufficiently%2520accurate%250Arepresentations%2520of%2520the%2520environmental%2520dynamics%2520due%2520to%2520uncertainties%2520in%2520complex%250Asystems%2520and%2520environments.%2520An%2520inaccurate%2520environment%2520model%2520may%2520degrade%2520the%250Asample%2520efficiency%2520and%2520performance%2520of%2520model-based%2520RL.%2520Furthermore%252C%2520while%250Amodel-based%2520RL%2520can%2520improve%2520sample%2520efficiency%252C%2520it%2520often%2520still%2520requires%250Asubstantial%2520training%2520time%2520to%2520learn%2520from%2520scratch%252C%2520potentially%2520limiting%2520its%250Aadvantages%2520over%2520model-free%2520approaches.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%250Aintroduces%2520a%2520knowledge-informed%2520model-based%2520residual%2520reinforcement%2520learning%250Aframework%2520aimed%2520at%2520enhancing%2520learning%2520efficiency%2520by%2520infusing%2520established%2520expert%250Aknowledge%2520into%2520the%2520learning%2520process%2520and%2520avoiding%2520the%2520issue%2520of%2520beginning%2520from%250Azero.%2520Our%2520approach%2520integrates%2520traffic%2520expert%2520knowledge%2520into%2520a%2520virtual%250Aenvironment%2520model%252C%2520employing%2520the%2520Intelligent%2520Driver%2520Model%2520%2528IDM%2529%2520for%2520basic%250Adynamics%2520and%2520neural%2520networks%2520for%2520residual%2520dynamics%252C%2520thus%2520ensuring%2520adaptability%250Ato%2520complex%2520scenarios.%2520We%2520propose%2520a%2520novel%2520strategy%2520that%2520combines%2520traditional%250Acontrol%2520methods%2520with%2520residual%2520RL%252C%2520facilitating%2520efficient%2520learning%2520and%2520policy%250Aoptimization%2520without%2520the%2520need%2520to%2520learn%2520from%2520scratch.%2520The%2520proposed%2520approach%2520is%250Aapplied%2520to%2520CAV%2520trajectory%2520control%2520tasks%2520for%2520the%2520dissipation%2520of%2520stop-and-go%250Awaves%2520in%2520mixed%2520traffic%2520flow.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520proposed%250Aapproach%2520enables%2520the%2520CAV%2520agent%2520to%2520achieve%2520superior%2520performance%2520in%2520trajectory%250Acontrol%2520compared%2520to%2520the%2520baseline%2520agents%2520in%2520terms%2520of%2520sample%2520efficiency%252C%2520traffic%250Aflow%2520smoothness%2520and%2520traffic%2520mobility.%2520The%2520source%2520code%2520and%2520supplementary%250Amaterials%2520are%2520available%2520at%2520https%253A//github.com/zihaosheng/traffic-expertise-RL/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Traffic%20expertise%20meets%20residual%20RL%3A%20Knowledge-informed%20model-based%0A%20%20residual%20reinforcement%20learning%20for%20CAV%20trajectory%20control&entry.906535625=Zihao%20Sheng%20and%20Zilin%20Huang%20and%20Sikai%20Chen&entry.1292438233=%20%20Model-based%20reinforcement%20learning%20%28RL%29%20is%20anticipated%20to%20exhibit%20higher%0Asample%20efficiency%20compared%20to%20model-free%20RL%20by%20utilizing%20a%20virtual%20environment%0Amodel.%20However%2C%20it%20is%20challenging%20to%20obtain%20sufficiently%20accurate%0Arepresentations%20of%20the%20environmental%20dynamics%20due%20to%20uncertainties%20in%20complex%0Asystems%20and%20environments.%20An%20inaccurate%20environment%20model%20may%20degrade%20the%0Asample%20efficiency%20and%20performance%20of%20model-based%20RL.%20Furthermore%2C%20while%0Amodel-based%20RL%20can%20improve%20sample%20efficiency%2C%20it%20often%20still%20requires%0Asubstantial%20training%20time%20to%20learn%20from%20scratch%2C%20potentially%20limiting%20its%0Aadvantages%20over%20model-free%20approaches.%20To%20address%20these%20challenges%2C%20this%20paper%0Aintroduces%20a%20knowledge-informed%20model-based%20residual%20reinforcement%20learning%0Aframework%20aimed%20at%20enhancing%20learning%20efficiency%20by%20infusing%20established%20expert%0Aknowledge%20into%20the%20learning%20process%20and%20avoiding%20the%20issue%20of%20beginning%20from%0Azero.%20Our%20approach%20integrates%20traffic%20expert%20knowledge%20into%20a%20virtual%0Aenvironment%20model%2C%20employing%20the%20Intelligent%20Driver%20Model%20%28IDM%29%20for%20basic%0Adynamics%20and%20neural%20networks%20for%20residual%20dynamics%2C%20thus%20ensuring%20adaptability%0Ato%20complex%20scenarios.%20We%20propose%20a%20novel%20strategy%20that%20combines%20traditional%0Acontrol%20methods%20with%20residual%20RL%2C%20facilitating%20efficient%20learning%20and%20policy%0Aoptimization%20without%20the%20need%20to%20learn%20from%20scratch.%20The%20proposed%20approach%20is%0Aapplied%20to%20CAV%20trajectory%20control%20tasks%20for%20the%20dissipation%20of%20stop-and-go%0Awaves%20in%20mixed%20traffic%20flow.%20Experimental%20results%20demonstrate%20that%20our%20proposed%0Aapproach%20enables%20the%20CAV%20agent%20to%20achieve%20superior%20performance%20in%20trajectory%0Acontrol%20compared%20to%20the%20baseline%20agents%20in%20terms%20of%20sample%20efficiency%2C%20traffic%0Aflow%20smoothness%20and%20traffic%20mobility.%20The%20source%20code%20and%20supplementary%0Amaterials%20are%20available%20at%20https%3A//github.com/zihaosheng/traffic-expertise-RL/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17380v1&entry.124074799=Read"},
{"title": "Human-Free Automated Prompting for Vision-Language Anomaly Detection:\n  Prompt Optimization with Meta-guiding Prompt Scheme", "author": "Pi-Wei Chen and Jerry Chun-Wei Lin and Jia Ji and Feng-Hao Yeh and Chao-Chun Chen", "abstract": "  Pre-trained vision-language models (VLMs) are highly adaptable to various\ndownstream tasks through few-shot learning, making prompt-based anomaly\ndetection a promising approach. Traditional methods depend on human-crafted\nprompts that require prior knowledge of specific anomaly types. Our goal is to\ndevelop a human-free prompt-based anomaly detection framework that optimally\nlearns prompts through data-driven methods, eliminating the need for human\nintervention. The primary challenge in this approach is the lack of anomalous\nsamples during the training phase. Additionally, the Vision Transformer\n(ViT)-based image encoder in VLMs is not ideal for pixel-wise anomaly\nsegmentation due to a locality feature mismatch between the original image and\nthe output feature map. To tackle the first challenge, we have developed the\nObject-Attention Anomaly Generation Module (OAGM) to synthesize anomaly samples\nfor training. Furthermore, our Meta-Guiding Prompt-Tuning Scheme (MPTS)\niteratively adjusts the gradient-based optimization direction of learnable\nprompts to avoid overfitting to the synthesized anomalies. For the second\nchallenge, we propose Locality-Aware Attention, which ensures that each local\npatch feature attends only to nearby patch features, preserving the locality\nfeatures corresponding to their original locations. This framework allows for\nthe optimal prompt embeddings by searching in the continuous latent space via\nbackpropagation, free from human semantic constraints. Additionally, the\nmodified locality-aware attention improves the precision of pixel-wise anomaly\nsegmentation.\n", "link": "http://arxiv.org/abs/2406.18197v2", "date": "2024-08-30", "relevancy": 2.0807, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5287}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5162}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Free%20Automated%20Prompting%20for%20Vision-Language%20Anomaly%20Detection%3A%0A%20%20Prompt%20Optimization%20with%20Meta-guiding%20Prompt%20Scheme&body=Title%3A%20Human-Free%20Automated%20Prompting%20for%20Vision-Language%20Anomaly%20Detection%3A%0A%20%20Prompt%20Optimization%20with%20Meta-guiding%20Prompt%20Scheme%0AAuthor%3A%20Pi-Wei%20Chen%20and%20Jerry%20Chun-Wei%20Lin%20and%20Jia%20Ji%20and%20Feng-Hao%20Yeh%20and%20Chao-Chun%20Chen%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20models%20%28VLMs%29%20are%20highly%20adaptable%20to%20various%0Adownstream%20tasks%20through%20few-shot%20learning%2C%20making%20prompt-based%20anomaly%0Adetection%20a%20promising%20approach.%20Traditional%20methods%20depend%20on%20human-crafted%0Aprompts%20that%20require%20prior%20knowledge%20of%20specific%20anomaly%20types.%20Our%20goal%20is%20to%0Adevelop%20a%20human-free%20prompt-based%20anomaly%20detection%20framework%20that%20optimally%0Alearns%20prompts%20through%20data-driven%20methods%2C%20eliminating%20the%20need%20for%20human%0Aintervention.%20The%20primary%20challenge%20in%20this%20approach%20is%20the%20lack%20of%20anomalous%0Asamples%20during%20the%20training%20phase.%20Additionally%2C%20the%20Vision%20Transformer%0A%28ViT%29-based%20image%20encoder%20in%20VLMs%20is%20not%20ideal%20for%20pixel-wise%20anomaly%0Asegmentation%20due%20to%20a%20locality%20feature%20mismatch%20between%20the%20original%20image%20and%0Athe%20output%20feature%20map.%20To%20tackle%20the%20first%20challenge%2C%20we%20have%20developed%20the%0AObject-Attention%20Anomaly%20Generation%20Module%20%28OAGM%29%20to%20synthesize%20anomaly%20samples%0Afor%20training.%20Furthermore%2C%20our%20Meta-Guiding%20Prompt-Tuning%20Scheme%20%28MPTS%29%0Aiteratively%20adjusts%20the%20gradient-based%20optimization%20direction%20of%20learnable%0Aprompts%20to%20avoid%20overfitting%20to%20the%20synthesized%20anomalies.%20For%20the%20second%0Achallenge%2C%20we%20propose%20Locality-Aware%20Attention%2C%20which%20ensures%20that%20each%20local%0Apatch%20feature%20attends%20only%20to%20nearby%20patch%20features%2C%20preserving%20the%20locality%0Afeatures%20corresponding%20to%20their%20original%20locations.%20This%20framework%20allows%20for%0Athe%20optimal%20prompt%20embeddings%20by%20searching%20in%20the%20continuous%20latent%20space%20via%0Abackpropagation%2C%20free%20from%20human%20semantic%20constraints.%20Additionally%2C%20the%0Amodified%20locality-aware%20attention%20improves%20the%20precision%20of%20pixel-wise%20anomaly%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18197v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Free%2520Automated%2520Prompting%2520for%2520Vision-Language%2520Anomaly%2520Detection%253A%250A%2520%2520Prompt%2520Optimization%2520with%2520Meta-guiding%2520Prompt%2520Scheme%26entry.906535625%3DPi-Wei%2520Chen%2520and%2520Jerry%2520Chun-Wei%2520Lin%2520and%2520Jia%2520Ji%2520and%2520Feng-Hao%2520Yeh%2520and%2520Chao-Chun%2520Chen%26entry.1292438233%3D%2520%2520Pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%2520are%2520highly%2520adaptable%2520to%2520various%250Adownstream%2520tasks%2520through%2520few-shot%2520learning%252C%2520making%2520prompt-based%2520anomaly%250Adetection%2520a%2520promising%2520approach.%2520Traditional%2520methods%2520depend%2520on%2520human-crafted%250Aprompts%2520that%2520require%2520prior%2520knowledge%2520of%2520specific%2520anomaly%2520types.%2520Our%2520goal%2520is%2520to%250Adevelop%2520a%2520human-free%2520prompt-based%2520anomaly%2520detection%2520framework%2520that%2520optimally%250Alearns%2520prompts%2520through%2520data-driven%2520methods%252C%2520eliminating%2520the%2520need%2520for%2520human%250Aintervention.%2520The%2520primary%2520challenge%2520in%2520this%2520approach%2520is%2520the%2520lack%2520of%2520anomalous%250Asamples%2520during%2520the%2520training%2520phase.%2520Additionally%252C%2520the%2520Vision%2520Transformer%250A%2528ViT%2529-based%2520image%2520encoder%2520in%2520VLMs%2520is%2520not%2520ideal%2520for%2520pixel-wise%2520anomaly%250Asegmentation%2520due%2520to%2520a%2520locality%2520feature%2520mismatch%2520between%2520the%2520original%2520image%2520and%250Athe%2520output%2520feature%2520map.%2520To%2520tackle%2520the%2520first%2520challenge%252C%2520we%2520have%2520developed%2520the%250AObject-Attention%2520Anomaly%2520Generation%2520Module%2520%2528OAGM%2529%2520to%2520synthesize%2520anomaly%2520samples%250Afor%2520training.%2520Furthermore%252C%2520our%2520Meta-Guiding%2520Prompt-Tuning%2520Scheme%2520%2528MPTS%2529%250Aiteratively%2520adjusts%2520the%2520gradient-based%2520optimization%2520direction%2520of%2520learnable%250Aprompts%2520to%2520avoid%2520overfitting%2520to%2520the%2520synthesized%2520anomalies.%2520For%2520the%2520second%250Achallenge%252C%2520we%2520propose%2520Locality-Aware%2520Attention%252C%2520which%2520ensures%2520that%2520each%2520local%250Apatch%2520feature%2520attends%2520only%2520to%2520nearby%2520patch%2520features%252C%2520preserving%2520the%2520locality%250Afeatures%2520corresponding%2520to%2520their%2520original%2520locations.%2520This%2520framework%2520allows%2520for%250Athe%2520optimal%2520prompt%2520embeddings%2520by%2520searching%2520in%2520the%2520continuous%2520latent%2520space%2520via%250Abackpropagation%252C%2520free%2520from%2520human%2520semantic%2520constraints.%2520Additionally%252C%2520the%250Amodified%2520locality-aware%2520attention%2520improves%2520the%2520precision%2520of%2520pixel-wise%2520anomaly%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18197v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Free%20Automated%20Prompting%20for%20Vision-Language%20Anomaly%20Detection%3A%0A%20%20Prompt%20Optimization%20with%20Meta-guiding%20Prompt%20Scheme&entry.906535625=Pi-Wei%20Chen%20and%20Jerry%20Chun-Wei%20Lin%20and%20Jia%20Ji%20and%20Feng-Hao%20Yeh%20and%20Chao-Chun%20Chen&entry.1292438233=%20%20Pre-trained%20vision-language%20models%20%28VLMs%29%20are%20highly%20adaptable%20to%20various%0Adownstream%20tasks%20through%20few-shot%20learning%2C%20making%20prompt-based%20anomaly%0Adetection%20a%20promising%20approach.%20Traditional%20methods%20depend%20on%20human-crafted%0Aprompts%20that%20require%20prior%20knowledge%20of%20specific%20anomaly%20types.%20Our%20goal%20is%20to%0Adevelop%20a%20human-free%20prompt-based%20anomaly%20detection%20framework%20that%20optimally%0Alearns%20prompts%20through%20data-driven%20methods%2C%20eliminating%20the%20need%20for%20human%0Aintervention.%20The%20primary%20challenge%20in%20this%20approach%20is%20the%20lack%20of%20anomalous%0Asamples%20during%20the%20training%20phase.%20Additionally%2C%20the%20Vision%20Transformer%0A%28ViT%29-based%20image%20encoder%20in%20VLMs%20is%20not%20ideal%20for%20pixel-wise%20anomaly%0Asegmentation%20due%20to%20a%20locality%20feature%20mismatch%20between%20the%20original%20image%20and%0Athe%20output%20feature%20map.%20To%20tackle%20the%20first%20challenge%2C%20we%20have%20developed%20the%0AObject-Attention%20Anomaly%20Generation%20Module%20%28OAGM%29%20to%20synthesize%20anomaly%20samples%0Afor%20training.%20Furthermore%2C%20our%20Meta-Guiding%20Prompt-Tuning%20Scheme%20%28MPTS%29%0Aiteratively%20adjusts%20the%20gradient-based%20optimization%20direction%20of%20learnable%0Aprompts%20to%20avoid%20overfitting%20to%20the%20synthesized%20anomalies.%20For%20the%20second%0Achallenge%2C%20we%20propose%20Locality-Aware%20Attention%2C%20which%20ensures%20that%20each%20local%0Apatch%20feature%20attends%20only%20to%20nearby%20patch%20features%2C%20preserving%20the%20locality%0Afeatures%20corresponding%20to%20their%20original%20locations.%20This%20framework%20allows%20for%0Athe%20optimal%20prompt%20embeddings%20by%20searching%20in%20the%20continuous%20latent%20space%20via%0Abackpropagation%2C%20free%20from%20human%20semantic%20constraints.%20Additionally%2C%20the%0Amodified%20locality-aware%20attention%20improves%20the%20precision%20of%20pixel-wise%20anomaly%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18197v2&entry.124074799=Read"},
{"title": "Incorporating Unlabelled Data into Bayesian Neural Networks", "author": "Mrinank Sharma and Tom Rainforth and Yee Whye Teh and Vincent Fortuin", "abstract": "  Conventional Bayesian Neural Networks (BNNs) are unable to leverage\nunlabelled data to improve their predictions. To overcome this limitation, we\nintroduce Self-Supervised Bayesian Neural Networks, which use unlabelled data\nto learn models with suitable prior predictive distributions. This is achieved\nby leveraging contrastive pretraining techniques and optimising a variational\nlower bound. We then show that the prior predictive distributions of\nself-supervised BNNs capture problem semantics better than conventional BNN\npriors. In turn, our approach offers improved predictive performance over\nconventional BNNs, especially in low-budget regimes.\n", "link": "http://arxiv.org/abs/2304.01762v3", "date": "2024-08-30", "relevancy": 2.0614, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5501}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4915}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incorporating%20Unlabelled%20Data%20into%20Bayesian%20Neural%20Networks&body=Title%3A%20Incorporating%20Unlabelled%20Data%20into%20Bayesian%20Neural%20Networks%0AAuthor%3A%20Mrinank%20Sharma%20and%20Tom%20Rainforth%20and%20Yee%20Whye%20Teh%20and%20Vincent%20Fortuin%0AAbstract%3A%20%20%20Conventional%20Bayesian%20Neural%20Networks%20%28BNNs%29%20are%20unable%20to%20leverage%0Aunlabelled%20data%20to%20improve%20their%20predictions.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20Self-Supervised%20Bayesian%20Neural%20Networks%2C%20which%20use%20unlabelled%20data%0Ato%20learn%20models%20with%20suitable%20prior%20predictive%20distributions.%20This%20is%20achieved%0Aby%20leveraging%20contrastive%20pretraining%20techniques%20and%20optimising%20a%20variational%0Alower%20bound.%20We%20then%20show%20that%20the%20prior%20predictive%20distributions%20of%0Aself-supervised%20BNNs%20capture%20problem%20semantics%20better%20than%20conventional%20BNN%0Apriors.%20In%20turn%2C%20our%20approach%20offers%20improved%20predictive%20performance%20over%0Aconventional%20BNNs%2C%20especially%20in%20low-budget%20regimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.01762v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncorporating%2520Unlabelled%2520Data%2520into%2520Bayesian%2520Neural%2520Networks%26entry.906535625%3DMrinank%2520Sharma%2520and%2520Tom%2520Rainforth%2520and%2520Yee%2520Whye%2520Teh%2520and%2520Vincent%2520Fortuin%26entry.1292438233%3D%2520%2520Conventional%2520Bayesian%2520Neural%2520Networks%2520%2528BNNs%2529%2520are%2520unable%2520to%2520leverage%250Aunlabelled%2520data%2520to%2520improve%2520their%2520predictions.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Aintroduce%2520Self-Supervised%2520Bayesian%2520Neural%2520Networks%252C%2520which%2520use%2520unlabelled%2520data%250Ato%2520learn%2520models%2520with%2520suitable%2520prior%2520predictive%2520distributions.%2520This%2520is%2520achieved%250Aby%2520leveraging%2520contrastive%2520pretraining%2520techniques%2520and%2520optimising%2520a%2520variational%250Alower%2520bound.%2520We%2520then%2520show%2520that%2520the%2520prior%2520predictive%2520distributions%2520of%250Aself-supervised%2520BNNs%2520capture%2520problem%2520semantics%2520better%2520than%2520conventional%2520BNN%250Apriors.%2520In%2520turn%252C%2520our%2520approach%2520offers%2520improved%2520predictive%2520performance%2520over%250Aconventional%2520BNNs%252C%2520especially%2520in%2520low-budget%2520regimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.01762v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Unlabelled%20Data%20into%20Bayesian%20Neural%20Networks&entry.906535625=Mrinank%20Sharma%20and%20Tom%20Rainforth%20and%20Yee%20Whye%20Teh%20and%20Vincent%20Fortuin&entry.1292438233=%20%20Conventional%20Bayesian%20Neural%20Networks%20%28BNNs%29%20are%20unable%20to%20leverage%0Aunlabelled%20data%20to%20improve%20their%20predictions.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20Self-Supervised%20Bayesian%20Neural%20Networks%2C%20which%20use%20unlabelled%20data%0Ato%20learn%20models%20with%20suitable%20prior%20predictive%20distributions.%20This%20is%20achieved%0Aby%20leveraging%20contrastive%20pretraining%20techniques%20and%20optimising%20a%20variational%0Alower%20bound.%20We%20then%20show%20that%20the%20prior%20predictive%20distributions%20of%0Aself-supervised%20BNNs%20capture%20problem%20semantics%20better%20than%20conventional%20BNN%0Apriors.%20In%20turn%2C%20our%20approach%20offers%20improved%20predictive%20performance%20over%0Aconventional%20BNNs%2C%20especially%20in%20low-budget%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.01762v3&entry.124074799=Read"},
{"title": "Joint Estimation and Prediction of City-wide Delivery Demand: A Large\n  Language Model Empowered Graph-based Learning Approach", "author": "Tong Nie and Junlin He and Yuewen Mei and Guoyang Qin and Guilong Li and Jian Sun and Wei Ma", "abstract": "  The proliferation of e-commerce and urbanization has significantly\nintensified delivery operations in urban areas, boosting the volume and\ncomplexity of delivery demand. Data-driven predictive methods, especially those\nutilizing machine learning techniques, have emerged to handle these\ncomplexities in urban delivery demand management problems. One particularly\npressing problem that has not yet been sufficiently studied is the joint\nestimation and prediction of city-wide delivery demand. To this end, we\nformulate this problem as a graph-based spatiotemporal learning task. First, a\nmessage-passing neural network model is formalized to capture the interaction\nbetween demand patterns of associated regions. Second, by exploiting recent\nadvances in large language models, we extract general geospatial knowledge\nencodings from the unstructured locational data and integrate them into the\ndemand predictor. Last, to encourage the cross-city transferability of the\nmodel, an inductive training scheme is developed in an end-to-end routine.\nExtensive empirical results on two real-world delivery datasets, including\neight cities in China and the US, demonstrate that our model significantly\noutperforms state-of-the-art baselines in these challenging tasks.\n", "link": "http://arxiv.org/abs/2408.17258v1", "date": "2024-08-30", "relevancy": 2.0601, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5519}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5107}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Estimation%20and%20Prediction%20of%20City-wide%20Delivery%20Demand%3A%20A%20Large%0A%20%20Language%20Model%20Empowered%20Graph-based%20Learning%20Approach&body=Title%3A%20Joint%20Estimation%20and%20Prediction%20of%20City-wide%20Delivery%20Demand%3A%20A%20Large%0A%20%20Language%20Model%20Empowered%20Graph-based%20Learning%20Approach%0AAuthor%3A%20Tong%20Nie%20and%20Junlin%20He%20and%20Yuewen%20Mei%20and%20Guoyang%20Qin%20and%20Guilong%20Li%20and%20Jian%20Sun%20and%20Wei%20Ma%0AAbstract%3A%20%20%20The%20proliferation%20of%20e-commerce%20and%20urbanization%20has%20significantly%0Aintensified%20delivery%20operations%20in%20urban%20areas%2C%20boosting%20the%20volume%20and%0Acomplexity%20of%20delivery%20demand.%20Data-driven%20predictive%20methods%2C%20especially%20those%0Autilizing%20machine%20learning%20techniques%2C%20have%20emerged%20to%20handle%20these%0Acomplexities%20in%20urban%20delivery%20demand%20management%20problems.%20One%20particularly%0Apressing%20problem%20that%20has%20not%20yet%20been%20sufficiently%20studied%20is%20the%20joint%0Aestimation%20and%20prediction%20of%20city-wide%20delivery%20demand.%20To%20this%20end%2C%20we%0Aformulate%20this%20problem%20as%20a%20graph-based%20spatiotemporal%20learning%20task.%20First%2C%20a%0Amessage-passing%20neural%20network%20model%20is%20formalized%20to%20capture%20the%20interaction%0Abetween%20demand%20patterns%20of%20associated%20regions.%20Second%2C%20by%20exploiting%20recent%0Aadvances%20in%20large%20language%20models%2C%20we%20extract%20general%20geospatial%20knowledge%0Aencodings%20from%20the%20unstructured%20locational%20data%20and%20integrate%20them%20into%20the%0Ademand%20predictor.%20Last%2C%20to%20encourage%20the%20cross-city%20transferability%20of%20the%0Amodel%2C%20an%20inductive%20training%20scheme%20is%20developed%20in%20an%20end-to-end%20routine.%0AExtensive%20empirical%20results%20on%20two%20real-world%20delivery%20datasets%2C%20including%0Aeight%20cities%20in%20China%20and%20the%20US%2C%20demonstrate%20that%20our%20model%20significantly%0Aoutperforms%20state-of-the-art%20baselines%20in%20these%20challenging%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Estimation%2520and%2520Prediction%2520of%2520City-wide%2520Delivery%2520Demand%253A%2520A%2520Large%250A%2520%2520Language%2520Model%2520Empowered%2520Graph-based%2520Learning%2520Approach%26entry.906535625%3DTong%2520Nie%2520and%2520Junlin%2520He%2520and%2520Yuewen%2520Mei%2520and%2520Guoyang%2520Qin%2520and%2520Guilong%2520Li%2520and%2520Jian%2520Sun%2520and%2520Wei%2520Ma%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520e-commerce%2520and%2520urbanization%2520has%2520significantly%250Aintensified%2520delivery%2520operations%2520in%2520urban%2520areas%252C%2520boosting%2520the%2520volume%2520and%250Acomplexity%2520of%2520delivery%2520demand.%2520Data-driven%2520predictive%2520methods%252C%2520especially%2520those%250Autilizing%2520machine%2520learning%2520techniques%252C%2520have%2520emerged%2520to%2520handle%2520these%250Acomplexities%2520in%2520urban%2520delivery%2520demand%2520management%2520problems.%2520One%2520particularly%250Apressing%2520problem%2520that%2520has%2520not%2520yet%2520been%2520sufficiently%2520studied%2520is%2520the%2520joint%250Aestimation%2520and%2520prediction%2520of%2520city-wide%2520delivery%2520demand.%2520To%2520this%2520end%252C%2520we%250Aformulate%2520this%2520problem%2520as%2520a%2520graph-based%2520spatiotemporal%2520learning%2520task.%2520First%252C%2520a%250Amessage-passing%2520neural%2520network%2520model%2520is%2520formalized%2520to%2520capture%2520the%2520interaction%250Abetween%2520demand%2520patterns%2520of%2520associated%2520regions.%2520Second%252C%2520by%2520exploiting%2520recent%250Aadvances%2520in%2520large%2520language%2520models%252C%2520we%2520extract%2520general%2520geospatial%2520knowledge%250Aencodings%2520from%2520the%2520unstructured%2520locational%2520data%2520and%2520integrate%2520them%2520into%2520the%250Ademand%2520predictor.%2520Last%252C%2520to%2520encourage%2520the%2520cross-city%2520transferability%2520of%2520the%250Amodel%252C%2520an%2520inductive%2520training%2520scheme%2520is%2520developed%2520in%2520an%2520end-to-end%2520routine.%250AExtensive%2520empirical%2520results%2520on%2520two%2520real-world%2520delivery%2520datasets%252C%2520including%250Aeight%2520cities%2520in%2520China%2520and%2520the%2520US%252C%2520demonstrate%2520that%2520our%2520model%2520significantly%250Aoutperforms%2520state-of-the-art%2520baselines%2520in%2520these%2520challenging%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Estimation%20and%20Prediction%20of%20City-wide%20Delivery%20Demand%3A%20A%20Large%0A%20%20Language%20Model%20Empowered%20Graph-based%20Learning%20Approach&entry.906535625=Tong%20Nie%20and%20Junlin%20He%20and%20Yuewen%20Mei%20and%20Guoyang%20Qin%20and%20Guilong%20Li%20and%20Jian%20Sun%20and%20Wei%20Ma&entry.1292438233=%20%20The%20proliferation%20of%20e-commerce%20and%20urbanization%20has%20significantly%0Aintensified%20delivery%20operations%20in%20urban%20areas%2C%20boosting%20the%20volume%20and%0Acomplexity%20of%20delivery%20demand.%20Data-driven%20predictive%20methods%2C%20especially%20those%0Autilizing%20machine%20learning%20techniques%2C%20have%20emerged%20to%20handle%20these%0Acomplexities%20in%20urban%20delivery%20demand%20management%20problems.%20One%20particularly%0Apressing%20problem%20that%20has%20not%20yet%20been%20sufficiently%20studied%20is%20the%20joint%0Aestimation%20and%20prediction%20of%20city-wide%20delivery%20demand.%20To%20this%20end%2C%20we%0Aformulate%20this%20problem%20as%20a%20graph-based%20spatiotemporal%20learning%20task.%20First%2C%20a%0Amessage-passing%20neural%20network%20model%20is%20formalized%20to%20capture%20the%20interaction%0Abetween%20demand%20patterns%20of%20associated%20regions.%20Second%2C%20by%20exploiting%20recent%0Aadvances%20in%20large%20language%20models%2C%20we%20extract%20general%20geospatial%20knowledge%0Aencodings%20from%20the%20unstructured%20locational%20data%20and%20integrate%20them%20into%20the%0Ademand%20predictor.%20Last%2C%20to%20encourage%20the%20cross-city%20transferability%20of%20the%0Amodel%2C%20an%20inductive%20training%20scheme%20is%20developed%20in%20an%20end-to-end%20routine.%0AExtensive%20empirical%20results%20on%20two%20real-world%20delivery%20datasets%2C%20including%0Aeight%20cities%20in%20China%20and%20the%20US%2C%20demonstrate%20that%20our%20model%20significantly%0Aoutperforms%20state-of-the-art%20baselines%20in%20these%20challenging%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17258v1&entry.124074799=Read"},
{"title": "Continual learning with the neural tangent ensemble", "author": "Ari S. Benjamin and Christian Pehle and Kyle Daruwalla", "abstract": "  A natural strategy for continual learning is to weigh a Bayesian ensemble of\nfixed functions. This suggests that if a (single) neural network could be\ninterpreted as an ensemble, one could design effective algorithms that learn\nwithout forgetting. To realize this possibility, we observe that a neural\nnetwork classifier with N parameters can be interpreted as a weighted ensemble\nof N classifiers, and that in the lazy regime limit these classifiers are fixed\nthroughout learning. We term these classifiers the neural tangent experts and\nshow they output valid probability distributions over the labels. We then\nderive the likelihood and posterior probability of each expert given past data.\nSurprisingly, we learn that the posterior updates for these experts are\nequivalent to a scaled and projected form of stochastic gradient descent (SGD)\nover the network weights. Away from the lazy regime, networks can be seen as\nensembles of adaptive experts which improve over time. These results offer a\nnew interpretation of neural networks as Bayesian ensembles of experts,\nproviding a principled framework for understanding and mitigating catastrophic\nforgetting in continual learning settings.\n", "link": "http://arxiv.org/abs/2408.17394v1", "date": "2024-08-30", "relevancy": 2.0573, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5433}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5026}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20learning%20with%20the%20neural%20tangent%20ensemble&body=Title%3A%20Continual%20learning%20with%20the%20neural%20tangent%20ensemble%0AAuthor%3A%20Ari%20S.%20Benjamin%20and%20Christian%20Pehle%20and%20Kyle%20Daruwalla%0AAbstract%3A%20%20%20A%20natural%20strategy%20for%20continual%20learning%20is%20to%20weigh%20a%20Bayesian%20ensemble%20of%0Afixed%20functions.%20This%20suggests%20that%20if%20a%20%28single%29%20neural%20network%20could%20be%0Ainterpreted%20as%20an%20ensemble%2C%20one%20could%20design%20effective%20algorithms%20that%20learn%0Awithout%20forgetting.%20To%20realize%20this%20possibility%2C%20we%20observe%20that%20a%20neural%0Anetwork%20classifier%20with%20N%20parameters%20can%20be%20interpreted%20as%20a%20weighted%20ensemble%0Aof%20N%20classifiers%2C%20and%20that%20in%20the%20lazy%20regime%20limit%20these%20classifiers%20are%20fixed%0Athroughout%20learning.%20We%20term%20these%20classifiers%20the%20neural%20tangent%20experts%20and%0Ashow%20they%20output%20valid%20probability%20distributions%20over%20the%20labels.%20We%20then%0Aderive%20the%20likelihood%20and%20posterior%20probability%20of%20each%20expert%20given%20past%20data.%0ASurprisingly%2C%20we%20learn%20that%20the%20posterior%20updates%20for%20these%20experts%20are%0Aequivalent%20to%20a%20scaled%20and%20projected%20form%20of%20stochastic%20gradient%20descent%20%28SGD%29%0Aover%20the%20network%20weights.%20Away%20from%20the%20lazy%20regime%2C%20networks%20can%20be%20seen%20as%0Aensembles%20of%20adaptive%20experts%20which%20improve%20over%20time.%20These%20results%20offer%20a%0Anew%20interpretation%20of%20neural%20networks%20as%20Bayesian%20ensembles%20of%20experts%2C%0Aproviding%20a%20principled%20framework%20for%20understanding%20and%20mitigating%20catastrophic%0Aforgetting%20in%20continual%20learning%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520learning%2520with%2520the%2520neural%2520tangent%2520ensemble%26entry.906535625%3DAri%2520S.%2520Benjamin%2520and%2520Christian%2520Pehle%2520and%2520Kyle%2520Daruwalla%26entry.1292438233%3D%2520%2520A%2520natural%2520strategy%2520for%2520continual%2520learning%2520is%2520to%2520weigh%2520a%2520Bayesian%2520ensemble%2520of%250Afixed%2520functions.%2520This%2520suggests%2520that%2520if%2520a%2520%2528single%2529%2520neural%2520network%2520could%2520be%250Ainterpreted%2520as%2520an%2520ensemble%252C%2520one%2520could%2520design%2520effective%2520algorithms%2520that%2520learn%250Awithout%2520forgetting.%2520To%2520realize%2520this%2520possibility%252C%2520we%2520observe%2520that%2520a%2520neural%250Anetwork%2520classifier%2520with%2520N%2520parameters%2520can%2520be%2520interpreted%2520as%2520a%2520weighted%2520ensemble%250Aof%2520N%2520classifiers%252C%2520and%2520that%2520in%2520the%2520lazy%2520regime%2520limit%2520these%2520classifiers%2520are%2520fixed%250Athroughout%2520learning.%2520We%2520term%2520these%2520classifiers%2520the%2520neural%2520tangent%2520experts%2520and%250Ashow%2520they%2520output%2520valid%2520probability%2520distributions%2520over%2520the%2520labels.%2520We%2520then%250Aderive%2520the%2520likelihood%2520and%2520posterior%2520probability%2520of%2520each%2520expert%2520given%2520past%2520data.%250ASurprisingly%252C%2520we%2520learn%2520that%2520the%2520posterior%2520updates%2520for%2520these%2520experts%2520are%250Aequivalent%2520to%2520a%2520scaled%2520and%2520projected%2520form%2520of%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%250Aover%2520the%2520network%2520weights.%2520Away%2520from%2520the%2520lazy%2520regime%252C%2520networks%2520can%2520be%2520seen%2520as%250Aensembles%2520of%2520adaptive%2520experts%2520which%2520improve%2520over%2520time.%2520These%2520results%2520offer%2520a%250Anew%2520interpretation%2520of%2520neural%2520networks%2520as%2520Bayesian%2520ensembles%2520of%2520experts%252C%250Aproviding%2520a%2520principled%2520framework%2520for%2520understanding%2520and%2520mitigating%2520catastrophic%250Aforgetting%2520in%2520continual%2520learning%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20learning%20with%20the%20neural%20tangent%20ensemble&entry.906535625=Ari%20S.%20Benjamin%20and%20Christian%20Pehle%20and%20Kyle%20Daruwalla&entry.1292438233=%20%20A%20natural%20strategy%20for%20continual%20learning%20is%20to%20weigh%20a%20Bayesian%20ensemble%20of%0Afixed%20functions.%20This%20suggests%20that%20if%20a%20%28single%29%20neural%20network%20could%20be%0Ainterpreted%20as%20an%20ensemble%2C%20one%20could%20design%20effective%20algorithms%20that%20learn%0Awithout%20forgetting.%20To%20realize%20this%20possibility%2C%20we%20observe%20that%20a%20neural%0Anetwork%20classifier%20with%20N%20parameters%20can%20be%20interpreted%20as%20a%20weighted%20ensemble%0Aof%20N%20classifiers%2C%20and%20that%20in%20the%20lazy%20regime%20limit%20these%20classifiers%20are%20fixed%0Athroughout%20learning.%20We%20term%20these%20classifiers%20the%20neural%20tangent%20experts%20and%0Ashow%20they%20output%20valid%20probability%20distributions%20over%20the%20labels.%20We%20then%0Aderive%20the%20likelihood%20and%20posterior%20probability%20of%20each%20expert%20given%20past%20data.%0ASurprisingly%2C%20we%20learn%20that%20the%20posterior%20updates%20for%20these%20experts%20are%0Aequivalent%20to%20a%20scaled%20and%20projected%20form%20of%20stochastic%20gradient%20descent%20%28SGD%29%0Aover%20the%20network%20weights.%20Away%20from%20the%20lazy%20regime%2C%20networks%20can%20be%20seen%20as%0Aensembles%20of%20adaptive%20experts%20which%20improve%20over%20time.%20These%20results%20offer%20a%0Anew%20interpretation%20of%20neural%20networks%20as%20Bayesian%20ensembles%20of%20experts%2C%0Aproviding%20a%20principled%20framework%20for%20understanding%20and%20mitigating%20catastrophic%0Aforgetting%20in%20continual%20learning%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17394v1&entry.124074799=Read"},
{"title": "LightFF: Lightweight Inference for Forward-Forward Algorithm", "author": "Amin Aminifar and Baichuan Huang and Azra Abtahi and Amir Aminifar", "abstract": "  The human brain performs tasks with an outstanding energy efficiency, i.e.,\nwith approximately 20 Watts. The state-of-the-art Artificial/Deep Neural\nNetworks (ANN/DNN), on the other hand, have recently been shown to consume\nmassive amounts of energy. The training of these ANNs/DNNs is done almost\nexclusively based on the back-propagation algorithm, which is known to be\nbiologically implausible. This has led to a new generation of forward-only\ntechniques, including the Forward-Forward algorithm. In this paper, we propose\na lightweight inference scheme specifically designed for DNNs trained using the\nForward-Forward algorithm. We have evaluated our proposed lightweight inference\nscheme in the case of the MNIST and CIFAR datasets, as well as two real-world\napplications, namely, epileptic seizure detection and cardiac arrhythmia\nclassification using wearable technologies, where complexity overheads/energy\nconsumption is a major constraint, and demonstrate its relevance. Our code is\navailable at https://github.com/AminAminifar/LightFF.\n", "link": "http://arxiv.org/abs/2404.05241v6", "date": "2024-08-30", "relevancy": 2.051, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5195}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5081}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightFF%3A%20Lightweight%20Inference%20for%20Forward-Forward%20Algorithm&body=Title%3A%20LightFF%3A%20Lightweight%20Inference%20for%20Forward-Forward%20Algorithm%0AAuthor%3A%20Amin%20Aminifar%20and%20Baichuan%20Huang%20and%20Azra%20Abtahi%20and%20Amir%20Aminifar%0AAbstract%3A%20%20%20The%20human%20brain%20performs%20tasks%20with%20an%20outstanding%20energy%20efficiency%2C%20i.e.%2C%0Awith%20approximately%2020%20Watts.%20The%20state-of-the-art%20Artificial/Deep%20Neural%0ANetworks%20%28ANN/DNN%29%2C%20on%20the%20other%20hand%2C%20have%20recently%20been%20shown%20to%20consume%0Amassive%20amounts%20of%20energy.%20The%20training%20of%20these%20ANNs/DNNs%20is%20done%20almost%0Aexclusively%20based%20on%20the%20back-propagation%20algorithm%2C%20which%20is%20known%20to%20be%0Abiologically%20implausible.%20This%20has%20led%20to%20a%20new%20generation%20of%20forward-only%0Atechniques%2C%20including%20the%20Forward-Forward%20algorithm.%20In%20this%20paper%2C%20we%20propose%0Aa%20lightweight%20inference%20scheme%20specifically%20designed%20for%20DNNs%20trained%20using%20the%0AForward-Forward%20algorithm.%20We%20have%20evaluated%20our%20proposed%20lightweight%20inference%0Ascheme%20in%20the%20case%20of%20the%20MNIST%20and%20CIFAR%20datasets%2C%20as%20well%20as%20two%20real-world%0Aapplications%2C%20namely%2C%20epileptic%20seizure%20detection%20and%20cardiac%20arrhythmia%0Aclassification%20using%20wearable%20technologies%2C%20where%20complexity%20overheads/energy%0Aconsumption%20is%20a%20major%20constraint%2C%20and%20demonstrate%20its%20relevance.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/AminAminifar/LightFF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05241v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightFF%253A%2520Lightweight%2520Inference%2520for%2520Forward-Forward%2520Algorithm%26entry.906535625%3DAmin%2520Aminifar%2520and%2520Baichuan%2520Huang%2520and%2520Azra%2520Abtahi%2520and%2520Amir%2520Aminifar%26entry.1292438233%3D%2520%2520The%2520human%2520brain%2520performs%2520tasks%2520with%2520an%2520outstanding%2520energy%2520efficiency%252C%2520i.e.%252C%250Awith%2520approximately%252020%2520Watts.%2520The%2520state-of-the-art%2520Artificial/Deep%2520Neural%250ANetworks%2520%2528ANN/DNN%2529%252C%2520on%2520the%2520other%2520hand%252C%2520have%2520recently%2520been%2520shown%2520to%2520consume%250Amassive%2520amounts%2520of%2520energy.%2520The%2520training%2520of%2520these%2520ANNs/DNNs%2520is%2520done%2520almost%250Aexclusively%2520based%2520on%2520the%2520back-propagation%2520algorithm%252C%2520which%2520is%2520known%2520to%2520be%250Abiologically%2520implausible.%2520This%2520has%2520led%2520to%2520a%2520new%2520generation%2520of%2520forward-only%250Atechniques%252C%2520including%2520the%2520Forward-Forward%2520algorithm.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520lightweight%2520inference%2520scheme%2520specifically%2520designed%2520for%2520DNNs%2520trained%2520using%2520the%250AForward-Forward%2520algorithm.%2520We%2520have%2520evaluated%2520our%2520proposed%2520lightweight%2520inference%250Ascheme%2520in%2520the%2520case%2520of%2520the%2520MNIST%2520and%2520CIFAR%2520datasets%252C%2520as%2520well%2520as%2520two%2520real-world%250Aapplications%252C%2520namely%252C%2520epileptic%2520seizure%2520detection%2520and%2520cardiac%2520arrhythmia%250Aclassification%2520using%2520wearable%2520technologies%252C%2520where%2520complexity%2520overheads/energy%250Aconsumption%2520is%2520a%2520major%2520constraint%252C%2520and%2520demonstrate%2520its%2520relevance.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/AminAminifar/LightFF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05241v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightFF%3A%20Lightweight%20Inference%20for%20Forward-Forward%20Algorithm&entry.906535625=Amin%20Aminifar%20and%20Baichuan%20Huang%20and%20Azra%20Abtahi%20and%20Amir%20Aminifar&entry.1292438233=%20%20The%20human%20brain%20performs%20tasks%20with%20an%20outstanding%20energy%20efficiency%2C%20i.e.%2C%0Awith%20approximately%2020%20Watts.%20The%20state-of-the-art%20Artificial/Deep%20Neural%0ANetworks%20%28ANN/DNN%29%2C%20on%20the%20other%20hand%2C%20have%20recently%20been%20shown%20to%20consume%0Amassive%20amounts%20of%20energy.%20The%20training%20of%20these%20ANNs/DNNs%20is%20done%20almost%0Aexclusively%20based%20on%20the%20back-propagation%20algorithm%2C%20which%20is%20known%20to%20be%0Abiologically%20implausible.%20This%20has%20led%20to%20a%20new%20generation%20of%20forward-only%0Atechniques%2C%20including%20the%20Forward-Forward%20algorithm.%20In%20this%20paper%2C%20we%20propose%0Aa%20lightweight%20inference%20scheme%20specifically%20designed%20for%20DNNs%20trained%20using%20the%0AForward-Forward%20algorithm.%20We%20have%20evaluated%20our%20proposed%20lightweight%20inference%0Ascheme%20in%20the%20case%20of%20the%20MNIST%20and%20CIFAR%20datasets%2C%20as%20well%20as%20two%20real-world%0Aapplications%2C%20namely%2C%20epileptic%20seizure%20detection%20and%20cardiac%20arrhythmia%0Aclassification%20using%20wearable%20technologies%2C%20where%20complexity%20overheads/energy%0Aconsumption%20is%20a%20major%20constraint%2C%20and%20demonstrate%20its%20relevance.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/AminAminifar/LightFF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05241v6&entry.124074799=Read"},
{"title": "A Newton-CG based barrier-augmented Lagrangian method for general\n  nonconvex conic optimization", "author": "Chuan He and Heng Huang and Zhaosong Lu", "abstract": "  In this paper we consider finding an approximate second-order stationary\npoint (SOSP) of general nonconvex conic optimization that minimizes a twice\ndifferentiable function subject to nonlinear equality constraints and also a\nconvex conic constraint. In particular, we propose a Newton-conjugate gradient\n(Newton-CG) based barrier-augmented Lagrangian method for finding an\napproximate SOSP of this problem. Under some mild assumptions, we show that our\nmethod enjoys a total inner iteration complexity of $\\widetilde{\\cal\nO}(\\epsilon^{-11/2})$ and an operation complexity of $\\widetilde{\\cal\nO}(\\epsilon^{-11/2}\\min\\{n,\\epsilon^{-5/4}\\})$ for finding an\n$(\\epsilon,\\sqrt{\\epsilon})$-SOSP of general nonconvex conic optimization with\nhigh probability. Moreover, under a constraint qualification, these complexity\nbounds are improved to $\\widetilde{\\cal O}(\\epsilon^{-7/2})$ and\n$\\widetilde{\\cal O}(\\epsilon^{-7/2}\\min\\{n,\\epsilon^{-3/4}\\})$, respectively.\nTo the best of our knowledge, this is the first study on the complexity of\nfinding an approximate SOSP of general nonconvex conic optimization.\nPreliminary numerical results are presented to demonstrate superiority of the\nproposed method over first-order methods in terms of solution quality.\n", "link": "http://arxiv.org/abs/2301.04204v2", "date": "2024-08-30", "relevancy": 2.0446, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4174}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4052}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Newton-CG%20based%20barrier-augmented%20Lagrangian%20method%20for%20general%0A%20%20nonconvex%20conic%20optimization&body=Title%3A%20A%20Newton-CG%20based%20barrier-augmented%20Lagrangian%20method%20for%20general%0A%20%20nonconvex%20conic%20optimization%0AAuthor%3A%20Chuan%20He%20and%20Heng%20Huang%20and%20Zhaosong%20Lu%0AAbstract%3A%20%20%20In%20this%20paper%20we%20consider%20finding%20an%20approximate%20second-order%20stationary%0Apoint%20%28SOSP%29%20of%20general%20nonconvex%20conic%20optimization%20that%20minimizes%20a%20twice%0Adifferentiable%20function%20subject%20to%20nonlinear%20equality%20constraints%20and%20also%20a%0Aconvex%20conic%20constraint.%20In%20particular%2C%20we%20propose%20a%20Newton-conjugate%20gradient%0A%28Newton-CG%29%20based%20barrier-augmented%20Lagrangian%20method%20for%20finding%20an%0Aapproximate%20SOSP%20of%20this%20problem.%20Under%20some%20mild%20assumptions%2C%20we%20show%20that%20our%0Amethod%20enjoys%20a%20total%20inner%20iteration%20complexity%20of%20%24%5Cwidetilde%7B%5Ccal%0AO%7D%28%5Cepsilon%5E%7B-11/2%7D%29%24%20and%20an%20operation%20complexity%20of%20%24%5Cwidetilde%7B%5Ccal%0AO%7D%28%5Cepsilon%5E%7B-11/2%7D%5Cmin%5C%7Bn%2C%5Cepsilon%5E%7B-5/4%7D%5C%7D%29%24%20for%20finding%20an%0A%24%28%5Cepsilon%2C%5Csqrt%7B%5Cepsilon%7D%29%24-SOSP%20of%20general%20nonconvex%20conic%20optimization%20with%0Ahigh%20probability.%20Moreover%2C%20under%20a%20constraint%20qualification%2C%20these%20complexity%0Abounds%20are%20improved%20to%20%24%5Cwidetilde%7B%5Ccal%20O%7D%28%5Cepsilon%5E%7B-7/2%7D%29%24%20and%0A%24%5Cwidetilde%7B%5Ccal%20O%7D%28%5Cepsilon%5E%7B-7/2%7D%5Cmin%5C%7Bn%2C%5Cepsilon%5E%7B-3/4%7D%5C%7D%29%24%2C%20respectively.%0ATo%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20study%20on%20the%20complexity%20of%0Afinding%20an%20approximate%20SOSP%20of%20general%20nonconvex%20conic%20optimization.%0APreliminary%20numerical%20results%20are%20presented%20to%20demonstrate%20superiority%20of%20the%0Aproposed%20method%20over%20first-order%20methods%20in%20terms%20of%20solution%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.04204v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Newton-CG%2520based%2520barrier-augmented%2520Lagrangian%2520method%2520for%2520general%250A%2520%2520nonconvex%2520conic%2520optimization%26entry.906535625%3DChuan%2520He%2520and%2520Heng%2520Huang%2520and%2520Zhaosong%2520Lu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520consider%2520finding%2520an%2520approximate%2520second-order%2520stationary%250Apoint%2520%2528SOSP%2529%2520of%2520general%2520nonconvex%2520conic%2520optimization%2520that%2520minimizes%2520a%2520twice%250Adifferentiable%2520function%2520subject%2520to%2520nonlinear%2520equality%2520constraints%2520and%2520also%2520a%250Aconvex%2520conic%2520constraint.%2520In%2520particular%252C%2520we%2520propose%2520a%2520Newton-conjugate%2520gradient%250A%2528Newton-CG%2529%2520based%2520barrier-augmented%2520Lagrangian%2520method%2520for%2520finding%2520an%250Aapproximate%2520SOSP%2520of%2520this%2520problem.%2520Under%2520some%2520mild%2520assumptions%252C%2520we%2520show%2520that%2520our%250Amethod%2520enjoys%2520a%2520total%2520inner%2520iteration%2520complexity%2520of%2520%2524%255Cwidetilde%257B%255Ccal%250AO%257D%2528%255Cepsilon%255E%257B-11/2%257D%2529%2524%2520and%2520an%2520operation%2520complexity%2520of%2520%2524%255Cwidetilde%257B%255Ccal%250AO%257D%2528%255Cepsilon%255E%257B-11/2%257D%255Cmin%255C%257Bn%252C%255Cepsilon%255E%257B-5/4%257D%255C%257D%2529%2524%2520for%2520finding%2520an%250A%2524%2528%255Cepsilon%252C%255Csqrt%257B%255Cepsilon%257D%2529%2524-SOSP%2520of%2520general%2520nonconvex%2520conic%2520optimization%2520with%250Ahigh%2520probability.%2520Moreover%252C%2520under%2520a%2520constraint%2520qualification%252C%2520these%2520complexity%250Abounds%2520are%2520improved%2520to%2520%2524%255Cwidetilde%257B%255Ccal%2520O%257D%2528%255Cepsilon%255E%257B-7/2%257D%2529%2524%2520and%250A%2524%255Cwidetilde%257B%255Ccal%2520O%257D%2528%255Cepsilon%255E%257B-7/2%257D%255Cmin%255C%257Bn%252C%255Cepsilon%255E%257B-3/4%257D%255C%257D%2529%2524%252C%2520respectively.%250ATo%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520study%2520on%2520the%2520complexity%2520of%250Afinding%2520an%2520approximate%2520SOSP%2520of%2520general%2520nonconvex%2520conic%2520optimization.%250APreliminary%2520numerical%2520results%2520are%2520presented%2520to%2520demonstrate%2520superiority%2520of%2520the%250Aproposed%2520method%2520over%2520first-order%2520methods%2520in%2520terms%2520of%2520solution%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.04204v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Newton-CG%20based%20barrier-augmented%20Lagrangian%20method%20for%20general%0A%20%20nonconvex%20conic%20optimization&entry.906535625=Chuan%20He%20and%20Heng%20Huang%20and%20Zhaosong%20Lu&entry.1292438233=%20%20In%20this%20paper%20we%20consider%20finding%20an%20approximate%20second-order%20stationary%0Apoint%20%28SOSP%29%20of%20general%20nonconvex%20conic%20optimization%20that%20minimizes%20a%20twice%0Adifferentiable%20function%20subject%20to%20nonlinear%20equality%20constraints%20and%20also%20a%0Aconvex%20conic%20constraint.%20In%20particular%2C%20we%20propose%20a%20Newton-conjugate%20gradient%0A%28Newton-CG%29%20based%20barrier-augmented%20Lagrangian%20method%20for%20finding%20an%0Aapproximate%20SOSP%20of%20this%20problem.%20Under%20some%20mild%20assumptions%2C%20we%20show%20that%20our%0Amethod%20enjoys%20a%20total%20inner%20iteration%20complexity%20of%20%24%5Cwidetilde%7B%5Ccal%0AO%7D%28%5Cepsilon%5E%7B-11/2%7D%29%24%20and%20an%20operation%20complexity%20of%20%24%5Cwidetilde%7B%5Ccal%0AO%7D%28%5Cepsilon%5E%7B-11/2%7D%5Cmin%5C%7Bn%2C%5Cepsilon%5E%7B-5/4%7D%5C%7D%29%24%20for%20finding%20an%0A%24%28%5Cepsilon%2C%5Csqrt%7B%5Cepsilon%7D%29%24-SOSP%20of%20general%20nonconvex%20conic%20optimization%20with%0Ahigh%20probability.%20Moreover%2C%20under%20a%20constraint%20qualification%2C%20these%20complexity%0Abounds%20are%20improved%20to%20%24%5Cwidetilde%7B%5Ccal%20O%7D%28%5Cepsilon%5E%7B-7/2%7D%29%24%20and%0A%24%5Cwidetilde%7B%5Ccal%20O%7D%28%5Cepsilon%5E%7B-7/2%7D%5Cmin%5C%7Bn%2C%5Cepsilon%5E%7B-3/4%7D%5C%7D%29%24%2C%20respectively.%0ATo%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20study%20on%20the%20complexity%20of%0Afinding%20an%20approximate%20SOSP%20of%20general%20nonconvex%20conic%20optimization.%0APreliminary%20numerical%20results%20are%20presented%20to%20demonstrate%20superiority%20of%20the%0Aproposed%20method%20over%20first-order%20methods%20in%20terms%20of%20solution%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.04204v2&entry.124074799=Read"},
{"title": "Evaluating Reliability in Medical DNNs: A Critical Analysis of Feature\n  and Confidence-Based OOD Detection", "author": "Harry Anthony and Konstantinos Kamnitsas", "abstract": "  Reliable use of deep neural networks (DNNs) for medical image analysis\nrequires methods to identify inputs that differ significantly from the training\ndata, called out-of-distribution (OOD), to prevent erroneous predictions. OOD\ndetection methods can be categorised as either confidence-based (using the\nmodel's output layer for OOD detection) or feature-based (not using the output\nlayer). We created two new OOD benchmarks by dividing the D7P (dermatology) and\nBreastMNIST (ultrasound) datasets into subsets which either contain or don't\ncontain an artefact (rulers or annotations respectively). Models were trained\nwith artefact-free images, and images with the artefacts were used as OOD test\nsets. For each OOD image, we created a counterfactual by manually removing the\nartefact via image processing, to assess the artefact's impact on the model's\npredictions. We show that OOD artefacts can boost a model's softmax confidence\nin its predictions, due to correlations in training data among other factors.\nThis contradicts the common assumption that OOD artefacts should lead to more\nuncertain outputs, an assumption on which most confidence-based methods rely.\nWe use this to explain why feature-based methods (e.g. Mahalanobis score)\ntypically have greater OOD detection performance than confidence-based methods\n(e.g. MCP). However, we also show that feature-based methods typically perform\nworse at distinguishing between inputs that lead to correct and incorrect\npredictions (for both OOD and ID data). Following from these insights, we argue\nthat a combination of feature-based and confidence-based methods should be used\nwithin DNN pipelines to mitigate their respective weaknesses. These project's\ncode and OOD benchmarks are available at:\nhttps://github.com/HarryAnthony/Evaluating_OOD_detection.\n", "link": "http://arxiv.org/abs/2408.17337v1", "date": "2024-08-30", "relevancy": 2.0387, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5589}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5086}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Reliability%20in%20Medical%20DNNs%3A%20A%20Critical%20Analysis%20of%20Feature%0A%20%20and%20Confidence-Based%20OOD%20Detection&body=Title%3A%20Evaluating%20Reliability%20in%20Medical%20DNNs%3A%20A%20Critical%20Analysis%20of%20Feature%0A%20%20and%20Confidence-Based%20OOD%20Detection%0AAuthor%3A%20Harry%20Anthony%20and%20Konstantinos%20Kamnitsas%0AAbstract%3A%20%20%20Reliable%20use%20of%20deep%20neural%20networks%20%28DNNs%29%20for%20medical%20image%20analysis%0Arequires%20methods%20to%20identify%20inputs%20that%20differ%20significantly%20from%20the%20training%0Adata%2C%20called%20out-of-distribution%20%28OOD%29%2C%20to%20prevent%20erroneous%20predictions.%20OOD%0Adetection%20methods%20can%20be%20categorised%20as%20either%20confidence-based%20%28using%20the%0Amodel%27s%20output%20layer%20for%20OOD%20detection%29%20or%20feature-based%20%28not%20using%20the%20output%0Alayer%29.%20We%20created%20two%20new%20OOD%20benchmarks%20by%20dividing%20the%20D7P%20%28dermatology%29%20and%0ABreastMNIST%20%28ultrasound%29%20datasets%20into%20subsets%20which%20either%20contain%20or%20don%27t%0Acontain%20an%20artefact%20%28rulers%20or%20annotations%20respectively%29.%20Models%20were%20trained%0Awith%20artefact-free%20images%2C%20and%20images%20with%20the%20artefacts%20were%20used%20as%20OOD%20test%0Asets.%20For%20each%20OOD%20image%2C%20we%20created%20a%20counterfactual%20by%20manually%20removing%20the%0Aartefact%20via%20image%20processing%2C%20to%20assess%20the%20artefact%27s%20impact%20on%20the%20model%27s%0Apredictions.%20We%20show%20that%20OOD%20artefacts%20can%20boost%20a%20model%27s%20softmax%20confidence%0Ain%20its%20predictions%2C%20due%20to%20correlations%20in%20training%20data%20among%20other%20factors.%0AThis%20contradicts%20the%20common%20assumption%20that%20OOD%20artefacts%20should%20lead%20to%20more%0Auncertain%20outputs%2C%20an%20assumption%20on%20which%20most%20confidence-based%20methods%20rely.%0AWe%20use%20this%20to%20explain%20why%20feature-based%20methods%20%28e.g.%20Mahalanobis%20score%29%0Atypically%20have%20greater%20OOD%20detection%20performance%20than%20confidence-based%20methods%0A%28e.g.%20MCP%29.%20However%2C%20we%20also%20show%20that%20feature-based%20methods%20typically%20perform%0Aworse%20at%20distinguishing%20between%20inputs%20that%20lead%20to%20correct%20and%20incorrect%0Apredictions%20%28for%20both%20OOD%20and%20ID%20data%29.%20Following%20from%20these%20insights%2C%20we%20argue%0Athat%20a%20combination%20of%20feature-based%20and%20confidence-based%20methods%20should%20be%20used%0Awithin%20DNN%20pipelines%20to%20mitigate%20their%20respective%20weaknesses.%20These%20project%27s%0Acode%20and%20OOD%20benchmarks%20are%20available%20at%3A%0Ahttps%3A//github.com/HarryAnthony/Evaluating_OOD_detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Reliability%2520in%2520Medical%2520DNNs%253A%2520A%2520Critical%2520Analysis%2520of%2520Feature%250A%2520%2520and%2520Confidence-Based%2520OOD%2520Detection%26entry.906535625%3DHarry%2520Anthony%2520and%2520Konstantinos%2520Kamnitsas%26entry.1292438233%3D%2520%2520Reliable%2520use%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520for%2520medical%2520image%2520analysis%250Arequires%2520methods%2520to%2520identify%2520inputs%2520that%2520differ%2520significantly%2520from%2520the%2520training%250Adata%252C%2520called%2520out-of-distribution%2520%2528OOD%2529%252C%2520to%2520prevent%2520erroneous%2520predictions.%2520OOD%250Adetection%2520methods%2520can%2520be%2520categorised%2520as%2520either%2520confidence-based%2520%2528using%2520the%250Amodel%2527s%2520output%2520layer%2520for%2520OOD%2520detection%2529%2520or%2520feature-based%2520%2528not%2520using%2520the%2520output%250Alayer%2529.%2520We%2520created%2520two%2520new%2520OOD%2520benchmarks%2520by%2520dividing%2520the%2520D7P%2520%2528dermatology%2529%2520and%250ABreastMNIST%2520%2528ultrasound%2529%2520datasets%2520into%2520subsets%2520which%2520either%2520contain%2520or%2520don%2527t%250Acontain%2520an%2520artefact%2520%2528rulers%2520or%2520annotations%2520respectively%2529.%2520Models%2520were%2520trained%250Awith%2520artefact-free%2520images%252C%2520and%2520images%2520with%2520the%2520artefacts%2520were%2520used%2520as%2520OOD%2520test%250Asets.%2520For%2520each%2520OOD%2520image%252C%2520we%2520created%2520a%2520counterfactual%2520by%2520manually%2520removing%2520the%250Aartefact%2520via%2520image%2520processing%252C%2520to%2520assess%2520the%2520artefact%2527s%2520impact%2520on%2520the%2520model%2527s%250Apredictions.%2520We%2520show%2520that%2520OOD%2520artefacts%2520can%2520boost%2520a%2520model%2527s%2520softmax%2520confidence%250Ain%2520its%2520predictions%252C%2520due%2520to%2520correlations%2520in%2520training%2520data%2520among%2520other%2520factors.%250AThis%2520contradicts%2520the%2520common%2520assumption%2520that%2520OOD%2520artefacts%2520should%2520lead%2520to%2520more%250Auncertain%2520outputs%252C%2520an%2520assumption%2520on%2520which%2520most%2520confidence-based%2520methods%2520rely.%250AWe%2520use%2520this%2520to%2520explain%2520why%2520feature-based%2520methods%2520%2528e.g.%2520Mahalanobis%2520score%2529%250Atypically%2520have%2520greater%2520OOD%2520detection%2520performance%2520than%2520confidence-based%2520methods%250A%2528e.g.%2520MCP%2529.%2520However%252C%2520we%2520also%2520show%2520that%2520feature-based%2520methods%2520typically%2520perform%250Aworse%2520at%2520distinguishing%2520between%2520inputs%2520that%2520lead%2520to%2520correct%2520and%2520incorrect%250Apredictions%2520%2528for%2520both%2520OOD%2520and%2520ID%2520data%2529.%2520Following%2520from%2520these%2520insights%252C%2520we%2520argue%250Athat%2520a%2520combination%2520of%2520feature-based%2520and%2520confidence-based%2520methods%2520should%2520be%2520used%250Awithin%2520DNN%2520pipelines%2520to%2520mitigate%2520their%2520respective%2520weaknesses.%2520These%2520project%2527s%250Acode%2520and%2520OOD%2520benchmarks%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/HarryAnthony/Evaluating_OOD_detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Reliability%20in%20Medical%20DNNs%3A%20A%20Critical%20Analysis%20of%20Feature%0A%20%20and%20Confidence-Based%20OOD%20Detection&entry.906535625=Harry%20Anthony%20and%20Konstantinos%20Kamnitsas&entry.1292438233=%20%20Reliable%20use%20of%20deep%20neural%20networks%20%28DNNs%29%20for%20medical%20image%20analysis%0Arequires%20methods%20to%20identify%20inputs%20that%20differ%20significantly%20from%20the%20training%0Adata%2C%20called%20out-of-distribution%20%28OOD%29%2C%20to%20prevent%20erroneous%20predictions.%20OOD%0Adetection%20methods%20can%20be%20categorised%20as%20either%20confidence-based%20%28using%20the%0Amodel%27s%20output%20layer%20for%20OOD%20detection%29%20or%20feature-based%20%28not%20using%20the%20output%0Alayer%29.%20We%20created%20two%20new%20OOD%20benchmarks%20by%20dividing%20the%20D7P%20%28dermatology%29%20and%0ABreastMNIST%20%28ultrasound%29%20datasets%20into%20subsets%20which%20either%20contain%20or%20don%27t%0Acontain%20an%20artefact%20%28rulers%20or%20annotations%20respectively%29.%20Models%20were%20trained%0Awith%20artefact-free%20images%2C%20and%20images%20with%20the%20artefacts%20were%20used%20as%20OOD%20test%0Asets.%20For%20each%20OOD%20image%2C%20we%20created%20a%20counterfactual%20by%20manually%20removing%20the%0Aartefact%20via%20image%20processing%2C%20to%20assess%20the%20artefact%27s%20impact%20on%20the%20model%27s%0Apredictions.%20We%20show%20that%20OOD%20artefacts%20can%20boost%20a%20model%27s%20softmax%20confidence%0Ain%20its%20predictions%2C%20due%20to%20correlations%20in%20training%20data%20among%20other%20factors.%0AThis%20contradicts%20the%20common%20assumption%20that%20OOD%20artefacts%20should%20lead%20to%20more%0Auncertain%20outputs%2C%20an%20assumption%20on%20which%20most%20confidence-based%20methods%20rely.%0AWe%20use%20this%20to%20explain%20why%20feature-based%20methods%20%28e.g.%20Mahalanobis%20score%29%0Atypically%20have%20greater%20OOD%20detection%20performance%20than%20confidence-based%20methods%0A%28e.g.%20MCP%29.%20However%2C%20we%20also%20show%20that%20feature-based%20methods%20typically%20perform%0Aworse%20at%20distinguishing%20between%20inputs%20that%20lead%20to%20correct%20and%20incorrect%0Apredictions%20%28for%20both%20OOD%20and%20ID%20data%29.%20Following%20from%20these%20insights%2C%20we%20argue%0Athat%20a%20combination%20of%20feature-based%20and%20confidence-based%20methods%20should%20be%20used%0Awithin%20DNN%20pipelines%20to%20mitigate%20their%20respective%20weaknesses.%20These%20project%27s%0Acode%20and%20OOD%20benchmarks%20are%20available%20at%3A%0Ahttps%3A//github.com/HarryAnthony/Evaluating_OOD_detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17337v1&entry.124074799=Read"},
{"title": "Advancing Multi-talker ASR Performance with Large Language Models", "author": "Mohan Shi and Zengrui Jin and Yaoxun Xu and Yong Xu and Shi-Xiong Zhang and Kun Wei and Yiwen Shao and Chunlei Zhang and Dong Yu", "abstract": "  Recognizing overlapping speech from multiple speakers in conversational\nscenarios is one of the most challenging problem for automatic speech\nrecognition (ASR). Serialized output training (SOT) is a classic method to\naddress multi-talker ASR, with the idea of concatenating transcriptions from\nmultiple speakers according to the emission times of their speech for training.\nHowever, SOT-style transcriptions, derived from concatenating multiple related\nutterances in a conversation, depend significantly on modeling long contexts.\nTherefore, compared to traditional methods that primarily emphasize encoder\nperformance in attention-based encoder-decoder (AED) architectures, a novel\napproach utilizing large language models (LLMs) that leverages the capabilities\nof pre-trained decoders may be better suited for such complex and challenging\nscenarios. In this paper, we propose an LLM-based SOT approach for multi-talker\nASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on\nmulti-talker dataset using appropriate strategies. Experimental results\ndemonstrate that our approach surpasses traditional AED-based methods on the\nsimulated dataset LibriMix and achieves state-of-the-art performance on the\nevaluation set of the real-world dataset AMI, outperforming the AED model\ntrained with 1000 times more supervised data in previous works.\n", "link": "http://arxiv.org/abs/2408.17431v1", "date": "2024-08-30", "relevancy": 2.0376, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.53}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5039}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Multi-talker%20ASR%20Performance%20with%20Large%20Language%20Models&body=Title%3A%20Advancing%20Multi-talker%20ASR%20Performance%20with%20Large%20Language%20Models%0AAuthor%3A%20Mohan%20Shi%20and%20Zengrui%20Jin%20and%20Yaoxun%20Xu%20and%20Yong%20Xu%20and%20Shi-Xiong%20Zhang%20and%20Kun%20Wei%20and%20Yiwen%20Shao%20and%20Chunlei%20Zhang%20and%20Dong%20Yu%0AAbstract%3A%20%20%20Recognizing%20overlapping%20speech%20from%20multiple%20speakers%20in%20conversational%0Ascenarios%20is%20one%20of%20the%20most%20challenging%20problem%20for%20automatic%20speech%0Arecognition%20%28ASR%29.%20Serialized%20output%20training%20%28SOT%29%20is%20a%20classic%20method%20to%0Aaddress%20multi-talker%20ASR%2C%20with%20the%20idea%20of%20concatenating%20transcriptions%20from%0Amultiple%20speakers%20according%20to%20the%20emission%20times%20of%20their%20speech%20for%20training.%0AHowever%2C%20SOT-style%20transcriptions%2C%20derived%20from%20concatenating%20multiple%20related%0Autterances%20in%20a%20conversation%2C%20depend%20significantly%20on%20modeling%20long%20contexts.%0ATherefore%2C%20compared%20to%20traditional%20methods%20that%20primarily%20emphasize%20encoder%0Aperformance%20in%20attention-based%20encoder-decoder%20%28AED%29%20architectures%2C%20a%20novel%0Aapproach%20utilizing%20large%20language%20models%20%28LLMs%29%20that%20leverages%20the%20capabilities%0Aof%20pre-trained%20decoders%20may%20be%20better%20suited%20for%20such%20complex%20and%20challenging%0Ascenarios.%20In%20this%20paper%2C%20we%20propose%20an%20LLM-based%20SOT%20approach%20for%20multi-talker%0AASR%2C%20leveraging%20pre-trained%20speech%20encoder%20and%20LLM%2C%20fine-tuning%20them%20on%0Amulti-talker%20dataset%20using%20appropriate%20strategies.%20Experimental%20results%0Ademonstrate%20that%20our%20approach%20surpasses%20traditional%20AED-based%20methods%20on%20the%0Asimulated%20dataset%20LibriMix%20and%20achieves%20state-of-the-art%20performance%20on%20the%0Aevaluation%20set%20of%20the%20real-world%20dataset%20AMI%2C%20outperforming%20the%20AED%20model%0Atrained%20with%201000%20times%20more%20supervised%20data%20in%20previous%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Multi-talker%2520ASR%2520Performance%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DMohan%2520Shi%2520and%2520Zengrui%2520Jin%2520and%2520Yaoxun%2520Xu%2520and%2520Yong%2520Xu%2520and%2520Shi-Xiong%2520Zhang%2520and%2520Kun%2520Wei%2520and%2520Yiwen%2520Shao%2520and%2520Chunlei%2520Zhang%2520and%2520Dong%2520Yu%26entry.1292438233%3D%2520%2520Recognizing%2520overlapping%2520speech%2520from%2520multiple%2520speakers%2520in%2520conversational%250Ascenarios%2520is%2520one%2520of%2520the%2520most%2520challenging%2520problem%2520for%2520automatic%2520speech%250Arecognition%2520%2528ASR%2529.%2520Serialized%2520output%2520training%2520%2528SOT%2529%2520is%2520a%2520classic%2520method%2520to%250Aaddress%2520multi-talker%2520ASR%252C%2520with%2520the%2520idea%2520of%2520concatenating%2520transcriptions%2520from%250Amultiple%2520speakers%2520according%2520to%2520the%2520emission%2520times%2520of%2520their%2520speech%2520for%2520training.%250AHowever%252C%2520SOT-style%2520transcriptions%252C%2520derived%2520from%2520concatenating%2520multiple%2520related%250Autterances%2520in%2520a%2520conversation%252C%2520depend%2520significantly%2520on%2520modeling%2520long%2520contexts.%250ATherefore%252C%2520compared%2520to%2520traditional%2520methods%2520that%2520primarily%2520emphasize%2520encoder%250Aperformance%2520in%2520attention-based%2520encoder-decoder%2520%2528AED%2529%2520architectures%252C%2520a%2520novel%250Aapproach%2520utilizing%2520large%2520language%2520models%2520%2528LLMs%2529%2520that%2520leverages%2520the%2520capabilities%250Aof%2520pre-trained%2520decoders%2520may%2520be%2520better%2520suited%2520for%2520such%2520complex%2520and%2520challenging%250Ascenarios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520LLM-based%2520SOT%2520approach%2520for%2520multi-talker%250AASR%252C%2520leveraging%2520pre-trained%2520speech%2520encoder%2520and%2520LLM%252C%2520fine-tuning%2520them%2520on%250Amulti-talker%2520dataset%2520using%2520appropriate%2520strategies.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520approach%2520surpasses%2520traditional%2520AED-based%2520methods%2520on%2520the%250Asimulated%2520dataset%2520LibriMix%2520and%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%250Aevaluation%2520set%2520of%2520the%2520real-world%2520dataset%2520AMI%252C%2520outperforming%2520the%2520AED%2520model%250Atrained%2520with%25201000%2520times%2520more%2520supervised%2520data%2520in%2520previous%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Multi-talker%20ASR%20Performance%20with%20Large%20Language%20Models&entry.906535625=Mohan%20Shi%20and%20Zengrui%20Jin%20and%20Yaoxun%20Xu%20and%20Yong%20Xu%20and%20Shi-Xiong%20Zhang%20and%20Kun%20Wei%20and%20Yiwen%20Shao%20and%20Chunlei%20Zhang%20and%20Dong%20Yu&entry.1292438233=%20%20Recognizing%20overlapping%20speech%20from%20multiple%20speakers%20in%20conversational%0Ascenarios%20is%20one%20of%20the%20most%20challenging%20problem%20for%20automatic%20speech%0Arecognition%20%28ASR%29.%20Serialized%20output%20training%20%28SOT%29%20is%20a%20classic%20method%20to%0Aaddress%20multi-talker%20ASR%2C%20with%20the%20idea%20of%20concatenating%20transcriptions%20from%0Amultiple%20speakers%20according%20to%20the%20emission%20times%20of%20their%20speech%20for%20training.%0AHowever%2C%20SOT-style%20transcriptions%2C%20derived%20from%20concatenating%20multiple%20related%0Autterances%20in%20a%20conversation%2C%20depend%20significantly%20on%20modeling%20long%20contexts.%0ATherefore%2C%20compared%20to%20traditional%20methods%20that%20primarily%20emphasize%20encoder%0Aperformance%20in%20attention-based%20encoder-decoder%20%28AED%29%20architectures%2C%20a%20novel%0Aapproach%20utilizing%20large%20language%20models%20%28LLMs%29%20that%20leverages%20the%20capabilities%0Aof%20pre-trained%20decoders%20may%20be%20better%20suited%20for%20such%20complex%20and%20challenging%0Ascenarios.%20In%20this%20paper%2C%20we%20propose%20an%20LLM-based%20SOT%20approach%20for%20multi-talker%0AASR%2C%20leveraging%20pre-trained%20speech%20encoder%20and%20LLM%2C%20fine-tuning%20them%20on%0Amulti-talker%20dataset%20using%20appropriate%20strategies.%20Experimental%20results%0Ademonstrate%20that%20our%20approach%20surpasses%20traditional%20AED-based%20methods%20on%20the%0Asimulated%20dataset%20LibriMix%20and%20achieves%20state-of-the-art%20performance%20on%20the%0Aevaluation%20set%20of%20the%20real-world%20dataset%20AMI%2C%20outperforming%20the%20AED%20model%0Atrained%20with%201000%20times%20more%20supervised%20data%20in%20previous%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17431v1&entry.124074799=Read"},
{"title": "Lamarr: LHCb ultra-fast simulation based on machine learning models\n  deployed within Gauss", "author": "Matteo Barbetti", "abstract": "  About 90% of the computing resources available to the LHCb experiment has\nbeen spent to produce simulated data samples for Run 2 of the Large Hadron\nCollider at CERN. The upgraded LHCb detector will be able to collect larger\ndata samples, requiring many more simulated events to analyze the data to be\ncollected in Run 3. Simulation is a key necessity of analysis to interpret\nsignal, reject background and measure efficiencies. The needed simulation will\nfar exceed the pledged resources, requiring an evolution in technologies and\ntechniques to produce these simulated data samples. In this contribution, we\ndiscuss Lamarr, a Gaudi-based framework to speed-up the simulation production\nparameterizing both the detector response and the reconstruction algorithms of\nthe LHCb experiment. Deep Generative Models powered by several algorithms and\nstrategies are employed to effectively parameterize the high-level response of\nthe single components of the LHCb detector, encoding within neural networks the\nexperimental errors and uncertainties introduced in the detection and\nreconstruction phases. Where possible, models are trained directly on real\ndata, statistically subtracting any background components by applying\nappropriate reweighing procedures. Embedding Lamarr in the general LHCb Gauss\nSimulation framework allows to combine its execution with any of the available\ngenerators in a seamless way. The resulting software package enables a\nsimulation process independent of the detailed simulation used to date.\n", "link": "http://arxiv.org/abs/2303.11428v3", "date": "2024-08-30", "relevancy": 2.0071, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5103}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.501}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lamarr%3A%20LHCb%20ultra-fast%20simulation%20based%20on%20machine%20learning%20models%0A%20%20deployed%20within%20Gauss&body=Title%3A%20Lamarr%3A%20LHCb%20ultra-fast%20simulation%20based%20on%20machine%20learning%20models%0A%20%20deployed%20within%20Gauss%0AAuthor%3A%20Matteo%20Barbetti%0AAbstract%3A%20%20%20About%2090%25%20of%20the%20computing%20resources%20available%20to%20the%20LHCb%20experiment%20has%0Abeen%20spent%20to%20produce%20simulated%20data%20samples%20for%20Run%202%20of%20the%20Large%20Hadron%0ACollider%20at%20CERN.%20The%20upgraded%20LHCb%20detector%20will%20be%20able%20to%20collect%20larger%0Adata%20samples%2C%20requiring%20many%20more%20simulated%20events%20to%20analyze%20the%20data%20to%20be%0Acollected%20in%20Run%203.%20Simulation%20is%20a%20key%20necessity%20of%20analysis%20to%20interpret%0Asignal%2C%20reject%20background%20and%20measure%20efficiencies.%20The%20needed%20simulation%20will%0Afar%20exceed%20the%20pledged%20resources%2C%20requiring%20an%20evolution%20in%20technologies%20and%0Atechniques%20to%20produce%20these%20simulated%20data%20samples.%20In%20this%20contribution%2C%20we%0Adiscuss%20Lamarr%2C%20a%20Gaudi-based%20framework%20to%20speed-up%20the%20simulation%20production%0Aparameterizing%20both%20the%20detector%20response%20and%20the%20reconstruction%20algorithms%20of%0Athe%20LHCb%20experiment.%20Deep%20Generative%20Models%20powered%20by%20several%20algorithms%20and%0Astrategies%20are%20employed%20to%20effectively%20parameterize%20the%20high-level%20response%20of%0Athe%20single%20components%20of%20the%20LHCb%20detector%2C%20encoding%20within%20neural%20networks%20the%0Aexperimental%20errors%20and%20uncertainties%20introduced%20in%20the%20detection%20and%0Areconstruction%20phases.%20Where%20possible%2C%20models%20are%20trained%20directly%20on%20real%0Adata%2C%20statistically%20subtracting%20any%20background%20components%20by%20applying%0Aappropriate%20reweighing%20procedures.%20Embedding%20Lamarr%20in%20the%20general%20LHCb%20Gauss%0ASimulation%20framework%20allows%20to%20combine%20its%20execution%20with%20any%20of%20the%20available%0Agenerators%20in%20a%20seamless%20way.%20The%20resulting%20software%20package%20enables%20a%0Asimulation%20process%20independent%20of%20the%20detailed%20simulation%20used%20to%20date.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.11428v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLamarr%253A%2520LHCb%2520ultra-fast%2520simulation%2520based%2520on%2520machine%2520learning%2520models%250A%2520%2520deployed%2520within%2520Gauss%26entry.906535625%3DMatteo%2520Barbetti%26entry.1292438233%3D%2520%2520About%252090%2525%2520of%2520the%2520computing%2520resources%2520available%2520to%2520the%2520LHCb%2520experiment%2520has%250Abeen%2520spent%2520to%2520produce%2520simulated%2520data%2520samples%2520for%2520Run%25202%2520of%2520the%2520Large%2520Hadron%250ACollider%2520at%2520CERN.%2520The%2520upgraded%2520LHCb%2520detector%2520will%2520be%2520able%2520to%2520collect%2520larger%250Adata%2520samples%252C%2520requiring%2520many%2520more%2520simulated%2520events%2520to%2520analyze%2520the%2520data%2520to%2520be%250Acollected%2520in%2520Run%25203.%2520Simulation%2520is%2520a%2520key%2520necessity%2520of%2520analysis%2520to%2520interpret%250Asignal%252C%2520reject%2520background%2520and%2520measure%2520efficiencies.%2520The%2520needed%2520simulation%2520will%250Afar%2520exceed%2520the%2520pledged%2520resources%252C%2520requiring%2520an%2520evolution%2520in%2520technologies%2520and%250Atechniques%2520to%2520produce%2520these%2520simulated%2520data%2520samples.%2520In%2520this%2520contribution%252C%2520we%250Adiscuss%2520Lamarr%252C%2520a%2520Gaudi-based%2520framework%2520to%2520speed-up%2520the%2520simulation%2520production%250Aparameterizing%2520both%2520the%2520detector%2520response%2520and%2520the%2520reconstruction%2520algorithms%2520of%250Athe%2520LHCb%2520experiment.%2520Deep%2520Generative%2520Models%2520powered%2520by%2520several%2520algorithms%2520and%250Astrategies%2520are%2520employed%2520to%2520effectively%2520parameterize%2520the%2520high-level%2520response%2520of%250Athe%2520single%2520components%2520of%2520the%2520LHCb%2520detector%252C%2520encoding%2520within%2520neural%2520networks%2520the%250Aexperimental%2520errors%2520and%2520uncertainties%2520introduced%2520in%2520the%2520detection%2520and%250Areconstruction%2520phases.%2520Where%2520possible%252C%2520models%2520are%2520trained%2520directly%2520on%2520real%250Adata%252C%2520statistically%2520subtracting%2520any%2520background%2520components%2520by%2520applying%250Aappropriate%2520reweighing%2520procedures.%2520Embedding%2520Lamarr%2520in%2520the%2520general%2520LHCb%2520Gauss%250ASimulation%2520framework%2520allows%2520to%2520combine%2520its%2520execution%2520with%2520any%2520of%2520the%2520available%250Agenerators%2520in%2520a%2520seamless%2520way.%2520The%2520resulting%2520software%2520package%2520enables%2520a%250Asimulation%2520process%2520independent%2520of%2520the%2520detailed%2520simulation%2520used%2520to%2520date.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.11428v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lamarr%3A%20LHCb%20ultra-fast%20simulation%20based%20on%20machine%20learning%20models%0A%20%20deployed%20within%20Gauss&entry.906535625=Matteo%20Barbetti&entry.1292438233=%20%20About%2090%25%20of%20the%20computing%20resources%20available%20to%20the%20LHCb%20experiment%20has%0Abeen%20spent%20to%20produce%20simulated%20data%20samples%20for%20Run%202%20of%20the%20Large%20Hadron%0ACollider%20at%20CERN.%20The%20upgraded%20LHCb%20detector%20will%20be%20able%20to%20collect%20larger%0Adata%20samples%2C%20requiring%20many%20more%20simulated%20events%20to%20analyze%20the%20data%20to%20be%0Acollected%20in%20Run%203.%20Simulation%20is%20a%20key%20necessity%20of%20analysis%20to%20interpret%0Asignal%2C%20reject%20background%20and%20measure%20efficiencies.%20The%20needed%20simulation%20will%0Afar%20exceed%20the%20pledged%20resources%2C%20requiring%20an%20evolution%20in%20technologies%20and%0Atechniques%20to%20produce%20these%20simulated%20data%20samples.%20In%20this%20contribution%2C%20we%0Adiscuss%20Lamarr%2C%20a%20Gaudi-based%20framework%20to%20speed-up%20the%20simulation%20production%0Aparameterizing%20both%20the%20detector%20response%20and%20the%20reconstruction%20algorithms%20of%0Athe%20LHCb%20experiment.%20Deep%20Generative%20Models%20powered%20by%20several%20algorithms%20and%0Astrategies%20are%20employed%20to%20effectively%20parameterize%20the%20high-level%20response%20of%0Athe%20single%20components%20of%20the%20LHCb%20detector%2C%20encoding%20within%20neural%20networks%20the%0Aexperimental%20errors%20and%20uncertainties%20introduced%20in%20the%20detection%20and%0Areconstruction%20phases.%20Where%20possible%2C%20models%20are%20trained%20directly%20on%20real%0Adata%2C%20statistically%20subtracting%20any%20background%20components%20by%20applying%0Aappropriate%20reweighing%20procedures.%20Embedding%20Lamarr%20in%20the%20general%20LHCb%20Gauss%0ASimulation%20framework%20allows%20to%20combine%20its%20execution%20with%20any%20of%20the%20available%0Agenerators%20in%20a%20seamless%20way.%20The%20resulting%20software%20package%20enables%20a%0Asimulation%20process%20independent%20of%20the%20detailed%20simulation%20used%20to%20date.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.11428v3&entry.124074799=Read"},
{"title": "Evolving Virtual World with Delta-Engine", "author": "Hongqiu Wu and Zekai Xu and Tianyang Xu and Shize Wei and Yan Wang and Jiale Hong and Weiqi Wu and Hai Zhao and Min Zhang and Zhezhi He", "abstract": "  In this paper, we focus on the \\emph{virtual world}, a cyberspace where\npeople can live in. An ideal virtual world shares great similarity with our\nreal world. One of the crucial aspects is its evolving nature, reflected by\nindividuals' capability to grow and thereby influence the objective world. Such\ndynamics is unpredictable and beyond the reach of existing systems. For this,\nwe propose a special engine called \\textbf{\\emph{Delta-Engine}} to drive this\nvirtual world. $\\Delta$ associates the world's evolution to the engine's\nscalability. It consists of a base engine and a neural proxy. The base engine\nprograms the prototype of the virtual world; given a trigger, the neural proxy\ngenerates new snippets on the base engine through \\emph{incremental\nprediction}. This paper presents a full-stack introduction to the delta-engine.\nThe key feature of the delta-engine is its scalability to unknown elements\nwithin the world, Technically, it derives from the prefect co-work of the\nneural proxy and the base engine, and the alignment with high-quality data. We\nintroduce an engine-oriented fine-tuning method that embeds the base engine\ninto the proxy. We then discuss the human-LLM collaborative design to produce\nnovel and interesting data efficiently. Eventually, we propose three evaluation\nprinciples to comprehensively assess the performance of a delta engine: naive\nevaluation, incremental evaluation, and adversarial evaluation.\n", "link": "http://arxiv.org/abs/2408.05842v3", "date": "2024-08-30", "relevancy": 2.0054, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5181}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4969}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evolving%20Virtual%20World%20with%20Delta-Engine&body=Title%3A%20Evolving%20Virtual%20World%20with%20Delta-Engine%0AAuthor%3A%20Hongqiu%20Wu%20and%20Zekai%20Xu%20and%20Tianyang%20Xu%20and%20Shize%20Wei%20and%20Yan%20Wang%20and%20Jiale%20Hong%20and%20Weiqi%20Wu%20and%20Hai%20Zhao%20and%20Min%20Zhang%20and%20Zhezhi%20He%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20focus%20on%20the%20%5Cemph%7Bvirtual%20world%7D%2C%20a%20cyberspace%20where%0Apeople%20can%20live%20in.%20An%20ideal%20virtual%20world%20shares%20great%20similarity%20with%20our%0Areal%20world.%20One%20of%20the%20crucial%20aspects%20is%20its%20evolving%20nature%2C%20reflected%20by%0Aindividuals%27%20capability%20to%20grow%20and%20thereby%20influence%20the%20objective%20world.%20Such%0Adynamics%20is%20unpredictable%20and%20beyond%20the%20reach%20of%20existing%20systems.%20For%20this%2C%0Awe%20propose%20a%20special%20engine%20called%20%5Ctextbf%7B%5Cemph%7BDelta-Engine%7D%7D%20to%20drive%20this%0Avirtual%20world.%20%24%5CDelta%24%20associates%20the%20world%27s%20evolution%20to%20the%20engine%27s%0Ascalability.%20It%20consists%20of%20a%20base%20engine%20and%20a%20neural%20proxy.%20The%20base%20engine%0Aprograms%20the%20prototype%20of%20the%20virtual%20world%3B%20given%20a%20trigger%2C%20the%20neural%20proxy%0Agenerates%20new%20snippets%20on%20the%20base%20engine%20through%20%5Cemph%7Bincremental%0Aprediction%7D.%20This%20paper%20presents%20a%20full-stack%20introduction%20to%20the%20delta-engine.%0AThe%20key%20feature%20of%20the%20delta-engine%20is%20its%20scalability%20to%20unknown%20elements%0Awithin%20the%20world%2C%20Technically%2C%20it%20derives%20from%20the%20prefect%20co-work%20of%20the%0Aneural%20proxy%20and%20the%20base%20engine%2C%20and%20the%20alignment%20with%20high-quality%20data.%20We%0Aintroduce%20an%20engine-oriented%20fine-tuning%20method%20that%20embeds%20the%20base%20engine%0Ainto%20the%20proxy.%20We%20then%20discuss%20the%20human-LLM%20collaborative%20design%20to%20produce%0Anovel%20and%20interesting%20data%20efficiently.%20Eventually%2C%20we%20propose%20three%20evaluation%0Aprinciples%20to%20comprehensively%20assess%20the%20performance%20of%20a%20delta%20engine%3A%20naive%0Aevaluation%2C%20incremental%20evaluation%2C%20and%20adversarial%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05842v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolving%2520Virtual%2520World%2520with%2520Delta-Engine%26entry.906535625%3DHongqiu%2520Wu%2520and%2520Zekai%2520Xu%2520and%2520Tianyang%2520Xu%2520and%2520Shize%2520Wei%2520and%2520Yan%2520Wang%2520and%2520Jiale%2520Hong%2520and%2520Weiqi%2520Wu%2520and%2520Hai%2520Zhao%2520and%2520Min%2520Zhang%2520and%2520Zhezhi%2520He%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%2520%255Cemph%257Bvirtual%2520world%257D%252C%2520a%2520cyberspace%2520where%250Apeople%2520can%2520live%2520in.%2520An%2520ideal%2520virtual%2520world%2520shares%2520great%2520similarity%2520with%2520our%250Areal%2520world.%2520One%2520of%2520the%2520crucial%2520aspects%2520is%2520its%2520evolving%2520nature%252C%2520reflected%2520by%250Aindividuals%2527%2520capability%2520to%2520grow%2520and%2520thereby%2520influence%2520the%2520objective%2520world.%2520Such%250Adynamics%2520is%2520unpredictable%2520and%2520beyond%2520the%2520reach%2520of%2520existing%2520systems.%2520For%2520this%252C%250Awe%2520propose%2520a%2520special%2520engine%2520called%2520%255Ctextbf%257B%255Cemph%257BDelta-Engine%257D%257D%2520to%2520drive%2520this%250Avirtual%2520world.%2520%2524%255CDelta%2524%2520associates%2520the%2520world%2527s%2520evolution%2520to%2520the%2520engine%2527s%250Ascalability.%2520It%2520consists%2520of%2520a%2520base%2520engine%2520and%2520a%2520neural%2520proxy.%2520The%2520base%2520engine%250Aprograms%2520the%2520prototype%2520of%2520the%2520virtual%2520world%253B%2520given%2520a%2520trigger%252C%2520the%2520neural%2520proxy%250Agenerates%2520new%2520snippets%2520on%2520the%2520base%2520engine%2520through%2520%255Cemph%257Bincremental%250Aprediction%257D.%2520This%2520paper%2520presents%2520a%2520full-stack%2520introduction%2520to%2520the%2520delta-engine.%250AThe%2520key%2520feature%2520of%2520the%2520delta-engine%2520is%2520its%2520scalability%2520to%2520unknown%2520elements%250Awithin%2520the%2520world%252C%2520Technically%252C%2520it%2520derives%2520from%2520the%2520prefect%2520co-work%2520of%2520the%250Aneural%2520proxy%2520and%2520the%2520base%2520engine%252C%2520and%2520the%2520alignment%2520with%2520high-quality%2520data.%2520We%250Aintroduce%2520an%2520engine-oriented%2520fine-tuning%2520method%2520that%2520embeds%2520the%2520base%2520engine%250Ainto%2520the%2520proxy.%2520We%2520then%2520discuss%2520the%2520human-LLM%2520collaborative%2520design%2520to%2520produce%250Anovel%2520and%2520interesting%2520data%2520efficiently.%2520Eventually%252C%2520we%2520propose%2520three%2520evaluation%250Aprinciples%2520to%2520comprehensively%2520assess%2520the%2520performance%2520of%2520a%2520delta%2520engine%253A%2520naive%250Aevaluation%252C%2520incremental%2520evaluation%252C%2520and%2520adversarial%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05842v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolving%20Virtual%20World%20with%20Delta-Engine&entry.906535625=Hongqiu%20Wu%20and%20Zekai%20Xu%20and%20Tianyang%20Xu%20and%20Shize%20Wei%20and%20Yan%20Wang%20and%20Jiale%20Hong%20and%20Weiqi%20Wu%20and%20Hai%20Zhao%20and%20Min%20Zhang%20and%20Zhezhi%20He&entry.1292438233=%20%20In%20this%20paper%2C%20we%20focus%20on%20the%20%5Cemph%7Bvirtual%20world%7D%2C%20a%20cyberspace%20where%0Apeople%20can%20live%20in.%20An%20ideal%20virtual%20world%20shares%20great%20similarity%20with%20our%0Areal%20world.%20One%20of%20the%20crucial%20aspects%20is%20its%20evolving%20nature%2C%20reflected%20by%0Aindividuals%27%20capability%20to%20grow%20and%20thereby%20influence%20the%20objective%20world.%20Such%0Adynamics%20is%20unpredictable%20and%20beyond%20the%20reach%20of%20existing%20systems.%20For%20this%2C%0Awe%20propose%20a%20special%20engine%20called%20%5Ctextbf%7B%5Cemph%7BDelta-Engine%7D%7D%20to%20drive%20this%0Avirtual%20world.%20%24%5CDelta%24%20associates%20the%20world%27s%20evolution%20to%20the%20engine%27s%0Ascalability.%20It%20consists%20of%20a%20base%20engine%20and%20a%20neural%20proxy.%20The%20base%20engine%0Aprograms%20the%20prototype%20of%20the%20virtual%20world%3B%20given%20a%20trigger%2C%20the%20neural%20proxy%0Agenerates%20new%20snippets%20on%20the%20base%20engine%20through%20%5Cemph%7Bincremental%0Aprediction%7D.%20This%20paper%20presents%20a%20full-stack%20introduction%20to%20the%20delta-engine.%0AThe%20key%20feature%20of%20the%20delta-engine%20is%20its%20scalability%20to%20unknown%20elements%0Awithin%20the%20world%2C%20Technically%2C%20it%20derives%20from%20the%20prefect%20co-work%20of%20the%0Aneural%20proxy%20and%20the%20base%20engine%2C%20and%20the%20alignment%20with%20high-quality%20data.%20We%0Aintroduce%20an%20engine-oriented%20fine-tuning%20method%20that%20embeds%20the%20base%20engine%0Ainto%20the%20proxy.%20We%20then%20discuss%20the%20human-LLM%20collaborative%20design%20to%20produce%0Anovel%20and%20interesting%20data%20efficiently.%20Eventually%2C%20we%20propose%20three%20evaluation%0Aprinciples%20to%20comprehensively%20assess%20the%20performance%20of%20a%20delta%20engine%3A%20naive%0Aevaluation%2C%20incremental%20evaluation%2C%20and%20adversarial%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05842v3&entry.124074799=Read"},
{"title": "Investigating Neuron Ablation in Attention Heads: The Case for Peak\n  Activation Centering", "author": "Nicholas Pochinkov and Ben Pasero and Skylar Shibayama", "abstract": "  The use of transformer-based models is growing rapidly throughout society.\nWith this growth, it is important to understand how they work, and in\nparticular, how the attention mechanisms represent concepts. Though there are\nmany interpretability methods, many look at models through their neuronal\nactivations, which are poorly understood. We describe different lenses through\nwhich to view neuron activations, and investigate the effectiveness in language\nmodels and vision transformers through various methods of neural ablation: zero\nablation, mean ablation, activation resampling, and a novel approach we term\n'peak ablation'. Through experimental analysis, we find that in different\nregimes and models, each method can offer the lowest degradation of model\nperformance compared to other methods, with resampling usually causing the most\nsignificant performance deterioration. We make our code available at\nhttps://github.com/nickypro/investigating-ablation.\n", "link": "http://arxiv.org/abs/2408.17322v1", "date": "2024-08-30", "relevancy": 1.9946, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5308}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5006}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Neuron%20Ablation%20in%20Attention%20Heads%3A%20The%20Case%20for%20Peak%0A%20%20Activation%20Centering&body=Title%3A%20Investigating%20Neuron%20Ablation%20in%20Attention%20Heads%3A%20The%20Case%20for%20Peak%0A%20%20Activation%20Centering%0AAuthor%3A%20Nicholas%20Pochinkov%20and%20Ben%20Pasero%20and%20Skylar%20Shibayama%0AAbstract%3A%20%20%20The%20use%20of%20transformer-based%20models%20is%20growing%20rapidly%20throughout%20society.%0AWith%20this%20growth%2C%20it%20is%20important%20to%20understand%20how%20they%20work%2C%20and%20in%0Aparticular%2C%20how%20the%20attention%20mechanisms%20represent%20concepts.%20Though%20there%20are%0Amany%20interpretability%20methods%2C%20many%20look%20at%20models%20through%20their%20neuronal%0Aactivations%2C%20which%20are%20poorly%20understood.%20We%20describe%20different%20lenses%20through%0Awhich%20to%20view%20neuron%20activations%2C%20and%20investigate%20the%20effectiveness%20in%20language%0Amodels%20and%20vision%20transformers%20through%20various%20methods%20of%20neural%20ablation%3A%20zero%0Aablation%2C%20mean%20ablation%2C%20activation%20resampling%2C%20and%20a%20novel%20approach%20we%20term%0A%27peak%20ablation%27.%20Through%20experimental%20analysis%2C%20we%20find%20that%20in%20different%0Aregimes%20and%20models%2C%20each%20method%20can%20offer%20the%20lowest%20degradation%20of%20model%0Aperformance%20compared%20to%20other%20methods%2C%20with%20resampling%20usually%20causing%20the%20most%0Asignificant%20performance%20deterioration.%20We%20make%20our%20code%20available%20at%0Ahttps%3A//github.com/nickypro/investigating-ablation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Neuron%2520Ablation%2520in%2520Attention%2520Heads%253A%2520The%2520Case%2520for%2520Peak%250A%2520%2520Activation%2520Centering%26entry.906535625%3DNicholas%2520Pochinkov%2520and%2520Ben%2520Pasero%2520and%2520Skylar%2520Shibayama%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520transformer-based%2520models%2520is%2520growing%2520rapidly%2520throughout%2520society.%250AWith%2520this%2520growth%252C%2520it%2520is%2520important%2520to%2520understand%2520how%2520they%2520work%252C%2520and%2520in%250Aparticular%252C%2520how%2520the%2520attention%2520mechanisms%2520represent%2520concepts.%2520Though%2520there%2520are%250Amany%2520interpretability%2520methods%252C%2520many%2520look%2520at%2520models%2520through%2520their%2520neuronal%250Aactivations%252C%2520which%2520are%2520poorly%2520understood.%2520We%2520describe%2520different%2520lenses%2520through%250Awhich%2520to%2520view%2520neuron%2520activations%252C%2520and%2520investigate%2520the%2520effectiveness%2520in%2520language%250Amodels%2520and%2520vision%2520transformers%2520through%2520various%2520methods%2520of%2520neural%2520ablation%253A%2520zero%250Aablation%252C%2520mean%2520ablation%252C%2520activation%2520resampling%252C%2520and%2520a%2520novel%2520approach%2520we%2520term%250A%2527peak%2520ablation%2527.%2520Through%2520experimental%2520analysis%252C%2520we%2520find%2520that%2520in%2520different%250Aregimes%2520and%2520models%252C%2520each%2520method%2520can%2520offer%2520the%2520lowest%2520degradation%2520of%2520model%250Aperformance%2520compared%2520to%2520other%2520methods%252C%2520with%2520resampling%2520usually%2520causing%2520the%2520most%250Asignificant%2520performance%2520deterioration.%2520We%2520make%2520our%2520code%2520available%2520at%250Ahttps%253A//github.com/nickypro/investigating-ablation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Neuron%20Ablation%20in%20Attention%20Heads%3A%20The%20Case%20for%20Peak%0A%20%20Activation%20Centering&entry.906535625=Nicholas%20Pochinkov%20and%20Ben%20Pasero%20and%20Skylar%20Shibayama&entry.1292438233=%20%20The%20use%20of%20transformer-based%20models%20is%20growing%20rapidly%20throughout%20society.%0AWith%20this%20growth%2C%20it%20is%20important%20to%20understand%20how%20they%20work%2C%20and%20in%0Aparticular%2C%20how%20the%20attention%20mechanisms%20represent%20concepts.%20Though%20there%20are%0Amany%20interpretability%20methods%2C%20many%20look%20at%20models%20through%20their%20neuronal%0Aactivations%2C%20which%20are%20poorly%20understood.%20We%20describe%20different%20lenses%20through%0Awhich%20to%20view%20neuron%20activations%2C%20and%20investigate%20the%20effectiveness%20in%20language%0Amodels%20and%20vision%20transformers%20through%20various%20methods%20of%20neural%20ablation%3A%20zero%0Aablation%2C%20mean%20ablation%2C%20activation%20resampling%2C%20and%20a%20novel%20approach%20we%20term%0A%27peak%20ablation%27.%20Through%20experimental%20analysis%2C%20we%20find%20that%20in%20different%0Aregimes%20and%20models%2C%20each%20method%20can%20offer%20the%20lowest%20degradation%20of%20model%0Aperformance%20compared%20to%20other%20methods%2C%20with%20resampling%20usually%20causing%20the%20most%0Asignificant%20performance%20deterioration.%20We%20make%20our%20code%20available%20at%0Ahttps%3A//github.com/nickypro/investigating-ablation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17322v1&entry.124074799=Read"},
{"title": "LSMS: Language-guided Scale-aware MedSegmentor for Medical Image\n  Referring Segmentation", "author": "Shuyi Ouyang and Jinyang Zhang and Xiangye Lin and Xilai Wang and Qingqing Chen and Yen-Wei Chen and Lanfen Lin", "abstract": "  Conventional medical image segmentation methods have been found inadequate in\nfacilitating physicians with the identification of specific lesions for\ndiagnosis and treatment. Given the utility of text as an instructional format,\nwe introduce a novel task termed Medical Image Referring Segmentation (MIRS),\nwhich requires segmenting specified lesions in images based on the given\nlanguage expressions. Due to the varying object scales in medical images, MIRS\ndemands robust vision-language modeling and comprehensive multi-scale\ninteraction for precise localization and segmentation under linguistic\nguidance. However, existing medical image segmentation methods fall short in\nmeeting these demands, resulting in insufficient segmentation accuracy. In\nresponse, we propose an approach named Language-guided Scale-aware MedSegmentor\n(LSMS), incorporating two appealing designs: (1)~a Scale-aware Vision-Language\nAttention module that leverages diverse convolutional kernels to acquire rich\nvisual knowledge and interact closely with linguistic features, thereby\nenhancing lesion localization capability; (2)~a Full-Scale Decoder that\nglobally models multi-modal features across various scales, capturing\ncomplementary information between scales to accurately outline lesion\nboundaries. Addressing the lack of suitable datasets for MIRS, we constructed a\nvision-language medical dataset called Reference Hepatic Lesion Segmentation\n(RefHL-Seg). This dataset comprises 2,283 abdominal CT slices from 231 cases,\nwith corresponding textual annotations and segmentation masks for various liver\nlesions in images. We validated the performance of LSMS for MIRS and\nconventional medical image segmentation tasks across various datasets. Our LSMS\nconsistently outperforms on all datasets with lower computational costs. The\ncode and datasets will be released.\n", "link": "http://arxiv.org/abs/2408.17347v1", "date": "2024-08-30", "relevancy": 1.9933, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5251}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5068}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LSMS%3A%20Language-guided%20Scale-aware%20MedSegmentor%20for%20Medical%20Image%0A%20%20Referring%20Segmentation&body=Title%3A%20LSMS%3A%20Language-guided%20Scale-aware%20MedSegmentor%20for%20Medical%20Image%0A%20%20Referring%20Segmentation%0AAuthor%3A%20Shuyi%20Ouyang%20and%20Jinyang%20Zhang%20and%20Xiangye%20Lin%20and%20Xilai%20Wang%20and%20Qingqing%20Chen%20and%20Yen-Wei%20Chen%20and%20Lanfen%20Lin%0AAbstract%3A%20%20%20Conventional%20medical%20image%20segmentation%20methods%20have%20been%20found%20inadequate%20in%0Afacilitating%20physicians%20with%20the%20identification%20of%20specific%20lesions%20for%0Adiagnosis%20and%20treatment.%20Given%20the%20utility%20of%20text%20as%20an%20instructional%20format%2C%0Awe%20introduce%20a%20novel%20task%20termed%20Medical%20Image%20Referring%20Segmentation%20%28MIRS%29%2C%0Awhich%20requires%20segmenting%20specified%20lesions%20in%20images%20based%20on%20the%20given%0Alanguage%20expressions.%20Due%20to%20the%20varying%20object%20scales%20in%20medical%20images%2C%20MIRS%0Ademands%20robust%20vision-language%20modeling%20and%20comprehensive%20multi-scale%0Ainteraction%20for%20precise%20localization%20and%20segmentation%20under%20linguistic%0Aguidance.%20However%2C%20existing%20medical%20image%20segmentation%20methods%20fall%20short%20in%0Ameeting%20these%20demands%2C%20resulting%20in%20insufficient%20segmentation%20accuracy.%20In%0Aresponse%2C%20we%20propose%20an%20approach%20named%20Language-guided%20Scale-aware%20MedSegmentor%0A%28LSMS%29%2C%20incorporating%20two%20appealing%20designs%3A%20%281%29~a%20Scale-aware%20Vision-Language%0AAttention%20module%20that%20leverages%20diverse%20convolutional%20kernels%20to%20acquire%20rich%0Avisual%20knowledge%20and%20interact%20closely%20with%20linguistic%20features%2C%20thereby%0Aenhancing%20lesion%20localization%20capability%3B%20%282%29~a%20Full-Scale%20Decoder%20that%0Aglobally%20models%20multi-modal%20features%20across%20various%20scales%2C%20capturing%0Acomplementary%20information%20between%20scales%20to%20accurately%20outline%20lesion%0Aboundaries.%20Addressing%20the%20lack%20of%20suitable%20datasets%20for%20MIRS%2C%20we%20constructed%20a%0Avision-language%20medical%20dataset%20called%20Reference%20Hepatic%20Lesion%20Segmentation%0A%28RefHL-Seg%29.%20This%20dataset%20comprises%202%2C283%20abdominal%20CT%20slices%20from%20231%20cases%2C%0Awith%20corresponding%20textual%20annotations%20and%20segmentation%20masks%20for%20various%20liver%0Alesions%20in%20images.%20We%20validated%20the%20performance%20of%20LSMS%20for%20MIRS%20and%0Aconventional%20medical%20image%20segmentation%20tasks%20across%20various%20datasets.%20Our%20LSMS%0Aconsistently%20outperforms%20on%20all%20datasets%20with%20lower%20computational%20costs.%20The%0Acode%20and%20datasets%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLSMS%253A%2520Language-guided%2520Scale-aware%2520MedSegmentor%2520for%2520Medical%2520Image%250A%2520%2520Referring%2520Segmentation%26entry.906535625%3DShuyi%2520Ouyang%2520and%2520Jinyang%2520Zhang%2520and%2520Xiangye%2520Lin%2520and%2520Xilai%2520Wang%2520and%2520Qingqing%2520Chen%2520and%2520Yen-Wei%2520Chen%2520and%2520Lanfen%2520Lin%26entry.1292438233%3D%2520%2520Conventional%2520medical%2520image%2520segmentation%2520methods%2520have%2520been%2520found%2520inadequate%2520in%250Afacilitating%2520physicians%2520with%2520the%2520identification%2520of%2520specific%2520lesions%2520for%250Adiagnosis%2520and%2520treatment.%2520Given%2520the%2520utility%2520of%2520text%2520as%2520an%2520instructional%2520format%252C%250Awe%2520introduce%2520a%2520novel%2520task%2520termed%2520Medical%2520Image%2520Referring%2520Segmentation%2520%2528MIRS%2529%252C%250Awhich%2520requires%2520segmenting%2520specified%2520lesions%2520in%2520images%2520based%2520on%2520the%2520given%250Alanguage%2520expressions.%2520Due%2520to%2520the%2520varying%2520object%2520scales%2520in%2520medical%2520images%252C%2520MIRS%250Ademands%2520robust%2520vision-language%2520modeling%2520and%2520comprehensive%2520multi-scale%250Ainteraction%2520for%2520precise%2520localization%2520and%2520segmentation%2520under%2520linguistic%250Aguidance.%2520However%252C%2520existing%2520medical%2520image%2520segmentation%2520methods%2520fall%2520short%2520in%250Ameeting%2520these%2520demands%252C%2520resulting%2520in%2520insufficient%2520segmentation%2520accuracy.%2520In%250Aresponse%252C%2520we%2520propose%2520an%2520approach%2520named%2520Language-guided%2520Scale-aware%2520MedSegmentor%250A%2528LSMS%2529%252C%2520incorporating%2520two%2520appealing%2520designs%253A%2520%25281%2529~a%2520Scale-aware%2520Vision-Language%250AAttention%2520module%2520that%2520leverages%2520diverse%2520convolutional%2520kernels%2520to%2520acquire%2520rich%250Avisual%2520knowledge%2520and%2520interact%2520closely%2520with%2520linguistic%2520features%252C%2520thereby%250Aenhancing%2520lesion%2520localization%2520capability%253B%2520%25282%2529~a%2520Full-Scale%2520Decoder%2520that%250Aglobally%2520models%2520multi-modal%2520features%2520across%2520various%2520scales%252C%2520capturing%250Acomplementary%2520information%2520between%2520scales%2520to%2520accurately%2520outline%2520lesion%250Aboundaries.%2520Addressing%2520the%2520lack%2520of%2520suitable%2520datasets%2520for%2520MIRS%252C%2520we%2520constructed%2520a%250Avision-language%2520medical%2520dataset%2520called%2520Reference%2520Hepatic%2520Lesion%2520Segmentation%250A%2528RefHL-Seg%2529.%2520This%2520dataset%2520comprises%25202%252C283%2520abdominal%2520CT%2520slices%2520from%2520231%2520cases%252C%250Awith%2520corresponding%2520textual%2520annotations%2520and%2520segmentation%2520masks%2520for%2520various%2520liver%250Alesions%2520in%2520images.%2520We%2520validated%2520the%2520performance%2520of%2520LSMS%2520for%2520MIRS%2520and%250Aconventional%2520medical%2520image%2520segmentation%2520tasks%2520across%2520various%2520datasets.%2520Our%2520LSMS%250Aconsistently%2520outperforms%2520on%2520all%2520datasets%2520with%2520lower%2520computational%2520costs.%2520The%250Acode%2520and%2520datasets%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LSMS%3A%20Language-guided%20Scale-aware%20MedSegmentor%20for%20Medical%20Image%0A%20%20Referring%20Segmentation&entry.906535625=Shuyi%20Ouyang%20and%20Jinyang%20Zhang%20and%20Xiangye%20Lin%20and%20Xilai%20Wang%20and%20Qingqing%20Chen%20and%20Yen-Wei%20Chen%20and%20Lanfen%20Lin&entry.1292438233=%20%20Conventional%20medical%20image%20segmentation%20methods%20have%20been%20found%20inadequate%20in%0Afacilitating%20physicians%20with%20the%20identification%20of%20specific%20lesions%20for%0Adiagnosis%20and%20treatment.%20Given%20the%20utility%20of%20text%20as%20an%20instructional%20format%2C%0Awe%20introduce%20a%20novel%20task%20termed%20Medical%20Image%20Referring%20Segmentation%20%28MIRS%29%2C%0Awhich%20requires%20segmenting%20specified%20lesions%20in%20images%20based%20on%20the%20given%0Alanguage%20expressions.%20Due%20to%20the%20varying%20object%20scales%20in%20medical%20images%2C%20MIRS%0Ademands%20robust%20vision-language%20modeling%20and%20comprehensive%20multi-scale%0Ainteraction%20for%20precise%20localization%20and%20segmentation%20under%20linguistic%0Aguidance.%20However%2C%20existing%20medical%20image%20segmentation%20methods%20fall%20short%20in%0Ameeting%20these%20demands%2C%20resulting%20in%20insufficient%20segmentation%20accuracy.%20In%0Aresponse%2C%20we%20propose%20an%20approach%20named%20Language-guided%20Scale-aware%20MedSegmentor%0A%28LSMS%29%2C%20incorporating%20two%20appealing%20designs%3A%20%281%29~a%20Scale-aware%20Vision-Language%0AAttention%20module%20that%20leverages%20diverse%20convolutional%20kernels%20to%20acquire%20rich%0Avisual%20knowledge%20and%20interact%20closely%20with%20linguistic%20features%2C%20thereby%0Aenhancing%20lesion%20localization%20capability%3B%20%282%29~a%20Full-Scale%20Decoder%20that%0Aglobally%20models%20multi-modal%20features%20across%20various%20scales%2C%20capturing%0Acomplementary%20information%20between%20scales%20to%20accurately%20outline%20lesion%0Aboundaries.%20Addressing%20the%20lack%20of%20suitable%20datasets%20for%20MIRS%2C%20we%20constructed%20a%0Avision-language%20medical%20dataset%20called%20Reference%20Hepatic%20Lesion%20Segmentation%0A%28RefHL-Seg%29.%20This%20dataset%20comprises%202%2C283%20abdominal%20CT%20slices%20from%20231%20cases%2C%0Awith%20corresponding%20textual%20annotations%20and%20segmentation%20masks%20for%20various%20liver%0Alesions%20in%20images.%20We%20validated%20the%20performance%20of%20LSMS%20for%20MIRS%20and%0Aconventional%20medical%20image%20segmentation%20tasks%20across%20various%20datasets.%20Our%20LSMS%0Aconsistently%20outperforms%20on%20all%20datasets%20with%20lower%20computational%20costs.%20The%0Acode%20and%20datasets%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17347v1&entry.124074799=Read"},
{"title": "Learning Dynamic Bayesian Networks from Data: Foundations, First\n  Principles and Numerical Comparisons", "author": "Vyacheslav Kungurtsev and Fadwa Idlahcen and Petr Rysavy and Pavel Rytir and Ales Wodecki", "abstract": "  In this paper, we present a guide to the foundations of learning Dynamic\nBayesian Networks (DBNs) from data in the form of multiple samples of\ntrajectories for some length of time. We present the formalism for a generic as\nwell as a set of common types of DBNs for particular variable distributions. We\npresent the analytical form of the models, with a comprehensive discussion on\nthe interdependence between structure and weights in a DBN model and their\nimplications for learning. Next, we give a broad overview of learning methods\nand describe and categorize them based on the most important statistical\nfeatures, and how they treat the interplay between learning structure and\nweights. We give the analytical form of the likelihood and Bayesian score\nfunctions, emphasizing the distinction from the static case. We discuss\nfunctions used in optimization to enforce structural requirements. We briefly\ndiscuss more complex extensions and representations. Finally we present a set\nof comparisons in different settings for various distinct but representative\nalgorithms across the variants.\n", "link": "http://arxiv.org/abs/2406.17585v2", "date": "2024-08-30", "relevancy": 1.9699, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5293}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5164}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Dynamic%20Bayesian%20Networks%20from%20Data%3A%20Foundations%2C%20First%0A%20%20Principles%20and%20Numerical%20Comparisons&body=Title%3A%20Learning%20Dynamic%20Bayesian%20Networks%20from%20Data%3A%20Foundations%2C%20First%0A%20%20Principles%20and%20Numerical%20Comparisons%0AAuthor%3A%20Vyacheslav%20Kungurtsev%20and%20Fadwa%20Idlahcen%20and%20Petr%20Rysavy%20and%20Pavel%20Rytir%20and%20Ales%20Wodecki%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20guide%20to%20the%20foundations%20of%20learning%20Dynamic%0ABayesian%20Networks%20%28DBNs%29%20from%20data%20in%20the%20form%20of%20multiple%20samples%20of%0Atrajectories%20for%20some%20length%20of%20time.%20We%20present%20the%20formalism%20for%20a%20generic%20as%0Awell%20as%20a%20set%20of%20common%20types%20of%20DBNs%20for%20particular%20variable%20distributions.%20We%0Apresent%20the%20analytical%20form%20of%20the%20models%2C%20with%20a%20comprehensive%20discussion%20on%0Athe%20interdependence%20between%20structure%20and%20weights%20in%20a%20DBN%20model%20and%20their%0Aimplications%20for%20learning.%20Next%2C%20we%20give%20a%20broad%20overview%20of%20learning%20methods%0Aand%20describe%20and%20categorize%20them%20based%20on%20the%20most%20important%20statistical%0Afeatures%2C%20and%20how%20they%20treat%20the%20interplay%20between%20learning%20structure%20and%0Aweights.%20We%20give%20the%20analytical%20form%20of%20the%20likelihood%20and%20Bayesian%20score%0Afunctions%2C%20emphasizing%20the%20distinction%20from%20the%20static%20case.%20We%20discuss%0Afunctions%20used%20in%20optimization%20to%20enforce%20structural%20requirements.%20We%20briefly%0Adiscuss%20more%20complex%20extensions%20and%20representations.%20Finally%20we%20present%20a%20set%0Aof%20comparisons%20in%20different%20settings%20for%20various%20distinct%20but%20representative%0Aalgorithms%20across%20the%20variants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17585v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Dynamic%2520Bayesian%2520Networks%2520from%2520Data%253A%2520Foundations%252C%2520First%250A%2520%2520Principles%2520and%2520Numerical%2520Comparisons%26entry.906535625%3DVyacheslav%2520Kungurtsev%2520and%2520Fadwa%2520Idlahcen%2520and%2520Petr%2520Rysavy%2520and%2520Pavel%2520Rytir%2520and%2520Ales%2520Wodecki%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520guide%2520to%2520the%2520foundations%2520of%2520learning%2520Dynamic%250ABayesian%2520Networks%2520%2528DBNs%2529%2520from%2520data%2520in%2520the%2520form%2520of%2520multiple%2520samples%2520of%250Atrajectories%2520for%2520some%2520length%2520of%2520time.%2520We%2520present%2520the%2520formalism%2520for%2520a%2520generic%2520as%250Awell%2520as%2520a%2520set%2520of%2520common%2520types%2520of%2520DBNs%2520for%2520particular%2520variable%2520distributions.%2520We%250Apresent%2520the%2520analytical%2520form%2520of%2520the%2520models%252C%2520with%2520a%2520comprehensive%2520discussion%2520on%250Athe%2520interdependence%2520between%2520structure%2520and%2520weights%2520in%2520a%2520DBN%2520model%2520and%2520their%250Aimplications%2520for%2520learning.%2520Next%252C%2520we%2520give%2520a%2520broad%2520overview%2520of%2520learning%2520methods%250Aand%2520describe%2520and%2520categorize%2520them%2520based%2520on%2520the%2520most%2520important%2520statistical%250Afeatures%252C%2520and%2520how%2520they%2520treat%2520the%2520interplay%2520between%2520learning%2520structure%2520and%250Aweights.%2520We%2520give%2520the%2520analytical%2520form%2520of%2520the%2520likelihood%2520and%2520Bayesian%2520score%250Afunctions%252C%2520emphasizing%2520the%2520distinction%2520from%2520the%2520static%2520case.%2520We%2520discuss%250Afunctions%2520used%2520in%2520optimization%2520to%2520enforce%2520structural%2520requirements.%2520We%2520briefly%250Adiscuss%2520more%2520complex%2520extensions%2520and%2520representations.%2520Finally%2520we%2520present%2520a%2520set%250Aof%2520comparisons%2520in%2520different%2520settings%2520for%2520various%2520distinct%2520but%2520representative%250Aalgorithms%2520across%2520the%2520variants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17585v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Dynamic%20Bayesian%20Networks%20from%20Data%3A%20Foundations%2C%20First%0A%20%20Principles%20and%20Numerical%20Comparisons&entry.906535625=Vyacheslav%20Kungurtsev%20and%20Fadwa%20Idlahcen%20and%20Petr%20Rysavy%20and%20Pavel%20Rytir%20and%20Ales%20Wodecki&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20guide%20to%20the%20foundations%20of%20learning%20Dynamic%0ABayesian%20Networks%20%28DBNs%29%20from%20data%20in%20the%20form%20of%20multiple%20samples%20of%0Atrajectories%20for%20some%20length%20of%20time.%20We%20present%20the%20formalism%20for%20a%20generic%20as%0Awell%20as%20a%20set%20of%20common%20types%20of%20DBNs%20for%20particular%20variable%20distributions.%20We%0Apresent%20the%20analytical%20form%20of%20the%20models%2C%20with%20a%20comprehensive%20discussion%20on%0Athe%20interdependence%20between%20structure%20and%20weights%20in%20a%20DBN%20model%20and%20their%0Aimplications%20for%20learning.%20Next%2C%20we%20give%20a%20broad%20overview%20of%20learning%20methods%0Aand%20describe%20and%20categorize%20them%20based%20on%20the%20most%20important%20statistical%0Afeatures%2C%20and%20how%20they%20treat%20the%20interplay%20between%20learning%20structure%20and%0Aweights.%20We%20give%20the%20analytical%20form%20of%20the%20likelihood%20and%20Bayesian%20score%0Afunctions%2C%20emphasizing%20the%20distinction%20from%20the%20static%20case.%20We%20discuss%0Afunctions%20used%20in%20optimization%20to%20enforce%20structural%20requirements.%20We%20briefly%0Adiscuss%20more%20complex%20extensions%20and%20representations.%20Finally%20we%20present%20a%20set%0Aof%20comparisons%20in%20different%20settings%20for%20various%20distinct%20but%20representative%0Aalgorithms%20across%20the%20variants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17585v2&entry.124074799=Read"},
{"title": "Efficient Testable Learning of General Halfspaces with Adversarial Label\n  Noise", "author": "Ilias Diakonikolas and Daniel M. Kane and Sihan Liu and Nikos Zarifis", "abstract": "  We study the task of testable learning of general -- not necessarily\nhomogeneous -- halfspaces with adversarial label noise with respect to the\nGaussian distribution. In the testable learning framework, the goal is to\ndevelop a tester-learner such that if the data passes the tester, then one can\ntrust the output of the robust learner on the data.Our main result is the first\npolynomial time tester-learner for general halfspaces that achieves\ndimension-independent misclassification error. At the heart of our approach is\na new methodology to reduce testable learning of general halfspaces to testable\nlearning of nearly homogeneous halfspaces that may be of broader interest.\n", "link": "http://arxiv.org/abs/2408.17165v1", "date": "2024-08-30", "relevancy": 1.9569, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.504}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.479}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Testable%20Learning%20of%20General%20Halfspaces%20with%20Adversarial%20Label%0A%20%20Noise&body=Title%3A%20Efficient%20Testable%20Learning%20of%20General%20Halfspaces%20with%20Adversarial%20Label%0A%20%20Noise%0AAuthor%3A%20Ilias%20Diakonikolas%20and%20Daniel%20M.%20Kane%20and%20Sihan%20Liu%20and%20Nikos%20Zarifis%0AAbstract%3A%20%20%20We%20study%20the%20task%20of%20testable%20learning%20of%20general%20--%20not%20necessarily%0Ahomogeneous%20--%20halfspaces%20with%20adversarial%20label%20noise%20with%20respect%20to%20the%0AGaussian%20distribution.%20In%20the%20testable%20learning%20framework%2C%20the%20goal%20is%20to%0Adevelop%20a%20tester-learner%20such%20that%20if%20the%20data%20passes%20the%20tester%2C%20then%20one%20can%0Atrust%20the%20output%20of%20the%20robust%20learner%20on%20the%20data.Our%20main%20result%20is%20the%20first%0Apolynomial%20time%20tester-learner%20for%20general%20halfspaces%20that%20achieves%0Adimension-independent%20misclassification%20error.%20At%20the%20heart%20of%20our%20approach%20is%0Aa%20new%20methodology%20to%20reduce%20testable%20learning%20of%20general%20halfspaces%20to%20testable%0Alearning%20of%20nearly%20homogeneous%20halfspaces%20that%20may%20be%20of%20broader%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Testable%2520Learning%2520of%2520General%2520Halfspaces%2520with%2520Adversarial%2520Label%250A%2520%2520Noise%26entry.906535625%3DIlias%2520Diakonikolas%2520and%2520Daniel%2520M.%2520Kane%2520and%2520Sihan%2520Liu%2520and%2520Nikos%2520Zarifis%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520task%2520of%2520testable%2520learning%2520of%2520general%2520--%2520not%2520necessarily%250Ahomogeneous%2520--%2520halfspaces%2520with%2520adversarial%2520label%2520noise%2520with%2520respect%2520to%2520the%250AGaussian%2520distribution.%2520In%2520the%2520testable%2520learning%2520framework%252C%2520the%2520goal%2520is%2520to%250Adevelop%2520a%2520tester-learner%2520such%2520that%2520if%2520the%2520data%2520passes%2520the%2520tester%252C%2520then%2520one%2520can%250Atrust%2520the%2520output%2520of%2520the%2520robust%2520learner%2520on%2520the%2520data.Our%2520main%2520result%2520is%2520the%2520first%250Apolynomial%2520time%2520tester-learner%2520for%2520general%2520halfspaces%2520that%2520achieves%250Adimension-independent%2520misclassification%2520error.%2520At%2520the%2520heart%2520of%2520our%2520approach%2520is%250Aa%2520new%2520methodology%2520to%2520reduce%2520testable%2520learning%2520of%2520general%2520halfspaces%2520to%2520testable%250Alearning%2520of%2520nearly%2520homogeneous%2520halfspaces%2520that%2520may%2520be%2520of%2520broader%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Testable%20Learning%20of%20General%20Halfspaces%20with%20Adversarial%20Label%0A%20%20Noise&entry.906535625=Ilias%20Diakonikolas%20and%20Daniel%20M.%20Kane%20and%20Sihan%20Liu%20and%20Nikos%20Zarifis&entry.1292438233=%20%20We%20study%20the%20task%20of%20testable%20learning%20of%20general%20--%20not%20necessarily%0Ahomogeneous%20--%20halfspaces%20with%20adversarial%20label%20noise%20with%20respect%20to%20the%0AGaussian%20distribution.%20In%20the%20testable%20learning%20framework%2C%20the%20goal%20is%20to%0Adevelop%20a%20tester-learner%20such%20that%20if%20the%20data%20passes%20the%20tester%2C%20then%20one%20can%0Atrust%20the%20output%20of%20the%20robust%20learner%20on%20the%20data.Our%20main%20result%20is%20the%20first%0Apolynomial%20time%20tester-learner%20for%20general%20halfspaces%20that%20achieves%0Adimension-independent%20misclassification%20error.%20At%20the%20heart%20of%20our%20approach%20is%0Aa%20new%20methodology%20to%20reduce%20testable%20learning%20of%20general%20halfspaces%20to%20testable%0Alearning%20of%20nearly%20homogeneous%20halfspaces%20that%20may%20be%20of%20broader%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17165v1&entry.124074799=Read"},
{"title": "Novel Methods for Analyzing Cellular Interactions in Deep Learning-Based\n  Image Cytometry: Spatial Interaction Potential and Co-Localization Index", "author": "Toru Nagasaka and Kimihiro Yamashita and Mitsugu Fujita", "abstract": "  The study presents a novel approach for quantifying cellular interactions in\ndigital pathology using deep learning-based image cytometry. Traditional\nmethods struggle with the diversity and heterogeneity of cells within tissues.\nTo address this, we introduce the Spatial Interaction Potential (SIP) and the\nCo-Localization Index (CLI), leveraging deep learning classification\nprobabilities. SIP assesses the potential for cell-to-cell interactions,\nsimilar to an electric field, while CLI incorporates distances between cells,\naccounting for dynamic cell movements. Our approach enhances traditional\nmethods, providing a more sophisticated analysis of cellular interactions. We\nvalidate SIP and CLI through simulations and apply them to colorectal cancer\nspecimens, demonstrating strong correlations with actual biological data. This\ninnovative method offers significant improvements in understanding cellular\ninteractions and has potential applications in various fields of digital\npathology.\n", "link": "http://arxiv.org/abs/2408.16008v2", "date": "2024-08-30", "relevancy": 1.956, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5026}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.487}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20Methods%20for%20Analyzing%20Cellular%20Interactions%20in%20Deep%20Learning-Based%0A%20%20Image%20Cytometry%3A%20Spatial%20Interaction%20Potential%20and%20Co-Localization%20Index&body=Title%3A%20Novel%20Methods%20for%20Analyzing%20Cellular%20Interactions%20in%20Deep%20Learning-Based%0A%20%20Image%20Cytometry%3A%20Spatial%20Interaction%20Potential%20and%20Co-Localization%20Index%0AAuthor%3A%20Toru%20Nagasaka%20and%20Kimihiro%20Yamashita%20and%20Mitsugu%20Fujita%0AAbstract%3A%20%20%20The%20study%20presents%20a%20novel%20approach%20for%20quantifying%20cellular%20interactions%20in%0Adigital%20pathology%20using%20deep%20learning-based%20image%20cytometry.%20Traditional%0Amethods%20struggle%20with%20the%20diversity%20and%20heterogeneity%20of%20cells%20within%20tissues.%0ATo%20address%20this%2C%20we%20introduce%20the%20Spatial%20Interaction%20Potential%20%28SIP%29%20and%20the%0ACo-Localization%20Index%20%28CLI%29%2C%20leveraging%20deep%20learning%20classification%0Aprobabilities.%20SIP%20assesses%20the%20potential%20for%20cell-to-cell%20interactions%2C%0Asimilar%20to%20an%20electric%20field%2C%20while%20CLI%20incorporates%20distances%20between%20cells%2C%0Aaccounting%20for%20dynamic%20cell%20movements.%20Our%20approach%20enhances%20traditional%0Amethods%2C%20providing%20a%20more%20sophisticated%20analysis%20of%20cellular%20interactions.%20We%0Avalidate%20SIP%20and%20CLI%20through%20simulations%20and%20apply%20them%20to%20colorectal%20cancer%0Aspecimens%2C%20demonstrating%20strong%20correlations%20with%20actual%20biological%20data.%20This%0Ainnovative%20method%20offers%20significant%20improvements%20in%20understanding%20cellular%0Ainteractions%20and%20has%20potential%20applications%20in%20various%20fields%20of%20digital%0Apathology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16008v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520Methods%2520for%2520Analyzing%2520Cellular%2520Interactions%2520in%2520Deep%2520Learning-Based%250A%2520%2520Image%2520Cytometry%253A%2520Spatial%2520Interaction%2520Potential%2520and%2520Co-Localization%2520Index%26entry.906535625%3DToru%2520Nagasaka%2520and%2520Kimihiro%2520Yamashita%2520and%2520Mitsugu%2520Fujita%26entry.1292438233%3D%2520%2520The%2520study%2520presents%2520a%2520novel%2520approach%2520for%2520quantifying%2520cellular%2520interactions%2520in%250Adigital%2520pathology%2520using%2520deep%2520learning-based%2520image%2520cytometry.%2520Traditional%250Amethods%2520struggle%2520with%2520the%2520diversity%2520and%2520heterogeneity%2520of%2520cells%2520within%2520tissues.%250ATo%2520address%2520this%252C%2520we%2520introduce%2520the%2520Spatial%2520Interaction%2520Potential%2520%2528SIP%2529%2520and%2520the%250ACo-Localization%2520Index%2520%2528CLI%2529%252C%2520leveraging%2520deep%2520learning%2520classification%250Aprobabilities.%2520SIP%2520assesses%2520the%2520potential%2520for%2520cell-to-cell%2520interactions%252C%250Asimilar%2520to%2520an%2520electric%2520field%252C%2520while%2520CLI%2520incorporates%2520distances%2520between%2520cells%252C%250Aaccounting%2520for%2520dynamic%2520cell%2520movements.%2520Our%2520approach%2520enhances%2520traditional%250Amethods%252C%2520providing%2520a%2520more%2520sophisticated%2520analysis%2520of%2520cellular%2520interactions.%2520We%250Avalidate%2520SIP%2520and%2520CLI%2520through%2520simulations%2520and%2520apply%2520them%2520to%2520colorectal%2520cancer%250Aspecimens%252C%2520demonstrating%2520strong%2520correlations%2520with%2520actual%2520biological%2520data.%2520This%250Ainnovative%2520method%2520offers%2520significant%2520improvements%2520in%2520understanding%2520cellular%250Ainteractions%2520and%2520has%2520potential%2520applications%2520in%2520various%2520fields%2520of%2520digital%250Apathology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16008v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20Methods%20for%20Analyzing%20Cellular%20Interactions%20in%20Deep%20Learning-Based%0A%20%20Image%20Cytometry%3A%20Spatial%20Interaction%20Potential%20and%20Co-Localization%20Index&entry.906535625=Toru%20Nagasaka%20and%20Kimihiro%20Yamashita%20and%20Mitsugu%20Fujita&entry.1292438233=%20%20The%20study%20presents%20a%20novel%20approach%20for%20quantifying%20cellular%20interactions%20in%0Adigital%20pathology%20using%20deep%20learning-based%20image%20cytometry.%20Traditional%0Amethods%20struggle%20with%20the%20diversity%20and%20heterogeneity%20of%20cells%20within%20tissues.%0ATo%20address%20this%2C%20we%20introduce%20the%20Spatial%20Interaction%20Potential%20%28SIP%29%20and%20the%0ACo-Localization%20Index%20%28CLI%29%2C%20leveraging%20deep%20learning%20classification%0Aprobabilities.%20SIP%20assesses%20the%20potential%20for%20cell-to-cell%20interactions%2C%0Asimilar%20to%20an%20electric%20field%2C%20while%20CLI%20incorporates%20distances%20between%20cells%2C%0Aaccounting%20for%20dynamic%20cell%20movements.%20Our%20approach%20enhances%20traditional%0Amethods%2C%20providing%20a%20more%20sophisticated%20analysis%20of%20cellular%20interactions.%20We%0Avalidate%20SIP%20and%20CLI%20through%20simulations%20and%20apply%20them%20to%20colorectal%20cancer%0Aspecimens%2C%20demonstrating%20strong%20correlations%20with%20actual%20biological%20data.%20This%0Ainnovative%20method%20offers%20significant%20improvements%20in%20understanding%20cellular%0Ainteractions%20and%20has%20potential%20applications%20in%20various%20fields%20of%20digital%0Apathology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16008v2&entry.124074799=Read"},
{"title": "Diversifying the Mixture-of-Experts Representation for Language Models\n  with Orthogonal Optimizer", "author": "Boan Liu and Liang Ding and Li Shen and Keqin Peng and Yu Cao and Dazhao Cheng and Dacheng Tao", "abstract": "  The Mixture of Experts (MoE) has emerged as a highly successful technique in\ndeep learning, based on the principle of divide-and-conquer to maximize model\ncapacity without significant additional computational cost. Even in the era of\nlarge-scale language models (LLMs), MoE continues to play a crucial role, as\nsome researchers have indicated that GPT-4 adopts the MoE structure to ensure\ndiverse inference results. However, MoE is susceptible to performance\ndegeneracy, particularly evident in the issues of imbalance and homogeneous\nrepresentation among experts. While previous studies have extensively addressed\nthe problem of imbalance, the challenge of homogeneous representation remains\nunresolved. In this study, we shed light on the homogeneous representation\nproblem, wherein experts in the MoE fail to specialize and lack diversity,\nleading to frustratingly high similarities in their representations (up to 99\\%\nin a well-performed MoE model). This problem restricts the expressive power of\nthe MoE and, we argue, contradicts its original intention. To tackle this\nissue, we propose a straightforward yet highly effective solution: OMoE, an\northogonal expert optimizer. Additionally, we introduce an alternating training\nstrategy that encourages each expert to update in a direction orthogonal to the\nsubspace spanned by other experts. Our algorithm facilitates MoE training in\ntwo key ways: firstly, it explicitly enhances representation diversity, and\nsecondly, it implicitly fosters interaction between experts during orthogonal\nweights computation. Through extensive experiments, we demonstrate that our\nproposed optimization algorithm significantly improves the performance of\nfine-tuning the MoE model on the GLUE benchmark, SuperGLUE benchmark,\nquestion-answering task, and name entity recognition tasks.\n", "link": "http://arxiv.org/abs/2310.09762v2", "date": "2024-08-30", "relevancy": 1.9559, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4923}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diversifying%20the%20Mixture-of-Experts%20Representation%20for%20Language%20Models%0A%20%20with%20Orthogonal%20Optimizer&body=Title%3A%20Diversifying%20the%20Mixture-of-Experts%20Representation%20for%20Language%20Models%0A%20%20with%20Orthogonal%20Optimizer%0AAuthor%3A%20Boan%20Liu%20and%20Liang%20Ding%20and%20Li%20Shen%20and%20Keqin%20Peng%20and%20Yu%20Cao%20and%20Dazhao%20Cheng%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20The%20Mixture%20of%20Experts%20%28MoE%29%20has%20emerged%20as%20a%20highly%20successful%20technique%20in%0Adeep%20learning%2C%20based%20on%20the%20principle%20of%20divide-and-conquer%20to%20maximize%20model%0Acapacity%20without%20significant%20additional%20computational%20cost.%20Even%20in%20the%20era%20of%0Alarge-scale%20language%20models%20%28LLMs%29%2C%20MoE%20continues%20to%20play%20a%20crucial%20role%2C%20as%0Asome%20researchers%20have%20indicated%20that%20GPT-4%20adopts%20the%20MoE%20structure%20to%20ensure%0Adiverse%20inference%20results.%20However%2C%20MoE%20is%20susceptible%20to%20performance%0Adegeneracy%2C%20particularly%20evident%20in%20the%20issues%20of%20imbalance%20and%20homogeneous%0Arepresentation%20among%20experts.%20While%20previous%20studies%20have%20extensively%20addressed%0Athe%20problem%20of%20imbalance%2C%20the%20challenge%20of%20homogeneous%20representation%20remains%0Aunresolved.%20In%20this%20study%2C%20we%20shed%20light%20on%20the%20homogeneous%20representation%0Aproblem%2C%20wherein%20experts%20in%20the%20MoE%20fail%20to%20specialize%20and%20lack%20diversity%2C%0Aleading%20to%20frustratingly%20high%20similarities%20in%20their%20representations%20%28up%20to%2099%5C%25%0Ain%20a%20well-performed%20MoE%20model%29.%20This%20problem%20restricts%20the%20expressive%20power%20of%0Athe%20MoE%20and%2C%20we%20argue%2C%20contradicts%20its%20original%20intention.%20To%20tackle%20this%0Aissue%2C%20we%20propose%20a%20straightforward%20yet%20highly%20effective%20solution%3A%20OMoE%2C%20an%0Aorthogonal%20expert%20optimizer.%20Additionally%2C%20we%20introduce%20an%20alternating%20training%0Astrategy%20that%20encourages%20each%20expert%20to%20update%20in%20a%20direction%20orthogonal%20to%20the%0Asubspace%20spanned%20by%20other%20experts.%20Our%20algorithm%20facilitates%20MoE%20training%20in%0Atwo%20key%20ways%3A%20firstly%2C%20it%20explicitly%20enhances%20representation%20diversity%2C%20and%0Asecondly%2C%20it%20implicitly%20fosters%20interaction%20between%20experts%20during%20orthogonal%0Aweights%20computation.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%0Aproposed%20optimization%20algorithm%20significantly%20improves%20the%20performance%20of%0Afine-tuning%20the%20MoE%20model%20on%20the%20GLUE%20benchmark%2C%20SuperGLUE%20benchmark%2C%0Aquestion-answering%20task%2C%20and%20name%20entity%20recognition%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiversifying%2520the%2520Mixture-of-Experts%2520Representation%2520for%2520Language%2520Models%250A%2520%2520with%2520Orthogonal%2520Optimizer%26entry.906535625%3DBoan%2520Liu%2520and%2520Liang%2520Ding%2520and%2520Li%2520Shen%2520and%2520Keqin%2520Peng%2520and%2520Yu%2520Cao%2520and%2520Dazhao%2520Cheng%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520The%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520has%2520emerged%2520as%2520a%2520highly%2520successful%2520technique%2520in%250Adeep%2520learning%252C%2520based%2520on%2520the%2520principle%2520of%2520divide-and-conquer%2520to%2520maximize%2520model%250Acapacity%2520without%2520significant%2520additional%2520computational%2520cost.%2520Even%2520in%2520the%2520era%2520of%250Alarge-scale%2520language%2520models%2520%2528LLMs%2529%252C%2520MoE%2520continues%2520to%2520play%2520a%2520crucial%2520role%252C%2520as%250Asome%2520researchers%2520have%2520indicated%2520that%2520GPT-4%2520adopts%2520the%2520MoE%2520structure%2520to%2520ensure%250Adiverse%2520inference%2520results.%2520However%252C%2520MoE%2520is%2520susceptible%2520to%2520performance%250Adegeneracy%252C%2520particularly%2520evident%2520in%2520the%2520issues%2520of%2520imbalance%2520and%2520homogeneous%250Arepresentation%2520among%2520experts.%2520While%2520previous%2520studies%2520have%2520extensively%2520addressed%250Athe%2520problem%2520of%2520imbalance%252C%2520the%2520challenge%2520of%2520homogeneous%2520representation%2520remains%250Aunresolved.%2520In%2520this%2520study%252C%2520we%2520shed%2520light%2520on%2520the%2520homogeneous%2520representation%250Aproblem%252C%2520wherein%2520experts%2520in%2520the%2520MoE%2520fail%2520to%2520specialize%2520and%2520lack%2520diversity%252C%250Aleading%2520to%2520frustratingly%2520high%2520similarities%2520in%2520their%2520representations%2520%2528up%2520to%252099%255C%2525%250Ain%2520a%2520well-performed%2520MoE%2520model%2529.%2520This%2520problem%2520restricts%2520the%2520expressive%2520power%2520of%250Athe%2520MoE%2520and%252C%2520we%2520argue%252C%2520contradicts%2520its%2520original%2520intention.%2520To%2520tackle%2520this%250Aissue%252C%2520we%2520propose%2520a%2520straightforward%2520yet%2520highly%2520effective%2520solution%253A%2520OMoE%252C%2520an%250Aorthogonal%2520expert%2520optimizer.%2520Additionally%252C%2520we%2520introduce%2520an%2520alternating%2520training%250Astrategy%2520that%2520encourages%2520each%2520expert%2520to%2520update%2520in%2520a%2520direction%2520orthogonal%2520to%2520the%250Asubspace%2520spanned%2520by%2520other%2520experts.%2520Our%2520algorithm%2520facilitates%2520MoE%2520training%2520in%250Atwo%2520key%2520ways%253A%2520firstly%252C%2520it%2520explicitly%2520enhances%2520representation%2520diversity%252C%2520and%250Asecondly%252C%2520it%2520implicitly%2520fosters%2520interaction%2520between%2520experts%2520during%2520orthogonal%250Aweights%2520computation.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%250Aproposed%2520optimization%2520algorithm%2520significantly%2520improves%2520the%2520performance%2520of%250Afine-tuning%2520the%2520MoE%2520model%2520on%2520the%2520GLUE%2520benchmark%252C%2520SuperGLUE%2520benchmark%252C%250Aquestion-answering%2520task%252C%2520and%2520name%2520entity%2520recognition%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.09762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diversifying%20the%20Mixture-of-Experts%20Representation%20for%20Language%20Models%0A%20%20with%20Orthogonal%20Optimizer&entry.906535625=Boan%20Liu%20and%20Liang%20Ding%20and%20Li%20Shen%20and%20Keqin%20Peng%20and%20Yu%20Cao%20and%20Dazhao%20Cheng%20and%20Dacheng%20Tao&entry.1292438233=%20%20The%20Mixture%20of%20Experts%20%28MoE%29%20has%20emerged%20as%20a%20highly%20successful%20technique%20in%0Adeep%20learning%2C%20based%20on%20the%20principle%20of%20divide-and-conquer%20to%20maximize%20model%0Acapacity%20without%20significant%20additional%20computational%20cost.%20Even%20in%20the%20era%20of%0Alarge-scale%20language%20models%20%28LLMs%29%2C%20MoE%20continues%20to%20play%20a%20crucial%20role%2C%20as%0Asome%20researchers%20have%20indicated%20that%20GPT-4%20adopts%20the%20MoE%20structure%20to%20ensure%0Adiverse%20inference%20results.%20However%2C%20MoE%20is%20susceptible%20to%20performance%0Adegeneracy%2C%20particularly%20evident%20in%20the%20issues%20of%20imbalance%20and%20homogeneous%0Arepresentation%20among%20experts.%20While%20previous%20studies%20have%20extensively%20addressed%0Athe%20problem%20of%20imbalance%2C%20the%20challenge%20of%20homogeneous%20representation%20remains%0Aunresolved.%20In%20this%20study%2C%20we%20shed%20light%20on%20the%20homogeneous%20representation%0Aproblem%2C%20wherein%20experts%20in%20the%20MoE%20fail%20to%20specialize%20and%20lack%20diversity%2C%0Aleading%20to%20frustratingly%20high%20similarities%20in%20their%20representations%20%28up%20to%2099%5C%25%0Ain%20a%20well-performed%20MoE%20model%29.%20This%20problem%20restricts%20the%20expressive%20power%20of%0Athe%20MoE%20and%2C%20we%20argue%2C%20contradicts%20its%20original%20intention.%20To%20tackle%20this%0Aissue%2C%20we%20propose%20a%20straightforward%20yet%20highly%20effective%20solution%3A%20OMoE%2C%20an%0Aorthogonal%20expert%20optimizer.%20Additionally%2C%20we%20introduce%20an%20alternating%20training%0Astrategy%20that%20encourages%20each%20expert%20to%20update%20in%20a%20direction%20orthogonal%20to%20the%0Asubspace%20spanned%20by%20other%20experts.%20Our%20algorithm%20facilitates%20MoE%20training%20in%0Atwo%20key%20ways%3A%20firstly%2C%20it%20explicitly%20enhances%20representation%20diversity%2C%20and%0Asecondly%2C%20it%20implicitly%20fosters%20interaction%20between%20experts%20during%20orthogonal%0Aweights%20computation.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%0Aproposed%20optimization%20algorithm%20significantly%20improves%20the%20performance%20of%0Afine-tuning%20the%20MoE%20model%20on%20the%20GLUE%20benchmark%2C%20SuperGLUE%20benchmark%2C%0Aquestion-answering%20task%2C%20and%20name%20entity%20recognition%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09762v2&entry.124074799=Read"},
{"title": "Hold Me Tight: Stable Encoder-Decoder Design for Speech Enhancement", "author": "Daniel Haider and Felix Perfler and Vincent Lostanlen and Martin Ehler and Peter Balazs", "abstract": "  Convolutional layers with 1-D filters are often used as frontend to encode\naudio signals. Unlike fixed time-frequency representations, they can adapt to\nthe local characteristics of input data. However, 1-D filters on raw audio are\nhard to train and often suffer from instabilities. In this paper, we address\nthese problems with hybrid solutions, i.e., combining theory-driven and\ndata-driven approaches. First, we preprocess the audio signals via a auditory\nfilterbank, guaranteeing good frequency localization for the learned encoder.\nSecond, we use results from frame theory to define an unsupervised learning\nobjective that encourages energy conservation and perfect reconstruction.\nThird, we adapt mixed compressed spectral norms as learning objectives to the\nencoder coefficients. Using these solutions in a low-complexity\nencoder-mask-decoder model significantly improves the perceptual evaluation of\nspeech quality (PESQ) in speech enhancement.\n", "link": "http://arxiv.org/abs/2408.17358v1", "date": "2024-08-30", "relevancy": 1.9518, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5066}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4764}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hold%20Me%20Tight%3A%20Stable%20Encoder-Decoder%20Design%20for%20Speech%20Enhancement&body=Title%3A%20Hold%20Me%20Tight%3A%20Stable%20Encoder-Decoder%20Design%20for%20Speech%20Enhancement%0AAuthor%3A%20Daniel%20Haider%20and%20Felix%20Perfler%20and%20Vincent%20Lostanlen%20and%20Martin%20Ehler%20and%20Peter%20Balazs%0AAbstract%3A%20%20%20Convolutional%20layers%20with%201-D%20filters%20are%20often%20used%20as%20frontend%20to%20encode%0Aaudio%20signals.%20Unlike%20fixed%20time-frequency%20representations%2C%20they%20can%20adapt%20to%0Athe%20local%20characteristics%20of%20input%20data.%20However%2C%201-D%20filters%20on%20raw%20audio%20are%0Ahard%20to%20train%20and%20often%20suffer%20from%20instabilities.%20In%20this%20paper%2C%20we%20address%0Athese%20problems%20with%20hybrid%20solutions%2C%20i.e.%2C%20combining%20theory-driven%20and%0Adata-driven%20approaches.%20First%2C%20we%20preprocess%20the%20audio%20signals%20via%20a%20auditory%0Afilterbank%2C%20guaranteeing%20good%20frequency%20localization%20for%20the%20learned%20encoder.%0ASecond%2C%20we%20use%20results%20from%20frame%20theory%20to%20define%20an%20unsupervised%20learning%0Aobjective%20that%20encourages%20energy%20conservation%20and%20perfect%20reconstruction.%0AThird%2C%20we%20adapt%20mixed%20compressed%20spectral%20norms%20as%20learning%20objectives%20to%20the%0Aencoder%20coefficients.%20Using%20these%20solutions%20in%20a%20low-complexity%0Aencoder-mask-decoder%20model%20significantly%20improves%20the%20perceptual%20evaluation%20of%0Aspeech%20quality%20%28PESQ%29%20in%20speech%20enhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHold%2520Me%2520Tight%253A%2520Stable%2520Encoder-Decoder%2520Design%2520for%2520Speech%2520Enhancement%26entry.906535625%3DDaniel%2520Haider%2520and%2520Felix%2520Perfler%2520and%2520Vincent%2520Lostanlen%2520and%2520Martin%2520Ehler%2520and%2520Peter%2520Balazs%26entry.1292438233%3D%2520%2520Convolutional%2520layers%2520with%25201-D%2520filters%2520are%2520often%2520used%2520as%2520frontend%2520to%2520encode%250Aaudio%2520signals.%2520Unlike%2520fixed%2520time-frequency%2520representations%252C%2520they%2520can%2520adapt%2520to%250Athe%2520local%2520characteristics%2520of%2520input%2520data.%2520However%252C%25201-D%2520filters%2520on%2520raw%2520audio%2520are%250Ahard%2520to%2520train%2520and%2520often%2520suffer%2520from%2520instabilities.%2520In%2520this%2520paper%252C%2520we%2520address%250Athese%2520problems%2520with%2520hybrid%2520solutions%252C%2520i.e.%252C%2520combining%2520theory-driven%2520and%250Adata-driven%2520approaches.%2520First%252C%2520we%2520preprocess%2520the%2520audio%2520signals%2520via%2520a%2520auditory%250Afilterbank%252C%2520guaranteeing%2520good%2520frequency%2520localization%2520for%2520the%2520learned%2520encoder.%250ASecond%252C%2520we%2520use%2520results%2520from%2520frame%2520theory%2520to%2520define%2520an%2520unsupervised%2520learning%250Aobjective%2520that%2520encourages%2520energy%2520conservation%2520and%2520perfect%2520reconstruction.%250AThird%252C%2520we%2520adapt%2520mixed%2520compressed%2520spectral%2520norms%2520as%2520learning%2520objectives%2520to%2520the%250Aencoder%2520coefficients.%2520Using%2520these%2520solutions%2520in%2520a%2520low-complexity%250Aencoder-mask-decoder%2520model%2520significantly%2520improves%2520the%2520perceptual%2520evaluation%2520of%250Aspeech%2520quality%2520%2528PESQ%2529%2520in%2520speech%2520enhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hold%20Me%20Tight%3A%20Stable%20Encoder-Decoder%20Design%20for%20Speech%20Enhancement&entry.906535625=Daniel%20Haider%20and%20Felix%20Perfler%20and%20Vincent%20Lostanlen%20and%20Martin%20Ehler%20and%20Peter%20Balazs&entry.1292438233=%20%20Convolutional%20layers%20with%201-D%20filters%20are%20often%20used%20as%20frontend%20to%20encode%0Aaudio%20signals.%20Unlike%20fixed%20time-frequency%20representations%2C%20they%20can%20adapt%20to%0Athe%20local%20characteristics%20of%20input%20data.%20However%2C%201-D%20filters%20on%20raw%20audio%20are%0Ahard%20to%20train%20and%20often%20suffer%20from%20instabilities.%20In%20this%20paper%2C%20we%20address%0Athese%20problems%20with%20hybrid%20solutions%2C%20i.e.%2C%20combining%20theory-driven%20and%0Adata-driven%20approaches.%20First%2C%20we%20preprocess%20the%20audio%20signals%20via%20a%20auditory%0Afilterbank%2C%20guaranteeing%20good%20frequency%20localization%20for%20the%20learned%20encoder.%0ASecond%2C%20we%20use%20results%20from%20frame%20theory%20to%20define%20an%20unsupervised%20learning%0Aobjective%20that%20encourages%20energy%20conservation%20and%20perfect%20reconstruction.%0AThird%2C%20we%20adapt%20mixed%20compressed%20spectral%20norms%20as%20learning%20objectives%20to%20the%0Aencoder%20coefficients.%20Using%20these%20solutions%20in%20a%20low-complexity%0Aencoder-mask-decoder%20model%20significantly%20improves%20the%20perceptual%20evaluation%20of%0Aspeech%20quality%20%28PESQ%29%20in%20speech%20enhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17358v1&entry.124074799=Read"},
{"title": "A nonlinear elasticity model in computer vision", "author": "John M. Ball and Christopher L. Horner", "abstract": "  The purpose of this paper is to analyze a nonlinear elasticity model\npreviously introduced by the authors for comparing two images, regarded as\nbounded open subsets of $\\R^n$ together with associated vector-valued intensity\nmaps. Optimal transformations between the images are sought as minimisers of an\nintegral functional among orientation-preserving homeomorphisms. The existence\nof minimisers is proved under natural coercivity and polyconvexity conditions,\nassuming only that the intensity functions are bounded measurable. Variants of\nthe existence theorem are also proved, first under the constraint that finite\nsets of landmark points in the two images are mapped one to the other, and\nsecond when one image is to be compared to an unknown part of another.\n  The question is studied as to whether for images related by a linear mapping\nthe unique minimizer is given by that linear mapping. For a natural class of\nfunctional integrands an example is given guaranteeing that this property holds\nfor pairs of images in which the second is a scaling of the first by a constant\nfactor. However for the property to hold for arbitrary pairs of linearly\nrelated images it is shown that the integrand has to depend on the gradient of\nthe transformation as a convex function of its determinant alone. This suggests\na new model in which the integrand depends also on second derivatives of the\ntransformation, and an example is given for which both existence of minimizers\nis assured and the above property holds for all pairs of linearly related\nimages.\n", "link": "http://arxiv.org/abs/2408.17237v1", "date": "2024-08-30", "relevancy": 1.9474, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5213}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.483}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20nonlinear%20elasticity%20model%20in%20computer%20vision&body=Title%3A%20A%20nonlinear%20elasticity%20model%20in%20computer%20vision%0AAuthor%3A%20John%20M.%20Ball%20and%20Christopher%20L.%20Horner%0AAbstract%3A%20%20%20The%20purpose%20of%20this%20paper%20is%20to%20analyze%20a%20nonlinear%20elasticity%20model%0Apreviously%20introduced%20by%20the%20authors%20for%20comparing%20two%20images%2C%20regarded%20as%0Abounded%20open%20subsets%20of%20%24%5CR%5En%24%20together%20with%20associated%20vector-valued%20intensity%0Amaps.%20Optimal%20transformations%20between%20the%20images%20are%20sought%20as%20minimisers%20of%20an%0Aintegral%20functional%20among%20orientation-preserving%20homeomorphisms.%20The%20existence%0Aof%20minimisers%20is%20proved%20under%20natural%20coercivity%20and%20polyconvexity%20conditions%2C%0Aassuming%20only%20that%20the%20intensity%20functions%20are%20bounded%20measurable.%20Variants%20of%0Athe%20existence%20theorem%20are%20also%20proved%2C%20first%20under%20the%20constraint%20that%20finite%0Asets%20of%20landmark%20points%20in%20the%20two%20images%20are%20mapped%20one%20to%20the%20other%2C%20and%0Asecond%20when%20one%20image%20is%20to%20be%20compared%20to%20an%20unknown%20part%20of%20another.%0A%20%20The%20question%20is%20studied%20as%20to%20whether%20for%20images%20related%20by%20a%20linear%20mapping%0Athe%20unique%20minimizer%20is%20given%20by%20that%20linear%20mapping.%20For%20a%20natural%20class%20of%0Afunctional%20integrands%20an%20example%20is%20given%20guaranteeing%20that%20this%20property%20holds%0Afor%20pairs%20of%20images%20in%20which%20the%20second%20is%20a%20scaling%20of%20the%20first%20by%20a%20constant%0Afactor.%20However%20for%20the%20property%20to%20hold%20for%20arbitrary%20pairs%20of%20linearly%0Arelated%20images%20it%20is%20shown%20that%20the%20integrand%20has%20to%20depend%20on%20the%20gradient%20of%0Athe%20transformation%20as%20a%20convex%20function%20of%20its%20determinant%20alone.%20This%20suggests%0Aa%20new%20model%20in%20which%20the%20integrand%20depends%20also%20on%20second%20derivatives%20of%20the%0Atransformation%2C%20and%20an%20example%20is%20given%20for%20which%20both%20existence%20of%20minimizers%0Ais%20assured%20and%20the%20above%20property%20holds%20for%20all%20pairs%20of%20linearly%20related%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520nonlinear%2520elasticity%2520model%2520in%2520computer%2520vision%26entry.906535625%3DJohn%2520M.%2520Ball%2520and%2520Christopher%2520L.%2520Horner%26entry.1292438233%3D%2520%2520The%2520purpose%2520of%2520this%2520paper%2520is%2520to%2520analyze%2520a%2520nonlinear%2520elasticity%2520model%250Apreviously%2520introduced%2520by%2520the%2520authors%2520for%2520comparing%2520two%2520images%252C%2520regarded%2520as%250Abounded%2520open%2520subsets%2520of%2520%2524%255CR%255En%2524%2520together%2520with%2520associated%2520vector-valued%2520intensity%250Amaps.%2520Optimal%2520transformations%2520between%2520the%2520images%2520are%2520sought%2520as%2520minimisers%2520of%2520an%250Aintegral%2520functional%2520among%2520orientation-preserving%2520homeomorphisms.%2520The%2520existence%250Aof%2520minimisers%2520is%2520proved%2520under%2520natural%2520coercivity%2520and%2520polyconvexity%2520conditions%252C%250Aassuming%2520only%2520that%2520the%2520intensity%2520functions%2520are%2520bounded%2520measurable.%2520Variants%2520of%250Athe%2520existence%2520theorem%2520are%2520also%2520proved%252C%2520first%2520under%2520the%2520constraint%2520that%2520finite%250Asets%2520of%2520landmark%2520points%2520in%2520the%2520two%2520images%2520are%2520mapped%2520one%2520to%2520the%2520other%252C%2520and%250Asecond%2520when%2520one%2520image%2520is%2520to%2520be%2520compared%2520to%2520an%2520unknown%2520part%2520of%2520another.%250A%2520%2520The%2520question%2520is%2520studied%2520as%2520to%2520whether%2520for%2520images%2520related%2520by%2520a%2520linear%2520mapping%250Athe%2520unique%2520minimizer%2520is%2520given%2520by%2520that%2520linear%2520mapping.%2520For%2520a%2520natural%2520class%2520of%250Afunctional%2520integrands%2520an%2520example%2520is%2520given%2520guaranteeing%2520that%2520this%2520property%2520holds%250Afor%2520pairs%2520of%2520images%2520in%2520which%2520the%2520second%2520is%2520a%2520scaling%2520of%2520the%2520first%2520by%2520a%2520constant%250Afactor.%2520However%2520for%2520the%2520property%2520to%2520hold%2520for%2520arbitrary%2520pairs%2520of%2520linearly%250Arelated%2520images%2520it%2520is%2520shown%2520that%2520the%2520integrand%2520has%2520to%2520depend%2520on%2520the%2520gradient%2520of%250Athe%2520transformation%2520as%2520a%2520convex%2520function%2520of%2520its%2520determinant%2520alone.%2520This%2520suggests%250Aa%2520new%2520model%2520in%2520which%2520the%2520integrand%2520depends%2520also%2520on%2520second%2520derivatives%2520of%2520the%250Atransformation%252C%2520and%2520an%2520example%2520is%2520given%2520for%2520which%2520both%2520existence%2520of%2520minimizers%250Ais%2520assured%2520and%2520the%2520above%2520property%2520holds%2520for%2520all%2520pairs%2520of%2520linearly%2520related%250Aimages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20nonlinear%20elasticity%20model%20in%20computer%20vision&entry.906535625=John%20M.%20Ball%20and%20Christopher%20L.%20Horner&entry.1292438233=%20%20The%20purpose%20of%20this%20paper%20is%20to%20analyze%20a%20nonlinear%20elasticity%20model%0Apreviously%20introduced%20by%20the%20authors%20for%20comparing%20two%20images%2C%20regarded%20as%0Abounded%20open%20subsets%20of%20%24%5CR%5En%24%20together%20with%20associated%20vector-valued%20intensity%0Amaps.%20Optimal%20transformations%20between%20the%20images%20are%20sought%20as%20minimisers%20of%20an%0Aintegral%20functional%20among%20orientation-preserving%20homeomorphisms.%20The%20existence%0Aof%20minimisers%20is%20proved%20under%20natural%20coercivity%20and%20polyconvexity%20conditions%2C%0Aassuming%20only%20that%20the%20intensity%20functions%20are%20bounded%20measurable.%20Variants%20of%0Athe%20existence%20theorem%20are%20also%20proved%2C%20first%20under%20the%20constraint%20that%20finite%0Asets%20of%20landmark%20points%20in%20the%20two%20images%20are%20mapped%20one%20to%20the%20other%2C%20and%0Asecond%20when%20one%20image%20is%20to%20be%20compared%20to%20an%20unknown%20part%20of%20another.%0A%20%20The%20question%20is%20studied%20as%20to%20whether%20for%20images%20related%20by%20a%20linear%20mapping%0Athe%20unique%20minimizer%20is%20given%20by%20that%20linear%20mapping.%20For%20a%20natural%20class%20of%0Afunctional%20integrands%20an%20example%20is%20given%20guaranteeing%20that%20this%20property%20holds%0Afor%20pairs%20of%20images%20in%20which%20the%20second%20is%20a%20scaling%20of%20the%20first%20by%20a%20constant%0Afactor.%20However%20for%20the%20property%20to%20hold%20for%20arbitrary%20pairs%20of%20linearly%0Arelated%20images%20it%20is%20shown%20that%20the%20integrand%20has%20to%20depend%20on%20the%20gradient%20of%0Athe%20transformation%20as%20a%20convex%20function%20of%20its%20determinant%20alone.%20This%20suggests%0Aa%20new%20model%20in%20which%20the%20integrand%20depends%20also%20on%20second%20derivatives%20of%20the%0Atransformation%2C%20and%20an%20example%20is%20given%20for%20which%20both%20existence%20of%20minimizers%0Ais%20assured%20and%20the%20above%20property%20holds%20for%20all%20pairs%20of%20linearly%20related%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17237v1&entry.124074799=Read"},
{"title": "Can We Remove the Square-Root in Adaptive Gradient Methods? A\n  Second-Order Perspective", "author": "Wu Lin and Felix Dangel and Runa Eschenhagen and Juhan Bae and Richard E. Turner and Alireza Makhzani", "abstract": "  Adaptive gradient optimizers like Adam(W) are the default training algorithms\nfor many deep learning architectures, such as transformers. Their diagonal\npreconditioner is based on the gradient outer product which is incorporated\ninto the parameter update via a square root. While these methods are often\nmotivated as approximate second-order methods, the square root represents a\nfundamental difference. In this work, we investigate how the behavior of\nadaptive methods changes when we remove the root, i.e., strengthen their\nsecond-order motivation. Surprisingly, we find that such square-root-free\nadaptive methods close the generalization gap to SGD on convolutional\narchitectures, while maintaining their root-based counterpart's performance on\ntransformers. The second-order perspective also has practical benefits for\ndeveloping non-diagonal methods that can incorporate arbitrary curvature\napproximations through the concept of preconditioner invariance. In contrast to\nroot-based methods like Shampoo, root-free counterparts work well and fast with\nhalf-precision since they do not require numerically unstable matrix root\ndecompositions and inversions. Overall, our findings provide new insights into\nthe development of adaptive methods and raise important questions regarding the\noverlooked role of adaptivity in their success. (experiment code:\nhttps://github.com/yorkerlin/remove-the-square-root optimizer code:\nhttps://github.com/f-dangel/sirfshampoo)\n", "link": "http://arxiv.org/abs/2402.03496v9", "date": "2024-08-30", "relevancy": 1.9367, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5083}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.494}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20We%20Remove%20the%20Square-Root%20in%20Adaptive%20Gradient%20Methods%3F%20A%0A%20%20Second-Order%20Perspective&body=Title%3A%20Can%20We%20Remove%20the%20Square-Root%20in%20Adaptive%20Gradient%20Methods%3F%20A%0A%20%20Second-Order%20Perspective%0AAuthor%3A%20Wu%20Lin%20and%20Felix%20Dangel%20and%20Runa%20Eschenhagen%20and%20Juhan%20Bae%20and%20Richard%20E.%20Turner%20and%20Alireza%20Makhzani%0AAbstract%3A%20%20%20Adaptive%20gradient%20optimizers%20like%20Adam%28W%29%20are%20the%20default%20training%20algorithms%0Afor%20many%20deep%20learning%20architectures%2C%20such%20as%20transformers.%20Their%20diagonal%0Apreconditioner%20is%20based%20on%20the%20gradient%20outer%20product%20which%20is%20incorporated%0Ainto%20the%20parameter%20update%20via%20a%20square%20root.%20While%20these%20methods%20are%20often%0Amotivated%20as%20approximate%20second-order%20methods%2C%20the%20square%20root%20represents%20a%0Afundamental%20difference.%20In%20this%20work%2C%20we%20investigate%20how%20the%20behavior%20of%0Aadaptive%20methods%20changes%20when%20we%20remove%20the%20root%2C%20i.e.%2C%20strengthen%20their%0Asecond-order%20motivation.%20Surprisingly%2C%20we%20find%20that%20such%20square-root-free%0Aadaptive%20methods%20close%20the%20generalization%20gap%20to%20SGD%20on%20convolutional%0Aarchitectures%2C%20while%20maintaining%20their%20root-based%20counterpart%27s%20performance%20on%0Atransformers.%20The%20second-order%20perspective%20also%20has%20practical%20benefits%20for%0Adeveloping%20non-diagonal%20methods%20that%20can%20incorporate%20arbitrary%20curvature%0Aapproximations%20through%20the%20concept%20of%20preconditioner%20invariance.%20In%20contrast%20to%0Aroot-based%20methods%20like%20Shampoo%2C%20root-free%20counterparts%20work%20well%20and%20fast%20with%0Ahalf-precision%20since%20they%20do%20not%20require%20numerically%20unstable%20matrix%20root%0Adecompositions%20and%20inversions.%20Overall%2C%20our%20findings%20provide%20new%20insights%20into%0Athe%20development%20of%20adaptive%20methods%20and%20raise%20important%20questions%20regarding%20the%0Aoverlooked%20role%20of%20adaptivity%20in%20their%20success.%20%28experiment%20code%3A%0Ahttps%3A//github.com/yorkerlin/remove-the-square-root%20optimizer%20code%3A%0Ahttps%3A//github.com/f-dangel/sirfshampoo%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03496v9%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520We%2520Remove%2520the%2520Square-Root%2520in%2520Adaptive%2520Gradient%2520Methods%253F%2520A%250A%2520%2520Second-Order%2520Perspective%26entry.906535625%3DWu%2520Lin%2520and%2520Felix%2520Dangel%2520and%2520Runa%2520Eschenhagen%2520and%2520Juhan%2520Bae%2520and%2520Richard%2520E.%2520Turner%2520and%2520Alireza%2520Makhzani%26entry.1292438233%3D%2520%2520Adaptive%2520gradient%2520optimizers%2520like%2520Adam%2528W%2529%2520are%2520the%2520default%2520training%2520algorithms%250Afor%2520many%2520deep%2520learning%2520architectures%252C%2520such%2520as%2520transformers.%2520Their%2520diagonal%250Apreconditioner%2520is%2520based%2520on%2520the%2520gradient%2520outer%2520product%2520which%2520is%2520incorporated%250Ainto%2520the%2520parameter%2520update%2520via%2520a%2520square%2520root.%2520While%2520these%2520methods%2520are%2520often%250Amotivated%2520as%2520approximate%2520second-order%2520methods%252C%2520the%2520square%2520root%2520represents%2520a%250Afundamental%2520difference.%2520In%2520this%2520work%252C%2520we%2520investigate%2520how%2520the%2520behavior%2520of%250Aadaptive%2520methods%2520changes%2520when%2520we%2520remove%2520the%2520root%252C%2520i.e.%252C%2520strengthen%2520their%250Asecond-order%2520motivation.%2520Surprisingly%252C%2520we%2520find%2520that%2520such%2520square-root-free%250Aadaptive%2520methods%2520close%2520the%2520generalization%2520gap%2520to%2520SGD%2520on%2520convolutional%250Aarchitectures%252C%2520while%2520maintaining%2520their%2520root-based%2520counterpart%2527s%2520performance%2520on%250Atransformers.%2520The%2520second-order%2520perspective%2520also%2520has%2520practical%2520benefits%2520for%250Adeveloping%2520non-diagonal%2520methods%2520that%2520can%2520incorporate%2520arbitrary%2520curvature%250Aapproximations%2520through%2520the%2520concept%2520of%2520preconditioner%2520invariance.%2520In%2520contrast%2520to%250Aroot-based%2520methods%2520like%2520Shampoo%252C%2520root-free%2520counterparts%2520work%2520well%2520and%2520fast%2520with%250Ahalf-precision%2520since%2520they%2520do%2520not%2520require%2520numerically%2520unstable%2520matrix%2520root%250Adecompositions%2520and%2520inversions.%2520Overall%252C%2520our%2520findings%2520provide%2520new%2520insights%2520into%250Athe%2520development%2520of%2520adaptive%2520methods%2520and%2520raise%2520important%2520questions%2520regarding%2520the%250Aoverlooked%2520role%2520of%2520adaptivity%2520in%2520their%2520success.%2520%2528experiment%2520code%253A%250Ahttps%253A//github.com/yorkerlin/remove-the-square-root%2520optimizer%2520code%253A%250Ahttps%253A//github.com/f-dangel/sirfshampoo%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03496v9%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20We%20Remove%20the%20Square-Root%20in%20Adaptive%20Gradient%20Methods%3F%20A%0A%20%20Second-Order%20Perspective&entry.906535625=Wu%20Lin%20and%20Felix%20Dangel%20and%20Runa%20Eschenhagen%20and%20Juhan%20Bae%20and%20Richard%20E.%20Turner%20and%20Alireza%20Makhzani&entry.1292438233=%20%20Adaptive%20gradient%20optimizers%20like%20Adam%28W%29%20are%20the%20default%20training%20algorithms%0Afor%20many%20deep%20learning%20architectures%2C%20such%20as%20transformers.%20Their%20diagonal%0Apreconditioner%20is%20based%20on%20the%20gradient%20outer%20product%20which%20is%20incorporated%0Ainto%20the%20parameter%20update%20via%20a%20square%20root.%20While%20these%20methods%20are%20often%0Amotivated%20as%20approximate%20second-order%20methods%2C%20the%20square%20root%20represents%20a%0Afundamental%20difference.%20In%20this%20work%2C%20we%20investigate%20how%20the%20behavior%20of%0Aadaptive%20methods%20changes%20when%20we%20remove%20the%20root%2C%20i.e.%2C%20strengthen%20their%0Asecond-order%20motivation.%20Surprisingly%2C%20we%20find%20that%20such%20square-root-free%0Aadaptive%20methods%20close%20the%20generalization%20gap%20to%20SGD%20on%20convolutional%0Aarchitectures%2C%20while%20maintaining%20their%20root-based%20counterpart%27s%20performance%20on%0Atransformers.%20The%20second-order%20perspective%20also%20has%20practical%20benefits%20for%0Adeveloping%20non-diagonal%20methods%20that%20can%20incorporate%20arbitrary%20curvature%0Aapproximations%20through%20the%20concept%20of%20preconditioner%20invariance.%20In%20contrast%20to%0Aroot-based%20methods%20like%20Shampoo%2C%20root-free%20counterparts%20work%20well%20and%20fast%20with%0Ahalf-precision%20since%20they%20do%20not%20require%20numerically%20unstable%20matrix%20root%0Adecompositions%20and%20inversions.%20Overall%2C%20our%20findings%20provide%20new%20insights%20into%0Athe%20development%20of%20adaptive%20methods%20and%20raise%20important%20questions%20regarding%20the%0Aoverlooked%20role%20of%20adaptivity%20in%20their%20success.%20%28experiment%20code%3A%0Ahttps%3A//github.com/yorkerlin/remove-the-square-root%20optimizer%20code%3A%0Ahttps%3A//github.com/f-dangel/sirfshampoo%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03496v9&entry.124074799=Read"},
{"title": "Information-Based Trajectory Planning for Autonomous Absolute Tracking\n  in Cislunar Space", "author": "Trevor N. Wolf and Brandon A. Jones", "abstract": "  The resurgence of lunar operations requires advancements in cislunar\nnavigation and Space Situational Awareness (SSA). Challenges associated to\nthese tasks have created an interest in autonomous planning, navigation, and\ntracking technologies that operate with little ground-based intervention. This\nresearch introduces a trajectory planning tool for a low-thrust mobile\nobserver, aimed at maximizing navigation and tracking performance with\nsatellite-to-satellite relative measurements. We formulate an expression for\nthe information gathered over an observation period based on the mutual\ninformation between augmented observer/target states and the associated\nmeasurement set collected. We then develop an optimal trajectory design problem\nfor a mobile observer, balancing information gain and control effort, and solve\nthis problem with a Sequential Convex Programming (SCP) approach. The developed\nmethods are demonstrated in scenarios involving spacecraft in the cislunar\nregime, demonstrating the potential for improved autonomous navigation and\ntracking.\n", "link": "http://arxiv.org/abs/2408.17435v1", "date": "2024-08-30", "relevancy": 1.9332, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4928}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4825}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Information-Based%20Trajectory%20Planning%20for%20Autonomous%20Absolute%20Tracking%0A%20%20in%20Cislunar%20Space&body=Title%3A%20Information-Based%20Trajectory%20Planning%20for%20Autonomous%20Absolute%20Tracking%0A%20%20in%20Cislunar%20Space%0AAuthor%3A%20Trevor%20N.%20Wolf%20and%20Brandon%20A.%20Jones%0AAbstract%3A%20%20%20The%20resurgence%20of%20lunar%20operations%20requires%20advancements%20in%20cislunar%0Anavigation%20and%20Space%20Situational%20Awareness%20%28SSA%29.%20Challenges%20associated%20to%0Athese%20tasks%20have%20created%20an%20interest%20in%20autonomous%20planning%2C%20navigation%2C%20and%0Atracking%20technologies%20that%20operate%20with%20little%20ground-based%20intervention.%20This%0Aresearch%20introduces%20a%20trajectory%20planning%20tool%20for%20a%20low-thrust%20mobile%0Aobserver%2C%20aimed%20at%20maximizing%20navigation%20and%20tracking%20performance%20with%0Asatellite-to-satellite%20relative%20measurements.%20We%20formulate%20an%20expression%20for%0Athe%20information%20gathered%20over%20an%20observation%20period%20based%20on%20the%20mutual%0Ainformation%20between%20augmented%20observer/target%20states%20and%20the%20associated%0Ameasurement%20set%20collected.%20We%20then%20develop%20an%20optimal%20trajectory%20design%20problem%0Afor%20a%20mobile%20observer%2C%20balancing%20information%20gain%20and%20control%20effort%2C%20and%20solve%0Athis%20problem%20with%20a%20Sequential%20Convex%20Programming%20%28SCP%29%20approach.%20The%20developed%0Amethods%20are%20demonstrated%20in%20scenarios%20involving%20spacecraft%20in%20the%20cislunar%0Aregime%2C%20demonstrating%20the%20potential%20for%20improved%20autonomous%20navigation%20and%0Atracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformation-Based%2520Trajectory%2520Planning%2520for%2520Autonomous%2520Absolute%2520Tracking%250A%2520%2520in%2520Cislunar%2520Space%26entry.906535625%3DTrevor%2520N.%2520Wolf%2520and%2520Brandon%2520A.%2520Jones%26entry.1292438233%3D%2520%2520The%2520resurgence%2520of%2520lunar%2520operations%2520requires%2520advancements%2520in%2520cislunar%250Anavigation%2520and%2520Space%2520Situational%2520Awareness%2520%2528SSA%2529.%2520Challenges%2520associated%2520to%250Athese%2520tasks%2520have%2520created%2520an%2520interest%2520in%2520autonomous%2520planning%252C%2520navigation%252C%2520and%250Atracking%2520technologies%2520that%2520operate%2520with%2520little%2520ground-based%2520intervention.%2520This%250Aresearch%2520introduces%2520a%2520trajectory%2520planning%2520tool%2520for%2520a%2520low-thrust%2520mobile%250Aobserver%252C%2520aimed%2520at%2520maximizing%2520navigation%2520and%2520tracking%2520performance%2520with%250Asatellite-to-satellite%2520relative%2520measurements.%2520We%2520formulate%2520an%2520expression%2520for%250Athe%2520information%2520gathered%2520over%2520an%2520observation%2520period%2520based%2520on%2520the%2520mutual%250Ainformation%2520between%2520augmented%2520observer/target%2520states%2520and%2520the%2520associated%250Ameasurement%2520set%2520collected.%2520We%2520then%2520develop%2520an%2520optimal%2520trajectory%2520design%2520problem%250Afor%2520a%2520mobile%2520observer%252C%2520balancing%2520information%2520gain%2520and%2520control%2520effort%252C%2520and%2520solve%250Athis%2520problem%2520with%2520a%2520Sequential%2520Convex%2520Programming%2520%2528SCP%2529%2520approach.%2520The%2520developed%250Amethods%2520are%2520demonstrated%2520in%2520scenarios%2520involving%2520spacecraft%2520in%2520the%2520cislunar%250Aregime%252C%2520demonstrating%2520the%2520potential%2520for%2520improved%2520autonomous%2520navigation%2520and%250Atracking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Information-Based%20Trajectory%20Planning%20for%20Autonomous%20Absolute%20Tracking%0A%20%20in%20Cislunar%20Space&entry.906535625=Trevor%20N.%20Wolf%20and%20Brandon%20A.%20Jones&entry.1292438233=%20%20The%20resurgence%20of%20lunar%20operations%20requires%20advancements%20in%20cislunar%0Anavigation%20and%20Space%20Situational%20Awareness%20%28SSA%29.%20Challenges%20associated%20to%0Athese%20tasks%20have%20created%20an%20interest%20in%20autonomous%20planning%2C%20navigation%2C%20and%0Atracking%20technologies%20that%20operate%20with%20little%20ground-based%20intervention.%20This%0Aresearch%20introduces%20a%20trajectory%20planning%20tool%20for%20a%20low-thrust%20mobile%0Aobserver%2C%20aimed%20at%20maximizing%20navigation%20and%20tracking%20performance%20with%0Asatellite-to-satellite%20relative%20measurements.%20We%20formulate%20an%20expression%20for%0Athe%20information%20gathered%20over%20an%20observation%20period%20based%20on%20the%20mutual%0Ainformation%20between%20augmented%20observer/target%20states%20and%20the%20associated%0Ameasurement%20set%20collected.%20We%20then%20develop%20an%20optimal%20trajectory%20design%20problem%0Afor%20a%20mobile%20observer%2C%20balancing%20information%20gain%20and%20control%20effort%2C%20and%20solve%0Athis%20problem%20with%20a%20Sequential%20Convex%20Programming%20%28SCP%29%20approach.%20The%20developed%0Amethods%20are%20demonstrated%20in%20scenarios%20involving%20spacecraft%20in%20the%20cislunar%0Aregime%2C%20demonstrating%20the%20potential%20for%20improved%20autonomous%20navigation%20and%0Atracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17435v1&entry.124074799=Read"},
{"title": "NDP: Next Distribution Prediction as a More Broad Target", "author": "Junhao Ruan and Abudukeyumu Abudula and Xinyu Liu and Bei Li and Yinqiao Li and Chenglong Wang and Yuchun Fan and Yuan Ge and Tong Xiao and Jingbo Zhu", "abstract": "  Large language models (LLMs) trained on next-token prediction (NTP) paradigm\nhave demonstrated powerful capabilities. However, the existing NTP paradigm\ncontains several limitations, particularly related to planned task\ncomplications and error propagation during inference. In our work, we extend\nthe critique of NTP, highlighting its limitation also due to training with a\nnarrow objective: the prediction of a sub-optimal one-hot distribution. To\nsupport this critique, we conducted a pre-experiment treating the output\ndistribution from powerful LLMs as efficient world data compression. By\nevaluating the similarity between the $n$-gram distribution and the one-hot\ndistribution with LLMs, we observed that the $n$-gram distributions align more\nclosely with the output distribution of LLMs. Based on this insight, we\nintroduce Next Distribution Prediction (NDP), which uses $n$-gram distributions\nto replace the one-hot targets, enhancing learning without extra online\ntraining time. We conducted experiments across translation, general task,\nlanguage transfer, and medical domain adaptation. Compared to NTP, NDP can\nachieve up to +2.97 COMET improvement in translation tasks, +0.61 average\nimprovement in general tasks, and incredible +10.75 average improvement in the\nmedical domain. This demonstrates the concrete benefits of addressing the\ntarget narrowing problem, pointing to a new direction for future work on\nimproving NTP.\n", "link": "http://arxiv.org/abs/2408.17377v1", "date": "2024-08-30", "relevancy": 1.9078, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4833}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4792}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NDP%3A%20Next%20Distribution%20Prediction%20as%20a%20More%20Broad%20Target&body=Title%3A%20NDP%3A%20Next%20Distribution%20Prediction%20as%20a%20More%20Broad%20Target%0AAuthor%3A%20Junhao%20Ruan%20and%20Abudukeyumu%20Abudula%20and%20Xinyu%20Liu%20and%20Bei%20Li%20and%20Yinqiao%20Li%20and%20Chenglong%20Wang%20and%20Yuchun%20Fan%20and%20Yuan%20Ge%20and%20Tong%20Xiao%20and%20Jingbo%20Zhu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20trained%20on%20next-token%20prediction%20%28NTP%29%20paradigm%0Ahave%20demonstrated%20powerful%20capabilities.%20However%2C%20the%20existing%20NTP%20paradigm%0Acontains%20several%20limitations%2C%20particularly%20related%20to%20planned%20task%0Acomplications%20and%20error%20propagation%20during%20inference.%20In%20our%20work%2C%20we%20extend%0Athe%20critique%20of%20NTP%2C%20highlighting%20its%20limitation%20also%20due%20to%20training%20with%20a%0Anarrow%20objective%3A%20the%20prediction%20of%20a%20sub-optimal%20one-hot%20distribution.%20To%0Asupport%20this%20critique%2C%20we%20conducted%20a%20pre-experiment%20treating%20the%20output%0Adistribution%20from%20powerful%20LLMs%20as%20efficient%20world%20data%20compression.%20By%0Aevaluating%20the%20similarity%20between%20the%20%24n%24-gram%20distribution%20and%20the%20one-hot%0Adistribution%20with%20LLMs%2C%20we%20observed%20that%20the%20%24n%24-gram%20distributions%20align%20more%0Aclosely%20with%20the%20output%20distribution%20of%20LLMs.%20Based%20on%20this%20insight%2C%20we%0Aintroduce%20Next%20Distribution%20Prediction%20%28NDP%29%2C%20which%20uses%20%24n%24-gram%20distributions%0Ato%20replace%20the%20one-hot%20targets%2C%20enhancing%20learning%20without%20extra%20online%0Atraining%20time.%20We%20conducted%20experiments%20across%20translation%2C%20general%20task%2C%0Alanguage%20transfer%2C%20and%20medical%20domain%20adaptation.%20Compared%20to%20NTP%2C%20NDP%20can%0Aachieve%20up%20to%20%2B2.97%20COMET%20improvement%20in%20translation%20tasks%2C%20%2B0.61%20average%0Aimprovement%20in%20general%20tasks%2C%20and%20incredible%20%2B10.75%20average%20improvement%20in%20the%0Amedical%20domain.%20This%20demonstrates%20the%20concrete%20benefits%20of%20addressing%20the%0Atarget%20narrowing%20problem%2C%20pointing%20to%20a%20new%20direction%20for%20future%20work%20on%0Aimproving%20NTP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNDP%253A%2520Next%2520Distribution%2520Prediction%2520as%2520a%2520More%2520Broad%2520Target%26entry.906535625%3DJunhao%2520Ruan%2520and%2520Abudukeyumu%2520Abudula%2520and%2520Xinyu%2520Liu%2520and%2520Bei%2520Li%2520and%2520Yinqiao%2520Li%2520and%2520Chenglong%2520Wang%2520and%2520Yuchun%2520Fan%2520and%2520Yuan%2520Ge%2520and%2520Tong%2520Xiao%2520and%2520Jingbo%2520Zhu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520trained%2520on%2520next-token%2520prediction%2520%2528NTP%2529%2520paradigm%250Ahave%2520demonstrated%2520powerful%2520capabilities.%2520However%252C%2520the%2520existing%2520NTP%2520paradigm%250Acontains%2520several%2520limitations%252C%2520particularly%2520related%2520to%2520planned%2520task%250Acomplications%2520and%2520error%2520propagation%2520during%2520inference.%2520In%2520our%2520work%252C%2520we%2520extend%250Athe%2520critique%2520of%2520NTP%252C%2520highlighting%2520its%2520limitation%2520also%2520due%2520to%2520training%2520with%2520a%250Anarrow%2520objective%253A%2520the%2520prediction%2520of%2520a%2520sub-optimal%2520one-hot%2520distribution.%2520To%250Asupport%2520this%2520critique%252C%2520we%2520conducted%2520a%2520pre-experiment%2520treating%2520the%2520output%250Adistribution%2520from%2520powerful%2520LLMs%2520as%2520efficient%2520world%2520data%2520compression.%2520By%250Aevaluating%2520the%2520similarity%2520between%2520the%2520%2524n%2524-gram%2520distribution%2520and%2520the%2520one-hot%250Adistribution%2520with%2520LLMs%252C%2520we%2520observed%2520that%2520the%2520%2524n%2524-gram%2520distributions%2520align%2520more%250Aclosely%2520with%2520the%2520output%2520distribution%2520of%2520LLMs.%2520Based%2520on%2520this%2520insight%252C%2520we%250Aintroduce%2520Next%2520Distribution%2520Prediction%2520%2528NDP%2529%252C%2520which%2520uses%2520%2524n%2524-gram%2520distributions%250Ato%2520replace%2520the%2520one-hot%2520targets%252C%2520enhancing%2520learning%2520without%2520extra%2520online%250Atraining%2520time.%2520We%2520conducted%2520experiments%2520across%2520translation%252C%2520general%2520task%252C%250Alanguage%2520transfer%252C%2520and%2520medical%2520domain%2520adaptation.%2520Compared%2520to%2520NTP%252C%2520NDP%2520can%250Aachieve%2520up%2520to%2520%252B2.97%2520COMET%2520improvement%2520in%2520translation%2520tasks%252C%2520%252B0.61%2520average%250Aimprovement%2520in%2520general%2520tasks%252C%2520and%2520incredible%2520%252B10.75%2520average%2520improvement%2520in%2520the%250Amedical%2520domain.%2520This%2520demonstrates%2520the%2520concrete%2520benefits%2520of%2520addressing%2520the%250Atarget%2520narrowing%2520problem%252C%2520pointing%2520to%2520a%2520new%2520direction%2520for%2520future%2520work%2520on%250Aimproving%2520NTP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NDP%3A%20Next%20Distribution%20Prediction%20as%20a%20More%20Broad%20Target&entry.906535625=Junhao%20Ruan%20and%20Abudukeyumu%20Abudula%20and%20Xinyu%20Liu%20and%20Bei%20Li%20and%20Yinqiao%20Li%20and%20Chenglong%20Wang%20and%20Yuchun%20Fan%20and%20Yuan%20Ge%20and%20Tong%20Xiao%20and%20Jingbo%20Zhu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20trained%20on%20next-token%20prediction%20%28NTP%29%20paradigm%0Ahave%20demonstrated%20powerful%20capabilities.%20However%2C%20the%20existing%20NTP%20paradigm%0Acontains%20several%20limitations%2C%20particularly%20related%20to%20planned%20task%0Acomplications%20and%20error%20propagation%20during%20inference.%20In%20our%20work%2C%20we%20extend%0Athe%20critique%20of%20NTP%2C%20highlighting%20its%20limitation%20also%20due%20to%20training%20with%20a%0Anarrow%20objective%3A%20the%20prediction%20of%20a%20sub-optimal%20one-hot%20distribution.%20To%0Asupport%20this%20critique%2C%20we%20conducted%20a%20pre-experiment%20treating%20the%20output%0Adistribution%20from%20powerful%20LLMs%20as%20efficient%20world%20data%20compression.%20By%0Aevaluating%20the%20similarity%20between%20the%20%24n%24-gram%20distribution%20and%20the%20one-hot%0Adistribution%20with%20LLMs%2C%20we%20observed%20that%20the%20%24n%24-gram%20distributions%20align%20more%0Aclosely%20with%20the%20output%20distribution%20of%20LLMs.%20Based%20on%20this%20insight%2C%20we%0Aintroduce%20Next%20Distribution%20Prediction%20%28NDP%29%2C%20which%20uses%20%24n%24-gram%20distributions%0Ato%20replace%20the%20one-hot%20targets%2C%20enhancing%20learning%20without%20extra%20online%0Atraining%20time.%20We%20conducted%20experiments%20across%20translation%2C%20general%20task%2C%0Alanguage%20transfer%2C%20and%20medical%20domain%20adaptation.%20Compared%20to%20NTP%2C%20NDP%20can%0Aachieve%20up%20to%20%2B2.97%20COMET%20improvement%20in%20translation%20tasks%2C%20%2B0.61%20average%0Aimprovement%20in%20general%20tasks%2C%20and%20incredible%20%2B10.75%20average%20improvement%20in%20the%0Amedical%20domain.%20This%20demonstrates%20the%20concrete%20benefits%20of%20addressing%20the%0Atarget%20narrowing%20problem%2C%20pointing%20to%20a%20new%20direction%20for%20future%20work%20on%0Aimproving%20NTP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17377v1&entry.124074799=Read"},
{"title": "C-RADAR: A Centralized Deep Learning System for Intrusion Detection in\n  Software Defined Networks", "author": "Osama Mustafa and Khizer Ali and Talha Naqash", "abstract": "  The popularity of Software Defined Networks (SDNs) has grown in recent years,\nmainly because of their ability to simplify network management and improve\nnetwork flexibility. However, this also makes them vulnerable to various types\nof cyber attacks. SDNs work on a centralized control plane which makes them\nmore prone to network attacks. Research has demonstrated that deep learning\n(DL) methods can be successful in identifying intrusions in conventional\nnetworks, but their application in SDNs is still an open research area. In this\nresearch, we propose the use of DL techniques for intrusion detection in SDNs.\nWe measure the effectiveness of our method by experimentation on a dataset of\nnetwork traffic and comparing it to existing techniques. Our results show that\nthe DL-based approach outperforms traditional methods in terms of detection\naccuracy and computational efficiency. The deep learning architecture that has\nbeen used in this research is a Long Short Term Memory Network and\nSelf-Attention based architecture i.e. LSTM-Attn which achieves an Fl-score of\n0.9721. Furthermore, this technique can be trained to detect new attack\npatterns and improve the overall security of SDNs.\n", "link": "http://arxiv.org/abs/2408.17356v1", "date": "2024-08-30", "relevancy": 1.9028, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5078}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4711}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C-RADAR%3A%20A%20Centralized%20Deep%20Learning%20System%20for%20Intrusion%20Detection%20in%0A%20%20Software%20Defined%20Networks&body=Title%3A%20C-RADAR%3A%20A%20Centralized%20Deep%20Learning%20System%20for%20Intrusion%20Detection%20in%0A%20%20Software%20Defined%20Networks%0AAuthor%3A%20Osama%20Mustafa%20and%20Khizer%20Ali%20and%20Talha%20Naqash%0AAbstract%3A%20%20%20The%20popularity%20of%20Software%20Defined%20Networks%20%28SDNs%29%20has%20grown%20in%20recent%20years%2C%0Amainly%20because%20of%20their%20ability%20to%20simplify%20network%20management%20and%20improve%0Anetwork%20flexibility.%20However%2C%20this%20also%20makes%20them%20vulnerable%20to%20various%20types%0Aof%20cyber%20attacks.%20SDNs%20work%20on%20a%20centralized%20control%20plane%20which%20makes%20them%0Amore%20prone%20to%20network%20attacks.%20Research%20has%20demonstrated%20that%20deep%20learning%0A%28DL%29%20methods%20can%20be%20successful%20in%20identifying%20intrusions%20in%20conventional%0Anetworks%2C%20but%20their%20application%20in%20SDNs%20is%20still%20an%20open%20research%20area.%20In%20this%0Aresearch%2C%20we%20propose%20the%20use%20of%20DL%20techniques%20for%20intrusion%20detection%20in%20SDNs.%0AWe%20measure%20the%20effectiveness%20of%20our%20method%20by%20experimentation%20on%20a%20dataset%20of%0Anetwork%20traffic%20and%20comparing%20it%20to%20existing%20techniques.%20Our%20results%20show%20that%0Athe%20DL-based%20approach%20outperforms%20traditional%20methods%20in%20terms%20of%20detection%0Aaccuracy%20and%20computational%20efficiency.%20The%20deep%20learning%20architecture%20that%20has%0Abeen%20used%20in%20this%20research%20is%20a%20Long%20Short%20Term%20Memory%20Network%20and%0ASelf-Attention%20based%20architecture%20i.e.%20LSTM-Attn%20which%20achieves%20an%20Fl-score%20of%0A0.9721.%20Furthermore%2C%20this%20technique%20can%20be%20trained%20to%20detect%20new%20attack%0Apatterns%20and%20improve%20the%20overall%20security%20of%20SDNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC-RADAR%253A%2520A%2520Centralized%2520Deep%2520Learning%2520System%2520for%2520Intrusion%2520Detection%2520in%250A%2520%2520Software%2520Defined%2520Networks%26entry.906535625%3DOsama%2520Mustafa%2520and%2520Khizer%2520Ali%2520and%2520Talha%2520Naqash%26entry.1292438233%3D%2520%2520The%2520popularity%2520of%2520Software%2520Defined%2520Networks%2520%2528SDNs%2529%2520has%2520grown%2520in%2520recent%2520years%252C%250Amainly%2520because%2520of%2520their%2520ability%2520to%2520simplify%2520network%2520management%2520and%2520improve%250Anetwork%2520flexibility.%2520However%252C%2520this%2520also%2520makes%2520them%2520vulnerable%2520to%2520various%2520types%250Aof%2520cyber%2520attacks.%2520SDNs%2520work%2520on%2520a%2520centralized%2520control%2520plane%2520which%2520makes%2520them%250Amore%2520prone%2520to%2520network%2520attacks.%2520Research%2520has%2520demonstrated%2520that%2520deep%2520learning%250A%2528DL%2529%2520methods%2520can%2520be%2520successful%2520in%2520identifying%2520intrusions%2520in%2520conventional%250Anetworks%252C%2520but%2520their%2520application%2520in%2520SDNs%2520is%2520still%2520an%2520open%2520research%2520area.%2520In%2520this%250Aresearch%252C%2520we%2520propose%2520the%2520use%2520of%2520DL%2520techniques%2520for%2520intrusion%2520detection%2520in%2520SDNs.%250AWe%2520measure%2520the%2520effectiveness%2520of%2520our%2520method%2520by%2520experimentation%2520on%2520a%2520dataset%2520of%250Anetwork%2520traffic%2520and%2520comparing%2520it%2520to%2520existing%2520techniques.%2520Our%2520results%2520show%2520that%250Athe%2520DL-based%2520approach%2520outperforms%2520traditional%2520methods%2520in%2520terms%2520of%2520detection%250Aaccuracy%2520and%2520computational%2520efficiency.%2520The%2520deep%2520learning%2520architecture%2520that%2520has%250Abeen%2520used%2520in%2520this%2520research%2520is%2520a%2520Long%2520Short%2520Term%2520Memory%2520Network%2520and%250ASelf-Attention%2520based%2520architecture%2520i.e.%2520LSTM-Attn%2520which%2520achieves%2520an%2520Fl-score%2520of%250A0.9721.%2520Furthermore%252C%2520this%2520technique%2520can%2520be%2520trained%2520to%2520detect%2520new%2520attack%250Apatterns%2520and%2520improve%2520the%2520overall%2520security%2520of%2520SDNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C-RADAR%3A%20A%20Centralized%20Deep%20Learning%20System%20for%20Intrusion%20Detection%20in%0A%20%20Software%20Defined%20Networks&entry.906535625=Osama%20Mustafa%20and%20Khizer%20Ali%20and%20Talha%20Naqash&entry.1292438233=%20%20The%20popularity%20of%20Software%20Defined%20Networks%20%28SDNs%29%20has%20grown%20in%20recent%20years%2C%0Amainly%20because%20of%20their%20ability%20to%20simplify%20network%20management%20and%20improve%0Anetwork%20flexibility.%20However%2C%20this%20also%20makes%20them%20vulnerable%20to%20various%20types%0Aof%20cyber%20attacks.%20SDNs%20work%20on%20a%20centralized%20control%20plane%20which%20makes%20them%0Amore%20prone%20to%20network%20attacks.%20Research%20has%20demonstrated%20that%20deep%20learning%0A%28DL%29%20methods%20can%20be%20successful%20in%20identifying%20intrusions%20in%20conventional%0Anetworks%2C%20but%20their%20application%20in%20SDNs%20is%20still%20an%20open%20research%20area.%20In%20this%0Aresearch%2C%20we%20propose%20the%20use%20of%20DL%20techniques%20for%20intrusion%20detection%20in%20SDNs.%0AWe%20measure%20the%20effectiveness%20of%20our%20method%20by%20experimentation%20on%20a%20dataset%20of%0Anetwork%20traffic%20and%20comparing%20it%20to%20existing%20techniques.%20Our%20results%20show%20that%0Athe%20DL-based%20approach%20outperforms%20traditional%20methods%20in%20terms%20of%20detection%0Aaccuracy%20and%20computational%20efficiency.%20The%20deep%20learning%20architecture%20that%20has%0Abeen%20used%20in%20this%20research%20is%20a%20Long%20Short%20Term%20Memory%20Network%20and%0ASelf-Attention%20based%20architecture%20i.e.%20LSTM-Attn%20which%20achieves%20an%20Fl-score%20of%0A0.9721.%20Furthermore%2C%20this%20technique%20can%20be%20trained%20to%20detect%20new%20attack%0Apatterns%20and%20improve%20the%20overall%20security%20of%20SDNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17356v1&entry.124074799=Read"},
{"title": "MoRe Fine-Tuning with 10x Fewer Parameters", "author": "Wenxuan Tan and Nicholas Roberts and Tzu-Heng Huang and Jitian Zhao and John Cooper and Samuel Guo and Chengyu Duan and Frederic Sala", "abstract": "  Parameter-efficient fine-tuning (PEFT) techniques have unlocked the potential\nto cheaply and easily specialize large pretrained models. However, the most\nprominent approaches, like low-rank adapters (LoRA), depend on heuristics or\nrules-of-thumb for their architectural choices -- potentially limiting their\nperformance for new models and architectures. This limitation suggests that\ntechniques from neural architecture search could be used to obtain optimal\nadapter architectures, but these are often expensive and difficult to\nimplement. We address this challenge with Monarch Rectangular Fine-tuning\n(MoRe), a simple framework to search over adapter architectures that relies on\nthe Monarch matrix class. Theoretically, we show that MoRe is more expressive\nthan LoRA. Empirically, our approach is more parameter-efficient and performant\nthan state-of-the-art PEFTs on a range of tasks and models, with as few as 5\\%\nof LoRA's parameters.\n", "link": "http://arxiv.org/abs/2408.17383v1", "date": "2024-08-30", "relevancy": 1.9009, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4875}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4672}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoRe%20Fine-Tuning%20with%2010x%20Fewer%20Parameters&body=Title%3A%20MoRe%20Fine-Tuning%20with%2010x%20Fewer%20Parameters%0AAuthor%3A%20Wenxuan%20Tan%20and%20Nicholas%20Roberts%20and%20Tzu-Heng%20Huang%20and%20Jitian%20Zhao%20and%20John%20Cooper%20and%20Samuel%20Guo%20and%20Chengyu%20Duan%20and%20Frederic%20Sala%0AAbstract%3A%20%20%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20techniques%20have%20unlocked%20the%20potential%0Ato%20cheaply%20and%20easily%20specialize%20large%20pretrained%20models.%20However%2C%20the%20most%0Aprominent%20approaches%2C%20like%20low-rank%20adapters%20%28LoRA%29%2C%20depend%20on%20heuristics%20or%0Arules-of-thumb%20for%20their%20architectural%20choices%20--%20potentially%20limiting%20their%0Aperformance%20for%20new%20models%20and%20architectures.%20This%20limitation%20suggests%20that%0Atechniques%20from%20neural%20architecture%20search%20could%20be%20used%20to%20obtain%20optimal%0Aadapter%20architectures%2C%20but%20these%20are%20often%20expensive%20and%20difficult%20to%0Aimplement.%20We%20address%20this%20challenge%20with%20Monarch%20Rectangular%20Fine-tuning%0A%28MoRe%29%2C%20a%20simple%20framework%20to%20search%20over%20adapter%20architectures%20that%20relies%20on%0Athe%20Monarch%20matrix%20class.%20Theoretically%2C%20we%20show%20that%20MoRe%20is%20more%20expressive%0Athan%20LoRA.%20Empirically%2C%20our%20approach%20is%20more%20parameter-efficient%20and%20performant%0Athan%20state-of-the-art%20PEFTs%20on%20a%20range%20of%20tasks%20and%20models%2C%20with%20as%20few%20as%205%5C%25%0Aof%20LoRA%27s%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoRe%2520Fine-Tuning%2520with%252010x%2520Fewer%2520Parameters%26entry.906535625%3DWenxuan%2520Tan%2520and%2520Nicholas%2520Roberts%2520and%2520Tzu-Heng%2520Huang%2520and%2520Jitian%2520Zhao%2520and%2520John%2520Cooper%2520and%2520Samuel%2520Guo%2520and%2520Chengyu%2520Duan%2520and%2520Frederic%2520Sala%26entry.1292438233%3D%2520%2520Parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520techniques%2520have%2520unlocked%2520the%2520potential%250Ato%2520cheaply%2520and%2520easily%2520specialize%2520large%2520pretrained%2520models.%2520However%252C%2520the%2520most%250Aprominent%2520approaches%252C%2520like%2520low-rank%2520adapters%2520%2528LoRA%2529%252C%2520depend%2520on%2520heuristics%2520or%250Arules-of-thumb%2520for%2520their%2520architectural%2520choices%2520--%2520potentially%2520limiting%2520their%250Aperformance%2520for%2520new%2520models%2520and%2520architectures.%2520This%2520limitation%2520suggests%2520that%250Atechniques%2520from%2520neural%2520architecture%2520search%2520could%2520be%2520used%2520to%2520obtain%2520optimal%250Aadapter%2520architectures%252C%2520but%2520these%2520are%2520often%2520expensive%2520and%2520difficult%2520to%250Aimplement.%2520We%2520address%2520this%2520challenge%2520with%2520Monarch%2520Rectangular%2520Fine-tuning%250A%2528MoRe%2529%252C%2520a%2520simple%2520framework%2520to%2520search%2520over%2520adapter%2520architectures%2520that%2520relies%2520on%250Athe%2520Monarch%2520matrix%2520class.%2520Theoretically%252C%2520we%2520show%2520that%2520MoRe%2520is%2520more%2520expressive%250Athan%2520LoRA.%2520Empirically%252C%2520our%2520approach%2520is%2520more%2520parameter-efficient%2520and%2520performant%250Athan%2520state-of-the-art%2520PEFTs%2520on%2520a%2520range%2520of%2520tasks%2520and%2520models%252C%2520with%2520as%2520few%2520as%25205%255C%2525%250Aof%2520LoRA%2527s%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoRe%20Fine-Tuning%20with%2010x%20Fewer%20Parameters&entry.906535625=Wenxuan%20Tan%20and%20Nicholas%20Roberts%20and%20Tzu-Heng%20Huang%20and%20Jitian%20Zhao%20and%20John%20Cooper%20and%20Samuel%20Guo%20and%20Chengyu%20Duan%20and%20Frederic%20Sala&entry.1292438233=%20%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20techniques%20have%20unlocked%20the%20potential%0Ato%20cheaply%20and%20easily%20specialize%20large%20pretrained%20models.%20However%2C%20the%20most%0Aprominent%20approaches%2C%20like%20low-rank%20adapters%20%28LoRA%29%2C%20depend%20on%20heuristics%20or%0Arules-of-thumb%20for%20their%20architectural%20choices%20--%20potentially%20limiting%20their%0Aperformance%20for%20new%20models%20and%20architectures.%20This%20limitation%20suggests%20that%0Atechniques%20from%20neural%20architecture%20search%20could%20be%20used%20to%20obtain%20optimal%0Aadapter%20architectures%2C%20but%20these%20are%20often%20expensive%20and%20difficult%20to%0Aimplement.%20We%20address%20this%20challenge%20with%20Monarch%20Rectangular%20Fine-tuning%0A%28MoRe%29%2C%20a%20simple%20framework%20to%20search%20over%20adapter%20architectures%20that%20relies%20on%0Athe%20Monarch%20matrix%20class.%20Theoretically%2C%20we%20show%20that%20MoRe%20is%20more%20expressive%0Athan%20LoRA.%20Empirically%2C%20our%20approach%20is%20more%20parameter-efficient%20and%20performant%0Athan%20state-of-the-art%20PEFTs%20on%20a%20range%20of%20tasks%20and%20models%2C%20with%20as%20few%20as%205%5C%25%0Aof%20LoRA%27s%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17383v1&entry.124074799=Read"},
{"title": "Object-Centric Diffusion for Efficient Video Editing", "author": "Kumara Kahatapitiya and Adil Karjauv and Davide Abati and Fatih Porikli and Yuki M. Asano and Amirhossein Habibian", "abstract": "  Diffusion-based video editing have reached impressive quality and can\ntransform either the global style, local structure, and attributes of given\nvideo inputs, following textual edit prompts. However, such solutions typically\nincur heavy memory and computational costs to generate temporally-coherent\nframes, either in the form of diffusion inversion and/or cross-frame attention.\nIn this paper, we conduct an analysis of such inefficiencies, and suggest\nsimple yet effective modifications that allow significant speed-ups whilst\nmaintaining quality. Moreover, we introduce Object-Centric Diffusion, to fix\ngeneration artifacts and further reduce latency by allocating more computations\ntowards foreground edited regions, arguably more important for perceptual\nquality. We achieve this by two novel proposals: i) Object-Centric Sampling,\ndecoupling the diffusion steps spent on salient or background regions and\nspending most on the former, and ii) Object-Centric Token Merging, which\nreduces cost of cross-frame attention by fusing redundant tokens in unimportant\nbackground regions. Both techniques are readily applicable to a given video\nediting model without retraining, and can drastically reduce its memory and\ncomputational cost. We evaluate our proposals on inversion-based and\ncontrol-signal-based editing pipelines, and show a latency reduction up to 10x\nfor a comparable synthesis quality. Project page:\nqualcomm-ai-research.github.io/object-centric-diffusion.\n", "link": "http://arxiv.org/abs/2401.05735v3", "date": "2024-08-30", "relevancy": 1.8888, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6738}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6204}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-Centric%20Diffusion%20for%20Efficient%20Video%20Editing&body=Title%3A%20Object-Centric%20Diffusion%20for%20Efficient%20Video%20Editing%0AAuthor%3A%20Kumara%20Kahatapitiya%20and%20Adil%20Karjauv%20and%20Davide%20Abati%20and%20Fatih%20Porikli%20and%20Yuki%20M.%20Asano%20and%20Amirhossein%20Habibian%0AAbstract%3A%20%20%20Diffusion-based%20video%20editing%20have%20reached%20impressive%20quality%20and%20can%0Atransform%20either%20the%20global%20style%2C%20local%20structure%2C%20and%20attributes%20of%20given%0Avideo%20inputs%2C%20following%20textual%20edit%20prompts.%20However%2C%20such%20solutions%20typically%0Aincur%20heavy%20memory%20and%20computational%20costs%20to%20generate%20temporally-coherent%0Aframes%2C%20either%20in%20the%20form%20of%20diffusion%20inversion%20and/or%20cross-frame%20attention.%0AIn%20this%20paper%2C%20we%20conduct%20an%20analysis%20of%20such%20inefficiencies%2C%20and%20suggest%0Asimple%20yet%20effective%20modifications%20that%20allow%20significant%20speed-ups%20whilst%0Amaintaining%20quality.%20Moreover%2C%20we%20introduce%20Object-Centric%20Diffusion%2C%20to%20fix%0Ageneration%20artifacts%20and%20further%20reduce%20latency%20by%20allocating%20more%20computations%0Atowards%20foreground%20edited%20regions%2C%20arguably%20more%20important%20for%20perceptual%0Aquality.%20We%20achieve%20this%20by%20two%20novel%20proposals%3A%20i%29%20Object-Centric%20Sampling%2C%0Adecoupling%20the%20diffusion%20steps%20spent%20on%20salient%20or%20background%20regions%20and%0Aspending%20most%20on%20the%20former%2C%20and%20ii%29%20Object-Centric%20Token%20Merging%2C%20which%0Areduces%20cost%20of%20cross-frame%20attention%20by%20fusing%20redundant%20tokens%20in%20unimportant%0Abackground%20regions.%20Both%20techniques%20are%20readily%20applicable%20to%20a%20given%20video%0Aediting%20model%20without%20retraining%2C%20and%20can%20drastically%20reduce%20its%20memory%20and%0Acomputational%20cost.%20We%20evaluate%20our%20proposals%20on%20inversion-based%20and%0Acontrol-signal-based%20editing%20pipelines%2C%20and%20show%20a%20latency%20reduction%20up%20to%2010x%0Afor%20a%20comparable%20synthesis%20quality.%20Project%20page%3A%0Aqualcomm-ai-research.github.io/object-centric-diffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.05735v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-Centric%2520Diffusion%2520for%2520Efficient%2520Video%2520Editing%26entry.906535625%3DKumara%2520Kahatapitiya%2520and%2520Adil%2520Karjauv%2520and%2520Davide%2520Abati%2520and%2520Fatih%2520Porikli%2520and%2520Yuki%2520M.%2520Asano%2520and%2520Amirhossein%2520Habibian%26entry.1292438233%3D%2520%2520Diffusion-based%2520video%2520editing%2520have%2520reached%2520impressive%2520quality%2520and%2520can%250Atransform%2520either%2520the%2520global%2520style%252C%2520local%2520structure%252C%2520and%2520attributes%2520of%2520given%250Avideo%2520inputs%252C%2520following%2520textual%2520edit%2520prompts.%2520However%252C%2520such%2520solutions%2520typically%250Aincur%2520heavy%2520memory%2520and%2520computational%2520costs%2520to%2520generate%2520temporally-coherent%250Aframes%252C%2520either%2520in%2520the%2520form%2520of%2520diffusion%2520inversion%2520and/or%2520cross-frame%2520attention.%250AIn%2520this%2520paper%252C%2520we%2520conduct%2520an%2520analysis%2520of%2520such%2520inefficiencies%252C%2520and%2520suggest%250Asimple%2520yet%2520effective%2520modifications%2520that%2520allow%2520significant%2520speed-ups%2520whilst%250Amaintaining%2520quality.%2520Moreover%252C%2520we%2520introduce%2520Object-Centric%2520Diffusion%252C%2520to%2520fix%250Ageneration%2520artifacts%2520and%2520further%2520reduce%2520latency%2520by%2520allocating%2520more%2520computations%250Atowards%2520foreground%2520edited%2520regions%252C%2520arguably%2520more%2520important%2520for%2520perceptual%250Aquality.%2520We%2520achieve%2520this%2520by%2520two%2520novel%2520proposals%253A%2520i%2529%2520Object-Centric%2520Sampling%252C%250Adecoupling%2520the%2520diffusion%2520steps%2520spent%2520on%2520salient%2520or%2520background%2520regions%2520and%250Aspending%2520most%2520on%2520the%2520former%252C%2520and%2520ii%2529%2520Object-Centric%2520Token%2520Merging%252C%2520which%250Areduces%2520cost%2520of%2520cross-frame%2520attention%2520by%2520fusing%2520redundant%2520tokens%2520in%2520unimportant%250Abackground%2520regions.%2520Both%2520techniques%2520are%2520readily%2520applicable%2520to%2520a%2520given%2520video%250Aediting%2520model%2520without%2520retraining%252C%2520and%2520can%2520drastically%2520reduce%2520its%2520memory%2520and%250Acomputational%2520cost.%2520We%2520evaluate%2520our%2520proposals%2520on%2520inversion-based%2520and%250Acontrol-signal-based%2520editing%2520pipelines%252C%2520and%2520show%2520a%2520latency%2520reduction%2520up%2520to%252010x%250Afor%2520a%2520comparable%2520synthesis%2520quality.%2520Project%2520page%253A%250Aqualcomm-ai-research.github.io/object-centric-diffusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.05735v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-Centric%20Diffusion%20for%20Efficient%20Video%20Editing&entry.906535625=Kumara%20Kahatapitiya%20and%20Adil%20Karjauv%20and%20Davide%20Abati%20and%20Fatih%20Porikli%20and%20Yuki%20M.%20Asano%20and%20Amirhossein%20Habibian&entry.1292438233=%20%20Diffusion-based%20video%20editing%20have%20reached%20impressive%20quality%20and%20can%0Atransform%20either%20the%20global%20style%2C%20local%20structure%2C%20and%20attributes%20of%20given%0Avideo%20inputs%2C%20following%20textual%20edit%20prompts.%20However%2C%20such%20solutions%20typically%0Aincur%20heavy%20memory%20and%20computational%20costs%20to%20generate%20temporally-coherent%0Aframes%2C%20either%20in%20the%20form%20of%20diffusion%20inversion%20and/or%20cross-frame%20attention.%0AIn%20this%20paper%2C%20we%20conduct%20an%20analysis%20of%20such%20inefficiencies%2C%20and%20suggest%0Asimple%20yet%20effective%20modifications%20that%20allow%20significant%20speed-ups%20whilst%0Amaintaining%20quality.%20Moreover%2C%20we%20introduce%20Object-Centric%20Diffusion%2C%20to%20fix%0Ageneration%20artifacts%20and%20further%20reduce%20latency%20by%20allocating%20more%20computations%0Atowards%20foreground%20edited%20regions%2C%20arguably%20more%20important%20for%20perceptual%0Aquality.%20We%20achieve%20this%20by%20two%20novel%20proposals%3A%20i%29%20Object-Centric%20Sampling%2C%0Adecoupling%20the%20diffusion%20steps%20spent%20on%20salient%20or%20background%20regions%20and%0Aspending%20most%20on%20the%20former%2C%20and%20ii%29%20Object-Centric%20Token%20Merging%2C%20which%0Areduces%20cost%20of%20cross-frame%20attention%20by%20fusing%20redundant%20tokens%20in%20unimportant%0Abackground%20regions.%20Both%20techniques%20are%20readily%20applicable%20to%20a%20given%20video%0Aediting%20model%20without%20retraining%2C%20and%20can%20drastically%20reduce%20its%20memory%20and%0Acomputational%20cost.%20We%20evaluate%20our%20proposals%20on%20inversion-based%20and%0Acontrol-signal-based%20editing%20pipelines%2C%20and%20show%20a%20latency%20reduction%20up%20to%2010x%0Afor%20a%20comparable%20synthesis%20quality.%20Project%20page%3A%0Aqualcomm-ai-research.github.io/object-centric-diffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.05735v3&entry.124074799=Read"},
{"title": "Learning and Verifying Maximal Taylor-Neural Lyapunov functions", "author": "Matthieu Barreau and Nicola Bastianello", "abstract": "  We introduce a novel neural network architecture, termed Taylor-neural\nLyapunov functions, designed to approximate Lyapunov functions with formal\ncertification. This architecture innovatively encodes local approximations and\nextends them globally by leveraging neural networks to approximate the\nresiduals. Our method recasts the problem of estimating the largest region of\nattraction - specifically for maximal Lyapunov functions - into a learning\nproblem, ensuring convergence around the origin through robust control theory.\nPhysics-informed machine learning techniques further refine the estimation of\nthe largest region of attraction. Remarkably, this method is versatile,\noperating effectively even without simulated data points. We validate the\nefficacy of our approach by providing numerical certificates of convergence\nacross multiple examples. Our proposed methodology not only competes closely\nwith state-of-the-art approaches, such as sum-of-squares and LyZNet, but also\nachieves comparable results even in the absence of simulated data. This work\nrepresents a significant advancement in control theory, with broad potential\napplications in the design of stable control systems and beyond.\n", "link": "http://arxiv.org/abs/2408.17246v1", "date": "2024-08-30", "relevancy": 1.8864, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.489}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.469}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20and%20Verifying%20Maximal%20Taylor-Neural%20Lyapunov%20functions&body=Title%3A%20Learning%20and%20Verifying%20Maximal%20Taylor-Neural%20Lyapunov%20functions%0AAuthor%3A%20Matthieu%20Barreau%20and%20Nicola%20Bastianello%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20neural%20network%20architecture%2C%20termed%20Taylor-neural%0ALyapunov%20functions%2C%20designed%20to%20approximate%20Lyapunov%20functions%20with%20formal%0Acertification.%20This%20architecture%20innovatively%20encodes%20local%20approximations%20and%0Aextends%20them%20globally%20by%20leveraging%20neural%20networks%20to%20approximate%20the%0Aresiduals.%20Our%20method%20recasts%20the%20problem%20of%20estimating%20the%20largest%20region%20of%0Aattraction%20-%20specifically%20for%20maximal%20Lyapunov%20functions%20-%20into%20a%20learning%0Aproblem%2C%20ensuring%20convergence%20around%20the%20origin%20through%20robust%20control%20theory.%0APhysics-informed%20machine%20learning%20techniques%20further%20refine%20the%20estimation%20of%0Athe%20largest%20region%20of%20attraction.%20Remarkably%2C%20this%20method%20is%20versatile%2C%0Aoperating%20effectively%20even%20without%20simulated%20data%20points.%20We%20validate%20the%0Aefficacy%20of%20our%20approach%20by%20providing%20numerical%20certificates%20of%20convergence%0Aacross%20multiple%20examples.%20Our%20proposed%20methodology%20not%20only%20competes%20closely%0Awith%20state-of-the-art%20approaches%2C%20such%20as%20sum-of-squares%20and%20LyZNet%2C%20but%20also%0Aachieves%20comparable%20results%20even%20in%20the%20absence%20of%20simulated%20data.%20This%20work%0Arepresents%20a%20significant%20advancement%20in%20control%20theory%2C%20with%20broad%20potential%0Aapplications%20in%20the%20design%20of%20stable%20control%20systems%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520and%2520Verifying%2520Maximal%2520Taylor-Neural%2520Lyapunov%2520functions%26entry.906535625%3DMatthieu%2520Barreau%2520and%2520Nicola%2520Bastianello%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520neural%2520network%2520architecture%252C%2520termed%2520Taylor-neural%250ALyapunov%2520functions%252C%2520designed%2520to%2520approximate%2520Lyapunov%2520functions%2520with%2520formal%250Acertification.%2520This%2520architecture%2520innovatively%2520encodes%2520local%2520approximations%2520and%250Aextends%2520them%2520globally%2520by%2520leveraging%2520neural%2520networks%2520to%2520approximate%2520the%250Aresiduals.%2520Our%2520method%2520recasts%2520the%2520problem%2520of%2520estimating%2520the%2520largest%2520region%2520of%250Aattraction%2520-%2520specifically%2520for%2520maximal%2520Lyapunov%2520functions%2520-%2520into%2520a%2520learning%250Aproblem%252C%2520ensuring%2520convergence%2520around%2520the%2520origin%2520through%2520robust%2520control%2520theory.%250APhysics-informed%2520machine%2520learning%2520techniques%2520further%2520refine%2520the%2520estimation%2520of%250Athe%2520largest%2520region%2520of%2520attraction.%2520Remarkably%252C%2520this%2520method%2520is%2520versatile%252C%250Aoperating%2520effectively%2520even%2520without%2520simulated%2520data%2520points.%2520We%2520validate%2520the%250Aefficacy%2520of%2520our%2520approach%2520by%2520providing%2520numerical%2520certificates%2520of%2520convergence%250Aacross%2520multiple%2520examples.%2520Our%2520proposed%2520methodology%2520not%2520only%2520competes%2520closely%250Awith%2520state-of-the-art%2520approaches%252C%2520such%2520as%2520sum-of-squares%2520and%2520LyZNet%252C%2520but%2520also%250Aachieves%2520comparable%2520results%2520even%2520in%2520the%2520absence%2520of%2520simulated%2520data.%2520This%2520work%250Arepresents%2520a%2520significant%2520advancement%2520in%2520control%2520theory%252C%2520with%2520broad%2520potential%250Aapplications%2520in%2520the%2520design%2520of%2520stable%2520control%2520systems%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20and%20Verifying%20Maximal%20Taylor-Neural%20Lyapunov%20functions&entry.906535625=Matthieu%20Barreau%20and%20Nicola%20Bastianello&entry.1292438233=%20%20We%20introduce%20a%20novel%20neural%20network%20architecture%2C%20termed%20Taylor-neural%0ALyapunov%20functions%2C%20designed%20to%20approximate%20Lyapunov%20functions%20with%20formal%0Acertification.%20This%20architecture%20innovatively%20encodes%20local%20approximations%20and%0Aextends%20them%20globally%20by%20leveraging%20neural%20networks%20to%20approximate%20the%0Aresiduals.%20Our%20method%20recasts%20the%20problem%20of%20estimating%20the%20largest%20region%20of%0Aattraction%20-%20specifically%20for%20maximal%20Lyapunov%20functions%20-%20into%20a%20learning%0Aproblem%2C%20ensuring%20convergence%20around%20the%20origin%20through%20robust%20control%20theory.%0APhysics-informed%20machine%20learning%20techniques%20further%20refine%20the%20estimation%20of%0Athe%20largest%20region%20of%20attraction.%20Remarkably%2C%20this%20method%20is%20versatile%2C%0Aoperating%20effectively%20even%20without%20simulated%20data%20points.%20We%20validate%20the%0Aefficacy%20of%20our%20approach%20by%20providing%20numerical%20certificates%20of%20convergence%0Aacross%20multiple%20examples.%20Our%20proposed%20methodology%20not%20only%20competes%20closely%0Awith%20state-of-the-art%20approaches%2C%20such%20as%20sum-of-squares%20and%20LyZNet%2C%20but%20also%0Aachieves%20comparable%20results%20even%20in%20the%20absence%20of%20simulated%20data.%20This%20work%0Arepresents%20a%20significant%20advancement%20in%20control%20theory%2C%20with%20broad%20potential%0Aapplications%20in%20the%20design%20of%20stable%20control%20systems%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17246v1&entry.124074799=Read"},
{"title": "Self-supervised learning for crystal property prediction via denoising", "author": "Alexander New and Nam Q. Le and Michael J. Pekala and Christopher D. Stiles", "abstract": "  Accurate prediction of the properties of crystalline materials is crucial for\ntargeted discovery, and this prediction is increasingly done with data-driven\nmodels. However, for many properties of interest, the number of materials for\nwhich a specific property has been determined is much smaller than the number\nof known materials. To overcome this disparity, we propose a novel\nself-supervised learning (SSL) strategy for material property prediction. Our\napproach, crystal denoising self-supervised learning (CDSSL), pretrains\npredictive models (e.g., graph networks) with a pretext task based on\nrecovering valid material structures when given perturbed versions of these\nstructures. We demonstrate that CDSSL models out-perform models trained without\nSSL, across material types, properties, and dataset sizes.\n", "link": "http://arxiv.org/abs/2408.17255v1", "date": "2024-08-30", "relevancy": 1.8843, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4782}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4676}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20learning%20for%20crystal%20property%20prediction%20via%20denoising&body=Title%3A%20Self-supervised%20learning%20for%20crystal%20property%20prediction%20via%20denoising%0AAuthor%3A%20Alexander%20New%20and%20Nam%20Q.%20Le%20and%20Michael%20J.%20Pekala%20and%20Christopher%20D.%20Stiles%0AAbstract%3A%20%20%20Accurate%20prediction%20of%20the%20properties%20of%20crystalline%20materials%20is%20crucial%20for%0Atargeted%20discovery%2C%20and%20this%20prediction%20is%20increasingly%20done%20with%20data-driven%0Amodels.%20However%2C%20for%20many%20properties%20of%20interest%2C%20the%20number%20of%20materials%20for%0Awhich%20a%20specific%20property%20has%20been%20determined%20is%20much%20smaller%20than%20the%20number%0Aof%20known%20materials.%20To%20overcome%20this%20disparity%2C%20we%20propose%20a%20novel%0Aself-supervised%20learning%20%28SSL%29%20strategy%20for%20material%20property%20prediction.%20Our%0Aapproach%2C%20crystal%20denoising%20self-supervised%20learning%20%28CDSSL%29%2C%20pretrains%0Apredictive%20models%20%28e.g.%2C%20graph%20networks%29%20with%20a%20pretext%20task%20based%20on%0Arecovering%20valid%20material%20structures%20when%20given%20perturbed%20versions%20of%20these%0Astructures.%20We%20demonstrate%20that%20CDSSL%20models%20out-perform%20models%20trained%20without%0ASSL%2C%20across%20material%20types%2C%20properties%2C%20and%20dataset%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520learning%2520for%2520crystal%2520property%2520prediction%2520via%2520denoising%26entry.906535625%3DAlexander%2520New%2520and%2520Nam%2520Q.%2520Le%2520and%2520Michael%2520J.%2520Pekala%2520and%2520Christopher%2520D.%2520Stiles%26entry.1292438233%3D%2520%2520Accurate%2520prediction%2520of%2520the%2520properties%2520of%2520crystalline%2520materials%2520is%2520crucial%2520for%250Atargeted%2520discovery%252C%2520and%2520this%2520prediction%2520is%2520increasingly%2520done%2520with%2520data-driven%250Amodels.%2520However%252C%2520for%2520many%2520properties%2520of%2520interest%252C%2520the%2520number%2520of%2520materials%2520for%250Awhich%2520a%2520specific%2520property%2520has%2520been%2520determined%2520is%2520much%2520smaller%2520than%2520the%2520number%250Aof%2520known%2520materials.%2520To%2520overcome%2520this%2520disparity%252C%2520we%2520propose%2520a%2520novel%250Aself-supervised%2520learning%2520%2528SSL%2529%2520strategy%2520for%2520material%2520property%2520prediction.%2520Our%250Aapproach%252C%2520crystal%2520denoising%2520self-supervised%2520learning%2520%2528CDSSL%2529%252C%2520pretrains%250Apredictive%2520models%2520%2528e.g.%252C%2520graph%2520networks%2529%2520with%2520a%2520pretext%2520task%2520based%2520on%250Arecovering%2520valid%2520material%2520structures%2520when%2520given%2520perturbed%2520versions%2520of%2520these%250Astructures.%2520We%2520demonstrate%2520that%2520CDSSL%2520models%2520out-perform%2520models%2520trained%2520without%250ASSL%252C%2520across%2520material%2520types%252C%2520properties%252C%2520and%2520dataset%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20learning%20for%20crystal%20property%20prediction%20via%20denoising&entry.906535625=Alexander%20New%20and%20Nam%20Q.%20Le%20and%20Michael%20J.%20Pekala%20and%20Christopher%20D.%20Stiles&entry.1292438233=%20%20Accurate%20prediction%20of%20the%20properties%20of%20crystalline%20materials%20is%20crucial%20for%0Atargeted%20discovery%2C%20and%20this%20prediction%20is%20increasingly%20done%20with%20data-driven%0Amodels.%20However%2C%20for%20many%20properties%20of%20interest%2C%20the%20number%20of%20materials%20for%0Awhich%20a%20specific%20property%20has%20been%20determined%20is%20much%20smaller%20than%20the%20number%0Aof%20known%20materials.%20To%20overcome%20this%20disparity%2C%20we%20propose%20a%20novel%0Aself-supervised%20learning%20%28SSL%29%20strategy%20for%20material%20property%20prediction.%20Our%0Aapproach%2C%20crystal%20denoising%20self-supervised%20learning%20%28CDSSL%29%2C%20pretrains%0Apredictive%20models%20%28e.g.%2C%20graph%20networks%29%20with%20a%20pretext%20task%20based%20on%0Arecovering%20valid%20material%20structures%20when%20given%20perturbed%20versions%20of%20these%0Astructures.%20We%20demonstrate%20that%20CDSSL%20models%20out-perform%20models%20trained%20without%0ASSL%2C%20across%20material%20types%2C%20properties%2C%20and%20dataset%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17255v1&entry.124074799=Read"},
{"title": "On the Curse of Memory in Recurrent Neural Networks: Approximation and\n  Optimization Analysis", "author": "Zhong Li and Jiequn Han and Weinan E and Qianxiao Li", "abstract": "  We study the approximation properties and optimization dynamics of recurrent\nneural networks (RNNs) when applied to learn input-output relationships in\ntemporal data. We consider the simple but representative setting of using\ncontinuous-time linear RNNs to learn from data generated by linear\nrelationships. Mathematically, the latter can be understood as a sequence of\nlinear functionals. We prove a universal approximation theorem of such linear\nfunctionals, and characterize the approximation rate and its relation with\nmemory. Moreover, we perform a fine-grained dynamical analysis of training\nlinear RNNs, which further reveal the intricate interactions between memory and\nlearning. A unifying theme uncovered is the non-trivial effect of memory, a\nnotion that can be made precise in our framework, on approximation and\noptimization: when there is long term memory in the target, it takes a large\nnumber of neurons to approximate it. Moreover, the training process will suffer\nfrom slow downs. In particular, both of these effects become exponentially more\npronounced with memory - a phenomenon we call the \"curse of memory\". These\nanalyses represent a basic step towards a concrete mathematical understanding\nof new phenomenon that may arise in learning temporal relationships using\nrecurrent architectures.\n", "link": "http://arxiv.org/abs/2009.07799v3", "date": "2024-08-30", "relevancy": 1.8713, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4752}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4636}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Curse%20of%20Memory%20in%20Recurrent%20Neural%20Networks%3A%20Approximation%20and%0A%20%20Optimization%20Analysis&body=Title%3A%20On%20the%20Curse%20of%20Memory%20in%20Recurrent%20Neural%20Networks%3A%20Approximation%20and%0A%20%20Optimization%20Analysis%0AAuthor%3A%20Zhong%20Li%20and%20Jiequn%20Han%20and%20Weinan%20E%20and%20Qianxiao%20Li%0AAbstract%3A%20%20%20We%20study%20the%20approximation%20properties%20and%20optimization%20dynamics%20of%20recurrent%0Aneural%20networks%20%28RNNs%29%20when%20applied%20to%20learn%20input-output%20relationships%20in%0Atemporal%20data.%20We%20consider%20the%20simple%20but%20representative%20setting%20of%20using%0Acontinuous-time%20linear%20RNNs%20to%20learn%20from%20data%20generated%20by%20linear%0Arelationships.%20Mathematically%2C%20the%20latter%20can%20be%20understood%20as%20a%20sequence%20of%0Alinear%20functionals.%20We%20prove%20a%20universal%20approximation%20theorem%20of%20such%20linear%0Afunctionals%2C%20and%20characterize%20the%20approximation%20rate%20and%20its%20relation%20with%0Amemory.%20Moreover%2C%20we%20perform%20a%20fine-grained%20dynamical%20analysis%20of%20training%0Alinear%20RNNs%2C%20which%20further%20reveal%20the%20intricate%20interactions%20between%20memory%20and%0Alearning.%20A%20unifying%20theme%20uncovered%20is%20the%20non-trivial%20effect%20of%20memory%2C%20a%0Anotion%20that%20can%20be%20made%20precise%20in%20our%20framework%2C%20on%20approximation%20and%0Aoptimization%3A%20when%20there%20is%20long%20term%20memory%20in%20the%20target%2C%20it%20takes%20a%20large%0Anumber%20of%20neurons%20to%20approximate%20it.%20Moreover%2C%20the%20training%20process%20will%20suffer%0Afrom%20slow%20downs.%20In%20particular%2C%20both%20of%20these%20effects%20become%20exponentially%20more%0Apronounced%20with%20memory%20-%20a%20phenomenon%20we%20call%20the%20%22curse%20of%20memory%22.%20These%0Aanalyses%20represent%20a%20basic%20step%20towards%20a%20concrete%20mathematical%20understanding%0Aof%20new%20phenomenon%20that%20may%20arise%20in%20learning%20temporal%20relationships%20using%0Arecurrent%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2009.07799v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Curse%2520of%2520Memory%2520in%2520Recurrent%2520Neural%2520Networks%253A%2520Approximation%2520and%250A%2520%2520Optimization%2520Analysis%26entry.906535625%3DZhong%2520Li%2520and%2520Jiequn%2520Han%2520and%2520Weinan%2520E%2520and%2520Qianxiao%2520Li%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520approximation%2520properties%2520and%2520optimization%2520dynamics%2520of%2520recurrent%250Aneural%2520networks%2520%2528RNNs%2529%2520when%2520applied%2520to%2520learn%2520input-output%2520relationships%2520in%250Atemporal%2520data.%2520We%2520consider%2520the%2520simple%2520but%2520representative%2520setting%2520of%2520using%250Acontinuous-time%2520linear%2520RNNs%2520to%2520learn%2520from%2520data%2520generated%2520by%2520linear%250Arelationships.%2520Mathematically%252C%2520the%2520latter%2520can%2520be%2520understood%2520as%2520a%2520sequence%2520of%250Alinear%2520functionals.%2520We%2520prove%2520a%2520universal%2520approximation%2520theorem%2520of%2520such%2520linear%250Afunctionals%252C%2520and%2520characterize%2520the%2520approximation%2520rate%2520and%2520its%2520relation%2520with%250Amemory.%2520Moreover%252C%2520we%2520perform%2520a%2520fine-grained%2520dynamical%2520analysis%2520of%2520training%250Alinear%2520RNNs%252C%2520which%2520further%2520reveal%2520the%2520intricate%2520interactions%2520between%2520memory%2520and%250Alearning.%2520A%2520unifying%2520theme%2520uncovered%2520is%2520the%2520non-trivial%2520effect%2520of%2520memory%252C%2520a%250Anotion%2520that%2520can%2520be%2520made%2520precise%2520in%2520our%2520framework%252C%2520on%2520approximation%2520and%250Aoptimization%253A%2520when%2520there%2520is%2520long%2520term%2520memory%2520in%2520the%2520target%252C%2520it%2520takes%2520a%2520large%250Anumber%2520of%2520neurons%2520to%2520approximate%2520it.%2520Moreover%252C%2520the%2520training%2520process%2520will%2520suffer%250Afrom%2520slow%2520downs.%2520In%2520particular%252C%2520both%2520of%2520these%2520effects%2520become%2520exponentially%2520more%250Apronounced%2520with%2520memory%2520-%2520a%2520phenomenon%2520we%2520call%2520the%2520%2522curse%2520of%2520memory%2522.%2520These%250Aanalyses%2520represent%2520a%2520basic%2520step%2520towards%2520a%2520concrete%2520mathematical%2520understanding%250Aof%2520new%2520phenomenon%2520that%2520may%2520arise%2520in%2520learning%2520temporal%2520relationships%2520using%250Arecurrent%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2009.07799v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Curse%20of%20Memory%20in%20Recurrent%20Neural%20Networks%3A%20Approximation%20and%0A%20%20Optimization%20Analysis&entry.906535625=Zhong%20Li%20and%20Jiequn%20Han%20and%20Weinan%20E%20and%20Qianxiao%20Li&entry.1292438233=%20%20We%20study%20the%20approximation%20properties%20and%20optimization%20dynamics%20of%20recurrent%0Aneural%20networks%20%28RNNs%29%20when%20applied%20to%20learn%20input-output%20relationships%20in%0Atemporal%20data.%20We%20consider%20the%20simple%20but%20representative%20setting%20of%20using%0Acontinuous-time%20linear%20RNNs%20to%20learn%20from%20data%20generated%20by%20linear%0Arelationships.%20Mathematically%2C%20the%20latter%20can%20be%20understood%20as%20a%20sequence%20of%0Alinear%20functionals.%20We%20prove%20a%20universal%20approximation%20theorem%20of%20such%20linear%0Afunctionals%2C%20and%20characterize%20the%20approximation%20rate%20and%20its%20relation%20with%0Amemory.%20Moreover%2C%20we%20perform%20a%20fine-grained%20dynamical%20analysis%20of%20training%0Alinear%20RNNs%2C%20which%20further%20reveal%20the%20intricate%20interactions%20between%20memory%20and%0Alearning.%20A%20unifying%20theme%20uncovered%20is%20the%20non-trivial%20effect%20of%20memory%2C%20a%0Anotion%20that%20can%20be%20made%20precise%20in%20our%20framework%2C%20on%20approximation%20and%0Aoptimization%3A%20when%20there%20is%20long%20term%20memory%20in%20the%20target%2C%20it%20takes%20a%20large%0Anumber%20of%20neurons%20to%20approximate%20it.%20Moreover%2C%20the%20training%20process%20will%20suffer%0Afrom%20slow%20downs.%20In%20particular%2C%20both%20of%20these%20effects%20become%20exponentially%20more%0Apronounced%20with%20memory%20-%20a%20phenomenon%20we%20call%20the%20%22curse%20of%20memory%22.%20These%0Aanalyses%20represent%20a%20basic%20step%20towards%20a%20concrete%20mathematical%20understanding%0Aof%20new%20phenomenon%20that%20may%20arise%20in%20learning%20temporal%20relationships%20using%0Arecurrent%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2009.07799v3&entry.124074799=Read"},
{"title": "LASSO-MOGAT: A Multi-Omics Graph Attention Framework for Cancer\n  Classification", "author": "Fadi Alharbi and Aleksandar Vakanski and Murtada K. Elbashir and Mohanad Mohammed", "abstract": "  The application of machine learning methods to analyze changes in gene\nexpression patterns has recently emerged as a powerful approach in cancer\nresearch, enhancing our understanding of the molecular mechanisms underpinning\ncancer development and progression. Combining gene expression data with other\ntypes of omics data has been reported by numerous works to improve cancer\nclassification outcomes. Despite these advances, effectively integrating\nhigh-dimensional multi-omics data and capturing the complex relationships\nacross different biological layers remains challenging. This paper introduces\nLASSO-MOGAT (LASSO-Multi-Omics Gated ATtention), a novel graph-based deep\nlearning framework that integrates messenger RNA, microRNA, and DNA methylation\ndata to classify 31 cancer types. Utilizing differential expression analysis\nwith LIMMA and LASSO regression for feature selection, and leveraging Graph\nAttention Networks (GATs) to incorporate protein-protein interaction (PPI)\nnetworks, LASSO-MOGAT effectively captures intricate relationships within\nmulti-omics data. Experimental validation using five-fold cross-validation\ndemonstrates the method's precision, reliability, and capacity for providing\ncomprehensive insights into cancer molecular mechanisms. The computation of\nattention coefficients for the edges in the graph by the proposed\ngraph-attention architecture based on protein-protein interactions proved\nbeneficial for identifying synergies in multi-omics data for cancer\nclassification.\n", "link": "http://arxiv.org/abs/2408.17384v1", "date": "2024-08-30", "relevancy": 1.8493, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4711}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.468}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LASSO-MOGAT%3A%20A%20Multi-Omics%20Graph%20Attention%20Framework%20for%20Cancer%0A%20%20Classification&body=Title%3A%20LASSO-MOGAT%3A%20A%20Multi-Omics%20Graph%20Attention%20Framework%20for%20Cancer%0A%20%20Classification%0AAuthor%3A%20Fadi%20Alharbi%20and%20Aleksandar%20Vakanski%20and%20Murtada%20K.%20Elbashir%20and%20Mohanad%20Mohammed%0AAbstract%3A%20%20%20The%20application%20of%20machine%20learning%20methods%20to%20analyze%20changes%20in%20gene%0Aexpression%20patterns%20has%20recently%20emerged%20as%20a%20powerful%20approach%20in%20cancer%0Aresearch%2C%20enhancing%20our%20understanding%20of%20the%20molecular%20mechanisms%20underpinning%0Acancer%20development%20and%20progression.%20Combining%20gene%20expression%20data%20with%20other%0Atypes%20of%20omics%20data%20has%20been%20reported%20by%20numerous%20works%20to%20improve%20cancer%0Aclassification%20outcomes.%20Despite%20these%20advances%2C%20effectively%20integrating%0Ahigh-dimensional%20multi-omics%20data%20and%20capturing%20the%20complex%20relationships%0Aacross%20different%20biological%20layers%20remains%20challenging.%20This%20paper%20introduces%0ALASSO-MOGAT%20%28LASSO-Multi-Omics%20Gated%20ATtention%29%2C%20a%20novel%20graph-based%20deep%0Alearning%20framework%20that%20integrates%20messenger%20RNA%2C%20microRNA%2C%20and%20DNA%20methylation%0Adata%20to%20classify%2031%20cancer%20types.%20Utilizing%20differential%20expression%20analysis%0Awith%20LIMMA%20and%20LASSO%20regression%20for%20feature%20selection%2C%20and%20leveraging%20Graph%0AAttention%20Networks%20%28GATs%29%20to%20incorporate%20protein-protein%20interaction%20%28PPI%29%0Anetworks%2C%20LASSO-MOGAT%20effectively%20captures%20intricate%20relationships%20within%0Amulti-omics%20data.%20Experimental%20validation%20using%20five-fold%20cross-validation%0Ademonstrates%20the%20method%27s%20precision%2C%20reliability%2C%20and%20capacity%20for%20providing%0Acomprehensive%20insights%20into%20cancer%20molecular%20mechanisms.%20The%20computation%20of%0Aattention%20coefficients%20for%20the%20edges%20in%20the%20graph%20by%20the%20proposed%0Agraph-attention%20architecture%20based%20on%20protein-protein%20interactions%20proved%0Abeneficial%20for%20identifying%20synergies%20in%20multi-omics%20data%20for%20cancer%0Aclassification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17384v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLASSO-MOGAT%253A%2520A%2520Multi-Omics%2520Graph%2520Attention%2520Framework%2520for%2520Cancer%250A%2520%2520Classification%26entry.906535625%3DFadi%2520Alharbi%2520and%2520Aleksandar%2520Vakanski%2520and%2520Murtada%2520K.%2520Elbashir%2520and%2520Mohanad%2520Mohammed%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520machine%2520learning%2520methods%2520to%2520analyze%2520changes%2520in%2520gene%250Aexpression%2520patterns%2520has%2520recently%2520emerged%2520as%2520a%2520powerful%2520approach%2520in%2520cancer%250Aresearch%252C%2520enhancing%2520our%2520understanding%2520of%2520the%2520molecular%2520mechanisms%2520underpinning%250Acancer%2520development%2520and%2520progression.%2520Combining%2520gene%2520expression%2520data%2520with%2520other%250Atypes%2520of%2520omics%2520data%2520has%2520been%2520reported%2520by%2520numerous%2520works%2520to%2520improve%2520cancer%250Aclassification%2520outcomes.%2520Despite%2520these%2520advances%252C%2520effectively%2520integrating%250Ahigh-dimensional%2520multi-omics%2520data%2520and%2520capturing%2520the%2520complex%2520relationships%250Aacross%2520different%2520biological%2520layers%2520remains%2520challenging.%2520This%2520paper%2520introduces%250ALASSO-MOGAT%2520%2528LASSO-Multi-Omics%2520Gated%2520ATtention%2529%252C%2520a%2520novel%2520graph-based%2520deep%250Alearning%2520framework%2520that%2520integrates%2520messenger%2520RNA%252C%2520microRNA%252C%2520and%2520DNA%2520methylation%250Adata%2520to%2520classify%252031%2520cancer%2520types.%2520Utilizing%2520differential%2520expression%2520analysis%250Awith%2520LIMMA%2520and%2520LASSO%2520regression%2520for%2520feature%2520selection%252C%2520and%2520leveraging%2520Graph%250AAttention%2520Networks%2520%2528GATs%2529%2520to%2520incorporate%2520protein-protein%2520interaction%2520%2528PPI%2529%250Anetworks%252C%2520LASSO-MOGAT%2520effectively%2520captures%2520intricate%2520relationships%2520within%250Amulti-omics%2520data.%2520Experimental%2520validation%2520using%2520five-fold%2520cross-validation%250Ademonstrates%2520the%2520method%2527s%2520precision%252C%2520reliability%252C%2520and%2520capacity%2520for%2520providing%250Acomprehensive%2520insights%2520into%2520cancer%2520molecular%2520mechanisms.%2520The%2520computation%2520of%250Aattention%2520coefficients%2520for%2520the%2520edges%2520in%2520the%2520graph%2520by%2520the%2520proposed%250Agraph-attention%2520architecture%2520based%2520on%2520protein-protein%2520interactions%2520proved%250Abeneficial%2520for%2520identifying%2520synergies%2520in%2520multi-omics%2520data%2520for%2520cancer%250Aclassification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17384v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LASSO-MOGAT%3A%20A%20Multi-Omics%20Graph%20Attention%20Framework%20for%20Cancer%0A%20%20Classification&entry.906535625=Fadi%20Alharbi%20and%20Aleksandar%20Vakanski%20and%20Murtada%20K.%20Elbashir%20and%20Mohanad%20Mohammed&entry.1292438233=%20%20The%20application%20of%20machine%20learning%20methods%20to%20analyze%20changes%20in%20gene%0Aexpression%20patterns%20has%20recently%20emerged%20as%20a%20powerful%20approach%20in%20cancer%0Aresearch%2C%20enhancing%20our%20understanding%20of%20the%20molecular%20mechanisms%20underpinning%0Acancer%20development%20and%20progression.%20Combining%20gene%20expression%20data%20with%20other%0Atypes%20of%20omics%20data%20has%20been%20reported%20by%20numerous%20works%20to%20improve%20cancer%0Aclassification%20outcomes.%20Despite%20these%20advances%2C%20effectively%20integrating%0Ahigh-dimensional%20multi-omics%20data%20and%20capturing%20the%20complex%20relationships%0Aacross%20different%20biological%20layers%20remains%20challenging.%20This%20paper%20introduces%0ALASSO-MOGAT%20%28LASSO-Multi-Omics%20Gated%20ATtention%29%2C%20a%20novel%20graph-based%20deep%0Alearning%20framework%20that%20integrates%20messenger%20RNA%2C%20microRNA%2C%20and%20DNA%20methylation%0Adata%20to%20classify%2031%20cancer%20types.%20Utilizing%20differential%20expression%20analysis%0Awith%20LIMMA%20and%20LASSO%20regression%20for%20feature%20selection%2C%20and%20leveraging%20Graph%0AAttention%20Networks%20%28GATs%29%20to%20incorporate%20protein-protein%20interaction%20%28PPI%29%0Anetworks%2C%20LASSO-MOGAT%20effectively%20captures%20intricate%20relationships%20within%0Amulti-omics%20data.%20Experimental%20validation%20using%20five-fold%20cross-validation%0Ademonstrates%20the%20method%27s%20precision%2C%20reliability%2C%20and%20capacity%20for%20providing%0Acomprehensive%20insights%20into%20cancer%20molecular%20mechanisms.%20The%20computation%20of%0Aattention%20coefficients%20for%20the%20edges%20in%20the%20graph%20by%20the%20proposed%0Agraph-attention%20architecture%20based%20on%20protein-protein%20interactions%20proved%0Abeneficial%20for%20identifying%20synergies%20in%20multi-omics%20data%20for%20cancer%0Aclassification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17384v1&entry.124074799=Read"},
{"title": "Flexible and Effective Mixing of Large Language Models into a Mixture of\n  Domain Experts", "author": "Rhui Dih Lee and Laura Wynter and Raghu Kiran Ganti", "abstract": "  We present a toolkit for creating low-cost Mixture-of-Domain-Experts (MOE)\nfrom trained models. The toolkit can be used for creating a mixture from models\nor from adapters. We perform extensive tests and offer guidance on defining the\narchitecture of the resulting MOE using the toolkit. A public repository is\navailable.\n", "link": "http://arxiv.org/abs/2408.17280v1", "date": "2024-08-30", "relevancy": 1.8433, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4717}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4557}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flexible%20and%20Effective%20Mixing%20of%20Large%20Language%20Models%20into%20a%20Mixture%20of%0A%20%20Domain%20Experts&body=Title%3A%20Flexible%20and%20Effective%20Mixing%20of%20Large%20Language%20Models%20into%20a%20Mixture%20of%0A%20%20Domain%20Experts%0AAuthor%3A%20Rhui%20Dih%20Lee%20and%20Laura%20Wynter%20and%20Raghu%20Kiran%20Ganti%0AAbstract%3A%20%20%20We%20present%20a%20toolkit%20for%20creating%20low-cost%20Mixture-of-Domain-Experts%20%28MOE%29%0Afrom%20trained%20models.%20The%20toolkit%20can%20be%20used%20for%20creating%20a%20mixture%20from%20models%0Aor%20from%20adapters.%20We%20perform%20extensive%20tests%20and%20offer%20guidance%20on%20defining%20the%0Aarchitecture%20of%20the%20resulting%20MOE%20using%20the%20toolkit.%20A%20public%20repository%20is%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexible%2520and%2520Effective%2520Mixing%2520of%2520Large%2520Language%2520Models%2520into%2520a%2520Mixture%2520of%250A%2520%2520Domain%2520Experts%26entry.906535625%3DRhui%2520Dih%2520Lee%2520and%2520Laura%2520Wynter%2520and%2520Raghu%2520Kiran%2520Ganti%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520toolkit%2520for%2520creating%2520low-cost%2520Mixture-of-Domain-Experts%2520%2528MOE%2529%250Afrom%2520trained%2520models.%2520The%2520toolkit%2520can%2520be%2520used%2520for%2520creating%2520a%2520mixture%2520from%2520models%250Aor%2520from%2520adapters.%2520We%2520perform%2520extensive%2520tests%2520and%2520offer%2520guidance%2520on%2520defining%2520the%250Aarchitecture%2520of%2520the%2520resulting%2520MOE%2520using%2520the%2520toolkit.%2520A%2520public%2520repository%2520is%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flexible%20and%20Effective%20Mixing%20of%20Large%20Language%20Models%20into%20a%20Mixture%20of%0A%20%20Domain%20Experts&entry.906535625=Rhui%20Dih%20Lee%20and%20Laura%20Wynter%20and%20Raghu%20Kiran%20Ganti&entry.1292438233=%20%20We%20present%20a%20toolkit%20for%20creating%20low-cost%20Mixture-of-Domain-Experts%20%28MOE%29%0Afrom%20trained%20models.%20The%20toolkit%20can%20be%20used%20for%20creating%20a%20mixture%20from%20models%0Aor%20from%20adapters.%20We%20perform%20extensive%20tests%20and%20offer%20guidance%20on%20defining%20the%0Aarchitecture%20of%20the%20resulting%20MOE%20using%20the%20toolkit.%20A%20public%20repository%20is%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17280v1&entry.124074799=Read"},
{"title": "SafeTail: Efficient Tail Latency Optimization in Edge Service Scheduling\n  via Computational Redundancy Management", "author": "Jyoti Shokhanda and Utkarsh Pal and Aman Kumar and Soumi Chattopadhyay and Arani Bhattacharya", "abstract": "  Optimizing tail latency while efficiently managing computational resources is\ncrucial for delivering high-performance, latency-sensitive services in edge\ncomputing. Emerging applications, such as augmented reality, require\nlow-latency computing services with high reliability on user devices, which\noften have limited computational capabilities. Consequently, these devices\ndepend on nearby edge servers for processing. However, inherent uncertainties\nin network and computation latencies stemming from variability in wireless\nnetworks and fluctuating server loads make service delivery on time\nchallenging. Existing approaches often focus on optimizing median latency but\nfall short of addressing the specific challenges of tail latency in edge\nenvironments, particularly under uncertain network and computational\nconditions. Although some methods do address tail latency, they typically rely\non fixed or excessive redundancy and lack adaptability to dynamic network\nconditions, often being designed for cloud environments rather than the unique\ndemands of edge computing. In this paper, we introduce SafeTail, a framework\nthat meets both median and tail response time targets, with tail latency\ndefined as latency beyond the 90^th percentile threshold. SafeTail addresses\nthis challenge by selectively replicating services across multiple edge servers\nto meet target latencies. SafeTail employs a reward-based deep learning\nframework to learn optimal placement strategies, balancing the need to achieve\ntarget latencies with minimizing additional resource usage. Through\ntrace-driven simulations, SafeTail demonstrated near-optimal performance and\noutperformed most baseline strategies across three diverse services.\n", "link": "http://arxiv.org/abs/2408.17171v1", "date": "2024-08-30", "relevancy": 1.8238, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4645}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4576}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafeTail%3A%20Efficient%20Tail%20Latency%20Optimization%20in%20Edge%20Service%20Scheduling%0A%20%20via%20Computational%20Redundancy%20Management&body=Title%3A%20SafeTail%3A%20Efficient%20Tail%20Latency%20Optimization%20in%20Edge%20Service%20Scheduling%0A%20%20via%20Computational%20Redundancy%20Management%0AAuthor%3A%20Jyoti%20Shokhanda%20and%20Utkarsh%20Pal%20and%20Aman%20Kumar%20and%20Soumi%20Chattopadhyay%20and%20Arani%20Bhattacharya%0AAbstract%3A%20%20%20Optimizing%20tail%20latency%20while%20efficiently%20managing%20computational%20resources%20is%0Acrucial%20for%20delivering%20high-performance%2C%20latency-sensitive%20services%20in%20edge%0Acomputing.%20Emerging%20applications%2C%20such%20as%20augmented%20reality%2C%20require%0Alow-latency%20computing%20services%20with%20high%20reliability%20on%20user%20devices%2C%20which%0Aoften%20have%20limited%20computational%20capabilities.%20Consequently%2C%20these%20devices%0Adepend%20on%20nearby%20edge%20servers%20for%20processing.%20However%2C%20inherent%20uncertainties%0Ain%20network%20and%20computation%20latencies%20stemming%20from%20variability%20in%20wireless%0Anetworks%20and%20fluctuating%20server%20loads%20make%20service%20delivery%20on%20time%0Achallenging.%20Existing%20approaches%20often%20focus%20on%20optimizing%20median%20latency%20but%0Afall%20short%20of%20addressing%20the%20specific%20challenges%20of%20tail%20latency%20in%20edge%0Aenvironments%2C%20particularly%20under%20uncertain%20network%20and%20computational%0Aconditions.%20Although%20some%20methods%20do%20address%20tail%20latency%2C%20they%20typically%20rely%0Aon%20fixed%20or%20excessive%20redundancy%20and%20lack%20adaptability%20to%20dynamic%20network%0Aconditions%2C%20often%20being%20designed%20for%20cloud%20environments%20rather%20than%20the%20unique%0Ademands%20of%20edge%20computing.%20In%20this%20paper%2C%20we%20introduce%20SafeTail%2C%20a%20framework%0Athat%20meets%20both%20median%20and%20tail%20response%20time%20targets%2C%20with%20tail%20latency%0Adefined%20as%20latency%20beyond%20the%2090%5Eth%20percentile%20threshold.%20SafeTail%20addresses%0Athis%20challenge%20by%20selectively%20replicating%20services%20across%20multiple%20edge%20servers%0Ato%20meet%20target%20latencies.%20SafeTail%20employs%20a%20reward-based%20deep%20learning%0Aframework%20to%20learn%20optimal%20placement%20strategies%2C%20balancing%20the%20need%20to%20achieve%0Atarget%20latencies%20with%20minimizing%20additional%20resource%20usage.%20Through%0Atrace-driven%20simulations%2C%20SafeTail%20demonstrated%20near-optimal%20performance%20and%0Aoutperformed%20most%20baseline%20strategies%20across%20three%20diverse%20services.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafeTail%253A%2520Efficient%2520Tail%2520Latency%2520Optimization%2520in%2520Edge%2520Service%2520Scheduling%250A%2520%2520via%2520Computational%2520Redundancy%2520Management%26entry.906535625%3DJyoti%2520Shokhanda%2520and%2520Utkarsh%2520Pal%2520and%2520Aman%2520Kumar%2520and%2520Soumi%2520Chattopadhyay%2520and%2520Arani%2520Bhattacharya%26entry.1292438233%3D%2520%2520Optimizing%2520tail%2520latency%2520while%2520efficiently%2520managing%2520computational%2520resources%2520is%250Acrucial%2520for%2520delivering%2520high-performance%252C%2520latency-sensitive%2520services%2520in%2520edge%250Acomputing.%2520Emerging%2520applications%252C%2520such%2520as%2520augmented%2520reality%252C%2520require%250Alow-latency%2520computing%2520services%2520with%2520high%2520reliability%2520on%2520user%2520devices%252C%2520which%250Aoften%2520have%2520limited%2520computational%2520capabilities.%2520Consequently%252C%2520these%2520devices%250Adepend%2520on%2520nearby%2520edge%2520servers%2520for%2520processing.%2520However%252C%2520inherent%2520uncertainties%250Ain%2520network%2520and%2520computation%2520latencies%2520stemming%2520from%2520variability%2520in%2520wireless%250Anetworks%2520and%2520fluctuating%2520server%2520loads%2520make%2520service%2520delivery%2520on%2520time%250Achallenging.%2520Existing%2520approaches%2520often%2520focus%2520on%2520optimizing%2520median%2520latency%2520but%250Afall%2520short%2520of%2520addressing%2520the%2520specific%2520challenges%2520of%2520tail%2520latency%2520in%2520edge%250Aenvironments%252C%2520particularly%2520under%2520uncertain%2520network%2520and%2520computational%250Aconditions.%2520Although%2520some%2520methods%2520do%2520address%2520tail%2520latency%252C%2520they%2520typically%2520rely%250Aon%2520fixed%2520or%2520excessive%2520redundancy%2520and%2520lack%2520adaptability%2520to%2520dynamic%2520network%250Aconditions%252C%2520often%2520being%2520designed%2520for%2520cloud%2520environments%2520rather%2520than%2520the%2520unique%250Ademands%2520of%2520edge%2520computing.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SafeTail%252C%2520a%2520framework%250Athat%2520meets%2520both%2520median%2520and%2520tail%2520response%2520time%2520targets%252C%2520with%2520tail%2520latency%250Adefined%2520as%2520latency%2520beyond%2520the%252090%255Eth%2520percentile%2520threshold.%2520SafeTail%2520addresses%250Athis%2520challenge%2520by%2520selectively%2520replicating%2520services%2520across%2520multiple%2520edge%2520servers%250Ato%2520meet%2520target%2520latencies.%2520SafeTail%2520employs%2520a%2520reward-based%2520deep%2520learning%250Aframework%2520to%2520learn%2520optimal%2520placement%2520strategies%252C%2520balancing%2520the%2520need%2520to%2520achieve%250Atarget%2520latencies%2520with%2520minimizing%2520additional%2520resource%2520usage.%2520Through%250Atrace-driven%2520simulations%252C%2520SafeTail%2520demonstrated%2520near-optimal%2520performance%2520and%250Aoutperformed%2520most%2520baseline%2520strategies%2520across%2520three%2520diverse%2520services.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafeTail%3A%20Efficient%20Tail%20Latency%20Optimization%20in%20Edge%20Service%20Scheduling%0A%20%20via%20Computational%20Redundancy%20Management&entry.906535625=Jyoti%20Shokhanda%20and%20Utkarsh%20Pal%20and%20Aman%20Kumar%20and%20Soumi%20Chattopadhyay%20and%20Arani%20Bhattacharya&entry.1292438233=%20%20Optimizing%20tail%20latency%20while%20efficiently%20managing%20computational%20resources%20is%0Acrucial%20for%20delivering%20high-performance%2C%20latency-sensitive%20services%20in%20edge%0Acomputing.%20Emerging%20applications%2C%20such%20as%20augmented%20reality%2C%20require%0Alow-latency%20computing%20services%20with%20high%20reliability%20on%20user%20devices%2C%20which%0Aoften%20have%20limited%20computational%20capabilities.%20Consequently%2C%20these%20devices%0Adepend%20on%20nearby%20edge%20servers%20for%20processing.%20However%2C%20inherent%20uncertainties%0Ain%20network%20and%20computation%20latencies%20stemming%20from%20variability%20in%20wireless%0Anetworks%20and%20fluctuating%20server%20loads%20make%20service%20delivery%20on%20time%0Achallenging.%20Existing%20approaches%20often%20focus%20on%20optimizing%20median%20latency%20but%0Afall%20short%20of%20addressing%20the%20specific%20challenges%20of%20tail%20latency%20in%20edge%0Aenvironments%2C%20particularly%20under%20uncertain%20network%20and%20computational%0Aconditions.%20Although%20some%20methods%20do%20address%20tail%20latency%2C%20they%20typically%20rely%0Aon%20fixed%20or%20excessive%20redundancy%20and%20lack%20adaptability%20to%20dynamic%20network%0Aconditions%2C%20often%20being%20designed%20for%20cloud%20environments%20rather%20than%20the%20unique%0Ademands%20of%20edge%20computing.%20In%20this%20paper%2C%20we%20introduce%20SafeTail%2C%20a%20framework%0Athat%20meets%20both%20median%20and%20tail%20response%20time%20targets%2C%20with%20tail%20latency%0Adefined%20as%20latency%20beyond%20the%2090%5Eth%20percentile%20threshold.%20SafeTail%20addresses%0Athis%20challenge%20by%20selectively%20replicating%20services%20across%20multiple%20edge%20servers%0Ato%20meet%20target%20latencies.%20SafeTail%20employs%20a%20reward-based%20deep%20learning%0Aframework%20to%20learn%20optimal%20placement%20strategies%2C%20balancing%20the%20need%20to%20achieve%0Atarget%20latencies%20with%20minimizing%20additional%20resource%20usage.%20Through%0Atrace-driven%20simulations%2C%20SafeTail%20demonstrated%20near-optimal%20performance%20and%0Aoutperformed%20most%20baseline%20strategies%20across%20three%20diverse%20services.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17171v1&entry.124074799=Read"},
{"title": "Bayesian Optimization for Non-Convex Two-Stage Stochastic Optimization\n  Problems", "author": "Jack M. Buckingham and Ivo Couckuyt and Juergen Branke", "abstract": "  Bayesian optimization is a sample-efficient method for solving expensive,\nblack-box optimization problems. Stochastic programming concerns optimization\nunder uncertainty where, typically, average performance is the quantity of\ninterest. In the first stage of a two-stage problem, here-and-now decisions\nmust be made in the face of this uncertainty, while in the second stage,\nwait-and-see decisions are made after the uncertainty has been resolved. Many\nmethods in stochastic programming assume that the objective is cheap to\nevaluate and linear or convex. In this work, we apply Bayesian optimization to\nsolve non-convex, two-stage stochastic programs which are expensive to\nevaluate. We formulate a knowledge-gradient-based acquisition function to\njointly optimize the first- and second-stage variables, establish a guarantee\nof asymptotic consistency and provide a computationally efficient\napproximation. We demonstrate comparable empirical results to an alternative we\nformulate which alternates its focus between the two variable types, and\nsuperior empirical results over the standard, naive, two-step benchmark. We\nshow that differences in the dimension and length scales between the variable\ntypes can lead to inefficiencies of the two-step algorithm, while the joint and\nalternating acquisition functions perform well in all problems tested.\nExperiments are conducted on both synthetic and real-world examples.\n", "link": "http://arxiv.org/abs/2408.17387v1", "date": "2024-08-30", "relevancy": 1.8214, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5239}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4624}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Optimization%20for%20Non-Convex%20Two-Stage%20Stochastic%20Optimization%0A%20%20Problems&body=Title%3A%20Bayesian%20Optimization%20for%20Non-Convex%20Two-Stage%20Stochastic%20Optimization%0A%20%20Problems%0AAuthor%3A%20Jack%20M.%20Buckingham%20and%20Ivo%20Couckuyt%20and%20Juergen%20Branke%0AAbstract%3A%20%20%20Bayesian%20optimization%20is%20a%20sample-efficient%20method%20for%20solving%20expensive%2C%0Ablack-box%20optimization%20problems.%20Stochastic%20programming%20concerns%20optimization%0Aunder%20uncertainty%20where%2C%20typically%2C%20average%20performance%20is%20the%20quantity%20of%0Ainterest.%20In%20the%20first%20stage%20of%20a%20two-stage%20problem%2C%20here-and-now%20decisions%0Amust%20be%20made%20in%20the%20face%20of%20this%20uncertainty%2C%20while%20in%20the%20second%20stage%2C%0Await-and-see%20decisions%20are%20made%20after%20the%20uncertainty%20has%20been%20resolved.%20Many%0Amethods%20in%20stochastic%20programming%20assume%20that%20the%20objective%20is%20cheap%20to%0Aevaluate%20and%20linear%20or%20convex.%20In%20this%20work%2C%20we%20apply%20Bayesian%20optimization%20to%0Asolve%20non-convex%2C%20two-stage%20stochastic%20programs%20which%20are%20expensive%20to%0Aevaluate.%20We%20formulate%20a%20knowledge-gradient-based%20acquisition%20function%20to%0Ajointly%20optimize%20the%20first-%20and%20second-stage%20variables%2C%20establish%20a%20guarantee%0Aof%20asymptotic%20consistency%20and%20provide%20a%20computationally%20efficient%0Aapproximation.%20We%20demonstrate%20comparable%20empirical%20results%20to%20an%20alternative%20we%0Aformulate%20which%20alternates%20its%20focus%20between%20the%20two%20variable%20types%2C%20and%0Asuperior%20empirical%20results%20over%20the%20standard%2C%20naive%2C%20two-step%20benchmark.%20We%0Ashow%20that%20differences%20in%20the%20dimension%20and%20length%20scales%20between%20the%20variable%0Atypes%20can%20lead%20to%20inefficiencies%20of%20the%20two-step%20algorithm%2C%20while%20the%20joint%20and%0Aalternating%20acquisition%20functions%20perform%20well%20in%20all%20problems%20tested.%0AExperiments%20are%20conducted%20on%20both%20synthetic%20and%20real-world%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Optimization%2520for%2520Non-Convex%2520Two-Stage%2520Stochastic%2520Optimization%250A%2520%2520Problems%26entry.906535625%3DJack%2520M.%2520Buckingham%2520and%2520Ivo%2520Couckuyt%2520and%2520Juergen%2520Branke%26entry.1292438233%3D%2520%2520Bayesian%2520optimization%2520is%2520a%2520sample-efficient%2520method%2520for%2520solving%2520expensive%252C%250Ablack-box%2520optimization%2520problems.%2520Stochastic%2520programming%2520concerns%2520optimization%250Aunder%2520uncertainty%2520where%252C%2520typically%252C%2520average%2520performance%2520is%2520the%2520quantity%2520of%250Ainterest.%2520In%2520the%2520first%2520stage%2520of%2520a%2520two-stage%2520problem%252C%2520here-and-now%2520decisions%250Amust%2520be%2520made%2520in%2520the%2520face%2520of%2520this%2520uncertainty%252C%2520while%2520in%2520the%2520second%2520stage%252C%250Await-and-see%2520decisions%2520are%2520made%2520after%2520the%2520uncertainty%2520has%2520been%2520resolved.%2520Many%250Amethods%2520in%2520stochastic%2520programming%2520assume%2520that%2520the%2520objective%2520is%2520cheap%2520to%250Aevaluate%2520and%2520linear%2520or%2520convex.%2520In%2520this%2520work%252C%2520we%2520apply%2520Bayesian%2520optimization%2520to%250Asolve%2520non-convex%252C%2520two-stage%2520stochastic%2520programs%2520which%2520are%2520expensive%2520to%250Aevaluate.%2520We%2520formulate%2520a%2520knowledge-gradient-based%2520acquisition%2520function%2520to%250Ajointly%2520optimize%2520the%2520first-%2520and%2520second-stage%2520variables%252C%2520establish%2520a%2520guarantee%250Aof%2520asymptotic%2520consistency%2520and%2520provide%2520a%2520computationally%2520efficient%250Aapproximation.%2520We%2520demonstrate%2520comparable%2520empirical%2520results%2520to%2520an%2520alternative%2520we%250Aformulate%2520which%2520alternates%2520its%2520focus%2520between%2520the%2520two%2520variable%2520types%252C%2520and%250Asuperior%2520empirical%2520results%2520over%2520the%2520standard%252C%2520naive%252C%2520two-step%2520benchmark.%2520We%250Ashow%2520that%2520differences%2520in%2520the%2520dimension%2520and%2520length%2520scales%2520between%2520the%2520variable%250Atypes%2520can%2520lead%2520to%2520inefficiencies%2520of%2520the%2520two-step%2520algorithm%252C%2520while%2520the%2520joint%2520and%250Aalternating%2520acquisition%2520functions%2520perform%2520well%2520in%2520all%2520problems%2520tested.%250AExperiments%2520are%2520conducted%2520on%2520both%2520synthetic%2520and%2520real-world%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Optimization%20for%20Non-Convex%20Two-Stage%20Stochastic%20Optimization%0A%20%20Problems&entry.906535625=Jack%20M.%20Buckingham%20and%20Ivo%20Couckuyt%20and%20Juergen%20Branke&entry.1292438233=%20%20Bayesian%20optimization%20is%20a%20sample-efficient%20method%20for%20solving%20expensive%2C%0Ablack-box%20optimization%20problems.%20Stochastic%20programming%20concerns%20optimization%0Aunder%20uncertainty%20where%2C%20typically%2C%20average%20performance%20is%20the%20quantity%20of%0Ainterest.%20In%20the%20first%20stage%20of%20a%20two-stage%20problem%2C%20here-and-now%20decisions%0Amust%20be%20made%20in%20the%20face%20of%20this%20uncertainty%2C%20while%20in%20the%20second%20stage%2C%0Await-and-see%20decisions%20are%20made%20after%20the%20uncertainty%20has%20been%20resolved.%20Many%0Amethods%20in%20stochastic%20programming%20assume%20that%20the%20objective%20is%20cheap%20to%0Aevaluate%20and%20linear%20or%20convex.%20In%20this%20work%2C%20we%20apply%20Bayesian%20optimization%20to%0Asolve%20non-convex%2C%20two-stage%20stochastic%20programs%20which%20are%20expensive%20to%0Aevaluate.%20We%20formulate%20a%20knowledge-gradient-based%20acquisition%20function%20to%0Ajointly%20optimize%20the%20first-%20and%20second-stage%20variables%2C%20establish%20a%20guarantee%0Aof%20asymptotic%20consistency%20and%20provide%20a%20computationally%20efficient%0Aapproximation.%20We%20demonstrate%20comparable%20empirical%20results%20to%20an%20alternative%20we%0Aformulate%20which%20alternates%20its%20focus%20between%20the%20two%20variable%20types%2C%20and%0Asuperior%20empirical%20results%20over%20the%20standard%2C%20naive%2C%20two-step%20benchmark.%20We%0Ashow%20that%20differences%20in%20the%20dimension%20and%20length%20scales%20between%20the%20variable%0Atypes%20can%20lead%20to%20inefficiencies%20of%20the%20two-step%20algorithm%2C%20while%20the%20joint%20and%0Aalternating%20acquisition%20functions%20perform%20well%20in%20all%20problems%20tested.%0AExperiments%20are%20conducted%20on%20both%20synthetic%20and%20real-world%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17387v1&entry.124074799=Read"},
{"title": "AASIST3: KAN-Enhanced AASIST Speech Deepfake Detection using SSL\n  Features and Additional Regularization for the ASVspoof 2024 Challenge", "author": "Kirill Borodin and Vasiliy Kudryavtsev and Dmitrii Korzh and Alexey Efimenko and Grach Mkrtchian and Mikhail Gorodnichev and Oleg Y. Rogov", "abstract": "  Automatic Speaker Verification (ASV) systems, which identify speakers based\non their voice characteristics, have numerous applications, such as user\nauthentication in financial transactions, exclusive access control in smart\ndevices, and forensic fraud detection. However, the advancement of deep\nlearning algorithms has enabled the generation of synthetic audio through\nText-to-Speech (TTS) and Voice Conversion (VC) systems, exposing ASV systems to\npotential vulnerabilities. To counteract this, we propose a novel architecture\nnamed AASIST3. By enhancing the existing AASIST framework with\nKolmogorov-Arnold networks, additional layers, encoders, and pre-emphasis\ntechniques, AASIST3 achieves a more than twofold improvement in performance. It\ndemonstrates minDCF results of 0.5357 in the closed condition and 0.1414 in the\nopen condition, significantly enhancing the detection of synthetic voices and\nimproving ASV security.\n", "link": "http://arxiv.org/abs/2408.17352v1", "date": "2024-08-30", "relevancy": 1.8206, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4603}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4548}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AASIST3%3A%20KAN-Enhanced%20AASIST%20Speech%20Deepfake%20Detection%20using%20SSL%0A%20%20Features%20and%20Additional%20Regularization%20for%20the%20ASVspoof%202024%20Challenge&body=Title%3A%20AASIST3%3A%20KAN-Enhanced%20AASIST%20Speech%20Deepfake%20Detection%20using%20SSL%0A%20%20Features%20and%20Additional%20Regularization%20for%20the%20ASVspoof%202024%20Challenge%0AAuthor%3A%20Kirill%20Borodin%20and%20Vasiliy%20Kudryavtsev%20and%20Dmitrii%20Korzh%20and%20Alexey%20Efimenko%20and%20Grach%20Mkrtchian%20and%20Mikhail%20Gorodnichev%20and%20Oleg%20Y.%20Rogov%0AAbstract%3A%20%20%20Automatic%20Speaker%20Verification%20%28ASV%29%20systems%2C%20which%20identify%20speakers%20based%0Aon%20their%20voice%20characteristics%2C%20have%20numerous%20applications%2C%20such%20as%20user%0Aauthentication%20in%20financial%20transactions%2C%20exclusive%20access%20control%20in%20smart%0Adevices%2C%20and%20forensic%20fraud%20detection.%20However%2C%20the%20advancement%20of%20deep%0Alearning%20algorithms%20has%20enabled%20the%20generation%20of%20synthetic%20audio%20through%0AText-to-Speech%20%28TTS%29%20and%20Voice%20Conversion%20%28VC%29%20systems%2C%20exposing%20ASV%20systems%20to%0Apotential%20vulnerabilities.%20To%20counteract%20this%2C%20we%20propose%20a%20novel%20architecture%0Anamed%20AASIST3.%20By%20enhancing%20the%20existing%20AASIST%20framework%20with%0AKolmogorov-Arnold%20networks%2C%20additional%20layers%2C%20encoders%2C%20and%20pre-emphasis%0Atechniques%2C%20AASIST3%20achieves%20a%20more%20than%20twofold%20improvement%20in%20performance.%20It%0Ademonstrates%20minDCF%20results%20of%200.5357%20in%20the%20closed%20condition%20and%200.1414%20in%20the%0Aopen%20condition%2C%20significantly%20enhancing%20the%20detection%20of%20synthetic%20voices%20and%0Aimproving%20ASV%20security.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAASIST3%253A%2520KAN-Enhanced%2520AASIST%2520Speech%2520Deepfake%2520Detection%2520using%2520SSL%250A%2520%2520Features%2520and%2520Additional%2520Regularization%2520for%2520the%2520ASVspoof%25202024%2520Challenge%26entry.906535625%3DKirill%2520Borodin%2520and%2520Vasiliy%2520Kudryavtsev%2520and%2520Dmitrii%2520Korzh%2520and%2520Alexey%2520Efimenko%2520and%2520Grach%2520Mkrtchian%2520and%2520Mikhail%2520Gorodnichev%2520and%2520Oleg%2520Y.%2520Rogov%26entry.1292438233%3D%2520%2520Automatic%2520Speaker%2520Verification%2520%2528ASV%2529%2520systems%252C%2520which%2520identify%2520speakers%2520based%250Aon%2520their%2520voice%2520characteristics%252C%2520have%2520numerous%2520applications%252C%2520such%2520as%2520user%250Aauthentication%2520in%2520financial%2520transactions%252C%2520exclusive%2520access%2520control%2520in%2520smart%250Adevices%252C%2520and%2520forensic%2520fraud%2520detection.%2520However%252C%2520the%2520advancement%2520of%2520deep%250Alearning%2520algorithms%2520has%2520enabled%2520the%2520generation%2520of%2520synthetic%2520audio%2520through%250AText-to-Speech%2520%2528TTS%2529%2520and%2520Voice%2520Conversion%2520%2528VC%2529%2520systems%252C%2520exposing%2520ASV%2520systems%2520to%250Apotential%2520vulnerabilities.%2520To%2520counteract%2520this%252C%2520we%2520propose%2520a%2520novel%2520architecture%250Anamed%2520AASIST3.%2520By%2520enhancing%2520the%2520existing%2520AASIST%2520framework%2520with%250AKolmogorov-Arnold%2520networks%252C%2520additional%2520layers%252C%2520encoders%252C%2520and%2520pre-emphasis%250Atechniques%252C%2520AASIST3%2520achieves%2520a%2520more%2520than%2520twofold%2520improvement%2520in%2520performance.%2520It%250Ademonstrates%2520minDCF%2520results%2520of%25200.5357%2520in%2520the%2520closed%2520condition%2520and%25200.1414%2520in%2520the%250Aopen%2520condition%252C%2520significantly%2520enhancing%2520the%2520detection%2520of%2520synthetic%2520voices%2520and%250Aimproving%2520ASV%2520security.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AASIST3%3A%20KAN-Enhanced%20AASIST%20Speech%20Deepfake%20Detection%20using%20SSL%0A%20%20Features%20and%20Additional%20Regularization%20for%20the%20ASVspoof%202024%20Challenge&entry.906535625=Kirill%20Borodin%20and%20Vasiliy%20Kudryavtsev%20and%20Dmitrii%20Korzh%20and%20Alexey%20Efimenko%20and%20Grach%20Mkrtchian%20and%20Mikhail%20Gorodnichev%20and%20Oleg%20Y.%20Rogov&entry.1292438233=%20%20Automatic%20Speaker%20Verification%20%28ASV%29%20systems%2C%20which%20identify%20speakers%20based%0Aon%20their%20voice%20characteristics%2C%20have%20numerous%20applications%2C%20such%20as%20user%0Aauthentication%20in%20financial%20transactions%2C%20exclusive%20access%20control%20in%20smart%0Adevices%2C%20and%20forensic%20fraud%20detection.%20However%2C%20the%20advancement%20of%20deep%0Alearning%20algorithms%20has%20enabled%20the%20generation%20of%20synthetic%20audio%20through%0AText-to-Speech%20%28TTS%29%20and%20Voice%20Conversion%20%28VC%29%20systems%2C%20exposing%20ASV%20systems%20to%0Apotential%20vulnerabilities.%20To%20counteract%20this%2C%20we%20propose%20a%20novel%20architecture%0Anamed%20AASIST3.%20By%20enhancing%20the%20existing%20AASIST%20framework%20with%0AKolmogorov-Arnold%20networks%2C%20additional%20layers%2C%20encoders%2C%20and%20pre-emphasis%0Atechniques%2C%20AASIST3%20achieves%20a%20more%20than%20twofold%20improvement%20in%20performance.%20It%0Ademonstrates%20minDCF%20results%20of%200.5357%20in%20the%20closed%20condition%20and%200.1414%20in%20the%0Aopen%20condition%2C%20significantly%20enhancing%20the%20detection%20of%20synthetic%20voices%20and%0Aimproving%20ASV%20security.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17352v1&entry.124074799=Read"},
{"title": "A Permuted Autoregressive Approach to Word-Level Recognition for Urdu\n  Digital Text", "author": "Ahmed Mustafa and Muhammad Tahir Rafique and Muhammad Ijlal Baig and Hasan Sajid and Muhammad Jawad Khan and Karam Dad Kallu", "abstract": "  This research paper introduces a novel word-level Optical Character\nRecognition (OCR) model specifically designed for digital Urdu text, leveraging\ntransformer-based architectures and attention mechanisms to address the\ndistinct challenges of Urdu script recognition, including its diverse text\nstyles, fonts, and variations. The model employs a permuted autoregressive\nsequence (PARSeq) architecture, which enhances its performance by enabling\ncontext-aware inference and iterative refinement through the training of\nmultiple token permutations. This method allows the model to adeptly manage\ncharacter reordering and overlapping characters, commonly encountered in Urdu\nscript. Trained on a dataset comprising approximately 160,000 Urdu text images,\nthe model demonstrates a high level of accuracy in capturing the intricacies of\nUrdu script, achieving a CER of 0.178. Despite ongoing challenges in handling\ncertain text variations, the model exhibits superior accuracy and effectiveness\nin practical applications. Future work will focus on refining the model through\nadvanced data augmentation techniques and the integration of context-aware\nlanguage models to further enhance its performance and robustness in Urdu text\nrecognition.\n", "link": "http://arxiv.org/abs/2408.15119v3", "date": "2024-08-30", "relevancy": 1.8166, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4805}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4493}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Permuted%20Autoregressive%20Approach%20to%20Word-Level%20Recognition%20for%20Urdu%0A%20%20Digital%20Text&body=Title%3A%20A%20Permuted%20Autoregressive%20Approach%20to%20Word-Level%20Recognition%20for%20Urdu%0A%20%20Digital%20Text%0AAuthor%3A%20Ahmed%20Mustafa%20and%20Muhammad%20Tahir%20Rafique%20and%20Muhammad%20Ijlal%20Baig%20and%20Hasan%20Sajid%20and%20Muhammad%20Jawad%20Khan%20and%20Karam%20Dad%20Kallu%0AAbstract%3A%20%20%20This%20research%20paper%20introduces%20a%20novel%20word-level%20Optical%20Character%0ARecognition%20%28OCR%29%20model%20specifically%20designed%20for%20digital%20Urdu%20text%2C%20leveraging%0Atransformer-based%20architectures%20and%20attention%20mechanisms%20to%20address%20the%0Adistinct%20challenges%20of%20Urdu%20script%20recognition%2C%20including%20its%20diverse%20text%0Astyles%2C%20fonts%2C%20and%20variations.%20The%20model%20employs%20a%20permuted%20autoregressive%0Asequence%20%28PARSeq%29%20architecture%2C%20which%20enhances%20its%20performance%20by%20enabling%0Acontext-aware%20inference%20and%20iterative%20refinement%20through%20the%20training%20of%0Amultiple%20token%20permutations.%20This%20method%20allows%20the%20model%20to%20adeptly%20manage%0Acharacter%20reordering%20and%20overlapping%20characters%2C%20commonly%20encountered%20in%20Urdu%0Ascript.%20Trained%20on%20a%20dataset%20comprising%20approximately%20160%2C000%20Urdu%20text%20images%2C%0Athe%20model%20demonstrates%20a%20high%20level%20of%20accuracy%20in%20capturing%20the%20intricacies%20of%0AUrdu%20script%2C%20achieving%20a%20CER%20of%200.178.%20Despite%20ongoing%20challenges%20in%20handling%0Acertain%20text%20variations%2C%20the%20model%20exhibits%20superior%20accuracy%20and%20effectiveness%0Ain%20practical%20applications.%20Future%20work%20will%20focus%20on%20refining%20the%20model%20through%0Aadvanced%20data%20augmentation%20techniques%20and%20the%20integration%20of%20context-aware%0Alanguage%20models%20to%20further%20enhance%20its%20performance%20and%20robustness%20in%20Urdu%20text%0Arecognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15119v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Permuted%2520Autoregressive%2520Approach%2520to%2520Word-Level%2520Recognition%2520for%2520Urdu%250A%2520%2520Digital%2520Text%26entry.906535625%3DAhmed%2520Mustafa%2520and%2520Muhammad%2520Tahir%2520Rafique%2520and%2520Muhammad%2520Ijlal%2520Baig%2520and%2520Hasan%2520Sajid%2520and%2520Muhammad%2520Jawad%2520Khan%2520and%2520Karam%2520Dad%2520Kallu%26entry.1292438233%3D%2520%2520This%2520research%2520paper%2520introduces%2520a%2520novel%2520word-level%2520Optical%2520Character%250ARecognition%2520%2528OCR%2529%2520model%2520specifically%2520designed%2520for%2520digital%2520Urdu%2520text%252C%2520leveraging%250Atransformer-based%2520architectures%2520and%2520attention%2520mechanisms%2520to%2520address%2520the%250Adistinct%2520challenges%2520of%2520Urdu%2520script%2520recognition%252C%2520including%2520its%2520diverse%2520text%250Astyles%252C%2520fonts%252C%2520and%2520variations.%2520The%2520model%2520employs%2520a%2520permuted%2520autoregressive%250Asequence%2520%2528PARSeq%2529%2520architecture%252C%2520which%2520enhances%2520its%2520performance%2520by%2520enabling%250Acontext-aware%2520inference%2520and%2520iterative%2520refinement%2520through%2520the%2520training%2520of%250Amultiple%2520token%2520permutations.%2520This%2520method%2520allows%2520the%2520model%2520to%2520adeptly%2520manage%250Acharacter%2520reordering%2520and%2520overlapping%2520characters%252C%2520commonly%2520encountered%2520in%2520Urdu%250Ascript.%2520Trained%2520on%2520a%2520dataset%2520comprising%2520approximately%2520160%252C000%2520Urdu%2520text%2520images%252C%250Athe%2520model%2520demonstrates%2520a%2520high%2520level%2520of%2520accuracy%2520in%2520capturing%2520the%2520intricacies%2520of%250AUrdu%2520script%252C%2520achieving%2520a%2520CER%2520of%25200.178.%2520Despite%2520ongoing%2520challenges%2520in%2520handling%250Acertain%2520text%2520variations%252C%2520the%2520model%2520exhibits%2520superior%2520accuracy%2520and%2520effectiveness%250Ain%2520practical%2520applications.%2520Future%2520work%2520will%2520focus%2520on%2520refining%2520the%2520model%2520through%250Aadvanced%2520data%2520augmentation%2520techniques%2520and%2520the%2520integration%2520of%2520context-aware%250Alanguage%2520models%2520to%2520further%2520enhance%2520its%2520performance%2520and%2520robustness%2520in%2520Urdu%2520text%250Arecognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15119v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Permuted%20Autoregressive%20Approach%20to%20Word-Level%20Recognition%20for%20Urdu%0A%20%20Digital%20Text&entry.906535625=Ahmed%20Mustafa%20and%20Muhammad%20Tahir%20Rafique%20and%20Muhammad%20Ijlal%20Baig%20and%20Hasan%20Sajid%20and%20Muhammad%20Jawad%20Khan%20and%20Karam%20Dad%20Kallu&entry.1292438233=%20%20This%20research%20paper%20introduces%20a%20novel%20word-level%20Optical%20Character%0ARecognition%20%28OCR%29%20model%20specifically%20designed%20for%20digital%20Urdu%20text%2C%20leveraging%0Atransformer-based%20architectures%20and%20attention%20mechanisms%20to%20address%20the%0Adistinct%20challenges%20of%20Urdu%20script%20recognition%2C%20including%20its%20diverse%20text%0Astyles%2C%20fonts%2C%20and%20variations.%20The%20model%20employs%20a%20permuted%20autoregressive%0Asequence%20%28PARSeq%29%20architecture%2C%20which%20enhances%20its%20performance%20by%20enabling%0Acontext-aware%20inference%20and%20iterative%20refinement%20through%20the%20training%20of%0Amultiple%20token%20permutations.%20This%20method%20allows%20the%20model%20to%20adeptly%20manage%0Acharacter%20reordering%20and%20overlapping%20characters%2C%20commonly%20encountered%20in%20Urdu%0Ascript.%20Trained%20on%20a%20dataset%20comprising%20approximately%20160%2C000%20Urdu%20text%20images%2C%0Athe%20model%20demonstrates%20a%20high%20level%20of%20accuracy%20in%20capturing%20the%20intricacies%20of%0AUrdu%20script%2C%20achieving%20a%20CER%20of%200.178.%20Despite%20ongoing%20challenges%20in%20handling%0Acertain%20text%20variations%2C%20the%20model%20exhibits%20superior%20accuracy%20and%20effectiveness%0Ain%20practical%20applications.%20Future%20work%20will%20focus%20on%20refining%20the%20model%20through%0Aadvanced%20data%20augmentation%20techniques%20and%20the%20integration%20of%20context-aware%0Alanguage%20models%20to%20further%20enhance%20its%20performance%20and%20robustness%20in%20Urdu%20text%0Arecognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15119v3&entry.124074799=Read"},
{"title": "Recursive Estimation of Conditional Kernel Mean Embeddings", "author": "Ambrus Tam\u00e1s and Bal\u00e1zs Csan\u00e1d Cs\u00e1ji", "abstract": "  Kernel mean embeddings, a widely used technique in machine learning, map\nprobability distributions to elements of a reproducing kernel Hilbert space\n(RKHS). For supervised learning problems, where input-output pairs are\nobserved, the conditional distribution of outputs given the inputs is a key\nobject. The input dependent conditional distribution of an output can be\nencoded with an RKHS valued function, the conditional kernel mean map. In this\npaper we present a new recursive algorithm to estimate the conditional kernel\nmean map in a Hilbert space valued $L_2$ space, that is in a Bochner space. We\nprove the weak and strong $L_2$ consistency of our recursive estimator under\nmild conditions. The idea is to generalize Stone's theorem for Hilbert space\nvalued regression in a locally compact Polish space. We present new insights\nabout conditional kernel mean embeddings and give strong asymptotic bounds\nregarding the convergence of the proposed recursive method. Finally, the\nresults are demonstrated on three application domains: for inputs coming from\nEuclidean spaces, Riemannian manifolds and locally compact subsets of function\nspaces.\n", "link": "http://arxiv.org/abs/2302.05955v2", "date": "2024-08-30", "relevancy": 1.7863, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5107}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4347}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recursive%20Estimation%20of%20Conditional%20Kernel%20Mean%20Embeddings&body=Title%3A%20Recursive%20Estimation%20of%20Conditional%20Kernel%20Mean%20Embeddings%0AAuthor%3A%20Ambrus%20Tam%C3%A1s%20and%20Bal%C3%A1zs%20Csan%C3%A1d%20Cs%C3%A1ji%0AAbstract%3A%20%20%20Kernel%20mean%20embeddings%2C%20a%20widely%20used%20technique%20in%20machine%20learning%2C%20map%0Aprobability%20distributions%20to%20elements%20of%20a%20reproducing%20kernel%20Hilbert%20space%0A%28RKHS%29.%20For%20supervised%20learning%20problems%2C%20where%20input-output%20pairs%20are%0Aobserved%2C%20the%20conditional%20distribution%20of%20outputs%20given%20the%20inputs%20is%20a%20key%0Aobject.%20The%20input%20dependent%20conditional%20distribution%20of%20an%20output%20can%20be%0Aencoded%20with%20an%20RKHS%20valued%20function%2C%20the%20conditional%20kernel%20mean%20map.%20In%20this%0Apaper%20we%20present%20a%20new%20recursive%20algorithm%20to%20estimate%20the%20conditional%20kernel%0Amean%20map%20in%20a%20Hilbert%20space%20valued%20%24L_2%24%20space%2C%20that%20is%20in%20a%20Bochner%20space.%20We%0Aprove%20the%20weak%20and%20strong%20%24L_2%24%20consistency%20of%20our%20recursive%20estimator%20under%0Amild%20conditions.%20The%20idea%20is%20to%20generalize%20Stone%27s%20theorem%20for%20Hilbert%20space%0Avalued%20regression%20in%20a%20locally%20compact%20Polish%20space.%20We%20present%20new%20insights%0Aabout%20conditional%20kernel%20mean%20embeddings%20and%20give%20strong%20asymptotic%20bounds%0Aregarding%20the%20convergence%20of%20the%20proposed%20recursive%20method.%20Finally%2C%20the%0Aresults%20are%20demonstrated%20on%20three%20application%20domains%3A%20for%20inputs%20coming%20from%0AEuclidean%20spaces%2C%20Riemannian%20manifolds%20and%20locally%20compact%20subsets%20of%20function%0Aspaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.05955v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecursive%2520Estimation%2520of%2520Conditional%2520Kernel%2520Mean%2520Embeddings%26entry.906535625%3DAmbrus%2520Tam%25C3%25A1s%2520and%2520Bal%25C3%25A1zs%2520Csan%25C3%25A1d%2520Cs%25C3%25A1ji%26entry.1292438233%3D%2520%2520Kernel%2520mean%2520embeddings%252C%2520a%2520widely%2520used%2520technique%2520in%2520machine%2520learning%252C%2520map%250Aprobability%2520distributions%2520to%2520elements%2520of%2520a%2520reproducing%2520kernel%2520Hilbert%2520space%250A%2528RKHS%2529.%2520For%2520supervised%2520learning%2520problems%252C%2520where%2520input-output%2520pairs%2520are%250Aobserved%252C%2520the%2520conditional%2520distribution%2520of%2520outputs%2520given%2520the%2520inputs%2520is%2520a%2520key%250Aobject.%2520The%2520input%2520dependent%2520conditional%2520distribution%2520of%2520an%2520output%2520can%2520be%250Aencoded%2520with%2520an%2520RKHS%2520valued%2520function%252C%2520the%2520conditional%2520kernel%2520mean%2520map.%2520In%2520this%250Apaper%2520we%2520present%2520a%2520new%2520recursive%2520algorithm%2520to%2520estimate%2520the%2520conditional%2520kernel%250Amean%2520map%2520in%2520a%2520Hilbert%2520space%2520valued%2520%2524L_2%2524%2520space%252C%2520that%2520is%2520in%2520a%2520Bochner%2520space.%2520We%250Aprove%2520the%2520weak%2520and%2520strong%2520%2524L_2%2524%2520consistency%2520of%2520our%2520recursive%2520estimator%2520under%250Amild%2520conditions.%2520The%2520idea%2520is%2520to%2520generalize%2520Stone%2527s%2520theorem%2520for%2520Hilbert%2520space%250Avalued%2520regression%2520in%2520a%2520locally%2520compact%2520Polish%2520space.%2520We%2520present%2520new%2520insights%250Aabout%2520conditional%2520kernel%2520mean%2520embeddings%2520and%2520give%2520strong%2520asymptotic%2520bounds%250Aregarding%2520the%2520convergence%2520of%2520the%2520proposed%2520recursive%2520method.%2520Finally%252C%2520the%250Aresults%2520are%2520demonstrated%2520on%2520three%2520application%2520domains%253A%2520for%2520inputs%2520coming%2520from%250AEuclidean%2520spaces%252C%2520Riemannian%2520manifolds%2520and%2520locally%2520compact%2520subsets%2520of%2520function%250Aspaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.05955v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recursive%20Estimation%20of%20Conditional%20Kernel%20Mean%20Embeddings&entry.906535625=Ambrus%20Tam%C3%A1s%20and%20Bal%C3%A1zs%20Csan%C3%A1d%20Cs%C3%A1ji&entry.1292438233=%20%20Kernel%20mean%20embeddings%2C%20a%20widely%20used%20technique%20in%20machine%20learning%2C%20map%0Aprobability%20distributions%20to%20elements%20of%20a%20reproducing%20kernel%20Hilbert%20space%0A%28RKHS%29.%20For%20supervised%20learning%20problems%2C%20where%20input-output%20pairs%20are%0Aobserved%2C%20the%20conditional%20distribution%20of%20outputs%20given%20the%20inputs%20is%20a%20key%0Aobject.%20The%20input%20dependent%20conditional%20distribution%20of%20an%20output%20can%20be%0Aencoded%20with%20an%20RKHS%20valued%20function%2C%20the%20conditional%20kernel%20mean%20map.%20In%20this%0Apaper%20we%20present%20a%20new%20recursive%20algorithm%20to%20estimate%20the%20conditional%20kernel%0Amean%20map%20in%20a%20Hilbert%20space%20valued%20%24L_2%24%20space%2C%20that%20is%20in%20a%20Bochner%20space.%20We%0Aprove%20the%20weak%20and%20strong%20%24L_2%24%20consistency%20of%20our%20recursive%20estimator%20under%0Amild%20conditions.%20The%20idea%20is%20to%20generalize%20Stone%27s%20theorem%20for%20Hilbert%20space%0Avalued%20regression%20in%20a%20locally%20compact%20Polish%20space.%20We%20present%20new%20insights%0Aabout%20conditional%20kernel%20mean%20embeddings%20and%20give%20strong%20asymptotic%20bounds%0Aregarding%20the%20convergence%20of%20the%20proposed%20recursive%20method.%20Finally%2C%20the%0Aresults%20are%20demonstrated%20on%20three%20application%20domains%3A%20for%20inputs%20coming%20from%0AEuclidean%20spaces%2C%20Riemannian%20manifolds%20and%20locally%20compact%20subsets%20of%20function%0Aspaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.05955v2&entry.124074799=Read"},
{"title": "Language models align with human judgments on key grammatical\n  constructions", "author": "Jennifer Hu and Kyle Mahowald and Gary Lupyan and Anna Ivanova and Roger Levy", "abstract": "  Do large language models (LLMs) make human-like linguistic generalizations?\nDentella et al. (2023) (\"DGL\") prompt several LLMs (\"Is the following sentence\ngrammatically correct in English?\") to elicit grammaticality judgments of 80\nEnglish sentences, concluding that LLMs demonstrate a \"yes-response bias\" and a\n\"failure to distinguish grammatical from ungrammatical sentences\". We\nre-evaluate LLM performance using well-established practices and find that\nDGL's data in fact provide evidence for just how well LLMs capture human\nbehaviors. Models not only achieve high accuracy overall, but also capture\nfine-grained variation in human linguistic judgments.\n", "link": "http://arxiv.org/abs/2402.01676v2", "date": "2024-08-30", "relevancy": 1.7678, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4598}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.448}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20models%20align%20with%20human%20judgments%20on%20key%20grammatical%0A%20%20constructions&body=Title%3A%20Language%20models%20align%20with%20human%20judgments%20on%20key%20grammatical%0A%20%20constructions%0AAuthor%3A%20Jennifer%20Hu%20and%20Kyle%20Mahowald%20and%20Gary%20Lupyan%20and%20Anna%20Ivanova%20and%20Roger%20Levy%0AAbstract%3A%20%20%20Do%20large%20language%20models%20%28LLMs%29%20make%20human-like%20linguistic%20generalizations%3F%0ADentella%20et%20al.%20%282023%29%20%28%22DGL%22%29%20prompt%20several%20LLMs%20%28%22Is%20the%20following%20sentence%0Agrammatically%20correct%20in%20English%3F%22%29%20to%20elicit%20grammaticality%20judgments%20of%2080%0AEnglish%20sentences%2C%20concluding%20that%20LLMs%20demonstrate%20a%20%22yes-response%20bias%22%20and%20a%0A%22failure%20to%20distinguish%20grammatical%20from%20ungrammatical%20sentences%22.%20We%0Are-evaluate%20LLM%20performance%20using%20well-established%20practices%20and%20find%20that%0ADGL%27s%20data%20in%20fact%20provide%20evidence%20for%20just%20how%20well%20LLMs%20capture%20human%0Abehaviors.%20Models%20not%20only%20achieve%20high%20accuracy%20overall%2C%20but%20also%20capture%0Afine-grained%20variation%20in%20human%20linguistic%20judgments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01676v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520models%2520align%2520with%2520human%2520judgments%2520on%2520key%2520grammatical%250A%2520%2520constructions%26entry.906535625%3DJennifer%2520Hu%2520and%2520Kyle%2520Mahowald%2520and%2520Gary%2520Lupyan%2520and%2520Anna%2520Ivanova%2520and%2520Roger%2520Levy%26entry.1292438233%3D%2520%2520Do%2520large%2520language%2520models%2520%2528LLMs%2529%2520make%2520human-like%2520linguistic%2520generalizations%253F%250ADentella%2520et%2520al.%2520%25282023%2529%2520%2528%2522DGL%2522%2529%2520prompt%2520several%2520LLMs%2520%2528%2522Is%2520the%2520following%2520sentence%250Agrammatically%2520correct%2520in%2520English%253F%2522%2529%2520to%2520elicit%2520grammaticality%2520judgments%2520of%252080%250AEnglish%2520sentences%252C%2520concluding%2520that%2520LLMs%2520demonstrate%2520a%2520%2522yes-response%2520bias%2522%2520and%2520a%250A%2522failure%2520to%2520distinguish%2520grammatical%2520from%2520ungrammatical%2520sentences%2522.%2520We%250Are-evaluate%2520LLM%2520performance%2520using%2520well-established%2520practices%2520and%2520find%2520that%250ADGL%2527s%2520data%2520in%2520fact%2520provide%2520evidence%2520for%2520just%2520how%2520well%2520LLMs%2520capture%2520human%250Abehaviors.%2520Models%2520not%2520only%2520achieve%2520high%2520accuracy%2520overall%252C%2520but%2520also%2520capture%250Afine-grained%2520variation%2520in%2520human%2520linguistic%2520judgments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01676v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20models%20align%20with%20human%20judgments%20on%20key%20grammatical%0A%20%20constructions&entry.906535625=Jennifer%20Hu%20and%20Kyle%20Mahowald%20and%20Gary%20Lupyan%20and%20Anna%20Ivanova%20and%20Roger%20Levy&entry.1292438233=%20%20Do%20large%20language%20models%20%28LLMs%29%20make%20human-like%20linguistic%20generalizations%3F%0ADentella%20et%20al.%20%282023%29%20%28%22DGL%22%29%20prompt%20several%20LLMs%20%28%22Is%20the%20following%20sentence%0Agrammatically%20correct%20in%20English%3F%22%29%20to%20elicit%20grammaticality%20judgments%20of%2080%0AEnglish%20sentences%2C%20concluding%20that%20LLMs%20demonstrate%20a%20%22yes-response%20bias%22%20and%20a%0A%22failure%20to%20distinguish%20grammatical%20from%20ungrammatical%20sentences%22.%20We%0Are-evaluate%20LLM%20performance%20using%20well-established%20practices%20and%20find%20that%0ADGL%27s%20data%20in%20fact%20provide%20evidence%20for%20just%20how%20well%20LLMs%20capture%20human%0Abehaviors.%20Models%20not%20only%20achieve%20high%20accuracy%20overall%2C%20but%20also%20capture%0Afine-grained%20variation%20in%20human%20linguistic%20judgments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01676v2&entry.124074799=Read"},
{"title": "Object-Oriented Grid Mapping in Dynamic Environments", "author": "Matti Pekkanen and Francesco Verdoja and Ville Kyrki", "abstract": "  Grid maps, especially occupancy grid maps, are ubiquitous in many mobile\nrobot applications. To simplify the process of learning the map, grid maps\nsubdivide the world into a grid of cells whose occupancies are independently\nestimated using measurements in the perceptual field of the particular cell.\nHowever, the world consists of objects that span multiple cells, which means\nthat measurements falling onto a cell provide evidence of the occupancy of\nother cells belonging to the same object. Current models do not capture this\ncorrelation and, therefore, do not use object-level information for estimating\nthe state of the environment. In this work, we present a way to generalize the\nupdate of grid maps, relaxing the assumption of independence. We propose\nmodeling the relationship between the measurements and the occupancy of each\ncell as a set of latent variables and jointly estimate those variables and the\nposterior of the map. We propose a method to estimate the latent variables by\nclustering based on semantic labels and an extension to the Normal\nDistributions Transform Occupancy Map (NDT-OM) to facilitate the proposed map\nupdate method. We perform comprehensive map creation and localization\nexperiments with real-world data sets and show that the proposed method creates\nbetter maps in highly dynamic environments compared to state-of-the-art\nmethods. Finally, we demonstrate the ability of the proposed method to remove\noccluded objects from the map in a lifelong map update scenario.\n", "link": "http://arxiv.org/abs/2309.08324v2", "date": "2024-08-30", "relevancy": 1.7665, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6061}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5747}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-Oriented%20Grid%20Mapping%20in%20Dynamic%20Environments&body=Title%3A%20Object-Oriented%20Grid%20Mapping%20in%20Dynamic%20Environments%0AAuthor%3A%20Matti%20Pekkanen%20and%20Francesco%20Verdoja%20and%20Ville%20Kyrki%0AAbstract%3A%20%20%20Grid%20maps%2C%20especially%20occupancy%20grid%20maps%2C%20are%20ubiquitous%20in%20many%20mobile%0Arobot%20applications.%20To%20simplify%20the%20process%20of%20learning%20the%20map%2C%20grid%20maps%0Asubdivide%20the%20world%20into%20a%20grid%20of%20cells%20whose%20occupancies%20are%20independently%0Aestimated%20using%20measurements%20in%20the%20perceptual%20field%20of%20the%20particular%20cell.%0AHowever%2C%20the%20world%20consists%20of%20objects%20that%20span%20multiple%20cells%2C%20which%20means%0Athat%20measurements%20falling%20onto%20a%20cell%20provide%20evidence%20of%20the%20occupancy%20of%0Aother%20cells%20belonging%20to%20the%20same%20object.%20Current%20models%20do%20not%20capture%20this%0Acorrelation%20and%2C%20therefore%2C%20do%20not%20use%20object-level%20information%20for%20estimating%0Athe%20state%20of%20the%20environment.%20In%20this%20work%2C%20we%20present%20a%20way%20to%20generalize%20the%0Aupdate%20of%20grid%20maps%2C%20relaxing%20the%20assumption%20of%20independence.%20We%20propose%0Amodeling%20the%20relationship%20between%20the%20measurements%20and%20the%20occupancy%20of%20each%0Acell%20as%20a%20set%20of%20latent%20variables%20and%20jointly%20estimate%20those%20variables%20and%20the%0Aposterior%20of%20the%20map.%20We%20propose%20a%20method%20to%20estimate%20the%20latent%20variables%20by%0Aclustering%20based%20on%20semantic%20labels%20and%20an%20extension%20to%20the%20Normal%0ADistributions%20Transform%20Occupancy%20Map%20%28NDT-OM%29%20to%20facilitate%20the%20proposed%20map%0Aupdate%20method.%20We%20perform%20comprehensive%20map%20creation%20and%20localization%0Aexperiments%20with%20real-world%20data%20sets%20and%20show%20that%20the%20proposed%20method%20creates%0Abetter%20maps%20in%20highly%20dynamic%20environments%20compared%20to%20state-of-the-art%0Amethods.%20Finally%2C%20we%20demonstrate%20the%20ability%20of%20the%20proposed%20method%20to%20remove%0Aoccluded%20objects%20from%20the%20map%20in%20a%20lifelong%20map%20update%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08324v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-Oriented%2520Grid%2520Mapping%2520in%2520Dynamic%2520Environments%26entry.906535625%3DMatti%2520Pekkanen%2520and%2520Francesco%2520Verdoja%2520and%2520Ville%2520Kyrki%26entry.1292438233%3D%2520%2520Grid%2520maps%252C%2520especially%2520occupancy%2520grid%2520maps%252C%2520are%2520ubiquitous%2520in%2520many%2520mobile%250Arobot%2520applications.%2520To%2520simplify%2520the%2520process%2520of%2520learning%2520the%2520map%252C%2520grid%2520maps%250Asubdivide%2520the%2520world%2520into%2520a%2520grid%2520of%2520cells%2520whose%2520occupancies%2520are%2520independently%250Aestimated%2520using%2520measurements%2520in%2520the%2520perceptual%2520field%2520of%2520the%2520particular%2520cell.%250AHowever%252C%2520the%2520world%2520consists%2520of%2520objects%2520that%2520span%2520multiple%2520cells%252C%2520which%2520means%250Athat%2520measurements%2520falling%2520onto%2520a%2520cell%2520provide%2520evidence%2520of%2520the%2520occupancy%2520of%250Aother%2520cells%2520belonging%2520to%2520the%2520same%2520object.%2520Current%2520models%2520do%2520not%2520capture%2520this%250Acorrelation%2520and%252C%2520therefore%252C%2520do%2520not%2520use%2520object-level%2520information%2520for%2520estimating%250Athe%2520state%2520of%2520the%2520environment.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520way%2520to%2520generalize%2520the%250Aupdate%2520of%2520grid%2520maps%252C%2520relaxing%2520the%2520assumption%2520of%2520independence.%2520We%2520propose%250Amodeling%2520the%2520relationship%2520between%2520the%2520measurements%2520and%2520the%2520occupancy%2520of%2520each%250Acell%2520as%2520a%2520set%2520of%2520latent%2520variables%2520and%2520jointly%2520estimate%2520those%2520variables%2520and%2520the%250Aposterior%2520of%2520the%2520map.%2520We%2520propose%2520a%2520method%2520to%2520estimate%2520the%2520latent%2520variables%2520by%250Aclustering%2520based%2520on%2520semantic%2520labels%2520and%2520an%2520extension%2520to%2520the%2520Normal%250ADistributions%2520Transform%2520Occupancy%2520Map%2520%2528NDT-OM%2529%2520to%2520facilitate%2520the%2520proposed%2520map%250Aupdate%2520method.%2520We%2520perform%2520comprehensive%2520map%2520creation%2520and%2520localization%250Aexperiments%2520with%2520real-world%2520data%2520sets%2520and%2520show%2520that%2520the%2520proposed%2520method%2520creates%250Abetter%2520maps%2520in%2520highly%2520dynamic%2520environments%2520compared%2520to%2520state-of-the-art%250Amethods.%2520Finally%252C%2520we%2520demonstrate%2520the%2520ability%2520of%2520the%2520proposed%2520method%2520to%2520remove%250Aoccluded%2520objects%2520from%2520the%2520map%2520in%2520a%2520lifelong%2520map%2520update%2520scenario.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08324v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-Oriented%20Grid%20Mapping%20in%20Dynamic%20Environments&entry.906535625=Matti%20Pekkanen%20and%20Francesco%20Verdoja%20and%20Ville%20Kyrki&entry.1292438233=%20%20Grid%20maps%2C%20especially%20occupancy%20grid%20maps%2C%20are%20ubiquitous%20in%20many%20mobile%0Arobot%20applications.%20To%20simplify%20the%20process%20of%20learning%20the%20map%2C%20grid%20maps%0Asubdivide%20the%20world%20into%20a%20grid%20of%20cells%20whose%20occupancies%20are%20independently%0Aestimated%20using%20measurements%20in%20the%20perceptual%20field%20of%20the%20particular%20cell.%0AHowever%2C%20the%20world%20consists%20of%20objects%20that%20span%20multiple%20cells%2C%20which%20means%0Athat%20measurements%20falling%20onto%20a%20cell%20provide%20evidence%20of%20the%20occupancy%20of%0Aother%20cells%20belonging%20to%20the%20same%20object.%20Current%20models%20do%20not%20capture%20this%0Acorrelation%20and%2C%20therefore%2C%20do%20not%20use%20object-level%20information%20for%20estimating%0Athe%20state%20of%20the%20environment.%20In%20this%20work%2C%20we%20present%20a%20way%20to%20generalize%20the%0Aupdate%20of%20grid%20maps%2C%20relaxing%20the%20assumption%20of%20independence.%20We%20propose%0Amodeling%20the%20relationship%20between%20the%20measurements%20and%20the%20occupancy%20of%20each%0Acell%20as%20a%20set%20of%20latent%20variables%20and%20jointly%20estimate%20those%20variables%20and%20the%0Aposterior%20of%20the%20map.%20We%20propose%20a%20method%20to%20estimate%20the%20latent%20variables%20by%0Aclustering%20based%20on%20semantic%20labels%20and%20an%20extension%20to%20the%20Normal%0ADistributions%20Transform%20Occupancy%20Map%20%28NDT-OM%29%20to%20facilitate%20the%20proposed%20map%0Aupdate%20method.%20We%20perform%20comprehensive%20map%20creation%20and%20localization%0Aexperiments%20with%20real-world%20data%20sets%20and%20show%20that%20the%20proposed%20method%20creates%0Abetter%20maps%20in%20highly%20dynamic%20environments%20compared%20to%20state-of-the-art%0Amethods.%20Finally%2C%20we%20demonstrate%20the%20ability%20of%20the%20proposed%20method%20to%20remove%0Aoccluded%20objects%20from%20the%20map%20in%20a%20lifelong%20map%20update%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08324v2&entry.124074799=Read"},
{"title": "Geometry of Lightning Self-Attention: Identifiability and Dimension", "author": "Nathan W. Henry and Giovanni Luca Marchetti and Kathl\u00e9n Kohn", "abstract": "  We consider function spaces defined by self-attention networks without\nnormalization, and theoretically analyze their geometry. Since these networks\nare polynomial, we rely on tools from algebraic geometry. In particular, we\nstudy the identifiability of deep attention by providing a description of the\ngeneric fibers of the parametrization for an arbitrary number of layers and, as\na consequence, compute the dimension of the function space. Additionally, for a\nsingle-layer model, we characterize the singular and boundary points. Finally,\nwe formulate a conjectural extension of our results to normalized\nself-attention networks, prove it for a single layer, and numerically verify it\nin the deep case.\n", "link": "http://arxiv.org/abs/2408.17221v1", "date": "2024-08-30", "relevancy": 1.7489, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4464}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4393}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20of%20Lightning%20Self-Attention%3A%20Identifiability%20and%20Dimension&body=Title%3A%20Geometry%20of%20Lightning%20Self-Attention%3A%20Identifiability%20and%20Dimension%0AAuthor%3A%20Nathan%20W.%20Henry%20and%20Giovanni%20Luca%20Marchetti%20and%20Kathl%C3%A9n%20Kohn%0AAbstract%3A%20%20%20We%20consider%20function%20spaces%20defined%20by%20self-attention%20networks%20without%0Anormalization%2C%20and%20theoretically%20analyze%20their%20geometry.%20Since%20these%20networks%0Aare%20polynomial%2C%20we%20rely%20on%20tools%20from%20algebraic%20geometry.%20In%20particular%2C%20we%0Astudy%20the%20identifiability%20of%20deep%20attention%20by%20providing%20a%20description%20of%20the%0Ageneric%20fibers%20of%20the%20parametrization%20for%20an%20arbitrary%20number%20of%20layers%20and%2C%20as%0Aa%20consequence%2C%20compute%20the%20dimension%20of%20the%20function%20space.%20Additionally%2C%20for%20a%0Asingle-layer%20model%2C%20we%20characterize%20the%20singular%20and%20boundary%20points.%20Finally%2C%0Awe%20formulate%20a%20conjectural%20extension%20of%20our%20results%20to%20normalized%0Aself-attention%20networks%2C%20prove%20it%20for%20a%20single%20layer%2C%20and%20numerically%20verify%20it%0Ain%20the%20deep%20case.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520of%2520Lightning%2520Self-Attention%253A%2520Identifiability%2520and%2520Dimension%26entry.906535625%3DNathan%2520W.%2520Henry%2520and%2520Giovanni%2520Luca%2520Marchetti%2520and%2520Kathl%25C3%25A9n%2520Kohn%26entry.1292438233%3D%2520%2520We%2520consider%2520function%2520spaces%2520defined%2520by%2520self-attention%2520networks%2520without%250Anormalization%252C%2520and%2520theoretically%2520analyze%2520their%2520geometry.%2520Since%2520these%2520networks%250Aare%2520polynomial%252C%2520we%2520rely%2520on%2520tools%2520from%2520algebraic%2520geometry.%2520In%2520particular%252C%2520we%250Astudy%2520the%2520identifiability%2520of%2520deep%2520attention%2520by%2520providing%2520a%2520description%2520of%2520the%250Ageneric%2520fibers%2520of%2520the%2520parametrization%2520for%2520an%2520arbitrary%2520number%2520of%2520layers%2520and%252C%2520as%250Aa%2520consequence%252C%2520compute%2520the%2520dimension%2520of%2520the%2520function%2520space.%2520Additionally%252C%2520for%2520a%250Asingle-layer%2520model%252C%2520we%2520characterize%2520the%2520singular%2520and%2520boundary%2520points.%2520Finally%252C%250Awe%2520formulate%2520a%2520conjectural%2520extension%2520of%2520our%2520results%2520to%2520normalized%250Aself-attention%2520networks%252C%2520prove%2520it%2520for%2520a%2520single%2520layer%252C%2520and%2520numerically%2520verify%2520it%250Ain%2520the%2520deep%2520case.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20of%20Lightning%20Self-Attention%3A%20Identifiability%20and%20Dimension&entry.906535625=Nathan%20W.%20Henry%20and%20Giovanni%20Luca%20Marchetti%20and%20Kathl%C3%A9n%20Kohn&entry.1292438233=%20%20We%20consider%20function%20spaces%20defined%20by%20self-attention%20networks%20without%0Anormalization%2C%20and%20theoretically%20analyze%20their%20geometry.%20Since%20these%20networks%0Aare%20polynomial%2C%20we%20rely%20on%20tools%20from%20algebraic%20geometry.%20In%20particular%2C%20we%0Astudy%20the%20identifiability%20of%20deep%20attention%20by%20providing%20a%20description%20of%20the%0Ageneric%20fibers%20of%20the%20parametrization%20for%20an%20arbitrary%20number%20of%20layers%20and%2C%20as%0Aa%20consequence%2C%20compute%20the%20dimension%20of%20the%20function%20space.%20Additionally%2C%20for%20a%0Asingle-layer%20model%2C%20we%20characterize%20the%20singular%20and%20boundary%20points.%20Finally%2C%0Awe%20formulate%20a%20conjectural%20extension%20of%20our%20results%20to%20normalized%0Aself-attention%20networks%2C%20prove%20it%20for%20a%20single%20layer%2C%20and%20numerically%20verify%20it%0Ain%20the%20deep%20case.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17221v1&entry.124074799=Read"},
{"title": "\"Benefit Game: Alien Seaweed Swarms\" -- Real-time Gamification of\n  Digital Seaweed Ecology", "author": "Dan-Lu Fei and Zi-Wei Wu and Kang Zhang", "abstract": "  \"Benefit Game: Alien Seaweed Swarms\" combines artificial life art and\ninteractive game with installation to explore the impact of human activity on\nfragile seaweed ecosystems. The project aims to promote ecological\nconsciousness by creating a balance in digital seaweed ecologies. Inspired by\nthe real species \"Laminaria saccharina\", the author employs Procedural Content\nGeneration via Machine Learning technology to generate variations of virtual\nseaweeds and symbiotic fungi. The audience can explore the consequences of\nhuman activities through gameplay and observe the ecosystem's feedback on the\nbenefits and risks of seaweed aquaculture. This Benefit Game offers dynamic and\nreal-time responsive artificial seaweed ecosystems for an interactive\nexperience that enhances ecological consciousness.\n", "link": "http://arxiv.org/abs/2408.17186v1", "date": "2024-08-30", "relevancy": 1.7487, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4611}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.446}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22Benefit%20Game%3A%20Alien%20Seaweed%20Swarms%22%20--%20Real-time%20Gamification%20of%0A%20%20Digital%20Seaweed%20Ecology&body=Title%3A%20%22Benefit%20Game%3A%20Alien%20Seaweed%20Swarms%22%20--%20Real-time%20Gamification%20of%0A%20%20Digital%20Seaweed%20Ecology%0AAuthor%3A%20Dan-Lu%20Fei%20and%20Zi-Wei%20Wu%20and%20Kang%20Zhang%0AAbstract%3A%20%20%20%22Benefit%20Game%3A%20Alien%20Seaweed%20Swarms%22%20combines%20artificial%20life%20art%20and%0Ainteractive%20game%20with%20installation%20to%20explore%20the%20impact%20of%20human%20activity%20on%0Afragile%20seaweed%20ecosystems.%20The%20project%20aims%20to%20promote%20ecological%0Aconsciousness%20by%20creating%20a%20balance%20in%20digital%20seaweed%20ecologies.%20Inspired%20by%0Athe%20real%20species%20%22Laminaria%20saccharina%22%2C%20the%20author%20employs%20Procedural%20Content%0AGeneration%20via%20Machine%20Learning%20technology%20to%20generate%20variations%20of%20virtual%0Aseaweeds%20and%20symbiotic%20fungi.%20The%20audience%20can%20explore%20the%20consequences%20of%0Ahuman%20activities%20through%20gameplay%20and%20observe%20the%20ecosystem%27s%20feedback%20on%20the%0Abenefits%20and%20risks%20of%20seaweed%20aquaculture.%20This%20Benefit%20Game%20offers%20dynamic%20and%0Areal-time%20responsive%20artificial%20seaweed%20ecosystems%20for%20an%20interactive%0Aexperience%20that%20enhances%20ecological%20consciousness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522Benefit%2520Game%253A%2520Alien%2520Seaweed%2520Swarms%2522%2520--%2520Real-time%2520Gamification%2520of%250A%2520%2520Digital%2520Seaweed%2520Ecology%26entry.906535625%3DDan-Lu%2520Fei%2520and%2520Zi-Wei%2520Wu%2520and%2520Kang%2520Zhang%26entry.1292438233%3D%2520%2520%2522Benefit%2520Game%253A%2520Alien%2520Seaweed%2520Swarms%2522%2520combines%2520artificial%2520life%2520art%2520and%250Ainteractive%2520game%2520with%2520installation%2520to%2520explore%2520the%2520impact%2520of%2520human%2520activity%2520on%250Afragile%2520seaweed%2520ecosystems.%2520The%2520project%2520aims%2520to%2520promote%2520ecological%250Aconsciousness%2520by%2520creating%2520a%2520balance%2520in%2520digital%2520seaweed%2520ecologies.%2520Inspired%2520by%250Athe%2520real%2520species%2520%2522Laminaria%2520saccharina%2522%252C%2520the%2520author%2520employs%2520Procedural%2520Content%250AGeneration%2520via%2520Machine%2520Learning%2520technology%2520to%2520generate%2520variations%2520of%2520virtual%250Aseaweeds%2520and%2520symbiotic%2520fungi.%2520The%2520audience%2520can%2520explore%2520the%2520consequences%2520of%250Ahuman%2520activities%2520through%2520gameplay%2520and%2520observe%2520the%2520ecosystem%2527s%2520feedback%2520on%2520the%250Abenefits%2520and%2520risks%2520of%2520seaweed%2520aquaculture.%2520This%2520Benefit%2520Game%2520offers%2520dynamic%2520and%250Areal-time%2520responsive%2520artificial%2520seaweed%2520ecosystems%2520for%2520an%2520interactive%250Aexperience%2520that%2520enhances%2520ecological%2520consciousness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22Benefit%20Game%3A%20Alien%20Seaweed%20Swarms%22%20--%20Real-time%20Gamification%20of%0A%20%20Digital%20Seaweed%20Ecology&entry.906535625=Dan-Lu%20Fei%20and%20Zi-Wei%20Wu%20and%20Kang%20Zhang&entry.1292438233=%20%20%22Benefit%20Game%3A%20Alien%20Seaweed%20Swarms%22%20combines%20artificial%20life%20art%20and%0Ainteractive%20game%20with%20installation%20to%20explore%20the%20impact%20of%20human%20activity%20on%0Afragile%20seaweed%20ecosystems.%20The%20project%20aims%20to%20promote%20ecological%0Aconsciousness%20by%20creating%20a%20balance%20in%20digital%20seaweed%20ecologies.%20Inspired%20by%0Athe%20real%20species%20%22Laminaria%20saccharina%22%2C%20the%20author%20employs%20Procedural%20Content%0AGeneration%20via%20Machine%20Learning%20technology%20to%20generate%20variations%20of%20virtual%0Aseaweeds%20and%20symbiotic%20fungi.%20The%20audience%20can%20explore%20the%20consequences%20of%0Ahuman%20activities%20through%20gameplay%20and%20observe%20the%20ecosystem%27s%20feedback%20on%20the%0Abenefits%20and%20risks%20of%20seaweed%20aquaculture.%20This%20Benefit%20Game%20offers%20dynamic%20and%0Areal-time%20responsive%20artificial%20seaweed%20ecosystems%20for%20an%20interactive%0Aexperience%20that%20enhances%20ecological%20consciousness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17186v1&entry.124074799=Read"},
{"title": "Invariant Causal Prediction with Local Models", "author": "Alexander Mey and Rui Manuel Castro", "abstract": "  We consider the task of identifying the causal parents of a target variable\namong a set of candidates from observational data. Our main assumption is that\nthe candidate variables are observed in different environments which may, under\ncertain assumptions, be regarded as interventions on the observed system. We\nassume a linear relationship between target and candidates, which can be\ndifferent in each environment with the only restriction that the causal\nstructure is invariant across environments. Within our proposed setting we\nprovide sufficient conditions for identifiability of the causal parents and\nintroduce a practical method called L-ICP ($\\textbf{L}$ocalized\n$\\textbf{I}$nvariant $\\textbf{Ca}$usal $\\textbf{P}$rediction), which is based\non a hypothesis test for parent identification using a ratio of minimum and\nmaximum statistics. We then show in a simplified setting that the statistical\npower of L-ICP converges exponentially fast in the sample size, and finally we\nanalyze the behavior of L-ICP experimentally in more general settings.\n", "link": "http://arxiv.org/abs/2401.05218v2", "date": "2024-08-30", "relevancy": 1.7369, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4596}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4357}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invariant%20Causal%20Prediction%20with%20Local%20Models&body=Title%3A%20Invariant%20Causal%20Prediction%20with%20Local%20Models%0AAuthor%3A%20Alexander%20Mey%20and%20Rui%20Manuel%20Castro%0AAbstract%3A%20%20%20We%20consider%20the%20task%20of%20identifying%20the%20causal%20parents%20of%20a%20target%20variable%0Aamong%20a%20set%20of%20candidates%20from%20observational%20data.%20Our%20main%20assumption%20is%20that%0Athe%20candidate%20variables%20are%20observed%20in%20different%20environments%20which%20may%2C%20under%0Acertain%20assumptions%2C%20be%20regarded%20as%20interventions%20on%20the%20observed%20system.%20We%0Aassume%20a%20linear%20relationship%20between%20target%20and%20candidates%2C%20which%20can%20be%0Adifferent%20in%20each%20environment%20with%20the%20only%20restriction%20that%20the%20causal%0Astructure%20is%20invariant%20across%20environments.%20Within%20our%20proposed%20setting%20we%0Aprovide%20sufficient%20conditions%20for%20identifiability%20of%20the%20causal%20parents%20and%0Aintroduce%20a%20practical%20method%20called%20L-ICP%20%28%24%5Ctextbf%7BL%7D%24ocalized%0A%24%5Ctextbf%7BI%7D%24nvariant%20%24%5Ctextbf%7BCa%7D%24usal%20%24%5Ctextbf%7BP%7D%24rediction%29%2C%20which%20is%20based%0Aon%20a%20hypothesis%20test%20for%20parent%20identification%20using%20a%20ratio%20of%20minimum%20and%0Amaximum%20statistics.%20We%20then%20show%20in%20a%20simplified%20setting%20that%20the%20statistical%0Apower%20of%20L-ICP%20converges%20exponentially%20fast%20in%20the%20sample%20size%2C%20and%20finally%20we%0Aanalyze%20the%20behavior%20of%20L-ICP%20experimentally%20in%20more%20general%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.05218v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvariant%2520Causal%2520Prediction%2520with%2520Local%2520Models%26entry.906535625%3DAlexander%2520Mey%2520and%2520Rui%2520Manuel%2520Castro%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520task%2520of%2520identifying%2520the%2520causal%2520parents%2520of%2520a%2520target%2520variable%250Aamong%2520a%2520set%2520of%2520candidates%2520from%2520observational%2520data.%2520Our%2520main%2520assumption%2520is%2520that%250Athe%2520candidate%2520variables%2520are%2520observed%2520in%2520different%2520environments%2520which%2520may%252C%2520under%250Acertain%2520assumptions%252C%2520be%2520regarded%2520as%2520interventions%2520on%2520the%2520observed%2520system.%2520We%250Aassume%2520a%2520linear%2520relationship%2520between%2520target%2520and%2520candidates%252C%2520which%2520can%2520be%250Adifferent%2520in%2520each%2520environment%2520with%2520the%2520only%2520restriction%2520that%2520the%2520causal%250Astructure%2520is%2520invariant%2520across%2520environments.%2520Within%2520our%2520proposed%2520setting%2520we%250Aprovide%2520sufficient%2520conditions%2520for%2520identifiability%2520of%2520the%2520causal%2520parents%2520and%250Aintroduce%2520a%2520practical%2520method%2520called%2520L-ICP%2520%2528%2524%255Ctextbf%257BL%257D%2524ocalized%250A%2524%255Ctextbf%257BI%257D%2524nvariant%2520%2524%255Ctextbf%257BCa%257D%2524usal%2520%2524%255Ctextbf%257BP%257D%2524rediction%2529%252C%2520which%2520is%2520based%250Aon%2520a%2520hypothesis%2520test%2520for%2520parent%2520identification%2520using%2520a%2520ratio%2520of%2520minimum%2520and%250Amaximum%2520statistics.%2520We%2520then%2520show%2520in%2520a%2520simplified%2520setting%2520that%2520the%2520statistical%250Apower%2520of%2520L-ICP%2520converges%2520exponentially%2520fast%2520in%2520the%2520sample%2520size%252C%2520and%2520finally%2520we%250Aanalyze%2520the%2520behavior%2520of%2520L-ICP%2520experimentally%2520in%2520more%2520general%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.05218v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invariant%20Causal%20Prediction%20with%20Local%20Models&entry.906535625=Alexander%20Mey%20and%20Rui%20Manuel%20Castro&entry.1292438233=%20%20We%20consider%20the%20task%20of%20identifying%20the%20causal%20parents%20of%20a%20target%20variable%0Aamong%20a%20set%20of%20candidates%20from%20observational%20data.%20Our%20main%20assumption%20is%20that%0Athe%20candidate%20variables%20are%20observed%20in%20different%20environments%20which%20may%2C%20under%0Acertain%20assumptions%2C%20be%20regarded%20as%20interventions%20on%20the%20observed%20system.%20We%0Aassume%20a%20linear%20relationship%20between%20target%20and%20candidates%2C%20which%20can%20be%0Adifferent%20in%20each%20environment%20with%20the%20only%20restriction%20that%20the%20causal%0Astructure%20is%20invariant%20across%20environments.%20Within%20our%20proposed%20setting%20we%0Aprovide%20sufficient%20conditions%20for%20identifiability%20of%20the%20causal%20parents%20and%0Aintroduce%20a%20practical%20method%20called%20L-ICP%20%28%24%5Ctextbf%7BL%7D%24ocalized%0A%24%5Ctextbf%7BI%7D%24nvariant%20%24%5Ctextbf%7BCa%7D%24usal%20%24%5Ctextbf%7BP%7D%24rediction%29%2C%20which%20is%20based%0Aon%20a%20hypothesis%20test%20for%20parent%20identification%20using%20a%20ratio%20of%20minimum%20and%0Amaximum%20statistics.%20We%20then%20show%20in%20a%20simplified%20setting%20that%20the%20statistical%0Apower%20of%20L-ICP%20converges%20exponentially%20fast%20in%20the%20sample%20size%2C%20and%20finally%20we%0Aanalyze%20the%20behavior%20of%20L-ICP%20experimentally%20in%20more%20general%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.05218v2&entry.124074799=Read"},
{"title": "GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion\n  Models and Large Language Models", "author": "Jian Ma and Yonglin Deng and Chen Chen and Haonan Lu and Zhenyu Yang", "abstract": "  Posters play a crucial role in marketing and advertising by enhancing visual\ncommunication and brand visibility, making significant contributions to\nindustrial design. With the latest advancements in controllable T2I diffusion\nmodels, increasing research has focused on rendering text within synthesized\nimages. Despite improvements in text rendering accuracy, the field of automatic\nposter generation remains underexplored. In this paper, we propose an automatic\nposter generation framework with text rendering capabilities leveraging LLMs,\nutilizing a triple-cross attention mechanism based on alignment learning. This\nframework aims to create precise poster text within a detailed contextual\nbackground. Additionally, the framework supports controllable fonts, adjustable\nimage resolution, and the rendering of posters with descriptions and text in\nboth English and Chinese.Furthermore, we introduce a high-resolution font\ndataset and a poster dataset with resolutions exceeding 1024 pixels. Our\napproach leverages the SDXL architecture. Extensive experiments validate our\nmethod's capability in generating poster images with complex and contextually\nrich backgrounds.Codes is available at\nhttps://github.com/OPPO-Mente-Lab/GlyphDraw2.\n", "link": "http://arxiv.org/abs/2407.02252v2", "date": "2024-08-30", "relevancy": 1.7367, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5949}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5836}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GlyphDraw2%3A%20Automatic%20Generation%20of%20Complex%20Glyph%20Posters%20with%20Diffusion%0A%20%20Models%20and%20Large%20Language%20Models&body=Title%3A%20GlyphDraw2%3A%20Automatic%20Generation%20of%20Complex%20Glyph%20Posters%20with%20Diffusion%0A%20%20Models%20and%20Large%20Language%20Models%0AAuthor%3A%20Jian%20Ma%20and%20Yonglin%20Deng%20and%20Chen%20Chen%20and%20Haonan%20Lu%20and%20Zhenyu%20Yang%0AAbstract%3A%20%20%20Posters%20play%20a%20crucial%20role%20in%20marketing%20and%20advertising%20by%20enhancing%20visual%0Acommunication%20and%20brand%20visibility%2C%20making%20significant%20contributions%20to%0Aindustrial%20design.%20With%20the%20latest%20advancements%20in%20controllable%20T2I%20diffusion%0Amodels%2C%20increasing%20research%20has%20focused%20on%20rendering%20text%20within%20synthesized%0Aimages.%20Despite%20improvements%20in%20text%20rendering%20accuracy%2C%20the%20field%20of%20automatic%0Aposter%20generation%20remains%20underexplored.%20In%20this%20paper%2C%20we%20propose%20an%20automatic%0Aposter%20generation%20framework%20with%20text%20rendering%20capabilities%20leveraging%20LLMs%2C%0Autilizing%20a%20triple-cross%20attention%20mechanism%20based%20on%20alignment%20learning.%20This%0Aframework%20aims%20to%20create%20precise%20poster%20text%20within%20a%20detailed%20contextual%0Abackground.%20Additionally%2C%20the%20framework%20supports%20controllable%20fonts%2C%20adjustable%0Aimage%20resolution%2C%20and%20the%20rendering%20of%20posters%20with%20descriptions%20and%20text%20in%0Aboth%20English%20and%20Chinese.Furthermore%2C%20we%20introduce%20a%20high-resolution%20font%0Adataset%20and%20a%20poster%20dataset%20with%20resolutions%20exceeding%201024%20pixels.%20Our%0Aapproach%20leverages%20the%20SDXL%20architecture.%20Extensive%20experiments%20validate%20our%0Amethod%27s%20capability%20in%20generating%20poster%20images%20with%20complex%20and%20contextually%0Arich%20backgrounds.Codes%20is%20available%20at%0Ahttps%3A//github.com/OPPO-Mente-Lab/GlyphDraw2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlyphDraw2%253A%2520Automatic%2520Generation%2520of%2520Complex%2520Glyph%2520Posters%2520with%2520Diffusion%250A%2520%2520Models%2520and%2520Large%2520Language%2520Models%26entry.906535625%3DJian%2520Ma%2520and%2520Yonglin%2520Deng%2520and%2520Chen%2520Chen%2520and%2520Haonan%2520Lu%2520and%2520Zhenyu%2520Yang%26entry.1292438233%3D%2520%2520Posters%2520play%2520a%2520crucial%2520role%2520in%2520marketing%2520and%2520advertising%2520by%2520enhancing%2520visual%250Acommunication%2520and%2520brand%2520visibility%252C%2520making%2520significant%2520contributions%2520to%250Aindustrial%2520design.%2520With%2520the%2520latest%2520advancements%2520in%2520controllable%2520T2I%2520diffusion%250Amodels%252C%2520increasing%2520research%2520has%2520focused%2520on%2520rendering%2520text%2520within%2520synthesized%250Aimages.%2520Despite%2520improvements%2520in%2520text%2520rendering%2520accuracy%252C%2520the%2520field%2520of%2520automatic%250Aposter%2520generation%2520remains%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520automatic%250Aposter%2520generation%2520framework%2520with%2520text%2520rendering%2520capabilities%2520leveraging%2520LLMs%252C%250Autilizing%2520a%2520triple-cross%2520attention%2520mechanism%2520based%2520on%2520alignment%2520learning.%2520This%250Aframework%2520aims%2520to%2520create%2520precise%2520poster%2520text%2520within%2520a%2520detailed%2520contextual%250Abackground.%2520Additionally%252C%2520the%2520framework%2520supports%2520controllable%2520fonts%252C%2520adjustable%250Aimage%2520resolution%252C%2520and%2520the%2520rendering%2520of%2520posters%2520with%2520descriptions%2520and%2520text%2520in%250Aboth%2520English%2520and%2520Chinese.Furthermore%252C%2520we%2520introduce%2520a%2520high-resolution%2520font%250Adataset%2520and%2520a%2520poster%2520dataset%2520with%2520resolutions%2520exceeding%25201024%2520pixels.%2520Our%250Aapproach%2520leverages%2520the%2520SDXL%2520architecture.%2520Extensive%2520experiments%2520validate%2520our%250Amethod%2527s%2520capability%2520in%2520generating%2520poster%2520images%2520with%2520complex%2520and%2520contextually%250Arich%2520backgrounds.Codes%2520is%2520available%2520at%250Ahttps%253A//github.com/OPPO-Mente-Lab/GlyphDraw2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GlyphDraw2%3A%20Automatic%20Generation%20of%20Complex%20Glyph%20Posters%20with%20Diffusion%0A%20%20Models%20and%20Large%20Language%20Models&entry.906535625=Jian%20Ma%20and%20Yonglin%20Deng%20and%20Chen%20Chen%20and%20Haonan%20Lu%20and%20Zhenyu%20Yang&entry.1292438233=%20%20Posters%20play%20a%20crucial%20role%20in%20marketing%20and%20advertising%20by%20enhancing%20visual%0Acommunication%20and%20brand%20visibility%2C%20making%20significant%20contributions%20to%0Aindustrial%20design.%20With%20the%20latest%20advancements%20in%20controllable%20T2I%20diffusion%0Amodels%2C%20increasing%20research%20has%20focused%20on%20rendering%20text%20within%20synthesized%0Aimages.%20Despite%20improvements%20in%20text%20rendering%20accuracy%2C%20the%20field%20of%20automatic%0Aposter%20generation%20remains%20underexplored.%20In%20this%20paper%2C%20we%20propose%20an%20automatic%0Aposter%20generation%20framework%20with%20text%20rendering%20capabilities%20leveraging%20LLMs%2C%0Autilizing%20a%20triple-cross%20attention%20mechanism%20based%20on%20alignment%20learning.%20This%0Aframework%20aims%20to%20create%20precise%20poster%20text%20within%20a%20detailed%20contextual%0Abackground.%20Additionally%2C%20the%20framework%20supports%20controllable%20fonts%2C%20adjustable%0Aimage%20resolution%2C%20and%20the%20rendering%20of%20posters%20with%20descriptions%20and%20text%20in%0Aboth%20English%20and%20Chinese.Furthermore%2C%20we%20introduce%20a%20high-resolution%20font%0Adataset%20and%20a%20poster%20dataset%20with%20resolutions%20exceeding%201024%20pixels.%20Our%0Aapproach%20leverages%20the%20SDXL%20architecture.%20Extensive%20experiments%20validate%20our%0Amethod%27s%20capability%20in%20generating%20poster%20images%20with%20complex%20and%20contextually%0Arich%20backgrounds.Codes%20is%20available%20at%0Ahttps%3A//github.com/OPPO-Mente-Lab/GlyphDraw2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02252v2&entry.124074799=Read"},
{"title": "Leveraging Graph Neural Networks to Forecast Electricity Consumption", "author": "Eloi Campagne and Yvenn Amara-Ouali and Yannig Goude and Argyris Kalogeratos", "abstract": "  Accurate electricity demand forecasting is essential for several reasons,\nespecially as the integration of renewable energy sources and the transition to\na decentralized network paradigm introduce greater complexity and uncertainty.\nThe proposed methodology leverages graph-based representations to effectively\ncapture the spatial distribution and relational intricacies inherent in this\ndecentralized network structure. This research work offers a novel approach\nthat extends beyond the conventional Generalized Additive Model framework by\nconsidering models like Graph Convolutional Networks or Graph SAGE. These\ngraph-based models enable the incorporation of various levels of\ninterconnectedness and information sharing among nodes, where each node\ncorresponds to the combined load (i.e. consumption) of a subset of consumers\n(e.g. the regions of a country). More specifically, we introduce a range of\nmethods for inferring graphs tailored to consumption forecasting, along with a\nframework for evaluating the developed models in terms of both performance and\nexplainability. We conduct experiments on electricity forecasting, in both a\nsynthetic and a real framework considering the French mainland regions, and the\nperformance and merits of our approach are discussed.\n", "link": "http://arxiv.org/abs/2408.17366v1", "date": "2024-08-30", "relevancy": 1.7346, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.444}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4319}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Graph%20Neural%20Networks%20to%20Forecast%20Electricity%20Consumption&body=Title%3A%20Leveraging%20Graph%20Neural%20Networks%20to%20Forecast%20Electricity%20Consumption%0AAuthor%3A%20Eloi%20Campagne%20and%20Yvenn%20Amara-Ouali%20and%20Yannig%20Goude%20and%20Argyris%20Kalogeratos%0AAbstract%3A%20%20%20Accurate%20electricity%20demand%20forecasting%20is%20essential%20for%20several%20reasons%2C%0Aespecially%20as%20the%20integration%20of%20renewable%20energy%20sources%20and%20the%20transition%20to%0Aa%20decentralized%20network%20paradigm%20introduce%20greater%20complexity%20and%20uncertainty.%0AThe%20proposed%20methodology%20leverages%20graph-based%20representations%20to%20effectively%0Acapture%20the%20spatial%20distribution%20and%20relational%20intricacies%20inherent%20in%20this%0Adecentralized%20network%20structure.%20This%20research%20work%20offers%20a%20novel%20approach%0Athat%20extends%20beyond%20the%20conventional%20Generalized%20Additive%20Model%20framework%20by%0Aconsidering%20models%20like%20Graph%20Convolutional%20Networks%20or%20Graph%20SAGE.%20These%0Agraph-based%20models%20enable%20the%20incorporation%20of%20various%20levels%20of%0Ainterconnectedness%20and%20information%20sharing%20among%20nodes%2C%20where%20each%20node%0Acorresponds%20to%20the%20combined%20load%20%28i.e.%20consumption%29%20of%20a%20subset%20of%20consumers%0A%28e.g.%20the%20regions%20of%20a%20country%29.%20More%20specifically%2C%20we%20introduce%20a%20range%20of%0Amethods%20for%20inferring%20graphs%20tailored%20to%20consumption%20forecasting%2C%20along%20with%20a%0Aframework%20for%20evaluating%20the%20developed%20models%20in%20terms%20of%20both%20performance%20and%0Aexplainability.%20We%20conduct%20experiments%20on%20electricity%20forecasting%2C%20in%20both%20a%0Asynthetic%20and%20a%20real%20framework%20considering%20the%20French%20mainland%20regions%2C%20and%20the%0Aperformance%20and%20merits%20of%20our%20approach%20are%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Graph%2520Neural%2520Networks%2520to%2520Forecast%2520Electricity%2520Consumption%26entry.906535625%3DEloi%2520Campagne%2520and%2520Yvenn%2520Amara-Ouali%2520and%2520Yannig%2520Goude%2520and%2520Argyris%2520Kalogeratos%26entry.1292438233%3D%2520%2520Accurate%2520electricity%2520demand%2520forecasting%2520is%2520essential%2520for%2520several%2520reasons%252C%250Aespecially%2520as%2520the%2520integration%2520of%2520renewable%2520energy%2520sources%2520and%2520the%2520transition%2520to%250Aa%2520decentralized%2520network%2520paradigm%2520introduce%2520greater%2520complexity%2520and%2520uncertainty.%250AThe%2520proposed%2520methodology%2520leverages%2520graph-based%2520representations%2520to%2520effectively%250Acapture%2520the%2520spatial%2520distribution%2520and%2520relational%2520intricacies%2520inherent%2520in%2520this%250Adecentralized%2520network%2520structure.%2520This%2520research%2520work%2520offers%2520a%2520novel%2520approach%250Athat%2520extends%2520beyond%2520the%2520conventional%2520Generalized%2520Additive%2520Model%2520framework%2520by%250Aconsidering%2520models%2520like%2520Graph%2520Convolutional%2520Networks%2520or%2520Graph%2520SAGE.%2520These%250Agraph-based%2520models%2520enable%2520the%2520incorporation%2520of%2520various%2520levels%2520of%250Ainterconnectedness%2520and%2520information%2520sharing%2520among%2520nodes%252C%2520where%2520each%2520node%250Acorresponds%2520to%2520the%2520combined%2520load%2520%2528i.e.%2520consumption%2529%2520of%2520a%2520subset%2520of%2520consumers%250A%2528e.g.%2520the%2520regions%2520of%2520a%2520country%2529.%2520More%2520specifically%252C%2520we%2520introduce%2520a%2520range%2520of%250Amethods%2520for%2520inferring%2520graphs%2520tailored%2520to%2520consumption%2520forecasting%252C%2520along%2520with%2520a%250Aframework%2520for%2520evaluating%2520the%2520developed%2520models%2520in%2520terms%2520of%2520both%2520performance%2520and%250Aexplainability.%2520We%2520conduct%2520experiments%2520on%2520electricity%2520forecasting%252C%2520in%2520both%2520a%250Asynthetic%2520and%2520a%2520real%2520framework%2520considering%2520the%2520French%2520mainland%2520regions%252C%2520and%2520the%250Aperformance%2520and%2520merits%2520of%2520our%2520approach%2520are%2520discussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Graph%20Neural%20Networks%20to%20Forecast%20Electricity%20Consumption&entry.906535625=Eloi%20Campagne%20and%20Yvenn%20Amara-Ouali%20and%20Yannig%20Goude%20and%20Argyris%20Kalogeratos&entry.1292438233=%20%20Accurate%20electricity%20demand%20forecasting%20is%20essential%20for%20several%20reasons%2C%0Aespecially%20as%20the%20integration%20of%20renewable%20energy%20sources%20and%20the%20transition%20to%0Aa%20decentralized%20network%20paradigm%20introduce%20greater%20complexity%20and%20uncertainty.%0AThe%20proposed%20methodology%20leverages%20graph-based%20representations%20to%20effectively%0Acapture%20the%20spatial%20distribution%20and%20relational%20intricacies%20inherent%20in%20this%0Adecentralized%20network%20structure.%20This%20research%20work%20offers%20a%20novel%20approach%0Athat%20extends%20beyond%20the%20conventional%20Generalized%20Additive%20Model%20framework%20by%0Aconsidering%20models%20like%20Graph%20Convolutional%20Networks%20or%20Graph%20SAGE.%20These%0Agraph-based%20models%20enable%20the%20incorporation%20of%20various%20levels%20of%0Ainterconnectedness%20and%20information%20sharing%20among%20nodes%2C%20where%20each%20node%0Acorresponds%20to%20the%20combined%20load%20%28i.e.%20consumption%29%20of%20a%20subset%20of%20consumers%0A%28e.g.%20the%20regions%20of%20a%20country%29.%20More%20specifically%2C%20we%20introduce%20a%20range%20of%0Amethods%20for%20inferring%20graphs%20tailored%20to%20consumption%20forecasting%2C%20along%20with%20a%0Aframework%20for%20evaluating%20the%20developed%20models%20in%20terms%20of%20both%20performance%20and%0Aexplainability.%20We%20conduct%20experiments%20on%20electricity%20forecasting%2C%20in%20both%20a%0Asynthetic%20and%20a%20real%20framework%20considering%20the%20French%20mainland%20regions%2C%20and%20the%0Aperformance%20and%20merits%20of%20our%20approach%20are%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17366v1&entry.124074799=Read"},
{"title": "Image-Perfect Imperfections: Safety, Bias, and Authenticity in the\n  Shadow of Text-To-Image Model Evolution", "author": "Yixin Wu and Yun Shen and Michael Backes and Yang Zhang", "abstract": "  Text-to-image models, such as Stable Diffusion (SD), undergo iterative\nupdates to improve image quality and address concerns such as safety.\nImprovements in image quality are straightforward to assess. However, how model\nupdates resolve existing concerns and whether they raise new questions remain\nunexplored. This study takes an initial step in investigating the evolution of\ntext-to-image models from the perspectives of safety, bias, and authenticity.\nOur findings, centered on Stable Diffusion, indicate that model updates paint a\nmixed picture. While updates progressively reduce the generation of unsafe\nimages, the bias issue, particularly in gender, intensifies. We also find that\nnegative stereotypes either persist within the same Non-White race group or\nshift towards other Non-White race groups through SD updates, yet with minimal\nassociation of these traits with the White race group. Additionally, our\nevaluation reveals a new concern stemming from SD updates: State-of-the-art\nfake image detectors, initially trained for earlier SD versions, struggle to\nidentify fake images generated by updated versions. We show that fine-tuning\nthese detectors on fake images generated by updated versions achieves at least\n96.6\\% accuracy across various SD versions, addressing this issue. Our insights\nhighlight the importance of continued efforts to mitigate biases and\nvulnerabilities in evolving text-to-image models.\n", "link": "http://arxiv.org/abs/2408.17285v1", "date": "2024-08-30", "relevancy": 1.7321, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5902}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5761}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image-Perfect%20Imperfections%3A%20Safety%2C%20Bias%2C%20and%20Authenticity%20in%20the%0A%20%20Shadow%20of%20Text-To-Image%20Model%20Evolution&body=Title%3A%20Image-Perfect%20Imperfections%3A%20Safety%2C%20Bias%2C%20and%20Authenticity%20in%20the%0A%20%20Shadow%20of%20Text-To-Image%20Model%20Evolution%0AAuthor%3A%20Yixin%20Wu%20and%20Yun%20Shen%20and%20Michael%20Backes%20and%20Yang%20Zhang%0AAbstract%3A%20%20%20Text-to-image%20models%2C%20such%20as%20Stable%20Diffusion%20%28SD%29%2C%20undergo%20iterative%0Aupdates%20to%20improve%20image%20quality%20and%20address%20concerns%20such%20as%20safety.%0AImprovements%20in%20image%20quality%20are%20straightforward%20to%20assess.%20However%2C%20how%20model%0Aupdates%20resolve%20existing%20concerns%20and%20whether%20they%20raise%20new%20questions%20remain%0Aunexplored.%20This%20study%20takes%20an%20initial%20step%20in%20investigating%20the%20evolution%20of%0Atext-to-image%20models%20from%20the%20perspectives%20of%20safety%2C%20bias%2C%20and%20authenticity.%0AOur%20findings%2C%20centered%20on%20Stable%20Diffusion%2C%20indicate%20that%20model%20updates%20paint%20a%0Amixed%20picture.%20While%20updates%20progressively%20reduce%20the%20generation%20of%20unsafe%0Aimages%2C%20the%20bias%20issue%2C%20particularly%20in%20gender%2C%20intensifies.%20We%20also%20find%20that%0Anegative%20stereotypes%20either%20persist%20within%20the%20same%20Non-White%20race%20group%20or%0Ashift%20towards%20other%20Non-White%20race%20groups%20through%20SD%20updates%2C%20yet%20with%20minimal%0Aassociation%20of%20these%20traits%20with%20the%20White%20race%20group.%20Additionally%2C%20our%0Aevaluation%20reveals%20a%20new%20concern%20stemming%20from%20SD%20updates%3A%20State-of-the-art%0Afake%20image%20detectors%2C%20initially%20trained%20for%20earlier%20SD%20versions%2C%20struggle%20to%0Aidentify%20fake%20images%20generated%20by%20updated%20versions.%20We%20show%20that%20fine-tuning%0Athese%20detectors%20on%20fake%20images%20generated%20by%20updated%20versions%20achieves%20at%20least%0A96.6%5C%25%20accuracy%20across%20various%20SD%20versions%2C%20addressing%20this%20issue.%20Our%20insights%0Ahighlight%20the%20importance%20of%20continued%20efforts%20to%20mitigate%20biases%20and%0Avulnerabilities%20in%20evolving%20text-to-image%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage-Perfect%2520Imperfections%253A%2520Safety%252C%2520Bias%252C%2520and%2520Authenticity%2520in%2520the%250A%2520%2520Shadow%2520of%2520Text-To-Image%2520Model%2520Evolution%26entry.906535625%3DYixin%2520Wu%2520and%2520Yun%2520Shen%2520and%2520Michael%2520Backes%2520and%2520Yang%2520Zhang%26entry.1292438233%3D%2520%2520Text-to-image%2520models%252C%2520such%2520as%2520Stable%2520Diffusion%2520%2528SD%2529%252C%2520undergo%2520iterative%250Aupdates%2520to%2520improve%2520image%2520quality%2520and%2520address%2520concerns%2520such%2520as%2520safety.%250AImprovements%2520in%2520image%2520quality%2520are%2520straightforward%2520to%2520assess.%2520However%252C%2520how%2520model%250Aupdates%2520resolve%2520existing%2520concerns%2520and%2520whether%2520they%2520raise%2520new%2520questions%2520remain%250Aunexplored.%2520This%2520study%2520takes%2520an%2520initial%2520step%2520in%2520investigating%2520the%2520evolution%2520of%250Atext-to-image%2520models%2520from%2520the%2520perspectives%2520of%2520safety%252C%2520bias%252C%2520and%2520authenticity.%250AOur%2520findings%252C%2520centered%2520on%2520Stable%2520Diffusion%252C%2520indicate%2520that%2520model%2520updates%2520paint%2520a%250Amixed%2520picture.%2520While%2520updates%2520progressively%2520reduce%2520the%2520generation%2520of%2520unsafe%250Aimages%252C%2520the%2520bias%2520issue%252C%2520particularly%2520in%2520gender%252C%2520intensifies.%2520We%2520also%2520find%2520that%250Anegative%2520stereotypes%2520either%2520persist%2520within%2520the%2520same%2520Non-White%2520race%2520group%2520or%250Ashift%2520towards%2520other%2520Non-White%2520race%2520groups%2520through%2520SD%2520updates%252C%2520yet%2520with%2520minimal%250Aassociation%2520of%2520these%2520traits%2520with%2520the%2520White%2520race%2520group.%2520Additionally%252C%2520our%250Aevaluation%2520reveals%2520a%2520new%2520concern%2520stemming%2520from%2520SD%2520updates%253A%2520State-of-the-art%250Afake%2520image%2520detectors%252C%2520initially%2520trained%2520for%2520earlier%2520SD%2520versions%252C%2520struggle%2520to%250Aidentify%2520fake%2520images%2520generated%2520by%2520updated%2520versions.%2520We%2520show%2520that%2520fine-tuning%250Athese%2520detectors%2520on%2520fake%2520images%2520generated%2520by%2520updated%2520versions%2520achieves%2520at%2520least%250A96.6%255C%2525%2520accuracy%2520across%2520various%2520SD%2520versions%252C%2520addressing%2520this%2520issue.%2520Our%2520insights%250Ahighlight%2520the%2520importance%2520of%2520continued%2520efforts%2520to%2520mitigate%2520biases%2520and%250Avulnerabilities%2520in%2520evolving%2520text-to-image%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image-Perfect%20Imperfections%3A%20Safety%2C%20Bias%2C%20and%20Authenticity%20in%20the%0A%20%20Shadow%20of%20Text-To-Image%20Model%20Evolution&entry.906535625=Yixin%20Wu%20and%20Yun%20Shen%20and%20Michael%20Backes%20and%20Yang%20Zhang&entry.1292438233=%20%20Text-to-image%20models%2C%20such%20as%20Stable%20Diffusion%20%28SD%29%2C%20undergo%20iterative%0Aupdates%20to%20improve%20image%20quality%20and%20address%20concerns%20such%20as%20safety.%0AImprovements%20in%20image%20quality%20are%20straightforward%20to%20assess.%20However%2C%20how%20model%0Aupdates%20resolve%20existing%20concerns%20and%20whether%20they%20raise%20new%20questions%20remain%0Aunexplored.%20This%20study%20takes%20an%20initial%20step%20in%20investigating%20the%20evolution%20of%0Atext-to-image%20models%20from%20the%20perspectives%20of%20safety%2C%20bias%2C%20and%20authenticity.%0AOur%20findings%2C%20centered%20on%20Stable%20Diffusion%2C%20indicate%20that%20model%20updates%20paint%20a%0Amixed%20picture.%20While%20updates%20progressively%20reduce%20the%20generation%20of%20unsafe%0Aimages%2C%20the%20bias%20issue%2C%20particularly%20in%20gender%2C%20intensifies.%20We%20also%20find%20that%0Anegative%20stereotypes%20either%20persist%20within%20the%20same%20Non-White%20race%20group%20or%0Ashift%20towards%20other%20Non-White%20race%20groups%20through%20SD%20updates%2C%20yet%20with%20minimal%0Aassociation%20of%20these%20traits%20with%20the%20White%20race%20group.%20Additionally%2C%20our%0Aevaluation%20reveals%20a%20new%20concern%20stemming%20from%20SD%20updates%3A%20State-of-the-art%0Afake%20image%20detectors%2C%20initially%20trained%20for%20earlier%20SD%20versions%2C%20struggle%20to%0Aidentify%20fake%20images%20generated%20by%20updated%20versions.%20We%20show%20that%20fine-tuning%0Athese%20detectors%20on%20fake%20images%20generated%20by%20updated%20versions%20achieves%20at%20least%0A96.6%5C%25%20accuracy%20across%20various%20SD%20versions%2C%20addressing%20this%20issue.%20Our%20insights%0Ahighlight%20the%20importance%20of%20continued%20efforts%20to%20mitigate%20biases%20and%0Avulnerabilities%20in%20evolving%20text-to-image%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17285v1&entry.124074799=Read"},
{"title": "Fair Best Arm Identification with Fixed Confidence", "author": "Alessio Russo and Filippo Vannella", "abstract": "  In this work, we present a novel framework for Best Arm Identification (BAI)\nunder fairness constraints, a setting that we refer to as \\textit{F-BAI} (fair\nBAI). Unlike traditional BAI, which solely focuses on identifying the optimal\narm with minimal sample complexity, F-BAI also includes a set of fairness\nconstraints. These constraints impose a lower limit on the selection rate of\neach arm and can be either model-agnostic or model-dependent. For this setting,\nwe establish an instance-specific sample complexity lower bound and analyze the\n\\textit{price of fairness}, quantifying how fairness impacts sample complexity.\nBased on the sample complexity lower bound, we propose F-TaS, an algorithm\nprovably matching the sample complexity lower bound, while ensuring that the\nfairness constraints are satisfied. Numerical results, conducted using both a\nsynthetic model and a practical wireless scheduling application, show the\nefficiency of F-TaS in minimizing the sample complexity while achieving low\nfairness violations.\n", "link": "http://arxiv.org/abs/2408.17313v1", "date": "2024-08-30", "relevancy": 1.7231, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.454}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4499}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Best%20Arm%20Identification%20with%20Fixed%20Confidence&body=Title%3A%20Fair%20Best%20Arm%20Identification%20with%20Fixed%20Confidence%0AAuthor%3A%20Alessio%20Russo%20and%20Filippo%20Vannella%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20novel%20framework%20for%20Best%20Arm%20Identification%20%28BAI%29%0Aunder%20fairness%20constraints%2C%20a%20setting%20that%20we%20refer%20to%20as%20%5Ctextit%7BF-BAI%7D%20%28fair%0ABAI%29.%20Unlike%20traditional%20BAI%2C%20which%20solely%20focuses%20on%20identifying%20the%20optimal%0Aarm%20with%20minimal%20sample%20complexity%2C%20F-BAI%20also%20includes%20a%20set%20of%20fairness%0Aconstraints.%20These%20constraints%20impose%20a%20lower%20limit%20on%20the%20selection%20rate%20of%0Aeach%20arm%20and%20can%20be%20either%20model-agnostic%20or%20model-dependent.%20For%20this%20setting%2C%0Awe%20establish%20an%20instance-specific%20sample%20complexity%20lower%20bound%20and%20analyze%20the%0A%5Ctextit%7Bprice%20of%20fairness%7D%2C%20quantifying%20how%20fairness%20impacts%20sample%20complexity.%0ABased%20on%20the%20sample%20complexity%20lower%20bound%2C%20we%20propose%20F-TaS%2C%20an%20algorithm%0Aprovably%20matching%20the%20sample%20complexity%20lower%20bound%2C%20while%20ensuring%20that%20the%0Afairness%20constraints%20are%20satisfied.%20Numerical%20results%2C%20conducted%20using%20both%20a%0Asynthetic%20model%20and%20a%20practical%20wireless%20scheduling%20application%2C%20show%20the%0Aefficiency%20of%20F-TaS%20in%20minimizing%20the%20sample%20complexity%20while%20achieving%20low%0Afairness%20violations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Best%2520Arm%2520Identification%2520with%2520Fixed%2520Confidence%26entry.906535625%3DAlessio%2520Russo%2520and%2520Filippo%2520Vannella%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520framework%2520for%2520Best%2520Arm%2520Identification%2520%2528BAI%2529%250Aunder%2520fairness%2520constraints%252C%2520a%2520setting%2520that%2520we%2520refer%2520to%2520as%2520%255Ctextit%257BF-BAI%257D%2520%2528fair%250ABAI%2529.%2520Unlike%2520traditional%2520BAI%252C%2520which%2520solely%2520focuses%2520on%2520identifying%2520the%2520optimal%250Aarm%2520with%2520minimal%2520sample%2520complexity%252C%2520F-BAI%2520also%2520includes%2520a%2520set%2520of%2520fairness%250Aconstraints.%2520These%2520constraints%2520impose%2520a%2520lower%2520limit%2520on%2520the%2520selection%2520rate%2520of%250Aeach%2520arm%2520and%2520can%2520be%2520either%2520model-agnostic%2520or%2520model-dependent.%2520For%2520this%2520setting%252C%250Awe%2520establish%2520an%2520instance-specific%2520sample%2520complexity%2520lower%2520bound%2520and%2520analyze%2520the%250A%255Ctextit%257Bprice%2520of%2520fairness%257D%252C%2520quantifying%2520how%2520fairness%2520impacts%2520sample%2520complexity.%250ABased%2520on%2520the%2520sample%2520complexity%2520lower%2520bound%252C%2520we%2520propose%2520F-TaS%252C%2520an%2520algorithm%250Aprovably%2520matching%2520the%2520sample%2520complexity%2520lower%2520bound%252C%2520while%2520ensuring%2520that%2520the%250Afairness%2520constraints%2520are%2520satisfied.%2520Numerical%2520results%252C%2520conducted%2520using%2520both%2520a%250Asynthetic%2520model%2520and%2520a%2520practical%2520wireless%2520scheduling%2520application%252C%2520show%2520the%250Aefficiency%2520of%2520F-TaS%2520in%2520minimizing%2520the%2520sample%2520complexity%2520while%2520achieving%2520low%250Afairness%2520violations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Best%20Arm%20Identification%20with%20Fixed%20Confidence&entry.906535625=Alessio%20Russo%20and%20Filippo%20Vannella&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20novel%20framework%20for%20Best%20Arm%20Identification%20%28BAI%29%0Aunder%20fairness%20constraints%2C%20a%20setting%20that%20we%20refer%20to%20as%20%5Ctextit%7BF-BAI%7D%20%28fair%0ABAI%29.%20Unlike%20traditional%20BAI%2C%20which%20solely%20focuses%20on%20identifying%20the%20optimal%0Aarm%20with%20minimal%20sample%20complexity%2C%20F-BAI%20also%20includes%20a%20set%20of%20fairness%0Aconstraints.%20These%20constraints%20impose%20a%20lower%20limit%20on%20the%20selection%20rate%20of%0Aeach%20arm%20and%20can%20be%20either%20model-agnostic%20or%20model-dependent.%20For%20this%20setting%2C%0Awe%20establish%20an%20instance-specific%20sample%20complexity%20lower%20bound%20and%20analyze%20the%0A%5Ctextit%7Bprice%20of%20fairness%7D%2C%20quantifying%20how%20fairness%20impacts%20sample%20complexity.%0ABased%20on%20the%20sample%20complexity%20lower%20bound%2C%20we%20propose%20F-TaS%2C%20an%20algorithm%0Aprovably%20matching%20the%20sample%20complexity%20lower%20bound%2C%20while%20ensuring%20that%20the%0Afairness%20constraints%20are%20satisfied.%20Numerical%20results%2C%20conducted%20using%20both%20a%0Asynthetic%20model%20and%20a%20practical%20wireless%20scheduling%20application%2C%20show%20the%0Aefficiency%20of%20F-TaS%20in%20minimizing%20the%20sample%20complexity%20while%20achieving%20low%0Afairness%20violations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17313v1&entry.124074799=Read"},
{"title": "Complexity of High-Dimensional Identity Testing with Coordinate\n  Conditional Sampling", "author": "Antonio Blanca and Zongchen Chen and Daniel \u0160tefankovi\u010d and Eric Vigoda", "abstract": "  We study the identity testing problem for high-dimensional distributions.\nGiven as input an explicit distribution $\\mu$, an $\\varepsilon>0$, and access\nto sampling oracle(s) for a hidden distribution $\\pi$, the goal in identity\ntesting is to distinguish whether the two distributions $\\mu$ and $\\pi$ are\nidentical or are at least $\\varepsilon$-far apart. When there is only access to\nfull samples from the hidden distribution $\\pi$, it is known that exponentially\nmany samples (in the dimension) may be needed for identity testing, and hence\nprevious works have studied identity testing with additional access to various\n\"conditional\" sampling oracles. We consider a significantly weaker conditional\nsampling oracle, which we call the $\\mathsf{Coordinate\\ Oracle}$, and provide a\ncomputational and statistical characterization of the identity testing problem\nin this new model.\n  We prove that if an analytic property known as approximate tensorization of\nentropy holds for an $n$-dimensional visible distribution $\\mu$, then there is\nan efficient identity testing algorithm for any hidden distribution $\\pi$ using\n$\\tilde{O}(n/\\varepsilon)$ queries to the $\\mathsf{Coordinate\\ Oracle}$.\nApproximate tensorization of entropy is a pertinent condition as recent works\nhave established it for a large class of high-dimensional distributions. We\nalso prove a computational phase transition: for a well-studied class of\n$n$-dimensional distributions, specifically sparse antiferromagnetic Ising\nmodels over $\\{+1,-1\\}^n$, we show that in the regime where approximate\ntensorization of entropy fails, there is no efficient identity testing\nalgorithm unless $\\mathsf{RP}=\\mathsf{NP}$. We complement our results with a\nmatching $\\Omega(n/\\varepsilon)$ statistical lower bound for the sample\ncomplexity of identity testing in the $\\mathsf{Coordinate\\ Oracle}$ model.\n", "link": "http://arxiv.org/abs/2207.09102v3", "date": "2024-08-30", "relevancy": 1.7152, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4486}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4162}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Complexity%20of%20High-Dimensional%20Identity%20Testing%20with%20Coordinate%0A%20%20Conditional%20Sampling&body=Title%3A%20Complexity%20of%20High-Dimensional%20Identity%20Testing%20with%20Coordinate%0A%20%20Conditional%20Sampling%0AAuthor%3A%20Antonio%20Blanca%20and%20Zongchen%20Chen%20and%20Daniel%20%C5%A0tefankovi%C4%8D%20and%20Eric%20Vigoda%0AAbstract%3A%20%20%20We%20study%20the%20identity%20testing%20problem%20for%20high-dimensional%20distributions.%0AGiven%20as%20input%20an%20explicit%20distribution%20%24%5Cmu%24%2C%20an%20%24%5Cvarepsilon%3E0%24%2C%20and%20access%0Ato%20sampling%20oracle%28s%29%20for%20a%20hidden%20distribution%20%24%5Cpi%24%2C%20the%20goal%20in%20identity%0Atesting%20is%20to%20distinguish%20whether%20the%20two%20distributions%20%24%5Cmu%24%20and%20%24%5Cpi%24%20are%0Aidentical%20or%20are%20at%20least%20%24%5Cvarepsilon%24-far%20apart.%20When%20there%20is%20only%20access%20to%0Afull%20samples%20from%20the%20hidden%20distribution%20%24%5Cpi%24%2C%20it%20is%20known%20that%20exponentially%0Amany%20samples%20%28in%20the%20dimension%29%20may%20be%20needed%20for%20identity%20testing%2C%20and%20hence%0Aprevious%20works%20have%20studied%20identity%20testing%20with%20additional%20access%20to%20various%0A%22conditional%22%20sampling%20oracles.%20We%20consider%20a%20significantly%20weaker%20conditional%0Asampling%20oracle%2C%20which%20we%20call%20the%20%24%5Cmathsf%7BCoordinate%5C%20Oracle%7D%24%2C%20and%20provide%20a%0Acomputational%20and%20statistical%20characterization%20of%20the%20identity%20testing%20problem%0Ain%20this%20new%20model.%0A%20%20We%20prove%20that%20if%20an%20analytic%20property%20known%20as%20approximate%20tensorization%20of%0Aentropy%20holds%20for%20an%20%24n%24-dimensional%20visible%20distribution%20%24%5Cmu%24%2C%20then%20there%20is%0Aan%20efficient%20identity%20testing%20algorithm%20for%20any%20hidden%20distribution%20%24%5Cpi%24%20using%0A%24%5Ctilde%7BO%7D%28n/%5Cvarepsilon%29%24%20queries%20to%20the%20%24%5Cmathsf%7BCoordinate%5C%20Oracle%7D%24.%0AApproximate%20tensorization%20of%20entropy%20is%20a%20pertinent%20condition%20as%20recent%20works%0Ahave%20established%20it%20for%20a%20large%20class%20of%20high-dimensional%20distributions.%20We%0Aalso%20prove%20a%20computational%20phase%20transition%3A%20for%20a%20well-studied%20class%20of%0A%24n%24-dimensional%20distributions%2C%20specifically%20sparse%20antiferromagnetic%20Ising%0Amodels%20over%20%24%5C%7B%2B1%2C-1%5C%7D%5En%24%2C%20we%20show%20that%20in%20the%20regime%20where%20approximate%0Atensorization%20of%20entropy%20fails%2C%20there%20is%20no%20efficient%20identity%20testing%0Aalgorithm%20unless%20%24%5Cmathsf%7BRP%7D%3D%5Cmathsf%7BNP%7D%24.%20We%20complement%20our%20results%20with%20a%0Amatching%20%24%5COmega%28n/%5Cvarepsilon%29%24%20statistical%20lower%20bound%20for%20the%20sample%0Acomplexity%20of%20identity%20testing%20in%20the%20%24%5Cmathsf%7BCoordinate%5C%20Oracle%7D%24%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.09102v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplexity%2520of%2520High-Dimensional%2520Identity%2520Testing%2520with%2520Coordinate%250A%2520%2520Conditional%2520Sampling%26entry.906535625%3DAntonio%2520Blanca%2520and%2520Zongchen%2520Chen%2520and%2520Daniel%2520%25C5%25A0tefankovi%25C4%258D%2520and%2520Eric%2520Vigoda%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520identity%2520testing%2520problem%2520for%2520high-dimensional%2520distributions.%250AGiven%2520as%2520input%2520an%2520explicit%2520distribution%2520%2524%255Cmu%2524%252C%2520an%2520%2524%255Cvarepsilon%253E0%2524%252C%2520and%2520access%250Ato%2520sampling%2520oracle%2528s%2529%2520for%2520a%2520hidden%2520distribution%2520%2524%255Cpi%2524%252C%2520the%2520goal%2520in%2520identity%250Atesting%2520is%2520to%2520distinguish%2520whether%2520the%2520two%2520distributions%2520%2524%255Cmu%2524%2520and%2520%2524%255Cpi%2524%2520are%250Aidentical%2520or%2520are%2520at%2520least%2520%2524%255Cvarepsilon%2524-far%2520apart.%2520When%2520there%2520is%2520only%2520access%2520to%250Afull%2520samples%2520from%2520the%2520hidden%2520distribution%2520%2524%255Cpi%2524%252C%2520it%2520is%2520known%2520that%2520exponentially%250Amany%2520samples%2520%2528in%2520the%2520dimension%2529%2520may%2520be%2520needed%2520for%2520identity%2520testing%252C%2520and%2520hence%250Aprevious%2520works%2520have%2520studied%2520identity%2520testing%2520with%2520additional%2520access%2520to%2520various%250A%2522conditional%2522%2520sampling%2520oracles.%2520We%2520consider%2520a%2520significantly%2520weaker%2520conditional%250Asampling%2520oracle%252C%2520which%2520we%2520call%2520the%2520%2524%255Cmathsf%257BCoordinate%255C%2520Oracle%257D%2524%252C%2520and%2520provide%2520a%250Acomputational%2520and%2520statistical%2520characterization%2520of%2520the%2520identity%2520testing%2520problem%250Ain%2520this%2520new%2520model.%250A%2520%2520We%2520prove%2520that%2520if%2520an%2520analytic%2520property%2520known%2520as%2520approximate%2520tensorization%2520of%250Aentropy%2520holds%2520for%2520an%2520%2524n%2524-dimensional%2520visible%2520distribution%2520%2524%255Cmu%2524%252C%2520then%2520there%2520is%250Aan%2520efficient%2520identity%2520testing%2520algorithm%2520for%2520any%2520hidden%2520distribution%2520%2524%255Cpi%2524%2520using%250A%2524%255Ctilde%257BO%257D%2528n/%255Cvarepsilon%2529%2524%2520queries%2520to%2520the%2520%2524%255Cmathsf%257BCoordinate%255C%2520Oracle%257D%2524.%250AApproximate%2520tensorization%2520of%2520entropy%2520is%2520a%2520pertinent%2520condition%2520as%2520recent%2520works%250Ahave%2520established%2520it%2520for%2520a%2520large%2520class%2520of%2520high-dimensional%2520distributions.%2520We%250Aalso%2520prove%2520a%2520computational%2520phase%2520transition%253A%2520for%2520a%2520well-studied%2520class%2520of%250A%2524n%2524-dimensional%2520distributions%252C%2520specifically%2520sparse%2520antiferromagnetic%2520Ising%250Amodels%2520over%2520%2524%255C%257B%252B1%252C-1%255C%257D%255En%2524%252C%2520we%2520show%2520that%2520in%2520the%2520regime%2520where%2520approximate%250Atensorization%2520of%2520entropy%2520fails%252C%2520there%2520is%2520no%2520efficient%2520identity%2520testing%250Aalgorithm%2520unless%2520%2524%255Cmathsf%257BRP%257D%253D%255Cmathsf%257BNP%257D%2524.%2520We%2520complement%2520our%2520results%2520with%2520a%250Amatching%2520%2524%255COmega%2528n/%255Cvarepsilon%2529%2524%2520statistical%2520lower%2520bound%2520for%2520the%2520sample%250Acomplexity%2520of%2520identity%2520testing%2520in%2520the%2520%2524%255Cmathsf%257BCoordinate%255C%2520Oracle%257D%2524%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.09102v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complexity%20of%20High-Dimensional%20Identity%20Testing%20with%20Coordinate%0A%20%20Conditional%20Sampling&entry.906535625=Antonio%20Blanca%20and%20Zongchen%20Chen%20and%20Daniel%20%C5%A0tefankovi%C4%8D%20and%20Eric%20Vigoda&entry.1292438233=%20%20We%20study%20the%20identity%20testing%20problem%20for%20high-dimensional%20distributions.%0AGiven%20as%20input%20an%20explicit%20distribution%20%24%5Cmu%24%2C%20an%20%24%5Cvarepsilon%3E0%24%2C%20and%20access%0Ato%20sampling%20oracle%28s%29%20for%20a%20hidden%20distribution%20%24%5Cpi%24%2C%20the%20goal%20in%20identity%0Atesting%20is%20to%20distinguish%20whether%20the%20two%20distributions%20%24%5Cmu%24%20and%20%24%5Cpi%24%20are%0Aidentical%20or%20are%20at%20least%20%24%5Cvarepsilon%24-far%20apart.%20When%20there%20is%20only%20access%20to%0Afull%20samples%20from%20the%20hidden%20distribution%20%24%5Cpi%24%2C%20it%20is%20known%20that%20exponentially%0Amany%20samples%20%28in%20the%20dimension%29%20may%20be%20needed%20for%20identity%20testing%2C%20and%20hence%0Aprevious%20works%20have%20studied%20identity%20testing%20with%20additional%20access%20to%20various%0A%22conditional%22%20sampling%20oracles.%20We%20consider%20a%20significantly%20weaker%20conditional%0Asampling%20oracle%2C%20which%20we%20call%20the%20%24%5Cmathsf%7BCoordinate%5C%20Oracle%7D%24%2C%20and%20provide%20a%0Acomputational%20and%20statistical%20characterization%20of%20the%20identity%20testing%20problem%0Ain%20this%20new%20model.%0A%20%20We%20prove%20that%20if%20an%20analytic%20property%20known%20as%20approximate%20tensorization%20of%0Aentropy%20holds%20for%20an%20%24n%24-dimensional%20visible%20distribution%20%24%5Cmu%24%2C%20then%20there%20is%0Aan%20efficient%20identity%20testing%20algorithm%20for%20any%20hidden%20distribution%20%24%5Cpi%24%20using%0A%24%5Ctilde%7BO%7D%28n/%5Cvarepsilon%29%24%20queries%20to%20the%20%24%5Cmathsf%7BCoordinate%5C%20Oracle%7D%24.%0AApproximate%20tensorization%20of%20entropy%20is%20a%20pertinent%20condition%20as%20recent%20works%0Ahave%20established%20it%20for%20a%20large%20class%20of%20high-dimensional%20distributions.%20We%0Aalso%20prove%20a%20computational%20phase%20transition%3A%20for%20a%20well-studied%20class%20of%0A%24n%24-dimensional%20distributions%2C%20specifically%20sparse%20antiferromagnetic%20Ising%0Amodels%20over%20%24%5C%7B%2B1%2C-1%5C%7D%5En%24%2C%20we%20show%20that%20in%20the%20regime%20where%20approximate%0Atensorization%20of%20entropy%20fails%2C%20there%20is%20no%20efficient%20identity%20testing%0Aalgorithm%20unless%20%24%5Cmathsf%7BRP%7D%3D%5Cmathsf%7BNP%7D%24.%20We%20complement%20our%20results%20with%20a%0Amatching%20%24%5COmega%28n/%5Cvarepsilon%29%24%20statistical%20lower%20bound%20for%20the%20sample%0Acomplexity%20of%20identity%20testing%20in%20the%20%24%5Cmathsf%7BCoordinate%5C%20Oracle%7D%24%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.09102v3&entry.124074799=Read"},
{"title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey", "author": "Sibo Yi and Yule Liu and Zhen Sun and Tianshuo Cong and Xinlei He and Jiaxing Song and Ke Xu and Qi Li", "abstract": "  Large Language Models (LLMs) have performed exceptionally in various\ntext-generative tasks, including question answering, translation, code\ncompletion, etc. However, the over-assistance of LLMs has raised the challenge\nof \"jailbreaking\", which induces the model to generate malicious responses\nagainst the usage policy and society by designing adversarial prompts. With the\nemergence of jailbreak attack methods exploiting different vulnerabilities in\nLLMs, the corresponding safety alignment measures are also evolving. In this\npaper, we propose a comprehensive and detailed taxonomy of jailbreak attack and\ndefense methods. For instance, the attack methods are divided into black-box\nand white-box attacks based on the transparency of the target model. Meanwhile,\nwe classify defense methods into prompt-level and model-level defenses.\nAdditionally, we further subdivide these attack and defense methods into\ndistinct sub-classes and present a coherent diagram illustrating their\nrelationships. We also conduct an investigation into the current evaluation\nmethods and compare them from different perspectives. Our findings aim to\ninspire future research and practical implementations in safeguarding LLMs\nagainst adversarial attacks. Above all, although jailbreak remains a\nsignificant concern within the community, we believe that our work enhances the\nunderstanding of this domain and provides a foundation for developing more\nsecure LLMs.\n", "link": "http://arxiv.org/abs/2407.04295v2", "date": "2024-08-30", "relevancy": 1.7067, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4274}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4273}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jailbreak%20Attacks%20and%20Defenses%20Against%20Large%20Language%20Models%3A%20A%20Survey&body=Title%3A%20Jailbreak%20Attacks%20and%20Defenses%20Against%20Large%20Language%20Models%3A%20A%20Survey%0AAuthor%3A%20Sibo%20Yi%20and%20Yule%20Liu%20and%20Zhen%20Sun%20and%20Tianshuo%20Cong%20and%20Xinlei%20He%20and%20Jiaxing%20Song%20and%20Ke%20Xu%20and%20Qi%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20performed%20exceptionally%20in%20various%0Atext-generative%20tasks%2C%20including%20question%20answering%2C%20translation%2C%20code%0Acompletion%2C%20etc.%20However%2C%20the%20over-assistance%20of%20LLMs%20has%20raised%20the%20challenge%0Aof%20%22jailbreaking%22%2C%20which%20induces%20the%20model%20to%20generate%20malicious%20responses%0Aagainst%20the%20usage%20policy%20and%20society%20by%20designing%20adversarial%20prompts.%20With%20the%0Aemergence%20of%20jailbreak%20attack%20methods%20exploiting%20different%20vulnerabilities%20in%0ALLMs%2C%20the%20corresponding%20safety%20alignment%20measures%20are%20also%20evolving.%20In%20this%0Apaper%2C%20we%20propose%20a%20comprehensive%20and%20detailed%20taxonomy%20of%20jailbreak%20attack%20and%0Adefense%20methods.%20For%20instance%2C%20the%20attack%20methods%20are%20divided%20into%20black-box%0Aand%20white-box%20attacks%20based%20on%20the%20transparency%20of%20the%20target%20model.%20Meanwhile%2C%0Awe%20classify%20defense%20methods%20into%20prompt-level%20and%20model-level%20defenses.%0AAdditionally%2C%20we%20further%20subdivide%20these%20attack%20and%20defense%20methods%20into%0Adistinct%20sub-classes%20and%20present%20a%20coherent%20diagram%20illustrating%20their%0Arelationships.%20We%20also%20conduct%20an%20investigation%20into%20the%20current%20evaluation%0Amethods%20and%20compare%20them%20from%20different%20perspectives.%20Our%20findings%20aim%20to%0Ainspire%20future%20research%20and%20practical%20implementations%20in%20safeguarding%20LLMs%0Aagainst%20adversarial%20attacks.%20Above%20all%2C%20although%20jailbreak%20remains%20a%0Asignificant%20concern%20within%20the%20community%2C%20we%20believe%20that%20our%20work%20enhances%20the%0Aunderstanding%20of%20this%20domain%20and%20provides%20a%20foundation%20for%20developing%20more%0Asecure%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04295v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJailbreak%2520Attacks%2520and%2520Defenses%2520Against%2520Large%2520Language%2520Models%253A%2520A%2520Survey%26entry.906535625%3DSibo%2520Yi%2520and%2520Yule%2520Liu%2520and%2520Zhen%2520Sun%2520and%2520Tianshuo%2520Cong%2520and%2520Xinlei%2520He%2520and%2520Jiaxing%2520Song%2520and%2520Ke%2520Xu%2520and%2520Qi%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520performed%2520exceptionally%2520in%2520various%250Atext-generative%2520tasks%252C%2520including%2520question%2520answering%252C%2520translation%252C%2520code%250Acompletion%252C%2520etc.%2520However%252C%2520the%2520over-assistance%2520of%2520LLMs%2520has%2520raised%2520the%2520challenge%250Aof%2520%2522jailbreaking%2522%252C%2520which%2520induces%2520the%2520model%2520to%2520generate%2520malicious%2520responses%250Aagainst%2520the%2520usage%2520policy%2520and%2520society%2520by%2520designing%2520adversarial%2520prompts.%2520With%2520the%250Aemergence%2520of%2520jailbreak%2520attack%2520methods%2520exploiting%2520different%2520vulnerabilities%2520in%250ALLMs%252C%2520the%2520corresponding%2520safety%2520alignment%2520measures%2520are%2520also%2520evolving.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520comprehensive%2520and%2520detailed%2520taxonomy%2520of%2520jailbreak%2520attack%2520and%250Adefense%2520methods.%2520For%2520instance%252C%2520the%2520attack%2520methods%2520are%2520divided%2520into%2520black-box%250Aand%2520white-box%2520attacks%2520based%2520on%2520the%2520transparency%2520of%2520the%2520target%2520model.%2520Meanwhile%252C%250Awe%2520classify%2520defense%2520methods%2520into%2520prompt-level%2520and%2520model-level%2520defenses.%250AAdditionally%252C%2520we%2520further%2520subdivide%2520these%2520attack%2520and%2520defense%2520methods%2520into%250Adistinct%2520sub-classes%2520and%2520present%2520a%2520coherent%2520diagram%2520illustrating%2520their%250Arelationships.%2520We%2520also%2520conduct%2520an%2520investigation%2520into%2520the%2520current%2520evaluation%250Amethods%2520and%2520compare%2520them%2520from%2520different%2520perspectives.%2520Our%2520findings%2520aim%2520to%250Ainspire%2520future%2520research%2520and%2520practical%2520implementations%2520in%2520safeguarding%2520LLMs%250Aagainst%2520adversarial%2520attacks.%2520Above%2520all%252C%2520although%2520jailbreak%2520remains%2520a%250Asignificant%2520concern%2520within%2520the%2520community%252C%2520we%2520believe%2520that%2520our%2520work%2520enhances%2520the%250Aunderstanding%2520of%2520this%2520domain%2520and%2520provides%2520a%2520foundation%2520for%2520developing%2520more%250Asecure%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04295v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jailbreak%20Attacks%20and%20Defenses%20Against%20Large%20Language%20Models%3A%20A%20Survey&entry.906535625=Sibo%20Yi%20and%20Yule%20Liu%20and%20Zhen%20Sun%20and%20Tianshuo%20Cong%20and%20Xinlei%20He%20and%20Jiaxing%20Song%20and%20Ke%20Xu%20and%20Qi%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20performed%20exceptionally%20in%20various%0Atext-generative%20tasks%2C%20including%20question%20answering%2C%20translation%2C%20code%0Acompletion%2C%20etc.%20However%2C%20the%20over-assistance%20of%20LLMs%20has%20raised%20the%20challenge%0Aof%20%22jailbreaking%22%2C%20which%20induces%20the%20model%20to%20generate%20malicious%20responses%0Aagainst%20the%20usage%20policy%20and%20society%20by%20designing%20adversarial%20prompts.%20With%20the%0Aemergence%20of%20jailbreak%20attack%20methods%20exploiting%20different%20vulnerabilities%20in%0ALLMs%2C%20the%20corresponding%20safety%20alignment%20measures%20are%20also%20evolving.%20In%20this%0Apaper%2C%20we%20propose%20a%20comprehensive%20and%20detailed%20taxonomy%20of%20jailbreak%20attack%20and%0Adefense%20methods.%20For%20instance%2C%20the%20attack%20methods%20are%20divided%20into%20black-box%0Aand%20white-box%20attacks%20based%20on%20the%20transparency%20of%20the%20target%20model.%20Meanwhile%2C%0Awe%20classify%20defense%20methods%20into%20prompt-level%20and%20model-level%20defenses.%0AAdditionally%2C%20we%20further%20subdivide%20these%20attack%20and%20defense%20methods%20into%0Adistinct%20sub-classes%20and%20present%20a%20coherent%20diagram%20illustrating%20their%0Arelationships.%20We%20also%20conduct%20an%20investigation%20into%20the%20current%20evaluation%0Amethods%20and%20compare%20them%20from%20different%20perspectives.%20Our%20findings%20aim%20to%0Ainspire%20future%20research%20and%20practical%20implementations%20in%20safeguarding%20LLMs%0Aagainst%20adversarial%20attacks.%20Above%20all%2C%20although%20jailbreak%20remains%20a%0Asignificant%20concern%20within%20the%20community%2C%20we%20believe%20that%20our%20work%20enhances%20the%0Aunderstanding%20of%20this%20domain%20and%20provides%20a%20foundation%20for%20developing%20more%0Asecure%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04295v2&entry.124074799=Read"},
{"title": "Foundational Models for Pathology and Endoscopy Images: Application for\n  Gastric Inflammation", "author": "Hamideh Kerdegari and Kyle Higgins and Dennis Veselkov and Ivan Laponogov and Inese Polaka and Miguel Coimbra and Junior Andrea Pescino and Marcis Leja and Mario Dinis-Ribeiro and Tania Fleitas Kanonnikoff and Kirill Veselkov", "abstract": "  The integration of artificial intelligence (AI) in medical diagnostics\nrepresents a significant advancement in managing upper gastrointestinal (GI)\ncancer, a major cause of global cancer mortality. Specifically for gastric\ncancer (GC), chronic inflammation causes changes in the mucosa such as atrophy,\nintestinal metaplasia (IM), dysplasia and ultimately cancer. Early detection\nthrough endoscopic regular surveillance is essential for better outcomes.\nFoundation models (FM), which are machine or deep learning models trained on\ndiverse data and applicable to broad use cases, offer a promising solution to\nenhance the accuracy of endoscopy and its subsequent pathology image analysis.\nThis review explores the recent advancements, applications, and challenges\nassociated with FM in endoscopy and pathology imaging. We started by\nelucidating the core principles and architectures underlying these models,\nincluding their training methodologies and the pivotal role of large-scale data\nin developing their predictive capabilities. Moreover, this work discusses\nemerging trends and future research directions, emphasizing the integration of\nmultimodal data, the development of more robust and equitable models, and the\npotential for real-time diagnostic support. This review aims to provide a\nroadmap for researchers and practitioners in navigating the complexities of\nincorporating FM into clinical practice for prevention/management of GC cases,\nthereby improving patient outcomes.\n", "link": "http://arxiv.org/abs/2406.18249v2", "date": "2024-08-30", "relevancy": 1.6978, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4281}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4262}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundational%20Models%20for%20Pathology%20and%20Endoscopy%20Images%3A%20Application%20for%0A%20%20Gastric%20Inflammation&body=Title%3A%20Foundational%20Models%20for%20Pathology%20and%20Endoscopy%20Images%3A%20Application%20for%0A%20%20Gastric%20Inflammation%0AAuthor%3A%20Hamideh%20Kerdegari%20and%20Kyle%20Higgins%20and%20Dennis%20Veselkov%20and%20Ivan%20Laponogov%20and%20Inese%20Polaka%20and%20Miguel%20Coimbra%20and%20Junior%20Andrea%20Pescino%20and%20Marcis%20Leja%20and%20Mario%20Dinis-Ribeiro%20and%20Tania%20Fleitas%20Kanonnikoff%20and%20Kirill%20Veselkov%0AAbstract%3A%20%20%20The%20integration%20of%20artificial%20intelligence%20%28AI%29%20in%20medical%20diagnostics%0Arepresents%20a%20significant%20advancement%20in%20managing%20upper%20gastrointestinal%20%28GI%29%0Acancer%2C%20a%20major%20cause%20of%20global%20cancer%20mortality.%20Specifically%20for%20gastric%0Acancer%20%28GC%29%2C%20chronic%20inflammation%20causes%20changes%20in%20the%20mucosa%20such%20as%20atrophy%2C%0Aintestinal%20metaplasia%20%28IM%29%2C%20dysplasia%20and%20ultimately%20cancer.%20Early%20detection%0Athrough%20endoscopic%20regular%20surveillance%20is%20essential%20for%20better%20outcomes.%0AFoundation%20models%20%28FM%29%2C%20which%20are%20machine%20or%20deep%20learning%20models%20trained%20on%0Adiverse%20data%20and%20applicable%20to%20broad%20use%20cases%2C%20offer%20a%20promising%20solution%20to%0Aenhance%20the%20accuracy%20of%20endoscopy%20and%20its%20subsequent%20pathology%20image%20analysis.%0AThis%20review%20explores%20the%20recent%20advancements%2C%20applications%2C%20and%20challenges%0Aassociated%20with%20FM%20in%20endoscopy%20and%20pathology%20imaging.%20We%20started%20by%0Aelucidating%20the%20core%20principles%20and%20architectures%20underlying%20these%20models%2C%0Aincluding%20their%20training%20methodologies%20and%20the%20pivotal%20role%20of%20large-scale%20data%0Ain%20developing%20their%20predictive%20capabilities.%20Moreover%2C%20this%20work%20discusses%0Aemerging%20trends%20and%20future%20research%20directions%2C%20emphasizing%20the%20integration%20of%0Amultimodal%20data%2C%20the%20development%20of%20more%20robust%20and%20equitable%20models%2C%20and%20the%0Apotential%20for%20real-time%20diagnostic%20support.%20This%20review%20aims%20to%20provide%20a%0Aroadmap%20for%20researchers%20and%20practitioners%20in%20navigating%20the%20complexities%20of%0Aincorporating%20FM%20into%20clinical%20practice%20for%20prevention/management%20of%20GC%20cases%2C%0Athereby%20improving%20patient%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18249v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundational%2520Models%2520for%2520Pathology%2520and%2520Endoscopy%2520Images%253A%2520Application%2520for%250A%2520%2520Gastric%2520Inflammation%26entry.906535625%3DHamideh%2520Kerdegari%2520and%2520Kyle%2520Higgins%2520and%2520Dennis%2520Veselkov%2520and%2520Ivan%2520Laponogov%2520and%2520Inese%2520Polaka%2520and%2520Miguel%2520Coimbra%2520and%2520Junior%2520Andrea%2520Pescino%2520and%2520Marcis%2520Leja%2520and%2520Mario%2520Dinis-Ribeiro%2520and%2520Tania%2520Fleitas%2520Kanonnikoff%2520and%2520Kirill%2520Veselkov%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520in%2520medical%2520diagnostics%250Arepresents%2520a%2520significant%2520advancement%2520in%2520managing%2520upper%2520gastrointestinal%2520%2528GI%2529%250Acancer%252C%2520a%2520major%2520cause%2520of%2520global%2520cancer%2520mortality.%2520Specifically%2520for%2520gastric%250Acancer%2520%2528GC%2529%252C%2520chronic%2520inflammation%2520causes%2520changes%2520in%2520the%2520mucosa%2520such%2520as%2520atrophy%252C%250Aintestinal%2520metaplasia%2520%2528IM%2529%252C%2520dysplasia%2520and%2520ultimately%2520cancer.%2520Early%2520detection%250Athrough%2520endoscopic%2520regular%2520surveillance%2520is%2520essential%2520for%2520better%2520outcomes.%250AFoundation%2520models%2520%2528FM%2529%252C%2520which%2520are%2520machine%2520or%2520deep%2520learning%2520models%2520trained%2520on%250Adiverse%2520data%2520and%2520applicable%2520to%2520broad%2520use%2520cases%252C%2520offer%2520a%2520promising%2520solution%2520to%250Aenhance%2520the%2520accuracy%2520of%2520endoscopy%2520and%2520its%2520subsequent%2520pathology%2520image%2520analysis.%250AThis%2520review%2520explores%2520the%2520recent%2520advancements%252C%2520applications%252C%2520and%2520challenges%250Aassociated%2520with%2520FM%2520in%2520endoscopy%2520and%2520pathology%2520imaging.%2520We%2520started%2520by%250Aelucidating%2520the%2520core%2520principles%2520and%2520architectures%2520underlying%2520these%2520models%252C%250Aincluding%2520their%2520training%2520methodologies%2520and%2520the%2520pivotal%2520role%2520of%2520large-scale%2520data%250Ain%2520developing%2520their%2520predictive%2520capabilities.%2520Moreover%252C%2520this%2520work%2520discusses%250Aemerging%2520trends%2520and%2520future%2520research%2520directions%252C%2520emphasizing%2520the%2520integration%2520of%250Amultimodal%2520data%252C%2520the%2520development%2520of%2520more%2520robust%2520and%2520equitable%2520models%252C%2520and%2520the%250Apotential%2520for%2520real-time%2520diagnostic%2520support.%2520This%2520review%2520aims%2520to%2520provide%2520a%250Aroadmap%2520for%2520researchers%2520and%2520practitioners%2520in%2520navigating%2520the%2520complexities%2520of%250Aincorporating%2520FM%2520into%2520clinical%2520practice%2520for%2520prevention/management%2520of%2520GC%2520cases%252C%250Athereby%2520improving%2520patient%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18249v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundational%20Models%20for%20Pathology%20and%20Endoscopy%20Images%3A%20Application%20for%0A%20%20Gastric%20Inflammation&entry.906535625=Hamideh%20Kerdegari%20and%20Kyle%20Higgins%20and%20Dennis%20Veselkov%20and%20Ivan%20Laponogov%20and%20Inese%20Polaka%20and%20Miguel%20Coimbra%20and%20Junior%20Andrea%20Pescino%20and%20Marcis%20Leja%20and%20Mario%20Dinis-Ribeiro%20and%20Tania%20Fleitas%20Kanonnikoff%20and%20Kirill%20Veselkov&entry.1292438233=%20%20The%20integration%20of%20artificial%20intelligence%20%28AI%29%20in%20medical%20diagnostics%0Arepresents%20a%20significant%20advancement%20in%20managing%20upper%20gastrointestinal%20%28GI%29%0Acancer%2C%20a%20major%20cause%20of%20global%20cancer%20mortality.%20Specifically%20for%20gastric%0Acancer%20%28GC%29%2C%20chronic%20inflammation%20causes%20changes%20in%20the%20mucosa%20such%20as%20atrophy%2C%0Aintestinal%20metaplasia%20%28IM%29%2C%20dysplasia%20and%20ultimately%20cancer.%20Early%20detection%0Athrough%20endoscopic%20regular%20surveillance%20is%20essential%20for%20better%20outcomes.%0AFoundation%20models%20%28FM%29%2C%20which%20are%20machine%20or%20deep%20learning%20models%20trained%20on%0Adiverse%20data%20and%20applicable%20to%20broad%20use%20cases%2C%20offer%20a%20promising%20solution%20to%0Aenhance%20the%20accuracy%20of%20endoscopy%20and%20its%20subsequent%20pathology%20image%20analysis.%0AThis%20review%20explores%20the%20recent%20advancements%2C%20applications%2C%20and%20challenges%0Aassociated%20with%20FM%20in%20endoscopy%20and%20pathology%20imaging.%20We%20started%20by%0Aelucidating%20the%20core%20principles%20and%20architectures%20underlying%20these%20models%2C%0Aincluding%20their%20training%20methodologies%20and%20the%20pivotal%20role%20of%20large-scale%20data%0Ain%20developing%20their%20predictive%20capabilities.%20Moreover%2C%20this%20work%20discusses%0Aemerging%20trends%20and%20future%20research%20directions%2C%20emphasizing%20the%20integration%20of%0Amultimodal%20data%2C%20the%20development%20of%20more%20robust%20and%20equitable%20models%2C%20and%20the%0Apotential%20for%20real-time%20diagnostic%20support.%20This%20review%20aims%20to%20provide%20a%0Aroadmap%20for%20researchers%20and%20practitioners%20in%20navigating%20the%20complexities%20of%0Aincorporating%20FM%20into%20clinical%20practice%20for%20prevention/management%20of%20GC%20cases%2C%0Athereby%20improving%20patient%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18249v2&entry.124074799=Read"},
{"title": "Structuring a Training Strategy to Robustify Perception Models with\n  Realistic Image Augmentations", "author": "Ahmed Hammam and Bharathwaj Krishnaswami Sreedhar and Nura Kawa and Tim Patzelt and Oliver De Candido", "abstract": "  Advancing Machine Learning (ML)-based perception models for autonomous\nsystems necessitates addressing weak spots within the models, particularly in\nchallenging Operational Design Domains (ODDs). These are environmental\noperating conditions of an autonomous vehicle which can contain difficult\nconditions, e.g., lens flare at night or objects reflected in a wet street.\nThis report introduces a novel methodology for training with augmentations to\nenhance model robustness and performance in such conditions. The proposed\napproach leverages customized physics-based augmentation functions, to generate\nrealistic training data that simulates diverse ODD scenarios.\n  We present a comprehensive framework that includes identifying weak spots in\nML models, selecting suitable augmentations, and devising effective training\nstrategies. The methodology integrates hyperparameter optimization and latent\nspace optimization to fine-tune augmentation parameters, ensuring they\nmaximally improve the ML models' performance. Experimental results demonstrate\nimprovements in model performance, as measured by commonly used metrics such as\nmean Average Precision (mAP) and mean Intersection over Union (mIoU) on\nopen-source object detection and semantic segmentation models and datasets.\n  Our findings emphasize that optimal training strategies are model- and\ndata-specific and highlight the benefits of integrating augmentations into the\ntraining pipeline. By incorporating augmentations, we observe enhanced\nrobustness of ML-based perception models, making them more resilient to edge\ncases encountered in real-world ODDs. This work underlines the importance of\ncustomized augmentations and offers an effective solution for improving the\nsafety and reliability of autonomous driving functions.\n", "link": "http://arxiv.org/abs/2408.17311v1", "date": "2024-08-30", "relevancy": 1.6924, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5707}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5703}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structuring%20a%20Training%20Strategy%20to%20Robustify%20Perception%20Models%20with%0A%20%20Realistic%20Image%20Augmentations&body=Title%3A%20Structuring%20a%20Training%20Strategy%20to%20Robustify%20Perception%20Models%20with%0A%20%20Realistic%20Image%20Augmentations%0AAuthor%3A%20Ahmed%20Hammam%20and%20Bharathwaj%20Krishnaswami%20Sreedhar%20and%20Nura%20Kawa%20and%20Tim%20Patzelt%20and%20Oliver%20De%20Candido%0AAbstract%3A%20%20%20Advancing%20Machine%20Learning%20%28ML%29-based%20perception%20models%20for%20autonomous%0Asystems%20necessitates%20addressing%20weak%20spots%20within%20the%20models%2C%20particularly%20in%0Achallenging%20Operational%20Design%20Domains%20%28ODDs%29.%20These%20are%20environmental%0Aoperating%20conditions%20of%20an%20autonomous%20vehicle%20which%20can%20contain%20difficult%0Aconditions%2C%20e.g.%2C%20lens%20flare%20at%20night%20or%20objects%20reflected%20in%20a%20wet%20street.%0AThis%20report%20introduces%20a%20novel%20methodology%20for%20training%20with%20augmentations%20to%0Aenhance%20model%20robustness%20and%20performance%20in%20such%20conditions.%20The%20proposed%0Aapproach%20leverages%20customized%20physics-based%20augmentation%20functions%2C%20to%20generate%0Arealistic%20training%20data%20that%20simulates%20diverse%20ODD%20scenarios.%0A%20%20We%20present%20a%20comprehensive%20framework%20that%20includes%20identifying%20weak%20spots%20in%0AML%20models%2C%20selecting%20suitable%20augmentations%2C%20and%20devising%20effective%20training%0Astrategies.%20The%20methodology%20integrates%20hyperparameter%20optimization%20and%20latent%0Aspace%20optimization%20to%20fine-tune%20augmentation%20parameters%2C%20ensuring%20they%0Amaximally%20improve%20the%20ML%20models%27%20performance.%20Experimental%20results%20demonstrate%0Aimprovements%20in%20model%20performance%2C%20as%20measured%20by%20commonly%20used%20metrics%20such%20as%0Amean%20Average%20Precision%20%28mAP%29%20and%20mean%20Intersection%20over%20Union%20%28mIoU%29%20on%0Aopen-source%20object%20detection%20and%20semantic%20segmentation%20models%20and%20datasets.%0A%20%20Our%20findings%20emphasize%20that%20optimal%20training%20strategies%20are%20model-%20and%0Adata-specific%20and%20highlight%20the%20benefits%20of%20integrating%20augmentations%20into%20the%0Atraining%20pipeline.%20By%20incorporating%20augmentations%2C%20we%20observe%20enhanced%0Arobustness%20of%20ML-based%20perception%20models%2C%20making%20them%20more%20resilient%20to%20edge%0Acases%20encountered%20in%20real-world%20ODDs.%20This%20work%20underlines%20the%20importance%20of%0Acustomized%20augmentations%20and%20offers%20an%20effective%20solution%20for%20improving%20the%0Asafety%20and%20reliability%20of%20autonomous%20driving%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructuring%2520a%2520Training%2520Strategy%2520to%2520Robustify%2520Perception%2520Models%2520with%250A%2520%2520Realistic%2520Image%2520Augmentations%26entry.906535625%3DAhmed%2520Hammam%2520and%2520Bharathwaj%2520Krishnaswami%2520Sreedhar%2520and%2520Nura%2520Kawa%2520and%2520Tim%2520Patzelt%2520and%2520Oliver%2520De%2520Candido%26entry.1292438233%3D%2520%2520Advancing%2520Machine%2520Learning%2520%2528ML%2529-based%2520perception%2520models%2520for%2520autonomous%250Asystems%2520necessitates%2520addressing%2520weak%2520spots%2520within%2520the%2520models%252C%2520particularly%2520in%250Achallenging%2520Operational%2520Design%2520Domains%2520%2528ODDs%2529.%2520These%2520are%2520environmental%250Aoperating%2520conditions%2520of%2520an%2520autonomous%2520vehicle%2520which%2520can%2520contain%2520difficult%250Aconditions%252C%2520e.g.%252C%2520lens%2520flare%2520at%2520night%2520or%2520objects%2520reflected%2520in%2520a%2520wet%2520street.%250AThis%2520report%2520introduces%2520a%2520novel%2520methodology%2520for%2520training%2520with%2520augmentations%2520to%250Aenhance%2520model%2520robustness%2520and%2520performance%2520in%2520such%2520conditions.%2520The%2520proposed%250Aapproach%2520leverages%2520customized%2520physics-based%2520augmentation%2520functions%252C%2520to%2520generate%250Arealistic%2520training%2520data%2520that%2520simulates%2520diverse%2520ODD%2520scenarios.%250A%2520%2520We%2520present%2520a%2520comprehensive%2520framework%2520that%2520includes%2520identifying%2520weak%2520spots%2520in%250AML%2520models%252C%2520selecting%2520suitable%2520augmentations%252C%2520and%2520devising%2520effective%2520training%250Astrategies.%2520The%2520methodology%2520integrates%2520hyperparameter%2520optimization%2520and%2520latent%250Aspace%2520optimization%2520to%2520fine-tune%2520augmentation%2520parameters%252C%2520ensuring%2520they%250Amaximally%2520improve%2520the%2520ML%2520models%2527%2520performance.%2520Experimental%2520results%2520demonstrate%250Aimprovements%2520in%2520model%2520performance%252C%2520as%2520measured%2520by%2520commonly%2520used%2520metrics%2520such%2520as%250Amean%2520Average%2520Precision%2520%2528mAP%2529%2520and%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520on%250Aopen-source%2520object%2520detection%2520and%2520semantic%2520segmentation%2520models%2520and%2520datasets.%250A%2520%2520Our%2520findings%2520emphasize%2520that%2520optimal%2520training%2520strategies%2520are%2520model-%2520and%250Adata-specific%2520and%2520highlight%2520the%2520benefits%2520of%2520integrating%2520augmentations%2520into%2520the%250Atraining%2520pipeline.%2520By%2520incorporating%2520augmentations%252C%2520we%2520observe%2520enhanced%250Arobustness%2520of%2520ML-based%2520perception%2520models%252C%2520making%2520them%2520more%2520resilient%2520to%2520edge%250Acases%2520encountered%2520in%2520real-world%2520ODDs.%2520This%2520work%2520underlines%2520the%2520importance%2520of%250Acustomized%2520augmentations%2520and%2520offers%2520an%2520effective%2520solution%2520for%2520improving%2520the%250Asafety%2520and%2520reliability%2520of%2520autonomous%2520driving%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structuring%20a%20Training%20Strategy%20to%20Robustify%20Perception%20Models%20with%0A%20%20Realistic%20Image%20Augmentations&entry.906535625=Ahmed%20Hammam%20and%20Bharathwaj%20Krishnaswami%20Sreedhar%20and%20Nura%20Kawa%20and%20Tim%20Patzelt%20and%20Oliver%20De%20Candido&entry.1292438233=%20%20Advancing%20Machine%20Learning%20%28ML%29-based%20perception%20models%20for%20autonomous%0Asystems%20necessitates%20addressing%20weak%20spots%20within%20the%20models%2C%20particularly%20in%0Achallenging%20Operational%20Design%20Domains%20%28ODDs%29.%20These%20are%20environmental%0Aoperating%20conditions%20of%20an%20autonomous%20vehicle%20which%20can%20contain%20difficult%0Aconditions%2C%20e.g.%2C%20lens%20flare%20at%20night%20or%20objects%20reflected%20in%20a%20wet%20street.%0AThis%20report%20introduces%20a%20novel%20methodology%20for%20training%20with%20augmentations%20to%0Aenhance%20model%20robustness%20and%20performance%20in%20such%20conditions.%20The%20proposed%0Aapproach%20leverages%20customized%20physics-based%20augmentation%20functions%2C%20to%20generate%0Arealistic%20training%20data%20that%20simulates%20diverse%20ODD%20scenarios.%0A%20%20We%20present%20a%20comprehensive%20framework%20that%20includes%20identifying%20weak%20spots%20in%0AML%20models%2C%20selecting%20suitable%20augmentations%2C%20and%20devising%20effective%20training%0Astrategies.%20The%20methodology%20integrates%20hyperparameter%20optimization%20and%20latent%0Aspace%20optimization%20to%20fine-tune%20augmentation%20parameters%2C%20ensuring%20they%0Amaximally%20improve%20the%20ML%20models%27%20performance.%20Experimental%20results%20demonstrate%0Aimprovements%20in%20model%20performance%2C%20as%20measured%20by%20commonly%20used%20metrics%20such%20as%0Amean%20Average%20Precision%20%28mAP%29%20and%20mean%20Intersection%20over%20Union%20%28mIoU%29%20on%0Aopen-source%20object%20detection%20and%20semantic%20segmentation%20models%20and%20datasets.%0A%20%20Our%20findings%20emphasize%20that%20optimal%20training%20strategies%20are%20model-%20and%0Adata-specific%20and%20highlight%20the%20benefits%20of%20integrating%20augmentations%20into%20the%0Atraining%20pipeline.%20By%20incorporating%20augmentations%2C%20we%20observe%20enhanced%0Arobustness%20of%20ML-based%20perception%20models%2C%20making%20them%20more%20resilient%20to%20edge%0Acases%20encountered%20in%20real-world%20ODDs.%20This%20work%20underlines%20the%20importance%20of%0Acustomized%20augmentations%20and%20offers%20an%20effective%20solution%20for%20improving%20the%0Asafety%20and%20reliability%20of%20autonomous%20driving%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17311v1&entry.124074799=Read"},
{"title": "EMPOWER: Embodied Multi-role Open-vocabulary Planning with Online\n  Grounding and Execution", "author": "Francesco Argenziano and Michele Brienza and Vincenzo Suriani and Daniele Nardi and Domenico D. Bloisi", "abstract": "  Task planning for robots in real-life settings presents significant\nchallenges. These challenges stem from three primary issues: the difficulty in\nidentifying grounded sequences of steps to achieve a goal; the lack of a\nstandardized mapping between high-level actions and low-level commands; and the\nchallenge of maintaining low computational overhead given the limited resources\nof robotic hardware. We introduce EMPOWER, a framework designed for\nopen-vocabulary online grounding and planning for embodied agents aimed at\naddressing these issues. By leveraging efficient pre-trained foundation models\nand a multi-role mechanism, EMPOWER demonstrates notable improvements in\ngrounded planning and execution. Quantitative results highlight the\neffectiveness of our approach, achieving an average success rate of 0.73 across\nsix different real-life scenarios using a TIAGo robot.\n", "link": "http://arxiv.org/abs/2408.17379v1", "date": "2024-08-30", "relevancy": 1.6812, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6251}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5734}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMPOWER%3A%20Embodied%20Multi-role%20Open-vocabulary%20Planning%20with%20Online%0A%20%20Grounding%20and%20Execution&body=Title%3A%20EMPOWER%3A%20Embodied%20Multi-role%20Open-vocabulary%20Planning%20with%20Online%0A%20%20Grounding%20and%20Execution%0AAuthor%3A%20Francesco%20Argenziano%20and%20Michele%20Brienza%20and%20Vincenzo%20Suriani%20and%20Daniele%20Nardi%20and%20Domenico%20D.%20Bloisi%0AAbstract%3A%20%20%20Task%20planning%20for%20robots%20in%20real-life%20settings%20presents%20significant%0Achallenges.%20These%20challenges%20stem%20from%20three%20primary%20issues%3A%20the%20difficulty%20in%0Aidentifying%20grounded%20sequences%20of%20steps%20to%20achieve%20a%20goal%3B%20the%20lack%20of%20a%0Astandardized%20mapping%20between%20high-level%20actions%20and%20low-level%20commands%3B%20and%20the%0Achallenge%20of%20maintaining%20low%20computational%20overhead%20given%20the%20limited%20resources%0Aof%20robotic%20hardware.%20We%20introduce%20EMPOWER%2C%20a%20framework%20designed%20for%0Aopen-vocabulary%20online%20grounding%20and%20planning%20for%20embodied%20agents%20aimed%20at%0Aaddressing%20these%20issues.%20By%20leveraging%20efficient%20pre-trained%20foundation%20models%0Aand%20a%20multi-role%20mechanism%2C%20EMPOWER%20demonstrates%20notable%20improvements%20in%0Agrounded%20planning%20and%20execution.%20Quantitative%20results%20highlight%20the%0Aeffectiveness%20of%20our%20approach%2C%20achieving%20an%20average%20success%20rate%20of%200.73%20across%0Asix%20different%20real-life%20scenarios%20using%20a%20TIAGo%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMPOWER%253A%2520Embodied%2520Multi-role%2520Open-vocabulary%2520Planning%2520with%2520Online%250A%2520%2520Grounding%2520and%2520Execution%26entry.906535625%3DFrancesco%2520Argenziano%2520and%2520Michele%2520Brienza%2520and%2520Vincenzo%2520Suriani%2520and%2520Daniele%2520Nardi%2520and%2520Domenico%2520D.%2520Bloisi%26entry.1292438233%3D%2520%2520Task%2520planning%2520for%2520robots%2520in%2520real-life%2520settings%2520presents%2520significant%250Achallenges.%2520These%2520challenges%2520stem%2520from%2520three%2520primary%2520issues%253A%2520the%2520difficulty%2520in%250Aidentifying%2520grounded%2520sequences%2520of%2520steps%2520to%2520achieve%2520a%2520goal%253B%2520the%2520lack%2520of%2520a%250Astandardized%2520mapping%2520between%2520high-level%2520actions%2520and%2520low-level%2520commands%253B%2520and%2520the%250Achallenge%2520of%2520maintaining%2520low%2520computational%2520overhead%2520given%2520the%2520limited%2520resources%250Aof%2520robotic%2520hardware.%2520We%2520introduce%2520EMPOWER%252C%2520a%2520framework%2520designed%2520for%250Aopen-vocabulary%2520online%2520grounding%2520and%2520planning%2520for%2520embodied%2520agents%2520aimed%2520at%250Aaddressing%2520these%2520issues.%2520By%2520leveraging%2520efficient%2520pre-trained%2520foundation%2520models%250Aand%2520a%2520multi-role%2520mechanism%252C%2520EMPOWER%2520demonstrates%2520notable%2520improvements%2520in%250Agrounded%2520planning%2520and%2520execution.%2520Quantitative%2520results%2520highlight%2520the%250Aeffectiveness%2520of%2520our%2520approach%252C%2520achieving%2520an%2520average%2520success%2520rate%2520of%25200.73%2520across%250Asix%2520different%2520real-life%2520scenarios%2520using%2520a%2520TIAGo%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMPOWER%3A%20Embodied%20Multi-role%20Open-vocabulary%20Planning%20with%20Online%0A%20%20Grounding%20and%20Execution&entry.906535625=Francesco%20Argenziano%20and%20Michele%20Brienza%20and%20Vincenzo%20Suriani%20and%20Daniele%20Nardi%20and%20Domenico%20D.%20Bloisi&entry.1292438233=%20%20Task%20planning%20for%20robots%20in%20real-life%20settings%20presents%20significant%0Achallenges.%20These%20challenges%20stem%20from%20three%20primary%20issues%3A%20the%20difficulty%20in%0Aidentifying%20grounded%20sequences%20of%20steps%20to%20achieve%20a%20goal%3B%20the%20lack%20of%20a%0Astandardized%20mapping%20between%20high-level%20actions%20and%20low-level%20commands%3B%20and%20the%0Achallenge%20of%20maintaining%20low%20computational%20overhead%20given%20the%20limited%20resources%0Aof%20robotic%20hardware.%20We%20introduce%20EMPOWER%2C%20a%20framework%20designed%20for%0Aopen-vocabulary%20online%20grounding%20and%20planning%20for%20embodied%20agents%20aimed%20at%0Aaddressing%20these%20issues.%20By%20leveraging%20efficient%20pre-trained%20foundation%20models%0Aand%20a%20multi-role%20mechanism%2C%20EMPOWER%20demonstrates%20notable%20improvements%20in%0Agrounded%20planning%20and%20execution.%20Quantitative%20results%20highlight%20the%0Aeffectiveness%20of%20our%20approach%2C%20achieving%20an%20average%20success%20rate%20of%200.73%20across%0Asix%20different%20real-life%20scenarios%20using%20a%20TIAGo%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17379v1&entry.124074799=Read"},
{"title": "Generative AI Enables Medical Image Segmentation in Ultra Low-Data\n  Regimes", "author": "Li Zhang and Basu Jindal and Ahmed Alaa and Robert Weinreb and David Wilson and Eran Segal and James Zou and Pengtao Xie", "abstract": "  Semantic segmentation of medical images is pivotal in applications like\ndisease diagnosis and treatment planning. While deep learning has excelled in\nautomating this task, a major hurdle is the need for numerous annotated\nsegmentation masks, which are resource-intensive to produce due to the required\nexpertise and time. This scenario often leads to ultra low-data regimes, where\nannotated images are extremely limited, posing significant challenges for the\ngeneralization of conventional deep learning methods on test images. To address\nthis, we introduce a generative deep learning framework, which uniquely\ngenerates high-quality paired segmentation masks and medical images, serving as\nauxiliary data for training robust models in data-scarce environments. Unlike\ntraditional generative models that treat data generation and segmentation model\ntraining as separate processes, our method employs multi-level optimization for\nend-to-end data generation. This approach allows segmentation performance to\ndirectly influence the data generation process, ensuring that the generated\ndata is specifically tailored to enhance the performance of the segmentation\nmodel. Our method demonstrated strong generalization performance across 9\ndiverse medical image segmentation tasks and on 16 datasets, in ultra-low data\nregimes, spanning various diseases, organs, and imaging modalities. When\napplied to various segmentation models, it achieved performance improvements of\n10-20\\% (absolute), in both same-domain and out-of-domain scenarios. Notably,\nit requires 8 to 20 times less training data than existing methods to achieve\ncomparable results. This advancement significantly improves the feasibility and\ncost-effectiveness of applying deep learning in medical imaging, particularly\nin scenarios with limited data availability.\n", "link": "http://arxiv.org/abs/2408.17421v1", "date": "2024-08-30", "relevancy": 1.6799, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5749}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5425}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20Enables%20Medical%20Image%20Segmentation%20in%20Ultra%20Low-Data%0A%20%20Regimes&body=Title%3A%20Generative%20AI%20Enables%20Medical%20Image%20Segmentation%20in%20Ultra%20Low-Data%0A%20%20Regimes%0AAuthor%3A%20Li%20Zhang%20and%20Basu%20Jindal%20and%20Ahmed%20Alaa%20and%20Robert%20Weinreb%20and%20David%20Wilson%20and%20Eran%20Segal%20and%20James%20Zou%20and%20Pengtao%20Xie%0AAbstract%3A%20%20%20Semantic%20segmentation%20of%20medical%20images%20is%20pivotal%20in%20applications%20like%0Adisease%20diagnosis%20and%20treatment%20planning.%20While%20deep%20learning%20has%20excelled%20in%0Aautomating%20this%20task%2C%20a%20major%20hurdle%20is%20the%20need%20for%20numerous%20annotated%0Asegmentation%20masks%2C%20which%20are%20resource-intensive%20to%20produce%20due%20to%20the%20required%0Aexpertise%20and%20time.%20This%20scenario%20often%20leads%20to%20ultra%20low-data%20regimes%2C%20where%0Aannotated%20images%20are%20extremely%20limited%2C%20posing%20significant%20challenges%20for%20the%0Ageneralization%20of%20conventional%20deep%20learning%20methods%20on%20test%20images.%20To%20address%0Athis%2C%20we%20introduce%20a%20generative%20deep%20learning%20framework%2C%20which%20uniquely%0Agenerates%20high-quality%20paired%20segmentation%20masks%20and%20medical%20images%2C%20serving%20as%0Aauxiliary%20data%20for%20training%20robust%20models%20in%20data-scarce%20environments.%20Unlike%0Atraditional%20generative%20models%20that%20treat%20data%20generation%20and%20segmentation%20model%0Atraining%20as%20separate%20processes%2C%20our%20method%20employs%20multi-level%20optimization%20for%0Aend-to-end%20data%20generation.%20This%20approach%20allows%20segmentation%20performance%20to%0Adirectly%20influence%20the%20data%20generation%20process%2C%20ensuring%20that%20the%20generated%0Adata%20is%20specifically%20tailored%20to%20enhance%20the%20performance%20of%20the%20segmentation%0Amodel.%20Our%20method%20demonstrated%20strong%20generalization%20performance%20across%209%0Adiverse%20medical%20image%20segmentation%20tasks%20and%20on%2016%20datasets%2C%20in%20ultra-low%20data%0Aregimes%2C%20spanning%20various%20diseases%2C%20organs%2C%20and%20imaging%20modalities.%20When%0Aapplied%20to%20various%20segmentation%20models%2C%20it%20achieved%20performance%20improvements%20of%0A10-20%5C%25%20%28absolute%29%2C%20in%20both%20same-domain%20and%20out-of-domain%20scenarios.%20Notably%2C%0Ait%20requires%208%20to%2020%20times%20less%20training%20data%20than%20existing%20methods%20to%20achieve%0Acomparable%20results.%20This%20advancement%20significantly%20improves%20the%20feasibility%20and%0Acost-effectiveness%20of%20applying%20deep%20learning%20in%20medical%20imaging%2C%20particularly%0Ain%20scenarios%20with%20limited%20data%20availability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520Enables%2520Medical%2520Image%2520Segmentation%2520in%2520Ultra%2520Low-Data%250A%2520%2520Regimes%26entry.906535625%3DLi%2520Zhang%2520and%2520Basu%2520Jindal%2520and%2520Ahmed%2520Alaa%2520and%2520Robert%2520Weinreb%2520and%2520David%2520Wilson%2520and%2520Eran%2520Segal%2520and%2520James%2520Zou%2520and%2520Pengtao%2520Xie%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520of%2520medical%2520images%2520is%2520pivotal%2520in%2520applications%2520like%250Adisease%2520diagnosis%2520and%2520treatment%2520planning.%2520While%2520deep%2520learning%2520has%2520excelled%2520in%250Aautomating%2520this%2520task%252C%2520a%2520major%2520hurdle%2520is%2520the%2520need%2520for%2520numerous%2520annotated%250Asegmentation%2520masks%252C%2520which%2520are%2520resource-intensive%2520to%2520produce%2520due%2520to%2520the%2520required%250Aexpertise%2520and%2520time.%2520This%2520scenario%2520often%2520leads%2520to%2520ultra%2520low-data%2520regimes%252C%2520where%250Aannotated%2520images%2520are%2520extremely%2520limited%252C%2520posing%2520significant%2520challenges%2520for%2520the%250Ageneralization%2520of%2520conventional%2520deep%2520learning%2520methods%2520on%2520test%2520images.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520a%2520generative%2520deep%2520learning%2520framework%252C%2520which%2520uniquely%250Agenerates%2520high-quality%2520paired%2520segmentation%2520masks%2520and%2520medical%2520images%252C%2520serving%2520as%250Aauxiliary%2520data%2520for%2520training%2520robust%2520models%2520in%2520data-scarce%2520environments.%2520Unlike%250Atraditional%2520generative%2520models%2520that%2520treat%2520data%2520generation%2520and%2520segmentation%2520model%250Atraining%2520as%2520separate%2520processes%252C%2520our%2520method%2520employs%2520multi-level%2520optimization%2520for%250Aend-to-end%2520data%2520generation.%2520This%2520approach%2520allows%2520segmentation%2520performance%2520to%250Adirectly%2520influence%2520the%2520data%2520generation%2520process%252C%2520ensuring%2520that%2520the%2520generated%250Adata%2520is%2520specifically%2520tailored%2520to%2520enhance%2520the%2520performance%2520of%2520the%2520segmentation%250Amodel.%2520Our%2520method%2520demonstrated%2520strong%2520generalization%2520performance%2520across%25209%250Adiverse%2520medical%2520image%2520segmentation%2520tasks%2520and%2520on%252016%2520datasets%252C%2520in%2520ultra-low%2520data%250Aregimes%252C%2520spanning%2520various%2520diseases%252C%2520organs%252C%2520and%2520imaging%2520modalities.%2520When%250Aapplied%2520to%2520various%2520segmentation%2520models%252C%2520it%2520achieved%2520performance%2520improvements%2520of%250A10-20%255C%2525%2520%2528absolute%2529%252C%2520in%2520both%2520same-domain%2520and%2520out-of-domain%2520scenarios.%2520Notably%252C%250Ait%2520requires%25208%2520to%252020%2520times%2520less%2520training%2520data%2520than%2520existing%2520methods%2520to%2520achieve%250Acomparable%2520results.%2520This%2520advancement%2520significantly%2520improves%2520the%2520feasibility%2520and%250Acost-effectiveness%2520of%2520applying%2520deep%2520learning%2520in%2520medical%2520imaging%252C%2520particularly%250Ain%2520scenarios%2520with%2520limited%2520data%2520availability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20Enables%20Medical%20Image%20Segmentation%20in%20Ultra%20Low-Data%0A%20%20Regimes&entry.906535625=Li%20Zhang%20and%20Basu%20Jindal%20and%20Ahmed%20Alaa%20and%20Robert%20Weinreb%20and%20David%20Wilson%20and%20Eran%20Segal%20and%20James%20Zou%20and%20Pengtao%20Xie&entry.1292438233=%20%20Semantic%20segmentation%20of%20medical%20images%20is%20pivotal%20in%20applications%20like%0Adisease%20diagnosis%20and%20treatment%20planning.%20While%20deep%20learning%20has%20excelled%20in%0Aautomating%20this%20task%2C%20a%20major%20hurdle%20is%20the%20need%20for%20numerous%20annotated%0Asegmentation%20masks%2C%20which%20are%20resource-intensive%20to%20produce%20due%20to%20the%20required%0Aexpertise%20and%20time.%20This%20scenario%20often%20leads%20to%20ultra%20low-data%20regimes%2C%20where%0Aannotated%20images%20are%20extremely%20limited%2C%20posing%20significant%20challenges%20for%20the%0Ageneralization%20of%20conventional%20deep%20learning%20methods%20on%20test%20images.%20To%20address%0Athis%2C%20we%20introduce%20a%20generative%20deep%20learning%20framework%2C%20which%20uniquely%0Agenerates%20high-quality%20paired%20segmentation%20masks%20and%20medical%20images%2C%20serving%20as%0Aauxiliary%20data%20for%20training%20robust%20models%20in%20data-scarce%20environments.%20Unlike%0Atraditional%20generative%20models%20that%20treat%20data%20generation%20and%20segmentation%20model%0Atraining%20as%20separate%20processes%2C%20our%20method%20employs%20multi-level%20optimization%20for%0Aend-to-end%20data%20generation.%20This%20approach%20allows%20segmentation%20performance%20to%0Adirectly%20influence%20the%20data%20generation%20process%2C%20ensuring%20that%20the%20generated%0Adata%20is%20specifically%20tailored%20to%20enhance%20the%20performance%20of%20the%20segmentation%0Amodel.%20Our%20method%20demonstrated%20strong%20generalization%20performance%20across%209%0Adiverse%20medical%20image%20segmentation%20tasks%20and%20on%2016%20datasets%2C%20in%20ultra-low%20data%0Aregimes%2C%20spanning%20various%20diseases%2C%20organs%2C%20and%20imaging%20modalities.%20When%0Aapplied%20to%20various%20segmentation%20models%2C%20it%20achieved%20performance%20improvements%20of%0A10-20%5C%25%20%28absolute%29%2C%20in%20both%20same-domain%20and%20out-of-domain%20scenarios.%20Notably%2C%0Ait%20requires%208%20to%2020%20times%20less%20training%20data%20than%20existing%20methods%20to%20achieve%0Acomparable%20results.%20This%20advancement%20significantly%20improves%20the%20feasibility%20and%0Acost-effectiveness%20of%20applying%20deep%20learning%20in%20medical%20imaging%2C%20particularly%0Ain%20scenarios%20with%20limited%20data%20availability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17421v1&entry.124074799=Read"},
{"title": "UrBench: A Comprehensive Benchmark for Evaluating Large Multimodal\n  Models in Multi-View Urban Scenarios", "author": "Baichuan Zhou and Haote Yang and Dairong Chen and Junyan Ye and Tianyi Bai and Jinhua Yu and Songyang Zhang and Dahua Lin and Conghui He and Weijia Li", "abstract": "  Recent evaluations of Large Multimodal Models (LMMs) have explored their\ncapabilities in various domains, with only few benchmarks specifically focusing\non urban environments. Moreover, existing urban benchmarks have been limited to\nevaluating LMMs with basic region-level urban tasks under singular views,\nleading to incomplete evaluations of LMMs' abilities in urban environments. To\naddress these issues, we present UrBench, a comprehensive benchmark designed\nfor evaluating LMMs in complex multi-view urban scenarios. UrBench contains\n11.6K meticulously curated questions at both region-level and role-level that\ncover 4 task dimensions: Geo-Localization, Scene Reasoning, Scene\nUnderstanding, and Object Understanding, totaling 14 task types. In\nconstructing UrBench, we utilize data from existing datasets and additionally\ncollect data from 11 cities, creating new annotations using a cross-view\ndetection-matching method. With these images and annotations, we then integrate\nLMM-based, rule-based, and human-based methods to construct large-scale\nhigh-quality questions. Our evaluations on 21 LMMs show that current LMMs\nstruggle in the urban environments in several aspects. Even the best performing\nGPT-4o lags behind humans in most tasks, ranging from simple tasks such as\ncounting to complex tasks such as orientation, localization and object\nattribute recognition, with an average performance gap of 17.4%. Our benchmark\nalso reveals that LMMs exhibit inconsistent behaviors with different urban\nviews, especially with respect to understanding cross-view relations. UrBench\ndatasets and benchmark results will be publicly available at\nhttps://opendatalab.github.io/UrBench/.\n", "link": "http://arxiv.org/abs/2408.17267v1", "date": "2024-08-30", "relevancy": 1.6722, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5652}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5614}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UrBench%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Large%20Multimodal%0A%20%20Models%20in%20Multi-View%20Urban%20Scenarios&body=Title%3A%20UrBench%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Large%20Multimodal%0A%20%20Models%20in%20Multi-View%20Urban%20Scenarios%0AAuthor%3A%20Baichuan%20Zhou%20and%20Haote%20Yang%20and%20Dairong%20Chen%20and%20Junyan%20Ye%20and%20Tianyi%20Bai%20and%20Jinhua%20Yu%20and%20Songyang%20Zhang%20and%20Dahua%20Lin%20and%20Conghui%20He%20and%20Weijia%20Li%0AAbstract%3A%20%20%20Recent%20evaluations%20of%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20explored%20their%0Acapabilities%20in%20various%20domains%2C%20with%20only%20few%20benchmarks%20specifically%20focusing%0Aon%20urban%20environments.%20Moreover%2C%20existing%20urban%20benchmarks%20have%20been%20limited%20to%0Aevaluating%20LMMs%20with%20basic%20region-level%20urban%20tasks%20under%20singular%20views%2C%0Aleading%20to%20incomplete%20evaluations%20of%20LMMs%27%20abilities%20in%20urban%20environments.%20To%0Aaddress%20these%20issues%2C%20we%20present%20UrBench%2C%20a%20comprehensive%20benchmark%20designed%0Afor%20evaluating%20LMMs%20in%20complex%20multi-view%20urban%20scenarios.%20UrBench%20contains%0A11.6K%20meticulously%20curated%20questions%20at%20both%20region-level%20and%20role-level%20that%0Acover%204%20task%20dimensions%3A%20Geo-Localization%2C%20Scene%20Reasoning%2C%20Scene%0AUnderstanding%2C%20and%20Object%20Understanding%2C%20totaling%2014%20task%20types.%20In%0Aconstructing%20UrBench%2C%20we%20utilize%20data%20from%20existing%20datasets%20and%20additionally%0Acollect%20data%20from%2011%20cities%2C%20creating%20new%20annotations%20using%20a%20cross-view%0Adetection-matching%20method.%20With%20these%20images%20and%20annotations%2C%20we%20then%20integrate%0ALMM-based%2C%20rule-based%2C%20and%20human-based%20methods%20to%20construct%20large-scale%0Ahigh-quality%20questions.%20Our%20evaluations%20on%2021%20LMMs%20show%20that%20current%20LMMs%0Astruggle%20in%20the%20urban%20environments%20in%20several%20aspects.%20Even%20the%20best%20performing%0AGPT-4o%20lags%20behind%20humans%20in%20most%20tasks%2C%20ranging%20from%20simple%20tasks%20such%20as%0Acounting%20to%20complex%20tasks%20such%20as%20orientation%2C%20localization%20and%20object%0Aattribute%20recognition%2C%20with%20an%20average%20performance%20gap%20of%2017.4%25.%20Our%20benchmark%0Aalso%20reveals%20that%20LMMs%20exhibit%20inconsistent%20behaviors%20with%20different%20urban%0Aviews%2C%20especially%20with%20respect%20to%20understanding%20cross-view%20relations.%20UrBench%0Adatasets%20and%20benchmark%20results%20will%20be%20publicly%20available%20at%0Ahttps%3A//opendatalab.github.io/UrBench/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Evaluating%2520Large%2520Multimodal%250A%2520%2520Models%2520in%2520Multi-View%2520Urban%2520Scenarios%26entry.906535625%3DBaichuan%2520Zhou%2520and%2520Haote%2520Yang%2520and%2520Dairong%2520Chen%2520and%2520Junyan%2520Ye%2520and%2520Tianyi%2520Bai%2520and%2520Jinhua%2520Yu%2520and%2520Songyang%2520Zhang%2520and%2520Dahua%2520Lin%2520and%2520Conghui%2520He%2520and%2520Weijia%2520Li%26entry.1292438233%3D%2520%2520Recent%2520evaluations%2520of%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520have%2520explored%2520their%250Acapabilities%2520in%2520various%2520domains%252C%2520with%2520only%2520few%2520benchmarks%2520specifically%2520focusing%250Aon%2520urban%2520environments.%2520Moreover%252C%2520existing%2520urban%2520benchmarks%2520have%2520been%2520limited%2520to%250Aevaluating%2520LMMs%2520with%2520basic%2520region-level%2520urban%2520tasks%2520under%2520singular%2520views%252C%250Aleading%2520to%2520incomplete%2520evaluations%2520of%2520LMMs%2527%2520abilities%2520in%2520urban%2520environments.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520present%2520UrBench%252C%2520a%2520comprehensive%2520benchmark%2520designed%250Afor%2520evaluating%2520LMMs%2520in%2520complex%2520multi-view%2520urban%2520scenarios.%2520UrBench%2520contains%250A11.6K%2520meticulously%2520curated%2520questions%2520at%2520both%2520region-level%2520and%2520role-level%2520that%250Acover%25204%2520task%2520dimensions%253A%2520Geo-Localization%252C%2520Scene%2520Reasoning%252C%2520Scene%250AUnderstanding%252C%2520and%2520Object%2520Understanding%252C%2520totaling%252014%2520task%2520types.%2520In%250Aconstructing%2520UrBench%252C%2520we%2520utilize%2520data%2520from%2520existing%2520datasets%2520and%2520additionally%250Acollect%2520data%2520from%252011%2520cities%252C%2520creating%2520new%2520annotations%2520using%2520a%2520cross-view%250Adetection-matching%2520method.%2520With%2520these%2520images%2520and%2520annotations%252C%2520we%2520then%2520integrate%250ALMM-based%252C%2520rule-based%252C%2520and%2520human-based%2520methods%2520to%2520construct%2520large-scale%250Ahigh-quality%2520questions.%2520Our%2520evaluations%2520on%252021%2520LMMs%2520show%2520that%2520current%2520LMMs%250Astruggle%2520in%2520the%2520urban%2520environments%2520in%2520several%2520aspects.%2520Even%2520the%2520best%2520performing%250AGPT-4o%2520lags%2520behind%2520humans%2520in%2520most%2520tasks%252C%2520ranging%2520from%2520simple%2520tasks%2520such%2520as%250Acounting%2520to%2520complex%2520tasks%2520such%2520as%2520orientation%252C%2520localization%2520and%2520object%250Aattribute%2520recognition%252C%2520with%2520an%2520average%2520performance%2520gap%2520of%252017.4%2525.%2520Our%2520benchmark%250Aalso%2520reveals%2520that%2520LMMs%2520exhibit%2520inconsistent%2520behaviors%2520with%2520different%2520urban%250Aviews%252C%2520especially%2520with%2520respect%2520to%2520understanding%2520cross-view%2520relations.%2520UrBench%250Adatasets%2520and%2520benchmark%2520results%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//opendatalab.github.io/UrBench/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrBench%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Large%20Multimodal%0A%20%20Models%20in%20Multi-View%20Urban%20Scenarios&entry.906535625=Baichuan%20Zhou%20and%20Haote%20Yang%20and%20Dairong%20Chen%20and%20Junyan%20Ye%20and%20Tianyi%20Bai%20and%20Jinhua%20Yu%20and%20Songyang%20Zhang%20and%20Dahua%20Lin%20and%20Conghui%20He%20and%20Weijia%20Li&entry.1292438233=%20%20Recent%20evaluations%20of%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20explored%20their%0Acapabilities%20in%20various%20domains%2C%20with%20only%20few%20benchmarks%20specifically%20focusing%0Aon%20urban%20environments.%20Moreover%2C%20existing%20urban%20benchmarks%20have%20been%20limited%20to%0Aevaluating%20LMMs%20with%20basic%20region-level%20urban%20tasks%20under%20singular%20views%2C%0Aleading%20to%20incomplete%20evaluations%20of%20LMMs%27%20abilities%20in%20urban%20environments.%20To%0Aaddress%20these%20issues%2C%20we%20present%20UrBench%2C%20a%20comprehensive%20benchmark%20designed%0Afor%20evaluating%20LMMs%20in%20complex%20multi-view%20urban%20scenarios.%20UrBench%20contains%0A11.6K%20meticulously%20curated%20questions%20at%20both%20region-level%20and%20role-level%20that%0Acover%204%20task%20dimensions%3A%20Geo-Localization%2C%20Scene%20Reasoning%2C%20Scene%0AUnderstanding%2C%20and%20Object%20Understanding%2C%20totaling%2014%20task%20types.%20In%0Aconstructing%20UrBench%2C%20we%20utilize%20data%20from%20existing%20datasets%20and%20additionally%0Acollect%20data%20from%2011%20cities%2C%20creating%20new%20annotations%20using%20a%20cross-view%0Adetection-matching%20method.%20With%20these%20images%20and%20annotations%2C%20we%20then%20integrate%0ALMM-based%2C%20rule-based%2C%20and%20human-based%20methods%20to%20construct%20large-scale%0Ahigh-quality%20questions.%20Our%20evaluations%20on%2021%20LMMs%20show%20that%20current%20LMMs%0Astruggle%20in%20the%20urban%20environments%20in%20several%20aspects.%20Even%20the%20best%20performing%0AGPT-4o%20lags%20behind%20humans%20in%20most%20tasks%2C%20ranging%20from%20simple%20tasks%20such%20as%0Acounting%20to%20complex%20tasks%20such%20as%20orientation%2C%20localization%20and%20object%0Aattribute%20recognition%2C%20with%20an%20average%20performance%20gap%20of%2017.4%25.%20Our%20benchmark%0Aalso%20reveals%20that%20LMMs%20exhibit%20inconsistent%20behaviors%20with%20different%20urban%0Aviews%2C%20especially%20with%20respect%20to%20understanding%20cross-view%20relations.%20UrBench%0Adatasets%20and%20benchmark%20results%20will%20be%20publicly%20available%20at%0Ahttps%3A//opendatalab.github.io/UrBench/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17267v1&entry.124074799=Read"},
{"title": "Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with\n  Robotic and Human Co-Workers", "author": "Aleksandar Krnjaic and Raul D. Steleac and Jonathan D. Thomas and Georgios Papoudakis and Lukas Sch\u00e4fer and Andrew Wing Keung To and Kuan-Ho Lao and Murat Cubuktepe and Matthew Haley and Peter B\u00f6rsting and Stefano V. Albrecht", "abstract": "  We consider a warehouse in which dozens of mobile robots and human pickers\nwork together to collect and deliver items within the warehouse. The\nfundamental problem we tackle, called the order-picking problem, is how these\nworker agents must coordinate their movement and actions in the warehouse to\nmaximise performance in this task. Established industry methods using heuristic\napproaches require large engineering efforts to optimise for innately variable\nwarehouse configurations. In contrast, multi-agent reinforcement learning\n(MARL) can be flexibly applied to diverse warehouse configurations (e.g. size,\nlayout, number/types of workers, item replenishment frequency), and different\ntypes of order-picking paradigms (e.g. Goods-to-Person and Person-to-Goods), as\nthe agents can learn how to cooperate optimally through experience. We develop\nhierarchical MARL algorithms in which a manager agent assigns goals to worker\nagents, and the policies of the manager and workers are co-trained toward\nmaximising a global objective (e.g. pick rate). Our hierarchical algorithms\nachieve significant gains in sample efficiency over baseline MARL algorithms\nand overall pick rates over multiple established industry heuristics in a\ndiverse set of warehouse configurations and different order-picking paradigms.\n", "link": "http://arxiv.org/abs/2212.11498v3", "date": "2024-08-30", "relevancy": 1.6435, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5761}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5754}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Multi-Agent%20Reinforcement%20Learning%20for%20Warehouse%20Logistics%20with%0A%20%20Robotic%20and%20Human%20Co-Workers&body=Title%3A%20Scalable%20Multi-Agent%20Reinforcement%20Learning%20for%20Warehouse%20Logistics%20with%0A%20%20Robotic%20and%20Human%20Co-Workers%0AAuthor%3A%20Aleksandar%20Krnjaic%20and%20Raul%20D.%20Steleac%20and%20Jonathan%20D.%20Thomas%20and%20Georgios%20Papoudakis%20and%20Lukas%20Sch%C3%A4fer%20and%20Andrew%20Wing%20Keung%20To%20and%20Kuan-Ho%20Lao%20and%20Murat%20Cubuktepe%20and%20Matthew%20Haley%20and%20Peter%20B%C3%B6rsting%20and%20Stefano%20V.%20Albrecht%0AAbstract%3A%20%20%20We%20consider%20a%20warehouse%20in%20which%20dozens%20of%20mobile%20robots%20and%20human%20pickers%0Awork%20together%20to%20collect%20and%20deliver%20items%20within%20the%20warehouse.%20The%0Afundamental%20problem%20we%20tackle%2C%20called%20the%20order-picking%20problem%2C%20is%20how%20these%0Aworker%20agents%20must%20coordinate%20their%20movement%20and%20actions%20in%20the%20warehouse%20to%0Amaximise%20performance%20in%20this%20task.%20Established%20industry%20methods%20using%20heuristic%0Aapproaches%20require%20large%20engineering%20efforts%20to%20optimise%20for%20innately%20variable%0Awarehouse%20configurations.%20In%20contrast%2C%20multi-agent%20reinforcement%20learning%0A%28MARL%29%20can%20be%20flexibly%20applied%20to%20diverse%20warehouse%20configurations%20%28e.g.%20size%2C%0Alayout%2C%20number/types%20of%20workers%2C%20item%20replenishment%20frequency%29%2C%20and%20different%0Atypes%20of%20order-picking%20paradigms%20%28e.g.%20Goods-to-Person%20and%20Person-to-Goods%29%2C%20as%0Athe%20agents%20can%20learn%20how%20to%20cooperate%20optimally%20through%20experience.%20We%20develop%0Ahierarchical%20MARL%20algorithms%20in%20which%20a%20manager%20agent%20assigns%20goals%20to%20worker%0Aagents%2C%20and%20the%20policies%20of%20the%20manager%20and%20workers%20are%20co-trained%20toward%0Amaximising%20a%20global%20objective%20%28e.g.%20pick%20rate%29.%20Our%20hierarchical%20algorithms%0Aachieve%20significant%20gains%20in%20sample%20efficiency%20over%20baseline%20MARL%20algorithms%0Aand%20overall%20pick%20rates%20over%20multiple%20established%20industry%20heuristics%20in%20a%0Adiverse%20set%20of%20warehouse%20configurations%20and%20different%20order-picking%20paradigms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.11498v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Multi-Agent%2520Reinforcement%2520Learning%2520for%2520Warehouse%2520Logistics%2520with%250A%2520%2520Robotic%2520and%2520Human%2520Co-Workers%26entry.906535625%3DAleksandar%2520Krnjaic%2520and%2520Raul%2520D.%2520Steleac%2520and%2520Jonathan%2520D.%2520Thomas%2520and%2520Georgios%2520Papoudakis%2520and%2520Lukas%2520Sch%25C3%25A4fer%2520and%2520Andrew%2520Wing%2520Keung%2520To%2520and%2520Kuan-Ho%2520Lao%2520and%2520Murat%2520Cubuktepe%2520and%2520Matthew%2520Haley%2520and%2520Peter%2520B%25C3%25B6rsting%2520and%2520Stefano%2520V.%2520Albrecht%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520warehouse%2520in%2520which%2520dozens%2520of%2520mobile%2520robots%2520and%2520human%2520pickers%250Awork%2520together%2520to%2520collect%2520and%2520deliver%2520items%2520within%2520the%2520warehouse.%2520The%250Afundamental%2520problem%2520we%2520tackle%252C%2520called%2520the%2520order-picking%2520problem%252C%2520is%2520how%2520these%250Aworker%2520agents%2520must%2520coordinate%2520their%2520movement%2520and%2520actions%2520in%2520the%2520warehouse%2520to%250Amaximise%2520performance%2520in%2520this%2520task.%2520Established%2520industry%2520methods%2520using%2520heuristic%250Aapproaches%2520require%2520large%2520engineering%2520efforts%2520to%2520optimise%2520for%2520innately%2520variable%250Awarehouse%2520configurations.%2520In%2520contrast%252C%2520multi-agent%2520reinforcement%2520learning%250A%2528MARL%2529%2520can%2520be%2520flexibly%2520applied%2520to%2520diverse%2520warehouse%2520configurations%2520%2528e.g.%2520size%252C%250Alayout%252C%2520number/types%2520of%2520workers%252C%2520item%2520replenishment%2520frequency%2529%252C%2520and%2520different%250Atypes%2520of%2520order-picking%2520paradigms%2520%2528e.g.%2520Goods-to-Person%2520and%2520Person-to-Goods%2529%252C%2520as%250Athe%2520agents%2520can%2520learn%2520how%2520to%2520cooperate%2520optimally%2520through%2520experience.%2520We%2520develop%250Ahierarchical%2520MARL%2520algorithms%2520in%2520which%2520a%2520manager%2520agent%2520assigns%2520goals%2520to%2520worker%250Aagents%252C%2520and%2520the%2520policies%2520of%2520the%2520manager%2520and%2520workers%2520are%2520co-trained%2520toward%250Amaximising%2520a%2520global%2520objective%2520%2528e.g.%2520pick%2520rate%2529.%2520Our%2520hierarchical%2520algorithms%250Aachieve%2520significant%2520gains%2520in%2520sample%2520efficiency%2520over%2520baseline%2520MARL%2520algorithms%250Aand%2520overall%2520pick%2520rates%2520over%2520multiple%2520established%2520industry%2520heuristics%2520in%2520a%250Adiverse%2520set%2520of%2520warehouse%2520configurations%2520and%2520different%2520order-picking%2520paradigms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.11498v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Multi-Agent%20Reinforcement%20Learning%20for%20Warehouse%20Logistics%20with%0A%20%20Robotic%20and%20Human%20Co-Workers&entry.906535625=Aleksandar%20Krnjaic%20and%20Raul%20D.%20Steleac%20and%20Jonathan%20D.%20Thomas%20and%20Georgios%20Papoudakis%20and%20Lukas%20Sch%C3%A4fer%20and%20Andrew%20Wing%20Keung%20To%20and%20Kuan-Ho%20Lao%20and%20Murat%20Cubuktepe%20and%20Matthew%20Haley%20and%20Peter%20B%C3%B6rsting%20and%20Stefano%20V.%20Albrecht&entry.1292438233=%20%20We%20consider%20a%20warehouse%20in%20which%20dozens%20of%20mobile%20robots%20and%20human%20pickers%0Awork%20together%20to%20collect%20and%20deliver%20items%20within%20the%20warehouse.%20The%0Afundamental%20problem%20we%20tackle%2C%20called%20the%20order-picking%20problem%2C%20is%20how%20these%0Aworker%20agents%20must%20coordinate%20their%20movement%20and%20actions%20in%20the%20warehouse%20to%0Amaximise%20performance%20in%20this%20task.%20Established%20industry%20methods%20using%20heuristic%0Aapproaches%20require%20large%20engineering%20efforts%20to%20optimise%20for%20innately%20variable%0Awarehouse%20configurations.%20In%20contrast%2C%20multi-agent%20reinforcement%20learning%0A%28MARL%29%20can%20be%20flexibly%20applied%20to%20diverse%20warehouse%20configurations%20%28e.g.%20size%2C%0Alayout%2C%20number/types%20of%20workers%2C%20item%20replenishment%20frequency%29%2C%20and%20different%0Atypes%20of%20order-picking%20paradigms%20%28e.g.%20Goods-to-Person%20and%20Person-to-Goods%29%2C%20as%0Athe%20agents%20can%20learn%20how%20to%20cooperate%20optimally%20through%20experience.%20We%20develop%0Ahierarchical%20MARL%20algorithms%20in%20which%20a%20manager%20agent%20assigns%20goals%20to%20worker%0Aagents%2C%20and%20the%20policies%20of%20the%20manager%20and%20workers%20are%20co-trained%20toward%0Amaximising%20a%20global%20objective%20%28e.g.%20pick%20rate%29.%20Our%20hierarchical%20algorithms%0Aachieve%20significant%20gains%20in%20sample%20efficiency%20over%20baseline%20MARL%20algorithms%0Aand%20overall%20pick%20rates%20over%20multiple%20established%20industry%20heuristics%20in%20a%0Adiverse%20set%20of%20warehouse%20configurations%20and%20different%20order-picking%20paradigms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.11498v3&entry.124074799=Read"},
{"title": "EMHI: A Multimodal Egocentric Human Motion Dataset with HMD and\n  Body-Worn IMUs", "author": "Zhen Fan and Peng Dai and Zhuo Su and Xu Gao and Zheng Lv and Jiarui Zhang and Tianyuan Du and Guidong Wang and Yang Zhang", "abstract": "  Egocentric human pose estimation (HPE) using wearable sensors is essential\nfor VR/AR applications. Most methods rely solely on either egocentric-view\nimages or sparse Inertial Measurement Unit (IMU) signals, leading to\ninaccuracies due to self-occlusion in images or the sparseness and drift of\ninertial sensors. Most importantly, the lack of real-world datasets containing\nboth modalities is a major obstacle to progress in this field. To overcome the\nbarrier, we propose EMHI, a multimodal \\textbf{E}gocentric human\n\\textbf{M}otion dataset with \\textbf{H}ead-Mounted Display (HMD) and body-worn\n\\textbf{I}MUs, with all data collected under the real VR product suite.\nSpecifically, EMHI provides synchronized stereo images from downward-sloping\ncameras on the headset and IMU data from body-worn sensors, along with pose\nannotations in SMPL format. This dataset consists of 885 sequences captured by\n58 subjects performing 39 actions, totaling about 28.5 hours of recording. We\nevaluate the annotations by comparing them with optical marker-based SMPL\nfitting results. To substantiate the reliability of our dataset, we introduce\nMEPoser, a new baseline method for multimodal egocentric HPE, which employs a\nmultimodal fusion encoder, temporal feature encoder, and MLP-based regression\nheads. The experiments on EMHI show that MEPoser outperforms existing\nsingle-modal methods and demonstrates the value of our dataset in solving the\nproblem of egocentric HPE. We believe the release of EMHI and the method could\nadvance the research of egocentric HPE and expedite the practical\nimplementation of this technology in VR/AR products.\n", "link": "http://arxiv.org/abs/2408.17168v1", "date": "2024-08-30", "relevancy": 1.6403, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5537}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5411}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMHI%3A%20A%20Multimodal%20Egocentric%20Human%20Motion%20Dataset%20with%20HMD%20and%0A%20%20Body-Worn%20IMUs&body=Title%3A%20EMHI%3A%20A%20Multimodal%20Egocentric%20Human%20Motion%20Dataset%20with%20HMD%20and%0A%20%20Body-Worn%20IMUs%0AAuthor%3A%20Zhen%20Fan%20and%20Peng%20Dai%20and%20Zhuo%20Su%20and%20Xu%20Gao%20and%20Zheng%20Lv%20and%20Jiarui%20Zhang%20and%20Tianyuan%20Du%20and%20Guidong%20Wang%20and%20Yang%20Zhang%0AAbstract%3A%20%20%20Egocentric%20human%20pose%20estimation%20%28HPE%29%20using%20wearable%20sensors%20is%20essential%0Afor%20VR/AR%20applications.%20Most%20methods%20rely%20solely%20on%20either%20egocentric-view%0Aimages%20or%20sparse%20Inertial%20Measurement%20Unit%20%28IMU%29%20signals%2C%20leading%20to%0Ainaccuracies%20due%20to%20self-occlusion%20in%20images%20or%20the%20sparseness%20and%20drift%20of%0Ainertial%20sensors.%20Most%20importantly%2C%20the%20lack%20of%20real-world%20datasets%20containing%0Aboth%20modalities%20is%20a%20major%20obstacle%20to%20progress%20in%20this%20field.%20To%20overcome%20the%0Abarrier%2C%20we%20propose%20EMHI%2C%20a%20multimodal%20%5Ctextbf%7BE%7Dgocentric%20human%0A%5Ctextbf%7BM%7Dotion%20dataset%20with%20%5Ctextbf%7BH%7Dead-Mounted%20Display%20%28HMD%29%20and%20body-worn%0A%5Ctextbf%7BI%7DMUs%2C%20with%20all%20data%20collected%20under%20the%20real%20VR%20product%20suite.%0ASpecifically%2C%20EMHI%20provides%20synchronized%20stereo%20images%20from%20downward-sloping%0Acameras%20on%20the%20headset%20and%20IMU%20data%20from%20body-worn%20sensors%2C%20along%20with%20pose%0Aannotations%20in%20SMPL%20format.%20This%20dataset%20consists%20of%20885%20sequences%20captured%20by%0A58%20subjects%20performing%2039%20actions%2C%20totaling%20about%2028.5%20hours%20of%20recording.%20We%0Aevaluate%20the%20annotations%20by%20comparing%20them%20with%20optical%20marker-based%20SMPL%0Afitting%20results.%20To%20substantiate%20the%20reliability%20of%20our%20dataset%2C%20we%20introduce%0AMEPoser%2C%20a%20new%20baseline%20method%20for%20multimodal%20egocentric%20HPE%2C%20which%20employs%20a%0Amultimodal%20fusion%20encoder%2C%20temporal%20feature%20encoder%2C%20and%20MLP-based%20regression%0Aheads.%20The%20experiments%20on%20EMHI%20show%20that%20MEPoser%20outperforms%20existing%0Asingle-modal%20methods%20and%20demonstrates%20the%20value%20of%20our%20dataset%20in%20solving%20the%0Aproblem%20of%20egocentric%20HPE.%20We%20believe%20the%20release%20of%20EMHI%20and%20the%20method%20could%0Aadvance%20the%20research%20of%20egocentric%20HPE%20and%20expedite%20the%20practical%0Aimplementation%20of%20this%20technology%20in%20VR/AR%20products.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMHI%253A%2520A%2520Multimodal%2520Egocentric%2520Human%2520Motion%2520Dataset%2520with%2520HMD%2520and%250A%2520%2520Body-Worn%2520IMUs%26entry.906535625%3DZhen%2520Fan%2520and%2520Peng%2520Dai%2520and%2520Zhuo%2520Su%2520and%2520Xu%2520Gao%2520and%2520Zheng%2520Lv%2520and%2520Jiarui%2520Zhang%2520and%2520Tianyuan%2520Du%2520and%2520Guidong%2520Wang%2520and%2520Yang%2520Zhang%26entry.1292438233%3D%2520%2520Egocentric%2520human%2520pose%2520estimation%2520%2528HPE%2529%2520using%2520wearable%2520sensors%2520is%2520essential%250Afor%2520VR/AR%2520applications.%2520Most%2520methods%2520rely%2520solely%2520on%2520either%2520egocentric-view%250Aimages%2520or%2520sparse%2520Inertial%2520Measurement%2520Unit%2520%2528IMU%2529%2520signals%252C%2520leading%2520to%250Ainaccuracies%2520due%2520to%2520self-occlusion%2520in%2520images%2520or%2520the%2520sparseness%2520and%2520drift%2520of%250Ainertial%2520sensors.%2520Most%2520importantly%252C%2520the%2520lack%2520of%2520real-world%2520datasets%2520containing%250Aboth%2520modalities%2520is%2520a%2520major%2520obstacle%2520to%2520progress%2520in%2520this%2520field.%2520To%2520overcome%2520the%250Abarrier%252C%2520we%2520propose%2520EMHI%252C%2520a%2520multimodal%2520%255Ctextbf%257BE%257Dgocentric%2520human%250A%255Ctextbf%257BM%257Dotion%2520dataset%2520with%2520%255Ctextbf%257BH%257Dead-Mounted%2520Display%2520%2528HMD%2529%2520and%2520body-worn%250A%255Ctextbf%257BI%257DMUs%252C%2520with%2520all%2520data%2520collected%2520under%2520the%2520real%2520VR%2520product%2520suite.%250ASpecifically%252C%2520EMHI%2520provides%2520synchronized%2520stereo%2520images%2520from%2520downward-sloping%250Acameras%2520on%2520the%2520headset%2520and%2520IMU%2520data%2520from%2520body-worn%2520sensors%252C%2520along%2520with%2520pose%250Aannotations%2520in%2520SMPL%2520format.%2520This%2520dataset%2520consists%2520of%2520885%2520sequences%2520captured%2520by%250A58%2520subjects%2520performing%252039%2520actions%252C%2520totaling%2520about%252028.5%2520hours%2520of%2520recording.%2520We%250Aevaluate%2520the%2520annotations%2520by%2520comparing%2520them%2520with%2520optical%2520marker-based%2520SMPL%250Afitting%2520results.%2520To%2520substantiate%2520the%2520reliability%2520of%2520our%2520dataset%252C%2520we%2520introduce%250AMEPoser%252C%2520a%2520new%2520baseline%2520method%2520for%2520multimodal%2520egocentric%2520HPE%252C%2520which%2520employs%2520a%250Amultimodal%2520fusion%2520encoder%252C%2520temporal%2520feature%2520encoder%252C%2520and%2520MLP-based%2520regression%250Aheads.%2520The%2520experiments%2520on%2520EMHI%2520show%2520that%2520MEPoser%2520outperforms%2520existing%250Asingle-modal%2520methods%2520and%2520demonstrates%2520the%2520value%2520of%2520our%2520dataset%2520in%2520solving%2520the%250Aproblem%2520of%2520egocentric%2520HPE.%2520We%2520believe%2520the%2520release%2520of%2520EMHI%2520and%2520the%2520method%2520could%250Aadvance%2520the%2520research%2520of%2520egocentric%2520HPE%2520and%2520expedite%2520the%2520practical%250Aimplementation%2520of%2520this%2520technology%2520in%2520VR/AR%2520products.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMHI%3A%20A%20Multimodal%20Egocentric%20Human%20Motion%20Dataset%20with%20HMD%20and%0A%20%20Body-Worn%20IMUs&entry.906535625=Zhen%20Fan%20and%20Peng%20Dai%20and%20Zhuo%20Su%20and%20Xu%20Gao%20and%20Zheng%20Lv%20and%20Jiarui%20Zhang%20and%20Tianyuan%20Du%20and%20Guidong%20Wang%20and%20Yang%20Zhang&entry.1292438233=%20%20Egocentric%20human%20pose%20estimation%20%28HPE%29%20using%20wearable%20sensors%20is%20essential%0Afor%20VR/AR%20applications.%20Most%20methods%20rely%20solely%20on%20either%20egocentric-view%0Aimages%20or%20sparse%20Inertial%20Measurement%20Unit%20%28IMU%29%20signals%2C%20leading%20to%0Ainaccuracies%20due%20to%20self-occlusion%20in%20images%20or%20the%20sparseness%20and%20drift%20of%0Ainertial%20sensors.%20Most%20importantly%2C%20the%20lack%20of%20real-world%20datasets%20containing%0Aboth%20modalities%20is%20a%20major%20obstacle%20to%20progress%20in%20this%20field.%20To%20overcome%20the%0Abarrier%2C%20we%20propose%20EMHI%2C%20a%20multimodal%20%5Ctextbf%7BE%7Dgocentric%20human%0A%5Ctextbf%7BM%7Dotion%20dataset%20with%20%5Ctextbf%7BH%7Dead-Mounted%20Display%20%28HMD%29%20and%20body-worn%0A%5Ctextbf%7BI%7DMUs%2C%20with%20all%20data%20collected%20under%20the%20real%20VR%20product%20suite.%0ASpecifically%2C%20EMHI%20provides%20synchronized%20stereo%20images%20from%20downward-sloping%0Acameras%20on%20the%20headset%20and%20IMU%20data%20from%20body-worn%20sensors%2C%20along%20with%20pose%0Aannotations%20in%20SMPL%20format.%20This%20dataset%20consists%20of%20885%20sequences%20captured%20by%0A58%20subjects%20performing%2039%20actions%2C%20totaling%20about%2028.5%20hours%20of%20recording.%20We%0Aevaluate%20the%20annotations%20by%20comparing%20them%20with%20optical%20marker-based%20SMPL%0Afitting%20results.%20To%20substantiate%20the%20reliability%20of%20our%20dataset%2C%20we%20introduce%0AMEPoser%2C%20a%20new%20baseline%20method%20for%20multimodal%20egocentric%20HPE%2C%20which%20employs%20a%0Amultimodal%20fusion%20encoder%2C%20temporal%20feature%20encoder%2C%20and%20MLP-based%20regression%0Aheads.%20The%20experiments%20on%20EMHI%20show%20that%20MEPoser%20outperforms%20existing%0Asingle-modal%20methods%20and%20demonstrates%20the%20value%20of%20our%20dataset%20in%20solving%20the%0Aproblem%20of%20egocentric%20HPE.%20We%20believe%20the%20release%20of%20EMHI%20and%20the%20method%20could%0Aadvance%20the%20research%20of%20egocentric%20HPE%20and%20expedite%20the%20practical%0Aimplementation%20of%20this%20technology%20in%20VR/AR%20products.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17168v1&entry.124074799=Read"},
{"title": "Short-term Wind Speed Forecasting for Power Integration in Smart Grids\n  based on Hybrid LSSVM-SVMD Method", "author": "Ephrem Admasu Yekun and Alem H. Fitwib and Selvi Karpaga Subramaniand and Anubhav Kumard and Teshome Goa Tella", "abstract": "  Owing to its minimal pollution and efficient energy use, wind energy has\nbecome one of the most widely exploited renewable energy resources. The\nsuccessful integration of wind power into the grid system is contingent upon\naccurate wind speed forecasting models. However, the task of wind speed\nforecasting is challenging due to the inherent intermittent characteristics of\nwind speed. In this paper, a hybrid machine learning approach is developed for\npredicting short-term wind speed. First, the wind data was decomposed into\nmodal components using Successive Variational Mode Decomposition (SVMD). Then,\neach sub-signal was fitted into a Least Squares Support Vector Machines (LSSVM)\nmodel, with its hyperparameter optimized by a novel variant of Quantum-behaved\nParticle Swarm Optimization (QPSO), QPSO with elitist breeding (EBQPSO).\nSecond, the residuals making up for the differences between the original wind\nseries and the aggregate of the SVMD modes were modeled using long short-term\nmodel (LSTM). Then, the overall predicted values were computed using the\naggregate of the LSSVM and the LSTM models. Finally, the performance of the\nproposed model was compared against state-of-the-art benchmark models for\nforecasting wind speed using two separate data sets collected from a local wind\nfarm. Empirical results show significant improvement in performance by the\nproposed method, achieving a 1.21% to 32.76% reduction in root mean square\nerror (RMSE) and a 2.05% to 40.75% reduction in mean average error (MAE)\ncompared to the benchmark methods. The entire code implementation of this work\nis freely available in Github.\n", "link": "http://arxiv.org/abs/2408.17185v1", "date": "2024-08-30", "relevancy": 0.8568, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4345}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4277}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Short-term%20Wind%20Speed%20Forecasting%20for%20Power%20Integration%20in%20Smart%20Grids%0A%20%20based%20on%20Hybrid%20LSSVM-SVMD%20Method&body=Title%3A%20Short-term%20Wind%20Speed%20Forecasting%20for%20Power%20Integration%20in%20Smart%20Grids%0A%20%20based%20on%20Hybrid%20LSSVM-SVMD%20Method%0AAuthor%3A%20Ephrem%20Admasu%20Yekun%20and%20Alem%20H.%20Fitwib%20and%20Selvi%20Karpaga%20Subramaniand%20and%20Anubhav%20Kumard%20and%20Teshome%20Goa%20Tella%0AAbstract%3A%20%20%20Owing%20to%20its%20minimal%20pollution%20and%20efficient%20energy%20use%2C%20wind%20energy%20has%0Abecome%20one%20of%20the%20most%20widely%20exploited%20renewable%20energy%20resources.%20The%0Asuccessful%20integration%20of%20wind%20power%20into%20the%20grid%20system%20is%20contingent%20upon%0Aaccurate%20wind%20speed%20forecasting%20models.%20However%2C%20the%20task%20of%20wind%20speed%0Aforecasting%20is%20challenging%20due%20to%20the%20inherent%20intermittent%20characteristics%20of%0Awind%20speed.%20In%20this%20paper%2C%20a%20hybrid%20machine%20learning%20approach%20is%20developed%20for%0Apredicting%20short-term%20wind%20speed.%20First%2C%20the%20wind%20data%20was%20decomposed%20into%0Amodal%20components%20using%20Successive%20Variational%20Mode%20Decomposition%20%28SVMD%29.%20Then%2C%0Aeach%20sub-signal%20was%20fitted%20into%20a%20Least%20Squares%20Support%20Vector%20Machines%20%28LSSVM%29%0Amodel%2C%20with%20its%20hyperparameter%20optimized%20by%20a%20novel%20variant%20of%20Quantum-behaved%0AParticle%20Swarm%20Optimization%20%28QPSO%29%2C%20QPSO%20with%20elitist%20breeding%20%28EBQPSO%29.%0ASecond%2C%20the%20residuals%20making%20up%20for%20the%20differences%20between%20the%20original%20wind%0Aseries%20and%20the%20aggregate%20of%20the%20SVMD%20modes%20were%20modeled%20using%20long%20short-term%0Amodel%20%28LSTM%29.%20Then%2C%20the%20overall%20predicted%20values%20were%20computed%20using%20the%0Aaggregate%20of%20the%20LSSVM%20and%20the%20LSTM%20models.%20Finally%2C%20the%20performance%20of%20the%0Aproposed%20model%20was%20compared%20against%20state-of-the-art%20benchmark%20models%20for%0Aforecasting%20wind%20speed%20using%20two%20separate%20data%20sets%20collected%20from%20a%20local%20wind%0Afarm.%20Empirical%20results%20show%20significant%20improvement%20in%20performance%20by%20the%0Aproposed%20method%2C%20achieving%20a%201.21%25%20to%2032.76%25%20reduction%20in%20root%20mean%20square%0Aerror%20%28RMSE%29%20and%20a%202.05%25%20to%2040.75%25%20reduction%20in%20mean%20average%20error%20%28MAE%29%0Acompared%20to%20the%20benchmark%20methods.%20The%20entire%20code%20implementation%20of%20this%20work%0Ais%20freely%20available%20in%20Github.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShort-term%2520Wind%2520Speed%2520Forecasting%2520for%2520Power%2520Integration%2520in%2520Smart%2520Grids%250A%2520%2520based%2520on%2520Hybrid%2520LSSVM-SVMD%2520Method%26entry.906535625%3DEphrem%2520Admasu%2520Yekun%2520and%2520Alem%2520H.%2520Fitwib%2520and%2520Selvi%2520Karpaga%2520Subramaniand%2520and%2520Anubhav%2520Kumard%2520and%2520Teshome%2520Goa%2520Tella%26entry.1292438233%3D%2520%2520Owing%2520to%2520its%2520minimal%2520pollution%2520and%2520efficient%2520energy%2520use%252C%2520wind%2520energy%2520has%250Abecome%2520one%2520of%2520the%2520most%2520widely%2520exploited%2520renewable%2520energy%2520resources.%2520The%250Asuccessful%2520integration%2520of%2520wind%2520power%2520into%2520the%2520grid%2520system%2520is%2520contingent%2520upon%250Aaccurate%2520wind%2520speed%2520forecasting%2520models.%2520However%252C%2520the%2520task%2520of%2520wind%2520speed%250Aforecasting%2520is%2520challenging%2520due%2520to%2520the%2520inherent%2520intermittent%2520characteristics%2520of%250Awind%2520speed.%2520In%2520this%2520paper%252C%2520a%2520hybrid%2520machine%2520learning%2520approach%2520is%2520developed%2520for%250Apredicting%2520short-term%2520wind%2520speed.%2520First%252C%2520the%2520wind%2520data%2520was%2520decomposed%2520into%250Amodal%2520components%2520using%2520Successive%2520Variational%2520Mode%2520Decomposition%2520%2528SVMD%2529.%2520Then%252C%250Aeach%2520sub-signal%2520was%2520fitted%2520into%2520a%2520Least%2520Squares%2520Support%2520Vector%2520Machines%2520%2528LSSVM%2529%250Amodel%252C%2520with%2520its%2520hyperparameter%2520optimized%2520by%2520a%2520novel%2520variant%2520of%2520Quantum-behaved%250AParticle%2520Swarm%2520Optimization%2520%2528QPSO%2529%252C%2520QPSO%2520with%2520elitist%2520breeding%2520%2528EBQPSO%2529.%250ASecond%252C%2520the%2520residuals%2520making%2520up%2520for%2520the%2520differences%2520between%2520the%2520original%2520wind%250Aseries%2520and%2520the%2520aggregate%2520of%2520the%2520SVMD%2520modes%2520were%2520modeled%2520using%2520long%2520short-term%250Amodel%2520%2528LSTM%2529.%2520Then%252C%2520the%2520overall%2520predicted%2520values%2520were%2520computed%2520using%2520the%250Aaggregate%2520of%2520the%2520LSSVM%2520and%2520the%2520LSTM%2520models.%2520Finally%252C%2520the%2520performance%2520of%2520the%250Aproposed%2520model%2520was%2520compared%2520against%2520state-of-the-art%2520benchmark%2520models%2520for%250Aforecasting%2520wind%2520speed%2520using%2520two%2520separate%2520data%2520sets%2520collected%2520from%2520a%2520local%2520wind%250Afarm.%2520Empirical%2520results%2520show%2520significant%2520improvement%2520in%2520performance%2520by%2520the%250Aproposed%2520method%252C%2520achieving%2520a%25201.21%2525%2520to%252032.76%2525%2520reduction%2520in%2520root%2520mean%2520square%250Aerror%2520%2528RMSE%2529%2520and%2520a%25202.05%2525%2520to%252040.75%2525%2520reduction%2520in%2520mean%2520average%2520error%2520%2528MAE%2529%250Acompared%2520to%2520the%2520benchmark%2520methods.%2520The%2520entire%2520code%2520implementation%2520of%2520this%2520work%250Ais%2520freely%2520available%2520in%2520Github.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Short-term%20Wind%20Speed%20Forecasting%20for%20Power%20Integration%20in%20Smart%20Grids%0A%20%20based%20on%20Hybrid%20LSSVM-SVMD%20Method&entry.906535625=Ephrem%20Admasu%20Yekun%20and%20Alem%20H.%20Fitwib%20and%20Selvi%20Karpaga%20Subramaniand%20and%20Anubhav%20Kumard%20and%20Teshome%20Goa%20Tella&entry.1292438233=%20%20Owing%20to%20its%20minimal%20pollution%20and%20efficient%20energy%20use%2C%20wind%20energy%20has%0Abecome%20one%20of%20the%20most%20widely%20exploited%20renewable%20energy%20resources.%20The%0Asuccessful%20integration%20of%20wind%20power%20into%20the%20grid%20system%20is%20contingent%20upon%0Aaccurate%20wind%20speed%20forecasting%20models.%20However%2C%20the%20task%20of%20wind%20speed%0Aforecasting%20is%20challenging%20due%20to%20the%20inherent%20intermittent%20characteristics%20of%0Awind%20speed.%20In%20this%20paper%2C%20a%20hybrid%20machine%20learning%20approach%20is%20developed%20for%0Apredicting%20short-term%20wind%20speed.%20First%2C%20the%20wind%20data%20was%20decomposed%20into%0Amodal%20components%20using%20Successive%20Variational%20Mode%20Decomposition%20%28SVMD%29.%20Then%2C%0Aeach%20sub-signal%20was%20fitted%20into%20a%20Least%20Squares%20Support%20Vector%20Machines%20%28LSSVM%29%0Amodel%2C%20with%20its%20hyperparameter%20optimized%20by%20a%20novel%20variant%20of%20Quantum-behaved%0AParticle%20Swarm%20Optimization%20%28QPSO%29%2C%20QPSO%20with%20elitist%20breeding%20%28EBQPSO%29.%0ASecond%2C%20the%20residuals%20making%20up%20for%20the%20differences%20between%20the%20original%20wind%0Aseries%20and%20the%20aggregate%20of%20the%20SVMD%20modes%20were%20modeled%20using%20long%20short-term%0Amodel%20%28LSTM%29.%20Then%2C%20the%20overall%20predicted%20values%20were%20computed%20using%20the%0Aaggregate%20of%20the%20LSSVM%20and%20the%20LSTM%20models.%20Finally%2C%20the%20performance%20of%20the%0Aproposed%20model%20was%20compared%20against%20state-of-the-art%20benchmark%20models%20for%0Aforecasting%20wind%20speed%20using%20two%20separate%20data%20sets%20collected%20from%20a%20local%20wind%0Afarm.%20Empirical%20results%20show%20significant%20improvement%20in%20performance%20by%20the%0Aproposed%20method%2C%20achieving%20a%201.21%25%20to%2032.76%25%20reduction%20in%20root%20mean%20square%0Aerror%20%28RMSE%29%20and%20a%202.05%25%20to%2040.75%25%20reduction%20in%20mean%20average%20error%20%28MAE%29%0Acompared%20to%20the%20benchmark%20methods.%20The%20entire%20code%20implementation%20of%20this%20work%0Ais%20freely%20available%20in%20Github.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17185v1&entry.124074799=Read"},
{"title": "A methodological framework for Resilience as a Service (RaaS) in\n  multimodal urban transportation networks", "author": "Sara Jaber and Mostafa Ameli and S. M. Hassan Mahdavi and Neila Bhouri", "abstract": "  Public transportation systems are experiencing an increase in commuter\ntraffic. This increase underscores the need for resilience strategies to manage\nunexpected service disruptions, ensuring rapid and effective responses that\nminimize adverse effects on stakeholders and enhance the system's ability to\nmaintain essential functions and recover quickly. This study aims to explore\nthe management of public transport disruptions through resilience as a service\n(RaaS) strategies, developing an optimization model to effectively allocate\nresources and minimize the cost for operators and passengers. The proposed\nmodel includes multiple transportation options, such as buses, taxis, and\nautomated vans, and evaluates them as bridging alternatives to rail-disrupted\nservices based on factors such as their availability, capacity, speed, and\nproximity to the disrupted station. This ensures that the most suitable\nvehicles are deployed to maintain service continuity. Applied to a case study\nin the Ile de France region, Paris and suburbs, complemented by a microscopic\nsimulation, the model is compared to existing solutions such as bus bridging\nand reserve fleets. The results highlight the model's performance in minimizing\ncosts and enhancing stakeholder satisfaction, optimizing transport management\nduring disruptions.\n", "link": "http://arxiv.org/abs/2408.17233v1", "date": "2024-08-30", "relevancy": 1.2031, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4117}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4037}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20methodological%20framework%20for%20Resilience%20as%20a%20Service%20%28RaaS%29%20in%0A%20%20multimodal%20urban%20transportation%20networks&body=Title%3A%20A%20methodological%20framework%20for%20Resilience%20as%20a%20Service%20%28RaaS%29%20in%0A%20%20multimodal%20urban%20transportation%20networks%0AAuthor%3A%20Sara%20Jaber%20and%20Mostafa%20Ameli%20and%20S.%20M.%20Hassan%20Mahdavi%20and%20Neila%20Bhouri%0AAbstract%3A%20%20%20Public%20transportation%20systems%20are%20experiencing%20an%20increase%20in%20commuter%0Atraffic.%20This%20increase%20underscores%20the%20need%20for%20resilience%20strategies%20to%20manage%0Aunexpected%20service%20disruptions%2C%20ensuring%20rapid%20and%20effective%20responses%20that%0Aminimize%20adverse%20effects%20on%20stakeholders%20and%20enhance%20the%20system%27s%20ability%20to%0Amaintain%20essential%20functions%20and%20recover%20quickly.%20This%20study%20aims%20to%20explore%0Athe%20management%20of%20public%20transport%20disruptions%20through%20resilience%20as%20a%20service%0A%28RaaS%29%20strategies%2C%20developing%20an%20optimization%20model%20to%20effectively%20allocate%0Aresources%20and%20minimize%20the%20cost%20for%20operators%20and%20passengers.%20The%20proposed%0Amodel%20includes%20multiple%20transportation%20options%2C%20such%20as%20buses%2C%20taxis%2C%20and%0Aautomated%20vans%2C%20and%20evaluates%20them%20as%20bridging%20alternatives%20to%20rail-disrupted%0Aservices%20based%20on%20factors%20such%20as%20their%20availability%2C%20capacity%2C%20speed%2C%20and%0Aproximity%20to%20the%20disrupted%20station.%20This%20ensures%20that%20the%20most%20suitable%0Avehicles%20are%20deployed%20to%20maintain%20service%20continuity.%20Applied%20to%20a%20case%20study%0Ain%20the%20Ile%20de%20France%20region%2C%20Paris%20and%20suburbs%2C%20complemented%20by%20a%20microscopic%0Asimulation%2C%20the%20model%20is%20compared%20to%20existing%20solutions%20such%20as%20bus%20bridging%0Aand%20reserve%20fleets.%20The%20results%20highlight%20the%20model%27s%20performance%20in%20minimizing%0Acosts%20and%20enhancing%20stakeholder%20satisfaction%2C%20optimizing%20transport%20management%0Aduring%20disruptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520methodological%2520framework%2520for%2520Resilience%2520as%2520a%2520Service%2520%2528RaaS%2529%2520in%250A%2520%2520multimodal%2520urban%2520transportation%2520networks%26entry.906535625%3DSara%2520Jaber%2520and%2520Mostafa%2520Ameli%2520and%2520S.%2520M.%2520Hassan%2520Mahdavi%2520and%2520Neila%2520Bhouri%26entry.1292438233%3D%2520%2520Public%2520transportation%2520systems%2520are%2520experiencing%2520an%2520increase%2520in%2520commuter%250Atraffic.%2520This%2520increase%2520underscores%2520the%2520need%2520for%2520resilience%2520strategies%2520to%2520manage%250Aunexpected%2520service%2520disruptions%252C%2520ensuring%2520rapid%2520and%2520effective%2520responses%2520that%250Aminimize%2520adverse%2520effects%2520on%2520stakeholders%2520and%2520enhance%2520the%2520system%2527s%2520ability%2520to%250Amaintain%2520essential%2520functions%2520and%2520recover%2520quickly.%2520This%2520study%2520aims%2520to%2520explore%250Athe%2520management%2520of%2520public%2520transport%2520disruptions%2520through%2520resilience%2520as%2520a%2520service%250A%2528RaaS%2529%2520strategies%252C%2520developing%2520an%2520optimization%2520model%2520to%2520effectively%2520allocate%250Aresources%2520and%2520minimize%2520the%2520cost%2520for%2520operators%2520and%2520passengers.%2520The%2520proposed%250Amodel%2520includes%2520multiple%2520transportation%2520options%252C%2520such%2520as%2520buses%252C%2520taxis%252C%2520and%250Aautomated%2520vans%252C%2520and%2520evaluates%2520them%2520as%2520bridging%2520alternatives%2520to%2520rail-disrupted%250Aservices%2520based%2520on%2520factors%2520such%2520as%2520their%2520availability%252C%2520capacity%252C%2520speed%252C%2520and%250Aproximity%2520to%2520the%2520disrupted%2520station.%2520This%2520ensures%2520that%2520the%2520most%2520suitable%250Avehicles%2520are%2520deployed%2520to%2520maintain%2520service%2520continuity.%2520Applied%2520to%2520a%2520case%2520study%250Ain%2520the%2520Ile%2520de%2520France%2520region%252C%2520Paris%2520and%2520suburbs%252C%2520complemented%2520by%2520a%2520microscopic%250Asimulation%252C%2520the%2520model%2520is%2520compared%2520to%2520existing%2520solutions%2520such%2520as%2520bus%2520bridging%250Aand%2520reserve%2520fleets.%2520The%2520results%2520highlight%2520the%2520model%2527s%2520performance%2520in%2520minimizing%250Acosts%2520and%2520enhancing%2520stakeholder%2520satisfaction%252C%2520optimizing%2520transport%2520management%250Aduring%2520disruptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20methodological%20framework%20for%20Resilience%20as%20a%20Service%20%28RaaS%29%20in%0A%20%20multimodal%20urban%20transportation%20networks&entry.906535625=Sara%20Jaber%20and%20Mostafa%20Ameli%20and%20S.%20M.%20Hassan%20Mahdavi%20and%20Neila%20Bhouri&entry.1292438233=%20%20Public%20transportation%20systems%20are%20experiencing%20an%20increase%20in%20commuter%0Atraffic.%20This%20increase%20underscores%20the%20need%20for%20resilience%20strategies%20to%20manage%0Aunexpected%20service%20disruptions%2C%20ensuring%20rapid%20and%20effective%20responses%20that%0Aminimize%20adverse%20effects%20on%20stakeholders%20and%20enhance%20the%20system%27s%20ability%20to%0Amaintain%20essential%20functions%20and%20recover%20quickly.%20This%20study%20aims%20to%20explore%0Athe%20management%20of%20public%20transport%20disruptions%20through%20resilience%20as%20a%20service%0A%28RaaS%29%20strategies%2C%20developing%20an%20optimization%20model%20to%20effectively%20allocate%0Aresources%20and%20minimize%20the%20cost%20for%20operators%20and%20passengers.%20The%20proposed%0Amodel%20includes%20multiple%20transportation%20options%2C%20such%20as%20buses%2C%20taxis%2C%20and%0Aautomated%20vans%2C%20and%20evaluates%20them%20as%20bridging%20alternatives%20to%20rail-disrupted%0Aservices%20based%20on%20factors%20such%20as%20their%20availability%2C%20capacity%2C%20speed%2C%20and%0Aproximity%20to%20the%20disrupted%20station.%20This%20ensures%20that%20the%20most%20suitable%0Avehicles%20are%20deployed%20to%20maintain%20service%20continuity.%20Applied%20to%20a%20case%20study%0Ain%20the%20Ile%20de%20France%20region%2C%20Paris%20and%20suburbs%2C%20complemented%20by%20a%20microscopic%0Asimulation%2C%20the%20model%20is%20compared%20to%20existing%20solutions%20such%20as%20bus%20bridging%0Aand%20reserve%20fleets.%20The%20results%20highlight%20the%20model%27s%20performance%20in%20minimizing%0Acosts%20and%20enhancing%20stakeholder%20satisfaction%2C%20optimizing%20transport%20management%0Aduring%20disruptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17233v1&entry.124074799=Read"},
{"title": "A Deep-Learning Technique to Locate Cryptographic Operations in\n  Side-Channel Traces", "author": "Giuseppe Chiari and Davide Galli and Francesco Lattari and Matteo Matteucci and Davide Zoni", "abstract": "  Side-channel attacks allow extracting secret information from the execution\nof cryptographic primitives by correlating the partially known computed data\nand the measured side-channel signal. However, to set up a successful\nside-channel attack, the attacker has to perform i) the challenging task of\nlocating the time instant in which the target cryptographic primitive is\nexecuted inside a side-channel trace and then ii)the time-alignment of the\nmeasured data on that time instant. This paper presents a novel deep-learning\ntechnique to locate the time instant in which the target computed cryptographic\noperations are executed in the side-channel trace. In contrast to\nstate-of-the-art solutions, the proposed methodology works even in the presence\nof trace deformations obtained through random delay insertion techniques. We\nvalidated our proposal through a successful attack against a variety of\nunprotected and protected cryptographic primitives that have been executed on\nan FPGA-implemented system-on-chip featuring a RISC-V CPU.\n", "link": "http://arxiv.org/abs/2402.19037v2", "date": "2024-08-30", "relevancy": 1.6225, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4368}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4039}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.3948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Deep-Learning%20Technique%20to%20Locate%20Cryptographic%20Operations%20in%0A%20%20Side-Channel%20Traces&body=Title%3A%20A%20Deep-Learning%20Technique%20to%20Locate%20Cryptographic%20Operations%20in%0A%20%20Side-Channel%20Traces%0AAuthor%3A%20Giuseppe%20Chiari%20and%20Davide%20Galli%20and%20Francesco%20Lattari%20and%20Matteo%20Matteucci%20and%20Davide%20Zoni%0AAbstract%3A%20%20%20Side-channel%20attacks%20allow%20extracting%20secret%20information%20from%20the%20execution%0Aof%20cryptographic%20primitives%20by%20correlating%20the%20partially%20known%20computed%20data%0Aand%20the%20measured%20side-channel%20signal.%20However%2C%20to%20set%20up%20a%20successful%0Aside-channel%20attack%2C%20the%20attacker%20has%20to%20perform%20i%29%20the%20challenging%20task%20of%0Alocating%20the%20time%20instant%20in%20which%20the%20target%20cryptographic%20primitive%20is%0Aexecuted%20inside%20a%20side-channel%20trace%20and%20then%20ii%29the%20time-alignment%20of%20the%0Ameasured%20data%20on%20that%20time%20instant.%20This%20paper%20presents%20a%20novel%20deep-learning%0Atechnique%20to%20locate%20the%20time%20instant%20in%20which%20the%20target%20computed%20cryptographic%0Aoperations%20are%20executed%20in%20the%20side-channel%20trace.%20In%20contrast%20to%0Astate-of-the-art%20solutions%2C%20the%20proposed%20methodology%20works%20even%20in%20the%20presence%0Aof%20trace%20deformations%20obtained%20through%20random%20delay%20insertion%20techniques.%20We%0Avalidated%20our%20proposal%20through%20a%20successful%20attack%20against%20a%20variety%20of%0Aunprotected%20and%20protected%20cryptographic%20primitives%20that%20have%20been%20executed%20on%0Aan%20FPGA-implemented%20system-on-chip%20featuring%20a%20RISC-V%20CPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19037v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Deep-Learning%2520Technique%2520to%2520Locate%2520Cryptographic%2520Operations%2520in%250A%2520%2520Side-Channel%2520Traces%26entry.906535625%3DGiuseppe%2520Chiari%2520and%2520Davide%2520Galli%2520and%2520Francesco%2520Lattari%2520and%2520Matteo%2520Matteucci%2520and%2520Davide%2520Zoni%26entry.1292438233%3D%2520%2520Side-channel%2520attacks%2520allow%2520extracting%2520secret%2520information%2520from%2520the%2520execution%250Aof%2520cryptographic%2520primitives%2520by%2520correlating%2520the%2520partially%2520known%2520computed%2520data%250Aand%2520the%2520measured%2520side-channel%2520signal.%2520However%252C%2520to%2520set%2520up%2520a%2520successful%250Aside-channel%2520attack%252C%2520the%2520attacker%2520has%2520to%2520perform%2520i%2529%2520the%2520challenging%2520task%2520of%250Alocating%2520the%2520time%2520instant%2520in%2520which%2520the%2520target%2520cryptographic%2520primitive%2520is%250Aexecuted%2520inside%2520a%2520side-channel%2520trace%2520and%2520then%2520ii%2529the%2520time-alignment%2520of%2520the%250Ameasured%2520data%2520on%2520that%2520time%2520instant.%2520This%2520paper%2520presents%2520a%2520novel%2520deep-learning%250Atechnique%2520to%2520locate%2520the%2520time%2520instant%2520in%2520which%2520the%2520target%2520computed%2520cryptographic%250Aoperations%2520are%2520executed%2520in%2520the%2520side-channel%2520trace.%2520In%2520contrast%2520to%250Astate-of-the-art%2520solutions%252C%2520the%2520proposed%2520methodology%2520works%2520even%2520in%2520the%2520presence%250Aof%2520trace%2520deformations%2520obtained%2520through%2520random%2520delay%2520insertion%2520techniques.%2520We%250Avalidated%2520our%2520proposal%2520through%2520a%2520successful%2520attack%2520against%2520a%2520variety%2520of%250Aunprotected%2520and%2520protected%2520cryptographic%2520primitives%2520that%2520have%2520been%2520executed%2520on%250Aan%2520FPGA-implemented%2520system-on-chip%2520featuring%2520a%2520RISC-V%2520CPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19037v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Deep-Learning%20Technique%20to%20Locate%20Cryptographic%20Operations%20in%0A%20%20Side-Channel%20Traces&entry.906535625=Giuseppe%20Chiari%20and%20Davide%20Galli%20and%20Francesco%20Lattari%20and%20Matteo%20Matteucci%20and%20Davide%20Zoni&entry.1292438233=%20%20Side-channel%20attacks%20allow%20extracting%20secret%20information%20from%20the%20execution%0Aof%20cryptographic%20primitives%20by%20correlating%20the%20partially%20known%20computed%20data%0Aand%20the%20measured%20side-channel%20signal.%20However%2C%20to%20set%20up%20a%20successful%0Aside-channel%20attack%2C%20the%20attacker%20has%20to%20perform%20i%29%20the%20challenging%20task%20of%0Alocating%20the%20time%20instant%20in%20which%20the%20target%20cryptographic%20primitive%20is%0Aexecuted%20inside%20a%20side-channel%20trace%20and%20then%20ii%29the%20time-alignment%20of%20the%0Ameasured%20data%20on%20that%20time%20instant.%20This%20paper%20presents%20a%20novel%20deep-learning%0Atechnique%20to%20locate%20the%20time%20instant%20in%20which%20the%20target%20computed%20cryptographic%0Aoperations%20are%20executed%20in%20the%20side-channel%20trace.%20In%20contrast%20to%0Astate-of-the-art%20solutions%2C%20the%20proposed%20methodology%20works%20even%20in%20the%20presence%0Aof%20trace%20deformations%20obtained%20through%20random%20delay%20insertion%20techniques.%20We%0Avalidated%20our%20proposal%20through%20a%20successful%20attack%20against%20a%20variety%20of%0Aunprotected%20and%20protected%20cryptographic%20primitives%20that%20have%20been%20executed%20on%0Aan%20FPGA-implemented%20system-on-chip%20featuring%20a%20RISC-V%20CPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19037v2&entry.124074799=Read"},
{"title": "Equation identification for fluid flows via physics-informed neural\n  networks", "author": "Alexander New and Marisel Villafa\u00f1e-Delgado and Charles Shugert", "abstract": "  Scientific machine learning (SciML) methods such as physics-informed neural\nnetworks (PINNs) are used to estimate parameters of interest from governing\nequations and small quantities of data. However, there has been little work in\nassessing how well PINNs perform for inverse problems across wide ranges of\ngoverning equations across the mathematical sciences. We present a new and\nchallenging benchmark problem for inverse PINNs based on a parametric sweep of\nthe 2D Burgers' equation with rotational flow. We show that a novel strategy\nthat alternates between first- and second-order optimization proves superior to\ntypical first-order strategies for estimating parameters. In addition, we\npropose a novel data-driven method to characterize PINN effectiveness in the\ninverse setting. PINNs' physics-informed regularization enables them to\nleverage small quantities of data more efficiently than the data-driven\nbaseline. However, both PINNs and the baseline can fail to recover parameters\nfor highly inviscid flows, motivating the need for further development of PINN\nmethods.\n", "link": "http://arxiv.org/abs/2408.17271v1", "date": "2024-08-30", "relevancy": 1.424, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4844}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4798}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equation%20identification%20for%20fluid%20flows%20via%20physics-informed%20neural%0A%20%20networks&body=Title%3A%20Equation%20identification%20for%20fluid%20flows%20via%20physics-informed%20neural%0A%20%20networks%0AAuthor%3A%20Alexander%20New%20and%20Marisel%20Villafa%C3%B1e-Delgado%20and%20Charles%20Shugert%0AAbstract%3A%20%20%20Scientific%20machine%20learning%20%28SciML%29%20methods%20such%20as%20physics-informed%20neural%0Anetworks%20%28PINNs%29%20are%20used%20to%20estimate%20parameters%20of%20interest%20from%20governing%0Aequations%20and%20small%20quantities%20of%20data.%20However%2C%20there%20has%20been%20little%20work%20in%0Aassessing%20how%20well%20PINNs%20perform%20for%20inverse%20problems%20across%20wide%20ranges%20of%0Agoverning%20equations%20across%20the%20mathematical%20sciences.%20We%20present%20a%20new%20and%0Achallenging%20benchmark%20problem%20for%20inverse%20PINNs%20based%20on%20a%20parametric%20sweep%20of%0Athe%202D%20Burgers%27%20equation%20with%20rotational%20flow.%20We%20show%20that%20a%20novel%20strategy%0Athat%20alternates%20between%20first-%20and%20second-order%20optimization%20proves%20superior%20to%0Atypical%20first-order%20strategies%20for%20estimating%20parameters.%20In%20addition%2C%20we%0Apropose%20a%20novel%20data-driven%20method%20to%20characterize%20PINN%20effectiveness%20in%20the%0Ainverse%20setting.%20PINNs%27%20physics-informed%20regularization%20enables%20them%20to%0Aleverage%20small%20quantities%20of%20data%20more%20efficiently%20than%20the%20data-driven%0Abaseline.%20However%2C%20both%20PINNs%20and%20the%20baseline%20can%20fail%20to%20recover%20parameters%0Afor%20highly%20inviscid%20flows%2C%20motivating%20the%20need%20for%20further%20development%20of%20PINN%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquation%2520identification%2520for%2520fluid%2520flows%2520via%2520physics-informed%2520neural%250A%2520%2520networks%26entry.906535625%3DAlexander%2520New%2520and%2520Marisel%2520Villafa%25C3%25B1e-Delgado%2520and%2520Charles%2520Shugert%26entry.1292438233%3D%2520%2520Scientific%2520machine%2520learning%2520%2528SciML%2529%2520methods%2520such%2520as%2520physics-informed%2520neural%250Anetworks%2520%2528PINNs%2529%2520are%2520used%2520to%2520estimate%2520parameters%2520of%2520interest%2520from%2520governing%250Aequations%2520and%2520small%2520quantities%2520of%2520data.%2520However%252C%2520there%2520has%2520been%2520little%2520work%2520in%250Aassessing%2520how%2520well%2520PINNs%2520perform%2520for%2520inverse%2520problems%2520across%2520wide%2520ranges%2520of%250Agoverning%2520equations%2520across%2520the%2520mathematical%2520sciences.%2520We%2520present%2520a%2520new%2520and%250Achallenging%2520benchmark%2520problem%2520for%2520inverse%2520PINNs%2520based%2520on%2520a%2520parametric%2520sweep%2520of%250Athe%25202D%2520Burgers%2527%2520equation%2520with%2520rotational%2520flow.%2520We%2520show%2520that%2520a%2520novel%2520strategy%250Athat%2520alternates%2520between%2520first-%2520and%2520second-order%2520optimization%2520proves%2520superior%2520to%250Atypical%2520first-order%2520strategies%2520for%2520estimating%2520parameters.%2520In%2520addition%252C%2520we%250Apropose%2520a%2520novel%2520data-driven%2520method%2520to%2520characterize%2520PINN%2520effectiveness%2520in%2520the%250Ainverse%2520setting.%2520PINNs%2527%2520physics-informed%2520regularization%2520enables%2520them%2520to%250Aleverage%2520small%2520quantities%2520of%2520data%2520more%2520efficiently%2520than%2520the%2520data-driven%250Abaseline.%2520However%252C%2520both%2520PINNs%2520and%2520the%2520baseline%2520can%2520fail%2520to%2520recover%2520parameters%250Afor%2520highly%2520inviscid%2520flows%252C%2520motivating%2520the%2520need%2520for%2520further%2520development%2520of%2520PINN%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equation%20identification%20for%20fluid%20flows%20via%20physics-informed%20neural%0A%20%20networks&entry.906535625=Alexander%20New%20and%20Marisel%20Villafa%C3%B1e-Delgado%20and%20Charles%20Shugert&entry.1292438233=%20%20Scientific%20machine%20learning%20%28SciML%29%20methods%20such%20as%20physics-informed%20neural%0Anetworks%20%28PINNs%29%20are%20used%20to%20estimate%20parameters%20of%20interest%20from%20governing%0Aequations%20and%20small%20quantities%20of%20data.%20However%2C%20there%20has%20been%20little%20work%20in%0Aassessing%20how%20well%20PINNs%20perform%20for%20inverse%20problems%20across%20wide%20ranges%20of%0Agoverning%20equations%20across%20the%20mathematical%20sciences.%20We%20present%20a%20new%20and%0Achallenging%20benchmark%20problem%20for%20inverse%20PINNs%20based%20on%20a%20parametric%20sweep%20of%0Athe%202D%20Burgers%27%20equation%20with%20rotational%20flow.%20We%20show%20that%20a%20novel%20strategy%0Athat%20alternates%20between%20first-%20and%20second-order%20optimization%20proves%20superior%20to%0Atypical%20first-order%20strategies%20for%20estimating%20parameters.%20In%20addition%2C%20we%0Apropose%20a%20novel%20data-driven%20method%20to%20characterize%20PINN%20effectiveness%20in%20the%0Ainverse%20setting.%20PINNs%27%20physics-informed%20regularization%20enables%20them%20to%0Aleverage%20small%20quantities%20of%20data%20more%20efficiently%20than%20the%20data-driven%0Abaseline.%20However%2C%20both%20PINNs%20and%20the%20baseline%20can%20fail%20to%20recover%20parameters%0Afor%20highly%20inviscid%20flows%2C%20motivating%20the%20need%20for%20further%20development%20of%20PINN%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17271v1&entry.124074799=Read"},
{"title": "CondSeg: Ellipse Estimation of Pupil and Iris via Conditioned\n  Segmentation", "author": "Zhuang Jia and Jiangfan Deng and Liying Chi and Xiang Long and Daniel K. Du", "abstract": "  Parsing of eye components (i.e. pupil, iris and sclera) is fundamental for\neye tracking and gaze estimation for AR/VR products. Mainstream approaches\ntackle this problem as a multi-class segmentation task, providing only visible\npart of pupil/iris, other methods regress elliptical parameters using\nhuman-annotated full pupil/iris parameters. In this paper, we consider two\npriors: projected full pupil/iris circle can be modelled with ellipses (ellipse\nprior), and the visibility of pupil/iris is controlled by openness of\neye-region (condition prior), and design a novel method CondSeg to estimate\nelliptical parameters of pupil/iris directly from segmentation labels, without\nexplicitly annotating full ellipses, and use eye-region mask to control the\nvisibility of estimated pupil/iris ellipses. Conditioned segmentation loss is\nused to optimize the parameters by transforming parameterized ellipses into\npixel-wise soft masks in a differentiable way. Our method is tested on public\ndatasets (OpenEDS-2019/-2020) and shows competitive results on segmentation\nmetrics, and provides accurate elliptical parameters for further applications\nof eye tracking simultaneously.\n", "link": "http://arxiv.org/abs/2408.17231v1", "date": "2024-08-30", "relevancy": 0.9893, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4989}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4944}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CondSeg%3A%20Ellipse%20Estimation%20of%20Pupil%20and%20Iris%20via%20Conditioned%0A%20%20Segmentation&body=Title%3A%20CondSeg%3A%20Ellipse%20Estimation%20of%20Pupil%20and%20Iris%20via%20Conditioned%0A%20%20Segmentation%0AAuthor%3A%20Zhuang%20Jia%20and%20Jiangfan%20Deng%20and%20Liying%20Chi%20and%20Xiang%20Long%20and%20Daniel%20K.%20Du%0AAbstract%3A%20%20%20Parsing%20of%20eye%20components%20%28i.e.%20pupil%2C%20iris%20and%20sclera%29%20is%20fundamental%20for%0Aeye%20tracking%20and%20gaze%20estimation%20for%20AR/VR%20products.%20Mainstream%20approaches%0Atackle%20this%20problem%20as%20a%20multi-class%20segmentation%20task%2C%20providing%20only%20visible%0Apart%20of%20pupil/iris%2C%20other%20methods%20regress%20elliptical%20parameters%20using%0Ahuman-annotated%20full%20pupil/iris%20parameters.%20In%20this%20paper%2C%20we%20consider%20two%0Apriors%3A%20projected%20full%20pupil/iris%20circle%20can%20be%20modelled%20with%20ellipses%20%28ellipse%0Aprior%29%2C%20and%20the%20visibility%20of%20pupil/iris%20is%20controlled%20by%20openness%20of%0Aeye-region%20%28condition%20prior%29%2C%20and%20design%20a%20novel%20method%20CondSeg%20to%20estimate%0Aelliptical%20parameters%20of%20pupil/iris%20directly%20from%20segmentation%20labels%2C%20without%0Aexplicitly%20annotating%20full%20ellipses%2C%20and%20use%20eye-region%20mask%20to%20control%20the%0Avisibility%20of%20estimated%20pupil/iris%20ellipses.%20Conditioned%20segmentation%20loss%20is%0Aused%20to%20optimize%20the%20parameters%20by%20transforming%20parameterized%20ellipses%20into%0Apixel-wise%20soft%20masks%20in%20a%20differentiable%20way.%20Our%20method%20is%20tested%20on%20public%0Adatasets%20%28OpenEDS-2019/-2020%29%20and%20shows%20competitive%20results%20on%20segmentation%0Ametrics%2C%20and%20provides%20accurate%20elliptical%20parameters%20for%20further%20applications%0Aof%20eye%20tracking%20simultaneously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCondSeg%253A%2520Ellipse%2520Estimation%2520of%2520Pupil%2520and%2520Iris%2520via%2520Conditioned%250A%2520%2520Segmentation%26entry.906535625%3DZhuang%2520Jia%2520and%2520Jiangfan%2520Deng%2520and%2520Liying%2520Chi%2520and%2520Xiang%2520Long%2520and%2520Daniel%2520K.%2520Du%26entry.1292438233%3D%2520%2520Parsing%2520of%2520eye%2520components%2520%2528i.e.%2520pupil%252C%2520iris%2520and%2520sclera%2529%2520is%2520fundamental%2520for%250Aeye%2520tracking%2520and%2520gaze%2520estimation%2520for%2520AR/VR%2520products.%2520Mainstream%2520approaches%250Atackle%2520this%2520problem%2520as%2520a%2520multi-class%2520segmentation%2520task%252C%2520providing%2520only%2520visible%250Apart%2520of%2520pupil/iris%252C%2520other%2520methods%2520regress%2520elliptical%2520parameters%2520using%250Ahuman-annotated%2520full%2520pupil/iris%2520parameters.%2520In%2520this%2520paper%252C%2520we%2520consider%2520two%250Apriors%253A%2520projected%2520full%2520pupil/iris%2520circle%2520can%2520be%2520modelled%2520with%2520ellipses%2520%2528ellipse%250Aprior%2529%252C%2520and%2520the%2520visibility%2520of%2520pupil/iris%2520is%2520controlled%2520by%2520openness%2520of%250Aeye-region%2520%2528condition%2520prior%2529%252C%2520and%2520design%2520a%2520novel%2520method%2520CondSeg%2520to%2520estimate%250Aelliptical%2520parameters%2520of%2520pupil/iris%2520directly%2520from%2520segmentation%2520labels%252C%2520without%250Aexplicitly%2520annotating%2520full%2520ellipses%252C%2520and%2520use%2520eye-region%2520mask%2520to%2520control%2520the%250Avisibility%2520of%2520estimated%2520pupil/iris%2520ellipses.%2520Conditioned%2520segmentation%2520loss%2520is%250Aused%2520to%2520optimize%2520the%2520parameters%2520by%2520transforming%2520parameterized%2520ellipses%2520into%250Apixel-wise%2520soft%2520masks%2520in%2520a%2520differentiable%2520way.%2520Our%2520method%2520is%2520tested%2520on%2520public%250Adatasets%2520%2528OpenEDS-2019/-2020%2529%2520and%2520shows%2520competitive%2520results%2520on%2520segmentation%250Ametrics%252C%2520and%2520provides%2520accurate%2520elliptical%2520parameters%2520for%2520further%2520applications%250Aof%2520eye%2520tracking%2520simultaneously.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CondSeg%3A%20Ellipse%20Estimation%20of%20Pupil%20and%20Iris%20via%20Conditioned%0A%20%20Segmentation&entry.906535625=Zhuang%20Jia%20and%20Jiangfan%20Deng%20and%20Liying%20Chi%20and%20Xiang%20Long%20and%20Daniel%20K.%20Du&entry.1292438233=%20%20Parsing%20of%20eye%20components%20%28i.e.%20pupil%2C%20iris%20and%20sclera%29%20is%20fundamental%20for%0Aeye%20tracking%20and%20gaze%20estimation%20for%20AR/VR%20products.%20Mainstream%20approaches%0Atackle%20this%20problem%20as%20a%20multi-class%20segmentation%20task%2C%20providing%20only%20visible%0Apart%20of%20pupil/iris%2C%20other%20methods%20regress%20elliptical%20parameters%20using%0Ahuman-annotated%20full%20pupil/iris%20parameters.%20In%20this%20paper%2C%20we%20consider%20two%0Apriors%3A%20projected%20full%20pupil/iris%20circle%20can%20be%20modelled%20with%20ellipses%20%28ellipse%0Aprior%29%2C%20and%20the%20visibility%20of%20pupil/iris%20is%20controlled%20by%20openness%20of%0Aeye-region%20%28condition%20prior%29%2C%20and%20design%20a%20novel%20method%20CondSeg%20to%20estimate%0Aelliptical%20parameters%20of%20pupil/iris%20directly%20from%20segmentation%20labels%2C%20without%0Aexplicitly%20annotating%20full%20ellipses%2C%20and%20use%20eye-region%20mask%20to%20control%20the%0Avisibility%20of%20estimated%20pupil/iris%20ellipses.%20Conditioned%20segmentation%20loss%20is%0Aused%20to%20optimize%20the%20parameters%20by%20transforming%20parameterized%20ellipses%20into%0Apixel-wise%20soft%20masks%20in%20a%20differentiable%20way.%20Our%20method%20is%20tested%20on%20public%0Adatasets%20%28OpenEDS-2019/-2020%29%20and%20shows%20competitive%20results%20on%20segmentation%0Ametrics%2C%20and%20provides%20accurate%20elliptical%20parameters%20for%20further%20applications%0Aof%20eye%20tracking%20simultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17231v1&entry.124074799=Read"},
{"title": "Accelerating the discovery of steady-states of planetary interior\n  dynamics with machine learning", "author": "Siddhant Agarwal and Nicola Tosi and Christian H\u00fcttig and David S. Greenberg and Ali Can Bekar", "abstract": "  Simulating mantle convection often requires reaching a computationally\nexpensive steady-state, crucial for deriving scaling laws for thermal and\ndynamical flow properties and benchmarking numerical solutions. The strong\ntemperature dependence of the rheology of mantle rocks causes viscosity\nvariations of several orders of magnitude, leading to a slow-evolving stagnant\nlid where heat conduction dominates, overlying a rapidly-evolving and strongly\nconvecting region. Time-stepping methods, while effective for fluids with\nconstant viscosity, are hindered by the Courant criterion, which restricts the\ntime step based on the system's maximum velocity and grid size. Consequently,\nachieving steady-state requires a large number of time steps due to the\ndisparate time scales governing the stagnant and convecting regions.\n  We present a concept for accelerating mantle convection simulations using\nmachine learning. We generate a dataset of 128 two-dimensional simulations with\nmixed basal and internal heating, and pressure- and temperature-dependent\nviscosity. We train a feedforward neural network on 97 simulations to predict\nsteady-state temperature profiles. These can then be used to initialize\nnumerical time stepping methods for different simulation parameters. Compared\nto typical initializations, the number of time steps required to reach\nsteady-state is reduced by a median factor of 3.75. The benefit of this method\nlies in requiring very few simulations to train on, providing a solution with\nno prediction error as we initialize a numerical method, and posing minimal\ncomputational overhead at inference time. We demonstrate the effectiveness of\nour approach and discuss the potential implications for accelerated simulations\nfor advancing mantle convection research.\n", "link": "http://arxiv.org/abs/2408.17298v1", "date": "2024-08-30", "relevancy": 1.393, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.483}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4594}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20the%20discovery%20of%20steady-states%20of%20planetary%20interior%0A%20%20dynamics%20with%20machine%20learning&body=Title%3A%20Accelerating%20the%20discovery%20of%20steady-states%20of%20planetary%20interior%0A%20%20dynamics%20with%20machine%20learning%0AAuthor%3A%20Siddhant%20Agarwal%20and%20Nicola%20Tosi%20and%20Christian%20H%C3%BCttig%20and%20David%20S.%20Greenberg%20and%20Ali%20Can%20Bekar%0AAbstract%3A%20%20%20Simulating%20mantle%20convection%20often%20requires%20reaching%20a%20computationally%0Aexpensive%20steady-state%2C%20crucial%20for%20deriving%20scaling%20laws%20for%20thermal%20and%0Adynamical%20flow%20properties%20and%20benchmarking%20numerical%20solutions.%20The%20strong%0Atemperature%20dependence%20of%20the%20rheology%20of%20mantle%20rocks%20causes%20viscosity%0Avariations%20of%20several%20orders%20of%20magnitude%2C%20leading%20to%20a%20slow-evolving%20stagnant%0Alid%20where%20heat%20conduction%20dominates%2C%20overlying%20a%20rapidly-evolving%20and%20strongly%0Aconvecting%20region.%20Time-stepping%20methods%2C%20while%20effective%20for%20fluids%20with%0Aconstant%20viscosity%2C%20are%20hindered%20by%20the%20Courant%20criterion%2C%20which%20restricts%20the%0Atime%20step%20based%20on%20the%20system%27s%20maximum%20velocity%20and%20grid%20size.%20Consequently%2C%0Aachieving%20steady-state%20requires%20a%20large%20number%20of%20time%20steps%20due%20to%20the%0Adisparate%20time%20scales%20governing%20the%20stagnant%20and%20convecting%20regions.%0A%20%20We%20present%20a%20concept%20for%20accelerating%20mantle%20convection%20simulations%20using%0Amachine%20learning.%20We%20generate%20a%20dataset%20of%20128%20two-dimensional%20simulations%20with%0Amixed%20basal%20and%20internal%20heating%2C%20and%20pressure-%20and%20temperature-dependent%0Aviscosity.%20We%20train%20a%20feedforward%20neural%20network%20on%2097%20simulations%20to%20predict%0Asteady-state%20temperature%20profiles.%20These%20can%20then%20be%20used%20to%20initialize%0Anumerical%20time%20stepping%20methods%20for%20different%20simulation%20parameters.%20Compared%0Ato%20typical%20initializations%2C%20the%20number%20of%20time%20steps%20required%20to%20reach%0Asteady-state%20is%20reduced%20by%20a%20median%20factor%20of%203.75.%20The%20benefit%20of%20this%20method%0Alies%20in%20requiring%20very%20few%20simulations%20to%20train%20on%2C%20providing%20a%20solution%20with%0Ano%20prediction%20error%20as%20we%20initialize%20a%20numerical%20method%2C%20and%20posing%20minimal%0Acomputational%20overhead%20at%20inference%20time.%20We%20demonstrate%20the%20effectiveness%20of%0Aour%20approach%20and%20discuss%20the%20potential%20implications%20for%20accelerated%20simulations%0Afor%20advancing%20mantle%20convection%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17298v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520the%2520discovery%2520of%2520steady-states%2520of%2520planetary%2520interior%250A%2520%2520dynamics%2520with%2520machine%2520learning%26entry.906535625%3DSiddhant%2520Agarwal%2520and%2520Nicola%2520Tosi%2520and%2520Christian%2520H%25C3%25BCttig%2520and%2520David%2520S.%2520Greenberg%2520and%2520Ali%2520Can%2520Bekar%26entry.1292438233%3D%2520%2520Simulating%2520mantle%2520convection%2520often%2520requires%2520reaching%2520a%2520computationally%250Aexpensive%2520steady-state%252C%2520crucial%2520for%2520deriving%2520scaling%2520laws%2520for%2520thermal%2520and%250Adynamical%2520flow%2520properties%2520and%2520benchmarking%2520numerical%2520solutions.%2520The%2520strong%250Atemperature%2520dependence%2520of%2520the%2520rheology%2520of%2520mantle%2520rocks%2520causes%2520viscosity%250Avariations%2520of%2520several%2520orders%2520of%2520magnitude%252C%2520leading%2520to%2520a%2520slow-evolving%2520stagnant%250Alid%2520where%2520heat%2520conduction%2520dominates%252C%2520overlying%2520a%2520rapidly-evolving%2520and%2520strongly%250Aconvecting%2520region.%2520Time-stepping%2520methods%252C%2520while%2520effective%2520for%2520fluids%2520with%250Aconstant%2520viscosity%252C%2520are%2520hindered%2520by%2520the%2520Courant%2520criterion%252C%2520which%2520restricts%2520the%250Atime%2520step%2520based%2520on%2520the%2520system%2527s%2520maximum%2520velocity%2520and%2520grid%2520size.%2520Consequently%252C%250Aachieving%2520steady-state%2520requires%2520a%2520large%2520number%2520of%2520time%2520steps%2520due%2520to%2520the%250Adisparate%2520time%2520scales%2520governing%2520the%2520stagnant%2520and%2520convecting%2520regions.%250A%2520%2520We%2520present%2520a%2520concept%2520for%2520accelerating%2520mantle%2520convection%2520simulations%2520using%250Amachine%2520learning.%2520We%2520generate%2520a%2520dataset%2520of%2520128%2520two-dimensional%2520simulations%2520with%250Amixed%2520basal%2520and%2520internal%2520heating%252C%2520and%2520pressure-%2520and%2520temperature-dependent%250Aviscosity.%2520We%2520train%2520a%2520feedforward%2520neural%2520network%2520on%252097%2520simulations%2520to%2520predict%250Asteady-state%2520temperature%2520profiles.%2520These%2520can%2520then%2520be%2520used%2520to%2520initialize%250Anumerical%2520time%2520stepping%2520methods%2520for%2520different%2520simulation%2520parameters.%2520Compared%250Ato%2520typical%2520initializations%252C%2520the%2520number%2520of%2520time%2520steps%2520required%2520to%2520reach%250Asteady-state%2520is%2520reduced%2520by%2520a%2520median%2520factor%2520of%25203.75.%2520The%2520benefit%2520of%2520this%2520method%250Alies%2520in%2520requiring%2520very%2520few%2520simulations%2520to%2520train%2520on%252C%2520providing%2520a%2520solution%2520with%250Ano%2520prediction%2520error%2520as%2520we%2520initialize%2520a%2520numerical%2520method%252C%2520and%2520posing%2520minimal%250Acomputational%2520overhead%2520at%2520inference%2520time.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520approach%2520and%2520discuss%2520the%2520potential%2520implications%2520for%2520accelerated%2520simulations%250Afor%2520advancing%2520mantle%2520convection%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17298v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20the%20discovery%20of%20steady-states%20of%20planetary%20interior%0A%20%20dynamics%20with%20machine%20learning&entry.906535625=Siddhant%20Agarwal%20and%20Nicola%20Tosi%20and%20Christian%20H%C3%BCttig%20and%20David%20S.%20Greenberg%20and%20Ali%20Can%20Bekar&entry.1292438233=%20%20Simulating%20mantle%20convection%20often%20requires%20reaching%20a%20computationally%0Aexpensive%20steady-state%2C%20crucial%20for%20deriving%20scaling%20laws%20for%20thermal%20and%0Adynamical%20flow%20properties%20and%20benchmarking%20numerical%20solutions.%20The%20strong%0Atemperature%20dependence%20of%20the%20rheology%20of%20mantle%20rocks%20causes%20viscosity%0Avariations%20of%20several%20orders%20of%20magnitude%2C%20leading%20to%20a%20slow-evolving%20stagnant%0Alid%20where%20heat%20conduction%20dominates%2C%20overlying%20a%20rapidly-evolving%20and%20strongly%0Aconvecting%20region.%20Time-stepping%20methods%2C%20while%20effective%20for%20fluids%20with%0Aconstant%20viscosity%2C%20are%20hindered%20by%20the%20Courant%20criterion%2C%20which%20restricts%20the%0Atime%20step%20based%20on%20the%20system%27s%20maximum%20velocity%20and%20grid%20size.%20Consequently%2C%0Aachieving%20steady-state%20requires%20a%20large%20number%20of%20time%20steps%20due%20to%20the%0Adisparate%20time%20scales%20governing%20the%20stagnant%20and%20convecting%20regions.%0A%20%20We%20present%20a%20concept%20for%20accelerating%20mantle%20convection%20simulations%20using%0Amachine%20learning.%20We%20generate%20a%20dataset%20of%20128%20two-dimensional%20simulations%20with%0Amixed%20basal%20and%20internal%20heating%2C%20and%20pressure-%20and%20temperature-dependent%0Aviscosity.%20We%20train%20a%20feedforward%20neural%20network%20on%2097%20simulations%20to%20predict%0Asteady-state%20temperature%20profiles.%20These%20can%20then%20be%20used%20to%20initialize%0Anumerical%20time%20stepping%20methods%20for%20different%20simulation%20parameters.%20Compared%0Ato%20typical%20initializations%2C%20the%20number%20of%20time%20steps%20required%20to%20reach%0Asteady-state%20is%20reduced%20by%20a%20median%20factor%20of%203.75.%20The%20benefit%20of%20this%20method%0Alies%20in%20requiring%20very%20few%20simulations%20to%20train%20on%2C%20providing%20a%20solution%20with%0Ano%20prediction%20error%20as%20we%20initialize%20a%20numerical%20method%2C%20and%20posing%20minimal%0Acomputational%20overhead%20at%20inference%20time.%20We%20demonstrate%20the%20effectiveness%20of%0Aour%20approach%20and%20discuss%20the%20potential%20implications%20for%20accelerated%20simulations%0Afor%20advancing%20mantle%20convection%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17298v1&entry.124074799=Read"},
{"title": "SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame\n  Selection", "author": "Ismail Rasim Ulgen and Shreeram Suresh Chandra and Junchen Lu and Berrak Sisman", "abstract": "  Synthesizing the voices of unseen speakers is a persisting challenge in\nmulti-speaker text-to-speech (TTS). Most multi-speaker TTS models rely on\nmodeling speaker characteristics through speaker conditioning during training.\nModeling unseen speaker attributes through this approach has necessitated an\nincrease in model complexity, which makes it challenging to reproduce results\nand improve upon them. We design a simple alternative to this. We propose\nSelectTTS, a novel method to select the appropriate frames from the target\nspeaker and decode using frame-level self-supervised learning (SSL) features.\nWe show that this approach can effectively capture speaker characteristics for\nunseen speakers, and achieves comparable results to other multi-speaker TTS\nframeworks in both objective and subjective metrics. With SelectTTS, we show\nthat frame selection from the target speaker's speech is a direct way to\nachieve generalization in unseen speakers with low model complexity. We achieve\nbetter speaker similarity performance than SOTA baselines XTTS-v2 and VALL-E\nwith over an 8x reduction in model parameters and a 270x reduction in training\ndata\n", "link": "http://arxiv.org/abs/2408.17432v1", "date": "2024-08-30", "relevancy": 1.5185, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5255}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5032}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelectTTS%3A%20Synthesizing%20Anyone%27s%20Voice%20via%20Discrete%20Unit-Based%20Frame%0A%20%20Selection&body=Title%3A%20SelectTTS%3A%20Synthesizing%20Anyone%27s%20Voice%20via%20Discrete%20Unit-Based%20Frame%0A%20%20Selection%0AAuthor%3A%20Ismail%20Rasim%20Ulgen%20and%20Shreeram%20Suresh%20Chandra%20and%20Junchen%20Lu%20and%20Berrak%20Sisman%0AAbstract%3A%20%20%20Synthesizing%20the%20voices%20of%20unseen%20speakers%20is%20a%20persisting%20challenge%20in%0Amulti-speaker%20text-to-speech%20%28TTS%29.%20Most%20multi-speaker%20TTS%20models%20rely%20on%0Amodeling%20speaker%20characteristics%20through%20speaker%20conditioning%20during%20training.%0AModeling%20unseen%20speaker%20attributes%20through%20this%20approach%20has%20necessitated%20an%0Aincrease%20in%20model%20complexity%2C%20which%20makes%20it%20challenging%20to%20reproduce%20results%0Aand%20improve%20upon%20them.%20We%20design%20a%20simple%20alternative%20to%20this.%20We%20propose%0ASelectTTS%2C%20a%20novel%20method%20to%20select%20the%20appropriate%20frames%20from%20the%20target%0Aspeaker%20and%20decode%20using%20frame-level%20self-supervised%20learning%20%28SSL%29%20features.%0AWe%20show%20that%20this%20approach%20can%20effectively%20capture%20speaker%20characteristics%20for%0Aunseen%20speakers%2C%20and%20achieves%20comparable%20results%20to%20other%20multi-speaker%20TTS%0Aframeworks%20in%20both%20objective%20and%20subjective%20metrics.%20With%20SelectTTS%2C%20we%20show%0Athat%20frame%20selection%20from%20the%20target%20speaker%27s%20speech%20is%20a%20direct%20way%20to%0Aachieve%20generalization%20in%20unseen%20speakers%20with%20low%20model%20complexity.%20We%20achieve%0Abetter%20speaker%20similarity%20performance%20than%20SOTA%20baselines%20XTTS-v2%20and%20VALL-E%0Awith%20over%20an%208x%20reduction%20in%20model%20parameters%20and%20a%20270x%20reduction%20in%20training%0Adata%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelectTTS%253A%2520Synthesizing%2520Anyone%2527s%2520Voice%2520via%2520Discrete%2520Unit-Based%2520Frame%250A%2520%2520Selection%26entry.906535625%3DIsmail%2520Rasim%2520Ulgen%2520and%2520Shreeram%2520Suresh%2520Chandra%2520and%2520Junchen%2520Lu%2520and%2520Berrak%2520Sisman%26entry.1292438233%3D%2520%2520Synthesizing%2520the%2520voices%2520of%2520unseen%2520speakers%2520is%2520a%2520persisting%2520challenge%2520in%250Amulti-speaker%2520text-to-speech%2520%2528TTS%2529.%2520Most%2520multi-speaker%2520TTS%2520models%2520rely%2520on%250Amodeling%2520speaker%2520characteristics%2520through%2520speaker%2520conditioning%2520during%2520training.%250AModeling%2520unseen%2520speaker%2520attributes%2520through%2520this%2520approach%2520has%2520necessitated%2520an%250Aincrease%2520in%2520model%2520complexity%252C%2520which%2520makes%2520it%2520challenging%2520to%2520reproduce%2520results%250Aand%2520improve%2520upon%2520them.%2520We%2520design%2520a%2520simple%2520alternative%2520to%2520this.%2520We%2520propose%250ASelectTTS%252C%2520a%2520novel%2520method%2520to%2520select%2520the%2520appropriate%2520frames%2520from%2520the%2520target%250Aspeaker%2520and%2520decode%2520using%2520frame-level%2520self-supervised%2520learning%2520%2528SSL%2529%2520features.%250AWe%2520show%2520that%2520this%2520approach%2520can%2520effectively%2520capture%2520speaker%2520characteristics%2520for%250Aunseen%2520speakers%252C%2520and%2520achieves%2520comparable%2520results%2520to%2520other%2520multi-speaker%2520TTS%250Aframeworks%2520in%2520both%2520objective%2520and%2520subjective%2520metrics.%2520With%2520SelectTTS%252C%2520we%2520show%250Athat%2520frame%2520selection%2520from%2520the%2520target%2520speaker%2527s%2520speech%2520is%2520a%2520direct%2520way%2520to%250Aachieve%2520generalization%2520in%2520unseen%2520speakers%2520with%2520low%2520model%2520complexity.%2520We%2520achieve%250Abetter%2520speaker%2520similarity%2520performance%2520than%2520SOTA%2520baselines%2520XTTS-v2%2520and%2520VALL-E%250Awith%2520over%2520an%25208x%2520reduction%2520in%2520model%2520parameters%2520and%2520a%2520270x%2520reduction%2520in%2520training%250Adata%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelectTTS%3A%20Synthesizing%20Anyone%27s%20Voice%20via%20Discrete%20Unit-Based%20Frame%0A%20%20Selection&entry.906535625=Ismail%20Rasim%20Ulgen%20and%20Shreeram%20Suresh%20Chandra%20and%20Junchen%20Lu%20and%20Berrak%20Sisman&entry.1292438233=%20%20Synthesizing%20the%20voices%20of%20unseen%20speakers%20is%20a%20persisting%20challenge%20in%0Amulti-speaker%20text-to-speech%20%28TTS%29.%20Most%20multi-speaker%20TTS%20models%20rely%20on%0Amodeling%20speaker%20characteristics%20through%20speaker%20conditioning%20during%20training.%0AModeling%20unseen%20speaker%20attributes%20through%20this%20approach%20has%20necessitated%20an%0Aincrease%20in%20model%20complexity%2C%20which%20makes%20it%20challenging%20to%20reproduce%20results%0Aand%20improve%20upon%20them.%20We%20design%20a%20simple%20alternative%20to%20this.%20We%20propose%0ASelectTTS%2C%20a%20novel%20method%20to%20select%20the%20appropriate%20frames%20from%20the%20target%0Aspeaker%20and%20decode%20using%20frame-level%20self-supervised%20learning%20%28SSL%29%20features.%0AWe%20show%20that%20this%20approach%20can%20effectively%20capture%20speaker%20characteristics%20for%0Aunseen%20speakers%2C%20and%20achieves%20comparable%20results%20to%20other%20multi-speaker%20TTS%0Aframeworks%20in%20both%20objective%20and%20subjective%20metrics.%20With%20SelectTTS%2C%20we%20show%0Athat%20frame%20selection%20from%20the%20target%20speaker%27s%20speech%20is%20a%20direct%20way%20to%0Aachieve%20generalization%20in%20unseen%20speakers%20with%20low%20model%20complexity.%20We%20achieve%0Abetter%20speaker%20similarity%20performance%20than%20SOTA%20baselines%20XTTS-v2%20and%20VALL-E%0Awith%20over%20an%208x%20reduction%20in%20model%20parameters%20and%20a%20270x%20reduction%20in%20training%0Adata%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17432v1&entry.124074799=Read"},
{"title": "Minor DPO reject penalty to increase training robustness", "author": "Shiming Xie and Hong Chen and Fred Yu and Zeye Sun and Xiuyu Wu and Yingfan Hu", "abstract": "  Learning from human preference is a paradigm used in large-scale language\nmodel (LLM) fine-tuning step to better align pretrained LLM to human preference\nfor downstream task. In the past it uses reinforcement learning from human\nfeedback (RLHF) algorithm to optimize the LLM policy to align with these\npreferences and not to draft too far from the original model. Recently, Direct\nPreference Optimization (DPO) has been proposed to solve the alignment problem\nwith a simplified RL-free method. Using preference pairs of chosen and reject\ndata, DPO models the relative log probability as implicit reward function and\noptimize LLM policy using a simple binary cross entropy objective directly. DPO\nis quite straight forward and easy to be understood. It perform efficiently and\nwell in most cases. In this article, we analyze the working mechanism of\n$\\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,\nand understand the potential shortage brought by the DPO simplification. With\nthese insights, we propose MinorDPO, which is better aligned to the original RL\nalgorithm, and increase the stability of preference optimization process.\n", "link": "http://arxiv.org/abs/2408.09834v3", "date": "2024-08-30", "relevancy": 1.4024, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4812}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4579}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minor%20DPO%20reject%20penalty%20to%20increase%20training%20robustness&body=Title%3A%20Minor%20DPO%20reject%20penalty%20to%20increase%20training%20robustness%0AAuthor%3A%20Shiming%20Xie%20and%20Hong%20Chen%20and%20Fred%20Yu%20and%20Zeye%20Sun%20and%20Xiuyu%20Wu%20and%20Yingfan%20Hu%0AAbstract%3A%20%20%20Learning%20from%20human%20preference%20is%20a%20paradigm%20used%20in%20large-scale%20language%0Amodel%20%28LLM%29%20fine-tuning%20step%20to%20better%20align%20pretrained%20LLM%20to%20human%20preference%0Afor%20downstream%20task.%20In%20the%20past%20it%20uses%20reinforcement%20learning%20from%20human%0Afeedback%20%28RLHF%29%20algorithm%20to%20optimize%20the%20LLM%20policy%20to%20align%20with%20these%0Apreferences%20and%20not%20to%20draft%20too%20far%20from%20the%20original%20model.%20Recently%2C%20Direct%0APreference%20Optimization%20%28DPO%29%20has%20been%20proposed%20to%20solve%20the%20alignment%20problem%0Awith%20a%20simplified%20RL-free%20method.%20Using%20preference%20pairs%20of%20chosen%20and%20reject%0Adata%2C%20DPO%20models%20the%20relative%20log%20probability%20as%20implicit%20reward%20function%20and%0Aoptimize%20LLM%20policy%20using%20a%20simple%20binary%20cross%20entropy%20objective%20directly.%20DPO%0Ais%20quite%20straight%20forward%20and%20easy%20to%20be%20understood.%20It%20perform%20efficiently%20and%0Awell%20in%20most%20cases.%20In%20this%20article%2C%20we%20analyze%20the%20working%20mechanism%20of%0A%24%5Cbeta%24%20in%20DPO%2C%20disclose%20its%20syntax%20difference%20between%20RL%20algorithm%20and%20DPO%2C%0Aand%20understand%20the%20potential%20shortage%20brought%20by%20the%20DPO%20simplification.%20With%0Athese%20insights%2C%20we%20propose%20MinorDPO%2C%20which%20is%20better%20aligned%20to%20the%20original%20RL%0Aalgorithm%2C%20and%20increase%20the%20stability%20of%20preference%20optimization%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09834v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinor%2520DPO%2520reject%2520penalty%2520to%2520increase%2520training%2520robustness%26entry.906535625%3DShiming%2520Xie%2520and%2520Hong%2520Chen%2520and%2520Fred%2520Yu%2520and%2520Zeye%2520Sun%2520and%2520Xiuyu%2520Wu%2520and%2520Yingfan%2520Hu%26entry.1292438233%3D%2520%2520Learning%2520from%2520human%2520preference%2520is%2520a%2520paradigm%2520used%2520in%2520large-scale%2520language%250Amodel%2520%2528LLM%2529%2520fine-tuning%2520step%2520to%2520better%2520align%2520pretrained%2520LLM%2520to%2520human%2520preference%250Afor%2520downstream%2520task.%2520In%2520the%2520past%2520it%2520uses%2520reinforcement%2520learning%2520from%2520human%250Afeedback%2520%2528RLHF%2529%2520algorithm%2520to%2520optimize%2520the%2520LLM%2520policy%2520to%2520align%2520with%2520these%250Apreferences%2520and%2520not%2520to%2520draft%2520too%2520far%2520from%2520the%2520original%2520model.%2520Recently%252C%2520Direct%250APreference%2520Optimization%2520%2528DPO%2529%2520has%2520been%2520proposed%2520to%2520solve%2520the%2520alignment%2520problem%250Awith%2520a%2520simplified%2520RL-free%2520method.%2520Using%2520preference%2520pairs%2520of%2520chosen%2520and%2520reject%250Adata%252C%2520DPO%2520models%2520the%2520relative%2520log%2520probability%2520as%2520implicit%2520reward%2520function%2520and%250Aoptimize%2520LLM%2520policy%2520using%2520a%2520simple%2520binary%2520cross%2520entropy%2520objective%2520directly.%2520DPO%250Ais%2520quite%2520straight%2520forward%2520and%2520easy%2520to%2520be%2520understood.%2520It%2520perform%2520efficiently%2520and%250Awell%2520in%2520most%2520cases.%2520In%2520this%2520article%252C%2520we%2520analyze%2520the%2520working%2520mechanism%2520of%250A%2524%255Cbeta%2524%2520in%2520DPO%252C%2520disclose%2520its%2520syntax%2520difference%2520between%2520RL%2520algorithm%2520and%2520DPO%252C%250Aand%2520understand%2520the%2520potential%2520shortage%2520brought%2520by%2520the%2520DPO%2520simplification.%2520With%250Athese%2520insights%252C%2520we%2520propose%2520MinorDPO%252C%2520which%2520is%2520better%2520aligned%2520to%2520the%2520original%2520RL%250Aalgorithm%252C%2520and%2520increase%2520the%2520stability%2520of%2520preference%2520optimization%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09834v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minor%20DPO%20reject%20penalty%20to%20increase%20training%20robustness&entry.906535625=Shiming%20Xie%20and%20Hong%20Chen%20and%20Fred%20Yu%20and%20Zeye%20Sun%20and%20Xiuyu%20Wu%20and%20Yingfan%20Hu&entry.1292438233=%20%20Learning%20from%20human%20preference%20is%20a%20paradigm%20used%20in%20large-scale%20language%0Amodel%20%28LLM%29%20fine-tuning%20step%20to%20better%20align%20pretrained%20LLM%20to%20human%20preference%0Afor%20downstream%20task.%20In%20the%20past%20it%20uses%20reinforcement%20learning%20from%20human%0Afeedback%20%28RLHF%29%20algorithm%20to%20optimize%20the%20LLM%20policy%20to%20align%20with%20these%0Apreferences%20and%20not%20to%20draft%20too%20far%20from%20the%20original%20model.%20Recently%2C%20Direct%0APreference%20Optimization%20%28DPO%29%20has%20been%20proposed%20to%20solve%20the%20alignment%20problem%0Awith%20a%20simplified%20RL-free%20method.%20Using%20preference%20pairs%20of%20chosen%20and%20reject%0Adata%2C%20DPO%20models%20the%20relative%20log%20probability%20as%20implicit%20reward%20function%20and%0Aoptimize%20LLM%20policy%20using%20a%20simple%20binary%20cross%20entropy%20objective%20directly.%20DPO%0Ais%20quite%20straight%20forward%20and%20easy%20to%20be%20understood.%20It%20perform%20efficiently%20and%0Awell%20in%20most%20cases.%20In%20this%20article%2C%20we%20analyze%20the%20working%20mechanism%20of%0A%24%5Cbeta%24%20in%20DPO%2C%20disclose%20its%20syntax%20difference%20between%20RL%20algorithm%20and%20DPO%2C%0Aand%20understand%20the%20potential%20shortage%20brought%20by%20the%20DPO%20simplification.%20With%0Athese%20insights%2C%20we%20propose%20MinorDPO%2C%20which%20is%20better%20aligned%20to%20the%20original%20RL%0Aalgorithm%2C%20and%20increase%20the%20stability%20of%20preference%20optimization%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09834v3&entry.124074799=Read"},
{"title": "DCUDF2: Improving Efficiency and Accuracy in Extracting Zero Level Sets\n  from Unsigned Distance Fields", "author": "Xuhui Chen and Fugang Yu and Fei Hou and Wencheng Wang and Zhebin Zhang and Ying He", "abstract": "  Unsigned distance fields (UDFs) allow for the representation of models with\ncomplex topologies, but extracting accurate zero level sets from these fields\nposes significant challenges, particularly in preserving topological accuracy\nand capturing fine geometric details. To overcome these issues, we introduce\nDCUDF2, an enhancement over DCUDF--the current state-of-the-art method--for\nextracting zero level sets from UDFs. Our approach utilizes an accuracy-aware\nloss function, enhanced with self-adaptive weights, to improve geometric\nquality significantly. We also propose a topology correction strategy that\nreduces the dependence on hyper-parameter, increasing the robustness of our\nmethod. Furthermore, we develop new operations leveraging self-adaptive weights\nto boost runtime efficiency. Extensive experiments on surface extraction across\ndiverse datasets demonstrate that DCUDF2 outperforms DCUDF and existing methods\nin both geometric fidelity and topological accuracy. We will make the source\ncode publicly available.\n", "link": "http://arxiv.org/abs/2408.17284v1", "date": "2024-08-30", "relevancy": 1.473, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5177}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4973}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DCUDF2%3A%20Improving%20Efficiency%20and%20Accuracy%20in%20Extracting%20Zero%20Level%20Sets%0A%20%20from%20Unsigned%20Distance%20Fields&body=Title%3A%20DCUDF2%3A%20Improving%20Efficiency%20and%20Accuracy%20in%20Extracting%20Zero%20Level%20Sets%0A%20%20from%20Unsigned%20Distance%20Fields%0AAuthor%3A%20Xuhui%20Chen%20and%20Fugang%20Yu%20and%20Fei%20Hou%20and%20Wencheng%20Wang%20and%20Zhebin%20Zhang%20and%20Ying%20He%0AAbstract%3A%20%20%20Unsigned%20distance%20fields%20%28UDFs%29%20allow%20for%20the%20representation%20of%20models%20with%0Acomplex%20topologies%2C%20but%20extracting%20accurate%20zero%20level%20sets%20from%20these%20fields%0Aposes%20significant%20challenges%2C%20particularly%20in%20preserving%20topological%20accuracy%0Aand%20capturing%20fine%20geometric%20details.%20To%20overcome%20these%20issues%2C%20we%20introduce%0ADCUDF2%2C%20an%20enhancement%20over%20DCUDF--the%20current%20state-of-the-art%20method--for%0Aextracting%20zero%20level%20sets%20from%20UDFs.%20Our%20approach%20utilizes%20an%20accuracy-aware%0Aloss%20function%2C%20enhanced%20with%20self-adaptive%20weights%2C%20to%20improve%20geometric%0Aquality%20significantly.%20We%20also%20propose%20a%20topology%20correction%20strategy%20that%0Areduces%20the%20dependence%20on%20hyper-parameter%2C%20increasing%20the%20robustness%20of%20our%0Amethod.%20Furthermore%2C%20we%20develop%20new%20operations%20leveraging%20self-adaptive%20weights%0Ato%20boost%20runtime%20efficiency.%20Extensive%20experiments%20on%20surface%20extraction%20across%0Adiverse%20datasets%20demonstrate%20that%20DCUDF2%20outperforms%20DCUDF%20and%20existing%20methods%0Ain%20both%20geometric%20fidelity%20and%20topological%20accuracy.%20We%20will%20make%20the%20source%0Acode%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDCUDF2%253A%2520Improving%2520Efficiency%2520and%2520Accuracy%2520in%2520Extracting%2520Zero%2520Level%2520Sets%250A%2520%2520from%2520Unsigned%2520Distance%2520Fields%26entry.906535625%3DXuhui%2520Chen%2520and%2520Fugang%2520Yu%2520and%2520Fei%2520Hou%2520and%2520Wencheng%2520Wang%2520and%2520Zhebin%2520Zhang%2520and%2520Ying%2520He%26entry.1292438233%3D%2520%2520Unsigned%2520distance%2520fields%2520%2528UDFs%2529%2520allow%2520for%2520the%2520representation%2520of%2520models%2520with%250Acomplex%2520topologies%252C%2520but%2520extracting%2520accurate%2520zero%2520level%2520sets%2520from%2520these%2520fields%250Aposes%2520significant%2520challenges%252C%2520particularly%2520in%2520preserving%2520topological%2520accuracy%250Aand%2520capturing%2520fine%2520geometric%2520details.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520introduce%250ADCUDF2%252C%2520an%2520enhancement%2520over%2520DCUDF--the%2520current%2520state-of-the-art%2520method--for%250Aextracting%2520zero%2520level%2520sets%2520from%2520UDFs.%2520Our%2520approach%2520utilizes%2520an%2520accuracy-aware%250Aloss%2520function%252C%2520enhanced%2520with%2520self-adaptive%2520weights%252C%2520to%2520improve%2520geometric%250Aquality%2520significantly.%2520We%2520also%2520propose%2520a%2520topology%2520correction%2520strategy%2520that%250Areduces%2520the%2520dependence%2520on%2520hyper-parameter%252C%2520increasing%2520the%2520robustness%2520of%2520our%250Amethod.%2520Furthermore%252C%2520we%2520develop%2520new%2520operations%2520leveraging%2520self-adaptive%2520weights%250Ato%2520boost%2520runtime%2520efficiency.%2520Extensive%2520experiments%2520on%2520surface%2520extraction%2520across%250Adiverse%2520datasets%2520demonstrate%2520that%2520DCUDF2%2520outperforms%2520DCUDF%2520and%2520existing%2520methods%250Ain%2520both%2520geometric%2520fidelity%2520and%2520topological%2520accuracy.%2520We%2520will%2520make%2520the%2520source%250Acode%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCUDF2%3A%20Improving%20Efficiency%20and%20Accuracy%20in%20Extracting%20Zero%20Level%20Sets%0A%20%20from%20Unsigned%20Distance%20Fields&entry.906535625=Xuhui%20Chen%20and%20Fugang%20Yu%20and%20Fei%20Hou%20and%20Wencheng%20Wang%20and%20Zhebin%20Zhang%20and%20Ying%20He&entry.1292438233=%20%20Unsigned%20distance%20fields%20%28UDFs%29%20allow%20for%20the%20representation%20of%20models%20with%0Acomplex%20topologies%2C%20but%20extracting%20accurate%20zero%20level%20sets%20from%20these%20fields%0Aposes%20significant%20challenges%2C%20particularly%20in%20preserving%20topological%20accuracy%0Aand%20capturing%20fine%20geometric%20details.%20To%20overcome%20these%20issues%2C%20we%20introduce%0ADCUDF2%2C%20an%20enhancement%20over%20DCUDF--the%20current%20state-of-the-art%20method--for%0Aextracting%20zero%20level%20sets%20from%20UDFs.%20Our%20approach%20utilizes%20an%20accuracy-aware%0Aloss%20function%2C%20enhanced%20with%20self-adaptive%20weights%2C%20to%20improve%20geometric%0Aquality%20significantly.%20We%20also%20propose%20a%20topology%20correction%20strategy%20that%0Areduces%20the%20dependence%20on%20hyper-parameter%2C%20increasing%20the%20robustness%20of%20our%0Amethod.%20Furthermore%2C%20we%20develop%20new%20operations%20leveraging%20self-adaptive%20weights%0Ato%20boost%20runtime%20efficiency.%20Extensive%20experiments%20on%20surface%20extraction%20across%0Adiverse%20datasets%20demonstrate%20that%20DCUDF2%20outperforms%20DCUDF%20and%20existing%20methods%0Ain%20both%20geometric%20fidelity%20and%20topological%20accuracy.%20We%20will%20make%20the%20source%0Acode%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17284v1&entry.124074799=Read"},
{"title": "A Survey on Knowledge Editing of Neural Networks", "author": "Vittorio Mazzia and Alessandro Pedrani and Andrea Caciolai and Kay Rottmann and Davide Bernardi", "abstract": "  Deep neural networks are becoming increasingly pervasive in academia and\nindustry, matching and surpassing human performance on a wide variety of fields\nand related tasks. However, just as humans, even the largest artificial neural\nnetworks make mistakes, and once-correct predictions can become invalid as the\nworld progresses in time. Augmenting datasets with samples that account for\nmistakes or up-to-date information has become a common workaround in practical\napplications. However, the well-known phenomenon of catastrophic forgetting\nposes a challenge in achieving precise changes in the implicitly memorized\nknowledge of neural network parameters, often requiring a full model\nre-training to achieve desired behaviors. That is expensive, unreliable, and\nincompatible with the current trend of large self-supervised pre-training,\nmaking it necessary to find more efficient and effective methods for adapting\nneural network models to changing data. To address this need, knowledge editing\nis emerging as a novel area of research that aims to enable reliable,\ndata-efficient, and fast changes to a pre-trained target model, without\naffecting model behaviors on previously learned tasks. In this survey, we\nprovide a brief review of this recent artificial intelligence field of\nresearch. We first introduce the problem of editing neural networks, formalize\nit in a common framework and differentiate it from more notorious branches of\nresearch such as continuous learning. Next, we provide a review of the most\nrelevant knowledge editing approaches and datasets proposed so far, grouping\nworks under four different families: regularization techniques, meta-learning,\ndirect model editing, and architectural strategies. Finally, we outline some\nintersections with other fields of research and potential directions for future\nworks.\n", "link": "http://arxiv.org/abs/2310.19704v3", "date": "2024-08-30", "relevancy": 1.4595, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5053}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4721}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Knowledge%20Editing%20of%20Neural%20Networks&body=Title%3A%20A%20Survey%20on%20Knowledge%20Editing%20of%20Neural%20Networks%0AAuthor%3A%20Vittorio%20Mazzia%20and%20Alessandro%20Pedrani%20and%20Andrea%20Caciolai%20and%20Kay%20Rottmann%20and%20Davide%20Bernardi%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20becoming%20increasingly%20pervasive%20in%20academia%20and%0Aindustry%2C%20matching%20and%20surpassing%20human%20performance%20on%20a%20wide%20variety%20of%20fields%0Aand%20related%20tasks.%20However%2C%20just%20as%20humans%2C%20even%20the%20largest%20artificial%20neural%0Anetworks%20make%20mistakes%2C%20and%20once-correct%20predictions%20can%20become%20invalid%20as%20the%0Aworld%20progresses%20in%20time.%20Augmenting%20datasets%20with%20samples%20that%20account%20for%0Amistakes%20or%20up-to-date%20information%20has%20become%20a%20common%20workaround%20in%20practical%0Aapplications.%20However%2C%20the%20well-known%20phenomenon%20of%20catastrophic%20forgetting%0Aposes%20a%20challenge%20in%20achieving%20precise%20changes%20in%20the%20implicitly%20memorized%0Aknowledge%20of%20neural%20network%20parameters%2C%20often%20requiring%20a%20full%20model%0Are-training%20to%20achieve%20desired%20behaviors.%20That%20is%20expensive%2C%20unreliable%2C%20and%0Aincompatible%20with%20the%20current%20trend%20of%20large%20self-supervised%20pre-training%2C%0Amaking%20it%20necessary%20to%20find%20more%20efficient%20and%20effective%20methods%20for%20adapting%0Aneural%20network%20models%20to%20changing%20data.%20To%20address%20this%20need%2C%20knowledge%20editing%0Ais%20emerging%20as%20a%20novel%20area%20of%20research%20that%20aims%20to%20enable%20reliable%2C%0Adata-efficient%2C%20and%20fast%20changes%20to%20a%20pre-trained%20target%20model%2C%20without%0Aaffecting%20model%20behaviors%20on%20previously%20learned%20tasks.%20In%20this%20survey%2C%20we%0Aprovide%20a%20brief%20review%20of%20this%20recent%20artificial%20intelligence%20field%20of%0Aresearch.%20We%20first%20introduce%20the%20problem%20of%20editing%20neural%20networks%2C%20formalize%0Ait%20in%20a%20common%20framework%20and%20differentiate%20it%20from%20more%20notorious%20branches%20of%0Aresearch%20such%20as%20continuous%20learning.%20Next%2C%20we%20provide%20a%20review%20of%20the%20most%0Arelevant%20knowledge%20editing%20approaches%20and%20datasets%20proposed%20so%20far%2C%20grouping%0Aworks%20under%20four%20different%20families%3A%20regularization%20techniques%2C%20meta-learning%2C%0Adirect%20model%20editing%2C%20and%20architectural%20strategies.%20Finally%2C%20we%20outline%20some%0Aintersections%20with%20other%20fields%20of%20research%20and%20potential%20directions%20for%20future%0Aworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19704v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Knowledge%2520Editing%2520of%2520Neural%2520Networks%26entry.906535625%3DVittorio%2520Mazzia%2520and%2520Alessandro%2520Pedrani%2520and%2520Andrea%2520Caciolai%2520and%2520Kay%2520Rottmann%2520and%2520Davide%2520Bernardi%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520are%2520becoming%2520increasingly%2520pervasive%2520in%2520academia%2520and%250Aindustry%252C%2520matching%2520and%2520surpassing%2520human%2520performance%2520on%2520a%2520wide%2520variety%2520of%2520fields%250Aand%2520related%2520tasks.%2520However%252C%2520just%2520as%2520humans%252C%2520even%2520the%2520largest%2520artificial%2520neural%250Anetworks%2520make%2520mistakes%252C%2520and%2520once-correct%2520predictions%2520can%2520become%2520invalid%2520as%2520the%250Aworld%2520progresses%2520in%2520time.%2520Augmenting%2520datasets%2520with%2520samples%2520that%2520account%2520for%250Amistakes%2520or%2520up-to-date%2520information%2520has%2520become%2520a%2520common%2520workaround%2520in%2520practical%250Aapplications.%2520However%252C%2520the%2520well-known%2520phenomenon%2520of%2520catastrophic%2520forgetting%250Aposes%2520a%2520challenge%2520in%2520achieving%2520precise%2520changes%2520in%2520the%2520implicitly%2520memorized%250Aknowledge%2520of%2520neural%2520network%2520parameters%252C%2520often%2520requiring%2520a%2520full%2520model%250Are-training%2520to%2520achieve%2520desired%2520behaviors.%2520That%2520is%2520expensive%252C%2520unreliable%252C%2520and%250Aincompatible%2520with%2520the%2520current%2520trend%2520of%2520large%2520self-supervised%2520pre-training%252C%250Amaking%2520it%2520necessary%2520to%2520find%2520more%2520efficient%2520and%2520effective%2520methods%2520for%2520adapting%250Aneural%2520network%2520models%2520to%2520changing%2520data.%2520To%2520address%2520this%2520need%252C%2520knowledge%2520editing%250Ais%2520emerging%2520as%2520a%2520novel%2520area%2520of%2520research%2520that%2520aims%2520to%2520enable%2520reliable%252C%250Adata-efficient%252C%2520and%2520fast%2520changes%2520to%2520a%2520pre-trained%2520target%2520model%252C%2520without%250Aaffecting%2520model%2520behaviors%2520on%2520previously%2520learned%2520tasks.%2520In%2520this%2520survey%252C%2520we%250Aprovide%2520a%2520brief%2520review%2520of%2520this%2520recent%2520artificial%2520intelligence%2520field%2520of%250Aresearch.%2520We%2520first%2520introduce%2520the%2520problem%2520of%2520editing%2520neural%2520networks%252C%2520formalize%250Ait%2520in%2520a%2520common%2520framework%2520and%2520differentiate%2520it%2520from%2520more%2520notorious%2520branches%2520of%250Aresearch%2520such%2520as%2520continuous%2520learning.%2520Next%252C%2520we%2520provide%2520a%2520review%2520of%2520the%2520most%250Arelevant%2520knowledge%2520editing%2520approaches%2520and%2520datasets%2520proposed%2520so%2520far%252C%2520grouping%250Aworks%2520under%2520four%2520different%2520families%253A%2520regularization%2520techniques%252C%2520meta-learning%252C%250Adirect%2520model%2520editing%252C%2520and%2520architectural%2520strategies.%2520Finally%252C%2520we%2520outline%2520some%250Aintersections%2520with%2520other%2520fields%2520of%2520research%2520and%2520potential%2520directions%2520for%2520future%250Aworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19704v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Knowledge%20Editing%20of%20Neural%20Networks&entry.906535625=Vittorio%20Mazzia%20and%20Alessandro%20Pedrani%20and%20Andrea%20Caciolai%20and%20Kay%20Rottmann%20and%20Davide%20Bernardi&entry.1292438233=%20%20Deep%20neural%20networks%20are%20becoming%20increasingly%20pervasive%20in%20academia%20and%0Aindustry%2C%20matching%20and%20surpassing%20human%20performance%20on%20a%20wide%20variety%20of%20fields%0Aand%20related%20tasks.%20However%2C%20just%20as%20humans%2C%20even%20the%20largest%20artificial%20neural%0Anetworks%20make%20mistakes%2C%20and%20once-correct%20predictions%20can%20become%20invalid%20as%20the%0Aworld%20progresses%20in%20time.%20Augmenting%20datasets%20with%20samples%20that%20account%20for%0Amistakes%20or%20up-to-date%20information%20has%20become%20a%20common%20workaround%20in%20practical%0Aapplications.%20However%2C%20the%20well-known%20phenomenon%20of%20catastrophic%20forgetting%0Aposes%20a%20challenge%20in%20achieving%20precise%20changes%20in%20the%20implicitly%20memorized%0Aknowledge%20of%20neural%20network%20parameters%2C%20often%20requiring%20a%20full%20model%0Are-training%20to%20achieve%20desired%20behaviors.%20That%20is%20expensive%2C%20unreliable%2C%20and%0Aincompatible%20with%20the%20current%20trend%20of%20large%20self-supervised%20pre-training%2C%0Amaking%20it%20necessary%20to%20find%20more%20efficient%20and%20effective%20methods%20for%20adapting%0Aneural%20network%20models%20to%20changing%20data.%20To%20address%20this%20need%2C%20knowledge%20editing%0Ais%20emerging%20as%20a%20novel%20area%20of%20research%20that%20aims%20to%20enable%20reliable%2C%0Adata-efficient%2C%20and%20fast%20changes%20to%20a%20pre-trained%20target%20model%2C%20without%0Aaffecting%20model%20behaviors%20on%20previously%20learned%20tasks.%20In%20this%20survey%2C%20we%0Aprovide%20a%20brief%20review%20of%20this%20recent%20artificial%20intelligence%20field%20of%0Aresearch.%20We%20first%20introduce%20the%20problem%20of%20editing%20neural%20networks%2C%20formalize%0Ait%20in%20a%20common%20framework%20and%20differentiate%20it%20from%20more%20notorious%20branches%20of%0Aresearch%20such%20as%20continuous%20learning.%20Next%2C%20we%20provide%20a%20review%20of%20the%20most%0Arelevant%20knowledge%20editing%20approaches%20and%20datasets%20proposed%20so%20far%2C%20grouping%0Aworks%20under%20four%20different%20families%3A%20regularization%20techniques%2C%20meta-learning%2C%0Adirect%20model%20editing%2C%20and%20architectural%20strategies.%20Finally%2C%20we%20outline%20some%0Aintersections%20with%20other%20fields%20of%20research%20and%20potential%20directions%20for%20future%0Aworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19704v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


