<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251223.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images", "author": "Linfei Li and Lin Zhang and Zhong Wang and Ying Shen", "abstract": "Recent advances in generative AI have accelerated the production of ultra-high-resolution visual content, posing significant challenges for efficient compression and real-time decoding on end-user devices. Inspired by 3D Gaussian Splatting, recent 2D Gaussian image models improve representation efficiency, yet existing methods struggle to balance compression ratio and reconstruction fidelity in ultra-high-resolution scenarios. To address this issue, we propose SmartSplat, a highly adaptive and feature-aware GS-based image compression framework that supports arbitrary image resolutions and compression ratios. SmartSplat leverages image-aware features such as gradients and color variances, introducing a Gradient-Color Guided Variational Sampling strategy together with an Exclusion-based Uniform Sampling scheme to improve the non-overlapping coverage of Gaussian primitives in pixel space. In addition, we propose a Scale-Adaptive Gaussian Color Sampling method to enhance color initialization across scales. Through joint optimization of spatial layout, scale, and color initialization, SmartSplat efficiently captures both local structures and global textures using a limited number of Gaussians, achieving high reconstruction quality under strong compression. Extensive experiments on DIV8K and a newly constructed 16K dataset demonstrate that SmartSplat consistently outperforms state-of-the-art methods at comparable compression ratios and exceeds their compression limits, showing strong scalability and practical applicability. The code is publicly available at https://github.com/lif314/SmartSplat.", "link": "http://arxiv.org/abs/2512.20377v1", "date": "2025-12-23", "relevancy": 3.4157, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7068}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6886}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SmartSplat%3A%20Feature-Smart%20Gaussians%20for%20Scalable%20Compression%20of%20Ultra-High-Resolution%20Images&body=Title%3A%20SmartSplat%3A%20Feature-Smart%20Gaussians%20for%20Scalable%20Compression%20of%20Ultra-High-Resolution%20Images%0AAuthor%3A%20Linfei%20Li%20and%20Lin%20Zhang%20and%20Zhong%20Wang%20and%20Ying%20Shen%0AAbstract%3A%20Recent%20advances%20in%20generative%20AI%20have%20accelerated%20the%20production%20of%20ultra-high-resolution%20visual%20content%2C%20posing%20significant%20challenges%20for%20efficient%20compression%20and%20real-time%20decoding%20on%20end-user%20devices.%20Inspired%20by%203D%20Gaussian%20Splatting%2C%20recent%202D%20Gaussian%20image%20models%20improve%20representation%20efficiency%2C%20yet%20existing%20methods%20struggle%20to%20balance%20compression%20ratio%20and%20reconstruction%20fidelity%20in%20ultra-high-resolution%20scenarios.%20To%20address%20this%20issue%2C%20we%20propose%20SmartSplat%2C%20a%20highly%20adaptive%20and%20feature-aware%20GS-based%20image%20compression%20framework%20that%20supports%20arbitrary%20image%20resolutions%20and%20compression%20ratios.%20SmartSplat%20leverages%20image-aware%20features%20such%20as%20gradients%20and%20color%20variances%2C%20introducing%20a%20Gradient-Color%20Guided%20Variational%20Sampling%20strategy%20together%20with%20an%20Exclusion-based%20Uniform%20Sampling%20scheme%20to%20improve%20the%20non-overlapping%20coverage%20of%20Gaussian%20primitives%20in%20pixel%20space.%20In%20addition%2C%20we%20propose%20a%20Scale-Adaptive%20Gaussian%20Color%20Sampling%20method%20to%20enhance%20color%20initialization%20across%20scales.%20Through%20joint%20optimization%20of%20spatial%20layout%2C%20scale%2C%20and%20color%20initialization%2C%20SmartSplat%20efficiently%20captures%20both%20local%20structures%20and%20global%20textures%20using%20a%20limited%20number%20of%20Gaussians%2C%20achieving%20high%20reconstruction%20quality%20under%20strong%20compression.%20Extensive%20experiments%20on%20DIV8K%20and%20a%20newly%20constructed%2016K%20dataset%20demonstrate%20that%20SmartSplat%20consistently%20outperforms%20state-of-the-art%20methods%20at%20comparable%20compression%20ratios%20and%20exceeds%20their%20compression%20limits%2C%20showing%20strong%20scalability%20and%20practical%20applicability.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/lif314/SmartSplat.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmartSplat%253A%2520Feature-Smart%2520Gaussians%2520for%2520Scalable%2520Compression%2520of%2520Ultra-High-Resolution%2520Images%26entry.906535625%3DLinfei%2520Li%2520and%2520Lin%2520Zhang%2520and%2520Zhong%2520Wang%2520and%2520Ying%2520Shen%26entry.1292438233%3DRecent%2520advances%2520in%2520generative%2520AI%2520have%2520accelerated%2520the%2520production%2520of%2520ultra-high-resolution%2520visual%2520content%252C%2520posing%2520significant%2520challenges%2520for%2520efficient%2520compression%2520and%2520real-time%2520decoding%2520on%2520end-user%2520devices.%2520Inspired%2520by%25203D%2520Gaussian%2520Splatting%252C%2520recent%25202D%2520Gaussian%2520image%2520models%2520improve%2520representation%2520efficiency%252C%2520yet%2520existing%2520methods%2520struggle%2520to%2520balance%2520compression%2520ratio%2520and%2520reconstruction%2520fidelity%2520in%2520ultra-high-resolution%2520scenarios.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520SmartSplat%252C%2520a%2520highly%2520adaptive%2520and%2520feature-aware%2520GS-based%2520image%2520compression%2520framework%2520that%2520supports%2520arbitrary%2520image%2520resolutions%2520and%2520compression%2520ratios.%2520SmartSplat%2520leverages%2520image-aware%2520features%2520such%2520as%2520gradients%2520and%2520color%2520variances%252C%2520introducing%2520a%2520Gradient-Color%2520Guided%2520Variational%2520Sampling%2520strategy%2520together%2520with%2520an%2520Exclusion-based%2520Uniform%2520Sampling%2520scheme%2520to%2520improve%2520the%2520non-overlapping%2520coverage%2520of%2520Gaussian%2520primitives%2520in%2520pixel%2520space.%2520In%2520addition%252C%2520we%2520propose%2520a%2520Scale-Adaptive%2520Gaussian%2520Color%2520Sampling%2520method%2520to%2520enhance%2520color%2520initialization%2520across%2520scales.%2520Through%2520joint%2520optimization%2520of%2520spatial%2520layout%252C%2520scale%252C%2520and%2520color%2520initialization%252C%2520SmartSplat%2520efficiently%2520captures%2520both%2520local%2520structures%2520and%2520global%2520textures%2520using%2520a%2520limited%2520number%2520of%2520Gaussians%252C%2520achieving%2520high%2520reconstruction%2520quality%2520under%2520strong%2520compression.%2520Extensive%2520experiments%2520on%2520DIV8K%2520and%2520a%2520newly%2520constructed%252016K%2520dataset%2520demonstrate%2520that%2520SmartSplat%2520consistently%2520outperforms%2520state-of-the-art%2520methods%2520at%2520comparable%2520compression%2520ratios%2520and%2520exceeds%2520their%2520compression%2520limits%252C%2520showing%2520strong%2520scalability%2520and%2520practical%2520applicability.%2520The%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/lif314/SmartSplat.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SmartSplat%3A%20Feature-Smart%20Gaussians%20for%20Scalable%20Compression%20of%20Ultra-High-Resolution%20Images&entry.906535625=Linfei%20Li%20and%20Lin%20Zhang%20and%20Zhong%20Wang%20and%20Ying%20Shen&entry.1292438233=Recent%20advances%20in%20generative%20AI%20have%20accelerated%20the%20production%20of%20ultra-high-resolution%20visual%20content%2C%20posing%20significant%20challenges%20for%20efficient%20compression%20and%20real-time%20decoding%20on%20end-user%20devices.%20Inspired%20by%203D%20Gaussian%20Splatting%2C%20recent%202D%20Gaussian%20image%20models%20improve%20representation%20efficiency%2C%20yet%20existing%20methods%20struggle%20to%20balance%20compression%20ratio%20and%20reconstruction%20fidelity%20in%20ultra-high-resolution%20scenarios.%20To%20address%20this%20issue%2C%20we%20propose%20SmartSplat%2C%20a%20highly%20adaptive%20and%20feature-aware%20GS-based%20image%20compression%20framework%20that%20supports%20arbitrary%20image%20resolutions%20and%20compression%20ratios.%20SmartSplat%20leverages%20image-aware%20features%20such%20as%20gradients%20and%20color%20variances%2C%20introducing%20a%20Gradient-Color%20Guided%20Variational%20Sampling%20strategy%20together%20with%20an%20Exclusion-based%20Uniform%20Sampling%20scheme%20to%20improve%20the%20non-overlapping%20coverage%20of%20Gaussian%20primitives%20in%20pixel%20space.%20In%20addition%2C%20we%20propose%20a%20Scale-Adaptive%20Gaussian%20Color%20Sampling%20method%20to%20enhance%20color%20initialization%20across%20scales.%20Through%20joint%20optimization%20of%20spatial%20layout%2C%20scale%2C%20and%20color%20initialization%2C%20SmartSplat%20efficiently%20captures%20both%20local%20structures%20and%20global%20textures%20using%20a%20limited%20number%20of%20Gaussians%2C%20achieving%20high%20reconstruction%20quality%20under%20strong%20compression.%20Extensive%20experiments%20on%20DIV8K%20and%20a%20newly%20constructed%2016K%20dataset%20demonstrate%20that%20SmartSplat%20consistently%20outperforms%20state-of-the-art%20methods%20at%20comparable%20compression%20ratios%20and%20exceeds%20their%20compression%20limits%2C%20showing%20strong%20scalability%20and%20practical%20applicability.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/lif314/SmartSplat.&entry.1838667208=http%3A//arxiv.org/abs/2512.20377v1&entry.124074799=Read"},
{"title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "author": "Shengchao Zhou and Yuxin Chen and Yuying Ge and Wei Huang and Jiehong Lin and Ying Shan and Xiaojuan Qi", "abstract": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "link": "http://arxiv.org/abs/2512.20557v1", "date": "2025-12-23", "relevancy": 3.1501, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6608}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Reason%20in%204D%3A%20Dynamic%20Spatial%20Understanding%20for%20Vision%20Language%20Models&body=Title%3A%20Learning%20to%20Reason%20in%204D%3A%20Dynamic%20Spatial%20Understanding%20for%20Vision%20Language%20Models%0AAuthor%3A%20Shengchao%20Zhou%20and%20Yuxin%20Chen%20and%20Yuying%20Ge%20and%20Wei%20Huang%20and%20Jiehong%20Lin%20and%20Ying%20Shan%20and%20Xiaojuan%20Qi%0AAbstract%3A%20Vision-language%20models%20%28VLM%29%20excel%20at%20general%20understanding%20yet%20remain%20weak%20at%20dynamic%20spatial%20reasoning%20%28DSR%29%2C%20i.e.%2C%20reasoning%20about%20the%20evolvement%20of%20object%20geometry%20and%20relationship%20in%203D%20space%20over%20time%2C%20largely%20due%20to%20the%20scarcity%20of%20scalable%204D-aware%20training%20resources.%20To%20bridge%20this%20gap%20across%20aspects%20of%20dataset%2C%20benchmark%20and%20model%2C%20we%20introduce%20DSR%20Suite.%20First%2C%20we%20propose%20an%20automated%20pipeline%20that%20generates%20multiple-choice%20question-answer%20pairs%20from%20in-the-wild%20videos%20for%20DSR.%20By%20leveraging%20modern%20vision%20foundation%20models%2C%20the%20pipeline%20extracts%20rich%20geometric%20and%20motion%20information%2C%20including%20camera%20poses%2C%20local%20point%20clouds%2C%20object%20masks%2C%20orientations%2C%20and%203D%20trajectories.%20These%20geometric%20cues%20enable%20the%20construction%20of%20DSR-Train%20for%20learning%20and%20further%20human-refined%20DSR-Bench%20for%20evaluation.%20Compared%20with%20previous%20works%2C%20our%20data%20emphasize%20%28i%29%20in-the-wild%20video%20sources%2C%20%28ii%29%20object-%20and%20scene-level%203D%20requirements%2C%20%28iii%29%20viewpoint%20transformations%2C%20%28iv%29%20multi-object%20interactions%2C%20and%20%28v%29%20fine-grained%2C%20procedural%20answers.%20Beyond%20data%2C%20we%20propose%20a%20lightweight%20Geometry%20Selection%20Module%20%28GSM%29%20to%20seamlessly%20integrate%20geometric%20priors%20into%20VLMs%2C%20which%20condenses%20question%20semantics%20and%20extracts%20question-relevant%20knowledge%20from%20pretrained%204D%20reconstruction%20priors%20into%20a%20compact%20set%20of%20geometry%20tokens.%20This%20targeted%20extraction%20avoids%20overwhelming%20the%20model%20with%20irrelevant%20knowledge.%20Experiments%20show%20that%20integrating%20DSR-Train%20and%20GSM%20into%20Qwen2.5-VL-7B%20significantly%20enhances%20its%20dynamic%20spatial%20reasoning%20capability%2C%20while%20maintaining%20accuracy%20on%20general%20video%20understanding%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Reason%2520in%25204D%253A%2520Dynamic%2520Spatial%2520Understanding%2520for%2520Vision%2520Language%2520Models%26entry.906535625%3DShengchao%2520Zhou%2520and%2520Yuxin%2520Chen%2520and%2520Yuying%2520Ge%2520and%2520Wei%2520Huang%2520and%2520Jiehong%2520Lin%2520and%2520Ying%2520Shan%2520and%2520Xiaojuan%2520Qi%26entry.1292438233%3DVision-language%2520models%2520%2528VLM%2529%2520excel%2520at%2520general%2520understanding%2520yet%2520remain%2520weak%2520at%2520dynamic%2520spatial%2520reasoning%2520%2528DSR%2529%252C%2520i.e.%252C%2520reasoning%2520about%2520the%2520evolvement%2520of%2520object%2520geometry%2520and%2520relationship%2520in%25203D%2520space%2520over%2520time%252C%2520largely%2520due%2520to%2520the%2520scarcity%2520of%2520scalable%25204D-aware%2520training%2520resources.%2520To%2520bridge%2520this%2520gap%2520across%2520aspects%2520of%2520dataset%252C%2520benchmark%2520and%2520model%252C%2520we%2520introduce%2520DSR%2520Suite.%2520First%252C%2520we%2520propose%2520an%2520automated%2520pipeline%2520that%2520generates%2520multiple-choice%2520question-answer%2520pairs%2520from%2520in-the-wild%2520videos%2520for%2520DSR.%2520By%2520leveraging%2520modern%2520vision%2520foundation%2520models%252C%2520the%2520pipeline%2520extracts%2520rich%2520geometric%2520and%2520motion%2520information%252C%2520including%2520camera%2520poses%252C%2520local%2520point%2520clouds%252C%2520object%2520masks%252C%2520orientations%252C%2520and%25203D%2520trajectories.%2520These%2520geometric%2520cues%2520enable%2520the%2520construction%2520of%2520DSR-Train%2520for%2520learning%2520and%2520further%2520human-refined%2520DSR-Bench%2520for%2520evaluation.%2520Compared%2520with%2520previous%2520works%252C%2520our%2520data%2520emphasize%2520%2528i%2529%2520in-the-wild%2520video%2520sources%252C%2520%2528ii%2529%2520object-%2520and%2520scene-level%25203D%2520requirements%252C%2520%2528iii%2529%2520viewpoint%2520transformations%252C%2520%2528iv%2529%2520multi-object%2520interactions%252C%2520and%2520%2528v%2529%2520fine-grained%252C%2520procedural%2520answers.%2520Beyond%2520data%252C%2520we%2520propose%2520a%2520lightweight%2520Geometry%2520Selection%2520Module%2520%2528GSM%2529%2520to%2520seamlessly%2520integrate%2520geometric%2520priors%2520into%2520VLMs%252C%2520which%2520condenses%2520question%2520semantics%2520and%2520extracts%2520question-relevant%2520knowledge%2520from%2520pretrained%25204D%2520reconstruction%2520priors%2520into%2520a%2520compact%2520set%2520of%2520geometry%2520tokens.%2520This%2520targeted%2520extraction%2520avoids%2520overwhelming%2520the%2520model%2520with%2520irrelevant%2520knowledge.%2520Experiments%2520show%2520that%2520integrating%2520DSR-Train%2520and%2520GSM%2520into%2520Qwen2.5-VL-7B%2520significantly%2520enhances%2520its%2520dynamic%2520spatial%2520reasoning%2520capability%252C%2520while%2520maintaining%2520accuracy%2520on%2520general%2520video%2520understanding%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Reason%20in%204D%3A%20Dynamic%20Spatial%20Understanding%20for%20Vision%20Language%20Models&entry.906535625=Shengchao%20Zhou%20and%20Yuxin%20Chen%20and%20Yuying%20Ge%20and%20Wei%20Huang%20and%20Jiehong%20Lin%20and%20Ying%20Shan%20and%20Xiaojuan%20Qi&entry.1292438233=Vision-language%20models%20%28VLM%29%20excel%20at%20general%20understanding%20yet%20remain%20weak%20at%20dynamic%20spatial%20reasoning%20%28DSR%29%2C%20i.e.%2C%20reasoning%20about%20the%20evolvement%20of%20object%20geometry%20and%20relationship%20in%203D%20space%20over%20time%2C%20largely%20due%20to%20the%20scarcity%20of%20scalable%204D-aware%20training%20resources.%20To%20bridge%20this%20gap%20across%20aspects%20of%20dataset%2C%20benchmark%20and%20model%2C%20we%20introduce%20DSR%20Suite.%20First%2C%20we%20propose%20an%20automated%20pipeline%20that%20generates%20multiple-choice%20question-answer%20pairs%20from%20in-the-wild%20videos%20for%20DSR.%20By%20leveraging%20modern%20vision%20foundation%20models%2C%20the%20pipeline%20extracts%20rich%20geometric%20and%20motion%20information%2C%20including%20camera%20poses%2C%20local%20point%20clouds%2C%20object%20masks%2C%20orientations%2C%20and%203D%20trajectories.%20These%20geometric%20cues%20enable%20the%20construction%20of%20DSR-Train%20for%20learning%20and%20further%20human-refined%20DSR-Bench%20for%20evaluation.%20Compared%20with%20previous%20works%2C%20our%20data%20emphasize%20%28i%29%20in-the-wild%20video%20sources%2C%20%28ii%29%20object-%20and%20scene-level%203D%20requirements%2C%20%28iii%29%20viewpoint%20transformations%2C%20%28iv%29%20multi-object%20interactions%2C%20and%20%28v%29%20fine-grained%2C%20procedural%20answers.%20Beyond%20data%2C%20we%20propose%20a%20lightweight%20Geometry%20Selection%20Module%20%28GSM%29%20to%20seamlessly%20integrate%20geometric%20priors%20into%20VLMs%2C%20which%20condenses%20question%20semantics%20and%20extracts%20question-relevant%20knowledge%20from%20pretrained%204D%20reconstruction%20priors%20into%20a%20compact%20set%20of%20geometry%20tokens.%20This%20targeted%20extraction%20avoids%20overwhelming%20the%20model%20with%20irrelevant%20knowledge.%20Experiments%20show%20that%20integrating%20DSR-Train%20and%20GSM%20into%20Qwen2.5-VL-7B%20significantly%20enhances%20its%20dynamic%20spatial%20reasoning%20capability%2C%20while%20maintaining%20accuracy%20on%20general%20video%20understanding%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.20557v1&entry.124074799=Read"},
{"title": "SemanticGen: Video Generation in Semantic Space", "author": "Jianhong Bai and Xiaoshi Wu and Xintao Wang and Fu Xiao and Yuanxing Zhang and Qinghe Wang and Xiaoyu Shi and Menghan Xia and Zuozhu Liu and Haoji Hu and Pengfei Wan and Kun Gai", "abstract": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "link": "http://arxiv.org/abs/2512.20619v1", "date": "2025-12-23", "relevancy": 3.1171, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6353}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6294}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SemanticGen%3A%20Video%20Generation%20in%20Semantic%20Space&body=Title%3A%20SemanticGen%3A%20Video%20Generation%20in%20Semantic%20Space%0AAuthor%3A%20Jianhong%20Bai%20and%20Xiaoshi%20Wu%20and%20Xintao%20Wang%20and%20Fu%20Xiao%20and%20Yuanxing%20Zhang%20and%20Qinghe%20Wang%20and%20Xiaoyu%20Shi%20and%20Menghan%20Xia%20and%20Zuozhu%20Liu%20and%20Haoji%20Hu%20and%20Pengfei%20Wan%20and%20Kun%20Gai%0AAbstract%3A%20State-of-the-art%20video%20generative%20models%20typically%20learn%20the%20distribution%20of%20video%20latents%20in%20the%20VAE%20space%20and%20map%20them%20to%20pixels%20using%20a%20VAE%20decoder.%20While%20this%20approach%20can%20generate%20high-quality%20videos%2C%20it%20suffers%20from%20slow%20convergence%20and%20is%20computationally%20expensive%20when%20generating%20long%20videos.%20In%20this%20paper%2C%20we%20introduce%20SemanticGen%2C%20a%20novel%20solution%20to%20address%20these%20limitations%20by%20generating%20videos%20in%20the%20semantic%20space.%20Our%20main%20insight%20is%20that%2C%20due%20to%20the%20inherent%20redundancy%20in%20videos%2C%20the%20generation%20process%20should%20begin%20in%20a%20compact%2C%20high-level%20semantic%20space%20for%20global%20planning%2C%20followed%20by%20the%20addition%20of%20high-frequency%20details%2C%20rather%20than%20directly%20modeling%20a%20vast%20set%20of%20low-level%20video%20tokens%20using%20bi-directional%20attention.%20SemanticGen%20adopts%20a%20two-stage%20generation%20process.%20In%20the%20first%20stage%2C%20a%20diffusion%20model%20generates%20compact%20semantic%20video%20features%2C%20which%20define%20the%20global%20layout%20of%20the%20video.%20In%20the%20second%20stage%2C%20another%20diffusion%20model%20generates%20VAE%20latents%20conditioned%20on%20these%20semantic%20features%20to%20produce%20the%20final%20output.%20We%20observe%20that%20generation%20in%20the%20semantic%20space%20leads%20to%20faster%20convergence%20compared%20to%20the%20VAE%20latent%20space.%20Our%20method%20is%20also%20effective%20and%20computationally%20efficient%20when%20extended%20to%20long%20video%20generation.%20Extensive%20experiments%20demonstrate%20that%20SemanticGen%20produces%20high-quality%20videos%20and%20outperforms%20state-of-the-art%20approaches%20and%20strong%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemanticGen%253A%2520Video%2520Generation%2520in%2520Semantic%2520Space%26entry.906535625%3DJianhong%2520Bai%2520and%2520Xiaoshi%2520Wu%2520and%2520Xintao%2520Wang%2520and%2520Fu%2520Xiao%2520and%2520Yuanxing%2520Zhang%2520and%2520Qinghe%2520Wang%2520and%2520Xiaoyu%2520Shi%2520and%2520Menghan%2520Xia%2520and%2520Zuozhu%2520Liu%2520and%2520Haoji%2520Hu%2520and%2520Pengfei%2520Wan%2520and%2520Kun%2520Gai%26entry.1292438233%3DState-of-the-art%2520video%2520generative%2520models%2520typically%2520learn%2520the%2520distribution%2520of%2520video%2520latents%2520in%2520the%2520VAE%2520space%2520and%2520map%2520them%2520to%2520pixels%2520using%2520a%2520VAE%2520decoder.%2520While%2520this%2520approach%2520can%2520generate%2520high-quality%2520videos%252C%2520it%2520suffers%2520from%2520slow%2520convergence%2520and%2520is%2520computationally%2520expensive%2520when%2520generating%2520long%2520videos.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SemanticGen%252C%2520a%2520novel%2520solution%2520to%2520address%2520these%2520limitations%2520by%2520generating%2520videos%2520in%2520the%2520semantic%2520space.%2520Our%2520main%2520insight%2520is%2520that%252C%2520due%2520to%2520the%2520inherent%2520redundancy%2520in%2520videos%252C%2520the%2520generation%2520process%2520should%2520begin%2520in%2520a%2520compact%252C%2520high-level%2520semantic%2520space%2520for%2520global%2520planning%252C%2520followed%2520by%2520the%2520addition%2520of%2520high-frequency%2520details%252C%2520rather%2520than%2520directly%2520modeling%2520a%2520vast%2520set%2520of%2520low-level%2520video%2520tokens%2520using%2520bi-directional%2520attention.%2520SemanticGen%2520adopts%2520a%2520two-stage%2520generation%2520process.%2520In%2520the%2520first%2520stage%252C%2520a%2520diffusion%2520model%2520generates%2520compact%2520semantic%2520video%2520features%252C%2520which%2520define%2520the%2520global%2520layout%2520of%2520the%2520video.%2520In%2520the%2520second%2520stage%252C%2520another%2520diffusion%2520model%2520generates%2520VAE%2520latents%2520conditioned%2520on%2520these%2520semantic%2520features%2520to%2520produce%2520the%2520final%2520output.%2520We%2520observe%2520that%2520generation%2520in%2520the%2520semantic%2520space%2520leads%2520to%2520faster%2520convergence%2520compared%2520to%2520the%2520VAE%2520latent%2520space.%2520Our%2520method%2520is%2520also%2520effective%2520and%2520computationally%2520efficient%2520when%2520extended%2520to%2520long%2520video%2520generation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SemanticGen%2520produces%2520high-quality%2520videos%2520and%2520outperforms%2520state-of-the-art%2520approaches%2520and%2520strong%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemanticGen%3A%20Video%20Generation%20in%20Semantic%20Space&entry.906535625=Jianhong%20Bai%20and%20Xiaoshi%20Wu%20and%20Xintao%20Wang%20and%20Fu%20Xiao%20and%20Yuanxing%20Zhang%20and%20Qinghe%20Wang%20and%20Xiaoyu%20Shi%20and%20Menghan%20Xia%20and%20Zuozhu%20Liu%20and%20Haoji%20Hu%20and%20Pengfei%20Wan%20and%20Kun%20Gai&entry.1292438233=State-of-the-art%20video%20generative%20models%20typically%20learn%20the%20distribution%20of%20video%20latents%20in%20the%20VAE%20space%20and%20map%20them%20to%20pixels%20using%20a%20VAE%20decoder.%20While%20this%20approach%20can%20generate%20high-quality%20videos%2C%20it%20suffers%20from%20slow%20convergence%20and%20is%20computationally%20expensive%20when%20generating%20long%20videos.%20In%20this%20paper%2C%20we%20introduce%20SemanticGen%2C%20a%20novel%20solution%20to%20address%20these%20limitations%20by%20generating%20videos%20in%20the%20semantic%20space.%20Our%20main%20insight%20is%20that%2C%20due%20to%20the%20inherent%20redundancy%20in%20videos%2C%20the%20generation%20process%20should%20begin%20in%20a%20compact%2C%20high-level%20semantic%20space%20for%20global%20planning%2C%20followed%20by%20the%20addition%20of%20high-frequency%20details%2C%20rather%20than%20directly%20modeling%20a%20vast%20set%20of%20low-level%20video%20tokens%20using%20bi-directional%20attention.%20SemanticGen%20adopts%20a%20two-stage%20generation%20process.%20In%20the%20first%20stage%2C%20a%20diffusion%20model%20generates%20compact%20semantic%20video%20features%2C%20which%20define%20the%20global%20layout%20of%20the%20video.%20In%20the%20second%20stage%2C%20another%20diffusion%20model%20generates%20VAE%20latents%20conditioned%20on%20these%20semantic%20features%20to%20produce%20the%20final%20output.%20We%20observe%20that%20generation%20in%20the%20semantic%20space%20leads%20to%20faster%20convergence%20compared%20to%20the%20VAE%20latent%20space.%20Our%20method%20is%20also%20effective%20and%20computationally%20efficient%20when%20extended%20to%20long%20video%20generation.%20Extensive%20experiments%20demonstrate%20that%20SemanticGen%20produces%20high-quality%20videos%20and%20outperforms%20state-of-the-art%20approaches%20and%20strong%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2512.20619v1&entry.124074799=Read"},
{"title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model", "author": "Team Seedance and Heyi Chen and Siyan Chen and Xin Chen and Yanfei Chen and Ying Chen and Zhuo Chen and Feng Cheng and Tianheng Cheng and Xinqi Cheng and Xuyan Chi and Jian Cong and Jing Cui and Qinpeng Cui and Qide Dong and Junliang Fan and Jing Fang and Zetao Fang and Chengjian Feng and Han Feng and Mingyuan Gao and Yu Gao and Dong Guo and Qiushan Guo and Boyang Hao and Qingkai Hao and Bibo He and Qian He and Tuyen Hoang and Ruoqing Hu and Xi Hu and Weilin Huang and Zhaoyang Huang and Zhongyi Huang and Donglei Ji and Siqi Jiang and Wei Jiang and Yunpu Jiang and Zhuo Jiang and Ashley Kim and Jianan Kong and Zhichao Lai and Shanshan Lao and Yichong Leng and Ai Li and Feiya Li and Gen Li and Huixia Li and JiaShi Li and Liang Li and Ming Li and Shanshan Li and Tao Li and Xian Li and Xiaojie Li and Xiaoyang Li and Xingxing Li and Yameng Li and Yifu Li and Yiying Li and Chao Liang and Han Liang and Jianzhong Liang and Ying Liang and Zhiqiang Liang and Wang Liao and Yalin Liao and Heng Lin and Kengyu Lin and Shanchuan Lin and Xi Lin and Zhijie Lin and Feng Ling and Fangfang Liu and Gaohong Liu and Jiawei Liu and Jie Liu and Jihao Liu and Shouda Liu and Shu Liu and Sichao Liu and Songwei Liu and Xin Liu and Xue Liu and Yibo Liu and Zikun Liu and Zuxi Liu and Junlin Lyu and Lecheng Lyu and Qian Lyu and Han Mu and Xiaonan Nie and Jingzhe Ning and Xitong Pan and Yanghua Peng and Lianke Qin and Xueqiong Qu and Yuxi Ren and Kai Shen and Guang Shi and Lei Shi and Yan Song and Yinglong Song and Fan Sun and Li Sun and Renfei Sun and Yan Sun and Zeyu Sun and Wenjing Tang and Yaxue Tang and Zirui Tao and Feng Wang and Furui Wang and Jinran Wang and Junkai Wang and Ke Wang and Kexin Wang and Qingyi Wang and Rui Wang and Sen Wang and Shuai Wang and Tingru Wang and Weichen Wang and Xin Wang and Yanhui Wang and Yue Wang and Yuping Wang and Yuxuan Wang and Ziyu Wang and Guoqiang Wei and Wanru Wei and Di Wu and Guohong Wu and Hanjie Wu and Jian Wu and Jie Wu and Ruolan Wu and Xinglong Wu and Yonghui Wu and Ruiqi Xia and Liang Xiang and Fei Xiao and XueFeng Xiao and Pan Xie and Shuangyi Xie and Shuang Xu and Jinlan Xue and Shen Yan and Bangbang Yang and Ceyuan Yang and Jiaqi Yang and Runkai Yang and Tao Yang and Yang Yang and Yihang Yang and ZhiXian Yang and Ziyan Yang and Songting Yao and Yifan Yao and Zilyu Ye and Bowen Yu and Jian Yu and Chujie Yuan and Linxiao Yuan and Sichun Zeng and Weihong Zeng and Xuejiao Zeng and Yan Zeng and Chuntao Zhang and Heng Zhang and Jingjie Zhang and Kuo Zhang and Liang Zhang and Liying Zhang and Manlin Zhang and Ting Zhang and Weida Zhang and Xiaohe Zhang and Xinyan Zhang and Yan Zhang and Yuan Zhang and Zixiang Zhang and Fengxuan Zhao and Huating Zhao and Yang Zhao and Hao Zheng and Jianbin Zheng and Xiaozheng Zheng and Yangyang Zheng and Yijie Zheng and Jiexin Zhou and Jiahui Zhu and Kuan Zhu and Shenhan Zhu and Wenjia Zhu and Benhui Zou and Feilong Zuo", "abstract": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.", "link": "http://arxiv.org/abs/2512.13507v3", "date": "2025-12-23", "relevancy": 2.9518, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6033}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5856}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seedance%201.5%20pro%3A%20A%20Native%20Audio-Visual%20Joint%20Generation%20Foundation%20Model&body=Title%3A%20Seedance%201.5%20pro%3A%20A%20Native%20Audio-Visual%20Joint%20Generation%20Foundation%20Model%0AAuthor%3A%20Team%20Seedance%20and%20Heyi%20Chen%20and%20Siyan%20Chen%20and%20Xin%20Chen%20and%20Yanfei%20Chen%20and%20Ying%20Chen%20and%20Zhuo%20Chen%20and%20Feng%20Cheng%20and%20Tianheng%20Cheng%20and%20Xinqi%20Cheng%20and%20Xuyan%20Chi%20and%20Jian%20Cong%20and%20Jing%20Cui%20and%20Qinpeng%20Cui%20and%20Qide%20Dong%20and%20Junliang%20Fan%20and%20Jing%20Fang%20and%20Zetao%20Fang%20and%20Chengjian%20Feng%20and%20Han%20Feng%20and%20Mingyuan%20Gao%20and%20Yu%20Gao%20and%20Dong%20Guo%20and%20Qiushan%20Guo%20and%20Boyang%20Hao%20and%20Qingkai%20Hao%20and%20Bibo%20He%20and%20Qian%20He%20and%20Tuyen%20Hoang%20and%20Ruoqing%20Hu%20and%20Xi%20Hu%20and%20Weilin%20Huang%20and%20Zhaoyang%20Huang%20and%20Zhongyi%20Huang%20and%20Donglei%20Ji%20and%20Siqi%20Jiang%20and%20Wei%20Jiang%20and%20Yunpu%20Jiang%20and%20Zhuo%20Jiang%20and%20Ashley%20Kim%20and%20Jianan%20Kong%20and%20Zhichao%20Lai%20and%20Shanshan%20Lao%20and%20Yichong%20Leng%20and%20Ai%20Li%20and%20Feiya%20Li%20and%20Gen%20Li%20and%20Huixia%20Li%20and%20JiaShi%20Li%20and%20Liang%20Li%20and%20Ming%20Li%20and%20Shanshan%20Li%20and%20Tao%20Li%20and%20Xian%20Li%20and%20Xiaojie%20Li%20and%20Xiaoyang%20Li%20and%20Xingxing%20Li%20and%20Yameng%20Li%20and%20Yifu%20Li%20and%20Yiying%20Li%20and%20Chao%20Liang%20and%20Han%20Liang%20and%20Jianzhong%20Liang%20and%20Ying%20Liang%20and%20Zhiqiang%20Liang%20and%20Wang%20Liao%20and%20Yalin%20Liao%20and%20Heng%20Lin%20and%20Kengyu%20Lin%20and%20Shanchuan%20Lin%20and%20Xi%20Lin%20and%20Zhijie%20Lin%20and%20Feng%20Ling%20and%20Fangfang%20Liu%20and%20Gaohong%20Liu%20and%20Jiawei%20Liu%20and%20Jie%20Liu%20and%20Jihao%20Liu%20and%20Shouda%20Liu%20and%20Shu%20Liu%20and%20Sichao%20Liu%20and%20Songwei%20Liu%20and%20Xin%20Liu%20and%20Xue%20Liu%20and%20Yibo%20Liu%20and%20Zikun%20Liu%20and%20Zuxi%20Liu%20and%20Junlin%20Lyu%20and%20Lecheng%20Lyu%20and%20Qian%20Lyu%20and%20Han%20Mu%20and%20Xiaonan%20Nie%20and%20Jingzhe%20Ning%20and%20Xitong%20Pan%20and%20Yanghua%20Peng%20and%20Lianke%20Qin%20and%20Xueqiong%20Qu%20and%20Yuxi%20Ren%20and%20Kai%20Shen%20and%20Guang%20Shi%20and%20Lei%20Shi%20and%20Yan%20Song%20and%20Yinglong%20Song%20and%20Fan%20Sun%20and%20Li%20Sun%20and%20Renfei%20Sun%20and%20Yan%20Sun%20and%20Zeyu%20Sun%20and%20Wenjing%20Tang%20and%20Yaxue%20Tang%20and%20Zirui%20Tao%20and%20Feng%20Wang%20and%20Furui%20Wang%20and%20Jinran%20Wang%20and%20Junkai%20Wang%20and%20Ke%20Wang%20and%20Kexin%20Wang%20and%20Qingyi%20Wang%20and%20Rui%20Wang%20and%20Sen%20Wang%20and%20Shuai%20Wang%20and%20Tingru%20Wang%20and%20Weichen%20Wang%20and%20Xin%20Wang%20and%20Yanhui%20Wang%20and%20Yue%20Wang%20and%20Yuping%20Wang%20and%20Yuxuan%20Wang%20and%20Ziyu%20Wang%20and%20Guoqiang%20Wei%20and%20Wanru%20Wei%20and%20Di%20Wu%20and%20Guohong%20Wu%20and%20Hanjie%20Wu%20and%20Jian%20Wu%20and%20Jie%20Wu%20and%20Ruolan%20Wu%20and%20Xinglong%20Wu%20and%20Yonghui%20Wu%20and%20Ruiqi%20Xia%20and%20Liang%20Xiang%20and%20Fei%20Xiao%20and%20XueFeng%20Xiao%20and%20Pan%20Xie%20and%20Shuangyi%20Xie%20and%20Shuang%20Xu%20and%20Jinlan%20Xue%20and%20Shen%20Yan%20and%20Bangbang%20Yang%20and%20Ceyuan%20Yang%20and%20Jiaqi%20Yang%20and%20Runkai%20Yang%20and%20Tao%20Yang%20and%20Yang%20Yang%20and%20Yihang%20Yang%20and%20ZhiXian%20Yang%20and%20Ziyan%20Yang%20and%20Songting%20Yao%20and%20Yifan%20Yao%20and%20Zilyu%20Ye%20and%20Bowen%20Yu%20and%20Jian%20Yu%20and%20Chujie%20Yuan%20and%20Linxiao%20Yuan%20and%20Sichun%20Zeng%20and%20Weihong%20Zeng%20and%20Xuejiao%20Zeng%20and%20Yan%20Zeng%20and%20Chuntao%20Zhang%20and%20Heng%20Zhang%20and%20Jingjie%20Zhang%20and%20Kuo%20Zhang%20and%20Liang%20Zhang%20and%20Liying%20Zhang%20and%20Manlin%20Zhang%20and%20Ting%20Zhang%20and%20Weida%20Zhang%20and%20Xiaohe%20Zhang%20and%20Xinyan%20Zhang%20and%20Yan%20Zhang%20and%20Yuan%20Zhang%20and%20Zixiang%20Zhang%20and%20Fengxuan%20Zhao%20and%20Huating%20Zhao%20and%20Yang%20Zhao%20and%20Hao%20Zheng%20and%20Jianbin%20Zheng%20and%20Xiaozheng%20Zheng%20and%20Yangyang%20Zheng%20and%20Yijie%20Zheng%20and%20Jiexin%20Zhou%20and%20Jiahui%20Zhu%20and%20Kuan%20Zhu%20and%20Shenhan%20Zhu%20and%20Wenjia%20Zhu%20and%20Benhui%20Zou%20and%20Feilong%20Zuo%0AAbstract%3A%20Recent%20strides%20in%20video%20generation%20have%20paved%20the%20way%20for%20unified%20audio-visual%20generation.%20In%20this%20work%2C%20we%20present%20Seedance%201.5%20pro%2C%20a%20foundational%20model%20engineered%20specifically%20for%20native%2C%20joint%20audio-video%20generation.%20Leveraging%20a%20dual-branch%20Diffusion%20Transformer%20architecture%2C%20the%20model%20integrates%20a%20cross-modal%20joint%20module%20with%20a%20specialized%20multi-stage%20data%20pipeline%2C%20achieving%20exceptional%20audio-visual%20synchronization%20and%20superior%20generation%20quality.%20To%20ensure%20practical%20utility%2C%20we%20implement%20meticulous%20post-training%20optimizations%2C%20including%20Supervised%20Fine-Tuning%20%28SFT%29%20on%20high-quality%20datasets%20and%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20with%20multi-dimensional%20reward%20models.%20Furthermore%2C%20we%20introduce%20an%20acceleration%20framework%20that%20boosts%20inference%20speed%20by%20over%2010X.%20Seedance%201.5%20pro%20distinguishes%20itself%20through%20precise%20multilingual%20and%20dialect%20lip-syncing%2C%20dynamic%20cinematic%20camera%20control%2C%20and%20enhanced%20narrative%20coherence%2C%20positioning%20it%20as%20a%20robust%20engine%20for%20professional-grade%20content%20creation.%20Seedance%201.5%20pro%20is%20now%20accessible%20on%20Volcano%20Engine%20at%20https%3A//console.volcengine.com/ark/region%3Aark%2Bcn-beijing/experience/vision%3Ftype%3DGenVideo.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13507v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeedance%25201.5%2520pro%253A%2520A%2520Native%2520Audio-Visual%2520Joint%2520Generation%2520Foundation%2520Model%26entry.906535625%3DTeam%2520Seedance%2520and%2520Heyi%2520Chen%2520and%2520Siyan%2520Chen%2520and%2520Xin%2520Chen%2520and%2520Yanfei%2520Chen%2520and%2520Ying%2520Chen%2520and%2520Zhuo%2520Chen%2520and%2520Feng%2520Cheng%2520and%2520Tianheng%2520Cheng%2520and%2520Xinqi%2520Cheng%2520and%2520Xuyan%2520Chi%2520and%2520Jian%2520Cong%2520and%2520Jing%2520Cui%2520and%2520Qinpeng%2520Cui%2520and%2520Qide%2520Dong%2520and%2520Junliang%2520Fan%2520and%2520Jing%2520Fang%2520and%2520Zetao%2520Fang%2520and%2520Chengjian%2520Feng%2520and%2520Han%2520Feng%2520and%2520Mingyuan%2520Gao%2520and%2520Yu%2520Gao%2520and%2520Dong%2520Guo%2520and%2520Qiushan%2520Guo%2520and%2520Boyang%2520Hao%2520and%2520Qingkai%2520Hao%2520and%2520Bibo%2520He%2520and%2520Qian%2520He%2520and%2520Tuyen%2520Hoang%2520and%2520Ruoqing%2520Hu%2520and%2520Xi%2520Hu%2520and%2520Weilin%2520Huang%2520and%2520Zhaoyang%2520Huang%2520and%2520Zhongyi%2520Huang%2520and%2520Donglei%2520Ji%2520and%2520Siqi%2520Jiang%2520and%2520Wei%2520Jiang%2520and%2520Yunpu%2520Jiang%2520and%2520Zhuo%2520Jiang%2520and%2520Ashley%2520Kim%2520and%2520Jianan%2520Kong%2520and%2520Zhichao%2520Lai%2520and%2520Shanshan%2520Lao%2520and%2520Yichong%2520Leng%2520and%2520Ai%2520Li%2520and%2520Feiya%2520Li%2520and%2520Gen%2520Li%2520and%2520Huixia%2520Li%2520and%2520JiaShi%2520Li%2520and%2520Liang%2520Li%2520and%2520Ming%2520Li%2520and%2520Shanshan%2520Li%2520and%2520Tao%2520Li%2520and%2520Xian%2520Li%2520and%2520Xiaojie%2520Li%2520and%2520Xiaoyang%2520Li%2520and%2520Xingxing%2520Li%2520and%2520Yameng%2520Li%2520and%2520Yifu%2520Li%2520and%2520Yiying%2520Li%2520and%2520Chao%2520Liang%2520and%2520Han%2520Liang%2520and%2520Jianzhong%2520Liang%2520and%2520Ying%2520Liang%2520and%2520Zhiqiang%2520Liang%2520and%2520Wang%2520Liao%2520and%2520Yalin%2520Liao%2520and%2520Heng%2520Lin%2520and%2520Kengyu%2520Lin%2520and%2520Shanchuan%2520Lin%2520and%2520Xi%2520Lin%2520and%2520Zhijie%2520Lin%2520and%2520Feng%2520Ling%2520and%2520Fangfang%2520Liu%2520and%2520Gaohong%2520Liu%2520and%2520Jiawei%2520Liu%2520and%2520Jie%2520Liu%2520and%2520Jihao%2520Liu%2520and%2520Shouda%2520Liu%2520and%2520Shu%2520Liu%2520and%2520Sichao%2520Liu%2520and%2520Songwei%2520Liu%2520and%2520Xin%2520Liu%2520and%2520Xue%2520Liu%2520and%2520Yibo%2520Liu%2520and%2520Zikun%2520Liu%2520and%2520Zuxi%2520Liu%2520and%2520Junlin%2520Lyu%2520and%2520Lecheng%2520Lyu%2520and%2520Qian%2520Lyu%2520and%2520Han%2520Mu%2520and%2520Xiaonan%2520Nie%2520and%2520Jingzhe%2520Ning%2520and%2520Xitong%2520Pan%2520and%2520Yanghua%2520Peng%2520and%2520Lianke%2520Qin%2520and%2520Xueqiong%2520Qu%2520and%2520Yuxi%2520Ren%2520and%2520Kai%2520Shen%2520and%2520Guang%2520Shi%2520and%2520Lei%2520Shi%2520and%2520Yan%2520Song%2520and%2520Yinglong%2520Song%2520and%2520Fan%2520Sun%2520and%2520Li%2520Sun%2520and%2520Renfei%2520Sun%2520and%2520Yan%2520Sun%2520and%2520Zeyu%2520Sun%2520and%2520Wenjing%2520Tang%2520and%2520Yaxue%2520Tang%2520and%2520Zirui%2520Tao%2520and%2520Feng%2520Wang%2520and%2520Furui%2520Wang%2520and%2520Jinran%2520Wang%2520and%2520Junkai%2520Wang%2520and%2520Ke%2520Wang%2520and%2520Kexin%2520Wang%2520and%2520Qingyi%2520Wang%2520and%2520Rui%2520Wang%2520and%2520Sen%2520Wang%2520and%2520Shuai%2520Wang%2520and%2520Tingru%2520Wang%2520and%2520Weichen%2520Wang%2520and%2520Xin%2520Wang%2520and%2520Yanhui%2520Wang%2520and%2520Yue%2520Wang%2520and%2520Yuping%2520Wang%2520and%2520Yuxuan%2520Wang%2520and%2520Ziyu%2520Wang%2520and%2520Guoqiang%2520Wei%2520and%2520Wanru%2520Wei%2520and%2520Di%2520Wu%2520and%2520Guohong%2520Wu%2520and%2520Hanjie%2520Wu%2520and%2520Jian%2520Wu%2520and%2520Jie%2520Wu%2520and%2520Ruolan%2520Wu%2520and%2520Xinglong%2520Wu%2520and%2520Yonghui%2520Wu%2520and%2520Ruiqi%2520Xia%2520and%2520Liang%2520Xiang%2520and%2520Fei%2520Xiao%2520and%2520XueFeng%2520Xiao%2520and%2520Pan%2520Xie%2520and%2520Shuangyi%2520Xie%2520and%2520Shuang%2520Xu%2520and%2520Jinlan%2520Xue%2520and%2520Shen%2520Yan%2520and%2520Bangbang%2520Yang%2520and%2520Ceyuan%2520Yang%2520and%2520Jiaqi%2520Yang%2520and%2520Runkai%2520Yang%2520and%2520Tao%2520Yang%2520and%2520Yang%2520Yang%2520and%2520Yihang%2520Yang%2520and%2520ZhiXian%2520Yang%2520and%2520Ziyan%2520Yang%2520and%2520Songting%2520Yao%2520and%2520Yifan%2520Yao%2520and%2520Zilyu%2520Ye%2520and%2520Bowen%2520Yu%2520and%2520Jian%2520Yu%2520and%2520Chujie%2520Yuan%2520and%2520Linxiao%2520Yuan%2520and%2520Sichun%2520Zeng%2520and%2520Weihong%2520Zeng%2520and%2520Xuejiao%2520Zeng%2520and%2520Yan%2520Zeng%2520and%2520Chuntao%2520Zhang%2520and%2520Heng%2520Zhang%2520and%2520Jingjie%2520Zhang%2520and%2520Kuo%2520Zhang%2520and%2520Liang%2520Zhang%2520and%2520Liying%2520Zhang%2520and%2520Manlin%2520Zhang%2520and%2520Ting%2520Zhang%2520and%2520Weida%2520Zhang%2520and%2520Xiaohe%2520Zhang%2520and%2520Xinyan%2520Zhang%2520and%2520Yan%2520Zhang%2520and%2520Yuan%2520Zhang%2520and%2520Zixiang%2520Zhang%2520and%2520Fengxuan%2520Zhao%2520and%2520Huating%2520Zhao%2520and%2520Yang%2520Zhao%2520and%2520Hao%2520Zheng%2520and%2520Jianbin%2520Zheng%2520and%2520Xiaozheng%2520Zheng%2520and%2520Yangyang%2520Zheng%2520and%2520Yijie%2520Zheng%2520and%2520Jiexin%2520Zhou%2520and%2520Jiahui%2520Zhu%2520and%2520Kuan%2520Zhu%2520and%2520Shenhan%2520Zhu%2520and%2520Wenjia%2520Zhu%2520and%2520Benhui%2520Zou%2520and%2520Feilong%2520Zuo%26entry.1292438233%3DRecent%2520strides%2520in%2520video%2520generation%2520have%2520paved%2520the%2520way%2520for%2520unified%2520audio-visual%2520generation.%2520In%2520this%2520work%252C%2520we%2520present%2520Seedance%25201.5%2520pro%252C%2520a%2520foundational%2520model%2520engineered%2520specifically%2520for%2520native%252C%2520joint%2520audio-video%2520generation.%2520Leveraging%2520a%2520dual-branch%2520Diffusion%2520Transformer%2520architecture%252C%2520the%2520model%2520integrates%2520a%2520cross-modal%2520joint%2520module%2520with%2520a%2520specialized%2520multi-stage%2520data%2520pipeline%252C%2520achieving%2520exceptional%2520audio-visual%2520synchronization%2520and%2520superior%2520generation%2520quality.%2520To%2520ensure%2520practical%2520utility%252C%2520we%2520implement%2520meticulous%2520post-training%2520optimizations%252C%2520including%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520on%2520high-quality%2520datasets%2520and%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520with%2520multi-dimensional%2520reward%2520models.%2520Furthermore%252C%2520we%2520introduce%2520an%2520acceleration%2520framework%2520that%2520boosts%2520inference%2520speed%2520by%2520over%252010X.%2520Seedance%25201.5%2520pro%2520distinguishes%2520itself%2520through%2520precise%2520multilingual%2520and%2520dialect%2520lip-syncing%252C%2520dynamic%2520cinematic%2520camera%2520control%252C%2520and%2520enhanced%2520narrative%2520coherence%252C%2520positioning%2520it%2520as%2520a%2520robust%2520engine%2520for%2520professional-grade%2520content%2520creation.%2520Seedance%25201.5%2520pro%2520is%2520now%2520accessible%2520on%2520Volcano%2520Engine%2520at%2520https%253A//console.volcengine.com/ark/region%253Aark%252Bcn-beijing/experience/vision%253Ftype%253DGenVideo.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13507v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seedance%201.5%20pro%3A%20A%20Native%20Audio-Visual%20Joint%20Generation%20Foundation%20Model&entry.906535625=Team%20Seedance%20and%20Heyi%20Chen%20and%20Siyan%20Chen%20and%20Xin%20Chen%20and%20Yanfei%20Chen%20and%20Ying%20Chen%20and%20Zhuo%20Chen%20and%20Feng%20Cheng%20and%20Tianheng%20Cheng%20and%20Xinqi%20Cheng%20and%20Xuyan%20Chi%20and%20Jian%20Cong%20and%20Jing%20Cui%20and%20Qinpeng%20Cui%20and%20Qide%20Dong%20and%20Junliang%20Fan%20and%20Jing%20Fang%20and%20Zetao%20Fang%20and%20Chengjian%20Feng%20and%20Han%20Feng%20and%20Mingyuan%20Gao%20and%20Yu%20Gao%20and%20Dong%20Guo%20and%20Qiushan%20Guo%20and%20Boyang%20Hao%20and%20Qingkai%20Hao%20and%20Bibo%20He%20and%20Qian%20He%20and%20Tuyen%20Hoang%20and%20Ruoqing%20Hu%20and%20Xi%20Hu%20and%20Weilin%20Huang%20and%20Zhaoyang%20Huang%20and%20Zhongyi%20Huang%20and%20Donglei%20Ji%20and%20Siqi%20Jiang%20and%20Wei%20Jiang%20and%20Yunpu%20Jiang%20and%20Zhuo%20Jiang%20and%20Ashley%20Kim%20and%20Jianan%20Kong%20and%20Zhichao%20Lai%20and%20Shanshan%20Lao%20and%20Yichong%20Leng%20and%20Ai%20Li%20and%20Feiya%20Li%20and%20Gen%20Li%20and%20Huixia%20Li%20and%20JiaShi%20Li%20and%20Liang%20Li%20and%20Ming%20Li%20and%20Shanshan%20Li%20and%20Tao%20Li%20and%20Xian%20Li%20and%20Xiaojie%20Li%20and%20Xiaoyang%20Li%20and%20Xingxing%20Li%20and%20Yameng%20Li%20and%20Yifu%20Li%20and%20Yiying%20Li%20and%20Chao%20Liang%20and%20Han%20Liang%20and%20Jianzhong%20Liang%20and%20Ying%20Liang%20and%20Zhiqiang%20Liang%20and%20Wang%20Liao%20and%20Yalin%20Liao%20and%20Heng%20Lin%20and%20Kengyu%20Lin%20and%20Shanchuan%20Lin%20and%20Xi%20Lin%20and%20Zhijie%20Lin%20and%20Feng%20Ling%20and%20Fangfang%20Liu%20and%20Gaohong%20Liu%20and%20Jiawei%20Liu%20and%20Jie%20Liu%20and%20Jihao%20Liu%20and%20Shouda%20Liu%20and%20Shu%20Liu%20and%20Sichao%20Liu%20and%20Songwei%20Liu%20and%20Xin%20Liu%20and%20Xue%20Liu%20and%20Yibo%20Liu%20and%20Zikun%20Liu%20and%20Zuxi%20Liu%20and%20Junlin%20Lyu%20and%20Lecheng%20Lyu%20and%20Qian%20Lyu%20and%20Han%20Mu%20and%20Xiaonan%20Nie%20and%20Jingzhe%20Ning%20and%20Xitong%20Pan%20and%20Yanghua%20Peng%20and%20Lianke%20Qin%20and%20Xueqiong%20Qu%20and%20Yuxi%20Ren%20and%20Kai%20Shen%20and%20Guang%20Shi%20and%20Lei%20Shi%20and%20Yan%20Song%20and%20Yinglong%20Song%20and%20Fan%20Sun%20and%20Li%20Sun%20and%20Renfei%20Sun%20and%20Yan%20Sun%20and%20Zeyu%20Sun%20and%20Wenjing%20Tang%20and%20Yaxue%20Tang%20and%20Zirui%20Tao%20and%20Feng%20Wang%20and%20Furui%20Wang%20and%20Jinran%20Wang%20and%20Junkai%20Wang%20and%20Ke%20Wang%20and%20Kexin%20Wang%20and%20Qingyi%20Wang%20and%20Rui%20Wang%20and%20Sen%20Wang%20and%20Shuai%20Wang%20and%20Tingru%20Wang%20and%20Weichen%20Wang%20and%20Xin%20Wang%20and%20Yanhui%20Wang%20and%20Yue%20Wang%20and%20Yuping%20Wang%20and%20Yuxuan%20Wang%20and%20Ziyu%20Wang%20and%20Guoqiang%20Wei%20and%20Wanru%20Wei%20and%20Di%20Wu%20and%20Guohong%20Wu%20and%20Hanjie%20Wu%20and%20Jian%20Wu%20and%20Jie%20Wu%20and%20Ruolan%20Wu%20and%20Xinglong%20Wu%20and%20Yonghui%20Wu%20and%20Ruiqi%20Xia%20and%20Liang%20Xiang%20and%20Fei%20Xiao%20and%20XueFeng%20Xiao%20and%20Pan%20Xie%20and%20Shuangyi%20Xie%20and%20Shuang%20Xu%20and%20Jinlan%20Xue%20and%20Shen%20Yan%20and%20Bangbang%20Yang%20and%20Ceyuan%20Yang%20and%20Jiaqi%20Yang%20and%20Runkai%20Yang%20and%20Tao%20Yang%20and%20Yang%20Yang%20and%20Yihang%20Yang%20and%20ZhiXian%20Yang%20and%20Ziyan%20Yang%20and%20Songting%20Yao%20and%20Yifan%20Yao%20and%20Zilyu%20Ye%20and%20Bowen%20Yu%20and%20Jian%20Yu%20and%20Chujie%20Yuan%20and%20Linxiao%20Yuan%20and%20Sichun%20Zeng%20and%20Weihong%20Zeng%20and%20Xuejiao%20Zeng%20and%20Yan%20Zeng%20and%20Chuntao%20Zhang%20and%20Heng%20Zhang%20and%20Jingjie%20Zhang%20and%20Kuo%20Zhang%20and%20Liang%20Zhang%20and%20Liying%20Zhang%20and%20Manlin%20Zhang%20and%20Ting%20Zhang%20and%20Weida%20Zhang%20and%20Xiaohe%20Zhang%20and%20Xinyan%20Zhang%20and%20Yan%20Zhang%20and%20Yuan%20Zhang%20and%20Zixiang%20Zhang%20and%20Fengxuan%20Zhao%20and%20Huating%20Zhao%20and%20Yang%20Zhao%20and%20Hao%20Zheng%20and%20Jianbin%20Zheng%20and%20Xiaozheng%20Zheng%20and%20Yangyang%20Zheng%20and%20Yijie%20Zheng%20and%20Jiexin%20Zhou%20and%20Jiahui%20Zhu%20and%20Kuan%20Zhu%20and%20Shenhan%20Zhu%20and%20Wenjia%20Zhu%20and%20Benhui%20Zou%20and%20Feilong%20Zuo&entry.1292438233=Recent%20strides%20in%20video%20generation%20have%20paved%20the%20way%20for%20unified%20audio-visual%20generation.%20In%20this%20work%2C%20we%20present%20Seedance%201.5%20pro%2C%20a%20foundational%20model%20engineered%20specifically%20for%20native%2C%20joint%20audio-video%20generation.%20Leveraging%20a%20dual-branch%20Diffusion%20Transformer%20architecture%2C%20the%20model%20integrates%20a%20cross-modal%20joint%20module%20with%20a%20specialized%20multi-stage%20data%20pipeline%2C%20achieving%20exceptional%20audio-visual%20synchronization%20and%20superior%20generation%20quality.%20To%20ensure%20practical%20utility%2C%20we%20implement%20meticulous%20post-training%20optimizations%2C%20including%20Supervised%20Fine-Tuning%20%28SFT%29%20on%20high-quality%20datasets%20and%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20with%20multi-dimensional%20reward%20models.%20Furthermore%2C%20we%20introduce%20an%20acceleration%20framework%20that%20boosts%20inference%20speed%20by%20over%2010X.%20Seedance%201.5%20pro%20distinguishes%20itself%20through%20precise%20multilingual%20and%20dialect%20lip-syncing%2C%20dynamic%20cinematic%20camera%20control%2C%20and%20enhanced%20narrative%20coherence%2C%20positioning%20it%20as%20a%20robust%20engine%20for%20professional-grade%20content%20creation.%20Seedance%201.5%20pro%20is%20now%20accessible%20on%20Volcano%20Engine%20at%20https%3A//console.volcengine.com/ark/region%3Aark%2Bcn-beijing/experience/vision%3Ftype%3DGenVideo.&entry.1838667208=http%3A//arxiv.org/abs/2512.13507v3&entry.124074799=Read"},
{"title": "Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation", "author": "Tianrui Zhu and Shiyi Zhang and Zhirui Sun and Jingqi Tian and Yansong Tang", "abstract": "Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose Memorize-and-Generate (MAG), a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce MAG-Bench to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks.", "link": "http://arxiv.org/abs/2512.18741v2", "date": "2025-12-23", "relevancy": 2.8984, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5832}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5787}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memorize-and-Generate%3A%20Towards%20Long-Term%20Consistency%20in%20Real-Time%20Video%20Generation&body=Title%3A%20Memorize-and-Generate%3A%20Towards%20Long-Term%20Consistency%20in%20Real-Time%20Video%20Generation%0AAuthor%3A%20Tianrui%20Zhu%20and%20Shiyi%20Zhang%20and%20Zhirui%20Sun%20and%20Jingqi%20Tian%20and%20Yansong%20Tang%0AAbstract%3A%20Frame-level%20autoregressive%20%28frame-AR%29%20models%20have%20achieved%20significant%20progress%2C%20enabling%20real-time%20video%20generation%20comparable%20to%20bidirectional%20diffusion%20models%20and%20serving%20as%20a%20foundation%20for%20interactive%20world%20models%20and%20game%20engines.%20However%2C%20current%20approaches%20in%20long%20video%20generation%20typically%20rely%20on%20window%20attention%2C%20which%20naively%20discards%20historical%20context%20outside%20the%20window%2C%20leading%20to%20catastrophic%20forgetting%20and%20scene%20inconsistency%3B%20conversely%2C%20retaining%20full%20history%20incurs%20prohibitive%20memory%20costs.%20To%20address%20this%20trade-off%2C%20we%20propose%20Memorize-and-Generate%20%28MAG%29%2C%20a%20framework%20that%20decouples%20memory%20compression%20and%20frame%20generation%20into%20distinct%20tasks.%20Specifically%2C%20we%20train%20a%20memory%20model%20to%20compress%20historical%20information%20into%20a%20compact%20KV%20cache%2C%20and%20a%20separate%20generator%20model%20to%20synthesize%20subsequent%20frames%20utilizing%20this%20compressed%20representation.%20Furthermore%2C%20we%20introduce%20MAG-Bench%20to%20strictly%20evaluate%20historical%20memory%20retention.%20Extensive%20experiments%20demonstrate%20that%20MAG%20achieves%20superior%20historical%20scene%20consistency%20while%20maintaining%20competitive%20performance%20on%20standard%20video%20generation%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.18741v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemorize-and-Generate%253A%2520Towards%2520Long-Term%2520Consistency%2520in%2520Real-Time%2520Video%2520Generation%26entry.906535625%3DTianrui%2520Zhu%2520and%2520Shiyi%2520Zhang%2520and%2520Zhirui%2520Sun%2520and%2520Jingqi%2520Tian%2520and%2520Yansong%2520Tang%26entry.1292438233%3DFrame-level%2520autoregressive%2520%2528frame-AR%2529%2520models%2520have%2520achieved%2520significant%2520progress%252C%2520enabling%2520real-time%2520video%2520generation%2520comparable%2520to%2520bidirectional%2520diffusion%2520models%2520and%2520serving%2520as%2520a%2520foundation%2520for%2520interactive%2520world%2520models%2520and%2520game%2520engines.%2520However%252C%2520current%2520approaches%2520in%2520long%2520video%2520generation%2520typically%2520rely%2520on%2520window%2520attention%252C%2520which%2520naively%2520discards%2520historical%2520context%2520outside%2520the%2520window%252C%2520leading%2520to%2520catastrophic%2520forgetting%2520and%2520scene%2520inconsistency%253B%2520conversely%252C%2520retaining%2520full%2520history%2520incurs%2520prohibitive%2520memory%2520costs.%2520To%2520address%2520this%2520trade-off%252C%2520we%2520propose%2520Memorize-and-Generate%2520%2528MAG%2529%252C%2520a%2520framework%2520that%2520decouples%2520memory%2520compression%2520and%2520frame%2520generation%2520into%2520distinct%2520tasks.%2520Specifically%252C%2520we%2520train%2520a%2520memory%2520model%2520to%2520compress%2520historical%2520information%2520into%2520a%2520compact%2520KV%2520cache%252C%2520and%2520a%2520separate%2520generator%2520model%2520to%2520synthesize%2520subsequent%2520frames%2520utilizing%2520this%2520compressed%2520representation.%2520Furthermore%252C%2520we%2520introduce%2520MAG-Bench%2520to%2520strictly%2520evaluate%2520historical%2520memory%2520retention.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MAG%2520achieves%2520superior%2520historical%2520scene%2520consistency%2520while%2520maintaining%2520competitive%2520performance%2520on%2520standard%2520video%2520generation%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.18741v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memorize-and-Generate%3A%20Towards%20Long-Term%20Consistency%20in%20Real-Time%20Video%20Generation&entry.906535625=Tianrui%20Zhu%20and%20Shiyi%20Zhang%20and%20Zhirui%20Sun%20and%20Jingqi%20Tian%20and%20Yansong%20Tang&entry.1292438233=Frame-level%20autoregressive%20%28frame-AR%29%20models%20have%20achieved%20significant%20progress%2C%20enabling%20real-time%20video%20generation%20comparable%20to%20bidirectional%20diffusion%20models%20and%20serving%20as%20a%20foundation%20for%20interactive%20world%20models%20and%20game%20engines.%20However%2C%20current%20approaches%20in%20long%20video%20generation%20typically%20rely%20on%20window%20attention%2C%20which%20naively%20discards%20historical%20context%20outside%20the%20window%2C%20leading%20to%20catastrophic%20forgetting%20and%20scene%20inconsistency%3B%20conversely%2C%20retaining%20full%20history%20incurs%20prohibitive%20memory%20costs.%20To%20address%20this%20trade-off%2C%20we%20propose%20Memorize-and-Generate%20%28MAG%29%2C%20a%20framework%20that%20decouples%20memory%20compression%20and%20frame%20generation%20into%20distinct%20tasks.%20Specifically%2C%20we%20train%20a%20memory%20model%20to%20compress%20historical%20information%20into%20a%20compact%20KV%20cache%2C%20and%20a%20separate%20generator%20model%20to%20synthesize%20subsequent%20frames%20utilizing%20this%20compressed%20representation.%20Furthermore%2C%20we%20introduce%20MAG-Bench%20to%20strictly%20evaluate%20historical%20memory%20retention.%20Extensive%20experiments%20demonstrate%20that%20MAG%20achieves%20superior%20historical%20scene%20consistency%20while%20maintaining%20competitive%20performance%20on%20standard%20video%20generation%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.18741v2&entry.124074799=Read"},
{"title": "Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion", "author": "Xuanyu Hu", "abstract": "Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.", "link": "http://arxiv.org/abs/2512.20249v1", "date": "2025-12-23", "relevancy": 2.8752, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5922}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5922}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Multimodal%20Brain%20Decoding%20via%20Cross-Subject%20Soft-ROI%20Fusion&body=Title%3A%20Unified%20Multimodal%20Brain%20Decoding%20via%20Cross-Subject%20Soft-ROI%20Fusion%0AAuthor%3A%20Xuanyu%20Hu%0AAbstract%3A%20Multimodal%20brain%20decoding%20aims%20to%20reconstruct%20semantic%20information%20that%20is%20consistent%20with%20visual%20stimuli%20from%20brain%20activity%20signals%20such%20as%20fMRI%2C%20and%20then%20generate%20readable%20natural%20language%20descriptions.%20However%2C%20multimodal%20brain%20decoding%20still%20faces%20key%20challenges%20in%20cross-subject%20generalization%20and%20interpretability.%20We%20propose%20a%20BrainROI%20model%20and%20achieve%20leading-level%20results%20in%20brain-captioning%20evaluation%20on%20the%20NSD%20dataset.%20Under%20the%20cross-subject%20setting%2C%20compared%20with%20recent%20state-of-the-art%20methods%20and%20representative%20baselines%2C%20metrics%20such%20as%20BLEU-4%20and%20CIDEr%20show%20clear%20improvements.%20Firstly%2C%20to%20address%20the%20heterogeneity%20of%20functional%20brain%20topology%20across%20subjects%2C%20we%20design%20a%20new%20fMRI%20encoder.%20We%20use%20multi-atlas%20soft%20functional%20parcellations%20%28soft-ROI%29%20as%20a%20shared%20space.%20We%20extend%20the%20discrete%20ROI%20Concatenation%20strategy%20in%20MINDLLM%20to%20a%20voxel-wise%20gated%20fusion%20mechanism%20%28Voxel-gate%29.%20We%20also%20ensure%20consistent%20ROI%20mapping%20through%20global%20label%20alignment%2C%20which%20enhances%20cross-subject%20transferability.%20Secondly%2C%20to%20overcome%20the%20limitations%20of%20manual%20and%20black-box%20prompting%20methods%20in%20stability%20and%20transparency%2C%20we%20introduce%20an%20interpretable%20prompt%20optimization%20process.%20In%20a%20small-sample%20closed%20loop%2C%20we%20use%20a%20locally%20deployed%20Qwen%20model%20to%20iteratively%20generate%20and%20select%20human-readable%20prompts.%20This%20process%20improves%20the%20stability%20of%20prompt%20design%20and%20preserves%20an%20auditable%20optimization%20trajectory.%20Finally%2C%20we%20impose%20parameterized%20decoding%20constraints%20during%20inference%20to%20further%20improve%20the%20stability%20and%20quality%20of%20the%20generated%20descriptions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Multimodal%2520Brain%2520Decoding%2520via%2520Cross-Subject%2520Soft-ROI%2520Fusion%26entry.906535625%3DXuanyu%2520Hu%26entry.1292438233%3DMultimodal%2520brain%2520decoding%2520aims%2520to%2520reconstruct%2520semantic%2520information%2520that%2520is%2520consistent%2520with%2520visual%2520stimuli%2520from%2520brain%2520activity%2520signals%2520such%2520as%2520fMRI%252C%2520and%2520then%2520generate%2520readable%2520natural%2520language%2520descriptions.%2520However%252C%2520multimodal%2520brain%2520decoding%2520still%2520faces%2520key%2520challenges%2520in%2520cross-subject%2520generalization%2520and%2520interpretability.%2520We%2520propose%2520a%2520BrainROI%2520model%2520and%2520achieve%2520leading-level%2520results%2520in%2520brain-captioning%2520evaluation%2520on%2520the%2520NSD%2520dataset.%2520Under%2520the%2520cross-subject%2520setting%252C%2520compared%2520with%2520recent%2520state-of-the-art%2520methods%2520and%2520representative%2520baselines%252C%2520metrics%2520such%2520as%2520BLEU-4%2520and%2520CIDEr%2520show%2520clear%2520improvements.%2520Firstly%252C%2520to%2520address%2520the%2520heterogeneity%2520of%2520functional%2520brain%2520topology%2520across%2520subjects%252C%2520we%2520design%2520a%2520new%2520fMRI%2520encoder.%2520We%2520use%2520multi-atlas%2520soft%2520functional%2520parcellations%2520%2528soft-ROI%2529%2520as%2520a%2520shared%2520space.%2520We%2520extend%2520the%2520discrete%2520ROI%2520Concatenation%2520strategy%2520in%2520MINDLLM%2520to%2520a%2520voxel-wise%2520gated%2520fusion%2520mechanism%2520%2528Voxel-gate%2529.%2520We%2520also%2520ensure%2520consistent%2520ROI%2520mapping%2520through%2520global%2520label%2520alignment%252C%2520which%2520enhances%2520cross-subject%2520transferability.%2520Secondly%252C%2520to%2520overcome%2520the%2520limitations%2520of%2520manual%2520and%2520black-box%2520prompting%2520methods%2520in%2520stability%2520and%2520transparency%252C%2520we%2520introduce%2520an%2520interpretable%2520prompt%2520optimization%2520process.%2520In%2520a%2520small-sample%2520closed%2520loop%252C%2520we%2520use%2520a%2520locally%2520deployed%2520Qwen%2520model%2520to%2520iteratively%2520generate%2520and%2520select%2520human-readable%2520prompts.%2520This%2520process%2520improves%2520the%2520stability%2520of%2520prompt%2520design%2520and%2520preserves%2520an%2520auditable%2520optimization%2520trajectory.%2520Finally%252C%2520we%2520impose%2520parameterized%2520decoding%2520constraints%2520during%2520inference%2520to%2520further%2520improve%2520the%2520stability%2520and%2520quality%2520of%2520the%2520generated%2520descriptions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Multimodal%20Brain%20Decoding%20via%20Cross-Subject%20Soft-ROI%20Fusion&entry.906535625=Xuanyu%20Hu&entry.1292438233=Multimodal%20brain%20decoding%20aims%20to%20reconstruct%20semantic%20information%20that%20is%20consistent%20with%20visual%20stimuli%20from%20brain%20activity%20signals%20such%20as%20fMRI%2C%20and%20then%20generate%20readable%20natural%20language%20descriptions.%20However%2C%20multimodal%20brain%20decoding%20still%20faces%20key%20challenges%20in%20cross-subject%20generalization%20and%20interpretability.%20We%20propose%20a%20BrainROI%20model%20and%20achieve%20leading-level%20results%20in%20brain-captioning%20evaluation%20on%20the%20NSD%20dataset.%20Under%20the%20cross-subject%20setting%2C%20compared%20with%20recent%20state-of-the-art%20methods%20and%20representative%20baselines%2C%20metrics%20such%20as%20BLEU-4%20and%20CIDEr%20show%20clear%20improvements.%20Firstly%2C%20to%20address%20the%20heterogeneity%20of%20functional%20brain%20topology%20across%20subjects%2C%20we%20design%20a%20new%20fMRI%20encoder.%20We%20use%20multi-atlas%20soft%20functional%20parcellations%20%28soft-ROI%29%20as%20a%20shared%20space.%20We%20extend%20the%20discrete%20ROI%20Concatenation%20strategy%20in%20MINDLLM%20to%20a%20voxel-wise%20gated%20fusion%20mechanism%20%28Voxel-gate%29.%20We%20also%20ensure%20consistent%20ROI%20mapping%20through%20global%20label%20alignment%2C%20which%20enhances%20cross-subject%20transferability.%20Secondly%2C%20to%20overcome%20the%20limitations%20of%20manual%20and%20black-box%20prompting%20methods%20in%20stability%20and%20transparency%2C%20we%20introduce%20an%20interpretable%20prompt%20optimization%20process.%20In%20a%20small-sample%20closed%20loop%2C%20we%20use%20a%20locally%20deployed%20Qwen%20model%20to%20iteratively%20generate%20and%20select%20human-readable%20prompts.%20This%20process%20improves%20the%20stability%20of%20prompt%20design%20and%20preserves%20an%20auditable%20optimization%20trajectory.%20Finally%2C%20we%20impose%20parameterized%20decoding%20constraints%20during%20inference%20to%20further%20improve%20the%20stability%20and%20quality%20of%20the%20generated%20descriptions.&entry.1838667208=http%3A//arxiv.org/abs/2512.20249v1&entry.124074799=Read"},
{"title": "AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment", "author": "Anna \u0160\u00e1rov\u00e1 Mike\u0161t\u00edkov\u00e1 and M\u00e9d\u00e9ric Fourmy and Martin C\u00edfka and Josef Sivic and Vladimir Petrik", "abstract": "Single-view RGB model-based object pose estimation methods achieve strong generalization but are fundamentally limited by depth ambiguity, clutter, and occlusions. Multi-view pose estimation methods have the potential to solve these issues, but existing works rely on precise single-view pose estimates or lack generalization to unseen objects. We address these challenges via the following three contributions. First, we introduce AlignPose, a 6D object pose estimation method that aggregates information from multiple extrinsically calibrated RGB views and does not require any object-specific training or symmetry annotation. Second, the key component of this approach is a new multi-view feature-metric refinement specifically designed for object pose. It optimizes a single, consistent world-frame object pose minimizing the feature discrepancy between on-the-fly rendered object features and observed image features across all views simultaneously. Third, we report extensive experiments on four datasets (YCB-V, T-LESS, ITODD-MV, HouseCat6D) using the BOP benchmark evaluation and show that AlignPose outperforms other published methods, especially on challenging industrial datasets where multiple views are readily available in practice.", "link": "http://arxiv.org/abs/2512.20538v1", "date": "2025-12-23", "relevancy": 2.8715, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6022}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5616}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignPose%3A%20Generalizable%206D%20Pose%20Estimation%20via%20Multi-view%20Feature-metric%20Alignment&body=Title%3A%20AlignPose%3A%20Generalizable%206D%20Pose%20Estimation%20via%20Multi-view%20Feature-metric%20Alignment%0AAuthor%3A%20Anna%20%C5%A0%C3%A1rov%C3%A1%20Mike%C5%A1t%C3%ADkov%C3%A1%20and%20M%C3%A9d%C3%A9ric%20Fourmy%20and%20Martin%20C%C3%ADfka%20and%20Josef%20Sivic%20and%20Vladimir%20Petrik%0AAbstract%3A%20Single-view%20RGB%20model-based%20object%20pose%20estimation%20methods%20achieve%20strong%20generalization%20but%20are%20fundamentally%20limited%20by%20depth%20ambiguity%2C%20clutter%2C%20and%20occlusions.%20Multi-view%20pose%20estimation%20methods%20have%20the%20potential%20to%20solve%20these%20issues%2C%20but%20existing%20works%20rely%20on%20precise%20single-view%20pose%20estimates%20or%20lack%20generalization%20to%20unseen%20objects.%20We%20address%20these%20challenges%20via%20the%20following%20three%20contributions.%20First%2C%20we%20introduce%20AlignPose%2C%20a%206D%20object%20pose%20estimation%20method%20that%20aggregates%20information%20from%20multiple%20extrinsically%20calibrated%20RGB%20views%20and%20does%20not%20require%20any%20object-specific%20training%20or%20symmetry%20annotation.%20Second%2C%20the%20key%20component%20of%20this%20approach%20is%20a%20new%20multi-view%20feature-metric%20refinement%20specifically%20designed%20for%20object%20pose.%20It%20optimizes%20a%20single%2C%20consistent%20world-frame%20object%20pose%20minimizing%20the%20feature%20discrepancy%20between%20on-the-fly%20rendered%20object%20features%20and%20observed%20image%20features%20across%20all%20views%20simultaneously.%20Third%2C%20we%20report%20extensive%20experiments%20on%20four%20datasets%20%28YCB-V%2C%20T-LESS%2C%20ITODD-MV%2C%20HouseCat6D%29%20using%20the%20BOP%20benchmark%20evaluation%20and%20show%20that%20AlignPose%20outperforms%20other%20published%20methods%2C%20especially%20on%20challenging%20industrial%20datasets%20where%20multiple%20views%20are%20readily%20available%20in%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignPose%253A%2520Generalizable%25206D%2520Pose%2520Estimation%2520via%2520Multi-view%2520Feature-metric%2520Alignment%26entry.906535625%3DAnna%2520%25C5%25A0%25C3%25A1rov%25C3%25A1%2520Mike%25C5%25A1t%25C3%25ADkov%25C3%25A1%2520and%2520M%25C3%25A9d%25C3%25A9ric%2520Fourmy%2520and%2520Martin%2520C%25C3%25ADfka%2520and%2520Josef%2520Sivic%2520and%2520Vladimir%2520Petrik%26entry.1292438233%3DSingle-view%2520RGB%2520model-based%2520object%2520pose%2520estimation%2520methods%2520achieve%2520strong%2520generalization%2520but%2520are%2520fundamentally%2520limited%2520by%2520depth%2520ambiguity%252C%2520clutter%252C%2520and%2520occlusions.%2520Multi-view%2520pose%2520estimation%2520methods%2520have%2520the%2520potential%2520to%2520solve%2520these%2520issues%252C%2520but%2520existing%2520works%2520rely%2520on%2520precise%2520single-view%2520pose%2520estimates%2520or%2520lack%2520generalization%2520to%2520unseen%2520objects.%2520We%2520address%2520these%2520challenges%2520via%2520the%2520following%2520three%2520contributions.%2520First%252C%2520we%2520introduce%2520AlignPose%252C%2520a%25206D%2520object%2520pose%2520estimation%2520method%2520that%2520aggregates%2520information%2520from%2520multiple%2520extrinsically%2520calibrated%2520RGB%2520views%2520and%2520does%2520not%2520require%2520any%2520object-specific%2520training%2520or%2520symmetry%2520annotation.%2520Second%252C%2520the%2520key%2520component%2520of%2520this%2520approach%2520is%2520a%2520new%2520multi-view%2520feature-metric%2520refinement%2520specifically%2520designed%2520for%2520object%2520pose.%2520It%2520optimizes%2520a%2520single%252C%2520consistent%2520world-frame%2520object%2520pose%2520minimizing%2520the%2520feature%2520discrepancy%2520between%2520on-the-fly%2520rendered%2520object%2520features%2520and%2520observed%2520image%2520features%2520across%2520all%2520views%2520simultaneously.%2520Third%252C%2520we%2520report%2520extensive%2520experiments%2520on%2520four%2520datasets%2520%2528YCB-V%252C%2520T-LESS%252C%2520ITODD-MV%252C%2520HouseCat6D%2529%2520using%2520the%2520BOP%2520benchmark%2520evaluation%2520and%2520show%2520that%2520AlignPose%2520outperforms%2520other%2520published%2520methods%252C%2520especially%2520on%2520challenging%2520industrial%2520datasets%2520where%2520multiple%2520views%2520are%2520readily%2520available%2520in%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignPose%3A%20Generalizable%206D%20Pose%20Estimation%20via%20Multi-view%20Feature-metric%20Alignment&entry.906535625=Anna%20%C5%A0%C3%A1rov%C3%A1%20Mike%C5%A1t%C3%ADkov%C3%A1%20and%20M%C3%A9d%C3%A9ric%20Fourmy%20and%20Martin%20C%C3%ADfka%20and%20Josef%20Sivic%20and%20Vladimir%20Petrik&entry.1292438233=Single-view%20RGB%20model-based%20object%20pose%20estimation%20methods%20achieve%20strong%20generalization%20but%20are%20fundamentally%20limited%20by%20depth%20ambiguity%2C%20clutter%2C%20and%20occlusions.%20Multi-view%20pose%20estimation%20methods%20have%20the%20potential%20to%20solve%20these%20issues%2C%20but%20existing%20works%20rely%20on%20precise%20single-view%20pose%20estimates%20or%20lack%20generalization%20to%20unseen%20objects.%20We%20address%20these%20challenges%20via%20the%20following%20three%20contributions.%20First%2C%20we%20introduce%20AlignPose%2C%20a%206D%20object%20pose%20estimation%20method%20that%20aggregates%20information%20from%20multiple%20extrinsically%20calibrated%20RGB%20views%20and%20does%20not%20require%20any%20object-specific%20training%20or%20symmetry%20annotation.%20Second%2C%20the%20key%20component%20of%20this%20approach%20is%20a%20new%20multi-view%20feature-metric%20refinement%20specifically%20designed%20for%20object%20pose.%20It%20optimizes%20a%20single%2C%20consistent%20world-frame%20object%20pose%20minimizing%20the%20feature%20discrepancy%20between%20on-the-fly%20rendered%20object%20features%20and%20observed%20image%20features%20across%20all%20views%20simultaneously.%20Third%2C%20we%20report%20extensive%20experiments%20on%20four%20datasets%20%28YCB-V%2C%20T-LESS%2C%20ITODD-MV%2C%20HouseCat6D%29%20using%20the%20BOP%20benchmark%20evaluation%20and%20show%20that%20AlignPose%20outperforms%20other%20published%20methods%2C%20especially%20on%20challenging%20industrial%20datasets%20where%20multiple%20views%20are%20readily%20available%20in%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2512.20538v1&entry.124074799=Read"},
{"title": "SirenPose: Dynamic Scene Reconstruction via Geometric Supervision", "author": "Kaitong Cai and Jensen Zhang and Jing Yang and Keze Wang", "abstract": "We introduce SirenPose, a geometry-aware loss formulation that integrates the periodic activation properties of sinusoidal representation networks with keypoint-based geometric supervision, enabling accurate and temporally consistent reconstruction of dynamic 3D scenes from monocular videos. Existing approaches often struggle with motion fidelity and spatiotemporal coherence in challenging settings involving fast motion, multi-object interaction, occlusion, and rapid scene changes. SirenPose incorporates physics inspired constraints to enforce coherent keypoint predictions across both spatial and temporal dimensions, while leveraging high frequency signal modeling to capture fine grained geometric details. We further expand the UniKPT dataset to 600,000 annotated instances and integrate graph neural networks to model keypoint relationships and structural correlations. Extensive experiments on benchmarks including Sintel, Bonn, and DAVIS demonstrate that SirenPose consistently outperforms state-of-the-art methods. On DAVIS, SirenPose achieves a 17.8 percent reduction in FVD, a 28.7 percent reduction in FID, and a 6.0 percent improvement in LPIPS compared to MoSCA. It also improves temporal consistency, geometric accuracy, user score, and motion smoothness. In pose estimation, SirenPose outperforms Monst3R with lower absolute trajectory error as well as reduced translational and rotational relative pose error, highlighting its effectiveness in handling rapid motion, complex dynamics, and physically plausible reconstruction.", "link": "http://arxiv.org/abs/2512.20531v1", "date": "2025-12-23", "relevancy": 2.8601, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5947}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5687}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SirenPose%3A%20Dynamic%20Scene%20Reconstruction%20via%20Geometric%20Supervision&body=Title%3A%20SirenPose%3A%20Dynamic%20Scene%20Reconstruction%20via%20Geometric%20Supervision%0AAuthor%3A%20Kaitong%20Cai%20and%20Jensen%20Zhang%20and%20Jing%20Yang%20and%20Keze%20Wang%0AAbstract%3A%20We%20introduce%20SirenPose%2C%20a%20geometry-aware%20loss%20formulation%20that%20integrates%20the%20periodic%20activation%20properties%20of%20sinusoidal%20representation%20networks%20with%20keypoint-based%20geometric%20supervision%2C%20enabling%20accurate%20and%20temporally%20consistent%20reconstruction%20of%20dynamic%203D%20scenes%20from%20monocular%20videos.%20Existing%20approaches%20often%20struggle%20with%20motion%20fidelity%20and%20spatiotemporal%20coherence%20in%20challenging%20settings%20involving%20fast%20motion%2C%20multi-object%20interaction%2C%20occlusion%2C%20and%20rapid%20scene%20changes.%20SirenPose%20incorporates%20physics%20inspired%20constraints%20to%20enforce%20coherent%20keypoint%20predictions%20across%20both%20spatial%20and%20temporal%20dimensions%2C%20while%20leveraging%20high%20frequency%20signal%20modeling%20to%20capture%20fine%20grained%20geometric%20details.%20We%20further%20expand%20the%20UniKPT%20dataset%20to%20600%2C000%20annotated%20instances%20and%20integrate%20graph%20neural%20networks%20to%20model%20keypoint%20relationships%20and%20structural%20correlations.%20Extensive%20experiments%20on%20benchmarks%20including%20Sintel%2C%20Bonn%2C%20and%20DAVIS%20demonstrate%20that%20SirenPose%20consistently%20outperforms%20state-of-the-art%20methods.%20On%20DAVIS%2C%20SirenPose%20achieves%20a%2017.8%20percent%20reduction%20in%20FVD%2C%20a%2028.7%20percent%20reduction%20in%20FID%2C%20and%20a%206.0%20percent%20improvement%20in%20LPIPS%20compared%20to%20MoSCA.%20It%20also%20improves%20temporal%20consistency%2C%20geometric%20accuracy%2C%20user%20score%2C%20and%20motion%20smoothness.%20In%20pose%20estimation%2C%20SirenPose%20outperforms%20Monst3R%20with%20lower%20absolute%20trajectory%20error%20as%20well%20as%20reduced%20translational%20and%20rotational%20relative%20pose%20error%2C%20highlighting%20its%20effectiveness%20in%20handling%20rapid%20motion%2C%20complex%20dynamics%2C%20and%20physically%20plausible%20reconstruction.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSirenPose%253A%2520Dynamic%2520Scene%2520Reconstruction%2520via%2520Geometric%2520Supervision%26entry.906535625%3DKaitong%2520Cai%2520and%2520Jensen%2520Zhang%2520and%2520Jing%2520Yang%2520and%2520Keze%2520Wang%26entry.1292438233%3DWe%2520introduce%2520SirenPose%252C%2520a%2520geometry-aware%2520loss%2520formulation%2520that%2520integrates%2520the%2520periodic%2520activation%2520properties%2520of%2520sinusoidal%2520representation%2520networks%2520with%2520keypoint-based%2520geometric%2520supervision%252C%2520enabling%2520accurate%2520and%2520temporally%2520consistent%2520reconstruction%2520of%2520dynamic%25203D%2520scenes%2520from%2520monocular%2520videos.%2520Existing%2520approaches%2520often%2520struggle%2520with%2520motion%2520fidelity%2520and%2520spatiotemporal%2520coherence%2520in%2520challenging%2520settings%2520involving%2520fast%2520motion%252C%2520multi-object%2520interaction%252C%2520occlusion%252C%2520and%2520rapid%2520scene%2520changes.%2520SirenPose%2520incorporates%2520physics%2520inspired%2520constraints%2520to%2520enforce%2520coherent%2520keypoint%2520predictions%2520across%2520both%2520spatial%2520and%2520temporal%2520dimensions%252C%2520while%2520leveraging%2520high%2520frequency%2520signal%2520modeling%2520to%2520capture%2520fine%2520grained%2520geometric%2520details.%2520We%2520further%2520expand%2520the%2520UniKPT%2520dataset%2520to%2520600%252C000%2520annotated%2520instances%2520and%2520integrate%2520graph%2520neural%2520networks%2520to%2520model%2520keypoint%2520relationships%2520and%2520structural%2520correlations.%2520Extensive%2520experiments%2520on%2520benchmarks%2520including%2520Sintel%252C%2520Bonn%252C%2520and%2520DAVIS%2520demonstrate%2520that%2520SirenPose%2520consistently%2520outperforms%2520state-of-the-art%2520methods.%2520On%2520DAVIS%252C%2520SirenPose%2520achieves%2520a%252017.8%2520percent%2520reduction%2520in%2520FVD%252C%2520a%252028.7%2520percent%2520reduction%2520in%2520FID%252C%2520and%2520a%25206.0%2520percent%2520improvement%2520in%2520LPIPS%2520compared%2520to%2520MoSCA.%2520It%2520also%2520improves%2520temporal%2520consistency%252C%2520geometric%2520accuracy%252C%2520user%2520score%252C%2520and%2520motion%2520smoothness.%2520In%2520pose%2520estimation%252C%2520SirenPose%2520outperforms%2520Monst3R%2520with%2520lower%2520absolute%2520trajectory%2520error%2520as%2520well%2520as%2520reduced%2520translational%2520and%2520rotational%2520relative%2520pose%2520error%252C%2520highlighting%2520its%2520effectiveness%2520in%2520handling%2520rapid%2520motion%252C%2520complex%2520dynamics%252C%2520and%2520physically%2520plausible%2520reconstruction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SirenPose%3A%20Dynamic%20Scene%20Reconstruction%20via%20Geometric%20Supervision&entry.906535625=Kaitong%20Cai%20and%20Jensen%20Zhang%20and%20Jing%20Yang%20and%20Keze%20Wang&entry.1292438233=We%20introduce%20SirenPose%2C%20a%20geometry-aware%20loss%20formulation%20that%20integrates%20the%20periodic%20activation%20properties%20of%20sinusoidal%20representation%20networks%20with%20keypoint-based%20geometric%20supervision%2C%20enabling%20accurate%20and%20temporally%20consistent%20reconstruction%20of%20dynamic%203D%20scenes%20from%20monocular%20videos.%20Existing%20approaches%20often%20struggle%20with%20motion%20fidelity%20and%20spatiotemporal%20coherence%20in%20challenging%20settings%20involving%20fast%20motion%2C%20multi-object%20interaction%2C%20occlusion%2C%20and%20rapid%20scene%20changes.%20SirenPose%20incorporates%20physics%20inspired%20constraints%20to%20enforce%20coherent%20keypoint%20predictions%20across%20both%20spatial%20and%20temporal%20dimensions%2C%20while%20leveraging%20high%20frequency%20signal%20modeling%20to%20capture%20fine%20grained%20geometric%20details.%20We%20further%20expand%20the%20UniKPT%20dataset%20to%20600%2C000%20annotated%20instances%20and%20integrate%20graph%20neural%20networks%20to%20model%20keypoint%20relationships%20and%20structural%20correlations.%20Extensive%20experiments%20on%20benchmarks%20including%20Sintel%2C%20Bonn%2C%20and%20DAVIS%20demonstrate%20that%20SirenPose%20consistently%20outperforms%20state-of-the-art%20methods.%20On%20DAVIS%2C%20SirenPose%20achieves%20a%2017.8%20percent%20reduction%20in%20FVD%2C%20a%2028.7%20percent%20reduction%20in%20FID%2C%20and%20a%206.0%20percent%20improvement%20in%20LPIPS%20compared%20to%20MoSCA.%20It%20also%20improves%20temporal%20consistency%2C%20geometric%20accuracy%2C%20user%20score%2C%20and%20motion%20smoothness.%20In%20pose%20estimation%2C%20SirenPose%20outperforms%20Monst3R%20with%20lower%20absolute%20trajectory%20error%20as%20well%20as%20reduced%20translational%20and%20rotational%20relative%20pose%20error%2C%20highlighting%20its%20effectiveness%20in%20handling%20rapid%20motion%2C%20complex%20dynamics%2C%20and%20physically%20plausible%20reconstruction.&entry.1838667208=http%3A//arxiv.org/abs/2512.20531v1&entry.124074799=Read"},
{"title": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems", "author": "YuChe Hsu and AnJui Wang and TsaiChing Ni and YuanFu Yang", "abstract": "We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.", "link": "http://arxiv.org/abs/2512.20387v1", "date": "2025-12-23", "relevancy": 2.8542, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5855}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5855}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Digital%20Twins%3A%20Vision-Language%20Simulation%20Models%20for%20Executable%20Industrial%20Systems&body=Title%3A%20Generative%20Digital%20Twins%3A%20Vision-Language%20Simulation%20Models%20for%20Executable%20Industrial%20Systems%0AAuthor%3A%20YuChe%20Hsu%20and%20AnJui%20Wang%20and%20TsaiChing%20Ni%20and%20YuanFu%20Yang%0AAbstract%3A%20We%20propose%20a%20Vision-Language%20Simulation%20Model%20%28VLSM%29%20that%20unifies%20visual%20and%20textual%20understanding%20to%20synthesize%20executable%20FlexScript%20from%20layout%20sketches%20and%20natural-language%20prompts%2C%20enabling%20cross-modal%20reasoning%20for%20industrial%20simulation%20systems.%20To%20support%20this%20new%20paradigm%2C%20the%20study%20constructs%20the%20first%20large-scale%20dataset%20for%20generative%20digital%20twins%2C%20comprising%20over%20120%2C000%20prompt-sketch-code%20triplets%20that%20enable%20multimodal%20learning%20between%20textual%20descriptions%2C%20spatial%20structures%2C%20and%20simulation%20logic.%20In%20parallel%2C%20three%20novel%20evaluation%20metrics%2C%20Structural%20Validity%20Rate%20%28SVR%29%2C%20Parameter%20Match%20Rate%20%28PMR%29%2C%20and%20Execution%20Success%20Rate%20%28ESR%29%2C%20are%20proposed%20specifically%20for%20this%20task%20to%20comprehensively%20evaluate%20structural%20integrity%2C%20parameter%20fidelity%2C%20and%20simulator%20executability.%20Through%20systematic%20ablation%20across%20vision%20encoders%2C%20connectors%2C%20and%20code-pretrained%20language%20backbones%2C%20the%20proposed%20models%20achieve%20near-perfect%20structural%20accuracy%20and%20high%20execution%20robustness.%20This%20work%20establishes%20a%20foundation%20for%20generative%20digital%20twins%20that%20integrate%20visual%20reasoning%20and%20language%20understanding%20into%20executable%20industrial%20simulation%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Digital%2520Twins%253A%2520Vision-Language%2520Simulation%2520Models%2520for%2520Executable%2520Industrial%2520Systems%26entry.906535625%3DYuChe%2520Hsu%2520and%2520AnJui%2520Wang%2520and%2520TsaiChing%2520Ni%2520and%2520YuanFu%2520Yang%26entry.1292438233%3DWe%2520propose%2520a%2520Vision-Language%2520Simulation%2520Model%2520%2528VLSM%2529%2520that%2520unifies%2520visual%2520and%2520textual%2520understanding%2520to%2520synthesize%2520executable%2520FlexScript%2520from%2520layout%2520sketches%2520and%2520natural-language%2520prompts%252C%2520enabling%2520cross-modal%2520reasoning%2520for%2520industrial%2520simulation%2520systems.%2520To%2520support%2520this%2520new%2520paradigm%252C%2520the%2520study%2520constructs%2520the%2520first%2520large-scale%2520dataset%2520for%2520generative%2520digital%2520twins%252C%2520comprising%2520over%2520120%252C000%2520prompt-sketch-code%2520triplets%2520that%2520enable%2520multimodal%2520learning%2520between%2520textual%2520descriptions%252C%2520spatial%2520structures%252C%2520and%2520simulation%2520logic.%2520In%2520parallel%252C%2520three%2520novel%2520evaluation%2520metrics%252C%2520Structural%2520Validity%2520Rate%2520%2528SVR%2529%252C%2520Parameter%2520Match%2520Rate%2520%2528PMR%2529%252C%2520and%2520Execution%2520Success%2520Rate%2520%2528ESR%2529%252C%2520are%2520proposed%2520specifically%2520for%2520this%2520task%2520to%2520comprehensively%2520evaluate%2520structural%2520integrity%252C%2520parameter%2520fidelity%252C%2520and%2520simulator%2520executability.%2520Through%2520systematic%2520ablation%2520across%2520vision%2520encoders%252C%2520connectors%252C%2520and%2520code-pretrained%2520language%2520backbones%252C%2520the%2520proposed%2520models%2520achieve%2520near-perfect%2520structural%2520accuracy%2520and%2520high%2520execution%2520robustness.%2520This%2520work%2520establishes%2520a%2520foundation%2520for%2520generative%2520digital%2520twins%2520that%2520integrate%2520visual%2520reasoning%2520and%2520language%2520understanding%2520into%2520executable%2520industrial%2520simulation%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Digital%20Twins%3A%20Vision-Language%20Simulation%20Models%20for%20Executable%20Industrial%20Systems&entry.906535625=YuChe%20Hsu%20and%20AnJui%20Wang%20and%20TsaiChing%20Ni%20and%20YuanFu%20Yang&entry.1292438233=We%20propose%20a%20Vision-Language%20Simulation%20Model%20%28VLSM%29%20that%20unifies%20visual%20and%20textual%20understanding%20to%20synthesize%20executable%20FlexScript%20from%20layout%20sketches%20and%20natural-language%20prompts%2C%20enabling%20cross-modal%20reasoning%20for%20industrial%20simulation%20systems.%20To%20support%20this%20new%20paradigm%2C%20the%20study%20constructs%20the%20first%20large-scale%20dataset%20for%20generative%20digital%20twins%2C%20comprising%20over%20120%2C000%20prompt-sketch-code%20triplets%20that%20enable%20multimodal%20learning%20between%20textual%20descriptions%2C%20spatial%20structures%2C%20and%20simulation%20logic.%20In%20parallel%2C%20three%20novel%20evaluation%20metrics%2C%20Structural%20Validity%20Rate%20%28SVR%29%2C%20Parameter%20Match%20Rate%20%28PMR%29%2C%20and%20Execution%20Success%20Rate%20%28ESR%29%2C%20are%20proposed%20specifically%20for%20this%20task%20to%20comprehensively%20evaluate%20structural%20integrity%2C%20parameter%20fidelity%2C%20and%20simulator%20executability.%20Through%20systematic%20ablation%20across%20vision%20encoders%2C%20connectors%2C%20and%20code-pretrained%20language%20backbones%2C%20the%20proposed%20models%20achieve%20near-perfect%20structural%20accuracy%20and%20high%20execution%20robustness.%20This%20work%20establishes%20a%20foundation%20for%20generative%20digital%20twins%20that%20integrate%20visual%20reasoning%20and%20language%20understanding%20into%20executable%20industrial%20simulation%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.20387v1&entry.124074799=Read"},
{"title": "FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models", "author": "Kaitong Cai and Jusheng Zhang and Jing Yang and Yijia Fan and Pengtao Xie and Jian Wang and Keze Wang", "abstract": "Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.\n  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.\n  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.", "link": "http://arxiv.org/abs/2512.20561v1", "date": "2025-12-23", "relevancy": 2.7555, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5606}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlashVLM%3A%20Text-Guided%20Visual%20Token%20Selection%20for%20Large%20Multimodal%20Models&body=Title%3A%20FlashVLM%3A%20Text-Guided%20Visual%20Token%20Selection%20for%20Large%20Multimodal%20Models%0AAuthor%3A%20Kaitong%20Cai%20and%20Jusheng%20Zhang%20and%20Jing%20Yang%20and%20Yijia%20Fan%20and%20Pengtao%20Xie%20and%20Jian%20Wang%20and%20Keze%20Wang%0AAbstract%3A%20Large%20vision-language%20models%20%28VLMs%29%20typically%20process%20hundreds%20or%20thousands%20of%20visual%20tokens%20per%20image%20or%20video%20frame%2C%20incurring%20quadratic%20attention%20cost%20and%20substantial%20redundancy.%20Existing%20token%20reduction%20methods%20often%20ignore%20the%20textual%20query%20or%20rely%20on%20deep%20attention%20maps%2C%20whose%20instability%20under%20aggressive%20pruning%20leads%20to%20degraded%20semantic%20alignment.%0A%20%20We%20propose%20FlashVLM%2C%20a%20text%20guided%20visual%20token%20selection%20framework%20that%20dynamically%20adapts%20visual%20inputs%20to%20the%20query.%20Instead%20of%20relying%20on%20noisy%20attention%20weights%2C%20FlashVLM%20computes%20an%20explicit%20cross%20modal%20similarity%20between%20projected%20image%20tokens%20and%20normalized%20text%20embeddings%20in%20the%20language%20model%20space.%20This%20extrinsic%20relevance%20is%20fused%20with%20intrinsic%20visual%20saliency%20using%20log%20domain%20weighting%20and%20temperature%20controlled%20sharpening.%20In%20addition%2C%20a%20diversity%20preserving%20partition%20retains%20a%20minimal%20yet%20representative%20set%20of%20background%20tokens%20to%20maintain%20global%20context.%0A%20%20Under%20identical%20token%20budgets%20and%20evaluation%20protocols%2C%20FlashVLM%20achieves%20beyond%20lossless%20compression%2C%20slightly%20surpassing%20the%20unpruned%20baseline%20while%20pruning%20up%20to%2077.8%20percent%20of%20visual%20tokens%20on%20LLaVA%201.5%2C%20and%20maintaining%2092.8%20percent%20accuracy%20even%20under%2094.4%20percent%20compression.%20Extensive%20experiments%20on%2014%20image%20and%20video%20benchmarks%20demonstrate%20that%20FlashVLM%20delivers%20state%20of%20the%20art%20efficiency%20performance%20trade%20offs%20while%20maintaining%20strong%20robustness%20and%20generalization%20across%20mainstream%20VLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlashVLM%253A%2520Text-Guided%2520Visual%2520Token%2520Selection%2520for%2520Large%2520Multimodal%2520Models%26entry.906535625%3DKaitong%2520Cai%2520and%2520Jusheng%2520Zhang%2520and%2520Jing%2520Yang%2520and%2520Yijia%2520Fan%2520and%2520Pengtao%2520Xie%2520and%2520Jian%2520Wang%2520and%2520Keze%2520Wang%26entry.1292438233%3DLarge%2520vision-language%2520models%2520%2528VLMs%2529%2520typically%2520process%2520hundreds%2520or%2520thousands%2520of%2520visual%2520tokens%2520per%2520image%2520or%2520video%2520frame%252C%2520incurring%2520quadratic%2520attention%2520cost%2520and%2520substantial%2520redundancy.%2520Existing%2520token%2520reduction%2520methods%2520often%2520ignore%2520the%2520textual%2520query%2520or%2520rely%2520on%2520deep%2520attention%2520maps%252C%2520whose%2520instability%2520under%2520aggressive%2520pruning%2520leads%2520to%2520degraded%2520semantic%2520alignment.%250A%2520%2520We%2520propose%2520FlashVLM%252C%2520a%2520text%2520guided%2520visual%2520token%2520selection%2520framework%2520that%2520dynamically%2520adapts%2520visual%2520inputs%2520to%2520the%2520query.%2520Instead%2520of%2520relying%2520on%2520noisy%2520attention%2520weights%252C%2520FlashVLM%2520computes%2520an%2520explicit%2520cross%2520modal%2520similarity%2520between%2520projected%2520image%2520tokens%2520and%2520normalized%2520text%2520embeddings%2520in%2520the%2520language%2520model%2520space.%2520This%2520extrinsic%2520relevance%2520is%2520fused%2520with%2520intrinsic%2520visual%2520saliency%2520using%2520log%2520domain%2520weighting%2520and%2520temperature%2520controlled%2520sharpening.%2520In%2520addition%252C%2520a%2520diversity%2520preserving%2520partition%2520retains%2520a%2520minimal%2520yet%2520representative%2520set%2520of%2520background%2520tokens%2520to%2520maintain%2520global%2520context.%250A%2520%2520Under%2520identical%2520token%2520budgets%2520and%2520evaluation%2520protocols%252C%2520FlashVLM%2520achieves%2520beyond%2520lossless%2520compression%252C%2520slightly%2520surpassing%2520the%2520unpruned%2520baseline%2520while%2520pruning%2520up%2520to%252077.8%2520percent%2520of%2520visual%2520tokens%2520on%2520LLaVA%25201.5%252C%2520and%2520maintaining%252092.8%2520percent%2520accuracy%2520even%2520under%252094.4%2520percent%2520compression.%2520Extensive%2520experiments%2520on%252014%2520image%2520and%2520video%2520benchmarks%2520demonstrate%2520that%2520FlashVLM%2520delivers%2520state%2520of%2520the%2520art%2520efficiency%2520performance%2520trade%2520offs%2520while%2520maintaining%2520strong%2520robustness%2520and%2520generalization%2520across%2520mainstream%2520VLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlashVLM%3A%20Text-Guided%20Visual%20Token%20Selection%20for%20Large%20Multimodal%20Models&entry.906535625=Kaitong%20Cai%20and%20Jusheng%20Zhang%20and%20Jing%20Yang%20and%20Yijia%20Fan%20and%20Pengtao%20Xie%20and%20Jian%20Wang%20and%20Keze%20Wang&entry.1292438233=Large%20vision-language%20models%20%28VLMs%29%20typically%20process%20hundreds%20or%20thousands%20of%20visual%20tokens%20per%20image%20or%20video%20frame%2C%20incurring%20quadratic%20attention%20cost%20and%20substantial%20redundancy.%20Existing%20token%20reduction%20methods%20often%20ignore%20the%20textual%20query%20or%20rely%20on%20deep%20attention%20maps%2C%20whose%20instability%20under%20aggressive%20pruning%20leads%20to%20degraded%20semantic%20alignment.%0A%20%20We%20propose%20FlashVLM%2C%20a%20text%20guided%20visual%20token%20selection%20framework%20that%20dynamically%20adapts%20visual%20inputs%20to%20the%20query.%20Instead%20of%20relying%20on%20noisy%20attention%20weights%2C%20FlashVLM%20computes%20an%20explicit%20cross%20modal%20similarity%20between%20projected%20image%20tokens%20and%20normalized%20text%20embeddings%20in%20the%20language%20model%20space.%20This%20extrinsic%20relevance%20is%20fused%20with%20intrinsic%20visual%20saliency%20using%20log%20domain%20weighting%20and%20temperature%20controlled%20sharpening.%20In%20addition%2C%20a%20diversity%20preserving%20partition%20retains%20a%20minimal%20yet%20representative%20set%20of%20background%20tokens%20to%20maintain%20global%20context.%0A%20%20Under%20identical%20token%20budgets%20and%20evaluation%20protocols%2C%20FlashVLM%20achieves%20beyond%20lossless%20compression%2C%20slightly%20surpassing%20the%20unpruned%20baseline%20while%20pruning%20up%20to%2077.8%20percent%20of%20visual%20tokens%20on%20LLaVA%201.5%2C%20and%20maintaining%2092.8%20percent%20accuracy%20even%20under%2094.4%20percent%20compression.%20Extensive%20experiments%20on%2014%20image%20and%20video%20benchmarks%20demonstrate%20that%20FlashVLM%20delivers%20state%20of%20the%20art%20efficiency%20performance%20trade%20offs%20while%20maintaining%20strong%20robustness%20and%20generalization%20across%20mainstream%20VLMs.&entry.1838667208=http%3A//arxiv.org/abs/2512.20561v1&entry.124074799=Read"},
{"title": "LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving", "author": "Long Nguyen and Micha Fauth and Bernhard Jaeger and Daniel Dauner and Maximilian Igl and Andreas Geiger and Kashyap Chitta", "abstract": "Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead.", "link": "http://arxiv.org/abs/2512.20563v1", "date": "2025-12-23", "relevancy": 2.6946, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5529}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5421}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEAD%3A%20Minimizing%20Learner-Expert%20Asymmetry%20in%20End-to-End%20Driving&body=Title%3A%20LEAD%3A%20Minimizing%20Learner-Expert%20Asymmetry%20in%20End-to-End%20Driving%0AAuthor%3A%20Long%20Nguyen%20and%20Micha%20Fauth%20and%20Bernhard%20Jaeger%20and%20Daniel%20Dauner%20and%20Maximilian%20Igl%20and%20Andreas%20Geiger%20and%20Kashyap%20Chitta%0AAbstract%3A%20Simulators%20can%20generate%20virtually%20unlimited%20driving%20data%2C%20yet%20imitation%20learning%20policies%20in%20simulation%20still%20struggle%20to%20achieve%20robust%20closed-loop%20performance.%20Motivated%20by%20this%20gap%2C%20we%20empirically%20study%20how%20misalignment%20between%20privileged%20expert%20demonstrations%20and%20sensor-based%20student%20observations%20can%20limit%20the%20effectiveness%20of%20imitation%20learning.%20More%20precisely%2C%20experts%20have%20significantly%20higher%20visibility%20%28e.g.%2C%20ignoring%20occlusions%29%20and%20far%20lower%20uncertainty%20%28e.g.%2C%20knowing%20other%20vehicles%27%20actions%29%2C%20making%20them%20difficult%20to%20imitate%20reliably.%20Furthermore%2C%20navigational%20intent%20%28i.e.%2C%20the%20route%20to%20follow%29%20is%20under-specified%20in%20student%20models%20at%20test%20time%20via%20only%20a%20single%20target%20point.%20We%20demonstrate%20that%20these%20asymmetries%20can%20measurably%20limit%20driving%20performance%20in%20CARLA%20and%20offer%20practical%20interventions%20to%20address%20them.%20After%20careful%20modifications%20to%20narrow%20the%20gaps%20between%20expert%20and%20student%2C%20our%20TransFuser%20v6%20%28TFv6%29%20student%20policy%20achieves%20a%20new%20state%20of%20the%20art%20on%20all%20major%20publicly%20available%20CARLA%20closed-loop%20benchmarks%2C%20reaching%2095%20DS%20on%20Bench2Drive%20and%20more%20than%20doubling%20prior%20performances%20on%20Longest6~v2%20and%20Town13.%20Additionally%2C%20by%20integrating%20perception%20supervision%20from%20our%20dataset%20into%20a%20shared%20sim-to-real%20pipeline%2C%20we%20show%20consistent%20gains%20on%20the%20NAVSIM%20and%20Waymo%20Vision-Based%20End-to-End%20driving%20benchmarks.%20Our%20code%2C%20data%2C%20and%20models%20are%20publicly%20available%20at%20https%3A//github.com/autonomousvision/lead.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEAD%253A%2520Minimizing%2520Learner-Expert%2520Asymmetry%2520in%2520End-to-End%2520Driving%26entry.906535625%3DLong%2520Nguyen%2520and%2520Micha%2520Fauth%2520and%2520Bernhard%2520Jaeger%2520and%2520Daniel%2520Dauner%2520and%2520Maximilian%2520Igl%2520and%2520Andreas%2520Geiger%2520and%2520Kashyap%2520Chitta%26entry.1292438233%3DSimulators%2520can%2520generate%2520virtually%2520unlimited%2520driving%2520data%252C%2520yet%2520imitation%2520learning%2520policies%2520in%2520simulation%2520still%2520struggle%2520to%2520achieve%2520robust%2520closed-loop%2520performance.%2520Motivated%2520by%2520this%2520gap%252C%2520we%2520empirically%2520study%2520how%2520misalignment%2520between%2520privileged%2520expert%2520demonstrations%2520and%2520sensor-based%2520student%2520observations%2520can%2520limit%2520the%2520effectiveness%2520of%2520imitation%2520learning.%2520More%2520precisely%252C%2520experts%2520have%2520significantly%2520higher%2520visibility%2520%2528e.g.%252C%2520ignoring%2520occlusions%2529%2520and%2520far%2520lower%2520uncertainty%2520%2528e.g.%252C%2520knowing%2520other%2520vehicles%2527%2520actions%2529%252C%2520making%2520them%2520difficult%2520to%2520imitate%2520reliably.%2520Furthermore%252C%2520navigational%2520intent%2520%2528i.e.%252C%2520the%2520route%2520to%2520follow%2529%2520is%2520under-specified%2520in%2520student%2520models%2520at%2520test%2520time%2520via%2520only%2520a%2520single%2520target%2520point.%2520We%2520demonstrate%2520that%2520these%2520asymmetries%2520can%2520measurably%2520limit%2520driving%2520performance%2520in%2520CARLA%2520and%2520offer%2520practical%2520interventions%2520to%2520address%2520them.%2520After%2520careful%2520modifications%2520to%2520narrow%2520the%2520gaps%2520between%2520expert%2520and%2520student%252C%2520our%2520TransFuser%2520v6%2520%2528TFv6%2529%2520student%2520policy%2520achieves%2520a%2520new%2520state%2520of%2520the%2520art%2520on%2520all%2520major%2520publicly%2520available%2520CARLA%2520closed-loop%2520benchmarks%252C%2520reaching%252095%2520DS%2520on%2520Bench2Drive%2520and%2520more%2520than%2520doubling%2520prior%2520performances%2520on%2520Longest6~v2%2520and%2520Town13.%2520Additionally%252C%2520by%2520integrating%2520perception%2520supervision%2520from%2520our%2520dataset%2520into%2520a%2520shared%2520sim-to-real%2520pipeline%252C%2520we%2520show%2520consistent%2520gains%2520on%2520the%2520NAVSIM%2520and%2520Waymo%2520Vision-Based%2520End-to-End%2520driving%2520benchmarks.%2520Our%2520code%252C%2520data%252C%2520and%2520models%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/autonomousvision/lead.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEAD%3A%20Minimizing%20Learner-Expert%20Asymmetry%20in%20End-to-End%20Driving&entry.906535625=Long%20Nguyen%20and%20Micha%20Fauth%20and%20Bernhard%20Jaeger%20and%20Daniel%20Dauner%20and%20Maximilian%20Igl%20and%20Andreas%20Geiger%20and%20Kashyap%20Chitta&entry.1292438233=Simulators%20can%20generate%20virtually%20unlimited%20driving%20data%2C%20yet%20imitation%20learning%20policies%20in%20simulation%20still%20struggle%20to%20achieve%20robust%20closed-loop%20performance.%20Motivated%20by%20this%20gap%2C%20we%20empirically%20study%20how%20misalignment%20between%20privileged%20expert%20demonstrations%20and%20sensor-based%20student%20observations%20can%20limit%20the%20effectiveness%20of%20imitation%20learning.%20More%20precisely%2C%20experts%20have%20significantly%20higher%20visibility%20%28e.g.%2C%20ignoring%20occlusions%29%20and%20far%20lower%20uncertainty%20%28e.g.%2C%20knowing%20other%20vehicles%27%20actions%29%2C%20making%20them%20difficult%20to%20imitate%20reliably.%20Furthermore%2C%20navigational%20intent%20%28i.e.%2C%20the%20route%20to%20follow%29%20is%20under-specified%20in%20student%20models%20at%20test%20time%20via%20only%20a%20single%20target%20point.%20We%20demonstrate%20that%20these%20asymmetries%20can%20measurably%20limit%20driving%20performance%20in%20CARLA%20and%20offer%20practical%20interventions%20to%20address%20them.%20After%20careful%20modifications%20to%20narrow%20the%20gaps%20between%20expert%20and%20student%2C%20our%20TransFuser%20v6%20%28TFv6%29%20student%20policy%20achieves%20a%20new%20state%20of%20the%20art%20on%20all%20major%20publicly%20available%20CARLA%20closed-loop%20benchmarks%2C%20reaching%2095%20DS%20on%20Bench2Drive%20and%20more%20than%20doubling%20prior%20performances%20on%20Longest6~v2%20and%20Town13.%20Additionally%2C%20by%20integrating%20perception%20supervision%20from%20our%20dataset%20into%20a%20shared%20sim-to-real%20pipeline%2C%20we%20show%20consistent%20gains%20on%20the%20NAVSIM%20and%20Waymo%20Vision-Based%20End-to-End%20driving%20benchmarks.%20Our%20code%2C%20data%2C%20and%20models%20are%20publicly%20available%20at%20https%3A//github.com/autonomousvision/lead.&entry.1838667208=http%3A//arxiv.org/abs/2512.20563v1&entry.124074799=Read"},
{"title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "author": "Yuxi Xiao and Longfei Li and Shen Yan and Xinhang Liu and Sida Peng and Yunchao Wei and Xiaowei Zhou and Bingyi Kang", "abstract": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "link": "http://arxiv.org/abs/2512.20617v1", "date": "2025-12-23", "relevancy": 2.6804, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5336}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialTree%3A%20How%20Spatial%20Abilities%20Branch%20Out%20in%20MLLMs&body=Title%3A%20SpatialTree%3A%20How%20Spatial%20Abilities%20Branch%20Out%20in%20MLLMs%0AAuthor%3A%20Yuxi%20Xiao%20and%20Longfei%20Li%20and%20Shen%20Yan%20and%20Xinhang%20Liu%20and%20Sida%20Peng%20and%20Yunchao%20Wei%20and%20Xiaowei%20Zhou%20and%20Bingyi%20Kang%0AAbstract%3A%20Cognitive%20science%20suggests%20that%20spatial%20ability%20develops%20progressively-from%20perception%20to%20reasoning%20and%20interaction.%20Yet%20in%20multimodal%20LLMs%20%28MLLMs%29%2C%20this%20hierarchy%20remains%20poorly%20understood%2C%20as%20most%20studies%20focus%20on%20a%20narrow%20set%20of%20tasks.%20We%20introduce%20SpatialTree%2C%20a%20cognitive-science-inspired%20hierarchy%20that%20organizes%20spatial%20abilities%20into%20four%20levels%3A%20low-level%20perception%20%28L1%29%2C%20mental%20mapping%20%28L2%29%2C%20simulation%20%28L3%29%2C%20and%20agentic%20competence%20%28L4%29.%20Based%20on%20this%20taxonomy%2C%20we%20construct%20the%20first%20capability-centric%20hierarchical%20benchmark%2C%20thoroughly%20evaluating%20mainstream%20MLLMs%20across%2027%20sub-abilities.%20The%20evaluation%20results%20reveal%20a%20clear%20structure%3A%20L1%20skills%20are%20largely%20orthogonal%2C%20whereas%20higher-level%20skills%20are%20strongly%20correlated%2C%20indicating%20increasing%20interdependency.%20Through%20targeted%20supervised%20fine-tuning%2C%20we%20uncover%20a%20surprising%20transfer%20dynamic-negative%20transfer%20within%20L1%2C%20but%20strong%20cross-level%20transfer%20from%20low-%20to%20high-level%20abilities%20with%20notable%20synergy.%20Finally%2C%20we%20explore%20how%20to%20improve%20the%20entire%20hierarchy.%20We%20find%20that%20naive%20RL%20that%20encourages%20extensive%20%22thinking%22%20is%20unreliable%3A%20it%20helps%20complex%20reasoning%20but%20hurts%20intuitive%20perception.%20We%20propose%20a%20simple%20auto-think%20strategy%20that%20suppresses%20unnecessary%20deliberation%2C%20enabling%20RL%20to%20consistently%20improve%20performance%20across%20all%20levels.%20By%20building%20SpatialTree%2C%20we%20provide%20a%20proof-of-concept%20framework%20for%20understanding%20and%20systematically%20scaling%20spatial%20abilities%20in%20MLLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialTree%253A%2520How%2520Spatial%2520Abilities%2520Branch%2520Out%2520in%2520MLLMs%26entry.906535625%3DYuxi%2520Xiao%2520and%2520Longfei%2520Li%2520and%2520Shen%2520Yan%2520and%2520Xinhang%2520Liu%2520and%2520Sida%2520Peng%2520and%2520Yunchao%2520Wei%2520and%2520Xiaowei%2520Zhou%2520and%2520Bingyi%2520Kang%26entry.1292438233%3DCognitive%2520science%2520suggests%2520that%2520spatial%2520ability%2520develops%2520progressively-from%2520perception%2520to%2520reasoning%2520and%2520interaction.%2520Yet%2520in%2520multimodal%2520LLMs%2520%2528MLLMs%2529%252C%2520this%2520hierarchy%2520remains%2520poorly%2520understood%252C%2520as%2520most%2520studies%2520focus%2520on%2520a%2520narrow%2520set%2520of%2520tasks.%2520We%2520introduce%2520SpatialTree%252C%2520a%2520cognitive-science-inspired%2520hierarchy%2520that%2520organizes%2520spatial%2520abilities%2520into%2520four%2520levels%253A%2520low-level%2520perception%2520%2528L1%2529%252C%2520mental%2520mapping%2520%2528L2%2529%252C%2520simulation%2520%2528L3%2529%252C%2520and%2520agentic%2520competence%2520%2528L4%2529.%2520Based%2520on%2520this%2520taxonomy%252C%2520we%2520construct%2520the%2520first%2520capability-centric%2520hierarchical%2520benchmark%252C%2520thoroughly%2520evaluating%2520mainstream%2520MLLMs%2520across%252027%2520sub-abilities.%2520The%2520evaluation%2520results%2520reveal%2520a%2520clear%2520structure%253A%2520L1%2520skills%2520are%2520largely%2520orthogonal%252C%2520whereas%2520higher-level%2520skills%2520are%2520strongly%2520correlated%252C%2520indicating%2520increasing%2520interdependency.%2520Through%2520targeted%2520supervised%2520fine-tuning%252C%2520we%2520uncover%2520a%2520surprising%2520transfer%2520dynamic-negative%2520transfer%2520within%2520L1%252C%2520but%2520strong%2520cross-level%2520transfer%2520from%2520low-%2520to%2520high-level%2520abilities%2520with%2520notable%2520synergy.%2520Finally%252C%2520we%2520explore%2520how%2520to%2520improve%2520the%2520entire%2520hierarchy.%2520We%2520find%2520that%2520naive%2520RL%2520that%2520encourages%2520extensive%2520%2522thinking%2522%2520is%2520unreliable%253A%2520it%2520helps%2520complex%2520reasoning%2520but%2520hurts%2520intuitive%2520perception.%2520We%2520propose%2520a%2520simple%2520auto-think%2520strategy%2520that%2520suppresses%2520unnecessary%2520deliberation%252C%2520enabling%2520RL%2520to%2520consistently%2520improve%2520performance%2520across%2520all%2520levels.%2520By%2520building%2520SpatialTree%252C%2520we%2520provide%2520a%2520proof-of-concept%2520framework%2520for%2520understanding%2520and%2520systematically%2520scaling%2520spatial%2520abilities%2520in%2520MLLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialTree%3A%20How%20Spatial%20Abilities%20Branch%20Out%20in%20MLLMs&entry.906535625=Yuxi%20Xiao%20and%20Longfei%20Li%20and%20Shen%20Yan%20and%20Xinhang%20Liu%20and%20Sida%20Peng%20and%20Yunchao%20Wei%20and%20Xiaowei%20Zhou%20and%20Bingyi%20Kang&entry.1292438233=Cognitive%20science%20suggests%20that%20spatial%20ability%20develops%20progressively-from%20perception%20to%20reasoning%20and%20interaction.%20Yet%20in%20multimodal%20LLMs%20%28MLLMs%29%2C%20this%20hierarchy%20remains%20poorly%20understood%2C%20as%20most%20studies%20focus%20on%20a%20narrow%20set%20of%20tasks.%20We%20introduce%20SpatialTree%2C%20a%20cognitive-science-inspired%20hierarchy%20that%20organizes%20spatial%20abilities%20into%20four%20levels%3A%20low-level%20perception%20%28L1%29%2C%20mental%20mapping%20%28L2%29%2C%20simulation%20%28L3%29%2C%20and%20agentic%20competence%20%28L4%29.%20Based%20on%20this%20taxonomy%2C%20we%20construct%20the%20first%20capability-centric%20hierarchical%20benchmark%2C%20thoroughly%20evaluating%20mainstream%20MLLMs%20across%2027%20sub-abilities.%20The%20evaluation%20results%20reveal%20a%20clear%20structure%3A%20L1%20skills%20are%20largely%20orthogonal%2C%20whereas%20higher-level%20skills%20are%20strongly%20correlated%2C%20indicating%20increasing%20interdependency.%20Through%20targeted%20supervised%20fine-tuning%2C%20we%20uncover%20a%20surprising%20transfer%20dynamic-negative%20transfer%20within%20L1%2C%20but%20strong%20cross-level%20transfer%20from%20low-%20to%20high-level%20abilities%20with%20notable%20synergy.%20Finally%2C%20we%20explore%20how%20to%20improve%20the%20entire%20hierarchy.%20We%20find%20that%20naive%20RL%20that%20encourages%20extensive%20%22thinking%22%20is%20unreliable%3A%20it%20helps%20complex%20reasoning%20but%20hurts%20intuitive%20perception.%20We%20propose%20a%20simple%20auto-think%20strategy%20that%20suppresses%20unnecessary%20deliberation%2C%20enabling%20RL%20to%20consistently%20improve%20performance%20across%20all%20levels.%20By%20building%20SpatialTree%2C%20we%20provide%20a%20proof-of-concept%20framework%20for%20understanding%20and%20systematically%20scaling%20spatial%20abilities%20in%20MLLMs.&entry.1838667208=http%3A//arxiv.org/abs/2512.20617v1&entry.124074799=Read"},
{"title": "Non-Intrusive Parametrized-Background Data-Weak Reconstruction of Cardiac Displacement Fields from Sparse MRI-like Observations", "author": "Francesco C. Mantegazza and Federica Caforio and Christoph Augustin and Matthias A. F. Gsell and Gundolf Haase and Elias Karabelas", "abstract": "Personalized cardiac diagnostics require accurate reconstruction of myocardial displacement fields from sparse clinical imaging data, yet current methods often demand intrusive access to computational models. In this work, we apply the non-intrusive Parametrized-Background Data-Weak (PBDW) approach to three-dimensional (3D) cardiac displacement field reconstruction from limited Magnetic Resonance Image (MRI)-like observations. Our implementation requires only solution snapshots -- no governing equations, assembly routines, or solver access -- enabling immediate deployment across commercial and research codes using different constitutive models. Additionally, we introduce two enhancements: an H-size minibatch worst-case Orthogonal Matching Pursuit (wOMP) algorithm that improves Sensor Selection (SS) computational efficiency while maintaining reconstruction accuracy, and memory optimization techniques exploiting block matrix structures in vectorial problems. We demonstrate the effectiveness of the method through validation on a 3D left ventricular model with simulated scar tissue. Starting with noise-free reconstruction, we systematically incorporate Gaussian noise and spatial sparsity mimicking realistic MRI acquisition protocols. Results show exceptional accuracy in noise-free conditions (relative L2 error of order O(1e-5)), robust performance with 10% noise (relative L2 error of order O(1e-2)), and effective reconstruction from sparse measurements (relative L2 error of order O(1e-2)). The online reconstruction achieves four-order-of-magnitude computational speed-up compared to full Finite Element (FE) simulations, with reconstruction times under one tenth of second for sparse scenarios, demonstrating significant potential for integration into clinical cardiac modeling workflows.", "link": "http://arxiv.org/abs/2509.14844v2", "date": "2025-12-23", "relevancy": 2.6542, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5309}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5309}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Intrusive%20Parametrized-Background%20Data-Weak%20Reconstruction%20of%20Cardiac%20Displacement%20Fields%20from%20Sparse%20MRI-like%20Observations&body=Title%3A%20Non-Intrusive%20Parametrized-Background%20Data-Weak%20Reconstruction%20of%20Cardiac%20Displacement%20Fields%20from%20Sparse%20MRI-like%20Observations%0AAuthor%3A%20Francesco%20C.%20Mantegazza%20and%20Federica%20Caforio%20and%20Christoph%20Augustin%20and%20Matthias%20A.%20F.%20Gsell%20and%20Gundolf%20Haase%20and%20Elias%20Karabelas%0AAbstract%3A%20Personalized%20cardiac%20diagnostics%20require%20accurate%20reconstruction%20of%20myocardial%20displacement%20fields%20from%20sparse%20clinical%20imaging%20data%2C%20yet%20current%20methods%20often%20demand%20intrusive%20access%20to%20computational%20models.%20In%20this%20work%2C%20we%20apply%20the%20non-intrusive%20Parametrized-Background%20Data-Weak%20%28PBDW%29%20approach%20to%20three-dimensional%20%283D%29%20cardiac%20displacement%20field%20reconstruction%20from%20limited%20Magnetic%20Resonance%20Image%20%28MRI%29-like%20observations.%20Our%20implementation%20requires%20only%20solution%20snapshots%20--%20no%20governing%20equations%2C%20assembly%20routines%2C%20or%20solver%20access%20--%20enabling%20immediate%20deployment%20across%20commercial%20and%20research%20codes%20using%20different%20constitutive%20models.%20Additionally%2C%20we%20introduce%20two%20enhancements%3A%20an%20H-size%20minibatch%20worst-case%20Orthogonal%20Matching%20Pursuit%20%28wOMP%29%20algorithm%20that%20improves%20Sensor%20Selection%20%28SS%29%20computational%20efficiency%20while%20maintaining%20reconstruction%20accuracy%2C%20and%20memory%20optimization%20techniques%20exploiting%20block%20matrix%20structures%20in%20vectorial%20problems.%20We%20demonstrate%20the%20effectiveness%20of%20the%20method%20through%20validation%20on%20a%203D%20left%20ventricular%20model%20with%20simulated%20scar%20tissue.%20Starting%20with%20noise-free%20reconstruction%2C%20we%20systematically%20incorporate%20Gaussian%20noise%20and%20spatial%20sparsity%20mimicking%20realistic%20MRI%20acquisition%20protocols.%20Results%20show%20exceptional%20accuracy%20in%20noise-free%20conditions%20%28relative%20L2%20error%20of%20order%20O%281e-5%29%29%2C%20robust%20performance%20with%2010%25%20noise%20%28relative%20L2%20error%20of%20order%20O%281e-2%29%29%2C%20and%20effective%20reconstruction%20from%20sparse%20measurements%20%28relative%20L2%20error%20of%20order%20O%281e-2%29%29.%20The%20online%20reconstruction%20achieves%20four-order-of-magnitude%20computational%20speed-up%20compared%20to%20full%20Finite%20Element%20%28FE%29%20simulations%2C%20with%20reconstruction%20times%20under%20one%20tenth%20of%20second%20for%20sparse%20scenarios%2C%20demonstrating%20significant%20potential%20for%20integration%20into%20clinical%20cardiac%20modeling%20workflows.%0ALink%3A%20http%3A//arxiv.org/abs/2509.14844v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Intrusive%2520Parametrized-Background%2520Data-Weak%2520Reconstruction%2520of%2520Cardiac%2520Displacement%2520Fields%2520from%2520Sparse%2520MRI-like%2520Observations%26entry.906535625%3DFrancesco%2520C.%2520Mantegazza%2520and%2520Federica%2520Caforio%2520and%2520Christoph%2520Augustin%2520and%2520Matthias%2520A.%2520F.%2520Gsell%2520and%2520Gundolf%2520Haase%2520and%2520Elias%2520Karabelas%26entry.1292438233%3DPersonalized%2520cardiac%2520diagnostics%2520require%2520accurate%2520reconstruction%2520of%2520myocardial%2520displacement%2520fields%2520from%2520sparse%2520clinical%2520imaging%2520data%252C%2520yet%2520current%2520methods%2520often%2520demand%2520intrusive%2520access%2520to%2520computational%2520models.%2520In%2520this%2520work%252C%2520we%2520apply%2520the%2520non-intrusive%2520Parametrized-Background%2520Data-Weak%2520%2528PBDW%2529%2520approach%2520to%2520three-dimensional%2520%25283D%2529%2520cardiac%2520displacement%2520field%2520reconstruction%2520from%2520limited%2520Magnetic%2520Resonance%2520Image%2520%2528MRI%2529-like%2520observations.%2520Our%2520implementation%2520requires%2520only%2520solution%2520snapshots%2520--%2520no%2520governing%2520equations%252C%2520assembly%2520routines%252C%2520or%2520solver%2520access%2520--%2520enabling%2520immediate%2520deployment%2520across%2520commercial%2520and%2520research%2520codes%2520using%2520different%2520constitutive%2520models.%2520Additionally%252C%2520we%2520introduce%2520two%2520enhancements%253A%2520an%2520H-size%2520minibatch%2520worst-case%2520Orthogonal%2520Matching%2520Pursuit%2520%2528wOMP%2529%2520algorithm%2520that%2520improves%2520Sensor%2520Selection%2520%2528SS%2529%2520computational%2520efficiency%2520while%2520maintaining%2520reconstruction%2520accuracy%252C%2520and%2520memory%2520optimization%2520techniques%2520exploiting%2520block%2520matrix%2520structures%2520in%2520vectorial%2520problems.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520method%2520through%2520validation%2520on%2520a%25203D%2520left%2520ventricular%2520model%2520with%2520simulated%2520scar%2520tissue.%2520Starting%2520with%2520noise-free%2520reconstruction%252C%2520we%2520systematically%2520incorporate%2520Gaussian%2520noise%2520and%2520spatial%2520sparsity%2520mimicking%2520realistic%2520MRI%2520acquisition%2520protocols.%2520Results%2520show%2520exceptional%2520accuracy%2520in%2520noise-free%2520conditions%2520%2528relative%2520L2%2520error%2520of%2520order%2520O%25281e-5%2529%2529%252C%2520robust%2520performance%2520with%252010%2525%2520noise%2520%2528relative%2520L2%2520error%2520of%2520order%2520O%25281e-2%2529%2529%252C%2520and%2520effective%2520reconstruction%2520from%2520sparse%2520measurements%2520%2528relative%2520L2%2520error%2520of%2520order%2520O%25281e-2%2529%2529.%2520The%2520online%2520reconstruction%2520achieves%2520four-order-of-magnitude%2520computational%2520speed-up%2520compared%2520to%2520full%2520Finite%2520Element%2520%2528FE%2529%2520simulations%252C%2520with%2520reconstruction%2520times%2520under%2520one%2520tenth%2520of%2520second%2520for%2520sparse%2520scenarios%252C%2520demonstrating%2520significant%2520potential%2520for%2520integration%2520into%2520clinical%2520cardiac%2520modeling%2520workflows.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14844v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Intrusive%20Parametrized-Background%20Data-Weak%20Reconstruction%20of%20Cardiac%20Displacement%20Fields%20from%20Sparse%20MRI-like%20Observations&entry.906535625=Francesco%20C.%20Mantegazza%20and%20Federica%20Caforio%20and%20Christoph%20Augustin%20and%20Matthias%20A.%20F.%20Gsell%20and%20Gundolf%20Haase%20and%20Elias%20Karabelas&entry.1292438233=Personalized%20cardiac%20diagnostics%20require%20accurate%20reconstruction%20of%20myocardial%20displacement%20fields%20from%20sparse%20clinical%20imaging%20data%2C%20yet%20current%20methods%20often%20demand%20intrusive%20access%20to%20computational%20models.%20In%20this%20work%2C%20we%20apply%20the%20non-intrusive%20Parametrized-Background%20Data-Weak%20%28PBDW%29%20approach%20to%20three-dimensional%20%283D%29%20cardiac%20displacement%20field%20reconstruction%20from%20limited%20Magnetic%20Resonance%20Image%20%28MRI%29-like%20observations.%20Our%20implementation%20requires%20only%20solution%20snapshots%20--%20no%20governing%20equations%2C%20assembly%20routines%2C%20or%20solver%20access%20--%20enabling%20immediate%20deployment%20across%20commercial%20and%20research%20codes%20using%20different%20constitutive%20models.%20Additionally%2C%20we%20introduce%20two%20enhancements%3A%20an%20H-size%20minibatch%20worst-case%20Orthogonal%20Matching%20Pursuit%20%28wOMP%29%20algorithm%20that%20improves%20Sensor%20Selection%20%28SS%29%20computational%20efficiency%20while%20maintaining%20reconstruction%20accuracy%2C%20and%20memory%20optimization%20techniques%20exploiting%20block%20matrix%20structures%20in%20vectorial%20problems.%20We%20demonstrate%20the%20effectiveness%20of%20the%20method%20through%20validation%20on%20a%203D%20left%20ventricular%20model%20with%20simulated%20scar%20tissue.%20Starting%20with%20noise-free%20reconstruction%2C%20we%20systematically%20incorporate%20Gaussian%20noise%20and%20spatial%20sparsity%20mimicking%20realistic%20MRI%20acquisition%20protocols.%20Results%20show%20exceptional%20accuracy%20in%20noise-free%20conditions%20%28relative%20L2%20error%20of%20order%20O%281e-5%29%29%2C%20robust%20performance%20with%2010%25%20noise%20%28relative%20L2%20error%20of%20order%20O%281e-2%29%29%2C%20and%20effective%20reconstruction%20from%20sparse%20measurements%20%28relative%20L2%20error%20of%20order%20O%281e-2%29%29.%20The%20online%20reconstruction%20achieves%20four-order-of-magnitude%20computational%20speed-up%20compared%20to%20full%20Finite%20Element%20%28FE%29%20simulations%2C%20with%20reconstruction%20times%20under%20one%20tenth%20of%20second%20for%20sparse%20scenarios%2C%20demonstrating%20significant%20potential%20for%20integration%20into%20clinical%20cardiac%20modeling%20workflows.&entry.1838667208=http%3A//arxiv.org/abs/2509.14844v2&entry.124074799=Read"},
{"title": "HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training", "author": "Yuanjian Xu and Yuan Shuai and Jianing Hao and Guang Zhang", "abstract": "Neural Stochastic Differential Equations (Neural SDEs) provide a principled framework for modeling continuous-time stochastic processes and have been widely adopted in fields ranging from physics to finance. Recent advances suggest that Generative Adversarial Networks (GANs) offer a promising solution to learning the complex path distributions induced by SDEs. However, a critical bottleneck lies in designing a discriminator that faithfully captures temporal dependencies while remaining computationally efficient. Prior works have explored Neural Controlled Differential Equations (CDEs) as discriminators due to their ability to model continuous-time dynamics, but such architectures suffer from high computational costs and exacerbate the instability of adversarial training. To address these limitations, we introduce HGAN-SDEs, a novel GAN-based framework that leverages Neural Hermite functions to construct a structured and efficient discriminator. Hermite functions provide an expressive yet lightweight basis for approximating path-level dynamics, enabling both reduced runtime complexity and improved training stability. We establish the universal approximation property of our framework for a broad class of SDE-driven distributions and theoretically characterize its convergence behavior. Extensive empirical evaluations on synthetic and real-world systems demonstrate that HGAN-SDEs achieve superior sample quality and learning efficiency compared to existing generative models for SDEs", "link": "http://arxiv.org/abs/2512.20272v1", "date": "2025-12-23", "relevancy": 2.6332, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5455}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5175}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HGAN-SDEs%3A%20Learning%20Neural%20Stochastic%20Differential%20Equations%20with%20Hermite-Guided%20Adversarial%20Training&body=Title%3A%20HGAN-SDEs%3A%20Learning%20Neural%20Stochastic%20Differential%20Equations%20with%20Hermite-Guided%20Adversarial%20Training%0AAuthor%3A%20Yuanjian%20Xu%20and%20Yuan%20Shuai%20and%20Jianing%20Hao%20and%20Guang%20Zhang%0AAbstract%3A%20Neural%20Stochastic%20Differential%20Equations%20%28Neural%20SDEs%29%20provide%20a%20principled%20framework%20for%20modeling%20continuous-time%20stochastic%20processes%20and%20have%20been%20widely%20adopted%20in%20fields%20ranging%20from%20physics%20to%20finance.%20Recent%20advances%20suggest%20that%20Generative%20Adversarial%20Networks%20%28GANs%29%20offer%20a%20promising%20solution%20to%20learning%20the%20complex%20path%20distributions%20induced%20by%20SDEs.%20However%2C%20a%20critical%20bottleneck%20lies%20in%20designing%20a%20discriminator%20that%20faithfully%20captures%20temporal%20dependencies%20while%20remaining%20computationally%20efficient.%20Prior%20works%20have%20explored%20Neural%20Controlled%20Differential%20Equations%20%28CDEs%29%20as%20discriminators%20due%20to%20their%20ability%20to%20model%20continuous-time%20dynamics%2C%20but%20such%20architectures%20suffer%20from%20high%20computational%20costs%20and%20exacerbate%20the%20instability%20of%20adversarial%20training.%20To%20address%20these%20limitations%2C%20we%20introduce%20HGAN-SDEs%2C%20a%20novel%20GAN-based%20framework%20that%20leverages%20Neural%20Hermite%20functions%20to%20construct%20a%20structured%20and%20efficient%20discriminator.%20Hermite%20functions%20provide%20an%20expressive%20yet%20lightweight%20basis%20for%20approximating%20path-level%20dynamics%2C%20enabling%20both%20reduced%20runtime%20complexity%20and%20improved%20training%20stability.%20We%20establish%20the%20universal%20approximation%20property%20of%20our%20framework%20for%20a%20broad%20class%20of%20SDE-driven%20distributions%20and%20theoretically%20characterize%20its%20convergence%20behavior.%20Extensive%20empirical%20evaluations%20on%20synthetic%20and%20real-world%20systems%20demonstrate%20that%20HGAN-SDEs%20achieve%20superior%20sample%20quality%20and%20learning%20efficiency%20compared%20to%20existing%20generative%20models%20for%20SDEs%0ALink%3A%20http%3A//arxiv.org/abs/2512.20272v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHGAN-SDEs%253A%2520Learning%2520Neural%2520Stochastic%2520Differential%2520Equations%2520with%2520Hermite-Guided%2520Adversarial%2520Training%26entry.906535625%3DYuanjian%2520Xu%2520and%2520Yuan%2520Shuai%2520and%2520Jianing%2520Hao%2520and%2520Guang%2520Zhang%26entry.1292438233%3DNeural%2520Stochastic%2520Differential%2520Equations%2520%2528Neural%2520SDEs%2529%2520provide%2520a%2520principled%2520framework%2520for%2520modeling%2520continuous-time%2520stochastic%2520processes%2520and%2520have%2520been%2520widely%2520adopted%2520in%2520fields%2520ranging%2520from%2520physics%2520to%2520finance.%2520Recent%2520advances%2520suggest%2520that%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520offer%2520a%2520promising%2520solution%2520to%2520learning%2520the%2520complex%2520path%2520distributions%2520induced%2520by%2520SDEs.%2520However%252C%2520a%2520critical%2520bottleneck%2520lies%2520in%2520designing%2520a%2520discriminator%2520that%2520faithfully%2520captures%2520temporal%2520dependencies%2520while%2520remaining%2520computationally%2520efficient.%2520Prior%2520works%2520have%2520explored%2520Neural%2520Controlled%2520Differential%2520Equations%2520%2528CDEs%2529%2520as%2520discriminators%2520due%2520to%2520their%2520ability%2520to%2520model%2520continuous-time%2520dynamics%252C%2520but%2520such%2520architectures%2520suffer%2520from%2520high%2520computational%2520costs%2520and%2520exacerbate%2520the%2520instability%2520of%2520adversarial%2520training.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520HGAN-SDEs%252C%2520a%2520novel%2520GAN-based%2520framework%2520that%2520leverages%2520Neural%2520Hermite%2520functions%2520to%2520construct%2520a%2520structured%2520and%2520efficient%2520discriminator.%2520Hermite%2520functions%2520provide%2520an%2520expressive%2520yet%2520lightweight%2520basis%2520for%2520approximating%2520path-level%2520dynamics%252C%2520enabling%2520both%2520reduced%2520runtime%2520complexity%2520and%2520improved%2520training%2520stability.%2520We%2520establish%2520the%2520universal%2520approximation%2520property%2520of%2520our%2520framework%2520for%2520a%2520broad%2520class%2520of%2520SDE-driven%2520distributions%2520and%2520theoretically%2520characterize%2520its%2520convergence%2520behavior.%2520Extensive%2520empirical%2520evaluations%2520on%2520synthetic%2520and%2520real-world%2520systems%2520demonstrate%2520that%2520HGAN-SDEs%2520achieve%2520superior%2520sample%2520quality%2520and%2520learning%2520efficiency%2520compared%2520to%2520existing%2520generative%2520models%2520for%2520SDEs%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20272v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HGAN-SDEs%3A%20Learning%20Neural%20Stochastic%20Differential%20Equations%20with%20Hermite-Guided%20Adversarial%20Training&entry.906535625=Yuanjian%20Xu%20and%20Yuan%20Shuai%20and%20Jianing%20Hao%20and%20Guang%20Zhang&entry.1292438233=Neural%20Stochastic%20Differential%20Equations%20%28Neural%20SDEs%29%20provide%20a%20principled%20framework%20for%20modeling%20continuous-time%20stochastic%20processes%20and%20have%20been%20widely%20adopted%20in%20fields%20ranging%20from%20physics%20to%20finance.%20Recent%20advances%20suggest%20that%20Generative%20Adversarial%20Networks%20%28GANs%29%20offer%20a%20promising%20solution%20to%20learning%20the%20complex%20path%20distributions%20induced%20by%20SDEs.%20However%2C%20a%20critical%20bottleneck%20lies%20in%20designing%20a%20discriminator%20that%20faithfully%20captures%20temporal%20dependencies%20while%20remaining%20computationally%20efficient.%20Prior%20works%20have%20explored%20Neural%20Controlled%20Differential%20Equations%20%28CDEs%29%20as%20discriminators%20due%20to%20their%20ability%20to%20model%20continuous-time%20dynamics%2C%20but%20such%20architectures%20suffer%20from%20high%20computational%20costs%20and%20exacerbate%20the%20instability%20of%20adversarial%20training.%20To%20address%20these%20limitations%2C%20we%20introduce%20HGAN-SDEs%2C%20a%20novel%20GAN-based%20framework%20that%20leverages%20Neural%20Hermite%20functions%20to%20construct%20a%20structured%20and%20efficient%20discriminator.%20Hermite%20functions%20provide%20an%20expressive%20yet%20lightweight%20basis%20for%20approximating%20path-level%20dynamics%2C%20enabling%20both%20reduced%20runtime%20complexity%20and%20improved%20training%20stability.%20We%20establish%20the%20universal%20approximation%20property%20of%20our%20framework%20for%20a%20broad%20class%20of%20SDE-driven%20distributions%20and%20theoretically%20characterize%20its%20convergence%20behavior.%20Extensive%20empirical%20evaluations%20on%20synthetic%20and%20real-world%20systems%20demonstrate%20that%20HGAN-SDEs%20achieve%20superior%20sample%20quality%20and%20learning%20efficiency%20compared%20to%20existing%20generative%20models%20for%20SDEs&entry.1838667208=http%3A//arxiv.org/abs/2512.20272v1&entry.124074799=Read"},
{"title": "Improving Speech Emotion Recognition with Mutual Information Regularized Generative Model", "author": "Chung-Soo Ahn and Rajib Rana and Sunil Sivadas and Carlos Busso and Jagath C. Rajapakse", "abstract": "Lack of large, well-annotated emotional speech corpora continues to limit the performance and robustness of speech emotion recognition (SER), particularly as models grow more complex and the demand for multimodal systems increases. While generative data augmentation offers a promising solution, existing approaches often produce emotionally inconsistent samples due to oversimplified conditioning on categorical labels. This paper introduces a novel mutual-information-regularised generative framework that combines cross-modal alignment with feature-level synthesis. Building on an InfoGAN-style architecture, our method first learns a semantically aligned audio-text representation space using pre-trained transformers and contrastive objectives. A feature generator is then trained to produce emotion-aware audio features while employing mutual information as a quantitative regulariser to ensure strong dependency between generated features and their conditioning variables. We extend this approach to multimodal settings, enabling the generation of novel, paired (audio, text) features. Comprehensive evaluation on three benchmark datasets (IEMOCAP, MSP-IMPROV, MSP-Podcast) demonstrates that our framework consistently outperforms existing augmentation methods, achieving state-of-the-art performance with improvements of up to 2.6% in unimodal SER and 3.2% in multimodal emotion recognition. Most importantly, we demonstrate that mutual information functions as both a regulariser and a measurable metric for generative quality, offering a systematic approach to data augmentation in affective computing.", "link": "http://arxiv.org/abs/2510.10078v3", "date": "2025-12-23", "relevancy": 2.546, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5249}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5092}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Speech%20Emotion%20Recognition%20with%20Mutual%20Information%20Regularized%20Generative%20Model&body=Title%3A%20Improving%20Speech%20Emotion%20Recognition%20with%20Mutual%20Information%20Regularized%20Generative%20Model%0AAuthor%3A%20Chung-Soo%20Ahn%20and%20Rajib%20Rana%20and%20Sunil%20Sivadas%20and%20Carlos%20Busso%20and%20Jagath%20C.%20Rajapakse%0AAbstract%3A%20Lack%20of%20large%2C%20well-annotated%20emotional%20speech%20corpora%20continues%20to%20limit%20the%20performance%20and%20robustness%20of%20speech%20emotion%20recognition%20%28SER%29%2C%20particularly%20as%20models%20grow%20more%20complex%20and%20the%20demand%20for%20multimodal%20systems%20increases.%20While%20generative%20data%20augmentation%20offers%20a%20promising%20solution%2C%20existing%20approaches%20often%20produce%20emotionally%20inconsistent%20samples%20due%20to%20oversimplified%20conditioning%20on%20categorical%20labels.%20This%20paper%20introduces%20a%20novel%20mutual-information-regularised%20generative%20framework%20that%20combines%20cross-modal%20alignment%20with%20feature-level%20synthesis.%20Building%20on%20an%20InfoGAN-style%20architecture%2C%20our%20method%20first%20learns%20a%20semantically%20aligned%20audio-text%20representation%20space%20using%20pre-trained%20transformers%20and%20contrastive%20objectives.%20A%20feature%20generator%20is%20then%20trained%20to%20produce%20emotion-aware%20audio%20features%20while%20employing%20mutual%20information%20as%20a%20quantitative%20regulariser%20to%20ensure%20strong%20dependency%20between%20generated%20features%20and%20their%20conditioning%20variables.%20We%20extend%20this%20approach%20to%20multimodal%20settings%2C%20enabling%20the%20generation%20of%20novel%2C%20paired%20%28audio%2C%20text%29%20features.%20Comprehensive%20evaluation%20on%20three%20benchmark%20datasets%20%28IEMOCAP%2C%20MSP-IMPROV%2C%20MSP-Podcast%29%20demonstrates%20that%20our%20framework%20consistently%20outperforms%20existing%20augmentation%20methods%2C%20achieving%20state-of-the-art%20performance%20with%20improvements%20of%20up%20to%202.6%25%20in%20unimodal%20SER%20and%203.2%25%20in%20multimodal%20emotion%20recognition.%20Most%20importantly%2C%20we%20demonstrate%20that%20mutual%20information%20functions%20as%20both%20a%20regulariser%20and%20a%20measurable%20metric%20for%20generative%20quality%2C%20offering%20a%20systematic%20approach%20to%20data%20augmentation%20in%20affective%20computing.%0ALink%3A%20http%3A//arxiv.org/abs/2510.10078v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Speech%2520Emotion%2520Recognition%2520with%2520Mutual%2520Information%2520Regularized%2520Generative%2520Model%26entry.906535625%3DChung-Soo%2520Ahn%2520and%2520Rajib%2520Rana%2520and%2520Sunil%2520Sivadas%2520and%2520Carlos%2520Busso%2520and%2520Jagath%2520C.%2520Rajapakse%26entry.1292438233%3DLack%2520of%2520large%252C%2520well-annotated%2520emotional%2520speech%2520corpora%2520continues%2520to%2520limit%2520the%2520performance%2520and%2520robustness%2520of%2520speech%2520emotion%2520recognition%2520%2528SER%2529%252C%2520particularly%2520as%2520models%2520grow%2520more%2520complex%2520and%2520the%2520demand%2520for%2520multimodal%2520systems%2520increases.%2520While%2520generative%2520data%2520augmentation%2520offers%2520a%2520promising%2520solution%252C%2520existing%2520approaches%2520often%2520produce%2520emotionally%2520inconsistent%2520samples%2520due%2520to%2520oversimplified%2520conditioning%2520on%2520categorical%2520labels.%2520This%2520paper%2520introduces%2520a%2520novel%2520mutual-information-regularised%2520generative%2520framework%2520that%2520combines%2520cross-modal%2520alignment%2520with%2520feature-level%2520synthesis.%2520Building%2520on%2520an%2520InfoGAN-style%2520architecture%252C%2520our%2520method%2520first%2520learns%2520a%2520semantically%2520aligned%2520audio-text%2520representation%2520space%2520using%2520pre-trained%2520transformers%2520and%2520contrastive%2520objectives.%2520A%2520feature%2520generator%2520is%2520then%2520trained%2520to%2520produce%2520emotion-aware%2520audio%2520features%2520while%2520employing%2520mutual%2520information%2520as%2520a%2520quantitative%2520regulariser%2520to%2520ensure%2520strong%2520dependency%2520between%2520generated%2520features%2520and%2520their%2520conditioning%2520variables.%2520We%2520extend%2520this%2520approach%2520to%2520multimodal%2520settings%252C%2520enabling%2520the%2520generation%2520of%2520novel%252C%2520paired%2520%2528audio%252C%2520text%2529%2520features.%2520Comprehensive%2520evaluation%2520on%2520three%2520benchmark%2520datasets%2520%2528IEMOCAP%252C%2520MSP-IMPROV%252C%2520MSP-Podcast%2529%2520demonstrates%2520that%2520our%2520framework%2520consistently%2520outperforms%2520existing%2520augmentation%2520methods%252C%2520achieving%2520state-of-the-art%2520performance%2520with%2520improvements%2520of%2520up%2520to%25202.6%2525%2520in%2520unimodal%2520SER%2520and%25203.2%2525%2520in%2520multimodal%2520emotion%2520recognition.%2520Most%2520importantly%252C%2520we%2520demonstrate%2520that%2520mutual%2520information%2520functions%2520as%2520both%2520a%2520regulariser%2520and%2520a%2520measurable%2520metric%2520for%2520generative%2520quality%252C%2520offering%2520a%2520systematic%2520approach%2520to%2520data%2520augmentation%2520in%2520affective%2520computing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10078v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Speech%20Emotion%20Recognition%20with%20Mutual%20Information%20Regularized%20Generative%20Model&entry.906535625=Chung-Soo%20Ahn%20and%20Rajib%20Rana%20and%20Sunil%20Sivadas%20and%20Carlos%20Busso%20and%20Jagath%20C.%20Rajapakse&entry.1292438233=Lack%20of%20large%2C%20well-annotated%20emotional%20speech%20corpora%20continues%20to%20limit%20the%20performance%20and%20robustness%20of%20speech%20emotion%20recognition%20%28SER%29%2C%20particularly%20as%20models%20grow%20more%20complex%20and%20the%20demand%20for%20multimodal%20systems%20increases.%20While%20generative%20data%20augmentation%20offers%20a%20promising%20solution%2C%20existing%20approaches%20often%20produce%20emotionally%20inconsistent%20samples%20due%20to%20oversimplified%20conditioning%20on%20categorical%20labels.%20This%20paper%20introduces%20a%20novel%20mutual-information-regularised%20generative%20framework%20that%20combines%20cross-modal%20alignment%20with%20feature-level%20synthesis.%20Building%20on%20an%20InfoGAN-style%20architecture%2C%20our%20method%20first%20learns%20a%20semantically%20aligned%20audio-text%20representation%20space%20using%20pre-trained%20transformers%20and%20contrastive%20objectives.%20A%20feature%20generator%20is%20then%20trained%20to%20produce%20emotion-aware%20audio%20features%20while%20employing%20mutual%20information%20as%20a%20quantitative%20regulariser%20to%20ensure%20strong%20dependency%20between%20generated%20features%20and%20their%20conditioning%20variables.%20We%20extend%20this%20approach%20to%20multimodal%20settings%2C%20enabling%20the%20generation%20of%20novel%2C%20paired%20%28audio%2C%20text%29%20features.%20Comprehensive%20evaluation%20on%20three%20benchmark%20datasets%20%28IEMOCAP%2C%20MSP-IMPROV%2C%20MSP-Podcast%29%20demonstrates%20that%20our%20framework%20consistently%20outperforms%20existing%20augmentation%20methods%2C%20achieving%20state-of-the-art%20performance%20with%20improvements%20of%20up%20to%202.6%25%20in%20unimodal%20SER%20and%203.2%25%20in%20multimodal%20emotion%20recognition.%20Most%20importantly%2C%20we%20demonstrate%20that%20mutual%20information%20functions%20as%20both%20a%20regulariser%20and%20a%20measurable%20metric%20for%20generative%20quality%2C%20offering%20a%20systematic%20approach%20to%20data%20augmentation%20in%20affective%20computing.&entry.1838667208=http%3A//arxiv.org/abs/2510.10078v3&entry.124074799=Read"},
{"title": "Learning Informative Attention Weights for Person Re-Identification", "author": "Yancheng Wang and Nebojsa Jojic and Yingzhen Yang", "abstract": "Attention mechanisms have been widely used in deep learning, and recent efforts have been devoted to incorporating attention modules into deep neural networks (DNNs) for person Re-Identification (Re-ID) to enhance their discriminative feature learning capabilities. Existing attention modules, including self-attention and channel attention, learn attention weights that quantify the importance of feature tokens or feature channels. However, existing attention methods do not explicitly ensure that the attention weights are informative for predicting the identity of the person in the input image, and may consequently introduce noisy information from the input image. To address this issue, we propose a novel method termed Reduction of Information Bottleneck loss (RIB), motivated by the principle of the Information Bottleneck (IB). A novel distribution-free and efficient variational upper bound for the IB loss (IBB), which can be optimized by standard SGD, is derived and incorporated into the training loss of the RIB models. RIB is applied to DNNs with self-attention modules through a novel Differentiable Channel Selection Attention module, or DCS-Attention, that selects the most informative channels for computing attention weights, leading to competitive models termed RIB-DCS. RIB is also incorporated into DNNs with existing channel attention modules to promote the learning of informative channel attention weights, leading to models termed RIB-CA. Both RIB-DCS and RIB-CA are applied to fixed neural network backbones and learnable backbones with Differentiable Neural Architecture Search (DNAS). Extensive experiments on multiple person Re-ID benchmarks show that RIB significantly enhances the prediction accuracy of DNNs for person Re-ID, even for the occluded person Re-ID.", "link": "http://arxiv.org/abs/2505.08961v2", "date": "2025-12-23", "relevancy": 2.5023, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5037}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5015}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Informative%20Attention%20Weights%20for%20Person%20Re-Identification&body=Title%3A%20Learning%20Informative%20Attention%20Weights%20for%20Person%20Re-Identification%0AAuthor%3A%20Yancheng%20Wang%20and%20Nebojsa%20Jojic%20and%20Yingzhen%20Yang%0AAbstract%3A%20Attention%20mechanisms%20have%20been%20widely%20used%20in%20deep%20learning%2C%20and%20recent%20efforts%20have%20been%20devoted%20to%20incorporating%20attention%20modules%20into%20deep%20neural%20networks%20%28DNNs%29%20for%20person%20Re-Identification%20%28Re-ID%29%20to%20enhance%20their%20discriminative%20feature%20learning%20capabilities.%20Existing%20attention%20modules%2C%20including%20self-attention%20and%20channel%20attention%2C%20learn%20attention%20weights%20that%20quantify%20the%20importance%20of%20feature%20tokens%20or%20feature%20channels.%20However%2C%20existing%20attention%20methods%20do%20not%20explicitly%20ensure%20that%20the%20attention%20weights%20are%20informative%20for%20predicting%20the%20identity%20of%20the%20person%20in%20the%20input%20image%2C%20and%20may%20consequently%20introduce%20noisy%20information%20from%20the%20input%20image.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20method%20termed%20Reduction%20of%20Information%20Bottleneck%20loss%20%28RIB%29%2C%20motivated%20by%20the%20principle%20of%20the%20Information%20Bottleneck%20%28IB%29.%20A%20novel%20distribution-free%20and%20efficient%20variational%20upper%20bound%20for%20the%20IB%20loss%20%28IBB%29%2C%20which%20can%20be%20optimized%20by%20standard%20SGD%2C%20is%20derived%20and%20incorporated%20into%20the%20training%20loss%20of%20the%20RIB%20models.%20RIB%20is%20applied%20to%20DNNs%20with%20self-attention%20modules%20through%20a%20novel%20Differentiable%20Channel%20Selection%20Attention%20module%2C%20or%20DCS-Attention%2C%20that%20selects%20the%20most%20informative%20channels%20for%20computing%20attention%20weights%2C%20leading%20to%20competitive%20models%20termed%20RIB-DCS.%20RIB%20is%20also%20incorporated%20into%20DNNs%20with%20existing%20channel%20attention%20modules%20to%20promote%20the%20learning%20of%20informative%20channel%20attention%20weights%2C%20leading%20to%20models%20termed%20RIB-CA.%20Both%20RIB-DCS%20and%20RIB-CA%20are%20applied%20to%20fixed%20neural%20network%20backbones%20and%20learnable%20backbones%20with%20Differentiable%20Neural%20Architecture%20Search%20%28DNAS%29.%20Extensive%20experiments%20on%20multiple%20person%20Re-ID%20benchmarks%20show%20that%20RIB%20significantly%20enhances%20the%20prediction%20accuracy%20of%20DNNs%20for%20person%20Re-ID%2C%20even%20for%20the%20occluded%20person%20Re-ID.%0ALink%3A%20http%3A//arxiv.org/abs/2505.08961v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Informative%2520Attention%2520Weights%2520for%2520Person%2520Re-Identification%26entry.906535625%3DYancheng%2520Wang%2520and%2520Nebojsa%2520Jojic%2520and%2520Yingzhen%2520Yang%26entry.1292438233%3DAttention%2520mechanisms%2520have%2520been%2520widely%2520used%2520in%2520deep%2520learning%252C%2520and%2520recent%2520efforts%2520have%2520been%2520devoted%2520to%2520incorporating%2520attention%2520modules%2520into%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520for%2520person%2520Re-Identification%2520%2528Re-ID%2529%2520to%2520enhance%2520their%2520discriminative%2520feature%2520learning%2520capabilities.%2520Existing%2520attention%2520modules%252C%2520including%2520self-attention%2520and%2520channel%2520attention%252C%2520learn%2520attention%2520weights%2520that%2520quantify%2520the%2520importance%2520of%2520feature%2520tokens%2520or%2520feature%2520channels.%2520However%252C%2520existing%2520attention%2520methods%2520do%2520not%2520explicitly%2520ensure%2520that%2520the%2520attention%2520weights%2520are%2520informative%2520for%2520predicting%2520the%2520identity%2520of%2520the%2520person%2520in%2520the%2520input%2520image%252C%2520and%2520may%2520consequently%2520introduce%2520noisy%2520information%2520from%2520the%2520input%2520image.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520method%2520termed%2520Reduction%2520of%2520Information%2520Bottleneck%2520loss%2520%2528RIB%2529%252C%2520motivated%2520by%2520the%2520principle%2520of%2520the%2520Information%2520Bottleneck%2520%2528IB%2529.%2520A%2520novel%2520distribution-free%2520and%2520efficient%2520variational%2520upper%2520bound%2520for%2520the%2520IB%2520loss%2520%2528IBB%2529%252C%2520which%2520can%2520be%2520optimized%2520by%2520standard%2520SGD%252C%2520is%2520derived%2520and%2520incorporated%2520into%2520the%2520training%2520loss%2520of%2520the%2520RIB%2520models.%2520RIB%2520is%2520applied%2520to%2520DNNs%2520with%2520self-attention%2520modules%2520through%2520a%2520novel%2520Differentiable%2520Channel%2520Selection%2520Attention%2520module%252C%2520or%2520DCS-Attention%252C%2520that%2520selects%2520the%2520most%2520informative%2520channels%2520for%2520computing%2520attention%2520weights%252C%2520leading%2520to%2520competitive%2520models%2520termed%2520RIB-DCS.%2520RIB%2520is%2520also%2520incorporated%2520into%2520DNNs%2520with%2520existing%2520channel%2520attention%2520modules%2520to%2520promote%2520the%2520learning%2520of%2520informative%2520channel%2520attention%2520weights%252C%2520leading%2520to%2520models%2520termed%2520RIB-CA.%2520Both%2520RIB-DCS%2520and%2520RIB-CA%2520are%2520applied%2520to%2520fixed%2520neural%2520network%2520backbones%2520and%2520learnable%2520backbones%2520with%2520Differentiable%2520Neural%2520Architecture%2520Search%2520%2528DNAS%2529.%2520Extensive%2520experiments%2520on%2520multiple%2520person%2520Re-ID%2520benchmarks%2520show%2520that%2520RIB%2520significantly%2520enhances%2520the%2520prediction%2520accuracy%2520of%2520DNNs%2520for%2520person%2520Re-ID%252C%2520even%2520for%2520the%2520occluded%2520person%2520Re-ID.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08961v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Informative%20Attention%20Weights%20for%20Person%20Re-Identification&entry.906535625=Yancheng%20Wang%20and%20Nebojsa%20Jojic%20and%20Yingzhen%20Yang&entry.1292438233=Attention%20mechanisms%20have%20been%20widely%20used%20in%20deep%20learning%2C%20and%20recent%20efforts%20have%20been%20devoted%20to%20incorporating%20attention%20modules%20into%20deep%20neural%20networks%20%28DNNs%29%20for%20person%20Re-Identification%20%28Re-ID%29%20to%20enhance%20their%20discriminative%20feature%20learning%20capabilities.%20Existing%20attention%20modules%2C%20including%20self-attention%20and%20channel%20attention%2C%20learn%20attention%20weights%20that%20quantify%20the%20importance%20of%20feature%20tokens%20or%20feature%20channels.%20However%2C%20existing%20attention%20methods%20do%20not%20explicitly%20ensure%20that%20the%20attention%20weights%20are%20informative%20for%20predicting%20the%20identity%20of%20the%20person%20in%20the%20input%20image%2C%20and%20may%20consequently%20introduce%20noisy%20information%20from%20the%20input%20image.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20method%20termed%20Reduction%20of%20Information%20Bottleneck%20loss%20%28RIB%29%2C%20motivated%20by%20the%20principle%20of%20the%20Information%20Bottleneck%20%28IB%29.%20A%20novel%20distribution-free%20and%20efficient%20variational%20upper%20bound%20for%20the%20IB%20loss%20%28IBB%29%2C%20which%20can%20be%20optimized%20by%20standard%20SGD%2C%20is%20derived%20and%20incorporated%20into%20the%20training%20loss%20of%20the%20RIB%20models.%20RIB%20is%20applied%20to%20DNNs%20with%20self-attention%20modules%20through%20a%20novel%20Differentiable%20Channel%20Selection%20Attention%20module%2C%20or%20DCS-Attention%2C%20that%20selects%20the%20most%20informative%20channels%20for%20computing%20attention%20weights%2C%20leading%20to%20competitive%20models%20termed%20RIB-DCS.%20RIB%20is%20also%20incorporated%20into%20DNNs%20with%20existing%20channel%20attention%20modules%20to%20promote%20the%20learning%20of%20informative%20channel%20attention%20weights%2C%20leading%20to%20models%20termed%20RIB-CA.%20Both%20RIB-DCS%20and%20RIB-CA%20are%20applied%20to%20fixed%20neural%20network%20backbones%20and%20learnable%20backbones%20with%20Differentiable%20Neural%20Architecture%20Search%20%28DNAS%29.%20Extensive%20experiments%20on%20multiple%20person%20Re-ID%20benchmarks%20show%20that%20RIB%20significantly%20enhances%20the%20prediction%20accuracy%20of%20DNNs%20for%20person%20Re-ID%2C%20even%20for%20the%20occluded%20person%20Re-ID.&entry.1838667208=http%3A//arxiv.org/abs/2505.08961v2&entry.124074799=Read"},
{"title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer", "author": "Yuzhuo Chen and Zehua Ma and Jianhua Wang and Kai Kang and Shunyu Yao and Weiming Zhang", "abstract": "In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC's superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC's performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.", "link": "http://arxiv.org/abs/2508.00477v2", "date": "2025-12-23", "relevancy": 2.4964, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6367}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6251}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAMIC%3A%20Layout-Aware%20Multi-Image%20Composition%20via%20Scalability%20of%20Multimodal%20Diffusion%20Transformer&body=Title%3A%20LAMIC%3A%20Layout-Aware%20Multi-Image%20Composition%20via%20Scalability%20of%20Multimodal%20Diffusion%20Transformer%0AAuthor%3A%20Yuzhuo%20Chen%20and%20Zehua%20Ma%20and%20Jianhua%20Wang%20and%20Kai%20Kang%20and%20Shunyu%20Yao%20and%20Weiming%20Zhang%0AAbstract%3A%20In%20controllable%20image%20synthesis%2C%20generating%20coherent%20and%20consistent%20images%20from%20multiple%20references%20with%20spatial%20layout%20awareness%20remains%20an%20open%20challenge.%20We%20present%20LAMIC%2C%20a%20Layout-Aware%20Multi-Image%20Composition%20framework%20that%2C%20for%20the%20first%20time%2C%20extends%20single-reference%20diffusion%20models%20to%20multi-reference%20scenarios%20in%20a%20training-free%20manner.%20Built%20upon%20the%20MMDiT%20model%2C%20LAMIC%20introduces%20two%20plug-and-play%20attention%20mechanisms%3A%201%29%20Group%20Isolation%20Attention%20%28GIA%29%20to%20enhance%20entity%20disentanglement%3B%20and%202%29%20Region-Modulated%20Attention%20%28RMA%29%20to%20enable%20layout-aware%20generation.%20To%20comprehensively%20evaluate%20model%20capabilities%2C%20we%20further%20introduce%20three%20metrics%3A%201%29%20Inclusion%20Ratio%20%28IN-R%29%20and%20Fill%20Ratio%20%28FI-R%29%20for%20assessing%20layout%20control%3B%20and%202%29%20Background%20Similarity%20%28BG-S%29%20for%20measuring%20background%20consistency.%20Extensive%20experiments%20show%20that%20LAMIC%20achieves%20state-of-the-art%20performance%20across%20most%20major%20metrics%3A%20it%20consistently%20outperforms%20existing%20multi-reference%20baselines%20in%20ID-S%2C%20BG-S%2C%20IN-R%20and%20AVG%20scores%20across%20all%20settings%2C%20and%20achieves%20the%20best%20DPG%20in%20complex%20composition%20tasks.%20These%20results%20demonstrate%20LAMIC%27s%20superior%20abilities%20in%20identity%20keeping%2C%20background%20preservation%2C%20layout%20control%2C%20and%20prompt-following%2C%20all%20achieved%20without%20any%20training%20or%20fine-tuning%2C%20showcasing%20strong%20zero-shot%20generalization%20ability.%20By%20inheriting%20the%20strengths%20of%20advanced%20single-reference%20models%20and%20enabling%20seamless%20extension%20to%20multi-image%20scenarios%2C%20LAMIC%20establishes%20a%20new%20training-free%20paradigm%20for%20controllable%20multi-image%20composition.%20As%20foundation%20models%20continue%20to%20evolve%2C%20LAMIC%27s%20performance%20is%20expected%20to%20scale%20accordingly.%20Our%20implementation%20is%20available%20at%3A%20https%3A//github.com/Suchenl/LAMIC.%0ALink%3A%20http%3A//arxiv.org/abs/2508.00477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAMIC%253A%2520Layout-Aware%2520Multi-Image%2520Composition%2520via%2520Scalability%2520of%2520Multimodal%2520Diffusion%2520Transformer%26entry.906535625%3DYuzhuo%2520Chen%2520and%2520Zehua%2520Ma%2520and%2520Jianhua%2520Wang%2520and%2520Kai%2520Kang%2520and%2520Shunyu%2520Yao%2520and%2520Weiming%2520Zhang%26entry.1292438233%3DIn%2520controllable%2520image%2520synthesis%252C%2520generating%2520coherent%2520and%2520consistent%2520images%2520from%2520multiple%2520references%2520with%2520spatial%2520layout%2520awareness%2520remains%2520an%2520open%2520challenge.%2520We%2520present%2520LAMIC%252C%2520a%2520Layout-Aware%2520Multi-Image%2520Composition%2520framework%2520that%252C%2520for%2520the%2520first%2520time%252C%2520extends%2520single-reference%2520diffusion%2520models%2520to%2520multi-reference%2520scenarios%2520in%2520a%2520training-free%2520manner.%2520Built%2520upon%2520the%2520MMDiT%2520model%252C%2520LAMIC%2520introduces%2520two%2520plug-and-play%2520attention%2520mechanisms%253A%25201%2529%2520Group%2520Isolation%2520Attention%2520%2528GIA%2529%2520to%2520enhance%2520entity%2520disentanglement%253B%2520and%25202%2529%2520Region-Modulated%2520Attention%2520%2528RMA%2529%2520to%2520enable%2520layout-aware%2520generation.%2520To%2520comprehensively%2520evaluate%2520model%2520capabilities%252C%2520we%2520further%2520introduce%2520three%2520metrics%253A%25201%2529%2520Inclusion%2520Ratio%2520%2528IN-R%2529%2520and%2520Fill%2520Ratio%2520%2528FI-R%2529%2520for%2520assessing%2520layout%2520control%253B%2520and%25202%2529%2520Background%2520Similarity%2520%2528BG-S%2529%2520for%2520measuring%2520background%2520consistency.%2520Extensive%2520experiments%2520show%2520that%2520LAMIC%2520achieves%2520state-of-the-art%2520performance%2520across%2520most%2520major%2520metrics%253A%2520it%2520consistently%2520outperforms%2520existing%2520multi-reference%2520baselines%2520in%2520ID-S%252C%2520BG-S%252C%2520IN-R%2520and%2520AVG%2520scores%2520across%2520all%2520settings%252C%2520and%2520achieves%2520the%2520best%2520DPG%2520in%2520complex%2520composition%2520tasks.%2520These%2520results%2520demonstrate%2520LAMIC%2527s%2520superior%2520abilities%2520in%2520identity%2520keeping%252C%2520background%2520preservation%252C%2520layout%2520control%252C%2520and%2520prompt-following%252C%2520all%2520achieved%2520without%2520any%2520training%2520or%2520fine-tuning%252C%2520showcasing%2520strong%2520zero-shot%2520generalization%2520ability.%2520By%2520inheriting%2520the%2520strengths%2520of%2520advanced%2520single-reference%2520models%2520and%2520enabling%2520seamless%2520extension%2520to%2520multi-image%2520scenarios%252C%2520LAMIC%2520establishes%2520a%2520new%2520training-free%2520paradigm%2520for%2520controllable%2520multi-image%2520composition.%2520As%2520foundation%2520models%2520continue%2520to%2520evolve%252C%2520LAMIC%2527s%2520performance%2520is%2520expected%2520to%2520scale%2520accordingly.%2520Our%2520implementation%2520is%2520available%2520at%253A%2520https%253A//github.com/Suchenl/LAMIC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAMIC%3A%20Layout-Aware%20Multi-Image%20Composition%20via%20Scalability%20of%20Multimodal%20Diffusion%20Transformer&entry.906535625=Yuzhuo%20Chen%20and%20Zehua%20Ma%20and%20Jianhua%20Wang%20and%20Kai%20Kang%20and%20Shunyu%20Yao%20and%20Weiming%20Zhang&entry.1292438233=In%20controllable%20image%20synthesis%2C%20generating%20coherent%20and%20consistent%20images%20from%20multiple%20references%20with%20spatial%20layout%20awareness%20remains%20an%20open%20challenge.%20We%20present%20LAMIC%2C%20a%20Layout-Aware%20Multi-Image%20Composition%20framework%20that%2C%20for%20the%20first%20time%2C%20extends%20single-reference%20diffusion%20models%20to%20multi-reference%20scenarios%20in%20a%20training-free%20manner.%20Built%20upon%20the%20MMDiT%20model%2C%20LAMIC%20introduces%20two%20plug-and-play%20attention%20mechanisms%3A%201%29%20Group%20Isolation%20Attention%20%28GIA%29%20to%20enhance%20entity%20disentanglement%3B%20and%202%29%20Region-Modulated%20Attention%20%28RMA%29%20to%20enable%20layout-aware%20generation.%20To%20comprehensively%20evaluate%20model%20capabilities%2C%20we%20further%20introduce%20three%20metrics%3A%201%29%20Inclusion%20Ratio%20%28IN-R%29%20and%20Fill%20Ratio%20%28FI-R%29%20for%20assessing%20layout%20control%3B%20and%202%29%20Background%20Similarity%20%28BG-S%29%20for%20measuring%20background%20consistency.%20Extensive%20experiments%20show%20that%20LAMIC%20achieves%20state-of-the-art%20performance%20across%20most%20major%20metrics%3A%20it%20consistently%20outperforms%20existing%20multi-reference%20baselines%20in%20ID-S%2C%20BG-S%2C%20IN-R%20and%20AVG%20scores%20across%20all%20settings%2C%20and%20achieves%20the%20best%20DPG%20in%20complex%20composition%20tasks.%20These%20results%20demonstrate%20LAMIC%27s%20superior%20abilities%20in%20identity%20keeping%2C%20background%20preservation%2C%20layout%20control%2C%20and%20prompt-following%2C%20all%20achieved%20without%20any%20training%20or%20fine-tuning%2C%20showcasing%20strong%20zero-shot%20generalization%20ability.%20By%20inheriting%20the%20strengths%20of%20advanced%20single-reference%20models%20and%20enabling%20seamless%20extension%20to%20multi-image%20scenarios%2C%20LAMIC%20establishes%20a%20new%20training-free%20paradigm%20for%20controllable%20multi-image%20composition.%20As%20foundation%20models%20continue%20to%20evolve%2C%20LAMIC%27s%20performance%20is%20expected%20to%20scale%20accordingly.%20Our%20implementation%20is%20available%20at%3A%20https%3A//github.com/Suchenl/LAMIC.&entry.1838667208=http%3A//arxiv.org/abs/2508.00477v2&entry.124074799=Read"},
{"title": "Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG", "author": "Kichang Lee and Songkuk Kim and JaeYeon Park and JeongGil Ko", "abstract": "On-device machine learning is often constrained by limited storage, particularly in continuous data collection scenarios. This paper presents an empirical study on storage-aware learning, focusing on the trade-off between data quantity and quality via compression. We demonstrate that naive strategies, such as uniform data dropping or one-size-fits-all compression, are suboptimal. Our findings further reveal that data samples exhibit varying sensitivities to compression, supporting the feasibility of a sample-wise adaptive compression strategy. These insights provide a foundation for developing a new class of storage-aware learning systems. The primary contribution of this work is the systematic characterization of this under-explored challenge, offering valuable insights that advance the understanding of storage-aware learning.", "link": "http://arxiv.org/abs/2508.12833v2", "date": "2025-12-23", "relevancy": 2.4936, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5236}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4866}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Storage-Aware%20Learning%20with%20Compressed%20Data%20An%20Empirical%20Exploratory%20Study%20on%20JPEG&body=Title%3A%20Toward%20Storage-Aware%20Learning%20with%20Compressed%20Data%20An%20Empirical%20Exploratory%20Study%20on%20JPEG%0AAuthor%3A%20Kichang%20Lee%20and%20Songkuk%20Kim%20and%20JaeYeon%20Park%20and%20JeongGil%20Ko%0AAbstract%3A%20On-device%20machine%20learning%20is%20often%20constrained%20by%20limited%20storage%2C%20particularly%20in%20continuous%20data%20collection%20scenarios.%20This%20paper%20presents%20an%20empirical%20study%20on%20storage-aware%20learning%2C%20focusing%20on%20the%20trade-off%20between%20data%20quantity%20and%20quality%20via%20compression.%20We%20demonstrate%20that%20naive%20strategies%2C%20such%20as%20uniform%20data%20dropping%20or%20one-size-fits-all%20compression%2C%20are%20suboptimal.%20Our%20findings%20further%20reveal%20that%20data%20samples%20exhibit%20varying%20sensitivities%20to%20compression%2C%20supporting%20the%20feasibility%20of%20a%20sample-wise%20adaptive%20compression%20strategy.%20These%20insights%20provide%20a%20foundation%20for%20developing%20a%20new%20class%20of%20storage-aware%20learning%20systems.%20The%20primary%20contribution%20of%20this%20work%20is%20the%20systematic%20characterization%20of%20this%20under-explored%20challenge%2C%20offering%20valuable%20insights%20that%20advance%20the%20understanding%20of%20storage-aware%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2508.12833v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Storage-Aware%2520Learning%2520with%2520Compressed%2520Data%2520An%2520Empirical%2520Exploratory%2520Study%2520on%2520JPEG%26entry.906535625%3DKichang%2520Lee%2520and%2520Songkuk%2520Kim%2520and%2520JaeYeon%2520Park%2520and%2520JeongGil%2520Ko%26entry.1292438233%3DOn-device%2520machine%2520learning%2520is%2520often%2520constrained%2520by%2520limited%2520storage%252C%2520particularly%2520in%2520continuous%2520data%2520collection%2520scenarios.%2520This%2520paper%2520presents%2520an%2520empirical%2520study%2520on%2520storage-aware%2520learning%252C%2520focusing%2520on%2520the%2520trade-off%2520between%2520data%2520quantity%2520and%2520quality%2520via%2520compression.%2520We%2520demonstrate%2520that%2520naive%2520strategies%252C%2520such%2520as%2520uniform%2520data%2520dropping%2520or%2520one-size-fits-all%2520compression%252C%2520are%2520suboptimal.%2520Our%2520findings%2520further%2520reveal%2520that%2520data%2520samples%2520exhibit%2520varying%2520sensitivities%2520to%2520compression%252C%2520supporting%2520the%2520feasibility%2520of%2520a%2520sample-wise%2520adaptive%2520compression%2520strategy.%2520These%2520insights%2520provide%2520a%2520foundation%2520for%2520developing%2520a%2520new%2520class%2520of%2520storage-aware%2520learning%2520systems.%2520The%2520primary%2520contribution%2520of%2520this%2520work%2520is%2520the%2520systematic%2520characterization%2520of%2520this%2520under-explored%2520challenge%252C%2520offering%2520valuable%2520insights%2520that%2520advance%2520the%2520understanding%2520of%2520storage-aware%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12833v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Storage-Aware%20Learning%20with%20Compressed%20Data%20An%20Empirical%20Exploratory%20Study%20on%20JPEG&entry.906535625=Kichang%20Lee%20and%20Songkuk%20Kim%20and%20JaeYeon%20Park%20and%20JeongGil%20Ko&entry.1292438233=On-device%20machine%20learning%20is%20often%20constrained%20by%20limited%20storage%2C%20particularly%20in%20continuous%20data%20collection%20scenarios.%20This%20paper%20presents%20an%20empirical%20study%20on%20storage-aware%20learning%2C%20focusing%20on%20the%20trade-off%20between%20data%20quantity%20and%20quality%20via%20compression.%20We%20demonstrate%20that%20naive%20strategies%2C%20such%20as%20uniform%20data%20dropping%20or%20one-size-fits-all%20compression%2C%20are%20suboptimal.%20Our%20findings%20further%20reveal%20that%20data%20samples%20exhibit%20varying%20sensitivities%20to%20compression%2C%20supporting%20the%20feasibility%20of%20a%20sample-wise%20adaptive%20compression%20strategy.%20These%20insights%20provide%20a%20foundation%20for%20developing%20a%20new%20class%20of%20storage-aware%20learning%20systems.%20The%20primary%20contribution%20of%20this%20work%20is%20the%20systematic%20characterization%20of%20this%20under-explored%20challenge%2C%20offering%20valuable%20insights%20that%20advance%20the%20understanding%20of%20storage-aware%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2508.12833v2&entry.124074799=Read"},
{"title": "Snapshot 3D image projection using a diffractive decoder", "author": "Cagatay Isil and Alexander Chen and Yuhang Li and F. Onuralp Ardic and Shiqi Chen and Che-Yung Shen and Aydogan Ozcan", "abstract": "3D image display is essential for next-generation volumetric imaging; however, dense depth multiplexing for 3D image projection remains challenging because diffraction-induced cross-talk rapidly increases as the axial image planes get closer. Here, we introduce a 3D display system comprising a digital encoder and a diffractive optical decoder, which simultaneously projects different images onto multiple target axial planes with high axial resolution. By leveraging multi-layer diffractive wavefront decoding and deep learning-based end-to-end optimization, the system achieves high-fidelity depth-resolved 3D image projection in a snapshot, enabling axial plane separations on the order of a wavelength. The digital encoder leverages a Fourier encoder network to capture multi-scale spatial and frequency-domain features from input images, integrates axial position encoding, and generates a unified phase representation that simultaneously encodes all images to be axially projected in a single snapshot through a jointly-optimized diffractive decoder. We characterized the impact of diffractive decoder depth, output diffraction efficiency, spatial light modulator resolution, and axial encoding density, revealing trade-offs that govern axial separation and 3D image projection quality. We further demonstrated the capability to display volumetric images containing 28 axial slices, as well as the ability to dynamically reconfigure the axial locations of the image planes, performed on demand. Finally, we experimentally validated the presented approach, demonstrating close agreement between the measured results and the target images. These results establish the diffractive 3D display system as a compact and scalable framework for depth-resolved snapshot 3D image projection, with potential applications in holographic displays, AR/VR interfaces, and volumetric optical computing.", "link": "http://arxiv.org/abs/2512.20464v1", "date": "2025-12-23", "relevancy": 2.4703, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6281}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6281}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Snapshot%203D%20image%20projection%20using%20a%20diffractive%20decoder&body=Title%3A%20Snapshot%203D%20image%20projection%20using%20a%20diffractive%20decoder%0AAuthor%3A%20Cagatay%20Isil%20and%20Alexander%20Chen%20and%20Yuhang%20Li%20and%20F.%20Onuralp%20Ardic%20and%20Shiqi%20Chen%20and%20Che-Yung%20Shen%20and%20Aydogan%20Ozcan%0AAbstract%3A%203D%20image%20display%20is%20essential%20for%20next-generation%20volumetric%20imaging%3B%20however%2C%20dense%20depth%20multiplexing%20for%203D%20image%20projection%20remains%20challenging%20because%20diffraction-induced%20cross-talk%20rapidly%20increases%20as%20the%20axial%20image%20planes%20get%20closer.%20Here%2C%20we%20introduce%20a%203D%20display%20system%20comprising%20a%20digital%20encoder%20and%20a%20diffractive%20optical%20decoder%2C%20which%20simultaneously%20projects%20different%20images%20onto%20multiple%20target%20axial%20planes%20with%20high%20axial%20resolution.%20By%20leveraging%20multi-layer%20diffractive%20wavefront%20decoding%20and%20deep%20learning-based%20end-to-end%20optimization%2C%20the%20system%20achieves%20high-fidelity%20depth-resolved%203D%20image%20projection%20in%20a%20snapshot%2C%20enabling%20axial%20plane%20separations%20on%20the%20order%20of%20a%20wavelength.%20The%20digital%20encoder%20leverages%20a%20Fourier%20encoder%20network%20to%20capture%20multi-scale%20spatial%20and%20frequency-domain%20features%20from%20input%20images%2C%20integrates%20axial%20position%20encoding%2C%20and%20generates%20a%20unified%20phase%20representation%20that%20simultaneously%20encodes%20all%20images%20to%20be%20axially%20projected%20in%20a%20single%20snapshot%20through%20a%20jointly-optimized%20diffractive%20decoder.%20We%20characterized%20the%20impact%20of%20diffractive%20decoder%20depth%2C%20output%20diffraction%20efficiency%2C%20spatial%20light%20modulator%20resolution%2C%20and%20axial%20encoding%20density%2C%20revealing%20trade-offs%20that%20govern%20axial%20separation%20and%203D%20image%20projection%20quality.%20We%20further%20demonstrated%20the%20capability%20to%20display%20volumetric%20images%20containing%2028%20axial%20slices%2C%20as%20well%20as%20the%20ability%20to%20dynamically%20reconfigure%20the%20axial%20locations%20of%20the%20image%20planes%2C%20performed%20on%20demand.%20Finally%2C%20we%20experimentally%20validated%20the%20presented%20approach%2C%20demonstrating%20close%20agreement%20between%20the%20measured%20results%20and%20the%20target%20images.%20These%20results%20establish%20the%20diffractive%203D%20display%20system%20as%20a%20compact%20and%20scalable%20framework%20for%20depth-resolved%20snapshot%203D%20image%20projection%2C%20with%20potential%20applications%20in%20holographic%20displays%2C%20AR/VR%20interfaces%2C%20and%20volumetric%20optical%20computing.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSnapshot%25203D%2520image%2520projection%2520using%2520a%2520diffractive%2520decoder%26entry.906535625%3DCagatay%2520Isil%2520and%2520Alexander%2520Chen%2520and%2520Yuhang%2520Li%2520and%2520F.%2520Onuralp%2520Ardic%2520and%2520Shiqi%2520Chen%2520and%2520Che-Yung%2520Shen%2520and%2520Aydogan%2520Ozcan%26entry.1292438233%3D3D%2520image%2520display%2520is%2520essential%2520for%2520next-generation%2520volumetric%2520imaging%253B%2520however%252C%2520dense%2520depth%2520multiplexing%2520for%25203D%2520image%2520projection%2520remains%2520challenging%2520because%2520diffraction-induced%2520cross-talk%2520rapidly%2520increases%2520as%2520the%2520axial%2520image%2520planes%2520get%2520closer.%2520Here%252C%2520we%2520introduce%2520a%25203D%2520display%2520system%2520comprising%2520a%2520digital%2520encoder%2520and%2520a%2520diffractive%2520optical%2520decoder%252C%2520which%2520simultaneously%2520projects%2520different%2520images%2520onto%2520multiple%2520target%2520axial%2520planes%2520with%2520high%2520axial%2520resolution.%2520By%2520leveraging%2520multi-layer%2520diffractive%2520wavefront%2520decoding%2520and%2520deep%2520learning-based%2520end-to-end%2520optimization%252C%2520the%2520system%2520achieves%2520high-fidelity%2520depth-resolved%25203D%2520image%2520projection%2520in%2520a%2520snapshot%252C%2520enabling%2520axial%2520plane%2520separations%2520on%2520the%2520order%2520of%2520a%2520wavelength.%2520The%2520digital%2520encoder%2520leverages%2520a%2520Fourier%2520encoder%2520network%2520to%2520capture%2520multi-scale%2520spatial%2520and%2520frequency-domain%2520features%2520from%2520input%2520images%252C%2520integrates%2520axial%2520position%2520encoding%252C%2520and%2520generates%2520a%2520unified%2520phase%2520representation%2520that%2520simultaneously%2520encodes%2520all%2520images%2520to%2520be%2520axially%2520projected%2520in%2520a%2520single%2520snapshot%2520through%2520a%2520jointly-optimized%2520diffractive%2520decoder.%2520We%2520characterized%2520the%2520impact%2520of%2520diffractive%2520decoder%2520depth%252C%2520output%2520diffraction%2520efficiency%252C%2520spatial%2520light%2520modulator%2520resolution%252C%2520and%2520axial%2520encoding%2520density%252C%2520revealing%2520trade-offs%2520that%2520govern%2520axial%2520separation%2520and%25203D%2520image%2520projection%2520quality.%2520We%2520further%2520demonstrated%2520the%2520capability%2520to%2520display%2520volumetric%2520images%2520containing%252028%2520axial%2520slices%252C%2520as%2520well%2520as%2520the%2520ability%2520to%2520dynamically%2520reconfigure%2520the%2520axial%2520locations%2520of%2520the%2520image%2520planes%252C%2520performed%2520on%2520demand.%2520Finally%252C%2520we%2520experimentally%2520validated%2520the%2520presented%2520approach%252C%2520demonstrating%2520close%2520agreement%2520between%2520the%2520measured%2520results%2520and%2520the%2520target%2520images.%2520These%2520results%2520establish%2520the%2520diffractive%25203D%2520display%2520system%2520as%2520a%2520compact%2520and%2520scalable%2520framework%2520for%2520depth-resolved%2520snapshot%25203D%2520image%2520projection%252C%2520with%2520potential%2520applications%2520in%2520holographic%2520displays%252C%2520AR/VR%2520interfaces%252C%2520and%2520volumetric%2520optical%2520computing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Snapshot%203D%20image%20projection%20using%20a%20diffractive%20decoder&entry.906535625=Cagatay%20Isil%20and%20Alexander%20Chen%20and%20Yuhang%20Li%20and%20F.%20Onuralp%20Ardic%20and%20Shiqi%20Chen%20and%20Che-Yung%20Shen%20and%20Aydogan%20Ozcan&entry.1292438233=3D%20image%20display%20is%20essential%20for%20next-generation%20volumetric%20imaging%3B%20however%2C%20dense%20depth%20multiplexing%20for%203D%20image%20projection%20remains%20challenging%20because%20diffraction-induced%20cross-talk%20rapidly%20increases%20as%20the%20axial%20image%20planes%20get%20closer.%20Here%2C%20we%20introduce%20a%203D%20display%20system%20comprising%20a%20digital%20encoder%20and%20a%20diffractive%20optical%20decoder%2C%20which%20simultaneously%20projects%20different%20images%20onto%20multiple%20target%20axial%20planes%20with%20high%20axial%20resolution.%20By%20leveraging%20multi-layer%20diffractive%20wavefront%20decoding%20and%20deep%20learning-based%20end-to-end%20optimization%2C%20the%20system%20achieves%20high-fidelity%20depth-resolved%203D%20image%20projection%20in%20a%20snapshot%2C%20enabling%20axial%20plane%20separations%20on%20the%20order%20of%20a%20wavelength.%20The%20digital%20encoder%20leverages%20a%20Fourier%20encoder%20network%20to%20capture%20multi-scale%20spatial%20and%20frequency-domain%20features%20from%20input%20images%2C%20integrates%20axial%20position%20encoding%2C%20and%20generates%20a%20unified%20phase%20representation%20that%20simultaneously%20encodes%20all%20images%20to%20be%20axially%20projected%20in%20a%20single%20snapshot%20through%20a%20jointly-optimized%20diffractive%20decoder.%20We%20characterized%20the%20impact%20of%20diffractive%20decoder%20depth%2C%20output%20diffraction%20efficiency%2C%20spatial%20light%20modulator%20resolution%2C%20and%20axial%20encoding%20density%2C%20revealing%20trade-offs%20that%20govern%20axial%20separation%20and%203D%20image%20projection%20quality.%20We%20further%20demonstrated%20the%20capability%20to%20display%20volumetric%20images%20containing%2028%20axial%20slices%2C%20as%20well%20as%20the%20ability%20to%20dynamically%20reconfigure%20the%20axial%20locations%20of%20the%20image%20planes%2C%20performed%20on%20demand.%20Finally%2C%20we%20experimentally%20validated%20the%20presented%20approach%2C%20demonstrating%20close%20agreement%20between%20the%20measured%20results%20and%20the%20target%20images.%20These%20results%20establish%20the%20diffractive%203D%20display%20system%20as%20a%20compact%20and%20scalable%20framework%20for%20depth-resolved%20snapshot%203D%20image%20projection%2C%20with%20potential%20applications%20in%20holographic%20displays%2C%20AR/VR%20interfaces%2C%20and%20volumetric%20optical%20computing.&entry.1838667208=http%3A//arxiv.org/abs/2512.20464v1&entry.124074799=Read"},
{"title": "CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images", "author": "Yujia Fu and Zhiyu Dong and Tianwen Qian and Chenye Zheng and Danian Ji and Linhai Zhuo", "abstract": "Accurate assessment of bowel cleanliness is essential for effective colonoscopy procedures. The Boston Bowel Preparation Scale (BBPS) offers a standardized scoring system but suffers from subjectivity and inter-observer variability when performed manually. In this paper, to support robust training and evaluation, we construct a high-quality colonoscopy dataset comprising 2,240 images from 517 subjects, annotated with expert-agreed BBPS scores. We propose a novel automated BBPS scoring framework that leverages the CLIP model with adapter-based transfer learning and a dedicated fecal-feature extraction branch. Our method fuses global visual features with stool-related textual priors to improve the accuracy of bowel cleanliness evaluation without requiring explicit segmentation. Extensive experiments on both our dataset and the public NERTHU dataset demonstrate the superiority of our approach over existing baselines, highlighting its potential for clinical deployment in computer-aided colonoscopy analysis.", "link": "http://arxiv.org/abs/2512.20374v1", "date": "2025-12-23", "relevancy": 2.4466, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4924}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4887}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP%20Based%20Region-Aware%20Feature%20Fusion%20for%20Automated%20BBPS%20Scoring%20in%20Colonoscopy%20Images&body=Title%3A%20CLIP%20Based%20Region-Aware%20Feature%20Fusion%20for%20Automated%20BBPS%20Scoring%20in%20Colonoscopy%20Images%0AAuthor%3A%20Yujia%20Fu%20and%20Zhiyu%20Dong%20and%20Tianwen%20Qian%20and%20Chenye%20Zheng%20and%20Danian%20Ji%20and%20Linhai%20Zhuo%0AAbstract%3A%20Accurate%20assessment%20of%20bowel%20cleanliness%20is%20essential%20for%20effective%20colonoscopy%20procedures.%20The%20Boston%20Bowel%20Preparation%20Scale%20%28BBPS%29%20offers%20a%20standardized%20scoring%20system%20but%20suffers%20from%20subjectivity%20and%20inter-observer%20variability%20when%20performed%20manually.%20In%20this%20paper%2C%20to%20support%20robust%20training%20and%20evaluation%2C%20we%20construct%20a%20high-quality%20colonoscopy%20dataset%20comprising%202%2C240%20images%20from%20517%20subjects%2C%20annotated%20with%20expert-agreed%20BBPS%20scores.%20We%20propose%20a%20novel%20automated%20BBPS%20scoring%20framework%20that%20leverages%20the%20CLIP%20model%20with%20adapter-based%20transfer%20learning%20and%20a%20dedicated%20fecal-feature%20extraction%20branch.%20Our%20method%20fuses%20global%20visual%20features%20with%20stool-related%20textual%20priors%20to%20improve%20the%20accuracy%20of%20bowel%20cleanliness%20evaluation%20without%20requiring%20explicit%20segmentation.%20Extensive%20experiments%20on%20both%20our%20dataset%20and%20the%20public%20NERTHU%20dataset%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20existing%20baselines%2C%20highlighting%20its%20potential%20for%20clinical%20deployment%20in%20computer-aided%20colonoscopy%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP%2520Based%2520Region-Aware%2520Feature%2520Fusion%2520for%2520Automated%2520BBPS%2520Scoring%2520in%2520Colonoscopy%2520Images%26entry.906535625%3DYujia%2520Fu%2520and%2520Zhiyu%2520Dong%2520and%2520Tianwen%2520Qian%2520and%2520Chenye%2520Zheng%2520and%2520Danian%2520Ji%2520and%2520Linhai%2520Zhuo%26entry.1292438233%3DAccurate%2520assessment%2520of%2520bowel%2520cleanliness%2520is%2520essential%2520for%2520effective%2520colonoscopy%2520procedures.%2520The%2520Boston%2520Bowel%2520Preparation%2520Scale%2520%2528BBPS%2529%2520offers%2520a%2520standardized%2520scoring%2520system%2520but%2520suffers%2520from%2520subjectivity%2520and%2520inter-observer%2520variability%2520when%2520performed%2520manually.%2520In%2520this%2520paper%252C%2520to%2520support%2520robust%2520training%2520and%2520evaluation%252C%2520we%2520construct%2520a%2520high-quality%2520colonoscopy%2520dataset%2520comprising%25202%252C240%2520images%2520from%2520517%2520subjects%252C%2520annotated%2520with%2520expert-agreed%2520BBPS%2520scores.%2520We%2520propose%2520a%2520novel%2520automated%2520BBPS%2520scoring%2520framework%2520that%2520leverages%2520the%2520CLIP%2520model%2520with%2520adapter-based%2520transfer%2520learning%2520and%2520a%2520dedicated%2520fecal-feature%2520extraction%2520branch.%2520Our%2520method%2520fuses%2520global%2520visual%2520features%2520with%2520stool-related%2520textual%2520priors%2520to%2520improve%2520the%2520accuracy%2520of%2520bowel%2520cleanliness%2520evaluation%2520without%2520requiring%2520explicit%2520segmentation.%2520Extensive%2520experiments%2520on%2520both%2520our%2520dataset%2520and%2520the%2520public%2520NERTHU%2520dataset%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520over%2520existing%2520baselines%252C%2520highlighting%2520its%2520potential%2520for%2520clinical%2520deployment%2520in%2520computer-aided%2520colonoscopy%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP%20Based%20Region-Aware%20Feature%20Fusion%20for%20Automated%20BBPS%20Scoring%20in%20Colonoscopy%20Images&entry.906535625=Yujia%20Fu%20and%20Zhiyu%20Dong%20and%20Tianwen%20Qian%20and%20Chenye%20Zheng%20and%20Danian%20Ji%20and%20Linhai%20Zhuo&entry.1292438233=Accurate%20assessment%20of%20bowel%20cleanliness%20is%20essential%20for%20effective%20colonoscopy%20procedures.%20The%20Boston%20Bowel%20Preparation%20Scale%20%28BBPS%29%20offers%20a%20standardized%20scoring%20system%20but%20suffers%20from%20subjectivity%20and%20inter-observer%20variability%20when%20performed%20manually.%20In%20this%20paper%2C%20to%20support%20robust%20training%20and%20evaluation%2C%20we%20construct%20a%20high-quality%20colonoscopy%20dataset%20comprising%202%2C240%20images%20from%20517%20subjects%2C%20annotated%20with%20expert-agreed%20BBPS%20scores.%20We%20propose%20a%20novel%20automated%20BBPS%20scoring%20framework%20that%20leverages%20the%20CLIP%20model%20with%20adapter-based%20transfer%20learning%20and%20a%20dedicated%20fecal-feature%20extraction%20branch.%20Our%20method%20fuses%20global%20visual%20features%20with%20stool-related%20textual%20priors%20to%20improve%20the%20accuracy%20of%20bowel%20cleanliness%20evaluation%20without%20requiring%20explicit%20segmentation.%20Extensive%20experiments%20on%20both%20our%20dataset%20and%20the%20public%20NERTHU%20dataset%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20existing%20baselines%2C%20highlighting%20its%20potential%20for%20clinical%20deployment%20in%20computer-aided%20colonoscopy%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2512.20374v1&entry.124074799=Read"},
{"title": "Learning Provably Improves the Convergence of Gradient Descent", "author": "Qingyu Song and Wei Lin and Hong Xu", "abstract": "Learn to Optimize (L2O) trains deep neural network-based solvers for optimization, achieving success in accelerating convex problems and improving non-convex solutions. However, L2O lacks rigorous theoretical backing for its own training convergence, as existing analyses often use unrealistic assumptions -- a gap this work highlights empirically. We bridge this gap by proving the training convergence of L2O models that learn Gradient Descent (GD) hyperparameters for quadratic programming, leveraging the Neural Tangent Kernel (NTK) theory. We propose a deterministic initialization strategy to support our theoretical results and promote stable training over extended optimization horizons by mitigating gradient explosion. Our L2O framework demonstrates over 50% better optimality than GD and superior robustness over state-of-the-art L2O methods on synthetic datasets. The code of our method can be found from https://github.com/NetX-lab/MathL2OProof-Official.", "link": "http://arxiv.org/abs/2501.18092v6", "date": "2025-12-23", "relevancy": 2.4436, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5069}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4852}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Provably%20Improves%20the%20Convergence%20of%20Gradient%20Descent&body=Title%3A%20Learning%20Provably%20Improves%20the%20Convergence%20of%20Gradient%20Descent%0AAuthor%3A%20Qingyu%20Song%20and%20Wei%20Lin%20and%20Hong%20Xu%0AAbstract%3A%20Learn%20to%20Optimize%20%28L2O%29%20trains%20deep%20neural%20network-based%20solvers%20for%20optimization%2C%20achieving%20success%20in%20accelerating%20convex%20problems%20and%20improving%20non-convex%20solutions.%20However%2C%20L2O%20lacks%20rigorous%20theoretical%20backing%20for%20its%20own%20training%20convergence%2C%20as%20existing%20analyses%20often%20use%20unrealistic%20assumptions%20--%20a%20gap%20this%20work%20highlights%20empirically.%20We%20bridge%20this%20gap%20by%20proving%20the%20training%20convergence%20of%20L2O%20models%20that%20learn%20Gradient%20Descent%20%28GD%29%20hyperparameters%20for%20quadratic%20programming%2C%20leveraging%20the%20Neural%20Tangent%20Kernel%20%28NTK%29%20theory.%20We%20propose%20a%20deterministic%20initialization%20strategy%20to%20support%20our%20theoretical%20results%20and%20promote%20stable%20training%20over%20extended%20optimization%20horizons%20by%20mitigating%20gradient%20explosion.%20Our%20L2O%20framework%20demonstrates%20over%2050%25%20better%20optimality%20than%20GD%20and%20superior%20robustness%20over%20state-of-the-art%20L2O%20methods%20on%20synthetic%20datasets.%20The%20code%20of%20our%20method%20can%20be%20found%20from%20https%3A//github.com/NetX-lab/MathL2OProof-Official.%0ALink%3A%20http%3A//arxiv.org/abs/2501.18092v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Provably%2520Improves%2520the%2520Convergence%2520of%2520Gradient%2520Descent%26entry.906535625%3DQingyu%2520Song%2520and%2520Wei%2520Lin%2520and%2520Hong%2520Xu%26entry.1292438233%3DLearn%2520to%2520Optimize%2520%2528L2O%2529%2520trains%2520deep%2520neural%2520network-based%2520solvers%2520for%2520optimization%252C%2520achieving%2520success%2520in%2520accelerating%2520convex%2520problems%2520and%2520improving%2520non-convex%2520solutions.%2520However%252C%2520L2O%2520lacks%2520rigorous%2520theoretical%2520backing%2520for%2520its%2520own%2520training%2520convergence%252C%2520as%2520existing%2520analyses%2520often%2520use%2520unrealistic%2520assumptions%2520--%2520a%2520gap%2520this%2520work%2520highlights%2520empirically.%2520We%2520bridge%2520this%2520gap%2520by%2520proving%2520the%2520training%2520convergence%2520of%2520L2O%2520models%2520that%2520learn%2520Gradient%2520Descent%2520%2528GD%2529%2520hyperparameters%2520for%2520quadratic%2520programming%252C%2520leveraging%2520the%2520Neural%2520Tangent%2520Kernel%2520%2528NTK%2529%2520theory.%2520We%2520propose%2520a%2520deterministic%2520initialization%2520strategy%2520to%2520support%2520our%2520theoretical%2520results%2520and%2520promote%2520stable%2520training%2520over%2520extended%2520optimization%2520horizons%2520by%2520mitigating%2520gradient%2520explosion.%2520Our%2520L2O%2520framework%2520demonstrates%2520over%252050%2525%2520better%2520optimality%2520than%2520GD%2520and%2520superior%2520robustness%2520over%2520state-of-the-art%2520L2O%2520methods%2520on%2520synthetic%2520datasets.%2520The%2520code%2520of%2520our%2520method%2520can%2520be%2520found%2520from%2520https%253A//github.com/NetX-lab/MathL2OProof-Official.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18092v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Provably%20Improves%20the%20Convergence%20of%20Gradient%20Descent&entry.906535625=Qingyu%20Song%20and%20Wei%20Lin%20and%20Hong%20Xu&entry.1292438233=Learn%20to%20Optimize%20%28L2O%29%20trains%20deep%20neural%20network-based%20solvers%20for%20optimization%2C%20achieving%20success%20in%20accelerating%20convex%20problems%20and%20improving%20non-convex%20solutions.%20However%2C%20L2O%20lacks%20rigorous%20theoretical%20backing%20for%20its%20own%20training%20convergence%2C%20as%20existing%20analyses%20often%20use%20unrealistic%20assumptions%20--%20a%20gap%20this%20work%20highlights%20empirically.%20We%20bridge%20this%20gap%20by%20proving%20the%20training%20convergence%20of%20L2O%20models%20that%20learn%20Gradient%20Descent%20%28GD%29%20hyperparameters%20for%20quadratic%20programming%2C%20leveraging%20the%20Neural%20Tangent%20Kernel%20%28NTK%29%20theory.%20We%20propose%20a%20deterministic%20initialization%20strategy%20to%20support%20our%20theoretical%20results%20and%20promote%20stable%20training%20over%20extended%20optimization%20horizons%20by%20mitigating%20gradient%20explosion.%20Our%20L2O%20framework%20demonstrates%20over%2050%25%20better%20optimality%20than%20GD%20and%20superior%20robustness%20over%20state-of-the-art%20L2O%20methods%20on%20synthetic%20datasets.%20The%20code%20of%20our%20method%20can%20be%20found%20from%20https%3A//github.com/NetX-lab/MathL2OProof-Official.&entry.1838667208=http%3A//arxiv.org/abs/2501.18092v6&entry.124074799=Read"},
{"title": "Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding", "author": "Xiangrui Cai and Shaocheng Ma and Lei Cao and Jie Li and Tianyu Liu and Yilin Dong", "abstract": "Electroencephalography (EEG) signal decoding is a key technology that translates brain activity into executable commands, laying the foundation for direct brain-machine interfacing and intelligent interaction. To address the inherent spatiotemporal heterogeneity of EEG signals, this paper proposes a multi-branch parallel architecture, where each temporal scale is equipped with an independent spatial feature extraction module. To further enhance multi-branch feature fusion, we propose a Fusion of Multiscale Features via Centralized Sparse-attention Network (EEG-CSANet), a centralized sparse-attention network. It employs a main-auxiliary branch architecture, where the main branch models core spatiotemporal patterns via multiscale self-attention, and the auxiliary branch facilitates efficient local interactions through sparse cross-attention. Experimental results show that EEG-CSANet achieves state-of-the-art (SOTA) performance across five public datasets (BCIC-IV-2A, BCIC-IV-2B, HGD, SEED, and SEED-VIG), with accuracies of 88.54%, 91.09%, 99.43%, 96.03%, and 90.56%, respectively. Such performance demonstrates its strong adaptability and robustness across various EEG decoding tasks. Moreover, extensive ablation studies are conducted to enhance the interpretability of EEG-CSANet. In the future, we hope that EEG-CSANet could serve as a promising baseline model in the field of EEG signal decoding. The source code is publicly available at: https://github.com/Xiangrui-Cai/EEG-CSANet", "link": "http://arxiv.org/abs/2512.18689v2", "date": "2025-12-23", "relevancy": 2.4395, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5203}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusion%20of%20Multiscale%20Features%20Via%20Centralized%20Sparse-attention%20Network%20for%20EEG%20Decoding&body=Title%3A%20Fusion%20of%20Multiscale%20Features%20Via%20Centralized%20Sparse-attention%20Network%20for%20EEG%20Decoding%0AAuthor%3A%20Xiangrui%20Cai%20and%20Shaocheng%20Ma%20and%20Lei%20Cao%20and%20Jie%20Li%20and%20Tianyu%20Liu%20and%20Yilin%20Dong%0AAbstract%3A%20Electroencephalography%20%28EEG%29%20signal%20decoding%20is%20a%20key%20technology%20that%20translates%20brain%20activity%20into%20executable%20commands%2C%20laying%20the%20foundation%20for%20direct%20brain-machine%20interfacing%20and%20intelligent%20interaction.%20To%20address%20the%20inherent%20spatiotemporal%20heterogeneity%20of%20EEG%20signals%2C%20this%20paper%20proposes%20a%20multi-branch%20parallel%20architecture%2C%20where%20each%20temporal%20scale%20is%20equipped%20with%20an%20independent%20spatial%20feature%20extraction%20module.%20To%20further%20enhance%20multi-branch%20feature%20fusion%2C%20we%20propose%20a%20Fusion%20of%20Multiscale%20Features%20via%20Centralized%20Sparse-attention%20Network%20%28EEG-CSANet%29%2C%20a%20centralized%20sparse-attention%20network.%20It%20employs%20a%20main-auxiliary%20branch%20architecture%2C%20where%20the%20main%20branch%20models%20core%20spatiotemporal%20patterns%20via%20multiscale%20self-attention%2C%20and%20the%20auxiliary%20branch%20facilitates%20efficient%20local%20interactions%20through%20sparse%20cross-attention.%20Experimental%20results%20show%20that%20EEG-CSANet%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20across%20five%20public%20datasets%20%28BCIC-IV-2A%2C%20BCIC-IV-2B%2C%20HGD%2C%20SEED%2C%20and%20SEED-VIG%29%2C%20with%20accuracies%20of%2088.54%25%2C%2091.09%25%2C%2099.43%25%2C%2096.03%25%2C%20and%2090.56%25%2C%20respectively.%20Such%20performance%20demonstrates%20its%20strong%20adaptability%20and%20robustness%20across%20various%20EEG%20decoding%20tasks.%20Moreover%2C%20extensive%20ablation%20studies%20are%20conducted%20to%20enhance%20the%20interpretability%20of%20EEG-CSANet.%20In%20the%20future%2C%20we%20hope%20that%20EEG-CSANet%20could%20serve%20as%20a%20promising%20baseline%20model%20in%20the%20field%20of%20EEG%20signal%20decoding.%20The%20source%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/Xiangrui-Cai/EEG-CSANet%0ALink%3A%20http%3A//arxiv.org/abs/2512.18689v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusion%2520of%2520Multiscale%2520Features%2520Via%2520Centralized%2520Sparse-attention%2520Network%2520for%2520EEG%2520Decoding%26entry.906535625%3DXiangrui%2520Cai%2520and%2520Shaocheng%2520Ma%2520and%2520Lei%2520Cao%2520and%2520Jie%2520Li%2520and%2520Tianyu%2520Liu%2520and%2520Yilin%2520Dong%26entry.1292438233%3DElectroencephalography%2520%2528EEG%2529%2520signal%2520decoding%2520is%2520a%2520key%2520technology%2520that%2520translates%2520brain%2520activity%2520into%2520executable%2520commands%252C%2520laying%2520the%2520foundation%2520for%2520direct%2520brain-machine%2520interfacing%2520and%2520intelligent%2520interaction.%2520To%2520address%2520the%2520inherent%2520spatiotemporal%2520heterogeneity%2520of%2520EEG%2520signals%252C%2520this%2520paper%2520proposes%2520a%2520multi-branch%2520parallel%2520architecture%252C%2520where%2520each%2520temporal%2520scale%2520is%2520equipped%2520with%2520an%2520independent%2520spatial%2520feature%2520extraction%2520module.%2520To%2520further%2520enhance%2520multi-branch%2520feature%2520fusion%252C%2520we%2520propose%2520a%2520Fusion%2520of%2520Multiscale%2520Features%2520via%2520Centralized%2520Sparse-attention%2520Network%2520%2528EEG-CSANet%2529%252C%2520a%2520centralized%2520sparse-attention%2520network.%2520It%2520employs%2520a%2520main-auxiliary%2520branch%2520architecture%252C%2520where%2520the%2520main%2520branch%2520models%2520core%2520spatiotemporal%2520patterns%2520via%2520multiscale%2520self-attention%252C%2520and%2520the%2520auxiliary%2520branch%2520facilitates%2520efficient%2520local%2520interactions%2520through%2520sparse%2520cross-attention.%2520Experimental%2520results%2520show%2520that%2520EEG-CSANet%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520across%2520five%2520public%2520datasets%2520%2528BCIC-IV-2A%252C%2520BCIC-IV-2B%252C%2520HGD%252C%2520SEED%252C%2520and%2520SEED-VIG%2529%252C%2520with%2520accuracies%2520of%252088.54%2525%252C%252091.09%2525%252C%252099.43%2525%252C%252096.03%2525%252C%2520and%252090.56%2525%252C%2520respectively.%2520Such%2520performance%2520demonstrates%2520its%2520strong%2520adaptability%2520and%2520robustness%2520across%2520various%2520EEG%2520decoding%2520tasks.%2520Moreover%252C%2520extensive%2520ablation%2520studies%2520are%2520conducted%2520to%2520enhance%2520the%2520interpretability%2520of%2520EEG-CSANet.%2520In%2520the%2520future%252C%2520we%2520hope%2520that%2520EEG-CSANet%2520could%2520serve%2520as%2520a%2520promising%2520baseline%2520model%2520in%2520the%2520field%2520of%2520EEG%2520signal%2520decoding.%2520The%2520source%2520code%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/Xiangrui-Cai/EEG-CSANet%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.18689v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusion%20of%20Multiscale%20Features%20Via%20Centralized%20Sparse-attention%20Network%20for%20EEG%20Decoding&entry.906535625=Xiangrui%20Cai%20and%20Shaocheng%20Ma%20and%20Lei%20Cao%20and%20Jie%20Li%20and%20Tianyu%20Liu%20and%20Yilin%20Dong&entry.1292438233=Electroencephalography%20%28EEG%29%20signal%20decoding%20is%20a%20key%20technology%20that%20translates%20brain%20activity%20into%20executable%20commands%2C%20laying%20the%20foundation%20for%20direct%20brain-machine%20interfacing%20and%20intelligent%20interaction.%20To%20address%20the%20inherent%20spatiotemporal%20heterogeneity%20of%20EEG%20signals%2C%20this%20paper%20proposes%20a%20multi-branch%20parallel%20architecture%2C%20where%20each%20temporal%20scale%20is%20equipped%20with%20an%20independent%20spatial%20feature%20extraction%20module.%20To%20further%20enhance%20multi-branch%20feature%20fusion%2C%20we%20propose%20a%20Fusion%20of%20Multiscale%20Features%20via%20Centralized%20Sparse-attention%20Network%20%28EEG-CSANet%29%2C%20a%20centralized%20sparse-attention%20network.%20It%20employs%20a%20main-auxiliary%20branch%20architecture%2C%20where%20the%20main%20branch%20models%20core%20spatiotemporal%20patterns%20via%20multiscale%20self-attention%2C%20and%20the%20auxiliary%20branch%20facilitates%20efficient%20local%20interactions%20through%20sparse%20cross-attention.%20Experimental%20results%20show%20that%20EEG-CSANet%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20across%20five%20public%20datasets%20%28BCIC-IV-2A%2C%20BCIC-IV-2B%2C%20HGD%2C%20SEED%2C%20and%20SEED-VIG%29%2C%20with%20accuracies%20of%2088.54%25%2C%2091.09%25%2C%2099.43%25%2C%2096.03%25%2C%20and%2090.56%25%2C%20respectively.%20Such%20performance%20demonstrates%20its%20strong%20adaptability%20and%20robustness%20across%20various%20EEG%20decoding%20tasks.%20Moreover%2C%20extensive%20ablation%20studies%20are%20conducted%20to%20enhance%20the%20interpretability%20of%20EEG-CSANet.%20In%20the%20future%2C%20we%20hope%20that%20EEG-CSANet%20could%20serve%20as%20a%20promising%20baseline%20model%20in%20the%20field%20of%20EEG%20signal%20decoding.%20The%20source%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/Xiangrui-Cai/EEG-CSANet&entry.1838667208=http%3A//arxiv.org/abs/2512.18689v2&entry.124074799=Read"},
{"title": "Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition", "author": "Gorjan Radevski", "abstract": "This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.\n  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.\n  Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.\n  Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.\n  Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.\n  Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.\n  These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.", "link": "http://arxiv.org/abs/2512.20501v1", "date": "2025-12-23", "relevancy": 2.407, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6308}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Modalities%20and%20Transferring%20Knowledge%3A%20Enhanced%20Multimodal%20Understanding%20and%20Recognition&body=Title%3A%20Bridging%20Modalities%20and%20Transferring%20Knowledge%3A%20Enhanced%20Multimodal%20Understanding%20and%20Recognition%0AAuthor%3A%20Gorjan%20Radevski%0AAbstract%3A%20This%20manuscript%20explores%20multimodal%20alignment%2C%20translation%2C%20fusion%2C%20and%20transference%20to%20enhance%20machine%20understanding%20of%20complex%20inputs.%20We%20organize%20the%20work%20into%20five%20chapters%2C%20each%20addressing%20unique%20challenges%20in%20multimodal%20machine%20learning.%0A%20%20Chapter%203%20introduces%20Spatial-Reasoning%20Bert%20for%20translating%20text-based%20spatial%20relations%20into%202D%20arrangements%20between%20clip-arts.%20This%20enables%20effective%20decoding%20of%20spatial%20language%20into%20visual%20representations%2C%20paving%20the%20way%20for%20automated%20scene%20generation%20aligned%20with%20human%20spatial%20understanding.%0A%20%20Chapter%204%20presents%20a%20method%20for%20translating%20medical%20texts%20into%20specific%203D%20locations%20within%20an%20anatomical%20atlas.%20We%20introduce%20a%20loss%20function%20leveraging%20spatial%20co-occurrences%20of%20medical%20terms%20to%20create%20interpretable%20mappings%2C%20significantly%20enhancing%20medical%20text%20navigability.%0A%20%20Chapter%205%20tackles%20translating%20structured%20text%20into%20canonical%20facts%20within%20knowledge%20graphs.%20We%20develop%20a%20benchmark%20for%20linking%20natural%20language%20to%20entities%20and%20predicates%2C%20addressing%20ambiguities%20in%20text%20extraction%20to%20provide%20clearer%2C%20actionable%20insights.%0A%20%20Chapter%206%20explores%20multimodal%20fusion%20methods%20for%20compositional%20action%20recognition.%20We%20propose%20a%20method%20fusing%20video%20frames%20and%20object%20detection%20representations%2C%20improving%20recognition%20robustness%20and%20accuracy.%0A%20%20Chapter%207%20investigates%20multimodal%20knowledge%20transference%20for%20egocentric%20action%20recognition.%20We%20demonstrate%20how%20multimodal%20knowledge%20distillation%20enables%20RGB-only%20models%20to%20mimic%20multimodal%20fusion-based%20capabilities%2C%20reducing%20computational%20requirements%20while%20maintaining%20performance.%0A%20%20These%20contributions%20advance%20methodologies%20for%20spatial%20language%20understanding%2C%20medical%20text%20interpretation%2C%20knowledge%20graph%20enrichment%2C%20and%20action%20recognition%2C%20enhancing%20computational%20systems%27%20ability%20to%20process%20complex%2C%20multimodal%20inputs%20across%20diverse%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20501v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Modalities%2520and%2520Transferring%2520Knowledge%253A%2520Enhanced%2520Multimodal%2520Understanding%2520and%2520Recognition%26entry.906535625%3DGorjan%2520Radevski%26entry.1292438233%3DThis%2520manuscript%2520explores%2520multimodal%2520alignment%252C%2520translation%252C%2520fusion%252C%2520and%2520transference%2520to%2520enhance%2520machine%2520understanding%2520of%2520complex%2520inputs.%2520We%2520organize%2520the%2520work%2520into%2520five%2520chapters%252C%2520each%2520addressing%2520unique%2520challenges%2520in%2520multimodal%2520machine%2520learning.%250A%2520%2520Chapter%25203%2520introduces%2520Spatial-Reasoning%2520Bert%2520for%2520translating%2520text-based%2520spatial%2520relations%2520into%25202D%2520arrangements%2520between%2520clip-arts.%2520This%2520enables%2520effective%2520decoding%2520of%2520spatial%2520language%2520into%2520visual%2520representations%252C%2520paving%2520the%2520way%2520for%2520automated%2520scene%2520generation%2520aligned%2520with%2520human%2520spatial%2520understanding.%250A%2520%2520Chapter%25204%2520presents%2520a%2520method%2520for%2520translating%2520medical%2520texts%2520into%2520specific%25203D%2520locations%2520within%2520an%2520anatomical%2520atlas.%2520We%2520introduce%2520a%2520loss%2520function%2520leveraging%2520spatial%2520co-occurrences%2520of%2520medical%2520terms%2520to%2520create%2520interpretable%2520mappings%252C%2520significantly%2520enhancing%2520medical%2520text%2520navigability.%250A%2520%2520Chapter%25205%2520tackles%2520translating%2520structured%2520text%2520into%2520canonical%2520facts%2520within%2520knowledge%2520graphs.%2520We%2520develop%2520a%2520benchmark%2520for%2520linking%2520natural%2520language%2520to%2520entities%2520and%2520predicates%252C%2520addressing%2520ambiguities%2520in%2520text%2520extraction%2520to%2520provide%2520clearer%252C%2520actionable%2520insights.%250A%2520%2520Chapter%25206%2520explores%2520multimodal%2520fusion%2520methods%2520for%2520compositional%2520action%2520recognition.%2520We%2520propose%2520a%2520method%2520fusing%2520video%2520frames%2520and%2520object%2520detection%2520representations%252C%2520improving%2520recognition%2520robustness%2520and%2520accuracy.%250A%2520%2520Chapter%25207%2520investigates%2520multimodal%2520knowledge%2520transference%2520for%2520egocentric%2520action%2520recognition.%2520We%2520demonstrate%2520how%2520multimodal%2520knowledge%2520distillation%2520enables%2520RGB-only%2520models%2520to%2520mimic%2520multimodal%2520fusion-based%2520capabilities%252C%2520reducing%2520computational%2520requirements%2520while%2520maintaining%2520performance.%250A%2520%2520These%2520contributions%2520advance%2520methodologies%2520for%2520spatial%2520language%2520understanding%252C%2520medical%2520text%2520interpretation%252C%2520knowledge%2520graph%2520enrichment%252C%2520and%2520action%2520recognition%252C%2520enhancing%2520computational%2520systems%2527%2520ability%2520to%2520process%2520complex%252C%2520multimodal%2520inputs%2520across%2520diverse%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20501v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Modalities%20and%20Transferring%20Knowledge%3A%20Enhanced%20Multimodal%20Understanding%20and%20Recognition&entry.906535625=Gorjan%20Radevski&entry.1292438233=This%20manuscript%20explores%20multimodal%20alignment%2C%20translation%2C%20fusion%2C%20and%20transference%20to%20enhance%20machine%20understanding%20of%20complex%20inputs.%20We%20organize%20the%20work%20into%20five%20chapters%2C%20each%20addressing%20unique%20challenges%20in%20multimodal%20machine%20learning.%0A%20%20Chapter%203%20introduces%20Spatial-Reasoning%20Bert%20for%20translating%20text-based%20spatial%20relations%20into%202D%20arrangements%20between%20clip-arts.%20This%20enables%20effective%20decoding%20of%20spatial%20language%20into%20visual%20representations%2C%20paving%20the%20way%20for%20automated%20scene%20generation%20aligned%20with%20human%20spatial%20understanding.%0A%20%20Chapter%204%20presents%20a%20method%20for%20translating%20medical%20texts%20into%20specific%203D%20locations%20within%20an%20anatomical%20atlas.%20We%20introduce%20a%20loss%20function%20leveraging%20spatial%20co-occurrences%20of%20medical%20terms%20to%20create%20interpretable%20mappings%2C%20significantly%20enhancing%20medical%20text%20navigability.%0A%20%20Chapter%205%20tackles%20translating%20structured%20text%20into%20canonical%20facts%20within%20knowledge%20graphs.%20We%20develop%20a%20benchmark%20for%20linking%20natural%20language%20to%20entities%20and%20predicates%2C%20addressing%20ambiguities%20in%20text%20extraction%20to%20provide%20clearer%2C%20actionable%20insights.%0A%20%20Chapter%206%20explores%20multimodal%20fusion%20methods%20for%20compositional%20action%20recognition.%20We%20propose%20a%20method%20fusing%20video%20frames%20and%20object%20detection%20representations%2C%20improving%20recognition%20robustness%20and%20accuracy.%0A%20%20Chapter%207%20investigates%20multimodal%20knowledge%20transference%20for%20egocentric%20action%20recognition.%20We%20demonstrate%20how%20multimodal%20knowledge%20distillation%20enables%20RGB-only%20models%20to%20mimic%20multimodal%20fusion-based%20capabilities%2C%20reducing%20computational%20requirements%20while%20maintaining%20performance.%0A%20%20These%20contributions%20advance%20methodologies%20for%20spatial%20language%20understanding%2C%20medical%20text%20interpretation%2C%20knowledge%20graph%20enrichment%2C%20and%20action%20recognition%2C%20enhancing%20computational%20systems%27%20ability%20to%20process%20complex%2C%20multimodal%20inputs%20across%20diverse%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.20501v1&entry.124074799=Read"},
{"title": "Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge", "author": "Marta Moscati and Ahmed Abdullah and Muhammad Saad Saeed and Shah Nawaz and Rohan Kumar Das and Muhammad Zaigham Zaheer and Junaid Mir and Muhammad Haroon Yousaf and Khalid Mahmood Malik and Markus Schedl", "abstract": "Over half of the world's population is bilingual and people often communicate under multilingual scenarios. The Face-Voice Association in Multilingual Environments (FAME) 2026 Challenge, held at ICASSP 2026, focuses on developing methods for face-voice association that are effective when the language at test-time is different than the training one. This report provides a brief summary of the challenge.", "link": "http://arxiv.org/abs/2512.20376v1", "date": "2025-12-23", "relevancy": 2.3807, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linking%20Faces%20and%20Voices%20Across%20Languages%3A%20Insights%20from%20the%20FAME%202026%20Challenge&body=Title%3A%20Linking%20Faces%20and%20Voices%20Across%20Languages%3A%20Insights%20from%20the%20FAME%202026%20Challenge%0AAuthor%3A%20Marta%20Moscati%20and%20Ahmed%20Abdullah%20and%20Muhammad%20Saad%20Saeed%20and%20Shah%20Nawaz%20and%20Rohan%20Kumar%20Das%20and%20Muhammad%20Zaigham%20Zaheer%20and%20Junaid%20Mir%20and%20Muhammad%20Haroon%20Yousaf%20and%20Khalid%20Mahmood%20Malik%20and%20Markus%20Schedl%0AAbstract%3A%20Over%20half%20of%20the%20world%27s%20population%20is%20bilingual%20and%20people%20often%20communicate%20under%20multilingual%20scenarios.%20The%20Face-Voice%20Association%20in%20Multilingual%20Environments%20%28FAME%29%202026%20Challenge%2C%20held%20at%20ICASSP%202026%2C%20focuses%20on%20developing%20methods%20for%20face-voice%20association%20that%20are%20effective%20when%20the%20language%20at%20test-time%20is%20different%20than%20the%20training%20one.%20This%20report%20provides%20a%20brief%20summary%20of%20the%20challenge.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20376v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinking%2520Faces%2520and%2520Voices%2520Across%2520Languages%253A%2520Insights%2520from%2520the%2520FAME%25202026%2520Challenge%26entry.906535625%3DMarta%2520Moscati%2520and%2520Ahmed%2520Abdullah%2520and%2520Muhammad%2520Saad%2520Saeed%2520and%2520Shah%2520Nawaz%2520and%2520Rohan%2520Kumar%2520Das%2520and%2520Muhammad%2520Zaigham%2520Zaheer%2520and%2520Junaid%2520Mir%2520and%2520Muhammad%2520Haroon%2520Yousaf%2520and%2520Khalid%2520Mahmood%2520Malik%2520and%2520Markus%2520Schedl%26entry.1292438233%3DOver%2520half%2520of%2520the%2520world%2527s%2520population%2520is%2520bilingual%2520and%2520people%2520often%2520communicate%2520under%2520multilingual%2520scenarios.%2520The%2520Face-Voice%2520Association%2520in%2520Multilingual%2520Environments%2520%2528FAME%2529%25202026%2520Challenge%252C%2520held%2520at%2520ICASSP%25202026%252C%2520focuses%2520on%2520developing%2520methods%2520for%2520face-voice%2520association%2520that%2520are%2520effective%2520when%2520the%2520language%2520at%2520test-time%2520is%2520different%2520than%2520the%2520training%2520one.%2520This%2520report%2520provides%2520a%2520brief%2520summary%2520of%2520the%2520challenge.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20376v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linking%20Faces%20and%20Voices%20Across%20Languages%3A%20Insights%20from%20the%20FAME%202026%20Challenge&entry.906535625=Marta%20Moscati%20and%20Ahmed%20Abdullah%20and%20Muhammad%20Saad%20Saeed%20and%20Shah%20Nawaz%20and%20Rohan%20Kumar%20Das%20and%20Muhammad%20Zaigham%20Zaheer%20and%20Junaid%20Mir%20and%20Muhammad%20Haroon%20Yousaf%20and%20Khalid%20Mahmood%20Malik%20and%20Markus%20Schedl&entry.1292438233=Over%20half%20of%20the%20world%27s%20population%20is%20bilingual%20and%20people%20often%20communicate%20under%20multilingual%20scenarios.%20The%20Face-Voice%20Association%20in%20Multilingual%20Environments%20%28FAME%29%202026%20Challenge%2C%20held%20at%20ICASSP%202026%2C%20focuses%20on%20developing%20methods%20for%20face-voice%20association%20that%20are%20effective%20when%20the%20language%20at%20test-time%20is%20different%20than%20the%20training%20one.%20This%20report%20provides%20a%20brief%20summary%20of%20the%20challenge.&entry.1838667208=http%3A//arxiv.org/abs/2512.20376v1&entry.124074799=Read"},
{"title": "Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent", "author": "Zhiyu Liu and Zhi Han and Yandong Tang and Jun Fan and Yao Wang", "abstract": "The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.", "link": "http://arxiv.org/abs/2512.07490v3", "date": "2025-12-23", "relevancy": 2.3703, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4749}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4739}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Low-Tubal-Rank%20Tensor%20Estimation%20via%20Alternating%20Preconditioned%20Gradient%20Descent&body=Title%3A%20Efficient%20Low-Tubal-Rank%20Tensor%20Estimation%20via%20Alternating%20Preconditioned%20Gradient%20Descent%0AAuthor%3A%20Zhiyu%20Liu%20and%20Zhi%20Han%20and%20Yandong%20Tang%20and%20Jun%20Fan%20and%20Yao%20Wang%0AAbstract%3A%20The%20problem%20of%20low-tubal-rank%20tensor%20estimation%20is%20a%20fundamental%20task%20with%20wide%20applications%20across%20high-dimensional%20signal%20processing%2C%20machine%20learning%2C%20and%20image%20science.%20Traditional%20approaches%20tackle%20such%20a%20problem%20by%20performing%20tensor%20singular%20value%20decomposition%2C%20which%20is%20computationally%20expensive%20and%20becomes%20infeasible%20for%20large-scale%20tensors.%20Recent%20approaches%20address%20this%20issue%20by%20factorizing%20the%20tensor%20into%20two%20smaller%20factor%20tensors%20and%20solving%20the%20resulting%20problem%20using%20gradient%20descent.%20However%2C%20this%20kind%20of%20approach%20requires%20an%20accurate%20estimate%20of%20the%20tensor%20rank%2C%20and%20when%20the%20rank%20is%20overestimated%2C%20the%20convergence%20of%20gradient%20descent%20and%20its%20variants%20slows%20down%20significantly%20or%20even%20diverges.%20To%20address%20this%20problem%2C%20we%20propose%20an%20Alternating%20Preconditioned%20Gradient%20Descent%20%28APGD%29%20algorithm%2C%20which%20accelerates%20convergence%20in%20the%20over-parameterized%20setting%20by%20adding%20a%20preconditioning%20term%20to%20the%20original%20gradient%20and%20updating%20these%20two%20factors%20alternately.%20Based%20on%20certain%20geometric%20assumptions%20on%20the%20objective%20function%2C%20we%20establish%20linear%20convergence%20guarantees%20for%20more%20general%20low-tubal-rank%20tensor%20estimation%20problems.%20Then%20we%20further%20analyze%20the%20specific%20cases%20of%20low-tubal-rank%20tensor%20factorization%20and%20low-tubal-rank%20tensor%20recovery.%20Our%20theoretical%20results%20show%20that%20APGD%20achieves%20linear%20convergence%20even%20under%20over-parameterization%2C%20and%20the%20convergence%20rate%20is%20independent%20of%20the%20tensor%20condition%20number.%20Extensive%20simulations%20on%20synthetic%20data%20are%20carried%20out%20to%20validate%20our%20theoretical%20assertions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07490v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Low-Tubal-Rank%2520Tensor%2520Estimation%2520via%2520Alternating%2520Preconditioned%2520Gradient%2520Descent%26entry.906535625%3DZhiyu%2520Liu%2520and%2520Zhi%2520Han%2520and%2520Yandong%2520Tang%2520and%2520Jun%2520Fan%2520and%2520Yao%2520Wang%26entry.1292438233%3DThe%2520problem%2520of%2520low-tubal-rank%2520tensor%2520estimation%2520is%2520a%2520fundamental%2520task%2520with%2520wide%2520applications%2520across%2520high-dimensional%2520signal%2520processing%252C%2520machine%2520learning%252C%2520and%2520image%2520science.%2520Traditional%2520approaches%2520tackle%2520such%2520a%2520problem%2520by%2520performing%2520tensor%2520singular%2520value%2520decomposition%252C%2520which%2520is%2520computationally%2520expensive%2520and%2520becomes%2520infeasible%2520for%2520large-scale%2520tensors.%2520Recent%2520approaches%2520address%2520this%2520issue%2520by%2520factorizing%2520the%2520tensor%2520into%2520two%2520smaller%2520factor%2520tensors%2520and%2520solving%2520the%2520resulting%2520problem%2520using%2520gradient%2520descent.%2520However%252C%2520this%2520kind%2520of%2520approach%2520requires%2520an%2520accurate%2520estimate%2520of%2520the%2520tensor%2520rank%252C%2520and%2520when%2520the%2520rank%2520is%2520overestimated%252C%2520the%2520convergence%2520of%2520gradient%2520descent%2520and%2520its%2520variants%2520slows%2520down%2520significantly%2520or%2520even%2520diverges.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520an%2520Alternating%2520Preconditioned%2520Gradient%2520Descent%2520%2528APGD%2529%2520algorithm%252C%2520which%2520accelerates%2520convergence%2520in%2520the%2520over-parameterized%2520setting%2520by%2520adding%2520a%2520preconditioning%2520term%2520to%2520the%2520original%2520gradient%2520and%2520updating%2520these%2520two%2520factors%2520alternately.%2520Based%2520on%2520certain%2520geometric%2520assumptions%2520on%2520the%2520objective%2520function%252C%2520we%2520establish%2520linear%2520convergence%2520guarantees%2520for%2520more%2520general%2520low-tubal-rank%2520tensor%2520estimation%2520problems.%2520Then%2520we%2520further%2520analyze%2520the%2520specific%2520cases%2520of%2520low-tubal-rank%2520tensor%2520factorization%2520and%2520low-tubal-rank%2520tensor%2520recovery.%2520Our%2520theoretical%2520results%2520show%2520that%2520APGD%2520achieves%2520linear%2520convergence%2520even%2520under%2520over-parameterization%252C%2520and%2520the%2520convergence%2520rate%2520is%2520independent%2520of%2520the%2520tensor%2520condition%2520number.%2520Extensive%2520simulations%2520on%2520synthetic%2520data%2520are%2520carried%2520out%2520to%2520validate%2520our%2520theoretical%2520assertions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07490v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Low-Tubal-Rank%20Tensor%20Estimation%20via%20Alternating%20Preconditioned%20Gradient%20Descent&entry.906535625=Zhiyu%20Liu%20and%20Zhi%20Han%20and%20Yandong%20Tang%20and%20Jun%20Fan%20and%20Yao%20Wang&entry.1292438233=The%20problem%20of%20low-tubal-rank%20tensor%20estimation%20is%20a%20fundamental%20task%20with%20wide%20applications%20across%20high-dimensional%20signal%20processing%2C%20machine%20learning%2C%20and%20image%20science.%20Traditional%20approaches%20tackle%20such%20a%20problem%20by%20performing%20tensor%20singular%20value%20decomposition%2C%20which%20is%20computationally%20expensive%20and%20becomes%20infeasible%20for%20large-scale%20tensors.%20Recent%20approaches%20address%20this%20issue%20by%20factorizing%20the%20tensor%20into%20two%20smaller%20factor%20tensors%20and%20solving%20the%20resulting%20problem%20using%20gradient%20descent.%20However%2C%20this%20kind%20of%20approach%20requires%20an%20accurate%20estimate%20of%20the%20tensor%20rank%2C%20and%20when%20the%20rank%20is%20overestimated%2C%20the%20convergence%20of%20gradient%20descent%20and%20its%20variants%20slows%20down%20significantly%20or%20even%20diverges.%20To%20address%20this%20problem%2C%20we%20propose%20an%20Alternating%20Preconditioned%20Gradient%20Descent%20%28APGD%29%20algorithm%2C%20which%20accelerates%20convergence%20in%20the%20over-parameterized%20setting%20by%20adding%20a%20preconditioning%20term%20to%20the%20original%20gradient%20and%20updating%20these%20two%20factors%20alternately.%20Based%20on%20certain%20geometric%20assumptions%20on%20the%20objective%20function%2C%20we%20establish%20linear%20convergence%20guarantees%20for%20more%20general%20low-tubal-rank%20tensor%20estimation%20problems.%20Then%20we%20further%20analyze%20the%20specific%20cases%20of%20low-tubal-rank%20tensor%20factorization%20and%20low-tubal-rank%20tensor%20recovery.%20Our%20theoretical%20results%20show%20that%20APGD%20achieves%20linear%20convergence%20even%20under%20over-parameterization%2C%20and%20the%20convergence%20rate%20is%20independent%20of%20the%20tensor%20condition%20number.%20Extensive%20simulations%20on%20synthetic%20data%20are%20carried%20out%20to%20validate%20our%20theoretical%20assertions.&entry.1838667208=http%3A//arxiv.org/abs/2512.07490v3&entry.124074799=Read"},
{"title": "Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings", "author": "Marko \u010cechovi\u010d and Nat\u00e1lia Komorn\u00edkov\u00e1 and Dominik Mach\u00e1\u010dek and Ond\u0159ej Bojar", "abstract": "Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.\n  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.", "link": "http://arxiv.org/abs/2512.20204v1", "date": "2025-12-23", "relevancy": 2.3593, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4837}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4837}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Corpus%20of%20Cross-lingual%20Dialogues%20with%20Minutes%20and%20Detection%20of%20Misunderstandings&body=Title%3A%20Corpus%20of%20Cross-lingual%20Dialogues%20with%20Minutes%20and%20Detection%20of%20Misunderstandings%0AAuthor%3A%20Marko%20%C4%8Cechovi%C4%8D%20and%20Nat%C3%A1lia%20Komorn%C3%ADkov%C3%A1%20and%20Dominik%20Mach%C3%A1%C4%8Dek%20and%20Ond%C5%99ej%20Bojar%0AAbstract%3A%20Speech%20processing%20and%20translation%20technology%20have%20the%20potential%20to%20facilitate%20meetings%20of%20individuals%20who%20do%20not%20share%20any%20common%20language.%20To%20evaluate%20automatic%20systems%20for%20such%20a%20task%2C%20a%20versatile%20and%20realistic%20evaluation%20corpus%20is%20needed.%20Therefore%2C%20we%20create%20and%20present%20a%20corpus%20of%20cross-lingual%20dialogues%20between%20individuals%20without%20a%20common%20language%20who%20were%20facilitated%20by%20automatic%20simultaneous%20speech%20translation.%20The%20corpus%20consists%20of%205%20hours%20of%20speech%20recordings%20with%20ASR%20and%20gold%20transcripts%20in%2012%20original%20languages%20and%20automatic%20and%20corrected%20translations%20into%20English.%20For%20the%20purposes%20of%20research%20into%20cross-lingual%20summarization%2C%20our%20corpus%20also%20includes%20written%20summaries%20%28minutes%29%20of%20the%20meetings.%0A%20%20Moreover%2C%20we%20propose%20automatic%20detection%20of%20misunderstandings.%20For%20an%20overview%20of%20this%20task%20and%20its%20complexity%2C%20we%20attempt%20to%20quantify%20misunderstandings%20in%20cross-lingual%20meetings.%20We%20annotate%20misunderstandings%20manually%20and%20also%20test%20the%20ability%20of%20current%20large%20language%20models%20to%20detect%20them%20automatically.%20The%20results%20show%20that%20the%20Gemini%20model%20is%20able%20to%20identify%20text%20spans%20with%20misunderstandings%20with%20recall%20of%2077%25%20and%20precision%20of%2047%25.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorpus%2520of%2520Cross-lingual%2520Dialogues%2520with%2520Minutes%2520and%2520Detection%2520of%2520Misunderstandings%26entry.906535625%3DMarko%2520%25C4%258Cechovi%25C4%258D%2520and%2520Nat%25C3%25A1lia%2520Komorn%25C3%25ADkov%25C3%25A1%2520and%2520Dominik%2520Mach%25C3%25A1%25C4%258Dek%2520and%2520Ond%25C5%2599ej%2520Bojar%26entry.1292438233%3DSpeech%2520processing%2520and%2520translation%2520technology%2520have%2520the%2520potential%2520to%2520facilitate%2520meetings%2520of%2520individuals%2520who%2520do%2520not%2520share%2520any%2520common%2520language.%2520To%2520evaluate%2520automatic%2520systems%2520for%2520such%2520a%2520task%252C%2520a%2520versatile%2520and%2520realistic%2520evaluation%2520corpus%2520is%2520needed.%2520Therefore%252C%2520we%2520create%2520and%2520present%2520a%2520corpus%2520of%2520cross-lingual%2520dialogues%2520between%2520individuals%2520without%2520a%2520common%2520language%2520who%2520were%2520facilitated%2520by%2520automatic%2520simultaneous%2520speech%2520translation.%2520The%2520corpus%2520consists%2520of%25205%2520hours%2520of%2520speech%2520recordings%2520with%2520ASR%2520and%2520gold%2520transcripts%2520in%252012%2520original%2520languages%2520and%2520automatic%2520and%2520corrected%2520translations%2520into%2520English.%2520For%2520the%2520purposes%2520of%2520research%2520into%2520cross-lingual%2520summarization%252C%2520our%2520corpus%2520also%2520includes%2520written%2520summaries%2520%2528minutes%2529%2520of%2520the%2520meetings.%250A%2520%2520Moreover%252C%2520we%2520propose%2520automatic%2520detection%2520of%2520misunderstandings.%2520For%2520an%2520overview%2520of%2520this%2520task%2520and%2520its%2520complexity%252C%2520we%2520attempt%2520to%2520quantify%2520misunderstandings%2520in%2520cross-lingual%2520meetings.%2520We%2520annotate%2520misunderstandings%2520manually%2520and%2520also%2520test%2520the%2520ability%2520of%2520current%2520large%2520language%2520models%2520to%2520detect%2520them%2520automatically.%2520The%2520results%2520show%2520that%2520the%2520Gemini%2520model%2520is%2520able%2520to%2520identify%2520text%2520spans%2520with%2520misunderstandings%2520with%2520recall%2520of%252077%2525%2520and%2520precision%2520of%252047%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Corpus%20of%20Cross-lingual%20Dialogues%20with%20Minutes%20and%20Detection%20of%20Misunderstandings&entry.906535625=Marko%20%C4%8Cechovi%C4%8D%20and%20Nat%C3%A1lia%20Komorn%C3%ADkov%C3%A1%20and%20Dominik%20Mach%C3%A1%C4%8Dek%20and%20Ond%C5%99ej%20Bojar&entry.1292438233=Speech%20processing%20and%20translation%20technology%20have%20the%20potential%20to%20facilitate%20meetings%20of%20individuals%20who%20do%20not%20share%20any%20common%20language.%20To%20evaluate%20automatic%20systems%20for%20such%20a%20task%2C%20a%20versatile%20and%20realistic%20evaluation%20corpus%20is%20needed.%20Therefore%2C%20we%20create%20and%20present%20a%20corpus%20of%20cross-lingual%20dialogues%20between%20individuals%20without%20a%20common%20language%20who%20were%20facilitated%20by%20automatic%20simultaneous%20speech%20translation.%20The%20corpus%20consists%20of%205%20hours%20of%20speech%20recordings%20with%20ASR%20and%20gold%20transcripts%20in%2012%20original%20languages%20and%20automatic%20and%20corrected%20translations%20into%20English.%20For%20the%20purposes%20of%20research%20into%20cross-lingual%20summarization%2C%20our%20corpus%20also%20includes%20written%20summaries%20%28minutes%29%20of%20the%20meetings.%0A%20%20Moreover%2C%20we%20propose%20automatic%20detection%20of%20misunderstandings.%20For%20an%20overview%20of%20this%20task%20and%20its%20complexity%2C%20we%20attempt%20to%20quantify%20misunderstandings%20in%20cross-lingual%20meetings.%20We%20annotate%20misunderstandings%20manually%20and%20also%20test%20the%20ability%20of%20current%20large%20language%20models%20to%20detect%20them%20automatically.%20The%20results%20show%20that%20the%20Gemini%20model%20is%20able%20to%20identify%20text%20spans%20with%20misunderstandings%20with%20recall%20of%2077%25%20and%20precision%20of%2047%25.&entry.1838667208=http%3A//arxiv.org/abs/2512.20204v1&entry.124074799=Read"},
{"title": "UbiQVision: Quantifying Uncertainty in XAI for Image Recognition", "author": "Akshat Dubey and Aleksandar An\u017eel and Bahar \u0130lgen and Georges Hattab", "abstract": "Recent advances in deep learning have led to its widespread adoption across diverse domains, including medical imaging. This progress is driven by increasingly sophisticated model architectures, such as ResNets, Vision Transformers, and Hybrid Convolutional Neural Networks, that offer enhanced performance at the cost of greater complexity. This complexity often compromises model explainability and interpretability. SHAP has emerged as a prominent method for providing interpretable visualizations that aid domain experts in understanding model predictions. However, SHAP explanations can be unstable and unreliable in the presence of epistemic and aleatoric uncertainty. In this study, we address this challenge by using Dirichlet posterior sampling and Dempster-Shafer theory to quantify the uncertainty that arises from these unstable explanations in medical imaging applications. The framework uses a belief, plausible, and fusion map approach alongside statistical quantitative analysis to produce quantification of uncertainty in SHAP. Furthermore, we evaluated our framework on three medical imaging datasets with varying class distributions, image qualities, and modality types which introduces noise due to varying image resolutions and modality-specific aspect covering the examples from pathology, ophthalmology, and radiology, introducing significant epistemic uncertainty.", "link": "http://arxiv.org/abs/2512.20288v1", "date": "2025-12-23", "relevancy": 2.3565, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6061}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5916}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UbiQVision%3A%20Quantifying%20Uncertainty%20in%20XAI%20for%20Image%20Recognition&body=Title%3A%20UbiQVision%3A%20Quantifying%20Uncertainty%20in%20XAI%20for%20Image%20Recognition%0AAuthor%3A%20Akshat%20Dubey%20and%20Aleksandar%20An%C5%BEel%20and%20Bahar%20%C4%B0lgen%20and%20Georges%20Hattab%0AAbstract%3A%20Recent%20advances%20in%20deep%20learning%20have%20led%20to%20its%20widespread%20adoption%20across%20diverse%20domains%2C%20including%20medical%20imaging.%20This%20progress%20is%20driven%20by%20increasingly%20sophisticated%20model%20architectures%2C%20such%20as%20ResNets%2C%20Vision%20Transformers%2C%20and%20Hybrid%20Convolutional%20Neural%20Networks%2C%20that%20offer%20enhanced%20performance%20at%20the%20cost%20of%20greater%20complexity.%20This%20complexity%20often%20compromises%20model%20explainability%20and%20interpretability.%20SHAP%20has%20emerged%20as%20a%20prominent%20method%20for%20providing%20interpretable%20visualizations%20that%20aid%20domain%20experts%20in%20understanding%20model%20predictions.%20However%2C%20SHAP%20explanations%20can%20be%20unstable%20and%20unreliable%20in%20the%20presence%20of%20epistemic%20and%20aleatoric%20uncertainty.%20In%20this%20study%2C%20we%20address%20this%20challenge%20by%20using%20Dirichlet%20posterior%20sampling%20and%20Dempster-Shafer%20theory%20to%20quantify%20the%20uncertainty%20that%20arises%20from%20these%20unstable%20explanations%20in%20medical%20imaging%20applications.%20The%20framework%20uses%20a%20belief%2C%20plausible%2C%20and%20fusion%20map%20approach%20alongside%20statistical%20quantitative%20analysis%20to%20produce%20quantification%20of%20uncertainty%20in%20SHAP.%20Furthermore%2C%20we%20evaluated%20our%20framework%20on%20three%20medical%20imaging%20datasets%20with%20varying%20class%20distributions%2C%20image%20qualities%2C%20and%20modality%20types%20which%20introduces%20noise%20due%20to%20varying%20image%20resolutions%20and%20modality-specific%20aspect%20covering%20the%20examples%20from%20pathology%2C%20ophthalmology%2C%20and%20radiology%2C%20introducing%20significant%20epistemic%20uncertainty.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUbiQVision%253A%2520Quantifying%2520Uncertainty%2520in%2520XAI%2520for%2520Image%2520Recognition%26entry.906535625%3DAkshat%2520Dubey%2520and%2520Aleksandar%2520An%25C5%25BEel%2520and%2520Bahar%2520%25C4%25B0lgen%2520and%2520Georges%2520Hattab%26entry.1292438233%3DRecent%2520advances%2520in%2520deep%2520learning%2520have%2520led%2520to%2520its%2520widespread%2520adoption%2520across%2520diverse%2520domains%252C%2520including%2520medical%2520imaging.%2520This%2520progress%2520is%2520driven%2520by%2520increasingly%2520sophisticated%2520model%2520architectures%252C%2520such%2520as%2520ResNets%252C%2520Vision%2520Transformers%252C%2520and%2520Hybrid%2520Convolutional%2520Neural%2520Networks%252C%2520that%2520offer%2520enhanced%2520performance%2520at%2520the%2520cost%2520of%2520greater%2520complexity.%2520This%2520complexity%2520often%2520compromises%2520model%2520explainability%2520and%2520interpretability.%2520SHAP%2520has%2520emerged%2520as%2520a%2520prominent%2520method%2520for%2520providing%2520interpretable%2520visualizations%2520that%2520aid%2520domain%2520experts%2520in%2520understanding%2520model%2520predictions.%2520However%252C%2520SHAP%2520explanations%2520can%2520be%2520unstable%2520and%2520unreliable%2520in%2520the%2520presence%2520of%2520epistemic%2520and%2520aleatoric%2520uncertainty.%2520In%2520this%2520study%252C%2520we%2520address%2520this%2520challenge%2520by%2520using%2520Dirichlet%2520posterior%2520sampling%2520and%2520Dempster-Shafer%2520theory%2520to%2520quantify%2520the%2520uncertainty%2520that%2520arises%2520from%2520these%2520unstable%2520explanations%2520in%2520medical%2520imaging%2520applications.%2520The%2520framework%2520uses%2520a%2520belief%252C%2520plausible%252C%2520and%2520fusion%2520map%2520approach%2520alongside%2520statistical%2520quantitative%2520analysis%2520to%2520produce%2520quantification%2520of%2520uncertainty%2520in%2520SHAP.%2520Furthermore%252C%2520we%2520evaluated%2520our%2520framework%2520on%2520three%2520medical%2520imaging%2520datasets%2520with%2520varying%2520class%2520distributions%252C%2520image%2520qualities%252C%2520and%2520modality%2520types%2520which%2520introduces%2520noise%2520due%2520to%2520varying%2520image%2520resolutions%2520and%2520modality-specific%2520aspect%2520covering%2520the%2520examples%2520from%2520pathology%252C%2520ophthalmology%252C%2520and%2520radiology%252C%2520introducing%2520significant%2520epistemic%2520uncertainty.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UbiQVision%3A%20Quantifying%20Uncertainty%20in%20XAI%20for%20Image%20Recognition&entry.906535625=Akshat%20Dubey%20and%20Aleksandar%20An%C5%BEel%20and%20Bahar%20%C4%B0lgen%20and%20Georges%20Hattab&entry.1292438233=Recent%20advances%20in%20deep%20learning%20have%20led%20to%20its%20widespread%20adoption%20across%20diverse%20domains%2C%20including%20medical%20imaging.%20This%20progress%20is%20driven%20by%20increasingly%20sophisticated%20model%20architectures%2C%20such%20as%20ResNets%2C%20Vision%20Transformers%2C%20and%20Hybrid%20Convolutional%20Neural%20Networks%2C%20that%20offer%20enhanced%20performance%20at%20the%20cost%20of%20greater%20complexity.%20This%20complexity%20often%20compromises%20model%20explainability%20and%20interpretability.%20SHAP%20has%20emerged%20as%20a%20prominent%20method%20for%20providing%20interpretable%20visualizations%20that%20aid%20domain%20experts%20in%20understanding%20model%20predictions.%20However%2C%20SHAP%20explanations%20can%20be%20unstable%20and%20unreliable%20in%20the%20presence%20of%20epistemic%20and%20aleatoric%20uncertainty.%20In%20this%20study%2C%20we%20address%20this%20challenge%20by%20using%20Dirichlet%20posterior%20sampling%20and%20Dempster-Shafer%20theory%20to%20quantify%20the%20uncertainty%20that%20arises%20from%20these%20unstable%20explanations%20in%20medical%20imaging%20applications.%20The%20framework%20uses%20a%20belief%2C%20plausible%2C%20and%20fusion%20map%20approach%20alongside%20statistical%20quantitative%20analysis%20to%20produce%20quantification%20of%20uncertainty%20in%20SHAP.%20Furthermore%2C%20we%20evaluated%20our%20framework%20on%20three%20medical%20imaging%20datasets%20with%20varying%20class%20distributions%2C%20image%20qualities%2C%20and%20modality%20types%20which%20introduces%20noise%20due%20to%20varying%20image%20resolutions%20and%20modality-specific%20aspect%20covering%20the%20examples%20from%20pathology%2C%20ophthalmology%2C%20and%20radiology%2C%20introducing%20significant%20epistemic%20uncertainty.&entry.1838667208=http%3A//arxiv.org/abs/2512.20288v1&entry.124074799=Read"},
{"title": "Active Intelligence in Video Avatars via Closed-loop World Modeling", "author": "Xuanhua He and Tianyu Yang and Ke Cao and Ruiqi Wu and Cheng Meng and Yong Zhang and Zhuoliang Kang and Xiaoming Wei and Qifeng Chen", "abstract": "Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.", "link": "http://arxiv.org/abs/2512.20615v1", "date": "2025-12-23", "relevancy": 2.3511, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5999}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5799}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Intelligence%20in%20Video%20Avatars%20via%20Closed-loop%20World%20Modeling&body=Title%3A%20Active%20Intelligence%20in%20Video%20Avatars%20via%20Closed-loop%20World%20Modeling%0AAuthor%3A%20Xuanhua%20He%20and%20Tianyu%20Yang%20and%20Ke%20Cao%20and%20Ruiqi%20Wu%20and%20Cheng%20Meng%20and%20Yong%20Zhang%20and%20Zhuoliang%20Kang%20and%20Xiaoming%20Wei%20and%20Qifeng%20Chen%0AAbstract%3A%20Current%20video%20avatar%20generation%20methods%20excel%20at%20identity%20preservation%20and%20motion%20alignment%20but%20lack%20genuine%20agency%2C%20they%20cannot%20autonomously%20pursue%20long-term%20goals%20through%20adaptive%20environmental%20interaction.%20We%20address%20this%20by%20introducing%20L-IVA%20%28Long-horizon%20Interactive%20Visual%20Avatar%29%2C%20a%20task%20and%20benchmark%20for%20evaluating%20goal-directed%20planning%20in%20stochastic%20generative%20environments%2C%20and%20ORCA%20%28Online%20Reasoning%20and%20Cognitive%20Architecture%29%2C%20the%20first%20framework%20enabling%20active%20intelligence%20in%20video%20avatars.%20ORCA%20embodies%20Internal%20World%20Model%20%28IWM%29%20capabilities%20through%20two%20key%20innovations%3A%20%281%29%20a%20closed-loop%20OTAR%20cycle%20%28Observe-Think-Act-Reflect%29%20that%20maintains%20robust%20state%20tracking%20under%20generative%20uncertainty%20by%20continuously%20verifying%20predicted%20outcomes%20against%20actual%20generations%2C%20and%20%282%29%20a%20hierarchical%20dual-system%20architecture%20where%20System%202%20performs%20strategic%20reasoning%20with%20state%20prediction%20while%20System%201%20translates%20abstract%20plans%20into%20precise%2C%20model-specific%20action%20captions.%20By%20formulating%20avatar%20control%20as%20a%20POMDP%20and%20implementing%20continuous%20belief%20updating%20with%20outcome%20verification%2C%20ORCA%20enables%20autonomous%20multi-step%20task%20completion%20in%20open-domain%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20ORCA%20significantly%20outperforms%20open-loop%20and%20non-reflective%20baselines%20in%20task%20success%20rate%20and%20behavioral%20coherence%2C%20validating%20our%20IWM-inspired%20design%20for%20advancing%20video%20avatar%20intelligence%20from%20passive%20animation%20to%20active%2C%20goal-oriented%20behavior.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Intelligence%2520in%2520Video%2520Avatars%2520via%2520Closed-loop%2520World%2520Modeling%26entry.906535625%3DXuanhua%2520He%2520and%2520Tianyu%2520Yang%2520and%2520Ke%2520Cao%2520and%2520Ruiqi%2520Wu%2520and%2520Cheng%2520Meng%2520and%2520Yong%2520Zhang%2520and%2520Zhuoliang%2520Kang%2520and%2520Xiaoming%2520Wei%2520and%2520Qifeng%2520Chen%26entry.1292438233%3DCurrent%2520video%2520avatar%2520generation%2520methods%2520excel%2520at%2520identity%2520preservation%2520and%2520motion%2520alignment%2520but%2520lack%2520genuine%2520agency%252C%2520they%2520cannot%2520autonomously%2520pursue%2520long-term%2520goals%2520through%2520adaptive%2520environmental%2520interaction.%2520We%2520address%2520this%2520by%2520introducing%2520L-IVA%2520%2528Long-horizon%2520Interactive%2520Visual%2520Avatar%2529%252C%2520a%2520task%2520and%2520benchmark%2520for%2520evaluating%2520goal-directed%2520planning%2520in%2520stochastic%2520generative%2520environments%252C%2520and%2520ORCA%2520%2528Online%2520Reasoning%2520and%2520Cognitive%2520Architecture%2529%252C%2520the%2520first%2520framework%2520enabling%2520active%2520intelligence%2520in%2520video%2520avatars.%2520ORCA%2520embodies%2520Internal%2520World%2520Model%2520%2528IWM%2529%2520capabilities%2520through%2520two%2520key%2520innovations%253A%2520%25281%2529%2520a%2520closed-loop%2520OTAR%2520cycle%2520%2528Observe-Think-Act-Reflect%2529%2520that%2520maintains%2520robust%2520state%2520tracking%2520under%2520generative%2520uncertainty%2520by%2520continuously%2520verifying%2520predicted%2520outcomes%2520against%2520actual%2520generations%252C%2520and%2520%25282%2529%2520a%2520hierarchical%2520dual-system%2520architecture%2520where%2520System%25202%2520performs%2520strategic%2520reasoning%2520with%2520state%2520prediction%2520while%2520System%25201%2520translates%2520abstract%2520plans%2520into%2520precise%252C%2520model-specific%2520action%2520captions.%2520By%2520formulating%2520avatar%2520control%2520as%2520a%2520POMDP%2520and%2520implementing%2520continuous%2520belief%2520updating%2520with%2520outcome%2520verification%252C%2520ORCA%2520enables%2520autonomous%2520multi-step%2520task%2520completion%2520in%2520open-domain%2520scenarios.%2520Extensive%2520experiments%2520demonstrate%2520that%2520ORCA%2520significantly%2520outperforms%2520open-loop%2520and%2520non-reflective%2520baselines%2520in%2520task%2520success%2520rate%2520and%2520behavioral%2520coherence%252C%2520validating%2520our%2520IWM-inspired%2520design%2520for%2520advancing%2520video%2520avatar%2520intelligence%2520from%2520passive%2520animation%2520to%2520active%252C%2520goal-oriented%2520behavior.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Intelligence%20in%20Video%20Avatars%20via%20Closed-loop%20World%20Modeling&entry.906535625=Xuanhua%20He%20and%20Tianyu%20Yang%20and%20Ke%20Cao%20and%20Ruiqi%20Wu%20and%20Cheng%20Meng%20and%20Yong%20Zhang%20and%20Zhuoliang%20Kang%20and%20Xiaoming%20Wei%20and%20Qifeng%20Chen&entry.1292438233=Current%20video%20avatar%20generation%20methods%20excel%20at%20identity%20preservation%20and%20motion%20alignment%20but%20lack%20genuine%20agency%2C%20they%20cannot%20autonomously%20pursue%20long-term%20goals%20through%20adaptive%20environmental%20interaction.%20We%20address%20this%20by%20introducing%20L-IVA%20%28Long-horizon%20Interactive%20Visual%20Avatar%29%2C%20a%20task%20and%20benchmark%20for%20evaluating%20goal-directed%20planning%20in%20stochastic%20generative%20environments%2C%20and%20ORCA%20%28Online%20Reasoning%20and%20Cognitive%20Architecture%29%2C%20the%20first%20framework%20enabling%20active%20intelligence%20in%20video%20avatars.%20ORCA%20embodies%20Internal%20World%20Model%20%28IWM%29%20capabilities%20through%20two%20key%20innovations%3A%20%281%29%20a%20closed-loop%20OTAR%20cycle%20%28Observe-Think-Act-Reflect%29%20that%20maintains%20robust%20state%20tracking%20under%20generative%20uncertainty%20by%20continuously%20verifying%20predicted%20outcomes%20against%20actual%20generations%2C%20and%20%282%29%20a%20hierarchical%20dual-system%20architecture%20where%20System%202%20performs%20strategic%20reasoning%20with%20state%20prediction%20while%20System%201%20translates%20abstract%20plans%20into%20precise%2C%20model-specific%20action%20captions.%20By%20formulating%20avatar%20control%20as%20a%20POMDP%20and%20implementing%20continuous%20belief%20updating%20with%20outcome%20verification%2C%20ORCA%20enables%20autonomous%20multi-step%20task%20completion%20in%20open-domain%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20ORCA%20significantly%20outperforms%20open-loop%20and%20non-reflective%20baselines%20in%20task%20success%20rate%20and%20behavioral%20coherence%2C%20validating%20our%20IWM-inspired%20design%20for%20advancing%20video%20avatar%20intelligence%20from%20passive%20animation%20to%20active%2C%20goal-oriented%20behavior.&entry.1838667208=http%3A//arxiv.org/abs/2512.20615v1&entry.124074799=Read"},
{"title": "Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding", "author": "Anh Dao and Manh Tran and Yufei Zhang and Xiaoming Liu and Zijun Cui", "abstract": "Human motion understanding has advanced rapidly through vision-based progress in recognition, tracking, and captioning. However, most existing methods overlook physical cues such as joint actuation forces that are fundamental in biomechanics. This gap motivates our study: if and when do physically inferred forces enhance motion understanding? By incorporating forces into established motion understanding pipelines, we systematically evaluate their impact across baseline models on 3 major tasks: gait recognition, action recognition, and fine-grained video captioning. Across 8 benchmarks, incorporating forces yields consistent performance gains; for example, on CASIA-B, Rank-1 gait recognition accuracy improved from 89.52% to 90.39% (+0.87), with larger gain observed under challenging conditions: +2.7% when wearing a coat and +3.0% at the side view. On Gait3D, performance also increases from 46.0% to 47.3% (+1.3). In action recognition, CTR-GCN achieved +2.00% on Penn Action, while high-exertion classes like punching/slapping improved by +6.96%. Even in video captioning, Qwen2.5-VL's ROUGE-L score rose from 0.310 to 0.339 (+0.029), indicating that physics-inferred forces enhance temporal grounding and semantic richness. These results demonstrate that force cues can substantially complement visual and kinematic features under dynamic, occluded, or appearance-varying conditions.", "link": "http://arxiv.org/abs/2512.20451v1", "date": "2025-12-23", "relevancy": 2.3334, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6204}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5864}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Motion%20Pattern%3A%20An%20Empirical%20Study%20of%20Physical%20Forces%20for%20Human%20Motion%20Understanding&body=Title%3A%20Beyond%20Motion%20Pattern%3A%20An%20Empirical%20Study%20of%20Physical%20Forces%20for%20Human%20Motion%20Understanding%0AAuthor%3A%20Anh%20Dao%20and%20Manh%20Tran%20and%20Yufei%20Zhang%20and%20Xiaoming%20Liu%20and%20Zijun%20Cui%0AAbstract%3A%20Human%20motion%20understanding%20has%20advanced%20rapidly%20through%20vision-based%20progress%20in%20recognition%2C%20tracking%2C%20and%20captioning.%20However%2C%20most%20existing%20methods%20overlook%20physical%20cues%20such%20as%20joint%20actuation%20forces%20that%20are%20fundamental%20in%20biomechanics.%20This%20gap%20motivates%20our%20study%3A%20if%20and%20when%20do%20physically%20inferred%20forces%20enhance%20motion%20understanding%3F%20By%20incorporating%20forces%20into%20established%20motion%20understanding%20pipelines%2C%20we%20systematically%20evaluate%20their%20impact%20across%20baseline%20models%20on%203%20major%20tasks%3A%20gait%20recognition%2C%20action%20recognition%2C%20and%20fine-grained%20video%20captioning.%20Across%208%20benchmarks%2C%20incorporating%20forces%20yields%20consistent%20performance%20gains%3B%20for%20example%2C%20on%20CASIA-B%2C%20Rank-1%20gait%20recognition%20accuracy%20improved%20from%2089.52%25%20to%2090.39%25%20%28%2B0.87%29%2C%20with%20larger%20gain%20observed%20under%20challenging%20conditions%3A%20%2B2.7%25%20when%20wearing%20a%20coat%20and%20%2B3.0%25%20at%20the%20side%20view.%20On%20Gait3D%2C%20performance%20also%20increases%20from%2046.0%25%20to%2047.3%25%20%28%2B1.3%29.%20In%20action%20recognition%2C%20CTR-GCN%20achieved%20%2B2.00%25%20on%20Penn%20Action%2C%20while%20high-exertion%20classes%20like%20punching/slapping%20improved%20by%20%2B6.96%25.%20Even%20in%20video%20captioning%2C%20Qwen2.5-VL%27s%20ROUGE-L%20score%20rose%20from%200.310%20to%200.339%20%28%2B0.029%29%2C%20indicating%20that%20physics-inferred%20forces%20enhance%20temporal%20grounding%20and%20semantic%20richness.%20These%20results%20demonstrate%20that%20force%20cues%20can%20substantially%20complement%20visual%20and%20kinematic%20features%20under%20dynamic%2C%20occluded%2C%20or%20appearance-varying%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Motion%2520Pattern%253A%2520An%2520Empirical%2520Study%2520of%2520Physical%2520Forces%2520for%2520Human%2520Motion%2520Understanding%26entry.906535625%3DAnh%2520Dao%2520and%2520Manh%2520Tran%2520and%2520Yufei%2520Zhang%2520and%2520Xiaoming%2520Liu%2520and%2520Zijun%2520Cui%26entry.1292438233%3DHuman%2520motion%2520understanding%2520has%2520advanced%2520rapidly%2520through%2520vision-based%2520progress%2520in%2520recognition%252C%2520tracking%252C%2520and%2520captioning.%2520However%252C%2520most%2520existing%2520methods%2520overlook%2520physical%2520cues%2520such%2520as%2520joint%2520actuation%2520forces%2520that%2520are%2520fundamental%2520in%2520biomechanics.%2520This%2520gap%2520motivates%2520our%2520study%253A%2520if%2520and%2520when%2520do%2520physically%2520inferred%2520forces%2520enhance%2520motion%2520understanding%253F%2520By%2520incorporating%2520forces%2520into%2520established%2520motion%2520understanding%2520pipelines%252C%2520we%2520systematically%2520evaluate%2520their%2520impact%2520across%2520baseline%2520models%2520on%25203%2520major%2520tasks%253A%2520gait%2520recognition%252C%2520action%2520recognition%252C%2520and%2520fine-grained%2520video%2520captioning.%2520Across%25208%2520benchmarks%252C%2520incorporating%2520forces%2520yields%2520consistent%2520performance%2520gains%253B%2520for%2520example%252C%2520on%2520CASIA-B%252C%2520Rank-1%2520gait%2520recognition%2520accuracy%2520improved%2520from%252089.52%2525%2520to%252090.39%2525%2520%2528%252B0.87%2529%252C%2520with%2520larger%2520gain%2520observed%2520under%2520challenging%2520conditions%253A%2520%252B2.7%2525%2520when%2520wearing%2520a%2520coat%2520and%2520%252B3.0%2525%2520at%2520the%2520side%2520view.%2520On%2520Gait3D%252C%2520performance%2520also%2520increases%2520from%252046.0%2525%2520to%252047.3%2525%2520%2528%252B1.3%2529.%2520In%2520action%2520recognition%252C%2520CTR-GCN%2520achieved%2520%252B2.00%2525%2520on%2520Penn%2520Action%252C%2520while%2520high-exertion%2520classes%2520like%2520punching/slapping%2520improved%2520by%2520%252B6.96%2525.%2520Even%2520in%2520video%2520captioning%252C%2520Qwen2.5-VL%2527s%2520ROUGE-L%2520score%2520rose%2520from%25200.310%2520to%25200.339%2520%2528%252B0.029%2529%252C%2520indicating%2520that%2520physics-inferred%2520forces%2520enhance%2520temporal%2520grounding%2520and%2520semantic%2520richness.%2520These%2520results%2520demonstrate%2520that%2520force%2520cues%2520can%2520substantially%2520complement%2520visual%2520and%2520kinematic%2520features%2520under%2520dynamic%252C%2520occluded%252C%2520or%2520appearance-varying%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Motion%20Pattern%3A%20An%20Empirical%20Study%20of%20Physical%20Forces%20for%20Human%20Motion%20Understanding&entry.906535625=Anh%20Dao%20and%20Manh%20Tran%20and%20Yufei%20Zhang%20and%20Xiaoming%20Liu%20and%20Zijun%20Cui&entry.1292438233=Human%20motion%20understanding%20has%20advanced%20rapidly%20through%20vision-based%20progress%20in%20recognition%2C%20tracking%2C%20and%20captioning.%20However%2C%20most%20existing%20methods%20overlook%20physical%20cues%20such%20as%20joint%20actuation%20forces%20that%20are%20fundamental%20in%20biomechanics.%20This%20gap%20motivates%20our%20study%3A%20if%20and%20when%20do%20physically%20inferred%20forces%20enhance%20motion%20understanding%3F%20By%20incorporating%20forces%20into%20established%20motion%20understanding%20pipelines%2C%20we%20systematically%20evaluate%20their%20impact%20across%20baseline%20models%20on%203%20major%20tasks%3A%20gait%20recognition%2C%20action%20recognition%2C%20and%20fine-grained%20video%20captioning.%20Across%208%20benchmarks%2C%20incorporating%20forces%20yields%20consistent%20performance%20gains%3B%20for%20example%2C%20on%20CASIA-B%2C%20Rank-1%20gait%20recognition%20accuracy%20improved%20from%2089.52%25%20to%2090.39%25%20%28%2B0.87%29%2C%20with%20larger%20gain%20observed%20under%20challenging%20conditions%3A%20%2B2.7%25%20when%20wearing%20a%20coat%20and%20%2B3.0%25%20at%20the%20side%20view.%20On%20Gait3D%2C%20performance%20also%20increases%20from%2046.0%25%20to%2047.3%25%20%28%2B1.3%29.%20In%20action%20recognition%2C%20CTR-GCN%20achieved%20%2B2.00%25%20on%20Penn%20Action%2C%20while%20high-exertion%20classes%20like%20punching/slapping%20improved%20by%20%2B6.96%25.%20Even%20in%20video%20captioning%2C%20Qwen2.5-VL%27s%20ROUGE-L%20score%20rose%20from%200.310%20to%200.339%20%28%2B0.029%29%2C%20indicating%20that%20physics-inferred%20forces%20enhance%20temporal%20grounding%20and%20semantic%20richness.%20These%20results%20demonstrate%20that%20force%20cues%20can%20substantially%20complement%20visual%20and%20kinematic%20features%20under%20dynamic%2C%20occluded%2C%20or%20appearance-varying%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2512.20451v1&entry.124074799=Read"},
{"title": "Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds", "author": "Tarik Houichime and Abdelghani Souhar and Younes El Amrani", "abstract": "The memory of contemporary Large Language Models is bound by a physical paradox: as they learn, they fill up. The linear accumulation (O(N)) of Key-Value states treats context as a warehouse of static artifacts, eventually forcing a destructive choice between amnesia and latency. We challenge this discrete orthodoxy, proposing that long-term memory is not the storage of items, but the persistence of a trajectory. We introduce Phonetic Trajectory Memory (PTM), a neuro-symbolic architecture that encodes language not as a sequence of tensors, but as a continuous path on an ergodic manifold governed by irrational rotation matrices. By decoupling the navigation (an invariant O(1) geometric signal) from the reconstruction (a probabilistic generative act), PTM achieves a compression magnitude of greater than 3,000x relative to dense caches. We demonstrate that retrieval becomes a process of resonance: the phonetic trace stabilizes the model against hallucination via \"Signal Consensus\" mechanism, securing up to approximately 92% factual accuracy. While this aggressive abstraction alters generative texture, it unlocks immediate access latency (approximately 34ms) independent of depth. Our results suggest that infinite context does not require infinite silicon; it requires treating memory not as data to be stored, but as a reconstructive process acting on a conserved, undying physical signal.", "link": "http://arxiv.org/abs/2512.20245v1", "date": "2025-12-23", "relevancy": 2.33, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory%20as%20Resonance%3A%20A%20Biomimetic%20Architecture%20for%20Infinite%20Context%20Memory%20on%20Ergodic%20Phonetic%20Manifolds&body=Title%3A%20Memory%20as%20Resonance%3A%20A%20Biomimetic%20Architecture%20for%20Infinite%20Context%20Memory%20on%20Ergodic%20Phonetic%20Manifolds%0AAuthor%3A%20Tarik%20Houichime%20and%20Abdelghani%20Souhar%20and%20Younes%20El%20Amrani%0AAbstract%3A%20The%20memory%20of%20contemporary%20Large%20Language%20Models%20is%20bound%20by%20a%20physical%20paradox%3A%20as%20they%20learn%2C%20they%20fill%20up.%20The%20linear%20accumulation%20%28O%28N%29%29%20of%20Key-Value%20states%20treats%20context%20as%20a%20warehouse%20of%20static%20artifacts%2C%20eventually%20forcing%20a%20destructive%20choice%20between%20amnesia%20and%20latency.%20We%20challenge%20this%20discrete%20orthodoxy%2C%20proposing%20that%20long-term%20memory%20is%20not%20the%20storage%20of%20items%2C%20but%20the%20persistence%20of%20a%20trajectory.%20We%20introduce%20Phonetic%20Trajectory%20Memory%20%28PTM%29%2C%20a%20neuro-symbolic%20architecture%20that%20encodes%20language%20not%20as%20a%20sequence%20of%20tensors%2C%20but%20as%20a%20continuous%20path%20on%20an%20ergodic%20manifold%20governed%20by%20irrational%20rotation%20matrices.%20By%20decoupling%20the%20navigation%20%28an%20invariant%20O%281%29%20geometric%20signal%29%20from%20the%20reconstruction%20%28a%20probabilistic%20generative%20act%29%2C%20PTM%20achieves%20a%20compression%20magnitude%20of%20greater%20than%203%2C000x%20relative%20to%20dense%20caches.%20We%20demonstrate%20that%20retrieval%20becomes%20a%20process%20of%20resonance%3A%20the%20phonetic%20trace%20stabilizes%20the%20model%20against%20hallucination%20via%20%22Signal%20Consensus%22%20mechanism%2C%20securing%20up%20to%20approximately%2092%25%20factual%20accuracy.%20While%20this%20aggressive%20abstraction%20alters%20generative%20texture%2C%20it%20unlocks%20immediate%20access%20latency%20%28approximately%2034ms%29%20independent%20of%20depth.%20Our%20results%20suggest%20that%20infinite%20context%20does%20not%20require%20infinite%20silicon%3B%20it%20requires%20treating%20memory%20not%20as%20data%20to%20be%20stored%2C%20but%20as%20a%20reconstructive%20process%20acting%20on%20a%20conserved%2C%20undying%20physical%20signal.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory%2520as%2520Resonance%253A%2520A%2520Biomimetic%2520Architecture%2520for%2520Infinite%2520Context%2520Memory%2520on%2520Ergodic%2520Phonetic%2520Manifolds%26entry.906535625%3DTarik%2520Houichime%2520and%2520Abdelghani%2520Souhar%2520and%2520Younes%2520El%2520Amrani%26entry.1292438233%3DThe%2520memory%2520of%2520contemporary%2520Large%2520Language%2520Models%2520is%2520bound%2520by%2520a%2520physical%2520paradox%253A%2520as%2520they%2520learn%252C%2520they%2520fill%2520up.%2520The%2520linear%2520accumulation%2520%2528O%2528N%2529%2529%2520of%2520Key-Value%2520states%2520treats%2520context%2520as%2520a%2520warehouse%2520of%2520static%2520artifacts%252C%2520eventually%2520forcing%2520a%2520destructive%2520choice%2520between%2520amnesia%2520and%2520latency.%2520We%2520challenge%2520this%2520discrete%2520orthodoxy%252C%2520proposing%2520that%2520long-term%2520memory%2520is%2520not%2520the%2520storage%2520of%2520items%252C%2520but%2520the%2520persistence%2520of%2520a%2520trajectory.%2520We%2520introduce%2520Phonetic%2520Trajectory%2520Memory%2520%2528PTM%2529%252C%2520a%2520neuro-symbolic%2520architecture%2520that%2520encodes%2520language%2520not%2520as%2520a%2520sequence%2520of%2520tensors%252C%2520but%2520as%2520a%2520continuous%2520path%2520on%2520an%2520ergodic%2520manifold%2520governed%2520by%2520irrational%2520rotation%2520matrices.%2520By%2520decoupling%2520the%2520navigation%2520%2528an%2520invariant%2520O%25281%2529%2520geometric%2520signal%2529%2520from%2520the%2520reconstruction%2520%2528a%2520probabilistic%2520generative%2520act%2529%252C%2520PTM%2520achieves%2520a%2520compression%2520magnitude%2520of%2520greater%2520than%25203%252C000x%2520relative%2520to%2520dense%2520caches.%2520We%2520demonstrate%2520that%2520retrieval%2520becomes%2520a%2520process%2520of%2520resonance%253A%2520the%2520phonetic%2520trace%2520stabilizes%2520the%2520model%2520against%2520hallucination%2520via%2520%2522Signal%2520Consensus%2522%2520mechanism%252C%2520securing%2520up%2520to%2520approximately%252092%2525%2520factual%2520accuracy.%2520While%2520this%2520aggressive%2520abstraction%2520alters%2520generative%2520texture%252C%2520it%2520unlocks%2520immediate%2520access%2520latency%2520%2528approximately%252034ms%2529%2520independent%2520of%2520depth.%2520Our%2520results%2520suggest%2520that%2520infinite%2520context%2520does%2520not%2520require%2520infinite%2520silicon%253B%2520it%2520requires%2520treating%2520memory%2520not%2520as%2520data%2520to%2520be%2520stored%252C%2520but%2520as%2520a%2520reconstructive%2520process%2520acting%2520on%2520a%2520conserved%252C%2520undying%2520physical%2520signal.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20as%20Resonance%3A%20A%20Biomimetic%20Architecture%20for%20Infinite%20Context%20Memory%20on%20Ergodic%20Phonetic%20Manifolds&entry.906535625=Tarik%20Houichime%20and%20Abdelghani%20Souhar%20and%20Younes%20El%20Amrani&entry.1292438233=The%20memory%20of%20contemporary%20Large%20Language%20Models%20is%20bound%20by%20a%20physical%20paradox%3A%20as%20they%20learn%2C%20they%20fill%20up.%20The%20linear%20accumulation%20%28O%28N%29%29%20of%20Key-Value%20states%20treats%20context%20as%20a%20warehouse%20of%20static%20artifacts%2C%20eventually%20forcing%20a%20destructive%20choice%20between%20amnesia%20and%20latency.%20We%20challenge%20this%20discrete%20orthodoxy%2C%20proposing%20that%20long-term%20memory%20is%20not%20the%20storage%20of%20items%2C%20but%20the%20persistence%20of%20a%20trajectory.%20We%20introduce%20Phonetic%20Trajectory%20Memory%20%28PTM%29%2C%20a%20neuro-symbolic%20architecture%20that%20encodes%20language%20not%20as%20a%20sequence%20of%20tensors%2C%20but%20as%20a%20continuous%20path%20on%20an%20ergodic%20manifold%20governed%20by%20irrational%20rotation%20matrices.%20By%20decoupling%20the%20navigation%20%28an%20invariant%20O%281%29%20geometric%20signal%29%20from%20the%20reconstruction%20%28a%20probabilistic%20generative%20act%29%2C%20PTM%20achieves%20a%20compression%20magnitude%20of%20greater%20than%203%2C000x%20relative%20to%20dense%20caches.%20We%20demonstrate%20that%20retrieval%20becomes%20a%20process%20of%20resonance%3A%20the%20phonetic%20trace%20stabilizes%20the%20model%20against%20hallucination%20via%20%22Signal%20Consensus%22%20mechanism%2C%20securing%20up%20to%20approximately%2092%25%20factual%20accuracy.%20While%20this%20aggressive%20abstraction%20alters%20generative%20texture%2C%20it%20unlocks%20immediate%20access%20latency%20%28approximately%2034ms%29%20independent%20of%20depth.%20Our%20results%20suggest%20that%20infinite%20context%20does%20not%20require%20infinite%20silicon%3B%20it%20requires%20treating%20memory%20not%20as%20data%20to%20be%20stored%2C%20but%20as%20a%20reconstructive%20process%20acting%20on%20a%20conserved%2C%20undying%20physical%20signal.&entry.1838667208=http%3A//arxiv.org/abs/2512.20245v1&entry.124074799=Read"},
{"title": "Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks", "author": "Abdullah Al Shafi and Abdul Muntakim and Pintu Chandra Shill and Rowzatul Zannat and Abdullah Al-Amin", "abstract": "Skin cancer can be identified by dermoscopic examination and ocular inspection, but early detection significantly increases survival chances. Artificial intelligence (AI), using annotated skin images and Convolutional Neural Networks (CNNs), improves diagnostic accuracy. This paper presents an early skin cancer classification method using a soft voting ensemble of CNNs. In this investigation, three benchmark datasets, namely HAM10000, ISIC 2016, and ISIC 2019, were used. The process involved rebalancing, image augmentation, and filtering techniques, followed by a hybrid dual encoder for segmentation via transfer learning. Accurate segmentation focused classification models on clinically significant features, reducing background artifacts and improving accuracy. Classification was performed through an ensemble of MobileNetV2, VGG19, and InceptionV3, balancing accuracy and speed for real-world deployment. The method achieved lesion recognition accuracies of 96.32\\%, 90.86\\%, and 93.92\\% for the three datasets. The system performance was evaluated using established skin lesion detection metrics, yielding impressive results.", "link": "http://arxiv.org/abs/2512.20431v1", "date": "2025-12-23", "relevancy": 2.3217, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4796}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4603}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skin%20Lesion%20Classification%20Using%20a%20Soft%20Voting%20Ensemble%20of%20Convolutional%20Neural%20Networks&body=Title%3A%20Skin%20Lesion%20Classification%20Using%20a%20Soft%20Voting%20Ensemble%20of%20Convolutional%20Neural%20Networks%0AAuthor%3A%20Abdullah%20Al%20Shafi%20and%20Abdul%20Muntakim%20and%20Pintu%20Chandra%20Shill%20and%20Rowzatul%20Zannat%20and%20Abdullah%20Al-Amin%0AAbstract%3A%20Skin%20cancer%20can%20be%20identified%20by%20dermoscopic%20examination%20and%20ocular%20inspection%2C%20but%20early%20detection%20significantly%20increases%20survival%20chances.%20Artificial%20intelligence%20%28AI%29%2C%20using%20annotated%20skin%20images%20and%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20improves%20diagnostic%20accuracy.%20This%20paper%20presents%20an%20early%20skin%20cancer%20classification%20method%20using%20a%20soft%20voting%20ensemble%20of%20CNNs.%20In%20this%20investigation%2C%20three%20benchmark%20datasets%2C%20namely%20HAM10000%2C%20ISIC%202016%2C%20and%20ISIC%202019%2C%20were%20used.%20The%20process%20involved%20rebalancing%2C%20image%20augmentation%2C%20and%20filtering%20techniques%2C%20followed%20by%20a%20hybrid%20dual%20encoder%20for%20segmentation%20via%20transfer%20learning.%20Accurate%20segmentation%20focused%20classification%20models%20on%20clinically%20significant%20features%2C%20reducing%20background%20artifacts%20and%20improving%20accuracy.%20Classification%20was%20performed%20through%20an%20ensemble%20of%20MobileNetV2%2C%20VGG19%2C%20and%20InceptionV3%2C%20balancing%20accuracy%20and%20speed%20for%20real-world%20deployment.%20The%20method%20achieved%20lesion%20recognition%20accuracies%20of%2096.32%5C%25%2C%2090.86%5C%25%2C%20and%2093.92%5C%25%20for%20the%20three%20datasets.%20The%20system%20performance%20was%20evaluated%20using%20established%20skin%20lesion%20detection%20metrics%2C%20yielding%20impressive%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkin%2520Lesion%2520Classification%2520Using%2520a%2520Soft%2520Voting%2520Ensemble%2520of%2520Convolutional%2520Neural%2520Networks%26entry.906535625%3DAbdullah%2520Al%2520Shafi%2520and%2520Abdul%2520Muntakim%2520and%2520Pintu%2520Chandra%2520Shill%2520and%2520Rowzatul%2520Zannat%2520and%2520Abdullah%2520Al-Amin%26entry.1292438233%3DSkin%2520cancer%2520can%2520be%2520identified%2520by%2520dermoscopic%2520examination%2520and%2520ocular%2520inspection%252C%2520but%2520early%2520detection%2520significantly%2520increases%2520survival%2520chances.%2520Artificial%2520intelligence%2520%2528AI%2529%252C%2520using%2520annotated%2520skin%2520images%2520and%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%252C%2520improves%2520diagnostic%2520accuracy.%2520This%2520paper%2520presents%2520an%2520early%2520skin%2520cancer%2520classification%2520method%2520using%2520a%2520soft%2520voting%2520ensemble%2520of%2520CNNs.%2520In%2520this%2520investigation%252C%2520three%2520benchmark%2520datasets%252C%2520namely%2520HAM10000%252C%2520ISIC%25202016%252C%2520and%2520ISIC%25202019%252C%2520were%2520used.%2520The%2520process%2520involved%2520rebalancing%252C%2520image%2520augmentation%252C%2520and%2520filtering%2520techniques%252C%2520followed%2520by%2520a%2520hybrid%2520dual%2520encoder%2520for%2520segmentation%2520via%2520transfer%2520learning.%2520Accurate%2520segmentation%2520focused%2520classification%2520models%2520on%2520clinically%2520significant%2520features%252C%2520reducing%2520background%2520artifacts%2520and%2520improving%2520accuracy.%2520Classification%2520was%2520performed%2520through%2520an%2520ensemble%2520of%2520MobileNetV2%252C%2520VGG19%252C%2520and%2520InceptionV3%252C%2520balancing%2520accuracy%2520and%2520speed%2520for%2520real-world%2520deployment.%2520The%2520method%2520achieved%2520lesion%2520recognition%2520accuracies%2520of%252096.32%255C%2525%252C%252090.86%255C%2525%252C%2520and%252093.92%255C%2525%2520for%2520the%2520three%2520datasets.%2520The%2520system%2520performance%2520was%2520evaluated%2520using%2520established%2520skin%2520lesion%2520detection%2520metrics%252C%2520yielding%2520impressive%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skin%20Lesion%20Classification%20Using%20a%20Soft%20Voting%20Ensemble%20of%20Convolutional%20Neural%20Networks&entry.906535625=Abdullah%20Al%20Shafi%20and%20Abdul%20Muntakim%20and%20Pintu%20Chandra%20Shill%20and%20Rowzatul%20Zannat%20and%20Abdullah%20Al-Amin&entry.1292438233=Skin%20cancer%20can%20be%20identified%20by%20dermoscopic%20examination%20and%20ocular%20inspection%2C%20but%20early%20detection%20significantly%20increases%20survival%20chances.%20Artificial%20intelligence%20%28AI%29%2C%20using%20annotated%20skin%20images%20and%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20improves%20diagnostic%20accuracy.%20This%20paper%20presents%20an%20early%20skin%20cancer%20classification%20method%20using%20a%20soft%20voting%20ensemble%20of%20CNNs.%20In%20this%20investigation%2C%20three%20benchmark%20datasets%2C%20namely%20HAM10000%2C%20ISIC%202016%2C%20and%20ISIC%202019%2C%20were%20used.%20The%20process%20involved%20rebalancing%2C%20image%20augmentation%2C%20and%20filtering%20techniques%2C%20followed%20by%20a%20hybrid%20dual%20encoder%20for%20segmentation%20via%20transfer%20learning.%20Accurate%20segmentation%20focused%20classification%20models%20on%20clinically%20significant%20features%2C%20reducing%20background%20artifacts%20and%20improving%20accuracy.%20Classification%20was%20performed%20through%20an%20ensemble%20of%20MobileNetV2%2C%20VGG19%2C%20and%20InceptionV3%2C%20balancing%20accuracy%20and%20speed%20for%20real-world%20deployment.%20The%20method%20achieved%20lesion%20recognition%20accuracies%20of%2096.32%5C%25%2C%2090.86%5C%25%2C%20and%2093.92%5C%25%20for%20the%20three%20datasets.%20The%20system%20performance%20was%20evaluated%20using%20established%20skin%20lesion%20detection%20metrics%2C%20yielding%20impressive%20results.&entry.1838667208=http%3A//arxiv.org/abs/2512.20431v1&entry.124074799=Read"},
{"title": "FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration", "author": "Hao Wei and Peiji Wang and Qianhao Wang and Tong Qin and Fei Gao and Yulin Si", "abstract": "Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation. While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms. Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots. FAR-AVIO embeds a Schur complement formulation into an Extended Kalman Filter(EKF), enabling joint pose-landmark optimization for accuracy while maintaining constant-time updates by efficiently marginalizing landmark states. On top of this backbone, we introduce Adaptive Weight Adjustment and Reliability Evaluation(AWARE), an online sensor health module that continuously assesses the reliability of visual, inertial and DVL measurements and adaptively regulates their sigma weights, and we develop an efficient online calibration scheme that jointly estimates DVL-IMU extrinsics, without dedicated calibration manoeuvres. Numerical simulations and real-world underwater experiments consistently show that FAR-AVIO outperforms state-of-the-art underwater SLAM baselines in both localization accuracy and computational efficiency, enabling robust operation on low-power embedded platforms. Our implementation has been released as open source software at https://far-vido.gitbook.io/far-vido-docs.", "link": "http://arxiv.org/abs/2512.20355v1", "date": "2025-12-23", "relevancy": 2.3109, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5899}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5896}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAR-AVIO%3A%20Fast%20and%20Robust%20Schur-Complement%20Based%20Acoustic-Visual-Inertial%20Fusion%20Odometry%20with%20Sensor%20Calibration&body=Title%3A%20FAR-AVIO%3A%20Fast%20and%20Robust%20Schur-Complement%20Based%20Acoustic-Visual-Inertial%20Fusion%20Odometry%20with%20Sensor%20Calibration%0AAuthor%3A%20Hao%20Wei%20and%20Peiji%20Wang%20and%20Qianhao%20Wang%20and%20Tong%20Qin%20and%20Fei%20Gao%20and%20Yulin%20Si%0AAbstract%3A%20Underwater%20environments%20impose%20severe%20challenges%20to%20visual-inertial%20odometry%20systems%2C%20as%20strong%20light%20attenuation%2C%20marine%20snow%20and%20turbidity%2C%20together%20with%20weakly%20exciting%20motions%2C%20degrade%20inertial%20observability%20and%20cause%20frequent%20tracking%20failures%20over%20long-term%20operation.%20While%20tightly%20coupled%20acoustic-visual-inertial%20fusion%2C%20typically%20implemented%20through%20an%20acoustic%20Doppler%20Velocity%20Log%20%28DVL%29%20integrated%20with%20visual-inertial%20measurements%2C%20can%20provide%20accurate%20state%20estimation%2C%20the%20associated%20graph-based%20optimization%20is%20often%20computationally%20prohibitive%20for%20real-time%20deployment%20on%20resource-constrained%20platforms.%20Here%20we%20present%20FAR-AVIO%2C%20a%20Schur-Complement%20based%2C%20tightly%20coupled%20acoustic-visual-inertial%20odometry%20framework%20tailored%20for%20underwater%20robots.%20FAR-AVIO%20embeds%20a%20Schur%20complement%20formulation%20into%20an%20Extended%20Kalman%20Filter%28EKF%29%2C%20enabling%20joint%20pose-landmark%20optimization%20for%20accuracy%20while%20maintaining%20constant-time%20updates%20by%20efficiently%20marginalizing%20landmark%20states.%20On%20top%20of%20this%20backbone%2C%20we%20introduce%20Adaptive%20Weight%20Adjustment%20and%20Reliability%20Evaluation%28AWARE%29%2C%20an%20online%20sensor%20health%20module%20that%20continuously%20assesses%20the%20reliability%20of%20visual%2C%20inertial%20and%20DVL%20measurements%20and%20adaptively%20regulates%20their%20sigma%20weights%2C%20and%20we%20develop%20an%20efficient%20online%20calibration%20scheme%20that%20jointly%20estimates%20DVL-IMU%20extrinsics%2C%20without%20dedicated%20calibration%20manoeuvres.%20Numerical%20simulations%20and%20real-world%20underwater%20experiments%20consistently%20show%20that%20FAR-AVIO%20outperforms%20state-of-the-art%20underwater%20SLAM%20baselines%20in%20both%20localization%20accuracy%20and%20computational%20efficiency%2C%20enabling%20robust%20operation%20on%20low-power%20embedded%20platforms.%20Our%20implementation%20has%20been%20released%20as%20open%20source%20software%20at%20https%3A//far-vido.gitbook.io/far-vido-docs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAR-AVIO%253A%2520Fast%2520and%2520Robust%2520Schur-Complement%2520Based%2520Acoustic-Visual-Inertial%2520Fusion%2520Odometry%2520with%2520Sensor%2520Calibration%26entry.906535625%3DHao%2520Wei%2520and%2520Peiji%2520Wang%2520and%2520Qianhao%2520Wang%2520and%2520Tong%2520Qin%2520and%2520Fei%2520Gao%2520and%2520Yulin%2520Si%26entry.1292438233%3DUnderwater%2520environments%2520impose%2520severe%2520challenges%2520to%2520visual-inertial%2520odometry%2520systems%252C%2520as%2520strong%2520light%2520attenuation%252C%2520marine%2520snow%2520and%2520turbidity%252C%2520together%2520with%2520weakly%2520exciting%2520motions%252C%2520degrade%2520inertial%2520observability%2520and%2520cause%2520frequent%2520tracking%2520failures%2520over%2520long-term%2520operation.%2520While%2520tightly%2520coupled%2520acoustic-visual-inertial%2520fusion%252C%2520typically%2520implemented%2520through%2520an%2520acoustic%2520Doppler%2520Velocity%2520Log%2520%2528DVL%2529%2520integrated%2520with%2520visual-inertial%2520measurements%252C%2520can%2520provide%2520accurate%2520state%2520estimation%252C%2520the%2520associated%2520graph-based%2520optimization%2520is%2520often%2520computationally%2520prohibitive%2520for%2520real-time%2520deployment%2520on%2520resource-constrained%2520platforms.%2520Here%2520we%2520present%2520FAR-AVIO%252C%2520a%2520Schur-Complement%2520based%252C%2520tightly%2520coupled%2520acoustic-visual-inertial%2520odometry%2520framework%2520tailored%2520for%2520underwater%2520robots.%2520FAR-AVIO%2520embeds%2520a%2520Schur%2520complement%2520formulation%2520into%2520an%2520Extended%2520Kalman%2520Filter%2528EKF%2529%252C%2520enabling%2520joint%2520pose-landmark%2520optimization%2520for%2520accuracy%2520while%2520maintaining%2520constant-time%2520updates%2520by%2520efficiently%2520marginalizing%2520landmark%2520states.%2520On%2520top%2520of%2520this%2520backbone%252C%2520we%2520introduce%2520Adaptive%2520Weight%2520Adjustment%2520and%2520Reliability%2520Evaluation%2528AWARE%2529%252C%2520an%2520online%2520sensor%2520health%2520module%2520that%2520continuously%2520assesses%2520the%2520reliability%2520of%2520visual%252C%2520inertial%2520and%2520DVL%2520measurements%2520and%2520adaptively%2520regulates%2520their%2520sigma%2520weights%252C%2520and%2520we%2520develop%2520an%2520efficient%2520online%2520calibration%2520scheme%2520that%2520jointly%2520estimates%2520DVL-IMU%2520extrinsics%252C%2520without%2520dedicated%2520calibration%2520manoeuvres.%2520Numerical%2520simulations%2520and%2520real-world%2520underwater%2520experiments%2520consistently%2520show%2520that%2520FAR-AVIO%2520outperforms%2520state-of-the-art%2520underwater%2520SLAM%2520baselines%2520in%2520both%2520localization%2520accuracy%2520and%2520computational%2520efficiency%252C%2520enabling%2520robust%2520operation%2520on%2520low-power%2520embedded%2520platforms.%2520Our%2520implementation%2520has%2520been%2520released%2520as%2520open%2520source%2520software%2520at%2520https%253A//far-vido.gitbook.io/far-vido-docs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAR-AVIO%3A%20Fast%20and%20Robust%20Schur-Complement%20Based%20Acoustic-Visual-Inertial%20Fusion%20Odometry%20with%20Sensor%20Calibration&entry.906535625=Hao%20Wei%20and%20Peiji%20Wang%20and%20Qianhao%20Wang%20and%20Tong%20Qin%20and%20Fei%20Gao%20and%20Yulin%20Si&entry.1292438233=Underwater%20environments%20impose%20severe%20challenges%20to%20visual-inertial%20odometry%20systems%2C%20as%20strong%20light%20attenuation%2C%20marine%20snow%20and%20turbidity%2C%20together%20with%20weakly%20exciting%20motions%2C%20degrade%20inertial%20observability%20and%20cause%20frequent%20tracking%20failures%20over%20long-term%20operation.%20While%20tightly%20coupled%20acoustic-visual-inertial%20fusion%2C%20typically%20implemented%20through%20an%20acoustic%20Doppler%20Velocity%20Log%20%28DVL%29%20integrated%20with%20visual-inertial%20measurements%2C%20can%20provide%20accurate%20state%20estimation%2C%20the%20associated%20graph-based%20optimization%20is%20often%20computationally%20prohibitive%20for%20real-time%20deployment%20on%20resource-constrained%20platforms.%20Here%20we%20present%20FAR-AVIO%2C%20a%20Schur-Complement%20based%2C%20tightly%20coupled%20acoustic-visual-inertial%20odometry%20framework%20tailored%20for%20underwater%20robots.%20FAR-AVIO%20embeds%20a%20Schur%20complement%20formulation%20into%20an%20Extended%20Kalman%20Filter%28EKF%29%2C%20enabling%20joint%20pose-landmark%20optimization%20for%20accuracy%20while%20maintaining%20constant-time%20updates%20by%20efficiently%20marginalizing%20landmark%20states.%20On%20top%20of%20this%20backbone%2C%20we%20introduce%20Adaptive%20Weight%20Adjustment%20and%20Reliability%20Evaluation%28AWARE%29%2C%20an%20online%20sensor%20health%20module%20that%20continuously%20assesses%20the%20reliability%20of%20visual%2C%20inertial%20and%20DVL%20measurements%20and%20adaptively%20regulates%20their%20sigma%20weights%2C%20and%20we%20develop%20an%20efficient%20online%20calibration%20scheme%20that%20jointly%20estimates%20DVL-IMU%20extrinsics%2C%20without%20dedicated%20calibration%20manoeuvres.%20Numerical%20simulations%20and%20real-world%20underwater%20experiments%20consistently%20show%20that%20FAR-AVIO%20outperforms%20state-of-the-art%20underwater%20SLAM%20baselines%20in%20both%20localization%20accuracy%20and%20computational%20efficiency%2C%20enabling%20robust%20operation%20on%20low-power%20embedded%20platforms.%20Our%20implementation%20has%20been%20released%20as%20open%20source%20software%20at%20https%3A//far-vido.gitbook.io/far-vido-docs.&entry.1838667208=http%3A//arxiv.org/abs/2512.20355v1&entry.124074799=Read"},
{"title": "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs", "author": "Dhruv Anand and Ehsan Shareghi", "abstract": "We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.", "link": "http://arxiv.org/abs/2512.20595v1", "date": "2025-12-23", "relevancy": 2.3077, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5843}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5843}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cube%20Bench%3A%20A%20Benchmark%20for%20Spatial%20Visual%20Reasoning%20in%20MLLMs&body=Title%3A%20Cube%20Bench%3A%20A%20Benchmark%20for%20Spatial%20Visual%20Reasoning%20in%20MLLMs%0AAuthor%3A%20Dhruv%20Anand%20and%20Ehsan%20Shareghi%0AAbstract%3A%20We%20introduce%20Cube%20Bench%2C%20a%20Rubik%27s-cube%20benchmark%20for%20evaluating%20spatial%20and%20sequential%20reasoning%20in%20multimodal%20large%20language%20models%20%28MLLMs%29.%20The%20benchmark%20decomposes%20performance%20into%20five%20skills%3A%20%28i%29%20reconstructing%20cube%20faces%20from%20images%20and%20text%2C%20%28ii%29%20choosing%20the%20optimal%20next%20move%2C%20%28iii%29%20predicting%20the%20outcome%20of%20a%20candidate%20move%20without%20applying%20it%2C%20%28iv%29%20executing%20multi-step%20plans%20while%20recovering%20from%20mistakes%2C%20and%20%28v%29%20detecting%20and%20revising%20one%27s%20own%20errors.%20Using%20a%20shared%20set%20of%20scrambled%20cube%20states%2C%20identical%20prompts%20and%20parsers%2C%20and%20a%20single%20distance-to-solved%20metric%2C%20we%20compare%20recent%20MLLMs%20side%20by%20side%20as%20a%20function%20of%20scramble%20depth.%20Across%20seven%20MLLMs%2C%20accuracy%20drops%20sharply%20with%20depth%3B%20once%20a%20trajectory%20stalls%20or%20diverges%2C%20models%20rarely%20recover%2C%20and%20high%20face-reconstruction%20accuracy%20does%20not%20guarantee%20competent%20action%20selection%20or%20multi-step%20execution.%20A%20pronounced%20closed-%20vs%20open-source%20gap%20emerges%3A%20the%20strongest%20closed%20model%20leads%20on%20both%20single-step%20perception%20tasks%20and%20multi-step%20control%20tasks%2C%20while%20open-weight%20models%20cluster%20near%20chance%20on%20the%20hardest%20settings%3B%20yet%20even%20the%20best%20MLLM%20degrades%20at%20higher%20cube%20complexity.%20A%20simple%20self-correction%20via%20reflective%20thinking%20yields%20modest%20gains%20but%20can%20also%20introduce%20overthinking.%20Cube%20Bench%20offers%20a%20compact%2C%20reproducible%20probe%20of%20sequential%20spatial%20reasoning%20in%20MLLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCube%2520Bench%253A%2520A%2520Benchmark%2520for%2520Spatial%2520Visual%2520Reasoning%2520in%2520MLLMs%26entry.906535625%3DDhruv%2520Anand%2520and%2520Ehsan%2520Shareghi%26entry.1292438233%3DWe%2520introduce%2520Cube%2520Bench%252C%2520a%2520Rubik%2527s-cube%2520benchmark%2520for%2520evaluating%2520spatial%2520and%2520sequential%2520reasoning%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520The%2520benchmark%2520decomposes%2520performance%2520into%2520five%2520skills%253A%2520%2528i%2529%2520reconstructing%2520cube%2520faces%2520from%2520images%2520and%2520text%252C%2520%2528ii%2529%2520choosing%2520the%2520optimal%2520next%2520move%252C%2520%2528iii%2529%2520predicting%2520the%2520outcome%2520of%2520a%2520candidate%2520move%2520without%2520applying%2520it%252C%2520%2528iv%2529%2520executing%2520multi-step%2520plans%2520while%2520recovering%2520from%2520mistakes%252C%2520and%2520%2528v%2529%2520detecting%2520and%2520revising%2520one%2527s%2520own%2520errors.%2520Using%2520a%2520shared%2520set%2520of%2520scrambled%2520cube%2520states%252C%2520identical%2520prompts%2520and%2520parsers%252C%2520and%2520a%2520single%2520distance-to-solved%2520metric%252C%2520we%2520compare%2520recent%2520MLLMs%2520side%2520by%2520side%2520as%2520a%2520function%2520of%2520scramble%2520depth.%2520Across%2520seven%2520MLLMs%252C%2520accuracy%2520drops%2520sharply%2520with%2520depth%253B%2520once%2520a%2520trajectory%2520stalls%2520or%2520diverges%252C%2520models%2520rarely%2520recover%252C%2520and%2520high%2520face-reconstruction%2520accuracy%2520does%2520not%2520guarantee%2520competent%2520action%2520selection%2520or%2520multi-step%2520execution.%2520A%2520pronounced%2520closed-%2520vs%2520open-source%2520gap%2520emerges%253A%2520the%2520strongest%2520closed%2520model%2520leads%2520on%2520both%2520single-step%2520perception%2520tasks%2520and%2520multi-step%2520control%2520tasks%252C%2520while%2520open-weight%2520models%2520cluster%2520near%2520chance%2520on%2520the%2520hardest%2520settings%253B%2520yet%2520even%2520the%2520best%2520MLLM%2520degrades%2520at%2520higher%2520cube%2520complexity.%2520A%2520simple%2520self-correction%2520via%2520reflective%2520thinking%2520yields%2520modest%2520gains%2520but%2520can%2520also%2520introduce%2520overthinking.%2520Cube%2520Bench%2520offers%2520a%2520compact%252C%2520reproducible%2520probe%2520of%2520sequential%2520spatial%2520reasoning%2520in%2520MLLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cube%20Bench%3A%20A%20Benchmark%20for%20Spatial%20Visual%20Reasoning%20in%20MLLMs&entry.906535625=Dhruv%20Anand%20and%20Ehsan%20Shareghi&entry.1292438233=We%20introduce%20Cube%20Bench%2C%20a%20Rubik%27s-cube%20benchmark%20for%20evaluating%20spatial%20and%20sequential%20reasoning%20in%20multimodal%20large%20language%20models%20%28MLLMs%29.%20The%20benchmark%20decomposes%20performance%20into%20five%20skills%3A%20%28i%29%20reconstructing%20cube%20faces%20from%20images%20and%20text%2C%20%28ii%29%20choosing%20the%20optimal%20next%20move%2C%20%28iii%29%20predicting%20the%20outcome%20of%20a%20candidate%20move%20without%20applying%20it%2C%20%28iv%29%20executing%20multi-step%20plans%20while%20recovering%20from%20mistakes%2C%20and%20%28v%29%20detecting%20and%20revising%20one%27s%20own%20errors.%20Using%20a%20shared%20set%20of%20scrambled%20cube%20states%2C%20identical%20prompts%20and%20parsers%2C%20and%20a%20single%20distance-to-solved%20metric%2C%20we%20compare%20recent%20MLLMs%20side%20by%20side%20as%20a%20function%20of%20scramble%20depth.%20Across%20seven%20MLLMs%2C%20accuracy%20drops%20sharply%20with%20depth%3B%20once%20a%20trajectory%20stalls%20or%20diverges%2C%20models%20rarely%20recover%2C%20and%20high%20face-reconstruction%20accuracy%20does%20not%20guarantee%20competent%20action%20selection%20or%20multi-step%20execution.%20A%20pronounced%20closed-%20vs%20open-source%20gap%20emerges%3A%20the%20strongest%20closed%20model%20leads%20on%20both%20single-step%20perception%20tasks%20and%20multi-step%20control%20tasks%2C%20while%20open-weight%20models%20cluster%20near%20chance%20on%20the%20hardest%20settings%3B%20yet%20even%20the%20best%20MLLM%20degrades%20at%20higher%20cube%20complexity.%20A%20simple%20self-correction%20via%20reflective%20thinking%20yields%20modest%20gains%20but%20can%20also%20introduce%20overthinking.%20Cube%20Bench%20offers%20a%20compact%2C%20reproducible%20probe%20of%20sequential%20spatial%20reasoning%20in%20MLLMs.&entry.1838667208=http%3A//arxiv.org/abs/2512.20595v1&entry.124074799=Read"},
{"title": "TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation", "author": "Ji-Hoon Kim and Junseok Ahn and Doyeop Kwak and Joon Son Chung and Shinji Watanabe", "abstract": "The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.", "link": "http://arxiv.org/abs/2512.20296v1", "date": "2025-12-23", "relevancy": 2.3063, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6027}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5783}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAVID%3A%20Text-Driven%20Audio-Visual%20Interactive%20Dialogue%20Generation&body=Title%3A%20TAVID%3A%20Text-Driven%20Audio-Visual%20Interactive%20Dialogue%20Generation%0AAuthor%3A%20Ji-Hoon%20Kim%20and%20Junseok%20Ahn%20and%20Doyeop%20Kwak%20and%20Joon%20Son%20Chung%20and%20Shinji%20Watanabe%0AAbstract%3A%20The%20objective%20of%20this%20paper%20is%20to%20jointly%20synthesize%20interactive%20videos%20and%20conversational%20speech%20from%20text%20and%20reference%20images.%20With%20the%20ultimate%20goal%20of%20building%20human-like%20conversational%20systems%2C%20recent%20studies%20have%20explored%20talking%20or%20listening%20head%20generation%20as%20well%20as%20conversational%20speech%20generation.%20However%2C%20these%20works%20are%20typically%20studied%20in%20isolation%2C%20overlooking%20the%20multimodal%20nature%20of%20human%20conversation%2C%20which%20involves%20tightly%20coupled%20audio-visual%20interactions.%20In%20this%20paper%2C%20we%20introduce%20TAVID%2C%20a%20unified%20framework%20that%20generates%20both%20interactive%20faces%20and%20conversational%20speech%20in%20a%20synchronized%20manner.%20TAVID%20integrates%20face%20and%20speech%20generation%20pipelines%20through%20two%20cross-modal%20mappers%20%28i.e.%2C%20a%20motion%20mapper%20and%20a%20speaker%20mapper%29%2C%20which%20enable%20bidirectional%20exchange%20of%20complementary%20information%20between%20the%20audio%20and%20visual%20modalities.%20We%20evaluate%20our%20system%20across%20four%20dimensions%3A%20talking%20face%20realism%2C%20listening%20head%20responsiveness%2C%20dyadic%20interaction%20fluency%2C%20and%20speech%20quality.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20approach%20across%20all%20these%20aspects.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAVID%253A%2520Text-Driven%2520Audio-Visual%2520Interactive%2520Dialogue%2520Generation%26entry.906535625%3DJi-Hoon%2520Kim%2520and%2520Junseok%2520Ahn%2520and%2520Doyeop%2520Kwak%2520and%2520Joon%2520Son%2520Chung%2520and%2520Shinji%2520Watanabe%26entry.1292438233%3DThe%2520objective%2520of%2520this%2520paper%2520is%2520to%2520jointly%2520synthesize%2520interactive%2520videos%2520and%2520conversational%2520speech%2520from%2520text%2520and%2520reference%2520images.%2520With%2520the%2520ultimate%2520goal%2520of%2520building%2520human-like%2520conversational%2520systems%252C%2520recent%2520studies%2520have%2520explored%2520talking%2520or%2520listening%2520head%2520generation%2520as%2520well%2520as%2520conversational%2520speech%2520generation.%2520However%252C%2520these%2520works%2520are%2520typically%2520studied%2520in%2520isolation%252C%2520overlooking%2520the%2520multimodal%2520nature%2520of%2520human%2520conversation%252C%2520which%2520involves%2520tightly%2520coupled%2520audio-visual%2520interactions.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520TAVID%252C%2520a%2520unified%2520framework%2520that%2520generates%2520both%2520interactive%2520faces%2520and%2520conversational%2520speech%2520in%2520a%2520synchronized%2520manner.%2520TAVID%2520integrates%2520face%2520and%2520speech%2520generation%2520pipelines%2520through%2520two%2520cross-modal%2520mappers%2520%2528i.e.%252C%2520a%2520motion%2520mapper%2520and%2520a%2520speaker%2520mapper%2529%252C%2520which%2520enable%2520bidirectional%2520exchange%2520of%2520complementary%2520information%2520between%2520the%2520audio%2520and%2520visual%2520modalities.%2520We%2520evaluate%2520our%2520system%2520across%2520four%2520dimensions%253A%2520talking%2520face%2520realism%252C%2520listening%2520head%2520responsiveness%252C%2520dyadic%2520interaction%2520fluency%252C%2520and%2520speech%2520quality.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520across%2520all%2520these%2520aspects.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAVID%3A%20Text-Driven%20Audio-Visual%20Interactive%20Dialogue%20Generation&entry.906535625=Ji-Hoon%20Kim%20and%20Junseok%20Ahn%20and%20Doyeop%20Kwak%20and%20Joon%20Son%20Chung%20and%20Shinji%20Watanabe&entry.1292438233=The%20objective%20of%20this%20paper%20is%20to%20jointly%20synthesize%20interactive%20videos%20and%20conversational%20speech%20from%20text%20and%20reference%20images.%20With%20the%20ultimate%20goal%20of%20building%20human-like%20conversational%20systems%2C%20recent%20studies%20have%20explored%20talking%20or%20listening%20head%20generation%20as%20well%20as%20conversational%20speech%20generation.%20However%2C%20these%20works%20are%20typically%20studied%20in%20isolation%2C%20overlooking%20the%20multimodal%20nature%20of%20human%20conversation%2C%20which%20involves%20tightly%20coupled%20audio-visual%20interactions.%20In%20this%20paper%2C%20we%20introduce%20TAVID%2C%20a%20unified%20framework%20that%20generates%20both%20interactive%20faces%20and%20conversational%20speech%20in%20a%20synchronized%20manner.%20TAVID%20integrates%20face%20and%20speech%20generation%20pipelines%20through%20two%20cross-modal%20mappers%20%28i.e.%2C%20a%20motion%20mapper%20and%20a%20speaker%20mapper%29%2C%20which%20enable%20bidirectional%20exchange%20of%20complementary%20information%20between%20the%20audio%20and%20visual%20modalities.%20We%20evaluate%20our%20system%20across%20four%20dimensions%3A%20talking%20face%20realism%2C%20listening%20head%20responsiveness%2C%20dyadic%20interaction%20fluency%2C%20and%20speech%20quality.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20approach%20across%20all%20these%20aspects.&entry.1838667208=http%3A//arxiv.org/abs/2512.20296v1&entry.124074799=Read"},
{"title": "LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation", "author": "Xiangxuan Ren and Zhongdao Wang and Pin Tang and Guoqing Wang and Jilai Zheng and Chao Ma", "abstract": "3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.", "link": "http://arxiv.org/abs/2512.20217v1", "date": "2025-12-23", "relevancy": 2.2865, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5734}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.572}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiteFusion%3A%20Taming%203D%20Object%20Detectors%20from%20Vision-Based%20to%20Multi-Modal%20with%20Minimal%20Adaptation&body=Title%3A%20LiteFusion%3A%20Taming%203D%20Object%20Detectors%20from%20Vision-Based%20to%20Multi-Modal%20with%20Minimal%20Adaptation%0AAuthor%3A%20Xiangxuan%20Ren%20and%20Zhongdao%20Wang%20and%20Pin%20Tang%20and%20Guoqing%20Wang%20and%20Jilai%20Zheng%20and%20Chao%20Ma%0AAbstract%3A%203D%20object%20detection%20is%20fundamental%20for%20safe%20and%20robust%20intelligent%20transportation%20systems.%20Current%20multi-modal%203D%20object%20detectors%20often%20rely%20on%20complex%20architectures%20and%20training%20strategies%20to%20achieve%20higher%20detection%20accuracy.%20However%2C%20these%20methods%20heavily%20rely%20on%20the%20LiDAR%20sensor%20so%20that%20they%20suffer%20from%20large%20performance%20drops%20when%20LiDAR%20is%20absent%2C%20which%20compromises%20the%20robustness%20and%20safety%20of%20autonomous%20systems%20in%20practical%20scenarios.%20Moreover%2C%20existing%20multi-modal%20detectors%20face%20difficulties%20in%20deployment%20on%20diverse%20hardware%20platforms%2C%20such%20as%20NPUs%20and%20FPGAs%2C%20due%20to%20their%20reliance%20on%203D%20sparse%20convolution%20operators%2C%20which%20are%20primarily%20optimized%20for%20NVIDIA%20GPUs.%20To%20address%20these%20challenges%2C%20we%20reconsider%20the%20role%20of%20LiDAR%20in%20the%20camera-LiDAR%20fusion%20paradigm%20and%20introduce%20a%20novel%20multi-modal%203D%20detector%2C%20LiteFusion.%20Instead%20of%20treating%20LiDAR%20point%20clouds%20as%20an%20independent%20modality%20with%20a%20separate%20feature%20extraction%20backbone%2C%20LiteFusion%20utilizes%20LiDAR%20data%20as%20a%20complementary%20source%20of%20geometric%20information%20to%20enhance%20camera-based%20detection.%20This%20straightforward%20approach%20completely%20eliminates%20the%20reliance%20on%20a%203D%20backbone%2C%20making%20the%20method%20highly%20deployment-friendly.%20Specifically%2C%20LiteFusion%20integrates%20complementary%20features%20from%20LiDAR%20points%20into%20image%20features%20within%20a%20quaternion%20space%2C%20where%20the%20orthogonal%20constraints%20are%20well-preserved%20during%20network%20training.%20This%20helps%20model%20domain-specific%20relations%20across%20modalities%2C%20yielding%20a%20compact%20cross-modal%20embedding.%20Experiments%20on%20the%20nuScenes%20dataset%20show%20that%20LiteFusion%20improves%20the%20baseline%20vision-based%20detector%20by%20%2B20.4%25%20mAP%20and%20%2B19.7%25%20NDS%20with%20a%20minimal%20increase%20in%20parameters%20%281.1%25%29%20without%20using%20dedicated%20LiDAR%20encoders.%20Notably%2C%20even%20in%20the%20absence%20of%20LiDAR%20input%2C%20LiteFusion%20maintains%20strong%20results%20%2C%20highlighting%20its%20favorable%20robustness%20and%20effectiveness%20across%20diverse%20fusion%20paradigms%20and%20deployment%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiteFusion%253A%2520Taming%25203D%2520Object%2520Detectors%2520from%2520Vision-Based%2520to%2520Multi-Modal%2520with%2520Minimal%2520Adaptation%26entry.906535625%3DXiangxuan%2520Ren%2520and%2520Zhongdao%2520Wang%2520and%2520Pin%2520Tang%2520and%2520Guoqing%2520Wang%2520and%2520Jilai%2520Zheng%2520and%2520Chao%2520Ma%26entry.1292438233%3D3D%2520object%2520detection%2520is%2520fundamental%2520for%2520safe%2520and%2520robust%2520intelligent%2520transportation%2520systems.%2520Current%2520multi-modal%25203D%2520object%2520detectors%2520often%2520rely%2520on%2520complex%2520architectures%2520and%2520training%2520strategies%2520to%2520achieve%2520higher%2520detection%2520accuracy.%2520However%252C%2520these%2520methods%2520heavily%2520rely%2520on%2520the%2520LiDAR%2520sensor%2520so%2520that%2520they%2520suffer%2520from%2520large%2520performance%2520drops%2520when%2520LiDAR%2520is%2520absent%252C%2520which%2520compromises%2520the%2520robustness%2520and%2520safety%2520of%2520autonomous%2520systems%2520in%2520practical%2520scenarios.%2520Moreover%252C%2520existing%2520multi-modal%2520detectors%2520face%2520difficulties%2520in%2520deployment%2520on%2520diverse%2520hardware%2520platforms%252C%2520such%2520as%2520NPUs%2520and%2520FPGAs%252C%2520due%2520to%2520their%2520reliance%2520on%25203D%2520sparse%2520convolution%2520operators%252C%2520which%2520are%2520primarily%2520optimized%2520for%2520NVIDIA%2520GPUs.%2520To%2520address%2520these%2520challenges%252C%2520we%2520reconsider%2520the%2520role%2520of%2520LiDAR%2520in%2520the%2520camera-LiDAR%2520fusion%2520paradigm%2520and%2520introduce%2520a%2520novel%2520multi-modal%25203D%2520detector%252C%2520LiteFusion.%2520Instead%2520of%2520treating%2520LiDAR%2520point%2520clouds%2520as%2520an%2520independent%2520modality%2520with%2520a%2520separate%2520feature%2520extraction%2520backbone%252C%2520LiteFusion%2520utilizes%2520LiDAR%2520data%2520as%2520a%2520complementary%2520source%2520of%2520geometric%2520information%2520to%2520enhance%2520camera-based%2520detection.%2520This%2520straightforward%2520approach%2520completely%2520eliminates%2520the%2520reliance%2520on%2520a%25203D%2520backbone%252C%2520making%2520the%2520method%2520highly%2520deployment-friendly.%2520Specifically%252C%2520LiteFusion%2520integrates%2520complementary%2520features%2520from%2520LiDAR%2520points%2520into%2520image%2520features%2520within%2520a%2520quaternion%2520space%252C%2520where%2520the%2520orthogonal%2520constraints%2520are%2520well-preserved%2520during%2520network%2520training.%2520This%2520helps%2520model%2520domain-specific%2520relations%2520across%2520modalities%252C%2520yielding%2520a%2520compact%2520cross-modal%2520embedding.%2520Experiments%2520on%2520the%2520nuScenes%2520dataset%2520show%2520that%2520LiteFusion%2520improves%2520the%2520baseline%2520vision-based%2520detector%2520by%2520%252B20.4%2525%2520mAP%2520and%2520%252B19.7%2525%2520NDS%2520with%2520a%2520minimal%2520increase%2520in%2520parameters%2520%25281.1%2525%2529%2520without%2520using%2520dedicated%2520LiDAR%2520encoders.%2520Notably%252C%2520even%2520in%2520the%2520absence%2520of%2520LiDAR%2520input%252C%2520LiteFusion%2520maintains%2520strong%2520results%2520%252C%2520highlighting%2520its%2520favorable%2520robustness%2520and%2520effectiveness%2520across%2520diverse%2520fusion%2520paradigms%2520and%2520deployment%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiteFusion%3A%20Taming%203D%20Object%20Detectors%20from%20Vision-Based%20to%20Multi-Modal%20with%20Minimal%20Adaptation&entry.906535625=Xiangxuan%20Ren%20and%20Zhongdao%20Wang%20and%20Pin%20Tang%20and%20Guoqing%20Wang%20and%20Jilai%20Zheng%20and%20Chao%20Ma&entry.1292438233=3D%20object%20detection%20is%20fundamental%20for%20safe%20and%20robust%20intelligent%20transportation%20systems.%20Current%20multi-modal%203D%20object%20detectors%20often%20rely%20on%20complex%20architectures%20and%20training%20strategies%20to%20achieve%20higher%20detection%20accuracy.%20However%2C%20these%20methods%20heavily%20rely%20on%20the%20LiDAR%20sensor%20so%20that%20they%20suffer%20from%20large%20performance%20drops%20when%20LiDAR%20is%20absent%2C%20which%20compromises%20the%20robustness%20and%20safety%20of%20autonomous%20systems%20in%20practical%20scenarios.%20Moreover%2C%20existing%20multi-modal%20detectors%20face%20difficulties%20in%20deployment%20on%20diverse%20hardware%20platforms%2C%20such%20as%20NPUs%20and%20FPGAs%2C%20due%20to%20their%20reliance%20on%203D%20sparse%20convolution%20operators%2C%20which%20are%20primarily%20optimized%20for%20NVIDIA%20GPUs.%20To%20address%20these%20challenges%2C%20we%20reconsider%20the%20role%20of%20LiDAR%20in%20the%20camera-LiDAR%20fusion%20paradigm%20and%20introduce%20a%20novel%20multi-modal%203D%20detector%2C%20LiteFusion.%20Instead%20of%20treating%20LiDAR%20point%20clouds%20as%20an%20independent%20modality%20with%20a%20separate%20feature%20extraction%20backbone%2C%20LiteFusion%20utilizes%20LiDAR%20data%20as%20a%20complementary%20source%20of%20geometric%20information%20to%20enhance%20camera-based%20detection.%20This%20straightforward%20approach%20completely%20eliminates%20the%20reliance%20on%20a%203D%20backbone%2C%20making%20the%20method%20highly%20deployment-friendly.%20Specifically%2C%20LiteFusion%20integrates%20complementary%20features%20from%20LiDAR%20points%20into%20image%20features%20within%20a%20quaternion%20space%2C%20where%20the%20orthogonal%20constraints%20are%20well-preserved%20during%20network%20training.%20This%20helps%20model%20domain-specific%20relations%20across%20modalities%2C%20yielding%20a%20compact%20cross-modal%20embedding.%20Experiments%20on%20the%20nuScenes%20dataset%20show%20that%20LiteFusion%20improves%20the%20baseline%20vision-based%20detector%20by%20%2B20.4%25%20mAP%20and%20%2B19.7%25%20NDS%20with%20a%20minimal%20increase%20in%20parameters%20%281.1%25%29%20without%20using%20dedicated%20LiDAR%20encoders.%20Notably%2C%20even%20in%20the%20absence%20of%20LiDAR%20input%2C%20LiteFusion%20maintains%20strong%20results%20%2C%20highlighting%20its%20favorable%20robustness%20and%20effectiveness%20across%20diverse%20fusion%20paradigms%20and%20deployment%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.20217v1&entry.124074799=Read"},
{"title": "Reinforcement Learning for Large Model: A Survey", "author": "Weijia Wu and Chen Gao and Joya Chen and Kevin Qinghong Lin and Qingwei Meng and Yiming Zhang and Yuke Qiu and Hong Zhou and Mike Zheng Shou", "abstract": "Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.", "link": "http://arxiv.org/abs/2508.08189v3", "date": "2025-12-23", "relevancy": 2.2685, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20for%20Large%20Model%3A%20A%20Survey&body=Title%3A%20Reinforcement%20Learning%20for%20Large%20Model%3A%20A%20Survey%0AAuthor%3A%20Weijia%20Wu%20and%20Chen%20Gao%20and%20Joya%20Chen%20and%20Kevin%20Qinghong%20Lin%20and%20Qingwei%20Meng%20and%20Yiming%20Zhang%20and%20Yuke%20Qiu%20and%20Hong%20Zhou%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20Recent%20advances%20at%20the%20intersection%20of%20reinforcement%20learning%20%28RL%29%20and%20visual%20intelligence%20have%20enabled%20agents%20that%20not%20only%20perceive%20complex%20visual%20scenes%20but%20also%20reason%2C%20generate%2C%20and%20act%20within%20them.%20This%20survey%20offers%20a%20critical%20and%20up-to-date%20synthesis%20of%20the%20field.%20We%20first%20formalize%20visual%20RL%20problems%20and%20trace%20the%20evolution%20of%20policy-optimization%20strategies%20from%20RLHF%20to%20verifiable%20reward%20paradigms%2C%20and%20from%20Proximal%20Policy%20Optimization%20to%20Group%20Relative%20Policy%20Optimization.%20We%20then%20organize%20more%20than%20200%20representative%20works%20into%20four%20thematic%20pillars%3A%20multi-modal%20large%20language%20models%2C%20visual%20generation%2C%20unified%20model%20frameworks%2C%20and%20vision-language-action%20models.%20For%20each%20pillar%20we%20examine%20algorithmic%20design%2C%20reward%20engineering%2C%20benchmark%20progress%2C%20and%20we%20distill%20trends%20such%20as%20curriculum-driven%20training%2C%20preference-aligned%20diffusion%2C%20and%20unified%20reward%20modeling.%20Finally%2C%20we%20review%20evaluation%20protocols%20spanning%20set-level%20fidelity%2C%20sample-level%20preference%2C%20and%20state-level%20stability%2C%20and%20we%20identify%20open%20challenges%20that%20include%20sample%20efficiency%2C%20generalization%2C%20and%20safe%20deployment.%20Our%20goal%20is%20to%20provide%20researchers%20and%20practitioners%20with%20a%20coherent%20map%20of%20the%20rapidly%20expanding%20landscape%20of%20visual%20RL%20and%20to%20highlight%20promising%20directions%20for%20future%20inquiry.%20Resources%20are%20available%20at%3A%20https%3A//github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.%0ALink%3A%20http%3A//arxiv.org/abs/2508.08189v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520for%2520Large%2520Model%253A%2520A%2520Survey%26entry.906535625%3DWeijia%2520Wu%2520and%2520Chen%2520Gao%2520and%2520Joya%2520Chen%2520and%2520Kevin%2520Qinghong%2520Lin%2520and%2520Qingwei%2520Meng%2520and%2520Yiming%2520Zhang%2520and%2520Yuke%2520Qiu%2520and%2520Hong%2520Zhou%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3DRecent%2520advances%2520at%2520the%2520intersection%2520of%2520reinforcement%2520learning%2520%2528RL%2529%2520and%2520visual%2520intelligence%2520have%2520enabled%2520agents%2520that%2520not%2520only%2520perceive%2520complex%2520visual%2520scenes%2520but%2520also%2520reason%252C%2520generate%252C%2520and%2520act%2520within%2520them.%2520This%2520survey%2520offers%2520a%2520critical%2520and%2520up-to-date%2520synthesis%2520of%2520the%2520field.%2520We%2520first%2520formalize%2520visual%2520RL%2520problems%2520and%2520trace%2520the%2520evolution%2520of%2520policy-optimization%2520strategies%2520from%2520RLHF%2520to%2520verifiable%2520reward%2520paradigms%252C%2520and%2520from%2520Proximal%2520Policy%2520Optimization%2520to%2520Group%2520Relative%2520Policy%2520Optimization.%2520We%2520then%2520organize%2520more%2520than%2520200%2520representative%2520works%2520into%2520four%2520thematic%2520pillars%253A%2520multi-modal%2520large%2520language%2520models%252C%2520visual%2520generation%252C%2520unified%2520model%2520frameworks%252C%2520and%2520vision-language-action%2520models.%2520For%2520each%2520pillar%2520we%2520examine%2520algorithmic%2520design%252C%2520reward%2520engineering%252C%2520benchmark%2520progress%252C%2520and%2520we%2520distill%2520trends%2520such%2520as%2520curriculum-driven%2520training%252C%2520preference-aligned%2520diffusion%252C%2520and%2520unified%2520reward%2520modeling.%2520Finally%252C%2520we%2520review%2520evaluation%2520protocols%2520spanning%2520set-level%2520fidelity%252C%2520sample-level%2520preference%252C%2520and%2520state-level%2520stability%252C%2520and%2520we%2520identify%2520open%2520challenges%2520that%2520include%2520sample%2520efficiency%252C%2520generalization%252C%2520and%2520safe%2520deployment.%2520Our%2520goal%2520is%2520to%2520provide%2520researchers%2520and%2520practitioners%2520with%2520a%2520coherent%2520map%2520of%2520the%2520rapidly%2520expanding%2520landscape%2520of%2520visual%2520RL%2520and%2520to%2520highlight%2520promising%2520directions%2520for%2520future%2520inquiry.%2520Resources%2520are%2520available%2520at%253A%2520https%253A//github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08189v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20for%20Large%20Model%3A%20A%20Survey&entry.906535625=Weijia%20Wu%20and%20Chen%20Gao%20and%20Joya%20Chen%20and%20Kevin%20Qinghong%20Lin%20and%20Qingwei%20Meng%20and%20Yiming%20Zhang%20and%20Yuke%20Qiu%20and%20Hong%20Zhou%20and%20Mike%20Zheng%20Shou&entry.1292438233=Recent%20advances%20at%20the%20intersection%20of%20reinforcement%20learning%20%28RL%29%20and%20visual%20intelligence%20have%20enabled%20agents%20that%20not%20only%20perceive%20complex%20visual%20scenes%20but%20also%20reason%2C%20generate%2C%20and%20act%20within%20them.%20This%20survey%20offers%20a%20critical%20and%20up-to-date%20synthesis%20of%20the%20field.%20We%20first%20formalize%20visual%20RL%20problems%20and%20trace%20the%20evolution%20of%20policy-optimization%20strategies%20from%20RLHF%20to%20verifiable%20reward%20paradigms%2C%20and%20from%20Proximal%20Policy%20Optimization%20to%20Group%20Relative%20Policy%20Optimization.%20We%20then%20organize%20more%20than%20200%20representative%20works%20into%20four%20thematic%20pillars%3A%20multi-modal%20large%20language%20models%2C%20visual%20generation%2C%20unified%20model%20frameworks%2C%20and%20vision-language-action%20models.%20For%20each%20pillar%20we%20examine%20algorithmic%20design%2C%20reward%20engineering%2C%20benchmark%20progress%2C%20and%20we%20distill%20trends%20such%20as%20curriculum-driven%20training%2C%20preference-aligned%20diffusion%2C%20and%20unified%20reward%20modeling.%20Finally%2C%20we%20review%20evaluation%20protocols%20spanning%20set-level%20fidelity%2C%20sample-level%20preference%2C%20and%20state-level%20stability%2C%20and%20we%20identify%20open%20challenges%20that%20include%20sample%20efficiency%2C%20generalization%2C%20and%20safe%20deployment.%20Our%20goal%20is%20to%20provide%20researchers%20and%20practitioners%20with%20a%20coherent%20map%20of%20the%20rapidly%20expanding%20landscape%20of%20visual%20RL%20and%20to%20highlight%20promising%20directions%20for%20future%20inquiry.%20Resources%20are%20available%20at%3A%20https%3A//github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.&entry.1838667208=http%3A//arxiv.org/abs/2508.08189v3&entry.124074799=Read"},
{"title": "Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning", "author": "Kausthubh Manda and Raghuram Bharadwaj Diddigi", "abstract": "We study offline multitask reinforcement learning in settings where multiple tasks share a low-rank representation of their action-value functions. In this regime, a learner is provided with fixed datasets collected from several related tasks, without access to further online interaction, and seeks to exploit shared structure to improve statistical efficiency and generalization. We analyze a multitask variant of fitted Q-iteration that jointly learns a shared representation and task-specific value functions via Bellman error minimization on offline data. Under standard realizability and coverage assumptions commonly used in offline reinforcement learning, we establish finite-sample generalization guarantees for the learned value functions. Our analysis explicitly characterizes how pooling data across tasks improves estimation accuracy, yielding a $1/\\sqrt{nT}$ dependence on the total number of samples across tasks, while retaining the usual dependence on the horizon and concentrability coefficients arising from distribution shift. In addition, we consider a downstream offline setting in which a new task shares the same underlying representation as the upstream tasks. We study how reusing the representation learned during the multitask phase affects value estimation for this new task, and show that it can reduce the effective complexity of downstream learning relative to learning from scratch. Together, our results clarify the role of shared representations in multitask offline Q-learning and provide theoretical insight into when and how multitask structure can improve generalization in model-free, value-based reinforcement learning.", "link": "http://arxiv.org/abs/2512.20220v1", "date": "2025-12-23", "relevancy": 2.2683, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.459}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4579}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalisation%20in%20Multitask%20Fitted%20Q-Iteration%20and%20Offline%20Q-learning&body=Title%3A%20Generalisation%20in%20Multitask%20Fitted%20Q-Iteration%20and%20Offline%20Q-learning%0AAuthor%3A%20Kausthubh%20Manda%20and%20Raghuram%20Bharadwaj%20Diddigi%0AAbstract%3A%20We%20study%20offline%20multitask%20reinforcement%20learning%20in%20settings%20where%20multiple%20tasks%20share%20a%20low-rank%20representation%20of%20their%20action-value%20functions.%20In%20this%20regime%2C%20a%20learner%20is%20provided%20with%20fixed%20datasets%20collected%20from%20several%20related%20tasks%2C%20without%20access%20to%20further%20online%20interaction%2C%20and%20seeks%20to%20exploit%20shared%20structure%20to%20improve%20statistical%20efficiency%20and%20generalization.%20We%20analyze%20a%20multitask%20variant%20of%20fitted%20Q-iteration%20that%20jointly%20learns%20a%20shared%20representation%20and%20task-specific%20value%20functions%20via%20Bellman%20error%20minimization%20on%20offline%20data.%20Under%20standard%20realizability%20and%20coverage%20assumptions%20commonly%20used%20in%20offline%20reinforcement%20learning%2C%20we%20establish%20finite-sample%20generalization%20guarantees%20for%20the%20learned%20value%20functions.%20Our%20analysis%20explicitly%20characterizes%20how%20pooling%20data%20across%20tasks%20improves%20estimation%20accuracy%2C%20yielding%20a%20%241/%5Csqrt%7BnT%7D%24%20dependence%20on%20the%20total%20number%20of%20samples%20across%20tasks%2C%20while%20retaining%20the%20usual%20dependence%20on%20the%20horizon%20and%20concentrability%20coefficients%20arising%20from%20distribution%20shift.%20In%20addition%2C%20we%20consider%20a%20downstream%20offline%20setting%20in%20which%20a%20new%20task%20shares%20the%20same%20underlying%20representation%20as%20the%20upstream%20tasks.%20We%20study%20how%20reusing%20the%20representation%20learned%20during%20the%20multitask%20phase%20affects%20value%20estimation%20for%20this%20new%20task%2C%20and%20show%20that%20it%20can%20reduce%20the%20effective%20complexity%20of%20downstream%20learning%20relative%20to%20learning%20from%20scratch.%20Together%2C%20our%20results%20clarify%20the%20role%20of%20shared%20representations%20in%20multitask%20offline%20Q-learning%20and%20provide%20theoretical%20insight%20into%20when%20and%20how%20multitask%20structure%20can%20improve%20generalization%20in%20model-free%2C%20value-based%20reinforcement%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralisation%2520in%2520Multitask%2520Fitted%2520Q-Iteration%2520and%2520Offline%2520Q-learning%26entry.906535625%3DKausthubh%2520Manda%2520and%2520Raghuram%2520Bharadwaj%2520Diddigi%26entry.1292438233%3DWe%2520study%2520offline%2520multitask%2520reinforcement%2520learning%2520in%2520settings%2520where%2520multiple%2520tasks%2520share%2520a%2520low-rank%2520representation%2520of%2520their%2520action-value%2520functions.%2520In%2520this%2520regime%252C%2520a%2520learner%2520is%2520provided%2520with%2520fixed%2520datasets%2520collected%2520from%2520several%2520related%2520tasks%252C%2520without%2520access%2520to%2520further%2520online%2520interaction%252C%2520and%2520seeks%2520to%2520exploit%2520shared%2520structure%2520to%2520improve%2520statistical%2520efficiency%2520and%2520generalization.%2520We%2520analyze%2520a%2520multitask%2520variant%2520of%2520fitted%2520Q-iteration%2520that%2520jointly%2520learns%2520a%2520shared%2520representation%2520and%2520task-specific%2520value%2520functions%2520via%2520Bellman%2520error%2520minimization%2520on%2520offline%2520data.%2520Under%2520standard%2520realizability%2520and%2520coverage%2520assumptions%2520commonly%2520used%2520in%2520offline%2520reinforcement%2520learning%252C%2520we%2520establish%2520finite-sample%2520generalization%2520guarantees%2520for%2520the%2520learned%2520value%2520functions.%2520Our%2520analysis%2520explicitly%2520characterizes%2520how%2520pooling%2520data%2520across%2520tasks%2520improves%2520estimation%2520accuracy%252C%2520yielding%2520a%2520%25241/%255Csqrt%257BnT%257D%2524%2520dependence%2520on%2520the%2520total%2520number%2520of%2520samples%2520across%2520tasks%252C%2520while%2520retaining%2520the%2520usual%2520dependence%2520on%2520the%2520horizon%2520and%2520concentrability%2520coefficients%2520arising%2520from%2520distribution%2520shift.%2520In%2520addition%252C%2520we%2520consider%2520a%2520downstream%2520offline%2520setting%2520in%2520which%2520a%2520new%2520task%2520shares%2520the%2520same%2520underlying%2520representation%2520as%2520the%2520upstream%2520tasks.%2520We%2520study%2520how%2520reusing%2520the%2520representation%2520learned%2520during%2520the%2520multitask%2520phase%2520affects%2520value%2520estimation%2520for%2520this%2520new%2520task%252C%2520and%2520show%2520that%2520it%2520can%2520reduce%2520the%2520effective%2520complexity%2520of%2520downstream%2520learning%2520relative%2520to%2520learning%2520from%2520scratch.%2520Together%252C%2520our%2520results%2520clarify%2520the%2520role%2520of%2520shared%2520representations%2520in%2520multitask%2520offline%2520Q-learning%2520and%2520provide%2520theoretical%2520insight%2520into%2520when%2520and%2520how%2520multitask%2520structure%2520can%2520improve%2520generalization%2520in%2520model-free%252C%2520value-based%2520reinforcement%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalisation%20in%20Multitask%20Fitted%20Q-Iteration%20and%20Offline%20Q-learning&entry.906535625=Kausthubh%20Manda%20and%20Raghuram%20Bharadwaj%20Diddigi&entry.1292438233=We%20study%20offline%20multitask%20reinforcement%20learning%20in%20settings%20where%20multiple%20tasks%20share%20a%20low-rank%20representation%20of%20their%20action-value%20functions.%20In%20this%20regime%2C%20a%20learner%20is%20provided%20with%20fixed%20datasets%20collected%20from%20several%20related%20tasks%2C%20without%20access%20to%20further%20online%20interaction%2C%20and%20seeks%20to%20exploit%20shared%20structure%20to%20improve%20statistical%20efficiency%20and%20generalization.%20We%20analyze%20a%20multitask%20variant%20of%20fitted%20Q-iteration%20that%20jointly%20learns%20a%20shared%20representation%20and%20task-specific%20value%20functions%20via%20Bellman%20error%20minimization%20on%20offline%20data.%20Under%20standard%20realizability%20and%20coverage%20assumptions%20commonly%20used%20in%20offline%20reinforcement%20learning%2C%20we%20establish%20finite-sample%20generalization%20guarantees%20for%20the%20learned%20value%20functions.%20Our%20analysis%20explicitly%20characterizes%20how%20pooling%20data%20across%20tasks%20improves%20estimation%20accuracy%2C%20yielding%20a%20%241/%5Csqrt%7BnT%7D%24%20dependence%20on%20the%20total%20number%20of%20samples%20across%20tasks%2C%20while%20retaining%20the%20usual%20dependence%20on%20the%20horizon%20and%20concentrability%20coefficients%20arising%20from%20distribution%20shift.%20In%20addition%2C%20we%20consider%20a%20downstream%20offline%20setting%20in%20which%20a%20new%20task%20shares%20the%20same%20underlying%20representation%20as%20the%20upstream%20tasks.%20We%20study%20how%20reusing%20the%20representation%20learned%20during%20the%20multitask%20phase%20affects%20value%20estimation%20for%20this%20new%20task%2C%20and%20show%20that%20it%20can%20reduce%20the%20effective%20complexity%20of%20downstream%20learning%20relative%20to%20learning%20from%20scratch.%20Together%2C%20our%20results%20clarify%20the%20role%20of%20shared%20representations%20in%20multitask%20offline%20Q-learning%20and%20provide%20theoretical%20insight%20into%20when%20and%20how%20multitask%20structure%20can%20improve%20generalization%20in%20model-free%2C%20value-based%20reinforcement%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2512.20220v1&entry.124074799=Read"},
{"title": "DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning", "author": "Junho Yoon and Jaemo Jung and Hyunju Kim and Dongman Lee", "abstract": "Aligning egocentric video with wearable sensors have shown promise for human action recognition, but face practical limitations in user discomfort, privacy concerns, and scalability. We explore exocentric video with ambient sensors as a non-intrusive, scalable alternative. While prior egocentric-wearable works predominantly adopt Global Alignment by encoding entire sequences into unified representations, this approach fails in exocentric-ambient settings due to two problems: (P1) inability to capture local details such as subtle motions, and (P2) over-reliance on modality-invariant temporal patterns, causing misalignment between actions sharing similar temporal patterns with different spatio-semantic contexts. To resolve these problems, we propose DETACH, a decomposed spatio-temporal framework. This explicit decomposition preserves local details, while our novel sensor-spatial features discovered via online clustering provide semantic grounding for context-aware alignment. To align the decomposed features, our two-stage approach establishes spatial correspondence through mutual supervision, then performs temporal alignment via a spatial-temporal weighted contrastive loss that adaptively handles easy negatives, hard negatives, and false negatives. Comprehensive experiments with downstream tasks on Opportunity++ and HWU-USP datasets demonstrate substantial improvements over adapted egocentric-wearable baselines.", "link": "http://arxiv.org/abs/2512.20409v1", "date": "2025-12-23", "relevancy": 2.2586, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5674}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5664}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DETACH%20%3A%20Decomposed%20Spatio-Temporal%20Alignment%20for%20Exocentric%20Video%20and%20Ambient%20Sensors%20with%20Staged%20Learning&body=Title%3A%20DETACH%20%3A%20Decomposed%20Spatio-Temporal%20Alignment%20for%20Exocentric%20Video%20and%20Ambient%20Sensors%20with%20Staged%20Learning%0AAuthor%3A%20Junho%20Yoon%20and%20Jaemo%20Jung%20and%20Hyunju%20Kim%20and%20Dongman%20Lee%0AAbstract%3A%20Aligning%20egocentric%20video%20with%20wearable%20sensors%20have%20shown%20promise%20for%20human%20action%20recognition%2C%20but%20face%20practical%20limitations%20in%20user%20discomfort%2C%20privacy%20concerns%2C%20and%20scalability.%20We%20explore%20exocentric%20video%20with%20ambient%20sensors%20as%20a%20non-intrusive%2C%20scalable%20alternative.%20While%20prior%20egocentric-wearable%20works%20predominantly%20adopt%20Global%20Alignment%20by%20encoding%20entire%20sequences%20into%20unified%20representations%2C%20this%20approach%20fails%20in%20exocentric-ambient%20settings%20due%20to%20two%20problems%3A%20%28P1%29%20inability%20to%20capture%20local%20details%20such%20as%20subtle%20motions%2C%20and%20%28P2%29%20over-reliance%20on%20modality-invariant%20temporal%20patterns%2C%20causing%20misalignment%20between%20actions%20sharing%20similar%20temporal%20patterns%20with%20different%20spatio-semantic%20contexts.%20To%20resolve%20these%20problems%2C%20we%20propose%20DETACH%2C%20a%20decomposed%20spatio-temporal%20framework.%20This%20explicit%20decomposition%20preserves%20local%20details%2C%20while%20our%20novel%20sensor-spatial%20features%20discovered%20via%20online%20clustering%20provide%20semantic%20grounding%20for%20context-aware%20alignment.%20To%20align%20the%20decomposed%20features%2C%20our%20two-stage%20approach%20establishes%20spatial%20correspondence%20through%20mutual%20supervision%2C%20then%20performs%20temporal%20alignment%20via%20a%20spatial-temporal%20weighted%20contrastive%20loss%20that%20adaptively%20handles%20easy%20negatives%2C%20hard%20negatives%2C%20and%20false%20negatives.%20Comprehensive%20experiments%20with%20downstream%20tasks%20on%20Opportunity%2B%2B%20and%20HWU-USP%20datasets%20demonstrate%20substantial%20improvements%20over%20adapted%20egocentric-wearable%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDETACH%2520%253A%2520Decomposed%2520Spatio-Temporal%2520Alignment%2520for%2520Exocentric%2520Video%2520and%2520Ambient%2520Sensors%2520with%2520Staged%2520Learning%26entry.906535625%3DJunho%2520Yoon%2520and%2520Jaemo%2520Jung%2520and%2520Hyunju%2520Kim%2520and%2520Dongman%2520Lee%26entry.1292438233%3DAligning%2520egocentric%2520video%2520with%2520wearable%2520sensors%2520have%2520shown%2520promise%2520for%2520human%2520action%2520recognition%252C%2520but%2520face%2520practical%2520limitations%2520in%2520user%2520discomfort%252C%2520privacy%2520concerns%252C%2520and%2520scalability.%2520We%2520explore%2520exocentric%2520video%2520with%2520ambient%2520sensors%2520as%2520a%2520non-intrusive%252C%2520scalable%2520alternative.%2520While%2520prior%2520egocentric-wearable%2520works%2520predominantly%2520adopt%2520Global%2520Alignment%2520by%2520encoding%2520entire%2520sequences%2520into%2520unified%2520representations%252C%2520this%2520approach%2520fails%2520in%2520exocentric-ambient%2520settings%2520due%2520to%2520two%2520problems%253A%2520%2528P1%2529%2520inability%2520to%2520capture%2520local%2520details%2520such%2520as%2520subtle%2520motions%252C%2520and%2520%2528P2%2529%2520over-reliance%2520on%2520modality-invariant%2520temporal%2520patterns%252C%2520causing%2520misalignment%2520between%2520actions%2520sharing%2520similar%2520temporal%2520patterns%2520with%2520different%2520spatio-semantic%2520contexts.%2520To%2520resolve%2520these%2520problems%252C%2520we%2520propose%2520DETACH%252C%2520a%2520decomposed%2520spatio-temporal%2520framework.%2520This%2520explicit%2520decomposition%2520preserves%2520local%2520details%252C%2520while%2520our%2520novel%2520sensor-spatial%2520features%2520discovered%2520via%2520online%2520clustering%2520provide%2520semantic%2520grounding%2520for%2520context-aware%2520alignment.%2520To%2520align%2520the%2520decomposed%2520features%252C%2520our%2520two-stage%2520approach%2520establishes%2520spatial%2520correspondence%2520through%2520mutual%2520supervision%252C%2520then%2520performs%2520temporal%2520alignment%2520via%2520a%2520spatial-temporal%2520weighted%2520contrastive%2520loss%2520that%2520adaptively%2520handles%2520easy%2520negatives%252C%2520hard%2520negatives%252C%2520and%2520false%2520negatives.%2520Comprehensive%2520experiments%2520with%2520downstream%2520tasks%2520on%2520Opportunity%252B%252B%2520and%2520HWU-USP%2520datasets%2520demonstrate%2520substantial%2520improvements%2520over%2520adapted%2520egocentric-wearable%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DETACH%20%3A%20Decomposed%20Spatio-Temporal%20Alignment%20for%20Exocentric%20Video%20and%20Ambient%20Sensors%20with%20Staged%20Learning&entry.906535625=Junho%20Yoon%20and%20Jaemo%20Jung%20and%20Hyunju%20Kim%20and%20Dongman%20Lee&entry.1292438233=Aligning%20egocentric%20video%20with%20wearable%20sensors%20have%20shown%20promise%20for%20human%20action%20recognition%2C%20but%20face%20practical%20limitations%20in%20user%20discomfort%2C%20privacy%20concerns%2C%20and%20scalability.%20We%20explore%20exocentric%20video%20with%20ambient%20sensors%20as%20a%20non-intrusive%2C%20scalable%20alternative.%20While%20prior%20egocentric-wearable%20works%20predominantly%20adopt%20Global%20Alignment%20by%20encoding%20entire%20sequences%20into%20unified%20representations%2C%20this%20approach%20fails%20in%20exocentric-ambient%20settings%20due%20to%20two%20problems%3A%20%28P1%29%20inability%20to%20capture%20local%20details%20such%20as%20subtle%20motions%2C%20and%20%28P2%29%20over-reliance%20on%20modality-invariant%20temporal%20patterns%2C%20causing%20misalignment%20between%20actions%20sharing%20similar%20temporal%20patterns%20with%20different%20spatio-semantic%20contexts.%20To%20resolve%20these%20problems%2C%20we%20propose%20DETACH%2C%20a%20decomposed%20spatio-temporal%20framework.%20This%20explicit%20decomposition%20preserves%20local%20details%2C%20while%20our%20novel%20sensor-spatial%20features%20discovered%20via%20online%20clustering%20provide%20semantic%20grounding%20for%20context-aware%20alignment.%20To%20align%20the%20decomposed%20features%2C%20our%20two-stage%20approach%20establishes%20spatial%20correspondence%20through%20mutual%20supervision%2C%20then%20performs%20temporal%20alignment%20via%20a%20spatial-temporal%20weighted%20contrastive%20loss%20that%20adaptively%20handles%20easy%20negatives%2C%20hard%20negatives%2C%20and%20false%20negatives.%20Comprehensive%20experiments%20with%20downstream%20tasks%20on%20Opportunity%2B%2B%20and%20HWU-USP%20datasets%20demonstrate%20substantial%20improvements%20over%20adapted%20egocentric-wearable%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2512.20409v1&entry.124074799=Read"},
{"title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "author": "Runtao Liu and Ziyi Liu and Jiaqi Tang and Yue Ma and Renjie Pi and Jipeng Zhang and Qifeng Chen", "abstract": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "link": "http://arxiv.org/abs/2512.20618v1", "date": "2025-12-23", "relevancy": 2.232, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongVideoAgent%3A%20Multi-Agent%20Reasoning%20with%20Long%20Videos&body=Title%3A%20LongVideoAgent%3A%20Multi-Agent%20Reasoning%20with%20Long%20Videos%0AAuthor%3A%20Runtao%20Liu%20and%20Ziyi%20Liu%20and%20Jiaqi%20Tang%20and%20Yue%20Ma%20and%20Renjie%20Pi%20and%20Jipeng%20Zhang%20and%20Qifeng%20Chen%0AAbstract%3A%20Recent%20advances%20in%20multimodal%20LLMs%20and%20systems%20that%20use%20tools%20for%20long-video%20QA%20point%20to%20the%20promise%20of%20reasoning%20over%20hour-long%20episodes.%20However%2C%20many%20methods%20still%20compress%20content%20into%20lossy%20summaries%20or%20rely%20on%20limited%20toolsets%2C%20weakening%20temporal%20grounding%20and%20missing%20fine-grained%20cues.%20We%20propose%20a%20multi-agent%20framework%20in%20which%20a%20master%20LLM%20coordinates%20a%20grounding%20agent%20to%20localize%20question-relevant%20segments%20and%20a%20vision%20agent%20to%20extract%20targeted%20textual%20observations.%20The%20master%20agent%20plans%20with%20a%20step%20limit%2C%20and%20is%20trained%20with%20reinforcement%20learning%20to%20encourage%20concise%2C%20correct%2C%20and%20efficient%20multi-agent%20cooperation.%20This%20design%20helps%20the%20master%20agent%20focus%20on%20relevant%20clips%20via%20grounding%2C%20complements%20subtitles%20with%20visual%20detail%2C%20and%20yields%20interpretable%20trajectories.%20On%20our%20proposed%20LongTVQA%20and%20LongTVQA%2B%20which%20are%20episode-level%20datasets%20aggregated%20from%20TVQA/TVQA%2B%2C%20our%20multi-agent%20system%20significantly%20outperforms%20strong%20non-agent%20baselines.%20Experiments%20also%20show%20reinforcement%20learning%20further%20strengthens%20reasoning%20and%20planning%20for%20the%20trained%20agent.%20Code%20and%20data%20will%20be%20shared%20at%20https%3A//longvideoagent.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongVideoAgent%253A%2520Multi-Agent%2520Reasoning%2520with%2520Long%2520Videos%26entry.906535625%3DRuntao%2520Liu%2520and%2520Ziyi%2520Liu%2520and%2520Jiaqi%2520Tang%2520and%2520Yue%2520Ma%2520and%2520Renjie%2520Pi%2520and%2520Jipeng%2520Zhang%2520and%2520Qifeng%2520Chen%26entry.1292438233%3DRecent%2520advances%2520in%2520multimodal%2520LLMs%2520and%2520systems%2520that%2520use%2520tools%2520for%2520long-video%2520QA%2520point%2520to%2520the%2520promise%2520of%2520reasoning%2520over%2520hour-long%2520episodes.%2520However%252C%2520many%2520methods%2520still%2520compress%2520content%2520into%2520lossy%2520summaries%2520or%2520rely%2520on%2520limited%2520toolsets%252C%2520weakening%2520temporal%2520grounding%2520and%2520missing%2520fine-grained%2520cues.%2520We%2520propose%2520a%2520multi-agent%2520framework%2520in%2520which%2520a%2520master%2520LLM%2520coordinates%2520a%2520grounding%2520agent%2520to%2520localize%2520question-relevant%2520segments%2520and%2520a%2520vision%2520agent%2520to%2520extract%2520targeted%2520textual%2520observations.%2520The%2520master%2520agent%2520plans%2520with%2520a%2520step%2520limit%252C%2520and%2520is%2520trained%2520with%2520reinforcement%2520learning%2520to%2520encourage%2520concise%252C%2520correct%252C%2520and%2520efficient%2520multi-agent%2520cooperation.%2520This%2520design%2520helps%2520the%2520master%2520agent%2520focus%2520on%2520relevant%2520clips%2520via%2520grounding%252C%2520complements%2520subtitles%2520with%2520visual%2520detail%252C%2520and%2520yields%2520interpretable%2520trajectories.%2520On%2520our%2520proposed%2520LongTVQA%2520and%2520LongTVQA%252B%2520which%2520are%2520episode-level%2520datasets%2520aggregated%2520from%2520TVQA/TVQA%252B%252C%2520our%2520multi-agent%2520system%2520significantly%2520outperforms%2520strong%2520non-agent%2520baselines.%2520Experiments%2520also%2520show%2520reinforcement%2520learning%2520further%2520strengthens%2520reasoning%2520and%2520planning%2520for%2520the%2520trained%2520agent.%2520Code%2520and%2520data%2520will%2520be%2520shared%2520at%2520https%253A//longvideoagent.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongVideoAgent%3A%20Multi-Agent%20Reasoning%20with%20Long%20Videos&entry.906535625=Runtao%20Liu%20and%20Ziyi%20Liu%20and%20Jiaqi%20Tang%20and%20Yue%20Ma%20and%20Renjie%20Pi%20and%20Jipeng%20Zhang%20and%20Qifeng%20Chen&entry.1292438233=Recent%20advances%20in%20multimodal%20LLMs%20and%20systems%20that%20use%20tools%20for%20long-video%20QA%20point%20to%20the%20promise%20of%20reasoning%20over%20hour-long%20episodes.%20However%2C%20many%20methods%20still%20compress%20content%20into%20lossy%20summaries%20or%20rely%20on%20limited%20toolsets%2C%20weakening%20temporal%20grounding%20and%20missing%20fine-grained%20cues.%20We%20propose%20a%20multi-agent%20framework%20in%20which%20a%20master%20LLM%20coordinates%20a%20grounding%20agent%20to%20localize%20question-relevant%20segments%20and%20a%20vision%20agent%20to%20extract%20targeted%20textual%20observations.%20The%20master%20agent%20plans%20with%20a%20step%20limit%2C%20and%20is%20trained%20with%20reinforcement%20learning%20to%20encourage%20concise%2C%20correct%2C%20and%20efficient%20multi-agent%20cooperation.%20This%20design%20helps%20the%20master%20agent%20focus%20on%20relevant%20clips%20via%20grounding%2C%20complements%20subtitles%20with%20visual%20detail%2C%20and%20yields%20interpretable%20trajectories.%20On%20our%20proposed%20LongTVQA%20and%20LongTVQA%2B%20which%20are%20episode-level%20datasets%20aggregated%20from%20TVQA/TVQA%2B%2C%20our%20multi-agent%20system%20significantly%20outperforms%20strong%20non-agent%20baselines.%20Experiments%20also%20show%20reinforcement%20learning%20further%20strengthens%20reasoning%20and%20planning%20for%20the%20trained%20agent.%20Code%20and%20data%20will%20be%20shared%20at%20https%3A//longvideoagent.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2512.20618v1&entry.124074799=Read"},
{"title": "${D}^{3}${ETOR}: ${D}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive ${D}$ebiasing for Weakly-Supervised Camouflaged Object ${D}$etection with Scribble Annotations", "author": "Jiawei Ge and Jiuxin Cao and Xinyi Li and Xuelin Zhu and Chang Liu and Bo Liu and Chen Feng and Ioannis Patras", "abstract": "Weakly-Supervised Camouflaged Object Detection (WSCOD) aims to locate and segment objects that are visually concealed within their surrounding scenes, relying solely on sparse supervision such as scribble annotations. Despite recent progress, existing WSCOD methods still lag far behind fully supervised ones due to two major limitations: (1) the pseudo masks generated by general-purpose segmentation models (e.g., SAM) and filtered via rules are often unreliable, as these models lack the task-specific semantic understanding required for effective pseudo labeling in COD; and (2) the neglect of inherent annotation bias in scribbles, which hinders the model from capturing the global structure of camouflaged objects. To overcome these challenges, we propose ${D}^{3}$ETOR, a two-stage WSCOD framework consisting of Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing. In the first stage, we introduce an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to enhance the capability of SAM for COD, improving the interpretability and precision of pseudo masks. In the second stage, we design FADeNet, which progressively fuses multi-level frequency-aware features to balance global semantic understanding with local detail modeling, while dynamically reweighting supervision strength across regions to alleviate scribble bias. By jointly exploiting the supervision signals from both the pseudo masks and scribble semantics, ${D}^{3}$ETOR significantly narrows the gap between weakly and fully supervised COD, achieving state-of-the-art performance on multiple benchmarks.", "link": "http://arxiv.org/abs/2512.20260v1", "date": "2025-12-23", "relevancy": 2.2295, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%7BD%7D%5E%7B3%7D%24%7BETOR%7D%3A%20%24%7BD%7D%24ebate-Enhanced%20Pseudo%20Labeling%20and%20Frequency-Aware%20Progressive%20%24%7BD%7D%24ebiasing%20for%20Weakly-Supervised%20Camouflaged%20Object%20%24%7BD%7D%24etection%20with%20Scribble%20Annotations&body=Title%3A%20%24%7BD%7D%5E%7B3%7D%24%7BETOR%7D%3A%20%24%7BD%7D%24ebate-Enhanced%20Pseudo%20Labeling%20and%20Frequency-Aware%20Progressive%20%24%7BD%7D%24ebiasing%20for%20Weakly-Supervised%20Camouflaged%20Object%20%24%7BD%7D%24etection%20with%20Scribble%20Annotations%0AAuthor%3A%20Jiawei%20Ge%20and%20Jiuxin%20Cao%20and%20Xinyi%20Li%20and%20Xuelin%20Zhu%20and%20Chang%20Liu%20and%20Bo%20Liu%20and%20Chen%20Feng%20and%20Ioannis%20Patras%0AAbstract%3A%20Weakly-Supervised%20Camouflaged%20Object%20Detection%20%28WSCOD%29%20aims%20to%20locate%20and%20segment%20objects%20that%20are%20visually%20concealed%20within%20their%20surrounding%20scenes%2C%20relying%20solely%20on%20sparse%20supervision%20such%20as%20scribble%20annotations.%20Despite%20recent%20progress%2C%20existing%20WSCOD%20methods%20still%20lag%20far%20behind%20fully%20supervised%20ones%20due%20to%20two%20major%20limitations%3A%20%281%29%20the%20pseudo%20masks%20generated%20by%20general-purpose%20segmentation%20models%20%28e.g.%2C%20SAM%29%20and%20filtered%20via%20rules%20are%20often%20unreliable%2C%20as%20these%20models%20lack%20the%20task-specific%20semantic%20understanding%20required%20for%20effective%20pseudo%20labeling%20in%20COD%3B%20and%20%282%29%20the%20neglect%20of%20inherent%20annotation%20bias%20in%20scribbles%2C%20which%20hinders%20the%20model%20from%20capturing%20the%20global%20structure%20of%20camouflaged%20objects.%20To%20overcome%20these%20challenges%2C%20we%20propose%20%24%7BD%7D%5E%7B3%7D%24ETOR%2C%20a%20two-stage%20WSCOD%20framework%20consisting%20of%20Debate-Enhanced%20Pseudo%20Labeling%20and%20Frequency-Aware%20Progressive%20Debiasing.%20In%20the%20first%20stage%2C%20we%20introduce%20an%20adaptive%20entropy-driven%20point%20sampling%20method%20and%20a%20multi-agent%20debate%20mechanism%20to%20enhance%20the%20capability%20of%20SAM%20for%20COD%2C%20improving%20the%20interpretability%20and%20precision%20of%20pseudo%20masks.%20In%20the%20second%20stage%2C%20we%20design%20FADeNet%2C%20which%20progressively%20fuses%20multi-level%20frequency-aware%20features%20to%20balance%20global%20semantic%20understanding%20with%20local%20detail%20modeling%2C%20while%20dynamically%20reweighting%20supervision%20strength%20across%20regions%20to%20alleviate%20scribble%20bias.%20By%20jointly%20exploiting%20the%20supervision%20signals%20from%20both%20the%20pseudo%20masks%20and%20scribble%20semantics%2C%20%24%7BD%7D%5E%7B3%7D%24ETOR%20significantly%20narrows%20the%20gap%20between%20weakly%20and%20fully%20supervised%20COD%2C%20achieving%20state-of-the-art%20performance%20on%20multiple%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%257BD%257D%255E%257B3%257D%2524%257BETOR%257D%253A%2520%2524%257BD%257D%2524ebate-Enhanced%2520Pseudo%2520Labeling%2520and%2520Frequency-Aware%2520Progressive%2520%2524%257BD%257D%2524ebiasing%2520for%2520Weakly-Supervised%2520Camouflaged%2520Object%2520%2524%257BD%257D%2524etection%2520with%2520Scribble%2520Annotations%26entry.906535625%3DJiawei%2520Ge%2520and%2520Jiuxin%2520Cao%2520and%2520Xinyi%2520Li%2520and%2520Xuelin%2520Zhu%2520and%2520Chang%2520Liu%2520and%2520Bo%2520Liu%2520and%2520Chen%2520Feng%2520and%2520Ioannis%2520Patras%26entry.1292438233%3DWeakly-Supervised%2520Camouflaged%2520Object%2520Detection%2520%2528WSCOD%2529%2520aims%2520to%2520locate%2520and%2520segment%2520objects%2520that%2520are%2520visually%2520concealed%2520within%2520their%2520surrounding%2520scenes%252C%2520relying%2520solely%2520on%2520sparse%2520supervision%2520such%2520as%2520scribble%2520annotations.%2520Despite%2520recent%2520progress%252C%2520existing%2520WSCOD%2520methods%2520still%2520lag%2520far%2520behind%2520fully%2520supervised%2520ones%2520due%2520to%2520two%2520major%2520limitations%253A%2520%25281%2529%2520the%2520pseudo%2520masks%2520generated%2520by%2520general-purpose%2520segmentation%2520models%2520%2528e.g.%252C%2520SAM%2529%2520and%2520filtered%2520via%2520rules%2520are%2520often%2520unreliable%252C%2520as%2520these%2520models%2520lack%2520the%2520task-specific%2520semantic%2520understanding%2520required%2520for%2520effective%2520pseudo%2520labeling%2520in%2520COD%253B%2520and%2520%25282%2529%2520the%2520neglect%2520of%2520inherent%2520annotation%2520bias%2520in%2520scribbles%252C%2520which%2520hinders%2520the%2520model%2520from%2520capturing%2520the%2520global%2520structure%2520of%2520camouflaged%2520objects.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520%2524%257BD%257D%255E%257B3%257D%2524ETOR%252C%2520a%2520two-stage%2520WSCOD%2520framework%2520consisting%2520of%2520Debate-Enhanced%2520Pseudo%2520Labeling%2520and%2520Frequency-Aware%2520Progressive%2520Debiasing.%2520In%2520the%2520first%2520stage%252C%2520we%2520introduce%2520an%2520adaptive%2520entropy-driven%2520point%2520sampling%2520method%2520and%2520a%2520multi-agent%2520debate%2520mechanism%2520to%2520enhance%2520the%2520capability%2520of%2520SAM%2520for%2520COD%252C%2520improving%2520the%2520interpretability%2520and%2520precision%2520of%2520pseudo%2520masks.%2520In%2520the%2520second%2520stage%252C%2520we%2520design%2520FADeNet%252C%2520which%2520progressively%2520fuses%2520multi-level%2520frequency-aware%2520features%2520to%2520balance%2520global%2520semantic%2520understanding%2520with%2520local%2520detail%2520modeling%252C%2520while%2520dynamically%2520reweighting%2520supervision%2520strength%2520across%2520regions%2520to%2520alleviate%2520scribble%2520bias.%2520By%2520jointly%2520exploiting%2520the%2520supervision%2520signals%2520from%2520both%2520the%2520pseudo%2520masks%2520and%2520scribble%2520semantics%252C%2520%2524%257BD%257D%255E%257B3%257D%2524ETOR%2520significantly%2520narrows%2520the%2520gap%2520between%2520weakly%2520and%2520fully%2520supervised%2520COD%252C%2520achieving%2520state-of-the-art%2520performance%2520on%2520multiple%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%7BD%7D%5E%7B3%7D%24%7BETOR%7D%3A%20%24%7BD%7D%24ebate-Enhanced%20Pseudo%20Labeling%20and%20Frequency-Aware%20Progressive%20%24%7BD%7D%24ebiasing%20for%20Weakly-Supervised%20Camouflaged%20Object%20%24%7BD%7D%24etection%20with%20Scribble%20Annotations&entry.906535625=Jiawei%20Ge%20and%20Jiuxin%20Cao%20and%20Xinyi%20Li%20and%20Xuelin%20Zhu%20and%20Chang%20Liu%20and%20Bo%20Liu%20and%20Chen%20Feng%20and%20Ioannis%20Patras&entry.1292438233=Weakly-Supervised%20Camouflaged%20Object%20Detection%20%28WSCOD%29%20aims%20to%20locate%20and%20segment%20objects%20that%20are%20visually%20concealed%20within%20their%20surrounding%20scenes%2C%20relying%20solely%20on%20sparse%20supervision%20such%20as%20scribble%20annotations.%20Despite%20recent%20progress%2C%20existing%20WSCOD%20methods%20still%20lag%20far%20behind%20fully%20supervised%20ones%20due%20to%20two%20major%20limitations%3A%20%281%29%20the%20pseudo%20masks%20generated%20by%20general-purpose%20segmentation%20models%20%28e.g.%2C%20SAM%29%20and%20filtered%20via%20rules%20are%20often%20unreliable%2C%20as%20these%20models%20lack%20the%20task-specific%20semantic%20understanding%20required%20for%20effective%20pseudo%20labeling%20in%20COD%3B%20and%20%282%29%20the%20neglect%20of%20inherent%20annotation%20bias%20in%20scribbles%2C%20which%20hinders%20the%20model%20from%20capturing%20the%20global%20structure%20of%20camouflaged%20objects.%20To%20overcome%20these%20challenges%2C%20we%20propose%20%24%7BD%7D%5E%7B3%7D%24ETOR%2C%20a%20two-stage%20WSCOD%20framework%20consisting%20of%20Debate-Enhanced%20Pseudo%20Labeling%20and%20Frequency-Aware%20Progressive%20Debiasing.%20In%20the%20first%20stage%2C%20we%20introduce%20an%20adaptive%20entropy-driven%20point%20sampling%20method%20and%20a%20multi-agent%20debate%20mechanism%20to%20enhance%20the%20capability%20of%20SAM%20for%20COD%2C%20improving%20the%20interpretability%20and%20precision%20of%20pseudo%20masks.%20In%20the%20second%20stage%2C%20we%20design%20FADeNet%2C%20which%20progressively%20fuses%20multi-level%20frequency-aware%20features%20to%20balance%20global%20semantic%20understanding%20with%20local%20detail%20modeling%2C%20while%20dynamically%20reweighting%20supervision%20strength%20across%20regions%20to%20alleviate%20scribble%20bias.%20By%20jointly%20exploiting%20the%20supervision%20signals%20from%20both%20the%20pseudo%20masks%20and%20scribble%20semantics%2C%20%24%7BD%7D%5E%7B3%7D%24ETOR%20significantly%20narrows%20the%20gap%20between%20weakly%20and%20fully%20supervised%20COD%2C%20achieving%20state-of-the-art%20performance%20on%20multiple%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.20260v1&entry.124074799=Read"},
{"title": "TongSIM: A General Platform for Simulating Intelligent Machines", "author": "Zhe Sun and Kunlun Wu and Chuanjian Fu and Zeming Song and Langyong Shi and Zihe Xue and Bohan Jing and Ying Yang and Xiaomeng Gao and Aijia Li and Tianyu Guo and Huiying Li and Xueyuan Yang and Rongkai Liu and Xinyi He and Yuxi Wang and Yue Li and Mingyuan Liu and Yujie Lu and Hongzhao Xie and Shiyun Zhao and Bo Dai and Wei Wang and Tao Yuan and Song-Chun Zhu and Yujia Peng and Zhenliang Zhang", "abstract": "As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.", "link": "http://arxiv.org/abs/2512.20206v1", "date": "2025-12-23", "relevancy": 2.2273, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5618}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5555}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TongSIM%3A%20A%20General%20Platform%20for%20Simulating%20Intelligent%20Machines&body=Title%3A%20TongSIM%3A%20A%20General%20Platform%20for%20Simulating%20Intelligent%20Machines%0AAuthor%3A%20Zhe%20Sun%20and%20Kunlun%20Wu%20and%20Chuanjian%20Fu%20and%20Zeming%20Song%20and%20Langyong%20Shi%20and%20Zihe%20Xue%20and%20Bohan%20Jing%20and%20Ying%20Yang%20and%20Xiaomeng%20Gao%20and%20Aijia%20Li%20and%20Tianyu%20Guo%20and%20Huiying%20Li%20and%20Xueyuan%20Yang%20and%20Rongkai%20Liu%20and%20Xinyi%20He%20and%20Yuxi%20Wang%20and%20Yue%20Li%20and%20Mingyuan%20Liu%20and%20Yujie%20Lu%20and%20Hongzhao%20Xie%20and%20Shiyun%20Zhao%20and%20Bo%20Dai%20and%20Wei%20Wang%20and%20Tao%20Yuan%20and%20Song-Chun%20Zhu%20and%20Yujia%20Peng%20and%20Zhenliang%20Zhang%0AAbstract%3A%20As%20artificial%20intelligence%20%28AI%29%20rapidly%20advances%2C%20especially%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20research%20focus%20is%20shifting%20from%20single-modality%20text%20processing%20to%20the%20more%20complex%20domains%20of%20multimodal%20and%20embodied%20AI.%20Embodied%20intelligence%20focuses%20on%20training%20agents%20within%20realistic%20simulated%20environments%2C%20leveraging%20physical%20interaction%20and%20action%20feedback%20rather%20than%20conventionally%20labeled%20datasets.%20Yet%2C%20most%20existing%20simulation%20platforms%20remain%20narrowly%20designed%2C%20each%20tailored%20to%20specific%20tasks.%20A%20versatile%2C%20general-purpose%20training%20environment%20that%20can%20support%20everything%20from%20low-level%20embodied%20navigation%20to%20high-level%20composite%20activities%2C%20such%20as%20multi-agent%20social%20simulation%20and%20human-AI%20collaboration%2C%20remains%20largely%20unavailable.%20To%20bridge%20this%20gap%2C%20we%20introduce%20TongSIM%2C%20a%20high-fidelity%2C%20general-purpose%20platform%20for%20training%20and%20evaluating%20embodied%20agents.%20TongSIM%20offers%20practical%20advantages%20by%20providing%20over%20100%20diverse%2C%20multi-room%20indoor%20scenarios%20as%20well%20as%20an%20open-ended%2C%20interaction-rich%20outdoor%20town%20simulation%2C%20ensuring%20broad%20applicability%20across%20research%20needs.%20Its%20comprehensive%20evaluation%20framework%20and%20benchmarks%20enable%20precise%20assessment%20of%20agent%20capabilities%2C%20such%20as%20perception%2C%20cognition%2C%20decision-making%2C%20human-robot%20cooperation%2C%20and%20spatial%20and%20social%20reasoning.%20With%20features%20like%20customized%20scenes%2C%20task-adaptive%20fidelity%2C%20diverse%20agent%20types%2C%20and%20dynamic%20environmental%20simulation%2C%20TongSIM%20delivers%20flexibility%20and%20scalability%20for%20researchers%2C%20serving%20as%20a%20unified%20platform%20that%20accelerates%20training%2C%20evaluation%2C%20and%20advancement%20toward%20general%20embodied%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTongSIM%253A%2520A%2520General%2520Platform%2520for%2520Simulating%2520Intelligent%2520Machines%26entry.906535625%3DZhe%2520Sun%2520and%2520Kunlun%2520Wu%2520and%2520Chuanjian%2520Fu%2520and%2520Zeming%2520Song%2520and%2520Langyong%2520Shi%2520and%2520Zihe%2520Xue%2520and%2520Bohan%2520Jing%2520and%2520Ying%2520Yang%2520and%2520Xiaomeng%2520Gao%2520and%2520Aijia%2520Li%2520and%2520Tianyu%2520Guo%2520and%2520Huiying%2520Li%2520and%2520Xueyuan%2520Yang%2520and%2520Rongkai%2520Liu%2520and%2520Xinyi%2520He%2520and%2520Yuxi%2520Wang%2520and%2520Yue%2520Li%2520and%2520Mingyuan%2520Liu%2520and%2520Yujie%2520Lu%2520and%2520Hongzhao%2520Xie%2520and%2520Shiyun%2520Zhao%2520and%2520Bo%2520Dai%2520and%2520Wei%2520Wang%2520and%2520Tao%2520Yuan%2520and%2520Song-Chun%2520Zhu%2520and%2520Yujia%2520Peng%2520and%2520Zhenliang%2520Zhang%26entry.1292438233%3DAs%2520artificial%2520intelligence%2520%2528AI%2529%2520rapidly%2520advances%252C%2520especially%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520research%2520focus%2520is%2520shifting%2520from%2520single-modality%2520text%2520processing%2520to%2520the%2520more%2520complex%2520domains%2520of%2520multimodal%2520and%2520embodied%2520AI.%2520Embodied%2520intelligence%2520focuses%2520on%2520training%2520agents%2520within%2520realistic%2520simulated%2520environments%252C%2520leveraging%2520physical%2520interaction%2520and%2520action%2520feedback%2520rather%2520than%2520conventionally%2520labeled%2520datasets.%2520Yet%252C%2520most%2520existing%2520simulation%2520platforms%2520remain%2520narrowly%2520designed%252C%2520each%2520tailored%2520to%2520specific%2520tasks.%2520A%2520versatile%252C%2520general-purpose%2520training%2520environment%2520that%2520can%2520support%2520everything%2520from%2520low-level%2520embodied%2520navigation%2520to%2520high-level%2520composite%2520activities%252C%2520such%2520as%2520multi-agent%2520social%2520simulation%2520and%2520human-AI%2520collaboration%252C%2520remains%2520largely%2520unavailable.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520TongSIM%252C%2520a%2520high-fidelity%252C%2520general-purpose%2520platform%2520for%2520training%2520and%2520evaluating%2520embodied%2520agents.%2520TongSIM%2520offers%2520practical%2520advantages%2520by%2520providing%2520over%2520100%2520diverse%252C%2520multi-room%2520indoor%2520scenarios%2520as%2520well%2520as%2520an%2520open-ended%252C%2520interaction-rich%2520outdoor%2520town%2520simulation%252C%2520ensuring%2520broad%2520applicability%2520across%2520research%2520needs.%2520Its%2520comprehensive%2520evaluation%2520framework%2520and%2520benchmarks%2520enable%2520precise%2520assessment%2520of%2520agent%2520capabilities%252C%2520such%2520as%2520perception%252C%2520cognition%252C%2520decision-making%252C%2520human-robot%2520cooperation%252C%2520and%2520spatial%2520and%2520social%2520reasoning.%2520With%2520features%2520like%2520customized%2520scenes%252C%2520task-adaptive%2520fidelity%252C%2520diverse%2520agent%2520types%252C%2520and%2520dynamic%2520environmental%2520simulation%252C%2520TongSIM%2520delivers%2520flexibility%2520and%2520scalability%2520for%2520researchers%252C%2520serving%2520as%2520a%2520unified%2520platform%2520that%2520accelerates%2520training%252C%2520evaluation%252C%2520and%2520advancement%2520toward%2520general%2520embodied%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TongSIM%3A%20A%20General%20Platform%20for%20Simulating%20Intelligent%20Machines&entry.906535625=Zhe%20Sun%20and%20Kunlun%20Wu%20and%20Chuanjian%20Fu%20and%20Zeming%20Song%20and%20Langyong%20Shi%20and%20Zihe%20Xue%20and%20Bohan%20Jing%20and%20Ying%20Yang%20and%20Xiaomeng%20Gao%20and%20Aijia%20Li%20and%20Tianyu%20Guo%20and%20Huiying%20Li%20and%20Xueyuan%20Yang%20and%20Rongkai%20Liu%20and%20Xinyi%20He%20and%20Yuxi%20Wang%20and%20Yue%20Li%20and%20Mingyuan%20Liu%20and%20Yujie%20Lu%20and%20Hongzhao%20Xie%20and%20Shiyun%20Zhao%20and%20Bo%20Dai%20and%20Wei%20Wang%20and%20Tao%20Yuan%20and%20Song-Chun%20Zhu%20and%20Yujia%20Peng%20and%20Zhenliang%20Zhang&entry.1292438233=As%20artificial%20intelligence%20%28AI%29%20rapidly%20advances%2C%20especially%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20research%20focus%20is%20shifting%20from%20single-modality%20text%20processing%20to%20the%20more%20complex%20domains%20of%20multimodal%20and%20embodied%20AI.%20Embodied%20intelligence%20focuses%20on%20training%20agents%20within%20realistic%20simulated%20environments%2C%20leveraging%20physical%20interaction%20and%20action%20feedback%20rather%20than%20conventionally%20labeled%20datasets.%20Yet%2C%20most%20existing%20simulation%20platforms%20remain%20narrowly%20designed%2C%20each%20tailored%20to%20specific%20tasks.%20A%20versatile%2C%20general-purpose%20training%20environment%20that%20can%20support%20everything%20from%20low-level%20embodied%20navigation%20to%20high-level%20composite%20activities%2C%20such%20as%20multi-agent%20social%20simulation%20and%20human-AI%20collaboration%2C%20remains%20largely%20unavailable.%20To%20bridge%20this%20gap%2C%20we%20introduce%20TongSIM%2C%20a%20high-fidelity%2C%20general-purpose%20platform%20for%20training%20and%20evaluating%20embodied%20agents.%20TongSIM%20offers%20practical%20advantages%20by%20providing%20over%20100%20diverse%2C%20multi-room%20indoor%20scenarios%20as%20well%20as%20an%20open-ended%2C%20interaction-rich%20outdoor%20town%20simulation%2C%20ensuring%20broad%20applicability%20across%20research%20needs.%20Its%20comprehensive%20evaluation%20framework%20and%20benchmarks%20enable%20precise%20assessment%20of%20agent%20capabilities%2C%20such%20as%20perception%2C%20cognition%2C%20decision-making%2C%20human-robot%20cooperation%2C%20and%20spatial%20and%20social%20reasoning.%20With%20features%20like%20customized%20scenes%2C%20task-adaptive%20fidelity%2C%20diverse%20agent%20types%2C%20and%20dynamic%20environmental%20simulation%2C%20TongSIM%20delivers%20flexibility%20and%20scalability%20for%20researchers%2C%20serving%20as%20a%20unified%20platform%20that%20accelerates%20training%2C%20evaluation%2C%20and%20advancement%20toward%20general%20embodied%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2512.20206v1&entry.124074799=Read"},
{"title": "Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems", "author": "Yoshihiro Maruyama", "abstract": "We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures. Formulating linear and nonlinear layers in the categorical setup, we prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries.", "link": "http://arxiv.org/abs/2511.18417v2", "date": "2025-12-23", "relevancy": 2.2065, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4658}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.438}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Categorical%20Equivariant%20Deep%20Learning%3A%20Category-Equivariant%20Neural%20Networks%20and%20Universal%20Approximation%20Theorems&body=Title%3A%20Categorical%20Equivariant%20Deep%20Learning%3A%20Category-Equivariant%20Neural%20Networks%20and%20Universal%20Approximation%20Theorems%0AAuthor%3A%20Yoshihiro%20Maruyama%0AAbstract%3A%20We%20develop%20a%20theory%20of%20category-equivariant%20neural%20networks%20%28CENNs%29%20that%20unifies%20group/groupoid-equivariant%20networks%2C%20poset/lattice-equivariant%20networks%2C%20graph%20and%20sheaf%20neural%20networks.%20Equivariance%20is%20formulated%20as%20naturality%20in%20a%20topological%20category%20with%20Radon%20measures.%20Formulating%20linear%20and%20nonlinear%20layers%20in%20the%20categorical%20setup%2C%20we%20prove%20the%20equivariant%20universal%20approximation%20theorem%20in%20the%20general%20setting%3A%20the%20class%20of%20finite-depth%20CENNs%20is%20dense%20in%20the%20space%20of%20continuous%20equivariant%20transformations.%20We%20instantiate%20the%20framework%20for%20groups/groupoids%2C%20posets/lattices%2C%20graphs%20and%20cellular%20sheaves%2C%20deriving%20universal%20approximation%20theorems%20for%20them%20in%20a%20systematic%20manner.%20Categorical%20equivariant%20deep%20learning%20thus%20allows%20us%20to%20expand%20the%20horizons%20of%20equivariant%20deep%20learning%20beyond%20group%20actions%2C%20encompassing%20not%20only%20geometric%20symmetries%20but%20also%20contextual%20and%20compositional%20symmetries.%0ALink%3A%20http%3A//arxiv.org/abs/2511.18417v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCategorical%2520Equivariant%2520Deep%2520Learning%253A%2520Category-Equivariant%2520Neural%2520Networks%2520and%2520Universal%2520Approximation%2520Theorems%26entry.906535625%3DYoshihiro%2520Maruyama%26entry.1292438233%3DWe%2520develop%2520a%2520theory%2520of%2520category-equivariant%2520neural%2520networks%2520%2528CENNs%2529%2520that%2520unifies%2520group/groupoid-equivariant%2520networks%252C%2520poset/lattice-equivariant%2520networks%252C%2520graph%2520and%2520sheaf%2520neural%2520networks.%2520Equivariance%2520is%2520formulated%2520as%2520naturality%2520in%2520a%2520topological%2520category%2520with%2520Radon%2520measures.%2520Formulating%2520linear%2520and%2520nonlinear%2520layers%2520in%2520the%2520categorical%2520setup%252C%2520we%2520prove%2520the%2520equivariant%2520universal%2520approximation%2520theorem%2520in%2520the%2520general%2520setting%253A%2520the%2520class%2520of%2520finite-depth%2520CENNs%2520is%2520dense%2520in%2520the%2520space%2520of%2520continuous%2520equivariant%2520transformations.%2520We%2520instantiate%2520the%2520framework%2520for%2520groups/groupoids%252C%2520posets/lattices%252C%2520graphs%2520and%2520cellular%2520sheaves%252C%2520deriving%2520universal%2520approximation%2520theorems%2520for%2520them%2520in%2520a%2520systematic%2520manner.%2520Categorical%2520equivariant%2520deep%2520learning%2520thus%2520allows%2520us%2520to%2520expand%2520the%2520horizons%2520of%2520equivariant%2520deep%2520learning%2520beyond%2520group%2520actions%252C%2520encompassing%2520not%2520only%2520geometric%2520symmetries%2520but%2520also%2520contextual%2520and%2520compositional%2520symmetries.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.18417v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Categorical%20Equivariant%20Deep%20Learning%3A%20Category-Equivariant%20Neural%20Networks%20and%20Universal%20Approximation%20Theorems&entry.906535625=Yoshihiro%20Maruyama&entry.1292438233=We%20develop%20a%20theory%20of%20category-equivariant%20neural%20networks%20%28CENNs%29%20that%20unifies%20group/groupoid-equivariant%20networks%2C%20poset/lattice-equivariant%20networks%2C%20graph%20and%20sheaf%20neural%20networks.%20Equivariance%20is%20formulated%20as%20naturality%20in%20a%20topological%20category%20with%20Radon%20measures.%20Formulating%20linear%20and%20nonlinear%20layers%20in%20the%20categorical%20setup%2C%20we%20prove%20the%20equivariant%20universal%20approximation%20theorem%20in%20the%20general%20setting%3A%20the%20class%20of%20finite-depth%20CENNs%20is%20dense%20in%20the%20space%20of%20continuous%20equivariant%20transformations.%20We%20instantiate%20the%20framework%20for%20groups/groupoids%2C%20posets/lattices%2C%20graphs%20and%20cellular%20sheaves%2C%20deriving%20universal%20approximation%20theorems%20for%20them%20in%20a%20systematic%20manner.%20Categorical%20equivariant%20deep%20learning%20thus%20allows%20us%20to%20expand%20the%20horizons%20of%20equivariant%20deep%20learning%20beyond%20group%20actions%2C%20encompassing%20not%20only%20geometric%20symmetries%20but%20also%20contextual%20and%20compositional%20symmetries.&entry.1838667208=http%3A//arxiv.org/abs/2511.18417v2&entry.124074799=Read"},
{"title": "Chain-of-Anomaly Thoughts with Large Vision-Language Models", "author": "Pedro Domingos and Jo\u00e3o Pereira and Vasco Lopes and Jo\u00e3o Neves and David Semedo", "abstract": "Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.", "link": "http://arxiv.org/abs/2512.20417v1", "date": "2025-12-23", "relevancy": 2.1971, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5532}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain-of-Anomaly%20Thoughts%20with%20Large%20Vision-Language%20Models&body=Title%3A%20Chain-of-Anomaly%20Thoughts%20with%20Large%20Vision-Language%20Models%0AAuthor%3A%20Pedro%20Domingos%20and%20Jo%C3%A3o%20Pereira%20and%20Vasco%20Lopes%20and%20Jo%C3%A3o%20Neves%20and%20David%20Semedo%0AAbstract%3A%20Automated%20video%20surveillance%20with%20Large%20Vision-Language%20Models%20is%20limited%20by%20their%20inherent%20bias%20towards%20normality%2C%20often%20failing%20to%20detect%20crimes.%20While%20Chain-of-Thought%20reasoning%20strategies%20show%20significant%20potential%20for%20improving%20performance%20in%20language%20tasks%2C%20the%20lack%20of%20inductive%20anomaly%20biases%20in%20their%20reasoning%20further%20steers%20the%20models%20towards%20normal%20interpretations.%20To%20address%20this%2C%20we%20propose%20Chain-of-Anomaly-Thoughts%20%28CoAT%29%2C%20a%20multi-agent%20reasoning%20framework%20that%20introduces%20inductive%20criminal%20bias%20in%20the%20reasoning%20process%20through%20a%20final%2C%20anomaly-focused%20classification%20layer.%20Our%20method%20significantly%20improves%20Anomaly%20Detection%2C%20boosting%20F1-score%20by%2011.8%20p.p.%20on%20challenging%20low-resolution%20footage%20and%20Anomaly%20Classification%20by%203.78%20p.p.%20in%20high-resolution%20videos.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain-of-Anomaly%2520Thoughts%2520with%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DPedro%2520Domingos%2520and%2520Jo%25C3%25A3o%2520Pereira%2520and%2520Vasco%2520Lopes%2520and%2520Jo%25C3%25A3o%2520Neves%2520and%2520David%2520Semedo%26entry.1292438233%3DAutomated%2520video%2520surveillance%2520with%2520Large%2520Vision-Language%2520Models%2520is%2520limited%2520by%2520their%2520inherent%2520bias%2520towards%2520normality%252C%2520often%2520failing%2520to%2520detect%2520crimes.%2520While%2520Chain-of-Thought%2520reasoning%2520strategies%2520show%2520significant%2520potential%2520for%2520improving%2520performance%2520in%2520language%2520tasks%252C%2520the%2520lack%2520of%2520inductive%2520anomaly%2520biases%2520in%2520their%2520reasoning%2520further%2520steers%2520the%2520models%2520towards%2520normal%2520interpretations.%2520To%2520address%2520this%252C%2520we%2520propose%2520Chain-of-Anomaly-Thoughts%2520%2528CoAT%2529%252C%2520a%2520multi-agent%2520reasoning%2520framework%2520that%2520introduces%2520inductive%2520criminal%2520bias%2520in%2520the%2520reasoning%2520process%2520through%2520a%2520final%252C%2520anomaly-focused%2520classification%2520layer.%2520Our%2520method%2520significantly%2520improves%2520Anomaly%2520Detection%252C%2520boosting%2520F1-score%2520by%252011.8%2520p.p.%2520on%2520challenging%2520low-resolution%2520footage%2520and%2520Anomaly%2520Classification%2520by%25203.78%2520p.p.%2520in%2520high-resolution%2520videos.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain-of-Anomaly%20Thoughts%20with%20Large%20Vision-Language%20Models&entry.906535625=Pedro%20Domingos%20and%20Jo%C3%A3o%20Pereira%20and%20Vasco%20Lopes%20and%20Jo%C3%A3o%20Neves%20and%20David%20Semedo&entry.1292438233=Automated%20video%20surveillance%20with%20Large%20Vision-Language%20Models%20is%20limited%20by%20their%20inherent%20bias%20towards%20normality%2C%20often%20failing%20to%20detect%20crimes.%20While%20Chain-of-Thought%20reasoning%20strategies%20show%20significant%20potential%20for%20improving%20performance%20in%20language%20tasks%2C%20the%20lack%20of%20inductive%20anomaly%20biases%20in%20their%20reasoning%20further%20steers%20the%20models%20towards%20normal%20interpretations.%20To%20address%20this%2C%20we%20propose%20Chain-of-Anomaly-Thoughts%20%28CoAT%29%2C%20a%20multi-agent%20reasoning%20framework%20that%20introduces%20inductive%20criminal%20bias%20in%20the%20reasoning%20process%20through%20a%20final%2C%20anomaly-focused%20classification%20layer.%20Our%20method%20significantly%20improves%20Anomaly%20Detection%2C%20boosting%20F1-score%20by%2011.8%20p.p.%20on%20challenging%20low-resolution%20footage%20and%20Anomaly%20Classification%20by%203.78%20p.p.%20in%20high-resolution%20videos.&entry.1838667208=http%3A//arxiv.org/abs/2512.20417v1&entry.124074799=Read"},
{"title": "Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios", "author": "Mingwei Tang and Jiahao Nie and Guang Yang and Ziqing Cui and Jie Li", "abstract": "Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.", "link": "http://arxiv.org/abs/2512.20556v1", "date": "2025-12-23", "relevancy": 2.1696, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5601}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Grained%20Text-Guided%20Image%20Fusion%20for%20Multi-Exposure%20and%20Multi-Focus%20Scenarios&body=Title%3A%20Multi-Grained%20Text-Guided%20Image%20Fusion%20for%20Multi-Exposure%20and%20Multi-Focus%20Scenarios%0AAuthor%3A%20Mingwei%20Tang%20and%20Jiahao%20Nie%20and%20Guang%20Yang%20and%20Ziqing%20Cui%20and%20Jie%20Li%0AAbstract%3A%20Image%20fusion%20aims%20to%20synthesize%20a%20single%20high-quality%20image%20from%20a%20pair%20of%20inputs%20captured%20under%20challenging%20conditions%2C%20such%20as%20differing%20exposure%20levels%20or%20focal%20depths.%20A%20core%20challenge%20lies%20in%20effectively%20handling%20disparities%20in%20dynamic%20range%20and%20focus%20depth%20between%20the%20inputs.%20With%20the%20advent%20of%20vision-language%20models%2C%20recent%20methods%20incorporate%20textual%20descriptions%20as%20auxiliary%20guidance%20to%20enhance%20fusion%20quality.%20However%2C%20simply%20incorporating%20coarse-grained%20descriptions%20hampers%20the%20understanding%20of%20fine-grained%20details%20and%20poses%20challenges%20for%20precise%20cross-modal%20alignment.%20To%20address%20these%20limitations%2C%20we%20propose%20Multi-grained%20Text-guided%20Image%20Fusion%20%28MTIF%29%2C%20a%20novel%20fusion%20paradigm%20with%20three%20key%20designs.%20First%2C%20it%20introduces%20multi-grained%20textual%20descriptions%20that%20separately%20capture%20fine%20details%2C%20structural%20cues%2C%20and%20semantic%20content%2C%20guiding%20image%20fusion%20through%20a%20hierarchical%20cross-modal%20modulation%20module.%20Second%2C%20it%20involves%20supervision%20signals%20at%20each%20granularity%20to%20facilitate%20alignment%20between%20visual%20and%20textual%20features%20and%20enhance%20the%20utility%20of%20auxiliary%20text.%20Third%2C%20it%20adopts%20a%20saliency-driven%20enrichment%20module%20to%20augment%20training%20data%20with%20dense%20semantic%20content%2C%20further%20strengthening%20the%20cross-modal%20modulation%20and%20alignment.%20Extensive%20experiments%20show%20that%20MTIF%20consistently%20outperforms%20previous%20methods%20on%20both%20multi-exposure%20and%20multi-focus%20image%20fusion%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Grained%2520Text-Guided%2520Image%2520Fusion%2520for%2520Multi-Exposure%2520and%2520Multi-Focus%2520Scenarios%26entry.906535625%3DMingwei%2520Tang%2520and%2520Jiahao%2520Nie%2520and%2520Guang%2520Yang%2520and%2520Ziqing%2520Cui%2520and%2520Jie%2520Li%26entry.1292438233%3DImage%2520fusion%2520aims%2520to%2520synthesize%2520a%2520single%2520high-quality%2520image%2520from%2520a%2520pair%2520of%2520inputs%2520captured%2520under%2520challenging%2520conditions%252C%2520such%2520as%2520differing%2520exposure%2520levels%2520or%2520focal%2520depths.%2520A%2520core%2520challenge%2520lies%2520in%2520effectively%2520handling%2520disparities%2520in%2520dynamic%2520range%2520and%2520focus%2520depth%2520between%2520the%2520inputs.%2520With%2520the%2520advent%2520of%2520vision-language%2520models%252C%2520recent%2520methods%2520incorporate%2520textual%2520descriptions%2520as%2520auxiliary%2520guidance%2520to%2520enhance%2520fusion%2520quality.%2520However%252C%2520simply%2520incorporating%2520coarse-grained%2520descriptions%2520hampers%2520the%2520understanding%2520of%2520fine-grained%2520details%2520and%2520poses%2520challenges%2520for%2520precise%2520cross-modal%2520alignment.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Multi-grained%2520Text-guided%2520Image%2520Fusion%2520%2528MTIF%2529%252C%2520a%2520novel%2520fusion%2520paradigm%2520with%2520three%2520key%2520designs.%2520First%252C%2520it%2520introduces%2520multi-grained%2520textual%2520descriptions%2520that%2520separately%2520capture%2520fine%2520details%252C%2520structural%2520cues%252C%2520and%2520semantic%2520content%252C%2520guiding%2520image%2520fusion%2520through%2520a%2520hierarchical%2520cross-modal%2520modulation%2520module.%2520Second%252C%2520it%2520involves%2520supervision%2520signals%2520at%2520each%2520granularity%2520to%2520facilitate%2520alignment%2520between%2520visual%2520and%2520textual%2520features%2520and%2520enhance%2520the%2520utility%2520of%2520auxiliary%2520text.%2520Third%252C%2520it%2520adopts%2520a%2520saliency-driven%2520enrichment%2520module%2520to%2520augment%2520training%2520data%2520with%2520dense%2520semantic%2520content%252C%2520further%2520strengthening%2520the%2520cross-modal%2520modulation%2520and%2520alignment.%2520Extensive%2520experiments%2520show%2520that%2520MTIF%2520consistently%2520outperforms%2520previous%2520methods%2520on%2520both%2520multi-exposure%2520and%2520multi-focus%2520image%2520fusion%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Grained%20Text-Guided%20Image%20Fusion%20for%20Multi-Exposure%20and%20Multi-Focus%20Scenarios&entry.906535625=Mingwei%20Tang%20and%20Jiahao%20Nie%20and%20Guang%20Yang%20and%20Ziqing%20Cui%20and%20Jie%20Li&entry.1292438233=Image%20fusion%20aims%20to%20synthesize%20a%20single%20high-quality%20image%20from%20a%20pair%20of%20inputs%20captured%20under%20challenging%20conditions%2C%20such%20as%20differing%20exposure%20levels%20or%20focal%20depths.%20A%20core%20challenge%20lies%20in%20effectively%20handling%20disparities%20in%20dynamic%20range%20and%20focus%20depth%20between%20the%20inputs.%20With%20the%20advent%20of%20vision-language%20models%2C%20recent%20methods%20incorporate%20textual%20descriptions%20as%20auxiliary%20guidance%20to%20enhance%20fusion%20quality.%20However%2C%20simply%20incorporating%20coarse-grained%20descriptions%20hampers%20the%20understanding%20of%20fine-grained%20details%20and%20poses%20challenges%20for%20precise%20cross-modal%20alignment.%20To%20address%20these%20limitations%2C%20we%20propose%20Multi-grained%20Text-guided%20Image%20Fusion%20%28MTIF%29%2C%20a%20novel%20fusion%20paradigm%20with%20three%20key%20designs.%20First%2C%20it%20introduces%20multi-grained%20textual%20descriptions%20that%20separately%20capture%20fine%20details%2C%20structural%20cues%2C%20and%20semantic%20content%2C%20guiding%20image%20fusion%20through%20a%20hierarchical%20cross-modal%20modulation%20module.%20Second%2C%20it%20involves%20supervision%20signals%20at%20each%20granularity%20to%20facilitate%20alignment%20between%20visual%20and%20textual%20features%20and%20enhance%20the%20utility%20of%20auxiliary%20text.%20Third%2C%20it%20adopts%20a%20saliency-driven%20enrichment%20module%20to%20augment%20training%20data%20with%20dense%20semantic%20content%2C%20further%20strengthening%20the%20cross-modal%20modulation%20and%20alignment.%20Extensive%20experiments%20show%20that%20MTIF%20consistently%20outperforms%20previous%20methods%20on%20both%20multi-exposure%20and%20multi-focus%20image%20fusion%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.20556v1&entry.124074799=Read"},
{"title": "Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing", "author": "Maulana Bisyir Azhari and Donghun Han and Je In You and Sungjun Park and David Hyunchul Shim", "abstract": "The Abu Dhabi Autonomous Racing League(A2RL) x Drone Champions League competition(DCL) requires teams to perform high-speed autonomous drone racing using only a single camera and a low-quality inertial measurement unit -- a minimal sensor set that mirrors expert human drone racing pilots. This sensor limitation makes the system susceptible to drift from Visual-Inertial Odometry (VIO), particularly during long and fast flights with aggressive maneuvers. This paper presents the system developed for the championship, which achieved a competitive performance. Our approach corrected VIO drift by fusing its output with global position measurements derived from a YOLO-based gate detector using a Kalman filter. A perception-aware planner generated trajectories that balance speed with the need to keep gates visible for the perception system. The system demonstrated high performance, securing podium finishes across multiple categories: third place in the AI Grand Challenge with top speed of 43.2 km/h, second place in the AI Drag Race with over 59 km/h, and second place in the AI Multi-Drone Race. We detail the complete architecture and present a performance analysis based on experimental data from the competition, contributing our insights on building a successful system for monocular vision-based autonomous drone flight.", "link": "http://arxiv.org/abs/2512.20475v1", "date": "2025-12-23", "relevancy": 2.1692, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5493}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5417}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drift-Corrected%20Monocular%20VIO%20and%20Perception-Aware%20Planning%20for%20Autonomous%20Drone%20Racing&body=Title%3A%20Drift-Corrected%20Monocular%20VIO%20and%20Perception-Aware%20Planning%20for%20Autonomous%20Drone%20Racing%0AAuthor%3A%20Maulana%20Bisyir%20Azhari%20and%20Donghun%20Han%20and%20Je%20In%20You%20and%20Sungjun%20Park%20and%20David%20Hyunchul%20Shim%0AAbstract%3A%20The%20Abu%20Dhabi%20Autonomous%20Racing%20League%28A2RL%29%20x%20Drone%20Champions%20League%20competition%28DCL%29%20requires%20teams%20to%20perform%20high-speed%20autonomous%20drone%20racing%20using%20only%20a%20single%20camera%20and%20a%20low-quality%20inertial%20measurement%20unit%20--%20a%20minimal%20sensor%20set%20that%20mirrors%20expert%20human%20drone%20racing%20pilots.%20This%20sensor%20limitation%20makes%20the%20system%20susceptible%20to%20drift%20from%20Visual-Inertial%20Odometry%20%28VIO%29%2C%20particularly%20during%20long%20and%20fast%20flights%20with%20aggressive%20maneuvers.%20This%20paper%20presents%20the%20system%20developed%20for%20the%20championship%2C%20which%20achieved%20a%20competitive%20performance.%20Our%20approach%20corrected%20VIO%20drift%20by%20fusing%20its%20output%20with%20global%20position%20measurements%20derived%20from%20a%20YOLO-based%20gate%20detector%20using%20a%20Kalman%20filter.%20A%20perception-aware%20planner%20generated%20trajectories%20that%20balance%20speed%20with%20the%20need%20to%20keep%20gates%20visible%20for%20the%20perception%20system.%20The%20system%20demonstrated%20high%20performance%2C%20securing%20podium%20finishes%20across%20multiple%20categories%3A%20third%20place%20in%20the%20AI%20Grand%20Challenge%20with%20top%20speed%20of%2043.2%20km/h%2C%20second%20place%20in%20the%20AI%20Drag%20Race%20with%20over%2059%20km/h%2C%20and%20second%20place%20in%20the%20AI%20Multi-Drone%20Race.%20We%20detail%20the%20complete%20architecture%20and%20present%20a%20performance%20analysis%20based%20on%20experimental%20data%20from%20the%20competition%2C%20contributing%20our%20insights%20on%20building%20a%20successful%20system%20for%20monocular%20vision-based%20autonomous%20drone%20flight.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrift-Corrected%2520Monocular%2520VIO%2520and%2520Perception-Aware%2520Planning%2520for%2520Autonomous%2520Drone%2520Racing%26entry.906535625%3DMaulana%2520Bisyir%2520Azhari%2520and%2520Donghun%2520Han%2520and%2520Je%2520In%2520You%2520and%2520Sungjun%2520Park%2520and%2520David%2520Hyunchul%2520Shim%26entry.1292438233%3DThe%2520Abu%2520Dhabi%2520Autonomous%2520Racing%2520League%2528A2RL%2529%2520x%2520Drone%2520Champions%2520League%2520competition%2528DCL%2529%2520requires%2520teams%2520to%2520perform%2520high-speed%2520autonomous%2520drone%2520racing%2520using%2520only%2520a%2520single%2520camera%2520and%2520a%2520low-quality%2520inertial%2520measurement%2520unit%2520--%2520a%2520minimal%2520sensor%2520set%2520that%2520mirrors%2520expert%2520human%2520drone%2520racing%2520pilots.%2520This%2520sensor%2520limitation%2520makes%2520the%2520system%2520susceptible%2520to%2520drift%2520from%2520Visual-Inertial%2520Odometry%2520%2528VIO%2529%252C%2520particularly%2520during%2520long%2520and%2520fast%2520flights%2520with%2520aggressive%2520maneuvers.%2520This%2520paper%2520presents%2520the%2520system%2520developed%2520for%2520the%2520championship%252C%2520which%2520achieved%2520a%2520competitive%2520performance.%2520Our%2520approach%2520corrected%2520VIO%2520drift%2520by%2520fusing%2520its%2520output%2520with%2520global%2520position%2520measurements%2520derived%2520from%2520a%2520YOLO-based%2520gate%2520detector%2520using%2520a%2520Kalman%2520filter.%2520A%2520perception-aware%2520planner%2520generated%2520trajectories%2520that%2520balance%2520speed%2520with%2520the%2520need%2520to%2520keep%2520gates%2520visible%2520for%2520the%2520perception%2520system.%2520The%2520system%2520demonstrated%2520high%2520performance%252C%2520securing%2520podium%2520finishes%2520across%2520multiple%2520categories%253A%2520third%2520place%2520in%2520the%2520AI%2520Grand%2520Challenge%2520with%2520top%2520speed%2520of%252043.2%2520km/h%252C%2520second%2520place%2520in%2520the%2520AI%2520Drag%2520Race%2520with%2520over%252059%2520km/h%252C%2520and%2520second%2520place%2520in%2520the%2520AI%2520Multi-Drone%2520Race.%2520We%2520detail%2520the%2520complete%2520architecture%2520and%2520present%2520a%2520performance%2520analysis%2520based%2520on%2520experimental%2520data%2520from%2520the%2520competition%252C%2520contributing%2520our%2520insights%2520on%2520building%2520a%2520successful%2520system%2520for%2520monocular%2520vision-based%2520autonomous%2520drone%2520flight.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drift-Corrected%20Monocular%20VIO%20and%20Perception-Aware%20Planning%20for%20Autonomous%20Drone%20Racing&entry.906535625=Maulana%20Bisyir%20Azhari%20and%20Donghun%20Han%20and%20Je%20In%20You%20and%20Sungjun%20Park%20and%20David%20Hyunchul%20Shim&entry.1292438233=The%20Abu%20Dhabi%20Autonomous%20Racing%20League%28A2RL%29%20x%20Drone%20Champions%20League%20competition%28DCL%29%20requires%20teams%20to%20perform%20high-speed%20autonomous%20drone%20racing%20using%20only%20a%20single%20camera%20and%20a%20low-quality%20inertial%20measurement%20unit%20--%20a%20minimal%20sensor%20set%20that%20mirrors%20expert%20human%20drone%20racing%20pilots.%20This%20sensor%20limitation%20makes%20the%20system%20susceptible%20to%20drift%20from%20Visual-Inertial%20Odometry%20%28VIO%29%2C%20particularly%20during%20long%20and%20fast%20flights%20with%20aggressive%20maneuvers.%20This%20paper%20presents%20the%20system%20developed%20for%20the%20championship%2C%20which%20achieved%20a%20competitive%20performance.%20Our%20approach%20corrected%20VIO%20drift%20by%20fusing%20its%20output%20with%20global%20position%20measurements%20derived%20from%20a%20YOLO-based%20gate%20detector%20using%20a%20Kalman%20filter.%20A%20perception-aware%20planner%20generated%20trajectories%20that%20balance%20speed%20with%20the%20need%20to%20keep%20gates%20visible%20for%20the%20perception%20system.%20The%20system%20demonstrated%20high%20performance%2C%20securing%20podium%20finishes%20across%20multiple%20categories%3A%20third%20place%20in%20the%20AI%20Grand%20Challenge%20with%20top%20speed%20of%2043.2%20km/h%2C%20second%20place%20in%20the%20AI%20Drag%20Race%20with%20over%2059%20km/h%2C%20and%20second%20place%20in%20the%20AI%20Multi-Drone%20Race.%20We%20detail%20the%20complete%20architecture%20and%20present%20a%20performance%20analysis%20based%20on%20experimental%20data%20from%20the%20competition%2C%20contributing%20our%20insights%20on%20building%20a%20successful%20system%20for%20monocular%20vision-based%20autonomous%20drone%20flight.&entry.1838667208=http%3A//arxiv.org/abs/2512.20475v1&entry.124074799=Read"},
{"title": "HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection", "author": "Zhaolin Cai and Fan Li and Ziwei Zheng and Haixia Bi and Lijun He", "abstract": "Video Anomaly Detection (VAD) aims to locate events that deviate from normal patterns in videos. Traditional approaches often rely on extensive labeled data and incur high computational costs. Recent tuning-free methods based on Multimodal Large Language Models (MLLMs) offer a promising alternative by leveraging their rich world knowledge. However, these methods typically rely on textual outputs, which introduces information loss, exhibits normalcy bias, and suffers from prompt sensitivity, making them insufficient for capturing subtle anomalous cues. To address these constraints, we propose HeadHunt-VAD, a novel tuning-free VAD paradigm that bypasses textual generation by directly hunting robust anomaly-sensitive internal attention heads within the frozen MLLM. Central to our method is a Robust Head Identification module that systematically evaluates all attention heads using a multi-criteria analysis of saliency and stability, identifying a sparse subset of heads that are consistently discriminative across diverse prompts. Features from these expert heads are then fed into a lightweight anomaly scorer and a temporal locator, enabling efficient and accurate anomaly detection with interpretable outputs. Extensive experiments show that HeadHunt-VAD achieves state-of-the-art performance among tuning-free methods on two major VAD benchmarks while maintaining high efficiency, validating head-level probing in MLLMs as a powerful and practical solution for real-world anomaly detection.", "link": "http://arxiv.org/abs/2512.17601v2", "date": "2025-12-23", "relevancy": 2.1664, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5513}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.537}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeadHunt-VAD%3A%20Hunting%20Robust%20Anomaly-Sensitive%20Heads%20in%20MLLM%20for%20Tuning-Free%20Video%20Anomaly%20Detection&body=Title%3A%20HeadHunt-VAD%3A%20Hunting%20Robust%20Anomaly-Sensitive%20Heads%20in%20MLLM%20for%20Tuning-Free%20Video%20Anomaly%20Detection%0AAuthor%3A%20Zhaolin%20Cai%20and%20Fan%20Li%20and%20Ziwei%20Zheng%20and%20Haixia%20Bi%20and%20Lijun%20He%0AAbstract%3A%20Video%20Anomaly%20Detection%20%28VAD%29%20aims%20to%20locate%20events%20that%20deviate%20from%20normal%20patterns%20in%20videos.%20Traditional%20approaches%20often%20rely%20on%20extensive%20labeled%20data%20and%20incur%20high%20computational%20costs.%20Recent%20tuning-free%20methods%20based%20on%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20offer%20a%20promising%20alternative%20by%20leveraging%20their%20rich%20world%20knowledge.%20However%2C%20these%20methods%20typically%20rely%20on%20textual%20outputs%2C%20which%20introduces%20information%20loss%2C%20exhibits%20normalcy%20bias%2C%20and%20suffers%20from%20prompt%20sensitivity%2C%20making%20them%20insufficient%20for%20capturing%20subtle%20anomalous%20cues.%20To%20address%20these%20constraints%2C%20we%20propose%20HeadHunt-VAD%2C%20a%20novel%20tuning-free%20VAD%20paradigm%20that%20bypasses%20textual%20generation%20by%20directly%20hunting%20robust%20anomaly-sensitive%20internal%20attention%20heads%20within%20the%20frozen%20MLLM.%20Central%20to%20our%20method%20is%20a%20Robust%20Head%20Identification%20module%20that%20systematically%20evaluates%20all%20attention%20heads%20using%20a%20multi-criteria%20analysis%20of%20saliency%20and%20stability%2C%20identifying%20a%20sparse%20subset%20of%20heads%20that%20are%20consistently%20discriminative%20across%20diverse%20prompts.%20Features%20from%20these%20expert%20heads%20are%20then%20fed%20into%20a%20lightweight%20anomaly%20scorer%20and%20a%20temporal%20locator%2C%20enabling%20efficient%20and%20accurate%20anomaly%20detection%20with%20interpretable%20outputs.%20Extensive%20experiments%20show%20that%20HeadHunt-VAD%20achieves%20state-of-the-art%20performance%20among%20tuning-free%20methods%20on%20two%20major%20VAD%20benchmarks%20while%20maintaining%20high%20efficiency%2C%20validating%20head-level%20probing%20in%20MLLMs%20as%20a%20powerful%20and%20practical%20solution%20for%20real-world%20anomaly%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17601v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeadHunt-VAD%253A%2520Hunting%2520Robust%2520Anomaly-Sensitive%2520Heads%2520in%2520MLLM%2520for%2520Tuning-Free%2520Video%2520Anomaly%2520Detection%26entry.906535625%3DZhaolin%2520Cai%2520and%2520Fan%2520Li%2520and%2520Ziwei%2520Zheng%2520and%2520Haixia%2520Bi%2520and%2520Lijun%2520He%26entry.1292438233%3DVideo%2520Anomaly%2520Detection%2520%2528VAD%2529%2520aims%2520to%2520locate%2520events%2520that%2520deviate%2520from%2520normal%2520patterns%2520in%2520videos.%2520Traditional%2520approaches%2520often%2520rely%2520on%2520extensive%2520labeled%2520data%2520and%2520incur%2520high%2520computational%2520costs.%2520Recent%2520tuning-free%2520methods%2520based%2520on%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520offer%2520a%2520promising%2520alternative%2520by%2520leveraging%2520their%2520rich%2520world%2520knowledge.%2520However%252C%2520these%2520methods%2520typically%2520rely%2520on%2520textual%2520outputs%252C%2520which%2520introduces%2520information%2520loss%252C%2520exhibits%2520normalcy%2520bias%252C%2520and%2520suffers%2520from%2520prompt%2520sensitivity%252C%2520making%2520them%2520insufficient%2520for%2520capturing%2520subtle%2520anomalous%2520cues.%2520To%2520address%2520these%2520constraints%252C%2520we%2520propose%2520HeadHunt-VAD%252C%2520a%2520novel%2520tuning-free%2520VAD%2520paradigm%2520that%2520bypasses%2520textual%2520generation%2520by%2520directly%2520hunting%2520robust%2520anomaly-sensitive%2520internal%2520attention%2520heads%2520within%2520the%2520frozen%2520MLLM.%2520Central%2520to%2520our%2520method%2520is%2520a%2520Robust%2520Head%2520Identification%2520module%2520that%2520systematically%2520evaluates%2520all%2520attention%2520heads%2520using%2520a%2520multi-criteria%2520analysis%2520of%2520saliency%2520and%2520stability%252C%2520identifying%2520a%2520sparse%2520subset%2520of%2520heads%2520that%2520are%2520consistently%2520discriminative%2520across%2520diverse%2520prompts.%2520Features%2520from%2520these%2520expert%2520heads%2520are%2520then%2520fed%2520into%2520a%2520lightweight%2520anomaly%2520scorer%2520and%2520a%2520temporal%2520locator%252C%2520enabling%2520efficient%2520and%2520accurate%2520anomaly%2520detection%2520with%2520interpretable%2520outputs.%2520Extensive%2520experiments%2520show%2520that%2520HeadHunt-VAD%2520achieves%2520state-of-the-art%2520performance%2520among%2520tuning-free%2520methods%2520on%2520two%2520major%2520VAD%2520benchmarks%2520while%2520maintaining%2520high%2520efficiency%252C%2520validating%2520head-level%2520probing%2520in%2520MLLMs%2520as%2520a%2520powerful%2520and%2520practical%2520solution%2520for%2520real-world%2520anomaly%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17601v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeadHunt-VAD%3A%20Hunting%20Robust%20Anomaly-Sensitive%20Heads%20in%20MLLM%20for%20Tuning-Free%20Video%20Anomaly%20Detection&entry.906535625=Zhaolin%20Cai%20and%20Fan%20Li%20and%20Ziwei%20Zheng%20and%20Haixia%20Bi%20and%20Lijun%20He&entry.1292438233=Video%20Anomaly%20Detection%20%28VAD%29%20aims%20to%20locate%20events%20that%20deviate%20from%20normal%20patterns%20in%20videos.%20Traditional%20approaches%20often%20rely%20on%20extensive%20labeled%20data%20and%20incur%20high%20computational%20costs.%20Recent%20tuning-free%20methods%20based%20on%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20offer%20a%20promising%20alternative%20by%20leveraging%20their%20rich%20world%20knowledge.%20However%2C%20these%20methods%20typically%20rely%20on%20textual%20outputs%2C%20which%20introduces%20information%20loss%2C%20exhibits%20normalcy%20bias%2C%20and%20suffers%20from%20prompt%20sensitivity%2C%20making%20them%20insufficient%20for%20capturing%20subtle%20anomalous%20cues.%20To%20address%20these%20constraints%2C%20we%20propose%20HeadHunt-VAD%2C%20a%20novel%20tuning-free%20VAD%20paradigm%20that%20bypasses%20textual%20generation%20by%20directly%20hunting%20robust%20anomaly-sensitive%20internal%20attention%20heads%20within%20the%20frozen%20MLLM.%20Central%20to%20our%20method%20is%20a%20Robust%20Head%20Identification%20module%20that%20systematically%20evaluates%20all%20attention%20heads%20using%20a%20multi-criteria%20analysis%20of%20saliency%20and%20stability%2C%20identifying%20a%20sparse%20subset%20of%20heads%20that%20are%20consistently%20discriminative%20across%20diverse%20prompts.%20Features%20from%20these%20expert%20heads%20are%20then%20fed%20into%20a%20lightweight%20anomaly%20scorer%20and%20a%20temporal%20locator%2C%20enabling%20efficient%20and%20accurate%20anomaly%20detection%20with%20interpretable%20outputs.%20Extensive%20experiments%20show%20that%20HeadHunt-VAD%20achieves%20state-of-the-art%20performance%20among%20tuning-free%20methods%20on%20two%20major%20VAD%20benchmarks%20while%20maintaining%20high%20efficiency%2C%20validating%20head-level%20probing%20in%20MLLMs%20as%20a%20powerful%20and%20practical%20solution%20for%20real-world%20anomaly%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2512.17601v2&entry.124074799=Read"},
{"title": "UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas", "author": "Qijun Qin and Ziqi Zhang and Yihan Zhong and Feng Huang and Xikun Liu and Runzhi Hu and Hang Chen and Wei Hu and Dongzhe Su and Jun Zhang and Hoi-Fung Ng and Weisong Wen", "abstract": "Due to the limitations of a single autonomous vehicle, Cellular Vehicle-to-Everything (C-V2X) technology opens a new window for achieving fully autonomous driving through sensor information sharing. However, real-world datasets supporting vehicle-infrastructure cooperative navigation in complex urban environments remain rare. To address this gap, we present UrbanV2X, a comprehensive multisensory dataset collected from vehicles and roadside infrastructure in the Hong Kong C-V2X testbed, designed to support research on smart mobility applications in dense urban areas. Our onboard platform provides synchronized data from multiple industrial cameras, LiDARs, 4D radar, ultra-wideband (UWB), IMU, and high-precision GNSS-RTK/INS navigation systems. Meanwhile, our roadside infrastructure provides LiDAR, GNSS, and UWB measurements. The entire vehicle-infrastructure platform is synchronized using the Precision Time Protocol (PTP), with sensor calibration data provided. We also benchmark various navigation algorithms to evaluate the collected cooperative data. The dataset is publicly available at https://polyu-taslab.github.io/UrbanV2X/.", "link": "http://arxiv.org/abs/2512.20224v1", "date": "2025-12-23", "relevancy": 2.1659, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5508}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5482}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UrbanV2X%3A%20A%20Multisensory%20Vehicle-Infrastructure%20Dataset%20for%20Cooperative%20Navigation%20in%20Urban%20Areas&body=Title%3A%20UrbanV2X%3A%20A%20Multisensory%20Vehicle-Infrastructure%20Dataset%20for%20Cooperative%20Navigation%20in%20Urban%20Areas%0AAuthor%3A%20Qijun%20Qin%20and%20Ziqi%20Zhang%20and%20Yihan%20Zhong%20and%20Feng%20Huang%20and%20Xikun%20Liu%20and%20Runzhi%20Hu%20and%20Hang%20Chen%20and%20Wei%20Hu%20and%20Dongzhe%20Su%20and%20Jun%20Zhang%20and%20Hoi-Fung%20Ng%20and%20Weisong%20Wen%0AAbstract%3A%20Due%20to%20the%20limitations%20of%20a%20single%20autonomous%20vehicle%2C%20Cellular%20Vehicle-to-Everything%20%28C-V2X%29%20technology%20opens%20a%20new%20window%20for%20achieving%20fully%20autonomous%20driving%20through%20sensor%20information%20sharing.%20However%2C%20real-world%20datasets%20supporting%20vehicle-infrastructure%20cooperative%20navigation%20in%20complex%20urban%20environments%20remain%20rare.%20To%20address%20this%20gap%2C%20we%20present%20UrbanV2X%2C%20a%20comprehensive%20multisensory%20dataset%20collected%20from%20vehicles%20and%20roadside%20infrastructure%20in%20the%20Hong%20Kong%20C-V2X%20testbed%2C%20designed%20to%20support%20research%20on%20smart%20mobility%20applications%20in%20dense%20urban%20areas.%20Our%20onboard%20platform%20provides%20synchronized%20data%20from%20multiple%20industrial%20cameras%2C%20LiDARs%2C%204D%20radar%2C%20ultra-wideband%20%28UWB%29%2C%20IMU%2C%20and%20high-precision%20GNSS-RTK/INS%20navigation%20systems.%20Meanwhile%2C%20our%20roadside%20infrastructure%20provides%20LiDAR%2C%20GNSS%2C%20and%20UWB%20measurements.%20The%20entire%20vehicle-infrastructure%20platform%20is%20synchronized%20using%20the%20Precision%20Time%20Protocol%20%28PTP%29%2C%20with%20sensor%20calibration%20data%20provided.%20We%20also%20benchmark%20various%20navigation%20algorithms%20to%20evaluate%20the%20collected%20cooperative%20data.%20The%20dataset%20is%20publicly%20available%20at%20https%3A//polyu-taslab.github.io/UrbanV2X/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrbanV2X%253A%2520A%2520Multisensory%2520Vehicle-Infrastructure%2520Dataset%2520for%2520Cooperative%2520Navigation%2520in%2520Urban%2520Areas%26entry.906535625%3DQijun%2520Qin%2520and%2520Ziqi%2520Zhang%2520and%2520Yihan%2520Zhong%2520and%2520Feng%2520Huang%2520and%2520Xikun%2520Liu%2520and%2520Runzhi%2520Hu%2520and%2520Hang%2520Chen%2520and%2520Wei%2520Hu%2520and%2520Dongzhe%2520Su%2520and%2520Jun%2520Zhang%2520and%2520Hoi-Fung%2520Ng%2520and%2520Weisong%2520Wen%26entry.1292438233%3DDue%2520to%2520the%2520limitations%2520of%2520a%2520single%2520autonomous%2520vehicle%252C%2520Cellular%2520Vehicle-to-Everything%2520%2528C-V2X%2529%2520technology%2520opens%2520a%2520new%2520window%2520for%2520achieving%2520fully%2520autonomous%2520driving%2520through%2520sensor%2520information%2520sharing.%2520However%252C%2520real-world%2520datasets%2520supporting%2520vehicle-infrastructure%2520cooperative%2520navigation%2520in%2520complex%2520urban%2520environments%2520remain%2520rare.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520UrbanV2X%252C%2520a%2520comprehensive%2520multisensory%2520dataset%2520collected%2520from%2520vehicles%2520and%2520roadside%2520infrastructure%2520in%2520the%2520Hong%2520Kong%2520C-V2X%2520testbed%252C%2520designed%2520to%2520support%2520research%2520on%2520smart%2520mobility%2520applications%2520in%2520dense%2520urban%2520areas.%2520Our%2520onboard%2520platform%2520provides%2520synchronized%2520data%2520from%2520multiple%2520industrial%2520cameras%252C%2520LiDARs%252C%25204D%2520radar%252C%2520ultra-wideband%2520%2528UWB%2529%252C%2520IMU%252C%2520and%2520high-precision%2520GNSS-RTK/INS%2520navigation%2520systems.%2520Meanwhile%252C%2520our%2520roadside%2520infrastructure%2520provides%2520LiDAR%252C%2520GNSS%252C%2520and%2520UWB%2520measurements.%2520The%2520entire%2520vehicle-infrastructure%2520platform%2520is%2520synchronized%2520using%2520the%2520Precision%2520Time%2520Protocol%2520%2528PTP%2529%252C%2520with%2520sensor%2520calibration%2520data%2520provided.%2520We%2520also%2520benchmark%2520various%2520navigation%2520algorithms%2520to%2520evaluate%2520the%2520collected%2520cooperative%2520data.%2520The%2520dataset%2520is%2520publicly%2520available%2520at%2520https%253A//polyu-taslab.github.io/UrbanV2X/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrbanV2X%3A%20A%20Multisensory%20Vehicle-Infrastructure%20Dataset%20for%20Cooperative%20Navigation%20in%20Urban%20Areas&entry.906535625=Qijun%20Qin%20and%20Ziqi%20Zhang%20and%20Yihan%20Zhong%20and%20Feng%20Huang%20and%20Xikun%20Liu%20and%20Runzhi%20Hu%20and%20Hang%20Chen%20and%20Wei%20Hu%20and%20Dongzhe%20Su%20and%20Jun%20Zhang%20and%20Hoi-Fung%20Ng%20and%20Weisong%20Wen&entry.1292438233=Due%20to%20the%20limitations%20of%20a%20single%20autonomous%20vehicle%2C%20Cellular%20Vehicle-to-Everything%20%28C-V2X%29%20technology%20opens%20a%20new%20window%20for%20achieving%20fully%20autonomous%20driving%20through%20sensor%20information%20sharing.%20However%2C%20real-world%20datasets%20supporting%20vehicle-infrastructure%20cooperative%20navigation%20in%20complex%20urban%20environments%20remain%20rare.%20To%20address%20this%20gap%2C%20we%20present%20UrbanV2X%2C%20a%20comprehensive%20multisensory%20dataset%20collected%20from%20vehicles%20and%20roadside%20infrastructure%20in%20the%20Hong%20Kong%20C-V2X%20testbed%2C%20designed%20to%20support%20research%20on%20smart%20mobility%20applications%20in%20dense%20urban%20areas.%20Our%20onboard%20platform%20provides%20synchronized%20data%20from%20multiple%20industrial%20cameras%2C%20LiDARs%2C%204D%20radar%2C%20ultra-wideband%20%28UWB%29%2C%20IMU%2C%20and%20high-precision%20GNSS-RTK/INS%20navigation%20systems.%20Meanwhile%2C%20our%20roadside%20infrastructure%20provides%20LiDAR%2C%20GNSS%2C%20and%20UWB%20measurements.%20The%20entire%20vehicle-infrastructure%20platform%20is%20synchronized%20using%20the%20Precision%20Time%20Protocol%20%28PTP%29%2C%20with%20sensor%20calibration%20data%20provided.%20We%20also%20benchmark%20various%20navigation%20algorithms%20to%20evaluate%20the%20collected%20cooperative%20data.%20The%20dataset%20is%20publicly%20available%20at%20https%3A//polyu-taslab.github.io/UrbanV2X/.&entry.1838667208=http%3A//arxiv.org/abs/2512.20224v1&entry.124074799=Read"},
{"title": "Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI", "author": "Muhammad Usman and Azka Rehman and Muhammad Mutti Ur Rehman and Abd Ur Rehman and Muhammad Umar Farooq", "abstract": "Accurate segmentation of ischemic stroke lesions from diffusion magnetic resonance imaging (MRI) is essential for clinical decision-making and outcome assessment. Diffusion-Weighted Imaging (DWI) and Apparent Diffusion Coefficient (ADC) scans provide complementary information on acute and sub-acute ischemic changes; however, automated lesion delineation remains challenging due to variability in lesion appearance.\n  In this work, we study ischemic stroke lesion segmentation using multimodal diffusion MRI from the ISLES 2022 dataset. Several state-of-the-art convolutional and transformer-based architectures, including U-Net variants, Swin-UNet, and TransUNet, are benchmarked. Based on performance, a dual-encoder TransUNet architecture is proposed to learn modality-specific representations from DWI and ADC inputs. To incorporate spatial context, adjacent slice information is integrated using a three-slice input configuration.\n  All models are trained under a unified framework and evaluated using the Dice Similarity Coefficient (DSC). Results show that transformer-based models outperform convolutional baselines, and the proposed dual-encoder TransUNet achieves the best performance, reaching a Dice score of 85.4% on the test set. The proposed framework offers a robust solution for automated ischemic stroke lesion segmentation from diffusion MRI.", "link": "http://arxiv.org/abs/2512.20436v1", "date": "2025-12-23", "relevancy": 2.1534, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5435}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5399}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Encoder%20Transformer-Based%20Multimodal%20Learning%20for%20Ischemic%20Stroke%20Lesion%20Segmentation%20Using%20Diffusion%20MRI&body=Title%3A%20Dual-Encoder%20Transformer-Based%20Multimodal%20Learning%20for%20Ischemic%20Stroke%20Lesion%20Segmentation%20Using%20Diffusion%20MRI%0AAuthor%3A%20Muhammad%20Usman%20and%20Azka%20Rehman%20and%20Muhammad%20Mutti%20Ur%20Rehman%20and%20Abd%20Ur%20Rehman%20and%20Muhammad%20Umar%20Farooq%0AAbstract%3A%20Accurate%20segmentation%20of%20ischemic%20stroke%20lesions%20from%20diffusion%20magnetic%20resonance%20imaging%20%28MRI%29%20is%20essential%20for%20clinical%20decision-making%20and%20outcome%20assessment.%20Diffusion-Weighted%20Imaging%20%28DWI%29%20and%20Apparent%20Diffusion%20Coefficient%20%28ADC%29%20scans%20provide%20complementary%20information%20on%20acute%20and%20sub-acute%20ischemic%20changes%3B%20however%2C%20automated%20lesion%20delineation%20remains%20challenging%20due%20to%20variability%20in%20lesion%20appearance.%0A%20%20In%20this%20work%2C%20we%20study%20ischemic%20stroke%20lesion%20segmentation%20using%20multimodal%20diffusion%20MRI%20from%20the%20ISLES%202022%20dataset.%20Several%20state-of-the-art%20convolutional%20and%20transformer-based%20architectures%2C%20including%20U-Net%20variants%2C%20Swin-UNet%2C%20and%20TransUNet%2C%20are%20benchmarked.%20Based%20on%20performance%2C%20a%20dual-encoder%20TransUNet%20architecture%20is%20proposed%20to%20learn%20modality-specific%20representations%20from%20DWI%20and%20ADC%20inputs.%20To%20incorporate%20spatial%20context%2C%20adjacent%20slice%20information%20is%20integrated%20using%20a%20three-slice%20input%20configuration.%0A%20%20All%20models%20are%20trained%20under%20a%20unified%20framework%20and%20evaluated%20using%20the%20Dice%20Similarity%20Coefficient%20%28DSC%29.%20Results%20show%20that%20transformer-based%20models%20outperform%20convolutional%20baselines%2C%20and%20the%20proposed%20dual-encoder%20TransUNet%20achieves%20the%20best%20performance%2C%20reaching%20a%20Dice%20score%20of%2085.4%25%20on%20the%20test%20set.%20The%20proposed%20framework%20offers%20a%20robust%20solution%20for%20automated%20ischemic%20stroke%20lesion%20segmentation%20from%20diffusion%20MRI.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Encoder%2520Transformer-Based%2520Multimodal%2520Learning%2520for%2520Ischemic%2520Stroke%2520Lesion%2520Segmentation%2520Using%2520Diffusion%2520MRI%26entry.906535625%3DMuhammad%2520Usman%2520and%2520Azka%2520Rehman%2520and%2520Muhammad%2520Mutti%2520Ur%2520Rehman%2520and%2520Abd%2520Ur%2520Rehman%2520and%2520Muhammad%2520Umar%2520Farooq%26entry.1292438233%3DAccurate%2520segmentation%2520of%2520ischemic%2520stroke%2520lesions%2520from%2520diffusion%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520is%2520essential%2520for%2520clinical%2520decision-making%2520and%2520outcome%2520assessment.%2520Diffusion-Weighted%2520Imaging%2520%2528DWI%2529%2520and%2520Apparent%2520Diffusion%2520Coefficient%2520%2528ADC%2529%2520scans%2520provide%2520complementary%2520information%2520on%2520acute%2520and%2520sub-acute%2520ischemic%2520changes%253B%2520however%252C%2520automated%2520lesion%2520delineation%2520remains%2520challenging%2520due%2520to%2520variability%2520in%2520lesion%2520appearance.%250A%2520%2520In%2520this%2520work%252C%2520we%2520study%2520ischemic%2520stroke%2520lesion%2520segmentation%2520using%2520multimodal%2520diffusion%2520MRI%2520from%2520the%2520ISLES%25202022%2520dataset.%2520Several%2520state-of-the-art%2520convolutional%2520and%2520transformer-based%2520architectures%252C%2520including%2520U-Net%2520variants%252C%2520Swin-UNet%252C%2520and%2520TransUNet%252C%2520are%2520benchmarked.%2520Based%2520on%2520performance%252C%2520a%2520dual-encoder%2520TransUNet%2520architecture%2520is%2520proposed%2520to%2520learn%2520modality-specific%2520representations%2520from%2520DWI%2520and%2520ADC%2520inputs.%2520To%2520incorporate%2520spatial%2520context%252C%2520adjacent%2520slice%2520information%2520is%2520integrated%2520using%2520a%2520three-slice%2520input%2520configuration.%250A%2520%2520All%2520models%2520are%2520trained%2520under%2520a%2520unified%2520framework%2520and%2520evaluated%2520using%2520the%2520Dice%2520Similarity%2520Coefficient%2520%2528DSC%2529.%2520Results%2520show%2520that%2520transformer-based%2520models%2520outperform%2520convolutional%2520baselines%252C%2520and%2520the%2520proposed%2520dual-encoder%2520TransUNet%2520achieves%2520the%2520best%2520performance%252C%2520reaching%2520a%2520Dice%2520score%2520of%252085.4%2525%2520on%2520the%2520test%2520set.%2520The%2520proposed%2520framework%2520offers%2520a%2520robust%2520solution%2520for%2520automated%2520ischemic%2520stroke%2520lesion%2520segmentation%2520from%2520diffusion%2520MRI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Encoder%20Transformer-Based%20Multimodal%20Learning%20for%20Ischemic%20Stroke%20Lesion%20Segmentation%20Using%20Diffusion%20MRI&entry.906535625=Muhammad%20Usman%20and%20Azka%20Rehman%20and%20Muhammad%20Mutti%20Ur%20Rehman%20and%20Abd%20Ur%20Rehman%20and%20Muhammad%20Umar%20Farooq&entry.1292438233=Accurate%20segmentation%20of%20ischemic%20stroke%20lesions%20from%20diffusion%20magnetic%20resonance%20imaging%20%28MRI%29%20is%20essential%20for%20clinical%20decision-making%20and%20outcome%20assessment.%20Diffusion-Weighted%20Imaging%20%28DWI%29%20and%20Apparent%20Diffusion%20Coefficient%20%28ADC%29%20scans%20provide%20complementary%20information%20on%20acute%20and%20sub-acute%20ischemic%20changes%3B%20however%2C%20automated%20lesion%20delineation%20remains%20challenging%20due%20to%20variability%20in%20lesion%20appearance.%0A%20%20In%20this%20work%2C%20we%20study%20ischemic%20stroke%20lesion%20segmentation%20using%20multimodal%20diffusion%20MRI%20from%20the%20ISLES%202022%20dataset.%20Several%20state-of-the-art%20convolutional%20and%20transformer-based%20architectures%2C%20including%20U-Net%20variants%2C%20Swin-UNet%2C%20and%20TransUNet%2C%20are%20benchmarked.%20Based%20on%20performance%2C%20a%20dual-encoder%20TransUNet%20architecture%20is%20proposed%20to%20learn%20modality-specific%20representations%20from%20DWI%20and%20ADC%20inputs.%20To%20incorporate%20spatial%20context%2C%20adjacent%20slice%20information%20is%20integrated%20using%20a%20three-slice%20input%20configuration.%0A%20%20All%20models%20are%20trained%20under%20a%20unified%20framework%20and%20evaluated%20using%20the%20Dice%20Similarity%20Coefficient%20%28DSC%29.%20Results%20show%20that%20transformer-based%20models%20outperform%20convolutional%20baselines%2C%20and%20the%20proposed%20dual-encoder%20TransUNet%20achieves%20the%20best%20performance%2C%20reaching%20a%20Dice%20score%20of%2085.4%25%20on%20the%20test%20set.%20The%20proposed%20framework%20offers%20a%20robust%20solution%20for%20automated%20ischemic%20stroke%20lesion%20segmentation%20from%20diffusion%20MRI.&entry.1838667208=http%3A//arxiv.org/abs/2512.20436v1&entry.124074799=Read"},
{"title": "GeoTransolver: Learning Physics on Irregumar Domains Using Multi-scale Geometry Aware Physics Attention Transformer", "author": "Corey Adams and Rishikesh Ranade and Ram Cherukuri and Sanjay Choudhry", "abstract": "We present GeoTransolver, a Multiscale Geometry-Aware Physics Attention Transformer for CAE that replaces standard attention with GALE, coupling physics-aware self-attention on learned state slices with cross-attention to a shared geometry/global/boundary-condition context computed from multi-scale ball queries (inspired by DoMINO) and reused in every block. Implemented and released in NVIDIA PhysicsNeMo, GeoTransolver persistently projects geometry, global and boundary condition parameters into physical state spaces to anchor latent computations to domain structure and operating regimes. We benchmark GeoTransolver on DrivAerML, Luminary SHIFT-SUV, and Luminary SHIFT-Wing, comparing against Domino, Transolver (as released in PhysicsNeMo), and literature-reported AB-UPT, and evaluate drag/lift R2 and Relative L1 errors for field variables. GeoTransolver delivers better accuracy, improved robustness to geometry/regime shifts, and favorable data efficiency; we include ablations on DrivAerML and qualitative results such as contour plots and design trends for the best GeoTransolver models. By unifying multiscale geometry-aware context with physics-based attention in a scalable transformer, GeoTransolver advances operator learning for high-fidelity surrogate modeling across complex, irregular domains and non-linear physical regimes.", "link": "http://arxiv.org/abs/2512.20399v1", "date": "2025-12-23", "relevancy": 2.1426, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5368}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5366}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoTransolver%3A%20Learning%20Physics%20on%20Irregumar%20Domains%20Using%20Multi-scale%20Geometry%20Aware%20Physics%20Attention%20Transformer&body=Title%3A%20GeoTransolver%3A%20Learning%20Physics%20on%20Irregumar%20Domains%20Using%20Multi-scale%20Geometry%20Aware%20Physics%20Attention%20Transformer%0AAuthor%3A%20Corey%20Adams%20and%20Rishikesh%20Ranade%20and%20Ram%20Cherukuri%20and%20Sanjay%20Choudhry%0AAbstract%3A%20We%20present%20GeoTransolver%2C%20a%20Multiscale%20Geometry-Aware%20Physics%20Attention%20Transformer%20for%20CAE%20that%20replaces%20standard%20attention%20with%20GALE%2C%20coupling%20physics-aware%20self-attention%20on%20learned%20state%20slices%20with%20cross-attention%20to%20a%20shared%20geometry/global/boundary-condition%20context%20computed%20from%20multi-scale%20ball%20queries%20%28inspired%20by%20DoMINO%29%20and%20reused%20in%20every%20block.%20Implemented%20and%20released%20in%20NVIDIA%20PhysicsNeMo%2C%20GeoTransolver%20persistently%20projects%20geometry%2C%20global%20and%20boundary%20condition%20parameters%20into%20physical%20state%20spaces%20to%20anchor%20latent%20computations%20to%20domain%20structure%20and%20operating%20regimes.%20We%20benchmark%20GeoTransolver%20on%20DrivAerML%2C%20Luminary%20SHIFT-SUV%2C%20and%20Luminary%20SHIFT-Wing%2C%20comparing%20against%20Domino%2C%20Transolver%20%28as%20released%20in%20PhysicsNeMo%29%2C%20and%20literature-reported%20AB-UPT%2C%20and%20evaluate%20drag/lift%20R2%20and%20Relative%20L1%20errors%20for%20field%20variables.%20GeoTransolver%20delivers%20better%20accuracy%2C%20improved%20robustness%20to%20geometry/regime%20shifts%2C%20and%20favorable%20data%20efficiency%3B%20we%20include%20ablations%20on%20DrivAerML%20and%20qualitative%20results%20such%20as%20contour%20plots%20and%20design%20trends%20for%20the%20best%20GeoTransolver%20models.%20By%20unifying%20multiscale%20geometry-aware%20context%20with%20physics-based%20attention%20in%20a%20scalable%20transformer%2C%20GeoTransolver%20advances%20operator%20learning%20for%20high-fidelity%20surrogate%20modeling%20across%20complex%2C%20irregular%20domains%20and%20non-linear%20physical%20regimes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoTransolver%253A%2520Learning%2520Physics%2520on%2520Irregumar%2520Domains%2520Using%2520Multi-scale%2520Geometry%2520Aware%2520Physics%2520Attention%2520Transformer%26entry.906535625%3DCorey%2520Adams%2520and%2520Rishikesh%2520Ranade%2520and%2520Ram%2520Cherukuri%2520and%2520Sanjay%2520Choudhry%26entry.1292438233%3DWe%2520present%2520GeoTransolver%252C%2520a%2520Multiscale%2520Geometry-Aware%2520Physics%2520Attention%2520Transformer%2520for%2520CAE%2520that%2520replaces%2520standard%2520attention%2520with%2520GALE%252C%2520coupling%2520physics-aware%2520self-attention%2520on%2520learned%2520state%2520slices%2520with%2520cross-attention%2520to%2520a%2520shared%2520geometry/global/boundary-condition%2520context%2520computed%2520from%2520multi-scale%2520ball%2520queries%2520%2528inspired%2520by%2520DoMINO%2529%2520and%2520reused%2520in%2520every%2520block.%2520Implemented%2520and%2520released%2520in%2520NVIDIA%2520PhysicsNeMo%252C%2520GeoTransolver%2520persistently%2520projects%2520geometry%252C%2520global%2520and%2520boundary%2520condition%2520parameters%2520into%2520physical%2520state%2520spaces%2520to%2520anchor%2520latent%2520computations%2520to%2520domain%2520structure%2520and%2520operating%2520regimes.%2520We%2520benchmark%2520GeoTransolver%2520on%2520DrivAerML%252C%2520Luminary%2520SHIFT-SUV%252C%2520and%2520Luminary%2520SHIFT-Wing%252C%2520comparing%2520against%2520Domino%252C%2520Transolver%2520%2528as%2520released%2520in%2520PhysicsNeMo%2529%252C%2520and%2520literature-reported%2520AB-UPT%252C%2520and%2520evaluate%2520drag/lift%2520R2%2520and%2520Relative%2520L1%2520errors%2520for%2520field%2520variables.%2520GeoTransolver%2520delivers%2520better%2520accuracy%252C%2520improved%2520robustness%2520to%2520geometry/regime%2520shifts%252C%2520and%2520favorable%2520data%2520efficiency%253B%2520we%2520include%2520ablations%2520on%2520DrivAerML%2520and%2520qualitative%2520results%2520such%2520as%2520contour%2520plots%2520and%2520design%2520trends%2520for%2520the%2520best%2520GeoTransolver%2520models.%2520By%2520unifying%2520multiscale%2520geometry-aware%2520context%2520with%2520physics-based%2520attention%2520in%2520a%2520scalable%2520transformer%252C%2520GeoTransolver%2520advances%2520operator%2520learning%2520for%2520high-fidelity%2520surrogate%2520modeling%2520across%2520complex%252C%2520irregular%2520domains%2520and%2520non-linear%2520physical%2520regimes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoTransolver%3A%20Learning%20Physics%20on%20Irregumar%20Domains%20Using%20Multi-scale%20Geometry%20Aware%20Physics%20Attention%20Transformer&entry.906535625=Corey%20Adams%20and%20Rishikesh%20Ranade%20and%20Ram%20Cherukuri%20and%20Sanjay%20Choudhry&entry.1292438233=We%20present%20GeoTransolver%2C%20a%20Multiscale%20Geometry-Aware%20Physics%20Attention%20Transformer%20for%20CAE%20that%20replaces%20standard%20attention%20with%20GALE%2C%20coupling%20physics-aware%20self-attention%20on%20learned%20state%20slices%20with%20cross-attention%20to%20a%20shared%20geometry/global/boundary-condition%20context%20computed%20from%20multi-scale%20ball%20queries%20%28inspired%20by%20DoMINO%29%20and%20reused%20in%20every%20block.%20Implemented%20and%20released%20in%20NVIDIA%20PhysicsNeMo%2C%20GeoTransolver%20persistently%20projects%20geometry%2C%20global%20and%20boundary%20condition%20parameters%20into%20physical%20state%20spaces%20to%20anchor%20latent%20computations%20to%20domain%20structure%20and%20operating%20regimes.%20We%20benchmark%20GeoTransolver%20on%20DrivAerML%2C%20Luminary%20SHIFT-SUV%2C%20and%20Luminary%20SHIFT-Wing%2C%20comparing%20against%20Domino%2C%20Transolver%20%28as%20released%20in%20PhysicsNeMo%29%2C%20and%20literature-reported%20AB-UPT%2C%20and%20evaluate%20drag/lift%20R2%20and%20Relative%20L1%20errors%20for%20field%20variables.%20GeoTransolver%20delivers%20better%20accuracy%2C%20improved%20robustness%20to%20geometry/regime%20shifts%2C%20and%20favorable%20data%20efficiency%3B%20we%20include%20ablations%20on%20DrivAerML%20and%20qualitative%20results%20such%20as%20contour%20plots%20and%20design%20trends%20for%20the%20best%20GeoTransolver%20models.%20By%20unifying%20multiscale%20geometry-aware%20context%20with%20physics-based%20attention%20in%20a%20scalable%20transformer%2C%20GeoTransolver%20advances%20operator%20learning%20for%20high-fidelity%20surrogate%20modeling%20across%20complex%2C%20irregular%20domains%20and%20non-linear%20physical%20regimes.&entry.1838667208=http%3A//arxiv.org/abs/2512.20399v1&entry.124074799=Read"},
{"title": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System", "author": "Zhongyu Xia and Wenhao Chen and Yongtao Wang and Ming-Hsuan Yang", "abstract": "Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.", "link": "http://arxiv.org/abs/2512.20299v1", "date": "2025-12-23", "relevancy": 2.1408, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5543}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5451}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KnowVal%3A%20A%20Knowledge-Augmented%20and%20Value-Guided%20Autonomous%20Driving%20System&body=Title%3A%20KnowVal%3A%20A%20Knowledge-Augmented%20and%20Value-Guided%20Autonomous%20Driving%20System%0AAuthor%3A%20Zhongyu%20Xia%20and%20Wenhao%20Chen%20and%20Yongtao%20Wang%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20Visual-language%20reasoning%2C%20driving%20knowledge%2C%20and%20value%20alignment%20are%20essential%20for%20advanced%20autonomous%20driving%20systems.%20However%2C%20existing%20approaches%20largely%20rely%20on%20data-driven%20learning%2C%20making%20it%20difficult%20to%20capture%20the%20complex%20logic%20underlying%20decision-making%20through%20imitation%20or%20limited%20reinforcement%20rewards.%20To%20address%20this%2C%20we%20propose%20KnowVal%2C%20a%20new%20autonomous%20driving%20system%20that%20enables%20visual-language%20reasoning%20through%20the%20synergistic%20integration%20of%20open-world%20perception%20and%20knowledge%20retrieval.%20Specifically%2C%20we%20construct%20a%20comprehensive%20driving%20knowledge%20graph%20that%20encodes%20traffic%20laws%2C%20defensive%20driving%20principles%2C%20and%20ethical%20norms%2C%20complemented%20by%20an%20efficient%20LLM-based%20retrieval%20mechanism%20tailored%20for%20driving%20scenarios.%20Furthermore%2C%20we%20develop%20a%20human-preference%20dataset%20and%20train%20a%20Value%20Model%20to%20guide%20interpretable%2C%20value-aligned%20trajectory%20assessment.%20Experimental%20results%20show%20that%20our%20method%20substantially%20improves%20planning%20performance%20while%20remaining%20compatible%20with%20existing%20architectures.%20Notably%2C%20KnowVal%20achieves%20the%20lowest%20collision%20rate%20on%20nuScenes%20and%20state-of-the-art%20results%20on%20Bench2Drive.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowVal%253A%2520A%2520Knowledge-Augmented%2520and%2520Value-Guided%2520Autonomous%2520Driving%2520System%26entry.906535625%3DZhongyu%2520Xia%2520and%2520Wenhao%2520Chen%2520and%2520Yongtao%2520Wang%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3DVisual-language%2520reasoning%252C%2520driving%2520knowledge%252C%2520and%2520value%2520alignment%2520are%2520essential%2520for%2520advanced%2520autonomous%2520driving%2520systems.%2520However%252C%2520existing%2520approaches%2520largely%2520rely%2520on%2520data-driven%2520learning%252C%2520making%2520it%2520difficult%2520to%2520capture%2520the%2520complex%2520logic%2520underlying%2520decision-making%2520through%2520imitation%2520or%2520limited%2520reinforcement%2520rewards.%2520To%2520address%2520this%252C%2520we%2520propose%2520KnowVal%252C%2520a%2520new%2520autonomous%2520driving%2520system%2520that%2520enables%2520visual-language%2520reasoning%2520through%2520the%2520synergistic%2520integration%2520of%2520open-world%2520perception%2520and%2520knowledge%2520retrieval.%2520Specifically%252C%2520we%2520construct%2520a%2520comprehensive%2520driving%2520knowledge%2520graph%2520that%2520encodes%2520traffic%2520laws%252C%2520defensive%2520driving%2520principles%252C%2520and%2520ethical%2520norms%252C%2520complemented%2520by%2520an%2520efficient%2520LLM-based%2520retrieval%2520mechanism%2520tailored%2520for%2520driving%2520scenarios.%2520Furthermore%252C%2520we%2520develop%2520a%2520human-preference%2520dataset%2520and%2520train%2520a%2520Value%2520Model%2520to%2520guide%2520interpretable%252C%2520value-aligned%2520trajectory%2520assessment.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520substantially%2520improves%2520planning%2520performance%2520while%2520remaining%2520compatible%2520with%2520existing%2520architectures.%2520Notably%252C%2520KnowVal%2520achieves%2520the%2520lowest%2520collision%2520rate%2520on%2520nuScenes%2520and%2520state-of-the-art%2520results%2520on%2520Bench2Drive.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KnowVal%3A%20A%20Knowledge-Augmented%20and%20Value-Guided%20Autonomous%20Driving%20System&entry.906535625=Zhongyu%20Xia%20and%20Wenhao%20Chen%20and%20Yongtao%20Wang%20and%20Ming-Hsuan%20Yang&entry.1292438233=Visual-language%20reasoning%2C%20driving%20knowledge%2C%20and%20value%20alignment%20are%20essential%20for%20advanced%20autonomous%20driving%20systems.%20However%2C%20existing%20approaches%20largely%20rely%20on%20data-driven%20learning%2C%20making%20it%20difficult%20to%20capture%20the%20complex%20logic%20underlying%20decision-making%20through%20imitation%20or%20limited%20reinforcement%20rewards.%20To%20address%20this%2C%20we%20propose%20KnowVal%2C%20a%20new%20autonomous%20driving%20system%20that%20enables%20visual-language%20reasoning%20through%20the%20synergistic%20integration%20of%20open-world%20perception%20and%20knowledge%20retrieval.%20Specifically%2C%20we%20construct%20a%20comprehensive%20driving%20knowledge%20graph%20that%20encodes%20traffic%20laws%2C%20defensive%20driving%20principles%2C%20and%20ethical%20norms%2C%20complemented%20by%20an%20efficient%20LLM-based%20retrieval%20mechanism%20tailored%20for%20driving%20scenarios.%20Furthermore%2C%20we%20develop%20a%20human-preference%20dataset%20and%20train%20a%20Value%20Model%20to%20guide%20interpretable%2C%20value-aligned%20trajectory%20assessment.%20Experimental%20results%20show%20that%20our%20method%20substantially%20improves%20planning%20performance%20while%20remaining%20compatible%20with%20existing%20architectures.%20Notably%2C%20KnowVal%20achieves%20the%20lowest%20collision%20rate%20on%20nuScenes%20and%20state-of-the-art%20results%20on%20Bench2Drive.&entry.1838667208=http%3A//arxiv.org/abs/2512.20299v1&entry.124074799=Read"},
{"title": "Binarization-Aware Adjuster for Discrete Decision Learning with an Application to Edge Detection", "author": "Hao Shu", "abstract": "Discrete decision tasks in machine learning exhibit a fundamental misalignment between training and inference: models are optimized with continuous-valued outputs but evaluated using discrete predictions. This misalignment arises from the discontinuity of discretization operations, which prevents decision behavior from being directly incorporated into gradient-based optimization. To address this issue, we propose a theoretically grounded framework termed the Binarization-Aware Adjuster (BAA), which embeds binarization characteristics into continuous optimization. The framework is built upon the Distance Weight Function (DWF), which modulates loss contributions according to prediction correctness and proximity to the decision threshold, thereby aligning optimization emphasis with decision-critical regions while remaining compatible with standard learning pipelines. We apply the proposed BAA framework to the edge detection (ED) task, a representative binary decision problem. Experimental results on representative models and datasets show that incorporating BAA into optimization leads to consistent performance improvements, supporting its effectiveness. Overall, this work establishes a principled approach for aligning continuous optimization with discrete decision behavior, with its effectiveness demonstrated in a concrete application setting.", "link": "http://arxiv.org/abs/2506.12460v3", "date": "2025-12-23", "relevancy": 2.1353, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5577}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5178}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Binarization-Aware%20Adjuster%20for%20Discrete%20Decision%20Learning%20with%20an%20Application%20to%20Edge%20Detection&body=Title%3A%20Binarization-Aware%20Adjuster%20for%20Discrete%20Decision%20Learning%20with%20an%20Application%20to%20Edge%20Detection%0AAuthor%3A%20Hao%20Shu%0AAbstract%3A%20Discrete%20decision%20tasks%20in%20machine%20learning%20exhibit%20a%20fundamental%20misalignment%20between%20training%20and%20inference%3A%20models%20are%20optimized%20with%20continuous-valued%20outputs%20but%20evaluated%20using%20discrete%20predictions.%20This%20misalignment%20arises%20from%20the%20discontinuity%20of%20discretization%20operations%2C%20which%20prevents%20decision%20behavior%20from%20being%20directly%20incorporated%20into%20gradient-based%20optimization.%20To%20address%20this%20issue%2C%20we%20propose%20a%20theoretically%20grounded%20framework%20termed%20the%20Binarization-Aware%20Adjuster%20%28BAA%29%2C%20which%20embeds%20binarization%20characteristics%20into%20continuous%20optimization.%20The%20framework%20is%20built%20upon%20the%20Distance%20Weight%20Function%20%28DWF%29%2C%20which%20modulates%20loss%20contributions%20according%20to%20prediction%20correctness%20and%20proximity%20to%20the%20decision%20threshold%2C%20thereby%20aligning%20optimization%20emphasis%20with%20decision-critical%20regions%20while%20remaining%20compatible%20with%20standard%20learning%20pipelines.%20We%20apply%20the%20proposed%20BAA%20framework%20to%20the%20edge%20detection%20%28ED%29%20task%2C%20a%20representative%20binary%20decision%20problem.%20Experimental%20results%20on%20representative%20models%20and%20datasets%20show%20that%20incorporating%20BAA%20into%20optimization%20leads%20to%20consistent%20performance%20improvements%2C%20supporting%20its%20effectiveness.%20Overall%2C%20this%20work%20establishes%20a%20principled%20approach%20for%20aligning%20continuous%20optimization%20with%20discrete%20decision%20behavior%2C%20with%20its%20effectiveness%20demonstrated%20in%20a%20concrete%20application%20setting.%0ALink%3A%20http%3A//arxiv.org/abs/2506.12460v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBinarization-Aware%2520Adjuster%2520for%2520Discrete%2520Decision%2520Learning%2520with%2520an%2520Application%2520to%2520Edge%2520Detection%26entry.906535625%3DHao%2520Shu%26entry.1292438233%3DDiscrete%2520decision%2520tasks%2520in%2520machine%2520learning%2520exhibit%2520a%2520fundamental%2520misalignment%2520between%2520training%2520and%2520inference%253A%2520models%2520are%2520optimized%2520with%2520continuous-valued%2520outputs%2520but%2520evaluated%2520using%2520discrete%2520predictions.%2520This%2520misalignment%2520arises%2520from%2520the%2520discontinuity%2520of%2520discretization%2520operations%252C%2520which%2520prevents%2520decision%2520behavior%2520from%2520being%2520directly%2520incorporated%2520into%2520gradient-based%2520optimization.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520theoretically%2520grounded%2520framework%2520termed%2520the%2520Binarization-Aware%2520Adjuster%2520%2528BAA%2529%252C%2520which%2520embeds%2520binarization%2520characteristics%2520into%2520continuous%2520optimization.%2520The%2520framework%2520is%2520built%2520upon%2520the%2520Distance%2520Weight%2520Function%2520%2528DWF%2529%252C%2520which%2520modulates%2520loss%2520contributions%2520according%2520to%2520prediction%2520correctness%2520and%2520proximity%2520to%2520the%2520decision%2520threshold%252C%2520thereby%2520aligning%2520optimization%2520emphasis%2520with%2520decision-critical%2520regions%2520while%2520remaining%2520compatible%2520with%2520standard%2520learning%2520pipelines.%2520We%2520apply%2520the%2520proposed%2520BAA%2520framework%2520to%2520the%2520edge%2520detection%2520%2528ED%2529%2520task%252C%2520a%2520representative%2520binary%2520decision%2520problem.%2520Experimental%2520results%2520on%2520representative%2520models%2520and%2520datasets%2520show%2520that%2520incorporating%2520BAA%2520into%2520optimization%2520leads%2520to%2520consistent%2520performance%2520improvements%252C%2520supporting%2520its%2520effectiveness.%2520Overall%252C%2520this%2520work%2520establishes%2520a%2520principled%2520approach%2520for%2520aligning%2520continuous%2520optimization%2520with%2520discrete%2520decision%2520behavior%252C%2520with%2520its%2520effectiveness%2520demonstrated%2520in%2520a%2520concrete%2520application%2520setting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12460v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Binarization-Aware%20Adjuster%20for%20Discrete%20Decision%20Learning%20with%20an%20Application%20to%20Edge%20Detection&entry.906535625=Hao%20Shu&entry.1292438233=Discrete%20decision%20tasks%20in%20machine%20learning%20exhibit%20a%20fundamental%20misalignment%20between%20training%20and%20inference%3A%20models%20are%20optimized%20with%20continuous-valued%20outputs%20but%20evaluated%20using%20discrete%20predictions.%20This%20misalignment%20arises%20from%20the%20discontinuity%20of%20discretization%20operations%2C%20which%20prevents%20decision%20behavior%20from%20being%20directly%20incorporated%20into%20gradient-based%20optimization.%20To%20address%20this%20issue%2C%20we%20propose%20a%20theoretically%20grounded%20framework%20termed%20the%20Binarization-Aware%20Adjuster%20%28BAA%29%2C%20which%20embeds%20binarization%20characteristics%20into%20continuous%20optimization.%20The%20framework%20is%20built%20upon%20the%20Distance%20Weight%20Function%20%28DWF%29%2C%20which%20modulates%20loss%20contributions%20according%20to%20prediction%20correctness%20and%20proximity%20to%20the%20decision%20threshold%2C%20thereby%20aligning%20optimization%20emphasis%20with%20decision-critical%20regions%20while%20remaining%20compatible%20with%20standard%20learning%20pipelines.%20We%20apply%20the%20proposed%20BAA%20framework%20to%20the%20edge%20detection%20%28ED%29%20task%2C%20a%20representative%20binary%20decision%20problem.%20Experimental%20results%20on%20representative%20models%20and%20datasets%20show%20that%20incorporating%20BAA%20into%20optimization%20leads%20to%20consistent%20performance%20improvements%2C%20supporting%20its%20effectiveness.%20Overall%2C%20this%20work%20establishes%20a%20principled%20approach%20for%20aligning%20continuous%20optimization%20with%20discrete%20decision%20behavior%2C%20with%20its%20effectiveness%20demonstrated%20in%20a%20concrete%20application%20setting.&entry.1838667208=http%3A//arxiv.org/abs/2506.12460v3&entry.124074799=Read"},
{"title": "Field-Space Attention for Structure-Preserving Earth System Transformers", "author": "Maximilian Witte and Johannes Meuer and \u00c9tienne Pl\u00e9siat and Christopher Kadow", "abstract": "Accurate and physically consistent modeling of Earth system dynamics requires machine-learning architectures that operate directly on continuous geophysical fields and preserve their underlying geometric structure. Here we introduce Field-Space attention, a mechanism for Earth system Transformers that computes attention in the physical domain rather than in a learned latent space. By maintaining all intermediate representations as continuous fields on the sphere, the architecture enables interpretable internal states and facilitates the enforcement of scientific constraints. The model employs a fixed, non-learned multiscale decomposition and learns structure-preserving deformations of the input field, allowing coherent integration of coarse and fine-scale information while avoiding the optimization instabilities characteristic of standard single-scale Vision Transformers. Applied to global temperature super-resolution on a HEALPix grid, Field-Space Transformers converge more rapidly and stably than conventional Vision Transformers and U-Net baselines, while requiring substantially fewer parameters. The explicit preservation of field structure throughout the network allows physical and statistical priors to be embedded directly into the architecture, yielding improved fidelity and reliability in data-driven Earth system modeling. These results position Field-Space Attention as a compact, interpretable, and physically grounded building block for next-generation Earth system prediction and generative modeling frameworks.", "link": "http://arxiv.org/abs/2512.20350v1", "date": "2025-12-23", "relevancy": 2.1343, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5708}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5371}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Field-Space%20Attention%20for%20Structure-Preserving%20Earth%20System%20Transformers&body=Title%3A%20Field-Space%20Attention%20for%20Structure-Preserving%20Earth%20System%20Transformers%0AAuthor%3A%20Maximilian%20Witte%20and%20Johannes%20Meuer%20and%20%C3%89tienne%20Pl%C3%A9siat%20and%20Christopher%20Kadow%0AAbstract%3A%20Accurate%20and%20physically%20consistent%20modeling%20of%20Earth%20system%20dynamics%20requires%20machine-learning%20architectures%20that%20operate%20directly%20on%20continuous%20geophysical%20fields%20and%20preserve%20their%20underlying%20geometric%20structure.%20Here%20we%20introduce%20Field-Space%20attention%2C%20a%20mechanism%20for%20Earth%20system%20Transformers%20that%20computes%20attention%20in%20the%20physical%20domain%20rather%20than%20in%20a%20learned%20latent%20space.%20By%20maintaining%20all%20intermediate%20representations%20as%20continuous%20fields%20on%20the%20sphere%2C%20the%20architecture%20enables%20interpretable%20internal%20states%20and%20facilitates%20the%20enforcement%20of%20scientific%20constraints.%20The%20model%20employs%20a%20fixed%2C%20non-learned%20multiscale%20decomposition%20and%20learns%20structure-preserving%20deformations%20of%20the%20input%20field%2C%20allowing%20coherent%20integration%20of%20coarse%20and%20fine-scale%20information%20while%20avoiding%20the%20optimization%20instabilities%20characteristic%20of%20standard%20single-scale%20Vision%20Transformers.%20Applied%20to%20global%20temperature%20super-resolution%20on%20a%20HEALPix%20grid%2C%20Field-Space%20Transformers%20converge%20more%20rapidly%20and%20stably%20than%20conventional%20Vision%20Transformers%20and%20U-Net%20baselines%2C%20while%20requiring%20substantially%20fewer%20parameters.%20The%20explicit%20preservation%20of%20field%20structure%20throughout%20the%20network%20allows%20physical%20and%20statistical%20priors%20to%20be%20embedded%20directly%20into%20the%20architecture%2C%20yielding%20improved%20fidelity%20and%20reliability%20in%20data-driven%20Earth%20system%20modeling.%20These%20results%20position%20Field-Space%20Attention%20as%20a%20compact%2C%20interpretable%2C%20and%20physically%20grounded%20building%20block%20for%20next-generation%20Earth%20system%20prediction%20and%20generative%20modeling%20frameworks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DField-Space%2520Attention%2520for%2520Structure-Preserving%2520Earth%2520System%2520Transformers%26entry.906535625%3DMaximilian%2520Witte%2520and%2520Johannes%2520Meuer%2520and%2520%25C3%2589tienne%2520Pl%25C3%25A9siat%2520and%2520Christopher%2520Kadow%26entry.1292438233%3DAccurate%2520and%2520physically%2520consistent%2520modeling%2520of%2520Earth%2520system%2520dynamics%2520requires%2520machine-learning%2520architectures%2520that%2520operate%2520directly%2520on%2520continuous%2520geophysical%2520fields%2520and%2520preserve%2520their%2520underlying%2520geometric%2520structure.%2520Here%2520we%2520introduce%2520Field-Space%2520attention%252C%2520a%2520mechanism%2520for%2520Earth%2520system%2520Transformers%2520that%2520computes%2520attention%2520in%2520the%2520physical%2520domain%2520rather%2520than%2520in%2520a%2520learned%2520latent%2520space.%2520By%2520maintaining%2520all%2520intermediate%2520representations%2520as%2520continuous%2520fields%2520on%2520the%2520sphere%252C%2520the%2520architecture%2520enables%2520interpretable%2520internal%2520states%2520and%2520facilitates%2520the%2520enforcement%2520of%2520scientific%2520constraints.%2520The%2520model%2520employs%2520a%2520fixed%252C%2520non-learned%2520multiscale%2520decomposition%2520and%2520learns%2520structure-preserving%2520deformations%2520of%2520the%2520input%2520field%252C%2520allowing%2520coherent%2520integration%2520of%2520coarse%2520and%2520fine-scale%2520information%2520while%2520avoiding%2520the%2520optimization%2520instabilities%2520characteristic%2520of%2520standard%2520single-scale%2520Vision%2520Transformers.%2520Applied%2520to%2520global%2520temperature%2520super-resolution%2520on%2520a%2520HEALPix%2520grid%252C%2520Field-Space%2520Transformers%2520converge%2520more%2520rapidly%2520and%2520stably%2520than%2520conventional%2520Vision%2520Transformers%2520and%2520U-Net%2520baselines%252C%2520while%2520requiring%2520substantially%2520fewer%2520parameters.%2520The%2520explicit%2520preservation%2520of%2520field%2520structure%2520throughout%2520the%2520network%2520allows%2520physical%2520and%2520statistical%2520priors%2520to%2520be%2520embedded%2520directly%2520into%2520the%2520architecture%252C%2520yielding%2520improved%2520fidelity%2520and%2520reliability%2520in%2520data-driven%2520Earth%2520system%2520modeling.%2520These%2520results%2520position%2520Field-Space%2520Attention%2520as%2520a%2520compact%252C%2520interpretable%252C%2520and%2520physically%2520grounded%2520building%2520block%2520for%2520next-generation%2520Earth%2520system%2520prediction%2520and%2520generative%2520modeling%2520frameworks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Field-Space%20Attention%20for%20Structure-Preserving%20Earth%20System%20Transformers&entry.906535625=Maximilian%20Witte%20and%20Johannes%20Meuer%20and%20%C3%89tienne%20Pl%C3%A9siat%20and%20Christopher%20Kadow&entry.1292438233=Accurate%20and%20physically%20consistent%20modeling%20of%20Earth%20system%20dynamics%20requires%20machine-learning%20architectures%20that%20operate%20directly%20on%20continuous%20geophysical%20fields%20and%20preserve%20their%20underlying%20geometric%20structure.%20Here%20we%20introduce%20Field-Space%20attention%2C%20a%20mechanism%20for%20Earth%20system%20Transformers%20that%20computes%20attention%20in%20the%20physical%20domain%20rather%20than%20in%20a%20learned%20latent%20space.%20By%20maintaining%20all%20intermediate%20representations%20as%20continuous%20fields%20on%20the%20sphere%2C%20the%20architecture%20enables%20interpretable%20internal%20states%20and%20facilitates%20the%20enforcement%20of%20scientific%20constraints.%20The%20model%20employs%20a%20fixed%2C%20non-learned%20multiscale%20decomposition%20and%20learns%20structure-preserving%20deformations%20of%20the%20input%20field%2C%20allowing%20coherent%20integration%20of%20coarse%20and%20fine-scale%20information%20while%20avoiding%20the%20optimization%20instabilities%20characteristic%20of%20standard%20single-scale%20Vision%20Transformers.%20Applied%20to%20global%20temperature%20super-resolution%20on%20a%20HEALPix%20grid%2C%20Field-Space%20Transformers%20converge%20more%20rapidly%20and%20stably%20than%20conventional%20Vision%20Transformers%20and%20U-Net%20baselines%2C%20while%20requiring%20substantially%20fewer%20parameters.%20The%20explicit%20preservation%20of%20field%20structure%20throughout%20the%20network%20allows%20physical%20and%20statistical%20priors%20to%20be%20embedded%20directly%20into%20the%20architecture%2C%20yielding%20improved%20fidelity%20and%20reliability%20in%20data-driven%20Earth%20system%20modeling.%20These%20results%20position%20Field-Space%20Attention%20as%20a%20compact%2C%20interpretable%2C%20and%20physically%20grounded%20building%20block%20for%20next-generation%20Earth%20system%20prediction%20and%20generative%20modeling%20frameworks.&entry.1838667208=http%3A//arxiv.org/abs/2512.20350v1&entry.124074799=Read"},
{"title": "C3RL: Rethinking the Combination of Channel-independence and Channel-mixing from Representation Learning", "author": "Shusen Ma and Yun-Bo Zhao and Yu Kang", "abstract": "Multivariate time series forecasting has drawn increasing attention due to its practical importance. Existing approaches typically adopt either channel-mixing (CM) or channel-independence (CI) strategies. CM strategy can capture inter-variable dependencies but fails to discern variable-specific temporal patterns. CI strategy improves this aspect but fails to fully exploit cross-variable dependencies like CM. Hybrid strategies based on feature fusion offer limited generalization and interpretability. To address these issues, we propose C3RL, a novel representation learning framework that jointly models both CM and CI strategies. Motivated by contrastive learning in computer vision, C3RL treats the inputs of the two strategies as transposed views and builds a siamese network architecture: one strategy serves as the backbone, while the other complements it. By jointly optimizing contrastive and prediction losses with adaptive weighting, C3RL balances representation and forecasting performance. Extensive experiments on seven models show that C3RL boosts the best-case performance rate to 81.4% for models based on CI strategy and to 76.3% for models based on CM strategy, demonstrating strong generalization and effectiveness.", "link": "http://arxiv.org/abs/2507.17454v2", "date": "2025-12-23", "relevancy": 2.1324, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5573}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5194}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C3RL%3A%20Rethinking%20the%20Combination%20of%20Channel-independence%20and%20Channel-mixing%20from%20Representation%20Learning&body=Title%3A%20C3RL%3A%20Rethinking%20the%20Combination%20of%20Channel-independence%20and%20Channel-mixing%20from%20Representation%20Learning%0AAuthor%3A%20Shusen%20Ma%20and%20Yun-Bo%20Zhao%20and%20Yu%20Kang%0AAbstract%3A%20Multivariate%20time%20series%20forecasting%20has%20drawn%20increasing%20attention%20due%20to%20its%20practical%20importance.%20Existing%20approaches%20typically%20adopt%20either%20channel-mixing%20%28CM%29%20or%20channel-independence%20%28CI%29%20strategies.%20CM%20strategy%20can%20capture%20inter-variable%20dependencies%20but%20fails%20to%20discern%20variable-specific%20temporal%20patterns.%20CI%20strategy%20improves%20this%20aspect%20but%20fails%20to%20fully%20exploit%20cross-variable%20dependencies%20like%20CM.%20Hybrid%20strategies%20based%20on%20feature%20fusion%20offer%20limited%20generalization%20and%20interpretability.%20To%20address%20these%20issues%2C%20we%20propose%20C3RL%2C%20a%20novel%20representation%20learning%20framework%20that%20jointly%20models%20both%20CM%20and%20CI%20strategies.%20Motivated%20by%20contrastive%20learning%20in%20computer%20vision%2C%20C3RL%20treats%20the%20inputs%20of%20the%20two%20strategies%20as%20transposed%20views%20and%20builds%20a%20siamese%20network%20architecture%3A%20one%20strategy%20serves%20as%20the%20backbone%2C%20while%20the%20other%20complements%20it.%20By%20jointly%20optimizing%20contrastive%20and%20prediction%20losses%20with%20adaptive%20weighting%2C%20C3RL%20balances%20representation%20and%20forecasting%20performance.%20Extensive%20experiments%20on%20seven%20models%20show%20that%20C3RL%20boosts%20the%20best-case%20performance%20rate%20to%2081.4%25%20for%20models%20based%20on%20CI%20strategy%20and%20to%2076.3%25%20for%20models%20based%20on%20CM%20strategy%2C%20demonstrating%20strong%20generalization%20and%20effectiveness.%0ALink%3A%20http%3A//arxiv.org/abs/2507.17454v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC3RL%253A%2520Rethinking%2520the%2520Combination%2520of%2520Channel-independence%2520and%2520Channel-mixing%2520from%2520Representation%2520Learning%26entry.906535625%3DShusen%2520Ma%2520and%2520Yun-Bo%2520Zhao%2520and%2520Yu%2520Kang%26entry.1292438233%3DMultivariate%2520time%2520series%2520forecasting%2520has%2520drawn%2520increasing%2520attention%2520due%2520to%2520its%2520practical%2520importance.%2520Existing%2520approaches%2520typically%2520adopt%2520either%2520channel-mixing%2520%2528CM%2529%2520or%2520channel-independence%2520%2528CI%2529%2520strategies.%2520CM%2520strategy%2520can%2520capture%2520inter-variable%2520dependencies%2520but%2520fails%2520to%2520discern%2520variable-specific%2520temporal%2520patterns.%2520CI%2520strategy%2520improves%2520this%2520aspect%2520but%2520fails%2520to%2520fully%2520exploit%2520cross-variable%2520dependencies%2520like%2520CM.%2520Hybrid%2520strategies%2520based%2520on%2520feature%2520fusion%2520offer%2520limited%2520generalization%2520and%2520interpretability.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520C3RL%252C%2520a%2520novel%2520representation%2520learning%2520framework%2520that%2520jointly%2520models%2520both%2520CM%2520and%2520CI%2520strategies.%2520Motivated%2520by%2520contrastive%2520learning%2520in%2520computer%2520vision%252C%2520C3RL%2520treats%2520the%2520inputs%2520of%2520the%2520two%2520strategies%2520as%2520transposed%2520views%2520and%2520builds%2520a%2520siamese%2520network%2520architecture%253A%2520one%2520strategy%2520serves%2520as%2520the%2520backbone%252C%2520while%2520the%2520other%2520complements%2520it.%2520By%2520jointly%2520optimizing%2520contrastive%2520and%2520prediction%2520losses%2520with%2520adaptive%2520weighting%252C%2520C3RL%2520balances%2520representation%2520and%2520forecasting%2520performance.%2520Extensive%2520experiments%2520on%2520seven%2520models%2520show%2520that%2520C3RL%2520boosts%2520the%2520best-case%2520performance%2520rate%2520to%252081.4%2525%2520for%2520models%2520based%2520on%2520CI%2520strategy%2520and%2520to%252076.3%2525%2520for%2520models%2520based%2520on%2520CM%2520strategy%252C%2520demonstrating%2520strong%2520generalization%2520and%2520effectiveness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17454v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C3RL%3A%20Rethinking%20the%20Combination%20of%20Channel-independence%20and%20Channel-mixing%20from%20Representation%20Learning&entry.906535625=Shusen%20Ma%20and%20Yun-Bo%20Zhao%20and%20Yu%20Kang&entry.1292438233=Multivariate%20time%20series%20forecasting%20has%20drawn%20increasing%20attention%20due%20to%20its%20practical%20importance.%20Existing%20approaches%20typically%20adopt%20either%20channel-mixing%20%28CM%29%20or%20channel-independence%20%28CI%29%20strategies.%20CM%20strategy%20can%20capture%20inter-variable%20dependencies%20but%20fails%20to%20discern%20variable-specific%20temporal%20patterns.%20CI%20strategy%20improves%20this%20aspect%20but%20fails%20to%20fully%20exploit%20cross-variable%20dependencies%20like%20CM.%20Hybrid%20strategies%20based%20on%20feature%20fusion%20offer%20limited%20generalization%20and%20interpretability.%20To%20address%20these%20issues%2C%20we%20propose%20C3RL%2C%20a%20novel%20representation%20learning%20framework%20that%20jointly%20models%20both%20CM%20and%20CI%20strategies.%20Motivated%20by%20contrastive%20learning%20in%20computer%20vision%2C%20C3RL%20treats%20the%20inputs%20of%20the%20two%20strategies%20as%20transposed%20views%20and%20builds%20a%20siamese%20network%20architecture%3A%20one%20strategy%20serves%20as%20the%20backbone%2C%20while%20the%20other%20complements%20it.%20By%20jointly%20optimizing%20contrastive%20and%20prediction%20losses%20with%20adaptive%20weighting%2C%20C3RL%20balances%20representation%20and%20forecasting%20performance.%20Extensive%20experiments%20on%20seven%20models%20show%20that%20C3RL%20boosts%20the%20best-case%20performance%20rate%20to%2081.4%25%20for%20models%20based%20on%20CI%20strategy%20and%20to%2076.3%25%20for%20models%20based%20on%20CM%20strategy%2C%20demonstrating%20strong%20generalization%20and%20effectiveness.&entry.1838667208=http%3A//arxiv.org/abs/2507.17454v2&entry.124074799=Read"},
{"title": "Training Deep Morphological Neural Networks as Universal Approximators", "author": "Konstantinos Fotopoulos and Petros Maragos", "abstract": "We investigate deep morphological neural networks (DMNNs). We demonstrate that despite their inherent non-linearity, \"linear\" activations are essential for DMNNs. To preserve their inherent sparsity, we propose architectures that constraint the parameters of the \"linear\" activations: For the first (resp. second) architecture, we work under the constraint that the majority of parameters (resp. learnable parameters) should be part of morphological operations. We improve the generalization ability of our networks via residual connections and weight dropout. Our proposed networks can be successfully trained, and are more prunable than linear networks. To the best of our knowledge, we are the first to successfully train DMNNs under such constraints. Finally, we propose a hybrid network architecture combining linear and morphological layers, showing empirically that the inclusion of morphological layers significantly accelerates the convergence of gradient descent with large batches.", "link": "http://arxiv.org/abs/2505.09710v3", "date": "2025-12-23", "relevancy": 2.1199, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5371}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5317}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Deep%20Morphological%20Neural%20Networks%20as%20Universal%20Approximators&body=Title%3A%20Training%20Deep%20Morphological%20Neural%20Networks%20as%20Universal%20Approximators%0AAuthor%3A%20Konstantinos%20Fotopoulos%20and%20Petros%20Maragos%0AAbstract%3A%20We%20investigate%20deep%20morphological%20neural%20networks%20%28DMNNs%29.%20We%20demonstrate%20that%20despite%20their%20inherent%20non-linearity%2C%20%22linear%22%20activations%20are%20essential%20for%20DMNNs.%20To%20preserve%20their%20inherent%20sparsity%2C%20we%20propose%20architectures%20that%20constraint%20the%20parameters%20of%20the%20%22linear%22%20activations%3A%20For%20the%20first%20%28resp.%20second%29%20architecture%2C%20we%20work%20under%20the%20constraint%20that%20the%20majority%20of%20parameters%20%28resp.%20learnable%20parameters%29%20should%20be%20part%20of%20morphological%20operations.%20We%20improve%20the%20generalization%20ability%20of%20our%20networks%20via%20residual%20connections%20and%20weight%20dropout.%20Our%20proposed%20networks%20can%20be%20successfully%20trained%2C%20and%20are%20more%20prunable%20than%20linear%20networks.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20successfully%20train%20DMNNs%20under%20such%20constraints.%20Finally%2C%20we%20propose%20a%20hybrid%20network%20architecture%20combining%20linear%20and%20morphological%20layers%2C%20showing%20empirically%20that%20the%20inclusion%20of%20morphological%20layers%20significantly%20accelerates%20the%20convergence%20of%20gradient%20descent%20with%20large%20batches.%0ALink%3A%20http%3A//arxiv.org/abs/2505.09710v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Deep%2520Morphological%2520Neural%2520Networks%2520as%2520Universal%2520Approximators%26entry.906535625%3DKonstantinos%2520Fotopoulos%2520and%2520Petros%2520Maragos%26entry.1292438233%3DWe%2520investigate%2520deep%2520morphological%2520neural%2520networks%2520%2528DMNNs%2529.%2520We%2520demonstrate%2520that%2520despite%2520their%2520inherent%2520non-linearity%252C%2520%2522linear%2522%2520activations%2520are%2520essential%2520for%2520DMNNs.%2520To%2520preserve%2520their%2520inherent%2520sparsity%252C%2520we%2520propose%2520architectures%2520that%2520constraint%2520the%2520parameters%2520of%2520the%2520%2522linear%2522%2520activations%253A%2520For%2520the%2520first%2520%2528resp.%2520second%2529%2520architecture%252C%2520we%2520work%2520under%2520the%2520constraint%2520that%2520the%2520majority%2520of%2520parameters%2520%2528resp.%2520learnable%2520parameters%2529%2520should%2520be%2520part%2520of%2520morphological%2520operations.%2520We%2520improve%2520the%2520generalization%2520ability%2520of%2520our%2520networks%2520via%2520residual%2520connections%2520and%2520weight%2520dropout.%2520Our%2520proposed%2520networks%2520can%2520be%2520successfully%2520trained%252C%2520and%2520are%2520more%2520prunable%2520than%2520linear%2520networks.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520we%2520are%2520the%2520first%2520to%2520successfully%2520train%2520DMNNs%2520under%2520such%2520constraints.%2520Finally%252C%2520we%2520propose%2520a%2520hybrid%2520network%2520architecture%2520combining%2520linear%2520and%2520morphological%2520layers%252C%2520showing%2520empirically%2520that%2520the%2520inclusion%2520of%2520morphological%2520layers%2520significantly%2520accelerates%2520the%2520convergence%2520of%2520gradient%2520descent%2520with%2520large%2520batches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09710v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Deep%20Morphological%20Neural%20Networks%20as%20Universal%20Approximators&entry.906535625=Konstantinos%20Fotopoulos%20and%20Petros%20Maragos&entry.1292438233=We%20investigate%20deep%20morphological%20neural%20networks%20%28DMNNs%29.%20We%20demonstrate%20that%20despite%20their%20inherent%20non-linearity%2C%20%22linear%22%20activations%20are%20essential%20for%20DMNNs.%20To%20preserve%20their%20inherent%20sparsity%2C%20we%20propose%20architectures%20that%20constraint%20the%20parameters%20of%20the%20%22linear%22%20activations%3A%20For%20the%20first%20%28resp.%20second%29%20architecture%2C%20we%20work%20under%20the%20constraint%20that%20the%20majority%20of%20parameters%20%28resp.%20learnable%20parameters%29%20should%20be%20part%20of%20morphological%20operations.%20We%20improve%20the%20generalization%20ability%20of%20our%20networks%20via%20residual%20connections%20and%20weight%20dropout.%20Our%20proposed%20networks%20can%20be%20successfully%20trained%2C%20and%20are%20more%20prunable%20than%20linear%20networks.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20successfully%20train%20DMNNs%20under%20such%20constraints.%20Finally%2C%20we%20propose%20a%20hybrid%20network%20architecture%20combining%20linear%20and%20morphological%20layers%2C%20showing%20empirically%20that%20the%20inclusion%20of%20morphological%20layers%20significantly%20accelerates%20the%20convergence%20of%20gradient%20descent%20with%20large%20batches.&entry.1838667208=http%3A//arxiv.org/abs/2505.09710v3&entry.124074799=Read"},
{"title": "JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement", "author": "Tao Ye and Hongbin Ren and Chongbing Zhang and Haoran Chen and Xiaosong Li", "abstract": "Given the complexity of underwater environments and the variability of water as a medium, underwater images are inevitably subject to various types of degradation. The degradations present nonlinear coupling rather than simple superposition, which renders the effective processing of such coupled degradations particularly challenging. Most existing methods focus on designing specific branches, modules, or strategies for specific degradations, with little attention paid to the potential information embedded in their coupling. Consequently, they struggle to effectively capture and process the nonlinear interactions of multiple degradations from a bottom-up perspective. To address this issue, we propose JDPNet, a joint degradation processing network, that mines and unifies the potential information inherent in coupled degradations within a unified framework. Specifically, we introduce a joint feature-mining module, along with a probabilistic bootstrap distribution strategy, to facilitate effective mining and unified adjustment of coupled degradation features. Furthermore, to balance color, clarity, and contrast, we design a novel AquaBalanceLoss to guide the network in learning from multiple coupled degradation losses. Experiments on six publicly available underwater datasets, as well as two new datasets constructed in this study, show that JDPNet exhibits state-of-the-art performance while offering a better tradeoff between performance, parameter size, and computational cost.", "link": "http://arxiv.org/abs/2512.20213v1", "date": "2025-12-23", "relevancy": 2.1193, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5334}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5277}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JDPNet%3A%20A%20Network%20Based%20on%20Joint%20Degradation%20Processing%20for%20Underwater%20Image%20Enhancement&body=Title%3A%20JDPNet%3A%20A%20Network%20Based%20on%20Joint%20Degradation%20Processing%20for%20Underwater%20Image%20Enhancement%0AAuthor%3A%20Tao%20Ye%20and%20Hongbin%20Ren%20and%20Chongbing%20Zhang%20and%20Haoran%20Chen%20and%20Xiaosong%20Li%0AAbstract%3A%20Given%20the%20complexity%20of%20underwater%20environments%20and%20the%20variability%20of%20water%20as%20a%20medium%2C%20underwater%20images%20are%20inevitably%20subject%20to%20various%20types%20of%20degradation.%20The%20degradations%20present%20nonlinear%20coupling%20rather%20than%20simple%20superposition%2C%20which%20renders%20the%20effective%20processing%20of%20such%20coupled%20degradations%20particularly%20challenging.%20Most%20existing%20methods%20focus%20on%20designing%20specific%20branches%2C%20modules%2C%20or%20strategies%20for%20specific%20degradations%2C%20with%20little%20attention%20paid%20to%20the%20potential%20information%20embedded%20in%20their%20coupling.%20Consequently%2C%20they%20struggle%20to%20effectively%20capture%20and%20process%20the%20nonlinear%20interactions%20of%20multiple%20degradations%20from%20a%20bottom-up%20perspective.%20To%20address%20this%20issue%2C%20we%20propose%20JDPNet%2C%20a%20joint%20degradation%20processing%20network%2C%20that%20mines%20and%20unifies%20the%20potential%20information%20inherent%20in%20coupled%20degradations%20within%20a%20unified%20framework.%20Specifically%2C%20we%20introduce%20a%20joint%20feature-mining%20module%2C%20along%20with%20a%20probabilistic%20bootstrap%20distribution%20strategy%2C%20to%20facilitate%20effective%20mining%20and%20unified%20adjustment%20of%20coupled%20degradation%20features.%20Furthermore%2C%20to%20balance%20color%2C%20clarity%2C%20and%20contrast%2C%20we%20design%20a%20novel%20AquaBalanceLoss%20to%20guide%20the%20network%20in%20learning%20from%20multiple%20coupled%20degradation%20losses.%20Experiments%20on%20six%20publicly%20available%20underwater%20datasets%2C%20as%20well%20as%20two%20new%20datasets%20constructed%20in%20this%20study%2C%20show%20that%20JDPNet%20exhibits%20state-of-the-art%20performance%20while%20offering%20a%20better%20tradeoff%20between%20performance%2C%20parameter%20size%2C%20and%20computational%20cost.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJDPNet%253A%2520A%2520Network%2520Based%2520on%2520Joint%2520Degradation%2520Processing%2520for%2520Underwater%2520Image%2520Enhancement%26entry.906535625%3DTao%2520Ye%2520and%2520Hongbin%2520Ren%2520and%2520Chongbing%2520Zhang%2520and%2520Haoran%2520Chen%2520and%2520Xiaosong%2520Li%26entry.1292438233%3DGiven%2520the%2520complexity%2520of%2520underwater%2520environments%2520and%2520the%2520variability%2520of%2520water%2520as%2520a%2520medium%252C%2520underwater%2520images%2520are%2520inevitably%2520subject%2520to%2520various%2520types%2520of%2520degradation.%2520The%2520degradations%2520present%2520nonlinear%2520coupling%2520rather%2520than%2520simple%2520superposition%252C%2520which%2520renders%2520the%2520effective%2520processing%2520of%2520such%2520coupled%2520degradations%2520particularly%2520challenging.%2520Most%2520existing%2520methods%2520focus%2520on%2520designing%2520specific%2520branches%252C%2520modules%252C%2520or%2520strategies%2520for%2520specific%2520degradations%252C%2520with%2520little%2520attention%2520paid%2520to%2520the%2520potential%2520information%2520embedded%2520in%2520their%2520coupling.%2520Consequently%252C%2520they%2520struggle%2520to%2520effectively%2520capture%2520and%2520process%2520the%2520nonlinear%2520interactions%2520of%2520multiple%2520degradations%2520from%2520a%2520bottom-up%2520perspective.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520JDPNet%252C%2520a%2520joint%2520degradation%2520processing%2520network%252C%2520that%2520mines%2520and%2520unifies%2520the%2520potential%2520information%2520inherent%2520in%2520coupled%2520degradations%2520within%2520a%2520unified%2520framework.%2520Specifically%252C%2520we%2520introduce%2520a%2520joint%2520feature-mining%2520module%252C%2520along%2520with%2520a%2520probabilistic%2520bootstrap%2520distribution%2520strategy%252C%2520to%2520facilitate%2520effective%2520mining%2520and%2520unified%2520adjustment%2520of%2520coupled%2520degradation%2520features.%2520Furthermore%252C%2520to%2520balance%2520color%252C%2520clarity%252C%2520and%2520contrast%252C%2520we%2520design%2520a%2520novel%2520AquaBalanceLoss%2520to%2520guide%2520the%2520network%2520in%2520learning%2520from%2520multiple%2520coupled%2520degradation%2520losses.%2520Experiments%2520on%2520six%2520publicly%2520available%2520underwater%2520datasets%252C%2520as%2520well%2520as%2520two%2520new%2520datasets%2520constructed%2520in%2520this%2520study%252C%2520show%2520that%2520JDPNet%2520exhibits%2520state-of-the-art%2520performance%2520while%2520offering%2520a%2520better%2520tradeoff%2520between%2520performance%252C%2520parameter%2520size%252C%2520and%2520computational%2520cost.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JDPNet%3A%20A%20Network%20Based%20on%20Joint%20Degradation%20Processing%20for%20Underwater%20Image%20Enhancement&entry.906535625=Tao%20Ye%20and%20Hongbin%20Ren%20and%20Chongbing%20Zhang%20and%20Haoran%20Chen%20and%20Xiaosong%20Li&entry.1292438233=Given%20the%20complexity%20of%20underwater%20environments%20and%20the%20variability%20of%20water%20as%20a%20medium%2C%20underwater%20images%20are%20inevitably%20subject%20to%20various%20types%20of%20degradation.%20The%20degradations%20present%20nonlinear%20coupling%20rather%20than%20simple%20superposition%2C%20which%20renders%20the%20effective%20processing%20of%20such%20coupled%20degradations%20particularly%20challenging.%20Most%20existing%20methods%20focus%20on%20designing%20specific%20branches%2C%20modules%2C%20or%20strategies%20for%20specific%20degradations%2C%20with%20little%20attention%20paid%20to%20the%20potential%20information%20embedded%20in%20their%20coupling.%20Consequently%2C%20they%20struggle%20to%20effectively%20capture%20and%20process%20the%20nonlinear%20interactions%20of%20multiple%20degradations%20from%20a%20bottom-up%20perspective.%20To%20address%20this%20issue%2C%20we%20propose%20JDPNet%2C%20a%20joint%20degradation%20processing%20network%2C%20that%20mines%20and%20unifies%20the%20potential%20information%20inherent%20in%20coupled%20degradations%20within%20a%20unified%20framework.%20Specifically%2C%20we%20introduce%20a%20joint%20feature-mining%20module%2C%20along%20with%20a%20probabilistic%20bootstrap%20distribution%20strategy%2C%20to%20facilitate%20effective%20mining%20and%20unified%20adjustment%20of%20coupled%20degradation%20features.%20Furthermore%2C%20to%20balance%20color%2C%20clarity%2C%20and%20contrast%2C%20we%20design%20a%20novel%20AquaBalanceLoss%20to%20guide%20the%20network%20in%20learning%20from%20multiple%20coupled%20degradation%20losses.%20Experiments%20on%20six%20publicly%20available%20underwater%20datasets%2C%20as%20well%20as%20two%20new%20datasets%20constructed%20in%20this%20study%2C%20show%20that%20JDPNet%20exhibits%20state-of-the-art%20performance%20while%20offering%20a%20better%20tradeoff%20between%20performance%2C%20parameter%20size%2C%20and%20computational%20cost.&entry.1838667208=http%3A//arxiv.org/abs/2512.20213v1&entry.124074799=Read"},
{"title": "Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention", "author": "Yingzhen Yang", "abstract": "We study the problem of learning a low-degree spherical polynomial of degree $\\ell_0 = \u0398(1) \\ge 1$ defined on the unit sphere in $\\RR^d$ by training an over-parameterized two-layer neural network (NN) with channel attention in this paper. Our main result is the significantly improved sample complexity for learning such low-degree polynomials. We show that, for any regression risk $\\eps \\in (0,1)$, a carefully designed two-layer NN with channel attention and finite width of $m \\ge \u0398({n^4 \\log (2n/\u03b4)}/{d^{2\\ell_0}})$ trained by the vanilla gradient descent (GD) requires the lowest sample complexity of $n \\asymp \u0398(d^{\\ell_0}/\\eps)$ with probability $1-\u03b4$ for every $\u03b4\\in (0,1)$, in contrast with the representative sample complexity $\u0398\\pth{d^{\\ell_0} \\max\\set{\\eps^{-2},\\log d}}$, where $n$ is the training daata size. Moreover, such sample complexity is not improvable since the trained network renders a sharp rate of the nonparametric regression risk of the order $\u0398(d^{\\ell_0}/{n})$ with probability at least $1-\u03b4$. On the other hand, the minimax optimal rate for the regression risk with a kernel of rank $\u0398(d^{\\ell_0})$ is $\u0398(d^{\\ell_0}/{n})$, so that the rate of the nonparametric regression risk of the network trained by GD is minimax optimal. The training of the two-layer NN with channel attention consists of two stages. In Stage 1, a provable learnable channel selection algorithm identifies the ground-truth channel number $\\ell_0$ from the initial $L \\ge \\ell_0$ channels in the first-layer activation, with high probability. This learnable selection is achieved by an efficient one-step GD update on both layers, enabling feature learning for low-degree polynomial targets. In Stage 2, the second layer is trained by standard GD using the activation function with the selected channels.", "link": "http://arxiv.org/abs/2512.20562v1", "date": "2025-12-23", "relevancy": 2.0945, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4338}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4255}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shallow%20Neural%20Networks%20Learn%20Low-Degree%20Spherical%20Polynomials%20with%20Learnable%20Channel%20Attention&body=Title%3A%20Shallow%20Neural%20Networks%20Learn%20Low-Degree%20Spherical%20Polynomials%20with%20Learnable%20Channel%20Attention%0AAuthor%3A%20Yingzhen%20Yang%0AAbstract%3A%20We%20study%20the%20problem%20of%20learning%20a%20low-degree%20spherical%20polynomial%20of%20degree%20%24%5Cell_0%20%3D%20%CE%98%281%29%20%5Cge%201%24%20defined%20on%20the%20unit%20sphere%20in%20%24%5CRR%5Ed%24%20by%20training%20an%20over-parameterized%20two-layer%20neural%20network%20%28NN%29%20with%20channel%20attention%20in%20this%20paper.%20Our%20main%20result%20is%20the%20significantly%20improved%20sample%20complexity%20for%20learning%20such%20low-degree%20polynomials.%20We%20show%20that%2C%20for%20any%20regression%20risk%20%24%5Ceps%20%5Cin%20%280%2C1%29%24%2C%20a%20carefully%20designed%20two-layer%20NN%20with%20channel%20attention%20and%20finite%20width%20of%20%24m%20%5Cge%20%CE%98%28%7Bn%5E4%20%5Clog%20%282n/%CE%B4%29%7D/%7Bd%5E%7B2%5Cell_0%7D%7D%29%24%20trained%20by%20the%20vanilla%20gradient%20descent%20%28GD%29%20requires%20the%20lowest%20sample%20complexity%20of%20%24n%20%5Casymp%20%CE%98%28d%5E%7B%5Cell_0%7D/%5Ceps%29%24%20with%20probability%20%241-%CE%B4%24%20for%20every%20%24%CE%B4%5Cin%20%280%2C1%29%24%2C%20in%20contrast%20with%20the%20representative%20sample%20complexity%20%24%CE%98%5Cpth%7Bd%5E%7B%5Cell_0%7D%20%5Cmax%5Cset%7B%5Ceps%5E%7B-2%7D%2C%5Clog%20d%7D%7D%24%2C%20where%20%24n%24%20is%20the%20training%20daata%20size.%20Moreover%2C%20such%20sample%20complexity%20is%20not%20improvable%20since%20the%20trained%20network%20renders%20a%20sharp%20rate%20of%20the%20nonparametric%20regression%20risk%20of%20the%20order%20%24%CE%98%28d%5E%7B%5Cell_0%7D/%7Bn%7D%29%24%20with%20probability%20at%20least%20%241-%CE%B4%24.%20On%20the%20other%20hand%2C%20the%20minimax%20optimal%20rate%20for%20the%20regression%20risk%20with%20a%20kernel%20of%20rank%20%24%CE%98%28d%5E%7B%5Cell_0%7D%29%24%20is%20%24%CE%98%28d%5E%7B%5Cell_0%7D/%7Bn%7D%29%24%2C%20so%20that%20the%20rate%20of%20the%20nonparametric%20regression%20risk%20of%20the%20network%20trained%20by%20GD%20is%20minimax%20optimal.%20The%20training%20of%20the%20two-layer%20NN%20with%20channel%20attention%20consists%20of%20two%20stages.%20In%20Stage%201%2C%20a%20provable%20learnable%20channel%20selection%20algorithm%20identifies%20the%20ground-truth%20channel%20number%20%24%5Cell_0%24%20from%20the%20initial%20%24L%20%5Cge%20%5Cell_0%24%20channels%20in%20the%20first-layer%20activation%2C%20with%20high%20probability.%20This%20learnable%20selection%20is%20achieved%20by%20an%20efficient%20one-step%20GD%20update%20on%20both%20layers%2C%20enabling%20feature%20learning%20for%20low-degree%20polynomial%20targets.%20In%20Stage%202%2C%20the%20second%20layer%20is%20trained%20by%20standard%20GD%20using%20the%20activation%20function%20with%20the%20selected%20channels.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShallow%2520Neural%2520Networks%2520Learn%2520Low-Degree%2520Spherical%2520Polynomials%2520with%2520Learnable%2520Channel%2520Attention%26entry.906535625%3DYingzhen%2520Yang%26entry.1292438233%3DWe%2520study%2520the%2520problem%2520of%2520learning%2520a%2520low-degree%2520spherical%2520polynomial%2520of%2520degree%2520%2524%255Cell_0%2520%253D%2520%25CE%2598%25281%2529%2520%255Cge%25201%2524%2520defined%2520on%2520the%2520unit%2520sphere%2520in%2520%2524%255CRR%255Ed%2524%2520by%2520training%2520an%2520over-parameterized%2520two-layer%2520neural%2520network%2520%2528NN%2529%2520with%2520channel%2520attention%2520in%2520this%2520paper.%2520Our%2520main%2520result%2520is%2520the%2520significantly%2520improved%2520sample%2520complexity%2520for%2520learning%2520such%2520low-degree%2520polynomials.%2520We%2520show%2520that%252C%2520for%2520any%2520regression%2520risk%2520%2524%255Ceps%2520%255Cin%2520%25280%252C1%2529%2524%252C%2520a%2520carefully%2520designed%2520two-layer%2520NN%2520with%2520channel%2520attention%2520and%2520finite%2520width%2520of%2520%2524m%2520%255Cge%2520%25CE%2598%2528%257Bn%255E4%2520%255Clog%2520%25282n/%25CE%25B4%2529%257D/%257Bd%255E%257B2%255Cell_0%257D%257D%2529%2524%2520trained%2520by%2520the%2520vanilla%2520gradient%2520descent%2520%2528GD%2529%2520requires%2520the%2520lowest%2520sample%2520complexity%2520of%2520%2524n%2520%255Casymp%2520%25CE%2598%2528d%255E%257B%255Cell_0%257D/%255Ceps%2529%2524%2520with%2520probability%2520%25241-%25CE%25B4%2524%2520for%2520every%2520%2524%25CE%25B4%255Cin%2520%25280%252C1%2529%2524%252C%2520in%2520contrast%2520with%2520the%2520representative%2520sample%2520complexity%2520%2524%25CE%2598%255Cpth%257Bd%255E%257B%255Cell_0%257D%2520%255Cmax%255Cset%257B%255Ceps%255E%257B-2%257D%252C%255Clog%2520d%257D%257D%2524%252C%2520where%2520%2524n%2524%2520is%2520the%2520training%2520daata%2520size.%2520Moreover%252C%2520such%2520sample%2520complexity%2520is%2520not%2520improvable%2520since%2520the%2520trained%2520network%2520renders%2520a%2520sharp%2520rate%2520of%2520the%2520nonparametric%2520regression%2520risk%2520of%2520the%2520order%2520%2524%25CE%2598%2528d%255E%257B%255Cell_0%257D/%257Bn%257D%2529%2524%2520with%2520probability%2520at%2520least%2520%25241-%25CE%25B4%2524.%2520On%2520the%2520other%2520hand%252C%2520the%2520minimax%2520optimal%2520rate%2520for%2520the%2520regression%2520risk%2520with%2520a%2520kernel%2520of%2520rank%2520%2524%25CE%2598%2528d%255E%257B%255Cell_0%257D%2529%2524%2520is%2520%2524%25CE%2598%2528d%255E%257B%255Cell_0%257D/%257Bn%257D%2529%2524%252C%2520so%2520that%2520the%2520rate%2520of%2520the%2520nonparametric%2520regression%2520risk%2520of%2520the%2520network%2520trained%2520by%2520GD%2520is%2520minimax%2520optimal.%2520The%2520training%2520of%2520the%2520two-layer%2520NN%2520with%2520channel%2520attention%2520consists%2520of%2520two%2520stages.%2520In%2520Stage%25201%252C%2520a%2520provable%2520learnable%2520channel%2520selection%2520algorithm%2520identifies%2520the%2520ground-truth%2520channel%2520number%2520%2524%255Cell_0%2524%2520from%2520the%2520initial%2520%2524L%2520%255Cge%2520%255Cell_0%2524%2520channels%2520in%2520the%2520first-layer%2520activation%252C%2520with%2520high%2520probability.%2520This%2520learnable%2520selection%2520is%2520achieved%2520by%2520an%2520efficient%2520one-step%2520GD%2520update%2520on%2520both%2520layers%252C%2520enabling%2520feature%2520learning%2520for%2520low-degree%2520polynomial%2520targets.%2520In%2520Stage%25202%252C%2520the%2520second%2520layer%2520is%2520trained%2520by%2520standard%2520GD%2520using%2520the%2520activation%2520function%2520with%2520the%2520selected%2520channels.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shallow%20Neural%20Networks%20Learn%20Low-Degree%20Spherical%20Polynomials%20with%20Learnable%20Channel%20Attention&entry.906535625=Yingzhen%20Yang&entry.1292438233=We%20study%20the%20problem%20of%20learning%20a%20low-degree%20spherical%20polynomial%20of%20degree%20%24%5Cell_0%20%3D%20%CE%98%281%29%20%5Cge%201%24%20defined%20on%20the%20unit%20sphere%20in%20%24%5CRR%5Ed%24%20by%20training%20an%20over-parameterized%20two-layer%20neural%20network%20%28NN%29%20with%20channel%20attention%20in%20this%20paper.%20Our%20main%20result%20is%20the%20significantly%20improved%20sample%20complexity%20for%20learning%20such%20low-degree%20polynomials.%20We%20show%20that%2C%20for%20any%20regression%20risk%20%24%5Ceps%20%5Cin%20%280%2C1%29%24%2C%20a%20carefully%20designed%20two-layer%20NN%20with%20channel%20attention%20and%20finite%20width%20of%20%24m%20%5Cge%20%CE%98%28%7Bn%5E4%20%5Clog%20%282n/%CE%B4%29%7D/%7Bd%5E%7B2%5Cell_0%7D%7D%29%24%20trained%20by%20the%20vanilla%20gradient%20descent%20%28GD%29%20requires%20the%20lowest%20sample%20complexity%20of%20%24n%20%5Casymp%20%CE%98%28d%5E%7B%5Cell_0%7D/%5Ceps%29%24%20with%20probability%20%241-%CE%B4%24%20for%20every%20%24%CE%B4%5Cin%20%280%2C1%29%24%2C%20in%20contrast%20with%20the%20representative%20sample%20complexity%20%24%CE%98%5Cpth%7Bd%5E%7B%5Cell_0%7D%20%5Cmax%5Cset%7B%5Ceps%5E%7B-2%7D%2C%5Clog%20d%7D%7D%24%2C%20where%20%24n%24%20is%20the%20training%20daata%20size.%20Moreover%2C%20such%20sample%20complexity%20is%20not%20improvable%20since%20the%20trained%20network%20renders%20a%20sharp%20rate%20of%20the%20nonparametric%20regression%20risk%20of%20the%20order%20%24%CE%98%28d%5E%7B%5Cell_0%7D/%7Bn%7D%29%24%20with%20probability%20at%20least%20%241-%CE%B4%24.%20On%20the%20other%20hand%2C%20the%20minimax%20optimal%20rate%20for%20the%20regression%20risk%20with%20a%20kernel%20of%20rank%20%24%CE%98%28d%5E%7B%5Cell_0%7D%29%24%20is%20%24%CE%98%28d%5E%7B%5Cell_0%7D/%7Bn%7D%29%24%2C%20so%20that%20the%20rate%20of%20the%20nonparametric%20regression%20risk%20of%20the%20network%20trained%20by%20GD%20is%20minimax%20optimal.%20The%20training%20of%20the%20two-layer%20NN%20with%20channel%20attention%20consists%20of%20two%20stages.%20In%20Stage%201%2C%20a%20provable%20learnable%20channel%20selection%20algorithm%20identifies%20the%20ground-truth%20channel%20number%20%24%5Cell_0%24%20from%20the%20initial%20%24L%20%5Cge%20%5Cell_0%24%20channels%20in%20the%20first-layer%20activation%2C%20with%20high%20probability.%20This%20learnable%20selection%20is%20achieved%20by%20an%20efficient%20one-step%20GD%20update%20on%20both%20layers%2C%20enabling%20feature%20learning%20for%20low-degree%20polynomial%20targets.%20In%20Stage%202%2C%20the%20second%20layer%20is%20trained%20by%20standard%20GD%20using%20the%20activation%20function%20with%20the%20selected%20channels.&entry.1838667208=http%3A//arxiv.org/abs/2512.20562v1&entry.124074799=Read"},
{"title": "Learning Safe Autonomous Driving Policies Using Predictive Safety Representations", "author": "Mahesh Keswani and Raunak Bhattacharyya", "abstract": "Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.", "link": "http://arxiv.org/abs/2512.17586v2", "date": "2025-12-23", "relevancy": 2.0905, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5463}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5349}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Safe%20Autonomous%20Driving%20Policies%20Using%20Predictive%20Safety%20Representations&body=Title%3A%20Learning%20Safe%20Autonomous%20Driving%20Policies%20Using%20Predictive%20Safety%20Representations%0AAuthor%3A%20Mahesh%20Keswani%20and%20Raunak%20Bhattacharyya%0AAbstract%3A%20Safe%20reinforcement%20learning%20%28SafeRL%29%20is%20a%20prominent%20paradigm%20for%20autonomous%20driving%2C%20where%20agents%20are%20required%20to%20optimize%20performance%20under%20strict%20safety%20requirements.%20This%20dual%20objective%20creates%20a%20fundamental%20tension%2C%20as%20overly%20conservative%20policies%20limit%20driving%20efficiency%20while%20aggressive%20exploration%20risks%20safety%20violations.%20The%20Safety%20Representations%20for%20Safer%20Policy%20Learning%20%28SRPL%29%20framework%20addresses%20this%20challenge%20by%20equipping%20agents%20with%20a%20predictive%20model%20of%20future%20constraint%20violations%20and%20has%20shown%20promise%20in%20controlled%20environments.%20This%20paper%20investigates%20whether%20SRPL%20extends%20to%20real-world%20autonomous%20driving%20scenarios.%20Systematic%20experiments%20on%20the%20Waymo%20Open%20Motion%20Dataset%20%28WOMD%29%20and%20NuPlan%20demonstrate%20that%20SRPL%20can%20improve%20the%20reward-safety%20tradeoff%2C%20achieving%20statistically%20significant%20improvements%20in%20success%20rate%20%28effect%20sizes%20r%20%3D%200.65-0.86%29%20and%20cost%20reduction%20%28effect%20sizes%20r%20%3D%200.70-0.83%29%2C%20with%20p%20%3C%200.05%20for%20observed%20improvements.%20However%2C%20its%20effectiveness%20depends%20on%20the%20underlying%20policy%20optimizer%20and%20the%20dataset%20distribution.%20The%20results%20further%20show%20that%20predictive%20safety%20representations%20play%20a%20critical%20role%20in%20improving%20robustness%20to%20observation%20noise.%20Additionally%2C%20in%20zero-shot%20cross-dataset%20evaluation%2C%20SRPL-augmented%20agents%20demonstrate%20improved%20generalization%20compared%20to%20non-SRPL%20methods.%20These%20findings%20collectively%20demonstrate%20the%20potential%20of%20predictive%20safety%20representations%20to%20strengthen%20SafeRL%20for%20autonomous%20driving.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17586v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Safe%2520Autonomous%2520Driving%2520Policies%2520Using%2520Predictive%2520Safety%2520Representations%26entry.906535625%3DMahesh%2520Keswani%2520and%2520Raunak%2520Bhattacharyya%26entry.1292438233%3DSafe%2520reinforcement%2520learning%2520%2528SafeRL%2529%2520is%2520a%2520prominent%2520paradigm%2520for%2520autonomous%2520driving%252C%2520where%2520agents%2520are%2520required%2520to%2520optimize%2520performance%2520under%2520strict%2520safety%2520requirements.%2520This%2520dual%2520objective%2520creates%2520a%2520fundamental%2520tension%252C%2520as%2520overly%2520conservative%2520policies%2520limit%2520driving%2520efficiency%2520while%2520aggressive%2520exploration%2520risks%2520safety%2520violations.%2520The%2520Safety%2520Representations%2520for%2520Safer%2520Policy%2520Learning%2520%2528SRPL%2529%2520framework%2520addresses%2520this%2520challenge%2520by%2520equipping%2520agents%2520with%2520a%2520predictive%2520model%2520of%2520future%2520constraint%2520violations%2520and%2520has%2520shown%2520promise%2520in%2520controlled%2520environments.%2520This%2520paper%2520investigates%2520whether%2520SRPL%2520extends%2520to%2520real-world%2520autonomous%2520driving%2520scenarios.%2520Systematic%2520experiments%2520on%2520the%2520Waymo%2520Open%2520Motion%2520Dataset%2520%2528WOMD%2529%2520and%2520NuPlan%2520demonstrate%2520that%2520SRPL%2520can%2520improve%2520the%2520reward-safety%2520tradeoff%252C%2520achieving%2520statistically%2520significant%2520improvements%2520in%2520success%2520rate%2520%2528effect%2520sizes%2520r%2520%253D%25200.65-0.86%2529%2520and%2520cost%2520reduction%2520%2528effect%2520sizes%2520r%2520%253D%25200.70-0.83%2529%252C%2520with%2520p%2520%253C%25200.05%2520for%2520observed%2520improvements.%2520However%252C%2520its%2520effectiveness%2520depends%2520on%2520the%2520underlying%2520policy%2520optimizer%2520and%2520the%2520dataset%2520distribution.%2520The%2520results%2520further%2520show%2520that%2520predictive%2520safety%2520representations%2520play%2520a%2520critical%2520role%2520in%2520improving%2520robustness%2520to%2520observation%2520noise.%2520Additionally%252C%2520in%2520zero-shot%2520cross-dataset%2520evaluation%252C%2520SRPL-augmented%2520agents%2520demonstrate%2520improved%2520generalization%2520compared%2520to%2520non-SRPL%2520methods.%2520These%2520findings%2520collectively%2520demonstrate%2520the%2520potential%2520of%2520predictive%2520safety%2520representations%2520to%2520strengthen%2520SafeRL%2520for%2520autonomous%2520driving.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17586v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Safe%20Autonomous%20Driving%20Policies%20Using%20Predictive%20Safety%20Representations&entry.906535625=Mahesh%20Keswani%20and%20Raunak%20Bhattacharyya&entry.1292438233=Safe%20reinforcement%20learning%20%28SafeRL%29%20is%20a%20prominent%20paradigm%20for%20autonomous%20driving%2C%20where%20agents%20are%20required%20to%20optimize%20performance%20under%20strict%20safety%20requirements.%20This%20dual%20objective%20creates%20a%20fundamental%20tension%2C%20as%20overly%20conservative%20policies%20limit%20driving%20efficiency%20while%20aggressive%20exploration%20risks%20safety%20violations.%20The%20Safety%20Representations%20for%20Safer%20Policy%20Learning%20%28SRPL%29%20framework%20addresses%20this%20challenge%20by%20equipping%20agents%20with%20a%20predictive%20model%20of%20future%20constraint%20violations%20and%20has%20shown%20promise%20in%20controlled%20environments.%20This%20paper%20investigates%20whether%20SRPL%20extends%20to%20real-world%20autonomous%20driving%20scenarios.%20Systematic%20experiments%20on%20the%20Waymo%20Open%20Motion%20Dataset%20%28WOMD%29%20and%20NuPlan%20demonstrate%20that%20SRPL%20can%20improve%20the%20reward-safety%20tradeoff%2C%20achieving%20statistically%20significant%20improvements%20in%20success%20rate%20%28effect%20sizes%20r%20%3D%200.65-0.86%29%20and%20cost%20reduction%20%28effect%20sizes%20r%20%3D%200.70-0.83%29%2C%20with%20p%20%3C%200.05%20for%20observed%20improvements.%20However%2C%20its%20effectiveness%20depends%20on%20the%20underlying%20policy%20optimizer%20and%20the%20dataset%20distribution.%20The%20results%20further%20show%20that%20predictive%20safety%20representations%20play%20a%20critical%20role%20in%20improving%20robustness%20to%20observation%20noise.%20Additionally%2C%20in%20zero-shot%20cross-dataset%20evaluation%2C%20SRPL-augmented%20agents%20demonstrate%20improved%20generalization%20compared%20to%20non-SRPL%20methods.%20These%20findings%20collectively%20demonstrate%20the%20potential%20of%20predictive%20safety%20representations%20to%20strengthen%20SafeRL%20for%20autonomous%20driving.&entry.1838667208=http%3A//arxiv.org/abs/2512.17586v2&entry.124074799=Read"},
{"title": "Resolution scaling governs DINOv3 transfer performance in chest radiograph classification", "author": "Soroosh Tayebi Arasteh and Mina Shaigan and Christiane Kuhl and Jakob Nikolas Kather and Sven Nebelung and Daniel Truhn", "abstract": "Self-supervised learning (SSL) has advanced visual representation learning, but its value in chest radiography, a high-volume imaging modality with fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL models through Gram-anchored self-distillation. Whether these design choices improve transfer learning for chest radiography has not been systematically tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across seven datasets (n>814,000). Two representative backbones were evaluated: ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and 1024x1024 pixels. We additionally assessed frozen features from a 7B model. The primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2 achieved comparable performance on adult datasets. Increasing resolution to 512x512 yielded consistent improvements for DINOv3 over both DINOv2 and ImageNet. In contrast, results in pediatric cohort showed no differences across initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models using frozen DINOv3-7B features underperformed relative to fully finetuned 86-89M-parameter backbones, highlighting the importance of domain adaptation. Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains were most evident for boundary-dependent and small focal abnormalities. In chest radiography, higher input resolution is critical for leveraging the benefits of modern self-supervised models. 512x512 pixels represent a practical upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest performance, while larger inputs offer minimal return on cost. Clinically, these findings support use of finetuned, mid-sized backbones at 512x512 for chest radiograph interpretation, with the greatest gains expected in detecting subtle or boundary-centered lesions relevant to emergency and critical care settings.", "link": "http://arxiv.org/abs/2510.07191v2", "date": "2025-12-23", "relevancy": 2.0904, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resolution%20scaling%20governs%20DINOv3%20transfer%20performance%20in%20chest%20radiograph%20classification&body=Title%3A%20Resolution%20scaling%20governs%20DINOv3%20transfer%20performance%20in%20chest%20radiograph%20classification%0AAuthor%3A%20Soroosh%20Tayebi%20Arasteh%20and%20Mina%20Shaigan%20and%20Christiane%20Kuhl%20and%20Jakob%20Nikolas%20Kather%20and%20Sven%20Nebelung%20and%20Daniel%20Truhn%0AAbstract%3A%20Self-supervised%20learning%20%28SSL%29%20has%20advanced%20visual%20representation%20learning%2C%20but%20its%20value%20in%20chest%20radiography%2C%20a%20high-volume%20imaging%20modality%20with%20fine-grained%20findings%2C%20remains%20unclear.%20Meta%27s%20DINOv3%20extends%20earlier%20SSL%20models%20through%20Gram-anchored%20self-distillation.%20Whether%20these%20design%20choices%20improve%20transfer%20learning%20for%20chest%20radiography%20has%20not%20been%20systematically%20tested.%20We%20benchmarked%20DINOv3%20against%20DINOv2%20and%20ImageNet%20initialization%20across%20seven%20datasets%20%28n%3E814%2C000%29.%20Two%20representative%20backbones%20were%20evaluated%3A%20ViT-B/16%20and%20ConvNeXt-B.%20Images%20were%20analyzed%20at%20224x224%2C%20512x512%2C%20and%201024x1024%20pixels.%20We%20additionally%20assessed%20frozen%20features%20from%20a%207B%20model.%20The%20primary%20outcome%20was%20mean%20AUROC%20across%20labels.%20At%20224x224%2C%20DINOv3%20and%20DINOv2%20achieved%20comparable%20performance%20on%20adult%20datasets.%20Increasing%20resolution%20to%20512x512%20yielded%20consistent%20improvements%20for%20DINOv3%20over%20both%20DINOv2%20and%20ImageNet.%20In%20contrast%2C%20results%20in%20pediatric%20cohort%20showed%20no%20differences%20across%20initializations.%20Across%20all%20settings%2C%20ConvNeXt-B%20outperformed%20ViT-B/16.%20Models%20using%20frozen%20DINOv3-7B%20features%20underperformed%20relative%20to%20fully%20finetuned%2086-89M-parameter%20backbones%2C%20highlighting%20the%20importance%20of%20domain%20adaptation.%20Scaling%20to%201024x1024%20did%20not%20further%20improve%20accuracy.%20Resolution-related%20gains%20were%20most%20evident%20for%20boundary-dependent%20and%20small%20focal%20abnormalities.%20In%20chest%20radiography%2C%20higher%20input%20resolution%20is%20critical%20for%20leveraging%20the%20benefits%20of%20modern%20self-supervised%20models.%20512x512%20pixels%20represent%20a%20practical%20upper%20limit%20where%20DINOv3-initialized%20ConvNeXt-B%20networks%20provide%20the%20strongest%20performance%2C%20while%20larger%20inputs%20offer%20minimal%20return%20on%20cost.%20Clinically%2C%20these%20findings%20support%20use%20of%20finetuned%2C%20mid-sized%20backbones%20at%20512x512%20for%20chest%20radiograph%20interpretation%2C%20with%20the%20greatest%20gains%20expected%20in%20detecting%20subtle%20or%20boundary-centered%20lesions%20relevant%20to%20emergency%20and%20critical%20care%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2510.07191v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResolution%2520scaling%2520governs%2520DINOv3%2520transfer%2520performance%2520in%2520chest%2520radiograph%2520classification%26entry.906535625%3DSoroosh%2520Tayebi%2520Arasteh%2520and%2520Mina%2520Shaigan%2520and%2520Christiane%2520Kuhl%2520and%2520Jakob%2520Nikolas%2520Kather%2520and%2520Sven%2520Nebelung%2520and%2520Daniel%2520Truhn%26entry.1292438233%3DSelf-supervised%2520learning%2520%2528SSL%2529%2520has%2520advanced%2520visual%2520representation%2520learning%252C%2520but%2520its%2520value%2520in%2520chest%2520radiography%252C%2520a%2520high-volume%2520imaging%2520modality%2520with%2520fine-grained%2520findings%252C%2520remains%2520unclear.%2520Meta%2527s%2520DINOv3%2520extends%2520earlier%2520SSL%2520models%2520through%2520Gram-anchored%2520self-distillation.%2520Whether%2520these%2520design%2520choices%2520improve%2520transfer%2520learning%2520for%2520chest%2520radiography%2520has%2520not%2520been%2520systematically%2520tested.%2520We%2520benchmarked%2520DINOv3%2520against%2520DINOv2%2520and%2520ImageNet%2520initialization%2520across%2520seven%2520datasets%2520%2528n%253E814%252C000%2529.%2520Two%2520representative%2520backbones%2520were%2520evaluated%253A%2520ViT-B/16%2520and%2520ConvNeXt-B.%2520Images%2520were%2520analyzed%2520at%2520224x224%252C%2520512x512%252C%2520and%25201024x1024%2520pixels.%2520We%2520additionally%2520assessed%2520frozen%2520features%2520from%2520a%25207B%2520model.%2520The%2520primary%2520outcome%2520was%2520mean%2520AUROC%2520across%2520labels.%2520At%2520224x224%252C%2520DINOv3%2520and%2520DINOv2%2520achieved%2520comparable%2520performance%2520on%2520adult%2520datasets.%2520Increasing%2520resolution%2520to%2520512x512%2520yielded%2520consistent%2520improvements%2520for%2520DINOv3%2520over%2520both%2520DINOv2%2520and%2520ImageNet.%2520In%2520contrast%252C%2520results%2520in%2520pediatric%2520cohort%2520showed%2520no%2520differences%2520across%2520initializations.%2520Across%2520all%2520settings%252C%2520ConvNeXt-B%2520outperformed%2520ViT-B/16.%2520Models%2520using%2520frozen%2520DINOv3-7B%2520features%2520underperformed%2520relative%2520to%2520fully%2520finetuned%252086-89M-parameter%2520backbones%252C%2520highlighting%2520the%2520importance%2520of%2520domain%2520adaptation.%2520Scaling%2520to%25201024x1024%2520did%2520not%2520further%2520improve%2520accuracy.%2520Resolution-related%2520gains%2520were%2520most%2520evident%2520for%2520boundary-dependent%2520and%2520small%2520focal%2520abnormalities.%2520In%2520chest%2520radiography%252C%2520higher%2520input%2520resolution%2520is%2520critical%2520for%2520leveraging%2520the%2520benefits%2520of%2520modern%2520self-supervised%2520models.%2520512x512%2520pixels%2520represent%2520a%2520practical%2520upper%2520limit%2520where%2520DINOv3-initialized%2520ConvNeXt-B%2520networks%2520provide%2520the%2520strongest%2520performance%252C%2520while%2520larger%2520inputs%2520offer%2520minimal%2520return%2520on%2520cost.%2520Clinically%252C%2520these%2520findings%2520support%2520use%2520of%2520finetuned%252C%2520mid-sized%2520backbones%2520at%2520512x512%2520for%2520chest%2520radiograph%2520interpretation%252C%2520with%2520the%2520greatest%2520gains%2520expected%2520in%2520detecting%2520subtle%2520or%2520boundary-centered%2520lesions%2520relevant%2520to%2520emergency%2520and%2520critical%2520care%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07191v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resolution%20scaling%20governs%20DINOv3%20transfer%20performance%20in%20chest%20radiograph%20classification&entry.906535625=Soroosh%20Tayebi%20Arasteh%20and%20Mina%20Shaigan%20and%20Christiane%20Kuhl%20and%20Jakob%20Nikolas%20Kather%20and%20Sven%20Nebelung%20and%20Daniel%20Truhn&entry.1292438233=Self-supervised%20learning%20%28SSL%29%20has%20advanced%20visual%20representation%20learning%2C%20but%20its%20value%20in%20chest%20radiography%2C%20a%20high-volume%20imaging%20modality%20with%20fine-grained%20findings%2C%20remains%20unclear.%20Meta%27s%20DINOv3%20extends%20earlier%20SSL%20models%20through%20Gram-anchored%20self-distillation.%20Whether%20these%20design%20choices%20improve%20transfer%20learning%20for%20chest%20radiography%20has%20not%20been%20systematically%20tested.%20We%20benchmarked%20DINOv3%20against%20DINOv2%20and%20ImageNet%20initialization%20across%20seven%20datasets%20%28n%3E814%2C000%29.%20Two%20representative%20backbones%20were%20evaluated%3A%20ViT-B/16%20and%20ConvNeXt-B.%20Images%20were%20analyzed%20at%20224x224%2C%20512x512%2C%20and%201024x1024%20pixels.%20We%20additionally%20assessed%20frozen%20features%20from%20a%207B%20model.%20The%20primary%20outcome%20was%20mean%20AUROC%20across%20labels.%20At%20224x224%2C%20DINOv3%20and%20DINOv2%20achieved%20comparable%20performance%20on%20adult%20datasets.%20Increasing%20resolution%20to%20512x512%20yielded%20consistent%20improvements%20for%20DINOv3%20over%20both%20DINOv2%20and%20ImageNet.%20In%20contrast%2C%20results%20in%20pediatric%20cohort%20showed%20no%20differences%20across%20initializations.%20Across%20all%20settings%2C%20ConvNeXt-B%20outperformed%20ViT-B/16.%20Models%20using%20frozen%20DINOv3-7B%20features%20underperformed%20relative%20to%20fully%20finetuned%2086-89M-parameter%20backbones%2C%20highlighting%20the%20importance%20of%20domain%20adaptation.%20Scaling%20to%201024x1024%20did%20not%20further%20improve%20accuracy.%20Resolution-related%20gains%20were%20most%20evident%20for%20boundary-dependent%20and%20small%20focal%20abnormalities.%20In%20chest%20radiography%2C%20higher%20input%20resolution%20is%20critical%20for%20leveraging%20the%20benefits%20of%20modern%20self-supervised%20models.%20512x512%20pixels%20represent%20a%20practical%20upper%20limit%20where%20DINOv3-initialized%20ConvNeXt-B%20networks%20provide%20the%20strongest%20performance%2C%20while%20larger%20inputs%20offer%20minimal%20return%20on%20cost.%20Clinically%2C%20these%20findings%20support%20use%20of%20finetuned%2C%20mid-sized%20backbones%20at%20512x512%20for%20chest%20radiograph%20interpretation%2C%20with%20the%20greatest%20gains%20expected%20in%20detecting%20subtle%20or%20boundary-centered%20lesions%20relevant%20to%20emergency%20and%20critical%20care%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2510.07191v2&entry.124074799=Read"},
{"title": "IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing", "author": "Oikantik Nath and Sahithi Kukkala and Mitesh Khapra and Ravi Kiran Sarvadevabhatla", "abstract": "Document layout analysis is essential for downstream tasks such as information retrieval, extraction, OCR, and digitization. However, existing large-scale datasets like PubLayNet and DocBank lack fine-grained region labels and multilingual diversity, making them insufficient for representing complex document layouts. In contrast, human-annotated datasets such as M6Doc and D4LA offer richer labels and greater domain diversity, but are too small to train robust models and lack adequate multilingual coverage. This gap is especially pronounced for Indic documents, which encompass diverse scripts yet remain underrepresented in current datasets, further limiting progress in this space. To address these shortcomings, we introduce IndicDLP, a large-scale foundational document layout dataset spanning 11 representative Indic languages alongside English and 12 common document domains. Additionally, we curate UED-mini, a dataset derived from DocLayNet and M6Doc, to enhance pretraining and provide a solid foundation for Indic layout models. Our experiments demonstrate that fine-tuning existing English models on IndicDLP significantly boosts performance, validating its effectiveness. Moreover, models trained on IndicDLP generalize well beyond Indic layouts, making it a valuable resource for document digitization. This work bridges gaps in scale, diversity, and annotation granularity, driving inclusive and efficient document understanding.", "link": "http://arxiv.org/abs/2512.20236v1", "date": "2025-12-23", "relevancy": 2.0882, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5734}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5172}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IndicDLP%3A%20A%20Foundational%20Dataset%20for%20Multi-Lingual%20and%20Multi-Domain%20Document%20Layout%20Parsing&body=Title%3A%20IndicDLP%3A%20A%20Foundational%20Dataset%20for%20Multi-Lingual%20and%20Multi-Domain%20Document%20Layout%20Parsing%0AAuthor%3A%20Oikantik%20Nath%20and%20Sahithi%20Kukkala%20and%20Mitesh%20Khapra%20and%20Ravi%20Kiran%20Sarvadevabhatla%0AAbstract%3A%20Document%20layout%20analysis%20is%20essential%20for%20downstream%20tasks%20such%20as%20information%20retrieval%2C%20extraction%2C%20OCR%2C%20and%20digitization.%20However%2C%20existing%20large-scale%20datasets%20like%20PubLayNet%20and%20DocBank%20lack%20fine-grained%20region%20labels%20and%20multilingual%20diversity%2C%20making%20them%20insufficient%20for%20representing%20complex%20document%20layouts.%20In%20contrast%2C%20human-annotated%20datasets%20such%20as%20M6Doc%20and%20D4LA%20offer%20richer%20labels%20and%20greater%20domain%20diversity%2C%20but%20are%20too%20small%20to%20train%20robust%20models%20and%20lack%20adequate%20multilingual%20coverage.%20This%20gap%20is%20especially%20pronounced%20for%20Indic%20documents%2C%20which%20encompass%20diverse%20scripts%20yet%20remain%20underrepresented%20in%20current%20datasets%2C%20further%20limiting%20progress%20in%20this%20space.%20To%20address%20these%20shortcomings%2C%20we%20introduce%20IndicDLP%2C%20a%20large-scale%20foundational%20document%20layout%20dataset%20spanning%2011%20representative%20Indic%20languages%20alongside%20English%20and%2012%20common%20document%20domains.%20Additionally%2C%20we%20curate%20UED-mini%2C%20a%20dataset%20derived%20from%20DocLayNet%20and%20M6Doc%2C%20to%20enhance%20pretraining%20and%20provide%20a%20solid%20foundation%20for%20Indic%20layout%20models.%20Our%20experiments%20demonstrate%20that%20fine-tuning%20existing%20English%20models%20on%20IndicDLP%20significantly%20boosts%20performance%2C%20validating%20its%20effectiveness.%20Moreover%2C%20models%20trained%20on%20IndicDLP%20generalize%20well%20beyond%20Indic%20layouts%2C%20making%20it%20a%20valuable%20resource%20for%20document%20digitization.%20This%20work%20bridges%20gaps%20in%20scale%2C%20diversity%2C%20and%20annotation%20granularity%2C%20driving%20inclusive%20and%20efficient%20document%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndicDLP%253A%2520A%2520Foundational%2520Dataset%2520for%2520Multi-Lingual%2520and%2520Multi-Domain%2520Document%2520Layout%2520Parsing%26entry.906535625%3DOikantik%2520Nath%2520and%2520Sahithi%2520Kukkala%2520and%2520Mitesh%2520Khapra%2520and%2520Ravi%2520Kiran%2520Sarvadevabhatla%26entry.1292438233%3DDocument%2520layout%2520analysis%2520is%2520essential%2520for%2520downstream%2520tasks%2520such%2520as%2520information%2520retrieval%252C%2520extraction%252C%2520OCR%252C%2520and%2520digitization.%2520However%252C%2520existing%2520large-scale%2520datasets%2520like%2520PubLayNet%2520and%2520DocBank%2520lack%2520fine-grained%2520region%2520labels%2520and%2520multilingual%2520diversity%252C%2520making%2520them%2520insufficient%2520for%2520representing%2520complex%2520document%2520layouts.%2520In%2520contrast%252C%2520human-annotated%2520datasets%2520such%2520as%2520M6Doc%2520and%2520D4LA%2520offer%2520richer%2520labels%2520and%2520greater%2520domain%2520diversity%252C%2520but%2520are%2520too%2520small%2520to%2520train%2520robust%2520models%2520and%2520lack%2520adequate%2520multilingual%2520coverage.%2520This%2520gap%2520is%2520especially%2520pronounced%2520for%2520Indic%2520documents%252C%2520which%2520encompass%2520diverse%2520scripts%2520yet%2520remain%2520underrepresented%2520in%2520current%2520datasets%252C%2520further%2520limiting%2520progress%2520in%2520this%2520space.%2520To%2520address%2520these%2520shortcomings%252C%2520we%2520introduce%2520IndicDLP%252C%2520a%2520large-scale%2520foundational%2520document%2520layout%2520dataset%2520spanning%252011%2520representative%2520Indic%2520languages%2520alongside%2520English%2520and%252012%2520common%2520document%2520domains.%2520Additionally%252C%2520we%2520curate%2520UED-mini%252C%2520a%2520dataset%2520derived%2520from%2520DocLayNet%2520and%2520M6Doc%252C%2520to%2520enhance%2520pretraining%2520and%2520provide%2520a%2520solid%2520foundation%2520for%2520Indic%2520layout%2520models.%2520Our%2520experiments%2520demonstrate%2520that%2520fine-tuning%2520existing%2520English%2520models%2520on%2520IndicDLP%2520significantly%2520boosts%2520performance%252C%2520validating%2520its%2520effectiveness.%2520Moreover%252C%2520models%2520trained%2520on%2520IndicDLP%2520generalize%2520well%2520beyond%2520Indic%2520layouts%252C%2520making%2520it%2520a%2520valuable%2520resource%2520for%2520document%2520digitization.%2520This%2520work%2520bridges%2520gaps%2520in%2520scale%252C%2520diversity%252C%2520and%2520annotation%2520granularity%252C%2520driving%2520inclusive%2520and%2520efficient%2520document%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IndicDLP%3A%20A%20Foundational%20Dataset%20for%20Multi-Lingual%20and%20Multi-Domain%20Document%20Layout%20Parsing&entry.906535625=Oikantik%20Nath%20and%20Sahithi%20Kukkala%20and%20Mitesh%20Khapra%20and%20Ravi%20Kiran%20Sarvadevabhatla&entry.1292438233=Document%20layout%20analysis%20is%20essential%20for%20downstream%20tasks%20such%20as%20information%20retrieval%2C%20extraction%2C%20OCR%2C%20and%20digitization.%20However%2C%20existing%20large-scale%20datasets%20like%20PubLayNet%20and%20DocBank%20lack%20fine-grained%20region%20labels%20and%20multilingual%20diversity%2C%20making%20them%20insufficient%20for%20representing%20complex%20document%20layouts.%20In%20contrast%2C%20human-annotated%20datasets%20such%20as%20M6Doc%20and%20D4LA%20offer%20richer%20labels%20and%20greater%20domain%20diversity%2C%20but%20are%20too%20small%20to%20train%20robust%20models%20and%20lack%20adequate%20multilingual%20coverage.%20This%20gap%20is%20especially%20pronounced%20for%20Indic%20documents%2C%20which%20encompass%20diverse%20scripts%20yet%20remain%20underrepresented%20in%20current%20datasets%2C%20further%20limiting%20progress%20in%20this%20space.%20To%20address%20these%20shortcomings%2C%20we%20introduce%20IndicDLP%2C%20a%20large-scale%20foundational%20document%20layout%20dataset%20spanning%2011%20representative%20Indic%20languages%20alongside%20English%20and%2012%20common%20document%20domains.%20Additionally%2C%20we%20curate%20UED-mini%2C%20a%20dataset%20derived%20from%20DocLayNet%20and%20M6Doc%2C%20to%20enhance%20pretraining%20and%20provide%20a%20solid%20foundation%20for%20Indic%20layout%20models.%20Our%20experiments%20demonstrate%20that%20fine-tuning%20existing%20English%20models%20on%20IndicDLP%20significantly%20boosts%20performance%2C%20validating%20its%20effectiveness.%20Moreover%2C%20models%20trained%20on%20IndicDLP%20generalize%20well%20beyond%20Indic%20layouts%2C%20making%20it%20a%20valuable%20resource%20for%20document%20digitization.%20This%20work%20bridges%20gaps%20in%20scale%2C%20diversity%2C%20and%20annotation%20granularity%2C%20driving%20inclusive%20and%20efficient%20document%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2512.20236v1&entry.124074799=Read"},
{"title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "author": "Seijin Kobayashi and Yanick Schimpf and Maximilian Schlegel and Angelika Steger and Maciej Wolczyk and Johannes von Oswald and Nino Scherre and Kaitlin Maile and Guillaume Lajoie and Blake A. Richards and Rif A. Saurous and James Manyika and Blaise Ag\u00fcera y Arcas and Alexander Meulemans and Jo\u00e3o Sacramento", "abstract": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "link": "http://arxiv.org/abs/2512.20605v1", "date": "2025-12-23", "relevancy": 2.0817, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5381}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5275}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergent%20temporal%20abstractions%20in%20autoregressive%20models%20enable%20hierarchical%20reinforcement%20learning&body=Title%3A%20Emergent%20temporal%20abstractions%20in%20autoregressive%20models%20enable%20hierarchical%20reinforcement%20learning%0AAuthor%3A%20Seijin%20Kobayashi%20and%20Yanick%20Schimpf%20and%20Maximilian%20Schlegel%20and%20Angelika%20Steger%20and%20Maciej%20Wolczyk%20and%20Johannes%20von%20Oswald%20and%20Nino%20Scherre%20and%20Kaitlin%20Maile%20and%20Guillaume%20Lajoie%20and%20Blake%20A.%20Richards%20and%20Rif%20A.%20Saurous%20and%20James%20Manyika%20and%20Blaise%20Ag%C3%BCera%20y%20Arcas%20and%20Alexander%20Meulemans%20and%20Jo%C3%A3o%20Sacramento%0AAbstract%3A%20Large-scale%20autoregressive%20models%20pretrained%20on%20next-token%20prediction%20and%20finetuned%20with%20reinforcement%20learning%20%28RL%29%20have%20achieved%20unprecedented%20success%20on%20many%20problem%20domains.%20During%20RL%2C%20these%20models%20explore%20by%20generating%20new%20outputs%2C%20one%20token%20at%20a%20time.%20However%2C%20sampling%20actions%20token-by-token%20can%20result%20in%20highly%20inefficient%20learning%2C%20particularly%20when%20rewards%20are%20sparse.%20Here%2C%20we%20show%20that%20it%20is%20possible%20to%20overcome%20this%20problem%20by%20acting%20and%20exploring%20within%20the%20internal%20representations%20of%20an%20autoregressive%20model.%20Specifically%2C%20to%20discover%20temporally-abstract%20actions%2C%20we%20introduce%20a%20higher-order%2C%20non-causal%20sequence%20model%20whose%20outputs%20control%20the%20residual%20stream%20activations%20of%20a%20base%20autoregressive%20model.%20On%20grid%20world%20and%20MuJoCo-based%20tasks%20with%20hierarchical%20structure%2C%20we%20find%20that%20the%20higher-order%20model%20learns%20to%20compress%20long%20activation%20sequence%20chunks%20onto%20internal%20controllers.%20Critically%2C%20each%20controller%20executes%20a%20sequence%20of%20behaviorally%20meaningful%20actions%20that%20unfold%20over%20long%20timescales%20and%20are%20accompanied%20with%20a%20learned%20termination%20condition%2C%20such%20that%20composing%20multiple%20controllers%20over%20time%20leads%20to%20efficient%20exploration%20on%20novel%20tasks.%20We%20show%20that%20direct%20internal%20controller%20reinforcement%2C%20a%20process%20we%20term%20%22internal%20RL%22%2C%20enables%20learning%20from%20sparse%20rewards%20in%20cases%20where%20standard%20RL%20finetuning%20fails.%20Our%20results%20demonstrate%20the%20benefits%20of%20latent%20action%20generation%20and%20reinforcement%20in%20autoregressive%20models%2C%20suggesting%20internal%20RL%20as%20a%20promising%20avenue%20for%20realizing%20hierarchical%20RL%20within%20foundation%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergent%2520temporal%2520abstractions%2520in%2520autoregressive%2520models%2520enable%2520hierarchical%2520reinforcement%2520learning%26entry.906535625%3DSeijin%2520Kobayashi%2520and%2520Yanick%2520Schimpf%2520and%2520Maximilian%2520Schlegel%2520and%2520Angelika%2520Steger%2520and%2520Maciej%2520Wolczyk%2520and%2520Johannes%2520von%2520Oswald%2520and%2520Nino%2520Scherre%2520and%2520Kaitlin%2520Maile%2520and%2520Guillaume%2520Lajoie%2520and%2520Blake%2520A.%2520Richards%2520and%2520Rif%2520A.%2520Saurous%2520and%2520James%2520Manyika%2520and%2520Blaise%2520Ag%25C3%25BCera%2520y%2520Arcas%2520and%2520Alexander%2520Meulemans%2520and%2520Jo%25C3%25A3o%2520Sacramento%26entry.1292438233%3DLarge-scale%2520autoregressive%2520models%2520pretrained%2520on%2520next-token%2520prediction%2520and%2520finetuned%2520with%2520reinforcement%2520learning%2520%2528RL%2529%2520have%2520achieved%2520unprecedented%2520success%2520on%2520many%2520problem%2520domains.%2520During%2520RL%252C%2520these%2520models%2520explore%2520by%2520generating%2520new%2520outputs%252C%2520one%2520token%2520at%2520a%2520time.%2520However%252C%2520sampling%2520actions%2520token-by-token%2520can%2520result%2520in%2520highly%2520inefficient%2520learning%252C%2520particularly%2520when%2520rewards%2520are%2520sparse.%2520Here%252C%2520we%2520show%2520that%2520it%2520is%2520possible%2520to%2520overcome%2520this%2520problem%2520by%2520acting%2520and%2520exploring%2520within%2520the%2520internal%2520representations%2520of%2520an%2520autoregressive%2520model.%2520Specifically%252C%2520to%2520discover%2520temporally-abstract%2520actions%252C%2520we%2520introduce%2520a%2520higher-order%252C%2520non-causal%2520sequence%2520model%2520whose%2520outputs%2520control%2520the%2520residual%2520stream%2520activations%2520of%2520a%2520base%2520autoregressive%2520model.%2520On%2520grid%2520world%2520and%2520MuJoCo-based%2520tasks%2520with%2520hierarchical%2520structure%252C%2520we%2520find%2520that%2520the%2520higher-order%2520model%2520learns%2520to%2520compress%2520long%2520activation%2520sequence%2520chunks%2520onto%2520internal%2520controllers.%2520Critically%252C%2520each%2520controller%2520executes%2520a%2520sequence%2520of%2520behaviorally%2520meaningful%2520actions%2520that%2520unfold%2520over%2520long%2520timescales%2520and%2520are%2520accompanied%2520with%2520a%2520learned%2520termination%2520condition%252C%2520such%2520that%2520composing%2520multiple%2520controllers%2520over%2520time%2520leads%2520to%2520efficient%2520exploration%2520on%2520novel%2520tasks.%2520We%2520show%2520that%2520direct%2520internal%2520controller%2520reinforcement%252C%2520a%2520process%2520we%2520term%2520%2522internal%2520RL%2522%252C%2520enables%2520learning%2520from%2520sparse%2520rewards%2520in%2520cases%2520where%2520standard%2520RL%2520finetuning%2520fails.%2520Our%2520results%2520demonstrate%2520the%2520benefits%2520of%2520latent%2520action%2520generation%2520and%2520reinforcement%2520in%2520autoregressive%2520models%252C%2520suggesting%2520internal%2520RL%2520as%2520a%2520promising%2520avenue%2520for%2520realizing%2520hierarchical%2520RL%2520within%2520foundation%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergent%20temporal%20abstractions%20in%20autoregressive%20models%20enable%20hierarchical%20reinforcement%20learning&entry.906535625=Seijin%20Kobayashi%20and%20Yanick%20Schimpf%20and%20Maximilian%20Schlegel%20and%20Angelika%20Steger%20and%20Maciej%20Wolczyk%20and%20Johannes%20von%20Oswald%20and%20Nino%20Scherre%20and%20Kaitlin%20Maile%20and%20Guillaume%20Lajoie%20and%20Blake%20A.%20Richards%20and%20Rif%20A.%20Saurous%20and%20James%20Manyika%20and%20Blaise%20Ag%C3%BCera%20y%20Arcas%20and%20Alexander%20Meulemans%20and%20Jo%C3%A3o%20Sacramento&entry.1292438233=Large-scale%20autoregressive%20models%20pretrained%20on%20next-token%20prediction%20and%20finetuned%20with%20reinforcement%20learning%20%28RL%29%20have%20achieved%20unprecedented%20success%20on%20many%20problem%20domains.%20During%20RL%2C%20these%20models%20explore%20by%20generating%20new%20outputs%2C%20one%20token%20at%20a%20time.%20However%2C%20sampling%20actions%20token-by-token%20can%20result%20in%20highly%20inefficient%20learning%2C%20particularly%20when%20rewards%20are%20sparse.%20Here%2C%20we%20show%20that%20it%20is%20possible%20to%20overcome%20this%20problem%20by%20acting%20and%20exploring%20within%20the%20internal%20representations%20of%20an%20autoregressive%20model.%20Specifically%2C%20to%20discover%20temporally-abstract%20actions%2C%20we%20introduce%20a%20higher-order%2C%20non-causal%20sequence%20model%20whose%20outputs%20control%20the%20residual%20stream%20activations%20of%20a%20base%20autoregressive%20model.%20On%20grid%20world%20and%20MuJoCo-based%20tasks%20with%20hierarchical%20structure%2C%20we%20find%20that%20the%20higher-order%20model%20learns%20to%20compress%20long%20activation%20sequence%20chunks%20onto%20internal%20controllers.%20Critically%2C%20each%20controller%20executes%20a%20sequence%20of%20behaviorally%20meaningful%20actions%20that%20unfold%20over%20long%20timescales%20and%20are%20accompanied%20with%20a%20learned%20termination%20condition%2C%20such%20that%20composing%20multiple%20controllers%20over%20time%20leads%20to%20efficient%20exploration%20on%20novel%20tasks.%20We%20show%20that%20direct%20internal%20controller%20reinforcement%2C%20a%20process%20we%20term%20%22internal%20RL%22%2C%20enables%20learning%20from%20sparse%20rewards%20in%20cases%20where%20standard%20RL%20finetuning%20fails.%20Our%20results%20demonstrate%20the%20benefits%20of%20latent%20action%20generation%20and%20reinforcement%20in%20autoregressive%20models%2C%20suggesting%20internal%20RL%20as%20a%20promising%20avenue%20for%20realizing%20hierarchical%20RL%20within%20foundation%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.20605v1&entry.124074799=Read"},
{"title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers", "author": "Wenzheng Zeng and Mingyu Ouyang and Langyuan Cui and Hwee Tou Ng", "abstract": "Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.", "link": "http://arxiv.org/abs/2512.20292v1", "date": "2025-12-23", "relevancy": 2.0754, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5324}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.529}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlideTailor%3A%20Personalized%20Presentation%20Slide%20Generation%20for%20Scientific%20Papers&body=Title%3A%20SlideTailor%3A%20Personalized%20Presentation%20Slide%20Generation%20for%20Scientific%20Papers%0AAuthor%3A%20Wenzheng%20Zeng%20and%20Mingyu%20Ouyang%20and%20Langyuan%20Cui%20and%20Hwee%20Tou%20Ng%0AAbstract%3A%20Automatic%20presentation%20slide%20generation%20can%20greatly%20streamline%20content%20creation.%20However%2C%20since%20preferences%20of%20each%20user%20may%20vary%2C%20existing%20under-specified%20formulations%20often%20lead%20to%20suboptimal%20results%20that%20fail%20to%20align%20with%20individual%20user%20needs.%20We%20introduce%20a%20novel%20task%20that%20conditions%20paper-to-slides%20generation%20on%20user-specified%20preferences.%20We%20propose%20a%20human%20behavior-inspired%20agentic%20framework%2C%20SlideTailor%2C%20that%20progressively%20generates%20editable%20slides%20in%20a%20user-aligned%20manner.%20Instead%20of%20requiring%20users%20to%20write%20their%20preferences%20in%20detailed%20textual%20form%2C%20our%20system%20only%20asks%20for%20a%20paper-slides%20example%20pair%20and%20a%20visual%20template%20-%20natural%20and%20easy-to-provide%20artifacts%20that%20implicitly%20encode%20rich%20user%20preferences%20across%20content%20and%20visual%20style.%20Despite%20the%20implicit%20and%20unlabeled%20nature%20of%20these%20inputs%2C%20our%20framework%20effectively%20distills%20and%20generalizes%20the%20preferences%20to%20guide%20customized%20slide%20generation.%20We%20also%20introduce%20a%20novel%20chain-of-speech%20mechanism%20to%20align%20slide%20content%20with%20planned%20oral%20narration.%20Such%20a%20design%20significantly%20enhances%20the%20quality%20of%20generated%20slides%20and%20enables%20downstream%20applications%20like%20video%20presentations.%20To%20support%20this%20new%20task%2C%20we%20construct%20a%20benchmark%20dataset%20that%20captures%20diverse%20user%20preferences%2C%20with%20carefully%20designed%20interpretable%20metrics%20for%20robust%20evaluation.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlideTailor%253A%2520Personalized%2520Presentation%2520Slide%2520Generation%2520for%2520Scientific%2520Papers%26entry.906535625%3DWenzheng%2520Zeng%2520and%2520Mingyu%2520Ouyang%2520and%2520Langyuan%2520Cui%2520and%2520Hwee%2520Tou%2520Ng%26entry.1292438233%3DAutomatic%2520presentation%2520slide%2520generation%2520can%2520greatly%2520streamline%2520content%2520creation.%2520However%252C%2520since%2520preferences%2520of%2520each%2520user%2520may%2520vary%252C%2520existing%2520under-specified%2520formulations%2520often%2520lead%2520to%2520suboptimal%2520results%2520that%2520fail%2520to%2520align%2520with%2520individual%2520user%2520needs.%2520We%2520introduce%2520a%2520novel%2520task%2520that%2520conditions%2520paper-to-slides%2520generation%2520on%2520user-specified%2520preferences.%2520We%2520propose%2520a%2520human%2520behavior-inspired%2520agentic%2520framework%252C%2520SlideTailor%252C%2520that%2520progressively%2520generates%2520editable%2520slides%2520in%2520a%2520user-aligned%2520manner.%2520Instead%2520of%2520requiring%2520users%2520to%2520write%2520their%2520preferences%2520in%2520detailed%2520textual%2520form%252C%2520our%2520system%2520only%2520asks%2520for%2520a%2520paper-slides%2520example%2520pair%2520and%2520a%2520visual%2520template%2520-%2520natural%2520and%2520easy-to-provide%2520artifacts%2520that%2520implicitly%2520encode%2520rich%2520user%2520preferences%2520across%2520content%2520and%2520visual%2520style.%2520Despite%2520the%2520implicit%2520and%2520unlabeled%2520nature%2520of%2520these%2520inputs%252C%2520our%2520framework%2520effectively%2520distills%2520and%2520generalizes%2520the%2520preferences%2520to%2520guide%2520customized%2520slide%2520generation.%2520We%2520also%2520introduce%2520a%2520novel%2520chain-of-speech%2520mechanism%2520to%2520align%2520slide%2520content%2520with%2520planned%2520oral%2520narration.%2520Such%2520a%2520design%2520significantly%2520enhances%2520the%2520quality%2520of%2520generated%2520slides%2520and%2520enables%2520downstream%2520applications%2520like%2520video%2520presentations.%2520To%2520support%2520this%2520new%2520task%252C%2520we%2520construct%2520a%2520benchmark%2520dataset%2520that%2520captures%2520diverse%2520user%2520preferences%252C%2520with%2520carefully%2520designed%2520interpretable%2520metrics%2520for%2520robust%2520evaluation.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlideTailor%3A%20Personalized%20Presentation%20Slide%20Generation%20for%20Scientific%20Papers&entry.906535625=Wenzheng%20Zeng%20and%20Mingyu%20Ouyang%20and%20Langyuan%20Cui%20and%20Hwee%20Tou%20Ng&entry.1292438233=Automatic%20presentation%20slide%20generation%20can%20greatly%20streamline%20content%20creation.%20However%2C%20since%20preferences%20of%20each%20user%20may%20vary%2C%20existing%20under-specified%20formulations%20often%20lead%20to%20suboptimal%20results%20that%20fail%20to%20align%20with%20individual%20user%20needs.%20We%20introduce%20a%20novel%20task%20that%20conditions%20paper-to-slides%20generation%20on%20user-specified%20preferences.%20We%20propose%20a%20human%20behavior-inspired%20agentic%20framework%2C%20SlideTailor%2C%20that%20progressively%20generates%20editable%20slides%20in%20a%20user-aligned%20manner.%20Instead%20of%20requiring%20users%20to%20write%20their%20preferences%20in%20detailed%20textual%20form%2C%20our%20system%20only%20asks%20for%20a%20paper-slides%20example%20pair%20and%20a%20visual%20template%20-%20natural%20and%20easy-to-provide%20artifacts%20that%20implicitly%20encode%20rich%20user%20preferences%20across%20content%20and%20visual%20style.%20Despite%20the%20implicit%20and%20unlabeled%20nature%20of%20these%20inputs%2C%20our%20framework%20effectively%20distills%20and%20generalizes%20the%20preferences%20to%20guide%20customized%20slide%20generation.%20We%20also%20introduce%20a%20novel%20chain-of-speech%20mechanism%20to%20align%20slide%20content%20with%20planned%20oral%20narration.%20Such%20a%20design%20significantly%20enhances%20the%20quality%20of%20generated%20slides%20and%20enables%20downstream%20applications%20like%20video%20presentations.%20To%20support%20this%20new%20task%2C%20we%20construct%20a%20benchmark%20dataset%20that%20captures%20diverse%20user%20preferences%2C%20with%20carefully%20designed%20interpretable%20metrics%20for%20robust%20evaluation.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2512.20292v1&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming", "author": "Ningwei Bai and Chi Pui Chan and Qichen Yin and Tengyang Gong and Yunda Yan and Zezhi Tang", "abstract": "This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.", "link": "http://arxiv.org/abs/2512.15735v3", "date": "2025-12-23", "relevancy": 2.0515, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5354}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5037}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning%20Optimization%20for%20Uncertain%20Nonlinear%20Systems%20via%20Event-Triggered%20Robust%20Adaptive%20Dynamic%20Programming&body=Title%3A%20Deep%20Reinforcement%20Learning%20Optimization%20for%20Uncertain%20Nonlinear%20Systems%20via%20Event-Triggered%20Robust%20Adaptive%20Dynamic%20Programming%0AAuthor%3A%20Ningwei%20Bai%20and%20Chi%20Pui%20Chan%20and%20Qichen%20Yin%20and%20Tengyang%20Gong%20and%20Yunda%20Yan%20and%20Zezhi%20Tang%0AAbstract%3A%20This%20work%20proposes%20a%20unified%20control%20architecture%20that%20couples%20a%20Reinforcement%20Learning%20%28RL%29-driven%20controller%20with%20a%20disturbance-rejection%20Extended%20State%20Observer%20%28ESO%29%2C%20complemented%20by%20an%20Event-Triggered%20Mechanism%20%28ETM%29%20to%20limit%20unnecessary%20computations.%20The%20ESO%20is%20utilized%20to%20estimate%20the%20system%20states%20and%20the%20lumped%20disturbance%20in%20real%20time%2C%20forming%20the%20foundation%20for%20effective%20disturbance%20compensation.%20To%20obtain%20near-optimal%20behavior%20without%20an%20accurate%20system%20description%2C%20a%20value-iteration-based%20Adaptive%20Dynamic%20Programming%20%28ADP%29%20method%20is%20adopted%20for%20policy%20approximation.%20The%20inclusion%20of%20the%20ETM%20ensures%20that%20parameter%20updates%20of%20the%20learning%20module%20are%20executed%20only%20when%20the%20state%20deviation%20surpasses%20a%20predefined%20bound%2C%20thereby%20preventing%20excessive%20learning%20activity%20and%20substantially%20reducing%20computational%20load.%20A%20Lyapunov-oriented%20analysis%20is%20used%20to%20characterize%20the%20stability%20properties%20of%20the%20resulting%20closed-loop%20system.%20Numerical%20experiments%20further%20confirm%20that%20the%20developed%20approach%20maintains%20strong%20control%20performance%20and%20disturbance%20tolerance%2C%20while%20achieving%20a%20significant%20reduction%20in%20sampling%20and%20processing%20effort%20compared%20with%20standard%20time-triggered%20ADP%20schemes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15735v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Reinforcement%2520Learning%2520Optimization%2520for%2520Uncertain%2520Nonlinear%2520Systems%2520via%2520Event-Triggered%2520Robust%2520Adaptive%2520Dynamic%2520Programming%26entry.906535625%3DNingwei%2520Bai%2520and%2520Chi%2520Pui%2520Chan%2520and%2520Qichen%2520Yin%2520and%2520Tengyang%2520Gong%2520and%2520Yunda%2520Yan%2520and%2520Zezhi%2520Tang%26entry.1292438233%3DThis%2520work%2520proposes%2520a%2520unified%2520control%2520architecture%2520that%2520couples%2520a%2520Reinforcement%2520Learning%2520%2528RL%2529-driven%2520controller%2520with%2520a%2520disturbance-rejection%2520Extended%2520State%2520Observer%2520%2528ESO%2529%252C%2520complemented%2520by%2520an%2520Event-Triggered%2520Mechanism%2520%2528ETM%2529%2520to%2520limit%2520unnecessary%2520computations.%2520The%2520ESO%2520is%2520utilized%2520to%2520estimate%2520the%2520system%2520states%2520and%2520the%2520lumped%2520disturbance%2520in%2520real%2520time%252C%2520forming%2520the%2520foundation%2520for%2520effective%2520disturbance%2520compensation.%2520To%2520obtain%2520near-optimal%2520behavior%2520without%2520an%2520accurate%2520system%2520description%252C%2520a%2520value-iteration-based%2520Adaptive%2520Dynamic%2520Programming%2520%2528ADP%2529%2520method%2520is%2520adopted%2520for%2520policy%2520approximation.%2520The%2520inclusion%2520of%2520the%2520ETM%2520ensures%2520that%2520parameter%2520updates%2520of%2520the%2520learning%2520module%2520are%2520executed%2520only%2520when%2520the%2520state%2520deviation%2520surpasses%2520a%2520predefined%2520bound%252C%2520thereby%2520preventing%2520excessive%2520learning%2520activity%2520and%2520substantially%2520reducing%2520computational%2520load.%2520A%2520Lyapunov-oriented%2520analysis%2520is%2520used%2520to%2520characterize%2520the%2520stability%2520properties%2520of%2520the%2520resulting%2520closed-loop%2520system.%2520Numerical%2520experiments%2520further%2520confirm%2520that%2520the%2520developed%2520approach%2520maintains%2520strong%2520control%2520performance%2520and%2520disturbance%2520tolerance%252C%2520while%2520achieving%2520a%2520significant%2520reduction%2520in%2520sampling%2520and%2520processing%2520effort%2520compared%2520with%2520standard%2520time-triggered%2520ADP%2520schemes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15735v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%20Optimization%20for%20Uncertain%20Nonlinear%20Systems%20via%20Event-Triggered%20Robust%20Adaptive%20Dynamic%20Programming&entry.906535625=Ningwei%20Bai%20and%20Chi%20Pui%20Chan%20and%20Qichen%20Yin%20and%20Tengyang%20Gong%20and%20Yunda%20Yan%20and%20Zezhi%20Tang&entry.1292438233=This%20work%20proposes%20a%20unified%20control%20architecture%20that%20couples%20a%20Reinforcement%20Learning%20%28RL%29-driven%20controller%20with%20a%20disturbance-rejection%20Extended%20State%20Observer%20%28ESO%29%2C%20complemented%20by%20an%20Event-Triggered%20Mechanism%20%28ETM%29%20to%20limit%20unnecessary%20computations.%20The%20ESO%20is%20utilized%20to%20estimate%20the%20system%20states%20and%20the%20lumped%20disturbance%20in%20real%20time%2C%20forming%20the%20foundation%20for%20effective%20disturbance%20compensation.%20To%20obtain%20near-optimal%20behavior%20without%20an%20accurate%20system%20description%2C%20a%20value-iteration-based%20Adaptive%20Dynamic%20Programming%20%28ADP%29%20method%20is%20adopted%20for%20policy%20approximation.%20The%20inclusion%20of%20the%20ETM%20ensures%20that%20parameter%20updates%20of%20the%20learning%20module%20are%20executed%20only%20when%20the%20state%20deviation%20surpasses%20a%20predefined%20bound%2C%20thereby%20preventing%20excessive%20learning%20activity%20and%20substantially%20reducing%20computational%20load.%20A%20Lyapunov-oriented%20analysis%20is%20used%20to%20characterize%20the%20stability%20properties%20of%20the%20resulting%20closed-loop%20system.%20Numerical%20experiments%20further%20confirm%20that%20the%20developed%20approach%20maintains%20strong%20control%20performance%20and%20disturbance%20tolerance%2C%20while%20achieving%20a%20significant%20reduction%20in%20sampling%20and%20processing%20effort%20compared%20with%20standard%20time-triggered%20ADP%20schemes.&entry.1838667208=http%3A//arxiv.org/abs/2512.15735v3&entry.124074799=Read"},
{"title": "Heartcare Suite: A Unified Multimodal ECG Suite for Dual Signal-Image Modeling and Understanding", "author": "Yihan Xie and Sijing Li and Tianwei Lin and Zhuonan Wang and Chenglin Yang and Yu Zhong and Wenjie Yan and Wenqiao Zhang and Xiaogang Guo and Jun Xiao and Yueting Zhuang and Beng Chin Ooi", "abstract": "Although electrocardiograms (ECG) play a dominant role in cardiovascular diagnosis and treatment, their intrinsic data forms and representational patterns pose significant challenges for medical multimodal large language models (Med-MLLMs) in achieving cross-modal semantic alignment. To address this gap, we propose Heartcare Suite, a unified ECG suite designed for dual signal-image modeling and understanding. (i) Heartcare-400K: We build a finegrained ECG instruction dataset on top of our data pipeline engine--HeartAgent--by integrating 12,170 high quality clinical ECG reports from top hospitals with open-source data; (ii) Heartcare-Bench: a systematic benchmark assessing performance of models in multi-perspective ECG understanding and cross-modal generalization, providing guidance for optimizing ECG comprehension models; (iii) HeartcareGPT: built upon a structure-aware discrete tokenizer Beat, we propose the DSPA (Dual Stream Projection Alignment) paradigm--a dual encoder projection alignment mechanism enabling joint optimizing and modeling native ECG signal-image within a shared feature space. Heartcare achieves consistent improvements across diverse ECG understanding tasks, validating both the effectiveness of the unified modeling paradigm and the necessity of a high-quality data pipeline, and establishing a methodological foundation for extending Med-MLLMs toward physiological signal domains. Our project is available at https://github.com/DCDmllm/Heartcare-Suite .", "link": "http://arxiv.org/abs/2506.05831v3", "date": "2025-12-23", "relevancy": 2.0392, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5264}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5127}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heartcare%20Suite%3A%20A%20Unified%20Multimodal%20ECG%20Suite%20for%20Dual%20Signal-Image%20Modeling%20and%20Understanding&body=Title%3A%20Heartcare%20Suite%3A%20A%20Unified%20Multimodal%20ECG%20Suite%20for%20Dual%20Signal-Image%20Modeling%20and%20Understanding%0AAuthor%3A%20Yihan%20Xie%20and%20Sijing%20Li%20and%20Tianwei%20Lin%20and%20Zhuonan%20Wang%20and%20Chenglin%20Yang%20and%20Yu%20Zhong%20and%20Wenjie%20Yan%20and%20Wenqiao%20Zhang%20and%20Xiaogang%20Guo%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%20and%20Beng%20Chin%20Ooi%0AAbstract%3A%20Although%20electrocardiograms%20%28ECG%29%20play%20a%20dominant%20role%20in%20cardiovascular%20diagnosis%20and%20treatment%2C%20their%20intrinsic%20data%20forms%20and%20representational%20patterns%20pose%20significant%20challenges%20for%20medical%20multimodal%20large%20language%20models%20%28Med-MLLMs%29%20in%20achieving%20cross-modal%20semantic%20alignment.%20To%20address%20this%20gap%2C%20we%20propose%20Heartcare%20Suite%2C%20a%20unified%20ECG%20suite%20designed%20for%20dual%20signal-image%20modeling%20and%20understanding.%20%28i%29%20Heartcare-400K%3A%20We%20build%20a%20finegrained%20ECG%20instruction%20dataset%20on%20top%20of%20our%20data%20pipeline%20engine--HeartAgent--by%20integrating%2012%2C170%20high%20quality%20clinical%20ECG%20reports%20from%20top%20hospitals%20with%20open-source%20data%3B%20%28ii%29%20Heartcare-Bench%3A%20a%20systematic%20benchmark%20assessing%20performance%20of%20models%20in%20multi-perspective%20ECG%20understanding%20and%20cross-modal%20generalization%2C%20providing%20guidance%20for%20optimizing%20ECG%20comprehension%20models%3B%20%28iii%29%20HeartcareGPT%3A%20built%20upon%20a%20structure-aware%20discrete%20tokenizer%20Beat%2C%20we%20propose%20the%20DSPA%20%28Dual%20Stream%20Projection%20Alignment%29%20paradigm--a%20dual%20encoder%20projection%20alignment%20mechanism%20enabling%20joint%20optimizing%20and%20modeling%20native%20ECG%20signal-image%20within%20a%20shared%20feature%20space.%20Heartcare%20achieves%20consistent%20improvements%20across%20diverse%20ECG%20understanding%20tasks%2C%20validating%20both%20the%20effectiveness%20of%20the%20unified%20modeling%20paradigm%20and%20the%20necessity%20of%20a%20high-quality%20data%20pipeline%2C%20and%20establishing%20a%20methodological%20foundation%20for%20extending%20Med-MLLMs%20toward%20physiological%20signal%20domains.%20Our%20project%20is%20available%20at%20https%3A//github.com/DCDmllm/Heartcare-Suite%20.%0ALink%3A%20http%3A//arxiv.org/abs/2506.05831v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeartcare%2520Suite%253A%2520A%2520Unified%2520Multimodal%2520ECG%2520Suite%2520for%2520Dual%2520Signal-Image%2520Modeling%2520and%2520Understanding%26entry.906535625%3DYihan%2520Xie%2520and%2520Sijing%2520Li%2520and%2520Tianwei%2520Lin%2520and%2520Zhuonan%2520Wang%2520and%2520Chenglin%2520Yang%2520and%2520Yu%2520Zhong%2520and%2520Wenjie%2520Yan%2520and%2520Wenqiao%2520Zhang%2520and%2520Xiaogang%2520Guo%2520and%2520Jun%2520Xiao%2520and%2520Yueting%2520Zhuang%2520and%2520Beng%2520Chin%2520Ooi%26entry.1292438233%3DAlthough%2520electrocardiograms%2520%2528ECG%2529%2520play%2520a%2520dominant%2520role%2520in%2520cardiovascular%2520diagnosis%2520and%2520treatment%252C%2520their%2520intrinsic%2520data%2520forms%2520and%2520representational%2520patterns%2520pose%2520significant%2520challenges%2520for%2520medical%2520multimodal%2520large%2520language%2520models%2520%2528Med-MLLMs%2529%2520in%2520achieving%2520cross-modal%2520semantic%2520alignment.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520Heartcare%2520Suite%252C%2520a%2520unified%2520ECG%2520suite%2520designed%2520for%2520dual%2520signal-image%2520modeling%2520and%2520understanding.%2520%2528i%2529%2520Heartcare-400K%253A%2520We%2520build%2520a%2520finegrained%2520ECG%2520instruction%2520dataset%2520on%2520top%2520of%2520our%2520data%2520pipeline%2520engine--HeartAgent--by%2520integrating%252012%252C170%2520high%2520quality%2520clinical%2520ECG%2520reports%2520from%2520top%2520hospitals%2520with%2520open-source%2520data%253B%2520%2528ii%2529%2520Heartcare-Bench%253A%2520a%2520systematic%2520benchmark%2520assessing%2520performance%2520of%2520models%2520in%2520multi-perspective%2520ECG%2520understanding%2520and%2520cross-modal%2520generalization%252C%2520providing%2520guidance%2520for%2520optimizing%2520ECG%2520comprehension%2520models%253B%2520%2528iii%2529%2520HeartcareGPT%253A%2520built%2520upon%2520a%2520structure-aware%2520discrete%2520tokenizer%2520Beat%252C%2520we%2520propose%2520the%2520DSPA%2520%2528Dual%2520Stream%2520Projection%2520Alignment%2529%2520paradigm--a%2520dual%2520encoder%2520projection%2520alignment%2520mechanism%2520enabling%2520joint%2520optimizing%2520and%2520modeling%2520native%2520ECG%2520signal-image%2520within%2520a%2520shared%2520feature%2520space.%2520Heartcare%2520achieves%2520consistent%2520improvements%2520across%2520diverse%2520ECG%2520understanding%2520tasks%252C%2520validating%2520both%2520the%2520effectiveness%2520of%2520the%2520unified%2520modeling%2520paradigm%2520and%2520the%2520necessity%2520of%2520a%2520high-quality%2520data%2520pipeline%252C%2520and%2520establishing%2520a%2520methodological%2520foundation%2520for%2520extending%2520Med-MLLMs%2520toward%2520physiological%2520signal%2520domains.%2520Our%2520project%2520is%2520available%2520at%2520https%253A//github.com/DCDmllm/Heartcare-Suite%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05831v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heartcare%20Suite%3A%20A%20Unified%20Multimodal%20ECG%20Suite%20for%20Dual%20Signal-Image%20Modeling%20and%20Understanding&entry.906535625=Yihan%20Xie%20and%20Sijing%20Li%20and%20Tianwei%20Lin%20and%20Zhuonan%20Wang%20and%20Chenglin%20Yang%20and%20Yu%20Zhong%20and%20Wenjie%20Yan%20and%20Wenqiao%20Zhang%20and%20Xiaogang%20Guo%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%20and%20Beng%20Chin%20Ooi&entry.1292438233=Although%20electrocardiograms%20%28ECG%29%20play%20a%20dominant%20role%20in%20cardiovascular%20diagnosis%20and%20treatment%2C%20their%20intrinsic%20data%20forms%20and%20representational%20patterns%20pose%20significant%20challenges%20for%20medical%20multimodal%20large%20language%20models%20%28Med-MLLMs%29%20in%20achieving%20cross-modal%20semantic%20alignment.%20To%20address%20this%20gap%2C%20we%20propose%20Heartcare%20Suite%2C%20a%20unified%20ECG%20suite%20designed%20for%20dual%20signal-image%20modeling%20and%20understanding.%20%28i%29%20Heartcare-400K%3A%20We%20build%20a%20finegrained%20ECG%20instruction%20dataset%20on%20top%20of%20our%20data%20pipeline%20engine--HeartAgent--by%20integrating%2012%2C170%20high%20quality%20clinical%20ECG%20reports%20from%20top%20hospitals%20with%20open-source%20data%3B%20%28ii%29%20Heartcare-Bench%3A%20a%20systematic%20benchmark%20assessing%20performance%20of%20models%20in%20multi-perspective%20ECG%20understanding%20and%20cross-modal%20generalization%2C%20providing%20guidance%20for%20optimizing%20ECG%20comprehension%20models%3B%20%28iii%29%20HeartcareGPT%3A%20built%20upon%20a%20structure-aware%20discrete%20tokenizer%20Beat%2C%20we%20propose%20the%20DSPA%20%28Dual%20Stream%20Projection%20Alignment%29%20paradigm--a%20dual%20encoder%20projection%20alignment%20mechanism%20enabling%20joint%20optimizing%20and%20modeling%20native%20ECG%20signal-image%20within%20a%20shared%20feature%20space.%20Heartcare%20achieves%20consistent%20improvements%20across%20diverse%20ECG%20understanding%20tasks%2C%20validating%20both%20the%20effectiveness%20of%20the%20unified%20modeling%20paradigm%20and%20the%20necessity%20of%20a%20high-quality%20data%20pipeline%2C%20and%20establishing%20a%20methodological%20foundation%20for%20extending%20Med-MLLMs%20toward%20physiological%20signal%20domains.%20Our%20project%20is%20available%20at%20https%3A//github.com/DCDmllm/Heartcare-Suite%20.&entry.1838667208=http%3A//arxiv.org/abs/2506.05831v3&entry.124074799=Read"},
{"title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection", "author": "Yanhong Li and Songlin Yang and Shawn Tan and Mayank Mishra and Rameswar Panda and Jiawei Zhou and Yoon Kim", "abstract": "Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \\citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.", "link": "http://arxiv.org/abs/2512.20569v1", "date": "2025-12-23", "relevancy": 2.034, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5288}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5209}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilling%20to%20Hybrid%20Attention%20Models%20via%20KL-Guided%20Layer%20Selection&body=Title%3A%20Distilling%20to%20Hybrid%20Attention%20Models%20via%20KL-Guided%20Layer%20Selection%0AAuthor%3A%20Yanhong%20Li%20and%20Songlin%20Yang%20and%20Shawn%20Tan%20and%20Mayank%20Mishra%20and%20Rameswar%20Panda%20and%20Jiawei%20Zhou%20and%20Yoon%20Kim%0AAbstract%3A%20Distilling%20pretrained%20softmax%20attention%20Transformers%20into%20more%20efficient%20hybrid%20architectures%20that%20interleave%20softmax%20and%20linear%20attention%20layers%20is%20a%20promising%20approach%20for%20improving%20the%20inference%20efficiency%20of%20LLMs%20without%20requiring%20expensive%20pretraining%20from%20scratch.%20A%20critical%20factor%20in%20the%20conversion%20process%20is%20layer%20selection%2C%20i.e.%2C%20deciding%20on%20which%20layers%20to%20convert%20to%20linear%20attention%20variants.%20This%20paper%20describes%20a%20simple%20and%20efficient%20recipe%20for%20layer%20selection%20that%20uses%20layer%20importance%20scores%20derived%20from%20a%20small%20amount%20of%20training%20on%20generic%20text%20data.%20Once%20the%20layers%20have%20been%20selected%20we%20use%20a%20recent%20pipeline%20for%20the%20distillation%20process%20itself%20%5Ccitep%5BRADLADS%3B%5D%5B%5D%7Bgoldstein2025radlads%7D%2C%20which%20consists%20of%20attention%20weight%20transfer%2C%20hidden%20state%20alignment%2C%20KL-based%20distribution%20matching%2C%20followed%20by%20a%20small%20amount%20of%20finetuning.%20We%20find%20that%20this%20approach%20is%20more%20effective%20than%20existing%20approaches%20for%20layer%20selection%2C%20including%20heuristics%20that%20uniformly%20interleave%20linear%20attentions%20based%20on%20a%20fixed%20ratio%2C%20as%20well%20as%20more%20involved%20approaches%20that%20rely%20on%20specialized%20diagnostic%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilling%2520to%2520Hybrid%2520Attention%2520Models%2520via%2520KL-Guided%2520Layer%2520Selection%26entry.906535625%3DYanhong%2520Li%2520and%2520Songlin%2520Yang%2520and%2520Shawn%2520Tan%2520and%2520Mayank%2520Mishra%2520and%2520Rameswar%2520Panda%2520and%2520Jiawei%2520Zhou%2520and%2520Yoon%2520Kim%26entry.1292438233%3DDistilling%2520pretrained%2520softmax%2520attention%2520Transformers%2520into%2520more%2520efficient%2520hybrid%2520architectures%2520that%2520interleave%2520softmax%2520and%2520linear%2520attention%2520layers%2520is%2520a%2520promising%2520approach%2520for%2520improving%2520the%2520inference%2520efficiency%2520of%2520LLMs%2520without%2520requiring%2520expensive%2520pretraining%2520from%2520scratch.%2520A%2520critical%2520factor%2520in%2520the%2520conversion%2520process%2520is%2520layer%2520selection%252C%2520i.e.%252C%2520deciding%2520on%2520which%2520layers%2520to%2520convert%2520to%2520linear%2520attention%2520variants.%2520This%2520paper%2520describes%2520a%2520simple%2520and%2520efficient%2520recipe%2520for%2520layer%2520selection%2520that%2520uses%2520layer%2520importance%2520scores%2520derived%2520from%2520a%2520small%2520amount%2520of%2520training%2520on%2520generic%2520text%2520data.%2520Once%2520the%2520layers%2520have%2520been%2520selected%2520we%2520use%2520a%2520recent%2520pipeline%2520for%2520the%2520distillation%2520process%2520itself%2520%255Ccitep%255BRADLADS%253B%255D%255B%255D%257Bgoldstein2025radlads%257D%252C%2520which%2520consists%2520of%2520attention%2520weight%2520transfer%252C%2520hidden%2520state%2520alignment%252C%2520KL-based%2520distribution%2520matching%252C%2520followed%2520by%2520a%2520small%2520amount%2520of%2520finetuning.%2520We%2520find%2520that%2520this%2520approach%2520is%2520more%2520effective%2520than%2520existing%2520approaches%2520for%2520layer%2520selection%252C%2520including%2520heuristics%2520that%2520uniformly%2520interleave%2520linear%2520attentions%2520based%2520on%2520a%2520fixed%2520ratio%252C%2520as%2520well%2520as%2520more%2520involved%2520approaches%2520that%2520rely%2520on%2520specialized%2520diagnostic%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20to%20Hybrid%20Attention%20Models%20via%20KL-Guided%20Layer%20Selection&entry.906535625=Yanhong%20Li%20and%20Songlin%20Yang%20and%20Shawn%20Tan%20and%20Mayank%20Mishra%20and%20Rameswar%20Panda%20and%20Jiawei%20Zhou%20and%20Yoon%20Kim&entry.1292438233=Distilling%20pretrained%20softmax%20attention%20Transformers%20into%20more%20efficient%20hybrid%20architectures%20that%20interleave%20softmax%20and%20linear%20attention%20layers%20is%20a%20promising%20approach%20for%20improving%20the%20inference%20efficiency%20of%20LLMs%20without%20requiring%20expensive%20pretraining%20from%20scratch.%20A%20critical%20factor%20in%20the%20conversion%20process%20is%20layer%20selection%2C%20i.e.%2C%20deciding%20on%20which%20layers%20to%20convert%20to%20linear%20attention%20variants.%20This%20paper%20describes%20a%20simple%20and%20efficient%20recipe%20for%20layer%20selection%20that%20uses%20layer%20importance%20scores%20derived%20from%20a%20small%20amount%20of%20training%20on%20generic%20text%20data.%20Once%20the%20layers%20have%20been%20selected%20we%20use%20a%20recent%20pipeline%20for%20the%20distillation%20process%20itself%20%5Ccitep%5BRADLADS%3B%5D%5B%5D%7Bgoldstein2025radlads%7D%2C%20which%20consists%20of%20attention%20weight%20transfer%2C%20hidden%20state%20alignment%2C%20KL-based%20distribution%20matching%2C%20followed%20by%20a%20small%20amount%20of%20finetuning.%20We%20find%20that%20this%20approach%20is%20more%20effective%20than%20existing%20approaches%20for%20layer%20selection%2C%20including%20heuristics%20that%20uniformly%20interleave%20linear%20attentions%20based%20on%20a%20fixed%20ratio%2C%20as%20well%20as%20more%20involved%20approaches%20that%20rely%20on%20specialized%20diagnostic%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2512.20569v1&entry.124074799=Read"},
{"title": "Boosted Control Functions: Distribution generalization and invariance in confounded models", "author": "Nicola Gnecco and Jonas Peters and Sebastian Engelke and Niklas Pfister", "abstract": "Modern machine learning methods and the availability of large-scale data have significantly advanced our ability to predict target quantities from large sets of covariates. However, these methods often struggle under distributional shifts, particularly in the presence of hidden confounding. While the impact of hidden confounding is well-studied in causal effect estimation, e.g., instrumental variables, its implications for prediction tasks under shifting distributions remain underexplored. This work addresses this gap by introducing a strong notion of invariance that, unlike existing weaker notions, allows for distribution generalization even in the presence of nonlinear, non-identifiable structural functions. Central to this framework is the Boosted Control Function (BCF), a novel, identifiable target of inference that satisfies the proposed strong invariance notion and is provably worst-case optimal under distributional shifts. The theoretical foundation of our work lies in Simultaneous Equation Models for Distribution Generalization (SIMDGs), which bridge machine learning with econometrics by describing data-generating processes under distributional shifts. To put these insights into practice, we propose the ControlTwicing algorithm to estimate the BCF using nonparametric machine-learning techniques and study its generalization performance on synthetic and real-world datasets compared to robust and empirical risk minimization approaches.", "link": "http://arxiv.org/abs/2310.05805v3", "date": "2025-12-23", "relevancy": 2.0253, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5287}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4912}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosted%20Control%20Functions%3A%20Distribution%20generalization%20and%20invariance%20in%20confounded%20models&body=Title%3A%20Boosted%20Control%20Functions%3A%20Distribution%20generalization%20and%20invariance%20in%20confounded%20models%0AAuthor%3A%20Nicola%20Gnecco%20and%20Jonas%20Peters%20and%20Sebastian%20Engelke%20and%20Niklas%20Pfister%0AAbstract%3A%20Modern%20machine%20learning%20methods%20and%20the%20availability%20of%20large-scale%20data%20have%20significantly%20advanced%20our%20ability%20to%20predict%20target%20quantities%20from%20large%20sets%20of%20covariates.%20However%2C%20these%20methods%20often%20struggle%20under%20distributional%20shifts%2C%20particularly%20in%20the%20presence%20of%20hidden%20confounding.%20While%20the%20impact%20of%20hidden%20confounding%20is%20well-studied%20in%20causal%20effect%20estimation%2C%20e.g.%2C%20instrumental%20variables%2C%20its%20implications%20for%20prediction%20tasks%20under%20shifting%20distributions%20remain%20underexplored.%20This%20work%20addresses%20this%20gap%20by%20introducing%20a%20strong%20notion%20of%20invariance%20that%2C%20unlike%20existing%20weaker%20notions%2C%20allows%20for%20distribution%20generalization%20even%20in%20the%20presence%20of%20nonlinear%2C%20non-identifiable%20structural%20functions.%20Central%20to%20this%20framework%20is%20the%20Boosted%20Control%20Function%20%28BCF%29%2C%20a%20novel%2C%20identifiable%20target%20of%20inference%20that%20satisfies%20the%20proposed%20strong%20invariance%20notion%20and%20is%20provably%20worst-case%20optimal%20under%20distributional%20shifts.%20The%20theoretical%20foundation%20of%20our%20work%20lies%20in%20Simultaneous%20Equation%20Models%20for%20Distribution%20Generalization%20%28SIMDGs%29%2C%20which%20bridge%20machine%20learning%20with%20econometrics%20by%20describing%20data-generating%20processes%20under%20distributional%20shifts.%20To%20put%20these%20insights%20into%20practice%2C%20we%20propose%20the%20ControlTwicing%20algorithm%20to%20estimate%20the%20BCF%20using%20nonparametric%20machine-learning%20techniques%20and%20study%20its%20generalization%20performance%20on%20synthetic%20and%20real-world%20datasets%20compared%20to%20robust%20and%20empirical%20risk%20minimization%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2310.05805v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosted%2520Control%2520Functions%253A%2520Distribution%2520generalization%2520and%2520invariance%2520in%2520confounded%2520models%26entry.906535625%3DNicola%2520Gnecco%2520and%2520Jonas%2520Peters%2520and%2520Sebastian%2520Engelke%2520and%2520Niklas%2520Pfister%26entry.1292438233%3DModern%2520machine%2520learning%2520methods%2520and%2520the%2520availability%2520of%2520large-scale%2520data%2520have%2520significantly%2520advanced%2520our%2520ability%2520to%2520predict%2520target%2520quantities%2520from%2520large%2520sets%2520of%2520covariates.%2520However%252C%2520these%2520methods%2520often%2520struggle%2520under%2520distributional%2520shifts%252C%2520particularly%2520in%2520the%2520presence%2520of%2520hidden%2520confounding.%2520While%2520the%2520impact%2520of%2520hidden%2520confounding%2520is%2520well-studied%2520in%2520causal%2520effect%2520estimation%252C%2520e.g.%252C%2520instrumental%2520variables%252C%2520its%2520implications%2520for%2520prediction%2520tasks%2520under%2520shifting%2520distributions%2520remain%2520underexplored.%2520This%2520work%2520addresses%2520this%2520gap%2520by%2520introducing%2520a%2520strong%2520notion%2520of%2520invariance%2520that%252C%2520unlike%2520existing%2520weaker%2520notions%252C%2520allows%2520for%2520distribution%2520generalization%2520even%2520in%2520the%2520presence%2520of%2520nonlinear%252C%2520non-identifiable%2520structural%2520functions.%2520Central%2520to%2520this%2520framework%2520is%2520the%2520Boosted%2520Control%2520Function%2520%2528BCF%2529%252C%2520a%2520novel%252C%2520identifiable%2520target%2520of%2520inference%2520that%2520satisfies%2520the%2520proposed%2520strong%2520invariance%2520notion%2520and%2520is%2520provably%2520worst-case%2520optimal%2520under%2520distributional%2520shifts.%2520The%2520theoretical%2520foundation%2520of%2520our%2520work%2520lies%2520in%2520Simultaneous%2520Equation%2520Models%2520for%2520Distribution%2520Generalization%2520%2528SIMDGs%2529%252C%2520which%2520bridge%2520machine%2520learning%2520with%2520econometrics%2520by%2520describing%2520data-generating%2520processes%2520under%2520distributional%2520shifts.%2520To%2520put%2520these%2520insights%2520into%2520practice%252C%2520we%2520propose%2520the%2520ControlTwicing%2520algorithm%2520to%2520estimate%2520the%2520BCF%2520using%2520nonparametric%2520machine-learning%2520techniques%2520and%2520study%2520its%2520generalization%2520performance%2520on%2520synthetic%2520and%2520real-world%2520datasets%2520compared%2520to%2520robust%2520and%2520empirical%2520risk%2520minimization%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05805v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosted%20Control%20Functions%3A%20Distribution%20generalization%20and%20invariance%20in%20confounded%20models&entry.906535625=Nicola%20Gnecco%20and%20Jonas%20Peters%20and%20Sebastian%20Engelke%20and%20Niklas%20Pfister&entry.1292438233=Modern%20machine%20learning%20methods%20and%20the%20availability%20of%20large-scale%20data%20have%20significantly%20advanced%20our%20ability%20to%20predict%20target%20quantities%20from%20large%20sets%20of%20covariates.%20However%2C%20these%20methods%20often%20struggle%20under%20distributional%20shifts%2C%20particularly%20in%20the%20presence%20of%20hidden%20confounding.%20While%20the%20impact%20of%20hidden%20confounding%20is%20well-studied%20in%20causal%20effect%20estimation%2C%20e.g.%2C%20instrumental%20variables%2C%20its%20implications%20for%20prediction%20tasks%20under%20shifting%20distributions%20remain%20underexplored.%20This%20work%20addresses%20this%20gap%20by%20introducing%20a%20strong%20notion%20of%20invariance%20that%2C%20unlike%20existing%20weaker%20notions%2C%20allows%20for%20distribution%20generalization%20even%20in%20the%20presence%20of%20nonlinear%2C%20non-identifiable%20structural%20functions.%20Central%20to%20this%20framework%20is%20the%20Boosted%20Control%20Function%20%28BCF%29%2C%20a%20novel%2C%20identifiable%20target%20of%20inference%20that%20satisfies%20the%20proposed%20strong%20invariance%20notion%20and%20is%20provably%20worst-case%20optimal%20under%20distributional%20shifts.%20The%20theoretical%20foundation%20of%20our%20work%20lies%20in%20Simultaneous%20Equation%20Models%20for%20Distribution%20Generalization%20%28SIMDGs%29%2C%20which%20bridge%20machine%20learning%20with%20econometrics%20by%20describing%20data-generating%20processes%20under%20distributional%20shifts.%20To%20put%20these%20insights%20into%20practice%2C%20we%20propose%20the%20ControlTwicing%20algorithm%20to%20estimate%20the%20BCF%20using%20nonparametric%20machine-learning%20techniques%20and%20study%20its%20generalization%20performance%20on%20synthetic%20and%20real-world%20datasets%20compared%20to%20robust%20and%20empirical%20risk%20minimization%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2310.05805v3&entry.124074799=Read"},
{"title": "Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model", "author": "Zhiyi Duan and Xiangren Wang and Hongyu Yuan and Qianli Xing", "abstract": "Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.", "link": "http://arxiv.org/abs/2512.20548v1", "date": "2025-12-23", "relevancy": 2.0232, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5481}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4956}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Multimodal%20Teacher%20Sentiment%20Analysis%3AThe%20Large-Scale%20T-MED%20Dataset%20%26%20The%20Effective%20AAM-TSA%20Model&body=Title%3A%20Advancing%20Multimodal%20Teacher%20Sentiment%20Analysis%3AThe%20Large-Scale%20T-MED%20Dataset%20%26%20The%20Effective%20AAM-TSA%20Model%0AAuthor%3A%20Zhiyi%20Duan%20and%20Xiangren%20Wang%20and%20Hongyu%20Yuan%20and%20Qianli%20Xing%0AAbstract%3A%20Teachers%27%20emotional%20states%20are%20critical%20in%20educational%20scenarios%2C%20profoundly%20impacting%20teaching%20efficacy%2C%20student%20engagement%2C%20and%20learning%20achievements.%20However%2C%20existing%20studies%20often%20fail%20to%20accurately%20capture%20teachers%27%20emotions%20due%20to%20the%20performative%20nature%20and%20overlook%20the%20critical%20impact%20of%20instructional%20information%20on%20emotional%20expression.In%20this%20paper%2C%20we%20systematically%20investigate%20teacher%20sentiment%20analysis%20by%20building%20both%20the%20dataset%20and%20the%20model%20accordingly.%20We%20construct%20the%20first%20large-scale%20teacher%20multimodal%20sentiment%20analysis%20dataset%2C%20T-MED.To%20ensure%20labeling%20accuracy%20and%20efficiency%2C%20we%20employ%20a%20human-machine%20collaborative%20labeling%20process.The%20T-MED%20dataset%20includes%2014%2C938%20instances%20of%20teacher%20emotional%20data%20from%20250%20real%20classrooms%20across%2011%20subjects%20ranging%20from%20K-12%20to%20higher%20education%2C%20integrating%20multimodal%20text%2C%20audio%2C%20video%2C%20and%20instructional%20information.Furthermore%2C%20we%20propose%20a%20novel%20asymmetric%20attention-based%20multimodal%20teacher%20sentiment%20analysis%20model%2C%20AAM-TSA.AAM-TSA%20introduces%20an%20asymmetric%20attention%20mechanism%20and%20hierarchical%20gating%20unit%20to%20enable%20differentiated%20cross-modal%20feature%20fusion%20and%20precise%20emotional%20classification.%20Experimental%20results%20demonstrate%20that%20AAM-TSA%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20in%20terms%20of%20accuracy%20and%20interpretability%20on%20the%20T-MED%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Multimodal%2520Teacher%2520Sentiment%2520Analysis%253AThe%2520Large-Scale%2520T-MED%2520Dataset%2520%2526%2520The%2520Effective%2520AAM-TSA%2520Model%26entry.906535625%3DZhiyi%2520Duan%2520and%2520Xiangren%2520Wang%2520and%2520Hongyu%2520Yuan%2520and%2520Qianli%2520Xing%26entry.1292438233%3DTeachers%2527%2520emotional%2520states%2520are%2520critical%2520in%2520educational%2520scenarios%252C%2520profoundly%2520impacting%2520teaching%2520efficacy%252C%2520student%2520engagement%252C%2520and%2520learning%2520achievements.%2520However%252C%2520existing%2520studies%2520often%2520fail%2520to%2520accurately%2520capture%2520teachers%2527%2520emotions%2520due%2520to%2520the%2520performative%2520nature%2520and%2520overlook%2520the%2520critical%2520impact%2520of%2520instructional%2520information%2520on%2520emotional%2520expression.In%2520this%2520paper%252C%2520we%2520systematically%2520investigate%2520teacher%2520sentiment%2520analysis%2520by%2520building%2520both%2520the%2520dataset%2520and%2520the%2520model%2520accordingly.%2520We%2520construct%2520the%2520first%2520large-scale%2520teacher%2520multimodal%2520sentiment%2520analysis%2520dataset%252C%2520T-MED.To%2520ensure%2520labeling%2520accuracy%2520and%2520efficiency%252C%2520we%2520employ%2520a%2520human-machine%2520collaborative%2520labeling%2520process.The%2520T-MED%2520dataset%2520includes%252014%252C938%2520instances%2520of%2520teacher%2520emotional%2520data%2520from%2520250%2520real%2520classrooms%2520across%252011%2520subjects%2520ranging%2520from%2520K-12%2520to%2520higher%2520education%252C%2520integrating%2520multimodal%2520text%252C%2520audio%252C%2520video%252C%2520and%2520instructional%2520information.Furthermore%252C%2520we%2520propose%2520a%2520novel%2520asymmetric%2520attention-based%2520multimodal%2520teacher%2520sentiment%2520analysis%2520model%252C%2520AAM-TSA.AAM-TSA%2520introduces%2520an%2520asymmetric%2520attention%2520mechanism%2520and%2520hierarchical%2520gating%2520unit%2520to%2520enable%2520differentiated%2520cross-modal%2520feature%2520fusion%2520and%2520precise%2520emotional%2520classification.%2520Experimental%2520results%2520demonstrate%2520that%2520AAM-TSA%2520significantly%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520accuracy%2520and%2520interpretability%2520on%2520the%2520T-MED%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Multimodal%20Teacher%20Sentiment%20Analysis%3AThe%20Large-Scale%20T-MED%20Dataset%20%26%20The%20Effective%20AAM-TSA%20Model&entry.906535625=Zhiyi%20Duan%20and%20Xiangren%20Wang%20and%20Hongyu%20Yuan%20and%20Qianli%20Xing&entry.1292438233=Teachers%27%20emotional%20states%20are%20critical%20in%20educational%20scenarios%2C%20profoundly%20impacting%20teaching%20efficacy%2C%20student%20engagement%2C%20and%20learning%20achievements.%20However%2C%20existing%20studies%20often%20fail%20to%20accurately%20capture%20teachers%27%20emotions%20due%20to%20the%20performative%20nature%20and%20overlook%20the%20critical%20impact%20of%20instructional%20information%20on%20emotional%20expression.In%20this%20paper%2C%20we%20systematically%20investigate%20teacher%20sentiment%20analysis%20by%20building%20both%20the%20dataset%20and%20the%20model%20accordingly.%20We%20construct%20the%20first%20large-scale%20teacher%20multimodal%20sentiment%20analysis%20dataset%2C%20T-MED.To%20ensure%20labeling%20accuracy%20and%20efficiency%2C%20we%20employ%20a%20human-machine%20collaborative%20labeling%20process.The%20T-MED%20dataset%20includes%2014%2C938%20instances%20of%20teacher%20emotional%20data%20from%20250%20real%20classrooms%20across%2011%20subjects%20ranging%20from%20K-12%20to%20higher%20education%2C%20integrating%20multimodal%20text%2C%20audio%2C%20video%2C%20and%20instructional%20information.Furthermore%2C%20we%20propose%20a%20novel%20asymmetric%20attention-based%20multimodal%20teacher%20sentiment%20analysis%20model%2C%20AAM-TSA.AAM-TSA%20introduces%20an%20asymmetric%20attention%20mechanism%20and%20hierarchical%20gating%20unit%20to%20enable%20differentiated%20cross-modal%20feature%20fusion%20and%20precise%20emotional%20classification.%20Experimental%20results%20demonstrate%20that%20AAM-TSA%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20in%20terms%20of%20accuracy%20and%20interpretability%20on%20the%20T-MED%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2512.20548v1&entry.124074799=Read"},
{"title": "Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents", "author": "Prahaladh Chandrahasan and Jiahe Jin and Zhihan Zhang and Tevin Wang and Andy Tang and Lucy Mo and Morteza Ziyadi and Leonardo F. R. Ribeiro and Zimeng Qiu and Markus Dreyer and Akari Asai and Chenyan Xiong", "abstract": "Effectively evaluating deep research agents that autonomously search the web, analyze information, and generate reports remains a major challenge, particularly when it comes to assessing long reports and giving detailed feedback on their intermediate steps. To address these gaps, we introduce Deep Research Comparator, a platform that offers a holistic framework for deep research agent hosting, side-by-side comparison, fine-grained human feedback collection, and ranking calculation. Given a user query, our platform displays the final reports from two different agents along with their intermediate steps during generation. Annotators can evaluate the overall quality of final reports based on side-by-side comparison, and also provide detailed feedback separately by assessing intermediate steps or specific text spans within the final report. Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This scaffold serves as a baseline that facilitates the easy integration of various large language models to transform them into deep research agents for evaluation. To demonstrate the platform's utility for deep research agent development, we have collected real user preference data from 17 annotators on three deep research agents. A demo video of our platform can be found at https://www.youtube.com/watch?v=g4d2dnbdseg.", "link": "http://arxiv.org/abs/2507.05495v2", "date": "2025-12-23", "relevancy": 2.0157, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5051}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5051}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Research%20Comparator%3A%20A%20Platform%20For%20Fine-grained%20Human%20Annotations%20of%20Deep%20Research%20Agents&body=Title%3A%20Deep%20Research%20Comparator%3A%20A%20Platform%20For%20Fine-grained%20Human%20Annotations%20of%20Deep%20Research%20Agents%0AAuthor%3A%20Prahaladh%20Chandrahasan%20and%20Jiahe%20Jin%20and%20Zhihan%20Zhang%20and%20Tevin%20Wang%20and%20Andy%20Tang%20and%20Lucy%20Mo%20and%20Morteza%20Ziyadi%20and%20Leonardo%20F.%20R.%20Ribeiro%20and%20Zimeng%20Qiu%20and%20Markus%20Dreyer%20and%20Akari%20Asai%20and%20Chenyan%20Xiong%0AAbstract%3A%20Effectively%20evaluating%20deep%20research%20agents%20that%20autonomously%20search%20the%20web%2C%20analyze%20information%2C%20and%20generate%20reports%20remains%20a%20major%20challenge%2C%20particularly%20when%20it%20comes%20to%20assessing%20long%20reports%20and%20giving%20detailed%20feedback%20on%20their%20intermediate%20steps.%20To%20address%20these%20gaps%2C%20we%20introduce%20Deep%20Research%20Comparator%2C%20a%20platform%20that%20offers%20a%20holistic%20framework%20for%20deep%20research%20agent%20hosting%2C%20side-by-side%20comparison%2C%20fine-grained%20human%20feedback%20collection%2C%20and%20ranking%20calculation.%20Given%20a%20user%20query%2C%20our%20platform%20displays%20the%20final%20reports%20from%20two%20different%20agents%20along%20with%20their%20intermediate%20steps%20during%20generation.%20Annotators%20can%20evaluate%20the%20overall%20quality%20of%20final%20reports%20based%20on%20side-by-side%20comparison%2C%20and%20also%20provide%20detailed%20feedback%20separately%20by%20assessing%20intermediate%20steps%20or%20specific%20text%20spans%20within%20the%20final%20report.%20Furthermore%2C%20we%20develop%20Simple%20Deepresearch%2C%20an%20end-to-end%20agent%20scaffold.%20This%20scaffold%20serves%20as%20a%20baseline%20that%20facilitates%20the%20easy%20integration%20of%20various%20large%20language%20models%20to%20transform%20them%20into%20deep%20research%20agents%20for%20evaluation.%20To%20demonstrate%20the%20platform%27s%20utility%20for%20deep%20research%20agent%20development%2C%20we%20have%20collected%20real%20user%20preference%20data%20from%2017%20annotators%20on%20three%20deep%20research%20agents.%20A%20demo%20video%20of%20our%20platform%20can%20be%20found%20at%20https%3A//www.youtube.com/watch%3Fv%3Dg4d2dnbdseg.%0ALink%3A%20http%3A//arxiv.org/abs/2507.05495v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Research%2520Comparator%253A%2520A%2520Platform%2520For%2520Fine-grained%2520Human%2520Annotations%2520of%2520Deep%2520Research%2520Agents%26entry.906535625%3DPrahaladh%2520Chandrahasan%2520and%2520Jiahe%2520Jin%2520and%2520Zhihan%2520Zhang%2520and%2520Tevin%2520Wang%2520and%2520Andy%2520Tang%2520and%2520Lucy%2520Mo%2520and%2520Morteza%2520Ziyadi%2520and%2520Leonardo%2520F.%2520R.%2520Ribeiro%2520and%2520Zimeng%2520Qiu%2520and%2520Markus%2520Dreyer%2520and%2520Akari%2520Asai%2520and%2520Chenyan%2520Xiong%26entry.1292438233%3DEffectively%2520evaluating%2520deep%2520research%2520agents%2520that%2520autonomously%2520search%2520the%2520web%252C%2520analyze%2520information%252C%2520and%2520generate%2520reports%2520remains%2520a%2520major%2520challenge%252C%2520particularly%2520when%2520it%2520comes%2520to%2520assessing%2520long%2520reports%2520and%2520giving%2520detailed%2520feedback%2520on%2520their%2520intermediate%2520steps.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%2520Deep%2520Research%2520Comparator%252C%2520a%2520platform%2520that%2520offers%2520a%2520holistic%2520framework%2520for%2520deep%2520research%2520agent%2520hosting%252C%2520side-by-side%2520comparison%252C%2520fine-grained%2520human%2520feedback%2520collection%252C%2520and%2520ranking%2520calculation.%2520Given%2520a%2520user%2520query%252C%2520our%2520platform%2520displays%2520the%2520final%2520reports%2520from%2520two%2520different%2520agents%2520along%2520with%2520their%2520intermediate%2520steps%2520during%2520generation.%2520Annotators%2520can%2520evaluate%2520the%2520overall%2520quality%2520of%2520final%2520reports%2520based%2520on%2520side-by-side%2520comparison%252C%2520and%2520also%2520provide%2520detailed%2520feedback%2520separately%2520by%2520assessing%2520intermediate%2520steps%2520or%2520specific%2520text%2520spans%2520within%2520the%2520final%2520report.%2520Furthermore%252C%2520we%2520develop%2520Simple%2520Deepresearch%252C%2520an%2520end-to-end%2520agent%2520scaffold.%2520This%2520scaffold%2520serves%2520as%2520a%2520baseline%2520that%2520facilitates%2520the%2520easy%2520integration%2520of%2520various%2520large%2520language%2520models%2520to%2520transform%2520them%2520into%2520deep%2520research%2520agents%2520for%2520evaluation.%2520To%2520demonstrate%2520the%2520platform%2527s%2520utility%2520for%2520deep%2520research%2520agent%2520development%252C%2520we%2520have%2520collected%2520real%2520user%2520preference%2520data%2520from%252017%2520annotators%2520on%2520three%2520deep%2520research%2520agents.%2520A%2520demo%2520video%2520of%2520our%2520platform%2520can%2520be%2520found%2520at%2520https%253A//www.youtube.com/watch%253Fv%253Dg4d2dnbdseg.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05495v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Research%20Comparator%3A%20A%20Platform%20For%20Fine-grained%20Human%20Annotations%20of%20Deep%20Research%20Agents&entry.906535625=Prahaladh%20Chandrahasan%20and%20Jiahe%20Jin%20and%20Zhihan%20Zhang%20and%20Tevin%20Wang%20and%20Andy%20Tang%20and%20Lucy%20Mo%20and%20Morteza%20Ziyadi%20and%20Leonardo%20F.%20R.%20Ribeiro%20and%20Zimeng%20Qiu%20and%20Markus%20Dreyer%20and%20Akari%20Asai%20and%20Chenyan%20Xiong&entry.1292438233=Effectively%20evaluating%20deep%20research%20agents%20that%20autonomously%20search%20the%20web%2C%20analyze%20information%2C%20and%20generate%20reports%20remains%20a%20major%20challenge%2C%20particularly%20when%20it%20comes%20to%20assessing%20long%20reports%20and%20giving%20detailed%20feedback%20on%20their%20intermediate%20steps.%20To%20address%20these%20gaps%2C%20we%20introduce%20Deep%20Research%20Comparator%2C%20a%20platform%20that%20offers%20a%20holistic%20framework%20for%20deep%20research%20agent%20hosting%2C%20side-by-side%20comparison%2C%20fine-grained%20human%20feedback%20collection%2C%20and%20ranking%20calculation.%20Given%20a%20user%20query%2C%20our%20platform%20displays%20the%20final%20reports%20from%20two%20different%20agents%20along%20with%20their%20intermediate%20steps%20during%20generation.%20Annotators%20can%20evaluate%20the%20overall%20quality%20of%20final%20reports%20based%20on%20side-by-side%20comparison%2C%20and%20also%20provide%20detailed%20feedback%20separately%20by%20assessing%20intermediate%20steps%20or%20specific%20text%20spans%20within%20the%20final%20report.%20Furthermore%2C%20we%20develop%20Simple%20Deepresearch%2C%20an%20end-to-end%20agent%20scaffold.%20This%20scaffold%20serves%20as%20a%20baseline%20that%20facilitates%20the%20easy%20integration%20of%20various%20large%20language%20models%20to%20transform%20them%20into%20deep%20research%20agents%20for%20evaluation.%20To%20demonstrate%20the%20platform%27s%20utility%20for%20deep%20research%20agent%20development%2C%20we%20have%20collected%20real%20user%20preference%20data%20from%2017%20annotators%20on%20three%20deep%20research%20agents.%20A%20demo%20video%20of%20our%20platform%20can%20be%20found%20at%20https%3A//www.youtube.com/watch%3Fv%3Dg4d2dnbdseg.&entry.1838667208=http%3A//arxiv.org/abs/2507.05495v2&entry.124074799=Read"},
{"title": "AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition", "author": "Rajdeep Chatterjee and Sudip Chakrabarty and Trishaani Acharjee and Deepanjali Mishra", "abstract": "Unmanned aerial vehicles (UAVs), commonly known as drones, are increasingly used across diverse domains, including logistics, agriculture, surveillance, and defense. While these systems provide numerous benefits, their misuse raises safety and security concerns, making effective detection mechanisms essential. Acoustic sensing offers a low-cost and non-intrusive alternative to vision or radar-based detection, as drone propellers generate distinctive sound patterns. This study introduces AUDRON (AUdio-based Drone Recognition Network), a hybrid deep learning framework for drone sound detection, employing a combination of Mel-Frequency Cepstral Coefficients (MFCC), Short-Time Fourier Transform (STFT) spectrograms processed with convolutional neural networks (CNNs), recurrent layers for temporal modeling, and autoencoder-based representations. Feature-level fusion integrates complementary information before classification. Experimental evaluation demonstrates that AUDRON effectively differentiates drone acoustic signatures from background noise, achieving high accuracy while maintaining generalizability across varying conditions. AUDRON achieves 98.51 percent and 97.11 percent accuracy in binary and multiclass classification. The results highlight the advantage of combining multiple feature representations with deep learning for reliable acoustic drone detection, suggesting the framework's potential for deployment in security and surveillance applications where visual or radar sensing may be limited.", "link": "http://arxiv.org/abs/2512.20407v1", "date": "2025-12-23", "relevancy": 2.0146, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5165}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4956}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AUDRON%3A%20A%20Deep%20Learning%20Framework%20with%20Fused%20Acoustic%20Signatures%20for%20Drone%20Type%20Recognition&body=Title%3A%20AUDRON%3A%20A%20Deep%20Learning%20Framework%20with%20Fused%20Acoustic%20Signatures%20for%20Drone%20Type%20Recognition%0AAuthor%3A%20Rajdeep%20Chatterjee%20and%20Sudip%20Chakrabarty%20and%20Trishaani%20Acharjee%20and%20Deepanjali%20Mishra%0AAbstract%3A%20Unmanned%20aerial%20vehicles%20%28UAVs%29%2C%20commonly%20known%20as%20drones%2C%20are%20increasingly%20used%20across%20diverse%20domains%2C%20including%20logistics%2C%20agriculture%2C%20surveillance%2C%20and%20defense.%20While%20these%20systems%20provide%20numerous%20benefits%2C%20their%20misuse%20raises%20safety%20and%20security%20concerns%2C%20making%20effective%20detection%20mechanisms%20essential.%20Acoustic%20sensing%20offers%20a%20low-cost%20and%20non-intrusive%20alternative%20to%20vision%20or%20radar-based%20detection%2C%20as%20drone%20propellers%20generate%20distinctive%20sound%20patterns.%20This%20study%20introduces%20AUDRON%20%28AUdio-based%20Drone%20Recognition%20Network%29%2C%20a%20hybrid%20deep%20learning%20framework%20for%20drone%20sound%20detection%2C%20employing%20a%20combination%20of%20Mel-Frequency%20Cepstral%20Coefficients%20%28MFCC%29%2C%20Short-Time%20Fourier%20Transform%20%28STFT%29%20spectrograms%20processed%20with%20convolutional%20neural%20networks%20%28CNNs%29%2C%20recurrent%20layers%20for%20temporal%20modeling%2C%20and%20autoencoder-based%20representations.%20Feature-level%20fusion%20integrates%20complementary%20information%20before%20classification.%20Experimental%20evaluation%20demonstrates%20that%20AUDRON%20effectively%20differentiates%20drone%20acoustic%20signatures%20from%20background%20noise%2C%20achieving%20high%20accuracy%20while%20maintaining%20generalizability%20across%20varying%20conditions.%20AUDRON%20achieves%2098.51%20percent%20and%2097.11%20percent%20accuracy%20in%20binary%20and%20multiclass%20classification.%20The%20results%20highlight%20the%20advantage%20of%20combining%20multiple%20feature%20representations%20with%20deep%20learning%20for%20reliable%20acoustic%20drone%20detection%2C%20suggesting%20the%20framework%27s%20potential%20for%20deployment%20in%20security%20and%20surveillance%20applications%20where%20visual%20or%20radar%20sensing%20may%20be%20limited.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAUDRON%253A%2520A%2520Deep%2520Learning%2520Framework%2520with%2520Fused%2520Acoustic%2520Signatures%2520for%2520Drone%2520Type%2520Recognition%26entry.906535625%3DRajdeep%2520Chatterjee%2520and%2520Sudip%2520Chakrabarty%2520and%2520Trishaani%2520Acharjee%2520and%2520Deepanjali%2520Mishra%26entry.1292438233%3DUnmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%252C%2520commonly%2520known%2520as%2520drones%252C%2520are%2520increasingly%2520used%2520across%2520diverse%2520domains%252C%2520including%2520logistics%252C%2520agriculture%252C%2520surveillance%252C%2520and%2520defense.%2520While%2520these%2520systems%2520provide%2520numerous%2520benefits%252C%2520their%2520misuse%2520raises%2520safety%2520and%2520security%2520concerns%252C%2520making%2520effective%2520detection%2520mechanisms%2520essential.%2520Acoustic%2520sensing%2520offers%2520a%2520low-cost%2520and%2520non-intrusive%2520alternative%2520to%2520vision%2520or%2520radar-based%2520detection%252C%2520as%2520drone%2520propellers%2520generate%2520distinctive%2520sound%2520patterns.%2520This%2520study%2520introduces%2520AUDRON%2520%2528AUdio-based%2520Drone%2520Recognition%2520Network%2529%252C%2520a%2520hybrid%2520deep%2520learning%2520framework%2520for%2520drone%2520sound%2520detection%252C%2520employing%2520a%2520combination%2520of%2520Mel-Frequency%2520Cepstral%2520Coefficients%2520%2528MFCC%2529%252C%2520Short-Time%2520Fourier%2520Transform%2520%2528STFT%2529%2520spectrograms%2520processed%2520with%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%2520recurrent%2520layers%2520for%2520temporal%2520modeling%252C%2520and%2520autoencoder-based%2520representations.%2520Feature-level%2520fusion%2520integrates%2520complementary%2520information%2520before%2520classification.%2520Experimental%2520evaluation%2520demonstrates%2520that%2520AUDRON%2520effectively%2520differentiates%2520drone%2520acoustic%2520signatures%2520from%2520background%2520noise%252C%2520achieving%2520high%2520accuracy%2520while%2520maintaining%2520generalizability%2520across%2520varying%2520conditions.%2520AUDRON%2520achieves%252098.51%2520percent%2520and%252097.11%2520percent%2520accuracy%2520in%2520binary%2520and%2520multiclass%2520classification.%2520The%2520results%2520highlight%2520the%2520advantage%2520of%2520combining%2520multiple%2520feature%2520representations%2520with%2520deep%2520learning%2520for%2520reliable%2520acoustic%2520drone%2520detection%252C%2520suggesting%2520the%2520framework%2527s%2520potential%2520for%2520deployment%2520in%2520security%2520and%2520surveillance%2520applications%2520where%2520visual%2520or%2520radar%2520sensing%2520may%2520be%2520limited.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AUDRON%3A%20A%20Deep%20Learning%20Framework%20with%20Fused%20Acoustic%20Signatures%20for%20Drone%20Type%20Recognition&entry.906535625=Rajdeep%20Chatterjee%20and%20Sudip%20Chakrabarty%20and%20Trishaani%20Acharjee%20and%20Deepanjali%20Mishra&entry.1292438233=Unmanned%20aerial%20vehicles%20%28UAVs%29%2C%20commonly%20known%20as%20drones%2C%20are%20increasingly%20used%20across%20diverse%20domains%2C%20including%20logistics%2C%20agriculture%2C%20surveillance%2C%20and%20defense.%20While%20these%20systems%20provide%20numerous%20benefits%2C%20their%20misuse%20raises%20safety%20and%20security%20concerns%2C%20making%20effective%20detection%20mechanisms%20essential.%20Acoustic%20sensing%20offers%20a%20low-cost%20and%20non-intrusive%20alternative%20to%20vision%20or%20radar-based%20detection%2C%20as%20drone%20propellers%20generate%20distinctive%20sound%20patterns.%20This%20study%20introduces%20AUDRON%20%28AUdio-based%20Drone%20Recognition%20Network%29%2C%20a%20hybrid%20deep%20learning%20framework%20for%20drone%20sound%20detection%2C%20employing%20a%20combination%20of%20Mel-Frequency%20Cepstral%20Coefficients%20%28MFCC%29%2C%20Short-Time%20Fourier%20Transform%20%28STFT%29%20spectrograms%20processed%20with%20convolutional%20neural%20networks%20%28CNNs%29%2C%20recurrent%20layers%20for%20temporal%20modeling%2C%20and%20autoencoder-based%20representations.%20Feature-level%20fusion%20integrates%20complementary%20information%20before%20classification.%20Experimental%20evaluation%20demonstrates%20that%20AUDRON%20effectively%20differentiates%20drone%20acoustic%20signatures%20from%20background%20noise%2C%20achieving%20high%20accuracy%20while%20maintaining%20generalizability%20across%20varying%20conditions.%20AUDRON%20achieves%2098.51%20percent%20and%2097.11%20percent%20accuracy%20in%20binary%20and%20multiclass%20classification.%20The%20results%20highlight%20the%20advantage%20of%20combining%20multiple%20feature%20representations%20with%20deep%20learning%20for%20reliable%20acoustic%20drone%20detection%2C%20suggesting%20the%20framework%27s%20potential%20for%20deployment%20in%20security%20and%20surveillance%20applications%20where%20visual%20or%20radar%20sensing%20may%20be%20limited.&entry.1838667208=http%3A//arxiv.org/abs/2512.20407v1&entry.124074799=Read"},
{"title": "Top-K Exterior Power Persistent Homology: Algorithm, Structure, and Stability", "author": "Yoshihiro Maruyama", "abstract": "Exterior powers play important roles in persistent homology in computational geometry. In the present paper we study the problem of extracting the $K$ longest intervals of the exterior-power layers of a tame persistence module. We prove a structural decomposition theorem that organizes the exterior-power layers into monotone per-anchor streams with explicit multiplicities, enabling a best-first algorithm. We also show that the Top-$K$ length vector is $2$-Lipschitz under bottleneck perturbations of the input barcode, and prove a comparison-model lower bound. Our experiments confirm the theory, showing speedups over full enumeration in high overlap cases. By enabling efficient extraction of the most prominent features, our approach makes higher-order persistence feasible for large datasets and thus broadly applicable to machine learning, data science, and scientific computing.", "link": "http://arxiv.org/abs/2512.20325v1", "date": "2025-12-23", "relevancy": 2.0095, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4065}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4022}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Top-K%20Exterior%20Power%20Persistent%20Homology%3A%20Algorithm%2C%20Structure%2C%20and%20Stability&body=Title%3A%20Top-K%20Exterior%20Power%20Persistent%20Homology%3A%20Algorithm%2C%20Structure%2C%20and%20Stability%0AAuthor%3A%20Yoshihiro%20Maruyama%0AAbstract%3A%20Exterior%20powers%20play%20important%20roles%20in%20persistent%20homology%20in%20computational%20geometry.%20In%20the%20present%20paper%20we%20study%20the%20problem%20of%20extracting%20the%20%24K%24%20longest%20intervals%20of%20the%20exterior-power%20layers%20of%20a%20tame%20persistence%20module.%20We%20prove%20a%20structural%20decomposition%20theorem%20that%20organizes%20the%20exterior-power%20layers%20into%20monotone%20per-anchor%20streams%20with%20explicit%20multiplicities%2C%20enabling%20a%20best-first%20algorithm.%20We%20also%20show%20that%20the%20Top-%24K%24%20length%20vector%20is%20%242%24-Lipschitz%20under%20bottleneck%20perturbations%20of%20the%20input%20barcode%2C%20and%20prove%20a%20comparison-model%20lower%20bound.%20Our%20experiments%20confirm%20the%20theory%2C%20showing%20speedups%20over%20full%20enumeration%20in%20high%20overlap%20cases.%20By%20enabling%20efficient%20extraction%20of%20the%20most%20prominent%20features%2C%20our%20approach%20makes%20higher-order%20persistence%20feasible%20for%20large%20datasets%20and%20thus%20broadly%20applicable%20to%20machine%20learning%2C%20data%20science%2C%20and%20scientific%20computing.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTop-K%2520Exterior%2520Power%2520Persistent%2520Homology%253A%2520Algorithm%252C%2520Structure%252C%2520and%2520Stability%26entry.906535625%3DYoshihiro%2520Maruyama%26entry.1292438233%3DExterior%2520powers%2520play%2520important%2520roles%2520in%2520persistent%2520homology%2520in%2520computational%2520geometry.%2520In%2520the%2520present%2520paper%2520we%2520study%2520the%2520problem%2520of%2520extracting%2520the%2520%2524K%2524%2520longest%2520intervals%2520of%2520the%2520exterior-power%2520layers%2520of%2520a%2520tame%2520persistence%2520module.%2520We%2520prove%2520a%2520structural%2520decomposition%2520theorem%2520that%2520organizes%2520the%2520exterior-power%2520layers%2520into%2520monotone%2520per-anchor%2520streams%2520with%2520explicit%2520multiplicities%252C%2520enabling%2520a%2520best-first%2520algorithm.%2520We%2520also%2520show%2520that%2520the%2520Top-%2524K%2524%2520length%2520vector%2520is%2520%25242%2524-Lipschitz%2520under%2520bottleneck%2520perturbations%2520of%2520the%2520input%2520barcode%252C%2520and%2520prove%2520a%2520comparison-model%2520lower%2520bound.%2520Our%2520experiments%2520confirm%2520the%2520theory%252C%2520showing%2520speedups%2520over%2520full%2520enumeration%2520in%2520high%2520overlap%2520cases.%2520By%2520enabling%2520efficient%2520extraction%2520of%2520the%2520most%2520prominent%2520features%252C%2520our%2520approach%2520makes%2520higher-order%2520persistence%2520feasible%2520for%2520large%2520datasets%2520and%2520thus%2520broadly%2520applicable%2520to%2520machine%2520learning%252C%2520data%2520science%252C%2520and%2520scientific%2520computing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Top-K%20Exterior%20Power%20Persistent%20Homology%3A%20Algorithm%2C%20Structure%2C%20and%20Stability&entry.906535625=Yoshihiro%20Maruyama&entry.1292438233=Exterior%20powers%20play%20important%20roles%20in%20persistent%20homology%20in%20computational%20geometry.%20In%20the%20present%20paper%20we%20study%20the%20problem%20of%20extracting%20the%20%24K%24%20longest%20intervals%20of%20the%20exterior-power%20layers%20of%20a%20tame%20persistence%20module.%20We%20prove%20a%20structural%20decomposition%20theorem%20that%20organizes%20the%20exterior-power%20layers%20into%20monotone%20per-anchor%20streams%20with%20explicit%20multiplicities%2C%20enabling%20a%20best-first%20algorithm.%20We%20also%20show%20that%20the%20Top-%24K%24%20length%20vector%20is%20%242%24-Lipschitz%20under%20bottleneck%20perturbations%20of%20the%20input%20barcode%2C%20and%20prove%20a%20comparison-model%20lower%20bound.%20Our%20experiments%20confirm%20the%20theory%2C%20showing%20speedups%20over%20full%20enumeration%20in%20high%20overlap%20cases.%20By%20enabling%20efficient%20extraction%20of%20the%20most%20prominent%20features%2C%20our%20approach%20makes%20higher-order%20persistence%20feasible%20for%20large%20datasets%20and%20thus%20broadly%20applicable%20to%20machine%20learning%2C%20data%20science%2C%20and%20scientific%20computing.&entry.1838667208=http%3A//arxiv.org/abs/2512.20325v1&entry.124074799=Read"},
{"title": "BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation", "author": "Jinghao Shi and Jianing Song", "abstract": "High-resolution remote sensing image semantic segmentation (HRSS) is a fundamental yet critical task in the field of Earth observation. However, it has long faced the challenges of high inter-class similarity and large intra-class variability. Existing approaches often struggle to effectively inject abstract yet strongly discriminative semantic knowledge into pixel-level feature learning, leading to blurred boundaries and class confusion in complex scenes. To address these challenges, we propose Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg). Specifically, we design a Heatmap-driven Bidirectional Information Synergy Module (HBIS), which establishes a bidirectional information flow between feature maps and class embeddings by generating class-level heatmaps. Based on HBIS, we further introduce a hierarchical supervision strategy, where the interpretable heatmaps generated by each HBIS module are directly utilized as low-resolution segmentation predictions for supervision, thereby enhancing the discriminative capacity of shallow features. In addition, to further improve the discriminability of the embedding representations, we propose a cross-layer class embedding Fisher Discriminative Loss to enforce intra-class compactness and enlarge inter-class separability. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves outstanding segmentation performance while offering stronger interpretability. The released code is available at https://github.com/ShiJinghao566/BiCoR-Seg.", "link": "http://arxiv.org/abs/2512.20255v1", "date": "2025-12-23", "relevancy": 2.0044, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5235}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4982}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiCoR-Seg%3A%20Bidirectional%20Co-Refinement%20Framework%20for%20High-Resolution%20Remote%20Sensing%20Image%20Segmentation&body=Title%3A%20BiCoR-Seg%3A%20Bidirectional%20Co-Refinement%20Framework%20for%20High-Resolution%20Remote%20Sensing%20Image%20Segmentation%0AAuthor%3A%20Jinghao%20Shi%20and%20Jianing%20Song%0AAbstract%3A%20High-resolution%20remote%20sensing%20image%20semantic%20segmentation%20%28HRSS%29%20is%20a%20fundamental%20yet%20critical%20task%20in%20the%20field%20of%20Earth%20observation.%20However%2C%20it%20has%20long%20faced%20the%20challenges%20of%20high%20inter-class%20similarity%20and%20large%20intra-class%20variability.%20Existing%20approaches%20often%20struggle%20to%20effectively%20inject%20abstract%20yet%20strongly%20discriminative%20semantic%20knowledge%20into%20pixel-level%20feature%20learning%2C%20leading%20to%20blurred%20boundaries%20and%20class%20confusion%20in%20complex%20scenes.%20To%20address%20these%20challenges%2C%20we%20propose%20Bidirectional%20Co-Refinement%20Framework%20for%20HRSS%20%28BiCoR-Seg%29.%20Specifically%2C%20we%20design%20a%20Heatmap-driven%20Bidirectional%20Information%20Synergy%20Module%20%28HBIS%29%2C%20which%20establishes%20a%20bidirectional%20information%20flow%20between%20feature%20maps%20and%20class%20embeddings%20by%20generating%20class-level%20heatmaps.%20Based%20on%20HBIS%2C%20we%20further%20introduce%20a%20hierarchical%20supervision%20strategy%2C%20where%20the%20interpretable%20heatmaps%20generated%20by%20each%20HBIS%20module%20are%20directly%20utilized%20as%20low-resolution%20segmentation%20predictions%20for%20supervision%2C%20thereby%20enhancing%20the%20discriminative%20capacity%20of%20shallow%20features.%20In%20addition%2C%20to%20further%20improve%20the%20discriminability%20of%20the%20embedding%20representations%2C%20we%20propose%20a%20cross-layer%20class%20embedding%20Fisher%20Discriminative%20Loss%20to%20enforce%20intra-class%20compactness%20and%20enlarge%20inter-class%20separability.%20Extensive%20experiments%20on%20the%20LoveDA%2C%20Vaihingen%2C%20and%20Potsdam%20datasets%20demonstrate%20that%20BiCoR-Seg%20achieves%20outstanding%20segmentation%20performance%20while%20offering%20stronger%20interpretability.%20The%20released%20code%20is%20available%20at%20https%3A//github.com/ShiJinghao566/BiCoR-Seg.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiCoR-Seg%253A%2520Bidirectional%2520Co-Refinement%2520Framework%2520for%2520High-Resolution%2520Remote%2520Sensing%2520Image%2520Segmentation%26entry.906535625%3DJinghao%2520Shi%2520and%2520Jianing%2520Song%26entry.1292438233%3DHigh-resolution%2520remote%2520sensing%2520image%2520semantic%2520segmentation%2520%2528HRSS%2529%2520is%2520a%2520fundamental%2520yet%2520critical%2520task%2520in%2520the%2520field%2520of%2520Earth%2520observation.%2520However%252C%2520it%2520has%2520long%2520faced%2520the%2520challenges%2520of%2520high%2520inter-class%2520similarity%2520and%2520large%2520intra-class%2520variability.%2520Existing%2520approaches%2520often%2520struggle%2520to%2520effectively%2520inject%2520abstract%2520yet%2520strongly%2520discriminative%2520semantic%2520knowledge%2520into%2520pixel-level%2520feature%2520learning%252C%2520leading%2520to%2520blurred%2520boundaries%2520and%2520class%2520confusion%2520in%2520complex%2520scenes.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Bidirectional%2520Co-Refinement%2520Framework%2520for%2520HRSS%2520%2528BiCoR-Seg%2529.%2520Specifically%252C%2520we%2520design%2520a%2520Heatmap-driven%2520Bidirectional%2520Information%2520Synergy%2520Module%2520%2528HBIS%2529%252C%2520which%2520establishes%2520a%2520bidirectional%2520information%2520flow%2520between%2520feature%2520maps%2520and%2520class%2520embeddings%2520by%2520generating%2520class-level%2520heatmaps.%2520Based%2520on%2520HBIS%252C%2520we%2520further%2520introduce%2520a%2520hierarchical%2520supervision%2520strategy%252C%2520where%2520the%2520interpretable%2520heatmaps%2520generated%2520by%2520each%2520HBIS%2520module%2520are%2520directly%2520utilized%2520as%2520low-resolution%2520segmentation%2520predictions%2520for%2520supervision%252C%2520thereby%2520enhancing%2520the%2520discriminative%2520capacity%2520of%2520shallow%2520features.%2520In%2520addition%252C%2520to%2520further%2520improve%2520the%2520discriminability%2520of%2520the%2520embedding%2520representations%252C%2520we%2520propose%2520a%2520cross-layer%2520class%2520embedding%2520Fisher%2520Discriminative%2520Loss%2520to%2520enforce%2520intra-class%2520compactness%2520and%2520enlarge%2520inter-class%2520separability.%2520Extensive%2520experiments%2520on%2520the%2520LoveDA%252C%2520Vaihingen%252C%2520and%2520Potsdam%2520datasets%2520demonstrate%2520that%2520BiCoR-Seg%2520achieves%2520outstanding%2520segmentation%2520performance%2520while%2520offering%2520stronger%2520interpretability.%2520The%2520released%2520code%2520is%2520available%2520at%2520https%253A//github.com/ShiJinghao566/BiCoR-Seg.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiCoR-Seg%3A%20Bidirectional%20Co-Refinement%20Framework%20for%20High-Resolution%20Remote%20Sensing%20Image%20Segmentation&entry.906535625=Jinghao%20Shi%20and%20Jianing%20Song&entry.1292438233=High-resolution%20remote%20sensing%20image%20semantic%20segmentation%20%28HRSS%29%20is%20a%20fundamental%20yet%20critical%20task%20in%20the%20field%20of%20Earth%20observation.%20However%2C%20it%20has%20long%20faced%20the%20challenges%20of%20high%20inter-class%20similarity%20and%20large%20intra-class%20variability.%20Existing%20approaches%20often%20struggle%20to%20effectively%20inject%20abstract%20yet%20strongly%20discriminative%20semantic%20knowledge%20into%20pixel-level%20feature%20learning%2C%20leading%20to%20blurred%20boundaries%20and%20class%20confusion%20in%20complex%20scenes.%20To%20address%20these%20challenges%2C%20we%20propose%20Bidirectional%20Co-Refinement%20Framework%20for%20HRSS%20%28BiCoR-Seg%29.%20Specifically%2C%20we%20design%20a%20Heatmap-driven%20Bidirectional%20Information%20Synergy%20Module%20%28HBIS%29%2C%20which%20establishes%20a%20bidirectional%20information%20flow%20between%20feature%20maps%20and%20class%20embeddings%20by%20generating%20class-level%20heatmaps.%20Based%20on%20HBIS%2C%20we%20further%20introduce%20a%20hierarchical%20supervision%20strategy%2C%20where%20the%20interpretable%20heatmaps%20generated%20by%20each%20HBIS%20module%20are%20directly%20utilized%20as%20low-resolution%20segmentation%20predictions%20for%20supervision%2C%20thereby%20enhancing%20the%20discriminative%20capacity%20of%20shallow%20features.%20In%20addition%2C%20to%20further%20improve%20the%20discriminability%20of%20the%20embedding%20representations%2C%20we%20propose%20a%20cross-layer%20class%20embedding%20Fisher%20Discriminative%20Loss%20to%20enforce%20intra-class%20compactness%20and%20enlarge%20inter-class%20separability.%20Extensive%20experiments%20on%20the%20LoveDA%2C%20Vaihingen%2C%20and%20Potsdam%20datasets%20demonstrate%20that%20BiCoR-Seg%20achieves%20outstanding%20segmentation%20performance%20while%20offering%20stronger%20interpretability.%20The%20released%20code%20is%20available%20at%20https%3A//github.com/ShiJinghao566/BiCoR-Seg.&entry.1838667208=http%3A//arxiv.org/abs/2512.20255v1&entry.124074799=Read"},
{"title": "Adaptive Multi-task Learning for Probabilistic Load Forecasting", "author": "Onintze Zaballa and Ver\u00f3nica \u00c1lvarez and Santiago Mazuelas", "abstract": "Simultaneous load forecasting across multiple entities (e.g., regions, buildings) is crucial for the efficient, reliable, and cost-effective operation of power systems. Accurate load forecasting is a challenging problem due to the inherent uncertainties in load demand, dynamic changes in consumption patterns, and correlations among entities. Multi-task learning has emerged as a powerful machine learning approach that enables the simultaneous learning across multiple related problems. However, its application to load forecasting remains underexplored and is limited to offline learning-based methods, which cannot capture changes in consumption patterns. This paper presents an adaptive multi-task learning method for probabilistic load forecasting. The proposed method can dynamically adapt to changes in consumption patterns and correlations among entities. In addition, the techniques presented provide reliable probabilistic predictions for loads of multiples entities and assess load uncertainties. Specifically, the method is based on vectorvalued hidden Markov models and uses a recursive process to update the model parameters and provide predictions with the most recent parameters. The performance of the proposed method is evaluated using datasets that contain the load demand of multiple entities and exhibit diverse and dynamic consumption patterns. The experimental results show that the presented techniques outperform existing methods both in terms of forecasting performance and uncertainty assessment.", "link": "http://arxiv.org/abs/2512.20232v1", "date": "2025-12-23", "relevancy": 1.9842, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5576}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4853}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Multi-task%20Learning%20for%20Probabilistic%20Load%20Forecasting&body=Title%3A%20Adaptive%20Multi-task%20Learning%20for%20Probabilistic%20Load%20Forecasting%0AAuthor%3A%20Onintze%20Zaballa%20and%20Ver%C3%B3nica%20%C3%81lvarez%20and%20Santiago%20Mazuelas%0AAbstract%3A%20Simultaneous%20load%20forecasting%20across%20multiple%20entities%20%28e.g.%2C%20regions%2C%20buildings%29%20is%20crucial%20for%20the%20efficient%2C%20reliable%2C%20and%20cost-effective%20operation%20of%20power%20systems.%20Accurate%20load%20forecasting%20is%20a%20challenging%20problem%20due%20to%20the%20inherent%20uncertainties%20in%20load%20demand%2C%20dynamic%20changes%20in%20consumption%20patterns%2C%20and%20correlations%20among%20entities.%20Multi-task%20learning%20has%20emerged%20as%20a%20powerful%20machine%20learning%20approach%20that%20enables%20the%20simultaneous%20learning%20across%20multiple%20related%20problems.%20However%2C%20its%20application%20to%20load%20forecasting%20remains%20underexplored%20and%20is%20limited%20to%20offline%20learning-based%20methods%2C%20which%20cannot%20capture%20changes%20in%20consumption%20patterns.%20This%20paper%20presents%20an%20adaptive%20multi-task%20learning%20method%20for%20probabilistic%20load%20forecasting.%20The%20proposed%20method%20can%20dynamically%20adapt%20to%20changes%20in%20consumption%20patterns%20and%20correlations%20among%20entities.%20In%20addition%2C%20the%20techniques%20presented%20provide%20reliable%20probabilistic%20predictions%20for%20loads%20of%20multiples%20entities%20and%20assess%20load%20uncertainties.%20Specifically%2C%20the%20method%20is%20based%20on%20vectorvalued%20hidden%20Markov%20models%20and%20uses%20a%20recursive%20process%20to%20update%20the%20model%20parameters%20and%20provide%20predictions%20with%20the%20most%20recent%20parameters.%20The%20performance%20of%20the%20proposed%20method%20is%20evaluated%20using%20datasets%20that%20contain%20the%20load%20demand%20of%20multiple%20entities%20and%20exhibit%20diverse%20and%20dynamic%20consumption%20patterns.%20The%20experimental%20results%20show%20that%20the%20presented%20techniques%20outperform%20existing%20methods%20both%20in%20terms%20of%20forecasting%20performance%20and%20uncertainty%20assessment.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Multi-task%2520Learning%2520for%2520Probabilistic%2520Load%2520Forecasting%26entry.906535625%3DOnintze%2520Zaballa%2520and%2520Ver%25C3%25B3nica%2520%25C3%2581lvarez%2520and%2520Santiago%2520Mazuelas%26entry.1292438233%3DSimultaneous%2520load%2520forecasting%2520across%2520multiple%2520entities%2520%2528e.g.%252C%2520regions%252C%2520buildings%2529%2520is%2520crucial%2520for%2520the%2520efficient%252C%2520reliable%252C%2520and%2520cost-effective%2520operation%2520of%2520power%2520systems.%2520Accurate%2520load%2520forecasting%2520is%2520a%2520challenging%2520problem%2520due%2520to%2520the%2520inherent%2520uncertainties%2520in%2520load%2520demand%252C%2520dynamic%2520changes%2520in%2520consumption%2520patterns%252C%2520and%2520correlations%2520among%2520entities.%2520Multi-task%2520learning%2520has%2520emerged%2520as%2520a%2520powerful%2520machine%2520learning%2520approach%2520that%2520enables%2520the%2520simultaneous%2520learning%2520across%2520multiple%2520related%2520problems.%2520However%252C%2520its%2520application%2520to%2520load%2520forecasting%2520remains%2520underexplored%2520and%2520is%2520limited%2520to%2520offline%2520learning-based%2520methods%252C%2520which%2520cannot%2520capture%2520changes%2520in%2520consumption%2520patterns.%2520This%2520paper%2520presents%2520an%2520adaptive%2520multi-task%2520learning%2520method%2520for%2520probabilistic%2520load%2520forecasting.%2520The%2520proposed%2520method%2520can%2520dynamically%2520adapt%2520to%2520changes%2520in%2520consumption%2520patterns%2520and%2520correlations%2520among%2520entities.%2520In%2520addition%252C%2520the%2520techniques%2520presented%2520provide%2520reliable%2520probabilistic%2520predictions%2520for%2520loads%2520of%2520multiples%2520entities%2520and%2520assess%2520load%2520uncertainties.%2520Specifically%252C%2520the%2520method%2520is%2520based%2520on%2520vectorvalued%2520hidden%2520Markov%2520models%2520and%2520uses%2520a%2520recursive%2520process%2520to%2520update%2520the%2520model%2520parameters%2520and%2520provide%2520predictions%2520with%2520the%2520most%2520recent%2520parameters.%2520The%2520performance%2520of%2520the%2520proposed%2520method%2520is%2520evaluated%2520using%2520datasets%2520that%2520contain%2520the%2520load%2520demand%2520of%2520multiple%2520entities%2520and%2520exhibit%2520diverse%2520and%2520dynamic%2520consumption%2520patterns.%2520The%2520experimental%2520results%2520show%2520that%2520the%2520presented%2520techniques%2520outperform%2520existing%2520methods%2520both%2520in%2520terms%2520of%2520forecasting%2520performance%2520and%2520uncertainty%2520assessment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Multi-task%20Learning%20for%20Probabilistic%20Load%20Forecasting&entry.906535625=Onintze%20Zaballa%20and%20Ver%C3%B3nica%20%C3%81lvarez%20and%20Santiago%20Mazuelas&entry.1292438233=Simultaneous%20load%20forecasting%20across%20multiple%20entities%20%28e.g.%2C%20regions%2C%20buildings%29%20is%20crucial%20for%20the%20efficient%2C%20reliable%2C%20and%20cost-effective%20operation%20of%20power%20systems.%20Accurate%20load%20forecasting%20is%20a%20challenging%20problem%20due%20to%20the%20inherent%20uncertainties%20in%20load%20demand%2C%20dynamic%20changes%20in%20consumption%20patterns%2C%20and%20correlations%20among%20entities.%20Multi-task%20learning%20has%20emerged%20as%20a%20powerful%20machine%20learning%20approach%20that%20enables%20the%20simultaneous%20learning%20across%20multiple%20related%20problems.%20However%2C%20its%20application%20to%20load%20forecasting%20remains%20underexplored%20and%20is%20limited%20to%20offline%20learning-based%20methods%2C%20which%20cannot%20capture%20changes%20in%20consumption%20patterns.%20This%20paper%20presents%20an%20adaptive%20multi-task%20learning%20method%20for%20probabilistic%20load%20forecasting.%20The%20proposed%20method%20can%20dynamically%20adapt%20to%20changes%20in%20consumption%20patterns%20and%20correlations%20among%20entities.%20In%20addition%2C%20the%20techniques%20presented%20provide%20reliable%20probabilistic%20predictions%20for%20loads%20of%20multiples%20entities%20and%20assess%20load%20uncertainties.%20Specifically%2C%20the%20method%20is%20based%20on%20vectorvalued%20hidden%20Markov%20models%20and%20uses%20a%20recursive%20process%20to%20update%20the%20model%20parameters%20and%20provide%20predictions%20with%20the%20most%20recent%20parameters.%20The%20performance%20of%20the%20proposed%20method%20is%20evaluated%20using%20datasets%20that%20contain%20the%20load%20demand%20of%20multiple%20entities%20and%20exhibit%20diverse%20and%20dynamic%20consumption%20patterns.%20The%20experimental%20results%20show%20that%20the%20presented%20techniques%20outperform%20existing%20methods%20both%20in%20terms%20of%20forecasting%20performance%20and%20uncertainty%20assessment.&entry.1838667208=http%3A//arxiv.org/abs/2512.20232v1&entry.124074799=Read"},
{"title": "Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems", "author": "James E. Gallagher and Edward J. Oughton and Jana Kosecka", "abstract": "Landmines remain a persistent humanitarian threat, with 110 million actively deployed mines across 60 countries, claiming 26,000 casualties annually. This research evaluates adaptive Red-Green-Blue (RGB) and Long-Wave Infrared (LWIR) fusion for Unmanned Aerial Systems (UAS)-based detection of surface-laid landmines, leveraging the thermal contrast between the ordnance and the surrounding soil to enhance feature extraction. Using You Only Look Once (YOLO) architectures (v8, v10, v11) across 114 test images, generating 35,640 model-condition evaluations, YOLOv11 achieved optimal performance (86.8% mAP), with 10 to 30% thermal fusion at 5 to 10m altitude identified as the optimal detection parameters. A complementary architectural comparison revealed that while RF-DETR achieved the highest accuracy (69.2% mAP), followed by Faster R-CNN (67.6%), YOLOv11 (64.2%), and RetinaNet (50.2%), YOLOv11 trained 17.7 times faster than the transformer-based RF-DETR (41 minutes versus 12 hours), presenting a critical accuracy-efficiency tradeoff for operational deployment. Aggregated multi-temporal training datasets outperformed season-specific approaches by 1.8 to 9.6%, suggesting that models benefit from exposure to diverse thermal conditions. Anti-Tank (AT) mines achieved 61.9% detection accuracy, compared with 19.2% for Anti-Personnel (AP) mines, reflecting both the size differential and thermal-mass differences between these ordnance classes. As this research examined surface-laid mines where thermal contrast is maximized, future research should quantify thermal contrast effects for mines buried at varying depths across heterogeneous soil types.", "link": "http://arxiv.org/abs/2512.20487v1", "date": "2025-12-23", "relevancy": 1.9741, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5035}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4881}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-temporal%20Adaptive%20Red-Green-Blue%20and%20Long-Wave%20Infrared%20Fusion%20for%20You%20Only%20Look%20Once-Based%20Landmine%20Detection%20from%20Unmanned%20Aerial%20Systems&body=Title%3A%20Multi-temporal%20Adaptive%20Red-Green-Blue%20and%20Long-Wave%20Infrared%20Fusion%20for%20You%20Only%20Look%20Once-Based%20Landmine%20Detection%20from%20Unmanned%20Aerial%20Systems%0AAuthor%3A%20James%20E.%20Gallagher%20and%20Edward%20J.%20Oughton%20and%20Jana%20Kosecka%0AAbstract%3A%20Landmines%20remain%20a%20persistent%20humanitarian%20threat%2C%20with%20110%20million%20actively%20deployed%20mines%20across%2060%20countries%2C%20claiming%2026%2C000%20casualties%20annually.%20This%20research%20evaluates%20adaptive%20Red-Green-Blue%20%28RGB%29%20and%20Long-Wave%20Infrared%20%28LWIR%29%20fusion%20for%20Unmanned%20Aerial%20Systems%20%28UAS%29-based%20detection%20of%20surface-laid%20landmines%2C%20leveraging%20the%20thermal%20contrast%20between%20the%20ordnance%20and%20the%20surrounding%20soil%20to%20enhance%20feature%20extraction.%20Using%20You%20Only%20Look%20Once%20%28YOLO%29%20architectures%20%28v8%2C%20v10%2C%20v11%29%20across%20114%20test%20images%2C%20generating%2035%2C640%20model-condition%20evaluations%2C%20YOLOv11%20achieved%20optimal%20performance%20%2886.8%25%20mAP%29%2C%20with%2010%20to%2030%25%20thermal%20fusion%20at%205%20to%2010m%20altitude%20identified%20as%20the%20optimal%20detection%20parameters.%20A%20complementary%20architectural%20comparison%20revealed%20that%20while%20RF-DETR%20achieved%20the%20highest%20accuracy%20%2869.2%25%20mAP%29%2C%20followed%20by%20Faster%20R-CNN%20%2867.6%25%29%2C%20YOLOv11%20%2864.2%25%29%2C%20and%20RetinaNet%20%2850.2%25%29%2C%20YOLOv11%20trained%2017.7%20times%20faster%20than%20the%20transformer-based%20RF-DETR%20%2841%20minutes%20versus%2012%20hours%29%2C%20presenting%20a%20critical%20accuracy-efficiency%20tradeoff%20for%20operational%20deployment.%20Aggregated%20multi-temporal%20training%20datasets%20outperformed%20season-specific%20approaches%20by%201.8%20to%209.6%25%2C%20suggesting%20that%20models%20benefit%20from%20exposure%20to%20diverse%20thermal%20conditions.%20Anti-Tank%20%28AT%29%20mines%20achieved%2061.9%25%20detection%20accuracy%2C%20compared%20with%2019.2%25%20for%20Anti-Personnel%20%28AP%29%20mines%2C%20reflecting%20both%20the%20size%20differential%20and%20thermal-mass%20differences%20between%20these%20ordnance%20classes.%20As%20this%20research%20examined%20surface-laid%20mines%20where%20thermal%20contrast%20is%20maximized%2C%20future%20research%20should%20quantify%20thermal%20contrast%20effects%20for%20mines%20buried%20at%20varying%20depths%20across%20heterogeneous%20soil%20types.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-temporal%2520Adaptive%2520Red-Green-Blue%2520and%2520Long-Wave%2520Infrared%2520Fusion%2520for%2520You%2520Only%2520Look%2520Once-Based%2520Landmine%2520Detection%2520from%2520Unmanned%2520Aerial%2520Systems%26entry.906535625%3DJames%2520E.%2520Gallagher%2520and%2520Edward%2520J.%2520Oughton%2520and%2520Jana%2520Kosecka%26entry.1292438233%3DLandmines%2520remain%2520a%2520persistent%2520humanitarian%2520threat%252C%2520with%2520110%2520million%2520actively%2520deployed%2520mines%2520across%252060%2520countries%252C%2520claiming%252026%252C000%2520casualties%2520annually.%2520This%2520research%2520evaluates%2520adaptive%2520Red-Green-Blue%2520%2528RGB%2529%2520and%2520Long-Wave%2520Infrared%2520%2528LWIR%2529%2520fusion%2520for%2520Unmanned%2520Aerial%2520Systems%2520%2528UAS%2529-based%2520detection%2520of%2520surface-laid%2520landmines%252C%2520leveraging%2520the%2520thermal%2520contrast%2520between%2520the%2520ordnance%2520and%2520the%2520surrounding%2520soil%2520to%2520enhance%2520feature%2520extraction.%2520Using%2520You%2520Only%2520Look%2520Once%2520%2528YOLO%2529%2520architectures%2520%2528v8%252C%2520v10%252C%2520v11%2529%2520across%2520114%2520test%2520images%252C%2520generating%252035%252C640%2520model-condition%2520evaluations%252C%2520YOLOv11%2520achieved%2520optimal%2520performance%2520%252886.8%2525%2520mAP%2529%252C%2520with%252010%2520to%252030%2525%2520thermal%2520fusion%2520at%25205%2520to%252010m%2520altitude%2520identified%2520as%2520the%2520optimal%2520detection%2520parameters.%2520A%2520complementary%2520architectural%2520comparison%2520revealed%2520that%2520while%2520RF-DETR%2520achieved%2520the%2520highest%2520accuracy%2520%252869.2%2525%2520mAP%2529%252C%2520followed%2520by%2520Faster%2520R-CNN%2520%252867.6%2525%2529%252C%2520YOLOv11%2520%252864.2%2525%2529%252C%2520and%2520RetinaNet%2520%252850.2%2525%2529%252C%2520YOLOv11%2520trained%252017.7%2520times%2520faster%2520than%2520the%2520transformer-based%2520RF-DETR%2520%252841%2520minutes%2520versus%252012%2520hours%2529%252C%2520presenting%2520a%2520critical%2520accuracy-efficiency%2520tradeoff%2520for%2520operational%2520deployment.%2520Aggregated%2520multi-temporal%2520training%2520datasets%2520outperformed%2520season-specific%2520approaches%2520by%25201.8%2520to%25209.6%2525%252C%2520suggesting%2520that%2520models%2520benefit%2520from%2520exposure%2520to%2520diverse%2520thermal%2520conditions.%2520Anti-Tank%2520%2528AT%2529%2520mines%2520achieved%252061.9%2525%2520detection%2520accuracy%252C%2520compared%2520with%252019.2%2525%2520for%2520Anti-Personnel%2520%2528AP%2529%2520mines%252C%2520reflecting%2520both%2520the%2520size%2520differential%2520and%2520thermal-mass%2520differences%2520between%2520these%2520ordnance%2520classes.%2520As%2520this%2520research%2520examined%2520surface-laid%2520mines%2520where%2520thermal%2520contrast%2520is%2520maximized%252C%2520future%2520research%2520should%2520quantify%2520thermal%2520contrast%2520effects%2520for%2520mines%2520buried%2520at%2520varying%2520depths%2520across%2520heterogeneous%2520soil%2520types.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-temporal%20Adaptive%20Red-Green-Blue%20and%20Long-Wave%20Infrared%20Fusion%20for%20You%20Only%20Look%20Once-Based%20Landmine%20Detection%20from%20Unmanned%20Aerial%20Systems&entry.906535625=James%20E.%20Gallagher%20and%20Edward%20J.%20Oughton%20and%20Jana%20Kosecka&entry.1292438233=Landmines%20remain%20a%20persistent%20humanitarian%20threat%2C%20with%20110%20million%20actively%20deployed%20mines%20across%2060%20countries%2C%20claiming%2026%2C000%20casualties%20annually.%20This%20research%20evaluates%20adaptive%20Red-Green-Blue%20%28RGB%29%20and%20Long-Wave%20Infrared%20%28LWIR%29%20fusion%20for%20Unmanned%20Aerial%20Systems%20%28UAS%29-based%20detection%20of%20surface-laid%20landmines%2C%20leveraging%20the%20thermal%20contrast%20between%20the%20ordnance%20and%20the%20surrounding%20soil%20to%20enhance%20feature%20extraction.%20Using%20You%20Only%20Look%20Once%20%28YOLO%29%20architectures%20%28v8%2C%20v10%2C%20v11%29%20across%20114%20test%20images%2C%20generating%2035%2C640%20model-condition%20evaluations%2C%20YOLOv11%20achieved%20optimal%20performance%20%2886.8%25%20mAP%29%2C%20with%2010%20to%2030%25%20thermal%20fusion%20at%205%20to%2010m%20altitude%20identified%20as%20the%20optimal%20detection%20parameters.%20A%20complementary%20architectural%20comparison%20revealed%20that%20while%20RF-DETR%20achieved%20the%20highest%20accuracy%20%2869.2%25%20mAP%29%2C%20followed%20by%20Faster%20R-CNN%20%2867.6%25%29%2C%20YOLOv11%20%2864.2%25%29%2C%20and%20RetinaNet%20%2850.2%25%29%2C%20YOLOv11%20trained%2017.7%20times%20faster%20than%20the%20transformer-based%20RF-DETR%20%2841%20minutes%20versus%2012%20hours%29%2C%20presenting%20a%20critical%20accuracy-efficiency%20tradeoff%20for%20operational%20deployment.%20Aggregated%20multi-temporal%20training%20datasets%20outperformed%20season-specific%20approaches%20by%201.8%20to%209.6%25%2C%20suggesting%20that%20models%20benefit%20from%20exposure%20to%20diverse%20thermal%20conditions.%20Anti-Tank%20%28AT%29%20mines%20achieved%2061.9%25%20detection%20accuracy%2C%20compared%20with%2019.2%25%20for%20Anti-Personnel%20%28AP%29%20mines%2C%20reflecting%20both%20the%20size%20differential%20and%20thermal-mass%20differences%20between%20these%20ordnance%20classes.%20As%20this%20research%20examined%20surface-laid%20mines%20where%20thermal%20contrast%20is%20maximized%2C%20future%20research%20should%20quantify%20thermal%20contrast%20effects%20for%20mines%20buried%20at%20varying%20depths%20across%20heterogeneous%20soil%20types.&entry.1838667208=http%3A//arxiv.org/abs/2512.20487v1&entry.124074799=Read"},
{"title": "Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures", "author": "Yedi Zhang and Andrew Saxe and Peter E. Latham", "abstract": "Neural networks trained with gradient descent often learn solutions of increasing complexity over time, a phenomenon known as simplicity bias. Despite being widely observed across architectures, existing theoretical treatments lack a unifying framework. We present a theoretical framework that explains a simplicity bias arising from saddle-to-saddle learning dynamics for a general class of neural networks, incorporating fully-connected, convolutional, and attention-based architectures. Here, simple means expressible with few hidden units, i.e., hidden neurons, convolutional kernels, or attention heads. Specifically, we show that linear networks learn solutions of increasing rank, ReLU networks learn solutions with an increasing number of kinks, convolutional networks learn solutions with an increasing number of convolutional kernels, and self-attention models learn solutions with an increasing number of attention heads. By analyzing fixed points, invariant manifolds, and dynamics of gradient descent learning, we show that saddle-to-saddle dynamics operates by iteratively evolving near an invariant manifold, approaching a saddle, and switching to another invariant manifold. Our analysis also illuminates the effects of data distribution and weight initialization on the duration and number of plateaus in learning, dissociating previously confounding factors. Overall, our theory offers a framework for understanding when and why gradient descent progressively learns increasingly complex solutions.", "link": "http://arxiv.org/abs/2512.20607v1", "date": "2025-12-23", "relevancy": 1.9693, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5502}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4542}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Saddle-to-Saddle%20Dynamics%20Explains%20A%20Simplicity%20Bias%20Across%20Neural%20Network%20Architectures&body=Title%3A%20Saddle-to-Saddle%20Dynamics%20Explains%20A%20Simplicity%20Bias%20Across%20Neural%20Network%20Architectures%0AAuthor%3A%20Yedi%20Zhang%20and%20Andrew%20Saxe%20and%20Peter%20E.%20Latham%0AAbstract%3A%20Neural%20networks%20trained%20with%20gradient%20descent%20often%20learn%20solutions%20of%20increasing%20complexity%20over%20time%2C%20a%20phenomenon%20known%20as%20simplicity%20bias.%20Despite%20being%20widely%20observed%20across%20architectures%2C%20existing%20theoretical%20treatments%20lack%20a%20unifying%20framework.%20We%20present%20a%20theoretical%20framework%20that%20explains%20a%20simplicity%20bias%20arising%20from%20saddle-to-saddle%20learning%20dynamics%20for%20a%20general%20class%20of%20neural%20networks%2C%20incorporating%20fully-connected%2C%20convolutional%2C%20and%20attention-based%20architectures.%20Here%2C%20simple%20means%20expressible%20with%20few%20hidden%20units%2C%20i.e.%2C%20hidden%20neurons%2C%20convolutional%20kernels%2C%20or%20attention%20heads.%20Specifically%2C%20we%20show%20that%20linear%20networks%20learn%20solutions%20of%20increasing%20rank%2C%20ReLU%20networks%20learn%20solutions%20with%20an%20increasing%20number%20of%20kinks%2C%20convolutional%20networks%20learn%20solutions%20with%20an%20increasing%20number%20of%20convolutional%20kernels%2C%20and%20self-attention%20models%20learn%20solutions%20with%20an%20increasing%20number%20of%20attention%20heads.%20By%20analyzing%20fixed%20points%2C%20invariant%20manifolds%2C%20and%20dynamics%20of%20gradient%20descent%20learning%2C%20we%20show%20that%20saddle-to-saddle%20dynamics%20operates%20by%20iteratively%20evolving%20near%20an%20invariant%20manifold%2C%20approaching%20a%20saddle%2C%20and%20switching%20to%20another%20invariant%20manifold.%20Our%20analysis%20also%20illuminates%20the%20effects%20of%20data%20distribution%20and%20weight%20initialization%20on%20the%20duration%20and%20number%20of%20plateaus%20in%20learning%2C%20dissociating%20previously%20confounding%20factors.%20Overall%2C%20our%20theory%20offers%20a%20framework%20for%20understanding%20when%20and%20why%20gradient%20descent%20progressively%20learns%20increasingly%20complex%20solutions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSaddle-to-Saddle%2520Dynamics%2520Explains%2520A%2520Simplicity%2520Bias%2520Across%2520Neural%2520Network%2520Architectures%26entry.906535625%3DYedi%2520Zhang%2520and%2520Andrew%2520Saxe%2520and%2520Peter%2520E.%2520Latham%26entry.1292438233%3DNeural%2520networks%2520trained%2520with%2520gradient%2520descent%2520often%2520learn%2520solutions%2520of%2520increasing%2520complexity%2520over%2520time%252C%2520a%2520phenomenon%2520known%2520as%2520simplicity%2520bias.%2520Despite%2520being%2520widely%2520observed%2520across%2520architectures%252C%2520existing%2520theoretical%2520treatments%2520lack%2520a%2520unifying%2520framework.%2520We%2520present%2520a%2520theoretical%2520framework%2520that%2520explains%2520a%2520simplicity%2520bias%2520arising%2520from%2520saddle-to-saddle%2520learning%2520dynamics%2520for%2520a%2520general%2520class%2520of%2520neural%2520networks%252C%2520incorporating%2520fully-connected%252C%2520convolutional%252C%2520and%2520attention-based%2520architectures.%2520Here%252C%2520simple%2520means%2520expressible%2520with%2520few%2520hidden%2520units%252C%2520i.e.%252C%2520hidden%2520neurons%252C%2520convolutional%2520kernels%252C%2520or%2520attention%2520heads.%2520Specifically%252C%2520we%2520show%2520that%2520linear%2520networks%2520learn%2520solutions%2520of%2520increasing%2520rank%252C%2520ReLU%2520networks%2520learn%2520solutions%2520with%2520an%2520increasing%2520number%2520of%2520kinks%252C%2520convolutional%2520networks%2520learn%2520solutions%2520with%2520an%2520increasing%2520number%2520of%2520convolutional%2520kernels%252C%2520and%2520self-attention%2520models%2520learn%2520solutions%2520with%2520an%2520increasing%2520number%2520of%2520attention%2520heads.%2520By%2520analyzing%2520fixed%2520points%252C%2520invariant%2520manifolds%252C%2520and%2520dynamics%2520of%2520gradient%2520descent%2520learning%252C%2520we%2520show%2520that%2520saddle-to-saddle%2520dynamics%2520operates%2520by%2520iteratively%2520evolving%2520near%2520an%2520invariant%2520manifold%252C%2520approaching%2520a%2520saddle%252C%2520and%2520switching%2520to%2520another%2520invariant%2520manifold.%2520Our%2520analysis%2520also%2520illuminates%2520the%2520effects%2520of%2520data%2520distribution%2520and%2520weight%2520initialization%2520on%2520the%2520duration%2520and%2520number%2520of%2520plateaus%2520in%2520learning%252C%2520dissociating%2520previously%2520confounding%2520factors.%2520Overall%252C%2520our%2520theory%2520offers%2520a%2520framework%2520for%2520understanding%2520when%2520and%2520why%2520gradient%2520descent%2520progressively%2520learns%2520increasingly%2520complex%2520solutions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Saddle-to-Saddle%20Dynamics%20Explains%20A%20Simplicity%20Bias%20Across%20Neural%20Network%20Architectures&entry.906535625=Yedi%20Zhang%20and%20Andrew%20Saxe%20and%20Peter%20E.%20Latham&entry.1292438233=Neural%20networks%20trained%20with%20gradient%20descent%20often%20learn%20solutions%20of%20increasing%20complexity%20over%20time%2C%20a%20phenomenon%20known%20as%20simplicity%20bias.%20Despite%20being%20widely%20observed%20across%20architectures%2C%20existing%20theoretical%20treatments%20lack%20a%20unifying%20framework.%20We%20present%20a%20theoretical%20framework%20that%20explains%20a%20simplicity%20bias%20arising%20from%20saddle-to-saddle%20learning%20dynamics%20for%20a%20general%20class%20of%20neural%20networks%2C%20incorporating%20fully-connected%2C%20convolutional%2C%20and%20attention-based%20architectures.%20Here%2C%20simple%20means%20expressible%20with%20few%20hidden%20units%2C%20i.e.%2C%20hidden%20neurons%2C%20convolutional%20kernels%2C%20or%20attention%20heads.%20Specifically%2C%20we%20show%20that%20linear%20networks%20learn%20solutions%20of%20increasing%20rank%2C%20ReLU%20networks%20learn%20solutions%20with%20an%20increasing%20number%20of%20kinks%2C%20convolutional%20networks%20learn%20solutions%20with%20an%20increasing%20number%20of%20convolutional%20kernels%2C%20and%20self-attention%20models%20learn%20solutions%20with%20an%20increasing%20number%20of%20attention%20heads.%20By%20analyzing%20fixed%20points%2C%20invariant%20manifolds%2C%20and%20dynamics%20of%20gradient%20descent%20learning%2C%20we%20show%20that%20saddle-to-saddle%20dynamics%20operates%20by%20iteratively%20evolving%20near%20an%20invariant%20manifold%2C%20approaching%20a%20saddle%2C%20and%20switching%20to%20another%20invariant%20manifold.%20Our%20analysis%20also%20illuminates%20the%20effects%20of%20data%20distribution%20and%20weight%20initialization%20on%20the%20duration%20and%20number%20of%20plateaus%20in%20learning%2C%20dissociating%20previously%20confounding%20factors.%20Overall%2C%20our%20theory%20offers%20a%20framework%20for%20understanding%20when%20and%20why%20gradient%20descent%20progressively%20learns%20increasingly%20complex%20solutions.&entry.1838667208=http%3A//arxiv.org/abs/2512.20607v1&entry.124074799=Read"},
{"title": "SynCraft: Guiding Large Language Models to Predict Edit Sequences for Molecular Synthesizability Optimization", "author": "Junren Li and Luhua Lai", "abstract": "Generative artificial intelligence has revolutionized the exploration of chemical space, yet a critical bottleneck remains that a substantial fraction of generated molecules is synthetically inaccessible. Current solutions, such as post-hoc filtering or projection-based methods, often compromise structural novelty or disrupt key pharmacophores by forcing molecules into pre-defined synthetic templates. Herein, we introduce SynCraft, a reasoning-based framework that reframes synthesizability optimization not as a sequence translation task, but as a precise structural editing problem. Leveraging the emergent reasoning capabilities of Large Language Models, SynCraft navigates the \"synthesis cliff\" where minimal structural modifications yield significant gains in synthetic feasibility. By predicting executable sequences of atom-level edits rather than generating SMILES strings directly, SynCraft circumvents the syntactic fragility of LLMs while harnessing their chemical intuition. Extensive benchmarks demonstrate that SynCraft outperforms state-of-the-art baselines in generating synthesizable analogs with high structural fidelity. Furthermore, through interaction-aware prompting, SynCraft successfully replicates expert medicinal chemistry intuition in editing PLK1 inhibitors and rescuing high-scoring but previously discarded RIPK1 candidates in previous molecular generation literatures.", "link": "http://arxiv.org/abs/2512.20333v1", "date": "2025-12-23", "relevancy": 1.9654, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5097}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4975}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynCraft%3A%20Guiding%20Large%20Language%20Models%20to%20Predict%20Edit%20Sequences%20for%20Molecular%20Synthesizability%20Optimization&body=Title%3A%20SynCraft%3A%20Guiding%20Large%20Language%20Models%20to%20Predict%20Edit%20Sequences%20for%20Molecular%20Synthesizability%20Optimization%0AAuthor%3A%20Junren%20Li%20and%20Luhua%20Lai%0AAbstract%3A%20Generative%20artificial%20intelligence%20has%20revolutionized%20the%20exploration%20of%20chemical%20space%2C%20yet%20a%20critical%20bottleneck%20remains%20that%20a%20substantial%20fraction%20of%20generated%20molecules%20is%20synthetically%20inaccessible.%20Current%20solutions%2C%20such%20as%20post-hoc%20filtering%20or%20projection-based%20methods%2C%20often%20compromise%20structural%20novelty%20or%20disrupt%20key%20pharmacophores%20by%20forcing%20molecules%20into%20pre-defined%20synthetic%20templates.%20Herein%2C%20we%20introduce%20SynCraft%2C%20a%20reasoning-based%20framework%20that%20reframes%20synthesizability%20optimization%20not%20as%20a%20sequence%20translation%20task%2C%20but%20as%20a%20precise%20structural%20editing%20problem.%20Leveraging%20the%20emergent%20reasoning%20capabilities%20of%20Large%20Language%20Models%2C%20SynCraft%20navigates%20the%20%22synthesis%20cliff%22%20where%20minimal%20structural%20modifications%20yield%20significant%20gains%20in%20synthetic%20feasibility.%20By%20predicting%20executable%20sequences%20of%20atom-level%20edits%20rather%20than%20generating%20SMILES%20strings%20directly%2C%20SynCraft%20circumvents%20the%20syntactic%20fragility%20of%20LLMs%20while%20harnessing%20their%20chemical%20intuition.%20Extensive%20benchmarks%20demonstrate%20that%20SynCraft%20outperforms%20state-of-the-art%20baselines%20in%20generating%20synthesizable%20analogs%20with%20high%20structural%20fidelity.%20Furthermore%2C%20through%20interaction-aware%20prompting%2C%20SynCraft%20successfully%20replicates%20expert%20medicinal%20chemistry%20intuition%20in%20editing%20PLK1%20inhibitors%20and%20rescuing%20high-scoring%20but%20previously%20discarded%20RIPK1%20candidates%20in%20previous%20molecular%20generation%20literatures.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynCraft%253A%2520Guiding%2520Large%2520Language%2520Models%2520to%2520Predict%2520Edit%2520Sequences%2520for%2520Molecular%2520Synthesizability%2520Optimization%26entry.906535625%3DJunren%2520Li%2520and%2520Luhua%2520Lai%26entry.1292438233%3DGenerative%2520artificial%2520intelligence%2520has%2520revolutionized%2520the%2520exploration%2520of%2520chemical%2520space%252C%2520yet%2520a%2520critical%2520bottleneck%2520remains%2520that%2520a%2520substantial%2520fraction%2520of%2520generated%2520molecules%2520is%2520synthetically%2520inaccessible.%2520Current%2520solutions%252C%2520such%2520as%2520post-hoc%2520filtering%2520or%2520projection-based%2520methods%252C%2520often%2520compromise%2520structural%2520novelty%2520or%2520disrupt%2520key%2520pharmacophores%2520by%2520forcing%2520molecules%2520into%2520pre-defined%2520synthetic%2520templates.%2520Herein%252C%2520we%2520introduce%2520SynCraft%252C%2520a%2520reasoning-based%2520framework%2520that%2520reframes%2520synthesizability%2520optimization%2520not%2520as%2520a%2520sequence%2520translation%2520task%252C%2520but%2520as%2520a%2520precise%2520structural%2520editing%2520problem.%2520Leveraging%2520the%2520emergent%2520reasoning%2520capabilities%2520of%2520Large%2520Language%2520Models%252C%2520SynCraft%2520navigates%2520the%2520%2522synthesis%2520cliff%2522%2520where%2520minimal%2520structural%2520modifications%2520yield%2520significant%2520gains%2520in%2520synthetic%2520feasibility.%2520By%2520predicting%2520executable%2520sequences%2520of%2520atom-level%2520edits%2520rather%2520than%2520generating%2520SMILES%2520strings%2520directly%252C%2520SynCraft%2520circumvents%2520the%2520syntactic%2520fragility%2520of%2520LLMs%2520while%2520harnessing%2520their%2520chemical%2520intuition.%2520Extensive%2520benchmarks%2520demonstrate%2520that%2520SynCraft%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520generating%2520synthesizable%2520analogs%2520with%2520high%2520structural%2520fidelity.%2520Furthermore%252C%2520through%2520interaction-aware%2520prompting%252C%2520SynCraft%2520successfully%2520replicates%2520expert%2520medicinal%2520chemistry%2520intuition%2520in%2520editing%2520PLK1%2520inhibitors%2520and%2520rescuing%2520high-scoring%2520but%2520previously%2520discarded%2520RIPK1%2520candidates%2520in%2520previous%2520molecular%2520generation%2520literatures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynCraft%3A%20Guiding%20Large%20Language%20Models%20to%20Predict%20Edit%20Sequences%20for%20Molecular%20Synthesizability%20Optimization&entry.906535625=Junren%20Li%20and%20Luhua%20Lai&entry.1292438233=Generative%20artificial%20intelligence%20has%20revolutionized%20the%20exploration%20of%20chemical%20space%2C%20yet%20a%20critical%20bottleneck%20remains%20that%20a%20substantial%20fraction%20of%20generated%20molecules%20is%20synthetically%20inaccessible.%20Current%20solutions%2C%20such%20as%20post-hoc%20filtering%20or%20projection-based%20methods%2C%20often%20compromise%20structural%20novelty%20or%20disrupt%20key%20pharmacophores%20by%20forcing%20molecules%20into%20pre-defined%20synthetic%20templates.%20Herein%2C%20we%20introduce%20SynCraft%2C%20a%20reasoning-based%20framework%20that%20reframes%20synthesizability%20optimization%20not%20as%20a%20sequence%20translation%20task%2C%20but%20as%20a%20precise%20structural%20editing%20problem.%20Leveraging%20the%20emergent%20reasoning%20capabilities%20of%20Large%20Language%20Models%2C%20SynCraft%20navigates%20the%20%22synthesis%20cliff%22%20where%20minimal%20structural%20modifications%20yield%20significant%20gains%20in%20synthetic%20feasibility.%20By%20predicting%20executable%20sequences%20of%20atom-level%20edits%20rather%20than%20generating%20SMILES%20strings%20directly%2C%20SynCraft%20circumvents%20the%20syntactic%20fragility%20of%20LLMs%20while%20harnessing%20their%20chemical%20intuition.%20Extensive%20benchmarks%20demonstrate%20that%20SynCraft%20outperforms%20state-of-the-art%20baselines%20in%20generating%20synthesizable%20analogs%20with%20high%20structural%20fidelity.%20Furthermore%2C%20through%20interaction-aware%20prompting%2C%20SynCraft%20successfully%20replicates%20expert%20medicinal%20chemistry%20intuition%20in%20editing%20PLK1%20inhibitors%20and%20rescuing%20high-scoring%20but%20previously%20discarded%20RIPK1%20candidates%20in%20previous%20molecular%20generation%20literatures.&entry.1838667208=http%3A//arxiv.org/abs/2512.20333v1&entry.124074799=Read"},
{"title": "Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models", "author": "Nikita Agrawal and Simon Mertel and Ruben Mayer", "abstract": "Post-training of foundation language models has emerged as a promising research domain in federated learning (FL) with the goal to enable privacy-preserving model improvements and adaptations to user's downstream tasks. Recent advances in this area adopt centralized post-training approaches that build upon black-box foundation language models where there is no access to model weights and architecture details. Although the use of black-box models has been successful in centralized post-training, their blind replication in FL raises several concerns. Our position is that using black-box models in FL contradicts the core principles of federation such as data privacy and autonomy. In this position paper, we critically analyze the usage of black-box models in federated post-training, and provide a detailed account of various aspects of openness and their implications for FL.", "link": "http://arxiv.org/abs/2505.23593v3", "date": "2025-12-23", "relevancy": 1.9647, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20Federated%20Foundation%20Language%20Model%20Post-Training%20Should%20Focus%20on%20Open-Source%20Models&body=Title%3A%20Position%3A%20Federated%20Foundation%20Language%20Model%20Post-Training%20Should%20Focus%20on%20Open-Source%20Models%0AAuthor%3A%20Nikita%20Agrawal%20and%20Simon%20Mertel%20and%20Ruben%20Mayer%0AAbstract%3A%20Post-training%20of%20foundation%20language%20models%20has%20emerged%20as%20a%20promising%20research%20domain%20in%20federated%20learning%20%28FL%29%20with%20the%20goal%20to%20enable%20privacy-preserving%20model%20improvements%20and%20adaptations%20to%20user%27s%20downstream%20tasks.%20Recent%20advances%20in%20this%20area%20adopt%20centralized%20post-training%20approaches%20that%20build%20upon%20black-box%20foundation%20language%20models%20where%20there%20is%20no%20access%20to%20model%20weights%20and%20architecture%20details.%20Although%20the%20use%20of%20black-box%20models%20has%20been%20successful%20in%20centralized%20post-training%2C%20their%20blind%20replication%20in%20FL%20raises%20several%20concerns.%20Our%20position%20is%20that%20using%20black-box%20models%20in%20FL%20contradicts%20the%20core%20principles%20of%20federation%20such%20as%20data%20privacy%20and%20autonomy.%20In%20this%20position%20paper%2C%20we%20critically%20analyze%20the%20usage%20of%20black-box%20models%20in%20federated%20post-training%2C%20and%20provide%20a%20detailed%20account%20of%20various%20aspects%20of%20openness%20and%20their%20implications%20for%20FL.%0ALink%3A%20http%3A//arxiv.org/abs/2505.23593v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520Federated%2520Foundation%2520Language%2520Model%2520Post-Training%2520Should%2520Focus%2520on%2520Open-Source%2520Models%26entry.906535625%3DNikita%2520Agrawal%2520and%2520Simon%2520Mertel%2520and%2520Ruben%2520Mayer%26entry.1292438233%3DPost-training%2520of%2520foundation%2520language%2520models%2520has%2520emerged%2520as%2520a%2520promising%2520research%2520domain%2520in%2520federated%2520learning%2520%2528FL%2529%2520with%2520the%2520goal%2520to%2520enable%2520privacy-preserving%2520model%2520improvements%2520and%2520adaptations%2520to%2520user%2527s%2520downstream%2520tasks.%2520Recent%2520advances%2520in%2520this%2520area%2520adopt%2520centralized%2520post-training%2520approaches%2520that%2520build%2520upon%2520black-box%2520foundation%2520language%2520models%2520where%2520there%2520is%2520no%2520access%2520to%2520model%2520weights%2520and%2520architecture%2520details.%2520Although%2520the%2520use%2520of%2520black-box%2520models%2520has%2520been%2520successful%2520in%2520centralized%2520post-training%252C%2520their%2520blind%2520replication%2520in%2520FL%2520raises%2520several%2520concerns.%2520Our%2520position%2520is%2520that%2520using%2520black-box%2520models%2520in%2520FL%2520contradicts%2520the%2520core%2520principles%2520of%2520federation%2520such%2520as%2520data%2520privacy%2520and%2520autonomy.%2520In%2520this%2520position%2520paper%252C%2520we%2520critically%2520analyze%2520the%2520usage%2520of%2520black-box%2520models%2520in%2520federated%2520post-training%252C%2520and%2520provide%2520a%2520detailed%2520account%2520of%2520various%2520aspects%2520of%2520openness%2520and%2520their%2520implications%2520for%2520FL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23593v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20Federated%20Foundation%20Language%20Model%20Post-Training%20Should%20Focus%20on%20Open-Source%20Models&entry.906535625=Nikita%20Agrawal%20and%20Simon%20Mertel%20and%20Ruben%20Mayer&entry.1292438233=Post-training%20of%20foundation%20language%20models%20has%20emerged%20as%20a%20promising%20research%20domain%20in%20federated%20learning%20%28FL%29%20with%20the%20goal%20to%20enable%20privacy-preserving%20model%20improvements%20and%20adaptations%20to%20user%27s%20downstream%20tasks.%20Recent%20advances%20in%20this%20area%20adopt%20centralized%20post-training%20approaches%20that%20build%20upon%20black-box%20foundation%20language%20models%20where%20there%20is%20no%20access%20to%20model%20weights%20and%20architecture%20details.%20Although%20the%20use%20of%20black-box%20models%20has%20been%20successful%20in%20centralized%20post-training%2C%20their%20blind%20replication%20in%20FL%20raises%20several%20concerns.%20Our%20position%20is%20that%20using%20black-box%20models%20in%20FL%20contradicts%20the%20core%20principles%20of%20federation%20such%20as%20data%20privacy%20and%20autonomy.%20In%20this%20position%20paper%2C%20we%20critically%20analyze%20the%20usage%20of%20black-box%20models%20in%20federated%20post-training%2C%20and%20provide%20a%20detailed%20account%20of%20various%20aspects%20of%20openness%20and%20their%20implications%20for%20FL.&entry.1838667208=http%3A//arxiv.org/abs/2505.23593v3&entry.124074799=Read"},
{"title": "Recurrent Off-Policy Deep Reinforcement Learning Doesn't Have to be Slow", "author": "Tyler Clark and Christine Evers and Jonathon Hare", "abstract": "Recurrent off-policy deep reinforcement learning models achieve state-of-the-art performance but are often sidelined due to their high computational demands. In response, we introduce RISE (Recurrent Integration via Simplified Encodings), a novel approach that can leverage recurrent networks in any image-based off-policy RL setting without significant computational overheads via using both learnable and non-learnable encoder layers. When integrating RISE into leading non-recurrent off-policy RL algorithms, we observe a 35.6% human-normalized interquartile mean (IQM) performance improvement across the Atari benchmark. We analyze various implementation strategies to highlight the versatility and potential of our proposed framework.", "link": "http://arxiv.org/abs/2512.20513v1", "date": "2025-12-23", "relevancy": 1.9591, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4907}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4905}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recurrent%20Off-Policy%20Deep%20Reinforcement%20Learning%20Doesn%27t%20Have%20to%20be%20Slow&body=Title%3A%20Recurrent%20Off-Policy%20Deep%20Reinforcement%20Learning%20Doesn%27t%20Have%20to%20be%20Slow%0AAuthor%3A%20Tyler%20Clark%20and%20Christine%20Evers%20and%20Jonathon%20Hare%0AAbstract%3A%20Recurrent%20off-policy%20deep%20reinforcement%20learning%20models%20achieve%20state-of-the-art%20performance%20but%20are%20often%20sidelined%20due%20to%20their%20high%20computational%20demands.%20In%20response%2C%20we%20introduce%20RISE%20%28Recurrent%20Integration%20via%20Simplified%20Encodings%29%2C%20a%20novel%20approach%20that%20can%20leverage%20recurrent%20networks%20in%20any%20image-based%20off-policy%20RL%20setting%20without%20significant%20computational%20overheads%20via%20using%20both%20learnable%20and%20non-learnable%20encoder%20layers.%20When%20integrating%20RISE%20into%20leading%20non-recurrent%20off-policy%20RL%20algorithms%2C%20we%20observe%20a%2035.6%25%20human-normalized%20interquartile%20mean%20%28IQM%29%20performance%20improvement%20across%20the%20Atari%20benchmark.%20We%20analyze%20various%20implementation%20strategies%20to%20highlight%20the%20versatility%20and%20potential%20of%20our%20proposed%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrent%2520Off-Policy%2520Deep%2520Reinforcement%2520Learning%2520Doesn%2527t%2520Have%2520to%2520be%2520Slow%26entry.906535625%3DTyler%2520Clark%2520and%2520Christine%2520Evers%2520and%2520Jonathon%2520Hare%26entry.1292438233%3DRecurrent%2520off-policy%2520deep%2520reinforcement%2520learning%2520models%2520achieve%2520state-of-the-art%2520performance%2520but%2520are%2520often%2520sidelined%2520due%2520to%2520their%2520high%2520computational%2520demands.%2520In%2520response%252C%2520we%2520introduce%2520RISE%2520%2528Recurrent%2520Integration%2520via%2520Simplified%2520Encodings%2529%252C%2520a%2520novel%2520approach%2520that%2520can%2520leverage%2520recurrent%2520networks%2520in%2520any%2520image-based%2520off-policy%2520RL%2520setting%2520without%2520significant%2520computational%2520overheads%2520via%2520using%2520both%2520learnable%2520and%2520non-learnable%2520encoder%2520layers.%2520When%2520integrating%2520RISE%2520into%2520leading%2520non-recurrent%2520off-policy%2520RL%2520algorithms%252C%2520we%2520observe%2520a%252035.6%2525%2520human-normalized%2520interquartile%2520mean%2520%2528IQM%2529%2520performance%2520improvement%2520across%2520the%2520Atari%2520benchmark.%2520We%2520analyze%2520various%2520implementation%2520strategies%2520to%2520highlight%2520the%2520versatility%2520and%2520potential%2520of%2520our%2520proposed%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recurrent%20Off-Policy%20Deep%20Reinforcement%20Learning%20Doesn%27t%20Have%20to%20be%20Slow&entry.906535625=Tyler%20Clark%20and%20Christine%20Evers%20and%20Jonathon%20Hare&entry.1292438233=Recurrent%20off-policy%20deep%20reinforcement%20learning%20models%20achieve%20state-of-the-art%20performance%20but%20are%20often%20sidelined%20due%20to%20their%20high%20computational%20demands.%20In%20response%2C%20we%20introduce%20RISE%20%28Recurrent%20Integration%20via%20Simplified%20Encodings%29%2C%20a%20novel%20approach%20that%20can%20leverage%20recurrent%20networks%20in%20any%20image-based%20off-policy%20RL%20setting%20without%20significant%20computational%20overheads%20via%20using%20both%20learnable%20and%20non-learnable%20encoder%20layers.%20When%20integrating%20RISE%20into%20leading%20non-recurrent%20off-policy%20RL%20algorithms%2C%20we%20observe%20a%2035.6%25%20human-normalized%20interquartile%20mean%20%28IQM%29%20performance%20improvement%20across%20the%20Atari%20benchmark.%20We%20analyze%20various%20implementation%20strategies%20to%20highlight%20the%20versatility%20and%20potential%20of%20our%20proposed%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2512.20513v1&entry.124074799=Read"},
{"title": "Learning a Neural Solver for Parametric PDE to Enhance Physics-Informed Methods", "author": "Lise Le Boudec and Emmanuel de Bezenac and Louis Serrano and Ramon Daniel Regueiro-Espino and Yuan Yin and Patrick Gallinari", "abstract": "Physics-informed deep learning often faces optimization challenges due to the complexity of solving partial differential equations (PDEs), which involve exploring large solution spaces, require numerous iterations, and can lead to unstable training. These challenges arise particularly from the ill-conditioning of the optimization problem caused by the differential terms in the loss function. To address these issues, we propose learning a solver, i.e., solving PDEs using a physics-informed iterative algorithm trained on data. Our method learns to condition a gradient descent algorithm that automatically adapts to each PDE instance, significantly accelerating and stabilizing the optimization process and enabling faster convergence of physics-aware models. Furthermore, while traditional physics-informed methods solve for a single PDE instance, our approach extends to parametric PDEs. Specifically, we integrate the physical loss gradient with PDE parameters, allowing our method to solve over a distribution of PDE parameters, including coefficients, initial conditions, and boundary conditions. We demonstrate the effectiveness of our approach through empirical experiments on multiple datasets, comparing both training and test-time optimization performance. The code is available at https://github.com/2ailesB/neural-parametric-solver.", "link": "http://arxiv.org/abs/2410.06820v4", "date": "2025-12-23", "relevancy": 1.959, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5033}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4952}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20a%20Neural%20Solver%20for%20Parametric%20PDE%20to%20Enhance%20Physics-Informed%20Methods&body=Title%3A%20Learning%20a%20Neural%20Solver%20for%20Parametric%20PDE%20to%20Enhance%20Physics-Informed%20Methods%0AAuthor%3A%20Lise%20Le%20Boudec%20and%20Emmanuel%20de%20Bezenac%20and%20Louis%20Serrano%20and%20Ramon%20Daniel%20Regueiro-Espino%20and%20Yuan%20Yin%20and%20Patrick%20Gallinari%0AAbstract%3A%20Physics-informed%20deep%20learning%20often%20faces%20optimization%20challenges%20due%20to%20the%20complexity%20of%20solving%20partial%20differential%20equations%20%28PDEs%29%2C%20which%20involve%20exploring%20large%20solution%20spaces%2C%20require%20numerous%20iterations%2C%20and%20can%20lead%20to%20unstable%20training.%20These%20challenges%20arise%20particularly%20from%20the%20ill-conditioning%20of%20the%20optimization%20problem%20caused%20by%20the%20differential%20terms%20in%20the%20loss%20function.%20To%20address%20these%20issues%2C%20we%20propose%20learning%20a%20solver%2C%20i.e.%2C%20solving%20PDEs%20using%20a%20physics-informed%20iterative%20algorithm%20trained%20on%20data.%20Our%20method%20learns%20to%20condition%20a%20gradient%20descent%20algorithm%20that%20automatically%20adapts%20to%20each%20PDE%20instance%2C%20significantly%20accelerating%20and%20stabilizing%20the%20optimization%20process%20and%20enabling%20faster%20convergence%20of%20physics-aware%20models.%20Furthermore%2C%20while%20traditional%20physics-informed%20methods%20solve%20for%20a%20single%20PDE%20instance%2C%20our%20approach%20extends%20to%20parametric%20PDEs.%20Specifically%2C%20we%20integrate%20the%20physical%20loss%20gradient%20with%20PDE%20parameters%2C%20allowing%20our%20method%20to%20solve%20over%20a%20distribution%20of%20PDE%20parameters%2C%20including%20coefficients%2C%20initial%20conditions%2C%20and%20boundary%20conditions.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20through%20empirical%20experiments%20on%20multiple%20datasets%2C%20comparing%20both%20training%20and%20test-time%20optimization%20performance.%20The%20code%20is%20available%20at%20https%3A//github.com/2ailesB/neural-parametric-solver.%0ALink%3A%20http%3A//arxiv.org/abs/2410.06820v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520a%2520Neural%2520Solver%2520for%2520Parametric%2520PDE%2520to%2520Enhance%2520Physics-Informed%2520Methods%26entry.906535625%3DLise%2520Le%2520Boudec%2520and%2520Emmanuel%2520de%2520Bezenac%2520and%2520Louis%2520Serrano%2520and%2520Ramon%2520Daniel%2520Regueiro-Espino%2520and%2520Yuan%2520Yin%2520and%2520Patrick%2520Gallinari%26entry.1292438233%3DPhysics-informed%2520deep%2520learning%2520often%2520faces%2520optimization%2520challenges%2520due%2520to%2520the%2520complexity%2520of%2520solving%2520partial%2520differential%2520equations%2520%2528PDEs%2529%252C%2520which%2520involve%2520exploring%2520large%2520solution%2520spaces%252C%2520require%2520numerous%2520iterations%252C%2520and%2520can%2520lead%2520to%2520unstable%2520training.%2520These%2520challenges%2520arise%2520particularly%2520from%2520the%2520ill-conditioning%2520of%2520the%2520optimization%2520problem%2520caused%2520by%2520the%2520differential%2520terms%2520in%2520the%2520loss%2520function.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520learning%2520a%2520solver%252C%2520i.e.%252C%2520solving%2520PDEs%2520using%2520a%2520physics-informed%2520iterative%2520algorithm%2520trained%2520on%2520data.%2520Our%2520method%2520learns%2520to%2520condition%2520a%2520gradient%2520descent%2520algorithm%2520that%2520automatically%2520adapts%2520to%2520each%2520PDE%2520instance%252C%2520significantly%2520accelerating%2520and%2520stabilizing%2520the%2520optimization%2520process%2520and%2520enabling%2520faster%2520convergence%2520of%2520physics-aware%2520models.%2520Furthermore%252C%2520while%2520traditional%2520physics-informed%2520methods%2520solve%2520for%2520a%2520single%2520PDE%2520instance%252C%2520our%2520approach%2520extends%2520to%2520parametric%2520PDEs.%2520Specifically%252C%2520we%2520integrate%2520the%2520physical%2520loss%2520gradient%2520with%2520PDE%2520parameters%252C%2520allowing%2520our%2520method%2520to%2520solve%2520over%2520a%2520distribution%2520of%2520PDE%2520parameters%252C%2520including%2520coefficients%252C%2520initial%2520conditions%252C%2520and%2520boundary%2520conditions.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520through%2520empirical%2520experiments%2520on%2520multiple%2520datasets%252C%2520comparing%2520both%2520training%2520and%2520test-time%2520optimization%2520performance.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/2ailesB/neural-parametric-solver.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06820v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20a%20Neural%20Solver%20for%20Parametric%20PDE%20to%20Enhance%20Physics-Informed%20Methods&entry.906535625=Lise%20Le%20Boudec%20and%20Emmanuel%20de%20Bezenac%20and%20Louis%20Serrano%20and%20Ramon%20Daniel%20Regueiro-Espino%20and%20Yuan%20Yin%20and%20Patrick%20Gallinari&entry.1292438233=Physics-informed%20deep%20learning%20often%20faces%20optimization%20challenges%20due%20to%20the%20complexity%20of%20solving%20partial%20differential%20equations%20%28PDEs%29%2C%20which%20involve%20exploring%20large%20solution%20spaces%2C%20require%20numerous%20iterations%2C%20and%20can%20lead%20to%20unstable%20training.%20These%20challenges%20arise%20particularly%20from%20the%20ill-conditioning%20of%20the%20optimization%20problem%20caused%20by%20the%20differential%20terms%20in%20the%20loss%20function.%20To%20address%20these%20issues%2C%20we%20propose%20learning%20a%20solver%2C%20i.e.%2C%20solving%20PDEs%20using%20a%20physics-informed%20iterative%20algorithm%20trained%20on%20data.%20Our%20method%20learns%20to%20condition%20a%20gradient%20descent%20algorithm%20that%20automatically%20adapts%20to%20each%20PDE%20instance%2C%20significantly%20accelerating%20and%20stabilizing%20the%20optimization%20process%20and%20enabling%20faster%20convergence%20of%20physics-aware%20models.%20Furthermore%2C%20while%20traditional%20physics-informed%20methods%20solve%20for%20a%20single%20PDE%20instance%2C%20our%20approach%20extends%20to%20parametric%20PDEs.%20Specifically%2C%20we%20integrate%20the%20physical%20loss%20gradient%20with%20PDE%20parameters%2C%20allowing%20our%20method%20to%20solve%20over%20a%20distribution%20of%20PDE%20parameters%2C%20including%20coefficients%2C%20initial%20conditions%2C%20and%20boundary%20conditions.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20through%20empirical%20experiments%20on%20multiple%20datasets%2C%20comparing%20both%20training%20and%20test-time%20optimization%20performance.%20The%20code%20is%20available%20at%20https%3A//github.com/2ailesB/neural-parametric-solver.&entry.1838667208=http%3A//arxiv.org/abs/2410.06820v4&entry.124074799=Read"},
{"title": "Spectral Bottleneck in Sinusoidal Representation Networks: Noise is All You Need", "author": "Hemanth Chandravamsi and Dhanush V. Shenoy and Itay Zinn and Ziv Chen and Shimon Pisnoy and Steven H. Frankel", "abstract": "This work identifies and attempts to address a fundamental limitation of implicit neural representations with sinusoidal activation. The fitting error of SIRENs is highly sensitive to the target frequency content and to the choice of initialization. In extreme cases, this sensitivity leads to a spectral bottleneck that can result in a zero-valued output. This phenomenon is characterized by analyzing the evolution of activation spectra and the empirical neural tangent kernel (NTK) during the training process. An unfavorable distribution of energy across frequency modes was noted to give rise to this failure mode. Furthermore, the effect of Gaussian perturbations applied to the baseline uniformly initialized weights is examined, showing how these perturbations influence activation spectra and the NTK eigenbasis of SIREN. Overall, initialization emerges as a central factor governing the evolution of SIRENs, indicating the need for adaptive, target-aware strategies as the target length increases and fine-scale detail becomes essential. The proposed weight initialization scheme (WINNER) represents a simple ad hoc step in this direction and demonstrates that fitting accuracy can be significantly improved by modifying the spectral profile of network activations through a target-aware initialization. The approach achieves state-of-the-art performance on audio fitting tasks and yields notable improvements in image fitting tasks.", "link": "http://arxiv.org/abs/2509.09719v2", "date": "2025-12-23", "relevancy": 1.9513, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5003}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4835}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Bottleneck%20in%20Sinusoidal%20Representation%20Networks%3A%20Noise%20is%20All%20You%20Need&body=Title%3A%20Spectral%20Bottleneck%20in%20Sinusoidal%20Representation%20Networks%3A%20Noise%20is%20All%20You%20Need%0AAuthor%3A%20Hemanth%20Chandravamsi%20and%20Dhanush%20V.%20Shenoy%20and%20Itay%20Zinn%20and%20Ziv%20Chen%20and%20Shimon%20Pisnoy%20and%20Steven%20H.%20Frankel%0AAbstract%3A%20This%20work%20identifies%20and%20attempts%20to%20address%20a%20fundamental%20limitation%20of%20implicit%20neural%20representations%20with%20sinusoidal%20activation.%20The%20fitting%20error%20of%20SIRENs%20is%20highly%20sensitive%20to%20the%20target%20frequency%20content%20and%20to%20the%20choice%20of%20initialization.%20In%20extreme%20cases%2C%20this%20sensitivity%20leads%20to%20a%20spectral%20bottleneck%20that%20can%20result%20in%20a%20zero-valued%20output.%20This%20phenomenon%20is%20characterized%20by%20analyzing%20the%20evolution%20of%20activation%20spectra%20and%20the%20empirical%20neural%20tangent%20kernel%20%28NTK%29%20during%20the%20training%20process.%20An%20unfavorable%20distribution%20of%20energy%20across%20frequency%20modes%20was%20noted%20to%20give%20rise%20to%20this%20failure%20mode.%20Furthermore%2C%20the%20effect%20of%20Gaussian%20perturbations%20applied%20to%20the%20baseline%20uniformly%20initialized%20weights%20is%20examined%2C%20showing%20how%20these%20perturbations%20influence%20activation%20spectra%20and%20the%20NTK%20eigenbasis%20of%20SIREN.%20Overall%2C%20initialization%20emerges%20as%20a%20central%20factor%20governing%20the%20evolution%20of%20SIRENs%2C%20indicating%20the%20need%20for%20adaptive%2C%20target-aware%20strategies%20as%20the%20target%20length%20increases%20and%20fine-scale%20detail%20becomes%20essential.%20The%20proposed%20weight%20initialization%20scheme%20%28WINNER%29%20represents%20a%20simple%20ad%20hoc%20step%20in%20this%20direction%20and%20demonstrates%20that%20fitting%20accuracy%20can%20be%20significantly%20improved%20by%20modifying%20the%20spectral%20profile%20of%20network%20activations%20through%20a%20target-aware%20initialization.%20The%20approach%20achieves%20state-of-the-art%20performance%20on%20audio%20fitting%20tasks%20and%20yields%20notable%20improvements%20in%20image%20fitting%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2509.09719v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Bottleneck%2520in%2520Sinusoidal%2520Representation%2520Networks%253A%2520Noise%2520is%2520All%2520You%2520Need%26entry.906535625%3DHemanth%2520Chandravamsi%2520and%2520Dhanush%2520V.%2520Shenoy%2520and%2520Itay%2520Zinn%2520and%2520Ziv%2520Chen%2520and%2520Shimon%2520Pisnoy%2520and%2520Steven%2520H.%2520Frankel%26entry.1292438233%3DThis%2520work%2520identifies%2520and%2520attempts%2520to%2520address%2520a%2520fundamental%2520limitation%2520of%2520implicit%2520neural%2520representations%2520with%2520sinusoidal%2520activation.%2520The%2520fitting%2520error%2520of%2520SIRENs%2520is%2520highly%2520sensitive%2520to%2520the%2520target%2520frequency%2520content%2520and%2520to%2520the%2520choice%2520of%2520initialization.%2520In%2520extreme%2520cases%252C%2520this%2520sensitivity%2520leads%2520to%2520a%2520spectral%2520bottleneck%2520that%2520can%2520result%2520in%2520a%2520zero-valued%2520output.%2520This%2520phenomenon%2520is%2520characterized%2520by%2520analyzing%2520the%2520evolution%2520of%2520activation%2520spectra%2520and%2520the%2520empirical%2520neural%2520tangent%2520kernel%2520%2528NTK%2529%2520during%2520the%2520training%2520process.%2520An%2520unfavorable%2520distribution%2520of%2520energy%2520across%2520frequency%2520modes%2520was%2520noted%2520to%2520give%2520rise%2520to%2520this%2520failure%2520mode.%2520Furthermore%252C%2520the%2520effect%2520of%2520Gaussian%2520perturbations%2520applied%2520to%2520the%2520baseline%2520uniformly%2520initialized%2520weights%2520is%2520examined%252C%2520showing%2520how%2520these%2520perturbations%2520influence%2520activation%2520spectra%2520and%2520the%2520NTK%2520eigenbasis%2520of%2520SIREN.%2520Overall%252C%2520initialization%2520emerges%2520as%2520a%2520central%2520factor%2520governing%2520the%2520evolution%2520of%2520SIRENs%252C%2520indicating%2520the%2520need%2520for%2520adaptive%252C%2520target-aware%2520strategies%2520as%2520the%2520target%2520length%2520increases%2520and%2520fine-scale%2520detail%2520becomes%2520essential.%2520The%2520proposed%2520weight%2520initialization%2520scheme%2520%2528WINNER%2529%2520represents%2520a%2520simple%2520ad%2520hoc%2520step%2520in%2520this%2520direction%2520and%2520demonstrates%2520that%2520fitting%2520accuracy%2520can%2520be%2520significantly%2520improved%2520by%2520modifying%2520the%2520spectral%2520profile%2520of%2520network%2520activations%2520through%2520a%2520target-aware%2520initialization.%2520The%2520approach%2520achieves%2520state-of-the-art%2520performance%2520on%2520audio%2520fitting%2520tasks%2520and%2520yields%2520notable%2520improvements%2520in%2520image%2520fitting%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09719v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Bottleneck%20in%20Sinusoidal%20Representation%20Networks%3A%20Noise%20is%20All%20You%20Need&entry.906535625=Hemanth%20Chandravamsi%20and%20Dhanush%20V.%20Shenoy%20and%20Itay%20Zinn%20and%20Ziv%20Chen%20and%20Shimon%20Pisnoy%20and%20Steven%20H.%20Frankel&entry.1292438233=This%20work%20identifies%20and%20attempts%20to%20address%20a%20fundamental%20limitation%20of%20implicit%20neural%20representations%20with%20sinusoidal%20activation.%20The%20fitting%20error%20of%20SIRENs%20is%20highly%20sensitive%20to%20the%20target%20frequency%20content%20and%20to%20the%20choice%20of%20initialization.%20In%20extreme%20cases%2C%20this%20sensitivity%20leads%20to%20a%20spectral%20bottleneck%20that%20can%20result%20in%20a%20zero-valued%20output.%20This%20phenomenon%20is%20characterized%20by%20analyzing%20the%20evolution%20of%20activation%20spectra%20and%20the%20empirical%20neural%20tangent%20kernel%20%28NTK%29%20during%20the%20training%20process.%20An%20unfavorable%20distribution%20of%20energy%20across%20frequency%20modes%20was%20noted%20to%20give%20rise%20to%20this%20failure%20mode.%20Furthermore%2C%20the%20effect%20of%20Gaussian%20perturbations%20applied%20to%20the%20baseline%20uniformly%20initialized%20weights%20is%20examined%2C%20showing%20how%20these%20perturbations%20influence%20activation%20spectra%20and%20the%20NTK%20eigenbasis%20of%20SIREN.%20Overall%2C%20initialization%20emerges%20as%20a%20central%20factor%20governing%20the%20evolution%20of%20SIRENs%2C%20indicating%20the%20need%20for%20adaptive%2C%20target-aware%20strategies%20as%20the%20target%20length%20increases%20and%20fine-scale%20detail%20becomes%20essential.%20The%20proposed%20weight%20initialization%20scheme%20%28WINNER%29%20represents%20a%20simple%20ad%20hoc%20step%20in%20this%20direction%20and%20demonstrates%20that%20fitting%20accuracy%20can%20be%20significantly%20improved%20by%20modifying%20the%20spectral%20profile%20of%20network%20activations%20through%20a%20target-aware%20initialization.%20The%20approach%20achieves%20state-of-the-art%20performance%20on%20audio%20fitting%20tasks%20and%20yields%20notable%20improvements%20in%20image%20fitting%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2509.09719v2&entry.124074799=Read"},
{"title": "Optimality-Informed Neural Networks for Solving Parametric Optimization Problems", "author": "Matthias K. Hoffmann and Amine Othmane and Kathrin Fla\u00dfkamp", "abstract": "Many engineering tasks require solving families of nonlinear constrained optimization problems, parametrized in setting-specific variables. This is computationally demanding, particularly, if solutions have to be computed across strongly varying parameter values, e.g., in real-time control or for model-based design. Thus, we propose to learn the mapping from parameters to the primal optimal solutions and to their corresponding duals using neural networks, giving a dense estimation in contrast to gridded approaches. Our approach, Optimality-informed Neural Networks (OptINNs), combines (i) a KKT-residual loss that penalizes violations of the first-order optimality conditions under standard constraint qualifications assumptions, and (ii) problem-specific output activations that enforce simple inequality constraints (e.g., box-type/positivity) by construction. This design reduces data requirements, allows the prediction of dual variables, and improves feasibility and closeness to optimality compared to penalty-only training. Taking quadratic penalties as a baseline, since this approach has been previously proposed for the considered problem class in literature, our method simplifies hyperparameter tuning and attains tighter adherence to optimality conditions. We evaluate OptINNs on different nonlinear optimization problems ranging from low to high dimensions. On small problems, OptINNs match a quadratic-penalty baseline in primal accuracy while additionally predicting dual variables with low error. On larger problems, OptINNs achieve lower constraint violations and lower primal error compared to neural networks based on the quadratic-penalty method. These results suggest that embedding feasibility and optimality into the network architecture and loss can make learning-based surrogates more accurate, feasible, and data-efficient for parametric optimization.", "link": "http://arxiv.org/abs/2512.20270v1", "date": "2025-12-23", "relevancy": 1.9491, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5377}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4756}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimality-Informed%20Neural%20Networks%20for%20Solving%20Parametric%20Optimization%20Problems&body=Title%3A%20Optimality-Informed%20Neural%20Networks%20for%20Solving%20Parametric%20Optimization%20Problems%0AAuthor%3A%20Matthias%20K.%20Hoffmann%20and%20Amine%20Othmane%20and%20Kathrin%20Fla%C3%9Fkamp%0AAbstract%3A%20Many%20engineering%20tasks%20require%20solving%20families%20of%20nonlinear%20constrained%20optimization%20problems%2C%20parametrized%20in%20setting-specific%20variables.%20This%20is%20computationally%20demanding%2C%20particularly%2C%20if%20solutions%20have%20to%20be%20computed%20across%20strongly%20varying%20parameter%20values%2C%20e.g.%2C%20in%20real-time%20control%20or%20for%20model-based%20design.%20Thus%2C%20we%20propose%20to%20learn%20the%20mapping%20from%20parameters%20to%20the%20primal%20optimal%20solutions%20and%20to%20their%20corresponding%20duals%20using%20neural%20networks%2C%20giving%20a%20dense%20estimation%20in%20contrast%20to%20gridded%20approaches.%20Our%20approach%2C%20Optimality-informed%20Neural%20Networks%20%28OptINNs%29%2C%20combines%20%28i%29%20a%20KKT-residual%20loss%20that%20penalizes%20violations%20of%20the%20first-order%20optimality%20conditions%20under%20standard%20constraint%20qualifications%20assumptions%2C%20and%20%28ii%29%20problem-specific%20output%20activations%20that%20enforce%20simple%20inequality%20constraints%20%28e.g.%2C%20box-type/positivity%29%20by%20construction.%20This%20design%20reduces%20data%20requirements%2C%20allows%20the%20prediction%20of%20dual%20variables%2C%20and%20improves%20feasibility%20and%20closeness%20to%20optimality%20compared%20to%20penalty-only%20training.%20Taking%20quadratic%20penalties%20as%20a%20baseline%2C%20since%20this%20approach%20has%20been%20previously%20proposed%20for%20the%20considered%20problem%20class%20in%20literature%2C%20our%20method%20simplifies%20hyperparameter%20tuning%20and%20attains%20tighter%20adherence%20to%20optimality%20conditions.%20We%20evaluate%20OptINNs%20on%20different%20nonlinear%20optimization%20problems%20ranging%20from%20low%20to%20high%20dimensions.%20On%20small%20problems%2C%20OptINNs%20match%20a%20quadratic-penalty%20baseline%20in%20primal%20accuracy%20while%20additionally%20predicting%20dual%20variables%20with%20low%20error.%20On%20larger%20problems%2C%20OptINNs%20achieve%20lower%20constraint%20violations%20and%20lower%20primal%20error%20compared%20to%20neural%20networks%20based%20on%20the%20quadratic-penalty%20method.%20These%20results%20suggest%20that%20embedding%20feasibility%20and%20optimality%20into%20the%20network%20architecture%20and%20loss%20can%20make%20learning-based%20surrogates%20more%20accurate%2C%20feasible%2C%20and%20data-efficient%20for%20parametric%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimality-Informed%2520Neural%2520Networks%2520for%2520Solving%2520Parametric%2520Optimization%2520Problems%26entry.906535625%3DMatthias%2520K.%2520Hoffmann%2520and%2520Amine%2520Othmane%2520and%2520Kathrin%2520Fla%25C3%259Fkamp%26entry.1292438233%3DMany%2520engineering%2520tasks%2520require%2520solving%2520families%2520of%2520nonlinear%2520constrained%2520optimization%2520problems%252C%2520parametrized%2520in%2520setting-specific%2520variables.%2520This%2520is%2520computationally%2520demanding%252C%2520particularly%252C%2520if%2520solutions%2520have%2520to%2520be%2520computed%2520across%2520strongly%2520varying%2520parameter%2520values%252C%2520e.g.%252C%2520in%2520real-time%2520control%2520or%2520for%2520model-based%2520design.%2520Thus%252C%2520we%2520propose%2520to%2520learn%2520the%2520mapping%2520from%2520parameters%2520to%2520the%2520primal%2520optimal%2520solutions%2520and%2520to%2520their%2520corresponding%2520duals%2520using%2520neural%2520networks%252C%2520giving%2520a%2520dense%2520estimation%2520in%2520contrast%2520to%2520gridded%2520approaches.%2520Our%2520approach%252C%2520Optimality-informed%2520Neural%2520Networks%2520%2528OptINNs%2529%252C%2520combines%2520%2528i%2529%2520a%2520KKT-residual%2520loss%2520that%2520penalizes%2520violations%2520of%2520the%2520first-order%2520optimality%2520conditions%2520under%2520standard%2520constraint%2520qualifications%2520assumptions%252C%2520and%2520%2528ii%2529%2520problem-specific%2520output%2520activations%2520that%2520enforce%2520simple%2520inequality%2520constraints%2520%2528e.g.%252C%2520box-type/positivity%2529%2520by%2520construction.%2520This%2520design%2520reduces%2520data%2520requirements%252C%2520allows%2520the%2520prediction%2520of%2520dual%2520variables%252C%2520and%2520improves%2520feasibility%2520and%2520closeness%2520to%2520optimality%2520compared%2520to%2520penalty-only%2520training.%2520Taking%2520quadratic%2520penalties%2520as%2520a%2520baseline%252C%2520since%2520this%2520approach%2520has%2520been%2520previously%2520proposed%2520for%2520the%2520considered%2520problem%2520class%2520in%2520literature%252C%2520our%2520method%2520simplifies%2520hyperparameter%2520tuning%2520and%2520attains%2520tighter%2520adherence%2520to%2520optimality%2520conditions.%2520We%2520evaluate%2520OptINNs%2520on%2520different%2520nonlinear%2520optimization%2520problems%2520ranging%2520from%2520low%2520to%2520high%2520dimensions.%2520On%2520small%2520problems%252C%2520OptINNs%2520match%2520a%2520quadratic-penalty%2520baseline%2520in%2520primal%2520accuracy%2520while%2520additionally%2520predicting%2520dual%2520variables%2520with%2520low%2520error.%2520On%2520larger%2520problems%252C%2520OptINNs%2520achieve%2520lower%2520constraint%2520violations%2520and%2520lower%2520primal%2520error%2520compared%2520to%2520neural%2520networks%2520based%2520on%2520the%2520quadratic-penalty%2520method.%2520These%2520results%2520suggest%2520that%2520embedding%2520feasibility%2520and%2520optimality%2520into%2520the%2520network%2520architecture%2520and%2520loss%2520can%2520make%2520learning-based%2520surrogates%2520more%2520accurate%252C%2520feasible%252C%2520and%2520data-efficient%2520for%2520parametric%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimality-Informed%20Neural%20Networks%20for%20Solving%20Parametric%20Optimization%20Problems&entry.906535625=Matthias%20K.%20Hoffmann%20and%20Amine%20Othmane%20and%20Kathrin%20Fla%C3%9Fkamp&entry.1292438233=Many%20engineering%20tasks%20require%20solving%20families%20of%20nonlinear%20constrained%20optimization%20problems%2C%20parametrized%20in%20setting-specific%20variables.%20This%20is%20computationally%20demanding%2C%20particularly%2C%20if%20solutions%20have%20to%20be%20computed%20across%20strongly%20varying%20parameter%20values%2C%20e.g.%2C%20in%20real-time%20control%20or%20for%20model-based%20design.%20Thus%2C%20we%20propose%20to%20learn%20the%20mapping%20from%20parameters%20to%20the%20primal%20optimal%20solutions%20and%20to%20their%20corresponding%20duals%20using%20neural%20networks%2C%20giving%20a%20dense%20estimation%20in%20contrast%20to%20gridded%20approaches.%20Our%20approach%2C%20Optimality-informed%20Neural%20Networks%20%28OptINNs%29%2C%20combines%20%28i%29%20a%20KKT-residual%20loss%20that%20penalizes%20violations%20of%20the%20first-order%20optimality%20conditions%20under%20standard%20constraint%20qualifications%20assumptions%2C%20and%20%28ii%29%20problem-specific%20output%20activations%20that%20enforce%20simple%20inequality%20constraints%20%28e.g.%2C%20box-type/positivity%29%20by%20construction.%20This%20design%20reduces%20data%20requirements%2C%20allows%20the%20prediction%20of%20dual%20variables%2C%20and%20improves%20feasibility%20and%20closeness%20to%20optimality%20compared%20to%20penalty-only%20training.%20Taking%20quadratic%20penalties%20as%20a%20baseline%2C%20since%20this%20approach%20has%20been%20previously%20proposed%20for%20the%20considered%20problem%20class%20in%20literature%2C%20our%20method%20simplifies%20hyperparameter%20tuning%20and%20attains%20tighter%20adherence%20to%20optimality%20conditions.%20We%20evaluate%20OptINNs%20on%20different%20nonlinear%20optimization%20problems%20ranging%20from%20low%20to%20high%20dimensions.%20On%20small%20problems%2C%20OptINNs%20match%20a%20quadratic-penalty%20baseline%20in%20primal%20accuracy%20while%20additionally%20predicting%20dual%20variables%20with%20low%20error.%20On%20larger%20problems%2C%20OptINNs%20achieve%20lower%20constraint%20violations%20and%20lower%20primal%20error%20compared%20to%20neural%20networks%20based%20on%20the%20quadratic-penalty%20method.%20These%20results%20suggest%20that%20embedding%20feasibility%20and%20optimality%20into%20the%20network%20architecture%20and%20loss%20can%20make%20learning-based%20surrogates%20more%20accurate%2C%20feasible%2C%20and%20data-efficient%20for%20parametric%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2512.20270v1&entry.124074799=Read"},
{"title": "LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing", "author": "Changyi Lin and Boda Huo and Mingyang Yu and Emily Ruppel and Bingqing Chen and Jonathan Francis and Ding Zhao", "abstract": "Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.", "link": "http://arxiv.org/abs/2512.20591v1", "date": "2025-12-23", "relevancy": 1.9227, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4944}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.493}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightTact%3A%20A%20Visual-Tactile%20Fingertip%20Sensor%20for%20Deformation-Independent%20Contact%20Sensing&body=Title%3A%20LightTact%3A%20A%20Visual-Tactile%20Fingertip%20Sensor%20for%20Deformation-Independent%20Contact%20Sensing%0AAuthor%3A%20Changyi%20Lin%20and%20Boda%20Huo%20and%20Mingyang%20Yu%20and%20Emily%20Ruppel%20and%20Bingqing%20Chen%20and%20Jonathan%20Francis%20and%20Ding%20Zhao%0AAbstract%3A%20Contact%20often%20occurs%20without%20macroscopic%20surface%20deformation%2C%20such%20as%20during%20interaction%20with%20liquids%2C%20semi-liquids%2C%20or%20ultra-soft%20materials.%20Most%20existing%20tactile%20sensors%20rely%20on%20deformation%20to%20infer%20contact%2C%20making%20such%20light-contact%20interactions%20difficult%20to%20perceive%20robustly.%20To%20address%20this%2C%20we%20present%20LightTact%2C%20a%20visual-tactile%20fingertip%20sensor%20that%20makes%20contact%20directly%20visible%20via%20a%20deformation-independent%2C%20optics-based%20principle.%20LightTact%20uses%20an%20ambient-blocking%20optical%20configuration%20that%20suppresses%20both%20external%20light%20and%20internal%20illumination%20at%20non-contact%20regions%2C%20while%20transmitting%20only%20the%20diffuse%20light%20generated%20at%20true%20contacts.%20As%20a%20result%2C%20LightTact%20produces%20high-contrast%20raw%20images%20in%20which%20non-contact%20pixels%20remain%20near-black%20%28mean%20gray%20value%20%3C%203%29%20and%20contact%20pixels%20preserve%20the%20natural%20appearance%20of%20the%20contacting%20surface.%20Built%20on%20this%2C%20LightTact%20achieves%20accurate%20pixel-level%20contact%20segmentation%20that%20is%20robust%20to%20material%20properties%2C%20contact%20force%2C%20surface%20appearance%2C%20and%20environmental%20lighting.%20We%20further%20integrate%20LightTact%20on%20a%20robotic%20arm%20and%20demonstrate%20manipulation%20behaviors%20driven%20by%20extremely%20light%20contact%2C%20including%20water%20spreading%2C%20facial-cream%20dipping%2C%20and%20thin-film%20interaction.%20Finally%2C%20we%20show%20that%20LightTact%27s%20spatially%20aligned%20visual-tactile%20images%20can%20be%20directly%20interpreted%20by%20existing%20vision-language%20models%2C%20enabling%20resistor%20value%20reasoning%20for%20robotic%20sorting.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightTact%253A%2520A%2520Visual-Tactile%2520Fingertip%2520Sensor%2520for%2520Deformation-Independent%2520Contact%2520Sensing%26entry.906535625%3DChangyi%2520Lin%2520and%2520Boda%2520Huo%2520and%2520Mingyang%2520Yu%2520and%2520Emily%2520Ruppel%2520and%2520Bingqing%2520Chen%2520and%2520Jonathan%2520Francis%2520and%2520Ding%2520Zhao%26entry.1292438233%3DContact%2520often%2520occurs%2520without%2520macroscopic%2520surface%2520deformation%252C%2520such%2520as%2520during%2520interaction%2520with%2520liquids%252C%2520semi-liquids%252C%2520or%2520ultra-soft%2520materials.%2520Most%2520existing%2520tactile%2520sensors%2520rely%2520on%2520deformation%2520to%2520infer%2520contact%252C%2520making%2520such%2520light-contact%2520interactions%2520difficult%2520to%2520perceive%2520robustly.%2520To%2520address%2520this%252C%2520we%2520present%2520LightTact%252C%2520a%2520visual-tactile%2520fingertip%2520sensor%2520that%2520makes%2520contact%2520directly%2520visible%2520via%2520a%2520deformation-independent%252C%2520optics-based%2520principle.%2520LightTact%2520uses%2520an%2520ambient-blocking%2520optical%2520configuration%2520that%2520suppresses%2520both%2520external%2520light%2520and%2520internal%2520illumination%2520at%2520non-contact%2520regions%252C%2520while%2520transmitting%2520only%2520the%2520diffuse%2520light%2520generated%2520at%2520true%2520contacts.%2520As%2520a%2520result%252C%2520LightTact%2520produces%2520high-contrast%2520raw%2520images%2520in%2520which%2520non-contact%2520pixels%2520remain%2520near-black%2520%2528mean%2520gray%2520value%2520%253C%25203%2529%2520and%2520contact%2520pixels%2520preserve%2520the%2520natural%2520appearance%2520of%2520the%2520contacting%2520surface.%2520Built%2520on%2520this%252C%2520LightTact%2520achieves%2520accurate%2520pixel-level%2520contact%2520segmentation%2520that%2520is%2520robust%2520to%2520material%2520properties%252C%2520contact%2520force%252C%2520surface%2520appearance%252C%2520and%2520environmental%2520lighting.%2520We%2520further%2520integrate%2520LightTact%2520on%2520a%2520robotic%2520arm%2520and%2520demonstrate%2520manipulation%2520behaviors%2520driven%2520by%2520extremely%2520light%2520contact%252C%2520including%2520water%2520spreading%252C%2520facial-cream%2520dipping%252C%2520and%2520thin-film%2520interaction.%2520Finally%252C%2520we%2520show%2520that%2520LightTact%2527s%2520spatially%2520aligned%2520visual-tactile%2520images%2520can%2520be%2520directly%2520interpreted%2520by%2520existing%2520vision-language%2520models%252C%2520enabling%2520resistor%2520value%2520reasoning%2520for%2520robotic%2520sorting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightTact%3A%20A%20Visual-Tactile%20Fingertip%20Sensor%20for%20Deformation-Independent%20Contact%20Sensing&entry.906535625=Changyi%20Lin%20and%20Boda%20Huo%20and%20Mingyang%20Yu%20and%20Emily%20Ruppel%20and%20Bingqing%20Chen%20and%20Jonathan%20Francis%20and%20Ding%20Zhao&entry.1292438233=Contact%20often%20occurs%20without%20macroscopic%20surface%20deformation%2C%20such%20as%20during%20interaction%20with%20liquids%2C%20semi-liquids%2C%20or%20ultra-soft%20materials.%20Most%20existing%20tactile%20sensors%20rely%20on%20deformation%20to%20infer%20contact%2C%20making%20such%20light-contact%20interactions%20difficult%20to%20perceive%20robustly.%20To%20address%20this%2C%20we%20present%20LightTact%2C%20a%20visual-tactile%20fingertip%20sensor%20that%20makes%20contact%20directly%20visible%20via%20a%20deformation-independent%2C%20optics-based%20principle.%20LightTact%20uses%20an%20ambient-blocking%20optical%20configuration%20that%20suppresses%20both%20external%20light%20and%20internal%20illumination%20at%20non-contact%20regions%2C%20while%20transmitting%20only%20the%20diffuse%20light%20generated%20at%20true%20contacts.%20As%20a%20result%2C%20LightTact%20produces%20high-contrast%20raw%20images%20in%20which%20non-contact%20pixels%20remain%20near-black%20%28mean%20gray%20value%20%3C%203%29%20and%20contact%20pixels%20preserve%20the%20natural%20appearance%20of%20the%20contacting%20surface.%20Built%20on%20this%2C%20LightTact%20achieves%20accurate%20pixel-level%20contact%20segmentation%20that%20is%20robust%20to%20material%20properties%2C%20contact%20force%2C%20surface%20appearance%2C%20and%20environmental%20lighting.%20We%20further%20integrate%20LightTact%20on%20a%20robotic%20arm%20and%20demonstrate%20manipulation%20behaviors%20driven%20by%20extremely%20light%20contact%2C%20including%20water%20spreading%2C%20facial-cream%20dipping%2C%20and%20thin-film%20interaction.%20Finally%2C%20we%20show%20that%20LightTact%27s%20spatially%20aligned%20visual-tactile%20images%20can%20be%20directly%20interpreted%20by%20existing%20vision-language%20models%2C%20enabling%20resistor%20value%20reasoning%20for%20robotic%20sorting.&entry.1838667208=http%3A//arxiv.org/abs/2512.20591v1&entry.124074799=Read"},
{"title": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents", "author": "Xingbo Du and Loka Li and Duzhen Zhang and Le Song", "abstract": "Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.", "link": "http://arxiv.org/abs/2512.20237v1", "date": "2025-12-23", "relevancy": 1.9163, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4936}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MemR%24%5E3%24%3A%20Memory%20Retrieval%20via%20Reflective%20Reasoning%20for%20LLM%20Agents&body=Title%3A%20MemR%24%5E3%24%3A%20Memory%20Retrieval%20via%20Reflective%20Reasoning%20for%20LLM%20Agents%0AAuthor%3A%20Xingbo%20Du%20and%20Loka%20Li%20and%20Duzhen%20Zhang%20and%20Le%20Song%0AAbstract%3A%20Memory%20systems%20have%20been%20designed%20to%20leverage%20past%20experiences%20in%20Large%20Language%20Model%20%28LLM%29%20agents.%20However%2C%20many%20deployed%20memory%20systems%20primarily%20optimize%20compression%20and%20storage%2C%20with%20comparatively%20less%20emphasis%20on%20explicit%2C%20closed-loop%20control%20of%20memory%20retrieval.%20From%20this%20observation%2C%20we%20build%20memory%20retrieval%20as%20an%20autonomous%2C%20accurate%2C%20and%20compatible%20agent%20system%2C%20named%20MemR%24%5E3%24%2C%20which%20has%20two%20core%20mechanisms%3A%201%29%20a%20router%20that%20selects%20among%20retrieve%2C%20reflect%2C%20and%20answer%20actions%20to%20optimize%20answer%20quality%3B%202%29%20a%20global%20evidence-gap%20tracker%20that%20explicitly%20renders%20the%20answering%20process%20transparent%20and%20tracks%20the%20evidence%20collection%20process.%20This%20design%20departs%20from%20the%20standard%20retrieve-then-answer%20pipeline%20by%20introducing%20a%20closed-loop%20control%20mechanism%20that%20enables%20autonomous%20decision-making.%20Empirical%20results%20on%20the%20LoCoMo%20benchmark%20demonstrate%20that%20MemR%24%5E3%24%20surpasses%20strong%20baselines%20on%20LLM-as-a-Judge%20score%2C%20and%20particularly%2C%20it%20improves%20existing%20retrievers%20across%20four%20categories%20with%20an%20overall%20improvement%20on%20RAG%20%28%2B7.29%25%29%20and%20Zep%20%28%2B1.94%25%29%20using%20GPT-4.1-mini%20backend%2C%20offering%20a%20plug-and-play%20controller%20for%20existing%20memory%20stores.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemR%2524%255E3%2524%253A%2520Memory%2520Retrieval%2520via%2520Reflective%2520Reasoning%2520for%2520LLM%2520Agents%26entry.906535625%3DXingbo%2520Du%2520and%2520Loka%2520Li%2520and%2520Duzhen%2520Zhang%2520and%2520Le%2520Song%26entry.1292438233%3DMemory%2520systems%2520have%2520been%2520designed%2520to%2520leverage%2520past%2520experiences%2520in%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents.%2520However%252C%2520many%2520deployed%2520memory%2520systems%2520primarily%2520optimize%2520compression%2520and%2520storage%252C%2520with%2520comparatively%2520less%2520emphasis%2520on%2520explicit%252C%2520closed-loop%2520control%2520of%2520memory%2520retrieval.%2520From%2520this%2520observation%252C%2520we%2520build%2520memory%2520retrieval%2520as%2520an%2520autonomous%252C%2520accurate%252C%2520and%2520compatible%2520agent%2520system%252C%2520named%2520MemR%2524%255E3%2524%252C%2520which%2520has%2520two%2520core%2520mechanisms%253A%25201%2529%2520a%2520router%2520that%2520selects%2520among%2520retrieve%252C%2520reflect%252C%2520and%2520answer%2520actions%2520to%2520optimize%2520answer%2520quality%253B%25202%2529%2520a%2520global%2520evidence-gap%2520tracker%2520that%2520explicitly%2520renders%2520the%2520answering%2520process%2520transparent%2520and%2520tracks%2520the%2520evidence%2520collection%2520process.%2520This%2520design%2520departs%2520from%2520the%2520standard%2520retrieve-then-answer%2520pipeline%2520by%2520introducing%2520a%2520closed-loop%2520control%2520mechanism%2520that%2520enables%2520autonomous%2520decision-making.%2520Empirical%2520results%2520on%2520the%2520LoCoMo%2520benchmark%2520demonstrate%2520that%2520MemR%2524%255E3%2524%2520surpasses%2520strong%2520baselines%2520on%2520LLM-as-a-Judge%2520score%252C%2520and%2520particularly%252C%2520it%2520improves%2520existing%2520retrievers%2520across%2520four%2520categories%2520with%2520an%2520overall%2520improvement%2520on%2520RAG%2520%2528%252B7.29%2525%2529%2520and%2520Zep%2520%2528%252B1.94%2525%2529%2520using%2520GPT-4.1-mini%2520backend%252C%2520offering%2520a%2520plug-and-play%2520controller%2520for%2520existing%2520memory%2520stores.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MemR%24%5E3%24%3A%20Memory%20Retrieval%20via%20Reflective%20Reasoning%20for%20LLM%20Agents&entry.906535625=Xingbo%20Du%20and%20Loka%20Li%20and%20Duzhen%20Zhang%20and%20Le%20Song&entry.1292438233=Memory%20systems%20have%20been%20designed%20to%20leverage%20past%20experiences%20in%20Large%20Language%20Model%20%28LLM%29%20agents.%20However%2C%20many%20deployed%20memory%20systems%20primarily%20optimize%20compression%20and%20storage%2C%20with%20comparatively%20less%20emphasis%20on%20explicit%2C%20closed-loop%20control%20of%20memory%20retrieval.%20From%20this%20observation%2C%20we%20build%20memory%20retrieval%20as%20an%20autonomous%2C%20accurate%2C%20and%20compatible%20agent%20system%2C%20named%20MemR%24%5E3%24%2C%20which%20has%20two%20core%20mechanisms%3A%201%29%20a%20router%20that%20selects%20among%20retrieve%2C%20reflect%2C%20and%20answer%20actions%20to%20optimize%20answer%20quality%3B%202%29%20a%20global%20evidence-gap%20tracker%20that%20explicitly%20renders%20the%20answering%20process%20transparent%20and%20tracks%20the%20evidence%20collection%20process.%20This%20design%20departs%20from%20the%20standard%20retrieve-then-answer%20pipeline%20by%20introducing%20a%20closed-loop%20control%20mechanism%20that%20enables%20autonomous%20decision-making.%20Empirical%20results%20on%20the%20LoCoMo%20benchmark%20demonstrate%20that%20MemR%24%5E3%24%20surpasses%20strong%20baselines%20on%20LLM-as-a-Judge%20score%2C%20and%20particularly%2C%20it%20improves%20existing%20retrievers%20across%20four%20categories%20with%20an%20overall%20improvement%20on%20RAG%20%28%2B7.29%25%29%20and%20Zep%20%28%2B1.94%25%29%20using%20GPT-4.1-mini%20backend%2C%20offering%20a%20plug-and-play%20controller%20for%20existing%20memory%20stores.&entry.1838667208=http%3A//arxiv.org/abs/2512.20237v1&entry.124074799=Read"},
{"title": "A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice", "author": "Yaowei Bai and Ruiheng Zhang and Yu Lei and Xuhua Duan and Jingfeng Yao and Shuguang Ju and Chaoyang Wang and Wei Yao and Yiwan Guo and Guilin Zhang and Chao Wan and Qian Yuan and Lei Chen and Wenjuan Tang and Biqiang Zhu and Xinggang Wang and Tao Sun and Wei Zhou and Dacheng Tao and Yongchao Xu and Chuansheng Zheng and Huangxuan Zhao and Bo Du", "abstract": "A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.", "link": "http://arxiv.org/abs/2512.20344v1", "date": "2025-12-23", "relevancy": 1.907, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4808}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20DeepSeek-Powered%20AI%20System%20for%20Automated%20Chest%20Radiograph%20Interpretation%20in%20Clinical%20Practice&body=Title%3A%20A%20DeepSeek-Powered%20AI%20System%20for%20Automated%20Chest%20Radiograph%20Interpretation%20in%20Clinical%20Practice%0AAuthor%3A%20Yaowei%20Bai%20and%20Ruiheng%20Zhang%20and%20Yu%20Lei%20and%20Xuhua%20Duan%20and%20Jingfeng%20Yao%20and%20Shuguang%20Ju%20and%20Chaoyang%20Wang%20and%20Wei%20Yao%20and%20Yiwan%20Guo%20and%20Guilin%20Zhang%20and%20Chao%20Wan%20and%20Qian%20Yuan%20and%20Lei%20Chen%20and%20Wenjuan%20Tang%20and%20Biqiang%20Zhu%20and%20Xinggang%20Wang%20and%20Tao%20Sun%20and%20Wei%20Zhou%20and%20Dacheng%20Tao%20and%20Yongchao%20Xu%20and%20Chuansheng%20Zheng%20and%20Huangxuan%20Zhao%20and%20Bo%20Du%0AAbstract%3A%20A%20global%20shortage%20of%20radiologists%20has%20been%20exacerbated%20by%20the%20significant%20volume%20of%20chest%20X-ray%20workloads%2C%20particularly%20in%20primary%20care.%20Although%20multimodal%20large%20language%20models%20show%20promise%2C%20existing%20evaluations%20predominantly%20rely%20on%20automated%20metrics%20or%20retrospective%20analyses%2C%20lacking%20rigorous%20prospective%20clinical%20validation.%20Janus-Pro-CXR%20%281B%29%2C%20a%20chest%20X-ray%20interpretation%20system%20based%20on%20DeepSeek%20Janus-Pro%20model%2C%20was%20developed%20and%20rigorously%20validated%20through%20a%20multicenter%20prospective%20trial%20%28NCT07117266%29.%20Our%20system%20outperforms%20state-of-the-art%20X-ray%20report%20generation%20models%20in%20automated%20report%20generation%2C%20surpassing%20even%20larger-scale%20models%20including%20ChatGPT%204o%20%28200B%20parameters%29%2C%20while%20demonstrating%20reliable%20detection%20of%20six%20clinically%20critical%20radiographic%20findings.%20Retrospective%20evaluation%20confirms%20significantly%20higher%20report%20accuracy%20than%20Janus-Pro%20and%20ChatGPT%204o.%20In%20prospective%20clinical%20deployment%2C%20AI%20assistance%20significantly%20improved%20report%20quality%20scores%2C%20reduced%20interpretation%20time%20by%2018.3%25%20%28P%20%3C%200.001%29%2C%20and%20was%20preferred%20by%20a%20majority%20of%20experts%20in%2054.3%25%20of%20cases.%20Through%20lightweight%20architecture%20and%20domain-specific%20optimization%2C%20Janus-Pro-CXR%20improves%20diagnostic%20reliability%20and%20workflow%20efficiency%2C%20particularly%20in%20resource-constrained%20settings.%20The%20model%20architecture%20and%20implementation%20framework%20will%20be%20open-sourced%20to%20facilitate%20the%20clinical%20translation%20of%20AI-assisted%20radiology%20solutions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520DeepSeek-Powered%2520AI%2520System%2520for%2520Automated%2520Chest%2520Radiograph%2520Interpretation%2520in%2520Clinical%2520Practice%26entry.906535625%3DYaowei%2520Bai%2520and%2520Ruiheng%2520Zhang%2520and%2520Yu%2520Lei%2520and%2520Xuhua%2520Duan%2520and%2520Jingfeng%2520Yao%2520and%2520Shuguang%2520Ju%2520and%2520Chaoyang%2520Wang%2520and%2520Wei%2520Yao%2520and%2520Yiwan%2520Guo%2520and%2520Guilin%2520Zhang%2520and%2520Chao%2520Wan%2520and%2520Qian%2520Yuan%2520and%2520Lei%2520Chen%2520and%2520Wenjuan%2520Tang%2520and%2520Biqiang%2520Zhu%2520and%2520Xinggang%2520Wang%2520and%2520Tao%2520Sun%2520and%2520Wei%2520Zhou%2520and%2520Dacheng%2520Tao%2520and%2520Yongchao%2520Xu%2520and%2520Chuansheng%2520Zheng%2520and%2520Huangxuan%2520Zhao%2520and%2520Bo%2520Du%26entry.1292438233%3DA%2520global%2520shortage%2520of%2520radiologists%2520has%2520been%2520exacerbated%2520by%2520the%2520significant%2520volume%2520of%2520chest%2520X-ray%2520workloads%252C%2520particularly%2520in%2520primary%2520care.%2520Although%2520multimodal%2520large%2520language%2520models%2520show%2520promise%252C%2520existing%2520evaluations%2520predominantly%2520rely%2520on%2520automated%2520metrics%2520or%2520retrospective%2520analyses%252C%2520lacking%2520rigorous%2520prospective%2520clinical%2520validation.%2520Janus-Pro-CXR%2520%25281B%2529%252C%2520a%2520chest%2520X-ray%2520interpretation%2520system%2520based%2520on%2520DeepSeek%2520Janus-Pro%2520model%252C%2520was%2520developed%2520and%2520rigorously%2520validated%2520through%2520a%2520multicenter%2520prospective%2520trial%2520%2528NCT07117266%2529.%2520Our%2520system%2520outperforms%2520state-of-the-art%2520X-ray%2520report%2520generation%2520models%2520in%2520automated%2520report%2520generation%252C%2520surpassing%2520even%2520larger-scale%2520models%2520including%2520ChatGPT%25204o%2520%2528200B%2520parameters%2529%252C%2520while%2520demonstrating%2520reliable%2520detection%2520of%2520six%2520clinically%2520critical%2520radiographic%2520findings.%2520Retrospective%2520evaluation%2520confirms%2520significantly%2520higher%2520report%2520accuracy%2520than%2520Janus-Pro%2520and%2520ChatGPT%25204o.%2520In%2520prospective%2520clinical%2520deployment%252C%2520AI%2520assistance%2520significantly%2520improved%2520report%2520quality%2520scores%252C%2520reduced%2520interpretation%2520time%2520by%252018.3%2525%2520%2528P%2520%253C%25200.001%2529%252C%2520and%2520was%2520preferred%2520by%2520a%2520majority%2520of%2520experts%2520in%252054.3%2525%2520of%2520cases.%2520Through%2520lightweight%2520architecture%2520and%2520domain-specific%2520optimization%252C%2520Janus-Pro-CXR%2520improves%2520diagnostic%2520reliability%2520and%2520workflow%2520efficiency%252C%2520particularly%2520in%2520resource-constrained%2520settings.%2520The%2520model%2520architecture%2520and%2520implementation%2520framework%2520will%2520be%2520open-sourced%2520to%2520facilitate%2520the%2520clinical%2520translation%2520of%2520AI-assisted%2520radiology%2520solutions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20DeepSeek-Powered%20AI%20System%20for%20Automated%20Chest%20Radiograph%20Interpretation%20in%20Clinical%20Practice&entry.906535625=Yaowei%20Bai%20and%20Ruiheng%20Zhang%20and%20Yu%20Lei%20and%20Xuhua%20Duan%20and%20Jingfeng%20Yao%20and%20Shuguang%20Ju%20and%20Chaoyang%20Wang%20and%20Wei%20Yao%20and%20Yiwan%20Guo%20and%20Guilin%20Zhang%20and%20Chao%20Wan%20and%20Qian%20Yuan%20and%20Lei%20Chen%20and%20Wenjuan%20Tang%20and%20Biqiang%20Zhu%20and%20Xinggang%20Wang%20and%20Tao%20Sun%20and%20Wei%20Zhou%20and%20Dacheng%20Tao%20and%20Yongchao%20Xu%20and%20Chuansheng%20Zheng%20and%20Huangxuan%20Zhao%20and%20Bo%20Du&entry.1292438233=A%20global%20shortage%20of%20radiologists%20has%20been%20exacerbated%20by%20the%20significant%20volume%20of%20chest%20X-ray%20workloads%2C%20particularly%20in%20primary%20care.%20Although%20multimodal%20large%20language%20models%20show%20promise%2C%20existing%20evaluations%20predominantly%20rely%20on%20automated%20metrics%20or%20retrospective%20analyses%2C%20lacking%20rigorous%20prospective%20clinical%20validation.%20Janus-Pro-CXR%20%281B%29%2C%20a%20chest%20X-ray%20interpretation%20system%20based%20on%20DeepSeek%20Janus-Pro%20model%2C%20was%20developed%20and%20rigorously%20validated%20through%20a%20multicenter%20prospective%20trial%20%28NCT07117266%29.%20Our%20system%20outperforms%20state-of-the-art%20X-ray%20report%20generation%20models%20in%20automated%20report%20generation%2C%20surpassing%20even%20larger-scale%20models%20including%20ChatGPT%204o%20%28200B%20parameters%29%2C%20while%20demonstrating%20reliable%20detection%20of%20six%20clinically%20critical%20radiographic%20findings.%20Retrospective%20evaluation%20confirms%20significantly%20higher%20report%20accuracy%20than%20Janus-Pro%20and%20ChatGPT%204o.%20In%20prospective%20clinical%20deployment%2C%20AI%20assistance%20significantly%20improved%20report%20quality%20scores%2C%20reduced%20interpretation%20time%20by%2018.3%25%20%28P%20%3C%200.001%29%2C%20and%20was%20preferred%20by%20a%20majority%20of%20experts%20in%2054.3%25%20of%20cases.%20Through%20lightweight%20architecture%20and%20domain-specific%20optimization%2C%20Janus-Pro-CXR%20improves%20diagnostic%20reliability%20and%20workflow%20efficiency%2C%20particularly%20in%20resource-constrained%20settings.%20The%20model%20architecture%20and%20implementation%20framework%20will%20be%20open-sourced%20to%20facilitate%20the%20clinical%20translation%20of%20AI-assisted%20radiology%20solutions.&entry.1838667208=http%3A//arxiv.org/abs/2512.20344v1&entry.124074799=Read"},
{"title": "Stochastic activations", "author": "Maria Lomeli and Matthijs Douze and Gergely Szilvasy and Loic Cabannes and Jade Copet and Sainbayar Sukhbaatar and Jason Weston and Gabriel Synnaeve and Pierre-Emmanuel Mazar\u00e9 and Herv\u00e9 J\u00e9gou", "abstract": "We introduce stochastic activations. This novel strategy randomly selects between several non-linear functions in the feed-forward layer of a large language model. In particular, we choose between SILU or RELU depending on a Bernoulli draw. This strategy circumvents the optimization problem associated with RELU, namely, the constant shape for negative inputs that prevents the gradient flow. We leverage this strategy in two ways:\n  (1) We use stochastic activations during pre-training and fine-tune the model with RELU, which is used at inference time to provide sparse latent vectors. This reduces the inference FLOPs and translates into a significant speedup in the CPU. Interestingly, this leads to much better results than training from scratch with the RELU activation function.\n  (2) We evaluate stochastic activations for generation. This strategy performs reasonably well: it is only slightly inferior to the best deterministic non-linearity, namely SILU combined with temperature scaling. This offers an alternative to existing strategies by providing a controlled way to increase the diversity of the generated text.", "link": "http://arxiv.org/abs/2509.22358v2", "date": "2025-12-23", "relevancy": 1.9025, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5038}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4711}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20activations&body=Title%3A%20Stochastic%20activations%0AAuthor%3A%20Maria%20Lomeli%20and%20Matthijs%20Douze%20and%20Gergely%20Szilvasy%20and%20Loic%20Cabannes%20and%20Jade%20Copet%20and%20Sainbayar%20Sukhbaatar%20and%20Jason%20Weston%20and%20Gabriel%20Synnaeve%20and%20Pierre-Emmanuel%20Mazar%C3%A9%20and%20Herv%C3%A9%20J%C3%A9gou%0AAbstract%3A%20We%20introduce%20stochastic%20activations.%20This%20novel%20strategy%20randomly%20selects%20between%20several%20non-linear%20functions%20in%20the%20feed-forward%20layer%20of%20a%20large%20language%20model.%20In%20particular%2C%20we%20choose%20between%20SILU%20or%20RELU%20depending%20on%20a%20Bernoulli%20draw.%20This%20strategy%20circumvents%20the%20optimization%20problem%20associated%20with%20RELU%2C%20namely%2C%20the%20constant%20shape%20for%20negative%20inputs%20that%20prevents%20the%20gradient%20flow.%20We%20leverage%20this%20strategy%20in%20two%20ways%3A%0A%20%20%281%29%20We%20use%20stochastic%20activations%20during%20pre-training%20and%20fine-tune%20the%20model%20with%20RELU%2C%20which%20is%20used%20at%20inference%20time%20to%20provide%20sparse%20latent%20vectors.%20This%20reduces%20the%20inference%20FLOPs%20and%20translates%20into%20a%20significant%20speedup%20in%20the%20CPU.%20Interestingly%2C%20this%20leads%20to%20much%20better%20results%20than%20training%20from%20scratch%20with%20the%20RELU%20activation%20function.%0A%20%20%282%29%20We%20evaluate%20stochastic%20activations%20for%20generation.%20This%20strategy%20performs%20reasonably%20well%3A%20it%20is%20only%20slightly%20inferior%20to%20the%20best%20deterministic%20non-linearity%2C%20namely%20SILU%20combined%20with%20temperature%20scaling.%20This%20offers%20an%20alternative%20to%20existing%20strategies%20by%20providing%20a%20controlled%20way%20to%20increase%20the%20diversity%20of%20the%20generated%20text.%0ALink%3A%20http%3A//arxiv.org/abs/2509.22358v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520activations%26entry.906535625%3DMaria%2520Lomeli%2520and%2520Matthijs%2520Douze%2520and%2520Gergely%2520Szilvasy%2520and%2520Loic%2520Cabannes%2520and%2520Jade%2520Copet%2520and%2520Sainbayar%2520Sukhbaatar%2520and%2520Jason%2520Weston%2520and%2520Gabriel%2520Synnaeve%2520and%2520Pierre-Emmanuel%2520Mazar%25C3%25A9%2520and%2520Herv%25C3%25A9%2520J%25C3%25A9gou%26entry.1292438233%3DWe%2520introduce%2520stochastic%2520activations.%2520This%2520novel%2520strategy%2520randomly%2520selects%2520between%2520several%2520non-linear%2520functions%2520in%2520the%2520feed-forward%2520layer%2520of%2520a%2520large%2520language%2520model.%2520In%2520particular%252C%2520we%2520choose%2520between%2520SILU%2520or%2520RELU%2520depending%2520on%2520a%2520Bernoulli%2520draw.%2520This%2520strategy%2520circumvents%2520the%2520optimization%2520problem%2520associated%2520with%2520RELU%252C%2520namely%252C%2520the%2520constant%2520shape%2520for%2520negative%2520inputs%2520that%2520prevents%2520the%2520gradient%2520flow.%2520We%2520leverage%2520this%2520strategy%2520in%2520two%2520ways%253A%250A%2520%2520%25281%2529%2520We%2520use%2520stochastic%2520activations%2520during%2520pre-training%2520and%2520fine-tune%2520the%2520model%2520with%2520RELU%252C%2520which%2520is%2520used%2520at%2520inference%2520time%2520to%2520provide%2520sparse%2520latent%2520vectors.%2520This%2520reduces%2520the%2520inference%2520FLOPs%2520and%2520translates%2520into%2520a%2520significant%2520speedup%2520in%2520the%2520CPU.%2520Interestingly%252C%2520this%2520leads%2520to%2520much%2520better%2520results%2520than%2520training%2520from%2520scratch%2520with%2520the%2520RELU%2520activation%2520function.%250A%2520%2520%25282%2529%2520We%2520evaluate%2520stochastic%2520activations%2520for%2520generation.%2520This%2520strategy%2520performs%2520reasonably%2520well%253A%2520it%2520is%2520only%2520slightly%2520inferior%2520to%2520the%2520best%2520deterministic%2520non-linearity%252C%2520namely%2520SILU%2520combined%2520with%2520temperature%2520scaling.%2520This%2520offers%2520an%2520alternative%2520to%2520existing%2520strategies%2520by%2520providing%2520a%2520controlled%2520way%2520to%2520increase%2520the%2520diversity%2520of%2520the%2520generated%2520text.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22358v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20activations&entry.906535625=Maria%20Lomeli%20and%20Matthijs%20Douze%20and%20Gergely%20Szilvasy%20and%20Loic%20Cabannes%20and%20Jade%20Copet%20and%20Sainbayar%20Sukhbaatar%20and%20Jason%20Weston%20and%20Gabriel%20Synnaeve%20and%20Pierre-Emmanuel%20Mazar%C3%A9%20and%20Herv%C3%A9%20J%C3%A9gou&entry.1292438233=We%20introduce%20stochastic%20activations.%20This%20novel%20strategy%20randomly%20selects%20between%20several%20non-linear%20functions%20in%20the%20feed-forward%20layer%20of%20a%20large%20language%20model.%20In%20particular%2C%20we%20choose%20between%20SILU%20or%20RELU%20depending%20on%20a%20Bernoulli%20draw.%20This%20strategy%20circumvents%20the%20optimization%20problem%20associated%20with%20RELU%2C%20namely%2C%20the%20constant%20shape%20for%20negative%20inputs%20that%20prevents%20the%20gradient%20flow.%20We%20leverage%20this%20strategy%20in%20two%20ways%3A%0A%20%20%281%29%20We%20use%20stochastic%20activations%20during%20pre-training%20and%20fine-tune%20the%20model%20with%20RELU%2C%20which%20is%20used%20at%20inference%20time%20to%20provide%20sparse%20latent%20vectors.%20This%20reduces%20the%20inference%20FLOPs%20and%20translates%20into%20a%20significant%20speedup%20in%20the%20CPU.%20Interestingly%2C%20this%20leads%20to%20much%20better%20results%20than%20training%20from%20scratch%20with%20the%20RELU%20activation%20function.%0A%20%20%282%29%20We%20evaluate%20stochastic%20activations%20for%20generation.%20This%20strategy%20performs%20reasonably%20well%3A%20it%20is%20only%20slightly%20inferior%20to%20the%20best%20deterministic%20non-linearity%2C%20namely%20SILU%20combined%20with%20temperature%20scaling.%20This%20offers%20an%20alternative%20to%20existing%20strategies%20by%20providing%20a%20controlled%20way%20to%20increase%20the%20diversity%20of%20the%20generated%20text.&entry.1838667208=http%3A//arxiv.org/abs/2509.22358v2&entry.124074799=Read"},
{"title": "Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud", "author": "Jixiao Yang and Jinyu Chen and Zixiao Huang and Chengda Xu and Chi Zhang and Sijia Li", "abstract": "Federated learning across multi-cloud environments faces critical challenges, including non-IID data distributions, malicious participant detection, and substantial cross-cloud communication costs (egress fees). Existing Byzantine-robust methods focus primarily on model accuracy while overlooking the economic implications of data transfer across cloud providers. This paper presents Cost-TrustFL, a hierarchical federated learning framework that jointly optimizes model performance and communication costs while providing robust defense against poisoning attacks. We propose a gradient-based approximate Shapley value computation method that reduces the complexity from exponential to linear, enabling lightweight reputation evaluation. Our cost-aware aggregation strategy prioritizes intra-cloud communication to minimize expensive cross-cloud data transfers. Experiments on CIFAR-10 and FEMNIST datasets demonstrate that Cost-TrustFL achieves 86.7% accuracy under 30% malicious clients while reducing communication costs by 32% compared to baseline methods. The framework maintains stable performance across varying non-IID degrees and attack intensities, making it practical for real-world multi-cloud deployments.", "link": "http://arxiv.org/abs/2512.20218v1", "date": "2025-12-23", "relevancy": 1.8986, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4876}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4733}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cost-TrustFL%3A%20Cost-Aware%20Hierarchical%20Federated%20Learning%20with%20Lightweight%20Reputation%20Evaluation%20across%20Multi-Cloud&body=Title%3A%20Cost-TrustFL%3A%20Cost-Aware%20Hierarchical%20Federated%20Learning%20with%20Lightweight%20Reputation%20Evaluation%20across%20Multi-Cloud%0AAuthor%3A%20Jixiao%20Yang%20and%20Jinyu%20Chen%20and%20Zixiao%20Huang%20and%20Chengda%20Xu%20and%20Chi%20Zhang%20and%20Sijia%20Li%0AAbstract%3A%20Federated%20learning%20across%20multi-cloud%20environments%20faces%20critical%20challenges%2C%20including%20non-IID%20data%20distributions%2C%20malicious%20participant%20detection%2C%20and%20substantial%20cross-cloud%20communication%20costs%20%28egress%20fees%29.%20Existing%20Byzantine-robust%20methods%20focus%20primarily%20on%20model%20accuracy%20while%20overlooking%20the%20economic%20implications%20of%20data%20transfer%20across%20cloud%20providers.%20This%20paper%20presents%20Cost-TrustFL%2C%20a%20hierarchical%20federated%20learning%20framework%20that%20jointly%20optimizes%20model%20performance%20and%20communication%20costs%20while%20providing%20robust%20defense%20against%20poisoning%20attacks.%20We%20propose%20a%20gradient-based%20approximate%20Shapley%20value%20computation%20method%20that%20reduces%20the%20complexity%20from%20exponential%20to%20linear%2C%20enabling%20lightweight%20reputation%20evaluation.%20Our%20cost-aware%20aggregation%20strategy%20prioritizes%20intra-cloud%20communication%20to%20minimize%20expensive%20cross-cloud%20data%20transfers.%20Experiments%20on%20CIFAR-10%20and%20FEMNIST%20datasets%20demonstrate%20that%20Cost-TrustFL%20achieves%2086.7%25%20accuracy%20under%2030%25%20malicious%20clients%20while%20reducing%20communication%20costs%20by%2032%25%20compared%20to%20baseline%20methods.%20The%20framework%20maintains%20stable%20performance%20across%20varying%20non-IID%20degrees%20and%20attack%20intensities%2C%20making%20it%20practical%20for%20real-world%20multi-cloud%20deployments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCost-TrustFL%253A%2520Cost-Aware%2520Hierarchical%2520Federated%2520Learning%2520with%2520Lightweight%2520Reputation%2520Evaluation%2520across%2520Multi-Cloud%26entry.906535625%3DJixiao%2520Yang%2520and%2520Jinyu%2520Chen%2520and%2520Zixiao%2520Huang%2520and%2520Chengda%2520Xu%2520and%2520Chi%2520Zhang%2520and%2520Sijia%2520Li%26entry.1292438233%3DFederated%2520learning%2520across%2520multi-cloud%2520environments%2520faces%2520critical%2520challenges%252C%2520including%2520non-IID%2520data%2520distributions%252C%2520malicious%2520participant%2520detection%252C%2520and%2520substantial%2520cross-cloud%2520communication%2520costs%2520%2528egress%2520fees%2529.%2520Existing%2520Byzantine-robust%2520methods%2520focus%2520primarily%2520on%2520model%2520accuracy%2520while%2520overlooking%2520the%2520economic%2520implications%2520of%2520data%2520transfer%2520across%2520cloud%2520providers.%2520This%2520paper%2520presents%2520Cost-TrustFL%252C%2520a%2520hierarchical%2520federated%2520learning%2520framework%2520that%2520jointly%2520optimizes%2520model%2520performance%2520and%2520communication%2520costs%2520while%2520providing%2520robust%2520defense%2520against%2520poisoning%2520attacks.%2520We%2520propose%2520a%2520gradient-based%2520approximate%2520Shapley%2520value%2520computation%2520method%2520that%2520reduces%2520the%2520complexity%2520from%2520exponential%2520to%2520linear%252C%2520enabling%2520lightweight%2520reputation%2520evaluation.%2520Our%2520cost-aware%2520aggregation%2520strategy%2520prioritizes%2520intra-cloud%2520communication%2520to%2520minimize%2520expensive%2520cross-cloud%2520data%2520transfers.%2520Experiments%2520on%2520CIFAR-10%2520and%2520FEMNIST%2520datasets%2520demonstrate%2520that%2520Cost-TrustFL%2520achieves%252086.7%2525%2520accuracy%2520under%252030%2525%2520malicious%2520clients%2520while%2520reducing%2520communication%2520costs%2520by%252032%2525%2520compared%2520to%2520baseline%2520methods.%2520The%2520framework%2520maintains%2520stable%2520performance%2520across%2520varying%2520non-IID%2520degrees%2520and%2520attack%2520intensities%252C%2520making%2520it%2520practical%2520for%2520real-world%2520multi-cloud%2520deployments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cost-TrustFL%3A%20Cost-Aware%20Hierarchical%20Federated%20Learning%20with%20Lightweight%20Reputation%20Evaluation%20across%20Multi-Cloud&entry.906535625=Jixiao%20Yang%20and%20Jinyu%20Chen%20and%20Zixiao%20Huang%20and%20Chengda%20Xu%20and%20Chi%20Zhang%20and%20Sijia%20Li&entry.1292438233=Federated%20learning%20across%20multi-cloud%20environments%20faces%20critical%20challenges%2C%20including%20non-IID%20data%20distributions%2C%20malicious%20participant%20detection%2C%20and%20substantial%20cross-cloud%20communication%20costs%20%28egress%20fees%29.%20Existing%20Byzantine-robust%20methods%20focus%20primarily%20on%20model%20accuracy%20while%20overlooking%20the%20economic%20implications%20of%20data%20transfer%20across%20cloud%20providers.%20This%20paper%20presents%20Cost-TrustFL%2C%20a%20hierarchical%20federated%20learning%20framework%20that%20jointly%20optimizes%20model%20performance%20and%20communication%20costs%20while%20providing%20robust%20defense%20against%20poisoning%20attacks.%20We%20propose%20a%20gradient-based%20approximate%20Shapley%20value%20computation%20method%20that%20reduces%20the%20complexity%20from%20exponential%20to%20linear%2C%20enabling%20lightweight%20reputation%20evaluation.%20Our%20cost-aware%20aggregation%20strategy%20prioritizes%20intra-cloud%20communication%20to%20minimize%20expensive%20cross-cloud%20data%20transfers.%20Experiments%20on%20CIFAR-10%20and%20FEMNIST%20datasets%20demonstrate%20that%20Cost-TrustFL%20achieves%2086.7%25%20accuracy%20under%2030%25%20malicious%20clients%20while%20reducing%20communication%20costs%20by%2032%25%20compared%20to%20baseline%20methods.%20The%20framework%20maintains%20stable%20performance%20across%20varying%20non-IID%20degrees%20and%20attack%20intensities%2C%20making%20it%20practical%20for%20real-world%20multi-cloud%20deployments.&entry.1838667208=http%3A//arxiv.org/abs/2512.20218v1&entry.124074799=Read"},
{"title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation", "author": "Nilesh Jain and Seyi Adeyinka and Leor Roseman and Aza Allsop", "abstract": "Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($\u03ba$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($\u03ba= 0.907$, cosine=95.3%), followed by GPT-4o ($\u03ba= 0.853$, cosine=92.6%) and Claude ($\u03ba= 0.842$, cosine=92.1%). All three models achieve a high agreement ($\u03ba> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.", "link": "http://arxiv.org/abs/2512.20352v1", "date": "2025-12-23", "relevancy": 1.8963, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4886}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4641}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-LLM%20Thematic%20Analysis%20with%20Dual%20Reliability%20Metrics%3A%20Combining%20Cohen%27s%20Kappa%20and%20Semantic%20Similarity%20for%20Qualitative%20Research%20Validation&body=Title%3A%20Multi-LLM%20Thematic%20Analysis%20with%20Dual%20Reliability%20Metrics%3A%20Combining%20Cohen%27s%20Kappa%20and%20Semantic%20Similarity%20for%20Qualitative%20Research%20Validation%0AAuthor%3A%20Nilesh%20Jain%20and%20Seyi%20Adeyinka%20and%20Leor%20Roseman%20and%20Aza%20Allsop%0AAbstract%3A%20Qualitative%20research%20faces%20a%20critical%20reliability%20challenge%3A%20traditional%20inter-rater%20agreement%20methods%20require%20multiple%20human%20coders%2C%20are%20time-intensive%2C%20and%20often%20yield%20moderate%20consistency.%20We%20present%20a%20multi-perspective%20validation%20framework%20for%20LLM-based%20thematic%20analysis%20that%20combines%20ensemble%20validation%20with%20dual%20reliability%20metrics%3A%20Cohen%27s%20Kappa%20%28%24%CE%BA%24%29%20for%20inter-rater%20agreement%20and%20cosine%20similarity%20for%20semantic%20consistency.%20Our%20framework%20enables%20configurable%20analysis%20parameters%20%281-6%20seeds%2C%20temperature%200.0-2.0%29%2C%20supports%20custom%20prompt%20structures%20with%20variable%20substitution%2C%20and%20provides%20consensus%20theme%20extraction%20across%20any%20JSON%20format.%20As%20proof-of-concept%2C%20we%20evaluate%20three%20leading%20LLMs%20%28Gemini%202.5%20Pro%2C%20GPT-4o%2C%20Claude%203.5%20Sonnet%29%20on%20a%20psychedelic%20art%20therapy%20interview%20transcript%2C%20conducting%20six%20independent%20runs%20per%20model.%20Results%20demonstrate%20Gemini%20achieves%20highest%20reliability%20%28%24%CE%BA%3D%200.907%24%2C%20cosine%3D95.3%25%29%2C%20followed%20by%20GPT-4o%20%28%24%CE%BA%3D%200.853%24%2C%20cosine%3D92.6%25%29%20and%20Claude%20%28%24%CE%BA%3D%200.842%24%2C%20cosine%3D92.1%25%29.%20All%20three%20models%20achieve%20a%20high%20agreement%20%28%24%CE%BA%3E%200.80%24%29%2C%20validating%20the%20multi-run%20ensemble%20approach.%20The%20framework%20successfully%20extracts%20consensus%20themes%20across%20runs%2C%20with%20Gemini%20identifying%206%20consensus%20themes%20%2850-83%25%20consistency%29%2C%20GPT-4o%20identifying%205%20themes%2C%20and%20Claude%204%20themes.%20Our%20open-source%20implementation%20provides%20researchers%20with%20transparent%20reliability%20metrics%2C%20flexible%20configuration%2C%20and%20structure-agnostic%20consensus%20extraction%2C%20establishing%20methodological%20foundations%20for%20reliable%20AI-assisted%20qualitative%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-LLM%2520Thematic%2520Analysis%2520with%2520Dual%2520Reliability%2520Metrics%253A%2520Combining%2520Cohen%2527s%2520Kappa%2520and%2520Semantic%2520Similarity%2520for%2520Qualitative%2520Research%2520Validation%26entry.906535625%3DNilesh%2520Jain%2520and%2520Seyi%2520Adeyinka%2520and%2520Leor%2520Roseman%2520and%2520Aza%2520Allsop%26entry.1292438233%3DQualitative%2520research%2520faces%2520a%2520critical%2520reliability%2520challenge%253A%2520traditional%2520inter-rater%2520agreement%2520methods%2520require%2520multiple%2520human%2520coders%252C%2520are%2520time-intensive%252C%2520and%2520often%2520yield%2520moderate%2520consistency.%2520We%2520present%2520a%2520multi-perspective%2520validation%2520framework%2520for%2520LLM-based%2520thematic%2520analysis%2520that%2520combines%2520ensemble%2520validation%2520with%2520dual%2520reliability%2520metrics%253A%2520Cohen%2527s%2520Kappa%2520%2528%2524%25CE%25BA%2524%2529%2520for%2520inter-rater%2520agreement%2520and%2520cosine%2520similarity%2520for%2520semantic%2520consistency.%2520Our%2520framework%2520enables%2520configurable%2520analysis%2520parameters%2520%25281-6%2520seeds%252C%2520temperature%25200.0-2.0%2529%252C%2520supports%2520custom%2520prompt%2520structures%2520with%2520variable%2520substitution%252C%2520and%2520provides%2520consensus%2520theme%2520extraction%2520across%2520any%2520JSON%2520format.%2520As%2520proof-of-concept%252C%2520we%2520evaluate%2520three%2520leading%2520LLMs%2520%2528Gemini%25202.5%2520Pro%252C%2520GPT-4o%252C%2520Claude%25203.5%2520Sonnet%2529%2520on%2520a%2520psychedelic%2520art%2520therapy%2520interview%2520transcript%252C%2520conducting%2520six%2520independent%2520runs%2520per%2520model.%2520Results%2520demonstrate%2520Gemini%2520achieves%2520highest%2520reliability%2520%2528%2524%25CE%25BA%253D%25200.907%2524%252C%2520cosine%253D95.3%2525%2529%252C%2520followed%2520by%2520GPT-4o%2520%2528%2524%25CE%25BA%253D%25200.853%2524%252C%2520cosine%253D92.6%2525%2529%2520and%2520Claude%2520%2528%2524%25CE%25BA%253D%25200.842%2524%252C%2520cosine%253D92.1%2525%2529.%2520All%2520three%2520models%2520achieve%2520a%2520high%2520agreement%2520%2528%2524%25CE%25BA%253E%25200.80%2524%2529%252C%2520validating%2520the%2520multi-run%2520ensemble%2520approach.%2520The%2520framework%2520successfully%2520extracts%2520consensus%2520themes%2520across%2520runs%252C%2520with%2520Gemini%2520identifying%25206%2520consensus%2520themes%2520%252850-83%2525%2520consistency%2529%252C%2520GPT-4o%2520identifying%25205%2520themes%252C%2520and%2520Claude%25204%2520themes.%2520Our%2520open-source%2520implementation%2520provides%2520researchers%2520with%2520transparent%2520reliability%2520metrics%252C%2520flexible%2520configuration%252C%2520and%2520structure-agnostic%2520consensus%2520extraction%252C%2520establishing%2520methodological%2520foundations%2520for%2520reliable%2520AI-assisted%2520qualitative%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-LLM%20Thematic%20Analysis%20with%20Dual%20Reliability%20Metrics%3A%20Combining%20Cohen%27s%20Kappa%20and%20Semantic%20Similarity%20for%20Qualitative%20Research%20Validation&entry.906535625=Nilesh%20Jain%20and%20Seyi%20Adeyinka%20and%20Leor%20Roseman%20and%20Aza%20Allsop&entry.1292438233=Qualitative%20research%20faces%20a%20critical%20reliability%20challenge%3A%20traditional%20inter-rater%20agreement%20methods%20require%20multiple%20human%20coders%2C%20are%20time-intensive%2C%20and%20often%20yield%20moderate%20consistency.%20We%20present%20a%20multi-perspective%20validation%20framework%20for%20LLM-based%20thematic%20analysis%20that%20combines%20ensemble%20validation%20with%20dual%20reliability%20metrics%3A%20Cohen%27s%20Kappa%20%28%24%CE%BA%24%29%20for%20inter-rater%20agreement%20and%20cosine%20similarity%20for%20semantic%20consistency.%20Our%20framework%20enables%20configurable%20analysis%20parameters%20%281-6%20seeds%2C%20temperature%200.0-2.0%29%2C%20supports%20custom%20prompt%20structures%20with%20variable%20substitution%2C%20and%20provides%20consensus%20theme%20extraction%20across%20any%20JSON%20format.%20As%20proof-of-concept%2C%20we%20evaluate%20three%20leading%20LLMs%20%28Gemini%202.5%20Pro%2C%20GPT-4o%2C%20Claude%203.5%20Sonnet%29%20on%20a%20psychedelic%20art%20therapy%20interview%20transcript%2C%20conducting%20six%20independent%20runs%20per%20model.%20Results%20demonstrate%20Gemini%20achieves%20highest%20reliability%20%28%24%CE%BA%3D%200.907%24%2C%20cosine%3D95.3%25%29%2C%20followed%20by%20GPT-4o%20%28%24%CE%BA%3D%200.853%24%2C%20cosine%3D92.6%25%29%20and%20Claude%20%28%24%CE%BA%3D%200.842%24%2C%20cosine%3D92.1%25%29.%20All%20three%20models%20achieve%20a%20high%20agreement%20%28%24%CE%BA%3E%200.80%24%29%2C%20validating%20the%20multi-run%20ensemble%20approach.%20The%20framework%20successfully%20extracts%20consensus%20themes%20across%20runs%2C%20with%20Gemini%20identifying%206%20consensus%20themes%20%2850-83%25%20consistency%29%2C%20GPT-4o%20identifying%205%20themes%2C%20and%20Claude%204%20themes.%20Our%20open-source%20implementation%20provides%20researchers%20with%20transparent%20reliability%20metrics%2C%20flexible%20configuration%2C%20and%20structure-agnostic%20consensus%20extraction%2C%20establishing%20methodological%20foundations%20for%20reliable%20AI-assisted%20qualitative%20research.&entry.1838667208=http%3A//arxiv.org/abs/2512.20352v1&entry.124074799=Read"},
{"title": "Toward Explaining Large Language Models in Software Engineering Tasks", "author": "Antonio Vitale and Khai-Nguyen Nguyen and Denys Poshyvanyk and Rocco Oliveto and Simone Scalabrino and Antonio Mastropaolo", "abstract": "Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.", "link": "http://arxiv.org/abs/2512.20328v1", "date": "2025-12-23", "relevancy": 1.8923, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4761}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Explaining%20Large%20Language%20Models%20in%20Software%20Engineering%20Tasks&body=Title%3A%20Toward%20Explaining%20Large%20Language%20Models%20in%20Software%20Engineering%20Tasks%0AAuthor%3A%20Antonio%20Vitale%20and%20Khai-Nguyen%20Nguyen%20and%20Denys%20Poshyvanyk%20and%20Rocco%20Oliveto%20and%20Simone%20Scalabrino%20and%20Antonio%20Mastropaolo%0AAbstract%3A%20Recent%20progress%20in%20Large%20Language%20Models%20%28LLMs%29%20has%20substantially%20advanced%20the%20automation%20of%20software%20engineering%20%28SE%29%20tasks%2C%20enabling%20complex%20activities%20such%20as%20code%20generation%20and%20code%20summarization.%20However%2C%20the%20black-box%20nature%20of%20LLMs%20remains%20a%20major%20barrier%20to%20their%20adoption%20in%20high-stakes%20and%20safety-critical%20domains%2C%20where%20explainability%20and%20transparency%20are%20vital%20for%20trust%2C%20accountability%2C%20and%20effective%20human%20supervision.%20Despite%20increasing%20interest%20in%20explainable%20AI%20for%20software%20engineering%2C%20existing%20methods%20lack%20domain-specific%20explanations%20aligned%20with%20how%20practitioners%20reason%20about%20SE%20artifacts.%20To%20address%20this%20gap%2C%20we%20introduce%20FeatureSHAP%2C%20the%20first%20fully%20automated%2C%20model-agnostic%20explainability%20framework%20tailored%20to%20software%20engineering%20tasks.%20Based%20on%20Shapley%20values%2C%20FeatureSHAP%20attributes%20model%20outputs%20to%20high-level%20input%20features%20through%20systematic%20input%20perturbation%20and%20task-specific%20similarity%20comparisons%2C%20while%20remaining%20compatible%20with%20both%20open-source%20and%20proprietary%20LLMs.%20We%20evaluate%20FeatureSHAP%20on%20two%20bi-modal%20SE%20tasks%3A%20code%20generation%20and%20code%20summarization.%20The%20results%20show%20that%20FeatureSHAP%20assigns%20less%20importance%20to%20irrelevant%20input%20features%20and%20produces%20explanations%20with%20higher%20fidelity%20than%20baseline%20methods.%20A%20practitioner%20survey%20involving%2037%20participants%20shows%20that%20FeatureSHAP%20helps%20practitioners%20better%20interpret%20model%20outputs%20and%20make%20more%20informed%20decisions.%20Collectively%2C%20FeatureSHAP%20represents%20a%20meaningful%20step%20toward%20practical%20explainable%20AI%20in%20software%20engineering.%20FeatureSHAP%20is%20available%20at%20https%3A//github.com/deviserlab/FeatureSHAP.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Explaining%2520Large%2520Language%2520Models%2520in%2520Software%2520Engineering%2520Tasks%26entry.906535625%3DAntonio%2520Vitale%2520and%2520Khai-Nguyen%2520Nguyen%2520and%2520Denys%2520Poshyvanyk%2520and%2520Rocco%2520Oliveto%2520and%2520Simone%2520Scalabrino%2520and%2520Antonio%2520Mastropaolo%26entry.1292438233%3DRecent%2520progress%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520substantially%2520advanced%2520the%2520automation%2520of%2520software%2520engineering%2520%2528SE%2529%2520tasks%252C%2520enabling%2520complex%2520activities%2520such%2520as%2520code%2520generation%2520and%2520code%2520summarization.%2520However%252C%2520the%2520black-box%2520nature%2520of%2520LLMs%2520remains%2520a%2520major%2520barrier%2520to%2520their%2520adoption%2520in%2520high-stakes%2520and%2520safety-critical%2520domains%252C%2520where%2520explainability%2520and%2520transparency%2520are%2520vital%2520for%2520trust%252C%2520accountability%252C%2520and%2520effective%2520human%2520supervision.%2520Despite%2520increasing%2520interest%2520in%2520explainable%2520AI%2520for%2520software%2520engineering%252C%2520existing%2520methods%2520lack%2520domain-specific%2520explanations%2520aligned%2520with%2520how%2520practitioners%2520reason%2520about%2520SE%2520artifacts.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520FeatureSHAP%252C%2520the%2520first%2520fully%2520automated%252C%2520model-agnostic%2520explainability%2520framework%2520tailored%2520to%2520software%2520engineering%2520tasks.%2520Based%2520on%2520Shapley%2520values%252C%2520FeatureSHAP%2520attributes%2520model%2520outputs%2520to%2520high-level%2520input%2520features%2520through%2520systematic%2520input%2520perturbation%2520and%2520task-specific%2520similarity%2520comparisons%252C%2520while%2520remaining%2520compatible%2520with%2520both%2520open-source%2520and%2520proprietary%2520LLMs.%2520We%2520evaluate%2520FeatureSHAP%2520on%2520two%2520bi-modal%2520SE%2520tasks%253A%2520code%2520generation%2520and%2520code%2520summarization.%2520The%2520results%2520show%2520that%2520FeatureSHAP%2520assigns%2520less%2520importance%2520to%2520irrelevant%2520input%2520features%2520and%2520produces%2520explanations%2520with%2520higher%2520fidelity%2520than%2520baseline%2520methods.%2520A%2520practitioner%2520survey%2520involving%252037%2520participants%2520shows%2520that%2520FeatureSHAP%2520helps%2520practitioners%2520better%2520interpret%2520model%2520outputs%2520and%2520make%2520more%2520informed%2520decisions.%2520Collectively%252C%2520FeatureSHAP%2520represents%2520a%2520meaningful%2520step%2520toward%2520practical%2520explainable%2520AI%2520in%2520software%2520engineering.%2520FeatureSHAP%2520is%2520available%2520at%2520https%253A//github.com/deviserlab/FeatureSHAP.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Explaining%20Large%20Language%20Models%20in%20Software%20Engineering%20Tasks&entry.906535625=Antonio%20Vitale%20and%20Khai-Nguyen%20Nguyen%20and%20Denys%20Poshyvanyk%20and%20Rocco%20Oliveto%20and%20Simone%20Scalabrino%20and%20Antonio%20Mastropaolo&entry.1292438233=Recent%20progress%20in%20Large%20Language%20Models%20%28LLMs%29%20has%20substantially%20advanced%20the%20automation%20of%20software%20engineering%20%28SE%29%20tasks%2C%20enabling%20complex%20activities%20such%20as%20code%20generation%20and%20code%20summarization.%20However%2C%20the%20black-box%20nature%20of%20LLMs%20remains%20a%20major%20barrier%20to%20their%20adoption%20in%20high-stakes%20and%20safety-critical%20domains%2C%20where%20explainability%20and%20transparency%20are%20vital%20for%20trust%2C%20accountability%2C%20and%20effective%20human%20supervision.%20Despite%20increasing%20interest%20in%20explainable%20AI%20for%20software%20engineering%2C%20existing%20methods%20lack%20domain-specific%20explanations%20aligned%20with%20how%20practitioners%20reason%20about%20SE%20artifacts.%20To%20address%20this%20gap%2C%20we%20introduce%20FeatureSHAP%2C%20the%20first%20fully%20automated%2C%20model-agnostic%20explainability%20framework%20tailored%20to%20software%20engineering%20tasks.%20Based%20on%20Shapley%20values%2C%20FeatureSHAP%20attributes%20model%20outputs%20to%20high-level%20input%20features%20through%20systematic%20input%20perturbation%20and%20task-specific%20similarity%20comparisons%2C%20while%20remaining%20compatible%20with%20both%20open-source%20and%20proprietary%20LLMs.%20We%20evaluate%20FeatureSHAP%20on%20two%20bi-modal%20SE%20tasks%3A%20code%20generation%20and%20code%20summarization.%20The%20results%20show%20that%20FeatureSHAP%20assigns%20less%20importance%20to%20irrelevant%20input%20features%20and%20produces%20explanations%20with%20higher%20fidelity%20than%20baseline%20methods.%20A%20practitioner%20survey%20involving%2037%20participants%20shows%20that%20FeatureSHAP%20helps%20practitioners%20better%20interpret%20model%20outputs%20and%20make%20more%20informed%20decisions.%20Collectively%2C%20FeatureSHAP%20represents%20a%20meaningful%20step%20toward%20practical%20explainable%20AI%20in%20software%20engineering.%20FeatureSHAP%20is%20available%20at%20https%3A//github.com/deviserlab/FeatureSHAP.&entry.1838667208=http%3A//arxiv.org/abs/2512.20328v1&entry.124074799=Read"},
{"title": "FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning", "author": "Mrinmay Sen and Subhrajit Nag", "abstract": "Data heterogeneity is a significant challenge in modern federated learning (FL) as it creates variance in local model updates, causing the aggregated global model to shift away from the true global optimum. Partial client participation in FL further exacerbates this issue by skewing the aggregation of local models towards the data distribution of participating clients. This creates additional variance in the global model updates, causing the global model to converge away from the optima of the global objective. These variances lead to instability in FL training, which degrades global model performance and slows down FL training. While existing literature primarily focuses on addressing data heterogeneity, the impact of partial client participation has received less attention. In this paper, we propose FedDPC, a novel FL method, designed to improve FL training and global model performance by mitigating both data heterogeneity and partial client participation. FedDPC addresses these issues by projecting each local update onto the previous global update, thereby controlling variance in both local and global updates. To further accelerate FL training, FedDPC employs adaptive scaling for each local update before aggregation. Extensive experiments on image classification tasks with multiple heterogeneously partitioned datasets validate the effectiveness of FedDPC. The results demonstrate that FedDPC outperforms state-of-the-art FL algorithms by achieving faster reduction in training loss and improved test accuracy across communication rounds.", "link": "http://arxiv.org/abs/2512.20329v1", "date": "2025-12-23", "relevancy": 1.8747, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4832}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4616}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedDPC%20%3A%20Handling%20Data%20Heterogeneity%20and%20Partial%20Client%20Participation%20in%20Federated%20Learning&body=Title%3A%20FedDPC%20%3A%20Handling%20Data%20Heterogeneity%20and%20Partial%20Client%20Participation%20in%20Federated%20Learning%0AAuthor%3A%20Mrinmay%20Sen%20and%20Subhrajit%20Nag%0AAbstract%3A%20Data%20heterogeneity%20is%20a%20significant%20challenge%20in%20modern%20federated%20learning%20%28FL%29%20as%20it%20creates%20variance%20in%20local%20model%20updates%2C%20causing%20the%20aggregated%20global%20model%20to%20shift%20away%20from%20the%20true%20global%20optimum.%20Partial%20client%20participation%20in%20FL%20further%20exacerbates%20this%20issue%20by%20skewing%20the%20aggregation%20of%20local%20models%20towards%20the%20data%20distribution%20of%20participating%20clients.%20This%20creates%20additional%20variance%20in%20the%20global%20model%20updates%2C%20causing%20the%20global%20model%20to%20converge%20away%20from%20the%20optima%20of%20the%20global%20objective.%20These%20variances%20lead%20to%20instability%20in%20FL%20training%2C%20which%20degrades%20global%20model%20performance%20and%20slows%20down%20FL%20training.%20While%20existing%20literature%20primarily%20focuses%20on%20addressing%20data%20heterogeneity%2C%20the%20impact%20of%20partial%20client%20participation%20has%20received%20less%20attention.%20In%20this%20paper%2C%20we%20propose%20FedDPC%2C%20a%20novel%20FL%20method%2C%20designed%20to%20improve%20FL%20training%20and%20global%20model%20performance%20by%20mitigating%20both%20data%20heterogeneity%20and%20partial%20client%20participation.%20FedDPC%20addresses%20these%20issues%20by%20projecting%20each%20local%20update%20onto%20the%20previous%20global%20update%2C%20thereby%20controlling%20variance%20in%20both%20local%20and%20global%20updates.%20To%20further%20accelerate%20FL%20training%2C%20FedDPC%20employs%20adaptive%20scaling%20for%20each%20local%20update%20before%20aggregation.%20Extensive%20experiments%20on%20image%20classification%20tasks%20with%20multiple%20heterogeneously%20partitioned%20datasets%20validate%20the%20effectiveness%20of%20FedDPC.%20The%20results%20demonstrate%20that%20FedDPC%20outperforms%20state-of-the-art%20FL%20algorithms%20by%20achieving%20faster%20reduction%20in%20training%20loss%20and%20improved%20test%20accuracy%20across%20communication%20rounds.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedDPC%2520%253A%2520Handling%2520Data%2520Heterogeneity%2520and%2520Partial%2520Client%2520Participation%2520in%2520Federated%2520Learning%26entry.906535625%3DMrinmay%2520Sen%2520and%2520Subhrajit%2520Nag%26entry.1292438233%3DData%2520heterogeneity%2520is%2520a%2520significant%2520challenge%2520in%2520modern%2520federated%2520learning%2520%2528FL%2529%2520as%2520it%2520creates%2520variance%2520in%2520local%2520model%2520updates%252C%2520causing%2520the%2520aggregated%2520global%2520model%2520to%2520shift%2520away%2520from%2520the%2520true%2520global%2520optimum.%2520Partial%2520client%2520participation%2520in%2520FL%2520further%2520exacerbates%2520this%2520issue%2520by%2520skewing%2520the%2520aggregation%2520of%2520local%2520models%2520towards%2520the%2520data%2520distribution%2520of%2520participating%2520clients.%2520This%2520creates%2520additional%2520variance%2520in%2520the%2520global%2520model%2520updates%252C%2520causing%2520the%2520global%2520model%2520to%2520converge%2520away%2520from%2520the%2520optima%2520of%2520the%2520global%2520objective.%2520These%2520variances%2520lead%2520to%2520instability%2520in%2520FL%2520training%252C%2520which%2520degrades%2520global%2520model%2520performance%2520and%2520slows%2520down%2520FL%2520training.%2520While%2520existing%2520literature%2520primarily%2520focuses%2520on%2520addressing%2520data%2520heterogeneity%252C%2520the%2520impact%2520of%2520partial%2520client%2520participation%2520has%2520received%2520less%2520attention.%2520In%2520this%2520paper%252C%2520we%2520propose%2520FedDPC%252C%2520a%2520novel%2520FL%2520method%252C%2520designed%2520to%2520improve%2520FL%2520training%2520and%2520global%2520model%2520performance%2520by%2520mitigating%2520both%2520data%2520heterogeneity%2520and%2520partial%2520client%2520participation.%2520FedDPC%2520addresses%2520these%2520issues%2520by%2520projecting%2520each%2520local%2520update%2520onto%2520the%2520previous%2520global%2520update%252C%2520thereby%2520controlling%2520variance%2520in%2520both%2520local%2520and%2520global%2520updates.%2520To%2520further%2520accelerate%2520FL%2520training%252C%2520FedDPC%2520employs%2520adaptive%2520scaling%2520for%2520each%2520local%2520update%2520before%2520aggregation.%2520Extensive%2520experiments%2520on%2520image%2520classification%2520tasks%2520with%2520multiple%2520heterogeneously%2520partitioned%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520FedDPC.%2520The%2520results%2520demonstrate%2520that%2520FedDPC%2520outperforms%2520state-of-the-art%2520FL%2520algorithms%2520by%2520achieving%2520faster%2520reduction%2520in%2520training%2520loss%2520and%2520improved%2520test%2520accuracy%2520across%2520communication%2520rounds.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedDPC%20%3A%20Handling%20Data%20Heterogeneity%20and%20Partial%20Client%20Participation%20in%20Federated%20Learning&entry.906535625=Mrinmay%20Sen%20and%20Subhrajit%20Nag&entry.1292438233=Data%20heterogeneity%20is%20a%20significant%20challenge%20in%20modern%20federated%20learning%20%28FL%29%20as%20it%20creates%20variance%20in%20local%20model%20updates%2C%20causing%20the%20aggregated%20global%20model%20to%20shift%20away%20from%20the%20true%20global%20optimum.%20Partial%20client%20participation%20in%20FL%20further%20exacerbates%20this%20issue%20by%20skewing%20the%20aggregation%20of%20local%20models%20towards%20the%20data%20distribution%20of%20participating%20clients.%20This%20creates%20additional%20variance%20in%20the%20global%20model%20updates%2C%20causing%20the%20global%20model%20to%20converge%20away%20from%20the%20optima%20of%20the%20global%20objective.%20These%20variances%20lead%20to%20instability%20in%20FL%20training%2C%20which%20degrades%20global%20model%20performance%20and%20slows%20down%20FL%20training.%20While%20existing%20literature%20primarily%20focuses%20on%20addressing%20data%20heterogeneity%2C%20the%20impact%20of%20partial%20client%20participation%20has%20received%20less%20attention.%20In%20this%20paper%2C%20we%20propose%20FedDPC%2C%20a%20novel%20FL%20method%2C%20designed%20to%20improve%20FL%20training%20and%20global%20model%20performance%20by%20mitigating%20both%20data%20heterogeneity%20and%20partial%20client%20participation.%20FedDPC%20addresses%20these%20issues%20by%20projecting%20each%20local%20update%20onto%20the%20previous%20global%20update%2C%20thereby%20controlling%20variance%20in%20both%20local%20and%20global%20updates.%20To%20further%20accelerate%20FL%20training%2C%20FedDPC%20employs%20adaptive%20scaling%20for%20each%20local%20update%20before%20aggregation.%20Extensive%20experiments%20on%20image%20classification%20tasks%20with%20multiple%20heterogeneously%20partitioned%20datasets%20validate%20the%20effectiveness%20of%20FedDPC.%20The%20results%20demonstrate%20that%20FedDPC%20outperforms%20state-of-the-art%20FL%20algorithms%20by%20achieving%20faster%20reduction%20in%20training%20loss%20and%20improved%20test%20accuracy%20across%20communication%20rounds.&entry.1838667208=http%3A//arxiv.org/abs/2512.20329v1&entry.124074799=Read"},
{"title": "Explaining Tournament Solutions with Minimal Supports", "author": "Cl\u00e9ment Contet and Umberto Grandi and J\u00e9r\u00f4me Mengin", "abstract": "Tournaments are widely used models to represent pairwise dominance between candidates, alternatives, or teams. We study the problem of providing certified explanations for why a candidate appears among the winners under various tournament rules. To this end, we identify minimal supports, minimal sub-tournaments in which the candidate is guaranteed to win regardless of how the rest of the tournament is completed (that is, the candidate is a necessary winner of the sub-tournament). This notion corresponds to an abductive explanation for the question,\"Why does the winner win the tournament?\", a central concept in formal explainable AI. We focus on common tournament solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule, the maximin rule, and the weighted uncovered set. For each rule we determine the size of the smallest minimal supports, and we present polynomial-time algorithms to compute them for all solutions except for the weighted uncovered set, for which the problem is NP-complete. Finally, we show how minimal supports can serve to produce compact, certified, and intuitive explanations for tournament solutions.", "link": "http://arxiv.org/abs/2509.09312v4", "date": "2025-12-23", "relevancy": 1.8631, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3796}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3709}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20Tournament%20Solutions%20with%20Minimal%20Supports&body=Title%3A%20Explaining%20Tournament%20Solutions%20with%20Minimal%20Supports%0AAuthor%3A%20Cl%C3%A9ment%20Contet%20and%20Umberto%20Grandi%20and%20J%C3%A9r%C3%B4me%20Mengin%0AAbstract%3A%20Tournaments%20are%20widely%20used%20models%20to%20represent%20pairwise%20dominance%20between%20candidates%2C%20alternatives%2C%20or%20teams.%20We%20study%20the%20problem%20of%20providing%20certified%20explanations%20for%20why%20a%20candidate%20appears%20among%20the%20winners%20under%20various%20tournament%20rules.%20To%20this%20end%2C%20we%20identify%20minimal%20supports%2C%20minimal%20sub-tournaments%20in%20which%20the%20candidate%20is%20guaranteed%20to%20win%20regardless%20of%20how%20the%20rest%20of%20the%20tournament%20is%20completed%20%28that%20is%2C%20the%20candidate%20is%20a%20necessary%20winner%20of%20the%20sub-tournament%29.%20This%20notion%20corresponds%20to%20an%20abductive%20explanation%20for%20the%20question%2C%22Why%20does%20the%20winner%20win%20the%20tournament%3F%22%2C%20a%20central%20concept%20in%20formal%20explainable%20AI.%20We%20focus%20on%20common%20tournament%20solutions%3A%20the%20top%20cycle%2C%20the%20uncovered%20set%2C%20the%20Copeland%20rule%2C%20the%20Borda%20rule%2C%20the%20maximin%20rule%2C%20and%20the%20weighted%20uncovered%20set.%20For%20each%20rule%20we%20determine%20the%20size%20of%20the%20smallest%20minimal%20supports%2C%20and%20we%20present%20polynomial-time%20algorithms%20to%20compute%20them%20for%20all%20solutions%20except%20for%20the%20weighted%20uncovered%20set%2C%20for%20which%20the%20problem%20is%20NP-complete.%20Finally%2C%20we%20show%20how%20minimal%20supports%20can%20serve%20to%20produce%20compact%2C%20certified%2C%20and%20intuitive%20explanations%20for%20tournament%20solutions.%0ALink%3A%20http%3A//arxiv.org/abs/2509.09312v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520Tournament%2520Solutions%2520with%2520Minimal%2520Supports%26entry.906535625%3DCl%25C3%25A9ment%2520Contet%2520and%2520Umberto%2520Grandi%2520and%2520J%25C3%25A9r%25C3%25B4me%2520Mengin%26entry.1292438233%3DTournaments%2520are%2520widely%2520used%2520models%2520to%2520represent%2520pairwise%2520dominance%2520between%2520candidates%252C%2520alternatives%252C%2520or%2520teams.%2520We%2520study%2520the%2520problem%2520of%2520providing%2520certified%2520explanations%2520for%2520why%2520a%2520candidate%2520appears%2520among%2520the%2520winners%2520under%2520various%2520tournament%2520rules.%2520To%2520this%2520end%252C%2520we%2520identify%2520minimal%2520supports%252C%2520minimal%2520sub-tournaments%2520in%2520which%2520the%2520candidate%2520is%2520guaranteed%2520to%2520win%2520regardless%2520of%2520how%2520the%2520rest%2520of%2520the%2520tournament%2520is%2520completed%2520%2528that%2520is%252C%2520the%2520candidate%2520is%2520a%2520necessary%2520winner%2520of%2520the%2520sub-tournament%2529.%2520This%2520notion%2520corresponds%2520to%2520an%2520abductive%2520explanation%2520for%2520the%2520question%252C%2522Why%2520does%2520the%2520winner%2520win%2520the%2520tournament%253F%2522%252C%2520a%2520central%2520concept%2520in%2520formal%2520explainable%2520AI.%2520We%2520focus%2520on%2520common%2520tournament%2520solutions%253A%2520the%2520top%2520cycle%252C%2520the%2520uncovered%2520set%252C%2520the%2520Copeland%2520rule%252C%2520the%2520Borda%2520rule%252C%2520the%2520maximin%2520rule%252C%2520and%2520the%2520weighted%2520uncovered%2520set.%2520For%2520each%2520rule%2520we%2520determine%2520the%2520size%2520of%2520the%2520smallest%2520minimal%2520supports%252C%2520and%2520we%2520present%2520polynomial-time%2520algorithms%2520to%2520compute%2520them%2520for%2520all%2520solutions%2520except%2520for%2520the%2520weighted%2520uncovered%2520set%252C%2520for%2520which%2520the%2520problem%2520is%2520NP-complete.%2520Finally%252C%2520we%2520show%2520how%2520minimal%2520supports%2520can%2520serve%2520to%2520produce%2520compact%252C%2520certified%252C%2520and%2520intuitive%2520explanations%2520for%2520tournament%2520solutions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09312v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20Tournament%20Solutions%20with%20Minimal%20Supports&entry.906535625=Cl%C3%A9ment%20Contet%20and%20Umberto%20Grandi%20and%20J%C3%A9r%C3%B4me%20Mengin&entry.1292438233=Tournaments%20are%20widely%20used%20models%20to%20represent%20pairwise%20dominance%20between%20candidates%2C%20alternatives%2C%20or%20teams.%20We%20study%20the%20problem%20of%20providing%20certified%20explanations%20for%20why%20a%20candidate%20appears%20among%20the%20winners%20under%20various%20tournament%20rules.%20To%20this%20end%2C%20we%20identify%20minimal%20supports%2C%20minimal%20sub-tournaments%20in%20which%20the%20candidate%20is%20guaranteed%20to%20win%20regardless%20of%20how%20the%20rest%20of%20the%20tournament%20is%20completed%20%28that%20is%2C%20the%20candidate%20is%20a%20necessary%20winner%20of%20the%20sub-tournament%29.%20This%20notion%20corresponds%20to%20an%20abductive%20explanation%20for%20the%20question%2C%22Why%20does%20the%20winner%20win%20the%20tournament%3F%22%2C%20a%20central%20concept%20in%20formal%20explainable%20AI.%20We%20focus%20on%20common%20tournament%20solutions%3A%20the%20top%20cycle%2C%20the%20uncovered%20set%2C%20the%20Copeland%20rule%2C%20the%20Borda%20rule%2C%20the%20maximin%20rule%2C%20and%20the%20weighted%20uncovered%20set.%20For%20each%20rule%20we%20determine%20the%20size%20of%20the%20smallest%20minimal%20supports%2C%20and%20we%20present%20polynomial-time%20algorithms%20to%20compute%20them%20for%20all%20solutions%20except%20for%20the%20weighted%20uncovered%20set%2C%20for%20which%20the%20problem%20is%20NP-complete.%20Finally%2C%20we%20show%20how%20minimal%20supports%20can%20serve%20to%20produce%20compact%2C%20certified%2C%20and%20intuitive%20explanations%20for%20tournament%20solutions.&entry.1838667208=http%3A//arxiv.org/abs/2509.09312v4&entry.124074799=Read"},
{"title": "Reinforcement Learning From State and Temporal Differences", "author": "Lex Weaver and Jonathan Baxter", "abstract": "TD($\u03bb$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($\u03bb$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($\u03bb$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($\u03bb$), called STD($\u03bb$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($\u03bb$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($\u03bb$) on the two-state system and a variation on the well known acrobot problem.", "link": "http://arxiv.org/abs/2512.08855v2", "date": "2025-12-23", "relevancy": 1.8575, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4672}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4635}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20From%20State%20and%20Temporal%20Differences&body=Title%3A%20Reinforcement%20Learning%20From%20State%20and%20Temporal%20Differences%0AAuthor%3A%20Lex%20Weaver%20and%20Jonathan%20Baxter%0AAbstract%3A%20TD%28%24%CE%BB%24%29%20with%20function%20approximation%20has%20proved%20empirically%20successful%20for%20some%20complex%20reinforcement%20learning%20problems.%20For%20linear%20approximation%2C%20TD%28%24%CE%BB%24%29%20has%20been%20shown%20to%20minimise%20the%20squared%20error%20between%20the%20approximate%20value%20of%20each%20state%20and%20the%20true%20value.%20However%2C%20as%20far%20as%20policy%20is%20concerned%2C%20it%20is%20error%20in%20the%20relative%20ordering%20of%20states%20that%20is%20critical%2C%20rather%20than%20error%20in%20the%20state%20values.%20We%20illustrate%20this%20point%2C%20both%20in%20simple%20two-state%20and%20three-state%20systems%20in%20which%20TD%28%24%CE%BB%24%29--starting%20from%20an%20optimal%20policy--converges%20to%20a%20sub-optimal%20policy%2C%20and%20also%20in%20backgammon.%20We%20then%20present%20a%20modified%20form%20of%20TD%28%24%CE%BB%24%29%2C%20called%20STD%28%24%CE%BB%24%29%2C%20in%20which%20function%20approximators%20are%20trained%20with%20respect%20to%20relative%20state%20values%20on%20binary%20decision%20problems.%20A%20theoretical%20analysis%2C%20including%20a%20proof%20of%20monotonic%20policy%20improvement%20for%20STD%28%24%CE%BB%24%29%20in%20the%20context%20of%20the%20two-state%20system%2C%20is%20presented%2C%20along%20with%20a%20comparison%20with%20Bertsekas%27%20differential%20training%20method%20%5B1%5D.%20This%20is%20followed%20by%20successful%20demonstrations%20of%20STD%28%24%CE%BB%24%29%20on%20the%20two-state%20system%20and%20a%20variation%20on%20the%20well%20known%20acrobot%20problem.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08855v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520From%2520State%2520and%2520Temporal%2520Differences%26entry.906535625%3DLex%2520Weaver%2520and%2520Jonathan%2520Baxter%26entry.1292438233%3DTD%2528%2524%25CE%25BB%2524%2529%2520with%2520function%2520approximation%2520has%2520proved%2520empirically%2520successful%2520for%2520some%2520complex%2520reinforcement%2520learning%2520problems.%2520For%2520linear%2520approximation%252C%2520TD%2528%2524%25CE%25BB%2524%2529%2520has%2520been%2520shown%2520to%2520minimise%2520the%2520squared%2520error%2520between%2520the%2520approximate%2520value%2520of%2520each%2520state%2520and%2520the%2520true%2520value.%2520However%252C%2520as%2520far%2520as%2520policy%2520is%2520concerned%252C%2520it%2520is%2520error%2520in%2520the%2520relative%2520ordering%2520of%2520states%2520that%2520is%2520critical%252C%2520rather%2520than%2520error%2520in%2520the%2520state%2520values.%2520We%2520illustrate%2520this%2520point%252C%2520both%2520in%2520simple%2520two-state%2520and%2520three-state%2520systems%2520in%2520which%2520TD%2528%2524%25CE%25BB%2524%2529--starting%2520from%2520an%2520optimal%2520policy--converges%2520to%2520a%2520sub-optimal%2520policy%252C%2520and%2520also%2520in%2520backgammon.%2520We%2520then%2520present%2520a%2520modified%2520form%2520of%2520TD%2528%2524%25CE%25BB%2524%2529%252C%2520called%2520STD%2528%2524%25CE%25BB%2524%2529%252C%2520in%2520which%2520function%2520approximators%2520are%2520trained%2520with%2520respect%2520to%2520relative%2520state%2520values%2520on%2520binary%2520decision%2520problems.%2520A%2520theoretical%2520analysis%252C%2520including%2520a%2520proof%2520of%2520monotonic%2520policy%2520improvement%2520for%2520STD%2528%2524%25CE%25BB%2524%2529%2520in%2520the%2520context%2520of%2520the%2520two-state%2520system%252C%2520is%2520presented%252C%2520along%2520with%2520a%2520comparison%2520with%2520Bertsekas%2527%2520differential%2520training%2520method%2520%255B1%255D.%2520This%2520is%2520followed%2520by%2520successful%2520demonstrations%2520of%2520STD%2528%2524%25CE%25BB%2524%2529%2520on%2520the%2520two-state%2520system%2520and%2520a%2520variation%2520on%2520the%2520well%2520known%2520acrobot%2520problem.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08855v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20From%20State%20and%20Temporal%20Differences&entry.906535625=Lex%20Weaver%20and%20Jonathan%20Baxter&entry.1292438233=TD%28%24%CE%BB%24%29%20with%20function%20approximation%20has%20proved%20empirically%20successful%20for%20some%20complex%20reinforcement%20learning%20problems.%20For%20linear%20approximation%2C%20TD%28%24%CE%BB%24%29%20has%20been%20shown%20to%20minimise%20the%20squared%20error%20between%20the%20approximate%20value%20of%20each%20state%20and%20the%20true%20value.%20However%2C%20as%20far%20as%20policy%20is%20concerned%2C%20it%20is%20error%20in%20the%20relative%20ordering%20of%20states%20that%20is%20critical%2C%20rather%20than%20error%20in%20the%20state%20values.%20We%20illustrate%20this%20point%2C%20both%20in%20simple%20two-state%20and%20three-state%20systems%20in%20which%20TD%28%24%CE%BB%24%29--starting%20from%20an%20optimal%20policy--converges%20to%20a%20sub-optimal%20policy%2C%20and%20also%20in%20backgammon.%20We%20then%20present%20a%20modified%20form%20of%20TD%28%24%CE%BB%24%29%2C%20called%20STD%28%24%CE%BB%24%29%2C%20in%20which%20function%20approximators%20are%20trained%20with%20respect%20to%20relative%20state%20values%20on%20binary%20decision%20problems.%20A%20theoretical%20analysis%2C%20including%20a%20proof%20of%20monotonic%20policy%20improvement%20for%20STD%28%24%CE%BB%24%29%20in%20the%20context%20of%20the%20two-state%20system%2C%20is%20presented%2C%20along%20with%20a%20comparison%20with%20Bertsekas%27%20differential%20training%20method%20%5B1%5D.%20This%20is%20followed%20by%20successful%20demonstrations%20of%20STD%28%24%CE%BB%24%29%20on%20the%20two-state%20system%20and%20a%20variation%20on%20the%20well%20known%20acrobot%20problem.&entry.1838667208=http%3A//arxiv.org/abs/2512.08855v2&entry.124074799=Read"},
{"title": "UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images", "author": "Yiming Zhao and Yuanpeng Gao and Yuxuan Luo and Jiwei Duan and Shisong Lin and Longfei Xiong and Zhouhui Lian", "abstract": "AI-assisted graphic design has emerged as a powerful tool for automating the creation and editing of design elements such as posters, banners, and advertisements. While diffusion-based text-to-image models have demonstrated strong capabilities in visual content generation, their text rendering performance, particularly for small-scale typography and non-Latin scripts, remains limited. In this paper, we propose UTDesign, a unified framework for high-precision stylized text editing and conditional text generation in design images, supporting both English and Chinese scripts. Our framework introduces a novel DiT-based text style transfer model trained from scratch on a synthetic dataset, capable of generating transparent RGBA text foregrounds that preserve the style of reference glyphs. We further extend this model into a conditional text generation framework by training a multi-modal condition encoder on a curated dataset with detailed text annotations, enabling accurate, style-consistent text synthesis conditioned on background images, prompts, and layout specifications. Finally, we integrate our approach into a fully automated text-to-design (T2D) pipeline by incorporating pre-trained text-to-image (T2I) models and an MLLM-based layout planner. Extensive experiments demonstrate that UTDesign achieves state-of-the-art performance among open-source methods in terms of stylistic consistency and text accuracy, and also exhibits unique advantages compared to proprietary commercial approaches. Code and data for this paper are available at https://github.com/ZYM-PKU/UTDesign.", "link": "http://arxiv.org/abs/2512.20479v1", "date": "2025-12-23", "relevancy": 1.855, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6377}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5964}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UTDesign%3A%20A%20Unified%20Framework%20for%20Stylized%20Text%20Editing%20and%20Generation%20in%20Graphic%20Design%20Images&body=Title%3A%20UTDesign%3A%20A%20Unified%20Framework%20for%20Stylized%20Text%20Editing%20and%20Generation%20in%20Graphic%20Design%20Images%0AAuthor%3A%20Yiming%20Zhao%20and%20Yuanpeng%20Gao%20and%20Yuxuan%20Luo%20and%20Jiwei%20Duan%20and%20Shisong%20Lin%20and%20Longfei%20Xiong%20and%20Zhouhui%20Lian%0AAbstract%3A%20AI-assisted%20graphic%20design%20has%20emerged%20as%20a%20powerful%20tool%20for%20automating%20the%20creation%20and%20editing%20of%20design%20elements%20such%20as%20posters%2C%20banners%2C%20and%20advertisements.%20While%20diffusion-based%20text-to-image%20models%20have%20demonstrated%20strong%20capabilities%20in%20visual%20content%20generation%2C%20their%20text%20rendering%20performance%2C%20particularly%20for%20small-scale%20typography%20and%20non-Latin%20scripts%2C%20remains%20limited.%20In%20this%20paper%2C%20we%20propose%20UTDesign%2C%20a%20unified%20framework%20for%20high-precision%20stylized%20text%20editing%20and%20conditional%20text%20generation%20in%20design%20images%2C%20supporting%20both%20English%20and%20Chinese%20scripts.%20Our%20framework%20introduces%20a%20novel%20DiT-based%20text%20style%20transfer%20model%20trained%20from%20scratch%20on%20a%20synthetic%20dataset%2C%20capable%20of%20generating%20transparent%20RGBA%20text%20foregrounds%20that%20preserve%20the%20style%20of%20reference%20glyphs.%20We%20further%20extend%20this%20model%20into%20a%20conditional%20text%20generation%20framework%20by%20training%20a%20multi-modal%20condition%20encoder%20on%20a%20curated%20dataset%20with%20detailed%20text%20annotations%2C%20enabling%20accurate%2C%20style-consistent%20text%20synthesis%20conditioned%20on%20background%20images%2C%20prompts%2C%20and%20layout%20specifications.%20Finally%2C%20we%20integrate%20our%20approach%20into%20a%20fully%20automated%20text-to-design%20%28T2D%29%20pipeline%20by%20incorporating%20pre-trained%20text-to-image%20%28T2I%29%20models%20and%20an%20MLLM-based%20layout%20planner.%20Extensive%20experiments%20demonstrate%20that%20UTDesign%20achieves%20state-of-the-art%20performance%20among%20open-source%20methods%20in%20terms%20of%20stylistic%20consistency%20and%20text%20accuracy%2C%20and%20also%20exhibits%20unique%20advantages%20compared%20to%20proprietary%20commercial%20approaches.%20Code%20and%20data%20for%20this%20paper%20are%20available%20at%20https%3A//github.com/ZYM-PKU/UTDesign.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20479v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUTDesign%253A%2520A%2520Unified%2520Framework%2520for%2520Stylized%2520Text%2520Editing%2520and%2520Generation%2520in%2520Graphic%2520Design%2520Images%26entry.906535625%3DYiming%2520Zhao%2520and%2520Yuanpeng%2520Gao%2520and%2520Yuxuan%2520Luo%2520and%2520Jiwei%2520Duan%2520and%2520Shisong%2520Lin%2520and%2520Longfei%2520Xiong%2520and%2520Zhouhui%2520Lian%26entry.1292438233%3DAI-assisted%2520graphic%2520design%2520has%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520automating%2520the%2520creation%2520and%2520editing%2520of%2520design%2520elements%2520such%2520as%2520posters%252C%2520banners%252C%2520and%2520advertisements.%2520While%2520diffusion-based%2520text-to-image%2520models%2520have%2520demonstrated%2520strong%2520capabilities%2520in%2520visual%2520content%2520generation%252C%2520their%2520text%2520rendering%2520performance%252C%2520particularly%2520for%2520small-scale%2520typography%2520and%2520non-Latin%2520scripts%252C%2520remains%2520limited.%2520In%2520this%2520paper%252C%2520we%2520propose%2520UTDesign%252C%2520a%2520unified%2520framework%2520for%2520high-precision%2520stylized%2520text%2520editing%2520and%2520conditional%2520text%2520generation%2520in%2520design%2520images%252C%2520supporting%2520both%2520English%2520and%2520Chinese%2520scripts.%2520Our%2520framework%2520introduces%2520a%2520novel%2520DiT-based%2520text%2520style%2520transfer%2520model%2520trained%2520from%2520scratch%2520on%2520a%2520synthetic%2520dataset%252C%2520capable%2520of%2520generating%2520transparent%2520RGBA%2520text%2520foregrounds%2520that%2520preserve%2520the%2520style%2520of%2520reference%2520glyphs.%2520We%2520further%2520extend%2520this%2520model%2520into%2520a%2520conditional%2520text%2520generation%2520framework%2520by%2520training%2520a%2520multi-modal%2520condition%2520encoder%2520on%2520a%2520curated%2520dataset%2520with%2520detailed%2520text%2520annotations%252C%2520enabling%2520accurate%252C%2520style-consistent%2520text%2520synthesis%2520conditioned%2520on%2520background%2520images%252C%2520prompts%252C%2520and%2520layout%2520specifications.%2520Finally%252C%2520we%2520integrate%2520our%2520approach%2520into%2520a%2520fully%2520automated%2520text-to-design%2520%2528T2D%2529%2520pipeline%2520by%2520incorporating%2520pre-trained%2520text-to-image%2520%2528T2I%2529%2520models%2520and%2520an%2520MLLM-based%2520layout%2520planner.%2520Extensive%2520experiments%2520demonstrate%2520that%2520UTDesign%2520achieves%2520state-of-the-art%2520performance%2520among%2520open-source%2520methods%2520in%2520terms%2520of%2520stylistic%2520consistency%2520and%2520text%2520accuracy%252C%2520and%2520also%2520exhibits%2520unique%2520advantages%2520compared%2520to%2520proprietary%2520commercial%2520approaches.%2520Code%2520and%2520data%2520for%2520this%2520paper%2520are%2520available%2520at%2520https%253A//github.com/ZYM-PKU/UTDesign.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20479v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UTDesign%3A%20A%20Unified%20Framework%20for%20Stylized%20Text%20Editing%20and%20Generation%20in%20Graphic%20Design%20Images&entry.906535625=Yiming%20Zhao%20and%20Yuanpeng%20Gao%20and%20Yuxuan%20Luo%20and%20Jiwei%20Duan%20and%20Shisong%20Lin%20and%20Longfei%20Xiong%20and%20Zhouhui%20Lian&entry.1292438233=AI-assisted%20graphic%20design%20has%20emerged%20as%20a%20powerful%20tool%20for%20automating%20the%20creation%20and%20editing%20of%20design%20elements%20such%20as%20posters%2C%20banners%2C%20and%20advertisements.%20While%20diffusion-based%20text-to-image%20models%20have%20demonstrated%20strong%20capabilities%20in%20visual%20content%20generation%2C%20their%20text%20rendering%20performance%2C%20particularly%20for%20small-scale%20typography%20and%20non-Latin%20scripts%2C%20remains%20limited.%20In%20this%20paper%2C%20we%20propose%20UTDesign%2C%20a%20unified%20framework%20for%20high-precision%20stylized%20text%20editing%20and%20conditional%20text%20generation%20in%20design%20images%2C%20supporting%20both%20English%20and%20Chinese%20scripts.%20Our%20framework%20introduces%20a%20novel%20DiT-based%20text%20style%20transfer%20model%20trained%20from%20scratch%20on%20a%20synthetic%20dataset%2C%20capable%20of%20generating%20transparent%20RGBA%20text%20foregrounds%20that%20preserve%20the%20style%20of%20reference%20glyphs.%20We%20further%20extend%20this%20model%20into%20a%20conditional%20text%20generation%20framework%20by%20training%20a%20multi-modal%20condition%20encoder%20on%20a%20curated%20dataset%20with%20detailed%20text%20annotations%2C%20enabling%20accurate%2C%20style-consistent%20text%20synthesis%20conditioned%20on%20background%20images%2C%20prompts%2C%20and%20layout%20specifications.%20Finally%2C%20we%20integrate%20our%20approach%20into%20a%20fully%20automated%20text-to-design%20%28T2D%29%20pipeline%20by%20incorporating%20pre-trained%20text-to-image%20%28T2I%29%20models%20and%20an%20MLLM-based%20layout%20planner.%20Extensive%20experiments%20demonstrate%20that%20UTDesign%20achieves%20state-of-the-art%20performance%20among%20open-source%20methods%20in%20terms%20of%20stylistic%20consistency%20and%20text%20accuracy%2C%20and%20also%20exhibits%20unique%20advantages%20compared%20to%20proprietary%20commercial%20approaches.%20Code%20and%20data%20for%20this%20paper%20are%20available%20at%20https%3A//github.com/ZYM-PKU/UTDesign.&entry.1838667208=http%3A//arxiv.org/abs/2512.20479v1&entry.124074799=Read"},
{"title": "Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition", "author": "Lingfeng Liu and Yixin Song and Dazhong Shen and Bing Yin and Hao Li and Yanyong Zhang and Chao Wang", "abstract": "Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant \"popularity direction\" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.", "link": "http://arxiv.org/abs/2512.10688v4", "date": "2025-12-23", "relevancy": 1.3461, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4603}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4527}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Popularity%20Bias%20in%20Collaborative%20Filtering%20via%20Analytical%20Vector%20Decomposition&body=Title%3A%20Rethinking%20Popularity%20Bias%20in%20Collaborative%20Filtering%20via%20Analytical%20Vector%20Decomposition%0AAuthor%3A%20Lingfeng%20Liu%20and%20Yixin%20Song%20and%20Dazhong%20Shen%20and%20Bing%20Yin%20and%20Hao%20Li%20and%20Yanyong%20Zhang%20and%20Chao%20Wang%0AAbstract%3A%20Popularity%20bias%20fundamentally%20undermines%20the%20personalization%20capabilities%20of%20collaborative%20filtering%20%28CF%29%20models%2C%20causing%20them%20to%20disproportionately%20recommend%20popular%20items%20while%20neglecting%20users%27%20genuine%20preferences%20for%20niche%20content.%20While%20existing%20approaches%20treat%20this%20as%20an%20external%20confounding%20factor%2C%20we%20reveal%20that%20popularity%20bias%20is%20an%20intrinsic%20geometric%20artifact%20of%20Bayesian%20Pairwise%20Ranking%20%28BPR%29%20optimization%20in%20CF%20models.%20Through%20rigorous%20mathematical%20analysis%2C%20we%20prove%20that%20BPR%20systematically%20organizes%20item%20embeddings%20along%20a%20dominant%20%22popularity%20direction%22%20where%20embedding%20magnitudes%20directly%20correlate%20with%20interaction%20frequency.%20This%20geometric%20distortion%20forces%20user%20embeddings%20to%20simultaneously%20handle%20two%20conflicting%20tasks-expressing%20genuine%20preference%20and%20calibrating%20against%20global%20popularity-trapping%20them%20in%20suboptimal%20configurations%20that%20favor%20popular%20items%20regardless%20of%20individual%20tastes.%20We%20propose%20Directional%20Decomposition%20and%20Correction%20%28DDC%29%2C%20a%20universally%20applicable%20framework%20that%20surgically%20corrects%20this%20embedding%20geometry%20through%20asymmetric%20directional%20updates.%20DDC%20guides%20positive%20interactions%20along%20personalized%20preference%20directions%20while%20steering%20negative%20interactions%20away%20from%20the%20global%20popularity%20direction%2C%20disentangling%20preference%20from%20popularity%20at%20the%20geometric%20source.%20Extensive%20experiments%20across%20multiple%20BPR-based%20architectures%20demonstrate%20that%20DDC%20significantly%20outperforms%20state-of-the-art%20debiasing%20methods%2C%20reducing%20training%20loss%20to%20less%20than%205%25%20of%20heavily-tuned%20baselines%20while%20achieving%20superior%20recommendation%20quality%20and%20fairness.%20Code%20is%20available%20in%20https%3A//github.com/LingFeng-Liu-AI/DDC.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10688v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Popularity%2520Bias%2520in%2520Collaborative%2520Filtering%2520via%2520Analytical%2520Vector%2520Decomposition%26entry.906535625%3DLingfeng%2520Liu%2520and%2520Yixin%2520Song%2520and%2520Dazhong%2520Shen%2520and%2520Bing%2520Yin%2520and%2520Hao%2520Li%2520and%2520Yanyong%2520Zhang%2520and%2520Chao%2520Wang%26entry.1292438233%3DPopularity%2520bias%2520fundamentally%2520undermines%2520the%2520personalization%2520capabilities%2520of%2520collaborative%2520filtering%2520%2528CF%2529%2520models%252C%2520causing%2520them%2520to%2520disproportionately%2520recommend%2520popular%2520items%2520while%2520neglecting%2520users%2527%2520genuine%2520preferences%2520for%2520niche%2520content.%2520While%2520existing%2520approaches%2520treat%2520this%2520as%2520an%2520external%2520confounding%2520factor%252C%2520we%2520reveal%2520that%2520popularity%2520bias%2520is%2520an%2520intrinsic%2520geometric%2520artifact%2520of%2520Bayesian%2520Pairwise%2520Ranking%2520%2528BPR%2529%2520optimization%2520in%2520CF%2520models.%2520Through%2520rigorous%2520mathematical%2520analysis%252C%2520we%2520prove%2520that%2520BPR%2520systematically%2520organizes%2520item%2520embeddings%2520along%2520a%2520dominant%2520%2522popularity%2520direction%2522%2520where%2520embedding%2520magnitudes%2520directly%2520correlate%2520with%2520interaction%2520frequency.%2520This%2520geometric%2520distortion%2520forces%2520user%2520embeddings%2520to%2520simultaneously%2520handle%2520two%2520conflicting%2520tasks-expressing%2520genuine%2520preference%2520and%2520calibrating%2520against%2520global%2520popularity-trapping%2520them%2520in%2520suboptimal%2520configurations%2520that%2520favor%2520popular%2520items%2520regardless%2520of%2520individual%2520tastes.%2520We%2520propose%2520Directional%2520Decomposition%2520and%2520Correction%2520%2528DDC%2529%252C%2520a%2520universally%2520applicable%2520framework%2520that%2520surgically%2520corrects%2520this%2520embedding%2520geometry%2520through%2520asymmetric%2520directional%2520updates.%2520DDC%2520guides%2520positive%2520interactions%2520along%2520personalized%2520preference%2520directions%2520while%2520steering%2520negative%2520interactions%2520away%2520from%2520the%2520global%2520popularity%2520direction%252C%2520disentangling%2520preference%2520from%2520popularity%2520at%2520the%2520geometric%2520source.%2520Extensive%2520experiments%2520across%2520multiple%2520BPR-based%2520architectures%2520demonstrate%2520that%2520DDC%2520significantly%2520outperforms%2520state-of-the-art%2520debiasing%2520methods%252C%2520reducing%2520training%2520loss%2520to%2520less%2520than%25205%2525%2520of%2520heavily-tuned%2520baselines%2520while%2520achieving%2520superior%2520recommendation%2520quality%2520and%2520fairness.%2520Code%2520is%2520available%2520in%2520https%253A//github.com/LingFeng-Liu-AI/DDC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10688v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Popularity%20Bias%20in%20Collaborative%20Filtering%20via%20Analytical%20Vector%20Decomposition&entry.906535625=Lingfeng%20Liu%20and%20Yixin%20Song%20and%20Dazhong%20Shen%20and%20Bing%20Yin%20and%20Hao%20Li%20and%20Yanyong%20Zhang%20and%20Chao%20Wang&entry.1292438233=Popularity%20bias%20fundamentally%20undermines%20the%20personalization%20capabilities%20of%20collaborative%20filtering%20%28CF%29%20models%2C%20causing%20them%20to%20disproportionately%20recommend%20popular%20items%20while%20neglecting%20users%27%20genuine%20preferences%20for%20niche%20content.%20While%20existing%20approaches%20treat%20this%20as%20an%20external%20confounding%20factor%2C%20we%20reveal%20that%20popularity%20bias%20is%20an%20intrinsic%20geometric%20artifact%20of%20Bayesian%20Pairwise%20Ranking%20%28BPR%29%20optimization%20in%20CF%20models.%20Through%20rigorous%20mathematical%20analysis%2C%20we%20prove%20that%20BPR%20systematically%20organizes%20item%20embeddings%20along%20a%20dominant%20%22popularity%20direction%22%20where%20embedding%20magnitudes%20directly%20correlate%20with%20interaction%20frequency.%20This%20geometric%20distortion%20forces%20user%20embeddings%20to%20simultaneously%20handle%20two%20conflicting%20tasks-expressing%20genuine%20preference%20and%20calibrating%20against%20global%20popularity-trapping%20them%20in%20suboptimal%20configurations%20that%20favor%20popular%20items%20regardless%20of%20individual%20tastes.%20We%20propose%20Directional%20Decomposition%20and%20Correction%20%28DDC%29%2C%20a%20universally%20applicable%20framework%20that%20surgically%20corrects%20this%20embedding%20geometry%20through%20asymmetric%20directional%20updates.%20DDC%20guides%20positive%20interactions%20along%20personalized%20preference%20directions%20while%20steering%20negative%20interactions%20away%20from%20the%20global%20popularity%20direction%2C%20disentangling%20preference%20from%20popularity%20at%20the%20geometric%20source.%20Extensive%20experiments%20across%20multiple%20BPR-based%20architectures%20demonstrate%20that%20DDC%20significantly%20outperforms%20state-of-the-art%20debiasing%20methods%2C%20reducing%20training%20loss%20to%20less%20than%205%25%20of%20heavily-tuned%20baselines%20while%20achieving%20superior%20recommendation%20quality%20and%20fairness.%20Code%20is%20available%20in%20https%3A//github.com/LingFeng-Liu-AI/DDC.&entry.1838667208=http%3A//arxiv.org/abs/2512.10688v4&entry.124074799=Read"},
{"title": "Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models", "author": "Kuofeng Gao and Yufei Zhu and Yiming Li and Jiawang Bai and Yong Yang and Zhifeng Li and Shu-Tao Xia", "abstract": "Text-to-image (T2I) diffusion models enable high-quality image generation conditioned on textual prompts. However, fine-tuning these pre-trained models for personalization raises concerns about unauthorized dataset usage. To address this issue, dataset ownership verification (DOV) has recently been proposed, which embeds watermarks into fine-tuning datasets via backdoor techniques. These watermarks remain dormant on benign samples but produce owner-specified outputs when triggered. Despite its promise, the robustness of DOV against copyright evasion attacks (CEA) remains unexplored. In this paper, we investigate how adversaries can circumvent these mechanisms, enabling models trained on watermarked datasets to bypass ownership verification. We begin by analyzing the limitations of potential attacks achieved by backdoor removal, including TPD and T2IShield. In practice, TPD suffers from inconsistent effectiveness due to randomness, while T2IShield fails when watermarks are embedded as local image patches. To this end, we introduce CEAT2I, the first CEA specifically targeting DOV in T2I diffusion models. CEAT2I consists of three stages: (1) motivated by the observation that T2I models converge faster on watermarked samples with respect to intermediate features rather than training loss, we reliably detect watermarked samples; (2) we iteratively ablate tokens from the prompts of detected samples and monitor feature shifts to identify trigger tokens; and (3) we apply a closed-form concept erasure method to remove the injected watermarks. Extensive experiments demonstrate that CEAT2I effectively evades state-of-the-art DOV mechanisms while preserving model performance. The code is available at https://github.com/csyufei/CEAT2I.", "link": "http://arxiv.org/abs/2505.02824v2", "date": "2025-12-23", "relevancy": 1.7257, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.597}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5654}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Dataset%20Copyright%20Evasion%20Attack%20against%20Personalized%20Text-to-Image%20Diffusion%20Models&body=Title%3A%20Towards%20Dataset%20Copyright%20Evasion%20Attack%20against%20Personalized%20Text-to-Image%20Diffusion%20Models%0AAuthor%3A%20Kuofeng%20Gao%20and%20Yufei%20Zhu%20and%20Yiming%20Li%20and%20Jiawang%20Bai%20and%20Yong%20Yang%20and%20Zhifeng%20Li%20and%20Shu-Tao%20Xia%0AAbstract%3A%20Text-to-image%20%28T2I%29%20diffusion%20models%20enable%20high-quality%20image%20generation%20conditioned%20on%20textual%20prompts.%20However%2C%20fine-tuning%20these%20pre-trained%20models%20for%20personalization%20raises%20concerns%20about%20unauthorized%20dataset%20usage.%20To%20address%20this%20issue%2C%20dataset%20ownership%20verification%20%28DOV%29%20has%20recently%20been%20proposed%2C%20which%20embeds%20watermarks%20into%20fine-tuning%20datasets%20via%20backdoor%20techniques.%20These%20watermarks%20remain%20dormant%20on%20benign%20samples%20but%20produce%20owner-specified%20outputs%20when%20triggered.%20Despite%20its%20promise%2C%20the%20robustness%20of%20DOV%20against%20copyright%20evasion%20attacks%20%28CEA%29%20remains%20unexplored.%20In%20this%20paper%2C%20we%20investigate%20how%20adversaries%20can%20circumvent%20these%20mechanisms%2C%20enabling%20models%20trained%20on%20watermarked%20datasets%20to%20bypass%20ownership%20verification.%20We%20begin%20by%20analyzing%20the%20limitations%20of%20potential%20attacks%20achieved%20by%20backdoor%20removal%2C%20including%20TPD%20and%20T2IShield.%20In%20practice%2C%20TPD%20suffers%20from%20inconsistent%20effectiveness%20due%20to%20randomness%2C%20while%20T2IShield%20fails%20when%20watermarks%20are%20embedded%20as%20local%20image%20patches.%20To%20this%20end%2C%20we%20introduce%20CEAT2I%2C%20the%20first%20CEA%20specifically%20targeting%20DOV%20in%20T2I%20diffusion%20models.%20CEAT2I%20consists%20of%20three%20stages%3A%20%281%29%20motivated%20by%20the%20observation%20that%20T2I%20models%20converge%20faster%20on%20watermarked%20samples%20with%20respect%20to%20intermediate%20features%20rather%20than%20training%20loss%2C%20we%20reliably%20detect%20watermarked%20samples%3B%20%282%29%20we%20iteratively%20ablate%20tokens%20from%20the%20prompts%20of%20detected%20samples%20and%20monitor%20feature%20shifts%20to%20identify%20trigger%20tokens%3B%20and%20%283%29%20we%20apply%20a%20closed-form%20concept%20erasure%20method%20to%20remove%20the%20injected%20watermarks.%20Extensive%20experiments%20demonstrate%20that%20CEAT2I%20effectively%20evades%20state-of-the-art%20DOV%20mechanisms%20while%20preserving%20model%20performance.%20The%20code%20is%20available%20at%20https%3A//github.com/csyufei/CEAT2I.%0ALink%3A%20http%3A//arxiv.org/abs/2505.02824v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Dataset%2520Copyright%2520Evasion%2520Attack%2520against%2520Personalized%2520Text-to-Image%2520Diffusion%2520Models%26entry.906535625%3DKuofeng%2520Gao%2520and%2520Yufei%2520Zhu%2520and%2520Yiming%2520Li%2520and%2520Jiawang%2520Bai%2520and%2520Yong%2520Yang%2520and%2520Zhifeng%2520Li%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3DText-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520enable%2520high-quality%2520image%2520generation%2520conditioned%2520on%2520textual%2520prompts.%2520However%252C%2520fine-tuning%2520these%2520pre-trained%2520models%2520for%2520personalization%2520raises%2520concerns%2520about%2520unauthorized%2520dataset%2520usage.%2520To%2520address%2520this%2520issue%252C%2520dataset%2520ownership%2520verification%2520%2528DOV%2529%2520has%2520recently%2520been%2520proposed%252C%2520which%2520embeds%2520watermarks%2520into%2520fine-tuning%2520datasets%2520via%2520backdoor%2520techniques.%2520These%2520watermarks%2520remain%2520dormant%2520on%2520benign%2520samples%2520but%2520produce%2520owner-specified%2520outputs%2520when%2520triggered.%2520Despite%2520its%2520promise%252C%2520the%2520robustness%2520of%2520DOV%2520against%2520copyright%2520evasion%2520attacks%2520%2528CEA%2529%2520remains%2520unexplored.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520how%2520adversaries%2520can%2520circumvent%2520these%2520mechanisms%252C%2520enabling%2520models%2520trained%2520on%2520watermarked%2520datasets%2520to%2520bypass%2520ownership%2520verification.%2520We%2520begin%2520by%2520analyzing%2520the%2520limitations%2520of%2520potential%2520attacks%2520achieved%2520by%2520backdoor%2520removal%252C%2520including%2520TPD%2520and%2520T2IShield.%2520In%2520practice%252C%2520TPD%2520suffers%2520from%2520inconsistent%2520effectiveness%2520due%2520to%2520randomness%252C%2520while%2520T2IShield%2520fails%2520when%2520watermarks%2520are%2520embedded%2520as%2520local%2520image%2520patches.%2520To%2520this%2520end%252C%2520we%2520introduce%2520CEAT2I%252C%2520the%2520first%2520CEA%2520specifically%2520targeting%2520DOV%2520in%2520T2I%2520diffusion%2520models.%2520CEAT2I%2520consists%2520of%2520three%2520stages%253A%2520%25281%2529%2520motivated%2520by%2520the%2520observation%2520that%2520T2I%2520models%2520converge%2520faster%2520on%2520watermarked%2520samples%2520with%2520respect%2520to%2520intermediate%2520features%2520rather%2520than%2520training%2520loss%252C%2520we%2520reliably%2520detect%2520watermarked%2520samples%253B%2520%25282%2529%2520we%2520iteratively%2520ablate%2520tokens%2520from%2520the%2520prompts%2520of%2520detected%2520samples%2520and%2520monitor%2520feature%2520shifts%2520to%2520identify%2520trigger%2520tokens%253B%2520and%2520%25283%2529%2520we%2520apply%2520a%2520closed-form%2520concept%2520erasure%2520method%2520to%2520remove%2520the%2520injected%2520watermarks.%2520Extensive%2520experiments%2520demonstrate%2520that%2520CEAT2I%2520effectively%2520evades%2520state-of-the-art%2520DOV%2520mechanisms%2520while%2520preserving%2520model%2520performance.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/csyufei/CEAT2I.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02824v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Dataset%20Copyright%20Evasion%20Attack%20against%20Personalized%20Text-to-Image%20Diffusion%20Models&entry.906535625=Kuofeng%20Gao%20and%20Yufei%20Zhu%20and%20Yiming%20Li%20and%20Jiawang%20Bai%20and%20Yong%20Yang%20and%20Zhifeng%20Li%20and%20Shu-Tao%20Xia&entry.1292438233=Text-to-image%20%28T2I%29%20diffusion%20models%20enable%20high-quality%20image%20generation%20conditioned%20on%20textual%20prompts.%20However%2C%20fine-tuning%20these%20pre-trained%20models%20for%20personalization%20raises%20concerns%20about%20unauthorized%20dataset%20usage.%20To%20address%20this%20issue%2C%20dataset%20ownership%20verification%20%28DOV%29%20has%20recently%20been%20proposed%2C%20which%20embeds%20watermarks%20into%20fine-tuning%20datasets%20via%20backdoor%20techniques.%20These%20watermarks%20remain%20dormant%20on%20benign%20samples%20but%20produce%20owner-specified%20outputs%20when%20triggered.%20Despite%20its%20promise%2C%20the%20robustness%20of%20DOV%20against%20copyright%20evasion%20attacks%20%28CEA%29%20remains%20unexplored.%20In%20this%20paper%2C%20we%20investigate%20how%20adversaries%20can%20circumvent%20these%20mechanisms%2C%20enabling%20models%20trained%20on%20watermarked%20datasets%20to%20bypass%20ownership%20verification.%20We%20begin%20by%20analyzing%20the%20limitations%20of%20potential%20attacks%20achieved%20by%20backdoor%20removal%2C%20including%20TPD%20and%20T2IShield.%20In%20practice%2C%20TPD%20suffers%20from%20inconsistent%20effectiveness%20due%20to%20randomness%2C%20while%20T2IShield%20fails%20when%20watermarks%20are%20embedded%20as%20local%20image%20patches.%20To%20this%20end%2C%20we%20introduce%20CEAT2I%2C%20the%20first%20CEA%20specifically%20targeting%20DOV%20in%20T2I%20diffusion%20models.%20CEAT2I%20consists%20of%20three%20stages%3A%20%281%29%20motivated%20by%20the%20observation%20that%20T2I%20models%20converge%20faster%20on%20watermarked%20samples%20with%20respect%20to%20intermediate%20features%20rather%20than%20training%20loss%2C%20we%20reliably%20detect%20watermarked%20samples%3B%20%282%29%20we%20iteratively%20ablate%20tokens%20from%20the%20prompts%20of%20detected%20samples%20and%20monitor%20feature%20shifts%20to%20identify%20trigger%20tokens%3B%20and%20%283%29%20we%20apply%20a%20closed-form%20concept%20erasure%20method%20to%20remove%20the%20injected%20watermarks.%20Extensive%20experiments%20demonstrate%20that%20CEAT2I%20effectively%20evades%20state-of-the-art%20DOV%20mechanisms%20while%20preserving%20model%20performance.%20The%20code%20is%20available%20at%20https%3A//github.com/csyufei/CEAT2I.&entry.1838667208=http%3A//arxiv.org/abs/2505.02824v2&entry.124074799=Read"},
{"title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification", "author": "Moises Andrade and Joonhyuk Cha and Brandon Ho and Vriksha Srihari and Karmesh Yadav and Zsolt Kira", "abstract": "Verifiers--functions assigning rewards to agent behavior--have been key for AI progress in domains like math and code. However, extending gains to domains without clear-cut success criteria (e.g., computer use) remains a challenge: while humans can recognize desired outcomes, translating this intuition into scalable rules is nontrivial. Multimodal Large Language Models (MLLMs) emerge as a promising solution, given their world knowledge, human-preference alignment, and reasoning skills. We evaluate MLLMs as verifiers across web navigation, computer use, and robotic manipulation, and identify a critical limitation: a strong tendency to over-validate agent behavior, a phenomenon we term agreement bias. This bias is pervasive across models, resilient to test-time scaling, and poses risks to existing methods relying on MLLM evaluations. We discuss methods to evaluate and improve MLLM verifiers and introduce Self-Grounded Verification (SGV), a lightweight method that harnesses MLLMs' own sampling mechanisms by modulating (un)conditional generation to better leverage their knowledge, alignment, and reasoning. SGV operates in two steps: first, the MLLM is elicited to generate broad priors about desired behavior, independent of the data under evaluation. Then, conditioned on self-generated priors, it reasons over and evaluates a candidate trajectory. SGV yields more human-aligned evaluations with gains of up to 25pp in failure detection, 14pp in accuracy, and benefits extending to downstream applications. In self-refinement and online supervision, SGV boosts task completion of a GUI specialist in OSWorld, a diffusion policy in robomimic, and a ReAct agent in VisualWebArena--setting a new state of the art, surpassing the previous best by 20pp. We release an updated version of VisualWebArena featuring more human-aligned evaluators, high-fidelity environment parallelism, and speedups of over 10x.", "link": "http://arxiv.org/abs/2507.11662v2", "date": "2025-12-23", "relevancy": 1.6229, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5771}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5336}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%27s%20Think%20in%20Two%20Steps%3A%20Mitigating%20Agreement%20Bias%20in%20MLLMs%20with%20Self-Grounded%20Verification&body=Title%3A%20Let%27s%20Think%20in%20Two%20Steps%3A%20Mitigating%20Agreement%20Bias%20in%20MLLMs%20with%20Self-Grounded%20Verification%0AAuthor%3A%20Moises%20Andrade%20and%20Joonhyuk%20Cha%20and%20Brandon%20Ho%20and%20Vriksha%20Srihari%20and%20Karmesh%20Yadav%20and%20Zsolt%20Kira%0AAbstract%3A%20Verifiers--functions%20assigning%20rewards%20to%20agent%20behavior--have%20been%20key%20for%20AI%20progress%20in%20domains%20like%20math%20and%20code.%20However%2C%20extending%20gains%20to%20domains%20without%20clear-cut%20success%20criteria%20%28e.g.%2C%20computer%20use%29%20remains%20a%20challenge%3A%20while%20humans%20can%20recognize%20desired%20outcomes%2C%20translating%20this%20intuition%20into%20scalable%20rules%20is%20nontrivial.%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20emerge%20as%20a%20promising%20solution%2C%20given%20their%20world%20knowledge%2C%20human-preference%20alignment%2C%20and%20reasoning%20skills.%20We%20evaluate%20MLLMs%20as%20verifiers%20across%20web%20navigation%2C%20computer%20use%2C%20and%20robotic%20manipulation%2C%20and%20identify%20a%20critical%20limitation%3A%20a%20strong%20tendency%20to%20over-validate%20agent%20behavior%2C%20a%20phenomenon%20we%20term%20agreement%20bias.%20This%20bias%20is%20pervasive%20across%20models%2C%20resilient%20to%20test-time%20scaling%2C%20and%20poses%20risks%20to%20existing%20methods%20relying%20on%20MLLM%20evaluations.%20We%20discuss%20methods%20to%20evaluate%20and%20improve%20MLLM%20verifiers%20and%20introduce%20Self-Grounded%20Verification%20%28SGV%29%2C%20a%20lightweight%20method%20that%20harnesses%20MLLMs%27%20own%20sampling%20mechanisms%20by%20modulating%20%28un%29conditional%20generation%20to%20better%20leverage%20their%20knowledge%2C%20alignment%2C%20and%20reasoning.%20SGV%20operates%20in%20two%20steps%3A%20first%2C%20the%20MLLM%20is%20elicited%20to%20generate%20broad%20priors%20about%20desired%20behavior%2C%20independent%20of%20the%20data%20under%20evaluation.%20Then%2C%20conditioned%20on%20self-generated%20priors%2C%20it%20reasons%20over%20and%20evaluates%20a%20candidate%20trajectory.%20SGV%20yields%20more%20human-aligned%20evaluations%20with%20gains%20of%20up%20to%2025pp%20in%20failure%20detection%2C%2014pp%20in%20accuracy%2C%20and%20benefits%20extending%20to%20downstream%20applications.%20In%20self-refinement%20and%20online%20supervision%2C%20SGV%20boosts%20task%20completion%20of%20a%20GUI%20specialist%20in%20OSWorld%2C%20a%20diffusion%20policy%20in%20robomimic%2C%20and%20a%20ReAct%20agent%20in%20VisualWebArena--setting%20a%20new%20state%20of%20the%20art%2C%20surpassing%20the%20previous%20best%20by%2020pp.%20We%20release%20an%20updated%20version%20of%20VisualWebArena%20featuring%20more%20human-aligned%20evaluators%2C%20high-fidelity%20environment%20parallelism%2C%20and%20speedups%20of%20over%2010x.%0ALink%3A%20http%3A//arxiv.org/abs/2507.11662v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2527s%2520Think%2520in%2520Two%2520Steps%253A%2520Mitigating%2520Agreement%2520Bias%2520in%2520MLLMs%2520with%2520Self-Grounded%2520Verification%26entry.906535625%3DMoises%2520Andrade%2520and%2520Joonhyuk%2520Cha%2520and%2520Brandon%2520Ho%2520and%2520Vriksha%2520Srihari%2520and%2520Karmesh%2520Yadav%2520and%2520Zsolt%2520Kira%26entry.1292438233%3DVerifiers--functions%2520assigning%2520rewards%2520to%2520agent%2520behavior--have%2520been%2520key%2520for%2520AI%2520progress%2520in%2520domains%2520like%2520math%2520and%2520code.%2520However%252C%2520extending%2520gains%2520to%2520domains%2520without%2520clear-cut%2520success%2520criteria%2520%2528e.g.%252C%2520computer%2520use%2529%2520remains%2520a%2520challenge%253A%2520while%2520humans%2520can%2520recognize%2520desired%2520outcomes%252C%2520translating%2520this%2520intuition%2520into%2520scalable%2520rules%2520is%2520nontrivial.%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520emerge%2520as%2520a%2520promising%2520solution%252C%2520given%2520their%2520world%2520knowledge%252C%2520human-preference%2520alignment%252C%2520and%2520reasoning%2520skills.%2520We%2520evaluate%2520MLLMs%2520as%2520verifiers%2520across%2520web%2520navigation%252C%2520computer%2520use%252C%2520and%2520robotic%2520manipulation%252C%2520and%2520identify%2520a%2520critical%2520limitation%253A%2520a%2520strong%2520tendency%2520to%2520over-validate%2520agent%2520behavior%252C%2520a%2520phenomenon%2520we%2520term%2520agreement%2520bias.%2520This%2520bias%2520is%2520pervasive%2520across%2520models%252C%2520resilient%2520to%2520test-time%2520scaling%252C%2520and%2520poses%2520risks%2520to%2520existing%2520methods%2520relying%2520on%2520MLLM%2520evaluations.%2520We%2520discuss%2520methods%2520to%2520evaluate%2520and%2520improve%2520MLLM%2520verifiers%2520and%2520introduce%2520Self-Grounded%2520Verification%2520%2528SGV%2529%252C%2520a%2520lightweight%2520method%2520that%2520harnesses%2520MLLMs%2527%2520own%2520sampling%2520mechanisms%2520by%2520modulating%2520%2528un%2529conditional%2520generation%2520to%2520better%2520leverage%2520their%2520knowledge%252C%2520alignment%252C%2520and%2520reasoning.%2520SGV%2520operates%2520in%2520two%2520steps%253A%2520first%252C%2520the%2520MLLM%2520is%2520elicited%2520to%2520generate%2520broad%2520priors%2520about%2520desired%2520behavior%252C%2520independent%2520of%2520the%2520data%2520under%2520evaluation.%2520Then%252C%2520conditioned%2520on%2520self-generated%2520priors%252C%2520it%2520reasons%2520over%2520and%2520evaluates%2520a%2520candidate%2520trajectory.%2520SGV%2520yields%2520more%2520human-aligned%2520evaluations%2520with%2520gains%2520of%2520up%2520to%252025pp%2520in%2520failure%2520detection%252C%252014pp%2520in%2520accuracy%252C%2520and%2520benefits%2520extending%2520to%2520downstream%2520applications.%2520In%2520self-refinement%2520and%2520online%2520supervision%252C%2520SGV%2520boosts%2520task%2520completion%2520of%2520a%2520GUI%2520specialist%2520in%2520OSWorld%252C%2520a%2520diffusion%2520policy%2520in%2520robomimic%252C%2520and%2520a%2520ReAct%2520agent%2520in%2520VisualWebArena--setting%2520a%2520new%2520state%2520of%2520the%2520art%252C%2520surpassing%2520the%2520previous%2520best%2520by%252020pp.%2520We%2520release%2520an%2520updated%2520version%2520of%2520VisualWebArena%2520featuring%2520more%2520human-aligned%2520evaluators%252C%2520high-fidelity%2520environment%2520parallelism%252C%2520and%2520speedups%2520of%2520over%252010x.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11662v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%27s%20Think%20in%20Two%20Steps%3A%20Mitigating%20Agreement%20Bias%20in%20MLLMs%20with%20Self-Grounded%20Verification&entry.906535625=Moises%20Andrade%20and%20Joonhyuk%20Cha%20and%20Brandon%20Ho%20and%20Vriksha%20Srihari%20and%20Karmesh%20Yadav%20and%20Zsolt%20Kira&entry.1292438233=Verifiers--functions%20assigning%20rewards%20to%20agent%20behavior--have%20been%20key%20for%20AI%20progress%20in%20domains%20like%20math%20and%20code.%20However%2C%20extending%20gains%20to%20domains%20without%20clear-cut%20success%20criteria%20%28e.g.%2C%20computer%20use%29%20remains%20a%20challenge%3A%20while%20humans%20can%20recognize%20desired%20outcomes%2C%20translating%20this%20intuition%20into%20scalable%20rules%20is%20nontrivial.%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20emerge%20as%20a%20promising%20solution%2C%20given%20their%20world%20knowledge%2C%20human-preference%20alignment%2C%20and%20reasoning%20skills.%20We%20evaluate%20MLLMs%20as%20verifiers%20across%20web%20navigation%2C%20computer%20use%2C%20and%20robotic%20manipulation%2C%20and%20identify%20a%20critical%20limitation%3A%20a%20strong%20tendency%20to%20over-validate%20agent%20behavior%2C%20a%20phenomenon%20we%20term%20agreement%20bias.%20This%20bias%20is%20pervasive%20across%20models%2C%20resilient%20to%20test-time%20scaling%2C%20and%20poses%20risks%20to%20existing%20methods%20relying%20on%20MLLM%20evaluations.%20We%20discuss%20methods%20to%20evaluate%20and%20improve%20MLLM%20verifiers%20and%20introduce%20Self-Grounded%20Verification%20%28SGV%29%2C%20a%20lightweight%20method%20that%20harnesses%20MLLMs%27%20own%20sampling%20mechanisms%20by%20modulating%20%28un%29conditional%20generation%20to%20better%20leverage%20their%20knowledge%2C%20alignment%2C%20and%20reasoning.%20SGV%20operates%20in%20two%20steps%3A%20first%2C%20the%20MLLM%20is%20elicited%20to%20generate%20broad%20priors%20about%20desired%20behavior%2C%20independent%20of%20the%20data%20under%20evaluation.%20Then%2C%20conditioned%20on%20self-generated%20priors%2C%20it%20reasons%20over%20and%20evaluates%20a%20candidate%20trajectory.%20SGV%20yields%20more%20human-aligned%20evaluations%20with%20gains%20of%20up%20to%2025pp%20in%20failure%20detection%2C%2014pp%20in%20accuracy%2C%20and%20benefits%20extending%20to%20downstream%20applications.%20In%20self-refinement%20and%20online%20supervision%2C%20SGV%20boosts%20task%20completion%20of%20a%20GUI%20specialist%20in%20OSWorld%2C%20a%20diffusion%20policy%20in%20robomimic%2C%20and%20a%20ReAct%20agent%20in%20VisualWebArena--setting%20a%20new%20state%20of%20the%20art%2C%20surpassing%20the%20previous%20best%20by%2020pp.%20We%20release%20an%20updated%20version%20of%20VisualWebArena%20featuring%20more%20human-aligned%20evaluators%2C%20high-fidelity%20environment%20parallelism%2C%20and%20speedups%20of%20over%2010x.&entry.1838667208=http%3A//arxiv.org/abs/2507.11662v2&entry.124074799=Read"},
{"title": "The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection", "author": "Qingdong He and Xueqin Chen and Yanjie Pan and Peng Tang and Pengcheng Xu and Zhenye Gan and Chengjie Wang and Xiaobin Hu and Jiangning Zhang and Yabiao Wang", "abstract": "Although diffusion transformer (DiT)-based video virtual try-on (VVT) has made significant progress in synthesizing realistic videos, existing methods still struggle to capture fine-grained garment dynamics and preserve background integrity across video frames. They also incur high computational costs due to additional interaction modules introduced into DiTs, while the limited scale and quality of existing public datasets also restrict model generalization and effective training. To address these challenges, we propose a novel framework, KeyTailor, along with a large-scale, high-definition dataset, ViT-HD. The core idea of KeyTailor is a keyframe-driven details injection strategy, motivated by the fact that keyframes inherently contain both foreground dynamics and background consistency. Specifically, KeyTailor adopts an instruction-guided keyframe sampling strategy to filter informative frames from the input video. Subsequently,two tailored keyframe-driven modules, the garment details enhancement module and the collaborative background optimization module, are employed to distill garment dynamics into garment-related latents and to optimize the integrity of background latents, both guided by keyframes.These enriched details are then injected into standard DiT blocks together with pose, mask, and noise latents, enabling efficient and realistic try-on video synthesis. This design ensures consistency without explicitly modifying the DiT architecture, while simultaneously avoiding additional complexity. In addition, our dataset ViT-HD comprises 15, 070 high-quality video samples at a resolution of 810*1080, covering diverse garments. Extensive experiments demonstrate that KeyTailor outperforms state-of-the-art baselines in terms of garment fidelity and background integrity across both dynamic and static scenarios.", "link": "http://arxiv.org/abs/2512.20340v1", "date": "2025-12-23", "relevancy": 1.8491, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6238}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6168}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20devil%20is%20in%20the%20details%3A%20Enhancing%20Video%20Virtual%20Try-On%20via%20Keyframe-Driven%20Details%20Injection&body=Title%3A%20The%20devil%20is%20in%20the%20details%3A%20Enhancing%20Video%20Virtual%20Try-On%20via%20Keyframe-Driven%20Details%20Injection%0AAuthor%3A%20Qingdong%20He%20and%20Xueqin%20Chen%20and%20Yanjie%20Pan%20and%20Peng%20Tang%20and%20Pengcheng%20Xu%20and%20Zhenye%20Gan%20and%20Chengjie%20Wang%20and%20Xiaobin%20Hu%20and%20Jiangning%20Zhang%20and%20Yabiao%20Wang%0AAbstract%3A%20Although%20diffusion%20transformer%20%28DiT%29-based%20video%20virtual%20try-on%20%28VVT%29%20has%20made%20significant%20progress%20in%20synthesizing%20realistic%20videos%2C%20existing%20methods%20still%20struggle%20to%20capture%20fine-grained%20garment%20dynamics%20and%20preserve%20background%20integrity%20across%20video%20frames.%20They%20also%20incur%20high%20computational%20costs%20due%20to%20additional%20interaction%20modules%20introduced%20into%20DiTs%2C%20while%20the%20limited%20scale%20and%20quality%20of%20existing%20public%20datasets%20also%20restrict%20model%20generalization%20and%20effective%20training.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20framework%2C%20KeyTailor%2C%20along%20with%20a%20large-scale%2C%20high-definition%20dataset%2C%20ViT-HD.%20The%20core%20idea%20of%20KeyTailor%20is%20a%20keyframe-driven%20details%20injection%20strategy%2C%20motivated%20by%20the%20fact%20that%20keyframes%20inherently%20contain%20both%20foreground%20dynamics%20and%20background%20consistency.%20Specifically%2C%20KeyTailor%20adopts%20an%20instruction-guided%20keyframe%20sampling%20strategy%20to%20filter%20informative%20frames%20from%20the%20input%20video.%20Subsequently%2Ctwo%20tailored%20keyframe-driven%20modules%2C%20the%20garment%20details%20enhancement%20module%20and%20the%20collaborative%20background%20optimization%20module%2C%20are%20employed%20to%20distill%20garment%20dynamics%20into%20garment-related%20latents%20and%20to%20optimize%20the%20integrity%20of%20background%20latents%2C%20both%20guided%20by%20keyframes.These%20enriched%20details%20are%20then%20injected%20into%20standard%20DiT%20blocks%20together%20with%20pose%2C%20mask%2C%20and%20noise%20latents%2C%20enabling%20efficient%20and%20realistic%20try-on%20video%20synthesis.%20This%20design%20ensures%20consistency%20without%20explicitly%20modifying%20the%20DiT%20architecture%2C%20while%20simultaneously%20avoiding%20additional%20complexity.%20In%20addition%2C%20our%20dataset%20ViT-HD%20comprises%2015%2C%20070%20high-quality%20video%20samples%20at%20a%20resolution%20of%20810%2A1080%2C%20covering%20diverse%20garments.%20Extensive%20experiments%20demonstrate%20that%20KeyTailor%20outperforms%20state-of-the-art%20baselines%20in%20terms%20of%20garment%20fidelity%20and%20background%20integrity%20across%20both%20dynamic%20and%20static%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520devil%2520is%2520in%2520the%2520details%253A%2520Enhancing%2520Video%2520Virtual%2520Try-On%2520via%2520Keyframe-Driven%2520Details%2520Injection%26entry.906535625%3DQingdong%2520He%2520and%2520Xueqin%2520Chen%2520and%2520Yanjie%2520Pan%2520and%2520Peng%2520Tang%2520and%2520Pengcheng%2520Xu%2520and%2520Zhenye%2520Gan%2520and%2520Chengjie%2520Wang%2520and%2520Xiaobin%2520Hu%2520and%2520Jiangning%2520Zhang%2520and%2520Yabiao%2520Wang%26entry.1292438233%3DAlthough%2520diffusion%2520transformer%2520%2528DiT%2529-based%2520video%2520virtual%2520try-on%2520%2528VVT%2529%2520has%2520made%2520significant%2520progress%2520in%2520synthesizing%2520realistic%2520videos%252C%2520existing%2520methods%2520still%2520struggle%2520to%2520capture%2520fine-grained%2520garment%2520dynamics%2520and%2520preserve%2520background%2520integrity%2520across%2520video%2520frames.%2520They%2520also%2520incur%2520high%2520computational%2520costs%2520due%2520to%2520additional%2520interaction%2520modules%2520introduced%2520into%2520DiTs%252C%2520while%2520the%2520limited%2520scale%2520and%2520quality%2520of%2520existing%2520public%2520datasets%2520also%2520restrict%2520model%2520generalization%2520and%2520effective%2520training.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520KeyTailor%252C%2520along%2520with%2520a%2520large-scale%252C%2520high-definition%2520dataset%252C%2520ViT-HD.%2520The%2520core%2520idea%2520of%2520KeyTailor%2520is%2520a%2520keyframe-driven%2520details%2520injection%2520strategy%252C%2520motivated%2520by%2520the%2520fact%2520that%2520keyframes%2520inherently%2520contain%2520both%2520foreground%2520dynamics%2520and%2520background%2520consistency.%2520Specifically%252C%2520KeyTailor%2520adopts%2520an%2520instruction-guided%2520keyframe%2520sampling%2520strategy%2520to%2520filter%2520informative%2520frames%2520from%2520the%2520input%2520video.%2520Subsequently%252Ctwo%2520tailored%2520keyframe-driven%2520modules%252C%2520the%2520garment%2520details%2520enhancement%2520module%2520and%2520the%2520collaborative%2520background%2520optimization%2520module%252C%2520are%2520employed%2520to%2520distill%2520garment%2520dynamics%2520into%2520garment-related%2520latents%2520and%2520to%2520optimize%2520the%2520integrity%2520of%2520background%2520latents%252C%2520both%2520guided%2520by%2520keyframes.These%2520enriched%2520details%2520are%2520then%2520injected%2520into%2520standard%2520DiT%2520blocks%2520together%2520with%2520pose%252C%2520mask%252C%2520and%2520noise%2520latents%252C%2520enabling%2520efficient%2520and%2520realistic%2520try-on%2520video%2520synthesis.%2520This%2520design%2520ensures%2520consistency%2520without%2520explicitly%2520modifying%2520the%2520DiT%2520architecture%252C%2520while%2520simultaneously%2520avoiding%2520additional%2520complexity.%2520In%2520addition%252C%2520our%2520dataset%2520ViT-HD%2520comprises%252015%252C%2520070%2520high-quality%2520video%2520samples%2520at%2520a%2520resolution%2520of%2520810%252A1080%252C%2520covering%2520diverse%2520garments.%2520Extensive%2520experiments%2520demonstrate%2520that%2520KeyTailor%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520terms%2520of%2520garment%2520fidelity%2520and%2520background%2520integrity%2520across%2520both%2520dynamic%2520and%2520static%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20devil%20is%20in%20the%20details%3A%20Enhancing%20Video%20Virtual%20Try-On%20via%20Keyframe-Driven%20Details%20Injection&entry.906535625=Qingdong%20He%20and%20Xueqin%20Chen%20and%20Yanjie%20Pan%20and%20Peng%20Tang%20and%20Pengcheng%20Xu%20and%20Zhenye%20Gan%20and%20Chengjie%20Wang%20and%20Xiaobin%20Hu%20and%20Jiangning%20Zhang%20and%20Yabiao%20Wang&entry.1292438233=Although%20diffusion%20transformer%20%28DiT%29-based%20video%20virtual%20try-on%20%28VVT%29%20has%20made%20significant%20progress%20in%20synthesizing%20realistic%20videos%2C%20existing%20methods%20still%20struggle%20to%20capture%20fine-grained%20garment%20dynamics%20and%20preserve%20background%20integrity%20across%20video%20frames.%20They%20also%20incur%20high%20computational%20costs%20due%20to%20additional%20interaction%20modules%20introduced%20into%20DiTs%2C%20while%20the%20limited%20scale%20and%20quality%20of%20existing%20public%20datasets%20also%20restrict%20model%20generalization%20and%20effective%20training.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20framework%2C%20KeyTailor%2C%20along%20with%20a%20large-scale%2C%20high-definition%20dataset%2C%20ViT-HD.%20The%20core%20idea%20of%20KeyTailor%20is%20a%20keyframe-driven%20details%20injection%20strategy%2C%20motivated%20by%20the%20fact%20that%20keyframes%20inherently%20contain%20both%20foreground%20dynamics%20and%20background%20consistency.%20Specifically%2C%20KeyTailor%20adopts%20an%20instruction-guided%20keyframe%20sampling%20strategy%20to%20filter%20informative%20frames%20from%20the%20input%20video.%20Subsequently%2Ctwo%20tailored%20keyframe-driven%20modules%2C%20the%20garment%20details%20enhancement%20module%20and%20the%20collaborative%20background%20optimization%20module%2C%20are%20employed%20to%20distill%20garment%20dynamics%20into%20garment-related%20latents%20and%20to%20optimize%20the%20integrity%20of%20background%20latents%2C%20both%20guided%20by%20keyframes.These%20enriched%20details%20are%20then%20injected%20into%20standard%20DiT%20blocks%20together%20with%20pose%2C%20mask%2C%20and%20noise%20latents%2C%20enabling%20efficient%20and%20realistic%20try-on%20video%20synthesis.%20This%20design%20ensures%20consistency%20without%20explicitly%20modifying%20the%20DiT%20architecture%2C%20while%20simultaneously%20avoiding%20additional%20complexity.%20In%20addition%2C%20our%20dataset%20ViT-HD%20comprises%2015%2C%20070%20high-quality%20video%20samples%20at%20a%20resolution%20of%20810%2A1080%2C%20covering%20diverse%20garments.%20Extensive%20experiments%20demonstrate%20that%20KeyTailor%20outperforms%20state-of-the-art%20baselines%20in%20terms%20of%20garment%20fidelity%20and%20background%20integrity%20across%20both%20dynamic%20and%20static%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.20340v1&entry.124074799=Read"},
{"title": "Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study", "author": "Carla Crivoi and Radu Tudor Ionescu", "abstract": "We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.", "link": "http://arxiv.org/abs/2512.19253v2", "date": "2025-12-23", "relevancy": 1.3677, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4821}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Unlearning%20in%20the%20Era%20of%20Quantum%20Machine%20Learning%3A%20An%20Empirical%20Study&body=Title%3A%20Machine%20Unlearning%20in%20the%20Era%20of%20Quantum%20Machine%20Learning%3A%20An%20Empirical%20Study%0AAuthor%3A%20Carla%20Crivoi%20and%20Radu%20Tudor%20Ionescu%0AAbstract%3A%20We%20present%20the%20first%20comprehensive%20empirical%20study%20of%20machine%20unlearning%20%28MU%29%20in%20hybrid%20quantum-classical%20neural%20networks.%20While%20MU%20has%20been%20extensively%20explored%20in%20classical%20deep%20learning%2C%20its%20behavior%20within%20variational%20quantum%20circuits%20%28VQCs%29%20and%20quantum-augmented%20architectures%20remains%20largely%20unexplored.%20First%2C%20we%20adapt%20a%20broad%20suite%20of%20unlearning%20methods%20to%20quantum%20settings%2C%20including%20gradient-based%2C%20distillation-based%2C%20regularization-based%20and%20certified%20techniques.%20Second%2C%20we%20introduce%20two%20new%20unlearning%20strategies%20tailored%20to%20hybrid%20models.%20Experiments%20across%20Iris%2C%20MNIST%2C%20and%20Fashion-MNIST%2C%20under%20both%20subset%20removal%20and%20full-class%20deletion%2C%20reveal%20that%20quantum%20models%20can%20support%20effective%20unlearning%2C%20but%20outcomes%20depend%20strongly%20on%20circuit%20depth%2C%20entanglement%20structure%2C%20and%20task%20complexity.%20Shallow%20VQCs%20display%20high%20intrinsic%20stability%20with%20minimal%20memorization%2C%20whereas%20deeper%20hybrid%20models%20exhibit%20stronger%20trade-offs%20between%20utility%2C%20forgetting%20strength%2C%20and%20alignment%20with%20retrain%20oracle.%20We%20find%20that%20certain%20methods%2C%20e.g.%20EU-k%2C%20LCA%2C%20and%20Certified%20Unlearning%2C%20consistently%20provide%20the%20best%20balance%20across%20metrics.%20These%20findings%20establish%20baseline%20empirical%20insights%20into%20quantum%20machine%20unlearning%20and%20highlight%20the%20need%20for%20quantum-aware%20algorithms%20and%20theoretical%20guarantees%2C%20as%20quantum%20machine%20learning%20systems%20continue%20to%20expand%20in%20scale%20and%20capability.%20We%20publicly%20release%20our%20code%20at%3A%20https%3A//github.com/CrivoiCarla/HQML.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19253v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Unlearning%2520in%2520the%2520Era%2520of%2520Quantum%2520Machine%2520Learning%253A%2520An%2520Empirical%2520Study%26entry.906535625%3DCarla%2520Crivoi%2520and%2520Radu%2520Tudor%2520Ionescu%26entry.1292438233%3DWe%2520present%2520the%2520first%2520comprehensive%2520empirical%2520study%2520of%2520machine%2520unlearning%2520%2528MU%2529%2520in%2520hybrid%2520quantum-classical%2520neural%2520networks.%2520While%2520MU%2520has%2520been%2520extensively%2520explored%2520in%2520classical%2520deep%2520learning%252C%2520its%2520behavior%2520within%2520variational%2520quantum%2520circuits%2520%2528VQCs%2529%2520and%2520quantum-augmented%2520architectures%2520remains%2520largely%2520unexplored.%2520First%252C%2520we%2520adapt%2520a%2520broad%2520suite%2520of%2520unlearning%2520methods%2520to%2520quantum%2520settings%252C%2520including%2520gradient-based%252C%2520distillation-based%252C%2520regularization-based%2520and%2520certified%2520techniques.%2520Second%252C%2520we%2520introduce%2520two%2520new%2520unlearning%2520strategies%2520tailored%2520to%2520hybrid%2520models.%2520Experiments%2520across%2520Iris%252C%2520MNIST%252C%2520and%2520Fashion-MNIST%252C%2520under%2520both%2520subset%2520removal%2520and%2520full-class%2520deletion%252C%2520reveal%2520that%2520quantum%2520models%2520can%2520support%2520effective%2520unlearning%252C%2520but%2520outcomes%2520depend%2520strongly%2520on%2520circuit%2520depth%252C%2520entanglement%2520structure%252C%2520and%2520task%2520complexity.%2520Shallow%2520VQCs%2520display%2520high%2520intrinsic%2520stability%2520with%2520minimal%2520memorization%252C%2520whereas%2520deeper%2520hybrid%2520models%2520exhibit%2520stronger%2520trade-offs%2520between%2520utility%252C%2520forgetting%2520strength%252C%2520and%2520alignment%2520with%2520retrain%2520oracle.%2520We%2520find%2520that%2520certain%2520methods%252C%2520e.g.%2520EU-k%252C%2520LCA%252C%2520and%2520Certified%2520Unlearning%252C%2520consistently%2520provide%2520the%2520best%2520balance%2520across%2520metrics.%2520These%2520findings%2520establish%2520baseline%2520empirical%2520insights%2520into%2520quantum%2520machine%2520unlearning%2520and%2520highlight%2520the%2520need%2520for%2520quantum-aware%2520algorithms%2520and%2520theoretical%2520guarantees%252C%2520as%2520quantum%2520machine%2520learning%2520systems%2520continue%2520to%2520expand%2520in%2520scale%2520and%2520capability.%2520We%2520publicly%2520release%2520our%2520code%2520at%253A%2520https%253A//github.com/CrivoiCarla/HQML.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19253v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Unlearning%20in%20the%20Era%20of%20Quantum%20Machine%20Learning%3A%20An%20Empirical%20Study&entry.906535625=Carla%20Crivoi%20and%20Radu%20Tudor%20Ionescu&entry.1292438233=We%20present%20the%20first%20comprehensive%20empirical%20study%20of%20machine%20unlearning%20%28MU%29%20in%20hybrid%20quantum-classical%20neural%20networks.%20While%20MU%20has%20been%20extensively%20explored%20in%20classical%20deep%20learning%2C%20its%20behavior%20within%20variational%20quantum%20circuits%20%28VQCs%29%20and%20quantum-augmented%20architectures%20remains%20largely%20unexplored.%20First%2C%20we%20adapt%20a%20broad%20suite%20of%20unlearning%20methods%20to%20quantum%20settings%2C%20including%20gradient-based%2C%20distillation-based%2C%20regularization-based%20and%20certified%20techniques.%20Second%2C%20we%20introduce%20two%20new%20unlearning%20strategies%20tailored%20to%20hybrid%20models.%20Experiments%20across%20Iris%2C%20MNIST%2C%20and%20Fashion-MNIST%2C%20under%20both%20subset%20removal%20and%20full-class%20deletion%2C%20reveal%20that%20quantum%20models%20can%20support%20effective%20unlearning%2C%20but%20outcomes%20depend%20strongly%20on%20circuit%20depth%2C%20entanglement%20structure%2C%20and%20task%20complexity.%20Shallow%20VQCs%20display%20high%20intrinsic%20stability%20with%20minimal%20memorization%2C%20whereas%20deeper%20hybrid%20models%20exhibit%20stronger%20trade-offs%20between%20utility%2C%20forgetting%20strength%2C%20and%20alignment%20with%20retrain%20oracle.%20We%20find%20that%20certain%20methods%2C%20e.g.%20EU-k%2C%20LCA%2C%20and%20Certified%20Unlearning%2C%20consistently%20provide%20the%20best%20balance%20across%20metrics.%20These%20findings%20establish%20baseline%20empirical%20insights%20into%20quantum%20machine%20unlearning%20and%20highlight%20the%20need%20for%20quantum-aware%20algorithms%20and%20theoretical%20guarantees%2C%20as%20quantum%20machine%20learning%20systems%20continue%20to%20expand%20in%20scale%20and%20capability.%20We%20publicly%20release%20our%20code%20at%3A%20https%3A//github.com/CrivoiCarla/HQML.&entry.1838667208=http%3A//arxiv.org/abs/2512.19253v2&entry.124074799=Read"},
{"title": "Relu and softplus neural nets as zero-sum turn-based games", "author": "Stephane Gaubert and Yiannis Vlassopoulos", "abstract": "We show that the output of a ReLU neural network can be interpreted as the value of a zero-sum, turn-based, stopping game, which we call the ReLU net game. The game runs in the direction opposite to that of the network, and the input of the network serves as the terminal reward of the game. In fact, evaluating the network is the same as running the Shapley-Bellman backward recursion for the value of the game. Using the expression of the value of the game as an expected total payoff with respect to the path measure induced by the transition probabilities and a pair of optimal policies, we derive a discrete Feynman-Kac-type path-integral formula for the network output. This game-theoretic representation can be used to derive bounds on the output from bounds on the input, leveraging the monotonicity of Shapley operators, and to verify robustness properties using policies as certificates. Moreover, training the neural network becomes an inverse game problem: given pairs of terminal rewards and corresponding values, one seeks transition probabilities and rewards of a game that reproduces them. Finally, we show that a similar approach applies to neural networks with Softplus activation functions, where the ReLU net game is replaced by its entropic regularization.", "link": "http://arxiv.org/abs/2512.20582v1", "date": "2025-12-23", "relevancy": 1.2834, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4477}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.411}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relu%20and%20softplus%20neural%20nets%20as%20zero-sum%20turn-based%20games&body=Title%3A%20Relu%20and%20softplus%20neural%20nets%20as%20zero-sum%20turn-based%20games%0AAuthor%3A%20Stephane%20Gaubert%20and%20Yiannis%20Vlassopoulos%0AAbstract%3A%20We%20show%20that%20the%20output%20of%20a%20ReLU%20neural%20network%20can%20be%20interpreted%20as%20the%20value%20of%20a%20zero-sum%2C%20turn-based%2C%20stopping%20game%2C%20which%20we%20call%20the%20ReLU%20net%20game.%20The%20game%20runs%20in%20the%20direction%20opposite%20to%20that%20of%20the%20network%2C%20and%20the%20input%20of%20the%20network%20serves%20as%20the%20terminal%20reward%20of%20the%20game.%20In%20fact%2C%20evaluating%20the%20network%20is%20the%20same%20as%20running%20the%20Shapley-Bellman%20backward%20recursion%20for%20the%20value%20of%20the%20game.%20Using%20the%20expression%20of%20the%20value%20of%20the%20game%20as%20an%20expected%20total%20payoff%20with%20respect%20to%20the%20path%20measure%20induced%20by%20the%20transition%20probabilities%20and%20a%20pair%20of%20optimal%20policies%2C%20we%20derive%20a%20discrete%20Feynman-Kac-type%20path-integral%20formula%20for%20the%20network%20output.%20This%20game-theoretic%20representation%20can%20be%20used%20to%20derive%20bounds%20on%20the%20output%20from%20bounds%20on%20the%20input%2C%20leveraging%20the%20monotonicity%20of%20Shapley%20operators%2C%20and%20to%20verify%20robustness%20properties%20using%20policies%20as%20certificates.%20Moreover%2C%20training%20the%20neural%20network%20becomes%20an%20inverse%20game%20problem%3A%20given%20pairs%20of%20terminal%20rewards%20and%20corresponding%20values%2C%20one%20seeks%20transition%20probabilities%20and%20rewards%20of%20a%20game%20that%20reproduces%20them.%20Finally%2C%20we%20show%20that%20a%20similar%20approach%20applies%20to%20neural%20networks%20with%20Softplus%20activation%20functions%2C%20where%20the%20ReLU%20net%20game%20is%20replaced%20by%20its%20entropic%20regularization.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelu%2520and%2520softplus%2520neural%2520nets%2520as%2520zero-sum%2520turn-based%2520games%26entry.906535625%3DStephane%2520Gaubert%2520and%2520Yiannis%2520Vlassopoulos%26entry.1292438233%3DWe%2520show%2520that%2520the%2520output%2520of%2520a%2520ReLU%2520neural%2520network%2520can%2520be%2520interpreted%2520as%2520the%2520value%2520of%2520a%2520zero-sum%252C%2520turn-based%252C%2520stopping%2520game%252C%2520which%2520we%2520call%2520the%2520ReLU%2520net%2520game.%2520The%2520game%2520runs%2520in%2520the%2520direction%2520opposite%2520to%2520that%2520of%2520the%2520network%252C%2520and%2520the%2520input%2520of%2520the%2520network%2520serves%2520as%2520the%2520terminal%2520reward%2520of%2520the%2520game.%2520In%2520fact%252C%2520evaluating%2520the%2520network%2520is%2520the%2520same%2520as%2520running%2520the%2520Shapley-Bellman%2520backward%2520recursion%2520for%2520the%2520value%2520of%2520the%2520game.%2520Using%2520the%2520expression%2520of%2520the%2520value%2520of%2520the%2520game%2520as%2520an%2520expected%2520total%2520payoff%2520with%2520respect%2520to%2520the%2520path%2520measure%2520induced%2520by%2520the%2520transition%2520probabilities%2520and%2520a%2520pair%2520of%2520optimal%2520policies%252C%2520we%2520derive%2520a%2520discrete%2520Feynman-Kac-type%2520path-integral%2520formula%2520for%2520the%2520network%2520output.%2520This%2520game-theoretic%2520representation%2520can%2520be%2520used%2520to%2520derive%2520bounds%2520on%2520the%2520output%2520from%2520bounds%2520on%2520the%2520input%252C%2520leveraging%2520the%2520monotonicity%2520of%2520Shapley%2520operators%252C%2520and%2520to%2520verify%2520robustness%2520properties%2520using%2520policies%2520as%2520certificates.%2520Moreover%252C%2520training%2520the%2520neural%2520network%2520becomes%2520an%2520inverse%2520game%2520problem%253A%2520given%2520pairs%2520of%2520terminal%2520rewards%2520and%2520corresponding%2520values%252C%2520one%2520seeks%2520transition%2520probabilities%2520and%2520rewards%2520of%2520a%2520game%2520that%2520reproduces%2520them.%2520Finally%252C%2520we%2520show%2520that%2520a%2520similar%2520approach%2520applies%2520to%2520neural%2520networks%2520with%2520Softplus%2520activation%2520functions%252C%2520where%2520the%2520ReLU%2520net%2520game%2520is%2520replaced%2520by%2520its%2520entropic%2520regularization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relu%20and%20softplus%20neural%20nets%20as%20zero-sum%20turn-based%20games&entry.906535625=Stephane%20Gaubert%20and%20Yiannis%20Vlassopoulos&entry.1292438233=We%20show%20that%20the%20output%20of%20a%20ReLU%20neural%20network%20can%20be%20interpreted%20as%20the%20value%20of%20a%20zero-sum%2C%20turn-based%2C%20stopping%20game%2C%20which%20we%20call%20the%20ReLU%20net%20game.%20The%20game%20runs%20in%20the%20direction%20opposite%20to%20that%20of%20the%20network%2C%20and%20the%20input%20of%20the%20network%20serves%20as%20the%20terminal%20reward%20of%20the%20game.%20In%20fact%2C%20evaluating%20the%20network%20is%20the%20same%20as%20running%20the%20Shapley-Bellman%20backward%20recursion%20for%20the%20value%20of%20the%20game.%20Using%20the%20expression%20of%20the%20value%20of%20the%20game%20as%20an%20expected%20total%20payoff%20with%20respect%20to%20the%20path%20measure%20induced%20by%20the%20transition%20probabilities%20and%20a%20pair%20of%20optimal%20policies%2C%20we%20derive%20a%20discrete%20Feynman-Kac-type%20path-integral%20formula%20for%20the%20network%20output.%20This%20game-theoretic%20representation%20can%20be%20used%20to%20derive%20bounds%20on%20the%20output%20from%20bounds%20on%20the%20input%2C%20leveraging%20the%20monotonicity%20of%20Shapley%20operators%2C%20and%20to%20verify%20robustness%20properties%20using%20policies%20as%20certificates.%20Moreover%2C%20training%20the%20neural%20network%20becomes%20an%20inverse%20game%20problem%3A%20given%20pairs%20of%20terminal%20rewards%20and%20corresponding%20values%2C%20one%20seeks%20transition%20probabilities%20and%20rewards%20of%20a%20game%20that%20reproduces%20them.%20Finally%2C%20we%20show%20that%20a%20similar%20approach%20applies%20to%20neural%20networks%20with%20Softplus%20activation%20functions%2C%20where%20the%20ReLU%20net%20game%20is%20replaced%20by%20its%20entropic%20regularization.&entry.1838667208=http%3A//arxiv.org/abs/2512.20582v1&entry.124074799=Read"},
{"title": "VibrantLeaves: A principled parametric image generator for training deep restoration models", "author": "Raphael Achddou and Yann Gousseau and Sa\u00efd Ladjal and Sabine S\u00fcsstrunk", "abstract": "Even though Deep Neural Networks are extremely powerful for image restoration tasks, they have several limitations. They are poorly understood and suffer from strong biases inherited from the training sets. One way to address these shortcomings is to have a better control over the training sets, in particular by using synthetic sets. In this paper, we propose a synthetic image generator relying on a few simple principles. In particular, we focus on geometric modeling, textures, and a simple modeling of image acquisition. These properties, integrated in a classical Dead Leaves model, enable the creation of efficient training sets. Standard image denoising and super-resolution networks can be trained on such datasets, reaching performance almost on par with training on natural image datasets. As a first step towards explainability, we provide a careful analysis of the considered principles, identifying which image properties are necessary to obtain good performances. Besides, such training also yields better robustness to various geometric and radiometric perturbations of the test sets.", "link": "http://arxiv.org/abs/2504.10201v2", "date": "2025-12-23", "relevancy": 1.7378, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5892}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5724}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VibrantLeaves%3A%20A%20principled%20parametric%20image%20generator%20for%20training%20deep%20restoration%20models&body=Title%3A%20VibrantLeaves%3A%20A%20principled%20parametric%20image%20generator%20for%20training%20deep%20restoration%20models%0AAuthor%3A%20Raphael%20Achddou%20and%20Yann%20Gousseau%20and%20Sa%C3%AFd%20Ladjal%20and%20Sabine%20S%C3%BCsstrunk%0AAbstract%3A%20Even%20though%20Deep%20Neural%20Networks%20are%20extremely%20powerful%20for%20image%20restoration%20tasks%2C%20they%20have%20several%20limitations.%20They%20are%20poorly%20understood%20and%20suffer%20from%20strong%20biases%20inherited%20from%20the%20training%20sets.%20One%20way%20to%20address%20these%20shortcomings%20is%20to%20have%20a%20better%20control%20over%20the%20training%20sets%2C%20in%20particular%20by%20using%20synthetic%20sets.%20In%20this%20paper%2C%20we%20propose%20a%20synthetic%20image%20generator%20relying%20on%20a%20few%20simple%20principles.%20In%20particular%2C%20we%20focus%20on%20geometric%20modeling%2C%20textures%2C%20and%20a%20simple%20modeling%20of%20image%20acquisition.%20These%20properties%2C%20integrated%20in%20a%20classical%20Dead%20Leaves%20model%2C%20enable%20the%20creation%20of%20efficient%20training%20sets.%20Standard%20image%20denoising%20and%20super-resolution%20networks%20can%20be%20trained%20on%20such%20datasets%2C%20reaching%20performance%20almost%20on%20par%20with%20training%20on%20natural%20image%20datasets.%20As%20a%20first%20step%20towards%20explainability%2C%20we%20provide%20a%20careful%20analysis%20of%20the%20considered%20principles%2C%20identifying%20which%20image%20properties%20are%20necessary%20to%20obtain%20good%20performances.%20Besides%2C%20such%20training%20also%20yields%20better%20robustness%20to%20various%20geometric%20and%20radiometric%20perturbations%20of%20the%20test%20sets.%0ALink%3A%20http%3A//arxiv.org/abs/2504.10201v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVibrantLeaves%253A%2520A%2520principled%2520parametric%2520image%2520generator%2520for%2520training%2520deep%2520restoration%2520models%26entry.906535625%3DRaphael%2520Achddou%2520and%2520Yann%2520Gousseau%2520and%2520Sa%25C3%25AFd%2520Ladjal%2520and%2520Sabine%2520S%25C3%25BCsstrunk%26entry.1292438233%3DEven%2520though%2520Deep%2520Neural%2520Networks%2520are%2520extremely%2520powerful%2520for%2520image%2520restoration%2520tasks%252C%2520they%2520have%2520several%2520limitations.%2520They%2520are%2520poorly%2520understood%2520and%2520suffer%2520from%2520strong%2520biases%2520inherited%2520from%2520the%2520training%2520sets.%2520One%2520way%2520to%2520address%2520these%2520shortcomings%2520is%2520to%2520have%2520a%2520better%2520control%2520over%2520the%2520training%2520sets%252C%2520in%2520particular%2520by%2520using%2520synthetic%2520sets.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520synthetic%2520image%2520generator%2520relying%2520on%2520a%2520few%2520simple%2520principles.%2520In%2520particular%252C%2520we%2520focus%2520on%2520geometric%2520modeling%252C%2520textures%252C%2520and%2520a%2520simple%2520modeling%2520of%2520image%2520acquisition.%2520These%2520properties%252C%2520integrated%2520in%2520a%2520classical%2520Dead%2520Leaves%2520model%252C%2520enable%2520the%2520creation%2520of%2520efficient%2520training%2520sets.%2520Standard%2520image%2520denoising%2520and%2520super-resolution%2520networks%2520can%2520be%2520trained%2520on%2520such%2520datasets%252C%2520reaching%2520performance%2520almost%2520on%2520par%2520with%2520training%2520on%2520natural%2520image%2520datasets.%2520As%2520a%2520first%2520step%2520towards%2520explainability%252C%2520we%2520provide%2520a%2520careful%2520analysis%2520of%2520the%2520considered%2520principles%252C%2520identifying%2520which%2520image%2520properties%2520are%2520necessary%2520to%2520obtain%2520good%2520performances.%2520Besides%252C%2520such%2520training%2520also%2520yields%2520better%2520robustness%2520to%2520various%2520geometric%2520and%2520radiometric%2520perturbations%2520of%2520the%2520test%2520sets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10201v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VibrantLeaves%3A%20A%20principled%20parametric%20image%20generator%20for%20training%20deep%20restoration%20models&entry.906535625=Raphael%20Achddou%20and%20Yann%20Gousseau%20and%20Sa%C3%AFd%20Ladjal%20and%20Sabine%20S%C3%BCsstrunk&entry.1292438233=Even%20though%20Deep%20Neural%20Networks%20are%20extremely%20powerful%20for%20image%20restoration%20tasks%2C%20they%20have%20several%20limitations.%20They%20are%20poorly%20understood%20and%20suffer%20from%20strong%20biases%20inherited%20from%20the%20training%20sets.%20One%20way%20to%20address%20these%20shortcomings%20is%20to%20have%20a%20better%20control%20over%20the%20training%20sets%2C%20in%20particular%20by%20using%20synthetic%20sets.%20In%20this%20paper%2C%20we%20propose%20a%20synthetic%20image%20generator%20relying%20on%20a%20few%20simple%20principles.%20In%20particular%2C%20we%20focus%20on%20geometric%20modeling%2C%20textures%2C%20and%20a%20simple%20modeling%20of%20image%20acquisition.%20These%20properties%2C%20integrated%20in%20a%20classical%20Dead%20Leaves%20model%2C%20enable%20the%20creation%20of%20efficient%20training%20sets.%20Standard%20image%20denoising%20and%20super-resolution%20networks%20can%20be%20trained%20on%20such%20datasets%2C%20reaching%20performance%20almost%20on%20par%20with%20training%20on%20natural%20image%20datasets.%20As%20a%20first%20step%20towards%20explainability%2C%20we%20provide%20a%20careful%20analysis%20of%20the%20considered%20principles%2C%20identifying%20which%20image%20properties%20are%20necessary%20to%20obtain%20good%20performances.%20Besides%2C%20such%20training%20also%20yields%20better%20robustness%20to%20various%20geometric%20and%20radiometric%20perturbations%20of%20the%20test%20sets.&entry.1838667208=http%3A//arxiv.org/abs/2504.10201v2&entry.124074799=Read"},
{"title": "LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation", "author": "Daniele Cardullo and Simone Teglia and Irene Amerini", "abstract": "With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.", "link": "http://arxiv.org/abs/2512.20257v1", "date": "2025-12-23", "relevancy": 1.7809, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6213}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5939}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LADLE-MM%3A%20Limited%20Annotation%20based%20Detector%20with%20Learned%20Ensembles%20for%20Multimodal%20Misinformation&body=Title%3A%20LADLE-MM%3A%20Limited%20Annotation%20based%20Detector%20with%20Learned%20Ensembles%20for%20Multimodal%20Misinformation%0AAuthor%3A%20Daniele%20Cardullo%20and%20Simone%20Teglia%20and%20Irene%20Amerini%0AAbstract%3A%20With%20the%20rise%20of%20easily%20accessible%20tools%20for%20generating%20and%20manipulating%20multimedia%20content%2C%20realistic%20synthetic%20alterations%20to%20digital%20media%20have%20become%20a%20widespread%20threat%2C%20often%20involving%20manipulations%20across%20multiple%20modalities%20simultaneously.%20Recently%2C%20such%20techniques%20have%20been%20increasingly%20employed%20to%20distort%20narratives%20of%20important%20events%20and%20to%20spread%20misinformation%20on%20social%20media%2C%20prompting%20the%20development%20of%20misinformation%20detectors.%20In%20the%20context%20of%20misinformation%20conveyed%20through%20image-text%20pairs%2C%20several%20detection%20methods%20have%20been%20proposed.%20However%2C%20these%20approaches%20typically%20rely%20on%20computationally%20intensive%20architectures%20or%20require%20large%20amounts%20of%20annotated%20data.%20In%20this%20work%20we%20introduce%20LADLE-MM%3A%20Limited%20Annotation%20based%20Detector%20with%20Learned%20Ensembles%20for%20Multimodal%20Misinformation%2C%20a%20model-soup%20initialized%20multimodal%20misinformation%20detector%20designed%20to%20operate%20under%20a%20limited%20annotation%20setup%20and%20constrained%20training%20resources.%20LADLE-MM%20is%20composed%20of%20two%20unimodal%20branches%20and%20a%20third%20multimodal%20one%20that%20enhances%20image%20and%20text%20representations%20with%20additional%20multimodal%20embeddings%20extracted%20from%20BLIP%2C%20serving%20as%20fixed%20reference%20space.%20Despite%20using%2060.3%25%20fewer%20trainable%20parameters%20than%20previous%20state-of-the-art%20models%2C%20LADLE-MM%20achieves%20competitive%20performance%20on%20both%20binary%20and%20multi-label%20classification%20tasks%20on%20the%20DGM4%20benchmark%2C%20outperforming%20existing%20methods%20when%20trained%20without%20grounding%20annotations.%20Moreover%2C%20when%20evaluated%20on%20the%20VERITE%20dataset%2C%20LADLE-MM%20outperforms%20current%20state-of-the-art%20approaches%20that%20utilize%20more%20complex%20architectures%20involving%20Large%20Vision-Language-Models%2C%20demonstrating%20the%20effective%20generalization%20ability%20in%20an%20open-set%20setting%20and%20strong%20robustness%20to%20unimodal%20bias.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20257v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLADLE-MM%253A%2520Limited%2520Annotation%2520based%2520Detector%2520with%2520Learned%2520Ensembles%2520for%2520Multimodal%2520Misinformation%26entry.906535625%3DDaniele%2520Cardullo%2520and%2520Simone%2520Teglia%2520and%2520Irene%2520Amerini%26entry.1292438233%3DWith%2520the%2520rise%2520of%2520easily%2520accessible%2520tools%2520for%2520generating%2520and%2520manipulating%2520multimedia%2520content%252C%2520realistic%2520synthetic%2520alterations%2520to%2520digital%2520media%2520have%2520become%2520a%2520widespread%2520threat%252C%2520often%2520involving%2520manipulations%2520across%2520multiple%2520modalities%2520simultaneously.%2520Recently%252C%2520such%2520techniques%2520have%2520been%2520increasingly%2520employed%2520to%2520distort%2520narratives%2520of%2520important%2520events%2520and%2520to%2520spread%2520misinformation%2520on%2520social%2520media%252C%2520prompting%2520the%2520development%2520of%2520misinformation%2520detectors.%2520In%2520the%2520context%2520of%2520misinformation%2520conveyed%2520through%2520image-text%2520pairs%252C%2520several%2520detection%2520methods%2520have%2520been%2520proposed.%2520However%252C%2520these%2520approaches%2520typically%2520rely%2520on%2520computationally%2520intensive%2520architectures%2520or%2520require%2520large%2520amounts%2520of%2520annotated%2520data.%2520In%2520this%2520work%2520we%2520introduce%2520LADLE-MM%253A%2520Limited%2520Annotation%2520based%2520Detector%2520with%2520Learned%2520Ensembles%2520for%2520Multimodal%2520Misinformation%252C%2520a%2520model-soup%2520initialized%2520multimodal%2520misinformation%2520detector%2520designed%2520to%2520operate%2520under%2520a%2520limited%2520annotation%2520setup%2520and%2520constrained%2520training%2520resources.%2520LADLE-MM%2520is%2520composed%2520of%2520two%2520unimodal%2520branches%2520and%2520a%2520third%2520multimodal%2520one%2520that%2520enhances%2520image%2520and%2520text%2520representations%2520with%2520additional%2520multimodal%2520embeddings%2520extracted%2520from%2520BLIP%252C%2520serving%2520as%2520fixed%2520reference%2520space.%2520Despite%2520using%252060.3%2525%2520fewer%2520trainable%2520parameters%2520than%2520previous%2520state-of-the-art%2520models%252C%2520LADLE-MM%2520achieves%2520competitive%2520performance%2520on%2520both%2520binary%2520and%2520multi-label%2520classification%2520tasks%2520on%2520the%2520DGM4%2520benchmark%252C%2520outperforming%2520existing%2520methods%2520when%2520trained%2520without%2520grounding%2520annotations.%2520Moreover%252C%2520when%2520evaluated%2520on%2520the%2520VERITE%2520dataset%252C%2520LADLE-MM%2520outperforms%2520current%2520state-of-the-art%2520approaches%2520that%2520utilize%2520more%2520complex%2520architectures%2520involving%2520Large%2520Vision-Language-Models%252C%2520demonstrating%2520the%2520effective%2520generalization%2520ability%2520in%2520an%2520open-set%2520setting%2520and%2520strong%2520robustness%2520to%2520unimodal%2520bias.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20257v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LADLE-MM%3A%20Limited%20Annotation%20based%20Detector%20with%20Learned%20Ensembles%20for%20Multimodal%20Misinformation&entry.906535625=Daniele%20Cardullo%20and%20Simone%20Teglia%20and%20Irene%20Amerini&entry.1292438233=With%20the%20rise%20of%20easily%20accessible%20tools%20for%20generating%20and%20manipulating%20multimedia%20content%2C%20realistic%20synthetic%20alterations%20to%20digital%20media%20have%20become%20a%20widespread%20threat%2C%20often%20involving%20manipulations%20across%20multiple%20modalities%20simultaneously.%20Recently%2C%20such%20techniques%20have%20been%20increasingly%20employed%20to%20distort%20narratives%20of%20important%20events%20and%20to%20spread%20misinformation%20on%20social%20media%2C%20prompting%20the%20development%20of%20misinformation%20detectors.%20In%20the%20context%20of%20misinformation%20conveyed%20through%20image-text%20pairs%2C%20several%20detection%20methods%20have%20been%20proposed.%20However%2C%20these%20approaches%20typically%20rely%20on%20computationally%20intensive%20architectures%20or%20require%20large%20amounts%20of%20annotated%20data.%20In%20this%20work%20we%20introduce%20LADLE-MM%3A%20Limited%20Annotation%20based%20Detector%20with%20Learned%20Ensembles%20for%20Multimodal%20Misinformation%2C%20a%20model-soup%20initialized%20multimodal%20misinformation%20detector%20designed%20to%20operate%20under%20a%20limited%20annotation%20setup%20and%20constrained%20training%20resources.%20LADLE-MM%20is%20composed%20of%20two%20unimodal%20branches%20and%20a%20third%20multimodal%20one%20that%20enhances%20image%20and%20text%20representations%20with%20additional%20multimodal%20embeddings%20extracted%20from%20BLIP%2C%20serving%20as%20fixed%20reference%20space.%20Despite%20using%2060.3%25%20fewer%20trainable%20parameters%20than%20previous%20state-of-the-art%20models%2C%20LADLE-MM%20achieves%20competitive%20performance%20on%20both%20binary%20and%20multi-label%20classification%20tasks%20on%20the%20DGM4%20benchmark%2C%20outperforming%20existing%20methods%20when%20trained%20without%20grounding%20annotations.%20Moreover%2C%20when%20evaluated%20on%20the%20VERITE%20dataset%2C%20LADLE-MM%20outperforms%20current%20state-of-the-art%20approaches%20that%20utilize%20more%20complex%20architectures%20involving%20Large%20Vision-Language-Models%2C%20demonstrating%20the%20effective%20generalization%20ability%20in%20an%20open-set%20setting%20and%20strong%20robustness%20to%20unimodal%20bias.&entry.1838667208=http%3A//arxiv.org/abs/2512.20257v1&entry.124074799=Read"},
{"title": "Design and Modeling of a Simple-Structured Continuously Variable Transmission Utilizing Shape Memory Alloy Superelasticity for Twisted String Actuator", "author": "Chanchan Xu and Shuai Dong and Xiaojie Wang", "abstract": "Twisted String Actuators (TSAs) are widely used in robotics but suffer from a limited range of Transmission Ratio (TR) variation, restricting their efficiency under varying loads.To overcome this, we propose a novel lightweight, simple-structured Continuously Variable Transmission (CVT) mechanism for TSA utilizing Shape Memory Alloy (SMA) superelasticity. The CVT mechanism consists solely of a pair of highly lightweight superelastic SMA rods connecting the ends of twisted strings. These rods deform under external loads, adjusting the inter-string distance to enable continuous TR variation.We develop a comprehensive theoretical model that integrates three critical nonlinearities", "link": "http://arxiv.org/abs/2512.20342v1", "date": "2025-12-23", "relevancy": 1.5692, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4008}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.3865}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Design%20and%20Modeling%20of%20a%20Simple-Structured%20Continuously%20Variable%20Transmission%20Utilizing%20Shape%20Memory%20Alloy%20Superelasticity%20for%20Twisted%20String%20Actuator&body=Title%3A%20Design%20and%20Modeling%20of%20a%20Simple-Structured%20Continuously%20Variable%20Transmission%20Utilizing%20Shape%20Memory%20Alloy%20Superelasticity%20for%20Twisted%20String%20Actuator%0AAuthor%3A%20Chanchan%20Xu%20and%20Shuai%20Dong%20and%20Xiaojie%20Wang%0AAbstract%3A%20Twisted%20String%20Actuators%20%28TSAs%29%20are%20widely%20used%20in%20robotics%20but%20suffer%20from%20a%20limited%20range%20of%20Transmission%20Ratio%20%28TR%29%20variation%2C%20restricting%20their%20efficiency%20under%20varying%20loads.To%20overcome%20this%2C%20we%20propose%20a%20novel%20lightweight%2C%20simple-structured%20Continuously%20Variable%20Transmission%20%28CVT%29%20mechanism%20for%20TSA%20utilizing%20Shape%20Memory%20Alloy%20%28SMA%29%20superelasticity.%20The%20CVT%20mechanism%20consists%20solely%20of%20a%20pair%20of%20highly%20lightweight%20superelastic%20SMA%20rods%20connecting%20the%20ends%20of%20twisted%20strings.%20These%20rods%20deform%20under%20external%20loads%2C%20adjusting%20the%20inter-string%20distance%20to%20enable%20continuous%20TR%20variation.We%20develop%20a%20comprehensive%20theoretical%20model%20that%20integrates%20three%20critical%20nonlinearities%0ALink%3A%20http%3A//arxiv.org/abs/2512.20342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesign%2520and%2520Modeling%2520of%2520a%2520Simple-Structured%2520Continuously%2520Variable%2520Transmission%2520Utilizing%2520Shape%2520Memory%2520Alloy%2520Superelasticity%2520for%2520Twisted%2520String%2520Actuator%26entry.906535625%3DChanchan%2520Xu%2520and%2520Shuai%2520Dong%2520and%2520Xiaojie%2520Wang%26entry.1292438233%3DTwisted%2520String%2520Actuators%2520%2528TSAs%2529%2520are%2520widely%2520used%2520in%2520robotics%2520but%2520suffer%2520from%2520a%2520limited%2520range%2520of%2520Transmission%2520Ratio%2520%2528TR%2529%2520variation%252C%2520restricting%2520their%2520efficiency%2520under%2520varying%2520loads.To%2520overcome%2520this%252C%2520we%2520propose%2520a%2520novel%2520lightweight%252C%2520simple-structured%2520Continuously%2520Variable%2520Transmission%2520%2528CVT%2529%2520mechanism%2520for%2520TSA%2520utilizing%2520Shape%2520Memory%2520Alloy%2520%2528SMA%2529%2520superelasticity.%2520The%2520CVT%2520mechanism%2520consists%2520solely%2520of%2520a%2520pair%2520of%2520highly%2520lightweight%2520superelastic%2520SMA%2520rods%2520connecting%2520the%2520ends%2520of%2520twisted%2520strings.%2520These%2520rods%2520deform%2520under%2520external%2520loads%252C%2520adjusting%2520the%2520inter-string%2520distance%2520to%2520enable%2520continuous%2520TR%2520variation.We%2520develop%2520a%2520comprehensive%2520theoretical%2520model%2520that%2520integrates%2520three%2520critical%2520nonlinearities%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%20and%20Modeling%20of%20a%20Simple-Structured%20Continuously%20Variable%20Transmission%20Utilizing%20Shape%20Memory%20Alloy%20Superelasticity%20for%20Twisted%20String%20Actuator&entry.906535625=Chanchan%20Xu%20and%20Shuai%20Dong%20and%20Xiaojie%20Wang&entry.1292438233=Twisted%20String%20Actuators%20%28TSAs%29%20are%20widely%20used%20in%20robotics%20but%20suffer%20from%20a%20limited%20range%20of%20Transmission%20Ratio%20%28TR%29%20variation%2C%20restricting%20their%20efficiency%20under%20varying%20loads.To%20overcome%20this%2C%20we%20propose%20a%20novel%20lightweight%2C%20simple-structured%20Continuously%20Variable%20Transmission%20%28CVT%29%20mechanism%20for%20TSA%20utilizing%20Shape%20Memory%20Alloy%20%28SMA%29%20superelasticity.%20The%20CVT%20mechanism%20consists%20solely%20of%20a%20pair%20of%20highly%20lightweight%20superelastic%20SMA%20rods%20connecting%20the%20ends%20of%20twisted%20strings.%20These%20rods%20deform%20under%20external%20loads%2C%20adjusting%20the%20inter-string%20distance%20to%20enable%20continuous%20TR%20variation.We%20develop%20a%20comprehensive%20theoretical%20model%20that%20integrates%20three%20critical%20nonlinearities&entry.1838667208=http%3A//arxiv.org/abs/2512.20342v1&entry.124074799=Read"},
{"title": "mLaSDI: Multi-stage latent space dynamics identification", "author": "William Anderson and Seung Whan Chung and Robert Stephany and Youngsoo Choi", "abstract": "Accurately solving partial differential equations (PDEs) is essential across many scientific disciplines. However, high-fidelity solvers can be computationally prohibitive, motivating the development of reduced-order models (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was proposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the training data via an autoencoder and learns user-specified ordinary differential equations (ODEs), governing the latent dynamics, enabling rapid predictions for unseen parameters. While LaSDI has produced effective ROMs for numerous problems, the autoencoder must simultaneously reconstruct the training data and satisfy the imposed latent dynamics, which are often competing objectives that limit accuracy, particularly for complex or high-frequency phenomena. To address this limitation, we propose multi-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, we train LaSDI sequentially in stages. After training the initial autoencoder, we train additional decoders which map the latent trajectories to residuals from previous stages. This staged residual learning, combined with periodic activation functions, enables recovery of high-frequency content without sacrificing interpretability of the latent dynamics. Numerical experiments on a multiscale oscillating system, unsteady wake flow, and the 1D-1V Vlasov equation demonstrate that mLaSDI achieves significantly lower reconstruction and prediction errors, often by an order of magnitude, while requiring less training time and reduced hyperparameter tuning compared to standard LaSDI.", "link": "http://arxiv.org/abs/2506.09207v3", "date": "2025-12-23", "relevancy": 1.4846, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5591}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4768}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mLaSDI%3A%20Multi-stage%20latent%20space%20dynamics%20identification&body=Title%3A%20mLaSDI%3A%20Multi-stage%20latent%20space%20dynamics%20identification%0AAuthor%3A%20William%20Anderson%20and%20Seung%20Whan%20Chung%20and%20Robert%20Stephany%20and%20Youngsoo%20Choi%0AAbstract%3A%20Accurately%20solving%20partial%20differential%20equations%20%28PDEs%29%20is%20essential%20across%20many%20scientific%20disciplines.%20However%2C%20high-fidelity%20solvers%20can%20be%20computationally%20prohibitive%2C%20motivating%20the%20development%20of%20reduced-order%20models%20%28ROMs%29.%20Recently%2C%20Latent%20Space%20Dynamics%20Identification%20%28LaSDI%29%20was%20proposed%20as%20a%20data-driven%2C%20non-intrusive%20ROM%20framework.%20LaSDI%20compresses%20the%20training%20data%20via%20an%20autoencoder%20and%20learns%20user-specified%20ordinary%20differential%20equations%20%28ODEs%29%2C%20governing%20the%20latent%20dynamics%2C%20enabling%20rapid%20predictions%20for%20unseen%20parameters.%20While%20LaSDI%20has%20produced%20effective%20ROMs%20for%20numerous%20problems%2C%20the%20autoencoder%20must%20simultaneously%20reconstruct%20the%20training%20data%20and%20satisfy%20the%20imposed%20latent%20dynamics%2C%20which%20are%20often%20competing%20objectives%20that%20limit%20accuracy%2C%20particularly%20for%20complex%20or%20high-frequency%20phenomena.%20To%20address%20this%20limitation%2C%20we%20propose%20multi-stage%20Latent%20Space%20Dynamics%20Identification%20%28mLaSDI%29.%20With%20mLaSDI%2C%20we%20train%20LaSDI%20sequentially%20in%20stages.%20After%20training%20the%20initial%20autoencoder%2C%20we%20train%20additional%20decoders%20which%20map%20the%20latent%20trajectories%20to%20residuals%20from%20previous%20stages.%20This%20staged%20residual%20learning%2C%20combined%20with%20periodic%20activation%20functions%2C%20enables%20recovery%20of%20high-frequency%20content%20without%20sacrificing%20interpretability%20of%20the%20latent%20dynamics.%20Numerical%20experiments%20on%20a%20multiscale%20oscillating%20system%2C%20unsteady%20wake%20flow%2C%20and%20the%201D-1V%20Vlasov%20equation%20demonstrate%20that%20mLaSDI%20achieves%20significantly%20lower%20reconstruction%20and%20prediction%20errors%2C%20often%20by%20an%20order%20of%20magnitude%2C%20while%20requiring%20less%20training%20time%20and%20reduced%20hyperparameter%20tuning%20compared%20to%20standard%20LaSDI.%0ALink%3A%20http%3A//arxiv.org/abs/2506.09207v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmLaSDI%253A%2520Multi-stage%2520latent%2520space%2520dynamics%2520identification%26entry.906535625%3DWilliam%2520Anderson%2520and%2520Seung%2520Whan%2520Chung%2520and%2520Robert%2520Stephany%2520and%2520Youngsoo%2520Choi%26entry.1292438233%3DAccurately%2520solving%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520is%2520essential%2520across%2520many%2520scientific%2520disciplines.%2520However%252C%2520high-fidelity%2520solvers%2520can%2520be%2520computationally%2520prohibitive%252C%2520motivating%2520the%2520development%2520of%2520reduced-order%2520models%2520%2528ROMs%2529.%2520Recently%252C%2520Latent%2520Space%2520Dynamics%2520Identification%2520%2528LaSDI%2529%2520was%2520proposed%2520as%2520a%2520data-driven%252C%2520non-intrusive%2520ROM%2520framework.%2520LaSDI%2520compresses%2520the%2520training%2520data%2520via%2520an%2520autoencoder%2520and%2520learns%2520user-specified%2520ordinary%2520differential%2520equations%2520%2528ODEs%2529%252C%2520governing%2520the%2520latent%2520dynamics%252C%2520enabling%2520rapid%2520predictions%2520for%2520unseen%2520parameters.%2520While%2520LaSDI%2520has%2520produced%2520effective%2520ROMs%2520for%2520numerous%2520problems%252C%2520the%2520autoencoder%2520must%2520simultaneously%2520reconstruct%2520the%2520training%2520data%2520and%2520satisfy%2520the%2520imposed%2520latent%2520dynamics%252C%2520which%2520are%2520often%2520competing%2520objectives%2520that%2520limit%2520accuracy%252C%2520particularly%2520for%2520complex%2520or%2520high-frequency%2520phenomena.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520multi-stage%2520Latent%2520Space%2520Dynamics%2520Identification%2520%2528mLaSDI%2529.%2520With%2520mLaSDI%252C%2520we%2520train%2520LaSDI%2520sequentially%2520in%2520stages.%2520After%2520training%2520the%2520initial%2520autoencoder%252C%2520we%2520train%2520additional%2520decoders%2520which%2520map%2520the%2520latent%2520trajectories%2520to%2520residuals%2520from%2520previous%2520stages.%2520This%2520staged%2520residual%2520learning%252C%2520combined%2520with%2520periodic%2520activation%2520functions%252C%2520enables%2520recovery%2520of%2520high-frequency%2520content%2520without%2520sacrificing%2520interpretability%2520of%2520the%2520latent%2520dynamics.%2520Numerical%2520experiments%2520on%2520a%2520multiscale%2520oscillating%2520system%252C%2520unsteady%2520wake%2520flow%252C%2520and%2520the%25201D-1V%2520Vlasov%2520equation%2520demonstrate%2520that%2520mLaSDI%2520achieves%2520significantly%2520lower%2520reconstruction%2520and%2520prediction%2520errors%252C%2520often%2520by%2520an%2520order%2520of%2520magnitude%252C%2520while%2520requiring%2520less%2520training%2520time%2520and%2520reduced%2520hyperparameter%2520tuning%2520compared%2520to%2520standard%2520LaSDI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09207v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mLaSDI%3A%20Multi-stage%20latent%20space%20dynamics%20identification&entry.906535625=William%20Anderson%20and%20Seung%20Whan%20Chung%20and%20Robert%20Stephany%20and%20Youngsoo%20Choi&entry.1292438233=Accurately%20solving%20partial%20differential%20equations%20%28PDEs%29%20is%20essential%20across%20many%20scientific%20disciplines.%20However%2C%20high-fidelity%20solvers%20can%20be%20computationally%20prohibitive%2C%20motivating%20the%20development%20of%20reduced-order%20models%20%28ROMs%29.%20Recently%2C%20Latent%20Space%20Dynamics%20Identification%20%28LaSDI%29%20was%20proposed%20as%20a%20data-driven%2C%20non-intrusive%20ROM%20framework.%20LaSDI%20compresses%20the%20training%20data%20via%20an%20autoencoder%20and%20learns%20user-specified%20ordinary%20differential%20equations%20%28ODEs%29%2C%20governing%20the%20latent%20dynamics%2C%20enabling%20rapid%20predictions%20for%20unseen%20parameters.%20While%20LaSDI%20has%20produced%20effective%20ROMs%20for%20numerous%20problems%2C%20the%20autoencoder%20must%20simultaneously%20reconstruct%20the%20training%20data%20and%20satisfy%20the%20imposed%20latent%20dynamics%2C%20which%20are%20often%20competing%20objectives%20that%20limit%20accuracy%2C%20particularly%20for%20complex%20or%20high-frequency%20phenomena.%20To%20address%20this%20limitation%2C%20we%20propose%20multi-stage%20Latent%20Space%20Dynamics%20Identification%20%28mLaSDI%29.%20With%20mLaSDI%2C%20we%20train%20LaSDI%20sequentially%20in%20stages.%20After%20training%20the%20initial%20autoencoder%2C%20we%20train%20additional%20decoders%20which%20map%20the%20latent%20trajectories%20to%20residuals%20from%20previous%20stages.%20This%20staged%20residual%20learning%2C%20combined%20with%20periodic%20activation%20functions%2C%20enables%20recovery%20of%20high-frequency%20content%20without%20sacrificing%20interpretability%20of%20the%20latent%20dynamics.%20Numerical%20experiments%20on%20a%20multiscale%20oscillating%20system%2C%20unsteady%20wake%20flow%2C%20and%20the%201D-1V%20Vlasov%20equation%20demonstrate%20that%20mLaSDI%20achieves%20significantly%20lower%20reconstruction%20and%20prediction%20errors%2C%20often%20by%20an%20order%20of%20magnitude%2C%20while%20requiring%20less%20training%20time%20and%20reduced%20hyperparameter%20tuning%20compared%20to%20standard%20LaSDI.&entry.1838667208=http%3A//arxiv.org/abs/2506.09207v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


