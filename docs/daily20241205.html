<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241204.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains\n  for High-Fidelity Indoor Scene Reconstruction", "author": "Wanting Zhang and Haodong Xiang and Zhichao Liao and Xiansong Lai and Xinghui Li and Long Zeng", "abstract": "  The reconstruction of indoor scenes remains challenging due to the inherent\ncomplexity of spatial structures and the prevalence of textureless regions.\nRecent advancements in 3D Gaussian Splatting have improved novel view synthesis\nwith accelerated processing but have yet to deliver comparable performance in\nsurface reconstruction. In this paper, we introduce 2DGS-Room, a novel method\nleveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction.\nSpecifically, we employ a seed-guided mechanism to control the distribution of\n2D Gaussians, with the density of seed points dynamically optimized through\nadaptive growth and pruning mechanisms. To further improve geometric accuracy,\nwe incorporate monocular depth and normal priors to provide constraints for\ndetails and textureless regions respectively. Additionally, multi-view\nconsistency constraints are employed to mitigate artifacts and further enhance\nreconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets\ndemonstrate that our method achieves state-of-the-art performance in indoor\nscene reconstruction.\n", "link": "http://arxiv.org/abs/2412.03428v1", "date": "2024-12-04", "relevancy": 3.4227, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7341}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6608}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%202DGS-Room%3A%20Seed-Guided%202D%20Gaussian%20Splatting%20with%20Geometric%20Constrains%0A%20%20for%20High-Fidelity%20Indoor%20Scene%20Reconstruction&body=Title%3A%202DGS-Room%3A%20Seed-Guided%202D%20Gaussian%20Splatting%20with%20Geometric%20Constrains%0A%20%20for%20High-Fidelity%20Indoor%20Scene%20Reconstruction%0AAuthor%3A%20Wanting%20Zhang%20and%20Haodong%20Xiang%20and%20Zhichao%20Liao%20and%20Xiansong%20Lai%20and%20Xinghui%20Li%20and%20Long%20Zeng%0AAbstract%3A%20%20%20The%20reconstruction%20of%20indoor%20scenes%20remains%20challenging%20due%20to%20the%20inherent%0Acomplexity%20of%20spatial%20structures%20and%20the%20prevalence%20of%20textureless%20regions.%0ARecent%20advancements%20in%203D%20Gaussian%20Splatting%20have%20improved%20novel%20view%20synthesis%0Awith%20accelerated%20processing%20but%20have%20yet%20to%20deliver%20comparable%20performance%20in%0Asurface%20reconstruction.%20In%20this%20paper%2C%20we%20introduce%202DGS-Room%2C%20a%20novel%20method%0Aleveraging%202D%20Gaussian%20Splatting%20for%20high-fidelity%20indoor%20scene%20reconstruction.%0ASpecifically%2C%20we%20employ%20a%20seed-guided%20mechanism%20to%20control%20the%20distribution%20of%0A2D%20Gaussians%2C%20with%20the%20density%20of%20seed%20points%20dynamically%20optimized%20through%0Aadaptive%20growth%20and%20pruning%20mechanisms.%20To%20further%20improve%20geometric%20accuracy%2C%0Awe%20incorporate%20monocular%20depth%20and%20normal%20priors%20to%20provide%20constraints%20for%0Adetails%20and%20textureless%20regions%20respectively.%20Additionally%2C%20multi-view%0Aconsistency%20constraints%20are%20employed%20to%20mitigate%20artifacts%20and%20further%20enhance%0Areconstruction%20quality.%20Extensive%20experiments%20on%20ScanNet%20and%20ScanNet%2B%2B%20datasets%0Ademonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20in%20indoor%0Ascene%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D2DGS-Room%253A%2520Seed-Guided%25202D%2520Gaussian%2520Splatting%2520with%2520Geometric%2520Constrains%250A%2520%2520for%2520High-Fidelity%2520Indoor%2520Scene%2520Reconstruction%26entry.906535625%3DWanting%2520Zhang%2520and%2520Haodong%2520Xiang%2520and%2520Zhichao%2520Liao%2520and%2520Xiansong%2520Lai%2520and%2520Xinghui%2520Li%2520and%2520Long%2520Zeng%26entry.1292438233%3D%2520%2520The%2520reconstruction%2520of%2520indoor%2520scenes%2520remains%2520challenging%2520due%2520to%2520the%2520inherent%250Acomplexity%2520of%2520spatial%2520structures%2520and%2520the%2520prevalence%2520of%2520textureless%2520regions.%250ARecent%2520advancements%2520in%25203D%2520Gaussian%2520Splatting%2520have%2520improved%2520novel%2520view%2520synthesis%250Awith%2520accelerated%2520processing%2520but%2520have%2520yet%2520to%2520deliver%2520comparable%2520performance%2520in%250Asurface%2520reconstruction.%2520In%2520this%2520paper%252C%2520we%2520introduce%25202DGS-Room%252C%2520a%2520novel%2520method%250Aleveraging%25202D%2520Gaussian%2520Splatting%2520for%2520high-fidelity%2520indoor%2520scene%2520reconstruction.%250ASpecifically%252C%2520we%2520employ%2520a%2520seed-guided%2520mechanism%2520to%2520control%2520the%2520distribution%2520of%250A2D%2520Gaussians%252C%2520with%2520the%2520density%2520of%2520seed%2520points%2520dynamically%2520optimized%2520through%250Aadaptive%2520growth%2520and%2520pruning%2520mechanisms.%2520To%2520further%2520improve%2520geometric%2520accuracy%252C%250Awe%2520incorporate%2520monocular%2520depth%2520and%2520normal%2520priors%2520to%2520provide%2520constraints%2520for%250Adetails%2520and%2520textureless%2520regions%2520respectively.%2520Additionally%252C%2520multi-view%250Aconsistency%2520constraints%2520are%2520employed%2520to%2520mitigate%2520artifacts%2520and%2520further%2520enhance%250Areconstruction%2520quality.%2520Extensive%2520experiments%2520on%2520ScanNet%2520and%2520ScanNet%252B%252B%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520in%2520indoor%250Ascene%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2DGS-Room%3A%20Seed-Guided%202D%20Gaussian%20Splatting%20with%20Geometric%20Constrains%0A%20%20for%20High-Fidelity%20Indoor%20Scene%20Reconstruction&entry.906535625=Wanting%20Zhang%20and%20Haodong%20Xiang%20and%20Zhichao%20Liao%20and%20Xiansong%20Lai%20and%20Xinghui%20Li%20and%20Long%20Zeng&entry.1292438233=%20%20The%20reconstruction%20of%20indoor%20scenes%20remains%20challenging%20due%20to%20the%20inherent%0Acomplexity%20of%20spatial%20structures%20and%20the%20prevalence%20of%20textureless%20regions.%0ARecent%20advancements%20in%203D%20Gaussian%20Splatting%20have%20improved%20novel%20view%20synthesis%0Awith%20accelerated%20processing%20but%20have%20yet%20to%20deliver%20comparable%20performance%20in%0Asurface%20reconstruction.%20In%20this%20paper%2C%20we%20introduce%202DGS-Room%2C%20a%20novel%20method%0Aleveraging%202D%20Gaussian%20Splatting%20for%20high-fidelity%20indoor%20scene%20reconstruction.%0ASpecifically%2C%20we%20employ%20a%20seed-guided%20mechanism%20to%20control%20the%20distribution%20of%0A2D%20Gaussians%2C%20with%20the%20density%20of%20seed%20points%20dynamically%20optimized%20through%0Aadaptive%20growth%20and%20pruning%20mechanisms.%20To%20further%20improve%20geometric%20accuracy%2C%0Awe%20incorporate%20monocular%20depth%20and%20normal%20priors%20to%20provide%20constraints%20for%0Adetails%20and%20textureless%20regions%20respectively.%20Additionally%2C%20multi-view%0Aconsistency%20constraints%20are%20employed%20to%20mitigate%20artifacts%20and%20further%20enhance%0Areconstruction%20quality.%20Extensive%20experiments%20on%20ScanNet%20and%20ScanNet%2B%2B%20datasets%0Ademonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20in%20indoor%0Ascene%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03428v1&entry.124074799=Read"},
{"title": "Urban4D: Semantic-Guided 4D Gaussian Splatting for Urban Scene\n  Reconstruction", "author": "Ziwen Li and Jiaxin Huang and Runnan Chen and Yunlong Che and Yandong Guo and Tongliang Liu and Fakhri Karray and Mingming Gong", "abstract": "  Reconstructing dynamic urban scenes presents significant challenges due to\ntheir intrinsic geometric structures and spatiotemporal dynamics. Existing\nmethods that attempt to model dynamic urban scenes without leveraging priors on\npotentially moving regions often produce suboptimal results. Meanwhile,\napproaches based on manual 3D annotations yield improved reconstruction quality\nbut are impractical due to labor-intensive labeling. In this paper, we revisit\nthe potential of 2D semantic maps for classifying dynamic and static Gaussians\nand integrating spatial and temporal dimensions for urban scene representation.\nWe introduce Urban4D, a novel framework that employs a semantic-guided\ndecomposition strategy inspired by advances in deep 2D semantic map generation.\nOur approach distinguishes potentially dynamic objects through reliable\nsemantic Gaussians. To explicitly model dynamic objects, we propose an\nintuitive and effective 4D Gaussian splatting (4DGS) representation that\naggregates temporal information through learnable time embeddings for each\nGaussian, predicting their deformations at desired timestamps using a\nmultilayer perceptron (MLP). For more accurate static reconstruction, we also\ndesign a k-nearest neighbor (KNN)-based consistency regularization to handle\nthe ground surface due to its low-texture characteristic. Extensive experiments\non real-world datasets demonstrate that Urban4D not only achieves comparable or\nbetter quality than previous state-of-the-art methods but also effectively\ncaptures dynamic objects while maintaining high visual fidelity for static\nelements.\n", "link": "http://arxiv.org/abs/2412.03473v1", "date": "2024-12-04", "relevancy": 3.4098, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6991}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6737}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Urban4D%3A%20Semantic-Guided%204D%20Gaussian%20Splatting%20for%20Urban%20Scene%0A%20%20Reconstruction&body=Title%3A%20Urban4D%3A%20Semantic-Guided%204D%20Gaussian%20Splatting%20for%20Urban%20Scene%0A%20%20Reconstruction%0AAuthor%3A%20Ziwen%20Li%20and%20Jiaxin%20Huang%20and%20Runnan%20Chen%20and%20Yunlong%20Che%20and%20Yandong%20Guo%20and%20Tongliang%20Liu%20and%20Fakhri%20Karray%20and%20Mingming%20Gong%0AAbstract%3A%20%20%20Reconstructing%20dynamic%20urban%20scenes%20presents%20significant%20challenges%20due%20to%0Atheir%20intrinsic%20geometric%20structures%20and%20spatiotemporal%20dynamics.%20Existing%0Amethods%20that%20attempt%20to%20model%20dynamic%20urban%20scenes%20without%20leveraging%20priors%20on%0Apotentially%20moving%20regions%20often%20produce%20suboptimal%20results.%20Meanwhile%2C%0Aapproaches%20based%20on%20manual%203D%20annotations%20yield%20improved%20reconstruction%20quality%0Abut%20are%20impractical%20due%20to%20labor-intensive%20labeling.%20In%20this%20paper%2C%20we%20revisit%0Athe%20potential%20of%202D%20semantic%20maps%20for%20classifying%20dynamic%20and%20static%20Gaussians%0Aand%20integrating%20spatial%20and%20temporal%20dimensions%20for%20urban%20scene%20representation.%0AWe%20introduce%20Urban4D%2C%20a%20novel%20framework%20that%20employs%20a%20semantic-guided%0Adecomposition%20strategy%20inspired%20by%20advances%20in%20deep%202D%20semantic%20map%20generation.%0AOur%20approach%20distinguishes%20potentially%20dynamic%20objects%20through%20reliable%0Asemantic%20Gaussians.%20To%20explicitly%20model%20dynamic%20objects%2C%20we%20propose%20an%0Aintuitive%20and%20effective%204D%20Gaussian%20splatting%20%284DGS%29%20representation%20that%0Aaggregates%20temporal%20information%20through%20learnable%20time%20embeddings%20for%20each%0AGaussian%2C%20predicting%20their%20deformations%20at%20desired%20timestamps%20using%20a%0Amultilayer%20perceptron%20%28MLP%29.%20For%20more%20accurate%20static%20reconstruction%2C%20we%20also%0Adesign%20a%20k-nearest%20neighbor%20%28KNN%29-based%20consistency%20regularization%20to%20handle%0Athe%20ground%20surface%20due%20to%20its%20low-texture%20characteristic.%20Extensive%20experiments%0Aon%20real-world%20datasets%20demonstrate%20that%20Urban4D%20not%20only%20achieves%20comparable%20or%0Abetter%20quality%20than%20previous%20state-of-the-art%20methods%20but%20also%20effectively%0Acaptures%20dynamic%20objects%20while%20maintaining%20high%20visual%20fidelity%20for%20static%0Aelements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrban4D%253A%2520Semantic-Guided%25204D%2520Gaussian%2520Splatting%2520for%2520Urban%2520Scene%250A%2520%2520Reconstruction%26entry.906535625%3DZiwen%2520Li%2520and%2520Jiaxin%2520Huang%2520and%2520Runnan%2520Chen%2520and%2520Yunlong%2520Che%2520and%2520Yandong%2520Guo%2520and%2520Tongliang%2520Liu%2520and%2520Fakhri%2520Karray%2520and%2520Mingming%2520Gong%26entry.1292438233%3D%2520%2520Reconstructing%2520dynamic%2520urban%2520scenes%2520presents%2520significant%2520challenges%2520due%2520to%250Atheir%2520intrinsic%2520geometric%2520structures%2520and%2520spatiotemporal%2520dynamics.%2520Existing%250Amethods%2520that%2520attempt%2520to%2520model%2520dynamic%2520urban%2520scenes%2520without%2520leveraging%2520priors%2520on%250Apotentially%2520moving%2520regions%2520often%2520produce%2520suboptimal%2520results.%2520Meanwhile%252C%250Aapproaches%2520based%2520on%2520manual%25203D%2520annotations%2520yield%2520improved%2520reconstruction%2520quality%250Abut%2520are%2520impractical%2520due%2520to%2520labor-intensive%2520labeling.%2520In%2520this%2520paper%252C%2520we%2520revisit%250Athe%2520potential%2520of%25202D%2520semantic%2520maps%2520for%2520classifying%2520dynamic%2520and%2520static%2520Gaussians%250Aand%2520integrating%2520spatial%2520and%2520temporal%2520dimensions%2520for%2520urban%2520scene%2520representation.%250AWe%2520introduce%2520Urban4D%252C%2520a%2520novel%2520framework%2520that%2520employs%2520a%2520semantic-guided%250Adecomposition%2520strategy%2520inspired%2520by%2520advances%2520in%2520deep%25202D%2520semantic%2520map%2520generation.%250AOur%2520approach%2520distinguishes%2520potentially%2520dynamic%2520objects%2520through%2520reliable%250Asemantic%2520Gaussians.%2520To%2520explicitly%2520model%2520dynamic%2520objects%252C%2520we%2520propose%2520an%250Aintuitive%2520and%2520effective%25204D%2520Gaussian%2520splatting%2520%25284DGS%2529%2520representation%2520that%250Aaggregates%2520temporal%2520information%2520through%2520learnable%2520time%2520embeddings%2520for%2520each%250AGaussian%252C%2520predicting%2520their%2520deformations%2520at%2520desired%2520timestamps%2520using%2520a%250Amultilayer%2520perceptron%2520%2528MLP%2529.%2520For%2520more%2520accurate%2520static%2520reconstruction%252C%2520we%2520also%250Adesign%2520a%2520k-nearest%2520neighbor%2520%2528KNN%2529-based%2520consistency%2520regularization%2520to%2520handle%250Athe%2520ground%2520surface%2520due%2520to%2520its%2520low-texture%2520characteristic.%2520Extensive%2520experiments%250Aon%2520real-world%2520datasets%2520demonstrate%2520that%2520Urban4D%2520not%2520only%2520achieves%2520comparable%2520or%250Abetter%2520quality%2520than%2520previous%2520state-of-the-art%2520methods%2520but%2520also%2520effectively%250Acaptures%2520dynamic%2520objects%2520while%2520maintaining%2520high%2520visual%2520fidelity%2520for%2520static%250Aelements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Urban4D%3A%20Semantic-Guided%204D%20Gaussian%20Splatting%20for%20Urban%20Scene%0A%20%20Reconstruction&entry.906535625=Ziwen%20Li%20and%20Jiaxin%20Huang%20and%20Runnan%20Chen%20and%20Yunlong%20Che%20and%20Yandong%20Guo%20and%20Tongliang%20Liu%20and%20Fakhri%20Karray%20and%20Mingming%20Gong&entry.1292438233=%20%20Reconstructing%20dynamic%20urban%20scenes%20presents%20significant%20challenges%20due%20to%0Atheir%20intrinsic%20geometric%20structures%20and%20spatiotemporal%20dynamics.%20Existing%0Amethods%20that%20attempt%20to%20model%20dynamic%20urban%20scenes%20without%20leveraging%20priors%20on%0Apotentially%20moving%20regions%20often%20produce%20suboptimal%20results.%20Meanwhile%2C%0Aapproaches%20based%20on%20manual%203D%20annotations%20yield%20improved%20reconstruction%20quality%0Abut%20are%20impractical%20due%20to%20labor-intensive%20labeling.%20In%20this%20paper%2C%20we%20revisit%0Athe%20potential%20of%202D%20semantic%20maps%20for%20classifying%20dynamic%20and%20static%20Gaussians%0Aand%20integrating%20spatial%20and%20temporal%20dimensions%20for%20urban%20scene%20representation.%0AWe%20introduce%20Urban4D%2C%20a%20novel%20framework%20that%20employs%20a%20semantic-guided%0Adecomposition%20strategy%20inspired%20by%20advances%20in%20deep%202D%20semantic%20map%20generation.%0AOur%20approach%20distinguishes%20potentially%20dynamic%20objects%20through%20reliable%0Asemantic%20Gaussians.%20To%20explicitly%20model%20dynamic%20objects%2C%20we%20propose%20an%0Aintuitive%20and%20effective%204D%20Gaussian%20splatting%20%284DGS%29%20representation%20that%0Aaggregates%20temporal%20information%20through%20learnable%20time%20embeddings%20for%20each%0AGaussian%2C%20predicting%20their%20deformations%20at%20desired%20timestamps%20using%20a%0Amultilayer%20perceptron%20%28MLP%29.%20For%20more%20accurate%20static%20reconstruction%2C%20we%20also%0Adesign%20a%20k-nearest%20neighbor%20%28KNN%29-based%20consistency%20regularization%20to%20handle%0Athe%20ground%20surface%20due%20to%20its%20low-texture%20characteristic.%20Extensive%20experiments%0Aon%20real-world%20datasets%20demonstrate%20that%20Urban4D%20not%20only%20achieves%20comparable%20or%0Abetter%20quality%20than%20previous%20state-of-the-art%20methods%20but%20also%20effectively%0Acaptures%20dynamic%20objects%20while%20maintaining%20high%20visual%20fidelity%20for%20static%0Aelements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03473v1&entry.124074799=Read"},
{"title": "Volumetrically Consistent 3D Gaussian Rasterization", "author": "Chinmay Talegaonkar and Yash Belhe and Ravi Ramamoorthi and Nicholas Antipa", "abstract": "  Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view\nsynthesis at high inference speeds. However, its splatting-based rendering\nmodel makes several approximations to the rendering equation, reducing physical\naccuracy. We show that splatting and its approximations are unnecessary, even\nwithin a rasterizer; we instead volumetrically integrate 3D Gaussians directly\nto compute the transmittance across them analytically. We use this analytic\ntransmittance to derive more physically-accurate alpha values than 3DGS, which\ncan directly be used within their framework. The result is a method that more\nclosely follows the volume rendering equation (similar to ray-tracing) while\nenjoying the speed benefits of rasterization. Our method represents opaque\nsurfaces with higher accuracy and fewer points than 3DGS. This enables it to\noutperform 3DGS for view synthesis (measured in SSIM and LPIPS). Being\nvolumetrically consistent also enables our method to work out of the box for\ntomography. We match the state-of-the-art 3DGS-based tomography method with\nfewer points. Being volumetrically consistent also enables our method to work\nout of the box for tomography. We match the state-of-the-art 3DGS-based\ntomography method with fewer points.\n", "link": "http://arxiv.org/abs/2412.03378v1", "date": "2024-12-04", "relevancy": 3.2754, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7273}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6233}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Volumetrically%20Consistent%203D%20Gaussian%20Rasterization&body=Title%3A%20Volumetrically%20Consistent%203D%20Gaussian%20Rasterization%0AAuthor%3A%20Chinmay%20Talegaonkar%20and%20Yash%20Belhe%20and%20Ravi%20Ramamoorthi%20and%20Nicholas%20Antipa%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20enabled%20photorealistic%20view%0Asynthesis%20at%20high%20inference%20speeds.%20However%2C%20its%20splatting-based%20rendering%0Amodel%20makes%20several%20approximations%20to%20the%20rendering%20equation%2C%20reducing%20physical%0Aaccuracy.%20We%20show%20that%20splatting%20and%20its%20approximations%20are%20unnecessary%2C%20even%0Awithin%20a%20rasterizer%3B%20we%20instead%20volumetrically%20integrate%203D%20Gaussians%20directly%0Ato%20compute%20the%20transmittance%20across%20them%20analytically.%20We%20use%20this%20analytic%0Atransmittance%20to%20derive%20more%20physically-accurate%20alpha%20values%20than%203DGS%2C%20which%0Acan%20directly%20be%20used%20within%20their%20framework.%20The%20result%20is%20a%20method%20that%20more%0Aclosely%20follows%20the%20volume%20rendering%20equation%20%28similar%20to%20ray-tracing%29%20while%0Aenjoying%20the%20speed%20benefits%20of%20rasterization.%20Our%20method%20represents%20opaque%0Asurfaces%20with%20higher%20accuracy%20and%20fewer%20points%20than%203DGS.%20This%20enables%20it%20to%0Aoutperform%203DGS%20for%20view%20synthesis%20%28measured%20in%20SSIM%20and%20LPIPS%29.%20Being%0Avolumetrically%20consistent%20also%20enables%20our%20method%20to%20work%20out%20of%20the%20box%20for%0Atomography.%20We%20match%20the%20state-of-the-art%203DGS-based%20tomography%20method%20with%0Afewer%20points.%20Being%20volumetrically%20consistent%20also%20enables%20our%20method%20to%20work%0Aout%20of%20the%20box%20for%20tomography.%20We%20match%20the%20state-of-the-art%203DGS-based%0Atomography%20method%20with%20fewer%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVolumetrically%2520Consistent%25203D%2520Gaussian%2520Rasterization%26entry.906535625%3DChinmay%2520Talegaonkar%2520and%2520Yash%2520Belhe%2520and%2520Ravi%2520Ramamoorthi%2520and%2520Nicholas%2520Antipa%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520enabled%2520photorealistic%2520view%250Asynthesis%2520at%2520high%2520inference%2520speeds.%2520However%252C%2520its%2520splatting-based%2520rendering%250Amodel%2520makes%2520several%2520approximations%2520to%2520the%2520rendering%2520equation%252C%2520reducing%2520physical%250Aaccuracy.%2520We%2520show%2520that%2520splatting%2520and%2520its%2520approximations%2520are%2520unnecessary%252C%2520even%250Awithin%2520a%2520rasterizer%253B%2520we%2520instead%2520volumetrically%2520integrate%25203D%2520Gaussians%2520directly%250Ato%2520compute%2520the%2520transmittance%2520across%2520them%2520analytically.%2520We%2520use%2520this%2520analytic%250Atransmittance%2520to%2520derive%2520more%2520physically-accurate%2520alpha%2520values%2520than%25203DGS%252C%2520which%250Acan%2520directly%2520be%2520used%2520within%2520their%2520framework.%2520The%2520result%2520is%2520a%2520method%2520that%2520more%250Aclosely%2520follows%2520the%2520volume%2520rendering%2520equation%2520%2528similar%2520to%2520ray-tracing%2529%2520while%250Aenjoying%2520the%2520speed%2520benefits%2520of%2520rasterization.%2520Our%2520method%2520represents%2520opaque%250Asurfaces%2520with%2520higher%2520accuracy%2520and%2520fewer%2520points%2520than%25203DGS.%2520This%2520enables%2520it%2520to%250Aoutperform%25203DGS%2520for%2520view%2520synthesis%2520%2528measured%2520in%2520SSIM%2520and%2520LPIPS%2529.%2520Being%250Avolumetrically%2520consistent%2520also%2520enables%2520our%2520method%2520to%2520work%2520out%2520of%2520the%2520box%2520for%250Atomography.%2520We%2520match%2520the%2520state-of-the-art%25203DGS-based%2520tomography%2520method%2520with%250Afewer%2520points.%2520Being%2520volumetrically%2520consistent%2520also%2520enables%2520our%2520method%2520to%2520work%250Aout%2520of%2520the%2520box%2520for%2520tomography.%2520We%2520match%2520the%2520state-of-the-art%25203DGS-based%250Atomography%2520method%2520with%2520fewer%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Volumetrically%20Consistent%203D%20Gaussian%20Rasterization&entry.906535625=Chinmay%20Talegaonkar%20and%20Yash%20Belhe%20and%20Ravi%20Ramamoorthi%20and%20Nicholas%20Antipa&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20enabled%20photorealistic%20view%0Asynthesis%20at%20high%20inference%20speeds.%20However%2C%20its%20splatting-based%20rendering%0Amodel%20makes%20several%20approximations%20to%20the%20rendering%20equation%2C%20reducing%20physical%0Aaccuracy.%20We%20show%20that%20splatting%20and%20its%20approximations%20are%20unnecessary%2C%20even%0Awithin%20a%20rasterizer%3B%20we%20instead%20volumetrically%20integrate%203D%20Gaussians%20directly%0Ato%20compute%20the%20transmittance%20across%20them%20analytically.%20We%20use%20this%20analytic%0Atransmittance%20to%20derive%20more%20physically-accurate%20alpha%20values%20than%203DGS%2C%20which%0Acan%20directly%20be%20used%20within%20their%20framework.%20The%20result%20is%20a%20method%20that%20more%0Aclosely%20follows%20the%20volume%20rendering%20equation%20%28similar%20to%20ray-tracing%29%20while%0Aenjoying%20the%20speed%20benefits%20of%20rasterization.%20Our%20method%20represents%20opaque%0Asurfaces%20with%20higher%20accuracy%20and%20fewer%20points%20than%203DGS.%20This%20enables%20it%20to%0Aoutperform%203DGS%20for%20view%20synthesis%20%28measured%20in%20SSIM%20and%20LPIPS%29.%20Being%0Avolumetrically%20consistent%20also%20enables%20our%20method%20to%20work%20out%20of%20the%20box%20for%0Atomography.%20We%20match%20the%20state-of-the-art%203DGS-based%20tomography%20method%20with%0Afewer%20points.%20Being%20volumetrically%20consistent%20also%20enables%20our%20method%20to%20work%0Aout%20of%20the%20box%20for%20tomography.%20We%20match%20the%20state-of-the-art%203DGS-based%0Atomography%20method%20with%20fewer%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03378v1&entry.124074799=Read"},
{"title": "SparseLGS: Sparse View Language Embedded Gaussian Splatting", "author": "Jun Hu and Zhang Chen and Zhong Li and Yi Xu and Juyong Zhang", "abstract": "  Recently, several studies have combined Gaussian Splatting to obtain scene\nrepresentations with language embeddings for open-vocabulary 3D scene\nunderstanding. While these methods perform well, they essentially require very\ndense multi-view inputs, limiting their applicability in real-world scenarios.\nIn this work, we propose SparseLGS to address the challenge of 3D scene\nunderstanding with pose-free and sparse view input images. Our method leverages\na learning-based dense stereo model to handle pose-free and sparse inputs, and\na three-step region matching approach to address the multi-view semantic\ninconsistency problem, which is especially important for sparse inputs.\nDifferent from directly learning high-dimensional CLIP features, we extract\nlow-dimensional information and build bijections to avoid excessive learning\nand storage costs. We introduce a reconstruction loss during semantic training\nto improve Gaussian positions and shapes. To the best of our knowledge, we are\nthe first to address the 3D semantic field problem with sparse pose-free\ninputs. Experimental results show that SparseLGS achieves comparable quality\nwhen reconstructing semantic fields with fewer inputs (3-4 views) compared to\nprevious SOTA methods with dense input. Besides, when using the same sparse\ninput, SparseLGS leads significantly in quality and heavily improves the\ncomputation speed (5$\\times$speedup). Project page:\nhttps://ustc3dv.github.io/SparseLGS\n", "link": "http://arxiv.org/abs/2412.02245v2", "date": "2024-12-04", "relevancy": 3.2735, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6865}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6742}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparseLGS%3A%20Sparse%20View%20Language%20Embedded%20Gaussian%20Splatting&body=Title%3A%20SparseLGS%3A%20Sparse%20View%20Language%20Embedded%20Gaussian%20Splatting%0AAuthor%3A%20Jun%20Hu%20and%20Zhang%20Chen%20and%20Zhong%20Li%20and%20Yi%20Xu%20and%20Juyong%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20several%20studies%20have%20combined%20Gaussian%20Splatting%20to%20obtain%20scene%0Arepresentations%20with%20language%20embeddings%20for%20open-vocabulary%203D%20scene%0Aunderstanding.%20While%20these%20methods%20perform%20well%2C%20they%20essentially%20require%20very%0Adense%20multi-view%20inputs%2C%20limiting%20their%20applicability%20in%20real-world%20scenarios.%0AIn%20this%20work%2C%20we%20propose%20SparseLGS%20to%20address%20the%20challenge%20of%203D%20scene%0Aunderstanding%20with%20pose-free%20and%20sparse%20view%20input%20images.%20Our%20method%20leverages%0Aa%20learning-based%20dense%20stereo%20model%20to%20handle%20pose-free%20and%20sparse%20inputs%2C%20and%0Aa%20three-step%20region%20matching%20approach%20to%20address%20the%20multi-view%20semantic%0Ainconsistency%20problem%2C%20which%20is%20especially%20important%20for%20sparse%20inputs.%0ADifferent%20from%20directly%20learning%20high-dimensional%20CLIP%20features%2C%20we%20extract%0Alow-dimensional%20information%20and%20build%20bijections%20to%20avoid%20excessive%20learning%0Aand%20storage%20costs.%20We%20introduce%20a%20reconstruction%20loss%20during%20semantic%20training%0Ato%20improve%20Gaussian%20positions%20and%20shapes.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%0Athe%20first%20to%20address%20the%203D%20semantic%20field%20problem%20with%20sparse%20pose-free%0Ainputs.%20Experimental%20results%20show%20that%20SparseLGS%20achieves%20comparable%20quality%0Awhen%20reconstructing%20semantic%20fields%20with%20fewer%20inputs%20%283-4%20views%29%20compared%20to%0Aprevious%20SOTA%20methods%20with%20dense%20input.%20Besides%2C%20when%20using%20the%20same%20sparse%0Ainput%2C%20SparseLGS%20leads%20significantly%20in%20quality%20and%20heavily%20improves%20the%0Acomputation%20speed%20%285%24%5Ctimes%24speedup%29.%20Project%20page%3A%0Ahttps%3A//ustc3dv.github.io/SparseLGS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02245v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparseLGS%253A%2520Sparse%2520View%2520Language%2520Embedded%2520Gaussian%2520Splatting%26entry.906535625%3DJun%2520Hu%2520and%2520Zhang%2520Chen%2520and%2520Zhong%2520Li%2520and%2520Yi%2520Xu%2520and%2520Juyong%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%2520several%2520studies%2520have%2520combined%2520Gaussian%2520Splatting%2520to%2520obtain%2520scene%250Arepresentations%2520with%2520language%2520embeddings%2520for%2520open-vocabulary%25203D%2520scene%250Aunderstanding.%2520While%2520these%2520methods%2520perform%2520well%252C%2520they%2520essentially%2520require%2520very%250Adense%2520multi-view%2520inputs%252C%2520limiting%2520their%2520applicability%2520in%2520real-world%2520scenarios.%250AIn%2520this%2520work%252C%2520we%2520propose%2520SparseLGS%2520to%2520address%2520the%2520challenge%2520of%25203D%2520scene%250Aunderstanding%2520with%2520pose-free%2520and%2520sparse%2520view%2520input%2520images.%2520Our%2520method%2520leverages%250Aa%2520learning-based%2520dense%2520stereo%2520model%2520to%2520handle%2520pose-free%2520and%2520sparse%2520inputs%252C%2520and%250Aa%2520three-step%2520region%2520matching%2520approach%2520to%2520address%2520the%2520multi-view%2520semantic%250Ainconsistency%2520problem%252C%2520which%2520is%2520especially%2520important%2520for%2520sparse%2520inputs.%250ADifferent%2520from%2520directly%2520learning%2520high-dimensional%2520CLIP%2520features%252C%2520we%2520extract%250Alow-dimensional%2520information%2520and%2520build%2520bijections%2520to%2520avoid%2520excessive%2520learning%250Aand%2520storage%2520costs.%2520We%2520introduce%2520a%2520reconstruction%2520loss%2520during%2520semantic%2520training%250Ato%2520improve%2520Gaussian%2520positions%2520and%2520shapes.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520we%2520are%250Athe%2520first%2520to%2520address%2520the%25203D%2520semantic%2520field%2520problem%2520with%2520sparse%2520pose-free%250Ainputs.%2520Experimental%2520results%2520show%2520that%2520SparseLGS%2520achieves%2520comparable%2520quality%250Awhen%2520reconstructing%2520semantic%2520fields%2520with%2520fewer%2520inputs%2520%25283-4%2520views%2529%2520compared%2520to%250Aprevious%2520SOTA%2520methods%2520with%2520dense%2520input.%2520Besides%252C%2520when%2520using%2520the%2520same%2520sparse%250Ainput%252C%2520SparseLGS%2520leads%2520significantly%2520in%2520quality%2520and%2520heavily%2520improves%2520the%250Acomputation%2520speed%2520%25285%2524%255Ctimes%2524speedup%2529.%2520Project%2520page%253A%250Ahttps%253A//ustc3dv.github.io/SparseLGS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02245v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparseLGS%3A%20Sparse%20View%20Language%20Embedded%20Gaussian%20Splatting&entry.906535625=Jun%20Hu%20and%20Zhang%20Chen%20and%20Zhong%20Li%20and%20Yi%20Xu%20and%20Juyong%20Zhang&entry.1292438233=%20%20Recently%2C%20several%20studies%20have%20combined%20Gaussian%20Splatting%20to%20obtain%20scene%0Arepresentations%20with%20language%20embeddings%20for%20open-vocabulary%203D%20scene%0Aunderstanding.%20While%20these%20methods%20perform%20well%2C%20they%20essentially%20require%20very%0Adense%20multi-view%20inputs%2C%20limiting%20their%20applicability%20in%20real-world%20scenarios.%0AIn%20this%20work%2C%20we%20propose%20SparseLGS%20to%20address%20the%20challenge%20of%203D%20scene%0Aunderstanding%20with%20pose-free%20and%20sparse%20view%20input%20images.%20Our%20method%20leverages%0Aa%20learning-based%20dense%20stereo%20model%20to%20handle%20pose-free%20and%20sparse%20inputs%2C%20and%0Aa%20three-step%20region%20matching%20approach%20to%20address%20the%20multi-view%20semantic%0Ainconsistency%20problem%2C%20which%20is%20especially%20important%20for%20sparse%20inputs.%0ADifferent%20from%20directly%20learning%20high-dimensional%20CLIP%20features%2C%20we%20extract%0Alow-dimensional%20information%20and%20build%20bijections%20to%20avoid%20excessive%20learning%0Aand%20storage%20costs.%20We%20introduce%20a%20reconstruction%20loss%20during%20semantic%20training%0Ato%20improve%20Gaussian%20positions%20and%20shapes.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%0Athe%20first%20to%20address%20the%203D%20semantic%20field%20problem%20with%20sparse%20pose-free%0Ainputs.%20Experimental%20results%20show%20that%20SparseLGS%20achieves%20comparable%20quality%0Awhen%20reconstructing%20semantic%20fields%20with%20fewer%20inputs%20%283-4%20views%29%20compared%20to%0Aprevious%20SOTA%20methods%20with%20dense%20input.%20Besides%2C%20when%20using%20the%20same%20sparse%0Ainput%2C%20SparseLGS%20leads%20significantly%20in%20quality%20and%20heavily%20improves%20the%0Acomputation%20speed%20%285%24%5Ctimes%24speedup%29.%20Project%20page%3A%0Ahttps%3A//ustc3dv.github.io/SparseLGS%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02245v2&entry.124074799=Read"},
{"title": "Functionality understanding and segmentation in 3D scenes", "author": "Jaime Corsetti and Francesco Giuliari and Alice Fasoli and Davide Boscaini and Fabio Poiesi", "abstract": "  Understanding functionalities in 3D scenes involves interpreting natural\nlanguage descriptions to locate functional interactive objects, such as handles\nand buttons, in a 3D environment. Functionality understanding is highly\nchallenging, as it requires both world knowledge to interpret language and\nspatial perception to identify fine-grained objects. For example, given a task\nlike 'turn on the ceiling light', an embodied AI agent must infer that it needs\nto locate the light switch, even though the switch is not explicitly mentioned\nin the task description. To date, no dedicated methods have been developed for\nthis problem. In this paper, we introduce Fun3DU, the first approach designed\nfor functionality understanding in 3D scenes. Fun3DU uses a language model to\nparse the task description through Chain-of-Thought reasoning in order to\nidentify the object of interest. The identified object is segmented across\nmultiple views of the captured scene by using a vision and language model. The\nsegmentation results from each view are lifted in 3D and aggregated into the\npoint cloud using geometric information. Fun3DU is training-free, relying\nentirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most\nrecent and only dataset to benchmark this task, which comprises over 3000 task\ndescriptions on 230 scenes. Our method significantly outperforms\nstate-of-the-art open-vocabulary 3D segmentation approaches. Project page:\nhttps://jcorsetti.github.io/fun3du\n", "link": "http://arxiv.org/abs/2411.16310v3", "date": "2024-12-04", "relevancy": 3.262, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.684}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Functionality%20understanding%20and%20segmentation%20in%203D%20scenes&body=Title%3A%20Functionality%20understanding%20and%20segmentation%20in%203D%20scenes%0AAuthor%3A%20Jaime%20Corsetti%20and%20Francesco%20Giuliari%20and%20Alice%20Fasoli%20and%20Davide%20Boscaini%20and%20Fabio%20Poiesi%0AAbstract%3A%20%20%20Understanding%20functionalities%20in%203D%20scenes%20involves%20interpreting%20natural%0Alanguage%20descriptions%20to%20locate%20functional%20interactive%20objects%2C%20such%20as%20handles%0Aand%20buttons%2C%20in%20a%203D%20environment.%20Functionality%20understanding%20is%20highly%0Achallenging%2C%20as%20it%20requires%20both%20world%20knowledge%20to%20interpret%20language%20and%0Aspatial%20perception%20to%20identify%20fine-grained%20objects.%20For%20example%2C%20given%20a%20task%0Alike%20%27turn%20on%20the%20ceiling%20light%27%2C%20an%20embodied%20AI%20agent%20must%20infer%20that%20it%20needs%0Ato%20locate%20the%20light%20switch%2C%20even%20though%20the%20switch%20is%20not%20explicitly%20mentioned%0Ain%20the%20task%20description.%20To%20date%2C%20no%20dedicated%20methods%20have%20been%20developed%20for%0Athis%20problem.%20In%20this%20paper%2C%20we%20introduce%20Fun3DU%2C%20the%20first%20approach%20designed%0Afor%20functionality%20understanding%20in%203D%20scenes.%20Fun3DU%20uses%20a%20language%20model%20to%0Aparse%20the%20task%20description%20through%20Chain-of-Thought%20reasoning%20in%20order%20to%0Aidentify%20the%20object%20of%20interest.%20The%20identified%20object%20is%20segmented%20across%0Amultiple%20views%20of%20the%20captured%20scene%20by%20using%20a%20vision%20and%20language%20model.%20The%0Asegmentation%20results%20from%20each%20view%20are%20lifted%20in%203D%20and%20aggregated%20into%20the%0Apoint%20cloud%20using%20geometric%20information.%20Fun3DU%20is%20training-free%2C%20relying%0Aentirely%20on%20pre-trained%20models.%20We%20evaluate%20Fun3DU%20on%20SceneFun3D%2C%20the%20most%0Arecent%20and%20only%20dataset%20to%20benchmark%20this%20task%2C%20which%20comprises%20over%203000%20task%0Adescriptions%20on%20230%20scenes.%20Our%20method%20significantly%20outperforms%0Astate-of-the-art%20open-vocabulary%203D%20segmentation%20approaches.%20Project%20page%3A%0Ahttps%3A//jcorsetti.github.io/fun3du%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16310v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunctionality%2520understanding%2520and%2520segmentation%2520in%25203D%2520scenes%26entry.906535625%3DJaime%2520Corsetti%2520and%2520Francesco%2520Giuliari%2520and%2520Alice%2520Fasoli%2520and%2520Davide%2520Boscaini%2520and%2520Fabio%2520Poiesi%26entry.1292438233%3D%2520%2520Understanding%2520functionalities%2520in%25203D%2520scenes%2520involves%2520interpreting%2520natural%250Alanguage%2520descriptions%2520to%2520locate%2520functional%2520interactive%2520objects%252C%2520such%2520as%2520handles%250Aand%2520buttons%252C%2520in%2520a%25203D%2520environment.%2520Functionality%2520understanding%2520is%2520highly%250Achallenging%252C%2520as%2520it%2520requires%2520both%2520world%2520knowledge%2520to%2520interpret%2520language%2520and%250Aspatial%2520perception%2520to%2520identify%2520fine-grained%2520objects.%2520For%2520example%252C%2520given%2520a%2520task%250Alike%2520%2527turn%2520on%2520the%2520ceiling%2520light%2527%252C%2520an%2520embodied%2520AI%2520agent%2520must%2520infer%2520that%2520it%2520needs%250Ato%2520locate%2520the%2520light%2520switch%252C%2520even%2520though%2520the%2520switch%2520is%2520not%2520explicitly%2520mentioned%250Ain%2520the%2520task%2520description.%2520To%2520date%252C%2520no%2520dedicated%2520methods%2520have%2520been%2520developed%2520for%250Athis%2520problem.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Fun3DU%252C%2520the%2520first%2520approach%2520designed%250Afor%2520functionality%2520understanding%2520in%25203D%2520scenes.%2520Fun3DU%2520uses%2520a%2520language%2520model%2520to%250Aparse%2520the%2520task%2520description%2520through%2520Chain-of-Thought%2520reasoning%2520in%2520order%2520to%250Aidentify%2520the%2520object%2520of%2520interest.%2520The%2520identified%2520object%2520is%2520segmented%2520across%250Amultiple%2520views%2520of%2520the%2520captured%2520scene%2520by%2520using%2520a%2520vision%2520and%2520language%2520model.%2520The%250Asegmentation%2520results%2520from%2520each%2520view%2520are%2520lifted%2520in%25203D%2520and%2520aggregated%2520into%2520the%250Apoint%2520cloud%2520using%2520geometric%2520information.%2520Fun3DU%2520is%2520training-free%252C%2520relying%250Aentirely%2520on%2520pre-trained%2520models.%2520We%2520evaluate%2520Fun3DU%2520on%2520SceneFun3D%252C%2520the%2520most%250Arecent%2520and%2520only%2520dataset%2520to%2520benchmark%2520this%2520task%252C%2520which%2520comprises%2520over%25203000%2520task%250Adescriptions%2520on%2520230%2520scenes.%2520Our%2520method%2520significantly%2520outperforms%250Astate-of-the-art%2520open-vocabulary%25203D%2520segmentation%2520approaches.%2520Project%2520page%253A%250Ahttps%253A//jcorsetti.github.io/fun3du%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16310v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Functionality%20understanding%20and%20segmentation%20in%203D%20scenes&entry.906535625=Jaime%20Corsetti%20and%20Francesco%20Giuliari%20and%20Alice%20Fasoli%20and%20Davide%20Boscaini%20and%20Fabio%20Poiesi&entry.1292438233=%20%20Understanding%20functionalities%20in%203D%20scenes%20involves%20interpreting%20natural%0Alanguage%20descriptions%20to%20locate%20functional%20interactive%20objects%2C%20such%20as%20handles%0Aand%20buttons%2C%20in%20a%203D%20environment.%20Functionality%20understanding%20is%20highly%0Achallenging%2C%20as%20it%20requires%20both%20world%20knowledge%20to%20interpret%20language%20and%0Aspatial%20perception%20to%20identify%20fine-grained%20objects.%20For%20example%2C%20given%20a%20task%0Alike%20%27turn%20on%20the%20ceiling%20light%27%2C%20an%20embodied%20AI%20agent%20must%20infer%20that%20it%20needs%0Ato%20locate%20the%20light%20switch%2C%20even%20though%20the%20switch%20is%20not%20explicitly%20mentioned%0Ain%20the%20task%20description.%20To%20date%2C%20no%20dedicated%20methods%20have%20been%20developed%20for%0Athis%20problem.%20In%20this%20paper%2C%20we%20introduce%20Fun3DU%2C%20the%20first%20approach%20designed%0Afor%20functionality%20understanding%20in%203D%20scenes.%20Fun3DU%20uses%20a%20language%20model%20to%0Aparse%20the%20task%20description%20through%20Chain-of-Thought%20reasoning%20in%20order%20to%0Aidentify%20the%20object%20of%20interest.%20The%20identified%20object%20is%20segmented%20across%0Amultiple%20views%20of%20the%20captured%20scene%20by%20using%20a%20vision%20and%20language%20model.%20The%0Asegmentation%20results%20from%20each%20view%20are%20lifted%20in%203D%20and%20aggregated%20into%20the%0Apoint%20cloud%20using%20geometric%20information.%20Fun3DU%20is%20training-free%2C%20relying%0Aentirely%20on%20pre-trained%20models.%20We%20evaluate%20Fun3DU%20on%20SceneFun3D%2C%20the%20most%0Arecent%20and%20only%20dataset%20to%20benchmark%20this%20task%2C%20which%20comprises%20over%203000%20task%0Adescriptions%20on%20230%20scenes.%20Our%20method%20significantly%20outperforms%0Astate-of-the-art%20open-vocabulary%203D%20segmentation%20approaches.%20Project%20page%3A%0Ahttps%3A//jcorsetti.github.io/fun3du%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16310v3&entry.124074799=Read"},
{"title": "NeRF and Gaussian Splatting SLAM in the Wild", "author": "Fabian Schmidt and Markus Enzweiler and Abhinav Valada", "abstract": "  Navigating outdoor environments with visual Simultaneous Localization and\nMapping (SLAM) systems poses significant challenges due to dynamic scenes,\nlighting variations, and seasonal changes, requiring robust solutions. While\ntraditional SLAM methods struggle with adaptability, deep learning-based\napproaches and emerging neural radiance fields as well as Gaussian\nSplatting-based SLAM methods, offer promising alternatives. However, these\nmethods have primarily been evaluated in controlled indoor environments with\nstable conditions, leaving a gap in understanding their performance in\nunstructured and variable outdoor settings. This study addresses this gap by\nevaluating these methods in natural outdoor environments, focusing on camera\ntracking accuracy, robustness to environmental factors, and computational\nefficiency, highlighting distinct trade-offs. Extensive evaluations demonstrate\nthat neural SLAM methods achieve superior robustness, particularly under\nchallenging conditions such as low light, but at a high computational cost. At\nthe same time, traditional methods perform the best across seasons but are\nhighly sensitive to variations in lighting conditions. The code of the\nbenchmark is publicly available at\nhttps://github.com/iis-esslingen/nerf-3dgs-benchmark.\n", "link": "http://arxiv.org/abs/2412.03263v1", "date": "2024-12-04", "relevancy": 3.2177, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7266}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6168}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRF%20and%20Gaussian%20Splatting%20SLAM%20in%20the%20Wild&body=Title%3A%20NeRF%20and%20Gaussian%20Splatting%20SLAM%20in%20the%20Wild%0AAuthor%3A%20Fabian%20Schmidt%20and%20Markus%20Enzweiler%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20Navigating%20outdoor%20environments%20with%20visual%20Simultaneous%20Localization%20and%0AMapping%20%28SLAM%29%20systems%20poses%20significant%20challenges%20due%20to%20dynamic%20scenes%2C%0Alighting%20variations%2C%20and%20seasonal%20changes%2C%20requiring%20robust%20solutions.%20While%0Atraditional%20SLAM%20methods%20struggle%20with%20adaptability%2C%20deep%20learning-based%0Aapproaches%20and%20emerging%20neural%20radiance%20fields%20as%20well%20as%20Gaussian%0ASplatting-based%20SLAM%20methods%2C%20offer%20promising%20alternatives.%20However%2C%20these%0Amethods%20have%20primarily%20been%20evaluated%20in%20controlled%20indoor%20environments%20with%0Astable%20conditions%2C%20leaving%20a%20gap%20in%20understanding%20their%20performance%20in%0Aunstructured%20and%20variable%20outdoor%20settings.%20This%20study%20addresses%20this%20gap%20by%0Aevaluating%20these%20methods%20in%20natural%20outdoor%20environments%2C%20focusing%20on%20camera%0Atracking%20accuracy%2C%20robustness%20to%20environmental%20factors%2C%20and%20computational%0Aefficiency%2C%20highlighting%20distinct%20trade-offs.%20Extensive%20evaluations%20demonstrate%0Athat%20neural%20SLAM%20methods%20achieve%20superior%20robustness%2C%20particularly%20under%0Achallenging%20conditions%20such%20as%20low%20light%2C%20but%20at%20a%20high%20computational%20cost.%20At%0Athe%20same%20time%2C%20traditional%20methods%20perform%20the%20best%20across%20seasons%20but%20are%0Ahighly%20sensitive%20to%20variations%20in%20lighting%20conditions.%20The%20code%20of%20the%0Abenchmark%20is%20publicly%20available%20at%0Ahttps%3A//github.com/iis-esslingen/nerf-3dgs-benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRF%2520and%2520Gaussian%2520Splatting%2520SLAM%2520in%2520the%2520Wild%26entry.906535625%3DFabian%2520Schmidt%2520and%2520Markus%2520Enzweiler%2520and%2520Abhinav%2520Valada%26entry.1292438233%3D%2520%2520Navigating%2520outdoor%2520environments%2520with%2520visual%2520Simultaneous%2520Localization%2520and%250AMapping%2520%2528SLAM%2529%2520systems%2520poses%2520significant%2520challenges%2520due%2520to%2520dynamic%2520scenes%252C%250Alighting%2520variations%252C%2520and%2520seasonal%2520changes%252C%2520requiring%2520robust%2520solutions.%2520While%250Atraditional%2520SLAM%2520methods%2520struggle%2520with%2520adaptability%252C%2520deep%2520learning-based%250Aapproaches%2520and%2520emerging%2520neural%2520radiance%2520fields%2520as%2520well%2520as%2520Gaussian%250ASplatting-based%2520SLAM%2520methods%252C%2520offer%2520promising%2520alternatives.%2520However%252C%2520these%250Amethods%2520have%2520primarily%2520been%2520evaluated%2520in%2520controlled%2520indoor%2520environments%2520with%250Astable%2520conditions%252C%2520leaving%2520a%2520gap%2520in%2520understanding%2520their%2520performance%2520in%250Aunstructured%2520and%2520variable%2520outdoor%2520settings.%2520This%2520study%2520addresses%2520this%2520gap%2520by%250Aevaluating%2520these%2520methods%2520in%2520natural%2520outdoor%2520environments%252C%2520focusing%2520on%2520camera%250Atracking%2520accuracy%252C%2520robustness%2520to%2520environmental%2520factors%252C%2520and%2520computational%250Aefficiency%252C%2520highlighting%2520distinct%2520trade-offs.%2520Extensive%2520evaluations%2520demonstrate%250Athat%2520neural%2520SLAM%2520methods%2520achieve%2520superior%2520robustness%252C%2520particularly%2520under%250Achallenging%2520conditions%2520such%2520as%2520low%2520light%252C%2520but%2520at%2520a%2520high%2520computational%2520cost.%2520At%250Athe%2520same%2520time%252C%2520traditional%2520methods%2520perform%2520the%2520best%2520across%2520seasons%2520but%2520are%250Ahighly%2520sensitive%2520to%2520variations%2520in%2520lighting%2520conditions.%2520The%2520code%2520of%2520the%250Abenchmark%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/iis-esslingen/nerf-3dgs-benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRF%20and%20Gaussian%20Splatting%20SLAM%20in%20the%20Wild&entry.906535625=Fabian%20Schmidt%20and%20Markus%20Enzweiler%20and%20Abhinav%20Valada&entry.1292438233=%20%20Navigating%20outdoor%20environments%20with%20visual%20Simultaneous%20Localization%20and%0AMapping%20%28SLAM%29%20systems%20poses%20significant%20challenges%20due%20to%20dynamic%20scenes%2C%0Alighting%20variations%2C%20and%20seasonal%20changes%2C%20requiring%20robust%20solutions.%20While%0Atraditional%20SLAM%20methods%20struggle%20with%20adaptability%2C%20deep%20learning-based%0Aapproaches%20and%20emerging%20neural%20radiance%20fields%20as%20well%20as%20Gaussian%0ASplatting-based%20SLAM%20methods%2C%20offer%20promising%20alternatives.%20However%2C%20these%0Amethods%20have%20primarily%20been%20evaluated%20in%20controlled%20indoor%20environments%20with%0Astable%20conditions%2C%20leaving%20a%20gap%20in%20understanding%20their%20performance%20in%0Aunstructured%20and%20variable%20outdoor%20settings.%20This%20study%20addresses%20this%20gap%20by%0Aevaluating%20these%20methods%20in%20natural%20outdoor%20environments%2C%20focusing%20on%20camera%0Atracking%20accuracy%2C%20robustness%20to%20environmental%20factors%2C%20and%20computational%0Aefficiency%2C%20highlighting%20distinct%20trade-offs.%20Extensive%20evaluations%20demonstrate%0Athat%20neural%20SLAM%20methods%20achieve%20superior%20robustness%2C%20particularly%20under%0Achallenging%20conditions%20such%20as%20low%20light%2C%20but%20at%20a%20high%20computational%20cost.%20At%0Athe%20same%20time%2C%20traditional%20methods%20perform%20the%20best%20across%20seasons%20but%20are%0Ahighly%20sensitive%20to%20variations%20in%20lighting%20conditions.%20The%20code%20of%20the%0Abenchmark%20is%20publicly%20available%20at%0Ahttps%3A//github.com/iis-esslingen/nerf-3dgs-benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03263v1&entry.124074799=Read"},
{"title": "SGSST: Scaling Gaussian Splatting StyleTransfer", "author": "Bruno Galerne and Jianling Wang and Lara Raad and Jean-Michel Morel", "abstract": "  Applying style transfer to a full 3D environment is a challenging task that\nhas seen many developments since the advent of neural rendering. 3D Gaussian\nsplatting (3DGS) has recently pushed further many limits of neural rendering in\nterms of training speed and reconstruction quality. This work introduces SGSST:\nScaling Gaussian Splatting Style Transfer, an optimization-based method to\napply style transfer to pretrained 3DGS scenes. We demonstrate that a new\nmultiscale loss based on global neural statistics, that we name SOS for\nSimultaneously Optimized Scales, enables style transfer to ultra-high\nresolution 3D scenes. Not only SGSST pioneers 3D scene style transfer at such\nhigh image resolutions, it also produces superior visual quality as assessed by\nthorough qualitative, quantitative and perceptual comparisons.\n", "link": "http://arxiv.org/abs/2412.03371v1", "date": "2024-12-04", "relevancy": 3.1883, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6798}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6502}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGSST%3A%20Scaling%20Gaussian%20Splatting%20StyleTransfer&body=Title%3A%20SGSST%3A%20Scaling%20Gaussian%20Splatting%20StyleTransfer%0AAuthor%3A%20Bruno%20Galerne%20and%20Jianling%20Wang%20and%20Lara%20Raad%20and%20Jean-Michel%20Morel%0AAbstract%3A%20%20%20Applying%20style%20transfer%20to%20a%20full%203D%20environment%20is%20a%20challenging%20task%20that%0Ahas%20seen%20many%20developments%20since%20the%20advent%20of%20neural%20rendering.%203D%20Gaussian%0Asplatting%20%283DGS%29%20has%20recently%20pushed%20further%20many%20limits%20of%20neural%20rendering%20in%0Aterms%20of%20training%20speed%20and%20reconstruction%20quality.%20This%20work%20introduces%20SGSST%3A%0AScaling%20Gaussian%20Splatting%20Style%20Transfer%2C%20an%20optimization-based%20method%20to%0Aapply%20style%20transfer%20to%20pretrained%203DGS%20scenes.%20We%20demonstrate%20that%20a%20new%0Amultiscale%20loss%20based%20on%20global%20neural%20statistics%2C%20that%20we%20name%20SOS%20for%0ASimultaneously%20Optimized%20Scales%2C%20enables%20style%20transfer%20to%20ultra-high%0Aresolution%203D%20scenes.%20Not%20only%20SGSST%20pioneers%203D%20scene%20style%20transfer%20at%20such%0Ahigh%20image%20resolutions%2C%20it%20also%20produces%20superior%20visual%20quality%20as%20assessed%20by%0Athorough%20qualitative%2C%20quantitative%20and%20perceptual%20comparisons.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGSST%253A%2520Scaling%2520Gaussian%2520Splatting%2520StyleTransfer%26entry.906535625%3DBruno%2520Galerne%2520and%2520Jianling%2520Wang%2520and%2520Lara%2520Raad%2520and%2520Jean-Michel%2520Morel%26entry.1292438233%3D%2520%2520Applying%2520style%2520transfer%2520to%2520a%2520full%25203D%2520environment%2520is%2520a%2520challenging%2520task%2520that%250Ahas%2520seen%2520many%2520developments%2520since%2520the%2520advent%2520of%2520neural%2520rendering.%25203D%2520Gaussian%250Asplatting%2520%25283DGS%2529%2520has%2520recently%2520pushed%2520further%2520many%2520limits%2520of%2520neural%2520rendering%2520in%250Aterms%2520of%2520training%2520speed%2520and%2520reconstruction%2520quality.%2520This%2520work%2520introduces%2520SGSST%253A%250AScaling%2520Gaussian%2520Splatting%2520Style%2520Transfer%252C%2520an%2520optimization-based%2520method%2520to%250Aapply%2520style%2520transfer%2520to%2520pretrained%25203DGS%2520scenes.%2520We%2520demonstrate%2520that%2520a%2520new%250Amultiscale%2520loss%2520based%2520on%2520global%2520neural%2520statistics%252C%2520that%2520we%2520name%2520SOS%2520for%250ASimultaneously%2520Optimized%2520Scales%252C%2520enables%2520style%2520transfer%2520to%2520ultra-high%250Aresolution%25203D%2520scenes.%2520Not%2520only%2520SGSST%2520pioneers%25203D%2520scene%2520style%2520transfer%2520at%2520such%250Ahigh%2520image%2520resolutions%252C%2520it%2520also%2520produces%2520superior%2520visual%2520quality%2520as%2520assessed%2520by%250Athorough%2520qualitative%252C%2520quantitative%2520and%2520perceptual%2520comparisons.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGSST%3A%20Scaling%20Gaussian%20Splatting%20StyleTransfer&entry.906535625=Bruno%20Galerne%20and%20Jianling%20Wang%20and%20Lara%20Raad%20and%20Jean-Michel%20Morel&entry.1292438233=%20%20Applying%20style%20transfer%20to%20a%20full%203D%20environment%20is%20a%20challenging%20task%20that%0Ahas%20seen%20many%20developments%20since%20the%20advent%20of%20neural%20rendering.%203D%20Gaussian%0Asplatting%20%283DGS%29%20has%20recently%20pushed%20further%20many%20limits%20of%20neural%20rendering%20in%0Aterms%20of%20training%20speed%20and%20reconstruction%20quality.%20This%20work%20introduces%20SGSST%3A%0AScaling%20Gaussian%20Splatting%20Style%20Transfer%2C%20an%20optimization-based%20method%20to%0Aapply%20style%20transfer%20to%20pretrained%203DGS%20scenes.%20We%20demonstrate%20that%20a%20new%0Amultiscale%20loss%20based%20on%20global%20neural%20statistics%2C%20that%20we%20name%20SOS%20for%0ASimultaneously%20Optimized%20Scales%2C%20enables%20style%20transfer%20to%20ultra-high%0Aresolution%203D%20scenes.%20Not%20only%20SGSST%20pioneers%203D%20scene%20style%20transfer%20at%20such%0Ahigh%20image%20resolutions%2C%20it%20also%20produces%20superior%20visual%20quality%20as%20assessed%20by%0Athorough%20qualitative%2C%20quantitative%20and%20perceptual%20comparisons.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03371v1&entry.124074799=Read"},
{"title": "PlanarSplatting: Accurate Planar Surface Reconstruction in 3 Minutes", "author": "Bin Tan and Rui Yu and Yujun Shen and Nan Xue", "abstract": "  This paper presents PlanarSplatting, an ultra-fast and accurate surface\nreconstruction approach for multiview indoor images. We take the 3D planes as\nthe main objective due to their compactness and structural expressiveness in\nindoor scenes, and develop an explicit optimization framework that learns to\nfit the expected surface of indoor scenes by splatting the 3D planes into 2.5D\ndepth and normal maps. As our PlanarSplatting operates directly on the 3D plane\nprimitives, it eliminates the dependencies on 2D/3D plane detection and plane\nmatching and tracking for planar surface reconstruction. Furthermore, the\nessential merits of plane-based representation plus CUDA-based implementation\nof planar splatting functions, PlanarSplatting reconstructs an indoor scene in\n3 minutes while having significantly better geometric accuracy. Thanks to our\nultra-fast reconstruction speed, the largest quantitative evaluation on the\nScanNet and ScanNet++ datasets over hundreds of scenes clearly demonstrated the\nadvantages of our method. We believe that our accurate and ultrafast planar\nsurface reconstruction method will be applied in the structured data curation\nfor surface reconstruction in the future. The code of our CUDA implementation\nwill be publicly available. Project page:\nhttps://icetttb.github.io/PlanarSplatting/\n", "link": "http://arxiv.org/abs/2412.03451v1", "date": "2024-12-04", "relevancy": 3.1545, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6722}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6187}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PlanarSplatting%3A%20Accurate%20Planar%20Surface%20Reconstruction%20in%203%20Minutes&body=Title%3A%20PlanarSplatting%3A%20Accurate%20Planar%20Surface%20Reconstruction%20in%203%20Minutes%0AAuthor%3A%20Bin%20Tan%20and%20Rui%20Yu%20and%20Yujun%20Shen%20and%20Nan%20Xue%0AAbstract%3A%20%20%20This%20paper%20presents%20PlanarSplatting%2C%20an%20ultra-fast%20and%20accurate%20surface%0Areconstruction%20approach%20for%20multiview%20indoor%20images.%20We%20take%20the%203D%20planes%20as%0Athe%20main%20objective%20due%20to%20their%20compactness%20and%20structural%20expressiveness%20in%0Aindoor%20scenes%2C%20and%20develop%20an%20explicit%20optimization%20framework%20that%20learns%20to%0Afit%20the%20expected%20surface%20of%20indoor%20scenes%20by%20splatting%20the%203D%20planes%20into%202.5D%0Adepth%20and%20normal%20maps.%20As%20our%20PlanarSplatting%20operates%20directly%20on%20the%203D%20plane%0Aprimitives%2C%20it%20eliminates%20the%20dependencies%20on%202D/3D%20plane%20detection%20and%20plane%0Amatching%20and%20tracking%20for%20planar%20surface%20reconstruction.%20Furthermore%2C%20the%0Aessential%20merits%20of%20plane-based%20representation%20plus%20CUDA-based%20implementation%0Aof%20planar%20splatting%20functions%2C%20PlanarSplatting%20reconstructs%20an%20indoor%20scene%20in%0A3%20minutes%20while%20having%20significantly%20better%20geometric%20accuracy.%20Thanks%20to%20our%0Aultra-fast%20reconstruction%20speed%2C%20the%20largest%20quantitative%20evaluation%20on%20the%0AScanNet%20and%20ScanNet%2B%2B%20datasets%20over%20hundreds%20of%20scenes%20clearly%20demonstrated%20the%0Aadvantages%20of%20our%20method.%20We%20believe%20that%20our%20accurate%20and%20ultrafast%20planar%0Asurface%20reconstruction%20method%20will%20be%20applied%20in%20the%20structured%20data%20curation%0Afor%20surface%20reconstruction%20in%20the%20future.%20The%20code%20of%20our%20CUDA%20implementation%0Awill%20be%20publicly%20available.%20Project%20page%3A%0Ahttps%3A//icetttb.github.io/PlanarSplatting/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanarSplatting%253A%2520Accurate%2520Planar%2520Surface%2520Reconstruction%2520in%25203%2520Minutes%26entry.906535625%3DBin%2520Tan%2520and%2520Rui%2520Yu%2520and%2520Yujun%2520Shen%2520and%2520Nan%2520Xue%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520PlanarSplatting%252C%2520an%2520ultra-fast%2520and%2520accurate%2520surface%250Areconstruction%2520approach%2520for%2520multiview%2520indoor%2520images.%2520We%2520take%2520the%25203D%2520planes%2520as%250Athe%2520main%2520objective%2520due%2520to%2520their%2520compactness%2520and%2520structural%2520expressiveness%2520in%250Aindoor%2520scenes%252C%2520and%2520develop%2520an%2520explicit%2520optimization%2520framework%2520that%2520learns%2520to%250Afit%2520the%2520expected%2520surface%2520of%2520indoor%2520scenes%2520by%2520splatting%2520the%25203D%2520planes%2520into%25202.5D%250Adepth%2520and%2520normal%2520maps.%2520As%2520our%2520PlanarSplatting%2520operates%2520directly%2520on%2520the%25203D%2520plane%250Aprimitives%252C%2520it%2520eliminates%2520the%2520dependencies%2520on%25202D/3D%2520plane%2520detection%2520and%2520plane%250Amatching%2520and%2520tracking%2520for%2520planar%2520surface%2520reconstruction.%2520Furthermore%252C%2520the%250Aessential%2520merits%2520of%2520plane-based%2520representation%2520plus%2520CUDA-based%2520implementation%250Aof%2520planar%2520splatting%2520functions%252C%2520PlanarSplatting%2520reconstructs%2520an%2520indoor%2520scene%2520in%250A3%2520minutes%2520while%2520having%2520significantly%2520better%2520geometric%2520accuracy.%2520Thanks%2520to%2520our%250Aultra-fast%2520reconstruction%2520speed%252C%2520the%2520largest%2520quantitative%2520evaluation%2520on%2520the%250AScanNet%2520and%2520ScanNet%252B%252B%2520datasets%2520over%2520hundreds%2520of%2520scenes%2520clearly%2520demonstrated%2520the%250Aadvantages%2520of%2520our%2520method.%2520We%2520believe%2520that%2520our%2520accurate%2520and%2520ultrafast%2520planar%250Asurface%2520reconstruction%2520method%2520will%2520be%2520applied%2520in%2520the%2520structured%2520data%2520curation%250Afor%2520surface%2520reconstruction%2520in%2520the%2520future.%2520The%2520code%2520of%2520our%2520CUDA%2520implementation%250Awill%2520be%2520publicly%2520available.%2520Project%2520page%253A%250Ahttps%253A//icetttb.github.io/PlanarSplatting/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PlanarSplatting%3A%20Accurate%20Planar%20Surface%20Reconstruction%20in%203%20Minutes&entry.906535625=Bin%20Tan%20and%20Rui%20Yu%20and%20Yujun%20Shen%20and%20Nan%20Xue&entry.1292438233=%20%20This%20paper%20presents%20PlanarSplatting%2C%20an%20ultra-fast%20and%20accurate%20surface%0Areconstruction%20approach%20for%20multiview%20indoor%20images.%20We%20take%20the%203D%20planes%20as%0Athe%20main%20objective%20due%20to%20their%20compactness%20and%20structural%20expressiveness%20in%0Aindoor%20scenes%2C%20and%20develop%20an%20explicit%20optimization%20framework%20that%20learns%20to%0Afit%20the%20expected%20surface%20of%20indoor%20scenes%20by%20splatting%20the%203D%20planes%20into%202.5D%0Adepth%20and%20normal%20maps.%20As%20our%20PlanarSplatting%20operates%20directly%20on%20the%203D%20plane%0Aprimitives%2C%20it%20eliminates%20the%20dependencies%20on%202D/3D%20plane%20detection%20and%20plane%0Amatching%20and%20tracking%20for%20planar%20surface%20reconstruction.%20Furthermore%2C%20the%0Aessential%20merits%20of%20plane-based%20representation%20plus%20CUDA-based%20implementation%0Aof%20planar%20splatting%20functions%2C%20PlanarSplatting%20reconstructs%20an%20indoor%20scene%20in%0A3%20minutes%20while%20having%20significantly%20better%20geometric%20accuracy.%20Thanks%20to%20our%0Aultra-fast%20reconstruction%20speed%2C%20the%20largest%20quantitative%20evaluation%20on%20the%0AScanNet%20and%20ScanNet%2B%2B%20datasets%20over%20hundreds%20of%20scenes%20clearly%20demonstrated%20the%0Aadvantages%20of%20our%20method.%20We%20believe%20that%20our%20accurate%20and%20ultrafast%20planar%0Asurface%20reconstruction%20method%20will%20be%20applied%20in%20the%20structured%20data%20curation%0Afor%20surface%20reconstruction%20in%20the%20future.%20The%20code%20of%20our%20CUDA%20implementation%0Awill%20be%20publicly%20available.%20Project%20page%3A%0Ahttps%3A//icetttb.github.io/PlanarSplatting/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03451v1&entry.124074799=Read"},
{"title": "GaussianBeV: 3D Gaussian Representation meets Perception Models for BeV\n  Segmentation", "author": "Florian Chabot and Nicolas Granger and Guillaume Lapouge", "abstract": "  The Bird's-eye View (BeV) representation is widely used for 3D perception\nfrom multi-view camera images. It allows to merge features from different\ncameras into a common space, providing a unified representation of the 3D\nscene. The key component is the view transformer, which transforms image views\ninto the BeV. However, actual view transformer methods based on geometry or\ncross-attention do not provide a sufficiently detailed representation of the\nscene, as they use a sub-sampling of the 3D space that is non-optimal for\nmodeling the fine structures of the environment. In this paper, we propose\nGaussianBeV, a novel method for transforming image features to BeV by finely\nrepresenting the scene using a set of 3D gaussians located and oriented in 3D\nspace. This representation is then splattered to produce the BeV feature map by\nadapting recent advances in 3D representation rendering based on gaussian\nsplatting. GaussianBeV is the first approach to use this 3D gaussian modeling\nand 3D scene rendering process online, i.e. without optimizing it on a specific\nscene and directly integrated into a single stage model for BeV scene\nunderstanding. Experiments show that the proposed representation is highly\neffective and place GaussianBeV as the new state-of-the-art on the BeV semantic\nsegmentation task on the nuScenes dataset.\n", "link": "http://arxiv.org/abs/2407.14108v2", "date": "2024-12-04", "relevancy": 3.0803, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6291}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6107}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianBeV%3A%203D%20Gaussian%20Representation%20meets%20Perception%20Models%20for%20BeV%0A%20%20Segmentation&body=Title%3A%20GaussianBeV%3A%203D%20Gaussian%20Representation%20meets%20Perception%20Models%20for%20BeV%0A%20%20Segmentation%0AAuthor%3A%20Florian%20Chabot%20and%20Nicolas%20Granger%20and%20Guillaume%20Lapouge%0AAbstract%3A%20%20%20The%20Bird%27s-eye%20View%20%28BeV%29%20representation%20is%20widely%20used%20for%203D%20perception%0Afrom%20multi-view%20camera%20images.%20It%20allows%20to%20merge%20features%20from%20different%0Acameras%20into%20a%20common%20space%2C%20providing%20a%20unified%20representation%20of%20the%203D%0Ascene.%20The%20key%20component%20is%20the%20view%20transformer%2C%20which%20transforms%20image%20views%0Ainto%20the%20BeV.%20However%2C%20actual%20view%20transformer%20methods%20based%20on%20geometry%20or%0Across-attention%20do%20not%20provide%20a%20sufficiently%20detailed%20representation%20of%20the%0Ascene%2C%20as%20they%20use%20a%20sub-sampling%20of%20the%203D%20space%20that%20is%20non-optimal%20for%0Amodeling%20the%20fine%20structures%20of%20the%20environment.%20In%20this%20paper%2C%20we%20propose%0AGaussianBeV%2C%20a%20novel%20method%20for%20transforming%20image%20features%20to%20BeV%20by%20finely%0Arepresenting%20the%20scene%20using%20a%20set%20of%203D%20gaussians%20located%20and%20oriented%20in%203D%0Aspace.%20This%20representation%20is%20then%20splattered%20to%20produce%20the%20BeV%20feature%20map%20by%0Aadapting%20recent%20advances%20in%203D%20representation%20rendering%20based%20on%20gaussian%0Asplatting.%20GaussianBeV%20is%20the%20first%20approach%20to%20use%20this%203D%20gaussian%20modeling%0Aand%203D%20scene%20rendering%20process%20online%2C%20i.e.%20without%20optimizing%20it%20on%20a%20specific%0Ascene%20and%20directly%20integrated%20into%20a%20single%20stage%20model%20for%20BeV%20scene%0Aunderstanding.%20Experiments%20show%20that%20the%20proposed%20representation%20is%20highly%0Aeffective%20and%20place%20GaussianBeV%20as%20the%20new%20state-of-the-art%20on%20the%20BeV%20semantic%0Asegmentation%20task%20on%20the%20nuScenes%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14108v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianBeV%253A%25203D%2520Gaussian%2520Representation%2520meets%2520Perception%2520Models%2520for%2520BeV%250A%2520%2520Segmentation%26entry.906535625%3DFlorian%2520Chabot%2520and%2520Nicolas%2520Granger%2520and%2520Guillaume%2520Lapouge%26entry.1292438233%3D%2520%2520The%2520Bird%2527s-eye%2520View%2520%2528BeV%2529%2520representation%2520is%2520widely%2520used%2520for%25203D%2520perception%250Afrom%2520multi-view%2520camera%2520images.%2520It%2520allows%2520to%2520merge%2520features%2520from%2520different%250Acameras%2520into%2520a%2520common%2520space%252C%2520providing%2520a%2520unified%2520representation%2520of%2520the%25203D%250Ascene.%2520The%2520key%2520component%2520is%2520the%2520view%2520transformer%252C%2520which%2520transforms%2520image%2520views%250Ainto%2520the%2520BeV.%2520However%252C%2520actual%2520view%2520transformer%2520methods%2520based%2520on%2520geometry%2520or%250Across-attention%2520do%2520not%2520provide%2520a%2520sufficiently%2520detailed%2520representation%2520of%2520the%250Ascene%252C%2520as%2520they%2520use%2520a%2520sub-sampling%2520of%2520the%25203D%2520space%2520that%2520is%2520non-optimal%2520for%250Amodeling%2520the%2520fine%2520structures%2520of%2520the%2520environment.%2520In%2520this%2520paper%252C%2520we%2520propose%250AGaussianBeV%252C%2520a%2520novel%2520method%2520for%2520transforming%2520image%2520features%2520to%2520BeV%2520by%2520finely%250Arepresenting%2520the%2520scene%2520using%2520a%2520set%2520of%25203D%2520gaussians%2520located%2520and%2520oriented%2520in%25203D%250Aspace.%2520This%2520representation%2520is%2520then%2520splattered%2520to%2520produce%2520the%2520BeV%2520feature%2520map%2520by%250Aadapting%2520recent%2520advances%2520in%25203D%2520representation%2520rendering%2520based%2520on%2520gaussian%250Asplatting.%2520GaussianBeV%2520is%2520the%2520first%2520approach%2520to%2520use%2520this%25203D%2520gaussian%2520modeling%250Aand%25203D%2520scene%2520rendering%2520process%2520online%252C%2520i.e.%2520without%2520optimizing%2520it%2520on%2520a%2520specific%250Ascene%2520and%2520directly%2520integrated%2520into%2520a%2520single%2520stage%2520model%2520for%2520BeV%2520scene%250Aunderstanding.%2520Experiments%2520show%2520that%2520the%2520proposed%2520representation%2520is%2520highly%250Aeffective%2520and%2520place%2520GaussianBeV%2520as%2520the%2520new%2520state-of-the-art%2520on%2520the%2520BeV%2520semantic%250Asegmentation%2520task%2520on%2520the%2520nuScenes%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14108v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianBeV%3A%203D%20Gaussian%20Representation%20meets%20Perception%20Models%20for%20BeV%0A%20%20Segmentation&entry.906535625=Florian%20Chabot%20and%20Nicolas%20Granger%20and%20Guillaume%20Lapouge&entry.1292438233=%20%20The%20Bird%27s-eye%20View%20%28BeV%29%20representation%20is%20widely%20used%20for%203D%20perception%0Afrom%20multi-view%20camera%20images.%20It%20allows%20to%20merge%20features%20from%20different%0Acameras%20into%20a%20common%20space%2C%20providing%20a%20unified%20representation%20of%20the%203D%0Ascene.%20The%20key%20component%20is%20the%20view%20transformer%2C%20which%20transforms%20image%20views%0Ainto%20the%20BeV.%20However%2C%20actual%20view%20transformer%20methods%20based%20on%20geometry%20or%0Across-attention%20do%20not%20provide%20a%20sufficiently%20detailed%20representation%20of%20the%0Ascene%2C%20as%20they%20use%20a%20sub-sampling%20of%20the%203D%20space%20that%20is%20non-optimal%20for%0Amodeling%20the%20fine%20structures%20of%20the%20environment.%20In%20this%20paper%2C%20we%20propose%0AGaussianBeV%2C%20a%20novel%20method%20for%20transforming%20image%20features%20to%20BeV%20by%20finely%0Arepresenting%20the%20scene%20using%20a%20set%20of%203D%20gaussians%20located%20and%20oriented%20in%203D%0Aspace.%20This%20representation%20is%20then%20splattered%20to%20produce%20the%20BeV%20feature%20map%20by%0Aadapting%20recent%20advances%20in%203D%20representation%20rendering%20based%20on%20gaussian%0Asplatting.%20GaussianBeV%20is%20the%20first%20approach%20to%20use%20this%203D%20gaussian%20modeling%0Aand%203D%20scene%20rendering%20process%20online%2C%20i.e.%20without%20optimizing%20it%20on%20a%20specific%0Ascene%20and%20directly%20integrated%20into%20a%20single%20stage%20model%20for%20BeV%20scene%0Aunderstanding.%20Experiments%20show%20that%20the%20proposed%20representation%20is%20highly%0Aeffective%20and%20place%20GaussianBeV%20as%20the%20new%20state-of-the-art%20on%20the%20BeV%20semantic%0Asegmentation%20task%20on%20the%20nuScenes%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14108v2&entry.124074799=Read"},
{"title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs", "author": "Shengbang Tong and Ellis Brown and Penghao Wu and Sanghyun Woo and Manoj Middepogu and Sai Charitha Akula and Jihan Yang and Shusheng Yang and Adithya Iyer and Xichen Pan and Ziteng Wang and Rob Fergus and Yann LeCun and Saining Xie", "abstract": "  We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, address the difficulties involved in consolidating and interpreting\nresults from various tasks, and introduce a new vision-centric benchmark,\nCV-Bench. To further improve visual grounding, we propose the Spatial Vision\nAggregator (SVA), a dynamic and spatially-aware connector that integrates\nhigh-resolution vision features with LLMs while reducing the number of tokens.\nAdditionally, we discuss the curation of high-quality visual instruction-tuning\ndata from publicly available sources, emphasizing the importance of data source\nbalancing and distribution ratio. Collectively, Cambrian-1 not only achieves\nstate-of-the-art performance but also serves as a comprehensive, open cookbook\nfor instruction-tuned MLLMs. We provide model weights, code, supporting tools,\ndatasets, and detailed instruction-tuning and evaluation recipes. We hope our\nrelease will inspire and accelerate advancements in multimodal systems and\nvisual representation learning.\n", "link": "http://arxiv.org/abs/2406.16860v2", "date": "2024-12-04", "relevancy": 2.981, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6005}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cambrian-1%3A%20A%20Fully%20Open%2C%20Vision-Centric%20Exploration%20of%20Multimodal%20LLMs&body=Title%3A%20Cambrian-1%3A%20A%20Fully%20Open%2C%20Vision-Centric%20Exploration%20of%20Multimodal%20LLMs%0AAuthor%3A%20Shengbang%20Tong%20and%20Ellis%20Brown%20and%20Penghao%20Wu%20and%20Sanghyun%20Woo%20and%20Manoj%20Middepogu%20and%20Sai%20Charitha%20Akula%20and%20Jihan%20Yang%20and%20Shusheng%20Yang%20and%20Adithya%20Iyer%20and%20Xichen%20Pan%20and%20Ziteng%20Wang%20and%20Rob%20Fergus%20and%20Yann%20LeCun%20and%20Saining%20Xie%0AAbstract%3A%20%20%20We%20introduce%20Cambrian-1%2C%20a%20family%20of%20multimodal%20LLMs%20%28MLLMs%29%20designed%20with%20a%0Avision-centric%20approach.%20While%20stronger%20language%20models%20can%20enhance%20multimodal%0Acapabilities%2C%20the%20design%20choices%20for%20vision%20components%20are%20often%20insufficiently%0Aexplored%20and%20disconnected%20from%20visual%20representation%20learning%20research.%20This%0Agap%20hinders%20accurate%20sensory%20grounding%20in%20real-world%20scenarios.%20Our%20study%20uses%0ALLMs%20and%20visual%20instruction%20tuning%20as%20an%20interface%20to%20evaluate%20various%20visual%0Arepresentations%2C%20offering%20new%20insights%20into%20different%20models%20and%20architectures%0A--%20self-supervised%2C%20strongly%20supervised%2C%20or%20combinations%20thereof%20--%20based%20on%0Aexperiments%20with%20over%2020%20vision%20encoders.%20We%20critically%20examine%20existing%20MLLM%0Abenchmarks%2C%20address%20the%20difficulties%20involved%20in%20consolidating%20and%20interpreting%0Aresults%20from%20various%20tasks%2C%20and%20introduce%20a%20new%20vision-centric%20benchmark%2C%0ACV-Bench.%20To%20further%20improve%20visual%20grounding%2C%20we%20propose%20the%20Spatial%20Vision%0AAggregator%20%28SVA%29%2C%20a%20dynamic%20and%20spatially-aware%20connector%20that%20integrates%0Ahigh-resolution%20vision%20features%20with%20LLMs%20while%20reducing%20the%20number%20of%20tokens.%0AAdditionally%2C%20we%20discuss%20the%20curation%20of%20high-quality%20visual%20instruction-tuning%0Adata%20from%20publicly%20available%20sources%2C%20emphasizing%20the%20importance%20of%20data%20source%0Abalancing%20and%20distribution%20ratio.%20Collectively%2C%20Cambrian-1%20not%20only%20achieves%0Astate-of-the-art%20performance%20but%20also%20serves%20as%20a%20comprehensive%2C%20open%20cookbook%0Afor%20instruction-tuned%20MLLMs.%20We%20provide%20model%20weights%2C%20code%2C%20supporting%20tools%2C%0Adatasets%2C%20and%20detailed%20instruction-tuning%20and%20evaluation%20recipes.%20We%20hope%20our%0Arelease%20will%20inspire%20and%20accelerate%20advancements%20in%20multimodal%20systems%20and%0Avisual%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16860v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCambrian-1%253A%2520A%2520Fully%2520Open%252C%2520Vision-Centric%2520Exploration%2520of%2520Multimodal%2520LLMs%26entry.906535625%3DShengbang%2520Tong%2520and%2520Ellis%2520Brown%2520and%2520Penghao%2520Wu%2520and%2520Sanghyun%2520Woo%2520and%2520Manoj%2520Middepogu%2520and%2520Sai%2520Charitha%2520Akula%2520and%2520Jihan%2520Yang%2520and%2520Shusheng%2520Yang%2520and%2520Adithya%2520Iyer%2520and%2520Xichen%2520Pan%2520and%2520Ziteng%2520Wang%2520and%2520Rob%2520Fergus%2520and%2520Yann%2520LeCun%2520and%2520Saining%2520Xie%26entry.1292438233%3D%2520%2520We%2520introduce%2520Cambrian-1%252C%2520a%2520family%2520of%2520multimodal%2520LLMs%2520%2528MLLMs%2529%2520designed%2520with%2520a%250Avision-centric%2520approach.%2520While%2520stronger%2520language%2520models%2520can%2520enhance%2520multimodal%250Acapabilities%252C%2520the%2520design%2520choices%2520for%2520vision%2520components%2520are%2520often%2520insufficiently%250Aexplored%2520and%2520disconnected%2520from%2520visual%2520representation%2520learning%2520research.%2520This%250Agap%2520hinders%2520accurate%2520sensory%2520grounding%2520in%2520real-world%2520scenarios.%2520Our%2520study%2520uses%250ALLMs%2520and%2520visual%2520instruction%2520tuning%2520as%2520an%2520interface%2520to%2520evaluate%2520various%2520visual%250Arepresentations%252C%2520offering%2520new%2520insights%2520into%2520different%2520models%2520and%2520architectures%250A--%2520self-supervised%252C%2520strongly%2520supervised%252C%2520or%2520combinations%2520thereof%2520--%2520based%2520on%250Aexperiments%2520with%2520over%252020%2520vision%2520encoders.%2520We%2520critically%2520examine%2520existing%2520MLLM%250Abenchmarks%252C%2520address%2520the%2520difficulties%2520involved%2520in%2520consolidating%2520and%2520interpreting%250Aresults%2520from%2520various%2520tasks%252C%2520and%2520introduce%2520a%2520new%2520vision-centric%2520benchmark%252C%250ACV-Bench.%2520To%2520further%2520improve%2520visual%2520grounding%252C%2520we%2520propose%2520the%2520Spatial%2520Vision%250AAggregator%2520%2528SVA%2529%252C%2520a%2520dynamic%2520and%2520spatially-aware%2520connector%2520that%2520integrates%250Ahigh-resolution%2520vision%2520features%2520with%2520LLMs%2520while%2520reducing%2520the%2520number%2520of%2520tokens.%250AAdditionally%252C%2520we%2520discuss%2520the%2520curation%2520of%2520high-quality%2520visual%2520instruction-tuning%250Adata%2520from%2520publicly%2520available%2520sources%252C%2520emphasizing%2520the%2520importance%2520of%2520data%2520source%250Abalancing%2520and%2520distribution%2520ratio.%2520Collectively%252C%2520Cambrian-1%2520not%2520only%2520achieves%250Astate-of-the-art%2520performance%2520but%2520also%2520serves%2520as%2520a%2520comprehensive%252C%2520open%2520cookbook%250Afor%2520instruction-tuned%2520MLLMs.%2520We%2520provide%2520model%2520weights%252C%2520code%252C%2520supporting%2520tools%252C%250Adatasets%252C%2520and%2520detailed%2520instruction-tuning%2520and%2520evaluation%2520recipes.%2520We%2520hope%2520our%250Arelease%2520will%2520inspire%2520and%2520accelerate%2520advancements%2520in%2520multimodal%2520systems%2520and%250Avisual%2520representation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16860v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cambrian-1%3A%20A%20Fully%20Open%2C%20Vision-Centric%20Exploration%20of%20Multimodal%20LLMs&entry.906535625=Shengbang%20Tong%20and%20Ellis%20Brown%20and%20Penghao%20Wu%20and%20Sanghyun%20Woo%20and%20Manoj%20Middepogu%20and%20Sai%20Charitha%20Akula%20and%20Jihan%20Yang%20and%20Shusheng%20Yang%20and%20Adithya%20Iyer%20and%20Xichen%20Pan%20and%20Ziteng%20Wang%20and%20Rob%20Fergus%20and%20Yann%20LeCun%20and%20Saining%20Xie&entry.1292438233=%20%20We%20introduce%20Cambrian-1%2C%20a%20family%20of%20multimodal%20LLMs%20%28MLLMs%29%20designed%20with%20a%0Avision-centric%20approach.%20While%20stronger%20language%20models%20can%20enhance%20multimodal%0Acapabilities%2C%20the%20design%20choices%20for%20vision%20components%20are%20often%20insufficiently%0Aexplored%20and%20disconnected%20from%20visual%20representation%20learning%20research.%20This%0Agap%20hinders%20accurate%20sensory%20grounding%20in%20real-world%20scenarios.%20Our%20study%20uses%0ALLMs%20and%20visual%20instruction%20tuning%20as%20an%20interface%20to%20evaluate%20various%20visual%0Arepresentations%2C%20offering%20new%20insights%20into%20different%20models%20and%20architectures%0A--%20self-supervised%2C%20strongly%20supervised%2C%20or%20combinations%20thereof%20--%20based%20on%0Aexperiments%20with%20over%2020%20vision%20encoders.%20We%20critically%20examine%20existing%20MLLM%0Abenchmarks%2C%20address%20the%20difficulties%20involved%20in%20consolidating%20and%20interpreting%0Aresults%20from%20various%20tasks%2C%20and%20introduce%20a%20new%20vision-centric%20benchmark%2C%0ACV-Bench.%20To%20further%20improve%20visual%20grounding%2C%20we%20propose%20the%20Spatial%20Vision%0AAggregator%20%28SVA%29%2C%20a%20dynamic%20and%20spatially-aware%20connector%20that%20integrates%0Ahigh-resolution%20vision%20features%20with%20LLMs%20while%20reducing%20the%20number%20of%20tokens.%0AAdditionally%2C%20we%20discuss%20the%20curation%20of%20high-quality%20visual%20instruction-tuning%0Adata%20from%20publicly%20available%20sources%2C%20emphasizing%20the%20importance%20of%20data%20source%0Abalancing%20and%20distribution%20ratio.%20Collectively%2C%20Cambrian-1%20not%20only%20achieves%0Astate-of-the-art%20performance%20but%20also%20serves%20as%20a%20comprehensive%2C%20open%20cookbook%0Afor%20instruction-tuned%20MLLMs.%20We%20provide%20model%20weights%2C%20code%2C%20supporting%20tools%2C%0Adatasets%2C%20and%20detailed%20instruction-tuning%20and%20evaluation%20recipes.%20We%20hope%20our%0Arelease%20will%20inspire%20and%20accelerate%20advancements%20in%20multimodal%20systems%20and%0Avisual%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16860v2&entry.124074799=Read"},
{"title": "FreeSim: Toward Free-viewpoint Camera Simulation in Driving Scenes", "author": "Lue Fan and Hao Zhang and Qitai Wang and Hongsheng Li and Zhaoxiang Zhang", "abstract": "  We propose FreeSim, a camera simulation method for autonomous driving.\nFreeSim emphasizes high-quality rendering from viewpoints beyond the recorded\nego trajectories. In such viewpoints, previous methods have unacceptable\ndegradation because the training data of these viewpoints is unavailable. To\naddress such data scarcity, we first propose a generative enhancement model\nwith a matched data construction strategy. The resulting model can generate\nhigh-quality images in a viewpoint slightly deviated from the recorded\ntrajectories, conditioned on the degraded rendering of this viewpoint. We then\npropose a progressive reconstruction strategy, which progressively adds\ngenerated images of unrecorded views into the reconstruction process, starting\nfrom slightly off-trajectory viewpoints and moving progressively farther away.\nWith this progressive generation-reconstruction pipeline, FreeSim supports\nhigh-quality off-trajectory view synthesis under large deviations of more than\n3 meters.\n", "link": "http://arxiv.org/abs/2412.03566v1", "date": "2024-12-04", "relevancy": 2.9747, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6044}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6044}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeSim%3A%20Toward%20Free-viewpoint%20Camera%20Simulation%20in%20Driving%20Scenes&body=Title%3A%20FreeSim%3A%20Toward%20Free-viewpoint%20Camera%20Simulation%20in%20Driving%20Scenes%0AAuthor%3A%20Lue%20Fan%20and%20Hao%20Zhang%20and%20Qitai%20Wang%20and%20Hongsheng%20Li%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20We%20propose%20FreeSim%2C%20a%20camera%20simulation%20method%20for%20autonomous%20driving.%0AFreeSim%20emphasizes%20high-quality%20rendering%20from%20viewpoints%20beyond%20the%20recorded%0Aego%20trajectories.%20In%20such%20viewpoints%2C%20previous%20methods%20have%20unacceptable%0Adegradation%20because%20the%20training%20data%20of%20these%20viewpoints%20is%20unavailable.%20To%0Aaddress%20such%20data%20scarcity%2C%20we%20first%20propose%20a%20generative%20enhancement%20model%0Awith%20a%20matched%20data%20construction%20strategy.%20The%20resulting%20model%20can%20generate%0Ahigh-quality%20images%20in%20a%20viewpoint%20slightly%20deviated%20from%20the%20recorded%0Atrajectories%2C%20conditioned%20on%20the%20degraded%20rendering%20of%20this%20viewpoint.%20We%20then%0Apropose%20a%20progressive%20reconstruction%20strategy%2C%20which%20progressively%20adds%0Agenerated%20images%20of%20unrecorded%20views%20into%20the%20reconstruction%20process%2C%20starting%0Afrom%20slightly%20off-trajectory%20viewpoints%20and%20moving%20progressively%20farther%20away.%0AWith%20this%20progressive%20generation-reconstruction%20pipeline%2C%20FreeSim%20supports%0Ahigh-quality%20off-trajectory%20view%20synthesis%20under%20large%20deviations%20of%20more%20than%0A3%20meters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeSim%253A%2520Toward%2520Free-viewpoint%2520Camera%2520Simulation%2520in%2520Driving%2520Scenes%26entry.906535625%3DLue%2520Fan%2520and%2520Hao%2520Zhang%2520and%2520Qitai%2520Wang%2520and%2520Hongsheng%2520Li%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520We%2520propose%2520FreeSim%252C%2520a%2520camera%2520simulation%2520method%2520for%2520autonomous%2520driving.%250AFreeSim%2520emphasizes%2520high-quality%2520rendering%2520from%2520viewpoints%2520beyond%2520the%2520recorded%250Aego%2520trajectories.%2520In%2520such%2520viewpoints%252C%2520previous%2520methods%2520have%2520unacceptable%250Adegradation%2520because%2520the%2520training%2520data%2520of%2520these%2520viewpoints%2520is%2520unavailable.%2520To%250Aaddress%2520such%2520data%2520scarcity%252C%2520we%2520first%2520propose%2520a%2520generative%2520enhancement%2520model%250Awith%2520a%2520matched%2520data%2520construction%2520strategy.%2520The%2520resulting%2520model%2520can%2520generate%250Ahigh-quality%2520images%2520in%2520a%2520viewpoint%2520slightly%2520deviated%2520from%2520the%2520recorded%250Atrajectories%252C%2520conditioned%2520on%2520the%2520degraded%2520rendering%2520of%2520this%2520viewpoint.%2520We%2520then%250Apropose%2520a%2520progressive%2520reconstruction%2520strategy%252C%2520which%2520progressively%2520adds%250Agenerated%2520images%2520of%2520unrecorded%2520views%2520into%2520the%2520reconstruction%2520process%252C%2520starting%250Afrom%2520slightly%2520off-trajectory%2520viewpoints%2520and%2520moving%2520progressively%2520farther%2520away.%250AWith%2520this%2520progressive%2520generation-reconstruction%2520pipeline%252C%2520FreeSim%2520supports%250Ahigh-quality%2520off-trajectory%2520view%2520synthesis%2520under%2520large%2520deviations%2520of%2520more%2520than%250A3%2520meters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeSim%3A%20Toward%20Free-viewpoint%20Camera%20Simulation%20in%20Driving%20Scenes&entry.906535625=Lue%20Fan%20and%20Hao%20Zhang%20and%20Qitai%20Wang%20and%20Hongsheng%20Li%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20We%20propose%20FreeSim%2C%20a%20camera%20simulation%20method%20for%20autonomous%20driving.%0AFreeSim%20emphasizes%20high-quality%20rendering%20from%20viewpoints%20beyond%20the%20recorded%0Aego%20trajectories.%20In%20such%20viewpoints%2C%20previous%20methods%20have%20unacceptable%0Adegradation%20because%20the%20training%20data%20of%20these%20viewpoints%20is%20unavailable.%20To%0Aaddress%20such%20data%20scarcity%2C%20we%20first%20propose%20a%20generative%20enhancement%20model%0Awith%20a%20matched%20data%20construction%20strategy.%20The%20resulting%20model%20can%20generate%0Ahigh-quality%20images%20in%20a%20viewpoint%20slightly%20deviated%20from%20the%20recorded%0Atrajectories%2C%20conditioned%20on%20the%20degraded%20rendering%20of%20this%20viewpoint.%20We%20then%0Apropose%20a%20progressive%20reconstruction%20strategy%2C%20which%20progressively%20adds%0Agenerated%20images%20of%20unrecorded%20views%20into%20the%20reconstruction%20process%2C%20starting%0Afrom%20slightly%20off-trajectory%20viewpoints%20and%20moving%20progressively%20farther%20away.%0AWith%20this%20progressive%20generation-reconstruction%20pipeline%2C%20FreeSim%20supports%0Ahigh-quality%20off-trajectory%20view%20synthesis%20under%20large%20deviations%20of%20more%20than%0A3%20meters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03566v1&entry.124074799=Read"},
{"title": "AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and\n  Pruning", "author": "Yiwu Zhong and Zhuoming Liu and Yin Li and Liwei Wang", "abstract": "  Large language models (LLMs) have enabled the creation of multi-modal LLMs\nthat exhibit strong comprehension of visual data such as images and videos.\nHowever, these models usually rely on extensive visual tokens from visual\nencoders, leading to high computational demands, which limits their\napplicability in resource-constrained environments and for long-context tasks.\nIn this work, we propose a training-free adaptive inference method for\nmulti-modal LLMs that can accommodate a broad range of efficiency requirements\nwith a minimum performance drop. Our method consists of a) iterative token\nmerging based on embedding similarity before LLMs, and b) progressive token\npruning within LLM layers based on multi-modal importance. With a minimalist\ndesign, our method can be applied to both video and image LLMs. Extensive\nexperiments on diverse video and image benchmarks demonstrate that, our method\nsubstantially reduces computation load (e.g., a $\\textbf{7-fold}$ reduction in\nFLOPs) while preserving the performance of video and image LLMs. Further, under\na similar computational cost, our method outperforms the state-of-the-art\nmethods in long video understanding (e.g., $\\textbf{+4.6}$ on MLVU).\nAdditionally, our in-depth analysis provides insights into token redundancy and\nLLM layer behaviors, offering guidance for future research in designing\nefficient multi-modal LLMs. Our code will be available at\nhttps://github.com/LaVi-Lab/AIM.\n", "link": "http://arxiv.org/abs/2412.03248v1", "date": "2024-12-04", "relevancy": 2.943, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6302}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5678}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIM%3A%20Adaptive%20Inference%20of%20Multi-Modal%20LLMs%20via%20Token%20Merging%20and%0A%20%20Pruning&body=Title%3A%20AIM%3A%20Adaptive%20Inference%20of%20Multi-Modal%20LLMs%20via%20Token%20Merging%20and%0A%20%20Pruning%0AAuthor%3A%20Yiwu%20Zhong%20and%20Zhuoming%20Liu%20and%20Yin%20Li%20and%20Liwei%20Wang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20enabled%20the%20creation%20of%20multi-modal%20LLMs%0Athat%20exhibit%20strong%20comprehension%20of%20visual%20data%20such%20as%20images%20and%20videos.%0AHowever%2C%20these%20models%20usually%20rely%20on%20extensive%20visual%20tokens%20from%20visual%0Aencoders%2C%20leading%20to%20high%20computational%20demands%2C%20which%20limits%20their%0Aapplicability%20in%20resource-constrained%20environments%20and%20for%20long-context%20tasks.%0AIn%20this%20work%2C%20we%20propose%20a%20training-free%20adaptive%20inference%20method%20for%0Amulti-modal%20LLMs%20that%20can%20accommodate%20a%20broad%20range%20of%20efficiency%20requirements%0Awith%20a%20minimum%20performance%20drop.%20Our%20method%20consists%20of%20a%29%20iterative%20token%0Amerging%20based%20on%20embedding%20similarity%20before%20LLMs%2C%20and%20b%29%20progressive%20token%0Apruning%20within%20LLM%20layers%20based%20on%20multi-modal%20importance.%20With%20a%20minimalist%0Adesign%2C%20our%20method%20can%20be%20applied%20to%20both%20video%20and%20image%20LLMs.%20Extensive%0Aexperiments%20on%20diverse%20video%20and%20image%20benchmarks%20demonstrate%20that%2C%20our%20method%0Asubstantially%20reduces%20computation%20load%20%28e.g.%2C%20a%20%24%5Ctextbf%7B7-fold%7D%24%20reduction%20in%0AFLOPs%29%20while%20preserving%20the%20performance%20of%20video%20and%20image%20LLMs.%20Further%2C%20under%0Aa%20similar%20computational%20cost%2C%20our%20method%20outperforms%20the%20state-of-the-art%0Amethods%20in%20long%20video%20understanding%20%28e.g.%2C%20%24%5Ctextbf%7B%2B4.6%7D%24%20on%20MLVU%29.%0AAdditionally%2C%20our%20in-depth%20analysis%20provides%20insights%20into%20token%20redundancy%20and%0ALLM%20layer%20behaviors%2C%20offering%20guidance%20for%20future%20research%20in%20designing%0Aefficient%20multi-modal%20LLMs.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/LaVi-Lab/AIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIM%253A%2520Adaptive%2520Inference%2520of%2520Multi-Modal%2520LLMs%2520via%2520Token%2520Merging%2520and%250A%2520%2520Pruning%26entry.906535625%3DYiwu%2520Zhong%2520and%2520Zhuoming%2520Liu%2520and%2520Yin%2520Li%2520and%2520Liwei%2520Wang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520enabled%2520the%2520creation%2520of%2520multi-modal%2520LLMs%250Athat%2520exhibit%2520strong%2520comprehension%2520of%2520visual%2520data%2520such%2520as%2520images%2520and%2520videos.%250AHowever%252C%2520these%2520models%2520usually%2520rely%2520on%2520extensive%2520visual%2520tokens%2520from%2520visual%250Aencoders%252C%2520leading%2520to%2520high%2520computational%2520demands%252C%2520which%2520limits%2520their%250Aapplicability%2520in%2520resource-constrained%2520environments%2520and%2520for%2520long-context%2520tasks.%250AIn%2520this%2520work%252C%2520we%2520propose%2520a%2520training-free%2520adaptive%2520inference%2520method%2520for%250Amulti-modal%2520LLMs%2520that%2520can%2520accommodate%2520a%2520broad%2520range%2520of%2520efficiency%2520requirements%250Awith%2520a%2520minimum%2520performance%2520drop.%2520Our%2520method%2520consists%2520of%2520a%2529%2520iterative%2520token%250Amerging%2520based%2520on%2520embedding%2520similarity%2520before%2520LLMs%252C%2520and%2520b%2529%2520progressive%2520token%250Apruning%2520within%2520LLM%2520layers%2520based%2520on%2520multi-modal%2520importance.%2520With%2520a%2520minimalist%250Adesign%252C%2520our%2520method%2520can%2520be%2520applied%2520to%2520both%2520video%2520and%2520image%2520LLMs.%2520Extensive%250Aexperiments%2520on%2520diverse%2520video%2520and%2520image%2520benchmarks%2520demonstrate%2520that%252C%2520our%2520method%250Asubstantially%2520reduces%2520computation%2520load%2520%2528e.g.%252C%2520a%2520%2524%255Ctextbf%257B7-fold%257D%2524%2520reduction%2520in%250AFLOPs%2529%2520while%2520preserving%2520the%2520performance%2520of%2520video%2520and%2520image%2520LLMs.%2520Further%252C%2520under%250Aa%2520similar%2520computational%2520cost%252C%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%250Amethods%2520in%2520long%2520video%2520understanding%2520%2528e.g.%252C%2520%2524%255Ctextbf%257B%252B4.6%257D%2524%2520on%2520MLVU%2529.%250AAdditionally%252C%2520our%2520in-depth%2520analysis%2520provides%2520insights%2520into%2520token%2520redundancy%2520and%250ALLM%2520layer%2520behaviors%252C%2520offering%2520guidance%2520for%2520future%2520research%2520in%2520designing%250Aefficient%2520multi-modal%2520LLMs.%2520Our%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/LaVi-Lab/AIM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIM%3A%20Adaptive%20Inference%20of%20Multi-Modal%20LLMs%20via%20Token%20Merging%20and%0A%20%20Pruning&entry.906535625=Yiwu%20Zhong%20and%20Zhuoming%20Liu%20and%20Yin%20Li%20and%20Liwei%20Wang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20enabled%20the%20creation%20of%20multi-modal%20LLMs%0Athat%20exhibit%20strong%20comprehension%20of%20visual%20data%20such%20as%20images%20and%20videos.%0AHowever%2C%20these%20models%20usually%20rely%20on%20extensive%20visual%20tokens%20from%20visual%0Aencoders%2C%20leading%20to%20high%20computational%20demands%2C%20which%20limits%20their%0Aapplicability%20in%20resource-constrained%20environments%20and%20for%20long-context%20tasks.%0AIn%20this%20work%2C%20we%20propose%20a%20training-free%20adaptive%20inference%20method%20for%0Amulti-modal%20LLMs%20that%20can%20accommodate%20a%20broad%20range%20of%20efficiency%20requirements%0Awith%20a%20minimum%20performance%20drop.%20Our%20method%20consists%20of%20a%29%20iterative%20token%0Amerging%20based%20on%20embedding%20similarity%20before%20LLMs%2C%20and%20b%29%20progressive%20token%0Apruning%20within%20LLM%20layers%20based%20on%20multi-modal%20importance.%20With%20a%20minimalist%0Adesign%2C%20our%20method%20can%20be%20applied%20to%20both%20video%20and%20image%20LLMs.%20Extensive%0Aexperiments%20on%20diverse%20video%20and%20image%20benchmarks%20demonstrate%20that%2C%20our%20method%0Asubstantially%20reduces%20computation%20load%20%28e.g.%2C%20a%20%24%5Ctextbf%7B7-fold%7D%24%20reduction%20in%0AFLOPs%29%20while%20preserving%20the%20performance%20of%20video%20and%20image%20LLMs.%20Further%2C%20under%0Aa%20similar%20computational%20cost%2C%20our%20method%20outperforms%20the%20state-of-the-art%0Amethods%20in%20long%20video%20understanding%20%28e.g.%2C%20%24%5Ctextbf%7B%2B4.6%7D%24%20on%20MLVU%29.%0AAdditionally%2C%20our%20in-depth%20analysis%20provides%20insights%20into%20token%20redundancy%20and%0ALLM%20layer%20behaviors%2C%20offering%20guidance%20for%20future%20research%20in%20designing%0Aefficient%20multi-modal%20LLMs.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/LaVi-Lab/AIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03248v1&entry.124074799=Read"},
{"title": "GERD: Geometric event response data generation", "author": "Jens Egholm Pedersen and Dimitris Korakovounis and J\u00f6rg Conradt", "abstract": "  Event-based vision sensors are appealing because of their time resolution,\nhigher dynamic range, and low-power consumption. They also provide data that is\nfundamentally different from conventional frame-based cameras: events are\nsparse, discrete, and require integration in time. Unlike conventional models\ngrounded in established geometric and physical principles, event-based models\nlack comparable foundations. We introduce a method to generate event-based data\nunder controlled transformations. Specifically, we subject a prototypical\nobject to transformations that change over time to produce carefully curated\nevent videos. We hope this work simplifies studies for geometric approaches in\nevent-based vision. GERD is available at https://github.com/ncskth/gerd\n", "link": "http://arxiv.org/abs/2412.03259v1", "date": "2024-12-04", "relevancy": 2.9367, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6086}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.591}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GERD%3A%20Geometric%20event%20response%20data%20generation&body=Title%3A%20GERD%3A%20Geometric%20event%20response%20data%20generation%0AAuthor%3A%20Jens%20Egholm%20Pedersen%20and%20Dimitris%20Korakovounis%20and%20J%C3%B6rg%20Conradt%0AAbstract%3A%20%20%20Event-based%20vision%20sensors%20are%20appealing%20because%20of%20their%20time%20resolution%2C%0Ahigher%20dynamic%20range%2C%20and%20low-power%20consumption.%20They%20also%20provide%20data%20that%20is%0Afundamentally%20different%20from%20conventional%20frame-based%20cameras%3A%20events%20are%0Asparse%2C%20discrete%2C%20and%20require%20integration%20in%20time.%20Unlike%20conventional%20models%0Agrounded%20in%20established%20geometric%20and%20physical%20principles%2C%20event-based%20models%0Alack%20comparable%20foundations.%20We%20introduce%20a%20method%20to%20generate%20event-based%20data%0Aunder%20controlled%20transformations.%20Specifically%2C%20we%20subject%20a%20prototypical%0Aobject%20to%20transformations%20that%20change%20over%20time%20to%20produce%20carefully%20curated%0Aevent%20videos.%20We%20hope%20this%20work%20simplifies%20studies%20for%20geometric%20approaches%20in%0Aevent-based%20vision.%20GERD%20is%20available%20at%20https%3A//github.com/ncskth/gerd%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGERD%253A%2520Geometric%2520event%2520response%2520data%2520generation%26entry.906535625%3DJens%2520Egholm%2520Pedersen%2520and%2520Dimitris%2520Korakovounis%2520and%2520J%25C3%25B6rg%2520Conradt%26entry.1292438233%3D%2520%2520Event-based%2520vision%2520sensors%2520are%2520appealing%2520because%2520of%2520their%2520time%2520resolution%252C%250Ahigher%2520dynamic%2520range%252C%2520and%2520low-power%2520consumption.%2520They%2520also%2520provide%2520data%2520that%2520is%250Afundamentally%2520different%2520from%2520conventional%2520frame-based%2520cameras%253A%2520events%2520are%250Asparse%252C%2520discrete%252C%2520and%2520require%2520integration%2520in%2520time.%2520Unlike%2520conventional%2520models%250Agrounded%2520in%2520established%2520geometric%2520and%2520physical%2520principles%252C%2520event-based%2520models%250Alack%2520comparable%2520foundations.%2520We%2520introduce%2520a%2520method%2520to%2520generate%2520event-based%2520data%250Aunder%2520controlled%2520transformations.%2520Specifically%252C%2520we%2520subject%2520a%2520prototypical%250Aobject%2520to%2520transformations%2520that%2520change%2520over%2520time%2520to%2520produce%2520carefully%2520curated%250Aevent%2520videos.%2520We%2520hope%2520this%2520work%2520simplifies%2520studies%2520for%2520geometric%2520approaches%2520in%250Aevent-based%2520vision.%2520GERD%2520is%2520available%2520at%2520https%253A//github.com/ncskth/gerd%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GERD%3A%20Geometric%20event%20response%20data%20generation&entry.906535625=Jens%20Egholm%20Pedersen%20and%20Dimitris%20Korakovounis%20and%20J%C3%B6rg%20Conradt&entry.1292438233=%20%20Event-based%20vision%20sensors%20are%20appealing%20because%20of%20their%20time%20resolution%2C%0Ahigher%20dynamic%20range%2C%20and%20low-power%20consumption.%20They%20also%20provide%20data%20that%20is%0Afundamentally%20different%20from%20conventional%20frame-based%20cameras%3A%20events%20are%0Asparse%2C%20discrete%2C%20and%20require%20integration%20in%20time.%20Unlike%20conventional%20models%0Agrounded%20in%20established%20geometric%20and%20physical%20principles%2C%20event-based%20models%0Alack%20comparable%20foundations.%20We%20introduce%20a%20method%20to%20generate%20event-based%20data%0Aunder%20controlled%20transformations.%20Specifically%2C%20we%20subject%20a%20prototypical%0Aobject%20to%20transformations%20that%20change%20over%20time%20to%20produce%20carefully%20curated%0Aevent%20videos.%20We%20hope%20this%20work%20simplifies%20studies%20for%20geometric%20approaches%20in%0Aevent-based%20vision.%20GERD%20is%20available%20at%20https%3A//github.com/ncskth/gerd%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03259v1&entry.124074799=Read"},
{"title": "Dense Scene Reconstruction from Light-Field Images Affected by Rolling\n  Shutter", "author": "Hermes McGriff and Renato Martins and Nicolas Andreff and Cedric Demonceaux", "abstract": "  This paper presents a dense depth estimation approach from light-field (LF)\nimages that is able to compensate for strong rolling shutter (RS) effects. Our\nmethod estimates RS compensated views and dense RS compensated disparity maps.\nWe present a two-stage method based on a 2D Gaussians Splatting that allows for\na ``render and compare\" strategy with a point cloud formulation. In the first\nstage, a subset of sub-aperture images is used to estimate an RS agnostic 3D\nshape that is related to the scene target shape ``up to a motion\". In the\nsecond stage, the deformation of the 3D shape is computed by estimating an\nadmissible camera motion. We demonstrate the effectiveness and advantages of\nthis approach through several experiments conducted for different scenes and\ntypes of motions. Due to lack of suitable datasets for evaluation, we also\npresent a new carefully designed synthetic dataset of RS LF images. The source\ncode, trained models and dataset will be made publicly available at:\nhttps://github.com/ICB-Vision-AI/DenseRSLF\n", "link": "http://arxiv.org/abs/2412.03518v1", "date": "2024-12-04", "relevancy": 2.929, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6469}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5584}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dense%20Scene%20Reconstruction%20from%20Light-Field%20Images%20Affected%20by%20Rolling%0A%20%20Shutter&body=Title%3A%20Dense%20Scene%20Reconstruction%20from%20Light-Field%20Images%20Affected%20by%20Rolling%0A%20%20Shutter%0AAuthor%3A%20Hermes%20McGriff%20and%20Renato%20Martins%20and%20Nicolas%20Andreff%20and%20Cedric%20Demonceaux%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20dense%20depth%20estimation%20approach%20from%20light-field%20%28LF%29%0Aimages%20that%20is%20able%20to%20compensate%20for%20strong%20rolling%20shutter%20%28RS%29%20effects.%20Our%0Amethod%20estimates%20RS%20compensated%20views%20and%20dense%20RS%20compensated%20disparity%20maps.%0AWe%20present%20a%20two-stage%20method%20based%20on%20a%202D%20Gaussians%20Splatting%20that%20allows%20for%0Aa%20%60%60render%20and%20compare%22%20strategy%20with%20a%20point%20cloud%20formulation.%20In%20the%20first%0Astage%2C%20a%20subset%20of%20sub-aperture%20images%20is%20used%20to%20estimate%20an%20RS%20agnostic%203D%0Ashape%20that%20is%20related%20to%20the%20scene%20target%20shape%20%60%60up%20to%20a%20motion%22.%20In%20the%0Asecond%20stage%2C%20the%20deformation%20of%20the%203D%20shape%20is%20computed%20by%20estimating%20an%0Aadmissible%20camera%20motion.%20We%20demonstrate%20the%20effectiveness%20and%20advantages%20of%0Athis%20approach%20through%20several%20experiments%20conducted%20for%20different%20scenes%20and%0Atypes%20of%20motions.%20Due%20to%20lack%20of%20suitable%20datasets%20for%20evaluation%2C%20we%20also%0Apresent%20a%20new%20carefully%20designed%20synthetic%20dataset%20of%20RS%20LF%20images.%20The%20source%0Acode%2C%20trained%20models%20and%20dataset%20will%20be%20made%20publicly%20available%20at%3A%0Ahttps%3A//github.com/ICB-Vision-AI/DenseRSLF%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDense%2520Scene%2520Reconstruction%2520from%2520Light-Field%2520Images%2520Affected%2520by%2520Rolling%250A%2520%2520Shutter%26entry.906535625%3DHermes%2520McGriff%2520and%2520Renato%2520Martins%2520and%2520Nicolas%2520Andreff%2520and%2520Cedric%2520Demonceaux%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520dense%2520depth%2520estimation%2520approach%2520from%2520light-field%2520%2528LF%2529%250Aimages%2520that%2520is%2520able%2520to%2520compensate%2520for%2520strong%2520rolling%2520shutter%2520%2528RS%2529%2520effects.%2520Our%250Amethod%2520estimates%2520RS%2520compensated%2520views%2520and%2520dense%2520RS%2520compensated%2520disparity%2520maps.%250AWe%2520present%2520a%2520two-stage%2520method%2520based%2520on%2520a%25202D%2520Gaussians%2520Splatting%2520that%2520allows%2520for%250Aa%2520%2560%2560render%2520and%2520compare%2522%2520strategy%2520with%2520a%2520point%2520cloud%2520formulation.%2520In%2520the%2520first%250Astage%252C%2520a%2520subset%2520of%2520sub-aperture%2520images%2520is%2520used%2520to%2520estimate%2520an%2520RS%2520agnostic%25203D%250Ashape%2520that%2520is%2520related%2520to%2520the%2520scene%2520target%2520shape%2520%2560%2560up%2520to%2520a%2520motion%2522.%2520In%2520the%250Asecond%2520stage%252C%2520the%2520deformation%2520of%2520the%25203D%2520shape%2520is%2520computed%2520by%2520estimating%2520an%250Aadmissible%2520camera%2520motion.%2520We%2520demonstrate%2520the%2520effectiveness%2520and%2520advantages%2520of%250Athis%2520approach%2520through%2520several%2520experiments%2520conducted%2520for%2520different%2520scenes%2520and%250Atypes%2520of%2520motions.%2520Due%2520to%2520lack%2520of%2520suitable%2520datasets%2520for%2520evaluation%252C%2520we%2520also%250Apresent%2520a%2520new%2520carefully%2520designed%2520synthetic%2520dataset%2520of%2520RS%2520LF%2520images.%2520The%2520source%250Acode%252C%2520trained%2520models%2520and%2520dataset%2520will%2520be%2520made%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/ICB-Vision-AI/DenseRSLF%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense%20Scene%20Reconstruction%20from%20Light-Field%20Images%20Affected%20by%20Rolling%0A%20%20Shutter&entry.906535625=Hermes%20McGriff%20and%20Renato%20Martins%20and%20Nicolas%20Andreff%20and%20Cedric%20Demonceaux&entry.1292438233=%20%20This%20paper%20presents%20a%20dense%20depth%20estimation%20approach%20from%20light-field%20%28LF%29%0Aimages%20that%20is%20able%20to%20compensate%20for%20strong%20rolling%20shutter%20%28RS%29%20effects.%20Our%0Amethod%20estimates%20RS%20compensated%20views%20and%20dense%20RS%20compensated%20disparity%20maps.%0AWe%20present%20a%20two-stage%20method%20based%20on%20a%202D%20Gaussians%20Splatting%20that%20allows%20for%0Aa%20%60%60render%20and%20compare%22%20strategy%20with%20a%20point%20cloud%20formulation.%20In%20the%20first%0Astage%2C%20a%20subset%20of%20sub-aperture%20images%20is%20used%20to%20estimate%20an%20RS%20agnostic%203D%0Ashape%20that%20is%20related%20to%20the%20scene%20target%20shape%20%60%60up%20to%20a%20motion%22.%20In%20the%0Asecond%20stage%2C%20the%20deformation%20of%20the%203D%20shape%20is%20computed%20by%20estimating%20an%0Aadmissible%20camera%20motion.%20We%20demonstrate%20the%20effectiveness%20and%20advantages%20of%0Athis%20approach%20through%20several%20experiments%20conducted%20for%20different%20scenes%20and%0Atypes%20of%20motions.%20Due%20to%20lack%20of%20suitable%20datasets%20for%20evaluation%2C%20we%20also%0Apresent%20a%20new%20carefully%20designed%20synthetic%20dataset%20of%20RS%20LF%20images.%20The%20source%0Acode%2C%20trained%20models%20and%20dataset%20will%20be%20made%20publicly%20available%20at%3A%0Ahttps%3A//github.com/ICB-Vision-AI/DenseRSLF%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03518v1&entry.124074799=Read"},
{"title": "VoxNeRF: Bridging Voxel Representation and Neural Radiance Fields for\n  Enhanced Indoor View Synthesis", "author": "Sen Wang and Qing Cheng and Stefano Gasperini and Wei Zhang and Shun-Cheng Wu and Niclas Zeller and Daniel Cremers and Nassir Navab", "abstract": "  The generation of high-fidelity view synthesis is essential for robotic\nnavigation and interaction but remains challenging, particularly in indoor\nenvironments and real-time scenarios. Existing techniques often require\nsignificant computational resources for both training and rendering, and they\nfrequently result in suboptimal 3D representations due to insufficient\ngeometric structuring. To address these limitations, we introduce VoxNeRF, a\nnovel approach that utilizes easy-to-obtain geometry priors to enhance both the\nquality and efficiency of neural indoor reconstruction and novel view\nsynthesis. We propose an efficient voxel-guided sampling technique that\nallocates computational resources selectively to the most relevant segments of\nrays based on a voxel-encoded geometry prior, significantly reducing training\nand rendering time. Additionally, we incorporate a robust depth loss to improve\nreconstruction and rendering quality in sparse view settings. Our approach is\nvalidated with extensive experiments on ScanNet and ScanNet++ where VoxNeRF\noutperforms existing state-of-the-art methods and establishes a new benchmark\nfor indoor immersive interpolation and extrapolation settings.\n", "link": "http://arxiv.org/abs/2311.05289v2", "date": "2024-12-04", "relevancy": 2.9285, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6082}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5784}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VoxNeRF%3A%20Bridging%20Voxel%20Representation%20and%20Neural%20Radiance%20Fields%20for%0A%20%20Enhanced%20Indoor%20View%20Synthesis&body=Title%3A%20VoxNeRF%3A%20Bridging%20Voxel%20Representation%20and%20Neural%20Radiance%20Fields%20for%0A%20%20Enhanced%20Indoor%20View%20Synthesis%0AAuthor%3A%20Sen%20Wang%20and%20Qing%20Cheng%20and%20Stefano%20Gasperini%20and%20Wei%20Zhang%20and%20Shun-Cheng%20Wu%20and%20Niclas%20Zeller%20and%20Daniel%20Cremers%20and%20Nassir%20Navab%0AAbstract%3A%20%20%20The%20generation%20of%20high-fidelity%20view%20synthesis%20is%20essential%20for%20robotic%0Anavigation%20and%20interaction%20but%20remains%20challenging%2C%20particularly%20in%20indoor%0Aenvironments%20and%20real-time%20scenarios.%20Existing%20techniques%20often%20require%0Asignificant%20computational%20resources%20for%20both%20training%20and%20rendering%2C%20and%20they%0Afrequently%20result%20in%20suboptimal%203D%20representations%20due%20to%20insufficient%0Ageometric%20structuring.%20To%20address%20these%20limitations%2C%20we%20introduce%20VoxNeRF%2C%20a%0Anovel%20approach%20that%20utilizes%20easy-to-obtain%20geometry%20priors%20to%20enhance%20both%20the%0Aquality%20and%20efficiency%20of%20neural%20indoor%20reconstruction%20and%20novel%20view%0Asynthesis.%20We%20propose%20an%20efficient%20voxel-guided%20sampling%20technique%20that%0Aallocates%20computational%20resources%20selectively%20to%20the%20most%20relevant%20segments%20of%0Arays%20based%20on%20a%20voxel-encoded%20geometry%20prior%2C%20significantly%20reducing%20training%0Aand%20rendering%20time.%20Additionally%2C%20we%20incorporate%20a%20robust%20depth%20loss%20to%20improve%0Areconstruction%20and%20rendering%20quality%20in%20sparse%20view%20settings.%20Our%20approach%20is%0Avalidated%20with%20extensive%20experiments%20on%20ScanNet%20and%20ScanNet%2B%2B%20where%20VoxNeRF%0Aoutperforms%20existing%20state-of-the-art%20methods%20and%20establishes%20a%20new%20benchmark%0Afor%20indoor%20immersive%20interpolation%20and%20extrapolation%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.05289v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoxNeRF%253A%2520Bridging%2520Voxel%2520Representation%2520and%2520Neural%2520Radiance%2520Fields%2520for%250A%2520%2520Enhanced%2520Indoor%2520View%2520Synthesis%26entry.906535625%3DSen%2520Wang%2520and%2520Qing%2520Cheng%2520and%2520Stefano%2520Gasperini%2520and%2520Wei%2520Zhang%2520and%2520Shun-Cheng%2520Wu%2520and%2520Niclas%2520Zeller%2520and%2520Daniel%2520Cremers%2520and%2520Nassir%2520Navab%26entry.1292438233%3D%2520%2520The%2520generation%2520of%2520high-fidelity%2520view%2520synthesis%2520is%2520essential%2520for%2520robotic%250Anavigation%2520and%2520interaction%2520but%2520remains%2520challenging%252C%2520particularly%2520in%2520indoor%250Aenvironments%2520and%2520real-time%2520scenarios.%2520Existing%2520techniques%2520often%2520require%250Asignificant%2520computational%2520resources%2520for%2520both%2520training%2520and%2520rendering%252C%2520and%2520they%250Afrequently%2520result%2520in%2520suboptimal%25203D%2520representations%2520due%2520to%2520insufficient%250Ageometric%2520structuring.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520VoxNeRF%252C%2520a%250Anovel%2520approach%2520that%2520utilizes%2520easy-to-obtain%2520geometry%2520priors%2520to%2520enhance%2520both%2520the%250Aquality%2520and%2520efficiency%2520of%2520neural%2520indoor%2520reconstruction%2520and%2520novel%2520view%250Asynthesis.%2520We%2520propose%2520an%2520efficient%2520voxel-guided%2520sampling%2520technique%2520that%250Aallocates%2520computational%2520resources%2520selectively%2520to%2520the%2520most%2520relevant%2520segments%2520of%250Arays%2520based%2520on%2520a%2520voxel-encoded%2520geometry%2520prior%252C%2520significantly%2520reducing%2520training%250Aand%2520rendering%2520time.%2520Additionally%252C%2520we%2520incorporate%2520a%2520robust%2520depth%2520loss%2520to%2520improve%250Areconstruction%2520and%2520rendering%2520quality%2520in%2520sparse%2520view%2520settings.%2520Our%2520approach%2520is%250Avalidated%2520with%2520extensive%2520experiments%2520on%2520ScanNet%2520and%2520ScanNet%252B%252B%2520where%2520VoxNeRF%250Aoutperforms%2520existing%2520state-of-the-art%2520methods%2520and%2520establishes%2520a%2520new%2520benchmark%250Afor%2520indoor%2520immersive%2520interpolation%2520and%2520extrapolation%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.05289v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VoxNeRF%3A%20Bridging%20Voxel%20Representation%20and%20Neural%20Radiance%20Fields%20for%0A%20%20Enhanced%20Indoor%20View%20Synthesis&entry.906535625=Sen%20Wang%20and%20Qing%20Cheng%20and%20Stefano%20Gasperini%20and%20Wei%20Zhang%20and%20Shun-Cheng%20Wu%20and%20Niclas%20Zeller%20and%20Daniel%20Cremers%20and%20Nassir%20Navab&entry.1292438233=%20%20The%20generation%20of%20high-fidelity%20view%20synthesis%20is%20essential%20for%20robotic%0Anavigation%20and%20interaction%20but%20remains%20challenging%2C%20particularly%20in%20indoor%0Aenvironments%20and%20real-time%20scenarios.%20Existing%20techniques%20often%20require%0Asignificant%20computational%20resources%20for%20both%20training%20and%20rendering%2C%20and%20they%0Afrequently%20result%20in%20suboptimal%203D%20representations%20due%20to%20insufficient%0Ageometric%20structuring.%20To%20address%20these%20limitations%2C%20we%20introduce%20VoxNeRF%2C%20a%0Anovel%20approach%20that%20utilizes%20easy-to-obtain%20geometry%20priors%20to%20enhance%20both%20the%0Aquality%20and%20efficiency%20of%20neural%20indoor%20reconstruction%20and%20novel%20view%0Asynthesis.%20We%20propose%20an%20efficient%20voxel-guided%20sampling%20technique%20that%0Aallocates%20computational%20resources%20selectively%20to%20the%20most%20relevant%20segments%20of%0Arays%20based%20on%20a%20voxel-encoded%20geometry%20prior%2C%20significantly%20reducing%20training%0Aand%20rendering%20time.%20Additionally%2C%20we%20incorporate%20a%20robust%20depth%20loss%20to%20improve%0Areconstruction%20and%20rendering%20quality%20in%20sparse%20view%20settings.%20Our%20approach%20is%0Avalidated%20with%20extensive%20experiments%20on%20ScanNet%20and%20ScanNet%2B%2B%20where%20VoxNeRF%0Aoutperforms%20existing%20state-of-the-art%20methods%20and%20establishes%20a%20new%20benchmark%0Afor%20indoor%20immersive%20interpolation%20and%20extrapolation%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.05289v2&entry.124074799=Read"},
{"title": "Seeing Beyond Views: Multi-View Driving Scene Video Generation with\n  Holistic Attention", "author": "Hannan Lu and Xiaohe Wu and Shudong Wang and Xiameng Qin and Xinyu Zhang and Junyu Han and Wangmeng Zuo and Ji Tao", "abstract": "  Generating multi-view videos for autonomous driving training has recently\ngained much attention, with the challenge of addressing both cross-view and\ncross-frame consistency. Existing methods typically apply decoupled attention\nmechanisms for spatial, temporal, and view dimensions. However, these\napproaches often struggle to maintain consistency across dimensions,\nparticularly when handling fast-moving objects that appear at different times\nand viewpoints. In this paper, we present CogDriving, a novel network designed\nfor synthesizing high-quality multi-view driving videos. CogDriving leverages a\nDiffusion Transformer architecture with holistic-4D attention modules, enabling\nsimultaneous associations across the spatial, temporal, and viewpoint\ndimensions. We also propose a lightweight controller tailored for CogDriving,\ni.e., Micro-Controller, which uses only 1.1% of the parameters of the standard\nControlNet, enabling precise control over Bird's-Eye-View layouts. To enhance\nthe generation of object instances crucial for autonomous driving, we propose a\nre-weighted learning objective, dynamically adjusting the learning weights for\nobject instances during training. CogDriving demonstrates strong performance on\nthe nuScenes validation set, achieving an FVD score of 37.8, highlighting its\nability to generate realistic driving videos. The project can be found at\nhttps://luhannan.github.io/CogDrivingPage/.\n", "link": "http://arxiv.org/abs/2412.03520v1", "date": "2024-12-04", "relevancy": 2.9257, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5885}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5885}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20Beyond%20Views%3A%20Multi-View%20Driving%20Scene%20Video%20Generation%20with%0A%20%20Holistic%20Attention&body=Title%3A%20Seeing%20Beyond%20Views%3A%20Multi-View%20Driving%20Scene%20Video%20Generation%20with%0A%20%20Holistic%20Attention%0AAuthor%3A%20Hannan%20Lu%20and%20Xiaohe%20Wu%20and%20Shudong%20Wang%20and%20Xiameng%20Qin%20and%20Xinyu%20Zhang%20and%20Junyu%20Han%20and%20Wangmeng%20Zuo%20and%20Ji%20Tao%0AAbstract%3A%20%20%20Generating%20multi-view%20videos%20for%20autonomous%20driving%20training%20has%20recently%0Agained%20much%20attention%2C%20with%20the%20challenge%20of%20addressing%20both%20cross-view%20and%0Across-frame%20consistency.%20Existing%20methods%20typically%20apply%20decoupled%20attention%0Amechanisms%20for%20spatial%2C%20temporal%2C%20and%20view%20dimensions.%20However%2C%20these%0Aapproaches%20often%20struggle%20to%20maintain%20consistency%20across%20dimensions%2C%0Aparticularly%20when%20handling%20fast-moving%20objects%20that%20appear%20at%20different%20times%0Aand%20viewpoints.%20In%20this%20paper%2C%20we%20present%20CogDriving%2C%20a%20novel%20network%20designed%0Afor%20synthesizing%20high-quality%20multi-view%20driving%20videos.%20CogDriving%20leverages%20a%0ADiffusion%20Transformer%20architecture%20with%20holistic-4D%20attention%20modules%2C%20enabling%0Asimultaneous%20associations%20across%20the%20spatial%2C%20temporal%2C%20and%20viewpoint%0Adimensions.%20We%20also%20propose%20a%20lightweight%20controller%20tailored%20for%20CogDriving%2C%0Ai.e.%2C%20Micro-Controller%2C%20which%20uses%20only%201.1%25%20of%20the%20parameters%20of%20the%20standard%0AControlNet%2C%20enabling%20precise%20control%20over%20Bird%27s-Eye-View%20layouts.%20To%20enhance%0Athe%20generation%20of%20object%20instances%20crucial%20for%20autonomous%20driving%2C%20we%20propose%20a%0Are-weighted%20learning%20objective%2C%20dynamically%20adjusting%20the%20learning%20weights%20for%0Aobject%20instances%20during%20training.%20CogDriving%20demonstrates%20strong%20performance%20on%0Athe%20nuScenes%20validation%20set%2C%20achieving%20an%20FVD%20score%20of%2037.8%2C%20highlighting%20its%0Aability%20to%20generate%20realistic%20driving%20videos.%20The%20project%20can%20be%20found%20at%0Ahttps%3A//luhannan.github.io/CogDrivingPage/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520Beyond%2520Views%253A%2520Multi-View%2520Driving%2520Scene%2520Video%2520Generation%2520with%250A%2520%2520Holistic%2520Attention%26entry.906535625%3DHannan%2520Lu%2520and%2520Xiaohe%2520Wu%2520and%2520Shudong%2520Wang%2520and%2520Xiameng%2520Qin%2520and%2520Xinyu%2520Zhang%2520and%2520Junyu%2520Han%2520and%2520Wangmeng%2520Zuo%2520and%2520Ji%2520Tao%26entry.1292438233%3D%2520%2520Generating%2520multi-view%2520videos%2520for%2520autonomous%2520driving%2520training%2520has%2520recently%250Agained%2520much%2520attention%252C%2520with%2520the%2520challenge%2520of%2520addressing%2520both%2520cross-view%2520and%250Across-frame%2520consistency.%2520Existing%2520methods%2520typically%2520apply%2520decoupled%2520attention%250Amechanisms%2520for%2520spatial%252C%2520temporal%252C%2520and%2520view%2520dimensions.%2520However%252C%2520these%250Aapproaches%2520often%2520struggle%2520to%2520maintain%2520consistency%2520across%2520dimensions%252C%250Aparticularly%2520when%2520handling%2520fast-moving%2520objects%2520that%2520appear%2520at%2520different%2520times%250Aand%2520viewpoints.%2520In%2520this%2520paper%252C%2520we%2520present%2520CogDriving%252C%2520a%2520novel%2520network%2520designed%250Afor%2520synthesizing%2520high-quality%2520multi-view%2520driving%2520videos.%2520CogDriving%2520leverages%2520a%250ADiffusion%2520Transformer%2520architecture%2520with%2520holistic-4D%2520attention%2520modules%252C%2520enabling%250Asimultaneous%2520associations%2520across%2520the%2520spatial%252C%2520temporal%252C%2520and%2520viewpoint%250Adimensions.%2520We%2520also%2520propose%2520a%2520lightweight%2520controller%2520tailored%2520for%2520CogDriving%252C%250Ai.e.%252C%2520Micro-Controller%252C%2520which%2520uses%2520only%25201.1%2525%2520of%2520the%2520parameters%2520of%2520the%2520standard%250AControlNet%252C%2520enabling%2520precise%2520control%2520over%2520Bird%2527s-Eye-View%2520layouts.%2520To%2520enhance%250Athe%2520generation%2520of%2520object%2520instances%2520crucial%2520for%2520autonomous%2520driving%252C%2520we%2520propose%2520a%250Are-weighted%2520learning%2520objective%252C%2520dynamically%2520adjusting%2520the%2520learning%2520weights%2520for%250Aobject%2520instances%2520during%2520training.%2520CogDriving%2520demonstrates%2520strong%2520performance%2520on%250Athe%2520nuScenes%2520validation%2520set%252C%2520achieving%2520an%2520FVD%2520score%2520of%252037.8%252C%2520highlighting%2520its%250Aability%2520to%2520generate%2520realistic%2520driving%2520videos.%2520The%2520project%2520can%2520be%2520found%2520at%250Ahttps%253A//luhannan.github.io/CogDrivingPage/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20Beyond%20Views%3A%20Multi-View%20Driving%20Scene%20Video%20Generation%20with%0A%20%20Holistic%20Attention&entry.906535625=Hannan%20Lu%20and%20Xiaohe%20Wu%20and%20Shudong%20Wang%20and%20Xiameng%20Qin%20and%20Xinyu%20Zhang%20and%20Junyu%20Han%20and%20Wangmeng%20Zuo%20and%20Ji%20Tao&entry.1292438233=%20%20Generating%20multi-view%20videos%20for%20autonomous%20driving%20training%20has%20recently%0Agained%20much%20attention%2C%20with%20the%20challenge%20of%20addressing%20both%20cross-view%20and%0Across-frame%20consistency.%20Existing%20methods%20typically%20apply%20decoupled%20attention%0Amechanisms%20for%20spatial%2C%20temporal%2C%20and%20view%20dimensions.%20However%2C%20these%0Aapproaches%20often%20struggle%20to%20maintain%20consistency%20across%20dimensions%2C%0Aparticularly%20when%20handling%20fast-moving%20objects%20that%20appear%20at%20different%20times%0Aand%20viewpoints.%20In%20this%20paper%2C%20we%20present%20CogDriving%2C%20a%20novel%20network%20designed%0Afor%20synthesizing%20high-quality%20multi-view%20driving%20videos.%20CogDriving%20leverages%20a%0ADiffusion%20Transformer%20architecture%20with%20holistic-4D%20attention%20modules%2C%20enabling%0Asimultaneous%20associations%20across%20the%20spatial%2C%20temporal%2C%20and%20viewpoint%0Adimensions.%20We%20also%20propose%20a%20lightweight%20controller%20tailored%20for%20CogDriving%2C%0Ai.e.%2C%20Micro-Controller%2C%20which%20uses%20only%201.1%25%20of%20the%20parameters%20of%20the%20standard%0AControlNet%2C%20enabling%20precise%20control%20over%20Bird%27s-Eye-View%20layouts.%20To%20enhance%0Athe%20generation%20of%20object%20instances%20crucial%20for%20autonomous%20driving%2C%20we%20propose%20a%0Are-weighted%20learning%20objective%2C%20dynamically%20adjusting%20the%20learning%20weights%20for%0Aobject%20instances%20during%20training.%20CogDriving%20demonstrates%20strong%20performance%20on%0Athe%20nuScenes%20validation%20set%2C%20achieving%20an%20FVD%20score%20of%2037.8%2C%20highlighting%20its%0Aability%20to%20generate%20realistic%20driving%20videos.%20The%20project%20can%20be%20found%20at%0Ahttps%3A//luhannan.github.io/CogDrivingPage/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03520v1&entry.124074799=Read"},
{"title": "Imagine360: Immersive 360 Video Generation from Perspective Anchor", "author": "Jing Tan and Shuai Yang and Tong Wu and Jingwen He and Yuwei Guo and Ziwei Liu and Dahua Lin", "abstract": "  $360^\\circ$ videos offer a hyper-immersive experience that allows the viewers\nto explore a dynamic scene from full 360 degrees. To achieve more user-friendly\nand personalized content creation in $360^\\circ$ video format, we seek to lift\nstandard perspective videos into $360^\\circ$ equirectangular videos. To this\nend, we introduce Imagine360, the first perspective-to-$360^\\circ$ video\ngeneration framework that creates high-quality $360^\\circ$ videos with rich and\ndiverse motion patterns from video anchors. Imagine360 learns fine-grained\nspherical visual and motion patterns from limited $360^\\circ$ video data with\nseveral key designs. 1) Firstly we adopt the dual-branch design, including a\nperspective and a panorama video denoising branch to provide local and global\nconstraints for $360^\\circ$ video generation, with motion module and spatial\nLoRA layers fine-tuned on extended web $360^\\circ$ videos. 2) Additionally, an\nantipodal mask is devised to capture long-range motion dependencies, enhancing\nthe reversed camera motion between antipodal pixels across hemispheres. 3) To\nhandle diverse perspective video inputs, we propose elevation-aware designs\nthat adapt to varying video masking due to changing elevations across frames.\nExtensive experiments show Imagine360 achieves superior graphics quality and\nmotion coherence among state-of-the-art $360^\\circ$ video generation methods.\nWe believe Imagine360 holds promise for advancing personalized, immersive\n$360^\\circ$ video creation.\n", "link": "http://arxiv.org/abs/2412.03552v1", "date": "2024-12-04", "relevancy": 2.8793, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5775}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5775}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imagine360%3A%20Immersive%20360%20Video%20Generation%20from%20Perspective%20Anchor&body=Title%3A%20Imagine360%3A%20Immersive%20360%20Video%20Generation%20from%20Perspective%20Anchor%0AAuthor%3A%20Jing%20Tan%20and%20Shuai%20Yang%20and%20Tong%20Wu%20and%20Jingwen%20He%20and%20Yuwei%20Guo%20and%20Ziwei%20Liu%20and%20Dahua%20Lin%0AAbstract%3A%20%20%20%24360%5E%5Ccirc%24%20videos%20offer%20a%20hyper-immersive%20experience%20that%20allows%20the%20viewers%0Ato%20explore%20a%20dynamic%20scene%20from%20full%20360%20degrees.%20To%20achieve%20more%20user-friendly%0Aand%20personalized%20content%20creation%20in%20%24360%5E%5Ccirc%24%20video%20format%2C%20we%20seek%20to%20lift%0Astandard%20perspective%20videos%20into%20%24360%5E%5Ccirc%24%20equirectangular%20videos.%20To%20this%0Aend%2C%20we%20introduce%20Imagine360%2C%20the%20first%20perspective-to-%24360%5E%5Ccirc%24%20video%0Ageneration%20framework%20that%20creates%20high-quality%20%24360%5E%5Ccirc%24%20videos%20with%20rich%20and%0Adiverse%20motion%20patterns%20from%20video%20anchors.%20Imagine360%20learns%20fine-grained%0Aspherical%20visual%20and%20motion%20patterns%20from%20limited%20%24360%5E%5Ccirc%24%20video%20data%20with%0Aseveral%20key%20designs.%201%29%20Firstly%20we%20adopt%20the%20dual-branch%20design%2C%20including%20a%0Aperspective%20and%20a%20panorama%20video%20denoising%20branch%20to%20provide%20local%20and%20global%0Aconstraints%20for%20%24360%5E%5Ccirc%24%20video%20generation%2C%20with%20motion%20module%20and%20spatial%0ALoRA%20layers%20fine-tuned%20on%20extended%20web%20%24360%5E%5Ccirc%24%20videos.%202%29%20Additionally%2C%20an%0Aantipodal%20mask%20is%20devised%20to%20capture%20long-range%20motion%20dependencies%2C%20enhancing%0Athe%20reversed%20camera%20motion%20between%20antipodal%20pixels%20across%20hemispheres.%203%29%20To%0Ahandle%20diverse%20perspective%20video%20inputs%2C%20we%20propose%20elevation-aware%20designs%0Athat%20adapt%20to%20varying%20video%20masking%20due%20to%20changing%20elevations%20across%20frames.%0AExtensive%20experiments%20show%20Imagine360%20achieves%20superior%20graphics%20quality%20and%0Amotion%20coherence%20among%20state-of-the-art%20%24360%5E%5Ccirc%24%20video%20generation%20methods.%0AWe%20believe%20Imagine360%20holds%20promise%20for%20advancing%20personalized%2C%20immersive%0A%24360%5E%5Ccirc%24%20video%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagine360%253A%2520Immersive%2520360%2520Video%2520Generation%2520from%2520Perspective%2520Anchor%26entry.906535625%3DJing%2520Tan%2520and%2520Shuai%2520Yang%2520and%2520Tong%2520Wu%2520and%2520Jingwen%2520He%2520and%2520Yuwei%2520Guo%2520and%2520Ziwei%2520Liu%2520and%2520Dahua%2520Lin%26entry.1292438233%3D%2520%2520%2524360%255E%255Ccirc%2524%2520videos%2520offer%2520a%2520hyper-immersive%2520experience%2520that%2520allows%2520the%2520viewers%250Ato%2520explore%2520a%2520dynamic%2520scene%2520from%2520full%2520360%2520degrees.%2520To%2520achieve%2520more%2520user-friendly%250Aand%2520personalized%2520content%2520creation%2520in%2520%2524360%255E%255Ccirc%2524%2520video%2520format%252C%2520we%2520seek%2520to%2520lift%250Astandard%2520perspective%2520videos%2520into%2520%2524360%255E%255Ccirc%2524%2520equirectangular%2520videos.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520Imagine360%252C%2520the%2520first%2520perspective-to-%2524360%255E%255Ccirc%2524%2520video%250Ageneration%2520framework%2520that%2520creates%2520high-quality%2520%2524360%255E%255Ccirc%2524%2520videos%2520with%2520rich%2520and%250Adiverse%2520motion%2520patterns%2520from%2520video%2520anchors.%2520Imagine360%2520learns%2520fine-grained%250Aspherical%2520visual%2520and%2520motion%2520patterns%2520from%2520limited%2520%2524360%255E%255Ccirc%2524%2520video%2520data%2520with%250Aseveral%2520key%2520designs.%25201%2529%2520Firstly%2520we%2520adopt%2520the%2520dual-branch%2520design%252C%2520including%2520a%250Aperspective%2520and%2520a%2520panorama%2520video%2520denoising%2520branch%2520to%2520provide%2520local%2520and%2520global%250Aconstraints%2520for%2520%2524360%255E%255Ccirc%2524%2520video%2520generation%252C%2520with%2520motion%2520module%2520and%2520spatial%250ALoRA%2520layers%2520fine-tuned%2520on%2520extended%2520web%2520%2524360%255E%255Ccirc%2524%2520videos.%25202%2529%2520Additionally%252C%2520an%250Aantipodal%2520mask%2520is%2520devised%2520to%2520capture%2520long-range%2520motion%2520dependencies%252C%2520enhancing%250Athe%2520reversed%2520camera%2520motion%2520between%2520antipodal%2520pixels%2520across%2520hemispheres.%25203%2529%2520To%250Ahandle%2520diverse%2520perspective%2520video%2520inputs%252C%2520we%2520propose%2520elevation-aware%2520designs%250Athat%2520adapt%2520to%2520varying%2520video%2520masking%2520due%2520to%2520changing%2520elevations%2520across%2520frames.%250AExtensive%2520experiments%2520show%2520Imagine360%2520achieves%2520superior%2520graphics%2520quality%2520and%250Amotion%2520coherence%2520among%2520state-of-the-art%2520%2524360%255E%255Ccirc%2524%2520video%2520generation%2520methods.%250AWe%2520believe%2520Imagine360%2520holds%2520promise%2520for%2520advancing%2520personalized%252C%2520immersive%250A%2524360%255E%255Ccirc%2524%2520video%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imagine360%3A%20Immersive%20360%20Video%20Generation%20from%20Perspective%20Anchor&entry.906535625=Jing%20Tan%20and%20Shuai%20Yang%20and%20Tong%20Wu%20and%20Jingwen%20He%20and%20Yuwei%20Guo%20and%20Ziwei%20Liu%20and%20Dahua%20Lin&entry.1292438233=%20%20%24360%5E%5Ccirc%24%20videos%20offer%20a%20hyper-immersive%20experience%20that%20allows%20the%20viewers%0Ato%20explore%20a%20dynamic%20scene%20from%20full%20360%20degrees.%20To%20achieve%20more%20user-friendly%0Aand%20personalized%20content%20creation%20in%20%24360%5E%5Ccirc%24%20video%20format%2C%20we%20seek%20to%20lift%0Astandard%20perspective%20videos%20into%20%24360%5E%5Ccirc%24%20equirectangular%20videos.%20To%20this%0Aend%2C%20we%20introduce%20Imagine360%2C%20the%20first%20perspective-to-%24360%5E%5Ccirc%24%20video%0Ageneration%20framework%20that%20creates%20high-quality%20%24360%5E%5Ccirc%24%20videos%20with%20rich%20and%0Adiverse%20motion%20patterns%20from%20video%20anchors.%20Imagine360%20learns%20fine-grained%0Aspherical%20visual%20and%20motion%20patterns%20from%20limited%20%24360%5E%5Ccirc%24%20video%20data%20with%0Aseveral%20key%20designs.%201%29%20Firstly%20we%20adopt%20the%20dual-branch%20design%2C%20including%20a%0Aperspective%20and%20a%20panorama%20video%20denoising%20branch%20to%20provide%20local%20and%20global%0Aconstraints%20for%20%24360%5E%5Ccirc%24%20video%20generation%2C%20with%20motion%20module%20and%20spatial%0ALoRA%20layers%20fine-tuned%20on%20extended%20web%20%24360%5E%5Ccirc%24%20videos.%202%29%20Additionally%2C%20an%0Aantipodal%20mask%20is%20devised%20to%20capture%20long-range%20motion%20dependencies%2C%20enhancing%0Athe%20reversed%20camera%20motion%20between%20antipodal%20pixels%20across%20hemispheres.%203%29%20To%0Ahandle%20diverse%20perspective%20video%20inputs%2C%20we%20propose%20elevation-aware%20designs%0Athat%20adapt%20to%20varying%20video%20masking%20due%20to%20changing%20elevations%20across%20frames.%0AExtensive%20experiments%20show%20Imagine360%20achieves%20superior%20graphics%20quality%20and%0Amotion%20coherence%20among%20state-of-the-art%20%24360%5E%5Ccirc%24%20video%20generation%20methods.%0AWe%20believe%20Imagine360%20holds%20promise%20for%20advancing%20personalized%2C%20immersive%0A%24360%5E%5Ccirc%24%20video%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03552v1&entry.124074799=Read"},
{"title": "Enhancing Perception Capabilities of Multimodal LLMs with Training-Free\n  Fusion", "author": "Zhuokun Chen and Jinwu Hu and Zeshuai Deng and Yufeng Wang and Bohan Zhuang and Mingkui Tan", "abstract": "  Multimodal LLMs (MLLMs) equip language models with visual capabilities by\naligning vision encoders with language models. Existing methods to enhance the\nvisual perception of MLLMs often involve designing more powerful vision\nencoders, which requires exploring a vast design space and re-aligning each\npotential encoder with the language model, resulting in prohibitively high\ntraining costs. In this paper, we introduce VisionFuse, a novel integration\nframework that efficiently utilizes multiple vision encoders from off-the-shelf\nMLLMs to enhance visual perception without requiring additional training. Our\napproach is motivated by the observation that different MLLMs tend to focus on\ndistinct regions given the same query and image. Moreover, we find that the\nfeature distributions of vision encoders within an MLLM family, a group of\nMLLMs sharing the same pretrained LLM, are highly aligned. Building on these\ninsights, VisionFuse enriches the visual context by concatenating the tokens\ngenerated by the vision encoders of selected MLLMs within a family. By merging\nthe parameters of language models from these MLLMs, VisionFuse allows a single\nlanguage model to align with various vision encoders, significantly reducing\ndeployment overhead. We conduct comprehensive evaluations across multiple\nmultimodal benchmarks using various MLLM combinations, demonstrating\nsubstantial improvements in multimodal tasks. Notably, when integrating\nMiniGemini-8B and SLIME-8B, VisionFuse achieves an average performance increase\nof over 4%.\n", "link": "http://arxiv.org/abs/2412.01289v2", "date": "2024-12-04", "relevancy": 2.8588, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5775}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5775}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Perception%20Capabilities%20of%20Multimodal%20LLMs%20with%20Training-Free%0A%20%20Fusion&body=Title%3A%20Enhancing%20Perception%20Capabilities%20of%20Multimodal%20LLMs%20with%20Training-Free%0A%20%20Fusion%0AAuthor%3A%20Zhuokun%20Chen%20and%20Jinwu%20Hu%20and%20Zeshuai%20Deng%20and%20Yufeng%20Wang%20and%20Bohan%20Zhuang%20and%20Mingkui%20Tan%0AAbstract%3A%20%20%20Multimodal%20LLMs%20%28MLLMs%29%20equip%20language%20models%20with%20visual%20capabilities%20by%0Aaligning%20vision%20encoders%20with%20language%20models.%20Existing%20methods%20to%20enhance%20the%0Avisual%20perception%20of%20MLLMs%20often%20involve%20designing%20more%20powerful%20vision%0Aencoders%2C%20which%20requires%20exploring%20a%20vast%20design%20space%20and%20re-aligning%20each%0Apotential%20encoder%20with%20the%20language%20model%2C%20resulting%20in%20prohibitively%20high%0Atraining%20costs.%20In%20this%20paper%2C%20we%20introduce%20VisionFuse%2C%20a%20novel%20integration%0Aframework%20that%20efficiently%20utilizes%20multiple%20vision%20encoders%20from%20off-the-shelf%0AMLLMs%20to%20enhance%20visual%20perception%20without%20requiring%20additional%20training.%20Our%0Aapproach%20is%20motivated%20by%20the%20observation%20that%20different%20MLLMs%20tend%20to%20focus%20on%0Adistinct%20regions%20given%20the%20same%20query%20and%20image.%20Moreover%2C%20we%20find%20that%20the%0Afeature%20distributions%20of%20vision%20encoders%20within%20an%20MLLM%20family%2C%20a%20group%20of%0AMLLMs%20sharing%20the%20same%20pretrained%20LLM%2C%20are%20highly%20aligned.%20Building%20on%20these%0Ainsights%2C%20VisionFuse%20enriches%20the%20visual%20context%20by%20concatenating%20the%20tokens%0Agenerated%20by%20the%20vision%20encoders%20of%20selected%20MLLMs%20within%20a%20family.%20By%20merging%0Athe%20parameters%20of%20language%20models%20from%20these%20MLLMs%2C%20VisionFuse%20allows%20a%20single%0Alanguage%20model%20to%20align%20with%20various%20vision%20encoders%2C%20significantly%20reducing%0Adeployment%20overhead.%20We%20conduct%20comprehensive%20evaluations%20across%20multiple%0Amultimodal%20benchmarks%20using%20various%20MLLM%20combinations%2C%20demonstrating%0Asubstantial%20improvements%20in%20multimodal%20tasks.%20Notably%2C%20when%20integrating%0AMiniGemini-8B%20and%20SLIME-8B%2C%20VisionFuse%20achieves%20an%20average%20performance%20increase%0Aof%20over%204%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01289v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Perception%2520Capabilities%2520of%2520Multimodal%2520LLMs%2520with%2520Training-Free%250A%2520%2520Fusion%26entry.906535625%3DZhuokun%2520Chen%2520and%2520Jinwu%2520Hu%2520and%2520Zeshuai%2520Deng%2520and%2520Yufeng%2520Wang%2520and%2520Bohan%2520Zhuang%2520and%2520Mingkui%2520Tan%26entry.1292438233%3D%2520%2520Multimodal%2520LLMs%2520%2528MLLMs%2529%2520equip%2520language%2520models%2520with%2520visual%2520capabilities%2520by%250Aaligning%2520vision%2520encoders%2520with%2520language%2520models.%2520Existing%2520methods%2520to%2520enhance%2520the%250Avisual%2520perception%2520of%2520MLLMs%2520often%2520involve%2520designing%2520more%2520powerful%2520vision%250Aencoders%252C%2520which%2520requires%2520exploring%2520a%2520vast%2520design%2520space%2520and%2520re-aligning%2520each%250Apotential%2520encoder%2520with%2520the%2520language%2520model%252C%2520resulting%2520in%2520prohibitively%2520high%250Atraining%2520costs.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520VisionFuse%252C%2520a%2520novel%2520integration%250Aframework%2520that%2520efficiently%2520utilizes%2520multiple%2520vision%2520encoders%2520from%2520off-the-shelf%250AMLLMs%2520to%2520enhance%2520visual%2520perception%2520without%2520requiring%2520additional%2520training.%2520Our%250Aapproach%2520is%2520motivated%2520by%2520the%2520observation%2520that%2520different%2520MLLMs%2520tend%2520to%2520focus%2520on%250Adistinct%2520regions%2520given%2520the%2520same%2520query%2520and%2520image.%2520Moreover%252C%2520we%2520find%2520that%2520the%250Afeature%2520distributions%2520of%2520vision%2520encoders%2520within%2520an%2520MLLM%2520family%252C%2520a%2520group%2520of%250AMLLMs%2520sharing%2520the%2520same%2520pretrained%2520LLM%252C%2520are%2520highly%2520aligned.%2520Building%2520on%2520these%250Ainsights%252C%2520VisionFuse%2520enriches%2520the%2520visual%2520context%2520by%2520concatenating%2520the%2520tokens%250Agenerated%2520by%2520the%2520vision%2520encoders%2520of%2520selected%2520MLLMs%2520within%2520a%2520family.%2520By%2520merging%250Athe%2520parameters%2520of%2520language%2520models%2520from%2520these%2520MLLMs%252C%2520VisionFuse%2520allows%2520a%2520single%250Alanguage%2520model%2520to%2520align%2520with%2520various%2520vision%2520encoders%252C%2520significantly%2520reducing%250Adeployment%2520overhead.%2520We%2520conduct%2520comprehensive%2520evaluations%2520across%2520multiple%250Amultimodal%2520benchmarks%2520using%2520various%2520MLLM%2520combinations%252C%2520demonstrating%250Asubstantial%2520improvements%2520in%2520multimodal%2520tasks.%2520Notably%252C%2520when%2520integrating%250AMiniGemini-8B%2520and%2520SLIME-8B%252C%2520VisionFuse%2520achieves%2520an%2520average%2520performance%2520increase%250Aof%2520over%25204%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01289v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Perception%20Capabilities%20of%20Multimodal%20LLMs%20with%20Training-Free%0A%20%20Fusion&entry.906535625=Zhuokun%20Chen%20and%20Jinwu%20Hu%20and%20Zeshuai%20Deng%20and%20Yufeng%20Wang%20and%20Bohan%20Zhuang%20and%20Mingkui%20Tan&entry.1292438233=%20%20Multimodal%20LLMs%20%28MLLMs%29%20equip%20language%20models%20with%20visual%20capabilities%20by%0Aaligning%20vision%20encoders%20with%20language%20models.%20Existing%20methods%20to%20enhance%20the%0Avisual%20perception%20of%20MLLMs%20often%20involve%20designing%20more%20powerful%20vision%0Aencoders%2C%20which%20requires%20exploring%20a%20vast%20design%20space%20and%20re-aligning%20each%0Apotential%20encoder%20with%20the%20language%20model%2C%20resulting%20in%20prohibitively%20high%0Atraining%20costs.%20In%20this%20paper%2C%20we%20introduce%20VisionFuse%2C%20a%20novel%20integration%0Aframework%20that%20efficiently%20utilizes%20multiple%20vision%20encoders%20from%20off-the-shelf%0AMLLMs%20to%20enhance%20visual%20perception%20without%20requiring%20additional%20training.%20Our%0Aapproach%20is%20motivated%20by%20the%20observation%20that%20different%20MLLMs%20tend%20to%20focus%20on%0Adistinct%20regions%20given%20the%20same%20query%20and%20image.%20Moreover%2C%20we%20find%20that%20the%0Afeature%20distributions%20of%20vision%20encoders%20within%20an%20MLLM%20family%2C%20a%20group%20of%0AMLLMs%20sharing%20the%20same%20pretrained%20LLM%2C%20are%20highly%20aligned.%20Building%20on%20these%0Ainsights%2C%20VisionFuse%20enriches%20the%20visual%20context%20by%20concatenating%20the%20tokens%0Agenerated%20by%20the%20vision%20encoders%20of%20selected%20MLLMs%20within%20a%20family.%20By%20merging%0Athe%20parameters%20of%20language%20models%20from%20these%20MLLMs%2C%20VisionFuse%20allows%20a%20single%0Alanguage%20model%20to%20align%20with%20various%20vision%20encoders%2C%20significantly%20reducing%0Adeployment%20overhead.%20We%20conduct%20comprehensive%20evaluations%20across%20multiple%0Amultimodal%20benchmarks%20using%20various%20MLLM%20combinations%2C%20demonstrating%0Asubstantial%20improvements%20in%20multimodal%20tasks.%20Notably%2C%20when%20integrating%0AMiniGemini-8B%20and%20SLIME-8B%2C%20VisionFuse%20achieves%20an%20average%20performance%20increase%0Aof%20over%204%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01289v2&entry.124074799=Read"},
{"title": "The Matrix: Infinite-Horizon World Generation with Real-Time Moving\n  Control", "author": "Ruili Feng and Han Zhang and Zhantao Yang and Jie Xiao and Zhilei Shu and Zhiheng Liu and Andy Zheng and Yukun Huang and Yu Liu and Hongyang Zhang", "abstract": "  We present The Matrix, the first foundational realistic world simulator\ncapable of generating continuous 720p high-fidelity real-scene video streams\nwith real-time, responsive control in both first- and third-person\nperspectives, enabling immersive exploration of richly dynamic environments.\nTrained on limited supervised data from AAA games like Forza Horizon 5 and\nCyberpunk 2077, complemented by large-scale unsupervised footage from\nreal-world settings like Tokyo streets, The Matrix allows users to traverse\ndiverse terrains -- deserts, grasslands, water bodies, and urban landscapes --\nin continuous, uncut hour-long sequences. Operating at 16 FPS, the system\nsupports real-time interactivity and demonstrates zero-shot generalization,\ntranslating virtual game environments to real-world contexts where collecting\ncontinuous movement data is often infeasible. For example, The Matrix can\nsimulate a BMW X3 driving through an office setting--an environment present in\nneither gaming data nor real-world sources. This approach showcases the\npotential of AAA game data to advance robust world models, bridging the gap\nbetween simulations and real-world applications in scenarios with limited data.\n", "link": "http://arxiv.org/abs/2412.03568v1", "date": "2024-12-04", "relevancy": 2.8518, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5949}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5753}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Matrix%3A%20Infinite-Horizon%20World%20Generation%20with%20Real-Time%20Moving%0A%20%20Control&body=Title%3A%20The%20Matrix%3A%20Infinite-Horizon%20World%20Generation%20with%20Real-Time%20Moving%0A%20%20Control%0AAuthor%3A%20Ruili%20Feng%20and%20Han%20Zhang%20and%20Zhantao%20Yang%20and%20Jie%20Xiao%20and%20Zhilei%20Shu%20and%20Zhiheng%20Liu%20and%20Andy%20Zheng%20and%20Yukun%20Huang%20and%20Yu%20Liu%20and%20Hongyang%20Zhang%0AAbstract%3A%20%20%20We%20present%20The%20Matrix%2C%20the%20first%20foundational%20realistic%20world%20simulator%0Acapable%20of%20generating%20continuous%20720p%20high-fidelity%20real-scene%20video%20streams%0Awith%20real-time%2C%20responsive%20control%20in%20both%20first-%20and%20third-person%0Aperspectives%2C%20enabling%20immersive%20exploration%20of%20richly%20dynamic%20environments.%0ATrained%20on%20limited%20supervised%20data%20from%20AAA%20games%20like%20Forza%20Horizon%205%20and%0ACyberpunk%202077%2C%20complemented%20by%20large-scale%20unsupervised%20footage%20from%0Areal-world%20settings%20like%20Tokyo%20streets%2C%20The%20Matrix%20allows%20users%20to%20traverse%0Adiverse%20terrains%20--%20deserts%2C%20grasslands%2C%20water%20bodies%2C%20and%20urban%20landscapes%20--%0Ain%20continuous%2C%20uncut%20hour-long%20sequences.%20Operating%20at%2016%20FPS%2C%20the%20system%0Asupports%20real-time%20interactivity%20and%20demonstrates%20zero-shot%20generalization%2C%0Atranslating%20virtual%20game%20environments%20to%20real-world%20contexts%20where%20collecting%0Acontinuous%20movement%20data%20is%20often%20infeasible.%20For%20example%2C%20The%20Matrix%20can%0Asimulate%20a%20BMW%20X3%20driving%20through%20an%20office%20setting--an%20environment%20present%20in%0Aneither%20gaming%20data%20nor%20real-world%20sources.%20This%20approach%20showcases%20the%0Apotential%20of%20AAA%20game%20data%20to%20advance%20robust%20world%20models%2C%20bridging%20the%20gap%0Abetween%20simulations%20and%20real-world%20applications%20in%20scenarios%20with%20limited%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Matrix%253A%2520Infinite-Horizon%2520World%2520Generation%2520with%2520Real-Time%2520Moving%250A%2520%2520Control%26entry.906535625%3DRuili%2520Feng%2520and%2520Han%2520Zhang%2520and%2520Zhantao%2520Yang%2520and%2520Jie%2520Xiao%2520and%2520Zhilei%2520Shu%2520and%2520Zhiheng%2520Liu%2520and%2520Andy%2520Zheng%2520and%2520Yukun%2520Huang%2520and%2520Yu%2520Liu%2520and%2520Hongyang%2520Zhang%26entry.1292438233%3D%2520%2520We%2520present%2520The%2520Matrix%252C%2520the%2520first%2520foundational%2520realistic%2520world%2520simulator%250Acapable%2520of%2520generating%2520continuous%2520720p%2520high-fidelity%2520real-scene%2520video%2520streams%250Awith%2520real-time%252C%2520responsive%2520control%2520in%2520both%2520first-%2520and%2520third-person%250Aperspectives%252C%2520enabling%2520immersive%2520exploration%2520of%2520richly%2520dynamic%2520environments.%250ATrained%2520on%2520limited%2520supervised%2520data%2520from%2520AAA%2520games%2520like%2520Forza%2520Horizon%25205%2520and%250ACyberpunk%25202077%252C%2520complemented%2520by%2520large-scale%2520unsupervised%2520footage%2520from%250Areal-world%2520settings%2520like%2520Tokyo%2520streets%252C%2520The%2520Matrix%2520allows%2520users%2520to%2520traverse%250Adiverse%2520terrains%2520--%2520deserts%252C%2520grasslands%252C%2520water%2520bodies%252C%2520and%2520urban%2520landscapes%2520--%250Ain%2520continuous%252C%2520uncut%2520hour-long%2520sequences.%2520Operating%2520at%252016%2520FPS%252C%2520the%2520system%250Asupports%2520real-time%2520interactivity%2520and%2520demonstrates%2520zero-shot%2520generalization%252C%250Atranslating%2520virtual%2520game%2520environments%2520to%2520real-world%2520contexts%2520where%2520collecting%250Acontinuous%2520movement%2520data%2520is%2520often%2520infeasible.%2520For%2520example%252C%2520The%2520Matrix%2520can%250Asimulate%2520a%2520BMW%2520X3%2520driving%2520through%2520an%2520office%2520setting--an%2520environment%2520present%2520in%250Aneither%2520gaming%2520data%2520nor%2520real-world%2520sources.%2520This%2520approach%2520showcases%2520the%250Apotential%2520of%2520AAA%2520game%2520data%2520to%2520advance%2520robust%2520world%2520models%252C%2520bridging%2520the%2520gap%250Abetween%2520simulations%2520and%2520real-world%2520applications%2520in%2520scenarios%2520with%2520limited%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Matrix%3A%20Infinite-Horizon%20World%20Generation%20with%20Real-Time%20Moving%0A%20%20Control&entry.906535625=Ruili%20Feng%20and%20Han%20Zhang%20and%20Zhantao%20Yang%20and%20Jie%20Xiao%20and%20Zhilei%20Shu%20and%20Zhiheng%20Liu%20and%20Andy%20Zheng%20and%20Yukun%20Huang%20and%20Yu%20Liu%20and%20Hongyang%20Zhang&entry.1292438233=%20%20We%20present%20The%20Matrix%2C%20the%20first%20foundational%20realistic%20world%20simulator%0Acapable%20of%20generating%20continuous%20720p%20high-fidelity%20real-scene%20video%20streams%0Awith%20real-time%2C%20responsive%20control%20in%20both%20first-%20and%20third-person%0Aperspectives%2C%20enabling%20immersive%20exploration%20of%20richly%20dynamic%20environments.%0ATrained%20on%20limited%20supervised%20data%20from%20AAA%20games%20like%20Forza%20Horizon%205%20and%0ACyberpunk%202077%2C%20complemented%20by%20large-scale%20unsupervised%20footage%20from%0Areal-world%20settings%20like%20Tokyo%20streets%2C%20The%20Matrix%20allows%20users%20to%20traverse%0Adiverse%20terrains%20--%20deserts%2C%20grasslands%2C%20water%20bodies%2C%20and%20urban%20landscapes%20--%0Ain%20continuous%2C%20uncut%20hour-long%20sequences.%20Operating%20at%2016%20FPS%2C%20the%20system%0Asupports%20real-time%20interactivity%20and%20demonstrates%20zero-shot%20generalization%2C%0Atranslating%20virtual%20game%20environments%20to%20real-world%20contexts%20where%20collecting%0Acontinuous%20movement%20data%20is%20often%20infeasible.%20For%20example%2C%20The%20Matrix%20can%0Asimulate%20a%20BMW%20X3%20driving%20through%20an%20office%20setting--an%20environment%20present%20in%0Aneither%20gaming%20data%20nor%20real-world%20sources.%20This%20approach%20showcases%20the%0Apotential%20of%20AAA%20game%20data%20to%20advance%20robust%20world%20models%2C%20bridging%20the%20gap%0Abetween%20simulations%20and%20real-world%20applications%20in%20scenarios%20with%20limited%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03568v1&entry.124074799=Read"},
{"title": "STRIDE: Single-video based Temporally Continuous Occlusion-Robust 3D\n  Pose Estimation", "author": "Rohit Lal and Saketh Bachu and Yash Garg and Arindam Dutta and Calvin-Khang Ta and Dripta S. Raychaudhuri and Hannah Dela Cruz and M. Salman Asif and Amit K. Roy-Chowdhury", "abstract": "  The capability to accurately estimate 3D human poses is crucial for diverse\nfields such as action recognition, gait recognition, and virtual/augmented\nreality. However, a persistent and significant challenge within this field is\nthe accurate prediction of human poses under conditions of severe occlusion.\nTraditional image-based estimators struggle with heavy occlusions due to a lack\nof temporal context, resulting in inconsistent predictions. While video-based\nmodels benefit from processing temporal data, they encounter limitations when\nfaced with prolonged occlusions that extend over multiple frames. This\nchallenge arises because these models struggle to generalize beyond their\ntraining datasets, and the variety of occlusions is hard to capture in the\ntraining data. Addressing these challenges, we propose STRIDE (Single-video\nbased TempoRally contInuous Occlusion-Robust 3D Pose Estimation), a novel\nTest-Time Training (TTT) approach to fit a human motion prior for each video.\nThis approach specifically handles occlusions that were not encountered during\nthe model's training. By employing STRIDE, we can refine a sequence of noisy\ninitial pose estimates into accurate, temporally coherent poses during test\ntime, effectively overcoming the limitations of prior methods. Our framework\ndemonstrates flexibility by being model-agnostic, allowing us to use any\noff-the-shelf 3D pose estimation method for improving robustness and temporal\nconsistency. We validate STRIDE's efficacy through comprehensive experiments on\nchallenging datasets like Occluded Human3.6M, Human3.6M, and OCMotion, where it\nnot only outperforms existing single-image and video-based pose estimation\nmodels but also showcases superior handling of substantial occlusions,\nachieving fast, robust, accurate, and temporally consistent 3D pose estimates.\nCode is made publicly available at https://github.com/take2rohit/stride\n", "link": "http://arxiv.org/abs/2312.16221v4", "date": "2024-12-04", "relevancy": 2.8514, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5606}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STRIDE%3A%20Single-video%20based%20Temporally%20Continuous%20Occlusion-Robust%203D%0A%20%20Pose%20Estimation&body=Title%3A%20STRIDE%3A%20Single-video%20based%20Temporally%20Continuous%20Occlusion-Robust%203D%0A%20%20Pose%20Estimation%0AAuthor%3A%20Rohit%20Lal%20and%20Saketh%20Bachu%20and%20Yash%20Garg%20and%20Arindam%20Dutta%20and%20Calvin-Khang%20Ta%20and%20Dripta%20S.%20Raychaudhuri%20and%20Hannah%20Dela%20Cruz%20and%20M.%20Salman%20Asif%20and%20Amit%20K.%20Roy-Chowdhury%0AAbstract%3A%20%20%20The%20capability%20to%20accurately%20estimate%203D%20human%20poses%20is%20crucial%20for%20diverse%0Afields%20such%20as%20action%20recognition%2C%20gait%20recognition%2C%20and%20virtual/augmented%0Areality.%20However%2C%20a%20persistent%20and%20significant%20challenge%20within%20this%20field%20is%0Athe%20accurate%20prediction%20of%20human%20poses%20under%20conditions%20of%20severe%20occlusion.%0ATraditional%20image-based%20estimators%20struggle%20with%20heavy%20occlusions%20due%20to%20a%20lack%0Aof%20temporal%20context%2C%20resulting%20in%20inconsistent%20predictions.%20While%20video-based%0Amodels%20benefit%20from%20processing%20temporal%20data%2C%20they%20encounter%20limitations%20when%0Afaced%20with%20prolonged%20occlusions%20that%20extend%20over%20multiple%20frames.%20This%0Achallenge%20arises%20because%20these%20models%20struggle%20to%20generalize%20beyond%20their%0Atraining%20datasets%2C%20and%20the%20variety%20of%20occlusions%20is%20hard%20to%20capture%20in%20the%0Atraining%20data.%20Addressing%20these%20challenges%2C%20we%20propose%20STRIDE%20%28Single-video%0Abased%20TempoRally%20contInuous%20Occlusion-Robust%203D%20Pose%20Estimation%29%2C%20a%20novel%0ATest-Time%20Training%20%28TTT%29%20approach%20to%20fit%20a%20human%20motion%20prior%20for%20each%20video.%0AThis%20approach%20specifically%20handles%20occlusions%20that%20were%20not%20encountered%20during%0Athe%20model%27s%20training.%20By%20employing%20STRIDE%2C%20we%20can%20refine%20a%20sequence%20of%20noisy%0Ainitial%20pose%20estimates%20into%20accurate%2C%20temporally%20coherent%20poses%20during%20test%0Atime%2C%20effectively%20overcoming%20the%20limitations%20of%20prior%20methods.%20Our%20framework%0Ademonstrates%20flexibility%20by%20being%20model-agnostic%2C%20allowing%20us%20to%20use%20any%0Aoff-the-shelf%203D%20pose%20estimation%20method%20for%20improving%20robustness%20and%20temporal%0Aconsistency.%20We%20validate%20STRIDE%27s%20efficacy%20through%20comprehensive%20experiments%20on%0Achallenging%20datasets%20like%20Occluded%20Human3.6M%2C%20Human3.6M%2C%20and%20OCMotion%2C%20where%20it%0Anot%20only%20outperforms%20existing%20single-image%20and%20video-based%20pose%20estimation%0Amodels%20but%20also%20showcases%20superior%20handling%20of%20substantial%20occlusions%2C%0Aachieving%20fast%2C%20robust%2C%20accurate%2C%20and%20temporally%20consistent%203D%20pose%20estimates.%0ACode%20is%20made%20publicly%20available%20at%20https%3A//github.com/take2rohit/stride%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16221v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTRIDE%253A%2520Single-video%2520based%2520Temporally%2520Continuous%2520Occlusion-Robust%25203D%250A%2520%2520Pose%2520Estimation%26entry.906535625%3DRohit%2520Lal%2520and%2520Saketh%2520Bachu%2520and%2520Yash%2520Garg%2520and%2520Arindam%2520Dutta%2520and%2520Calvin-Khang%2520Ta%2520and%2520Dripta%2520S.%2520Raychaudhuri%2520and%2520Hannah%2520Dela%2520Cruz%2520and%2520M.%2520Salman%2520Asif%2520and%2520Amit%2520K.%2520Roy-Chowdhury%26entry.1292438233%3D%2520%2520The%2520capability%2520to%2520accurately%2520estimate%25203D%2520human%2520poses%2520is%2520crucial%2520for%2520diverse%250Afields%2520such%2520as%2520action%2520recognition%252C%2520gait%2520recognition%252C%2520and%2520virtual/augmented%250Areality.%2520However%252C%2520a%2520persistent%2520and%2520significant%2520challenge%2520within%2520this%2520field%2520is%250Athe%2520accurate%2520prediction%2520of%2520human%2520poses%2520under%2520conditions%2520of%2520severe%2520occlusion.%250ATraditional%2520image-based%2520estimators%2520struggle%2520with%2520heavy%2520occlusions%2520due%2520to%2520a%2520lack%250Aof%2520temporal%2520context%252C%2520resulting%2520in%2520inconsistent%2520predictions.%2520While%2520video-based%250Amodels%2520benefit%2520from%2520processing%2520temporal%2520data%252C%2520they%2520encounter%2520limitations%2520when%250Afaced%2520with%2520prolonged%2520occlusions%2520that%2520extend%2520over%2520multiple%2520frames.%2520This%250Achallenge%2520arises%2520because%2520these%2520models%2520struggle%2520to%2520generalize%2520beyond%2520their%250Atraining%2520datasets%252C%2520and%2520the%2520variety%2520of%2520occlusions%2520is%2520hard%2520to%2520capture%2520in%2520the%250Atraining%2520data.%2520Addressing%2520these%2520challenges%252C%2520we%2520propose%2520STRIDE%2520%2528Single-video%250Abased%2520TempoRally%2520contInuous%2520Occlusion-Robust%25203D%2520Pose%2520Estimation%2529%252C%2520a%2520novel%250ATest-Time%2520Training%2520%2528TTT%2529%2520approach%2520to%2520fit%2520a%2520human%2520motion%2520prior%2520for%2520each%2520video.%250AThis%2520approach%2520specifically%2520handles%2520occlusions%2520that%2520were%2520not%2520encountered%2520during%250Athe%2520model%2527s%2520training.%2520By%2520employing%2520STRIDE%252C%2520we%2520can%2520refine%2520a%2520sequence%2520of%2520noisy%250Ainitial%2520pose%2520estimates%2520into%2520accurate%252C%2520temporally%2520coherent%2520poses%2520during%2520test%250Atime%252C%2520effectively%2520overcoming%2520the%2520limitations%2520of%2520prior%2520methods.%2520Our%2520framework%250Ademonstrates%2520flexibility%2520by%2520being%2520model-agnostic%252C%2520allowing%2520us%2520to%2520use%2520any%250Aoff-the-shelf%25203D%2520pose%2520estimation%2520method%2520for%2520improving%2520robustness%2520and%2520temporal%250Aconsistency.%2520We%2520validate%2520STRIDE%2527s%2520efficacy%2520through%2520comprehensive%2520experiments%2520on%250Achallenging%2520datasets%2520like%2520Occluded%2520Human3.6M%252C%2520Human3.6M%252C%2520and%2520OCMotion%252C%2520where%2520it%250Anot%2520only%2520outperforms%2520existing%2520single-image%2520and%2520video-based%2520pose%2520estimation%250Amodels%2520but%2520also%2520showcases%2520superior%2520handling%2520of%2520substantial%2520occlusions%252C%250Aachieving%2520fast%252C%2520robust%252C%2520accurate%252C%2520and%2520temporally%2520consistent%25203D%2520pose%2520estimates.%250ACode%2520is%2520made%2520publicly%2520available%2520at%2520https%253A//github.com/take2rohit/stride%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.16221v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STRIDE%3A%20Single-video%20based%20Temporally%20Continuous%20Occlusion-Robust%203D%0A%20%20Pose%20Estimation&entry.906535625=Rohit%20Lal%20and%20Saketh%20Bachu%20and%20Yash%20Garg%20and%20Arindam%20Dutta%20and%20Calvin-Khang%20Ta%20and%20Dripta%20S.%20Raychaudhuri%20and%20Hannah%20Dela%20Cruz%20and%20M.%20Salman%20Asif%20and%20Amit%20K.%20Roy-Chowdhury&entry.1292438233=%20%20The%20capability%20to%20accurately%20estimate%203D%20human%20poses%20is%20crucial%20for%20diverse%0Afields%20such%20as%20action%20recognition%2C%20gait%20recognition%2C%20and%20virtual/augmented%0Areality.%20However%2C%20a%20persistent%20and%20significant%20challenge%20within%20this%20field%20is%0Athe%20accurate%20prediction%20of%20human%20poses%20under%20conditions%20of%20severe%20occlusion.%0ATraditional%20image-based%20estimators%20struggle%20with%20heavy%20occlusions%20due%20to%20a%20lack%0Aof%20temporal%20context%2C%20resulting%20in%20inconsistent%20predictions.%20While%20video-based%0Amodels%20benefit%20from%20processing%20temporal%20data%2C%20they%20encounter%20limitations%20when%0Afaced%20with%20prolonged%20occlusions%20that%20extend%20over%20multiple%20frames.%20This%0Achallenge%20arises%20because%20these%20models%20struggle%20to%20generalize%20beyond%20their%0Atraining%20datasets%2C%20and%20the%20variety%20of%20occlusions%20is%20hard%20to%20capture%20in%20the%0Atraining%20data.%20Addressing%20these%20challenges%2C%20we%20propose%20STRIDE%20%28Single-video%0Abased%20TempoRally%20contInuous%20Occlusion-Robust%203D%20Pose%20Estimation%29%2C%20a%20novel%0ATest-Time%20Training%20%28TTT%29%20approach%20to%20fit%20a%20human%20motion%20prior%20for%20each%20video.%0AThis%20approach%20specifically%20handles%20occlusions%20that%20were%20not%20encountered%20during%0Athe%20model%27s%20training.%20By%20employing%20STRIDE%2C%20we%20can%20refine%20a%20sequence%20of%20noisy%0Ainitial%20pose%20estimates%20into%20accurate%2C%20temporally%20coherent%20poses%20during%20test%0Atime%2C%20effectively%20overcoming%20the%20limitations%20of%20prior%20methods.%20Our%20framework%0Ademonstrates%20flexibility%20by%20being%20model-agnostic%2C%20allowing%20us%20to%20use%20any%0Aoff-the-shelf%203D%20pose%20estimation%20method%20for%20improving%20robustness%20and%20temporal%0Aconsistency.%20We%20validate%20STRIDE%27s%20efficacy%20through%20comprehensive%20experiments%20on%0Achallenging%20datasets%20like%20Occluded%20Human3.6M%2C%20Human3.6M%2C%20and%20OCMotion%2C%20where%20it%0Anot%20only%20outperforms%20existing%20single-image%20and%20video-based%20pose%20estimation%0Amodels%20but%20also%20showcases%20superior%20handling%20of%20substantial%20occlusions%2C%0Aachieving%20fast%2C%20robust%2C%20accurate%2C%20and%20temporally%20consistent%203D%20pose%20estimates.%0ACode%20is%20made%20publicly%20available%20at%20https%3A//github.com/take2rohit/stride%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16221v4&entry.124074799=Read"},
{"title": "Equivariant Representation Learning for Augmentation-based\n  Self-Supervised Learning via Image Reconstruction", "author": "Qin Wang and Kai Krajsek and Hanno Scharr", "abstract": "  Augmentation-based self-supervised learning methods have shown remarkable\nsuccess in self-supervised visual representation learning, excelling in\nlearning invariant features but often neglecting equivariant ones. This\nlimitation reduces the generalizability of foundation models, particularly for\ndownstream tasks requiring equivariance. We propose integrating an image\nreconstruction task as an auxiliary component in augmentation-based\nself-supervised learning algorithms to facilitate equivariant feature learning\nwithout additional parameters. Our method implements a cross-attention\nmechanism to blend features learned from two augmented views, subsequently\nreconstructing one of them. This approach is adaptable to various datasets and\naugmented-pair based learning methods. We evaluate its effectiveness on\nlearning equivariant features through multiple linear regression tasks and\ndownstream applications on both artificial (3DIEBench) and natural (ImageNet)\ndatasets. Results consistently demonstrate significant improvements over\nstandard augmentation-based self-supervised learning methods and\nstate-of-the-art approaches, particularly excelling in scenarios involving\ncombined augmentations. Our method enhances the learning of both invariant and\nequivariant features, leading to more robust and generalizable visual\nrepresentations for computer vision tasks.\n", "link": "http://arxiv.org/abs/2412.03314v1", "date": "2024-12-04", "relevancy": 2.8339, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6038}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5661}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20Representation%20Learning%20for%20Augmentation-based%0A%20%20Self-Supervised%20Learning%20via%20Image%20Reconstruction&body=Title%3A%20Equivariant%20Representation%20Learning%20for%20Augmentation-based%0A%20%20Self-Supervised%20Learning%20via%20Image%20Reconstruction%0AAuthor%3A%20Qin%20Wang%20and%20Kai%20Krajsek%20and%20Hanno%20Scharr%0AAbstract%3A%20%20%20Augmentation-based%20self-supervised%20learning%20methods%20have%20shown%20remarkable%0Asuccess%20in%20self-supervised%20visual%20representation%20learning%2C%20excelling%20in%0Alearning%20invariant%20features%20but%20often%20neglecting%20equivariant%20ones.%20This%0Alimitation%20reduces%20the%20generalizability%20of%20foundation%20models%2C%20particularly%20for%0Adownstream%20tasks%20requiring%20equivariance.%20We%20propose%20integrating%20an%20image%0Areconstruction%20task%20as%20an%20auxiliary%20component%20in%20augmentation-based%0Aself-supervised%20learning%20algorithms%20to%20facilitate%20equivariant%20feature%20learning%0Awithout%20additional%20parameters.%20Our%20method%20implements%20a%20cross-attention%0Amechanism%20to%20blend%20features%20learned%20from%20two%20augmented%20views%2C%20subsequently%0Areconstructing%20one%20of%20them.%20This%20approach%20is%20adaptable%20to%20various%20datasets%20and%0Aaugmented-pair%20based%20learning%20methods.%20We%20evaluate%20its%20effectiveness%20on%0Alearning%20equivariant%20features%20through%20multiple%20linear%20regression%20tasks%20and%0Adownstream%20applications%20on%20both%20artificial%20%283DIEBench%29%20and%20natural%20%28ImageNet%29%0Adatasets.%20Results%20consistently%20demonstrate%20significant%20improvements%20over%0Astandard%20augmentation-based%20self-supervised%20learning%20methods%20and%0Astate-of-the-art%20approaches%2C%20particularly%20excelling%20in%20scenarios%20involving%0Acombined%20augmentations.%20Our%20method%20enhances%20the%20learning%20of%20both%20invariant%20and%0Aequivariant%20features%2C%20leading%20to%20more%20robust%20and%20generalizable%20visual%0Arepresentations%20for%20computer%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520Representation%2520Learning%2520for%2520Augmentation-based%250A%2520%2520Self-Supervised%2520Learning%2520via%2520Image%2520Reconstruction%26entry.906535625%3DQin%2520Wang%2520and%2520Kai%2520Krajsek%2520and%2520Hanno%2520Scharr%26entry.1292438233%3D%2520%2520Augmentation-based%2520self-supervised%2520learning%2520methods%2520have%2520shown%2520remarkable%250Asuccess%2520in%2520self-supervised%2520visual%2520representation%2520learning%252C%2520excelling%2520in%250Alearning%2520invariant%2520features%2520but%2520often%2520neglecting%2520equivariant%2520ones.%2520This%250Alimitation%2520reduces%2520the%2520generalizability%2520of%2520foundation%2520models%252C%2520particularly%2520for%250Adownstream%2520tasks%2520requiring%2520equivariance.%2520We%2520propose%2520integrating%2520an%2520image%250Areconstruction%2520task%2520as%2520an%2520auxiliary%2520component%2520in%2520augmentation-based%250Aself-supervised%2520learning%2520algorithms%2520to%2520facilitate%2520equivariant%2520feature%2520learning%250Awithout%2520additional%2520parameters.%2520Our%2520method%2520implements%2520a%2520cross-attention%250Amechanism%2520to%2520blend%2520features%2520learned%2520from%2520two%2520augmented%2520views%252C%2520subsequently%250Areconstructing%2520one%2520of%2520them.%2520This%2520approach%2520is%2520adaptable%2520to%2520various%2520datasets%2520and%250Aaugmented-pair%2520based%2520learning%2520methods.%2520We%2520evaluate%2520its%2520effectiveness%2520on%250Alearning%2520equivariant%2520features%2520through%2520multiple%2520linear%2520regression%2520tasks%2520and%250Adownstream%2520applications%2520on%2520both%2520artificial%2520%25283DIEBench%2529%2520and%2520natural%2520%2528ImageNet%2529%250Adatasets.%2520Results%2520consistently%2520demonstrate%2520significant%2520improvements%2520over%250Astandard%2520augmentation-based%2520self-supervised%2520learning%2520methods%2520and%250Astate-of-the-art%2520approaches%252C%2520particularly%2520excelling%2520in%2520scenarios%2520involving%250Acombined%2520augmentations.%2520Our%2520method%2520enhances%2520the%2520learning%2520of%2520both%2520invariant%2520and%250Aequivariant%2520features%252C%2520leading%2520to%2520more%2520robust%2520and%2520generalizable%2520visual%250Arepresentations%2520for%2520computer%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20Representation%20Learning%20for%20Augmentation-based%0A%20%20Self-Supervised%20Learning%20via%20Image%20Reconstruction&entry.906535625=Qin%20Wang%20and%20Kai%20Krajsek%20and%20Hanno%20Scharr&entry.1292438233=%20%20Augmentation-based%20self-supervised%20learning%20methods%20have%20shown%20remarkable%0Asuccess%20in%20self-supervised%20visual%20representation%20learning%2C%20excelling%20in%0Alearning%20invariant%20features%20but%20often%20neglecting%20equivariant%20ones.%20This%0Alimitation%20reduces%20the%20generalizability%20of%20foundation%20models%2C%20particularly%20for%0Adownstream%20tasks%20requiring%20equivariance.%20We%20propose%20integrating%20an%20image%0Areconstruction%20task%20as%20an%20auxiliary%20component%20in%20augmentation-based%0Aself-supervised%20learning%20algorithms%20to%20facilitate%20equivariant%20feature%20learning%0Awithout%20additional%20parameters.%20Our%20method%20implements%20a%20cross-attention%0Amechanism%20to%20blend%20features%20learned%20from%20two%20augmented%20views%2C%20subsequently%0Areconstructing%20one%20of%20them.%20This%20approach%20is%20adaptable%20to%20various%20datasets%20and%0Aaugmented-pair%20based%20learning%20methods.%20We%20evaluate%20its%20effectiveness%20on%0Alearning%20equivariant%20features%20through%20multiple%20linear%20regression%20tasks%20and%0Adownstream%20applications%20on%20both%20artificial%20%283DIEBench%29%20and%20natural%20%28ImageNet%29%0Adatasets.%20Results%20consistently%20demonstrate%20significant%20improvements%20over%0Astandard%20augmentation-based%20self-supervised%20learning%20methods%20and%0Astate-of-the-art%20approaches%2C%20particularly%20excelling%20in%20scenarios%20involving%0Acombined%20augmentations.%20Our%20method%20enhances%20the%20learning%20of%20both%20invariant%20and%0Aequivariant%20features%2C%20leading%20to%20more%20robust%20and%20generalizable%20visual%0Arepresentations%20for%20computer%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03314v1&entry.124074799=Read"},
{"title": "Training-Free Mitigation of Language Reasoning Degradation After\n  Multimodal Instruction Tuning", "author": "Neale Ratzlaff and Man Luo and Xin Su and Vasudev Lal and Phillip Howard", "abstract": "  Multimodal models typically combine a powerful large language model (LLM)\nwith a vision encoder and are then trained on multimodal data via instruction\ntuning. While this process adapts LLMs to multimodal settings, it remains\nunclear whether this adaptation compromises their original language reasoning\ncapabilities. In this work, we explore the effects of multimodal instruction\ntuning on language reasoning performance. We focus on LLaVA, a leading\nmultimodal framework that integrates LLMs such as Vicuna or Mistral with the\nCLIP vision encoder. We compare the performance of the original LLMs with their\nmultimodal-adapted counterparts across eight language reasoning tasks. Our\nexperiments yield several key insights. First, the impact of multimodal\nlearning varies between Vicuna and Mistral: we observe a degradation in\nlanguage reasoning for Mistral but improvements for Vicuna across most tasks.\nSecond, while multimodal instruction learning consistently degrades performance\non mathematical reasoning tasks (e.g., GSM8K), it enhances performance on\ncommonsense reasoning tasks (e.g., CommonsenseQA). Finally, we demonstrate that\na training-free model merging technique can effectively mitigate the language\nreasoning degradation observed in multimodal-adapted Mistral and even improve\nperformance on visual tasks.\n", "link": "http://arxiv.org/abs/2412.03467v1", "date": "2024-12-04", "relevancy": 2.8225, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5966}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5484}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Mitigation%20of%20Language%20Reasoning%20Degradation%20After%0A%20%20Multimodal%20Instruction%20Tuning&body=Title%3A%20Training-Free%20Mitigation%20of%20Language%20Reasoning%20Degradation%20After%0A%20%20Multimodal%20Instruction%20Tuning%0AAuthor%3A%20Neale%20Ratzlaff%20and%20Man%20Luo%20and%20Xin%20Su%20and%20Vasudev%20Lal%20and%20Phillip%20Howard%0AAbstract%3A%20%20%20Multimodal%20models%20typically%20combine%20a%20powerful%20large%20language%20model%20%28LLM%29%0Awith%20a%20vision%20encoder%20and%20are%20then%20trained%20on%20multimodal%20data%20via%20instruction%0Atuning.%20While%20this%20process%20adapts%20LLMs%20to%20multimodal%20settings%2C%20it%20remains%0Aunclear%20whether%20this%20adaptation%20compromises%20their%20original%20language%20reasoning%0Acapabilities.%20In%20this%20work%2C%20we%20explore%20the%20effects%20of%20multimodal%20instruction%0Atuning%20on%20language%20reasoning%20performance.%20We%20focus%20on%20LLaVA%2C%20a%20leading%0Amultimodal%20framework%20that%20integrates%20LLMs%20such%20as%20Vicuna%20or%20Mistral%20with%20the%0ACLIP%20vision%20encoder.%20We%20compare%20the%20performance%20of%20the%20original%20LLMs%20with%20their%0Amultimodal-adapted%20counterparts%20across%20eight%20language%20reasoning%20tasks.%20Our%0Aexperiments%20yield%20several%20key%20insights.%20First%2C%20the%20impact%20of%20multimodal%0Alearning%20varies%20between%20Vicuna%20and%20Mistral%3A%20we%20observe%20a%20degradation%20in%0Alanguage%20reasoning%20for%20Mistral%20but%20improvements%20for%20Vicuna%20across%20most%20tasks.%0ASecond%2C%20while%20multimodal%20instruction%20learning%20consistently%20degrades%20performance%0Aon%20mathematical%20reasoning%20tasks%20%28e.g.%2C%20GSM8K%29%2C%20it%20enhances%20performance%20on%0Acommonsense%20reasoning%20tasks%20%28e.g.%2C%20CommonsenseQA%29.%20Finally%2C%20we%20demonstrate%20that%0Aa%20training-free%20model%20merging%20technique%20can%20effectively%20mitigate%20the%20language%0Areasoning%20degradation%20observed%20in%20multimodal-adapted%20Mistral%20and%20even%20improve%0Aperformance%20on%20visual%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Mitigation%2520of%2520Language%2520Reasoning%2520Degradation%2520After%250A%2520%2520Multimodal%2520Instruction%2520Tuning%26entry.906535625%3DNeale%2520Ratzlaff%2520and%2520Man%2520Luo%2520and%2520Xin%2520Su%2520and%2520Vasudev%2520Lal%2520and%2520Phillip%2520Howard%26entry.1292438233%3D%2520%2520Multimodal%2520models%2520typically%2520combine%2520a%2520powerful%2520large%2520language%2520model%2520%2528LLM%2529%250Awith%2520a%2520vision%2520encoder%2520and%2520are%2520then%2520trained%2520on%2520multimodal%2520data%2520via%2520instruction%250Atuning.%2520While%2520this%2520process%2520adapts%2520LLMs%2520to%2520multimodal%2520settings%252C%2520it%2520remains%250Aunclear%2520whether%2520this%2520adaptation%2520compromises%2520their%2520original%2520language%2520reasoning%250Acapabilities.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520effects%2520of%2520multimodal%2520instruction%250Atuning%2520on%2520language%2520reasoning%2520performance.%2520We%2520focus%2520on%2520LLaVA%252C%2520a%2520leading%250Amultimodal%2520framework%2520that%2520integrates%2520LLMs%2520such%2520as%2520Vicuna%2520or%2520Mistral%2520with%2520the%250ACLIP%2520vision%2520encoder.%2520We%2520compare%2520the%2520performance%2520of%2520the%2520original%2520LLMs%2520with%2520their%250Amultimodal-adapted%2520counterparts%2520across%2520eight%2520language%2520reasoning%2520tasks.%2520Our%250Aexperiments%2520yield%2520several%2520key%2520insights.%2520First%252C%2520the%2520impact%2520of%2520multimodal%250Alearning%2520varies%2520between%2520Vicuna%2520and%2520Mistral%253A%2520we%2520observe%2520a%2520degradation%2520in%250Alanguage%2520reasoning%2520for%2520Mistral%2520but%2520improvements%2520for%2520Vicuna%2520across%2520most%2520tasks.%250ASecond%252C%2520while%2520multimodal%2520instruction%2520learning%2520consistently%2520degrades%2520performance%250Aon%2520mathematical%2520reasoning%2520tasks%2520%2528e.g.%252C%2520GSM8K%2529%252C%2520it%2520enhances%2520performance%2520on%250Acommonsense%2520reasoning%2520tasks%2520%2528e.g.%252C%2520CommonsenseQA%2529.%2520Finally%252C%2520we%2520demonstrate%2520that%250Aa%2520training-free%2520model%2520merging%2520technique%2520can%2520effectively%2520mitigate%2520the%2520language%250Areasoning%2520degradation%2520observed%2520in%2520multimodal-adapted%2520Mistral%2520and%2520even%2520improve%250Aperformance%2520on%2520visual%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Mitigation%20of%20Language%20Reasoning%20Degradation%20After%0A%20%20Multimodal%20Instruction%20Tuning&entry.906535625=Neale%20Ratzlaff%20and%20Man%20Luo%20and%20Xin%20Su%20and%20Vasudev%20Lal%20and%20Phillip%20Howard&entry.1292438233=%20%20Multimodal%20models%20typically%20combine%20a%20powerful%20large%20language%20model%20%28LLM%29%0Awith%20a%20vision%20encoder%20and%20are%20then%20trained%20on%20multimodal%20data%20via%20instruction%0Atuning.%20While%20this%20process%20adapts%20LLMs%20to%20multimodal%20settings%2C%20it%20remains%0Aunclear%20whether%20this%20adaptation%20compromises%20their%20original%20language%20reasoning%0Acapabilities.%20In%20this%20work%2C%20we%20explore%20the%20effects%20of%20multimodal%20instruction%0Atuning%20on%20language%20reasoning%20performance.%20We%20focus%20on%20LLaVA%2C%20a%20leading%0Amultimodal%20framework%20that%20integrates%20LLMs%20such%20as%20Vicuna%20or%20Mistral%20with%20the%0ACLIP%20vision%20encoder.%20We%20compare%20the%20performance%20of%20the%20original%20LLMs%20with%20their%0Amultimodal-adapted%20counterparts%20across%20eight%20language%20reasoning%20tasks.%20Our%0Aexperiments%20yield%20several%20key%20insights.%20First%2C%20the%20impact%20of%20multimodal%0Alearning%20varies%20between%20Vicuna%20and%20Mistral%3A%20we%20observe%20a%20degradation%20in%0Alanguage%20reasoning%20for%20Mistral%20but%20improvements%20for%20Vicuna%20across%20most%20tasks.%0ASecond%2C%20while%20multimodal%20instruction%20learning%20consistently%20degrades%20performance%0Aon%20mathematical%20reasoning%20tasks%20%28e.g.%2C%20GSM8K%29%2C%20it%20enhances%20performance%20on%0Acommonsense%20reasoning%20tasks%20%28e.g.%2C%20CommonsenseQA%29.%20Finally%2C%20we%20demonstrate%20that%0Aa%20training-free%20model%20merging%20technique%20can%20effectively%20mitigate%20the%20language%0Areasoning%20degradation%20observed%20in%20multimodal-adapted%20Mistral%20and%20even%20improve%0Aperformance%20on%20visual%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03467v1&entry.124074799=Read"},
{"title": "Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual\n  Prompt Instruction Tuning", "author": "Wujian Peng and Lingchen Meng and Yitong Chen and Yiweng Xie and Yang Liu and Tao Gui and Hang Xu and Xipeng Qiu and Zuxuan Wu and Yu-Gang Jiang", "abstract": "  Large Multimodal Models (LMMs) have made significant breakthroughs with the\nadvancement of instruction tuning. However, while existing models can\nunderstand images and videos at a holistic level, they still struggle with\ninstance-level understanding that requires a more nuanced comprehension and\nalignment. Instance-level understanding is crucial, as it focuses on the\nspecific elements that we are most interested in. Excitingly, existing works\nfind that the state-of-the-art LMMs exhibit strong instance understanding\ncapabilities when provided with explicit visual cues. Motivated by this, we\nintroduce an automated annotation pipeline assisted by GPT-4o to extract\ninstance-level information from images and videos through explicit visual\nprompting for instance guidance. Building upon this pipeline, we proposed\nInst-IT, a solution to enhance LMMs in Instance understanding via explicit\nvisual prompt Instruction Tuning. Inst-IT consists of a benchmark to diagnose\nmultimodal instance-level understanding, a large-scale instruction-tuning\ndataset, and a continuous instruction-tuning training paradigm to effectively\nenhance spatial-temporal instance understanding capabilities of existing LMMs.\nExperimental results show that, with the boost of Inst-IT, our models not only\nachieve outstanding performance on Inst-IT Bench but also demonstrate\nsignificant improvements across various generic image and video understanding\nbenchmarks. This highlights that our dataset not only boosts instance-level\nunderstanding but also strengthens the overall capabilities of generic image\nand video comprehension.\n", "link": "http://arxiv.org/abs/2412.03565v1", "date": "2024-12-04", "relevancy": 2.8201, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inst-IT%3A%20Boosting%20Multimodal%20Instance%20Understanding%20via%20Explicit%20Visual%0A%20%20Prompt%20Instruction%20Tuning&body=Title%3A%20Inst-IT%3A%20Boosting%20Multimodal%20Instance%20Understanding%20via%20Explicit%20Visual%0A%20%20Prompt%20Instruction%20Tuning%0AAuthor%3A%20Wujian%20Peng%20and%20Lingchen%20Meng%20and%20Yitong%20Chen%20and%20Yiweng%20Xie%20and%20Yang%20Liu%20and%20Tao%20Gui%20and%20Hang%20Xu%20and%20Xipeng%20Qiu%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20made%20significant%20breakthroughs%20with%20the%0Aadvancement%20of%20instruction%20tuning.%20However%2C%20while%20existing%20models%20can%0Aunderstand%20images%20and%20videos%20at%20a%20holistic%20level%2C%20they%20still%20struggle%20with%0Ainstance-level%20understanding%20that%20requires%20a%20more%20nuanced%20comprehension%20and%0Aalignment.%20Instance-level%20understanding%20is%20crucial%2C%20as%20it%20focuses%20on%20the%0Aspecific%20elements%20that%20we%20are%20most%20interested%20in.%20Excitingly%2C%20existing%20works%0Afind%20that%20the%20state-of-the-art%20LMMs%20exhibit%20strong%20instance%20understanding%0Acapabilities%20when%20provided%20with%20explicit%20visual%20cues.%20Motivated%20by%20this%2C%20we%0Aintroduce%20an%20automated%20annotation%20pipeline%20assisted%20by%20GPT-4o%20to%20extract%0Ainstance-level%20information%20from%20images%20and%20videos%20through%20explicit%20visual%0Aprompting%20for%20instance%20guidance.%20Building%20upon%20this%20pipeline%2C%20we%20proposed%0AInst-IT%2C%20a%20solution%20to%20enhance%20LMMs%20in%20Instance%20understanding%20via%20explicit%0Avisual%20prompt%20Instruction%20Tuning.%20Inst-IT%20consists%20of%20a%20benchmark%20to%20diagnose%0Amultimodal%20instance-level%20understanding%2C%20a%20large-scale%20instruction-tuning%0Adataset%2C%20and%20a%20continuous%20instruction-tuning%20training%20paradigm%20to%20effectively%0Aenhance%20spatial-temporal%20instance%20understanding%20capabilities%20of%20existing%20LMMs.%0AExperimental%20results%20show%20that%2C%20with%20the%20boost%20of%20Inst-IT%2C%20our%20models%20not%20only%0Aachieve%20outstanding%20performance%20on%20Inst-IT%20Bench%20but%20also%20demonstrate%0Asignificant%20improvements%20across%20various%20generic%20image%20and%20video%20understanding%0Abenchmarks.%20This%20highlights%20that%20our%20dataset%20not%20only%20boosts%20instance-level%0Aunderstanding%20but%20also%20strengthens%20the%20overall%20capabilities%20of%20generic%20image%0Aand%20video%20comprehension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInst-IT%253A%2520Boosting%2520Multimodal%2520Instance%2520Understanding%2520via%2520Explicit%2520Visual%250A%2520%2520Prompt%2520Instruction%2520Tuning%26entry.906535625%3DWujian%2520Peng%2520and%2520Lingchen%2520Meng%2520and%2520Yitong%2520Chen%2520and%2520Yiweng%2520Xie%2520and%2520Yang%2520Liu%2520and%2520Tao%2520Gui%2520and%2520Hang%2520Xu%2520and%2520Xipeng%2520Qiu%2520and%2520Zuxuan%2520Wu%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520have%2520made%2520significant%2520breakthroughs%2520with%2520the%250Aadvancement%2520of%2520instruction%2520tuning.%2520However%252C%2520while%2520existing%2520models%2520can%250Aunderstand%2520images%2520and%2520videos%2520at%2520a%2520holistic%2520level%252C%2520they%2520still%2520struggle%2520with%250Ainstance-level%2520understanding%2520that%2520requires%2520a%2520more%2520nuanced%2520comprehension%2520and%250Aalignment.%2520Instance-level%2520understanding%2520is%2520crucial%252C%2520as%2520it%2520focuses%2520on%2520the%250Aspecific%2520elements%2520that%2520we%2520are%2520most%2520interested%2520in.%2520Excitingly%252C%2520existing%2520works%250Afind%2520that%2520the%2520state-of-the-art%2520LMMs%2520exhibit%2520strong%2520instance%2520understanding%250Acapabilities%2520when%2520provided%2520with%2520explicit%2520visual%2520cues.%2520Motivated%2520by%2520this%252C%2520we%250Aintroduce%2520an%2520automated%2520annotation%2520pipeline%2520assisted%2520by%2520GPT-4o%2520to%2520extract%250Ainstance-level%2520information%2520from%2520images%2520and%2520videos%2520through%2520explicit%2520visual%250Aprompting%2520for%2520instance%2520guidance.%2520Building%2520upon%2520this%2520pipeline%252C%2520we%2520proposed%250AInst-IT%252C%2520a%2520solution%2520to%2520enhance%2520LMMs%2520in%2520Instance%2520understanding%2520via%2520explicit%250Avisual%2520prompt%2520Instruction%2520Tuning.%2520Inst-IT%2520consists%2520of%2520a%2520benchmark%2520to%2520diagnose%250Amultimodal%2520instance-level%2520understanding%252C%2520a%2520large-scale%2520instruction-tuning%250Adataset%252C%2520and%2520a%2520continuous%2520instruction-tuning%2520training%2520paradigm%2520to%2520effectively%250Aenhance%2520spatial-temporal%2520instance%2520understanding%2520capabilities%2520of%2520existing%2520LMMs.%250AExperimental%2520results%2520show%2520that%252C%2520with%2520the%2520boost%2520of%2520Inst-IT%252C%2520our%2520models%2520not%2520only%250Aachieve%2520outstanding%2520performance%2520on%2520Inst-IT%2520Bench%2520but%2520also%2520demonstrate%250Asignificant%2520improvements%2520across%2520various%2520generic%2520image%2520and%2520video%2520understanding%250Abenchmarks.%2520This%2520highlights%2520that%2520our%2520dataset%2520not%2520only%2520boosts%2520instance-level%250Aunderstanding%2520but%2520also%2520strengthens%2520the%2520overall%2520capabilities%2520of%2520generic%2520image%250Aand%2520video%2520comprehension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inst-IT%3A%20Boosting%20Multimodal%20Instance%20Understanding%20via%20Explicit%20Visual%0A%20%20Prompt%20Instruction%20Tuning&entry.906535625=Wujian%20Peng%20and%20Lingchen%20Meng%20and%20Yitong%20Chen%20and%20Yiweng%20Xie%20and%20Yang%20Liu%20and%20Tao%20Gui%20and%20Hang%20Xu%20and%20Xipeng%20Qiu%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20made%20significant%20breakthroughs%20with%20the%0Aadvancement%20of%20instruction%20tuning.%20However%2C%20while%20existing%20models%20can%0Aunderstand%20images%20and%20videos%20at%20a%20holistic%20level%2C%20they%20still%20struggle%20with%0Ainstance-level%20understanding%20that%20requires%20a%20more%20nuanced%20comprehension%20and%0Aalignment.%20Instance-level%20understanding%20is%20crucial%2C%20as%20it%20focuses%20on%20the%0Aspecific%20elements%20that%20we%20are%20most%20interested%20in.%20Excitingly%2C%20existing%20works%0Afind%20that%20the%20state-of-the-art%20LMMs%20exhibit%20strong%20instance%20understanding%0Acapabilities%20when%20provided%20with%20explicit%20visual%20cues.%20Motivated%20by%20this%2C%20we%0Aintroduce%20an%20automated%20annotation%20pipeline%20assisted%20by%20GPT-4o%20to%20extract%0Ainstance-level%20information%20from%20images%20and%20videos%20through%20explicit%20visual%0Aprompting%20for%20instance%20guidance.%20Building%20upon%20this%20pipeline%2C%20we%20proposed%0AInst-IT%2C%20a%20solution%20to%20enhance%20LMMs%20in%20Instance%20understanding%20via%20explicit%0Avisual%20prompt%20Instruction%20Tuning.%20Inst-IT%20consists%20of%20a%20benchmark%20to%20diagnose%0Amultimodal%20instance-level%20understanding%2C%20a%20large-scale%20instruction-tuning%0Adataset%2C%20and%20a%20continuous%20instruction-tuning%20training%20paradigm%20to%20effectively%0Aenhance%20spatial-temporal%20instance%20understanding%20capabilities%20of%20existing%20LMMs.%0AExperimental%20results%20show%20that%2C%20with%20the%20boost%20of%20Inst-IT%2C%20our%20models%20not%20only%0Aachieve%20outstanding%20performance%20on%20Inst-IT%20Bench%20but%20also%20demonstrate%0Asignificant%20improvements%20across%20various%20generic%20image%20and%20video%20understanding%0Abenchmarks.%20This%20highlights%20that%20our%20dataset%20not%20only%20boosts%20instance-level%0Aunderstanding%20but%20also%20strengthens%20the%20overall%20capabilities%20of%20generic%20image%0Aand%20video%20comprehension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03565v1&entry.124074799=Read"},
{"title": "FLAIR: VLM with Fine-grained Language-informed Image Representations", "author": "Rui Xiao and Sanghwan Kim and Mariana-Iuliana Georgescu and Zeynep Akata and Stephan Alaniz", "abstract": "  CLIP has shown impressive results in aligning images and texts at scale.\nHowever, its ability to capture detailed visual features remains limited\nbecause CLIP matches images and texts at a global level. To address this issue,\nwe propose FLAIR, Fine-grained Language-informed Image Representations, an\napproach that utilizes long and detailed image descriptions to learn localized\nimage embeddings. By sampling diverse sub-captions that describe fine-grained\ndetails about an image, we train our vision-language model to produce not only\nglobal embeddings but also text-specific image representations. Our model\nintroduces text-conditioned attention pooling on top of local image tokens to\nproduce fine-grained image representations that excel at retrieving detailed\nimage content. We achieve state-of-the-art performance on both, existing\nmultimodal retrieval benchmarks, as well as, our newly introduced fine-grained\nretrieval task which evaluates vision-language models' ability to retrieve\npartial image content. Furthermore, our experiments demonstrate the\neffectiveness of FLAIR trained on 30M image-text pairs in capturing\nfine-grained visual information, including zero-shot semantic segmentation,\noutperforming models trained on billions of pairs. Code is available at\nhttps://github.com/ExplainableML/flair .\n", "link": "http://arxiv.org/abs/2412.03561v1", "date": "2024-12-04", "relevancy": 2.8061, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5439}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLAIR%3A%20VLM%20with%20Fine-grained%20Language-informed%20Image%20Representations&body=Title%3A%20FLAIR%3A%20VLM%20with%20Fine-grained%20Language-informed%20Image%20Representations%0AAuthor%3A%20Rui%20Xiao%20and%20Sanghwan%20Kim%20and%20Mariana-Iuliana%20Georgescu%20and%20Zeynep%20Akata%20and%20Stephan%20Alaniz%0AAbstract%3A%20%20%20CLIP%20has%20shown%20impressive%20results%20in%20aligning%20images%20and%20texts%20at%20scale.%0AHowever%2C%20its%20ability%20to%20capture%20detailed%20visual%20features%20remains%20limited%0Abecause%20CLIP%20matches%20images%20and%20texts%20at%20a%20global%20level.%20To%20address%20this%20issue%2C%0Awe%20propose%20FLAIR%2C%20Fine-grained%20Language-informed%20Image%20Representations%2C%20an%0Aapproach%20that%20utilizes%20long%20and%20detailed%20image%20descriptions%20to%20learn%20localized%0Aimage%20embeddings.%20By%20sampling%20diverse%20sub-captions%20that%20describe%20fine-grained%0Adetails%20about%20an%20image%2C%20we%20train%20our%20vision-language%20model%20to%20produce%20not%20only%0Aglobal%20embeddings%20but%20also%20text-specific%20image%20representations.%20Our%20model%0Aintroduces%20text-conditioned%20attention%20pooling%20on%20top%20of%20local%20image%20tokens%20to%0Aproduce%20fine-grained%20image%20representations%20that%20excel%20at%20retrieving%20detailed%0Aimage%20content.%20We%20achieve%20state-of-the-art%20performance%20on%20both%2C%20existing%0Amultimodal%20retrieval%20benchmarks%2C%20as%20well%20as%2C%20our%20newly%20introduced%20fine-grained%0Aretrieval%20task%20which%20evaluates%20vision-language%20models%27%20ability%20to%20retrieve%0Apartial%20image%20content.%20Furthermore%2C%20our%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20FLAIR%20trained%20on%2030M%20image-text%20pairs%20in%20capturing%0Afine-grained%20visual%20information%2C%20including%20zero-shot%20semantic%20segmentation%2C%0Aoutperforming%20models%20trained%20on%20billions%20of%20pairs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ExplainableML/flair%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLAIR%253A%2520VLM%2520with%2520Fine-grained%2520Language-informed%2520Image%2520Representations%26entry.906535625%3DRui%2520Xiao%2520and%2520Sanghwan%2520Kim%2520and%2520Mariana-Iuliana%2520Georgescu%2520and%2520Zeynep%2520Akata%2520and%2520Stephan%2520Alaniz%26entry.1292438233%3D%2520%2520CLIP%2520has%2520shown%2520impressive%2520results%2520in%2520aligning%2520images%2520and%2520texts%2520at%2520scale.%250AHowever%252C%2520its%2520ability%2520to%2520capture%2520detailed%2520visual%2520features%2520remains%2520limited%250Abecause%2520CLIP%2520matches%2520images%2520and%2520texts%2520at%2520a%2520global%2520level.%2520To%2520address%2520this%2520issue%252C%250Awe%2520propose%2520FLAIR%252C%2520Fine-grained%2520Language-informed%2520Image%2520Representations%252C%2520an%250Aapproach%2520that%2520utilizes%2520long%2520and%2520detailed%2520image%2520descriptions%2520to%2520learn%2520localized%250Aimage%2520embeddings.%2520By%2520sampling%2520diverse%2520sub-captions%2520that%2520describe%2520fine-grained%250Adetails%2520about%2520an%2520image%252C%2520we%2520train%2520our%2520vision-language%2520model%2520to%2520produce%2520not%2520only%250Aglobal%2520embeddings%2520but%2520also%2520text-specific%2520image%2520representations.%2520Our%2520model%250Aintroduces%2520text-conditioned%2520attention%2520pooling%2520on%2520top%2520of%2520local%2520image%2520tokens%2520to%250Aproduce%2520fine-grained%2520image%2520representations%2520that%2520excel%2520at%2520retrieving%2520detailed%250Aimage%2520content.%2520We%2520achieve%2520state-of-the-art%2520performance%2520on%2520both%252C%2520existing%250Amultimodal%2520retrieval%2520benchmarks%252C%2520as%2520well%2520as%252C%2520our%2520newly%2520introduced%2520fine-grained%250Aretrieval%2520task%2520which%2520evaluates%2520vision-language%2520models%2527%2520ability%2520to%2520retrieve%250Apartial%2520image%2520content.%2520Furthermore%252C%2520our%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520FLAIR%2520trained%2520on%252030M%2520image-text%2520pairs%2520in%2520capturing%250Afine-grained%2520visual%2520information%252C%2520including%2520zero-shot%2520semantic%2520segmentation%252C%250Aoutperforming%2520models%2520trained%2520on%2520billions%2520of%2520pairs.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/ExplainableML/flair%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLAIR%3A%20VLM%20with%20Fine-grained%20Language-informed%20Image%20Representations&entry.906535625=Rui%20Xiao%20and%20Sanghwan%20Kim%20and%20Mariana-Iuliana%20Georgescu%20and%20Zeynep%20Akata%20and%20Stephan%20Alaniz&entry.1292438233=%20%20CLIP%20has%20shown%20impressive%20results%20in%20aligning%20images%20and%20texts%20at%20scale.%0AHowever%2C%20its%20ability%20to%20capture%20detailed%20visual%20features%20remains%20limited%0Abecause%20CLIP%20matches%20images%20and%20texts%20at%20a%20global%20level.%20To%20address%20this%20issue%2C%0Awe%20propose%20FLAIR%2C%20Fine-grained%20Language-informed%20Image%20Representations%2C%20an%0Aapproach%20that%20utilizes%20long%20and%20detailed%20image%20descriptions%20to%20learn%20localized%0Aimage%20embeddings.%20By%20sampling%20diverse%20sub-captions%20that%20describe%20fine-grained%0Adetails%20about%20an%20image%2C%20we%20train%20our%20vision-language%20model%20to%20produce%20not%20only%0Aglobal%20embeddings%20but%20also%20text-specific%20image%20representations.%20Our%20model%0Aintroduces%20text-conditioned%20attention%20pooling%20on%20top%20of%20local%20image%20tokens%20to%0Aproduce%20fine-grained%20image%20representations%20that%20excel%20at%20retrieving%20detailed%0Aimage%20content.%20We%20achieve%20state-of-the-art%20performance%20on%20both%2C%20existing%0Amultimodal%20retrieval%20benchmarks%2C%20as%20well%20as%2C%20our%20newly%20introduced%20fine-grained%0Aretrieval%20task%20which%20evaluates%20vision-language%20models%27%20ability%20to%20retrieve%0Apartial%20image%20content.%20Furthermore%2C%20our%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20FLAIR%20trained%20on%2030M%20image-text%20pairs%20in%20capturing%0Afine-grained%20visual%20information%2C%20including%20zero-shot%20semantic%20segmentation%2C%0Aoutperforming%20models%20trained%20on%20billions%20of%20pairs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ExplainableML/flair%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03561v1&entry.124074799=Read"},
{"title": "Measure Anything: Real-time, Multi-stage Vision-based Dimensional\n  Measurement using Segment Anything", "author": "Yongkyu Lee and Shivam Kumar Panda and Wei Wang and Mohammad Khalid Jawed", "abstract": "  We present Measure Anything, a comprehensive vision-based framework for\ndimensional measurement of objects with circular cross-sections, leveraging the\nSegment Anything Model (SAM). Our approach estimates key geometric features --\nincluding diameter, length, and volume -- for rod-like geometries with varying\ncurvature and general objects with constant skeleton slope. The framework\nintegrates segmentation, mask processing, skeleton construction, and 2D-3D\ntransformation, packaged in a user-friendly interface. We validate our\nframework by estimating the diameters of Canola stems -- collected from\nagricultural fields in North Dakota -- which are thin and non-uniform, posing\nchallenges for existing methods. Measuring its diameters is critical, as it is\na phenotypic traits that correlates with the health and yield of Canola crops.\nThis application also exemplifies the potential of Measure Anything, where\nintegrating intelligent models -- such as keypoint detection -- extends its\nscalability to fully automate the measurement process for high-throughput\napplications. Furthermore, we showcase its versatility in robotic grasping,\nleveraging extracted geometric features to identify optimal grasp points.\n", "link": "http://arxiv.org/abs/2412.03472v1", "date": "2024-12-04", "relevancy": 2.787, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5649}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5649}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measure%20Anything%3A%20Real-time%2C%20Multi-stage%20Vision-based%20Dimensional%0A%20%20Measurement%20using%20Segment%20Anything&body=Title%3A%20Measure%20Anything%3A%20Real-time%2C%20Multi-stage%20Vision-based%20Dimensional%0A%20%20Measurement%20using%20Segment%20Anything%0AAuthor%3A%20Yongkyu%20Lee%20and%20Shivam%20Kumar%20Panda%20and%20Wei%20Wang%20and%20Mohammad%20Khalid%20Jawed%0AAbstract%3A%20%20%20We%20present%20Measure%20Anything%2C%20a%20comprehensive%20vision-based%20framework%20for%0Adimensional%20measurement%20of%20objects%20with%20circular%20cross-sections%2C%20leveraging%20the%0ASegment%20Anything%20Model%20%28SAM%29.%20Our%20approach%20estimates%20key%20geometric%20features%20--%0Aincluding%20diameter%2C%20length%2C%20and%20volume%20--%20for%20rod-like%20geometries%20with%20varying%0Acurvature%20and%20general%20objects%20with%20constant%20skeleton%20slope.%20The%20framework%0Aintegrates%20segmentation%2C%20mask%20processing%2C%20skeleton%20construction%2C%20and%202D-3D%0Atransformation%2C%20packaged%20in%20a%20user-friendly%20interface.%20We%20validate%20our%0Aframework%20by%20estimating%20the%20diameters%20of%20Canola%20stems%20--%20collected%20from%0Aagricultural%20fields%20in%20North%20Dakota%20--%20which%20are%20thin%20and%20non-uniform%2C%20posing%0Achallenges%20for%20existing%20methods.%20Measuring%20its%20diameters%20is%20critical%2C%20as%20it%20is%0Aa%20phenotypic%20traits%20that%20correlates%20with%20the%20health%20and%20yield%20of%20Canola%20crops.%0AThis%20application%20also%20exemplifies%20the%20potential%20of%20Measure%20Anything%2C%20where%0Aintegrating%20intelligent%20models%20--%20such%20as%20keypoint%20detection%20--%20extends%20its%0Ascalability%20to%20fully%20automate%20the%20measurement%20process%20for%20high-throughput%0Aapplications.%20Furthermore%2C%20we%20showcase%20its%20versatility%20in%20robotic%20grasping%2C%0Aleveraging%20extracted%20geometric%20features%20to%20identify%20optimal%20grasp%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasure%2520Anything%253A%2520Real-time%252C%2520Multi-stage%2520Vision-based%2520Dimensional%250A%2520%2520Measurement%2520using%2520Segment%2520Anything%26entry.906535625%3DYongkyu%2520Lee%2520and%2520Shivam%2520Kumar%2520Panda%2520and%2520Wei%2520Wang%2520and%2520Mohammad%2520Khalid%2520Jawed%26entry.1292438233%3D%2520%2520We%2520present%2520Measure%2520Anything%252C%2520a%2520comprehensive%2520vision-based%2520framework%2520for%250Adimensional%2520measurement%2520of%2520objects%2520with%2520circular%2520cross-sections%252C%2520leveraging%2520the%250ASegment%2520Anything%2520Model%2520%2528SAM%2529.%2520Our%2520approach%2520estimates%2520key%2520geometric%2520features%2520--%250Aincluding%2520diameter%252C%2520length%252C%2520and%2520volume%2520--%2520for%2520rod-like%2520geometries%2520with%2520varying%250Acurvature%2520and%2520general%2520objects%2520with%2520constant%2520skeleton%2520slope.%2520The%2520framework%250Aintegrates%2520segmentation%252C%2520mask%2520processing%252C%2520skeleton%2520construction%252C%2520and%25202D-3D%250Atransformation%252C%2520packaged%2520in%2520a%2520user-friendly%2520interface.%2520We%2520validate%2520our%250Aframework%2520by%2520estimating%2520the%2520diameters%2520of%2520Canola%2520stems%2520--%2520collected%2520from%250Aagricultural%2520fields%2520in%2520North%2520Dakota%2520--%2520which%2520are%2520thin%2520and%2520non-uniform%252C%2520posing%250Achallenges%2520for%2520existing%2520methods.%2520Measuring%2520its%2520diameters%2520is%2520critical%252C%2520as%2520it%2520is%250Aa%2520phenotypic%2520traits%2520that%2520correlates%2520with%2520the%2520health%2520and%2520yield%2520of%2520Canola%2520crops.%250AThis%2520application%2520also%2520exemplifies%2520the%2520potential%2520of%2520Measure%2520Anything%252C%2520where%250Aintegrating%2520intelligent%2520models%2520--%2520such%2520as%2520keypoint%2520detection%2520--%2520extends%2520its%250Ascalability%2520to%2520fully%2520automate%2520the%2520measurement%2520process%2520for%2520high-throughput%250Aapplications.%2520Furthermore%252C%2520we%2520showcase%2520its%2520versatility%2520in%2520robotic%2520grasping%252C%250Aleveraging%2520extracted%2520geometric%2520features%2520to%2520identify%2520optimal%2520grasp%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measure%20Anything%3A%20Real-time%2C%20Multi-stage%20Vision-based%20Dimensional%0A%20%20Measurement%20using%20Segment%20Anything&entry.906535625=Yongkyu%20Lee%20and%20Shivam%20Kumar%20Panda%20and%20Wei%20Wang%20and%20Mohammad%20Khalid%20Jawed&entry.1292438233=%20%20We%20present%20Measure%20Anything%2C%20a%20comprehensive%20vision-based%20framework%20for%0Adimensional%20measurement%20of%20objects%20with%20circular%20cross-sections%2C%20leveraging%20the%0ASegment%20Anything%20Model%20%28SAM%29.%20Our%20approach%20estimates%20key%20geometric%20features%20--%0Aincluding%20diameter%2C%20length%2C%20and%20volume%20--%20for%20rod-like%20geometries%20with%20varying%0Acurvature%20and%20general%20objects%20with%20constant%20skeleton%20slope.%20The%20framework%0Aintegrates%20segmentation%2C%20mask%20processing%2C%20skeleton%20construction%2C%20and%202D-3D%0Atransformation%2C%20packaged%20in%20a%20user-friendly%20interface.%20We%20validate%20our%0Aframework%20by%20estimating%20the%20diameters%20of%20Canola%20stems%20--%20collected%20from%0Aagricultural%20fields%20in%20North%20Dakota%20--%20which%20are%20thin%20and%20non-uniform%2C%20posing%0Achallenges%20for%20existing%20methods.%20Measuring%20its%20diameters%20is%20critical%2C%20as%20it%20is%0Aa%20phenotypic%20traits%20that%20correlates%20with%20the%20health%20and%20yield%20of%20Canola%20crops.%0AThis%20application%20also%20exemplifies%20the%20potential%20of%20Measure%20Anything%2C%20where%0Aintegrating%20intelligent%20models%20--%20such%20as%20keypoint%20detection%20--%20extends%20its%0Ascalability%20to%20fully%20automate%20the%20measurement%20process%20for%20high-throughput%0Aapplications.%20Furthermore%2C%20we%20showcase%20its%20versatility%20in%20robotic%20grasping%2C%0Aleveraging%20extracted%20geometric%20features%20to%20identify%20optimal%20grasp%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03472v1&entry.124074799=Read"},
{"title": "Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular\n  Videos", "author": "Hanxue Liang and Jiawei Ren and Ashkan Mirzaei and Antonio Torralba and Ziwei Liu and Igor Gilitschenski and Sanja Fidler and Cengiz Oztireli and Huan Ling and Zan Gojcic and Jiahui Huang", "abstract": "  Recent advancements in static feed-forward scene reconstruction have\ndemonstrated significant progress in high-quality novel view synthesis.\nHowever, these models often struggle with generalizability across diverse\nenvironments and fail to effectively handle dynamic content. We present BTimer\n(short for BulletTimer), the first motion-aware feed-forward model for\nreal-time reconstruction and novel view synthesis of dynamic scenes. Our\napproach reconstructs the full scene in a 3D Gaussian Splatting representation\nat a given target ('bullet') timestamp by aggregating information from all the\ncontext frames. Such a formulation allows BTimer to gain scalability and\ngeneralization by leveraging both static and dynamic scene datasets. Given a\ncasual monocular dynamic video, BTimer reconstructs a bullet-time scene within\n150ms while reaching state-of-the-art performance on both static and dynamic\nscene datasets, even compared with optimization-based approaches.\n", "link": "http://arxiv.org/abs/2412.03526v1", "date": "2024-12-04", "relevancy": 2.7784, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5713}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5499}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feed-Forward%20Bullet-Time%20Reconstruction%20of%20Dynamic%20Scenes%20from%20Monocular%0A%20%20Videos&body=Title%3A%20Feed-Forward%20Bullet-Time%20Reconstruction%20of%20Dynamic%20Scenes%20from%20Monocular%0A%20%20Videos%0AAuthor%3A%20Hanxue%20Liang%20and%20Jiawei%20Ren%20and%20Ashkan%20Mirzaei%20and%20Antonio%20Torralba%20and%20Ziwei%20Liu%20and%20Igor%20Gilitschenski%20and%20Sanja%20Fidler%20and%20Cengiz%20Oztireli%20and%20Huan%20Ling%20and%20Zan%20Gojcic%20and%20Jiahui%20Huang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20static%20feed-forward%20scene%20reconstruction%20have%0Ademonstrated%20significant%20progress%20in%20high-quality%20novel%20view%20synthesis.%0AHowever%2C%20these%20models%20often%20struggle%20with%20generalizability%20across%20diverse%0Aenvironments%20and%20fail%20to%20effectively%20handle%20dynamic%20content.%20We%20present%20BTimer%0A%28short%20for%20BulletTimer%29%2C%20the%20first%20motion-aware%20feed-forward%20model%20for%0Areal-time%20reconstruction%20and%20novel%20view%20synthesis%20of%20dynamic%20scenes.%20Our%0Aapproach%20reconstructs%20the%20full%20scene%20in%20a%203D%20Gaussian%20Splatting%20representation%0Aat%20a%20given%20target%20%28%27bullet%27%29%20timestamp%20by%20aggregating%20information%20from%20all%20the%0Acontext%20frames.%20Such%20a%20formulation%20allows%20BTimer%20to%20gain%20scalability%20and%0Ageneralization%20by%20leveraging%20both%20static%20and%20dynamic%20scene%20datasets.%20Given%20a%0Acasual%20monocular%20dynamic%20video%2C%20BTimer%20reconstructs%20a%20bullet-time%20scene%20within%0A150ms%20while%20reaching%20state-of-the-art%20performance%20on%20both%20static%20and%20dynamic%0Ascene%20datasets%2C%20even%20compared%20with%20optimization-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeed-Forward%2520Bullet-Time%2520Reconstruction%2520of%2520Dynamic%2520Scenes%2520from%2520Monocular%250A%2520%2520Videos%26entry.906535625%3DHanxue%2520Liang%2520and%2520Jiawei%2520Ren%2520and%2520Ashkan%2520Mirzaei%2520and%2520Antonio%2520Torralba%2520and%2520Ziwei%2520Liu%2520and%2520Igor%2520Gilitschenski%2520and%2520Sanja%2520Fidler%2520and%2520Cengiz%2520Oztireli%2520and%2520Huan%2520Ling%2520and%2520Zan%2520Gojcic%2520and%2520Jiahui%2520Huang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520static%2520feed-forward%2520scene%2520reconstruction%2520have%250Ademonstrated%2520significant%2520progress%2520in%2520high-quality%2520novel%2520view%2520synthesis.%250AHowever%252C%2520these%2520models%2520often%2520struggle%2520with%2520generalizability%2520across%2520diverse%250Aenvironments%2520and%2520fail%2520to%2520effectively%2520handle%2520dynamic%2520content.%2520We%2520present%2520BTimer%250A%2528short%2520for%2520BulletTimer%2529%252C%2520the%2520first%2520motion-aware%2520feed-forward%2520model%2520for%250Areal-time%2520reconstruction%2520and%2520novel%2520view%2520synthesis%2520of%2520dynamic%2520scenes.%2520Our%250Aapproach%2520reconstructs%2520the%2520full%2520scene%2520in%2520a%25203D%2520Gaussian%2520Splatting%2520representation%250Aat%2520a%2520given%2520target%2520%2528%2527bullet%2527%2529%2520timestamp%2520by%2520aggregating%2520information%2520from%2520all%2520the%250Acontext%2520frames.%2520Such%2520a%2520formulation%2520allows%2520BTimer%2520to%2520gain%2520scalability%2520and%250Ageneralization%2520by%2520leveraging%2520both%2520static%2520and%2520dynamic%2520scene%2520datasets.%2520Given%2520a%250Acasual%2520monocular%2520dynamic%2520video%252C%2520BTimer%2520reconstructs%2520a%2520bullet-time%2520scene%2520within%250A150ms%2520while%2520reaching%2520state-of-the-art%2520performance%2520on%2520both%2520static%2520and%2520dynamic%250Ascene%2520datasets%252C%2520even%2520compared%2520with%2520optimization-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feed-Forward%20Bullet-Time%20Reconstruction%20of%20Dynamic%20Scenes%20from%20Monocular%0A%20%20Videos&entry.906535625=Hanxue%20Liang%20and%20Jiawei%20Ren%20and%20Ashkan%20Mirzaei%20and%20Antonio%20Torralba%20and%20Ziwei%20Liu%20and%20Igor%20Gilitschenski%20and%20Sanja%20Fidler%20and%20Cengiz%20Oztireli%20and%20Huan%20Ling%20and%20Zan%20Gojcic%20and%20Jiahui%20Huang&entry.1292438233=%20%20Recent%20advancements%20in%20static%20feed-forward%20scene%20reconstruction%20have%0Ademonstrated%20significant%20progress%20in%20high-quality%20novel%20view%20synthesis.%0AHowever%2C%20these%20models%20often%20struggle%20with%20generalizability%20across%20diverse%0Aenvironments%20and%20fail%20to%20effectively%20handle%20dynamic%20content.%20We%20present%20BTimer%0A%28short%20for%20BulletTimer%29%2C%20the%20first%20motion-aware%20feed-forward%20model%20for%0Areal-time%20reconstruction%20and%20novel%20view%20synthesis%20of%20dynamic%20scenes.%20Our%0Aapproach%20reconstructs%20the%20full%20scene%20in%20a%203D%20Gaussian%20Splatting%20representation%0Aat%20a%20given%20target%20%28%27bullet%27%29%20timestamp%20by%20aggregating%20information%20from%20all%20the%0Acontext%20frames.%20Such%20a%20formulation%20allows%20BTimer%20to%20gain%20scalability%20and%0Ageneralization%20by%20leveraging%20both%20static%20and%20dynamic%20scene%20datasets.%20Given%20a%0Acasual%20monocular%20dynamic%20video%2C%20BTimer%20reconstructs%20a%20bullet-time%20scene%20within%0A150ms%20while%20reaching%20state-of-the-art%20performance%20on%20both%20static%20and%20dynamic%0Ascene%20datasets%2C%20even%20compared%20with%20optimization-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03526v1&entry.124074799=Read"},
{"title": "Beyond [cls]: Exploring the true potential of Masked Image Modeling\n  representations", "author": "Marcin Przewi\u0119\u017alikowski and Randall Balestriero and Wojciech Jasi\u0144ski and Marek \u015amieja and Bartosz Zieli\u0144ski", "abstract": "  Masked Image Modeling (MIM) has emerged as a popular method for\nSelf-Supervised Learning (SSL) of visual representations. However, for\nhigh-level perception tasks, MIM-pretrained models offer lower out-of-the-box\nrepresentation quality than the Joint-Embedding Architectures (JEA) - another\nprominent SSL paradigm. To understand this performance gap, we analyze the\ninformation flow in Vision Transformers (ViT) learned by both approaches. We\nreveal that whereas JEAs construct their representation on a selected set of\nrelevant image fragments, MIM models aggregate nearly whole image content.\nMoreover, we demonstrate that MIM-trained ViTs retain valuable information\nwithin their patch tokens, which is not effectively captured by the global\n[cls] token representations. Therefore, selective aggregation of relevant patch\ntokens, without any fine-tuning, results in consistently higher-quality of MIM\nrepresentations. To our knowledge, we are the first to highlight the lack of\neffective representation aggregation as an emergent issue of MIM and propose\ndirections to address it, contributing to future advances in Self-Supervised\nLearning.\n", "link": "http://arxiv.org/abs/2412.03215v1", "date": "2024-12-04", "relevancy": 2.7721, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5775}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5437}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20%5Bcls%5D%3A%20Exploring%20the%20true%20potential%20of%20Masked%20Image%20Modeling%0A%20%20representations&body=Title%3A%20Beyond%20%5Bcls%5D%3A%20Exploring%20the%20true%20potential%20of%20Masked%20Image%20Modeling%0A%20%20representations%0AAuthor%3A%20Marcin%20Przewi%C4%99%C5%BAlikowski%20and%20Randall%20Balestriero%20and%20Wojciech%20Jasi%C5%84ski%20and%20Marek%20%C5%9Amieja%20and%20Bartosz%20Zieli%C5%84ski%0AAbstract%3A%20%20%20Masked%20Image%20Modeling%20%28MIM%29%20has%20emerged%20as%20a%20popular%20method%20for%0ASelf-Supervised%20Learning%20%28SSL%29%20of%20visual%20representations.%20However%2C%20for%0Ahigh-level%20perception%20tasks%2C%20MIM-pretrained%20models%20offer%20lower%20out-of-the-box%0Arepresentation%20quality%20than%20the%20Joint-Embedding%20Architectures%20%28JEA%29%20-%20another%0Aprominent%20SSL%20paradigm.%20To%20understand%20this%20performance%20gap%2C%20we%20analyze%20the%0Ainformation%20flow%20in%20Vision%20Transformers%20%28ViT%29%20learned%20by%20both%20approaches.%20We%0Areveal%20that%20whereas%20JEAs%20construct%20their%20representation%20on%20a%20selected%20set%20of%0Arelevant%20image%20fragments%2C%20MIM%20models%20aggregate%20nearly%20whole%20image%20content.%0AMoreover%2C%20we%20demonstrate%20that%20MIM-trained%20ViTs%20retain%20valuable%20information%0Awithin%20their%20patch%20tokens%2C%20which%20is%20not%20effectively%20captured%20by%20the%20global%0A%5Bcls%5D%20token%20representations.%20Therefore%2C%20selective%20aggregation%20of%20relevant%20patch%0Atokens%2C%20without%20any%20fine-tuning%2C%20results%20in%20consistently%20higher-quality%20of%20MIM%0Arepresentations.%20To%20our%20knowledge%2C%20we%20are%20the%20first%20to%20highlight%20the%20lack%20of%0Aeffective%20representation%20aggregation%20as%20an%20emergent%20issue%20of%20MIM%20and%20propose%0Adirections%20to%20address%20it%2C%20contributing%20to%20future%20advances%20in%20Self-Supervised%0ALearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520%255Bcls%255D%253A%2520Exploring%2520the%2520true%2520potential%2520of%2520Masked%2520Image%2520Modeling%250A%2520%2520representations%26entry.906535625%3DMarcin%2520Przewi%25C4%2599%25C5%25BAlikowski%2520and%2520Randall%2520Balestriero%2520and%2520Wojciech%2520Jasi%25C5%2584ski%2520and%2520Marek%2520%25C5%259Amieja%2520and%2520Bartosz%2520Zieli%25C5%2584ski%26entry.1292438233%3D%2520%2520Masked%2520Image%2520Modeling%2520%2528MIM%2529%2520has%2520emerged%2520as%2520a%2520popular%2520method%2520for%250ASelf-Supervised%2520Learning%2520%2528SSL%2529%2520of%2520visual%2520representations.%2520However%252C%2520for%250Ahigh-level%2520perception%2520tasks%252C%2520MIM-pretrained%2520models%2520offer%2520lower%2520out-of-the-box%250Arepresentation%2520quality%2520than%2520the%2520Joint-Embedding%2520Architectures%2520%2528JEA%2529%2520-%2520another%250Aprominent%2520SSL%2520paradigm.%2520To%2520understand%2520this%2520performance%2520gap%252C%2520we%2520analyze%2520the%250Ainformation%2520flow%2520in%2520Vision%2520Transformers%2520%2528ViT%2529%2520learned%2520by%2520both%2520approaches.%2520We%250Areveal%2520that%2520whereas%2520JEAs%2520construct%2520their%2520representation%2520on%2520a%2520selected%2520set%2520of%250Arelevant%2520image%2520fragments%252C%2520MIM%2520models%2520aggregate%2520nearly%2520whole%2520image%2520content.%250AMoreover%252C%2520we%2520demonstrate%2520that%2520MIM-trained%2520ViTs%2520retain%2520valuable%2520information%250Awithin%2520their%2520patch%2520tokens%252C%2520which%2520is%2520not%2520effectively%2520captured%2520by%2520the%2520global%250A%255Bcls%255D%2520token%2520representations.%2520Therefore%252C%2520selective%2520aggregation%2520of%2520relevant%2520patch%250Atokens%252C%2520without%2520any%2520fine-tuning%252C%2520results%2520in%2520consistently%2520higher-quality%2520of%2520MIM%250Arepresentations.%2520To%2520our%2520knowledge%252C%2520we%2520are%2520the%2520first%2520to%2520highlight%2520the%2520lack%2520of%250Aeffective%2520representation%2520aggregation%2520as%2520an%2520emergent%2520issue%2520of%2520MIM%2520and%2520propose%250Adirections%2520to%2520address%2520it%252C%2520contributing%2520to%2520future%2520advances%2520in%2520Self-Supervised%250ALearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20%5Bcls%5D%3A%20Exploring%20the%20true%20potential%20of%20Masked%20Image%20Modeling%0A%20%20representations&entry.906535625=Marcin%20Przewi%C4%99%C5%BAlikowski%20and%20Randall%20Balestriero%20and%20Wojciech%20Jasi%C5%84ski%20and%20Marek%20%C5%9Amieja%20and%20Bartosz%20Zieli%C5%84ski&entry.1292438233=%20%20Masked%20Image%20Modeling%20%28MIM%29%20has%20emerged%20as%20a%20popular%20method%20for%0ASelf-Supervised%20Learning%20%28SSL%29%20of%20visual%20representations.%20However%2C%20for%0Ahigh-level%20perception%20tasks%2C%20MIM-pretrained%20models%20offer%20lower%20out-of-the-box%0Arepresentation%20quality%20than%20the%20Joint-Embedding%20Architectures%20%28JEA%29%20-%20another%0Aprominent%20SSL%20paradigm.%20To%20understand%20this%20performance%20gap%2C%20we%20analyze%20the%0Ainformation%20flow%20in%20Vision%20Transformers%20%28ViT%29%20learned%20by%20both%20approaches.%20We%0Areveal%20that%20whereas%20JEAs%20construct%20their%20representation%20on%20a%20selected%20set%20of%0Arelevant%20image%20fragments%2C%20MIM%20models%20aggregate%20nearly%20whole%20image%20content.%0AMoreover%2C%20we%20demonstrate%20that%20MIM-trained%20ViTs%20retain%20valuable%20information%0Awithin%20their%20patch%20tokens%2C%20which%20is%20not%20effectively%20captured%20by%20the%20global%0A%5Bcls%5D%20token%20representations.%20Therefore%2C%20selective%20aggregation%20of%20relevant%20patch%0Atokens%2C%20without%20any%20fine-tuning%2C%20results%20in%20consistently%20higher-quality%20of%20MIM%0Arepresentations.%20To%20our%20knowledge%2C%20we%20are%20the%20first%20to%20highlight%20the%20lack%20of%0Aeffective%20representation%20aggregation%20as%20an%20emergent%20issue%20of%20MIM%20and%20propose%0Adirections%20to%20address%20it%2C%20contributing%20to%20future%20advances%20in%20Self-Supervised%0ALearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03215v1&entry.124074799=Read"},
{"title": "KKLIP: Knowledge Distillation Exploiting K-means Clustering for\n  Language-Image Pre-Training", "author": "Kuei-Chun Kao", "abstract": "  Recently, CLIP has emerged as a valuable model for aligning image and text\ninformation in multi-modal scenarios. However, researchers have observed\nlimitations in the ability of CLIP's text and image encoders to extract\ndetailed knowledge from caption-image pairs. In response, this paper introduces\nKKLIP, a novel approach designed to enhance the quality of CLIP by\nincorporating a new knowledge distillation (KD) method derived from Llama 2.\nOur method comprises three objectives: Text Embedding Distillation, Concept\nLearning, and Contrastive Learning. Firstly, Text Embedding Distillation\ninvolves training the KKLIP text encoder to emulate the teacher model, Llama 2.\nSecondly, Concept Learning assigns a soft concept label to each caption-image\npair through offline k-means clustering of text information from Llama 2,\nallowing KKLIP to learn from these soft concept labels. Finally, Contrastive\nLearning harmonizes text and image embeddings. Our experimental results\ndemonstrate that KKLIP enhances the quality of both text and image encoders.\n", "link": "http://arxiv.org/abs/2412.03513v1", "date": "2024-12-04", "relevancy": 2.7522, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6209}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KKLIP%3A%20Knowledge%20Distillation%20Exploiting%20K-means%20Clustering%20for%0A%20%20Language-Image%20Pre-Training&body=Title%3A%20KKLIP%3A%20Knowledge%20Distillation%20Exploiting%20K-means%20Clustering%20for%0A%20%20Language-Image%20Pre-Training%0AAuthor%3A%20Kuei-Chun%20Kao%0AAbstract%3A%20%20%20Recently%2C%20CLIP%20has%20emerged%20as%20a%20valuable%20model%20for%20aligning%20image%20and%20text%0Ainformation%20in%20multi-modal%20scenarios.%20However%2C%20researchers%20have%20observed%0Alimitations%20in%20the%20ability%20of%20CLIP%27s%20text%20and%20image%20encoders%20to%20extract%0Adetailed%20knowledge%20from%20caption-image%20pairs.%20In%20response%2C%20this%20paper%20introduces%0AKKLIP%2C%20a%20novel%20approach%20designed%20to%20enhance%20the%20quality%20of%20CLIP%20by%0Aincorporating%20a%20new%20knowledge%20distillation%20%28KD%29%20method%20derived%20from%20Llama%202.%0AOur%20method%20comprises%20three%20objectives%3A%20Text%20Embedding%20Distillation%2C%20Concept%0ALearning%2C%20and%20Contrastive%20Learning.%20Firstly%2C%20Text%20Embedding%20Distillation%0Ainvolves%20training%20the%20KKLIP%20text%20encoder%20to%20emulate%20the%20teacher%20model%2C%20Llama%202.%0ASecondly%2C%20Concept%20Learning%20assigns%20a%20soft%20concept%20label%20to%20each%20caption-image%0Apair%20through%20offline%20k-means%20clustering%20of%20text%20information%20from%20Llama%202%2C%0Aallowing%20KKLIP%20to%20learn%20from%20these%20soft%20concept%20labels.%20Finally%2C%20Contrastive%0ALearning%20harmonizes%20text%20and%20image%20embeddings.%20Our%20experimental%20results%0Ademonstrate%20that%20KKLIP%20enhances%20the%20quality%20of%20both%20text%20and%20image%20encoders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKKLIP%253A%2520Knowledge%2520Distillation%2520Exploiting%2520K-means%2520Clustering%2520for%250A%2520%2520Language-Image%2520Pre-Training%26entry.906535625%3DKuei-Chun%2520Kao%26entry.1292438233%3D%2520%2520Recently%252C%2520CLIP%2520has%2520emerged%2520as%2520a%2520valuable%2520model%2520for%2520aligning%2520image%2520and%2520text%250Ainformation%2520in%2520multi-modal%2520scenarios.%2520However%252C%2520researchers%2520have%2520observed%250Alimitations%2520in%2520the%2520ability%2520of%2520CLIP%2527s%2520text%2520and%2520image%2520encoders%2520to%2520extract%250Adetailed%2520knowledge%2520from%2520caption-image%2520pairs.%2520In%2520response%252C%2520this%2520paper%2520introduces%250AKKLIP%252C%2520a%2520novel%2520approach%2520designed%2520to%2520enhance%2520the%2520quality%2520of%2520CLIP%2520by%250Aincorporating%2520a%2520new%2520knowledge%2520distillation%2520%2528KD%2529%2520method%2520derived%2520from%2520Llama%25202.%250AOur%2520method%2520comprises%2520three%2520objectives%253A%2520Text%2520Embedding%2520Distillation%252C%2520Concept%250ALearning%252C%2520and%2520Contrastive%2520Learning.%2520Firstly%252C%2520Text%2520Embedding%2520Distillation%250Ainvolves%2520training%2520the%2520KKLIP%2520text%2520encoder%2520to%2520emulate%2520the%2520teacher%2520model%252C%2520Llama%25202.%250ASecondly%252C%2520Concept%2520Learning%2520assigns%2520a%2520soft%2520concept%2520label%2520to%2520each%2520caption-image%250Apair%2520through%2520offline%2520k-means%2520clustering%2520of%2520text%2520information%2520from%2520Llama%25202%252C%250Aallowing%2520KKLIP%2520to%2520learn%2520from%2520these%2520soft%2520concept%2520labels.%2520Finally%252C%2520Contrastive%250ALearning%2520harmonizes%2520text%2520and%2520image%2520embeddings.%2520Our%2520experimental%2520results%250Ademonstrate%2520that%2520KKLIP%2520enhances%2520the%2520quality%2520of%2520both%2520text%2520and%2520image%2520encoders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KKLIP%3A%20Knowledge%20Distillation%20Exploiting%20K-means%20Clustering%20for%0A%20%20Language-Image%20Pre-Training&entry.906535625=Kuei-Chun%20Kao&entry.1292438233=%20%20Recently%2C%20CLIP%20has%20emerged%20as%20a%20valuable%20model%20for%20aligning%20image%20and%20text%0Ainformation%20in%20multi-modal%20scenarios.%20However%2C%20researchers%20have%20observed%0Alimitations%20in%20the%20ability%20of%20CLIP%27s%20text%20and%20image%20encoders%20to%20extract%0Adetailed%20knowledge%20from%20caption-image%20pairs.%20In%20response%2C%20this%20paper%20introduces%0AKKLIP%2C%20a%20novel%20approach%20designed%20to%20enhance%20the%20quality%20of%20CLIP%20by%0Aincorporating%20a%20new%20knowledge%20distillation%20%28KD%29%20method%20derived%20from%20Llama%202.%0AOur%20method%20comprises%20three%20objectives%3A%20Text%20Embedding%20Distillation%2C%20Concept%0ALearning%2C%20and%20Contrastive%20Learning.%20Firstly%2C%20Text%20Embedding%20Distillation%0Ainvolves%20training%20the%20KKLIP%20text%20encoder%20to%20emulate%20the%20teacher%20model%2C%20Llama%202.%0ASecondly%2C%20Concept%20Learning%20assigns%20a%20soft%20concept%20label%20to%20each%20caption-image%0Apair%20through%20offline%20k-means%20clustering%20of%20text%20information%20from%20Llama%202%2C%0Aallowing%20KKLIP%20to%20learn%20from%20these%20soft%20concept%20labels.%20Finally%2C%20Contrastive%0ALearning%20harmonizes%20text%20and%20image%20embeddings.%20Our%20experimental%20results%0Ademonstrate%20that%20KKLIP%20enhances%20the%20quality%20of%20both%20text%20and%20image%20encoders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03513v1&entry.124074799=Read"},
{"title": "Survey of different Large Language Model Architectures: Trends,\n  Benchmarks, and Challenges", "author": "Minghao Shao and Abdul Basit and Ramesh Karri and Muhammad Shafique", "abstract": "  Large Language Models (LLMs) represent a class of deep learning models adept\nat understanding natural language and generating coherent responses to various\nprompts or queries. These models far exceed the complexity of conventional\nneural networks, often encompassing dozens of neural network layers and\ncontaining billions to trillions of parameters. They are typically trained on\nvast datasets, utilizing architectures based on transformer blocks. Present-day\nLLMs are multi-functional, capable of performing a range of tasks from text\ngeneration and language translation to question answering, as well as code\ngeneration and analysis. An advanced subset of these models, known as\nMultimodal Large Language Models (MLLMs), extends LLM capabilities to process\nand interpret multiple data modalities, including images, audio, and video.\nThis enhancement empowers MLLMs with capabilities like video editing, image\ncomprehension, and captioning for visual content. This survey provides a\ncomprehensive overview of the recent advancements in LLMs. We begin by tracing\nthe evolution of LLMs and subsequently delve into the advent and nuances of\nMLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical\nfeatures, strengths, and limitations. Additionally, we present a comparative\nanalysis of these models and discuss their challenges, potential limitations,\nand prospects for future development.\n", "link": "http://arxiv.org/abs/2412.03220v1", "date": "2024-12-04", "relevancy": 2.749, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Survey%20of%20different%20Large%20Language%20Model%20Architectures%3A%20Trends%2C%0A%20%20Benchmarks%2C%20and%20Challenges&body=Title%3A%20Survey%20of%20different%20Large%20Language%20Model%20Architectures%3A%20Trends%2C%0A%20%20Benchmarks%2C%20and%20Challenges%0AAuthor%3A%20Minghao%20Shao%20and%20Abdul%20Basit%20and%20Ramesh%20Karri%20and%20Muhammad%20Shafique%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20represent%20a%20class%20of%20deep%20learning%20models%20adept%0Aat%20understanding%20natural%20language%20and%20generating%20coherent%20responses%20to%20various%0Aprompts%20or%20queries.%20These%20models%20far%20exceed%20the%20complexity%20of%20conventional%0Aneural%20networks%2C%20often%20encompassing%20dozens%20of%20neural%20network%20layers%20and%0Acontaining%20billions%20to%20trillions%20of%20parameters.%20They%20are%20typically%20trained%20on%0Avast%20datasets%2C%20utilizing%20architectures%20based%20on%20transformer%20blocks.%20Present-day%0ALLMs%20are%20multi-functional%2C%20capable%20of%20performing%20a%20range%20of%20tasks%20from%20text%0Ageneration%20and%20language%20translation%20to%20question%20answering%2C%20as%20well%20as%20code%0Ageneration%20and%20analysis.%20An%20advanced%20subset%20of%20these%20models%2C%20known%20as%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20extends%20LLM%20capabilities%20to%20process%0Aand%20interpret%20multiple%20data%20modalities%2C%20including%20images%2C%20audio%2C%20and%20video.%0AThis%20enhancement%20empowers%20MLLMs%20with%20capabilities%20like%20video%20editing%2C%20image%0Acomprehension%2C%20and%20captioning%20for%20visual%20content.%20This%20survey%20provides%20a%0Acomprehensive%20overview%20of%20the%20recent%20advancements%20in%20LLMs.%20We%20begin%20by%20tracing%0Athe%20evolution%20of%20LLMs%20and%20subsequently%20delve%20into%20the%20advent%20and%20nuances%20of%0AMLLMs.%20We%20analyze%20emerging%20state-of-the-art%20MLLMs%2C%20exploring%20their%20technical%0Afeatures%2C%20strengths%2C%20and%20limitations.%20Additionally%2C%20we%20present%20a%20comparative%0Aanalysis%20of%20these%20models%20and%20discuss%20their%20challenges%2C%20potential%20limitations%2C%0Aand%20prospects%20for%20future%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurvey%2520of%2520different%2520Large%2520Language%2520Model%2520Architectures%253A%2520Trends%252C%250A%2520%2520Benchmarks%252C%2520and%2520Challenges%26entry.906535625%3DMinghao%2520Shao%2520and%2520Abdul%2520Basit%2520and%2520Ramesh%2520Karri%2520and%2520Muhammad%2520Shafique%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520represent%2520a%2520class%2520of%2520deep%2520learning%2520models%2520adept%250Aat%2520understanding%2520natural%2520language%2520and%2520generating%2520coherent%2520responses%2520to%2520various%250Aprompts%2520or%2520queries.%2520These%2520models%2520far%2520exceed%2520the%2520complexity%2520of%2520conventional%250Aneural%2520networks%252C%2520often%2520encompassing%2520dozens%2520of%2520neural%2520network%2520layers%2520and%250Acontaining%2520billions%2520to%2520trillions%2520of%2520parameters.%2520They%2520are%2520typically%2520trained%2520on%250Avast%2520datasets%252C%2520utilizing%2520architectures%2520based%2520on%2520transformer%2520blocks.%2520Present-day%250ALLMs%2520are%2520multi-functional%252C%2520capable%2520of%2520performing%2520a%2520range%2520of%2520tasks%2520from%2520text%250Ageneration%2520and%2520language%2520translation%2520to%2520question%2520answering%252C%2520as%2520well%2520as%2520code%250Ageneration%2520and%2520analysis.%2520An%2520advanced%2520subset%2520of%2520these%2520models%252C%2520known%2520as%250AMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520extends%2520LLM%2520capabilities%2520to%2520process%250Aand%2520interpret%2520multiple%2520data%2520modalities%252C%2520including%2520images%252C%2520audio%252C%2520and%2520video.%250AThis%2520enhancement%2520empowers%2520MLLMs%2520with%2520capabilities%2520like%2520video%2520editing%252C%2520image%250Acomprehension%252C%2520and%2520captioning%2520for%2520visual%2520content.%2520This%2520survey%2520provides%2520a%250Acomprehensive%2520overview%2520of%2520the%2520recent%2520advancements%2520in%2520LLMs.%2520We%2520begin%2520by%2520tracing%250Athe%2520evolution%2520of%2520LLMs%2520and%2520subsequently%2520delve%2520into%2520the%2520advent%2520and%2520nuances%2520of%250AMLLMs.%2520We%2520analyze%2520emerging%2520state-of-the-art%2520MLLMs%252C%2520exploring%2520their%2520technical%250Afeatures%252C%2520strengths%252C%2520and%2520limitations.%2520Additionally%252C%2520we%2520present%2520a%2520comparative%250Aanalysis%2520of%2520these%2520models%2520and%2520discuss%2520their%2520challenges%252C%2520potential%2520limitations%252C%250Aand%2520prospects%2520for%2520future%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Survey%20of%20different%20Large%20Language%20Model%20Architectures%3A%20Trends%2C%0A%20%20Benchmarks%2C%20and%20Challenges&entry.906535625=Minghao%20Shao%20and%20Abdul%20Basit%20and%20Ramesh%20Karri%20and%20Muhammad%20Shafique&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20represent%20a%20class%20of%20deep%20learning%20models%20adept%0Aat%20understanding%20natural%20language%20and%20generating%20coherent%20responses%20to%20various%0Aprompts%20or%20queries.%20These%20models%20far%20exceed%20the%20complexity%20of%20conventional%0Aneural%20networks%2C%20often%20encompassing%20dozens%20of%20neural%20network%20layers%20and%0Acontaining%20billions%20to%20trillions%20of%20parameters.%20They%20are%20typically%20trained%20on%0Avast%20datasets%2C%20utilizing%20architectures%20based%20on%20transformer%20blocks.%20Present-day%0ALLMs%20are%20multi-functional%2C%20capable%20of%20performing%20a%20range%20of%20tasks%20from%20text%0Ageneration%20and%20language%20translation%20to%20question%20answering%2C%20as%20well%20as%20code%0Ageneration%20and%20analysis.%20An%20advanced%20subset%20of%20these%20models%2C%20known%20as%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20extends%20LLM%20capabilities%20to%20process%0Aand%20interpret%20multiple%20data%20modalities%2C%20including%20images%2C%20audio%2C%20and%20video.%0AThis%20enhancement%20empowers%20MLLMs%20with%20capabilities%20like%20video%20editing%2C%20image%0Acomprehension%2C%20and%20captioning%20for%20visual%20content.%20This%20survey%20provides%20a%0Acomprehensive%20overview%20of%20the%20recent%20advancements%20in%20LLMs.%20We%20begin%20by%20tracing%0Athe%20evolution%20of%20LLMs%20and%20subsequently%20delve%20into%20the%20advent%20and%20nuances%20of%0AMLLMs.%20We%20analyze%20emerging%20state-of-the-art%20MLLMs%2C%20exploring%20their%20technical%0Afeatures%2C%20strengths%2C%20and%20limitations.%20Additionally%2C%20we%20present%20a%20comparative%0Aanalysis%20of%20these%20models%20and%20discuss%20their%20challenges%2C%20potential%20limitations%2C%0Aand%20prospects%20for%20future%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03220v1&entry.124074799=Read"},
{"title": "MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation", "author": "Zehuan Huang and Yuan-Chen Guo and Xingqiao An and Yunhan Yang and Yangguang Li and Zi-Xin Zou and Ding Liang and Xihui Liu and Yan-Pei Cao and Lu Sheng", "abstract": "  This paper introduces MIDI, a novel paradigm for compositional 3D scene\ngeneration from a single image. Unlike existing methods that rely on\nreconstruction or retrieval techniques or recent approaches that employ\nmulti-stage object-by-object generation, MIDI extends pre-trained image-to-3D\nobject generation models to multi-instance diffusion models, enabling the\nsimultaneous generation of multiple 3D instances with accurate spatial\nrelationships and high generalizability. At its core, MIDI incorporates a novel\nmulti-instance attention mechanism, that effectively captures inter-object\ninteractions and spatial coherence directly within the generation process,\nwithout the need for complex multi-step processes. The method utilizes partial\nobject images and global scene context as inputs, directly modeling object\ncompletion during 3D generation. During training, we effectively supervise the\ninteractions between 3D instances using a limited amount of scene-level data,\nwhile incorporating single-object data for regularization, thereby maintaining\nthe pre-trained generalization ability. MIDI demonstrates state-of-the-art\nperformance in image-to-scene generation, validated through evaluations on\nsynthetic data, real-world scene data, and stylized scene images generated by\ntext-to-image diffusion models.\n", "link": "http://arxiv.org/abs/2412.03558v1", "date": "2024-12-04", "relevancy": 2.7022, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6862}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6862}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIDI%3A%20Multi-Instance%20Diffusion%20for%20Single%20Image%20to%203D%20Scene%20Generation&body=Title%3A%20MIDI%3A%20Multi-Instance%20Diffusion%20for%20Single%20Image%20to%203D%20Scene%20Generation%0AAuthor%3A%20Zehuan%20Huang%20and%20Yuan-Chen%20Guo%20and%20Xingqiao%20An%20and%20Yunhan%20Yang%20and%20Yangguang%20Li%20and%20Zi-Xin%20Zou%20and%20Ding%20Liang%20and%20Xihui%20Liu%20and%20Yan-Pei%20Cao%20and%20Lu%20Sheng%0AAbstract%3A%20%20%20This%20paper%20introduces%20MIDI%2C%20a%20novel%20paradigm%20for%20compositional%203D%20scene%0Ageneration%20from%20a%20single%20image.%20Unlike%20existing%20methods%20that%20rely%20on%0Areconstruction%20or%20retrieval%20techniques%20or%20recent%20approaches%20that%20employ%0Amulti-stage%20object-by-object%20generation%2C%20MIDI%20extends%20pre-trained%20image-to-3D%0Aobject%20generation%20models%20to%20multi-instance%20diffusion%20models%2C%20enabling%20the%0Asimultaneous%20generation%20of%20multiple%203D%20instances%20with%20accurate%20spatial%0Arelationships%20and%20high%20generalizability.%20At%20its%20core%2C%20MIDI%20incorporates%20a%20novel%0Amulti-instance%20attention%20mechanism%2C%20that%20effectively%20captures%20inter-object%0Ainteractions%20and%20spatial%20coherence%20directly%20within%20the%20generation%20process%2C%0Awithout%20the%20need%20for%20complex%20multi-step%20processes.%20The%20method%20utilizes%20partial%0Aobject%20images%20and%20global%20scene%20context%20as%20inputs%2C%20directly%20modeling%20object%0Acompletion%20during%203D%20generation.%20During%20training%2C%20we%20effectively%20supervise%20the%0Ainteractions%20between%203D%20instances%20using%20a%20limited%20amount%20of%20scene-level%20data%2C%0Awhile%20incorporating%20single-object%20data%20for%20regularization%2C%20thereby%20maintaining%0Athe%20pre-trained%20generalization%20ability.%20MIDI%20demonstrates%20state-of-the-art%0Aperformance%20in%20image-to-scene%20generation%2C%20validated%20through%20evaluations%20on%0Asynthetic%20data%2C%20real-world%20scene%20data%2C%20and%20stylized%20scene%20images%20generated%20by%0Atext-to-image%20diffusion%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIDI%253A%2520Multi-Instance%2520Diffusion%2520for%2520Single%2520Image%2520to%25203D%2520Scene%2520Generation%26entry.906535625%3DZehuan%2520Huang%2520and%2520Yuan-Chen%2520Guo%2520and%2520Xingqiao%2520An%2520and%2520Yunhan%2520Yang%2520and%2520Yangguang%2520Li%2520and%2520Zi-Xin%2520Zou%2520and%2520Ding%2520Liang%2520and%2520Xihui%2520Liu%2520and%2520Yan-Pei%2520Cao%2520and%2520Lu%2520Sheng%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520MIDI%252C%2520a%2520novel%2520paradigm%2520for%2520compositional%25203D%2520scene%250Ageneration%2520from%2520a%2520single%2520image.%2520Unlike%2520existing%2520methods%2520that%2520rely%2520on%250Areconstruction%2520or%2520retrieval%2520techniques%2520or%2520recent%2520approaches%2520that%2520employ%250Amulti-stage%2520object-by-object%2520generation%252C%2520MIDI%2520extends%2520pre-trained%2520image-to-3D%250Aobject%2520generation%2520models%2520to%2520multi-instance%2520diffusion%2520models%252C%2520enabling%2520the%250Asimultaneous%2520generation%2520of%2520multiple%25203D%2520instances%2520with%2520accurate%2520spatial%250Arelationships%2520and%2520high%2520generalizability.%2520At%2520its%2520core%252C%2520MIDI%2520incorporates%2520a%2520novel%250Amulti-instance%2520attention%2520mechanism%252C%2520that%2520effectively%2520captures%2520inter-object%250Ainteractions%2520and%2520spatial%2520coherence%2520directly%2520within%2520the%2520generation%2520process%252C%250Awithout%2520the%2520need%2520for%2520complex%2520multi-step%2520processes.%2520The%2520method%2520utilizes%2520partial%250Aobject%2520images%2520and%2520global%2520scene%2520context%2520as%2520inputs%252C%2520directly%2520modeling%2520object%250Acompletion%2520during%25203D%2520generation.%2520During%2520training%252C%2520we%2520effectively%2520supervise%2520the%250Ainteractions%2520between%25203D%2520instances%2520using%2520a%2520limited%2520amount%2520of%2520scene-level%2520data%252C%250Awhile%2520incorporating%2520single-object%2520data%2520for%2520regularization%252C%2520thereby%2520maintaining%250Athe%2520pre-trained%2520generalization%2520ability.%2520MIDI%2520demonstrates%2520state-of-the-art%250Aperformance%2520in%2520image-to-scene%2520generation%252C%2520validated%2520through%2520evaluations%2520on%250Asynthetic%2520data%252C%2520real-world%2520scene%2520data%252C%2520and%2520stylized%2520scene%2520images%2520generated%2520by%250Atext-to-image%2520diffusion%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIDI%3A%20Multi-Instance%20Diffusion%20for%20Single%20Image%20to%203D%20Scene%20Generation&entry.906535625=Zehuan%20Huang%20and%20Yuan-Chen%20Guo%20and%20Xingqiao%20An%20and%20Yunhan%20Yang%20and%20Yangguang%20Li%20and%20Zi-Xin%20Zou%20and%20Ding%20Liang%20and%20Xihui%20Liu%20and%20Yan-Pei%20Cao%20and%20Lu%20Sheng&entry.1292438233=%20%20This%20paper%20introduces%20MIDI%2C%20a%20novel%20paradigm%20for%20compositional%203D%20scene%0Ageneration%20from%20a%20single%20image.%20Unlike%20existing%20methods%20that%20rely%20on%0Areconstruction%20or%20retrieval%20techniques%20or%20recent%20approaches%20that%20employ%0Amulti-stage%20object-by-object%20generation%2C%20MIDI%20extends%20pre-trained%20image-to-3D%0Aobject%20generation%20models%20to%20multi-instance%20diffusion%20models%2C%20enabling%20the%0Asimultaneous%20generation%20of%20multiple%203D%20instances%20with%20accurate%20spatial%0Arelationships%20and%20high%20generalizability.%20At%20its%20core%2C%20MIDI%20incorporates%20a%20novel%0Amulti-instance%20attention%20mechanism%2C%20that%20effectively%20captures%20inter-object%0Ainteractions%20and%20spatial%20coherence%20directly%20within%20the%20generation%20process%2C%0Awithout%20the%20need%20for%20complex%20multi-step%20processes.%20The%20method%20utilizes%20partial%0Aobject%20images%20and%20global%20scene%20context%20as%20inputs%2C%20directly%20modeling%20object%0Acompletion%20during%203D%20generation.%20During%20training%2C%20we%20effectively%20supervise%20the%0Ainteractions%20between%203D%20instances%20using%20a%20limited%20amount%20of%20scene-level%20data%2C%0Awhile%20incorporating%20single-object%20data%20for%20regularization%2C%20thereby%20maintaining%0Athe%20pre-trained%20generalization%20ability.%20MIDI%20demonstrates%20state-of-the-art%0Aperformance%20in%20image-to-scene%20generation%2C%20validated%20through%20evaluations%20on%0Asynthetic%20data%2C%20real-world%20scene%20data%2C%20and%20stylized%20scene%20images%20generated%20by%0Atext-to-image%20diffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03558v1&entry.124074799=Read"},
{"title": "Monocular Lane Detection Based on Deep Learning: A Survey", "author": "Xin He and Haiyun Guo and Kuan Zhu and Bingke Zhu and Xu Zhao and Jianwu Fang and Jinqiao Wang", "abstract": "  Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on them have demonstrated superior performance and emerged as a\nkey research direction in autonomous driving perception. The core designs of\nthese algorithmic frameworks can be summarized as follows: (1) Task paradigm,\nfocusing on lane instance-level discrimination; (2) Lane modeling, representing\nlanes as a set of learnable parameters in the neural network; (3) Global\ncontext supplementation, enhancing inference on the obscure lanes; (4)\nPerspective effect elimination, providing accurate 3D lanes for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. Besides, this\npaper compares the performance of mainstream methods on different benchmarks\nand investigates their inference speed under a unified setting for fair\ncomparison. Moreover, we present some extended works on lane detection,\nincluding multi-task perception, video lane detection, online high-definition\nmap construction, and lane topology reasoning, to offer readers a comprehensive\nroadmap for the evolution of lane detection. Finally, we point out some\npotential future research directions in this field. We exhaustively collect the\npapers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch.\n", "link": "http://arxiv.org/abs/2411.16316v5", "date": "2024-12-04", "relevancy": 2.6726, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5413}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5408}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monocular%20Lane%20Detection%20Based%20on%20Deep%20Learning%3A%20A%20Survey&body=Title%3A%20Monocular%20Lane%20Detection%20Based%20on%20Deep%20Learning%3A%20A%20Survey%0AAuthor%3A%20Xin%20He%20and%20Haiyun%20Guo%20and%20Kuan%20Zhu%20and%20Bingke%20Zhu%20and%20Xu%20Zhao%20and%20Jianwu%20Fang%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Lane%20detection%20plays%20an%20important%20role%20in%20autonomous%20driving%20perception%0Asystems.%20As%20deep%20learning%20algorithms%20gain%20popularity%2C%20monocular%20lane%20detection%0Amethods%20based%20on%20them%20have%20demonstrated%20superior%20performance%20and%20emerged%20as%20a%0Akey%20research%20direction%20in%20autonomous%20driving%20perception.%20The%20core%20designs%20of%0Athese%20algorithmic%20frameworks%20can%20be%20summarized%20as%20follows%3A%20%281%29%20Task%20paradigm%2C%0Afocusing%20on%20lane%20instance-level%20discrimination%3B%20%282%29%20Lane%20modeling%2C%20representing%0Alanes%20as%20a%20set%20of%20learnable%20parameters%20in%20the%20neural%20network%3B%20%283%29%20Global%0Acontext%20supplementation%2C%20enhancing%20inference%20on%20the%20obscure%20lanes%3B%20%284%29%0APerspective%20effect%20elimination%2C%20providing%20accurate%203D%20lanes%20for%20downstream%0Aapplications.%20From%20these%20perspectives%2C%20this%20paper%20presents%20a%20comprehensive%0Aoverview%20of%20existing%20methods%2C%20encompassing%20both%20the%20increasingly%20mature%202D%20lane%0Adetection%20approaches%20and%20the%20developing%203D%20lane%20detection%20works.%20Besides%2C%20this%0Apaper%20compares%20the%20performance%20of%20mainstream%20methods%20on%20different%20benchmarks%0Aand%20investigates%20their%20inference%20speed%20under%20a%20unified%20setting%20for%20fair%0Acomparison.%20Moreover%2C%20we%20present%20some%20extended%20works%20on%20lane%20detection%2C%0Aincluding%20multi-task%20perception%2C%20video%20lane%20detection%2C%20online%20high-definition%0Amap%20construction%2C%20and%20lane%20topology%20reasoning%2C%20to%20offer%20readers%20a%20comprehensive%0Aroadmap%20for%20the%20evolution%20of%20lane%20detection.%20Finally%2C%20we%20point%20out%20some%0Apotential%20future%20research%20directions%20in%20this%20field.%20We%20exhaustively%20collect%20the%0Apapers%20and%20codes%20of%20existing%20works%20at%0Ahttps%3A//github.com/Core9724/Awesome-Lane-Detection%20and%20will%20keep%20tracing%20the%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16316v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonocular%2520Lane%2520Detection%2520Based%2520on%2520Deep%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DXin%2520He%2520and%2520Haiyun%2520Guo%2520and%2520Kuan%2520Zhu%2520and%2520Bingke%2520Zhu%2520and%2520Xu%2520Zhao%2520and%2520Jianwu%2520Fang%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Lane%2520detection%2520plays%2520an%2520important%2520role%2520in%2520autonomous%2520driving%2520perception%250Asystems.%2520As%2520deep%2520learning%2520algorithms%2520gain%2520popularity%252C%2520monocular%2520lane%2520detection%250Amethods%2520based%2520on%2520them%2520have%2520demonstrated%2520superior%2520performance%2520and%2520emerged%2520as%2520a%250Akey%2520research%2520direction%2520in%2520autonomous%2520driving%2520perception.%2520The%2520core%2520designs%2520of%250Athese%2520algorithmic%2520frameworks%2520can%2520be%2520summarized%2520as%2520follows%253A%2520%25281%2529%2520Task%2520paradigm%252C%250Afocusing%2520on%2520lane%2520instance-level%2520discrimination%253B%2520%25282%2529%2520Lane%2520modeling%252C%2520representing%250Alanes%2520as%2520a%2520set%2520of%2520learnable%2520parameters%2520in%2520the%2520neural%2520network%253B%2520%25283%2529%2520Global%250Acontext%2520supplementation%252C%2520enhancing%2520inference%2520on%2520the%2520obscure%2520lanes%253B%2520%25284%2529%250APerspective%2520effect%2520elimination%252C%2520providing%2520accurate%25203D%2520lanes%2520for%2520downstream%250Aapplications.%2520From%2520these%2520perspectives%252C%2520this%2520paper%2520presents%2520a%2520comprehensive%250Aoverview%2520of%2520existing%2520methods%252C%2520encompassing%2520both%2520the%2520increasingly%2520mature%25202D%2520lane%250Adetection%2520approaches%2520and%2520the%2520developing%25203D%2520lane%2520detection%2520works.%2520Besides%252C%2520this%250Apaper%2520compares%2520the%2520performance%2520of%2520mainstream%2520methods%2520on%2520different%2520benchmarks%250Aand%2520investigates%2520their%2520inference%2520speed%2520under%2520a%2520unified%2520setting%2520for%2520fair%250Acomparison.%2520Moreover%252C%2520we%2520present%2520some%2520extended%2520works%2520on%2520lane%2520detection%252C%250Aincluding%2520multi-task%2520perception%252C%2520video%2520lane%2520detection%252C%2520online%2520high-definition%250Amap%2520construction%252C%2520and%2520lane%2520topology%2520reasoning%252C%2520to%2520offer%2520readers%2520a%2520comprehensive%250Aroadmap%2520for%2520the%2520evolution%2520of%2520lane%2520detection.%2520Finally%252C%2520we%2520point%2520out%2520some%250Apotential%2520future%2520research%2520directions%2520in%2520this%2520field.%2520We%2520exhaustively%2520collect%2520the%250Apapers%2520and%2520codes%2520of%2520existing%2520works%2520at%250Ahttps%253A//github.com/Core9724/Awesome-Lane-Detection%2520and%2520will%2520keep%2520tracing%2520the%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16316v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monocular%20Lane%20Detection%20Based%20on%20Deep%20Learning%3A%20A%20Survey&entry.906535625=Xin%20He%20and%20Haiyun%20Guo%20and%20Kuan%20Zhu%20and%20Bingke%20Zhu%20and%20Xu%20Zhao%20and%20Jianwu%20Fang%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Lane%20detection%20plays%20an%20important%20role%20in%20autonomous%20driving%20perception%0Asystems.%20As%20deep%20learning%20algorithms%20gain%20popularity%2C%20monocular%20lane%20detection%0Amethods%20based%20on%20them%20have%20demonstrated%20superior%20performance%20and%20emerged%20as%20a%0Akey%20research%20direction%20in%20autonomous%20driving%20perception.%20The%20core%20designs%20of%0Athese%20algorithmic%20frameworks%20can%20be%20summarized%20as%20follows%3A%20%281%29%20Task%20paradigm%2C%0Afocusing%20on%20lane%20instance-level%20discrimination%3B%20%282%29%20Lane%20modeling%2C%20representing%0Alanes%20as%20a%20set%20of%20learnable%20parameters%20in%20the%20neural%20network%3B%20%283%29%20Global%0Acontext%20supplementation%2C%20enhancing%20inference%20on%20the%20obscure%20lanes%3B%20%284%29%0APerspective%20effect%20elimination%2C%20providing%20accurate%203D%20lanes%20for%20downstream%0Aapplications.%20From%20these%20perspectives%2C%20this%20paper%20presents%20a%20comprehensive%0Aoverview%20of%20existing%20methods%2C%20encompassing%20both%20the%20increasingly%20mature%202D%20lane%0Adetection%20approaches%20and%20the%20developing%203D%20lane%20detection%20works.%20Besides%2C%20this%0Apaper%20compares%20the%20performance%20of%20mainstream%20methods%20on%20different%20benchmarks%0Aand%20investigates%20their%20inference%20speed%20under%20a%20unified%20setting%20for%20fair%0Acomparison.%20Moreover%2C%20we%20present%20some%20extended%20works%20on%20lane%20detection%2C%0Aincluding%20multi-task%20perception%2C%20video%20lane%20detection%2C%20online%20high-definition%0Amap%20construction%2C%20and%20lane%20topology%20reasoning%2C%20to%20offer%20readers%20a%20comprehensive%0Aroadmap%20for%20the%20evolution%20of%20lane%20detection.%20Finally%2C%20we%20point%20out%20some%0Apotential%20future%20research%20directions%20in%20this%20field.%20We%20exhaustively%20collect%20the%0Apapers%20and%20codes%20of%20existing%20works%20at%0Ahttps%3A//github.com/Core9724/Awesome-Lane-Detection%20and%20will%20keep%20tracing%20the%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16316v5&entry.124074799=Read"},
{"title": "Intuitive Axial Augmentation Using Polar-Sine-Based Piecewise Distortion\n  for Medical Slice-Wise Segmentation", "author": "Yiqin Zhang and Qingkui Chen and Chen Huang and Zhengjie Zhang and Meiling Chen and Zhibing Fu", "abstract": "  Most data-driven models for medical image analysis rely on universal\naugmentations to improve performance. Experimental evidence has confirmed their\neffectiveness, but the unclear mechanism underlying them poses a barrier to the\nwidespread acceptance and trust in such methods within the medical community.\nWe revisit and acknowledge the unique characteristics of medical images apart\nfrom traditional digital images, and consequently, proposed a medical-specific\naugmentation algorithm that is more elastic and aligns well with radiology scan\nprocedure. The method performs piecewise affine with sinusoidal distorted ray\naccording to radius on polar coordinates, thus simulating uncertain postures of\nhuman lying flat on the scanning table. Our method could generate human\nvisceral distribution without affecting the fundamental relative position on\naxial plane. Two non-adaptive algorithms, namely Meta-based Scan Table Removal\nand Similarity-Guided Parameter Search, are introduced to bolster robustness of\nour augmentation method. Experiments show our method improves accuracy across\nmultiple famous segmentation frameworks without requiring more data samples.\nOur preview code is available in: https://github.com/MGAMZ/PSBPD.\n", "link": "http://arxiv.org/abs/2412.03352v1", "date": "2024-12-04", "relevancy": 2.6629, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5595}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5191}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intuitive%20Axial%20Augmentation%20Using%20Polar-Sine-Based%20Piecewise%20Distortion%0A%20%20for%20Medical%20Slice-Wise%20Segmentation&body=Title%3A%20Intuitive%20Axial%20Augmentation%20Using%20Polar-Sine-Based%20Piecewise%20Distortion%0A%20%20for%20Medical%20Slice-Wise%20Segmentation%0AAuthor%3A%20Yiqin%20Zhang%20and%20Qingkui%20Chen%20and%20Chen%20Huang%20and%20Zhengjie%20Zhang%20and%20Meiling%20Chen%20and%20Zhibing%20Fu%0AAbstract%3A%20%20%20Most%20data-driven%20models%20for%20medical%20image%20analysis%20rely%20on%20universal%0Aaugmentations%20to%20improve%20performance.%20Experimental%20evidence%20has%20confirmed%20their%0Aeffectiveness%2C%20but%20the%20unclear%20mechanism%20underlying%20them%20poses%20a%20barrier%20to%20the%0Awidespread%20acceptance%20and%20trust%20in%20such%20methods%20within%20the%20medical%20community.%0AWe%20revisit%20and%20acknowledge%20the%20unique%20characteristics%20of%20medical%20images%20apart%0Afrom%20traditional%20digital%20images%2C%20and%20consequently%2C%20proposed%20a%20medical-specific%0Aaugmentation%20algorithm%20that%20is%20more%20elastic%20and%20aligns%20well%20with%20radiology%20scan%0Aprocedure.%20The%20method%20performs%20piecewise%20affine%20with%20sinusoidal%20distorted%20ray%0Aaccording%20to%20radius%20on%20polar%20coordinates%2C%20thus%20simulating%20uncertain%20postures%20of%0Ahuman%20lying%20flat%20on%20the%20scanning%20table.%20Our%20method%20could%20generate%20human%0Avisceral%20distribution%20without%20affecting%20the%20fundamental%20relative%20position%20on%0Aaxial%20plane.%20Two%20non-adaptive%20algorithms%2C%20namely%20Meta-based%20Scan%20Table%20Removal%0Aand%20Similarity-Guided%20Parameter%20Search%2C%20are%20introduced%20to%20bolster%20robustness%20of%0Aour%20augmentation%20method.%20Experiments%20show%20our%20method%20improves%20accuracy%20across%0Amultiple%20famous%20segmentation%20frameworks%20without%20requiring%20more%20data%20samples.%0AOur%20preview%20code%20is%20available%20in%3A%20https%3A//github.com/MGAMZ/PSBPD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntuitive%2520Axial%2520Augmentation%2520Using%2520Polar-Sine-Based%2520Piecewise%2520Distortion%250A%2520%2520for%2520Medical%2520Slice-Wise%2520Segmentation%26entry.906535625%3DYiqin%2520Zhang%2520and%2520Qingkui%2520Chen%2520and%2520Chen%2520Huang%2520and%2520Zhengjie%2520Zhang%2520and%2520Meiling%2520Chen%2520and%2520Zhibing%2520Fu%26entry.1292438233%3D%2520%2520Most%2520data-driven%2520models%2520for%2520medical%2520image%2520analysis%2520rely%2520on%2520universal%250Aaugmentations%2520to%2520improve%2520performance.%2520Experimental%2520evidence%2520has%2520confirmed%2520their%250Aeffectiveness%252C%2520but%2520the%2520unclear%2520mechanism%2520underlying%2520them%2520poses%2520a%2520barrier%2520to%2520the%250Awidespread%2520acceptance%2520and%2520trust%2520in%2520such%2520methods%2520within%2520the%2520medical%2520community.%250AWe%2520revisit%2520and%2520acknowledge%2520the%2520unique%2520characteristics%2520of%2520medical%2520images%2520apart%250Afrom%2520traditional%2520digital%2520images%252C%2520and%2520consequently%252C%2520proposed%2520a%2520medical-specific%250Aaugmentation%2520algorithm%2520that%2520is%2520more%2520elastic%2520and%2520aligns%2520well%2520with%2520radiology%2520scan%250Aprocedure.%2520The%2520method%2520performs%2520piecewise%2520affine%2520with%2520sinusoidal%2520distorted%2520ray%250Aaccording%2520to%2520radius%2520on%2520polar%2520coordinates%252C%2520thus%2520simulating%2520uncertain%2520postures%2520of%250Ahuman%2520lying%2520flat%2520on%2520the%2520scanning%2520table.%2520Our%2520method%2520could%2520generate%2520human%250Avisceral%2520distribution%2520without%2520affecting%2520the%2520fundamental%2520relative%2520position%2520on%250Aaxial%2520plane.%2520Two%2520non-adaptive%2520algorithms%252C%2520namely%2520Meta-based%2520Scan%2520Table%2520Removal%250Aand%2520Similarity-Guided%2520Parameter%2520Search%252C%2520are%2520introduced%2520to%2520bolster%2520robustness%2520of%250Aour%2520augmentation%2520method.%2520Experiments%2520show%2520our%2520method%2520improves%2520accuracy%2520across%250Amultiple%2520famous%2520segmentation%2520frameworks%2520without%2520requiring%2520more%2520data%2520samples.%250AOur%2520preview%2520code%2520is%2520available%2520in%253A%2520https%253A//github.com/MGAMZ/PSBPD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intuitive%20Axial%20Augmentation%20Using%20Polar-Sine-Based%20Piecewise%20Distortion%0A%20%20for%20Medical%20Slice-Wise%20Segmentation&entry.906535625=Yiqin%20Zhang%20and%20Qingkui%20Chen%20and%20Chen%20Huang%20and%20Zhengjie%20Zhang%20and%20Meiling%20Chen%20and%20Zhibing%20Fu&entry.1292438233=%20%20Most%20data-driven%20models%20for%20medical%20image%20analysis%20rely%20on%20universal%0Aaugmentations%20to%20improve%20performance.%20Experimental%20evidence%20has%20confirmed%20their%0Aeffectiveness%2C%20but%20the%20unclear%20mechanism%20underlying%20them%20poses%20a%20barrier%20to%20the%0Awidespread%20acceptance%20and%20trust%20in%20such%20methods%20within%20the%20medical%20community.%0AWe%20revisit%20and%20acknowledge%20the%20unique%20characteristics%20of%20medical%20images%20apart%0Afrom%20traditional%20digital%20images%2C%20and%20consequently%2C%20proposed%20a%20medical-specific%0Aaugmentation%20algorithm%20that%20is%20more%20elastic%20and%20aligns%20well%20with%20radiology%20scan%0Aprocedure.%20The%20method%20performs%20piecewise%20affine%20with%20sinusoidal%20distorted%20ray%0Aaccording%20to%20radius%20on%20polar%20coordinates%2C%20thus%20simulating%20uncertain%20postures%20of%0Ahuman%20lying%20flat%20on%20the%20scanning%20table.%20Our%20method%20could%20generate%20human%0Avisceral%20distribution%20without%20affecting%20the%20fundamental%20relative%20position%20on%0Aaxial%20plane.%20Two%20non-adaptive%20algorithms%2C%20namely%20Meta-based%20Scan%20Table%20Removal%0Aand%20Similarity-Guided%20Parameter%20Search%2C%20are%20introduced%20to%20bolster%20robustness%20of%0Aour%20augmentation%20method.%20Experiments%20show%20our%20method%20improves%20accuracy%20across%0Amultiple%20famous%20segmentation%20frameworks%20without%20requiring%20more%20data%20samples.%0AOur%20preview%20code%20is%20available%20in%3A%20https%3A//github.com/MGAMZ/PSBPD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03352v1&entry.124074799=Read"},
{"title": "Cluster Specific Representation Learning", "author": "Mahalakshmi Sabanayagam and Omar Al-Dabooni and Pascal Esser", "abstract": "  Representation learning aims to extract meaningful lower-dimensional\nembeddings from data, known as representations. Despite its widespread\napplication, there is no established definition of a ``good'' representation.\nTypically, the representation quality is evaluated based on its performance in\ndownstream tasks such as clustering, de-noising, etc. However, this\ntask-specific approach has a limitation where a representation that performs\nwell for one task may not necessarily be effective for another. This highlights\nthe need for a more agnostic formulation, which is the focus of our work. We\npropose a downstream-agnostic formulation: when inherent clusters exist in the\ndata, the representations should be specific to each cluster. Under this idea,\nwe develop a meta-algorithm that jointly learns cluster-specific\nrepresentations and cluster assignments. As our approach is easy to integrate\nwith any representation learning framework, we demonstrate its effectiveness in\nvarious setups, including Autoencoders, Variational Autoencoders, Contrastive\nlearning models, and Restricted Boltzmann Machines. We qualitatively compare\nour cluster-specific embeddings to standard embeddings and downstream tasks\nsuch as de-noising and clustering. While our method slightly increases runtime\nand parameters compared to the standard model, the experiments clearly show\nthat it extracts the inherent cluster structures in the data, resulting in\nimproved performance in relevant applications.\n", "link": "http://arxiv.org/abs/2412.03471v1", "date": "2024-12-04", "relevancy": 2.6487, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5627}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5408}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cluster%20Specific%20Representation%20Learning&body=Title%3A%20Cluster%20Specific%20Representation%20Learning%0AAuthor%3A%20Mahalakshmi%20Sabanayagam%20and%20Omar%20Al-Dabooni%20and%20Pascal%20Esser%0AAbstract%3A%20%20%20Representation%20learning%20aims%20to%20extract%20meaningful%20lower-dimensional%0Aembeddings%20from%20data%2C%20known%20as%20representations.%20Despite%20its%20widespread%0Aapplication%2C%20there%20is%20no%20established%20definition%20of%20a%20%60%60good%27%27%20representation.%0ATypically%2C%20the%20representation%20quality%20is%20evaluated%20based%20on%20its%20performance%20in%0Adownstream%20tasks%20such%20as%20clustering%2C%20de-noising%2C%20etc.%20However%2C%20this%0Atask-specific%20approach%20has%20a%20limitation%20where%20a%20representation%20that%20performs%0Awell%20for%20one%20task%20may%20not%20necessarily%20be%20effective%20for%20another.%20This%20highlights%0Athe%20need%20for%20a%20more%20agnostic%20formulation%2C%20which%20is%20the%20focus%20of%20our%20work.%20We%0Apropose%20a%20downstream-agnostic%20formulation%3A%20when%20inherent%20clusters%20exist%20in%20the%0Adata%2C%20the%20representations%20should%20be%20specific%20to%20each%20cluster.%20Under%20this%20idea%2C%0Awe%20develop%20a%20meta-algorithm%20that%20jointly%20learns%20cluster-specific%0Arepresentations%20and%20cluster%20assignments.%20As%20our%20approach%20is%20easy%20to%20integrate%0Awith%20any%20representation%20learning%20framework%2C%20we%20demonstrate%20its%20effectiveness%20in%0Avarious%20setups%2C%20including%20Autoencoders%2C%20Variational%20Autoencoders%2C%20Contrastive%0Alearning%20models%2C%20and%20Restricted%20Boltzmann%20Machines.%20We%20qualitatively%20compare%0Aour%20cluster-specific%20embeddings%20to%20standard%20embeddings%20and%20downstream%20tasks%0Asuch%20as%20de-noising%20and%20clustering.%20While%20our%20method%20slightly%20increases%20runtime%0Aand%20parameters%20compared%20to%20the%20standard%20model%2C%20the%20experiments%20clearly%20show%0Athat%20it%20extracts%20the%20inherent%20cluster%20structures%20in%20the%20data%2C%20resulting%20in%0Aimproved%20performance%20in%20relevant%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCluster%2520Specific%2520Representation%2520Learning%26entry.906535625%3DMahalakshmi%2520Sabanayagam%2520and%2520Omar%2520Al-Dabooni%2520and%2520Pascal%2520Esser%26entry.1292438233%3D%2520%2520Representation%2520learning%2520aims%2520to%2520extract%2520meaningful%2520lower-dimensional%250Aembeddings%2520from%2520data%252C%2520known%2520as%2520representations.%2520Despite%2520its%2520widespread%250Aapplication%252C%2520there%2520is%2520no%2520established%2520definition%2520of%2520a%2520%2560%2560good%2527%2527%2520representation.%250ATypically%252C%2520the%2520representation%2520quality%2520is%2520evaluated%2520based%2520on%2520its%2520performance%2520in%250Adownstream%2520tasks%2520such%2520as%2520clustering%252C%2520de-noising%252C%2520etc.%2520However%252C%2520this%250Atask-specific%2520approach%2520has%2520a%2520limitation%2520where%2520a%2520representation%2520that%2520performs%250Awell%2520for%2520one%2520task%2520may%2520not%2520necessarily%2520be%2520effective%2520for%2520another.%2520This%2520highlights%250Athe%2520need%2520for%2520a%2520more%2520agnostic%2520formulation%252C%2520which%2520is%2520the%2520focus%2520of%2520our%2520work.%2520We%250Apropose%2520a%2520downstream-agnostic%2520formulation%253A%2520when%2520inherent%2520clusters%2520exist%2520in%2520the%250Adata%252C%2520the%2520representations%2520should%2520be%2520specific%2520to%2520each%2520cluster.%2520Under%2520this%2520idea%252C%250Awe%2520develop%2520a%2520meta-algorithm%2520that%2520jointly%2520learns%2520cluster-specific%250Arepresentations%2520and%2520cluster%2520assignments.%2520As%2520our%2520approach%2520is%2520easy%2520to%2520integrate%250Awith%2520any%2520representation%2520learning%2520framework%252C%2520we%2520demonstrate%2520its%2520effectiveness%2520in%250Avarious%2520setups%252C%2520including%2520Autoencoders%252C%2520Variational%2520Autoencoders%252C%2520Contrastive%250Alearning%2520models%252C%2520and%2520Restricted%2520Boltzmann%2520Machines.%2520We%2520qualitatively%2520compare%250Aour%2520cluster-specific%2520embeddings%2520to%2520standard%2520embeddings%2520and%2520downstream%2520tasks%250Asuch%2520as%2520de-noising%2520and%2520clustering.%2520While%2520our%2520method%2520slightly%2520increases%2520runtime%250Aand%2520parameters%2520compared%2520to%2520the%2520standard%2520model%252C%2520the%2520experiments%2520clearly%2520show%250Athat%2520it%2520extracts%2520the%2520inherent%2520cluster%2520structures%2520in%2520the%2520data%252C%2520resulting%2520in%250Aimproved%2520performance%2520in%2520relevant%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cluster%20Specific%20Representation%20Learning&entry.906535625=Mahalakshmi%20Sabanayagam%20and%20Omar%20Al-Dabooni%20and%20Pascal%20Esser&entry.1292438233=%20%20Representation%20learning%20aims%20to%20extract%20meaningful%20lower-dimensional%0Aembeddings%20from%20data%2C%20known%20as%20representations.%20Despite%20its%20widespread%0Aapplication%2C%20there%20is%20no%20established%20definition%20of%20a%20%60%60good%27%27%20representation.%0ATypically%2C%20the%20representation%20quality%20is%20evaluated%20based%20on%20its%20performance%20in%0Adownstream%20tasks%20such%20as%20clustering%2C%20de-noising%2C%20etc.%20However%2C%20this%0Atask-specific%20approach%20has%20a%20limitation%20where%20a%20representation%20that%20performs%0Awell%20for%20one%20task%20may%20not%20necessarily%20be%20effective%20for%20another.%20This%20highlights%0Athe%20need%20for%20a%20more%20agnostic%20formulation%2C%20which%20is%20the%20focus%20of%20our%20work.%20We%0Apropose%20a%20downstream-agnostic%20formulation%3A%20when%20inherent%20clusters%20exist%20in%20the%0Adata%2C%20the%20representations%20should%20be%20specific%20to%20each%20cluster.%20Under%20this%20idea%2C%0Awe%20develop%20a%20meta-algorithm%20that%20jointly%20learns%20cluster-specific%0Arepresentations%20and%20cluster%20assignments.%20As%20our%20approach%20is%20easy%20to%20integrate%0Awith%20any%20representation%20learning%20framework%2C%20we%20demonstrate%20its%20effectiveness%20in%0Avarious%20setups%2C%20including%20Autoencoders%2C%20Variational%20Autoencoders%2C%20Contrastive%0Alearning%20models%2C%20and%20Restricted%20Boltzmann%20Machines.%20We%20qualitatively%20compare%0Aour%20cluster-specific%20embeddings%20to%20standard%20embeddings%20and%20downstream%20tasks%0Asuch%20as%20de-noising%20and%20clustering.%20While%20our%20method%20slightly%20increases%20runtime%0Aand%20parameters%20compared%20to%20the%20standard%20model%2C%20the%20experiments%20clearly%20show%0Athat%20it%20extracts%20the%20inherent%20cluster%20structures%20in%20the%20data%2C%20resulting%20in%0Aimproved%20performance%20in%20relevant%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03471v1&entry.124074799=Read"},
{"title": "A Spatio-Temporal Representation Learning as an Alternative to\n  Traditional Glosses in Sign Language Translation and Production", "author": "Eui Jun Hwang and Sukmin Cho and Huije Lee and Youngwoo Yoon and Jong C. Park", "abstract": "  This work addresses the challenges associated with the use of glosses in both\nSign Language Translation (SLT) and Sign Language Production (SLP). While\nglosses have long been used as a bridge between sign language and spoken\nlanguage, they come with two major limitations that impede the advancement of\nsign language systems. First, annotating the glosses is a labor-intensive and\ntime-consuming process, which limits the scalability of datasets. Second, the\nglosses oversimplify sign language by stripping away its spatio-temporal\ndynamics, reducing complex signs to basic labels and missing the subtle\nmovements essential for precise interpretation. To address these limitations,\nwe introduce Universal Gloss-level Representation (UniGloR), a framework\ndesigned to capture the spatio-temporal features inherent in sign language,\nproviding a more dynamic and detailed alternative to the use of the glosses.\nThe core idea of UniGloR is simple yet effective: We derive dense\nspatio-temporal representations from sign keypoint sequences using\nself-supervised learning and seamlessly integrate them into SLT and SLP tasks.\nOur experiments in a keypoint-based setting demonstrate that UniGloR either\noutperforms or matches the performance of previous SLT and SLP methods on two\nwidely-used datasets: PHOENIX14T and How2Sign.\n", "link": "http://arxiv.org/abs/2407.02854v2", "date": "2024-12-04", "relevancy": 2.6479, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5472}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5336}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Spatio-Temporal%20Representation%20Learning%20as%20an%20Alternative%20to%0A%20%20Traditional%20Glosses%20in%20Sign%20Language%20Translation%20and%20Production&body=Title%3A%20A%20Spatio-Temporal%20Representation%20Learning%20as%20an%20Alternative%20to%0A%20%20Traditional%20Glosses%20in%20Sign%20Language%20Translation%20and%20Production%0AAuthor%3A%20Eui%20Jun%20Hwang%20and%20Sukmin%20Cho%20and%20Huije%20Lee%20and%20Youngwoo%20Yoon%20and%20Jong%20C.%20Park%0AAbstract%3A%20%20%20This%20work%20addresses%20the%20challenges%20associated%20with%20the%20use%20of%20glosses%20in%20both%0ASign%20Language%20Translation%20%28SLT%29%20and%20Sign%20Language%20Production%20%28SLP%29.%20While%0Aglosses%20have%20long%20been%20used%20as%20a%20bridge%20between%20sign%20language%20and%20spoken%0Alanguage%2C%20they%20come%20with%20two%20major%20limitations%20that%20impede%20the%20advancement%20of%0Asign%20language%20systems.%20First%2C%20annotating%20the%20glosses%20is%20a%20labor-intensive%20and%0Atime-consuming%20process%2C%20which%20limits%20the%20scalability%20of%20datasets.%20Second%2C%20the%0Aglosses%20oversimplify%20sign%20language%20by%20stripping%20away%20its%20spatio-temporal%0Adynamics%2C%20reducing%20complex%20signs%20to%20basic%20labels%20and%20missing%20the%20subtle%0Amovements%20essential%20for%20precise%20interpretation.%20To%20address%20these%20limitations%2C%0Awe%20introduce%20Universal%20Gloss-level%20Representation%20%28UniGloR%29%2C%20a%20framework%0Adesigned%20to%20capture%20the%20spatio-temporal%20features%20inherent%20in%20sign%20language%2C%0Aproviding%20a%20more%20dynamic%20and%20detailed%20alternative%20to%20the%20use%20of%20the%20glosses.%0AThe%20core%20idea%20of%20UniGloR%20is%20simple%20yet%20effective%3A%20We%20derive%20dense%0Aspatio-temporal%20representations%20from%20sign%20keypoint%20sequences%20using%0Aself-supervised%20learning%20and%20seamlessly%20integrate%20them%20into%20SLT%20and%20SLP%20tasks.%0AOur%20experiments%20in%20a%20keypoint-based%20setting%20demonstrate%20that%20UniGloR%20either%0Aoutperforms%20or%20matches%20the%20performance%20of%20previous%20SLT%20and%20SLP%20methods%20on%20two%0Awidely-used%20datasets%3A%20PHOENIX14T%20and%20How2Sign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02854v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Spatio-Temporal%2520Representation%2520Learning%2520as%2520an%2520Alternative%2520to%250A%2520%2520Traditional%2520Glosses%2520in%2520Sign%2520Language%2520Translation%2520and%2520Production%26entry.906535625%3DEui%2520Jun%2520Hwang%2520and%2520Sukmin%2520Cho%2520and%2520Huije%2520Lee%2520and%2520Youngwoo%2520Yoon%2520and%2520Jong%2520C.%2520Park%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520the%2520challenges%2520associated%2520with%2520the%2520use%2520of%2520glosses%2520in%2520both%250ASign%2520Language%2520Translation%2520%2528SLT%2529%2520and%2520Sign%2520Language%2520Production%2520%2528SLP%2529.%2520While%250Aglosses%2520have%2520long%2520been%2520used%2520as%2520a%2520bridge%2520between%2520sign%2520language%2520and%2520spoken%250Alanguage%252C%2520they%2520come%2520with%2520two%2520major%2520limitations%2520that%2520impede%2520the%2520advancement%2520of%250Asign%2520language%2520systems.%2520First%252C%2520annotating%2520the%2520glosses%2520is%2520a%2520labor-intensive%2520and%250Atime-consuming%2520process%252C%2520which%2520limits%2520the%2520scalability%2520of%2520datasets.%2520Second%252C%2520the%250Aglosses%2520oversimplify%2520sign%2520language%2520by%2520stripping%2520away%2520its%2520spatio-temporal%250Adynamics%252C%2520reducing%2520complex%2520signs%2520to%2520basic%2520labels%2520and%2520missing%2520the%2520subtle%250Amovements%2520essential%2520for%2520precise%2520interpretation.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520introduce%2520Universal%2520Gloss-level%2520Representation%2520%2528UniGloR%2529%252C%2520a%2520framework%250Adesigned%2520to%2520capture%2520the%2520spatio-temporal%2520features%2520inherent%2520in%2520sign%2520language%252C%250Aproviding%2520a%2520more%2520dynamic%2520and%2520detailed%2520alternative%2520to%2520the%2520use%2520of%2520the%2520glosses.%250AThe%2520core%2520idea%2520of%2520UniGloR%2520is%2520simple%2520yet%2520effective%253A%2520We%2520derive%2520dense%250Aspatio-temporal%2520representations%2520from%2520sign%2520keypoint%2520sequences%2520using%250Aself-supervised%2520learning%2520and%2520seamlessly%2520integrate%2520them%2520into%2520SLT%2520and%2520SLP%2520tasks.%250AOur%2520experiments%2520in%2520a%2520keypoint-based%2520setting%2520demonstrate%2520that%2520UniGloR%2520either%250Aoutperforms%2520or%2520matches%2520the%2520performance%2520of%2520previous%2520SLT%2520and%2520SLP%2520methods%2520on%2520two%250Awidely-used%2520datasets%253A%2520PHOENIX14T%2520and%2520How2Sign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02854v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Spatio-Temporal%20Representation%20Learning%20as%20an%20Alternative%20to%0A%20%20Traditional%20Glosses%20in%20Sign%20Language%20Translation%20and%20Production&entry.906535625=Eui%20Jun%20Hwang%20and%20Sukmin%20Cho%20and%20Huije%20Lee%20and%20Youngwoo%20Yoon%20and%20Jong%20C.%20Park&entry.1292438233=%20%20This%20work%20addresses%20the%20challenges%20associated%20with%20the%20use%20of%20glosses%20in%20both%0ASign%20Language%20Translation%20%28SLT%29%20and%20Sign%20Language%20Production%20%28SLP%29.%20While%0Aglosses%20have%20long%20been%20used%20as%20a%20bridge%20between%20sign%20language%20and%20spoken%0Alanguage%2C%20they%20come%20with%20two%20major%20limitations%20that%20impede%20the%20advancement%20of%0Asign%20language%20systems.%20First%2C%20annotating%20the%20glosses%20is%20a%20labor-intensive%20and%0Atime-consuming%20process%2C%20which%20limits%20the%20scalability%20of%20datasets.%20Second%2C%20the%0Aglosses%20oversimplify%20sign%20language%20by%20stripping%20away%20its%20spatio-temporal%0Adynamics%2C%20reducing%20complex%20signs%20to%20basic%20labels%20and%20missing%20the%20subtle%0Amovements%20essential%20for%20precise%20interpretation.%20To%20address%20these%20limitations%2C%0Awe%20introduce%20Universal%20Gloss-level%20Representation%20%28UniGloR%29%2C%20a%20framework%0Adesigned%20to%20capture%20the%20spatio-temporal%20features%20inherent%20in%20sign%20language%2C%0Aproviding%20a%20more%20dynamic%20and%20detailed%20alternative%20to%20the%20use%20of%20the%20glosses.%0AThe%20core%20idea%20of%20UniGloR%20is%20simple%20yet%20effective%3A%20We%20derive%20dense%0Aspatio-temporal%20representations%20from%20sign%20keypoint%20sequences%20using%0Aself-supervised%20learning%20and%20seamlessly%20integrate%20them%20into%20SLT%20and%20SLP%20tasks.%0AOur%20experiments%20in%20a%20keypoint-based%20setting%20demonstrate%20that%20UniGloR%20either%0Aoutperforms%20or%20matches%20the%20performance%20of%20previous%20SLT%20and%20SLP%20methods%20on%20two%0Awidely-used%20datasets%3A%20PHOENIX14T%20and%20How2Sign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02854v2&entry.124074799=Read"},
{"title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation", "author": "Ao Wang and Hui Chen and Jianchao Tan and Kefeng Zhang and Xunliang Cai and Zijia Lin and Jungong Han and Guiguang Ding", "abstract": "  Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.\n", "link": "http://arxiv.org/abs/2412.03409v1", "date": "2024-12-04", "relevancy": 2.6415, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrefixKV%3A%20Adaptive%20Prefix%20KV%20Cache%20is%20What%20Vision%20Instruction-Following%0A%20%20Models%20Need%20for%20Efficient%20Generation&body=Title%3A%20PrefixKV%3A%20Adaptive%20Prefix%20KV%20Cache%20is%20What%20Vision%20Instruction-Following%0A%20%20Models%20Need%20for%20Efficient%20Generation%0AAuthor%3A%20Ao%20Wang%20and%20Hui%20Chen%20and%20Jianchao%20Tan%20and%20Kefeng%20Zhang%20and%20Xunliang%20Cai%20and%20Zijia%20Lin%20and%20Jungong%20Han%20and%20Guiguang%20Ding%0AAbstract%3A%20%20%20Recently%2C%20large%20vision-language%20models%20%28LVLMs%29%20have%20rapidly%20gained%20popularity%0Afor%20their%20strong%20generation%20and%20reasoning%20capabilities%20given%20diverse%20multimodal%0Ainputs.%20However%2C%20these%20models%20incur%20significant%20computational%20and%20memory%0Aoverhead%20during%20inference%2C%20which%20greatly%20hinders%20the%20efficient%20deployment%20in%0Apractical%20scenarios.%20The%20extensive%20key-value%20%28KV%29%20cache%2C%20necessitated%20by%20the%0Alengthy%20input%20and%20output%20sequences%2C%20notably%20contributes%20to%20the%20high%20inference%0Acost.%20Based%20on%20this%2C%20recent%20works%20have%20investigated%20ways%20to%20reduce%20the%20KV%20cache%0Asize%20for%20higher%20efficiency.%20Although%20effective%2C%20they%20generally%20overlook%20the%0Adistinct%20importance%20distributions%20of%20KV%20vectors%20across%20layers%20and%20maintain%20the%0Asame%20cache%20size%20for%20each%20layer%20during%20the%20next%20token%20prediction.%20This%20results%0Ain%20the%20significant%20contextual%20information%20loss%20for%20certain%20layers%2C%20leading%20to%0Anotable%20performance%20decline.%20To%20address%20this%2C%20we%20present%20PrefixKV.%20It%20reframes%0Athe%20challenge%20of%20determining%20KV%20cache%20sizes%20for%20all%20layers%20into%20the%20task%20of%0Asearching%20for%20the%20optimal%20global%20prefix%20configuration.%20With%20an%20adaptive%0Alayer-wise%20KV%20retention%20recipe%20based%20on%20binary%20search%2C%20the%20maximum%20contextual%0Ainformation%20can%20thus%20be%20preserved%20in%20each%20layer%2C%20facilitating%20the%20generation.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20the%20state-of-the-art%0Aperformance%20compared%20with%20others.%20It%20exhibits%20superior%20inference%20efficiency%20and%0Ageneration%20quality%20trade-offs%2C%20showing%20promising%20potential%20for%20practical%0Aapplications.%20Code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/THU-MIG/PrefixKV%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrefixKV%253A%2520Adaptive%2520Prefix%2520KV%2520Cache%2520is%2520What%2520Vision%2520Instruction-Following%250A%2520%2520Models%2520Need%2520for%2520Efficient%2520Generation%26entry.906535625%3DAo%2520Wang%2520and%2520Hui%2520Chen%2520and%2520Jianchao%2520Tan%2520and%2520Kefeng%2520Zhang%2520and%2520Xunliang%2520Cai%2520and%2520Zijia%2520Lin%2520and%2520Jungong%2520Han%2520and%2520Guiguang%2520Ding%26entry.1292438233%3D%2520%2520Recently%252C%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520rapidly%2520gained%2520popularity%250Afor%2520their%2520strong%2520generation%2520and%2520reasoning%2520capabilities%2520given%2520diverse%2520multimodal%250Ainputs.%2520However%252C%2520these%2520models%2520incur%2520significant%2520computational%2520and%2520memory%250Aoverhead%2520during%2520inference%252C%2520which%2520greatly%2520hinders%2520the%2520efficient%2520deployment%2520in%250Apractical%2520scenarios.%2520The%2520extensive%2520key-value%2520%2528KV%2529%2520cache%252C%2520necessitated%2520by%2520the%250Alengthy%2520input%2520and%2520output%2520sequences%252C%2520notably%2520contributes%2520to%2520the%2520high%2520inference%250Acost.%2520Based%2520on%2520this%252C%2520recent%2520works%2520have%2520investigated%2520ways%2520to%2520reduce%2520the%2520KV%2520cache%250Asize%2520for%2520higher%2520efficiency.%2520Although%2520effective%252C%2520they%2520generally%2520overlook%2520the%250Adistinct%2520importance%2520distributions%2520of%2520KV%2520vectors%2520across%2520layers%2520and%2520maintain%2520the%250Asame%2520cache%2520size%2520for%2520each%2520layer%2520during%2520the%2520next%2520token%2520prediction.%2520This%2520results%250Ain%2520the%2520significant%2520contextual%2520information%2520loss%2520for%2520certain%2520layers%252C%2520leading%2520to%250Anotable%2520performance%2520decline.%2520To%2520address%2520this%252C%2520we%2520present%2520PrefixKV.%2520It%2520reframes%250Athe%2520challenge%2520of%2520determining%2520KV%2520cache%2520sizes%2520for%2520all%2520layers%2520into%2520the%2520task%2520of%250Asearching%2520for%2520the%2520optimal%2520global%2520prefix%2520configuration.%2520With%2520an%2520adaptive%250Alayer-wise%2520KV%2520retention%2520recipe%2520based%2520on%2520binary%2520search%252C%2520the%2520maximum%2520contextual%250Ainformation%2520can%2520thus%2520be%2520preserved%2520in%2520each%2520layer%252C%2520facilitating%2520the%2520generation.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520the%2520state-of-the-art%250Aperformance%2520compared%2520with%2520others.%2520It%2520exhibits%2520superior%2520inference%2520efficiency%2520and%250Ageneration%2520quality%2520trade-offs%252C%2520showing%2520promising%2520potential%2520for%2520practical%250Aapplications.%2520Code%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/THU-MIG/PrefixKV%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrefixKV%3A%20Adaptive%20Prefix%20KV%20Cache%20is%20What%20Vision%20Instruction-Following%0A%20%20Models%20Need%20for%20Efficient%20Generation&entry.906535625=Ao%20Wang%20and%20Hui%20Chen%20and%20Jianchao%20Tan%20and%20Kefeng%20Zhang%20and%20Xunliang%20Cai%20and%20Zijia%20Lin%20and%20Jungong%20Han%20and%20Guiguang%20Ding&entry.1292438233=%20%20Recently%2C%20large%20vision-language%20models%20%28LVLMs%29%20have%20rapidly%20gained%20popularity%0Afor%20their%20strong%20generation%20and%20reasoning%20capabilities%20given%20diverse%20multimodal%0Ainputs.%20However%2C%20these%20models%20incur%20significant%20computational%20and%20memory%0Aoverhead%20during%20inference%2C%20which%20greatly%20hinders%20the%20efficient%20deployment%20in%0Apractical%20scenarios.%20The%20extensive%20key-value%20%28KV%29%20cache%2C%20necessitated%20by%20the%0Alengthy%20input%20and%20output%20sequences%2C%20notably%20contributes%20to%20the%20high%20inference%0Acost.%20Based%20on%20this%2C%20recent%20works%20have%20investigated%20ways%20to%20reduce%20the%20KV%20cache%0Asize%20for%20higher%20efficiency.%20Although%20effective%2C%20they%20generally%20overlook%20the%0Adistinct%20importance%20distributions%20of%20KV%20vectors%20across%20layers%20and%20maintain%20the%0Asame%20cache%20size%20for%20each%20layer%20during%20the%20next%20token%20prediction.%20This%20results%0Ain%20the%20significant%20contextual%20information%20loss%20for%20certain%20layers%2C%20leading%20to%0Anotable%20performance%20decline.%20To%20address%20this%2C%20we%20present%20PrefixKV.%20It%20reframes%0Athe%20challenge%20of%20determining%20KV%20cache%20sizes%20for%20all%20layers%20into%20the%20task%20of%0Asearching%20for%20the%20optimal%20global%20prefix%20configuration.%20With%20an%20adaptive%0Alayer-wise%20KV%20retention%20recipe%20based%20on%20binary%20search%2C%20the%20maximum%20contextual%0Ainformation%20can%20thus%20be%20preserved%20in%20each%20layer%2C%20facilitating%20the%20generation.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20the%20state-of-the-art%0Aperformance%20compared%20with%20others.%20It%20exhibits%20superior%20inference%20efficiency%20and%0Ageneration%20quality%20trade-offs%2C%20showing%20promising%20potential%20for%20practical%0Aapplications.%20Code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/THU-MIG/PrefixKV%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03409v1&entry.124074799=Read"},
{"title": "FCL-ViT: Task-Aware Attention Tuning for Continual Learning", "author": "Anestis Kaimakamidis and Ioannis Pitas", "abstract": "  Continual Learning (CL) involves adapting the prior Deep Neural Network (DNN)\nknowledge to new tasks, without forgetting the old ones. However, modern CL\ntechniques focus on provisioning memory capabilities to existing DNN models\nrather than designing new ones that are able to adapt according to the task at\nhand. This paper presents the novel Feedback Continual Learning Vision\nTransformer (FCL-ViT) that uses a feedback mechanism to generate real-time\ndynamic attention features tailored to the current task. The FCL-ViT operates\nin two Phases. In phase 1, the generic image features are produced and\ndetermine where the Transformer should attend on the current image. In phase 2,\ntask-specific image features are generated that leverage dynamic attention. To\nthis end, Tunable self-Attention Blocks (TABs) and Task Specific Blocks (TSBs)\nare introduced that operate in both phases and are responsible for tuning the\nTABs attention, respectively. The FCL-ViT surpasses state-of-the-art\nperformance on Continual Learning compared to benchmark methods, while\nretaining a small number of trainable DNN parameters.\n", "link": "http://arxiv.org/abs/2412.02509v2", "date": "2024-12-04", "relevancy": 2.64, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5392}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5284}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning&body=Title%3A%20FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning%0AAuthor%3A%20Anestis%20Kaimakamidis%20and%20Ioannis%20Pitas%0AAbstract%3A%20%20%20Continual%20Learning%20%28CL%29%20involves%20adapting%20the%20prior%20Deep%20Neural%20Network%20%28DNN%29%0Aknowledge%20to%20new%20tasks%2C%20without%20forgetting%20the%20old%20ones.%20However%2C%20modern%20CL%0Atechniques%20focus%20on%20provisioning%20memory%20capabilities%20to%20existing%20DNN%20models%0Arather%20than%20designing%20new%20ones%20that%20are%20able%20to%20adapt%20according%20to%20the%20task%20at%0Ahand.%20This%20paper%20presents%20the%20novel%20Feedback%20Continual%20Learning%20Vision%0ATransformer%20%28FCL-ViT%29%20that%20uses%20a%20feedback%20mechanism%20to%20generate%20real-time%0Adynamic%20attention%20features%20tailored%20to%20the%20current%20task.%20The%20FCL-ViT%20operates%0Ain%20two%20Phases.%20In%20phase%201%2C%20the%20generic%20image%20features%20are%20produced%20and%0Adetermine%20where%20the%20Transformer%20should%20attend%20on%20the%20current%20image.%20In%20phase%202%2C%0Atask-specific%20image%20features%20are%20generated%20that%20leverage%20dynamic%20attention.%20To%0Athis%20end%2C%20Tunable%20self-Attention%20Blocks%20%28TABs%29%20and%20Task%20Specific%20Blocks%20%28TSBs%29%0Aare%20introduced%20that%20operate%20in%20both%20phases%20and%20are%20responsible%20for%20tuning%20the%0ATABs%20attention%2C%20respectively.%20The%20FCL-ViT%20surpasses%20state-of-the-art%0Aperformance%20on%20Continual%20Learning%20compared%20to%20benchmark%20methods%2C%20while%0Aretaining%20a%20small%20number%20of%20trainable%20DNN%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02509v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFCL-ViT%253A%2520Task-Aware%2520Attention%2520Tuning%2520for%2520Continual%2520Learning%26entry.906535625%3DAnestis%2520Kaimakamidis%2520and%2520Ioannis%2520Pitas%26entry.1292438233%3D%2520%2520Continual%2520Learning%2520%2528CL%2529%2520involves%2520adapting%2520the%2520prior%2520Deep%2520Neural%2520Network%2520%2528DNN%2529%250Aknowledge%2520to%2520new%2520tasks%252C%2520without%2520forgetting%2520the%2520old%2520ones.%2520However%252C%2520modern%2520CL%250Atechniques%2520focus%2520on%2520provisioning%2520memory%2520capabilities%2520to%2520existing%2520DNN%2520models%250Arather%2520than%2520designing%2520new%2520ones%2520that%2520are%2520able%2520to%2520adapt%2520according%2520to%2520the%2520task%2520at%250Ahand.%2520This%2520paper%2520presents%2520the%2520novel%2520Feedback%2520Continual%2520Learning%2520Vision%250ATransformer%2520%2528FCL-ViT%2529%2520that%2520uses%2520a%2520feedback%2520mechanism%2520to%2520generate%2520real-time%250Adynamic%2520attention%2520features%2520tailored%2520to%2520the%2520current%2520task.%2520The%2520FCL-ViT%2520operates%250Ain%2520two%2520Phases.%2520In%2520phase%25201%252C%2520the%2520generic%2520image%2520features%2520are%2520produced%2520and%250Adetermine%2520where%2520the%2520Transformer%2520should%2520attend%2520on%2520the%2520current%2520image.%2520In%2520phase%25202%252C%250Atask-specific%2520image%2520features%2520are%2520generated%2520that%2520leverage%2520dynamic%2520attention.%2520To%250Athis%2520end%252C%2520Tunable%2520self-Attention%2520Blocks%2520%2528TABs%2529%2520and%2520Task%2520Specific%2520Blocks%2520%2528TSBs%2529%250Aare%2520introduced%2520that%2520operate%2520in%2520both%2520phases%2520and%2520are%2520responsible%2520for%2520tuning%2520the%250ATABs%2520attention%252C%2520respectively.%2520The%2520FCL-ViT%2520surpasses%2520state-of-the-art%250Aperformance%2520on%2520Continual%2520Learning%2520compared%2520to%2520benchmark%2520methods%252C%2520while%250Aretaining%2520a%2520small%2520number%2520of%2520trainable%2520DNN%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02509v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning&entry.906535625=Anestis%20Kaimakamidis%20and%20Ioannis%20Pitas&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20involves%20adapting%20the%20prior%20Deep%20Neural%20Network%20%28DNN%29%0Aknowledge%20to%20new%20tasks%2C%20without%20forgetting%20the%20old%20ones.%20However%2C%20modern%20CL%0Atechniques%20focus%20on%20provisioning%20memory%20capabilities%20to%20existing%20DNN%20models%0Arather%20than%20designing%20new%20ones%20that%20are%20able%20to%20adapt%20according%20to%20the%20task%20at%0Ahand.%20This%20paper%20presents%20the%20novel%20Feedback%20Continual%20Learning%20Vision%0ATransformer%20%28FCL-ViT%29%20that%20uses%20a%20feedback%20mechanism%20to%20generate%20real-time%0Adynamic%20attention%20features%20tailored%20to%20the%20current%20task.%20The%20FCL-ViT%20operates%0Ain%20two%20Phases.%20In%20phase%201%2C%20the%20generic%20image%20features%20are%20produced%20and%0Adetermine%20where%20the%20Transformer%20should%20attend%20on%20the%20current%20image.%20In%20phase%202%2C%0Atask-specific%20image%20features%20are%20generated%20that%20leverage%20dynamic%20attention.%20To%0Athis%20end%2C%20Tunable%20self-Attention%20Blocks%20%28TABs%29%20and%20Task%20Specific%20Blocks%20%28TSBs%29%0Aare%20introduced%20that%20operate%20in%20both%20phases%20and%20are%20responsible%20for%20tuning%20the%0ATABs%20attention%2C%20respectively.%20The%20FCL-ViT%20surpasses%20state-of-the-art%0Aperformance%20on%20Continual%20Learning%20compared%20to%20benchmark%20methods%2C%20while%0Aretaining%20a%20small%20number%20of%20trainable%20DNN%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02509v2&entry.124074799=Read"},
{"title": "Breaking the Frame: Visual Place Recognition by Overlap Prediction", "author": "Tong Wei and Philipp Lindenberger and Jiri Matas and Daniel Barath", "abstract": "  Visual place recognition methods struggle with occlusions and partial visual\noverlaps. We propose a novel visual place recognition approach based on overlap\nprediction, called VOP, shifting from traditional reliance on global image\nsimilarities and local features to image overlap prediction. VOP proceeds\nco-visible image sections by obtaining patch-level embeddings using a Vision\nTransformer backbone and establishing patch-to-patch correspondences without\nrequiring expensive feature detection and matching. Our approach uses a voting\nmechanism to assess overlap scores for potential database images. It provides a\nnuanced image retrieval metric in challenging scenarios. Experimental results\nshow that VOP leads to more accurate relative pose estimation and localization\nresults on the retrieved image pairs than state-of-the-art baselines on a\nnumber of large-scale, real-world indoor and outdoor benchmarks. The code is\navailable at https://github.com/weitong8591/vop.git.\n", "link": "http://arxiv.org/abs/2406.16204v3", "date": "2024-12-04", "relevancy": 2.6287, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5223}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Frame%3A%20Visual%20Place%20Recognition%20by%20Overlap%20Prediction&body=Title%3A%20Breaking%20the%20Frame%3A%20Visual%20Place%20Recognition%20by%20Overlap%20Prediction%0AAuthor%3A%20Tong%20Wei%20and%20Philipp%20Lindenberger%20and%20Jiri%20Matas%20and%20Daniel%20Barath%0AAbstract%3A%20%20%20Visual%20place%20recognition%20methods%20struggle%20with%20occlusions%20and%20partial%20visual%0Aoverlaps.%20We%20propose%20a%20novel%20visual%20place%20recognition%20approach%20based%20on%20overlap%0Aprediction%2C%20called%20VOP%2C%20shifting%20from%20traditional%20reliance%20on%20global%20image%0Asimilarities%20and%20local%20features%20to%20image%20overlap%20prediction.%20VOP%20proceeds%0Aco-visible%20image%20sections%20by%20obtaining%20patch-level%20embeddings%20using%20a%20Vision%0ATransformer%20backbone%20and%20establishing%20patch-to-patch%20correspondences%20without%0Arequiring%20expensive%20feature%20detection%20and%20matching.%20Our%20approach%20uses%20a%20voting%0Amechanism%20to%20assess%20overlap%20scores%20for%20potential%20database%20images.%20It%20provides%20a%0Anuanced%20image%20retrieval%20metric%20in%20challenging%20scenarios.%20Experimental%20results%0Ashow%20that%20VOP%20leads%20to%20more%20accurate%20relative%20pose%20estimation%20and%20localization%0Aresults%20on%20the%20retrieved%20image%20pairs%20than%20state-of-the-art%20baselines%20on%20a%0Anumber%20of%20large-scale%2C%20real-world%20indoor%20and%20outdoor%20benchmarks.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/weitong8591/vop.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16204v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Frame%253A%2520Visual%2520Place%2520Recognition%2520by%2520Overlap%2520Prediction%26entry.906535625%3DTong%2520Wei%2520and%2520Philipp%2520Lindenberger%2520and%2520Jiri%2520Matas%2520and%2520Daniel%2520Barath%26entry.1292438233%3D%2520%2520Visual%2520place%2520recognition%2520methods%2520struggle%2520with%2520occlusions%2520and%2520partial%2520visual%250Aoverlaps.%2520We%2520propose%2520a%2520novel%2520visual%2520place%2520recognition%2520approach%2520based%2520on%2520overlap%250Aprediction%252C%2520called%2520VOP%252C%2520shifting%2520from%2520traditional%2520reliance%2520on%2520global%2520image%250Asimilarities%2520and%2520local%2520features%2520to%2520image%2520overlap%2520prediction.%2520VOP%2520proceeds%250Aco-visible%2520image%2520sections%2520by%2520obtaining%2520patch-level%2520embeddings%2520using%2520a%2520Vision%250ATransformer%2520backbone%2520and%2520establishing%2520patch-to-patch%2520correspondences%2520without%250Arequiring%2520expensive%2520feature%2520detection%2520and%2520matching.%2520Our%2520approach%2520uses%2520a%2520voting%250Amechanism%2520to%2520assess%2520overlap%2520scores%2520for%2520potential%2520database%2520images.%2520It%2520provides%2520a%250Anuanced%2520image%2520retrieval%2520metric%2520in%2520challenging%2520scenarios.%2520Experimental%2520results%250Ashow%2520that%2520VOP%2520leads%2520to%2520more%2520accurate%2520relative%2520pose%2520estimation%2520and%2520localization%250Aresults%2520on%2520the%2520retrieved%2520image%2520pairs%2520than%2520state-of-the-art%2520baselines%2520on%2520a%250Anumber%2520of%2520large-scale%252C%2520real-world%2520indoor%2520and%2520outdoor%2520benchmarks.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/weitong8591/vop.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16204v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Frame%3A%20Visual%20Place%20Recognition%20by%20Overlap%20Prediction&entry.906535625=Tong%20Wei%20and%20Philipp%20Lindenberger%20and%20Jiri%20Matas%20and%20Daniel%20Barath&entry.1292438233=%20%20Visual%20place%20recognition%20methods%20struggle%20with%20occlusions%20and%20partial%20visual%0Aoverlaps.%20We%20propose%20a%20novel%20visual%20place%20recognition%20approach%20based%20on%20overlap%0Aprediction%2C%20called%20VOP%2C%20shifting%20from%20traditional%20reliance%20on%20global%20image%0Asimilarities%20and%20local%20features%20to%20image%20overlap%20prediction.%20VOP%20proceeds%0Aco-visible%20image%20sections%20by%20obtaining%20patch-level%20embeddings%20using%20a%20Vision%0ATransformer%20backbone%20and%20establishing%20patch-to-patch%20correspondences%20without%0Arequiring%20expensive%20feature%20detection%20and%20matching.%20Our%20approach%20uses%20a%20voting%0Amechanism%20to%20assess%20overlap%20scores%20for%20potential%20database%20images.%20It%20provides%20a%0Anuanced%20image%20retrieval%20metric%20in%20challenging%20scenarios.%20Experimental%20results%0Ashow%20that%20VOP%20leads%20to%20more%20accurate%20relative%20pose%20estimation%20and%20localization%0Aresults%20on%20the%20retrieved%20image%20pairs%20than%20state-of-the-art%20baselines%20on%20a%0Anumber%20of%20large-scale%2C%20real-world%20indoor%20and%20outdoor%20benchmarks.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/weitong8591/vop.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16204v3&entry.124074799=Read"},
{"title": "Task-driven Image Fusion with Learnable Fusion Loss", "author": "Haowen Bai and Jiangshe Zhang and Zixiang Zhao and Yichen Wu and Lilun Deng and Yukun Cui and Tao Feng and Shuang Xu", "abstract": "  Multi-modal image fusion aggregates information from multiple sensor sources,\nachieving superior visual quality and perceptual characteristics compared to\nany single source, often enhancing downstream tasks. However, current fusion\nmethods for downstream tasks still use predefined fusion objectives that\npotentially mismatch the downstream tasks, limiting adaptive guidance and\nreducing model flexibility. To address this, we propose Task-driven Image\nFusion (TDFusion), a fusion framework incorporating a learnable fusion loss\nguided by task loss. Specifically, our fusion loss includes learnable\nparameters modeled by a neural network called the loss generation module. This\nmodule is supervised by the loss of downstream tasks in a meta-learning manner.\nThe learning objective is to minimize the task loss of the fused images, once\nthe fusion module has been optimized by the fusion loss. Iterative updates\nbetween the fusion module and the loss module ensure that the fusion network\nevolves toward minimizing task loss, guiding the fusion process toward the task\nobjectives. TDFusion's training relies solely on the loss of downstream tasks,\nmaking it adaptable to any specific task. It can be applied to any architecture\nof fusion and task networks. Experiments demonstrate TDFusion's performance in\nboth fusion and task-related applications, including four public fusion\ndatasets, semantic segmentation, and object detection. The code will be\nreleased.\n", "link": "http://arxiv.org/abs/2412.03240v1", "date": "2024-12-04", "relevancy": 2.6243, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5289}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5238}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-driven%20Image%20Fusion%20with%20Learnable%20Fusion%20Loss&body=Title%3A%20Task-driven%20Image%20Fusion%20with%20Learnable%20Fusion%20Loss%0AAuthor%3A%20Haowen%20Bai%20and%20Jiangshe%20Zhang%20and%20Zixiang%20Zhao%20and%20Yichen%20Wu%20and%20Lilun%20Deng%20and%20Yukun%20Cui%20and%20Tao%20Feng%20and%20Shuang%20Xu%0AAbstract%3A%20%20%20Multi-modal%20image%20fusion%20aggregates%20information%20from%20multiple%20sensor%20sources%2C%0Aachieving%20superior%20visual%20quality%20and%20perceptual%20characteristics%20compared%20to%0Aany%20single%20source%2C%20often%20enhancing%20downstream%20tasks.%20However%2C%20current%20fusion%0Amethods%20for%20downstream%20tasks%20still%20use%20predefined%20fusion%20objectives%20that%0Apotentially%20mismatch%20the%20downstream%20tasks%2C%20limiting%20adaptive%20guidance%20and%0Areducing%20model%20flexibility.%20To%20address%20this%2C%20we%20propose%20Task-driven%20Image%0AFusion%20%28TDFusion%29%2C%20a%20fusion%20framework%20incorporating%20a%20learnable%20fusion%20loss%0Aguided%20by%20task%20loss.%20Specifically%2C%20our%20fusion%20loss%20includes%20learnable%0Aparameters%20modeled%20by%20a%20neural%20network%20called%20the%20loss%20generation%20module.%20This%0Amodule%20is%20supervised%20by%20the%20loss%20of%20downstream%20tasks%20in%20a%20meta-learning%20manner.%0AThe%20learning%20objective%20is%20to%20minimize%20the%20task%20loss%20of%20the%20fused%20images%2C%20once%0Athe%20fusion%20module%20has%20been%20optimized%20by%20the%20fusion%20loss.%20Iterative%20updates%0Abetween%20the%20fusion%20module%20and%20the%20loss%20module%20ensure%20that%20the%20fusion%20network%0Aevolves%20toward%20minimizing%20task%20loss%2C%20guiding%20the%20fusion%20process%20toward%20the%20task%0Aobjectives.%20TDFusion%27s%20training%20relies%20solely%20on%20the%20loss%20of%20downstream%20tasks%2C%0Amaking%20it%20adaptable%20to%20any%20specific%20task.%20It%20can%20be%20applied%20to%20any%20architecture%0Aof%20fusion%20and%20task%20networks.%20Experiments%20demonstrate%20TDFusion%27s%20performance%20in%0Aboth%20fusion%20and%20task-related%20applications%2C%20including%20four%20public%20fusion%0Adatasets%2C%20semantic%20segmentation%2C%20and%20object%20detection.%20The%20code%20will%20be%0Areleased.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-driven%2520Image%2520Fusion%2520with%2520Learnable%2520Fusion%2520Loss%26entry.906535625%3DHaowen%2520Bai%2520and%2520Jiangshe%2520Zhang%2520and%2520Zixiang%2520Zhao%2520and%2520Yichen%2520Wu%2520and%2520Lilun%2520Deng%2520and%2520Yukun%2520Cui%2520and%2520Tao%2520Feng%2520and%2520Shuang%2520Xu%26entry.1292438233%3D%2520%2520Multi-modal%2520image%2520fusion%2520aggregates%2520information%2520from%2520multiple%2520sensor%2520sources%252C%250Aachieving%2520superior%2520visual%2520quality%2520and%2520perceptual%2520characteristics%2520compared%2520to%250Aany%2520single%2520source%252C%2520often%2520enhancing%2520downstream%2520tasks.%2520However%252C%2520current%2520fusion%250Amethods%2520for%2520downstream%2520tasks%2520still%2520use%2520predefined%2520fusion%2520objectives%2520that%250Apotentially%2520mismatch%2520the%2520downstream%2520tasks%252C%2520limiting%2520adaptive%2520guidance%2520and%250Areducing%2520model%2520flexibility.%2520To%2520address%2520this%252C%2520we%2520propose%2520Task-driven%2520Image%250AFusion%2520%2528TDFusion%2529%252C%2520a%2520fusion%2520framework%2520incorporating%2520a%2520learnable%2520fusion%2520loss%250Aguided%2520by%2520task%2520loss.%2520Specifically%252C%2520our%2520fusion%2520loss%2520includes%2520learnable%250Aparameters%2520modeled%2520by%2520a%2520neural%2520network%2520called%2520the%2520loss%2520generation%2520module.%2520This%250Amodule%2520is%2520supervised%2520by%2520the%2520loss%2520of%2520downstream%2520tasks%2520in%2520a%2520meta-learning%2520manner.%250AThe%2520learning%2520objective%2520is%2520to%2520minimize%2520the%2520task%2520loss%2520of%2520the%2520fused%2520images%252C%2520once%250Athe%2520fusion%2520module%2520has%2520been%2520optimized%2520by%2520the%2520fusion%2520loss.%2520Iterative%2520updates%250Abetween%2520the%2520fusion%2520module%2520and%2520the%2520loss%2520module%2520ensure%2520that%2520the%2520fusion%2520network%250Aevolves%2520toward%2520minimizing%2520task%2520loss%252C%2520guiding%2520the%2520fusion%2520process%2520toward%2520the%2520task%250Aobjectives.%2520TDFusion%2527s%2520training%2520relies%2520solely%2520on%2520the%2520loss%2520of%2520downstream%2520tasks%252C%250Amaking%2520it%2520adaptable%2520to%2520any%2520specific%2520task.%2520It%2520can%2520be%2520applied%2520to%2520any%2520architecture%250Aof%2520fusion%2520and%2520task%2520networks.%2520Experiments%2520demonstrate%2520TDFusion%2527s%2520performance%2520in%250Aboth%2520fusion%2520and%2520task-related%2520applications%252C%2520including%2520four%2520public%2520fusion%250Adatasets%252C%2520semantic%2520segmentation%252C%2520and%2520object%2520detection.%2520The%2520code%2520will%2520be%250Areleased.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-driven%20Image%20Fusion%20with%20Learnable%20Fusion%20Loss&entry.906535625=Haowen%20Bai%20and%20Jiangshe%20Zhang%20and%20Zixiang%20Zhao%20and%20Yichen%20Wu%20and%20Lilun%20Deng%20and%20Yukun%20Cui%20and%20Tao%20Feng%20and%20Shuang%20Xu&entry.1292438233=%20%20Multi-modal%20image%20fusion%20aggregates%20information%20from%20multiple%20sensor%20sources%2C%0Aachieving%20superior%20visual%20quality%20and%20perceptual%20characteristics%20compared%20to%0Aany%20single%20source%2C%20often%20enhancing%20downstream%20tasks.%20However%2C%20current%20fusion%0Amethods%20for%20downstream%20tasks%20still%20use%20predefined%20fusion%20objectives%20that%0Apotentially%20mismatch%20the%20downstream%20tasks%2C%20limiting%20adaptive%20guidance%20and%0Areducing%20model%20flexibility.%20To%20address%20this%2C%20we%20propose%20Task-driven%20Image%0AFusion%20%28TDFusion%29%2C%20a%20fusion%20framework%20incorporating%20a%20learnable%20fusion%20loss%0Aguided%20by%20task%20loss.%20Specifically%2C%20our%20fusion%20loss%20includes%20learnable%0Aparameters%20modeled%20by%20a%20neural%20network%20called%20the%20loss%20generation%20module.%20This%0Amodule%20is%20supervised%20by%20the%20loss%20of%20downstream%20tasks%20in%20a%20meta-learning%20manner.%0AThe%20learning%20objective%20is%20to%20minimize%20the%20task%20loss%20of%20the%20fused%20images%2C%20once%0Athe%20fusion%20module%20has%20been%20optimized%20by%20the%20fusion%20loss.%20Iterative%20updates%0Abetween%20the%20fusion%20module%20and%20the%20loss%20module%20ensure%20that%20the%20fusion%20network%0Aevolves%20toward%20minimizing%20task%20loss%2C%20guiding%20the%20fusion%20process%20toward%20the%20task%0Aobjectives.%20TDFusion%27s%20training%20relies%20solely%20on%20the%20loss%20of%20downstream%20tasks%2C%0Amaking%20it%20adaptable%20to%20any%20specific%20task.%20It%20can%20be%20applied%20to%20any%20architecture%0Aof%20fusion%20and%20task%20networks.%20Experiments%20demonstrate%20TDFusion%27s%20performance%20in%0Aboth%20fusion%20and%20task-related%20applications%2C%20including%20four%20public%20fusion%0Adatasets%2C%20semantic%20segmentation%2C%20and%20object%20detection.%20The%20code%20will%20be%0Areleased.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03240v1&entry.124074799=Read"},
{"title": "Style3D: Attention-guided Multi-view Style Transfer for 3D Object\n  Generation", "author": "Bingjie Song and Xin Huang and Ruting Xie and Xue Wang and Qing Wang", "abstract": "  We present Style3D, a novel approach for generating stylized 3D objects from\na content image and a style image. Unlike most previous methods that require\ncase- or style-specific training, Style3D supports instant 3D object\nstylization. Our key insight is that 3D object stylization can be decomposed\ninto two interconnected processes: multi-view dual-feature alignment and\nsparse-view spatial reconstruction. We introduce MultiFusion Attention, an\nattention-guided technique to achieve multi-view stylization from the\ncontent-style pair. Specifically, the query features from the content image\npreserve geometric consistency across multiple views, while the key and value\nfeatures from the style image are used to guide the stylistic transfer. This\ndual-feature alignment ensures that spatial coherence and stylistic fidelity\nare maintained across multi-view images. Finally, a large 3D reconstruction\nmodel is introduced to generate coherent stylized 3D objects. By establishing\nan interplay between structural and stylistic features across multiple views,\nour approach enables a holistic 3D stylization process. Extensive experiments\ndemonstrate that Style3D offers a more flexible and scalable solution for\ngenerating style-consistent 3D assets, surpassing existing methods in both\ncomputational efficiency and visual quality.\n", "link": "http://arxiv.org/abs/2412.03571v1", "date": "2024-12-04", "relevancy": 2.6073, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6626}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6626}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Style3D%3A%20Attention-guided%20Multi-view%20Style%20Transfer%20for%203D%20Object%0A%20%20Generation&body=Title%3A%20Style3D%3A%20Attention-guided%20Multi-view%20Style%20Transfer%20for%203D%20Object%0A%20%20Generation%0AAuthor%3A%20Bingjie%20Song%20and%20Xin%20Huang%20and%20Ruting%20Xie%20and%20Xue%20Wang%20and%20Qing%20Wang%0AAbstract%3A%20%20%20We%20present%20Style3D%2C%20a%20novel%20approach%20for%20generating%20stylized%203D%20objects%20from%0Aa%20content%20image%20and%20a%20style%20image.%20Unlike%20most%20previous%20methods%20that%20require%0Acase-%20or%20style-specific%20training%2C%20Style3D%20supports%20instant%203D%20object%0Astylization.%20Our%20key%20insight%20is%20that%203D%20object%20stylization%20can%20be%20decomposed%0Ainto%20two%20interconnected%20processes%3A%20multi-view%20dual-feature%20alignment%20and%0Asparse-view%20spatial%20reconstruction.%20We%20introduce%20MultiFusion%20Attention%2C%20an%0Aattention-guided%20technique%20to%20achieve%20multi-view%20stylization%20from%20the%0Acontent-style%20pair.%20Specifically%2C%20the%20query%20features%20from%20the%20content%20image%0Apreserve%20geometric%20consistency%20across%20multiple%20views%2C%20while%20the%20key%20and%20value%0Afeatures%20from%20the%20style%20image%20are%20used%20to%20guide%20the%20stylistic%20transfer.%20This%0Adual-feature%20alignment%20ensures%20that%20spatial%20coherence%20and%20stylistic%20fidelity%0Aare%20maintained%20across%20multi-view%20images.%20Finally%2C%20a%20large%203D%20reconstruction%0Amodel%20is%20introduced%20to%20generate%20coherent%20stylized%203D%20objects.%20By%20establishing%0Aan%20interplay%20between%20structural%20and%20stylistic%20features%20across%20multiple%20views%2C%0Aour%20approach%20enables%20a%20holistic%203D%20stylization%20process.%20Extensive%20experiments%0Ademonstrate%20that%20Style3D%20offers%20a%20more%20flexible%20and%20scalable%20solution%20for%0Agenerating%20style-consistent%203D%20assets%2C%20surpassing%20existing%20methods%20in%20both%0Acomputational%20efficiency%20and%20visual%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyle3D%253A%2520Attention-guided%2520Multi-view%2520Style%2520Transfer%2520for%25203D%2520Object%250A%2520%2520Generation%26entry.906535625%3DBingjie%2520Song%2520and%2520Xin%2520Huang%2520and%2520Ruting%2520Xie%2520and%2520Xue%2520Wang%2520and%2520Qing%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520Style3D%252C%2520a%2520novel%2520approach%2520for%2520generating%2520stylized%25203D%2520objects%2520from%250Aa%2520content%2520image%2520and%2520a%2520style%2520image.%2520Unlike%2520most%2520previous%2520methods%2520that%2520require%250Acase-%2520or%2520style-specific%2520training%252C%2520Style3D%2520supports%2520instant%25203D%2520object%250Astylization.%2520Our%2520key%2520insight%2520is%2520that%25203D%2520object%2520stylization%2520can%2520be%2520decomposed%250Ainto%2520two%2520interconnected%2520processes%253A%2520multi-view%2520dual-feature%2520alignment%2520and%250Asparse-view%2520spatial%2520reconstruction.%2520We%2520introduce%2520MultiFusion%2520Attention%252C%2520an%250Aattention-guided%2520technique%2520to%2520achieve%2520multi-view%2520stylization%2520from%2520the%250Acontent-style%2520pair.%2520Specifically%252C%2520the%2520query%2520features%2520from%2520the%2520content%2520image%250Apreserve%2520geometric%2520consistency%2520across%2520multiple%2520views%252C%2520while%2520the%2520key%2520and%2520value%250Afeatures%2520from%2520the%2520style%2520image%2520are%2520used%2520to%2520guide%2520the%2520stylistic%2520transfer.%2520This%250Adual-feature%2520alignment%2520ensures%2520that%2520spatial%2520coherence%2520and%2520stylistic%2520fidelity%250Aare%2520maintained%2520across%2520multi-view%2520images.%2520Finally%252C%2520a%2520large%25203D%2520reconstruction%250Amodel%2520is%2520introduced%2520to%2520generate%2520coherent%2520stylized%25203D%2520objects.%2520By%2520establishing%250Aan%2520interplay%2520between%2520structural%2520and%2520stylistic%2520features%2520across%2520multiple%2520views%252C%250Aour%2520approach%2520enables%2520a%2520holistic%25203D%2520stylization%2520process.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520Style3D%2520offers%2520a%2520more%2520flexible%2520and%2520scalable%2520solution%2520for%250Agenerating%2520style-consistent%25203D%2520assets%252C%2520surpassing%2520existing%2520methods%2520in%2520both%250Acomputational%2520efficiency%2520and%2520visual%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Style3D%3A%20Attention-guided%20Multi-view%20Style%20Transfer%20for%203D%20Object%0A%20%20Generation&entry.906535625=Bingjie%20Song%20and%20Xin%20Huang%20and%20Ruting%20Xie%20and%20Xue%20Wang%20and%20Qing%20Wang&entry.1292438233=%20%20We%20present%20Style3D%2C%20a%20novel%20approach%20for%20generating%20stylized%203D%20objects%20from%0Aa%20content%20image%20and%20a%20style%20image.%20Unlike%20most%20previous%20methods%20that%20require%0Acase-%20or%20style-specific%20training%2C%20Style3D%20supports%20instant%203D%20object%0Astylization.%20Our%20key%20insight%20is%20that%203D%20object%20stylization%20can%20be%20decomposed%0Ainto%20two%20interconnected%20processes%3A%20multi-view%20dual-feature%20alignment%20and%0Asparse-view%20spatial%20reconstruction.%20We%20introduce%20MultiFusion%20Attention%2C%20an%0Aattention-guided%20technique%20to%20achieve%20multi-view%20stylization%20from%20the%0Acontent-style%20pair.%20Specifically%2C%20the%20query%20features%20from%20the%20content%20image%0Apreserve%20geometric%20consistency%20across%20multiple%20views%2C%20while%20the%20key%20and%20value%0Afeatures%20from%20the%20style%20image%20are%20used%20to%20guide%20the%20stylistic%20transfer.%20This%0Adual-feature%20alignment%20ensures%20that%20spatial%20coherence%20and%20stylistic%20fidelity%0Aare%20maintained%20across%20multi-view%20images.%20Finally%2C%20a%20large%203D%20reconstruction%0Amodel%20is%20introduced%20to%20generate%20coherent%20stylized%203D%20objects.%20By%20establishing%0Aan%20interplay%20between%20structural%20and%20stylistic%20features%20across%20multiple%20views%2C%0Aour%20approach%20enables%20a%20holistic%203D%20stylization%20process.%20Extensive%20experiments%0Ademonstrate%20that%20Style3D%20offers%20a%20more%20flexible%20and%20scalable%20solution%20for%0Agenerating%20style-consistent%203D%20assets%2C%20surpassing%20existing%20methods%20in%20both%0Acomputational%20efficiency%20and%20visual%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03571v1&entry.124074799=Read"},
{"title": "One Step Learning, One Step Review", "author": "Xiaolong Huang and Qiankun Li and Xueran Li and Xuesong Gao", "abstract": "  Visual fine-tuning has garnered significant attention with the rise of\npre-trained vision models. The current prevailing method, full fine-tuning,\nsuffers from the issue of knowledge forgetting as it focuses solely on fitting\nthe downstream training set. In this paper, we propose a novel weight\nrollback-based fine-tuning method called OLOR (One step Learning, One step\nReview). OLOR combines fine-tuning with optimizers, incorporating a weight\nrollback term into the weight update term at each step. This ensures\nconsistency in the weight range of upstream and downstream models, effectively\nmitigating knowledge forgetting and enhancing fine-tuning performance. In\naddition, a layer-wise penalty is presented to employ penalty decay and the\ndiversified decay rate to adjust the weight rollback levels of layers for\nadapting varying downstream tasks. Through extensive experiments on various\ntasks such as image classification, object detection, semantic segmentation,\nand instance segmentation, we demonstrate the general applicability and\nstate-of-the-art performance of our proposed OLOR. Code is available at\nhttps://github.com/rainbow-xiao/OLOR-AAAI-2024.\n", "link": "http://arxiv.org/abs/2401.10962v2", "date": "2024-12-04", "relevancy": 2.5903, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5262}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5188}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Step%20Learning%2C%20One%20Step%20Review&body=Title%3A%20One%20Step%20Learning%2C%20One%20Step%20Review%0AAuthor%3A%20Xiaolong%20Huang%20and%20Qiankun%20Li%20and%20Xueran%20Li%20and%20Xuesong%20Gao%0AAbstract%3A%20%20%20Visual%20fine-tuning%20has%20garnered%20significant%20attention%20with%20the%20rise%20of%0Apre-trained%20vision%20models.%20The%20current%20prevailing%20method%2C%20full%20fine-tuning%2C%0Asuffers%20from%20the%20issue%20of%20knowledge%20forgetting%20as%20it%20focuses%20solely%20on%20fitting%0Athe%20downstream%20training%20set.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20weight%0Arollback-based%20fine-tuning%20method%20called%20OLOR%20%28One%20step%20Learning%2C%20One%20step%0AReview%29.%20OLOR%20combines%20fine-tuning%20with%20optimizers%2C%20incorporating%20a%20weight%0Arollback%20term%20into%20the%20weight%20update%20term%20at%20each%20step.%20This%20ensures%0Aconsistency%20in%20the%20weight%20range%20of%20upstream%20and%20downstream%20models%2C%20effectively%0Amitigating%20knowledge%20forgetting%20and%20enhancing%20fine-tuning%20performance.%20In%0Aaddition%2C%20a%20layer-wise%20penalty%20is%20presented%20to%20employ%20penalty%20decay%20and%20the%0Adiversified%20decay%20rate%20to%20adjust%20the%20weight%20rollback%20levels%20of%20layers%20for%0Aadapting%20varying%20downstream%20tasks.%20Through%20extensive%20experiments%20on%20various%0Atasks%20such%20as%20image%20classification%2C%20object%20detection%2C%20semantic%20segmentation%2C%0Aand%20instance%20segmentation%2C%20we%20demonstrate%20the%20general%20applicability%20and%0Astate-of-the-art%20performance%20of%20our%20proposed%20OLOR.%20Code%20is%20available%20at%0Ahttps%3A//github.com/rainbow-xiao/OLOR-AAAI-2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10962v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Step%2520Learning%252C%2520One%2520Step%2520Review%26entry.906535625%3DXiaolong%2520Huang%2520and%2520Qiankun%2520Li%2520and%2520Xueran%2520Li%2520and%2520Xuesong%2520Gao%26entry.1292438233%3D%2520%2520Visual%2520fine-tuning%2520has%2520garnered%2520significant%2520attention%2520with%2520the%2520rise%2520of%250Apre-trained%2520vision%2520models.%2520The%2520current%2520prevailing%2520method%252C%2520full%2520fine-tuning%252C%250Asuffers%2520from%2520the%2520issue%2520of%2520knowledge%2520forgetting%2520as%2520it%2520focuses%2520solely%2520on%2520fitting%250Athe%2520downstream%2520training%2520set.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520weight%250Arollback-based%2520fine-tuning%2520method%2520called%2520OLOR%2520%2528One%2520step%2520Learning%252C%2520One%2520step%250AReview%2529.%2520OLOR%2520combines%2520fine-tuning%2520with%2520optimizers%252C%2520incorporating%2520a%2520weight%250Arollback%2520term%2520into%2520the%2520weight%2520update%2520term%2520at%2520each%2520step.%2520This%2520ensures%250Aconsistency%2520in%2520the%2520weight%2520range%2520of%2520upstream%2520and%2520downstream%2520models%252C%2520effectively%250Amitigating%2520knowledge%2520forgetting%2520and%2520enhancing%2520fine-tuning%2520performance.%2520In%250Aaddition%252C%2520a%2520layer-wise%2520penalty%2520is%2520presented%2520to%2520employ%2520penalty%2520decay%2520and%2520the%250Adiversified%2520decay%2520rate%2520to%2520adjust%2520the%2520weight%2520rollback%2520levels%2520of%2520layers%2520for%250Aadapting%2520varying%2520downstream%2520tasks.%2520Through%2520extensive%2520experiments%2520on%2520various%250Atasks%2520such%2520as%2520image%2520classification%252C%2520object%2520detection%252C%2520semantic%2520segmentation%252C%250Aand%2520instance%2520segmentation%252C%2520we%2520demonstrate%2520the%2520general%2520applicability%2520and%250Astate-of-the-art%2520performance%2520of%2520our%2520proposed%2520OLOR.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/rainbow-xiao/OLOR-AAAI-2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10962v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Step%20Learning%2C%20One%20Step%20Review&entry.906535625=Xiaolong%20Huang%20and%20Qiankun%20Li%20and%20Xueran%20Li%20and%20Xuesong%20Gao&entry.1292438233=%20%20Visual%20fine-tuning%20has%20garnered%20significant%20attention%20with%20the%20rise%20of%0Apre-trained%20vision%20models.%20The%20current%20prevailing%20method%2C%20full%20fine-tuning%2C%0Asuffers%20from%20the%20issue%20of%20knowledge%20forgetting%20as%20it%20focuses%20solely%20on%20fitting%0Athe%20downstream%20training%20set.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20weight%0Arollback-based%20fine-tuning%20method%20called%20OLOR%20%28One%20step%20Learning%2C%20One%20step%0AReview%29.%20OLOR%20combines%20fine-tuning%20with%20optimizers%2C%20incorporating%20a%20weight%0Arollback%20term%20into%20the%20weight%20update%20term%20at%20each%20step.%20This%20ensures%0Aconsistency%20in%20the%20weight%20range%20of%20upstream%20and%20downstream%20models%2C%20effectively%0Amitigating%20knowledge%20forgetting%20and%20enhancing%20fine-tuning%20performance.%20In%0Aaddition%2C%20a%20layer-wise%20penalty%20is%20presented%20to%20employ%20penalty%20decay%20and%20the%0Adiversified%20decay%20rate%20to%20adjust%20the%20weight%20rollback%20levels%20of%20layers%20for%0Aadapting%20varying%20downstream%20tasks.%20Through%20extensive%20experiments%20on%20various%0Atasks%20such%20as%20image%20classification%2C%20object%20detection%2C%20semantic%20segmentation%2C%0Aand%20instance%20segmentation%2C%20we%20demonstrate%20the%20general%20applicability%20and%0Astate-of-the-art%20performance%20of%20our%20proposed%20OLOR.%20Code%20is%20available%20at%0Ahttps%3A//github.com/rainbow-xiao/OLOR-AAAI-2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10962v2&entry.124074799=Read"},
{"title": "LuxEmbedder: A Cross-Lingual Approach to Enhanced Luxembourgish Sentence\n  Embeddings", "author": "Fred Philippy and Siwen Guo and Jacques Klein and Tegawend\u00e9 F. Bissyand\u00e9", "abstract": "  Sentence embedding models play a key role in various Natural Language\nProcessing tasks, such as in Topic Modeling, Document Clustering and\nRecommendation Systems. However, these models rely heavily on parallel data,\nwhich can be scarce for many low-resource languages, including Luxembourgish.\nThis scarcity results in suboptimal performance of monolingual and\ncross-lingual sentence embedding models for these languages. To address this\nissue, we compile a relatively small but high-quality human-generated\ncross-lingual parallel dataset to train \\tool, an enhanced sentence embedding\nmodel for Luxembourgish with strong cross-lingual capabilities. Additionally,\nwe present evidence suggesting that including low-resource languages in\nparallel training datasets can be more advantageous for other low-resource\nlanguages than relying solely on high-resource language pairs. Furthermore,\nrecognizing the lack of sentence embedding benchmarks for low-resource\nlanguages, we create a paraphrase detection benchmark specifically for\nLuxembourgish, aiming to partially fill this gap and promote further research.\n", "link": "http://arxiv.org/abs/2412.03331v1", "date": "2024-12-04", "relevancy": 2.5888, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LuxEmbedder%3A%20A%20Cross-Lingual%20Approach%20to%20Enhanced%20Luxembourgish%20Sentence%0A%20%20Embeddings&body=Title%3A%20LuxEmbedder%3A%20A%20Cross-Lingual%20Approach%20to%20Enhanced%20Luxembourgish%20Sentence%0A%20%20Embeddings%0AAuthor%3A%20Fred%20Philippy%20and%20Siwen%20Guo%20and%20Jacques%20Klein%20and%20Tegawend%C3%A9%20F.%20Bissyand%C3%A9%0AAbstract%3A%20%20%20Sentence%20embedding%20models%20play%20a%20key%20role%20in%20various%20Natural%20Language%0AProcessing%20tasks%2C%20such%20as%20in%20Topic%20Modeling%2C%20Document%20Clustering%20and%0ARecommendation%20Systems.%20However%2C%20these%20models%20rely%20heavily%20on%20parallel%20data%2C%0Awhich%20can%20be%20scarce%20for%20many%20low-resource%20languages%2C%20including%20Luxembourgish.%0AThis%20scarcity%20results%20in%20suboptimal%20performance%20of%20monolingual%20and%0Across-lingual%20sentence%20embedding%20models%20for%20these%20languages.%20To%20address%20this%0Aissue%2C%20we%20compile%20a%20relatively%20small%20but%20high-quality%20human-generated%0Across-lingual%20parallel%20dataset%20to%20train%20%5Ctool%2C%20an%20enhanced%20sentence%20embedding%0Amodel%20for%20Luxembourgish%20with%20strong%20cross-lingual%20capabilities.%20Additionally%2C%0Awe%20present%20evidence%20suggesting%20that%20including%20low-resource%20languages%20in%0Aparallel%20training%20datasets%20can%20be%20more%20advantageous%20for%20other%20low-resource%0Alanguages%20than%20relying%20solely%20on%20high-resource%20language%20pairs.%20Furthermore%2C%0Arecognizing%20the%20lack%20of%20sentence%20embedding%20benchmarks%20for%20low-resource%0Alanguages%2C%20we%20create%20a%20paraphrase%20detection%20benchmark%20specifically%20for%0ALuxembourgish%2C%20aiming%20to%20partially%20fill%20this%20gap%20and%20promote%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLuxEmbedder%253A%2520A%2520Cross-Lingual%2520Approach%2520to%2520Enhanced%2520Luxembourgish%2520Sentence%250A%2520%2520Embeddings%26entry.906535625%3DFred%2520Philippy%2520and%2520Siwen%2520Guo%2520and%2520Jacques%2520Klein%2520and%2520Tegawend%25C3%25A9%2520F.%2520Bissyand%25C3%25A9%26entry.1292438233%3D%2520%2520Sentence%2520embedding%2520models%2520play%2520a%2520key%2520role%2520in%2520various%2520Natural%2520Language%250AProcessing%2520tasks%252C%2520such%2520as%2520in%2520Topic%2520Modeling%252C%2520Document%2520Clustering%2520and%250ARecommendation%2520Systems.%2520However%252C%2520these%2520models%2520rely%2520heavily%2520on%2520parallel%2520data%252C%250Awhich%2520can%2520be%2520scarce%2520for%2520many%2520low-resource%2520languages%252C%2520including%2520Luxembourgish.%250AThis%2520scarcity%2520results%2520in%2520suboptimal%2520performance%2520of%2520monolingual%2520and%250Across-lingual%2520sentence%2520embedding%2520models%2520for%2520these%2520languages.%2520To%2520address%2520this%250Aissue%252C%2520we%2520compile%2520a%2520relatively%2520small%2520but%2520high-quality%2520human-generated%250Across-lingual%2520parallel%2520dataset%2520to%2520train%2520%255Ctool%252C%2520an%2520enhanced%2520sentence%2520embedding%250Amodel%2520for%2520Luxembourgish%2520with%2520strong%2520cross-lingual%2520capabilities.%2520Additionally%252C%250Awe%2520present%2520evidence%2520suggesting%2520that%2520including%2520low-resource%2520languages%2520in%250Aparallel%2520training%2520datasets%2520can%2520be%2520more%2520advantageous%2520for%2520other%2520low-resource%250Alanguages%2520than%2520relying%2520solely%2520on%2520high-resource%2520language%2520pairs.%2520Furthermore%252C%250Arecognizing%2520the%2520lack%2520of%2520sentence%2520embedding%2520benchmarks%2520for%2520low-resource%250Alanguages%252C%2520we%2520create%2520a%2520paraphrase%2520detection%2520benchmark%2520specifically%2520for%250ALuxembourgish%252C%2520aiming%2520to%2520partially%2520fill%2520this%2520gap%2520and%2520promote%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LuxEmbedder%3A%20A%20Cross-Lingual%20Approach%20to%20Enhanced%20Luxembourgish%20Sentence%0A%20%20Embeddings&entry.906535625=Fred%20Philippy%20and%20Siwen%20Guo%20and%20Jacques%20Klein%20and%20Tegawend%C3%A9%20F.%20Bissyand%C3%A9&entry.1292438233=%20%20Sentence%20embedding%20models%20play%20a%20key%20role%20in%20various%20Natural%20Language%0AProcessing%20tasks%2C%20such%20as%20in%20Topic%20Modeling%2C%20Document%20Clustering%20and%0ARecommendation%20Systems.%20However%2C%20these%20models%20rely%20heavily%20on%20parallel%20data%2C%0Awhich%20can%20be%20scarce%20for%20many%20low-resource%20languages%2C%20including%20Luxembourgish.%0AThis%20scarcity%20results%20in%20suboptimal%20performance%20of%20monolingual%20and%0Across-lingual%20sentence%20embedding%20models%20for%20these%20languages.%20To%20address%20this%0Aissue%2C%20we%20compile%20a%20relatively%20small%20but%20high-quality%20human-generated%0Across-lingual%20parallel%20dataset%20to%20train%20%5Ctool%2C%20an%20enhanced%20sentence%20embedding%0Amodel%20for%20Luxembourgish%20with%20strong%20cross-lingual%20capabilities.%20Additionally%2C%0Awe%20present%20evidence%20suggesting%20that%20including%20low-resource%20languages%20in%0Aparallel%20training%20datasets%20can%20be%20more%20advantageous%20for%20other%20low-resource%0Alanguages%20than%20relying%20solely%20on%20high-resource%20language%20pairs.%20Furthermore%2C%0Arecognizing%20the%20lack%20of%20sentence%20embedding%20benchmarks%20for%20low-resource%0Alanguages%2C%20we%20create%20a%20paraphrase%20detection%20benchmark%20specifically%20for%0ALuxembourgish%2C%20aiming%20to%20partially%20fill%20this%20gap%20and%20promote%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03331v1&entry.124074799=Read"},
{"title": "Geometry-guided Cross-view Diffusion for One-to-many Cross-view Image\n  Synthesis", "author": "Tao Jun Lin and Wenqing Wang and Yujiao Shi and Akhil Perincherry and Ankit Vora and Hongdong Li", "abstract": "  This paper presents a novel approach for cross-view synthesis aimed at\ngenerating plausible ground-level images from corresponding satellite imagery\nor vice versa. We refer to these tasks as satellite-to-ground (Sat2Grd) and\nground-to-satellite (Grd2Sat) synthesis, respectively. Unlike previous works\nthat typically focus on one-to-one generation, producing a single output image\nfrom a single input image, our approach acknowledges the inherent one-to-many\nnature of the problem. This recognition stems from the challenges posed by\ndifferences in illumination, weather conditions, and occlusions between the two\nviews. To effectively model this uncertainty, we leverage recent advancements\nin diffusion models. Specifically, we exploit random Gaussian noise to\nrepresent the diverse possibilities learnt from the target view data. We\nintroduce a Geometry-guided Cross-view Condition (GCC) strategy to establish\nexplicit geometric correspondences between satellite and street-view features.\nThis enables us to resolve the geometry ambiguity introduced by camera pose\nbetween image pairs, boosting the performance of cross-view image synthesis.\nThrough extensive quantitative and qualitative analyses on three benchmark\ncross-view datasets, we demonstrate the superiority of our proposed\ngeometry-guided cross-view condition over baseline methods, including recent\nstate-of-the-art approaches in cross-view image synthesis. Our method generates\nimages of higher quality, fidelity, and diversity than other state-of-the-art\napproaches.\n", "link": "http://arxiv.org/abs/2412.03315v1", "date": "2024-12-04", "relevancy": 2.5825, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6491}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6491}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-guided%20Cross-view%20Diffusion%20for%20One-to-many%20Cross-view%20Image%0A%20%20Synthesis&body=Title%3A%20Geometry-guided%20Cross-view%20Diffusion%20for%20One-to-many%20Cross-view%20Image%0A%20%20Synthesis%0AAuthor%3A%20Tao%20Jun%20Lin%20and%20Wenqing%20Wang%20and%20Yujiao%20Shi%20and%20Akhil%20Perincherry%20and%20Ankit%20Vora%20and%20Hongdong%20Li%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20for%20cross-view%20synthesis%20aimed%20at%0Agenerating%20plausible%20ground-level%20images%20from%20corresponding%20satellite%20imagery%0Aor%20vice%20versa.%20We%20refer%20to%20these%20tasks%20as%20satellite-to-ground%20%28Sat2Grd%29%20and%0Aground-to-satellite%20%28Grd2Sat%29%20synthesis%2C%20respectively.%20Unlike%20previous%20works%0Athat%20typically%20focus%20on%20one-to-one%20generation%2C%20producing%20a%20single%20output%20image%0Afrom%20a%20single%20input%20image%2C%20our%20approach%20acknowledges%20the%20inherent%20one-to-many%0Anature%20of%20the%20problem.%20This%20recognition%20stems%20from%20the%20challenges%20posed%20by%0Adifferences%20in%20illumination%2C%20weather%20conditions%2C%20and%20occlusions%20between%20the%20two%0Aviews.%20To%20effectively%20model%20this%20uncertainty%2C%20we%20leverage%20recent%20advancements%0Ain%20diffusion%20models.%20Specifically%2C%20we%20exploit%20random%20Gaussian%20noise%20to%0Arepresent%20the%20diverse%20possibilities%20learnt%20from%20the%20target%20view%20data.%20We%0Aintroduce%20a%20Geometry-guided%20Cross-view%20Condition%20%28GCC%29%20strategy%20to%20establish%0Aexplicit%20geometric%20correspondences%20between%20satellite%20and%20street-view%20features.%0AThis%20enables%20us%20to%20resolve%20the%20geometry%20ambiguity%20introduced%20by%20camera%20pose%0Abetween%20image%20pairs%2C%20boosting%20the%20performance%20of%20cross-view%20image%20synthesis.%0AThrough%20extensive%20quantitative%20and%20qualitative%20analyses%20on%20three%20benchmark%0Across-view%20datasets%2C%20we%20demonstrate%20the%20superiority%20of%20our%20proposed%0Ageometry-guided%20cross-view%20condition%20over%20baseline%20methods%2C%20including%20recent%0Astate-of-the-art%20approaches%20in%20cross-view%20image%20synthesis.%20Our%20method%20generates%0Aimages%20of%20higher%20quality%2C%20fidelity%2C%20and%20diversity%20than%20other%20state-of-the-art%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-guided%2520Cross-view%2520Diffusion%2520for%2520One-to-many%2520Cross-view%2520Image%250A%2520%2520Synthesis%26entry.906535625%3DTao%2520Jun%2520Lin%2520and%2520Wenqing%2520Wang%2520and%2520Yujiao%2520Shi%2520and%2520Akhil%2520Perincherry%2520and%2520Ankit%2520Vora%2520and%2520Hongdong%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520cross-view%2520synthesis%2520aimed%2520at%250Agenerating%2520plausible%2520ground-level%2520images%2520from%2520corresponding%2520satellite%2520imagery%250Aor%2520vice%2520versa.%2520We%2520refer%2520to%2520these%2520tasks%2520as%2520satellite-to-ground%2520%2528Sat2Grd%2529%2520and%250Aground-to-satellite%2520%2528Grd2Sat%2529%2520synthesis%252C%2520respectively.%2520Unlike%2520previous%2520works%250Athat%2520typically%2520focus%2520on%2520one-to-one%2520generation%252C%2520producing%2520a%2520single%2520output%2520image%250Afrom%2520a%2520single%2520input%2520image%252C%2520our%2520approach%2520acknowledges%2520the%2520inherent%2520one-to-many%250Anature%2520of%2520the%2520problem.%2520This%2520recognition%2520stems%2520from%2520the%2520challenges%2520posed%2520by%250Adifferences%2520in%2520illumination%252C%2520weather%2520conditions%252C%2520and%2520occlusions%2520between%2520the%2520two%250Aviews.%2520To%2520effectively%2520model%2520this%2520uncertainty%252C%2520we%2520leverage%2520recent%2520advancements%250Ain%2520diffusion%2520models.%2520Specifically%252C%2520we%2520exploit%2520random%2520Gaussian%2520noise%2520to%250Arepresent%2520the%2520diverse%2520possibilities%2520learnt%2520from%2520the%2520target%2520view%2520data.%2520We%250Aintroduce%2520a%2520Geometry-guided%2520Cross-view%2520Condition%2520%2528GCC%2529%2520strategy%2520to%2520establish%250Aexplicit%2520geometric%2520correspondences%2520between%2520satellite%2520and%2520street-view%2520features.%250AThis%2520enables%2520us%2520to%2520resolve%2520the%2520geometry%2520ambiguity%2520introduced%2520by%2520camera%2520pose%250Abetween%2520image%2520pairs%252C%2520boosting%2520the%2520performance%2520of%2520cross-view%2520image%2520synthesis.%250AThrough%2520extensive%2520quantitative%2520and%2520qualitative%2520analyses%2520on%2520three%2520benchmark%250Across-view%2520datasets%252C%2520we%2520demonstrate%2520the%2520superiority%2520of%2520our%2520proposed%250Ageometry-guided%2520cross-view%2520condition%2520over%2520baseline%2520methods%252C%2520including%2520recent%250Astate-of-the-art%2520approaches%2520in%2520cross-view%2520image%2520synthesis.%2520Our%2520method%2520generates%250Aimages%2520of%2520higher%2520quality%252C%2520fidelity%252C%2520and%2520diversity%2520than%2520other%2520state-of-the-art%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-guided%20Cross-view%20Diffusion%20for%20One-to-many%20Cross-view%20Image%0A%20%20Synthesis&entry.906535625=Tao%20Jun%20Lin%20and%20Wenqing%20Wang%20and%20Yujiao%20Shi%20and%20Akhil%20Perincherry%20and%20Ankit%20Vora%20and%20Hongdong%20Li&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20for%20cross-view%20synthesis%20aimed%20at%0Agenerating%20plausible%20ground-level%20images%20from%20corresponding%20satellite%20imagery%0Aor%20vice%20versa.%20We%20refer%20to%20these%20tasks%20as%20satellite-to-ground%20%28Sat2Grd%29%20and%0Aground-to-satellite%20%28Grd2Sat%29%20synthesis%2C%20respectively.%20Unlike%20previous%20works%0Athat%20typically%20focus%20on%20one-to-one%20generation%2C%20producing%20a%20single%20output%20image%0Afrom%20a%20single%20input%20image%2C%20our%20approach%20acknowledges%20the%20inherent%20one-to-many%0Anature%20of%20the%20problem.%20This%20recognition%20stems%20from%20the%20challenges%20posed%20by%0Adifferences%20in%20illumination%2C%20weather%20conditions%2C%20and%20occlusions%20between%20the%20two%0Aviews.%20To%20effectively%20model%20this%20uncertainty%2C%20we%20leverage%20recent%20advancements%0Ain%20diffusion%20models.%20Specifically%2C%20we%20exploit%20random%20Gaussian%20noise%20to%0Arepresent%20the%20diverse%20possibilities%20learnt%20from%20the%20target%20view%20data.%20We%0Aintroduce%20a%20Geometry-guided%20Cross-view%20Condition%20%28GCC%29%20strategy%20to%20establish%0Aexplicit%20geometric%20correspondences%20between%20satellite%20and%20street-view%20features.%0AThis%20enables%20us%20to%20resolve%20the%20geometry%20ambiguity%20introduced%20by%20camera%20pose%0Abetween%20image%20pairs%2C%20boosting%20the%20performance%20of%20cross-view%20image%20synthesis.%0AThrough%20extensive%20quantitative%20and%20qualitative%20analyses%20on%20three%20benchmark%0Across-view%20datasets%2C%20we%20demonstrate%20the%20superiority%20of%20our%20proposed%0Ageometry-guided%20cross-view%20condition%20over%20baseline%20methods%2C%20including%20recent%0Astate-of-the-art%20approaches%20in%20cross-view%20image%20synthesis.%20Our%20method%20generates%0Aimages%20of%20higher%20quality%2C%20fidelity%2C%20and%20diversity%20than%20other%20state-of-the-art%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03315v1&entry.124074799=Read"},
{"title": "MaterialPicker: Multi-Modal Material Generation with Diffusion\n  Transformers", "author": "Xiaohe Ma and Valentin Deschaintre and Milo\u0161 Ha\u0161an and Fujun Luan and Kun Zhou and Hongzhi Wu and Yiwei Hu", "abstract": "  High-quality material generation is key for virtual environment authoring and\ninverse rendering. We propose MaterialPicker, a multi-modal material generator\nleveraging a Diffusion Transformer (DiT) architecture, improving and\nsimplifying the creation of high-quality materials from text prompts and/or\nphotographs. Our method can generate a material based on an image crop of a\nmaterial sample, even if the captured surface is distorted, viewed at an angle\nor partially occluded, as is often the case in photographs of natural scenes.\nWe further allow the user to specify a text prompt to provide additional\nguidance for the generation. We finetune a pre-trained DiT-based video\ngenerator into a material generator, where each material map is treated as a\nframe in a video sequence. We evaluate our approach both quantitatively and\nqualitatively and show that it enables more diverse material generation and\nbetter distortion correction than previous work.\n", "link": "http://arxiv.org/abs/2412.03225v1", "date": "2024-12-04", "relevancy": 2.5553, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6678}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6239}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaterialPicker%3A%20Multi-Modal%20Material%20Generation%20with%20Diffusion%0A%20%20Transformers&body=Title%3A%20MaterialPicker%3A%20Multi-Modal%20Material%20Generation%20with%20Diffusion%0A%20%20Transformers%0AAuthor%3A%20Xiaohe%20Ma%20and%20Valentin%20Deschaintre%20and%20Milo%C5%A1%20Ha%C5%A1an%20and%20Fujun%20Luan%20and%20Kun%20Zhou%20and%20Hongzhi%20Wu%20and%20Yiwei%20Hu%0AAbstract%3A%20%20%20High-quality%20material%20generation%20is%20key%20for%20virtual%20environment%20authoring%20and%0Ainverse%20rendering.%20We%20propose%20MaterialPicker%2C%20a%20multi-modal%20material%20generator%0Aleveraging%20a%20Diffusion%20Transformer%20%28DiT%29%20architecture%2C%20improving%20and%0Asimplifying%20the%20creation%20of%20high-quality%20materials%20from%20text%20prompts%20and/or%0Aphotographs.%20Our%20method%20can%20generate%20a%20material%20based%20on%20an%20image%20crop%20of%20a%0Amaterial%20sample%2C%20even%20if%20the%20captured%20surface%20is%20distorted%2C%20viewed%20at%20an%20angle%0Aor%20partially%20occluded%2C%20as%20is%20often%20the%20case%20in%20photographs%20of%20natural%20scenes.%0AWe%20further%20allow%20the%20user%20to%20specify%20a%20text%20prompt%20to%20provide%20additional%0Aguidance%20for%20the%20generation.%20We%20finetune%20a%20pre-trained%20DiT-based%20video%0Agenerator%20into%20a%20material%20generator%2C%20where%20each%20material%20map%20is%20treated%20as%20a%0Aframe%20in%20a%20video%20sequence.%20We%20evaluate%20our%20approach%20both%20quantitatively%20and%0Aqualitatively%20and%20show%20that%20it%20enables%20more%20diverse%20material%20generation%20and%0Abetter%20distortion%20correction%20than%20previous%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaterialPicker%253A%2520Multi-Modal%2520Material%2520Generation%2520with%2520Diffusion%250A%2520%2520Transformers%26entry.906535625%3DXiaohe%2520Ma%2520and%2520Valentin%2520Deschaintre%2520and%2520Milo%25C5%25A1%2520Ha%25C5%25A1an%2520and%2520Fujun%2520Luan%2520and%2520Kun%2520Zhou%2520and%2520Hongzhi%2520Wu%2520and%2520Yiwei%2520Hu%26entry.1292438233%3D%2520%2520High-quality%2520material%2520generation%2520is%2520key%2520for%2520virtual%2520environment%2520authoring%2520and%250Ainverse%2520rendering.%2520We%2520propose%2520MaterialPicker%252C%2520a%2520multi-modal%2520material%2520generator%250Aleveraging%2520a%2520Diffusion%2520Transformer%2520%2528DiT%2529%2520architecture%252C%2520improving%2520and%250Asimplifying%2520the%2520creation%2520of%2520high-quality%2520materials%2520from%2520text%2520prompts%2520and/or%250Aphotographs.%2520Our%2520method%2520can%2520generate%2520a%2520material%2520based%2520on%2520an%2520image%2520crop%2520of%2520a%250Amaterial%2520sample%252C%2520even%2520if%2520the%2520captured%2520surface%2520is%2520distorted%252C%2520viewed%2520at%2520an%2520angle%250Aor%2520partially%2520occluded%252C%2520as%2520is%2520often%2520the%2520case%2520in%2520photographs%2520of%2520natural%2520scenes.%250AWe%2520further%2520allow%2520the%2520user%2520to%2520specify%2520a%2520text%2520prompt%2520to%2520provide%2520additional%250Aguidance%2520for%2520the%2520generation.%2520We%2520finetune%2520a%2520pre-trained%2520DiT-based%2520video%250Agenerator%2520into%2520a%2520material%2520generator%252C%2520where%2520each%2520material%2520map%2520is%2520treated%2520as%2520a%250Aframe%2520in%2520a%2520video%2520sequence.%2520We%2520evaluate%2520our%2520approach%2520both%2520quantitatively%2520and%250Aqualitatively%2520and%2520show%2520that%2520it%2520enables%2520more%2520diverse%2520material%2520generation%2520and%250Abetter%2520distortion%2520correction%2520than%2520previous%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaterialPicker%3A%20Multi-Modal%20Material%20Generation%20with%20Diffusion%0A%20%20Transformers&entry.906535625=Xiaohe%20Ma%20and%20Valentin%20Deschaintre%20and%20Milo%C5%A1%20Ha%C5%A1an%20and%20Fujun%20Luan%20and%20Kun%20Zhou%20and%20Hongzhi%20Wu%20and%20Yiwei%20Hu&entry.1292438233=%20%20High-quality%20material%20generation%20is%20key%20for%20virtual%20environment%20authoring%20and%0Ainverse%20rendering.%20We%20propose%20MaterialPicker%2C%20a%20multi-modal%20material%20generator%0Aleveraging%20a%20Diffusion%20Transformer%20%28DiT%29%20architecture%2C%20improving%20and%0Asimplifying%20the%20creation%20of%20high-quality%20materials%20from%20text%20prompts%20and/or%0Aphotographs.%20Our%20method%20can%20generate%20a%20material%20based%20on%20an%20image%20crop%20of%20a%0Amaterial%20sample%2C%20even%20if%20the%20captured%20surface%20is%20distorted%2C%20viewed%20at%20an%20angle%0Aor%20partially%20occluded%2C%20as%20is%20often%20the%20case%20in%20photographs%20of%20natural%20scenes.%0AWe%20further%20allow%20the%20user%20to%20specify%20a%20text%20prompt%20to%20provide%20additional%0Aguidance%20for%20the%20generation.%20We%20finetune%20a%20pre-trained%20DiT-based%20video%0Agenerator%20into%20a%20material%20generator%2C%20where%20each%20material%20map%20is%20treated%20as%20a%0Aframe%20in%20a%20video%20sequence.%20We%20evaluate%20our%20approach%20both%20quantitatively%20and%0Aqualitatively%20and%20show%20that%20it%20enables%20more%20diverse%20material%20generation%20and%0Abetter%20distortion%20correction%20than%20previous%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03225v1&entry.124074799=Read"},
{"title": "CamI2V: Camera-Controlled Image-to-Video Diffusion Model", "author": "Guangcong Zheng and Teng Li and Rui Jiang and Yehao Lu and Tao Wu and Xi Li", "abstract": "  Recent advancements have integrated camera pose as a user-friendly and\nphysics-informed condition in video diffusion models, enabling precise camera\ncontrol. In this paper, we identify one of the key challenges as effectively\nmodeling noisy cross-frame interactions to enhance geometry consistency and\ncamera controllability. We innovatively associate the quality of a condition\nwith its ability to reduce uncertainty and interpret noisy cross-frame features\nas a form of noisy condition. Recognizing that noisy conditions provide\ndeterministic information while also introducing randomness and potential\nmisguidance due to added noise, we propose applying epipolar attention to only\naggregate features along corresponding epipolar lines, thereby accessing an\noptimal amount of noisy conditions. Additionally, we address scenarios where\nepipolar lines disappear, commonly caused by rapid camera movements, dynamic\nobjects, or occlusions, ensuring robust performance in diverse environments.\nFurthermore, we develop a more robust and reproducible evaluation pipeline to\naddress the inaccuracies and instabilities of existing camera control metrics.\nOur method achieves a 25.64% improvement in camera controllability on the\nRealEstate10K dataset without compromising dynamics or generation quality and\ndemonstrates strong generalization to out-of-domain images. Training and\ninference require only 24GB and 12GB of memory, respectively, for 16-frame\nsequences at 256x256 resolution. We will release all checkpoints, along with\ntraining and evaluation code. Dynamic videos are best viewed at\nhttps://zgctroy.github.io/CamI2V.\n", "link": "http://arxiv.org/abs/2410.15957v3", "date": "2024-12-04", "relevancy": 2.5467, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6448}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6377}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CamI2V%3A%20Camera-Controlled%20Image-to-Video%20Diffusion%20Model&body=Title%3A%20CamI2V%3A%20Camera-Controlled%20Image-to-Video%20Diffusion%20Model%0AAuthor%3A%20Guangcong%20Zheng%20and%20Teng%20Li%20and%20Rui%20Jiang%20and%20Yehao%20Lu%20and%20Tao%20Wu%20and%20Xi%20Li%0AAbstract%3A%20%20%20Recent%20advancements%20have%20integrated%20camera%20pose%20as%20a%20user-friendly%20and%0Aphysics-informed%20condition%20in%20video%20diffusion%20models%2C%20enabling%20precise%20camera%0Acontrol.%20In%20this%20paper%2C%20we%20identify%20one%20of%20the%20key%20challenges%20as%20effectively%0Amodeling%20noisy%20cross-frame%20interactions%20to%20enhance%20geometry%20consistency%20and%0Acamera%20controllability.%20We%20innovatively%20associate%20the%20quality%20of%20a%20condition%0Awith%20its%20ability%20to%20reduce%20uncertainty%20and%20interpret%20noisy%20cross-frame%20features%0Aas%20a%20form%20of%20noisy%20condition.%20Recognizing%20that%20noisy%20conditions%20provide%0Adeterministic%20information%20while%20also%20introducing%20randomness%20and%20potential%0Amisguidance%20due%20to%20added%20noise%2C%20we%20propose%20applying%20epipolar%20attention%20to%20only%0Aaggregate%20features%20along%20corresponding%20epipolar%20lines%2C%20thereby%20accessing%20an%0Aoptimal%20amount%20of%20noisy%20conditions.%20Additionally%2C%20we%20address%20scenarios%20where%0Aepipolar%20lines%20disappear%2C%20commonly%20caused%20by%20rapid%20camera%20movements%2C%20dynamic%0Aobjects%2C%20or%20occlusions%2C%20ensuring%20robust%20performance%20in%20diverse%20environments.%0AFurthermore%2C%20we%20develop%20a%20more%20robust%20and%20reproducible%20evaluation%20pipeline%20to%0Aaddress%20the%20inaccuracies%20and%20instabilities%20of%20existing%20camera%20control%20metrics.%0AOur%20method%20achieves%20a%2025.64%25%20improvement%20in%20camera%20controllability%20on%20the%0ARealEstate10K%20dataset%20without%20compromising%20dynamics%20or%20generation%20quality%20and%0Ademonstrates%20strong%20generalization%20to%20out-of-domain%20images.%20Training%20and%0Ainference%20require%20only%2024GB%20and%2012GB%20of%20memory%2C%20respectively%2C%20for%2016-frame%0Asequences%20at%20256x256%20resolution.%20We%20will%20release%20all%20checkpoints%2C%20along%20with%0Atraining%20and%20evaluation%20code.%20Dynamic%20videos%20are%20best%20viewed%20at%0Ahttps%3A//zgctroy.github.io/CamI2V.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15957v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCamI2V%253A%2520Camera-Controlled%2520Image-to-Video%2520Diffusion%2520Model%26entry.906535625%3DGuangcong%2520Zheng%2520and%2520Teng%2520Li%2520and%2520Rui%2520Jiang%2520and%2520Yehao%2520Lu%2520and%2520Tao%2520Wu%2520and%2520Xi%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520have%2520integrated%2520camera%2520pose%2520as%2520a%2520user-friendly%2520and%250Aphysics-informed%2520condition%2520in%2520video%2520diffusion%2520models%252C%2520enabling%2520precise%2520camera%250Acontrol.%2520In%2520this%2520paper%252C%2520we%2520identify%2520one%2520of%2520the%2520key%2520challenges%2520as%2520effectively%250Amodeling%2520noisy%2520cross-frame%2520interactions%2520to%2520enhance%2520geometry%2520consistency%2520and%250Acamera%2520controllability.%2520We%2520innovatively%2520associate%2520the%2520quality%2520of%2520a%2520condition%250Awith%2520its%2520ability%2520to%2520reduce%2520uncertainty%2520and%2520interpret%2520noisy%2520cross-frame%2520features%250Aas%2520a%2520form%2520of%2520noisy%2520condition.%2520Recognizing%2520that%2520noisy%2520conditions%2520provide%250Adeterministic%2520information%2520while%2520also%2520introducing%2520randomness%2520and%2520potential%250Amisguidance%2520due%2520to%2520added%2520noise%252C%2520we%2520propose%2520applying%2520epipolar%2520attention%2520to%2520only%250Aaggregate%2520features%2520along%2520corresponding%2520epipolar%2520lines%252C%2520thereby%2520accessing%2520an%250Aoptimal%2520amount%2520of%2520noisy%2520conditions.%2520Additionally%252C%2520we%2520address%2520scenarios%2520where%250Aepipolar%2520lines%2520disappear%252C%2520commonly%2520caused%2520by%2520rapid%2520camera%2520movements%252C%2520dynamic%250Aobjects%252C%2520or%2520occlusions%252C%2520ensuring%2520robust%2520performance%2520in%2520diverse%2520environments.%250AFurthermore%252C%2520we%2520develop%2520a%2520more%2520robust%2520and%2520reproducible%2520evaluation%2520pipeline%2520to%250Aaddress%2520the%2520inaccuracies%2520and%2520instabilities%2520of%2520existing%2520camera%2520control%2520metrics.%250AOur%2520method%2520achieves%2520a%252025.64%2525%2520improvement%2520in%2520camera%2520controllability%2520on%2520the%250ARealEstate10K%2520dataset%2520without%2520compromising%2520dynamics%2520or%2520generation%2520quality%2520and%250Ademonstrates%2520strong%2520generalization%2520to%2520out-of-domain%2520images.%2520Training%2520and%250Ainference%2520require%2520only%252024GB%2520and%252012GB%2520of%2520memory%252C%2520respectively%252C%2520for%252016-frame%250Asequences%2520at%2520256x256%2520resolution.%2520We%2520will%2520release%2520all%2520checkpoints%252C%2520along%2520with%250Atraining%2520and%2520evaluation%2520code.%2520Dynamic%2520videos%2520are%2520best%2520viewed%2520at%250Ahttps%253A//zgctroy.github.io/CamI2V.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15957v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CamI2V%3A%20Camera-Controlled%20Image-to-Video%20Diffusion%20Model&entry.906535625=Guangcong%20Zheng%20and%20Teng%20Li%20and%20Rui%20Jiang%20and%20Yehao%20Lu%20and%20Tao%20Wu%20and%20Xi%20Li&entry.1292438233=%20%20Recent%20advancements%20have%20integrated%20camera%20pose%20as%20a%20user-friendly%20and%0Aphysics-informed%20condition%20in%20video%20diffusion%20models%2C%20enabling%20precise%20camera%0Acontrol.%20In%20this%20paper%2C%20we%20identify%20one%20of%20the%20key%20challenges%20as%20effectively%0Amodeling%20noisy%20cross-frame%20interactions%20to%20enhance%20geometry%20consistency%20and%0Acamera%20controllability.%20We%20innovatively%20associate%20the%20quality%20of%20a%20condition%0Awith%20its%20ability%20to%20reduce%20uncertainty%20and%20interpret%20noisy%20cross-frame%20features%0Aas%20a%20form%20of%20noisy%20condition.%20Recognizing%20that%20noisy%20conditions%20provide%0Adeterministic%20information%20while%20also%20introducing%20randomness%20and%20potential%0Amisguidance%20due%20to%20added%20noise%2C%20we%20propose%20applying%20epipolar%20attention%20to%20only%0Aaggregate%20features%20along%20corresponding%20epipolar%20lines%2C%20thereby%20accessing%20an%0Aoptimal%20amount%20of%20noisy%20conditions.%20Additionally%2C%20we%20address%20scenarios%20where%0Aepipolar%20lines%20disappear%2C%20commonly%20caused%20by%20rapid%20camera%20movements%2C%20dynamic%0Aobjects%2C%20or%20occlusions%2C%20ensuring%20robust%20performance%20in%20diverse%20environments.%0AFurthermore%2C%20we%20develop%20a%20more%20robust%20and%20reproducible%20evaluation%20pipeline%20to%0Aaddress%20the%20inaccuracies%20and%20instabilities%20of%20existing%20camera%20control%20metrics.%0AOur%20method%20achieves%20a%2025.64%25%20improvement%20in%20camera%20controllability%20on%20the%0ARealEstate10K%20dataset%20without%20compromising%20dynamics%20or%20generation%20quality%20and%0Ademonstrates%20strong%20generalization%20to%20out-of-domain%20images.%20Training%20and%0Ainference%20require%20only%2024GB%20and%2012GB%20of%20memory%2C%20respectively%2C%20for%2016-frame%0Asequences%20at%20256x256%20resolution.%20We%20will%20release%20all%20checkpoints%2C%20along%20with%0Atraining%20and%20evaluation%20code.%20Dynamic%20videos%20are%20best%20viewed%20at%0Ahttps%3A//zgctroy.github.io/CamI2V.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15957v3&entry.124074799=Read"},
{"title": "PaliGemma 2: A Family of Versatile VLMs for Transfer", "author": "Andreas Steiner and Andr\u00e9 Susano Pinto and Michael Tschannen and Daniel Keysers and Xiao Wang and Yonatan Bitton and Alexey Gritsenko and Matthias Minderer and Anthony Sherbondy and Shangbang Long and Siyang Qin and Reeve Ingle and Emanuele Bugliarello and Sahar Kazemzadeh and Thomas Mesnard and Ibrahim Alabdulmohsin and Lucas Beyer and Xiaohua Zhai", "abstract": "  PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM)\nbased on the Gemma 2 family of language models. We combine the SigLIP-So400m\nvision encoder that was also used by PaliGemma with the whole range of Gemma 2\nmodels, from the 2B one all the way up to the 27B model. We train these models\nat three resolutions (224px, 448px, and 896px) in multiple stages to equip them\nwith broad knowledge for transfer via fine-tuning. The resulting family of base\nmodels covering different model sizes and resolutions allows us to investigate\nfactors impacting transfer performance (such as learning rate) and to analyze\nthe interplay between the type of task, model size, and resolution. We further\nincrease the number and breadth of transfer tasks beyond the scope of PaliGemma\nincluding different OCR-related tasks such as table structure recognition,\nmolecular structure recognition, music score recognition, as well as long\nfine-grained captioning and radiography report generation, on which PaliGemma 2\nobtains state-of-the-art results.\n", "link": "http://arxiv.org/abs/2412.03555v1", "date": "2024-12-04", "relevancy": 2.5176, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5054}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PaliGemma%202%3A%20A%20Family%20of%20Versatile%20VLMs%20for%20Transfer&body=Title%3A%20PaliGemma%202%3A%20A%20Family%20of%20Versatile%20VLMs%20for%20Transfer%0AAuthor%3A%20Andreas%20Steiner%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Michael%20Tschannen%20and%20Daniel%20Keysers%20and%20Xiao%20Wang%20and%20Yonatan%20Bitton%20and%20Alexey%20Gritsenko%20and%20Matthias%20Minderer%20and%20Anthony%20Sherbondy%20and%20Shangbang%20Long%20and%20Siyang%20Qin%20and%20Reeve%20Ingle%20and%20Emanuele%20Bugliarello%20and%20Sahar%20Kazemzadeh%20and%20Thomas%20Mesnard%20and%20Ibrahim%20Alabdulmohsin%20and%20Lucas%20Beyer%20and%20Xiaohua%20Zhai%0AAbstract%3A%20%20%20PaliGemma%202%20is%20an%20upgrade%20of%20the%20PaliGemma%20open%20Vision-Language%20Model%20%28VLM%29%0Abased%20on%20the%20Gemma%202%20family%20of%20language%20models.%20We%20combine%20the%20SigLIP-So400m%0Avision%20encoder%20that%20was%20also%20used%20by%20PaliGemma%20with%20the%20whole%20range%20of%20Gemma%202%0Amodels%2C%20from%20the%202B%20one%20all%20the%20way%20up%20to%20the%2027B%20model.%20We%20train%20these%20models%0Aat%20three%20resolutions%20%28224px%2C%20448px%2C%20and%20896px%29%20in%20multiple%20stages%20to%20equip%20them%0Awith%20broad%20knowledge%20for%20transfer%20via%20fine-tuning.%20The%20resulting%20family%20of%20base%0Amodels%20covering%20different%20model%20sizes%20and%20resolutions%20allows%20us%20to%20investigate%0Afactors%20impacting%20transfer%20performance%20%28such%20as%20learning%20rate%29%20and%20to%20analyze%0Athe%20interplay%20between%20the%20type%20of%20task%2C%20model%20size%2C%20and%20resolution.%20We%20further%0Aincrease%20the%20number%20and%20breadth%20of%20transfer%20tasks%20beyond%20the%20scope%20of%20PaliGemma%0Aincluding%20different%20OCR-related%20tasks%20such%20as%20table%20structure%20recognition%2C%0Amolecular%20structure%20recognition%2C%20music%20score%20recognition%2C%20as%20well%20as%20long%0Afine-grained%20captioning%20and%20radiography%20report%20generation%2C%20on%20which%20PaliGemma%202%0Aobtains%20state-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaliGemma%25202%253A%2520A%2520Family%2520of%2520Versatile%2520VLMs%2520for%2520Transfer%26entry.906535625%3DAndreas%2520Steiner%2520and%2520Andr%25C3%25A9%2520Susano%2520Pinto%2520and%2520Michael%2520Tschannen%2520and%2520Daniel%2520Keysers%2520and%2520Xiao%2520Wang%2520and%2520Yonatan%2520Bitton%2520and%2520Alexey%2520Gritsenko%2520and%2520Matthias%2520Minderer%2520and%2520Anthony%2520Sherbondy%2520and%2520Shangbang%2520Long%2520and%2520Siyang%2520Qin%2520and%2520Reeve%2520Ingle%2520and%2520Emanuele%2520Bugliarello%2520and%2520Sahar%2520Kazemzadeh%2520and%2520Thomas%2520Mesnard%2520and%2520Ibrahim%2520Alabdulmohsin%2520and%2520Lucas%2520Beyer%2520and%2520Xiaohua%2520Zhai%26entry.1292438233%3D%2520%2520PaliGemma%25202%2520is%2520an%2520upgrade%2520of%2520the%2520PaliGemma%2520open%2520Vision-Language%2520Model%2520%2528VLM%2529%250Abased%2520on%2520the%2520Gemma%25202%2520family%2520of%2520language%2520models.%2520We%2520combine%2520the%2520SigLIP-So400m%250Avision%2520encoder%2520that%2520was%2520also%2520used%2520by%2520PaliGemma%2520with%2520the%2520whole%2520range%2520of%2520Gemma%25202%250Amodels%252C%2520from%2520the%25202B%2520one%2520all%2520the%2520way%2520up%2520to%2520the%252027B%2520model.%2520We%2520train%2520these%2520models%250Aat%2520three%2520resolutions%2520%2528224px%252C%2520448px%252C%2520and%2520896px%2529%2520in%2520multiple%2520stages%2520to%2520equip%2520them%250Awith%2520broad%2520knowledge%2520for%2520transfer%2520via%2520fine-tuning.%2520The%2520resulting%2520family%2520of%2520base%250Amodels%2520covering%2520different%2520model%2520sizes%2520and%2520resolutions%2520allows%2520us%2520to%2520investigate%250Afactors%2520impacting%2520transfer%2520performance%2520%2528such%2520as%2520learning%2520rate%2529%2520and%2520to%2520analyze%250Athe%2520interplay%2520between%2520the%2520type%2520of%2520task%252C%2520model%2520size%252C%2520and%2520resolution.%2520We%2520further%250Aincrease%2520the%2520number%2520and%2520breadth%2520of%2520transfer%2520tasks%2520beyond%2520the%2520scope%2520of%2520PaliGemma%250Aincluding%2520different%2520OCR-related%2520tasks%2520such%2520as%2520table%2520structure%2520recognition%252C%250Amolecular%2520structure%2520recognition%252C%2520music%2520score%2520recognition%252C%2520as%2520well%2520as%2520long%250Afine-grained%2520captioning%2520and%2520radiography%2520report%2520generation%252C%2520on%2520which%2520PaliGemma%25202%250Aobtains%2520state-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PaliGemma%202%3A%20A%20Family%20of%20Versatile%20VLMs%20for%20Transfer&entry.906535625=Andreas%20Steiner%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Michael%20Tschannen%20and%20Daniel%20Keysers%20and%20Xiao%20Wang%20and%20Yonatan%20Bitton%20and%20Alexey%20Gritsenko%20and%20Matthias%20Minderer%20and%20Anthony%20Sherbondy%20and%20Shangbang%20Long%20and%20Siyang%20Qin%20and%20Reeve%20Ingle%20and%20Emanuele%20Bugliarello%20and%20Sahar%20Kazemzadeh%20and%20Thomas%20Mesnard%20and%20Ibrahim%20Alabdulmohsin%20and%20Lucas%20Beyer%20and%20Xiaohua%20Zhai&entry.1292438233=%20%20PaliGemma%202%20is%20an%20upgrade%20of%20the%20PaliGemma%20open%20Vision-Language%20Model%20%28VLM%29%0Abased%20on%20the%20Gemma%202%20family%20of%20language%20models.%20We%20combine%20the%20SigLIP-So400m%0Avision%20encoder%20that%20was%20also%20used%20by%20PaliGemma%20with%20the%20whole%20range%20of%20Gemma%202%0Amodels%2C%20from%20the%202B%20one%20all%20the%20way%20up%20to%20the%2027B%20model.%20We%20train%20these%20models%0Aat%20three%20resolutions%20%28224px%2C%20448px%2C%20and%20896px%29%20in%20multiple%20stages%20to%20equip%20them%0Awith%20broad%20knowledge%20for%20transfer%20via%20fine-tuning.%20The%20resulting%20family%20of%20base%0Amodels%20covering%20different%20model%20sizes%20and%20resolutions%20allows%20us%20to%20investigate%0Afactors%20impacting%20transfer%20performance%20%28such%20as%20learning%20rate%29%20and%20to%20analyze%0Athe%20interplay%20between%20the%20type%20of%20task%2C%20model%20size%2C%20and%20resolution.%20We%20further%0Aincrease%20the%20number%20and%20breadth%20of%20transfer%20tasks%20beyond%20the%20scope%20of%20PaliGemma%0Aincluding%20different%20OCR-related%20tasks%20such%20as%20table%20structure%20recognition%2C%0Amolecular%20structure%20recognition%2C%20music%20score%20recognition%2C%20as%20well%20as%20long%0Afine-grained%20captioning%20and%20radiography%20report%20generation%2C%20on%20which%20PaliGemma%202%0Aobtains%20state-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03555v1&entry.124074799=Read"},
{"title": "Granular Ball Twin Support Vector Machine with Universum Data", "author": "M. A. Ganaie and Vrushank Ahire", "abstract": "  Classification with support vector machines (SVM) often suffers from limited\nperformance when relying solely on labeled data from target classes and is\nsensitive to noise and outliers. Incorporating prior knowledge from Universum\ndata and more robust data representations can enhance accuracy and efficiency.\nMotivated by these findings, we propose a novel Granular Ball Twin Support\nVector Machine with Universum Data (GBU-TSVM) that extends the TSVM framework\nto leverage both Universum samples and granular ball computing during model\ntraining. Unlike existing TSVM methods, the proposed GBU-TSVM represents data\ninstances as hyper-balls rather than points in the feature space. This\ninnovative approach improves the model's robustness and efficiency,\nparticularly in handling noisy and large datasets. By grouping data points into\ngranular balls, the model achieves superior computational efficiency, increased\nnoise resistance, and enhanced interpretability. Additionally, the inclusion of\nUniversum data, which consists of samples that are not strictly from the target\nclasses, further refines the classification boundaries. This integration\nenriches the model with contextual information, refining classification\nboundaries and boosting overall accuracy. Experimental results on UCI benchmark\ndatasets demonstrate that the GBU-TSVM outperforms existing TSVM models in both\naccuracy and computational efficiency. These findings highlight the potential\nof the GBU-TSVM model in setting a new standard in data representation and\nclassification.\n", "link": "http://arxiv.org/abs/2412.03375v1", "date": "2024-12-04", "relevancy": 2.4933, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5132}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4987}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Granular%20Ball%20Twin%20Support%20Vector%20Machine%20with%20Universum%20Data&body=Title%3A%20Granular%20Ball%20Twin%20Support%20Vector%20Machine%20with%20Universum%20Data%0AAuthor%3A%20M.%20A.%20Ganaie%20and%20Vrushank%20Ahire%0AAbstract%3A%20%20%20Classification%20with%20support%20vector%20machines%20%28SVM%29%20often%20suffers%20from%20limited%0Aperformance%20when%20relying%20solely%20on%20labeled%20data%20from%20target%20classes%20and%20is%0Asensitive%20to%20noise%20and%20outliers.%20Incorporating%20prior%20knowledge%20from%20Universum%0Adata%20and%20more%20robust%20data%20representations%20can%20enhance%20accuracy%20and%20efficiency.%0AMotivated%20by%20these%20findings%2C%20we%20propose%20a%20novel%20Granular%20Ball%20Twin%20Support%0AVector%20Machine%20with%20Universum%20Data%20%28GBU-TSVM%29%20that%20extends%20the%20TSVM%20framework%0Ato%20leverage%20both%20Universum%20samples%20and%20granular%20ball%20computing%20during%20model%0Atraining.%20Unlike%20existing%20TSVM%20methods%2C%20the%20proposed%20GBU-TSVM%20represents%20data%0Ainstances%20as%20hyper-balls%20rather%20than%20points%20in%20the%20feature%20space.%20This%0Ainnovative%20approach%20improves%20the%20model%27s%20robustness%20and%20efficiency%2C%0Aparticularly%20in%20handling%20noisy%20and%20large%20datasets.%20By%20grouping%20data%20points%20into%0Agranular%20balls%2C%20the%20model%20achieves%20superior%20computational%20efficiency%2C%20increased%0Anoise%20resistance%2C%20and%20enhanced%20interpretability.%20Additionally%2C%20the%20inclusion%20of%0AUniversum%20data%2C%20which%20consists%20of%20samples%20that%20are%20not%20strictly%20from%20the%20target%0Aclasses%2C%20further%20refines%20the%20classification%20boundaries.%20This%20integration%0Aenriches%20the%20model%20with%20contextual%20information%2C%20refining%20classification%0Aboundaries%20and%20boosting%20overall%20accuracy.%20Experimental%20results%20on%20UCI%20benchmark%0Adatasets%20demonstrate%20that%20the%20GBU-TSVM%20outperforms%20existing%20TSVM%20models%20in%20both%0Aaccuracy%20and%20computational%20efficiency.%20These%20findings%20highlight%20the%20potential%0Aof%20the%20GBU-TSVM%20model%20in%20setting%20a%20new%20standard%20in%20data%20representation%20and%0Aclassification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGranular%2520Ball%2520Twin%2520Support%2520Vector%2520Machine%2520with%2520Universum%2520Data%26entry.906535625%3DM.%2520A.%2520Ganaie%2520and%2520Vrushank%2520Ahire%26entry.1292438233%3D%2520%2520Classification%2520with%2520support%2520vector%2520machines%2520%2528SVM%2529%2520often%2520suffers%2520from%2520limited%250Aperformance%2520when%2520relying%2520solely%2520on%2520labeled%2520data%2520from%2520target%2520classes%2520and%2520is%250Asensitive%2520to%2520noise%2520and%2520outliers.%2520Incorporating%2520prior%2520knowledge%2520from%2520Universum%250Adata%2520and%2520more%2520robust%2520data%2520representations%2520can%2520enhance%2520accuracy%2520and%2520efficiency.%250AMotivated%2520by%2520these%2520findings%252C%2520we%2520propose%2520a%2520novel%2520Granular%2520Ball%2520Twin%2520Support%250AVector%2520Machine%2520with%2520Universum%2520Data%2520%2528GBU-TSVM%2529%2520that%2520extends%2520the%2520TSVM%2520framework%250Ato%2520leverage%2520both%2520Universum%2520samples%2520and%2520granular%2520ball%2520computing%2520during%2520model%250Atraining.%2520Unlike%2520existing%2520TSVM%2520methods%252C%2520the%2520proposed%2520GBU-TSVM%2520represents%2520data%250Ainstances%2520as%2520hyper-balls%2520rather%2520than%2520points%2520in%2520the%2520feature%2520space.%2520This%250Ainnovative%2520approach%2520improves%2520the%2520model%2527s%2520robustness%2520and%2520efficiency%252C%250Aparticularly%2520in%2520handling%2520noisy%2520and%2520large%2520datasets.%2520By%2520grouping%2520data%2520points%2520into%250Agranular%2520balls%252C%2520the%2520model%2520achieves%2520superior%2520computational%2520efficiency%252C%2520increased%250Anoise%2520resistance%252C%2520and%2520enhanced%2520interpretability.%2520Additionally%252C%2520the%2520inclusion%2520of%250AUniversum%2520data%252C%2520which%2520consists%2520of%2520samples%2520that%2520are%2520not%2520strictly%2520from%2520the%2520target%250Aclasses%252C%2520further%2520refines%2520the%2520classification%2520boundaries.%2520This%2520integration%250Aenriches%2520the%2520model%2520with%2520contextual%2520information%252C%2520refining%2520classification%250Aboundaries%2520and%2520boosting%2520overall%2520accuracy.%2520Experimental%2520results%2520on%2520UCI%2520benchmark%250Adatasets%2520demonstrate%2520that%2520the%2520GBU-TSVM%2520outperforms%2520existing%2520TSVM%2520models%2520in%2520both%250Aaccuracy%2520and%2520computational%2520efficiency.%2520These%2520findings%2520highlight%2520the%2520potential%250Aof%2520the%2520GBU-TSVM%2520model%2520in%2520setting%2520a%2520new%2520standard%2520in%2520data%2520representation%2520and%250Aclassification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Granular%20Ball%20Twin%20Support%20Vector%20Machine%20with%20Universum%20Data&entry.906535625=M.%20A.%20Ganaie%20and%20Vrushank%20Ahire&entry.1292438233=%20%20Classification%20with%20support%20vector%20machines%20%28SVM%29%20often%20suffers%20from%20limited%0Aperformance%20when%20relying%20solely%20on%20labeled%20data%20from%20target%20classes%20and%20is%0Asensitive%20to%20noise%20and%20outliers.%20Incorporating%20prior%20knowledge%20from%20Universum%0Adata%20and%20more%20robust%20data%20representations%20can%20enhance%20accuracy%20and%20efficiency.%0AMotivated%20by%20these%20findings%2C%20we%20propose%20a%20novel%20Granular%20Ball%20Twin%20Support%0AVector%20Machine%20with%20Universum%20Data%20%28GBU-TSVM%29%20that%20extends%20the%20TSVM%20framework%0Ato%20leverage%20both%20Universum%20samples%20and%20granular%20ball%20computing%20during%20model%0Atraining.%20Unlike%20existing%20TSVM%20methods%2C%20the%20proposed%20GBU-TSVM%20represents%20data%0Ainstances%20as%20hyper-balls%20rather%20than%20points%20in%20the%20feature%20space.%20This%0Ainnovative%20approach%20improves%20the%20model%27s%20robustness%20and%20efficiency%2C%0Aparticularly%20in%20handling%20noisy%20and%20large%20datasets.%20By%20grouping%20data%20points%20into%0Agranular%20balls%2C%20the%20model%20achieves%20superior%20computational%20efficiency%2C%20increased%0Anoise%20resistance%2C%20and%20enhanced%20interpretability.%20Additionally%2C%20the%20inclusion%20of%0AUniversum%20data%2C%20which%20consists%20of%20samples%20that%20are%20not%20strictly%20from%20the%20target%0Aclasses%2C%20further%20refines%20the%20classification%20boundaries.%20This%20integration%0Aenriches%20the%20model%20with%20contextual%20information%2C%20refining%20classification%0Aboundaries%20and%20boosting%20overall%20accuracy.%20Experimental%20results%20on%20UCI%20benchmark%0Adatasets%20demonstrate%20that%20the%20GBU-TSVM%20outperforms%20existing%20TSVM%20models%20in%20both%0Aaccuracy%20and%20computational%20efficiency.%20These%20findings%20highlight%20the%20potential%0Aof%20the%20GBU-TSVM%20model%20in%20setting%20a%20new%20standard%20in%20data%20representation%20and%0Aclassification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03375v1&entry.124074799=Read"},
{"title": "Tight PAC-Bayesian Risk Certificates for Contrastive Learning", "author": "Anna van Elst and Debarghya Ghoshdastidar", "abstract": "  Contrastive representation learning is a modern paradigm for learning\nrepresentations of unlabeled data via augmentations -- precisely, contrastive\nmodels learn to embed semantically similar pairs of samples (positive pairs)\ncloser than independently drawn samples (negative samples). In spite of its\nempirical success and widespread use in foundation models, statistical theory\nfor contrastive learning remains less explored. Recent works have developed\ngeneralization error bounds for contrastive losses, but the resulting risk\ncertificates are either vacuous (certificates based on Rademacher complexity or\n$f$-divergence) or require strong assumptions about samples that are\nunreasonable in practice. The present paper develops non-vacuous PAC-Bayesian\nrisk certificates for contrastive representation learning, considering the\npractical considerations of the popular SimCLR framework. Notably, we take into\naccount that SimCLR reuses positive pairs of augmented data as negative samples\nfor other data, thereby inducing strong dependence and making classical PAC or\nPAC-Bayesian bounds inapplicable. We further refine existing bounds on the\ndownstream classification loss by incorporating SimCLR-specific factors,\nincluding data augmentation and temperature scaling, and derive risk\ncertificates for the contrastive zero-one risk. The resulting bounds for\ncontrastive loss and downstream prediction are much tighter than those of\nprevious risk certificates, as demonstrated by experiments on CIFAR-10.\n", "link": "http://arxiv.org/abs/2412.03486v1", "date": "2024-12-04", "relevancy": 2.4574, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5175}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4817}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tight%20PAC-Bayesian%20Risk%20Certificates%20for%20Contrastive%20Learning&body=Title%3A%20Tight%20PAC-Bayesian%20Risk%20Certificates%20for%20Contrastive%20Learning%0AAuthor%3A%20Anna%20van%20Elst%20and%20Debarghya%20Ghoshdastidar%0AAbstract%3A%20%20%20Contrastive%20representation%20learning%20is%20a%20modern%20paradigm%20for%20learning%0Arepresentations%20of%20unlabeled%20data%20via%20augmentations%20--%20precisely%2C%20contrastive%0Amodels%20learn%20to%20embed%20semantically%20similar%20pairs%20of%20samples%20%28positive%20pairs%29%0Acloser%20than%20independently%20drawn%20samples%20%28negative%20samples%29.%20In%20spite%20of%20its%0Aempirical%20success%20and%20widespread%20use%20in%20foundation%20models%2C%20statistical%20theory%0Afor%20contrastive%20learning%20remains%20less%20explored.%20Recent%20works%20have%20developed%0Ageneralization%20error%20bounds%20for%20contrastive%20losses%2C%20but%20the%20resulting%20risk%0Acertificates%20are%20either%20vacuous%20%28certificates%20based%20on%20Rademacher%20complexity%20or%0A%24f%24-divergence%29%20or%20require%20strong%20assumptions%20about%20samples%20that%20are%0Aunreasonable%20in%20practice.%20The%20present%20paper%20develops%20non-vacuous%20PAC-Bayesian%0Arisk%20certificates%20for%20contrastive%20representation%20learning%2C%20considering%20the%0Apractical%20considerations%20of%20the%20popular%20SimCLR%20framework.%20Notably%2C%20we%20take%20into%0Aaccount%20that%20SimCLR%20reuses%20positive%20pairs%20of%20augmented%20data%20as%20negative%20samples%0Afor%20other%20data%2C%20thereby%20inducing%20strong%20dependence%20and%20making%20classical%20PAC%20or%0APAC-Bayesian%20bounds%20inapplicable.%20We%20further%20refine%20existing%20bounds%20on%20the%0Adownstream%20classification%20loss%20by%20incorporating%20SimCLR-specific%20factors%2C%0Aincluding%20data%20augmentation%20and%20temperature%20scaling%2C%20and%20derive%20risk%0Acertificates%20for%20the%20contrastive%20zero-one%20risk.%20The%20resulting%20bounds%20for%0Acontrastive%20loss%20and%20downstream%20prediction%20are%20much%20tighter%20than%20those%20of%0Aprevious%20risk%20certificates%2C%20as%20demonstrated%20by%20experiments%20on%20CIFAR-10.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTight%2520PAC-Bayesian%2520Risk%2520Certificates%2520for%2520Contrastive%2520Learning%26entry.906535625%3DAnna%2520van%2520Elst%2520and%2520Debarghya%2520Ghoshdastidar%26entry.1292438233%3D%2520%2520Contrastive%2520representation%2520learning%2520is%2520a%2520modern%2520paradigm%2520for%2520learning%250Arepresentations%2520of%2520unlabeled%2520data%2520via%2520augmentations%2520--%2520precisely%252C%2520contrastive%250Amodels%2520learn%2520to%2520embed%2520semantically%2520similar%2520pairs%2520of%2520samples%2520%2528positive%2520pairs%2529%250Acloser%2520than%2520independently%2520drawn%2520samples%2520%2528negative%2520samples%2529.%2520In%2520spite%2520of%2520its%250Aempirical%2520success%2520and%2520widespread%2520use%2520in%2520foundation%2520models%252C%2520statistical%2520theory%250Afor%2520contrastive%2520learning%2520remains%2520less%2520explored.%2520Recent%2520works%2520have%2520developed%250Ageneralization%2520error%2520bounds%2520for%2520contrastive%2520losses%252C%2520but%2520the%2520resulting%2520risk%250Acertificates%2520are%2520either%2520vacuous%2520%2528certificates%2520based%2520on%2520Rademacher%2520complexity%2520or%250A%2524f%2524-divergence%2529%2520or%2520require%2520strong%2520assumptions%2520about%2520samples%2520that%2520are%250Aunreasonable%2520in%2520practice.%2520The%2520present%2520paper%2520develops%2520non-vacuous%2520PAC-Bayesian%250Arisk%2520certificates%2520for%2520contrastive%2520representation%2520learning%252C%2520considering%2520the%250Apractical%2520considerations%2520of%2520the%2520popular%2520SimCLR%2520framework.%2520Notably%252C%2520we%2520take%2520into%250Aaccount%2520that%2520SimCLR%2520reuses%2520positive%2520pairs%2520of%2520augmented%2520data%2520as%2520negative%2520samples%250Afor%2520other%2520data%252C%2520thereby%2520inducing%2520strong%2520dependence%2520and%2520making%2520classical%2520PAC%2520or%250APAC-Bayesian%2520bounds%2520inapplicable.%2520We%2520further%2520refine%2520existing%2520bounds%2520on%2520the%250Adownstream%2520classification%2520loss%2520by%2520incorporating%2520SimCLR-specific%2520factors%252C%250Aincluding%2520data%2520augmentation%2520and%2520temperature%2520scaling%252C%2520and%2520derive%2520risk%250Acertificates%2520for%2520the%2520contrastive%2520zero-one%2520risk.%2520The%2520resulting%2520bounds%2520for%250Acontrastive%2520loss%2520and%2520downstream%2520prediction%2520are%2520much%2520tighter%2520than%2520those%2520of%250Aprevious%2520risk%2520certificates%252C%2520as%2520demonstrated%2520by%2520experiments%2520on%2520CIFAR-10.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tight%20PAC-Bayesian%20Risk%20Certificates%20for%20Contrastive%20Learning&entry.906535625=Anna%20van%20Elst%20and%20Debarghya%20Ghoshdastidar&entry.1292438233=%20%20Contrastive%20representation%20learning%20is%20a%20modern%20paradigm%20for%20learning%0Arepresentations%20of%20unlabeled%20data%20via%20augmentations%20--%20precisely%2C%20contrastive%0Amodels%20learn%20to%20embed%20semantically%20similar%20pairs%20of%20samples%20%28positive%20pairs%29%0Acloser%20than%20independently%20drawn%20samples%20%28negative%20samples%29.%20In%20spite%20of%20its%0Aempirical%20success%20and%20widespread%20use%20in%20foundation%20models%2C%20statistical%20theory%0Afor%20contrastive%20learning%20remains%20less%20explored.%20Recent%20works%20have%20developed%0Ageneralization%20error%20bounds%20for%20contrastive%20losses%2C%20but%20the%20resulting%20risk%0Acertificates%20are%20either%20vacuous%20%28certificates%20based%20on%20Rademacher%20complexity%20or%0A%24f%24-divergence%29%20or%20require%20strong%20assumptions%20about%20samples%20that%20are%0Aunreasonable%20in%20practice.%20The%20present%20paper%20develops%20non-vacuous%20PAC-Bayesian%0Arisk%20certificates%20for%20contrastive%20representation%20learning%2C%20considering%20the%0Apractical%20considerations%20of%20the%20popular%20SimCLR%20framework.%20Notably%2C%20we%20take%20into%0Aaccount%20that%20SimCLR%20reuses%20positive%20pairs%20of%20augmented%20data%20as%20negative%20samples%0Afor%20other%20data%2C%20thereby%20inducing%20strong%20dependence%20and%20making%20classical%20PAC%20or%0APAC-Bayesian%20bounds%20inapplicable.%20We%20further%20refine%20existing%20bounds%20on%20the%0Adownstream%20classification%20loss%20by%20incorporating%20SimCLR-specific%20factors%2C%0Aincluding%20data%20augmentation%20and%20temperature%20scaling%2C%20and%20derive%20risk%0Acertificates%20for%20the%20contrastive%20zero-one%20risk.%20The%20resulting%20bounds%20for%0Acontrastive%20loss%20and%20downstream%20prediction%20are%20much%20tighter%20than%20those%20of%0Aprevious%20risk%20certificates%2C%20as%20demonstrated%20by%20experiments%20on%20CIFAR-10.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03486v1&entry.124074799=Read"},
{"title": "Number Cookbook: Number Understanding of Language Models and How to\n  Improve It", "author": "Haotong Yang and Yi Hu and Shijia Kang and Zhouchen Lin and Muhan Zhang", "abstract": "  Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as tokenizers, PEs, and number\nformats), comprehensively evaluating their effectiveness using our testbed. We\nalso finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)\nnaive finetuning can improve NUPA a lot on many but not all tasks, and 2)\nsurprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work provides a more detailed and comprehensive\nunderstanding of NUPA in LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook.\n", "link": "http://arxiv.org/abs/2411.03766v2", "date": "2024-12-04", "relevancy": 2.4381, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Number%20Cookbook%3A%20Number%20Understanding%20of%20Language%20Models%20and%20How%20to%0A%20%20Improve%20It&body=Title%3A%20Number%20Cookbook%3A%20Number%20Understanding%20of%20Language%20Models%20and%20How%20to%0A%20%20Improve%20It%0AAuthor%3A%20Haotong%20Yang%20and%20Yi%20Hu%20and%20Shijia%20Kang%20and%20Zhouchen%20Lin%20and%20Muhan%20Zhang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20can%20solve%20an%20increasing%20number%20of%20complex%0Areasoning%20tasks%20while%20making%20surprising%20mistakes%20in%20basic%20numerical%0Aunderstanding%20and%20processing%20%28such%20as%209.11%20%3E%209.9%29.%20The%20latter%20ability%20is%0Aessential%20for%20tackling%20complex%20arithmetic%20and%20mathematical%20problems%20and%20serves%0Aas%20a%20foundation%20for%20most%20reasoning%20tasks%2C%20but%20previous%20work%20paid%20little%0Aattention%20to%20it%20or%20only%20discussed%20several%20restricted%20tasks%20%28like%20integer%0Aaddition%29.%20In%20this%20paper%2C%20we%20comprehensively%20investigate%20the%20numerical%0Aunderstanding%20and%20processing%20ability%20%28NUPA%29%20of%20LLMs.%20Firstly%2C%20we%20introduce%20a%0Abenchmark%20covering%20four%20common%20numerical%20representations%20and%2017%20distinct%0Anumerical%20tasks%20in%20four%20major%20categories%2C%20resulting%20in%2041%20meaningful%0Acombinations%20in%20total.%20These%20tasks%20are%20derived%20from%20primary%20and%20secondary%0Aeducation%20curricula%2C%20encompassing%20nearly%20all%20everyday%20numerical%20understanding%0Aand%20processing%20scenarios%2C%20and%20the%20rules%20of%20these%20tasks%20are%20very%20simple%20and%0Aclear.%20Through%20the%20benchmark%2C%20we%20find%20that%20current%20LLMs%20fail%20frequently%20in%20many%0Aof%20the%20tasks.%20To%20study%20the%20problem%2C%20we%20train%20small%20models%20with%20existing%20and%0Apotential%20techniques%20for%20enhancing%20NUPA%20%28such%20as%20tokenizers%2C%20PEs%2C%20and%20number%0Aformats%29%2C%20comprehensively%20evaluating%20their%20effectiveness%20using%20our%20testbed.%20We%0Aalso%20finetune%20practical-scale%20LLMs%20on%20our%20proposed%20NUPA%20tasks%20and%20find%20that%201%29%0Anaive%20finetuning%20can%20improve%20NUPA%20a%20lot%20on%20many%20but%20not%20all%20tasks%2C%20and%202%29%0Asurprisingly%2C%20techniques%20designed%20to%20enhance%20NUPA%20prove%20ineffective%20for%0Afinetuning%20pretrained%20models.%20We%20further%20explore%20the%20impact%20of%20chain-of-thought%0Atechniques%20on%20NUPA.%20Our%20work%20provides%20a%20more%20detailed%20and%20comprehensive%0Aunderstanding%20of%20NUPA%20in%20LLMs.%20Our%20benchmark%20and%20code%20are%20released%20at%0Ahttps%3A//github.com/GraphPKU/number_cookbook.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03766v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNumber%2520Cookbook%253A%2520Number%2520Understanding%2520of%2520Language%2520Models%2520and%2520How%2520to%250A%2520%2520Improve%2520It%26entry.906535625%3DHaotong%2520Yang%2520and%2520Yi%2520Hu%2520and%2520Shijia%2520Kang%2520and%2520Zhouchen%2520Lin%2520and%2520Muhan%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520can%2520solve%2520an%2520increasing%2520number%2520of%2520complex%250Areasoning%2520tasks%2520while%2520making%2520surprising%2520mistakes%2520in%2520basic%2520numerical%250Aunderstanding%2520and%2520processing%2520%2528such%2520as%25209.11%2520%253E%25209.9%2529.%2520The%2520latter%2520ability%2520is%250Aessential%2520for%2520tackling%2520complex%2520arithmetic%2520and%2520mathematical%2520problems%2520and%2520serves%250Aas%2520a%2520foundation%2520for%2520most%2520reasoning%2520tasks%252C%2520but%2520previous%2520work%2520paid%2520little%250Aattention%2520to%2520it%2520or%2520only%2520discussed%2520several%2520restricted%2520tasks%2520%2528like%2520integer%250Aaddition%2529.%2520In%2520this%2520paper%252C%2520we%2520comprehensively%2520investigate%2520the%2520numerical%250Aunderstanding%2520and%2520processing%2520ability%2520%2528NUPA%2529%2520of%2520LLMs.%2520Firstly%252C%2520we%2520introduce%2520a%250Abenchmark%2520covering%2520four%2520common%2520numerical%2520representations%2520and%252017%2520distinct%250Anumerical%2520tasks%2520in%2520four%2520major%2520categories%252C%2520resulting%2520in%252041%2520meaningful%250Acombinations%2520in%2520total.%2520These%2520tasks%2520are%2520derived%2520from%2520primary%2520and%2520secondary%250Aeducation%2520curricula%252C%2520encompassing%2520nearly%2520all%2520everyday%2520numerical%2520understanding%250Aand%2520processing%2520scenarios%252C%2520and%2520the%2520rules%2520of%2520these%2520tasks%2520are%2520very%2520simple%2520and%250Aclear.%2520Through%2520the%2520benchmark%252C%2520we%2520find%2520that%2520current%2520LLMs%2520fail%2520frequently%2520in%2520many%250Aof%2520the%2520tasks.%2520To%2520study%2520the%2520problem%252C%2520we%2520train%2520small%2520models%2520with%2520existing%2520and%250Apotential%2520techniques%2520for%2520enhancing%2520NUPA%2520%2528such%2520as%2520tokenizers%252C%2520PEs%252C%2520and%2520number%250Aformats%2529%252C%2520comprehensively%2520evaluating%2520their%2520effectiveness%2520using%2520our%2520testbed.%2520We%250Aalso%2520finetune%2520practical-scale%2520LLMs%2520on%2520our%2520proposed%2520NUPA%2520tasks%2520and%2520find%2520that%25201%2529%250Anaive%2520finetuning%2520can%2520improve%2520NUPA%2520a%2520lot%2520on%2520many%2520but%2520not%2520all%2520tasks%252C%2520and%25202%2529%250Asurprisingly%252C%2520techniques%2520designed%2520to%2520enhance%2520NUPA%2520prove%2520ineffective%2520for%250Afinetuning%2520pretrained%2520models.%2520We%2520further%2520explore%2520the%2520impact%2520of%2520chain-of-thought%250Atechniques%2520on%2520NUPA.%2520Our%2520work%2520provides%2520a%2520more%2520detailed%2520and%2520comprehensive%250Aunderstanding%2520of%2520NUPA%2520in%2520LLMs.%2520Our%2520benchmark%2520and%2520code%2520are%2520released%2520at%250Ahttps%253A//github.com/GraphPKU/number_cookbook.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03766v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Number%20Cookbook%3A%20Number%20Understanding%20of%20Language%20Models%20and%20How%20to%0A%20%20Improve%20It&entry.906535625=Haotong%20Yang%20and%20Yi%20Hu%20and%20Shijia%20Kang%20and%20Zhouchen%20Lin%20and%20Muhan%20Zhang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20can%20solve%20an%20increasing%20number%20of%20complex%0Areasoning%20tasks%20while%20making%20surprising%20mistakes%20in%20basic%20numerical%0Aunderstanding%20and%20processing%20%28such%20as%209.11%20%3E%209.9%29.%20The%20latter%20ability%20is%0Aessential%20for%20tackling%20complex%20arithmetic%20and%20mathematical%20problems%20and%20serves%0Aas%20a%20foundation%20for%20most%20reasoning%20tasks%2C%20but%20previous%20work%20paid%20little%0Aattention%20to%20it%20or%20only%20discussed%20several%20restricted%20tasks%20%28like%20integer%0Aaddition%29.%20In%20this%20paper%2C%20we%20comprehensively%20investigate%20the%20numerical%0Aunderstanding%20and%20processing%20ability%20%28NUPA%29%20of%20LLMs.%20Firstly%2C%20we%20introduce%20a%0Abenchmark%20covering%20four%20common%20numerical%20representations%20and%2017%20distinct%0Anumerical%20tasks%20in%20four%20major%20categories%2C%20resulting%20in%2041%20meaningful%0Acombinations%20in%20total.%20These%20tasks%20are%20derived%20from%20primary%20and%20secondary%0Aeducation%20curricula%2C%20encompassing%20nearly%20all%20everyday%20numerical%20understanding%0Aand%20processing%20scenarios%2C%20and%20the%20rules%20of%20these%20tasks%20are%20very%20simple%20and%0Aclear.%20Through%20the%20benchmark%2C%20we%20find%20that%20current%20LLMs%20fail%20frequently%20in%20many%0Aof%20the%20tasks.%20To%20study%20the%20problem%2C%20we%20train%20small%20models%20with%20existing%20and%0Apotential%20techniques%20for%20enhancing%20NUPA%20%28such%20as%20tokenizers%2C%20PEs%2C%20and%20number%0Aformats%29%2C%20comprehensively%20evaluating%20their%20effectiveness%20using%20our%20testbed.%20We%0Aalso%20finetune%20practical-scale%20LLMs%20on%20our%20proposed%20NUPA%20tasks%20and%20find%20that%201%29%0Anaive%20finetuning%20can%20improve%20NUPA%20a%20lot%20on%20many%20but%20not%20all%20tasks%2C%20and%202%29%0Asurprisingly%2C%20techniques%20designed%20to%20enhance%20NUPA%20prove%20ineffective%20for%0Afinetuning%20pretrained%20models.%20We%20further%20explore%20the%20impact%20of%20chain-of-thought%0Atechniques%20on%20NUPA.%20Our%20work%20provides%20a%20more%20detailed%20and%20comprehensive%0Aunderstanding%20of%20NUPA%20in%20LLMs.%20Our%20benchmark%20and%20code%20are%20released%20at%0Ahttps%3A//github.com/GraphPKU/number_cookbook.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03766v2&entry.124074799=Read"},
{"title": "Sparse-view Pose Estimation and Reconstruction via Analysis by\n  Generative Synthesis", "author": "Qitao Zhao and Shubham Tulsiani", "abstract": "  Inferring the 3D structure underlying a set of multi-view images typically\nrequires solving two co-dependent tasks -- accurate 3D reconstruction requires\nprecise camera poses, and predicting camera poses relies on (implicitly or\nexplicitly) modeling the underlying 3D. The classical framework of analysis by\nsynthesis casts this inference as a joint optimization seeking to explain the\nobserved pixels, and recent instantiations learn expressive 3D representations\n(e.g., Neural Fields) with gradient-descent-based pose refinement of initial\npose estimates. However, given a sparse set of observed views, the observations\nmay not provide sufficient direct evidence to obtain complete and accurate 3D.\nMoreover, large errors in pose estimation may not be easily corrected and can\nfurther degrade the inferred 3D. To allow robust 3D reconstruction and pose\nestimation in this challenging setup, we propose SparseAGS, a method that\nadapts this analysis-by-synthesis approach by: a) including\nnovel-view-synthesis-based generative priors in conjunction with photometric\nobjectives to improve the quality of the inferred 3D, and b) explicitly\nreasoning about outliers and using a discrete search with a continuous\noptimization-based strategy to correct them. We validate our framework across\nreal-world and synthetic datasets in combination with several off-the-shelf\npose estimation systems as initialization. We find that it significantly\nimproves the base systems' pose accuracy while yielding high-quality 3D\nreconstructions that outperform the results from current multi-view\nreconstruction baselines.\n", "link": "http://arxiv.org/abs/2412.03570v1", "date": "2024-12-04", "relevancy": 2.438, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6215}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6065}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse-view%20Pose%20Estimation%20and%20Reconstruction%20via%20Analysis%20by%0A%20%20Generative%20Synthesis&body=Title%3A%20Sparse-view%20Pose%20Estimation%20and%20Reconstruction%20via%20Analysis%20by%0A%20%20Generative%20Synthesis%0AAuthor%3A%20Qitao%20Zhao%20and%20Shubham%20Tulsiani%0AAbstract%3A%20%20%20Inferring%20the%203D%20structure%20underlying%20a%20set%20of%20multi-view%20images%20typically%0Arequires%20solving%20two%20co-dependent%20tasks%20--%20accurate%203D%20reconstruction%20requires%0Aprecise%20camera%20poses%2C%20and%20predicting%20camera%20poses%20relies%20on%20%28implicitly%20or%0Aexplicitly%29%20modeling%20the%20underlying%203D.%20The%20classical%20framework%20of%20analysis%20by%0Asynthesis%20casts%20this%20inference%20as%20a%20joint%20optimization%20seeking%20to%20explain%20the%0Aobserved%20pixels%2C%20and%20recent%20instantiations%20learn%20expressive%203D%20representations%0A%28e.g.%2C%20Neural%20Fields%29%20with%20gradient-descent-based%20pose%20refinement%20of%20initial%0Apose%20estimates.%20However%2C%20given%20a%20sparse%20set%20of%20observed%20views%2C%20the%20observations%0Amay%20not%20provide%20sufficient%20direct%20evidence%20to%20obtain%20complete%20and%20accurate%203D.%0AMoreover%2C%20large%20errors%20in%20pose%20estimation%20may%20not%20be%20easily%20corrected%20and%20can%0Afurther%20degrade%20the%20inferred%203D.%20To%20allow%20robust%203D%20reconstruction%20and%20pose%0Aestimation%20in%20this%20challenging%20setup%2C%20we%20propose%20SparseAGS%2C%20a%20method%20that%0Aadapts%20this%20analysis-by-synthesis%20approach%20by%3A%20a%29%20including%0Anovel-view-synthesis-based%20generative%20priors%20in%20conjunction%20with%20photometric%0Aobjectives%20to%20improve%20the%20quality%20of%20the%20inferred%203D%2C%20and%20b%29%20explicitly%0Areasoning%20about%20outliers%20and%20using%20a%20discrete%20search%20with%20a%20continuous%0Aoptimization-based%20strategy%20to%20correct%20them.%20We%20validate%20our%20framework%20across%0Areal-world%20and%20synthetic%20datasets%20in%20combination%20with%20several%20off-the-shelf%0Apose%20estimation%20systems%20as%20initialization.%20We%20find%20that%20it%20significantly%0Aimproves%20the%20base%20systems%27%20pose%20accuracy%20while%20yielding%20high-quality%203D%0Areconstructions%20that%20outperform%20the%20results%20from%20current%20multi-view%0Areconstruction%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03570v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse-view%2520Pose%2520Estimation%2520and%2520Reconstruction%2520via%2520Analysis%2520by%250A%2520%2520Generative%2520Synthesis%26entry.906535625%3DQitao%2520Zhao%2520and%2520Shubham%2520Tulsiani%26entry.1292438233%3D%2520%2520Inferring%2520the%25203D%2520structure%2520underlying%2520a%2520set%2520of%2520multi-view%2520images%2520typically%250Arequires%2520solving%2520two%2520co-dependent%2520tasks%2520--%2520accurate%25203D%2520reconstruction%2520requires%250Aprecise%2520camera%2520poses%252C%2520and%2520predicting%2520camera%2520poses%2520relies%2520on%2520%2528implicitly%2520or%250Aexplicitly%2529%2520modeling%2520the%2520underlying%25203D.%2520The%2520classical%2520framework%2520of%2520analysis%2520by%250Asynthesis%2520casts%2520this%2520inference%2520as%2520a%2520joint%2520optimization%2520seeking%2520to%2520explain%2520the%250Aobserved%2520pixels%252C%2520and%2520recent%2520instantiations%2520learn%2520expressive%25203D%2520representations%250A%2528e.g.%252C%2520Neural%2520Fields%2529%2520with%2520gradient-descent-based%2520pose%2520refinement%2520of%2520initial%250Apose%2520estimates.%2520However%252C%2520given%2520a%2520sparse%2520set%2520of%2520observed%2520views%252C%2520the%2520observations%250Amay%2520not%2520provide%2520sufficient%2520direct%2520evidence%2520to%2520obtain%2520complete%2520and%2520accurate%25203D.%250AMoreover%252C%2520large%2520errors%2520in%2520pose%2520estimation%2520may%2520not%2520be%2520easily%2520corrected%2520and%2520can%250Afurther%2520degrade%2520the%2520inferred%25203D.%2520To%2520allow%2520robust%25203D%2520reconstruction%2520and%2520pose%250Aestimation%2520in%2520this%2520challenging%2520setup%252C%2520we%2520propose%2520SparseAGS%252C%2520a%2520method%2520that%250Aadapts%2520this%2520analysis-by-synthesis%2520approach%2520by%253A%2520a%2529%2520including%250Anovel-view-synthesis-based%2520generative%2520priors%2520in%2520conjunction%2520with%2520photometric%250Aobjectives%2520to%2520improve%2520the%2520quality%2520of%2520the%2520inferred%25203D%252C%2520and%2520b%2529%2520explicitly%250Areasoning%2520about%2520outliers%2520and%2520using%2520a%2520discrete%2520search%2520with%2520a%2520continuous%250Aoptimization-based%2520strategy%2520to%2520correct%2520them.%2520We%2520validate%2520our%2520framework%2520across%250Areal-world%2520and%2520synthetic%2520datasets%2520in%2520combination%2520with%2520several%2520off-the-shelf%250Apose%2520estimation%2520systems%2520as%2520initialization.%2520We%2520find%2520that%2520it%2520significantly%250Aimproves%2520the%2520base%2520systems%2527%2520pose%2520accuracy%2520while%2520yielding%2520high-quality%25203D%250Areconstructions%2520that%2520outperform%2520the%2520results%2520from%2520current%2520multi-view%250Areconstruction%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03570v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse-view%20Pose%20Estimation%20and%20Reconstruction%20via%20Analysis%20by%0A%20%20Generative%20Synthesis&entry.906535625=Qitao%20Zhao%20and%20Shubham%20Tulsiani&entry.1292438233=%20%20Inferring%20the%203D%20structure%20underlying%20a%20set%20of%20multi-view%20images%20typically%0Arequires%20solving%20two%20co-dependent%20tasks%20--%20accurate%203D%20reconstruction%20requires%0Aprecise%20camera%20poses%2C%20and%20predicting%20camera%20poses%20relies%20on%20%28implicitly%20or%0Aexplicitly%29%20modeling%20the%20underlying%203D.%20The%20classical%20framework%20of%20analysis%20by%0Asynthesis%20casts%20this%20inference%20as%20a%20joint%20optimization%20seeking%20to%20explain%20the%0Aobserved%20pixels%2C%20and%20recent%20instantiations%20learn%20expressive%203D%20representations%0A%28e.g.%2C%20Neural%20Fields%29%20with%20gradient-descent-based%20pose%20refinement%20of%20initial%0Apose%20estimates.%20However%2C%20given%20a%20sparse%20set%20of%20observed%20views%2C%20the%20observations%0Amay%20not%20provide%20sufficient%20direct%20evidence%20to%20obtain%20complete%20and%20accurate%203D.%0AMoreover%2C%20large%20errors%20in%20pose%20estimation%20may%20not%20be%20easily%20corrected%20and%20can%0Afurther%20degrade%20the%20inferred%203D.%20To%20allow%20robust%203D%20reconstruction%20and%20pose%0Aestimation%20in%20this%20challenging%20setup%2C%20we%20propose%20SparseAGS%2C%20a%20method%20that%0Aadapts%20this%20analysis-by-synthesis%20approach%20by%3A%20a%29%20including%0Anovel-view-synthesis-based%20generative%20priors%20in%20conjunction%20with%20photometric%0Aobjectives%20to%20improve%20the%20quality%20of%20the%20inferred%203D%2C%20and%20b%29%20explicitly%0Areasoning%20about%20outliers%20and%20using%20a%20discrete%20search%20with%20a%20continuous%0Aoptimization-based%20strategy%20to%20correct%20them.%20We%20validate%20our%20framework%20across%0Areal-world%20and%20synthetic%20datasets%20in%20combination%20with%20several%20off-the-shelf%0Apose%20estimation%20systems%20as%20initialization.%20We%20find%20that%20it%20significantly%0Aimproves%20the%20base%20systems%27%20pose%20accuracy%20while%20yielding%20high-quality%203D%0Areconstructions%20that%20outperform%20the%20results%20from%20current%20multi-view%0Areconstruction%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03570v1&entry.124074799=Read"},
{"title": "LLM as a Complementary Optimizer to Gradient Descent: A Case Study in\n  Prompt Tuning", "author": "Zixian Guo and Ming Liu and Zhilong Ji and Jinfeng Bai and Yiwen Guo and Wangmeng Zuo", "abstract": "  Mastering a skill generally relies on both hands-on experience from doers and\ninsightful, high-level guidance by mentors. Will this strategy also work well\nfor solving complex non-convex optimization problems? Here, a common\ngradient-based optimizer acts like a disciplined doer, making locally optimal\nupdates at each step. Large Language Models (LLMs) can also search for better\nsolutions by inferring from natural language instructions, akin to a high-level\nmentor. In this paper, we show that these two participators are complementary\nto each other and can effectively collaborate as a combined optimization\nframework. The collaborative optimization is achieved by alternating between\nthe gradient-based and LLM-based optimizers. We instruct LLMs to generate\npossibly improved solutions by taking parameter trajectories recorded during\nthe previous stage of gradient-based optimization into account. Inferred\nresults of LLMs are used as restarting points for the next stage of gradient\noptimization. We verify the effectiveness of this optimization framework on\nprompt tuning. By leveraging both the locally rigorous gradient-based optimizer\nand the high-level deductive LLM-based optimizer, the combined optimization\nmethod consistently yields improvements over competitive baselines on a variety\nof tasks. Our results demonstrate the synergistic effect of conventional\ngradient-based optimization and the inference ability of LLMs. The code is\nreleased at https://github.com/guozix/LLM-catalyst.\n", "link": "http://arxiv.org/abs/2405.19732v4", "date": "2024-12-04", "relevancy": 2.4289, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4902}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4853}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20as%20a%20Complementary%20Optimizer%20to%20Gradient%20Descent%3A%20A%20Case%20Study%20in%0A%20%20Prompt%20Tuning&body=Title%3A%20LLM%20as%20a%20Complementary%20Optimizer%20to%20Gradient%20Descent%3A%20A%20Case%20Study%20in%0A%20%20Prompt%20Tuning%0AAuthor%3A%20Zixian%20Guo%20and%20Ming%20Liu%20and%20Zhilong%20Ji%20and%20Jinfeng%20Bai%20and%20Yiwen%20Guo%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20Mastering%20a%20skill%20generally%20relies%20on%20both%20hands-on%20experience%20from%20doers%20and%0Ainsightful%2C%20high-level%20guidance%20by%20mentors.%20Will%20this%20strategy%20also%20work%20well%0Afor%20solving%20complex%20non-convex%20optimization%20problems%3F%20Here%2C%20a%20common%0Agradient-based%20optimizer%20acts%20like%20a%20disciplined%20doer%2C%20making%20locally%20optimal%0Aupdates%20at%20each%20step.%20Large%20Language%20Models%20%28LLMs%29%20can%20also%20search%20for%20better%0Asolutions%20by%20inferring%20from%20natural%20language%20instructions%2C%20akin%20to%20a%20high-level%0Amentor.%20In%20this%20paper%2C%20we%20show%20that%20these%20two%20participators%20are%20complementary%0Ato%20each%20other%20and%20can%20effectively%20collaborate%20as%20a%20combined%20optimization%0Aframework.%20The%20collaborative%20optimization%20is%20achieved%20by%20alternating%20between%0Athe%20gradient-based%20and%20LLM-based%20optimizers.%20We%20instruct%20LLMs%20to%20generate%0Apossibly%20improved%20solutions%20by%20taking%20parameter%20trajectories%20recorded%20during%0Athe%20previous%20stage%20of%20gradient-based%20optimization%20into%20account.%20Inferred%0Aresults%20of%20LLMs%20are%20used%20as%20restarting%20points%20for%20the%20next%20stage%20of%20gradient%0Aoptimization.%20We%20verify%20the%20effectiveness%20of%20this%20optimization%20framework%20on%0Aprompt%20tuning.%20By%20leveraging%20both%20the%20locally%20rigorous%20gradient-based%20optimizer%0Aand%20the%20high-level%20deductive%20LLM-based%20optimizer%2C%20the%20combined%20optimization%0Amethod%20consistently%20yields%20improvements%20over%20competitive%20baselines%20on%20a%20variety%0Aof%20tasks.%20Our%20results%20demonstrate%20the%20synergistic%20effect%20of%20conventional%0Agradient-based%20optimization%20and%20the%20inference%20ability%20of%20LLMs.%20The%20code%20is%0Areleased%20at%20https%3A//github.com/guozix/LLM-catalyst.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19732v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520as%2520a%2520Complementary%2520Optimizer%2520to%2520Gradient%2520Descent%253A%2520A%2520Case%2520Study%2520in%250A%2520%2520Prompt%2520Tuning%26entry.906535625%3DZixian%2520Guo%2520and%2520Ming%2520Liu%2520and%2520Zhilong%2520Ji%2520and%2520Jinfeng%2520Bai%2520and%2520Yiwen%2520Guo%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520Mastering%2520a%2520skill%2520generally%2520relies%2520on%2520both%2520hands-on%2520experience%2520from%2520doers%2520and%250Ainsightful%252C%2520high-level%2520guidance%2520by%2520mentors.%2520Will%2520this%2520strategy%2520also%2520work%2520well%250Afor%2520solving%2520complex%2520non-convex%2520optimization%2520problems%253F%2520Here%252C%2520a%2520common%250Agradient-based%2520optimizer%2520acts%2520like%2520a%2520disciplined%2520doer%252C%2520making%2520locally%2520optimal%250Aupdates%2520at%2520each%2520step.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520also%2520search%2520for%2520better%250Asolutions%2520by%2520inferring%2520from%2520natural%2520language%2520instructions%252C%2520akin%2520to%2520a%2520high-level%250Amentor.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520these%2520two%2520participators%2520are%2520complementary%250Ato%2520each%2520other%2520and%2520can%2520effectively%2520collaborate%2520as%2520a%2520combined%2520optimization%250Aframework.%2520The%2520collaborative%2520optimization%2520is%2520achieved%2520by%2520alternating%2520between%250Athe%2520gradient-based%2520and%2520LLM-based%2520optimizers.%2520We%2520instruct%2520LLMs%2520to%2520generate%250Apossibly%2520improved%2520solutions%2520by%2520taking%2520parameter%2520trajectories%2520recorded%2520during%250Athe%2520previous%2520stage%2520of%2520gradient-based%2520optimization%2520into%2520account.%2520Inferred%250Aresults%2520of%2520LLMs%2520are%2520used%2520as%2520restarting%2520points%2520for%2520the%2520next%2520stage%2520of%2520gradient%250Aoptimization.%2520We%2520verify%2520the%2520effectiveness%2520of%2520this%2520optimization%2520framework%2520on%250Aprompt%2520tuning.%2520By%2520leveraging%2520both%2520the%2520locally%2520rigorous%2520gradient-based%2520optimizer%250Aand%2520the%2520high-level%2520deductive%2520LLM-based%2520optimizer%252C%2520the%2520combined%2520optimization%250Amethod%2520consistently%2520yields%2520improvements%2520over%2520competitive%2520baselines%2520on%2520a%2520variety%250Aof%2520tasks.%2520Our%2520results%2520demonstrate%2520the%2520synergistic%2520effect%2520of%2520conventional%250Agradient-based%2520optimization%2520and%2520the%2520inference%2520ability%2520of%2520LLMs.%2520The%2520code%2520is%250Areleased%2520at%2520https%253A//github.com/guozix/LLM-catalyst.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19732v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20as%20a%20Complementary%20Optimizer%20to%20Gradient%20Descent%3A%20A%20Case%20Study%20in%0A%20%20Prompt%20Tuning&entry.906535625=Zixian%20Guo%20and%20Ming%20Liu%20and%20Zhilong%20Ji%20and%20Jinfeng%20Bai%20and%20Yiwen%20Guo%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20Mastering%20a%20skill%20generally%20relies%20on%20both%20hands-on%20experience%20from%20doers%20and%0Ainsightful%2C%20high-level%20guidance%20by%20mentors.%20Will%20this%20strategy%20also%20work%20well%0Afor%20solving%20complex%20non-convex%20optimization%20problems%3F%20Here%2C%20a%20common%0Agradient-based%20optimizer%20acts%20like%20a%20disciplined%20doer%2C%20making%20locally%20optimal%0Aupdates%20at%20each%20step.%20Large%20Language%20Models%20%28LLMs%29%20can%20also%20search%20for%20better%0Asolutions%20by%20inferring%20from%20natural%20language%20instructions%2C%20akin%20to%20a%20high-level%0Amentor.%20In%20this%20paper%2C%20we%20show%20that%20these%20two%20participators%20are%20complementary%0Ato%20each%20other%20and%20can%20effectively%20collaborate%20as%20a%20combined%20optimization%0Aframework.%20The%20collaborative%20optimization%20is%20achieved%20by%20alternating%20between%0Athe%20gradient-based%20and%20LLM-based%20optimizers.%20We%20instruct%20LLMs%20to%20generate%0Apossibly%20improved%20solutions%20by%20taking%20parameter%20trajectories%20recorded%20during%0Athe%20previous%20stage%20of%20gradient-based%20optimization%20into%20account.%20Inferred%0Aresults%20of%20LLMs%20are%20used%20as%20restarting%20points%20for%20the%20next%20stage%20of%20gradient%0Aoptimization.%20We%20verify%20the%20effectiveness%20of%20this%20optimization%20framework%20on%0Aprompt%20tuning.%20By%20leveraging%20both%20the%20locally%20rigorous%20gradient-based%20optimizer%0Aand%20the%20high-level%20deductive%20LLM-based%20optimizer%2C%20the%20combined%20optimization%0Amethod%20consistently%20yields%20improvements%20over%20competitive%20baselines%20on%20a%20variety%0Aof%20tasks.%20Our%20results%20demonstrate%20the%20synergistic%20effect%20of%20conventional%0Agradient-based%20optimization%20and%20the%20inference%20ability%20of%20LLMs.%20The%20code%20is%0Areleased%20at%20https%3A//github.com/guozix/LLM-catalyst.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19732v4&entry.124074799=Read"},
{"title": "StructChart: On the Schema, Metric, and Augmentation for Visual Chart\n  Understanding", "author": "Renqiu Xia and Haoyang Peng and Hancheng Ye and Mingsheng Li and Xiangchao Yan and Peng Ye and Botian Shi and Yu Qiao and Junchi Yan and Bo Zhang", "abstract": "  Charts are common in literature across various scientific fields, conveying\nrich information easily accessible to readers. Current chart-related tasks\nfocus on either chart perception that extracts information from the visual\ncharts, or chart reasoning given the extracted data, e.g. in a tabular form. In\nthis paper, we introduce StructChart, a novel framework that leverages\nStructured Triplet Representations (STR) to achieve a unified and\nlabel-efficient approach to chart perception and reasoning tasks, which is\ngenerally applicable to different downstream tasks, beyond the\nquestion-answering task as specifically studied in peer works. Specifically,\nStructChart first reformulates the chart data from the tubular form (linearized\nCSV) to STR, which can friendlily reduce the task gap between chart perception\nand reasoning. We then propose a Structuring Chart-oriented Representation\nMetric (SCRM) to quantitatively evaluate the chart perception task performance.\nTo augment the training, we further explore the potential of Large Language\nModels (LLMs) to enhance the diversity in both chart visual style and\nstatistical information. Extensive experiments on various chart-related tasks\ndemonstrate the effectiveness and potential of a unified chart\nperception-reasoning paradigm to push the frontier of chart understanding.\n", "link": "http://arxiv.org/abs/2309.11268v5", "date": "2024-12-04", "relevancy": 2.422, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StructChart%3A%20On%20the%20Schema%2C%20Metric%2C%20and%20Augmentation%20for%20Visual%20Chart%0A%20%20Understanding&body=Title%3A%20StructChart%3A%20On%20the%20Schema%2C%20Metric%2C%20and%20Augmentation%20for%20Visual%20Chart%0A%20%20Understanding%0AAuthor%3A%20Renqiu%20Xia%20and%20Haoyang%20Peng%20and%20Hancheng%20Ye%20and%20Mingsheng%20Li%20and%20Xiangchao%20Yan%20and%20Peng%20Ye%20and%20Botian%20Shi%20and%20Yu%20Qiao%20and%20Junchi%20Yan%20and%20Bo%20Zhang%0AAbstract%3A%20%20%20Charts%20are%20common%20in%20literature%20across%20various%20scientific%20fields%2C%20conveying%0Arich%20information%20easily%20accessible%20to%20readers.%20Current%20chart-related%20tasks%0Afocus%20on%20either%20chart%20perception%20that%20extracts%20information%20from%20the%20visual%0Acharts%2C%20or%20chart%20reasoning%20given%20the%20extracted%20data%2C%20e.g.%20in%20a%20tabular%20form.%20In%0Athis%20paper%2C%20we%20introduce%20StructChart%2C%20a%20novel%20framework%20that%20leverages%0AStructured%20Triplet%20Representations%20%28STR%29%20to%20achieve%20a%20unified%20and%0Alabel-efficient%20approach%20to%20chart%20perception%20and%20reasoning%20tasks%2C%20which%20is%0Agenerally%20applicable%20to%20different%20downstream%20tasks%2C%20beyond%20the%0Aquestion-answering%20task%20as%20specifically%20studied%20in%20peer%20works.%20Specifically%2C%0AStructChart%20first%20reformulates%20the%20chart%20data%20from%20the%20tubular%20form%20%28linearized%0ACSV%29%20to%20STR%2C%20which%20can%20friendlily%20reduce%20the%20task%20gap%20between%20chart%20perception%0Aand%20reasoning.%20We%20then%20propose%20a%20Structuring%20Chart-oriented%20Representation%0AMetric%20%28SCRM%29%20to%20quantitatively%20evaluate%20the%20chart%20perception%20task%20performance.%0ATo%20augment%20the%20training%2C%20we%20further%20explore%20the%20potential%20of%20Large%20Language%0AModels%20%28LLMs%29%20to%20enhance%20the%20diversity%20in%20both%20chart%20visual%20style%20and%0Astatistical%20information.%20Extensive%20experiments%20on%20various%20chart-related%20tasks%0Ademonstrate%20the%20effectiveness%20and%20potential%20of%20a%20unified%20chart%0Aperception-reasoning%20paradigm%20to%20push%20the%20frontier%20of%20chart%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11268v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructChart%253A%2520On%2520the%2520Schema%252C%2520Metric%252C%2520and%2520Augmentation%2520for%2520Visual%2520Chart%250A%2520%2520Understanding%26entry.906535625%3DRenqiu%2520Xia%2520and%2520Haoyang%2520Peng%2520and%2520Hancheng%2520Ye%2520and%2520Mingsheng%2520Li%2520and%2520Xiangchao%2520Yan%2520and%2520Peng%2520Ye%2520and%2520Botian%2520Shi%2520and%2520Yu%2520Qiao%2520and%2520Junchi%2520Yan%2520and%2520Bo%2520Zhang%26entry.1292438233%3D%2520%2520Charts%2520are%2520common%2520in%2520literature%2520across%2520various%2520scientific%2520fields%252C%2520conveying%250Arich%2520information%2520easily%2520accessible%2520to%2520readers.%2520Current%2520chart-related%2520tasks%250Afocus%2520on%2520either%2520chart%2520perception%2520that%2520extracts%2520information%2520from%2520the%2520visual%250Acharts%252C%2520or%2520chart%2520reasoning%2520given%2520the%2520extracted%2520data%252C%2520e.g.%2520in%2520a%2520tabular%2520form.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520StructChart%252C%2520a%2520novel%2520framework%2520that%2520leverages%250AStructured%2520Triplet%2520Representations%2520%2528STR%2529%2520to%2520achieve%2520a%2520unified%2520and%250Alabel-efficient%2520approach%2520to%2520chart%2520perception%2520and%2520reasoning%2520tasks%252C%2520which%2520is%250Agenerally%2520applicable%2520to%2520different%2520downstream%2520tasks%252C%2520beyond%2520the%250Aquestion-answering%2520task%2520as%2520specifically%2520studied%2520in%2520peer%2520works.%2520Specifically%252C%250AStructChart%2520first%2520reformulates%2520the%2520chart%2520data%2520from%2520the%2520tubular%2520form%2520%2528linearized%250ACSV%2529%2520to%2520STR%252C%2520which%2520can%2520friendlily%2520reduce%2520the%2520task%2520gap%2520between%2520chart%2520perception%250Aand%2520reasoning.%2520We%2520then%2520propose%2520a%2520Structuring%2520Chart-oriented%2520Representation%250AMetric%2520%2528SCRM%2529%2520to%2520quantitatively%2520evaluate%2520the%2520chart%2520perception%2520task%2520performance.%250ATo%2520augment%2520the%2520training%252C%2520we%2520further%2520explore%2520the%2520potential%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520to%2520enhance%2520the%2520diversity%2520in%2520both%2520chart%2520visual%2520style%2520and%250Astatistical%2520information.%2520Extensive%2520experiments%2520on%2520various%2520chart-related%2520tasks%250Ademonstrate%2520the%2520effectiveness%2520and%2520potential%2520of%2520a%2520unified%2520chart%250Aperception-reasoning%2520paradigm%2520to%2520push%2520the%2520frontier%2520of%2520chart%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.11268v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StructChart%3A%20On%20the%20Schema%2C%20Metric%2C%20and%20Augmentation%20for%20Visual%20Chart%0A%20%20Understanding&entry.906535625=Renqiu%20Xia%20and%20Haoyang%20Peng%20and%20Hancheng%20Ye%20and%20Mingsheng%20Li%20and%20Xiangchao%20Yan%20and%20Peng%20Ye%20and%20Botian%20Shi%20and%20Yu%20Qiao%20and%20Junchi%20Yan%20and%20Bo%20Zhang&entry.1292438233=%20%20Charts%20are%20common%20in%20literature%20across%20various%20scientific%20fields%2C%20conveying%0Arich%20information%20easily%20accessible%20to%20readers.%20Current%20chart-related%20tasks%0Afocus%20on%20either%20chart%20perception%20that%20extracts%20information%20from%20the%20visual%0Acharts%2C%20or%20chart%20reasoning%20given%20the%20extracted%20data%2C%20e.g.%20in%20a%20tabular%20form.%20In%0Athis%20paper%2C%20we%20introduce%20StructChart%2C%20a%20novel%20framework%20that%20leverages%0AStructured%20Triplet%20Representations%20%28STR%29%20to%20achieve%20a%20unified%20and%0Alabel-efficient%20approach%20to%20chart%20perception%20and%20reasoning%20tasks%2C%20which%20is%0Agenerally%20applicable%20to%20different%20downstream%20tasks%2C%20beyond%20the%0Aquestion-answering%20task%20as%20specifically%20studied%20in%20peer%20works.%20Specifically%2C%0AStructChart%20first%20reformulates%20the%20chart%20data%20from%20the%20tubular%20form%20%28linearized%0ACSV%29%20to%20STR%2C%20which%20can%20friendlily%20reduce%20the%20task%20gap%20between%20chart%20perception%0Aand%20reasoning.%20We%20then%20propose%20a%20Structuring%20Chart-oriented%20Representation%0AMetric%20%28SCRM%29%20to%20quantitatively%20evaluate%20the%20chart%20perception%20task%20performance.%0ATo%20augment%20the%20training%2C%20we%20further%20explore%20the%20potential%20of%20Large%20Language%0AModels%20%28LLMs%29%20to%20enhance%20the%20diversity%20in%20both%20chart%20visual%20style%20and%0Astatistical%20information.%20Extensive%20experiments%20on%20various%20chart-related%20tasks%0Ademonstrate%20the%20effectiveness%20and%20potential%20of%20a%20unified%20chart%0Aperception-reasoning%20paradigm%20to%20push%20the%20frontier%20of%20chart%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11268v5&entry.124074799=Read"},
{"title": "DIVE: Taming DINO for Subject-Driven Video Editing", "author": "Yi Huang and Wei Xiong and He Zhang and Chaoqi Chen and Jianzhuang Liu and Mingfu Yan and Shifeng Chen", "abstract": "  Building on the success of diffusion models in image generation and editing,\nvideo editing has recently gained substantial attention. However, maintaining\ntemporal consistency and motion alignment still remains challenging. To address\nthese issues, this paper proposes DINO-guided Video Editing (DIVE), a framework\ndesigned to facilitate subject-driven editing in source videos conditioned on\neither target text prompts or reference images with specific identities. The\ncore of DIVE lies in leveraging the powerful semantic features extracted from a\npretrained DINOv2 model as implicit correspondences to guide the editing\nprocess. Specifically, to ensure temporal motion consistency, DIVE employs DINO\nfeatures to align with the motion trajectory of the source video. Extensive\nexperiments on diverse real-world videos demonstrate that our framework can\nachieve high-quality editing results with robust motion consistency,\nhighlighting the potential of DINO to contribute to video editing. For precise\nsubject editing, DIVE incorporates the DINO features of reference images into a\npretrained text-to-image model to learn Low-Rank Adaptations (LoRAs),\neffectively registering the target subject's identity. Project page:\nhttps://dino-video-editing.github.io\n", "link": "http://arxiv.org/abs/2412.03347v1", "date": "2024-12-04", "relevancy": 2.3947, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6126}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIVE%3A%20Taming%20DINO%20for%20Subject-Driven%20Video%20Editing&body=Title%3A%20DIVE%3A%20Taming%20DINO%20for%20Subject-Driven%20Video%20Editing%0AAuthor%3A%20Yi%20Huang%20and%20Wei%20Xiong%20and%20He%20Zhang%20and%20Chaoqi%20Chen%20and%20Jianzhuang%20Liu%20and%20Mingfu%20Yan%20and%20Shifeng%20Chen%0AAbstract%3A%20%20%20Building%20on%20the%20success%20of%20diffusion%20models%20in%20image%20generation%20and%20editing%2C%0Avideo%20editing%20has%20recently%20gained%20substantial%20attention.%20However%2C%20maintaining%0Atemporal%20consistency%20and%20motion%20alignment%20still%20remains%20challenging.%20To%20address%0Athese%20issues%2C%20this%20paper%20proposes%20DINO-guided%20Video%20Editing%20%28DIVE%29%2C%20a%20framework%0Adesigned%20to%20facilitate%20subject-driven%20editing%20in%20source%20videos%20conditioned%20on%0Aeither%20target%20text%20prompts%20or%20reference%20images%20with%20specific%20identities.%20The%0Acore%20of%20DIVE%20lies%20in%20leveraging%20the%20powerful%20semantic%20features%20extracted%20from%20a%0Apretrained%20DINOv2%20model%20as%20implicit%20correspondences%20to%20guide%20the%20editing%0Aprocess.%20Specifically%2C%20to%20ensure%20temporal%20motion%20consistency%2C%20DIVE%20employs%20DINO%0Afeatures%20to%20align%20with%20the%20motion%20trajectory%20of%20the%20source%20video.%20Extensive%0Aexperiments%20on%20diverse%20real-world%20videos%20demonstrate%20that%20our%20framework%20can%0Aachieve%20high-quality%20editing%20results%20with%20robust%20motion%20consistency%2C%0Ahighlighting%20the%20potential%20of%20DINO%20to%20contribute%20to%20video%20editing.%20For%20precise%0Asubject%20editing%2C%20DIVE%20incorporates%20the%20DINO%20features%20of%20reference%20images%20into%20a%0Apretrained%20text-to-image%20model%20to%20learn%20Low-Rank%20Adaptations%20%28LoRAs%29%2C%0Aeffectively%20registering%20the%20target%20subject%27s%20identity.%20Project%20page%3A%0Ahttps%3A//dino-video-editing.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIVE%253A%2520Taming%2520DINO%2520for%2520Subject-Driven%2520Video%2520Editing%26entry.906535625%3DYi%2520Huang%2520and%2520Wei%2520Xiong%2520and%2520He%2520Zhang%2520and%2520Chaoqi%2520Chen%2520and%2520Jianzhuang%2520Liu%2520and%2520Mingfu%2520Yan%2520and%2520Shifeng%2520Chen%26entry.1292438233%3D%2520%2520Building%2520on%2520the%2520success%2520of%2520diffusion%2520models%2520in%2520image%2520generation%2520and%2520editing%252C%250Avideo%2520editing%2520has%2520recently%2520gained%2520substantial%2520attention.%2520However%252C%2520maintaining%250Atemporal%2520consistency%2520and%2520motion%2520alignment%2520still%2520remains%2520challenging.%2520To%2520address%250Athese%2520issues%252C%2520this%2520paper%2520proposes%2520DINO-guided%2520Video%2520Editing%2520%2528DIVE%2529%252C%2520a%2520framework%250Adesigned%2520to%2520facilitate%2520subject-driven%2520editing%2520in%2520source%2520videos%2520conditioned%2520on%250Aeither%2520target%2520text%2520prompts%2520or%2520reference%2520images%2520with%2520specific%2520identities.%2520The%250Acore%2520of%2520DIVE%2520lies%2520in%2520leveraging%2520the%2520powerful%2520semantic%2520features%2520extracted%2520from%2520a%250Apretrained%2520DINOv2%2520model%2520as%2520implicit%2520correspondences%2520to%2520guide%2520the%2520editing%250Aprocess.%2520Specifically%252C%2520to%2520ensure%2520temporal%2520motion%2520consistency%252C%2520DIVE%2520employs%2520DINO%250Afeatures%2520to%2520align%2520with%2520the%2520motion%2520trajectory%2520of%2520the%2520source%2520video.%2520Extensive%250Aexperiments%2520on%2520diverse%2520real-world%2520videos%2520demonstrate%2520that%2520our%2520framework%2520can%250Aachieve%2520high-quality%2520editing%2520results%2520with%2520robust%2520motion%2520consistency%252C%250Ahighlighting%2520the%2520potential%2520of%2520DINO%2520to%2520contribute%2520to%2520video%2520editing.%2520For%2520precise%250Asubject%2520editing%252C%2520DIVE%2520incorporates%2520the%2520DINO%2520features%2520of%2520reference%2520images%2520into%2520a%250Apretrained%2520text-to-image%2520model%2520to%2520learn%2520Low-Rank%2520Adaptations%2520%2528LoRAs%2529%252C%250Aeffectively%2520registering%2520the%2520target%2520subject%2527s%2520identity.%2520Project%2520page%253A%250Ahttps%253A//dino-video-editing.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIVE%3A%20Taming%20DINO%20for%20Subject-Driven%20Video%20Editing&entry.906535625=Yi%20Huang%20and%20Wei%20Xiong%20and%20He%20Zhang%20and%20Chaoqi%20Chen%20and%20Jianzhuang%20Liu%20and%20Mingfu%20Yan%20and%20Shifeng%20Chen&entry.1292438233=%20%20Building%20on%20the%20success%20of%20diffusion%20models%20in%20image%20generation%20and%20editing%2C%0Avideo%20editing%20has%20recently%20gained%20substantial%20attention.%20However%2C%20maintaining%0Atemporal%20consistency%20and%20motion%20alignment%20still%20remains%20challenging.%20To%20address%0Athese%20issues%2C%20this%20paper%20proposes%20DINO-guided%20Video%20Editing%20%28DIVE%29%2C%20a%20framework%0Adesigned%20to%20facilitate%20subject-driven%20editing%20in%20source%20videos%20conditioned%20on%0Aeither%20target%20text%20prompts%20or%20reference%20images%20with%20specific%20identities.%20The%0Acore%20of%20DIVE%20lies%20in%20leveraging%20the%20powerful%20semantic%20features%20extracted%20from%20a%0Apretrained%20DINOv2%20model%20as%20implicit%20correspondences%20to%20guide%20the%20editing%0Aprocess.%20Specifically%2C%20to%20ensure%20temporal%20motion%20consistency%2C%20DIVE%20employs%20DINO%0Afeatures%20to%20align%20with%20the%20motion%20trajectory%20of%20the%20source%20video.%20Extensive%0Aexperiments%20on%20diverse%20real-world%20videos%20demonstrate%20that%20our%20framework%20can%0Aachieve%20high-quality%20editing%20results%20with%20robust%20motion%20consistency%2C%0Ahighlighting%20the%20potential%20of%20DINO%20to%20contribute%20to%20video%20editing.%20For%20precise%0Asubject%20editing%2C%20DIVE%20incorporates%20the%20DINO%20features%20of%20reference%20images%20into%20a%0Apretrained%20text-to-image%20model%20to%20learn%20Low-Rank%20Adaptations%20%28LoRAs%29%2C%0Aeffectively%20registering%20the%20target%20subject%27s%20identity.%20Project%20page%3A%0Ahttps%3A//dino-video-editing.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03347v1&entry.124074799=Read"},
{"title": "Intent-driven In-context Learning for Few-shot Dialogue State Tracking", "author": "Zihao Yi and Zhe Xu and Ying Shen", "abstract": "  Dialogue state tracking (DST) plays an essential role in task-oriented\ndialogue systems. However, user's input may contain implicit information,\nposing significant challenges for DST tasks. Additionally, DST data includes\ncomplex information, which not only contains a large amount of noise unrelated\nto the current turn, but also makes constructing DST datasets expensive. To\naddress these challenges, we introduce Intent-driven In-context Learning for\nFew-shot DST (IDIC-DST). By extracting user's intent, we propose an\nIntent-driven Dialogue Information Augmentation module to augment the dialogue\ninformation, which can track dialogue states more effectively. Moreover, we\nmask noisy information from DST data and rewrite user's input in the\nIntent-driven Examples Retrieval module, where we retrieve similar examples. We\nthen utilize a pre-trained large language model to update the dialogue state\nusing the augmented dialogue information and examples. Experimental results\ndemonstrate that IDIC-DST achieves state-of-the-art performance in few-shot\nsettings on MultiWOZ 2.1 and MultiWOZ 2.4 datasets.\n", "link": "http://arxiv.org/abs/2412.03270v1", "date": "2024-12-04", "relevancy": 2.3934, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4931}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4731}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intent-driven%20In-context%20Learning%20for%20Few-shot%20Dialogue%20State%20Tracking&body=Title%3A%20Intent-driven%20In-context%20Learning%20for%20Few-shot%20Dialogue%20State%20Tracking%0AAuthor%3A%20Zihao%20Yi%20and%20Zhe%20Xu%20and%20Ying%20Shen%0AAbstract%3A%20%20%20Dialogue%20state%20tracking%20%28DST%29%20plays%20an%20essential%20role%20in%20task-oriented%0Adialogue%20systems.%20However%2C%20user%27s%20input%20may%20contain%20implicit%20information%2C%0Aposing%20significant%20challenges%20for%20DST%20tasks.%20Additionally%2C%20DST%20data%20includes%0Acomplex%20information%2C%20which%20not%20only%20contains%20a%20large%20amount%20of%20noise%20unrelated%0Ato%20the%20current%20turn%2C%20but%20also%20makes%20constructing%20DST%20datasets%20expensive.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20Intent-driven%20In-context%20Learning%20for%0AFew-shot%20DST%20%28IDIC-DST%29.%20By%20extracting%20user%27s%20intent%2C%20we%20propose%20an%0AIntent-driven%20Dialogue%20Information%20Augmentation%20module%20to%20augment%20the%20dialogue%0Ainformation%2C%20which%20can%20track%20dialogue%20states%20more%20effectively.%20Moreover%2C%20we%0Amask%20noisy%20information%20from%20DST%20data%20and%20rewrite%20user%27s%20input%20in%20the%0AIntent-driven%20Examples%20Retrieval%20module%2C%20where%20we%20retrieve%20similar%20examples.%20We%0Athen%20utilize%20a%20pre-trained%20large%20language%20model%20to%20update%20the%20dialogue%20state%0Ausing%20the%20augmented%20dialogue%20information%20and%20examples.%20Experimental%20results%0Ademonstrate%20that%20IDIC-DST%20achieves%20state-of-the-art%20performance%20in%20few-shot%0Asettings%20on%20MultiWOZ%202.1%20and%20MultiWOZ%202.4%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntent-driven%2520In-context%2520Learning%2520for%2520Few-shot%2520Dialogue%2520State%2520Tracking%26entry.906535625%3DZihao%2520Yi%2520and%2520Zhe%2520Xu%2520and%2520Ying%2520Shen%26entry.1292438233%3D%2520%2520Dialogue%2520state%2520tracking%2520%2528DST%2529%2520plays%2520an%2520essential%2520role%2520in%2520task-oriented%250Adialogue%2520systems.%2520However%252C%2520user%2527s%2520input%2520may%2520contain%2520implicit%2520information%252C%250Aposing%2520significant%2520challenges%2520for%2520DST%2520tasks.%2520Additionally%252C%2520DST%2520data%2520includes%250Acomplex%2520information%252C%2520which%2520not%2520only%2520contains%2520a%2520large%2520amount%2520of%2520noise%2520unrelated%250Ato%2520the%2520current%2520turn%252C%2520but%2520also%2520makes%2520constructing%2520DST%2520datasets%2520expensive.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520Intent-driven%2520In-context%2520Learning%2520for%250AFew-shot%2520DST%2520%2528IDIC-DST%2529.%2520By%2520extracting%2520user%2527s%2520intent%252C%2520we%2520propose%2520an%250AIntent-driven%2520Dialogue%2520Information%2520Augmentation%2520module%2520to%2520augment%2520the%2520dialogue%250Ainformation%252C%2520which%2520can%2520track%2520dialogue%2520states%2520more%2520effectively.%2520Moreover%252C%2520we%250Amask%2520noisy%2520information%2520from%2520DST%2520data%2520and%2520rewrite%2520user%2527s%2520input%2520in%2520the%250AIntent-driven%2520Examples%2520Retrieval%2520module%252C%2520where%2520we%2520retrieve%2520similar%2520examples.%2520We%250Athen%2520utilize%2520a%2520pre-trained%2520large%2520language%2520model%2520to%2520update%2520the%2520dialogue%2520state%250Ausing%2520the%2520augmented%2520dialogue%2520information%2520and%2520examples.%2520Experimental%2520results%250Ademonstrate%2520that%2520IDIC-DST%2520achieves%2520state-of-the-art%2520performance%2520in%2520few-shot%250Asettings%2520on%2520MultiWOZ%25202.1%2520and%2520MultiWOZ%25202.4%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intent-driven%20In-context%20Learning%20for%20Few-shot%20Dialogue%20State%20Tracking&entry.906535625=Zihao%20Yi%20and%20Zhe%20Xu%20and%20Ying%20Shen&entry.1292438233=%20%20Dialogue%20state%20tracking%20%28DST%29%20plays%20an%20essential%20role%20in%20task-oriented%0Adialogue%20systems.%20However%2C%20user%27s%20input%20may%20contain%20implicit%20information%2C%0Aposing%20significant%20challenges%20for%20DST%20tasks.%20Additionally%2C%20DST%20data%20includes%0Acomplex%20information%2C%20which%20not%20only%20contains%20a%20large%20amount%20of%20noise%20unrelated%0Ato%20the%20current%20turn%2C%20but%20also%20makes%20constructing%20DST%20datasets%20expensive.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20Intent-driven%20In-context%20Learning%20for%0AFew-shot%20DST%20%28IDIC-DST%29.%20By%20extracting%20user%27s%20intent%2C%20we%20propose%20an%0AIntent-driven%20Dialogue%20Information%20Augmentation%20module%20to%20augment%20the%20dialogue%0Ainformation%2C%20which%20can%20track%20dialogue%20states%20more%20effectively.%20Moreover%2C%20we%0Amask%20noisy%20information%20from%20DST%20data%20and%20rewrite%20user%27s%20input%20in%20the%0AIntent-driven%20Examples%20Retrieval%20module%2C%20where%20we%20retrieve%20similar%20examples.%20We%0Athen%20utilize%20a%20pre-trained%20large%20language%20model%20to%20update%20the%20dialogue%20state%0Ausing%20the%20augmented%20dialogue%20information%20and%20examples.%20Experimental%20results%0Ademonstrate%20that%20IDIC-DST%20achieves%20state-of-the-art%20performance%20in%20few-shot%0Asettings%20on%20MultiWOZ%202.1%20and%20MultiWOZ%202.4%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03270v1&entry.124074799=Read"},
{"title": "Semi-Supervised Transfer Boosting (SS-TrBoosting)", "author": "Lingfei Deng and Changming Zhao and Zhenbang Du and Kun Xia and Dongrui Wu", "abstract": "  Semi-supervised domain adaptation (SSDA) aims at training a high-performance\nmodel for a target domain using few labeled target data, many unlabeled target\ndata, and plenty of auxiliary data from a source domain. Previous works in SSDA\nmainly focused on learning transferable representations across domains.\nHowever, it is difficult to find a feature space where the source and target\ndomains share the same conditional probability distribution. Additionally,\nthere is no flexible and effective strategy extending existing unsupervised\ndomain adaptation (UDA) approaches to SSDA settings. In order to solve the\nabove two challenges, we propose a novel fine-tuning framework, semi-supervised\ntransfer boosting (SS-TrBoosting). Given a well-trained deep learning-based UDA\nor SSDA model, we use it as the initial model, generate additional base\nlearners by boosting, and then use all of them as an ensemble. More\nspecifically, half of the base learners are generated by supervised domain\nadaptation, and half by semi-supervised learning. Furthermore, for more\nefficient data transmission and better data privacy protection, we propose a\nsource data generation approach to extend SS-TrBoosting to semi-supervised\nsource-free domain adaptation (SS-SFDA). Extensive experiments showed that\nSS-TrBoosting can be applied to a variety of existing UDA, SSDA and SFDA\napproaches to further improve their performance.\n", "link": "http://arxiv.org/abs/2412.03212v1", "date": "2024-12-04", "relevancy": 2.3811, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4895}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4754}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Transfer%20Boosting%20%28SS-TrBoosting%29&body=Title%3A%20Semi-Supervised%20Transfer%20Boosting%20%28SS-TrBoosting%29%0AAuthor%3A%20Lingfei%20Deng%20and%20Changming%20Zhao%20and%20Zhenbang%20Du%20and%20Kun%20Xia%20and%20Dongrui%20Wu%0AAbstract%3A%20%20%20Semi-supervised%20domain%20adaptation%20%28SSDA%29%20aims%20at%20training%20a%20high-performance%0Amodel%20for%20a%20target%20domain%20using%20few%20labeled%20target%20data%2C%20many%20unlabeled%20target%0Adata%2C%20and%20plenty%20of%20auxiliary%20data%20from%20a%20source%20domain.%20Previous%20works%20in%20SSDA%0Amainly%20focused%20on%20learning%20transferable%20representations%20across%20domains.%0AHowever%2C%20it%20is%20difficult%20to%20find%20a%20feature%20space%20where%20the%20source%20and%20target%0Adomains%20share%20the%20same%20conditional%20probability%20distribution.%20Additionally%2C%0Athere%20is%20no%20flexible%20and%20effective%20strategy%20extending%20existing%20unsupervised%0Adomain%20adaptation%20%28UDA%29%20approaches%20to%20SSDA%20settings.%20In%20order%20to%20solve%20the%0Aabove%20two%20challenges%2C%20we%20propose%20a%20novel%20fine-tuning%20framework%2C%20semi-supervised%0Atransfer%20boosting%20%28SS-TrBoosting%29.%20Given%20a%20well-trained%20deep%20learning-based%20UDA%0Aor%20SSDA%20model%2C%20we%20use%20it%20as%20the%20initial%20model%2C%20generate%20additional%20base%0Alearners%20by%20boosting%2C%20and%20then%20use%20all%20of%20them%20as%20an%20ensemble.%20More%0Aspecifically%2C%20half%20of%20the%20base%20learners%20are%20generated%20by%20supervised%20domain%0Aadaptation%2C%20and%20half%20by%20semi-supervised%20learning.%20Furthermore%2C%20for%20more%0Aefficient%20data%20transmission%20and%20better%20data%20privacy%20protection%2C%20we%20propose%20a%0Asource%20data%20generation%20approach%20to%20extend%20SS-TrBoosting%20to%20semi-supervised%0Asource-free%20domain%20adaptation%20%28SS-SFDA%29.%20Extensive%20experiments%20showed%20that%0ASS-TrBoosting%20can%20be%20applied%20to%20a%20variety%20of%20existing%20UDA%2C%20SSDA%20and%20SFDA%0Aapproaches%20to%20further%20improve%20their%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%2520Transfer%2520Boosting%2520%2528SS-TrBoosting%2529%26entry.906535625%3DLingfei%2520Deng%2520and%2520Changming%2520Zhao%2520and%2520Zhenbang%2520Du%2520and%2520Kun%2520Xia%2520and%2520Dongrui%2520Wu%26entry.1292438233%3D%2520%2520Semi-supervised%2520domain%2520adaptation%2520%2528SSDA%2529%2520aims%2520at%2520training%2520a%2520high-performance%250Amodel%2520for%2520a%2520target%2520domain%2520using%2520few%2520labeled%2520target%2520data%252C%2520many%2520unlabeled%2520target%250Adata%252C%2520and%2520plenty%2520of%2520auxiliary%2520data%2520from%2520a%2520source%2520domain.%2520Previous%2520works%2520in%2520SSDA%250Amainly%2520focused%2520on%2520learning%2520transferable%2520representations%2520across%2520domains.%250AHowever%252C%2520it%2520is%2520difficult%2520to%2520find%2520a%2520feature%2520space%2520where%2520the%2520source%2520and%2520target%250Adomains%2520share%2520the%2520same%2520conditional%2520probability%2520distribution.%2520Additionally%252C%250Athere%2520is%2520no%2520flexible%2520and%2520effective%2520strategy%2520extending%2520existing%2520unsupervised%250Adomain%2520adaptation%2520%2528UDA%2529%2520approaches%2520to%2520SSDA%2520settings.%2520In%2520order%2520to%2520solve%2520the%250Aabove%2520two%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520fine-tuning%2520framework%252C%2520semi-supervised%250Atransfer%2520boosting%2520%2528SS-TrBoosting%2529.%2520Given%2520a%2520well-trained%2520deep%2520learning-based%2520UDA%250Aor%2520SSDA%2520model%252C%2520we%2520use%2520it%2520as%2520the%2520initial%2520model%252C%2520generate%2520additional%2520base%250Alearners%2520by%2520boosting%252C%2520and%2520then%2520use%2520all%2520of%2520them%2520as%2520an%2520ensemble.%2520More%250Aspecifically%252C%2520half%2520of%2520the%2520base%2520learners%2520are%2520generated%2520by%2520supervised%2520domain%250Aadaptation%252C%2520and%2520half%2520by%2520semi-supervised%2520learning.%2520Furthermore%252C%2520for%2520more%250Aefficient%2520data%2520transmission%2520and%2520better%2520data%2520privacy%2520protection%252C%2520we%2520propose%2520a%250Asource%2520data%2520generation%2520approach%2520to%2520extend%2520SS-TrBoosting%2520to%2520semi-supervised%250Asource-free%2520domain%2520adaptation%2520%2528SS-SFDA%2529.%2520Extensive%2520experiments%2520showed%2520that%250ASS-TrBoosting%2520can%2520be%2520applied%2520to%2520a%2520variety%2520of%2520existing%2520UDA%252C%2520SSDA%2520and%2520SFDA%250Aapproaches%2520to%2520further%2520improve%2520their%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Transfer%20Boosting%20%28SS-TrBoosting%29&entry.906535625=Lingfei%20Deng%20and%20Changming%20Zhao%20and%20Zhenbang%20Du%20and%20Kun%20Xia%20and%20Dongrui%20Wu&entry.1292438233=%20%20Semi-supervised%20domain%20adaptation%20%28SSDA%29%20aims%20at%20training%20a%20high-performance%0Amodel%20for%20a%20target%20domain%20using%20few%20labeled%20target%20data%2C%20many%20unlabeled%20target%0Adata%2C%20and%20plenty%20of%20auxiliary%20data%20from%20a%20source%20domain.%20Previous%20works%20in%20SSDA%0Amainly%20focused%20on%20learning%20transferable%20representations%20across%20domains.%0AHowever%2C%20it%20is%20difficult%20to%20find%20a%20feature%20space%20where%20the%20source%20and%20target%0Adomains%20share%20the%20same%20conditional%20probability%20distribution.%20Additionally%2C%0Athere%20is%20no%20flexible%20and%20effective%20strategy%20extending%20existing%20unsupervised%0Adomain%20adaptation%20%28UDA%29%20approaches%20to%20SSDA%20settings.%20In%20order%20to%20solve%20the%0Aabove%20two%20challenges%2C%20we%20propose%20a%20novel%20fine-tuning%20framework%2C%20semi-supervised%0Atransfer%20boosting%20%28SS-TrBoosting%29.%20Given%20a%20well-trained%20deep%20learning-based%20UDA%0Aor%20SSDA%20model%2C%20we%20use%20it%20as%20the%20initial%20model%2C%20generate%20additional%20base%0Alearners%20by%20boosting%2C%20and%20then%20use%20all%20of%20them%20as%20an%20ensemble.%20More%0Aspecifically%2C%20half%20of%20the%20base%20learners%20are%20generated%20by%20supervised%20domain%0Aadaptation%2C%20and%20half%20by%20semi-supervised%20learning.%20Furthermore%2C%20for%20more%0Aefficient%20data%20transmission%20and%20better%20data%20privacy%20protection%2C%20we%20propose%20a%0Asource%20data%20generation%20approach%20to%20extend%20SS-TrBoosting%20to%20semi-supervised%0Asource-free%20domain%20adaptation%20%28SS-SFDA%29.%20Extensive%20experiments%20showed%20that%0ASS-TrBoosting%20can%20be%20applied%20to%20a%20variety%20of%20existing%20UDA%2C%20SSDA%20and%20SFDA%0Aapproaches%20to%20further%20improve%20their%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03212v1&entry.124074799=Read"},
{"title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression", "author": "Guangda Liu and Chengwei Li and Jieru Zhao and Chenqi Zhang and Minyi Guo", "abstract": "  Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.\n", "link": "http://arxiv.org/abs/2412.03213v1", "date": "2024-12-04", "relevancy": 2.3617, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4779}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4779}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClusterKV%3A%20Manipulating%20LLM%20KV%20Cache%20in%20Semantic%20Space%20for%20Recallable%0A%20%20Compression&body=Title%3A%20ClusterKV%3A%20Manipulating%20LLM%20KV%20Cache%20in%20Semantic%20Space%20for%20Recallable%0A%20%20Compression%0AAuthor%3A%20Guangda%20Liu%20and%20Chengwei%20Li%20and%20Jieru%20Zhao%20and%20Chenqi%20Zhang%20and%20Minyi%20Guo%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20widely%20deployed%20in%20a%20variety%20of%0Aapplications%2C%20and%20the%20context%20length%20is%20rapidly%20increasing%20to%20handle%20tasks%20such%0Aas%20long-document%20QA%20and%20complex%20logical%20reasoning.%20However%2C%20long%20context%20poses%0Asignificant%20challenges%20for%20inference%20efficiency%2C%20including%20high%20memory%20costs%20of%0Akey-value%20%28KV%29%20cache%20and%20increased%20latency%20due%20to%20extensive%20memory%20accesses.%0ARecent%20works%20have%20proposed%20compressing%20KV%20cache%20to%20approximate%20computation%2C%20but%0Athese%20methods%20either%20evict%20tokens%20permanently%2C%20never%20recalling%20them%20for%20later%0Ainference%2C%20or%20recall%20previous%20tokens%20at%20the%20granularity%20of%20pages%20divided%20by%0Atextual%20positions.%20Both%20approaches%20degrade%20the%20model%20accuracy%20and%20output%0Aquality.%20To%20achieve%20efficient%20and%20accurate%20recallable%20KV%20cache%20compression%2C%20we%0Aintroduce%20ClusterKV%2C%20which%20recalls%20tokens%20at%20the%20granularity%20of%20semantic%0Aclusters.%20We%20design%20and%20implement%20efficient%20algorithms%20and%20systems%20for%0Aclustering%2C%20selection%2C%20indexing%20and%20caching.%20Experiment%20results%20show%20that%0AClusterKV%20attains%20negligible%20accuracy%20loss%20across%20various%20tasks%20with%2032k%0Acontext%20lengths%2C%20using%20only%20a%201k%20to%202k%20KV%20cache%20budget%2C%20and%20achieves%20up%20to%20a%0A2%24%5Ctimes%24%20speedup%20in%20latency%20and%20a%202.5%24%5Ctimes%24%20improvement%20in%20decoding%0Athroughput.%20Compared%20to%20SoTA%20recallable%20KV%20compression%20methods%2C%20ClusterKV%0Ademonstrates%20higher%20model%20accuracy%20and%20output%20quality%2C%20while%20maintaining%20or%0Aexceeding%20inference%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClusterKV%253A%2520Manipulating%2520LLM%2520KV%2520Cache%2520in%2520Semantic%2520Space%2520for%2520Recallable%250A%2520%2520Compression%26entry.906535625%3DGuangda%2520Liu%2520and%2520Chengwei%2520Li%2520and%2520Jieru%2520Zhao%2520and%2520Chenqi%2520Zhang%2520and%2520Minyi%2520Guo%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520widely%2520deployed%2520in%2520a%2520variety%2520of%250Aapplications%252C%2520and%2520the%2520context%2520length%2520is%2520rapidly%2520increasing%2520to%2520handle%2520tasks%2520such%250Aas%2520long-document%2520QA%2520and%2520complex%2520logical%2520reasoning.%2520However%252C%2520long%2520context%2520poses%250Asignificant%2520challenges%2520for%2520inference%2520efficiency%252C%2520including%2520high%2520memory%2520costs%2520of%250Akey-value%2520%2528KV%2529%2520cache%2520and%2520increased%2520latency%2520due%2520to%2520extensive%2520memory%2520accesses.%250ARecent%2520works%2520have%2520proposed%2520compressing%2520KV%2520cache%2520to%2520approximate%2520computation%252C%2520but%250Athese%2520methods%2520either%2520evict%2520tokens%2520permanently%252C%2520never%2520recalling%2520them%2520for%2520later%250Ainference%252C%2520or%2520recall%2520previous%2520tokens%2520at%2520the%2520granularity%2520of%2520pages%2520divided%2520by%250Atextual%2520positions.%2520Both%2520approaches%2520degrade%2520the%2520model%2520accuracy%2520and%2520output%250Aquality.%2520To%2520achieve%2520efficient%2520and%2520accurate%2520recallable%2520KV%2520cache%2520compression%252C%2520we%250Aintroduce%2520ClusterKV%252C%2520which%2520recalls%2520tokens%2520at%2520the%2520granularity%2520of%2520semantic%250Aclusters.%2520We%2520design%2520and%2520implement%2520efficient%2520algorithms%2520and%2520systems%2520for%250Aclustering%252C%2520selection%252C%2520indexing%2520and%2520caching.%2520Experiment%2520results%2520show%2520that%250AClusterKV%2520attains%2520negligible%2520accuracy%2520loss%2520across%2520various%2520tasks%2520with%252032k%250Acontext%2520lengths%252C%2520using%2520only%2520a%25201k%2520to%25202k%2520KV%2520cache%2520budget%252C%2520and%2520achieves%2520up%2520to%2520a%250A2%2524%255Ctimes%2524%2520speedup%2520in%2520latency%2520and%2520a%25202.5%2524%255Ctimes%2524%2520improvement%2520in%2520decoding%250Athroughput.%2520Compared%2520to%2520SoTA%2520recallable%2520KV%2520compression%2520methods%252C%2520ClusterKV%250Ademonstrates%2520higher%2520model%2520accuracy%2520and%2520output%2520quality%252C%2520while%2520maintaining%2520or%250Aexceeding%2520inference%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClusterKV%3A%20Manipulating%20LLM%20KV%20Cache%20in%20Semantic%20Space%20for%20Recallable%0A%20%20Compression&entry.906535625=Guangda%20Liu%20and%20Chengwei%20Li%20and%20Jieru%20Zhao%20and%20Chenqi%20Zhang%20and%20Minyi%20Guo&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20widely%20deployed%20in%20a%20variety%20of%0Aapplications%2C%20and%20the%20context%20length%20is%20rapidly%20increasing%20to%20handle%20tasks%20such%0Aas%20long-document%20QA%20and%20complex%20logical%20reasoning.%20However%2C%20long%20context%20poses%0Asignificant%20challenges%20for%20inference%20efficiency%2C%20including%20high%20memory%20costs%20of%0Akey-value%20%28KV%29%20cache%20and%20increased%20latency%20due%20to%20extensive%20memory%20accesses.%0ARecent%20works%20have%20proposed%20compressing%20KV%20cache%20to%20approximate%20computation%2C%20but%0Athese%20methods%20either%20evict%20tokens%20permanently%2C%20never%20recalling%20them%20for%20later%0Ainference%2C%20or%20recall%20previous%20tokens%20at%20the%20granularity%20of%20pages%20divided%20by%0Atextual%20positions.%20Both%20approaches%20degrade%20the%20model%20accuracy%20and%20output%0Aquality.%20To%20achieve%20efficient%20and%20accurate%20recallable%20KV%20cache%20compression%2C%20we%0Aintroduce%20ClusterKV%2C%20which%20recalls%20tokens%20at%20the%20granularity%20of%20semantic%0Aclusters.%20We%20design%20and%20implement%20efficient%20algorithms%20and%20systems%20for%0Aclustering%2C%20selection%2C%20indexing%20and%20caching.%20Experiment%20results%20show%20that%0AClusterKV%20attains%20negligible%20accuracy%20loss%20across%20various%20tasks%20with%2032k%0Acontext%20lengths%2C%20using%20only%20a%201k%20to%202k%20KV%20cache%20budget%2C%20and%20achieves%20up%20to%20a%0A2%24%5Ctimes%24%20speedup%20in%20latency%20and%20a%202.5%24%5Ctimes%24%20improvement%20in%20decoding%0Athroughput.%20Compared%20to%20SoTA%20recallable%20KV%20compression%20methods%2C%20ClusterKV%0Ademonstrates%20higher%20model%20accuracy%20and%20output%20quality%2C%20while%20maintaining%20or%0Aexceeding%20inference%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03213v1&entry.124074799=Read"},
{"title": "SINGER: Vivid Audio-driven Singing Video Generation with Multi-scale\n  Spectral Diffusion Model", "author": "Yan Li and Ziya Zhou and Zhiqiang Wang and Wei Xue and Wenhan Luo and Yike Guo", "abstract": "  Recent advancements in generative models have significantly enhanced talking\nface video generation, yet singing video generation remains underexplored. The\ndifferences between human talking and singing limit the performance of existing\ntalking face video generation models when applied to singing. The fundamental\ndifferences between talking and singing-specifically in audio characteristics\nand behavioral expressions-limit the effectiveness of existing models. We\nobserve that the differences between singing and talking audios manifest in\nterms of frequency and amplitude. To address this, we have designed a\nmulti-scale spectral module to help the model learn singing patterns in the\nspectral domain. Additionally, we develop a spectral-filtering module that aids\nthe model in learning the human behaviors associated with singing audio. These\ntwo modules are integrated into the diffusion model to enhance singing video\ngeneration performance, resulting in our proposed model, SINGER. Furthermore,\nthe lack of high-quality real-world singing face videos has hindered the\ndevelopment of the singing video generation community. To address this gap, we\nhave collected an in-the-wild audio-visual singing dataset to facilitate\nresearch in this area. Our experiments demonstrate that SINGER is capable of\ngenerating vivid singing videos and outperforms state-of-the-art methods in\nboth objective and subjective evaluations.\n", "link": "http://arxiv.org/abs/2412.03430v1", "date": "2024-12-04", "relevancy": 2.351, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6069}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5819}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SINGER%3A%20Vivid%20Audio-driven%20Singing%20Video%20Generation%20with%20Multi-scale%0A%20%20Spectral%20Diffusion%20Model&body=Title%3A%20SINGER%3A%20Vivid%20Audio-driven%20Singing%20Video%20Generation%20with%20Multi-scale%0A%20%20Spectral%20Diffusion%20Model%0AAuthor%3A%20Yan%20Li%20and%20Ziya%20Zhou%20and%20Zhiqiang%20Wang%20and%20Wei%20Xue%20and%20Wenhan%20Luo%20and%20Yike%20Guo%0AAbstract%3A%20%20%20Recent%20advancements%20in%20generative%20models%20have%20significantly%20enhanced%20talking%0Aface%20video%20generation%2C%20yet%20singing%20video%20generation%20remains%20underexplored.%20The%0Adifferences%20between%20human%20talking%20and%20singing%20limit%20the%20performance%20of%20existing%0Atalking%20face%20video%20generation%20models%20when%20applied%20to%20singing.%20The%20fundamental%0Adifferences%20between%20talking%20and%20singing-specifically%20in%20audio%20characteristics%0Aand%20behavioral%20expressions-limit%20the%20effectiveness%20of%20existing%20models.%20We%0Aobserve%20that%20the%20differences%20between%20singing%20and%20talking%20audios%20manifest%20in%0Aterms%20of%20frequency%20and%20amplitude.%20To%20address%20this%2C%20we%20have%20designed%20a%0Amulti-scale%20spectral%20module%20to%20help%20the%20model%20learn%20singing%20patterns%20in%20the%0Aspectral%20domain.%20Additionally%2C%20we%20develop%20a%20spectral-filtering%20module%20that%20aids%0Athe%20model%20in%20learning%20the%20human%20behaviors%20associated%20with%20singing%20audio.%20These%0Atwo%20modules%20are%20integrated%20into%20the%20diffusion%20model%20to%20enhance%20singing%20video%0Ageneration%20performance%2C%20resulting%20in%20our%20proposed%20model%2C%20SINGER.%20Furthermore%2C%0Athe%20lack%20of%20high-quality%20real-world%20singing%20face%20videos%20has%20hindered%20the%0Adevelopment%20of%20the%20singing%20video%20generation%20community.%20To%20address%20this%20gap%2C%20we%0Ahave%20collected%20an%20in-the-wild%20audio-visual%20singing%20dataset%20to%20facilitate%0Aresearch%20in%20this%20area.%20Our%20experiments%20demonstrate%20that%20SINGER%20is%20capable%20of%0Agenerating%20vivid%20singing%20videos%20and%20outperforms%20state-of-the-art%20methods%20in%0Aboth%20objective%20and%20subjective%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSINGER%253A%2520Vivid%2520Audio-driven%2520Singing%2520Video%2520Generation%2520with%2520Multi-scale%250A%2520%2520Spectral%2520Diffusion%2520Model%26entry.906535625%3DYan%2520Li%2520and%2520Ziya%2520Zhou%2520and%2520Zhiqiang%2520Wang%2520and%2520Wei%2520Xue%2520and%2520Wenhan%2520Luo%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520generative%2520models%2520have%2520significantly%2520enhanced%2520talking%250Aface%2520video%2520generation%252C%2520yet%2520singing%2520video%2520generation%2520remains%2520underexplored.%2520The%250Adifferences%2520between%2520human%2520talking%2520and%2520singing%2520limit%2520the%2520performance%2520of%2520existing%250Atalking%2520face%2520video%2520generation%2520models%2520when%2520applied%2520to%2520singing.%2520The%2520fundamental%250Adifferences%2520between%2520talking%2520and%2520singing-specifically%2520in%2520audio%2520characteristics%250Aand%2520behavioral%2520expressions-limit%2520the%2520effectiveness%2520of%2520existing%2520models.%2520We%250Aobserve%2520that%2520the%2520differences%2520between%2520singing%2520and%2520talking%2520audios%2520manifest%2520in%250Aterms%2520of%2520frequency%2520and%2520amplitude.%2520To%2520address%2520this%252C%2520we%2520have%2520designed%2520a%250Amulti-scale%2520spectral%2520module%2520to%2520help%2520the%2520model%2520learn%2520singing%2520patterns%2520in%2520the%250Aspectral%2520domain.%2520Additionally%252C%2520we%2520develop%2520a%2520spectral-filtering%2520module%2520that%2520aids%250Athe%2520model%2520in%2520learning%2520the%2520human%2520behaviors%2520associated%2520with%2520singing%2520audio.%2520These%250Atwo%2520modules%2520are%2520integrated%2520into%2520the%2520diffusion%2520model%2520to%2520enhance%2520singing%2520video%250Ageneration%2520performance%252C%2520resulting%2520in%2520our%2520proposed%2520model%252C%2520SINGER.%2520Furthermore%252C%250Athe%2520lack%2520of%2520high-quality%2520real-world%2520singing%2520face%2520videos%2520has%2520hindered%2520the%250Adevelopment%2520of%2520the%2520singing%2520video%2520generation%2520community.%2520To%2520address%2520this%2520gap%252C%2520we%250Ahave%2520collected%2520an%2520in-the-wild%2520audio-visual%2520singing%2520dataset%2520to%2520facilitate%250Aresearch%2520in%2520this%2520area.%2520Our%2520experiments%2520demonstrate%2520that%2520SINGER%2520is%2520capable%2520of%250Agenerating%2520vivid%2520singing%2520videos%2520and%2520outperforms%2520state-of-the-art%2520methods%2520in%250Aboth%2520objective%2520and%2520subjective%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SINGER%3A%20Vivid%20Audio-driven%20Singing%20Video%20Generation%20with%20Multi-scale%0A%20%20Spectral%20Diffusion%20Model&entry.906535625=Yan%20Li%20and%20Ziya%20Zhou%20and%20Zhiqiang%20Wang%20and%20Wei%20Xue%20and%20Wenhan%20Luo%20and%20Yike%20Guo&entry.1292438233=%20%20Recent%20advancements%20in%20generative%20models%20have%20significantly%20enhanced%20talking%0Aface%20video%20generation%2C%20yet%20singing%20video%20generation%20remains%20underexplored.%20The%0Adifferences%20between%20human%20talking%20and%20singing%20limit%20the%20performance%20of%20existing%0Atalking%20face%20video%20generation%20models%20when%20applied%20to%20singing.%20The%20fundamental%0Adifferences%20between%20talking%20and%20singing-specifically%20in%20audio%20characteristics%0Aand%20behavioral%20expressions-limit%20the%20effectiveness%20of%20existing%20models.%20We%0Aobserve%20that%20the%20differences%20between%20singing%20and%20talking%20audios%20manifest%20in%0Aterms%20of%20frequency%20and%20amplitude.%20To%20address%20this%2C%20we%20have%20designed%20a%0Amulti-scale%20spectral%20module%20to%20help%20the%20model%20learn%20singing%20patterns%20in%20the%0Aspectral%20domain.%20Additionally%2C%20we%20develop%20a%20spectral-filtering%20module%20that%20aids%0Athe%20model%20in%20learning%20the%20human%20behaviors%20associated%20with%20singing%20audio.%20These%0Atwo%20modules%20are%20integrated%20into%20the%20diffusion%20model%20to%20enhance%20singing%20video%0Ageneration%20performance%2C%20resulting%20in%20our%20proposed%20model%2C%20SINGER.%20Furthermore%2C%0Athe%20lack%20of%20high-quality%20real-world%20singing%20face%20videos%20has%20hindered%20the%0Adevelopment%20of%20the%20singing%20video%20generation%20community.%20To%20address%20this%20gap%2C%20we%0Ahave%20collected%20an%20in-the-wild%20audio-visual%20singing%20dataset%20to%20facilitate%0Aresearch%20in%20this%20area.%20Our%20experiments%20demonstrate%20that%20SINGER%20is%20capable%20of%0Agenerating%20vivid%20singing%20videos%20and%20outperforms%20state-of-the-art%20methods%20in%0Aboth%20objective%20and%20subjective%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03430v1&entry.124074799=Read"},
{"title": "Skel3D: Skeleton Guided Novel View Synthesis", "author": "Aron F\u00f3thi and Bence Fazekas and Natabara M\u00e1t\u00e9 Gy\u00f6ngy\u00f6ssy and Kristian Fenech", "abstract": "  In this paper, we present an approach for monocular open-set novel view\nsynthesis (NVS) that leverages object skeletons to guide the underlying\ndiffusion model. Building upon a baseline that utilizes a pre-trained 2D image\ngenerator, our method takes advantage of the Objaverse dataset, which includes\nanimated objects with bone structures. By introducing a skeleton guide layer\nfollowing the existing ray conditioning normalization (RCN) layer, our approach\nenhances pose accuracy and multi-view consistency. The skeleton guide layer\nprovides detailed structural information for the generative model, improving\nthe quality of synthesized views. Experimental results demonstrate that our\nskeleton-guided method significantly enhances consistency and accuracy across\ndiverse object categories within the Objaverse dataset. Our method outperforms\nexisting state-of-the-art NVS techniques both quantitatively and qualitatively,\nwithout relying on explicit 3D representations.\n", "link": "http://arxiv.org/abs/2412.03407v1", "date": "2024-12-04", "relevancy": 2.3505, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5947}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5947}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skel3D%3A%20Skeleton%20Guided%20Novel%20View%20Synthesis&body=Title%3A%20Skel3D%3A%20Skeleton%20Guided%20Novel%20View%20Synthesis%0AAuthor%3A%20Aron%20F%C3%B3thi%20and%20Bence%20Fazekas%20and%20Natabara%20M%C3%A1t%C3%A9%20Gy%C3%B6ngy%C3%B6ssy%20and%20Kristian%20Fenech%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20an%20approach%20for%20monocular%20open-set%20novel%20view%0Asynthesis%20%28NVS%29%20that%20leverages%20object%20skeletons%20to%20guide%20the%20underlying%0Adiffusion%20model.%20Building%20upon%20a%20baseline%20that%20utilizes%20a%20pre-trained%202D%20image%0Agenerator%2C%20our%20method%20takes%20advantage%20of%20the%20Objaverse%20dataset%2C%20which%20includes%0Aanimated%20objects%20with%20bone%20structures.%20By%20introducing%20a%20skeleton%20guide%20layer%0Afollowing%20the%20existing%20ray%20conditioning%20normalization%20%28RCN%29%20layer%2C%20our%20approach%0Aenhances%20pose%20accuracy%20and%20multi-view%20consistency.%20The%20skeleton%20guide%20layer%0Aprovides%20detailed%20structural%20information%20for%20the%20generative%20model%2C%20improving%0Athe%20quality%20of%20synthesized%20views.%20Experimental%20results%20demonstrate%20that%20our%0Askeleton-guided%20method%20significantly%20enhances%20consistency%20and%20accuracy%20across%0Adiverse%20object%20categories%20within%20the%20Objaverse%20dataset.%20Our%20method%20outperforms%0Aexisting%20state-of-the-art%20NVS%20techniques%20both%20quantitatively%20and%20qualitatively%2C%0Awithout%20relying%20on%20explicit%203D%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkel3D%253A%2520Skeleton%2520Guided%2520Novel%2520View%2520Synthesis%26entry.906535625%3DAron%2520F%25C3%25B3thi%2520and%2520Bence%2520Fazekas%2520and%2520Natabara%2520M%25C3%25A1t%25C3%25A9%2520Gy%25C3%25B6ngy%25C3%25B6ssy%2520and%2520Kristian%2520Fenech%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520approach%2520for%2520monocular%2520open-set%2520novel%2520view%250Asynthesis%2520%2528NVS%2529%2520that%2520leverages%2520object%2520skeletons%2520to%2520guide%2520the%2520underlying%250Adiffusion%2520model.%2520Building%2520upon%2520a%2520baseline%2520that%2520utilizes%2520a%2520pre-trained%25202D%2520image%250Agenerator%252C%2520our%2520method%2520takes%2520advantage%2520of%2520the%2520Objaverse%2520dataset%252C%2520which%2520includes%250Aanimated%2520objects%2520with%2520bone%2520structures.%2520By%2520introducing%2520a%2520skeleton%2520guide%2520layer%250Afollowing%2520the%2520existing%2520ray%2520conditioning%2520normalization%2520%2528RCN%2529%2520layer%252C%2520our%2520approach%250Aenhances%2520pose%2520accuracy%2520and%2520multi-view%2520consistency.%2520The%2520skeleton%2520guide%2520layer%250Aprovides%2520detailed%2520structural%2520information%2520for%2520the%2520generative%2520model%252C%2520improving%250Athe%2520quality%2520of%2520synthesized%2520views.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Askeleton-guided%2520method%2520significantly%2520enhances%2520consistency%2520and%2520accuracy%2520across%250Adiverse%2520object%2520categories%2520within%2520the%2520Objaverse%2520dataset.%2520Our%2520method%2520outperforms%250Aexisting%2520state-of-the-art%2520NVS%2520techniques%2520both%2520quantitatively%2520and%2520qualitatively%252C%250Awithout%2520relying%2520on%2520explicit%25203D%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skel3D%3A%20Skeleton%20Guided%20Novel%20View%20Synthesis&entry.906535625=Aron%20F%C3%B3thi%20and%20Bence%20Fazekas%20and%20Natabara%20M%C3%A1t%C3%A9%20Gy%C3%B6ngy%C3%B6ssy%20and%20Kristian%20Fenech&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20an%20approach%20for%20monocular%20open-set%20novel%20view%0Asynthesis%20%28NVS%29%20that%20leverages%20object%20skeletons%20to%20guide%20the%20underlying%0Adiffusion%20model.%20Building%20upon%20a%20baseline%20that%20utilizes%20a%20pre-trained%202D%20image%0Agenerator%2C%20our%20method%20takes%20advantage%20of%20the%20Objaverse%20dataset%2C%20which%20includes%0Aanimated%20objects%20with%20bone%20structures.%20By%20introducing%20a%20skeleton%20guide%20layer%0Afollowing%20the%20existing%20ray%20conditioning%20normalization%20%28RCN%29%20layer%2C%20our%20approach%0Aenhances%20pose%20accuracy%20and%20multi-view%20consistency.%20The%20skeleton%20guide%20layer%0Aprovides%20detailed%20structural%20information%20for%20the%20generative%20model%2C%20improving%0Athe%20quality%20of%20synthesized%20views.%20Experimental%20results%20demonstrate%20that%20our%0Askeleton-guided%20method%20significantly%20enhances%20consistency%20and%20accuracy%20across%0Adiverse%20object%20categories%20within%20the%20Objaverse%20dataset.%20Our%20method%20outperforms%0Aexisting%20state-of-the-art%20NVS%20techniques%20both%20quantitatively%20and%20qualitatively%2C%0Awithout%20relying%20on%20explicit%203D%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03407v1&entry.124074799=Read"},
{"title": "PatchDPO: Patch-level DPO for Finetuning-free Personalized Image\n  Generation", "author": "Qihan Huang and Long Chan and Jinlong Liu and Wanggui He and Hao Jiang and Mingli Song and Jie Song", "abstract": "  Finetuning-free personalized image generation can synthesize customized\nimages without test-time finetuning, attracting wide research interest owing to\nits high efficiency. Current finetuning-free methods simply adopt a single\ntraining stage with a simple image reconstruction task, and they typically\ngenerate low-quality images inconsistent with the reference images during\ntest-time. To mitigate this problem, inspired by the recent DPO (i.e., direct\npreference optimization) technique, this work proposes an additional training\nstage to improve the pre-trained personalized generation models. However,\ntraditional DPO only determines the overall superiority or inferiority of two\nsamples, which is not suitable for personalized image generation because the\ngenerated images are commonly inconsistent with the reference images only in\nsome local image patches. To tackle this problem, this work proposes PatchDPO\nthat estimates the quality of image patches within each generated image and\naccordingly trains the model. To this end, PatchDPO first leverages the\npre-trained vision model with a proposed self-supervised training method to\nestimate the patch quality. Next, PatchDPO adopts a weighted training approach\nto train the model with the estimated patch quality, which rewards the image\npatches with high quality while penalizing the image patches with low quality.\nExperiment results demonstrate that PatchDPO significantly improves the\nperformance of multiple pre-trained personalized generation models, and\nachieves state-of-the-art performance on both single-object and multi-object\npersonalized image generation. Our code is available at\nhttps://github.com/hqhQAQ/PatchDPO.\n", "link": "http://arxiv.org/abs/2412.03177v1", "date": "2024-12-04", "relevancy": 2.3393, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6017}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5804}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PatchDPO%3A%20Patch-level%20DPO%20for%20Finetuning-free%20Personalized%20Image%0A%20%20Generation&body=Title%3A%20PatchDPO%3A%20Patch-level%20DPO%20for%20Finetuning-free%20Personalized%20Image%0A%20%20Generation%0AAuthor%3A%20Qihan%20Huang%20and%20Long%20Chan%20and%20Jinlong%20Liu%20and%20Wanggui%20He%20and%20Hao%20Jiang%20and%20Mingli%20Song%20and%20Jie%20Song%0AAbstract%3A%20%20%20Finetuning-free%20personalized%20image%20generation%20can%20synthesize%20customized%0Aimages%20without%20test-time%20finetuning%2C%20attracting%20wide%20research%20interest%20owing%20to%0Aits%20high%20efficiency.%20Current%20finetuning-free%20methods%20simply%20adopt%20a%20single%0Atraining%20stage%20with%20a%20simple%20image%20reconstruction%20task%2C%20and%20they%20typically%0Agenerate%20low-quality%20images%20inconsistent%20with%20the%20reference%20images%20during%0Atest-time.%20To%20mitigate%20this%20problem%2C%20inspired%20by%20the%20recent%20DPO%20%28i.e.%2C%20direct%0Apreference%20optimization%29%20technique%2C%20this%20work%20proposes%20an%20additional%20training%0Astage%20to%20improve%20the%20pre-trained%20personalized%20generation%20models.%20However%2C%0Atraditional%20DPO%20only%20determines%20the%20overall%20superiority%20or%20inferiority%20of%20two%0Asamples%2C%20which%20is%20not%20suitable%20for%20personalized%20image%20generation%20because%20the%0Agenerated%20images%20are%20commonly%20inconsistent%20with%20the%20reference%20images%20only%20in%0Asome%20local%20image%20patches.%20To%20tackle%20this%20problem%2C%20this%20work%20proposes%20PatchDPO%0Athat%20estimates%20the%20quality%20of%20image%20patches%20within%20each%20generated%20image%20and%0Aaccordingly%20trains%20the%20model.%20To%20this%20end%2C%20PatchDPO%20first%20leverages%20the%0Apre-trained%20vision%20model%20with%20a%20proposed%20self-supervised%20training%20method%20to%0Aestimate%20the%20patch%20quality.%20Next%2C%20PatchDPO%20adopts%20a%20weighted%20training%20approach%0Ato%20train%20the%20model%20with%20the%20estimated%20patch%20quality%2C%20which%20rewards%20the%20image%0Apatches%20with%20high%20quality%20while%20penalizing%20the%20image%20patches%20with%20low%20quality.%0AExperiment%20results%20demonstrate%20that%20PatchDPO%20significantly%20improves%20the%0Aperformance%20of%20multiple%20pre-trained%20personalized%20generation%20models%2C%20and%0Aachieves%20state-of-the-art%20performance%20on%20both%20single-object%20and%20multi-object%0Apersonalized%20image%20generation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/hqhQAQ/PatchDPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatchDPO%253A%2520Patch-level%2520DPO%2520for%2520Finetuning-free%2520Personalized%2520Image%250A%2520%2520Generation%26entry.906535625%3DQihan%2520Huang%2520and%2520Long%2520Chan%2520and%2520Jinlong%2520Liu%2520and%2520Wanggui%2520He%2520and%2520Hao%2520Jiang%2520and%2520Mingli%2520Song%2520and%2520Jie%2520Song%26entry.1292438233%3D%2520%2520Finetuning-free%2520personalized%2520image%2520generation%2520can%2520synthesize%2520customized%250Aimages%2520without%2520test-time%2520finetuning%252C%2520attracting%2520wide%2520research%2520interest%2520owing%2520to%250Aits%2520high%2520efficiency.%2520Current%2520finetuning-free%2520methods%2520simply%2520adopt%2520a%2520single%250Atraining%2520stage%2520with%2520a%2520simple%2520image%2520reconstruction%2520task%252C%2520and%2520they%2520typically%250Agenerate%2520low-quality%2520images%2520inconsistent%2520with%2520the%2520reference%2520images%2520during%250Atest-time.%2520To%2520mitigate%2520this%2520problem%252C%2520inspired%2520by%2520the%2520recent%2520DPO%2520%2528i.e.%252C%2520direct%250Apreference%2520optimization%2529%2520technique%252C%2520this%2520work%2520proposes%2520an%2520additional%2520training%250Astage%2520to%2520improve%2520the%2520pre-trained%2520personalized%2520generation%2520models.%2520However%252C%250Atraditional%2520DPO%2520only%2520determines%2520the%2520overall%2520superiority%2520or%2520inferiority%2520of%2520two%250Asamples%252C%2520which%2520is%2520not%2520suitable%2520for%2520personalized%2520image%2520generation%2520because%2520the%250Agenerated%2520images%2520are%2520commonly%2520inconsistent%2520with%2520the%2520reference%2520images%2520only%2520in%250Asome%2520local%2520image%2520patches.%2520To%2520tackle%2520this%2520problem%252C%2520this%2520work%2520proposes%2520PatchDPO%250Athat%2520estimates%2520the%2520quality%2520of%2520image%2520patches%2520within%2520each%2520generated%2520image%2520and%250Aaccordingly%2520trains%2520the%2520model.%2520To%2520this%2520end%252C%2520PatchDPO%2520first%2520leverages%2520the%250Apre-trained%2520vision%2520model%2520with%2520a%2520proposed%2520self-supervised%2520training%2520method%2520to%250Aestimate%2520the%2520patch%2520quality.%2520Next%252C%2520PatchDPO%2520adopts%2520a%2520weighted%2520training%2520approach%250Ato%2520train%2520the%2520model%2520with%2520the%2520estimated%2520patch%2520quality%252C%2520which%2520rewards%2520the%2520image%250Apatches%2520with%2520high%2520quality%2520while%2520penalizing%2520the%2520image%2520patches%2520with%2520low%2520quality.%250AExperiment%2520results%2520demonstrate%2520that%2520PatchDPO%2520significantly%2520improves%2520the%250Aperformance%2520of%2520multiple%2520pre-trained%2520personalized%2520generation%2520models%252C%2520and%250Aachieves%2520state-of-the-art%2520performance%2520on%2520both%2520single-object%2520and%2520multi-object%250Apersonalized%2520image%2520generation.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/hqhQAQ/PatchDPO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PatchDPO%3A%20Patch-level%20DPO%20for%20Finetuning-free%20Personalized%20Image%0A%20%20Generation&entry.906535625=Qihan%20Huang%20and%20Long%20Chan%20and%20Jinlong%20Liu%20and%20Wanggui%20He%20and%20Hao%20Jiang%20and%20Mingli%20Song%20and%20Jie%20Song&entry.1292438233=%20%20Finetuning-free%20personalized%20image%20generation%20can%20synthesize%20customized%0Aimages%20without%20test-time%20finetuning%2C%20attracting%20wide%20research%20interest%20owing%20to%0Aits%20high%20efficiency.%20Current%20finetuning-free%20methods%20simply%20adopt%20a%20single%0Atraining%20stage%20with%20a%20simple%20image%20reconstruction%20task%2C%20and%20they%20typically%0Agenerate%20low-quality%20images%20inconsistent%20with%20the%20reference%20images%20during%0Atest-time.%20To%20mitigate%20this%20problem%2C%20inspired%20by%20the%20recent%20DPO%20%28i.e.%2C%20direct%0Apreference%20optimization%29%20technique%2C%20this%20work%20proposes%20an%20additional%20training%0Astage%20to%20improve%20the%20pre-trained%20personalized%20generation%20models.%20However%2C%0Atraditional%20DPO%20only%20determines%20the%20overall%20superiority%20or%20inferiority%20of%20two%0Asamples%2C%20which%20is%20not%20suitable%20for%20personalized%20image%20generation%20because%20the%0Agenerated%20images%20are%20commonly%20inconsistent%20with%20the%20reference%20images%20only%20in%0Asome%20local%20image%20patches.%20To%20tackle%20this%20problem%2C%20this%20work%20proposes%20PatchDPO%0Athat%20estimates%20the%20quality%20of%20image%20patches%20within%20each%20generated%20image%20and%0Aaccordingly%20trains%20the%20model.%20To%20this%20end%2C%20PatchDPO%20first%20leverages%20the%0Apre-trained%20vision%20model%20with%20a%20proposed%20self-supervised%20training%20method%20to%0Aestimate%20the%20patch%20quality.%20Next%2C%20PatchDPO%20adopts%20a%20weighted%20training%20approach%0Ato%20train%20the%20model%20with%20the%20estimated%20patch%20quality%2C%20which%20rewards%20the%20image%0Apatches%20with%20high%20quality%20while%20penalizing%20the%20image%20patches%20with%20low%20quality.%0AExperiment%20results%20demonstrate%20that%20PatchDPO%20significantly%20improves%20the%0Aperformance%20of%20multiple%20pre-trained%20personalized%20generation%20models%2C%20and%0Aachieves%20state-of-the-art%20performance%20on%20both%20single-object%20and%20multi-object%0Apersonalized%20image%20generation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/hqhQAQ/PatchDPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03177v1&entry.124074799=Read"},
{"title": "Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion", "author": "Shengyuan Zhang and An Zhao and Ling Yang and Zejian Li and Chenye Meng and Haoran Xu and Tianrun Chen and AnYang Wei and Perry Pengyun GU and Lingyun Sun", "abstract": "  Diffusion models have been applied to 3D LiDAR scene completion due to their\nstrong training stability and high completion quality. However, the slow\nsampling speed limits the practical application of diffusion-based scene\ncompletion models since autonomous vehicles require an efficient perception of\nsurrounding environments. This paper proposes a novel distillation method\ntailored for 3D LiDAR scene completion models, dubbed $\\textbf{ScoreLiDAR}$,\nwhich achieves efficient yet high-quality scene completion. ScoreLiDAR enables\nthe distilled model to sample in significantly fewer steps after distillation.\nTo improve completion quality, we also introduce a novel $\\textbf{Structural\nLoss}$, which encourages the distilled model to capture the geometric structure\nof the 3D LiDAR scene. The loss contains a scene-wise term constraining the\nholistic structure and a point-wise term constraining the key landmark points\nand their relative configuration. Extensive experiments demonstrate that\nScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37\nseconds per frame ($>$5$\\times$) on SemanticKITTI and achieves superior\nperformance compared to state-of-the-art 3D LiDAR scene completion models. Our\ncode is publicly available at https://github.com/happyw1nd/ScoreLiDAR.\n", "link": "http://arxiv.org/abs/2412.03515v1", "date": "2024-12-04", "relevancy": 2.3371, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5861}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5861}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilling%20Diffusion%20Models%20to%20Efficient%203D%20LiDAR%20Scene%20Completion&body=Title%3A%20Distilling%20Diffusion%20Models%20to%20Efficient%203D%20LiDAR%20Scene%20Completion%0AAuthor%3A%20Shengyuan%20Zhang%20and%20An%20Zhao%20and%20Ling%20Yang%20and%20Zejian%20Li%20and%20Chenye%20Meng%20and%20Haoran%20Xu%20and%20Tianrun%20Chen%20and%20AnYang%20Wei%20and%20Perry%20Pengyun%20GU%20and%20Lingyun%20Sun%0AAbstract%3A%20%20%20Diffusion%20models%20have%20been%20applied%20to%203D%20LiDAR%20scene%20completion%20due%20to%20their%0Astrong%20training%20stability%20and%20high%20completion%20quality.%20However%2C%20the%20slow%0Asampling%20speed%20limits%20the%20practical%20application%20of%20diffusion-based%20scene%0Acompletion%20models%20since%20autonomous%20vehicles%20require%20an%20efficient%20perception%20of%0Asurrounding%20environments.%20This%20paper%20proposes%20a%20novel%20distillation%20method%0Atailored%20for%203D%20LiDAR%20scene%20completion%20models%2C%20dubbed%20%24%5Ctextbf%7BScoreLiDAR%7D%24%2C%0Awhich%20achieves%20efficient%20yet%20high-quality%20scene%20completion.%20ScoreLiDAR%20enables%0Athe%20distilled%20model%20to%20sample%20in%20significantly%20fewer%20steps%20after%20distillation.%0ATo%20improve%20completion%20quality%2C%20we%20also%20introduce%20a%20novel%20%24%5Ctextbf%7BStructural%0ALoss%7D%24%2C%20which%20encourages%20the%20distilled%20model%20to%20capture%20the%20geometric%20structure%0Aof%20the%203D%20LiDAR%20scene.%20The%20loss%20contains%20a%20scene-wise%20term%20constraining%20the%0Aholistic%20structure%20and%20a%20point-wise%20term%20constraining%20the%20key%20landmark%20points%0Aand%20their%20relative%20configuration.%20Extensive%20experiments%20demonstrate%20that%0AScoreLiDAR%20significantly%20accelerates%20the%20completion%20time%20from%2030.55%20to%205.37%0Aseconds%20per%20frame%20%28%24%3E%245%24%5Ctimes%24%29%20on%20SemanticKITTI%20and%20achieves%20superior%0Aperformance%20compared%20to%20state-of-the-art%203D%20LiDAR%20scene%20completion%20models.%20Our%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/happyw1nd/ScoreLiDAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilling%2520Diffusion%2520Models%2520to%2520Efficient%25203D%2520LiDAR%2520Scene%2520Completion%26entry.906535625%3DShengyuan%2520Zhang%2520and%2520An%2520Zhao%2520and%2520Ling%2520Yang%2520and%2520Zejian%2520Li%2520and%2520Chenye%2520Meng%2520and%2520Haoran%2520Xu%2520and%2520Tianrun%2520Chen%2520and%2520AnYang%2520Wei%2520and%2520Perry%2520Pengyun%2520GU%2520and%2520Lingyun%2520Sun%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520been%2520applied%2520to%25203D%2520LiDAR%2520scene%2520completion%2520due%2520to%2520their%250Astrong%2520training%2520stability%2520and%2520high%2520completion%2520quality.%2520However%252C%2520the%2520slow%250Asampling%2520speed%2520limits%2520the%2520practical%2520application%2520of%2520diffusion-based%2520scene%250Acompletion%2520models%2520since%2520autonomous%2520vehicles%2520require%2520an%2520efficient%2520perception%2520of%250Asurrounding%2520environments.%2520This%2520paper%2520proposes%2520a%2520novel%2520distillation%2520method%250Atailored%2520for%25203D%2520LiDAR%2520scene%2520completion%2520models%252C%2520dubbed%2520%2524%255Ctextbf%257BScoreLiDAR%257D%2524%252C%250Awhich%2520achieves%2520efficient%2520yet%2520high-quality%2520scene%2520completion.%2520ScoreLiDAR%2520enables%250Athe%2520distilled%2520model%2520to%2520sample%2520in%2520significantly%2520fewer%2520steps%2520after%2520distillation.%250ATo%2520improve%2520completion%2520quality%252C%2520we%2520also%2520introduce%2520a%2520novel%2520%2524%255Ctextbf%257BStructural%250ALoss%257D%2524%252C%2520which%2520encourages%2520the%2520distilled%2520model%2520to%2520capture%2520the%2520geometric%2520structure%250Aof%2520the%25203D%2520LiDAR%2520scene.%2520The%2520loss%2520contains%2520a%2520scene-wise%2520term%2520constraining%2520the%250Aholistic%2520structure%2520and%2520a%2520point-wise%2520term%2520constraining%2520the%2520key%2520landmark%2520points%250Aand%2520their%2520relative%2520configuration.%2520Extensive%2520experiments%2520demonstrate%2520that%250AScoreLiDAR%2520significantly%2520accelerates%2520the%2520completion%2520time%2520from%252030.55%2520to%25205.37%250Aseconds%2520per%2520frame%2520%2528%2524%253E%25245%2524%255Ctimes%2524%2529%2520on%2520SemanticKITTI%2520and%2520achieves%2520superior%250Aperformance%2520compared%2520to%2520state-of-the-art%25203D%2520LiDAR%2520scene%2520completion%2520models.%2520Our%250Acode%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/happyw1nd/ScoreLiDAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20Diffusion%20Models%20to%20Efficient%203D%20LiDAR%20Scene%20Completion&entry.906535625=Shengyuan%20Zhang%20and%20An%20Zhao%20and%20Ling%20Yang%20and%20Zejian%20Li%20and%20Chenye%20Meng%20and%20Haoran%20Xu%20and%20Tianrun%20Chen%20and%20AnYang%20Wei%20and%20Perry%20Pengyun%20GU%20and%20Lingyun%20Sun&entry.1292438233=%20%20Diffusion%20models%20have%20been%20applied%20to%203D%20LiDAR%20scene%20completion%20due%20to%20their%0Astrong%20training%20stability%20and%20high%20completion%20quality.%20However%2C%20the%20slow%0Asampling%20speed%20limits%20the%20practical%20application%20of%20diffusion-based%20scene%0Acompletion%20models%20since%20autonomous%20vehicles%20require%20an%20efficient%20perception%20of%0Asurrounding%20environments.%20This%20paper%20proposes%20a%20novel%20distillation%20method%0Atailored%20for%203D%20LiDAR%20scene%20completion%20models%2C%20dubbed%20%24%5Ctextbf%7BScoreLiDAR%7D%24%2C%0Awhich%20achieves%20efficient%20yet%20high-quality%20scene%20completion.%20ScoreLiDAR%20enables%0Athe%20distilled%20model%20to%20sample%20in%20significantly%20fewer%20steps%20after%20distillation.%0ATo%20improve%20completion%20quality%2C%20we%20also%20introduce%20a%20novel%20%24%5Ctextbf%7BStructural%0ALoss%7D%24%2C%20which%20encourages%20the%20distilled%20model%20to%20capture%20the%20geometric%20structure%0Aof%20the%203D%20LiDAR%20scene.%20The%20loss%20contains%20a%20scene-wise%20term%20constraining%20the%0Aholistic%20structure%20and%20a%20point-wise%20term%20constraining%20the%20key%20landmark%20points%0Aand%20their%20relative%20configuration.%20Extensive%20experiments%20demonstrate%20that%0AScoreLiDAR%20significantly%20accelerates%20the%20completion%20time%20from%2030.55%20to%205.37%0Aseconds%20per%20frame%20%28%24%3E%245%24%5Ctimes%24%29%20on%20SemanticKITTI%20and%20achieves%20superior%0Aperformance%20compared%20to%20state-of-the-art%203D%20LiDAR%20scene%20completion%20models.%20Our%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/happyw1nd/ScoreLiDAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03515v1&entry.124074799=Read"},
{"title": "Risk-aware Classification via Uncertainty Quantification", "author": "Murat Sensoy and Lance M. Kaplan and Simon Julier and Maryam Saleki and Federico Cerutti", "abstract": "  Autonomous and semi-autonomous systems are using deep learning models to\nimprove decision-making. However, deep classifiers can be overly confident in\ntheir incorrect predictions, a major issue especially in safety-critical\ndomains. The present study introduces three foundational desiderata for\ndeveloping real-world risk-aware classification systems. Expanding upon the\npreviously proposed Evidential Deep Learning (EDL), we demonstrate the unity\nbetween these principles and EDL's operational attributes. We then augment EDL\nempowering autonomous agents to exercise discretion during structured\ndecision-making when uncertainty and risks are inherent. We rigorously examine\nempirical scenarios to substantiate these theoretical innovations. In contrast\nto existing risk-aware classifiers, our proposed methodologies consistently\nexhibit superior performance, underscoring their transformative potential in\nrisk-conscious classification strategies.\n", "link": "http://arxiv.org/abs/2412.03391v1", "date": "2024-12-04", "relevancy": 2.3307, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6616}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5915}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Risk-aware%20Classification%20via%20Uncertainty%20Quantification&body=Title%3A%20Risk-aware%20Classification%20via%20Uncertainty%20Quantification%0AAuthor%3A%20Murat%20Sensoy%20and%20Lance%20M.%20Kaplan%20and%20Simon%20Julier%20and%20Maryam%20Saleki%20and%20Federico%20Cerutti%0AAbstract%3A%20%20%20Autonomous%20and%20semi-autonomous%20systems%20are%20using%20deep%20learning%20models%20to%0Aimprove%20decision-making.%20However%2C%20deep%20classifiers%20can%20be%20overly%20confident%20in%0Atheir%20incorrect%20predictions%2C%20a%20major%20issue%20especially%20in%20safety-critical%0Adomains.%20The%20present%20study%20introduces%20three%20foundational%20desiderata%20for%0Adeveloping%20real-world%20risk-aware%20classification%20systems.%20Expanding%20upon%20the%0Apreviously%20proposed%20Evidential%20Deep%20Learning%20%28EDL%29%2C%20we%20demonstrate%20the%20unity%0Abetween%20these%20principles%20and%20EDL%27s%20operational%20attributes.%20We%20then%20augment%20EDL%0Aempowering%20autonomous%20agents%20to%20exercise%20discretion%20during%20structured%0Adecision-making%20when%20uncertainty%20and%20risks%20are%20inherent.%20We%20rigorously%20examine%0Aempirical%20scenarios%20to%20substantiate%20these%20theoretical%20innovations.%20In%20contrast%0Ato%20existing%20risk-aware%20classifiers%2C%20our%20proposed%20methodologies%20consistently%0Aexhibit%20superior%20performance%2C%20underscoring%20their%20transformative%20potential%20in%0Arisk-conscious%20classification%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRisk-aware%2520Classification%2520via%2520Uncertainty%2520Quantification%26entry.906535625%3DMurat%2520Sensoy%2520and%2520Lance%2520M.%2520Kaplan%2520and%2520Simon%2520Julier%2520and%2520Maryam%2520Saleki%2520and%2520Federico%2520Cerutti%26entry.1292438233%3D%2520%2520Autonomous%2520and%2520semi-autonomous%2520systems%2520are%2520using%2520deep%2520learning%2520models%2520to%250Aimprove%2520decision-making.%2520However%252C%2520deep%2520classifiers%2520can%2520be%2520overly%2520confident%2520in%250Atheir%2520incorrect%2520predictions%252C%2520a%2520major%2520issue%2520especially%2520in%2520safety-critical%250Adomains.%2520The%2520present%2520study%2520introduces%2520three%2520foundational%2520desiderata%2520for%250Adeveloping%2520real-world%2520risk-aware%2520classification%2520systems.%2520Expanding%2520upon%2520the%250Apreviously%2520proposed%2520Evidential%2520Deep%2520Learning%2520%2528EDL%2529%252C%2520we%2520demonstrate%2520the%2520unity%250Abetween%2520these%2520principles%2520and%2520EDL%2527s%2520operational%2520attributes.%2520We%2520then%2520augment%2520EDL%250Aempowering%2520autonomous%2520agents%2520to%2520exercise%2520discretion%2520during%2520structured%250Adecision-making%2520when%2520uncertainty%2520and%2520risks%2520are%2520inherent.%2520We%2520rigorously%2520examine%250Aempirical%2520scenarios%2520to%2520substantiate%2520these%2520theoretical%2520innovations.%2520In%2520contrast%250Ato%2520existing%2520risk-aware%2520classifiers%252C%2520our%2520proposed%2520methodologies%2520consistently%250Aexhibit%2520superior%2520performance%252C%2520underscoring%2520their%2520transformative%2520potential%2520in%250Arisk-conscious%2520classification%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Risk-aware%20Classification%20via%20Uncertainty%20Quantification&entry.906535625=Murat%20Sensoy%20and%20Lance%20M.%20Kaplan%20and%20Simon%20Julier%20and%20Maryam%20Saleki%20and%20Federico%20Cerutti&entry.1292438233=%20%20Autonomous%20and%20semi-autonomous%20systems%20are%20using%20deep%20learning%20models%20to%0Aimprove%20decision-making.%20However%2C%20deep%20classifiers%20can%20be%20overly%20confident%20in%0Atheir%20incorrect%20predictions%2C%20a%20major%20issue%20especially%20in%20safety-critical%0Adomains.%20The%20present%20study%20introduces%20three%20foundational%20desiderata%20for%0Adeveloping%20real-world%20risk-aware%20classification%20systems.%20Expanding%20upon%20the%0Apreviously%20proposed%20Evidential%20Deep%20Learning%20%28EDL%29%2C%20we%20demonstrate%20the%20unity%0Abetween%20these%20principles%20and%20EDL%27s%20operational%20attributes.%20We%20then%20augment%20EDL%0Aempowering%20autonomous%20agents%20to%20exercise%20discretion%20during%20structured%0Adecision-making%20when%20uncertainty%20and%20risks%20are%20inherent.%20We%20rigorously%20examine%0Aempirical%20scenarios%20to%20substantiate%20these%20theoretical%20innovations.%20In%20contrast%0Ato%20existing%20risk-aware%20classifiers%2C%20our%20proposed%20methodologies%20consistently%0Aexhibit%20superior%20performance%2C%20underscoring%20their%20transformative%20potential%20in%0Arisk-conscious%20classification%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03391v1&entry.124074799=Read"},
{"title": "NVComposer: Boosting Generative Novel View Synthesis with Multiple\n  Sparse and Unposed Images", "author": "Lingen Li and Zhaoyang Zhang and Yaowei Li and Jiale Xu and Xiaoyu Li and Wenbo Hu and Weihao Cheng and Jinwei Gu and Tianfan Xue and Ying Shan", "abstract": "  Recent advancements in generative models have significantly improved novel\nview synthesis (NVS) from multi-view data. However, existing methods depend on\nexternal multi-view alignment processes, such as explicit pose estimation or\npre-reconstruction, which limits their flexibility and accessibility,\nespecially when alignment is unstable due to insufficient overlap or occlusions\nbetween views. In this paper, we propose NVComposer, a novel approach that\neliminates the need for explicit external alignment. NVComposer enables the\ngenerative model to implicitly infer spatial and geometric relationships\nbetween multiple conditional views by introducing two key components: 1) an\nimage-pose dual-stream diffusion model that simultaneously generates target\nnovel views and condition camera poses, and 2) a geometry-aware feature\nalignment module that distills geometric priors from dense stereo models during\ntraining. Extensive experiments demonstrate that NVComposer achieves\nstate-of-the-art performance in generative multi-view NVS tasks, removing the\nreliance on external alignment and thus improving model accessibility. Our\napproach shows substantial improvements in synthesis quality as the number of\nunposed input views increases, highlighting its potential for more flexible and\naccessible generative NVS systems.\n", "link": "http://arxiv.org/abs/2412.03517v1", "date": "2024-12-04", "relevancy": 2.3278, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6484}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5687}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NVComposer%3A%20Boosting%20Generative%20Novel%20View%20Synthesis%20with%20Multiple%0A%20%20Sparse%20and%20Unposed%20Images&body=Title%3A%20NVComposer%3A%20Boosting%20Generative%20Novel%20View%20Synthesis%20with%20Multiple%0A%20%20Sparse%20and%20Unposed%20Images%0AAuthor%3A%20Lingen%20Li%20and%20Zhaoyang%20Zhang%20and%20Yaowei%20Li%20and%20Jiale%20Xu%20and%20Xiaoyu%20Li%20and%20Wenbo%20Hu%20and%20Weihao%20Cheng%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Recent%20advancements%20in%20generative%20models%20have%20significantly%20improved%20novel%0Aview%20synthesis%20%28NVS%29%20from%20multi-view%20data.%20However%2C%20existing%20methods%20depend%20on%0Aexternal%20multi-view%20alignment%20processes%2C%20such%20as%20explicit%20pose%20estimation%20or%0Apre-reconstruction%2C%20which%20limits%20their%20flexibility%20and%20accessibility%2C%0Aespecially%20when%20alignment%20is%20unstable%20due%20to%20insufficient%20overlap%20or%20occlusions%0Abetween%20views.%20In%20this%20paper%2C%20we%20propose%20NVComposer%2C%20a%20novel%20approach%20that%0Aeliminates%20the%20need%20for%20explicit%20external%20alignment.%20NVComposer%20enables%20the%0Agenerative%20model%20to%20implicitly%20infer%20spatial%20and%20geometric%20relationships%0Abetween%20multiple%20conditional%20views%20by%20introducing%20two%20key%20components%3A%201%29%20an%0Aimage-pose%20dual-stream%20diffusion%20model%20that%20simultaneously%20generates%20target%0Anovel%20views%20and%20condition%20camera%20poses%2C%20and%202%29%20a%20geometry-aware%20feature%0Aalignment%20module%20that%20distills%20geometric%20priors%20from%20dense%20stereo%20models%20during%0Atraining.%20Extensive%20experiments%20demonstrate%20that%20NVComposer%20achieves%0Astate-of-the-art%20performance%20in%20generative%20multi-view%20NVS%20tasks%2C%20removing%20the%0Areliance%20on%20external%20alignment%20and%20thus%20improving%20model%20accessibility.%20Our%0Aapproach%20shows%20substantial%20improvements%20in%20synthesis%20quality%20as%20the%20number%20of%0Aunposed%20input%20views%20increases%2C%20highlighting%20its%20potential%20for%20more%20flexible%20and%0Aaccessible%20generative%20NVS%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNVComposer%253A%2520Boosting%2520Generative%2520Novel%2520View%2520Synthesis%2520with%2520Multiple%250A%2520%2520Sparse%2520and%2520Unposed%2520Images%26entry.906535625%3DLingen%2520Li%2520and%2520Zhaoyang%2520Zhang%2520and%2520Yaowei%2520Li%2520and%2520Jiale%2520Xu%2520and%2520Xiaoyu%2520Li%2520and%2520Wenbo%2520Hu%2520and%2520Weihao%2520Cheng%2520and%2520Jinwei%2520Gu%2520and%2520Tianfan%2520Xue%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520generative%2520models%2520have%2520significantly%2520improved%2520novel%250Aview%2520synthesis%2520%2528NVS%2529%2520from%2520multi-view%2520data.%2520However%252C%2520existing%2520methods%2520depend%2520on%250Aexternal%2520multi-view%2520alignment%2520processes%252C%2520such%2520as%2520explicit%2520pose%2520estimation%2520or%250Apre-reconstruction%252C%2520which%2520limits%2520their%2520flexibility%2520and%2520accessibility%252C%250Aespecially%2520when%2520alignment%2520is%2520unstable%2520due%2520to%2520insufficient%2520overlap%2520or%2520occlusions%250Abetween%2520views.%2520In%2520this%2520paper%252C%2520we%2520propose%2520NVComposer%252C%2520a%2520novel%2520approach%2520that%250Aeliminates%2520the%2520need%2520for%2520explicit%2520external%2520alignment.%2520NVComposer%2520enables%2520the%250Agenerative%2520model%2520to%2520implicitly%2520infer%2520spatial%2520and%2520geometric%2520relationships%250Abetween%2520multiple%2520conditional%2520views%2520by%2520introducing%2520two%2520key%2520components%253A%25201%2529%2520an%250Aimage-pose%2520dual-stream%2520diffusion%2520model%2520that%2520simultaneously%2520generates%2520target%250Anovel%2520views%2520and%2520condition%2520camera%2520poses%252C%2520and%25202%2529%2520a%2520geometry-aware%2520feature%250Aalignment%2520module%2520that%2520distills%2520geometric%2520priors%2520from%2520dense%2520stereo%2520models%2520during%250Atraining.%2520Extensive%2520experiments%2520demonstrate%2520that%2520NVComposer%2520achieves%250Astate-of-the-art%2520performance%2520in%2520generative%2520multi-view%2520NVS%2520tasks%252C%2520removing%2520the%250Areliance%2520on%2520external%2520alignment%2520and%2520thus%2520improving%2520model%2520accessibility.%2520Our%250Aapproach%2520shows%2520substantial%2520improvements%2520in%2520synthesis%2520quality%2520as%2520the%2520number%2520of%250Aunposed%2520input%2520views%2520increases%252C%2520highlighting%2520its%2520potential%2520for%2520more%2520flexible%2520and%250Aaccessible%2520generative%2520NVS%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NVComposer%3A%20Boosting%20Generative%20Novel%20View%20Synthesis%20with%20Multiple%0A%20%20Sparse%20and%20Unposed%20Images&entry.906535625=Lingen%20Li%20and%20Zhaoyang%20Zhang%20and%20Yaowei%20Li%20and%20Jiale%20Xu%20and%20Xiaoyu%20Li%20and%20Wenbo%20Hu%20and%20Weihao%20Cheng%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue%20and%20Ying%20Shan&entry.1292438233=%20%20Recent%20advancements%20in%20generative%20models%20have%20significantly%20improved%20novel%0Aview%20synthesis%20%28NVS%29%20from%20multi-view%20data.%20However%2C%20existing%20methods%20depend%20on%0Aexternal%20multi-view%20alignment%20processes%2C%20such%20as%20explicit%20pose%20estimation%20or%0Apre-reconstruction%2C%20which%20limits%20their%20flexibility%20and%20accessibility%2C%0Aespecially%20when%20alignment%20is%20unstable%20due%20to%20insufficient%20overlap%20or%20occlusions%0Abetween%20views.%20In%20this%20paper%2C%20we%20propose%20NVComposer%2C%20a%20novel%20approach%20that%0Aeliminates%20the%20need%20for%20explicit%20external%20alignment.%20NVComposer%20enables%20the%0Agenerative%20model%20to%20implicitly%20infer%20spatial%20and%20geometric%20relationships%0Abetween%20multiple%20conditional%20views%20by%20introducing%20two%20key%20components%3A%201%29%20an%0Aimage-pose%20dual-stream%20diffusion%20model%20that%20simultaneously%20generates%20target%0Anovel%20views%20and%20condition%20camera%20poses%2C%20and%202%29%20a%20geometry-aware%20feature%0Aalignment%20module%20that%20distills%20geometric%20priors%20from%20dense%20stereo%20models%20during%0Atraining.%20Extensive%20experiments%20demonstrate%20that%20NVComposer%20achieves%0Astate-of-the-art%20performance%20in%20generative%20multi-view%20NVS%20tasks%2C%20removing%20the%0Areliance%20on%20external%20alignment%20and%20thus%20improving%20model%20accessibility.%20Our%0Aapproach%20shows%20substantial%20improvements%20in%20synthesis%20quality%20as%20the%20number%20of%0Aunposed%20input%20views%20increases%2C%20highlighting%20its%20potential%20for%20more%20flexible%20and%0Aaccessible%20generative%20NVS%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03517v1&entry.124074799=Read"},
{"title": "FANAL -- Financial Activity News Alerting Language Modeling Framework", "author": "Urjitkumar Patel and Fang-Chun Yeh and Chinmay Gondhalekar and Hari Nalluri", "abstract": "  In the rapidly evolving financial sector, the accurate and timely\ninterpretation of market news is essential for stakeholders needing to navigate\nunpredictable events. This paper introduces FANAL (Financial Activity News\nAlerting Language Modeling Framework), a specialized BERT-based framework\nengineered for real-time financial event detection and analysis, categorizing\nnews into twelve distinct financial categories. FANAL leverages silver-labeled\ndata processed through XGBoost and employs advanced fine-tuning techniques,\nalongside ORBERT (Odds Ratio BERT), a novel variant of BERT fine-tuned with\nORPO (Odds Ratio Preference Optimization) for superior class-wise probability\ncalibration and alignment with financial event relevance. We evaluate FANAL's\nperformance against leading large language models, including GPT-4o, Llama-3.1\n8B, and Phi-3, demonstrating its superior accuracy and cost efficiency. This\nframework sets a new standard for financial intelligence and responsiveness,\nsignificantly outstripping existing models in both performance and\naffordability.\n", "link": "http://arxiv.org/abs/2412.03527v1", "date": "2024-12-04", "relevancy": 2.3263, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FANAL%20--%20Financial%20Activity%20News%20Alerting%20Language%20Modeling%20Framework&body=Title%3A%20FANAL%20--%20Financial%20Activity%20News%20Alerting%20Language%20Modeling%20Framework%0AAuthor%3A%20Urjitkumar%20Patel%20and%20Fang-Chun%20Yeh%20and%20Chinmay%20Gondhalekar%20and%20Hari%20Nalluri%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20financial%20sector%2C%20the%20accurate%20and%20timely%0Ainterpretation%20of%20market%20news%20is%20essential%20for%20stakeholders%20needing%20to%20navigate%0Aunpredictable%20events.%20This%20paper%20introduces%20FANAL%20%28Financial%20Activity%20News%0AAlerting%20Language%20Modeling%20Framework%29%2C%20a%20specialized%20BERT-based%20framework%0Aengineered%20for%20real-time%20financial%20event%20detection%20and%20analysis%2C%20categorizing%0Anews%20into%20twelve%20distinct%20financial%20categories.%20FANAL%20leverages%20silver-labeled%0Adata%20processed%20through%20XGBoost%20and%20employs%20advanced%20fine-tuning%20techniques%2C%0Aalongside%20ORBERT%20%28Odds%20Ratio%20BERT%29%2C%20a%20novel%20variant%20of%20BERT%20fine-tuned%20with%0AORPO%20%28Odds%20Ratio%20Preference%20Optimization%29%20for%20superior%20class-wise%20probability%0Acalibration%20and%20alignment%20with%20financial%20event%20relevance.%20We%20evaluate%20FANAL%27s%0Aperformance%20against%20leading%20large%20language%20models%2C%20including%20GPT-4o%2C%20Llama-3.1%0A8B%2C%20and%20Phi-3%2C%20demonstrating%20its%20superior%20accuracy%20and%20cost%20efficiency.%20This%0Aframework%20sets%20a%20new%20standard%20for%20financial%20intelligence%20and%20responsiveness%2C%0Asignificantly%20outstripping%20existing%20models%20in%20both%20performance%20and%0Aaffordability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFANAL%2520--%2520Financial%2520Activity%2520News%2520Alerting%2520Language%2520Modeling%2520Framework%26entry.906535625%3DUrjitkumar%2520Patel%2520and%2520Fang-Chun%2520Yeh%2520and%2520Chinmay%2520Gondhalekar%2520and%2520Hari%2520Nalluri%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520financial%2520sector%252C%2520the%2520accurate%2520and%2520timely%250Ainterpretation%2520of%2520market%2520news%2520is%2520essential%2520for%2520stakeholders%2520needing%2520to%2520navigate%250Aunpredictable%2520events.%2520This%2520paper%2520introduces%2520FANAL%2520%2528Financial%2520Activity%2520News%250AAlerting%2520Language%2520Modeling%2520Framework%2529%252C%2520a%2520specialized%2520BERT-based%2520framework%250Aengineered%2520for%2520real-time%2520financial%2520event%2520detection%2520and%2520analysis%252C%2520categorizing%250Anews%2520into%2520twelve%2520distinct%2520financial%2520categories.%2520FANAL%2520leverages%2520silver-labeled%250Adata%2520processed%2520through%2520XGBoost%2520and%2520employs%2520advanced%2520fine-tuning%2520techniques%252C%250Aalongside%2520ORBERT%2520%2528Odds%2520Ratio%2520BERT%2529%252C%2520a%2520novel%2520variant%2520of%2520BERT%2520fine-tuned%2520with%250AORPO%2520%2528Odds%2520Ratio%2520Preference%2520Optimization%2529%2520for%2520superior%2520class-wise%2520probability%250Acalibration%2520and%2520alignment%2520with%2520financial%2520event%2520relevance.%2520We%2520evaluate%2520FANAL%2527s%250Aperformance%2520against%2520leading%2520large%2520language%2520models%252C%2520including%2520GPT-4o%252C%2520Llama-3.1%250A8B%252C%2520and%2520Phi-3%252C%2520demonstrating%2520its%2520superior%2520accuracy%2520and%2520cost%2520efficiency.%2520This%250Aframework%2520sets%2520a%2520new%2520standard%2520for%2520financial%2520intelligence%2520and%2520responsiveness%252C%250Asignificantly%2520outstripping%2520existing%2520models%2520in%2520both%2520performance%2520and%250Aaffordability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FANAL%20--%20Financial%20Activity%20News%20Alerting%20Language%20Modeling%20Framework&entry.906535625=Urjitkumar%20Patel%20and%20Fang-Chun%20Yeh%20and%20Chinmay%20Gondhalekar%20and%20Hari%20Nalluri&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20financial%20sector%2C%20the%20accurate%20and%20timely%0Ainterpretation%20of%20market%20news%20is%20essential%20for%20stakeholders%20needing%20to%20navigate%0Aunpredictable%20events.%20This%20paper%20introduces%20FANAL%20%28Financial%20Activity%20News%0AAlerting%20Language%20Modeling%20Framework%29%2C%20a%20specialized%20BERT-based%20framework%0Aengineered%20for%20real-time%20financial%20event%20detection%20and%20analysis%2C%20categorizing%0Anews%20into%20twelve%20distinct%20financial%20categories.%20FANAL%20leverages%20silver-labeled%0Adata%20processed%20through%20XGBoost%20and%20employs%20advanced%20fine-tuning%20techniques%2C%0Aalongside%20ORBERT%20%28Odds%20Ratio%20BERT%29%2C%20a%20novel%20variant%20of%20BERT%20fine-tuned%20with%0AORPO%20%28Odds%20Ratio%20Preference%20Optimization%29%20for%20superior%20class-wise%20probability%0Acalibration%20and%20alignment%20with%20financial%20event%20relevance.%20We%20evaluate%20FANAL%27s%0Aperformance%20against%20leading%20large%20language%20models%2C%20including%20GPT-4o%2C%20Llama-3.1%0A8B%2C%20and%20Phi-3%2C%20demonstrating%20its%20superior%20accuracy%20and%20cost%20efficiency.%20This%0Aframework%20sets%20a%20new%20standard%20for%20financial%20intelligence%20and%20responsiveness%2C%0Asignificantly%20outstripping%20existing%20models%20in%20both%20performance%20and%0Aaffordability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03527v1&entry.124074799=Read"},
{"title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models", "author": "Mahtab Bigverdi and Zelun Luo and Cheng-Yu Hsieh and Ethan Shen and Dongping Chen and Linda G. Shapiro and Ranjay Krishna", "abstract": "  Multimodal language models (MLMs) still face challenges in fundamental visual\nperception tasks where specialized models excel. Tasks requiring reasoning\nabout 3D structures benefit from depth estimation, and reasoning about 2D\nobject instances benefits from object detection. Yet, MLMs can not produce\nintermediate depth or boxes to reason over. Finetuning MLMs on relevant data\ndoesn't generalize well and outsourcing computation to specialized vision tools\nis too compute-intensive and memory-inefficient. To address this, we introduce\nPerception Tokens, intrinsic image representations designed to assist reasoning\ntasks where language is insufficient. Perception tokens act as auxiliary\nreasoning tokens, akin to chain-of-thought prompts in language models. For\nexample, in a depth-related task, an MLM augmented with perception tokens can\nreason by generating a depth map as tokens, enabling it to solve the problem\neffectively. We propose AURORA, a training method that augments MLMs with\nperception tokens for improved reasoning over visual inputs. AURORA leverages a\nVQVAE to transform intermediate image representations, such as depth maps into\na tokenized format and bounding box tokens, which is then used in a multi-task\ntraining framework. AURORA achieves notable improvements across counting\nbenchmarks: +10.8% on BLINK, +11.3% on CVBench, and +8.3% on SEED-Bench,\noutperforming finetuning approaches in generalization across datasets. It also\nimproves on relative depth: over +6% on BLINK. With perception tokens, AURORA\nexpands the scope of MLMs beyond language-based reasoning, paving the way for\nmore effective visual reasoning capabilities.\n", "link": "http://arxiv.org/abs/2412.03548v1", "date": "2024-12-04", "relevancy": 2.3212, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5876}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception%20Tokens%20Enhance%20Visual%20Reasoning%20in%20Multimodal%20Language%20Models&body=Title%3A%20Perception%20Tokens%20Enhance%20Visual%20Reasoning%20in%20Multimodal%20Language%20Models%0AAuthor%3A%20Mahtab%20Bigverdi%20and%20Zelun%20Luo%20and%20Cheng-Yu%20Hsieh%20and%20Ethan%20Shen%20and%20Dongping%20Chen%20and%20Linda%20G.%20Shapiro%20and%20Ranjay%20Krishna%0AAbstract%3A%20%20%20Multimodal%20language%20models%20%28MLMs%29%20still%20face%20challenges%20in%20fundamental%20visual%0Aperception%20tasks%20where%20specialized%20models%20excel.%20Tasks%20requiring%20reasoning%0Aabout%203D%20structures%20benefit%20from%20depth%20estimation%2C%20and%20reasoning%20about%202D%0Aobject%20instances%20benefits%20from%20object%20detection.%20Yet%2C%20MLMs%20can%20not%20produce%0Aintermediate%20depth%20or%20boxes%20to%20reason%20over.%20Finetuning%20MLMs%20on%20relevant%20data%0Adoesn%27t%20generalize%20well%20and%20outsourcing%20computation%20to%20specialized%20vision%20tools%0Ais%20too%20compute-intensive%20and%20memory-inefficient.%20To%20address%20this%2C%20we%20introduce%0APerception%20Tokens%2C%20intrinsic%20image%20representations%20designed%20to%20assist%20reasoning%0Atasks%20where%20language%20is%20insufficient.%20Perception%20tokens%20act%20as%20auxiliary%0Areasoning%20tokens%2C%20akin%20to%20chain-of-thought%20prompts%20in%20language%20models.%20For%0Aexample%2C%20in%20a%20depth-related%20task%2C%20an%20MLM%20augmented%20with%20perception%20tokens%20can%0Areason%20by%20generating%20a%20depth%20map%20as%20tokens%2C%20enabling%20it%20to%20solve%20the%20problem%0Aeffectively.%20We%20propose%20AURORA%2C%20a%20training%20method%20that%20augments%20MLMs%20with%0Aperception%20tokens%20for%20improved%20reasoning%20over%20visual%20inputs.%20AURORA%20leverages%20a%0AVQVAE%20to%20transform%20intermediate%20image%20representations%2C%20such%20as%20depth%20maps%20into%0Aa%20tokenized%20format%20and%20bounding%20box%20tokens%2C%20which%20is%20then%20used%20in%20a%20multi-task%0Atraining%20framework.%20AURORA%20achieves%20notable%20improvements%20across%20counting%0Abenchmarks%3A%20%2B10.8%25%20on%20BLINK%2C%20%2B11.3%25%20on%20CVBench%2C%20and%20%2B8.3%25%20on%20SEED-Bench%2C%0Aoutperforming%20finetuning%20approaches%20in%20generalization%20across%20datasets.%20It%20also%0Aimproves%20on%20relative%20depth%3A%20over%20%2B6%25%20on%20BLINK.%20With%20perception%20tokens%2C%20AURORA%0Aexpands%20the%20scope%20of%20MLMs%20beyond%20language-based%20reasoning%2C%20paving%20the%20way%20for%0Amore%20effective%20visual%20reasoning%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception%2520Tokens%2520Enhance%2520Visual%2520Reasoning%2520in%2520Multimodal%2520Language%2520Models%26entry.906535625%3DMahtab%2520Bigverdi%2520and%2520Zelun%2520Luo%2520and%2520Cheng-Yu%2520Hsieh%2520and%2520Ethan%2520Shen%2520and%2520Dongping%2520Chen%2520and%2520Linda%2520G.%2520Shapiro%2520and%2520Ranjay%2520Krishna%26entry.1292438233%3D%2520%2520Multimodal%2520language%2520models%2520%2528MLMs%2529%2520still%2520face%2520challenges%2520in%2520fundamental%2520visual%250Aperception%2520tasks%2520where%2520specialized%2520models%2520excel.%2520Tasks%2520requiring%2520reasoning%250Aabout%25203D%2520structures%2520benefit%2520from%2520depth%2520estimation%252C%2520and%2520reasoning%2520about%25202D%250Aobject%2520instances%2520benefits%2520from%2520object%2520detection.%2520Yet%252C%2520MLMs%2520can%2520not%2520produce%250Aintermediate%2520depth%2520or%2520boxes%2520to%2520reason%2520over.%2520Finetuning%2520MLMs%2520on%2520relevant%2520data%250Adoesn%2527t%2520generalize%2520well%2520and%2520outsourcing%2520computation%2520to%2520specialized%2520vision%2520tools%250Ais%2520too%2520compute-intensive%2520and%2520memory-inefficient.%2520To%2520address%2520this%252C%2520we%2520introduce%250APerception%2520Tokens%252C%2520intrinsic%2520image%2520representations%2520designed%2520to%2520assist%2520reasoning%250Atasks%2520where%2520language%2520is%2520insufficient.%2520Perception%2520tokens%2520act%2520as%2520auxiliary%250Areasoning%2520tokens%252C%2520akin%2520to%2520chain-of-thought%2520prompts%2520in%2520language%2520models.%2520For%250Aexample%252C%2520in%2520a%2520depth-related%2520task%252C%2520an%2520MLM%2520augmented%2520with%2520perception%2520tokens%2520can%250Areason%2520by%2520generating%2520a%2520depth%2520map%2520as%2520tokens%252C%2520enabling%2520it%2520to%2520solve%2520the%2520problem%250Aeffectively.%2520We%2520propose%2520AURORA%252C%2520a%2520training%2520method%2520that%2520augments%2520MLMs%2520with%250Aperception%2520tokens%2520for%2520improved%2520reasoning%2520over%2520visual%2520inputs.%2520AURORA%2520leverages%2520a%250AVQVAE%2520to%2520transform%2520intermediate%2520image%2520representations%252C%2520such%2520as%2520depth%2520maps%2520into%250Aa%2520tokenized%2520format%2520and%2520bounding%2520box%2520tokens%252C%2520which%2520is%2520then%2520used%2520in%2520a%2520multi-task%250Atraining%2520framework.%2520AURORA%2520achieves%2520notable%2520improvements%2520across%2520counting%250Abenchmarks%253A%2520%252B10.8%2525%2520on%2520BLINK%252C%2520%252B11.3%2525%2520on%2520CVBench%252C%2520and%2520%252B8.3%2525%2520on%2520SEED-Bench%252C%250Aoutperforming%2520finetuning%2520approaches%2520in%2520generalization%2520across%2520datasets.%2520It%2520also%250Aimproves%2520on%2520relative%2520depth%253A%2520over%2520%252B6%2525%2520on%2520BLINK.%2520With%2520perception%2520tokens%252C%2520AURORA%250Aexpands%2520the%2520scope%2520of%2520MLMs%2520beyond%2520language-based%2520reasoning%252C%2520paving%2520the%2520way%2520for%250Amore%2520effective%2520visual%2520reasoning%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception%20Tokens%20Enhance%20Visual%20Reasoning%20in%20Multimodal%20Language%20Models&entry.906535625=Mahtab%20Bigverdi%20and%20Zelun%20Luo%20and%20Cheng-Yu%20Hsieh%20and%20Ethan%20Shen%20and%20Dongping%20Chen%20and%20Linda%20G.%20Shapiro%20and%20Ranjay%20Krishna&entry.1292438233=%20%20Multimodal%20language%20models%20%28MLMs%29%20still%20face%20challenges%20in%20fundamental%20visual%0Aperception%20tasks%20where%20specialized%20models%20excel.%20Tasks%20requiring%20reasoning%0Aabout%203D%20structures%20benefit%20from%20depth%20estimation%2C%20and%20reasoning%20about%202D%0Aobject%20instances%20benefits%20from%20object%20detection.%20Yet%2C%20MLMs%20can%20not%20produce%0Aintermediate%20depth%20or%20boxes%20to%20reason%20over.%20Finetuning%20MLMs%20on%20relevant%20data%0Adoesn%27t%20generalize%20well%20and%20outsourcing%20computation%20to%20specialized%20vision%20tools%0Ais%20too%20compute-intensive%20and%20memory-inefficient.%20To%20address%20this%2C%20we%20introduce%0APerception%20Tokens%2C%20intrinsic%20image%20representations%20designed%20to%20assist%20reasoning%0Atasks%20where%20language%20is%20insufficient.%20Perception%20tokens%20act%20as%20auxiliary%0Areasoning%20tokens%2C%20akin%20to%20chain-of-thought%20prompts%20in%20language%20models.%20For%0Aexample%2C%20in%20a%20depth-related%20task%2C%20an%20MLM%20augmented%20with%20perception%20tokens%20can%0Areason%20by%20generating%20a%20depth%20map%20as%20tokens%2C%20enabling%20it%20to%20solve%20the%20problem%0Aeffectively.%20We%20propose%20AURORA%2C%20a%20training%20method%20that%20augments%20MLMs%20with%0Aperception%20tokens%20for%20improved%20reasoning%20over%20visual%20inputs.%20AURORA%20leverages%20a%0AVQVAE%20to%20transform%20intermediate%20image%20representations%2C%20such%20as%20depth%20maps%20into%0Aa%20tokenized%20format%20and%20bounding%20box%20tokens%2C%20which%20is%20then%20used%20in%20a%20multi-task%0Atraining%20framework.%20AURORA%20achieves%20notable%20improvements%20across%20counting%0Abenchmarks%3A%20%2B10.8%25%20on%20BLINK%2C%20%2B11.3%25%20on%20CVBench%2C%20and%20%2B8.3%25%20on%20SEED-Bench%2C%0Aoutperforming%20finetuning%20approaches%20in%20generalization%20across%20datasets.%20It%20also%0Aimproves%20on%20relative%20depth%3A%20over%20%2B6%25%20on%20BLINK.%20With%20perception%20tokens%2C%20AURORA%0Aexpands%20the%20scope%20of%20MLMs%20beyond%20language-based%20reasoning%2C%20paving%20the%20way%20for%0Amore%20effective%20visual%20reasoning%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03548v1&entry.124074799=Read"},
{"title": "How to Segment in 3D Using 2D Models: Automated 3D Segmentation of\n  Prostate Cancer Metastatic Lesions on PET Volumes Using Multi-angle Maximum\n  Intensity Projections and Diffusion Models", "author": "Amirhosein Toosi and Sara Harsini and Fran\u00e7ois B\u00e9nard and Carlos Uribe and Arman Rahmim", "abstract": "  Prostate specific membrane antigen (PSMA) positron emission\ntomography/computed tomography (PET/CT) imaging provides a tremendously\nexciting frontier in visualization of prostate cancer (PCa) metastatic lesions.\nHowever, accurate segmentation of metastatic lesions is challenging due to low\nsignal-to-noise ratios and variable sizes, shapes, and locations of the\nlesions. This study proposes a novel approach for automated segmentation of\nmetastatic lesions in PSMA PET/CT 3D volumetric images using 2D denoising\ndiffusion probabilistic models (DDPMs). Instead of 2D trans-axial slices or 3D\nvolumes, the proposed approach segments the lesions on generated multi-angle\nmaximum intensity projections (MA-MIPs) of the PSMA PET images, then obtains\nthe final 3D segmentation masks from 3D ordered subset expectation maximization\n(OSEM) reconstruction of 2D MA-MIPs segmentations. Our proposed method achieved\nsuperior performance compared to state-of-the-art 3D segmentation approaches in\nterms of accuracy and robustness in detecting and segmenting small metastatic\nPCa lesions. The proposed method has significant potential as a tool for\nquantitative analysis of metastatic burden in PCa patients.\n", "link": "http://arxiv.org/abs/2407.18555v3", "date": "2024-12-04", "relevancy": 2.3109, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5858}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5858}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Segment%20in%203D%20Using%202D%20Models%3A%20Automated%203D%20Segmentation%20of%0A%20%20Prostate%20Cancer%20Metastatic%20Lesions%20on%20PET%20Volumes%20Using%20Multi-angle%20Maximum%0A%20%20Intensity%20Projections%20and%20Diffusion%20Models&body=Title%3A%20How%20to%20Segment%20in%203D%20Using%202D%20Models%3A%20Automated%203D%20Segmentation%20of%0A%20%20Prostate%20Cancer%20Metastatic%20Lesions%20on%20PET%20Volumes%20Using%20Multi-angle%20Maximum%0A%20%20Intensity%20Projections%20and%20Diffusion%20Models%0AAuthor%3A%20Amirhosein%20Toosi%20and%20Sara%20Harsini%20and%20Fran%C3%A7ois%20B%C3%A9nard%20and%20Carlos%20Uribe%20and%20Arman%20Rahmim%0AAbstract%3A%20%20%20Prostate%20specific%20membrane%20antigen%20%28PSMA%29%20positron%20emission%0Atomography/computed%20tomography%20%28PET/CT%29%20imaging%20provides%20a%20tremendously%0Aexciting%20frontier%20in%20visualization%20of%20prostate%20cancer%20%28PCa%29%20metastatic%20lesions.%0AHowever%2C%20accurate%20segmentation%20of%20metastatic%20lesions%20is%20challenging%20due%20to%20low%0Asignal-to-noise%20ratios%20and%20variable%20sizes%2C%20shapes%2C%20and%20locations%20of%20the%0Alesions.%20This%20study%20proposes%20a%20novel%20approach%20for%20automated%20segmentation%20of%0Ametastatic%20lesions%20in%20PSMA%20PET/CT%203D%20volumetric%20images%20using%202D%20denoising%0Adiffusion%20probabilistic%20models%20%28DDPMs%29.%20Instead%20of%202D%20trans-axial%20slices%20or%203D%0Avolumes%2C%20the%20proposed%20approach%20segments%20the%20lesions%20on%20generated%20multi-angle%0Amaximum%20intensity%20projections%20%28MA-MIPs%29%20of%20the%20PSMA%20PET%20images%2C%20then%20obtains%0Athe%20final%203D%20segmentation%20masks%20from%203D%20ordered%20subset%20expectation%20maximization%0A%28OSEM%29%20reconstruction%20of%202D%20MA-MIPs%20segmentations.%20Our%20proposed%20method%20achieved%0Asuperior%20performance%20compared%20to%20state-of-the-art%203D%20segmentation%20approaches%20in%0Aterms%20of%20accuracy%20and%20robustness%20in%20detecting%20and%20segmenting%20small%20metastatic%0APCa%20lesions.%20The%20proposed%20method%20has%20significant%20potential%20as%20a%20tool%20for%0Aquantitative%20analysis%20of%20metastatic%20burden%20in%20PCa%20patients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18555v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Segment%2520in%25203D%2520Using%25202D%2520Models%253A%2520Automated%25203D%2520Segmentation%2520of%250A%2520%2520Prostate%2520Cancer%2520Metastatic%2520Lesions%2520on%2520PET%2520Volumes%2520Using%2520Multi-angle%2520Maximum%250A%2520%2520Intensity%2520Projections%2520and%2520Diffusion%2520Models%26entry.906535625%3DAmirhosein%2520Toosi%2520and%2520Sara%2520Harsini%2520and%2520Fran%25C3%25A7ois%2520B%25C3%25A9nard%2520and%2520Carlos%2520Uribe%2520and%2520Arman%2520Rahmim%26entry.1292438233%3D%2520%2520Prostate%2520specific%2520membrane%2520antigen%2520%2528PSMA%2529%2520positron%2520emission%250Atomography/computed%2520tomography%2520%2528PET/CT%2529%2520imaging%2520provides%2520a%2520tremendously%250Aexciting%2520frontier%2520in%2520visualization%2520of%2520prostate%2520cancer%2520%2528PCa%2529%2520metastatic%2520lesions.%250AHowever%252C%2520accurate%2520segmentation%2520of%2520metastatic%2520lesions%2520is%2520challenging%2520due%2520to%2520low%250Asignal-to-noise%2520ratios%2520and%2520variable%2520sizes%252C%2520shapes%252C%2520and%2520locations%2520of%2520the%250Alesions.%2520This%2520study%2520proposes%2520a%2520novel%2520approach%2520for%2520automated%2520segmentation%2520of%250Ametastatic%2520lesions%2520in%2520PSMA%2520PET/CT%25203D%2520volumetric%2520images%2520using%25202D%2520denoising%250Adiffusion%2520probabilistic%2520models%2520%2528DDPMs%2529.%2520Instead%2520of%25202D%2520trans-axial%2520slices%2520or%25203D%250Avolumes%252C%2520the%2520proposed%2520approach%2520segments%2520the%2520lesions%2520on%2520generated%2520multi-angle%250Amaximum%2520intensity%2520projections%2520%2528MA-MIPs%2529%2520of%2520the%2520PSMA%2520PET%2520images%252C%2520then%2520obtains%250Athe%2520final%25203D%2520segmentation%2520masks%2520from%25203D%2520ordered%2520subset%2520expectation%2520maximization%250A%2528OSEM%2529%2520reconstruction%2520of%25202D%2520MA-MIPs%2520segmentations.%2520Our%2520proposed%2520method%2520achieved%250Asuperior%2520performance%2520compared%2520to%2520state-of-the-art%25203D%2520segmentation%2520approaches%2520in%250Aterms%2520of%2520accuracy%2520and%2520robustness%2520in%2520detecting%2520and%2520segmenting%2520small%2520metastatic%250APCa%2520lesions.%2520The%2520proposed%2520method%2520has%2520significant%2520potential%2520as%2520a%2520tool%2520for%250Aquantitative%2520analysis%2520of%2520metastatic%2520burden%2520in%2520PCa%2520patients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18555v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Segment%20in%203D%20Using%202D%20Models%3A%20Automated%203D%20Segmentation%20of%0A%20%20Prostate%20Cancer%20Metastatic%20Lesions%20on%20PET%20Volumes%20Using%20Multi-angle%20Maximum%0A%20%20Intensity%20Projections%20and%20Diffusion%20Models&entry.906535625=Amirhosein%20Toosi%20and%20Sara%20Harsini%20and%20Fran%C3%A7ois%20B%C3%A9nard%20and%20Carlos%20Uribe%20and%20Arman%20Rahmim&entry.1292438233=%20%20Prostate%20specific%20membrane%20antigen%20%28PSMA%29%20positron%20emission%0Atomography/computed%20tomography%20%28PET/CT%29%20imaging%20provides%20a%20tremendously%0Aexciting%20frontier%20in%20visualization%20of%20prostate%20cancer%20%28PCa%29%20metastatic%20lesions.%0AHowever%2C%20accurate%20segmentation%20of%20metastatic%20lesions%20is%20challenging%20due%20to%20low%0Asignal-to-noise%20ratios%20and%20variable%20sizes%2C%20shapes%2C%20and%20locations%20of%20the%0Alesions.%20This%20study%20proposes%20a%20novel%20approach%20for%20automated%20segmentation%20of%0Ametastatic%20lesions%20in%20PSMA%20PET/CT%203D%20volumetric%20images%20using%202D%20denoising%0Adiffusion%20probabilistic%20models%20%28DDPMs%29.%20Instead%20of%202D%20trans-axial%20slices%20or%203D%0Avolumes%2C%20the%20proposed%20approach%20segments%20the%20lesions%20on%20generated%20multi-angle%0Amaximum%20intensity%20projections%20%28MA-MIPs%29%20of%20the%20PSMA%20PET%20images%2C%20then%20obtains%0Athe%20final%203D%20segmentation%20masks%20from%203D%20ordered%20subset%20expectation%20maximization%0A%28OSEM%29%20reconstruction%20of%202D%20MA-MIPs%20segmentations.%20Our%20proposed%20method%20achieved%0Asuperior%20performance%20compared%20to%20state-of-the-art%203D%20segmentation%20approaches%20in%0Aterms%20of%20accuracy%20and%20robustness%20in%20detecting%20and%20segmenting%20small%20metastatic%0APCa%20lesions.%20The%20proposed%20method%20has%20significant%20potential%20as%20a%20tool%20for%0Aquantitative%20analysis%20of%20metastatic%20burden%20in%20PCa%20patients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18555v3&entry.124074799=Read"},
{"title": "Mapping using Transformers for Volumes -- Network for Super-Resolution\n  with Long-Range Interactions", "author": "August Leander H\u00f8eg and Sophia W. Bardenfleth and Hans Martin Kjer and Tim B. Dyrby and Vedrana Andersen Dahl and Anders Dahl", "abstract": "  Until now, it has been difficult for volumetric super-resolution to utilize\nthe recent advances in transformer-based models seen in 2D super-resolution.\nThe memory required for self-attention in 3D volumes limits the receptive\nfield. Therefore, long-range interactions are not used in 3D to the extent done\nin 2D and the strength of transformers is not realized. We propose a\nmulti-scale transformer-based model based on hierarchical attention blocks\ncombined with carrier tokens at multiple scales to overcome this. Here\ninformation from larger regions at coarse resolution is sequentially carried on\nto finer-resolution regions to predict the super-resolved image. Using\ntransformer layers at each resolution, our coarse-to-fine modeling limits the\nnumber of tokens at each scale and enables attention over larger regions than\nwhat has previously been possible. We experimentally compare our method,\nMTVNet, against state-of-the-art volumetric super-resolution models on five 3D\ndatasets demonstrating the advantage of an increased receptive field. This\nadvantage is especially pronounced for images that are larger than what is seen\nin popularly used 3D datasets. Our code is available at\nhttps://github.com/AugustHoeg/MTVNet\n", "link": "http://arxiv.org/abs/2412.03379v1", "date": "2024-12-04", "relevancy": 2.3067, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6289}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5674}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mapping%20using%20Transformers%20for%20Volumes%20--%20Network%20for%20Super-Resolution%0A%20%20with%20Long-Range%20Interactions&body=Title%3A%20Mapping%20using%20Transformers%20for%20Volumes%20--%20Network%20for%20Super-Resolution%0A%20%20with%20Long-Range%20Interactions%0AAuthor%3A%20August%20Leander%20H%C3%B8eg%20and%20Sophia%20W.%20Bardenfleth%20and%20Hans%20Martin%20Kjer%20and%20Tim%20B.%20Dyrby%20and%20Vedrana%20Andersen%20Dahl%20and%20Anders%20Dahl%0AAbstract%3A%20%20%20Until%20now%2C%20it%20has%20been%20difficult%20for%20volumetric%20super-resolution%20to%20utilize%0Athe%20recent%20advances%20in%20transformer-based%20models%20seen%20in%202D%20super-resolution.%0AThe%20memory%20required%20for%20self-attention%20in%203D%20volumes%20limits%20the%20receptive%0Afield.%20Therefore%2C%20long-range%20interactions%20are%20not%20used%20in%203D%20to%20the%20extent%20done%0Ain%202D%20and%20the%20strength%20of%20transformers%20is%20not%20realized.%20We%20propose%20a%0Amulti-scale%20transformer-based%20model%20based%20on%20hierarchical%20attention%20blocks%0Acombined%20with%20carrier%20tokens%20at%20multiple%20scales%20to%20overcome%20this.%20Here%0Ainformation%20from%20larger%20regions%20at%20coarse%20resolution%20is%20sequentially%20carried%20on%0Ato%20finer-resolution%20regions%20to%20predict%20the%20super-resolved%20image.%20Using%0Atransformer%20layers%20at%20each%20resolution%2C%20our%20coarse-to-fine%20modeling%20limits%20the%0Anumber%20of%20tokens%20at%20each%20scale%20and%20enables%20attention%20over%20larger%20regions%20than%0Awhat%20has%20previously%20been%20possible.%20We%20experimentally%20compare%20our%20method%2C%0AMTVNet%2C%20against%20state-of-the-art%20volumetric%20super-resolution%20models%20on%20five%203D%0Adatasets%20demonstrating%20the%20advantage%20of%20an%20increased%20receptive%20field.%20This%0Aadvantage%20is%20especially%20pronounced%20for%20images%20that%20are%20larger%20than%20what%20is%20seen%0Ain%20popularly%20used%203D%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/AugustHoeg/MTVNet%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapping%2520using%2520Transformers%2520for%2520Volumes%2520--%2520Network%2520for%2520Super-Resolution%250A%2520%2520with%2520Long-Range%2520Interactions%26entry.906535625%3DAugust%2520Leander%2520H%25C3%25B8eg%2520and%2520Sophia%2520W.%2520Bardenfleth%2520and%2520Hans%2520Martin%2520Kjer%2520and%2520Tim%2520B.%2520Dyrby%2520and%2520Vedrana%2520Andersen%2520Dahl%2520and%2520Anders%2520Dahl%26entry.1292438233%3D%2520%2520Until%2520now%252C%2520it%2520has%2520been%2520difficult%2520for%2520volumetric%2520super-resolution%2520to%2520utilize%250Athe%2520recent%2520advances%2520in%2520transformer-based%2520models%2520seen%2520in%25202D%2520super-resolution.%250AThe%2520memory%2520required%2520for%2520self-attention%2520in%25203D%2520volumes%2520limits%2520the%2520receptive%250Afield.%2520Therefore%252C%2520long-range%2520interactions%2520are%2520not%2520used%2520in%25203D%2520to%2520the%2520extent%2520done%250Ain%25202D%2520and%2520the%2520strength%2520of%2520transformers%2520is%2520not%2520realized.%2520We%2520propose%2520a%250Amulti-scale%2520transformer-based%2520model%2520based%2520on%2520hierarchical%2520attention%2520blocks%250Acombined%2520with%2520carrier%2520tokens%2520at%2520multiple%2520scales%2520to%2520overcome%2520this.%2520Here%250Ainformation%2520from%2520larger%2520regions%2520at%2520coarse%2520resolution%2520is%2520sequentially%2520carried%2520on%250Ato%2520finer-resolution%2520regions%2520to%2520predict%2520the%2520super-resolved%2520image.%2520Using%250Atransformer%2520layers%2520at%2520each%2520resolution%252C%2520our%2520coarse-to-fine%2520modeling%2520limits%2520the%250Anumber%2520of%2520tokens%2520at%2520each%2520scale%2520and%2520enables%2520attention%2520over%2520larger%2520regions%2520than%250Awhat%2520has%2520previously%2520been%2520possible.%2520We%2520experimentally%2520compare%2520our%2520method%252C%250AMTVNet%252C%2520against%2520state-of-the-art%2520volumetric%2520super-resolution%2520models%2520on%2520five%25203D%250Adatasets%2520demonstrating%2520the%2520advantage%2520of%2520an%2520increased%2520receptive%2520field.%2520This%250Aadvantage%2520is%2520especially%2520pronounced%2520for%2520images%2520that%2520are%2520larger%2520than%2520what%2520is%2520seen%250Ain%2520popularly%2520used%25203D%2520datasets.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/AugustHoeg/MTVNet%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mapping%20using%20Transformers%20for%20Volumes%20--%20Network%20for%20Super-Resolution%0A%20%20with%20Long-Range%20Interactions&entry.906535625=August%20Leander%20H%C3%B8eg%20and%20Sophia%20W.%20Bardenfleth%20and%20Hans%20Martin%20Kjer%20and%20Tim%20B.%20Dyrby%20and%20Vedrana%20Andersen%20Dahl%20and%20Anders%20Dahl&entry.1292438233=%20%20Until%20now%2C%20it%20has%20been%20difficult%20for%20volumetric%20super-resolution%20to%20utilize%0Athe%20recent%20advances%20in%20transformer-based%20models%20seen%20in%202D%20super-resolution.%0AThe%20memory%20required%20for%20self-attention%20in%203D%20volumes%20limits%20the%20receptive%0Afield.%20Therefore%2C%20long-range%20interactions%20are%20not%20used%20in%203D%20to%20the%20extent%20done%0Ain%202D%20and%20the%20strength%20of%20transformers%20is%20not%20realized.%20We%20propose%20a%0Amulti-scale%20transformer-based%20model%20based%20on%20hierarchical%20attention%20blocks%0Acombined%20with%20carrier%20tokens%20at%20multiple%20scales%20to%20overcome%20this.%20Here%0Ainformation%20from%20larger%20regions%20at%20coarse%20resolution%20is%20sequentially%20carried%20on%0Ato%20finer-resolution%20regions%20to%20predict%20the%20super-resolved%20image.%20Using%0Atransformer%20layers%20at%20each%20resolution%2C%20our%20coarse-to-fine%20modeling%20limits%20the%0Anumber%20of%20tokens%20at%20each%20scale%20and%20enables%20attention%20over%20larger%20regions%20than%0Awhat%20has%20previously%20been%20possible.%20We%20experimentally%20compare%20our%20method%2C%0AMTVNet%2C%20against%20state-of-the-art%20volumetric%20super-resolution%20models%20on%20five%203D%0Adatasets%20demonstrating%20the%20advantage%20of%20an%20increased%20receptive%20field.%20This%0Aadvantage%20is%20especially%20pronounced%20for%20images%20that%20are%20larger%20than%20what%20is%20seen%0Ain%20popularly%20used%203D%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/AugustHoeg/MTVNet%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03379v1&entry.124074799=Read"},
{"title": "IRisPath: Enhancing Off-Road Navigation with Robust IR-RGB Fusion for\n  Improved Day and Night Traversability", "author": "Saksham Sharma and Akshit Raizada and Suresh Sundaram", "abstract": "  Autonomous off-road navigation is required for applications in agriculture,\nconstruction, search and rescue and defence. Traditional on-road autonomous\nmethods struggle with dynamic terrains, leading to poor vehicle control on\noff-road. Recent deep-learning models have used perception sensors along with\nkinesthetic feedback for navigation on such terrains. However, this approach\nhas out-of-domain uncertainty. Factors like change in weather and time of day\nimpacts the performance of the model. We propose a multi modal fusion network\nFuseIsPath capable of using LWIR and RGB images to provide robustness against\ndynamic weather and light conditions. To aid further works in this domain, we\nalso open-source a day-night dataset with LWIR and RGB images along with\npseudo-labels for traversability. In order to co-register the two images we\ndeveloped a novel method for targetless extrinsic calibration of LWIR, LiDAR\nand RGB cameras with translation accuracy of 1.7cm and rotation accuracy of\n0.827degree.\n", "link": "http://arxiv.org/abs/2412.03173v1", "date": "2024-12-04", "relevancy": 2.3034, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6149}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5691}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRisPath%3A%20Enhancing%20Off-Road%20Navigation%20with%20Robust%20IR-RGB%20Fusion%20for%0A%20%20Improved%20Day%20and%20Night%20Traversability&body=Title%3A%20IRisPath%3A%20Enhancing%20Off-Road%20Navigation%20with%20Robust%20IR-RGB%20Fusion%20for%0A%20%20Improved%20Day%20and%20Night%20Traversability%0AAuthor%3A%20Saksham%20Sharma%20and%20Akshit%20Raizada%20and%20Suresh%20Sundaram%0AAbstract%3A%20%20%20Autonomous%20off-road%20navigation%20is%20required%20for%20applications%20in%20agriculture%2C%0Aconstruction%2C%20search%20and%20rescue%20and%20defence.%20Traditional%20on-road%20autonomous%0Amethods%20struggle%20with%20dynamic%20terrains%2C%20leading%20to%20poor%20vehicle%20control%20on%0Aoff-road.%20Recent%20deep-learning%20models%20have%20used%20perception%20sensors%20along%20with%0Akinesthetic%20feedback%20for%20navigation%20on%20such%20terrains.%20However%2C%20this%20approach%0Ahas%20out-of-domain%20uncertainty.%20Factors%20like%20change%20in%20weather%20and%20time%20of%20day%0Aimpacts%20the%20performance%20of%20the%20model.%20We%20propose%20a%20multi%20modal%20fusion%20network%0AFuseIsPath%20capable%20of%20using%20LWIR%20and%20RGB%20images%20to%20provide%20robustness%20against%0Adynamic%20weather%20and%20light%20conditions.%20To%20aid%20further%20works%20in%20this%20domain%2C%20we%0Aalso%20open-source%20a%20day-night%20dataset%20with%20LWIR%20and%20RGB%20images%20along%20with%0Apseudo-labels%20for%20traversability.%20In%20order%20to%20co-register%20the%20two%20images%20we%0Adeveloped%20a%20novel%20method%20for%20targetless%20extrinsic%20calibration%20of%20LWIR%2C%20LiDAR%0Aand%20RGB%20cameras%20with%20translation%20accuracy%20of%201.7cm%20and%20rotation%20accuracy%20of%0A0.827degree.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRisPath%253A%2520Enhancing%2520Off-Road%2520Navigation%2520with%2520Robust%2520IR-RGB%2520Fusion%2520for%250A%2520%2520Improved%2520Day%2520and%2520Night%2520Traversability%26entry.906535625%3DSaksham%2520Sharma%2520and%2520Akshit%2520Raizada%2520and%2520Suresh%2520Sundaram%26entry.1292438233%3D%2520%2520Autonomous%2520off-road%2520navigation%2520is%2520required%2520for%2520applications%2520in%2520agriculture%252C%250Aconstruction%252C%2520search%2520and%2520rescue%2520and%2520defence.%2520Traditional%2520on-road%2520autonomous%250Amethods%2520struggle%2520with%2520dynamic%2520terrains%252C%2520leading%2520to%2520poor%2520vehicle%2520control%2520on%250Aoff-road.%2520Recent%2520deep-learning%2520models%2520have%2520used%2520perception%2520sensors%2520along%2520with%250Akinesthetic%2520feedback%2520for%2520navigation%2520on%2520such%2520terrains.%2520However%252C%2520this%2520approach%250Ahas%2520out-of-domain%2520uncertainty.%2520Factors%2520like%2520change%2520in%2520weather%2520and%2520time%2520of%2520day%250Aimpacts%2520the%2520performance%2520of%2520the%2520model.%2520We%2520propose%2520a%2520multi%2520modal%2520fusion%2520network%250AFuseIsPath%2520capable%2520of%2520using%2520LWIR%2520and%2520RGB%2520images%2520to%2520provide%2520robustness%2520against%250Adynamic%2520weather%2520and%2520light%2520conditions.%2520To%2520aid%2520further%2520works%2520in%2520this%2520domain%252C%2520we%250Aalso%2520open-source%2520a%2520day-night%2520dataset%2520with%2520LWIR%2520and%2520RGB%2520images%2520along%2520with%250Apseudo-labels%2520for%2520traversability.%2520In%2520order%2520to%2520co-register%2520the%2520two%2520images%2520we%250Adeveloped%2520a%2520novel%2520method%2520for%2520targetless%2520extrinsic%2520calibration%2520of%2520LWIR%252C%2520LiDAR%250Aand%2520RGB%2520cameras%2520with%2520translation%2520accuracy%2520of%25201.7cm%2520and%2520rotation%2520accuracy%2520of%250A0.827degree.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRisPath%3A%20Enhancing%20Off-Road%20Navigation%20with%20Robust%20IR-RGB%20Fusion%20for%0A%20%20Improved%20Day%20and%20Night%20Traversability&entry.906535625=Saksham%20Sharma%20and%20Akshit%20Raizada%20and%20Suresh%20Sundaram&entry.1292438233=%20%20Autonomous%20off-road%20navigation%20is%20required%20for%20applications%20in%20agriculture%2C%0Aconstruction%2C%20search%20and%20rescue%20and%20defence.%20Traditional%20on-road%20autonomous%0Amethods%20struggle%20with%20dynamic%20terrains%2C%20leading%20to%20poor%20vehicle%20control%20on%0Aoff-road.%20Recent%20deep-learning%20models%20have%20used%20perception%20sensors%20along%20with%0Akinesthetic%20feedback%20for%20navigation%20on%20such%20terrains.%20However%2C%20this%20approach%0Ahas%20out-of-domain%20uncertainty.%20Factors%20like%20change%20in%20weather%20and%20time%20of%20day%0Aimpacts%20the%20performance%20of%20the%20model.%20We%20propose%20a%20multi%20modal%20fusion%20network%0AFuseIsPath%20capable%20of%20using%20LWIR%20and%20RGB%20images%20to%20provide%20robustness%20against%0Adynamic%20weather%20and%20light%20conditions.%20To%20aid%20further%20works%20in%20this%20domain%2C%20we%0Aalso%20open-source%20a%20day-night%20dataset%20with%20LWIR%20and%20RGB%20images%20along%20with%0Apseudo-labels%20for%20traversability.%20In%20order%20to%20co-register%20the%20two%20images%20we%0Adeveloped%20a%20novel%20method%20for%20targetless%20extrinsic%20calibration%20of%20LWIR%2C%20LiDAR%0Aand%20RGB%20cameras%20with%20translation%20accuracy%20of%201.7cm%20and%20rotation%20accuracy%20of%0A0.827degree.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03173v1&entry.124074799=Read"},
{"title": "Best-of-N Jailbreaking", "author": "John Hughes and Sara Price and Aengus Lynch and Rylan Schaeffer and Fazl Barez and Sanmi Koyejo and Henry Sleight and Erik Jones and Ethan Perez and Mrinank Sharma", "abstract": "  We introduce Best-of-N (BoN) Jailbreaking, a simple black-box algorithm that\njailbreaks frontier AI systems across modalities. BoN Jailbreaking works by\nrepeatedly sampling variations of a prompt with a combination of augmentations\n- such as random shuffling or capitalization for textual prompts - until a\nharmful response is elicited. We find that BoN Jailbreaking achieves high\nattack success rates (ASRs) on closed-source language models, such as 89% on\nGPT-4o and 78% on Claude 3.5 Sonnet when sampling 10,000 augmented prompts.\nFurther, it is similarly effective at circumventing state-of-the-art\nopen-source defenses like circuit breakers. BoN also seamlessly extends to\nother modalities: it jailbreaks vision language models (VLMs) such as GPT-4o\nand audio language models (ALMs) like Gemini 1.5 Pro, using modality-specific\naugmentations. BoN reliably improves when we sample more augmented prompts.\nAcross all modalities, ASR, as a function of the number of samples (N),\nempirically follows power-law-like behavior for many orders of magnitude. BoN\nJailbreaking can also be composed with other black-box algorithms for even more\neffective attacks - combining BoN with an optimized prefix attack achieves up\nto a 35% increase in ASR. Overall, our work indicates that, despite their\ncapability, language models are sensitive to seemingly innocuous changes to\ninputs, which attackers can exploit across modalities.\n", "link": "http://arxiv.org/abs/2412.03556v1", "date": "2024-12-04", "relevancy": 2.3007, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Best-of-N%20Jailbreaking&body=Title%3A%20Best-of-N%20Jailbreaking%0AAuthor%3A%20John%20Hughes%20and%20Sara%20Price%20and%20Aengus%20Lynch%20and%20Rylan%20Schaeffer%20and%20Fazl%20Barez%20and%20Sanmi%20Koyejo%20and%20Henry%20Sleight%20and%20Erik%20Jones%20and%20Ethan%20Perez%20and%20Mrinank%20Sharma%0AAbstract%3A%20%20%20We%20introduce%20Best-of-N%20%28BoN%29%20Jailbreaking%2C%20a%20simple%20black-box%20algorithm%20that%0Ajailbreaks%20frontier%20AI%20systems%20across%20modalities.%20BoN%20Jailbreaking%20works%20by%0Arepeatedly%20sampling%20variations%20of%20a%20prompt%20with%20a%20combination%20of%20augmentations%0A-%20such%20as%20random%20shuffling%20or%20capitalization%20for%20textual%20prompts%20-%20until%20a%0Aharmful%20response%20is%20elicited.%20We%20find%20that%20BoN%20Jailbreaking%20achieves%20high%0Aattack%20success%20rates%20%28ASRs%29%20on%20closed-source%20language%20models%2C%20such%20as%2089%25%20on%0AGPT-4o%20and%2078%25%20on%20Claude%203.5%20Sonnet%20when%20sampling%2010%2C000%20augmented%20prompts.%0AFurther%2C%20it%20is%20similarly%20effective%20at%20circumventing%20state-of-the-art%0Aopen-source%20defenses%20like%20circuit%20breakers.%20BoN%20also%20seamlessly%20extends%20to%0Aother%20modalities%3A%20it%20jailbreaks%20vision%20language%20models%20%28VLMs%29%20such%20as%20GPT-4o%0Aand%20audio%20language%20models%20%28ALMs%29%20like%20Gemini%201.5%20Pro%2C%20using%20modality-specific%0Aaugmentations.%20BoN%20reliably%20improves%20when%20we%20sample%20more%20augmented%20prompts.%0AAcross%20all%20modalities%2C%20ASR%2C%20as%20a%20function%20of%20the%20number%20of%20samples%20%28N%29%2C%0Aempirically%20follows%20power-law-like%20behavior%20for%20many%20orders%20of%20magnitude.%20BoN%0AJailbreaking%20can%20also%20be%20composed%20with%20other%20black-box%20algorithms%20for%20even%20more%0Aeffective%20attacks%20-%20combining%20BoN%20with%20an%20optimized%20prefix%20attack%20achieves%20up%0Ato%20a%2035%25%20increase%20in%20ASR.%20Overall%2C%20our%20work%20indicates%20that%2C%20despite%20their%0Acapability%2C%20language%20models%20are%20sensitive%20to%20seemingly%20innocuous%20changes%20to%0Ainputs%2C%20which%20attackers%20can%20exploit%20across%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBest-of-N%2520Jailbreaking%26entry.906535625%3DJohn%2520Hughes%2520and%2520Sara%2520Price%2520and%2520Aengus%2520Lynch%2520and%2520Rylan%2520Schaeffer%2520and%2520Fazl%2520Barez%2520and%2520Sanmi%2520Koyejo%2520and%2520Henry%2520Sleight%2520and%2520Erik%2520Jones%2520and%2520Ethan%2520Perez%2520and%2520Mrinank%2520Sharma%26entry.1292438233%3D%2520%2520We%2520introduce%2520Best-of-N%2520%2528BoN%2529%2520Jailbreaking%252C%2520a%2520simple%2520black-box%2520algorithm%2520that%250Ajailbreaks%2520frontier%2520AI%2520systems%2520across%2520modalities.%2520BoN%2520Jailbreaking%2520works%2520by%250Arepeatedly%2520sampling%2520variations%2520of%2520a%2520prompt%2520with%2520a%2520combination%2520of%2520augmentations%250A-%2520such%2520as%2520random%2520shuffling%2520or%2520capitalization%2520for%2520textual%2520prompts%2520-%2520until%2520a%250Aharmful%2520response%2520is%2520elicited.%2520We%2520find%2520that%2520BoN%2520Jailbreaking%2520achieves%2520high%250Aattack%2520success%2520rates%2520%2528ASRs%2529%2520on%2520closed-source%2520language%2520models%252C%2520such%2520as%252089%2525%2520on%250AGPT-4o%2520and%252078%2525%2520on%2520Claude%25203.5%2520Sonnet%2520when%2520sampling%252010%252C000%2520augmented%2520prompts.%250AFurther%252C%2520it%2520is%2520similarly%2520effective%2520at%2520circumventing%2520state-of-the-art%250Aopen-source%2520defenses%2520like%2520circuit%2520breakers.%2520BoN%2520also%2520seamlessly%2520extends%2520to%250Aother%2520modalities%253A%2520it%2520jailbreaks%2520vision%2520language%2520models%2520%2528VLMs%2529%2520such%2520as%2520GPT-4o%250Aand%2520audio%2520language%2520models%2520%2528ALMs%2529%2520like%2520Gemini%25201.5%2520Pro%252C%2520using%2520modality-specific%250Aaugmentations.%2520BoN%2520reliably%2520improves%2520when%2520we%2520sample%2520more%2520augmented%2520prompts.%250AAcross%2520all%2520modalities%252C%2520ASR%252C%2520as%2520a%2520function%2520of%2520the%2520number%2520of%2520samples%2520%2528N%2529%252C%250Aempirically%2520follows%2520power-law-like%2520behavior%2520for%2520many%2520orders%2520of%2520magnitude.%2520BoN%250AJailbreaking%2520can%2520also%2520be%2520composed%2520with%2520other%2520black-box%2520algorithms%2520for%2520even%2520more%250Aeffective%2520attacks%2520-%2520combining%2520BoN%2520with%2520an%2520optimized%2520prefix%2520attack%2520achieves%2520up%250Ato%2520a%252035%2525%2520increase%2520in%2520ASR.%2520Overall%252C%2520our%2520work%2520indicates%2520that%252C%2520despite%2520their%250Acapability%252C%2520language%2520models%2520are%2520sensitive%2520to%2520seemingly%2520innocuous%2520changes%2520to%250Ainputs%252C%2520which%2520attackers%2520can%2520exploit%2520across%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Best-of-N%20Jailbreaking&entry.906535625=John%20Hughes%20and%20Sara%20Price%20and%20Aengus%20Lynch%20and%20Rylan%20Schaeffer%20and%20Fazl%20Barez%20and%20Sanmi%20Koyejo%20and%20Henry%20Sleight%20and%20Erik%20Jones%20and%20Ethan%20Perez%20and%20Mrinank%20Sharma&entry.1292438233=%20%20We%20introduce%20Best-of-N%20%28BoN%29%20Jailbreaking%2C%20a%20simple%20black-box%20algorithm%20that%0Ajailbreaks%20frontier%20AI%20systems%20across%20modalities.%20BoN%20Jailbreaking%20works%20by%0Arepeatedly%20sampling%20variations%20of%20a%20prompt%20with%20a%20combination%20of%20augmentations%0A-%20such%20as%20random%20shuffling%20or%20capitalization%20for%20textual%20prompts%20-%20until%20a%0Aharmful%20response%20is%20elicited.%20We%20find%20that%20BoN%20Jailbreaking%20achieves%20high%0Aattack%20success%20rates%20%28ASRs%29%20on%20closed-source%20language%20models%2C%20such%20as%2089%25%20on%0AGPT-4o%20and%2078%25%20on%20Claude%203.5%20Sonnet%20when%20sampling%2010%2C000%20augmented%20prompts.%0AFurther%2C%20it%20is%20similarly%20effective%20at%20circumventing%20state-of-the-art%0Aopen-source%20defenses%20like%20circuit%20breakers.%20BoN%20also%20seamlessly%20extends%20to%0Aother%20modalities%3A%20it%20jailbreaks%20vision%20language%20models%20%28VLMs%29%20such%20as%20GPT-4o%0Aand%20audio%20language%20models%20%28ALMs%29%20like%20Gemini%201.5%20Pro%2C%20using%20modality-specific%0Aaugmentations.%20BoN%20reliably%20improves%20when%20we%20sample%20more%20augmented%20prompts.%0AAcross%20all%20modalities%2C%20ASR%2C%20as%20a%20function%20of%20the%20number%20of%20samples%20%28N%29%2C%0Aempirically%20follows%20power-law-like%20behavior%20for%20many%20orders%20of%20magnitude.%20BoN%0AJailbreaking%20can%20also%20be%20composed%20with%20other%20black-box%20algorithms%20for%20even%20more%0Aeffective%20attacks%20-%20combining%20BoN%20with%20an%20optimized%20prefix%20attack%20achieves%20up%0Ato%20a%2035%25%20increase%20in%20ASR.%20Overall%2C%20our%20work%20indicates%20that%2C%20despite%20their%0Acapability%2C%20language%20models%20are%20sensitive%20to%20seemingly%20innocuous%20changes%20to%0Ainputs%2C%20which%20attackers%20can%20exploit%20across%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03556v1&entry.124074799=Read"},
{"title": "Navigation World Models", "author": "Amir Bar and Gaoyue Zhou and Danny Tran and Trevor Darrell and Yann LeCun", "abstract": "  Navigation is a fundamental skill of agents with visual-motor capabilities.\nWe introduce a Navigation World Model (NWM), a controllable video generation\nmodel that predicts future visual observations based on past observations and\nnavigation actions. To capture complex environment dynamics, NWM employs a\nConditional Diffusion Transformer (CDiT), trained on a diverse collection of\negocentric videos of both human and robotic agents, and scaled up to 1 billion\nparameters. In familiar environments, NWM can plan navigation trajectories by\nsimulating them and evaluating whether they achieve the desired goal. Unlike\nsupervised navigation policies with fixed behavior, NWM can dynamically\nincorporate constraints during planning. Experiments demonstrate its\neffectiveness in planning trajectories from scratch or by ranking trajectories\nsampled from an external policy. Furthermore, NWM leverages its learned visual\npriors to imagine trajectories in unfamiliar environments from a single input\nimage, making it a flexible and powerful tool for next-generation navigation\nsystems.\n", "link": "http://arxiv.org/abs/2412.03572v1", "date": "2024-12-04", "relevancy": 2.2646, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5894}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5544}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigation%20World%20Models&body=Title%3A%20Navigation%20World%20Models%0AAuthor%3A%20Amir%20Bar%20and%20Gaoyue%20Zhou%20and%20Danny%20Tran%20and%20Trevor%20Darrell%20and%20Yann%20LeCun%0AAbstract%3A%20%20%20Navigation%20is%20a%20fundamental%20skill%20of%20agents%20with%20visual-motor%20capabilities.%0AWe%20introduce%20a%20Navigation%20World%20Model%20%28NWM%29%2C%20a%20controllable%20video%20generation%0Amodel%20that%20predicts%20future%20visual%20observations%20based%20on%20past%20observations%20and%0Anavigation%20actions.%20To%20capture%20complex%20environment%20dynamics%2C%20NWM%20employs%20a%0AConditional%20Diffusion%20Transformer%20%28CDiT%29%2C%20trained%20on%20a%20diverse%20collection%20of%0Aegocentric%20videos%20of%20both%20human%20and%20robotic%20agents%2C%20and%20scaled%20up%20to%201%20billion%0Aparameters.%20In%20familiar%20environments%2C%20NWM%20can%20plan%20navigation%20trajectories%20by%0Asimulating%20them%20and%20evaluating%20whether%20they%20achieve%20the%20desired%20goal.%20Unlike%0Asupervised%20navigation%20policies%20with%20fixed%20behavior%2C%20NWM%20can%20dynamically%0Aincorporate%20constraints%20during%20planning.%20Experiments%20demonstrate%20its%0Aeffectiveness%20in%20planning%20trajectories%20from%20scratch%20or%20by%20ranking%20trajectories%0Asampled%20from%20an%20external%20policy.%20Furthermore%2C%20NWM%20leverages%20its%20learned%20visual%0Apriors%20to%20imagine%20trajectories%20in%20unfamiliar%20environments%20from%20a%20single%20input%0Aimage%2C%20making%20it%20a%20flexible%20and%20powerful%20tool%20for%20next-generation%20navigation%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03572v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigation%2520World%2520Models%26entry.906535625%3DAmir%2520Bar%2520and%2520Gaoyue%2520Zhou%2520and%2520Danny%2520Tran%2520and%2520Trevor%2520Darrell%2520and%2520Yann%2520LeCun%26entry.1292438233%3D%2520%2520Navigation%2520is%2520a%2520fundamental%2520skill%2520of%2520agents%2520with%2520visual-motor%2520capabilities.%250AWe%2520introduce%2520a%2520Navigation%2520World%2520Model%2520%2528NWM%2529%252C%2520a%2520controllable%2520video%2520generation%250Amodel%2520that%2520predicts%2520future%2520visual%2520observations%2520based%2520on%2520past%2520observations%2520and%250Anavigation%2520actions.%2520To%2520capture%2520complex%2520environment%2520dynamics%252C%2520NWM%2520employs%2520a%250AConditional%2520Diffusion%2520Transformer%2520%2528CDiT%2529%252C%2520trained%2520on%2520a%2520diverse%2520collection%2520of%250Aegocentric%2520videos%2520of%2520both%2520human%2520and%2520robotic%2520agents%252C%2520and%2520scaled%2520up%2520to%25201%2520billion%250Aparameters.%2520In%2520familiar%2520environments%252C%2520NWM%2520can%2520plan%2520navigation%2520trajectories%2520by%250Asimulating%2520them%2520and%2520evaluating%2520whether%2520they%2520achieve%2520the%2520desired%2520goal.%2520Unlike%250Asupervised%2520navigation%2520policies%2520with%2520fixed%2520behavior%252C%2520NWM%2520can%2520dynamically%250Aincorporate%2520constraints%2520during%2520planning.%2520Experiments%2520demonstrate%2520its%250Aeffectiveness%2520in%2520planning%2520trajectories%2520from%2520scratch%2520or%2520by%2520ranking%2520trajectories%250Asampled%2520from%2520an%2520external%2520policy.%2520Furthermore%252C%2520NWM%2520leverages%2520its%2520learned%2520visual%250Apriors%2520to%2520imagine%2520trajectories%2520in%2520unfamiliar%2520environments%2520from%2520a%2520single%2520input%250Aimage%252C%2520making%2520it%2520a%2520flexible%2520and%2520powerful%2520tool%2520for%2520next-generation%2520navigation%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03572v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigation%20World%20Models&entry.906535625=Amir%20Bar%20and%20Gaoyue%20Zhou%20and%20Danny%20Tran%20and%20Trevor%20Darrell%20and%20Yann%20LeCun&entry.1292438233=%20%20Navigation%20is%20a%20fundamental%20skill%20of%20agents%20with%20visual-motor%20capabilities.%0AWe%20introduce%20a%20Navigation%20World%20Model%20%28NWM%29%2C%20a%20controllable%20video%20generation%0Amodel%20that%20predicts%20future%20visual%20observations%20based%20on%20past%20observations%20and%0Anavigation%20actions.%20To%20capture%20complex%20environment%20dynamics%2C%20NWM%20employs%20a%0AConditional%20Diffusion%20Transformer%20%28CDiT%29%2C%20trained%20on%20a%20diverse%20collection%20of%0Aegocentric%20videos%20of%20both%20human%20and%20robotic%20agents%2C%20and%20scaled%20up%20to%201%20billion%0Aparameters.%20In%20familiar%20environments%2C%20NWM%20can%20plan%20navigation%20trajectories%20by%0Asimulating%20them%20and%20evaluating%20whether%20they%20achieve%20the%20desired%20goal.%20Unlike%0Asupervised%20navigation%20policies%20with%20fixed%20behavior%2C%20NWM%20can%20dynamically%0Aincorporate%20constraints%20during%20planning.%20Experiments%20demonstrate%20its%0Aeffectiveness%20in%20planning%20trajectories%20from%20scratch%20or%20by%20ranking%20trajectories%0Asampled%20from%20an%20external%20policy.%20Furthermore%2C%20NWM%20leverages%20its%20learned%20visual%0Apriors%20to%20imagine%20trajectories%20in%20unfamiliar%20environments%20from%20a%20single%20input%0Aimage%2C%20making%20it%20a%20flexible%20and%20powerful%20tool%20for%20next-generation%20navigation%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03572v1&entry.124074799=Read"},
{"title": "Optimizing Dense Visual Predictions Through Multi-Task Coherence and\n  Prioritization", "author": "Maxime Fontana and Michael Spratling and Miaojing Shi", "abstract": "  Multi-Task Learning (MTL) involves the concurrent training of multiple tasks,\noffering notable advantages for dense prediction tasks in computer vision. MTL\nnot only reduces training and inference time as opposed to having multiple\nsingle-task models, but also enhances task accuracy through the interaction of\nmultiple tasks. However, existing methods face limitations. They often rely on\nsuboptimal cross-task interactions, resulting in task-specific predictions with\npoor geometric and predictive coherence. In addition, many approaches use\ninadequate loss weighting strategies, which do not address the inherent\nvariability in task evolution during training. To overcome these challenges, we\npropose an advanced MTL model specifically designed for dense vision tasks. Our\nmodel leverages state-of-the-art vision transformers with task-specific\ndecoders. To enhance cross-task coherence, we introduce a trace-back method\nthat improves both cross-task geometric and predictive features. Furthermore,\nwe present a novel dynamic task balancing approach that projects task losses\nonto a common scale and prioritizes more challenging tasks during training.\nExtensive experiments demonstrate the superiority of our method, establishing\nnew state-of-the-art performance across two benchmark datasets. The code is\navailable at:https://github.com/Klodivio355/MT-CP\n", "link": "http://arxiv.org/abs/2412.03179v1", "date": "2024-12-04", "relevancy": 2.2594, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5939}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Dense%20Visual%20Predictions%20Through%20Multi-Task%20Coherence%20and%0A%20%20Prioritization&body=Title%3A%20Optimizing%20Dense%20Visual%20Predictions%20Through%20Multi-Task%20Coherence%20and%0A%20%20Prioritization%0AAuthor%3A%20Maxime%20Fontana%20and%20Michael%20Spratling%20and%20Miaojing%20Shi%0AAbstract%3A%20%20%20Multi-Task%20Learning%20%28MTL%29%20involves%20the%20concurrent%20training%20of%20multiple%20tasks%2C%0Aoffering%20notable%20advantages%20for%20dense%20prediction%20tasks%20in%20computer%20vision.%20MTL%0Anot%20only%20reduces%20training%20and%20inference%20time%20as%20opposed%20to%20having%20multiple%0Asingle-task%20models%2C%20but%20also%20enhances%20task%20accuracy%20through%20the%20interaction%20of%0Amultiple%20tasks.%20However%2C%20existing%20methods%20face%20limitations.%20They%20often%20rely%20on%0Asuboptimal%20cross-task%20interactions%2C%20resulting%20in%20task-specific%20predictions%20with%0Apoor%20geometric%20and%20predictive%20coherence.%20In%20addition%2C%20many%20approaches%20use%0Ainadequate%20loss%20weighting%20strategies%2C%20which%20do%20not%20address%20the%20inherent%0Avariability%20in%20task%20evolution%20during%20training.%20To%20overcome%20these%20challenges%2C%20we%0Apropose%20an%20advanced%20MTL%20model%20specifically%20designed%20for%20dense%20vision%20tasks.%20Our%0Amodel%20leverages%20state-of-the-art%20vision%20transformers%20with%20task-specific%0Adecoders.%20To%20enhance%20cross-task%20coherence%2C%20we%20introduce%20a%20trace-back%20method%0Athat%20improves%20both%20cross-task%20geometric%20and%20predictive%20features.%20Furthermore%2C%0Awe%20present%20a%20novel%20dynamic%20task%20balancing%20approach%20that%20projects%20task%20losses%0Aonto%20a%20common%20scale%20and%20prioritizes%20more%20challenging%20tasks%20during%20training.%0AExtensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20method%2C%20establishing%0Anew%20state-of-the-art%20performance%20across%20two%20benchmark%20datasets.%20The%20code%20is%0Aavailable%20at%3Ahttps%3A//github.com/Klodivio355/MT-CP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Dense%2520Visual%2520Predictions%2520Through%2520Multi-Task%2520Coherence%2520and%250A%2520%2520Prioritization%26entry.906535625%3DMaxime%2520Fontana%2520and%2520Michael%2520Spratling%2520and%2520Miaojing%2520Shi%26entry.1292438233%3D%2520%2520Multi-Task%2520Learning%2520%2528MTL%2529%2520involves%2520the%2520concurrent%2520training%2520of%2520multiple%2520tasks%252C%250Aoffering%2520notable%2520advantages%2520for%2520dense%2520prediction%2520tasks%2520in%2520computer%2520vision.%2520MTL%250Anot%2520only%2520reduces%2520training%2520and%2520inference%2520time%2520as%2520opposed%2520to%2520having%2520multiple%250Asingle-task%2520models%252C%2520but%2520also%2520enhances%2520task%2520accuracy%2520through%2520the%2520interaction%2520of%250Amultiple%2520tasks.%2520However%252C%2520existing%2520methods%2520face%2520limitations.%2520They%2520often%2520rely%2520on%250Asuboptimal%2520cross-task%2520interactions%252C%2520resulting%2520in%2520task-specific%2520predictions%2520with%250Apoor%2520geometric%2520and%2520predictive%2520coherence.%2520In%2520addition%252C%2520many%2520approaches%2520use%250Ainadequate%2520loss%2520weighting%2520strategies%252C%2520which%2520do%2520not%2520address%2520the%2520inherent%250Avariability%2520in%2520task%2520evolution%2520during%2520training.%2520To%2520overcome%2520these%2520challenges%252C%2520we%250Apropose%2520an%2520advanced%2520MTL%2520model%2520specifically%2520designed%2520for%2520dense%2520vision%2520tasks.%2520Our%250Amodel%2520leverages%2520state-of-the-art%2520vision%2520transformers%2520with%2520task-specific%250Adecoders.%2520To%2520enhance%2520cross-task%2520coherence%252C%2520we%2520introduce%2520a%2520trace-back%2520method%250Athat%2520improves%2520both%2520cross-task%2520geometric%2520and%2520predictive%2520features.%2520Furthermore%252C%250Awe%2520present%2520a%2520novel%2520dynamic%2520task%2520balancing%2520approach%2520that%2520projects%2520task%2520losses%250Aonto%2520a%2520common%2520scale%2520and%2520prioritizes%2520more%2520challenging%2520tasks%2520during%2520training.%250AExtensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%252C%2520establishing%250Anew%2520state-of-the-art%2520performance%2520across%2520two%2520benchmark%2520datasets.%2520The%2520code%2520is%250Aavailable%2520at%253Ahttps%253A//github.com/Klodivio355/MT-CP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Dense%20Visual%20Predictions%20Through%20Multi-Task%20Coherence%20and%0A%20%20Prioritization&entry.906535625=Maxime%20Fontana%20and%20Michael%20Spratling%20and%20Miaojing%20Shi&entry.1292438233=%20%20Multi-Task%20Learning%20%28MTL%29%20involves%20the%20concurrent%20training%20of%20multiple%20tasks%2C%0Aoffering%20notable%20advantages%20for%20dense%20prediction%20tasks%20in%20computer%20vision.%20MTL%0Anot%20only%20reduces%20training%20and%20inference%20time%20as%20opposed%20to%20having%20multiple%0Asingle-task%20models%2C%20but%20also%20enhances%20task%20accuracy%20through%20the%20interaction%20of%0Amultiple%20tasks.%20However%2C%20existing%20methods%20face%20limitations.%20They%20often%20rely%20on%0Asuboptimal%20cross-task%20interactions%2C%20resulting%20in%20task-specific%20predictions%20with%0Apoor%20geometric%20and%20predictive%20coherence.%20In%20addition%2C%20many%20approaches%20use%0Ainadequate%20loss%20weighting%20strategies%2C%20which%20do%20not%20address%20the%20inherent%0Avariability%20in%20task%20evolution%20during%20training.%20To%20overcome%20these%20challenges%2C%20we%0Apropose%20an%20advanced%20MTL%20model%20specifically%20designed%20for%20dense%20vision%20tasks.%20Our%0Amodel%20leverages%20state-of-the-art%20vision%20transformers%20with%20task-specific%0Adecoders.%20To%20enhance%20cross-task%20coherence%2C%20we%20introduce%20a%20trace-back%20method%0Athat%20improves%20both%20cross-task%20geometric%20and%20predictive%20features.%20Furthermore%2C%0Awe%20present%20a%20novel%20dynamic%20task%20balancing%20approach%20that%20projects%20task%20losses%0Aonto%20a%20common%20scale%20and%20prioritizes%20more%20challenging%20tasks%20during%20training.%0AExtensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20method%2C%20establishing%0Anew%20state-of-the-art%20performance%20across%20two%20benchmark%20datasets.%20The%20code%20is%0Aavailable%20at%3Ahttps%3A//github.com/Klodivio355/MT-CP%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03179v1&entry.124074799=Read"},
{"title": "Rotograb: Combining Biomimetic Hands with Industrial Grippers using a\n  Rotating Thumb", "author": "Arnaud Bersier and Matteo Leonforte and Alessio Vanetta and Sarah Lia Andrea Wotke and Andrea Nappi and Yifan Zhou and Sebastiano Oliani and Alexander M. K\u00fcbler and Robert K. Katzschmann", "abstract": "  The development of robotic grippers and hands for automation aims to emulate\nhuman dexterity without sacrificing the efficiency of industrial grippers. This\nstudy introduces Rotograb, a tendon-actuated robotic hand featuring a novel\nrotating thumb. The aim is to combine the dexterity of human hands with the\nefficiency of industrial grippers. The rotating thumb enlarges the workspace\nand allows in-hand manipulation. A novel joint design minimizes movement\ninterference and simplifies kinematics, using a cutout for tendon routing. We\nintegrate teleoperation, using a depth camera for real-time tracking and\nautonomous manipulation powered by reinforcement learning with proximal policy\noptimization. Experimental evaluations demonstrate that Rotograb's rotating\nthumb greatly improves both operational versatility and workspace. It can\nhandle various grasping and manipulation tasks with objects from the YCB\ndataset, with particularly good results when rotating objects within its grasp.\nRotograb represents a notable step towards bridging the capability gap between\nhuman hands and industrial grippers. The tendon-routing and thumb-rotating\nmechanisms allow for a new level of control and dexterity. Integrating\nteleoperation and autonomous learning underscores Rotograb's adaptability and\nsophistication, promising substantial advancements in both robotics research\nand practical applications.\n", "link": "http://arxiv.org/abs/2412.03279v1", "date": "2024-12-04", "relevancy": 2.2593, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5964}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5608}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rotograb%3A%20Combining%20Biomimetic%20Hands%20with%20Industrial%20Grippers%20using%20a%0A%20%20Rotating%20Thumb&body=Title%3A%20Rotograb%3A%20Combining%20Biomimetic%20Hands%20with%20Industrial%20Grippers%20using%20a%0A%20%20Rotating%20Thumb%0AAuthor%3A%20Arnaud%20Bersier%20and%20Matteo%20Leonforte%20and%20Alessio%20Vanetta%20and%20Sarah%20Lia%20Andrea%20Wotke%20and%20Andrea%20Nappi%20and%20Yifan%20Zhou%20and%20Sebastiano%20Oliani%20and%20Alexander%20M.%20K%C3%BCbler%20and%20Robert%20K.%20Katzschmann%0AAbstract%3A%20%20%20The%20development%20of%20robotic%20grippers%20and%20hands%20for%20automation%20aims%20to%20emulate%0Ahuman%20dexterity%20without%20sacrificing%20the%20efficiency%20of%20industrial%20grippers.%20This%0Astudy%20introduces%20Rotograb%2C%20a%20tendon-actuated%20robotic%20hand%20featuring%20a%20novel%0Arotating%20thumb.%20The%20aim%20is%20to%20combine%20the%20dexterity%20of%20human%20hands%20with%20the%0Aefficiency%20of%20industrial%20grippers.%20The%20rotating%20thumb%20enlarges%20the%20workspace%0Aand%20allows%20in-hand%20manipulation.%20A%20novel%20joint%20design%20minimizes%20movement%0Ainterference%20and%20simplifies%20kinematics%2C%20using%20a%20cutout%20for%20tendon%20routing.%20We%0Aintegrate%20teleoperation%2C%20using%20a%20depth%20camera%20for%20real-time%20tracking%20and%0Aautonomous%20manipulation%20powered%20by%20reinforcement%20learning%20with%20proximal%20policy%0Aoptimization.%20Experimental%20evaluations%20demonstrate%20that%20Rotograb%27s%20rotating%0Athumb%20greatly%20improves%20both%20operational%20versatility%20and%20workspace.%20It%20can%0Ahandle%20various%20grasping%20and%20manipulation%20tasks%20with%20objects%20from%20the%20YCB%0Adataset%2C%20with%20particularly%20good%20results%20when%20rotating%20objects%20within%20its%20grasp.%0ARotograb%20represents%20a%20notable%20step%20towards%20bridging%20the%20capability%20gap%20between%0Ahuman%20hands%20and%20industrial%20grippers.%20The%20tendon-routing%20and%20thumb-rotating%0Amechanisms%20allow%20for%20a%20new%20level%20of%20control%20and%20dexterity.%20Integrating%0Ateleoperation%20and%20autonomous%20learning%20underscores%20Rotograb%27s%20adaptability%20and%0Asophistication%2C%20promising%20substantial%20advancements%20in%20both%20robotics%20research%0Aand%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRotograb%253A%2520Combining%2520Biomimetic%2520Hands%2520with%2520Industrial%2520Grippers%2520using%2520a%250A%2520%2520Rotating%2520Thumb%26entry.906535625%3DArnaud%2520Bersier%2520and%2520Matteo%2520Leonforte%2520and%2520Alessio%2520Vanetta%2520and%2520Sarah%2520Lia%2520Andrea%2520Wotke%2520and%2520Andrea%2520Nappi%2520and%2520Yifan%2520Zhou%2520and%2520Sebastiano%2520Oliani%2520and%2520Alexander%2520M.%2520K%25C3%25BCbler%2520and%2520Robert%2520K.%2520Katzschmann%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520robotic%2520grippers%2520and%2520hands%2520for%2520automation%2520aims%2520to%2520emulate%250Ahuman%2520dexterity%2520without%2520sacrificing%2520the%2520efficiency%2520of%2520industrial%2520grippers.%2520This%250Astudy%2520introduces%2520Rotograb%252C%2520a%2520tendon-actuated%2520robotic%2520hand%2520featuring%2520a%2520novel%250Arotating%2520thumb.%2520The%2520aim%2520is%2520to%2520combine%2520the%2520dexterity%2520of%2520human%2520hands%2520with%2520the%250Aefficiency%2520of%2520industrial%2520grippers.%2520The%2520rotating%2520thumb%2520enlarges%2520the%2520workspace%250Aand%2520allows%2520in-hand%2520manipulation.%2520A%2520novel%2520joint%2520design%2520minimizes%2520movement%250Ainterference%2520and%2520simplifies%2520kinematics%252C%2520using%2520a%2520cutout%2520for%2520tendon%2520routing.%2520We%250Aintegrate%2520teleoperation%252C%2520using%2520a%2520depth%2520camera%2520for%2520real-time%2520tracking%2520and%250Aautonomous%2520manipulation%2520powered%2520by%2520reinforcement%2520learning%2520with%2520proximal%2520policy%250Aoptimization.%2520Experimental%2520evaluations%2520demonstrate%2520that%2520Rotograb%2527s%2520rotating%250Athumb%2520greatly%2520improves%2520both%2520operational%2520versatility%2520and%2520workspace.%2520It%2520can%250Ahandle%2520various%2520grasping%2520and%2520manipulation%2520tasks%2520with%2520objects%2520from%2520the%2520YCB%250Adataset%252C%2520with%2520particularly%2520good%2520results%2520when%2520rotating%2520objects%2520within%2520its%2520grasp.%250ARotograb%2520represents%2520a%2520notable%2520step%2520towards%2520bridging%2520the%2520capability%2520gap%2520between%250Ahuman%2520hands%2520and%2520industrial%2520grippers.%2520The%2520tendon-routing%2520and%2520thumb-rotating%250Amechanisms%2520allow%2520for%2520a%2520new%2520level%2520of%2520control%2520and%2520dexterity.%2520Integrating%250Ateleoperation%2520and%2520autonomous%2520learning%2520underscores%2520Rotograb%2527s%2520adaptability%2520and%250Asophistication%252C%2520promising%2520substantial%2520advancements%2520in%2520both%2520robotics%2520research%250Aand%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rotograb%3A%20Combining%20Biomimetic%20Hands%20with%20Industrial%20Grippers%20using%20a%0A%20%20Rotating%20Thumb&entry.906535625=Arnaud%20Bersier%20and%20Matteo%20Leonforte%20and%20Alessio%20Vanetta%20and%20Sarah%20Lia%20Andrea%20Wotke%20and%20Andrea%20Nappi%20and%20Yifan%20Zhou%20and%20Sebastiano%20Oliani%20and%20Alexander%20M.%20K%C3%BCbler%20and%20Robert%20K.%20Katzschmann&entry.1292438233=%20%20The%20development%20of%20robotic%20grippers%20and%20hands%20for%20automation%20aims%20to%20emulate%0Ahuman%20dexterity%20without%20sacrificing%20the%20efficiency%20of%20industrial%20grippers.%20This%0Astudy%20introduces%20Rotograb%2C%20a%20tendon-actuated%20robotic%20hand%20featuring%20a%20novel%0Arotating%20thumb.%20The%20aim%20is%20to%20combine%20the%20dexterity%20of%20human%20hands%20with%20the%0Aefficiency%20of%20industrial%20grippers.%20The%20rotating%20thumb%20enlarges%20the%20workspace%0Aand%20allows%20in-hand%20manipulation.%20A%20novel%20joint%20design%20minimizes%20movement%0Ainterference%20and%20simplifies%20kinematics%2C%20using%20a%20cutout%20for%20tendon%20routing.%20We%0Aintegrate%20teleoperation%2C%20using%20a%20depth%20camera%20for%20real-time%20tracking%20and%0Aautonomous%20manipulation%20powered%20by%20reinforcement%20learning%20with%20proximal%20policy%0Aoptimization.%20Experimental%20evaluations%20demonstrate%20that%20Rotograb%27s%20rotating%0Athumb%20greatly%20improves%20both%20operational%20versatility%20and%20workspace.%20It%20can%0Ahandle%20various%20grasping%20and%20manipulation%20tasks%20with%20objects%20from%20the%20YCB%0Adataset%2C%20with%20particularly%20good%20results%20when%20rotating%20objects%20within%20its%20grasp.%0ARotograb%20represents%20a%20notable%20step%20towards%20bridging%20the%20capability%20gap%20between%0Ahuman%20hands%20and%20industrial%20grippers.%20The%20tendon-routing%20and%20thumb-rotating%0Amechanisms%20allow%20for%20a%20new%20level%20of%20control%20and%20dexterity.%20Integrating%0Ateleoperation%20and%20autonomous%20learning%20underscores%20Rotograb%27s%20adaptability%20and%0Asophistication%2C%20promising%20substantial%20advancements%20in%20both%20robotics%20research%0Aand%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03279v1&entry.124074799=Read"},
{"title": "Fab-ME: A Vision State-Space and Attention-Enhanced Framework for Fabric\n  Defect Detection", "author": "Shuai Wang and Huiyan Kong and Baotian Li and Fa Zheng", "abstract": "  Effective defect detection is critical for ensuring the quality,\nfunctionality, and economic value of textile products. However, existing\nmethods face challenges in achieving high accuracy, real-time performance, and\nefficient global information extraction. To address these issues, we propose\nFab-ME, an advanced framework based on YOLOv8s, specifically designed for the\naccurate detection of 20 fabric defect types. Our contributions include the\nintroduction of the cross-stage partial bottleneck with two convolutions (C2F)\nvision state-space (C2F-VMamba) module, which integrates visual state-space\n(VSS) blocks into the YOLOv8s feature fusion network neck, enhancing the\ncapture of intricate details and global context while maintaining high\nprocessing speeds. Additionally, we incorporate an enhanced multi-scale channel\nattention (EMCA) module into the final layer of the feature extraction network,\nsignificantly improving sensitivity to small targets. Experimental results on\nthe Tianchi fabric defect detection dataset demonstrate that Fab-ME achieves a\n3.3\\% improvement in mAP@0.5 compared to the original YOLOv8s, validating its\neffectiveness for precise and efficient fabric defect detection.\n", "link": "http://arxiv.org/abs/2412.03200v1", "date": "2024-12-04", "relevancy": 2.2538, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6031}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5807}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fab-ME%3A%20A%20Vision%20State-Space%20and%20Attention-Enhanced%20Framework%20for%20Fabric%0A%20%20Defect%20Detection&body=Title%3A%20Fab-ME%3A%20A%20Vision%20State-Space%20and%20Attention-Enhanced%20Framework%20for%20Fabric%0A%20%20Defect%20Detection%0AAuthor%3A%20Shuai%20Wang%20and%20Huiyan%20Kong%20and%20Baotian%20Li%20and%20Fa%20Zheng%0AAbstract%3A%20%20%20Effective%20defect%20detection%20is%20critical%20for%20ensuring%20the%20quality%2C%0Afunctionality%2C%20and%20economic%20value%20of%20textile%20products.%20However%2C%20existing%0Amethods%20face%20challenges%20in%20achieving%20high%20accuracy%2C%20real-time%20performance%2C%20and%0Aefficient%20global%20information%20extraction.%20To%20address%20these%20issues%2C%20we%20propose%0AFab-ME%2C%20an%20advanced%20framework%20based%20on%20YOLOv8s%2C%20specifically%20designed%20for%20the%0Aaccurate%20detection%20of%2020%20fabric%20defect%20types.%20Our%20contributions%20include%20the%0Aintroduction%20of%20the%20cross-stage%20partial%20bottleneck%20with%20two%20convolutions%20%28C2F%29%0Avision%20state-space%20%28C2F-VMamba%29%20module%2C%20which%20integrates%20visual%20state-space%0A%28VSS%29%20blocks%20into%20the%20YOLOv8s%20feature%20fusion%20network%20neck%2C%20enhancing%20the%0Acapture%20of%20intricate%20details%20and%20global%20context%20while%20maintaining%20high%0Aprocessing%20speeds.%20Additionally%2C%20we%20incorporate%20an%20enhanced%20multi-scale%20channel%0Aattention%20%28EMCA%29%20module%20into%20the%20final%20layer%20of%20the%20feature%20extraction%20network%2C%0Asignificantly%20improving%20sensitivity%20to%20small%20targets.%20Experimental%20results%20on%0Athe%20Tianchi%20fabric%20defect%20detection%20dataset%20demonstrate%20that%20Fab-ME%20achieves%20a%0A3.3%5C%25%20improvement%20in%20mAP%400.5%20compared%20to%20the%20original%20YOLOv8s%2C%20validating%20its%0Aeffectiveness%20for%20precise%20and%20efficient%20fabric%20defect%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFab-ME%253A%2520A%2520Vision%2520State-Space%2520and%2520Attention-Enhanced%2520Framework%2520for%2520Fabric%250A%2520%2520Defect%2520Detection%26entry.906535625%3DShuai%2520Wang%2520and%2520Huiyan%2520Kong%2520and%2520Baotian%2520Li%2520and%2520Fa%2520Zheng%26entry.1292438233%3D%2520%2520Effective%2520defect%2520detection%2520is%2520critical%2520for%2520ensuring%2520the%2520quality%252C%250Afunctionality%252C%2520and%2520economic%2520value%2520of%2520textile%2520products.%2520However%252C%2520existing%250Amethods%2520face%2520challenges%2520in%2520achieving%2520high%2520accuracy%252C%2520real-time%2520performance%252C%2520and%250Aefficient%2520global%2520information%2520extraction.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250AFab-ME%252C%2520an%2520advanced%2520framework%2520based%2520on%2520YOLOv8s%252C%2520specifically%2520designed%2520for%2520the%250Aaccurate%2520detection%2520of%252020%2520fabric%2520defect%2520types.%2520Our%2520contributions%2520include%2520the%250Aintroduction%2520of%2520the%2520cross-stage%2520partial%2520bottleneck%2520with%2520two%2520convolutions%2520%2528C2F%2529%250Avision%2520state-space%2520%2528C2F-VMamba%2529%2520module%252C%2520which%2520integrates%2520visual%2520state-space%250A%2528VSS%2529%2520blocks%2520into%2520the%2520YOLOv8s%2520feature%2520fusion%2520network%2520neck%252C%2520enhancing%2520the%250Acapture%2520of%2520intricate%2520details%2520and%2520global%2520context%2520while%2520maintaining%2520high%250Aprocessing%2520speeds.%2520Additionally%252C%2520we%2520incorporate%2520an%2520enhanced%2520multi-scale%2520channel%250Aattention%2520%2528EMCA%2529%2520module%2520into%2520the%2520final%2520layer%2520of%2520the%2520feature%2520extraction%2520network%252C%250Asignificantly%2520improving%2520sensitivity%2520to%2520small%2520targets.%2520Experimental%2520results%2520on%250Athe%2520Tianchi%2520fabric%2520defect%2520detection%2520dataset%2520demonstrate%2520that%2520Fab-ME%2520achieves%2520a%250A3.3%255C%2525%2520improvement%2520in%2520mAP%25400.5%2520compared%2520to%2520the%2520original%2520YOLOv8s%252C%2520validating%2520its%250Aeffectiveness%2520for%2520precise%2520and%2520efficient%2520fabric%2520defect%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fab-ME%3A%20A%20Vision%20State-Space%20and%20Attention-Enhanced%20Framework%20for%20Fabric%0A%20%20Defect%20Detection&entry.906535625=Shuai%20Wang%20and%20Huiyan%20Kong%20and%20Baotian%20Li%20and%20Fa%20Zheng&entry.1292438233=%20%20Effective%20defect%20detection%20is%20critical%20for%20ensuring%20the%20quality%2C%0Afunctionality%2C%20and%20economic%20value%20of%20textile%20products.%20However%2C%20existing%0Amethods%20face%20challenges%20in%20achieving%20high%20accuracy%2C%20real-time%20performance%2C%20and%0Aefficient%20global%20information%20extraction.%20To%20address%20these%20issues%2C%20we%20propose%0AFab-ME%2C%20an%20advanced%20framework%20based%20on%20YOLOv8s%2C%20specifically%20designed%20for%20the%0Aaccurate%20detection%20of%2020%20fabric%20defect%20types.%20Our%20contributions%20include%20the%0Aintroduction%20of%20the%20cross-stage%20partial%20bottleneck%20with%20two%20convolutions%20%28C2F%29%0Avision%20state-space%20%28C2F-VMamba%29%20module%2C%20which%20integrates%20visual%20state-space%0A%28VSS%29%20blocks%20into%20the%20YOLOv8s%20feature%20fusion%20network%20neck%2C%20enhancing%20the%0Acapture%20of%20intricate%20details%20and%20global%20context%20while%20maintaining%20high%0Aprocessing%20speeds.%20Additionally%2C%20we%20incorporate%20an%20enhanced%20multi-scale%20channel%0Aattention%20%28EMCA%29%20module%20into%20the%20final%20layer%20of%20the%20feature%20extraction%20network%2C%0Asignificantly%20improving%20sensitivity%20to%20small%20targets.%20Experimental%20results%20on%0Athe%20Tianchi%20fabric%20defect%20detection%20dataset%20demonstrate%20that%20Fab-ME%20achieves%20a%0A3.3%5C%25%20improvement%20in%20mAP%400.5%20compared%20to%20the%20original%20YOLOv8s%2C%20validating%20its%0Aeffectiveness%20for%20precise%20and%20efficient%20fabric%20defect%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03200v1&entry.124074799=Read"},
{"title": "BIMCaP: BIM-based AI-supported LiDAR-Camera Pose Refinement", "author": "Miguel Arturo Vega Torres and Anna Ribic and Borja Garc\u00eda de Soto and Andr\u00e9 Borrmann", "abstract": "  This paper introduces BIMCaP, a novel method to integrate mobile 3D sparse\nLiDAR data and camera measurements with pre-existing building information\nmodels (BIMs), enhancing fast and accurate indoor mapping with affordable\nsensors. BIMCaP refines sensor poses by leveraging a 3D BIM and employing a\nbundle adjustment technique to align real-world measurements with the model.\nExperiments using real-world open-access data show that BIMCaP achieves\nsuperior accuracy, reducing translational error by over 4 cm compared to\ncurrent state-of-the-art methods. This advancement enhances the accuracy and\ncost-effectiveness of 3D mapping methodologies like SLAM. BIMCaP's improvements\nbenefit various fields, including construction site management and emergency\nresponse, by providing up-to-date, aligned digital maps for better\ndecision-making and productivity. Link to the repository:\nhttps://github.com/MigVega/BIMCaP\n", "link": "http://arxiv.org/abs/2412.03434v1", "date": "2024-12-04", "relevancy": 2.2493, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5914}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5445}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BIMCaP%3A%20BIM-based%20AI-supported%20LiDAR-Camera%20Pose%20Refinement&body=Title%3A%20BIMCaP%3A%20BIM-based%20AI-supported%20LiDAR-Camera%20Pose%20Refinement%0AAuthor%3A%20Miguel%20Arturo%20Vega%20Torres%20and%20Anna%20Ribic%20and%20Borja%20Garc%C3%ADa%20de%20Soto%20and%20Andr%C3%A9%20Borrmann%0AAbstract%3A%20%20%20This%20paper%20introduces%20BIMCaP%2C%20a%20novel%20method%20to%20integrate%20mobile%203D%20sparse%0ALiDAR%20data%20and%20camera%20measurements%20with%20pre-existing%20building%20information%0Amodels%20%28BIMs%29%2C%20enhancing%20fast%20and%20accurate%20indoor%20mapping%20with%20affordable%0Asensors.%20BIMCaP%20refines%20sensor%20poses%20by%20leveraging%20a%203D%20BIM%20and%20employing%20a%0Abundle%20adjustment%20technique%20to%20align%20real-world%20measurements%20with%20the%20model.%0AExperiments%20using%20real-world%20open-access%20data%20show%20that%20BIMCaP%20achieves%0Asuperior%20accuracy%2C%20reducing%20translational%20error%20by%20over%204%20cm%20compared%20to%0Acurrent%20state-of-the-art%20methods.%20This%20advancement%20enhances%20the%20accuracy%20and%0Acost-effectiveness%20of%203D%20mapping%20methodologies%20like%20SLAM.%20BIMCaP%27s%20improvements%0Abenefit%20various%20fields%2C%20including%20construction%20site%20management%20and%20emergency%0Aresponse%2C%20by%20providing%20up-to-date%2C%20aligned%20digital%20maps%20for%20better%0Adecision-making%20and%20productivity.%20Link%20to%20the%20repository%3A%0Ahttps%3A//github.com/MigVega/BIMCaP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBIMCaP%253A%2520BIM-based%2520AI-supported%2520LiDAR-Camera%2520Pose%2520Refinement%26entry.906535625%3DMiguel%2520Arturo%2520Vega%2520Torres%2520and%2520Anna%2520Ribic%2520and%2520Borja%2520Garc%25C3%25ADa%2520de%2520Soto%2520and%2520Andr%25C3%25A9%2520Borrmann%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520BIMCaP%252C%2520a%2520novel%2520method%2520to%2520integrate%2520mobile%25203D%2520sparse%250ALiDAR%2520data%2520and%2520camera%2520measurements%2520with%2520pre-existing%2520building%2520information%250Amodels%2520%2528BIMs%2529%252C%2520enhancing%2520fast%2520and%2520accurate%2520indoor%2520mapping%2520with%2520affordable%250Asensors.%2520BIMCaP%2520refines%2520sensor%2520poses%2520by%2520leveraging%2520a%25203D%2520BIM%2520and%2520employing%2520a%250Abundle%2520adjustment%2520technique%2520to%2520align%2520real-world%2520measurements%2520with%2520the%2520model.%250AExperiments%2520using%2520real-world%2520open-access%2520data%2520show%2520that%2520BIMCaP%2520achieves%250Asuperior%2520accuracy%252C%2520reducing%2520translational%2520error%2520by%2520over%25204%2520cm%2520compared%2520to%250Acurrent%2520state-of-the-art%2520methods.%2520This%2520advancement%2520enhances%2520the%2520accuracy%2520and%250Acost-effectiveness%2520of%25203D%2520mapping%2520methodologies%2520like%2520SLAM.%2520BIMCaP%2527s%2520improvements%250Abenefit%2520various%2520fields%252C%2520including%2520construction%2520site%2520management%2520and%2520emergency%250Aresponse%252C%2520by%2520providing%2520up-to-date%252C%2520aligned%2520digital%2520maps%2520for%2520better%250Adecision-making%2520and%2520productivity.%2520Link%2520to%2520the%2520repository%253A%250Ahttps%253A//github.com/MigVega/BIMCaP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BIMCaP%3A%20BIM-based%20AI-supported%20LiDAR-Camera%20Pose%20Refinement&entry.906535625=Miguel%20Arturo%20Vega%20Torres%20and%20Anna%20Ribic%20and%20Borja%20Garc%C3%ADa%20de%20Soto%20and%20Andr%C3%A9%20Borrmann&entry.1292438233=%20%20This%20paper%20introduces%20BIMCaP%2C%20a%20novel%20method%20to%20integrate%20mobile%203D%20sparse%0ALiDAR%20data%20and%20camera%20measurements%20with%20pre-existing%20building%20information%0Amodels%20%28BIMs%29%2C%20enhancing%20fast%20and%20accurate%20indoor%20mapping%20with%20affordable%0Asensors.%20BIMCaP%20refines%20sensor%20poses%20by%20leveraging%20a%203D%20BIM%20and%20employing%20a%0Abundle%20adjustment%20technique%20to%20align%20real-world%20measurements%20with%20the%20model.%0AExperiments%20using%20real-world%20open-access%20data%20show%20that%20BIMCaP%20achieves%0Asuperior%20accuracy%2C%20reducing%20translational%20error%20by%20over%204%20cm%20compared%20to%0Acurrent%20state-of-the-art%20methods.%20This%20advancement%20enhances%20the%20accuracy%20and%0Acost-effectiveness%20of%203D%20mapping%20methodologies%20like%20SLAM.%20BIMCaP%27s%20improvements%0Abenefit%20various%20fields%2C%20including%20construction%20site%20management%20and%20emergency%0Aresponse%2C%20by%20providing%20up-to-date%2C%20aligned%20digital%20maps%20for%20better%0Adecision-making%20and%20productivity.%20Link%20to%20the%20repository%3A%0Ahttps%3A//github.com/MigVega/BIMCaP%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03434v1&entry.124074799=Read"},
{"title": "Instance-Warp: Saliency Guided Image Warping for Unsupervised Domain\n  Adaptation", "author": "Shen Zheng and Anurag Ghosh and Srinivasa G. Narasimhan", "abstract": "  Driving is challenging in conditions like night, rain, and snow. Lack of good\nlabeled datasets has hampered progress in scene understanding under such\nconditions. Unsupervised Domain Adaptation (UDA) using large labeled clear-day\ndatasets is a promising research direction in such cases. However, many UDA\nmethods are trained with dominant scene backgrounds (e.g., roads, sky,\nsidewalks) that appear dramatically different across domains. As a result, they\nstruggle to learn effective features of smaller and often sparse foreground\nobjects (e.g., people, vehicles, signs).\n  In this work, we improve UDA training by applying in-place image warping to\nfocus on salient objects. We design instance-level saliency guidance to\nadaptively oversample object regions and undersample background areas, which\nreduces adverse effects from background context and enhances backbone feature\nlearning. Our approach improves adaptation across geographies, lighting, and\nweather conditions, and is agnostic to the task (segmentation, detection),\ndomain adaptation algorithm, saliency guidance, and underlying model\narchitecture. Result highlights include +6.1 mAP50 for BDD100K Clear\n$\\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\\rightarrow$ Night, +3.0\nmAP50 for BDD100K Clear $\\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes\n$\\rightarrow$ ACDC. Besides, Our method adds minimal training memory and no\nadditional inference latency. Code is available at\nhttps://github.com/ShenZheng2000/Instance-Warp\n", "link": "http://arxiv.org/abs/2403.12712v3", "date": "2024-12-04", "relevancy": 2.2222, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5569}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5551}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance-Warp%3A%20Saliency%20Guided%20Image%20Warping%20for%20Unsupervised%20Domain%0A%20%20Adaptation&body=Title%3A%20Instance-Warp%3A%20Saliency%20Guided%20Image%20Warping%20for%20Unsupervised%20Domain%0A%20%20Adaptation%0AAuthor%3A%20Shen%20Zheng%20and%20Anurag%20Ghosh%20and%20Srinivasa%20G.%20Narasimhan%0AAbstract%3A%20%20%20Driving%20is%20challenging%20in%20conditions%20like%20night%2C%20rain%2C%20and%20snow.%20Lack%20of%20good%0Alabeled%20datasets%20has%20hampered%20progress%20in%20scene%20understanding%20under%20such%0Aconditions.%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20using%20large%20labeled%20clear-day%0Adatasets%20is%20a%20promising%20research%20direction%20in%20such%20cases.%20However%2C%20many%20UDA%0Amethods%20are%20trained%20with%20dominant%20scene%20backgrounds%20%28e.g.%2C%20roads%2C%20sky%2C%0Asidewalks%29%20that%20appear%20dramatically%20different%20across%20domains.%20As%20a%20result%2C%20they%0Astruggle%20to%20learn%20effective%20features%20of%20smaller%20and%20often%20sparse%20foreground%0Aobjects%20%28e.g.%2C%20people%2C%20vehicles%2C%20signs%29.%0A%20%20In%20this%20work%2C%20we%20improve%20UDA%20training%20by%20applying%20in-place%20image%20warping%20to%0Afocus%20on%20salient%20objects.%20We%20design%20instance-level%20saliency%20guidance%20to%0Aadaptively%20oversample%20object%20regions%20and%20undersample%20background%20areas%2C%20which%0Areduces%20adverse%20effects%20from%20background%20context%20and%20enhances%20backbone%20feature%0Alearning.%20Our%20approach%20improves%20adaptation%20across%20geographies%2C%20lighting%2C%20and%0Aweather%20conditions%2C%20and%20is%20agnostic%20to%20the%20task%20%28segmentation%2C%20detection%29%2C%0Adomain%20adaptation%20algorithm%2C%20saliency%20guidance%2C%20and%20underlying%20model%0Aarchitecture.%20Result%20highlights%20include%20%2B6.1%20mAP50%20for%20BDD100K%20Clear%0A%24%5Crightarrow%24%20DENSE%20Foggy%2C%20%2B3.7%20mAP50%20for%20BDD100K%20Day%20%24%5Crightarrow%24%20Night%2C%20%2B3.0%0AmAP50%20for%20BDD100K%20Clear%20%24%5Crightarrow%24%20Rainy%2C%20and%20%2B6.3%20mIoU%20for%20Cityscapes%0A%24%5Crightarrow%24%20ACDC.%20Besides%2C%20Our%20method%20adds%20minimal%20training%20memory%20and%20no%0Aadditional%20inference%20latency.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ShenZheng2000/Instance-Warp%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12712v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance-Warp%253A%2520Saliency%2520Guided%2520Image%2520Warping%2520for%2520Unsupervised%2520Domain%250A%2520%2520Adaptation%26entry.906535625%3DShen%2520Zheng%2520and%2520Anurag%2520Ghosh%2520and%2520Srinivasa%2520G.%2520Narasimhan%26entry.1292438233%3D%2520%2520Driving%2520is%2520challenging%2520in%2520conditions%2520like%2520night%252C%2520rain%252C%2520and%2520snow.%2520Lack%2520of%2520good%250Alabeled%2520datasets%2520has%2520hampered%2520progress%2520in%2520scene%2520understanding%2520under%2520such%250Aconditions.%2520Unsupervised%2520Domain%2520Adaptation%2520%2528UDA%2529%2520using%2520large%2520labeled%2520clear-day%250Adatasets%2520is%2520a%2520promising%2520research%2520direction%2520in%2520such%2520cases.%2520However%252C%2520many%2520UDA%250Amethods%2520are%2520trained%2520with%2520dominant%2520scene%2520backgrounds%2520%2528e.g.%252C%2520roads%252C%2520sky%252C%250Asidewalks%2529%2520that%2520appear%2520dramatically%2520different%2520across%2520domains.%2520As%2520a%2520result%252C%2520they%250Astruggle%2520to%2520learn%2520effective%2520features%2520of%2520smaller%2520and%2520often%2520sparse%2520foreground%250Aobjects%2520%2528e.g.%252C%2520people%252C%2520vehicles%252C%2520signs%2529.%250A%2520%2520In%2520this%2520work%252C%2520we%2520improve%2520UDA%2520training%2520by%2520applying%2520in-place%2520image%2520warping%2520to%250Afocus%2520on%2520salient%2520objects.%2520We%2520design%2520instance-level%2520saliency%2520guidance%2520to%250Aadaptively%2520oversample%2520object%2520regions%2520and%2520undersample%2520background%2520areas%252C%2520which%250Areduces%2520adverse%2520effects%2520from%2520background%2520context%2520and%2520enhances%2520backbone%2520feature%250Alearning.%2520Our%2520approach%2520improves%2520adaptation%2520across%2520geographies%252C%2520lighting%252C%2520and%250Aweather%2520conditions%252C%2520and%2520is%2520agnostic%2520to%2520the%2520task%2520%2528segmentation%252C%2520detection%2529%252C%250Adomain%2520adaptation%2520algorithm%252C%2520saliency%2520guidance%252C%2520and%2520underlying%2520model%250Aarchitecture.%2520Result%2520highlights%2520include%2520%252B6.1%2520mAP50%2520for%2520BDD100K%2520Clear%250A%2524%255Crightarrow%2524%2520DENSE%2520Foggy%252C%2520%252B3.7%2520mAP50%2520for%2520BDD100K%2520Day%2520%2524%255Crightarrow%2524%2520Night%252C%2520%252B3.0%250AmAP50%2520for%2520BDD100K%2520Clear%2520%2524%255Crightarrow%2524%2520Rainy%252C%2520and%2520%252B6.3%2520mIoU%2520for%2520Cityscapes%250A%2524%255Crightarrow%2524%2520ACDC.%2520Besides%252C%2520Our%2520method%2520adds%2520minimal%2520training%2520memory%2520and%2520no%250Aadditional%2520inference%2520latency.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/ShenZheng2000/Instance-Warp%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12712v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance-Warp%3A%20Saliency%20Guided%20Image%20Warping%20for%20Unsupervised%20Domain%0A%20%20Adaptation&entry.906535625=Shen%20Zheng%20and%20Anurag%20Ghosh%20and%20Srinivasa%20G.%20Narasimhan&entry.1292438233=%20%20Driving%20is%20challenging%20in%20conditions%20like%20night%2C%20rain%2C%20and%20snow.%20Lack%20of%20good%0Alabeled%20datasets%20has%20hampered%20progress%20in%20scene%20understanding%20under%20such%0Aconditions.%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20using%20large%20labeled%20clear-day%0Adatasets%20is%20a%20promising%20research%20direction%20in%20such%20cases.%20However%2C%20many%20UDA%0Amethods%20are%20trained%20with%20dominant%20scene%20backgrounds%20%28e.g.%2C%20roads%2C%20sky%2C%0Asidewalks%29%20that%20appear%20dramatically%20different%20across%20domains.%20As%20a%20result%2C%20they%0Astruggle%20to%20learn%20effective%20features%20of%20smaller%20and%20often%20sparse%20foreground%0Aobjects%20%28e.g.%2C%20people%2C%20vehicles%2C%20signs%29.%0A%20%20In%20this%20work%2C%20we%20improve%20UDA%20training%20by%20applying%20in-place%20image%20warping%20to%0Afocus%20on%20salient%20objects.%20We%20design%20instance-level%20saliency%20guidance%20to%0Aadaptively%20oversample%20object%20regions%20and%20undersample%20background%20areas%2C%20which%0Areduces%20adverse%20effects%20from%20background%20context%20and%20enhances%20backbone%20feature%0Alearning.%20Our%20approach%20improves%20adaptation%20across%20geographies%2C%20lighting%2C%20and%0Aweather%20conditions%2C%20and%20is%20agnostic%20to%20the%20task%20%28segmentation%2C%20detection%29%2C%0Adomain%20adaptation%20algorithm%2C%20saliency%20guidance%2C%20and%20underlying%20model%0Aarchitecture.%20Result%20highlights%20include%20%2B6.1%20mAP50%20for%20BDD100K%20Clear%0A%24%5Crightarrow%24%20DENSE%20Foggy%2C%20%2B3.7%20mAP50%20for%20BDD100K%20Day%20%24%5Crightarrow%24%20Night%2C%20%2B3.0%0AmAP50%20for%20BDD100K%20Clear%20%24%5Crightarrow%24%20Rainy%2C%20and%20%2B6.3%20mIoU%20for%20Cityscapes%0A%24%5Crightarrow%24%20ACDC.%20Besides%2C%20Our%20method%20adds%20minimal%20training%20memory%20and%20no%0Aadditional%20inference%20latency.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ShenZheng2000/Instance-Warp%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12712v3&entry.124074799=Read"},
{"title": "Distributionally robust self-supervised learning for tabular data", "author": "Shantanu Ghosh and Tiankang Xie and Mikhail Kuznetsov", "abstract": "  Machine learning (ML) models trained using Empirical Risk Minimization (ERM)\noften exhibit systematic errors on specific subpopulations of tabular data,\nknown as error slices. Learning robust representation in presence of error\nslices is challenging, especially in self-supervised settings during the\nfeature reconstruction phase, due to high cardinality features and the\ncomplexity of constructing error sets. Traditional robust representation\nlearning methods are largely focused on improving worst group performance in\nsupervised setting in computer vision, leaving a gap in approaches tailored for\ntabular data. We address this gap by developing a framework to learn robust\nrepresentation in tabular data during self-supervised pre-training. Our\napproach utilizes an encoder-decoder model trained with Masked Language\nModeling (MLM) loss to learn robust latent representations. This paper applies\nthe Just Train Twice (JTT) and Deep Feature Reweighting (DFR) methods during\nthe pre-training phase for tabular data. These methods fine-tune the ERM\npre-trained model by up-weighting error-prone samples or creating balanced\ndatasets for specific categorical features. This results in specialized models\nfor each feature, which are then used in an ensemble approach to enhance\ndownstream classification performance. This methodology improves robustness\nacross slices, thus enhancing overall generalization performance. Extensive\nexperiments across various datasets demonstrate the efficacy of our approach.\nThe code is available:\n\\url{https://github.com/amazon-science/distributionally-robust-self-supervised-learning-for-tabular-data}.\n", "link": "http://arxiv.org/abs/2410.08511v5", "date": "2024-12-04", "relevancy": 2.212, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5669}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5469}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributionally%20robust%20self-supervised%20learning%20for%20tabular%20data&body=Title%3A%20Distributionally%20robust%20self-supervised%20learning%20for%20tabular%20data%0AAuthor%3A%20Shantanu%20Ghosh%20and%20Tiankang%20Xie%20and%20Mikhail%20Kuznetsov%0AAbstract%3A%20%20%20Machine%20learning%20%28ML%29%20models%20trained%20using%20Empirical%20Risk%20Minimization%20%28ERM%29%0Aoften%20exhibit%20systematic%20errors%20on%20specific%20subpopulations%20of%20tabular%20data%2C%0Aknown%20as%20error%20slices.%20Learning%20robust%20representation%20in%20presence%20of%20error%0Aslices%20is%20challenging%2C%20especially%20in%20self-supervised%20settings%20during%20the%0Afeature%20reconstruction%20phase%2C%20due%20to%20high%20cardinality%20features%20and%20the%0Acomplexity%20of%20constructing%20error%20sets.%20Traditional%20robust%20representation%0Alearning%20methods%20are%20largely%20focused%20on%20improving%20worst%20group%20performance%20in%0Asupervised%20setting%20in%20computer%20vision%2C%20leaving%20a%20gap%20in%20approaches%20tailored%20for%0Atabular%20data.%20We%20address%20this%20gap%20by%20developing%20a%20framework%20to%20learn%20robust%0Arepresentation%20in%20tabular%20data%20during%20self-supervised%20pre-training.%20Our%0Aapproach%20utilizes%20an%20encoder-decoder%20model%20trained%20with%20Masked%20Language%0AModeling%20%28MLM%29%20loss%20to%20learn%20robust%20latent%20representations.%20This%20paper%20applies%0Athe%20Just%20Train%20Twice%20%28JTT%29%20and%20Deep%20Feature%20Reweighting%20%28DFR%29%20methods%20during%0Athe%20pre-training%20phase%20for%20tabular%20data.%20These%20methods%20fine-tune%20the%20ERM%0Apre-trained%20model%20by%20up-weighting%20error-prone%20samples%20or%20creating%20balanced%0Adatasets%20for%20specific%20categorical%20features.%20This%20results%20in%20specialized%20models%0Afor%20each%20feature%2C%20which%20are%20then%20used%20in%20an%20ensemble%20approach%20to%20enhance%0Adownstream%20classification%20performance.%20This%20methodology%20improves%20robustness%0Aacross%20slices%2C%20thus%20enhancing%20overall%20generalization%20performance.%20Extensive%0Aexperiments%20across%20various%20datasets%20demonstrate%20the%20efficacy%20of%20our%20approach.%0AThe%20code%20is%20available%3A%0A%5Curl%7Bhttps%3A//github.com/amazon-science/distributionally-robust-self-supervised-learning-for-tabular-data%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08511v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributionally%2520robust%2520self-supervised%2520learning%2520for%2520tabular%2520data%26entry.906535625%3DShantanu%2520Ghosh%2520and%2520Tiankang%2520Xie%2520and%2520Mikhail%2520Kuznetsov%26entry.1292438233%3D%2520%2520Machine%2520learning%2520%2528ML%2529%2520models%2520trained%2520using%2520Empirical%2520Risk%2520Minimization%2520%2528ERM%2529%250Aoften%2520exhibit%2520systematic%2520errors%2520on%2520specific%2520subpopulations%2520of%2520tabular%2520data%252C%250Aknown%2520as%2520error%2520slices.%2520Learning%2520robust%2520representation%2520in%2520presence%2520of%2520error%250Aslices%2520is%2520challenging%252C%2520especially%2520in%2520self-supervised%2520settings%2520during%2520the%250Afeature%2520reconstruction%2520phase%252C%2520due%2520to%2520high%2520cardinality%2520features%2520and%2520the%250Acomplexity%2520of%2520constructing%2520error%2520sets.%2520Traditional%2520robust%2520representation%250Alearning%2520methods%2520are%2520largely%2520focused%2520on%2520improving%2520worst%2520group%2520performance%2520in%250Asupervised%2520setting%2520in%2520computer%2520vision%252C%2520leaving%2520a%2520gap%2520in%2520approaches%2520tailored%2520for%250Atabular%2520data.%2520We%2520address%2520this%2520gap%2520by%2520developing%2520a%2520framework%2520to%2520learn%2520robust%250Arepresentation%2520in%2520tabular%2520data%2520during%2520self-supervised%2520pre-training.%2520Our%250Aapproach%2520utilizes%2520an%2520encoder-decoder%2520model%2520trained%2520with%2520Masked%2520Language%250AModeling%2520%2528MLM%2529%2520loss%2520to%2520learn%2520robust%2520latent%2520representations.%2520This%2520paper%2520applies%250Athe%2520Just%2520Train%2520Twice%2520%2528JTT%2529%2520and%2520Deep%2520Feature%2520Reweighting%2520%2528DFR%2529%2520methods%2520during%250Athe%2520pre-training%2520phase%2520for%2520tabular%2520data.%2520These%2520methods%2520fine-tune%2520the%2520ERM%250Apre-trained%2520model%2520by%2520up-weighting%2520error-prone%2520samples%2520or%2520creating%2520balanced%250Adatasets%2520for%2520specific%2520categorical%2520features.%2520This%2520results%2520in%2520specialized%2520models%250Afor%2520each%2520feature%252C%2520which%2520are%2520then%2520used%2520in%2520an%2520ensemble%2520approach%2520to%2520enhance%250Adownstream%2520classification%2520performance.%2520This%2520methodology%2520improves%2520robustness%250Aacross%2520slices%252C%2520thus%2520enhancing%2520overall%2520generalization%2520performance.%2520Extensive%250Aexperiments%2520across%2520various%2520datasets%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520approach.%250AThe%2520code%2520is%2520available%253A%250A%255Curl%257Bhttps%253A//github.com/amazon-science/distributionally-robust-self-supervised-learning-for-tabular-data%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08511v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributionally%20robust%20self-supervised%20learning%20for%20tabular%20data&entry.906535625=Shantanu%20Ghosh%20and%20Tiankang%20Xie%20and%20Mikhail%20Kuznetsov&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20models%20trained%20using%20Empirical%20Risk%20Minimization%20%28ERM%29%0Aoften%20exhibit%20systematic%20errors%20on%20specific%20subpopulations%20of%20tabular%20data%2C%0Aknown%20as%20error%20slices.%20Learning%20robust%20representation%20in%20presence%20of%20error%0Aslices%20is%20challenging%2C%20especially%20in%20self-supervised%20settings%20during%20the%0Afeature%20reconstruction%20phase%2C%20due%20to%20high%20cardinality%20features%20and%20the%0Acomplexity%20of%20constructing%20error%20sets.%20Traditional%20robust%20representation%0Alearning%20methods%20are%20largely%20focused%20on%20improving%20worst%20group%20performance%20in%0Asupervised%20setting%20in%20computer%20vision%2C%20leaving%20a%20gap%20in%20approaches%20tailored%20for%0Atabular%20data.%20We%20address%20this%20gap%20by%20developing%20a%20framework%20to%20learn%20robust%0Arepresentation%20in%20tabular%20data%20during%20self-supervised%20pre-training.%20Our%0Aapproach%20utilizes%20an%20encoder-decoder%20model%20trained%20with%20Masked%20Language%0AModeling%20%28MLM%29%20loss%20to%20learn%20robust%20latent%20representations.%20This%20paper%20applies%0Athe%20Just%20Train%20Twice%20%28JTT%29%20and%20Deep%20Feature%20Reweighting%20%28DFR%29%20methods%20during%0Athe%20pre-training%20phase%20for%20tabular%20data.%20These%20methods%20fine-tune%20the%20ERM%0Apre-trained%20model%20by%20up-weighting%20error-prone%20samples%20or%20creating%20balanced%0Adatasets%20for%20specific%20categorical%20features.%20This%20results%20in%20specialized%20models%0Afor%20each%20feature%2C%20which%20are%20then%20used%20in%20an%20ensemble%20approach%20to%20enhance%0Adownstream%20classification%20performance.%20This%20methodology%20improves%20robustness%0Aacross%20slices%2C%20thus%20enhancing%20overall%20generalization%20performance.%20Extensive%0Aexperiments%20across%20various%20datasets%20demonstrate%20the%20efficacy%20of%20our%20approach.%0AThe%20code%20is%20available%3A%0A%5Curl%7Bhttps%3A//github.com/amazon-science/distributionally-robust-self-supervised-learning-for-tabular-data%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08511v5&entry.124074799=Read"},
{"title": "Data Fusion of Semantic and Depth Information in the Context of Object\n  Detection", "author": "Md Abu Yusuf and Md Rezaul Karim Khan and Partha Pratim Saha and Mohammed Mahbubur Rahaman", "abstract": "  Considerable study has already been conducted regarding autonomous driving in\nmodern era. An autonomous driving system must be extremely good at detecting\nobjects surrounding the car to ensure safety. In this paper, classification,\nand estimation of an object's (pedestrian) position (concerning an ego 3D\ncoordinate system) are studied and the distance between the ego vehicle and the\nobject in the context of autonomous driving is measured. To classify the\nobject, faster Region-based Convolution Neural Network (R-CNN) with inception\nv2 is utilized. First, a network is trained with customized dataset to estimate\nthe reference position of objects as well as the distance from the vehicle.\nFrom camera calibration to computing the distance, cutting-edge technologies of\ncomputer vision algorithms in a series of processes are applied to generate a\n3D reference point of the region of interest. The foremost step in this process\nis generating a disparity map using the concept of stereo vision.\n", "link": "http://arxiv.org/abs/2412.03490v1", "date": "2024-12-04", "relevancy": 2.2101, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5787}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Fusion%20of%20Semantic%20and%20Depth%20Information%20in%20the%20Context%20of%20Object%0A%20%20Detection&body=Title%3A%20Data%20Fusion%20of%20Semantic%20and%20Depth%20Information%20in%20the%20Context%20of%20Object%0A%20%20Detection%0AAuthor%3A%20Md%20Abu%20Yusuf%20and%20Md%20Rezaul%20Karim%20Khan%20and%20Partha%20Pratim%20Saha%20and%20Mohammed%20Mahbubur%20Rahaman%0AAbstract%3A%20%20%20Considerable%20study%20has%20already%20been%20conducted%20regarding%20autonomous%20driving%20in%0Amodern%20era.%20An%20autonomous%20driving%20system%20must%20be%20extremely%20good%20at%20detecting%0Aobjects%20surrounding%20the%20car%20to%20ensure%20safety.%20In%20this%20paper%2C%20classification%2C%0Aand%20estimation%20of%20an%20object%27s%20%28pedestrian%29%20position%20%28concerning%20an%20ego%203D%0Acoordinate%20system%29%20are%20studied%20and%20the%20distance%20between%20the%20ego%20vehicle%20and%20the%0Aobject%20in%20the%20context%20of%20autonomous%20driving%20is%20measured.%20To%20classify%20the%0Aobject%2C%20faster%20Region-based%20Convolution%20Neural%20Network%20%28R-CNN%29%20with%20inception%0Av2%20is%20utilized.%20First%2C%20a%20network%20is%20trained%20with%20customized%20dataset%20to%20estimate%0Athe%20reference%20position%20of%20objects%20as%20well%20as%20the%20distance%20from%20the%20vehicle.%0AFrom%20camera%20calibration%20to%20computing%20the%20distance%2C%20cutting-edge%20technologies%20of%0Acomputer%20vision%20algorithms%20in%20a%20series%20of%20processes%20are%20applied%20to%20generate%20a%0A3D%20reference%20point%20of%20the%20region%20of%20interest.%20The%20foremost%20step%20in%20this%20process%0Ais%20generating%20a%20disparity%20map%20using%20the%20concept%20of%20stereo%20vision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Fusion%2520of%2520Semantic%2520and%2520Depth%2520Information%2520in%2520the%2520Context%2520of%2520Object%250A%2520%2520Detection%26entry.906535625%3DMd%2520Abu%2520Yusuf%2520and%2520Md%2520Rezaul%2520Karim%2520Khan%2520and%2520Partha%2520Pratim%2520Saha%2520and%2520Mohammed%2520Mahbubur%2520Rahaman%26entry.1292438233%3D%2520%2520Considerable%2520study%2520has%2520already%2520been%2520conducted%2520regarding%2520autonomous%2520driving%2520in%250Amodern%2520era.%2520An%2520autonomous%2520driving%2520system%2520must%2520be%2520extremely%2520good%2520at%2520detecting%250Aobjects%2520surrounding%2520the%2520car%2520to%2520ensure%2520safety.%2520In%2520this%2520paper%252C%2520classification%252C%250Aand%2520estimation%2520of%2520an%2520object%2527s%2520%2528pedestrian%2529%2520position%2520%2528concerning%2520an%2520ego%25203D%250Acoordinate%2520system%2529%2520are%2520studied%2520and%2520the%2520distance%2520between%2520the%2520ego%2520vehicle%2520and%2520the%250Aobject%2520in%2520the%2520context%2520of%2520autonomous%2520driving%2520is%2520measured.%2520To%2520classify%2520the%250Aobject%252C%2520faster%2520Region-based%2520Convolution%2520Neural%2520Network%2520%2528R-CNN%2529%2520with%2520inception%250Av2%2520is%2520utilized.%2520First%252C%2520a%2520network%2520is%2520trained%2520with%2520customized%2520dataset%2520to%2520estimate%250Athe%2520reference%2520position%2520of%2520objects%2520as%2520well%2520as%2520the%2520distance%2520from%2520the%2520vehicle.%250AFrom%2520camera%2520calibration%2520to%2520computing%2520the%2520distance%252C%2520cutting-edge%2520technologies%2520of%250Acomputer%2520vision%2520algorithms%2520in%2520a%2520series%2520of%2520processes%2520are%2520applied%2520to%2520generate%2520a%250A3D%2520reference%2520point%2520of%2520the%2520region%2520of%2520interest.%2520The%2520foremost%2520step%2520in%2520this%2520process%250Ais%2520generating%2520a%2520disparity%2520map%2520using%2520the%2520concept%2520of%2520stereo%2520vision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Fusion%20of%20Semantic%20and%20Depth%20Information%20in%20the%20Context%20of%20Object%0A%20%20Detection&entry.906535625=Md%20Abu%20Yusuf%20and%20Md%20Rezaul%20Karim%20Khan%20and%20Partha%20Pratim%20Saha%20and%20Mohammed%20Mahbubur%20Rahaman&entry.1292438233=%20%20Considerable%20study%20has%20already%20been%20conducted%20regarding%20autonomous%20driving%20in%0Amodern%20era.%20An%20autonomous%20driving%20system%20must%20be%20extremely%20good%20at%20detecting%0Aobjects%20surrounding%20the%20car%20to%20ensure%20safety.%20In%20this%20paper%2C%20classification%2C%0Aand%20estimation%20of%20an%20object%27s%20%28pedestrian%29%20position%20%28concerning%20an%20ego%203D%0Acoordinate%20system%29%20are%20studied%20and%20the%20distance%20between%20the%20ego%20vehicle%20and%20the%0Aobject%20in%20the%20context%20of%20autonomous%20driving%20is%20measured.%20To%20classify%20the%0Aobject%2C%20faster%20Region-based%20Convolution%20Neural%20Network%20%28R-CNN%29%20with%20inception%0Av2%20is%20utilized.%20First%2C%20a%20network%20is%20trained%20with%20customized%20dataset%20to%20estimate%0Athe%20reference%20position%20of%20objects%20as%20well%20as%20the%20distance%20from%20the%20vehicle.%0AFrom%20camera%20calibration%20to%20computing%20the%20distance%2C%20cutting-edge%20technologies%20of%0Acomputer%20vision%20algorithms%20in%20a%20series%20of%20processes%20are%20applied%20to%20generate%20a%0A3D%20reference%20point%20of%20the%20region%20of%20interest.%20The%20foremost%20step%20in%20this%20process%0Ais%20generating%20a%20disparity%20map%20using%20the%20concept%20of%20stereo%20vision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03490v1&entry.124074799=Read"},
{"title": "Streaming Detection of Queried Event Start", "author": "Cristobal Eyzaguirre and Eric Tang and Shyamal Buch and Adrien Gaidon and Jiajun Wu and Juan Carlos Niebles", "abstract": "  Robotics, autonomous driving, augmented reality, and many embodied computer\nvision applications must quickly react to user-defined events unfolding in real\ntime. We address this setting by proposing a novel task for multimodal video\nunderstanding-Streaming Detection of Queried Event Start (SDQES). The goal of\nSDQES is to identify the beginning of a complex event as described by a natural\nlanguage query, with high accuracy and low latency. We introduce a new\nbenchmark based on the Ego4D dataset, as well as new task-specific metrics to\nstudy streaming multimodal detection of diverse events in an egocentric video\nsetting. Inspired by parameter-efficient fine-tuning methods in NLP and for\nvideo tasks, we propose adapter-based baselines that enable image-to-video\ntransfer learning, allowing for efficient online video modeling. We evaluate\nthree vision-language backbones and three adapter architectures on both\nshort-clip and untrimmed video settings.\n", "link": "http://arxiv.org/abs/2412.03567v1", "date": "2024-12-04", "relevancy": 2.203, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5559}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Streaming%20Detection%20of%20Queried%20Event%20Start&body=Title%3A%20Streaming%20Detection%20of%20Queried%20Event%20Start%0AAuthor%3A%20Cristobal%20Eyzaguirre%20and%20Eric%20Tang%20and%20Shyamal%20Buch%20and%20Adrien%20Gaidon%20and%20Jiajun%20Wu%20and%20Juan%20Carlos%20Niebles%0AAbstract%3A%20%20%20Robotics%2C%20autonomous%20driving%2C%20augmented%20reality%2C%20and%20many%20embodied%20computer%0Avision%20applications%20must%20quickly%20react%20to%20user-defined%20events%20unfolding%20in%20real%0Atime.%20We%20address%20this%20setting%20by%20proposing%20a%20novel%20task%20for%20multimodal%20video%0Aunderstanding-Streaming%20Detection%20of%20Queried%20Event%20Start%20%28SDQES%29.%20The%20goal%20of%0ASDQES%20is%20to%20identify%20the%20beginning%20of%20a%20complex%20event%20as%20described%20by%20a%20natural%0Alanguage%20query%2C%20with%20high%20accuracy%20and%20low%20latency.%20We%20introduce%20a%20new%0Abenchmark%20based%20on%20the%20Ego4D%20dataset%2C%20as%20well%20as%20new%20task-specific%20metrics%20to%0Astudy%20streaming%20multimodal%20detection%20of%20diverse%20events%20in%20an%20egocentric%20video%0Asetting.%20Inspired%20by%20parameter-efficient%20fine-tuning%20methods%20in%20NLP%20and%20for%0Avideo%20tasks%2C%20we%20propose%20adapter-based%20baselines%20that%20enable%20image-to-video%0Atransfer%20learning%2C%20allowing%20for%20efficient%20online%20video%20modeling.%20We%20evaluate%0Athree%20vision-language%20backbones%20and%20three%20adapter%20architectures%20on%20both%0Ashort-clip%20and%20untrimmed%20video%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreaming%2520Detection%2520of%2520Queried%2520Event%2520Start%26entry.906535625%3DCristobal%2520Eyzaguirre%2520and%2520Eric%2520Tang%2520and%2520Shyamal%2520Buch%2520and%2520Adrien%2520Gaidon%2520and%2520Jiajun%2520Wu%2520and%2520Juan%2520Carlos%2520Niebles%26entry.1292438233%3D%2520%2520Robotics%252C%2520autonomous%2520driving%252C%2520augmented%2520reality%252C%2520and%2520many%2520embodied%2520computer%250Avision%2520applications%2520must%2520quickly%2520react%2520to%2520user-defined%2520events%2520unfolding%2520in%2520real%250Atime.%2520We%2520address%2520this%2520setting%2520by%2520proposing%2520a%2520novel%2520task%2520for%2520multimodal%2520video%250Aunderstanding-Streaming%2520Detection%2520of%2520Queried%2520Event%2520Start%2520%2528SDQES%2529.%2520The%2520goal%2520of%250ASDQES%2520is%2520to%2520identify%2520the%2520beginning%2520of%2520a%2520complex%2520event%2520as%2520described%2520by%2520a%2520natural%250Alanguage%2520query%252C%2520with%2520high%2520accuracy%2520and%2520low%2520latency.%2520We%2520introduce%2520a%2520new%250Abenchmark%2520based%2520on%2520the%2520Ego4D%2520dataset%252C%2520as%2520well%2520as%2520new%2520task-specific%2520metrics%2520to%250Astudy%2520streaming%2520multimodal%2520detection%2520of%2520diverse%2520events%2520in%2520an%2520egocentric%2520video%250Asetting.%2520Inspired%2520by%2520parameter-efficient%2520fine-tuning%2520methods%2520in%2520NLP%2520and%2520for%250Avideo%2520tasks%252C%2520we%2520propose%2520adapter-based%2520baselines%2520that%2520enable%2520image-to-video%250Atransfer%2520learning%252C%2520allowing%2520for%2520efficient%2520online%2520video%2520modeling.%2520We%2520evaluate%250Athree%2520vision-language%2520backbones%2520and%2520three%2520adapter%2520architectures%2520on%2520both%250Ashort-clip%2520and%2520untrimmed%2520video%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Streaming%20Detection%20of%20Queried%20Event%20Start&entry.906535625=Cristobal%20Eyzaguirre%20and%20Eric%20Tang%20and%20Shyamal%20Buch%20and%20Adrien%20Gaidon%20and%20Jiajun%20Wu%20and%20Juan%20Carlos%20Niebles&entry.1292438233=%20%20Robotics%2C%20autonomous%20driving%2C%20augmented%20reality%2C%20and%20many%20embodied%20computer%0Avision%20applications%20must%20quickly%20react%20to%20user-defined%20events%20unfolding%20in%20real%0Atime.%20We%20address%20this%20setting%20by%20proposing%20a%20novel%20task%20for%20multimodal%20video%0Aunderstanding-Streaming%20Detection%20of%20Queried%20Event%20Start%20%28SDQES%29.%20The%20goal%20of%0ASDQES%20is%20to%20identify%20the%20beginning%20of%20a%20complex%20event%20as%20described%20by%20a%20natural%0Alanguage%20query%2C%20with%20high%20accuracy%20and%20low%20latency.%20We%20introduce%20a%20new%0Abenchmark%20based%20on%20the%20Ego4D%20dataset%2C%20as%20well%20as%20new%20task-specific%20metrics%20to%0Astudy%20streaming%20multimodal%20detection%20of%20diverse%20events%20in%20an%20egocentric%20video%0Asetting.%20Inspired%20by%20parameter-efficient%20fine-tuning%20methods%20in%20NLP%20and%20for%0Avideo%20tasks%2C%20we%20propose%20adapter-based%20baselines%20that%20enable%20image-to-video%0Atransfer%20learning%2C%20allowing%20for%20efficient%20online%20video%20modeling.%20We%20evaluate%0Athree%20vision-language%20backbones%20and%20three%20adapter%20architectures%20on%20both%0Ashort-clip%20and%20untrimmed%20video%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03567v1&entry.124074799=Read"},
{"title": "Fast and reliable uncertainty quantification with neural network\n  ensembles for industrial image classification", "author": "Arthur Thuy and Dries F. Benoit", "abstract": "  Image classification with neural networks (NNs) is widely used in industrial\nprocesses, situations where the model likely encounters unknown objects during\ndeployment, i.e., out-of-distribution (OOD) data. Worryingly, NNs tend to make\nconfident yet incorrect predictions when confronted with OOD data. To increase\nthe models' reliability, they should quantify the uncertainty in their own\npredictions, communicating when the output should (not) be trusted. Deep\nensembles, composed of multiple independent NNs, have been shown to perform\nstrongly but are computationally expensive. Recent research has proposed more\nefficient NN ensembles, namely the snapshot, batch, and multi-input\nmulti-output ensemble. This study investigates the predictive and uncertainty\nperformance of efficient NN ensembles in the context of image classification\nfor industrial processes. It is the first to provide a comprehensive comparison\nand it proposes a novel Diversity Quality metric to quantify the ensembles'\nperformance on the in-distribution and OOD sets in one single metric. The\nresults highlight the batch ensemble as a cost-effective and competitive\nalternative to the deep ensemble. It matches the deep ensemble in both\nuncertainty and accuracy while exhibiting considerable savings in training\ntime, test time, and memory storage.\n", "link": "http://arxiv.org/abs/2403.10182v3", "date": "2024-12-04", "relevancy": 2.1898, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5726}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5584}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20reliable%20uncertainty%20quantification%20with%20neural%20network%0A%20%20ensembles%20for%20industrial%20image%20classification&body=Title%3A%20Fast%20and%20reliable%20uncertainty%20quantification%20with%20neural%20network%0A%20%20ensembles%20for%20industrial%20image%20classification%0AAuthor%3A%20Arthur%20Thuy%20and%20Dries%20F.%20Benoit%0AAbstract%3A%20%20%20Image%20classification%20with%20neural%20networks%20%28NNs%29%20is%20widely%20used%20in%20industrial%0Aprocesses%2C%20situations%20where%20the%20model%20likely%20encounters%20unknown%20objects%20during%0Adeployment%2C%20i.e.%2C%20out-of-distribution%20%28OOD%29%20data.%20Worryingly%2C%20NNs%20tend%20to%20make%0Aconfident%20yet%20incorrect%20predictions%20when%20confronted%20with%20OOD%20data.%20To%20increase%0Athe%20models%27%20reliability%2C%20they%20should%20quantify%20the%20uncertainty%20in%20their%20own%0Apredictions%2C%20communicating%20when%20the%20output%20should%20%28not%29%20be%20trusted.%20Deep%0Aensembles%2C%20composed%20of%20multiple%20independent%20NNs%2C%20have%20been%20shown%20to%20perform%0Astrongly%20but%20are%20computationally%20expensive.%20Recent%20research%20has%20proposed%20more%0Aefficient%20NN%20ensembles%2C%20namely%20the%20snapshot%2C%20batch%2C%20and%20multi-input%0Amulti-output%20ensemble.%20This%20study%20investigates%20the%20predictive%20and%20uncertainty%0Aperformance%20of%20efficient%20NN%20ensembles%20in%20the%20context%20of%20image%20classification%0Afor%20industrial%20processes.%20It%20is%20the%20first%20to%20provide%20a%20comprehensive%20comparison%0Aand%20it%20proposes%20a%20novel%20Diversity%20Quality%20metric%20to%20quantify%20the%20ensembles%27%0Aperformance%20on%20the%20in-distribution%20and%20OOD%20sets%20in%20one%20single%20metric.%20The%0Aresults%20highlight%20the%20batch%20ensemble%20as%20a%20cost-effective%20and%20competitive%0Aalternative%20to%20the%20deep%20ensemble.%20It%20matches%20the%20deep%20ensemble%20in%20both%0Auncertainty%20and%20accuracy%20while%20exhibiting%20considerable%20savings%20in%20training%0Atime%2C%20test%20time%2C%20and%20memory%20storage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10182v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520reliable%2520uncertainty%2520quantification%2520with%2520neural%2520network%250A%2520%2520ensembles%2520for%2520industrial%2520image%2520classification%26entry.906535625%3DArthur%2520Thuy%2520and%2520Dries%2520F.%2520Benoit%26entry.1292438233%3D%2520%2520Image%2520classification%2520with%2520neural%2520networks%2520%2528NNs%2529%2520is%2520widely%2520used%2520in%2520industrial%250Aprocesses%252C%2520situations%2520where%2520the%2520model%2520likely%2520encounters%2520unknown%2520objects%2520during%250Adeployment%252C%2520i.e.%252C%2520out-of-distribution%2520%2528OOD%2529%2520data.%2520Worryingly%252C%2520NNs%2520tend%2520to%2520make%250Aconfident%2520yet%2520incorrect%2520predictions%2520when%2520confronted%2520with%2520OOD%2520data.%2520To%2520increase%250Athe%2520models%2527%2520reliability%252C%2520they%2520should%2520quantify%2520the%2520uncertainty%2520in%2520their%2520own%250Apredictions%252C%2520communicating%2520when%2520the%2520output%2520should%2520%2528not%2529%2520be%2520trusted.%2520Deep%250Aensembles%252C%2520composed%2520of%2520multiple%2520independent%2520NNs%252C%2520have%2520been%2520shown%2520to%2520perform%250Astrongly%2520but%2520are%2520computationally%2520expensive.%2520Recent%2520research%2520has%2520proposed%2520more%250Aefficient%2520NN%2520ensembles%252C%2520namely%2520the%2520snapshot%252C%2520batch%252C%2520and%2520multi-input%250Amulti-output%2520ensemble.%2520This%2520study%2520investigates%2520the%2520predictive%2520and%2520uncertainty%250Aperformance%2520of%2520efficient%2520NN%2520ensembles%2520in%2520the%2520context%2520of%2520image%2520classification%250Afor%2520industrial%2520processes.%2520It%2520is%2520the%2520first%2520to%2520provide%2520a%2520comprehensive%2520comparison%250Aand%2520it%2520proposes%2520a%2520novel%2520Diversity%2520Quality%2520metric%2520to%2520quantify%2520the%2520ensembles%2527%250Aperformance%2520on%2520the%2520in-distribution%2520and%2520OOD%2520sets%2520in%2520one%2520single%2520metric.%2520The%250Aresults%2520highlight%2520the%2520batch%2520ensemble%2520as%2520a%2520cost-effective%2520and%2520competitive%250Aalternative%2520to%2520the%2520deep%2520ensemble.%2520It%2520matches%2520the%2520deep%2520ensemble%2520in%2520both%250Auncertainty%2520and%2520accuracy%2520while%2520exhibiting%2520considerable%2520savings%2520in%2520training%250Atime%252C%2520test%2520time%252C%2520and%2520memory%2520storage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10182v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20reliable%20uncertainty%20quantification%20with%20neural%20network%0A%20%20ensembles%20for%20industrial%20image%20classification&entry.906535625=Arthur%20Thuy%20and%20Dries%20F.%20Benoit&entry.1292438233=%20%20Image%20classification%20with%20neural%20networks%20%28NNs%29%20is%20widely%20used%20in%20industrial%0Aprocesses%2C%20situations%20where%20the%20model%20likely%20encounters%20unknown%20objects%20during%0Adeployment%2C%20i.e.%2C%20out-of-distribution%20%28OOD%29%20data.%20Worryingly%2C%20NNs%20tend%20to%20make%0Aconfident%20yet%20incorrect%20predictions%20when%20confronted%20with%20OOD%20data.%20To%20increase%0Athe%20models%27%20reliability%2C%20they%20should%20quantify%20the%20uncertainty%20in%20their%20own%0Apredictions%2C%20communicating%20when%20the%20output%20should%20%28not%29%20be%20trusted.%20Deep%0Aensembles%2C%20composed%20of%20multiple%20independent%20NNs%2C%20have%20been%20shown%20to%20perform%0Astrongly%20but%20are%20computationally%20expensive.%20Recent%20research%20has%20proposed%20more%0Aefficient%20NN%20ensembles%2C%20namely%20the%20snapshot%2C%20batch%2C%20and%20multi-input%0Amulti-output%20ensemble.%20This%20study%20investigates%20the%20predictive%20and%20uncertainty%0Aperformance%20of%20efficient%20NN%20ensembles%20in%20the%20context%20of%20image%20classification%0Afor%20industrial%20processes.%20It%20is%20the%20first%20to%20provide%20a%20comprehensive%20comparison%0Aand%20it%20proposes%20a%20novel%20Diversity%20Quality%20metric%20to%20quantify%20the%20ensembles%27%0Aperformance%20on%20the%20in-distribution%20and%20OOD%20sets%20in%20one%20single%20metric.%20The%0Aresults%20highlight%20the%20batch%20ensemble%20as%20a%20cost-effective%20and%20competitive%0Aalternative%20to%20the%20deep%20ensemble.%20It%20matches%20the%20deep%20ensemble%20in%20both%0Auncertainty%20and%20accuracy%20while%20exhibiting%20considerable%20savings%20in%20training%0Atime%2C%20test%20time%2C%20and%20memory%20storage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10182v3&entry.124074799=Read"},
{"title": "UniVAD: A Training-free Unified Model for Few-shot Visual Anomaly\n  Detection", "author": "Zhaopeng Gu and Bingke Zhu and Guibo Zhu and Yingying Chen and Ming Tang and Jinqiao Wang", "abstract": "  Visual Anomaly Detection (VAD) aims to identify abnormal samples in images\nthat deviate from normal patterns, covering multiple domains, including\nindustrial, logical, and medical fields. Due to the domain gaps between these\nfields, existing VAD methods are typically tailored to each domain, with\nspecialized detection techniques and model architectures that are difficult to\ngeneralize across different domains. Moreover, even within the same domain,\ncurrent VAD approaches often follow a \"one-category-one-model\" paradigm,\nrequiring large amounts of normal samples to train class-specific models,\nresulting in poor generalizability and hindering unified evaluation across\ndomains. To address this issue, we propose a generalized few-shot VAD method,\nUniVAD, capable of detecting anomalies across various domains, such as\nindustrial, logical, and medical anomalies, with a training-free unified model.\nUniVAD only needs few normal samples as references during testing to detect\nanomalies in previously unseen objects, without training on the specific\ndomain. Specifically, UniVAD employs a Contextual Component Clustering ($C^3$)\nmodule based on clustering and vision foundation models to segment components\nwithin the image accurately, and leverages Component-Aware Patch Matching\n(CAPM) and Graph-Enhanced Component Modeling (GECM) modules to detect anomalies\nat different semantic levels, which are aggregated to produce the final\ndetection result. We conduct experiments on nine datasets spanning industrial,\nlogical, and medical fields, and the results demonstrate that UniVAD achieves\nstate-of-the-art performance in few-shot anomaly detection tasks across\nmultiple domains, outperforming domain-specific anomaly detection models. The\ncode will be made publicly available.\n", "link": "http://arxiv.org/abs/2412.03342v1", "date": "2024-12-04", "relevancy": 2.1866, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5677}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5429}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniVAD%3A%20A%20Training-free%20Unified%20Model%20for%20Few-shot%20Visual%20Anomaly%0A%20%20Detection&body=Title%3A%20UniVAD%3A%20A%20Training-free%20Unified%20Model%20for%20Few-shot%20Visual%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Zhaopeng%20Gu%20and%20Bingke%20Zhu%20and%20Guibo%20Zhu%20and%20Yingying%20Chen%20and%20Ming%20Tang%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Visual%20Anomaly%20Detection%20%28VAD%29%20aims%20to%20identify%20abnormal%20samples%20in%20images%0Athat%20deviate%20from%20normal%20patterns%2C%20covering%20multiple%20domains%2C%20including%0Aindustrial%2C%20logical%2C%20and%20medical%20fields.%20Due%20to%20the%20domain%20gaps%20between%20these%0Afields%2C%20existing%20VAD%20methods%20are%20typically%20tailored%20to%20each%20domain%2C%20with%0Aspecialized%20detection%20techniques%20and%20model%20architectures%20that%20are%20difficult%20to%0Ageneralize%20across%20different%20domains.%20Moreover%2C%20even%20within%20the%20same%20domain%2C%0Acurrent%20VAD%20approaches%20often%20follow%20a%20%22one-category-one-model%22%20paradigm%2C%0Arequiring%20large%20amounts%20of%20normal%20samples%20to%20train%20class-specific%20models%2C%0Aresulting%20in%20poor%20generalizability%20and%20hindering%20unified%20evaluation%20across%0Adomains.%20To%20address%20this%20issue%2C%20we%20propose%20a%20generalized%20few-shot%20VAD%20method%2C%0AUniVAD%2C%20capable%20of%20detecting%20anomalies%20across%20various%20domains%2C%20such%20as%0Aindustrial%2C%20logical%2C%20and%20medical%20anomalies%2C%20with%20a%20training-free%20unified%20model.%0AUniVAD%20only%20needs%20few%20normal%20samples%20as%20references%20during%20testing%20to%20detect%0Aanomalies%20in%20previously%20unseen%20objects%2C%20without%20training%20on%20the%20specific%0Adomain.%20Specifically%2C%20UniVAD%20employs%20a%20Contextual%20Component%20Clustering%20%28%24C%5E3%24%29%0Amodule%20based%20on%20clustering%20and%20vision%20foundation%20models%20to%20segment%20components%0Awithin%20the%20image%20accurately%2C%20and%20leverages%20Component-Aware%20Patch%20Matching%0A%28CAPM%29%20and%20Graph-Enhanced%20Component%20Modeling%20%28GECM%29%20modules%20to%20detect%20anomalies%0Aat%20different%20semantic%20levels%2C%20which%20are%20aggregated%20to%20produce%20the%20final%0Adetection%20result.%20We%20conduct%20experiments%20on%20nine%20datasets%20spanning%20industrial%2C%0Alogical%2C%20and%20medical%20fields%2C%20and%20the%20results%20demonstrate%20that%20UniVAD%20achieves%0Astate-of-the-art%20performance%20in%20few-shot%20anomaly%20detection%20tasks%20across%0Amultiple%20domains%2C%20outperforming%20domain-specific%20anomaly%20detection%20models.%20The%0Acode%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniVAD%253A%2520A%2520Training-free%2520Unified%2520Model%2520for%2520Few-shot%2520Visual%2520Anomaly%250A%2520%2520Detection%26entry.906535625%3DZhaopeng%2520Gu%2520and%2520Bingke%2520Zhu%2520and%2520Guibo%2520Zhu%2520and%2520Yingying%2520Chen%2520and%2520Ming%2520Tang%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Visual%2520Anomaly%2520Detection%2520%2528VAD%2529%2520aims%2520to%2520identify%2520abnormal%2520samples%2520in%2520images%250Athat%2520deviate%2520from%2520normal%2520patterns%252C%2520covering%2520multiple%2520domains%252C%2520including%250Aindustrial%252C%2520logical%252C%2520and%2520medical%2520fields.%2520Due%2520to%2520the%2520domain%2520gaps%2520between%2520these%250Afields%252C%2520existing%2520VAD%2520methods%2520are%2520typically%2520tailored%2520to%2520each%2520domain%252C%2520with%250Aspecialized%2520detection%2520techniques%2520and%2520model%2520architectures%2520that%2520are%2520difficult%2520to%250Ageneralize%2520across%2520different%2520domains.%2520Moreover%252C%2520even%2520within%2520the%2520same%2520domain%252C%250Acurrent%2520VAD%2520approaches%2520often%2520follow%2520a%2520%2522one-category-one-model%2522%2520paradigm%252C%250Arequiring%2520large%2520amounts%2520of%2520normal%2520samples%2520to%2520train%2520class-specific%2520models%252C%250Aresulting%2520in%2520poor%2520generalizability%2520and%2520hindering%2520unified%2520evaluation%2520across%250Adomains.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520generalized%2520few-shot%2520VAD%2520method%252C%250AUniVAD%252C%2520capable%2520of%2520detecting%2520anomalies%2520across%2520various%2520domains%252C%2520such%2520as%250Aindustrial%252C%2520logical%252C%2520and%2520medical%2520anomalies%252C%2520with%2520a%2520training-free%2520unified%2520model.%250AUniVAD%2520only%2520needs%2520few%2520normal%2520samples%2520as%2520references%2520during%2520testing%2520to%2520detect%250Aanomalies%2520in%2520previously%2520unseen%2520objects%252C%2520without%2520training%2520on%2520the%2520specific%250Adomain.%2520Specifically%252C%2520UniVAD%2520employs%2520a%2520Contextual%2520Component%2520Clustering%2520%2528%2524C%255E3%2524%2529%250Amodule%2520based%2520on%2520clustering%2520and%2520vision%2520foundation%2520models%2520to%2520segment%2520components%250Awithin%2520the%2520image%2520accurately%252C%2520and%2520leverages%2520Component-Aware%2520Patch%2520Matching%250A%2528CAPM%2529%2520and%2520Graph-Enhanced%2520Component%2520Modeling%2520%2528GECM%2529%2520modules%2520to%2520detect%2520anomalies%250Aat%2520different%2520semantic%2520levels%252C%2520which%2520are%2520aggregated%2520to%2520produce%2520the%2520final%250Adetection%2520result.%2520We%2520conduct%2520experiments%2520on%2520nine%2520datasets%2520spanning%2520industrial%252C%250Alogical%252C%2520and%2520medical%2520fields%252C%2520and%2520the%2520results%2520demonstrate%2520that%2520UniVAD%2520achieves%250Astate-of-the-art%2520performance%2520in%2520few-shot%2520anomaly%2520detection%2520tasks%2520across%250Amultiple%2520domains%252C%2520outperforming%2520domain-specific%2520anomaly%2520detection%2520models.%2520The%250Acode%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniVAD%3A%20A%20Training-free%20Unified%20Model%20for%20Few-shot%20Visual%20Anomaly%0A%20%20Detection&entry.906535625=Zhaopeng%20Gu%20and%20Bingke%20Zhu%20and%20Guibo%20Zhu%20and%20Yingying%20Chen%20and%20Ming%20Tang%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Visual%20Anomaly%20Detection%20%28VAD%29%20aims%20to%20identify%20abnormal%20samples%20in%20images%0Athat%20deviate%20from%20normal%20patterns%2C%20covering%20multiple%20domains%2C%20including%0Aindustrial%2C%20logical%2C%20and%20medical%20fields.%20Due%20to%20the%20domain%20gaps%20between%20these%0Afields%2C%20existing%20VAD%20methods%20are%20typically%20tailored%20to%20each%20domain%2C%20with%0Aspecialized%20detection%20techniques%20and%20model%20architectures%20that%20are%20difficult%20to%0Ageneralize%20across%20different%20domains.%20Moreover%2C%20even%20within%20the%20same%20domain%2C%0Acurrent%20VAD%20approaches%20often%20follow%20a%20%22one-category-one-model%22%20paradigm%2C%0Arequiring%20large%20amounts%20of%20normal%20samples%20to%20train%20class-specific%20models%2C%0Aresulting%20in%20poor%20generalizability%20and%20hindering%20unified%20evaluation%20across%0Adomains.%20To%20address%20this%20issue%2C%20we%20propose%20a%20generalized%20few-shot%20VAD%20method%2C%0AUniVAD%2C%20capable%20of%20detecting%20anomalies%20across%20various%20domains%2C%20such%20as%0Aindustrial%2C%20logical%2C%20and%20medical%20anomalies%2C%20with%20a%20training-free%20unified%20model.%0AUniVAD%20only%20needs%20few%20normal%20samples%20as%20references%20during%20testing%20to%20detect%0Aanomalies%20in%20previously%20unseen%20objects%2C%20without%20training%20on%20the%20specific%0Adomain.%20Specifically%2C%20UniVAD%20employs%20a%20Contextual%20Component%20Clustering%20%28%24C%5E3%24%29%0Amodule%20based%20on%20clustering%20and%20vision%20foundation%20models%20to%20segment%20components%0Awithin%20the%20image%20accurately%2C%20and%20leverages%20Component-Aware%20Patch%20Matching%0A%28CAPM%29%20and%20Graph-Enhanced%20Component%20Modeling%20%28GECM%29%20modules%20to%20detect%20anomalies%0Aat%20different%20semantic%20levels%2C%20which%20are%20aggregated%20to%20produce%20the%20final%0Adetection%20result.%20We%20conduct%20experiments%20on%20nine%20datasets%20spanning%20industrial%2C%0Alogical%2C%20and%20medical%20fields%2C%20and%20the%20results%20demonstrate%20that%20UniVAD%20achieves%0Astate-of-the-art%20performance%20in%20few-shot%20anomaly%20detection%20tasks%20across%0Amultiple%20domains%2C%20outperforming%20domain-specific%20anomaly%20detection%20models.%20The%0Acode%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03342v1&entry.124074799=Read"},
{"title": "Biologically-inspired Semi-supervised Semantic Segmentation for\n  Biomedical Imaging", "author": "Luca Ciampi and Gabriele Lagani and Giuseppe Amato and Fabrizio Falchi", "abstract": "  We propose a novel two-stage semi-supervised learning approach for training\ndownsampling-upsampling semantic segmentation architectures. The first stage\ndoes not use backpropagation. Rather, it exploits the bio-inspired Hebbian\nprinciple \"fire together, wire together\" as a local learning rule for updating\nthe weights of both convolutional and transpose-convolutional layers, allowing\nunsupervised discovery of data features. In the second stage, the model is\nfine-tuned with standard backpropagation on a small subset of labeled data. We\nevaluate our methodology through experiments conducted on several widely used\nbiomedical datasets, deeming that this domain is paramount in computer vision\nand is notably impacted by data scarcity. Results show that our proposed method\noutperforms SOTA approaches across different levels of label availability.\nFurthermore, we show that using our unsupervised stage to initialize the SOTA\napproaches leads to performance improvements. The code to replicate our\nexperiments can be found at:\nhttps://github.com/ciampluca/hebbian-medical-image-segmentation\n", "link": "http://arxiv.org/abs/2412.03192v1", "date": "2024-12-04", "relevancy": 2.1818, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.537}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biologically-inspired%20Semi-supervised%20Semantic%20Segmentation%20for%0A%20%20Biomedical%20Imaging&body=Title%3A%20Biologically-inspired%20Semi-supervised%20Semantic%20Segmentation%20for%0A%20%20Biomedical%20Imaging%0AAuthor%3A%20Luca%20Ciampi%20and%20Gabriele%20Lagani%20and%20Giuseppe%20Amato%20and%20Fabrizio%20Falchi%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20two-stage%20semi-supervised%20learning%20approach%20for%20training%0Adownsampling-upsampling%20semantic%20segmentation%20architectures.%20The%20first%20stage%0Adoes%20not%20use%20backpropagation.%20Rather%2C%20it%20exploits%20the%20bio-inspired%20Hebbian%0Aprinciple%20%22fire%20together%2C%20wire%20together%22%20as%20a%20local%20learning%20rule%20for%20updating%0Athe%20weights%20of%20both%20convolutional%20and%20transpose-convolutional%20layers%2C%20allowing%0Aunsupervised%20discovery%20of%20data%20features.%20In%20the%20second%20stage%2C%20the%20model%20is%0Afine-tuned%20with%20standard%20backpropagation%20on%20a%20small%20subset%20of%20labeled%20data.%20We%0Aevaluate%20our%20methodology%20through%20experiments%20conducted%20on%20several%20widely%20used%0Abiomedical%20datasets%2C%20deeming%20that%20this%20domain%20is%20paramount%20in%20computer%20vision%0Aand%20is%20notably%20impacted%20by%20data%20scarcity.%20Results%20show%20that%20our%20proposed%20method%0Aoutperforms%20SOTA%20approaches%20across%20different%20levels%20of%20label%20availability.%0AFurthermore%2C%20we%20show%20that%20using%20our%20unsupervised%20stage%20to%20initialize%20the%20SOTA%0Aapproaches%20leads%20to%20performance%20improvements.%20The%20code%20to%20replicate%20our%0Aexperiments%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/ciampluca/hebbian-medical-image-segmentation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiologically-inspired%2520Semi-supervised%2520Semantic%2520Segmentation%2520for%250A%2520%2520Biomedical%2520Imaging%26entry.906535625%3DLuca%2520Ciampi%2520and%2520Gabriele%2520Lagani%2520and%2520Giuseppe%2520Amato%2520and%2520Fabrizio%2520Falchi%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520two-stage%2520semi-supervised%2520learning%2520approach%2520for%2520training%250Adownsampling-upsampling%2520semantic%2520segmentation%2520architectures.%2520The%2520first%2520stage%250Adoes%2520not%2520use%2520backpropagation.%2520Rather%252C%2520it%2520exploits%2520the%2520bio-inspired%2520Hebbian%250Aprinciple%2520%2522fire%2520together%252C%2520wire%2520together%2522%2520as%2520a%2520local%2520learning%2520rule%2520for%2520updating%250Athe%2520weights%2520of%2520both%2520convolutional%2520and%2520transpose-convolutional%2520layers%252C%2520allowing%250Aunsupervised%2520discovery%2520of%2520data%2520features.%2520In%2520the%2520second%2520stage%252C%2520the%2520model%2520is%250Afine-tuned%2520with%2520standard%2520backpropagation%2520on%2520a%2520small%2520subset%2520of%2520labeled%2520data.%2520We%250Aevaluate%2520our%2520methodology%2520through%2520experiments%2520conducted%2520on%2520several%2520widely%2520used%250Abiomedical%2520datasets%252C%2520deeming%2520that%2520this%2520domain%2520is%2520paramount%2520in%2520computer%2520vision%250Aand%2520is%2520notably%2520impacted%2520by%2520data%2520scarcity.%2520Results%2520show%2520that%2520our%2520proposed%2520method%250Aoutperforms%2520SOTA%2520approaches%2520across%2520different%2520levels%2520of%2520label%2520availability.%250AFurthermore%252C%2520we%2520show%2520that%2520using%2520our%2520unsupervised%2520stage%2520to%2520initialize%2520the%2520SOTA%250Aapproaches%2520leads%2520to%2520performance%2520improvements.%2520The%2520code%2520to%2520replicate%2520our%250Aexperiments%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//github.com/ciampluca/hebbian-medical-image-segmentation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biologically-inspired%20Semi-supervised%20Semantic%20Segmentation%20for%0A%20%20Biomedical%20Imaging&entry.906535625=Luca%20Ciampi%20and%20Gabriele%20Lagani%20and%20Giuseppe%20Amato%20and%20Fabrizio%20Falchi&entry.1292438233=%20%20We%20propose%20a%20novel%20two-stage%20semi-supervised%20learning%20approach%20for%20training%0Adownsampling-upsampling%20semantic%20segmentation%20architectures.%20The%20first%20stage%0Adoes%20not%20use%20backpropagation.%20Rather%2C%20it%20exploits%20the%20bio-inspired%20Hebbian%0Aprinciple%20%22fire%20together%2C%20wire%20together%22%20as%20a%20local%20learning%20rule%20for%20updating%0Athe%20weights%20of%20both%20convolutional%20and%20transpose-convolutional%20layers%2C%20allowing%0Aunsupervised%20discovery%20of%20data%20features.%20In%20the%20second%20stage%2C%20the%20model%20is%0Afine-tuned%20with%20standard%20backpropagation%20on%20a%20small%20subset%20of%20labeled%20data.%20We%0Aevaluate%20our%20methodology%20through%20experiments%20conducted%20on%20several%20widely%20used%0Abiomedical%20datasets%2C%20deeming%20that%20this%20domain%20is%20paramount%20in%20computer%20vision%0Aand%20is%20notably%20impacted%20by%20data%20scarcity.%20Results%20show%20that%20our%20proposed%20method%0Aoutperforms%20SOTA%20approaches%20across%20different%20levels%20of%20label%20availability.%0AFurthermore%2C%20we%20show%20that%20using%20our%20unsupervised%20stage%20to%20initialize%20the%20SOTA%0Aapproaches%20leads%20to%20performance%20improvements.%20The%20code%20to%20replicate%20our%0Aexperiments%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/ciampluca/hebbian-medical-image-segmentation%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03192v1&entry.124074799=Read"},
{"title": "Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for\n  Training-Free Acceleration", "author": "Yuhang Han and Xuyang Liu and Pengxiang Ding and Donglin Wang and Honggang Chen and Qingsen Yan and Siteng Huang", "abstract": "  To accelerate the inference of heavy Multimodal Large Language Models\n(MLLMs), this study rethinks the current landscape of training-free token\nreduction research. We regret to find that the critical components of existing\nmethods are tightly intertwined, with their interconnections and effects\nremaining unclear for comparison, transfer, and expansion. Therefore, we\npropose a unified ''filter-correlate-compress'' paradigm that decomposes the\ntoken reduction into three distinct stages within a pipeline, maintaining\nconsistent design objectives and elements while allowing for unique\nimplementations. We additionally demystify the popular works and subsume them\ninto our paradigm to showcase its universality. Finally, we offer a suite of\nmethods grounded in the paradigm, striking a balance between speed and accuracy\nthroughout different phases of the inference. Experimental results across 10\nbenchmarks indicate that our methods can achieve up to an 82.4% reduction in\nFLOPs with a minimal impact on performance, simultaneously surpassing\nstate-of-the-art training-free methods. Our project page is at\nhttps://ficoco-accelerate.github.io/.\n", "link": "http://arxiv.org/abs/2411.17686v2", "date": "2024-12-04", "relevancy": 2.1786, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5467}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Token%20Reduction%20in%20MLLMs%3A%20Towards%20a%20Unified%20Paradigm%20for%0A%20%20Training-Free%20Acceleration&body=Title%3A%20Rethinking%20Token%20Reduction%20in%20MLLMs%3A%20Towards%20a%20Unified%20Paradigm%20for%0A%20%20Training-Free%20Acceleration%0AAuthor%3A%20Yuhang%20Han%20and%20Xuyang%20Liu%20and%20Pengxiang%20Ding%20and%20Donglin%20Wang%20and%20Honggang%20Chen%20and%20Qingsen%20Yan%20and%20Siteng%20Huang%0AAbstract%3A%20%20%20To%20accelerate%20the%20inference%20of%20heavy%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%2C%20this%20study%20rethinks%20the%20current%20landscape%20of%20training-free%20token%0Areduction%20research.%20We%20regret%20to%20find%20that%20the%20critical%20components%20of%20existing%0Amethods%20are%20tightly%20intertwined%2C%20with%20their%20interconnections%20and%20effects%0Aremaining%20unclear%20for%20comparison%2C%20transfer%2C%20and%20expansion.%20Therefore%2C%20we%0Apropose%20a%20unified%20%27%27filter-correlate-compress%27%27%20paradigm%20that%20decomposes%20the%0Atoken%20reduction%20into%20three%20distinct%20stages%20within%20a%20pipeline%2C%20maintaining%0Aconsistent%20design%20objectives%20and%20elements%20while%20allowing%20for%20unique%0Aimplementations.%20We%20additionally%20demystify%20the%20popular%20works%20and%20subsume%20them%0Ainto%20our%20paradigm%20to%20showcase%20its%20universality.%20Finally%2C%20we%20offer%20a%20suite%20of%0Amethods%20grounded%20in%20the%20paradigm%2C%20striking%20a%20balance%20between%20speed%20and%20accuracy%0Athroughout%20different%20phases%20of%20the%20inference.%20Experimental%20results%20across%2010%0Abenchmarks%20indicate%20that%20our%20methods%20can%20achieve%20up%20to%20an%2082.4%25%20reduction%20in%0AFLOPs%20with%20a%20minimal%20impact%20on%20performance%2C%20simultaneously%20surpassing%0Astate-of-the-art%20training-free%20methods.%20Our%20project%20page%20is%20at%0Ahttps%3A//ficoco-accelerate.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17686v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Token%2520Reduction%2520in%2520MLLMs%253A%2520Towards%2520a%2520Unified%2520Paradigm%2520for%250A%2520%2520Training-Free%2520Acceleration%26entry.906535625%3DYuhang%2520Han%2520and%2520Xuyang%2520Liu%2520and%2520Pengxiang%2520Ding%2520and%2520Donglin%2520Wang%2520and%2520Honggang%2520Chen%2520and%2520Qingsen%2520Yan%2520and%2520Siteng%2520Huang%26entry.1292438233%3D%2520%2520To%2520accelerate%2520the%2520inference%2520of%2520heavy%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%252C%2520this%2520study%2520rethinks%2520the%2520current%2520landscape%2520of%2520training-free%2520token%250Areduction%2520research.%2520We%2520regret%2520to%2520find%2520that%2520the%2520critical%2520components%2520of%2520existing%250Amethods%2520are%2520tightly%2520intertwined%252C%2520with%2520their%2520interconnections%2520and%2520effects%250Aremaining%2520unclear%2520for%2520comparison%252C%2520transfer%252C%2520and%2520expansion.%2520Therefore%252C%2520we%250Apropose%2520a%2520unified%2520%2527%2527filter-correlate-compress%2527%2527%2520paradigm%2520that%2520decomposes%2520the%250Atoken%2520reduction%2520into%2520three%2520distinct%2520stages%2520within%2520a%2520pipeline%252C%2520maintaining%250Aconsistent%2520design%2520objectives%2520and%2520elements%2520while%2520allowing%2520for%2520unique%250Aimplementations.%2520We%2520additionally%2520demystify%2520the%2520popular%2520works%2520and%2520subsume%2520them%250Ainto%2520our%2520paradigm%2520to%2520showcase%2520its%2520universality.%2520Finally%252C%2520we%2520offer%2520a%2520suite%2520of%250Amethods%2520grounded%2520in%2520the%2520paradigm%252C%2520striking%2520a%2520balance%2520between%2520speed%2520and%2520accuracy%250Athroughout%2520different%2520phases%2520of%2520the%2520inference.%2520Experimental%2520results%2520across%252010%250Abenchmarks%2520indicate%2520that%2520our%2520methods%2520can%2520achieve%2520up%2520to%2520an%252082.4%2525%2520reduction%2520in%250AFLOPs%2520with%2520a%2520minimal%2520impact%2520on%2520performance%252C%2520simultaneously%2520surpassing%250Astate-of-the-art%2520training-free%2520methods.%2520Our%2520project%2520page%2520is%2520at%250Ahttps%253A//ficoco-accelerate.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17686v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Token%20Reduction%20in%20MLLMs%3A%20Towards%20a%20Unified%20Paradigm%20for%0A%20%20Training-Free%20Acceleration&entry.906535625=Yuhang%20Han%20and%20Xuyang%20Liu%20and%20Pengxiang%20Ding%20and%20Donglin%20Wang%20and%20Honggang%20Chen%20and%20Qingsen%20Yan%20and%20Siteng%20Huang&entry.1292438233=%20%20To%20accelerate%20the%20inference%20of%20heavy%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%2C%20this%20study%20rethinks%20the%20current%20landscape%20of%20training-free%20token%0Areduction%20research.%20We%20regret%20to%20find%20that%20the%20critical%20components%20of%20existing%0Amethods%20are%20tightly%20intertwined%2C%20with%20their%20interconnections%20and%20effects%0Aremaining%20unclear%20for%20comparison%2C%20transfer%2C%20and%20expansion.%20Therefore%2C%20we%0Apropose%20a%20unified%20%27%27filter-correlate-compress%27%27%20paradigm%20that%20decomposes%20the%0Atoken%20reduction%20into%20three%20distinct%20stages%20within%20a%20pipeline%2C%20maintaining%0Aconsistent%20design%20objectives%20and%20elements%20while%20allowing%20for%20unique%0Aimplementations.%20We%20additionally%20demystify%20the%20popular%20works%20and%20subsume%20them%0Ainto%20our%20paradigm%20to%20showcase%20its%20universality.%20Finally%2C%20we%20offer%20a%20suite%20of%0Amethods%20grounded%20in%20the%20paradigm%2C%20striking%20a%20balance%20between%20speed%20and%20accuracy%0Athroughout%20different%20phases%20of%20the%20inference.%20Experimental%20results%20across%2010%0Abenchmarks%20indicate%20that%20our%20methods%20can%20achieve%20up%20to%20an%2082.4%25%20reduction%20in%0AFLOPs%20with%20a%20minimal%20impact%20on%20performance%2C%20simultaneously%20surpassing%0Astate-of-the-art%20training-free%20methods.%20Our%20project%20page%20is%20at%0Ahttps%3A//ficoco-accelerate.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17686v2&entry.124074799=Read"},
{"title": "A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for\n  accelerating Large VLMs", "author": "Wangbo Zhao and Yizeng Han and Jiasheng Tang and Zhikai Li and Yibing Song and Kai Wang and Zhangyang Wang and Yang You", "abstract": "  Vision-language models (VLMs) have shown remarkable success across various\nmulti-modal tasks, yet large VLMs encounter significant efficiency challenges\ndue to processing numerous visual tokens. A promising approach to accelerating\nlarge VLM inference is using partial information, such as attention maps from\nspecific layers, to assess token importance and prune less essential tokens.\nHowever, our study reveals three key insights: (i) Partial attention\ninformation is insufficient for accurately identifying critical visual tokens,\nresulting in suboptimal performance, especially at low token retention ratios;\n(ii) Global attention information, such as the attention map aggregated across\nall layers, more effectively preserves essential tokens and maintains\ncomparable performance under aggressive pruning. However, the attention maps\nfrom all layers requires a full inference pass, which increases computational\nload and is therefore impractical in existing methods; and (iii) The global\nattention map aggregated from a small VLM closely resembles that of a large\nVLM, suggesting an efficient alternative. Based on these findings, we introduce\na \\textbf{training-free} method, \\underline{\\textbf{S}}mall VLM\n\\underline{\\textbf{G}}uidance for accelerating \\underline{\\textbf{L}}arge VLMs\n(\\textbf{SGL}). Specifically, we employ the attention map aggregated from a\nsmall VLM to guide visual token pruning in a large VLM. Additionally, an early\nexiting mechanism is developed to fully use the small VLM's predictions,\ndynamically invoking the larger VLM only when necessary, yielding a superior\ntrade-off between accuracy and computation. Extensive evaluations across 11\nbenchmarks demonstrate the effectiveness and generalizability of SGL, achieving\nup to 91\\% pruning ratio for visual tokens while retaining competitive\nperformance.\n", "link": "http://arxiv.org/abs/2412.03324v1", "date": "2024-12-04", "relevancy": 2.1776, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5553}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5367}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Stitch%20in%20Time%20Saves%20Nine%3A%20Small%20VLM%20is%20a%20Precise%20Guidance%20for%0A%20%20accelerating%20Large%20VLMs&body=Title%3A%20A%20Stitch%20in%20Time%20Saves%20Nine%3A%20Small%20VLM%20is%20a%20Precise%20Guidance%20for%0A%20%20accelerating%20Large%20VLMs%0AAuthor%3A%20Wangbo%20Zhao%20and%20Yizeng%20Han%20and%20Jiasheng%20Tang%20and%20Zhikai%20Li%20and%20Yibing%20Song%20and%20Kai%20Wang%20and%20Zhangyang%20Wang%20and%20Yang%20You%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20shown%20remarkable%20success%20across%20various%0Amulti-modal%20tasks%2C%20yet%20large%20VLMs%20encounter%20significant%20efficiency%20challenges%0Adue%20to%20processing%20numerous%20visual%20tokens.%20A%20promising%20approach%20to%20accelerating%0Alarge%20VLM%20inference%20is%20using%20partial%20information%2C%20such%20as%20attention%20maps%20from%0Aspecific%20layers%2C%20to%20assess%20token%20importance%20and%20prune%20less%20essential%20tokens.%0AHowever%2C%20our%20study%20reveals%20three%20key%20insights%3A%20%28i%29%20Partial%20attention%0Ainformation%20is%20insufficient%20for%20accurately%20identifying%20critical%20visual%20tokens%2C%0Aresulting%20in%20suboptimal%20performance%2C%20especially%20at%20low%20token%20retention%20ratios%3B%0A%28ii%29%20Global%20attention%20information%2C%20such%20as%20the%20attention%20map%20aggregated%20across%0Aall%20layers%2C%20more%20effectively%20preserves%20essential%20tokens%20and%20maintains%0Acomparable%20performance%20under%20aggressive%20pruning.%20However%2C%20the%20attention%20maps%0Afrom%20all%20layers%20requires%20a%20full%20inference%20pass%2C%20which%20increases%20computational%0Aload%20and%20is%20therefore%20impractical%20in%20existing%20methods%3B%20and%20%28iii%29%20The%20global%0Aattention%20map%20aggregated%20from%20a%20small%20VLM%20closely%20resembles%20that%20of%20a%20large%0AVLM%2C%20suggesting%20an%20efficient%20alternative.%20Based%20on%20these%20findings%2C%20we%20introduce%0Aa%20%5Ctextbf%7Btraining-free%7D%20method%2C%20%5Cunderline%7B%5Ctextbf%7BS%7D%7Dmall%20VLM%0A%5Cunderline%7B%5Ctextbf%7BG%7D%7Duidance%20for%20accelerating%20%5Cunderline%7B%5Ctextbf%7BL%7D%7Darge%20VLMs%0A%28%5Ctextbf%7BSGL%7D%29.%20Specifically%2C%20we%20employ%20the%20attention%20map%20aggregated%20from%20a%0Asmall%20VLM%20to%20guide%20visual%20token%20pruning%20in%20a%20large%20VLM.%20Additionally%2C%20an%20early%0Aexiting%20mechanism%20is%20developed%20to%20fully%20use%20the%20small%20VLM%27s%20predictions%2C%0Adynamically%20invoking%20the%20larger%20VLM%20only%20when%20necessary%2C%20yielding%20a%20superior%0Atrade-off%20between%20accuracy%20and%20computation.%20Extensive%20evaluations%20across%2011%0Abenchmarks%20demonstrate%20the%20effectiveness%20and%20generalizability%20of%20SGL%2C%20achieving%0Aup%20to%2091%5C%25%20pruning%20ratio%20for%20visual%20tokens%20while%20retaining%20competitive%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Stitch%2520in%2520Time%2520Saves%2520Nine%253A%2520Small%2520VLM%2520is%2520a%2520Precise%2520Guidance%2520for%250A%2520%2520accelerating%2520Large%2520VLMs%26entry.906535625%3DWangbo%2520Zhao%2520and%2520Yizeng%2520Han%2520and%2520Jiasheng%2520Tang%2520and%2520Zhikai%2520Li%2520and%2520Yibing%2520Song%2520and%2520Kai%2520Wang%2520and%2520Zhangyang%2520Wang%2520and%2520Yang%2520You%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520remarkable%2520success%2520across%2520various%250Amulti-modal%2520tasks%252C%2520yet%2520large%2520VLMs%2520encounter%2520significant%2520efficiency%2520challenges%250Adue%2520to%2520processing%2520numerous%2520visual%2520tokens.%2520A%2520promising%2520approach%2520to%2520accelerating%250Alarge%2520VLM%2520inference%2520is%2520using%2520partial%2520information%252C%2520such%2520as%2520attention%2520maps%2520from%250Aspecific%2520layers%252C%2520to%2520assess%2520token%2520importance%2520and%2520prune%2520less%2520essential%2520tokens.%250AHowever%252C%2520our%2520study%2520reveals%2520three%2520key%2520insights%253A%2520%2528i%2529%2520Partial%2520attention%250Ainformation%2520is%2520insufficient%2520for%2520accurately%2520identifying%2520critical%2520visual%2520tokens%252C%250Aresulting%2520in%2520suboptimal%2520performance%252C%2520especially%2520at%2520low%2520token%2520retention%2520ratios%253B%250A%2528ii%2529%2520Global%2520attention%2520information%252C%2520such%2520as%2520the%2520attention%2520map%2520aggregated%2520across%250Aall%2520layers%252C%2520more%2520effectively%2520preserves%2520essential%2520tokens%2520and%2520maintains%250Acomparable%2520performance%2520under%2520aggressive%2520pruning.%2520However%252C%2520the%2520attention%2520maps%250Afrom%2520all%2520layers%2520requires%2520a%2520full%2520inference%2520pass%252C%2520which%2520increases%2520computational%250Aload%2520and%2520is%2520therefore%2520impractical%2520in%2520existing%2520methods%253B%2520and%2520%2528iii%2529%2520The%2520global%250Aattention%2520map%2520aggregated%2520from%2520a%2520small%2520VLM%2520closely%2520resembles%2520that%2520of%2520a%2520large%250AVLM%252C%2520suggesting%2520an%2520efficient%2520alternative.%2520Based%2520on%2520these%2520findings%252C%2520we%2520introduce%250Aa%2520%255Ctextbf%257Btraining-free%257D%2520method%252C%2520%255Cunderline%257B%255Ctextbf%257BS%257D%257Dmall%2520VLM%250A%255Cunderline%257B%255Ctextbf%257BG%257D%257Duidance%2520for%2520accelerating%2520%255Cunderline%257B%255Ctextbf%257BL%257D%257Darge%2520VLMs%250A%2528%255Ctextbf%257BSGL%257D%2529.%2520Specifically%252C%2520we%2520employ%2520the%2520attention%2520map%2520aggregated%2520from%2520a%250Asmall%2520VLM%2520to%2520guide%2520visual%2520token%2520pruning%2520in%2520a%2520large%2520VLM.%2520Additionally%252C%2520an%2520early%250Aexiting%2520mechanism%2520is%2520developed%2520to%2520fully%2520use%2520the%2520small%2520VLM%2527s%2520predictions%252C%250Adynamically%2520invoking%2520the%2520larger%2520VLM%2520only%2520when%2520necessary%252C%2520yielding%2520a%2520superior%250Atrade-off%2520between%2520accuracy%2520and%2520computation.%2520Extensive%2520evaluations%2520across%252011%250Abenchmarks%2520demonstrate%2520the%2520effectiveness%2520and%2520generalizability%2520of%2520SGL%252C%2520achieving%250Aup%2520to%252091%255C%2525%2520pruning%2520ratio%2520for%2520visual%2520tokens%2520while%2520retaining%2520competitive%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Stitch%20in%20Time%20Saves%20Nine%3A%20Small%20VLM%20is%20a%20Precise%20Guidance%20for%0A%20%20accelerating%20Large%20VLMs&entry.906535625=Wangbo%20Zhao%20and%20Yizeng%20Han%20and%20Jiasheng%20Tang%20and%20Zhikai%20Li%20and%20Yibing%20Song%20and%20Kai%20Wang%20and%20Zhangyang%20Wang%20and%20Yang%20You&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20shown%20remarkable%20success%20across%20various%0Amulti-modal%20tasks%2C%20yet%20large%20VLMs%20encounter%20significant%20efficiency%20challenges%0Adue%20to%20processing%20numerous%20visual%20tokens.%20A%20promising%20approach%20to%20accelerating%0Alarge%20VLM%20inference%20is%20using%20partial%20information%2C%20such%20as%20attention%20maps%20from%0Aspecific%20layers%2C%20to%20assess%20token%20importance%20and%20prune%20less%20essential%20tokens.%0AHowever%2C%20our%20study%20reveals%20three%20key%20insights%3A%20%28i%29%20Partial%20attention%0Ainformation%20is%20insufficient%20for%20accurately%20identifying%20critical%20visual%20tokens%2C%0Aresulting%20in%20suboptimal%20performance%2C%20especially%20at%20low%20token%20retention%20ratios%3B%0A%28ii%29%20Global%20attention%20information%2C%20such%20as%20the%20attention%20map%20aggregated%20across%0Aall%20layers%2C%20more%20effectively%20preserves%20essential%20tokens%20and%20maintains%0Acomparable%20performance%20under%20aggressive%20pruning.%20However%2C%20the%20attention%20maps%0Afrom%20all%20layers%20requires%20a%20full%20inference%20pass%2C%20which%20increases%20computational%0Aload%20and%20is%20therefore%20impractical%20in%20existing%20methods%3B%20and%20%28iii%29%20The%20global%0Aattention%20map%20aggregated%20from%20a%20small%20VLM%20closely%20resembles%20that%20of%20a%20large%0AVLM%2C%20suggesting%20an%20efficient%20alternative.%20Based%20on%20these%20findings%2C%20we%20introduce%0Aa%20%5Ctextbf%7Btraining-free%7D%20method%2C%20%5Cunderline%7B%5Ctextbf%7BS%7D%7Dmall%20VLM%0A%5Cunderline%7B%5Ctextbf%7BG%7D%7Duidance%20for%20accelerating%20%5Cunderline%7B%5Ctextbf%7BL%7D%7Darge%20VLMs%0A%28%5Ctextbf%7BSGL%7D%29.%20Specifically%2C%20we%20employ%20the%20attention%20map%20aggregated%20from%20a%0Asmall%20VLM%20to%20guide%20visual%20token%20pruning%20in%20a%20large%20VLM.%20Additionally%2C%20an%20early%0Aexiting%20mechanism%20is%20developed%20to%20fully%20use%20the%20small%20VLM%27s%20predictions%2C%0Adynamically%20invoking%20the%20larger%20VLM%20only%20when%20necessary%2C%20yielding%20a%20superior%0Atrade-off%20between%20accuracy%20and%20computation.%20Extensive%20evaluations%20across%2011%0Abenchmarks%20demonstrate%20the%20effectiveness%20and%20generalizability%20of%20SGL%2C%20achieving%0Aup%20to%2091%5C%25%20pruning%20ratio%20for%20visual%20tokens%20while%20retaining%20competitive%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03324v1&entry.124074799=Read"},
{"title": "Continual Low-Rank Scaled Dot-product Attention", "author": "Gin\u00e9s Carreto Pic\u00f3n and Illia Oleksiienko and Lukas Hedegaard and Arian Bakhtiarnia and Alexandros Iosifidis", "abstract": "  Transformers are widely used for their ability to capture data relations in\nsequence processing, with great success for a wide range of static tasks.\nHowever, the computational and memory footprint of their main component, i.e.,\nthe Scaled Dot-product Attention, is commonly overlooked. This makes their\nadoption in applications involving stream data processing with constraints in\nresponse latency, computational and memory resources infeasible. Some works\nhave proposed methods to lower the computational cost of transformers, i.e.\nlow-rank approximations, sparsity in attention, and efficient formulations for\nContinual Inference. In this paper, we introduce a new formulation of the\nScaled Dot-product Attention based on the Nystr\\\"om approximation that is\nsuitable for Continual Inference. In experiments on Online Audio Classification\nand Online Action Detection tasks, the proposed Continual Scaled Dot-product\nAttention can lower the number of operations by up to three orders of magnitude\ncompared to the original Transformers while retaining the predictive\nperformance of competing models.\n", "link": "http://arxiv.org/abs/2412.03214v1", "date": "2024-12-04", "relevancy": 2.1323, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5883}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5645}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Low-Rank%20Scaled%20Dot-product%20Attention&body=Title%3A%20Continual%20Low-Rank%20Scaled%20Dot-product%20Attention%0AAuthor%3A%20Gin%C3%A9s%20Carreto%20Pic%C3%B3n%20and%20Illia%20Oleksiienko%20and%20Lukas%20Hedegaard%20and%20Arian%20Bakhtiarnia%20and%20Alexandros%20Iosifidis%0AAbstract%3A%20%20%20Transformers%20are%20widely%20used%20for%20their%20ability%20to%20capture%20data%20relations%20in%0Asequence%20processing%2C%20with%20great%20success%20for%20a%20wide%20range%20of%20static%20tasks.%0AHowever%2C%20the%20computational%20and%20memory%20footprint%20of%20their%20main%20component%2C%20i.e.%2C%0Athe%20Scaled%20Dot-product%20Attention%2C%20is%20commonly%20overlooked.%20This%20makes%20their%0Aadoption%20in%20applications%20involving%20stream%20data%20processing%20with%20constraints%20in%0Aresponse%20latency%2C%20computational%20and%20memory%20resources%20infeasible.%20Some%20works%0Ahave%20proposed%20methods%20to%20lower%20the%20computational%20cost%20of%20transformers%2C%20i.e.%0Alow-rank%20approximations%2C%20sparsity%20in%20attention%2C%20and%20efficient%20formulations%20for%0AContinual%20Inference.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20formulation%20of%20the%0AScaled%20Dot-product%20Attention%20based%20on%20the%20Nystr%5C%22om%20approximation%20that%20is%0Asuitable%20for%20Continual%20Inference.%20In%20experiments%20on%20Online%20Audio%20Classification%0Aand%20Online%20Action%20Detection%20tasks%2C%20the%20proposed%20Continual%20Scaled%20Dot-product%0AAttention%20can%20lower%20the%20number%20of%20operations%20by%20up%20to%20three%20orders%20of%20magnitude%0Acompared%20to%20the%20original%20Transformers%20while%20retaining%20the%20predictive%0Aperformance%20of%20competing%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Low-Rank%2520Scaled%2520Dot-product%2520Attention%26entry.906535625%3DGin%25C3%25A9s%2520Carreto%2520Pic%25C3%25B3n%2520and%2520Illia%2520Oleksiienko%2520and%2520Lukas%2520Hedegaard%2520and%2520Arian%2520Bakhtiarnia%2520and%2520Alexandros%2520Iosifidis%26entry.1292438233%3D%2520%2520Transformers%2520are%2520widely%2520used%2520for%2520their%2520ability%2520to%2520capture%2520data%2520relations%2520in%250Asequence%2520processing%252C%2520with%2520great%2520success%2520for%2520a%2520wide%2520range%2520of%2520static%2520tasks.%250AHowever%252C%2520the%2520computational%2520and%2520memory%2520footprint%2520of%2520their%2520main%2520component%252C%2520i.e.%252C%250Athe%2520Scaled%2520Dot-product%2520Attention%252C%2520is%2520commonly%2520overlooked.%2520This%2520makes%2520their%250Aadoption%2520in%2520applications%2520involving%2520stream%2520data%2520processing%2520with%2520constraints%2520in%250Aresponse%2520latency%252C%2520computational%2520and%2520memory%2520resources%2520infeasible.%2520Some%2520works%250Ahave%2520proposed%2520methods%2520to%2520lower%2520the%2520computational%2520cost%2520of%2520transformers%252C%2520i.e.%250Alow-rank%2520approximations%252C%2520sparsity%2520in%2520attention%252C%2520and%2520efficient%2520formulations%2520for%250AContinual%2520Inference.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520formulation%2520of%2520the%250AScaled%2520Dot-product%2520Attention%2520based%2520on%2520the%2520Nystr%255C%2522om%2520approximation%2520that%2520is%250Asuitable%2520for%2520Continual%2520Inference.%2520In%2520experiments%2520on%2520Online%2520Audio%2520Classification%250Aand%2520Online%2520Action%2520Detection%2520tasks%252C%2520the%2520proposed%2520Continual%2520Scaled%2520Dot-product%250AAttention%2520can%2520lower%2520the%2520number%2520of%2520operations%2520by%2520up%2520to%2520three%2520orders%2520of%2520magnitude%250Acompared%2520to%2520the%2520original%2520Transformers%2520while%2520retaining%2520the%2520predictive%250Aperformance%2520of%2520competing%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Low-Rank%20Scaled%20Dot-product%20Attention&entry.906535625=Gin%C3%A9s%20Carreto%20Pic%C3%B3n%20and%20Illia%20Oleksiienko%20and%20Lukas%20Hedegaard%20and%20Arian%20Bakhtiarnia%20and%20Alexandros%20Iosifidis&entry.1292438233=%20%20Transformers%20are%20widely%20used%20for%20their%20ability%20to%20capture%20data%20relations%20in%0Asequence%20processing%2C%20with%20great%20success%20for%20a%20wide%20range%20of%20static%20tasks.%0AHowever%2C%20the%20computational%20and%20memory%20footprint%20of%20their%20main%20component%2C%20i.e.%2C%0Athe%20Scaled%20Dot-product%20Attention%2C%20is%20commonly%20overlooked.%20This%20makes%20their%0Aadoption%20in%20applications%20involving%20stream%20data%20processing%20with%20constraints%20in%0Aresponse%20latency%2C%20computational%20and%20memory%20resources%20infeasible.%20Some%20works%0Ahave%20proposed%20methods%20to%20lower%20the%20computational%20cost%20of%20transformers%2C%20i.e.%0Alow-rank%20approximations%2C%20sparsity%20in%20attention%2C%20and%20efficient%20formulations%20for%0AContinual%20Inference.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20formulation%20of%20the%0AScaled%20Dot-product%20Attention%20based%20on%20the%20Nystr%5C%22om%20approximation%20that%20is%0Asuitable%20for%20Continual%20Inference.%20In%20experiments%20on%20Online%20Audio%20Classification%0Aand%20Online%20Action%20Detection%20tasks%2C%20the%20proposed%20Continual%20Scaled%20Dot-product%0AAttention%20can%20lower%20the%20number%20of%20operations%20by%20up%20to%20three%20orders%20of%20magnitude%0Acompared%20to%20the%20original%20Transformers%20while%20retaining%20the%20predictive%0Aperformance%20of%20competing%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03214v1&entry.124074799=Read"},
{"title": "EgoPressure: A Dataset for Hand Pressure and Pose Estimation in\n  Egocentric Vision", "author": "Yiming Zhao and Taein Kwon and Paul Streli and Marc Pollefeys and Christian Holz", "abstract": "  Touch contact and pressure are essential for understanding how humans\ninteract with and manipulate objects, insights which can significantly benefit\napplications in mixed reality and robotics. However, estimating these\ninteractions from an egocentric camera perspective is challenging, largely due\nto the lack of comprehensive datasets that provide both accurate hand poses on\ncontacting surfaces and detailed annotations of pressure information. In this\npaper, we introduce EgoPressure, a novel egocentric dataset that captures\ndetailed touch contact and pressure interactions. EgoPressure provides\nhigh-resolution pressure intensity annotations for each contact point and\nincludes accurate hand pose meshes obtained through our proposed multi-view,\nsequence-based optimization method processing data from an 8-camera capture\nrig. Our dataset comprises 5 hours of recorded interactions from 21\nparticipants captured simultaneously by one head-mounted and seven stationary\nKinect cameras, which acquire RGB images and depth maps at 30 Hz. To support\nfuture research and benchmarking, we present several baseline models for\nestimating applied pressure on external surfaces from RGB images, with and\nwithout hand pose information. We further explore the joint estimation of the\nhand mesh and applied pressure. Our experiments demonstrate that pressure and\nhand pose are complementary for understanding hand-object interactions. ng of\nhand-object interactions in AR/VR and robotics research. Project page:\n\\url{https://yiming-zhao.github.io/EgoPressure/}.\n", "link": "http://arxiv.org/abs/2409.02224v2", "date": "2024-12-04", "relevancy": 2.1277, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5415}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoPressure%3A%20A%20Dataset%20for%20Hand%20Pressure%20and%20Pose%20Estimation%20in%0A%20%20Egocentric%20Vision&body=Title%3A%20EgoPressure%3A%20A%20Dataset%20for%20Hand%20Pressure%20and%20Pose%20Estimation%20in%0A%20%20Egocentric%20Vision%0AAuthor%3A%20Yiming%20Zhao%20and%20Taein%20Kwon%20and%20Paul%20Streli%20and%20Marc%20Pollefeys%20and%20Christian%20Holz%0AAbstract%3A%20%20%20Touch%20contact%20and%20pressure%20are%20essential%20for%20understanding%20how%20humans%0Ainteract%20with%20and%20manipulate%20objects%2C%20insights%20which%20can%20significantly%20benefit%0Aapplications%20in%20mixed%20reality%20and%20robotics.%20However%2C%20estimating%20these%0Ainteractions%20from%20an%20egocentric%20camera%20perspective%20is%20challenging%2C%20largely%20due%0Ato%20the%20lack%20of%20comprehensive%20datasets%20that%20provide%20both%20accurate%20hand%20poses%20on%0Acontacting%20surfaces%20and%20detailed%20annotations%20of%20pressure%20information.%20In%20this%0Apaper%2C%20we%20introduce%20EgoPressure%2C%20a%20novel%20egocentric%20dataset%20that%20captures%0Adetailed%20touch%20contact%20and%20pressure%20interactions.%20EgoPressure%20provides%0Ahigh-resolution%20pressure%20intensity%20annotations%20for%20each%20contact%20point%20and%0Aincludes%20accurate%20hand%20pose%20meshes%20obtained%20through%20our%20proposed%20multi-view%2C%0Asequence-based%20optimization%20method%20processing%20data%20from%20an%208-camera%20capture%0Arig.%20Our%20dataset%20comprises%205%20hours%20of%20recorded%20interactions%20from%2021%0Aparticipants%20captured%20simultaneously%20by%20one%20head-mounted%20and%20seven%20stationary%0AKinect%20cameras%2C%20which%20acquire%20RGB%20images%20and%20depth%20maps%20at%2030%20Hz.%20To%20support%0Afuture%20research%20and%20benchmarking%2C%20we%20present%20several%20baseline%20models%20for%0Aestimating%20applied%20pressure%20on%20external%20surfaces%20from%20RGB%20images%2C%20with%20and%0Awithout%20hand%20pose%20information.%20We%20further%20explore%20the%20joint%20estimation%20of%20the%0Ahand%20mesh%20and%20applied%20pressure.%20Our%20experiments%20demonstrate%20that%20pressure%20and%0Ahand%20pose%20are%20complementary%20for%20understanding%20hand-object%20interactions.%20ng%20of%0Ahand-object%20interactions%20in%20AR/VR%20and%20robotics%20research.%20Project%20page%3A%0A%5Curl%7Bhttps%3A//yiming-zhao.github.io/EgoPressure/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02224v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoPressure%253A%2520A%2520Dataset%2520for%2520Hand%2520Pressure%2520and%2520Pose%2520Estimation%2520in%250A%2520%2520Egocentric%2520Vision%26entry.906535625%3DYiming%2520Zhao%2520and%2520Taein%2520Kwon%2520and%2520Paul%2520Streli%2520and%2520Marc%2520Pollefeys%2520and%2520Christian%2520Holz%26entry.1292438233%3D%2520%2520Touch%2520contact%2520and%2520pressure%2520are%2520essential%2520for%2520understanding%2520how%2520humans%250Ainteract%2520with%2520and%2520manipulate%2520objects%252C%2520insights%2520which%2520can%2520significantly%2520benefit%250Aapplications%2520in%2520mixed%2520reality%2520and%2520robotics.%2520However%252C%2520estimating%2520these%250Ainteractions%2520from%2520an%2520egocentric%2520camera%2520perspective%2520is%2520challenging%252C%2520largely%2520due%250Ato%2520the%2520lack%2520of%2520comprehensive%2520datasets%2520that%2520provide%2520both%2520accurate%2520hand%2520poses%2520on%250Acontacting%2520surfaces%2520and%2520detailed%2520annotations%2520of%2520pressure%2520information.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520EgoPressure%252C%2520a%2520novel%2520egocentric%2520dataset%2520that%2520captures%250Adetailed%2520touch%2520contact%2520and%2520pressure%2520interactions.%2520EgoPressure%2520provides%250Ahigh-resolution%2520pressure%2520intensity%2520annotations%2520for%2520each%2520contact%2520point%2520and%250Aincludes%2520accurate%2520hand%2520pose%2520meshes%2520obtained%2520through%2520our%2520proposed%2520multi-view%252C%250Asequence-based%2520optimization%2520method%2520processing%2520data%2520from%2520an%25208-camera%2520capture%250Arig.%2520Our%2520dataset%2520comprises%25205%2520hours%2520of%2520recorded%2520interactions%2520from%252021%250Aparticipants%2520captured%2520simultaneously%2520by%2520one%2520head-mounted%2520and%2520seven%2520stationary%250AKinect%2520cameras%252C%2520which%2520acquire%2520RGB%2520images%2520and%2520depth%2520maps%2520at%252030%2520Hz.%2520To%2520support%250Afuture%2520research%2520and%2520benchmarking%252C%2520we%2520present%2520several%2520baseline%2520models%2520for%250Aestimating%2520applied%2520pressure%2520on%2520external%2520surfaces%2520from%2520RGB%2520images%252C%2520with%2520and%250Awithout%2520hand%2520pose%2520information.%2520We%2520further%2520explore%2520the%2520joint%2520estimation%2520of%2520the%250Ahand%2520mesh%2520and%2520applied%2520pressure.%2520Our%2520experiments%2520demonstrate%2520that%2520pressure%2520and%250Ahand%2520pose%2520are%2520complementary%2520for%2520understanding%2520hand-object%2520interactions.%2520ng%2520of%250Ahand-object%2520interactions%2520in%2520AR/VR%2520and%2520robotics%2520research.%2520Project%2520page%253A%250A%255Curl%257Bhttps%253A//yiming-zhao.github.io/EgoPressure/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02224v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoPressure%3A%20A%20Dataset%20for%20Hand%20Pressure%20and%20Pose%20Estimation%20in%0A%20%20Egocentric%20Vision&entry.906535625=Yiming%20Zhao%20and%20Taein%20Kwon%20and%20Paul%20Streli%20and%20Marc%20Pollefeys%20and%20Christian%20Holz&entry.1292438233=%20%20Touch%20contact%20and%20pressure%20are%20essential%20for%20understanding%20how%20humans%0Ainteract%20with%20and%20manipulate%20objects%2C%20insights%20which%20can%20significantly%20benefit%0Aapplications%20in%20mixed%20reality%20and%20robotics.%20However%2C%20estimating%20these%0Ainteractions%20from%20an%20egocentric%20camera%20perspective%20is%20challenging%2C%20largely%20due%0Ato%20the%20lack%20of%20comprehensive%20datasets%20that%20provide%20both%20accurate%20hand%20poses%20on%0Acontacting%20surfaces%20and%20detailed%20annotations%20of%20pressure%20information.%20In%20this%0Apaper%2C%20we%20introduce%20EgoPressure%2C%20a%20novel%20egocentric%20dataset%20that%20captures%0Adetailed%20touch%20contact%20and%20pressure%20interactions.%20EgoPressure%20provides%0Ahigh-resolution%20pressure%20intensity%20annotations%20for%20each%20contact%20point%20and%0Aincludes%20accurate%20hand%20pose%20meshes%20obtained%20through%20our%20proposed%20multi-view%2C%0Asequence-based%20optimization%20method%20processing%20data%20from%20an%208-camera%20capture%0Arig.%20Our%20dataset%20comprises%205%20hours%20of%20recorded%20interactions%20from%2021%0Aparticipants%20captured%20simultaneously%20by%20one%20head-mounted%20and%20seven%20stationary%0AKinect%20cameras%2C%20which%20acquire%20RGB%20images%20and%20depth%20maps%20at%2030%20Hz.%20To%20support%0Afuture%20research%20and%20benchmarking%2C%20we%20present%20several%20baseline%20models%20for%0Aestimating%20applied%20pressure%20on%20external%20surfaces%20from%20RGB%20images%2C%20with%20and%0Awithout%20hand%20pose%20information.%20We%20further%20explore%20the%20joint%20estimation%20of%20the%0Ahand%20mesh%20and%20applied%20pressure.%20Our%20experiments%20demonstrate%20that%20pressure%20and%0Ahand%20pose%20are%20complementary%20for%20understanding%20hand-object%20interactions.%20ng%20of%0Ahand-object%20interactions%20in%20AR/VR%20and%20robotics%20research.%20Project%20page%3A%0A%5Curl%7Bhttps%3A//yiming-zhao.github.io/EgoPressure/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02224v2&entry.124074799=Read"},
{"title": "Multi-Momentum Observer Contact Estimation for Bipedal Robots", "author": "J. Joe Payne and Daniel A. Hagen and Denis Garagi\u0107 and Aaron M. Johnson", "abstract": "  As bipedal robots become more and more popular in commercial and industrial\nsettings, the ability to control them with a high degree of reliability is\ncritical. To that end, this paper considers how to accurately estimate which\nfeet are currently in contact with the ground so as to avoid improper control\nactions that could jeopardize the stability of the robot. Additionally, modern\nalgorithms for estimating the position and orientation of a robot's base frame\nrely heavily on such contact mode estimates. Dedicated contact sensors on the\nfeet can be used to estimate this contact mode, but these sensors are prone to\nnoise, time delays, damage/yielding from repeated impacts with the ground, and\nare not available on every robot. To overcome these limitations, we propose a\nmomentum observer based method for contact mode estimation that does not rely\non such contact sensors. Often, momentum observers assume that the robot's base\nframe can be treated as an inertial frame. However, since many humanoids' legs\nrepresent a significant portion of the overall mass, the proposed method\ninstead utilizes multiple simultaneous dynamic models. Each of these models\nassumes a different contact condition. A given contact assumption is then used\nto constrain the full dynamics in order to avoid assuming that either the body\nis an inertial frame or that a fully accurate estimate of body velocity is\nknown. The (dis)agreement between each model's estimates and measurements is\nused to determine which contact mode is most likely using a Markov-style fusion\nmethod. The proposed method produces contact detection accuracy of up to 98.44%\nwith a low noise simulation and 77.12% when utilizing data collect on the\nSarcos Guardian XO robot (a hybrid humanoid/exoskeleton).\n", "link": "http://arxiv.org/abs/2412.03462v1", "date": "2024-12-04", "relevancy": 2.1242, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6507}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5356}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Momentum%20Observer%20Contact%20Estimation%20for%20Bipedal%20Robots&body=Title%3A%20Multi-Momentum%20Observer%20Contact%20Estimation%20for%20Bipedal%20Robots%0AAuthor%3A%20J.%20Joe%20Payne%20and%20Daniel%20A.%20Hagen%20and%20Denis%20Garagi%C4%87%20and%20Aaron%20M.%20Johnson%0AAbstract%3A%20%20%20As%20bipedal%20robots%20become%20more%20and%20more%20popular%20in%20commercial%20and%20industrial%0Asettings%2C%20the%20ability%20to%20control%20them%20with%20a%20high%20degree%20of%20reliability%20is%0Acritical.%20To%20that%20end%2C%20this%20paper%20considers%20how%20to%20accurately%20estimate%20which%0Afeet%20are%20currently%20in%20contact%20with%20the%20ground%20so%20as%20to%20avoid%20improper%20control%0Aactions%20that%20could%20jeopardize%20the%20stability%20of%20the%20robot.%20Additionally%2C%20modern%0Aalgorithms%20for%20estimating%20the%20position%20and%20orientation%20of%20a%20robot%27s%20base%20frame%0Arely%20heavily%20on%20such%20contact%20mode%20estimates.%20Dedicated%20contact%20sensors%20on%20the%0Afeet%20can%20be%20used%20to%20estimate%20this%20contact%20mode%2C%20but%20these%20sensors%20are%20prone%20to%0Anoise%2C%20time%20delays%2C%20damage/yielding%20from%20repeated%20impacts%20with%20the%20ground%2C%20and%0Aare%20not%20available%20on%20every%20robot.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%0Amomentum%20observer%20based%20method%20for%20contact%20mode%20estimation%20that%20does%20not%20rely%0Aon%20such%20contact%20sensors.%20Often%2C%20momentum%20observers%20assume%20that%20the%20robot%27s%20base%0Aframe%20can%20be%20treated%20as%20an%20inertial%20frame.%20However%2C%20since%20many%20humanoids%27%20legs%0Arepresent%20a%20significant%20portion%20of%20the%20overall%20mass%2C%20the%20proposed%20method%0Ainstead%20utilizes%20multiple%20simultaneous%20dynamic%20models.%20Each%20of%20these%20models%0Aassumes%20a%20different%20contact%20condition.%20A%20given%20contact%20assumption%20is%20then%20used%0Ato%20constrain%20the%20full%20dynamics%20in%20order%20to%20avoid%20assuming%20that%20either%20the%20body%0Ais%20an%20inertial%20frame%20or%20that%20a%20fully%20accurate%20estimate%20of%20body%20velocity%20is%0Aknown.%20The%20%28dis%29agreement%20between%20each%20model%27s%20estimates%20and%20measurements%20is%0Aused%20to%20determine%20which%20contact%20mode%20is%20most%20likely%20using%20a%20Markov-style%20fusion%0Amethod.%20The%20proposed%20method%20produces%20contact%20detection%20accuracy%20of%20up%20to%2098.44%25%0Awith%20a%20low%20noise%20simulation%20and%2077.12%25%20when%20utilizing%20data%20collect%20on%20the%0ASarcos%20Guardian%20XO%20robot%20%28a%20hybrid%20humanoid/exoskeleton%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Momentum%2520Observer%2520Contact%2520Estimation%2520for%2520Bipedal%2520Robots%26entry.906535625%3DJ.%2520Joe%2520Payne%2520and%2520Daniel%2520A.%2520Hagen%2520and%2520Denis%2520Garagi%25C4%2587%2520and%2520Aaron%2520M.%2520Johnson%26entry.1292438233%3D%2520%2520As%2520bipedal%2520robots%2520become%2520more%2520and%2520more%2520popular%2520in%2520commercial%2520and%2520industrial%250Asettings%252C%2520the%2520ability%2520to%2520control%2520them%2520with%2520a%2520high%2520degree%2520of%2520reliability%2520is%250Acritical.%2520To%2520that%2520end%252C%2520this%2520paper%2520considers%2520how%2520to%2520accurately%2520estimate%2520which%250Afeet%2520are%2520currently%2520in%2520contact%2520with%2520the%2520ground%2520so%2520as%2520to%2520avoid%2520improper%2520control%250Aactions%2520that%2520could%2520jeopardize%2520the%2520stability%2520of%2520the%2520robot.%2520Additionally%252C%2520modern%250Aalgorithms%2520for%2520estimating%2520the%2520position%2520and%2520orientation%2520of%2520a%2520robot%2527s%2520base%2520frame%250Arely%2520heavily%2520on%2520such%2520contact%2520mode%2520estimates.%2520Dedicated%2520contact%2520sensors%2520on%2520the%250Afeet%2520can%2520be%2520used%2520to%2520estimate%2520this%2520contact%2520mode%252C%2520but%2520these%2520sensors%2520are%2520prone%2520to%250Anoise%252C%2520time%2520delays%252C%2520damage/yielding%2520from%2520repeated%2520impacts%2520with%2520the%2520ground%252C%2520and%250Aare%2520not%2520available%2520on%2520every%2520robot.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%250Amomentum%2520observer%2520based%2520method%2520for%2520contact%2520mode%2520estimation%2520that%2520does%2520not%2520rely%250Aon%2520such%2520contact%2520sensors.%2520Often%252C%2520momentum%2520observers%2520assume%2520that%2520the%2520robot%2527s%2520base%250Aframe%2520can%2520be%2520treated%2520as%2520an%2520inertial%2520frame.%2520However%252C%2520since%2520many%2520humanoids%2527%2520legs%250Arepresent%2520a%2520significant%2520portion%2520of%2520the%2520overall%2520mass%252C%2520the%2520proposed%2520method%250Ainstead%2520utilizes%2520multiple%2520simultaneous%2520dynamic%2520models.%2520Each%2520of%2520these%2520models%250Aassumes%2520a%2520different%2520contact%2520condition.%2520A%2520given%2520contact%2520assumption%2520is%2520then%2520used%250Ato%2520constrain%2520the%2520full%2520dynamics%2520in%2520order%2520to%2520avoid%2520assuming%2520that%2520either%2520the%2520body%250Ais%2520an%2520inertial%2520frame%2520or%2520that%2520a%2520fully%2520accurate%2520estimate%2520of%2520body%2520velocity%2520is%250Aknown.%2520The%2520%2528dis%2529agreement%2520between%2520each%2520model%2527s%2520estimates%2520and%2520measurements%2520is%250Aused%2520to%2520determine%2520which%2520contact%2520mode%2520is%2520most%2520likely%2520using%2520a%2520Markov-style%2520fusion%250Amethod.%2520The%2520proposed%2520method%2520produces%2520contact%2520detection%2520accuracy%2520of%2520up%2520to%252098.44%2525%250Awith%2520a%2520low%2520noise%2520simulation%2520and%252077.12%2525%2520when%2520utilizing%2520data%2520collect%2520on%2520the%250ASarcos%2520Guardian%2520XO%2520robot%2520%2528a%2520hybrid%2520humanoid/exoskeleton%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Momentum%20Observer%20Contact%20Estimation%20for%20Bipedal%20Robots&entry.906535625=J.%20Joe%20Payne%20and%20Daniel%20A.%20Hagen%20and%20Denis%20Garagi%C4%87%20and%20Aaron%20M.%20Johnson&entry.1292438233=%20%20As%20bipedal%20robots%20become%20more%20and%20more%20popular%20in%20commercial%20and%20industrial%0Asettings%2C%20the%20ability%20to%20control%20them%20with%20a%20high%20degree%20of%20reliability%20is%0Acritical.%20To%20that%20end%2C%20this%20paper%20considers%20how%20to%20accurately%20estimate%20which%0Afeet%20are%20currently%20in%20contact%20with%20the%20ground%20so%20as%20to%20avoid%20improper%20control%0Aactions%20that%20could%20jeopardize%20the%20stability%20of%20the%20robot.%20Additionally%2C%20modern%0Aalgorithms%20for%20estimating%20the%20position%20and%20orientation%20of%20a%20robot%27s%20base%20frame%0Arely%20heavily%20on%20such%20contact%20mode%20estimates.%20Dedicated%20contact%20sensors%20on%20the%0Afeet%20can%20be%20used%20to%20estimate%20this%20contact%20mode%2C%20but%20these%20sensors%20are%20prone%20to%0Anoise%2C%20time%20delays%2C%20damage/yielding%20from%20repeated%20impacts%20with%20the%20ground%2C%20and%0Aare%20not%20available%20on%20every%20robot.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%0Amomentum%20observer%20based%20method%20for%20contact%20mode%20estimation%20that%20does%20not%20rely%0Aon%20such%20contact%20sensors.%20Often%2C%20momentum%20observers%20assume%20that%20the%20robot%27s%20base%0Aframe%20can%20be%20treated%20as%20an%20inertial%20frame.%20However%2C%20since%20many%20humanoids%27%20legs%0Arepresent%20a%20significant%20portion%20of%20the%20overall%20mass%2C%20the%20proposed%20method%0Ainstead%20utilizes%20multiple%20simultaneous%20dynamic%20models.%20Each%20of%20these%20models%0Aassumes%20a%20different%20contact%20condition.%20A%20given%20contact%20assumption%20is%20then%20used%0Ato%20constrain%20the%20full%20dynamics%20in%20order%20to%20avoid%20assuming%20that%20either%20the%20body%0Ais%20an%20inertial%20frame%20or%20that%20a%20fully%20accurate%20estimate%20of%20body%20velocity%20is%0Aknown.%20The%20%28dis%29agreement%20between%20each%20model%27s%20estimates%20and%20measurements%20is%0Aused%20to%20determine%20which%20contact%20mode%20is%20most%20likely%20using%20a%20Markov-style%20fusion%0Amethod.%20The%20proposed%20method%20produces%20contact%20detection%20accuracy%20of%20up%20to%2098.44%25%0Awith%20a%20low%20noise%20simulation%20and%2077.12%25%20when%20utilizing%20data%20collect%20on%20the%0ASarcos%20Guardian%20XO%20robot%20%28a%20hybrid%20humanoid/exoskeleton%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03462v1&entry.124074799=Read"},
{"title": "Local Lesion Generation is Effective for Capsule Endoscopy Image Data\n  Augmentation in a Limited Data Setting", "author": "Adrian B. Ch\u0142opowiec and Adam R. Ch\u0142opowiec and Krzysztof Galus and Wojciech Cebula and Martin Tabakov", "abstract": "  Limited medical imaging datasets challenge deep learning models by increasing\nrisks of overfitting and reduced generalization, particularly in Generative\nAdversarial Networks (GANs), where discriminators may overfit, leading to\ntraining divergence. This constraint also impairs classification models trained\non small datasets. Generative Data Augmentation (GDA) addresses this by\nexpanding training datasets with synthetic data, although it requires training\na generative model. We propose and evaluate two local lesion generation\napproaches to address the challenge of augmenting small medical image datasets.\nThe first approach employs the Poisson Image Editing algorithm, a classical\nimage processing technique, to create realistic image composites that\noutperform current state-of-the-art methods. The second approach introduces a\nnovel generative method, leveraging a fine-tuned Image Inpainting GAN to\nsynthesize realistic lesions within specified regions of real training images.\nA comprehensive comparison of the two proposed methods demonstrates that\neffective local lesion generation in a data-constrained setting allows for\nreaching new state-of-the-art results in capsule endoscopy lesion\nclassification. Combination of our techniques achieves a macro F1-score of\n33.07%, surpassing the previous best result by 7.84 percentage points (p.p.) on\nthe highly imbalanced Kvasir Capsule Dataset, a benchmark for capsule\nendoscopy. To the best of our knowledge, this work is the first to apply a\nfine-tuned Image Inpainting GAN for GDA in medical imaging, demonstrating that\nan image-conditional GAN can be adapted effectively to limited datasets to\ngenerate high-quality examples, facilitating effective data augmentation.\nAdditionally, we show that combining this GAN-based approach with classical\nimage processing techniques further improves the results.\n", "link": "http://arxiv.org/abs/2411.03098v2", "date": "2024-12-04", "relevancy": 2.1226, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5716}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5339}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Lesion%20Generation%20is%20Effective%20for%20Capsule%20Endoscopy%20Image%20Data%0A%20%20Augmentation%20in%20a%20Limited%20Data%20Setting&body=Title%3A%20Local%20Lesion%20Generation%20is%20Effective%20for%20Capsule%20Endoscopy%20Image%20Data%0A%20%20Augmentation%20in%20a%20Limited%20Data%20Setting%0AAuthor%3A%20Adrian%20B.%20Ch%C5%82opowiec%20and%20Adam%20R.%20Ch%C5%82opowiec%20and%20Krzysztof%20Galus%20and%20Wojciech%20Cebula%20and%20Martin%20Tabakov%0AAbstract%3A%20%20%20Limited%20medical%20imaging%20datasets%20challenge%20deep%20learning%20models%20by%20increasing%0Arisks%20of%20overfitting%20and%20reduced%20generalization%2C%20particularly%20in%20Generative%0AAdversarial%20Networks%20%28GANs%29%2C%20where%20discriminators%20may%20overfit%2C%20leading%20to%0Atraining%20divergence.%20This%20constraint%20also%20impairs%20classification%20models%20trained%0Aon%20small%20datasets.%20Generative%20Data%20Augmentation%20%28GDA%29%20addresses%20this%20by%0Aexpanding%20training%20datasets%20with%20synthetic%20data%2C%20although%20it%20requires%20training%0Aa%20generative%20model.%20We%20propose%20and%20evaluate%20two%20local%20lesion%20generation%0Aapproaches%20to%20address%20the%20challenge%20of%20augmenting%20small%20medical%20image%20datasets.%0AThe%20first%20approach%20employs%20the%20Poisson%20Image%20Editing%20algorithm%2C%20a%20classical%0Aimage%20processing%20technique%2C%20to%20create%20realistic%20image%20composites%20that%0Aoutperform%20current%20state-of-the-art%20methods.%20The%20second%20approach%20introduces%20a%0Anovel%20generative%20method%2C%20leveraging%20a%20fine-tuned%20Image%20Inpainting%20GAN%20to%0Asynthesize%20realistic%20lesions%20within%20specified%20regions%20of%20real%20training%20images.%0AA%20comprehensive%20comparison%20of%20the%20two%20proposed%20methods%20demonstrates%20that%0Aeffective%20local%20lesion%20generation%20in%20a%20data-constrained%20setting%20allows%20for%0Areaching%20new%20state-of-the-art%20results%20in%20capsule%20endoscopy%20lesion%0Aclassification.%20Combination%20of%20our%20techniques%20achieves%20a%20macro%20F1-score%20of%0A33.07%25%2C%20surpassing%20the%20previous%20best%20result%20by%207.84%20percentage%20points%20%28p.p.%29%20on%0Athe%20highly%20imbalanced%20Kvasir%20Capsule%20Dataset%2C%20a%20benchmark%20for%20capsule%0Aendoscopy.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20is%20the%20first%20to%20apply%20a%0Afine-tuned%20Image%20Inpainting%20GAN%20for%20GDA%20in%20medical%20imaging%2C%20demonstrating%20that%0Aan%20image-conditional%20GAN%20can%20be%20adapted%20effectively%20to%20limited%20datasets%20to%0Agenerate%20high-quality%20examples%2C%20facilitating%20effective%20data%20augmentation.%0AAdditionally%2C%20we%20show%20that%20combining%20this%20GAN-based%20approach%20with%20classical%0Aimage%20processing%20techniques%20further%20improves%20the%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03098v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Lesion%2520Generation%2520is%2520Effective%2520for%2520Capsule%2520Endoscopy%2520Image%2520Data%250A%2520%2520Augmentation%2520in%2520a%2520Limited%2520Data%2520Setting%26entry.906535625%3DAdrian%2520B.%2520Ch%25C5%2582opowiec%2520and%2520Adam%2520R.%2520Ch%25C5%2582opowiec%2520and%2520Krzysztof%2520Galus%2520and%2520Wojciech%2520Cebula%2520and%2520Martin%2520Tabakov%26entry.1292438233%3D%2520%2520Limited%2520medical%2520imaging%2520datasets%2520challenge%2520deep%2520learning%2520models%2520by%2520increasing%250Arisks%2520of%2520overfitting%2520and%2520reduced%2520generalization%252C%2520particularly%2520in%2520Generative%250AAdversarial%2520Networks%2520%2528GANs%2529%252C%2520where%2520discriminators%2520may%2520overfit%252C%2520leading%2520to%250Atraining%2520divergence.%2520This%2520constraint%2520also%2520impairs%2520classification%2520models%2520trained%250Aon%2520small%2520datasets.%2520Generative%2520Data%2520Augmentation%2520%2528GDA%2529%2520addresses%2520this%2520by%250Aexpanding%2520training%2520datasets%2520with%2520synthetic%2520data%252C%2520although%2520it%2520requires%2520training%250Aa%2520generative%2520model.%2520We%2520propose%2520and%2520evaluate%2520two%2520local%2520lesion%2520generation%250Aapproaches%2520to%2520address%2520the%2520challenge%2520of%2520augmenting%2520small%2520medical%2520image%2520datasets.%250AThe%2520first%2520approach%2520employs%2520the%2520Poisson%2520Image%2520Editing%2520algorithm%252C%2520a%2520classical%250Aimage%2520processing%2520technique%252C%2520to%2520create%2520realistic%2520image%2520composites%2520that%250Aoutperform%2520current%2520state-of-the-art%2520methods.%2520The%2520second%2520approach%2520introduces%2520a%250Anovel%2520generative%2520method%252C%2520leveraging%2520a%2520fine-tuned%2520Image%2520Inpainting%2520GAN%2520to%250Asynthesize%2520realistic%2520lesions%2520within%2520specified%2520regions%2520of%2520real%2520training%2520images.%250AA%2520comprehensive%2520comparison%2520of%2520the%2520two%2520proposed%2520methods%2520demonstrates%2520that%250Aeffective%2520local%2520lesion%2520generation%2520in%2520a%2520data-constrained%2520setting%2520allows%2520for%250Areaching%2520new%2520state-of-the-art%2520results%2520in%2520capsule%2520endoscopy%2520lesion%250Aclassification.%2520Combination%2520of%2520our%2520techniques%2520achieves%2520a%2520macro%2520F1-score%2520of%250A33.07%2525%252C%2520surpassing%2520the%2520previous%2520best%2520result%2520by%25207.84%2520percentage%2520points%2520%2528p.p.%2529%2520on%250Athe%2520highly%2520imbalanced%2520Kvasir%2520Capsule%2520Dataset%252C%2520a%2520benchmark%2520for%2520capsule%250Aendoscopy.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%2520is%2520the%2520first%2520to%2520apply%2520a%250Afine-tuned%2520Image%2520Inpainting%2520GAN%2520for%2520GDA%2520in%2520medical%2520imaging%252C%2520demonstrating%2520that%250Aan%2520image-conditional%2520GAN%2520can%2520be%2520adapted%2520effectively%2520to%2520limited%2520datasets%2520to%250Agenerate%2520high-quality%2520examples%252C%2520facilitating%2520effective%2520data%2520augmentation.%250AAdditionally%252C%2520we%2520show%2520that%2520combining%2520this%2520GAN-based%2520approach%2520with%2520classical%250Aimage%2520processing%2520techniques%2520further%2520improves%2520the%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03098v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Lesion%20Generation%20is%20Effective%20for%20Capsule%20Endoscopy%20Image%20Data%0A%20%20Augmentation%20in%20a%20Limited%20Data%20Setting&entry.906535625=Adrian%20B.%20Ch%C5%82opowiec%20and%20Adam%20R.%20Ch%C5%82opowiec%20and%20Krzysztof%20Galus%20and%20Wojciech%20Cebula%20and%20Martin%20Tabakov&entry.1292438233=%20%20Limited%20medical%20imaging%20datasets%20challenge%20deep%20learning%20models%20by%20increasing%0Arisks%20of%20overfitting%20and%20reduced%20generalization%2C%20particularly%20in%20Generative%0AAdversarial%20Networks%20%28GANs%29%2C%20where%20discriminators%20may%20overfit%2C%20leading%20to%0Atraining%20divergence.%20This%20constraint%20also%20impairs%20classification%20models%20trained%0Aon%20small%20datasets.%20Generative%20Data%20Augmentation%20%28GDA%29%20addresses%20this%20by%0Aexpanding%20training%20datasets%20with%20synthetic%20data%2C%20although%20it%20requires%20training%0Aa%20generative%20model.%20We%20propose%20and%20evaluate%20two%20local%20lesion%20generation%0Aapproaches%20to%20address%20the%20challenge%20of%20augmenting%20small%20medical%20image%20datasets.%0AThe%20first%20approach%20employs%20the%20Poisson%20Image%20Editing%20algorithm%2C%20a%20classical%0Aimage%20processing%20technique%2C%20to%20create%20realistic%20image%20composites%20that%0Aoutperform%20current%20state-of-the-art%20methods.%20The%20second%20approach%20introduces%20a%0Anovel%20generative%20method%2C%20leveraging%20a%20fine-tuned%20Image%20Inpainting%20GAN%20to%0Asynthesize%20realistic%20lesions%20within%20specified%20regions%20of%20real%20training%20images.%0AA%20comprehensive%20comparison%20of%20the%20two%20proposed%20methods%20demonstrates%20that%0Aeffective%20local%20lesion%20generation%20in%20a%20data-constrained%20setting%20allows%20for%0Areaching%20new%20state-of-the-art%20results%20in%20capsule%20endoscopy%20lesion%0Aclassification.%20Combination%20of%20our%20techniques%20achieves%20a%20macro%20F1-score%20of%0A33.07%25%2C%20surpassing%20the%20previous%20best%20result%20by%207.84%20percentage%20points%20%28p.p.%29%20on%0Athe%20highly%20imbalanced%20Kvasir%20Capsule%20Dataset%2C%20a%20benchmark%20for%20capsule%0Aendoscopy.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20is%20the%20first%20to%20apply%20a%0Afine-tuned%20Image%20Inpainting%20GAN%20for%20GDA%20in%20medical%20imaging%2C%20demonstrating%20that%0Aan%20image-conditional%20GAN%20can%20be%20adapted%20effectively%20to%20limited%20datasets%20to%0Agenerate%20high-quality%20examples%2C%20facilitating%20effective%20data%20augmentation.%0AAdditionally%2C%20we%20show%20that%20combining%20this%20GAN-based%20approach%20with%20classical%0Aimage%20processing%20techniques%20further%20improves%20the%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03098v2&entry.124074799=Read"},
{"title": "DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving", "author": "Dingrui Wang and Marc Kaufeld and Johannes Betz", "abstract": "  We present a novel autonomous driving framework, DualAD, designed to imitate\nhuman reasoning during driving. DualAD comprises two layers: a rule-based\nmotion planner at the bottom layer that handles routine driving tasks requiring\nminimal reasoning, and an upper layer featuring a rule-based text encoder that\nconverts driving scenarios from absolute states into text description. This\ntext is then processed by a large language model (LLM) to make driving\ndecisions. The upper layer intervenes in the bottom layer's decisions when\npotential danger is detected, mimicking human reasoning in critical situations.\nClosed-loop experiments demonstrate that DualAD, using a zero-shot pre-trained\nmodel, significantly outperforms rule-based motion planners that lack reasoning\nabilities. Our experiments also highlight the effectiveness of the text\nencoder, which considerably enhances the model's scenario understanding.\nAdditionally, the integrated DualAD model improves with stronger LLMs,\nindicating the framework's potential for further enhancement. Code and\nbenchmarks are available at github.com/TUM-AVS/DualAD.\n", "link": "http://arxiv.org/abs/2409.18053v3", "date": "2024-12-04", "relevancy": 2.119, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5613}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5271}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualAD%3A%20Dual-Layer%20Planning%20for%20Reasoning%20in%20Autonomous%20Driving&body=Title%3A%20DualAD%3A%20Dual-Layer%20Planning%20for%20Reasoning%20in%20Autonomous%20Driving%0AAuthor%3A%20Dingrui%20Wang%20and%20Marc%20Kaufeld%20and%20Johannes%20Betz%0AAbstract%3A%20%20%20We%20present%20a%20novel%20autonomous%20driving%20framework%2C%20DualAD%2C%20designed%20to%20imitate%0Ahuman%20reasoning%20during%20driving.%20DualAD%20comprises%20two%20layers%3A%20a%20rule-based%0Amotion%20planner%20at%20the%20bottom%20layer%20that%20handles%20routine%20driving%20tasks%20requiring%0Aminimal%20reasoning%2C%20and%20an%20upper%20layer%20featuring%20a%20rule-based%20text%20encoder%20that%0Aconverts%20driving%20scenarios%20from%20absolute%20states%20into%20text%20description.%20This%0Atext%20is%20then%20processed%20by%20a%20large%20language%20model%20%28LLM%29%20to%20make%20driving%0Adecisions.%20The%20upper%20layer%20intervenes%20in%20the%20bottom%20layer%27s%20decisions%20when%0Apotential%20danger%20is%20detected%2C%20mimicking%20human%20reasoning%20in%20critical%20situations.%0AClosed-loop%20experiments%20demonstrate%20that%20DualAD%2C%20using%20a%20zero-shot%20pre-trained%0Amodel%2C%20significantly%20outperforms%20rule-based%20motion%20planners%20that%20lack%20reasoning%0Aabilities.%20Our%20experiments%20also%20highlight%20the%20effectiveness%20of%20the%20text%0Aencoder%2C%20which%20considerably%20enhances%20the%20model%27s%20scenario%20understanding.%0AAdditionally%2C%20the%20integrated%20DualAD%20model%20improves%20with%20stronger%20LLMs%2C%0Aindicating%20the%20framework%27s%20potential%20for%20further%20enhancement.%20Code%20and%0Abenchmarks%20are%20available%20at%20github.com/TUM-AVS/DualAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18053v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualAD%253A%2520Dual-Layer%2520Planning%2520for%2520Reasoning%2520in%2520Autonomous%2520Driving%26entry.906535625%3DDingrui%2520Wang%2520and%2520Marc%2520Kaufeld%2520and%2520Johannes%2520Betz%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520autonomous%2520driving%2520framework%252C%2520DualAD%252C%2520designed%2520to%2520imitate%250Ahuman%2520reasoning%2520during%2520driving.%2520DualAD%2520comprises%2520two%2520layers%253A%2520a%2520rule-based%250Amotion%2520planner%2520at%2520the%2520bottom%2520layer%2520that%2520handles%2520routine%2520driving%2520tasks%2520requiring%250Aminimal%2520reasoning%252C%2520and%2520an%2520upper%2520layer%2520featuring%2520a%2520rule-based%2520text%2520encoder%2520that%250Aconverts%2520driving%2520scenarios%2520from%2520absolute%2520states%2520into%2520text%2520description.%2520This%250Atext%2520is%2520then%2520processed%2520by%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520make%2520driving%250Adecisions.%2520The%2520upper%2520layer%2520intervenes%2520in%2520the%2520bottom%2520layer%2527s%2520decisions%2520when%250Apotential%2520danger%2520is%2520detected%252C%2520mimicking%2520human%2520reasoning%2520in%2520critical%2520situations.%250AClosed-loop%2520experiments%2520demonstrate%2520that%2520DualAD%252C%2520using%2520a%2520zero-shot%2520pre-trained%250Amodel%252C%2520significantly%2520outperforms%2520rule-based%2520motion%2520planners%2520that%2520lack%2520reasoning%250Aabilities.%2520Our%2520experiments%2520also%2520highlight%2520the%2520effectiveness%2520of%2520the%2520text%250Aencoder%252C%2520which%2520considerably%2520enhances%2520the%2520model%2527s%2520scenario%2520understanding.%250AAdditionally%252C%2520the%2520integrated%2520DualAD%2520model%2520improves%2520with%2520stronger%2520LLMs%252C%250Aindicating%2520the%2520framework%2527s%2520potential%2520for%2520further%2520enhancement.%2520Code%2520and%250Abenchmarks%2520are%2520available%2520at%2520github.com/TUM-AVS/DualAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18053v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualAD%3A%20Dual-Layer%20Planning%20for%20Reasoning%20in%20Autonomous%20Driving&entry.906535625=Dingrui%20Wang%20and%20Marc%20Kaufeld%20and%20Johannes%20Betz&entry.1292438233=%20%20We%20present%20a%20novel%20autonomous%20driving%20framework%2C%20DualAD%2C%20designed%20to%20imitate%0Ahuman%20reasoning%20during%20driving.%20DualAD%20comprises%20two%20layers%3A%20a%20rule-based%0Amotion%20planner%20at%20the%20bottom%20layer%20that%20handles%20routine%20driving%20tasks%20requiring%0Aminimal%20reasoning%2C%20and%20an%20upper%20layer%20featuring%20a%20rule-based%20text%20encoder%20that%0Aconverts%20driving%20scenarios%20from%20absolute%20states%20into%20text%20description.%20This%0Atext%20is%20then%20processed%20by%20a%20large%20language%20model%20%28LLM%29%20to%20make%20driving%0Adecisions.%20The%20upper%20layer%20intervenes%20in%20the%20bottom%20layer%27s%20decisions%20when%0Apotential%20danger%20is%20detected%2C%20mimicking%20human%20reasoning%20in%20critical%20situations.%0AClosed-loop%20experiments%20demonstrate%20that%20DualAD%2C%20using%20a%20zero-shot%20pre-trained%0Amodel%2C%20significantly%20outperforms%20rule-based%20motion%20planners%20that%20lack%20reasoning%0Aabilities.%20Our%20experiments%20also%20highlight%20the%20effectiveness%20of%20the%20text%0Aencoder%2C%20which%20considerably%20enhances%20the%20model%27s%20scenario%20understanding.%0AAdditionally%2C%20the%20integrated%20DualAD%20model%20improves%20with%20stronger%20LLMs%2C%0Aindicating%20the%20framework%27s%20potential%20for%20further%20enhancement.%20Code%20and%0Abenchmarks%20are%20available%20at%20github.com/TUM-AVS/DualAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18053v3&entry.124074799=Read"},
{"title": "A Bidirectional Siamese Recurrent Neural Network for Accurate Gait\n  Recognition Using Body Landmarks", "author": "Proma Hossain Progga and Md. Jobayer Rahman and Swapnil Biswas and Md. Shakil Ahmed and Arif Reza Anwary and Swakkhar Shatabda", "abstract": "  Gait recognition is a significant biometric technique for person\nidentification, particularly in scenarios where other physiological biometrics\nare impractical or ineffective. In this paper, we address the challenges\nassociated with gait recognition and present a novel approach to improve its\naccuracy and reliability. The proposed method leverages advanced techniques,\nincluding sequential gait landmarks obtained through the Mediapipe pose\nestimation model, Procrustes analysis for alignment, and a Siamese\nbiGRU-dualStack Neural Network architecture for capturing temporal\ndependencies. Extensive experiments were conducted on large-scale cross-view\ndatasets to demonstrate the effectiveness of the approach, achieving high\nrecognition accuracy compared to other models. The model demonstrated\naccuracies of 95.7%, 94.44%, 87.71%, and 86.6% on CASIA-B, SZU RGB-D, OU-MVLP,\nand Gait3D datasets respectively. The results highlight the potential\napplications of the proposed method in various practical domains, indicating\nits significant contribution to the field of gait recognition.\n", "link": "http://arxiv.org/abs/2412.03498v1", "date": "2024-12-04", "relevancy": 2.1053, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5626}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5305}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bidirectional%20Siamese%20Recurrent%20Neural%20Network%20for%20Accurate%20Gait%0A%20%20Recognition%20Using%20Body%20Landmarks&body=Title%3A%20A%20Bidirectional%20Siamese%20Recurrent%20Neural%20Network%20for%20Accurate%20Gait%0A%20%20Recognition%20Using%20Body%20Landmarks%0AAuthor%3A%20Proma%20Hossain%20Progga%20and%20Md.%20Jobayer%20Rahman%20and%20Swapnil%20Biswas%20and%20Md.%20Shakil%20Ahmed%20and%20Arif%20Reza%20Anwary%20and%20Swakkhar%20Shatabda%0AAbstract%3A%20%20%20Gait%20recognition%20is%20a%20significant%20biometric%20technique%20for%20person%0Aidentification%2C%20particularly%20in%20scenarios%20where%20other%20physiological%20biometrics%0Aare%20impractical%20or%20ineffective.%20In%20this%20paper%2C%20we%20address%20the%20challenges%0Aassociated%20with%20gait%20recognition%20and%20present%20a%20novel%20approach%20to%20improve%20its%0Aaccuracy%20and%20reliability.%20The%20proposed%20method%20leverages%20advanced%20techniques%2C%0Aincluding%20sequential%20gait%20landmarks%20obtained%20through%20the%20Mediapipe%20pose%0Aestimation%20model%2C%20Procrustes%20analysis%20for%20alignment%2C%20and%20a%20Siamese%0AbiGRU-dualStack%20Neural%20Network%20architecture%20for%20capturing%20temporal%0Adependencies.%20Extensive%20experiments%20were%20conducted%20on%20large-scale%20cross-view%0Adatasets%20to%20demonstrate%20the%20effectiveness%20of%20the%20approach%2C%20achieving%20high%0Arecognition%20accuracy%20compared%20to%20other%20models.%20The%20model%20demonstrated%0Aaccuracies%20of%2095.7%25%2C%2094.44%25%2C%2087.71%25%2C%20and%2086.6%25%20on%20CASIA-B%2C%20SZU%20RGB-D%2C%20OU-MVLP%2C%0Aand%20Gait3D%20datasets%20respectively.%20The%20results%20highlight%20the%20potential%0Aapplications%20of%20the%20proposed%20method%20in%20various%20practical%20domains%2C%20indicating%0Aits%20significant%20contribution%20to%20the%20field%20of%20gait%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bidirectional%2520Siamese%2520Recurrent%2520Neural%2520Network%2520for%2520Accurate%2520Gait%250A%2520%2520Recognition%2520Using%2520Body%2520Landmarks%26entry.906535625%3DProma%2520Hossain%2520Progga%2520and%2520Md.%2520Jobayer%2520Rahman%2520and%2520Swapnil%2520Biswas%2520and%2520Md.%2520Shakil%2520Ahmed%2520and%2520Arif%2520Reza%2520Anwary%2520and%2520Swakkhar%2520Shatabda%26entry.1292438233%3D%2520%2520Gait%2520recognition%2520is%2520a%2520significant%2520biometric%2520technique%2520for%2520person%250Aidentification%252C%2520particularly%2520in%2520scenarios%2520where%2520other%2520physiological%2520biometrics%250Aare%2520impractical%2520or%2520ineffective.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520challenges%250Aassociated%2520with%2520gait%2520recognition%2520and%2520present%2520a%2520novel%2520approach%2520to%2520improve%2520its%250Aaccuracy%2520and%2520reliability.%2520The%2520proposed%2520method%2520leverages%2520advanced%2520techniques%252C%250Aincluding%2520sequential%2520gait%2520landmarks%2520obtained%2520through%2520the%2520Mediapipe%2520pose%250Aestimation%2520model%252C%2520Procrustes%2520analysis%2520for%2520alignment%252C%2520and%2520a%2520Siamese%250AbiGRU-dualStack%2520Neural%2520Network%2520architecture%2520for%2520capturing%2520temporal%250Adependencies.%2520Extensive%2520experiments%2520were%2520conducted%2520on%2520large-scale%2520cross-view%250Adatasets%2520to%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520approach%252C%2520achieving%2520high%250Arecognition%2520accuracy%2520compared%2520to%2520other%2520models.%2520The%2520model%2520demonstrated%250Aaccuracies%2520of%252095.7%2525%252C%252094.44%2525%252C%252087.71%2525%252C%2520and%252086.6%2525%2520on%2520CASIA-B%252C%2520SZU%2520RGB-D%252C%2520OU-MVLP%252C%250Aand%2520Gait3D%2520datasets%2520respectively.%2520The%2520results%2520highlight%2520the%2520potential%250Aapplications%2520of%2520the%2520proposed%2520method%2520in%2520various%2520practical%2520domains%252C%2520indicating%250Aits%2520significant%2520contribution%2520to%2520the%2520field%2520of%2520gait%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bidirectional%20Siamese%20Recurrent%20Neural%20Network%20for%20Accurate%20Gait%0A%20%20Recognition%20Using%20Body%20Landmarks&entry.906535625=Proma%20Hossain%20Progga%20and%20Md.%20Jobayer%20Rahman%20and%20Swapnil%20Biswas%20and%20Md.%20Shakil%20Ahmed%20and%20Arif%20Reza%20Anwary%20and%20Swakkhar%20Shatabda&entry.1292438233=%20%20Gait%20recognition%20is%20a%20significant%20biometric%20technique%20for%20person%0Aidentification%2C%20particularly%20in%20scenarios%20where%20other%20physiological%20biometrics%0Aare%20impractical%20or%20ineffective.%20In%20this%20paper%2C%20we%20address%20the%20challenges%0Aassociated%20with%20gait%20recognition%20and%20present%20a%20novel%20approach%20to%20improve%20its%0Aaccuracy%20and%20reliability.%20The%20proposed%20method%20leverages%20advanced%20techniques%2C%0Aincluding%20sequential%20gait%20landmarks%20obtained%20through%20the%20Mediapipe%20pose%0Aestimation%20model%2C%20Procrustes%20analysis%20for%20alignment%2C%20and%20a%20Siamese%0AbiGRU-dualStack%20Neural%20Network%20architecture%20for%20capturing%20temporal%0Adependencies.%20Extensive%20experiments%20were%20conducted%20on%20large-scale%20cross-view%0Adatasets%20to%20demonstrate%20the%20effectiveness%20of%20the%20approach%2C%20achieving%20high%0Arecognition%20accuracy%20compared%20to%20other%20models.%20The%20model%20demonstrated%0Aaccuracies%20of%2095.7%25%2C%2094.44%25%2C%2087.71%25%2C%20and%2086.6%25%20on%20CASIA-B%2C%20SZU%20RGB-D%2C%20OU-MVLP%2C%0Aand%20Gait3D%20datasets%20respectively.%20The%20results%20highlight%20the%20potential%0Aapplications%20of%20the%20proposed%20method%20in%20various%20practical%20domains%2C%20indicating%0Aits%20significant%20contribution%20to%20the%20field%20of%20gait%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03498v1&entry.124074799=Read"},
{"title": "Yo'LLaVA: Your Personalized Language and Vision Assistant", "author": "Thao Nguyen and Haotian Liu and Yuheng Li and Mu Cai and Utkarsh Ojha and Yong Jae Lee", "abstract": "  Large Multimodal Models (LMMs) have shown remarkable capabilities across a\nvariety of tasks (e.g., image captioning, visual question answering). While\nbroad, their knowledge remains generic (e.g., recognizing a dog), and they are\nunable to handle personalized subjects (e.g., recognizing a user's pet dog).\nHuman reasoning, in contrast, typically operates within the context of specific\nsubjects in our surroundings. For example, one might ask, \"What should I buy\nfor my dog's birthday?\"; as opposed to a generic inquiry about \"What should I\nbuy for a dog's birthday?\". Similarly, when looking at a friend's image, the\ninterest lies in seeing their activities (e.g., \"my friend is holding a cat\"),\nrather than merely observing generic human actions (e.g., \"a man is holding a\ncat\"). In this paper, we introduce the novel task of personalizing LMMs, so\nthat they can have conversations about a specific subject. We propose Yo'LLaVA,\nwhich learns to embed a personalized subject into a set of latent tokens given\na handful of example images of the subject. Our qualitative and quantitative\nanalyses reveal that Yo'LLaVA can learn the concept more efficiently using\nfewer tokens and more effectively encode the visual attributes compared to\nstrong prompting baselines (e.g., LLaVA).\n", "link": "http://arxiv.org/abs/2406.09400v2", "date": "2024-12-04", "relevancy": 2.1026, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5298}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5232}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Yo%27LLaVA%3A%20Your%20Personalized%20Language%20and%20Vision%20Assistant&body=Title%3A%20Yo%27LLaVA%3A%20Your%20Personalized%20Language%20and%20Vision%20Assistant%0AAuthor%3A%20Thao%20Nguyen%20and%20Haotian%20Liu%20and%20Yuheng%20Li%20and%20Mu%20Cai%20and%20Utkarsh%20Ojha%20and%20Yong%20Jae%20Lee%0AAbstract%3A%20%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20shown%20remarkable%20capabilities%20across%20a%0Avariety%20of%20tasks%20%28e.g.%2C%20image%20captioning%2C%20visual%20question%20answering%29.%20While%0Abroad%2C%20their%20knowledge%20remains%20generic%20%28e.g.%2C%20recognizing%20a%20dog%29%2C%20and%20they%20are%0Aunable%20to%20handle%20personalized%20subjects%20%28e.g.%2C%20recognizing%20a%20user%27s%20pet%20dog%29.%0AHuman%20reasoning%2C%20in%20contrast%2C%20typically%20operates%20within%20the%20context%20of%20specific%0Asubjects%20in%20our%20surroundings.%20For%20example%2C%20one%20might%20ask%2C%20%22What%20should%20I%20buy%0Afor%20my%20dog%27s%20birthday%3F%22%3B%20as%20opposed%20to%20a%20generic%20inquiry%20about%20%22What%20should%20I%0Abuy%20for%20a%20dog%27s%20birthday%3F%22.%20Similarly%2C%20when%20looking%20at%20a%20friend%27s%20image%2C%20the%0Ainterest%20lies%20in%20seeing%20their%20activities%20%28e.g.%2C%20%22my%20friend%20is%20holding%20a%20cat%22%29%2C%0Arather%20than%20merely%20observing%20generic%20human%20actions%20%28e.g.%2C%20%22a%20man%20is%20holding%20a%0Acat%22%29.%20In%20this%20paper%2C%20we%20introduce%20the%20novel%20task%20of%20personalizing%20LMMs%2C%20so%0Athat%20they%20can%20have%20conversations%20about%20a%20specific%20subject.%20We%20propose%20Yo%27LLaVA%2C%0Awhich%20learns%20to%20embed%20a%20personalized%20subject%20into%20a%20set%20of%20latent%20tokens%20given%0Aa%20handful%20of%20example%20images%20of%20the%20subject.%20Our%20qualitative%20and%20quantitative%0Aanalyses%20reveal%20that%20Yo%27LLaVA%20can%20learn%20the%20concept%20more%20efficiently%20using%0Afewer%20tokens%20and%20more%20effectively%20encode%20the%20visual%20attributes%20compared%20to%0Astrong%20prompting%20baselines%20%28e.g.%2C%20LLaVA%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09400v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYo%2527LLaVA%253A%2520Your%2520Personalized%2520Language%2520and%2520Vision%2520Assistant%26entry.906535625%3DThao%2520Nguyen%2520and%2520Haotian%2520Liu%2520and%2520Yuheng%2520Li%2520and%2520Mu%2520Cai%2520and%2520Utkarsh%2520Ojha%2520and%2520Yong%2520Jae%2520Lee%26entry.1292438233%3D%2520%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520across%2520a%250Avariety%2520of%2520tasks%2520%2528e.g.%252C%2520image%2520captioning%252C%2520visual%2520question%2520answering%2529.%2520While%250Abroad%252C%2520their%2520knowledge%2520remains%2520generic%2520%2528e.g.%252C%2520recognizing%2520a%2520dog%2529%252C%2520and%2520they%2520are%250Aunable%2520to%2520handle%2520personalized%2520subjects%2520%2528e.g.%252C%2520recognizing%2520a%2520user%2527s%2520pet%2520dog%2529.%250AHuman%2520reasoning%252C%2520in%2520contrast%252C%2520typically%2520operates%2520within%2520the%2520context%2520of%2520specific%250Asubjects%2520in%2520our%2520surroundings.%2520For%2520example%252C%2520one%2520might%2520ask%252C%2520%2522What%2520should%2520I%2520buy%250Afor%2520my%2520dog%2527s%2520birthday%253F%2522%253B%2520as%2520opposed%2520to%2520a%2520generic%2520inquiry%2520about%2520%2522What%2520should%2520I%250Abuy%2520for%2520a%2520dog%2527s%2520birthday%253F%2522.%2520Similarly%252C%2520when%2520looking%2520at%2520a%2520friend%2527s%2520image%252C%2520the%250Ainterest%2520lies%2520in%2520seeing%2520their%2520activities%2520%2528e.g.%252C%2520%2522my%2520friend%2520is%2520holding%2520a%2520cat%2522%2529%252C%250Arather%2520than%2520merely%2520observing%2520generic%2520human%2520actions%2520%2528e.g.%252C%2520%2522a%2520man%2520is%2520holding%2520a%250Acat%2522%2529.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520novel%2520task%2520of%2520personalizing%2520LMMs%252C%2520so%250Athat%2520they%2520can%2520have%2520conversations%2520about%2520a%2520specific%2520subject.%2520We%2520propose%2520Yo%2527LLaVA%252C%250Awhich%2520learns%2520to%2520embed%2520a%2520personalized%2520subject%2520into%2520a%2520set%2520of%2520latent%2520tokens%2520given%250Aa%2520handful%2520of%2520example%2520images%2520of%2520the%2520subject.%2520Our%2520qualitative%2520and%2520quantitative%250Aanalyses%2520reveal%2520that%2520Yo%2527LLaVA%2520can%2520learn%2520the%2520concept%2520more%2520efficiently%2520using%250Afewer%2520tokens%2520and%2520more%2520effectively%2520encode%2520the%2520visual%2520attributes%2520compared%2520to%250Astrong%2520prompting%2520baselines%2520%2528e.g.%252C%2520LLaVA%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09400v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Yo%27LLaVA%3A%20Your%20Personalized%20Language%20and%20Vision%20Assistant&entry.906535625=Thao%20Nguyen%20and%20Haotian%20Liu%20and%20Yuheng%20Li%20and%20Mu%20Cai%20and%20Utkarsh%20Ojha%20and%20Yong%20Jae%20Lee&entry.1292438233=%20%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20shown%20remarkable%20capabilities%20across%20a%0Avariety%20of%20tasks%20%28e.g.%2C%20image%20captioning%2C%20visual%20question%20answering%29.%20While%0Abroad%2C%20their%20knowledge%20remains%20generic%20%28e.g.%2C%20recognizing%20a%20dog%29%2C%20and%20they%20are%0Aunable%20to%20handle%20personalized%20subjects%20%28e.g.%2C%20recognizing%20a%20user%27s%20pet%20dog%29.%0AHuman%20reasoning%2C%20in%20contrast%2C%20typically%20operates%20within%20the%20context%20of%20specific%0Asubjects%20in%20our%20surroundings.%20For%20example%2C%20one%20might%20ask%2C%20%22What%20should%20I%20buy%0Afor%20my%20dog%27s%20birthday%3F%22%3B%20as%20opposed%20to%20a%20generic%20inquiry%20about%20%22What%20should%20I%0Abuy%20for%20a%20dog%27s%20birthday%3F%22.%20Similarly%2C%20when%20looking%20at%20a%20friend%27s%20image%2C%20the%0Ainterest%20lies%20in%20seeing%20their%20activities%20%28e.g.%2C%20%22my%20friend%20is%20holding%20a%20cat%22%29%2C%0Arather%20than%20merely%20observing%20generic%20human%20actions%20%28e.g.%2C%20%22a%20man%20is%20holding%20a%0Acat%22%29.%20In%20this%20paper%2C%20we%20introduce%20the%20novel%20task%20of%20personalizing%20LMMs%2C%20so%0Athat%20they%20can%20have%20conversations%20about%20a%20specific%20subject.%20We%20propose%20Yo%27LLaVA%2C%0Awhich%20learns%20to%20embed%20a%20personalized%20subject%20into%20a%20set%20of%20latent%20tokens%20given%0Aa%20handful%20of%20example%20images%20of%20the%20subject.%20Our%20qualitative%20and%20quantitative%0Aanalyses%20reveal%20that%20Yo%27LLaVA%20can%20learn%20the%20concept%20more%20efficiently%20using%0Afewer%20tokens%20and%20more%20effectively%20encode%20the%20visual%20attributes%20compared%20to%0Astrong%20prompting%20baselines%20%28e.g.%2C%20LLaVA%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09400v2&entry.124074799=Read"},
{"title": "NODE-AdvGAN: Improving the transferability and perceptual similarity of\n  adversarial examples by dynamic-system-driven adversarial generative model", "author": "Xinheng Xie and Yue Wu and Cuiyu He", "abstract": "  Understanding adversarial examples is crucial for improving the model's\nrobustness, as they introduce imperceptible perturbations that deceive models.\nEffective adversarial examples, therefore, offer the potential to train more\nrobust models by removing their singularities. We propose NODE-AdvGAN, a novel\napproach that treats adversarial generation as a continuous process and employs\na Neural Ordinary Differential Equation (NODE) for simulating the dynamics of\nthe generator. By mimicking the iterative nature of traditional gradient-based\nmethods, NODE-AdvGAN generates smoother and more precise perturbations that\npreserve high perceptual similarity when added to benign images. We also\npropose a new training strategy, NODE-AdvGAN-T, which enhances transferability\nin black-box attacks by effectively tuning noise parameters during training.\nExperiments demonstrate that NODE-AdvGAN and NODE-AdvGAN-T generate more\neffective adversarial examples that achieve higher attack success rates while\npreserving better perceptual quality than traditional GAN-based methods.\n", "link": "http://arxiv.org/abs/2412.03539v1", "date": "2024-12-04", "relevancy": 2.1024, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.532}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5274}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NODE-AdvGAN%3A%20Improving%20the%20transferability%20and%20perceptual%20similarity%20of%0A%20%20adversarial%20examples%20by%20dynamic-system-driven%20adversarial%20generative%20model&body=Title%3A%20NODE-AdvGAN%3A%20Improving%20the%20transferability%20and%20perceptual%20similarity%20of%0A%20%20adversarial%20examples%20by%20dynamic-system-driven%20adversarial%20generative%20model%0AAuthor%3A%20Xinheng%20Xie%20and%20Yue%20Wu%20and%20Cuiyu%20He%0AAbstract%3A%20%20%20Understanding%20adversarial%20examples%20is%20crucial%20for%20improving%20the%20model%27s%0Arobustness%2C%20as%20they%20introduce%20imperceptible%20perturbations%20that%20deceive%20models.%0AEffective%20adversarial%20examples%2C%20therefore%2C%20offer%20the%20potential%20to%20train%20more%0Arobust%20models%20by%20removing%20their%20singularities.%20We%20propose%20NODE-AdvGAN%2C%20a%20novel%0Aapproach%20that%20treats%20adversarial%20generation%20as%20a%20continuous%20process%20and%20employs%0Aa%20Neural%20Ordinary%20Differential%20Equation%20%28NODE%29%20for%20simulating%20the%20dynamics%20of%0Athe%20generator.%20By%20mimicking%20the%20iterative%20nature%20of%20traditional%20gradient-based%0Amethods%2C%20NODE-AdvGAN%20generates%20smoother%20and%20more%20precise%20perturbations%20that%0Apreserve%20high%20perceptual%20similarity%20when%20added%20to%20benign%20images.%20We%20also%0Apropose%20a%20new%20training%20strategy%2C%20NODE-AdvGAN-T%2C%20which%20enhances%20transferability%0Ain%20black-box%20attacks%20by%20effectively%20tuning%20noise%20parameters%20during%20training.%0AExperiments%20demonstrate%20that%20NODE-AdvGAN%20and%20NODE-AdvGAN-T%20generate%20more%0Aeffective%20adversarial%20examples%20that%20achieve%20higher%20attack%20success%20rates%20while%0Apreserving%20better%20perceptual%20quality%20than%20traditional%20GAN-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNODE-AdvGAN%253A%2520Improving%2520the%2520transferability%2520and%2520perceptual%2520similarity%2520of%250A%2520%2520adversarial%2520examples%2520by%2520dynamic-system-driven%2520adversarial%2520generative%2520model%26entry.906535625%3DXinheng%2520Xie%2520and%2520Yue%2520Wu%2520and%2520Cuiyu%2520He%26entry.1292438233%3D%2520%2520Understanding%2520adversarial%2520examples%2520is%2520crucial%2520for%2520improving%2520the%2520model%2527s%250Arobustness%252C%2520as%2520they%2520introduce%2520imperceptible%2520perturbations%2520that%2520deceive%2520models.%250AEffective%2520adversarial%2520examples%252C%2520therefore%252C%2520offer%2520the%2520potential%2520to%2520train%2520more%250Arobust%2520models%2520by%2520removing%2520their%2520singularities.%2520We%2520propose%2520NODE-AdvGAN%252C%2520a%2520novel%250Aapproach%2520that%2520treats%2520adversarial%2520generation%2520as%2520a%2520continuous%2520process%2520and%2520employs%250Aa%2520Neural%2520Ordinary%2520Differential%2520Equation%2520%2528NODE%2529%2520for%2520simulating%2520the%2520dynamics%2520of%250Athe%2520generator.%2520By%2520mimicking%2520the%2520iterative%2520nature%2520of%2520traditional%2520gradient-based%250Amethods%252C%2520NODE-AdvGAN%2520generates%2520smoother%2520and%2520more%2520precise%2520perturbations%2520that%250Apreserve%2520high%2520perceptual%2520similarity%2520when%2520added%2520to%2520benign%2520images.%2520We%2520also%250Apropose%2520a%2520new%2520training%2520strategy%252C%2520NODE-AdvGAN-T%252C%2520which%2520enhances%2520transferability%250Ain%2520black-box%2520attacks%2520by%2520effectively%2520tuning%2520noise%2520parameters%2520during%2520training.%250AExperiments%2520demonstrate%2520that%2520NODE-AdvGAN%2520and%2520NODE-AdvGAN-T%2520generate%2520more%250Aeffective%2520adversarial%2520examples%2520that%2520achieve%2520higher%2520attack%2520success%2520rates%2520while%250Apreserving%2520better%2520perceptual%2520quality%2520than%2520traditional%2520GAN-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NODE-AdvGAN%3A%20Improving%20the%20transferability%20and%20perceptual%20similarity%20of%0A%20%20adversarial%20examples%20by%20dynamic-system-driven%20adversarial%20generative%20model&entry.906535625=Xinheng%20Xie%20and%20Yue%20Wu%20and%20Cuiyu%20He&entry.1292438233=%20%20Understanding%20adversarial%20examples%20is%20crucial%20for%20improving%20the%20model%27s%0Arobustness%2C%20as%20they%20introduce%20imperceptible%20perturbations%20that%20deceive%20models.%0AEffective%20adversarial%20examples%2C%20therefore%2C%20offer%20the%20potential%20to%20train%20more%0Arobust%20models%20by%20removing%20their%20singularities.%20We%20propose%20NODE-AdvGAN%2C%20a%20novel%0Aapproach%20that%20treats%20adversarial%20generation%20as%20a%20continuous%20process%20and%20employs%0Aa%20Neural%20Ordinary%20Differential%20Equation%20%28NODE%29%20for%20simulating%20the%20dynamics%20of%0Athe%20generator.%20By%20mimicking%20the%20iterative%20nature%20of%20traditional%20gradient-based%0Amethods%2C%20NODE-AdvGAN%20generates%20smoother%20and%20more%20precise%20perturbations%20that%0Apreserve%20high%20perceptual%20similarity%20when%20added%20to%20benign%20images.%20We%20also%0Apropose%20a%20new%20training%20strategy%2C%20NODE-AdvGAN-T%2C%20which%20enhances%20transferability%0Ain%20black-box%20attacks%20by%20effectively%20tuning%20noise%20parameters%20during%20training.%0AExperiments%20demonstrate%20that%20NODE-AdvGAN%20and%20NODE-AdvGAN-T%20generate%20more%0Aeffective%20adversarial%20examples%20that%20achieve%20higher%20attack%20success%20rates%20while%0Apreserving%20better%20perceptual%20quality%20than%20traditional%20GAN-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03539v1&entry.124074799=Read"},
{"title": "Improving Linguistic Diversity of Large Language Models with Possibility\n  Exploration Fine-Tuning", "author": "Long Mai and Julie Carson-Berndsen", "abstract": "  While Large Language Models (LLMs) have made significant strides in\nreplicating human-like abilities, there are concerns about a reduction in the\nlinguistic diversity of their outputs. This results in the homogenization of\nviewpoints and perspectives, as well as the underrepresentation of specific\ndemographic groups. Although several fine-tuning and prompting techniques have\nbeen suggested to tackle the issue, they are often tailored to specific tasks\nor come with a substantial increase in computational cost and latency. This\nmakes them challenging to apply to applications that demand very low latency,\nsuch as chatbots and virtual assistants. We propose Possibility Exploration\nFine-Tuning (PEFT), a task-agnostic framework that enhances the text diversity\nof LLMs without increasing latency or computational cost. Given the same\nprompt, models fine-tuned with PEFT can simultaneously generate multiple\ndiverse responses, each corresponding with a controllable possibility number.\nExperiments on dialogue and story generation tasks demonstrate that PEFT\nsignificantly enhances the diversity of LLM outputs, as evidenced by lower\nsimilarity between candidate responses. Since PEFT emphasizes semantic\ndiversity over lexical diversity, it can also notably reduce demographic bias\nin dialogue systems. The implementations and datasets are available in our\nrepository: https://github.com/mailong25/peft_diversity\n", "link": "http://arxiv.org/abs/2412.03343v1", "date": "2024-12-04", "relevancy": 2.0922, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5231}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5231}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Linguistic%20Diversity%20of%20Large%20Language%20Models%20with%20Possibility%0A%20%20Exploration%20Fine-Tuning&body=Title%3A%20Improving%20Linguistic%20Diversity%20of%20Large%20Language%20Models%20with%20Possibility%0A%20%20Exploration%20Fine-Tuning%0AAuthor%3A%20Long%20Mai%20and%20Julie%20Carson-Berndsen%0AAbstract%3A%20%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20strides%20in%0Areplicating%20human-like%20abilities%2C%20there%20are%20concerns%20about%20a%20reduction%20in%20the%0Alinguistic%20diversity%20of%20their%20outputs.%20This%20results%20in%20the%20homogenization%20of%0Aviewpoints%20and%20perspectives%2C%20as%20well%20as%20the%20underrepresentation%20of%20specific%0Ademographic%20groups.%20Although%20several%20fine-tuning%20and%20prompting%20techniques%20have%0Abeen%20suggested%20to%20tackle%20the%20issue%2C%20they%20are%20often%20tailored%20to%20specific%20tasks%0Aor%20come%20with%20a%20substantial%20increase%20in%20computational%20cost%20and%20latency.%20This%0Amakes%20them%20challenging%20to%20apply%20to%20applications%20that%20demand%20very%20low%20latency%2C%0Asuch%20as%20chatbots%20and%20virtual%20assistants.%20We%20propose%20Possibility%20Exploration%0AFine-Tuning%20%28PEFT%29%2C%20a%20task-agnostic%20framework%20that%20enhances%20the%20text%20diversity%0Aof%20LLMs%20without%20increasing%20latency%20or%20computational%20cost.%20Given%20the%20same%0Aprompt%2C%20models%20fine-tuned%20with%20PEFT%20can%20simultaneously%20generate%20multiple%0Adiverse%20responses%2C%20each%20corresponding%20with%20a%20controllable%20possibility%20number.%0AExperiments%20on%20dialogue%20and%20story%20generation%20tasks%20demonstrate%20that%20PEFT%0Asignificantly%20enhances%20the%20diversity%20of%20LLM%20outputs%2C%20as%20evidenced%20by%20lower%0Asimilarity%20between%20candidate%20responses.%20Since%20PEFT%20emphasizes%20semantic%0Adiversity%20over%20lexical%20diversity%2C%20it%20can%20also%20notably%20reduce%20demographic%20bias%0Ain%20dialogue%20systems.%20The%20implementations%20and%20datasets%20are%20available%20in%20our%0Arepository%3A%20https%3A//github.com/mailong25/peft_diversity%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Linguistic%2520Diversity%2520of%2520Large%2520Language%2520Models%2520with%2520Possibility%250A%2520%2520Exploration%2520Fine-Tuning%26entry.906535625%3DLong%2520Mai%2520and%2520Julie%2520Carson-Berndsen%26entry.1292438233%3D%2520%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520strides%2520in%250Areplicating%2520human-like%2520abilities%252C%2520there%2520are%2520concerns%2520about%2520a%2520reduction%2520in%2520the%250Alinguistic%2520diversity%2520of%2520their%2520outputs.%2520This%2520results%2520in%2520the%2520homogenization%2520of%250Aviewpoints%2520and%2520perspectives%252C%2520as%2520well%2520as%2520the%2520underrepresentation%2520of%2520specific%250Ademographic%2520groups.%2520Although%2520several%2520fine-tuning%2520and%2520prompting%2520techniques%2520have%250Abeen%2520suggested%2520to%2520tackle%2520the%2520issue%252C%2520they%2520are%2520often%2520tailored%2520to%2520specific%2520tasks%250Aor%2520come%2520with%2520a%2520substantial%2520increase%2520in%2520computational%2520cost%2520and%2520latency.%2520This%250Amakes%2520them%2520challenging%2520to%2520apply%2520to%2520applications%2520that%2520demand%2520very%2520low%2520latency%252C%250Asuch%2520as%2520chatbots%2520and%2520virtual%2520assistants.%2520We%2520propose%2520Possibility%2520Exploration%250AFine-Tuning%2520%2528PEFT%2529%252C%2520a%2520task-agnostic%2520framework%2520that%2520enhances%2520the%2520text%2520diversity%250Aof%2520LLMs%2520without%2520increasing%2520latency%2520or%2520computational%2520cost.%2520Given%2520the%2520same%250Aprompt%252C%2520models%2520fine-tuned%2520with%2520PEFT%2520can%2520simultaneously%2520generate%2520multiple%250Adiverse%2520responses%252C%2520each%2520corresponding%2520with%2520a%2520controllable%2520possibility%2520number.%250AExperiments%2520on%2520dialogue%2520and%2520story%2520generation%2520tasks%2520demonstrate%2520that%2520PEFT%250Asignificantly%2520enhances%2520the%2520diversity%2520of%2520LLM%2520outputs%252C%2520as%2520evidenced%2520by%2520lower%250Asimilarity%2520between%2520candidate%2520responses.%2520Since%2520PEFT%2520emphasizes%2520semantic%250Adiversity%2520over%2520lexical%2520diversity%252C%2520it%2520can%2520also%2520notably%2520reduce%2520demographic%2520bias%250Ain%2520dialogue%2520systems.%2520The%2520implementations%2520and%2520datasets%2520are%2520available%2520in%2520our%250Arepository%253A%2520https%253A//github.com/mailong25/peft_diversity%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Linguistic%20Diversity%20of%20Large%20Language%20Models%20with%20Possibility%0A%20%20Exploration%20Fine-Tuning&entry.906535625=Long%20Mai%20and%20Julie%20Carson-Berndsen&entry.1292438233=%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20strides%20in%0Areplicating%20human-like%20abilities%2C%20there%20are%20concerns%20about%20a%20reduction%20in%20the%0Alinguistic%20diversity%20of%20their%20outputs.%20This%20results%20in%20the%20homogenization%20of%0Aviewpoints%20and%20perspectives%2C%20as%20well%20as%20the%20underrepresentation%20of%20specific%0Ademographic%20groups.%20Although%20several%20fine-tuning%20and%20prompting%20techniques%20have%0Abeen%20suggested%20to%20tackle%20the%20issue%2C%20they%20are%20often%20tailored%20to%20specific%20tasks%0Aor%20come%20with%20a%20substantial%20increase%20in%20computational%20cost%20and%20latency.%20This%0Amakes%20them%20challenging%20to%20apply%20to%20applications%20that%20demand%20very%20low%20latency%2C%0Asuch%20as%20chatbots%20and%20virtual%20assistants.%20We%20propose%20Possibility%20Exploration%0AFine-Tuning%20%28PEFT%29%2C%20a%20task-agnostic%20framework%20that%20enhances%20the%20text%20diversity%0Aof%20LLMs%20without%20increasing%20latency%20or%20computational%20cost.%20Given%20the%20same%0Aprompt%2C%20models%20fine-tuned%20with%20PEFT%20can%20simultaneously%20generate%20multiple%0Adiverse%20responses%2C%20each%20corresponding%20with%20a%20controllable%20possibility%20number.%0AExperiments%20on%20dialogue%20and%20story%20generation%20tasks%20demonstrate%20that%20PEFT%0Asignificantly%20enhances%20the%20diversity%20of%20LLM%20outputs%2C%20as%20evidenced%20by%20lower%0Asimilarity%20between%20candidate%20responses.%20Since%20PEFT%20emphasizes%20semantic%0Adiversity%20over%20lexical%20diversity%2C%20it%20can%20also%20notably%20reduce%20demographic%20bias%0Ain%20dialogue%20systems.%20The%20implementations%20and%20datasets%20are%20available%20in%20our%0Arepository%3A%20https%3A//github.com/mailong25/peft_diversity%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03343v1&entry.124074799=Read"},
{"title": "Defending Against Repetitive Backdoor Attacks on Semi-supervised\n  Learning through Lens of Rate-Distortion-Perception Trade-off", "author": "Cheng-Yi Lee and Ching-Chia Kao and Cheng-Han Yeh and Chun-Shien Lu and Chia-Mu Yu and Chu-Song Chen", "abstract": "  Semi-supervised learning (SSL) has achieved remarkable performance with a\nsmall fraction of labeled data by leveraging vast amounts of unlabeled data\nfrom the Internet. However, this large pool of untrusted data is extremely\nvulnerable to data poisoning, leading to potential backdoor attacks. Current\nbackdoor defenses are not yet effective against such a vulnerability in SSL. In\nthis study, we propose a novel method, Unlabeled Data Purification (UPure), to\ndisrupt the association between trigger patterns and target classes by\nintroducing perturbations in the frequency domain. By leveraging the\nRate-Distortion-Perception (RDP) trade-off, we further identify the frequency\nband, where the perturbations are added, and justify this selection. Notably,\nUPure purifies poisoned unlabeled data without the need of extra clean labeled\ndata. Extensive experiments on four benchmark datasets and five SSL algorithms\ndemonstrate that UPure effectively reduces the attack success rate from 99.78%\nto 0% while maintaining model accuracy. Code is available here:\n\\url{https://github.com/chengyi-chris/UPure}.\n", "link": "http://arxiv.org/abs/2407.10180v2", "date": "2024-12-04", "relevancy": 2.0896, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5368}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.528}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defending%20Against%20Repetitive%20Backdoor%20Attacks%20on%20Semi-supervised%0A%20%20Learning%20through%20Lens%20of%20Rate-Distortion-Perception%20Trade-off&body=Title%3A%20Defending%20Against%20Repetitive%20Backdoor%20Attacks%20on%20Semi-supervised%0A%20%20Learning%20through%20Lens%20of%20Rate-Distortion-Perception%20Trade-off%0AAuthor%3A%20Cheng-Yi%20Lee%20and%20Ching-Chia%20Kao%20and%20Cheng-Han%20Yeh%20and%20Chun-Shien%20Lu%20and%20Chia-Mu%20Yu%20and%20Chu-Song%20Chen%0AAbstract%3A%20%20%20Semi-supervised%20learning%20%28SSL%29%20has%20achieved%20remarkable%20performance%20with%20a%0Asmall%20fraction%20of%20labeled%20data%20by%20leveraging%20vast%20amounts%20of%20unlabeled%20data%0Afrom%20the%20Internet.%20However%2C%20this%20large%20pool%20of%20untrusted%20data%20is%20extremely%0Avulnerable%20to%20data%20poisoning%2C%20leading%20to%20potential%20backdoor%20attacks.%20Current%0Abackdoor%20defenses%20are%20not%20yet%20effective%20against%20such%20a%20vulnerability%20in%20SSL.%20In%0Athis%20study%2C%20we%20propose%20a%20novel%20method%2C%20Unlabeled%20Data%20Purification%20%28UPure%29%2C%20to%0Adisrupt%20the%20association%20between%20trigger%20patterns%20and%20target%20classes%20by%0Aintroducing%20perturbations%20in%20the%20frequency%20domain.%20By%20leveraging%20the%0ARate-Distortion-Perception%20%28RDP%29%20trade-off%2C%20we%20further%20identify%20the%20frequency%0Aband%2C%20where%20the%20perturbations%20are%20added%2C%20and%20justify%20this%20selection.%20Notably%2C%0AUPure%20purifies%20poisoned%20unlabeled%20data%20without%20the%20need%20of%20extra%20clean%20labeled%0Adata.%20Extensive%20experiments%20on%20four%20benchmark%20datasets%20and%20five%20SSL%20algorithms%0Ademonstrate%20that%20UPure%20effectively%20reduces%20the%20attack%20success%20rate%20from%2099.78%25%0Ato%200%25%20while%20maintaining%20model%20accuracy.%20Code%20is%20available%20here%3A%0A%5Curl%7Bhttps%3A//github.com/chengyi-chris/UPure%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10180v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefending%2520Against%2520Repetitive%2520Backdoor%2520Attacks%2520on%2520Semi-supervised%250A%2520%2520Learning%2520through%2520Lens%2520of%2520Rate-Distortion-Perception%2520Trade-off%26entry.906535625%3DCheng-Yi%2520Lee%2520and%2520Ching-Chia%2520Kao%2520and%2520Cheng-Han%2520Yeh%2520and%2520Chun-Shien%2520Lu%2520and%2520Chia-Mu%2520Yu%2520and%2520Chu-Song%2520Chen%26entry.1292438233%3D%2520%2520Semi-supervised%2520learning%2520%2528SSL%2529%2520has%2520achieved%2520remarkable%2520performance%2520with%2520a%250Asmall%2520fraction%2520of%2520labeled%2520data%2520by%2520leveraging%2520vast%2520amounts%2520of%2520unlabeled%2520data%250Afrom%2520the%2520Internet.%2520However%252C%2520this%2520large%2520pool%2520of%2520untrusted%2520data%2520is%2520extremely%250Avulnerable%2520to%2520data%2520poisoning%252C%2520leading%2520to%2520potential%2520backdoor%2520attacks.%2520Current%250Abackdoor%2520defenses%2520are%2520not%2520yet%2520effective%2520against%2520such%2520a%2520vulnerability%2520in%2520SSL.%2520In%250Athis%2520study%252C%2520we%2520propose%2520a%2520novel%2520method%252C%2520Unlabeled%2520Data%2520Purification%2520%2528UPure%2529%252C%2520to%250Adisrupt%2520the%2520association%2520between%2520trigger%2520patterns%2520and%2520target%2520classes%2520by%250Aintroducing%2520perturbations%2520in%2520the%2520frequency%2520domain.%2520By%2520leveraging%2520the%250ARate-Distortion-Perception%2520%2528RDP%2529%2520trade-off%252C%2520we%2520further%2520identify%2520the%2520frequency%250Aband%252C%2520where%2520the%2520perturbations%2520are%2520added%252C%2520and%2520justify%2520this%2520selection.%2520Notably%252C%250AUPure%2520purifies%2520poisoned%2520unlabeled%2520data%2520without%2520the%2520need%2520of%2520extra%2520clean%2520labeled%250Adata.%2520Extensive%2520experiments%2520on%2520four%2520benchmark%2520datasets%2520and%2520five%2520SSL%2520algorithms%250Ademonstrate%2520that%2520UPure%2520effectively%2520reduces%2520the%2520attack%2520success%2520rate%2520from%252099.78%2525%250Ato%25200%2525%2520while%2520maintaining%2520model%2520accuracy.%2520Code%2520is%2520available%2520here%253A%250A%255Curl%257Bhttps%253A//github.com/chengyi-chris/UPure%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10180v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defending%20Against%20Repetitive%20Backdoor%20Attacks%20on%20Semi-supervised%0A%20%20Learning%20through%20Lens%20of%20Rate-Distortion-Perception%20Trade-off&entry.906535625=Cheng-Yi%20Lee%20and%20Ching-Chia%20Kao%20and%20Cheng-Han%20Yeh%20and%20Chun-Shien%20Lu%20and%20Chia-Mu%20Yu%20and%20Chu-Song%20Chen&entry.1292438233=%20%20Semi-supervised%20learning%20%28SSL%29%20has%20achieved%20remarkable%20performance%20with%20a%0Asmall%20fraction%20of%20labeled%20data%20by%20leveraging%20vast%20amounts%20of%20unlabeled%20data%0Afrom%20the%20Internet.%20However%2C%20this%20large%20pool%20of%20untrusted%20data%20is%20extremely%0Avulnerable%20to%20data%20poisoning%2C%20leading%20to%20potential%20backdoor%20attacks.%20Current%0Abackdoor%20defenses%20are%20not%20yet%20effective%20against%20such%20a%20vulnerability%20in%20SSL.%20In%0Athis%20study%2C%20we%20propose%20a%20novel%20method%2C%20Unlabeled%20Data%20Purification%20%28UPure%29%2C%20to%0Adisrupt%20the%20association%20between%20trigger%20patterns%20and%20target%20classes%20by%0Aintroducing%20perturbations%20in%20the%20frequency%20domain.%20By%20leveraging%20the%0ARate-Distortion-Perception%20%28RDP%29%20trade-off%2C%20we%20further%20identify%20the%20frequency%0Aband%2C%20where%20the%20perturbations%20are%20added%2C%20and%20justify%20this%20selection.%20Notably%2C%0AUPure%20purifies%20poisoned%20unlabeled%20data%20without%20the%20need%20of%20extra%20clean%20labeled%0Adata.%20Extensive%20experiments%20on%20four%20benchmark%20datasets%20and%20five%20SSL%20algorithms%0Ademonstrate%20that%20UPure%20effectively%20reduces%20the%20attack%20success%20rate%20from%2099.78%25%0Ato%200%25%20while%20maintaining%20model%20accuracy.%20Code%20is%20available%20here%3A%0A%5Curl%7Bhttps%3A//github.com/chengyi-chris/UPure%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10180v2&entry.124074799=Read"},
{"title": "Benchmarking Pretrained Attention-based Models for Real-Time Recognition\n  in Robot-Assisted Esophagectomy", "author": "Ronald L. P. D. de Jong and Yasmina al Khalil and Tim J. M. Jaspers and Romy C. van Jaarsveld and Gino M. Kuiper and Yiping Li and Richard van Hillegersberg and Jelle P. Ruurda and Marcel Breeuwer and Fons van der Sommen", "abstract": "  Esophageal cancer is among the most common types of cancer worldwide. It is\ntraditionally treated using open esophagectomy, but in recent years,\nrobot-assisted minimally invasive esophagectomy (RAMIE) has emerged as a\npromising alternative. However, robot-assisted surgery can be challenging for\nnovice surgeons, as they often suffer from a loss of spatial orientation.\nComputer-aided anatomy recognition holds promise for improving surgical\nnavigation, but research in this area remains limited. In this study, we\ndeveloped a comprehensive dataset for semantic segmentation in RAMIE, featuring\nthe largest collection of vital anatomical structures and surgical instruments\nto date. Handling this diverse set of classes presents challenges, including\nclass imbalance and the recognition of complex structures such as nerves. This\nstudy aims to understand the challenges and limitations of current\nstate-of-the-art algorithms on this novel dataset and problem. Therefore, we\nbenchmarked eight real-time deep learning models using two pretraining\ndatasets. We assessed both traditional and attention-based networks,\nhypothesizing that attention-based networks better capture global patterns and\naddress challenges such as occlusion caused by blood or other tissues. The\nbenchmark includes our RAMIE dataset and the publicly available CholecSeg8k\ndataset, enabling a thorough assessment of surgical segmentation tasks. Our\nfindings indicate that pretraining on ADE20k, a dataset for semantic\nsegmentation, is more effective than pretraining on ImageNet. Furthermore,\nattention-based models outperform traditional convolutional neural networks,\nwith SegNeXt and Mask2Former achieving higher Dice scores, and Mask2Former\nadditionally excelling in average symmetric surface distance.\n", "link": "http://arxiv.org/abs/2412.03401v1", "date": "2024-12-04", "relevancy": 2.0277, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Pretrained%20Attention-based%20Models%20for%20Real-Time%20Recognition%0A%20%20in%20Robot-Assisted%20Esophagectomy&body=Title%3A%20Benchmarking%20Pretrained%20Attention-based%20Models%20for%20Real-Time%20Recognition%0A%20%20in%20Robot-Assisted%20Esophagectomy%0AAuthor%3A%20Ronald%20L.%20P.%20D.%20de%20Jong%20and%20Yasmina%20al%20Khalil%20and%20Tim%20J.%20M.%20Jaspers%20and%20Romy%20C.%20van%20Jaarsveld%20and%20Gino%20M.%20Kuiper%20and%20Yiping%20Li%20and%20Richard%20van%20Hillegersberg%20and%20Jelle%20P.%20Ruurda%20and%20Marcel%20Breeuwer%20and%20Fons%20van%20der%20Sommen%0AAbstract%3A%20%20%20Esophageal%20cancer%20is%20among%20the%20most%20common%20types%20of%20cancer%20worldwide.%20It%20is%0Atraditionally%20treated%20using%20open%20esophagectomy%2C%20but%20in%20recent%20years%2C%0Arobot-assisted%20minimally%20invasive%20esophagectomy%20%28RAMIE%29%20has%20emerged%20as%20a%0Apromising%20alternative.%20However%2C%20robot-assisted%20surgery%20can%20be%20challenging%20for%0Anovice%20surgeons%2C%20as%20they%20often%20suffer%20from%20a%20loss%20of%20spatial%20orientation.%0AComputer-aided%20anatomy%20recognition%20holds%20promise%20for%20improving%20surgical%0Anavigation%2C%20but%20research%20in%20this%20area%20remains%20limited.%20In%20this%20study%2C%20we%0Adeveloped%20a%20comprehensive%20dataset%20for%20semantic%20segmentation%20in%20RAMIE%2C%20featuring%0Athe%20largest%20collection%20of%20vital%20anatomical%20structures%20and%20surgical%20instruments%0Ato%20date.%20Handling%20this%20diverse%20set%20of%20classes%20presents%20challenges%2C%20including%0Aclass%20imbalance%20and%20the%20recognition%20of%20complex%20structures%20such%20as%20nerves.%20This%0Astudy%20aims%20to%20understand%20the%20challenges%20and%20limitations%20of%20current%0Astate-of-the-art%20algorithms%20on%20this%20novel%20dataset%20and%20problem.%20Therefore%2C%20we%0Abenchmarked%20eight%20real-time%20deep%20learning%20models%20using%20two%20pretraining%0Adatasets.%20We%20assessed%20both%20traditional%20and%20attention-based%20networks%2C%0Ahypothesizing%20that%20attention-based%20networks%20better%20capture%20global%20patterns%20and%0Aaddress%20challenges%20such%20as%20occlusion%20caused%20by%20blood%20or%20other%20tissues.%20The%0Abenchmark%20includes%20our%20RAMIE%20dataset%20and%20the%20publicly%20available%20CholecSeg8k%0Adataset%2C%20enabling%20a%20thorough%20assessment%20of%20surgical%20segmentation%20tasks.%20Our%0Afindings%20indicate%20that%20pretraining%20on%20ADE20k%2C%20a%20dataset%20for%20semantic%0Asegmentation%2C%20is%20more%20effective%20than%20pretraining%20on%20ImageNet.%20Furthermore%2C%0Aattention-based%20models%20outperform%20traditional%20convolutional%20neural%20networks%2C%0Awith%20SegNeXt%20and%20Mask2Former%20achieving%20higher%20Dice%20scores%2C%20and%20Mask2Former%0Aadditionally%20excelling%20in%20average%20symmetric%20surface%20distance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Pretrained%2520Attention-based%2520Models%2520for%2520Real-Time%2520Recognition%250A%2520%2520in%2520Robot-Assisted%2520Esophagectomy%26entry.906535625%3DRonald%2520L.%2520P.%2520D.%2520de%2520Jong%2520and%2520Yasmina%2520al%2520Khalil%2520and%2520Tim%2520J.%2520M.%2520Jaspers%2520and%2520Romy%2520C.%2520van%2520Jaarsveld%2520and%2520Gino%2520M.%2520Kuiper%2520and%2520Yiping%2520Li%2520and%2520Richard%2520van%2520Hillegersberg%2520and%2520Jelle%2520P.%2520Ruurda%2520and%2520Marcel%2520Breeuwer%2520and%2520Fons%2520van%2520der%2520Sommen%26entry.1292438233%3D%2520%2520Esophageal%2520cancer%2520is%2520among%2520the%2520most%2520common%2520types%2520of%2520cancer%2520worldwide.%2520It%2520is%250Atraditionally%2520treated%2520using%2520open%2520esophagectomy%252C%2520but%2520in%2520recent%2520years%252C%250Arobot-assisted%2520minimally%2520invasive%2520esophagectomy%2520%2528RAMIE%2529%2520has%2520emerged%2520as%2520a%250Apromising%2520alternative.%2520However%252C%2520robot-assisted%2520surgery%2520can%2520be%2520challenging%2520for%250Anovice%2520surgeons%252C%2520as%2520they%2520often%2520suffer%2520from%2520a%2520loss%2520of%2520spatial%2520orientation.%250AComputer-aided%2520anatomy%2520recognition%2520holds%2520promise%2520for%2520improving%2520surgical%250Anavigation%252C%2520but%2520research%2520in%2520this%2520area%2520remains%2520limited.%2520In%2520this%2520study%252C%2520we%250Adeveloped%2520a%2520comprehensive%2520dataset%2520for%2520semantic%2520segmentation%2520in%2520RAMIE%252C%2520featuring%250Athe%2520largest%2520collection%2520of%2520vital%2520anatomical%2520structures%2520and%2520surgical%2520instruments%250Ato%2520date.%2520Handling%2520this%2520diverse%2520set%2520of%2520classes%2520presents%2520challenges%252C%2520including%250Aclass%2520imbalance%2520and%2520the%2520recognition%2520of%2520complex%2520structures%2520such%2520as%2520nerves.%2520This%250Astudy%2520aims%2520to%2520understand%2520the%2520challenges%2520and%2520limitations%2520of%2520current%250Astate-of-the-art%2520algorithms%2520on%2520this%2520novel%2520dataset%2520and%2520problem.%2520Therefore%252C%2520we%250Abenchmarked%2520eight%2520real-time%2520deep%2520learning%2520models%2520using%2520two%2520pretraining%250Adatasets.%2520We%2520assessed%2520both%2520traditional%2520and%2520attention-based%2520networks%252C%250Ahypothesizing%2520that%2520attention-based%2520networks%2520better%2520capture%2520global%2520patterns%2520and%250Aaddress%2520challenges%2520such%2520as%2520occlusion%2520caused%2520by%2520blood%2520or%2520other%2520tissues.%2520The%250Abenchmark%2520includes%2520our%2520RAMIE%2520dataset%2520and%2520the%2520publicly%2520available%2520CholecSeg8k%250Adataset%252C%2520enabling%2520a%2520thorough%2520assessment%2520of%2520surgical%2520segmentation%2520tasks.%2520Our%250Afindings%2520indicate%2520that%2520pretraining%2520on%2520ADE20k%252C%2520a%2520dataset%2520for%2520semantic%250Asegmentation%252C%2520is%2520more%2520effective%2520than%2520pretraining%2520on%2520ImageNet.%2520Furthermore%252C%250Aattention-based%2520models%2520outperform%2520traditional%2520convolutional%2520neural%2520networks%252C%250Awith%2520SegNeXt%2520and%2520Mask2Former%2520achieving%2520higher%2520Dice%2520scores%252C%2520and%2520Mask2Former%250Aadditionally%2520excelling%2520in%2520average%2520symmetric%2520surface%2520distance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Pretrained%20Attention-based%20Models%20for%20Real-Time%20Recognition%0A%20%20in%20Robot-Assisted%20Esophagectomy&entry.906535625=Ronald%20L.%20P.%20D.%20de%20Jong%20and%20Yasmina%20al%20Khalil%20and%20Tim%20J.%20M.%20Jaspers%20and%20Romy%20C.%20van%20Jaarsveld%20and%20Gino%20M.%20Kuiper%20and%20Yiping%20Li%20and%20Richard%20van%20Hillegersberg%20and%20Jelle%20P.%20Ruurda%20and%20Marcel%20Breeuwer%20and%20Fons%20van%20der%20Sommen&entry.1292438233=%20%20Esophageal%20cancer%20is%20among%20the%20most%20common%20types%20of%20cancer%20worldwide.%20It%20is%0Atraditionally%20treated%20using%20open%20esophagectomy%2C%20but%20in%20recent%20years%2C%0Arobot-assisted%20minimally%20invasive%20esophagectomy%20%28RAMIE%29%20has%20emerged%20as%20a%0Apromising%20alternative.%20However%2C%20robot-assisted%20surgery%20can%20be%20challenging%20for%0Anovice%20surgeons%2C%20as%20they%20often%20suffer%20from%20a%20loss%20of%20spatial%20orientation.%0AComputer-aided%20anatomy%20recognition%20holds%20promise%20for%20improving%20surgical%0Anavigation%2C%20but%20research%20in%20this%20area%20remains%20limited.%20In%20this%20study%2C%20we%0Adeveloped%20a%20comprehensive%20dataset%20for%20semantic%20segmentation%20in%20RAMIE%2C%20featuring%0Athe%20largest%20collection%20of%20vital%20anatomical%20structures%20and%20surgical%20instruments%0Ato%20date.%20Handling%20this%20diverse%20set%20of%20classes%20presents%20challenges%2C%20including%0Aclass%20imbalance%20and%20the%20recognition%20of%20complex%20structures%20such%20as%20nerves.%20This%0Astudy%20aims%20to%20understand%20the%20challenges%20and%20limitations%20of%20current%0Astate-of-the-art%20algorithms%20on%20this%20novel%20dataset%20and%20problem.%20Therefore%2C%20we%0Abenchmarked%20eight%20real-time%20deep%20learning%20models%20using%20two%20pretraining%0Adatasets.%20We%20assessed%20both%20traditional%20and%20attention-based%20networks%2C%0Ahypothesizing%20that%20attention-based%20networks%20better%20capture%20global%20patterns%20and%0Aaddress%20challenges%20such%20as%20occlusion%20caused%20by%20blood%20or%20other%20tissues.%20The%0Abenchmark%20includes%20our%20RAMIE%20dataset%20and%20the%20publicly%20available%20CholecSeg8k%0Adataset%2C%20enabling%20a%20thorough%20assessment%20of%20surgical%20segmentation%20tasks.%20Our%0Afindings%20indicate%20that%20pretraining%20on%20ADE20k%2C%20a%20dataset%20for%20semantic%0Asegmentation%2C%20is%20more%20effective%20than%20pretraining%20on%20ImageNet.%20Furthermore%2C%0Aattention-based%20models%20outperform%20traditional%20convolutional%20neural%20networks%2C%0Awith%20SegNeXt%20and%20Mask2Former%20achieving%20higher%20Dice%20scores%2C%20and%20Mask2Former%0Aadditionally%20excelling%20in%20average%20symmetric%20surface%20distance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03401v1&entry.124074799=Read"},
{"title": "Composed Image Retrieval for Training-Free Domain Conversion", "author": "Nikos Efthymiadis and Bill Psomas and Zakaria Laskar and Konstantinos Karantzalos and Yannis Avrithis and Ond\u0159ej Chum and Giorgos Tolias", "abstract": "  This work addresses composed image retrieval in the context of domain\nconversion, where the content of a query image is retrieved in the domain\nspecified by the query text. We show that a strong vision-language model\nprovides sufficient descriptive power without additional training. The query\nimage is mapped to the text input space using textual inversion. Unlike common\npractice that invert in the continuous space of text tokens, we use the\ndiscrete word space via a nearest-neighbor search in a text vocabulary. With\nthis inversion, the image is softly mapped across the vocabulary and is made\nmore robust using retrieval-based augmentation. Database images are retrieved\nby a weighted ensemble of text queries combining mapped words with the domain\ntext. Our method outperforms prior art by a large margin on standard and newly\nintroduced benchmarks. Code: https://github.com/NikosEfth/freedom\n", "link": "http://arxiv.org/abs/2412.03297v1", "date": "2024-12-04", "relevancy": 1.5387, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5191}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.512}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Composed%20Image%20Retrieval%20for%20Training-Free%20Domain%20Conversion&body=Title%3A%20Composed%20Image%20Retrieval%20for%20Training-Free%20Domain%20Conversion%0AAuthor%3A%20Nikos%20Efthymiadis%20and%20Bill%20Psomas%20and%20Zakaria%20Laskar%20and%20Konstantinos%20Karantzalos%20and%20Yannis%20Avrithis%20and%20Ond%C5%99ej%20Chum%20and%20Giorgos%20Tolias%0AAbstract%3A%20%20%20This%20work%20addresses%20composed%20image%20retrieval%20in%20the%20context%20of%20domain%0Aconversion%2C%20where%20the%20content%20of%20a%20query%20image%20is%20retrieved%20in%20the%20domain%0Aspecified%20by%20the%20query%20text.%20We%20show%20that%20a%20strong%20vision-language%20model%0Aprovides%20sufficient%20descriptive%20power%20without%20additional%20training.%20The%20query%0Aimage%20is%20mapped%20to%20the%20text%20input%20space%20using%20textual%20inversion.%20Unlike%20common%0Apractice%20that%20invert%20in%20the%20continuous%20space%20of%20text%20tokens%2C%20we%20use%20the%0Adiscrete%20word%20space%20via%20a%20nearest-neighbor%20search%20in%20a%20text%20vocabulary.%20With%0Athis%20inversion%2C%20the%20image%20is%20softly%20mapped%20across%20the%20vocabulary%20and%20is%20made%0Amore%20robust%20using%20retrieval-based%20augmentation.%20Database%20images%20are%20retrieved%0Aby%20a%20weighted%20ensemble%20of%20text%20queries%20combining%20mapped%20words%20with%20the%20domain%0Atext.%20Our%20method%20outperforms%20prior%20art%20by%20a%20large%20margin%20on%20standard%20and%20newly%0Aintroduced%20benchmarks.%20Code%3A%20https%3A//github.com/NikosEfth/freedom%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComposed%2520Image%2520Retrieval%2520for%2520Training-Free%2520Domain%2520Conversion%26entry.906535625%3DNikos%2520Efthymiadis%2520and%2520Bill%2520Psomas%2520and%2520Zakaria%2520Laskar%2520and%2520Konstantinos%2520Karantzalos%2520and%2520Yannis%2520Avrithis%2520and%2520Ond%25C5%2599ej%2520Chum%2520and%2520Giorgos%2520Tolias%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520composed%2520image%2520retrieval%2520in%2520the%2520context%2520of%2520domain%250Aconversion%252C%2520where%2520the%2520content%2520of%2520a%2520query%2520image%2520is%2520retrieved%2520in%2520the%2520domain%250Aspecified%2520by%2520the%2520query%2520text.%2520We%2520show%2520that%2520a%2520strong%2520vision-language%2520model%250Aprovides%2520sufficient%2520descriptive%2520power%2520without%2520additional%2520training.%2520The%2520query%250Aimage%2520is%2520mapped%2520to%2520the%2520text%2520input%2520space%2520using%2520textual%2520inversion.%2520Unlike%2520common%250Apractice%2520that%2520invert%2520in%2520the%2520continuous%2520space%2520of%2520text%2520tokens%252C%2520we%2520use%2520the%250Adiscrete%2520word%2520space%2520via%2520a%2520nearest-neighbor%2520search%2520in%2520a%2520text%2520vocabulary.%2520With%250Athis%2520inversion%252C%2520the%2520image%2520is%2520softly%2520mapped%2520across%2520the%2520vocabulary%2520and%2520is%2520made%250Amore%2520robust%2520using%2520retrieval-based%2520augmentation.%2520Database%2520images%2520are%2520retrieved%250Aby%2520a%2520weighted%2520ensemble%2520of%2520text%2520queries%2520combining%2520mapped%2520words%2520with%2520the%2520domain%250Atext.%2520Our%2520method%2520outperforms%2520prior%2520art%2520by%2520a%2520large%2520margin%2520on%2520standard%2520and%2520newly%250Aintroduced%2520benchmarks.%2520Code%253A%2520https%253A//github.com/NikosEfth/freedom%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Composed%20Image%20Retrieval%20for%20Training-Free%20Domain%20Conversion&entry.906535625=Nikos%20Efthymiadis%20and%20Bill%20Psomas%20and%20Zakaria%20Laskar%20and%20Konstantinos%20Karantzalos%20and%20Yannis%20Avrithis%20and%20Ond%C5%99ej%20Chum%20and%20Giorgos%20Tolias&entry.1292438233=%20%20This%20work%20addresses%20composed%20image%20retrieval%20in%20the%20context%20of%20domain%0Aconversion%2C%20where%20the%20content%20of%20a%20query%20image%20is%20retrieved%20in%20the%20domain%0Aspecified%20by%20the%20query%20text.%20We%20show%20that%20a%20strong%20vision-language%20model%0Aprovides%20sufficient%20descriptive%20power%20without%20additional%20training.%20The%20query%0Aimage%20is%20mapped%20to%20the%20text%20input%20space%20using%20textual%20inversion.%20Unlike%20common%0Apractice%20that%20invert%20in%20the%20continuous%20space%20of%20text%20tokens%2C%20we%20use%20the%0Adiscrete%20word%20space%20via%20a%20nearest-neighbor%20search%20in%20a%20text%20vocabulary.%20With%0Athis%20inversion%2C%20the%20image%20is%20softly%20mapped%20across%20the%20vocabulary%20and%20is%20made%0Amore%20robust%20using%20retrieval-based%20augmentation.%20Database%20images%20are%20retrieved%0Aby%20a%20weighted%20ensemble%20of%20text%20queries%20combining%20mapped%20words%20with%20the%20domain%0Atext.%20Our%20method%20outperforms%20prior%20art%20by%20a%20large%20margin%20on%20standard%20and%20newly%0Aintroduced%20benchmarks.%20Code%3A%20https%3A//github.com/NikosEfth/freedom%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03297v1&entry.124074799=Read"},
{"title": "Design and Control of an Ultra-Slender Push-Pull Multisection Continuum\n  Manipulator for In-Situ Inspection of Aeroengine", "author": "Weiheng Zhong and Yuancan Huang and Da Hong and Nianfeng Shao", "abstract": "  Since the shape of industrial endoscopes is passively altered according to\nthe contact around it, manual inspection approaches of aeroengines through the\ninspection ports have unreachable areas, and it's difficult to traverse\nmultistage blades and inspect them simultaneously, which requires engine\ndisassembly or the cooperation of multiple operators, resulting in efficiency\ndecline and increased costs. To this end, this paper proposes a novel continuum\nmanipulator with push-pull multisection structure which provides a potential\nsolution for the disadvantages mentioned above due to its higher flexibility,\npassability, and controllability in confined spaces. The ultra-slender design\ncombined with a tendon-driven mechanism makes the manipulator acquire enough\nworkspace and more flexible postures while maintaining a light weight.\nConsidering the coupling between the tendons in multisection, a innovative\nkinematics decoupling control method is implemented, which can realize\nreal-time control in the case of limited computational resources. A prototype\nis built to validate the capabilities of mechatronic design and the performance\nof the control algorithm. The experimental results demonstrate the advantages\nof our continuum manipulator in the in-situ inspection of aeroengines'\nmultistage blades, which has the potential to be a replacement solution for\nindustrial endoscopes.\n", "link": "http://arxiv.org/abs/2412.03508v1", "date": "2024-12-04", "relevancy": 1.9289, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5092}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4847}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Design%20and%20Control%20of%20an%20Ultra-Slender%20Push-Pull%20Multisection%20Continuum%0A%20%20Manipulator%20for%20In-Situ%20Inspection%20of%20Aeroengine&body=Title%3A%20Design%20and%20Control%20of%20an%20Ultra-Slender%20Push-Pull%20Multisection%20Continuum%0A%20%20Manipulator%20for%20In-Situ%20Inspection%20of%20Aeroengine%0AAuthor%3A%20Weiheng%20Zhong%20and%20Yuancan%20Huang%20and%20Da%20Hong%20and%20Nianfeng%20Shao%0AAbstract%3A%20%20%20Since%20the%20shape%20of%20industrial%20endoscopes%20is%20passively%20altered%20according%20to%0Athe%20contact%20around%20it%2C%20manual%20inspection%20approaches%20of%20aeroengines%20through%20the%0Ainspection%20ports%20have%20unreachable%20areas%2C%20and%20it%27s%20difficult%20to%20traverse%0Amultistage%20blades%20and%20inspect%20them%20simultaneously%2C%20which%20requires%20engine%0Adisassembly%20or%20the%20cooperation%20of%20multiple%20operators%2C%20resulting%20in%20efficiency%0Adecline%20and%20increased%20costs.%20To%20this%20end%2C%20this%20paper%20proposes%20a%20novel%20continuum%0Amanipulator%20with%20push-pull%20multisection%20structure%20which%20provides%20a%20potential%0Asolution%20for%20the%20disadvantages%20mentioned%20above%20due%20to%20its%20higher%20flexibility%2C%0Apassability%2C%20and%20controllability%20in%20confined%20spaces.%20The%20ultra-slender%20design%0Acombined%20with%20a%20tendon-driven%20mechanism%20makes%20the%20manipulator%20acquire%20enough%0Aworkspace%20and%20more%20flexible%20postures%20while%20maintaining%20a%20light%20weight.%0AConsidering%20the%20coupling%20between%20the%20tendons%20in%20multisection%2C%20a%20innovative%0Akinematics%20decoupling%20control%20method%20is%20implemented%2C%20which%20can%20realize%0Areal-time%20control%20in%20the%20case%20of%20limited%20computational%20resources.%20A%20prototype%0Ais%20built%20to%20validate%20the%20capabilities%20of%20mechatronic%20design%20and%20the%20performance%0Aof%20the%20control%20algorithm.%20The%20experimental%20results%20demonstrate%20the%20advantages%0Aof%20our%20continuum%20manipulator%20in%20the%20in-situ%20inspection%20of%20aeroengines%27%0Amultistage%20blades%2C%20which%20has%20the%20potential%20to%20be%20a%20replacement%20solution%20for%0Aindustrial%20endoscopes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesign%2520and%2520Control%2520of%2520an%2520Ultra-Slender%2520Push-Pull%2520Multisection%2520Continuum%250A%2520%2520Manipulator%2520for%2520In-Situ%2520Inspection%2520of%2520Aeroengine%26entry.906535625%3DWeiheng%2520Zhong%2520and%2520Yuancan%2520Huang%2520and%2520Da%2520Hong%2520and%2520Nianfeng%2520Shao%26entry.1292438233%3D%2520%2520Since%2520the%2520shape%2520of%2520industrial%2520endoscopes%2520is%2520passively%2520altered%2520according%2520to%250Athe%2520contact%2520around%2520it%252C%2520manual%2520inspection%2520approaches%2520of%2520aeroengines%2520through%2520the%250Ainspection%2520ports%2520have%2520unreachable%2520areas%252C%2520and%2520it%2527s%2520difficult%2520to%2520traverse%250Amultistage%2520blades%2520and%2520inspect%2520them%2520simultaneously%252C%2520which%2520requires%2520engine%250Adisassembly%2520or%2520the%2520cooperation%2520of%2520multiple%2520operators%252C%2520resulting%2520in%2520efficiency%250Adecline%2520and%2520increased%2520costs.%2520To%2520this%2520end%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520continuum%250Amanipulator%2520with%2520push-pull%2520multisection%2520structure%2520which%2520provides%2520a%2520potential%250Asolution%2520for%2520the%2520disadvantages%2520mentioned%2520above%2520due%2520to%2520its%2520higher%2520flexibility%252C%250Apassability%252C%2520and%2520controllability%2520in%2520confined%2520spaces.%2520The%2520ultra-slender%2520design%250Acombined%2520with%2520a%2520tendon-driven%2520mechanism%2520makes%2520the%2520manipulator%2520acquire%2520enough%250Aworkspace%2520and%2520more%2520flexible%2520postures%2520while%2520maintaining%2520a%2520light%2520weight.%250AConsidering%2520the%2520coupling%2520between%2520the%2520tendons%2520in%2520multisection%252C%2520a%2520innovative%250Akinematics%2520decoupling%2520control%2520method%2520is%2520implemented%252C%2520which%2520can%2520realize%250Areal-time%2520control%2520in%2520the%2520case%2520of%2520limited%2520computational%2520resources.%2520A%2520prototype%250Ais%2520built%2520to%2520validate%2520the%2520capabilities%2520of%2520mechatronic%2520design%2520and%2520the%2520performance%250Aof%2520the%2520control%2520algorithm.%2520The%2520experimental%2520results%2520demonstrate%2520the%2520advantages%250Aof%2520our%2520continuum%2520manipulator%2520in%2520the%2520in-situ%2520inspection%2520of%2520aeroengines%2527%250Amultistage%2520blades%252C%2520which%2520has%2520the%2520potential%2520to%2520be%2520a%2520replacement%2520solution%2520for%250Aindustrial%2520endoscopes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%20and%20Control%20of%20an%20Ultra-Slender%20Push-Pull%20Multisection%20Continuum%0A%20%20Manipulator%20for%20In-Situ%20Inspection%20of%20Aeroengine&entry.906535625=Weiheng%20Zhong%20and%20Yuancan%20Huang%20and%20Da%20Hong%20and%20Nianfeng%20Shao&entry.1292438233=%20%20Since%20the%20shape%20of%20industrial%20endoscopes%20is%20passively%20altered%20according%20to%0Athe%20contact%20around%20it%2C%20manual%20inspection%20approaches%20of%20aeroengines%20through%20the%0Ainspection%20ports%20have%20unreachable%20areas%2C%20and%20it%27s%20difficult%20to%20traverse%0Amultistage%20blades%20and%20inspect%20them%20simultaneously%2C%20which%20requires%20engine%0Adisassembly%20or%20the%20cooperation%20of%20multiple%20operators%2C%20resulting%20in%20efficiency%0Adecline%20and%20increased%20costs.%20To%20this%20end%2C%20this%20paper%20proposes%20a%20novel%20continuum%0Amanipulator%20with%20push-pull%20multisection%20structure%20which%20provides%20a%20potential%0Asolution%20for%20the%20disadvantages%20mentioned%20above%20due%20to%20its%20higher%20flexibility%2C%0Apassability%2C%20and%20controllability%20in%20confined%20spaces.%20The%20ultra-slender%20design%0Acombined%20with%20a%20tendon-driven%20mechanism%20makes%20the%20manipulator%20acquire%20enough%0Aworkspace%20and%20more%20flexible%20postures%20while%20maintaining%20a%20light%20weight.%0AConsidering%20the%20coupling%20between%20the%20tendons%20in%20multisection%2C%20a%20innovative%0Akinematics%20decoupling%20control%20method%20is%20implemented%2C%20which%20can%20realize%0Areal-time%20control%20in%20the%20case%20of%20limited%20computational%20resources.%20A%20prototype%0Ais%20built%20to%20validate%20the%20capabilities%20of%20mechatronic%20design%20and%20the%20performance%0Aof%20the%20control%20algorithm.%20The%20experimental%20results%20demonstrate%20the%20advantages%0Aof%20our%20continuum%20manipulator%20in%20the%20in-situ%20inspection%20of%20aeroengines%27%0Amultistage%20blades%2C%20which%20has%20the%20potential%20to%20be%20a%20replacement%20solution%20for%0Aindustrial%20endoscopes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03508v1&entry.124074799=Read"},
{"title": "Elephants Never Forget: Memorization and Learning of Tabular Data in\n  Large Language Models", "author": "Sebastian Bordt and Harsha Nori and Vanessa Rodrigues and Besmira Nushi and Rich Caruana", "abstract": "  While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Specifically, we introduce a variety of different techniques to\nassess whether a language model has seen a tabular dataset during training.\nThis investigation reveals that LLMs have memorized many popular tabular\ndatasets verbatim. We then compare the few-shot learning performance of LLMs on\ndatasets that were seen during training to the performance on datasets released\nafter training. We find that LLMs perform better on datasets seen during\ntraining, indicating that memorization leads to overfitting. At the same time,\nLLMs show non-trivial performance on novel datasets and are surprisingly robust\nto data transformations. We then investigate the in-context statistical\nlearning abilities of LLMs. While LLMs are significantly better than random at\nsolving statistical classification problems, the sample efficiency of few-shot\nlearning lags behind traditional statistical learning algorithms, especially as\nthe dimension of the problem increases. This suggests that much of the observed\nfew-shot performance on novel real-world datasets is due to the LLM's world\nknowledge. Overall, our results highlight the importance of testing whether an\nLLM has seen an evaluation dataset during pre-training. We release the\nhttps://github.com/interpretml/LLM-Tabular-Memorization-Checker Python package\nto test LLMs for memorization of tabular datasets.\n", "link": "http://arxiv.org/abs/2404.06209v3", "date": "2024-12-04", "relevancy": 1.9726, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5092}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4839}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Elephants%20Never%20Forget%3A%20Memorization%20and%20Learning%20of%20Tabular%20Data%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20Elephants%20Never%20Forget%3A%20Memorization%20and%20Learning%20of%20Tabular%20Data%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Sebastian%20Bordt%20and%20Harsha%20Nori%20and%20Vanessa%20Rodrigues%20and%20Besmira%20Nushi%20and%20Rich%20Caruana%0AAbstract%3A%20%20%20While%20many%20have%20shown%20how%20Large%20Language%20Models%20%28LLMs%29%20can%20be%20applied%20to%20a%0Adiverse%20set%20of%20tasks%2C%20the%20critical%20issues%20of%20data%20contamination%20and%0Amemorization%20are%20often%20glossed%20over.%20In%20this%20work%2C%20we%20address%20this%20concern%20for%0Atabular%20data.%20Specifically%2C%20we%20introduce%20a%20variety%20of%20different%20techniques%20to%0Aassess%20whether%20a%20language%20model%20has%20seen%20a%20tabular%20dataset%20during%20training.%0AThis%20investigation%20reveals%20that%20LLMs%20have%20memorized%20many%20popular%20tabular%0Adatasets%20verbatim.%20We%20then%20compare%20the%20few-shot%20learning%20performance%20of%20LLMs%20on%0Adatasets%20that%20were%20seen%20during%20training%20to%20the%20performance%20on%20datasets%20released%0Aafter%20training.%20We%20find%20that%20LLMs%20perform%20better%20on%20datasets%20seen%20during%0Atraining%2C%20indicating%20that%20memorization%20leads%20to%20overfitting.%20At%20the%20same%20time%2C%0ALLMs%20show%20non-trivial%20performance%20on%20novel%20datasets%20and%20are%20surprisingly%20robust%0Ato%20data%20transformations.%20We%20then%20investigate%20the%20in-context%20statistical%0Alearning%20abilities%20of%20LLMs.%20While%20LLMs%20are%20significantly%20better%20than%20random%20at%0Asolving%20statistical%20classification%20problems%2C%20the%20sample%20efficiency%20of%20few-shot%0Alearning%20lags%20behind%20traditional%20statistical%20learning%20algorithms%2C%20especially%20as%0Athe%20dimension%20of%20the%20problem%20increases.%20This%20suggests%20that%20much%20of%20the%20observed%0Afew-shot%20performance%20on%20novel%20real-world%20datasets%20is%20due%20to%20the%20LLM%27s%20world%0Aknowledge.%20Overall%2C%20our%20results%20highlight%20the%20importance%20of%20testing%20whether%20an%0ALLM%20has%20seen%20an%20evaluation%20dataset%20during%20pre-training.%20We%20release%20the%0Ahttps%3A//github.com/interpretml/LLM-Tabular-Memorization-Checker%20Python%20package%0Ato%20test%20LLMs%20for%20memorization%20of%20tabular%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06209v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DElephants%2520Never%2520Forget%253A%2520Memorization%2520and%2520Learning%2520of%2520Tabular%2520Data%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DSebastian%2520Bordt%2520and%2520Harsha%2520Nori%2520and%2520Vanessa%2520Rodrigues%2520and%2520Besmira%2520Nushi%2520and%2520Rich%2520Caruana%26entry.1292438233%3D%2520%2520While%2520many%2520have%2520shown%2520how%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520be%2520applied%2520to%2520a%250Adiverse%2520set%2520of%2520tasks%252C%2520the%2520critical%2520issues%2520of%2520data%2520contamination%2520and%250Amemorization%2520are%2520often%2520glossed%2520over.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520concern%2520for%250Atabular%2520data.%2520Specifically%252C%2520we%2520introduce%2520a%2520variety%2520of%2520different%2520techniques%2520to%250Aassess%2520whether%2520a%2520language%2520model%2520has%2520seen%2520a%2520tabular%2520dataset%2520during%2520training.%250AThis%2520investigation%2520reveals%2520that%2520LLMs%2520have%2520memorized%2520many%2520popular%2520tabular%250Adatasets%2520verbatim.%2520We%2520then%2520compare%2520the%2520few-shot%2520learning%2520performance%2520of%2520LLMs%2520on%250Adatasets%2520that%2520were%2520seen%2520during%2520training%2520to%2520the%2520performance%2520on%2520datasets%2520released%250Aafter%2520training.%2520We%2520find%2520that%2520LLMs%2520perform%2520better%2520on%2520datasets%2520seen%2520during%250Atraining%252C%2520indicating%2520that%2520memorization%2520leads%2520to%2520overfitting.%2520At%2520the%2520same%2520time%252C%250ALLMs%2520show%2520non-trivial%2520performance%2520on%2520novel%2520datasets%2520and%2520are%2520surprisingly%2520robust%250Ato%2520data%2520transformations.%2520We%2520then%2520investigate%2520the%2520in-context%2520statistical%250Alearning%2520abilities%2520of%2520LLMs.%2520While%2520LLMs%2520are%2520significantly%2520better%2520than%2520random%2520at%250Asolving%2520statistical%2520classification%2520problems%252C%2520the%2520sample%2520efficiency%2520of%2520few-shot%250Alearning%2520lags%2520behind%2520traditional%2520statistical%2520learning%2520algorithms%252C%2520especially%2520as%250Athe%2520dimension%2520of%2520the%2520problem%2520increases.%2520This%2520suggests%2520that%2520much%2520of%2520the%2520observed%250Afew-shot%2520performance%2520on%2520novel%2520real-world%2520datasets%2520is%2520due%2520to%2520the%2520LLM%2527s%2520world%250Aknowledge.%2520Overall%252C%2520our%2520results%2520highlight%2520the%2520importance%2520of%2520testing%2520whether%2520an%250ALLM%2520has%2520seen%2520an%2520evaluation%2520dataset%2520during%2520pre-training.%2520We%2520release%2520the%250Ahttps%253A//github.com/interpretml/LLM-Tabular-Memorization-Checker%2520Python%2520package%250Ato%2520test%2520LLMs%2520for%2520memorization%2520of%2520tabular%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06209v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Elephants%20Never%20Forget%3A%20Memorization%20and%20Learning%20of%20Tabular%20Data%20in%0A%20%20Large%20Language%20Models&entry.906535625=Sebastian%20Bordt%20and%20Harsha%20Nori%20and%20Vanessa%20Rodrigues%20and%20Besmira%20Nushi%20and%20Rich%20Caruana&entry.1292438233=%20%20While%20many%20have%20shown%20how%20Large%20Language%20Models%20%28LLMs%29%20can%20be%20applied%20to%20a%0Adiverse%20set%20of%20tasks%2C%20the%20critical%20issues%20of%20data%20contamination%20and%0Amemorization%20are%20often%20glossed%20over.%20In%20this%20work%2C%20we%20address%20this%20concern%20for%0Atabular%20data.%20Specifically%2C%20we%20introduce%20a%20variety%20of%20different%20techniques%20to%0Aassess%20whether%20a%20language%20model%20has%20seen%20a%20tabular%20dataset%20during%20training.%0AThis%20investigation%20reveals%20that%20LLMs%20have%20memorized%20many%20popular%20tabular%0Adatasets%20verbatim.%20We%20then%20compare%20the%20few-shot%20learning%20performance%20of%20LLMs%20on%0Adatasets%20that%20were%20seen%20during%20training%20to%20the%20performance%20on%20datasets%20released%0Aafter%20training.%20We%20find%20that%20LLMs%20perform%20better%20on%20datasets%20seen%20during%0Atraining%2C%20indicating%20that%20memorization%20leads%20to%20overfitting.%20At%20the%20same%20time%2C%0ALLMs%20show%20non-trivial%20performance%20on%20novel%20datasets%20and%20are%20surprisingly%20robust%0Ato%20data%20transformations.%20We%20then%20investigate%20the%20in-context%20statistical%0Alearning%20abilities%20of%20LLMs.%20While%20LLMs%20are%20significantly%20better%20than%20random%20at%0Asolving%20statistical%20classification%20problems%2C%20the%20sample%20efficiency%20of%20few-shot%0Alearning%20lags%20behind%20traditional%20statistical%20learning%20algorithms%2C%20especially%20as%0Athe%20dimension%20of%20the%20problem%20increases.%20This%20suggests%20that%20much%20of%20the%20observed%0Afew-shot%20performance%20on%20novel%20real-world%20datasets%20is%20due%20to%20the%20LLM%27s%20world%0Aknowledge.%20Overall%2C%20our%20results%20highlight%20the%20importance%20of%20testing%20whether%20an%0ALLM%20has%20seen%20an%20evaluation%20dataset%20during%20pre-training.%20We%20release%20the%0Ahttps%3A//github.com/interpretml/LLM-Tabular-Memorization-Checker%20Python%20package%0Ato%20test%20LLMs%20for%20memorization%20of%20tabular%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06209v3&entry.124074799=Read"},
{"title": "Contextual Data Integration for Bike-sharing Demand Prediction with\n  Graph Neural Networks in Degraded Weather Conditions", "author": "Romain Rochas and Angelo Furno and Nour-Eddin El Faouzi", "abstract": "  Demand for bike sharing is impacted by various factors, such as weather\nconditions, events, and the availability of other transportation modes. This\nimpact remains elusive due to the complex interdependence of these factors or\nlocationrelated user behavior variations. It is also not clear which factor is\nadditional information which are not already contained in the historical\ndemand. Intermodal dependencies between bike-sharing and other modes are also\nunderexplored, and the value of this information has not been studied in\ndegraded situations. The proposed study analyzes the impact of adding\ncontextual data, such as weather, time embedding, and road traffic flow, to\npredict bike-sharing Origin-Destination (OD) flows in atypical weather\nsituations Our study highlights a mild relationship between prediction quality\nof bike-sharing demand and road traffic flow, while the introduced time\nembedding allows outperforming state-of-the-art results, particularly in the\ncase of degraded weather conditions. Including weather data as an additional\ninput further improves our model with respect to the basic ST-ED-RMGC\nprediction model by reducing of more than 20% the prediction error in degraded\nweather condition.\n", "link": "http://arxiv.org/abs/2412.03307v1", "date": "2024-12-04", "relevancy": 1.4273, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5092}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5012}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Data%20Integration%20for%20Bike-sharing%20Demand%20Prediction%20with%0A%20%20Graph%20Neural%20Networks%20in%20Degraded%20Weather%20Conditions&body=Title%3A%20Contextual%20Data%20Integration%20for%20Bike-sharing%20Demand%20Prediction%20with%0A%20%20Graph%20Neural%20Networks%20in%20Degraded%20Weather%20Conditions%0AAuthor%3A%20Romain%20Rochas%20and%20Angelo%20Furno%20and%20Nour-Eddin%20El%20Faouzi%0AAbstract%3A%20%20%20Demand%20for%20bike%20sharing%20is%20impacted%20by%20various%20factors%2C%20such%20as%20weather%0Aconditions%2C%20events%2C%20and%20the%20availability%20of%20other%20transportation%20modes.%20This%0Aimpact%20remains%20elusive%20due%20to%20the%20complex%20interdependence%20of%20these%20factors%20or%0Alocationrelated%20user%20behavior%20variations.%20It%20is%20also%20not%20clear%20which%20factor%20is%0Aadditional%20information%20which%20are%20not%20already%20contained%20in%20the%20historical%0Ademand.%20Intermodal%20dependencies%20between%20bike-sharing%20and%20other%20modes%20are%20also%0Aunderexplored%2C%20and%20the%20value%20of%20this%20information%20has%20not%20been%20studied%20in%0Adegraded%20situations.%20The%20proposed%20study%20analyzes%20the%20impact%20of%20adding%0Acontextual%20data%2C%20such%20as%20weather%2C%20time%20embedding%2C%20and%20road%20traffic%20flow%2C%20to%0Apredict%20bike-sharing%20Origin-Destination%20%28OD%29%20flows%20in%20atypical%20weather%0Asituations%20Our%20study%20highlights%20a%20mild%20relationship%20between%20prediction%20quality%0Aof%20bike-sharing%20demand%20and%20road%20traffic%20flow%2C%20while%20the%20introduced%20time%0Aembedding%20allows%20outperforming%20state-of-the-art%20results%2C%20particularly%20in%20the%0Acase%20of%20degraded%20weather%20conditions.%20Including%20weather%20data%20as%20an%20additional%0Ainput%20further%20improves%20our%20model%20with%20respect%20to%20the%20basic%20ST-ED-RMGC%0Aprediction%20model%20by%20reducing%20of%20more%20than%2020%25%20the%20prediction%20error%20in%20degraded%0Aweather%20condition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Data%2520Integration%2520for%2520Bike-sharing%2520Demand%2520Prediction%2520with%250A%2520%2520Graph%2520Neural%2520Networks%2520in%2520Degraded%2520Weather%2520Conditions%26entry.906535625%3DRomain%2520Rochas%2520and%2520Angelo%2520Furno%2520and%2520Nour-Eddin%2520El%2520Faouzi%26entry.1292438233%3D%2520%2520Demand%2520for%2520bike%2520sharing%2520is%2520impacted%2520by%2520various%2520factors%252C%2520such%2520as%2520weather%250Aconditions%252C%2520events%252C%2520and%2520the%2520availability%2520of%2520other%2520transportation%2520modes.%2520This%250Aimpact%2520remains%2520elusive%2520due%2520to%2520the%2520complex%2520interdependence%2520of%2520these%2520factors%2520or%250Alocationrelated%2520user%2520behavior%2520variations.%2520It%2520is%2520also%2520not%2520clear%2520which%2520factor%2520is%250Aadditional%2520information%2520which%2520are%2520not%2520already%2520contained%2520in%2520the%2520historical%250Ademand.%2520Intermodal%2520dependencies%2520between%2520bike-sharing%2520and%2520other%2520modes%2520are%2520also%250Aunderexplored%252C%2520and%2520the%2520value%2520of%2520this%2520information%2520has%2520not%2520been%2520studied%2520in%250Adegraded%2520situations.%2520The%2520proposed%2520study%2520analyzes%2520the%2520impact%2520of%2520adding%250Acontextual%2520data%252C%2520such%2520as%2520weather%252C%2520time%2520embedding%252C%2520and%2520road%2520traffic%2520flow%252C%2520to%250Apredict%2520bike-sharing%2520Origin-Destination%2520%2528OD%2529%2520flows%2520in%2520atypical%2520weather%250Asituations%2520Our%2520study%2520highlights%2520a%2520mild%2520relationship%2520between%2520prediction%2520quality%250Aof%2520bike-sharing%2520demand%2520and%2520road%2520traffic%2520flow%252C%2520while%2520the%2520introduced%2520time%250Aembedding%2520allows%2520outperforming%2520state-of-the-art%2520results%252C%2520particularly%2520in%2520the%250Acase%2520of%2520degraded%2520weather%2520conditions.%2520Including%2520weather%2520data%2520as%2520an%2520additional%250Ainput%2520further%2520improves%2520our%2520model%2520with%2520respect%2520to%2520the%2520basic%2520ST-ED-RMGC%250Aprediction%2520model%2520by%2520reducing%2520of%2520more%2520than%252020%2525%2520the%2520prediction%2520error%2520in%2520degraded%250Aweather%2520condition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Data%20Integration%20for%20Bike-sharing%20Demand%20Prediction%20with%0A%20%20Graph%20Neural%20Networks%20in%20Degraded%20Weather%20Conditions&entry.906535625=Romain%20Rochas%20and%20Angelo%20Furno%20and%20Nour-Eddin%20El%20Faouzi&entry.1292438233=%20%20Demand%20for%20bike%20sharing%20is%20impacted%20by%20various%20factors%2C%20such%20as%20weather%0Aconditions%2C%20events%2C%20and%20the%20availability%20of%20other%20transportation%20modes.%20This%0Aimpact%20remains%20elusive%20due%20to%20the%20complex%20interdependence%20of%20these%20factors%20or%0Alocationrelated%20user%20behavior%20variations.%20It%20is%20also%20not%20clear%20which%20factor%20is%0Aadditional%20information%20which%20are%20not%20already%20contained%20in%20the%20historical%0Ademand.%20Intermodal%20dependencies%20between%20bike-sharing%20and%20other%20modes%20are%20also%0Aunderexplored%2C%20and%20the%20value%20of%20this%20information%20has%20not%20been%20studied%20in%0Adegraded%20situations.%20The%20proposed%20study%20analyzes%20the%20impact%20of%20adding%0Acontextual%20data%2C%20such%20as%20weather%2C%20time%20embedding%2C%20and%20road%20traffic%20flow%2C%20to%0Apredict%20bike-sharing%20Origin-Destination%20%28OD%29%20flows%20in%20atypical%20weather%0Asituations%20Our%20study%20highlights%20a%20mild%20relationship%20between%20prediction%20quality%0Aof%20bike-sharing%20demand%20and%20road%20traffic%20flow%2C%20while%20the%20introduced%20time%0Aembedding%20allows%20outperforming%20state-of-the-art%20results%2C%20particularly%20in%20the%0Acase%20of%20degraded%20weather%20conditions.%20Including%20weather%20data%20as%20an%20additional%0Ainput%20further%20improves%20our%20model%20with%20respect%20to%20the%20basic%20ST-ED-RMGC%0Aprediction%20model%20by%20reducing%20of%20more%20than%2020%25%20the%20prediction%20error%20in%20degraded%0Aweather%20condition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03307v1&entry.124074799=Read"},
{"title": "Reactive Orchestration for Hierarchical Federated Learning Under a\n  Communication Cost Budget", "author": "Ivan \u010cili\u0107 and Anna Lackinger and Pantelis Frangoudis and Ivana Podnar \u017darko and Alireza Furutanpey and Ilir Murturi and Schahram Dustdar", "abstract": "  Deploying a Hierarchical Federated Learning (HFL) pipeline across the\ncomputing continuum (CC) requires careful organization of participants into a\nhierarchical structure with intermediate aggregation nodes between FL clients\nand the global FL server. This is challenging to achieve due to (i) cost\nconstraints, (ii) varying data distributions, and (iii) the volatile operating\nenvironment of the CC. In response to these challenges, we present a framework\nfor the adaptive orchestration of HFL pipelines, designed to be reactive to\nclient churn and infrastructure-level events, while balancing communication\ncost and ML model accuracy. Our mechanisms identify and react to events that\ncause HFL reconfiguration actions at runtime, building on multi-level\nmonitoring information (model accuracy, resource availability, resource cost).\nMoreover, our framework introduces a generic methodology for estimating\nreconfiguration costs to continuously re-evaluate the quality of adaptation\nactions, while being extensible to optimize for various HFL performance\ncriteria. By extending the Kubernetes ecosystem, our framework demonstrates the\nability to react promptly and effectively to changes in the operating\nenvironment, making the best of the available communication cost budget and\neffectively balancing costs and ML performance at runtime.\n", "link": "http://arxiv.org/abs/2412.03385v1", "date": "2024-12-04", "relevancy": 1.7333, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4376}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4338}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reactive%20Orchestration%20for%20Hierarchical%20Federated%20Learning%20Under%20a%0A%20%20Communication%20Cost%20Budget&body=Title%3A%20Reactive%20Orchestration%20for%20Hierarchical%20Federated%20Learning%20Under%20a%0A%20%20Communication%20Cost%20Budget%0AAuthor%3A%20Ivan%20%C4%8Cili%C4%87%20and%20Anna%20Lackinger%20and%20Pantelis%20Frangoudis%20and%20Ivana%20Podnar%20%C5%BDarko%20and%20Alireza%20Furutanpey%20and%20Ilir%20Murturi%20and%20Schahram%20Dustdar%0AAbstract%3A%20%20%20Deploying%20a%20Hierarchical%20Federated%20Learning%20%28HFL%29%20pipeline%20across%20the%0Acomputing%20continuum%20%28CC%29%20requires%20careful%20organization%20of%20participants%20into%20a%0Ahierarchical%20structure%20with%20intermediate%20aggregation%20nodes%20between%20FL%20clients%0Aand%20the%20global%20FL%20server.%20This%20is%20challenging%20to%20achieve%20due%20to%20%28i%29%20cost%0Aconstraints%2C%20%28ii%29%20varying%20data%20distributions%2C%20and%20%28iii%29%20the%20volatile%20operating%0Aenvironment%20of%20the%20CC.%20In%20response%20to%20these%20challenges%2C%20we%20present%20a%20framework%0Afor%20the%20adaptive%20orchestration%20of%20HFL%20pipelines%2C%20designed%20to%20be%20reactive%20to%0Aclient%20churn%20and%20infrastructure-level%20events%2C%20while%20balancing%20communication%0Acost%20and%20ML%20model%20accuracy.%20Our%20mechanisms%20identify%20and%20react%20to%20events%20that%0Acause%20HFL%20reconfiguration%20actions%20at%20runtime%2C%20building%20on%20multi-level%0Amonitoring%20information%20%28model%20accuracy%2C%20resource%20availability%2C%20resource%20cost%29.%0AMoreover%2C%20our%20framework%20introduces%20a%20generic%20methodology%20for%20estimating%0Areconfiguration%20costs%20to%20continuously%20re-evaluate%20the%20quality%20of%20adaptation%0Aactions%2C%20while%20being%20extensible%20to%20optimize%20for%20various%20HFL%20performance%0Acriteria.%20By%20extending%20the%20Kubernetes%20ecosystem%2C%20our%20framework%20demonstrates%20the%0Aability%20to%20react%20promptly%20and%20effectively%20to%20changes%20in%20the%20operating%0Aenvironment%2C%20making%20the%20best%20of%20the%20available%20communication%20cost%20budget%20and%0Aeffectively%20balancing%20costs%20and%20ML%20performance%20at%20runtime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReactive%2520Orchestration%2520for%2520Hierarchical%2520Federated%2520Learning%2520Under%2520a%250A%2520%2520Communication%2520Cost%2520Budget%26entry.906535625%3DIvan%2520%25C4%258Cili%25C4%2587%2520and%2520Anna%2520Lackinger%2520and%2520Pantelis%2520Frangoudis%2520and%2520Ivana%2520Podnar%2520%25C5%25BDarko%2520and%2520Alireza%2520Furutanpey%2520and%2520Ilir%2520Murturi%2520and%2520Schahram%2520Dustdar%26entry.1292438233%3D%2520%2520Deploying%2520a%2520Hierarchical%2520Federated%2520Learning%2520%2528HFL%2529%2520pipeline%2520across%2520the%250Acomputing%2520continuum%2520%2528CC%2529%2520requires%2520careful%2520organization%2520of%2520participants%2520into%2520a%250Ahierarchical%2520structure%2520with%2520intermediate%2520aggregation%2520nodes%2520between%2520FL%2520clients%250Aand%2520the%2520global%2520FL%2520server.%2520This%2520is%2520challenging%2520to%2520achieve%2520due%2520to%2520%2528i%2529%2520cost%250Aconstraints%252C%2520%2528ii%2529%2520varying%2520data%2520distributions%252C%2520and%2520%2528iii%2529%2520the%2520volatile%2520operating%250Aenvironment%2520of%2520the%2520CC.%2520In%2520response%2520to%2520these%2520challenges%252C%2520we%2520present%2520a%2520framework%250Afor%2520the%2520adaptive%2520orchestration%2520of%2520HFL%2520pipelines%252C%2520designed%2520to%2520be%2520reactive%2520to%250Aclient%2520churn%2520and%2520infrastructure-level%2520events%252C%2520while%2520balancing%2520communication%250Acost%2520and%2520ML%2520model%2520accuracy.%2520Our%2520mechanisms%2520identify%2520and%2520react%2520to%2520events%2520that%250Acause%2520HFL%2520reconfiguration%2520actions%2520at%2520runtime%252C%2520building%2520on%2520multi-level%250Amonitoring%2520information%2520%2528model%2520accuracy%252C%2520resource%2520availability%252C%2520resource%2520cost%2529.%250AMoreover%252C%2520our%2520framework%2520introduces%2520a%2520generic%2520methodology%2520for%2520estimating%250Areconfiguration%2520costs%2520to%2520continuously%2520re-evaluate%2520the%2520quality%2520of%2520adaptation%250Aactions%252C%2520while%2520being%2520extensible%2520to%2520optimize%2520for%2520various%2520HFL%2520performance%250Acriteria.%2520By%2520extending%2520the%2520Kubernetes%2520ecosystem%252C%2520our%2520framework%2520demonstrates%2520the%250Aability%2520to%2520react%2520promptly%2520and%2520effectively%2520to%2520changes%2520in%2520the%2520operating%250Aenvironment%252C%2520making%2520the%2520best%2520of%2520the%2520available%2520communication%2520cost%2520budget%2520and%250Aeffectively%2520balancing%2520costs%2520and%2520ML%2520performance%2520at%2520runtime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reactive%20Orchestration%20for%20Hierarchical%20Federated%20Learning%20Under%20a%0A%20%20Communication%20Cost%20Budget&entry.906535625=Ivan%20%C4%8Cili%C4%87%20and%20Anna%20Lackinger%20and%20Pantelis%20Frangoudis%20and%20Ivana%20Podnar%20%C5%BDarko%20and%20Alireza%20Furutanpey%20and%20Ilir%20Murturi%20and%20Schahram%20Dustdar&entry.1292438233=%20%20Deploying%20a%20Hierarchical%20Federated%20Learning%20%28HFL%29%20pipeline%20across%20the%0Acomputing%20continuum%20%28CC%29%20requires%20careful%20organization%20of%20participants%20into%20a%0Ahierarchical%20structure%20with%20intermediate%20aggregation%20nodes%20between%20FL%20clients%0Aand%20the%20global%20FL%20server.%20This%20is%20challenging%20to%20achieve%20due%20to%20%28i%29%20cost%0Aconstraints%2C%20%28ii%29%20varying%20data%20distributions%2C%20and%20%28iii%29%20the%20volatile%20operating%0Aenvironment%20of%20the%20CC.%20In%20response%20to%20these%20challenges%2C%20we%20present%20a%20framework%0Afor%20the%20adaptive%20orchestration%20of%20HFL%20pipelines%2C%20designed%20to%20be%20reactive%20to%0Aclient%20churn%20and%20infrastructure-level%20events%2C%20while%20balancing%20communication%0Acost%20and%20ML%20model%20accuracy.%20Our%20mechanisms%20identify%20and%20react%20to%20events%20that%0Acause%20HFL%20reconfiguration%20actions%20at%20runtime%2C%20building%20on%20multi-level%0Amonitoring%20information%20%28model%20accuracy%2C%20resource%20availability%2C%20resource%20cost%29.%0AMoreover%2C%20our%20framework%20introduces%20a%20generic%20methodology%20for%20estimating%0Areconfiguration%20costs%20to%20continuously%20re-evaluate%20the%20quality%20of%20adaptation%0Aactions%2C%20while%20being%20extensible%20to%20optimize%20for%20various%20HFL%20performance%0Acriteria.%20By%20extending%20the%20Kubernetes%20ecosystem%2C%20our%20framework%20demonstrates%20the%0Aability%20to%20react%20promptly%20and%20effectively%20to%20changes%20in%20the%20operating%0Aenvironment%2C%20making%20the%20best%20of%20the%20available%20communication%20cost%20budget%20and%0Aeffectively%20balancing%20costs%20and%20ML%20performance%20at%20runtime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03385v1&entry.124074799=Read"},
{"title": "Minimal Learning Machine for Multi-Label Learning", "author": "Joonas H\u00e4m\u00e4l\u00e4inen and Antoine Hubermont and Amauri Souza and C\u00e9sar L. C. Mattos and Jo\u00e3o P. P. Gomes and Tommi K\u00e4rkk\u00e4inen", "abstract": "  Distance-based supervised method, the minimal learning machine, constructs a\npredictive model from data by learning a mapping between input and output\ndistance matrices. In this paper, we propose new methods and evaluate how their\ncore component, the distance mapping, can be adapted to multi-label learning.\nThe proposed approach is based on combining the distance mapping with an\ninverse distance weighting. Although the proposal is one of the simplest\nmethods in the multi-label learning literature, it achieves state-of-the-art\nperformance for small to moderate-sized multi-label learning problems. In\naddition to its simplicity, the proposed method is fully deterministic: Its\nhyper-parameter can be selected via ranking loss-based statistic which has a\nclosed form, thus avoiding conventional cross-validation-based hyper-parameter\ntuning. In addition, due to its simple linear distance mapping-based\nconstruction, we demonstrate that the proposed method can assess the\nuncertainty of the predictions for multi-label classification, which is a\nvaluable capability for data-centric machine learning pipelines.\n", "link": "http://arxiv.org/abs/2305.05518v2", "date": "2024-12-04", "relevancy": 1.5195, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5111}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5017}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimal%20Learning%20Machine%20for%20Multi-Label%20Learning&body=Title%3A%20Minimal%20Learning%20Machine%20for%20Multi-Label%20Learning%0AAuthor%3A%20Joonas%20H%C3%A4m%C3%A4l%C3%A4inen%20and%20Antoine%20Hubermont%20and%20Amauri%20Souza%20and%20C%C3%A9sar%20L.%20C.%20Mattos%20and%20Jo%C3%A3o%20P.%20P.%20Gomes%20and%20Tommi%20K%C3%A4rkk%C3%A4inen%0AAbstract%3A%20%20%20Distance-based%20supervised%20method%2C%20the%20minimal%20learning%20machine%2C%20constructs%20a%0Apredictive%20model%20from%20data%20by%20learning%20a%20mapping%20between%20input%20and%20output%0Adistance%20matrices.%20In%20this%20paper%2C%20we%20propose%20new%20methods%20and%20evaluate%20how%20their%0Acore%20component%2C%20the%20distance%20mapping%2C%20can%20be%20adapted%20to%20multi-label%20learning.%0AThe%20proposed%20approach%20is%20based%20on%20combining%20the%20distance%20mapping%20with%20an%0Ainverse%20distance%20weighting.%20Although%20the%20proposal%20is%20one%20of%20the%20simplest%0Amethods%20in%20the%20multi-label%20learning%20literature%2C%20it%20achieves%20state-of-the-art%0Aperformance%20for%20small%20to%20moderate-sized%20multi-label%20learning%20problems.%20In%0Aaddition%20to%20its%20simplicity%2C%20the%20proposed%20method%20is%20fully%20deterministic%3A%20Its%0Ahyper-parameter%20can%20be%20selected%20via%20ranking%20loss-based%20statistic%20which%20has%20a%0Aclosed%20form%2C%20thus%20avoiding%20conventional%20cross-validation-based%20hyper-parameter%0Atuning.%20In%20addition%2C%20due%20to%20its%20simple%20linear%20distance%20mapping-based%0Aconstruction%2C%20we%20demonstrate%20that%20the%20proposed%20method%20can%20assess%20the%0Auncertainty%20of%20the%20predictions%20for%20multi-label%20classification%2C%20which%20is%20a%0Avaluable%20capability%20for%20data-centric%20machine%20learning%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.05518v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimal%2520Learning%2520Machine%2520for%2520Multi-Label%2520Learning%26entry.906535625%3DJoonas%2520H%25C3%25A4m%25C3%25A4l%25C3%25A4inen%2520and%2520Antoine%2520Hubermont%2520and%2520Amauri%2520Souza%2520and%2520C%25C3%25A9sar%2520L.%2520C.%2520Mattos%2520and%2520Jo%25C3%25A3o%2520P.%2520P.%2520Gomes%2520and%2520Tommi%2520K%25C3%25A4rkk%25C3%25A4inen%26entry.1292438233%3D%2520%2520Distance-based%2520supervised%2520method%252C%2520the%2520minimal%2520learning%2520machine%252C%2520constructs%2520a%250Apredictive%2520model%2520from%2520data%2520by%2520learning%2520a%2520mapping%2520between%2520input%2520and%2520output%250Adistance%2520matrices.%2520In%2520this%2520paper%252C%2520we%2520propose%2520new%2520methods%2520and%2520evaluate%2520how%2520their%250Acore%2520component%252C%2520the%2520distance%2520mapping%252C%2520can%2520be%2520adapted%2520to%2520multi-label%2520learning.%250AThe%2520proposed%2520approach%2520is%2520based%2520on%2520combining%2520the%2520distance%2520mapping%2520with%2520an%250Ainverse%2520distance%2520weighting.%2520Although%2520the%2520proposal%2520is%2520one%2520of%2520the%2520simplest%250Amethods%2520in%2520the%2520multi-label%2520learning%2520literature%252C%2520it%2520achieves%2520state-of-the-art%250Aperformance%2520for%2520small%2520to%2520moderate-sized%2520multi-label%2520learning%2520problems.%2520In%250Aaddition%2520to%2520its%2520simplicity%252C%2520the%2520proposed%2520method%2520is%2520fully%2520deterministic%253A%2520Its%250Ahyper-parameter%2520can%2520be%2520selected%2520via%2520ranking%2520loss-based%2520statistic%2520which%2520has%2520a%250Aclosed%2520form%252C%2520thus%2520avoiding%2520conventional%2520cross-validation-based%2520hyper-parameter%250Atuning.%2520In%2520addition%252C%2520due%2520to%2520its%2520simple%2520linear%2520distance%2520mapping-based%250Aconstruction%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%2520method%2520can%2520assess%2520the%250Auncertainty%2520of%2520the%2520predictions%2520for%2520multi-label%2520classification%252C%2520which%2520is%2520a%250Avaluable%2520capability%2520for%2520data-centric%2520machine%2520learning%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.05518v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimal%20Learning%20Machine%20for%20Multi-Label%20Learning&entry.906535625=Joonas%20H%C3%A4m%C3%A4l%C3%A4inen%20and%20Antoine%20Hubermont%20and%20Amauri%20Souza%20and%20C%C3%A9sar%20L.%20C.%20Mattos%20and%20Jo%C3%A3o%20P.%20P.%20Gomes%20and%20Tommi%20K%C3%A4rkk%C3%A4inen&entry.1292438233=%20%20Distance-based%20supervised%20method%2C%20the%20minimal%20learning%20machine%2C%20constructs%20a%0Apredictive%20model%20from%20data%20by%20learning%20a%20mapping%20between%20input%20and%20output%0Adistance%20matrices.%20In%20this%20paper%2C%20we%20propose%20new%20methods%20and%20evaluate%20how%20their%0Acore%20component%2C%20the%20distance%20mapping%2C%20can%20be%20adapted%20to%20multi-label%20learning.%0AThe%20proposed%20approach%20is%20based%20on%20combining%20the%20distance%20mapping%20with%20an%0Ainverse%20distance%20weighting.%20Although%20the%20proposal%20is%20one%20of%20the%20simplest%0Amethods%20in%20the%20multi-label%20learning%20literature%2C%20it%20achieves%20state-of-the-art%0Aperformance%20for%20small%20to%20moderate-sized%20multi-label%20learning%20problems.%20In%0Aaddition%20to%20its%20simplicity%2C%20the%20proposed%20method%20is%20fully%20deterministic%3A%20Its%0Ahyper-parameter%20can%20be%20selected%20via%20ranking%20loss-based%20statistic%20which%20has%20a%0Aclosed%20form%2C%20thus%20avoiding%20conventional%20cross-validation-based%20hyper-parameter%0Atuning.%20In%20addition%2C%20due%20to%20its%20simple%20linear%20distance%20mapping-based%0Aconstruction%2C%20we%20demonstrate%20that%20the%20proposed%20method%20can%20assess%20the%0Auncertainty%20of%20the%20predictions%20for%20multi-label%20classification%2C%20which%20is%20a%0Avaluable%20capability%20for%20data-centric%20machine%20learning%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.05518v2&entry.124074799=Read"},
{"title": "Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models", "author": "Andreas M\u00fcller and Denis Lukovnikov and Jonas Thietke and Asja Fischer and Erwin Quiring", "abstract": "  Integrating watermarking into the generation process of latent diffusion\nmodels (LDMs) simplifies detection and attribution of generated content.\nSemantic watermarks, such as Tree-Rings and Gaussian Shading, represent a novel\nclass of watermarking techniques that are easy to implement and highly robust\nagainst various perturbations. However, our work demonstrates a fundamental\nsecurity vulnerability of semantic watermarks. We show that attackers can\nleverage unrelated models, even with different latent spaces and architectures\n(UNet vs DiT), to perform powerful and realistic forgery attacks. Specifically,\nwe design two watermark forgery attacks. The first imprints a targeted\nwatermark into real images by manipulating the latent representation of an\narbitrary image in an unrelated LDM to get closer to the latent representation\nof a watermarked image. We also show that this technique can be used for\nwatermark removal. The second attack generates new images with the target\nwatermark by inverting a watermarked image and re-generating it with an\narbitrary prompt. Both attacks just need a single reference image with the\ntarget watermark. Overall, our findings question the applicability of semantic\nwatermarks by revealing that attackers can easily forge or remove these\nwatermarks under realistic conditions.\n", "link": "http://arxiv.org/abs/2412.03283v1", "date": "2024-12-04", "relevancy": 1.0603, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5482}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5275}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Black-Box%20Forgery%20Attacks%20on%20Semantic%20Watermarks%20for%20Diffusion%20Models&body=Title%3A%20Black-Box%20Forgery%20Attacks%20on%20Semantic%20Watermarks%20for%20Diffusion%20Models%0AAuthor%3A%20Andreas%20M%C3%BCller%20and%20Denis%20Lukovnikov%20and%20Jonas%20Thietke%20and%20Asja%20Fischer%20and%20Erwin%20Quiring%0AAbstract%3A%20%20%20Integrating%20watermarking%20into%20the%20generation%20process%20of%20latent%20diffusion%0Amodels%20%28LDMs%29%20simplifies%20detection%20and%20attribution%20of%20generated%20content.%0ASemantic%20watermarks%2C%20such%20as%20Tree-Rings%20and%20Gaussian%20Shading%2C%20represent%20a%20novel%0Aclass%20of%20watermarking%20techniques%20that%20are%20easy%20to%20implement%20and%20highly%20robust%0Aagainst%20various%20perturbations.%20However%2C%20our%20work%20demonstrates%20a%20fundamental%0Asecurity%20vulnerability%20of%20semantic%20watermarks.%20We%20show%20that%20attackers%20can%0Aleverage%20unrelated%20models%2C%20even%20with%20different%20latent%20spaces%20and%20architectures%0A%28UNet%20vs%20DiT%29%2C%20to%20perform%20powerful%20and%20realistic%20forgery%20attacks.%20Specifically%2C%0Awe%20design%20two%20watermark%20forgery%20attacks.%20The%20first%20imprints%20a%20targeted%0Awatermark%20into%20real%20images%20by%20manipulating%20the%20latent%20representation%20of%20an%0Aarbitrary%20image%20in%20an%20unrelated%20LDM%20to%20get%20closer%20to%20the%20latent%20representation%0Aof%20a%20watermarked%20image.%20We%20also%20show%20that%20this%20technique%20can%20be%20used%20for%0Awatermark%20removal.%20The%20second%20attack%20generates%20new%20images%20with%20the%20target%0Awatermark%20by%20inverting%20a%20watermarked%20image%20and%20re-generating%20it%20with%20an%0Aarbitrary%20prompt.%20Both%20attacks%20just%20need%20a%20single%20reference%20image%20with%20the%0Atarget%20watermark.%20Overall%2C%20our%20findings%20question%20the%20applicability%20of%20semantic%0Awatermarks%20by%20revealing%20that%20attackers%20can%20easily%20forge%20or%20remove%20these%0Awatermarks%20under%20realistic%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlack-Box%2520Forgery%2520Attacks%2520on%2520Semantic%2520Watermarks%2520for%2520Diffusion%2520Models%26entry.906535625%3DAndreas%2520M%25C3%25BCller%2520and%2520Denis%2520Lukovnikov%2520and%2520Jonas%2520Thietke%2520and%2520Asja%2520Fischer%2520and%2520Erwin%2520Quiring%26entry.1292438233%3D%2520%2520Integrating%2520watermarking%2520into%2520the%2520generation%2520process%2520of%2520latent%2520diffusion%250Amodels%2520%2528LDMs%2529%2520simplifies%2520detection%2520and%2520attribution%2520of%2520generated%2520content.%250ASemantic%2520watermarks%252C%2520such%2520as%2520Tree-Rings%2520and%2520Gaussian%2520Shading%252C%2520represent%2520a%2520novel%250Aclass%2520of%2520watermarking%2520techniques%2520that%2520are%2520easy%2520to%2520implement%2520and%2520highly%2520robust%250Aagainst%2520various%2520perturbations.%2520However%252C%2520our%2520work%2520demonstrates%2520a%2520fundamental%250Asecurity%2520vulnerability%2520of%2520semantic%2520watermarks.%2520We%2520show%2520that%2520attackers%2520can%250Aleverage%2520unrelated%2520models%252C%2520even%2520with%2520different%2520latent%2520spaces%2520and%2520architectures%250A%2528UNet%2520vs%2520DiT%2529%252C%2520to%2520perform%2520powerful%2520and%2520realistic%2520forgery%2520attacks.%2520Specifically%252C%250Awe%2520design%2520two%2520watermark%2520forgery%2520attacks.%2520The%2520first%2520imprints%2520a%2520targeted%250Awatermark%2520into%2520real%2520images%2520by%2520manipulating%2520the%2520latent%2520representation%2520of%2520an%250Aarbitrary%2520image%2520in%2520an%2520unrelated%2520LDM%2520to%2520get%2520closer%2520to%2520the%2520latent%2520representation%250Aof%2520a%2520watermarked%2520image.%2520We%2520also%2520show%2520that%2520this%2520technique%2520can%2520be%2520used%2520for%250Awatermark%2520removal.%2520The%2520second%2520attack%2520generates%2520new%2520images%2520with%2520the%2520target%250Awatermark%2520by%2520inverting%2520a%2520watermarked%2520image%2520and%2520re-generating%2520it%2520with%2520an%250Aarbitrary%2520prompt.%2520Both%2520attacks%2520just%2520need%2520a%2520single%2520reference%2520image%2520with%2520the%250Atarget%2520watermark.%2520Overall%252C%2520our%2520findings%2520question%2520the%2520applicability%2520of%2520semantic%250Awatermarks%2520by%2520revealing%2520that%2520attackers%2520can%2520easily%2520forge%2520or%2520remove%2520these%250Awatermarks%2520under%2520realistic%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Black-Box%20Forgery%20Attacks%20on%20Semantic%20Watermarks%20for%20Diffusion%20Models&entry.906535625=Andreas%20M%C3%BCller%20and%20Denis%20Lukovnikov%20and%20Jonas%20Thietke%20and%20Asja%20Fischer%20and%20Erwin%20Quiring&entry.1292438233=%20%20Integrating%20watermarking%20into%20the%20generation%20process%20of%20latent%20diffusion%0Amodels%20%28LDMs%29%20simplifies%20detection%20and%20attribution%20of%20generated%20content.%0ASemantic%20watermarks%2C%20such%20as%20Tree-Rings%20and%20Gaussian%20Shading%2C%20represent%20a%20novel%0Aclass%20of%20watermarking%20techniques%20that%20are%20easy%20to%20implement%20and%20highly%20robust%0Aagainst%20various%20perturbations.%20However%2C%20our%20work%20demonstrates%20a%20fundamental%0Asecurity%20vulnerability%20of%20semantic%20watermarks.%20We%20show%20that%20attackers%20can%0Aleverage%20unrelated%20models%2C%20even%20with%20different%20latent%20spaces%20and%20architectures%0A%28UNet%20vs%20DiT%29%2C%20to%20perform%20powerful%20and%20realistic%20forgery%20attacks.%20Specifically%2C%0Awe%20design%20two%20watermark%20forgery%20attacks.%20The%20first%20imprints%20a%20targeted%0Awatermark%20into%20real%20images%20by%20manipulating%20the%20latent%20representation%20of%20an%0Aarbitrary%20image%20in%20an%20unrelated%20LDM%20to%20get%20closer%20to%20the%20latent%20representation%0Aof%20a%20watermarked%20image.%20We%20also%20show%20that%20this%20technique%20can%20be%20used%20for%0Awatermark%20removal.%20The%20second%20attack%20generates%20new%20images%20with%20the%20target%0Awatermark%20by%20inverting%20a%20watermarked%20image%20and%20re-generating%20it%20with%20an%0Aarbitrary%20prompt.%20Both%20attacks%20just%20need%20a%20single%20reference%20image%20with%20the%0Atarget%20watermark.%20Overall%2C%20our%20findings%20question%20the%20applicability%20of%20semantic%0Awatermarks%20by%20revealing%20that%20attackers%20can%20easily%20forge%20or%20remove%20these%0Awatermarks%20under%20realistic%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03283v1&entry.124074799=Read"},
{"title": "Channel Reflection: Knowledge-Driven Data Augmentation for EEG-Based\n  Brain-Computer Interfaces", "author": "Ziwei Wang and Siyang Li and Jingwei Luo and Jiajing Liu and Dongrui Wu", "abstract": "  A brain-computer interface (BCI) enables direct communication between the\nhuman brain and external devices. Electroencephalography (EEG) based BCIs are\ncurrently the most popular for able-bodied users. To increase\nuser-friendliness, usually a small amount of user-specific EEG data are used\nfor calibration, which may not be enough to develop a pure data-driven decoding\nmodel. To cope with this typical calibration data shortage challenge in\nEEG-based BCIs, this paper proposes a parameter-free channel reflection (CR)\ndata augmentation approach that incorporates prior knowledge on the channel\ndistributions of different BCI paradigms in data augmentation. Experiments on\neight public EEG datasets across four different BCI paradigms (motor imagery,\nsteady-state visual evoked potential, P300, and seizure classifications) using\ndifferent decoding algorithms demonstrated that: 1) CR is effective, i.e., it\ncan noticeably improve the classification accuracy; 2) CR is robust, i.e., it\nconsistently outperforms existing data augmentation approaches in the\nliterature; and, 3) CR is flexible, i.e., it can be combined with other data\naugmentation approaches to further increase the performance. We suggest that\ndata augmentation approaches like CR should be an essential step in EEG-based\nBCIs. Our code is available online.\n", "link": "http://arxiv.org/abs/2412.03224v1", "date": "2024-12-04", "relevancy": 1.9127, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4859}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4809}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Channel%20Reflection%3A%20Knowledge-Driven%20Data%20Augmentation%20for%20EEG-Based%0A%20%20Brain-Computer%20Interfaces&body=Title%3A%20Channel%20Reflection%3A%20Knowledge-Driven%20Data%20Augmentation%20for%20EEG-Based%0A%20%20Brain-Computer%20Interfaces%0AAuthor%3A%20Ziwei%20Wang%20and%20Siyang%20Li%20and%20Jingwei%20Luo%20and%20Jiajing%20Liu%20and%20Dongrui%20Wu%0AAbstract%3A%20%20%20A%20brain-computer%20interface%20%28BCI%29%20enables%20direct%20communication%20between%20the%0Ahuman%20brain%20and%20external%20devices.%20Electroencephalography%20%28EEG%29%20based%20BCIs%20are%0Acurrently%20the%20most%20popular%20for%20able-bodied%20users.%20To%20increase%0Auser-friendliness%2C%20usually%20a%20small%20amount%20of%20user-specific%20EEG%20data%20are%20used%0Afor%20calibration%2C%20which%20may%20not%20be%20enough%20to%20develop%20a%20pure%20data-driven%20decoding%0Amodel.%20To%20cope%20with%20this%20typical%20calibration%20data%20shortage%20challenge%20in%0AEEG-based%20BCIs%2C%20this%20paper%20proposes%20a%20parameter-free%20channel%20reflection%20%28CR%29%0Adata%20augmentation%20approach%20that%20incorporates%20prior%20knowledge%20on%20the%20channel%0Adistributions%20of%20different%20BCI%20paradigms%20in%20data%20augmentation.%20Experiments%20on%0Aeight%20public%20EEG%20datasets%20across%20four%20different%20BCI%20paradigms%20%28motor%20imagery%2C%0Asteady-state%20visual%20evoked%20potential%2C%20P300%2C%20and%20seizure%20classifications%29%20using%0Adifferent%20decoding%20algorithms%20demonstrated%20that%3A%201%29%20CR%20is%20effective%2C%20i.e.%2C%20it%0Acan%20noticeably%20improve%20the%20classification%20accuracy%3B%202%29%20CR%20is%20robust%2C%20i.e.%2C%20it%0Aconsistently%20outperforms%20existing%20data%20augmentation%20approaches%20in%20the%0Aliterature%3B%20and%2C%203%29%20CR%20is%20flexible%2C%20i.e.%2C%20it%20can%20be%20combined%20with%20other%20data%0Aaugmentation%20approaches%20to%20further%20increase%20the%20performance.%20We%20suggest%20that%0Adata%20augmentation%20approaches%20like%20CR%20should%20be%20an%20essential%20step%20in%20EEG-based%0ABCIs.%20Our%20code%20is%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChannel%2520Reflection%253A%2520Knowledge-Driven%2520Data%2520Augmentation%2520for%2520EEG-Based%250A%2520%2520Brain-Computer%2520Interfaces%26entry.906535625%3DZiwei%2520Wang%2520and%2520Siyang%2520Li%2520and%2520Jingwei%2520Luo%2520and%2520Jiajing%2520Liu%2520and%2520Dongrui%2520Wu%26entry.1292438233%3D%2520%2520A%2520brain-computer%2520interface%2520%2528BCI%2529%2520enables%2520direct%2520communication%2520between%2520the%250Ahuman%2520brain%2520and%2520external%2520devices.%2520Electroencephalography%2520%2528EEG%2529%2520based%2520BCIs%2520are%250Acurrently%2520the%2520most%2520popular%2520for%2520able-bodied%2520users.%2520To%2520increase%250Auser-friendliness%252C%2520usually%2520a%2520small%2520amount%2520of%2520user-specific%2520EEG%2520data%2520are%2520used%250Afor%2520calibration%252C%2520which%2520may%2520not%2520be%2520enough%2520to%2520develop%2520a%2520pure%2520data-driven%2520decoding%250Amodel.%2520To%2520cope%2520with%2520this%2520typical%2520calibration%2520data%2520shortage%2520challenge%2520in%250AEEG-based%2520BCIs%252C%2520this%2520paper%2520proposes%2520a%2520parameter-free%2520channel%2520reflection%2520%2528CR%2529%250Adata%2520augmentation%2520approach%2520that%2520incorporates%2520prior%2520knowledge%2520on%2520the%2520channel%250Adistributions%2520of%2520different%2520BCI%2520paradigms%2520in%2520data%2520augmentation.%2520Experiments%2520on%250Aeight%2520public%2520EEG%2520datasets%2520across%2520four%2520different%2520BCI%2520paradigms%2520%2528motor%2520imagery%252C%250Asteady-state%2520visual%2520evoked%2520potential%252C%2520P300%252C%2520and%2520seizure%2520classifications%2529%2520using%250Adifferent%2520decoding%2520algorithms%2520demonstrated%2520that%253A%25201%2529%2520CR%2520is%2520effective%252C%2520i.e.%252C%2520it%250Acan%2520noticeably%2520improve%2520the%2520classification%2520accuracy%253B%25202%2529%2520CR%2520is%2520robust%252C%2520i.e.%252C%2520it%250Aconsistently%2520outperforms%2520existing%2520data%2520augmentation%2520approaches%2520in%2520the%250Aliterature%253B%2520and%252C%25203%2529%2520CR%2520is%2520flexible%252C%2520i.e.%252C%2520it%2520can%2520be%2520combined%2520with%2520other%2520data%250Aaugmentation%2520approaches%2520to%2520further%2520increase%2520the%2520performance.%2520We%2520suggest%2520that%250Adata%2520augmentation%2520approaches%2520like%2520CR%2520should%2520be%2520an%2520essential%2520step%2520in%2520EEG-based%250ABCIs.%2520Our%2520code%2520is%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Channel%20Reflection%3A%20Knowledge-Driven%20Data%20Augmentation%20for%20EEG-Based%0A%20%20Brain-Computer%20Interfaces&entry.906535625=Ziwei%20Wang%20and%20Siyang%20Li%20and%20Jingwei%20Luo%20and%20Jiajing%20Liu%20and%20Dongrui%20Wu&entry.1292438233=%20%20A%20brain-computer%20interface%20%28BCI%29%20enables%20direct%20communication%20between%20the%0Ahuman%20brain%20and%20external%20devices.%20Electroencephalography%20%28EEG%29%20based%20BCIs%20are%0Acurrently%20the%20most%20popular%20for%20able-bodied%20users.%20To%20increase%0Auser-friendliness%2C%20usually%20a%20small%20amount%20of%20user-specific%20EEG%20data%20are%20used%0Afor%20calibration%2C%20which%20may%20not%20be%20enough%20to%20develop%20a%20pure%20data-driven%20decoding%0Amodel.%20To%20cope%20with%20this%20typical%20calibration%20data%20shortage%20challenge%20in%0AEEG-based%20BCIs%2C%20this%20paper%20proposes%20a%20parameter-free%20channel%20reflection%20%28CR%29%0Adata%20augmentation%20approach%20that%20incorporates%20prior%20knowledge%20on%20the%20channel%0Adistributions%20of%20different%20BCI%20paradigms%20in%20data%20augmentation.%20Experiments%20on%0Aeight%20public%20EEG%20datasets%20across%20four%20different%20BCI%20paradigms%20%28motor%20imagery%2C%0Asteady-state%20visual%20evoked%20potential%2C%20P300%2C%20and%20seizure%20classifications%29%20using%0Adifferent%20decoding%20algorithms%20demonstrated%20that%3A%201%29%20CR%20is%20effective%2C%20i.e.%2C%20it%0Acan%20noticeably%20improve%20the%20classification%20accuracy%3B%202%29%20CR%20is%20robust%2C%20i.e.%2C%20it%0Aconsistently%20outperforms%20existing%20data%20augmentation%20approaches%20in%20the%0Aliterature%3B%20and%2C%203%29%20CR%20is%20flexible%2C%20i.e.%2C%20it%20can%20be%20combined%20with%20other%20data%0Aaugmentation%20approaches%20to%20further%20increase%20the%20performance.%20We%20suggest%20that%0Adata%20augmentation%20approaches%20like%20CR%20should%20be%20an%20essential%20step%20in%20EEG-based%0ABCIs.%20Our%20code%20is%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03224v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


