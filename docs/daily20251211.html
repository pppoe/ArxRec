<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251210.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures", "author": "Patrick Noras and Jun Myeong Choi and Didier Stricker and Pieter Peers and Roni Sengupta", "abstract": "Recent advances in Gaussian Splatting-based inverse rendering extend Gaussian primitives with shading parameters and physically grounded light transport, enabling high-quality material recovery from dense multi-view captures. However, these methods degrade sharply under sparse-view settings, where limited observations lead to severe ambiguity between geometry, reflectance, and lighting. We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation. GAINS first refines geometry using monocular depth/normal and diffusion priors, then employs segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery. Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings. Project page: https://patrickbail.github.io/gains/", "link": "http://arxiv.org/abs/2512.09925v1", "date": "2025-12-10", "relevancy": 3.1703, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.639}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6388}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAINS%3A%20Gaussian-based%20Inverse%20Rendering%20from%20Sparse%20Multi-View%20Captures&body=Title%3A%20GAINS%3A%20Gaussian-based%20Inverse%20Rendering%20from%20Sparse%20Multi-View%20Captures%0AAuthor%3A%20Patrick%20Noras%20and%20Jun%20Myeong%20Choi%20and%20Didier%20Stricker%20and%20Pieter%20Peers%20and%20Roni%20Sengupta%0AAbstract%3A%20Recent%20advances%20in%20Gaussian%20Splatting-based%20inverse%20rendering%20extend%20Gaussian%20primitives%20with%20shading%20parameters%20and%20physically%20grounded%20light%20transport%2C%20enabling%20high-quality%20material%20recovery%20from%20dense%20multi-view%20captures.%20However%2C%20these%20methods%20degrade%20sharply%20under%20sparse-view%20settings%2C%20where%20limited%20observations%20lead%20to%20severe%20ambiguity%20between%20geometry%2C%20reflectance%2C%20and%20lighting.%20We%20introduce%20GAINS%20%28Gaussian-based%20Inverse%20rendering%20from%20Sparse%20multi-view%20captures%29%2C%20a%20two-stage%20inverse%20rendering%20framework%20that%20leverages%20learning-based%20priors%20to%20stabilize%20geometry%20and%20material%20estimation.%20GAINS%20first%20refines%20geometry%20using%20monocular%20depth/normal%20and%20diffusion%20priors%2C%20then%20employs%20segmentation%2C%20intrinsic%20image%20decomposition%20%28IID%29%2C%20and%20diffusion%20priors%20to%20regularize%20material%20recovery.%20Extensive%20experiments%20on%20synthetic%20and%20real-world%20datasets%20show%20that%20GAINS%20significantly%20improves%20material%20parameter%20accuracy%2C%20relighting%20quality%2C%20and%20novel-view%20synthesis%20compared%20to%20state-of-the-art%20Gaussian-based%20inverse%20rendering%20methods%2C%20especially%20under%20sparse-view%20settings.%20Project%20page%3A%20https%3A//patrickbail.github.io/gains/%0ALink%3A%20http%3A//arxiv.org/abs/2512.09925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAINS%253A%2520Gaussian-based%2520Inverse%2520Rendering%2520from%2520Sparse%2520Multi-View%2520Captures%26entry.906535625%3DPatrick%2520Noras%2520and%2520Jun%2520Myeong%2520Choi%2520and%2520Didier%2520Stricker%2520and%2520Pieter%2520Peers%2520and%2520Roni%2520Sengupta%26entry.1292438233%3DRecent%2520advances%2520in%2520Gaussian%2520Splatting-based%2520inverse%2520rendering%2520extend%2520Gaussian%2520primitives%2520with%2520shading%2520parameters%2520and%2520physically%2520grounded%2520light%2520transport%252C%2520enabling%2520high-quality%2520material%2520recovery%2520from%2520dense%2520multi-view%2520captures.%2520However%252C%2520these%2520methods%2520degrade%2520sharply%2520under%2520sparse-view%2520settings%252C%2520where%2520limited%2520observations%2520lead%2520to%2520severe%2520ambiguity%2520between%2520geometry%252C%2520reflectance%252C%2520and%2520lighting.%2520We%2520introduce%2520GAINS%2520%2528Gaussian-based%2520Inverse%2520rendering%2520from%2520Sparse%2520multi-view%2520captures%2529%252C%2520a%2520two-stage%2520inverse%2520rendering%2520framework%2520that%2520leverages%2520learning-based%2520priors%2520to%2520stabilize%2520geometry%2520and%2520material%2520estimation.%2520GAINS%2520first%2520refines%2520geometry%2520using%2520monocular%2520depth/normal%2520and%2520diffusion%2520priors%252C%2520then%2520employs%2520segmentation%252C%2520intrinsic%2520image%2520decomposition%2520%2528IID%2529%252C%2520and%2520diffusion%2520priors%2520to%2520regularize%2520material%2520recovery.%2520Extensive%2520experiments%2520on%2520synthetic%2520and%2520real-world%2520datasets%2520show%2520that%2520GAINS%2520significantly%2520improves%2520material%2520parameter%2520accuracy%252C%2520relighting%2520quality%252C%2520and%2520novel-view%2520synthesis%2520compared%2520to%2520state-of-the-art%2520Gaussian-based%2520inverse%2520rendering%2520methods%252C%2520especially%2520under%2520sparse-view%2520settings.%2520Project%2520page%253A%2520https%253A//patrickbail.github.io/gains/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAINS%3A%20Gaussian-based%20Inverse%20Rendering%20from%20Sparse%20Multi-View%20Captures&entry.906535625=Patrick%20Noras%20and%20Jun%20Myeong%20Choi%20and%20Didier%20Stricker%20and%20Pieter%20Peers%20and%20Roni%20Sengupta&entry.1292438233=Recent%20advances%20in%20Gaussian%20Splatting-based%20inverse%20rendering%20extend%20Gaussian%20primitives%20with%20shading%20parameters%20and%20physically%20grounded%20light%20transport%2C%20enabling%20high-quality%20material%20recovery%20from%20dense%20multi-view%20captures.%20However%2C%20these%20methods%20degrade%20sharply%20under%20sparse-view%20settings%2C%20where%20limited%20observations%20lead%20to%20severe%20ambiguity%20between%20geometry%2C%20reflectance%2C%20and%20lighting.%20We%20introduce%20GAINS%20%28Gaussian-based%20Inverse%20rendering%20from%20Sparse%20multi-view%20captures%29%2C%20a%20two-stage%20inverse%20rendering%20framework%20that%20leverages%20learning-based%20priors%20to%20stabilize%20geometry%20and%20material%20estimation.%20GAINS%20first%20refines%20geometry%20using%20monocular%20depth/normal%20and%20diffusion%20priors%2C%20then%20employs%20segmentation%2C%20intrinsic%20image%20decomposition%20%28IID%29%2C%20and%20diffusion%20priors%20to%20regularize%20material%20recovery.%20Extensive%20experiments%20on%20synthetic%20and%20real-world%20datasets%20show%20that%20GAINS%20significantly%20improves%20material%20parameter%20accuracy%2C%20relighting%20quality%2C%20and%20novel-view%20synthesis%20compared%20to%20state-of-the-art%20Gaussian-based%20inverse%20rendering%20methods%2C%20especially%20under%20sparse-view%20settings.%20Project%20page%3A%20https%3A//patrickbail.github.io/gains/&entry.1838667208=http%3A//arxiv.org/abs/2512.09925v1&entry.124074799=Read"},
{"title": "Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization", "author": "Zhiheng Li and Weihua Wang and Qiang Shen and Yichen Zhao and Zheng Fang", "abstract": "Conventional SLAM systems using visual or LiDAR data often struggle in poor lighting and severe weather. Although 4D radar is suited for such environments, its sparse and noisy point clouds hinder accurate odometry estimation, while the radar maps suffer from obscure and incomplete structures. Thus, we propose Super4DR, a 4D radar-centric framework for learning-based odometry estimation and gaussian-based map optimization. First, we design a cluster-aware odometry network that incorporates object-level cues from the clustered radar points for inter-frame matching, alongside a hierarchical self-supervision mechanism to overcome outliers through spatio-temporal consistency, knowledge transfer, and feature contrast. Second, we propose using 3D gaussians as an intermediate representation, coupled with a radar-specific growth strategy, selective separation, and multi-view regularization, to recover blurry map areas and those undetected based on image texture. Experiments show that Super4DR achieves a 67% performance gain over prior self-supervised methods, nearly matches supervised odometry, and narrows the map quality disparity with LiDAR while enabling multi-modal image rendering.", "link": "http://arxiv.org/abs/2512.09608v1", "date": "2025-12-10", "relevancy": 3.0315, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6409}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5894}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Super4DR%3A%204D%20Radar-centric%20Self-supervised%20Odometry%20and%20Gaussian-based%20Map%20Optimization&body=Title%3A%20Super4DR%3A%204D%20Radar-centric%20Self-supervised%20Odometry%20and%20Gaussian-based%20Map%20Optimization%0AAuthor%3A%20Zhiheng%20Li%20and%20Weihua%20Wang%20and%20Qiang%20Shen%20and%20Yichen%20Zhao%20and%20Zheng%20Fang%0AAbstract%3A%20Conventional%20SLAM%20systems%20using%20visual%20or%20LiDAR%20data%20often%20struggle%20in%20poor%20lighting%20and%20severe%20weather.%20Although%204D%20radar%20is%20suited%20for%20such%20environments%2C%20its%20sparse%20and%20noisy%20point%20clouds%20hinder%20accurate%20odometry%20estimation%2C%20while%20the%20radar%20maps%20suffer%20from%20obscure%20and%20incomplete%20structures.%20Thus%2C%20we%20propose%20Super4DR%2C%20a%204D%20radar-centric%20framework%20for%20learning-based%20odometry%20estimation%20and%20gaussian-based%20map%20optimization.%20First%2C%20we%20design%20a%20cluster-aware%20odometry%20network%20that%20incorporates%20object-level%20cues%20from%20the%20clustered%20radar%20points%20for%20inter-frame%20matching%2C%20alongside%20a%20hierarchical%20self-supervision%20mechanism%20to%20overcome%20outliers%20through%20spatio-temporal%20consistency%2C%20knowledge%20transfer%2C%20and%20feature%20contrast.%20Second%2C%20we%20propose%20using%203D%20gaussians%20as%20an%20intermediate%20representation%2C%20coupled%20with%20a%20radar-specific%20growth%20strategy%2C%20selective%20separation%2C%20and%20multi-view%20regularization%2C%20to%20recover%20blurry%20map%20areas%20and%20those%20undetected%20based%20on%20image%20texture.%20Experiments%20show%20that%20Super4DR%20achieves%20a%2067%25%20performance%20gain%20over%20prior%20self-supervised%20methods%2C%20nearly%20matches%20supervised%20odometry%2C%20and%20narrows%20the%20map%20quality%20disparity%20with%20LiDAR%20while%20enabling%20multi-modal%20image%20rendering.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuper4DR%253A%25204D%2520Radar-centric%2520Self-supervised%2520Odometry%2520and%2520Gaussian-based%2520Map%2520Optimization%26entry.906535625%3DZhiheng%2520Li%2520and%2520Weihua%2520Wang%2520and%2520Qiang%2520Shen%2520and%2520Yichen%2520Zhao%2520and%2520Zheng%2520Fang%26entry.1292438233%3DConventional%2520SLAM%2520systems%2520using%2520visual%2520or%2520LiDAR%2520data%2520often%2520struggle%2520in%2520poor%2520lighting%2520and%2520severe%2520weather.%2520Although%25204D%2520radar%2520is%2520suited%2520for%2520such%2520environments%252C%2520its%2520sparse%2520and%2520noisy%2520point%2520clouds%2520hinder%2520accurate%2520odometry%2520estimation%252C%2520while%2520the%2520radar%2520maps%2520suffer%2520from%2520obscure%2520and%2520incomplete%2520structures.%2520Thus%252C%2520we%2520propose%2520Super4DR%252C%2520a%25204D%2520radar-centric%2520framework%2520for%2520learning-based%2520odometry%2520estimation%2520and%2520gaussian-based%2520map%2520optimization.%2520First%252C%2520we%2520design%2520a%2520cluster-aware%2520odometry%2520network%2520that%2520incorporates%2520object-level%2520cues%2520from%2520the%2520clustered%2520radar%2520points%2520for%2520inter-frame%2520matching%252C%2520alongside%2520a%2520hierarchical%2520self-supervision%2520mechanism%2520to%2520overcome%2520outliers%2520through%2520spatio-temporal%2520consistency%252C%2520knowledge%2520transfer%252C%2520and%2520feature%2520contrast.%2520Second%252C%2520we%2520propose%2520using%25203D%2520gaussians%2520as%2520an%2520intermediate%2520representation%252C%2520coupled%2520with%2520a%2520radar-specific%2520growth%2520strategy%252C%2520selective%2520separation%252C%2520and%2520multi-view%2520regularization%252C%2520to%2520recover%2520blurry%2520map%2520areas%2520and%2520those%2520undetected%2520based%2520on%2520image%2520texture.%2520Experiments%2520show%2520that%2520Super4DR%2520achieves%2520a%252067%2525%2520performance%2520gain%2520over%2520prior%2520self-supervised%2520methods%252C%2520nearly%2520matches%2520supervised%2520odometry%252C%2520and%2520narrows%2520the%2520map%2520quality%2520disparity%2520with%2520LiDAR%2520while%2520enabling%2520multi-modal%2520image%2520rendering.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Super4DR%3A%204D%20Radar-centric%20Self-supervised%20Odometry%20and%20Gaussian-based%20Map%20Optimization&entry.906535625=Zhiheng%20Li%20and%20Weihua%20Wang%20and%20Qiang%20Shen%20and%20Yichen%20Zhao%20and%20Zheng%20Fang&entry.1292438233=Conventional%20SLAM%20systems%20using%20visual%20or%20LiDAR%20data%20often%20struggle%20in%20poor%20lighting%20and%20severe%20weather.%20Although%204D%20radar%20is%20suited%20for%20such%20environments%2C%20its%20sparse%20and%20noisy%20point%20clouds%20hinder%20accurate%20odometry%20estimation%2C%20while%20the%20radar%20maps%20suffer%20from%20obscure%20and%20incomplete%20structures.%20Thus%2C%20we%20propose%20Super4DR%2C%20a%204D%20radar-centric%20framework%20for%20learning-based%20odometry%20estimation%20and%20gaussian-based%20map%20optimization.%20First%2C%20we%20design%20a%20cluster-aware%20odometry%20network%20that%20incorporates%20object-level%20cues%20from%20the%20clustered%20radar%20points%20for%20inter-frame%20matching%2C%20alongside%20a%20hierarchical%20self-supervision%20mechanism%20to%20overcome%20outliers%20through%20spatio-temporal%20consistency%2C%20knowledge%20transfer%2C%20and%20feature%20contrast.%20Second%2C%20we%20propose%20using%203D%20gaussians%20as%20an%20intermediate%20representation%2C%20coupled%20with%20a%20radar-specific%20growth%20strategy%2C%20selective%20separation%2C%20and%20multi-view%20regularization%2C%20to%20recover%20blurry%20map%20areas%20and%20those%20undetected%20based%20on%20image%20texture.%20Experiments%20show%20that%20Super4DR%20achieves%20a%2067%25%20performance%20gain%20over%20prior%20self-supervised%20methods%2C%20nearly%20matches%20supervised%20odometry%2C%20and%20narrows%20the%20map%20quality%20disparity%20with%20LiDAR%20while%20enabling%20multi-modal%20image%20rendering.&entry.1838667208=http%3A//arxiv.org/abs/2512.09608v1&entry.124074799=Read"},
{"title": "Tokenizing Motion: A Generative Approach for Scene Dynamics Compression", "author": "Shanzhi Yin and Zihan Zhang and Bolin Chen and Shiqi Wang and Yan Ye", "abstract": "This paper proposes a novel generative video compression framework that leverages motion pattern priors, derived from subtle dynamics in common scenes (e.g., swaying flowers or a boat drifting on water), rather than relying on video content priors (e.g., talking faces or human bodies). These compact motion priors enable a new approach to ultra-low bitrate communication while achieving high-quality reconstruction across diverse scene contents. At the encoder side, motion priors can be streamlined into compact representations via a dense-to-sparse transformation. At the decoder side, these priors facilitate the reconstruction of scene dynamics using an advanced flow-driven diffusion model. Experimental results illustrate that the proposed method can achieve superior rate-distortion-performance and outperform the state-of-the-art conventional-video codec Enhanced Compression Model (ECM) on-scene dynamics sequences. The project page can be found at-https://github.com/xyzysz/GNVDC.", "link": "http://arxiv.org/abs/2410.09768v4", "date": "2025-12-10", "relevancy": 2.9709, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6005}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5938}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tokenizing%20Motion%3A%20A%20Generative%20Approach%20for%20Scene%20Dynamics%20Compression&body=Title%3A%20Tokenizing%20Motion%3A%20A%20Generative%20Approach%20for%20Scene%20Dynamics%20Compression%0AAuthor%3A%20Shanzhi%20Yin%20and%20Zihan%20Zhang%20and%20Bolin%20Chen%20and%20Shiqi%20Wang%20and%20Yan%20Ye%0AAbstract%3A%20This%20paper%20proposes%20a%20novel%20generative%20video%20compression%20framework%20that%20leverages%20motion%20pattern%20priors%2C%20derived%20from%20subtle%20dynamics%20in%20common%20scenes%20%28e.g.%2C%20swaying%20flowers%20or%20a%20boat%20drifting%20on%20water%29%2C%20rather%20than%20relying%20on%20video%20content%20priors%20%28e.g.%2C%20talking%20faces%20or%20human%20bodies%29.%20These%20compact%20motion%20priors%20enable%20a%20new%20approach%20to%20ultra-low%20bitrate%20communication%20while%20achieving%20high-quality%20reconstruction%20across%20diverse%20scene%20contents.%20At%20the%20encoder%20side%2C%20motion%20priors%20can%20be%20streamlined%20into%20compact%20representations%20via%20a%20dense-to-sparse%20transformation.%20At%20the%20decoder%20side%2C%20these%20priors%20facilitate%20the%20reconstruction%20of%20scene%20dynamics%20using%20an%20advanced%20flow-driven%20diffusion%20model.%20Experimental%20results%20illustrate%20that%20the%20proposed%20method%20can%20achieve%20superior%20rate-distortion-performance%20and%20outperform%20the%20state-of-the-art%20conventional-video%20codec%20Enhanced%20Compression%20Model%20%28ECM%29%20on-scene%20dynamics%20sequences.%20The%20project%20page%20can%20be%20found%20at-https%3A//github.com/xyzysz/GNVDC.%0ALink%3A%20http%3A//arxiv.org/abs/2410.09768v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenizing%2520Motion%253A%2520A%2520Generative%2520Approach%2520for%2520Scene%2520Dynamics%2520Compression%26entry.906535625%3DShanzhi%2520Yin%2520and%2520Zihan%2520Zhang%2520and%2520Bolin%2520Chen%2520and%2520Shiqi%2520Wang%2520and%2520Yan%2520Ye%26entry.1292438233%3DThis%2520paper%2520proposes%2520a%2520novel%2520generative%2520video%2520compression%2520framework%2520that%2520leverages%2520motion%2520pattern%2520priors%252C%2520derived%2520from%2520subtle%2520dynamics%2520in%2520common%2520scenes%2520%2528e.g.%252C%2520swaying%2520flowers%2520or%2520a%2520boat%2520drifting%2520on%2520water%2529%252C%2520rather%2520than%2520relying%2520on%2520video%2520content%2520priors%2520%2528e.g.%252C%2520talking%2520faces%2520or%2520human%2520bodies%2529.%2520These%2520compact%2520motion%2520priors%2520enable%2520a%2520new%2520approach%2520to%2520ultra-low%2520bitrate%2520communication%2520while%2520achieving%2520high-quality%2520reconstruction%2520across%2520diverse%2520scene%2520contents.%2520At%2520the%2520encoder%2520side%252C%2520motion%2520priors%2520can%2520be%2520streamlined%2520into%2520compact%2520representations%2520via%2520a%2520dense-to-sparse%2520transformation.%2520At%2520the%2520decoder%2520side%252C%2520these%2520priors%2520facilitate%2520the%2520reconstruction%2520of%2520scene%2520dynamics%2520using%2520an%2520advanced%2520flow-driven%2520diffusion%2520model.%2520Experimental%2520results%2520illustrate%2520that%2520the%2520proposed%2520method%2520can%2520achieve%2520superior%2520rate-distortion-performance%2520and%2520outperform%2520the%2520state-of-the-art%2520conventional-video%2520codec%2520Enhanced%2520Compression%2520Model%2520%2528ECM%2529%2520on-scene%2520dynamics%2520sequences.%2520The%2520project%2520page%2520can%2520be%2520found%2520at-https%253A//github.com/xyzysz/GNVDC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09768v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tokenizing%20Motion%3A%20A%20Generative%20Approach%20for%20Scene%20Dynamics%20Compression&entry.906535625=Shanzhi%20Yin%20and%20Zihan%20Zhang%20and%20Bolin%20Chen%20and%20Shiqi%20Wang%20and%20Yan%20Ye&entry.1292438233=This%20paper%20proposes%20a%20novel%20generative%20video%20compression%20framework%20that%20leverages%20motion%20pattern%20priors%2C%20derived%20from%20subtle%20dynamics%20in%20common%20scenes%20%28e.g.%2C%20swaying%20flowers%20or%20a%20boat%20drifting%20on%20water%29%2C%20rather%20than%20relying%20on%20video%20content%20priors%20%28e.g.%2C%20talking%20faces%20or%20human%20bodies%29.%20These%20compact%20motion%20priors%20enable%20a%20new%20approach%20to%20ultra-low%20bitrate%20communication%20while%20achieving%20high-quality%20reconstruction%20across%20diverse%20scene%20contents.%20At%20the%20encoder%20side%2C%20motion%20priors%20can%20be%20streamlined%20into%20compact%20representations%20via%20a%20dense-to-sparse%20transformation.%20At%20the%20decoder%20side%2C%20these%20priors%20facilitate%20the%20reconstruction%20of%20scene%20dynamics%20using%20an%20advanced%20flow-driven%20diffusion%20model.%20Experimental%20results%20illustrate%20that%20the%20proposed%20method%20can%20achieve%20superior%20rate-distortion-performance%20and%20outperform%20the%20state-of-the-art%20conventional-video%20codec%20Enhanced%20Compression%20Model%20%28ECM%29%20on-scene%20dynamics%20sequences.%20The%20project%20page%20can%20be%20found%20at-https%3A//github.com/xyzysz/GNVDC.&entry.1838667208=http%3A//arxiv.org/abs/2410.09768v4&entry.124074799=Read"},
{"title": "Learning to Infer Parameterized Representations of Plants from 3D Scans", "author": "Samara Ghrer and Christophe Godin and Stefanie Wuhrer", "abstract": "Plants frequently contain numerous organs, organized in 3D branching systems defining the plant's architecture. Reconstructing the architecture of plants from unstructured observations is challenging because of self-occlusion and spatial proximity between organs, which are often thin structures. To achieve the challenging task, we propose an approach that allows to infer a parameterized representation of the plant's architecture from a given 3D scan of a plant. In addition to the plant's branching structure, this representation contains parametric information for each plant organ, and can therefore be used directly in a variety of tasks. In this data-driven approach, we train a recursive neural network with virtual plants generated using a procedural model. After training, the network allows to infer a parametric tree-like representation based on an input 3D point cloud. Our method is applicable to any plant that can be represented as binary axial tree. We quantitatively evaluate our approach on Chenopodium Album plants on reconstruction, segmentation and skeletonization, which are important problems in plant phenotyping. In addition to carrying out several tasks at once, our method achieves results on-par with strong baselines for each task. We apply our method, trained exclusively on synthetic data, to 3D scans and show that it generalizes well.", "link": "http://arxiv.org/abs/2505.22337v2", "date": "2025-12-10", "relevancy": 2.8487, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5773}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5773}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Infer%20Parameterized%20Representations%20of%20Plants%20from%203D%20Scans&body=Title%3A%20Learning%20to%20Infer%20Parameterized%20Representations%20of%20Plants%20from%203D%20Scans%0AAuthor%3A%20Samara%20Ghrer%20and%20Christophe%20Godin%20and%20Stefanie%20Wuhrer%0AAbstract%3A%20Plants%20frequently%20contain%20numerous%20organs%2C%20organized%20in%203D%20branching%20systems%20defining%20the%20plant%27s%20architecture.%20Reconstructing%20the%20architecture%20of%20plants%20from%20unstructured%20observations%20is%20challenging%20because%20of%20self-occlusion%20and%20spatial%20proximity%20between%20organs%2C%20which%20are%20often%20thin%20structures.%20To%20achieve%20the%20challenging%20task%2C%20we%20propose%20an%20approach%20that%20allows%20to%20infer%20a%20parameterized%20representation%20of%20the%20plant%27s%20architecture%20from%20a%20given%203D%20scan%20of%20a%20plant.%20In%20addition%20to%20the%20plant%27s%20branching%20structure%2C%20this%20representation%20contains%20parametric%20information%20for%20each%20plant%20organ%2C%20and%20can%20therefore%20be%20used%20directly%20in%20a%20variety%20of%20tasks.%20In%20this%20data-driven%20approach%2C%20we%20train%20a%20recursive%20neural%20network%20with%20virtual%20plants%20generated%20using%20a%20procedural%20model.%20After%20training%2C%20the%20network%20allows%20to%20infer%20a%20parametric%20tree-like%20representation%20based%20on%20an%20input%203D%20point%20cloud.%20Our%20method%20is%20applicable%20to%20any%20plant%20that%20can%20be%20represented%20as%20binary%20axial%20tree.%20We%20quantitatively%20evaluate%20our%20approach%20on%20Chenopodium%20Album%20plants%20on%20reconstruction%2C%20segmentation%20and%20skeletonization%2C%20which%20are%20important%20problems%20in%20plant%20phenotyping.%20In%20addition%20to%20carrying%20out%20several%20tasks%20at%20once%2C%20our%20method%20achieves%20results%20on-par%20with%20strong%20baselines%20for%20each%20task.%20We%20apply%20our%20method%2C%20trained%20exclusively%20on%20synthetic%20data%2C%20to%203D%20scans%20and%20show%20that%20it%20generalizes%20well.%0ALink%3A%20http%3A//arxiv.org/abs/2505.22337v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Infer%2520Parameterized%2520Representations%2520of%2520Plants%2520from%25203D%2520Scans%26entry.906535625%3DSamara%2520Ghrer%2520and%2520Christophe%2520Godin%2520and%2520Stefanie%2520Wuhrer%26entry.1292438233%3DPlants%2520frequently%2520contain%2520numerous%2520organs%252C%2520organized%2520in%25203D%2520branching%2520systems%2520defining%2520the%2520plant%2527s%2520architecture.%2520Reconstructing%2520the%2520architecture%2520of%2520plants%2520from%2520unstructured%2520observations%2520is%2520challenging%2520because%2520of%2520self-occlusion%2520and%2520spatial%2520proximity%2520between%2520organs%252C%2520which%2520are%2520often%2520thin%2520structures.%2520To%2520achieve%2520the%2520challenging%2520task%252C%2520we%2520propose%2520an%2520approach%2520that%2520allows%2520to%2520infer%2520a%2520parameterized%2520representation%2520of%2520the%2520plant%2527s%2520architecture%2520from%2520a%2520given%25203D%2520scan%2520of%2520a%2520plant.%2520In%2520addition%2520to%2520the%2520plant%2527s%2520branching%2520structure%252C%2520this%2520representation%2520contains%2520parametric%2520information%2520for%2520each%2520plant%2520organ%252C%2520and%2520can%2520therefore%2520be%2520used%2520directly%2520in%2520a%2520variety%2520of%2520tasks.%2520In%2520this%2520data-driven%2520approach%252C%2520we%2520train%2520a%2520recursive%2520neural%2520network%2520with%2520virtual%2520plants%2520generated%2520using%2520a%2520procedural%2520model.%2520After%2520training%252C%2520the%2520network%2520allows%2520to%2520infer%2520a%2520parametric%2520tree-like%2520representation%2520based%2520on%2520an%2520input%25203D%2520point%2520cloud.%2520Our%2520method%2520is%2520applicable%2520to%2520any%2520plant%2520that%2520can%2520be%2520represented%2520as%2520binary%2520axial%2520tree.%2520We%2520quantitatively%2520evaluate%2520our%2520approach%2520on%2520Chenopodium%2520Album%2520plants%2520on%2520reconstruction%252C%2520segmentation%2520and%2520skeletonization%252C%2520which%2520are%2520important%2520problems%2520in%2520plant%2520phenotyping.%2520In%2520addition%2520to%2520carrying%2520out%2520several%2520tasks%2520at%2520once%252C%2520our%2520method%2520achieves%2520results%2520on-par%2520with%2520strong%2520baselines%2520for%2520each%2520task.%2520We%2520apply%2520our%2520method%252C%2520trained%2520exclusively%2520on%2520synthetic%2520data%252C%2520to%25203D%2520scans%2520and%2520show%2520that%2520it%2520generalizes%2520well.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22337v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Infer%20Parameterized%20Representations%20of%20Plants%20from%203D%20Scans&entry.906535625=Samara%20Ghrer%20and%20Christophe%20Godin%20and%20Stefanie%20Wuhrer&entry.1292438233=Plants%20frequently%20contain%20numerous%20organs%2C%20organized%20in%203D%20branching%20systems%20defining%20the%20plant%27s%20architecture.%20Reconstructing%20the%20architecture%20of%20plants%20from%20unstructured%20observations%20is%20challenging%20because%20of%20self-occlusion%20and%20spatial%20proximity%20between%20organs%2C%20which%20are%20often%20thin%20structures.%20To%20achieve%20the%20challenging%20task%2C%20we%20propose%20an%20approach%20that%20allows%20to%20infer%20a%20parameterized%20representation%20of%20the%20plant%27s%20architecture%20from%20a%20given%203D%20scan%20of%20a%20plant.%20In%20addition%20to%20the%20plant%27s%20branching%20structure%2C%20this%20representation%20contains%20parametric%20information%20for%20each%20plant%20organ%2C%20and%20can%20therefore%20be%20used%20directly%20in%20a%20variety%20of%20tasks.%20In%20this%20data-driven%20approach%2C%20we%20train%20a%20recursive%20neural%20network%20with%20virtual%20plants%20generated%20using%20a%20procedural%20model.%20After%20training%2C%20the%20network%20allows%20to%20infer%20a%20parametric%20tree-like%20representation%20based%20on%20an%20input%203D%20point%20cloud.%20Our%20method%20is%20applicable%20to%20any%20plant%20that%20can%20be%20represented%20as%20binary%20axial%20tree.%20We%20quantitatively%20evaluate%20our%20approach%20on%20Chenopodium%20Album%20plants%20on%20reconstruction%2C%20segmentation%20and%20skeletonization%2C%20which%20are%20important%20problems%20in%20plant%20phenotyping.%20In%20addition%20to%20carrying%20out%20several%20tasks%20at%20once%2C%20our%20method%20achieves%20results%20on-par%20with%20strong%20baselines%20for%20each%20task.%20We%20apply%20our%20method%2C%20trained%20exclusively%20on%20synthetic%20data%2C%20to%203D%20scans%20and%20show%20that%20it%20generalizes%20well.&entry.1838667208=http%3A//arxiv.org/abs/2505.22337v2&entry.124074799=Read"},
{"title": "Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses", "author": "Richard Antonello and Javier Turek and Vy Vo and Alexander Huth", "abstract": "How related are the representations learned by neural language models, translation models, and language tagging tasks? We answer this question by adapting an encoder-decoder transfer learning method from computer vision to investigate the structure among 100 different feature spaces extracted from hidden representations of various networks trained on language tasks. This method reveals a low-dimensional structure where language models and translation models smoothly interpolate between word embeddings, syntactic and semantic tasks, and future word embeddings. We call this low-dimensional structure a language representation embedding because it encodes the relationships between representations needed to process language for a variety of NLP tasks. We find that this representation embedding can predict how well each individual feature space maps to human brain responses to natural language stimuli recorded using fMRI. Additionally, we find that the principal dimension of this structure can be used to create a metric which highlights the brain's natural language processing hierarchy. This suggests that the embedding captures some part of the brain's natural language representation structure.", "link": "http://arxiv.org/abs/2106.05426v5", "date": "2025-12-10", "relevancy": 2.8238, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5874}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Dimensional%20Structure%20in%20the%20Space%20of%20Language%20Representations%20is%20Reflected%20in%20Brain%20Responses&body=Title%3A%20Low-Dimensional%20Structure%20in%20the%20Space%20of%20Language%20Representations%20is%20Reflected%20in%20Brain%20Responses%0AAuthor%3A%20Richard%20Antonello%20and%20Javier%20Turek%20and%20Vy%20Vo%20and%20Alexander%20Huth%0AAbstract%3A%20How%20related%20are%20the%20representations%20learned%20by%20neural%20language%20models%2C%20translation%20models%2C%20and%20language%20tagging%20tasks%3F%20We%20answer%20this%20question%20by%20adapting%20an%20encoder-decoder%20transfer%20learning%20method%20from%20computer%20vision%20to%20investigate%20the%20structure%20among%20100%20different%20feature%20spaces%20extracted%20from%20hidden%20representations%20of%20various%20networks%20trained%20on%20language%20tasks.%20This%20method%20reveals%20a%20low-dimensional%20structure%20where%20language%20models%20and%20translation%20models%20smoothly%20interpolate%20between%20word%20embeddings%2C%20syntactic%20and%20semantic%20tasks%2C%20and%20future%20word%20embeddings.%20We%20call%20this%20low-dimensional%20structure%20a%20language%20representation%20embedding%20because%20it%20encodes%20the%20relationships%20between%20representations%20needed%20to%20process%20language%20for%20a%20variety%20of%20NLP%20tasks.%20We%20find%20that%20this%20representation%20embedding%20can%20predict%20how%20well%20each%20individual%20feature%20space%20maps%20to%20human%20brain%20responses%20to%20natural%20language%20stimuli%20recorded%20using%20fMRI.%20Additionally%2C%20we%20find%20that%20the%20principal%20dimension%20of%20this%20structure%20can%20be%20used%20to%20create%20a%20metric%20which%20highlights%20the%20brain%27s%20natural%20language%20processing%20hierarchy.%20This%20suggests%20that%20the%20embedding%20captures%20some%20part%20of%20the%20brain%27s%20natural%20language%20representation%20structure.%0ALink%3A%20http%3A//arxiv.org/abs/2106.05426v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Dimensional%2520Structure%2520in%2520the%2520Space%2520of%2520Language%2520Representations%2520is%2520Reflected%2520in%2520Brain%2520Responses%26entry.906535625%3DRichard%2520Antonello%2520and%2520Javier%2520Turek%2520and%2520Vy%2520Vo%2520and%2520Alexander%2520Huth%26entry.1292438233%3DHow%2520related%2520are%2520the%2520representations%2520learned%2520by%2520neural%2520language%2520models%252C%2520translation%2520models%252C%2520and%2520language%2520tagging%2520tasks%253F%2520We%2520answer%2520this%2520question%2520by%2520adapting%2520an%2520encoder-decoder%2520transfer%2520learning%2520method%2520from%2520computer%2520vision%2520to%2520investigate%2520the%2520structure%2520among%2520100%2520different%2520feature%2520spaces%2520extracted%2520from%2520hidden%2520representations%2520of%2520various%2520networks%2520trained%2520on%2520language%2520tasks.%2520This%2520method%2520reveals%2520a%2520low-dimensional%2520structure%2520where%2520language%2520models%2520and%2520translation%2520models%2520smoothly%2520interpolate%2520between%2520word%2520embeddings%252C%2520syntactic%2520and%2520semantic%2520tasks%252C%2520and%2520future%2520word%2520embeddings.%2520We%2520call%2520this%2520low-dimensional%2520structure%2520a%2520language%2520representation%2520embedding%2520because%2520it%2520encodes%2520the%2520relationships%2520between%2520representations%2520needed%2520to%2520process%2520language%2520for%2520a%2520variety%2520of%2520NLP%2520tasks.%2520We%2520find%2520that%2520this%2520representation%2520embedding%2520can%2520predict%2520how%2520well%2520each%2520individual%2520feature%2520space%2520maps%2520to%2520human%2520brain%2520responses%2520to%2520natural%2520language%2520stimuli%2520recorded%2520using%2520fMRI.%2520Additionally%252C%2520we%2520find%2520that%2520the%2520principal%2520dimension%2520of%2520this%2520structure%2520can%2520be%2520used%2520to%2520create%2520a%2520metric%2520which%2520highlights%2520the%2520brain%2527s%2520natural%2520language%2520processing%2520hierarchy.%2520This%2520suggests%2520that%2520the%2520embedding%2520captures%2520some%2520part%2520of%2520the%2520brain%2527s%2520natural%2520language%2520representation%2520structure.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2106.05426v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Dimensional%20Structure%20in%20the%20Space%20of%20Language%20Representations%20is%20Reflected%20in%20Brain%20Responses&entry.906535625=Richard%20Antonello%20and%20Javier%20Turek%20and%20Vy%20Vo%20and%20Alexander%20Huth&entry.1292438233=How%20related%20are%20the%20representations%20learned%20by%20neural%20language%20models%2C%20translation%20models%2C%20and%20language%20tagging%20tasks%3F%20We%20answer%20this%20question%20by%20adapting%20an%20encoder-decoder%20transfer%20learning%20method%20from%20computer%20vision%20to%20investigate%20the%20structure%20among%20100%20different%20feature%20spaces%20extracted%20from%20hidden%20representations%20of%20various%20networks%20trained%20on%20language%20tasks.%20This%20method%20reveals%20a%20low-dimensional%20structure%20where%20language%20models%20and%20translation%20models%20smoothly%20interpolate%20between%20word%20embeddings%2C%20syntactic%20and%20semantic%20tasks%2C%20and%20future%20word%20embeddings.%20We%20call%20this%20low-dimensional%20structure%20a%20language%20representation%20embedding%20because%20it%20encodes%20the%20relationships%20between%20representations%20needed%20to%20process%20language%20for%20a%20variety%20of%20NLP%20tasks.%20We%20find%20that%20this%20representation%20embedding%20can%20predict%20how%20well%20each%20individual%20feature%20space%20maps%20to%20human%20brain%20responses%20to%20natural%20language%20stimuli%20recorded%20using%20fMRI.%20Additionally%2C%20we%20find%20that%20the%20principal%20dimension%20of%20this%20structure%20can%20be%20used%20to%20create%20a%20metric%20which%20highlights%20the%20brain%27s%20natural%20language%20processing%20hierarchy.%20This%20suggests%20that%20the%20embedding%20captures%20some%20part%20of%20the%20brain%27s%20natural%20language%20representation%20structure.&entry.1838667208=http%3A//arxiv.org/abs/2106.05426v5&entry.124074799=Read"},
{"title": "GLaD: Geometric Latent Distillation for Vision-Language-Action Models", "author": "Minghao Guo and Meng Cao and Jiachen Tao and Rongtao Xu and Yan Yan and Xiaodan Liang and Ivan Laptev and Xiaojun Chang", "abstract": "Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.", "link": "http://arxiv.org/abs/2512.09619v1", "date": "2025-12-10", "relevancy": 2.8154, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLaD%3A%20Geometric%20Latent%20Distillation%20for%20Vision-Language-Action%20Models&body=Title%3A%20GLaD%3A%20Geometric%20Latent%20Distillation%20for%20Vision-Language-Action%20Models%0AAuthor%3A%20Minghao%20Guo%20and%20Meng%20Cao%20and%20Jiachen%20Tao%20and%20Rongtao%20Xu%20and%20Yan%20Yan%20and%20Xiaodan%20Liang%20and%20Ivan%20Laptev%20and%20Xiaojun%20Chang%0AAbstract%3A%20Most%20existing%20Vision-Language-Action%20%28VLA%29%20models%20rely%20primarily%20on%20RGB%20information%2C%20while%20ignoring%20geometric%20cues%20crucial%20for%20spatial%20reasoning%20and%20manipulation.%20In%20this%20work%2C%20we%20introduce%20GLaD%2C%20a%20geometry-aware%20VLA%20framework%20that%20incorporates%203D%20geometric%20priors%20during%20pretraining%20through%20knowledge%20distillation.%20Rather%20than%20distilling%20geometric%20features%20solely%20into%20the%20vision%20encoder%2C%20we%20align%20the%20LLM%27s%20hidden%20states%20corresponding%20to%20visual%20tokens%20with%20features%20from%20a%20frozen%20geometry-aware%20vision%20transformer%20%28VGGT%29%2C%20ensuring%20that%20geometric%20understanding%20is%20deeply%20integrated%20into%20the%20multimodal%20representations%20that%20drive%20action%20prediction.%20Pretrained%20on%20the%20Bridge%20dataset%20with%20this%20geometry%20distillation%20mechanism%2C%20GLaD%20achieves%2094.1%25%20average%20success%20rate%20across%20four%20LIBERO%20task%20suites%2C%20outperforming%20UniVLA%20%2892.5%25%29%20which%20uses%20identical%20pretraining%20data.%20These%20results%20validate%20that%20geometry-aware%20pretraining%20enhances%20spatial%20reasoning%20and%20policy%20generalization%20without%20requiring%20explicit%20depth%20sensors%20or%203D%20annotations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLaD%253A%2520Geometric%2520Latent%2520Distillation%2520for%2520Vision-Language-Action%2520Models%26entry.906535625%3DMinghao%2520Guo%2520and%2520Meng%2520Cao%2520and%2520Jiachen%2520Tao%2520and%2520Rongtao%2520Xu%2520and%2520Yan%2520Yan%2520and%2520Xiaodan%2520Liang%2520and%2520Ivan%2520Laptev%2520and%2520Xiaojun%2520Chang%26entry.1292438233%3DMost%2520existing%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520rely%2520primarily%2520on%2520RGB%2520information%252C%2520while%2520ignoring%2520geometric%2520cues%2520crucial%2520for%2520spatial%2520reasoning%2520and%2520manipulation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520GLaD%252C%2520a%2520geometry-aware%2520VLA%2520framework%2520that%2520incorporates%25203D%2520geometric%2520priors%2520during%2520pretraining%2520through%2520knowledge%2520distillation.%2520Rather%2520than%2520distilling%2520geometric%2520features%2520solely%2520into%2520the%2520vision%2520encoder%252C%2520we%2520align%2520the%2520LLM%2527s%2520hidden%2520states%2520corresponding%2520to%2520visual%2520tokens%2520with%2520features%2520from%2520a%2520frozen%2520geometry-aware%2520vision%2520transformer%2520%2528VGGT%2529%252C%2520ensuring%2520that%2520geometric%2520understanding%2520is%2520deeply%2520integrated%2520into%2520the%2520multimodal%2520representations%2520that%2520drive%2520action%2520prediction.%2520Pretrained%2520on%2520the%2520Bridge%2520dataset%2520with%2520this%2520geometry%2520distillation%2520mechanism%252C%2520GLaD%2520achieves%252094.1%2525%2520average%2520success%2520rate%2520across%2520four%2520LIBERO%2520task%2520suites%252C%2520outperforming%2520UniVLA%2520%252892.5%2525%2529%2520which%2520uses%2520identical%2520pretraining%2520data.%2520These%2520results%2520validate%2520that%2520geometry-aware%2520pretraining%2520enhances%2520spatial%2520reasoning%2520and%2520policy%2520generalization%2520without%2520requiring%2520explicit%2520depth%2520sensors%2520or%25203D%2520annotations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLaD%3A%20Geometric%20Latent%20Distillation%20for%20Vision-Language-Action%20Models&entry.906535625=Minghao%20Guo%20and%20Meng%20Cao%20and%20Jiachen%20Tao%20and%20Rongtao%20Xu%20and%20Yan%20Yan%20and%20Xiaodan%20Liang%20and%20Ivan%20Laptev%20and%20Xiaojun%20Chang&entry.1292438233=Most%20existing%20Vision-Language-Action%20%28VLA%29%20models%20rely%20primarily%20on%20RGB%20information%2C%20while%20ignoring%20geometric%20cues%20crucial%20for%20spatial%20reasoning%20and%20manipulation.%20In%20this%20work%2C%20we%20introduce%20GLaD%2C%20a%20geometry-aware%20VLA%20framework%20that%20incorporates%203D%20geometric%20priors%20during%20pretraining%20through%20knowledge%20distillation.%20Rather%20than%20distilling%20geometric%20features%20solely%20into%20the%20vision%20encoder%2C%20we%20align%20the%20LLM%27s%20hidden%20states%20corresponding%20to%20visual%20tokens%20with%20features%20from%20a%20frozen%20geometry-aware%20vision%20transformer%20%28VGGT%29%2C%20ensuring%20that%20geometric%20understanding%20is%20deeply%20integrated%20into%20the%20multimodal%20representations%20that%20drive%20action%20prediction.%20Pretrained%20on%20the%20Bridge%20dataset%20with%20this%20geometry%20distillation%20mechanism%2C%20GLaD%20achieves%2094.1%25%20average%20success%20rate%20across%20four%20LIBERO%20task%20suites%2C%20outperforming%20UniVLA%20%2892.5%25%29%20which%20uses%20identical%20pretraining%20data.%20These%20results%20validate%20that%20geometry-aware%20pretraining%20enhances%20spatial%20reasoning%20and%20policy%20generalization%20without%20requiring%20explicit%20depth%20sensors%20or%203D%20annotations.&entry.1838667208=http%3A//arxiv.org/abs/2512.09619v1&entry.124074799=Read"},
{"title": "Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment", "author": "Yuan Li and Zitang Sun and Yen-ju Chen and Shin'ya Nishida", "abstract": "Recent progress in BIQA has been driven by VLMs, whose semantic reasoning abilities suggest that they might extract visual features, generate descriptive text, and infer quality in a human-like manner. However, these models often produce textual descriptions that contradict their final quality predictions, and the predicted scores can change unstably during inference - behaviors not aligned with human reasoning. To understand these issues, we analyze the factors that cause contradictory assessments and instability. We first estimate the relationship between the final quality predictions and the generated visual features, finding that the predictions are not fully grounded in the features and that the logical connection between them is weak. Moreover, decoding intermediate VLM layers shows that the model frequently relies on a limited set of candidate tokens, which contributes to prediction instability. To encourage more human-like reasoning, we introduce a two-stage tuning method that explicitly separates visual perception from quality inference. In the first stage, the model learns visual features; in the second, it infers quality solely from these features. Experiments on SPAQ and KONIQ demonstrate that our approach reduces prediction instability from 22.00% to 12.39% and achieves average gains of 0.3124/0.3507 in SRCC/PLCC across LIVE, CSIQ, SPAQ, and KONIQ compared to the baseline. Further analyses show that our method improves both stability and the reliability of the inference process.", "link": "http://arxiv.org/abs/2512.09555v1", "date": "2025-12-10", "relevancy": 2.8093, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Reasonable%20Inference%20for%20Vision-Language%20Models%20in%20Blind%20Image%20Quality%20Assessment&body=Title%3A%20Building%20Reasonable%20Inference%20for%20Vision-Language%20Models%20in%20Blind%20Image%20Quality%20Assessment%0AAuthor%3A%20Yuan%20Li%20and%20Zitang%20Sun%20and%20Yen-ju%20Chen%20and%20Shin%27ya%20Nishida%0AAbstract%3A%20Recent%20progress%20in%20BIQA%20has%20been%20driven%20by%20VLMs%2C%20whose%20semantic%20reasoning%20abilities%20suggest%20that%20they%20might%20extract%20visual%20features%2C%20generate%20descriptive%20text%2C%20and%20infer%20quality%20in%20a%20human-like%20manner.%20However%2C%20these%20models%20often%20produce%20textual%20descriptions%20that%20contradict%20their%20final%20quality%20predictions%2C%20and%20the%20predicted%20scores%20can%20change%20unstably%20during%20inference%20-%20behaviors%20not%20aligned%20with%20human%20reasoning.%20To%20understand%20these%20issues%2C%20we%20analyze%20the%20factors%20that%20cause%20contradictory%20assessments%20and%20instability.%20We%20first%20estimate%20the%20relationship%20between%20the%20final%20quality%20predictions%20and%20the%20generated%20visual%20features%2C%20finding%20that%20the%20predictions%20are%20not%20fully%20grounded%20in%20the%20features%20and%20that%20the%20logical%20connection%20between%20them%20is%20weak.%20Moreover%2C%20decoding%20intermediate%20VLM%20layers%20shows%20that%20the%20model%20frequently%20relies%20on%20a%20limited%20set%20of%20candidate%20tokens%2C%20which%20contributes%20to%20prediction%20instability.%20To%20encourage%20more%20human-like%20reasoning%2C%20we%20introduce%20a%20two-stage%20tuning%20method%20that%20explicitly%20separates%20visual%20perception%20from%20quality%20inference.%20In%20the%20first%20stage%2C%20the%20model%20learns%20visual%20features%3B%20in%20the%20second%2C%20it%20infers%20quality%20solely%20from%20these%20features.%20Experiments%20on%20SPAQ%20and%20KONIQ%20demonstrate%20that%20our%20approach%20reduces%20prediction%20instability%20from%2022.00%25%20to%2012.39%25%20and%20achieves%20average%20gains%20of%200.3124/0.3507%20in%20SRCC/PLCC%20across%20LIVE%2C%20CSIQ%2C%20SPAQ%2C%20and%20KONIQ%20compared%20to%20the%20baseline.%20Further%20analyses%20show%20that%20our%20method%20improves%20both%20stability%20and%20the%20reliability%20of%20the%20inference%20process.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Reasonable%2520Inference%2520for%2520Vision-Language%2520Models%2520in%2520Blind%2520Image%2520Quality%2520Assessment%26entry.906535625%3DYuan%2520Li%2520and%2520Zitang%2520Sun%2520and%2520Yen-ju%2520Chen%2520and%2520Shin%2527ya%2520Nishida%26entry.1292438233%3DRecent%2520progress%2520in%2520BIQA%2520has%2520been%2520driven%2520by%2520VLMs%252C%2520whose%2520semantic%2520reasoning%2520abilities%2520suggest%2520that%2520they%2520might%2520extract%2520visual%2520features%252C%2520generate%2520descriptive%2520text%252C%2520and%2520infer%2520quality%2520in%2520a%2520human-like%2520manner.%2520However%252C%2520these%2520models%2520often%2520produce%2520textual%2520descriptions%2520that%2520contradict%2520their%2520final%2520quality%2520predictions%252C%2520and%2520the%2520predicted%2520scores%2520can%2520change%2520unstably%2520during%2520inference%2520-%2520behaviors%2520not%2520aligned%2520with%2520human%2520reasoning.%2520To%2520understand%2520these%2520issues%252C%2520we%2520analyze%2520the%2520factors%2520that%2520cause%2520contradictory%2520assessments%2520and%2520instability.%2520We%2520first%2520estimate%2520the%2520relationship%2520between%2520the%2520final%2520quality%2520predictions%2520and%2520the%2520generated%2520visual%2520features%252C%2520finding%2520that%2520the%2520predictions%2520are%2520not%2520fully%2520grounded%2520in%2520the%2520features%2520and%2520that%2520the%2520logical%2520connection%2520between%2520them%2520is%2520weak.%2520Moreover%252C%2520decoding%2520intermediate%2520VLM%2520layers%2520shows%2520that%2520the%2520model%2520frequently%2520relies%2520on%2520a%2520limited%2520set%2520of%2520candidate%2520tokens%252C%2520which%2520contributes%2520to%2520prediction%2520instability.%2520To%2520encourage%2520more%2520human-like%2520reasoning%252C%2520we%2520introduce%2520a%2520two-stage%2520tuning%2520method%2520that%2520explicitly%2520separates%2520visual%2520perception%2520from%2520quality%2520inference.%2520In%2520the%2520first%2520stage%252C%2520the%2520model%2520learns%2520visual%2520features%253B%2520in%2520the%2520second%252C%2520it%2520infers%2520quality%2520solely%2520from%2520these%2520features.%2520Experiments%2520on%2520SPAQ%2520and%2520KONIQ%2520demonstrate%2520that%2520our%2520approach%2520reduces%2520prediction%2520instability%2520from%252022.00%2525%2520to%252012.39%2525%2520and%2520achieves%2520average%2520gains%2520of%25200.3124/0.3507%2520in%2520SRCC/PLCC%2520across%2520LIVE%252C%2520CSIQ%252C%2520SPAQ%252C%2520and%2520KONIQ%2520compared%2520to%2520the%2520baseline.%2520Further%2520analyses%2520show%2520that%2520our%2520method%2520improves%2520both%2520stability%2520and%2520the%2520reliability%2520of%2520the%2520inference%2520process.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Reasonable%20Inference%20for%20Vision-Language%20Models%20in%20Blind%20Image%20Quality%20Assessment&entry.906535625=Yuan%20Li%20and%20Zitang%20Sun%20and%20Yen-ju%20Chen%20and%20Shin%27ya%20Nishida&entry.1292438233=Recent%20progress%20in%20BIQA%20has%20been%20driven%20by%20VLMs%2C%20whose%20semantic%20reasoning%20abilities%20suggest%20that%20they%20might%20extract%20visual%20features%2C%20generate%20descriptive%20text%2C%20and%20infer%20quality%20in%20a%20human-like%20manner.%20However%2C%20these%20models%20often%20produce%20textual%20descriptions%20that%20contradict%20their%20final%20quality%20predictions%2C%20and%20the%20predicted%20scores%20can%20change%20unstably%20during%20inference%20-%20behaviors%20not%20aligned%20with%20human%20reasoning.%20To%20understand%20these%20issues%2C%20we%20analyze%20the%20factors%20that%20cause%20contradictory%20assessments%20and%20instability.%20We%20first%20estimate%20the%20relationship%20between%20the%20final%20quality%20predictions%20and%20the%20generated%20visual%20features%2C%20finding%20that%20the%20predictions%20are%20not%20fully%20grounded%20in%20the%20features%20and%20that%20the%20logical%20connection%20between%20them%20is%20weak.%20Moreover%2C%20decoding%20intermediate%20VLM%20layers%20shows%20that%20the%20model%20frequently%20relies%20on%20a%20limited%20set%20of%20candidate%20tokens%2C%20which%20contributes%20to%20prediction%20instability.%20To%20encourage%20more%20human-like%20reasoning%2C%20we%20introduce%20a%20two-stage%20tuning%20method%20that%20explicitly%20separates%20visual%20perception%20from%20quality%20inference.%20In%20the%20first%20stage%2C%20the%20model%20learns%20visual%20features%3B%20in%20the%20second%2C%20it%20infers%20quality%20solely%20from%20these%20features.%20Experiments%20on%20SPAQ%20and%20KONIQ%20demonstrate%20that%20our%20approach%20reduces%20prediction%20instability%20from%2022.00%25%20to%2012.39%25%20and%20achieves%20average%20gains%20of%200.3124/0.3507%20in%20SRCC/PLCC%20across%20LIVE%2C%20CSIQ%2C%20SPAQ%2C%20and%20KONIQ%20compared%20to%20the%20baseline.%20Further%20analyses%20show%20that%20our%20method%20improves%20both%20stability%20and%20the%20reliability%20of%20the%20inference%20process.&entry.1838667208=http%3A//arxiv.org/abs/2512.09555v1&entry.124074799=Read"},
{"title": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models", "author": "Yifan Ye and Jiaqi Ma and Jun Cen and Zhihe Lu", "abstract": "Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \\href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}", "link": "http://arxiv.org/abs/2512.09927v1", "date": "2025-12-10", "relevancy": 2.7986, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token%20Expand-Merge%3A%20Training-Free%20Token%20Compression%20for%20Vision-Language-Action%20Models&body=Title%3A%20Token%20Expand-Merge%3A%20Training-Free%20Token%20Compression%20for%20Vision-Language-Action%20Models%0AAuthor%3A%20Yifan%20Ye%20and%20Jiaqi%20Ma%20and%20Jun%20Cen%20and%20Zhihe%20Lu%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20pretrained%20on%20large-scale%20multimodal%20datasets%20have%20emerged%20as%20powerful%20foundations%20for%20robotic%20perception%20and%20control.%20However%2C%20their%20massive%20scale%2C%20often%20billions%20of%20parameters%2C%20poses%20significant%20challenges%20for%20real-time%20deployment%2C%20as%20inference%20becomes%20computationally%20expensive%20and%20latency-sensitive%20in%20dynamic%20environments.%20To%20address%20this%2C%20we%20propose%20Token%20Expand-and-Merge-VLA%20%28TEAM-VLA%29%2C%20a%20training-free%20token%20compression%20framework%20that%20accelerates%20VLA%20inference%20while%20preserving%20task%20performance.%20TEAM-VLA%20introduces%20a%20dynamic%20token%20expansion%20mechanism%20that%20identifies%20and%20samples%20additional%20informative%20tokens%20in%20the%20spatial%20vicinity%20of%20attention-highlighted%20regions%2C%20enhancing%20contextual%20completeness.%20These%20expanded%20tokens%20are%20then%20selectively%20merged%20in%20deeper%20layers%20under%20action-aware%20guidance%2C%20effectively%20reducing%20redundancy%20while%20maintaining%20semantic%20coherence.%20By%20coupling%20expansion%20and%20merging%20within%20a%20single%20feed-forward%20pass%2C%20TEAM-VLA%20achieves%20a%20balanced%20trade-off%20between%20efficiency%20and%20effectiveness%2C%20without%20any%20retraining%20or%20parameter%20updates.%20Extensive%20experiments%20on%20LIBERO%20benchmark%20demonstrate%20that%20TEAM-VLA%20consistently%20improves%20inference%20speed%20while%20maintaining%20or%20even%20surpassing%20the%20task%20success%20rate%20of%20full%20VLA%20models.%20The%20code%20is%20public%20available%20on%20%5Chref%7Bhttps%3A//github.com/Jasper-aaa/TEAM-VLA%7D%7Bhttps%3A//github.com/Jasper-aaa/TEAM-VLA%7D%0ALink%3A%20http%3A//arxiv.org/abs/2512.09927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken%2520Expand-Merge%253A%2520Training-Free%2520Token%2520Compression%2520for%2520Vision-Language-Action%2520Models%26entry.906535625%3DYifan%2520Ye%2520and%2520Jiaqi%2520Ma%2520and%2520Jun%2520Cen%2520and%2520Zhihe%2520Lu%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520pretrained%2520on%2520large-scale%2520multimodal%2520datasets%2520have%2520emerged%2520as%2520powerful%2520foundations%2520for%2520robotic%2520perception%2520and%2520control.%2520However%252C%2520their%2520massive%2520scale%252C%2520often%2520billions%2520of%2520parameters%252C%2520poses%2520significant%2520challenges%2520for%2520real-time%2520deployment%252C%2520as%2520inference%2520becomes%2520computationally%2520expensive%2520and%2520latency-sensitive%2520in%2520dynamic%2520environments.%2520To%2520address%2520this%252C%2520we%2520propose%2520Token%2520Expand-and-Merge-VLA%2520%2528TEAM-VLA%2529%252C%2520a%2520training-free%2520token%2520compression%2520framework%2520that%2520accelerates%2520VLA%2520inference%2520while%2520preserving%2520task%2520performance.%2520TEAM-VLA%2520introduces%2520a%2520dynamic%2520token%2520expansion%2520mechanism%2520that%2520identifies%2520and%2520samples%2520additional%2520informative%2520tokens%2520in%2520the%2520spatial%2520vicinity%2520of%2520attention-highlighted%2520regions%252C%2520enhancing%2520contextual%2520completeness.%2520These%2520expanded%2520tokens%2520are%2520then%2520selectively%2520merged%2520in%2520deeper%2520layers%2520under%2520action-aware%2520guidance%252C%2520effectively%2520reducing%2520redundancy%2520while%2520maintaining%2520semantic%2520coherence.%2520By%2520coupling%2520expansion%2520and%2520merging%2520within%2520a%2520single%2520feed-forward%2520pass%252C%2520TEAM-VLA%2520achieves%2520a%2520balanced%2520trade-off%2520between%2520efficiency%2520and%2520effectiveness%252C%2520without%2520any%2520retraining%2520or%2520parameter%2520updates.%2520Extensive%2520experiments%2520on%2520LIBERO%2520benchmark%2520demonstrate%2520that%2520TEAM-VLA%2520consistently%2520improves%2520inference%2520speed%2520while%2520maintaining%2520or%2520even%2520surpassing%2520the%2520task%2520success%2520rate%2520of%2520full%2520VLA%2520models.%2520The%2520code%2520is%2520public%2520available%2520on%2520%255Chref%257Bhttps%253A//github.com/Jasper-aaa/TEAM-VLA%257D%257Bhttps%253A//github.com/Jasper-aaa/TEAM-VLA%257D%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token%20Expand-Merge%3A%20Training-Free%20Token%20Compression%20for%20Vision-Language-Action%20Models&entry.906535625=Yifan%20Ye%20and%20Jiaqi%20Ma%20and%20Jun%20Cen%20and%20Zhihe%20Lu&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20pretrained%20on%20large-scale%20multimodal%20datasets%20have%20emerged%20as%20powerful%20foundations%20for%20robotic%20perception%20and%20control.%20However%2C%20their%20massive%20scale%2C%20often%20billions%20of%20parameters%2C%20poses%20significant%20challenges%20for%20real-time%20deployment%2C%20as%20inference%20becomes%20computationally%20expensive%20and%20latency-sensitive%20in%20dynamic%20environments.%20To%20address%20this%2C%20we%20propose%20Token%20Expand-and-Merge-VLA%20%28TEAM-VLA%29%2C%20a%20training-free%20token%20compression%20framework%20that%20accelerates%20VLA%20inference%20while%20preserving%20task%20performance.%20TEAM-VLA%20introduces%20a%20dynamic%20token%20expansion%20mechanism%20that%20identifies%20and%20samples%20additional%20informative%20tokens%20in%20the%20spatial%20vicinity%20of%20attention-highlighted%20regions%2C%20enhancing%20contextual%20completeness.%20These%20expanded%20tokens%20are%20then%20selectively%20merged%20in%20deeper%20layers%20under%20action-aware%20guidance%2C%20effectively%20reducing%20redundancy%20while%20maintaining%20semantic%20coherence.%20By%20coupling%20expansion%20and%20merging%20within%20a%20single%20feed-forward%20pass%2C%20TEAM-VLA%20achieves%20a%20balanced%20trade-off%20between%20efficiency%20and%20effectiveness%2C%20without%20any%20retraining%20or%20parameter%20updates.%20Extensive%20experiments%20on%20LIBERO%20benchmark%20demonstrate%20that%20TEAM-VLA%20consistently%20improves%20inference%20speed%20while%20maintaining%20or%20even%20surpassing%20the%20task%20success%20rate%20of%20full%20VLA%20models.%20The%20code%20is%20public%20available%20on%20%5Chref%7Bhttps%3A//github.com/Jasper-aaa/TEAM-VLA%7D%7Bhttps%3A//github.com/Jasper-aaa/TEAM-VLA%7D&entry.1838667208=http%3A//arxiv.org/abs/2512.09927v1&entry.124074799=Read"},
{"title": "Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception", "author": "Nikos Theodoridis and Tim Brophy and Reenu Mohandas and Ganesh Sistu and Fiachra Collins and Anthony Scanlan and Ciaran Eising", "abstract": "Vision-Language Models (VLMs) are becoming increasingly powerful, demonstrating strong performance on a variety of tasks that require both visual and textual understanding. Their strong generalisation abilities make them a promising component for automated driving systems, which must handle unexpected corner cases. However, to be trusted in such safety-critical applications, a model must first possess a reliable perception system. Moreover, since critical objects and agents in traffic scenes are often at a distance, we require systems that are not \"shortsighted\", i.e., systems with strong perception capabilities at both close (up to 20 meters) and long (30+ meters) range. With this in mind, we introduce Distance-Annotated Traffic Perception Question Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused solely on perception-based questions in traffic scenes, enriched with distance annotations. By excluding questions that require reasoning, we ensure that model performance reflects perception capabilities alone. Since automated driving hardware has limited processing power and cannot support large VLMs, our study centers on smaller VLMs. More specifically, we evaluate several state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the simplicity of the questions, these models significantly underperform compared to humans (~60% average accuracy for the best-performing small VLM versus ~85% human performance). However, it is important to note that the human sample size was relatively small, which imposes statistical limitations. We also identify specific perception tasks, such as distinguishing left from right, that remain particularly challenging for these models.", "link": "http://arxiv.org/abs/2510.08352v2", "date": "2025-12-10", "relevancy": 2.7917, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5864}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Small%20Vision-Language%20Models%20on%20Distance-Dependent%20Traffic%20Perception&body=Title%3A%20Evaluating%20Small%20Vision-Language%20Models%20on%20Distance-Dependent%20Traffic%20Perception%0AAuthor%3A%20Nikos%20Theodoridis%20and%20Tim%20Brophy%20and%20Reenu%20Mohandas%20and%20Ganesh%20Sistu%20and%20Fiachra%20Collins%20and%20Anthony%20Scanlan%20and%20Ciaran%20Eising%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20are%20becoming%20increasingly%20powerful%2C%20demonstrating%20strong%20performance%20on%20a%20variety%20of%20tasks%20that%20require%20both%20visual%20and%20textual%20understanding.%20Their%20strong%20generalisation%20abilities%20make%20them%20a%20promising%20component%20for%20automated%20driving%20systems%2C%20which%20must%20handle%20unexpected%20corner%20cases.%20However%2C%20to%20be%20trusted%20in%20such%20safety-critical%20applications%2C%20a%20model%20must%20first%20possess%20a%20reliable%20perception%20system.%20Moreover%2C%20since%20critical%20objects%20and%20agents%20in%20traffic%20scenes%20are%20often%20at%20a%20distance%2C%20we%20require%20systems%20that%20are%20not%20%22shortsighted%22%2C%20i.e.%2C%20systems%20with%20strong%20perception%20capabilities%20at%20both%20close%20%28up%20to%2020%20meters%29%20and%20long%20%2830%2B%20meters%29%20range.%20With%20this%20in%20mind%2C%20we%20introduce%20Distance-Annotated%20Traffic%20Perception%20Question%20Answering%20%28DTPQA%29%2C%20the%20first%20Visual%20Question%20Answering%20%28VQA%29%20benchmark%20focused%20solely%20on%20perception-based%20questions%20in%20traffic%20scenes%2C%20enriched%20with%20distance%20annotations.%20By%20excluding%20questions%20that%20require%20reasoning%2C%20we%20ensure%20that%20model%20performance%20reflects%20perception%20capabilities%20alone.%20Since%20automated%20driving%20hardware%20has%20limited%20processing%20power%20and%20cannot%20support%20large%20VLMs%2C%20our%20study%20centers%20on%20smaller%20VLMs.%20More%20specifically%2C%20we%20evaluate%20several%20state-of-the-art%20%28SOTA%29%20small%20VLMs%20on%20DTPQA%20and%20show%20that%2C%20despite%20the%20simplicity%20of%20the%20questions%2C%20these%20models%20significantly%20underperform%20compared%20to%20humans%20%28~60%25%20average%20accuracy%20for%20the%20best-performing%20small%20VLM%20versus%20~85%25%20human%20performance%29.%20However%2C%20it%20is%20important%20to%20note%20that%20the%20human%20sample%20size%20was%20relatively%20small%2C%20which%20imposes%20statistical%20limitations.%20We%20also%20identify%20specific%20perception%20tasks%2C%20such%20as%20distinguishing%20left%20from%20right%2C%20that%20remain%20particularly%20challenging%20for%20these%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2510.08352v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Small%2520Vision-Language%2520Models%2520on%2520Distance-Dependent%2520Traffic%2520Perception%26entry.906535625%3DNikos%2520Theodoridis%2520and%2520Tim%2520Brophy%2520and%2520Reenu%2520Mohandas%2520and%2520Ganesh%2520Sistu%2520and%2520Fiachra%2520Collins%2520and%2520Anthony%2520Scanlan%2520and%2520Ciaran%2520Eising%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520are%2520becoming%2520increasingly%2520powerful%252C%2520demonstrating%2520strong%2520performance%2520on%2520a%2520variety%2520of%2520tasks%2520that%2520require%2520both%2520visual%2520and%2520textual%2520understanding.%2520Their%2520strong%2520generalisation%2520abilities%2520make%2520them%2520a%2520promising%2520component%2520for%2520automated%2520driving%2520systems%252C%2520which%2520must%2520handle%2520unexpected%2520corner%2520cases.%2520However%252C%2520to%2520be%2520trusted%2520in%2520such%2520safety-critical%2520applications%252C%2520a%2520model%2520must%2520first%2520possess%2520a%2520reliable%2520perception%2520system.%2520Moreover%252C%2520since%2520critical%2520objects%2520and%2520agents%2520in%2520traffic%2520scenes%2520are%2520often%2520at%2520a%2520distance%252C%2520we%2520require%2520systems%2520that%2520are%2520not%2520%2522shortsighted%2522%252C%2520i.e.%252C%2520systems%2520with%2520strong%2520perception%2520capabilities%2520at%2520both%2520close%2520%2528up%2520to%252020%2520meters%2529%2520and%2520long%2520%252830%252B%2520meters%2529%2520range.%2520With%2520this%2520in%2520mind%252C%2520we%2520introduce%2520Distance-Annotated%2520Traffic%2520Perception%2520Question%2520Answering%2520%2528DTPQA%2529%252C%2520the%2520first%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520benchmark%2520focused%2520solely%2520on%2520perception-based%2520questions%2520in%2520traffic%2520scenes%252C%2520enriched%2520with%2520distance%2520annotations.%2520By%2520excluding%2520questions%2520that%2520require%2520reasoning%252C%2520we%2520ensure%2520that%2520model%2520performance%2520reflects%2520perception%2520capabilities%2520alone.%2520Since%2520automated%2520driving%2520hardware%2520has%2520limited%2520processing%2520power%2520and%2520cannot%2520support%2520large%2520VLMs%252C%2520our%2520study%2520centers%2520on%2520smaller%2520VLMs.%2520More%2520specifically%252C%2520we%2520evaluate%2520several%2520state-of-the-art%2520%2528SOTA%2529%2520small%2520VLMs%2520on%2520DTPQA%2520and%2520show%2520that%252C%2520despite%2520the%2520simplicity%2520of%2520the%2520questions%252C%2520these%2520models%2520significantly%2520underperform%2520compared%2520to%2520humans%2520%2528~60%2525%2520average%2520accuracy%2520for%2520the%2520best-performing%2520small%2520VLM%2520versus%2520~85%2525%2520human%2520performance%2529.%2520However%252C%2520it%2520is%2520important%2520to%2520note%2520that%2520the%2520human%2520sample%2520size%2520was%2520relatively%2520small%252C%2520which%2520imposes%2520statistical%2520limitations.%2520We%2520also%2520identify%2520specific%2520perception%2520tasks%252C%2520such%2520as%2520distinguishing%2520left%2520from%2520right%252C%2520that%2520remain%2520particularly%2520challenging%2520for%2520these%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08352v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Small%20Vision-Language%20Models%20on%20Distance-Dependent%20Traffic%20Perception&entry.906535625=Nikos%20Theodoridis%20and%20Tim%20Brophy%20and%20Reenu%20Mohandas%20and%20Ganesh%20Sistu%20and%20Fiachra%20Collins%20and%20Anthony%20Scanlan%20and%20Ciaran%20Eising&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20are%20becoming%20increasingly%20powerful%2C%20demonstrating%20strong%20performance%20on%20a%20variety%20of%20tasks%20that%20require%20both%20visual%20and%20textual%20understanding.%20Their%20strong%20generalisation%20abilities%20make%20them%20a%20promising%20component%20for%20automated%20driving%20systems%2C%20which%20must%20handle%20unexpected%20corner%20cases.%20However%2C%20to%20be%20trusted%20in%20such%20safety-critical%20applications%2C%20a%20model%20must%20first%20possess%20a%20reliable%20perception%20system.%20Moreover%2C%20since%20critical%20objects%20and%20agents%20in%20traffic%20scenes%20are%20often%20at%20a%20distance%2C%20we%20require%20systems%20that%20are%20not%20%22shortsighted%22%2C%20i.e.%2C%20systems%20with%20strong%20perception%20capabilities%20at%20both%20close%20%28up%20to%2020%20meters%29%20and%20long%20%2830%2B%20meters%29%20range.%20With%20this%20in%20mind%2C%20we%20introduce%20Distance-Annotated%20Traffic%20Perception%20Question%20Answering%20%28DTPQA%29%2C%20the%20first%20Visual%20Question%20Answering%20%28VQA%29%20benchmark%20focused%20solely%20on%20perception-based%20questions%20in%20traffic%20scenes%2C%20enriched%20with%20distance%20annotations.%20By%20excluding%20questions%20that%20require%20reasoning%2C%20we%20ensure%20that%20model%20performance%20reflects%20perception%20capabilities%20alone.%20Since%20automated%20driving%20hardware%20has%20limited%20processing%20power%20and%20cannot%20support%20large%20VLMs%2C%20our%20study%20centers%20on%20smaller%20VLMs.%20More%20specifically%2C%20we%20evaluate%20several%20state-of-the-art%20%28SOTA%29%20small%20VLMs%20on%20DTPQA%20and%20show%20that%2C%20despite%20the%20simplicity%20of%20the%20questions%2C%20these%20models%20significantly%20underperform%20compared%20to%20humans%20%28~60%25%20average%20accuracy%20for%20the%20best-performing%20small%20VLM%20versus%20~85%25%20human%20performance%29.%20However%2C%20it%20is%20important%20to%20note%20that%20the%20human%20sample%20size%20was%20relatively%20small%2C%20which%20imposes%20statistical%20limitations.%20We%20also%20identify%20specific%20perception%20tasks%2C%20such%20as%20distinguishing%20left%20from%20right%2C%20that%20remain%20particularly%20challenging%20for%20these%20models.&entry.1838667208=http%3A//arxiv.org/abs/2510.08352v2&entry.124074799=Read"},
{"title": "RELOCATE: A Simple Training-Free Baseline for Visual Query Localization Using Region-Based Representations", "author": "Savya Khosla and Sethuraman T and Alexander Schwing and Derek Hoiem", "abstract": "We present RELOCATE, a simple training-free baseline designed to perform the challenging task of visual query localization in long videos. To eliminate the need for task-specific training and efficiently handle long videos, RELOCATE leverages a region-based representation derived from pretrained vision models. At a high level, it follows the classic object localization approach: (1) identify all objects in each video frame, (2) compare the objects with the given query and select the most similar ones, and (3) perform bidirectional tracking to get a spatio-temporal response. However, we propose some key enhancements to handle small objects, cluttered scenes, partial visibility, and varying appearances. Notably, we refine the selected objects for accurate localization and generate additional visual queries to capture visual variations. We evaluate RELOCATE on the challenging Ego4D Visual Query 2D Localization dataset, establishing a new baseline that outperforms prior task-specific methods by 49% (relative improvement) in spatio-temporal average precision.", "link": "http://arxiv.org/abs/2412.01826v2", "date": "2025-12-10", "relevancy": 2.7625, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5829}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RELOCATE%3A%20A%20Simple%20Training-Free%20Baseline%20for%20Visual%20Query%20Localization%20Using%20Region-Based%20Representations&body=Title%3A%20RELOCATE%3A%20A%20Simple%20Training-Free%20Baseline%20for%20Visual%20Query%20Localization%20Using%20Region-Based%20Representations%0AAuthor%3A%20Savya%20Khosla%20and%20Sethuraman%20T%20and%20Alexander%20Schwing%20and%20Derek%20Hoiem%0AAbstract%3A%20We%20present%20RELOCATE%2C%20a%20simple%20training-free%20baseline%20designed%20to%20perform%20the%20challenging%20task%20of%20visual%20query%20localization%20in%20long%20videos.%20To%20eliminate%20the%20need%20for%20task-specific%20training%20and%20efficiently%20handle%20long%20videos%2C%20RELOCATE%20leverages%20a%20region-based%20representation%20derived%20from%20pretrained%20vision%20models.%20At%20a%20high%20level%2C%20it%20follows%20the%20classic%20object%20localization%20approach%3A%20%281%29%20identify%20all%20objects%20in%20each%20video%20frame%2C%20%282%29%20compare%20the%20objects%20with%20the%20given%20query%20and%20select%20the%20most%20similar%20ones%2C%20and%20%283%29%20perform%20bidirectional%20tracking%20to%20get%20a%20spatio-temporal%20response.%20However%2C%20we%20propose%20some%20key%20enhancements%20to%20handle%20small%20objects%2C%20cluttered%20scenes%2C%20partial%20visibility%2C%20and%20varying%20appearances.%20Notably%2C%20we%20refine%20the%20selected%20objects%20for%20accurate%20localization%20and%20generate%20additional%20visual%20queries%20to%20capture%20visual%20variations.%20We%20evaluate%20RELOCATE%20on%20the%20challenging%20Ego4D%20Visual%20Query%202D%20Localization%20dataset%2C%20establishing%20a%20new%20baseline%20that%20outperforms%20prior%20task-specific%20methods%20by%2049%25%20%28relative%20improvement%29%20in%20spatio-temporal%20average%20precision.%0ALink%3A%20http%3A//arxiv.org/abs/2412.01826v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRELOCATE%253A%2520A%2520Simple%2520Training-Free%2520Baseline%2520for%2520Visual%2520Query%2520Localization%2520Using%2520Region-Based%2520Representations%26entry.906535625%3DSavya%2520Khosla%2520and%2520Sethuraman%2520T%2520and%2520Alexander%2520Schwing%2520and%2520Derek%2520Hoiem%26entry.1292438233%3DWe%2520present%2520RELOCATE%252C%2520a%2520simple%2520training-free%2520baseline%2520designed%2520to%2520perform%2520the%2520challenging%2520task%2520of%2520visual%2520query%2520localization%2520in%2520long%2520videos.%2520To%2520eliminate%2520the%2520need%2520for%2520task-specific%2520training%2520and%2520efficiently%2520handle%2520long%2520videos%252C%2520RELOCATE%2520leverages%2520a%2520region-based%2520representation%2520derived%2520from%2520pretrained%2520vision%2520models.%2520At%2520a%2520high%2520level%252C%2520it%2520follows%2520the%2520classic%2520object%2520localization%2520approach%253A%2520%25281%2529%2520identify%2520all%2520objects%2520in%2520each%2520video%2520frame%252C%2520%25282%2529%2520compare%2520the%2520objects%2520with%2520the%2520given%2520query%2520and%2520select%2520the%2520most%2520similar%2520ones%252C%2520and%2520%25283%2529%2520perform%2520bidirectional%2520tracking%2520to%2520get%2520a%2520spatio-temporal%2520response.%2520However%252C%2520we%2520propose%2520some%2520key%2520enhancements%2520to%2520handle%2520small%2520objects%252C%2520cluttered%2520scenes%252C%2520partial%2520visibility%252C%2520and%2520varying%2520appearances.%2520Notably%252C%2520we%2520refine%2520the%2520selected%2520objects%2520for%2520accurate%2520localization%2520and%2520generate%2520additional%2520visual%2520queries%2520to%2520capture%2520visual%2520variations.%2520We%2520evaluate%2520RELOCATE%2520on%2520the%2520challenging%2520Ego4D%2520Visual%2520Query%25202D%2520Localization%2520dataset%252C%2520establishing%2520a%2520new%2520baseline%2520that%2520outperforms%2520prior%2520task-specific%2520methods%2520by%252049%2525%2520%2528relative%2520improvement%2529%2520in%2520spatio-temporal%2520average%2520precision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01826v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RELOCATE%3A%20A%20Simple%20Training-Free%20Baseline%20for%20Visual%20Query%20Localization%20Using%20Region-Based%20Representations&entry.906535625=Savya%20Khosla%20and%20Sethuraman%20T%20and%20Alexander%20Schwing%20and%20Derek%20Hoiem&entry.1292438233=We%20present%20RELOCATE%2C%20a%20simple%20training-free%20baseline%20designed%20to%20perform%20the%20challenging%20task%20of%20visual%20query%20localization%20in%20long%20videos.%20To%20eliminate%20the%20need%20for%20task-specific%20training%20and%20efficiently%20handle%20long%20videos%2C%20RELOCATE%20leverages%20a%20region-based%20representation%20derived%20from%20pretrained%20vision%20models.%20At%20a%20high%20level%2C%20it%20follows%20the%20classic%20object%20localization%20approach%3A%20%281%29%20identify%20all%20objects%20in%20each%20video%20frame%2C%20%282%29%20compare%20the%20objects%20with%20the%20given%20query%20and%20select%20the%20most%20similar%20ones%2C%20and%20%283%29%20perform%20bidirectional%20tracking%20to%20get%20a%20spatio-temporal%20response.%20However%2C%20we%20propose%20some%20key%20enhancements%20to%20handle%20small%20objects%2C%20cluttered%20scenes%2C%20partial%20visibility%2C%20and%20varying%20appearances.%20Notably%2C%20we%20refine%20the%20selected%20objects%20for%20accurate%20localization%20and%20generate%20additional%20visual%20queries%20to%20capture%20visual%20variations.%20We%20evaluate%20RELOCATE%20on%20the%20challenging%20Ego4D%20Visual%20Query%202D%20Localization%20dataset%2C%20establishing%20a%20new%20baseline%20that%20outperforms%20prior%20task-specific%20methods%20by%2049%25%20%28relative%20improvement%29%20in%20spatio-temporal%20average%20precision.&entry.1838667208=http%3A//arxiv.org/abs/2412.01826v2&entry.124074799=Read"},
{"title": "NeuroSketch: An Effective Framework for Neural Decoding via Systematic Architectural Optimization", "author": "Gaorui Zhang and Zhizhang Yuan and Jialan Yang and Junru Chen and Li Meng and Yang Yang", "abstract": "Neural decoding, a critical component of Brain-Computer Interface (BCI), has recently attracted increasing research interest. Previous research has focused on leveraging signal processing and deep learning methods to enhance neural decoding performance. However, the in-depth exploration of model architectures remains underexplored, despite its proven effectiveness in other tasks such as energy forecasting and image classification. In this study, we propose NeuroSketch, an effective framework for neural decoding via systematic architecture optimization. Starting with the basic architecture study, we find that CNN-2D outperforms other architectures in neural decoding tasks and explore its effectiveness from temporal and spatial perspectives. Building on this, we optimize the architecture from macro- to micro-level, achieving improvements in performance at each step. The exploration process and model validations take over 5,000 experiments spanning three distinct modalities (visual, auditory, and speech), three types of brain signals (EEG, SEEG, and ECoG), and eight diverse decoding tasks. Experimental results indicate that NeuroSketch achieves state-of-the-art (SOTA) performance across all evaluated datasets, positioning it as a powerful tool for neural decoding. Our code and scripts are available at https://github.com/Galaxy-Dawn/NeuroSketch.", "link": "http://arxiv.org/abs/2512.09524v1", "date": "2025-12-10", "relevancy": 2.7599, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5666}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5666}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroSketch%3A%20An%20Effective%20Framework%20for%20Neural%20Decoding%20via%20Systematic%20Architectural%20Optimization&body=Title%3A%20NeuroSketch%3A%20An%20Effective%20Framework%20for%20Neural%20Decoding%20via%20Systematic%20Architectural%20Optimization%0AAuthor%3A%20Gaorui%20Zhang%20and%20Zhizhang%20Yuan%20and%20Jialan%20Yang%20and%20Junru%20Chen%20and%20Li%20Meng%20and%20Yang%20Yang%0AAbstract%3A%20Neural%20decoding%2C%20a%20critical%20component%20of%20Brain-Computer%20Interface%20%28BCI%29%2C%20has%20recently%20attracted%20increasing%20research%20interest.%20Previous%20research%20has%20focused%20on%20leveraging%20signal%20processing%20and%20deep%20learning%20methods%20to%20enhance%20neural%20decoding%20performance.%20However%2C%20the%20in-depth%20exploration%20of%20model%20architectures%20remains%20underexplored%2C%20despite%20its%20proven%20effectiveness%20in%20other%20tasks%20such%20as%20energy%20forecasting%20and%20image%20classification.%20In%20this%20study%2C%20we%20propose%20NeuroSketch%2C%20an%20effective%20framework%20for%20neural%20decoding%20via%20systematic%20architecture%20optimization.%20Starting%20with%20the%20basic%20architecture%20study%2C%20we%20find%20that%20CNN-2D%20outperforms%20other%20architectures%20in%20neural%20decoding%20tasks%20and%20explore%20its%20effectiveness%20from%20temporal%20and%20spatial%20perspectives.%20Building%20on%20this%2C%20we%20optimize%20the%20architecture%20from%20macro-%20to%20micro-level%2C%20achieving%20improvements%20in%20performance%20at%20each%20step.%20The%20exploration%20process%20and%20model%20validations%20take%20over%205%2C000%20experiments%20spanning%20three%20distinct%20modalities%20%28visual%2C%20auditory%2C%20and%20speech%29%2C%20three%20types%20of%20brain%20signals%20%28EEG%2C%20SEEG%2C%20and%20ECoG%29%2C%20and%20eight%20diverse%20decoding%20tasks.%20Experimental%20results%20indicate%20that%20NeuroSketch%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20across%20all%20evaluated%20datasets%2C%20positioning%20it%20as%20a%20powerful%20tool%20for%20neural%20decoding.%20Our%20code%20and%20scripts%20are%20available%20at%20https%3A//github.com/Galaxy-Dawn/NeuroSketch.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroSketch%253A%2520An%2520Effective%2520Framework%2520for%2520Neural%2520Decoding%2520via%2520Systematic%2520Architectural%2520Optimization%26entry.906535625%3DGaorui%2520Zhang%2520and%2520Zhizhang%2520Yuan%2520and%2520Jialan%2520Yang%2520and%2520Junru%2520Chen%2520and%2520Li%2520Meng%2520and%2520Yang%2520Yang%26entry.1292438233%3DNeural%2520decoding%252C%2520a%2520critical%2520component%2520of%2520Brain-Computer%2520Interface%2520%2528BCI%2529%252C%2520has%2520recently%2520attracted%2520increasing%2520research%2520interest.%2520Previous%2520research%2520has%2520focused%2520on%2520leveraging%2520signal%2520processing%2520and%2520deep%2520learning%2520methods%2520to%2520enhance%2520neural%2520decoding%2520performance.%2520However%252C%2520the%2520in-depth%2520exploration%2520of%2520model%2520architectures%2520remains%2520underexplored%252C%2520despite%2520its%2520proven%2520effectiveness%2520in%2520other%2520tasks%2520such%2520as%2520energy%2520forecasting%2520and%2520image%2520classification.%2520In%2520this%2520study%252C%2520we%2520propose%2520NeuroSketch%252C%2520an%2520effective%2520framework%2520for%2520neural%2520decoding%2520via%2520systematic%2520architecture%2520optimization.%2520Starting%2520with%2520the%2520basic%2520architecture%2520study%252C%2520we%2520find%2520that%2520CNN-2D%2520outperforms%2520other%2520architectures%2520in%2520neural%2520decoding%2520tasks%2520and%2520explore%2520its%2520effectiveness%2520from%2520temporal%2520and%2520spatial%2520perspectives.%2520Building%2520on%2520this%252C%2520we%2520optimize%2520the%2520architecture%2520from%2520macro-%2520to%2520micro-level%252C%2520achieving%2520improvements%2520in%2520performance%2520at%2520each%2520step.%2520The%2520exploration%2520process%2520and%2520model%2520validations%2520take%2520over%25205%252C000%2520experiments%2520spanning%2520three%2520distinct%2520modalities%2520%2528visual%252C%2520auditory%252C%2520and%2520speech%2529%252C%2520three%2520types%2520of%2520brain%2520signals%2520%2528EEG%252C%2520SEEG%252C%2520and%2520ECoG%2529%252C%2520and%2520eight%2520diverse%2520decoding%2520tasks.%2520Experimental%2520results%2520indicate%2520that%2520NeuroSketch%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520across%2520all%2520evaluated%2520datasets%252C%2520positioning%2520it%2520as%2520a%2520powerful%2520tool%2520for%2520neural%2520decoding.%2520Our%2520code%2520and%2520scripts%2520are%2520available%2520at%2520https%253A//github.com/Galaxy-Dawn/NeuroSketch.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroSketch%3A%20An%20Effective%20Framework%20for%20Neural%20Decoding%20via%20Systematic%20Architectural%20Optimization&entry.906535625=Gaorui%20Zhang%20and%20Zhizhang%20Yuan%20and%20Jialan%20Yang%20and%20Junru%20Chen%20and%20Li%20Meng%20and%20Yang%20Yang&entry.1292438233=Neural%20decoding%2C%20a%20critical%20component%20of%20Brain-Computer%20Interface%20%28BCI%29%2C%20has%20recently%20attracted%20increasing%20research%20interest.%20Previous%20research%20has%20focused%20on%20leveraging%20signal%20processing%20and%20deep%20learning%20methods%20to%20enhance%20neural%20decoding%20performance.%20However%2C%20the%20in-depth%20exploration%20of%20model%20architectures%20remains%20underexplored%2C%20despite%20its%20proven%20effectiveness%20in%20other%20tasks%20such%20as%20energy%20forecasting%20and%20image%20classification.%20In%20this%20study%2C%20we%20propose%20NeuroSketch%2C%20an%20effective%20framework%20for%20neural%20decoding%20via%20systematic%20architecture%20optimization.%20Starting%20with%20the%20basic%20architecture%20study%2C%20we%20find%20that%20CNN-2D%20outperforms%20other%20architectures%20in%20neural%20decoding%20tasks%20and%20explore%20its%20effectiveness%20from%20temporal%20and%20spatial%20perspectives.%20Building%20on%20this%2C%20we%20optimize%20the%20architecture%20from%20macro-%20to%20micro-level%2C%20achieving%20improvements%20in%20performance%20at%20each%20step.%20The%20exploration%20process%20and%20model%20validations%20take%20over%205%2C000%20experiments%20spanning%20three%20distinct%20modalities%20%28visual%2C%20auditory%2C%20and%20speech%29%2C%20three%20types%20of%20brain%20signals%20%28EEG%2C%20SEEG%2C%20and%20ECoG%29%2C%20and%20eight%20diverse%20decoding%20tasks.%20Experimental%20results%20indicate%20that%20NeuroSketch%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20across%20all%20evaluated%20datasets%2C%20positioning%20it%20as%20a%20powerful%20tool%20for%20neural%20decoding.%20Our%20code%20and%20scripts%20are%20available%20at%20https%3A//github.com/Galaxy-Dawn/NeuroSketch.&entry.1838667208=http%3A//arxiv.org/abs/2512.09524v1&entry.124074799=Read"},
{"title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning", "author": "Xinyu Liu and Hangjie Yuan and Yujie Wei and Jiazheng Xing and Yujin Han and Jiahao Pan and Yanbiao Ma and Chi-Min Chan and Kang Zhao and Shiwei Zhang and Wenhan Luo and Yike Guo", "abstract": "Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.", "link": "http://arxiv.org/abs/2512.09924v1", "date": "2025-12-10", "relevancy": 2.7472, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5596}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReViSE%3A%20Towards%20Reason-Informed%20Video%20Editing%20in%20Unified%20Models%20with%20Self-Reflective%20Learning&body=Title%3A%20ReViSE%3A%20Towards%20Reason-Informed%20Video%20Editing%20in%20Unified%20Models%20with%20Self-Reflective%20Learning%0AAuthor%3A%20Xinyu%20Liu%20and%20Hangjie%20Yuan%20and%20Yujie%20Wei%20and%20Jiazheng%20Xing%20and%20Yujin%20Han%20and%20Jiahao%20Pan%20and%20Yanbiao%20Ma%20and%20Chi-Min%20Chan%20and%20Kang%20Zhao%20and%20Shiwei%20Zhang%20and%20Wenhan%20Luo%20and%20Yike%20Guo%0AAbstract%3A%20Video%20unified%20models%20exhibit%20strong%20capabilities%20in%20understanding%20and%20generation%2C%20yet%20they%20struggle%20with%20reason-informed%20visual%20editing%20even%20when%20equipped%20with%20powerful%20internal%20vision-language%20models%20%28VLMs%29.%20We%20attribute%20this%20gap%20to%20two%20factors%3A%201%29%20existing%20datasets%20are%20inadequate%20for%20training%20and%20evaluating%20reasoning-aware%20video%20editing%2C%20and%202%29%20an%20inherent%20disconnect%20between%20the%20models%27%20reasoning%20and%20editing%20capabilities%2C%20which%20prevents%20the%20rich%20understanding%20from%20effectively%20instructing%20the%20editing%20process.%20Bridging%20this%20gap%20requires%20an%20integrated%20framework%20that%20connects%20reasoning%20with%20visual%20transformation.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20Reason-Informed%20Video%20Editing%20%28RVE%29%20task%2C%20which%20requires%20reasoning%20about%20physical%20plausibility%20and%20causal%20dynamics%20during%20editing.%20To%20support%20systematic%20evaluation%2C%20we%20construct%20RVE-Bench%2C%20a%20comprehensive%20benchmark%20with%20two%20complementary%20subsets%3A%20Reasoning-Informed%20Video%20Editing%20and%20In-Context%20Video%20Generation.%20These%20subsets%20cover%20diverse%20reasoning%20dimensions%20and%20real-world%20editing%20scenarios.%20Building%20upon%20this%20foundation%2C%20we%20propose%20the%20ReViSE%2C%20a%20Self-Reflective%20Reasoning%20%28SRF%29%20framework%20that%20unifies%20generation%20and%20evaluation%20within%20a%20single%20architecture.%20The%20model%27s%20internal%20VLM%20provides%20intrinsic%20feedback%20by%20assessing%20whether%20the%20edited%20video%20logically%20satisfies%20the%20given%20instruction.%20The%20differential%20feedback%20that%20refines%20the%20generator%27s%20reasoning%20behavior%20during%20training.%20Extensive%20experiments%20on%20RVE-Bench%20demonstrate%20that%20ReViSE%20significantly%20enhances%20editing%20accuracy%20and%20visual%20fidelity%2C%20achieving%20a%2032%25%20improvement%20of%20the%20Overall%20score%20in%20the%20reasoning-informed%20video%20editing%20subset%20over%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReViSE%253A%2520Towards%2520Reason-Informed%2520Video%2520Editing%2520in%2520Unified%2520Models%2520with%2520Self-Reflective%2520Learning%26entry.906535625%3DXinyu%2520Liu%2520and%2520Hangjie%2520Yuan%2520and%2520Yujie%2520Wei%2520and%2520Jiazheng%2520Xing%2520and%2520Yujin%2520Han%2520and%2520Jiahao%2520Pan%2520and%2520Yanbiao%2520Ma%2520and%2520Chi-Min%2520Chan%2520and%2520Kang%2520Zhao%2520and%2520Shiwei%2520Zhang%2520and%2520Wenhan%2520Luo%2520and%2520Yike%2520Guo%26entry.1292438233%3DVideo%2520unified%2520models%2520exhibit%2520strong%2520capabilities%2520in%2520understanding%2520and%2520generation%252C%2520yet%2520they%2520struggle%2520with%2520reason-informed%2520visual%2520editing%2520even%2520when%2520equipped%2520with%2520powerful%2520internal%2520vision-language%2520models%2520%2528VLMs%2529.%2520We%2520attribute%2520this%2520gap%2520to%2520two%2520factors%253A%25201%2529%2520existing%2520datasets%2520are%2520inadequate%2520for%2520training%2520and%2520evaluating%2520reasoning-aware%2520video%2520editing%252C%2520and%25202%2529%2520an%2520inherent%2520disconnect%2520between%2520the%2520models%2527%2520reasoning%2520and%2520editing%2520capabilities%252C%2520which%2520prevents%2520the%2520rich%2520understanding%2520from%2520effectively%2520instructing%2520the%2520editing%2520process.%2520Bridging%2520this%2520gap%2520requires%2520an%2520integrated%2520framework%2520that%2520connects%2520reasoning%2520with%2520visual%2520transformation.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%2520Reason-Informed%2520Video%2520Editing%2520%2528RVE%2529%2520task%252C%2520which%2520requires%2520reasoning%2520about%2520physical%2520plausibility%2520and%2520causal%2520dynamics%2520during%2520editing.%2520To%2520support%2520systematic%2520evaluation%252C%2520we%2520construct%2520RVE-Bench%252C%2520a%2520comprehensive%2520benchmark%2520with%2520two%2520complementary%2520subsets%253A%2520Reasoning-Informed%2520Video%2520Editing%2520and%2520In-Context%2520Video%2520Generation.%2520These%2520subsets%2520cover%2520diverse%2520reasoning%2520dimensions%2520and%2520real-world%2520editing%2520scenarios.%2520Building%2520upon%2520this%2520foundation%252C%2520we%2520propose%2520the%2520ReViSE%252C%2520a%2520Self-Reflective%2520Reasoning%2520%2528SRF%2529%2520framework%2520that%2520unifies%2520generation%2520and%2520evaluation%2520within%2520a%2520single%2520architecture.%2520The%2520model%2527s%2520internal%2520VLM%2520provides%2520intrinsic%2520feedback%2520by%2520assessing%2520whether%2520the%2520edited%2520video%2520logically%2520satisfies%2520the%2520given%2520instruction.%2520The%2520differential%2520feedback%2520that%2520refines%2520the%2520generator%2527s%2520reasoning%2520behavior%2520during%2520training.%2520Extensive%2520experiments%2520on%2520RVE-Bench%2520demonstrate%2520that%2520ReViSE%2520significantly%2520enhances%2520editing%2520accuracy%2520and%2520visual%2520fidelity%252C%2520achieving%2520a%252032%2525%2520improvement%2520of%2520the%2520Overall%2520score%2520in%2520the%2520reasoning-informed%2520video%2520editing%2520subset%2520over%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReViSE%3A%20Towards%20Reason-Informed%20Video%20Editing%20in%20Unified%20Models%20with%20Self-Reflective%20Learning&entry.906535625=Xinyu%20Liu%20and%20Hangjie%20Yuan%20and%20Yujie%20Wei%20and%20Jiazheng%20Xing%20and%20Yujin%20Han%20and%20Jiahao%20Pan%20and%20Yanbiao%20Ma%20and%20Chi-Min%20Chan%20and%20Kang%20Zhao%20and%20Shiwei%20Zhang%20and%20Wenhan%20Luo%20and%20Yike%20Guo&entry.1292438233=Video%20unified%20models%20exhibit%20strong%20capabilities%20in%20understanding%20and%20generation%2C%20yet%20they%20struggle%20with%20reason-informed%20visual%20editing%20even%20when%20equipped%20with%20powerful%20internal%20vision-language%20models%20%28VLMs%29.%20We%20attribute%20this%20gap%20to%20two%20factors%3A%201%29%20existing%20datasets%20are%20inadequate%20for%20training%20and%20evaluating%20reasoning-aware%20video%20editing%2C%20and%202%29%20an%20inherent%20disconnect%20between%20the%20models%27%20reasoning%20and%20editing%20capabilities%2C%20which%20prevents%20the%20rich%20understanding%20from%20effectively%20instructing%20the%20editing%20process.%20Bridging%20this%20gap%20requires%20an%20integrated%20framework%20that%20connects%20reasoning%20with%20visual%20transformation.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20Reason-Informed%20Video%20Editing%20%28RVE%29%20task%2C%20which%20requires%20reasoning%20about%20physical%20plausibility%20and%20causal%20dynamics%20during%20editing.%20To%20support%20systematic%20evaluation%2C%20we%20construct%20RVE-Bench%2C%20a%20comprehensive%20benchmark%20with%20two%20complementary%20subsets%3A%20Reasoning-Informed%20Video%20Editing%20and%20In-Context%20Video%20Generation.%20These%20subsets%20cover%20diverse%20reasoning%20dimensions%20and%20real-world%20editing%20scenarios.%20Building%20upon%20this%20foundation%2C%20we%20propose%20the%20ReViSE%2C%20a%20Self-Reflective%20Reasoning%20%28SRF%29%20framework%20that%20unifies%20generation%20and%20evaluation%20within%20a%20single%20architecture.%20The%20model%27s%20internal%20VLM%20provides%20intrinsic%20feedback%20by%20assessing%20whether%20the%20edited%20video%20logically%20satisfies%20the%20given%20instruction.%20The%20differential%20feedback%20that%20refines%20the%20generator%27s%20reasoning%20behavior%20during%20training.%20Extensive%20experiments%20on%20RVE-Bench%20demonstrate%20that%20ReViSE%20significantly%20enhances%20editing%20accuracy%20and%20visual%20fidelity%2C%20achieving%20a%2032%25%20improvement%20of%20the%20Overall%20score%20in%20the%20reasoning-informed%20video%20editing%20subset%20over%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.09924v1&entry.124074799=Read"},
{"title": "Masked Registration and Autoencoding of CT Images for Predictive Tibia Reconstruction", "author": "Hongyou Zhou and Cederic A\u00dfmann and Alaa Bejaoui and Heiko Tzsch\u00e4tzsch and Mark Heyland and Julian Zierke and Niklas Tuttle and Sebastian H\u00f6lzl and Timo Auer and David A. Back and Marc Toussaint", "abstract": "Surgical planning for complex tibial fractures can be challenging for surgeons, as the 3D structure of the later desirable bone alignment may be diffi- cult to imagine. To assist in such planning, we address the challenge of predicting a patient-specific reconstruction target from a CT of the fractured tibia. Our ap- proach combines neural registration and autoencoder models. Specifically, we first train a modified spatial transformer network (STN) to register a raw CT to a standardized coordinate system of a jointly trained tibia prototype. Subsequently, various autoencoder (AE) architectures are trained to model healthy tibial varia- tions. Both the STN and AE models are further designed to be robust to masked input, allowing us to apply them to fractured CTs and decode to a prediction of the patient-specific healthy bone in standard coordinates. Our contributions include: i) a 3D-adapted STN for global spatial registration, ii) a comparative analysis of AEs for bone CT modeling, and iii) the extension of both to handle masked inputs for predictive generation of healthy bone structures. Project page: https://github.com/HongyouZhou/repair", "link": "http://arxiv.org/abs/2512.09525v1", "date": "2025-12-10", "relevancy": 2.7008, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5572}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5462}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Registration%20and%20Autoencoding%20of%20CT%20Images%20for%20Predictive%20Tibia%20Reconstruction&body=Title%3A%20Masked%20Registration%20and%20Autoencoding%20of%20CT%20Images%20for%20Predictive%20Tibia%20Reconstruction%0AAuthor%3A%20Hongyou%20Zhou%20and%20Cederic%20A%C3%9Fmann%20and%20Alaa%20Bejaoui%20and%20Heiko%20Tzsch%C3%A4tzsch%20and%20Mark%20Heyland%20and%20Julian%20Zierke%20and%20Niklas%20Tuttle%20and%20Sebastian%20H%C3%B6lzl%20and%20Timo%20Auer%20and%20David%20A.%20Back%20and%20Marc%20Toussaint%0AAbstract%3A%20Surgical%20planning%20for%20complex%20tibial%20fractures%20can%20be%20challenging%20for%20surgeons%2C%20as%20the%203D%20structure%20of%20the%20later%20desirable%20bone%20alignment%20may%20be%20diffi-%20cult%20to%20imagine.%20To%20assist%20in%20such%20planning%2C%20we%20address%20the%20challenge%20of%20predicting%20a%20patient-specific%20reconstruction%20target%20from%20a%20CT%20of%20the%20fractured%20tibia.%20Our%20ap-%20proach%20combines%20neural%20registration%20and%20autoencoder%20models.%20Specifically%2C%20we%20first%20train%20a%20modified%20spatial%20transformer%20network%20%28STN%29%20to%20register%20a%20raw%20CT%20to%20a%20standardized%20coordinate%20system%20of%20a%20jointly%20trained%20tibia%20prototype.%20Subsequently%2C%20various%20autoencoder%20%28AE%29%20architectures%20are%20trained%20to%20model%20healthy%20tibial%20varia-%20tions.%20Both%20the%20STN%20and%20AE%20models%20are%20further%20designed%20to%20be%20robust%20to%20masked%20input%2C%20allowing%20us%20to%20apply%20them%20to%20fractured%20CTs%20and%20decode%20to%20a%20prediction%20of%20the%20patient-specific%20healthy%20bone%20in%20standard%20coordinates.%20Our%20contributions%20include%3A%20i%29%20a%203D-adapted%20STN%20for%20global%20spatial%20registration%2C%20ii%29%20a%20comparative%20analysis%20of%20AEs%20for%20bone%20CT%20modeling%2C%20and%20iii%29%20the%20extension%20of%20both%20to%20handle%20masked%20inputs%20for%20predictive%20generation%20of%20healthy%20bone%20structures.%20Project%20page%3A%20https%3A//github.com/HongyouZhou/repair%0ALink%3A%20http%3A//arxiv.org/abs/2512.09525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Registration%2520and%2520Autoencoding%2520of%2520CT%2520Images%2520for%2520Predictive%2520Tibia%2520Reconstruction%26entry.906535625%3DHongyou%2520Zhou%2520and%2520Cederic%2520A%25C3%259Fmann%2520and%2520Alaa%2520Bejaoui%2520and%2520Heiko%2520Tzsch%25C3%25A4tzsch%2520and%2520Mark%2520Heyland%2520and%2520Julian%2520Zierke%2520and%2520Niklas%2520Tuttle%2520and%2520Sebastian%2520H%25C3%25B6lzl%2520and%2520Timo%2520Auer%2520and%2520David%2520A.%2520Back%2520and%2520Marc%2520Toussaint%26entry.1292438233%3DSurgical%2520planning%2520for%2520complex%2520tibial%2520fractures%2520can%2520be%2520challenging%2520for%2520surgeons%252C%2520as%2520the%25203D%2520structure%2520of%2520the%2520later%2520desirable%2520bone%2520alignment%2520may%2520be%2520diffi-%2520cult%2520to%2520imagine.%2520To%2520assist%2520in%2520such%2520planning%252C%2520we%2520address%2520the%2520challenge%2520of%2520predicting%2520a%2520patient-specific%2520reconstruction%2520target%2520from%2520a%2520CT%2520of%2520the%2520fractured%2520tibia.%2520Our%2520ap-%2520proach%2520combines%2520neural%2520registration%2520and%2520autoencoder%2520models.%2520Specifically%252C%2520we%2520first%2520train%2520a%2520modified%2520spatial%2520transformer%2520network%2520%2528STN%2529%2520to%2520register%2520a%2520raw%2520CT%2520to%2520a%2520standardized%2520coordinate%2520system%2520of%2520a%2520jointly%2520trained%2520tibia%2520prototype.%2520Subsequently%252C%2520various%2520autoencoder%2520%2528AE%2529%2520architectures%2520are%2520trained%2520to%2520model%2520healthy%2520tibial%2520varia-%2520tions.%2520Both%2520the%2520STN%2520and%2520AE%2520models%2520are%2520further%2520designed%2520to%2520be%2520robust%2520to%2520masked%2520input%252C%2520allowing%2520us%2520to%2520apply%2520them%2520to%2520fractured%2520CTs%2520and%2520decode%2520to%2520a%2520prediction%2520of%2520the%2520patient-specific%2520healthy%2520bone%2520in%2520standard%2520coordinates.%2520Our%2520contributions%2520include%253A%2520i%2529%2520a%25203D-adapted%2520STN%2520for%2520global%2520spatial%2520registration%252C%2520ii%2529%2520a%2520comparative%2520analysis%2520of%2520AEs%2520for%2520bone%2520CT%2520modeling%252C%2520and%2520iii%2529%2520the%2520extension%2520of%2520both%2520to%2520handle%2520masked%2520inputs%2520for%2520predictive%2520generation%2520of%2520healthy%2520bone%2520structures.%2520Project%2520page%253A%2520https%253A//github.com/HongyouZhou/repair%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Registration%20and%20Autoencoding%20of%20CT%20Images%20for%20Predictive%20Tibia%20Reconstruction&entry.906535625=Hongyou%20Zhou%20and%20Cederic%20A%C3%9Fmann%20and%20Alaa%20Bejaoui%20and%20Heiko%20Tzsch%C3%A4tzsch%20and%20Mark%20Heyland%20and%20Julian%20Zierke%20and%20Niklas%20Tuttle%20and%20Sebastian%20H%C3%B6lzl%20and%20Timo%20Auer%20and%20David%20A.%20Back%20and%20Marc%20Toussaint&entry.1292438233=Surgical%20planning%20for%20complex%20tibial%20fractures%20can%20be%20challenging%20for%20surgeons%2C%20as%20the%203D%20structure%20of%20the%20later%20desirable%20bone%20alignment%20may%20be%20diffi-%20cult%20to%20imagine.%20To%20assist%20in%20such%20planning%2C%20we%20address%20the%20challenge%20of%20predicting%20a%20patient-specific%20reconstruction%20target%20from%20a%20CT%20of%20the%20fractured%20tibia.%20Our%20ap-%20proach%20combines%20neural%20registration%20and%20autoencoder%20models.%20Specifically%2C%20we%20first%20train%20a%20modified%20spatial%20transformer%20network%20%28STN%29%20to%20register%20a%20raw%20CT%20to%20a%20standardized%20coordinate%20system%20of%20a%20jointly%20trained%20tibia%20prototype.%20Subsequently%2C%20various%20autoencoder%20%28AE%29%20architectures%20are%20trained%20to%20model%20healthy%20tibial%20varia-%20tions.%20Both%20the%20STN%20and%20AE%20models%20are%20further%20designed%20to%20be%20robust%20to%20masked%20input%2C%20allowing%20us%20to%20apply%20them%20to%20fractured%20CTs%20and%20decode%20to%20a%20prediction%20of%20the%20patient-specific%20healthy%20bone%20in%20standard%20coordinates.%20Our%20contributions%20include%3A%20i%29%20a%203D-adapted%20STN%20for%20global%20spatial%20registration%2C%20ii%29%20a%20comparative%20analysis%20of%20AEs%20for%20bone%20CT%20modeling%2C%20and%20iii%29%20the%20extension%20of%20both%20to%20handle%20masked%20inputs%20for%20predictive%20generation%20of%20healthy%20bone%20structures.%20Project%20page%3A%20https%3A//github.com/HongyouZhou/repair&entry.1838667208=http%3A//arxiv.org/abs/2512.09525v1&entry.124074799=Read"},
{"title": "ChronusOmni: Improving Time Awareness of Omni Large Language Models", "author": "Yijing Chen and Yihan Wu and Kaisi Guan and Yuchen Ren and Yuyue Wang and Ruihua Song and Liyun Ru", "abstract": "Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.", "link": "http://arxiv.org/abs/2512.09841v1", "date": "2025-12-10", "relevancy": 2.6974, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5582}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChronusOmni%3A%20Improving%20Time%20Awareness%20of%20Omni%20Large%20Language%20Models&body=Title%3A%20ChronusOmni%3A%20Improving%20Time%20Awareness%20of%20Omni%20Large%20Language%20Models%0AAuthor%3A%20Yijing%20Chen%20and%20Yihan%20Wu%20and%20Kaisi%20Guan%20and%20Yuchen%20Ren%20and%20Yuyue%20Wang%20and%20Ruihua%20Song%20and%20Liyun%20Ru%0AAbstract%3A%20Time%20awareness%20is%20a%20fundamental%20ability%20of%20omni%20large%20language%20models%2C%20especially%20for%20understanding%20long%20videos%20and%20answering%20complex%20questions.%20Previous%20approaches%20mainly%20target%20vision-language%20scenarios%20and%20focus%20on%20the%20explicit%20temporal%20grounding%20questions%2C%20such%20as%20identifying%20when%20a%20visual%20event%20occurs%20or%20determining%20what%20event%20happens%20at%20aspecific%20time.%20However%2C%20they%20often%20make%20insufficient%20use%20of%20the%20audio%20modality%2C%20and%20overlook%20implicit%20temporal%20grounding%20across%20modalities--for%20example%2C%20identifying%20what%20is%20visually%20present%20when%20a%20character%20speaks%2C%20or%20determining%20what%20is%20said%20when%20a%20visual%20event%20occurs--despite%20such%20cross-modal%20temporal%20relations%20being%20prevalent%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20propose%20ChronusOmni%2C%20an%20omni%20large%20language%20model%20designed%20to%20enhance%20temporal%20awareness%20for%20both%20explicit%20and%20implicit%20audiovisual%20temporal%20grounding.%20First%2C%20we%20interleave%20text-based%20timestamp%20tokens%20with%20visual%20and%20audio%20representations%20at%20each%20time%20unit%2C%20enabling%20unified%20temporal%20modeling%20across%20modalities.%20Second%2C%20to%20enforce%20correct%20temporal%20ordering%20and%20strengthen%20fine-grained%20temporal%20reasoning%2C%20we%20incorporate%20reinforcement%20learning%20with%20specially%20designed%20reward%20functions.%20Moreover%2C%20we%20construct%20ChronusAV%2C%20a%20temporally-accurate%2C%20modality-complete%2C%20and%20cross-modal-aligned%20dataset%20to%20support%20the%20training%20and%20evaluation%20on%20audiovisual%20temporal%20grounding%20task.%20Experimental%20results%20demonstrate%20that%20ChronusOmni%20achieves%20state-of-the-art%20performance%20on%20ChronusAV%20with%20more%20than%2030%25%20improvement%20and%20top%20results%20on%20most%20metrics%20upon%20other%20temporal%20grounding%20benchmarks.%20This%20highlights%20the%20strong%20temporal%20awareness%20of%20our%20model%20across%20modalities%2C%20while%20preserving%20general%20video%20and%20audio%20understanding%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChronusOmni%253A%2520Improving%2520Time%2520Awareness%2520of%2520Omni%2520Large%2520Language%2520Models%26entry.906535625%3DYijing%2520Chen%2520and%2520Yihan%2520Wu%2520and%2520Kaisi%2520Guan%2520and%2520Yuchen%2520Ren%2520and%2520Yuyue%2520Wang%2520and%2520Ruihua%2520Song%2520and%2520Liyun%2520Ru%26entry.1292438233%3DTime%2520awareness%2520is%2520a%2520fundamental%2520ability%2520of%2520omni%2520large%2520language%2520models%252C%2520especially%2520for%2520understanding%2520long%2520videos%2520and%2520answering%2520complex%2520questions.%2520Previous%2520approaches%2520mainly%2520target%2520vision-language%2520scenarios%2520and%2520focus%2520on%2520the%2520explicit%2520temporal%2520grounding%2520questions%252C%2520such%2520as%2520identifying%2520when%2520a%2520visual%2520event%2520occurs%2520or%2520determining%2520what%2520event%2520happens%2520at%2520aspecific%2520time.%2520However%252C%2520they%2520often%2520make%2520insufficient%2520use%2520of%2520the%2520audio%2520modality%252C%2520and%2520overlook%2520implicit%2520temporal%2520grounding%2520across%2520modalities--for%2520example%252C%2520identifying%2520what%2520is%2520visually%2520present%2520when%2520a%2520character%2520speaks%252C%2520or%2520determining%2520what%2520is%2520said%2520when%2520a%2520visual%2520event%2520occurs--despite%2520such%2520cross-modal%2520temporal%2520relations%2520being%2520prevalent%2520in%2520real-world%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ChronusOmni%252C%2520an%2520omni%2520large%2520language%2520model%2520designed%2520to%2520enhance%2520temporal%2520awareness%2520for%2520both%2520explicit%2520and%2520implicit%2520audiovisual%2520temporal%2520grounding.%2520First%252C%2520we%2520interleave%2520text-based%2520timestamp%2520tokens%2520with%2520visual%2520and%2520audio%2520representations%2520at%2520each%2520time%2520unit%252C%2520enabling%2520unified%2520temporal%2520modeling%2520across%2520modalities.%2520Second%252C%2520to%2520enforce%2520correct%2520temporal%2520ordering%2520and%2520strengthen%2520fine-grained%2520temporal%2520reasoning%252C%2520we%2520incorporate%2520reinforcement%2520learning%2520with%2520specially%2520designed%2520reward%2520functions.%2520Moreover%252C%2520we%2520construct%2520ChronusAV%252C%2520a%2520temporally-accurate%252C%2520modality-complete%252C%2520and%2520cross-modal-aligned%2520dataset%2520to%2520support%2520the%2520training%2520and%2520evaluation%2520on%2520audiovisual%2520temporal%2520grounding%2520task.%2520Experimental%2520results%2520demonstrate%2520that%2520ChronusOmni%2520achieves%2520state-of-the-art%2520performance%2520on%2520ChronusAV%2520with%2520more%2520than%252030%2525%2520improvement%2520and%2520top%2520results%2520on%2520most%2520metrics%2520upon%2520other%2520temporal%2520grounding%2520benchmarks.%2520This%2520highlights%2520the%2520strong%2520temporal%2520awareness%2520of%2520our%2520model%2520across%2520modalities%252C%2520while%2520preserving%2520general%2520video%2520and%2520audio%2520understanding%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChronusOmni%3A%20Improving%20Time%20Awareness%20of%20Omni%20Large%20Language%20Models&entry.906535625=Yijing%20Chen%20and%20Yihan%20Wu%20and%20Kaisi%20Guan%20and%20Yuchen%20Ren%20and%20Yuyue%20Wang%20and%20Ruihua%20Song%20and%20Liyun%20Ru&entry.1292438233=Time%20awareness%20is%20a%20fundamental%20ability%20of%20omni%20large%20language%20models%2C%20especially%20for%20understanding%20long%20videos%20and%20answering%20complex%20questions.%20Previous%20approaches%20mainly%20target%20vision-language%20scenarios%20and%20focus%20on%20the%20explicit%20temporal%20grounding%20questions%2C%20such%20as%20identifying%20when%20a%20visual%20event%20occurs%20or%20determining%20what%20event%20happens%20at%20aspecific%20time.%20However%2C%20they%20often%20make%20insufficient%20use%20of%20the%20audio%20modality%2C%20and%20overlook%20implicit%20temporal%20grounding%20across%20modalities--for%20example%2C%20identifying%20what%20is%20visually%20present%20when%20a%20character%20speaks%2C%20or%20determining%20what%20is%20said%20when%20a%20visual%20event%20occurs--despite%20such%20cross-modal%20temporal%20relations%20being%20prevalent%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20propose%20ChronusOmni%2C%20an%20omni%20large%20language%20model%20designed%20to%20enhance%20temporal%20awareness%20for%20both%20explicit%20and%20implicit%20audiovisual%20temporal%20grounding.%20First%2C%20we%20interleave%20text-based%20timestamp%20tokens%20with%20visual%20and%20audio%20representations%20at%20each%20time%20unit%2C%20enabling%20unified%20temporal%20modeling%20across%20modalities.%20Second%2C%20to%20enforce%20correct%20temporal%20ordering%20and%20strengthen%20fine-grained%20temporal%20reasoning%2C%20we%20incorporate%20reinforcement%20learning%20with%20specially%20designed%20reward%20functions.%20Moreover%2C%20we%20construct%20ChronusAV%2C%20a%20temporally-accurate%2C%20modality-complete%2C%20and%20cross-modal-aligned%20dataset%20to%20support%20the%20training%20and%20evaluation%20on%20audiovisual%20temporal%20grounding%20task.%20Experimental%20results%20demonstrate%20that%20ChronusOmni%20achieves%20state-of-the-art%20performance%20on%20ChronusAV%20with%20more%20than%2030%25%20improvement%20and%20top%20results%20on%20most%20metrics%20upon%20other%20temporal%20grounding%20benchmarks.%20This%20highlights%20the%20strong%20temporal%20awareness%20of%20our%20model%20across%20modalities%2C%20while%20preserving%20general%20video%20and%20audio%20understanding%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2512.09841v1&entry.124074799=Read"},
{"title": "Gradient-Guided Learning Network for Infrared Small Target Detection", "author": "Jinmiao Zhao and Chuang Yu and Zelin Shi and Yunpeng Liu and Yingdi Zhang", "abstract": "Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net", "link": "http://arxiv.org/abs/2512.09497v1", "date": "2025-12-10", "relevancy": 2.6665, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5473}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5294}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-Guided%20Learning%20Network%20for%20Infrared%20Small%20Target%20Detection&body=Title%3A%20Gradient-Guided%20Learning%20Network%20for%20Infrared%20Small%20Target%20Detection%0AAuthor%3A%20Jinmiao%20Zhao%20and%20Chuang%20Yu%20and%20Zelin%20Shi%20and%20Yunpeng%20Liu%20and%20Yingdi%20Zhang%0AAbstract%3A%20Recently%2C%20infrared%20small%20target%20detection%20has%20attracted%20extensive%20attention.%20However%2C%20due%20to%20the%20small%20size%20and%20the%20lack%20of%20intrinsic%20features%20of%20infrared%20small%20targets%2C%20the%20existing%20methods%20generally%20have%20the%20problem%20of%20inaccurate%20edge%20positioning%20and%20the%20target%20is%20easily%20submerged%20by%20the%20background.%20Therefore%2C%20we%20propose%20an%20innovative%20gradient-guided%20learning%20network%20%28GGL-Net%29.%20Specifically%2C%20we%20are%20the%20first%20to%20explore%20the%20introduction%20of%20gradient%20magnitude%20images%20into%20the%20deep%20learning-based%20infrared%20small%20target%20detection%20method%2C%20which%20is%20conducive%20to%20emphasizing%20the%20edge%20details%20and%20alleviating%20the%20problem%20of%20inaccurate%20edge%20positioning%20of%20small%20targets.%20On%20this%20basis%2C%20we%20propose%20a%20novel%20dual-branch%20feature%20extraction%20network%20that%20utilizes%20the%20proposed%20gradient%20supplementary%20module%20%28GSM%29%20to%20encode%20raw%20gradient%20information%20into%20deeper%20network%20layers%20and%20embeds%20attention%20mechanisms%20reasonably%20to%20enhance%20feature%20extraction%20ability.%20In%20addition%2C%20we%20construct%20a%20two-way%20guidance%20fusion%20module%20%28TGFM%29%2C%20which%20fully%20considers%20the%20characteristics%20of%20feature%20maps%20at%20different%20levels.%20It%20can%20facilitate%20the%20effective%20fusion%20of%20multi-scale%20feature%20maps%20and%20extract%20richer%20semantic%20information%20and%20detailed%20information%20through%20reasonable%20two-way%20guidance.%20Extensive%20experiments%20prove%20that%20GGL-Net%20has%20achieves%20state-of-the-art%20results%20on%20the%20public%20real%20NUAA-SIRST%20dataset%20and%20the%20public%20synthetic%20NUDT-SIRST%20dataset.%20Our%20code%20has%20been%20integrated%20into%20https%3A//github.com/YuChuang1205/MSDA-Net%0ALink%3A%20http%3A//arxiv.org/abs/2512.09497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-Guided%2520Learning%2520Network%2520for%2520Infrared%2520Small%2520Target%2520Detection%26entry.906535625%3DJinmiao%2520Zhao%2520and%2520Chuang%2520Yu%2520and%2520Zelin%2520Shi%2520and%2520Yunpeng%2520Liu%2520and%2520Yingdi%2520Zhang%26entry.1292438233%3DRecently%252C%2520infrared%2520small%2520target%2520detection%2520has%2520attracted%2520extensive%2520attention.%2520However%252C%2520due%2520to%2520the%2520small%2520size%2520and%2520the%2520lack%2520of%2520intrinsic%2520features%2520of%2520infrared%2520small%2520targets%252C%2520the%2520existing%2520methods%2520generally%2520have%2520the%2520problem%2520of%2520inaccurate%2520edge%2520positioning%2520and%2520the%2520target%2520is%2520easily%2520submerged%2520by%2520the%2520background.%2520Therefore%252C%2520we%2520propose%2520an%2520innovative%2520gradient-guided%2520learning%2520network%2520%2528GGL-Net%2529.%2520Specifically%252C%2520we%2520are%2520the%2520first%2520to%2520explore%2520the%2520introduction%2520of%2520gradient%2520magnitude%2520images%2520into%2520the%2520deep%2520learning-based%2520infrared%2520small%2520target%2520detection%2520method%252C%2520which%2520is%2520conducive%2520to%2520emphasizing%2520the%2520edge%2520details%2520and%2520alleviating%2520the%2520problem%2520of%2520inaccurate%2520edge%2520positioning%2520of%2520small%2520targets.%2520On%2520this%2520basis%252C%2520we%2520propose%2520a%2520novel%2520dual-branch%2520feature%2520extraction%2520network%2520that%2520utilizes%2520the%2520proposed%2520gradient%2520supplementary%2520module%2520%2528GSM%2529%2520to%2520encode%2520raw%2520gradient%2520information%2520into%2520deeper%2520network%2520layers%2520and%2520embeds%2520attention%2520mechanisms%2520reasonably%2520to%2520enhance%2520feature%2520extraction%2520ability.%2520In%2520addition%252C%2520we%2520construct%2520a%2520two-way%2520guidance%2520fusion%2520module%2520%2528TGFM%2529%252C%2520which%2520fully%2520considers%2520the%2520characteristics%2520of%2520feature%2520maps%2520at%2520different%2520levels.%2520It%2520can%2520facilitate%2520the%2520effective%2520fusion%2520of%2520multi-scale%2520feature%2520maps%2520and%2520extract%2520richer%2520semantic%2520information%2520and%2520detailed%2520information%2520through%2520reasonable%2520two-way%2520guidance.%2520Extensive%2520experiments%2520prove%2520that%2520GGL-Net%2520has%2520achieves%2520state-of-the-art%2520results%2520on%2520the%2520public%2520real%2520NUAA-SIRST%2520dataset%2520and%2520the%2520public%2520synthetic%2520NUDT-SIRST%2520dataset.%2520Our%2520code%2520has%2520been%2520integrated%2520into%2520https%253A//github.com/YuChuang1205/MSDA-Net%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-Guided%20Learning%20Network%20for%20Infrared%20Small%20Target%20Detection&entry.906535625=Jinmiao%20Zhao%20and%20Chuang%20Yu%20and%20Zelin%20Shi%20and%20Yunpeng%20Liu%20and%20Yingdi%20Zhang&entry.1292438233=Recently%2C%20infrared%20small%20target%20detection%20has%20attracted%20extensive%20attention.%20However%2C%20due%20to%20the%20small%20size%20and%20the%20lack%20of%20intrinsic%20features%20of%20infrared%20small%20targets%2C%20the%20existing%20methods%20generally%20have%20the%20problem%20of%20inaccurate%20edge%20positioning%20and%20the%20target%20is%20easily%20submerged%20by%20the%20background.%20Therefore%2C%20we%20propose%20an%20innovative%20gradient-guided%20learning%20network%20%28GGL-Net%29.%20Specifically%2C%20we%20are%20the%20first%20to%20explore%20the%20introduction%20of%20gradient%20magnitude%20images%20into%20the%20deep%20learning-based%20infrared%20small%20target%20detection%20method%2C%20which%20is%20conducive%20to%20emphasizing%20the%20edge%20details%20and%20alleviating%20the%20problem%20of%20inaccurate%20edge%20positioning%20of%20small%20targets.%20On%20this%20basis%2C%20we%20propose%20a%20novel%20dual-branch%20feature%20extraction%20network%20that%20utilizes%20the%20proposed%20gradient%20supplementary%20module%20%28GSM%29%20to%20encode%20raw%20gradient%20information%20into%20deeper%20network%20layers%20and%20embeds%20attention%20mechanisms%20reasonably%20to%20enhance%20feature%20extraction%20ability.%20In%20addition%2C%20we%20construct%20a%20two-way%20guidance%20fusion%20module%20%28TGFM%29%2C%20which%20fully%20considers%20the%20characteristics%20of%20feature%20maps%20at%20different%20levels.%20It%20can%20facilitate%20the%20effective%20fusion%20of%20multi-scale%20feature%20maps%20and%20extract%20richer%20semantic%20information%20and%20detailed%20information%20through%20reasonable%20two-way%20guidance.%20Extensive%20experiments%20prove%20that%20GGL-Net%20has%20achieves%20state-of-the-art%20results%20on%20the%20public%20real%20NUAA-SIRST%20dataset%20and%20the%20public%20synthetic%20NUDT-SIRST%20dataset.%20Our%20code%20has%20been%20integrated%20into%20https%3A//github.com/YuChuang1205/MSDA-Net&entry.1838667208=http%3A//arxiv.org/abs/2512.09497v1&entry.124074799=Read"},
{"title": "SynthPix: A lightspeed PIV images generator", "author": "Antonio Terpin and Alan Bonomi and Francesco Banelli and Raffaello D'Andrea", "abstract": "We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.", "link": "http://arxiv.org/abs/2512.09664v1", "date": "2025-12-10", "relevancy": 2.6656, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5829}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5082}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynthPix%3A%20A%20lightspeed%20PIV%20images%20generator&body=Title%3A%20SynthPix%3A%20A%20lightspeed%20PIV%20images%20generator%0AAuthor%3A%20Antonio%20Terpin%20and%20Alan%20Bonomi%20and%20Francesco%20Banelli%20and%20Raffaello%20D%27Andrea%0AAbstract%3A%20We%20describe%20SynthPix%2C%20a%20synthetic%20image%20generator%20for%20Particle%20Image%20Velocimetry%20%28PIV%29%20with%20a%20focus%20on%20performance%20and%20parallelism%20on%20accelerators%2C%20implemented%20in%20JAX.%20SynthPix%20supports%20the%20same%20configuration%20parameters%20as%20existing%20tools%20but%20achieves%20a%20throughput%20several%20orders%20of%20magnitude%20higher%20in%20image-pair%20generation%20per%20second.%20SynthPix%20was%20developed%20to%20enable%20the%20training%20of%20data-hungry%20reinforcement%20learning%20methods%20for%20flow%20estimation%20and%20for%20reducing%20the%20iteration%20times%20during%20the%20development%20of%20fast%20flow%20estimation%20methods%20used%20in%20recent%20active%20fluids%20control%20studies%20with%20real-time%20PIV%20feedback.%20We%20believe%20SynthPix%20to%20be%20useful%20for%20the%20fluid%20dynamics%20community%2C%20and%20in%20this%20paper%20we%20describe%20the%20main%20ideas%20behind%20this%20software%20package.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthPix%253A%2520A%2520lightspeed%2520PIV%2520images%2520generator%26entry.906535625%3DAntonio%2520Terpin%2520and%2520Alan%2520Bonomi%2520and%2520Francesco%2520Banelli%2520and%2520Raffaello%2520D%2527Andrea%26entry.1292438233%3DWe%2520describe%2520SynthPix%252C%2520a%2520synthetic%2520image%2520generator%2520for%2520Particle%2520Image%2520Velocimetry%2520%2528PIV%2529%2520with%2520a%2520focus%2520on%2520performance%2520and%2520parallelism%2520on%2520accelerators%252C%2520implemented%2520in%2520JAX.%2520SynthPix%2520supports%2520the%2520same%2520configuration%2520parameters%2520as%2520existing%2520tools%2520but%2520achieves%2520a%2520throughput%2520several%2520orders%2520of%2520magnitude%2520higher%2520in%2520image-pair%2520generation%2520per%2520second.%2520SynthPix%2520was%2520developed%2520to%2520enable%2520the%2520training%2520of%2520data-hungry%2520reinforcement%2520learning%2520methods%2520for%2520flow%2520estimation%2520and%2520for%2520reducing%2520the%2520iteration%2520times%2520during%2520the%2520development%2520of%2520fast%2520flow%2520estimation%2520methods%2520used%2520in%2520recent%2520active%2520fluids%2520control%2520studies%2520with%2520real-time%2520PIV%2520feedback.%2520We%2520believe%2520SynthPix%2520to%2520be%2520useful%2520for%2520the%2520fluid%2520dynamics%2520community%252C%2520and%2520in%2520this%2520paper%2520we%2520describe%2520the%2520main%2520ideas%2520behind%2520this%2520software%2520package.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynthPix%3A%20A%20lightspeed%20PIV%20images%20generator&entry.906535625=Antonio%20Terpin%20and%20Alan%20Bonomi%20and%20Francesco%20Banelli%20and%20Raffaello%20D%27Andrea&entry.1292438233=We%20describe%20SynthPix%2C%20a%20synthetic%20image%20generator%20for%20Particle%20Image%20Velocimetry%20%28PIV%29%20with%20a%20focus%20on%20performance%20and%20parallelism%20on%20accelerators%2C%20implemented%20in%20JAX.%20SynthPix%20supports%20the%20same%20configuration%20parameters%20as%20existing%20tools%20but%20achieves%20a%20throughput%20several%20orders%20of%20magnitude%20higher%20in%20image-pair%20generation%20per%20second.%20SynthPix%20was%20developed%20to%20enable%20the%20training%20of%20data-hungry%20reinforcement%20learning%20methods%20for%20flow%20estimation%20and%20for%20reducing%20the%20iteration%20times%20during%20the%20development%20of%20fast%20flow%20estimation%20methods%20used%20in%20recent%20active%20fluids%20control%20studies%20with%20real-time%20PIV%20feedback.%20We%20believe%20SynthPix%20to%20be%20useful%20for%20the%20fluid%20dynamics%20community%2C%20and%20in%20this%20paper%20we%20describe%20the%20main%20ideas%20behind%20this%20software%20package.&entry.1838667208=http%3A//arxiv.org/abs/2512.09664v1&entry.124074799=Read"},
{"title": "Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics, Revealing a Three-Stage In-Context Learning Mechanism", "author": "Jiajun Bao and Nicolas Boull\u00e9 and Toni J. B. Liu and Rapha\u00ebl Sarfati and Christopher J. Earls", "abstract": "Large language models (LLMs) have demonstrated emergent in-context learning (ICL) capabilities across a range of tasks, including zero-shot time-series forecasting. We show that text-trained foundation models can accurately extrapolate spatiotemporal dynamics from discretized partial differential equation (PDE) solutions without fine-tuning or natural language prompting. Predictive accuracy improves with longer temporal contexts but degrades at finer spatial discretizations. In multi-step rollouts, where the model recursively predicts future spatial states over multiple time steps, errors grow algebraically with the time horizon, reminiscent of global error accumulation in classical finite-difference solvers. We interpret these trends as in-context neural scaling laws, where prediction quality varies predictably with both context length and output length. To better understand how LLMs are able to internally process PDE solutions so as to accurately roll them out, we analyze token-level output distributions and uncover a consistent three-stage ICL progression: beginning with syntactic pattern imitation, transitioning through an exploratory high-entropy phase, and culminating in confident, numerically grounded predictions.", "link": "http://arxiv.org/abs/2509.06322v2", "date": "2025-12-10", "relevancy": 2.6509, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5339}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5339}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Trained%20LLMs%20Can%20Zero-Shot%20Extrapolate%20PDE%20Dynamics%2C%20Revealing%20a%20Three-Stage%20In-Context%20Learning%20Mechanism&body=Title%3A%20Text-Trained%20LLMs%20Can%20Zero-Shot%20Extrapolate%20PDE%20Dynamics%2C%20Revealing%20a%20Three-Stage%20In-Context%20Learning%20Mechanism%0AAuthor%3A%20Jiajun%20Bao%20and%20Nicolas%20Boull%C3%A9%20and%20Toni%20J.%20B.%20Liu%20and%20Rapha%C3%ABl%20Sarfati%20and%20Christopher%20J.%20Earls%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20emergent%20in-context%20learning%20%28ICL%29%20capabilities%20across%20a%20range%20of%20tasks%2C%20including%20zero-shot%20time-series%20forecasting.%20We%20show%20that%20text-trained%20foundation%20models%20can%20accurately%20extrapolate%20spatiotemporal%20dynamics%20from%20discretized%20partial%20differential%20equation%20%28PDE%29%20solutions%20without%20fine-tuning%20or%20natural%20language%20prompting.%20Predictive%20accuracy%20improves%20with%20longer%20temporal%20contexts%20but%20degrades%20at%20finer%20spatial%20discretizations.%20In%20multi-step%20rollouts%2C%20where%20the%20model%20recursively%20predicts%20future%20spatial%20states%20over%20multiple%20time%20steps%2C%20errors%20grow%20algebraically%20with%20the%20time%20horizon%2C%20reminiscent%20of%20global%20error%20accumulation%20in%20classical%20finite-difference%20solvers.%20We%20interpret%20these%20trends%20as%20in-context%20neural%20scaling%20laws%2C%20where%20prediction%20quality%20varies%20predictably%20with%20both%20context%20length%20and%20output%20length.%20To%20better%20understand%20how%20LLMs%20are%20able%20to%20internally%20process%20PDE%20solutions%20so%20as%20to%20accurately%20roll%20them%20out%2C%20we%20analyze%20token-level%20output%20distributions%20and%20uncover%20a%20consistent%20three-stage%20ICL%20progression%3A%20beginning%20with%20syntactic%20pattern%20imitation%2C%20transitioning%20through%20an%20exploratory%20high-entropy%20phase%2C%20and%20culminating%20in%20confident%2C%20numerically%20grounded%20predictions.%0ALink%3A%20http%3A//arxiv.org/abs/2509.06322v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Trained%2520LLMs%2520Can%2520Zero-Shot%2520Extrapolate%2520PDE%2520Dynamics%252C%2520Revealing%2520a%2520Three-Stage%2520In-Context%2520Learning%2520Mechanism%26entry.906535625%3DJiajun%2520Bao%2520and%2520Nicolas%2520Boull%25C3%25A9%2520and%2520Toni%2520J.%2520B.%2520Liu%2520and%2520Rapha%25C3%25ABl%2520Sarfati%2520and%2520Christopher%2520J.%2520Earls%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520emergent%2520in-context%2520learning%2520%2528ICL%2529%2520capabilities%2520across%2520a%2520range%2520of%2520tasks%252C%2520including%2520zero-shot%2520time-series%2520forecasting.%2520We%2520show%2520that%2520text-trained%2520foundation%2520models%2520can%2520accurately%2520extrapolate%2520spatiotemporal%2520dynamics%2520from%2520discretized%2520partial%2520differential%2520equation%2520%2528PDE%2529%2520solutions%2520without%2520fine-tuning%2520or%2520natural%2520language%2520prompting.%2520Predictive%2520accuracy%2520improves%2520with%2520longer%2520temporal%2520contexts%2520but%2520degrades%2520at%2520finer%2520spatial%2520discretizations.%2520In%2520multi-step%2520rollouts%252C%2520where%2520the%2520model%2520recursively%2520predicts%2520future%2520spatial%2520states%2520over%2520multiple%2520time%2520steps%252C%2520errors%2520grow%2520algebraically%2520with%2520the%2520time%2520horizon%252C%2520reminiscent%2520of%2520global%2520error%2520accumulation%2520in%2520classical%2520finite-difference%2520solvers.%2520We%2520interpret%2520these%2520trends%2520as%2520in-context%2520neural%2520scaling%2520laws%252C%2520where%2520prediction%2520quality%2520varies%2520predictably%2520with%2520both%2520context%2520length%2520and%2520output%2520length.%2520To%2520better%2520understand%2520how%2520LLMs%2520are%2520able%2520to%2520internally%2520process%2520PDE%2520solutions%2520so%2520as%2520to%2520accurately%2520roll%2520them%2520out%252C%2520we%2520analyze%2520token-level%2520output%2520distributions%2520and%2520uncover%2520a%2520consistent%2520three-stage%2520ICL%2520progression%253A%2520beginning%2520with%2520syntactic%2520pattern%2520imitation%252C%2520transitioning%2520through%2520an%2520exploratory%2520high-entropy%2520phase%252C%2520and%2520culminating%2520in%2520confident%252C%2520numerically%2520grounded%2520predictions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06322v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Trained%20LLMs%20Can%20Zero-Shot%20Extrapolate%20PDE%20Dynamics%2C%20Revealing%20a%20Three-Stage%20In-Context%20Learning%20Mechanism&entry.906535625=Jiajun%20Bao%20and%20Nicolas%20Boull%C3%A9%20and%20Toni%20J.%20B.%20Liu%20and%20Rapha%C3%ABl%20Sarfati%20and%20Christopher%20J.%20Earls&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20emergent%20in-context%20learning%20%28ICL%29%20capabilities%20across%20a%20range%20of%20tasks%2C%20including%20zero-shot%20time-series%20forecasting.%20We%20show%20that%20text-trained%20foundation%20models%20can%20accurately%20extrapolate%20spatiotemporal%20dynamics%20from%20discretized%20partial%20differential%20equation%20%28PDE%29%20solutions%20without%20fine-tuning%20or%20natural%20language%20prompting.%20Predictive%20accuracy%20improves%20with%20longer%20temporal%20contexts%20but%20degrades%20at%20finer%20spatial%20discretizations.%20In%20multi-step%20rollouts%2C%20where%20the%20model%20recursively%20predicts%20future%20spatial%20states%20over%20multiple%20time%20steps%2C%20errors%20grow%20algebraically%20with%20the%20time%20horizon%2C%20reminiscent%20of%20global%20error%20accumulation%20in%20classical%20finite-difference%20solvers.%20We%20interpret%20these%20trends%20as%20in-context%20neural%20scaling%20laws%2C%20where%20prediction%20quality%20varies%20predictably%20with%20both%20context%20length%20and%20output%20length.%20To%20better%20understand%20how%20LLMs%20are%20able%20to%20internally%20process%20PDE%20solutions%20so%20as%20to%20accurately%20roll%20them%20out%2C%20we%20analyze%20token-level%20output%20distributions%20and%20uncover%20a%20consistent%20three-stage%20ICL%20progression%3A%20beginning%20with%20syntactic%20pattern%20imitation%2C%20transitioning%20through%20an%20exploratory%20high-entropy%20phase%2C%20and%20culminating%20in%20confident%2C%20numerically%20grounded%20predictions.&entry.1838667208=http%3A//arxiv.org/abs/2509.06322v2&entry.124074799=Read"},
{"title": "FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation", "author": "Hubert Kompanowski and Varun Jampani and Aaryaman Vasishta and Binh-Son Hua", "abstract": "Multiview diffusion models have rapidly emerged as a powerful tool for content creation with spatial consistency across viewpoints, offering rich visual realism without requiring explicit geometry and appearance representation. However, compared to meshes or radiance fields, existing multiview diffusion models offer limited appearance manipulation, particularly in terms of material, texture, or style.\n  In this paper, we present a lightweight adaptation technique for appearance transfer in multiview diffusion models. Our method learns to combine object identity from an input image with appearance cues rendered in a separate reference image, producing multi-view-consistent output that reflects the desired materials, textures, or styles. This allows explicit specification of appearance parameters at generation time while preserving the underlying object geometry and view coherence. We leverage three diffusion denoising processes responsible for generating the original object, the reference, and the target images, and perform reverse sampling to aggregate a small subset of layer-wise self-attention features from the object and the reference to influence the target generation. Our method requires only a few training examples to introduce appearance awareness to pretrained multiview models. The experiments show that our method provides a simple yet effective way toward multiview generation with diverse appearance, advocating the adoption of implicit generative 3D representations in practice.", "link": "http://arxiv.org/abs/2512.09617v1", "date": "2025-12-10", "relevancy": 2.6467, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6642}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6642}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FROMAT%3A%20Multiview%20Material%20Appearance%20Transfer%20via%20Few-Shot%20Self-Attention%20Adaptation&body=Title%3A%20FROMAT%3A%20Multiview%20Material%20Appearance%20Transfer%20via%20Few-Shot%20Self-Attention%20Adaptation%0AAuthor%3A%20Hubert%20Kompanowski%20and%20Varun%20Jampani%20and%20Aaryaman%20Vasishta%20and%20Binh-Son%20Hua%0AAbstract%3A%20Multiview%20diffusion%20models%20have%20rapidly%20emerged%20as%20a%20powerful%20tool%20for%20content%20creation%20with%20spatial%20consistency%20across%20viewpoints%2C%20offering%20rich%20visual%20realism%20without%20requiring%20explicit%20geometry%20and%20appearance%20representation.%20However%2C%20compared%20to%20meshes%20or%20radiance%20fields%2C%20existing%20multiview%20diffusion%20models%20offer%20limited%20appearance%20manipulation%2C%20particularly%20in%20terms%20of%20material%2C%20texture%2C%20or%20style.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20lightweight%20adaptation%20technique%20for%20appearance%20transfer%20in%20multiview%20diffusion%20models.%20Our%20method%20learns%20to%20combine%20object%20identity%20from%20an%20input%20image%20with%20appearance%20cues%20rendered%20in%20a%20separate%20reference%20image%2C%20producing%20multi-view-consistent%20output%20that%20reflects%20the%20desired%20materials%2C%20textures%2C%20or%20styles.%20This%20allows%20explicit%20specification%20of%20appearance%20parameters%20at%20generation%20time%20while%20preserving%20the%20underlying%20object%20geometry%20and%20view%20coherence.%20We%20leverage%20three%20diffusion%20denoising%20processes%20responsible%20for%20generating%20the%20original%20object%2C%20the%20reference%2C%20and%20the%20target%20images%2C%20and%20perform%20reverse%20sampling%20to%20aggregate%20a%20small%20subset%20of%20layer-wise%20self-attention%20features%20from%20the%20object%20and%20the%20reference%20to%20influence%20the%20target%20generation.%20Our%20method%20requires%20only%20a%20few%20training%20examples%20to%20introduce%20appearance%20awareness%20to%20pretrained%20multiview%20models.%20The%20experiments%20show%20that%20our%20method%20provides%20a%20simple%20yet%20effective%20way%20toward%20multiview%20generation%20with%20diverse%20appearance%2C%20advocating%20the%20adoption%20of%20implicit%20generative%203D%20representations%20in%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFROMAT%253A%2520Multiview%2520Material%2520Appearance%2520Transfer%2520via%2520Few-Shot%2520Self-Attention%2520Adaptation%26entry.906535625%3DHubert%2520Kompanowski%2520and%2520Varun%2520Jampani%2520and%2520Aaryaman%2520Vasishta%2520and%2520Binh-Son%2520Hua%26entry.1292438233%3DMultiview%2520diffusion%2520models%2520have%2520rapidly%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520content%2520creation%2520with%2520spatial%2520consistency%2520across%2520viewpoints%252C%2520offering%2520rich%2520visual%2520realism%2520without%2520requiring%2520explicit%2520geometry%2520and%2520appearance%2520representation.%2520However%252C%2520compared%2520to%2520meshes%2520or%2520radiance%2520fields%252C%2520existing%2520multiview%2520diffusion%2520models%2520offer%2520limited%2520appearance%2520manipulation%252C%2520particularly%2520in%2520terms%2520of%2520material%252C%2520texture%252C%2520or%2520style.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520lightweight%2520adaptation%2520technique%2520for%2520appearance%2520transfer%2520in%2520multiview%2520diffusion%2520models.%2520Our%2520method%2520learns%2520to%2520combine%2520object%2520identity%2520from%2520an%2520input%2520image%2520with%2520appearance%2520cues%2520rendered%2520in%2520a%2520separate%2520reference%2520image%252C%2520producing%2520multi-view-consistent%2520output%2520that%2520reflects%2520the%2520desired%2520materials%252C%2520textures%252C%2520or%2520styles.%2520This%2520allows%2520explicit%2520specification%2520of%2520appearance%2520parameters%2520at%2520generation%2520time%2520while%2520preserving%2520the%2520underlying%2520object%2520geometry%2520and%2520view%2520coherence.%2520We%2520leverage%2520three%2520diffusion%2520denoising%2520processes%2520responsible%2520for%2520generating%2520the%2520original%2520object%252C%2520the%2520reference%252C%2520and%2520the%2520target%2520images%252C%2520and%2520perform%2520reverse%2520sampling%2520to%2520aggregate%2520a%2520small%2520subset%2520of%2520layer-wise%2520self-attention%2520features%2520from%2520the%2520object%2520and%2520the%2520reference%2520to%2520influence%2520the%2520target%2520generation.%2520Our%2520method%2520requires%2520only%2520a%2520few%2520training%2520examples%2520to%2520introduce%2520appearance%2520awareness%2520to%2520pretrained%2520multiview%2520models.%2520The%2520experiments%2520show%2520that%2520our%2520method%2520provides%2520a%2520simple%2520yet%2520effective%2520way%2520toward%2520multiview%2520generation%2520with%2520diverse%2520appearance%252C%2520advocating%2520the%2520adoption%2520of%2520implicit%2520generative%25203D%2520representations%2520in%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FROMAT%3A%20Multiview%20Material%20Appearance%20Transfer%20via%20Few-Shot%20Self-Attention%20Adaptation&entry.906535625=Hubert%20Kompanowski%20and%20Varun%20Jampani%20and%20Aaryaman%20Vasishta%20and%20Binh-Son%20Hua&entry.1292438233=Multiview%20diffusion%20models%20have%20rapidly%20emerged%20as%20a%20powerful%20tool%20for%20content%20creation%20with%20spatial%20consistency%20across%20viewpoints%2C%20offering%20rich%20visual%20realism%20without%20requiring%20explicit%20geometry%20and%20appearance%20representation.%20However%2C%20compared%20to%20meshes%20or%20radiance%20fields%2C%20existing%20multiview%20diffusion%20models%20offer%20limited%20appearance%20manipulation%2C%20particularly%20in%20terms%20of%20material%2C%20texture%2C%20or%20style.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20lightweight%20adaptation%20technique%20for%20appearance%20transfer%20in%20multiview%20diffusion%20models.%20Our%20method%20learns%20to%20combine%20object%20identity%20from%20an%20input%20image%20with%20appearance%20cues%20rendered%20in%20a%20separate%20reference%20image%2C%20producing%20multi-view-consistent%20output%20that%20reflects%20the%20desired%20materials%2C%20textures%2C%20or%20styles.%20This%20allows%20explicit%20specification%20of%20appearance%20parameters%20at%20generation%20time%20while%20preserving%20the%20underlying%20object%20geometry%20and%20view%20coherence.%20We%20leverage%20three%20diffusion%20denoising%20processes%20responsible%20for%20generating%20the%20original%20object%2C%20the%20reference%2C%20and%20the%20target%20images%2C%20and%20perform%20reverse%20sampling%20to%20aggregate%20a%20small%20subset%20of%20layer-wise%20self-attention%20features%20from%20the%20object%20and%20the%20reference%20to%20influence%20the%20target%20generation.%20Our%20method%20requires%20only%20a%20few%20training%20examples%20to%20introduce%20appearance%20awareness%20to%20pretrained%20multiview%20models.%20The%20experiments%20show%20that%20our%20method%20provides%20a%20simple%20yet%20effective%20way%20toward%20multiview%20generation%20with%20diverse%20appearance%2C%20advocating%20the%20adoption%20of%20implicit%20generative%203D%20representations%20in%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2512.09617v1&entry.124074799=Read"},
{"title": "THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering", "author": "Jian Zhu", "abstract": "Multi-View Clustering (MVC) has garnered increasing attention in recent years. It is capable of partitioning data samples into distinct groups by learning a consensus representation. However, a significant challenge remains: the problem of untrustworthy fusion. This problem primarily arises from two key factors: 1) Existing methods often ignore the presence of inherent noise within individual views; 2) In traditional MVC methods using Contrastive Learning (CL), similarity computations typically rely on different views of the same instance, while neglecting the structural information from nearest neighbors within the same cluster. Consequently, this leads to the wrong direction for multi-view fusion. To address this problem, we present a novel Trusted Hierarchical Contrastive Representation Learning (THCRL). It consists of two key modules. Specifically, we propose the Deep Symmetry Hierarchical Fusion (DSHF) module, which leverages the UNet architecture integrated with multiple denoising mechanisms to achieve trustworthy fusion of multi-view data. Furthermore, we present the Average K-Nearest Neighbors Contrastive Learning (AKCL) module to align the fused representation with the view-specific representation. Unlike conventional strategies, AKCL enhances representation similarity among samples belonging to the same cluster, rather than merely focusing on the same sample across views, thereby reinforcing the confidence of the fused representation. Extensive experiments demonstrate that THCRL achieves the state-of-the-art performance in deep MVC tasks.", "link": "http://arxiv.org/abs/2512.00368v2", "date": "2025-12-10", "relevancy": 2.6408, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5613}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5116}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20THCRL%3A%20Trusted%20Hierarchical%20Contrastive%20Representation%20Learning%20for%20Multi-View%20Clustering&body=Title%3A%20THCRL%3A%20Trusted%20Hierarchical%20Contrastive%20Representation%20Learning%20for%20Multi-View%20Clustering%0AAuthor%3A%20Jian%20Zhu%0AAbstract%3A%20Multi-View%20Clustering%20%28MVC%29%20has%20garnered%20increasing%20attention%20in%20recent%20years.%20It%20is%20capable%20of%20partitioning%20data%20samples%20into%20distinct%20groups%20by%20learning%20a%20consensus%20representation.%20However%2C%20a%20significant%20challenge%20remains%3A%20the%20problem%20of%20untrustworthy%20fusion.%20This%20problem%20primarily%20arises%20from%20two%20key%20factors%3A%201%29%20Existing%20methods%20often%20ignore%20the%20presence%20of%20inherent%20noise%20within%20individual%20views%3B%202%29%20In%20traditional%20MVC%20methods%20using%20Contrastive%20Learning%20%28CL%29%2C%20similarity%20computations%20typically%20rely%20on%20different%20views%20of%20the%20same%20instance%2C%20while%20neglecting%20the%20structural%20information%20from%20nearest%20neighbors%20within%20the%20same%20cluster.%20Consequently%2C%20this%20leads%20to%20the%20wrong%20direction%20for%20multi-view%20fusion.%20To%20address%20this%20problem%2C%20we%20present%20a%20novel%20Trusted%20Hierarchical%20Contrastive%20Representation%20Learning%20%28THCRL%29.%20It%20consists%20of%20two%20key%20modules.%20Specifically%2C%20we%20propose%20the%20Deep%20Symmetry%20Hierarchical%20Fusion%20%28DSHF%29%20module%2C%20which%20leverages%20the%20UNet%20architecture%20integrated%20with%20multiple%20denoising%20mechanisms%20to%20achieve%20trustworthy%20fusion%20of%20multi-view%20data.%20Furthermore%2C%20we%20present%20the%20Average%20K-Nearest%20Neighbors%20Contrastive%20Learning%20%28AKCL%29%20module%20to%20align%20the%20fused%20representation%20with%20the%20view-specific%20representation.%20Unlike%20conventional%20strategies%2C%20AKCL%20enhances%20representation%20similarity%20among%20samples%20belonging%20to%20the%20same%20cluster%2C%20rather%20than%20merely%20focusing%20on%20the%20same%20sample%20across%20views%2C%20thereby%20reinforcing%20the%20confidence%20of%20the%20fused%20representation.%20Extensive%20experiments%20demonstrate%20that%20THCRL%20achieves%20the%20state-of-the-art%20performance%20in%20deep%20MVC%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.00368v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTHCRL%253A%2520Trusted%2520Hierarchical%2520Contrastive%2520Representation%2520Learning%2520for%2520Multi-View%2520Clustering%26entry.906535625%3DJian%2520Zhu%26entry.1292438233%3DMulti-View%2520Clustering%2520%2528MVC%2529%2520has%2520garnered%2520increasing%2520attention%2520in%2520recent%2520years.%2520It%2520is%2520capable%2520of%2520partitioning%2520data%2520samples%2520into%2520distinct%2520groups%2520by%2520learning%2520a%2520consensus%2520representation.%2520However%252C%2520a%2520significant%2520challenge%2520remains%253A%2520the%2520problem%2520of%2520untrustworthy%2520fusion.%2520This%2520problem%2520primarily%2520arises%2520from%2520two%2520key%2520factors%253A%25201%2529%2520Existing%2520methods%2520often%2520ignore%2520the%2520presence%2520of%2520inherent%2520noise%2520within%2520individual%2520views%253B%25202%2529%2520In%2520traditional%2520MVC%2520methods%2520using%2520Contrastive%2520Learning%2520%2528CL%2529%252C%2520similarity%2520computations%2520typically%2520rely%2520on%2520different%2520views%2520of%2520the%2520same%2520instance%252C%2520while%2520neglecting%2520the%2520structural%2520information%2520from%2520nearest%2520neighbors%2520within%2520the%2520same%2520cluster.%2520Consequently%252C%2520this%2520leads%2520to%2520the%2520wrong%2520direction%2520for%2520multi-view%2520fusion.%2520To%2520address%2520this%2520problem%252C%2520we%2520present%2520a%2520novel%2520Trusted%2520Hierarchical%2520Contrastive%2520Representation%2520Learning%2520%2528THCRL%2529.%2520It%2520consists%2520of%2520two%2520key%2520modules.%2520Specifically%252C%2520we%2520propose%2520the%2520Deep%2520Symmetry%2520Hierarchical%2520Fusion%2520%2528DSHF%2529%2520module%252C%2520which%2520leverages%2520the%2520UNet%2520architecture%2520integrated%2520with%2520multiple%2520denoising%2520mechanisms%2520to%2520achieve%2520trustworthy%2520fusion%2520of%2520multi-view%2520data.%2520Furthermore%252C%2520we%2520present%2520the%2520Average%2520K-Nearest%2520Neighbors%2520Contrastive%2520Learning%2520%2528AKCL%2529%2520module%2520to%2520align%2520the%2520fused%2520representation%2520with%2520the%2520view-specific%2520representation.%2520Unlike%2520conventional%2520strategies%252C%2520AKCL%2520enhances%2520representation%2520similarity%2520among%2520samples%2520belonging%2520to%2520the%2520same%2520cluster%252C%2520rather%2520than%2520merely%2520focusing%2520on%2520the%2520same%2520sample%2520across%2520views%252C%2520thereby%2520reinforcing%2520the%2520confidence%2520of%2520the%2520fused%2520representation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520THCRL%2520achieves%2520the%2520state-of-the-art%2520performance%2520in%2520deep%2520MVC%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00368v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=THCRL%3A%20Trusted%20Hierarchical%20Contrastive%20Representation%20Learning%20for%20Multi-View%20Clustering&entry.906535625=Jian%20Zhu&entry.1292438233=Multi-View%20Clustering%20%28MVC%29%20has%20garnered%20increasing%20attention%20in%20recent%20years.%20It%20is%20capable%20of%20partitioning%20data%20samples%20into%20distinct%20groups%20by%20learning%20a%20consensus%20representation.%20However%2C%20a%20significant%20challenge%20remains%3A%20the%20problem%20of%20untrustworthy%20fusion.%20This%20problem%20primarily%20arises%20from%20two%20key%20factors%3A%201%29%20Existing%20methods%20often%20ignore%20the%20presence%20of%20inherent%20noise%20within%20individual%20views%3B%202%29%20In%20traditional%20MVC%20methods%20using%20Contrastive%20Learning%20%28CL%29%2C%20similarity%20computations%20typically%20rely%20on%20different%20views%20of%20the%20same%20instance%2C%20while%20neglecting%20the%20structural%20information%20from%20nearest%20neighbors%20within%20the%20same%20cluster.%20Consequently%2C%20this%20leads%20to%20the%20wrong%20direction%20for%20multi-view%20fusion.%20To%20address%20this%20problem%2C%20we%20present%20a%20novel%20Trusted%20Hierarchical%20Contrastive%20Representation%20Learning%20%28THCRL%29.%20It%20consists%20of%20two%20key%20modules.%20Specifically%2C%20we%20propose%20the%20Deep%20Symmetry%20Hierarchical%20Fusion%20%28DSHF%29%20module%2C%20which%20leverages%20the%20UNet%20architecture%20integrated%20with%20multiple%20denoising%20mechanisms%20to%20achieve%20trustworthy%20fusion%20of%20multi-view%20data.%20Furthermore%2C%20we%20present%20the%20Average%20K-Nearest%20Neighbors%20Contrastive%20Learning%20%28AKCL%29%20module%20to%20align%20the%20fused%20representation%20with%20the%20view-specific%20representation.%20Unlike%20conventional%20strategies%2C%20AKCL%20enhances%20representation%20similarity%20among%20samples%20belonging%20to%20the%20same%20cluster%2C%20rather%20than%20merely%20focusing%20on%20the%20same%20sample%20across%20views%2C%20thereby%20reinforcing%20the%20confidence%20of%20the%20fused%20representation.%20Extensive%20experiments%20demonstrate%20that%20THCRL%20achieves%20the%20state-of-the-art%20performance%20in%20deep%20MVC%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.00368v2&entry.124074799=Read"},
{"title": "GeoDM: Geometry-aware Distribution Matching for Dataset Distillation", "author": "Xuhui Li and Zhengquan Luo and Zihui Cui and Zhiqiang Xu", "abstract": "Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \\textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.", "link": "http://arxiv.org/abs/2512.08317v2", "date": "2025-12-10", "relevancy": 2.6136, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5518}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.532}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoDM%3A%20Geometry-aware%20Distribution%20Matching%20for%20Dataset%20Distillation&body=Title%3A%20GeoDM%3A%20Geometry-aware%20Distribution%20Matching%20for%20Dataset%20Distillation%0AAuthor%3A%20Xuhui%20Li%20and%20Zhengquan%20Luo%20and%20Zihui%20Cui%20and%20Zhiqiang%20Xu%0AAbstract%3A%20Dataset%20distillation%20aims%20to%20synthesize%20a%20compact%20subset%20of%20the%20original%20data%2C%20enabling%20models%20trained%20on%20it%20to%20achieve%20performance%20comparable%20to%20those%20trained%20on%20the%20original%20large%20dataset.%20Existing%20distribution-matching%20methods%20are%20confined%20to%20Euclidean%20spaces%2C%20making%20them%20only%20capture%20linear%20structures%20and%20overlook%20the%20intrinsic%20geometry%20of%20real%20data%2C%20e.g.%2C%20curvature.%20However%2C%20high-dimensional%20data%20often%20lie%20on%20low-dimensional%20manifolds%2C%20suggesting%20that%20dataset%20distillation%20should%20have%20the%20distilled%20data%20manifold%20aligned%20with%20the%20original%20data%20manifold.%20In%20this%20work%2C%20we%20propose%20a%20geometry-aware%20distribution-matching%20framework%2C%20called%20%5Ctextbf%7BGeoDM%7D%2C%20which%20operates%20in%20the%20Cartesian%20product%20of%20Euclidean%2C%20hyperbolic%2C%20and%20spherical%20manifolds%2C%20with%20flat%2C%20hierarchical%2C%20and%20cyclical%20structures%20all%20captured%20by%20a%20unified%20representation.%20To%20adapt%20to%20the%20underlying%20data%20geometry%2C%20we%20introduce%20learnable%20curvature%20and%20weight%20parameters%20for%20three%20kinds%20of%20geometries.%20At%20the%20same%20time%2C%20we%20design%20an%20optimal%20transport%20loss%20to%20enhance%20the%20distribution%20fidelity.%20Our%20theoretical%20analysis%20shows%20that%20the%20geometry-aware%20distribution%20matching%20in%20a%20product%20space%20yields%20a%20smaller%20generalization%20error%20bound%20than%20the%20Euclidean%20counterparts.%20Extensive%20experiments%20conducted%20on%20standard%20benchmarks%20demonstrate%20that%20our%20algorithm%20outperforms%20state-of-the-art%20data%20distillation%20methods%20and%20remains%20effective%20across%20various%20distribution-matching%20strategies%20for%20the%20single%20geometries.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08317v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoDM%253A%2520Geometry-aware%2520Distribution%2520Matching%2520for%2520Dataset%2520Distillation%26entry.906535625%3DXuhui%2520Li%2520and%2520Zhengquan%2520Luo%2520and%2520Zihui%2520Cui%2520and%2520Zhiqiang%2520Xu%26entry.1292438233%3DDataset%2520distillation%2520aims%2520to%2520synthesize%2520a%2520compact%2520subset%2520of%2520the%2520original%2520data%252C%2520enabling%2520models%2520trained%2520on%2520it%2520to%2520achieve%2520performance%2520comparable%2520to%2520those%2520trained%2520on%2520the%2520original%2520large%2520dataset.%2520Existing%2520distribution-matching%2520methods%2520are%2520confined%2520to%2520Euclidean%2520spaces%252C%2520making%2520them%2520only%2520capture%2520linear%2520structures%2520and%2520overlook%2520the%2520intrinsic%2520geometry%2520of%2520real%2520data%252C%2520e.g.%252C%2520curvature.%2520However%252C%2520high-dimensional%2520data%2520often%2520lie%2520on%2520low-dimensional%2520manifolds%252C%2520suggesting%2520that%2520dataset%2520distillation%2520should%2520have%2520the%2520distilled%2520data%2520manifold%2520aligned%2520with%2520the%2520original%2520data%2520manifold.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520geometry-aware%2520distribution-matching%2520framework%252C%2520called%2520%255Ctextbf%257BGeoDM%257D%252C%2520which%2520operates%2520in%2520the%2520Cartesian%2520product%2520of%2520Euclidean%252C%2520hyperbolic%252C%2520and%2520spherical%2520manifolds%252C%2520with%2520flat%252C%2520hierarchical%252C%2520and%2520cyclical%2520structures%2520all%2520captured%2520by%2520a%2520unified%2520representation.%2520To%2520adapt%2520to%2520the%2520underlying%2520data%2520geometry%252C%2520we%2520introduce%2520learnable%2520curvature%2520and%2520weight%2520parameters%2520for%2520three%2520kinds%2520of%2520geometries.%2520At%2520the%2520same%2520time%252C%2520we%2520design%2520an%2520optimal%2520transport%2520loss%2520to%2520enhance%2520the%2520distribution%2520fidelity.%2520Our%2520theoretical%2520analysis%2520shows%2520that%2520the%2520geometry-aware%2520distribution%2520matching%2520in%2520a%2520product%2520space%2520yields%2520a%2520smaller%2520generalization%2520error%2520bound%2520than%2520the%2520Euclidean%2520counterparts.%2520Extensive%2520experiments%2520conducted%2520on%2520standard%2520benchmarks%2520demonstrate%2520that%2520our%2520algorithm%2520outperforms%2520state-of-the-art%2520data%2520distillation%2520methods%2520and%2520remains%2520effective%2520across%2520various%2520distribution-matching%2520strategies%2520for%2520the%2520single%2520geometries.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08317v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoDM%3A%20Geometry-aware%20Distribution%20Matching%20for%20Dataset%20Distillation&entry.906535625=Xuhui%20Li%20and%20Zhengquan%20Luo%20and%20Zihui%20Cui%20and%20Zhiqiang%20Xu&entry.1292438233=Dataset%20distillation%20aims%20to%20synthesize%20a%20compact%20subset%20of%20the%20original%20data%2C%20enabling%20models%20trained%20on%20it%20to%20achieve%20performance%20comparable%20to%20those%20trained%20on%20the%20original%20large%20dataset.%20Existing%20distribution-matching%20methods%20are%20confined%20to%20Euclidean%20spaces%2C%20making%20them%20only%20capture%20linear%20structures%20and%20overlook%20the%20intrinsic%20geometry%20of%20real%20data%2C%20e.g.%2C%20curvature.%20However%2C%20high-dimensional%20data%20often%20lie%20on%20low-dimensional%20manifolds%2C%20suggesting%20that%20dataset%20distillation%20should%20have%20the%20distilled%20data%20manifold%20aligned%20with%20the%20original%20data%20manifold.%20In%20this%20work%2C%20we%20propose%20a%20geometry-aware%20distribution-matching%20framework%2C%20called%20%5Ctextbf%7BGeoDM%7D%2C%20which%20operates%20in%20the%20Cartesian%20product%20of%20Euclidean%2C%20hyperbolic%2C%20and%20spherical%20manifolds%2C%20with%20flat%2C%20hierarchical%2C%20and%20cyclical%20structures%20all%20captured%20by%20a%20unified%20representation.%20To%20adapt%20to%20the%20underlying%20data%20geometry%2C%20we%20introduce%20learnable%20curvature%20and%20weight%20parameters%20for%20three%20kinds%20of%20geometries.%20At%20the%20same%20time%2C%20we%20design%20an%20optimal%20transport%20loss%20to%20enhance%20the%20distribution%20fidelity.%20Our%20theoretical%20analysis%20shows%20that%20the%20geometry-aware%20distribution%20matching%20in%20a%20product%20space%20yields%20a%20smaller%20generalization%20error%20bound%20than%20the%20Euclidean%20counterparts.%20Extensive%20experiments%20conducted%20on%20standard%20benchmarks%20demonstrate%20that%20our%20algorithm%20outperforms%20state-of-the-art%20data%20distillation%20methods%20and%20remains%20effective%20across%20various%20distribution-matching%20strategies%20for%20the%20single%20geometries.&entry.1838667208=http%3A//arxiv.org/abs/2512.08317v2&entry.124074799=Read"},
{"title": "VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification", "author": "Wanyue Zhang and Lin Geng Foo and Thabo Beeler and Rishabh Dabral and Christian Theobalt", "abstract": "Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: https://vcai.mpi-inf.mpg.de/projects/vhoi/.", "link": "http://arxiv.org/abs/2512.09646v1", "date": "2025-12-10", "relevancy": 2.591, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6939}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6154}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VHOI%3A%20Controllable%20Video%20Generation%20of%20Human-Object%20Interactions%20from%20Sparse%20Trajectories%20via%20Motion%20Densification&body=Title%3A%20VHOI%3A%20Controllable%20Video%20Generation%20of%20Human-Object%20Interactions%20from%20Sparse%20Trajectories%20via%20Motion%20Densification%0AAuthor%3A%20Wanyue%20Zhang%20and%20Lin%20Geng%20Foo%20and%20Thabo%20Beeler%20and%20Rishabh%20Dabral%20and%20Christian%20Theobalt%0AAbstract%3A%20Synthesizing%20realistic%20human-object%20interactions%20%28HOI%29%20in%20video%20is%20challenging%20due%20to%20the%20complex%2C%20instance-specific%20interaction%20dynamics%20of%20both%20humans%20and%20objects.%20Incorporating%20controllability%20in%20video%20generation%20further%20adds%20to%20the%20complexity.%20Existing%20controllable%20video%20generation%20approaches%20face%20a%20trade-off%3A%20sparse%20controls%20like%20keypoint%20trajectories%20are%20easy%20to%20specify%20but%20lack%20instance-awareness%2C%20while%20dense%20signals%20such%20as%20optical%20flow%2C%20depths%20or%203D%20meshes%20are%20informative%20but%20costly%20to%20obtain.%20We%20propose%20VHOI%2C%20a%20two-stage%20framework%20that%20first%20densifies%20sparse%20trajectories%20into%20HOI%20mask%20sequences%2C%20and%20then%20fine-tunes%20a%20video%20diffusion%20model%20conditioned%20on%20these%20dense%20masks.%20We%20introduce%20a%20novel%20HOI-aware%20motion%20representation%20that%20uses%20color%20encodings%20to%20distinguish%20not%20only%20human%20and%20object%20motion%2C%20but%20also%20body-part-specific%20dynamics.%20This%20design%20incorporates%20a%20human%20prior%20into%20the%20conditioning%20signal%20and%20strengthens%20the%20model%27s%20ability%20to%20understand%20and%20generate%20realistic%20HOI%20dynamics.%20Experiments%20demonstrate%20state-of-the-art%20results%20in%20controllable%20HOI%20video%20generation.%20VHOI%20is%20not%20limited%20to%20interaction-only%20scenarios%20and%20can%20also%20generate%20full%20human%20navigation%20leading%20up%20to%20object%20interactions%20in%20an%20end-to-end%20manner.%20Project%20page%3A%20https%3A//vcai.mpi-inf.mpg.de/projects/vhoi/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVHOI%253A%2520Controllable%2520Video%2520Generation%2520of%2520Human-Object%2520Interactions%2520from%2520Sparse%2520Trajectories%2520via%2520Motion%2520Densification%26entry.906535625%3DWanyue%2520Zhang%2520and%2520Lin%2520Geng%2520Foo%2520and%2520Thabo%2520Beeler%2520and%2520Rishabh%2520Dabral%2520and%2520Christian%2520Theobalt%26entry.1292438233%3DSynthesizing%2520realistic%2520human-object%2520interactions%2520%2528HOI%2529%2520in%2520video%2520is%2520challenging%2520due%2520to%2520the%2520complex%252C%2520instance-specific%2520interaction%2520dynamics%2520of%2520both%2520humans%2520and%2520objects.%2520Incorporating%2520controllability%2520in%2520video%2520generation%2520further%2520adds%2520to%2520the%2520complexity.%2520Existing%2520controllable%2520video%2520generation%2520approaches%2520face%2520a%2520trade-off%253A%2520sparse%2520controls%2520like%2520keypoint%2520trajectories%2520are%2520easy%2520to%2520specify%2520but%2520lack%2520instance-awareness%252C%2520while%2520dense%2520signals%2520such%2520as%2520optical%2520flow%252C%2520depths%2520or%25203D%2520meshes%2520are%2520informative%2520but%2520costly%2520to%2520obtain.%2520We%2520propose%2520VHOI%252C%2520a%2520two-stage%2520framework%2520that%2520first%2520densifies%2520sparse%2520trajectories%2520into%2520HOI%2520mask%2520sequences%252C%2520and%2520then%2520fine-tunes%2520a%2520video%2520diffusion%2520model%2520conditioned%2520on%2520these%2520dense%2520masks.%2520We%2520introduce%2520a%2520novel%2520HOI-aware%2520motion%2520representation%2520that%2520uses%2520color%2520encodings%2520to%2520distinguish%2520not%2520only%2520human%2520and%2520object%2520motion%252C%2520but%2520also%2520body-part-specific%2520dynamics.%2520This%2520design%2520incorporates%2520a%2520human%2520prior%2520into%2520the%2520conditioning%2520signal%2520and%2520strengthens%2520the%2520model%2527s%2520ability%2520to%2520understand%2520and%2520generate%2520realistic%2520HOI%2520dynamics.%2520Experiments%2520demonstrate%2520state-of-the-art%2520results%2520in%2520controllable%2520HOI%2520video%2520generation.%2520VHOI%2520is%2520not%2520limited%2520to%2520interaction-only%2520scenarios%2520and%2520can%2520also%2520generate%2520full%2520human%2520navigation%2520leading%2520up%2520to%2520object%2520interactions%2520in%2520an%2520end-to-end%2520manner.%2520Project%2520page%253A%2520https%253A//vcai.mpi-inf.mpg.de/projects/vhoi/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VHOI%3A%20Controllable%20Video%20Generation%20of%20Human-Object%20Interactions%20from%20Sparse%20Trajectories%20via%20Motion%20Densification&entry.906535625=Wanyue%20Zhang%20and%20Lin%20Geng%20Foo%20and%20Thabo%20Beeler%20and%20Rishabh%20Dabral%20and%20Christian%20Theobalt&entry.1292438233=Synthesizing%20realistic%20human-object%20interactions%20%28HOI%29%20in%20video%20is%20challenging%20due%20to%20the%20complex%2C%20instance-specific%20interaction%20dynamics%20of%20both%20humans%20and%20objects.%20Incorporating%20controllability%20in%20video%20generation%20further%20adds%20to%20the%20complexity.%20Existing%20controllable%20video%20generation%20approaches%20face%20a%20trade-off%3A%20sparse%20controls%20like%20keypoint%20trajectories%20are%20easy%20to%20specify%20but%20lack%20instance-awareness%2C%20while%20dense%20signals%20such%20as%20optical%20flow%2C%20depths%20or%203D%20meshes%20are%20informative%20but%20costly%20to%20obtain.%20We%20propose%20VHOI%2C%20a%20two-stage%20framework%20that%20first%20densifies%20sparse%20trajectories%20into%20HOI%20mask%20sequences%2C%20and%20then%20fine-tunes%20a%20video%20diffusion%20model%20conditioned%20on%20these%20dense%20masks.%20We%20introduce%20a%20novel%20HOI-aware%20motion%20representation%20that%20uses%20color%20encodings%20to%20distinguish%20not%20only%20human%20and%20object%20motion%2C%20but%20also%20body-part-specific%20dynamics.%20This%20design%20incorporates%20a%20human%20prior%20into%20the%20conditioning%20signal%20and%20strengthens%20the%20model%27s%20ability%20to%20understand%20and%20generate%20realistic%20HOI%20dynamics.%20Experiments%20demonstrate%20state-of-the-art%20results%20in%20controllable%20HOI%20video%20generation.%20VHOI%20is%20not%20limited%20to%20interaction-only%20scenarios%20and%20can%20also%20generate%20full%20human%20navigation%20leading%20up%20to%20object%20interactions%20in%20an%20end-to-end%20manner.%20Project%20page%3A%20https%3A//vcai.mpi-inf.mpg.de/projects/vhoi/.&entry.1838667208=http%3A//arxiv.org/abs/2512.09646v1&entry.124074799=Read"},
{"title": "MoReGen: Multi-Agent Motion-Reasoning Engine for Code-based Text-to-Video Synthesis", "author": "Xiangyu Bai and He Liang and Bishoy Galoaa and Utsav Nandi and Shayda Moezzi and Yuhang He and Sarah Ostadabbas", "abstract": "While text-to-video (T2V) generation has achieved remarkable progress in photorealism, generating intent-aligned videos that faithfully obey physics principles remains a core challenge. In this work, we systematically study Newtonian motion-controlled text-to-video generation and evaluation, emphasizing physical precision and motion coherence. We introduce MoReGen, a motion-aware, physics-grounded T2V framework that integrates multi-agent LLMs, physics simulators, and renderers to generate reproducible, physically accurate videos from text prompts in the code domain. To quantitatively assess physical validity, we propose object-trajectory correspondence as a direct evaluation metric and present MoReSet, a benchmark of 1,275 human-annotated videos spanning nine classes of Newtonian phenomena with scene descriptions, spatiotemporal relations, and ground-truth trajectories. Using MoReSet, we conduct experiments on existing T2V models, evaluating their physical validity through both our MoRe metrics and existing physics-based evaluators. Our results reveal that state-of-the-art models struggle to maintain physical validity, while MoReGen establishes a principled direction toward physically coherent video synthesis.", "link": "http://arxiv.org/abs/2512.04221v2", "date": "2025-12-10", "relevancy": 2.5879, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.686}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6209}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoReGen%3A%20Multi-Agent%20Motion-Reasoning%20Engine%20for%20Code-based%20Text-to-Video%20Synthesis&body=Title%3A%20MoReGen%3A%20Multi-Agent%20Motion-Reasoning%20Engine%20for%20Code-based%20Text-to-Video%20Synthesis%0AAuthor%3A%20Xiangyu%20Bai%20and%20He%20Liang%20and%20Bishoy%20Galoaa%20and%20Utsav%20Nandi%20and%20Shayda%20Moezzi%20and%20Yuhang%20He%20and%20Sarah%20Ostadabbas%0AAbstract%3A%20While%20text-to-video%20%28T2V%29%20generation%20has%20achieved%20remarkable%20progress%20in%20photorealism%2C%20generating%20intent-aligned%20videos%20that%20faithfully%20obey%20physics%20principles%20remains%20a%20core%20challenge.%20In%20this%20work%2C%20we%20systematically%20study%20Newtonian%20motion-controlled%20text-to-video%20generation%20and%20evaluation%2C%20emphasizing%20physical%20precision%20and%20motion%20coherence.%20We%20introduce%20MoReGen%2C%20a%20motion-aware%2C%20physics-grounded%20T2V%20framework%20that%20integrates%20multi-agent%20LLMs%2C%20physics%20simulators%2C%20and%20renderers%20to%20generate%20reproducible%2C%20physically%20accurate%20videos%20from%20text%20prompts%20in%20the%20code%20domain.%20To%20quantitatively%20assess%20physical%20validity%2C%20we%20propose%20object-trajectory%20correspondence%20as%20a%20direct%20evaluation%20metric%20and%20present%20MoReSet%2C%20a%20benchmark%20of%201%2C275%20human-annotated%20videos%20spanning%20nine%20classes%20of%20Newtonian%20phenomena%20with%20scene%20descriptions%2C%20spatiotemporal%20relations%2C%20and%20ground-truth%20trajectories.%20Using%20MoReSet%2C%20we%20conduct%20experiments%20on%20existing%20T2V%20models%2C%20evaluating%20their%20physical%20validity%20through%20both%20our%20MoRe%20metrics%20and%20existing%20physics-based%20evaluators.%20Our%20results%20reveal%20that%20state-of-the-art%20models%20struggle%20to%20maintain%20physical%20validity%2C%20while%20MoReGen%20establishes%20a%20principled%20direction%20toward%20physically%20coherent%20video%20synthesis.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04221v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoReGen%253A%2520Multi-Agent%2520Motion-Reasoning%2520Engine%2520for%2520Code-based%2520Text-to-Video%2520Synthesis%26entry.906535625%3DXiangyu%2520Bai%2520and%2520He%2520Liang%2520and%2520Bishoy%2520Galoaa%2520and%2520Utsav%2520Nandi%2520and%2520Shayda%2520Moezzi%2520and%2520Yuhang%2520He%2520and%2520Sarah%2520Ostadabbas%26entry.1292438233%3DWhile%2520text-to-video%2520%2528T2V%2529%2520generation%2520has%2520achieved%2520remarkable%2520progress%2520in%2520photorealism%252C%2520generating%2520intent-aligned%2520videos%2520that%2520faithfully%2520obey%2520physics%2520principles%2520remains%2520a%2520core%2520challenge.%2520In%2520this%2520work%252C%2520we%2520systematically%2520study%2520Newtonian%2520motion-controlled%2520text-to-video%2520generation%2520and%2520evaluation%252C%2520emphasizing%2520physical%2520precision%2520and%2520motion%2520coherence.%2520We%2520introduce%2520MoReGen%252C%2520a%2520motion-aware%252C%2520physics-grounded%2520T2V%2520framework%2520that%2520integrates%2520multi-agent%2520LLMs%252C%2520physics%2520simulators%252C%2520and%2520renderers%2520to%2520generate%2520reproducible%252C%2520physically%2520accurate%2520videos%2520from%2520text%2520prompts%2520in%2520the%2520code%2520domain.%2520To%2520quantitatively%2520assess%2520physical%2520validity%252C%2520we%2520propose%2520object-trajectory%2520correspondence%2520as%2520a%2520direct%2520evaluation%2520metric%2520and%2520present%2520MoReSet%252C%2520a%2520benchmark%2520of%25201%252C275%2520human-annotated%2520videos%2520spanning%2520nine%2520classes%2520of%2520Newtonian%2520phenomena%2520with%2520scene%2520descriptions%252C%2520spatiotemporal%2520relations%252C%2520and%2520ground-truth%2520trajectories.%2520Using%2520MoReSet%252C%2520we%2520conduct%2520experiments%2520on%2520existing%2520T2V%2520models%252C%2520evaluating%2520their%2520physical%2520validity%2520through%2520both%2520our%2520MoRe%2520metrics%2520and%2520existing%2520physics-based%2520evaluators.%2520Our%2520results%2520reveal%2520that%2520state-of-the-art%2520models%2520struggle%2520to%2520maintain%2520physical%2520validity%252C%2520while%2520MoReGen%2520establishes%2520a%2520principled%2520direction%2520toward%2520physically%2520coherent%2520video%2520synthesis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04221v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoReGen%3A%20Multi-Agent%20Motion-Reasoning%20Engine%20for%20Code-based%20Text-to-Video%20Synthesis&entry.906535625=Xiangyu%20Bai%20and%20He%20Liang%20and%20Bishoy%20Galoaa%20and%20Utsav%20Nandi%20and%20Shayda%20Moezzi%20and%20Yuhang%20He%20and%20Sarah%20Ostadabbas&entry.1292438233=While%20text-to-video%20%28T2V%29%20generation%20has%20achieved%20remarkable%20progress%20in%20photorealism%2C%20generating%20intent-aligned%20videos%20that%20faithfully%20obey%20physics%20principles%20remains%20a%20core%20challenge.%20In%20this%20work%2C%20we%20systematically%20study%20Newtonian%20motion-controlled%20text-to-video%20generation%20and%20evaluation%2C%20emphasizing%20physical%20precision%20and%20motion%20coherence.%20We%20introduce%20MoReGen%2C%20a%20motion-aware%2C%20physics-grounded%20T2V%20framework%20that%20integrates%20multi-agent%20LLMs%2C%20physics%20simulators%2C%20and%20renderers%20to%20generate%20reproducible%2C%20physically%20accurate%20videos%20from%20text%20prompts%20in%20the%20code%20domain.%20To%20quantitatively%20assess%20physical%20validity%2C%20we%20propose%20object-trajectory%20correspondence%20as%20a%20direct%20evaluation%20metric%20and%20present%20MoReSet%2C%20a%20benchmark%20of%201%2C275%20human-annotated%20videos%20spanning%20nine%20classes%20of%20Newtonian%20phenomena%20with%20scene%20descriptions%2C%20spatiotemporal%20relations%2C%20and%20ground-truth%20trajectories.%20Using%20MoReSet%2C%20we%20conduct%20experiments%20on%20existing%20T2V%20models%2C%20evaluating%20their%20physical%20validity%20through%20both%20our%20MoRe%20metrics%20and%20existing%20physics-based%20evaluators.%20Our%20results%20reveal%20that%20state-of-the-art%20models%20struggle%20to%20maintain%20physical%20validity%2C%20while%20MoReGen%20establishes%20a%20principled%20direction%20toward%20physically%20coherent%20video%20synthesis.&entry.1838667208=http%3A//arxiv.org/abs/2512.04221v2&entry.124074799=Read"},
{"title": "LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery", "author": "Seon-Hoon Kim and Hyeji Sim and Youeyun Jung and Ok-Chul Jung and Yerin Kim", "abstract": "Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.", "link": "http://arxiv.org/abs/2512.09700v1", "date": "2025-12-10", "relevancy": 2.575, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5196}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5154}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiM-YOLO%3A%20Less%20is%20More%20with%20Pyramid%20Level%20Shift%20and%20Normalized%20Auxiliary%20Branch%20for%20Ship%20Detection%20in%20Optical%20Remote%20Sensing%20Imagery&body=Title%3A%20LiM-YOLO%3A%20Less%20is%20More%20with%20Pyramid%20Level%20Shift%20and%20Normalized%20Auxiliary%20Branch%20for%20Ship%20Detection%20in%20Optical%20Remote%20Sensing%20Imagery%0AAuthor%3A%20Seon-Hoon%20Kim%20and%20Hyeji%20Sim%20and%20Youeyun%20Jung%20and%20Ok-Chul%20Jung%20and%20Yerin%20Kim%0AAbstract%3A%20Applying%20general-purpose%20object%20detectors%20to%20ship%20detection%20in%20satellite%20imagery%20presents%20significant%20challenges%20due%20to%20the%20extreme%20scale%20disparity%20and%20morphological%20anisotropy%20of%20maritime%20targets.%20Standard%20architectures%20utilizing%20stride-32%20%28P5%29%20layers%20often%20fail%20to%20resolve%20narrow%20vessels%2C%20resulting%20in%20spatial%20feature%20dilution.%20In%20this%20work%2C%20we%20propose%20LiM-YOLO%2C%20a%20specialized%20detector%20designed%20to%20resolve%20these%20domain-specific%20conflicts.%20Based%20on%20a%20statistical%20analysis%20of%20ship%20scales%2C%20we%20introduce%20a%20Pyramid%20Level%20Shift%20Strategy%20that%20reconfigures%20the%20detection%20head%20to%20P2-P4.%20This%20shift%20ensures%20compliance%20with%20Nyquist%20sampling%20criteria%20for%20small%20objects%20while%20eliminating%20the%20computational%20redundancy%20of%20deep%20layers.%20To%20further%20enhance%20training%20stability%20on%20high-resolution%20inputs%2C%20we%20incorporate%20a%20Group%20Normalized%20Convolutional%20Block%20for%20Linear%20Projection%20%28GN-CBLinear%29%2C%20which%20mitigates%20gradient%20volatility%20in%20micro-batch%20settings.%20Validated%20on%20SODA-A%2C%20DOTA-v1.5%2C%20FAIR1M-v2.0%2C%20and%20ShipRSImageNet-V1%2C%20LiM-YOLO%20demonstrates%20superior%20detection%20accuracy%20and%20efficiency%20compared%20to%20state-of-the-art%20models.%20The%20code%20is%20available%20at%20https%3A//github.com/egshkim/LiM-YOLO.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiM-YOLO%253A%2520Less%2520is%2520More%2520with%2520Pyramid%2520Level%2520Shift%2520and%2520Normalized%2520Auxiliary%2520Branch%2520for%2520Ship%2520Detection%2520in%2520Optical%2520Remote%2520Sensing%2520Imagery%26entry.906535625%3DSeon-Hoon%2520Kim%2520and%2520Hyeji%2520Sim%2520and%2520Youeyun%2520Jung%2520and%2520Ok-Chul%2520Jung%2520and%2520Yerin%2520Kim%26entry.1292438233%3DApplying%2520general-purpose%2520object%2520detectors%2520to%2520ship%2520detection%2520in%2520satellite%2520imagery%2520presents%2520significant%2520challenges%2520due%2520to%2520the%2520extreme%2520scale%2520disparity%2520and%2520morphological%2520anisotropy%2520of%2520maritime%2520targets.%2520Standard%2520architectures%2520utilizing%2520stride-32%2520%2528P5%2529%2520layers%2520often%2520fail%2520to%2520resolve%2520narrow%2520vessels%252C%2520resulting%2520in%2520spatial%2520feature%2520dilution.%2520In%2520this%2520work%252C%2520we%2520propose%2520LiM-YOLO%252C%2520a%2520specialized%2520detector%2520designed%2520to%2520resolve%2520these%2520domain-specific%2520conflicts.%2520Based%2520on%2520a%2520statistical%2520analysis%2520of%2520ship%2520scales%252C%2520we%2520introduce%2520a%2520Pyramid%2520Level%2520Shift%2520Strategy%2520that%2520reconfigures%2520the%2520detection%2520head%2520to%2520P2-P4.%2520This%2520shift%2520ensures%2520compliance%2520with%2520Nyquist%2520sampling%2520criteria%2520for%2520small%2520objects%2520while%2520eliminating%2520the%2520computational%2520redundancy%2520of%2520deep%2520layers.%2520To%2520further%2520enhance%2520training%2520stability%2520on%2520high-resolution%2520inputs%252C%2520we%2520incorporate%2520a%2520Group%2520Normalized%2520Convolutional%2520Block%2520for%2520Linear%2520Projection%2520%2528GN-CBLinear%2529%252C%2520which%2520mitigates%2520gradient%2520volatility%2520in%2520micro-batch%2520settings.%2520Validated%2520on%2520SODA-A%252C%2520DOTA-v1.5%252C%2520FAIR1M-v2.0%252C%2520and%2520ShipRSImageNet-V1%252C%2520LiM-YOLO%2520demonstrates%2520superior%2520detection%2520accuracy%2520and%2520efficiency%2520compared%2520to%2520state-of-the-art%2520models.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/egshkim/LiM-YOLO.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiM-YOLO%3A%20Less%20is%20More%20with%20Pyramid%20Level%20Shift%20and%20Normalized%20Auxiliary%20Branch%20for%20Ship%20Detection%20in%20Optical%20Remote%20Sensing%20Imagery&entry.906535625=Seon-Hoon%20Kim%20and%20Hyeji%20Sim%20and%20Youeyun%20Jung%20and%20Ok-Chul%20Jung%20and%20Yerin%20Kim&entry.1292438233=Applying%20general-purpose%20object%20detectors%20to%20ship%20detection%20in%20satellite%20imagery%20presents%20significant%20challenges%20due%20to%20the%20extreme%20scale%20disparity%20and%20morphological%20anisotropy%20of%20maritime%20targets.%20Standard%20architectures%20utilizing%20stride-32%20%28P5%29%20layers%20often%20fail%20to%20resolve%20narrow%20vessels%2C%20resulting%20in%20spatial%20feature%20dilution.%20In%20this%20work%2C%20we%20propose%20LiM-YOLO%2C%20a%20specialized%20detector%20designed%20to%20resolve%20these%20domain-specific%20conflicts.%20Based%20on%20a%20statistical%20analysis%20of%20ship%20scales%2C%20we%20introduce%20a%20Pyramid%20Level%20Shift%20Strategy%20that%20reconfigures%20the%20detection%20head%20to%20P2-P4.%20This%20shift%20ensures%20compliance%20with%20Nyquist%20sampling%20criteria%20for%20small%20objects%20while%20eliminating%20the%20computational%20redundancy%20of%20deep%20layers.%20To%20further%20enhance%20training%20stability%20on%20high-resolution%20inputs%2C%20we%20incorporate%20a%20Group%20Normalized%20Convolutional%20Block%20for%20Linear%20Projection%20%28GN-CBLinear%29%2C%20which%20mitigates%20gradient%20volatility%20in%20micro-batch%20settings.%20Validated%20on%20SODA-A%2C%20DOTA-v1.5%2C%20FAIR1M-v2.0%2C%20and%20ShipRSImageNet-V1%2C%20LiM-YOLO%20demonstrates%20superior%20detection%20accuracy%20and%20efficiency%20compared%20to%20state-of-the-art%20models.%20The%20code%20is%20available%20at%20https%3A//github.com/egshkim/LiM-YOLO.&entry.1838667208=http%3A//arxiv.org/abs/2512.09700v1&entry.124074799=Read"},
{"title": "MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement", "author": "Nikolai Lund K\u00fchne and Jesper Jensen and Jan \u00d8stergaard and Zheng-Hua Tan", "abstract": "With new sequence models like Mamba and xLSTM, several studies have shown that these models match or outperform the state-of-the-art in single-channel speech enhancement and audio representation learning. However, prior research has demonstrated that sequence models like LSTM and Mamba tend to overfit to the training set. To address this, previous works have shown that adding self-attention to LSTMs substantially improves generalization performance for single-channel speech enhancement. Nevertheless, neither the concept of hybrid Mamba and time-frequency attention models nor their generalization performance have been explored for speech enhancement. In this paper, we propose a novel hybrid architecture, MambAttention, which combines Mamba and shared time- and frequency-multi-head attention modules for generalizable single-channel speech enhancement. To train our model, we introduce VB-DemandEx, a dataset inspired by VoiceBank+Demand but with more challenging noise types and lower signal-to-noise ratios. Trained on VB-DemandEx, MambAttention significantly outperforms existing state-of-the-art discriminative LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar complexity across all reported metrics on two out-of-domain datasets: DNS 2020 without reverberation and EARS-WHAM_v2. MambAttention also matches or outperforms generative diffusion models in generalization performance while being competitive with language model baselines. Ablation studies highlight the importance of weight sharing between time- and frequency-multi-head attention modules for generalization performance. Finally, we explore integrating the shared time- and frequency-multi-head attention modules with LSTM and xLSTM, which yields a notable performance improvement on the out-of-domain datasets. Yet, MambAttention remains superior for cross-corpus generalization across all reported evaluation metrics.", "link": "http://arxiv.org/abs/2507.00966v3", "date": "2025-12-10", "relevancy": 2.5505, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5279}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5139}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambAttention%3A%20Mamba%20with%20Multi-Head%20Attention%20for%20Generalizable%20Single-Channel%20Speech%20Enhancement&body=Title%3A%20MambAttention%3A%20Mamba%20with%20Multi-Head%20Attention%20for%20Generalizable%20Single-Channel%20Speech%20Enhancement%0AAuthor%3A%20Nikolai%20Lund%20K%C3%BChne%20and%20Jesper%20Jensen%20and%20Jan%20%C3%98stergaard%20and%20Zheng-Hua%20Tan%0AAbstract%3A%20With%20new%20sequence%20models%20like%20Mamba%20and%20xLSTM%2C%20several%20studies%20have%20shown%20that%20these%20models%20match%20or%20outperform%20the%20state-of-the-art%20in%20single-channel%20speech%20enhancement%20and%20audio%20representation%20learning.%20However%2C%20prior%20research%20has%20demonstrated%20that%20sequence%20models%20like%20LSTM%20and%20Mamba%20tend%20to%20overfit%20to%20the%20training%20set.%20To%20address%20this%2C%20previous%20works%20have%20shown%20that%20adding%20self-attention%20to%20LSTMs%20substantially%20improves%20generalization%20performance%20for%20single-channel%20speech%20enhancement.%20Nevertheless%2C%20neither%20the%20concept%20of%20hybrid%20Mamba%20and%20time-frequency%20attention%20models%20nor%20their%20generalization%20performance%20have%20been%20explored%20for%20speech%20enhancement.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20hybrid%20architecture%2C%20MambAttention%2C%20which%20combines%20Mamba%20and%20shared%20time-%20and%20frequency-multi-head%20attention%20modules%20for%20generalizable%20single-channel%20speech%20enhancement.%20To%20train%20our%20model%2C%20we%20introduce%20VB-DemandEx%2C%20a%20dataset%20inspired%20by%20VoiceBank%2BDemand%20but%20with%20more%20challenging%20noise%20types%20and%20lower%20signal-to-noise%20ratios.%20Trained%20on%20VB-DemandEx%2C%20MambAttention%20significantly%20outperforms%20existing%20state-of-the-art%20discriminative%20LSTM-%2C%20xLSTM-%2C%20Mamba-%2C%20and%20Conformer-based%20systems%20of%20similar%20complexity%20across%20all%20reported%20metrics%20on%20two%20out-of-domain%20datasets%3A%20DNS%202020%20without%20reverberation%20and%20EARS-WHAM_v2.%20MambAttention%20also%20matches%20or%20outperforms%20generative%20diffusion%20models%20in%20generalization%20performance%20while%20being%20competitive%20with%20language%20model%20baselines.%20Ablation%20studies%20highlight%20the%20importance%20of%20weight%20sharing%20between%20time-%20and%20frequency-multi-head%20attention%20modules%20for%20generalization%20performance.%20Finally%2C%20we%20explore%20integrating%20the%20shared%20time-%20and%20frequency-multi-head%20attention%20modules%20with%20LSTM%20and%20xLSTM%2C%20which%20yields%20a%20notable%20performance%20improvement%20on%20the%20out-of-domain%20datasets.%20Yet%2C%20MambAttention%20remains%20superior%20for%20cross-corpus%20generalization%20across%20all%20reported%20evaluation%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2507.00966v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambAttention%253A%2520Mamba%2520with%2520Multi-Head%2520Attention%2520for%2520Generalizable%2520Single-Channel%2520Speech%2520Enhancement%26entry.906535625%3DNikolai%2520Lund%2520K%25C3%25BChne%2520and%2520Jesper%2520Jensen%2520and%2520Jan%2520%25C3%2598stergaard%2520and%2520Zheng-Hua%2520Tan%26entry.1292438233%3DWith%2520new%2520sequence%2520models%2520like%2520Mamba%2520and%2520xLSTM%252C%2520several%2520studies%2520have%2520shown%2520that%2520these%2520models%2520match%2520or%2520outperform%2520the%2520state-of-the-art%2520in%2520single-channel%2520speech%2520enhancement%2520and%2520audio%2520representation%2520learning.%2520However%252C%2520prior%2520research%2520has%2520demonstrated%2520that%2520sequence%2520models%2520like%2520LSTM%2520and%2520Mamba%2520tend%2520to%2520overfit%2520to%2520the%2520training%2520set.%2520To%2520address%2520this%252C%2520previous%2520works%2520have%2520shown%2520that%2520adding%2520self-attention%2520to%2520LSTMs%2520substantially%2520improves%2520generalization%2520performance%2520for%2520single-channel%2520speech%2520enhancement.%2520Nevertheless%252C%2520neither%2520the%2520concept%2520of%2520hybrid%2520Mamba%2520and%2520time-frequency%2520attention%2520models%2520nor%2520their%2520generalization%2520performance%2520have%2520been%2520explored%2520for%2520speech%2520enhancement.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520hybrid%2520architecture%252C%2520MambAttention%252C%2520which%2520combines%2520Mamba%2520and%2520shared%2520time-%2520and%2520frequency-multi-head%2520attention%2520modules%2520for%2520generalizable%2520single-channel%2520speech%2520enhancement.%2520To%2520train%2520our%2520model%252C%2520we%2520introduce%2520VB-DemandEx%252C%2520a%2520dataset%2520inspired%2520by%2520VoiceBank%252BDemand%2520but%2520with%2520more%2520challenging%2520noise%2520types%2520and%2520lower%2520signal-to-noise%2520ratios.%2520Trained%2520on%2520VB-DemandEx%252C%2520MambAttention%2520significantly%2520outperforms%2520existing%2520state-of-the-art%2520discriminative%2520LSTM-%252C%2520xLSTM-%252C%2520Mamba-%252C%2520and%2520Conformer-based%2520systems%2520of%2520similar%2520complexity%2520across%2520all%2520reported%2520metrics%2520on%2520two%2520out-of-domain%2520datasets%253A%2520DNS%25202020%2520without%2520reverberation%2520and%2520EARS-WHAM_v2.%2520MambAttention%2520also%2520matches%2520or%2520outperforms%2520generative%2520diffusion%2520models%2520in%2520generalization%2520performance%2520while%2520being%2520competitive%2520with%2520language%2520model%2520baselines.%2520Ablation%2520studies%2520highlight%2520the%2520importance%2520of%2520weight%2520sharing%2520between%2520time-%2520and%2520frequency-multi-head%2520attention%2520modules%2520for%2520generalization%2520performance.%2520Finally%252C%2520we%2520explore%2520integrating%2520the%2520shared%2520time-%2520and%2520frequency-multi-head%2520attention%2520modules%2520with%2520LSTM%2520and%2520xLSTM%252C%2520which%2520yields%2520a%2520notable%2520performance%2520improvement%2520on%2520the%2520out-of-domain%2520datasets.%2520Yet%252C%2520MambAttention%2520remains%2520superior%2520for%2520cross-corpus%2520generalization%2520across%2520all%2520reported%2520evaluation%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00966v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambAttention%3A%20Mamba%20with%20Multi-Head%20Attention%20for%20Generalizable%20Single-Channel%20Speech%20Enhancement&entry.906535625=Nikolai%20Lund%20K%C3%BChne%20and%20Jesper%20Jensen%20and%20Jan%20%C3%98stergaard%20and%20Zheng-Hua%20Tan&entry.1292438233=With%20new%20sequence%20models%20like%20Mamba%20and%20xLSTM%2C%20several%20studies%20have%20shown%20that%20these%20models%20match%20or%20outperform%20the%20state-of-the-art%20in%20single-channel%20speech%20enhancement%20and%20audio%20representation%20learning.%20However%2C%20prior%20research%20has%20demonstrated%20that%20sequence%20models%20like%20LSTM%20and%20Mamba%20tend%20to%20overfit%20to%20the%20training%20set.%20To%20address%20this%2C%20previous%20works%20have%20shown%20that%20adding%20self-attention%20to%20LSTMs%20substantially%20improves%20generalization%20performance%20for%20single-channel%20speech%20enhancement.%20Nevertheless%2C%20neither%20the%20concept%20of%20hybrid%20Mamba%20and%20time-frequency%20attention%20models%20nor%20their%20generalization%20performance%20have%20been%20explored%20for%20speech%20enhancement.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20hybrid%20architecture%2C%20MambAttention%2C%20which%20combines%20Mamba%20and%20shared%20time-%20and%20frequency-multi-head%20attention%20modules%20for%20generalizable%20single-channel%20speech%20enhancement.%20To%20train%20our%20model%2C%20we%20introduce%20VB-DemandEx%2C%20a%20dataset%20inspired%20by%20VoiceBank%2BDemand%20but%20with%20more%20challenging%20noise%20types%20and%20lower%20signal-to-noise%20ratios.%20Trained%20on%20VB-DemandEx%2C%20MambAttention%20significantly%20outperforms%20existing%20state-of-the-art%20discriminative%20LSTM-%2C%20xLSTM-%2C%20Mamba-%2C%20and%20Conformer-based%20systems%20of%20similar%20complexity%20across%20all%20reported%20metrics%20on%20two%20out-of-domain%20datasets%3A%20DNS%202020%20without%20reverberation%20and%20EARS-WHAM_v2.%20MambAttention%20also%20matches%20or%20outperforms%20generative%20diffusion%20models%20in%20generalization%20performance%20while%20being%20competitive%20with%20language%20model%20baselines.%20Ablation%20studies%20highlight%20the%20importance%20of%20weight%20sharing%20between%20time-%20and%20frequency-multi-head%20attention%20modules%20for%20generalization%20performance.%20Finally%2C%20we%20explore%20integrating%20the%20shared%20time-%20and%20frequency-multi-head%20attention%20modules%20with%20LSTM%20and%20xLSTM%2C%20which%20yields%20a%20notable%20performance%20improvement%20on%20the%20out-of-domain%20datasets.%20Yet%2C%20MambAttention%20remains%20superior%20for%20cross-corpus%20generalization%20across%20all%20reported%20evaluation%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2507.00966v3&entry.124074799=Read"},
{"title": "Dual Refinement Cycle Learning: Unsupervised Text Classification of Mamba and Community Detection on Text Attributed Graph", "author": "Hong Wang and Yinglong Zhang and Hanhan Guo and Xuewen Xia and Xing Xu", "abstract": "Pretrained language models offer strong text understanding capabilities but remain difficult to deploy in real-world text-attributed networks due to their heavy dependence on labeled data. Meanwhile, community detection methods typically ignore textual semantics, limiting their usefulness in downstream applications such as content organization, recommendation, and risk monitoring. To overcome these limitations, we present Dual Refinement Cycle Learning (DRCL), a fully unsupervised framework designed for practical scenarios where no labels or category definitions are available. DRCL integrates structural and semantic information through a warm-start initialization and a bidirectional refinement cycle between a GCN-based Community Detection Module (GCN-CDM) and a Text Semantic Modeling Module (TSMM). The two modules iteratively exchange pseudo-labels, allowing semantic cues to enhance structural clustering and structural patterns to guide text representation learning without manual supervision. Across several text-attributed graph datasets, DRCL consistently improves the structural and semantic quality of discovered communities. Moreover, a Mamba-based classifier trained solely from DRCL's community signals achieves accuracy comparable to supervised models, demonstrating its potential for deployment in large-scale systems where labeled data are scarce or costly. The code is available at https://github.com/wuanghoong/DRCL.git.", "link": "http://arxiv.org/abs/2512.07100v2", "date": "2025-12-10", "relevancy": 2.5455, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.529}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5049}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Refinement%20Cycle%20Learning%3A%20Unsupervised%20Text%20Classification%20of%20Mamba%20and%20Community%20Detection%20on%20Text%20Attributed%20Graph&body=Title%3A%20Dual%20Refinement%20Cycle%20Learning%3A%20Unsupervised%20Text%20Classification%20of%20Mamba%20and%20Community%20Detection%20on%20Text%20Attributed%20Graph%0AAuthor%3A%20Hong%20Wang%20and%20Yinglong%20Zhang%20and%20Hanhan%20Guo%20and%20Xuewen%20Xia%20and%20Xing%20Xu%0AAbstract%3A%20Pretrained%20language%20models%20offer%20strong%20text%20understanding%20capabilities%20but%20remain%20difficult%20to%20deploy%20in%20real-world%20text-attributed%20networks%20due%20to%20their%20heavy%20dependence%20on%20labeled%20data.%20Meanwhile%2C%20community%20detection%20methods%20typically%20ignore%20textual%20semantics%2C%20limiting%20their%20usefulness%20in%20downstream%20applications%20such%20as%20content%20organization%2C%20recommendation%2C%20and%20risk%20monitoring.%20To%20overcome%20these%20limitations%2C%20we%20present%20Dual%20Refinement%20Cycle%20Learning%20%28DRCL%29%2C%20a%20fully%20unsupervised%20framework%20designed%20for%20practical%20scenarios%20where%20no%20labels%20or%20category%20definitions%20are%20available.%20DRCL%20integrates%20structural%20and%20semantic%20information%20through%20a%20warm-start%20initialization%20and%20a%20bidirectional%20refinement%20cycle%20between%20a%20GCN-based%20Community%20Detection%20Module%20%28GCN-CDM%29%20and%20a%20Text%20Semantic%20Modeling%20Module%20%28TSMM%29.%20The%20two%20modules%20iteratively%20exchange%20pseudo-labels%2C%20allowing%20semantic%20cues%20to%20enhance%20structural%20clustering%20and%20structural%20patterns%20to%20guide%20text%20representation%20learning%20without%20manual%20supervision.%20Across%20several%20text-attributed%20graph%20datasets%2C%20DRCL%20consistently%20improves%20the%20structural%20and%20semantic%20quality%20of%20discovered%20communities.%20Moreover%2C%20a%20Mamba-based%20classifier%20trained%20solely%20from%20DRCL%27s%20community%20signals%20achieves%20accuracy%20comparable%20to%20supervised%20models%2C%20demonstrating%20its%20potential%20for%20deployment%20in%20large-scale%20systems%20where%20labeled%20data%20are%20scarce%20or%20costly.%20The%20code%20is%20available%20at%20https%3A//github.com/wuanghoong/DRCL.git.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07100v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Refinement%2520Cycle%2520Learning%253A%2520Unsupervised%2520Text%2520Classification%2520of%2520Mamba%2520and%2520Community%2520Detection%2520on%2520Text%2520Attributed%2520Graph%26entry.906535625%3DHong%2520Wang%2520and%2520Yinglong%2520Zhang%2520and%2520Hanhan%2520Guo%2520and%2520Xuewen%2520Xia%2520and%2520Xing%2520Xu%26entry.1292438233%3DPretrained%2520language%2520models%2520offer%2520strong%2520text%2520understanding%2520capabilities%2520but%2520remain%2520difficult%2520to%2520deploy%2520in%2520real-world%2520text-attributed%2520networks%2520due%2520to%2520their%2520heavy%2520dependence%2520on%2520labeled%2520data.%2520Meanwhile%252C%2520community%2520detection%2520methods%2520typically%2520ignore%2520textual%2520semantics%252C%2520limiting%2520their%2520usefulness%2520in%2520downstream%2520applications%2520such%2520as%2520content%2520organization%252C%2520recommendation%252C%2520and%2520risk%2520monitoring.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520present%2520Dual%2520Refinement%2520Cycle%2520Learning%2520%2528DRCL%2529%252C%2520a%2520fully%2520unsupervised%2520framework%2520designed%2520for%2520practical%2520scenarios%2520where%2520no%2520labels%2520or%2520category%2520definitions%2520are%2520available.%2520DRCL%2520integrates%2520structural%2520and%2520semantic%2520information%2520through%2520a%2520warm-start%2520initialization%2520and%2520a%2520bidirectional%2520refinement%2520cycle%2520between%2520a%2520GCN-based%2520Community%2520Detection%2520Module%2520%2528GCN-CDM%2529%2520and%2520a%2520Text%2520Semantic%2520Modeling%2520Module%2520%2528TSMM%2529.%2520The%2520two%2520modules%2520iteratively%2520exchange%2520pseudo-labels%252C%2520allowing%2520semantic%2520cues%2520to%2520enhance%2520structural%2520clustering%2520and%2520structural%2520patterns%2520to%2520guide%2520text%2520representation%2520learning%2520without%2520manual%2520supervision.%2520Across%2520several%2520text-attributed%2520graph%2520datasets%252C%2520DRCL%2520consistently%2520improves%2520the%2520structural%2520and%2520semantic%2520quality%2520of%2520discovered%2520communities.%2520Moreover%252C%2520a%2520Mamba-based%2520classifier%2520trained%2520solely%2520from%2520DRCL%2527s%2520community%2520signals%2520achieves%2520accuracy%2520comparable%2520to%2520supervised%2520models%252C%2520demonstrating%2520its%2520potential%2520for%2520deployment%2520in%2520large-scale%2520systems%2520where%2520labeled%2520data%2520are%2520scarce%2520or%2520costly.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/wuanghoong/DRCL.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07100v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Refinement%20Cycle%20Learning%3A%20Unsupervised%20Text%20Classification%20of%20Mamba%20and%20Community%20Detection%20on%20Text%20Attributed%20Graph&entry.906535625=Hong%20Wang%20and%20Yinglong%20Zhang%20and%20Hanhan%20Guo%20and%20Xuewen%20Xia%20and%20Xing%20Xu&entry.1292438233=Pretrained%20language%20models%20offer%20strong%20text%20understanding%20capabilities%20but%20remain%20difficult%20to%20deploy%20in%20real-world%20text-attributed%20networks%20due%20to%20their%20heavy%20dependence%20on%20labeled%20data.%20Meanwhile%2C%20community%20detection%20methods%20typically%20ignore%20textual%20semantics%2C%20limiting%20their%20usefulness%20in%20downstream%20applications%20such%20as%20content%20organization%2C%20recommendation%2C%20and%20risk%20monitoring.%20To%20overcome%20these%20limitations%2C%20we%20present%20Dual%20Refinement%20Cycle%20Learning%20%28DRCL%29%2C%20a%20fully%20unsupervised%20framework%20designed%20for%20practical%20scenarios%20where%20no%20labels%20or%20category%20definitions%20are%20available.%20DRCL%20integrates%20structural%20and%20semantic%20information%20through%20a%20warm-start%20initialization%20and%20a%20bidirectional%20refinement%20cycle%20between%20a%20GCN-based%20Community%20Detection%20Module%20%28GCN-CDM%29%20and%20a%20Text%20Semantic%20Modeling%20Module%20%28TSMM%29.%20The%20two%20modules%20iteratively%20exchange%20pseudo-labels%2C%20allowing%20semantic%20cues%20to%20enhance%20structural%20clustering%20and%20structural%20patterns%20to%20guide%20text%20representation%20learning%20without%20manual%20supervision.%20Across%20several%20text-attributed%20graph%20datasets%2C%20DRCL%20consistently%20improves%20the%20structural%20and%20semantic%20quality%20of%20discovered%20communities.%20Moreover%2C%20a%20Mamba-based%20classifier%20trained%20solely%20from%20DRCL%27s%20community%20signals%20achieves%20accuracy%20comparable%20to%20supervised%20models%2C%20demonstrating%20its%20potential%20for%20deployment%20in%20large-scale%20systems%20where%20labeled%20data%20are%20scarce%20or%20costly.%20The%20code%20is%20available%20at%20https%3A//github.com/wuanghoong/DRCL.git.&entry.1838667208=http%3A//arxiv.org/abs/2512.07100v2&entry.124074799=Read"},
{"title": "WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion", "author": "Sofiane Bouaziz and Adel Hafiane and Raphael Canals and Rachid Nedjai", "abstract": "Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a weakly-supervised generative network for daily 10 m LST estimation via spatio-temporal fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.05% and improves SSIM by 4.22%. Furthermore, WGAST effectively captures fine-scale thermal patterns, as validated against near-surface air temperature measurements from 33 near-ground sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.", "link": "http://arxiv.org/abs/2508.06485v2", "date": "2025-12-10", "relevancy": 2.5371, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5288}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5098}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WGAST%3A%20Weakly-Supervised%20Generative%20Network%20for%20Daily%2010%20m%20Land%20Surface%20Temperature%20Estimation%20via%20Spatio-Temporal%20Fusion&body=Title%3A%20WGAST%3A%20Weakly-Supervised%20Generative%20Network%20for%20Daily%2010%20m%20Land%20Surface%20Temperature%20Estimation%20via%20Spatio-Temporal%20Fusion%0AAuthor%3A%20Sofiane%20Bouaziz%20and%20Adel%20Hafiane%20and%20Raphael%20Canals%20and%20Rachid%20Nedjai%0AAbstract%3A%20Urbanization%2C%20climate%20change%2C%20and%20agricultural%20stress%20are%20increasing%20the%20demand%20for%20precise%20and%20timely%20environmental%20monitoring.%20Land%20Surface%20Temperature%20%28LST%29%20is%20a%20key%20variable%20in%20this%20context%20and%20is%20retrieved%20from%20remote%20sensing%20satellites.%20However%2C%20these%20systems%20face%20a%20trade-off%20between%20spatial%20and%20temporal%20resolution.%20While%20spatio-temporal%20fusion%20methods%20offer%20promising%20solutions%2C%20few%20have%20addressed%20the%20estimation%20of%20daily%20LST%20at%2010%20m%20resolution.%20In%20this%20study%2C%20we%20present%20WGAST%2C%20a%20weakly-supervised%20generative%20network%20for%20daily%2010%20m%20LST%20estimation%20via%20spatio-temporal%20fusion%20of%20Terra%20MODIS%2C%20Landsat%208%2C%20and%20Sentinel-2.%20WGAST%20is%20the%20first%20end-to-end%20deep%20learning%20framework%20designed%20for%20this%20task.%20It%20adopts%20a%20conditional%20generative%20adversarial%20architecture%2C%20with%20a%20generator%20composed%20of%20four%20stages%3A%20feature%20extraction%2C%20fusion%2C%20LST%20reconstruction%2C%20and%20noise%20suppression.%20The%20first%20stage%20employs%20a%20set%20of%20encoders%20to%20extract%20multi-level%20latent%20representations%20from%20the%20inputs%2C%20which%20are%20then%20fused%20in%20the%20second%20stage%20using%20cosine%20similarity%2C%20normalization%2C%20and%20temporal%20attention%20mechanisms.%20The%20third%20stage%20decodes%20the%20fused%20features%20into%20high-resolution%20LST%2C%20followed%20by%20a%20Gaussian%20filter%20to%20suppress%20high-frequency%20noise.%20Training%20follows%20a%20weakly%20supervised%20strategy%20based%20on%20physical%20averaging%20principles%20and%20reinforced%20by%20a%20PatchGAN%20discriminator.%20Experiments%20demonstrate%20that%20WGAST%20outperforms%20existing%20methods%20in%20both%20quantitative%20and%20qualitative%20evaluations.%20Compared%20to%20the%20best-performing%20baseline%2C%20on%20average%2C%20WGAST%20reduces%20RMSE%20by%2017.05%25%20and%20improves%20SSIM%20by%204.22%25.%20Furthermore%2C%20WGAST%20effectively%20captures%20fine-scale%20thermal%20patterns%2C%20as%20validated%20against%20near-surface%20air%20temperature%20measurements%20from%2033%20near-ground%20sensors.%20The%20code%20is%20available%20at%20https%3A//github.com/Sofianebouaziz1/WGAST.git.%0ALink%3A%20http%3A//arxiv.org/abs/2508.06485v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWGAST%253A%2520Weakly-Supervised%2520Generative%2520Network%2520for%2520Daily%252010%2520m%2520Land%2520Surface%2520Temperature%2520Estimation%2520via%2520Spatio-Temporal%2520Fusion%26entry.906535625%3DSofiane%2520Bouaziz%2520and%2520Adel%2520Hafiane%2520and%2520Raphael%2520Canals%2520and%2520Rachid%2520Nedjai%26entry.1292438233%3DUrbanization%252C%2520climate%2520change%252C%2520and%2520agricultural%2520stress%2520are%2520increasing%2520the%2520demand%2520for%2520precise%2520and%2520timely%2520environmental%2520monitoring.%2520Land%2520Surface%2520Temperature%2520%2528LST%2529%2520is%2520a%2520key%2520variable%2520in%2520this%2520context%2520and%2520is%2520retrieved%2520from%2520remote%2520sensing%2520satellites.%2520However%252C%2520these%2520systems%2520face%2520a%2520trade-off%2520between%2520spatial%2520and%2520temporal%2520resolution.%2520While%2520spatio-temporal%2520fusion%2520methods%2520offer%2520promising%2520solutions%252C%2520few%2520have%2520addressed%2520the%2520estimation%2520of%2520daily%2520LST%2520at%252010%2520m%2520resolution.%2520In%2520this%2520study%252C%2520we%2520present%2520WGAST%252C%2520a%2520weakly-supervised%2520generative%2520network%2520for%2520daily%252010%2520m%2520LST%2520estimation%2520via%2520spatio-temporal%2520fusion%2520of%2520Terra%2520MODIS%252C%2520Landsat%25208%252C%2520and%2520Sentinel-2.%2520WGAST%2520is%2520the%2520first%2520end-to-end%2520deep%2520learning%2520framework%2520designed%2520for%2520this%2520task.%2520It%2520adopts%2520a%2520conditional%2520generative%2520adversarial%2520architecture%252C%2520with%2520a%2520generator%2520composed%2520of%2520four%2520stages%253A%2520feature%2520extraction%252C%2520fusion%252C%2520LST%2520reconstruction%252C%2520and%2520noise%2520suppression.%2520The%2520first%2520stage%2520employs%2520a%2520set%2520of%2520encoders%2520to%2520extract%2520multi-level%2520latent%2520representations%2520from%2520the%2520inputs%252C%2520which%2520are%2520then%2520fused%2520in%2520the%2520second%2520stage%2520using%2520cosine%2520similarity%252C%2520normalization%252C%2520and%2520temporal%2520attention%2520mechanisms.%2520The%2520third%2520stage%2520decodes%2520the%2520fused%2520features%2520into%2520high-resolution%2520LST%252C%2520followed%2520by%2520a%2520Gaussian%2520filter%2520to%2520suppress%2520high-frequency%2520noise.%2520Training%2520follows%2520a%2520weakly%2520supervised%2520strategy%2520based%2520on%2520physical%2520averaging%2520principles%2520and%2520reinforced%2520by%2520a%2520PatchGAN%2520discriminator.%2520Experiments%2520demonstrate%2520that%2520WGAST%2520outperforms%2520existing%2520methods%2520in%2520both%2520quantitative%2520and%2520qualitative%2520evaluations.%2520Compared%2520to%2520the%2520best-performing%2520baseline%252C%2520on%2520average%252C%2520WGAST%2520reduces%2520RMSE%2520by%252017.05%2525%2520and%2520improves%2520SSIM%2520by%25204.22%2525.%2520Furthermore%252C%2520WGAST%2520effectively%2520captures%2520fine-scale%2520thermal%2520patterns%252C%2520as%2520validated%2520against%2520near-surface%2520air%2520temperature%2520measurements%2520from%252033%2520near-ground%2520sensors.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/Sofianebouaziz1/WGAST.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06485v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WGAST%3A%20Weakly-Supervised%20Generative%20Network%20for%20Daily%2010%20m%20Land%20Surface%20Temperature%20Estimation%20via%20Spatio-Temporal%20Fusion&entry.906535625=Sofiane%20Bouaziz%20and%20Adel%20Hafiane%20and%20Raphael%20Canals%20and%20Rachid%20Nedjai&entry.1292438233=Urbanization%2C%20climate%20change%2C%20and%20agricultural%20stress%20are%20increasing%20the%20demand%20for%20precise%20and%20timely%20environmental%20monitoring.%20Land%20Surface%20Temperature%20%28LST%29%20is%20a%20key%20variable%20in%20this%20context%20and%20is%20retrieved%20from%20remote%20sensing%20satellites.%20However%2C%20these%20systems%20face%20a%20trade-off%20between%20spatial%20and%20temporal%20resolution.%20While%20spatio-temporal%20fusion%20methods%20offer%20promising%20solutions%2C%20few%20have%20addressed%20the%20estimation%20of%20daily%20LST%20at%2010%20m%20resolution.%20In%20this%20study%2C%20we%20present%20WGAST%2C%20a%20weakly-supervised%20generative%20network%20for%20daily%2010%20m%20LST%20estimation%20via%20spatio-temporal%20fusion%20of%20Terra%20MODIS%2C%20Landsat%208%2C%20and%20Sentinel-2.%20WGAST%20is%20the%20first%20end-to-end%20deep%20learning%20framework%20designed%20for%20this%20task.%20It%20adopts%20a%20conditional%20generative%20adversarial%20architecture%2C%20with%20a%20generator%20composed%20of%20four%20stages%3A%20feature%20extraction%2C%20fusion%2C%20LST%20reconstruction%2C%20and%20noise%20suppression.%20The%20first%20stage%20employs%20a%20set%20of%20encoders%20to%20extract%20multi-level%20latent%20representations%20from%20the%20inputs%2C%20which%20are%20then%20fused%20in%20the%20second%20stage%20using%20cosine%20similarity%2C%20normalization%2C%20and%20temporal%20attention%20mechanisms.%20The%20third%20stage%20decodes%20the%20fused%20features%20into%20high-resolution%20LST%2C%20followed%20by%20a%20Gaussian%20filter%20to%20suppress%20high-frequency%20noise.%20Training%20follows%20a%20weakly%20supervised%20strategy%20based%20on%20physical%20averaging%20principles%20and%20reinforced%20by%20a%20PatchGAN%20discriminator.%20Experiments%20demonstrate%20that%20WGAST%20outperforms%20existing%20methods%20in%20both%20quantitative%20and%20qualitative%20evaluations.%20Compared%20to%20the%20best-performing%20baseline%2C%20on%20average%2C%20WGAST%20reduces%20RMSE%20by%2017.05%25%20and%20improves%20SSIM%20by%204.22%25.%20Furthermore%2C%20WGAST%20effectively%20captures%20fine-scale%20thermal%20patterns%2C%20as%20validated%20against%20near-surface%20air%20temperature%20measurements%20from%2033%20near-ground%20sensors.%20The%20code%20is%20available%20at%20https%3A//github.com/Sofianebouaziz1/WGAST.git.&entry.1838667208=http%3A//arxiv.org/abs/2508.06485v2&entry.124074799=Read"},
{"title": "Improving Graph Neural Network Training, Defense, Hypergraph Partitioning and Spectral Clustering via Adversarial Robustness Evaluation", "author": "Yongyu Wang", "abstract": "Graph Neural Networks (GNNs) are a highly effective neural network architecture for processing graph-structured data. Unlike traditional neural networks that rely solely on the features of the data as input, GNNs leverage both the graph structure, which represents the relationships between data points, and the feature matrix of the data to optimize their feature representation. This unique capability enables GNNs to achieve superior performance across various tasks. However, it also makes GNNs more susceptible to noise from both the graph structure and data features, which can significantly increase the training difficulty and degrade their performance. To address this issue, this paper proposes a novel method for selecting noise-sensitive training samples from the original training set to construct a smaller yet more effective training set for model training. These samples are used to help improve the model's ability to correctly process data in noisy environments. We have evaluated our approach on three of the most classical GNN models GCN, GAT, and GraphSAGE as well as three widely used benchmark datasets: Cora, Citeseer, and PubMed. Our experiments demonstrate that the proposed method can substantially boost the training of Graph Neural Networks compared to using randomly sampled training sets of the same size from the original training set and the larger original full training set. We further proposed a robust-node based hypergraph partitioning method, an adversarial robustness based graph pruning method for GNN defenses, a robust spectral clustering method and a related spectral edge attack method.", "link": "http://arxiv.org/abs/2412.14738v8", "date": "2025-12-10", "relevancy": 2.5366, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5389}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4955}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Graph%20Neural%20Network%20Training%2C%20Defense%2C%20Hypergraph%20Partitioning%20and%20Spectral%20Clustering%20via%20Adversarial%20Robustness%20Evaluation&body=Title%3A%20Improving%20Graph%20Neural%20Network%20Training%2C%20Defense%2C%20Hypergraph%20Partitioning%20and%20Spectral%20Clustering%20via%20Adversarial%20Robustness%20Evaluation%0AAuthor%3A%20Yongyu%20Wang%0AAbstract%3A%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20a%20highly%20effective%20neural%20network%20architecture%20for%20processing%20graph-structured%20data.%20Unlike%20traditional%20neural%20networks%20that%20rely%20solely%20on%20the%20features%20of%20the%20data%20as%20input%2C%20GNNs%20leverage%20both%20the%20graph%20structure%2C%20which%20represents%20the%20relationships%20between%20data%20points%2C%20and%20the%20feature%20matrix%20of%20the%20data%20to%20optimize%20their%20feature%20representation.%20This%20unique%20capability%20enables%20GNNs%20to%20achieve%20superior%20performance%20across%20various%20tasks.%20However%2C%20it%20also%20makes%20GNNs%20more%20susceptible%20to%20noise%20from%20both%20the%20graph%20structure%20and%20data%20features%2C%20which%20can%20significantly%20increase%20the%20training%20difficulty%20and%20degrade%20their%20performance.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20novel%20method%20for%20selecting%20noise-sensitive%20training%20samples%20from%20the%20original%20training%20set%20to%20construct%20a%20smaller%20yet%20more%20effective%20training%20set%20for%20model%20training.%20These%20samples%20are%20used%20to%20help%20improve%20the%20model%27s%20ability%20to%20correctly%20process%20data%20in%20noisy%20environments.%20We%20have%20evaluated%20our%20approach%20on%20three%20of%20the%20most%20classical%20GNN%20models%20GCN%2C%20GAT%2C%20and%20GraphSAGE%20as%20well%20as%20three%20widely%20used%20benchmark%20datasets%3A%20Cora%2C%20Citeseer%2C%20and%20PubMed.%20Our%20experiments%20demonstrate%20that%20the%20proposed%20method%20can%20substantially%20boost%20the%20training%20of%20Graph%20Neural%20Networks%20compared%20to%20using%20randomly%20sampled%20training%20sets%20of%20the%20same%20size%20from%20the%20original%20training%20set%20and%20the%20larger%20original%20full%20training%20set.%20We%20further%20proposed%20a%20robust-node%20based%20hypergraph%20partitioning%20method%2C%20an%20adversarial%20robustness%20based%20graph%20pruning%20method%20for%20GNN%20defenses%2C%20a%20robust%20spectral%20clustering%20method%20and%20a%20related%20spectral%20edge%20attack%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2412.14738v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Graph%2520Neural%2520Network%2520Training%252C%2520Defense%252C%2520Hypergraph%2520Partitioning%2520and%2520Spectral%2520Clustering%2520via%2520Adversarial%2520Robustness%2520Evaluation%26entry.906535625%3DYongyu%2520Wang%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520a%2520highly%2520effective%2520neural%2520network%2520architecture%2520for%2520processing%2520graph-structured%2520data.%2520Unlike%2520traditional%2520neural%2520networks%2520that%2520rely%2520solely%2520on%2520the%2520features%2520of%2520the%2520data%2520as%2520input%252C%2520GNNs%2520leverage%2520both%2520the%2520graph%2520structure%252C%2520which%2520represents%2520the%2520relationships%2520between%2520data%2520points%252C%2520and%2520the%2520feature%2520matrix%2520of%2520the%2520data%2520to%2520optimize%2520their%2520feature%2520representation.%2520This%2520unique%2520capability%2520enables%2520GNNs%2520to%2520achieve%2520superior%2520performance%2520across%2520various%2520tasks.%2520However%252C%2520it%2520also%2520makes%2520GNNs%2520more%2520susceptible%2520to%2520noise%2520from%2520both%2520the%2520graph%2520structure%2520and%2520data%2520features%252C%2520which%2520can%2520significantly%2520increase%2520the%2520training%2520difficulty%2520and%2520degrade%2520their%2520performance.%2520To%2520address%2520this%2520issue%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520method%2520for%2520selecting%2520noise-sensitive%2520training%2520samples%2520from%2520the%2520original%2520training%2520set%2520to%2520construct%2520a%2520smaller%2520yet%2520more%2520effective%2520training%2520set%2520for%2520model%2520training.%2520These%2520samples%2520are%2520used%2520to%2520help%2520improve%2520the%2520model%2527s%2520ability%2520to%2520correctly%2520process%2520data%2520in%2520noisy%2520environments.%2520We%2520have%2520evaluated%2520our%2520approach%2520on%2520three%2520of%2520the%2520most%2520classical%2520GNN%2520models%2520GCN%252C%2520GAT%252C%2520and%2520GraphSAGE%2520as%2520well%2520as%2520three%2520widely%2520used%2520benchmark%2520datasets%253A%2520Cora%252C%2520Citeseer%252C%2520and%2520PubMed.%2520Our%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520method%2520can%2520substantially%2520boost%2520the%2520training%2520of%2520Graph%2520Neural%2520Networks%2520compared%2520to%2520using%2520randomly%2520sampled%2520training%2520sets%2520of%2520the%2520same%2520size%2520from%2520the%2520original%2520training%2520set%2520and%2520the%2520larger%2520original%2520full%2520training%2520set.%2520We%2520further%2520proposed%2520a%2520robust-node%2520based%2520hypergraph%2520partitioning%2520method%252C%2520an%2520adversarial%2520robustness%2520based%2520graph%2520pruning%2520method%2520for%2520GNN%2520defenses%252C%2520a%2520robust%2520spectral%2520clustering%2520method%2520and%2520a%2520related%2520spectral%2520edge%2520attack%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14738v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Graph%20Neural%20Network%20Training%2C%20Defense%2C%20Hypergraph%20Partitioning%20and%20Spectral%20Clustering%20via%20Adversarial%20Robustness%20Evaluation&entry.906535625=Yongyu%20Wang&entry.1292438233=Graph%20Neural%20Networks%20%28GNNs%29%20are%20a%20highly%20effective%20neural%20network%20architecture%20for%20processing%20graph-structured%20data.%20Unlike%20traditional%20neural%20networks%20that%20rely%20solely%20on%20the%20features%20of%20the%20data%20as%20input%2C%20GNNs%20leverage%20both%20the%20graph%20structure%2C%20which%20represents%20the%20relationships%20between%20data%20points%2C%20and%20the%20feature%20matrix%20of%20the%20data%20to%20optimize%20their%20feature%20representation.%20This%20unique%20capability%20enables%20GNNs%20to%20achieve%20superior%20performance%20across%20various%20tasks.%20However%2C%20it%20also%20makes%20GNNs%20more%20susceptible%20to%20noise%20from%20both%20the%20graph%20structure%20and%20data%20features%2C%20which%20can%20significantly%20increase%20the%20training%20difficulty%20and%20degrade%20their%20performance.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20novel%20method%20for%20selecting%20noise-sensitive%20training%20samples%20from%20the%20original%20training%20set%20to%20construct%20a%20smaller%20yet%20more%20effective%20training%20set%20for%20model%20training.%20These%20samples%20are%20used%20to%20help%20improve%20the%20model%27s%20ability%20to%20correctly%20process%20data%20in%20noisy%20environments.%20We%20have%20evaluated%20our%20approach%20on%20three%20of%20the%20most%20classical%20GNN%20models%20GCN%2C%20GAT%2C%20and%20GraphSAGE%20as%20well%20as%20three%20widely%20used%20benchmark%20datasets%3A%20Cora%2C%20Citeseer%2C%20and%20PubMed.%20Our%20experiments%20demonstrate%20that%20the%20proposed%20method%20can%20substantially%20boost%20the%20training%20of%20Graph%20Neural%20Networks%20compared%20to%20using%20randomly%20sampled%20training%20sets%20of%20the%20same%20size%20from%20the%20original%20training%20set%20and%20the%20larger%20original%20full%20training%20set.%20We%20further%20proposed%20a%20robust-node%20based%20hypergraph%20partitioning%20method%2C%20an%20adversarial%20robustness%20based%20graph%20pruning%20method%20for%20GNN%20defenses%2C%20a%20robust%20spectral%20clustering%20method%20and%20a%20related%20spectral%20edge%20attack%20method.&entry.1838667208=http%3A//arxiv.org/abs/2412.14738v8&entry.124074799=Read"},
{"title": "Towards Robust Infrared Small Target Detection: A Feature-Enhanced and Sensitivity-Tunable Framework", "author": "Jinmiao Zhao and Zelin Shi and Chuang Yu and Yunpeng Liu and Yimian Dai", "abstract": "Recently, single-frame infrared small target (SIRST) detection technology has attracted widespread attention. Different from most existing deep learning-based methods that focus on improving network architectures, we propose a feature-enhanced and sensitivity-tunable (FEST) framework, which is compatible with existing SIRST detection networks and further enhances their detection performance. The FEST framework improves the model's robustness from two aspects: feature enhancement and target confidence regulation. For feature enhancement, we employ a multi-scale fusion strategy to improve the model's perception to multi-scale features of multi-size targets, and design an edge enhancement difficulty mining (EEDM) loss to guide the network to continuously focus on challenging target regions and edge features during training. For target confidence regulation, an adjustable sensitivity (AS) strategy is proposed for network post-processing. This strategy enhances the model's adaptability in complex scenarios and significantly improves the detection rate of infrared small targets while maintaining segmentation accuracy. Extensive experimental results show that our FEST framework can effectively enhance the performance of existing SIRST detection networks. The code is available at https://github.com/YuChuang1205/FEST-Framework", "link": "http://arxiv.org/abs/2407.20090v3", "date": "2025-12-10", "relevancy": 2.5295, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5314}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5058}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20Infrared%20Small%20Target%20Detection%3A%20A%20Feature-Enhanced%20and%20Sensitivity-Tunable%20Framework&body=Title%3A%20Towards%20Robust%20Infrared%20Small%20Target%20Detection%3A%20A%20Feature-Enhanced%20and%20Sensitivity-Tunable%20Framework%0AAuthor%3A%20Jinmiao%20Zhao%20and%20Zelin%20Shi%20and%20Chuang%20Yu%20and%20Yunpeng%20Liu%20and%20Yimian%20Dai%0AAbstract%3A%20Recently%2C%20single-frame%20infrared%20small%20target%20%28SIRST%29%20detection%20technology%20has%20attracted%20widespread%20attention.%20Different%20from%20most%20existing%20deep%20learning-based%20methods%20that%20focus%20on%20improving%20network%20architectures%2C%20we%20propose%20a%20feature-enhanced%20and%20sensitivity-tunable%20%28FEST%29%20framework%2C%20which%20is%20compatible%20with%20existing%20SIRST%20detection%20networks%20and%20further%20enhances%20their%20detection%20performance.%20The%20FEST%20framework%20improves%20the%20model%27s%20robustness%20from%20two%20aspects%3A%20feature%20enhancement%20and%20target%20confidence%20regulation.%20For%20feature%20enhancement%2C%20we%20employ%20a%20multi-scale%20fusion%20strategy%20to%20improve%20the%20model%27s%20perception%20to%20multi-scale%20features%20of%20multi-size%20targets%2C%20and%20design%20an%20edge%20enhancement%20difficulty%20mining%20%28EEDM%29%20loss%20to%20guide%20the%20network%20to%20continuously%20focus%20on%20challenging%20target%20regions%20and%20edge%20features%20during%20training.%20For%20target%20confidence%20regulation%2C%20an%20adjustable%20sensitivity%20%28AS%29%20strategy%20is%20proposed%20for%20network%20post-processing.%20This%20strategy%20enhances%20the%20model%27s%20adaptability%20in%20complex%20scenarios%20and%20significantly%20improves%20the%20detection%20rate%20of%20infrared%20small%20targets%20while%20maintaining%20segmentation%20accuracy.%20Extensive%20experimental%20results%20show%20that%20our%20FEST%20framework%20can%20effectively%20enhance%20the%20performance%20of%20existing%20SIRST%20detection%20networks.%20The%20code%20is%20available%20at%20https%3A//github.com/YuChuang1205/FEST-Framework%0ALink%3A%20http%3A//arxiv.org/abs/2407.20090v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Robust%2520Infrared%2520Small%2520Target%2520Detection%253A%2520A%2520Feature-Enhanced%2520and%2520Sensitivity-Tunable%2520Framework%26entry.906535625%3DJinmiao%2520Zhao%2520and%2520Zelin%2520Shi%2520and%2520Chuang%2520Yu%2520and%2520Yunpeng%2520Liu%2520and%2520Yimian%2520Dai%26entry.1292438233%3DRecently%252C%2520single-frame%2520infrared%2520small%2520target%2520%2528SIRST%2529%2520detection%2520technology%2520has%2520attracted%2520widespread%2520attention.%2520Different%2520from%2520most%2520existing%2520deep%2520learning-based%2520methods%2520that%2520focus%2520on%2520improving%2520network%2520architectures%252C%2520we%2520propose%2520a%2520feature-enhanced%2520and%2520sensitivity-tunable%2520%2528FEST%2529%2520framework%252C%2520which%2520is%2520compatible%2520with%2520existing%2520SIRST%2520detection%2520networks%2520and%2520further%2520enhances%2520their%2520detection%2520performance.%2520The%2520FEST%2520framework%2520improves%2520the%2520model%2527s%2520robustness%2520from%2520two%2520aspects%253A%2520feature%2520enhancement%2520and%2520target%2520confidence%2520regulation.%2520For%2520feature%2520enhancement%252C%2520we%2520employ%2520a%2520multi-scale%2520fusion%2520strategy%2520to%2520improve%2520the%2520model%2527s%2520perception%2520to%2520multi-scale%2520features%2520of%2520multi-size%2520targets%252C%2520and%2520design%2520an%2520edge%2520enhancement%2520difficulty%2520mining%2520%2528EEDM%2529%2520loss%2520to%2520guide%2520the%2520network%2520to%2520continuously%2520focus%2520on%2520challenging%2520target%2520regions%2520and%2520edge%2520features%2520during%2520training.%2520For%2520target%2520confidence%2520regulation%252C%2520an%2520adjustable%2520sensitivity%2520%2528AS%2529%2520strategy%2520is%2520proposed%2520for%2520network%2520post-processing.%2520This%2520strategy%2520enhances%2520the%2520model%2527s%2520adaptability%2520in%2520complex%2520scenarios%2520and%2520significantly%2520improves%2520the%2520detection%2520rate%2520of%2520infrared%2520small%2520targets%2520while%2520maintaining%2520segmentation%2520accuracy.%2520Extensive%2520experimental%2520results%2520show%2520that%2520our%2520FEST%2520framework%2520can%2520effectively%2520enhance%2520the%2520performance%2520of%2520existing%2520SIRST%2520detection%2520networks.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/YuChuang1205/FEST-Framework%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20090v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20Infrared%20Small%20Target%20Detection%3A%20A%20Feature-Enhanced%20and%20Sensitivity-Tunable%20Framework&entry.906535625=Jinmiao%20Zhao%20and%20Zelin%20Shi%20and%20Chuang%20Yu%20and%20Yunpeng%20Liu%20and%20Yimian%20Dai&entry.1292438233=Recently%2C%20single-frame%20infrared%20small%20target%20%28SIRST%29%20detection%20technology%20has%20attracted%20widespread%20attention.%20Different%20from%20most%20existing%20deep%20learning-based%20methods%20that%20focus%20on%20improving%20network%20architectures%2C%20we%20propose%20a%20feature-enhanced%20and%20sensitivity-tunable%20%28FEST%29%20framework%2C%20which%20is%20compatible%20with%20existing%20SIRST%20detection%20networks%20and%20further%20enhances%20their%20detection%20performance.%20The%20FEST%20framework%20improves%20the%20model%27s%20robustness%20from%20two%20aspects%3A%20feature%20enhancement%20and%20target%20confidence%20regulation.%20For%20feature%20enhancement%2C%20we%20employ%20a%20multi-scale%20fusion%20strategy%20to%20improve%20the%20model%27s%20perception%20to%20multi-scale%20features%20of%20multi-size%20targets%2C%20and%20design%20an%20edge%20enhancement%20difficulty%20mining%20%28EEDM%29%20loss%20to%20guide%20the%20network%20to%20continuously%20focus%20on%20challenging%20target%20regions%20and%20edge%20features%20during%20training.%20For%20target%20confidence%20regulation%2C%20an%20adjustable%20sensitivity%20%28AS%29%20strategy%20is%20proposed%20for%20network%20post-processing.%20This%20strategy%20enhances%20the%20model%27s%20adaptability%20in%20complex%20scenarios%20and%20significantly%20improves%20the%20detection%20rate%20of%20infrared%20small%20targets%20while%20maintaining%20segmentation%20accuracy.%20Extensive%20experimental%20results%20show%20that%20our%20FEST%20framework%20can%20effectively%20enhance%20the%20performance%20of%20existing%20SIRST%20detection%20networks.%20The%20code%20is%20available%20at%20https%3A//github.com/YuChuang1205/FEST-Framework&entry.1838667208=http%3A//arxiv.org/abs/2407.20090v3&entry.124074799=Read"},
{"title": "Supervised learning pays attention", "author": "Erin Craig and Robert Tibshirani", "abstract": "In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability.\n  Our method fits a local model for each test observation by weighting the training data according to attention, a supervised similarity measure that emphasizes features and interactions that are predictive of the outcome. Attention weighting allows the method to adapt to heterogeneous data in a data-driven way, without requiring cluster or similarity pre-specification. Further, our approach is uniquely interpretable: for each test observation, we identify which features are most predictive and which training observations are most relevant. We then show how to use attention weighting for time series and spatial data, and we present a method for adapting pretrained tree-based models to distributional shift using attention-weighted residual corrections. Across real and simulated datasets, attention weighting improves predictive performance while preserving interpretability, and theory shows that attention-weighting linear models attain lower mean squared error than the standard linear model under mixture-of-models data-generating processes with known subgroup structure.", "link": "http://arxiv.org/abs/2512.09912v1", "date": "2025-12-10", "relevancy": 2.5043, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5197}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4917}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervised%20learning%20pays%20attention&body=Title%3A%20Supervised%20learning%20pays%20attention%0AAuthor%3A%20Erin%20Craig%20and%20Robert%20Tibshirani%0AAbstract%3A%20In-context%20learning%20with%20attention%20enables%20large%20neural%20networks%20to%20make%20context-specific%20predictions%20by%20selectively%20focusing%20on%20relevant%20examples.%20Here%2C%20we%20adapt%20this%20idea%20to%20supervised%20learning%20procedures%20such%20as%20lasso%20regression%20and%20gradient%20boosting%2C%20for%20tabular%20data.%20Our%20goals%20are%20to%20%281%29%20flexibly%20fit%20personalized%20models%20for%20each%20prediction%20point%20and%20%282%29%20retain%20model%20simplicity%20and%20interpretability.%0A%20%20Our%20method%20fits%20a%20local%20model%20for%20each%20test%20observation%20by%20weighting%20the%20training%20data%20according%20to%20attention%2C%20a%20supervised%20similarity%20measure%20that%20emphasizes%20features%20and%20interactions%20that%20are%20predictive%20of%20the%20outcome.%20Attention%20weighting%20allows%20the%20method%20to%20adapt%20to%20heterogeneous%20data%20in%20a%20data-driven%20way%2C%20without%20requiring%20cluster%20or%20similarity%20pre-specification.%20Further%2C%20our%20approach%20is%20uniquely%20interpretable%3A%20for%20each%20test%20observation%2C%20we%20identify%20which%20features%20are%20most%20predictive%20and%20which%20training%20observations%20are%20most%20relevant.%20We%20then%20show%20how%20to%20use%20attention%20weighting%20for%20time%20series%20and%20spatial%20data%2C%20and%20we%20present%20a%20method%20for%20adapting%20pretrained%20tree-based%20models%20to%20distributional%20shift%20using%20attention-weighted%20residual%20corrections.%20Across%20real%20and%20simulated%20datasets%2C%20attention%20weighting%20improves%20predictive%20performance%20while%20preserving%20interpretability%2C%20and%20theory%20shows%20that%20attention-weighting%20linear%20models%20attain%20lower%20mean%20squared%20error%20than%20the%20standard%20linear%20model%20under%20mixture-of-models%20data-generating%20processes%20with%20known%20subgroup%20structure.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervised%2520learning%2520pays%2520attention%26entry.906535625%3DErin%2520Craig%2520and%2520Robert%2520Tibshirani%26entry.1292438233%3DIn-context%2520learning%2520with%2520attention%2520enables%2520large%2520neural%2520networks%2520to%2520make%2520context-specific%2520predictions%2520by%2520selectively%2520focusing%2520on%2520relevant%2520examples.%2520Here%252C%2520we%2520adapt%2520this%2520idea%2520to%2520supervised%2520learning%2520procedures%2520such%2520as%2520lasso%2520regression%2520and%2520gradient%2520boosting%252C%2520for%2520tabular%2520data.%2520Our%2520goals%2520are%2520to%2520%25281%2529%2520flexibly%2520fit%2520personalized%2520models%2520for%2520each%2520prediction%2520point%2520and%2520%25282%2529%2520retain%2520model%2520simplicity%2520and%2520interpretability.%250A%2520%2520Our%2520method%2520fits%2520a%2520local%2520model%2520for%2520each%2520test%2520observation%2520by%2520weighting%2520the%2520training%2520data%2520according%2520to%2520attention%252C%2520a%2520supervised%2520similarity%2520measure%2520that%2520emphasizes%2520features%2520and%2520interactions%2520that%2520are%2520predictive%2520of%2520the%2520outcome.%2520Attention%2520weighting%2520allows%2520the%2520method%2520to%2520adapt%2520to%2520heterogeneous%2520data%2520in%2520a%2520data-driven%2520way%252C%2520without%2520requiring%2520cluster%2520or%2520similarity%2520pre-specification.%2520Further%252C%2520our%2520approach%2520is%2520uniquely%2520interpretable%253A%2520for%2520each%2520test%2520observation%252C%2520we%2520identify%2520which%2520features%2520are%2520most%2520predictive%2520and%2520which%2520training%2520observations%2520are%2520most%2520relevant.%2520We%2520then%2520show%2520how%2520to%2520use%2520attention%2520weighting%2520for%2520time%2520series%2520and%2520spatial%2520data%252C%2520and%2520we%2520present%2520a%2520method%2520for%2520adapting%2520pretrained%2520tree-based%2520models%2520to%2520distributional%2520shift%2520using%2520attention-weighted%2520residual%2520corrections.%2520Across%2520real%2520and%2520simulated%2520datasets%252C%2520attention%2520weighting%2520improves%2520predictive%2520performance%2520while%2520preserving%2520interpretability%252C%2520and%2520theory%2520shows%2520that%2520attention-weighting%2520linear%2520models%2520attain%2520lower%2520mean%2520squared%2520error%2520than%2520the%2520standard%2520linear%2520model%2520under%2520mixture-of-models%2520data-generating%2520processes%2520with%2520known%2520subgroup%2520structure.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20learning%20pays%20attention&entry.906535625=Erin%20Craig%20and%20Robert%20Tibshirani&entry.1292438233=In-context%20learning%20with%20attention%20enables%20large%20neural%20networks%20to%20make%20context-specific%20predictions%20by%20selectively%20focusing%20on%20relevant%20examples.%20Here%2C%20we%20adapt%20this%20idea%20to%20supervised%20learning%20procedures%20such%20as%20lasso%20regression%20and%20gradient%20boosting%2C%20for%20tabular%20data.%20Our%20goals%20are%20to%20%281%29%20flexibly%20fit%20personalized%20models%20for%20each%20prediction%20point%20and%20%282%29%20retain%20model%20simplicity%20and%20interpretability.%0A%20%20Our%20method%20fits%20a%20local%20model%20for%20each%20test%20observation%20by%20weighting%20the%20training%20data%20according%20to%20attention%2C%20a%20supervised%20similarity%20measure%20that%20emphasizes%20features%20and%20interactions%20that%20are%20predictive%20of%20the%20outcome.%20Attention%20weighting%20allows%20the%20method%20to%20adapt%20to%20heterogeneous%20data%20in%20a%20data-driven%20way%2C%20without%20requiring%20cluster%20or%20similarity%20pre-specification.%20Further%2C%20our%20approach%20is%20uniquely%20interpretable%3A%20for%20each%20test%20observation%2C%20we%20identify%20which%20features%20are%20most%20predictive%20and%20which%20training%20observations%20are%20most%20relevant.%20We%20then%20show%20how%20to%20use%20attention%20weighting%20for%20time%20series%20and%20spatial%20data%2C%20and%20we%20present%20a%20method%20for%20adapting%20pretrained%20tree-based%20models%20to%20distributional%20shift%20using%20attention-weighted%20residual%20corrections.%20Across%20real%20and%20simulated%20datasets%2C%20attention%20weighting%20improves%20predictive%20performance%20while%20preserving%20interpretability%2C%20and%20theory%20shows%20that%20attention-weighting%20linear%20models%20attain%20lower%20mean%20squared%20error%20than%20the%20standard%20linear%20model%20under%20mixture-of-models%20data-generating%20processes%20with%20known%20subgroup%20structure.&entry.1838667208=http%3A//arxiv.org/abs/2512.09912v1&entry.124074799=Read"},
{"title": "Exploring Protein Language Model Architecture-Induced Biases for Antibody Comprehension", "author": " Mengren and  Liu and Yixiang Zhang and  Yiming and  Zhang", "abstract": "Recent advances in protein language models (PLMs) have demonstrated remarkable capabilities in understanding protein sequences. However, the extent to which different model architectures capture antibody-specific biological properties remains unexplored. In this work, we systematically investigate how architectural choices in PLMs influence their ability to comprehend antibody sequence characteristics and functions. We evaluate three state-of-the-art PLMs-AntiBERTa, BioBERT, and ESM2--against a general-purpose language model (GPT-2) baseline on antibody target specificity prediction tasks. Our results demonstrate that while all PLMs achieve high classification accuracy, they exhibit distinct biases in capturing biological features such as V gene usage, somatic hypermutation patterns, and isotype information. Through attention attribution analysis, we show that antibody-specific models like AntiBERTa naturally learn to focus on complementarity-determining regions (CDRs), while general protein models benefit significantly from explicit CDR-focused training strategies. These findings provide insights into the relationship between model architecture and biological feature extraction, offering valuable guidance for future PLM development in computational antibody design.", "link": "http://arxiv.org/abs/2512.09894v1", "date": "2025-12-10", "relevancy": 2.5029, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5136}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5136}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Protein%20Language%20Model%20Architecture-Induced%20Biases%20for%20Antibody%20Comprehension&body=Title%3A%20Exploring%20Protein%20Language%20Model%20Architecture-Induced%20Biases%20for%20Antibody%20Comprehension%0AAuthor%3A%20%20Mengren%20and%20%20Liu%20and%20Yixiang%20Zhang%20and%20%20Yiming%20and%20%20Zhang%0AAbstract%3A%20Recent%20advances%20in%20protein%20language%20models%20%28PLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20understanding%20protein%20sequences.%20However%2C%20the%20extent%20to%20which%20different%20model%20architectures%20capture%20antibody-specific%20biological%20properties%20remains%20unexplored.%20In%20this%20work%2C%20we%20systematically%20investigate%20how%20architectural%20choices%20in%20PLMs%20influence%20their%20ability%20to%20comprehend%20antibody%20sequence%20characteristics%20and%20functions.%20We%20evaluate%20three%20state-of-the-art%20PLMs-AntiBERTa%2C%20BioBERT%2C%20and%20ESM2--against%20a%20general-purpose%20language%20model%20%28GPT-2%29%20baseline%20on%20antibody%20target%20specificity%20prediction%20tasks.%20Our%20results%20demonstrate%20that%20while%20all%20PLMs%20achieve%20high%20classification%20accuracy%2C%20they%20exhibit%20distinct%20biases%20in%20capturing%20biological%20features%20such%20as%20V%20gene%20usage%2C%20somatic%20hypermutation%20patterns%2C%20and%20isotype%20information.%20Through%20attention%20attribution%20analysis%2C%20we%20show%20that%20antibody-specific%20models%20like%20AntiBERTa%20naturally%20learn%20to%20focus%20on%20complementarity-determining%20regions%20%28CDRs%29%2C%20while%20general%20protein%20models%20benefit%20significantly%20from%20explicit%20CDR-focused%20training%20strategies.%20These%20findings%20provide%20insights%20into%20the%20relationship%20between%20model%20architecture%20and%20biological%20feature%20extraction%2C%20offering%20valuable%20guidance%20for%20future%20PLM%20development%20in%20computational%20antibody%20design.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Protein%2520Language%2520Model%2520Architecture-Induced%2520Biases%2520for%2520Antibody%2520Comprehension%26entry.906535625%3D%2520Mengren%2520and%2520%2520Liu%2520and%2520Yixiang%2520Zhang%2520and%2520%2520Yiming%2520and%2520%2520Zhang%26entry.1292438233%3DRecent%2520advances%2520in%2520protein%2520language%2520models%2520%2528PLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%2520understanding%2520protein%2520sequences.%2520However%252C%2520the%2520extent%2520to%2520which%2520different%2520model%2520architectures%2520capture%2520antibody-specific%2520biological%2520properties%2520remains%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520systematically%2520investigate%2520how%2520architectural%2520choices%2520in%2520PLMs%2520influence%2520their%2520ability%2520to%2520comprehend%2520antibody%2520sequence%2520characteristics%2520and%2520functions.%2520We%2520evaluate%2520three%2520state-of-the-art%2520PLMs-AntiBERTa%252C%2520BioBERT%252C%2520and%2520ESM2--against%2520a%2520general-purpose%2520language%2520model%2520%2528GPT-2%2529%2520baseline%2520on%2520antibody%2520target%2520specificity%2520prediction%2520tasks.%2520Our%2520results%2520demonstrate%2520that%2520while%2520all%2520PLMs%2520achieve%2520high%2520classification%2520accuracy%252C%2520they%2520exhibit%2520distinct%2520biases%2520in%2520capturing%2520biological%2520features%2520such%2520as%2520V%2520gene%2520usage%252C%2520somatic%2520hypermutation%2520patterns%252C%2520and%2520isotype%2520information.%2520Through%2520attention%2520attribution%2520analysis%252C%2520we%2520show%2520that%2520antibody-specific%2520models%2520like%2520AntiBERTa%2520naturally%2520learn%2520to%2520focus%2520on%2520complementarity-determining%2520regions%2520%2528CDRs%2529%252C%2520while%2520general%2520protein%2520models%2520benefit%2520significantly%2520from%2520explicit%2520CDR-focused%2520training%2520strategies.%2520These%2520findings%2520provide%2520insights%2520into%2520the%2520relationship%2520between%2520model%2520architecture%2520and%2520biological%2520feature%2520extraction%252C%2520offering%2520valuable%2520guidance%2520for%2520future%2520PLM%2520development%2520in%2520computational%2520antibody%2520design.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Protein%20Language%20Model%20Architecture-Induced%20Biases%20for%20Antibody%20Comprehension&entry.906535625=%20Mengren%20and%20%20Liu%20and%20Yixiang%20Zhang%20and%20%20Yiming%20and%20%20Zhang&entry.1292438233=Recent%20advances%20in%20protein%20language%20models%20%28PLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20understanding%20protein%20sequences.%20However%2C%20the%20extent%20to%20which%20different%20model%20architectures%20capture%20antibody-specific%20biological%20properties%20remains%20unexplored.%20In%20this%20work%2C%20we%20systematically%20investigate%20how%20architectural%20choices%20in%20PLMs%20influence%20their%20ability%20to%20comprehend%20antibody%20sequence%20characteristics%20and%20functions.%20We%20evaluate%20three%20state-of-the-art%20PLMs-AntiBERTa%2C%20BioBERT%2C%20and%20ESM2--against%20a%20general-purpose%20language%20model%20%28GPT-2%29%20baseline%20on%20antibody%20target%20specificity%20prediction%20tasks.%20Our%20results%20demonstrate%20that%20while%20all%20PLMs%20achieve%20high%20classification%20accuracy%2C%20they%20exhibit%20distinct%20biases%20in%20capturing%20biological%20features%20such%20as%20V%20gene%20usage%2C%20somatic%20hypermutation%20patterns%2C%20and%20isotype%20information.%20Through%20attention%20attribution%20analysis%2C%20we%20show%20that%20antibody-specific%20models%20like%20AntiBERTa%20naturally%20learn%20to%20focus%20on%20complementarity-determining%20regions%20%28CDRs%29%2C%20while%20general%20protein%20models%20benefit%20significantly%20from%20explicit%20CDR-focused%20training%20strategies.%20These%20findings%20provide%20insights%20into%20the%20relationship%20between%20model%20architecture%20and%20biological%20feature%20extraction%2C%20offering%20valuable%20guidance%20for%20future%20PLM%20development%20in%20computational%20antibody%20design.&entry.1838667208=http%3A//arxiv.org/abs/2512.09894v1&entry.124074799=Read"},
{"title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time", "author": "Chuhan Zhang and Guillaume Le Moing and Skanda Koppula and Ignacio Rocco and Liliane Momeni and Junyu Xie and Shuyang Sun and Rahul Sukthankar and Jo\u00eblle K. Barral and Raia Hadsell and Zoubin Ghahramani and Andrew Zisserman and Junlin Zhang and Mehdi S. M. Sajjadi", "abstract": "Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.", "link": "http://arxiv.org/abs/2512.08924v2", "date": "2025-12-10", "relevancy": 2.4996, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6405}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6218}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficiently%20Reconstructing%20Dynamic%20Scenes%20One%20D4RT%20at%20a%20Time&body=Title%3A%20Efficiently%20Reconstructing%20Dynamic%20Scenes%20One%20D4RT%20at%20a%20Time%0AAuthor%3A%20Chuhan%20Zhang%20and%20Guillaume%20Le%20Moing%20and%20Skanda%20Koppula%20and%20Ignacio%20Rocco%20and%20Liliane%20Momeni%20and%20Junyu%20Xie%20and%20Shuyang%20Sun%20and%20Rahul%20Sukthankar%20and%20Jo%C3%ABlle%20K.%20Barral%20and%20Raia%20Hadsell%20and%20Zoubin%20Ghahramani%20and%20Andrew%20Zisserman%20and%20Junlin%20Zhang%20and%20Mehdi%20S.%20M.%20Sajjadi%0AAbstract%3A%20Understanding%20and%20reconstructing%20the%20complex%20geometry%20and%20motion%20of%20dynamic%20scenes%20from%20video%20remains%20a%20formidable%20challenge%20in%20computer%20vision.%20This%20paper%20introduces%20D4RT%2C%20a%20simple%20yet%20powerful%20feedforward%20model%20designed%20to%20efficiently%20solve%20this%20task.%20D4RT%20utilizes%20a%20unified%20transformer%20architecture%20to%20jointly%20infer%20depth%2C%20spatio-temporal%20correspondence%2C%20and%20full%20camera%20parameters%20from%20a%20single%20video.%20Its%20core%20innovation%20is%20a%20novel%20querying%20mechanism%20that%20sidesteps%20the%20heavy%20computation%20of%20dense%2C%20per-frame%20decoding%20and%20the%20complexity%20of%20managing%20multiple%2C%20task-specific%20decoders.%20Our%20decoding%20interface%20allows%20the%20model%20to%20independently%20and%20flexibly%20probe%20the%203D%20position%20of%20any%20point%20in%20space%20and%20time.%20The%20result%20is%20a%20lightweight%20and%20highly%20scalable%20method%20that%20enables%20remarkably%20efficient%20training%20and%20inference.%20We%20demonstrate%20that%20our%20approach%20sets%20a%20new%20state%20of%20the%20art%2C%20outperforming%20previous%20methods%20across%20a%20wide%20spectrum%20of%204D%20reconstruction%20tasks.%20We%20refer%20to%20the%20project%20webpage%20for%20animated%20results%3A%20https%3A//d4rt-paper.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08924v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficiently%2520Reconstructing%2520Dynamic%2520Scenes%2520One%2520D4RT%2520at%2520a%2520Time%26entry.906535625%3DChuhan%2520Zhang%2520and%2520Guillaume%2520Le%2520Moing%2520and%2520Skanda%2520Koppula%2520and%2520Ignacio%2520Rocco%2520and%2520Liliane%2520Momeni%2520and%2520Junyu%2520Xie%2520and%2520Shuyang%2520Sun%2520and%2520Rahul%2520Sukthankar%2520and%2520Jo%25C3%25ABlle%2520K.%2520Barral%2520and%2520Raia%2520Hadsell%2520and%2520Zoubin%2520Ghahramani%2520and%2520Andrew%2520Zisserman%2520and%2520Junlin%2520Zhang%2520and%2520Mehdi%2520S.%2520M.%2520Sajjadi%26entry.1292438233%3DUnderstanding%2520and%2520reconstructing%2520the%2520complex%2520geometry%2520and%2520motion%2520of%2520dynamic%2520scenes%2520from%2520video%2520remains%2520a%2520formidable%2520challenge%2520in%2520computer%2520vision.%2520This%2520paper%2520introduces%2520D4RT%252C%2520a%2520simple%2520yet%2520powerful%2520feedforward%2520model%2520designed%2520to%2520efficiently%2520solve%2520this%2520task.%2520D4RT%2520utilizes%2520a%2520unified%2520transformer%2520architecture%2520to%2520jointly%2520infer%2520depth%252C%2520spatio-temporal%2520correspondence%252C%2520and%2520full%2520camera%2520parameters%2520from%2520a%2520single%2520video.%2520Its%2520core%2520innovation%2520is%2520a%2520novel%2520querying%2520mechanism%2520that%2520sidesteps%2520the%2520heavy%2520computation%2520of%2520dense%252C%2520per-frame%2520decoding%2520and%2520the%2520complexity%2520of%2520managing%2520multiple%252C%2520task-specific%2520decoders.%2520Our%2520decoding%2520interface%2520allows%2520the%2520model%2520to%2520independently%2520and%2520flexibly%2520probe%2520the%25203D%2520position%2520of%2520any%2520point%2520in%2520space%2520and%2520time.%2520The%2520result%2520is%2520a%2520lightweight%2520and%2520highly%2520scalable%2520method%2520that%2520enables%2520remarkably%2520efficient%2520training%2520and%2520inference.%2520We%2520demonstrate%2520that%2520our%2520approach%2520sets%2520a%2520new%2520state%2520of%2520the%2520art%252C%2520outperforming%2520previous%2520methods%2520across%2520a%2520wide%2520spectrum%2520of%25204D%2520reconstruction%2520tasks.%2520We%2520refer%2520to%2520the%2520project%2520webpage%2520for%2520animated%2520results%253A%2520https%253A//d4rt-paper.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08924v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficiently%20Reconstructing%20Dynamic%20Scenes%20One%20D4RT%20at%20a%20Time&entry.906535625=Chuhan%20Zhang%20and%20Guillaume%20Le%20Moing%20and%20Skanda%20Koppula%20and%20Ignacio%20Rocco%20and%20Liliane%20Momeni%20and%20Junyu%20Xie%20and%20Shuyang%20Sun%20and%20Rahul%20Sukthankar%20and%20Jo%C3%ABlle%20K.%20Barral%20and%20Raia%20Hadsell%20and%20Zoubin%20Ghahramani%20and%20Andrew%20Zisserman%20and%20Junlin%20Zhang%20and%20Mehdi%20S.%20M.%20Sajjadi&entry.1292438233=Understanding%20and%20reconstructing%20the%20complex%20geometry%20and%20motion%20of%20dynamic%20scenes%20from%20video%20remains%20a%20formidable%20challenge%20in%20computer%20vision.%20This%20paper%20introduces%20D4RT%2C%20a%20simple%20yet%20powerful%20feedforward%20model%20designed%20to%20efficiently%20solve%20this%20task.%20D4RT%20utilizes%20a%20unified%20transformer%20architecture%20to%20jointly%20infer%20depth%2C%20spatio-temporal%20correspondence%2C%20and%20full%20camera%20parameters%20from%20a%20single%20video.%20Its%20core%20innovation%20is%20a%20novel%20querying%20mechanism%20that%20sidesteps%20the%20heavy%20computation%20of%20dense%2C%20per-frame%20decoding%20and%20the%20complexity%20of%20managing%20multiple%2C%20task-specific%20decoders.%20Our%20decoding%20interface%20allows%20the%20model%20to%20independently%20and%20flexibly%20probe%20the%203D%20position%20of%20any%20point%20in%20space%20and%20time.%20The%20result%20is%20a%20lightweight%20and%20highly%20scalable%20method%20that%20enables%20remarkably%20efficient%20training%20and%20inference.%20We%20demonstrate%20that%20our%20approach%20sets%20a%20new%20state%20of%20the%20art%2C%20outperforming%20previous%20methods%20across%20a%20wide%20spectrum%20of%204D%20reconstruction%20tasks.%20We%20refer%20to%20the%20project%20webpage%20for%20animated%20results%3A%20https%3A//d4rt-paper.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2512.08924v2&entry.124074799=Read"},
{"title": "OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring", "author": "Ruoxin Xiong and Yanyu Wang and Jiannan Cai and Kaijian Liu and Yuansheng Zhu and Pingbo Tang and Nora El-Gohary", "abstract": "The construction industry increasingly relies on visual data to support Artificial Intelligence (AI) and Machine Learning (ML) applications for site monitoring. High-quality, domain-specific datasets, comprising images, videos, and point clouds, capture site geometry and spatiotemporal dynamics, including the location and interaction of objects, workers, and materials. However, despite growing interest in leveraging visual datasets, existing resources vary widely in sizes, data modalities, annotation quality, and representativeness of real-world construction conditions. A systematic review to categorize their data characteristics and application contexts is still lacking, limiting the community's ability to fully understand the dataset landscape, identify critical gaps, and guide future directions toward more effective, reliable, and scalable AI applications in construction. To address this gap, this study conducts an extensive search of academic databases and open-data platforms, yielding 51 publicly available visual datasets that span the 2005-2024 period. These datasets are categorized using a structured data schema covering (i) data fundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and point cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv) downstream application domains (e.g., progress tracking). This study synthesizes these findings into an open-source catalog, OpenConstruction, supporting data-driven method development. Furthermore, the study discusses several critical limitations in the existing construction dataset landscape and presents a roadmap for future data infrastructure anchored in the Findability, Accessibility, Interoperability, and Reusability (FAIR) principles. By reviewing the current landscape and outlining strategic priorities, this study supports the advancement of data-centric solutions in the construction sector.", "link": "http://arxiv.org/abs/2508.11482v2", "date": "2025-12-10", "relevancy": 2.4952, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5019}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenConstruction%3A%20A%20Systematic%20Synthesis%20of%20Open%20Visual%20Datasets%20for%20Data-Centric%20Artificial%20Intelligence%20in%20Construction%20Monitoring&body=Title%3A%20OpenConstruction%3A%20A%20Systematic%20Synthesis%20of%20Open%20Visual%20Datasets%20for%20Data-Centric%20Artificial%20Intelligence%20in%20Construction%20Monitoring%0AAuthor%3A%20Ruoxin%20Xiong%20and%20Yanyu%20Wang%20and%20Jiannan%20Cai%20and%20Kaijian%20Liu%20and%20Yuansheng%20Zhu%20and%20Pingbo%20Tang%20and%20Nora%20El-Gohary%0AAbstract%3A%20The%20construction%20industry%20increasingly%20relies%20on%20visual%20data%20to%20support%20Artificial%20Intelligence%20%28AI%29%20and%20Machine%20Learning%20%28ML%29%20applications%20for%20site%20monitoring.%20High-quality%2C%20domain-specific%20datasets%2C%20comprising%20images%2C%20videos%2C%20and%20point%20clouds%2C%20capture%20site%20geometry%20and%20spatiotemporal%20dynamics%2C%20including%20the%20location%20and%20interaction%20of%20objects%2C%20workers%2C%20and%20materials.%20However%2C%20despite%20growing%20interest%20in%20leveraging%20visual%20datasets%2C%20existing%20resources%20vary%20widely%20in%20sizes%2C%20data%20modalities%2C%20annotation%20quality%2C%20and%20representativeness%20of%20real-world%20construction%20conditions.%20A%20systematic%20review%20to%20categorize%20their%20data%20characteristics%20and%20application%20contexts%20is%20still%20lacking%2C%20limiting%20the%20community%27s%20ability%20to%20fully%20understand%20the%20dataset%20landscape%2C%20identify%20critical%20gaps%2C%20and%20guide%20future%20directions%20toward%20more%20effective%2C%20reliable%2C%20and%20scalable%20AI%20applications%20in%20construction.%20To%20address%20this%20gap%2C%20this%20study%20conducts%20an%20extensive%20search%20of%20academic%20databases%20and%20open-data%20platforms%2C%20yielding%2051%20publicly%20available%20visual%20datasets%20that%20span%20the%202005-2024%20period.%20These%20datasets%20are%20categorized%20using%20a%20structured%20data%20schema%20covering%20%28i%29%20data%20fundamentals%20%28e.g.%2C%20size%20and%20license%29%2C%20%28ii%29%20data%20modalities%20%28e.g.%2C%20RGB%20and%20point%20cloud%29%2C%20%28iii%29%20annotation%20frameworks%20%28e.g.%2C%20bounding%20boxes%29%2C%20and%20%28iv%29%20downstream%20application%20domains%20%28e.g.%2C%20progress%20tracking%29.%20This%20study%20synthesizes%20these%20findings%20into%20an%20open-source%20catalog%2C%20OpenConstruction%2C%20supporting%20data-driven%20method%20development.%20Furthermore%2C%20the%20study%20discusses%20several%20critical%20limitations%20in%20the%20existing%20construction%20dataset%20landscape%20and%20presents%20a%20roadmap%20for%20future%20data%20infrastructure%20anchored%20in%20the%20Findability%2C%20Accessibility%2C%20Interoperability%2C%20and%20Reusability%20%28FAIR%29%20principles.%20By%20reviewing%20the%20current%20landscape%20and%20outlining%20strategic%20priorities%2C%20this%20study%20supports%20the%20advancement%20of%20data-centric%20solutions%20in%20the%20construction%20sector.%0ALink%3A%20http%3A//arxiv.org/abs/2508.11482v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenConstruction%253A%2520A%2520Systematic%2520Synthesis%2520of%2520Open%2520Visual%2520Datasets%2520for%2520Data-Centric%2520Artificial%2520Intelligence%2520in%2520Construction%2520Monitoring%26entry.906535625%3DRuoxin%2520Xiong%2520and%2520Yanyu%2520Wang%2520and%2520Jiannan%2520Cai%2520and%2520Kaijian%2520Liu%2520and%2520Yuansheng%2520Zhu%2520and%2520Pingbo%2520Tang%2520and%2520Nora%2520El-Gohary%26entry.1292438233%3DThe%2520construction%2520industry%2520increasingly%2520relies%2520on%2520visual%2520data%2520to%2520support%2520Artificial%2520Intelligence%2520%2528AI%2529%2520and%2520Machine%2520Learning%2520%2528ML%2529%2520applications%2520for%2520site%2520monitoring.%2520High-quality%252C%2520domain-specific%2520datasets%252C%2520comprising%2520images%252C%2520videos%252C%2520and%2520point%2520clouds%252C%2520capture%2520site%2520geometry%2520and%2520spatiotemporal%2520dynamics%252C%2520including%2520the%2520location%2520and%2520interaction%2520of%2520objects%252C%2520workers%252C%2520and%2520materials.%2520However%252C%2520despite%2520growing%2520interest%2520in%2520leveraging%2520visual%2520datasets%252C%2520existing%2520resources%2520vary%2520widely%2520in%2520sizes%252C%2520data%2520modalities%252C%2520annotation%2520quality%252C%2520and%2520representativeness%2520of%2520real-world%2520construction%2520conditions.%2520A%2520systematic%2520review%2520to%2520categorize%2520their%2520data%2520characteristics%2520and%2520application%2520contexts%2520is%2520still%2520lacking%252C%2520limiting%2520the%2520community%2527s%2520ability%2520to%2520fully%2520understand%2520the%2520dataset%2520landscape%252C%2520identify%2520critical%2520gaps%252C%2520and%2520guide%2520future%2520directions%2520toward%2520more%2520effective%252C%2520reliable%252C%2520and%2520scalable%2520AI%2520applications%2520in%2520construction.%2520To%2520address%2520this%2520gap%252C%2520this%2520study%2520conducts%2520an%2520extensive%2520search%2520of%2520academic%2520databases%2520and%2520open-data%2520platforms%252C%2520yielding%252051%2520publicly%2520available%2520visual%2520datasets%2520that%2520span%2520the%25202005-2024%2520period.%2520These%2520datasets%2520are%2520categorized%2520using%2520a%2520structured%2520data%2520schema%2520covering%2520%2528i%2529%2520data%2520fundamentals%2520%2528e.g.%252C%2520size%2520and%2520license%2529%252C%2520%2528ii%2529%2520data%2520modalities%2520%2528e.g.%252C%2520RGB%2520and%2520point%2520cloud%2529%252C%2520%2528iii%2529%2520annotation%2520frameworks%2520%2528e.g.%252C%2520bounding%2520boxes%2529%252C%2520and%2520%2528iv%2529%2520downstream%2520application%2520domains%2520%2528e.g.%252C%2520progress%2520tracking%2529.%2520This%2520study%2520synthesizes%2520these%2520findings%2520into%2520an%2520open-source%2520catalog%252C%2520OpenConstruction%252C%2520supporting%2520data-driven%2520method%2520development.%2520Furthermore%252C%2520the%2520study%2520discusses%2520several%2520critical%2520limitations%2520in%2520the%2520existing%2520construction%2520dataset%2520landscape%2520and%2520presents%2520a%2520roadmap%2520for%2520future%2520data%2520infrastructure%2520anchored%2520in%2520the%2520Findability%252C%2520Accessibility%252C%2520Interoperability%252C%2520and%2520Reusability%2520%2528FAIR%2529%2520principles.%2520By%2520reviewing%2520the%2520current%2520landscape%2520and%2520outlining%2520strategic%2520priorities%252C%2520this%2520study%2520supports%2520the%2520advancement%2520of%2520data-centric%2520solutions%2520in%2520the%2520construction%2520sector.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11482v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenConstruction%3A%20A%20Systematic%20Synthesis%20of%20Open%20Visual%20Datasets%20for%20Data-Centric%20Artificial%20Intelligence%20in%20Construction%20Monitoring&entry.906535625=Ruoxin%20Xiong%20and%20Yanyu%20Wang%20and%20Jiannan%20Cai%20and%20Kaijian%20Liu%20and%20Yuansheng%20Zhu%20and%20Pingbo%20Tang%20and%20Nora%20El-Gohary&entry.1292438233=The%20construction%20industry%20increasingly%20relies%20on%20visual%20data%20to%20support%20Artificial%20Intelligence%20%28AI%29%20and%20Machine%20Learning%20%28ML%29%20applications%20for%20site%20monitoring.%20High-quality%2C%20domain-specific%20datasets%2C%20comprising%20images%2C%20videos%2C%20and%20point%20clouds%2C%20capture%20site%20geometry%20and%20spatiotemporal%20dynamics%2C%20including%20the%20location%20and%20interaction%20of%20objects%2C%20workers%2C%20and%20materials.%20However%2C%20despite%20growing%20interest%20in%20leveraging%20visual%20datasets%2C%20existing%20resources%20vary%20widely%20in%20sizes%2C%20data%20modalities%2C%20annotation%20quality%2C%20and%20representativeness%20of%20real-world%20construction%20conditions.%20A%20systematic%20review%20to%20categorize%20their%20data%20characteristics%20and%20application%20contexts%20is%20still%20lacking%2C%20limiting%20the%20community%27s%20ability%20to%20fully%20understand%20the%20dataset%20landscape%2C%20identify%20critical%20gaps%2C%20and%20guide%20future%20directions%20toward%20more%20effective%2C%20reliable%2C%20and%20scalable%20AI%20applications%20in%20construction.%20To%20address%20this%20gap%2C%20this%20study%20conducts%20an%20extensive%20search%20of%20academic%20databases%20and%20open-data%20platforms%2C%20yielding%2051%20publicly%20available%20visual%20datasets%20that%20span%20the%202005-2024%20period.%20These%20datasets%20are%20categorized%20using%20a%20structured%20data%20schema%20covering%20%28i%29%20data%20fundamentals%20%28e.g.%2C%20size%20and%20license%29%2C%20%28ii%29%20data%20modalities%20%28e.g.%2C%20RGB%20and%20point%20cloud%29%2C%20%28iii%29%20annotation%20frameworks%20%28e.g.%2C%20bounding%20boxes%29%2C%20and%20%28iv%29%20downstream%20application%20domains%20%28e.g.%2C%20progress%20tracking%29.%20This%20study%20synthesizes%20these%20findings%20into%20an%20open-source%20catalog%2C%20OpenConstruction%2C%20supporting%20data-driven%20method%20development.%20Furthermore%2C%20the%20study%20discusses%20several%20critical%20limitations%20in%20the%20existing%20construction%20dataset%20landscape%20and%20presents%20a%20roadmap%20for%20future%20data%20infrastructure%20anchored%20in%20the%20Findability%2C%20Accessibility%2C%20Interoperability%2C%20and%20Reusability%20%28FAIR%29%20principles.%20By%20reviewing%20the%20current%20landscape%20and%20outlining%20strategic%20priorities%2C%20this%20study%20supports%20the%20advancement%20of%20data-centric%20solutions%20in%20the%20construction%20sector.&entry.1838667208=http%3A//arxiv.org/abs/2508.11482v2&entry.124074799=Read"},
{"title": "Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models", "author": "Magnus Ruud Kjaer and Rahul Thapa and Gauri Ganjoo and Hyatt Moore and Poul Joergen Jennum and Brandon M. Westover and James Zou and Emmanuel Mignot and Bryan He and Andreas Brink-Kjaer", "abstract": "Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.", "link": "http://arxiv.org/abs/2512.09591v1", "date": "2025-12-10", "relevancy": 2.4812, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5172}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4857}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stanford%20Sleep%20Bench%3A%20Evaluating%20Polysomnography%20Pre-training%20Methods%20for%20Sleep%20Foundation%20Models&body=Title%3A%20Stanford%20Sleep%20Bench%3A%20Evaluating%20Polysomnography%20Pre-training%20Methods%20for%20Sleep%20Foundation%20Models%0AAuthor%3A%20Magnus%20Ruud%20Kjaer%20and%20Rahul%20Thapa%20and%20Gauri%20Ganjoo%20and%20Hyatt%20Moore%20and%20Poul%20Joergen%20Jennum%20and%20Brandon%20M.%20Westover%20and%20James%20Zou%20and%20Emmanuel%20Mignot%20and%20Bryan%20He%20and%20Andreas%20Brink-Kjaer%0AAbstract%3A%20Polysomnography%20%28PSG%29%2C%20the%20gold%20standard%20test%20for%20sleep%20analysis%2C%20generates%20vast%20amounts%20of%20multimodal%20clinical%20data%2C%20presenting%20an%20opportunity%20to%20leverage%20self-supervised%20representation%20learning%20%28SSRL%29%20for%20pre-training%20foundation%20models%20to%20enhance%20sleep%20analysis.%20However%2C%20progress%20in%20sleep%20foundation%20models%20is%20hindered%20by%20two%20key%20limitations%3A%20%281%29%20the%20lack%20of%20a%20shared%20dataset%20and%20benchmark%20with%20diverse%20tasks%20for%20training%20and%20evaluation%2C%20and%20%282%29%20the%20absence%20of%20a%20systematic%20evaluation%20of%20SSRL%20approaches%20across%20sleep-related%20tasks.%20To%20address%20these%20gaps%2C%20we%20introduce%20Stanford%20Sleep%20Bench%2C%20a%20large-scale%20PSG%20dataset%20comprising%2017%2C467%20recordings%20totaling%20over%20163%2C000%20hours%20from%20a%20major%20sleep%20clinic%2C%20including%2013%20clinical%20disease%20prediction%20tasks%20alongside%20canonical%20sleep-related%20tasks%20such%20as%20sleep%20staging%2C%20apnea%20diagnosis%2C%20and%20age%20estimation.%20We%20systematically%20evaluate%20SSRL%20pre-training%20methods%20on%20Stanford%20Sleep%20Bench%2C%20assessing%20downstream%20performance%20across%20four%20tasks%3A%20sleep%20staging%2C%20apnea%20diagnosis%2C%20age%20estimation%2C%20and%20disease%20and%20mortality%20prediction.%20Our%20results%20show%20that%20multiple%20pretraining%20methods%20achieve%20comparable%20performance%20for%20sleep%20staging%2C%20apnea%20diagnosis%2C%20and%20age%20estimation.%20However%2C%20for%20mortality%20and%20disease%20prediction%2C%20contrastive%20learning%20significantly%20outperforms%20other%20approaches%20while%20also%20converging%20faster%20during%20pretraining.%20To%20facilitate%20reproducibility%20and%20advance%20sleep%20research%2C%20we%20will%20release%20Stanford%20Sleep%20Bench%20along%20with%20pretrained%20model%20weights%2C%20training%20pipelines%2C%20and%20evaluation%20code.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStanford%2520Sleep%2520Bench%253A%2520Evaluating%2520Polysomnography%2520Pre-training%2520Methods%2520for%2520Sleep%2520Foundation%2520Models%26entry.906535625%3DMagnus%2520Ruud%2520Kjaer%2520and%2520Rahul%2520Thapa%2520and%2520Gauri%2520Ganjoo%2520and%2520Hyatt%2520Moore%2520and%2520Poul%2520Joergen%2520Jennum%2520and%2520Brandon%2520M.%2520Westover%2520and%2520James%2520Zou%2520and%2520Emmanuel%2520Mignot%2520and%2520Bryan%2520He%2520and%2520Andreas%2520Brink-Kjaer%26entry.1292438233%3DPolysomnography%2520%2528PSG%2529%252C%2520the%2520gold%2520standard%2520test%2520for%2520sleep%2520analysis%252C%2520generates%2520vast%2520amounts%2520of%2520multimodal%2520clinical%2520data%252C%2520presenting%2520an%2520opportunity%2520to%2520leverage%2520self-supervised%2520representation%2520learning%2520%2528SSRL%2529%2520for%2520pre-training%2520foundation%2520models%2520to%2520enhance%2520sleep%2520analysis.%2520However%252C%2520progress%2520in%2520sleep%2520foundation%2520models%2520is%2520hindered%2520by%2520two%2520key%2520limitations%253A%2520%25281%2529%2520the%2520lack%2520of%2520a%2520shared%2520dataset%2520and%2520benchmark%2520with%2520diverse%2520tasks%2520for%2520training%2520and%2520evaluation%252C%2520and%2520%25282%2529%2520the%2520absence%2520of%2520a%2520systematic%2520evaluation%2520of%2520SSRL%2520approaches%2520across%2520sleep-related%2520tasks.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%2520Stanford%2520Sleep%2520Bench%252C%2520a%2520large-scale%2520PSG%2520dataset%2520comprising%252017%252C467%2520recordings%2520totaling%2520over%2520163%252C000%2520hours%2520from%2520a%2520major%2520sleep%2520clinic%252C%2520including%252013%2520clinical%2520disease%2520prediction%2520tasks%2520alongside%2520canonical%2520sleep-related%2520tasks%2520such%2520as%2520sleep%2520staging%252C%2520apnea%2520diagnosis%252C%2520and%2520age%2520estimation.%2520We%2520systematically%2520evaluate%2520SSRL%2520pre-training%2520methods%2520on%2520Stanford%2520Sleep%2520Bench%252C%2520assessing%2520downstream%2520performance%2520across%2520four%2520tasks%253A%2520sleep%2520staging%252C%2520apnea%2520diagnosis%252C%2520age%2520estimation%252C%2520and%2520disease%2520and%2520mortality%2520prediction.%2520Our%2520results%2520show%2520that%2520multiple%2520pretraining%2520methods%2520achieve%2520comparable%2520performance%2520for%2520sleep%2520staging%252C%2520apnea%2520diagnosis%252C%2520and%2520age%2520estimation.%2520However%252C%2520for%2520mortality%2520and%2520disease%2520prediction%252C%2520contrastive%2520learning%2520significantly%2520outperforms%2520other%2520approaches%2520while%2520also%2520converging%2520faster%2520during%2520pretraining.%2520To%2520facilitate%2520reproducibility%2520and%2520advance%2520sleep%2520research%252C%2520we%2520will%2520release%2520Stanford%2520Sleep%2520Bench%2520along%2520with%2520pretrained%2520model%2520weights%252C%2520training%2520pipelines%252C%2520and%2520evaluation%2520code.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stanford%20Sleep%20Bench%3A%20Evaluating%20Polysomnography%20Pre-training%20Methods%20for%20Sleep%20Foundation%20Models&entry.906535625=Magnus%20Ruud%20Kjaer%20and%20Rahul%20Thapa%20and%20Gauri%20Ganjoo%20and%20Hyatt%20Moore%20and%20Poul%20Joergen%20Jennum%20and%20Brandon%20M.%20Westover%20and%20James%20Zou%20and%20Emmanuel%20Mignot%20and%20Bryan%20He%20and%20Andreas%20Brink-Kjaer&entry.1292438233=Polysomnography%20%28PSG%29%2C%20the%20gold%20standard%20test%20for%20sleep%20analysis%2C%20generates%20vast%20amounts%20of%20multimodal%20clinical%20data%2C%20presenting%20an%20opportunity%20to%20leverage%20self-supervised%20representation%20learning%20%28SSRL%29%20for%20pre-training%20foundation%20models%20to%20enhance%20sleep%20analysis.%20However%2C%20progress%20in%20sleep%20foundation%20models%20is%20hindered%20by%20two%20key%20limitations%3A%20%281%29%20the%20lack%20of%20a%20shared%20dataset%20and%20benchmark%20with%20diverse%20tasks%20for%20training%20and%20evaluation%2C%20and%20%282%29%20the%20absence%20of%20a%20systematic%20evaluation%20of%20SSRL%20approaches%20across%20sleep-related%20tasks.%20To%20address%20these%20gaps%2C%20we%20introduce%20Stanford%20Sleep%20Bench%2C%20a%20large-scale%20PSG%20dataset%20comprising%2017%2C467%20recordings%20totaling%20over%20163%2C000%20hours%20from%20a%20major%20sleep%20clinic%2C%20including%2013%20clinical%20disease%20prediction%20tasks%20alongside%20canonical%20sleep-related%20tasks%20such%20as%20sleep%20staging%2C%20apnea%20diagnosis%2C%20and%20age%20estimation.%20We%20systematically%20evaluate%20SSRL%20pre-training%20methods%20on%20Stanford%20Sleep%20Bench%2C%20assessing%20downstream%20performance%20across%20four%20tasks%3A%20sleep%20staging%2C%20apnea%20diagnosis%2C%20age%20estimation%2C%20and%20disease%20and%20mortality%20prediction.%20Our%20results%20show%20that%20multiple%20pretraining%20methods%20achieve%20comparable%20performance%20for%20sleep%20staging%2C%20apnea%20diagnosis%2C%20and%20age%20estimation.%20However%2C%20for%20mortality%20and%20disease%20prediction%2C%20contrastive%20learning%20significantly%20outperforms%20other%20approaches%20while%20also%20converging%20faster%20during%20pretraining.%20To%20facilitate%20reproducibility%20and%20advance%20sleep%20research%2C%20we%20will%20release%20Stanford%20Sleep%20Bench%20along%20with%20pretrained%20model%20weights%2C%20training%20pipelines%2C%20and%20evaluation%20code.&entry.1838667208=http%3A//arxiv.org/abs/2512.09591v1&entry.124074799=Read"},
{"title": "Boosting In-Silicon Directed Evolution with Fine-Tuned Protein Language Model and Tree Search", "author": "Yaodong Yang and Yang Wang and Jinpeng Li and Pei Guo and Da Han and Guangyong Chen and Pheng-Ann Heng", "abstract": "Protein evolution through amino acid mutations is a cornerstone of life sciences. Recent advances in protein language models have shown rich evolutionary patterns, offering unprecedented potential for in-silicon directed evolution. However, existing directed evolution methods largely rely on heuristic evolution strategies and have yet to efficiently integrate the transformative protein language models with advanced optimization techniques, such as reinforcement learning, to learn optimal evolution policies. To bridge this gap, we propose AlphaDE, a novel framework that evolves protein sequences by harnessing the innovative paradigms of large language models, such as fine-tuning and test-time inference. First, AlphaDE fine-tunes pretrained protein language models using masked language modeling on homologous protein sequences to activate the evolutionary plausibility of the interested protein family. Second, AlphaDE introduces test-time inference based on Monte Carlo tree search, which effectively evolves proteins with evolutionary guidance from the fine-tuned protein language model. Extensive benchmark experiments show that AlphaDE remarkably outperforms previous state-of-the-art methods even with few-shot fine-tuning. A case study further demonstrates that AlphaDE supports condensing the protein sequence space of avGFP through computational evolution.", "link": "http://arxiv.org/abs/2511.09900v3", "date": "2025-12-10", "relevancy": 2.4798, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4981}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20In-Silicon%20Directed%20Evolution%20with%20Fine-Tuned%20Protein%20Language%20Model%20and%20Tree%20Search&body=Title%3A%20Boosting%20In-Silicon%20Directed%20Evolution%20with%20Fine-Tuned%20Protein%20Language%20Model%20and%20Tree%20Search%0AAuthor%3A%20Yaodong%20Yang%20and%20Yang%20Wang%20and%20Jinpeng%20Li%20and%20Pei%20Guo%20and%20Da%20Han%20and%20Guangyong%20Chen%20and%20Pheng-Ann%20Heng%0AAbstract%3A%20Protein%20evolution%20through%20amino%20acid%20mutations%20is%20a%20cornerstone%20of%20life%20sciences.%20Recent%20advances%20in%20protein%20language%20models%20have%20shown%20rich%20evolutionary%20patterns%2C%20offering%20unprecedented%20potential%20for%20in-silicon%20directed%20evolution.%20However%2C%20existing%20directed%20evolution%20methods%20largely%20rely%20on%20heuristic%20evolution%20strategies%20and%20have%20yet%20to%20efficiently%20integrate%20the%20transformative%20protein%20language%20models%20with%20advanced%20optimization%20techniques%2C%20such%20as%20reinforcement%20learning%2C%20to%20learn%20optimal%20evolution%20policies.%20To%20bridge%20this%20gap%2C%20we%20propose%20AlphaDE%2C%20a%20novel%20framework%20that%20evolves%20protein%20sequences%20by%20harnessing%20the%20innovative%20paradigms%20of%20large%20language%20models%2C%20such%20as%20fine-tuning%20and%20test-time%20inference.%20First%2C%20AlphaDE%20fine-tunes%20pretrained%20protein%20language%20models%20using%20masked%20language%20modeling%20on%20homologous%20protein%20sequences%20to%20activate%20the%20evolutionary%20plausibility%20of%20the%20interested%20protein%20family.%20Second%2C%20AlphaDE%20introduces%20test-time%20inference%20based%20on%20Monte%20Carlo%20tree%20search%2C%20which%20effectively%20evolves%20proteins%20with%20evolutionary%20guidance%20from%20the%20fine-tuned%20protein%20language%20model.%20Extensive%20benchmark%20experiments%20show%20that%20AlphaDE%20remarkably%20outperforms%20previous%20state-of-the-art%20methods%20even%20with%20few-shot%20fine-tuning.%20A%20case%20study%20further%20demonstrates%20that%20AlphaDE%20supports%20condensing%20the%20protein%20sequence%20space%20of%20avGFP%20through%20computational%20evolution.%0ALink%3A%20http%3A//arxiv.org/abs/2511.09900v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520In-Silicon%2520Directed%2520Evolution%2520with%2520Fine-Tuned%2520Protein%2520Language%2520Model%2520and%2520Tree%2520Search%26entry.906535625%3DYaodong%2520Yang%2520and%2520Yang%2520Wang%2520and%2520Jinpeng%2520Li%2520and%2520Pei%2520Guo%2520and%2520Da%2520Han%2520and%2520Guangyong%2520Chen%2520and%2520Pheng-Ann%2520Heng%26entry.1292438233%3DProtein%2520evolution%2520through%2520amino%2520acid%2520mutations%2520is%2520a%2520cornerstone%2520of%2520life%2520sciences.%2520Recent%2520advances%2520in%2520protein%2520language%2520models%2520have%2520shown%2520rich%2520evolutionary%2520patterns%252C%2520offering%2520unprecedented%2520potential%2520for%2520in-silicon%2520directed%2520evolution.%2520However%252C%2520existing%2520directed%2520evolution%2520methods%2520largely%2520rely%2520on%2520heuristic%2520evolution%2520strategies%2520and%2520have%2520yet%2520to%2520efficiently%2520integrate%2520the%2520transformative%2520protein%2520language%2520models%2520with%2520advanced%2520optimization%2520techniques%252C%2520such%2520as%2520reinforcement%2520learning%252C%2520to%2520learn%2520optimal%2520evolution%2520policies.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520AlphaDE%252C%2520a%2520novel%2520framework%2520that%2520evolves%2520protein%2520sequences%2520by%2520harnessing%2520the%2520innovative%2520paradigms%2520of%2520large%2520language%2520models%252C%2520such%2520as%2520fine-tuning%2520and%2520test-time%2520inference.%2520First%252C%2520AlphaDE%2520fine-tunes%2520pretrained%2520protein%2520language%2520models%2520using%2520masked%2520language%2520modeling%2520on%2520homologous%2520protein%2520sequences%2520to%2520activate%2520the%2520evolutionary%2520plausibility%2520of%2520the%2520interested%2520protein%2520family.%2520Second%252C%2520AlphaDE%2520introduces%2520test-time%2520inference%2520based%2520on%2520Monte%2520Carlo%2520tree%2520search%252C%2520which%2520effectively%2520evolves%2520proteins%2520with%2520evolutionary%2520guidance%2520from%2520the%2520fine-tuned%2520protein%2520language%2520model.%2520Extensive%2520benchmark%2520experiments%2520show%2520that%2520AlphaDE%2520remarkably%2520outperforms%2520previous%2520state-of-the-art%2520methods%2520even%2520with%2520few-shot%2520fine-tuning.%2520A%2520case%2520study%2520further%2520demonstrates%2520that%2520AlphaDE%2520supports%2520condensing%2520the%2520protein%2520sequence%2520space%2520of%2520avGFP%2520through%2520computational%2520evolution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.09900v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20In-Silicon%20Directed%20Evolution%20with%20Fine-Tuned%20Protein%20Language%20Model%20and%20Tree%20Search&entry.906535625=Yaodong%20Yang%20and%20Yang%20Wang%20and%20Jinpeng%20Li%20and%20Pei%20Guo%20and%20Da%20Han%20and%20Guangyong%20Chen%20and%20Pheng-Ann%20Heng&entry.1292438233=Protein%20evolution%20through%20amino%20acid%20mutations%20is%20a%20cornerstone%20of%20life%20sciences.%20Recent%20advances%20in%20protein%20language%20models%20have%20shown%20rich%20evolutionary%20patterns%2C%20offering%20unprecedented%20potential%20for%20in-silicon%20directed%20evolution.%20However%2C%20existing%20directed%20evolution%20methods%20largely%20rely%20on%20heuristic%20evolution%20strategies%20and%20have%20yet%20to%20efficiently%20integrate%20the%20transformative%20protein%20language%20models%20with%20advanced%20optimization%20techniques%2C%20such%20as%20reinforcement%20learning%2C%20to%20learn%20optimal%20evolution%20policies.%20To%20bridge%20this%20gap%2C%20we%20propose%20AlphaDE%2C%20a%20novel%20framework%20that%20evolves%20protein%20sequences%20by%20harnessing%20the%20innovative%20paradigms%20of%20large%20language%20models%2C%20such%20as%20fine-tuning%20and%20test-time%20inference.%20First%2C%20AlphaDE%20fine-tunes%20pretrained%20protein%20language%20models%20using%20masked%20language%20modeling%20on%20homologous%20protein%20sequences%20to%20activate%20the%20evolutionary%20plausibility%20of%20the%20interested%20protein%20family.%20Second%2C%20AlphaDE%20introduces%20test-time%20inference%20based%20on%20Monte%20Carlo%20tree%20search%2C%20which%20effectively%20evolves%20proteins%20with%20evolutionary%20guidance%20from%20the%20fine-tuned%20protein%20language%20model.%20Extensive%20benchmark%20experiments%20show%20that%20AlphaDE%20remarkably%20outperforms%20previous%20state-of-the-art%20methods%20even%20with%20few-shot%20fine-tuning.%20A%20case%20study%20further%20demonstrates%20that%20AlphaDE%20supports%20condensing%20the%20protein%20sequence%20space%20of%20avGFP%20through%20computational%20evolution.&entry.1838667208=http%3A//arxiv.org/abs/2511.09900v3&entry.124074799=Read"},
{"title": "Splatent: Splatting Diffusion Latents for Novel View Synthesis", "author": "Or Hirschorn and Omer Sela and Inbar Huberman-Spiegelglas and Netalee Efrat and Eli Alshan and Ianir Ideses and Frederic Devernay and Yochai Zvik and Lior Fritz", "abstract": "Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.", "link": "http://arxiv.org/abs/2512.09923v1", "date": "2025-12-10", "relevancy": 2.4791, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6563}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.594}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Splatent%3A%20Splatting%20Diffusion%20Latents%20for%20Novel%20View%20Synthesis&body=Title%3A%20Splatent%3A%20Splatting%20Diffusion%20Latents%20for%20Novel%20View%20Synthesis%0AAuthor%3A%20Or%20Hirschorn%20and%20Omer%20Sela%20and%20Inbar%20Huberman-Spiegelglas%20and%20Netalee%20Efrat%20and%20Eli%20Alshan%20and%20Ianir%20Ideses%20and%20Frederic%20Devernay%20and%20Yochai%20Zvik%20and%20Lior%20Fritz%0AAbstract%3A%20Radiance%20field%20representations%20have%20recently%20been%20explored%20in%20the%20latent%20space%20of%20VAEs%20that%20are%20commonly%20used%20by%20diffusion%20models.%20This%20direction%20offers%20efficient%20rendering%20and%20seamless%20integration%20with%20diffusion-based%20pipelines.%20However%2C%20these%20methods%20face%20a%20fundamental%20limitation%3A%20The%20VAE%20latent%20space%20lacks%20multi-view%20consistency%2C%20leading%20to%20blurred%20textures%20and%20missing%20details%20during%203D%20reconstruction.%20Existing%20approaches%20attempt%20to%20address%20this%20by%20fine-tuning%20the%20VAE%2C%20at%20the%20cost%20of%20reconstruction%20quality%2C%20or%20by%20relying%20on%20pre-trained%20diffusion%20models%20to%20recover%20fine-grained%20details%2C%20at%20the%20risk%20of%20some%20hallucinations.%20We%20present%20Splatent%2C%20a%20diffusion-based%20enhancement%20framework%20designed%20to%20operate%20on%20top%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20in%20the%20latent%20space%20of%20VAEs.%20Our%20key%20insight%20departs%20from%20the%20conventional%203D-centric%20view%3A%20rather%20than%20reconstructing%20fine-grained%20details%20in%203D%20space%2C%20we%20recover%20them%20in%202D%20from%20input%20views%20through%20multi-view%20attention%20mechanisms.%20This%20approach%20preserves%20the%20reconstruction%20quality%20of%20pretrained%20VAEs%20while%20achieving%20faithful%20detail%20recovery.%20Evaluated%20across%20multiple%20benchmarks%2C%20Splatent%20establishes%20a%20new%20state-of-the-art%20for%20VAE%20latent%20radiance%20field%20reconstruction.%20We%20further%20demonstrate%20that%20integrating%20our%20method%20with%20existing%20feed-forward%20frameworks%2C%20consistently%20improves%20detail%20preservation%2C%20opening%20new%20possibilities%20for%20high-quality%20sparse-view%203D%20reconstruction.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplatent%253A%2520Splatting%2520Diffusion%2520Latents%2520for%2520Novel%2520View%2520Synthesis%26entry.906535625%3DOr%2520Hirschorn%2520and%2520Omer%2520Sela%2520and%2520Inbar%2520Huberman-Spiegelglas%2520and%2520Netalee%2520Efrat%2520and%2520Eli%2520Alshan%2520and%2520Ianir%2520Ideses%2520and%2520Frederic%2520Devernay%2520and%2520Yochai%2520Zvik%2520and%2520Lior%2520Fritz%26entry.1292438233%3DRadiance%2520field%2520representations%2520have%2520recently%2520been%2520explored%2520in%2520the%2520latent%2520space%2520of%2520VAEs%2520that%2520are%2520commonly%2520used%2520by%2520diffusion%2520models.%2520This%2520direction%2520offers%2520efficient%2520rendering%2520and%2520seamless%2520integration%2520with%2520diffusion-based%2520pipelines.%2520However%252C%2520these%2520methods%2520face%2520a%2520fundamental%2520limitation%253A%2520The%2520VAE%2520latent%2520space%2520lacks%2520multi-view%2520consistency%252C%2520leading%2520to%2520blurred%2520textures%2520and%2520missing%2520details%2520during%25203D%2520reconstruction.%2520Existing%2520approaches%2520attempt%2520to%2520address%2520this%2520by%2520fine-tuning%2520the%2520VAE%252C%2520at%2520the%2520cost%2520of%2520reconstruction%2520quality%252C%2520or%2520by%2520relying%2520on%2520pre-trained%2520diffusion%2520models%2520to%2520recover%2520fine-grained%2520details%252C%2520at%2520the%2520risk%2520of%2520some%2520hallucinations.%2520We%2520present%2520Splatent%252C%2520a%2520diffusion-based%2520enhancement%2520framework%2520designed%2520to%2520operate%2520on%2520top%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520in%2520the%2520latent%2520space%2520of%2520VAEs.%2520Our%2520key%2520insight%2520departs%2520from%2520the%2520conventional%25203D-centric%2520view%253A%2520rather%2520than%2520reconstructing%2520fine-grained%2520details%2520in%25203D%2520space%252C%2520we%2520recover%2520them%2520in%25202D%2520from%2520input%2520views%2520through%2520multi-view%2520attention%2520mechanisms.%2520This%2520approach%2520preserves%2520the%2520reconstruction%2520quality%2520of%2520pretrained%2520VAEs%2520while%2520achieving%2520faithful%2520detail%2520recovery.%2520Evaluated%2520across%2520multiple%2520benchmarks%252C%2520Splatent%2520establishes%2520a%2520new%2520state-of-the-art%2520for%2520VAE%2520latent%2520radiance%2520field%2520reconstruction.%2520We%2520further%2520demonstrate%2520that%2520integrating%2520our%2520method%2520with%2520existing%2520feed-forward%2520frameworks%252C%2520consistently%2520improves%2520detail%2520preservation%252C%2520opening%2520new%2520possibilities%2520for%2520high-quality%2520sparse-view%25203D%2520reconstruction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Splatent%3A%20Splatting%20Diffusion%20Latents%20for%20Novel%20View%20Synthesis&entry.906535625=Or%20Hirschorn%20and%20Omer%20Sela%20and%20Inbar%20Huberman-Spiegelglas%20and%20Netalee%20Efrat%20and%20Eli%20Alshan%20and%20Ianir%20Ideses%20and%20Frederic%20Devernay%20and%20Yochai%20Zvik%20and%20Lior%20Fritz&entry.1292438233=Radiance%20field%20representations%20have%20recently%20been%20explored%20in%20the%20latent%20space%20of%20VAEs%20that%20are%20commonly%20used%20by%20diffusion%20models.%20This%20direction%20offers%20efficient%20rendering%20and%20seamless%20integration%20with%20diffusion-based%20pipelines.%20However%2C%20these%20methods%20face%20a%20fundamental%20limitation%3A%20The%20VAE%20latent%20space%20lacks%20multi-view%20consistency%2C%20leading%20to%20blurred%20textures%20and%20missing%20details%20during%203D%20reconstruction.%20Existing%20approaches%20attempt%20to%20address%20this%20by%20fine-tuning%20the%20VAE%2C%20at%20the%20cost%20of%20reconstruction%20quality%2C%20or%20by%20relying%20on%20pre-trained%20diffusion%20models%20to%20recover%20fine-grained%20details%2C%20at%20the%20risk%20of%20some%20hallucinations.%20We%20present%20Splatent%2C%20a%20diffusion-based%20enhancement%20framework%20designed%20to%20operate%20on%20top%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20in%20the%20latent%20space%20of%20VAEs.%20Our%20key%20insight%20departs%20from%20the%20conventional%203D-centric%20view%3A%20rather%20than%20reconstructing%20fine-grained%20details%20in%203D%20space%2C%20we%20recover%20them%20in%202D%20from%20input%20views%20through%20multi-view%20attention%20mechanisms.%20This%20approach%20preserves%20the%20reconstruction%20quality%20of%20pretrained%20VAEs%20while%20achieving%20faithful%20detail%20recovery.%20Evaluated%20across%20multiple%20benchmarks%2C%20Splatent%20establishes%20a%20new%20state-of-the-art%20for%20VAE%20latent%20radiance%20field%20reconstruction.%20We%20further%20demonstrate%20that%20integrating%20our%20method%20with%20existing%20feed-forward%20frameworks%2C%20consistently%20improves%20detail%20preservation%2C%20opening%20new%20possibilities%20for%20high-quality%20sparse-view%203D%20reconstruction.&entry.1838667208=http%3A//arxiv.org/abs/2512.09923v1&entry.124074799=Read"},
{"title": "Latent-Autoregressive GP-VAE Language Model", "author": "Yves Ruffenach", "abstract": "We investigate a fully Latent AutoRegressive scheme based on a Gaussian Process (GP) integrated into a Variational Autoencoder (VAE). In this setting, sequential dynamics are transferred from the observation space to a continuous latent space, while linguistic generation remains parallel through a non-autoregressive decoder. We present a complete methodological formulation, including a causal GP prior, a structured amortized posterior, and a training protocol based on a regularized ELBO. Empirical evaluation, conducted within a deliberately constrained proof-of-concept (POC) framework, shows that the model can be trained stably and that the sequential and parallel sampling variants exhibit consistent behavior. Overall, the results suggest that part of the temporal structure in a language model can be supported by the probabilistic geometry of the latent space rather than by explicit neural operations.", "link": "http://arxiv.org/abs/2512.09535v1", "date": "2025-12-10", "relevancy": 2.4578, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5424}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4821}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent-Autoregressive%20GP-VAE%20Language%20Model&body=Title%3A%20Latent-Autoregressive%20GP-VAE%20Language%20Model%0AAuthor%3A%20Yves%20Ruffenach%0AAbstract%3A%20We%20investigate%20a%20fully%20Latent%20AutoRegressive%20scheme%20based%20on%20a%20Gaussian%20Process%20%28GP%29%20integrated%20into%20a%20Variational%20Autoencoder%20%28VAE%29.%20In%20this%20setting%2C%20sequential%20dynamics%20are%20transferred%20from%20the%20observation%20space%20to%20a%20continuous%20latent%20space%2C%20while%20linguistic%20generation%20remains%20parallel%20through%20a%20non-autoregressive%20decoder.%20We%20present%20a%20complete%20methodological%20formulation%2C%20including%20a%20causal%20GP%20prior%2C%20a%20structured%20amortized%20posterior%2C%20and%20a%20training%20protocol%20based%20on%20a%20regularized%20ELBO.%20Empirical%20evaluation%2C%20conducted%20within%20a%20deliberately%20constrained%20proof-of-concept%20%28POC%29%20framework%2C%20shows%20that%20the%20model%20can%20be%20trained%20stably%20and%20that%20the%20sequential%20and%20parallel%20sampling%20variants%20exhibit%20consistent%20behavior.%20Overall%2C%20the%20results%20suggest%20that%20part%20of%20the%20temporal%20structure%20in%20a%20language%20model%20can%20be%20supported%20by%20the%20probabilistic%20geometry%20of%20the%20latent%20space%20rather%20than%20by%20explicit%20neural%20operations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent-Autoregressive%2520GP-VAE%2520Language%2520Model%26entry.906535625%3DYves%2520Ruffenach%26entry.1292438233%3DWe%2520investigate%2520a%2520fully%2520Latent%2520AutoRegressive%2520scheme%2520based%2520on%2520a%2520Gaussian%2520Process%2520%2528GP%2529%2520integrated%2520into%2520a%2520Variational%2520Autoencoder%2520%2528VAE%2529.%2520In%2520this%2520setting%252C%2520sequential%2520dynamics%2520are%2520transferred%2520from%2520the%2520observation%2520space%2520to%2520a%2520continuous%2520latent%2520space%252C%2520while%2520linguistic%2520generation%2520remains%2520parallel%2520through%2520a%2520non-autoregressive%2520decoder.%2520We%2520present%2520a%2520complete%2520methodological%2520formulation%252C%2520including%2520a%2520causal%2520GP%2520prior%252C%2520a%2520structured%2520amortized%2520posterior%252C%2520and%2520a%2520training%2520protocol%2520based%2520on%2520a%2520regularized%2520ELBO.%2520Empirical%2520evaluation%252C%2520conducted%2520within%2520a%2520deliberately%2520constrained%2520proof-of-concept%2520%2528POC%2529%2520framework%252C%2520shows%2520that%2520the%2520model%2520can%2520be%2520trained%2520stably%2520and%2520that%2520the%2520sequential%2520and%2520parallel%2520sampling%2520variants%2520exhibit%2520consistent%2520behavior.%2520Overall%252C%2520the%2520results%2520suggest%2520that%2520part%2520of%2520the%2520temporal%2520structure%2520in%2520a%2520language%2520model%2520can%2520be%2520supported%2520by%2520the%2520probabilistic%2520geometry%2520of%2520the%2520latent%2520space%2520rather%2520than%2520by%2520explicit%2520neural%2520operations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent-Autoregressive%20GP-VAE%20Language%20Model&entry.906535625=Yves%20Ruffenach&entry.1292438233=We%20investigate%20a%20fully%20Latent%20AutoRegressive%20scheme%20based%20on%20a%20Gaussian%20Process%20%28GP%29%20integrated%20into%20a%20Variational%20Autoencoder%20%28VAE%29.%20In%20this%20setting%2C%20sequential%20dynamics%20are%20transferred%20from%20the%20observation%20space%20to%20a%20continuous%20latent%20space%2C%20while%20linguistic%20generation%20remains%20parallel%20through%20a%20non-autoregressive%20decoder.%20We%20present%20a%20complete%20methodological%20formulation%2C%20including%20a%20causal%20GP%20prior%2C%20a%20structured%20amortized%20posterior%2C%20and%20a%20training%20protocol%20based%20on%20a%20regularized%20ELBO.%20Empirical%20evaluation%2C%20conducted%20within%20a%20deliberately%20constrained%20proof-of-concept%20%28POC%29%20framework%2C%20shows%20that%20the%20model%20can%20be%20trained%20stably%20and%20that%20the%20sequential%20and%20parallel%20sampling%20variants%20exhibit%20consistent%20behavior.%20Overall%2C%20the%20results%20suggest%20that%20part%20of%20the%20temporal%20structure%20in%20a%20language%20model%20can%20be%20supported%20by%20the%20probabilistic%20geometry%20of%20the%20latent%20space%20rather%20than%20by%20explicit%20neural%20operations.&entry.1838667208=http%3A//arxiv.org/abs/2512.09535v1&entry.124074799=Read"},
{"title": "DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation", "author": "Zhizhong Wang and Tianyi Chu and Zeyi Huang and Nanyang Wang and Kehan Li", "abstract": "Personalized Text-to-Image (PT2I) generation aims to produce customized images based on reference images. A prominent interest pertains to the integration of an image prompt adapter to facilitate zero-shot PT2I without test-time fine-tuning. However, current methods grapple with three fundamental challenges: 1. the elusive equilibrium between Concept Preservation (CP) and Prompt Following (PF), 2. the difficulty in retaining fine-grained concept details in reference images, and 3. the restricted scalability to extend to multi-subject personalization. To tackle these challenges, we present Dynamic Image Prompt Adapter (DynaIP), a cutting-edge plugin to enhance the fine-grained concept fidelity, CP-PF balance, and subject scalability of SOTA T2I multimodal diffusion transformers (MM-DiT) for PT2I generation. Our key finding is that MM-DiT inherently exhibit decoupling learning behavior when injecting reference image features into its dual branches via cross attentions. Based on this, we design an innovative Dynamic Decoupling Strategy that removes the interference of concept-agnostic information during inference, significantly enhancing the CP-PF balance and further bolstering the scalability of multi-subject compositions. Moreover, we identify the visual encoder as a key factor affecting fine-grained CP and reveal that the hierarchical features of commonly used CLIP can capture visual information at diverse granularity levels. Therefore, we introduce a novel Hierarchical Mixture-of-Experts Feature Fusion Module to fully leverage the hierarchical features of CLIP, remarkably elevating the fine-grained concept fidelity while also providing flexible control of visual granularity. Extensive experiments across single- and multi-subject PT2I tasks verify that our DynaIP outperforms existing approaches, marking a notable advancement in the field of PT2l generation.", "link": "http://arxiv.org/abs/2512.09814v1", "date": "2025-12-10", "relevancy": 2.4545, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6411}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6038}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaIP%3A%20Dynamic%20Image%20Prompt%20Adapter%20for%20Scalable%20Zero-shot%20Personalized%20Text-to-Image%20Generation&body=Title%3A%20DynaIP%3A%20Dynamic%20Image%20Prompt%20Adapter%20for%20Scalable%20Zero-shot%20Personalized%20Text-to-Image%20Generation%0AAuthor%3A%20Zhizhong%20Wang%20and%20Tianyi%20Chu%20and%20Zeyi%20Huang%20and%20Nanyang%20Wang%20and%20Kehan%20Li%0AAbstract%3A%20Personalized%20Text-to-Image%20%28PT2I%29%20generation%20aims%20to%20produce%20customized%20images%20based%20on%20reference%20images.%20A%20prominent%20interest%20pertains%20to%20the%20integration%20of%20an%20image%20prompt%20adapter%20to%20facilitate%20zero-shot%20PT2I%20without%20test-time%20fine-tuning.%20However%2C%20current%20methods%20grapple%20with%20three%20fundamental%20challenges%3A%201.%20the%20elusive%20equilibrium%20between%20Concept%20Preservation%20%28CP%29%20and%20Prompt%20Following%20%28PF%29%2C%202.%20the%20difficulty%20in%20retaining%20fine-grained%20concept%20details%20in%20reference%20images%2C%20and%203.%20the%20restricted%20scalability%20to%20extend%20to%20multi-subject%20personalization.%20To%20tackle%20these%20challenges%2C%20we%20present%20Dynamic%20Image%20Prompt%20Adapter%20%28DynaIP%29%2C%20a%20cutting-edge%20plugin%20to%20enhance%20the%20fine-grained%20concept%20fidelity%2C%20CP-PF%20balance%2C%20and%20subject%20scalability%20of%20SOTA%20T2I%20multimodal%20diffusion%20transformers%20%28MM-DiT%29%20for%20PT2I%20generation.%20Our%20key%20finding%20is%20that%20MM-DiT%20inherently%20exhibit%20decoupling%20learning%20behavior%20when%20injecting%20reference%20image%20features%20into%20its%20dual%20branches%20via%20cross%20attentions.%20Based%20on%20this%2C%20we%20design%20an%20innovative%20Dynamic%20Decoupling%20Strategy%20that%20removes%20the%20interference%20of%20concept-agnostic%20information%20during%20inference%2C%20significantly%20enhancing%20the%20CP-PF%20balance%20and%20further%20bolstering%20the%20scalability%20of%20multi-subject%20compositions.%20Moreover%2C%20we%20identify%20the%20visual%20encoder%20as%20a%20key%20factor%20affecting%20fine-grained%20CP%20and%20reveal%20that%20the%20hierarchical%20features%20of%20commonly%20used%20CLIP%20can%20capture%20visual%20information%20at%20diverse%20granularity%20levels.%20Therefore%2C%20we%20introduce%20a%20novel%20Hierarchical%20Mixture-of-Experts%20Feature%20Fusion%20Module%20to%20fully%20leverage%20the%20hierarchical%20features%20of%20CLIP%2C%20remarkably%20elevating%20the%20fine-grained%20concept%20fidelity%20while%20also%20providing%20flexible%20control%20of%20visual%20granularity.%20Extensive%20experiments%20across%20single-%20and%20multi-subject%20PT2I%20tasks%20verify%20that%20our%20DynaIP%20outperforms%20existing%20approaches%2C%20marking%20a%20notable%20advancement%20in%20the%20field%20of%20PT2l%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaIP%253A%2520Dynamic%2520Image%2520Prompt%2520Adapter%2520for%2520Scalable%2520Zero-shot%2520Personalized%2520Text-to-Image%2520Generation%26entry.906535625%3DZhizhong%2520Wang%2520and%2520Tianyi%2520Chu%2520and%2520Zeyi%2520Huang%2520and%2520Nanyang%2520Wang%2520and%2520Kehan%2520Li%26entry.1292438233%3DPersonalized%2520Text-to-Image%2520%2528PT2I%2529%2520generation%2520aims%2520to%2520produce%2520customized%2520images%2520based%2520on%2520reference%2520images.%2520A%2520prominent%2520interest%2520pertains%2520to%2520the%2520integration%2520of%2520an%2520image%2520prompt%2520adapter%2520to%2520facilitate%2520zero-shot%2520PT2I%2520without%2520test-time%2520fine-tuning.%2520However%252C%2520current%2520methods%2520grapple%2520with%2520three%2520fundamental%2520challenges%253A%25201.%2520the%2520elusive%2520equilibrium%2520between%2520Concept%2520Preservation%2520%2528CP%2529%2520and%2520Prompt%2520Following%2520%2528PF%2529%252C%25202.%2520the%2520difficulty%2520in%2520retaining%2520fine-grained%2520concept%2520details%2520in%2520reference%2520images%252C%2520and%25203.%2520the%2520restricted%2520scalability%2520to%2520extend%2520to%2520multi-subject%2520personalization.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520present%2520Dynamic%2520Image%2520Prompt%2520Adapter%2520%2528DynaIP%2529%252C%2520a%2520cutting-edge%2520plugin%2520to%2520enhance%2520the%2520fine-grained%2520concept%2520fidelity%252C%2520CP-PF%2520balance%252C%2520and%2520subject%2520scalability%2520of%2520SOTA%2520T2I%2520multimodal%2520diffusion%2520transformers%2520%2528MM-DiT%2529%2520for%2520PT2I%2520generation.%2520Our%2520key%2520finding%2520is%2520that%2520MM-DiT%2520inherently%2520exhibit%2520decoupling%2520learning%2520behavior%2520when%2520injecting%2520reference%2520image%2520features%2520into%2520its%2520dual%2520branches%2520via%2520cross%2520attentions.%2520Based%2520on%2520this%252C%2520we%2520design%2520an%2520innovative%2520Dynamic%2520Decoupling%2520Strategy%2520that%2520removes%2520the%2520interference%2520of%2520concept-agnostic%2520information%2520during%2520inference%252C%2520significantly%2520enhancing%2520the%2520CP-PF%2520balance%2520and%2520further%2520bolstering%2520the%2520scalability%2520of%2520multi-subject%2520compositions.%2520Moreover%252C%2520we%2520identify%2520the%2520visual%2520encoder%2520as%2520a%2520key%2520factor%2520affecting%2520fine-grained%2520CP%2520and%2520reveal%2520that%2520the%2520hierarchical%2520features%2520of%2520commonly%2520used%2520CLIP%2520can%2520capture%2520visual%2520information%2520at%2520diverse%2520granularity%2520levels.%2520Therefore%252C%2520we%2520introduce%2520a%2520novel%2520Hierarchical%2520Mixture-of-Experts%2520Feature%2520Fusion%2520Module%2520to%2520fully%2520leverage%2520the%2520hierarchical%2520features%2520of%2520CLIP%252C%2520remarkably%2520elevating%2520the%2520fine-grained%2520concept%2520fidelity%2520while%2520also%2520providing%2520flexible%2520control%2520of%2520visual%2520granularity.%2520Extensive%2520experiments%2520across%2520single-%2520and%2520multi-subject%2520PT2I%2520tasks%2520verify%2520that%2520our%2520DynaIP%2520outperforms%2520existing%2520approaches%252C%2520marking%2520a%2520notable%2520advancement%2520in%2520the%2520field%2520of%2520PT2l%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaIP%3A%20Dynamic%20Image%20Prompt%20Adapter%20for%20Scalable%20Zero-shot%20Personalized%20Text-to-Image%20Generation&entry.906535625=Zhizhong%20Wang%20and%20Tianyi%20Chu%20and%20Zeyi%20Huang%20and%20Nanyang%20Wang%20and%20Kehan%20Li&entry.1292438233=Personalized%20Text-to-Image%20%28PT2I%29%20generation%20aims%20to%20produce%20customized%20images%20based%20on%20reference%20images.%20A%20prominent%20interest%20pertains%20to%20the%20integration%20of%20an%20image%20prompt%20adapter%20to%20facilitate%20zero-shot%20PT2I%20without%20test-time%20fine-tuning.%20However%2C%20current%20methods%20grapple%20with%20three%20fundamental%20challenges%3A%201.%20the%20elusive%20equilibrium%20between%20Concept%20Preservation%20%28CP%29%20and%20Prompt%20Following%20%28PF%29%2C%202.%20the%20difficulty%20in%20retaining%20fine-grained%20concept%20details%20in%20reference%20images%2C%20and%203.%20the%20restricted%20scalability%20to%20extend%20to%20multi-subject%20personalization.%20To%20tackle%20these%20challenges%2C%20we%20present%20Dynamic%20Image%20Prompt%20Adapter%20%28DynaIP%29%2C%20a%20cutting-edge%20plugin%20to%20enhance%20the%20fine-grained%20concept%20fidelity%2C%20CP-PF%20balance%2C%20and%20subject%20scalability%20of%20SOTA%20T2I%20multimodal%20diffusion%20transformers%20%28MM-DiT%29%20for%20PT2I%20generation.%20Our%20key%20finding%20is%20that%20MM-DiT%20inherently%20exhibit%20decoupling%20learning%20behavior%20when%20injecting%20reference%20image%20features%20into%20its%20dual%20branches%20via%20cross%20attentions.%20Based%20on%20this%2C%20we%20design%20an%20innovative%20Dynamic%20Decoupling%20Strategy%20that%20removes%20the%20interference%20of%20concept-agnostic%20information%20during%20inference%2C%20significantly%20enhancing%20the%20CP-PF%20balance%20and%20further%20bolstering%20the%20scalability%20of%20multi-subject%20compositions.%20Moreover%2C%20we%20identify%20the%20visual%20encoder%20as%20a%20key%20factor%20affecting%20fine-grained%20CP%20and%20reveal%20that%20the%20hierarchical%20features%20of%20commonly%20used%20CLIP%20can%20capture%20visual%20information%20at%20diverse%20granularity%20levels.%20Therefore%2C%20we%20introduce%20a%20novel%20Hierarchical%20Mixture-of-Experts%20Feature%20Fusion%20Module%20to%20fully%20leverage%20the%20hierarchical%20features%20of%20CLIP%2C%20remarkably%20elevating%20the%20fine-grained%20concept%20fidelity%20while%20also%20providing%20flexible%20control%20of%20visual%20granularity.%20Extensive%20experiments%20across%20single-%20and%20multi-subject%20PT2I%20tasks%20verify%20that%20our%20DynaIP%20outperforms%20existing%20approaches%2C%20marking%20a%20notable%20advancement%20in%20the%20field%20of%20PT2l%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2512.09814v1&entry.124074799=Read"},
{"title": "Kaapana: A Comprehensive Open-Source Platform for Integrating AI in Medical Imaging Research Environments", "author": "\u00dcnal Ak\u00fcnal and Markus Bujotzek and Stefan Denner and Benjamin Hamm and Klaus Kades and Philipp Schader and Jonas Scherer and Marco Nolden and Peter Neher and Ralf Floca and Klaus Maier-Hein", "abstract": "Developing generalizable AI for medical imaging requires both access to large, multi-center datasets and standardized, reproducible tooling within research environments. However, leveraging real-world imaging data in clinical research environments is still hampered by strict regulatory constraints, fragmented software infrastructure, and the challenges inherent in conducting large-cohort multicentre studies. This leads to projects that rely on ad-hoc toolchains that are hard to reproduce, difficult to scale beyond single institutions and poorly suited for collaboration between clinicians and data scientists. We present Kaapana, a comprehensive open-source platform for medical imaging research that is designed to bridge this gap. Rather than building single-use, site-specific tooling, Kaapana provides a modular, extensible framework that unifies data ingestion, cohort curation, processing workflows and result inspection under a common user interface. By bringing the algorithm to the data, it enables institutions to keep control over their sensitive data while still participating in distributed experimentation and model development. By integrating flexible workflow orchestration with user-facing applications for researchers, Kaapana reduces technical overhead, improves reproducibility and enables conducting large-scale, collaborative, multi-centre imaging studies. We describe the core concepts of the platform and illustrate how they can support diverse use cases, from local prototyping to nation-wide research networks. The open-source codebase is available at https://github.com/kaapana/kaapana", "link": "http://arxiv.org/abs/2512.09644v1", "date": "2025-12-10", "relevancy": 2.4459, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4998}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4998}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kaapana%3A%20A%20Comprehensive%20Open-Source%20Platform%20for%20Integrating%20AI%20in%20Medical%20Imaging%20Research%20Environments&body=Title%3A%20Kaapana%3A%20A%20Comprehensive%20Open-Source%20Platform%20for%20Integrating%20AI%20in%20Medical%20Imaging%20Research%20Environments%0AAuthor%3A%20%C3%9Cnal%20Ak%C3%BCnal%20and%20Markus%20Bujotzek%20and%20Stefan%20Denner%20and%20Benjamin%20Hamm%20and%20Klaus%20Kades%20and%20Philipp%20Schader%20and%20Jonas%20Scherer%20and%20Marco%20Nolden%20and%20Peter%20Neher%20and%20Ralf%20Floca%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20Developing%20generalizable%20AI%20for%20medical%20imaging%20requires%20both%20access%20to%20large%2C%20multi-center%20datasets%20and%20standardized%2C%20reproducible%20tooling%20within%20research%20environments.%20However%2C%20leveraging%20real-world%20imaging%20data%20in%20clinical%20research%20environments%20is%20still%20hampered%20by%20strict%20regulatory%20constraints%2C%20fragmented%20software%20infrastructure%2C%20and%20the%20challenges%20inherent%20in%20conducting%20large-cohort%20multicentre%20studies.%20This%20leads%20to%20projects%20that%20rely%20on%20ad-hoc%20toolchains%20that%20are%20hard%20to%20reproduce%2C%20difficult%20to%20scale%20beyond%20single%20institutions%20and%20poorly%20suited%20for%20collaboration%20between%20clinicians%20and%20data%20scientists.%20We%20present%20Kaapana%2C%20a%20comprehensive%20open-source%20platform%20for%20medical%20imaging%20research%20that%20is%20designed%20to%20bridge%20this%20gap.%20Rather%20than%20building%20single-use%2C%20site-specific%20tooling%2C%20Kaapana%20provides%20a%20modular%2C%20extensible%20framework%20that%20unifies%20data%20ingestion%2C%20cohort%20curation%2C%20processing%20workflows%20and%20result%20inspection%20under%20a%20common%20user%20interface.%20By%20bringing%20the%20algorithm%20to%20the%20data%2C%20it%20enables%20institutions%20to%20keep%20control%20over%20their%20sensitive%20data%20while%20still%20participating%20in%20distributed%20experimentation%20and%20model%20development.%20By%20integrating%20flexible%20workflow%20orchestration%20with%20user-facing%20applications%20for%20researchers%2C%20Kaapana%20reduces%20technical%20overhead%2C%20improves%20reproducibility%20and%20enables%20conducting%20large-scale%2C%20collaborative%2C%20multi-centre%20imaging%20studies.%20We%20describe%20the%20core%20concepts%20of%20the%20platform%20and%20illustrate%20how%20they%20can%20support%20diverse%20use%20cases%2C%20from%20local%20prototyping%20to%20nation-wide%20research%20networks.%20The%20open-source%20codebase%20is%20available%20at%20https%3A//github.com/kaapana/kaapana%0ALink%3A%20http%3A//arxiv.org/abs/2512.09644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKaapana%253A%2520A%2520Comprehensive%2520Open-Source%2520Platform%2520for%2520Integrating%2520AI%2520in%2520Medical%2520Imaging%2520Research%2520Environments%26entry.906535625%3D%25C3%259Cnal%2520Ak%25C3%25BCnal%2520and%2520Markus%2520Bujotzek%2520and%2520Stefan%2520Denner%2520and%2520Benjamin%2520Hamm%2520and%2520Klaus%2520Kades%2520and%2520Philipp%2520Schader%2520and%2520Jonas%2520Scherer%2520and%2520Marco%2520Nolden%2520and%2520Peter%2520Neher%2520and%2520Ralf%2520Floca%2520and%2520Klaus%2520Maier-Hein%26entry.1292438233%3DDeveloping%2520generalizable%2520AI%2520for%2520medical%2520imaging%2520requires%2520both%2520access%2520to%2520large%252C%2520multi-center%2520datasets%2520and%2520standardized%252C%2520reproducible%2520tooling%2520within%2520research%2520environments.%2520However%252C%2520leveraging%2520real-world%2520imaging%2520data%2520in%2520clinical%2520research%2520environments%2520is%2520still%2520hampered%2520by%2520strict%2520regulatory%2520constraints%252C%2520fragmented%2520software%2520infrastructure%252C%2520and%2520the%2520challenges%2520inherent%2520in%2520conducting%2520large-cohort%2520multicentre%2520studies.%2520This%2520leads%2520to%2520projects%2520that%2520rely%2520on%2520ad-hoc%2520toolchains%2520that%2520are%2520hard%2520to%2520reproduce%252C%2520difficult%2520to%2520scale%2520beyond%2520single%2520institutions%2520and%2520poorly%2520suited%2520for%2520collaboration%2520between%2520clinicians%2520and%2520data%2520scientists.%2520We%2520present%2520Kaapana%252C%2520a%2520comprehensive%2520open-source%2520platform%2520for%2520medical%2520imaging%2520research%2520that%2520is%2520designed%2520to%2520bridge%2520this%2520gap.%2520Rather%2520than%2520building%2520single-use%252C%2520site-specific%2520tooling%252C%2520Kaapana%2520provides%2520a%2520modular%252C%2520extensible%2520framework%2520that%2520unifies%2520data%2520ingestion%252C%2520cohort%2520curation%252C%2520processing%2520workflows%2520and%2520result%2520inspection%2520under%2520a%2520common%2520user%2520interface.%2520By%2520bringing%2520the%2520algorithm%2520to%2520the%2520data%252C%2520it%2520enables%2520institutions%2520to%2520keep%2520control%2520over%2520their%2520sensitive%2520data%2520while%2520still%2520participating%2520in%2520distributed%2520experimentation%2520and%2520model%2520development.%2520By%2520integrating%2520flexible%2520workflow%2520orchestration%2520with%2520user-facing%2520applications%2520for%2520researchers%252C%2520Kaapana%2520reduces%2520technical%2520overhead%252C%2520improves%2520reproducibility%2520and%2520enables%2520conducting%2520large-scale%252C%2520collaborative%252C%2520multi-centre%2520imaging%2520studies.%2520We%2520describe%2520the%2520core%2520concepts%2520of%2520the%2520platform%2520and%2520illustrate%2520how%2520they%2520can%2520support%2520diverse%2520use%2520cases%252C%2520from%2520local%2520prototyping%2520to%2520nation-wide%2520research%2520networks.%2520The%2520open-source%2520codebase%2520is%2520available%2520at%2520https%253A//github.com/kaapana/kaapana%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kaapana%3A%20A%20Comprehensive%20Open-Source%20Platform%20for%20Integrating%20AI%20in%20Medical%20Imaging%20Research%20Environments&entry.906535625=%C3%9Cnal%20Ak%C3%BCnal%20and%20Markus%20Bujotzek%20and%20Stefan%20Denner%20and%20Benjamin%20Hamm%20and%20Klaus%20Kades%20and%20Philipp%20Schader%20and%20Jonas%20Scherer%20and%20Marco%20Nolden%20and%20Peter%20Neher%20and%20Ralf%20Floca%20and%20Klaus%20Maier-Hein&entry.1292438233=Developing%20generalizable%20AI%20for%20medical%20imaging%20requires%20both%20access%20to%20large%2C%20multi-center%20datasets%20and%20standardized%2C%20reproducible%20tooling%20within%20research%20environments.%20However%2C%20leveraging%20real-world%20imaging%20data%20in%20clinical%20research%20environments%20is%20still%20hampered%20by%20strict%20regulatory%20constraints%2C%20fragmented%20software%20infrastructure%2C%20and%20the%20challenges%20inherent%20in%20conducting%20large-cohort%20multicentre%20studies.%20This%20leads%20to%20projects%20that%20rely%20on%20ad-hoc%20toolchains%20that%20are%20hard%20to%20reproduce%2C%20difficult%20to%20scale%20beyond%20single%20institutions%20and%20poorly%20suited%20for%20collaboration%20between%20clinicians%20and%20data%20scientists.%20We%20present%20Kaapana%2C%20a%20comprehensive%20open-source%20platform%20for%20medical%20imaging%20research%20that%20is%20designed%20to%20bridge%20this%20gap.%20Rather%20than%20building%20single-use%2C%20site-specific%20tooling%2C%20Kaapana%20provides%20a%20modular%2C%20extensible%20framework%20that%20unifies%20data%20ingestion%2C%20cohort%20curation%2C%20processing%20workflows%20and%20result%20inspection%20under%20a%20common%20user%20interface.%20By%20bringing%20the%20algorithm%20to%20the%20data%2C%20it%20enables%20institutions%20to%20keep%20control%20over%20their%20sensitive%20data%20while%20still%20participating%20in%20distributed%20experimentation%20and%20model%20development.%20By%20integrating%20flexible%20workflow%20orchestration%20with%20user-facing%20applications%20for%20researchers%2C%20Kaapana%20reduces%20technical%20overhead%2C%20improves%20reproducibility%20and%20enables%20conducting%20large-scale%2C%20collaborative%2C%20multi-centre%20imaging%20studies.%20We%20describe%20the%20core%20concepts%20of%20the%20platform%20and%20illustrate%20how%20they%20can%20support%20diverse%20use%20cases%2C%20from%20local%20prototyping%20to%20nation-wide%20research%20networks.%20The%20open-source%20codebase%20is%20available%20at%20https%3A//github.com/kaapana/kaapana&entry.1838667208=http%3A//arxiv.org/abs/2512.09644v1&entry.124074799=Read"},
{"title": "TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B", "author": "Toshiki Nakai and Ravi Kiran Chikkala and Lena Sophie Oberkircher and Nicholas Jennings and Natalia Skachkova and Tatiana Anikina and Jesujoba Oluwadara Alabi", "abstract": "The 2025 Multimodal Models for Low-Resource Contexts and Social Impact (MMLoSo) Language Challenge addresses one of India's most pressing linguistic gaps: the lack of resources for its diverse low-resource languages (LRLs). In this study, we investigate whether enforcing cross-lingual similarity in specific internal layers of a decoder-only multilingual large language model (LLM) can improve translation quality from LRL to high-resource language (HRL). Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric that encourages representations of different languages to align, with REPINA, a regularization method that constrains parameter updates to remain close to the pretrained model, into a joint method we call TRepLiNa. In this research project, we experiment with zero-shot, few-shot, and fine-tuning settings using Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari, Santali, Bhili) with Hindi/English pivots. Our results show that aligning mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach to improving LRL translation, especially in data-scarce settings.", "link": "http://arxiv.org/abs/2510.06249v5", "date": "2025-12-10", "relevancy": 2.4287, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5168}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRepLiNa%3A%20Layer-wise%20CKA%2BREPINA%20Alignment%20Improves%20Low-Resource%20Machine%20Translation%20in%20Aya-23%208B&body=Title%3A%20TRepLiNa%3A%20Layer-wise%20CKA%2BREPINA%20Alignment%20Improves%20Low-Resource%20Machine%20Translation%20in%20Aya-23%208B%0AAuthor%3A%20Toshiki%20Nakai%20and%20Ravi%20Kiran%20Chikkala%20and%20Lena%20Sophie%20Oberkircher%20and%20Nicholas%20Jennings%20and%20Natalia%20Skachkova%20and%20Tatiana%20Anikina%20and%20Jesujoba%20Oluwadara%20Alabi%0AAbstract%3A%20The%202025%20Multimodal%20Models%20for%20Low-Resource%20Contexts%20and%20Social%20Impact%20%28MMLoSo%29%20Language%20Challenge%20addresses%20one%20of%20India%27s%20most%20pressing%20linguistic%20gaps%3A%20the%20lack%20of%20resources%20for%20its%20diverse%20low-resource%20languages%20%28LRLs%29.%20In%20this%20study%2C%20we%20investigate%20whether%20enforcing%20cross-lingual%20similarity%20in%20specific%20internal%20layers%20of%20a%20decoder-only%20multilingual%20large%20language%20model%20%28LLM%29%20can%20improve%20translation%20quality%20from%20LRL%20to%20high-resource%20language%20%28HRL%29.%20Specifically%2C%20we%20combine%20Centered%20Kernel%20Alignment%20%28CKA%29%2C%20a%20similarity%20metric%20that%20encourages%20representations%20of%20different%20languages%20to%20align%2C%20with%20REPINA%2C%20a%20regularization%20method%20that%20constrains%20parameter%20updates%20to%20remain%20close%20to%20the%20pretrained%20model%2C%20into%20a%20joint%20method%20we%20call%20TRepLiNa.%20In%20this%20research%20project%2C%20we%20experiment%20with%20zero-shot%2C%20few-shot%2C%20and%20fine-tuning%20settings%20using%20Aya-23%208B%20with%20QLoRA%20across%20MMLoSo%20shared%20task%20language%20pairs%20%28Mundari%2C%20Santali%2C%20Bhili%29%20with%20Hindi/English%20pivots.%20Our%20results%20show%20that%20aligning%20mid-level%20layers%20using%20TRepLiNa%20%28CKA%2BREPINA%29%20is%20a%20low-cost%2C%20practical%20approach%20to%20improving%20LRL%20translation%2C%20especially%20in%20data-scarce%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2510.06249v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRepLiNa%253A%2520Layer-wise%2520CKA%252BREPINA%2520Alignment%2520Improves%2520Low-Resource%2520Machine%2520Translation%2520in%2520Aya-23%25208B%26entry.906535625%3DToshiki%2520Nakai%2520and%2520Ravi%2520Kiran%2520Chikkala%2520and%2520Lena%2520Sophie%2520Oberkircher%2520and%2520Nicholas%2520Jennings%2520and%2520Natalia%2520Skachkova%2520and%2520Tatiana%2520Anikina%2520and%2520Jesujoba%2520Oluwadara%2520Alabi%26entry.1292438233%3DThe%25202025%2520Multimodal%2520Models%2520for%2520Low-Resource%2520Contexts%2520and%2520Social%2520Impact%2520%2528MMLoSo%2529%2520Language%2520Challenge%2520addresses%2520one%2520of%2520India%2527s%2520most%2520pressing%2520linguistic%2520gaps%253A%2520the%2520lack%2520of%2520resources%2520for%2520its%2520diverse%2520low-resource%2520languages%2520%2528LRLs%2529.%2520In%2520this%2520study%252C%2520we%2520investigate%2520whether%2520enforcing%2520cross-lingual%2520similarity%2520in%2520specific%2520internal%2520layers%2520of%2520a%2520decoder-only%2520multilingual%2520large%2520language%2520model%2520%2528LLM%2529%2520can%2520improve%2520translation%2520quality%2520from%2520LRL%2520to%2520high-resource%2520language%2520%2528HRL%2529.%2520Specifically%252C%2520we%2520combine%2520Centered%2520Kernel%2520Alignment%2520%2528CKA%2529%252C%2520a%2520similarity%2520metric%2520that%2520encourages%2520representations%2520of%2520different%2520languages%2520to%2520align%252C%2520with%2520REPINA%252C%2520a%2520regularization%2520method%2520that%2520constrains%2520parameter%2520updates%2520to%2520remain%2520close%2520to%2520the%2520pretrained%2520model%252C%2520into%2520a%2520joint%2520method%2520we%2520call%2520TRepLiNa.%2520In%2520this%2520research%2520project%252C%2520we%2520experiment%2520with%2520zero-shot%252C%2520few-shot%252C%2520and%2520fine-tuning%2520settings%2520using%2520Aya-23%25208B%2520with%2520QLoRA%2520across%2520MMLoSo%2520shared%2520task%2520language%2520pairs%2520%2528Mundari%252C%2520Santali%252C%2520Bhili%2529%2520with%2520Hindi/English%2520pivots.%2520Our%2520results%2520show%2520that%2520aligning%2520mid-level%2520layers%2520using%2520TRepLiNa%2520%2528CKA%252BREPINA%2529%2520is%2520a%2520low-cost%252C%2520practical%2520approach%2520to%2520improving%2520LRL%2520translation%252C%2520especially%2520in%2520data-scarce%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06249v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRepLiNa%3A%20Layer-wise%20CKA%2BREPINA%20Alignment%20Improves%20Low-Resource%20Machine%20Translation%20in%20Aya-23%208B&entry.906535625=Toshiki%20Nakai%20and%20Ravi%20Kiran%20Chikkala%20and%20Lena%20Sophie%20Oberkircher%20and%20Nicholas%20Jennings%20and%20Natalia%20Skachkova%20and%20Tatiana%20Anikina%20and%20Jesujoba%20Oluwadara%20Alabi&entry.1292438233=The%202025%20Multimodal%20Models%20for%20Low-Resource%20Contexts%20and%20Social%20Impact%20%28MMLoSo%29%20Language%20Challenge%20addresses%20one%20of%20India%27s%20most%20pressing%20linguistic%20gaps%3A%20the%20lack%20of%20resources%20for%20its%20diverse%20low-resource%20languages%20%28LRLs%29.%20In%20this%20study%2C%20we%20investigate%20whether%20enforcing%20cross-lingual%20similarity%20in%20specific%20internal%20layers%20of%20a%20decoder-only%20multilingual%20large%20language%20model%20%28LLM%29%20can%20improve%20translation%20quality%20from%20LRL%20to%20high-resource%20language%20%28HRL%29.%20Specifically%2C%20we%20combine%20Centered%20Kernel%20Alignment%20%28CKA%29%2C%20a%20similarity%20metric%20that%20encourages%20representations%20of%20different%20languages%20to%20align%2C%20with%20REPINA%2C%20a%20regularization%20method%20that%20constrains%20parameter%20updates%20to%20remain%20close%20to%20the%20pretrained%20model%2C%20into%20a%20joint%20method%20we%20call%20TRepLiNa.%20In%20this%20research%20project%2C%20we%20experiment%20with%20zero-shot%2C%20few-shot%2C%20and%20fine-tuning%20settings%20using%20Aya-23%208B%20with%20QLoRA%20across%20MMLoSo%20shared%20task%20language%20pairs%20%28Mundari%2C%20Santali%2C%20Bhili%29%20with%20Hindi/English%20pivots.%20Our%20results%20show%20that%20aligning%20mid-level%20layers%20using%20TRepLiNa%20%28CKA%2BREPINA%29%20is%20a%20low-cost%2C%20practical%20approach%20to%20improving%20LRL%20translation%2C%20especially%20in%20data-scarce%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2510.06249v5&entry.124074799=Read"},
{"title": "Physics-Aware Heterogeneous GNN Architecture for Real-Time BESS Optimization in Unbalanced Distribution Systems", "author": "Aoxiang Ma and Salah Ghamizi and Jun Cao and Pedro Rodriguez", "abstract": "Battery energy storage systems (BESS) have become increasingly vital in three-phase unbalanced distribution grids for maintaining voltage stability and enabling optimal dispatch. However, existing deep learning approaches often lack explicit three-phase representation, making it difficult to accurately model phase-specific dynamics and enforce operational constraints--leading to infeasible dispatch solutions. This paper demonstrates that by embedding detailed three-phase grid information--including phase voltages, unbalanced loads, and BESS states--into heterogeneous graph nodes, diverse GNN architectures (GCN, GAT, GraphSAGE, GPS) can jointly predict network state variables with high accuracy. Moreover, a physics-informed loss function incorporates critical battery constraints--SoC and C-rate limits--via soft penalties during training. Experimental validation on the CIGRE 18-bus distribution system shows that this embedding-loss approach achieves low prediction errors, with bus voltage MSEs of 6.92e-07 (GCN), 1.21e-06 (GAT), 3.29e-05 (GPS), and 9.04e-07 (SAGE). Importantly, the physics-informed method ensures nearly zero SoC and C-rate constraint violations, confirming its effectiveness for reliable, constraint-compliant dispatch.", "link": "http://arxiv.org/abs/2512.09780v1", "date": "2025-12-10", "relevancy": 2.4202, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5438}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4577}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Aware%20Heterogeneous%20GNN%20Architecture%20for%20Real-Time%20BESS%20Optimization%20in%20Unbalanced%20Distribution%20Systems&body=Title%3A%20Physics-Aware%20Heterogeneous%20GNN%20Architecture%20for%20Real-Time%20BESS%20Optimization%20in%20Unbalanced%20Distribution%20Systems%0AAuthor%3A%20Aoxiang%20Ma%20and%20Salah%20Ghamizi%20and%20Jun%20Cao%20and%20Pedro%20Rodriguez%0AAbstract%3A%20Battery%20energy%20storage%20systems%20%28BESS%29%20have%20become%20increasingly%20vital%20in%20three-phase%20unbalanced%20distribution%20grids%20for%20maintaining%20voltage%20stability%20and%20enabling%20optimal%20dispatch.%20However%2C%20existing%20deep%20learning%20approaches%20often%20lack%20explicit%20three-phase%20representation%2C%20making%20it%20difficult%20to%20accurately%20model%20phase-specific%20dynamics%20and%20enforce%20operational%20constraints--leading%20to%20infeasible%20dispatch%20solutions.%20This%20paper%20demonstrates%20that%20by%20embedding%20detailed%20three-phase%20grid%20information--including%20phase%20voltages%2C%20unbalanced%20loads%2C%20and%20BESS%20states--into%20heterogeneous%20graph%20nodes%2C%20diverse%20GNN%20architectures%20%28GCN%2C%20GAT%2C%20GraphSAGE%2C%20GPS%29%20can%20jointly%20predict%20network%20state%20variables%20with%20high%20accuracy.%20Moreover%2C%20a%20physics-informed%20loss%20function%20incorporates%20critical%20battery%20constraints--SoC%20and%20C-rate%20limits--via%20soft%20penalties%20during%20training.%20Experimental%20validation%20on%20the%20CIGRE%2018-bus%20distribution%20system%20shows%20that%20this%20embedding-loss%20approach%20achieves%20low%20prediction%20errors%2C%20with%20bus%20voltage%20MSEs%20of%206.92e-07%20%28GCN%29%2C%201.21e-06%20%28GAT%29%2C%203.29e-05%20%28GPS%29%2C%20and%209.04e-07%20%28SAGE%29.%20Importantly%2C%20the%20physics-informed%20method%20ensures%20nearly%20zero%20SoC%20and%20C-rate%20constraint%20violations%2C%20confirming%20its%20effectiveness%20for%20reliable%2C%20constraint-compliant%20dispatch.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Aware%2520Heterogeneous%2520GNN%2520Architecture%2520for%2520Real-Time%2520BESS%2520Optimization%2520in%2520Unbalanced%2520Distribution%2520Systems%26entry.906535625%3DAoxiang%2520Ma%2520and%2520Salah%2520Ghamizi%2520and%2520Jun%2520Cao%2520and%2520Pedro%2520Rodriguez%26entry.1292438233%3DBattery%2520energy%2520storage%2520systems%2520%2528BESS%2529%2520have%2520become%2520increasingly%2520vital%2520in%2520three-phase%2520unbalanced%2520distribution%2520grids%2520for%2520maintaining%2520voltage%2520stability%2520and%2520enabling%2520optimal%2520dispatch.%2520However%252C%2520existing%2520deep%2520learning%2520approaches%2520often%2520lack%2520explicit%2520three-phase%2520representation%252C%2520making%2520it%2520difficult%2520to%2520accurately%2520model%2520phase-specific%2520dynamics%2520and%2520enforce%2520operational%2520constraints--leading%2520to%2520infeasible%2520dispatch%2520solutions.%2520This%2520paper%2520demonstrates%2520that%2520by%2520embedding%2520detailed%2520three-phase%2520grid%2520information--including%2520phase%2520voltages%252C%2520unbalanced%2520loads%252C%2520and%2520BESS%2520states--into%2520heterogeneous%2520graph%2520nodes%252C%2520diverse%2520GNN%2520architectures%2520%2528GCN%252C%2520GAT%252C%2520GraphSAGE%252C%2520GPS%2529%2520can%2520jointly%2520predict%2520network%2520state%2520variables%2520with%2520high%2520accuracy.%2520Moreover%252C%2520a%2520physics-informed%2520loss%2520function%2520incorporates%2520critical%2520battery%2520constraints--SoC%2520and%2520C-rate%2520limits--via%2520soft%2520penalties%2520during%2520training.%2520Experimental%2520validation%2520on%2520the%2520CIGRE%252018-bus%2520distribution%2520system%2520shows%2520that%2520this%2520embedding-loss%2520approach%2520achieves%2520low%2520prediction%2520errors%252C%2520with%2520bus%2520voltage%2520MSEs%2520of%25206.92e-07%2520%2528GCN%2529%252C%25201.21e-06%2520%2528GAT%2529%252C%25203.29e-05%2520%2528GPS%2529%252C%2520and%25209.04e-07%2520%2528SAGE%2529.%2520Importantly%252C%2520the%2520physics-informed%2520method%2520ensures%2520nearly%2520zero%2520SoC%2520and%2520C-rate%2520constraint%2520violations%252C%2520confirming%2520its%2520effectiveness%2520for%2520reliable%252C%2520constraint-compliant%2520dispatch.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Aware%20Heterogeneous%20GNN%20Architecture%20for%20Real-Time%20BESS%20Optimization%20in%20Unbalanced%20Distribution%20Systems&entry.906535625=Aoxiang%20Ma%20and%20Salah%20Ghamizi%20and%20Jun%20Cao%20and%20Pedro%20Rodriguez&entry.1292438233=Battery%20energy%20storage%20systems%20%28BESS%29%20have%20become%20increasingly%20vital%20in%20three-phase%20unbalanced%20distribution%20grids%20for%20maintaining%20voltage%20stability%20and%20enabling%20optimal%20dispatch.%20However%2C%20existing%20deep%20learning%20approaches%20often%20lack%20explicit%20three-phase%20representation%2C%20making%20it%20difficult%20to%20accurately%20model%20phase-specific%20dynamics%20and%20enforce%20operational%20constraints--leading%20to%20infeasible%20dispatch%20solutions.%20This%20paper%20demonstrates%20that%20by%20embedding%20detailed%20three-phase%20grid%20information--including%20phase%20voltages%2C%20unbalanced%20loads%2C%20and%20BESS%20states--into%20heterogeneous%20graph%20nodes%2C%20diverse%20GNN%20architectures%20%28GCN%2C%20GAT%2C%20GraphSAGE%2C%20GPS%29%20can%20jointly%20predict%20network%20state%20variables%20with%20high%20accuracy.%20Moreover%2C%20a%20physics-informed%20loss%20function%20incorporates%20critical%20battery%20constraints--SoC%20and%20C-rate%20limits--via%20soft%20penalties%20during%20training.%20Experimental%20validation%20on%20the%20CIGRE%2018-bus%20distribution%20system%20shows%20that%20this%20embedding-loss%20approach%20achieves%20low%20prediction%20errors%2C%20with%20bus%20voltage%20MSEs%20of%206.92e-07%20%28GCN%29%2C%201.21e-06%20%28GAT%29%2C%203.29e-05%20%28GPS%29%2C%20and%209.04e-07%20%28SAGE%29.%20Importantly%2C%20the%20physics-informed%20method%20ensures%20nearly%20zero%20SoC%20and%20C-rate%20constraint%20violations%2C%20confirming%20its%20effectiveness%20for%20reliable%2C%20constraint-compliant%20dispatch.&entry.1838667208=http%3A//arxiv.org/abs/2512.09780v1&entry.124074799=Read"},
{"title": "Mixture of Lookup Key-Value Experts", "author": "Zongcheng Wang", "abstract": "Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \\textbf{M}ixture \\textbf{o}f \\textbf{L}ookup \\textbf{K}ey-\\textbf{V}alue Experts (\\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations.", "link": "http://arxiv.org/abs/2512.09723v1", "date": "2025-12-10", "relevancy": 2.4198, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4861}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4829}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture%20of%20Lookup%20Key-Value%20Experts&body=Title%3A%20Mixture%20of%20Lookup%20Key-Value%20Experts%0AAuthor%3A%20Zongcheng%20Wang%0AAbstract%3A%20Recent%20research%20has%20developed%20several%20LLM%20architectures%20suitable%20for%20inference%20on%20end-user%20devices%2C%20such%20as%20the%20Mixture%20of%20Lookup%20Experts%20%28MoLE%29~%5Cparencite%7Bjie_mixture_2025%7D.%20A%20key%20feature%20of%20MoLE%20is%20that%20each%20token%20id%20is%20associated%20with%20a%20dedicated%20group%20of%20experts.%20For%20a%20given%20input%2C%20only%20the%20experts%20corresponding%20to%20the%20input%20token%20id%20will%20be%20activated.%20Since%20the%20communication%20overhead%20of%20loading%20this%20small%20number%20of%20activated%20experts%20into%20RAM%20during%20inference%20is%20negligible%2C%20expert%20parameters%20can%20be%20offloaded%20to%20storage%2C%20making%20MoLE%20suitable%20for%20resource-constrained%20devices.%20However%2C%20MoLE%27s%20context-independent%20expert%20selection%20mechanism%2C%20based%20solely%20on%20input%20ids%2C%20may%20limit%20model%20performance.%20To%20address%20this%2C%20we%20propose%20the%20%5Ctextbf%7BM%7Dixture%20%5Ctextbf%7Bo%7Df%20%5Ctextbf%7BL%7Dookup%20%5Ctextbf%7BK%7Dey-%5Ctextbf%7BV%7Dalue%20Experts%20%28%5Ctextbf%7BMoLKV%7D%29%20model.%20In%20MoLKV%2C%20each%20expert%20is%20structured%20as%20a%20key-value%20pair.%20For%20a%20given%20input%2C%20the%20input-derived%20query%20interacts%20with%20the%20cached%20key-value%20experts%20from%20the%20current%20sequence%2C%20generating%20a%20context-aware%20expert%20output.%20This%20context-aware%20mechanism%20alleviates%20the%20limitation%20of%20MoLE%2C%20and%20experimental%20results%20demonstrate%20that%20MoLKV%20achieves%20significantly%20lower%20validation%20loss%20in%20small-scale%20evaluations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture%2520of%2520Lookup%2520Key-Value%2520Experts%26entry.906535625%3DZongcheng%2520Wang%26entry.1292438233%3DRecent%2520research%2520has%2520developed%2520several%2520LLM%2520architectures%2520suitable%2520for%2520inference%2520on%2520end-user%2520devices%252C%2520such%2520as%2520the%2520Mixture%2520of%2520Lookup%2520Experts%2520%2528MoLE%2529~%255Cparencite%257Bjie_mixture_2025%257D.%2520A%2520key%2520feature%2520of%2520MoLE%2520is%2520that%2520each%2520token%2520id%2520is%2520associated%2520with%2520a%2520dedicated%2520group%2520of%2520experts.%2520For%2520a%2520given%2520input%252C%2520only%2520the%2520experts%2520corresponding%2520to%2520the%2520input%2520token%2520id%2520will%2520be%2520activated.%2520Since%2520the%2520communication%2520overhead%2520of%2520loading%2520this%2520small%2520number%2520of%2520activated%2520experts%2520into%2520RAM%2520during%2520inference%2520is%2520negligible%252C%2520expert%2520parameters%2520can%2520be%2520offloaded%2520to%2520storage%252C%2520making%2520MoLE%2520suitable%2520for%2520resource-constrained%2520devices.%2520However%252C%2520MoLE%2527s%2520context-independent%2520expert%2520selection%2520mechanism%252C%2520based%2520solely%2520on%2520input%2520ids%252C%2520may%2520limit%2520model%2520performance.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520%255Ctextbf%257BM%257Dixture%2520%255Ctextbf%257Bo%257Df%2520%255Ctextbf%257BL%257Dookup%2520%255Ctextbf%257BK%257Dey-%255Ctextbf%257BV%257Dalue%2520Experts%2520%2528%255Ctextbf%257BMoLKV%257D%2529%2520model.%2520In%2520MoLKV%252C%2520each%2520expert%2520is%2520structured%2520as%2520a%2520key-value%2520pair.%2520For%2520a%2520given%2520input%252C%2520the%2520input-derived%2520query%2520interacts%2520with%2520the%2520cached%2520key-value%2520experts%2520from%2520the%2520current%2520sequence%252C%2520generating%2520a%2520context-aware%2520expert%2520output.%2520This%2520context-aware%2520mechanism%2520alleviates%2520the%2520limitation%2520of%2520MoLE%252C%2520and%2520experimental%2520results%2520demonstrate%2520that%2520MoLKV%2520achieves%2520significantly%2520lower%2520validation%2520loss%2520in%2520small-scale%2520evaluations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture%20of%20Lookup%20Key-Value%20Experts&entry.906535625=Zongcheng%20Wang&entry.1292438233=Recent%20research%20has%20developed%20several%20LLM%20architectures%20suitable%20for%20inference%20on%20end-user%20devices%2C%20such%20as%20the%20Mixture%20of%20Lookup%20Experts%20%28MoLE%29~%5Cparencite%7Bjie_mixture_2025%7D.%20A%20key%20feature%20of%20MoLE%20is%20that%20each%20token%20id%20is%20associated%20with%20a%20dedicated%20group%20of%20experts.%20For%20a%20given%20input%2C%20only%20the%20experts%20corresponding%20to%20the%20input%20token%20id%20will%20be%20activated.%20Since%20the%20communication%20overhead%20of%20loading%20this%20small%20number%20of%20activated%20experts%20into%20RAM%20during%20inference%20is%20negligible%2C%20expert%20parameters%20can%20be%20offloaded%20to%20storage%2C%20making%20MoLE%20suitable%20for%20resource-constrained%20devices.%20However%2C%20MoLE%27s%20context-independent%20expert%20selection%20mechanism%2C%20based%20solely%20on%20input%20ids%2C%20may%20limit%20model%20performance.%20To%20address%20this%2C%20we%20propose%20the%20%5Ctextbf%7BM%7Dixture%20%5Ctextbf%7Bo%7Df%20%5Ctextbf%7BL%7Dookup%20%5Ctextbf%7BK%7Dey-%5Ctextbf%7BV%7Dalue%20Experts%20%28%5Ctextbf%7BMoLKV%7D%29%20model.%20In%20MoLKV%2C%20each%20expert%20is%20structured%20as%20a%20key-value%20pair.%20For%20a%20given%20input%2C%20the%20input-derived%20query%20interacts%20with%20the%20cached%20key-value%20experts%20from%20the%20current%20sequence%2C%20generating%20a%20context-aware%20expert%20output.%20This%20context-aware%20mechanism%20alleviates%20the%20limitation%20of%20MoLE%2C%20and%20experimental%20results%20demonstrate%20that%20MoLKV%20achieves%20significantly%20lower%20validation%20loss%20in%20small-scale%20evaluations.&entry.1838667208=http%3A//arxiv.org/abs/2512.09723v1&entry.124074799=Read"},
{"title": "Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning", "author": "Kaichen He and Zihao Wang and Muyao Li and Anji Liu and Yitao Liang", "abstract": "The paradigm of agentic AI is shifting from engineered complex workflows to post-training native models. However, existing agents are typically confined to static, predefined action spaces--such as exclusively using APIs, GUI events, or robotic commands. This rigidity limits their adaptability in dynamic environments where the optimal granularity of interaction varies contextually. To bridge this gap, we propose CrossAgent, a unified agentic model that masters heterogeneous action spaces and autonomously selects the most effective interface for each step of a trajectory. We introduce a comprehensive training pipeline that integrates cold-start supervised fine-tuning with a Multi-Turn Group Relative Policy Optimization (GRPO) algorithm. This approach enables the agent to learn adaptive action switching--balancing high-level efficiency with low-level precision--without human-specified rules. Extensive experiments on over 800 tasks in the open-world Minecraft environment demonstrate that CrossAgent achieves state-of-the-art performance. By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning. All code and models are available at https://github.com/CraftJarvis/OpenHA", "link": "http://arxiv.org/abs/2512.09706v1", "date": "2025-12-10", "relevancy": 2.4125, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.6278}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5884}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20One%20Model%20to%20Master%20Cross-Level%20Agentic%20Actions%20via%20Reinforcement%20Learning&body=Title%3A%20Training%20One%20Model%20to%20Master%20Cross-Level%20Agentic%20Actions%20via%20Reinforcement%20Learning%0AAuthor%3A%20Kaichen%20He%20and%20Zihao%20Wang%20and%20Muyao%20Li%20and%20Anji%20Liu%20and%20Yitao%20Liang%0AAbstract%3A%20The%20paradigm%20of%20agentic%20AI%20is%20shifting%20from%20engineered%20complex%20workflows%20to%20post-training%20native%20models.%20However%2C%20existing%20agents%20are%20typically%20confined%20to%20static%2C%20predefined%20action%20spaces--such%20as%20exclusively%20using%20APIs%2C%20GUI%20events%2C%20or%20robotic%20commands.%20This%20rigidity%20limits%20their%20adaptability%20in%20dynamic%20environments%20where%20the%20optimal%20granularity%20of%20interaction%20varies%20contextually.%20To%20bridge%20this%20gap%2C%20we%20propose%20CrossAgent%2C%20a%20unified%20agentic%20model%20that%20masters%20heterogeneous%20action%20spaces%20and%20autonomously%20selects%20the%20most%20effective%20interface%20for%20each%20step%20of%20a%20trajectory.%20We%20introduce%20a%20comprehensive%20training%20pipeline%20that%20integrates%20cold-start%20supervised%20fine-tuning%20with%20a%20Multi-Turn%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20algorithm.%20This%20approach%20enables%20the%20agent%20to%20learn%20adaptive%20action%20switching--balancing%20high-level%20efficiency%20with%20low-level%20precision--without%20human-specified%20rules.%20Extensive%20experiments%20on%20over%20800%20tasks%20in%20the%20open-world%20Minecraft%20environment%20demonstrate%20that%20CrossAgent%20achieves%20state-of-the-art%20performance.%20By%20dynamically%20leveraging%20the%20strengths%20of%20diverse%20action%20spaces%2C%20our%20model%20significantly%20outperforms%20fixed-action%20baselines%2C%20exhibiting%20superior%20generalization%20and%20efficiency%20in%20long-horizon%20reasoning.%20All%20code%20and%20models%20are%20available%20at%20https%3A//github.com/CraftJarvis/OpenHA%0ALink%3A%20http%3A//arxiv.org/abs/2512.09706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520One%2520Model%2520to%2520Master%2520Cross-Level%2520Agentic%2520Actions%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DKaichen%2520He%2520and%2520Zihao%2520Wang%2520and%2520Muyao%2520Li%2520and%2520Anji%2520Liu%2520and%2520Yitao%2520Liang%26entry.1292438233%3DThe%2520paradigm%2520of%2520agentic%2520AI%2520is%2520shifting%2520from%2520engineered%2520complex%2520workflows%2520to%2520post-training%2520native%2520models.%2520However%252C%2520existing%2520agents%2520are%2520typically%2520confined%2520to%2520static%252C%2520predefined%2520action%2520spaces--such%2520as%2520exclusively%2520using%2520APIs%252C%2520GUI%2520events%252C%2520or%2520robotic%2520commands.%2520This%2520rigidity%2520limits%2520their%2520adaptability%2520in%2520dynamic%2520environments%2520where%2520the%2520optimal%2520granularity%2520of%2520interaction%2520varies%2520contextually.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520CrossAgent%252C%2520a%2520unified%2520agentic%2520model%2520that%2520masters%2520heterogeneous%2520action%2520spaces%2520and%2520autonomously%2520selects%2520the%2520most%2520effective%2520interface%2520for%2520each%2520step%2520of%2520a%2520trajectory.%2520We%2520introduce%2520a%2520comprehensive%2520training%2520pipeline%2520that%2520integrates%2520cold-start%2520supervised%2520fine-tuning%2520with%2520a%2520Multi-Turn%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520algorithm.%2520This%2520approach%2520enables%2520the%2520agent%2520to%2520learn%2520adaptive%2520action%2520switching--balancing%2520high-level%2520efficiency%2520with%2520low-level%2520precision--without%2520human-specified%2520rules.%2520Extensive%2520experiments%2520on%2520over%2520800%2520tasks%2520in%2520the%2520open-world%2520Minecraft%2520environment%2520demonstrate%2520that%2520CrossAgent%2520achieves%2520state-of-the-art%2520performance.%2520By%2520dynamically%2520leveraging%2520the%2520strengths%2520of%2520diverse%2520action%2520spaces%252C%2520our%2520model%2520significantly%2520outperforms%2520fixed-action%2520baselines%252C%2520exhibiting%2520superior%2520generalization%2520and%2520efficiency%2520in%2520long-horizon%2520reasoning.%2520All%2520code%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/CraftJarvis/OpenHA%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20One%20Model%20to%20Master%20Cross-Level%20Agentic%20Actions%20via%20Reinforcement%20Learning&entry.906535625=Kaichen%20He%20and%20Zihao%20Wang%20and%20Muyao%20Li%20and%20Anji%20Liu%20and%20Yitao%20Liang&entry.1292438233=The%20paradigm%20of%20agentic%20AI%20is%20shifting%20from%20engineered%20complex%20workflows%20to%20post-training%20native%20models.%20However%2C%20existing%20agents%20are%20typically%20confined%20to%20static%2C%20predefined%20action%20spaces--such%20as%20exclusively%20using%20APIs%2C%20GUI%20events%2C%20or%20robotic%20commands.%20This%20rigidity%20limits%20their%20adaptability%20in%20dynamic%20environments%20where%20the%20optimal%20granularity%20of%20interaction%20varies%20contextually.%20To%20bridge%20this%20gap%2C%20we%20propose%20CrossAgent%2C%20a%20unified%20agentic%20model%20that%20masters%20heterogeneous%20action%20spaces%20and%20autonomously%20selects%20the%20most%20effective%20interface%20for%20each%20step%20of%20a%20trajectory.%20We%20introduce%20a%20comprehensive%20training%20pipeline%20that%20integrates%20cold-start%20supervised%20fine-tuning%20with%20a%20Multi-Turn%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20algorithm.%20This%20approach%20enables%20the%20agent%20to%20learn%20adaptive%20action%20switching--balancing%20high-level%20efficiency%20with%20low-level%20precision--without%20human-specified%20rules.%20Extensive%20experiments%20on%20over%20800%20tasks%20in%20the%20open-world%20Minecraft%20environment%20demonstrate%20that%20CrossAgent%20achieves%20state-of-the-art%20performance.%20By%20dynamically%20leveraging%20the%20strengths%20of%20diverse%20action%20spaces%2C%20our%20model%20significantly%20outperforms%20fixed-action%20baselines%2C%20exhibiting%20superior%20generalization%20and%20efficiency%20in%20long-horizon%20reasoning.%20All%20code%20and%20models%20are%20available%20at%20https%3A//github.com/CraftJarvis/OpenHA&entry.1838667208=http%3A//arxiv.org/abs/2512.09706v1&entry.124074799=Read"},
{"title": "ReMoSPLAT: Reactive Mobile Manipulation Control on a Gaussian Splat", "author": "Nicolas Marticorena and Tobias Fischer and Niko Suenderhauf", "abstract": "Reactive control can gracefully coordinate the motion of the base and the arm of a mobile manipulator. However, incorporating an accurate representation of the environment to avoid obstacles without involving costly planning remains a challenge. In this work, we present ReMoSPLAT, a reactive controller based on a quadratic program formulation for mobile manipulation that leverages a Gaussian Splat representation for collision avoidance. By integrating additional constraints and costs into the optimisation formulation, a mobile manipulator platform can reach its intended end effector pose while avoiding obstacles, even in cluttered scenes. We investigate the trade-offs of two methods for efficiently calculating robot-obstacle distances, comparing a purely geometric approach with a rasterisation-based approach. Our experiments in simulation on both synthetic and real-world scans demonstrate the feasibility of our method, showing that the proposed approach achieves performance comparable to controllers that rely on perfect ground-truth information.", "link": "http://arxiv.org/abs/2512.09656v1", "date": "2025-12-10", "relevancy": 2.4005, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6518}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5655}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReMoSPLAT%3A%20Reactive%20Mobile%20Manipulation%20Control%20on%20a%20Gaussian%20Splat&body=Title%3A%20ReMoSPLAT%3A%20Reactive%20Mobile%20Manipulation%20Control%20on%20a%20Gaussian%20Splat%0AAuthor%3A%20Nicolas%20Marticorena%20and%20Tobias%20Fischer%20and%20Niko%20Suenderhauf%0AAbstract%3A%20Reactive%20control%20can%20gracefully%20coordinate%20the%20motion%20of%20the%20base%20and%20the%20arm%20of%20a%20mobile%20manipulator.%20However%2C%20incorporating%20an%20accurate%20representation%20of%20the%20environment%20to%20avoid%20obstacles%20without%20involving%20costly%20planning%20remains%20a%20challenge.%20In%20this%20work%2C%20we%20present%20ReMoSPLAT%2C%20a%20reactive%20controller%20based%20on%20a%20quadratic%20program%20formulation%20for%20mobile%20manipulation%20that%20leverages%20a%20Gaussian%20Splat%20representation%20for%20collision%20avoidance.%20By%20integrating%20additional%20constraints%20and%20costs%20into%20the%20optimisation%20formulation%2C%20a%20mobile%20manipulator%20platform%20can%20reach%20its%20intended%20end%20effector%20pose%20while%20avoiding%20obstacles%2C%20even%20in%20cluttered%20scenes.%20We%20investigate%20the%20trade-offs%20of%20two%20methods%20for%20efficiently%20calculating%20robot-obstacle%20distances%2C%20comparing%20a%20purely%20geometric%20approach%20with%20a%20rasterisation-based%20approach.%20Our%20experiments%20in%20simulation%20on%20both%20synthetic%20and%20real-world%20scans%20demonstrate%20the%20feasibility%20of%20our%20method%2C%20showing%20that%20the%20proposed%20approach%20achieves%20performance%20comparable%20to%20controllers%20that%20rely%20on%20perfect%20ground-truth%20information.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReMoSPLAT%253A%2520Reactive%2520Mobile%2520Manipulation%2520Control%2520on%2520a%2520Gaussian%2520Splat%26entry.906535625%3DNicolas%2520Marticorena%2520and%2520Tobias%2520Fischer%2520and%2520Niko%2520Suenderhauf%26entry.1292438233%3DReactive%2520control%2520can%2520gracefully%2520coordinate%2520the%2520motion%2520of%2520the%2520base%2520and%2520the%2520arm%2520of%2520a%2520mobile%2520manipulator.%2520However%252C%2520incorporating%2520an%2520accurate%2520representation%2520of%2520the%2520environment%2520to%2520avoid%2520obstacles%2520without%2520involving%2520costly%2520planning%2520remains%2520a%2520challenge.%2520In%2520this%2520work%252C%2520we%2520present%2520ReMoSPLAT%252C%2520a%2520reactive%2520controller%2520based%2520on%2520a%2520quadratic%2520program%2520formulation%2520for%2520mobile%2520manipulation%2520that%2520leverages%2520a%2520Gaussian%2520Splat%2520representation%2520for%2520collision%2520avoidance.%2520By%2520integrating%2520additional%2520constraints%2520and%2520costs%2520into%2520the%2520optimisation%2520formulation%252C%2520a%2520mobile%2520manipulator%2520platform%2520can%2520reach%2520its%2520intended%2520end%2520effector%2520pose%2520while%2520avoiding%2520obstacles%252C%2520even%2520in%2520cluttered%2520scenes.%2520We%2520investigate%2520the%2520trade-offs%2520of%2520two%2520methods%2520for%2520efficiently%2520calculating%2520robot-obstacle%2520distances%252C%2520comparing%2520a%2520purely%2520geometric%2520approach%2520with%2520a%2520rasterisation-based%2520approach.%2520Our%2520experiments%2520in%2520simulation%2520on%2520both%2520synthetic%2520and%2520real-world%2520scans%2520demonstrate%2520the%2520feasibility%2520of%2520our%2520method%252C%2520showing%2520that%2520the%2520proposed%2520approach%2520achieves%2520performance%2520comparable%2520to%2520controllers%2520that%2520rely%2520on%2520perfect%2520ground-truth%2520information.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReMoSPLAT%3A%20Reactive%20Mobile%20Manipulation%20Control%20on%20a%20Gaussian%20Splat&entry.906535625=Nicolas%20Marticorena%20and%20Tobias%20Fischer%20and%20Niko%20Suenderhauf&entry.1292438233=Reactive%20control%20can%20gracefully%20coordinate%20the%20motion%20of%20the%20base%20and%20the%20arm%20of%20a%20mobile%20manipulator.%20However%2C%20incorporating%20an%20accurate%20representation%20of%20the%20environment%20to%20avoid%20obstacles%20without%20involving%20costly%20planning%20remains%20a%20challenge.%20In%20this%20work%2C%20we%20present%20ReMoSPLAT%2C%20a%20reactive%20controller%20based%20on%20a%20quadratic%20program%20formulation%20for%20mobile%20manipulation%20that%20leverages%20a%20Gaussian%20Splat%20representation%20for%20collision%20avoidance.%20By%20integrating%20additional%20constraints%20and%20costs%20into%20the%20optimisation%20formulation%2C%20a%20mobile%20manipulator%20platform%20can%20reach%20its%20intended%20end%20effector%20pose%20while%20avoiding%20obstacles%2C%20even%20in%20cluttered%20scenes.%20We%20investigate%20the%20trade-offs%20of%20two%20methods%20for%20efficiently%20calculating%20robot-obstacle%20distances%2C%20comparing%20a%20purely%20geometric%20approach%20with%20a%20rasterisation-based%20approach.%20Our%20experiments%20in%20simulation%20on%20both%20synthetic%20and%20real-world%20scans%20demonstrate%20the%20feasibility%20of%20our%20method%2C%20showing%20that%20the%20proposed%20approach%20achieves%20performance%20comparable%20to%20controllers%20that%20rely%20on%20perfect%20ground-truth%20information.&entry.1838667208=http%3A//arxiv.org/abs/2512.09656v1&entry.124074799=Read"},
{"title": "Benchmarking SAM2-based Trackers on FMOX", "author": "Senem Aktas and Charles Markham and John McDonald and Rozenn Dahyot", "abstract": "Several object tracking pipelines extending Segment Anything Model 2 (SAM2) have been proposed in the past year, where the approach is to follow and segment the object from a single exemplar template provided by the user on a initialization frame. We propose to benchmark these high performing trackers (SAM2, EfficientTAM, DAM4SAM and SAMURAI) on datasets containing fast moving objects (FMO) specifically designed to be challenging for tracking approaches. The goal is to understand better current limitations in state-of-the-art trackers by providing more detailed insights on the behavior of these trackers. We show that overall the trackers DAM4SAM and SAMURAI perform well on more challenging sequences.", "link": "http://arxiv.org/abs/2512.09633v1", "date": "2025-12-10", "relevancy": 2.3972, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5011}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4719}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20SAM2-based%20Trackers%20on%20FMOX&body=Title%3A%20Benchmarking%20SAM2-based%20Trackers%20on%20FMOX%0AAuthor%3A%20Senem%20Aktas%20and%20Charles%20Markham%20and%20John%20McDonald%20and%20Rozenn%20Dahyot%0AAbstract%3A%20Several%20object%20tracking%20pipelines%20extending%20Segment%20Anything%20Model%202%20%28SAM2%29%20have%20been%20proposed%20in%20the%20past%20year%2C%20where%20the%20approach%20is%20to%20follow%20and%20segment%20the%20object%20from%20a%20single%20exemplar%20template%20provided%20by%20the%20user%20on%20a%20initialization%20frame.%20We%20propose%20to%20benchmark%20these%20high%20performing%20trackers%20%28SAM2%2C%20EfficientTAM%2C%20DAM4SAM%20and%20SAMURAI%29%20on%20datasets%20containing%20fast%20moving%20objects%20%28FMO%29%20specifically%20designed%20to%20be%20challenging%20for%20tracking%20approaches.%20The%20goal%20is%20to%20understand%20better%20current%20limitations%20in%20state-of-the-art%20trackers%20by%20providing%20more%20detailed%20insights%20on%20the%20behavior%20of%20these%20trackers.%20We%20show%20that%20overall%20the%20trackers%20DAM4SAM%20and%20SAMURAI%20perform%20well%20on%20more%20challenging%20sequences.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520SAM2-based%2520Trackers%2520on%2520FMOX%26entry.906535625%3DSenem%2520Aktas%2520and%2520Charles%2520Markham%2520and%2520John%2520McDonald%2520and%2520Rozenn%2520Dahyot%26entry.1292438233%3DSeveral%2520object%2520tracking%2520pipelines%2520extending%2520Segment%2520Anything%2520Model%25202%2520%2528SAM2%2529%2520have%2520been%2520proposed%2520in%2520the%2520past%2520year%252C%2520where%2520the%2520approach%2520is%2520to%2520follow%2520and%2520segment%2520the%2520object%2520from%2520a%2520single%2520exemplar%2520template%2520provided%2520by%2520the%2520user%2520on%2520a%2520initialization%2520frame.%2520We%2520propose%2520to%2520benchmark%2520these%2520high%2520performing%2520trackers%2520%2528SAM2%252C%2520EfficientTAM%252C%2520DAM4SAM%2520and%2520SAMURAI%2529%2520on%2520datasets%2520containing%2520fast%2520moving%2520objects%2520%2528FMO%2529%2520specifically%2520designed%2520to%2520be%2520challenging%2520for%2520tracking%2520approaches.%2520The%2520goal%2520is%2520to%2520understand%2520better%2520current%2520limitations%2520in%2520state-of-the-art%2520trackers%2520by%2520providing%2520more%2520detailed%2520insights%2520on%2520the%2520behavior%2520of%2520these%2520trackers.%2520We%2520show%2520that%2520overall%2520the%2520trackers%2520DAM4SAM%2520and%2520SAMURAI%2520perform%2520well%2520on%2520more%2520challenging%2520sequences.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20SAM2-based%20Trackers%20on%20FMOX&entry.906535625=Senem%20Aktas%20and%20Charles%20Markham%20and%20John%20McDonald%20and%20Rozenn%20Dahyot&entry.1292438233=Several%20object%20tracking%20pipelines%20extending%20Segment%20Anything%20Model%202%20%28SAM2%29%20have%20been%20proposed%20in%20the%20past%20year%2C%20where%20the%20approach%20is%20to%20follow%20and%20segment%20the%20object%20from%20a%20single%20exemplar%20template%20provided%20by%20the%20user%20on%20a%20initialization%20frame.%20We%20propose%20to%20benchmark%20these%20high%20performing%20trackers%20%28SAM2%2C%20EfficientTAM%2C%20DAM4SAM%20and%20SAMURAI%29%20on%20datasets%20containing%20fast%20moving%20objects%20%28FMO%29%20specifically%20designed%20to%20be%20challenging%20for%20tracking%20approaches.%20The%20goal%20is%20to%20understand%20better%20current%20limitations%20in%20state-of-the-art%20trackers%20by%20providing%20more%20detailed%20insights%20on%20the%20behavior%20of%20these%20trackers.%20We%20show%20that%20overall%20the%20trackers%20DAM4SAM%20and%20SAMURAI%20perform%20well%20on%20more%20challenging%20sequences.&entry.1838667208=http%3A//arxiv.org/abs/2512.09633v1&entry.124074799=Read"},
{"title": "Representation Invariance and Allocation: When Subgroup Balance Matters", "author": "Anissa Alloula and Charles Jones and Zuzanna Wakefield-Skorniewska and Francesco Quinzan and Bart\u0142omiej Papie\u017c", "abstract": "Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions.", "link": "http://arxiv.org/abs/2512.09496v1", "date": "2025-12-10", "relevancy": 2.3962, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4813}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4813}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20Invariance%20and%20Allocation%3A%20When%20Subgroup%20Balance%20Matters&body=Title%3A%20Representation%20Invariance%20and%20Allocation%3A%20When%20Subgroup%20Balance%20Matters%0AAuthor%3A%20Anissa%20Alloula%20and%20Charles%20Jones%20and%20Zuzanna%20Wakefield-Skorniewska%20and%20Francesco%20Quinzan%20and%20Bart%C5%82omiej%20Papie%C5%BC%0AAbstract%3A%20Unequal%20representation%20of%20demographic%20groups%20in%20training%20data%20poses%20challenges%20to%20model%20generalisation%20across%20populations.%20Standard%20practice%20assumes%20that%20balancing%20subgroup%20representation%20optimises%20performance.%20However%2C%20recent%20empirical%20results%20contradict%20this%20assumption%3A%20in%20some%20cases%2C%20imbalanced%20data%20distributions%20actually%20improve%20subgroup%20performance%2C%20while%20in%20others%2C%20subgroup%20performance%20remains%20unaffected%20by%20the%20absence%20of%20an%20entire%20subgroup%20during%20training.%20We%20conduct%20a%20systematic%20study%20of%20subgroup%20allocation%20across%20four%20vision%20and%20language%20models%2C%20varying%20training%20data%20composition%20to%20characterise%20the%20sensitivity%20of%20subgroup%20performance%20to%20data%20balance.%20We%20propose%20the%20latent%20separation%20hypothesis%2C%20which%20states%20that%20a%20partially%20fine-tuned%20model%27s%20dependence%20on%20subgroup%20representation%20is%20determined%20by%20the%20degree%20of%20separation%20between%20subgroups%20in%20the%20latent%20space%20of%20the%20pre-trained%20model.%20We%20formalise%20this%20hypothesis%2C%20provide%20theoretical%20analysis%2C%20and%20validate%20it%20empirically.%20Finally%2C%20we%20present%20a%20practical%20application%20to%20foundation%20model%20fine-tuning%2C%20demonstrating%20that%20quantitative%20analysis%20of%20latent%20subgroup%20separation%20can%20inform%20data%20collection%20and%20balancing%20decisions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520Invariance%2520and%2520Allocation%253A%2520When%2520Subgroup%2520Balance%2520Matters%26entry.906535625%3DAnissa%2520Alloula%2520and%2520Charles%2520Jones%2520and%2520Zuzanna%2520Wakefield-Skorniewska%2520and%2520Francesco%2520Quinzan%2520and%2520Bart%25C5%2582omiej%2520Papie%25C5%25BC%26entry.1292438233%3DUnequal%2520representation%2520of%2520demographic%2520groups%2520in%2520training%2520data%2520poses%2520challenges%2520to%2520model%2520generalisation%2520across%2520populations.%2520Standard%2520practice%2520assumes%2520that%2520balancing%2520subgroup%2520representation%2520optimises%2520performance.%2520However%252C%2520recent%2520empirical%2520results%2520contradict%2520this%2520assumption%253A%2520in%2520some%2520cases%252C%2520imbalanced%2520data%2520distributions%2520actually%2520improve%2520subgroup%2520performance%252C%2520while%2520in%2520others%252C%2520subgroup%2520performance%2520remains%2520unaffected%2520by%2520the%2520absence%2520of%2520an%2520entire%2520subgroup%2520during%2520training.%2520We%2520conduct%2520a%2520systematic%2520study%2520of%2520subgroup%2520allocation%2520across%2520four%2520vision%2520and%2520language%2520models%252C%2520varying%2520training%2520data%2520composition%2520to%2520characterise%2520the%2520sensitivity%2520of%2520subgroup%2520performance%2520to%2520data%2520balance.%2520We%2520propose%2520the%2520latent%2520separation%2520hypothesis%252C%2520which%2520states%2520that%2520a%2520partially%2520fine-tuned%2520model%2527s%2520dependence%2520on%2520subgroup%2520representation%2520is%2520determined%2520by%2520the%2520degree%2520of%2520separation%2520between%2520subgroups%2520in%2520the%2520latent%2520space%2520of%2520the%2520pre-trained%2520model.%2520We%2520formalise%2520this%2520hypothesis%252C%2520provide%2520theoretical%2520analysis%252C%2520and%2520validate%2520it%2520empirically.%2520Finally%252C%2520we%2520present%2520a%2520practical%2520application%2520to%2520foundation%2520model%2520fine-tuning%252C%2520demonstrating%2520that%2520quantitative%2520analysis%2520of%2520latent%2520subgroup%2520separation%2520can%2520inform%2520data%2520collection%2520and%2520balancing%2520decisions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20Invariance%20and%20Allocation%3A%20When%20Subgroup%20Balance%20Matters&entry.906535625=Anissa%20Alloula%20and%20Charles%20Jones%20and%20Zuzanna%20Wakefield-Skorniewska%20and%20Francesco%20Quinzan%20and%20Bart%C5%82omiej%20Papie%C5%BC&entry.1292438233=Unequal%20representation%20of%20demographic%20groups%20in%20training%20data%20poses%20challenges%20to%20model%20generalisation%20across%20populations.%20Standard%20practice%20assumes%20that%20balancing%20subgroup%20representation%20optimises%20performance.%20However%2C%20recent%20empirical%20results%20contradict%20this%20assumption%3A%20in%20some%20cases%2C%20imbalanced%20data%20distributions%20actually%20improve%20subgroup%20performance%2C%20while%20in%20others%2C%20subgroup%20performance%20remains%20unaffected%20by%20the%20absence%20of%20an%20entire%20subgroup%20during%20training.%20We%20conduct%20a%20systematic%20study%20of%20subgroup%20allocation%20across%20four%20vision%20and%20language%20models%2C%20varying%20training%20data%20composition%20to%20characterise%20the%20sensitivity%20of%20subgroup%20performance%20to%20data%20balance.%20We%20propose%20the%20latent%20separation%20hypothesis%2C%20which%20states%20that%20a%20partially%20fine-tuned%20model%27s%20dependence%20on%20subgroup%20representation%20is%20determined%20by%20the%20degree%20of%20separation%20between%20subgroups%20in%20the%20latent%20space%20of%20the%20pre-trained%20model.%20We%20formalise%20this%20hypothesis%2C%20provide%20theoretical%20analysis%2C%20and%20validate%20it%20empirically.%20Finally%2C%20we%20present%20a%20practical%20application%20to%20foundation%20model%20fine-tuning%2C%20demonstrating%20that%20quantitative%20analysis%20of%20latent%20subgroup%20separation%20can%20inform%20data%20collection%20and%20balancing%20decisions.&entry.1838667208=http%3A//arxiv.org/abs/2512.09496v1&entry.124074799=Read"},
{"title": "UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories", "author": "Yanghong Mei and Yirong Yang and Longteng Guo and Qunbo Wang and Ming-Ming Yu and Xingjian He and Wenjun Wu and Jing Liu", "abstract": "Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.", "link": "http://arxiv.org/abs/2512.09607v1", "date": "2025-12-10", "relevancy": 2.3837, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6039}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5957}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UrbanNav%3A%20Learning%20Language-Guided%20Urban%20Navigation%20from%20Web-Scale%20Human%20Trajectories&body=Title%3A%20UrbanNav%3A%20Learning%20Language-Guided%20Urban%20Navigation%20from%20Web-Scale%20Human%20Trajectories%0AAuthor%3A%20Yanghong%20Mei%20and%20Yirong%20Yang%20and%20Longteng%20Guo%20and%20Qunbo%20Wang%20and%20Ming-Ming%20Yu%20and%20Xingjian%20He%20and%20Wenjun%20Wu%20and%20Jing%20Liu%0AAbstract%3A%20Navigating%20complex%20urban%20environments%20using%20natural%20language%20instructions%20poses%20significant%20challenges%20for%20embodied%20agents%2C%20including%20noisy%20language%20instructions%2C%20ambiguous%20spatial%20references%2C%20diverse%20landmarks%2C%20and%20dynamic%20street%20scenes.%20Current%20visual%20navigation%20methods%20are%20typically%20limited%20to%20simulated%20or%20off-street%20environments%2C%20and%20often%20rely%20on%20precise%20goal%20formats%2C%20such%20as%20specific%20coordinates%20or%20images.%20This%20limits%20their%20effectiveness%20for%20autonomous%20agents%20like%20last-mile%20delivery%20robots%20navigating%20unfamiliar%20cities.%20To%20address%20these%20limitations%2C%20we%20introduce%20UrbanNav%2C%20a%20scalable%20framework%20that%20trains%20embodied%20agents%20to%20follow%20free-form%20language%20instructions%20in%20diverse%20urban%20settings.%20Leveraging%20web-scale%20city%20walking%20videos%2C%20we%20develop%20an%20scalable%20annotation%20pipeline%20that%20aligns%20human%20navigation%20trajectories%20with%20language%20instructions%20grounded%20in%20real-world%20landmarks.%20UrbanNav%20encompasses%20over%201%2C500%20hours%20of%20navigation%20data%20and%203%20million%20instruction-trajectory-landmark%20triplets%2C%20capturing%20a%20wide%20range%20of%20urban%20scenarios.%20Our%20model%20learns%20robust%20navigation%20policies%20to%20tackle%20complex%20urban%20scenarios%2C%20demonstrating%20superior%20spatial%20reasoning%2C%20robustness%20to%20noisy%20instructions%2C%20and%20generalization%20to%20unseen%20urban%20settings.%20Experimental%20results%20show%20that%20UrbanNav%20significantly%20outperforms%20existing%20methods%2C%20highlighting%20the%20potential%20of%20large-scale%20web%20video%20data%20to%20enable%20language-guided%2C%20real-world%20urban%20navigation%20for%20embodied%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrbanNav%253A%2520Learning%2520Language-Guided%2520Urban%2520Navigation%2520from%2520Web-Scale%2520Human%2520Trajectories%26entry.906535625%3DYanghong%2520Mei%2520and%2520Yirong%2520Yang%2520and%2520Longteng%2520Guo%2520and%2520Qunbo%2520Wang%2520and%2520Ming-Ming%2520Yu%2520and%2520Xingjian%2520He%2520and%2520Wenjun%2520Wu%2520and%2520Jing%2520Liu%26entry.1292438233%3DNavigating%2520complex%2520urban%2520environments%2520using%2520natural%2520language%2520instructions%2520poses%2520significant%2520challenges%2520for%2520embodied%2520agents%252C%2520including%2520noisy%2520language%2520instructions%252C%2520ambiguous%2520spatial%2520references%252C%2520diverse%2520landmarks%252C%2520and%2520dynamic%2520street%2520scenes.%2520Current%2520visual%2520navigation%2520methods%2520are%2520typically%2520limited%2520to%2520simulated%2520or%2520off-street%2520environments%252C%2520and%2520often%2520rely%2520on%2520precise%2520goal%2520formats%252C%2520such%2520as%2520specific%2520coordinates%2520or%2520images.%2520This%2520limits%2520their%2520effectiveness%2520for%2520autonomous%2520agents%2520like%2520last-mile%2520delivery%2520robots%2520navigating%2520unfamiliar%2520cities.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520UrbanNav%252C%2520a%2520scalable%2520framework%2520that%2520trains%2520embodied%2520agents%2520to%2520follow%2520free-form%2520language%2520instructions%2520in%2520diverse%2520urban%2520settings.%2520Leveraging%2520web-scale%2520city%2520walking%2520videos%252C%2520we%2520develop%2520an%2520scalable%2520annotation%2520pipeline%2520that%2520aligns%2520human%2520navigation%2520trajectories%2520with%2520language%2520instructions%2520grounded%2520in%2520real-world%2520landmarks.%2520UrbanNav%2520encompasses%2520over%25201%252C500%2520hours%2520of%2520navigation%2520data%2520and%25203%2520million%2520instruction-trajectory-landmark%2520triplets%252C%2520capturing%2520a%2520wide%2520range%2520of%2520urban%2520scenarios.%2520Our%2520model%2520learns%2520robust%2520navigation%2520policies%2520to%2520tackle%2520complex%2520urban%2520scenarios%252C%2520demonstrating%2520superior%2520spatial%2520reasoning%252C%2520robustness%2520to%2520noisy%2520instructions%252C%2520and%2520generalization%2520to%2520unseen%2520urban%2520settings.%2520Experimental%2520results%2520show%2520that%2520UrbanNav%2520significantly%2520outperforms%2520existing%2520methods%252C%2520highlighting%2520the%2520potential%2520of%2520large-scale%2520web%2520video%2520data%2520to%2520enable%2520language-guided%252C%2520real-world%2520urban%2520navigation%2520for%2520embodied%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrbanNav%3A%20Learning%20Language-Guided%20Urban%20Navigation%20from%20Web-Scale%20Human%20Trajectories&entry.906535625=Yanghong%20Mei%20and%20Yirong%20Yang%20and%20Longteng%20Guo%20and%20Qunbo%20Wang%20and%20Ming-Ming%20Yu%20and%20Xingjian%20He%20and%20Wenjun%20Wu%20and%20Jing%20Liu&entry.1292438233=Navigating%20complex%20urban%20environments%20using%20natural%20language%20instructions%20poses%20significant%20challenges%20for%20embodied%20agents%2C%20including%20noisy%20language%20instructions%2C%20ambiguous%20spatial%20references%2C%20diverse%20landmarks%2C%20and%20dynamic%20street%20scenes.%20Current%20visual%20navigation%20methods%20are%20typically%20limited%20to%20simulated%20or%20off-street%20environments%2C%20and%20often%20rely%20on%20precise%20goal%20formats%2C%20such%20as%20specific%20coordinates%20or%20images.%20This%20limits%20their%20effectiveness%20for%20autonomous%20agents%20like%20last-mile%20delivery%20robots%20navigating%20unfamiliar%20cities.%20To%20address%20these%20limitations%2C%20we%20introduce%20UrbanNav%2C%20a%20scalable%20framework%20that%20trains%20embodied%20agents%20to%20follow%20free-form%20language%20instructions%20in%20diverse%20urban%20settings.%20Leveraging%20web-scale%20city%20walking%20videos%2C%20we%20develop%20an%20scalable%20annotation%20pipeline%20that%20aligns%20human%20navigation%20trajectories%20with%20language%20instructions%20grounded%20in%20real-world%20landmarks.%20UrbanNav%20encompasses%20over%201%2C500%20hours%20of%20navigation%20data%20and%203%20million%20instruction-trajectory-landmark%20triplets%2C%20capturing%20a%20wide%20range%20of%20urban%20scenarios.%20Our%20model%20learns%20robust%20navigation%20policies%20to%20tackle%20complex%20urban%20scenarios%2C%20demonstrating%20superior%20spatial%20reasoning%2C%20robustness%20to%20noisy%20instructions%2C%20and%20generalization%20to%20unseen%20urban%20settings.%20Experimental%20results%20show%20that%20UrbanNav%20significantly%20outperforms%20existing%20methods%2C%20highlighting%20the%20potential%20of%20large-scale%20web%20video%20data%20to%20enable%20language-guided%2C%20real-world%20urban%20navigation%20for%20embodied%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2512.09607v1&entry.124074799=Read"},
{"title": "FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning", "author": "Khurram Khalil and Khaza Anuarul Hoque", "abstract": "Generative Artificial Intelligence models, such as Large Language Models (LLMs) and Large Vision Models (VLMs), exhibit state-of-the-art performance but remain vulnerable to hardware-based threats, specifically bit-flip attacks (BFAs). Existing BFA discovery methods lack generalizability and struggle to scale, often failing to analyze the vast parameter space and complex interdependencies of modern foundation models in a reasonable time. This paper proposes FlipLLM, a reinforcement learning (RL) architecture-agnostic framework that formulates BFA discovery as a sequential decision-making problem. FlipLLM combines sensitivity-guided layer pruning with Q-learning to efficiently identify minimal, high-impact bit sets that can induce catastrophic failure. We demonstrate the effectiveness and generalizability of FlipLLM by applying it to a diverse set of models, including prominent text-only LLMs (GPT-2 Large, LLaMA 3.1 8B, and DeepSeek-V2 7B), VLMs such as LLaVA 1.6, and datasets, such as MMLU, MMLU-Pro, VQAv2, and TextVQA. Our results show that FlipLLM can identify critical bits that are vulnerable to BFAs up to 2.5x faster than SOTA methods. We demonstrate that flipping the FlipLLM-identified bits plummets the accuracy of LLaMA 3.1 8B from 69.9% to ~0.2%, and for LLaVA's VQA score from 78% to almost 0%, by flipping as few as 5 and 7 bits, respectively. Further analysis reveals that applying standard hardware protection mechanisms, such as ECC SECDED, to the FlipLLM-identified bit locations completely mitigates the BFA impact, demonstrating the practical value of our framework in guiding hardware-level defenses. FlipLLM offers the first scalable and adaptive methodology for exploring the BFA vulnerability of both language and multimodal foundation models, paving the way for comprehensive hardware-security evaluation.", "link": "http://arxiv.org/abs/2512.09872v1", "date": "2025-12-10", "relevancy": 2.3805, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4814}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4752}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlipLLM%3A%20Efficient%20Bit-Flip%20Attacks%20on%20Multimodal%20LLMs%20using%20Reinforcement%20Learning&body=Title%3A%20FlipLLM%3A%20Efficient%20Bit-Flip%20Attacks%20on%20Multimodal%20LLMs%20using%20Reinforcement%20Learning%0AAuthor%3A%20Khurram%20Khalil%20and%20Khaza%20Anuarul%20Hoque%0AAbstract%3A%20Generative%20Artificial%20Intelligence%20models%2C%20such%20as%20Large%20Language%20Models%20%28LLMs%29%20and%20Large%20Vision%20Models%20%28VLMs%29%2C%20exhibit%20state-of-the-art%20performance%20but%20remain%20vulnerable%20to%20hardware-based%20threats%2C%20specifically%20bit-flip%20attacks%20%28BFAs%29.%20Existing%20BFA%20discovery%20methods%20lack%20generalizability%20and%20struggle%20to%20scale%2C%20often%20failing%20to%20analyze%20the%20vast%20parameter%20space%20and%20complex%20interdependencies%20of%20modern%20foundation%20models%20in%20a%20reasonable%20time.%20This%20paper%20proposes%20FlipLLM%2C%20a%20reinforcement%20learning%20%28RL%29%20architecture-agnostic%20framework%20that%20formulates%20BFA%20discovery%20as%20a%20sequential%20decision-making%20problem.%20FlipLLM%20combines%20sensitivity-guided%20layer%20pruning%20with%20Q-learning%20to%20efficiently%20identify%20minimal%2C%20high-impact%20bit%20sets%20that%20can%20induce%20catastrophic%20failure.%20We%20demonstrate%20the%20effectiveness%20and%20generalizability%20of%20FlipLLM%20by%20applying%20it%20to%20a%20diverse%20set%20of%20models%2C%20including%20prominent%20text-only%20LLMs%20%28GPT-2%20Large%2C%20LLaMA%203.1%208B%2C%20and%20DeepSeek-V2%207B%29%2C%20VLMs%20such%20as%20LLaVA%201.6%2C%20and%20datasets%2C%20such%20as%20MMLU%2C%20MMLU-Pro%2C%20VQAv2%2C%20and%20TextVQA.%20Our%20results%20show%20that%20FlipLLM%20can%20identify%20critical%20bits%20that%20are%20vulnerable%20to%20BFAs%20up%20to%202.5x%20faster%20than%20SOTA%20methods.%20We%20demonstrate%20that%20flipping%20the%20FlipLLM-identified%20bits%20plummets%20the%20accuracy%20of%20LLaMA%203.1%208B%20from%2069.9%25%20to%20~0.2%25%2C%20and%20for%20LLaVA%27s%20VQA%20score%20from%2078%25%20to%20almost%200%25%2C%20by%20flipping%20as%20few%20as%205%20and%207%20bits%2C%20respectively.%20Further%20analysis%20reveals%20that%20applying%20standard%20hardware%20protection%20mechanisms%2C%20such%20as%20ECC%20SECDED%2C%20to%20the%20FlipLLM-identified%20bit%20locations%20completely%20mitigates%20the%20BFA%20impact%2C%20demonstrating%20the%20practical%20value%20of%20our%20framework%20in%20guiding%20hardware-level%20defenses.%20FlipLLM%20offers%20the%20first%20scalable%20and%20adaptive%20methodology%20for%20exploring%20the%20BFA%20vulnerability%20of%20both%20language%20and%20multimodal%20foundation%20models%2C%20paving%20the%20way%20for%20comprehensive%20hardware-security%20evaluation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlipLLM%253A%2520Efficient%2520Bit-Flip%2520Attacks%2520on%2520Multimodal%2520LLMs%2520using%2520Reinforcement%2520Learning%26entry.906535625%3DKhurram%2520Khalil%2520and%2520Khaza%2520Anuarul%2520Hoque%26entry.1292438233%3DGenerative%2520Artificial%2520Intelligence%2520models%252C%2520such%2520as%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Large%2520Vision%2520Models%2520%2528VLMs%2529%252C%2520exhibit%2520state-of-the-art%2520performance%2520but%2520remain%2520vulnerable%2520to%2520hardware-based%2520threats%252C%2520specifically%2520bit-flip%2520attacks%2520%2528BFAs%2529.%2520Existing%2520BFA%2520discovery%2520methods%2520lack%2520generalizability%2520and%2520struggle%2520to%2520scale%252C%2520often%2520failing%2520to%2520analyze%2520the%2520vast%2520parameter%2520space%2520and%2520complex%2520interdependencies%2520of%2520modern%2520foundation%2520models%2520in%2520a%2520reasonable%2520time.%2520This%2520paper%2520proposes%2520FlipLLM%252C%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520architecture-agnostic%2520framework%2520that%2520formulates%2520BFA%2520discovery%2520as%2520a%2520sequential%2520decision-making%2520problem.%2520FlipLLM%2520combines%2520sensitivity-guided%2520layer%2520pruning%2520with%2520Q-learning%2520to%2520efficiently%2520identify%2520minimal%252C%2520high-impact%2520bit%2520sets%2520that%2520can%2520induce%2520catastrophic%2520failure.%2520We%2520demonstrate%2520the%2520effectiveness%2520and%2520generalizability%2520of%2520FlipLLM%2520by%2520applying%2520it%2520to%2520a%2520diverse%2520set%2520of%2520models%252C%2520including%2520prominent%2520text-only%2520LLMs%2520%2528GPT-2%2520Large%252C%2520LLaMA%25203.1%25208B%252C%2520and%2520DeepSeek-V2%25207B%2529%252C%2520VLMs%2520such%2520as%2520LLaVA%25201.6%252C%2520and%2520datasets%252C%2520such%2520as%2520MMLU%252C%2520MMLU-Pro%252C%2520VQAv2%252C%2520and%2520TextVQA.%2520Our%2520results%2520show%2520that%2520FlipLLM%2520can%2520identify%2520critical%2520bits%2520that%2520are%2520vulnerable%2520to%2520BFAs%2520up%2520to%25202.5x%2520faster%2520than%2520SOTA%2520methods.%2520We%2520demonstrate%2520that%2520flipping%2520the%2520FlipLLM-identified%2520bits%2520plummets%2520the%2520accuracy%2520of%2520LLaMA%25203.1%25208B%2520from%252069.9%2525%2520to%2520~0.2%2525%252C%2520and%2520for%2520LLaVA%2527s%2520VQA%2520score%2520from%252078%2525%2520to%2520almost%25200%2525%252C%2520by%2520flipping%2520as%2520few%2520as%25205%2520and%25207%2520bits%252C%2520respectively.%2520Further%2520analysis%2520reveals%2520that%2520applying%2520standard%2520hardware%2520protection%2520mechanisms%252C%2520such%2520as%2520ECC%2520SECDED%252C%2520to%2520the%2520FlipLLM-identified%2520bit%2520locations%2520completely%2520mitigates%2520the%2520BFA%2520impact%252C%2520demonstrating%2520the%2520practical%2520value%2520of%2520our%2520framework%2520in%2520guiding%2520hardware-level%2520defenses.%2520FlipLLM%2520offers%2520the%2520first%2520scalable%2520and%2520adaptive%2520methodology%2520for%2520exploring%2520the%2520BFA%2520vulnerability%2520of%2520both%2520language%2520and%2520multimodal%2520foundation%2520models%252C%2520paving%2520the%2520way%2520for%2520comprehensive%2520hardware-security%2520evaluation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlipLLM%3A%20Efficient%20Bit-Flip%20Attacks%20on%20Multimodal%20LLMs%20using%20Reinforcement%20Learning&entry.906535625=Khurram%20Khalil%20and%20Khaza%20Anuarul%20Hoque&entry.1292438233=Generative%20Artificial%20Intelligence%20models%2C%20such%20as%20Large%20Language%20Models%20%28LLMs%29%20and%20Large%20Vision%20Models%20%28VLMs%29%2C%20exhibit%20state-of-the-art%20performance%20but%20remain%20vulnerable%20to%20hardware-based%20threats%2C%20specifically%20bit-flip%20attacks%20%28BFAs%29.%20Existing%20BFA%20discovery%20methods%20lack%20generalizability%20and%20struggle%20to%20scale%2C%20often%20failing%20to%20analyze%20the%20vast%20parameter%20space%20and%20complex%20interdependencies%20of%20modern%20foundation%20models%20in%20a%20reasonable%20time.%20This%20paper%20proposes%20FlipLLM%2C%20a%20reinforcement%20learning%20%28RL%29%20architecture-agnostic%20framework%20that%20formulates%20BFA%20discovery%20as%20a%20sequential%20decision-making%20problem.%20FlipLLM%20combines%20sensitivity-guided%20layer%20pruning%20with%20Q-learning%20to%20efficiently%20identify%20minimal%2C%20high-impact%20bit%20sets%20that%20can%20induce%20catastrophic%20failure.%20We%20demonstrate%20the%20effectiveness%20and%20generalizability%20of%20FlipLLM%20by%20applying%20it%20to%20a%20diverse%20set%20of%20models%2C%20including%20prominent%20text-only%20LLMs%20%28GPT-2%20Large%2C%20LLaMA%203.1%208B%2C%20and%20DeepSeek-V2%207B%29%2C%20VLMs%20such%20as%20LLaVA%201.6%2C%20and%20datasets%2C%20such%20as%20MMLU%2C%20MMLU-Pro%2C%20VQAv2%2C%20and%20TextVQA.%20Our%20results%20show%20that%20FlipLLM%20can%20identify%20critical%20bits%20that%20are%20vulnerable%20to%20BFAs%20up%20to%202.5x%20faster%20than%20SOTA%20methods.%20We%20demonstrate%20that%20flipping%20the%20FlipLLM-identified%20bits%20plummets%20the%20accuracy%20of%20LLaMA%203.1%208B%20from%2069.9%25%20to%20~0.2%25%2C%20and%20for%20LLaVA%27s%20VQA%20score%20from%2078%25%20to%20almost%200%25%2C%20by%20flipping%20as%20few%20as%205%20and%207%20bits%2C%20respectively.%20Further%20analysis%20reveals%20that%20applying%20standard%20hardware%20protection%20mechanisms%2C%20such%20as%20ECC%20SECDED%2C%20to%20the%20FlipLLM-identified%20bit%20locations%20completely%20mitigates%20the%20BFA%20impact%2C%20demonstrating%20the%20practical%20value%20of%20our%20framework%20in%20guiding%20hardware-level%20defenses.%20FlipLLM%20offers%20the%20first%20scalable%20and%20adaptive%20methodology%20for%20exploring%20the%20BFA%20vulnerability%20of%20both%20language%20and%20multimodal%20foundation%20models%2C%20paving%20the%20way%20for%20comprehensive%20hardware-security%20evaluation.&entry.1838667208=http%3A//arxiv.org/abs/2512.09872v1&entry.124074799=Read"},
{"title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving", "author": "Hao Lu and Ziyang Liu and Guangfeng Jiang and Yuanfei Luo and Sheng Chen and Yangang Zhang and Ying-Cong Chen", "abstract": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.", "link": "http://arxiv.org/abs/2512.09864v1", "date": "2025-12-10", "relevancy": 2.3674, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5955}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5906}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniUGP%3A%20Unifying%20Understanding%2C%20Generation%2C%20and%20Planing%20For%20End-to-end%20Autonomous%20Driving&body=Title%3A%20UniUGP%3A%20Unifying%20Understanding%2C%20Generation%2C%20and%20Planing%20For%20End-to-end%20Autonomous%20Driving%0AAuthor%3A%20Hao%20Lu%20and%20Ziyang%20Liu%20and%20Guangfeng%20Jiang%20and%20Yuanfei%20Luo%20and%20Sheng%20Chen%20and%20Yangang%20Zhang%20and%20Ying-Cong%20Chen%0AAbstract%3A%20Autonomous%20driving%20%28AD%29%20systems%20struggle%20in%20long-tail%20scenarios%20due%20to%20limited%20world%20knowledge%20and%20weak%20visual%20dynamic%20modeling.%20Existing%20vision-language-action%20%28VLA%29-based%20methods%20cannot%20leverage%20unlabeled%20videos%20for%20visual%20causal%20learning%2C%20while%20world%20model-based%20methods%20lack%20reasoning%20capabilities%20from%20large%20language%20models.%20In%20this%20paper%2C%20we%20construct%20multiple%20specialized%20datasets%20providing%20reasoning%20and%20planning%20annotations%20for%20complex%20scenarios.%20Then%2C%20a%20unified%20Understanding-Generation-Planning%20framework%2C%20named%20UniUGP%2C%20is%20proposed%20to%20synergize%20scene%20reasoning%2C%20future%20video%20generation%2C%20and%20trajectory%20planning%20through%20a%20hybrid%20expert%20architecture.%20By%20integrating%20pre-trained%20VLMs%20and%20video%20generation%20models%2C%20UniUGP%20leverages%20visual%20dynamics%20and%20semantic%20reasoning%20to%20enhance%20planning%20performance.%20Taking%20multi-frame%20observations%20and%20language%20instructions%20as%20input%2C%20it%20produces%20interpretable%20chain-of-thought%20reasoning%2C%20physically%20consistent%20trajectories%2C%20and%20coherent%20future%20videos.%20We%20introduce%20a%20four-stage%20training%20strategy%20that%20progressively%20builds%20these%20capabilities%20across%20multiple%20existing%20AD%20datasets%2C%20along%20with%20the%20proposed%20specialized%20datasets.%20Experiments%20demonstrate%20state-of-the-art%20performance%20in%20perception%2C%20reasoning%2C%20and%20decision-making%2C%20with%20superior%20generalization%20to%20challenging%20long-tail%20situations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniUGP%253A%2520Unifying%2520Understanding%252C%2520Generation%252C%2520and%2520Planing%2520For%2520End-to-end%2520Autonomous%2520Driving%26entry.906535625%3DHao%2520Lu%2520and%2520Ziyang%2520Liu%2520and%2520Guangfeng%2520Jiang%2520and%2520Yuanfei%2520Luo%2520and%2520Sheng%2520Chen%2520and%2520Yangang%2520Zhang%2520and%2520Ying-Cong%2520Chen%26entry.1292438233%3DAutonomous%2520driving%2520%2528AD%2529%2520systems%2520struggle%2520in%2520long-tail%2520scenarios%2520due%2520to%2520limited%2520world%2520knowledge%2520and%2520weak%2520visual%2520dynamic%2520modeling.%2520Existing%2520vision-language-action%2520%2528VLA%2529-based%2520methods%2520cannot%2520leverage%2520unlabeled%2520videos%2520for%2520visual%2520causal%2520learning%252C%2520while%2520world%2520model-based%2520methods%2520lack%2520reasoning%2520capabilities%2520from%2520large%2520language%2520models.%2520In%2520this%2520paper%252C%2520we%2520construct%2520multiple%2520specialized%2520datasets%2520providing%2520reasoning%2520and%2520planning%2520annotations%2520for%2520complex%2520scenarios.%2520Then%252C%2520a%2520unified%2520Understanding-Generation-Planning%2520framework%252C%2520named%2520UniUGP%252C%2520is%2520proposed%2520to%2520synergize%2520scene%2520reasoning%252C%2520future%2520video%2520generation%252C%2520and%2520trajectory%2520planning%2520through%2520a%2520hybrid%2520expert%2520architecture.%2520By%2520integrating%2520pre-trained%2520VLMs%2520and%2520video%2520generation%2520models%252C%2520UniUGP%2520leverages%2520visual%2520dynamics%2520and%2520semantic%2520reasoning%2520to%2520enhance%2520planning%2520performance.%2520Taking%2520multi-frame%2520observations%2520and%2520language%2520instructions%2520as%2520input%252C%2520it%2520produces%2520interpretable%2520chain-of-thought%2520reasoning%252C%2520physically%2520consistent%2520trajectories%252C%2520and%2520coherent%2520future%2520videos.%2520We%2520introduce%2520a%2520four-stage%2520training%2520strategy%2520that%2520progressively%2520builds%2520these%2520capabilities%2520across%2520multiple%2520existing%2520AD%2520datasets%252C%2520along%2520with%2520the%2520proposed%2520specialized%2520datasets.%2520Experiments%2520demonstrate%2520state-of-the-art%2520performance%2520in%2520perception%252C%2520reasoning%252C%2520and%2520decision-making%252C%2520with%2520superior%2520generalization%2520to%2520challenging%2520long-tail%2520situations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniUGP%3A%20Unifying%20Understanding%2C%20Generation%2C%20and%20Planing%20For%20End-to-end%20Autonomous%20Driving&entry.906535625=Hao%20Lu%20and%20Ziyang%20Liu%20and%20Guangfeng%20Jiang%20and%20Yuanfei%20Luo%20and%20Sheng%20Chen%20and%20Yangang%20Zhang%20and%20Ying-Cong%20Chen&entry.1292438233=Autonomous%20driving%20%28AD%29%20systems%20struggle%20in%20long-tail%20scenarios%20due%20to%20limited%20world%20knowledge%20and%20weak%20visual%20dynamic%20modeling.%20Existing%20vision-language-action%20%28VLA%29-based%20methods%20cannot%20leverage%20unlabeled%20videos%20for%20visual%20causal%20learning%2C%20while%20world%20model-based%20methods%20lack%20reasoning%20capabilities%20from%20large%20language%20models.%20In%20this%20paper%2C%20we%20construct%20multiple%20specialized%20datasets%20providing%20reasoning%20and%20planning%20annotations%20for%20complex%20scenarios.%20Then%2C%20a%20unified%20Understanding-Generation-Planning%20framework%2C%20named%20UniUGP%2C%20is%20proposed%20to%20synergize%20scene%20reasoning%2C%20future%20video%20generation%2C%20and%20trajectory%20planning%20through%20a%20hybrid%20expert%20architecture.%20By%20integrating%20pre-trained%20VLMs%20and%20video%20generation%20models%2C%20UniUGP%20leverages%20visual%20dynamics%20and%20semantic%20reasoning%20to%20enhance%20planning%20performance.%20Taking%20multi-frame%20observations%20and%20language%20instructions%20as%20input%2C%20it%20produces%20interpretable%20chain-of-thought%20reasoning%2C%20physically%20consistent%20trajectories%2C%20and%20coherent%20future%20videos.%20We%20introduce%20a%20four-stage%20training%20strategy%20that%20progressively%20builds%20these%20capabilities%20across%20multiple%20existing%20AD%20datasets%2C%20along%20with%20the%20proposed%20specialized%20datasets.%20Experiments%20demonstrate%20state-of-the-art%20performance%20in%20perception%2C%20reasoning%2C%20and%20decision-making%2C%20with%20superior%20generalization%20to%20challenging%20long-tail%20situations.&entry.1838667208=http%3A//arxiv.org/abs/2512.09864v1&entry.124074799=Read"},
{"title": "Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach", "author": "Zhengquan Luo and Guy Tadmor and Or Amar and David Zeevi and Zhiqiang Xu", "abstract": "Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.", "link": "http://arxiv.org/abs/2512.07332v2", "date": "2025-12-10", "relevancy": 2.3602, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4946}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4657}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local-Curvature-Aware%20Knowledge%20Graph%20Embedding%3A%20An%20Extended%20Ricci%20Flow%20Approach&body=Title%3A%20Local-Curvature-Aware%20Knowledge%20Graph%20Embedding%3A%20An%20Extended%20Ricci%20Flow%20Approach%0AAuthor%3A%20Zhengquan%20Luo%20and%20Guy%20Tadmor%20and%20Or%20Amar%20and%20David%20Zeevi%20and%20Zhiqiang%20Xu%0AAbstract%3A%20Knowledge%20graph%20embedding%20%28KGE%29%20relies%20on%20the%20geometry%20of%20the%20embedding%20space%20to%20encode%20semantic%20and%20structural%20relations.%20Existing%20methods%20place%20all%20entities%20on%20one%20homogeneous%20manifold%2C%20Euclidean%2C%20spherical%2C%20hyperbolic%2C%20or%20their%20product/multi-curvature%20variants%2C%20to%20model%20linear%2C%20symmetric%2C%20or%20hierarchical%20patterns.%20Yet%20a%20predefined%2C%20homogeneous%20manifold%20cannot%20accommodate%20the%20sharply%20varying%20curvature%20that%20real-world%20graphs%20exhibit%20across%20local%20regions.%20Since%20this%20geometry%20is%20imposed%20a%20priori%2C%20any%20mismatch%20with%20the%20knowledge%20graph%27s%20local%20curvatures%20will%20distort%20distances%20between%20entities%20and%20hurt%20the%20expressiveness%20of%20the%20resulting%20KGE.%20To%20rectify%20this%2C%20we%20propose%20RicciKGE%20to%20have%20the%20KGE%20loss%20gradient%20coupled%20with%20local%20curvatures%20in%20an%20extended%20Ricci%20flow%20such%20that%20entity%20embeddings%20co-evolve%20dynamically%20with%20the%20underlying%20manifold%20geometry%20towards%20mutual%20adaptation.%20Theoretically%2C%20when%20the%20coupling%20coefficient%20is%20bounded%20and%20properly%20selected%2C%20we%20rigorously%20prove%20that%20i%29%20all%20the%20edge-wise%20curvatures%20decay%20exponentially%2C%20meaning%20that%20the%20manifold%20is%20driven%20toward%20the%20Euclidean%20flatness%3B%20and%20ii%29%20the%20KGE%20distances%20strictly%20converge%20to%20a%20global%20optimum%2C%20which%20indicates%20that%20geometric%20flattening%20and%20embedding%20optimization%20are%20promoting%20each%20other.%20Experimental%20improvements%20on%20link%20prediction%20and%20node%20classification%20benchmarks%20demonstrate%20RicciKGE%27s%20effectiveness%20in%20adapting%20to%20heterogeneous%20knowledge%20graph%20structures.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07332v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal-Curvature-Aware%2520Knowledge%2520Graph%2520Embedding%253A%2520An%2520Extended%2520Ricci%2520Flow%2520Approach%26entry.906535625%3DZhengquan%2520Luo%2520and%2520Guy%2520Tadmor%2520and%2520Or%2520Amar%2520and%2520David%2520Zeevi%2520and%2520Zhiqiang%2520Xu%26entry.1292438233%3DKnowledge%2520graph%2520embedding%2520%2528KGE%2529%2520relies%2520on%2520the%2520geometry%2520of%2520the%2520embedding%2520space%2520to%2520encode%2520semantic%2520and%2520structural%2520relations.%2520Existing%2520methods%2520place%2520all%2520entities%2520on%2520one%2520homogeneous%2520manifold%252C%2520Euclidean%252C%2520spherical%252C%2520hyperbolic%252C%2520or%2520their%2520product/multi-curvature%2520variants%252C%2520to%2520model%2520linear%252C%2520symmetric%252C%2520or%2520hierarchical%2520patterns.%2520Yet%2520a%2520predefined%252C%2520homogeneous%2520manifold%2520cannot%2520accommodate%2520the%2520sharply%2520varying%2520curvature%2520that%2520real-world%2520graphs%2520exhibit%2520across%2520local%2520regions.%2520Since%2520this%2520geometry%2520is%2520imposed%2520a%2520priori%252C%2520any%2520mismatch%2520with%2520the%2520knowledge%2520graph%2527s%2520local%2520curvatures%2520will%2520distort%2520distances%2520between%2520entities%2520and%2520hurt%2520the%2520expressiveness%2520of%2520the%2520resulting%2520KGE.%2520To%2520rectify%2520this%252C%2520we%2520propose%2520RicciKGE%2520to%2520have%2520the%2520KGE%2520loss%2520gradient%2520coupled%2520with%2520local%2520curvatures%2520in%2520an%2520extended%2520Ricci%2520flow%2520such%2520that%2520entity%2520embeddings%2520co-evolve%2520dynamically%2520with%2520the%2520underlying%2520manifold%2520geometry%2520towards%2520mutual%2520adaptation.%2520Theoretically%252C%2520when%2520the%2520coupling%2520coefficient%2520is%2520bounded%2520and%2520properly%2520selected%252C%2520we%2520rigorously%2520prove%2520that%2520i%2529%2520all%2520the%2520edge-wise%2520curvatures%2520decay%2520exponentially%252C%2520meaning%2520that%2520the%2520manifold%2520is%2520driven%2520toward%2520the%2520Euclidean%2520flatness%253B%2520and%2520ii%2529%2520the%2520KGE%2520distances%2520strictly%2520converge%2520to%2520a%2520global%2520optimum%252C%2520which%2520indicates%2520that%2520geometric%2520flattening%2520and%2520embedding%2520optimization%2520are%2520promoting%2520each%2520other.%2520Experimental%2520improvements%2520on%2520link%2520prediction%2520and%2520node%2520classification%2520benchmarks%2520demonstrate%2520RicciKGE%2527s%2520effectiveness%2520in%2520adapting%2520to%2520heterogeneous%2520knowledge%2520graph%2520structures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07332v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local-Curvature-Aware%20Knowledge%20Graph%20Embedding%3A%20An%20Extended%20Ricci%20Flow%20Approach&entry.906535625=Zhengquan%20Luo%20and%20Guy%20Tadmor%20and%20Or%20Amar%20and%20David%20Zeevi%20and%20Zhiqiang%20Xu&entry.1292438233=Knowledge%20graph%20embedding%20%28KGE%29%20relies%20on%20the%20geometry%20of%20the%20embedding%20space%20to%20encode%20semantic%20and%20structural%20relations.%20Existing%20methods%20place%20all%20entities%20on%20one%20homogeneous%20manifold%2C%20Euclidean%2C%20spherical%2C%20hyperbolic%2C%20or%20their%20product/multi-curvature%20variants%2C%20to%20model%20linear%2C%20symmetric%2C%20or%20hierarchical%20patterns.%20Yet%20a%20predefined%2C%20homogeneous%20manifold%20cannot%20accommodate%20the%20sharply%20varying%20curvature%20that%20real-world%20graphs%20exhibit%20across%20local%20regions.%20Since%20this%20geometry%20is%20imposed%20a%20priori%2C%20any%20mismatch%20with%20the%20knowledge%20graph%27s%20local%20curvatures%20will%20distort%20distances%20between%20entities%20and%20hurt%20the%20expressiveness%20of%20the%20resulting%20KGE.%20To%20rectify%20this%2C%20we%20propose%20RicciKGE%20to%20have%20the%20KGE%20loss%20gradient%20coupled%20with%20local%20curvatures%20in%20an%20extended%20Ricci%20flow%20such%20that%20entity%20embeddings%20co-evolve%20dynamically%20with%20the%20underlying%20manifold%20geometry%20towards%20mutual%20adaptation.%20Theoretically%2C%20when%20the%20coupling%20coefficient%20is%20bounded%20and%20properly%20selected%2C%20we%20rigorously%20prove%20that%20i%29%20all%20the%20edge-wise%20curvatures%20decay%20exponentially%2C%20meaning%20that%20the%20manifold%20is%20driven%20toward%20the%20Euclidean%20flatness%3B%20and%20ii%29%20the%20KGE%20distances%20strictly%20converge%20to%20a%20global%20optimum%2C%20which%20indicates%20that%20geometric%20flattening%20and%20embedding%20optimization%20are%20promoting%20each%20other.%20Experimental%20improvements%20on%20link%20prediction%20and%20node%20classification%20benchmarks%20demonstrate%20RicciKGE%27s%20effectiveness%20in%20adapting%20to%20heterogeneous%20knowledge%20graph%20structures.&entry.1838667208=http%3A//arxiv.org/abs/2512.07332v2&entry.124074799=Read"},
{"title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models", "author": "Jingcong Liang and Siyuan Wang and Miren Tian and Yitong Li and Duyu Tang and Zhongyu Wei", "abstract": "Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the hit rate of an expert cache utilizing a length of future information under a cache limit. We analyze 20 MoE LLMs with diverse sizes and architectures and use toy models to verify key factors related to local routing consistency. We find a strong trade-off between local routing consistency and *local* load balance, while showing that *global* load balance can coexist with local routing consistency. Meanwhile, settings like shared experts that decrease expert combination space can lead to low local routing consistency. We further reveal that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models balance between cache effectiveness and efficiency with cache sizes approximately twice the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .", "link": "http://arxiv.org/abs/2505.16056v3", "date": "2025-12-10", "relevancy": 2.3538, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.477}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20All%20Models%20Suit%20Expert%20Offloading%3A%20On%20Local%20Routing%20Consistency%20of%20Mixture-of-Expert%20Models&body=Title%3A%20Not%20All%20Models%20Suit%20Expert%20Offloading%3A%20On%20Local%20Routing%20Consistency%20of%20Mixture-of-Expert%20Models%0AAuthor%3A%20Jingcong%20Liang%20and%20Siyuan%20Wang%20and%20Miren%20Tian%20and%20Yitong%20Li%20and%20Duyu%20Tang%20and%20Zhongyu%20Wei%0AAbstract%3A%20Mixture-of-Experts%20%28MoE%29%20enables%20efficient%20scaling%20of%20large%20language%20models%20%28LLMs%29%20with%20sparsely%20activated%20experts%20during%20inference.%20To%20effectively%20deploy%20large%20MoE%20models%20on%20memory-constrained%20devices%2C%20many%20systems%20introduce%20%2Aexpert%20offloading%2A%20that%20caches%20a%20subset%20of%20experts%20in%20fast%20memory%2C%20leaving%20others%20on%20slow%20memory%20to%20run%20on%20CPU%20or%20load%20on%20demand.%20While%20some%20research%20has%20exploited%20the%20locality%20of%20expert%20activations%2C%20where%20consecutive%20tokens%20activate%20similar%20experts%2C%20the%20degree%20of%20this%20%2A%2Alocal%20routing%20consistency%2A%2A%20varies%20across%20models%20and%20remains%20understudied.%20In%20this%20paper%2C%20we%20propose%20two%20metrics%20to%20measure%20local%20routing%20consistency%20of%20MoE%20models%3A%20%281%29%20%2A%2ASegment%20Routing%20Best%20Performance%20%28SRP%29%2A%2A%2C%20which%20evaluates%20how%20well%20a%20fixed%20group%20of%20experts%20can%20cover%20the%20needs%20of%20a%20segment%20of%20tokens%2C%20and%20%282%29%20%2A%2ASegment%20Cache%20Best%20Hit%20Rate%20%28SCH%29%2A%2A%2C%20which%20measures%20the%20hit%20rate%20of%20an%20expert%20cache%20utilizing%20a%20length%20of%20future%20information%20under%20a%20cache%20limit.%20We%20analyze%2020%20MoE%20LLMs%20with%20diverse%20sizes%20and%20architectures%20and%20use%20toy%20models%20to%20verify%20key%20factors%20related%20to%20local%20routing%20consistency.%20We%20find%20a%20strong%20trade-off%20between%20local%20routing%20consistency%20and%20%2Alocal%2A%20load%20balance%2C%20while%20showing%20that%20%2Aglobal%2A%20load%20balance%20can%20coexist%20with%20local%20routing%20consistency.%20Meanwhile%2C%20settings%20like%20shared%20experts%20that%20decrease%20expert%20combination%20space%20can%20lead%20to%20low%20local%20routing%20consistency.%20We%20further%20reveal%20that%20domain-specialized%20experts%20contribute%20more%20to%20routing%20consistency%20than%20vocabulary-specialized%20ones%2C%20and%20that%20most%20models%20balance%20between%20cache%20effectiveness%20and%20efficiency%20with%20cache%20sizes%20approximately%20twice%20the%20active%20experts.%20These%20findings%20pave%20the%20way%20for%20memory-efficient%20MoE%20design%20and%20deployment%20without%20compromising%20inference%20speed.%20We%20publish%20the%20code%20for%20replicating%20experiments%20at%20https%3A//github.com/ljcleo/moe-lrc%20.%0ALink%3A%20http%3A//arxiv.org/abs/2505.16056v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520All%2520Models%2520Suit%2520Expert%2520Offloading%253A%2520On%2520Local%2520Routing%2520Consistency%2520of%2520Mixture-of-Expert%2520Models%26entry.906535625%3DJingcong%2520Liang%2520and%2520Siyuan%2520Wang%2520and%2520Miren%2520Tian%2520and%2520Yitong%2520Li%2520and%2520Duyu%2520Tang%2520and%2520Zhongyu%2520Wei%26entry.1292438233%3DMixture-of-Experts%2520%2528MoE%2529%2520enables%2520efficient%2520scaling%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520sparsely%2520activated%2520experts%2520during%2520inference.%2520To%2520effectively%2520deploy%2520large%2520MoE%2520models%2520on%2520memory-constrained%2520devices%252C%2520many%2520systems%2520introduce%2520%252Aexpert%2520offloading%252A%2520that%2520caches%2520a%2520subset%2520of%2520experts%2520in%2520fast%2520memory%252C%2520leaving%2520others%2520on%2520slow%2520memory%2520to%2520run%2520on%2520CPU%2520or%2520load%2520on%2520demand.%2520While%2520some%2520research%2520has%2520exploited%2520the%2520locality%2520of%2520expert%2520activations%252C%2520where%2520consecutive%2520tokens%2520activate%2520similar%2520experts%252C%2520the%2520degree%2520of%2520this%2520%252A%252Alocal%2520routing%2520consistency%252A%252A%2520varies%2520across%2520models%2520and%2520remains%2520understudied.%2520In%2520this%2520paper%252C%2520we%2520propose%2520two%2520metrics%2520to%2520measure%2520local%2520routing%2520consistency%2520of%2520MoE%2520models%253A%2520%25281%2529%2520%252A%252ASegment%2520Routing%2520Best%2520Performance%2520%2528SRP%2529%252A%252A%252C%2520which%2520evaluates%2520how%2520well%2520a%2520fixed%2520group%2520of%2520experts%2520can%2520cover%2520the%2520needs%2520of%2520a%2520segment%2520of%2520tokens%252C%2520and%2520%25282%2529%2520%252A%252ASegment%2520Cache%2520Best%2520Hit%2520Rate%2520%2528SCH%2529%252A%252A%252C%2520which%2520measures%2520the%2520hit%2520rate%2520of%2520an%2520expert%2520cache%2520utilizing%2520a%2520length%2520of%2520future%2520information%2520under%2520a%2520cache%2520limit.%2520We%2520analyze%252020%2520MoE%2520LLMs%2520with%2520diverse%2520sizes%2520and%2520architectures%2520and%2520use%2520toy%2520models%2520to%2520verify%2520key%2520factors%2520related%2520to%2520local%2520routing%2520consistency.%2520We%2520find%2520a%2520strong%2520trade-off%2520between%2520local%2520routing%2520consistency%2520and%2520%252Alocal%252A%2520load%2520balance%252C%2520while%2520showing%2520that%2520%252Aglobal%252A%2520load%2520balance%2520can%2520coexist%2520with%2520local%2520routing%2520consistency.%2520Meanwhile%252C%2520settings%2520like%2520shared%2520experts%2520that%2520decrease%2520expert%2520combination%2520space%2520can%2520lead%2520to%2520low%2520local%2520routing%2520consistency.%2520We%2520further%2520reveal%2520that%2520domain-specialized%2520experts%2520contribute%2520more%2520to%2520routing%2520consistency%2520than%2520vocabulary-specialized%2520ones%252C%2520and%2520that%2520most%2520models%2520balance%2520between%2520cache%2520effectiveness%2520and%2520efficiency%2520with%2520cache%2520sizes%2520approximately%2520twice%2520the%2520active%2520experts.%2520These%2520findings%2520pave%2520the%2520way%2520for%2520memory-efficient%2520MoE%2520design%2520and%2520deployment%2520without%2520compromising%2520inference%2520speed.%2520We%2520publish%2520the%2520code%2520for%2520replicating%2520experiments%2520at%2520https%253A//github.com/ljcleo/moe-lrc%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16056v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Models%20Suit%20Expert%20Offloading%3A%20On%20Local%20Routing%20Consistency%20of%20Mixture-of-Expert%20Models&entry.906535625=Jingcong%20Liang%20and%20Siyuan%20Wang%20and%20Miren%20Tian%20and%20Yitong%20Li%20and%20Duyu%20Tang%20and%20Zhongyu%20Wei&entry.1292438233=Mixture-of-Experts%20%28MoE%29%20enables%20efficient%20scaling%20of%20large%20language%20models%20%28LLMs%29%20with%20sparsely%20activated%20experts%20during%20inference.%20To%20effectively%20deploy%20large%20MoE%20models%20on%20memory-constrained%20devices%2C%20many%20systems%20introduce%20%2Aexpert%20offloading%2A%20that%20caches%20a%20subset%20of%20experts%20in%20fast%20memory%2C%20leaving%20others%20on%20slow%20memory%20to%20run%20on%20CPU%20or%20load%20on%20demand.%20While%20some%20research%20has%20exploited%20the%20locality%20of%20expert%20activations%2C%20where%20consecutive%20tokens%20activate%20similar%20experts%2C%20the%20degree%20of%20this%20%2A%2Alocal%20routing%20consistency%2A%2A%20varies%20across%20models%20and%20remains%20understudied.%20In%20this%20paper%2C%20we%20propose%20two%20metrics%20to%20measure%20local%20routing%20consistency%20of%20MoE%20models%3A%20%281%29%20%2A%2ASegment%20Routing%20Best%20Performance%20%28SRP%29%2A%2A%2C%20which%20evaluates%20how%20well%20a%20fixed%20group%20of%20experts%20can%20cover%20the%20needs%20of%20a%20segment%20of%20tokens%2C%20and%20%282%29%20%2A%2ASegment%20Cache%20Best%20Hit%20Rate%20%28SCH%29%2A%2A%2C%20which%20measures%20the%20hit%20rate%20of%20an%20expert%20cache%20utilizing%20a%20length%20of%20future%20information%20under%20a%20cache%20limit.%20We%20analyze%2020%20MoE%20LLMs%20with%20diverse%20sizes%20and%20architectures%20and%20use%20toy%20models%20to%20verify%20key%20factors%20related%20to%20local%20routing%20consistency.%20We%20find%20a%20strong%20trade-off%20between%20local%20routing%20consistency%20and%20%2Alocal%2A%20load%20balance%2C%20while%20showing%20that%20%2Aglobal%2A%20load%20balance%20can%20coexist%20with%20local%20routing%20consistency.%20Meanwhile%2C%20settings%20like%20shared%20experts%20that%20decrease%20expert%20combination%20space%20can%20lead%20to%20low%20local%20routing%20consistency.%20We%20further%20reveal%20that%20domain-specialized%20experts%20contribute%20more%20to%20routing%20consistency%20than%20vocabulary-specialized%20ones%2C%20and%20that%20most%20models%20balance%20between%20cache%20effectiveness%20and%20efficiency%20with%20cache%20sizes%20approximately%20twice%20the%20active%20experts.%20These%20findings%20pave%20the%20way%20for%20memory-efficient%20MoE%20design%20and%20deployment%20without%20compromising%20inference%20speed.%20We%20publish%20the%20code%20for%20replicating%20experiments%20at%20https%3A//github.com/ljcleo/moe-lrc%20.&entry.1838667208=http%3A//arxiv.org/abs/2505.16056v3&entry.124074799=Read"},
{"title": "System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection", "author": "Binglin Wu and Jiaxiu Zou and Xianneng Li", "abstract": "The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.", "link": "http://arxiv.org/abs/2512.09563v1", "date": "2025-12-10", "relevancy": 2.3516, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4808}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20System%20Report%20for%20CCL25-Eval%20Task%2010%3A%20Prompt-Driven%20Large%20Language%20Model%20Merge%20for%20Fine-Grained%20Chinese%20Hate%20Speech%20Detection&body=Title%3A%20System%20Report%20for%20CCL25-Eval%20Task%2010%3A%20Prompt-Driven%20Large%20Language%20Model%20Merge%20for%20Fine-Grained%20Chinese%20Hate%20Speech%20Detection%0AAuthor%3A%20Binglin%20Wu%20and%20Jiaxiu%20Zou%20and%20Xianneng%20Li%0AAbstract%3A%20The%20proliferation%20of%20hate%20speech%20on%20Chinese%20social%20media%20poses%20urgent%20societal%20risks%2C%20yet%20traditional%20systems%20struggle%20to%20decode%20context-dependent%20rhetorical%20strategies%20and%20evolving%20slang.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20novel%20three-stage%20LLM-based%20framework%3A%20Prompt%20Engineering%2C%20Supervised%20Fine-tuning%2C%20and%20LLM%20Merging.%20First%2C%20context-aware%20prompts%20are%20designed%20to%20guide%20LLMs%20in%20extracting%20implicit%20hate%20patterns.%20Next%2C%20task-specific%20features%20are%20integrated%20during%20supervised%20fine-tuning%20to%20enhance%20domain%20adaptation.%20Finally%2C%20merging%20fine-tuned%20LLMs%20improves%20robustness%20against%20out-of-distribution%20cases.%20Evaluations%20on%20the%20STATE-ToxiCN%20benchmark%20validate%20the%20framework%27s%20effectiveness%2C%20demonstrating%20superior%20performance%20over%20baseline%20methods%20in%20detecting%20fine-grained%20hate%20speech.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystem%2520Report%2520for%2520CCL25-Eval%2520Task%252010%253A%2520Prompt-Driven%2520Large%2520Language%2520Model%2520Merge%2520for%2520Fine-Grained%2520Chinese%2520Hate%2520Speech%2520Detection%26entry.906535625%3DBinglin%2520Wu%2520and%2520Jiaxiu%2520Zou%2520and%2520Xianneng%2520Li%26entry.1292438233%3DThe%2520proliferation%2520of%2520hate%2520speech%2520on%2520Chinese%2520social%2520media%2520poses%2520urgent%2520societal%2520risks%252C%2520yet%2520traditional%2520systems%2520struggle%2520to%2520decode%2520context-dependent%2520rhetorical%2520strategies%2520and%2520evolving%2520slang.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520novel%2520three-stage%2520LLM-based%2520framework%253A%2520Prompt%2520Engineering%252C%2520Supervised%2520Fine-tuning%252C%2520and%2520LLM%2520Merging.%2520First%252C%2520context-aware%2520prompts%2520are%2520designed%2520to%2520guide%2520LLMs%2520in%2520extracting%2520implicit%2520hate%2520patterns.%2520Next%252C%2520task-specific%2520features%2520are%2520integrated%2520during%2520supervised%2520fine-tuning%2520to%2520enhance%2520domain%2520adaptation.%2520Finally%252C%2520merging%2520fine-tuned%2520LLMs%2520improves%2520robustness%2520against%2520out-of-distribution%2520cases.%2520Evaluations%2520on%2520the%2520STATE-ToxiCN%2520benchmark%2520validate%2520the%2520framework%2527s%2520effectiveness%252C%2520demonstrating%2520superior%2520performance%2520over%2520baseline%2520methods%2520in%2520detecting%2520fine-grained%2520hate%2520speech.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=System%20Report%20for%20CCL25-Eval%20Task%2010%3A%20Prompt-Driven%20Large%20Language%20Model%20Merge%20for%20Fine-Grained%20Chinese%20Hate%20Speech%20Detection&entry.906535625=Binglin%20Wu%20and%20Jiaxiu%20Zou%20and%20Xianneng%20Li&entry.1292438233=The%20proliferation%20of%20hate%20speech%20on%20Chinese%20social%20media%20poses%20urgent%20societal%20risks%2C%20yet%20traditional%20systems%20struggle%20to%20decode%20context-dependent%20rhetorical%20strategies%20and%20evolving%20slang.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20novel%20three-stage%20LLM-based%20framework%3A%20Prompt%20Engineering%2C%20Supervised%20Fine-tuning%2C%20and%20LLM%20Merging.%20First%2C%20context-aware%20prompts%20are%20designed%20to%20guide%20LLMs%20in%20extracting%20implicit%20hate%20patterns.%20Next%2C%20task-specific%20features%20are%20integrated%20during%20supervised%20fine-tuning%20to%20enhance%20domain%20adaptation.%20Finally%2C%20merging%20fine-tuned%20LLMs%20improves%20robustness%20against%20out-of-distribution%20cases.%20Evaluations%20on%20the%20STATE-ToxiCN%20benchmark%20validate%20the%20framework%27s%20effectiveness%2C%20demonstrating%20superior%20performance%20over%20baseline%20methods%20in%20detecting%20fine-grained%20hate%20speech.&entry.1838667208=http%3A//arxiv.org/abs/2512.09563v1&entry.124074799=Read"},
{"title": "VisualActBench: Can VLMs See and Act like a Human?", "author": "Daoan Zhang and Pai Liu and Xiaofei Zhou and Yuan Ge and Guangchen Lan and Jing Bi and Christopher Brinton and Ehsan Hoque and Jiebo Luo", "abstract": "Vision-Language Models (VLMs) have achieved impressive progress in perceiving and describing visual environments. However, their ability to proactively reason and act based solely on visual inputs, without explicit textual prompts, remains underexplored. We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios. Each action is labeled with an Action Prioritization Level (APL) and a proactive-reactive type to assess models' human-aligned reasoning and value sensitivity. We evaluate 29 VLMs on VisualActBench and find that while frontier models like GPT4o demonstrate relatively strong performance, a significant gap remains compared to human-level reasoning, particularly in generating proactive, high-priority actions. Our results highlight limitations in current VLMs' ability to interpret complex context, anticipate outcomes, and align with human decision-making frameworks. VisualActBench establishes a comprehensive foundation for assessing and improving the real-world readiness of proactive, vision-centric AI agents.", "link": "http://arxiv.org/abs/2512.09907v1", "date": "2025-12-10", "relevancy": 2.3476, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.594}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.594}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisualActBench%3A%20Can%20VLMs%20See%20and%20Act%20like%20a%20Human%3F&body=Title%3A%20VisualActBench%3A%20Can%20VLMs%20See%20and%20Act%20like%20a%20Human%3F%0AAuthor%3A%20Daoan%20Zhang%20and%20Pai%20Liu%20and%20Xiaofei%20Zhou%20and%20Yuan%20Ge%20and%20Guangchen%20Lan%20and%20Jing%20Bi%20and%20Christopher%20Brinton%20and%20Ehsan%20Hoque%20and%20Jiebo%20Luo%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20impressive%20progress%20in%20perceiving%20and%20describing%20visual%20environments.%20However%2C%20their%20ability%20to%20proactively%20reason%20and%20act%20based%20solely%20on%20visual%20inputs%2C%20without%20explicit%20textual%20prompts%2C%20remains%20underexplored.%20We%20introduce%20a%20new%20task%2C%20Visual%20Action%20Reasoning%2C%20and%20propose%20VisualActBench%2C%20a%20large-scale%20benchmark%20comprising%201%2C074%20videos%20and%203%2C733%20human-annotated%20actions%20across%20four%20real-world%20scenarios.%20Each%20action%20is%20labeled%20with%20an%20Action%20Prioritization%20Level%20%28APL%29%20and%20a%20proactive-reactive%20type%20to%20assess%20models%27%20human-aligned%20reasoning%20and%20value%20sensitivity.%20We%20evaluate%2029%20VLMs%20on%20VisualActBench%20and%20find%20that%20while%20frontier%20models%20like%20GPT4o%20demonstrate%20relatively%20strong%20performance%2C%20a%20significant%20gap%20remains%20compared%20to%20human-level%20reasoning%2C%20particularly%20in%20generating%20proactive%2C%20high-priority%20actions.%20Our%20results%20highlight%20limitations%20in%20current%20VLMs%27%20ability%20to%20interpret%20complex%20context%2C%20anticipate%20outcomes%2C%20and%20align%20with%20human%20decision-making%20frameworks.%20VisualActBench%20establishes%20a%20comprehensive%20foundation%20for%20assessing%20and%20improving%20the%20real-world%20readiness%20of%20proactive%2C%20vision-centric%20AI%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualActBench%253A%2520Can%2520VLMs%2520See%2520and%2520Act%2520like%2520a%2520Human%253F%26entry.906535625%3DDaoan%2520Zhang%2520and%2520Pai%2520Liu%2520and%2520Xiaofei%2520Zhou%2520and%2520Yuan%2520Ge%2520and%2520Guangchen%2520Lan%2520and%2520Jing%2520Bi%2520and%2520Christopher%2520Brinton%2520and%2520Ehsan%2520Hoque%2520and%2520Jiebo%2520Luo%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520have%2520achieved%2520impressive%2520progress%2520in%2520perceiving%2520and%2520describing%2520visual%2520environments.%2520However%252C%2520their%2520ability%2520to%2520proactively%2520reason%2520and%2520act%2520based%2520solely%2520on%2520visual%2520inputs%252C%2520without%2520explicit%2520textual%2520prompts%252C%2520remains%2520underexplored.%2520We%2520introduce%2520a%2520new%2520task%252C%2520Visual%2520Action%2520Reasoning%252C%2520and%2520propose%2520VisualActBench%252C%2520a%2520large-scale%2520benchmark%2520comprising%25201%252C074%2520videos%2520and%25203%252C733%2520human-annotated%2520actions%2520across%2520four%2520real-world%2520scenarios.%2520Each%2520action%2520is%2520labeled%2520with%2520an%2520Action%2520Prioritization%2520Level%2520%2528APL%2529%2520and%2520a%2520proactive-reactive%2520type%2520to%2520assess%2520models%2527%2520human-aligned%2520reasoning%2520and%2520value%2520sensitivity.%2520We%2520evaluate%252029%2520VLMs%2520on%2520VisualActBench%2520and%2520find%2520that%2520while%2520frontier%2520models%2520like%2520GPT4o%2520demonstrate%2520relatively%2520strong%2520performance%252C%2520a%2520significant%2520gap%2520remains%2520compared%2520to%2520human-level%2520reasoning%252C%2520particularly%2520in%2520generating%2520proactive%252C%2520high-priority%2520actions.%2520Our%2520results%2520highlight%2520limitations%2520in%2520current%2520VLMs%2527%2520ability%2520to%2520interpret%2520complex%2520context%252C%2520anticipate%2520outcomes%252C%2520and%2520align%2520with%2520human%2520decision-making%2520frameworks.%2520VisualActBench%2520establishes%2520a%2520comprehensive%2520foundation%2520for%2520assessing%2520and%2520improving%2520the%2520real-world%2520readiness%2520of%2520proactive%252C%2520vision-centric%2520AI%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisualActBench%3A%20Can%20VLMs%20See%20and%20Act%20like%20a%20Human%3F&entry.906535625=Daoan%20Zhang%20and%20Pai%20Liu%20and%20Xiaofei%20Zhou%20and%20Yuan%20Ge%20and%20Guangchen%20Lan%20and%20Jing%20Bi%20and%20Christopher%20Brinton%20and%20Ehsan%20Hoque%20and%20Jiebo%20Luo&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20impressive%20progress%20in%20perceiving%20and%20describing%20visual%20environments.%20However%2C%20their%20ability%20to%20proactively%20reason%20and%20act%20based%20solely%20on%20visual%20inputs%2C%20without%20explicit%20textual%20prompts%2C%20remains%20underexplored.%20We%20introduce%20a%20new%20task%2C%20Visual%20Action%20Reasoning%2C%20and%20propose%20VisualActBench%2C%20a%20large-scale%20benchmark%20comprising%201%2C074%20videos%20and%203%2C733%20human-annotated%20actions%20across%20four%20real-world%20scenarios.%20Each%20action%20is%20labeled%20with%20an%20Action%20Prioritization%20Level%20%28APL%29%20and%20a%20proactive-reactive%20type%20to%20assess%20models%27%20human-aligned%20reasoning%20and%20value%20sensitivity.%20We%20evaluate%2029%20VLMs%20on%20VisualActBench%20and%20find%20that%20while%20frontier%20models%20like%20GPT4o%20demonstrate%20relatively%20strong%20performance%2C%20a%20significant%20gap%20remains%20compared%20to%20human-level%20reasoning%2C%20particularly%20in%20generating%20proactive%2C%20high-priority%20actions.%20Our%20results%20highlight%20limitations%20in%20current%20VLMs%27%20ability%20to%20interpret%20complex%20context%2C%20anticipate%20outcomes%2C%20and%20align%20with%20human%20decision-making%20frameworks.%20VisualActBench%20establishes%20a%20comprehensive%20foundation%20for%20assessing%20and%20improving%20the%20real-world%20readiness%20of%20proactive%2C%20vision-centric%20AI%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2512.09907v1&entry.124074799=Read"},
{"title": "Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition", "author": "Vladimir Balditsyn and Philippe Lalanda and German Vega and St\u00e9phanie Chollet", "abstract": "The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.", "link": "http://arxiv.org/abs/2512.09775v1", "date": "2025-12-10", "relevancy": 2.3471, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6045}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5968}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Uncertainty%20in%20Machine%20Learning-Based%20Pervasive%20Systems%3A%20Application%20to%20Human%20Activity%20Recognition&body=Title%3A%20Quantifying%20Uncertainty%20in%20Machine%20Learning-Based%20Pervasive%20Systems%3A%20Application%20to%20Human%20Activity%20Recognition%0AAuthor%3A%20Vladimir%20Balditsyn%20and%20Philippe%20Lalanda%20and%20German%20Vega%20and%20St%C3%A9phanie%20Chollet%0AAbstract%3A%20The%20recent%20convergence%20of%20pervasive%20computing%20and%20machine%20learning%20has%20given%20rise%20to%20numerous%20services%2C%20impacting%20almost%20all%20areas%20of%20economic%20and%20social%20activity.%20However%2C%20the%20use%20of%20AI%20techniques%20precludes%20certain%20standard%20software%20development%20practices%2C%20which%20emphasize%20rigorous%20testing%20to%20ensure%20the%20elimination%20of%20all%20bugs%20and%20adherence%20to%20well-defined%20specifications.%20ML%20models%20are%20trained%20on%20numerous%20high-dimensional%20examples%20rather%20than%20being%20manually%20coded.%20Consequently%2C%20the%20boundaries%20of%20their%20operating%20range%20are%20uncertain%2C%20and%20they%20cannot%20guarantee%20absolute%20error-free%20performance.%20In%20this%20paper%2C%20we%20propose%20to%20quantify%20uncertainty%20in%20ML-based%20systems.%20To%20achieve%20this%2C%20we%20propose%20to%20adapt%20and%20jointly%20utilize%20a%20set%20of%20selected%20techniques%20to%20evaluate%20the%20relevance%20of%20model%20predictions%20at%20runtime.%20We%20apply%20and%20evaluate%20these%20proposals%20in%20the%20highly%20heterogeneous%20and%20evolving%20domain%20of%20Human%20Activity%20Recognition%20%28HAR%29.%20The%20results%20presented%20demonstrate%20the%20relevance%20of%20the%20approach%2C%20and%20we%20discuss%20in%20detail%20the%20assistance%20provided%20to%20domain%20experts.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520Uncertainty%2520in%2520Machine%2520Learning-Based%2520Pervasive%2520Systems%253A%2520Application%2520to%2520Human%2520Activity%2520Recognition%26entry.906535625%3DVladimir%2520Balditsyn%2520and%2520Philippe%2520Lalanda%2520and%2520German%2520Vega%2520and%2520St%25C3%25A9phanie%2520Chollet%26entry.1292438233%3DThe%2520recent%2520convergence%2520of%2520pervasive%2520computing%2520and%2520machine%2520learning%2520has%2520given%2520rise%2520to%2520numerous%2520services%252C%2520impacting%2520almost%2520all%2520areas%2520of%2520economic%2520and%2520social%2520activity.%2520However%252C%2520the%2520use%2520of%2520AI%2520techniques%2520precludes%2520certain%2520standard%2520software%2520development%2520practices%252C%2520which%2520emphasize%2520rigorous%2520testing%2520to%2520ensure%2520the%2520elimination%2520of%2520all%2520bugs%2520and%2520adherence%2520to%2520well-defined%2520specifications.%2520ML%2520models%2520are%2520trained%2520on%2520numerous%2520high-dimensional%2520examples%2520rather%2520than%2520being%2520manually%2520coded.%2520Consequently%252C%2520the%2520boundaries%2520of%2520their%2520operating%2520range%2520are%2520uncertain%252C%2520and%2520they%2520cannot%2520guarantee%2520absolute%2520error-free%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520quantify%2520uncertainty%2520in%2520ML-based%2520systems.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520to%2520adapt%2520and%2520jointly%2520utilize%2520a%2520set%2520of%2520selected%2520techniques%2520to%2520evaluate%2520the%2520relevance%2520of%2520model%2520predictions%2520at%2520runtime.%2520We%2520apply%2520and%2520evaluate%2520these%2520proposals%2520in%2520the%2520highly%2520heterogeneous%2520and%2520evolving%2520domain%2520of%2520Human%2520Activity%2520Recognition%2520%2528HAR%2529.%2520The%2520results%2520presented%2520demonstrate%2520the%2520relevance%2520of%2520the%2520approach%252C%2520and%2520we%2520discuss%2520in%2520detail%2520the%2520assistance%2520provided%2520to%2520domain%2520experts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Uncertainty%20in%20Machine%20Learning-Based%20Pervasive%20Systems%3A%20Application%20to%20Human%20Activity%20Recognition&entry.906535625=Vladimir%20Balditsyn%20and%20Philippe%20Lalanda%20and%20German%20Vega%20and%20St%C3%A9phanie%20Chollet&entry.1292438233=The%20recent%20convergence%20of%20pervasive%20computing%20and%20machine%20learning%20has%20given%20rise%20to%20numerous%20services%2C%20impacting%20almost%20all%20areas%20of%20economic%20and%20social%20activity.%20However%2C%20the%20use%20of%20AI%20techniques%20precludes%20certain%20standard%20software%20development%20practices%2C%20which%20emphasize%20rigorous%20testing%20to%20ensure%20the%20elimination%20of%20all%20bugs%20and%20adherence%20to%20well-defined%20specifications.%20ML%20models%20are%20trained%20on%20numerous%20high-dimensional%20examples%20rather%20than%20being%20manually%20coded.%20Consequently%2C%20the%20boundaries%20of%20their%20operating%20range%20are%20uncertain%2C%20and%20they%20cannot%20guarantee%20absolute%20error-free%20performance.%20In%20this%20paper%2C%20we%20propose%20to%20quantify%20uncertainty%20in%20ML-based%20systems.%20To%20achieve%20this%2C%20we%20propose%20to%20adapt%20and%20jointly%20utilize%20a%20set%20of%20selected%20techniques%20to%20evaluate%20the%20relevance%20of%20model%20predictions%20at%20runtime.%20We%20apply%20and%20evaluate%20these%20proposals%20in%20the%20highly%20heterogeneous%20and%20evolving%20domain%20of%20Human%20Activity%20Recognition%20%28HAR%29.%20The%20results%20presented%20demonstrate%20the%20relevance%20of%20the%20approach%2C%20and%20we%20discuss%20in%20detail%20the%20assistance%20provided%20to%20domain%20experts.&entry.1838667208=http%3A//arxiv.org/abs/2512.09775v1&entry.124074799=Read"},
{"title": "AugLift: Uncertainty Aware Depth Descriptors for Robust 2D to 3D Pose Lifting", "author": "Nikolai Warner and Wenjin Zhang and Hamid Badiozamani and Irfan Essa and Apaar Sadhwani", "abstract": "Lifting based 3D human pose estimators infer 3D joints from 2D keypoints, but often struggle to generalize to real world settings with noisy 2D detections. We revisit the input to lifting and propose AugLift, a simple augmentation of standard lifting that enriches each 2D keypoint (x, y) with an Uncertainty Aware Depth Descriptor (UADD). We run a single off the shelf monocular depth estimator to obtain a depth map, and for every keypoint with detector confidence c we extract depth statistics from its confidence scaled neighborhood, forming a compact, interpretable UADD (c, d, d_min, d_max) that captures both local geometry and reliability. AugLift is modular, requires no new sensors or architectural changes, and integrates by expanding the input layer of existing lifting models.\n  Across four datasets and four lifting architectures, AugLift boosts cross dataset (out of distribution) performance on unseen data by an average of 10.1 percent, while also improving in distribution performance by 4.0 percent as measured by MPJPE. A post hoc analysis clarifies when and why it helps: gains are largest on novel poses and significantly occluded joints, where depth statistics resolve front back ambiguities while confidence calibrates the spatial neighborhoods from which they are drawn. We also study interaction with recent image feature lifting methods and find the signals are complementary: adding UADD to image conditioned lifting yields both ID and OOD gains. A learned depth feature extension (AugLiftV2) improves performance further while trading off interpretability. Together, these results indicate that lightweight, confidence aware depth cues are a powerful plug in for robust 2D to 3D pose lifting.", "link": "http://arxiv.org/abs/2508.07112v3", "date": "2025-12-10", "relevancy": 2.3351, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.649}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5768}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AugLift%3A%20Uncertainty%20Aware%20Depth%20Descriptors%20for%20Robust%202D%20to%203D%20Pose%20Lifting&body=Title%3A%20AugLift%3A%20Uncertainty%20Aware%20Depth%20Descriptors%20for%20Robust%202D%20to%203D%20Pose%20Lifting%0AAuthor%3A%20Nikolai%20Warner%20and%20Wenjin%20Zhang%20and%20Hamid%20Badiozamani%20and%20Irfan%20Essa%20and%20Apaar%20Sadhwani%0AAbstract%3A%20Lifting%20based%203D%20human%20pose%20estimators%20infer%203D%20joints%20from%202D%20keypoints%2C%20but%20often%20struggle%20to%20generalize%20to%20real%20world%20settings%20with%20noisy%202D%20detections.%20We%20revisit%20the%20input%20to%20lifting%20and%20propose%20AugLift%2C%20a%20simple%20augmentation%20of%20standard%20lifting%20that%20enriches%20each%202D%20keypoint%20%28x%2C%20y%29%20with%20an%20Uncertainty%20Aware%20Depth%20Descriptor%20%28UADD%29.%20We%20run%20a%20single%20off%20the%20shelf%20monocular%20depth%20estimator%20to%20obtain%20a%20depth%20map%2C%20and%20for%20every%20keypoint%20with%20detector%20confidence%20c%20we%20extract%20depth%20statistics%20from%20its%20confidence%20scaled%20neighborhood%2C%20forming%20a%20compact%2C%20interpretable%20UADD%20%28c%2C%20d%2C%20d_min%2C%20d_max%29%20that%20captures%20both%20local%20geometry%20and%20reliability.%20AugLift%20is%20modular%2C%20requires%20no%20new%20sensors%20or%20architectural%20changes%2C%20and%20integrates%20by%20expanding%20the%20input%20layer%20of%20existing%20lifting%20models.%0A%20%20Across%20four%20datasets%20and%20four%20lifting%20architectures%2C%20AugLift%20boosts%20cross%20dataset%20%28out%20of%20distribution%29%20performance%20on%20unseen%20data%20by%20an%20average%20of%2010.1%20percent%2C%20while%20also%20improving%20in%20distribution%20performance%20by%204.0%20percent%20as%20measured%20by%20MPJPE.%20A%20post%20hoc%20analysis%20clarifies%20when%20and%20why%20it%20helps%3A%20gains%20are%20largest%20on%20novel%20poses%20and%20significantly%20occluded%20joints%2C%20where%20depth%20statistics%20resolve%20front%20back%20ambiguities%20while%20confidence%20calibrates%20the%20spatial%20neighborhoods%20from%20which%20they%20are%20drawn.%20We%20also%20study%20interaction%20with%20recent%20image%20feature%20lifting%20methods%20and%20find%20the%20signals%20are%20complementary%3A%20adding%20UADD%20to%20image%20conditioned%20lifting%20yields%20both%20ID%20and%20OOD%20gains.%20A%20learned%20depth%20feature%20extension%20%28AugLiftV2%29%20improves%20performance%20further%20while%20trading%20off%20interpretability.%20Together%2C%20these%20results%20indicate%20that%20lightweight%2C%20confidence%20aware%20depth%20cues%20are%20a%20powerful%20plug%20in%20for%20robust%202D%20to%203D%20pose%20lifting.%0ALink%3A%20http%3A//arxiv.org/abs/2508.07112v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugLift%253A%2520Uncertainty%2520Aware%2520Depth%2520Descriptors%2520for%2520Robust%25202D%2520to%25203D%2520Pose%2520Lifting%26entry.906535625%3DNikolai%2520Warner%2520and%2520Wenjin%2520Zhang%2520and%2520Hamid%2520Badiozamani%2520and%2520Irfan%2520Essa%2520and%2520Apaar%2520Sadhwani%26entry.1292438233%3DLifting%2520based%25203D%2520human%2520pose%2520estimators%2520infer%25203D%2520joints%2520from%25202D%2520keypoints%252C%2520but%2520often%2520struggle%2520to%2520generalize%2520to%2520real%2520world%2520settings%2520with%2520noisy%25202D%2520detections.%2520We%2520revisit%2520the%2520input%2520to%2520lifting%2520and%2520propose%2520AugLift%252C%2520a%2520simple%2520augmentation%2520of%2520standard%2520lifting%2520that%2520enriches%2520each%25202D%2520keypoint%2520%2528x%252C%2520y%2529%2520with%2520an%2520Uncertainty%2520Aware%2520Depth%2520Descriptor%2520%2528UADD%2529.%2520We%2520run%2520a%2520single%2520off%2520the%2520shelf%2520monocular%2520depth%2520estimator%2520to%2520obtain%2520a%2520depth%2520map%252C%2520and%2520for%2520every%2520keypoint%2520with%2520detector%2520confidence%2520c%2520we%2520extract%2520depth%2520statistics%2520from%2520its%2520confidence%2520scaled%2520neighborhood%252C%2520forming%2520a%2520compact%252C%2520interpretable%2520UADD%2520%2528c%252C%2520d%252C%2520d_min%252C%2520d_max%2529%2520that%2520captures%2520both%2520local%2520geometry%2520and%2520reliability.%2520AugLift%2520is%2520modular%252C%2520requires%2520no%2520new%2520sensors%2520or%2520architectural%2520changes%252C%2520and%2520integrates%2520by%2520expanding%2520the%2520input%2520layer%2520of%2520existing%2520lifting%2520models.%250A%2520%2520Across%2520four%2520datasets%2520and%2520four%2520lifting%2520architectures%252C%2520AugLift%2520boosts%2520cross%2520dataset%2520%2528out%2520of%2520distribution%2529%2520performance%2520on%2520unseen%2520data%2520by%2520an%2520average%2520of%252010.1%2520percent%252C%2520while%2520also%2520improving%2520in%2520distribution%2520performance%2520by%25204.0%2520percent%2520as%2520measured%2520by%2520MPJPE.%2520A%2520post%2520hoc%2520analysis%2520clarifies%2520when%2520and%2520why%2520it%2520helps%253A%2520gains%2520are%2520largest%2520on%2520novel%2520poses%2520and%2520significantly%2520occluded%2520joints%252C%2520where%2520depth%2520statistics%2520resolve%2520front%2520back%2520ambiguities%2520while%2520confidence%2520calibrates%2520the%2520spatial%2520neighborhoods%2520from%2520which%2520they%2520are%2520drawn.%2520We%2520also%2520study%2520interaction%2520with%2520recent%2520image%2520feature%2520lifting%2520methods%2520and%2520find%2520the%2520signals%2520are%2520complementary%253A%2520adding%2520UADD%2520to%2520image%2520conditioned%2520lifting%2520yields%2520both%2520ID%2520and%2520OOD%2520gains.%2520A%2520learned%2520depth%2520feature%2520extension%2520%2528AugLiftV2%2529%2520improves%2520performance%2520further%2520while%2520trading%2520off%2520interpretability.%2520Together%252C%2520these%2520results%2520indicate%2520that%2520lightweight%252C%2520confidence%2520aware%2520depth%2520cues%2520are%2520a%2520powerful%2520plug%2520in%2520for%2520robust%25202D%2520to%25203D%2520pose%2520lifting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07112v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AugLift%3A%20Uncertainty%20Aware%20Depth%20Descriptors%20for%20Robust%202D%20to%203D%20Pose%20Lifting&entry.906535625=Nikolai%20Warner%20and%20Wenjin%20Zhang%20and%20Hamid%20Badiozamani%20and%20Irfan%20Essa%20and%20Apaar%20Sadhwani&entry.1292438233=Lifting%20based%203D%20human%20pose%20estimators%20infer%203D%20joints%20from%202D%20keypoints%2C%20but%20often%20struggle%20to%20generalize%20to%20real%20world%20settings%20with%20noisy%202D%20detections.%20We%20revisit%20the%20input%20to%20lifting%20and%20propose%20AugLift%2C%20a%20simple%20augmentation%20of%20standard%20lifting%20that%20enriches%20each%202D%20keypoint%20%28x%2C%20y%29%20with%20an%20Uncertainty%20Aware%20Depth%20Descriptor%20%28UADD%29.%20We%20run%20a%20single%20off%20the%20shelf%20monocular%20depth%20estimator%20to%20obtain%20a%20depth%20map%2C%20and%20for%20every%20keypoint%20with%20detector%20confidence%20c%20we%20extract%20depth%20statistics%20from%20its%20confidence%20scaled%20neighborhood%2C%20forming%20a%20compact%2C%20interpretable%20UADD%20%28c%2C%20d%2C%20d_min%2C%20d_max%29%20that%20captures%20both%20local%20geometry%20and%20reliability.%20AugLift%20is%20modular%2C%20requires%20no%20new%20sensors%20or%20architectural%20changes%2C%20and%20integrates%20by%20expanding%20the%20input%20layer%20of%20existing%20lifting%20models.%0A%20%20Across%20four%20datasets%20and%20four%20lifting%20architectures%2C%20AugLift%20boosts%20cross%20dataset%20%28out%20of%20distribution%29%20performance%20on%20unseen%20data%20by%20an%20average%20of%2010.1%20percent%2C%20while%20also%20improving%20in%20distribution%20performance%20by%204.0%20percent%20as%20measured%20by%20MPJPE.%20A%20post%20hoc%20analysis%20clarifies%20when%20and%20why%20it%20helps%3A%20gains%20are%20largest%20on%20novel%20poses%20and%20significantly%20occluded%20joints%2C%20where%20depth%20statistics%20resolve%20front%20back%20ambiguities%20while%20confidence%20calibrates%20the%20spatial%20neighborhoods%20from%20which%20they%20are%20drawn.%20We%20also%20study%20interaction%20with%20recent%20image%20feature%20lifting%20methods%20and%20find%20the%20signals%20are%20complementary%3A%20adding%20UADD%20to%20image%20conditioned%20lifting%20yields%20both%20ID%20and%20OOD%20gains.%20A%20learned%20depth%20feature%20extension%20%28AugLiftV2%29%20improves%20performance%20further%20while%20trading%20off%20interpretability.%20Together%2C%20these%20results%20indicate%20that%20lightweight%2C%20confidence%20aware%20depth%20cues%20are%20a%20powerful%20plug%20in%20for%20robust%202D%20to%203D%20pose%20lifting.&entry.1838667208=http%3A//arxiv.org/abs/2508.07112v3&entry.124074799=Read"},
{"title": "Matrix-game 2.0: An open-source real-time and streaming interactive world model", "author": "Xianglong He and Chunli Peng and Zexiang Liu and Boyang Wang and Yifan Zhang and Qi Cui and Fei Kang and Biao Jiang and Mengyin An and Yangyang Ren and Baixin Xu and Hao-Xiang Guo and Kaixiong Gong and Size Wu and Wei Li and Xuchen Song and Yang Liu and Yangguang Li and Yahui Zhou", "abstract": "Recent advances in interactive video generations have demonstrated diffusion model's potential as world models by capturing complex physical dynamics and interactive behaviors. However, existing interactive world models depend on bidirectional attention and lengthy inference steps, severely limiting real-time performance. Consequently, they are hard to simulate real-world dynamics, where outcomes must update instantaneously based on historical context and current actions. To address this, we present Matrix-Game 2.0, an interactive world model generates long videos on-the-fly via few-step auto-regressive diffusion. Our framework consists of three key components: (1) A scalable data production pipeline for Unreal Engine and GTA5 environments to effectively produce massive amounts (about 1200 hours) of video data with diverse interaction annotations; (2) An action injection module that enables frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step distillation based on the casual architecture for real-time and streaming video generation. Matrix Game 2.0 can generate high-quality minute-level videos across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our model weights and codebase to advance research in interactive world modeling.", "link": "http://arxiv.org/abs/2508.13009v3", "date": "2025-12-10", "relevancy": 2.3302, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5945}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5807}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matrix-game%202.0%3A%20An%20open-source%20real-time%20and%20streaming%20interactive%20world%20model&body=Title%3A%20Matrix-game%202.0%3A%20An%20open-source%20real-time%20and%20streaming%20interactive%20world%20model%0AAuthor%3A%20Xianglong%20He%20and%20Chunli%20Peng%20and%20Zexiang%20Liu%20and%20Boyang%20Wang%20and%20Yifan%20Zhang%20and%20Qi%20Cui%20and%20Fei%20Kang%20and%20Biao%20Jiang%20and%20Mengyin%20An%20and%20Yangyang%20Ren%20and%20Baixin%20Xu%20and%20Hao-Xiang%20Guo%20and%20Kaixiong%20Gong%20and%20Size%20Wu%20and%20Wei%20Li%20and%20Xuchen%20Song%20and%20Yang%20Liu%20and%20Yangguang%20Li%20and%20Yahui%20Zhou%0AAbstract%3A%20Recent%20advances%20in%20interactive%20video%20generations%20have%20demonstrated%20diffusion%20model%27s%20potential%20as%20world%20models%20by%20capturing%20complex%20physical%20dynamics%20and%20interactive%20behaviors.%20However%2C%20existing%20interactive%20world%20models%20depend%20on%20bidirectional%20attention%20and%20lengthy%20inference%20steps%2C%20severely%20limiting%20real-time%20performance.%20Consequently%2C%20they%20are%20hard%20to%20simulate%20real-world%20dynamics%2C%20where%20outcomes%20must%20update%20instantaneously%20based%20on%20historical%20context%20and%20current%20actions.%20To%20address%20this%2C%20we%20present%20Matrix-Game%202.0%2C%20an%20interactive%20world%20model%20generates%20long%20videos%20on-the-fly%20via%20few-step%20auto-regressive%20diffusion.%20Our%20framework%20consists%20of%20three%20key%20components%3A%20%281%29%20A%20scalable%20data%20production%20pipeline%20for%20Unreal%20Engine%20and%20GTA5%20environments%20to%20effectively%20produce%20massive%20amounts%20%28about%201200%20hours%29%20of%20video%20data%20with%20diverse%20interaction%20annotations%3B%20%282%29%20An%20action%20injection%20module%20that%20enables%20frame-level%20mouse%20and%20keyboard%20inputs%20as%20interactive%20conditions%3B%20%283%29%20A%20few-step%20distillation%20based%20on%20the%20casual%20architecture%20for%20real-time%20and%20streaming%20video%20generation.%20Matrix%20Game%202.0%20can%20generate%20high-quality%20minute-level%20videos%20across%20diverse%20scenes%20at%20an%20ultra-fast%20speed%20of%2025%20FPS.%20We%20open-source%20our%20model%20weights%20and%20codebase%20to%20advance%20research%20in%20interactive%20world%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2508.13009v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatrix-game%25202.0%253A%2520An%2520open-source%2520real-time%2520and%2520streaming%2520interactive%2520world%2520model%26entry.906535625%3DXianglong%2520He%2520and%2520Chunli%2520Peng%2520and%2520Zexiang%2520Liu%2520and%2520Boyang%2520Wang%2520and%2520Yifan%2520Zhang%2520and%2520Qi%2520Cui%2520and%2520Fei%2520Kang%2520and%2520Biao%2520Jiang%2520and%2520Mengyin%2520An%2520and%2520Yangyang%2520Ren%2520and%2520Baixin%2520Xu%2520and%2520Hao-Xiang%2520Guo%2520and%2520Kaixiong%2520Gong%2520and%2520Size%2520Wu%2520and%2520Wei%2520Li%2520and%2520Xuchen%2520Song%2520and%2520Yang%2520Liu%2520and%2520Yangguang%2520Li%2520and%2520Yahui%2520Zhou%26entry.1292438233%3DRecent%2520advances%2520in%2520interactive%2520video%2520generations%2520have%2520demonstrated%2520diffusion%2520model%2527s%2520potential%2520as%2520world%2520models%2520by%2520capturing%2520complex%2520physical%2520dynamics%2520and%2520interactive%2520behaviors.%2520However%252C%2520existing%2520interactive%2520world%2520models%2520depend%2520on%2520bidirectional%2520attention%2520and%2520lengthy%2520inference%2520steps%252C%2520severely%2520limiting%2520real-time%2520performance.%2520Consequently%252C%2520they%2520are%2520hard%2520to%2520simulate%2520real-world%2520dynamics%252C%2520where%2520outcomes%2520must%2520update%2520instantaneously%2520based%2520on%2520historical%2520context%2520and%2520current%2520actions.%2520To%2520address%2520this%252C%2520we%2520present%2520Matrix-Game%25202.0%252C%2520an%2520interactive%2520world%2520model%2520generates%2520long%2520videos%2520on-the-fly%2520via%2520few-step%2520auto-regressive%2520diffusion.%2520Our%2520framework%2520consists%2520of%2520three%2520key%2520components%253A%2520%25281%2529%2520A%2520scalable%2520data%2520production%2520pipeline%2520for%2520Unreal%2520Engine%2520and%2520GTA5%2520environments%2520to%2520effectively%2520produce%2520massive%2520amounts%2520%2528about%25201200%2520hours%2529%2520of%2520video%2520data%2520with%2520diverse%2520interaction%2520annotations%253B%2520%25282%2529%2520An%2520action%2520injection%2520module%2520that%2520enables%2520frame-level%2520mouse%2520and%2520keyboard%2520inputs%2520as%2520interactive%2520conditions%253B%2520%25283%2529%2520A%2520few-step%2520distillation%2520based%2520on%2520the%2520casual%2520architecture%2520for%2520real-time%2520and%2520streaming%2520video%2520generation.%2520Matrix%2520Game%25202.0%2520can%2520generate%2520high-quality%2520minute-level%2520videos%2520across%2520diverse%2520scenes%2520at%2520an%2520ultra-fast%2520speed%2520of%252025%2520FPS.%2520We%2520open-source%2520our%2520model%2520weights%2520and%2520codebase%2520to%2520advance%2520research%2520in%2520interactive%2520world%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13009v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matrix-game%202.0%3A%20An%20open-source%20real-time%20and%20streaming%20interactive%20world%20model&entry.906535625=Xianglong%20He%20and%20Chunli%20Peng%20and%20Zexiang%20Liu%20and%20Boyang%20Wang%20and%20Yifan%20Zhang%20and%20Qi%20Cui%20and%20Fei%20Kang%20and%20Biao%20Jiang%20and%20Mengyin%20An%20and%20Yangyang%20Ren%20and%20Baixin%20Xu%20and%20Hao-Xiang%20Guo%20and%20Kaixiong%20Gong%20and%20Size%20Wu%20and%20Wei%20Li%20and%20Xuchen%20Song%20and%20Yang%20Liu%20and%20Yangguang%20Li%20and%20Yahui%20Zhou&entry.1292438233=Recent%20advances%20in%20interactive%20video%20generations%20have%20demonstrated%20diffusion%20model%27s%20potential%20as%20world%20models%20by%20capturing%20complex%20physical%20dynamics%20and%20interactive%20behaviors.%20However%2C%20existing%20interactive%20world%20models%20depend%20on%20bidirectional%20attention%20and%20lengthy%20inference%20steps%2C%20severely%20limiting%20real-time%20performance.%20Consequently%2C%20they%20are%20hard%20to%20simulate%20real-world%20dynamics%2C%20where%20outcomes%20must%20update%20instantaneously%20based%20on%20historical%20context%20and%20current%20actions.%20To%20address%20this%2C%20we%20present%20Matrix-Game%202.0%2C%20an%20interactive%20world%20model%20generates%20long%20videos%20on-the-fly%20via%20few-step%20auto-regressive%20diffusion.%20Our%20framework%20consists%20of%20three%20key%20components%3A%20%281%29%20A%20scalable%20data%20production%20pipeline%20for%20Unreal%20Engine%20and%20GTA5%20environments%20to%20effectively%20produce%20massive%20amounts%20%28about%201200%20hours%29%20of%20video%20data%20with%20diverse%20interaction%20annotations%3B%20%282%29%20An%20action%20injection%20module%20that%20enables%20frame-level%20mouse%20and%20keyboard%20inputs%20as%20interactive%20conditions%3B%20%283%29%20A%20few-step%20distillation%20based%20on%20the%20casual%20architecture%20for%20real-time%20and%20streaming%20video%20generation.%20Matrix%20Game%202.0%20can%20generate%20high-quality%20minute-level%20videos%20across%20diverse%20scenes%20at%20an%20ultra-fast%20speed%20of%2025%20FPS.%20We%20open-source%20our%20model%20weights%20and%20codebase%20to%20advance%20research%20in%20interactive%20world%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2508.13009v3&entry.124074799=Read"},
{"title": "NordFKB: a fine-grained benchmark dataset for geospatial AI in Norway", "author": "Sander Riis\u00f8en Jyhne and Aditya Gupta and Ben Worsley and Marianne Andersen and Ivar Oveland and Alexander Salveson Nossum", "abstract": "We present NordFKB, a fine-grained benchmark dataset for geospatial AI in Norway, derived from the authoritative, highly accurate, national Felles KartdataBase (FKB). The dataset contains high-resolution orthophotos paired with detailed annotations for 36 semantic classes, including both per-class binary segmentation masks in GeoTIFF format and COCO-style bounding box annotations. Data is collected from seven geographically diverse areas, ensuring variation in climate, topography, and urbanization. Only tiles containing at least one annotated object are included, and training/validation splits are created through random sampling across areas to ensure representative class and context distributions. Human expert review and quality control ensures high annotation accuracy. Alongside the dataset, we release a benchmarking repository with standardized evaluation protocols and tools for semantic segmentation and object detection, enabling reproducible and comparable research. NordFKB provides a robust foundation for advancing AI methods in mapping, land administration, and spatial planning, and paves the way for future expansions in coverage, temporal scope, and data modalities.", "link": "http://arxiv.org/abs/2512.09913v1", "date": "2025-12-10", "relevancy": 2.3283, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4851}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4644}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NordFKB%3A%20a%20fine-grained%20benchmark%20dataset%20for%20geospatial%20AI%20in%20Norway&body=Title%3A%20NordFKB%3A%20a%20fine-grained%20benchmark%20dataset%20for%20geospatial%20AI%20in%20Norway%0AAuthor%3A%20Sander%20Riis%C3%B8en%20Jyhne%20and%20Aditya%20Gupta%20and%20Ben%20Worsley%20and%20Marianne%20Andersen%20and%20Ivar%20Oveland%20and%20Alexander%20Salveson%20Nossum%0AAbstract%3A%20We%20present%20NordFKB%2C%20a%20fine-grained%20benchmark%20dataset%20for%20geospatial%20AI%20in%20Norway%2C%20derived%20from%20the%20authoritative%2C%20highly%20accurate%2C%20national%20Felles%20KartdataBase%20%28FKB%29.%20The%20dataset%20contains%20high-resolution%20orthophotos%20paired%20with%20detailed%20annotations%20for%2036%20semantic%20classes%2C%20including%20both%20per-class%20binary%20segmentation%20masks%20in%20GeoTIFF%20format%20and%20COCO-style%20bounding%20box%20annotations.%20Data%20is%20collected%20from%20seven%20geographically%20diverse%20areas%2C%20ensuring%20variation%20in%20climate%2C%20topography%2C%20and%20urbanization.%20Only%20tiles%20containing%20at%20least%20one%20annotated%20object%20are%20included%2C%20and%20training/validation%20splits%20are%20created%20through%20random%20sampling%20across%20areas%20to%20ensure%20representative%20class%20and%20context%20distributions.%20Human%20expert%20review%20and%20quality%20control%20ensures%20high%20annotation%20accuracy.%20Alongside%20the%20dataset%2C%20we%20release%20a%20benchmarking%20repository%20with%20standardized%20evaluation%20protocols%20and%20tools%20for%20semantic%20segmentation%20and%20object%20detection%2C%20enabling%20reproducible%20and%20comparable%20research.%20NordFKB%20provides%20a%20robust%20foundation%20for%20advancing%20AI%20methods%20in%20mapping%2C%20land%20administration%2C%20and%20spatial%20planning%2C%20and%20paves%20the%20way%20for%20future%20expansions%20in%20coverage%2C%20temporal%20scope%2C%20and%20data%20modalities.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNordFKB%253A%2520a%2520fine-grained%2520benchmark%2520dataset%2520for%2520geospatial%2520AI%2520in%2520Norway%26entry.906535625%3DSander%2520Riis%25C3%25B8en%2520Jyhne%2520and%2520Aditya%2520Gupta%2520and%2520Ben%2520Worsley%2520and%2520Marianne%2520Andersen%2520and%2520Ivar%2520Oveland%2520and%2520Alexander%2520Salveson%2520Nossum%26entry.1292438233%3DWe%2520present%2520NordFKB%252C%2520a%2520fine-grained%2520benchmark%2520dataset%2520for%2520geospatial%2520AI%2520in%2520Norway%252C%2520derived%2520from%2520the%2520authoritative%252C%2520highly%2520accurate%252C%2520national%2520Felles%2520KartdataBase%2520%2528FKB%2529.%2520The%2520dataset%2520contains%2520high-resolution%2520orthophotos%2520paired%2520with%2520detailed%2520annotations%2520for%252036%2520semantic%2520classes%252C%2520including%2520both%2520per-class%2520binary%2520segmentation%2520masks%2520in%2520GeoTIFF%2520format%2520and%2520COCO-style%2520bounding%2520box%2520annotations.%2520Data%2520is%2520collected%2520from%2520seven%2520geographically%2520diverse%2520areas%252C%2520ensuring%2520variation%2520in%2520climate%252C%2520topography%252C%2520and%2520urbanization.%2520Only%2520tiles%2520containing%2520at%2520least%2520one%2520annotated%2520object%2520are%2520included%252C%2520and%2520training/validation%2520splits%2520are%2520created%2520through%2520random%2520sampling%2520across%2520areas%2520to%2520ensure%2520representative%2520class%2520and%2520context%2520distributions.%2520Human%2520expert%2520review%2520and%2520quality%2520control%2520ensures%2520high%2520annotation%2520accuracy.%2520Alongside%2520the%2520dataset%252C%2520we%2520release%2520a%2520benchmarking%2520repository%2520with%2520standardized%2520evaluation%2520protocols%2520and%2520tools%2520for%2520semantic%2520segmentation%2520and%2520object%2520detection%252C%2520enabling%2520reproducible%2520and%2520comparable%2520research.%2520NordFKB%2520provides%2520a%2520robust%2520foundation%2520for%2520advancing%2520AI%2520methods%2520in%2520mapping%252C%2520land%2520administration%252C%2520and%2520spatial%2520planning%252C%2520and%2520paves%2520the%2520way%2520for%2520future%2520expansions%2520in%2520coverage%252C%2520temporal%2520scope%252C%2520and%2520data%2520modalities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NordFKB%3A%20a%20fine-grained%20benchmark%20dataset%20for%20geospatial%20AI%20in%20Norway&entry.906535625=Sander%20Riis%C3%B8en%20Jyhne%20and%20Aditya%20Gupta%20and%20Ben%20Worsley%20and%20Marianne%20Andersen%20and%20Ivar%20Oveland%20and%20Alexander%20Salveson%20Nossum&entry.1292438233=We%20present%20NordFKB%2C%20a%20fine-grained%20benchmark%20dataset%20for%20geospatial%20AI%20in%20Norway%2C%20derived%20from%20the%20authoritative%2C%20highly%20accurate%2C%20national%20Felles%20KartdataBase%20%28FKB%29.%20The%20dataset%20contains%20high-resolution%20orthophotos%20paired%20with%20detailed%20annotations%20for%2036%20semantic%20classes%2C%20including%20both%20per-class%20binary%20segmentation%20masks%20in%20GeoTIFF%20format%20and%20COCO-style%20bounding%20box%20annotations.%20Data%20is%20collected%20from%20seven%20geographically%20diverse%20areas%2C%20ensuring%20variation%20in%20climate%2C%20topography%2C%20and%20urbanization.%20Only%20tiles%20containing%20at%20least%20one%20annotated%20object%20are%20included%2C%20and%20training/validation%20splits%20are%20created%20through%20random%20sampling%20across%20areas%20to%20ensure%20representative%20class%20and%20context%20distributions.%20Human%20expert%20review%20and%20quality%20control%20ensures%20high%20annotation%20accuracy.%20Alongside%20the%20dataset%2C%20we%20release%20a%20benchmarking%20repository%20with%20standardized%20evaluation%20protocols%20and%20tools%20for%20semantic%20segmentation%20and%20object%20detection%2C%20enabling%20reproducible%20and%20comparable%20research.%20NordFKB%20provides%20a%20robust%20foundation%20for%20advancing%20AI%20methods%20in%20mapping%2C%20land%20administration%2C%20and%20spatial%20planning%2C%20and%20paves%20the%20way%20for%20future%20expansions%20in%20coverage%2C%20temporal%20scope%2C%20and%20data%20modalities.&entry.1838667208=http%3A//arxiv.org/abs/2512.09913v1&entry.124074799=Read"},
{"title": "CS3D: An Efficient Facial Expression Recognition via Event Vision", "author": "Zhe Wang and Qijin Song and Yucen Peng and Weibang Bai", "abstract": "Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\\% of the original C3D required on the same device.", "link": "http://arxiv.org/abs/2512.09592v1", "date": "2025-12-10", "relevancy": 2.3138, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CS3D%3A%20An%20Efficient%20Facial%20Expression%20Recognition%20via%20Event%20Vision&body=Title%3A%20CS3D%3A%20An%20Efficient%20Facial%20Expression%20Recognition%20via%20Event%20Vision%0AAuthor%3A%20Zhe%20Wang%20and%20Qijin%20Song%20and%20Yucen%20Peng%20and%20Weibang%20Bai%0AAbstract%3A%20Responsive%20and%20accurate%20facial%20expression%20recognition%20is%20crucial%20to%20human-robot%20interaction%20for%20daily%20service%20robots.%20Nowadays%2C%20event%20cameras%20are%20becoming%20more%20widely%20adopted%20as%20they%20surpass%20RGB%20cameras%20in%20capturing%20facial%20expression%20changes%20due%20to%20their%20high%20temporal%20resolution%2C%20low%20latency%2C%20computational%20efficiency%2C%20and%20robustness%20in%20low-light%20conditions.%20Despite%20these%20advantages%2C%20event-based%20approaches%20still%20encounter%20practical%20challenges%2C%20particularly%20in%20adopting%20mainstream%20deep%20learning%20models.%20Traditional%20deep%20learning%20methods%20for%20facial%20expression%20analysis%20are%20energy-intensive%2C%20making%20them%20difficult%20to%20deploy%20on%20edge%20computing%20devices%20and%20thereby%20increasing%20costs%2C%20especially%20for%20high-frequency%2C%20dynamic%2C%20event%20vision-based%20approaches.%20To%20address%20this%20challenging%20issue%2C%20we%20proposed%20the%20CS3D%20framework%20by%20decomposing%20the%20Convolutional%203D%20method%20to%20reduce%20the%20computational%20complexity%20and%20energy%20consumption.%20Additionally%2C%20by%20utilizing%20soft%20spiking%20neurons%20and%20a%20spatial-temporal%20attention%20mechanism%2C%20the%20ability%20to%20retain%20information%20is%20enhanced%2C%20thus%20improving%20the%20accuracy%20of%20facial%20expression%20detection.%20Experimental%20results%20indicate%20that%20our%20proposed%20CS3D%20method%20attains%20higher%20accuracy%20on%20multiple%20datasets%20compared%20to%20architectures%20such%20as%20the%20RNN%2C%20Transformer%2C%20and%20C3D%2C%20while%20the%20energy%20consumption%20of%20the%20CS3D%20method%20is%20just%2021.97%5C%25%20of%20the%20original%20C3D%20required%20on%20the%20same%20device.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCS3D%253A%2520An%2520Efficient%2520Facial%2520Expression%2520Recognition%2520via%2520Event%2520Vision%26entry.906535625%3DZhe%2520Wang%2520and%2520Qijin%2520Song%2520and%2520Yucen%2520Peng%2520and%2520Weibang%2520Bai%26entry.1292438233%3DResponsive%2520and%2520accurate%2520facial%2520expression%2520recognition%2520is%2520crucial%2520to%2520human-robot%2520interaction%2520for%2520daily%2520service%2520robots.%2520Nowadays%252C%2520event%2520cameras%2520are%2520becoming%2520more%2520widely%2520adopted%2520as%2520they%2520surpass%2520RGB%2520cameras%2520in%2520capturing%2520facial%2520expression%2520changes%2520due%2520to%2520their%2520high%2520temporal%2520resolution%252C%2520low%2520latency%252C%2520computational%2520efficiency%252C%2520and%2520robustness%2520in%2520low-light%2520conditions.%2520Despite%2520these%2520advantages%252C%2520event-based%2520approaches%2520still%2520encounter%2520practical%2520challenges%252C%2520particularly%2520in%2520adopting%2520mainstream%2520deep%2520learning%2520models.%2520Traditional%2520deep%2520learning%2520methods%2520for%2520facial%2520expression%2520analysis%2520are%2520energy-intensive%252C%2520making%2520them%2520difficult%2520to%2520deploy%2520on%2520edge%2520computing%2520devices%2520and%2520thereby%2520increasing%2520costs%252C%2520especially%2520for%2520high-frequency%252C%2520dynamic%252C%2520event%2520vision-based%2520approaches.%2520To%2520address%2520this%2520challenging%2520issue%252C%2520we%2520proposed%2520the%2520CS3D%2520framework%2520by%2520decomposing%2520the%2520Convolutional%25203D%2520method%2520to%2520reduce%2520the%2520computational%2520complexity%2520and%2520energy%2520consumption.%2520Additionally%252C%2520by%2520utilizing%2520soft%2520spiking%2520neurons%2520and%2520a%2520spatial-temporal%2520attention%2520mechanism%252C%2520the%2520ability%2520to%2520retain%2520information%2520is%2520enhanced%252C%2520thus%2520improving%2520the%2520accuracy%2520of%2520facial%2520expression%2520detection.%2520Experimental%2520results%2520indicate%2520that%2520our%2520proposed%2520CS3D%2520method%2520attains%2520higher%2520accuracy%2520on%2520multiple%2520datasets%2520compared%2520to%2520architectures%2520such%2520as%2520the%2520RNN%252C%2520Transformer%252C%2520and%2520C3D%252C%2520while%2520the%2520energy%2520consumption%2520of%2520the%2520CS3D%2520method%2520is%2520just%252021.97%255C%2525%2520of%2520the%2520original%2520C3D%2520required%2520on%2520the%2520same%2520device.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CS3D%3A%20An%20Efficient%20Facial%20Expression%20Recognition%20via%20Event%20Vision&entry.906535625=Zhe%20Wang%20and%20Qijin%20Song%20and%20Yucen%20Peng%20and%20Weibang%20Bai&entry.1292438233=Responsive%20and%20accurate%20facial%20expression%20recognition%20is%20crucial%20to%20human-robot%20interaction%20for%20daily%20service%20robots.%20Nowadays%2C%20event%20cameras%20are%20becoming%20more%20widely%20adopted%20as%20they%20surpass%20RGB%20cameras%20in%20capturing%20facial%20expression%20changes%20due%20to%20their%20high%20temporal%20resolution%2C%20low%20latency%2C%20computational%20efficiency%2C%20and%20robustness%20in%20low-light%20conditions.%20Despite%20these%20advantages%2C%20event-based%20approaches%20still%20encounter%20practical%20challenges%2C%20particularly%20in%20adopting%20mainstream%20deep%20learning%20models.%20Traditional%20deep%20learning%20methods%20for%20facial%20expression%20analysis%20are%20energy-intensive%2C%20making%20them%20difficult%20to%20deploy%20on%20edge%20computing%20devices%20and%20thereby%20increasing%20costs%2C%20especially%20for%20high-frequency%2C%20dynamic%2C%20event%20vision-based%20approaches.%20To%20address%20this%20challenging%20issue%2C%20we%20proposed%20the%20CS3D%20framework%20by%20decomposing%20the%20Convolutional%203D%20method%20to%20reduce%20the%20computational%20complexity%20and%20energy%20consumption.%20Additionally%2C%20by%20utilizing%20soft%20spiking%20neurons%20and%20a%20spatial-temporal%20attention%20mechanism%2C%20the%20ability%20to%20retain%20information%20is%20enhanced%2C%20thus%20improving%20the%20accuracy%20of%20facial%20expression%20detection.%20Experimental%20results%20indicate%20that%20our%20proposed%20CS3D%20method%20attains%20higher%20accuracy%20on%20multiple%20datasets%20compared%20to%20architectures%20such%20as%20the%20RNN%2C%20Transformer%2C%20and%20C3D%2C%20while%20the%20energy%20consumption%20of%20the%20CS3D%20method%20is%20just%2021.97%5C%25%20of%20the%20original%20C3D%20required%20on%20the%20same%20device.&entry.1838667208=http%3A//arxiv.org/abs/2512.09592v1&entry.124074799=Read"},
{"title": "Self-Organization and Spectral Mechanism of Attractor Landscapes in High-Capacity Kernel Hopfield Networks", "author": "Akira Tamamori", "abstract": "Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by unifying the geometric analysis of the attractor landscape with the spectral theory of kernel machines. Using a novel metric, \\textit{Pinnacle Sharpness}, we first uncover a rich phase diagram of attractor stability, identifying a \\textit{Ridge of Optimization} where the network achieves maximal robustness under high-load conditions. Phenomenologically, this ridge is characterized by a \\textit{Force Antagonism}, where a strong driving force is balanced by a collective feedback force. Theoretically, we reveal that this phenomenon arises from a specific reorganization of the weight spectrum, which we term \\textit{Spectral Concentration}. Unlike a simple rank-1 collapse, our analysis shows that the network on the ridge self-organizes into a critical state: the leading eigenvalue is amplified to maximize global stability (Direct Force), while the trailing eigenvalues are preserved to maintain high memory capacity (Indirect Force). These findings provide a complete physical picture of how high-capacity associative memories are formed, demonstrating that optimal performance is achieved by tuning the system to a spectral Goldilocks zone between rank collapse and diffusion.", "link": "http://arxiv.org/abs/2511.13053v6", "date": "2025-12-10", "relevancy": 2.3127, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4701}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.465}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Organization%20and%20Spectral%20Mechanism%20of%20Attractor%20Landscapes%20in%20High-Capacity%20Kernel%20Hopfield%20Networks&body=Title%3A%20Self-Organization%20and%20Spectral%20Mechanism%20of%20Attractor%20Landscapes%20in%20High-Capacity%20Kernel%20Hopfield%20Networks%0AAuthor%3A%20Akira%20Tamamori%0AAbstract%3A%20Kernel-based%20learning%20methods%20can%20dramatically%20increase%20the%20storage%20capacity%20of%20Hopfield%20networks%2C%20yet%20the%20dynamical%20mechanism%20behind%20this%20enhancement%20remains%20poorly%20understood.%20We%20address%20this%20gap%20by%20unifying%20the%20geometric%20analysis%20of%20the%20attractor%20landscape%20with%20the%20spectral%20theory%20of%20kernel%20machines.%20Using%20a%20novel%20metric%2C%20%5Ctextit%7BPinnacle%20Sharpness%7D%2C%20we%20first%20uncover%20a%20rich%20phase%20diagram%20of%20attractor%20stability%2C%20identifying%20a%20%5Ctextit%7BRidge%20of%20Optimization%7D%20where%20the%20network%20achieves%20maximal%20robustness%20under%20high-load%20conditions.%20Phenomenologically%2C%20this%20ridge%20is%20characterized%20by%20a%20%5Ctextit%7BForce%20Antagonism%7D%2C%20where%20a%20strong%20driving%20force%20is%20balanced%20by%20a%20collective%20feedback%20force.%20Theoretically%2C%20we%20reveal%20that%20this%20phenomenon%20arises%20from%20a%20specific%20reorganization%20of%20the%20weight%20spectrum%2C%20which%20we%20term%20%5Ctextit%7BSpectral%20Concentration%7D.%20Unlike%20a%20simple%20rank-1%20collapse%2C%20our%20analysis%20shows%20that%20the%20network%20on%20the%20ridge%20self-organizes%20into%20a%20critical%20state%3A%20the%20leading%20eigenvalue%20is%20amplified%20to%20maximize%20global%20stability%20%28Direct%20Force%29%2C%20while%20the%20trailing%20eigenvalues%20are%20preserved%20to%20maintain%20high%20memory%20capacity%20%28Indirect%20Force%29.%20These%20findings%20provide%20a%20complete%20physical%20picture%20of%20how%20high-capacity%20associative%20memories%20are%20formed%2C%20demonstrating%20that%20optimal%20performance%20is%20achieved%20by%20tuning%20the%20system%20to%20a%20spectral%20Goldilocks%20zone%20between%20rank%20collapse%20and%20diffusion.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13053v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Organization%2520and%2520Spectral%2520Mechanism%2520of%2520Attractor%2520Landscapes%2520in%2520High-Capacity%2520Kernel%2520Hopfield%2520Networks%26entry.906535625%3DAkira%2520Tamamori%26entry.1292438233%3DKernel-based%2520learning%2520methods%2520can%2520dramatically%2520increase%2520the%2520storage%2520capacity%2520of%2520Hopfield%2520networks%252C%2520yet%2520the%2520dynamical%2520mechanism%2520behind%2520this%2520enhancement%2520remains%2520poorly%2520understood.%2520We%2520address%2520this%2520gap%2520by%2520unifying%2520the%2520geometric%2520analysis%2520of%2520the%2520attractor%2520landscape%2520with%2520the%2520spectral%2520theory%2520of%2520kernel%2520machines.%2520Using%2520a%2520novel%2520metric%252C%2520%255Ctextit%257BPinnacle%2520Sharpness%257D%252C%2520we%2520first%2520uncover%2520a%2520rich%2520phase%2520diagram%2520of%2520attractor%2520stability%252C%2520identifying%2520a%2520%255Ctextit%257BRidge%2520of%2520Optimization%257D%2520where%2520the%2520network%2520achieves%2520maximal%2520robustness%2520under%2520high-load%2520conditions.%2520Phenomenologically%252C%2520this%2520ridge%2520is%2520characterized%2520by%2520a%2520%255Ctextit%257BForce%2520Antagonism%257D%252C%2520where%2520a%2520strong%2520driving%2520force%2520is%2520balanced%2520by%2520a%2520collective%2520feedback%2520force.%2520Theoretically%252C%2520we%2520reveal%2520that%2520this%2520phenomenon%2520arises%2520from%2520a%2520specific%2520reorganization%2520of%2520the%2520weight%2520spectrum%252C%2520which%2520we%2520term%2520%255Ctextit%257BSpectral%2520Concentration%257D.%2520Unlike%2520a%2520simple%2520rank-1%2520collapse%252C%2520our%2520analysis%2520shows%2520that%2520the%2520network%2520on%2520the%2520ridge%2520self-organizes%2520into%2520a%2520critical%2520state%253A%2520the%2520leading%2520eigenvalue%2520is%2520amplified%2520to%2520maximize%2520global%2520stability%2520%2528Direct%2520Force%2529%252C%2520while%2520the%2520trailing%2520eigenvalues%2520are%2520preserved%2520to%2520maintain%2520high%2520memory%2520capacity%2520%2528Indirect%2520Force%2529.%2520These%2520findings%2520provide%2520a%2520complete%2520physical%2520picture%2520of%2520how%2520high-capacity%2520associative%2520memories%2520are%2520formed%252C%2520demonstrating%2520that%2520optimal%2520performance%2520is%2520achieved%2520by%2520tuning%2520the%2520system%2520to%2520a%2520spectral%2520Goldilocks%2520zone%2520between%2520rank%2520collapse%2520and%2520diffusion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13053v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Organization%20and%20Spectral%20Mechanism%20of%20Attractor%20Landscapes%20in%20High-Capacity%20Kernel%20Hopfield%20Networks&entry.906535625=Akira%20Tamamori&entry.1292438233=Kernel-based%20learning%20methods%20can%20dramatically%20increase%20the%20storage%20capacity%20of%20Hopfield%20networks%2C%20yet%20the%20dynamical%20mechanism%20behind%20this%20enhancement%20remains%20poorly%20understood.%20We%20address%20this%20gap%20by%20unifying%20the%20geometric%20analysis%20of%20the%20attractor%20landscape%20with%20the%20spectral%20theory%20of%20kernel%20machines.%20Using%20a%20novel%20metric%2C%20%5Ctextit%7BPinnacle%20Sharpness%7D%2C%20we%20first%20uncover%20a%20rich%20phase%20diagram%20of%20attractor%20stability%2C%20identifying%20a%20%5Ctextit%7BRidge%20of%20Optimization%7D%20where%20the%20network%20achieves%20maximal%20robustness%20under%20high-load%20conditions.%20Phenomenologically%2C%20this%20ridge%20is%20characterized%20by%20a%20%5Ctextit%7BForce%20Antagonism%7D%2C%20where%20a%20strong%20driving%20force%20is%20balanced%20by%20a%20collective%20feedback%20force.%20Theoretically%2C%20we%20reveal%20that%20this%20phenomenon%20arises%20from%20a%20specific%20reorganization%20of%20the%20weight%20spectrum%2C%20which%20we%20term%20%5Ctextit%7BSpectral%20Concentration%7D.%20Unlike%20a%20simple%20rank-1%20collapse%2C%20our%20analysis%20shows%20that%20the%20network%20on%20the%20ridge%20self-organizes%20into%20a%20critical%20state%3A%20the%20leading%20eigenvalue%20is%20amplified%20to%20maximize%20global%20stability%20%28Direct%20Force%29%2C%20while%20the%20trailing%20eigenvalues%20are%20preserved%20to%20maintain%20high%20memory%20capacity%20%28Indirect%20Force%29.%20These%20findings%20provide%20a%20complete%20physical%20picture%20of%20how%20high-capacity%20associative%20memories%20are%20formed%2C%20demonstrating%20that%20optimal%20performance%20is%20achieved%20by%20tuning%20the%20system%20to%20a%20spectral%20Goldilocks%20zone%20between%20rank%20collapse%20and%20diffusion.&entry.1838667208=http%3A//arxiv.org/abs/2511.13053v6&entry.124074799=Read"},
{"title": "Visual Heading Prediction for Autonomous Aerial Vehicles", "author": "Reza Ahmari and Ahmad Mohammadi and Vahid Hemmati and Mohammed Mynuddin and Parham Kebria and Mahmoud Nabil Mahmoud and Xiaohong Yuan and Abdollah Homaifar", "abstract": "The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506\u00b0 and a root mean squared error of 0.1957\u00b0, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration", "link": "http://arxiv.org/abs/2512.09898v1", "date": "2025-12-10", "relevancy": 2.2997, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5856}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5723}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Heading%20Prediction%20for%20Autonomous%20Aerial%20Vehicles&body=Title%3A%20Visual%20Heading%20Prediction%20for%20Autonomous%20Aerial%20Vehicles%0AAuthor%3A%20Reza%20Ahmari%20and%20Ahmad%20Mohammadi%20and%20Vahid%20Hemmati%20and%20Mohammed%20Mynuddin%20and%20Parham%20Kebria%20and%20Mahmoud%20Nabil%20Mahmoud%20and%20Xiaohong%20Yuan%20and%20Abdollah%20Homaifar%0AAbstract%3A%20The%20integration%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%20Unmanned%20Ground%20Vehicles%20%28UGVs%29%20is%20increasingly%20central%20to%20the%20development%20of%20intelligent%20autonomous%20systems%20for%20applications%20such%20as%20search%20and%20rescue%2C%20environmental%20monitoring%2C%20and%20logistics.%20However%2C%20precise%20coordination%20between%20these%20platforms%20in%20real-time%20scenarios%20presents%20major%20challenges%2C%20particularly%20when%20external%20localization%20infrastructure%20such%20as%20GPS%20or%20GNSS%20is%20unavailable%20or%20degraded%20%5B1%5D.%20This%20paper%20proposes%20a%20vision-based%2C%20data-driven%20framework%20for%20real-time%20UAV-UGV%20integration%2C%20with%20a%20focus%20on%20robust%20UGV%20detection%20and%20heading%20angle%20prediction%20for%20navigation%20and%20coordination.%20The%20system%20employs%20a%20fine-tuned%20YOLOv5%20model%20to%20detect%20UGVs%20and%20extract%20bounding%20box%20features%2C%20which%20are%20then%20used%20by%20a%20lightweight%20artificial%20neural%20network%20%28ANN%29%20to%20estimate%20the%20UAV%27s%20required%20heading%20angle.%20A%20VICON%20motion%20capture%20system%20was%20used%20to%20generate%20ground-truth%20data%20during%20training%2C%20resulting%20in%20a%20dataset%20of%20over%2013%2C000%20annotated%20images%20collected%20in%20a%20controlled%20lab%20environment.%20The%20trained%20ANN%20achieves%20a%20mean%20absolute%20error%20of%200.1506%C2%B0%20and%20a%20root%20mean%20squared%20error%20of%200.1957%C2%B0%2C%20offering%20accurate%20heading%20angle%20predictions%20using%20only%20monocular%20camera%20inputs.%20Experimental%20evaluations%20achieve%2095%25%20accuracy%20in%20UGV%20detection.%20This%20work%20contributes%20a%20vision-based%2C%20infrastructure-%20independent%20solution%20that%20demonstrates%20strong%20potential%20for%20deployment%20in%20GPS/GNSS-denied%20environments%2C%20supporting%20reliable%20multi-agent%20coordination%20under%20realistic%20dynamic%20conditions.%20A%20demonstration%20video%20showcasing%20the%20system%27s%20real-time%20performance%2C%20including%20UGV%20detection%2C%20heading%20angle%20prediction%2C%20and%20UAV%20alignment%20under%20dynamic%20conditions%2C%20is%20available%20at%3A%20https%3A//github.com/Kooroshraf/UAV-UGV-Integration%0ALink%3A%20http%3A//arxiv.org/abs/2512.09898v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Heading%2520Prediction%2520for%2520Autonomous%2520Aerial%2520Vehicles%26entry.906535625%3DReza%2520Ahmari%2520and%2520Ahmad%2520Mohammadi%2520and%2520Vahid%2520Hemmati%2520and%2520Mohammed%2520Mynuddin%2520and%2520Parham%2520Kebria%2520and%2520Mahmoud%2520Nabil%2520Mahmoud%2520and%2520Xiaohong%2520Yuan%2520and%2520Abdollah%2520Homaifar%26entry.1292438233%3DThe%2520integration%2520of%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520and%2520Unmanned%2520Ground%2520Vehicles%2520%2528UGVs%2529%2520is%2520increasingly%2520central%2520to%2520the%2520development%2520of%2520intelligent%2520autonomous%2520systems%2520for%2520applications%2520such%2520as%2520search%2520and%2520rescue%252C%2520environmental%2520monitoring%252C%2520and%2520logistics.%2520However%252C%2520precise%2520coordination%2520between%2520these%2520platforms%2520in%2520real-time%2520scenarios%2520presents%2520major%2520challenges%252C%2520particularly%2520when%2520external%2520localization%2520infrastructure%2520such%2520as%2520GPS%2520or%2520GNSS%2520is%2520unavailable%2520or%2520degraded%2520%255B1%255D.%2520This%2520paper%2520proposes%2520a%2520vision-based%252C%2520data-driven%2520framework%2520for%2520real-time%2520UAV-UGV%2520integration%252C%2520with%2520a%2520focus%2520on%2520robust%2520UGV%2520detection%2520and%2520heading%2520angle%2520prediction%2520for%2520navigation%2520and%2520coordination.%2520The%2520system%2520employs%2520a%2520fine-tuned%2520YOLOv5%2520model%2520to%2520detect%2520UGVs%2520and%2520extract%2520bounding%2520box%2520features%252C%2520which%2520are%2520then%2520used%2520by%2520a%2520lightweight%2520artificial%2520neural%2520network%2520%2528ANN%2529%2520to%2520estimate%2520the%2520UAV%2527s%2520required%2520heading%2520angle.%2520A%2520VICON%2520motion%2520capture%2520system%2520was%2520used%2520to%2520generate%2520ground-truth%2520data%2520during%2520training%252C%2520resulting%2520in%2520a%2520dataset%2520of%2520over%252013%252C000%2520annotated%2520images%2520collected%2520in%2520a%2520controlled%2520lab%2520environment.%2520The%2520trained%2520ANN%2520achieves%2520a%2520mean%2520absolute%2520error%2520of%25200.1506%25C2%25B0%2520and%2520a%2520root%2520mean%2520squared%2520error%2520of%25200.1957%25C2%25B0%252C%2520offering%2520accurate%2520heading%2520angle%2520predictions%2520using%2520only%2520monocular%2520camera%2520inputs.%2520Experimental%2520evaluations%2520achieve%252095%2525%2520accuracy%2520in%2520UGV%2520detection.%2520This%2520work%2520contributes%2520a%2520vision-based%252C%2520infrastructure-%2520independent%2520solution%2520that%2520demonstrates%2520strong%2520potential%2520for%2520deployment%2520in%2520GPS/GNSS-denied%2520environments%252C%2520supporting%2520reliable%2520multi-agent%2520coordination%2520under%2520realistic%2520dynamic%2520conditions.%2520A%2520demonstration%2520video%2520showcasing%2520the%2520system%2527s%2520real-time%2520performance%252C%2520including%2520UGV%2520detection%252C%2520heading%2520angle%2520prediction%252C%2520and%2520UAV%2520alignment%2520under%2520dynamic%2520conditions%252C%2520is%2520available%2520at%253A%2520https%253A//github.com/Kooroshraf/UAV-UGV-Integration%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09898v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Heading%20Prediction%20for%20Autonomous%20Aerial%20Vehicles&entry.906535625=Reza%20Ahmari%20and%20Ahmad%20Mohammadi%20and%20Vahid%20Hemmati%20and%20Mohammed%20Mynuddin%20and%20Parham%20Kebria%20and%20Mahmoud%20Nabil%20Mahmoud%20and%20Xiaohong%20Yuan%20and%20Abdollah%20Homaifar&entry.1292438233=The%20integration%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%20Unmanned%20Ground%20Vehicles%20%28UGVs%29%20is%20increasingly%20central%20to%20the%20development%20of%20intelligent%20autonomous%20systems%20for%20applications%20such%20as%20search%20and%20rescue%2C%20environmental%20monitoring%2C%20and%20logistics.%20However%2C%20precise%20coordination%20between%20these%20platforms%20in%20real-time%20scenarios%20presents%20major%20challenges%2C%20particularly%20when%20external%20localization%20infrastructure%20such%20as%20GPS%20or%20GNSS%20is%20unavailable%20or%20degraded%20%5B1%5D.%20This%20paper%20proposes%20a%20vision-based%2C%20data-driven%20framework%20for%20real-time%20UAV-UGV%20integration%2C%20with%20a%20focus%20on%20robust%20UGV%20detection%20and%20heading%20angle%20prediction%20for%20navigation%20and%20coordination.%20The%20system%20employs%20a%20fine-tuned%20YOLOv5%20model%20to%20detect%20UGVs%20and%20extract%20bounding%20box%20features%2C%20which%20are%20then%20used%20by%20a%20lightweight%20artificial%20neural%20network%20%28ANN%29%20to%20estimate%20the%20UAV%27s%20required%20heading%20angle.%20A%20VICON%20motion%20capture%20system%20was%20used%20to%20generate%20ground-truth%20data%20during%20training%2C%20resulting%20in%20a%20dataset%20of%20over%2013%2C000%20annotated%20images%20collected%20in%20a%20controlled%20lab%20environment.%20The%20trained%20ANN%20achieves%20a%20mean%20absolute%20error%20of%200.1506%C2%B0%20and%20a%20root%20mean%20squared%20error%20of%200.1957%C2%B0%2C%20offering%20accurate%20heading%20angle%20predictions%20using%20only%20monocular%20camera%20inputs.%20Experimental%20evaluations%20achieve%2095%25%20accuracy%20in%20UGV%20detection.%20This%20work%20contributes%20a%20vision-based%2C%20infrastructure-%20independent%20solution%20that%20demonstrates%20strong%20potential%20for%20deployment%20in%20GPS/GNSS-denied%20environments%2C%20supporting%20reliable%20multi-agent%20coordination%20under%20realistic%20dynamic%20conditions.%20A%20demonstration%20video%20showcasing%20the%20system%27s%20real-time%20performance%2C%20including%20UGV%20detection%2C%20heading%20angle%20prediction%2C%20and%20UAV%20alignment%20under%20dynamic%20conditions%2C%20is%20available%20at%3A%20https%3A//github.com/Kooroshraf/UAV-UGV-Integration&entry.1838667208=http%3A//arxiv.org/abs/2512.09898v1&entry.124074799=Read"},
{"title": "ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics", "author": "Donato Caramia and Florian T. Pokorny and Giuseppe Triggiani and Denis Ruffino and David Naso and Paolo Roberto Massenio", "abstract": "Occlusions in robotic bin picking compromise accurate and reliable grasp planning. We present ViTA-Seg, a class-agnostic Vision Transformer framework for real-time amodal segmentation that leverages global attention to recover complete object masks, including hidden regions. We proposte two architectures: a) Single-Head for amodal mask prediction; b) Dual-Head for amodal and occluded mask prediction. We also introduce ViTA-SimData, a photo-realistic synthetic dataset tailored to industrial bin-picking scenario. Extensive experiments on two amodal benchmarks, COOCA and KINS, demonstrate that ViTA-Seg Dual Head achieves strong amodal and occlusion segmentation accuracy with computational efficiency, enabling robust, real-time robotic manipulation.", "link": "http://arxiv.org/abs/2512.09510v1", "date": "2025-12-10", "relevancy": 2.2967, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5978}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5722}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViTA-Seg%3A%20Vision%20Transformer%20for%20Amodal%20Segmentation%20in%20Robotics&body=Title%3A%20ViTA-Seg%3A%20Vision%20Transformer%20for%20Amodal%20Segmentation%20in%20Robotics%0AAuthor%3A%20Donato%20Caramia%20and%20Florian%20T.%20Pokorny%20and%20Giuseppe%20Triggiani%20and%20Denis%20Ruffino%20and%20David%20Naso%20and%20Paolo%20Roberto%20Massenio%0AAbstract%3A%20Occlusions%20in%20robotic%20bin%20picking%20compromise%20accurate%20and%20reliable%20grasp%20planning.%20We%20present%20ViTA-Seg%2C%20a%20class-agnostic%20Vision%20Transformer%20framework%20for%20real-time%20amodal%20segmentation%20that%20leverages%20global%20attention%20to%20recover%20complete%20object%20masks%2C%20including%20hidden%20regions.%20We%20proposte%20two%20architectures%3A%20a%29%20Single-Head%20for%20amodal%20mask%20prediction%3B%20b%29%20Dual-Head%20for%20amodal%20and%20occluded%20mask%20prediction.%20We%20also%20introduce%20ViTA-SimData%2C%20a%20photo-realistic%20synthetic%20dataset%20tailored%20to%20industrial%20bin-picking%20scenario.%20Extensive%20experiments%20on%20two%20amodal%20benchmarks%2C%20COOCA%20and%20KINS%2C%20demonstrate%20that%20ViTA-Seg%20Dual%20Head%20achieves%20strong%20amodal%20and%20occlusion%20segmentation%20accuracy%20with%20computational%20efficiency%2C%20enabling%20robust%2C%20real-time%20robotic%20manipulation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViTA-Seg%253A%2520Vision%2520Transformer%2520for%2520Amodal%2520Segmentation%2520in%2520Robotics%26entry.906535625%3DDonato%2520Caramia%2520and%2520Florian%2520T.%2520Pokorny%2520and%2520Giuseppe%2520Triggiani%2520and%2520Denis%2520Ruffino%2520and%2520David%2520Naso%2520and%2520Paolo%2520Roberto%2520Massenio%26entry.1292438233%3DOcclusions%2520in%2520robotic%2520bin%2520picking%2520compromise%2520accurate%2520and%2520reliable%2520grasp%2520planning.%2520We%2520present%2520ViTA-Seg%252C%2520a%2520class-agnostic%2520Vision%2520Transformer%2520framework%2520for%2520real-time%2520amodal%2520segmentation%2520that%2520leverages%2520global%2520attention%2520to%2520recover%2520complete%2520object%2520masks%252C%2520including%2520hidden%2520regions.%2520We%2520proposte%2520two%2520architectures%253A%2520a%2529%2520Single-Head%2520for%2520amodal%2520mask%2520prediction%253B%2520b%2529%2520Dual-Head%2520for%2520amodal%2520and%2520occluded%2520mask%2520prediction.%2520We%2520also%2520introduce%2520ViTA-SimData%252C%2520a%2520photo-realistic%2520synthetic%2520dataset%2520tailored%2520to%2520industrial%2520bin-picking%2520scenario.%2520Extensive%2520experiments%2520on%2520two%2520amodal%2520benchmarks%252C%2520COOCA%2520and%2520KINS%252C%2520demonstrate%2520that%2520ViTA-Seg%2520Dual%2520Head%2520achieves%2520strong%2520amodal%2520and%2520occlusion%2520segmentation%2520accuracy%2520with%2520computational%2520efficiency%252C%2520enabling%2520robust%252C%2520real-time%2520robotic%2520manipulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViTA-Seg%3A%20Vision%20Transformer%20for%20Amodal%20Segmentation%20in%20Robotics&entry.906535625=Donato%20Caramia%20and%20Florian%20T.%20Pokorny%20and%20Giuseppe%20Triggiani%20and%20Denis%20Ruffino%20and%20David%20Naso%20and%20Paolo%20Roberto%20Massenio&entry.1292438233=Occlusions%20in%20robotic%20bin%20picking%20compromise%20accurate%20and%20reliable%20grasp%20planning.%20We%20present%20ViTA-Seg%2C%20a%20class-agnostic%20Vision%20Transformer%20framework%20for%20real-time%20amodal%20segmentation%20that%20leverages%20global%20attention%20to%20recover%20complete%20object%20masks%2C%20including%20hidden%20regions.%20We%20proposte%20two%20architectures%3A%20a%29%20Single-Head%20for%20amodal%20mask%20prediction%3B%20b%29%20Dual-Head%20for%20amodal%20and%20occluded%20mask%20prediction.%20We%20also%20introduce%20ViTA-SimData%2C%20a%20photo-realistic%20synthetic%20dataset%20tailored%20to%20industrial%20bin-picking%20scenario.%20Extensive%20experiments%20on%20two%20amodal%20benchmarks%2C%20COOCA%20and%20KINS%2C%20demonstrate%20that%20ViTA-Seg%20Dual%20Head%20achieves%20strong%20amodal%20and%20occlusion%20segmentation%20accuracy%20with%20computational%20efficiency%2C%20enabling%20robust%2C%20real-time%20robotic%20manipulation.&entry.1838667208=http%3A//arxiv.org/abs/2512.09510v1&entry.124074799=Read"},
{"title": "FastPose-ViT: A Vision Transformer for Real-Time Spacecraft Pose Estimation", "author": "Pierre Ancey and Andrew Price and Saqib Javed and Mathieu Salzmann", "abstract": "Estimating the 6-degrees-of-freedom (6DoF) pose of a spacecraft from a single image is critical for autonomous operations like in-orbit servicing and space debris removal. Existing state-of-the-art methods often rely on iterative Perspective-n-Point (PnP)-based algorithms, which are computationally intensive and ill-suited for real-time deployment on resource-constrained edge devices. To overcome these limitations, we propose FastPose-ViT, a Vision Transformer (ViT)-based architecture that directly regresses the 6DoF pose. Our approach processes cropped images from object bounding boxes and introduces a novel mathematical formalism to map these localized predictions back to the full-image scale. This formalism is derived from the principles of projective geometry and the concept of \"apparent rotation\", where the model predicts an apparent rotation matrix that is then corrected to find the true orientation. We demonstrate that our method outperforms other non-PnP strategies and achieves performance competitive with state-of-the-art PnP-based techniques on the SPEED dataset. Furthermore, we validate our model's suitability for real-world space missions by quantizing it and deploying it on power-constrained edge hardware. On the NVIDIA Jetson Orin Nano, our end-to-end pipeline achieves a latency of ~75 ms per frame under sequential execution, and a non-blocking throughput of up to 33 FPS when stages are scheduled concurrently.", "link": "http://arxiv.org/abs/2512.09792v1", "date": "2025-12-10", "relevancy": 2.2841, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5746}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5744}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastPose-ViT%3A%20A%20Vision%20Transformer%20for%20Real-Time%20Spacecraft%20Pose%20Estimation&body=Title%3A%20FastPose-ViT%3A%20A%20Vision%20Transformer%20for%20Real-Time%20Spacecraft%20Pose%20Estimation%0AAuthor%3A%20Pierre%20Ancey%20and%20Andrew%20Price%20and%20Saqib%20Javed%20and%20Mathieu%20Salzmann%0AAbstract%3A%20Estimating%20the%206-degrees-of-freedom%20%286DoF%29%20pose%20of%20a%20spacecraft%20from%20a%20single%20image%20is%20critical%20for%20autonomous%20operations%20like%20in-orbit%20servicing%20and%20space%20debris%20removal.%20Existing%20state-of-the-art%20methods%20often%20rely%20on%20iterative%20Perspective-n-Point%20%28PnP%29-based%20algorithms%2C%20which%20are%20computationally%20intensive%20and%20ill-suited%20for%20real-time%20deployment%20on%20resource-constrained%20edge%20devices.%20To%20overcome%20these%20limitations%2C%20we%20propose%20FastPose-ViT%2C%20a%20Vision%20Transformer%20%28ViT%29-based%20architecture%20that%20directly%20regresses%20the%206DoF%20pose.%20Our%20approach%20processes%20cropped%20images%20from%20object%20bounding%20boxes%20and%20introduces%20a%20novel%20mathematical%20formalism%20to%20map%20these%20localized%20predictions%20back%20to%20the%20full-image%20scale.%20This%20formalism%20is%20derived%20from%20the%20principles%20of%20projective%20geometry%20and%20the%20concept%20of%20%22apparent%20rotation%22%2C%20where%20the%20model%20predicts%20an%20apparent%20rotation%20matrix%20that%20is%20then%20corrected%20to%20find%20the%20true%20orientation.%20We%20demonstrate%20that%20our%20method%20outperforms%20other%20non-PnP%20strategies%20and%20achieves%20performance%20competitive%20with%20state-of-the-art%20PnP-based%20techniques%20on%20the%20SPEED%20dataset.%20Furthermore%2C%20we%20validate%20our%20model%27s%20suitability%20for%20real-world%20space%20missions%20by%20quantizing%20it%20and%20deploying%20it%20on%20power-constrained%20edge%20hardware.%20On%20the%20NVIDIA%20Jetson%20Orin%20Nano%2C%20our%20end-to-end%20pipeline%20achieves%20a%20latency%20of%20~75%20ms%20per%20frame%20under%20sequential%20execution%2C%20and%20a%20non-blocking%20throughput%20of%20up%20to%2033%20FPS%20when%20stages%20are%20scheduled%20concurrently.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastPose-ViT%253A%2520A%2520Vision%2520Transformer%2520for%2520Real-Time%2520Spacecraft%2520Pose%2520Estimation%26entry.906535625%3DPierre%2520Ancey%2520and%2520Andrew%2520Price%2520and%2520Saqib%2520Javed%2520and%2520Mathieu%2520Salzmann%26entry.1292438233%3DEstimating%2520the%25206-degrees-of-freedom%2520%25286DoF%2529%2520pose%2520of%2520a%2520spacecraft%2520from%2520a%2520single%2520image%2520is%2520critical%2520for%2520autonomous%2520operations%2520like%2520in-orbit%2520servicing%2520and%2520space%2520debris%2520removal.%2520Existing%2520state-of-the-art%2520methods%2520often%2520rely%2520on%2520iterative%2520Perspective-n-Point%2520%2528PnP%2529-based%2520algorithms%252C%2520which%2520are%2520computationally%2520intensive%2520and%2520ill-suited%2520for%2520real-time%2520deployment%2520on%2520resource-constrained%2520edge%2520devices.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520FastPose-ViT%252C%2520a%2520Vision%2520Transformer%2520%2528ViT%2529-based%2520architecture%2520that%2520directly%2520regresses%2520the%25206DoF%2520pose.%2520Our%2520approach%2520processes%2520cropped%2520images%2520from%2520object%2520bounding%2520boxes%2520and%2520introduces%2520a%2520novel%2520mathematical%2520formalism%2520to%2520map%2520these%2520localized%2520predictions%2520back%2520to%2520the%2520full-image%2520scale.%2520This%2520formalism%2520is%2520derived%2520from%2520the%2520principles%2520of%2520projective%2520geometry%2520and%2520the%2520concept%2520of%2520%2522apparent%2520rotation%2522%252C%2520where%2520the%2520model%2520predicts%2520an%2520apparent%2520rotation%2520matrix%2520that%2520is%2520then%2520corrected%2520to%2520find%2520the%2520true%2520orientation.%2520We%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520other%2520non-PnP%2520strategies%2520and%2520achieves%2520performance%2520competitive%2520with%2520state-of-the-art%2520PnP-based%2520techniques%2520on%2520the%2520SPEED%2520dataset.%2520Furthermore%252C%2520we%2520validate%2520our%2520model%2527s%2520suitability%2520for%2520real-world%2520space%2520missions%2520by%2520quantizing%2520it%2520and%2520deploying%2520it%2520on%2520power-constrained%2520edge%2520hardware.%2520On%2520the%2520NVIDIA%2520Jetson%2520Orin%2520Nano%252C%2520our%2520end-to-end%2520pipeline%2520achieves%2520a%2520latency%2520of%2520~75%2520ms%2520per%2520frame%2520under%2520sequential%2520execution%252C%2520and%2520a%2520non-blocking%2520throughput%2520of%2520up%2520to%252033%2520FPS%2520when%2520stages%2520are%2520scheduled%2520concurrently.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastPose-ViT%3A%20A%20Vision%20Transformer%20for%20Real-Time%20Spacecraft%20Pose%20Estimation&entry.906535625=Pierre%20Ancey%20and%20Andrew%20Price%20and%20Saqib%20Javed%20and%20Mathieu%20Salzmann&entry.1292438233=Estimating%20the%206-degrees-of-freedom%20%286DoF%29%20pose%20of%20a%20spacecraft%20from%20a%20single%20image%20is%20critical%20for%20autonomous%20operations%20like%20in-orbit%20servicing%20and%20space%20debris%20removal.%20Existing%20state-of-the-art%20methods%20often%20rely%20on%20iterative%20Perspective-n-Point%20%28PnP%29-based%20algorithms%2C%20which%20are%20computationally%20intensive%20and%20ill-suited%20for%20real-time%20deployment%20on%20resource-constrained%20edge%20devices.%20To%20overcome%20these%20limitations%2C%20we%20propose%20FastPose-ViT%2C%20a%20Vision%20Transformer%20%28ViT%29-based%20architecture%20that%20directly%20regresses%20the%206DoF%20pose.%20Our%20approach%20processes%20cropped%20images%20from%20object%20bounding%20boxes%20and%20introduces%20a%20novel%20mathematical%20formalism%20to%20map%20these%20localized%20predictions%20back%20to%20the%20full-image%20scale.%20This%20formalism%20is%20derived%20from%20the%20principles%20of%20projective%20geometry%20and%20the%20concept%20of%20%22apparent%20rotation%22%2C%20where%20the%20model%20predicts%20an%20apparent%20rotation%20matrix%20that%20is%20then%20corrected%20to%20find%20the%20true%20orientation.%20We%20demonstrate%20that%20our%20method%20outperforms%20other%20non-PnP%20strategies%20and%20achieves%20performance%20competitive%20with%20state-of-the-art%20PnP-based%20techniques%20on%20the%20SPEED%20dataset.%20Furthermore%2C%20we%20validate%20our%20model%27s%20suitability%20for%20real-world%20space%20missions%20by%20quantizing%20it%20and%20deploying%20it%20on%20power-constrained%20edge%20hardware.%20On%20the%20NVIDIA%20Jetson%20Orin%20Nano%2C%20our%20end-to-end%20pipeline%20achieves%20a%20latency%20of%20~75%20ms%20per%20frame%20under%20sequential%20execution%2C%20and%20a%20non-blocking%20throughput%20of%20up%20to%2033%20FPS%20when%20stages%20are%20scheduled%20concurrently.&entry.1838667208=http%3A//arxiv.org/abs/2512.09792v1&entry.124074799=Read"},
{"title": "Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual Speech Recognition Evaluation", "author": "Vaibhav Srivastav and Steven Zheng and Eric Bezzam and Eustache Le Bihan and Adel Moumen and Sanchit Gandhi", "abstract": "Despite rapid progress, ASR evaluation remains saturated with short-form English, and efficiency is rarely reported. We present the Open ASR Leaderboard, a fully reproducible benchmark and interactive leaderboard comparing 60+ open-source and proprietary systems across 11 datasets, including a dedicated multilingual track. We standardize text normalization and report both word error rate (WER) and inverse real-time factor (RTFx), enabling fair accuracy-efficiency comparisons. For English transcription, Conformer encoders paired with LLM decoders achieve the best average WER but are slower, while CTC and TDT decoders deliver much better RTFx, making them attractive for long-form and offline use. Whisper-derived encoders fine-tuned for English improve accuracy but often trade off multilingual coverage. All code and dataset loaders are open-sourced to support transparent, extensible evaluation.", "link": "http://arxiv.org/abs/2510.06961v3", "date": "2025-12-10", "relevancy": 2.2794, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4598}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4598}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20ASR%20Leaderboard%3A%20Towards%20Reproducible%20and%20Transparent%20Multilingual%20Speech%20Recognition%20Evaluation&body=Title%3A%20Open%20ASR%20Leaderboard%3A%20Towards%20Reproducible%20and%20Transparent%20Multilingual%20Speech%20Recognition%20Evaluation%0AAuthor%3A%20Vaibhav%20Srivastav%20and%20Steven%20Zheng%20and%20Eric%20Bezzam%20and%20Eustache%20Le%20Bihan%20and%20Adel%20Moumen%20and%20Sanchit%20Gandhi%0AAbstract%3A%20Despite%20rapid%20progress%2C%20ASR%20evaluation%20remains%20saturated%20with%20short-form%20English%2C%20and%20efficiency%20is%20rarely%20reported.%20We%20present%20the%20Open%20ASR%20Leaderboard%2C%20a%20fully%20reproducible%20benchmark%20and%20interactive%20leaderboard%20comparing%2060%2B%20open-source%20and%20proprietary%20systems%20across%2011%20datasets%2C%20including%20a%20dedicated%20multilingual%20track.%20We%20standardize%20text%20normalization%20and%20report%20both%20word%20error%20rate%20%28WER%29%20and%20inverse%20real-time%20factor%20%28RTFx%29%2C%20enabling%20fair%20accuracy-efficiency%20comparisons.%20For%20English%20transcription%2C%20Conformer%20encoders%20paired%20with%20LLM%20decoders%20achieve%20the%20best%20average%20WER%20but%20are%20slower%2C%20while%20CTC%20and%20TDT%20decoders%20deliver%20much%20better%20RTFx%2C%20making%20them%20attractive%20for%20long-form%20and%20offline%20use.%20Whisper-derived%20encoders%20fine-tuned%20for%20English%20improve%20accuracy%20but%20often%20trade%20off%20multilingual%20coverage.%20All%20code%20and%20dataset%20loaders%20are%20open-sourced%20to%20support%20transparent%2C%20extensible%20evaluation.%0ALink%3A%20http%3A//arxiv.org/abs/2510.06961v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520ASR%2520Leaderboard%253A%2520Towards%2520Reproducible%2520and%2520Transparent%2520Multilingual%2520Speech%2520Recognition%2520Evaluation%26entry.906535625%3DVaibhav%2520Srivastav%2520and%2520Steven%2520Zheng%2520and%2520Eric%2520Bezzam%2520and%2520Eustache%2520Le%2520Bihan%2520and%2520Adel%2520Moumen%2520and%2520Sanchit%2520Gandhi%26entry.1292438233%3DDespite%2520rapid%2520progress%252C%2520ASR%2520evaluation%2520remains%2520saturated%2520with%2520short-form%2520English%252C%2520and%2520efficiency%2520is%2520rarely%2520reported.%2520We%2520present%2520the%2520Open%2520ASR%2520Leaderboard%252C%2520a%2520fully%2520reproducible%2520benchmark%2520and%2520interactive%2520leaderboard%2520comparing%252060%252B%2520open-source%2520and%2520proprietary%2520systems%2520across%252011%2520datasets%252C%2520including%2520a%2520dedicated%2520multilingual%2520track.%2520We%2520standardize%2520text%2520normalization%2520and%2520report%2520both%2520word%2520error%2520rate%2520%2528WER%2529%2520and%2520inverse%2520real-time%2520factor%2520%2528RTFx%2529%252C%2520enabling%2520fair%2520accuracy-efficiency%2520comparisons.%2520For%2520English%2520transcription%252C%2520Conformer%2520encoders%2520paired%2520with%2520LLM%2520decoders%2520achieve%2520the%2520best%2520average%2520WER%2520but%2520are%2520slower%252C%2520while%2520CTC%2520and%2520TDT%2520decoders%2520deliver%2520much%2520better%2520RTFx%252C%2520making%2520them%2520attractive%2520for%2520long-form%2520and%2520offline%2520use.%2520Whisper-derived%2520encoders%2520fine-tuned%2520for%2520English%2520improve%2520accuracy%2520but%2520often%2520trade%2520off%2520multilingual%2520coverage.%2520All%2520code%2520and%2520dataset%2520loaders%2520are%2520open-sourced%2520to%2520support%2520transparent%252C%2520extensible%2520evaluation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06961v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20ASR%20Leaderboard%3A%20Towards%20Reproducible%20and%20Transparent%20Multilingual%20Speech%20Recognition%20Evaluation&entry.906535625=Vaibhav%20Srivastav%20and%20Steven%20Zheng%20and%20Eric%20Bezzam%20and%20Eustache%20Le%20Bihan%20and%20Adel%20Moumen%20and%20Sanchit%20Gandhi&entry.1292438233=Despite%20rapid%20progress%2C%20ASR%20evaluation%20remains%20saturated%20with%20short-form%20English%2C%20and%20efficiency%20is%20rarely%20reported.%20We%20present%20the%20Open%20ASR%20Leaderboard%2C%20a%20fully%20reproducible%20benchmark%20and%20interactive%20leaderboard%20comparing%2060%2B%20open-source%20and%20proprietary%20systems%20across%2011%20datasets%2C%20including%20a%20dedicated%20multilingual%20track.%20We%20standardize%20text%20normalization%20and%20report%20both%20word%20error%20rate%20%28WER%29%20and%20inverse%20real-time%20factor%20%28RTFx%29%2C%20enabling%20fair%20accuracy-efficiency%20comparisons.%20For%20English%20transcription%2C%20Conformer%20encoders%20paired%20with%20LLM%20decoders%20achieve%20the%20best%20average%20WER%20but%20are%20slower%2C%20while%20CTC%20and%20TDT%20decoders%20deliver%20much%20better%20RTFx%2C%20making%20them%20attractive%20for%20long-form%20and%20offline%20use.%20Whisper-derived%20encoders%20fine-tuned%20for%20English%20improve%20accuracy%20but%20often%20trade%20off%20multilingual%20coverage.%20All%20code%20and%20dataset%20loaders%20are%20open-sourced%20to%20support%20transparent%2C%20extensible%20evaluation.&entry.1838667208=http%3A//arxiv.org/abs/2510.06961v3&entry.124074799=Read"},
{"title": "Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment", "author": "Yuan Li and Zitang Sun and Yen-Ju Chen and Shin'ya Nishida", "abstract": "Recent advances in Image Quality Assessment (IQA) have leveraged Multi-modal Large Language Models (MLLMs) to generate descriptive explanations. However, despite their strong visual perception modules, these models often fail to reliably detect basic low-level distortions such as blur, noise, and compression, and may produce inconsistent evaluations across repeated inferences. This raises an essential question: do MLLM-based IQA systems truly perceive the visual features that matter? To examine this issue, we introduce a low-level distortion perception task that requires models to classify specific distortion types. Our component-wise analysis shows that although MLLMs are structurally capable of representing such distortions, they tend to overfit training templates, leading to biases in quality scoring. As a result, critical low-level features are weakened or lost during the vision-language alignment transfer stage. Furthermore, by computing the semantic distance between visual features and corresponding semantic tokens before and after component-wise fine-tuning, we show that improving the alignment of the vision encoder dramatically enhances distortion recognition accuracy, increasing it from 14.92% to 84.43%. Overall, these findings indicate that incorporating dedicated constraints on the vision encoder can strengthen text-explainable visual representations and enable MLLM-based pipelines to produce more coherent and interpretable reasoning in vision-centric tasks.", "link": "http://arxiv.org/abs/2512.09573v1", "date": "2025-12-10", "relevancy": 2.2583, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5679}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5679}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigate%20the%20Low-level%20Visual%20Perception%20in%20Vision-Language%20based%20Image%20Quality%20Assessment&body=Title%3A%20Investigate%20the%20Low-level%20Visual%20Perception%20in%20Vision-Language%20based%20Image%20Quality%20Assessment%0AAuthor%3A%20Yuan%20Li%20and%20Zitang%20Sun%20and%20Yen-Ju%20Chen%20and%20Shin%27ya%20Nishida%0AAbstract%3A%20Recent%20advances%20in%20Image%20Quality%20Assessment%20%28IQA%29%20have%20leveraged%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20to%20generate%20descriptive%20explanations.%20However%2C%20despite%20their%20strong%20visual%20perception%20modules%2C%20these%20models%20often%20fail%20to%20reliably%20detect%20basic%20low-level%20distortions%20such%20as%20blur%2C%20noise%2C%20and%20compression%2C%20and%20may%20produce%20inconsistent%20evaluations%20across%20repeated%20inferences.%20This%20raises%20an%20essential%20question%3A%20do%20MLLM-based%20IQA%20systems%20truly%20perceive%20the%20visual%20features%20that%20matter%3F%20To%20examine%20this%20issue%2C%20we%20introduce%20a%20low-level%20distortion%20perception%20task%20that%20requires%20models%20to%20classify%20specific%20distortion%20types.%20Our%20component-wise%20analysis%20shows%20that%20although%20MLLMs%20are%20structurally%20capable%20of%20representing%20such%20distortions%2C%20they%20tend%20to%20overfit%20training%20templates%2C%20leading%20to%20biases%20in%20quality%20scoring.%20As%20a%20result%2C%20critical%20low-level%20features%20are%20weakened%20or%20lost%20during%20the%20vision-language%20alignment%20transfer%20stage.%20Furthermore%2C%20by%20computing%20the%20semantic%20distance%20between%20visual%20features%20and%20corresponding%20semantic%20tokens%20before%20and%20after%20component-wise%20fine-tuning%2C%20we%20show%20that%20improving%20the%20alignment%20of%20the%20vision%20encoder%20dramatically%20enhances%20distortion%20recognition%20accuracy%2C%20increasing%20it%20from%2014.92%25%20to%2084.43%25.%20Overall%2C%20these%20findings%20indicate%20that%20incorporating%20dedicated%20constraints%20on%20the%20vision%20encoder%20can%20strengthen%20text-explainable%20visual%20representations%20and%20enable%20MLLM-based%20pipelines%20to%20produce%20more%20coherent%20and%20interpretable%20reasoning%20in%20vision-centric%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigate%2520the%2520Low-level%2520Visual%2520Perception%2520in%2520Vision-Language%2520based%2520Image%2520Quality%2520Assessment%26entry.906535625%3DYuan%2520Li%2520and%2520Zitang%2520Sun%2520and%2520Yen-Ju%2520Chen%2520and%2520Shin%2527ya%2520Nishida%26entry.1292438233%3DRecent%2520advances%2520in%2520Image%2520Quality%2520Assessment%2520%2528IQA%2529%2520have%2520leveraged%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520to%2520generate%2520descriptive%2520explanations.%2520However%252C%2520despite%2520their%2520strong%2520visual%2520perception%2520modules%252C%2520these%2520models%2520often%2520fail%2520to%2520reliably%2520detect%2520basic%2520low-level%2520distortions%2520such%2520as%2520blur%252C%2520noise%252C%2520and%2520compression%252C%2520and%2520may%2520produce%2520inconsistent%2520evaluations%2520across%2520repeated%2520inferences.%2520This%2520raises%2520an%2520essential%2520question%253A%2520do%2520MLLM-based%2520IQA%2520systems%2520truly%2520perceive%2520the%2520visual%2520features%2520that%2520matter%253F%2520To%2520examine%2520this%2520issue%252C%2520we%2520introduce%2520a%2520low-level%2520distortion%2520perception%2520task%2520that%2520requires%2520models%2520to%2520classify%2520specific%2520distortion%2520types.%2520Our%2520component-wise%2520analysis%2520shows%2520that%2520although%2520MLLMs%2520are%2520structurally%2520capable%2520of%2520representing%2520such%2520distortions%252C%2520they%2520tend%2520to%2520overfit%2520training%2520templates%252C%2520leading%2520to%2520biases%2520in%2520quality%2520scoring.%2520As%2520a%2520result%252C%2520critical%2520low-level%2520features%2520are%2520weakened%2520or%2520lost%2520during%2520the%2520vision-language%2520alignment%2520transfer%2520stage.%2520Furthermore%252C%2520by%2520computing%2520the%2520semantic%2520distance%2520between%2520visual%2520features%2520and%2520corresponding%2520semantic%2520tokens%2520before%2520and%2520after%2520component-wise%2520fine-tuning%252C%2520we%2520show%2520that%2520improving%2520the%2520alignment%2520of%2520the%2520vision%2520encoder%2520dramatically%2520enhances%2520distortion%2520recognition%2520accuracy%252C%2520increasing%2520it%2520from%252014.92%2525%2520to%252084.43%2525.%2520Overall%252C%2520these%2520findings%2520indicate%2520that%2520incorporating%2520dedicated%2520constraints%2520on%2520the%2520vision%2520encoder%2520can%2520strengthen%2520text-explainable%2520visual%2520representations%2520and%2520enable%2520MLLM-based%2520pipelines%2520to%2520produce%2520more%2520coherent%2520and%2520interpretable%2520reasoning%2520in%2520vision-centric%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigate%20the%20Low-level%20Visual%20Perception%20in%20Vision-Language%20based%20Image%20Quality%20Assessment&entry.906535625=Yuan%20Li%20and%20Zitang%20Sun%20and%20Yen-Ju%20Chen%20and%20Shin%27ya%20Nishida&entry.1292438233=Recent%20advances%20in%20Image%20Quality%20Assessment%20%28IQA%29%20have%20leveraged%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20to%20generate%20descriptive%20explanations.%20However%2C%20despite%20their%20strong%20visual%20perception%20modules%2C%20these%20models%20often%20fail%20to%20reliably%20detect%20basic%20low-level%20distortions%20such%20as%20blur%2C%20noise%2C%20and%20compression%2C%20and%20may%20produce%20inconsistent%20evaluations%20across%20repeated%20inferences.%20This%20raises%20an%20essential%20question%3A%20do%20MLLM-based%20IQA%20systems%20truly%20perceive%20the%20visual%20features%20that%20matter%3F%20To%20examine%20this%20issue%2C%20we%20introduce%20a%20low-level%20distortion%20perception%20task%20that%20requires%20models%20to%20classify%20specific%20distortion%20types.%20Our%20component-wise%20analysis%20shows%20that%20although%20MLLMs%20are%20structurally%20capable%20of%20representing%20such%20distortions%2C%20they%20tend%20to%20overfit%20training%20templates%2C%20leading%20to%20biases%20in%20quality%20scoring.%20As%20a%20result%2C%20critical%20low-level%20features%20are%20weakened%20or%20lost%20during%20the%20vision-language%20alignment%20transfer%20stage.%20Furthermore%2C%20by%20computing%20the%20semantic%20distance%20between%20visual%20features%20and%20corresponding%20semantic%20tokens%20before%20and%20after%20component-wise%20fine-tuning%2C%20we%20show%20that%20improving%20the%20alignment%20of%20the%20vision%20encoder%20dramatically%20enhances%20distortion%20recognition%20accuracy%2C%20increasing%20it%20from%2014.92%25%20to%2084.43%25.%20Overall%2C%20these%20findings%20indicate%20that%20incorporating%20dedicated%20constraints%20on%20the%20vision%20encoder%20can%20strengthen%20text-explainable%20visual%20representations%20and%20enable%20MLLM-based%20pipelines%20to%20produce%20more%20coherent%20and%20interpretable%20reasoning%20in%20vision-centric%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.09573v1&entry.124074799=Read"},
{"title": "IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting", "author": "Tao Zhang and Yuyang Hong and Yang Xia and Kun Ding and Zeyu Zhang and Ying Wang and Shiming Xiang and Chunhong Pan", "abstract": "Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.", "link": "http://arxiv.org/abs/2512.09663v1", "date": "2025-12-10", "relevancy": 2.2434, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5688}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5688}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IF-Bench%3A%20Benchmarking%20and%20Enhancing%20MLLMs%20for%20Infrared%20Images%20with%20Generative%20Visual%20Prompting&body=Title%3A%20IF-Bench%3A%20Benchmarking%20and%20Enhancing%20MLLMs%20for%20Infrared%20Images%20with%20Generative%20Visual%20Prompting%0AAuthor%3A%20Tao%20Zhang%20and%20Yuyang%20Hong%20and%20Yang%20Xia%20and%20Kun%20Ding%20and%20Zeyu%20Zhang%20and%20Ying%20Wang%20and%20Shiming%20Xiang%20and%20Chunhong%20Pan%0AAbstract%3A%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20led%20to%20impressive%20progress%20across%20various%20benchmarks.%20However%2C%20their%20capability%20in%20understanding%20infrared%20images%20remains%20unexplored.%20To%20address%20this%20gap%2C%20we%20introduce%20IF-Bench%2C%20the%20first%20high-quality%20benchmark%20designed%20for%20evaluating%20multimodal%20understanding%20of%20infrared%20images.%20IF-Bench%20consists%20of%20499%20images%20sourced%20from%2023%20infrared%20datasets%20and%20680%20carefully%20curated%20visual%20question-answer%20pairs%2C%20covering%2010%20essential%20dimensions%20of%20image%20understanding.%20Based%20on%20this%20benchmark%2C%20we%20systematically%20evaluate%20over%2040%20open-source%20and%20closed-source%20MLLMs%2C%20employing%20cyclic%20evaluation%2C%20bilingual%20assessment%2C%20and%20hybrid%20judgment%20strategies%20to%20enhance%20the%20reliability%20of%20the%20results.%20Our%20analysis%20reveals%20how%20model%20scale%2C%20architecture%2C%20and%20inference%20paradigms%20affect%20infrared%20image%20comprehension%2C%20providing%20valuable%20insights%20for%20this%20area.%20Furthermore%2C%20we%20propose%20a%20training-free%20generative%20visual%20prompting%20%28GenViP%29%20method%2C%20which%20leverages%20advanced%20image%20editing%20models%20to%20translate%20infrared%20images%20into%20semantically%20and%20spatially%20aligned%20RGB%20counterparts%2C%20thereby%20mitigating%20domain%20distribution%20shifts.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20consistently%20yields%20significant%20performance%20improvements%20across%20a%20wide%20range%20of%20MLLMs.%20The%20benchmark%20and%20code%20are%20available%20at%20https%3A//github.com/casiatao/IF-Bench.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIF-Bench%253A%2520Benchmarking%2520and%2520Enhancing%2520MLLMs%2520for%2520Infrared%2520Images%2520with%2520Generative%2520Visual%2520Prompting%26entry.906535625%3DTao%2520Zhang%2520and%2520Yuyang%2520Hong%2520and%2520Yang%2520Xia%2520and%2520Kun%2520Ding%2520and%2520Zeyu%2520Zhang%2520and%2520Ying%2520Wang%2520and%2520Shiming%2520Xiang%2520and%2520Chunhong%2520Pan%26entry.1292438233%3DRecent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520led%2520to%2520impressive%2520progress%2520across%2520various%2520benchmarks.%2520However%252C%2520their%2520capability%2520in%2520understanding%2520infrared%2520images%2520remains%2520unexplored.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520IF-Bench%252C%2520the%2520first%2520high-quality%2520benchmark%2520designed%2520for%2520evaluating%2520multimodal%2520understanding%2520of%2520infrared%2520images.%2520IF-Bench%2520consists%2520of%2520499%2520images%2520sourced%2520from%252023%2520infrared%2520datasets%2520and%2520680%2520carefully%2520curated%2520visual%2520question-answer%2520pairs%252C%2520covering%252010%2520essential%2520dimensions%2520of%2520image%2520understanding.%2520Based%2520on%2520this%2520benchmark%252C%2520we%2520systematically%2520evaluate%2520over%252040%2520open-source%2520and%2520closed-source%2520MLLMs%252C%2520employing%2520cyclic%2520evaluation%252C%2520bilingual%2520assessment%252C%2520and%2520hybrid%2520judgment%2520strategies%2520to%2520enhance%2520the%2520reliability%2520of%2520the%2520results.%2520Our%2520analysis%2520reveals%2520how%2520model%2520scale%252C%2520architecture%252C%2520and%2520inference%2520paradigms%2520affect%2520infrared%2520image%2520comprehension%252C%2520providing%2520valuable%2520insights%2520for%2520this%2520area.%2520Furthermore%252C%2520we%2520propose%2520a%2520training-free%2520generative%2520visual%2520prompting%2520%2528GenViP%2529%2520method%252C%2520which%2520leverages%2520advanced%2520image%2520editing%2520models%2520to%2520translate%2520infrared%2520images%2520into%2520semantically%2520and%2520spatially%2520aligned%2520RGB%2520counterparts%252C%2520thereby%2520mitigating%2520domain%2520distribution%2520shifts.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520consistently%2520yields%2520significant%2520performance%2520improvements%2520across%2520a%2520wide%2520range%2520of%2520MLLMs.%2520The%2520benchmark%2520and%2520code%2520are%2520available%2520at%2520https%253A//github.com/casiatao/IF-Bench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IF-Bench%3A%20Benchmarking%20and%20Enhancing%20MLLMs%20for%20Infrared%20Images%20with%20Generative%20Visual%20Prompting&entry.906535625=Tao%20Zhang%20and%20Yuyang%20Hong%20and%20Yang%20Xia%20and%20Kun%20Ding%20and%20Zeyu%20Zhang%20and%20Ying%20Wang%20and%20Shiming%20Xiang%20and%20Chunhong%20Pan&entry.1292438233=Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20led%20to%20impressive%20progress%20across%20various%20benchmarks.%20However%2C%20their%20capability%20in%20understanding%20infrared%20images%20remains%20unexplored.%20To%20address%20this%20gap%2C%20we%20introduce%20IF-Bench%2C%20the%20first%20high-quality%20benchmark%20designed%20for%20evaluating%20multimodal%20understanding%20of%20infrared%20images.%20IF-Bench%20consists%20of%20499%20images%20sourced%20from%2023%20infrared%20datasets%20and%20680%20carefully%20curated%20visual%20question-answer%20pairs%2C%20covering%2010%20essential%20dimensions%20of%20image%20understanding.%20Based%20on%20this%20benchmark%2C%20we%20systematically%20evaluate%20over%2040%20open-source%20and%20closed-source%20MLLMs%2C%20employing%20cyclic%20evaluation%2C%20bilingual%20assessment%2C%20and%20hybrid%20judgment%20strategies%20to%20enhance%20the%20reliability%20of%20the%20results.%20Our%20analysis%20reveals%20how%20model%20scale%2C%20architecture%2C%20and%20inference%20paradigms%20affect%20infrared%20image%20comprehension%2C%20providing%20valuable%20insights%20for%20this%20area.%20Furthermore%2C%20we%20propose%20a%20training-free%20generative%20visual%20prompting%20%28GenViP%29%20method%2C%20which%20leverages%20advanced%20image%20editing%20models%20to%20translate%20infrared%20images%20into%20semantically%20and%20spatially%20aligned%20RGB%20counterparts%2C%20thereby%20mitigating%20domain%20distribution%20shifts.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20consistently%20yields%20significant%20performance%20improvements%20across%20a%20wide%20range%20of%20MLLMs.%20The%20benchmark%20and%20code%20are%20available%20at%20https%3A//github.com/casiatao/IF-Bench.&entry.1838667208=http%3A//arxiv.org/abs/2512.09663v1&entry.124074799=Read"},
{"title": "Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?", "author": "Qingyuan Liang and Zhao Zhang and Zeyu Sun and Zheng Lin and Qi Luo and Yueyi Xiao and Yizhou Chen and Yuqun Zhang and Haotian Zhang and Lu Zhang and Bin Chen and Yingfei Xiong", "abstract": "Grammar serves as a cornerstone in programming languages and software engineering, providing frameworks to define the syntactic space and program structure. Existing research demonstrates the effectiveness of grammar-based code representations in small-scale models, showing their ability to reduce syntax errors and enhance performance. However, as language models scale to the billion level or beyond, syntax-level errors become rare, making it unclear whether grammar information still provides performance benefits. To explore this, we develop a series of billion-scale GrammarCoder models, incorporating grammar rules in the code generation process. Experiments on HumanEval (+) and MBPP (+) demonstrate a notable improvement in code generation accuracy. Further analysis shows that grammar-based representations enhance LLMs' ability to discern subtle code differences, reducing semantic errors caused by minor variations. These findings suggest that grammar-based code representations remain valuable even in billion-scale models, not only by maintaining syntax correctness but also by improving semantic differentiation.", "link": "http://arxiv.org/abs/2503.05507v2", "date": "2025-12-10", "relevancy": 2.212, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4514}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grammar-Based%20Code%20Representation%3A%20Is%20It%20a%20Worthy%20Pursuit%20for%20LLMs%3F&body=Title%3A%20Grammar-Based%20Code%20Representation%3A%20Is%20It%20a%20Worthy%20Pursuit%20for%20LLMs%3F%0AAuthor%3A%20Qingyuan%20Liang%20and%20Zhao%20Zhang%20and%20Zeyu%20Sun%20and%20Zheng%20Lin%20and%20Qi%20Luo%20and%20Yueyi%20Xiao%20and%20Yizhou%20Chen%20and%20Yuqun%20Zhang%20and%20Haotian%20Zhang%20and%20Lu%20Zhang%20and%20Bin%20Chen%20and%20Yingfei%20Xiong%0AAbstract%3A%20Grammar%20serves%20as%20a%20cornerstone%20in%20programming%20languages%20and%20software%20engineering%2C%20providing%20frameworks%20to%20define%20the%20syntactic%20space%20and%20program%20structure.%20Existing%20research%20demonstrates%20the%20effectiveness%20of%20grammar-based%20code%20representations%20in%20small-scale%20models%2C%20showing%20their%20ability%20to%20reduce%20syntax%20errors%20and%20enhance%20performance.%20However%2C%20as%20language%20models%20scale%20to%20the%20billion%20level%20or%20beyond%2C%20syntax-level%20errors%20become%20rare%2C%20making%20it%20unclear%20whether%20grammar%20information%20still%20provides%20performance%20benefits.%20To%20explore%20this%2C%20we%20develop%20a%20series%20of%20billion-scale%20GrammarCoder%20models%2C%20incorporating%20grammar%20rules%20in%20the%20code%20generation%20process.%20Experiments%20on%20HumanEval%20%28%2B%29%20and%20MBPP%20%28%2B%29%20demonstrate%20a%20notable%20improvement%20in%20code%20generation%20accuracy.%20Further%20analysis%20shows%20that%20grammar-based%20representations%20enhance%20LLMs%27%20ability%20to%20discern%20subtle%20code%20differences%2C%20reducing%20semantic%20errors%20caused%20by%20minor%20variations.%20These%20findings%20suggest%20that%20grammar-based%20code%20representations%20remain%20valuable%20even%20in%20billion-scale%20models%2C%20not%20only%20by%20maintaining%20syntax%20correctness%20but%20also%20by%20improving%20semantic%20differentiation.%0ALink%3A%20http%3A//arxiv.org/abs/2503.05507v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrammar-Based%2520Code%2520Representation%253A%2520Is%2520It%2520a%2520Worthy%2520Pursuit%2520for%2520LLMs%253F%26entry.906535625%3DQingyuan%2520Liang%2520and%2520Zhao%2520Zhang%2520and%2520Zeyu%2520Sun%2520and%2520Zheng%2520Lin%2520and%2520Qi%2520Luo%2520and%2520Yueyi%2520Xiao%2520and%2520Yizhou%2520Chen%2520and%2520Yuqun%2520Zhang%2520and%2520Haotian%2520Zhang%2520and%2520Lu%2520Zhang%2520and%2520Bin%2520Chen%2520and%2520Yingfei%2520Xiong%26entry.1292438233%3DGrammar%2520serves%2520as%2520a%2520cornerstone%2520in%2520programming%2520languages%2520and%2520software%2520engineering%252C%2520providing%2520frameworks%2520to%2520define%2520the%2520syntactic%2520space%2520and%2520program%2520structure.%2520Existing%2520research%2520demonstrates%2520the%2520effectiveness%2520of%2520grammar-based%2520code%2520representations%2520in%2520small-scale%2520models%252C%2520showing%2520their%2520ability%2520to%2520reduce%2520syntax%2520errors%2520and%2520enhance%2520performance.%2520However%252C%2520as%2520language%2520models%2520scale%2520to%2520the%2520billion%2520level%2520or%2520beyond%252C%2520syntax-level%2520errors%2520become%2520rare%252C%2520making%2520it%2520unclear%2520whether%2520grammar%2520information%2520still%2520provides%2520performance%2520benefits.%2520To%2520explore%2520this%252C%2520we%2520develop%2520a%2520series%2520of%2520billion-scale%2520GrammarCoder%2520models%252C%2520incorporating%2520grammar%2520rules%2520in%2520the%2520code%2520generation%2520process.%2520Experiments%2520on%2520HumanEval%2520%2528%252B%2529%2520and%2520MBPP%2520%2528%252B%2529%2520demonstrate%2520a%2520notable%2520improvement%2520in%2520code%2520generation%2520accuracy.%2520Further%2520analysis%2520shows%2520that%2520grammar-based%2520representations%2520enhance%2520LLMs%2527%2520ability%2520to%2520discern%2520subtle%2520code%2520differences%252C%2520reducing%2520semantic%2520errors%2520caused%2520by%2520minor%2520variations.%2520These%2520findings%2520suggest%2520that%2520grammar-based%2520code%2520representations%2520remain%2520valuable%2520even%2520in%2520billion-scale%2520models%252C%2520not%2520only%2520by%2520maintaining%2520syntax%2520correctness%2520but%2520also%2520by%2520improving%2520semantic%2520differentiation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05507v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grammar-Based%20Code%20Representation%3A%20Is%20It%20a%20Worthy%20Pursuit%20for%20LLMs%3F&entry.906535625=Qingyuan%20Liang%20and%20Zhao%20Zhang%20and%20Zeyu%20Sun%20and%20Zheng%20Lin%20and%20Qi%20Luo%20and%20Yueyi%20Xiao%20and%20Yizhou%20Chen%20and%20Yuqun%20Zhang%20and%20Haotian%20Zhang%20and%20Lu%20Zhang%20and%20Bin%20Chen%20and%20Yingfei%20Xiong&entry.1292438233=Grammar%20serves%20as%20a%20cornerstone%20in%20programming%20languages%20and%20software%20engineering%2C%20providing%20frameworks%20to%20define%20the%20syntactic%20space%20and%20program%20structure.%20Existing%20research%20demonstrates%20the%20effectiveness%20of%20grammar-based%20code%20representations%20in%20small-scale%20models%2C%20showing%20their%20ability%20to%20reduce%20syntax%20errors%20and%20enhance%20performance.%20However%2C%20as%20language%20models%20scale%20to%20the%20billion%20level%20or%20beyond%2C%20syntax-level%20errors%20become%20rare%2C%20making%20it%20unclear%20whether%20grammar%20information%20still%20provides%20performance%20benefits.%20To%20explore%20this%2C%20we%20develop%20a%20series%20of%20billion-scale%20GrammarCoder%20models%2C%20incorporating%20grammar%20rules%20in%20the%20code%20generation%20process.%20Experiments%20on%20HumanEval%20%28%2B%29%20and%20MBPP%20%28%2B%29%20demonstrate%20a%20notable%20improvement%20in%20code%20generation%20accuracy.%20Further%20analysis%20shows%20that%20grammar-based%20representations%20enhance%20LLMs%27%20ability%20to%20discern%20subtle%20code%20differences%2C%20reducing%20semantic%20errors%20caused%20by%20minor%20variations.%20These%20findings%20suggest%20that%20grammar-based%20code%20representations%20remain%20valuable%20even%20in%20billion-scale%20models%2C%20not%20only%20by%20maintaining%20syntax%20correctness%20but%20also%20by%20improving%20semantic%20differentiation.&entry.1838667208=http%3A//arxiv.org/abs/2503.05507v2&entry.124074799=Read"},
{"title": "Revisiting Intermediate-Layer Matching in Knowledge Distillation: Layer-Selection Strategy Doesn't Matter (Much)", "author": "Zony Yu and Yuqiao Wen and Lili Mou", "abstract": "Knowledge distillation (KD) is a popular method of transferring knowledge from a large \"teacher\" model to a small \"student\" model. Previous work has explored various layer-selection strategies (e.g., forward matching and in-order random matching) for intermediate-layer matching in KD, where a student layer is forced to resemble a certain teacher layer. In this work, we revisit such layer-selection strategies and observe an intriguing phenomenon that layer-selection strategy does not matter (much) in intermediate-layer matching -- even seemingly nonsensical matching strategies such as reverse matching still result in surprisingly good student performance. We provide an interpretation for this phenomenon by examining the angles between teacher layers viewed from the student's perspective. Our work sheds light on KD practice, as layer-selection strategies may not be the main focus of KD system design, and vanilla forward matching works well in most setups.", "link": "http://arxiv.org/abs/2502.04499v2", "date": "2025-12-10", "relevancy": 2.1964, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4669}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4258}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Intermediate-Layer%20Matching%20in%20Knowledge%20Distillation%3A%20Layer-Selection%20Strategy%20Doesn%27t%20Matter%20%28Much%29&body=Title%3A%20Revisiting%20Intermediate-Layer%20Matching%20in%20Knowledge%20Distillation%3A%20Layer-Selection%20Strategy%20Doesn%27t%20Matter%20%28Much%29%0AAuthor%3A%20Zony%20Yu%20and%20Yuqiao%20Wen%20and%20Lili%20Mou%0AAbstract%3A%20Knowledge%20distillation%20%28KD%29%20is%20a%20popular%20method%20of%20transferring%20knowledge%20from%20a%20large%20%22teacher%22%20model%20to%20a%20small%20%22student%22%20model.%20Previous%20work%20has%20explored%20various%20layer-selection%20strategies%20%28e.g.%2C%20forward%20matching%20and%20in-order%20random%20matching%29%20for%20intermediate-layer%20matching%20in%20KD%2C%20where%20a%20student%20layer%20is%20forced%20to%20resemble%20a%20certain%20teacher%20layer.%20In%20this%20work%2C%20we%20revisit%20such%20layer-selection%20strategies%20and%20observe%20an%20intriguing%20phenomenon%20that%20layer-selection%20strategy%20does%20not%20matter%20%28much%29%20in%20intermediate-layer%20matching%20--%20even%20seemingly%20nonsensical%20matching%20strategies%20such%20as%20reverse%20matching%20still%20result%20in%20surprisingly%20good%20student%20performance.%20We%20provide%20an%20interpretation%20for%20this%20phenomenon%20by%20examining%20the%20angles%20between%20teacher%20layers%20viewed%20from%20the%20student%27s%20perspective.%20Our%20work%20sheds%20light%20on%20KD%20practice%2C%20as%20layer-selection%20strategies%20may%20not%20be%20the%20main%20focus%20of%20KD%20system%20design%2C%20and%20vanilla%20forward%20matching%20works%20well%20in%20most%20setups.%0ALink%3A%20http%3A//arxiv.org/abs/2502.04499v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Intermediate-Layer%2520Matching%2520in%2520Knowledge%2520Distillation%253A%2520Layer-Selection%2520Strategy%2520Doesn%2527t%2520Matter%2520%2528Much%2529%26entry.906535625%3DZony%2520Yu%2520and%2520Yuqiao%2520Wen%2520and%2520Lili%2520Mou%26entry.1292438233%3DKnowledge%2520distillation%2520%2528KD%2529%2520is%2520a%2520popular%2520method%2520of%2520transferring%2520knowledge%2520from%2520a%2520large%2520%2522teacher%2522%2520model%2520to%2520a%2520small%2520%2522student%2522%2520model.%2520Previous%2520work%2520has%2520explored%2520various%2520layer-selection%2520strategies%2520%2528e.g.%252C%2520forward%2520matching%2520and%2520in-order%2520random%2520matching%2529%2520for%2520intermediate-layer%2520matching%2520in%2520KD%252C%2520where%2520a%2520student%2520layer%2520is%2520forced%2520to%2520resemble%2520a%2520certain%2520teacher%2520layer.%2520In%2520this%2520work%252C%2520we%2520revisit%2520such%2520layer-selection%2520strategies%2520and%2520observe%2520an%2520intriguing%2520phenomenon%2520that%2520layer-selection%2520strategy%2520does%2520not%2520matter%2520%2528much%2529%2520in%2520intermediate-layer%2520matching%2520--%2520even%2520seemingly%2520nonsensical%2520matching%2520strategies%2520such%2520as%2520reverse%2520matching%2520still%2520result%2520in%2520surprisingly%2520good%2520student%2520performance.%2520We%2520provide%2520an%2520interpretation%2520for%2520this%2520phenomenon%2520by%2520examining%2520the%2520angles%2520between%2520teacher%2520layers%2520viewed%2520from%2520the%2520student%2527s%2520perspective.%2520Our%2520work%2520sheds%2520light%2520on%2520KD%2520practice%252C%2520as%2520layer-selection%2520strategies%2520may%2520not%2520be%2520the%2520main%2520focus%2520of%2520KD%2520system%2520design%252C%2520and%2520vanilla%2520forward%2520matching%2520works%2520well%2520in%2520most%2520setups.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04499v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Intermediate-Layer%20Matching%20in%20Knowledge%20Distillation%3A%20Layer-Selection%20Strategy%20Doesn%27t%20Matter%20%28Much%29&entry.906535625=Zony%20Yu%20and%20Yuqiao%20Wen%20and%20Lili%20Mou&entry.1292438233=Knowledge%20distillation%20%28KD%29%20is%20a%20popular%20method%20of%20transferring%20knowledge%20from%20a%20large%20%22teacher%22%20model%20to%20a%20small%20%22student%22%20model.%20Previous%20work%20has%20explored%20various%20layer-selection%20strategies%20%28e.g.%2C%20forward%20matching%20and%20in-order%20random%20matching%29%20for%20intermediate-layer%20matching%20in%20KD%2C%20where%20a%20student%20layer%20is%20forced%20to%20resemble%20a%20certain%20teacher%20layer.%20In%20this%20work%2C%20we%20revisit%20such%20layer-selection%20strategies%20and%20observe%20an%20intriguing%20phenomenon%20that%20layer-selection%20strategy%20does%20not%20matter%20%28much%29%20in%20intermediate-layer%20matching%20--%20even%20seemingly%20nonsensical%20matching%20strategies%20such%20as%20reverse%20matching%20still%20result%20in%20surprisingly%20good%20student%20performance.%20We%20provide%20an%20interpretation%20for%20this%20phenomenon%20by%20examining%20the%20angles%20between%20teacher%20layers%20viewed%20from%20the%20student%27s%20perspective.%20Our%20work%20sheds%20light%20on%20KD%20practice%2C%20as%20layer-selection%20strategies%20may%20not%20be%20the%20main%20focus%20of%20KD%20system%20design%2C%20and%20vanilla%20forward%20matching%20works%20well%20in%20most%20setups.&entry.1838667208=http%3A//arxiv.org/abs/2502.04499v2&entry.124074799=Read"},
{"title": "Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory", "author": "Akira Tamamori", "abstract": "High-capacity kernel Hopfield networks exhibit a \\textit{Ridge of Optimization} characterized by extreme stability. While previously linked to \\textit{Spectral Concentration}, its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the Edge of Stability, a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of \\textit{Dual Equilibrium} in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.", "link": "http://arxiv.org/abs/2511.23083v2", "date": "2025-12-10", "relevancy": 2.1914, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4488}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4384}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Concentration%20at%20the%20Edge%20of%20Stability%3A%20Information%20Geometry%20of%20Kernel%20Associative%20Memory&body=Title%3A%20Spectral%20Concentration%20at%20the%20Edge%20of%20Stability%3A%20Information%20Geometry%20of%20Kernel%20Associative%20Memory%0AAuthor%3A%20Akira%20Tamamori%0AAbstract%3A%20High-capacity%20kernel%20Hopfield%20networks%20exhibit%20a%20%5Ctextit%7BRidge%20of%20Optimization%7D%20characterized%20by%20extreme%20stability.%20While%20previously%20linked%20to%20%5Ctextit%7BSpectral%20Concentration%7D%2C%20its%20origin%20remains%20elusive.%20Here%2C%20we%20analyze%20the%20network%20dynamics%20on%20a%20statistical%20manifold%2C%20revealing%20that%20the%20Ridge%20corresponds%20to%20the%20Edge%20of%20Stability%2C%20a%20critical%20boundary%20where%20the%20Fisher%20Information%20Matrix%20becomes%20singular.%20We%20demonstrate%20that%20the%20apparent%20Euclidean%20force%20antagonism%20is%20a%20manifestation%20of%20%5Ctextit%7BDual%20Equilibrium%7D%20in%20the%20Riemannian%20space.%20This%20unifies%20learning%20dynamics%20and%20capacity%20via%20the%20Minimum%20Description%20Length%20principle%2C%20offering%20a%20geometric%20theory%20of%20self-organized%20criticality.%0ALink%3A%20http%3A//arxiv.org/abs/2511.23083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Concentration%2520at%2520the%2520Edge%2520of%2520Stability%253A%2520Information%2520Geometry%2520of%2520Kernel%2520Associative%2520Memory%26entry.906535625%3DAkira%2520Tamamori%26entry.1292438233%3DHigh-capacity%2520kernel%2520Hopfield%2520networks%2520exhibit%2520a%2520%255Ctextit%257BRidge%2520of%2520Optimization%257D%2520characterized%2520by%2520extreme%2520stability.%2520While%2520previously%2520linked%2520to%2520%255Ctextit%257BSpectral%2520Concentration%257D%252C%2520its%2520origin%2520remains%2520elusive.%2520Here%252C%2520we%2520analyze%2520the%2520network%2520dynamics%2520on%2520a%2520statistical%2520manifold%252C%2520revealing%2520that%2520the%2520Ridge%2520corresponds%2520to%2520the%2520Edge%2520of%2520Stability%252C%2520a%2520critical%2520boundary%2520where%2520the%2520Fisher%2520Information%2520Matrix%2520becomes%2520singular.%2520We%2520demonstrate%2520that%2520the%2520apparent%2520Euclidean%2520force%2520antagonism%2520is%2520a%2520manifestation%2520of%2520%255Ctextit%257BDual%2520Equilibrium%257D%2520in%2520the%2520Riemannian%2520space.%2520This%2520unifies%2520learning%2520dynamics%2520and%2520capacity%2520via%2520the%2520Minimum%2520Description%2520Length%2520principle%252C%2520offering%2520a%2520geometric%2520theory%2520of%2520self-organized%2520criticality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.23083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Concentration%20at%20the%20Edge%20of%20Stability%3A%20Information%20Geometry%20of%20Kernel%20Associative%20Memory&entry.906535625=Akira%20Tamamori&entry.1292438233=High-capacity%20kernel%20Hopfield%20networks%20exhibit%20a%20%5Ctextit%7BRidge%20of%20Optimization%7D%20characterized%20by%20extreme%20stability.%20While%20previously%20linked%20to%20%5Ctextit%7BSpectral%20Concentration%7D%2C%20its%20origin%20remains%20elusive.%20Here%2C%20we%20analyze%20the%20network%20dynamics%20on%20a%20statistical%20manifold%2C%20revealing%20that%20the%20Ridge%20corresponds%20to%20the%20Edge%20of%20Stability%2C%20a%20critical%20boundary%20where%20the%20Fisher%20Information%20Matrix%20becomes%20singular.%20We%20demonstrate%20that%20the%20apparent%20Euclidean%20force%20antagonism%20is%20a%20manifestation%20of%20%5Ctextit%7BDual%20Equilibrium%7D%20in%20the%20Riemannian%20space.%20This%20unifies%20learning%20dynamics%20and%20capacity%20via%20the%20Minimum%20Description%20Length%20principle%2C%20offering%20a%20geometric%20theory%20of%20self-organized%20criticality.&entry.1838667208=http%3A//arxiv.org/abs/2511.23083v2&entry.124074799=Read"},
{"title": "SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments", "author": "Haoye Lu and Pavan Seshadri and Kaheer Suleman", "abstract": "Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.", "link": "http://arxiv.org/abs/2512.09897v1", "date": "2025-12-10", "relevancy": 2.1902, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCOPE%3A%20Language%20Models%20as%20One-Time%20Teacher%20for%20Hierarchical%20Planning%20in%20Text%20Environments&body=Title%3A%20SCOPE%3A%20Language%20Models%20as%20One-Time%20Teacher%20for%20Hierarchical%20Planning%20in%20Text%20Environments%0AAuthor%3A%20Haoye%20Lu%20and%20Pavan%20Seshadri%20and%20Kaheer%20Suleman%0AAbstract%3A%20Long-term%20planning%20in%20complex%2C%20text-based%20environments%20presents%20significant%20challenges%20due%20to%20open-ended%20action%20spaces%2C%20ambiguous%20observations%2C%20and%20sparse%20feedback.%20Recent%20research%20suggests%20that%20large%20language%20models%20%28LLMs%29%20encode%20rich%20semantic%20knowledge%20about%20the%20world%2C%20which%20can%20be%20valuable%20for%20guiding%20agents%20in%20high-level%20reasoning%20and%20planning%20across%20both%20embodied%20and%20purely%20textual%20settings.%20However%2C%20existing%20approaches%20often%20depend%20heavily%20on%20querying%20LLMs%20during%20training%20and%20inference%2C%20making%20them%20computationally%20expensive%20and%20difficult%20to%20deploy%20efficiently.%20In%20addition%2C%20these%20methods%20typically%20employ%20a%20pretrained%2C%20unaltered%20LLM%20whose%20parameters%20remain%20fixed%20throughout%20training%2C%20providing%20no%20opportunity%20for%20adaptation%20to%20the%20target%20task.%20To%20address%20these%20limitations%2C%20we%20introduce%20SCOPE%20%28Subgoal-COnditioned%20Pretraining%20for%20Efficient%20planning%29%2C%20a%20one-shot%20hierarchical%20planner%20that%20leverages%20LLM-generated%20subgoals%20only%20at%20initialization%20to%20pretrain%20a%20lightweight%20student%20model.%20Unlike%20prior%20approaches%20that%20distill%20LLM%20knowledge%20by%20repeatedly%20prompting%20the%20model%20to%20adaptively%20generate%20subgoals%20during%20training%2C%20our%20method%20derives%20subgoals%20directly%20from%20example%20trajectories.%20This%20design%20removes%20the%20need%20for%20repeated%20LLM%20queries%2C%20significantly%20improving%20efficiency%2C%20though%20at%20the%20cost%20of%20reduced%20explainability%20and%20potentially%20suboptimal%20subgoals.%20Despite%20their%20suboptimality%2C%20our%20results%20on%20the%20TextCraft%20environment%20show%20that%20LLM-generated%20subgoals%20can%20still%20serve%20as%20a%20strong%20starting%20point%20for%20hierarchical%20goal%20decomposition%20in%20text-based%20planning%20tasks.%20Compared%20to%20the%20LLM-based%20hierarchical%20agent%20ADaPT%20%28Prasad%20et%20al.%2C%202024%29%2C%20which%20achieves%20a%200.52%20success%20rate%2C%20our%20method%20reaches%200.56%20and%20reduces%20inference%20time%20from%20164.4%20seconds%20to%20just%203.0%20seconds.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCOPE%253A%2520Language%2520Models%2520as%2520One-Time%2520Teacher%2520for%2520Hierarchical%2520Planning%2520in%2520Text%2520Environments%26entry.906535625%3DHaoye%2520Lu%2520and%2520Pavan%2520Seshadri%2520and%2520Kaheer%2520Suleman%26entry.1292438233%3DLong-term%2520planning%2520in%2520complex%252C%2520text-based%2520environments%2520presents%2520significant%2520challenges%2520due%2520to%2520open-ended%2520action%2520spaces%252C%2520ambiguous%2520observations%252C%2520and%2520sparse%2520feedback.%2520Recent%2520research%2520suggests%2520that%2520large%2520language%2520models%2520%2528LLMs%2529%2520encode%2520rich%2520semantic%2520knowledge%2520about%2520the%2520world%252C%2520which%2520can%2520be%2520valuable%2520for%2520guiding%2520agents%2520in%2520high-level%2520reasoning%2520and%2520planning%2520across%2520both%2520embodied%2520and%2520purely%2520textual%2520settings.%2520However%252C%2520existing%2520approaches%2520often%2520depend%2520heavily%2520on%2520querying%2520LLMs%2520during%2520training%2520and%2520inference%252C%2520making%2520them%2520computationally%2520expensive%2520and%2520difficult%2520to%2520deploy%2520efficiently.%2520In%2520addition%252C%2520these%2520methods%2520typically%2520employ%2520a%2520pretrained%252C%2520unaltered%2520LLM%2520whose%2520parameters%2520remain%2520fixed%2520throughout%2520training%252C%2520providing%2520no%2520opportunity%2520for%2520adaptation%2520to%2520the%2520target%2520task.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520SCOPE%2520%2528Subgoal-COnditioned%2520Pretraining%2520for%2520Efficient%2520planning%2529%252C%2520a%2520one-shot%2520hierarchical%2520planner%2520that%2520leverages%2520LLM-generated%2520subgoals%2520only%2520at%2520initialization%2520to%2520pretrain%2520a%2520lightweight%2520student%2520model.%2520Unlike%2520prior%2520approaches%2520that%2520distill%2520LLM%2520knowledge%2520by%2520repeatedly%2520prompting%2520the%2520model%2520to%2520adaptively%2520generate%2520subgoals%2520during%2520training%252C%2520our%2520method%2520derives%2520subgoals%2520directly%2520from%2520example%2520trajectories.%2520This%2520design%2520removes%2520the%2520need%2520for%2520repeated%2520LLM%2520queries%252C%2520significantly%2520improving%2520efficiency%252C%2520though%2520at%2520the%2520cost%2520of%2520reduced%2520explainability%2520and%2520potentially%2520suboptimal%2520subgoals.%2520Despite%2520their%2520suboptimality%252C%2520our%2520results%2520on%2520the%2520TextCraft%2520environment%2520show%2520that%2520LLM-generated%2520subgoals%2520can%2520still%2520serve%2520as%2520a%2520strong%2520starting%2520point%2520for%2520hierarchical%2520goal%2520decomposition%2520in%2520text-based%2520planning%2520tasks.%2520Compared%2520to%2520the%2520LLM-based%2520hierarchical%2520agent%2520ADaPT%2520%2528Prasad%2520et%2520al.%252C%25202024%2529%252C%2520which%2520achieves%2520a%25200.52%2520success%2520rate%252C%2520our%2520method%2520reaches%25200.56%2520and%2520reduces%2520inference%2520time%2520from%2520164.4%2520seconds%2520to%2520just%25203.0%2520seconds.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCOPE%3A%20Language%20Models%20as%20One-Time%20Teacher%20for%20Hierarchical%20Planning%20in%20Text%20Environments&entry.906535625=Haoye%20Lu%20and%20Pavan%20Seshadri%20and%20Kaheer%20Suleman&entry.1292438233=Long-term%20planning%20in%20complex%2C%20text-based%20environments%20presents%20significant%20challenges%20due%20to%20open-ended%20action%20spaces%2C%20ambiguous%20observations%2C%20and%20sparse%20feedback.%20Recent%20research%20suggests%20that%20large%20language%20models%20%28LLMs%29%20encode%20rich%20semantic%20knowledge%20about%20the%20world%2C%20which%20can%20be%20valuable%20for%20guiding%20agents%20in%20high-level%20reasoning%20and%20planning%20across%20both%20embodied%20and%20purely%20textual%20settings.%20However%2C%20existing%20approaches%20often%20depend%20heavily%20on%20querying%20LLMs%20during%20training%20and%20inference%2C%20making%20them%20computationally%20expensive%20and%20difficult%20to%20deploy%20efficiently.%20In%20addition%2C%20these%20methods%20typically%20employ%20a%20pretrained%2C%20unaltered%20LLM%20whose%20parameters%20remain%20fixed%20throughout%20training%2C%20providing%20no%20opportunity%20for%20adaptation%20to%20the%20target%20task.%20To%20address%20these%20limitations%2C%20we%20introduce%20SCOPE%20%28Subgoal-COnditioned%20Pretraining%20for%20Efficient%20planning%29%2C%20a%20one-shot%20hierarchical%20planner%20that%20leverages%20LLM-generated%20subgoals%20only%20at%20initialization%20to%20pretrain%20a%20lightweight%20student%20model.%20Unlike%20prior%20approaches%20that%20distill%20LLM%20knowledge%20by%20repeatedly%20prompting%20the%20model%20to%20adaptively%20generate%20subgoals%20during%20training%2C%20our%20method%20derives%20subgoals%20directly%20from%20example%20trajectories.%20This%20design%20removes%20the%20need%20for%20repeated%20LLM%20queries%2C%20significantly%20improving%20efficiency%2C%20though%20at%20the%20cost%20of%20reduced%20explainability%20and%20potentially%20suboptimal%20subgoals.%20Despite%20their%20suboptimality%2C%20our%20results%20on%20the%20TextCraft%20environment%20show%20that%20LLM-generated%20subgoals%20can%20still%20serve%20as%20a%20strong%20starting%20point%20for%20hierarchical%20goal%20decomposition%20in%20text-based%20planning%20tasks.%20Compared%20to%20the%20LLM-based%20hierarchical%20agent%20ADaPT%20%28Prasad%20et%20al.%2C%202024%29%2C%20which%20achieves%20a%200.52%20success%20rate%2C%20our%20method%20reaches%200.56%20and%20reduces%20inference%20time%20from%20164.4%20seconds%20to%20just%203.0%20seconds.&entry.1838667208=http%3A//arxiv.org/abs/2512.09897v1&entry.124074799=Read"},
{"title": "Rethinking Chain-of-Thought Reasoning for Videos", "author": "Yiwu Zhong and Zi-Yuan Hu and Yin Li and Liwei Wang", "abstract": "Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.", "link": "http://arxiv.org/abs/2512.09616v1", "date": "2025-12-10", "relevancy": 2.1821, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5492}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Chain-of-Thought%20Reasoning%20for%20Videos&body=Title%3A%20Rethinking%20Chain-of-Thought%20Reasoning%20for%20Videos%0AAuthor%3A%20Yiwu%20Zhong%20and%20Zi-Yuan%20Hu%20and%20Yin%20Li%20and%20Liwei%20Wang%0AAbstract%3A%20Chain-of-thought%20%28CoT%29%20reasoning%20has%20been%20highly%20successful%20in%20solving%20complex%20tasks%20in%20natural%20language%20processing%2C%20and%20recent%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20extended%20this%20paradigm%20to%20video%20reasoning.%20However%2C%20these%20models%20typically%20build%20on%20lengthy%20reasoning%20chains%20and%20large%20numbers%20of%20input%20visual%20tokens.%20Motivated%20by%20empirical%20observations%20from%20our%20benchmark%20study%2C%20we%20hypothesize%20that%20concise%20reasoning%20combined%20with%20a%20reduced%20set%20of%20visual%20tokens%20can%20be%20sufficient%20for%20effective%20video%20reasoning.%20To%20evaluate%20this%20hypothesis%2C%20we%20design%20and%20validate%20an%20efficient%20post-training%20and%20inference%20framework%20that%20enhances%20a%20video%20MLLM%27s%20reasoning%20capability.%20Our%20framework%20enables%20models%20to%20operate%20on%20compressed%20visual%20tokens%20and%20generate%20brief%20reasoning%20traces%20prior%20to%20answering.%20The%20resulting%20models%20achieve%20substantially%20improved%20inference%20efficiency%2C%20deliver%20competitive%20performance%20across%20diverse%20benchmarks%2C%20and%20avoid%20reliance%20on%20manual%20CoT%20annotations%20or%20supervised%20fine-tuning.%20Collectively%2C%20our%20results%20suggest%20that%20long%2C%20human-like%20CoT%20reasoning%20may%20not%20be%20necessary%20for%20general%20video%20reasoning%2C%20and%20that%20concise%20reasoning%20can%20be%20both%20effective%20and%20efficient.%20Our%20code%20will%20be%20released%20at%20https%3A//github.com/LaVi-Lab/Rethink_CoT_Video.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Chain-of-Thought%2520Reasoning%2520for%2520Videos%26entry.906535625%3DYiwu%2520Zhong%2520and%2520Zi-Yuan%2520Hu%2520and%2520Yin%2520Li%2520and%2520Liwei%2520Wang%26entry.1292438233%3DChain-of-thought%2520%2528CoT%2529%2520reasoning%2520has%2520been%2520highly%2520successful%2520in%2520solving%2520complex%2520tasks%2520in%2520natural%2520language%2520processing%252C%2520and%2520recent%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520extended%2520this%2520paradigm%2520to%2520video%2520reasoning.%2520However%252C%2520these%2520models%2520typically%2520build%2520on%2520lengthy%2520reasoning%2520chains%2520and%2520large%2520numbers%2520of%2520input%2520visual%2520tokens.%2520Motivated%2520by%2520empirical%2520observations%2520from%2520our%2520benchmark%2520study%252C%2520we%2520hypothesize%2520that%2520concise%2520reasoning%2520combined%2520with%2520a%2520reduced%2520set%2520of%2520visual%2520tokens%2520can%2520be%2520sufficient%2520for%2520effective%2520video%2520reasoning.%2520To%2520evaluate%2520this%2520hypothesis%252C%2520we%2520design%2520and%2520validate%2520an%2520efficient%2520post-training%2520and%2520inference%2520framework%2520that%2520enhances%2520a%2520video%2520MLLM%2527s%2520reasoning%2520capability.%2520Our%2520framework%2520enables%2520models%2520to%2520operate%2520on%2520compressed%2520visual%2520tokens%2520and%2520generate%2520brief%2520reasoning%2520traces%2520prior%2520to%2520answering.%2520The%2520resulting%2520models%2520achieve%2520substantially%2520improved%2520inference%2520efficiency%252C%2520deliver%2520competitive%2520performance%2520across%2520diverse%2520benchmarks%252C%2520and%2520avoid%2520reliance%2520on%2520manual%2520CoT%2520annotations%2520or%2520supervised%2520fine-tuning.%2520Collectively%252C%2520our%2520results%2520suggest%2520that%2520long%252C%2520human-like%2520CoT%2520reasoning%2520may%2520not%2520be%2520necessary%2520for%2520general%2520video%2520reasoning%252C%2520and%2520that%2520concise%2520reasoning%2520can%2520be%2520both%2520effective%2520and%2520efficient.%2520Our%2520code%2520will%2520be%2520released%2520at%2520https%253A//github.com/LaVi-Lab/Rethink_CoT_Video.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Chain-of-Thought%20Reasoning%20for%20Videos&entry.906535625=Yiwu%20Zhong%20and%20Zi-Yuan%20Hu%20and%20Yin%20Li%20and%20Liwei%20Wang&entry.1292438233=Chain-of-thought%20%28CoT%29%20reasoning%20has%20been%20highly%20successful%20in%20solving%20complex%20tasks%20in%20natural%20language%20processing%2C%20and%20recent%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20extended%20this%20paradigm%20to%20video%20reasoning.%20However%2C%20these%20models%20typically%20build%20on%20lengthy%20reasoning%20chains%20and%20large%20numbers%20of%20input%20visual%20tokens.%20Motivated%20by%20empirical%20observations%20from%20our%20benchmark%20study%2C%20we%20hypothesize%20that%20concise%20reasoning%20combined%20with%20a%20reduced%20set%20of%20visual%20tokens%20can%20be%20sufficient%20for%20effective%20video%20reasoning.%20To%20evaluate%20this%20hypothesis%2C%20we%20design%20and%20validate%20an%20efficient%20post-training%20and%20inference%20framework%20that%20enhances%20a%20video%20MLLM%27s%20reasoning%20capability.%20Our%20framework%20enables%20models%20to%20operate%20on%20compressed%20visual%20tokens%20and%20generate%20brief%20reasoning%20traces%20prior%20to%20answering.%20The%20resulting%20models%20achieve%20substantially%20improved%20inference%20efficiency%2C%20deliver%20competitive%20performance%20across%20diverse%20benchmarks%2C%20and%20avoid%20reliance%20on%20manual%20CoT%20annotations%20or%20supervised%20fine-tuning.%20Collectively%2C%20our%20results%20suggest%20that%20long%2C%20human-like%20CoT%20reasoning%20may%20not%20be%20necessary%20for%20general%20video%20reasoning%2C%20and%20that%20concise%20reasoning%20can%20be%20both%20effective%20and%20efficient.%20Our%20code%20will%20be%20released%20at%20https%3A//github.com/LaVi-Lab/Rethink_CoT_Video.&entry.1838667208=http%3A//arxiv.org/abs/2512.09616v1&entry.124074799=Read"},
{"title": "PathCo-LatticE: Pathology-Constrained Lattice-Of Experts Framework for Fully-supervised Few-Shot Cardiac MRI Segmentation", "author": "Mohamed Elbayumi and Mohammed S. M. Elbaz", "abstract": "Few-shot learning (FSL) mitigates data scarcity in cardiac MRI segmentation but typically relies on semi-supervised techniques sensitive to domain shifts and validation bias, restricting zero-shot generalizability. We propose PathCo-LatticE, a fully supervised FSL framework that replaces unlabeled data with pathology-guided synthetic supervision. First, our Virtual Patient Engine models continuous latent disease trajectories from sparse clinical anchors, using generative modeling to synthesize physiologically plausible, fully labeled 3D cohorts. Second, Self-Reinforcing Interleaved Validation (SIV) provides a leakage-free protocol that evaluates models online with progressively challenging synthetic samples, eliminating the need for real validation data. Finally, a dynamic Lattice-of-Experts (LoE) organizes specialized networks within a pathology-aware topology and activates the most relevant experts per input, enabling robust zero-shot generalization to unseen data without target-domain fine-tuning. We evaluated PathCo-LatticE in a strict out-of-distribution (OOD) setting, deriving all anchors and severity statistics from a single-source domain (ACDC) and performing zero-shot testing on the multi-center, multi-vendor M&Ms dataset. PathCo-LatticE outperforms four state-of-the-art FSL methods by 4.2-11% Dice starting from only 7 labeled anchors, and approaches fully supervised performance (within 1% Dice) with only 19 labeled anchors. The method shows superior harmonization across four vendors and generalization to unseen pathologies. [Code will be made publicly available].", "link": "http://arxiv.org/abs/2512.09779v1", "date": "2025-12-10", "relevancy": 2.1809, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5493}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.545}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PathCo-LatticE%3A%20Pathology-Constrained%20Lattice-Of%20Experts%20Framework%20for%20Fully-supervised%20Few-Shot%20Cardiac%20MRI%20Segmentation&body=Title%3A%20PathCo-LatticE%3A%20Pathology-Constrained%20Lattice-Of%20Experts%20Framework%20for%20Fully-supervised%20Few-Shot%20Cardiac%20MRI%20Segmentation%0AAuthor%3A%20Mohamed%20Elbayumi%20and%20Mohammed%20S.%20M.%20Elbaz%0AAbstract%3A%20Few-shot%20learning%20%28FSL%29%20mitigates%20data%20scarcity%20in%20cardiac%20MRI%20segmentation%20but%20typically%20relies%20on%20semi-supervised%20techniques%20sensitive%20to%20domain%20shifts%20and%20validation%20bias%2C%20restricting%20zero-shot%20generalizability.%20We%20propose%20PathCo-LatticE%2C%20a%20fully%20supervised%20FSL%20framework%20that%20replaces%20unlabeled%20data%20with%20pathology-guided%20synthetic%20supervision.%20First%2C%20our%20Virtual%20Patient%20Engine%20models%20continuous%20latent%20disease%20trajectories%20from%20sparse%20clinical%20anchors%2C%20using%20generative%20modeling%20to%20synthesize%20physiologically%20plausible%2C%20fully%20labeled%203D%20cohorts.%20Second%2C%20Self-Reinforcing%20Interleaved%20Validation%20%28SIV%29%20provides%20a%20leakage-free%20protocol%20that%20evaluates%20models%20online%20with%20progressively%20challenging%20synthetic%20samples%2C%20eliminating%20the%20need%20for%20real%20validation%20data.%20Finally%2C%20a%20dynamic%20Lattice-of-Experts%20%28LoE%29%20organizes%20specialized%20networks%20within%20a%20pathology-aware%20topology%20and%20activates%20the%20most%20relevant%20experts%20per%20input%2C%20enabling%20robust%20zero-shot%20generalization%20to%20unseen%20data%20without%20target-domain%20fine-tuning.%20We%20evaluated%20PathCo-LatticE%20in%20a%20strict%20out-of-distribution%20%28OOD%29%20setting%2C%20deriving%20all%20anchors%20and%20severity%20statistics%20from%20a%20single-source%20domain%20%28ACDC%29%20and%20performing%20zero-shot%20testing%20on%20the%20multi-center%2C%20multi-vendor%20M%26Ms%20dataset.%20PathCo-LatticE%20outperforms%20four%20state-of-the-art%20FSL%20methods%20by%204.2-11%25%20Dice%20starting%20from%20only%207%20labeled%20anchors%2C%20and%20approaches%20fully%20supervised%20performance%20%28within%201%25%20Dice%29%20with%20only%2019%20labeled%20anchors.%20The%20method%20shows%20superior%20harmonization%20across%20four%20vendors%20and%20generalization%20to%20unseen%20pathologies.%20%5BCode%20will%20be%20made%20publicly%20available%5D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPathCo-LatticE%253A%2520Pathology-Constrained%2520Lattice-Of%2520Experts%2520Framework%2520for%2520Fully-supervised%2520Few-Shot%2520Cardiac%2520MRI%2520Segmentation%26entry.906535625%3DMohamed%2520Elbayumi%2520and%2520Mohammed%2520S.%2520M.%2520Elbaz%26entry.1292438233%3DFew-shot%2520learning%2520%2528FSL%2529%2520mitigates%2520data%2520scarcity%2520in%2520cardiac%2520MRI%2520segmentation%2520but%2520typically%2520relies%2520on%2520semi-supervised%2520techniques%2520sensitive%2520to%2520domain%2520shifts%2520and%2520validation%2520bias%252C%2520restricting%2520zero-shot%2520generalizability.%2520We%2520propose%2520PathCo-LatticE%252C%2520a%2520fully%2520supervised%2520FSL%2520framework%2520that%2520replaces%2520unlabeled%2520data%2520with%2520pathology-guided%2520synthetic%2520supervision.%2520First%252C%2520our%2520Virtual%2520Patient%2520Engine%2520models%2520continuous%2520latent%2520disease%2520trajectories%2520from%2520sparse%2520clinical%2520anchors%252C%2520using%2520generative%2520modeling%2520to%2520synthesize%2520physiologically%2520plausible%252C%2520fully%2520labeled%25203D%2520cohorts.%2520Second%252C%2520Self-Reinforcing%2520Interleaved%2520Validation%2520%2528SIV%2529%2520provides%2520a%2520leakage-free%2520protocol%2520that%2520evaluates%2520models%2520online%2520with%2520progressively%2520challenging%2520synthetic%2520samples%252C%2520eliminating%2520the%2520need%2520for%2520real%2520validation%2520data.%2520Finally%252C%2520a%2520dynamic%2520Lattice-of-Experts%2520%2528LoE%2529%2520organizes%2520specialized%2520networks%2520within%2520a%2520pathology-aware%2520topology%2520and%2520activates%2520the%2520most%2520relevant%2520experts%2520per%2520input%252C%2520enabling%2520robust%2520zero-shot%2520generalization%2520to%2520unseen%2520data%2520without%2520target-domain%2520fine-tuning.%2520We%2520evaluated%2520PathCo-LatticE%2520in%2520a%2520strict%2520out-of-distribution%2520%2528OOD%2529%2520setting%252C%2520deriving%2520all%2520anchors%2520and%2520severity%2520statistics%2520from%2520a%2520single-source%2520domain%2520%2528ACDC%2529%2520and%2520performing%2520zero-shot%2520testing%2520on%2520the%2520multi-center%252C%2520multi-vendor%2520M%2526Ms%2520dataset.%2520PathCo-LatticE%2520outperforms%2520four%2520state-of-the-art%2520FSL%2520methods%2520by%25204.2-11%2525%2520Dice%2520starting%2520from%2520only%25207%2520labeled%2520anchors%252C%2520and%2520approaches%2520fully%2520supervised%2520performance%2520%2528within%25201%2525%2520Dice%2529%2520with%2520only%252019%2520labeled%2520anchors.%2520The%2520method%2520shows%2520superior%2520harmonization%2520across%2520four%2520vendors%2520and%2520generalization%2520to%2520unseen%2520pathologies.%2520%255BCode%2520will%2520be%2520made%2520publicly%2520available%255D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PathCo-LatticE%3A%20Pathology-Constrained%20Lattice-Of%20Experts%20Framework%20for%20Fully-supervised%20Few-Shot%20Cardiac%20MRI%20Segmentation&entry.906535625=Mohamed%20Elbayumi%20and%20Mohammed%20S.%20M.%20Elbaz&entry.1292438233=Few-shot%20learning%20%28FSL%29%20mitigates%20data%20scarcity%20in%20cardiac%20MRI%20segmentation%20but%20typically%20relies%20on%20semi-supervised%20techniques%20sensitive%20to%20domain%20shifts%20and%20validation%20bias%2C%20restricting%20zero-shot%20generalizability.%20We%20propose%20PathCo-LatticE%2C%20a%20fully%20supervised%20FSL%20framework%20that%20replaces%20unlabeled%20data%20with%20pathology-guided%20synthetic%20supervision.%20First%2C%20our%20Virtual%20Patient%20Engine%20models%20continuous%20latent%20disease%20trajectories%20from%20sparse%20clinical%20anchors%2C%20using%20generative%20modeling%20to%20synthesize%20physiologically%20plausible%2C%20fully%20labeled%203D%20cohorts.%20Second%2C%20Self-Reinforcing%20Interleaved%20Validation%20%28SIV%29%20provides%20a%20leakage-free%20protocol%20that%20evaluates%20models%20online%20with%20progressively%20challenging%20synthetic%20samples%2C%20eliminating%20the%20need%20for%20real%20validation%20data.%20Finally%2C%20a%20dynamic%20Lattice-of-Experts%20%28LoE%29%20organizes%20specialized%20networks%20within%20a%20pathology-aware%20topology%20and%20activates%20the%20most%20relevant%20experts%20per%20input%2C%20enabling%20robust%20zero-shot%20generalization%20to%20unseen%20data%20without%20target-domain%20fine-tuning.%20We%20evaluated%20PathCo-LatticE%20in%20a%20strict%20out-of-distribution%20%28OOD%29%20setting%2C%20deriving%20all%20anchors%20and%20severity%20statistics%20from%20a%20single-source%20domain%20%28ACDC%29%20and%20performing%20zero-shot%20testing%20on%20the%20multi-center%2C%20multi-vendor%20M%26Ms%20dataset.%20PathCo-LatticE%20outperforms%20four%20state-of-the-art%20FSL%20methods%20by%204.2-11%25%20Dice%20starting%20from%20only%207%20labeled%20anchors%2C%20and%20approaches%20fully%20supervised%20performance%20%28within%201%25%20Dice%29%20with%20only%2019%20labeled%20anchors.%20The%20method%20shows%20superior%20harmonization%20across%20four%20vendors%20and%20generalization%20to%20unseen%20pathologies.%20%5BCode%20will%20be%20made%20publicly%20available%5D.&entry.1838667208=http%3A//arxiv.org/abs/2512.09779v1&entry.124074799=Read"},
{"title": "Multi-Scale Direction-Aware Network for Infrared Small Target Detection", "author": "Jinmiao Zhao and Zelin Shi and Chuang Yu and Yunpeng Liu and Xinyi Ying and Yimian Dai", "abstract": "Infrared small target detection faces the problem that it is difficult to effectively separate the background and the target. Existing deep learning-based methods focus on edge and shape features, but ignore the richer structural differences and detailed information embedded in high-frequency components from different directions, thereby failing to fully exploit the value of high-frequency directional features in target perception. To address this limitation, we propose a multi-scale direction-aware network (MSDA-Net), which is the first attempt to integrate the high-frequency directional features of infrared small targets as domain prior knowledge into neural networks. Specifically, to fully mine the high-frequency directional features, on the one hand, a high-frequency direction injection (HFDI) module without trainable parameters is constructed to inject the high-frequency directional information of the original image into the network. On the other hand, a multi-scale direction-aware (MSDA) module is constructed, which promotes the full extraction of local relations at different scales and the full perception of key features in different directions. In addition, considering the characteristics of infrared small targets, we construct a feature aggregation (FA) structure to address target disappearance in high-level feature maps, and a feature calibration fusion (FCF) module to alleviate feature bias during cross-layer feature fusion. Extensive experimental results show that our MSDA-Net achieves state-of-the-art (SOTA) results on multiple public datasets. The code can be available at https://github.com/YuChuang1205/MSDA-Net", "link": "http://arxiv.org/abs/2406.02037v4", "date": "2025-12-10", "relevancy": 2.1796, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5683}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.53}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20Direction-Aware%20Network%20for%20Infrared%20Small%20Target%20Detection&body=Title%3A%20Multi-Scale%20Direction-Aware%20Network%20for%20Infrared%20Small%20Target%20Detection%0AAuthor%3A%20Jinmiao%20Zhao%20and%20Zelin%20Shi%20and%20Chuang%20Yu%20and%20Yunpeng%20Liu%20and%20Xinyi%20Ying%20and%20Yimian%20Dai%0AAbstract%3A%20Infrared%20small%20target%20detection%20faces%20the%20problem%20that%20it%20is%20difficult%20to%20effectively%20separate%20the%20background%20and%20the%20target.%20Existing%20deep%20learning-based%20methods%20focus%20on%20edge%20and%20shape%20features%2C%20but%20ignore%20the%20richer%20structural%20differences%20and%20detailed%20information%20embedded%20in%20high-frequency%20components%20from%20different%20directions%2C%20thereby%20failing%20to%20fully%20exploit%20the%20value%20of%20high-frequency%20directional%20features%20in%20target%20perception.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20multi-scale%20direction-aware%20network%20%28MSDA-Net%29%2C%20which%20is%20the%20first%20attempt%20to%20integrate%20the%20high-frequency%20directional%20features%20of%20infrared%20small%20targets%20as%20domain%20prior%20knowledge%20into%20neural%20networks.%20Specifically%2C%20to%20fully%20mine%20the%20high-frequency%20directional%20features%2C%20on%20the%20one%20hand%2C%20a%20high-frequency%20direction%20injection%20%28HFDI%29%20module%20without%20trainable%20parameters%20is%20constructed%20to%20inject%20the%20high-frequency%20directional%20information%20of%20the%20original%20image%20into%20the%20network.%20On%20the%20other%20hand%2C%20a%20multi-scale%20direction-aware%20%28MSDA%29%20module%20is%20constructed%2C%20which%20promotes%20the%20full%20extraction%20of%20local%20relations%20at%20different%20scales%20and%20the%20full%20perception%20of%20key%20features%20in%20different%20directions.%20In%20addition%2C%20considering%20the%20characteristics%20of%20infrared%20small%20targets%2C%20we%20construct%20a%20feature%20aggregation%20%28FA%29%20structure%20to%20address%20target%20disappearance%20in%20high-level%20feature%20maps%2C%20and%20a%20feature%20calibration%20fusion%20%28FCF%29%20module%20to%20alleviate%20feature%20bias%20during%20cross-layer%20feature%20fusion.%20Extensive%20experimental%20results%20show%20that%20our%20MSDA-Net%20achieves%20state-of-the-art%20%28SOTA%29%20results%20on%20multiple%20public%20datasets.%20The%20code%20can%20be%20available%20at%20https%3A//github.com/YuChuang1205/MSDA-Net%0ALink%3A%20http%3A//arxiv.org/abs/2406.02037v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Scale%2520Direction-Aware%2520Network%2520for%2520Infrared%2520Small%2520Target%2520Detection%26entry.906535625%3DJinmiao%2520Zhao%2520and%2520Zelin%2520Shi%2520and%2520Chuang%2520Yu%2520and%2520Yunpeng%2520Liu%2520and%2520Xinyi%2520Ying%2520and%2520Yimian%2520Dai%26entry.1292438233%3DInfrared%2520small%2520target%2520detection%2520faces%2520the%2520problem%2520that%2520it%2520is%2520difficult%2520to%2520effectively%2520separate%2520the%2520background%2520and%2520the%2520target.%2520Existing%2520deep%2520learning-based%2520methods%2520focus%2520on%2520edge%2520and%2520shape%2520features%252C%2520but%2520ignore%2520the%2520richer%2520structural%2520differences%2520and%2520detailed%2520information%2520embedded%2520in%2520high-frequency%2520components%2520from%2520different%2520directions%252C%2520thereby%2520failing%2520to%2520fully%2520exploit%2520the%2520value%2520of%2520high-frequency%2520directional%2520features%2520in%2520target%2520perception.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520multi-scale%2520direction-aware%2520network%2520%2528MSDA-Net%2529%252C%2520which%2520is%2520the%2520first%2520attempt%2520to%2520integrate%2520the%2520high-frequency%2520directional%2520features%2520of%2520infrared%2520small%2520targets%2520as%2520domain%2520prior%2520knowledge%2520into%2520neural%2520networks.%2520Specifically%252C%2520to%2520fully%2520mine%2520the%2520high-frequency%2520directional%2520features%252C%2520on%2520the%2520one%2520hand%252C%2520a%2520high-frequency%2520direction%2520injection%2520%2528HFDI%2529%2520module%2520without%2520trainable%2520parameters%2520is%2520constructed%2520to%2520inject%2520the%2520high-frequency%2520directional%2520information%2520of%2520the%2520original%2520image%2520into%2520the%2520network.%2520On%2520the%2520other%2520hand%252C%2520a%2520multi-scale%2520direction-aware%2520%2528MSDA%2529%2520module%2520is%2520constructed%252C%2520which%2520promotes%2520the%2520full%2520extraction%2520of%2520local%2520relations%2520at%2520different%2520scales%2520and%2520the%2520full%2520perception%2520of%2520key%2520features%2520in%2520different%2520directions.%2520In%2520addition%252C%2520considering%2520the%2520characteristics%2520of%2520infrared%2520small%2520targets%252C%2520we%2520construct%2520a%2520feature%2520aggregation%2520%2528FA%2529%2520structure%2520to%2520address%2520target%2520disappearance%2520in%2520high-level%2520feature%2520maps%252C%2520and%2520a%2520feature%2520calibration%2520fusion%2520%2528FCF%2529%2520module%2520to%2520alleviate%2520feature%2520bias%2520during%2520cross-layer%2520feature%2520fusion.%2520Extensive%2520experimental%2520results%2520show%2520that%2520our%2520MSDA-Net%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520results%2520on%2520multiple%2520public%2520datasets.%2520The%2520code%2520can%2520be%2520available%2520at%2520https%253A//github.com/YuChuang1205/MSDA-Net%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02037v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20Direction-Aware%20Network%20for%20Infrared%20Small%20Target%20Detection&entry.906535625=Jinmiao%20Zhao%20and%20Zelin%20Shi%20and%20Chuang%20Yu%20and%20Yunpeng%20Liu%20and%20Xinyi%20Ying%20and%20Yimian%20Dai&entry.1292438233=Infrared%20small%20target%20detection%20faces%20the%20problem%20that%20it%20is%20difficult%20to%20effectively%20separate%20the%20background%20and%20the%20target.%20Existing%20deep%20learning-based%20methods%20focus%20on%20edge%20and%20shape%20features%2C%20but%20ignore%20the%20richer%20structural%20differences%20and%20detailed%20information%20embedded%20in%20high-frequency%20components%20from%20different%20directions%2C%20thereby%20failing%20to%20fully%20exploit%20the%20value%20of%20high-frequency%20directional%20features%20in%20target%20perception.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20multi-scale%20direction-aware%20network%20%28MSDA-Net%29%2C%20which%20is%20the%20first%20attempt%20to%20integrate%20the%20high-frequency%20directional%20features%20of%20infrared%20small%20targets%20as%20domain%20prior%20knowledge%20into%20neural%20networks.%20Specifically%2C%20to%20fully%20mine%20the%20high-frequency%20directional%20features%2C%20on%20the%20one%20hand%2C%20a%20high-frequency%20direction%20injection%20%28HFDI%29%20module%20without%20trainable%20parameters%20is%20constructed%20to%20inject%20the%20high-frequency%20directional%20information%20of%20the%20original%20image%20into%20the%20network.%20On%20the%20other%20hand%2C%20a%20multi-scale%20direction-aware%20%28MSDA%29%20module%20is%20constructed%2C%20which%20promotes%20the%20full%20extraction%20of%20local%20relations%20at%20different%20scales%20and%20the%20full%20perception%20of%20key%20features%20in%20different%20directions.%20In%20addition%2C%20considering%20the%20characteristics%20of%20infrared%20small%20targets%2C%20we%20construct%20a%20feature%20aggregation%20%28FA%29%20structure%20to%20address%20target%20disappearance%20in%20high-level%20feature%20maps%2C%20and%20a%20feature%20calibration%20fusion%20%28FCF%29%20module%20to%20alleviate%20feature%20bias%20during%20cross-layer%20feature%20fusion.%20Extensive%20experimental%20results%20show%20that%20our%20MSDA-Net%20achieves%20state-of-the-art%20%28SOTA%29%20results%20on%20multiple%20public%20datasets.%20The%20code%20can%20be%20available%20at%20https%3A//github.com/YuChuang1205/MSDA-Net&entry.1838667208=http%3A//arxiv.org/abs/2406.02037v4&entry.124074799=Read"},
{"title": "Mastering Diverse, Unknown, and Cluttered Tracks for Robust Vision-Based Drone Racing", "author": "Feng Yu and Yu Hu and Yang Su and Yang Deng and Linzuo Zhang and Danping Zou", "abstract": "Most reinforcement learning(RL)-based methods for drone racing target fixed, obstacle-free tracks, leaving the generalization to unknown, cluttered environments largely unaddressed. This challenge stems from the need to balance racing speed and collision avoidance, limited feasible space causing policy exploration trapped in local optima during training, and perceptual ambiguity between gates and obstacles in depth maps-especially when gate positions are only coarsely specified. To overcome these issues, we propose a two-phase learning framework: an initial soft-collision training phase that preserves policy exploration for high-speed flight, followed by a hard-collision refinement phase that enforces robust obstacle avoidance. An adaptive, noise-augmented curriculum with an asymmetric actor-critic architecture gradually shifts the policy's reliance from privileged gate-state information to depth-based visual input. We further impose Lipschitz constraints and integrate a track-primitive generator to enhance motion stability and cross-environment generalization. We evaluate our framework through extensive simulation and ablation studies, and validate it in real-world experiments on a computationally constrained quadrotor. The system achieves agile flight while remaining robust to gate-position errors, developing a generalizable drone racing framework with the capability to operate in diverse, partially unknown and cluttered environments. https://yufengsjtu.github.io/MasterRacing.github.io/", "link": "http://arxiv.org/abs/2512.09571v1", "date": "2025-12-10", "relevancy": 2.1759, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5621}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5428}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mastering%20Diverse%2C%20Unknown%2C%20and%20Cluttered%20Tracks%20for%20Robust%20Vision-Based%20Drone%20Racing&body=Title%3A%20Mastering%20Diverse%2C%20Unknown%2C%20and%20Cluttered%20Tracks%20for%20Robust%20Vision-Based%20Drone%20Racing%0AAuthor%3A%20Feng%20Yu%20and%20Yu%20Hu%20and%20Yang%20Su%20and%20Yang%20Deng%20and%20Linzuo%20Zhang%20and%20Danping%20Zou%0AAbstract%3A%20Most%20reinforcement%20learning%28RL%29-based%20methods%20for%20drone%20racing%20target%20fixed%2C%20obstacle-free%20tracks%2C%20leaving%20the%20generalization%20to%20unknown%2C%20cluttered%20environments%20largely%20unaddressed.%20This%20challenge%20stems%20from%20the%20need%20to%20balance%20racing%20speed%20and%20collision%20avoidance%2C%20limited%20feasible%20space%20causing%20policy%20exploration%20trapped%20in%20local%20optima%20during%20training%2C%20and%20perceptual%20ambiguity%20between%20gates%20and%20obstacles%20in%20depth%20maps-especially%20when%20gate%20positions%20are%20only%20coarsely%20specified.%20To%20overcome%20these%20issues%2C%20we%20propose%20a%20two-phase%20learning%20framework%3A%20an%20initial%20soft-collision%20training%20phase%20that%20preserves%20policy%20exploration%20for%20high-speed%20flight%2C%20followed%20by%20a%20hard-collision%20refinement%20phase%20that%20enforces%20robust%20obstacle%20avoidance.%20An%20adaptive%2C%20noise-augmented%20curriculum%20with%20an%20asymmetric%20actor-critic%20architecture%20gradually%20shifts%20the%20policy%27s%20reliance%20from%20privileged%20gate-state%20information%20to%20depth-based%20visual%20input.%20We%20further%20impose%20Lipschitz%20constraints%20and%20integrate%20a%20track-primitive%20generator%20to%20enhance%20motion%20stability%20and%20cross-environment%20generalization.%20We%20evaluate%20our%20framework%20through%20extensive%20simulation%20and%20ablation%20studies%2C%20and%20validate%20it%20in%20real-world%20experiments%20on%20a%20computationally%20constrained%20quadrotor.%20The%20system%20achieves%20agile%20flight%20while%20remaining%20robust%20to%20gate-position%20errors%2C%20developing%20a%20generalizable%20drone%20racing%20framework%20with%20the%20capability%20to%20operate%20in%20diverse%2C%20partially%20unknown%20and%20cluttered%20environments.%20https%3A//yufengsjtu.github.io/MasterRacing.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2512.09571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMastering%2520Diverse%252C%2520Unknown%252C%2520and%2520Cluttered%2520Tracks%2520for%2520Robust%2520Vision-Based%2520Drone%2520Racing%26entry.906535625%3DFeng%2520Yu%2520and%2520Yu%2520Hu%2520and%2520Yang%2520Su%2520and%2520Yang%2520Deng%2520and%2520Linzuo%2520Zhang%2520and%2520Danping%2520Zou%26entry.1292438233%3DMost%2520reinforcement%2520learning%2528RL%2529-based%2520methods%2520for%2520drone%2520racing%2520target%2520fixed%252C%2520obstacle-free%2520tracks%252C%2520leaving%2520the%2520generalization%2520to%2520unknown%252C%2520cluttered%2520environments%2520largely%2520unaddressed.%2520This%2520challenge%2520stems%2520from%2520the%2520need%2520to%2520balance%2520racing%2520speed%2520and%2520collision%2520avoidance%252C%2520limited%2520feasible%2520space%2520causing%2520policy%2520exploration%2520trapped%2520in%2520local%2520optima%2520during%2520training%252C%2520and%2520perceptual%2520ambiguity%2520between%2520gates%2520and%2520obstacles%2520in%2520depth%2520maps-especially%2520when%2520gate%2520positions%2520are%2520only%2520coarsely%2520specified.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520propose%2520a%2520two-phase%2520learning%2520framework%253A%2520an%2520initial%2520soft-collision%2520training%2520phase%2520that%2520preserves%2520policy%2520exploration%2520for%2520high-speed%2520flight%252C%2520followed%2520by%2520a%2520hard-collision%2520refinement%2520phase%2520that%2520enforces%2520robust%2520obstacle%2520avoidance.%2520An%2520adaptive%252C%2520noise-augmented%2520curriculum%2520with%2520an%2520asymmetric%2520actor-critic%2520architecture%2520gradually%2520shifts%2520the%2520policy%2527s%2520reliance%2520from%2520privileged%2520gate-state%2520information%2520to%2520depth-based%2520visual%2520input.%2520We%2520further%2520impose%2520Lipschitz%2520constraints%2520and%2520integrate%2520a%2520track-primitive%2520generator%2520to%2520enhance%2520motion%2520stability%2520and%2520cross-environment%2520generalization.%2520We%2520evaluate%2520our%2520framework%2520through%2520extensive%2520simulation%2520and%2520ablation%2520studies%252C%2520and%2520validate%2520it%2520in%2520real-world%2520experiments%2520on%2520a%2520computationally%2520constrained%2520quadrotor.%2520The%2520system%2520achieves%2520agile%2520flight%2520while%2520remaining%2520robust%2520to%2520gate-position%2520errors%252C%2520developing%2520a%2520generalizable%2520drone%2520racing%2520framework%2520with%2520the%2520capability%2520to%2520operate%2520in%2520diverse%252C%2520partially%2520unknown%2520and%2520cluttered%2520environments.%2520https%253A//yufengsjtu.github.io/MasterRacing.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mastering%20Diverse%2C%20Unknown%2C%20and%20Cluttered%20Tracks%20for%20Robust%20Vision-Based%20Drone%20Racing&entry.906535625=Feng%20Yu%20and%20Yu%20Hu%20and%20Yang%20Su%20and%20Yang%20Deng%20and%20Linzuo%20Zhang%20and%20Danping%20Zou&entry.1292438233=Most%20reinforcement%20learning%28RL%29-based%20methods%20for%20drone%20racing%20target%20fixed%2C%20obstacle-free%20tracks%2C%20leaving%20the%20generalization%20to%20unknown%2C%20cluttered%20environments%20largely%20unaddressed.%20This%20challenge%20stems%20from%20the%20need%20to%20balance%20racing%20speed%20and%20collision%20avoidance%2C%20limited%20feasible%20space%20causing%20policy%20exploration%20trapped%20in%20local%20optima%20during%20training%2C%20and%20perceptual%20ambiguity%20between%20gates%20and%20obstacles%20in%20depth%20maps-especially%20when%20gate%20positions%20are%20only%20coarsely%20specified.%20To%20overcome%20these%20issues%2C%20we%20propose%20a%20two-phase%20learning%20framework%3A%20an%20initial%20soft-collision%20training%20phase%20that%20preserves%20policy%20exploration%20for%20high-speed%20flight%2C%20followed%20by%20a%20hard-collision%20refinement%20phase%20that%20enforces%20robust%20obstacle%20avoidance.%20An%20adaptive%2C%20noise-augmented%20curriculum%20with%20an%20asymmetric%20actor-critic%20architecture%20gradually%20shifts%20the%20policy%27s%20reliance%20from%20privileged%20gate-state%20information%20to%20depth-based%20visual%20input.%20We%20further%20impose%20Lipschitz%20constraints%20and%20integrate%20a%20track-primitive%20generator%20to%20enhance%20motion%20stability%20and%20cross-environment%20generalization.%20We%20evaluate%20our%20framework%20through%20extensive%20simulation%20and%20ablation%20studies%2C%20and%20validate%20it%20in%20real-world%20experiments%20on%20a%20computationally%20constrained%20quadrotor.%20The%20system%20achieves%20agile%20flight%20while%20remaining%20robust%20to%20gate-position%20errors%2C%20developing%20a%20generalizable%20drone%20racing%20framework%20with%20the%20capability%20to%20operate%20in%20diverse%2C%20partially%20unknown%20and%20cluttered%20environments.%20https%3A//yufengsjtu.github.io/MasterRacing.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2512.09571v1&entry.124074799=Read"},
{"title": "Next-Generation Reservoir Computing for Dynamical Inference", "author": "Rok Cestnik and Erik A. Martens", "abstract": "We present a simple and scalable implementation of next-generation reservoir computing (NGRC) for modeling dynamical systems from time-series data. The method uses a pseudorandom nonlinear projection of time-delay embedded inputs, allowing the feature-space dimension to be chosen independently of the observation size and offering a flexible alternative to polynomial-based NGRC projections. We demonstrate the approach on benchmark tasks, including attractor reconstruction and bifurcation diagram estimation, using partial and noisy measurements. We further show that small amounts of measurement noise during training act as an effective regularizer, improving long-term autonomous stability compared to standard regression alone. Across all tests, the models remain stable over long rollouts and generalize beyond the training data. The framework offers explicit control of system state during prediction, and these properties make NGRC a natural candidate for applications such as surrogate modeling and digital-twin applications.", "link": "http://arxiv.org/abs/2509.11338v2", "date": "2025-12-10", "relevancy": 2.1549, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5453}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5361}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next-Generation%20Reservoir%20Computing%20for%20Dynamical%20Inference&body=Title%3A%20Next-Generation%20Reservoir%20Computing%20for%20Dynamical%20Inference%0AAuthor%3A%20Rok%20Cestnik%20and%20Erik%20A.%20Martens%0AAbstract%3A%20We%20present%20a%20simple%20and%20scalable%20implementation%20of%20next-generation%20reservoir%20computing%20%28NGRC%29%20for%20modeling%20dynamical%20systems%20from%20time-series%20data.%20The%20method%20uses%20a%20pseudorandom%20nonlinear%20projection%20of%20time-delay%20embedded%20inputs%2C%20allowing%20the%20feature-space%20dimension%20to%20be%20chosen%20independently%20of%20the%20observation%20size%20and%20offering%20a%20flexible%20alternative%20to%20polynomial-based%20NGRC%20projections.%20We%20demonstrate%20the%20approach%20on%20benchmark%20tasks%2C%20including%20attractor%20reconstruction%20and%20bifurcation%20diagram%20estimation%2C%20using%20partial%20and%20noisy%20measurements.%20We%20further%20show%20that%20small%20amounts%20of%20measurement%20noise%20during%20training%20act%20as%20an%20effective%20regularizer%2C%20improving%20long-term%20autonomous%20stability%20compared%20to%20standard%20regression%20alone.%20Across%20all%20tests%2C%20the%20models%20remain%20stable%20over%20long%20rollouts%20and%20generalize%20beyond%20the%20training%20data.%20The%20framework%20offers%20explicit%20control%20of%20system%20state%20during%20prediction%2C%20and%20these%20properties%20make%20NGRC%20a%20natural%20candidate%20for%20applications%20such%20as%20surrogate%20modeling%20and%20digital-twin%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2509.11338v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext-Generation%2520Reservoir%2520Computing%2520for%2520Dynamical%2520Inference%26entry.906535625%3DRok%2520Cestnik%2520and%2520Erik%2520A.%2520Martens%26entry.1292438233%3DWe%2520present%2520a%2520simple%2520and%2520scalable%2520implementation%2520of%2520next-generation%2520reservoir%2520computing%2520%2528NGRC%2529%2520for%2520modeling%2520dynamical%2520systems%2520from%2520time-series%2520data.%2520The%2520method%2520uses%2520a%2520pseudorandom%2520nonlinear%2520projection%2520of%2520time-delay%2520embedded%2520inputs%252C%2520allowing%2520the%2520feature-space%2520dimension%2520to%2520be%2520chosen%2520independently%2520of%2520the%2520observation%2520size%2520and%2520offering%2520a%2520flexible%2520alternative%2520to%2520polynomial-based%2520NGRC%2520projections.%2520We%2520demonstrate%2520the%2520approach%2520on%2520benchmark%2520tasks%252C%2520including%2520attractor%2520reconstruction%2520and%2520bifurcation%2520diagram%2520estimation%252C%2520using%2520partial%2520and%2520noisy%2520measurements.%2520We%2520further%2520show%2520that%2520small%2520amounts%2520of%2520measurement%2520noise%2520during%2520training%2520act%2520as%2520an%2520effective%2520regularizer%252C%2520improving%2520long-term%2520autonomous%2520stability%2520compared%2520to%2520standard%2520regression%2520alone.%2520Across%2520all%2520tests%252C%2520the%2520models%2520remain%2520stable%2520over%2520long%2520rollouts%2520and%2520generalize%2520beyond%2520the%2520training%2520data.%2520The%2520framework%2520offers%2520explicit%2520control%2520of%2520system%2520state%2520during%2520prediction%252C%2520and%2520these%2520properties%2520make%2520NGRC%2520a%2520natural%2520candidate%2520for%2520applications%2520such%2520as%2520surrogate%2520modeling%2520and%2520digital-twin%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11338v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next-Generation%20Reservoir%20Computing%20for%20Dynamical%20Inference&entry.906535625=Rok%20Cestnik%20and%20Erik%20A.%20Martens&entry.1292438233=We%20present%20a%20simple%20and%20scalable%20implementation%20of%20next-generation%20reservoir%20computing%20%28NGRC%29%20for%20modeling%20dynamical%20systems%20from%20time-series%20data.%20The%20method%20uses%20a%20pseudorandom%20nonlinear%20projection%20of%20time-delay%20embedded%20inputs%2C%20allowing%20the%20feature-space%20dimension%20to%20be%20chosen%20independently%20of%20the%20observation%20size%20and%20offering%20a%20flexible%20alternative%20to%20polynomial-based%20NGRC%20projections.%20We%20demonstrate%20the%20approach%20on%20benchmark%20tasks%2C%20including%20attractor%20reconstruction%20and%20bifurcation%20diagram%20estimation%2C%20using%20partial%20and%20noisy%20measurements.%20We%20further%20show%20that%20small%20amounts%20of%20measurement%20noise%20during%20training%20act%20as%20an%20effective%20regularizer%2C%20improving%20long-term%20autonomous%20stability%20compared%20to%20standard%20regression%20alone.%20Across%20all%20tests%2C%20the%20models%20remain%20stable%20over%20long%20rollouts%20and%20generalize%20beyond%20the%20training%20data.%20The%20framework%20offers%20explicit%20control%20of%20system%20state%20during%20prediction%2C%20and%20these%20properties%20make%20NGRC%20a%20natural%20candidate%20for%20applications%20such%20as%20surrogate%20modeling%20and%20digital-twin%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2509.11338v2&entry.124074799=Read"},
{"title": "From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities", "author": "Shijia Feng and Michael Wray and Walterio Mayol-Cuevas", "abstract": "Understanding human skill performance is essential for intelligent assistive systems, with struggle recognition offering a natural cue for identifying user difficulties. While prior work focuses on offline struggle classification and localization, real-time applications require models capable of detecting and anticipating struggle online. We reformulate struggle localization as an online detection task and further extend it to anticipation, predicting struggle moments before they occur. We adapt two off-the-shelf models as baselines for online struggle detection and anticipation. Online struggle detection achieves 70-80% per-frame mAP, while struggle anticipation up to 2 seconds ahead yields comparable performance with slight drops. We further examine generalization across tasks and activities and analyse the impact of skill evolution. Despite larger domain gaps in activity-level generalization, models still outperform random baselines by 4-20%. Our feature-based models run at up to 143 FPS, and the whole pipeline, including feature extraction, operates at around 20 FPS, sufficient for real-time assistive applications.", "link": "http://arxiv.org/abs/2512.09847v1", "date": "2025-12-10", "relevancy": 2.1484, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5721}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5145}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Detection%20to%20Anticipation%3A%20Online%20Understanding%20of%20Struggles%20across%20Various%20Tasks%20and%20Activities&body=Title%3A%20From%20Detection%20to%20Anticipation%3A%20Online%20Understanding%20of%20Struggles%20across%20Various%20Tasks%20and%20Activities%0AAuthor%3A%20Shijia%20Feng%20and%20Michael%20Wray%20and%20Walterio%20Mayol-Cuevas%0AAbstract%3A%20Understanding%20human%20skill%20performance%20is%20essential%20for%20intelligent%20assistive%20systems%2C%20with%20struggle%20recognition%20offering%20a%20natural%20cue%20for%20identifying%20user%20difficulties.%20While%20prior%20work%20focuses%20on%20offline%20struggle%20classification%20and%20localization%2C%20real-time%20applications%20require%20models%20capable%20of%20detecting%20and%20anticipating%20struggle%20online.%20We%20reformulate%20struggle%20localization%20as%20an%20online%20detection%20task%20and%20further%20extend%20it%20to%20anticipation%2C%20predicting%20struggle%20moments%20before%20they%20occur.%20We%20adapt%20two%20off-the-shelf%20models%20as%20baselines%20for%20online%20struggle%20detection%20and%20anticipation.%20Online%20struggle%20detection%20achieves%2070-80%25%20per-frame%20mAP%2C%20while%20struggle%20anticipation%20up%20to%202%20seconds%20ahead%20yields%20comparable%20performance%20with%20slight%20drops.%20We%20further%20examine%20generalization%20across%20tasks%20and%20activities%20and%20analyse%20the%20impact%20of%20skill%20evolution.%20Despite%20larger%20domain%20gaps%20in%20activity-level%20generalization%2C%20models%20still%20outperform%20random%20baselines%20by%204-20%25.%20Our%20feature-based%20models%20run%20at%20up%20to%20143%20FPS%2C%20and%20the%20whole%20pipeline%2C%20including%20feature%20extraction%2C%20operates%20at%20around%2020%20FPS%2C%20sufficient%20for%20real-time%20assistive%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Detection%2520to%2520Anticipation%253A%2520Online%2520Understanding%2520of%2520Struggles%2520across%2520Various%2520Tasks%2520and%2520Activities%26entry.906535625%3DShijia%2520Feng%2520and%2520Michael%2520Wray%2520and%2520Walterio%2520Mayol-Cuevas%26entry.1292438233%3DUnderstanding%2520human%2520skill%2520performance%2520is%2520essential%2520for%2520intelligent%2520assistive%2520systems%252C%2520with%2520struggle%2520recognition%2520offering%2520a%2520natural%2520cue%2520for%2520identifying%2520user%2520difficulties.%2520While%2520prior%2520work%2520focuses%2520on%2520offline%2520struggle%2520classification%2520and%2520localization%252C%2520real-time%2520applications%2520require%2520models%2520capable%2520of%2520detecting%2520and%2520anticipating%2520struggle%2520online.%2520We%2520reformulate%2520struggle%2520localization%2520as%2520an%2520online%2520detection%2520task%2520and%2520further%2520extend%2520it%2520to%2520anticipation%252C%2520predicting%2520struggle%2520moments%2520before%2520they%2520occur.%2520We%2520adapt%2520two%2520off-the-shelf%2520models%2520as%2520baselines%2520for%2520online%2520struggle%2520detection%2520and%2520anticipation.%2520Online%2520struggle%2520detection%2520achieves%252070-80%2525%2520per-frame%2520mAP%252C%2520while%2520struggle%2520anticipation%2520up%2520to%25202%2520seconds%2520ahead%2520yields%2520comparable%2520performance%2520with%2520slight%2520drops.%2520We%2520further%2520examine%2520generalization%2520across%2520tasks%2520and%2520activities%2520and%2520analyse%2520the%2520impact%2520of%2520skill%2520evolution.%2520Despite%2520larger%2520domain%2520gaps%2520in%2520activity-level%2520generalization%252C%2520models%2520still%2520outperform%2520random%2520baselines%2520by%25204-20%2525.%2520Our%2520feature-based%2520models%2520run%2520at%2520up%2520to%2520143%2520FPS%252C%2520and%2520the%2520whole%2520pipeline%252C%2520including%2520feature%2520extraction%252C%2520operates%2520at%2520around%252020%2520FPS%252C%2520sufficient%2520for%2520real-time%2520assistive%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Detection%20to%20Anticipation%3A%20Online%20Understanding%20of%20Struggles%20across%20Various%20Tasks%20and%20Activities&entry.906535625=Shijia%20Feng%20and%20Michael%20Wray%20and%20Walterio%20Mayol-Cuevas&entry.1292438233=Understanding%20human%20skill%20performance%20is%20essential%20for%20intelligent%20assistive%20systems%2C%20with%20struggle%20recognition%20offering%20a%20natural%20cue%20for%20identifying%20user%20difficulties.%20While%20prior%20work%20focuses%20on%20offline%20struggle%20classification%20and%20localization%2C%20real-time%20applications%20require%20models%20capable%20of%20detecting%20and%20anticipating%20struggle%20online.%20We%20reformulate%20struggle%20localization%20as%20an%20online%20detection%20task%20and%20further%20extend%20it%20to%20anticipation%2C%20predicting%20struggle%20moments%20before%20they%20occur.%20We%20adapt%20two%20off-the-shelf%20models%20as%20baselines%20for%20online%20struggle%20detection%20and%20anticipation.%20Online%20struggle%20detection%20achieves%2070-80%25%20per-frame%20mAP%2C%20while%20struggle%20anticipation%20up%20to%202%20seconds%20ahead%20yields%20comparable%20performance%20with%20slight%20drops.%20We%20further%20examine%20generalization%20across%20tasks%20and%20activities%20and%20analyse%20the%20impact%20of%20skill%20evolution.%20Despite%20larger%20domain%20gaps%20in%20activity-level%20generalization%2C%20models%20still%20outperform%20random%20baselines%20by%204-20%25.%20Our%20feature-based%20models%20run%20at%20up%20to%20143%20FPS%2C%20and%20the%20whole%20pipeline%2C%20including%20feature%20extraction%2C%20operates%20at%20around%2020%20FPS%2C%20sufficient%20for%20real-time%20assistive%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.09847v1&entry.124074799=Read"},
{"title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models", "author": "Minghui Lin and Pengxiang Ding and Shu Wang and Zifeng Zhuang and Yang Liu and Xinyang Tong and Wenxuan Song and Shangke Lyu and Siteng Huang and Donglin Wang", "abstract": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.", "link": "http://arxiv.org/abs/2512.09928v1", "date": "2025-12-10", "relevancy": 2.1404, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5319}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiF-VLA%3A%20Hindsight%2C%20Insight%20and%20Foresight%20through%20Motion%20Representation%20for%20Vision-Language-Action%20Models&body=Title%3A%20HiF-VLA%3A%20Hindsight%2C%20Insight%20and%20Foresight%20through%20Motion%20Representation%20for%20Vision-Language-Action%20Models%0AAuthor%3A%20Minghui%20Lin%20and%20Pengxiang%20Ding%20and%20Shu%20Wang%20and%20Zifeng%20Zhuang%20and%20Yang%20Liu%20and%20Xinyang%20Tong%20and%20Wenxuan%20Song%20and%20Shangke%20Lyu%20and%20Siteng%20Huang%20and%20Donglin%20Wang%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20have%20recently%20enabled%20robotic%20manipulation%20by%20grounding%20visual%20and%20linguistic%20cues%20into%20actions.%20However%2C%20most%20VLAs%20assume%20the%20Markov%20property%2C%20relying%20only%20on%20the%20current%20observation%20and%20thus%20suffering%20from%20temporal%20myopia%20that%20degrades%20long-horizon%20coherence.%20In%20this%20work%2C%20we%20view%20motion%20as%20a%20more%20compact%20and%20informative%20representation%20of%20temporal%20context%20and%20world%20dynamics%2C%20capturing%20inter-state%20changes%20while%20filtering%20static%20pixel-level%20noise.%20Building%20on%20this%20idea%2C%20we%20propose%20HiF-VLA%20%28Hindsight%2C%20Insight%2C%20and%20Foresight%20for%20VLAs%29%2C%20a%20unified%20framework%20that%20leverages%20motion%20for%20bidirectional%20temporal%20reasoning.%20HiF-VLA%20encodes%20past%20dynamics%20through%20hindsight%20priors%2C%20anticipates%20future%20motion%20via%20foresight%20reasoning%2C%20and%20integrates%20both%20through%20a%20hindsight-modulated%20joint%20expert%20to%20enable%20a%20%27%27think-while-acting%27%27%20paradigm%20for%20long-horizon%20manipulation.%20As%20a%20result%2C%20HiF-VLA%20surpasses%20strong%20baselines%20on%20LIBERO-Long%20and%20CALVIN%20ABC-D%20benchmarks%2C%20while%20incurring%20negligible%20additional%20inference%20latency.%20Furthermore%2C%20HiF-VLA%20achieves%20substantial%20improvements%20in%20real-world%20long-horizon%20manipulation%20tasks%2C%20demonstrating%20its%20broad%20effectiveness%20in%20practical%20robotic%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiF-VLA%253A%2520Hindsight%252C%2520Insight%2520and%2520Foresight%2520through%2520Motion%2520Representation%2520for%2520Vision-Language-Action%2520Models%26entry.906535625%3DMinghui%2520Lin%2520and%2520Pengxiang%2520Ding%2520and%2520Shu%2520Wang%2520and%2520Zifeng%2520Zhuang%2520and%2520Yang%2520Liu%2520and%2520Xinyang%2520Tong%2520and%2520Wenxuan%2520Song%2520and%2520Shangke%2520Lyu%2520and%2520Siteng%2520Huang%2520and%2520Donglin%2520Wang%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520recently%2520enabled%2520robotic%2520manipulation%2520by%2520grounding%2520visual%2520and%2520linguistic%2520cues%2520into%2520actions.%2520However%252C%2520most%2520VLAs%2520assume%2520the%2520Markov%2520property%252C%2520relying%2520only%2520on%2520the%2520current%2520observation%2520and%2520thus%2520suffering%2520from%2520temporal%2520myopia%2520that%2520degrades%2520long-horizon%2520coherence.%2520In%2520this%2520work%252C%2520we%2520view%2520motion%2520as%2520a%2520more%2520compact%2520and%2520informative%2520representation%2520of%2520temporal%2520context%2520and%2520world%2520dynamics%252C%2520capturing%2520inter-state%2520changes%2520while%2520filtering%2520static%2520pixel-level%2520noise.%2520Building%2520on%2520this%2520idea%252C%2520we%2520propose%2520HiF-VLA%2520%2528Hindsight%252C%2520Insight%252C%2520and%2520Foresight%2520for%2520VLAs%2529%252C%2520a%2520unified%2520framework%2520that%2520leverages%2520motion%2520for%2520bidirectional%2520temporal%2520reasoning.%2520HiF-VLA%2520encodes%2520past%2520dynamics%2520through%2520hindsight%2520priors%252C%2520anticipates%2520future%2520motion%2520via%2520foresight%2520reasoning%252C%2520and%2520integrates%2520both%2520through%2520a%2520hindsight-modulated%2520joint%2520expert%2520to%2520enable%2520a%2520%2527%2527think-while-acting%2527%2527%2520paradigm%2520for%2520long-horizon%2520manipulation.%2520As%2520a%2520result%252C%2520HiF-VLA%2520surpasses%2520strong%2520baselines%2520on%2520LIBERO-Long%2520and%2520CALVIN%2520ABC-D%2520benchmarks%252C%2520while%2520incurring%2520negligible%2520additional%2520inference%2520latency.%2520Furthermore%252C%2520HiF-VLA%2520achieves%2520substantial%2520improvements%2520in%2520real-world%2520long-horizon%2520manipulation%2520tasks%252C%2520demonstrating%2520its%2520broad%2520effectiveness%2520in%2520practical%2520robotic%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiF-VLA%3A%20Hindsight%2C%20Insight%20and%20Foresight%20through%20Motion%20Representation%20for%20Vision-Language-Action%20Models&entry.906535625=Minghui%20Lin%20and%20Pengxiang%20Ding%20and%20Shu%20Wang%20and%20Zifeng%20Zhuang%20and%20Yang%20Liu%20and%20Xinyang%20Tong%20and%20Wenxuan%20Song%20and%20Shangke%20Lyu%20and%20Siteng%20Huang%20and%20Donglin%20Wang&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20have%20recently%20enabled%20robotic%20manipulation%20by%20grounding%20visual%20and%20linguistic%20cues%20into%20actions.%20However%2C%20most%20VLAs%20assume%20the%20Markov%20property%2C%20relying%20only%20on%20the%20current%20observation%20and%20thus%20suffering%20from%20temporal%20myopia%20that%20degrades%20long-horizon%20coherence.%20In%20this%20work%2C%20we%20view%20motion%20as%20a%20more%20compact%20and%20informative%20representation%20of%20temporal%20context%20and%20world%20dynamics%2C%20capturing%20inter-state%20changes%20while%20filtering%20static%20pixel-level%20noise.%20Building%20on%20this%20idea%2C%20we%20propose%20HiF-VLA%20%28Hindsight%2C%20Insight%2C%20and%20Foresight%20for%20VLAs%29%2C%20a%20unified%20framework%20that%20leverages%20motion%20for%20bidirectional%20temporal%20reasoning.%20HiF-VLA%20encodes%20past%20dynamics%20through%20hindsight%20priors%2C%20anticipates%20future%20motion%20via%20foresight%20reasoning%2C%20and%20integrates%20both%20through%20a%20hindsight-modulated%20joint%20expert%20to%20enable%20a%20%27%27think-while-acting%27%27%20paradigm%20for%20long-horizon%20manipulation.%20As%20a%20result%2C%20HiF-VLA%20surpasses%20strong%20baselines%20on%20LIBERO-Long%20and%20CALVIN%20ABC-D%20benchmarks%2C%20while%20incurring%20negligible%20additional%20inference%20latency.%20Furthermore%2C%20HiF-VLA%20achieves%20substantial%20improvements%20in%20real-world%20long-horizon%20manipulation%20tasks%2C%20demonstrating%20its%20broad%20effectiveness%20in%20practical%20robotic%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2512.09928v1&entry.124074799=Read"},
{"title": "CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation", "author": "Zhenyu Lu and Liupeng Li and Jinpeng Wang and Yan Feng and Bin Chen and Ke Chen and Yaowei Wang", "abstract": "Existing works on reasoning segmentation either connect hidden features from a language model directly to a mask decoder or represent positions in text, which limits interpretability and semantic detail. To solve this, we present CoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model that bridges language reasoning to segmentation through a differentiable and interpretable positional prior instantiated as a heatmap. By making the reasoning process clear via MCoT and expressing it as a dense, differentiable heatmap, this interface enhances interpretability and diagnostic analysis and yields more concentrated evidence on the target. A learnable concentration token aggregates features of the image and reasoning text to generate this positional prior, which is decoded to precise masks through a lightweight decoder, providing a direct connection between reasoning and segmentation. Across the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best reported metrics on each standard split under comparable protocols, with performance at or above the prior state of the art across both validation and test partitions. Extensive experiments demonstrate a strong positive correlation among the CoT trajectory, the generated heatmap, and the decoded mask, supporting an interpretable alignment between the reasoning output and downstream mask generation. Collectively, these findings support the utility of this paradigm in bridging reasoning and segmentation and show advantages in concentration driven by reasoning and in more precise mask prediction. Code, checkpoints and logs are released at https://github.com/ZhenyuLU-Heliodore/CoPRS.git.", "link": "http://arxiv.org/abs/2510.11173v2", "date": "2025-12-10", "relevancy": 2.1287, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5385}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5309}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoPRS%3A%20Learning%20Positional%20Prior%20from%20Chain-of-Thought%20for%20Reasoning%20Segmentation&body=Title%3A%20CoPRS%3A%20Learning%20Positional%20Prior%20from%20Chain-of-Thought%20for%20Reasoning%20Segmentation%0AAuthor%3A%20Zhenyu%20Lu%20and%20Liupeng%20Li%20and%20Jinpeng%20Wang%20and%20Yan%20Feng%20and%20Bin%20Chen%20and%20Ke%20Chen%20and%20Yaowei%20Wang%0AAbstract%3A%20Existing%20works%20on%20reasoning%20segmentation%20either%20connect%20hidden%20features%20from%20a%20language%20model%20directly%20to%20a%20mask%20decoder%20or%20represent%20positions%20in%20text%2C%20which%20limits%20interpretability%20and%20semantic%20detail.%20To%20solve%20this%2C%20we%20present%20CoPRS%2C%20a%20Multi-modal%20Chain-of-Thought%20%28MCoT%29-based%20positional%20perception%20model%20that%20bridges%20language%20reasoning%20to%20segmentation%20through%20a%20differentiable%20and%20interpretable%20positional%20prior%20instantiated%20as%20a%20heatmap.%20By%20making%20the%20reasoning%20process%20clear%20via%20MCoT%20and%20expressing%20it%20as%20a%20dense%2C%20differentiable%20heatmap%2C%20this%20interface%20enhances%20interpretability%20and%20diagnostic%20analysis%20and%20yields%20more%20concentrated%20evidence%20on%20the%20target.%20A%20learnable%20concentration%20token%20aggregates%20features%20of%20the%20image%20and%20reasoning%20text%20to%20generate%20this%20positional%20prior%2C%20which%20is%20decoded%20to%20precise%20masks%20through%20a%20lightweight%20decoder%2C%20providing%20a%20direct%20connection%20between%20reasoning%20and%20segmentation.%20Across%20the%20RefCOCO%20series%20and%20ReasonSeg%2C%20CoPRS%20matches%20or%20surpasses%20the%20best%20reported%20metrics%20on%20each%20standard%20split%20under%20comparable%20protocols%2C%20with%20performance%20at%20or%20above%20the%20prior%20state%20of%20the%20art%20across%20both%20validation%20and%20test%20partitions.%20Extensive%20experiments%20demonstrate%20a%20strong%20positive%20correlation%20among%20the%20CoT%20trajectory%2C%20the%20generated%20heatmap%2C%20and%20the%20decoded%20mask%2C%20supporting%20an%20interpretable%20alignment%20between%20the%20reasoning%20output%20and%20downstream%20mask%20generation.%20Collectively%2C%20these%20findings%20support%20the%20utility%20of%20this%20paradigm%20in%20bridging%20reasoning%20and%20segmentation%20and%20show%20advantages%20in%20concentration%20driven%20by%20reasoning%20and%20in%20more%20precise%20mask%20prediction.%20Code%2C%20checkpoints%20and%20logs%20are%20released%20at%20https%3A//github.com/ZhenyuLU-Heliodore/CoPRS.git.%0ALink%3A%20http%3A//arxiv.org/abs/2510.11173v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoPRS%253A%2520Learning%2520Positional%2520Prior%2520from%2520Chain-of-Thought%2520for%2520Reasoning%2520Segmentation%26entry.906535625%3DZhenyu%2520Lu%2520and%2520Liupeng%2520Li%2520and%2520Jinpeng%2520Wang%2520and%2520Yan%2520Feng%2520and%2520Bin%2520Chen%2520and%2520Ke%2520Chen%2520and%2520Yaowei%2520Wang%26entry.1292438233%3DExisting%2520works%2520on%2520reasoning%2520segmentation%2520either%2520connect%2520hidden%2520features%2520from%2520a%2520language%2520model%2520directly%2520to%2520a%2520mask%2520decoder%2520or%2520represent%2520positions%2520in%2520text%252C%2520which%2520limits%2520interpretability%2520and%2520semantic%2520detail.%2520To%2520solve%2520this%252C%2520we%2520present%2520CoPRS%252C%2520a%2520Multi-modal%2520Chain-of-Thought%2520%2528MCoT%2529-based%2520positional%2520perception%2520model%2520that%2520bridges%2520language%2520reasoning%2520to%2520segmentation%2520through%2520a%2520differentiable%2520and%2520interpretable%2520positional%2520prior%2520instantiated%2520as%2520a%2520heatmap.%2520By%2520making%2520the%2520reasoning%2520process%2520clear%2520via%2520MCoT%2520and%2520expressing%2520it%2520as%2520a%2520dense%252C%2520differentiable%2520heatmap%252C%2520this%2520interface%2520enhances%2520interpretability%2520and%2520diagnostic%2520analysis%2520and%2520yields%2520more%2520concentrated%2520evidence%2520on%2520the%2520target.%2520A%2520learnable%2520concentration%2520token%2520aggregates%2520features%2520of%2520the%2520image%2520and%2520reasoning%2520text%2520to%2520generate%2520this%2520positional%2520prior%252C%2520which%2520is%2520decoded%2520to%2520precise%2520masks%2520through%2520a%2520lightweight%2520decoder%252C%2520providing%2520a%2520direct%2520connection%2520between%2520reasoning%2520and%2520segmentation.%2520Across%2520the%2520RefCOCO%2520series%2520and%2520ReasonSeg%252C%2520CoPRS%2520matches%2520or%2520surpasses%2520the%2520best%2520reported%2520metrics%2520on%2520each%2520standard%2520split%2520under%2520comparable%2520protocols%252C%2520with%2520performance%2520at%2520or%2520above%2520the%2520prior%2520state%2520of%2520the%2520art%2520across%2520both%2520validation%2520and%2520test%2520partitions.%2520Extensive%2520experiments%2520demonstrate%2520a%2520strong%2520positive%2520correlation%2520among%2520the%2520CoT%2520trajectory%252C%2520the%2520generated%2520heatmap%252C%2520and%2520the%2520decoded%2520mask%252C%2520supporting%2520an%2520interpretable%2520alignment%2520between%2520the%2520reasoning%2520output%2520and%2520downstream%2520mask%2520generation.%2520Collectively%252C%2520these%2520findings%2520support%2520the%2520utility%2520of%2520this%2520paradigm%2520in%2520bridging%2520reasoning%2520and%2520segmentation%2520and%2520show%2520advantages%2520in%2520concentration%2520driven%2520by%2520reasoning%2520and%2520in%2520more%2520precise%2520mask%2520prediction.%2520Code%252C%2520checkpoints%2520and%2520logs%2520are%2520released%2520at%2520https%253A//github.com/ZhenyuLU-Heliodore/CoPRS.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11173v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoPRS%3A%20Learning%20Positional%20Prior%20from%20Chain-of-Thought%20for%20Reasoning%20Segmentation&entry.906535625=Zhenyu%20Lu%20and%20Liupeng%20Li%20and%20Jinpeng%20Wang%20and%20Yan%20Feng%20and%20Bin%20Chen%20and%20Ke%20Chen%20and%20Yaowei%20Wang&entry.1292438233=Existing%20works%20on%20reasoning%20segmentation%20either%20connect%20hidden%20features%20from%20a%20language%20model%20directly%20to%20a%20mask%20decoder%20or%20represent%20positions%20in%20text%2C%20which%20limits%20interpretability%20and%20semantic%20detail.%20To%20solve%20this%2C%20we%20present%20CoPRS%2C%20a%20Multi-modal%20Chain-of-Thought%20%28MCoT%29-based%20positional%20perception%20model%20that%20bridges%20language%20reasoning%20to%20segmentation%20through%20a%20differentiable%20and%20interpretable%20positional%20prior%20instantiated%20as%20a%20heatmap.%20By%20making%20the%20reasoning%20process%20clear%20via%20MCoT%20and%20expressing%20it%20as%20a%20dense%2C%20differentiable%20heatmap%2C%20this%20interface%20enhances%20interpretability%20and%20diagnostic%20analysis%20and%20yields%20more%20concentrated%20evidence%20on%20the%20target.%20A%20learnable%20concentration%20token%20aggregates%20features%20of%20the%20image%20and%20reasoning%20text%20to%20generate%20this%20positional%20prior%2C%20which%20is%20decoded%20to%20precise%20masks%20through%20a%20lightweight%20decoder%2C%20providing%20a%20direct%20connection%20between%20reasoning%20and%20segmentation.%20Across%20the%20RefCOCO%20series%20and%20ReasonSeg%2C%20CoPRS%20matches%20or%20surpasses%20the%20best%20reported%20metrics%20on%20each%20standard%20split%20under%20comparable%20protocols%2C%20with%20performance%20at%20or%20above%20the%20prior%20state%20of%20the%20art%20across%20both%20validation%20and%20test%20partitions.%20Extensive%20experiments%20demonstrate%20a%20strong%20positive%20correlation%20among%20the%20CoT%20trajectory%2C%20the%20generated%20heatmap%2C%20and%20the%20decoded%20mask%2C%20supporting%20an%20interpretable%20alignment%20between%20the%20reasoning%20output%20and%20downstream%20mask%20generation.%20Collectively%2C%20these%20findings%20support%20the%20utility%20of%20this%20paradigm%20in%20bridging%20reasoning%20and%20segmentation%20and%20show%20advantages%20in%20concentration%20driven%20by%20reasoning%20and%20in%20more%20precise%20mask%20prediction.%20Code%2C%20checkpoints%20and%20logs%20are%20released%20at%20https%3A//github.com/ZhenyuLU-Heliodore/CoPRS.git.&entry.1838667208=http%3A//arxiv.org/abs/2510.11173v2&entry.124074799=Read"},
{"title": "UnReflectAnything: RGB-Only Highlight Removal by Rendering Synthetic Specular Supervision", "author": "Alberto Rota and Mert Kiray and Mert Asim Karaoglu and Patrick Ruhkamp and Elena De Momi and Nassir Navabm and Benjamin Busam", "abstract": "Specular highlights distort appearance, obscure texture, and hinder geometric reasoning in both natural and surgical imagery. We present UnReflectAnything, an RGB-only framework that removes highlights from a single image by predicting a highlight map together with a reflection-free diffuse reconstruction. The model uses a frozen vision transformer encoder to extract multi-scale features, a lightweight head to localize specular regions, and a token-level inpainting module that restores corrupted feature patches before producing the final diffuse image. To overcome the lack of paired supervision, we introduce a Virtual Highlight Synthesis pipeline that renders physically plausible specularities using monocular geometry, Fresnel-aware shading, and randomized lighting which enables training on arbitrary RGB images with correct geometric structure. UnReflectAnything generalizes across natural and surgical domains where non-Lambertian surfaces and non-uniform lighting create severe highlights and it achieves competitive performance with state-of-the-art results on several benchmarks. Project Page: https://alberto-rota.github.io/UnReflectAnything/", "link": "http://arxiv.org/abs/2512.09583v1", "date": "2025-12-10", "relevancy": 2.1266, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5516}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.52}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnReflectAnything%3A%20RGB-Only%20Highlight%20Removal%20by%20Rendering%20Synthetic%20Specular%20Supervision&body=Title%3A%20UnReflectAnything%3A%20RGB-Only%20Highlight%20Removal%20by%20Rendering%20Synthetic%20Specular%20Supervision%0AAuthor%3A%20Alberto%20Rota%20and%20Mert%20Kiray%20and%20Mert%20Asim%20Karaoglu%20and%20Patrick%20Ruhkamp%20and%20Elena%20De%20Momi%20and%20Nassir%20Navabm%20and%20Benjamin%20Busam%0AAbstract%3A%20Specular%20highlights%20distort%20appearance%2C%20obscure%20texture%2C%20and%20hinder%20geometric%20reasoning%20in%20both%20natural%20and%20surgical%20imagery.%20We%20present%20UnReflectAnything%2C%20an%20RGB-only%20framework%20that%20removes%20highlights%20from%20a%20single%20image%20by%20predicting%20a%20highlight%20map%20together%20with%20a%20reflection-free%20diffuse%20reconstruction.%20The%20model%20uses%20a%20frozen%20vision%20transformer%20encoder%20to%20extract%20multi-scale%20features%2C%20a%20lightweight%20head%20to%20localize%20specular%20regions%2C%20and%20a%20token-level%20inpainting%20module%20that%20restores%20corrupted%20feature%20patches%20before%20producing%20the%20final%20diffuse%20image.%20To%20overcome%20the%20lack%20of%20paired%20supervision%2C%20we%20introduce%20a%20Virtual%20Highlight%20Synthesis%20pipeline%20that%20renders%20physically%20plausible%20specularities%20using%20monocular%20geometry%2C%20Fresnel-aware%20shading%2C%20and%20randomized%20lighting%20which%20enables%20training%20on%20arbitrary%20RGB%20images%20with%20correct%20geometric%20structure.%20UnReflectAnything%20generalizes%20across%20natural%20and%20surgical%20domains%20where%20non-Lambertian%20surfaces%20and%20non-uniform%20lighting%20create%20severe%20highlights%20and%20it%20achieves%20competitive%20performance%20with%20state-of-the-art%20results%20on%20several%20benchmarks.%20Project%20Page%3A%20https%3A//alberto-rota.github.io/UnReflectAnything/%0ALink%3A%20http%3A//arxiv.org/abs/2512.09583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnReflectAnything%253A%2520RGB-Only%2520Highlight%2520Removal%2520by%2520Rendering%2520Synthetic%2520Specular%2520Supervision%26entry.906535625%3DAlberto%2520Rota%2520and%2520Mert%2520Kiray%2520and%2520Mert%2520Asim%2520Karaoglu%2520and%2520Patrick%2520Ruhkamp%2520and%2520Elena%2520De%2520Momi%2520and%2520Nassir%2520Navabm%2520and%2520Benjamin%2520Busam%26entry.1292438233%3DSpecular%2520highlights%2520distort%2520appearance%252C%2520obscure%2520texture%252C%2520and%2520hinder%2520geometric%2520reasoning%2520in%2520both%2520natural%2520and%2520surgical%2520imagery.%2520We%2520present%2520UnReflectAnything%252C%2520an%2520RGB-only%2520framework%2520that%2520removes%2520highlights%2520from%2520a%2520single%2520image%2520by%2520predicting%2520a%2520highlight%2520map%2520together%2520with%2520a%2520reflection-free%2520diffuse%2520reconstruction.%2520The%2520model%2520uses%2520a%2520frozen%2520vision%2520transformer%2520encoder%2520to%2520extract%2520multi-scale%2520features%252C%2520a%2520lightweight%2520head%2520to%2520localize%2520specular%2520regions%252C%2520and%2520a%2520token-level%2520inpainting%2520module%2520that%2520restores%2520corrupted%2520feature%2520patches%2520before%2520producing%2520the%2520final%2520diffuse%2520image.%2520To%2520overcome%2520the%2520lack%2520of%2520paired%2520supervision%252C%2520we%2520introduce%2520a%2520Virtual%2520Highlight%2520Synthesis%2520pipeline%2520that%2520renders%2520physically%2520plausible%2520specularities%2520using%2520monocular%2520geometry%252C%2520Fresnel-aware%2520shading%252C%2520and%2520randomized%2520lighting%2520which%2520enables%2520training%2520on%2520arbitrary%2520RGB%2520images%2520with%2520correct%2520geometric%2520structure.%2520UnReflectAnything%2520generalizes%2520across%2520natural%2520and%2520surgical%2520domains%2520where%2520non-Lambertian%2520surfaces%2520and%2520non-uniform%2520lighting%2520create%2520severe%2520highlights%2520and%2520it%2520achieves%2520competitive%2520performance%2520with%2520state-of-the-art%2520results%2520on%2520several%2520benchmarks.%2520Project%2520Page%253A%2520https%253A//alberto-rota.github.io/UnReflectAnything/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnReflectAnything%3A%20RGB-Only%20Highlight%20Removal%20by%20Rendering%20Synthetic%20Specular%20Supervision&entry.906535625=Alberto%20Rota%20and%20Mert%20Kiray%20and%20Mert%20Asim%20Karaoglu%20and%20Patrick%20Ruhkamp%20and%20Elena%20De%20Momi%20and%20Nassir%20Navabm%20and%20Benjamin%20Busam&entry.1292438233=Specular%20highlights%20distort%20appearance%2C%20obscure%20texture%2C%20and%20hinder%20geometric%20reasoning%20in%20both%20natural%20and%20surgical%20imagery.%20We%20present%20UnReflectAnything%2C%20an%20RGB-only%20framework%20that%20removes%20highlights%20from%20a%20single%20image%20by%20predicting%20a%20highlight%20map%20together%20with%20a%20reflection-free%20diffuse%20reconstruction.%20The%20model%20uses%20a%20frozen%20vision%20transformer%20encoder%20to%20extract%20multi-scale%20features%2C%20a%20lightweight%20head%20to%20localize%20specular%20regions%2C%20and%20a%20token-level%20inpainting%20module%20that%20restores%20corrupted%20feature%20patches%20before%20producing%20the%20final%20diffuse%20image.%20To%20overcome%20the%20lack%20of%20paired%20supervision%2C%20we%20introduce%20a%20Virtual%20Highlight%20Synthesis%20pipeline%20that%20renders%20physically%20plausible%20specularities%20using%20monocular%20geometry%2C%20Fresnel-aware%20shading%2C%20and%20randomized%20lighting%20which%20enables%20training%20on%20arbitrary%20RGB%20images%20with%20correct%20geometric%20structure.%20UnReflectAnything%20generalizes%20across%20natural%20and%20surgical%20domains%20where%20non-Lambertian%20surfaces%20and%20non-uniform%20lighting%20create%20severe%20highlights%20and%20it%20achieves%20competitive%20performance%20with%20state-of-the-art%20results%20on%20several%20benchmarks.%20Project%20Page%3A%20https%3A//alberto-rota.github.io/UnReflectAnything/&entry.1838667208=http%3A//arxiv.org/abs/2512.09583v1&entry.124074799=Read"},
{"title": "Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized", "author": "Er Jin and Yang Zhang and Yongli Mou and Yanfei Dong and Stefan Decker and Kenji Kawaguchi and Johannes Stegmaier", "abstract": "Recent advances in generative models have demonstrated an exceptional ability to produce highly realistic images. However, previous studies show that generated images often resemble the training data, and this problem becomes more severe as the model size increases. Memorizing training data can lead to legal challenges, including copyright infringement, violations of portrait rights, and trademark violations. Existing approaches to mitigating memorization mainly focus on manipulating the denoising sampling process to steer image embeddings away from the memorized embedding space or employ unlearning methods that require training on datasets containing specific sets of memorized concepts. However, existing methods often incur substantial computational overhead during sampling, or focus narrowly on removing one or more groups of target concepts, imposing a significant limitation on their scalability. To understand and mitigate these problems, our work, UniForget, offers a new perspective on understanding the root cause of memorization. Our work demonstrates that specific parts of the model are responsible for copyrighted content generation. By applying model pruning, we can effectively suppress the probability of generating copyrighted content without targeting specific concepts while preserving the general generative capabilities of the model. Additionally, we show that our approach is both orthogonal and complementary to existing unlearning methods, thereby highlighting its potential to improve current unlearning and de-memorization techniques.", "link": "http://arxiv.org/abs/2512.09687v1", "date": "2025-12-10", "relevancy": 2.1194, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.581}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5336}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unconsciously%20Forget%3A%20Mitigating%20Memorization%3B%20Without%20Knowing%20What%20is%20being%20Memorized&body=Title%3A%20Unconsciously%20Forget%3A%20Mitigating%20Memorization%3B%20Without%20Knowing%20What%20is%20being%20Memorized%0AAuthor%3A%20Er%20Jin%20and%20Yang%20Zhang%20and%20Yongli%20Mou%20and%20Yanfei%20Dong%20and%20Stefan%20Decker%20and%20Kenji%20Kawaguchi%20and%20Johannes%20Stegmaier%0AAbstract%3A%20Recent%20advances%20in%20generative%20models%20have%20demonstrated%20an%20exceptional%20ability%20to%20produce%20highly%20realistic%20images.%20However%2C%20previous%20studies%20show%20that%20generated%20images%20often%20resemble%20the%20training%20data%2C%20and%20this%20problem%20becomes%20more%20severe%20as%20the%20model%20size%20increases.%20Memorizing%20training%20data%20can%20lead%20to%20legal%20challenges%2C%20including%20copyright%20infringement%2C%20violations%20of%20portrait%20rights%2C%20and%20trademark%20violations.%20Existing%20approaches%20to%20mitigating%20memorization%20mainly%20focus%20on%20manipulating%20the%20denoising%20sampling%20process%20to%20steer%20image%20embeddings%20away%20from%20the%20memorized%20embedding%20space%20or%20employ%20unlearning%20methods%20that%20require%20training%20on%20datasets%20containing%20specific%20sets%20of%20memorized%20concepts.%20However%2C%20existing%20methods%20often%20incur%20substantial%20computational%20overhead%20during%20sampling%2C%20or%20focus%20narrowly%20on%20removing%20one%20or%20more%20groups%20of%20target%20concepts%2C%20imposing%20a%20significant%20limitation%20on%20their%20scalability.%20To%20understand%20and%20mitigate%20these%20problems%2C%20our%20work%2C%20UniForget%2C%20offers%20a%20new%20perspective%20on%20understanding%20the%20root%20cause%20of%20memorization.%20Our%20work%20demonstrates%20that%20specific%20parts%20of%20the%20model%20are%20responsible%20for%20copyrighted%20content%20generation.%20By%20applying%20model%20pruning%2C%20we%20can%20effectively%20suppress%20the%20probability%20of%20generating%20copyrighted%20content%20without%20targeting%20specific%20concepts%20while%20preserving%20the%20general%20generative%20capabilities%20of%20the%20model.%20Additionally%2C%20we%20show%20that%20our%20approach%20is%20both%20orthogonal%20and%20complementary%20to%20existing%20unlearning%20methods%2C%20thereby%20highlighting%20its%20potential%20to%20improve%20current%20unlearning%20and%20de-memorization%20techniques.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnconsciously%2520Forget%253A%2520Mitigating%2520Memorization%253B%2520Without%2520Knowing%2520What%2520is%2520being%2520Memorized%26entry.906535625%3DEr%2520Jin%2520and%2520Yang%2520Zhang%2520and%2520Yongli%2520Mou%2520and%2520Yanfei%2520Dong%2520and%2520Stefan%2520Decker%2520and%2520Kenji%2520Kawaguchi%2520and%2520Johannes%2520Stegmaier%26entry.1292438233%3DRecent%2520advances%2520in%2520generative%2520models%2520have%2520demonstrated%2520an%2520exceptional%2520ability%2520to%2520produce%2520highly%2520realistic%2520images.%2520However%252C%2520previous%2520studies%2520show%2520that%2520generated%2520images%2520often%2520resemble%2520the%2520training%2520data%252C%2520and%2520this%2520problem%2520becomes%2520more%2520severe%2520as%2520the%2520model%2520size%2520increases.%2520Memorizing%2520training%2520data%2520can%2520lead%2520to%2520legal%2520challenges%252C%2520including%2520copyright%2520infringement%252C%2520violations%2520of%2520portrait%2520rights%252C%2520and%2520trademark%2520violations.%2520Existing%2520approaches%2520to%2520mitigating%2520memorization%2520mainly%2520focus%2520on%2520manipulating%2520the%2520denoising%2520sampling%2520process%2520to%2520steer%2520image%2520embeddings%2520away%2520from%2520the%2520memorized%2520embedding%2520space%2520or%2520employ%2520unlearning%2520methods%2520that%2520require%2520training%2520on%2520datasets%2520containing%2520specific%2520sets%2520of%2520memorized%2520concepts.%2520However%252C%2520existing%2520methods%2520often%2520incur%2520substantial%2520computational%2520overhead%2520during%2520sampling%252C%2520or%2520focus%2520narrowly%2520on%2520removing%2520one%2520or%2520more%2520groups%2520of%2520target%2520concepts%252C%2520imposing%2520a%2520significant%2520limitation%2520on%2520their%2520scalability.%2520To%2520understand%2520and%2520mitigate%2520these%2520problems%252C%2520our%2520work%252C%2520UniForget%252C%2520offers%2520a%2520new%2520perspective%2520on%2520understanding%2520the%2520root%2520cause%2520of%2520memorization.%2520Our%2520work%2520demonstrates%2520that%2520specific%2520parts%2520of%2520the%2520model%2520are%2520responsible%2520for%2520copyrighted%2520content%2520generation.%2520By%2520applying%2520model%2520pruning%252C%2520we%2520can%2520effectively%2520suppress%2520the%2520probability%2520of%2520generating%2520copyrighted%2520content%2520without%2520targeting%2520specific%2520concepts%2520while%2520preserving%2520the%2520general%2520generative%2520capabilities%2520of%2520the%2520model.%2520Additionally%252C%2520we%2520show%2520that%2520our%2520approach%2520is%2520both%2520orthogonal%2520and%2520complementary%2520to%2520existing%2520unlearning%2520methods%252C%2520thereby%2520highlighting%2520its%2520potential%2520to%2520improve%2520current%2520unlearning%2520and%2520de-memorization%2520techniques.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unconsciously%20Forget%3A%20Mitigating%20Memorization%3B%20Without%20Knowing%20What%20is%20being%20Memorized&entry.906535625=Er%20Jin%20and%20Yang%20Zhang%20and%20Yongli%20Mou%20and%20Yanfei%20Dong%20and%20Stefan%20Decker%20and%20Kenji%20Kawaguchi%20and%20Johannes%20Stegmaier&entry.1292438233=Recent%20advances%20in%20generative%20models%20have%20demonstrated%20an%20exceptional%20ability%20to%20produce%20highly%20realistic%20images.%20However%2C%20previous%20studies%20show%20that%20generated%20images%20often%20resemble%20the%20training%20data%2C%20and%20this%20problem%20becomes%20more%20severe%20as%20the%20model%20size%20increases.%20Memorizing%20training%20data%20can%20lead%20to%20legal%20challenges%2C%20including%20copyright%20infringement%2C%20violations%20of%20portrait%20rights%2C%20and%20trademark%20violations.%20Existing%20approaches%20to%20mitigating%20memorization%20mainly%20focus%20on%20manipulating%20the%20denoising%20sampling%20process%20to%20steer%20image%20embeddings%20away%20from%20the%20memorized%20embedding%20space%20or%20employ%20unlearning%20methods%20that%20require%20training%20on%20datasets%20containing%20specific%20sets%20of%20memorized%20concepts.%20However%2C%20existing%20methods%20often%20incur%20substantial%20computational%20overhead%20during%20sampling%2C%20or%20focus%20narrowly%20on%20removing%20one%20or%20more%20groups%20of%20target%20concepts%2C%20imposing%20a%20significant%20limitation%20on%20their%20scalability.%20To%20understand%20and%20mitigate%20these%20problems%2C%20our%20work%2C%20UniForget%2C%20offers%20a%20new%20perspective%20on%20understanding%20the%20root%20cause%20of%20memorization.%20Our%20work%20demonstrates%20that%20specific%20parts%20of%20the%20model%20are%20responsible%20for%20copyrighted%20content%20generation.%20By%20applying%20model%20pruning%2C%20we%20can%20effectively%20suppress%20the%20probability%20of%20generating%20copyrighted%20content%20without%20targeting%20specific%20concepts%20while%20preserving%20the%20general%20generative%20capabilities%20of%20the%20model.%20Additionally%2C%20we%20show%20that%20our%20approach%20is%20both%20orthogonal%20and%20complementary%20to%20existing%20unlearning%20methods%2C%20thereby%20highlighting%20its%20potential%20to%20improve%20current%20unlearning%20and%20de-memorization%20techniques.&entry.1838667208=http%3A//arxiv.org/abs/2512.09687v1&entry.124074799=Read"},
{"title": "RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning", "author": "Yucan Guo and Miao Su and Saiping Guan and Zihao Sun and Xiaolong Jin and Jiafeng Guo and Xueqi Cheng", "abstract": "Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \\model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \\model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \\model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.", "link": "http://arxiv.org/abs/2512.09487v1", "date": "2025-12-10", "relevancy": 2.1194, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5426}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5306}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RouteRAG%3A%20Efficient%20Retrieval-Augmented%20Generation%20from%20Text%20and%20Graph%20via%20Reinforcement%20Learning&body=Title%3A%20RouteRAG%3A%20Efficient%20Retrieval-Augmented%20Generation%20from%20Text%20and%20Graph%20via%20Reinforcement%20Learning%0AAuthor%3A%20Yucan%20Guo%20and%20Miao%20Su%20and%20Saiping%20Guan%20and%20Zihao%20Sun%20and%20Xiaolong%20Jin%20and%20Jiafeng%20Guo%20and%20Xueqi%20Cheng%0AAbstract%3A%20Retrieval-Augmented%20Generation%20%28RAG%29%20integrates%20non-parametric%20knowledge%20into%20Large%20Language%20Models%20%28LLMs%29%2C%20typically%20from%20unstructured%20texts%20and%20structured%20graphs.%20While%20recent%20progress%20has%20advanced%20text-based%20RAG%20to%20multi-turn%20reasoning%20through%20Reinforcement%20Learning%20%28RL%29%2C%20extending%20these%20advances%20to%20hybrid%20retrieval%20introduces%20additional%20challenges.%20Existing%20graph-based%20or%20hybrid%20systems%20typically%20depend%20on%20fixed%20or%20handcrafted%20retrieval%20pipelines%2C%20lacking%20the%20ability%20to%20integrate%20supplementary%20evidence%20as%20reasoning%20unfolds.%20Besides%2C%20while%20graph%20evidence%20provides%20relational%20structures%20crucial%20for%20multi-hop%20reasoning%2C%20it%20is%20substantially%20more%20expensive%20to%20retrieve.%20To%20address%20these%20limitations%2C%20we%20introduce%20%5Cmodel%7B%7D%2C%20an%20RL-based%20framework%20that%20enables%20LLMs%20to%20perform%20multi-turn%20and%20adaptive%20graph-text%20hybrid%20RAG.%20%5Cmodel%7B%7D%20jointly%20optimizes%20the%20entire%20generation%20process%20via%20RL%2C%20allowing%20the%20model%20to%20learn%20when%20to%20reason%2C%20what%20to%20retrieve%20from%20either%20texts%20or%20graphs%2C%20and%20when%20to%20produce%20final%20answers%2C%20all%20within%20a%20unified%20generation%20policy.%20To%20guide%20this%20learning%20process%2C%20we%20design%20a%20two-stage%20training%20framework%20that%20accounts%20for%20both%20task%20outcome%20and%20retrieval%20efficiency%2C%20enabling%20the%20model%20to%20exploit%20hybrid%20evidence%20while%20avoiding%20unnecessary%20retrieval%20overhead.%20Experimental%20results%20across%20five%20question%20answering%20benchmarks%20demonstrate%20that%20%5Cmodel%7B%7D%20significantly%20outperforms%20existing%20RAG%20baselines%2C%20highlighting%20the%20benefits%20of%20end-to-end%20RL%20in%20supporting%20adaptive%20and%20efficient%20retrieval%20for%20complex%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRouteRAG%253A%2520Efficient%2520Retrieval-Augmented%2520Generation%2520from%2520Text%2520and%2520Graph%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DYucan%2520Guo%2520and%2520Miao%2520Su%2520and%2520Saiping%2520Guan%2520and%2520Zihao%2520Sun%2520and%2520Xiaolong%2520Jin%2520and%2520Jiafeng%2520Guo%2520and%2520Xueqi%2520Cheng%26entry.1292438233%3DRetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520integrates%2520non-parametric%2520knowledge%2520into%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520typically%2520from%2520unstructured%2520texts%2520and%2520structured%2520graphs.%2520While%2520recent%2520progress%2520has%2520advanced%2520text-based%2520RAG%2520to%2520multi-turn%2520reasoning%2520through%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520extending%2520these%2520advances%2520to%2520hybrid%2520retrieval%2520introduces%2520additional%2520challenges.%2520Existing%2520graph-based%2520or%2520hybrid%2520systems%2520typically%2520depend%2520on%2520fixed%2520or%2520handcrafted%2520retrieval%2520pipelines%252C%2520lacking%2520the%2520ability%2520to%2520integrate%2520supplementary%2520evidence%2520as%2520reasoning%2520unfolds.%2520Besides%252C%2520while%2520graph%2520evidence%2520provides%2520relational%2520structures%2520crucial%2520for%2520multi-hop%2520reasoning%252C%2520it%2520is%2520substantially%2520more%2520expensive%2520to%2520retrieve.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520%255Cmodel%257B%257D%252C%2520an%2520RL-based%2520framework%2520that%2520enables%2520LLMs%2520to%2520perform%2520multi-turn%2520and%2520adaptive%2520graph-text%2520hybrid%2520RAG.%2520%255Cmodel%257B%257D%2520jointly%2520optimizes%2520the%2520entire%2520generation%2520process%2520via%2520RL%252C%2520allowing%2520the%2520model%2520to%2520learn%2520when%2520to%2520reason%252C%2520what%2520to%2520retrieve%2520from%2520either%2520texts%2520or%2520graphs%252C%2520and%2520when%2520to%2520produce%2520final%2520answers%252C%2520all%2520within%2520a%2520unified%2520generation%2520policy.%2520To%2520guide%2520this%2520learning%2520process%252C%2520we%2520design%2520a%2520two-stage%2520training%2520framework%2520that%2520accounts%2520for%2520both%2520task%2520outcome%2520and%2520retrieval%2520efficiency%252C%2520enabling%2520the%2520model%2520to%2520exploit%2520hybrid%2520evidence%2520while%2520avoiding%2520unnecessary%2520retrieval%2520overhead.%2520Experimental%2520results%2520across%2520five%2520question%2520answering%2520benchmarks%2520demonstrate%2520that%2520%255Cmodel%257B%257D%2520significantly%2520outperforms%2520existing%2520RAG%2520baselines%252C%2520highlighting%2520the%2520benefits%2520of%2520end-to-end%2520RL%2520in%2520supporting%2520adaptive%2520and%2520efficient%2520retrieval%2520for%2520complex%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RouteRAG%3A%20Efficient%20Retrieval-Augmented%20Generation%20from%20Text%20and%20Graph%20via%20Reinforcement%20Learning&entry.906535625=Yucan%20Guo%20and%20Miao%20Su%20and%20Saiping%20Guan%20and%20Zihao%20Sun%20and%20Xiaolong%20Jin%20and%20Jiafeng%20Guo%20and%20Xueqi%20Cheng&entry.1292438233=Retrieval-Augmented%20Generation%20%28RAG%29%20integrates%20non-parametric%20knowledge%20into%20Large%20Language%20Models%20%28LLMs%29%2C%20typically%20from%20unstructured%20texts%20and%20structured%20graphs.%20While%20recent%20progress%20has%20advanced%20text-based%20RAG%20to%20multi-turn%20reasoning%20through%20Reinforcement%20Learning%20%28RL%29%2C%20extending%20these%20advances%20to%20hybrid%20retrieval%20introduces%20additional%20challenges.%20Existing%20graph-based%20or%20hybrid%20systems%20typically%20depend%20on%20fixed%20or%20handcrafted%20retrieval%20pipelines%2C%20lacking%20the%20ability%20to%20integrate%20supplementary%20evidence%20as%20reasoning%20unfolds.%20Besides%2C%20while%20graph%20evidence%20provides%20relational%20structures%20crucial%20for%20multi-hop%20reasoning%2C%20it%20is%20substantially%20more%20expensive%20to%20retrieve.%20To%20address%20these%20limitations%2C%20we%20introduce%20%5Cmodel%7B%7D%2C%20an%20RL-based%20framework%20that%20enables%20LLMs%20to%20perform%20multi-turn%20and%20adaptive%20graph-text%20hybrid%20RAG.%20%5Cmodel%7B%7D%20jointly%20optimizes%20the%20entire%20generation%20process%20via%20RL%2C%20allowing%20the%20model%20to%20learn%20when%20to%20reason%2C%20what%20to%20retrieve%20from%20either%20texts%20or%20graphs%2C%20and%20when%20to%20produce%20final%20answers%2C%20all%20within%20a%20unified%20generation%20policy.%20To%20guide%20this%20learning%20process%2C%20we%20design%20a%20two-stage%20training%20framework%20that%20accounts%20for%20both%20task%20outcome%20and%20retrieval%20efficiency%2C%20enabling%20the%20model%20to%20exploit%20hybrid%20evidence%20while%20avoiding%20unnecessary%20retrieval%20overhead.%20Experimental%20results%20across%20five%20question%20answering%20benchmarks%20demonstrate%20that%20%5Cmodel%7B%7D%20significantly%20outperforms%20existing%20RAG%20baselines%2C%20highlighting%20the%20benefits%20of%20end-to-end%20RL%20in%20supporting%20adaptive%20and%20efficient%20retrieval%20for%20complex%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2512.09487v1&entry.124074799=Read"},
{"title": "MACS: Multi-source Audio-to-image Generation with Contextual Significance and Semantic Alignment", "author": "Hao Zhou and Xiaobao Guo and Yuzhe Zhu and Adams Wai-Kin Kong", "abstract": "Propelled by the breakthrough in deep generative models, audio-to-image generation has emerged as a pivotal cross-modal task that converts complex auditory signals into rich visual representations. However, previous works only focus on single-source audio inputs for image generation, ignoring the multi-source characteristic in natural auditory scenes, thus limiting the performance in generating comprehensive visual content. To bridge this gap, we propose a method called MACS to conduct multi-source audio-to-image generation. To our best knowledge, this is the first work that explicitly separates multi-source audio to capture the rich audio components before image generation. MACS is a two-stage method. In the first stage, multi-source audio inputs are separated by a weakly supervised method, where the audio and text labels are semantically aligned by casting into a common space using the large pre-trained CLAP model. We introduce a ranking loss to consider the contextual significance of the separated audio signals. In the second stage, effective image generation is achieved by mapping the separated audio signals to the generation condition using only a trainable adapter and a MLP layer. We preprocess the LLP dataset as the first full multi-source audio-to-image generation benchmark. The experiments are conducted on multi-source, mixed-source, and single-source audio-to-image generation tasks. The proposed MACS outperforms the current state-of-the-art methods in 17 out of the 21 evaluation indexes on all tasks and delivers superior visual quality.", "link": "http://arxiv.org/abs/2503.10287v3", "date": "2025-12-10", "relevancy": 2.1185, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5384}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.526}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MACS%3A%20Multi-source%20Audio-to-image%20Generation%20with%20Contextual%20Significance%20and%20Semantic%20Alignment&body=Title%3A%20MACS%3A%20Multi-source%20Audio-to-image%20Generation%20with%20Contextual%20Significance%20and%20Semantic%20Alignment%0AAuthor%3A%20Hao%20Zhou%20and%20Xiaobao%20Guo%20and%20Yuzhe%20Zhu%20and%20Adams%20Wai-Kin%20Kong%0AAbstract%3A%20Propelled%20by%20the%20breakthrough%20in%20deep%20generative%20models%2C%20audio-to-image%20generation%20has%20emerged%20as%20a%20pivotal%20cross-modal%20task%20that%20converts%20complex%20auditory%20signals%20into%20rich%20visual%20representations.%20However%2C%20previous%20works%20only%20focus%20on%20single-source%20audio%20inputs%20for%20image%20generation%2C%20ignoring%20the%20multi-source%20characteristic%20in%20natural%20auditory%20scenes%2C%20thus%20limiting%20the%20performance%20in%20generating%20comprehensive%20visual%20content.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20method%20called%20MACS%20to%20conduct%20multi-source%20audio-to-image%20generation.%20To%20our%20best%20knowledge%2C%20this%20is%20the%20first%20work%20that%20explicitly%20separates%20multi-source%20audio%20to%20capture%20the%20rich%20audio%20components%20before%20image%20generation.%20MACS%20is%20a%20two-stage%20method.%20In%20the%20first%20stage%2C%20multi-source%20audio%20inputs%20are%20separated%20by%20a%20weakly%20supervised%20method%2C%20where%20the%20audio%20and%20text%20labels%20are%20semantically%20aligned%20by%20casting%20into%20a%20common%20space%20using%20the%20large%20pre-trained%20CLAP%20model.%20We%20introduce%20a%20ranking%20loss%20to%20consider%20the%20contextual%20significance%20of%20the%20separated%20audio%20signals.%20In%20the%20second%20stage%2C%20effective%20image%20generation%20is%20achieved%20by%20mapping%20the%20separated%20audio%20signals%20to%20the%20generation%20condition%20using%20only%20a%20trainable%20adapter%20and%20a%20MLP%20layer.%20We%20preprocess%20the%20LLP%20dataset%20as%20the%20first%20full%20multi-source%20audio-to-image%20generation%20benchmark.%20The%20experiments%20are%20conducted%20on%20multi-source%2C%20mixed-source%2C%20and%20single-source%20audio-to-image%20generation%20tasks.%20The%20proposed%20MACS%20outperforms%20the%20current%20state-of-the-art%20methods%20in%2017%20out%20of%20the%2021%20evaluation%20indexes%20on%20all%20tasks%20and%20delivers%20superior%20visual%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2503.10287v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMACS%253A%2520Multi-source%2520Audio-to-image%2520Generation%2520with%2520Contextual%2520Significance%2520and%2520Semantic%2520Alignment%26entry.906535625%3DHao%2520Zhou%2520and%2520Xiaobao%2520Guo%2520and%2520Yuzhe%2520Zhu%2520and%2520Adams%2520Wai-Kin%2520Kong%26entry.1292438233%3DPropelled%2520by%2520the%2520breakthrough%2520in%2520deep%2520generative%2520models%252C%2520audio-to-image%2520generation%2520has%2520emerged%2520as%2520a%2520pivotal%2520cross-modal%2520task%2520that%2520converts%2520complex%2520auditory%2520signals%2520into%2520rich%2520visual%2520representations.%2520However%252C%2520previous%2520works%2520only%2520focus%2520on%2520single-source%2520audio%2520inputs%2520for%2520image%2520generation%252C%2520ignoring%2520the%2520multi-source%2520characteristic%2520in%2520natural%2520auditory%2520scenes%252C%2520thus%2520limiting%2520the%2520performance%2520in%2520generating%2520comprehensive%2520visual%2520content.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520method%2520called%2520MACS%2520to%2520conduct%2520multi-source%2520audio-to-image%2520generation.%2520To%2520our%2520best%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520that%2520explicitly%2520separates%2520multi-source%2520audio%2520to%2520capture%2520the%2520rich%2520audio%2520components%2520before%2520image%2520generation.%2520MACS%2520is%2520a%2520two-stage%2520method.%2520In%2520the%2520first%2520stage%252C%2520multi-source%2520audio%2520inputs%2520are%2520separated%2520by%2520a%2520weakly%2520supervised%2520method%252C%2520where%2520the%2520audio%2520and%2520text%2520labels%2520are%2520semantically%2520aligned%2520by%2520casting%2520into%2520a%2520common%2520space%2520using%2520the%2520large%2520pre-trained%2520CLAP%2520model.%2520We%2520introduce%2520a%2520ranking%2520loss%2520to%2520consider%2520the%2520contextual%2520significance%2520of%2520the%2520separated%2520audio%2520signals.%2520In%2520the%2520second%2520stage%252C%2520effective%2520image%2520generation%2520is%2520achieved%2520by%2520mapping%2520the%2520separated%2520audio%2520signals%2520to%2520the%2520generation%2520condition%2520using%2520only%2520a%2520trainable%2520adapter%2520and%2520a%2520MLP%2520layer.%2520We%2520preprocess%2520the%2520LLP%2520dataset%2520as%2520the%2520first%2520full%2520multi-source%2520audio-to-image%2520generation%2520benchmark.%2520The%2520experiments%2520are%2520conducted%2520on%2520multi-source%252C%2520mixed-source%252C%2520and%2520single-source%2520audio-to-image%2520generation%2520tasks.%2520The%2520proposed%2520MACS%2520outperforms%2520the%2520current%2520state-of-the-art%2520methods%2520in%252017%2520out%2520of%2520the%252021%2520evaluation%2520indexes%2520on%2520all%2520tasks%2520and%2520delivers%2520superior%2520visual%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10287v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MACS%3A%20Multi-source%20Audio-to-image%20Generation%20with%20Contextual%20Significance%20and%20Semantic%20Alignment&entry.906535625=Hao%20Zhou%20and%20Xiaobao%20Guo%20and%20Yuzhe%20Zhu%20and%20Adams%20Wai-Kin%20Kong&entry.1292438233=Propelled%20by%20the%20breakthrough%20in%20deep%20generative%20models%2C%20audio-to-image%20generation%20has%20emerged%20as%20a%20pivotal%20cross-modal%20task%20that%20converts%20complex%20auditory%20signals%20into%20rich%20visual%20representations.%20However%2C%20previous%20works%20only%20focus%20on%20single-source%20audio%20inputs%20for%20image%20generation%2C%20ignoring%20the%20multi-source%20characteristic%20in%20natural%20auditory%20scenes%2C%20thus%20limiting%20the%20performance%20in%20generating%20comprehensive%20visual%20content.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20method%20called%20MACS%20to%20conduct%20multi-source%20audio-to-image%20generation.%20To%20our%20best%20knowledge%2C%20this%20is%20the%20first%20work%20that%20explicitly%20separates%20multi-source%20audio%20to%20capture%20the%20rich%20audio%20components%20before%20image%20generation.%20MACS%20is%20a%20two-stage%20method.%20In%20the%20first%20stage%2C%20multi-source%20audio%20inputs%20are%20separated%20by%20a%20weakly%20supervised%20method%2C%20where%20the%20audio%20and%20text%20labels%20are%20semantically%20aligned%20by%20casting%20into%20a%20common%20space%20using%20the%20large%20pre-trained%20CLAP%20model.%20We%20introduce%20a%20ranking%20loss%20to%20consider%20the%20contextual%20significance%20of%20the%20separated%20audio%20signals.%20In%20the%20second%20stage%2C%20effective%20image%20generation%20is%20achieved%20by%20mapping%20the%20separated%20audio%20signals%20to%20the%20generation%20condition%20using%20only%20a%20trainable%20adapter%20and%20a%20MLP%20layer.%20We%20preprocess%20the%20LLP%20dataset%20as%20the%20first%20full%20multi-source%20audio-to-image%20generation%20benchmark.%20The%20experiments%20are%20conducted%20on%20multi-source%2C%20mixed-source%2C%20and%20single-source%20audio-to-image%20generation%20tasks.%20The%20proposed%20MACS%20outperforms%20the%20current%20state-of-the-art%20methods%20in%2017%20out%20of%20the%2021%20evaluation%20indexes%20on%20all%20tasks%20and%20delivers%20superior%20visual%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2503.10287v3&entry.124074799=Read"},
{"title": "Hands-on Evaluation of Visual Transformers for Object Recognition and Detection", "author": "Dimitrios N. Vlachogiannis and Dimitrios A. Koutsomitropoulos", "abstract": "Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.", "link": "http://arxiv.org/abs/2512.09579v1", "date": "2025-12-10", "relevancy": 2.117, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5389}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5304}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hands-on%20Evaluation%20of%20Visual%20Transformers%20for%20Object%20Recognition%20and%20Detection&body=Title%3A%20Hands-on%20Evaluation%20of%20Visual%20Transformers%20for%20Object%20Recognition%20and%20Detection%0AAuthor%3A%20Dimitrios%20N.%20Vlachogiannis%20and%20Dimitrios%20A.%20Koutsomitropoulos%0AAbstract%3A%20Convolutional%20Neural%20Networks%20%28CNNs%29%20for%20computer%20vision%20sometimes%20struggle%20with%20understanding%20images%20in%20a%20global%20context%2C%20as%20they%20mainly%20focus%20on%20local%20patterns.%20On%20the%20other%20hand%2C%20Vision%20Transformers%20%28ViTs%29%2C%20inspired%20by%20models%20originally%20created%20for%20language%20processing%2C%20use%20self-attention%20mechanisms%2C%20which%20allow%20them%20to%20understand%20relationships%20across%20the%20entire%20image.%20In%20this%20paper%2C%20we%20compare%20different%20types%20of%20ViTs%20%28pure%2C%20hierarchical%2C%20and%20hybrid%29%20against%20traditional%20CNN%20models%20across%20various%20tasks%2C%20including%20object%20recognition%2C%20detection%2C%20and%20medical%20image%20classification.%20We%20conduct%20thorough%20tests%20on%20standard%20datasets%20like%20ImageNet%20for%20image%20classification%20and%20COCO%20for%20object%20detection.%20Additionally%2C%20we%20apply%20these%20models%20to%20medical%20imaging%20using%20the%20ChestX-ray14%20dataset.%20We%20find%20that%20hybrid%20and%20hierarchical%20transformers%2C%20especially%20Swin%20and%20CvT%2C%20offer%20a%20strong%20balance%20between%20accuracy%20and%20computational%20resources.%20Furthermore%2C%20by%20experimenting%20with%20data%20augmentation%20techniques%20on%20medical%20images%2C%20we%20discover%20significant%20performance%20improvements%2C%20particularly%20with%20the%20Swin%20Transformer%20model.%20Overall%2C%20our%20results%20indicate%20that%20Vision%20Transformers%20are%20competitive%20and%2C%20in%20many%20cases%2C%20outperform%20traditional%20CNNs%2C%20especially%20in%20scenarios%20requiring%20the%20understanding%20of%20global%20visual%20contexts%20like%20medical%20imaging.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHands-on%2520Evaluation%2520of%2520Visual%2520Transformers%2520for%2520Object%2520Recognition%2520and%2520Detection%26entry.906535625%3DDimitrios%2520N.%2520Vlachogiannis%2520and%2520Dimitrios%2520A.%2520Koutsomitropoulos%26entry.1292438233%3DConvolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520for%2520computer%2520vision%2520sometimes%2520struggle%2520with%2520understanding%2520images%2520in%2520a%2520global%2520context%252C%2520as%2520they%2520mainly%2520focus%2520on%2520local%2520patterns.%2520On%2520the%2520other%2520hand%252C%2520Vision%2520Transformers%2520%2528ViTs%2529%252C%2520inspired%2520by%2520models%2520originally%2520created%2520for%2520language%2520processing%252C%2520use%2520self-attention%2520mechanisms%252C%2520which%2520allow%2520them%2520to%2520understand%2520relationships%2520across%2520the%2520entire%2520image.%2520In%2520this%2520paper%252C%2520we%2520compare%2520different%2520types%2520of%2520ViTs%2520%2528pure%252C%2520hierarchical%252C%2520and%2520hybrid%2529%2520against%2520traditional%2520CNN%2520models%2520across%2520various%2520tasks%252C%2520including%2520object%2520recognition%252C%2520detection%252C%2520and%2520medical%2520image%2520classification.%2520We%2520conduct%2520thorough%2520tests%2520on%2520standard%2520datasets%2520like%2520ImageNet%2520for%2520image%2520classification%2520and%2520COCO%2520for%2520object%2520detection.%2520Additionally%252C%2520we%2520apply%2520these%2520models%2520to%2520medical%2520imaging%2520using%2520the%2520ChestX-ray14%2520dataset.%2520We%2520find%2520that%2520hybrid%2520and%2520hierarchical%2520transformers%252C%2520especially%2520Swin%2520and%2520CvT%252C%2520offer%2520a%2520strong%2520balance%2520between%2520accuracy%2520and%2520computational%2520resources.%2520Furthermore%252C%2520by%2520experimenting%2520with%2520data%2520augmentation%2520techniques%2520on%2520medical%2520images%252C%2520we%2520discover%2520significant%2520performance%2520improvements%252C%2520particularly%2520with%2520the%2520Swin%2520Transformer%2520model.%2520Overall%252C%2520our%2520results%2520indicate%2520that%2520Vision%2520Transformers%2520are%2520competitive%2520and%252C%2520in%2520many%2520cases%252C%2520outperform%2520traditional%2520CNNs%252C%2520especially%2520in%2520scenarios%2520requiring%2520the%2520understanding%2520of%2520global%2520visual%2520contexts%2520like%2520medical%2520imaging.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hands-on%20Evaluation%20of%20Visual%20Transformers%20for%20Object%20Recognition%20and%20Detection&entry.906535625=Dimitrios%20N.%20Vlachogiannis%20and%20Dimitrios%20A.%20Koutsomitropoulos&entry.1292438233=Convolutional%20Neural%20Networks%20%28CNNs%29%20for%20computer%20vision%20sometimes%20struggle%20with%20understanding%20images%20in%20a%20global%20context%2C%20as%20they%20mainly%20focus%20on%20local%20patterns.%20On%20the%20other%20hand%2C%20Vision%20Transformers%20%28ViTs%29%2C%20inspired%20by%20models%20originally%20created%20for%20language%20processing%2C%20use%20self-attention%20mechanisms%2C%20which%20allow%20them%20to%20understand%20relationships%20across%20the%20entire%20image.%20In%20this%20paper%2C%20we%20compare%20different%20types%20of%20ViTs%20%28pure%2C%20hierarchical%2C%20and%20hybrid%29%20against%20traditional%20CNN%20models%20across%20various%20tasks%2C%20including%20object%20recognition%2C%20detection%2C%20and%20medical%20image%20classification.%20We%20conduct%20thorough%20tests%20on%20standard%20datasets%20like%20ImageNet%20for%20image%20classification%20and%20COCO%20for%20object%20detection.%20Additionally%2C%20we%20apply%20these%20models%20to%20medical%20imaging%20using%20the%20ChestX-ray14%20dataset.%20We%20find%20that%20hybrid%20and%20hierarchical%20transformers%2C%20especially%20Swin%20and%20CvT%2C%20offer%20a%20strong%20balance%20between%20accuracy%20and%20computational%20resources.%20Furthermore%2C%20by%20experimenting%20with%20data%20augmentation%20techniques%20on%20medical%20images%2C%20we%20discover%20significant%20performance%20improvements%2C%20particularly%20with%20the%20Swin%20Transformer%20model.%20Overall%2C%20our%20results%20indicate%20that%20Vision%20Transformers%20are%20competitive%20and%2C%20in%20many%20cases%2C%20outperform%20traditional%20CNNs%2C%20especially%20in%20scenarios%20requiring%20the%20understanding%20of%20global%20visual%20contexts%20like%20medical%20imaging.&entry.1838667208=http%3A//arxiv.org/abs/2512.09579v1&entry.124074799=Read"},
{"title": "Adaptive Gradient Calibration for Single-Positive Multi-Label Learning in Remote Sensing Image Scene Classification", "author": "Chenying Liu and Gianmarco Perantoni and Lorenzo Bruzzone and Xiao Xiang Zhu", "abstract": "Multi-label classification (MLC) offers a more comprehensive semantic understanding of Remote Sensing (RS) imagery compared to traditional single-label classification (SLC). However, obtaining complete annotations for MLC is particularly challenging due to the complexity and high cost of the labeling process. As a practical alternative, single-positive multi-label learning (SPML) has emerged, where each image is annotated with only one relevant label, and the model is expected to recover the full set of labels. While scalable, SPML introduces significant supervision ambiguity, demanding specialized solutions for model training. Although various SPML methods have been proposed in the computer vision domain, research in the RS context remains limited. To bridge this gap, we propose Adaptive Gradient Calibration (AdaGC), a novel and generalizable SPML framework tailored to RS imagery. AdaGC adopts a gradient calibration (GC) mechanism with a dual exponential moving average (EMA) module for robust pseudo-label generation. We introduce a theoretically grounded, training-dynamics-based indicator to adaptively trigger GC, which ensures GC's effectiveness by preventing it from being affected by model underfitting or overfitting to label noise. Extensive experiments on two benchmark RS datasets under two distinct label noise types demonstrate that AdaGC achieves state-of-the-art (SOTA) performance while maintaining strong robustness across diverse settings. The codes and data will be released at https://github.com/rslab-unitrento/AdaGC.", "link": "http://arxiv.org/abs/2510.08269v2", "date": "2025-12-10", "relevancy": 2.1115, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5352}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5289}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Gradient%20Calibration%20for%20Single-Positive%20Multi-Label%20Learning%20in%20Remote%20Sensing%20Image%20Scene%20Classification&body=Title%3A%20Adaptive%20Gradient%20Calibration%20for%20Single-Positive%20Multi-Label%20Learning%20in%20Remote%20Sensing%20Image%20Scene%20Classification%0AAuthor%3A%20Chenying%20Liu%20and%20Gianmarco%20Perantoni%20and%20Lorenzo%20Bruzzone%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20Multi-label%20classification%20%28MLC%29%20offers%20a%20more%20comprehensive%20semantic%20understanding%20of%20Remote%20Sensing%20%28RS%29%20imagery%20compared%20to%20traditional%20single-label%20classification%20%28SLC%29.%20However%2C%20obtaining%20complete%20annotations%20for%20MLC%20is%20particularly%20challenging%20due%20to%20the%20complexity%20and%20high%20cost%20of%20the%20labeling%20process.%20As%20a%20practical%20alternative%2C%20single-positive%20multi-label%20learning%20%28SPML%29%20has%20emerged%2C%20where%20each%20image%20is%20annotated%20with%20only%20one%20relevant%20label%2C%20and%20the%20model%20is%20expected%20to%20recover%20the%20full%20set%20of%20labels.%20While%20scalable%2C%20SPML%20introduces%20significant%20supervision%20ambiguity%2C%20demanding%20specialized%20solutions%20for%20model%20training.%20Although%20various%20SPML%20methods%20have%20been%20proposed%20in%20the%20computer%20vision%20domain%2C%20research%20in%20the%20RS%20context%20remains%20limited.%20To%20bridge%20this%20gap%2C%20we%20propose%20Adaptive%20Gradient%20Calibration%20%28AdaGC%29%2C%20a%20novel%20and%20generalizable%20SPML%20framework%20tailored%20to%20RS%20imagery.%20AdaGC%20adopts%20a%20gradient%20calibration%20%28GC%29%20mechanism%20with%20a%20dual%20exponential%20moving%20average%20%28EMA%29%20module%20for%20robust%20pseudo-label%20generation.%20We%20introduce%20a%20theoretically%20grounded%2C%20training-dynamics-based%20indicator%20to%20adaptively%20trigger%20GC%2C%20which%20ensures%20GC%27s%20effectiveness%20by%20preventing%20it%20from%20being%20affected%20by%20model%20underfitting%20or%20overfitting%20to%20label%20noise.%20Extensive%20experiments%20on%20two%20benchmark%20RS%20datasets%20under%20two%20distinct%20label%20noise%20types%20demonstrate%20that%20AdaGC%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20while%20maintaining%20strong%20robustness%20across%20diverse%20settings.%20The%20codes%20and%20data%20will%20be%20released%20at%20https%3A//github.com/rslab-unitrento/AdaGC.%0ALink%3A%20http%3A//arxiv.org/abs/2510.08269v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Gradient%2520Calibration%2520for%2520Single-Positive%2520Multi-Label%2520Learning%2520in%2520Remote%2520Sensing%2520Image%2520Scene%2520Classification%26entry.906535625%3DChenying%2520Liu%2520and%2520Gianmarco%2520Perantoni%2520and%2520Lorenzo%2520Bruzzone%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3DMulti-label%2520classification%2520%2528MLC%2529%2520offers%2520a%2520more%2520comprehensive%2520semantic%2520understanding%2520of%2520Remote%2520Sensing%2520%2528RS%2529%2520imagery%2520compared%2520to%2520traditional%2520single-label%2520classification%2520%2528SLC%2529.%2520However%252C%2520obtaining%2520complete%2520annotations%2520for%2520MLC%2520is%2520particularly%2520challenging%2520due%2520to%2520the%2520complexity%2520and%2520high%2520cost%2520of%2520the%2520labeling%2520process.%2520As%2520a%2520practical%2520alternative%252C%2520single-positive%2520multi-label%2520learning%2520%2528SPML%2529%2520has%2520emerged%252C%2520where%2520each%2520image%2520is%2520annotated%2520with%2520only%2520one%2520relevant%2520label%252C%2520and%2520the%2520model%2520is%2520expected%2520to%2520recover%2520the%2520full%2520set%2520of%2520labels.%2520While%2520scalable%252C%2520SPML%2520introduces%2520significant%2520supervision%2520ambiguity%252C%2520demanding%2520specialized%2520solutions%2520for%2520model%2520training.%2520Although%2520various%2520SPML%2520methods%2520have%2520been%2520proposed%2520in%2520the%2520computer%2520vision%2520domain%252C%2520research%2520in%2520the%2520RS%2520context%2520remains%2520limited.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520Adaptive%2520Gradient%2520Calibration%2520%2528AdaGC%2529%252C%2520a%2520novel%2520and%2520generalizable%2520SPML%2520framework%2520tailored%2520to%2520RS%2520imagery.%2520AdaGC%2520adopts%2520a%2520gradient%2520calibration%2520%2528GC%2529%2520mechanism%2520with%2520a%2520dual%2520exponential%2520moving%2520average%2520%2528EMA%2529%2520module%2520for%2520robust%2520pseudo-label%2520generation.%2520We%2520introduce%2520a%2520theoretically%2520grounded%252C%2520training-dynamics-based%2520indicator%2520to%2520adaptively%2520trigger%2520GC%252C%2520which%2520ensures%2520GC%2527s%2520effectiveness%2520by%2520preventing%2520it%2520from%2520being%2520affected%2520by%2520model%2520underfitting%2520or%2520overfitting%2520to%2520label%2520noise.%2520Extensive%2520experiments%2520on%2520two%2520benchmark%2520RS%2520datasets%2520under%2520two%2520distinct%2520label%2520noise%2520types%2520demonstrate%2520that%2520AdaGC%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520while%2520maintaining%2520strong%2520robustness%2520across%2520diverse%2520settings.%2520The%2520codes%2520and%2520data%2520will%2520be%2520released%2520at%2520https%253A//github.com/rslab-unitrento/AdaGC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08269v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Gradient%20Calibration%20for%20Single-Positive%20Multi-Label%20Learning%20in%20Remote%20Sensing%20Image%20Scene%20Classification&entry.906535625=Chenying%20Liu%20and%20Gianmarco%20Perantoni%20and%20Lorenzo%20Bruzzone%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=Multi-label%20classification%20%28MLC%29%20offers%20a%20more%20comprehensive%20semantic%20understanding%20of%20Remote%20Sensing%20%28RS%29%20imagery%20compared%20to%20traditional%20single-label%20classification%20%28SLC%29.%20However%2C%20obtaining%20complete%20annotations%20for%20MLC%20is%20particularly%20challenging%20due%20to%20the%20complexity%20and%20high%20cost%20of%20the%20labeling%20process.%20As%20a%20practical%20alternative%2C%20single-positive%20multi-label%20learning%20%28SPML%29%20has%20emerged%2C%20where%20each%20image%20is%20annotated%20with%20only%20one%20relevant%20label%2C%20and%20the%20model%20is%20expected%20to%20recover%20the%20full%20set%20of%20labels.%20While%20scalable%2C%20SPML%20introduces%20significant%20supervision%20ambiguity%2C%20demanding%20specialized%20solutions%20for%20model%20training.%20Although%20various%20SPML%20methods%20have%20been%20proposed%20in%20the%20computer%20vision%20domain%2C%20research%20in%20the%20RS%20context%20remains%20limited.%20To%20bridge%20this%20gap%2C%20we%20propose%20Adaptive%20Gradient%20Calibration%20%28AdaGC%29%2C%20a%20novel%20and%20generalizable%20SPML%20framework%20tailored%20to%20RS%20imagery.%20AdaGC%20adopts%20a%20gradient%20calibration%20%28GC%29%20mechanism%20with%20a%20dual%20exponential%20moving%20average%20%28EMA%29%20module%20for%20robust%20pseudo-label%20generation.%20We%20introduce%20a%20theoretically%20grounded%2C%20training-dynamics-based%20indicator%20to%20adaptively%20trigger%20GC%2C%20which%20ensures%20GC%27s%20effectiveness%20by%20preventing%20it%20from%20being%20affected%20by%20model%20underfitting%20or%20overfitting%20to%20label%20noise.%20Extensive%20experiments%20on%20two%20benchmark%20RS%20datasets%20under%20two%20distinct%20label%20noise%20types%20demonstrate%20that%20AdaGC%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20while%20maintaining%20strong%20robustness%20across%20diverse%20settings.%20The%20codes%20and%20data%20will%20be%20released%20at%20https%3A//github.com/rslab-unitrento/AdaGC.&entry.1838667208=http%3A//arxiv.org/abs/2510.08269v2&entry.124074799=Read"},
{"title": "Knowledge Diversion for Efficient Morphology Control and Policy Transfer", "author": "Fu Feng and Ruixiao Shi and Yucheng Xie and Jianlu Shen and Jing Wang and Xin Geng", "abstract": "Universal morphology control aims to learn a universal policy that generalizes across heterogeneous agent morphologies, with Transformer-based controllers emerging as a popular choice. However, such architectures incur substantial computational costs, resulting in high deployment overhead, and existing methods exhibit limited cross-task generalization, necessitating training from scratch for each new task. To this end, we propose \\textbf{DivMorph}, a modular training paradigm that leverages knowledge diversion to learn decomposable controllers. DivMorph factorizes randomly initialized Transformer weights into factor units via SVD prior to training and employs dynamic soft gating to modulate these units based on task and morphology embeddings, separating them into shared \\textit{learngenes} and morphology- and task-specific \\textit{tailors}, thereby achieving knowledge disentanglement. By selectively activating relevant components, DivMorph enables scalable and efficient policy deployment while supporting effective policy transfer to novel tasks. Extensive experiments demonstrate that DivMorph achieves state-of-the-art performance, achieving a 3$\\times$ improvement in sample efficiency over direct finetuning for cross-task transfer and a 17$\\times$ reduction in model size for single-agent deployment.", "link": "http://arxiv.org/abs/2512.09796v1", "date": "2025-12-10", "relevancy": 2.1015, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5337}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5262}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Diversion%20for%20Efficient%20Morphology%20Control%20and%20Policy%20Transfer&body=Title%3A%20Knowledge%20Diversion%20for%20Efficient%20Morphology%20Control%20and%20Policy%20Transfer%0AAuthor%3A%20Fu%20Feng%20and%20Ruixiao%20Shi%20and%20Yucheng%20Xie%20and%20Jianlu%20Shen%20and%20Jing%20Wang%20and%20Xin%20Geng%0AAbstract%3A%20Universal%20morphology%20control%20aims%20to%20learn%20a%20universal%20policy%20that%20generalizes%20across%20heterogeneous%20agent%20morphologies%2C%20with%20Transformer-based%20controllers%20emerging%20as%20a%20popular%20choice.%20However%2C%20such%20architectures%20incur%20substantial%20computational%20costs%2C%20resulting%20in%20high%20deployment%20overhead%2C%20and%20existing%20methods%20exhibit%20limited%20cross-task%20generalization%2C%20necessitating%20training%20from%20scratch%20for%20each%20new%20task.%20To%20this%20end%2C%20we%20propose%20%5Ctextbf%7BDivMorph%7D%2C%20a%20modular%20training%20paradigm%20that%20leverages%20knowledge%20diversion%20to%20learn%20decomposable%20controllers.%20DivMorph%20factorizes%20randomly%20initialized%20Transformer%20weights%20into%20factor%20units%20via%20SVD%20prior%20to%20training%20and%20employs%20dynamic%20soft%20gating%20to%20modulate%20these%20units%20based%20on%20task%20and%20morphology%20embeddings%2C%20separating%20them%20into%20shared%20%5Ctextit%7Blearngenes%7D%20and%20morphology-%20and%20task-specific%20%5Ctextit%7Btailors%7D%2C%20thereby%20achieving%20knowledge%20disentanglement.%20By%20selectively%20activating%20relevant%20components%2C%20DivMorph%20enables%20scalable%20and%20efficient%20policy%20deployment%20while%20supporting%20effective%20policy%20transfer%20to%20novel%20tasks.%20Extensive%20experiments%20demonstrate%20that%20DivMorph%20achieves%20state-of-the-art%20performance%2C%20achieving%20a%203%24%5Ctimes%24%20improvement%20in%20sample%20efficiency%20over%20direct%20finetuning%20for%20cross-task%20transfer%20and%20a%2017%24%5Ctimes%24%20reduction%20in%20model%20size%20for%20single-agent%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Diversion%2520for%2520Efficient%2520Morphology%2520Control%2520and%2520Policy%2520Transfer%26entry.906535625%3DFu%2520Feng%2520and%2520Ruixiao%2520Shi%2520and%2520Yucheng%2520Xie%2520and%2520Jianlu%2520Shen%2520and%2520Jing%2520Wang%2520and%2520Xin%2520Geng%26entry.1292438233%3DUniversal%2520morphology%2520control%2520aims%2520to%2520learn%2520a%2520universal%2520policy%2520that%2520generalizes%2520across%2520heterogeneous%2520agent%2520morphologies%252C%2520with%2520Transformer-based%2520controllers%2520emerging%2520as%2520a%2520popular%2520choice.%2520However%252C%2520such%2520architectures%2520incur%2520substantial%2520computational%2520costs%252C%2520resulting%2520in%2520high%2520deployment%2520overhead%252C%2520and%2520existing%2520methods%2520exhibit%2520limited%2520cross-task%2520generalization%252C%2520necessitating%2520training%2520from%2520scratch%2520for%2520each%2520new%2520task.%2520To%2520this%2520end%252C%2520we%2520propose%2520%255Ctextbf%257BDivMorph%257D%252C%2520a%2520modular%2520training%2520paradigm%2520that%2520leverages%2520knowledge%2520diversion%2520to%2520learn%2520decomposable%2520controllers.%2520DivMorph%2520factorizes%2520randomly%2520initialized%2520Transformer%2520weights%2520into%2520factor%2520units%2520via%2520SVD%2520prior%2520to%2520training%2520and%2520employs%2520dynamic%2520soft%2520gating%2520to%2520modulate%2520these%2520units%2520based%2520on%2520task%2520and%2520morphology%2520embeddings%252C%2520separating%2520them%2520into%2520shared%2520%255Ctextit%257Blearngenes%257D%2520and%2520morphology-%2520and%2520task-specific%2520%255Ctextit%257Btailors%257D%252C%2520thereby%2520achieving%2520knowledge%2520disentanglement.%2520By%2520selectively%2520activating%2520relevant%2520components%252C%2520DivMorph%2520enables%2520scalable%2520and%2520efficient%2520policy%2520deployment%2520while%2520supporting%2520effective%2520policy%2520transfer%2520to%2520novel%2520tasks.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DivMorph%2520achieves%2520state-of-the-art%2520performance%252C%2520achieving%2520a%25203%2524%255Ctimes%2524%2520improvement%2520in%2520sample%2520efficiency%2520over%2520direct%2520finetuning%2520for%2520cross-task%2520transfer%2520and%2520a%252017%2524%255Ctimes%2524%2520reduction%2520in%2520model%2520size%2520for%2520single-agent%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Diversion%20for%20Efficient%20Morphology%20Control%20and%20Policy%20Transfer&entry.906535625=Fu%20Feng%20and%20Ruixiao%20Shi%20and%20Yucheng%20Xie%20and%20Jianlu%20Shen%20and%20Jing%20Wang%20and%20Xin%20Geng&entry.1292438233=Universal%20morphology%20control%20aims%20to%20learn%20a%20universal%20policy%20that%20generalizes%20across%20heterogeneous%20agent%20morphologies%2C%20with%20Transformer-based%20controllers%20emerging%20as%20a%20popular%20choice.%20However%2C%20such%20architectures%20incur%20substantial%20computational%20costs%2C%20resulting%20in%20high%20deployment%20overhead%2C%20and%20existing%20methods%20exhibit%20limited%20cross-task%20generalization%2C%20necessitating%20training%20from%20scratch%20for%20each%20new%20task.%20To%20this%20end%2C%20we%20propose%20%5Ctextbf%7BDivMorph%7D%2C%20a%20modular%20training%20paradigm%20that%20leverages%20knowledge%20diversion%20to%20learn%20decomposable%20controllers.%20DivMorph%20factorizes%20randomly%20initialized%20Transformer%20weights%20into%20factor%20units%20via%20SVD%20prior%20to%20training%20and%20employs%20dynamic%20soft%20gating%20to%20modulate%20these%20units%20based%20on%20task%20and%20morphology%20embeddings%2C%20separating%20them%20into%20shared%20%5Ctextit%7Blearngenes%7D%20and%20morphology-%20and%20task-specific%20%5Ctextit%7Btailors%7D%2C%20thereby%20achieving%20knowledge%20disentanglement.%20By%20selectively%20activating%20relevant%20components%2C%20DivMorph%20enables%20scalable%20and%20efficient%20policy%20deployment%20while%20supporting%20effective%20policy%20transfer%20to%20novel%20tasks.%20Extensive%20experiments%20demonstrate%20that%20DivMorph%20achieves%20state-of-the-art%20performance%2C%20achieving%20a%203%24%5Ctimes%24%20improvement%20in%20sample%20efficiency%20over%20direct%20finetuning%20for%20cross-task%20transfer%20and%20a%2017%24%5Ctimes%24%20reduction%20in%20model%20size%20for%20single-agent%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2512.09796v1&entry.124074799=Read"},
{"title": "Py-DiSMech: A Scalable and Efficient Framework for Discrete Differential Geometry-Based Modeling and Control of Soft Robots", "author": "Radha Lahoti and Ryan Chaiyakul and M. Khalid Jawed", "abstract": "High-fidelity simulation has become essential to the design and control of soft robots, where large geometric deformations and complex contact interactions challenge conventional modeling tools. Recent advances in the field demand simulation frameworks that combine physical accuracy, computational scalability, and seamless integration with modern control and optimization pipelines. In this work, we present Py-DiSMech, a Python-based, open-source simulation framework for modeling and control of soft robotic structures grounded in the principles of Discrete Differential Geometry (DDG). By discretizing geometric quantities such as curvature and strain directly on meshes, Py-DiSMech captures the nonlinear deformation of rods, shells, and hybrid structures with high fidelity and reduced computational cost. The framework introduces (i) a fully vectorized NumPy implementation achieving order-of-magnitude speed-ups over existing geometry-based simulators; (ii) a penalty-energy-based fully implicit contact model that supports rod-rod, rod-shell, and shell-shell interactions; (iii) a natural-strain-based feedback-control module featuring a proportional-integral (PI) controller for shape regulation and trajectory tracking; and (iv) a modular, object-oriented software design enabling user-defined elastic energies, actuation schemes, and integration with machine-learning libraries. Benchmark comparisons demonstrate that Py-DiSMech substantially outperforms the state-of-the-art simulator Elastica in computational efficiency while maintaining physical accuracy. Together, these features establish Py-DiSMech as a scalable, extensible platform for simulation-driven design, control validation, and sim-to-real research in soft robotics.", "link": "http://arxiv.org/abs/2512.09911v1", "date": "2025-12-10", "relevancy": 2.0992, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.53}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5231}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Py-DiSMech%3A%20A%20Scalable%20and%20Efficient%20Framework%20for%20Discrete%20Differential%20Geometry-Based%20Modeling%20and%20Control%20of%20Soft%20Robots&body=Title%3A%20Py-DiSMech%3A%20A%20Scalable%20and%20Efficient%20Framework%20for%20Discrete%20Differential%20Geometry-Based%20Modeling%20and%20Control%20of%20Soft%20Robots%0AAuthor%3A%20Radha%20Lahoti%20and%20Ryan%20Chaiyakul%20and%20M.%20Khalid%20Jawed%0AAbstract%3A%20High-fidelity%20simulation%20has%20become%20essential%20to%20the%20design%20and%20control%20of%20soft%20robots%2C%20where%20large%20geometric%20deformations%20and%20complex%20contact%20interactions%20challenge%20conventional%20modeling%20tools.%20Recent%20advances%20in%20the%20field%20demand%20simulation%20frameworks%20that%20combine%20physical%20accuracy%2C%20computational%20scalability%2C%20and%20seamless%20integration%20with%20modern%20control%20and%20optimization%20pipelines.%20In%20this%20work%2C%20we%20present%20Py-DiSMech%2C%20a%20Python-based%2C%20open-source%20simulation%20framework%20for%20modeling%20and%20control%20of%20soft%20robotic%20structures%20grounded%20in%20the%20principles%20of%20Discrete%20Differential%20Geometry%20%28DDG%29.%20By%20discretizing%20geometric%20quantities%20such%20as%20curvature%20and%20strain%20directly%20on%20meshes%2C%20Py-DiSMech%20captures%20the%20nonlinear%20deformation%20of%20rods%2C%20shells%2C%20and%20hybrid%20structures%20with%20high%20fidelity%20and%20reduced%20computational%20cost.%20The%20framework%20introduces%20%28i%29%20a%20fully%20vectorized%20NumPy%20implementation%20achieving%20order-of-magnitude%20speed-ups%20over%20existing%20geometry-based%20simulators%3B%20%28ii%29%20a%20penalty-energy-based%20fully%20implicit%20contact%20model%20that%20supports%20rod-rod%2C%20rod-shell%2C%20and%20shell-shell%20interactions%3B%20%28iii%29%20a%20natural-strain-based%20feedback-control%20module%20featuring%20a%20proportional-integral%20%28PI%29%20controller%20for%20shape%20regulation%20and%20trajectory%20tracking%3B%20and%20%28iv%29%20a%20modular%2C%20object-oriented%20software%20design%20enabling%20user-defined%20elastic%20energies%2C%20actuation%20schemes%2C%20and%20integration%20with%20machine-learning%20libraries.%20Benchmark%20comparisons%20demonstrate%20that%20Py-DiSMech%20substantially%20outperforms%20the%20state-of-the-art%20simulator%20Elastica%20in%20computational%20efficiency%20while%20maintaining%20physical%20accuracy.%20Together%2C%20these%20features%20establish%20Py-DiSMech%20as%20a%20scalable%2C%20extensible%20platform%20for%20simulation-driven%20design%2C%20control%20validation%2C%20and%20sim-to-real%20research%20in%20soft%20robotics.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09911v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPy-DiSMech%253A%2520A%2520Scalable%2520and%2520Efficient%2520Framework%2520for%2520Discrete%2520Differential%2520Geometry-Based%2520Modeling%2520and%2520Control%2520of%2520Soft%2520Robots%26entry.906535625%3DRadha%2520Lahoti%2520and%2520Ryan%2520Chaiyakul%2520and%2520M.%2520Khalid%2520Jawed%26entry.1292438233%3DHigh-fidelity%2520simulation%2520has%2520become%2520essential%2520to%2520the%2520design%2520and%2520control%2520of%2520soft%2520robots%252C%2520where%2520large%2520geometric%2520deformations%2520and%2520complex%2520contact%2520interactions%2520challenge%2520conventional%2520modeling%2520tools.%2520Recent%2520advances%2520in%2520the%2520field%2520demand%2520simulation%2520frameworks%2520that%2520combine%2520physical%2520accuracy%252C%2520computational%2520scalability%252C%2520and%2520seamless%2520integration%2520with%2520modern%2520control%2520and%2520optimization%2520pipelines.%2520In%2520this%2520work%252C%2520we%2520present%2520Py-DiSMech%252C%2520a%2520Python-based%252C%2520open-source%2520simulation%2520framework%2520for%2520modeling%2520and%2520control%2520of%2520soft%2520robotic%2520structures%2520grounded%2520in%2520the%2520principles%2520of%2520Discrete%2520Differential%2520Geometry%2520%2528DDG%2529.%2520By%2520discretizing%2520geometric%2520quantities%2520such%2520as%2520curvature%2520and%2520strain%2520directly%2520on%2520meshes%252C%2520Py-DiSMech%2520captures%2520the%2520nonlinear%2520deformation%2520of%2520rods%252C%2520shells%252C%2520and%2520hybrid%2520structures%2520with%2520high%2520fidelity%2520and%2520reduced%2520computational%2520cost.%2520The%2520framework%2520introduces%2520%2528i%2529%2520a%2520fully%2520vectorized%2520NumPy%2520implementation%2520achieving%2520order-of-magnitude%2520speed-ups%2520over%2520existing%2520geometry-based%2520simulators%253B%2520%2528ii%2529%2520a%2520penalty-energy-based%2520fully%2520implicit%2520contact%2520model%2520that%2520supports%2520rod-rod%252C%2520rod-shell%252C%2520and%2520shell-shell%2520interactions%253B%2520%2528iii%2529%2520a%2520natural-strain-based%2520feedback-control%2520module%2520featuring%2520a%2520proportional-integral%2520%2528PI%2529%2520controller%2520for%2520shape%2520regulation%2520and%2520trajectory%2520tracking%253B%2520and%2520%2528iv%2529%2520a%2520modular%252C%2520object-oriented%2520software%2520design%2520enabling%2520user-defined%2520elastic%2520energies%252C%2520actuation%2520schemes%252C%2520and%2520integration%2520with%2520machine-learning%2520libraries.%2520Benchmark%2520comparisons%2520demonstrate%2520that%2520Py-DiSMech%2520substantially%2520outperforms%2520the%2520state-of-the-art%2520simulator%2520Elastica%2520in%2520computational%2520efficiency%2520while%2520maintaining%2520physical%2520accuracy.%2520Together%252C%2520these%2520features%2520establish%2520Py-DiSMech%2520as%2520a%2520scalable%252C%2520extensible%2520platform%2520for%2520simulation-driven%2520design%252C%2520control%2520validation%252C%2520and%2520sim-to-real%2520research%2520in%2520soft%2520robotics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09911v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Py-DiSMech%3A%20A%20Scalable%20and%20Efficient%20Framework%20for%20Discrete%20Differential%20Geometry-Based%20Modeling%20and%20Control%20of%20Soft%20Robots&entry.906535625=Radha%20Lahoti%20and%20Ryan%20Chaiyakul%20and%20M.%20Khalid%20Jawed&entry.1292438233=High-fidelity%20simulation%20has%20become%20essential%20to%20the%20design%20and%20control%20of%20soft%20robots%2C%20where%20large%20geometric%20deformations%20and%20complex%20contact%20interactions%20challenge%20conventional%20modeling%20tools.%20Recent%20advances%20in%20the%20field%20demand%20simulation%20frameworks%20that%20combine%20physical%20accuracy%2C%20computational%20scalability%2C%20and%20seamless%20integration%20with%20modern%20control%20and%20optimization%20pipelines.%20In%20this%20work%2C%20we%20present%20Py-DiSMech%2C%20a%20Python-based%2C%20open-source%20simulation%20framework%20for%20modeling%20and%20control%20of%20soft%20robotic%20structures%20grounded%20in%20the%20principles%20of%20Discrete%20Differential%20Geometry%20%28DDG%29.%20By%20discretizing%20geometric%20quantities%20such%20as%20curvature%20and%20strain%20directly%20on%20meshes%2C%20Py-DiSMech%20captures%20the%20nonlinear%20deformation%20of%20rods%2C%20shells%2C%20and%20hybrid%20structures%20with%20high%20fidelity%20and%20reduced%20computational%20cost.%20The%20framework%20introduces%20%28i%29%20a%20fully%20vectorized%20NumPy%20implementation%20achieving%20order-of-magnitude%20speed-ups%20over%20existing%20geometry-based%20simulators%3B%20%28ii%29%20a%20penalty-energy-based%20fully%20implicit%20contact%20model%20that%20supports%20rod-rod%2C%20rod-shell%2C%20and%20shell-shell%20interactions%3B%20%28iii%29%20a%20natural-strain-based%20feedback-control%20module%20featuring%20a%20proportional-integral%20%28PI%29%20controller%20for%20shape%20regulation%20and%20trajectory%20tracking%3B%20and%20%28iv%29%20a%20modular%2C%20object-oriented%20software%20design%20enabling%20user-defined%20elastic%20energies%2C%20actuation%20schemes%2C%20and%20integration%20with%20machine-learning%20libraries.%20Benchmark%20comparisons%20demonstrate%20that%20Py-DiSMech%20substantially%20outperforms%20the%20state-of-the-art%20simulator%20Elastica%20in%20computational%20efficiency%20while%20maintaining%20physical%20accuracy.%20Together%2C%20these%20features%20establish%20Py-DiSMech%20as%20a%20scalable%2C%20extensible%20platform%20for%20simulation-driven%20design%2C%20control%20validation%2C%20and%20sim-to-real%20research%20in%20soft%20robotics.&entry.1838667208=http%3A//arxiv.org/abs/2512.09911v1&entry.124074799=Read"},
{"title": "A Dual-Domain Convolutional Network for Hyperspectral Single-Image Super-Resolution", "author": "Murat Karayaka and Usman Muhammad and Jorma Laaksonen and Md Ziaul Hoque and Tapio Sepp\u00e4nen", "abstract": "This study presents a lightweight dual-domain super-resolution network (DDSRNet) that combines Spatial-Net with the discrete wavelet transform (DWT). Specifically, our proposed model comprises three main components: (1) a shallow feature extraction module, termed Spatial-Net, which performs residual learning and bilinear interpolation; (2) a low-frequency enhancement branch based on the DWT that refines coarse image structures; and (3) a shared high-frequency refinement branch that simultaneously enhances the LH (horizontal), HL (vertical), and HH (diagonal) wavelet subbands using a single CNN with shared weights. As a result, the DWT enables subband decomposition, while the inverse DWT reconstructs the final high-resolution output. By doing so, the integration of spatial- and frequency-domain learning enables DDSRNet to achieve highly competitive performance with low computational cost on three hyperspectral image datasets, demonstrating its effectiveness for hyperspectral image super-resolution.", "link": "http://arxiv.org/abs/2512.09546v1", "date": "2025-12-10", "relevancy": 2.0977, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5651}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5293}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Dual-Domain%20Convolutional%20Network%20for%20Hyperspectral%20Single-Image%20Super-Resolution&body=Title%3A%20A%20Dual-Domain%20Convolutional%20Network%20for%20Hyperspectral%20Single-Image%20Super-Resolution%0AAuthor%3A%20Murat%20Karayaka%20and%20Usman%20Muhammad%20and%20Jorma%20Laaksonen%20and%20Md%20Ziaul%20Hoque%20and%20Tapio%20Sepp%C3%A4nen%0AAbstract%3A%20This%20study%20presents%20a%20lightweight%20dual-domain%20super-resolution%20network%20%28DDSRNet%29%20that%20combines%20Spatial-Net%20with%20the%20discrete%20wavelet%20transform%20%28DWT%29.%20Specifically%2C%20our%20proposed%20model%20comprises%20three%20main%20components%3A%20%281%29%20a%20shallow%20feature%20extraction%20module%2C%20termed%20Spatial-Net%2C%20which%20performs%20residual%20learning%20and%20bilinear%20interpolation%3B%20%282%29%20a%20low-frequency%20enhancement%20branch%20based%20on%20the%20DWT%20that%20refines%20coarse%20image%20structures%3B%20and%20%283%29%20a%20shared%20high-frequency%20refinement%20branch%20that%20simultaneously%20enhances%20the%20LH%20%28horizontal%29%2C%20HL%20%28vertical%29%2C%20and%20HH%20%28diagonal%29%20wavelet%20subbands%20using%20a%20single%20CNN%20with%20shared%20weights.%20As%20a%20result%2C%20the%20DWT%20enables%20subband%20decomposition%2C%20while%20the%20inverse%20DWT%20reconstructs%20the%20final%20high-resolution%20output.%20By%20doing%20so%2C%20the%20integration%20of%20spatial-%20and%20frequency-domain%20learning%20enables%20DDSRNet%20to%20achieve%20highly%20competitive%20performance%20with%20low%20computational%20cost%20on%20three%20hyperspectral%20image%20datasets%2C%20demonstrating%20its%20effectiveness%20for%20hyperspectral%20image%20super-resolution.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Dual-Domain%2520Convolutional%2520Network%2520for%2520Hyperspectral%2520Single-Image%2520Super-Resolution%26entry.906535625%3DMurat%2520Karayaka%2520and%2520Usman%2520Muhammad%2520and%2520Jorma%2520Laaksonen%2520and%2520Md%2520Ziaul%2520Hoque%2520and%2520Tapio%2520Sepp%25C3%25A4nen%26entry.1292438233%3DThis%2520study%2520presents%2520a%2520lightweight%2520dual-domain%2520super-resolution%2520network%2520%2528DDSRNet%2529%2520that%2520combines%2520Spatial-Net%2520with%2520the%2520discrete%2520wavelet%2520transform%2520%2528DWT%2529.%2520Specifically%252C%2520our%2520proposed%2520model%2520comprises%2520three%2520main%2520components%253A%2520%25281%2529%2520a%2520shallow%2520feature%2520extraction%2520module%252C%2520termed%2520Spatial-Net%252C%2520which%2520performs%2520residual%2520learning%2520and%2520bilinear%2520interpolation%253B%2520%25282%2529%2520a%2520low-frequency%2520enhancement%2520branch%2520based%2520on%2520the%2520DWT%2520that%2520refines%2520coarse%2520image%2520structures%253B%2520and%2520%25283%2529%2520a%2520shared%2520high-frequency%2520refinement%2520branch%2520that%2520simultaneously%2520enhances%2520the%2520LH%2520%2528horizontal%2529%252C%2520HL%2520%2528vertical%2529%252C%2520and%2520HH%2520%2528diagonal%2529%2520wavelet%2520subbands%2520using%2520a%2520single%2520CNN%2520with%2520shared%2520weights.%2520As%2520a%2520result%252C%2520the%2520DWT%2520enables%2520subband%2520decomposition%252C%2520while%2520the%2520inverse%2520DWT%2520reconstructs%2520the%2520final%2520high-resolution%2520output.%2520By%2520doing%2520so%252C%2520the%2520integration%2520of%2520spatial-%2520and%2520frequency-domain%2520learning%2520enables%2520DDSRNet%2520to%2520achieve%2520highly%2520competitive%2520performance%2520with%2520low%2520computational%2520cost%2520on%2520three%2520hyperspectral%2520image%2520datasets%252C%2520demonstrating%2520its%2520effectiveness%2520for%2520hyperspectral%2520image%2520super-resolution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Dual-Domain%20Convolutional%20Network%20for%20Hyperspectral%20Single-Image%20Super-Resolution&entry.906535625=Murat%20Karayaka%20and%20Usman%20Muhammad%20and%20Jorma%20Laaksonen%20and%20Md%20Ziaul%20Hoque%20and%20Tapio%20Sepp%C3%A4nen&entry.1292438233=This%20study%20presents%20a%20lightweight%20dual-domain%20super-resolution%20network%20%28DDSRNet%29%20that%20combines%20Spatial-Net%20with%20the%20discrete%20wavelet%20transform%20%28DWT%29.%20Specifically%2C%20our%20proposed%20model%20comprises%20three%20main%20components%3A%20%281%29%20a%20shallow%20feature%20extraction%20module%2C%20termed%20Spatial-Net%2C%20which%20performs%20residual%20learning%20and%20bilinear%20interpolation%3B%20%282%29%20a%20low-frequency%20enhancement%20branch%20based%20on%20the%20DWT%20that%20refines%20coarse%20image%20structures%3B%20and%20%283%29%20a%20shared%20high-frequency%20refinement%20branch%20that%20simultaneously%20enhances%20the%20LH%20%28horizontal%29%2C%20HL%20%28vertical%29%2C%20and%20HH%20%28diagonal%29%20wavelet%20subbands%20using%20a%20single%20CNN%20with%20shared%20weights.%20As%20a%20result%2C%20the%20DWT%20enables%20subband%20decomposition%2C%20while%20the%20inverse%20DWT%20reconstructs%20the%20final%20high-resolution%20output.%20By%20doing%20so%2C%20the%20integration%20of%20spatial-%20and%20frequency-domain%20learning%20enables%20DDSRNet%20to%20achieve%20highly%20competitive%20performance%20with%20low%20computational%20cost%20on%20three%20hyperspectral%20image%20datasets%2C%20demonstrating%20its%20effectiveness%20for%20hyperspectral%20image%20super-resolution.&entry.1838667208=http%3A//arxiv.org/abs/2512.09546v1&entry.124074799=Read"},
{"title": "Incorporating Fairness in Neighborhood Graphs for Fair Spectral Clustering", "author": "Adithya K Moorthy and V Vijaya Saradhi and Bhanu Prasad", "abstract": "Graph clustering plays a pivotal role in unsupervised learning methods like spectral clustering, yet traditional methods for graph clustering often perpetuate bias through unfair graph constructions that may underrepresent some groups. The current research introduces novel approaches for constructing fair k-nearest neighbor (kNN) and fair epsilon-neighborhood graphs that proactively enforce demographic parity during graph formation. By incorporating fairness constraints at the earliest stage of neighborhood selection steps, our approaches incorporate proportional representation of sensitive features into the local graph structure while maintaining geometric consistency.Our work addresses a critical gap in pre-processing for fair spectral clustering, demonstrating that topological fairness in graph construction is essential for achieving equitable clustering outcomes. Widely used graph construction methods like kNN and epsilon-neighborhood graphs propagate edge based disparate impact on sensitive groups, leading to biased clustering results. Providing representation of each sensitive group in the neighborhood of every node leads to fairer spectral clustering results because the topological features of the graph naturally reflect equitable group ratios. This research fills an essential shortcoming in fair unsupervised learning, by illustrating how topological fairness in graph construction inherently facilitates fairer spectral clustering results without the need for changes to the clustering algorithm itself. Thorough experiments on three synthetic datasets, seven real-world tabular datasets, and three real-world image datasets prove that our fair graph construction methods surpass the current baselines in graph clustering tasks.", "link": "http://arxiv.org/abs/2512.09810v1", "date": "2025-12-10", "relevancy": 2.0887, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4294}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4135}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incorporating%20Fairness%20in%20Neighborhood%20Graphs%20for%20Fair%20Spectral%20Clustering&body=Title%3A%20Incorporating%20Fairness%20in%20Neighborhood%20Graphs%20for%20Fair%20Spectral%20Clustering%0AAuthor%3A%20Adithya%20K%20Moorthy%20and%20V%20Vijaya%20Saradhi%20and%20Bhanu%20Prasad%0AAbstract%3A%20Graph%20clustering%20plays%20a%20pivotal%20role%20in%20unsupervised%20learning%20methods%20like%20spectral%20clustering%2C%20yet%20traditional%20methods%20for%20graph%20clustering%20often%20perpetuate%20bias%20through%20unfair%20graph%20constructions%20that%20may%20underrepresent%20some%20groups.%20The%20current%20research%20introduces%20novel%20approaches%20for%20constructing%20fair%20k-nearest%20neighbor%20%28kNN%29%20and%20fair%20epsilon-neighborhood%20graphs%20that%20proactively%20enforce%20demographic%20parity%20during%20graph%20formation.%20By%20incorporating%20fairness%20constraints%20at%20the%20earliest%20stage%20of%20neighborhood%20selection%20steps%2C%20our%20approaches%20incorporate%20proportional%20representation%20of%20sensitive%20features%20into%20the%20local%20graph%20structure%20while%20maintaining%20geometric%20consistency.Our%20work%20addresses%20a%20critical%20gap%20in%20pre-processing%20for%20fair%20spectral%20clustering%2C%20demonstrating%20that%20topological%20fairness%20in%20graph%20construction%20is%20essential%20for%20achieving%20equitable%20clustering%20outcomes.%20Widely%20used%20graph%20construction%20methods%20like%20kNN%20and%20epsilon-neighborhood%20graphs%20propagate%20edge%20based%20disparate%20impact%20on%20sensitive%20groups%2C%20leading%20to%20biased%20clustering%20results.%20Providing%20representation%20of%20each%20sensitive%20group%20in%20the%20neighborhood%20of%20every%20node%20leads%20to%20fairer%20spectral%20clustering%20results%20because%20the%20topological%20features%20of%20the%20graph%20naturally%20reflect%20equitable%20group%20ratios.%20This%20research%20fills%20an%20essential%20shortcoming%20in%20fair%20unsupervised%20learning%2C%20by%20illustrating%20how%20topological%20fairness%20in%20graph%20construction%20inherently%20facilitates%20fairer%20spectral%20clustering%20results%20without%20the%20need%20for%20changes%20to%20the%20clustering%20algorithm%20itself.%20Thorough%20experiments%20on%20three%20synthetic%20datasets%2C%20seven%20real-world%20tabular%20datasets%2C%20and%20three%20real-world%20image%20datasets%20prove%20that%20our%20fair%20graph%20construction%20methods%20surpass%20the%20current%20baselines%20in%20graph%20clustering%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncorporating%2520Fairness%2520in%2520Neighborhood%2520Graphs%2520for%2520Fair%2520Spectral%2520Clustering%26entry.906535625%3DAdithya%2520K%2520Moorthy%2520and%2520V%2520Vijaya%2520Saradhi%2520and%2520Bhanu%2520Prasad%26entry.1292438233%3DGraph%2520clustering%2520plays%2520a%2520pivotal%2520role%2520in%2520unsupervised%2520learning%2520methods%2520like%2520spectral%2520clustering%252C%2520yet%2520traditional%2520methods%2520for%2520graph%2520clustering%2520often%2520perpetuate%2520bias%2520through%2520unfair%2520graph%2520constructions%2520that%2520may%2520underrepresent%2520some%2520groups.%2520The%2520current%2520research%2520introduces%2520novel%2520approaches%2520for%2520constructing%2520fair%2520k-nearest%2520neighbor%2520%2528kNN%2529%2520and%2520fair%2520epsilon-neighborhood%2520graphs%2520that%2520proactively%2520enforce%2520demographic%2520parity%2520during%2520graph%2520formation.%2520By%2520incorporating%2520fairness%2520constraints%2520at%2520the%2520earliest%2520stage%2520of%2520neighborhood%2520selection%2520steps%252C%2520our%2520approaches%2520incorporate%2520proportional%2520representation%2520of%2520sensitive%2520features%2520into%2520the%2520local%2520graph%2520structure%2520while%2520maintaining%2520geometric%2520consistency.Our%2520work%2520addresses%2520a%2520critical%2520gap%2520in%2520pre-processing%2520for%2520fair%2520spectral%2520clustering%252C%2520demonstrating%2520that%2520topological%2520fairness%2520in%2520graph%2520construction%2520is%2520essential%2520for%2520achieving%2520equitable%2520clustering%2520outcomes.%2520Widely%2520used%2520graph%2520construction%2520methods%2520like%2520kNN%2520and%2520epsilon-neighborhood%2520graphs%2520propagate%2520edge%2520based%2520disparate%2520impact%2520on%2520sensitive%2520groups%252C%2520leading%2520to%2520biased%2520clustering%2520results.%2520Providing%2520representation%2520of%2520each%2520sensitive%2520group%2520in%2520the%2520neighborhood%2520of%2520every%2520node%2520leads%2520to%2520fairer%2520spectral%2520clustering%2520results%2520because%2520the%2520topological%2520features%2520of%2520the%2520graph%2520naturally%2520reflect%2520equitable%2520group%2520ratios.%2520This%2520research%2520fills%2520an%2520essential%2520shortcoming%2520in%2520fair%2520unsupervised%2520learning%252C%2520by%2520illustrating%2520how%2520topological%2520fairness%2520in%2520graph%2520construction%2520inherently%2520facilitates%2520fairer%2520spectral%2520clustering%2520results%2520without%2520the%2520need%2520for%2520changes%2520to%2520the%2520clustering%2520algorithm%2520itself.%2520Thorough%2520experiments%2520on%2520three%2520synthetic%2520datasets%252C%2520seven%2520real-world%2520tabular%2520datasets%252C%2520and%2520three%2520real-world%2520image%2520datasets%2520prove%2520that%2520our%2520fair%2520graph%2520construction%2520methods%2520surpass%2520the%2520current%2520baselines%2520in%2520graph%2520clustering%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Fairness%20in%20Neighborhood%20Graphs%20for%20Fair%20Spectral%20Clustering&entry.906535625=Adithya%20K%20Moorthy%20and%20V%20Vijaya%20Saradhi%20and%20Bhanu%20Prasad&entry.1292438233=Graph%20clustering%20plays%20a%20pivotal%20role%20in%20unsupervised%20learning%20methods%20like%20spectral%20clustering%2C%20yet%20traditional%20methods%20for%20graph%20clustering%20often%20perpetuate%20bias%20through%20unfair%20graph%20constructions%20that%20may%20underrepresent%20some%20groups.%20The%20current%20research%20introduces%20novel%20approaches%20for%20constructing%20fair%20k-nearest%20neighbor%20%28kNN%29%20and%20fair%20epsilon-neighborhood%20graphs%20that%20proactively%20enforce%20demographic%20parity%20during%20graph%20formation.%20By%20incorporating%20fairness%20constraints%20at%20the%20earliest%20stage%20of%20neighborhood%20selection%20steps%2C%20our%20approaches%20incorporate%20proportional%20representation%20of%20sensitive%20features%20into%20the%20local%20graph%20structure%20while%20maintaining%20geometric%20consistency.Our%20work%20addresses%20a%20critical%20gap%20in%20pre-processing%20for%20fair%20spectral%20clustering%2C%20demonstrating%20that%20topological%20fairness%20in%20graph%20construction%20is%20essential%20for%20achieving%20equitable%20clustering%20outcomes.%20Widely%20used%20graph%20construction%20methods%20like%20kNN%20and%20epsilon-neighborhood%20graphs%20propagate%20edge%20based%20disparate%20impact%20on%20sensitive%20groups%2C%20leading%20to%20biased%20clustering%20results.%20Providing%20representation%20of%20each%20sensitive%20group%20in%20the%20neighborhood%20of%20every%20node%20leads%20to%20fairer%20spectral%20clustering%20results%20because%20the%20topological%20features%20of%20the%20graph%20naturally%20reflect%20equitable%20group%20ratios.%20This%20research%20fills%20an%20essential%20shortcoming%20in%20fair%20unsupervised%20learning%2C%20by%20illustrating%20how%20topological%20fairness%20in%20graph%20construction%20inherently%20facilitates%20fairer%20spectral%20clustering%20results%20without%20the%20need%20for%20changes%20to%20the%20clustering%20algorithm%20itself.%20Thorough%20experiments%20on%20three%20synthetic%20datasets%2C%20seven%20real-world%20tabular%20datasets%2C%20and%20three%20real-world%20image%20datasets%20prove%20that%20our%20fair%20graph%20construction%20methods%20surpass%20the%20current%20baselines%20in%20graph%20clustering%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.09810v1&entry.124074799=Read"},
{"title": "ModalSurv: Investigating opportunities and limitations of multimodal deep survival learning in prostate and bladder cancer", "author": "Noorul Wahab and Ethar Alzaid and Jiaqi Lv and Fayyaz Minhas and Adam Shephard and Shan E Ahmed Raza", "abstract": "Accurate survival prediction is essential for personalised cancer treatment. We propose ModalSurv, a multimodal deep survival framework integrating clinical, MRI, histopathology, and RNA-sequencing data via modality-specific projections and cross-attention fusion. On the CHIMERA Grand Challenge datasets, ModalSurv achieved a C-index of 0.7402 (1st) for prostate and 0.5740 (5th) for bladder cancer. Notably, clinical features alone outperformed multimodal models on external tests, highlighting challenges of limited multimodal alignment and potential overfitting. Local validation showed multimodal gains but limited generalisation. ModalSurv provides a systematic evaluation of multimodal survival modelling, underscoring both its promise and current limitations for scalable, generalisable cancer prognosis.", "link": "http://arxiv.org/abs/2509.05037v4", "date": "2025-12-10", "relevancy": 2.0806, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5464}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5423}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ModalSurv%3A%20Investigating%20opportunities%20and%20limitations%20of%20multimodal%20deep%20survival%20learning%20in%20prostate%20and%20bladder%20cancer&body=Title%3A%20ModalSurv%3A%20Investigating%20opportunities%20and%20limitations%20of%20multimodal%20deep%20survival%20learning%20in%20prostate%20and%20bladder%20cancer%0AAuthor%3A%20Noorul%20Wahab%20and%20Ethar%20Alzaid%20and%20Jiaqi%20Lv%20and%20Fayyaz%20Minhas%20and%20Adam%20Shephard%20and%20Shan%20E%20Ahmed%20Raza%0AAbstract%3A%20Accurate%20survival%20prediction%20is%20essential%20for%20personalised%20cancer%20treatment.%20We%20propose%20ModalSurv%2C%20a%20multimodal%20deep%20survival%20framework%20integrating%20clinical%2C%20MRI%2C%20histopathology%2C%20and%20RNA-sequencing%20data%20via%20modality-specific%20projections%20and%20cross-attention%20fusion.%20On%20the%20CHIMERA%20Grand%20Challenge%20datasets%2C%20ModalSurv%20achieved%20a%20C-index%20of%200.7402%20%281st%29%20for%20prostate%20and%200.5740%20%285th%29%20for%20bladder%20cancer.%20Notably%2C%20clinical%20features%20alone%20outperformed%20multimodal%20models%20on%20external%20tests%2C%20highlighting%20challenges%20of%20limited%20multimodal%20alignment%20and%20potential%20overfitting.%20Local%20validation%20showed%20multimodal%20gains%20but%20limited%20generalisation.%20ModalSurv%20provides%20a%20systematic%20evaluation%20of%20multimodal%20survival%20modelling%2C%20underscoring%20both%20its%20promise%20and%20current%20limitations%20for%20scalable%2C%20generalisable%20cancer%20prognosis.%0ALink%3A%20http%3A//arxiv.org/abs/2509.05037v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModalSurv%253A%2520Investigating%2520opportunities%2520and%2520limitations%2520of%2520multimodal%2520deep%2520survival%2520learning%2520in%2520prostate%2520and%2520bladder%2520cancer%26entry.906535625%3DNoorul%2520Wahab%2520and%2520Ethar%2520Alzaid%2520and%2520Jiaqi%2520Lv%2520and%2520Fayyaz%2520Minhas%2520and%2520Adam%2520Shephard%2520and%2520Shan%2520E%2520Ahmed%2520Raza%26entry.1292438233%3DAccurate%2520survival%2520prediction%2520is%2520essential%2520for%2520personalised%2520cancer%2520treatment.%2520We%2520propose%2520ModalSurv%252C%2520a%2520multimodal%2520deep%2520survival%2520framework%2520integrating%2520clinical%252C%2520MRI%252C%2520histopathology%252C%2520and%2520RNA-sequencing%2520data%2520via%2520modality-specific%2520projections%2520and%2520cross-attention%2520fusion.%2520On%2520the%2520CHIMERA%2520Grand%2520Challenge%2520datasets%252C%2520ModalSurv%2520achieved%2520a%2520C-index%2520of%25200.7402%2520%25281st%2529%2520for%2520prostate%2520and%25200.5740%2520%25285th%2529%2520for%2520bladder%2520cancer.%2520Notably%252C%2520clinical%2520features%2520alone%2520outperformed%2520multimodal%2520models%2520on%2520external%2520tests%252C%2520highlighting%2520challenges%2520of%2520limited%2520multimodal%2520alignment%2520and%2520potential%2520overfitting.%2520Local%2520validation%2520showed%2520multimodal%2520gains%2520but%2520limited%2520generalisation.%2520ModalSurv%2520provides%2520a%2520systematic%2520evaluation%2520of%2520multimodal%2520survival%2520modelling%252C%2520underscoring%2520both%2520its%2520promise%2520and%2520current%2520limitations%2520for%2520scalable%252C%2520generalisable%2520cancer%2520prognosis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05037v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ModalSurv%3A%20Investigating%20opportunities%20and%20limitations%20of%20multimodal%20deep%20survival%20learning%20in%20prostate%20and%20bladder%20cancer&entry.906535625=Noorul%20Wahab%20and%20Ethar%20Alzaid%20and%20Jiaqi%20Lv%20and%20Fayyaz%20Minhas%20and%20Adam%20Shephard%20and%20Shan%20E%20Ahmed%20Raza&entry.1292438233=Accurate%20survival%20prediction%20is%20essential%20for%20personalised%20cancer%20treatment.%20We%20propose%20ModalSurv%2C%20a%20multimodal%20deep%20survival%20framework%20integrating%20clinical%2C%20MRI%2C%20histopathology%2C%20and%20RNA-sequencing%20data%20via%20modality-specific%20projections%20and%20cross-attention%20fusion.%20On%20the%20CHIMERA%20Grand%20Challenge%20datasets%2C%20ModalSurv%20achieved%20a%20C-index%20of%200.7402%20%281st%29%20for%20prostate%20and%200.5740%20%285th%29%20for%20bladder%20cancer.%20Notably%2C%20clinical%20features%20alone%20outperformed%20multimodal%20models%20on%20external%20tests%2C%20highlighting%20challenges%20of%20limited%20multimodal%20alignment%20and%20potential%20overfitting.%20Local%20validation%20showed%20multimodal%20gains%20but%20limited%20generalisation.%20ModalSurv%20provides%20a%20systematic%20evaluation%20of%20multimodal%20survival%20modelling%2C%20underscoring%20both%20its%20promise%20and%20current%20limitations%20for%20scalable%2C%20generalisable%20cancer%20prognosis.&entry.1838667208=http%3A//arxiv.org/abs/2509.05037v4&entry.124074799=Read"},
{"title": "Transformers for Tabular Data: A Training Perspective of Self-Attention via Optimal Transport", "author": "Antonio Candelieri and Alessandro Quadrio", "abstract": "This thesis examines self-attention training through the lens of Optimal Transport (OT) and develops an OT-based alternative for tabular classification. The study tracks intermediate projections of the self-attention layer during training and evaluates their evolution using discrete OT metrics, including Wasserstein distance, Monge gap, optimality, and efficiency. Experiments are conducted on classification tasks with two and three classes, as well as on a biomedical dataset.\n  Results indicate that the final self-attention mapping often approximates the OT optimal coupling, yet the training trajectory remains inefficient. Pretraining the MLP section on synthetic data partially improves convergence but is sensitive to their initialization. To address these limitations, an OT-based algorithm is introduced: it generates class-specific dummy Gaussian distributions, computes an OT alignment with the data, and trains an MLP to generalize this mapping. The method achieves accuracy comparable to Transformers while reducing computational cost and scaling more efficiently under standardized inputs, though its performance depends on careful dummy-geometry design. All experiments and implementations are conducted in R.", "link": "http://arxiv.org/abs/2512.09530v1", "date": "2025-12-10", "relevancy": 2.0788, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5288}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5263}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformers%20for%20Tabular%20Data%3A%20A%20Training%20Perspective%20of%20Self-Attention%20via%20Optimal%20Transport&body=Title%3A%20Transformers%20for%20Tabular%20Data%3A%20A%20Training%20Perspective%20of%20Self-Attention%20via%20Optimal%20Transport%0AAuthor%3A%20Antonio%20Candelieri%20and%20Alessandro%20Quadrio%0AAbstract%3A%20This%20thesis%20examines%20self-attention%20training%20through%20the%20lens%20of%20Optimal%20Transport%20%28OT%29%20and%20develops%20an%20OT-based%20alternative%20for%20tabular%20classification.%20The%20study%20tracks%20intermediate%20projections%20of%20the%20self-attention%20layer%20during%20training%20and%20evaluates%20their%20evolution%20using%20discrete%20OT%20metrics%2C%20including%20Wasserstein%20distance%2C%20Monge%20gap%2C%20optimality%2C%20and%20efficiency.%20Experiments%20are%20conducted%20on%20classification%20tasks%20with%20two%20and%20three%20classes%2C%20as%20well%20as%20on%20a%20biomedical%20dataset.%0A%20%20Results%20indicate%20that%20the%20final%20self-attention%20mapping%20often%20approximates%20the%20OT%20optimal%20coupling%2C%20yet%20the%20training%20trajectory%20remains%20inefficient.%20Pretraining%20the%20MLP%20section%20on%20synthetic%20data%20partially%20improves%20convergence%20but%20is%20sensitive%20to%20their%20initialization.%20To%20address%20these%20limitations%2C%20an%20OT-based%20algorithm%20is%20introduced%3A%20it%20generates%20class-specific%20dummy%20Gaussian%20distributions%2C%20computes%20an%20OT%20alignment%20with%20the%20data%2C%20and%20trains%20an%20MLP%20to%20generalize%20this%20mapping.%20The%20method%20achieves%20accuracy%20comparable%20to%20Transformers%20while%20reducing%20computational%20cost%20and%20scaling%20more%20efficiently%20under%20standardized%20inputs%2C%20though%20its%20performance%20depends%20on%20careful%20dummy-geometry%20design.%20All%20experiments%20and%20implementations%20are%20conducted%20in%20R.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformers%2520for%2520Tabular%2520Data%253A%2520A%2520Training%2520Perspective%2520of%2520Self-Attention%2520via%2520Optimal%2520Transport%26entry.906535625%3DAntonio%2520Candelieri%2520and%2520Alessandro%2520Quadrio%26entry.1292438233%3DThis%2520thesis%2520examines%2520self-attention%2520training%2520through%2520the%2520lens%2520of%2520Optimal%2520Transport%2520%2528OT%2529%2520and%2520develops%2520an%2520OT-based%2520alternative%2520for%2520tabular%2520classification.%2520The%2520study%2520tracks%2520intermediate%2520projections%2520of%2520the%2520self-attention%2520layer%2520during%2520training%2520and%2520evaluates%2520their%2520evolution%2520using%2520discrete%2520OT%2520metrics%252C%2520including%2520Wasserstein%2520distance%252C%2520Monge%2520gap%252C%2520optimality%252C%2520and%2520efficiency.%2520Experiments%2520are%2520conducted%2520on%2520classification%2520tasks%2520with%2520two%2520and%2520three%2520classes%252C%2520as%2520well%2520as%2520on%2520a%2520biomedical%2520dataset.%250A%2520%2520Results%2520indicate%2520that%2520the%2520final%2520self-attention%2520mapping%2520often%2520approximates%2520the%2520OT%2520optimal%2520coupling%252C%2520yet%2520the%2520training%2520trajectory%2520remains%2520inefficient.%2520Pretraining%2520the%2520MLP%2520section%2520on%2520synthetic%2520data%2520partially%2520improves%2520convergence%2520but%2520is%2520sensitive%2520to%2520their%2520initialization.%2520To%2520address%2520these%2520limitations%252C%2520an%2520OT-based%2520algorithm%2520is%2520introduced%253A%2520it%2520generates%2520class-specific%2520dummy%2520Gaussian%2520distributions%252C%2520computes%2520an%2520OT%2520alignment%2520with%2520the%2520data%252C%2520and%2520trains%2520an%2520MLP%2520to%2520generalize%2520this%2520mapping.%2520The%2520method%2520achieves%2520accuracy%2520comparable%2520to%2520Transformers%2520while%2520reducing%2520computational%2520cost%2520and%2520scaling%2520more%2520efficiently%2520under%2520standardized%2520inputs%252C%2520though%2520its%2520performance%2520depends%2520on%2520careful%2520dummy-geometry%2520design.%2520All%2520experiments%2520and%2520implementations%2520are%2520conducted%2520in%2520R.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20for%20Tabular%20Data%3A%20A%20Training%20Perspective%20of%20Self-Attention%20via%20Optimal%20Transport&entry.906535625=Antonio%20Candelieri%20and%20Alessandro%20Quadrio&entry.1292438233=This%20thesis%20examines%20self-attention%20training%20through%20the%20lens%20of%20Optimal%20Transport%20%28OT%29%20and%20develops%20an%20OT-based%20alternative%20for%20tabular%20classification.%20The%20study%20tracks%20intermediate%20projections%20of%20the%20self-attention%20layer%20during%20training%20and%20evaluates%20their%20evolution%20using%20discrete%20OT%20metrics%2C%20including%20Wasserstein%20distance%2C%20Monge%20gap%2C%20optimality%2C%20and%20efficiency.%20Experiments%20are%20conducted%20on%20classification%20tasks%20with%20two%20and%20three%20classes%2C%20as%20well%20as%20on%20a%20biomedical%20dataset.%0A%20%20Results%20indicate%20that%20the%20final%20self-attention%20mapping%20often%20approximates%20the%20OT%20optimal%20coupling%2C%20yet%20the%20training%20trajectory%20remains%20inefficient.%20Pretraining%20the%20MLP%20section%20on%20synthetic%20data%20partially%20improves%20convergence%20but%20is%20sensitive%20to%20their%20initialization.%20To%20address%20these%20limitations%2C%20an%20OT-based%20algorithm%20is%20introduced%3A%20it%20generates%20class-specific%20dummy%20Gaussian%20distributions%2C%20computes%20an%20OT%20alignment%20with%20the%20data%2C%20and%20trains%20an%20MLP%20to%20generalize%20this%20mapping.%20The%20method%20achieves%20accuracy%20comparable%20to%20Transformers%20while%20reducing%20computational%20cost%20and%20scaling%20more%20efficiently%20under%20standardized%20inputs%2C%20though%20its%20performance%20depends%20on%20careful%20dummy-geometry%20design.%20All%20experiments%20and%20implementations%20are%20conducted%20in%20R.&entry.1838667208=http%3A//arxiv.org/abs/2512.09530v1&entry.124074799=Read"},
{"title": "Imitative Membership Inference Attack", "author": "Yuntao Du and Yuetian Chen and Hanshen Xiao and Bruno Ribeiro and Ninghui Li", "abstract": "A Membership Inference Attack (MIA) assesses how much a target machine learning model reveals about its training data by determining whether specific query instances were part of the training set. State-of-the-art MIAs rely on training hundreds of shadow models that are independent of the target model, leading to significant computational overhead. In this paper, we introduce Imitative Membership Inference Attack (IMIA), which employs a novel imitative training technique to strategically construct a small number of target-informed imitative models that closely replicate the target model's behavior for inference. Extensive experimental results demonstrate that IMIA substantially outperforms existing MIAs in various attack settings while only requiring less than 5% of the computational cost of state-of-the-art approaches.", "link": "http://arxiv.org/abs/2509.06796v2", "date": "2025-12-10", "relevancy": 2.0451, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4378}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4072}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitative%20Membership%20Inference%20Attack&body=Title%3A%20Imitative%20Membership%20Inference%20Attack%0AAuthor%3A%20Yuntao%20Du%20and%20Yuetian%20Chen%20and%20Hanshen%20Xiao%20and%20Bruno%20Ribeiro%20and%20Ninghui%20Li%0AAbstract%3A%20A%20Membership%20Inference%20Attack%20%28MIA%29%20assesses%20how%20much%20a%20target%20machine%20learning%20model%20reveals%20about%20its%20training%20data%20by%20determining%20whether%20specific%20query%20instances%20were%20part%20of%20the%20training%20set.%20State-of-the-art%20MIAs%20rely%20on%20training%20hundreds%20of%20shadow%20models%20that%20are%20independent%20of%20the%20target%20model%2C%20leading%20to%20significant%20computational%20overhead.%20In%20this%20paper%2C%20we%20introduce%20Imitative%20Membership%20Inference%20Attack%20%28IMIA%29%2C%20which%20employs%20a%20novel%20imitative%20training%20technique%20to%20strategically%20construct%20a%20small%20number%20of%20target-informed%20imitative%20models%20that%20closely%20replicate%20the%20target%20model%27s%20behavior%20for%20inference.%20Extensive%20experimental%20results%20demonstrate%20that%20IMIA%20substantially%20outperforms%20existing%20MIAs%20in%20various%20attack%20settings%20while%20only%20requiring%20less%20than%205%25%20of%20the%20computational%20cost%20of%20state-of-the-art%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2509.06796v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitative%2520Membership%2520Inference%2520Attack%26entry.906535625%3DYuntao%2520Du%2520and%2520Yuetian%2520Chen%2520and%2520Hanshen%2520Xiao%2520and%2520Bruno%2520Ribeiro%2520and%2520Ninghui%2520Li%26entry.1292438233%3DA%2520Membership%2520Inference%2520Attack%2520%2528MIA%2529%2520assesses%2520how%2520much%2520a%2520target%2520machine%2520learning%2520model%2520reveals%2520about%2520its%2520training%2520data%2520by%2520determining%2520whether%2520specific%2520query%2520instances%2520were%2520part%2520of%2520the%2520training%2520set.%2520State-of-the-art%2520MIAs%2520rely%2520on%2520training%2520hundreds%2520of%2520shadow%2520models%2520that%2520are%2520independent%2520of%2520the%2520target%2520model%252C%2520leading%2520to%2520significant%2520computational%2520overhead.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Imitative%2520Membership%2520Inference%2520Attack%2520%2528IMIA%2529%252C%2520which%2520employs%2520a%2520novel%2520imitative%2520training%2520technique%2520to%2520strategically%2520construct%2520a%2520small%2520number%2520of%2520target-informed%2520imitative%2520models%2520that%2520closely%2520replicate%2520the%2520target%2520model%2527s%2520behavior%2520for%2520inference.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520IMIA%2520substantially%2520outperforms%2520existing%2520MIAs%2520in%2520various%2520attack%2520settings%2520while%2520only%2520requiring%2520less%2520than%25205%2525%2520of%2520the%2520computational%2520cost%2520of%2520state-of-the-art%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06796v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitative%20Membership%20Inference%20Attack&entry.906535625=Yuntao%20Du%20and%20Yuetian%20Chen%20and%20Hanshen%20Xiao%20and%20Bruno%20Ribeiro%20and%20Ninghui%20Li&entry.1292438233=A%20Membership%20Inference%20Attack%20%28MIA%29%20assesses%20how%20much%20a%20target%20machine%20learning%20model%20reveals%20about%20its%20training%20data%20by%20determining%20whether%20specific%20query%20instances%20were%20part%20of%20the%20training%20set.%20State-of-the-art%20MIAs%20rely%20on%20training%20hundreds%20of%20shadow%20models%20that%20are%20independent%20of%20the%20target%20model%2C%20leading%20to%20significant%20computational%20overhead.%20In%20this%20paper%2C%20we%20introduce%20Imitative%20Membership%20Inference%20Attack%20%28IMIA%29%2C%20which%20employs%20a%20novel%20imitative%20training%20technique%20to%20strategically%20construct%20a%20small%20number%20of%20target-informed%20imitative%20models%20that%20closely%20replicate%20the%20target%20model%27s%20behavior%20for%20inference.%20Extensive%20experimental%20results%20demonstrate%20that%20IMIA%20substantially%20outperforms%20existing%20MIAs%20in%20various%20attack%20settings%20while%20only%20requiring%20less%20than%205%25%20of%20the%20computational%20cost%20of%20state-of-the-art%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2509.06796v2&entry.124074799=Read"},
{"title": "A Unified Formal Theory on the Logical Limits of Symbol Grounding", "author": "Zhangchi Liu", "abstract": "This paper synthesizes a series of formal proofs to construct a unified theory on the logical limits of the Symbol Grounding Problem. We distinguish between internal meaning (sense), which formal systems can possess via axioms, and external grounding (reference), which is a necessary condition for connecting symbols to the world. We demonstrate through a four-stage argument that meaningful grounding within a formal system must arise from a process that is external, dynamic, and non-fixed algorithmic. First, we show that for a purely symbolic system, the impossibility of grounding is a direct consequence of its definition. Second, we extend this limitation to systems with any finite, static set of pre-established meanings (Semantic Axioms). By formally modeling the computationalist hypothesis-which equates grounding with internal derivation-we prove via G\u00f6delian arguments that such systems cannot consistently and completely define a \"groundability predicate\" for all truths. Third, we demonstrate that the \"grounding act\" for emergent meanings cannot be inferred from internal rules but requires an axiomatic, meta-level update. Drawing on Turing's concept of Oracle Machines and Piccinini's analysis of the mathematical objection, we identify this update as physical transduction. Finally, we prove that this process cannot be simulated by a fixed judgment algorithm, validating the logical necessity of embodied interaction.", "link": "http://arxiv.org/abs/2509.20409v4", "date": "2025-12-10", "relevancy": 2.0374, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4092}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Formal%20Theory%20on%20the%20Logical%20Limits%20of%20Symbol%20Grounding&body=Title%3A%20A%20Unified%20Formal%20Theory%20on%20the%20Logical%20Limits%20of%20Symbol%20Grounding%0AAuthor%3A%20Zhangchi%20Liu%0AAbstract%3A%20This%20paper%20synthesizes%20a%20series%20of%20formal%20proofs%20to%20construct%20a%20unified%20theory%20on%20the%20logical%20limits%20of%20the%20Symbol%20Grounding%20Problem.%20We%20distinguish%20between%20internal%20meaning%20%28sense%29%2C%20which%20formal%20systems%20can%20possess%20via%20axioms%2C%20and%20external%20grounding%20%28reference%29%2C%20which%20is%20a%20necessary%20condition%20for%20connecting%20symbols%20to%20the%20world.%20We%20demonstrate%20through%20a%20four-stage%20argument%20that%20meaningful%20grounding%20within%20a%20formal%20system%20must%20arise%20from%20a%20process%20that%20is%20external%2C%20dynamic%2C%20and%20non-fixed%20algorithmic.%20First%2C%20we%20show%20that%20for%20a%20purely%20symbolic%20system%2C%20the%20impossibility%20of%20grounding%20is%20a%20direct%20consequence%20of%20its%20definition.%20Second%2C%20we%20extend%20this%20limitation%20to%20systems%20with%20any%20finite%2C%20static%20set%20of%20pre-established%20meanings%20%28Semantic%20Axioms%29.%20By%20formally%20modeling%20the%20computationalist%20hypothesis-which%20equates%20grounding%20with%20internal%20derivation-we%20prove%20via%20G%C3%B6delian%20arguments%20that%20such%20systems%20cannot%20consistently%20and%20completely%20define%20a%20%22groundability%20predicate%22%20for%20all%20truths.%20Third%2C%20we%20demonstrate%20that%20the%20%22grounding%20act%22%20for%20emergent%20meanings%20cannot%20be%20inferred%20from%20internal%20rules%20but%20requires%20an%20axiomatic%2C%20meta-level%20update.%20Drawing%20on%20Turing%27s%20concept%20of%20Oracle%20Machines%20and%20Piccinini%27s%20analysis%20of%20the%20mathematical%20objection%2C%20we%20identify%20this%20update%20as%20physical%20transduction.%20Finally%2C%20we%20prove%20that%20this%20process%20cannot%20be%20simulated%20by%20a%20fixed%20judgment%20algorithm%2C%20validating%20the%20logical%20necessity%20of%20embodied%20interaction.%0ALink%3A%20http%3A//arxiv.org/abs/2509.20409v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Formal%2520Theory%2520on%2520the%2520Logical%2520Limits%2520of%2520Symbol%2520Grounding%26entry.906535625%3DZhangchi%2520Liu%26entry.1292438233%3DThis%2520paper%2520synthesizes%2520a%2520series%2520of%2520formal%2520proofs%2520to%2520construct%2520a%2520unified%2520theory%2520on%2520the%2520logical%2520limits%2520of%2520the%2520Symbol%2520Grounding%2520Problem.%2520We%2520distinguish%2520between%2520internal%2520meaning%2520%2528sense%2529%252C%2520which%2520formal%2520systems%2520can%2520possess%2520via%2520axioms%252C%2520and%2520external%2520grounding%2520%2528reference%2529%252C%2520which%2520is%2520a%2520necessary%2520condition%2520for%2520connecting%2520symbols%2520to%2520the%2520world.%2520We%2520demonstrate%2520through%2520a%2520four-stage%2520argument%2520that%2520meaningful%2520grounding%2520within%2520a%2520formal%2520system%2520must%2520arise%2520from%2520a%2520process%2520that%2520is%2520external%252C%2520dynamic%252C%2520and%2520non-fixed%2520algorithmic.%2520First%252C%2520we%2520show%2520that%2520for%2520a%2520purely%2520symbolic%2520system%252C%2520the%2520impossibility%2520of%2520grounding%2520is%2520a%2520direct%2520consequence%2520of%2520its%2520definition.%2520Second%252C%2520we%2520extend%2520this%2520limitation%2520to%2520systems%2520with%2520any%2520finite%252C%2520static%2520set%2520of%2520pre-established%2520meanings%2520%2528Semantic%2520Axioms%2529.%2520By%2520formally%2520modeling%2520the%2520computationalist%2520hypothesis-which%2520equates%2520grounding%2520with%2520internal%2520derivation-we%2520prove%2520via%2520G%25C3%25B6delian%2520arguments%2520that%2520such%2520systems%2520cannot%2520consistently%2520and%2520completely%2520define%2520a%2520%2522groundability%2520predicate%2522%2520for%2520all%2520truths.%2520Third%252C%2520we%2520demonstrate%2520that%2520the%2520%2522grounding%2520act%2522%2520for%2520emergent%2520meanings%2520cannot%2520be%2520inferred%2520from%2520internal%2520rules%2520but%2520requires%2520an%2520axiomatic%252C%2520meta-level%2520update.%2520Drawing%2520on%2520Turing%2527s%2520concept%2520of%2520Oracle%2520Machines%2520and%2520Piccinini%2527s%2520analysis%2520of%2520the%2520mathematical%2520objection%252C%2520we%2520identify%2520this%2520update%2520as%2520physical%2520transduction.%2520Finally%252C%2520we%2520prove%2520that%2520this%2520process%2520cannot%2520be%2520simulated%2520by%2520a%2520fixed%2520judgment%2520algorithm%252C%2520validating%2520the%2520logical%2520necessity%2520of%2520embodied%2520interaction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20409v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Formal%20Theory%20on%20the%20Logical%20Limits%20of%20Symbol%20Grounding&entry.906535625=Zhangchi%20Liu&entry.1292438233=This%20paper%20synthesizes%20a%20series%20of%20formal%20proofs%20to%20construct%20a%20unified%20theory%20on%20the%20logical%20limits%20of%20the%20Symbol%20Grounding%20Problem.%20We%20distinguish%20between%20internal%20meaning%20%28sense%29%2C%20which%20formal%20systems%20can%20possess%20via%20axioms%2C%20and%20external%20grounding%20%28reference%29%2C%20which%20is%20a%20necessary%20condition%20for%20connecting%20symbols%20to%20the%20world.%20We%20demonstrate%20through%20a%20four-stage%20argument%20that%20meaningful%20grounding%20within%20a%20formal%20system%20must%20arise%20from%20a%20process%20that%20is%20external%2C%20dynamic%2C%20and%20non-fixed%20algorithmic.%20First%2C%20we%20show%20that%20for%20a%20purely%20symbolic%20system%2C%20the%20impossibility%20of%20grounding%20is%20a%20direct%20consequence%20of%20its%20definition.%20Second%2C%20we%20extend%20this%20limitation%20to%20systems%20with%20any%20finite%2C%20static%20set%20of%20pre-established%20meanings%20%28Semantic%20Axioms%29.%20By%20formally%20modeling%20the%20computationalist%20hypothesis-which%20equates%20grounding%20with%20internal%20derivation-we%20prove%20via%20G%C3%B6delian%20arguments%20that%20such%20systems%20cannot%20consistently%20and%20completely%20define%20a%20%22groundability%20predicate%22%20for%20all%20truths.%20Third%2C%20we%20demonstrate%20that%20the%20%22grounding%20act%22%20for%20emergent%20meanings%20cannot%20be%20inferred%20from%20internal%20rules%20but%20requires%20an%20axiomatic%2C%20meta-level%20update.%20Drawing%20on%20Turing%27s%20concept%20of%20Oracle%20Machines%20and%20Piccinini%27s%20analysis%20of%20the%20mathematical%20objection%2C%20we%20identify%20this%20update%20as%20physical%20transduction.%20Finally%2C%20we%20prove%20that%20this%20process%20cannot%20be%20simulated%20by%20a%20fixed%20judgment%20algorithm%2C%20validating%20the%20logical%20necessity%20of%20embodied%20interaction.&entry.1838667208=http%3A//arxiv.org/abs/2509.20409v4&entry.124074799=Read"},
{"title": "From Graphs to Gates: DNS-HyXNet, A Lightweight and Deployable Sequential Model for Real-Time DNS Tunnel Detection", "author": "Faraz Ali and Muhammad Afaq and Mahmood Niazi and Muzammil Behzad", "abstract": "Domain Name System (DNS) tunneling remains a covert channel for data exfiltration and command-and-control communication. Although graph-based methods such as GraphTunnel achieve strong accuracy, they introduce significant latency and computational overhead due to recursive parsing and graph construction, limiting their suitability for real-time deployment. This work presents DNS-HyXNet, a lightweight extended Long Short-Term Memory (xLSTM) hybrid framework designed for efficient sequence-based DNS tunnel detection. DNS-HyXNet integrates tokenized domain embeddings with normalized numerical DNS features and processes them through a two-layer xLSTM network that directly learns temporal dependencies from packet sequences, eliminating the need for graph reconstruction and enabling single-stage multi-class classification. The model was trained and evaluated on two public benchmark datasets with carefully tuned hyperparameters to ensure low memory consumption and fast inference. Across all experimental splits of the DNS-Tunnel-Datasets, DNS-HyXNet achieved up to 99.99% accuracy, with macro-averaged precision, recall, and F1-scores exceeding 99.96%, and demonstrated a per-sample detection latency of just 0.041 ms, confirming its scalability and real-time readiness. These results show that sequential modeling with xLSTM can effectively replace computationally expensive recursive graph generation, offering a deployable and energy-efficient alternative for real-time DNS tunnel detection on commodity hardware.", "link": "http://arxiv.org/abs/2512.09565v1", "date": "2025-12-10", "relevancy": 1.4533, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5132}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4828}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Graphs%20to%20Gates%3A%20DNS-HyXNet%2C%20A%20Lightweight%20and%20Deployable%20Sequential%20Model%20for%20Real-Time%20DNS%20Tunnel%20Detection&body=Title%3A%20From%20Graphs%20to%20Gates%3A%20DNS-HyXNet%2C%20A%20Lightweight%20and%20Deployable%20Sequential%20Model%20for%20Real-Time%20DNS%20Tunnel%20Detection%0AAuthor%3A%20Faraz%20Ali%20and%20Muhammad%20Afaq%20and%20Mahmood%20Niazi%20and%20Muzammil%20Behzad%0AAbstract%3A%20Domain%20Name%20System%20%28DNS%29%20tunneling%20remains%20a%20covert%20channel%20for%20data%20exfiltration%20and%20command-and-control%20communication.%20Although%20graph-based%20methods%20such%20as%20GraphTunnel%20achieve%20strong%20accuracy%2C%20they%20introduce%20significant%20latency%20and%20computational%20overhead%20due%20to%20recursive%20parsing%20and%20graph%20construction%2C%20limiting%20their%20suitability%20for%20real-time%20deployment.%20This%20work%20presents%20DNS-HyXNet%2C%20a%20lightweight%20extended%20Long%20Short-Term%20Memory%20%28xLSTM%29%20hybrid%20framework%20designed%20for%20efficient%20sequence-based%20DNS%20tunnel%20detection.%20DNS-HyXNet%20integrates%20tokenized%20domain%20embeddings%20with%20normalized%20numerical%20DNS%20features%20and%20processes%20them%20through%20a%20two-layer%20xLSTM%20network%20that%20directly%20learns%20temporal%20dependencies%20from%20packet%20sequences%2C%20eliminating%20the%20need%20for%20graph%20reconstruction%20and%20enabling%20single-stage%20multi-class%20classification.%20The%20model%20was%20trained%20and%20evaluated%20on%20two%20public%20benchmark%20datasets%20with%20carefully%20tuned%20hyperparameters%20to%20ensure%20low%20memory%20consumption%20and%20fast%20inference.%20Across%20all%20experimental%20splits%20of%20the%20DNS-Tunnel-Datasets%2C%20DNS-HyXNet%20achieved%20up%20to%2099.99%25%20accuracy%2C%20with%20macro-averaged%20precision%2C%20recall%2C%20and%20F1-scores%20exceeding%2099.96%25%2C%20and%20demonstrated%20a%20per-sample%20detection%20latency%20of%20just%200.041%20ms%2C%20confirming%20its%20scalability%20and%20real-time%20readiness.%20These%20results%20show%20that%20sequential%20modeling%20with%20xLSTM%20can%20effectively%20replace%20computationally%20expensive%20recursive%20graph%20generation%2C%20offering%20a%20deployable%20and%20energy-efficient%20alternative%20for%20real-time%20DNS%20tunnel%20detection%20on%20commodity%20hardware.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Graphs%2520to%2520Gates%253A%2520DNS-HyXNet%252C%2520A%2520Lightweight%2520and%2520Deployable%2520Sequential%2520Model%2520for%2520Real-Time%2520DNS%2520Tunnel%2520Detection%26entry.906535625%3DFaraz%2520Ali%2520and%2520Muhammad%2520Afaq%2520and%2520Mahmood%2520Niazi%2520and%2520Muzammil%2520Behzad%26entry.1292438233%3DDomain%2520Name%2520System%2520%2528DNS%2529%2520tunneling%2520remains%2520a%2520covert%2520channel%2520for%2520data%2520exfiltration%2520and%2520command-and-control%2520communication.%2520Although%2520graph-based%2520methods%2520such%2520as%2520GraphTunnel%2520achieve%2520strong%2520accuracy%252C%2520they%2520introduce%2520significant%2520latency%2520and%2520computational%2520overhead%2520due%2520to%2520recursive%2520parsing%2520and%2520graph%2520construction%252C%2520limiting%2520their%2520suitability%2520for%2520real-time%2520deployment.%2520This%2520work%2520presents%2520DNS-HyXNet%252C%2520a%2520lightweight%2520extended%2520Long%2520Short-Term%2520Memory%2520%2528xLSTM%2529%2520hybrid%2520framework%2520designed%2520for%2520efficient%2520sequence-based%2520DNS%2520tunnel%2520detection.%2520DNS-HyXNet%2520integrates%2520tokenized%2520domain%2520embeddings%2520with%2520normalized%2520numerical%2520DNS%2520features%2520and%2520processes%2520them%2520through%2520a%2520two-layer%2520xLSTM%2520network%2520that%2520directly%2520learns%2520temporal%2520dependencies%2520from%2520packet%2520sequences%252C%2520eliminating%2520the%2520need%2520for%2520graph%2520reconstruction%2520and%2520enabling%2520single-stage%2520multi-class%2520classification.%2520The%2520model%2520was%2520trained%2520and%2520evaluated%2520on%2520two%2520public%2520benchmark%2520datasets%2520with%2520carefully%2520tuned%2520hyperparameters%2520to%2520ensure%2520low%2520memory%2520consumption%2520and%2520fast%2520inference.%2520Across%2520all%2520experimental%2520splits%2520of%2520the%2520DNS-Tunnel-Datasets%252C%2520DNS-HyXNet%2520achieved%2520up%2520to%252099.99%2525%2520accuracy%252C%2520with%2520macro-averaged%2520precision%252C%2520recall%252C%2520and%2520F1-scores%2520exceeding%252099.96%2525%252C%2520and%2520demonstrated%2520a%2520per-sample%2520detection%2520latency%2520of%2520just%25200.041%2520ms%252C%2520confirming%2520its%2520scalability%2520and%2520real-time%2520readiness.%2520These%2520results%2520show%2520that%2520sequential%2520modeling%2520with%2520xLSTM%2520can%2520effectively%2520replace%2520computationally%2520expensive%2520recursive%2520graph%2520generation%252C%2520offering%2520a%2520deployable%2520and%2520energy-efficient%2520alternative%2520for%2520real-time%2520DNS%2520tunnel%2520detection%2520on%2520commodity%2520hardware.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Graphs%20to%20Gates%3A%20DNS-HyXNet%2C%20A%20Lightweight%20and%20Deployable%20Sequential%20Model%20for%20Real-Time%20DNS%20Tunnel%20Detection&entry.906535625=Faraz%20Ali%20and%20Muhammad%20Afaq%20and%20Mahmood%20Niazi%20and%20Muzammil%20Behzad&entry.1292438233=Domain%20Name%20System%20%28DNS%29%20tunneling%20remains%20a%20covert%20channel%20for%20data%20exfiltration%20and%20command-and-control%20communication.%20Although%20graph-based%20methods%20such%20as%20GraphTunnel%20achieve%20strong%20accuracy%2C%20they%20introduce%20significant%20latency%20and%20computational%20overhead%20due%20to%20recursive%20parsing%20and%20graph%20construction%2C%20limiting%20their%20suitability%20for%20real-time%20deployment.%20This%20work%20presents%20DNS-HyXNet%2C%20a%20lightweight%20extended%20Long%20Short-Term%20Memory%20%28xLSTM%29%20hybrid%20framework%20designed%20for%20efficient%20sequence-based%20DNS%20tunnel%20detection.%20DNS-HyXNet%20integrates%20tokenized%20domain%20embeddings%20with%20normalized%20numerical%20DNS%20features%20and%20processes%20them%20through%20a%20two-layer%20xLSTM%20network%20that%20directly%20learns%20temporal%20dependencies%20from%20packet%20sequences%2C%20eliminating%20the%20need%20for%20graph%20reconstruction%20and%20enabling%20single-stage%20multi-class%20classification.%20The%20model%20was%20trained%20and%20evaluated%20on%20two%20public%20benchmark%20datasets%20with%20carefully%20tuned%20hyperparameters%20to%20ensure%20low%20memory%20consumption%20and%20fast%20inference.%20Across%20all%20experimental%20splits%20of%20the%20DNS-Tunnel-Datasets%2C%20DNS-HyXNet%20achieved%20up%20to%2099.99%25%20accuracy%2C%20with%20macro-averaged%20precision%2C%20recall%2C%20and%20F1-scores%20exceeding%2099.96%25%2C%20and%20demonstrated%20a%20per-sample%20detection%20latency%20of%20just%200.041%20ms%2C%20confirming%20its%20scalability%20and%20real-time%20readiness.%20These%20results%20show%20that%20sequential%20modeling%20with%20xLSTM%20can%20effectively%20replace%20computationally%20expensive%20recursive%20graph%20generation%2C%20offering%20a%20deployable%20and%20energy-efficient%20alternative%20for%20real-time%20DNS%20tunnel%20detection%20on%20commodity%20hardware.&entry.1838667208=http%3A//arxiv.org/abs/2512.09565v1&entry.124074799=Read"},
{"title": "Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces", "author": "Nikita Gabdullin", "abstract": "The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.", "link": "http://arxiv.org/abs/2512.07509v2", "date": "2025-12-10", "relevancy": 2.0094, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5033}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5021}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20possible%20vector%20systems%20for%20faster%20training%20of%20neural%20networks%20with%20preconfigured%20latent%20spaces&body=Title%3A%20Exploring%20possible%20vector%20systems%20for%20faster%20training%20of%20neural%20networks%20with%20preconfigured%20latent%20spaces%0AAuthor%3A%20Nikita%20Gabdullin%0AAbstract%3A%20The%20overall%20neural%20network%20%28NN%29%20performance%20is%20closely%20related%20to%20the%20properties%20of%20its%20embedding%20distribution%20in%20latent%20space%20%28LS%29.%20It%20has%20recently%20been%20shown%20that%20predefined%20vector%20systems%2C%20specifically%20An%20root%20system%20vectors%2C%20can%20be%20used%20as%20targets%20for%20latent%20space%20configurations%20%28LSC%29%20to%20ensure%20the%20desired%20LS%20structure.%20One%20of%20the%20main%20LSC%20advantage%20is%20the%20possibility%20of%20training%20classifier%20NNs%20without%20classification%20layers%2C%20which%20facilitates%20training%20NNs%20on%20datasets%20with%20extremely%20large%20numbers%20of%20classes.%20This%20paper%20provides%20a%20more%20general%20overview%20of%20possible%20vector%20systems%20for%20NN%20training%20along%20with%20their%20properties%20and%20methods%20for%20vector%20system%20construction.%20These%20systems%20are%20used%20to%20configure%20LS%20of%20encoders%20and%20visual%20transformers%20to%20significantly%20speed%20up%20ImageNet-1K%20and%2050k-600k%20classes%20LSC%20training.%20It%20is%20also%20shown%20that%20using%20the%20minimum%20number%20of%20LS%20dimensions%20for%20a%20specific%20number%20of%20classes%20results%20in%20faster%20convergence.%20The%20latter%20has%20potential%20advantages%20for%20reducing%20the%20size%20of%20vector%20databases%20used%20to%20store%20NN%20embeddings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07509v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520possible%2520vector%2520systems%2520for%2520faster%2520training%2520of%2520neural%2520networks%2520with%2520preconfigured%2520latent%2520spaces%26entry.906535625%3DNikita%2520Gabdullin%26entry.1292438233%3DThe%2520overall%2520neural%2520network%2520%2528NN%2529%2520performance%2520is%2520closely%2520related%2520to%2520the%2520properties%2520of%2520its%2520embedding%2520distribution%2520in%2520latent%2520space%2520%2528LS%2529.%2520It%2520has%2520recently%2520been%2520shown%2520that%2520predefined%2520vector%2520systems%252C%2520specifically%2520An%2520root%2520system%2520vectors%252C%2520can%2520be%2520used%2520as%2520targets%2520for%2520latent%2520space%2520configurations%2520%2528LSC%2529%2520to%2520ensure%2520the%2520desired%2520LS%2520structure.%2520One%2520of%2520the%2520main%2520LSC%2520advantage%2520is%2520the%2520possibility%2520of%2520training%2520classifier%2520NNs%2520without%2520classification%2520layers%252C%2520which%2520facilitates%2520training%2520NNs%2520on%2520datasets%2520with%2520extremely%2520large%2520numbers%2520of%2520classes.%2520This%2520paper%2520provides%2520a%2520more%2520general%2520overview%2520of%2520possible%2520vector%2520systems%2520for%2520NN%2520training%2520along%2520with%2520their%2520properties%2520and%2520methods%2520for%2520vector%2520system%2520construction.%2520These%2520systems%2520are%2520used%2520to%2520configure%2520LS%2520of%2520encoders%2520and%2520visual%2520transformers%2520to%2520significantly%2520speed%2520up%2520ImageNet-1K%2520and%252050k-600k%2520classes%2520LSC%2520training.%2520It%2520is%2520also%2520shown%2520that%2520using%2520the%2520minimum%2520number%2520of%2520LS%2520dimensions%2520for%2520a%2520specific%2520number%2520of%2520classes%2520results%2520in%2520faster%2520convergence.%2520The%2520latter%2520has%2520potential%2520advantages%2520for%2520reducing%2520the%2520size%2520of%2520vector%2520databases%2520used%2520to%2520store%2520NN%2520embeddings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07509v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20possible%20vector%20systems%20for%20faster%20training%20of%20neural%20networks%20with%20preconfigured%20latent%20spaces&entry.906535625=Nikita%20Gabdullin&entry.1292438233=The%20overall%20neural%20network%20%28NN%29%20performance%20is%20closely%20related%20to%20the%20properties%20of%20its%20embedding%20distribution%20in%20latent%20space%20%28LS%29.%20It%20has%20recently%20been%20shown%20that%20predefined%20vector%20systems%2C%20specifically%20An%20root%20system%20vectors%2C%20can%20be%20used%20as%20targets%20for%20latent%20space%20configurations%20%28LSC%29%20to%20ensure%20the%20desired%20LS%20structure.%20One%20of%20the%20main%20LSC%20advantage%20is%20the%20possibility%20of%20training%20classifier%20NNs%20without%20classification%20layers%2C%20which%20facilitates%20training%20NNs%20on%20datasets%20with%20extremely%20large%20numbers%20of%20classes.%20This%20paper%20provides%20a%20more%20general%20overview%20of%20possible%20vector%20systems%20for%20NN%20training%20along%20with%20their%20properties%20and%20methods%20for%20vector%20system%20construction.%20These%20systems%20are%20used%20to%20configure%20LS%20of%20encoders%20and%20visual%20transformers%20to%20significantly%20speed%20up%20ImageNet-1K%20and%2050k-600k%20classes%20LSC%20training.%20It%20is%20also%20shown%20that%20using%20the%20minimum%20number%20of%20LS%20dimensions%20for%20a%20specific%20number%20of%20classes%20results%20in%20faster%20convergence.%20The%20latter%20has%20potential%20advantages%20for%20reducing%20the%20size%20of%20vector%20databases%20used%20to%20store%20NN%20embeddings.&entry.1838667208=http%3A//arxiv.org/abs/2512.07509v2&entry.124074799=Read"},
{"title": "Bridging the Basilisk Astrodynamics Framework with ROS 2 for Modular Spacecraft Simulation and Hardware Integration", "author": "Elias Krantz and Ngai Nam Chan and Gunnar Tibert and Huina Mao and Christer Fuglesang", "abstract": "Integrating high-fidelity spacecraft simulators with modular robotics frameworks remains a challenge for autonomy development. This paper presents a lightweight, open-source communication bridge between the Basilisk astrodynamics simulator and the Robot Operating System 2 (ROS 2), enabling real-time, bidirectional data exchange for spacecraft control. The bridge requires no changes to Basilisk's core and integrates seamlessly with ROS 2 nodes. We demonstrate its use in a leader-follower formation flying scenario using nonlinear model predictive control, deployed identically in both simulation and on the ATMOS planar microgravity testbed. This setup supports rapid development, hardware-in-the-loop testing, and seamless transition from simulation to hardware. The bridge offers a flexible and scalable platform for modular spacecraft autonomy and reproducible research workflows.", "link": "http://arxiv.org/abs/2512.09833v1", "date": "2025-12-10", "relevancy": 1.421, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5124}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4628}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Basilisk%20Astrodynamics%20Framework%20with%20ROS%202%20for%20Modular%20Spacecraft%20Simulation%20and%20Hardware%20Integration&body=Title%3A%20Bridging%20the%20Basilisk%20Astrodynamics%20Framework%20with%20ROS%202%20for%20Modular%20Spacecraft%20Simulation%20and%20Hardware%20Integration%0AAuthor%3A%20Elias%20Krantz%20and%20Ngai%20Nam%20Chan%20and%20Gunnar%20Tibert%20and%20Huina%20Mao%20and%20Christer%20Fuglesang%0AAbstract%3A%20Integrating%20high-fidelity%20spacecraft%20simulators%20with%20modular%20robotics%20frameworks%20remains%20a%20challenge%20for%20autonomy%20development.%20This%20paper%20presents%20a%20lightweight%2C%20open-source%20communication%20bridge%20between%20the%20Basilisk%20astrodynamics%20simulator%20and%20the%20Robot%20Operating%20System%202%20%28ROS%202%29%2C%20enabling%20real-time%2C%20bidirectional%20data%20exchange%20for%20spacecraft%20control.%20The%20bridge%20requires%20no%20changes%20to%20Basilisk%27s%20core%20and%20integrates%20seamlessly%20with%20ROS%202%20nodes.%20We%20demonstrate%20its%20use%20in%20a%20leader-follower%20formation%20flying%20scenario%20using%20nonlinear%20model%20predictive%20control%2C%20deployed%20identically%20in%20both%20simulation%20and%20on%20the%20ATMOS%20planar%20microgravity%20testbed.%20This%20setup%20supports%20rapid%20development%2C%20hardware-in-the-loop%20testing%2C%20and%20seamless%20transition%20from%20simulation%20to%20hardware.%20The%20bridge%20offers%20a%20flexible%20and%20scalable%20platform%20for%20modular%20spacecraft%20autonomy%20and%20reproducible%20research%20workflows.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520the%2520Basilisk%2520Astrodynamics%2520Framework%2520with%2520ROS%25202%2520for%2520Modular%2520Spacecraft%2520Simulation%2520and%2520Hardware%2520Integration%26entry.906535625%3DElias%2520Krantz%2520and%2520Ngai%2520Nam%2520Chan%2520and%2520Gunnar%2520Tibert%2520and%2520Huina%2520Mao%2520and%2520Christer%2520Fuglesang%26entry.1292438233%3DIntegrating%2520high-fidelity%2520spacecraft%2520simulators%2520with%2520modular%2520robotics%2520frameworks%2520remains%2520a%2520challenge%2520for%2520autonomy%2520development.%2520This%2520paper%2520presents%2520a%2520lightweight%252C%2520open-source%2520communication%2520bridge%2520between%2520the%2520Basilisk%2520astrodynamics%2520simulator%2520and%2520the%2520Robot%2520Operating%2520System%25202%2520%2528ROS%25202%2529%252C%2520enabling%2520real-time%252C%2520bidirectional%2520data%2520exchange%2520for%2520spacecraft%2520control.%2520The%2520bridge%2520requires%2520no%2520changes%2520to%2520Basilisk%2527s%2520core%2520and%2520integrates%2520seamlessly%2520with%2520ROS%25202%2520nodes.%2520We%2520demonstrate%2520its%2520use%2520in%2520a%2520leader-follower%2520formation%2520flying%2520scenario%2520using%2520nonlinear%2520model%2520predictive%2520control%252C%2520deployed%2520identically%2520in%2520both%2520simulation%2520and%2520on%2520the%2520ATMOS%2520planar%2520microgravity%2520testbed.%2520This%2520setup%2520supports%2520rapid%2520development%252C%2520hardware-in-the-loop%2520testing%252C%2520and%2520seamless%2520transition%2520from%2520simulation%2520to%2520hardware.%2520The%2520bridge%2520offers%2520a%2520flexible%2520and%2520scalable%2520platform%2520for%2520modular%2520spacecraft%2520autonomy%2520and%2520reproducible%2520research%2520workflows.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Basilisk%20Astrodynamics%20Framework%20with%20ROS%202%20for%20Modular%20Spacecraft%20Simulation%20and%20Hardware%20Integration&entry.906535625=Elias%20Krantz%20and%20Ngai%20Nam%20Chan%20and%20Gunnar%20Tibert%20and%20Huina%20Mao%20and%20Christer%20Fuglesang&entry.1292438233=Integrating%20high-fidelity%20spacecraft%20simulators%20with%20modular%20robotics%20frameworks%20remains%20a%20challenge%20for%20autonomy%20development.%20This%20paper%20presents%20a%20lightweight%2C%20open-source%20communication%20bridge%20between%20the%20Basilisk%20astrodynamics%20simulator%20and%20the%20Robot%20Operating%20System%202%20%28ROS%202%29%2C%20enabling%20real-time%2C%20bidirectional%20data%20exchange%20for%20spacecraft%20control.%20The%20bridge%20requires%20no%20changes%20to%20Basilisk%27s%20core%20and%20integrates%20seamlessly%20with%20ROS%202%20nodes.%20We%20demonstrate%20its%20use%20in%20a%20leader-follower%20formation%20flying%20scenario%20using%20nonlinear%20model%20predictive%20control%2C%20deployed%20identically%20in%20both%20simulation%20and%20on%20the%20ATMOS%20planar%20microgravity%20testbed.%20This%20setup%20supports%20rapid%20development%2C%20hardware-in-the-loop%20testing%2C%20and%20seamless%20transition%20from%20simulation%20to%20hardware.%20The%20bridge%20offers%20a%20flexible%20and%20scalable%20platform%20for%20modular%20spacecraft%20autonomy%20and%20reproducible%20research%20workflows.&entry.1838667208=http%3A//arxiv.org/abs/2512.09833v1&entry.124074799=Read"},
{"title": "Attention Sinks in Diffusion Language Models", "author": "Maximo Eduardo Rulli and Simone Petruzzi and Edoardo Michielon and Fabrizio Silvestri and Simone Scardapane and Alessio Devoto", "abstract": "Masked Diffusion Language Models (DLMs) have recently emerged as a promising alternative to traditional Autoregressive Models (ARMs). DLMs employ transformer encoders with bidirectional attention, enabling parallel token generation while maintaining competitive performance. Although their efficiency and effectiveness have been extensively studied, the internal mechanisms that govern DLMs remain largely unexplored. In this work, we conduct an empirical analysis of DLM attention patterns, focusing on the attention sinking phenomenon, an effect previously observed in various transformer-based architectures. Our findings reveal that DLMs also exhibit attention sinks, but with distinct characteristics. First, unlike in ARMs, the sink positions in DLMs tend to shift throughout the generation process, displaying a dynamic behaviour. Second, while ARMs are highly sensitive to the removal of attention sinks, DLMs remain robust: masking sinks leads to only a minor degradation in performance. These results provide new insights into the inner workings of diffusion-based language models and highlight fundamental differences in how they allocate and utilize attention compared to autoregressive models.", "link": "http://arxiv.org/abs/2510.15731v2", "date": "2025-12-10", "relevancy": 1.7012, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5739}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5696}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Sinks%20in%20Diffusion%20Language%20Models&body=Title%3A%20Attention%20Sinks%20in%20Diffusion%20Language%20Models%0AAuthor%3A%20Maximo%20Eduardo%20Rulli%20and%20Simone%20Petruzzi%20and%20Edoardo%20Michielon%20and%20Fabrizio%20Silvestri%20and%20Simone%20Scardapane%20and%20Alessio%20Devoto%0AAbstract%3A%20Masked%20Diffusion%20Language%20Models%20%28DLMs%29%20have%20recently%20emerged%20as%20a%20promising%20alternative%20to%20traditional%20Autoregressive%20Models%20%28ARMs%29.%20DLMs%20employ%20transformer%20encoders%20with%20bidirectional%20attention%2C%20enabling%20parallel%20token%20generation%20while%20maintaining%20competitive%20performance.%20Although%20their%20efficiency%20and%20effectiveness%20have%20been%20extensively%20studied%2C%20the%20internal%20mechanisms%20that%20govern%20DLMs%20remain%20largely%20unexplored.%20In%20this%20work%2C%20we%20conduct%20an%20empirical%20analysis%20of%20DLM%20attention%20patterns%2C%20focusing%20on%20the%20attention%20sinking%20phenomenon%2C%20an%20effect%20previously%20observed%20in%20various%20transformer-based%20architectures.%20Our%20findings%20reveal%20that%20DLMs%20also%20exhibit%20attention%20sinks%2C%20but%20with%20distinct%20characteristics.%20First%2C%20unlike%20in%20ARMs%2C%20the%20sink%20positions%20in%20DLMs%20tend%20to%20shift%20throughout%20the%20generation%20process%2C%20displaying%20a%20dynamic%20behaviour.%20Second%2C%20while%20ARMs%20are%20highly%20sensitive%20to%20the%20removal%20of%20attention%20sinks%2C%20DLMs%20remain%20robust%3A%20masking%20sinks%20leads%20to%20only%20a%20minor%20degradation%20in%20performance.%20These%20results%20provide%20new%20insights%20into%20the%20inner%20workings%20of%20diffusion-based%20language%20models%20and%20highlight%20fundamental%20differences%20in%20how%20they%20allocate%20and%20utilize%20attention%20compared%20to%20autoregressive%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2510.15731v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Sinks%2520in%2520Diffusion%2520Language%2520Models%26entry.906535625%3DMaximo%2520Eduardo%2520Rulli%2520and%2520Simone%2520Petruzzi%2520and%2520Edoardo%2520Michielon%2520and%2520Fabrizio%2520Silvestri%2520and%2520Simone%2520Scardapane%2520and%2520Alessio%2520Devoto%26entry.1292438233%3DMasked%2520Diffusion%2520Language%2520Models%2520%2528DLMs%2529%2520have%2520recently%2520emerged%2520as%2520a%2520promising%2520alternative%2520to%2520traditional%2520Autoregressive%2520Models%2520%2528ARMs%2529.%2520DLMs%2520employ%2520transformer%2520encoders%2520with%2520bidirectional%2520attention%252C%2520enabling%2520parallel%2520token%2520generation%2520while%2520maintaining%2520competitive%2520performance.%2520Although%2520their%2520efficiency%2520and%2520effectiveness%2520have%2520been%2520extensively%2520studied%252C%2520the%2520internal%2520mechanisms%2520that%2520govern%2520DLMs%2520remain%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520conduct%2520an%2520empirical%2520analysis%2520of%2520DLM%2520attention%2520patterns%252C%2520focusing%2520on%2520the%2520attention%2520sinking%2520phenomenon%252C%2520an%2520effect%2520previously%2520observed%2520in%2520various%2520transformer-based%2520architectures.%2520Our%2520findings%2520reveal%2520that%2520DLMs%2520also%2520exhibit%2520attention%2520sinks%252C%2520but%2520with%2520distinct%2520characteristics.%2520First%252C%2520unlike%2520in%2520ARMs%252C%2520the%2520sink%2520positions%2520in%2520DLMs%2520tend%2520to%2520shift%2520throughout%2520the%2520generation%2520process%252C%2520displaying%2520a%2520dynamic%2520behaviour.%2520Second%252C%2520while%2520ARMs%2520are%2520highly%2520sensitive%2520to%2520the%2520removal%2520of%2520attention%2520sinks%252C%2520DLMs%2520remain%2520robust%253A%2520masking%2520sinks%2520leads%2520to%2520only%2520a%2520minor%2520degradation%2520in%2520performance.%2520These%2520results%2520provide%2520new%2520insights%2520into%2520the%2520inner%2520workings%2520of%2520diffusion-based%2520language%2520models%2520and%2520highlight%2520fundamental%2520differences%2520in%2520how%2520they%2520allocate%2520and%2520utilize%2520attention%2520compared%2520to%2520autoregressive%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15731v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Sinks%20in%20Diffusion%20Language%20Models&entry.906535625=Maximo%20Eduardo%20Rulli%20and%20Simone%20Petruzzi%20and%20Edoardo%20Michielon%20and%20Fabrizio%20Silvestri%20and%20Simone%20Scardapane%20and%20Alessio%20Devoto&entry.1292438233=Masked%20Diffusion%20Language%20Models%20%28DLMs%29%20have%20recently%20emerged%20as%20a%20promising%20alternative%20to%20traditional%20Autoregressive%20Models%20%28ARMs%29.%20DLMs%20employ%20transformer%20encoders%20with%20bidirectional%20attention%2C%20enabling%20parallel%20token%20generation%20while%20maintaining%20competitive%20performance.%20Although%20their%20efficiency%20and%20effectiveness%20have%20been%20extensively%20studied%2C%20the%20internal%20mechanisms%20that%20govern%20DLMs%20remain%20largely%20unexplored.%20In%20this%20work%2C%20we%20conduct%20an%20empirical%20analysis%20of%20DLM%20attention%20patterns%2C%20focusing%20on%20the%20attention%20sinking%20phenomenon%2C%20an%20effect%20previously%20observed%20in%20various%20transformer-based%20architectures.%20Our%20findings%20reveal%20that%20DLMs%20also%20exhibit%20attention%20sinks%2C%20but%20with%20distinct%20characteristics.%20First%2C%20unlike%20in%20ARMs%2C%20the%20sink%20positions%20in%20DLMs%20tend%20to%20shift%20throughout%20the%20generation%20process%2C%20displaying%20a%20dynamic%20behaviour.%20Second%2C%20while%20ARMs%20are%20highly%20sensitive%20to%20the%20removal%20of%20attention%20sinks%2C%20DLMs%20remain%20robust%3A%20masking%20sinks%20leads%20to%20only%20a%20minor%20degradation%20in%20performance.%20These%20results%20provide%20new%20insights%20into%20the%20inner%20workings%20of%20diffusion-based%20language%20models%20and%20highlight%20fundamental%20differences%20in%20how%20they%20allocate%20and%20utilize%20attention%20compared%20to%20autoregressive%20models.&entry.1838667208=http%3A//arxiv.org/abs/2510.15731v2&entry.124074799=Read"},
{"title": "Conformal Bandits: Bringing statistical validity and reward efficiency to the small-gap regime", "author": "Simone Cuonzo and Nina Deliu", "abstract": "We introduce Conformal Bandits, a novel framework integrating Conformal Prediction (CP) into bandit problems, a classic paradigm for sequential decision-making under uncertainty. Traditional regret-minimisation bandit strategies like Thompson Sampling and Upper Confidence Bound (UCB) typically rely on distributional assumptions or asymptotic guarantees; further, they remain largely focused on regret, neglecting their statistical properties. We address this gap. Through the adoption of CP, we bridge the regret-minimising potential of a decision-making bandit policy with statistical guarantees in the form of finite-time prediction coverage.\n  We demonstrate the potential of it Conformal Bandits through simulation studies and an application to portfolio allocation, a typical small-gap regime, where differences in arm rewards are far too small for classical policies to achieve optimal regret bounds in finite sample. Motivated by this, we showcase our framework's practical advantage in terms of regret in small-gap settings, as well as its added value in achieving nominal coverage guarantees where classical UCB policies fail. Focusing on our application of interest, we further illustrate how integrating hidden Markov models to capture the regime-switching behaviour of financial markets, enhances the exploration-exploitation trade-off, and translates into higher risk-adjusted regret efficiency returns, while preserving coverage guarantees.", "link": "http://arxiv.org/abs/2512.09850v1", "date": "2025-12-10", "relevancy": 1.8706, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4955}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4628}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Bandits%3A%20Bringing%20statistical%20validity%20and%20reward%20efficiency%20to%20the%20small-gap%20regime&body=Title%3A%20Conformal%20Bandits%3A%20Bringing%20statistical%20validity%20and%20reward%20efficiency%20to%20the%20small-gap%20regime%0AAuthor%3A%20Simone%20Cuonzo%20and%20Nina%20Deliu%0AAbstract%3A%20We%20introduce%20Conformal%20Bandits%2C%20a%20novel%20framework%20integrating%20Conformal%20Prediction%20%28CP%29%20into%20bandit%20problems%2C%20a%20classic%20paradigm%20for%20sequential%20decision-making%20under%20uncertainty.%20Traditional%20regret-minimisation%20bandit%20strategies%20like%20Thompson%20Sampling%20and%20Upper%20Confidence%20Bound%20%28UCB%29%20typically%20rely%20on%20distributional%20assumptions%20or%20asymptotic%20guarantees%3B%20further%2C%20they%20remain%20largely%20focused%20on%20regret%2C%20neglecting%20their%20statistical%20properties.%20We%20address%20this%20gap.%20Through%20the%20adoption%20of%20CP%2C%20we%20bridge%20the%20regret-minimising%20potential%20of%20a%20decision-making%20bandit%20policy%20with%20statistical%20guarantees%20in%20the%20form%20of%20finite-time%20prediction%20coverage.%0A%20%20We%20demonstrate%20the%20potential%20of%20it%20Conformal%20Bandits%20through%20simulation%20studies%20and%20an%20application%20to%20portfolio%20allocation%2C%20a%20typical%20small-gap%20regime%2C%20where%20differences%20in%20arm%20rewards%20are%20far%20too%20small%20for%20classical%20policies%20to%20achieve%20optimal%20regret%20bounds%20in%20finite%20sample.%20Motivated%20by%20this%2C%20we%20showcase%20our%20framework%27s%20practical%20advantage%20in%20terms%20of%20regret%20in%20small-gap%20settings%2C%20as%20well%20as%20its%20added%20value%20in%20achieving%20nominal%20coverage%20guarantees%20where%20classical%20UCB%20policies%20fail.%20Focusing%20on%20our%20application%20of%20interest%2C%20we%20further%20illustrate%20how%20integrating%20hidden%20Markov%20models%20to%20capture%20the%20regime-switching%20behaviour%20of%20financial%20markets%2C%20enhances%20the%20exploration-exploitation%20trade-off%2C%20and%20translates%20into%20higher%20risk-adjusted%20regret%20efficiency%20returns%2C%20while%20preserving%20coverage%20guarantees.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Bandits%253A%2520Bringing%2520statistical%2520validity%2520and%2520reward%2520efficiency%2520to%2520the%2520small-gap%2520regime%26entry.906535625%3DSimone%2520Cuonzo%2520and%2520Nina%2520Deliu%26entry.1292438233%3DWe%2520introduce%2520Conformal%2520Bandits%252C%2520a%2520novel%2520framework%2520integrating%2520Conformal%2520Prediction%2520%2528CP%2529%2520into%2520bandit%2520problems%252C%2520a%2520classic%2520paradigm%2520for%2520sequential%2520decision-making%2520under%2520uncertainty.%2520Traditional%2520regret-minimisation%2520bandit%2520strategies%2520like%2520Thompson%2520Sampling%2520and%2520Upper%2520Confidence%2520Bound%2520%2528UCB%2529%2520typically%2520rely%2520on%2520distributional%2520assumptions%2520or%2520asymptotic%2520guarantees%253B%2520further%252C%2520they%2520remain%2520largely%2520focused%2520on%2520regret%252C%2520neglecting%2520their%2520statistical%2520properties.%2520We%2520address%2520this%2520gap.%2520Through%2520the%2520adoption%2520of%2520CP%252C%2520we%2520bridge%2520the%2520regret-minimising%2520potential%2520of%2520a%2520decision-making%2520bandit%2520policy%2520with%2520statistical%2520guarantees%2520in%2520the%2520form%2520of%2520finite-time%2520prediction%2520coverage.%250A%2520%2520We%2520demonstrate%2520the%2520potential%2520of%2520it%2520Conformal%2520Bandits%2520through%2520simulation%2520studies%2520and%2520an%2520application%2520to%2520portfolio%2520allocation%252C%2520a%2520typical%2520small-gap%2520regime%252C%2520where%2520differences%2520in%2520arm%2520rewards%2520are%2520far%2520too%2520small%2520for%2520classical%2520policies%2520to%2520achieve%2520optimal%2520regret%2520bounds%2520in%2520finite%2520sample.%2520Motivated%2520by%2520this%252C%2520we%2520showcase%2520our%2520framework%2527s%2520practical%2520advantage%2520in%2520terms%2520of%2520regret%2520in%2520small-gap%2520settings%252C%2520as%2520well%2520as%2520its%2520added%2520value%2520in%2520achieving%2520nominal%2520coverage%2520guarantees%2520where%2520classical%2520UCB%2520policies%2520fail.%2520Focusing%2520on%2520our%2520application%2520of%2520interest%252C%2520we%2520further%2520illustrate%2520how%2520integrating%2520hidden%2520Markov%2520models%2520to%2520capture%2520the%2520regime-switching%2520behaviour%2520of%2520financial%2520markets%252C%2520enhances%2520the%2520exploration-exploitation%2520trade-off%252C%2520and%2520translates%2520into%2520higher%2520risk-adjusted%2520regret%2520efficiency%2520returns%252C%2520while%2520preserving%2520coverage%2520guarantees.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Bandits%3A%20Bringing%20statistical%20validity%20and%20reward%20efficiency%20to%20the%20small-gap%20regime&entry.906535625=Simone%20Cuonzo%20and%20Nina%20Deliu&entry.1292438233=We%20introduce%20Conformal%20Bandits%2C%20a%20novel%20framework%20integrating%20Conformal%20Prediction%20%28CP%29%20into%20bandit%20problems%2C%20a%20classic%20paradigm%20for%20sequential%20decision-making%20under%20uncertainty.%20Traditional%20regret-minimisation%20bandit%20strategies%20like%20Thompson%20Sampling%20and%20Upper%20Confidence%20Bound%20%28UCB%29%20typically%20rely%20on%20distributional%20assumptions%20or%20asymptotic%20guarantees%3B%20further%2C%20they%20remain%20largely%20focused%20on%20regret%2C%20neglecting%20their%20statistical%20properties.%20We%20address%20this%20gap.%20Through%20the%20adoption%20of%20CP%2C%20we%20bridge%20the%20regret-minimising%20potential%20of%20a%20decision-making%20bandit%20policy%20with%20statistical%20guarantees%20in%20the%20form%20of%20finite-time%20prediction%20coverage.%0A%20%20We%20demonstrate%20the%20potential%20of%20it%20Conformal%20Bandits%20through%20simulation%20studies%20and%20an%20application%20to%20portfolio%20allocation%2C%20a%20typical%20small-gap%20regime%2C%20where%20differences%20in%20arm%20rewards%20are%20far%20too%20small%20for%20classical%20policies%20to%20achieve%20optimal%20regret%20bounds%20in%20finite%20sample.%20Motivated%20by%20this%2C%20we%20showcase%20our%20framework%27s%20practical%20advantage%20in%20terms%20of%20regret%20in%20small-gap%20settings%2C%20as%20well%20as%20its%20added%20value%20in%20achieving%20nominal%20coverage%20guarantees%20where%20classical%20UCB%20policies%20fail.%20Focusing%20on%20our%20application%20of%20interest%2C%20we%20further%20illustrate%20how%20integrating%20hidden%20Markov%20models%20to%20capture%20the%20regime-switching%20behaviour%20of%20financial%20markets%2C%20enhances%20the%20exploration-exploitation%20trade-off%2C%20and%20translates%20into%20higher%20risk-adjusted%20regret%20efficiency%20returns%2C%20while%20preserving%20coverage%20guarantees.&entry.1838667208=http%3A//arxiv.org/abs/2512.09850v1&entry.124074799=Read"},
{"title": "Semantic Data Augmentation Enhanced Invariant Risk Minimization for Medical Image Domain Generalization", "author": "Yaoyao Zhu and Xiuding Cai and Yingkai Wang and Yu Yao and Xu Luo and Zhongliang Fu", "abstract": "Deep learning has achieved remarkable success in medical image classification. However, its clinical application is often hindered by data heterogeneity caused by variations in scanner vendors, imaging protocols, and operators. Approaches such as invariant risk minimization (IRM) aim to address this challenge of out-of-distribution generalization. For instance, VIRM improves upon IRM by tackling the issue of insufficient feature support overlap, demonstrating promising potential. Nonetheless, these methods face limitations in medical imaging due to the scarcity of annotated data and the inefficiency of augmentation strategies. To address these issues, we propose a novel domain-oriented direction selector to replace the random augmentation strategy used in VIRM. Our method leverages inter-domain covariance as a guider for augmentation direction, guiding data augmentation towards the target domain. This approach effectively reduces domain discrepancies and enhances generalization performance. Experiments on a multi-center diabetic retinopathy dataset demonstrate that our method outperforms state-of-the-art approaches, particularly under limited data conditions and significant domain heterogeneity.", "link": "http://arxiv.org/abs/2502.05593v2", "date": "2025-12-10", "relevancy": 1.5389, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5524}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5028}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Data%20Augmentation%20Enhanced%20Invariant%20Risk%20Minimization%20for%20Medical%20Image%20Domain%20Generalization&body=Title%3A%20Semantic%20Data%20Augmentation%20Enhanced%20Invariant%20Risk%20Minimization%20for%20Medical%20Image%20Domain%20Generalization%0AAuthor%3A%20Yaoyao%20Zhu%20and%20Xiuding%20Cai%20and%20Yingkai%20Wang%20and%20Yu%20Yao%20and%20Xu%20Luo%20and%20Zhongliang%20Fu%0AAbstract%3A%20Deep%20learning%20has%20achieved%20remarkable%20success%20in%20medical%20image%20classification.%20However%2C%20its%20clinical%20application%20is%20often%20hindered%20by%20data%20heterogeneity%20caused%20by%20variations%20in%20scanner%20vendors%2C%20imaging%20protocols%2C%20and%20operators.%20Approaches%20such%20as%20invariant%20risk%20minimization%20%28IRM%29%20aim%20to%20address%20this%20challenge%20of%20out-of-distribution%20generalization.%20For%20instance%2C%20VIRM%20improves%20upon%20IRM%20by%20tackling%20the%20issue%20of%20insufficient%20feature%20support%20overlap%2C%20demonstrating%20promising%20potential.%20Nonetheless%2C%20these%20methods%20face%20limitations%20in%20medical%20imaging%20due%20to%20the%20scarcity%20of%20annotated%20data%20and%20the%20inefficiency%20of%20augmentation%20strategies.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20domain-oriented%20direction%20selector%20to%20replace%20the%20random%20augmentation%20strategy%20used%20in%20VIRM.%20Our%20method%20leverages%20inter-domain%20covariance%20as%20a%20guider%20for%20augmentation%20direction%2C%20guiding%20data%20augmentation%20towards%20the%20target%20domain.%20This%20approach%20effectively%20reduces%20domain%20discrepancies%20and%20enhances%20generalization%20performance.%20Experiments%20on%20a%20multi-center%20diabetic%20retinopathy%20dataset%20demonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20approaches%2C%20particularly%20under%20limited%20data%20conditions%20and%20significant%20domain%20heterogeneity.%0ALink%3A%20http%3A//arxiv.org/abs/2502.05593v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Data%2520Augmentation%2520Enhanced%2520Invariant%2520Risk%2520Minimization%2520for%2520Medical%2520Image%2520Domain%2520Generalization%26entry.906535625%3DYaoyao%2520Zhu%2520and%2520Xiuding%2520Cai%2520and%2520Yingkai%2520Wang%2520and%2520Yu%2520Yao%2520and%2520Xu%2520Luo%2520and%2520Zhongliang%2520Fu%26entry.1292438233%3DDeep%2520learning%2520has%2520achieved%2520remarkable%2520success%2520in%2520medical%2520image%2520classification.%2520However%252C%2520its%2520clinical%2520application%2520is%2520often%2520hindered%2520by%2520data%2520heterogeneity%2520caused%2520by%2520variations%2520in%2520scanner%2520vendors%252C%2520imaging%2520protocols%252C%2520and%2520operators.%2520Approaches%2520such%2520as%2520invariant%2520risk%2520minimization%2520%2528IRM%2529%2520aim%2520to%2520address%2520this%2520challenge%2520of%2520out-of-distribution%2520generalization.%2520For%2520instance%252C%2520VIRM%2520improves%2520upon%2520IRM%2520by%2520tackling%2520the%2520issue%2520of%2520insufficient%2520feature%2520support%2520overlap%252C%2520demonstrating%2520promising%2520potential.%2520Nonetheless%252C%2520these%2520methods%2520face%2520limitations%2520in%2520medical%2520imaging%2520due%2520to%2520the%2520scarcity%2520of%2520annotated%2520data%2520and%2520the%2520inefficiency%2520of%2520augmentation%2520strategies.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520domain-oriented%2520direction%2520selector%2520to%2520replace%2520the%2520random%2520augmentation%2520strategy%2520used%2520in%2520VIRM.%2520Our%2520method%2520leverages%2520inter-domain%2520covariance%2520as%2520a%2520guider%2520for%2520augmentation%2520direction%252C%2520guiding%2520data%2520augmentation%2520towards%2520the%2520target%2520domain.%2520This%2520approach%2520effectively%2520reduces%2520domain%2520discrepancies%2520and%2520enhances%2520generalization%2520performance.%2520Experiments%2520on%2520a%2520multi-center%2520diabetic%2520retinopathy%2520dataset%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520approaches%252C%2520particularly%2520under%2520limited%2520data%2520conditions%2520and%2520significant%2520domain%2520heterogeneity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05593v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Data%20Augmentation%20Enhanced%20Invariant%20Risk%20Minimization%20for%20Medical%20Image%20Domain%20Generalization&entry.906535625=Yaoyao%20Zhu%20and%20Xiuding%20Cai%20and%20Yingkai%20Wang%20and%20Yu%20Yao%20and%20Xu%20Luo%20and%20Zhongliang%20Fu&entry.1292438233=Deep%20learning%20has%20achieved%20remarkable%20success%20in%20medical%20image%20classification.%20However%2C%20its%20clinical%20application%20is%20often%20hindered%20by%20data%20heterogeneity%20caused%20by%20variations%20in%20scanner%20vendors%2C%20imaging%20protocols%2C%20and%20operators.%20Approaches%20such%20as%20invariant%20risk%20minimization%20%28IRM%29%20aim%20to%20address%20this%20challenge%20of%20out-of-distribution%20generalization.%20For%20instance%2C%20VIRM%20improves%20upon%20IRM%20by%20tackling%20the%20issue%20of%20insufficient%20feature%20support%20overlap%2C%20demonstrating%20promising%20potential.%20Nonetheless%2C%20these%20methods%20face%20limitations%20in%20medical%20imaging%20due%20to%20the%20scarcity%20of%20annotated%20data%20and%20the%20inefficiency%20of%20augmentation%20strategies.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20domain-oriented%20direction%20selector%20to%20replace%20the%20random%20augmentation%20strategy%20used%20in%20VIRM.%20Our%20method%20leverages%20inter-domain%20covariance%20as%20a%20guider%20for%20augmentation%20direction%2C%20guiding%20data%20augmentation%20towards%20the%20target%20domain.%20This%20approach%20effectively%20reduces%20domain%20discrepancies%20and%20enhances%20generalization%20performance.%20Experiments%20on%20a%20multi-center%20diabetic%20retinopathy%20dataset%20demonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20approaches%2C%20particularly%20under%20limited%20data%20conditions%20and%20significant%20domain%20heterogeneity.&entry.1838667208=http%3A//arxiv.org/abs/2502.05593v2&entry.124074799=Read"},
{"title": "REASAN: Learning Reactive Safe Navigation for Legged Robots", "author": "Qihao Yuan and Ziyu Cao and Ming Cao and Kailai Li", "abstract": "We present a novel modularized end-to-end framework for legged reactive navigation in complex dynamic environments using a single light detection and ranging (LiDAR) sensor. The system comprises four simulation-trained modules: three reinforcement-learning (RL) policies for locomotion, safety shielding, and navigation, and a transformer-based exteroceptive estimator that processes raw point-cloud inputs. This modular decomposition of complex legged motor-control tasks enables lightweight neural networks with simple architectures, trained using standard RL practices with targeted reward shaping and curriculum design, without reliance on heuristics or sophisticated policy-switching mechanisms. We conduct comprehensive ablations to validate our design choices and demonstrate improved robustness compared to existing approaches in challenging navigation tasks. The resulting reactive safe navigation (REASAN) system achieves fully onboard and real-time reactive navigation across both single- and multi-robot settings in complex environments. We release our training and deployment code at https://github.com/ASIG-X/REASAN.", "link": "http://arxiv.org/abs/2512.09537v1", "date": "2025-12-10", "relevancy": 1.7938, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6192}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5941}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REASAN%3A%20Learning%20Reactive%20Safe%20Navigation%20for%20Legged%20Robots&body=Title%3A%20REASAN%3A%20Learning%20Reactive%20Safe%20Navigation%20for%20Legged%20Robots%0AAuthor%3A%20Qihao%20Yuan%20and%20Ziyu%20Cao%20and%20Ming%20Cao%20and%20Kailai%20Li%0AAbstract%3A%20We%20present%20a%20novel%20modularized%20end-to-end%20framework%20for%20legged%20reactive%20navigation%20in%20complex%20dynamic%20environments%20using%20a%20single%20light%20detection%20and%20ranging%20%28LiDAR%29%20sensor.%20The%20system%20comprises%20four%20simulation-trained%20modules%3A%20three%20reinforcement-learning%20%28RL%29%20policies%20for%20locomotion%2C%20safety%20shielding%2C%20and%20navigation%2C%20and%20a%20transformer-based%20exteroceptive%20estimator%20that%20processes%20raw%20point-cloud%20inputs.%20This%20modular%20decomposition%20of%20complex%20legged%20motor-control%20tasks%20enables%20lightweight%20neural%20networks%20with%20simple%20architectures%2C%20trained%20using%20standard%20RL%20practices%20with%20targeted%20reward%20shaping%20and%20curriculum%20design%2C%20without%20reliance%20on%20heuristics%20or%20sophisticated%20policy-switching%20mechanisms.%20We%20conduct%20comprehensive%20ablations%20to%20validate%20our%20design%20choices%20and%20demonstrate%20improved%20robustness%20compared%20to%20existing%20approaches%20in%20challenging%20navigation%20tasks.%20The%20resulting%20reactive%20safe%20navigation%20%28REASAN%29%20system%20achieves%20fully%20onboard%20and%20real-time%20reactive%20navigation%20across%20both%20single-%20and%20multi-robot%20settings%20in%20complex%20environments.%20We%20release%20our%20training%20and%20deployment%20code%20at%20https%3A//github.com/ASIG-X/REASAN.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREASAN%253A%2520Learning%2520Reactive%2520Safe%2520Navigation%2520for%2520Legged%2520Robots%26entry.906535625%3DQihao%2520Yuan%2520and%2520Ziyu%2520Cao%2520and%2520Ming%2520Cao%2520and%2520Kailai%2520Li%26entry.1292438233%3DWe%2520present%2520a%2520novel%2520modularized%2520end-to-end%2520framework%2520for%2520legged%2520reactive%2520navigation%2520in%2520complex%2520dynamic%2520environments%2520using%2520a%2520single%2520light%2520detection%2520and%2520ranging%2520%2528LiDAR%2529%2520sensor.%2520The%2520system%2520comprises%2520four%2520simulation-trained%2520modules%253A%2520three%2520reinforcement-learning%2520%2528RL%2529%2520policies%2520for%2520locomotion%252C%2520safety%2520shielding%252C%2520and%2520navigation%252C%2520and%2520a%2520transformer-based%2520exteroceptive%2520estimator%2520that%2520processes%2520raw%2520point-cloud%2520inputs.%2520This%2520modular%2520decomposition%2520of%2520complex%2520legged%2520motor-control%2520tasks%2520enables%2520lightweight%2520neural%2520networks%2520with%2520simple%2520architectures%252C%2520trained%2520using%2520standard%2520RL%2520practices%2520with%2520targeted%2520reward%2520shaping%2520and%2520curriculum%2520design%252C%2520without%2520reliance%2520on%2520heuristics%2520or%2520sophisticated%2520policy-switching%2520mechanisms.%2520We%2520conduct%2520comprehensive%2520ablations%2520to%2520validate%2520our%2520design%2520choices%2520and%2520demonstrate%2520improved%2520robustness%2520compared%2520to%2520existing%2520approaches%2520in%2520challenging%2520navigation%2520tasks.%2520The%2520resulting%2520reactive%2520safe%2520navigation%2520%2528REASAN%2529%2520system%2520achieves%2520fully%2520onboard%2520and%2520real-time%2520reactive%2520navigation%2520across%2520both%2520single-%2520and%2520multi-robot%2520settings%2520in%2520complex%2520environments.%2520We%2520release%2520our%2520training%2520and%2520deployment%2520code%2520at%2520https%253A//github.com/ASIG-X/REASAN.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REASAN%3A%20Learning%20Reactive%20Safe%20Navigation%20for%20Legged%20Robots&entry.906535625=Qihao%20Yuan%20and%20Ziyu%20Cao%20and%20Ming%20Cao%20and%20Kailai%20Li&entry.1292438233=We%20present%20a%20novel%20modularized%20end-to-end%20framework%20for%20legged%20reactive%20navigation%20in%20complex%20dynamic%20environments%20using%20a%20single%20light%20detection%20and%20ranging%20%28LiDAR%29%20sensor.%20The%20system%20comprises%20four%20simulation-trained%20modules%3A%20three%20reinforcement-learning%20%28RL%29%20policies%20for%20locomotion%2C%20safety%20shielding%2C%20and%20navigation%2C%20and%20a%20transformer-based%20exteroceptive%20estimator%20that%20processes%20raw%20point-cloud%20inputs.%20This%20modular%20decomposition%20of%20complex%20legged%20motor-control%20tasks%20enables%20lightweight%20neural%20networks%20with%20simple%20architectures%2C%20trained%20using%20standard%20RL%20practices%20with%20targeted%20reward%20shaping%20and%20curriculum%20design%2C%20without%20reliance%20on%20heuristics%20or%20sophisticated%20policy-switching%20mechanisms.%20We%20conduct%20comprehensive%20ablations%20to%20validate%20our%20design%20choices%20and%20demonstrate%20improved%20robustness%20compared%20to%20existing%20approaches%20in%20challenging%20navigation%20tasks.%20The%20resulting%20reactive%20safe%20navigation%20%28REASAN%29%20system%20achieves%20fully%20onboard%20and%20real-time%20reactive%20navigation%20across%20both%20single-%20and%20multi-robot%20settings%20in%20complex%20environments.%20We%20release%20our%20training%20and%20deployment%20code%20at%20https%3A//github.com/ASIG-X/REASAN.&entry.1838667208=http%3A//arxiv.org/abs/2512.09537v1&entry.124074799=Read"},
{"title": "Predicting Polymer Solubility in Solvents Using SMILES Strings", "author": "Andrew Reinhard", "abstract": "Understanding and predicting polymer solubility in various solvents is critical for applications ranging from recycling to pharmaceutical formulation. This work presents a deep learning framework that predicts polymer solubility, expressed as weight percent (wt%), directly from SMILES representations of both polymers and solvents. A dataset of 8,049 polymer solvent pairs at 25 deg C was constructed from calibrated molecular dynamics simulations (Zhou et al., 2023), and molecular descriptors and fingerprints were combined into a 2,394 feature representation per sample. A fully connected neural network with six hidden layers was trained using the Adam optimizer and evaluated using mean squared error loss, achieving strong agreement between predicted and actual solubility values. Generalizability was demonstrated using experimentally measured data from the Materials Genome Project, where the model maintained high accuracy on 25 unseen polymer solvent combinations. These findings highlight the viability of SMILES based machine learning models for scalable solubility prediction and high-throughput solvent screening, supporting applications in green chemistry, polymer processing, and materials design.", "link": "http://arxiv.org/abs/2512.09784v1", "date": "2025-12-10", "relevancy": 1.2061, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4331}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3967}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Polymer%20Solubility%20in%20Solvents%20Using%20SMILES%20Strings&body=Title%3A%20Predicting%20Polymer%20Solubility%20in%20Solvents%20Using%20SMILES%20Strings%0AAuthor%3A%20Andrew%20Reinhard%0AAbstract%3A%20Understanding%20and%20predicting%20polymer%20solubility%20in%20various%20solvents%20is%20critical%20for%20applications%20ranging%20from%20recycling%20to%20pharmaceutical%20formulation.%20This%20work%20presents%20a%20deep%20learning%20framework%20that%20predicts%20polymer%20solubility%2C%20expressed%20as%20weight%20percent%20%28wt%25%29%2C%20directly%20from%20SMILES%20representations%20of%20both%20polymers%20and%20solvents.%20A%20dataset%20of%208%2C049%20polymer%20solvent%20pairs%20at%2025%20deg%20C%20was%20constructed%20from%20calibrated%20molecular%20dynamics%20simulations%20%28Zhou%20et%20al.%2C%202023%29%2C%20and%20molecular%20descriptors%20and%20fingerprints%20were%20combined%20into%20a%202%2C394%20feature%20representation%20per%20sample.%20A%20fully%20connected%20neural%20network%20with%20six%20hidden%20layers%20was%20trained%20using%20the%20Adam%20optimizer%20and%20evaluated%20using%20mean%20squared%20error%20loss%2C%20achieving%20strong%20agreement%20between%20predicted%20and%20actual%20solubility%20values.%20Generalizability%20was%20demonstrated%20using%20experimentally%20measured%20data%20from%20the%20Materials%20Genome%20Project%2C%20where%20the%20model%20maintained%20high%20accuracy%20on%2025%20unseen%20polymer%20solvent%20combinations.%20These%20findings%20highlight%20the%20viability%20of%20SMILES%20based%20machine%20learning%20models%20for%20scalable%20solubility%20prediction%20and%20high-throughput%20solvent%20screening%2C%20supporting%20applications%20in%20green%20chemistry%2C%20polymer%20processing%2C%20and%20materials%20design.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Polymer%2520Solubility%2520in%2520Solvents%2520Using%2520SMILES%2520Strings%26entry.906535625%3DAndrew%2520Reinhard%26entry.1292438233%3DUnderstanding%2520and%2520predicting%2520polymer%2520solubility%2520in%2520various%2520solvents%2520is%2520critical%2520for%2520applications%2520ranging%2520from%2520recycling%2520to%2520pharmaceutical%2520formulation.%2520This%2520work%2520presents%2520a%2520deep%2520learning%2520framework%2520that%2520predicts%2520polymer%2520solubility%252C%2520expressed%2520as%2520weight%2520percent%2520%2528wt%2525%2529%252C%2520directly%2520from%2520SMILES%2520representations%2520of%2520both%2520polymers%2520and%2520solvents.%2520A%2520dataset%2520of%25208%252C049%2520polymer%2520solvent%2520pairs%2520at%252025%2520deg%2520C%2520was%2520constructed%2520from%2520calibrated%2520molecular%2520dynamics%2520simulations%2520%2528Zhou%2520et%2520al.%252C%25202023%2529%252C%2520and%2520molecular%2520descriptors%2520and%2520fingerprints%2520were%2520combined%2520into%2520a%25202%252C394%2520feature%2520representation%2520per%2520sample.%2520A%2520fully%2520connected%2520neural%2520network%2520with%2520six%2520hidden%2520layers%2520was%2520trained%2520using%2520the%2520Adam%2520optimizer%2520and%2520evaluated%2520using%2520mean%2520squared%2520error%2520loss%252C%2520achieving%2520strong%2520agreement%2520between%2520predicted%2520and%2520actual%2520solubility%2520values.%2520Generalizability%2520was%2520demonstrated%2520using%2520experimentally%2520measured%2520data%2520from%2520the%2520Materials%2520Genome%2520Project%252C%2520where%2520the%2520model%2520maintained%2520high%2520accuracy%2520on%252025%2520unseen%2520polymer%2520solvent%2520combinations.%2520These%2520findings%2520highlight%2520the%2520viability%2520of%2520SMILES%2520based%2520machine%2520learning%2520models%2520for%2520scalable%2520solubility%2520prediction%2520and%2520high-throughput%2520solvent%2520screening%252C%2520supporting%2520applications%2520in%2520green%2520chemistry%252C%2520polymer%2520processing%252C%2520and%2520materials%2520design.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Polymer%20Solubility%20in%20Solvents%20Using%20SMILES%20Strings&entry.906535625=Andrew%20Reinhard&entry.1292438233=Understanding%20and%20predicting%20polymer%20solubility%20in%20various%20solvents%20is%20critical%20for%20applications%20ranging%20from%20recycling%20to%20pharmaceutical%20formulation.%20This%20work%20presents%20a%20deep%20learning%20framework%20that%20predicts%20polymer%20solubility%2C%20expressed%20as%20weight%20percent%20%28wt%25%29%2C%20directly%20from%20SMILES%20representations%20of%20both%20polymers%20and%20solvents.%20A%20dataset%20of%208%2C049%20polymer%20solvent%20pairs%20at%2025%20deg%20C%20was%20constructed%20from%20calibrated%20molecular%20dynamics%20simulations%20%28Zhou%20et%20al.%2C%202023%29%2C%20and%20molecular%20descriptors%20and%20fingerprints%20were%20combined%20into%20a%202%2C394%20feature%20representation%20per%20sample.%20A%20fully%20connected%20neural%20network%20with%20six%20hidden%20layers%20was%20trained%20using%20the%20Adam%20optimizer%20and%20evaluated%20using%20mean%20squared%20error%20loss%2C%20achieving%20strong%20agreement%20between%20predicted%20and%20actual%20solubility%20values.%20Generalizability%20was%20demonstrated%20using%20experimentally%20measured%20data%20from%20the%20Materials%20Genome%20Project%2C%20where%20the%20model%20maintained%20high%20accuracy%20on%2025%20unseen%20polymer%20solvent%20combinations.%20These%20findings%20highlight%20the%20viability%20of%20SMILES%20based%20machine%20learning%20models%20for%20scalable%20solubility%20prediction%20and%20high-throughput%20solvent%20screening%2C%20supporting%20applications%20in%20green%20chemistry%2C%20polymer%20processing%2C%20and%20materials%20design.&entry.1838667208=http%3A//arxiv.org/abs/2512.09784v1&entry.124074799=Read"},
{"title": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "author": "Team Seedream and  : and Yunpeng Chen and Yu Gao and Lixue Gong and Meng Guo and Qiushan Guo and Zhiyao Guo and Xiaoxia Hou and Weilin Huang and Yixuan Huang and Xiaowen Jian and Huafeng Kuang and Zhichao Lai and Fanshi Li and Liang Li and Xiaochen Lian and Chao Liao and Liyang Liu and Wei Liu and Yanzuo Lu and Zhengxiong Luo and Tongtong Ou and Guang Shi and Yichun Shi and Shiqi Sun and Yu Tian and Zhi Tian and Peng Wang and Rui Wang and Xun Wang and Ye Wang and Guofeng Wu and Jie Wu and Wenxu Wu and Yonghui Wu and Xin Xia and Xuefeng Xiao and Shuang Xu and Xin Yan and Ceyuan Yang and Jianchao Yang and Zhonghua Zhai and Chenlin Zhang and Heng Zhang and Qi Zhang and Xinyu Zhang and Yuwei Zhang and Shijia Zhao and Wenliang Zhao and Wenjia Zhu", "abstract": "We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. We further scale our model and data as Seedream 4.5. Seedream 4.0 and Seedream 4.5 are accessible on Volcano Engine https://www.volcengine.com/experience/ark?launch=seedream.", "link": "http://arxiv.org/abs/2509.20427v3", "date": "2025-12-10", "relevancy": 1.7763, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5947}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5942}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seedream%204.0%3A%20Toward%20Next-generation%20Multimodal%20Image%20Generation&body=Title%3A%20Seedream%204.0%3A%20Toward%20Next-generation%20Multimodal%20Image%20Generation%0AAuthor%3A%20Team%20Seedream%20and%20%20%3A%20and%20Yunpeng%20Chen%20and%20Yu%20Gao%20and%20Lixue%20Gong%20and%20Meng%20Guo%20and%20Qiushan%20Guo%20and%20Zhiyao%20Guo%20and%20Xiaoxia%20Hou%20and%20Weilin%20Huang%20and%20Yixuan%20Huang%20and%20Xiaowen%20Jian%20and%20Huafeng%20Kuang%20and%20Zhichao%20Lai%20and%20Fanshi%20Li%20and%20Liang%20Li%20and%20Xiaochen%20Lian%20and%20Chao%20Liao%20and%20Liyang%20Liu%20and%20Wei%20Liu%20and%20Yanzuo%20Lu%20and%20Zhengxiong%20Luo%20and%20Tongtong%20Ou%20and%20Guang%20Shi%20and%20Yichun%20Shi%20and%20Shiqi%20Sun%20and%20Yu%20Tian%20and%20Zhi%20Tian%20and%20Peng%20Wang%20and%20Rui%20Wang%20and%20Xun%20Wang%20and%20Ye%20Wang%20and%20Guofeng%20Wu%20and%20Jie%20Wu%20and%20Wenxu%20Wu%20and%20Yonghui%20Wu%20and%20Xin%20Xia%20and%20Xuefeng%20Xiao%20and%20Shuang%20Xu%20and%20Xin%20Yan%20and%20Ceyuan%20Yang%20and%20Jianchao%20Yang%20and%20Zhonghua%20Zhai%20and%20Chenlin%20Zhang%20and%20Heng%20Zhang%20and%20Qi%20Zhang%20and%20Xinyu%20Zhang%20and%20Yuwei%20Zhang%20and%20Shijia%20Zhao%20and%20Wenliang%20Zhao%20and%20Wenjia%20Zhu%0AAbstract%3A%20We%20introduce%20Seedream%204.0%2C%20an%20efficient%20and%20high-performance%20multimodal%20image%20generation%20system%20that%20unifies%20text-to-image%20%28T2I%29%20synthesis%2C%20image%20editing%2C%20and%20multi-image%20composition%20within%20a%20single%20framework.%20We%20develop%20a%20highly%20efficient%20diffusion%20transformer%20with%20a%20powerful%20VAE%20which%20also%20can%20reduce%20the%20number%20of%20image%20tokens%20considerably.%20This%20allows%20for%20efficient%20training%20of%20our%20model%2C%20and%20enables%20it%20to%20fast%20generate%20native%20high-resolution%20images%20%28e.g.%2C%201K-4K%29.%20Seedream%204.0%20is%20pretrained%20on%20billions%20of%20text-image%20pairs%20spanning%20diverse%20taxonomies%20and%20knowledge-centric%20concepts.%20Comprehensive%20data%20collection%20across%20hundreds%20of%20vertical%20scenarios%2C%20coupled%20with%20optimized%20strategies%2C%20ensures%20stable%20and%20large-scale%20training%2C%20with%20strong%20generalization.%20By%20incorporating%20a%20carefully%20fine-tuned%20VLM%20model%2C%20we%20perform%20multi-modal%20post-training%20for%20training%20both%20T2I%20and%20image%20editing%20tasks%20jointly.%20For%20inference%20acceleration%2C%20we%20integrate%20adversarial%20distillation%2C%20distribution%20matching%2C%20and%20quantization%2C%20as%20well%20as%20speculative%20decoding.%20It%20achieves%20an%20inference%20time%20of%20up%20to%201.8%20seconds%20for%20generating%20a%202K%20image%20%28without%20a%20LLM/VLM%20as%20PE%20model%29.%20Comprehensive%20evaluations%20reveal%20that%20Seedream%204.0%20can%20achieve%20state-of-the-art%20results%20on%20both%20T2I%20and%20multimodal%20image%20editing.%20In%20particular%2C%20it%20demonstrates%20exceptional%20multimodal%20capabilities%20in%20complex%20tasks%2C%20including%20precise%20image%20editing%20and%20in-context%20reasoning%2C%20and%20also%20allows%20for%20multi-image%20reference%2C%20and%20can%20generate%20multiple%20output%20images.%20This%20extends%20traditional%20T2I%20systems%20into%20an%20more%20interactive%20and%20multidimensional%20creative%20tool%2C%20pushing%20the%20boundary%20of%20generative%20AI%20for%20both%20creativity%20and%20professional%20applications.%20We%20further%20scale%20our%20model%20and%20data%20as%20Seedream%204.5.%20Seedream%204.0%20and%20Seedream%204.5%20are%20accessible%20on%20Volcano%20Engine%20https%3A//www.volcengine.com/experience/ark%3Flaunch%3Dseedream.%0ALink%3A%20http%3A//arxiv.org/abs/2509.20427v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeedream%25204.0%253A%2520Toward%2520Next-generation%2520Multimodal%2520Image%2520Generation%26entry.906535625%3DTeam%2520Seedream%2520and%2520%2520%253A%2520and%2520Yunpeng%2520Chen%2520and%2520Yu%2520Gao%2520and%2520Lixue%2520Gong%2520and%2520Meng%2520Guo%2520and%2520Qiushan%2520Guo%2520and%2520Zhiyao%2520Guo%2520and%2520Xiaoxia%2520Hou%2520and%2520Weilin%2520Huang%2520and%2520Yixuan%2520Huang%2520and%2520Xiaowen%2520Jian%2520and%2520Huafeng%2520Kuang%2520and%2520Zhichao%2520Lai%2520and%2520Fanshi%2520Li%2520and%2520Liang%2520Li%2520and%2520Xiaochen%2520Lian%2520and%2520Chao%2520Liao%2520and%2520Liyang%2520Liu%2520and%2520Wei%2520Liu%2520and%2520Yanzuo%2520Lu%2520and%2520Zhengxiong%2520Luo%2520and%2520Tongtong%2520Ou%2520and%2520Guang%2520Shi%2520and%2520Yichun%2520Shi%2520and%2520Shiqi%2520Sun%2520and%2520Yu%2520Tian%2520and%2520Zhi%2520Tian%2520and%2520Peng%2520Wang%2520and%2520Rui%2520Wang%2520and%2520Xun%2520Wang%2520and%2520Ye%2520Wang%2520and%2520Guofeng%2520Wu%2520and%2520Jie%2520Wu%2520and%2520Wenxu%2520Wu%2520and%2520Yonghui%2520Wu%2520and%2520Xin%2520Xia%2520and%2520Xuefeng%2520Xiao%2520and%2520Shuang%2520Xu%2520and%2520Xin%2520Yan%2520and%2520Ceyuan%2520Yang%2520and%2520Jianchao%2520Yang%2520and%2520Zhonghua%2520Zhai%2520and%2520Chenlin%2520Zhang%2520and%2520Heng%2520Zhang%2520and%2520Qi%2520Zhang%2520and%2520Xinyu%2520Zhang%2520and%2520Yuwei%2520Zhang%2520and%2520Shijia%2520Zhao%2520and%2520Wenliang%2520Zhao%2520and%2520Wenjia%2520Zhu%26entry.1292438233%3DWe%2520introduce%2520Seedream%25204.0%252C%2520an%2520efficient%2520and%2520high-performance%2520multimodal%2520image%2520generation%2520system%2520that%2520unifies%2520text-to-image%2520%2528T2I%2529%2520synthesis%252C%2520image%2520editing%252C%2520and%2520multi-image%2520composition%2520within%2520a%2520single%2520framework.%2520We%2520develop%2520a%2520highly%2520efficient%2520diffusion%2520transformer%2520with%2520a%2520powerful%2520VAE%2520which%2520also%2520can%2520reduce%2520the%2520number%2520of%2520image%2520tokens%2520considerably.%2520This%2520allows%2520for%2520efficient%2520training%2520of%2520our%2520model%252C%2520and%2520enables%2520it%2520to%2520fast%2520generate%2520native%2520high-resolution%2520images%2520%2528e.g.%252C%25201K-4K%2529.%2520Seedream%25204.0%2520is%2520pretrained%2520on%2520billions%2520of%2520text-image%2520pairs%2520spanning%2520diverse%2520taxonomies%2520and%2520knowledge-centric%2520concepts.%2520Comprehensive%2520data%2520collection%2520across%2520hundreds%2520of%2520vertical%2520scenarios%252C%2520coupled%2520with%2520optimized%2520strategies%252C%2520ensures%2520stable%2520and%2520large-scale%2520training%252C%2520with%2520strong%2520generalization.%2520By%2520incorporating%2520a%2520carefully%2520fine-tuned%2520VLM%2520model%252C%2520we%2520perform%2520multi-modal%2520post-training%2520for%2520training%2520both%2520T2I%2520and%2520image%2520editing%2520tasks%2520jointly.%2520For%2520inference%2520acceleration%252C%2520we%2520integrate%2520adversarial%2520distillation%252C%2520distribution%2520matching%252C%2520and%2520quantization%252C%2520as%2520well%2520as%2520speculative%2520decoding.%2520It%2520achieves%2520an%2520inference%2520time%2520of%2520up%2520to%25201.8%2520seconds%2520for%2520generating%2520a%25202K%2520image%2520%2528without%2520a%2520LLM/VLM%2520as%2520PE%2520model%2529.%2520Comprehensive%2520evaluations%2520reveal%2520that%2520Seedream%25204.0%2520can%2520achieve%2520state-of-the-art%2520results%2520on%2520both%2520T2I%2520and%2520multimodal%2520image%2520editing.%2520In%2520particular%252C%2520it%2520demonstrates%2520exceptional%2520multimodal%2520capabilities%2520in%2520complex%2520tasks%252C%2520including%2520precise%2520image%2520editing%2520and%2520in-context%2520reasoning%252C%2520and%2520also%2520allows%2520for%2520multi-image%2520reference%252C%2520and%2520can%2520generate%2520multiple%2520output%2520images.%2520This%2520extends%2520traditional%2520T2I%2520systems%2520into%2520an%2520more%2520interactive%2520and%2520multidimensional%2520creative%2520tool%252C%2520pushing%2520the%2520boundary%2520of%2520generative%2520AI%2520for%2520both%2520creativity%2520and%2520professional%2520applications.%2520We%2520further%2520scale%2520our%2520model%2520and%2520data%2520as%2520Seedream%25204.5.%2520Seedream%25204.0%2520and%2520Seedream%25204.5%2520are%2520accessible%2520on%2520Volcano%2520Engine%2520https%253A//www.volcengine.com/experience/ark%253Flaunch%253Dseedream.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20427v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seedream%204.0%3A%20Toward%20Next-generation%20Multimodal%20Image%20Generation&entry.906535625=Team%20Seedream%20and%20%20%3A%20and%20Yunpeng%20Chen%20and%20Yu%20Gao%20and%20Lixue%20Gong%20and%20Meng%20Guo%20and%20Qiushan%20Guo%20and%20Zhiyao%20Guo%20and%20Xiaoxia%20Hou%20and%20Weilin%20Huang%20and%20Yixuan%20Huang%20and%20Xiaowen%20Jian%20and%20Huafeng%20Kuang%20and%20Zhichao%20Lai%20and%20Fanshi%20Li%20and%20Liang%20Li%20and%20Xiaochen%20Lian%20and%20Chao%20Liao%20and%20Liyang%20Liu%20and%20Wei%20Liu%20and%20Yanzuo%20Lu%20and%20Zhengxiong%20Luo%20and%20Tongtong%20Ou%20and%20Guang%20Shi%20and%20Yichun%20Shi%20and%20Shiqi%20Sun%20and%20Yu%20Tian%20and%20Zhi%20Tian%20and%20Peng%20Wang%20and%20Rui%20Wang%20and%20Xun%20Wang%20and%20Ye%20Wang%20and%20Guofeng%20Wu%20and%20Jie%20Wu%20and%20Wenxu%20Wu%20and%20Yonghui%20Wu%20and%20Xin%20Xia%20and%20Xuefeng%20Xiao%20and%20Shuang%20Xu%20and%20Xin%20Yan%20and%20Ceyuan%20Yang%20and%20Jianchao%20Yang%20and%20Zhonghua%20Zhai%20and%20Chenlin%20Zhang%20and%20Heng%20Zhang%20and%20Qi%20Zhang%20and%20Xinyu%20Zhang%20and%20Yuwei%20Zhang%20and%20Shijia%20Zhao%20and%20Wenliang%20Zhao%20and%20Wenjia%20Zhu&entry.1292438233=We%20introduce%20Seedream%204.0%2C%20an%20efficient%20and%20high-performance%20multimodal%20image%20generation%20system%20that%20unifies%20text-to-image%20%28T2I%29%20synthesis%2C%20image%20editing%2C%20and%20multi-image%20composition%20within%20a%20single%20framework.%20We%20develop%20a%20highly%20efficient%20diffusion%20transformer%20with%20a%20powerful%20VAE%20which%20also%20can%20reduce%20the%20number%20of%20image%20tokens%20considerably.%20This%20allows%20for%20efficient%20training%20of%20our%20model%2C%20and%20enables%20it%20to%20fast%20generate%20native%20high-resolution%20images%20%28e.g.%2C%201K-4K%29.%20Seedream%204.0%20is%20pretrained%20on%20billions%20of%20text-image%20pairs%20spanning%20diverse%20taxonomies%20and%20knowledge-centric%20concepts.%20Comprehensive%20data%20collection%20across%20hundreds%20of%20vertical%20scenarios%2C%20coupled%20with%20optimized%20strategies%2C%20ensures%20stable%20and%20large-scale%20training%2C%20with%20strong%20generalization.%20By%20incorporating%20a%20carefully%20fine-tuned%20VLM%20model%2C%20we%20perform%20multi-modal%20post-training%20for%20training%20both%20T2I%20and%20image%20editing%20tasks%20jointly.%20For%20inference%20acceleration%2C%20we%20integrate%20adversarial%20distillation%2C%20distribution%20matching%2C%20and%20quantization%2C%20as%20well%20as%20speculative%20decoding.%20It%20achieves%20an%20inference%20time%20of%20up%20to%201.8%20seconds%20for%20generating%20a%202K%20image%20%28without%20a%20LLM/VLM%20as%20PE%20model%29.%20Comprehensive%20evaluations%20reveal%20that%20Seedream%204.0%20can%20achieve%20state-of-the-art%20results%20on%20both%20T2I%20and%20multimodal%20image%20editing.%20In%20particular%2C%20it%20demonstrates%20exceptional%20multimodal%20capabilities%20in%20complex%20tasks%2C%20including%20precise%20image%20editing%20and%20in-context%20reasoning%2C%20and%20also%20allows%20for%20multi-image%20reference%2C%20and%20can%20generate%20multiple%20output%20images.%20This%20extends%20traditional%20T2I%20systems%20into%20an%20more%20interactive%20and%20multidimensional%20creative%20tool%2C%20pushing%20the%20boundary%20of%20generative%20AI%20for%20both%20creativity%20and%20professional%20applications.%20We%20further%20scale%20our%20model%20and%20data%20as%20Seedream%204.5.%20Seedream%204.0%20and%20Seedream%204.5%20are%20accessible%20on%20Volcano%20Engine%20https%3A//www.volcengine.com/experience/ark%3Flaunch%3Dseedream.&entry.1838667208=http%3A//arxiv.org/abs/2509.20427v3&entry.124074799=Read"},
{"title": "Ethics Readiness of Artificial Intelligence: A Practical Evaluation Method", "author": "Laurynas Adomaitis and Vincent Israel-Jost and Alexei Grinbaum", "abstract": "We present Ethics Readiness Levels (ERLs), a four-level, iterative method to track how ethical reflection is implemented in the design of AI systems. ERLs bridge high-level ethical principles and everyday engineering by turning ethical values into concrete prompts, checks, and controls within real use cases. The evaluation is conducted using a dynamic, tree-like questionnaire built from context-specific indicators, ensuring relevance to the technology and application domain. Beyond being a managerial tool, ERLs help facilitate a structured dialogue between ethics experts and technical teams, while our scoring system helps track progress over time. We demonstrate the methodology through two case studies: an AI facial sketch generator for law enforcement and a collaborative industrial robot. The ERL tool effectively catalyzes concrete design changes and promotes a shift from narrow technological solutionism to a more reflective, ethics-by-design mindset.", "link": "http://arxiv.org/abs/2512.09729v1", "date": "2025-12-10", "relevancy": 1.3035, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.442}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4405}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ethics%20Readiness%20of%20Artificial%20Intelligence%3A%20A%20Practical%20Evaluation%20Method&body=Title%3A%20Ethics%20Readiness%20of%20Artificial%20Intelligence%3A%20A%20Practical%20Evaluation%20Method%0AAuthor%3A%20Laurynas%20Adomaitis%20and%20Vincent%20Israel-Jost%20and%20Alexei%20Grinbaum%0AAbstract%3A%20We%20present%20Ethics%20Readiness%20Levels%20%28ERLs%29%2C%20a%20four-level%2C%20iterative%20method%20to%20track%20how%20ethical%20reflection%20is%20implemented%20in%20the%20design%20of%20AI%20systems.%20ERLs%20bridge%20high-level%20ethical%20principles%20and%20everyday%20engineering%20by%20turning%20ethical%20values%20into%20concrete%20prompts%2C%20checks%2C%20and%20controls%20within%20real%20use%20cases.%20The%20evaluation%20is%20conducted%20using%20a%20dynamic%2C%20tree-like%20questionnaire%20built%20from%20context-specific%20indicators%2C%20ensuring%20relevance%20to%20the%20technology%20and%20application%20domain.%20Beyond%20being%20a%20managerial%20tool%2C%20ERLs%20help%20facilitate%20a%20structured%20dialogue%20between%20ethics%20experts%20and%20technical%20teams%2C%20while%20our%20scoring%20system%20helps%20track%20progress%20over%20time.%20We%20demonstrate%20the%20methodology%20through%20two%20case%20studies%3A%20an%20AI%20facial%20sketch%20generator%20for%20law%20enforcement%20and%20a%20collaborative%20industrial%20robot.%20The%20ERL%20tool%20effectively%20catalyzes%20concrete%20design%20changes%20and%20promotes%20a%20shift%20from%20narrow%20technological%20solutionism%20to%20a%20more%20reflective%2C%20ethics-by-design%20mindset.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEthics%2520Readiness%2520of%2520Artificial%2520Intelligence%253A%2520A%2520Practical%2520Evaluation%2520Method%26entry.906535625%3DLaurynas%2520Adomaitis%2520and%2520Vincent%2520Israel-Jost%2520and%2520Alexei%2520Grinbaum%26entry.1292438233%3DWe%2520present%2520Ethics%2520Readiness%2520Levels%2520%2528ERLs%2529%252C%2520a%2520four-level%252C%2520iterative%2520method%2520to%2520track%2520how%2520ethical%2520reflection%2520is%2520implemented%2520in%2520the%2520design%2520of%2520AI%2520systems.%2520ERLs%2520bridge%2520high-level%2520ethical%2520principles%2520and%2520everyday%2520engineering%2520by%2520turning%2520ethical%2520values%2520into%2520concrete%2520prompts%252C%2520checks%252C%2520and%2520controls%2520within%2520real%2520use%2520cases.%2520The%2520evaluation%2520is%2520conducted%2520using%2520a%2520dynamic%252C%2520tree-like%2520questionnaire%2520built%2520from%2520context-specific%2520indicators%252C%2520ensuring%2520relevance%2520to%2520the%2520technology%2520and%2520application%2520domain.%2520Beyond%2520being%2520a%2520managerial%2520tool%252C%2520ERLs%2520help%2520facilitate%2520a%2520structured%2520dialogue%2520between%2520ethics%2520experts%2520and%2520technical%2520teams%252C%2520while%2520our%2520scoring%2520system%2520helps%2520track%2520progress%2520over%2520time.%2520We%2520demonstrate%2520the%2520methodology%2520through%2520two%2520case%2520studies%253A%2520an%2520AI%2520facial%2520sketch%2520generator%2520for%2520law%2520enforcement%2520and%2520a%2520collaborative%2520industrial%2520robot.%2520The%2520ERL%2520tool%2520effectively%2520catalyzes%2520concrete%2520design%2520changes%2520and%2520promotes%2520a%2520shift%2520from%2520narrow%2520technological%2520solutionism%2520to%2520a%2520more%2520reflective%252C%2520ethics-by-design%2520mindset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ethics%20Readiness%20of%20Artificial%20Intelligence%3A%20A%20Practical%20Evaluation%20Method&entry.906535625=Laurynas%20Adomaitis%20and%20Vincent%20Israel-Jost%20and%20Alexei%20Grinbaum&entry.1292438233=We%20present%20Ethics%20Readiness%20Levels%20%28ERLs%29%2C%20a%20four-level%2C%20iterative%20method%20to%20track%20how%20ethical%20reflection%20is%20implemented%20in%20the%20design%20of%20AI%20systems.%20ERLs%20bridge%20high-level%20ethical%20principles%20and%20everyday%20engineering%20by%20turning%20ethical%20values%20into%20concrete%20prompts%2C%20checks%2C%20and%20controls%20within%20real%20use%20cases.%20The%20evaluation%20is%20conducted%20using%20a%20dynamic%2C%20tree-like%20questionnaire%20built%20from%20context-specific%20indicators%2C%20ensuring%20relevance%20to%20the%20technology%20and%20application%20domain.%20Beyond%20being%20a%20managerial%20tool%2C%20ERLs%20help%20facilitate%20a%20structured%20dialogue%20between%20ethics%20experts%20and%20technical%20teams%2C%20while%20our%20scoring%20system%20helps%20track%20progress%20over%20time.%20We%20demonstrate%20the%20methodology%20through%20two%20case%20studies%3A%20an%20AI%20facial%20sketch%20generator%20for%20law%20enforcement%20and%20a%20collaborative%20industrial%20robot.%20The%20ERL%20tool%20effectively%20catalyzes%20concrete%20design%20changes%20and%20promotes%20a%20shift%20from%20narrow%20technological%20solutionism%20to%20a%20more%20reflective%2C%20ethics-by-design%20mindset.&entry.1838667208=http%3A//arxiv.org/abs/2512.09729v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


