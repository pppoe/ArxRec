<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250909.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting", "author": "Mahtab Dahaghin and Milind G. Padalkar and Matteo Toso and Alessio Del Bue", "abstract": "  3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D\nscene representations from sets of multi-view images. However, inpainting\nmissing regions, whether due to occlusion or scene editing, remains a\nchallenging task, often leading to blurry details, artifacts, and inconsistent\ngeometry. In this work, we introduce SplatFill, a novel depth-guided approach\nfor 3DGS scene inpainting that achieves state-of-the-art perceptual quality and\nimproved efficiency. Our method combines two key ideas: (1) joint depth-based\nand object-based supervision to ensure inpainted Gaussians are accurately\nplaced in 3D space and aligned with surrounding geometry, and (2) we propose a\nconsistency-aware refinement scheme that selectively identifies and corrects\ninconsistent regions without disrupting the rest of the scene. Evaluations on\nthe SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing\nNeRF-based and 3DGS-based inpainting methods in visual fidelity but also\nreduces training time by 24.5%. Qualitative results show our method delivers\nsharper details, fewer artifacts, and greater coherence across challenging\nviewpoints.\n", "link": "http://arxiv.org/abs/2509.07809v1", "date": "2025-09-09", "relevancy": 3.3164, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6703}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6602}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SplatFill%3A%203D%20Scene%20Inpainting%20via%20Depth-Guided%20Gaussian%20Splatting&body=Title%3A%20SplatFill%3A%203D%20Scene%20Inpainting%20via%20Depth-Guided%20Gaussian%20Splatting%0AAuthor%3A%20Mahtab%20Dahaghin%20and%20Milind%20G.%20Padalkar%20and%20Matteo%20Toso%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20enabled%20the%20creation%20of%20highly%20realistic%203D%0Ascene%20representations%20from%20sets%20of%20multi-view%20images.%20However%2C%20inpainting%0Amissing%20regions%2C%20whether%20due%20to%20occlusion%20or%20scene%20editing%2C%20remains%20a%0Achallenging%20task%2C%20often%20leading%20to%20blurry%20details%2C%20artifacts%2C%20and%20inconsistent%0Ageometry.%20In%20this%20work%2C%20we%20introduce%20SplatFill%2C%20a%20novel%20depth-guided%20approach%0Afor%203DGS%20scene%20inpainting%20that%20achieves%20state-of-the-art%20perceptual%20quality%20and%0Aimproved%20efficiency.%20Our%20method%20combines%20two%20key%20ideas%3A%20%281%29%20joint%20depth-based%0Aand%20object-based%20supervision%20to%20ensure%20inpainted%20Gaussians%20are%20accurately%0Aplaced%20in%203D%20space%20and%20aligned%20with%20surrounding%20geometry%2C%20and%20%282%29%20we%20propose%20a%0Aconsistency-aware%20refinement%20scheme%20that%20selectively%20identifies%20and%20corrects%0Ainconsistent%20regions%20without%20disrupting%20the%20rest%20of%20the%20scene.%20Evaluations%20on%0Athe%20SPIn-NeRF%20dataset%20demonstrate%20that%20SplatFill%20not%20only%20surpasses%20existing%0ANeRF-based%20and%203DGS-based%20inpainting%20methods%20in%20visual%20fidelity%20but%20also%0Areduces%20training%20time%20by%2024.5%25.%20Qualitative%20results%20show%20our%20method%20delivers%0Asharper%20details%2C%20fewer%20artifacts%2C%20and%20greater%20coherence%20across%20challenging%0Aviewpoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplatFill%253A%25203D%2520Scene%2520Inpainting%2520via%2520Depth-Guided%2520Gaussian%2520Splatting%26entry.906535625%3DMahtab%2520Dahaghin%2520and%2520Milind%2520G.%2520Padalkar%2520and%2520Matteo%2520Toso%2520and%2520Alessio%2520Del%2520Bue%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520enabled%2520the%2520creation%2520of%2520highly%2520realistic%25203D%250Ascene%2520representations%2520from%2520sets%2520of%2520multi-view%2520images.%2520However%252C%2520inpainting%250Amissing%2520regions%252C%2520whether%2520due%2520to%2520occlusion%2520or%2520scene%2520editing%252C%2520remains%2520a%250Achallenging%2520task%252C%2520often%2520leading%2520to%2520blurry%2520details%252C%2520artifacts%252C%2520and%2520inconsistent%250Ageometry.%2520In%2520this%2520work%252C%2520we%2520introduce%2520SplatFill%252C%2520a%2520novel%2520depth-guided%2520approach%250Afor%25203DGS%2520scene%2520inpainting%2520that%2520achieves%2520state-of-the-art%2520perceptual%2520quality%2520and%250Aimproved%2520efficiency.%2520Our%2520method%2520combines%2520two%2520key%2520ideas%253A%2520%25281%2529%2520joint%2520depth-based%250Aand%2520object-based%2520supervision%2520to%2520ensure%2520inpainted%2520Gaussians%2520are%2520accurately%250Aplaced%2520in%25203D%2520space%2520and%2520aligned%2520with%2520surrounding%2520geometry%252C%2520and%2520%25282%2529%2520we%2520propose%2520a%250Aconsistency-aware%2520refinement%2520scheme%2520that%2520selectively%2520identifies%2520and%2520corrects%250Ainconsistent%2520regions%2520without%2520disrupting%2520the%2520rest%2520of%2520the%2520scene.%2520Evaluations%2520on%250Athe%2520SPIn-NeRF%2520dataset%2520demonstrate%2520that%2520SplatFill%2520not%2520only%2520surpasses%2520existing%250ANeRF-based%2520and%25203DGS-based%2520inpainting%2520methods%2520in%2520visual%2520fidelity%2520but%2520also%250Areduces%2520training%2520time%2520by%252024.5%2525.%2520Qualitative%2520results%2520show%2520our%2520method%2520delivers%250Asharper%2520details%252C%2520fewer%2520artifacts%252C%2520and%2520greater%2520coherence%2520across%2520challenging%250Aviewpoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SplatFill%3A%203D%20Scene%20Inpainting%20via%20Depth-Guided%20Gaussian%20Splatting&entry.906535625=Mahtab%20Dahaghin%20and%20Milind%20G.%20Padalkar%20and%20Matteo%20Toso%20and%20Alessio%20Del%20Bue&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20enabled%20the%20creation%20of%20highly%20realistic%203D%0Ascene%20representations%20from%20sets%20of%20multi-view%20images.%20However%2C%20inpainting%0Amissing%20regions%2C%20whether%20due%20to%20occlusion%20or%20scene%20editing%2C%20remains%20a%0Achallenging%20task%2C%20often%20leading%20to%20blurry%20details%2C%20artifacts%2C%20and%20inconsistent%0Ageometry.%20In%20this%20work%2C%20we%20introduce%20SplatFill%2C%20a%20novel%20depth-guided%20approach%0Afor%203DGS%20scene%20inpainting%20that%20achieves%20state-of-the-art%20perceptual%20quality%20and%0Aimproved%20efficiency.%20Our%20method%20combines%20two%20key%20ideas%3A%20%281%29%20joint%20depth-based%0Aand%20object-based%20supervision%20to%20ensure%20inpainted%20Gaussians%20are%20accurately%0Aplaced%20in%203D%20space%20and%20aligned%20with%20surrounding%20geometry%2C%20and%20%282%29%20we%20propose%20a%0Aconsistency-aware%20refinement%20scheme%20that%20selectively%20identifies%20and%20corrects%0Ainconsistent%20regions%20without%20disrupting%20the%20rest%20of%20the%20scene.%20Evaluations%20on%0Athe%20SPIn-NeRF%20dataset%20demonstrate%20that%20SplatFill%20not%20only%20surpasses%20existing%0ANeRF-based%20and%203DGS-based%20inpainting%20methods%20in%20visual%20fidelity%20but%20also%0Areduces%20training%20time%20by%2024.5%25.%20Qualitative%20results%20show%20our%20method%20delivers%0Asharper%20details%2C%20fewer%20artifacts%2C%20and%20greater%20coherence%20across%20challenging%0Aviewpoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07809v1&entry.124074799=Read"},
{"title": "HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting", "author": "Yimin Pan and Matthias Nie\u00dfner and Tobias Kirschstein", "abstract": "  Human hair reconstruction is a challenging problem in computer vision, with\ngrowing importance for applications in virtual reality and digital human\nmodeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient\nand explicit scene representations that naturally align with the structure of\nhair strands. In this work, we extend the 3DGS framework to enable strand-level\nhair geometry reconstruction from multi-view images. Our multi-stage pipeline\nfirst reconstructs detailed hair geometry using a differentiable Gaussian\nrasterizer, then merges individual Gaussian segments into coherent strands\nthrough a novel merging scheme, and finally refines and grows the strands under\nphotometric supervision.\n  While existing methods typically evaluate reconstruction quality at the\ngeometric level, they often neglect the connectivity and topology of hair\nstrands. To address this, we propose a new evaluation metric that serves as a\nproxy for assessing topological accuracy in strand reconstruction. Extensive\nexperiments on both synthetic and real-world datasets demonstrate that our\nmethod robustly handles a wide range of hairstyles and achieves efficient\nreconstruction, typically completing within one hour.\n  The project page can be found at: https://yimin-pan.github.io/hair-gs/\n", "link": "http://arxiv.org/abs/2509.07774v1", "date": "2025-09-09", "relevancy": 3.2642, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6935}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.655}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HairGS%3A%20Hair%20Strand%20Reconstruction%20based%20on%203D%20Gaussian%20Splatting&body=Title%3A%20HairGS%3A%20Hair%20Strand%20Reconstruction%20based%20on%203D%20Gaussian%20Splatting%0AAuthor%3A%20Yimin%20Pan%20and%20Matthias%20Nie%C3%9Fner%20and%20Tobias%20Kirschstein%0AAbstract%3A%20%20%20Human%20hair%20reconstruction%20is%20a%20challenging%20problem%20in%20computer%20vision%2C%20with%0Agrowing%20importance%20for%20applications%20in%20virtual%20reality%20and%20digital%20human%0Amodeling.%20Recent%20advances%20in%203D%20Gaussians%20Splatting%20%283DGS%29%20provide%20efficient%0Aand%20explicit%20scene%20representations%20that%20naturally%20align%20with%20the%20structure%20of%0Ahair%20strands.%20In%20this%20work%2C%20we%20extend%20the%203DGS%20framework%20to%20enable%20strand-level%0Ahair%20geometry%20reconstruction%20from%20multi-view%20images.%20Our%20multi-stage%20pipeline%0Afirst%20reconstructs%20detailed%20hair%20geometry%20using%20a%20differentiable%20Gaussian%0Arasterizer%2C%20then%20merges%20individual%20Gaussian%20segments%20into%20coherent%20strands%0Athrough%20a%20novel%20merging%20scheme%2C%20and%20finally%20refines%20and%20grows%20the%20strands%20under%0Aphotometric%20supervision.%0A%20%20While%20existing%20methods%20typically%20evaluate%20reconstruction%20quality%20at%20the%0Ageometric%20level%2C%20they%20often%20neglect%20the%20connectivity%20and%20topology%20of%20hair%0Astrands.%20To%20address%20this%2C%20we%20propose%20a%20new%20evaluation%20metric%20that%20serves%20as%20a%0Aproxy%20for%20assessing%20topological%20accuracy%20in%20strand%20reconstruction.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20our%0Amethod%20robustly%20handles%20a%20wide%20range%20of%20hairstyles%20and%20achieves%20efficient%0Areconstruction%2C%20typically%20completing%20within%20one%20hour.%0A%20%20The%20project%20page%20can%20be%20found%20at%3A%20https%3A//yimin-pan.github.io/hair-gs/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHairGS%253A%2520Hair%2520Strand%2520Reconstruction%2520based%2520on%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DYimin%2520Pan%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Tobias%2520Kirschstein%26entry.1292438233%3D%2520%2520Human%2520hair%2520reconstruction%2520is%2520a%2520challenging%2520problem%2520in%2520computer%2520vision%252C%2520with%250Agrowing%2520importance%2520for%2520applications%2520in%2520virtual%2520reality%2520and%2520digital%2520human%250Amodeling.%2520Recent%2520advances%2520in%25203D%2520Gaussians%2520Splatting%2520%25283DGS%2529%2520provide%2520efficient%250Aand%2520explicit%2520scene%2520representations%2520that%2520naturally%2520align%2520with%2520the%2520structure%2520of%250Ahair%2520strands.%2520In%2520this%2520work%252C%2520we%2520extend%2520the%25203DGS%2520framework%2520to%2520enable%2520strand-level%250Ahair%2520geometry%2520reconstruction%2520from%2520multi-view%2520images.%2520Our%2520multi-stage%2520pipeline%250Afirst%2520reconstructs%2520detailed%2520hair%2520geometry%2520using%2520a%2520differentiable%2520Gaussian%250Arasterizer%252C%2520then%2520merges%2520individual%2520Gaussian%2520segments%2520into%2520coherent%2520strands%250Athrough%2520a%2520novel%2520merging%2520scheme%252C%2520and%2520finally%2520refines%2520and%2520grows%2520the%2520strands%2520under%250Aphotometric%2520supervision.%250A%2520%2520While%2520existing%2520methods%2520typically%2520evaluate%2520reconstruction%2520quality%2520at%2520the%250Ageometric%2520level%252C%2520they%2520often%2520neglect%2520the%2520connectivity%2520and%2520topology%2520of%2520hair%250Astrands.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520new%2520evaluation%2520metric%2520that%2520serves%2520as%2520a%250Aproxy%2520for%2520assessing%2520topological%2520accuracy%2520in%2520strand%2520reconstruction.%2520Extensive%250Aexperiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520our%250Amethod%2520robustly%2520handles%2520a%2520wide%2520range%2520of%2520hairstyles%2520and%2520achieves%2520efficient%250Areconstruction%252C%2520typically%2520completing%2520within%2520one%2520hour.%250A%2520%2520The%2520project%2520page%2520can%2520be%2520found%2520at%253A%2520https%253A//yimin-pan.github.io/hair-gs/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HairGS%3A%20Hair%20Strand%20Reconstruction%20based%20on%203D%20Gaussian%20Splatting&entry.906535625=Yimin%20Pan%20and%20Matthias%20Nie%C3%9Fner%20and%20Tobias%20Kirschstein&entry.1292438233=%20%20Human%20hair%20reconstruction%20is%20a%20challenging%20problem%20in%20computer%20vision%2C%20with%0Agrowing%20importance%20for%20applications%20in%20virtual%20reality%20and%20digital%20human%0Amodeling.%20Recent%20advances%20in%203D%20Gaussians%20Splatting%20%283DGS%29%20provide%20efficient%0Aand%20explicit%20scene%20representations%20that%20naturally%20align%20with%20the%20structure%20of%0Ahair%20strands.%20In%20this%20work%2C%20we%20extend%20the%203DGS%20framework%20to%20enable%20strand-level%0Ahair%20geometry%20reconstruction%20from%20multi-view%20images.%20Our%20multi-stage%20pipeline%0Afirst%20reconstructs%20detailed%20hair%20geometry%20using%20a%20differentiable%20Gaussian%0Arasterizer%2C%20then%20merges%20individual%20Gaussian%20segments%20into%20coherent%20strands%0Athrough%20a%20novel%20merging%20scheme%2C%20and%20finally%20refines%20and%20grows%20the%20strands%20under%0Aphotometric%20supervision.%0A%20%20While%20existing%20methods%20typically%20evaluate%20reconstruction%20quality%20at%20the%0Ageometric%20level%2C%20they%20often%20neglect%20the%20connectivity%20and%20topology%20of%20hair%0Astrands.%20To%20address%20this%2C%20we%20propose%20a%20new%20evaluation%20metric%20that%20serves%20as%20a%0Aproxy%20for%20assessing%20topological%20accuracy%20in%20strand%20reconstruction.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20our%0Amethod%20robustly%20handles%20a%20wide%20range%20of%20hairstyles%20and%20achieves%20efficient%0Areconstruction%2C%20typically%20completing%20within%20one%20hour.%0A%20%20The%20project%20page%20can%20be%20found%20at%3A%20https%3A//yimin-pan.github.io/hair-gs/%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07774v1&entry.124074799=Read"},
{"title": "One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain\n  Randomization for One-Shot 6D Pose Estimation", "author": "Zheng Geng and Nan Wang and Shaocong Xu and Chongjie Ye and Bohan Li and Zhaoxi Chen and Sida Peng and Hao Zhao", "abstract": "  Estimating the 6D pose of arbitrary unseen objects from a single reference\nimage is critical for robotics operating in the long-tail of real-world\ninstances. However, this setting is notoriously challenging: 3D models are\nrarely available, single-view reconstructions lack metric scale, and domain\ngaps between generated models and real-world images undermine robustness. We\npropose OnePoseViaGen, a pipeline that tackles these challenges through two key\ncomponents. First, a coarse-to-fine alignment module jointly refines scale and\npose by combining multi-view feature matching with render-and-compare\nrefinement. Second, a text-guided generative domain randomization strategy\ndiversifies textures, enabling effective fine-tuning of pose estimators with\nsynthetic data. Together, these steps allow high-fidelity single-view 3D\ngeneration to support reliable one-shot 6D pose estimation. On challenging\nbenchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves\nstate-of-the-art performance far surpassing prior approaches. We further\ndemonstrate robust dexterous grasping with a real robot hand, validating the\npracticality of our method in real-world manipulation. Project page:\nhttps://gzwsama.github.io/OnePoseviaGen.github.io/\n", "link": "http://arxiv.org/abs/2509.07978v1", "date": "2025-09-09", "relevancy": 3.207, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6555}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6427}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20View%2C%20Many%20Worlds%3A%20Single-Image%20to%203D%20Object%20Meets%20Generative%20Domain%0A%20%20Randomization%20for%20One-Shot%206D%20Pose%20Estimation&body=Title%3A%20One%20View%2C%20Many%20Worlds%3A%20Single-Image%20to%203D%20Object%20Meets%20Generative%20Domain%0A%20%20Randomization%20for%20One-Shot%206D%20Pose%20Estimation%0AAuthor%3A%20Zheng%20Geng%20and%20Nan%20Wang%20and%20Shaocong%20Xu%20and%20Chongjie%20Ye%20and%20Bohan%20Li%20and%20Zhaoxi%20Chen%20and%20Sida%20Peng%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Estimating%20the%206D%20pose%20of%20arbitrary%20unseen%20objects%20from%20a%20single%20reference%0Aimage%20is%20critical%20for%20robotics%20operating%20in%20the%20long-tail%20of%20real-world%0Ainstances.%20However%2C%20this%20setting%20is%20notoriously%20challenging%3A%203D%20models%20are%0Ararely%20available%2C%20single-view%20reconstructions%20lack%20metric%20scale%2C%20and%20domain%0Agaps%20between%20generated%20models%20and%20real-world%20images%20undermine%20robustness.%20We%0Apropose%20OnePoseViaGen%2C%20a%20pipeline%20that%20tackles%20these%20challenges%20through%20two%20key%0Acomponents.%20First%2C%20a%20coarse-to-fine%20alignment%20module%20jointly%20refines%20scale%20and%0Apose%20by%20combining%20multi-view%20feature%20matching%20with%20render-and-compare%0Arefinement.%20Second%2C%20a%20text-guided%20generative%20domain%20randomization%20strategy%0Adiversifies%20textures%2C%20enabling%20effective%20fine-tuning%20of%20pose%20estimators%20with%0Asynthetic%20data.%20Together%2C%20these%20steps%20allow%20high-fidelity%20single-view%203D%0Ageneration%20to%20support%20reliable%20one-shot%206D%20pose%20estimation.%20On%20challenging%0Abenchmarks%20%28YCBInEOAT%2C%20Toyota-Light%2C%20LM-O%29%2C%20OnePoseViaGen%20achieves%0Astate-of-the-art%20performance%20far%20surpassing%20prior%20approaches.%20We%20further%0Ademonstrate%20robust%20dexterous%20grasping%20with%20a%20real%20robot%20hand%2C%20validating%20the%0Apracticality%20of%20our%20method%20in%20real-world%20manipulation.%20Project%20page%3A%0Ahttps%3A//gzwsama.github.io/OnePoseviaGen.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520View%252C%2520Many%2520Worlds%253A%2520Single-Image%2520to%25203D%2520Object%2520Meets%2520Generative%2520Domain%250A%2520%2520Randomization%2520for%2520One-Shot%25206D%2520Pose%2520Estimation%26entry.906535625%3DZheng%2520Geng%2520and%2520Nan%2520Wang%2520and%2520Shaocong%2520Xu%2520and%2520Chongjie%2520Ye%2520and%2520Bohan%2520Li%2520and%2520Zhaoxi%2520Chen%2520and%2520Sida%2520Peng%2520and%2520Hao%2520Zhao%26entry.1292438233%3D%2520%2520Estimating%2520the%25206D%2520pose%2520of%2520arbitrary%2520unseen%2520objects%2520from%2520a%2520single%2520reference%250Aimage%2520is%2520critical%2520for%2520robotics%2520operating%2520in%2520the%2520long-tail%2520of%2520real-world%250Ainstances.%2520However%252C%2520this%2520setting%2520is%2520notoriously%2520challenging%253A%25203D%2520models%2520are%250Ararely%2520available%252C%2520single-view%2520reconstructions%2520lack%2520metric%2520scale%252C%2520and%2520domain%250Agaps%2520between%2520generated%2520models%2520and%2520real-world%2520images%2520undermine%2520robustness.%2520We%250Apropose%2520OnePoseViaGen%252C%2520a%2520pipeline%2520that%2520tackles%2520these%2520challenges%2520through%2520two%2520key%250Acomponents.%2520First%252C%2520a%2520coarse-to-fine%2520alignment%2520module%2520jointly%2520refines%2520scale%2520and%250Apose%2520by%2520combining%2520multi-view%2520feature%2520matching%2520with%2520render-and-compare%250Arefinement.%2520Second%252C%2520a%2520text-guided%2520generative%2520domain%2520randomization%2520strategy%250Adiversifies%2520textures%252C%2520enabling%2520effective%2520fine-tuning%2520of%2520pose%2520estimators%2520with%250Asynthetic%2520data.%2520Together%252C%2520these%2520steps%2520allow%2520high-fidelity%2520single-view%25203D%250Ageneration%2520to%2520support%2520reliable%2520one-shot%25206D%2520pose%2520estimation.%2520On%2520challenging%250Abenchmarks%2520%2528YCBInEOAT%252C%2520Toyota-Light%252C%2520LM-O%2529%252C%2520OnePoseViaGen%2520achieves%250Astate-of-the-art%2520performance%2520far%2520surpassing%2520prior%2520approaches.%2520We%2520further%250Ademonstrate%2520robust%2520dexterous%2520grasping%2520with%2520a%2520real%2520robot%2520hand%252C%2520validating%2520the%250Apracticality%2520of%2520our%2520method%2520in%2520real-world%2520manipulation.%2520Project%2520page%253A%250Ahttps%253A//gzwsama.github.io/OnePoseviaGen.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20View%2C%20Many%20Worlds%3A%20Single-Image%20to%203D%20Object%20Meets%20Generative%20Domain%0A%20%20Randomization%20for%20One-Shot%206D%20Pose%20Estimation&entry.906535625=Zheng%20Geng%20and%20Nan%20Wang%20and%20Shaocong%20Xu%20and%20Chongjie%20Ye%20and%20Bohan%20Li%20and%20Zhaoxi%20Chen%20and%20Sida%20Peng%20and%20Hao%20Zhao&entry.1292438233=%20%20Estimating%20the%206D%20pose%20of%20arbitrary%20unseen%20objects%20from%20a%20single%20reference%0Aimage%20is%20critical%20for%20robotics%20operating%20in%20the%20long-tail%20of%20real-world%0Ainstances.%20However%2C%20this%20setting%20is%20notoriously%20challenging%3A%203D%20models%20are%0Ararely%20available%2C%20single-view%20reconstructions%20lack%20metric%20scale%2C%20and%20domain%0Agaps%20between%20generated%20models%20and%20real-world%20images%20undermine%20robustness.%20We%0Apropose%20OnePoseViaGen%2C%20a%20pipeline%20that%20tackles%20these%20challenges%20through%20two%20key%0Acomponents.%20First%2C%20a%20coarse-to-fine%20alignment%20module%20jointly%20refines%20scale%20and%0Apose%20by%20combining%20multi-view%20feature%20matching%20with%20render-and-compare%0Arefinement.%20Second%2C%20a%20text-guided%20generative%20domain%20randomization%20strategy%0Adiversifies%20textures%2C%20enabling%20effective%20fine-tuning%20of%20pose%20estimators%20with%0Asynthetic%20data.%20Together%2C%20these%20steps%20allow%20high-fidelity%20single-view%203D%0Ageneration%20to%20support%20reliable%20one-shot%206D%20pose%20estimation.%20On%20challenging%0Abenchmarks%20%28YCBInEOAT%2C%20Toyota-Light%2C%20LM-O%29%2C%20OnePoseViaGen%20achieves%0Astate-of-the-art%20performance%20far%20surpassing%20prior%20approaches.%20We%20further%0Ademonstrate%20robust%20dexterous%20grasping%20with%20a%20real%20robot%20hand%2C%20validating%20the%0Apracticality%20of%20our%20method%20in%20real-world%20manipulation.%20Project%20page%3A%0Ahttps%3A//gzwsama.github.io/OnePoseviaGen.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07978v1&entry.124074799=Read"},
{"title": "RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and\n  High-Quality Novel View Synthesis", "author": "Hugo Blanc and Jean-Emmanuel Deschaud and Alexis Paljic", "abstract": "  RayGauss has achieved state-of-the-art rendering quality for novel-view\nsynthesis on synthetic and indoor scenes by representing radiance and density\nfields with irregularly distributed elliptical basis functions, rendered via\nvolume ray casting using a Bounding Volume Hierarchy (BVH). However, its\ncomputational cost prevents real-time rendering on real-world scenes. Our\napproach, RayGaussX, builds on RayGauss by introducing key contributions that\naccelerate both training and inference. Specifically, we incorporate volumetric\nrendering acceleration strategies such as empty-space skipping and adaptive\nsampling, enhance ray coherence, and introduce scale regularization to reduce\nfalse-positive intersections. Additionally, we propose a new densification\ncriterion that improves density distribution in distant regions, leading to\nenhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5x\nto 12x faster training and 50x to 80x higher rendering speeds (FPS) on\nreal-world datasets while improving visual quality by up to +0.56 dB in PSNR.\nProject page with videos and code: https://raygaussx.github.io/.\n", "link": "http://arxiv.org/abs/2509.07782v1", "date": "2025-09-09", "relevancy": 3.1904, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.673}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.669}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RayGaussX%3A%20Accelerating%20Gaussian-Based%20Ray%20Marching%20for%20Real-Time%20and%0A%20%20High-Quality%20Novel%20View%20Synthesis&body=Title%3A%20RayGaussX%3A%20Accelerating%20Gaussian-Based%20Ray%20Marching%20for%20Real-Time%20and%0A%20%20High-Quality%20Novel%20View%20Synthesis%0AAuthor%3A%20Hugo%20Blanc%20and%20Jean-Emmanuel%20Deschaud%20and%20Alexis%20Paljic%0AAbstract%3A%20%20%20RayGauss%20has%20achieved%20state-of-the-art%20rendering%20quality%20for%20novel-view%0Asynthesis%20on%20synthetic%20and%20indoor%20scenes%20by%20representing%20radiance%20and%20density%0Afields%20with%20irregularly%20distributed%20elliptical%20basis%20functions%2C%20rendered%20via%0Avolume%20ray%20casting%20using%20a%20Bounding%20Volume%20Hierarchy%20%28BVH%29.%20However%2C%20its%0Acomputational%20cost%20prevents%20real-time%20rendering%20on%20real-world%20scenes.%20Our%0Aapproach%2C%20RayGaussX%2C%20builds%20on%20RayGauss%20by%20introducing%20key%20contributions%20that%0Aaccelerate%20both%20training%20and%20inference.%20Specifically%2C%20we%20incorporate%20volumetric%0Arendering%20acceleration%20strategies%20such%20as%20empty-space%20skipping%20and%20adaptive%0Asampling%2C%20enhance%20ray%20coherence%2C%20and%20introduce%20scale%20regularization%20to%20reduce%0Afalse-positive%20intersections.%20Additionally%2C%20we%20propose%20a%20new%20densification%0Acriterion%20that%20improves%20density%20distribution%20in%20distant%20regions%2C%20leading%20to%0Aenhanced%20graphical%20quality%20on%20larger%20scenes.%20As%20a%20result%2C%20RayGaussX%20achieves%205x%0Ato%2012x%20faster%20training%20and%2050x%20to%2080x%20higher%20rendering%20speeds%20%28FPS%29%20on%0Areal-world%20datasets%20while%20improving%20visual%20quality%20by%20up%20to%20%2B0.56%20dB%20in%20PSNR.%0AProject%20page%20with%20videos%20and%20code%3A%20https%3A//raygaussx.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRayGaussX%253A%2520Accelerating%2520Gaussian-Based%2520Ray%2520Marching%2520for%2520Real-Time%2520and%250A%2520%2520High-Quality%2520Novel%2520View%2520Synthesis%26entry.906535625%3DHugo%2520Blanc%2520and%2520Jean-Emmanuel%2520Deschaud%2520and%2520Alexis%2520Paljic%26entry.1292438233%3D%2520%2520RayGauss%2520has%2520achieved%2520state-of-the-art%2520rendering%2520quality%2520for%2520novel-view%250Asynthesis%2520on%2520synthetic%2520and%2520indoor%2520scenes%2520by%2520representing%2520radiance%2520and%2520density%250Afields%2520with%2520irregularly%2520distributed%2520elliptical%2520basis%2520functions%252C%2520rendered%2520via%250Avolume%2520ray%2520casting%2520using%2520a%2520Bounding%2520Volume%2520Hierarchy%2520%2528BVH%2529.%2520However%252C%2520its%250Acomputational%2520cost%2520prevents%2520real-time%2520rendering%2520on%2520real-world%2520scenes.%2520Our%250Aapproach%252C%2520RayGaussX%252C%2520builds%2520on%2520RayGauss%2520by%2520introducing%2520key%2520contributions%2520that%250Aaccelerate%2520both%2520training%2520and%2520inference.%2520Specifically%252C%2520we%2520incorporate%2520volumetric%250Arendering%2520acceleration%2520strategies%2520such%2520as%2520empty-space%2520skipping%2520and%2520adaptive%250Asampling%252C%2520enhance%2520ray%2520coherence%252C%2520and%2520introduce%2520scale%2520regularization%2520to%2520reduce%250Afalse-positive%2520intersections.%2520Additionally%252C%2520we%2520propose%2520a%2520new%2520densification%250Acriterion%2520that%2520improves%2520density%2520distribution%2520in%2520distant%2520regions%252C%2520leading%2520to%250Aenhanced%2520graphical%2520quality%2520on%2520larger%2520scenes.%2520As%2520a%2520result%252C%2520RayGaussX%2520achieves%25205x%250Ato%252012x%2520faster%2520training%2520and%252050x%2520to%252080x%2520higher%2520rendering%2520speeds%2520%2528FPS%2529%2520on%250Areal-world%2520datasets%2520while%2520improving%2520visual%2520quality%2520by%2520up%2520to%2520%252B0.56%2520dB%2520in%2520PSNR.%250AProject%2520page%2520with%2520videos%2520and%2520code%253A%2520https%253A//raygaussx.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RayGaussX%3A%20Accelerating%20Gaussian-Based%20Ray%20Marching%20for%20Real-Time%20and%0A%20%20High-Quality%20Novel%20View%20Synthesis&entry.906535625=Hugo%20Blanc%20and%20Jean-Emmanuel%20Deschaud%20and%20Alexis%20Paljic&entry.1292438233=%20%20RayGauss%20has%20achieved%20state-of-the-art%20rendering%20quality%20for%20novel-view%0Asynthesis%20on%20synthetic%20and%20indoor%20scenes%20by%20representing%20radiance%20and%20density%0Afields%20with%20irregularly%20distributed%20elliptical%20basis%20functions%2C%20rendered%20via%0Avolume%20ray%20casting%20using%20a%20Bounding%20Volume%20Hierarchy%20%28BVH%29.%20However%2C%20its%0Acomputational%20cost%20prevents%20real-time%20rendering%20on%20real-world%20scenes.%20Our%0Aapproach%2C%20RayGaussX%2C%20builds%20on%20RayGauss%20by%20introducing%20key%20contributions%20that%0Aaccelerate%20both%20training%20and%20inference.%20Specifically%2C%20we%20incorporate%20volumetric%0Arendering%20acceleration%20strategies%20such%20as%20empty-space%20skipping%20and%20adaptive%0Asampling%2C%20enhance%20ray%20coherence%2C%20and%20introduce%20scale%20regularization%20to%20reduce%0Afalse-positive%20intersections.%20Additionally%2C%20we%20propose%20a%20new%20densification%0Acriterion%20that%20improves%20density%20distribution%20in%20distant%20regions%2C%20leading%20to%0Aenhanced%20graphical%20quality%20on%20larger%20scenes.%20As%20a%20result%2C%20RayGaussX%20achieves%205x%0Ato%2012x%20faster%20training%20and%2050x%20to%2080x%20higher%20rendering%20speeds%20%28FPS%29%20on%0Areal-world%20datasets%20while%20improving%20visual%20quality%20by%20up%20to%20%2B0.56%20dB%20in%20PSNR.%0AProject%20page%20with%20videos%20and%20code%3A%20https%3A//raygaussx.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07782v1&entry.124074799=Read"},
{"title": "Point Linguist Model: Segment Any Object via Bridged Large 3D-Language\n  Model", "author": "Zhuoxu Huang and Mingqi Gao and Jungong Han", "abstract": "  3D object segmentation with Large Language Models (LLMs) has become a\nprevailing paradigm due to its broad semantics, task flexibility, and strong\ngeneralization. However, this paradigm is hindered by representation\nmisalignment: LLMs process high-level semantic tokens, whereas 3D point clouds\nconvey only dense geometric structures. In prior methods, misalignment limits\nboth input and output. At the input stage, dense point patches require heavy\npre-alignment, weakening object-level semantics and confusing similar\ndistractors. At the output stage, predictions depend only on dense features\nwithout explicit geometric cues, leading to a loss of fine-grained accuracy. To\naddress these limitations, we present the Point Linguist Model (PLM), a general\nframework that bridges the representation gap between LLMs and dense 3D point\nclouds without requiring large-scale pre-alignment between 3D-text or\n3D-images. Specifically, we introduce Object-centric Discriminative\nRepresentation (OcDR), which learns object-centric tokens that capture target\nsemantics and scene relations under a hard negative-aware training objective.\nThis mitigates the misalignment between LLM tokens and 3D points, enhances\nresilience to distractors, and facilitates semantic-level reasoning within\nLLMs. For accurate segmentation, we introduce the Geometric Reactivation\nDecoder (GRD), which predicts masks by combining OcDR tokens carrying\nLLM-inferred geometry with corresponding dense features, preserving\ncomprehensive dense features throughout the pipeline. Extensive experiments\nshow that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and\n+6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gains\nacross 7 benchmarks spanning 4 different tasks, demonstrating the effectiveness\nof comprehensive object-centric reasoning for robust 3D understanding.\n", "link": "http://arxiv.org/abs/2509.07825v1", "date": "2025-09-09", "relevancy": 3.1312, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.63}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.63}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point%20Linguist%20Model%3A%20Segment%20Any%20Object%20via%20Bridged%20Large%203D-Language%0A%20%20Model&body=Title%3A%20Point%20Linguist%20Model%3A%20Segment%20Any%20Object%20via%20Bridged%20Large%203D-Language%0A%20%20Model%0AAuthor%3A%20Zhuoxu%20Huang%20and%20Mingqi%20Gao%20and%20Jungong%20Han%0AAbstract%3A%20%20%203D%20object%20segmentation%20with%20Large%20Language%20Models%20%28LLMs%29%20has%20become%20a%0Aprevailing%20paradigm%20due%20to%20its%20broad%20semantics%2C%20task%20flexibility%2C%20and%20strong%0Ageneralization.%20However%2C%20this%20paradigm%20is%20hindered%20by%20representation%0Amisalignment%3A%20LLMs%20process%20high-level%20semantic%20tokens%2C%20whereas%203D%20point%20clouds%0Aconvey%20only%20dense%20geometric%20structures.%20In%20prior%20methods%2C%20misalignment%20limits%0Aboth%20input%20and%20output.%20At%20the%20input%20stage%2C%20dense%20point%20patches%20require%20heavy%0Apre-alignment%2C%20weakening%20object-level%20semantics%20and%20confusing%20similar%0Adistractors.%20At%20the%20output%20stage%2C%20predictions%20depend%20only%20on%20dense%20features%0Awithout%20explicit%20geometric%20cues%2C%20leading%20to%20a%20loss%20of%20fine-grained%20accuracy.%20To%0Aaddress%20these%20limitations%2C%20we%20present%20the%20Point%20Linguist%20Model%20%28PLM%29%2C%20a%20general%0Aframework%20that%20bridges%20the%20representation%20gap%20between%20LLMs%20and%20dense%203D%20point%0Aclouds%20without%20requiring%20large-scale%20pre-alignment%20between%203D-text%20or%0A3D-images.%20Specifically%2C%20we%20introduce%20Object-centric%20Discriminative%0ARepresentation%20%28OcDR%29%2C%20which%20learns%20object-centric%20tokens%20that%20capture%20target%0Asemantics%20and%20scene%20relations%20under%20a%20hard%20negative-aware%20training%20objective.%0AThis%20mitigates%20the%20misalignment%20between%20LLM%20tokens%20and%203D%20points%2C%20enhances%0Aresilience%20to%20distractors%2C%20and%20facilitates%20semantic-level%20reasoning%20within%0ALLMs.%20For%20accurate%20segmentation%2C%20we%20introduce%20the%20Geometric%20Reactivation%0ADecoder%20%28GRD%29%2C%20which%20predicts%20masks%20by%20combining%20OcDR%20tokens%20carrying%0ALLM-inferred%20geometry%20with%20corresponding%20dense%20features%2C%20preserving%0Acomprehensive%20dense%20features%20throughout%20the%20pipeline.%20Extensive%20experiments%0Ashow%20that%20PLM%20achieves%20significant%20improvements%20of%20%2B7.3%20mIoU%20on%20ScanNetv2%20and%0A%2B6.0%20mIoU%20on%20Multi3DRefer%20for%203D%20referring%20segmentation%2C%20with%20consistent%20gains%0Aacross%207%20benchmarks%20spanning%204%20different%20tasks%2C%20demonstrating%20the%20effectiveness%0Aof%20comprehensive%20object-centric%20reasoning%20for%20robust%203D%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint%2520Linguist%2520Model%253A%2520Segment%2520Any%2520Object%2520via%2520Bridged%2520Large%25203D-Language%250A%2520%2520Model%26entry.906535625%3DZhuoxu%2520Huang%2520and%2520Mingqi%2520Gao%2520and%2520Jungong%2520Han%26entry.1292438233%3D%2520%25203D%2520object%2520segmentation%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520become%2520a%250Aprevailing%2520paradigm%2520due%2520to%2520its%2520broad%2520semantics%252C%2520task%2520flexibility%252C%2520and%2520strong%250Ageneralization.%2520However%252C%2520this%2520paradigm%2520is%2520hindered%2520by%2520representation%250Amisalignment%253A%2520LLMs%2520process%2520high-level%2520semantic%2520tokens%252C%2520whereas%25203D%2520point%2520clouds%250Aconvey%2520only%2520dense%2520geometric%2520structures.%2520In%2520prior%2520methods%252C%2520misalignment%2520limits%250Aboth%2520input%2520and%2520output.%2520At%2520the%2520input%2520stage%252C%2520dense%2520point%2520patches%2520require%2520heavy%250Apre-alignment%252C%2520weakening%2520object-level%2520semantics%2520and%2520confusing%2520similar%250Adistractors.%2520At%2520the%2520output%2520stage%252C%2520predictions%2520depend%2520only%2520on%2520dense%2520features%250Awithout%2520explicit%2520geometric%2520cues%252C%2520leading%2520to%2520a%2520loss%2520of%2520fine-grained%2520accuracy.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520present%2520the%2520Point%2520Linguist%2520Model%2520%2528PLM%2529%252C%2520a%2520general%250Aframework%2520that%2520bridges%2520the%2520representation%2520gap%2520between%2520LLMs%2520and%2520dense%25203D%2520point%250Aclouds%2520without%2520requiring%2520large-scale%2520pre-alignment%2520between%25203D-text%2520or%250A3D-images.%2520Specifically%252C%2520we%2520introduce%2520Object-centric%2520Discriminative%250ARepresentation%2520%2528OcDR%2529%252C%2520which%2520learns%2520object-centric%2520tokens%2520that%2520capture%2520target%250Asemantics%2520and%2520scene%2520relations%2520under%2520a%2520hard%2520negative-aware%2520training%2520objective.%250AThis%2520mitigates%2520the%2520misalignment%2520between%2520LLM%2520tokens%2520and%25203D%2520points%252C%2520enhances%250Aresilience%2520to%2520distractors%252C%2520and%2520facilitates%2520semantic-level%2520reasoning%2520within%250ALLMs.%2520For%2520accurate%2520segmentation%252C%2520we%2520introduce%2520the%2520Geometric%2520Reactivation%250ADecoder%2520%2528GRD%2529%252C%2520which%2520predicts%2520masks%2520by%2520combining%2520OcDR%2520tokens%2520carrying%250ALLM-inferred%2520geometry%2520with%2520corresponding%2520dense%2520features%252C%2520preserving%250Acomprehensive%2520dense%2520features%2520throughout%2520the%2520pipeline.%2520Extensive%2520experiments%250Ashow%2520that%2520PLM%2520achieves%2520significant%2520improvements%2520of%2520%252B7.3%2520mIoU%2520on%2520ScanNetv2%2520and%250A%252B6.0%2520mIoU%2520on%2520Multi3DRefer%2520for%25203D%2520referring%2520segmentation%252C%2520with%2520consistent%2520gains%250Aacross%25207%2520benchmarks%2520spanning%25204%2520different%2520tasks%252C%2520demonstrating%2520the%2520effectiveness%250Aof%2520comprehensive%2520object-centric%2520reasoning%2520for%2520robust%25203D%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20Linguist%20Model%3A%20Segment%20Any%20Object%20via%20Bridged%20Large%203D-Language%0A%20%20Model&entry.906535625=Zhuoxu%20Huang%20and%20Mingqi%20Gao%20and%20Jungong%20Han&entry.1292438233=%20%203D%20object%20segmentation%20with%20Large%20Language%20Models%20%28LLMs%29%20has%20become%20a%0Aprevailing%20paradigm%20due%20to%20its%20broad%20semantics%2C%20task%20flexibility%2C%20and%20strong%0Ageneralization.%20However%2C%20this%20paradigm%20is%20hindered%20by%20representation%0Amisalignment%3A%20LLMs%20process%20high-level%20semantic%20tokens%2C%20whereas%203D%20point%20clouds%0Aconvey%20only%20dense%20geometric%20structures.%20In%20prior%20methods%2C%20misalignment%20limits%0Aboth%20input%20and%20output.%20At%20the%20input%20stage%2C%20dense%20point%20patches%20require%20heavy%0Apre-alignment%2C%20weakening%20object-level%20semantics%20and%20confusing%20similar%0Adistractors.%20At%20the%20output%20stage%2C%20predictions%20depend%20only%20on%20dense%20features%0Awithout%20explicit%20geometric%20cues%2C%20leading%20to%20a%20loss%20of%20fine-grained%20accuracy.%20To%0Aaddress%20these%20limitations%2C%20we%20present%20the%20Point%20Linguist%20Model%20%28PLM%29%2C%20a%20general%0Aframework%20that%20bridges%20the%20representation%20gap%20between%20LLMs%20and%20dense%203D%20point%0Aclouds%20without%20requiring%20large-scale%20pre-alignment%20between%203D-text%20or%0A3D-images.%20Specifically%2C%20we%20introduce%20Object-centric%20Discriminative%0ARepresentation%20%28OcDR%29%2C%20which%20learns%20object-centric%20tokens%20that%20capture%20target%0Asemantics%20and%20scene%20relations%20under%20a%20hard%20negative-aware%20training%20objective.%0AThis%20mitigates%20the%20misalignment%20between%20LLM%20tokens%20and%203D%20points%2C%20enhances%0Aresilience%20to%20distractors%2C%20and%20facilitates%20semantic-level%20reasoning%20within%0ALLMs.%20For%20accurate%20segmentation%2C%20we%20introduce%20the%20Geometric%20Reactivation%0ADecoder%20%28GRD%29%2C%20which%20predicts%20masks%20by%20combining%20OcDR%20tokens%20carrying%0ALLM-inferred%20geometry%20with%20corresponding%20dense%20features%2C%20preserving%0Acomprehensive%20dense%20features%20throughout%20the%20pipeline.%20Extensive%20experiments%0Ashow%20that%20PLM%20achieves%20significant%20improvements%20of%20%2B7.3%20mIoU%20on%20ScanNetv2%20and%0A%2B6.0%20mIoU%20on%20Multi3DRefer%20for%203D%20referring%20segmentation%2C%20with%20consistent%20gains%0Aacross%207%20benchmarks%20spanning%204%20different%20tasks%2C%20demonstrating%20the%20effectiveness%0Aof%20comprehensive%20object-centric%20reasoning%20for%20robust%203D%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07825v1&entry.124074799=Read"},
{"title": "BEAM: Bridging Physically-based Rendering and Gaussian Modeling for\n  Relightable Volumetric Video", "author": "Yu Hong and Yize Wu and Zhehao Shen and Chengcheng Guo and Yuheng Jiang and Yingliang Zhang and Jingyi Yu and Lan Xu", "abstract": "  Volumetric video enables immersive experiences by capturing dynamic 3D\nscenes, enabling diverse applications for virtual reality, education, and\ntelepresence. However, traditional methods struggle with fixed lighting\nconditions, while neural approaches face trade-offs in efficiency, quality, or\nadaptability for relightable scenarios. To address these limitations, we\npresent BEAM, a novel pipeline that bridges 4D Gaussian representations with\nphysically-based rendering (PBR) to produce high-quality, relightable\nvolumetric videos from multi-view RGB footage. BEAM recovers detailed geometry\nand PBR properties via a series of available Gaussian-based techniques. It\nfirst combines Gaussian-based human performance tracking with geometry-aware\nrasterization in a coarse-to-fine optimization framework to recover spatially\nand temporally consistent geometries. We further enhance Gaussian attributes by\nincorporating PBR properties step by step. We generate roughness via a\nmulti-view-conditioned diffusion model, and then derive AO and base color using\na 2D-to-3D strategy, incorporating a tailored Gaussian-based ray tracer for\nefficient visibility computation. Once recovered, these dynamic, relightable\nassets integrate seamlessly into traditional CG pipelines, supporting real-time\nrendering with deferred shading and offline rendering with ray tracing. By\noffering realistic, lifelike visualizations under diverse lighting conditions,\nBEAM opens new possibilities for interactive entertainment, storytelling, and\ncreative visualization.\n", "link": "http://arxiv.org/abs/2502.08297v2", "date": "2025-09-09", "relevancy": 3.0709, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6477}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6128}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEAM%3A%20Bridging%20Physically-based%20Rendering%20and%20Gaussian%20Modeling%20for%0A%20%20Relightable%20Volumetric%20Video&body=Title%3A%20BEAM%3A%20Bridging%20Physically-based%20Rendering%20and%20Gaussian%20Modeling%20for%0A%20%20Relightable%20Volumetric%20Video%0AAuthor%3A%20Yu%20Hong%20and%20Yize%20Wu%20and%20Zhehao%20Shen%20and%20Chengcheng%20Guo%20and%20Yuheng%20Jiang%20and%20Yingliang%20Zhang%20and%20Jingyi%20Yu%20and%20Lan%20Xu%0AAbstract%3A%20%20%20Volumetric%20video%20enables%20immersive%20experiences%20by%20capturing%20dynamic%203D%0Ascenes%2C%20enabling%20diverse%20applications%20for%20virtual%20reality%2C%20education%2C%20and%0Atelepresence.%20However%2C%20traditional%20methods%20struggle%20with%20fixed%20lighting%0Aconditions%2C%20while%20neural%20approaches%20face%20trade-offs%20in%20efficiency%2C%20quality%2C%20or%0Aadaptability%20for%20relightable%20scenarios.%20To%20address%20these%20limitations%2C%20we%0Apresent%20BEAM%2C%20a%20novel%20pipeline%20that%20bridges%204D%20Gaussian%20representations%20with%0Aphysically-based%20rendering%20%28PBR%29%20to%20produce%20high-quality%2C%20relightable%0Avolumetric%20videos%20from%20multi-view%20RGB%20footage.%20BEAM%20recovers%20detailed%20geometry%0Aand%20PBR%20properties%20via%20a%20series%20of%20available%20Gaussian-based%20techniques.%20It%0Afirst%20combines%20Gaussian-based%20human%20performance%20tracking%20with%20geometry-aware%0Arasterization%20in%20a%20coarse-to-fine%20optimization%20framework%20to%20recover%20spatially%0Aand%20temporally%20consistent%20geometries.%20We%20further%20enhance%20Gaussian%20attributes%20by%0Aincorporating%20PBR%20properties%20step%20by%20step.%20We%20generate%20roughness%20via%20a%0Amulti-view-conditioned%20diffusion%20model%2C%20and%20then%20derive%20AO%20and%20base%20color%20using%0Aa%202D-to-3D%20strategy%2C%20incorporating%20a%20tailored%20Gaussian-based%20ray%20tracer%20for%0Aefficient%20visibility%20computation.%20Once%20recovered%2C%20these%20dynamic%2C%20relightable%0Aassets%20integrate%20seamlessly%20into%20traditional%20CG%20pipelines%2C%20supporting%20real-time%0Arendering%20with%20deferred%20shading%20and%20offline%20rendering%20with%20ray%20tracing.%20By%0Aoffering%20realistic%2C%20lifelike%20visualizations%20under%20diverse%20lighting%20conditions%2C%0ABEAM%20opens%20new%20possibilities%20for%20interactive%20entertainment%2C%20storytelling%2C%20and%0Acreative%20visualization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08297v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEAM%253A%2520Bridging%2520Physically-based%2520Rendering%2520and%2520Gaussian%2520Modeling%2520for%250A%2520%2520Relightable%2520Volumetric%2520Video%26entry.906535625%3DYu%2520Hong%2520and%2520Yize%2520Wu%2520and%2520Zhehao%2520Shen%2520and%2520Chengcheng%2520Guo%2520and%2520Yuheng%2520Jiang%2520and%2520Yingliang%2520Zhang%2520and%2520Jingyi%2520Yu%2520and%2520Lan%2520Xu%26entry.1292438233%3D%2520%2520Volumetric%2520video%2520enables%2520immersive%2520experiences%2520by%2520capturing%2520dynamic%25203D%250Ascenes%252C%2520enabling%2520diverse%2520applications%2520for%2520virtual%2520reality%252C%2520education%252C%2520and%250Atelepresence.%2520However%252C%2520traditional%2520methods%2520struggle%2520with%2520fixed%2520lighting%250Aconditions%252C%2520while%2520neural%2520approaches%2520face%2520trade-offs%2520in%2520efficiency%252C%2520quality%252C%2520or%250Aadaptability%2520for%2520relightable%2520scenarios.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apresent%2520BEAM%252C%2520a%2520novel%2520pipeline%2520that%2520bridges%25204D%2520Gaussian%2520representations%2520with%250Aphysically-based%2520rendering%2520%2528PBR%2529%2520to%2520produce%2520high-quality%252C%2520relightable%250Avolumetric%2520videos%2520from%2520multi-view%2520RGB%2520footage.%2520BEAM%2520recovers%2520detailed%2520geometry%250Aand%2520PBR%2520properties%2520via%2520a%2520series%2520of%2520available%2520Gaussian-based%2520techniques.%2520It%250Afirst%2520combines%2520Gaussian-based%2520human%2520performance%2520tracking%2520with%2520geometry-aware%250Arasterization%2520in%2520a%2520coarse-to-fine%2520optimization%2520framework%2520to%2520recover%2520spatially%250Aand%2520temporally%2520consistent%2520geometries.%2520We%2520further%2520enhance%2520Gaussian%2520attributes%2520by%250Aincorporating%2520PBR%2520properties%2520step%2520by%2520step.%2520We%2520generate%2520roughness%2520via%2520a%250Amulti-view-conditioned%2520diffusion%2520model%252C%2520and%2520then%2520derive%2520AO%2520and%2520base%2520color%2520using%250Aa%25202D-to-3D%2520strategy%252C%2520incorporating%2520a%2520tailored%2520Gaussian-based%2520ray%2520tracer%2520for%250Aefficient%2520visibility%2520computation.%2520Once%2520recovered%252C%2520these%2520dynamic%252C%2520relightable%250Aassets%2520integrate%2520seamlessly%2520into%2520traditional%2520CG%2520pipelines%252C%2520supporting%2520real-time%250Arendering%2520with%2520deferred%2520shading%2520and%2520offline%2520rendering%2520with%2520ray%2520tracing.%2520By%250Aoffering%2520realistic%252C%2520lifelike%2520visualizations%2520under%2520diverse%2520lighting%2520conditions%252C%250ABEAM%2520opens%2520new%2520possibilities%2520for%2520interactive%2520entertainment%252C%2520storytelling%252C%2520and%250Acreative%2520visualization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08297v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEAM%3A%20Bridging%20Physically-based%20Rendering%20and%20Gaussian%20Modeling%20for%0A%20%20Relightable%20Volumetric%20Video&entry.906535625=Yu%20Hong%20and%20Yize%20Wu%20and%20Zhehao%20Shen%20and%20Chengcheng%20Guo%20and%20Yuheng%20Jiang%20and%20Yingliang%20Zhang%20and%20Jingyi%20Yu%20and%20Lan%20Xu&entry.1292438233=%20%20Volumetric%20video%20enables%20immersive%20experiences%20by%20capturing%20dynamic%203D%0Ascenes%2C%20enabling%20diverse%20applications%20for%20virtual%20reality%2C%20education%2C%20and%0Atelepresence.%20However%2C%20traditional%20methods%20struggle%20with%20fixed%20lighting%0Aconditions%2C%20while%20neural%20approaches%20face%20trade-offs%20in%20efficiency%2C%20quality%2C%20or%0Aadaptability%20for%20relightable%20scenarios.%20To%20address%20these%20limitations%2C%20we%0Apresent%20BEAM%2C%20a%20novel%20pipeline%20that%20bridges%204D%20Gaussian%20representations%20with%0Aphysically-based%20rendering%20%28PBR%29%20to%20produce%20high-quality%2C%20relightable%0Avolumetric%20videos%20from%20multi-view%20RGB%20footage.%20BEAM%20recovers%20detailed%20geometry%0Aand%20PBR%20properties%20via%20a%20series%20of%20available%20Gaussian-based%20techniques.%20It%0Afirst%20combines%20Gaussian-based%20human%20performance%20tracking%20with%20geometry-aware%0Arasterization%20in%20a%20coarse-to-fine%20optimization%20framework%20to%20recover%20spatially%0Aand%20temporally%20consistent%20geometries.%20We%20further%20enhance%20Gaussian%20attributes%20by%0Aincorporating%20PBR%20properties%20step%20by%20step.%20We%20generate%20roughness%20via%20a%0Amulti-view-conditioned%20diffusion%20model%2C%20and%20then%20derive%20AO%20and%20base%20color%20using%0Aa%202D-to-3D%20strategy%2C%20incorporating%20a%20tailored%20Gaussian-based%20ray%20tracer%20for%0Aefficient%20visibility%20computation.%20Once%20recovered%2C%20these%20dynamic%2C%20relightable%0Aassets%20integrate%20seamlessly%20into%20traditional%20CG%20pipelines%2C%20supporting%20real-time%0Arendering%20with%20deferred%20shading%20and%20offline%20rendering%20with%20ray%20tracing.%20By%0Aoffering%20realistic%2C%20lifelike%20visualizations%20under%20diverse%20lighting%20conditions%2C%0ABEAM%20opens%20new%20possibilities%20for%20interactive%20entertainment%2C%20storytelling%2C%20and%0Acreative%20visualization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08297v2&entry.124074799=Read"},
{"title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "author": "Qi Feng", "abstract": "  While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research.\n", "link": "http://arxiv.org/abs/2505.12363v4", "date": "2025-09-09", "relevancy": 2.9838, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6162}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6162}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Visuospatial%20Cognition%20via%20Hierarchical%20Fusion%20of%20Visual%20Experts&body=Title%3A%20Towards%20Visuospatial%20Cognition%20via%20Hierarchical%20Fusion%20of%20Visual%20Experts%0AAuthor%3A%20Qi%20Feng%0AAbstract%3A%20%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20general%0Avision-language%20tasks%2C%20visuospatial%20cognition%20-%20reasoning%20about%20spatial%0Alayouts%2C%20relations%2C%20and%20dynamics%20-%20remains%20a%20significant%20challenge.%20Existing%0Amodels%20often%20lack%20the%20necessary%20architectural%20components%20and%20specialized%0Atraining%20data%20for%20fine-grained%20spatial%20understanding.%20We%20introduce%20ViCA2%0A%28Visuospatial%20Cognitive%20Assistant%202%29%2C%20a%20novel%20MLLM%20designed%20to%20enhance%20spatial%0Areasoning.%20ViCA2%20features%20a%20dual%20vision%20encoder%20architecture%20integrating%20SigLIP%0Afor%20semantics%20and%20Hiera%20for%20spatial%20structure%2C%20coupled%20with%20a%20token%20ratio%0Acontrol%20mechanism%20for%20efficiency.%20We%20also%20developed%20ViCA-322K%2C%20a%20new%0Alarge-scale%20dataset%20with%20over%20322%2C000%20spatially%20grounded%20question-answer%20pairs%0Afor%20targeted%20instruction%20tuning.%20On%20the%20challenging%20VSI-Bench%20benchmark%2C%20our%0AViCA2-7B%20model%20achieves%20a%20state-of-the-art%20average%20score%20of%2056.8%2C%20significantly%0Asurpassing%20larger%20open-source%20models%20%28e.g.%2C%20LLaVA-NeXT-Video-72B%2C%2040.9%29%20and%0Aleading%20proprietary%20models%20%28Gemini-1.5%20Pro%2C%2045.4%29.%20This%20demonstrates%20the%0Aeffectiveness%20of%20our%20approach%20in%20achieving%20strong%20visuospatial%20intelligence%0Awith%20a%20compact%20model.%20We%20release%20ViCA2%2C%20its%20codebase%2C%20and%20the%20ViCA-322K%20dataset%0Ato%20facilitate%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12363v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Visuospatial%2520Cognition%2520via%2520Hierarchical%2520Fusion%2520of%2520Visual%2520Experts%26entry.906535625%3DQi%2520Feng%26entry.1292438233%3D%2520%2520While%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520excel%2520at%2520general%250Avision-language%2520tasks%252C%2520visuospatial%2520cognition%2520-%2520reasoning%2520about%2520spatial%250Alayouts%252C%2520relations%252C%2520and%2520dynamics%2520-%2520remains%2520a%2520significant%2520challenge.%2520Existing%250Amodels%2520often%2520lack%2520the%2520necessary%2520architectural%2520components%2520and%2520specialized%250Atraining%2520data%2520for%2520fine-grained%2520spatial%2520understanding.%2520We%2520introduce%2520ViCA2%250A%2528Visuospatial%2520Cognitive%2520Assistant%25202%2529%252C%2520a%2520novel%2520MLLM%2520designed%2520to%2520enhance%2520spatial%250Areasoning.%2520ViCA2%2520features%2520a%2520dual%2520vision%2520encoder%2520architecture%2520integrating%2520SigLIP%250Afor%2520semantics%2520and%2520Hiera%2520for%2520spatial%2520structure%252C%2520coupled%2520with%2520a%2520token%2520ratio%250Acontrol%2520mechanism%2520for%2520efficiency.%2520We%2520also%2520developed%2520ViCA-322K%252C%2520a%2520new%250Alarge-scale%2520dataset%2520with%2520over%2520322%252C000%2520spatially%2520grounded%2520question-answer%2520pairs%250Afor%2520targeted%2520instruction%2520tuning.%2520On%2520the%2520challenging%2520VSI-Bench%2520benchmark%252C%2520our%250AViCA2-7B%2520model%2520achieves%2520a%2520state-of-the-art%2520average%2520score%2520of%252056.8%252C%2520significantly%250Asurpassing%2520larger%2520open-source%2520models%2520%2528e.g.%252C%2520LLaVA-NeXT-Video-72B%252C%252040.9%2529%2520and%250Aleading%2520proprietary%2520models%2520%2528Gemini-1.5%2520Pro%252C%252045.4%2529.%2520This%2520demonstrates%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520in%2520achieving%2520strong%2520visuospatial%2520intelligence%250Awith%2520a%2520compact%2520model.%2520We%2520release%2520ViCA2%252C%2520its%2520codebase%252C%2520and%2520the%2520ViCA-322K%2520dataset%250Ato%2520facilitate%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12363v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Visuospatial%20Cognition%20via%20Hierarchical%20Fusion%20of%20Visual%20Experts&entry.906535625=Qi%20Feng&entry.1292438233=%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20general%0Avision-language%20tasks%2C%20visuospatial%20cognition%20-%20reasoning%20about%20spatial%0Alayouts%2C%20relations%2C%20and%20dynamics%20-%20remains%20a%20significant%20challenge.%20Existing%0Amodels%20often%20lack%20the%20necessary%20architectural%20components%20and%20specialized%0Atraining%20data%20for%20fine-grained%20spatial%20understanding.%20We%20introduce%20ViCA2%0A%28Visuospatial%20Cognitive%20Assistant%202%29%2C%20a%20novel%20MLLM%20designed%20to%20enhance%20spatial%0Areasoning.%20ViCA2%20features%20a%20dual%20vision%20encoder%20architecture%20integrating%20SigLIP%0Afor%20semantics%20and%20Hiera%20for%20spatial%20structure%2C%20coupled%20with%20a%20token%20ratio%0Acontrol%20mechanism%20for%20efficiency.%20We%20also%20developed%20ViCA-322K%2C%20a%20new%0Alarge-scale%20dataset%20with%20over%20322%2C000%20spatially%20grounded%20question-answer%20pairs%0Afor%20targeted%20instruction%20tuning.%20On%20the%20challenging%20VSI-Bench%20benchmark%2C%20our%0AViCA2-7B%20model%20achieves%20a%20state-of-the-art%20average%20score%20of%2056.8%2C%20significantly%0Asurpassing%20larger%20open-source%20models%20%28e.g.%2C%20LLaVA-NeXT-Video-72B%2C%2040.9%29%20and%0Aleading%20proprietary%20models%20%28Gemini-1.5%20Pro%2C%2045.4%29.%20This%20demonstrates%20the%0Aeffectiveness%20of%20our%20approach%20in%20achieving%20strong%20visuospatial%20intelligence%0Awith%20a%20compact%20model.%20We%20release%20ViCA2%2C%20its%20codebase%2C%20and%20the%20ViCA-322K%20dataset%0Ato%20facilitate%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12363v4&entry.124074799=Read"},
{"title": "Visual Representation Alignment for Multimodal Large Language Models", "author": "Heeji Yoon and Jaewoo Jung and Junwan Kim and Hyungyu Choi and Heeseong Shin and Sangbeom Lim and Honggyu An and Chaehyun Kim and Jisang Han and Donghyun Kim and Chanho Eom and Sunghwan Hong and Seungryong Kim", "abstract": "  Multimodal large language models (MLLMs) trained with visual instruction\ntuning have achieved strong performance across diverse tasks, yet they remain\nlimited in vision-centric tasks such as object counting or spatial reasoning.\nWe attribute this gap to the prevailing text-only supervision paradigm, which\nprovides only indirect guidance for the visual pathway and often leads MLLMs to\ndiscard fine-grained visual details during training. In this paper, we present\nVIsual Representation ALignment (VIRAL), a simple yet effective regularization\nstrategy that aligns the internal visual representations of MLLMs with those of\npre-trained vision foundation models (VFMs). By explicitly enforcing this\nalignment, VIRAL enables the model not only to retain critical visual details\nfrom the input vision encoder but also to complement additional visual\nknowledge from VFMs, thereby enhancing its ability to reason over complex\nvisual inputs. Our experiments demonstrate consistent improvements across all\ntasks on widely adopted multimodal benchmarks. Furthermore, we conduct\ncomprehensive ablation studies to validate the key design choices underlying\nour framework. We believe this simple finding opens up an important direction\nfor the effective integration of visual information in training MLLMs.\n", "link": "http://arxiv.org/abs/2509.07979v1", "date": "2025-09-09", "relevancy": 2.9626, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6043}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Representation%20Alignment%20for%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Visual%20Representation%20Alignment%20for%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Heeji%20Yoon%20and%20Jaewoo%20Jung%20and%20Junwan%20Kim%20and%20Hyungyu%20Choi%20and%20Heeseong%20Shin%20and%20Sangbeom%20Lim%20and%20Honggyu%20An%20and%20Chaehyun%20Kim%20and%20Jisang%20Han%20and%20Donghyun%20Kim%20and%20Chanho%20Eom%20and%20Sunghwan%20Hong%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20trained%20with%20visual%20instruction%0Atuning%20have%20achieved%20strong%20performance%20across%20diverse%20tasks%2C%20yet%20they%20remain%0Alimited%20in%20vision-centric%20tasks%20such%20as%20object%20counting%20or%20spatial%20reasoning.%0AWe%20attribute%20this%20gap%20to%20the%20prevailing%20text-only%20supervision%20paradigm%2C%20which%0Aprovides%20only%20indirect%20guidance%20for%20the%20visual%20pathway%20and%20often%20leads%20MLLMs%20to%0Adiscard%20fine-grained%20visual%20details%20during%20training.%20In%20this%20paper%2C%20we%20present%0AVIsual%20Representation%20ALignment%20%28VIRAL%29%2C%20a%20simple%20yet%20effective%20regularization%0Astrategy%20that%20aligns%20the%20internal%20visual%20representations%20of%20MLLMs%20with%20those%20of%0Apre-trained%20vision%20foundation%20models%20%28VFMs%29.%20By%20explicitly%20enforcing%20this%0Aalignment%2C%20VIRAL%20enables%20the%20model%20not%20only%20to%20retain%20critical%20visual%20details%0Afrom%20the%20input%20vision%20encoder%20but%20also%20to%20complement%20additional%20visual%0Aknowledge%20from%20VFMs%2C%20thereby%20enhancing%20its%20ability%20to%20reason%20over%20complex%0Avisual%20inputs.%20Our%20experiments%20demonstrate%20consistent%20improvements%20across%20all%0Atasks%20on%20widely%20adopted%20multimodal%20benchmarks.%20Furthermore%2C%20we%20conduct%0Acomprehensive%20ablation%20studies%20to%20validate%20the%20key%20design%20choices%20underlying%0Aour%20framework.%20We%20believe%20this%20simple%20finding%20opens%20up%20an%20important%20direction%0Afor%20the%20effective%20integration%20of%20visual%20information%20in%20training%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Representation%2520Alignment%2520for%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DHeeji%2520Yoon%2520and%2520Jaewoo%2520Jung%2520and%2520Junwan%2520Kim%2520and%2520Hyungyu%2520Choi%2520and%2520Heeseong%2520Shin%2520and%2520Sangbeom%2520Lim%2520and%2520Honggyu%2520An%2520and%2520Chaehyun%2520Kim%2520and%2520Jisang%2520Han%2520and%2520Donghyun%2520Kim%2520and%2520Chanho%2520Eom%2520and%2520Sunghwan%2520Hong%2520and%2520Seungryong%2520Kim%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520trained%2520with%2520visual%2520instruction%250Atuning%2520have%2520achieved%2520strong%2520performance%2520across%2520diverse%2520tasks%252C%2520yet%2520they%2520remain%250Alimited%2520in%2520vision-centric%2520tasks%2520such%2520as%2520object%2520counting%2520or%2520spatial%2520reasoning.%250AWe%2520attribute%2520this%2520gap%2520to%2520the%2520prevailing%2520text-only%2520supervision%2520paradigm%252C%2520which%250Aprovides%2520only%2520indirect%2520guidance%2520for%2520the%2520visual%2520pathway%2520and%2520often%2520leads%2520MLLMs%2520to%250Adiscard%2520fine-grained%2520visual%2520details%2520during%2520training.%2520In%2520this%2520paper%252C%2520we%2520present%250AVIsual%2520Representation%2520ALignment%2520%2528VIRAL%2529%252C%2520a%2520simple%2520yet%2520effective%2520regularization%250Astrategy%2520that%2520aligns%2520the%2520internal%2520visual%2520representations%2520of%2520MLLMs%2520with%2520those%2520of%250Apre-trained%2520vision%2520foundation%2520models%2520%2528VFMs%2529.%2520By%2520explicitly%2520enforcing%2520this%250Aalignment%252C%2520VIRAL%2520enables%2520the%2520model%2520not%2520only%2520to%2520retain%2520critical%2520visual%2520details%250Afrom%2520the%2520input%2520vision%2520encoder%2520but%2520also%2520to%2520complement%2520additional%2520visual%250Aknowledge%2520from%2520VFMs%252C%2520thereby%2520enhancing%2520its%2520ability%2520to%2520reason%2520over%2520complex%250Avisual%2520inputs.%2520Our%2520experiments%2520demonstrate%2520consistent%2520improvements%2520across%2520all%250Atasks%2520on%2520widely%2520adopted%2520multimodal%2520benchmarks.%2520Furthermore%252C%2520we%2520conduct%250Acomprehensive%2520ablation%2520studies%2520to%2520validate%2520the%2520key%2520design%2520choices%2520underlying%250Aour%2520framework.%2520We%2520believe%2520this%2520simple%2520finding%2520opens%2520up%2520an%2520important%2520direction%250Afor%2520the%2520effective%2520integration%2520of%2520visual%2520information%2520in%2520training%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Representation%20Alignment%20for%20Multimodal%20Large%20Language%20Models&entry.906535625=Heeji%20Yoon%20and%20Jaewoo%20Jung%20and%20Junwan%20Kim%20and%20Hyungyu%20Choi%20and%20Heeseong%20Shin%20and%20Sangbeom%20Lim%20and%20Honggyu%20An%20and%20Chaehyun%20Kim%20and%20Jisang%20Han%20and%20Donghyun%20Kim%20and%20Chanho%20Eom%20and%20Sunghwan%20Hong%20and%20Seungryong%20Kim&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20trained%20with%20visual%20instruction%0Atuning%20have%20achieved%20strong%20performance%20across%20diverse%20tasks%2C%20yet%20they%20remain%0Alimited%20in%20vision-centric%20tasks%20such%20as%20object%20counting%20or%20spatial%20reasoning.%0AWe%20attribute%20this%20gap%20to%20the%20prevailing%20text-only%20supervision%20paradigm%2C%20which%0Aprovides%20only%20indirect%20guidance%20for%20the%20visual%20pathway%20and%20often%20leads%20MLLMs%20to%0Adiscard%20fine-grained%20visual%20details%20during%20training.%20In%20this%20paper%2C%20we%20present%0AVIsual%20Representation%20ALignment%20%28VIRAL%29%2C%20a%20simple%20yet%20effective%20regularization%0Astrategy%20that%20aligns%20the%20internal%20visual%20representations%20of%20MLLMs%20with%20those%20of%0Apre-trained%20vision%20foundation%20models%20%28VFMs%29.%20By%20explicitly%20enforcing%20this%0Aalignment%2C%20VIRAL%20enables%20the%20model%20not%20only%20to%20retain%20critical%20visual%20details%0Afrom%20the%20input%20vision%20encoder%20but%20also%20to%20complement%20additional%20visual%0Aknowledge%20from%20VFMs%2C%20thereby%20enhancing%20its%20ability%20to%20reason%20over%20complex%0Avisual%20inputs.%20Our%20experiments%20demonstrate%20consistent%20improvements%20across%20all%0Atasks%20on%20widely%20adopted%20multimodal%20benchmarks.%20Furthermore%2C%20we%20conduct%0Acomprehensive%20ablation%20studies%20to%20validate%20the%20key%20design%20choices%20underlying%0Aour%20framework.%20We%20believe%20this%20simple%20finding%20opens%20up%20an%20important%20direction%0Afor%20the%20effective%20integration%20of%20visual%20information%20in%20training%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07979v1&entry.124074799=Read"},
{"title": "Data-Efficient Fine-Tuning of Vision-Language Models for Diagnosis of\n  Alzheimer's Disease", "author": "Fangqi Cheng and Surajit Ray and Xiaochen Yang", "abstract": "  Medical vision-language models (Med-VLMs) have shown impressive results in\ntasks such as report generation and visual question answering, but they still\nface several limitations. Most notably, they underutilize patient metadata and\nlack integration of clinical diagnostic knowledge. Moreover, most existing\nmodels are typically trained from scratch or fine-tuned on large-scale 2D\nimage-text pairs, requiring extensive computational resources, and their\neffectiveness on 3D medical imaging is often limited due to the absence of\nstructural information. To address these gaps, we propose a data-efficient\nfine-tuning pipeline to adapt 3D CT-based Med-VLMs for 3D MRI and demonstrate\nits application in Alzheimer's disease (AD) diagnosis. Our system introduces\ntwo key innovations. First, we convert structured metadata into synthetic\nreports, enriching textual input for improved image-text alignment. Second, we\nadd an auxiliary token trained to predict the mini-mental state examination\n(MMSE) score, a widely used clinical measure of cognitive function that\ncorrelates with AD severity. This provides additional supervision for\nfine-tuning. Applying lightweight prompt tuning to both image and text\nmodalities, our approach achieves state-of-the-art performance on two AD\ndatasets using 1,500 training images, outperforming existing methods fine-tuned\non 10,000 images. Code will be released upon publication.\n", "link": "http://arxiv.org/abs/2509.07613v1", "date": "2025-09-09", "relevancy": 2.8848, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5816}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5747}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Efficient%20Fine-Tuning%20of%20Vision-Language%20Models%20for%20Diagnosis%20of%0A%20%20Alzheimer%27s%20Disease&body=Title%3A%20Data-Efficient%20Fine-Tuning%20of%20Vision-Language%20Models%20for%20Diagnosis%20of%0A%20%20Alzheimer%27s%20Disease%0AAuthor%3A%20Fangqi%20Cheng%20and%20Surajit%20Ray%20and%20Xiaochen%20Yang%0AAbstract%3A%20%20%20Medical%20vision-language%20models%20%28Med-VLMs%29%20have%20shown%20impressive%20results%20in%0Atasks%20such%20as%20report%20generation%20and%20visual%20question%20answering%2C%20but%20they%20still%0Aface%20several%20limitations.%20Most%20notably%2C%20they%20underutilize%20patient%20metadata%20and%0Alack%20integration%20of%20clinical%20diagnostic%20knowledge.%20Moreover%2C%20most%20existing%0Amodels%20are%20typically%20trained%20from%20scratch%20or%20fine-tuned%20on%20large-scale%202D%0Aimage-text%20pairs%2C%20requiring%20extensive%20computational%20resources%2C%20and%20their%0Aeffectiveness%20on%203D%20medical%20imaging%20is%20often%20limited%20due%20to%20the%20absence%20of%0Astructural%20information.%20To%20address%20these%20gaps%2C%20we%20propose%20a%20data-efficient%0Afine-tuning%20pipeline%20to%20adapt%203D%20CT-based%20Med-VLMs%20for%203D%20MRI%20and%20demonstrate%0Aits%20application%20in%20Alzheimer%27s%20disease%20%28AD%29%20diagnosis.%20Our%20system%20introduces%0Atwo%20key%20innovations.%20First%2C%20we%20convert%20structured%20metadata%20into%20synthetic%0Areports%2C%20enriching%20textual%20input%20for%20improved%20image-text%20alignment.%20Second%2C%20we%0Aadd%20an%20auxiliary%20token%20trained%20to%20predict%20the%20mini-mental%20state%20examination%0A%28MMSE%29%20score%2C%20a%20widely%20used%20clinical%20measure%20of%20cognitive%20function%20that%0Acorrelates%20with%20AD%20severity.%20This%20provides%20additional%20supervision%20for%0Afine-tuning.%20Applying%20lightweight%20prompt%20tuning%20to%20both%20image%20and%20text%0Amodalities%2C%20our%20approach%20achieves%20state-of-the-art%20performance%20on%20two%20AD%0Adatasets%20using%201%2C500%20training%20images%2C%20outperforming%20existing%20methods%20fine-tuned%0Aon%2010%2C000%20images.%20Code%20will%20be%20released%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Efficient%2520Fine-Tuning%2520of%2520Vision-Language%2520Models%2520for%2520Diagnosis%2520of%250A%2520%2520Alzheimer%2527s%2520Disease%26entry.906535625%3DFangqi%2520Cheng%2520and%2520Surajit%2520Ray%2520and%2520Xiaochen%2520Yang%26entry.1292438233%3D%2520%2520Medical%2520vision-language%2520models%2520%2528Med-VLMs%2529%2520have%2520shown%2520impressive%2520results%2520in%250Atasks%2520such%2520as%2520report%2520generation%2520and%2520visual%2520question%2520answering%252C%2520but%2520they%2520still%250Aface%2520several%2520limitations.%2520Most%2520notably%252C%2520they%2520underutilize%2520patient%2520metadata%2520and%250Alack%2520integration%2520of%2520clinical%2520diagnostic%2520knowledge.%2520Moreover%252C%2520most%2520existing%250Amodels%2520are%2520typically%2520trained%2520from%2520scratch%2520or%2520fine-tuned%2520on%2520large-scale%25202D%250Aimage-text%2520pairs%252C%2520requiring%2520extensive%2520computational%2520resources%252C%2520and%2520their%250Aeffectiveness%2520on%25203D%2520medical%2520imaging%2520is%2520often%2520limited%2520due%2520to%2520the%2520absence%2520of%250Astructural%2520information.%2520To%2520address%2520these%2520gaps%252C%2520we%2520propose%2520a%2520data-efficient%250Afine-tuning%2520pipeline%2520to%2520adapt%25203D%2520CT-based%2520Med-VLMs%2520for%25203D%2520MRI%2520and%2520demonstrate%250Aits%2520application%2520in%2520Alzheimer%2527s%2520disease%2520%2528AD%2529%2520diagnosis.%2520Our%2520system%2520introduces%250Atwo%2520key%2520innovations.%2520First%252C%2520we%2520convert%2520structured%2520metadata%2520into%2520synthetic%250Areports%252C%2520enriching%2520textual%2520input%2520for%2520improved%2520image-text%2520alignment.%2520Second%252C%2520we%250Aadd%2520an%2520auxiliary%2520token%2520trained%2520to%2520predict%2520the%2520mini-mental%2520state%2520examination%250A%2528MMSE%2529%2520score%252C%2520a%2520widely%2520used%2520clinical%2520measure%2520of%2520cognitive%2520function%2520that%250Acorrelates%2520with%2520AD%2520severity.%2520This%2520provides%2520additional%2520supervision%2520for%250Afine-tuning.%2520Applying%2520lightweight%2520prompt%2520tuning%2520to%2520both%2520image%2520and%2520text%250Amodalities%252C%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520on%2520two%2520AD%250Adatasets%2520using%25201%252C500%2520training%2520images%252C%2520outperforming%2520existing%2520methods%2520fine-tuned%250Aon%252010%252C000%2520images.%2520Code%2520will%2520be%2520released%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Efficient%20Fine-Tuning%20of%20Vision-Language%20Models%20for%20Diagnosis%20of%0A%20%20Alzheimer%27s%20Disease&entry.906535625=Fangqi%20Cheng%20and%20Surajit%20Ray%20and%20Xiaochen%20Yang&entry.1292438233=%20%20Medical%20vision-language%20models%20%28Med-VLMs%29%20have%20shown%20impressive%20results%20in%0Atasks%20such%20as%20report%20generation%20and%20visual%20question%20answering%2C%20but%20they%20still%0Aface%20several%20limitations.%20Most%20notably%2C%20they%20underutilize%20patient%20metadata%20and%0Alack%20integration%20of%20clinical%20diagnostic%20knowledge.%20Moreover%2C%20most%20existing%0Amodels%20are%20typically%20trained%20from%20scratch%20or%20fine-tuned%20on%20large-scale%202D%0Aimage-text%20pairs%2C%20requiring%20extensive%20computational%20resources%2C%20and%20their%0Aeffectiveness%20on%203D%20medical%20imaging%20is%20often%20limited%20due%20to%20the%20absence%20of%0Astructural%20information.%20To%20address%20these%20gaps%2C%20we%20propose%20a%20data-efficient%0Afine-tuning%20pipeline%20to%20adapt%203D%20CT-based%20Med-VLMs%20for%203D%20MRI%20and%20demonstrate%0Aits%20application%20in%20Alzheimer%27s%20disease%20%28AD%29%20diagnosis.%20Our%20system%20introduces%0Atwo%20key%20innovations.%20First%2C%20we%20convert%20structured%20metadata%20into%20synthetic%0Areports%2C%20enriching%20textual%20input%20for%20improved%20image-text%20alignment.%20Second%2C%20we%0Aadd%20an%20auxiliary%20token%20trained%20to%20predict%20the%20mini-mental%20state%20examination%0A%28MMSE%29%20score%2C%20a%20widely%20used%20clinical%20measure%20of%20cognitive%20function%20that%0Acorrelates%20with%20AD%20severity.%20This%20provides%20additional%20supervision%20for%0Afine-tuning.%20Applying%20lightweight%20prompt%20tuning%20to%20both%20image%20and%20text%0Amodalities%2C%20our%20approach%20achieves%20state-of-the-art%20performance%20on%20two%20AD%0Adatasets%20using%201%2C500%20training%20images%2C%20outperforming%20existing%20methods%20fine-tuned%0Aon%2010%2C000%20images.%20Code%20will%20be%20released%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07613v1&entry.124074799=Read"},
{"title": "One Flight Over the Gap: A Survey from Perspective to Panoramic Vision", "author": "Xin Lin and Xian Ge and Dizhe Zhang and Zhaoliang Wan and Xianshun Wang and Xiangtai Li and Wenjie Jiang and Bo Du and Dacheng Tao and Ming-Hsuan Yang and Lu Qi", "abstract": "  Driven by the demand for spatial intelligence and holistic scene perception,\nomnidirectional images (ODIs), which provide a complete 360\\textdegree{} field\nof view, are receiving growing attention across diverse applications such as\nvirtual reality, autonomous driving, and embodied robotics. Despite their\nunique characteristics, ODIs exhibit remarkable differences from perspective\nimages in geometric projection, spatial distribution, and boundary continuity,\nmaking it challenging for direct domain adaption from perspective methods. This\nsurvey reviews recent panoramic vision techniques with a particular emphasis on\nthe perspective-to-panorama adaptation. We first revisit the panoramic imaging\npipeline and projection methods to build the prior knowledge required for\nanalyzing the structural disparities. Then, we summarize three challenges of\ndomain adaptation: severe geometric distortions near the poles, non-uniform\nsampling in Equirectangular Projection (ERP), and periodic boundary continuity.\nBuilding on this, we cover 20+ representative tasks drawn from more than 300\nresearch papers in two dimensions. On one hand, we present a cross-method\nanalysis of representative strategies for addressing panoramic specific\nchallenges across different tasks. On the other hand, we conduct a cross-task\ncomparison and classify panoramic vision into four major categories: visual\nquality enhancement and assessment, visual understanding, multimodal\nunderstanding, and visual generation. In addition, we discuss open challenges\nand future directions in data, models, and applications that will drive the\nadvancement of panoramic vision research. We hope that our work can provide new\ninsight and forward looking perspectives to advance the development of\npanoramic vision technologies. Our project page is\nhttps://insta360-research-team.github.io/Survey-of-Panorama\n", "link": "http://arxiv.org/abs/2509.04444v2", "date": "2025-09-09", "relevancy": 2.8762, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5736}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Flight%20Over%20the%20Gap%3A%20A%20Survey%20from%20Perspective%20to%20Panoramic%20Vision&body=Title%3A%20One%20Flight%20Over%20the%20Gap%3A%20A%20Survey%20from%20Perspective%20to%20Panoramic%20Vision%0AAuthor%3A%20Xin%20Lin%20and%20Xian%20Ge%20and%20Dizhe%20Zhang%20and%20Zhaoliang%20Wan%20and%20Xianshun%20Wang%20and%20Xiangtai%20Li%20and%20Wenjie%20Jiang%20and%20Bo%20Du%20and%20Dacheng%20Tao%20and%20Ming-Hsuan%20Yang%20and%20Lu%20Qi%0AAbstract%3A%20%20%20Driven%20by%20the%20demand%20for%20spatial%20intelligence%20and%20holistic%20scene%20perception%2C%0Aomnidirectional%20images%20%28ODIs%29%2C%20which%20provide%20a%20complete%20360%5Ctextdegree%7B%7D%20field%0Aof%20view%2C%20are%20receiving%20growing%20attention%20across%20diverse%20applications%20such%20as%0Avirtual%20reality%2C%20autonomous%20driving%2C%20and%20embodied%20robotics.%20Despite%20their%0Aunique%20characteristics%2C%20ODIs%20exhibit%20remarkable%20differences%20from%20perspective%0Aimages%20in%20geometric%20projection%2C%20spatial%20distribution%2C%20and%20boundary%20continuity%2C%0Amaking%20it%20challenging%20for%20direct%20domain%20adaption%20from%20perspective%20methods.%20This%0Asurvey%20reviews%20recent%20panoramic%20vision%20techniques%20with%20a%20particular%20emphasis%20on%0Athe%20perspective-to-panorama%20adaptation.%20We%20first%20revisit%20the%20panoramic%20imaging%0Apipeline%20and%20projection%20methods%20to%20build%20the%20prior%20knowledge%20required%20for%0Aanalyzing%20the%20structural%20disparities.%20Then%2C%20we%20summarize%20three%20challenges%20of%0Adomain%20adaptation%3A%20severe%20geometric%20distortions%20near%20the%20poles%2C%20non-uniform%0Asampling%20in%20Equirectangular%20Projection%20%28ERP%29%2C%20and%20periodic%20boundary%20continuity.%0ABuilding%20on%20this%2C%20we%20cover%2020%2B%20representative%20tasks%20drawn%20from%20more%20than%20300%0Aresearch%20papers%20in%20two%20dimensions.%20On%20one%20hand%2C%20we%20present%20a%20cross-method%0Aanalysis%20of%20representative%20strategies%20for%20addressing%20panoramic%20specific%0Achallenges%20across%20different%20tasks.%20On%20the%20other%20hand%2C%20we%20conduct%20a%20cross-task%0Acomparison%20and%20classify%20panoramic%20vision%20into%20four%20major%20categories%3A%20visual%0Aquality%20enhancement%20and%20assessment%2C%20visual%20understanding%2C%20multimodal%0Aunderstanding%2C%20and%20visual%20generation.%20In%20addition%2C%20we%20discuss%20open%20challenges%0Aand%20future%20directions%20in%20data%2C%20models%2C%20and%20applications%20that%20will%20drive%20the%0Aadvancement%20of%20panoramic%20vision%20research.%20We%20hope%20that%20our%20work%20can%20provide%20new%0Ainsight%20and%20forward%20looking%20perspectives%20to%20advance%20the%20development%20of%0Apanoramic%20vision%20technologies.%20Our%20project%20page%20is%0Ahttps%3A//insta360-research-team.github.io/Survey-of-Panorama%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04444v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Flight%2520Over%2520the%2520Gap%253A%2520A%2520Survey%2520from%2520Perspective%2520to%2520Panoramic%2520Vision%26entry.906535625%3DXin%2520Lin%2520and%2520Xian%2520Ge%2520and%2520Dizhe%2520Zhang%2520and%2520Zhaoliang%2520Wan%2520and%2520Xianshun%2520Wang%2520and%2520Xiangtai%2520Li%2520and%2520Wenjie%2520Jiang%2520and%2520Bo%2520Du%2520and%2520Dacheng%2520Tao%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Lu%2520Qi%26entry.1292438233%3D%2520%2520Driven%2520by%2520the%2520demand%2520for%2520spatial%2520intelligence%2520and%2520holistic%2520scene%2520perception%252C%250Aomnidirectional%2520images%2520%2528ODIs%2529%252C%2520which%2520provide%2520a%2520complete%2520360%255Ctextdegree%257B%257D%2520field%250Aof%2520view%252C%2520are%2520receiving%2520growing%2520attention%2520across%2520diverse%2520applications%2520such%2520as%250Avirtual%2520reality%252C%2520autonomous%2520driving%252C%2520and%2520embodied%2520robotics.%2520Despite%2520their%250Aunique%2520characteristics%252C%2520ODIs%2520exhibit%2520remarkable%2520differences%2520from%2520perspective%250Aimages%2520in%2520geometric%2520projection%252C%2520spatial%2520distribution%252C%2520and%2520boundary%2520continuity%252C%250Amaking%2520it%2520challenging%2520for%2520direct%2520domain%2520adaption%2520from%2520perspective%2520methods.%2520This%250Asurvey%2520reviews%2520recent%2520panoramic%2520vision%2520techniques%2520with%2520a%2520particular%2520emphasis%2520on%250Athe%2520perspective-to-panorama%2520adaptation.%2520We%2520first%2520revisit%2520the%2520panoramic%2520imaging%250Apipeline%2520and%2520projection%2520methods%2520to%2520build%2520the%2520prior%2520knowledge%2520required%2520for%250Aanalyzing%2520the%2520structural%2520disparities.%2520Then%252C%2520we%2520summarize%2520three%2520challenges%2520of%250Adomain%2520adaptation%253A%2520severe%2520geometric%2520distortions%2520near%2520the%2520poles%252C%2520non-uniform%250Asampling%2520in%2520Equirectangular%2520Projection%2520%2528ERP%2529%252C%2520and%2520periodic%2520boundary%2520continuity.%250ABuilding%2520on%2520this%252C%2520we%2520cover%252020%252B%2520representative%2520tasks%2520drawn%2520from%2520more%2520than%2520300%250Aresearch%2520papers%2520in%2520two%2520dimensions.%2520On%2520one%2520hand%252C%2520we%2520present%2520a%2520cross-method%250Aanalysis%2520of%2520representative%2520strategies%2520for%2520addressing%2520panoramic%2520specific%250Achallenges%2520across%2520different%2520tasks.%2520On%2520the%2520other%2520hand%252C%2520we%2520conduct%2520a%2520cross-task%250Acomparison%2520and%2520classify%2520panoramic%2520vision%2520into%2520four%2520major%2520categories%253A%2520visual%250Aquality%2520enhancement%2520and%2520assessment%252C%2520visual%2520understanding%252C%2520multimodal%250Aunderstanding%252C%2520and%2520visual%2520generation.%2520In%2520addition%252C%2520we%2520discuss%2520open%2520challenges%250Aand%2520future%2520directions%2520in%2520data%252C%2520models%252C%2520and%2520applications%2520that%2520will%2520drive%2520the%250Aadvancement%2520of%2520panoramic%2520vision%2520research.%2520We%2520hope%2520that%2520our%2520work%2520can%2520provide%2520new%250Ainsight%2520and%2520forward%2520looking%2520perspectives%2520to%2520advance%2520the%2520development%2520of%250Apanoramic%2520vision%2520technologies.%2520Our%2520project%2520page%2520is%250Ahttps%253A//insta360-research-team.github.io/Survey-of-Panorama%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04444v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Flight%20Over%20the%20Gap%3A%20A%20Survey%20from%20Perspective%20to%20Panoramic%20Vision&entry.906535625=Xin%20Lin%20and%20Xian%20Ge%20and%20Dizhe%20Zhang%20and%20Zhaoliang%20Wan%20and%20Xianshun%20Wang%20and%20Xiangtai%20Li%20and%20Wenjie%20Jiang%20and%20Bo%20Du%20and%20Dacheng%20Tao%20and%20Ming-Hsuan%20Yang%20and%20Lu%20Qi&entry.1292438233=%20%20Driven%20by%20the%20demand%20for%20spatial%20intelligence%20and%20holistic%20scene%20perception%2C%0Aomnidirectional%20images%20%28ODIs%29%2C%20which%20provide%20a%20complete%20360%5Ctextdegree%7B%7D%20field%0Aof%20view%2C%20are%20receiving%20growing%20attention%20across%20diverse%20applications%20such%20as%0Avirtual%20reality%2C%20autonomous%20driving%2C%20and%20embodied%20robotics.%20Despite%20their%0Aunique%20characteristics%2C%20ODIs%20exhibit%20remarkable%20differences%20from%20perspective%0Aimages%20in%20geometric%20projection%2C%20spatial%20distribution%2C%20and%20boundary%20continuity%2C%0Amaking%20it%20challenging%20for%20direct%20domain%20adaption%20from%20perspective%20methods.%20This%0Asurvey%20reviews%20recent%20panoramic%20vision%20techniques%20with%20a%20particular%20emphasis%20on%0Athe%20perspective-to-panorama%20adaptation.%20We%20first%20revisit%20the%20panoramic%20imaging%0Apipeline%20and%20projection%20methods%20to%20build%20the%20prior%20knowledge%20required%20for%0Aanalyzing%20the%20structural%20disparities.%20Then%2C%20we%20summarize%20three%20challenges%20of%0Adomain%20adaptation%3A%20severe%20geometric%20distortions%20near%20the%20poles%2C%20non-uniform%0Asampling%20in%20Equirectangular%20Projection%20%28ERP%29%2C%20and%20periodic%20boundary%20continuity.%0ABuilding%20on%20this%2C%20we%20cover%2020%2B%20representative%20tasks%20drawn%20from%20more%20than%20300%0Aresearch%20papers%20in%20two%20dimensions.%20On%20one%20hand%2C%20we%20present%20a%20cross-method%0Aanalysis%20of%20representative%20strategies%20for%20addressing%20panoramic%20specific%0Achallenges%20across%20different%20tasks.%20On%20the%20other%20hand%2C%20we%20conduct%20a%20cross-task%0Acomparison%20and%20classify%20panoramic%20vision%20into%20four%20major%20categories%3A%20visual%0Aquality%20enhancement%20and%20assessment%2C%20visual%20understanding%2C%20multimodal%0Aunderstanding%2C%20and%20visual%20generation.%20In%20addition%2C%20we%20discuss%20open%20challenges%0Aand%20future%20directions%20in%20data%2C%20models%2C%20and%20applications%20that%20will%20drive%20the%0Aadvancement%20of%20panoramic%20vision%20research.%20We%20hope%20that%20our%20work%20can%20provide%20new%0Ainsight%20and%20forward%20looking%20perspectives%20to%20advance%20the%20development%20of%0Apanoramic%20vision%20technologies.%20Our%20project%20page%20is%0Ahttps%3A//insta360-research-team.github.io/Survey-of-Panorama%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04444v2&entry.124074799=Read"},
{"title": "P3-SAM: Native 3D Part Segmentation", "author": "Changfeng Ma and Yang Li and Xinhao Yan and Jiachen Xu and Yunhan Yang and Chunshi Wang and Zibo Zhao and Yanwen Guo and Zhuo Chen and Chunchao Guo", "abstract": "  Segmenting 3D assets into their constituent parts is crucial for enhancing 3D\nunderstanding, facilitating model reuse, and supporting various applications\nsuch as part generation. However, current methods face limitations such as poor\nrobustness when dealing with complex objects and cannot fully automate the\nprocess. In this paper, we propose a native 3D point-promptable part\nsegmentation model termed P3-SAM, designed to fully automate the segmentation\nof any 3D objects into components. Inspired by SAM, P3-SAM consists of a\nfeature extractor, multiple segmentation heads, and an IoU predictor, enabling\ninteractive segmentation for users. We also propose an algorithm to\nautomatically select and merge masks predicted by our model for part instance\nsegmentation. Our model is trained on a newly built dataset containing nearly\n3.7 million models with reasonable segmentation labels. Comparisons show that\nour method achieves precise segmentation results and strong robustness on any\ncomplex objects, attaining state-of-the-art performance. Our code will be\nreleased soon.\n", "link": "http://arxiv.org/abs/2509.06784v2", "date": "2025-09-09", "relevancy": 2.8154, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5637}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5637}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P3-SAM%3A%20Native%203D%20Part%20Segmentation&body=Title%3A%20P3-SAM%3A%20Native%203D%20Part%20Segmentation%0AAuthor%3A%20Changfeng%20Ma%20and%20Yang%20Li%20and%20Xinhao%20Yan%20and%20Jiachen%20Xu%20and%20Yunhan%20Yang%20and%20Chunshi%20Wang%20and%20Zibo%20Zhao%20and%20Yanwen%20Guo%20and%20Zhuo%20Chen%20and%20Chunchao%20Guo%0AAbstract%3A%20%20%20Segmenting%203D%20assets%20into%20their%20constituent%20parts%20is%20crucial%20for%20enhancing%203D%0Aunderstanding%2C%20facilitating%20model%20reuse%2C%20and%20supporting%20various%20applications%0Asuch%20as%20part%20generation.%20However%2C%20current%20methods%20face%20limitations%20such%20as%20poor%0Arobustness%20when%20dealing%20with%20complex%20objects%20and%20cannot%20fully%20automate%20the%0Aprocess.%20In%20this%20paper%2C%20we%20propose%20a%20native%203D%20point-promptable%20part%0Asegmentation%20model%20termed%20P3-SAM%2C%20designed%20to%20fully%20automate%20the%20segmentation%0Aof%20any%203D%20objects%20into%20components.%20Inspired%20by%20SAM%2C%20P3-SAM%20consists%20of%20a%0Afeature%20extractor%2C%20multiple%20segmentation%20heads%2C%20and%20an%20IoU%20predictor%2C%20enabling%0Ainteractive%20segmentation%20for%20users.%20We%20also%20propose%20an%20algorithm%20to%0Aautomatically%20select%20and%20merge%20masks%20predicted%20by%20our%20model%20for%20part%20instance%0Asegmentation.%20Our%20model%20is%20trained%20on%20a%20newly%20built%20dataset%20containing%20nearly%0A3.7%20million%20models%20with%20reasonable%20segmentation%20labels.%20Comparisons%20show%20that%0Aour%20method%20achieves%20precise%20segmentation%20results%20and%20strong%20robustness%20on%20any%0Acomplex%20objects%2C%20attaining%20state-of-the-art%20performance.%20Our%20code%20will%20be%0Areleased%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06784v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP3-SAM%253A%2520Native%25203D%2520Part%2520Segmentation%26entry.906535625%3DChangfeng%2520Ma%2520and%2520Yang%2520Li%2520and%2520Xinhao%2520Yan%2520and%2520Jiachen%2520Xu%2520and%2520Yunhan%2520Yang%2520and%2520Chunshi%2520Wang%2520and%2520Zibo%2520Zhao%2520and%2520Yanwen%2520Guo%2520and%2520Zhuo%2520Chen%2520and%2520Chunchao%2520Guo%26entry.1292438233%3D%2520%2520Segmenting%25203D%2520assets%2520into%2520their%2520constituent%2520parts%2520is%2520crucial%2520for%2520enhancing%25203D%250Aunderstanding%252C%2520facilitating%2520model%2520reuse%252C%2520and%2520supporting%2520various%2520applications%250Asuch%2520as%2520part%2520generation.%2520However%252C%2520current%2520methods%2520face%2520limitations%2520such%2520as%2520poor%250Arobustness%2520when%2520dealing%2520with%2520complex%2520objects%2520and%2520cannot%2520fully%2520automate%2520the%250Aprocess.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520native%25203D%2520point-promptable%2520part%250Asegmentation%2520model%2520termed%2520P3-SAM%252C%2520designed%2520to%2520fully%2520automate%2520the%2520segmentation%250Aof%2520any%25203D%2520objects%2520into%2520components.%2520Inspired%2520by%2520SAM%252C%2520P3-SAM%2520consists%2520of%2520a%250Afeature%2520extractor%252C%2520multiple%2520segmentation%2520heads%252C%2520and%2520an%2520IoU%2520predictor%252C%2520enabling%250Ainteractive%2520segmentation%2520for%2520users.%2520We%2520also%2520propose%2520an%2520algorithm%2520to%250Aautomatically%2520select%2520and%2520merge%2520masks%2520predicted%2520by%2520our%2520model%2520for%2520part%2520instance%250Asegmentation.%2520Our%2520model%2520is%2520trained%2520on%2520a%2520newly%2520built%2520dataset%2520containing%2520nearly%250A3.7%2520million%2520models%2520with%2520reasonable%2520segmentation%2520labels.%2520Comparisons%2520show%2520that%250Aour%2520method%2520achieves%2520precise%2520segmentation%2520results%2520and%2520strong%2520robustness%2520on%2520any%250Acomplex%2520objects%252C%2520attaining%2520state-of-the-art%2520performance.%2520Our%2520code%2520will%2520be%250Areleased%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06784v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P3-SAM%3A%20Native%203D%20Part%20Segmentation&entry.906535625=Changfeng%20Ma%20and%20Yang%20Li%20and%20Xinhao%20Yan%20and%20Jiachen%20Xu%20and%20Yunhan%20Yang%20and%20Chunshi%20Wang%20and%20Zibo%20Zhao%20and%20Yanwen%20Guo%20and%20Zhuo%20Chen%20and%20Chunchao%20Guo&entry.1292438233=%20%20Segmenting%203D%20assets%20into%20their%20constituent%20parts%20is%20crucial%20for%20enhancing%203D%0Aunderstanding%2C%20facilitating%20model%20reuse%2C%20and%20supporting%20various%20applications%0Asuch%20as%20part%20generation.%20However%2C%20current%20methods%20face%20limitations%20such%20as%20poor%0Arobustness%20when%20dealing%20with%20complex%20objects%20and%20cannot%20fully%20automate%20the%0Aprocess.%20In%20this%20paper%2C%20we%20propose%20a%20native%203D%20point-promptable%20part%0Asegmentation%20model%20termed%20P3-SAM%2C%20designed%20to%20fully%20automate%20the%20segmentation%0Aof%20any%203D%20objects%20into%20components.%20Inspired%20by%20SAM%2C%20P3-SAM%20consists%20of%20a%0Afeature%20extractor%2C%20multiple%20segmentation%20heads%2C%20and%20an%20IoU%20predictor%2C%20enabling%0Ainteractive%20segmentation%20for%20users.%20We%20also%20propose%20an%20algorithm%20to%0Aautomatically%20select%20and%20merge%20masks%20predicted%20by%20our%20model%20for%20part%20instance%0Asegmentation.%20Our%20model%20is%20trained%20on%20a%20newly%20built%20dataset%20containing%20nearly%0A3.7%20million%20models%20with%20reasonable%20segmentation%20labels.%20Comparisons%20show%20that%0Aour%20method%20achieves%20precise%20segmentation%20results%20and%20strong%20robustness%20on%20any%0Acomplex%20objects%2C%20attaining%20state-of-the-art%20performance.%20Our%20code%20will%20be%0Areleased%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06784v2&entry.124074799=Read"},
{"title": "Large-scale Pre-training for Grounded Video Caption Generation", "author": "Evangelos Kazakos and Cordelia Schmid and Josef Sivic", "abstract": "  We propose a novel approach for captioning and object grounding in video,\nwhere the objects in the caption are grounded in the video via temporally dense\nbounding boxes. We introduce the following contributions. First, we present a\nlarge-scale automatic annotation method that aggregates frame-level captions\ngrounded with bounding boxes into temporally dense and consistent annotations.\nWe apply this approach on the HowTo100M dataset to construct a large-scale\npre-training dataset, named HowToGround1M. We also introduce a Grounded Video\nCaption Generation model, dubbed GROVE, and pre-train the model on\nHowToGround1M. Second, we introduce iGround--a dataset of 3513 videos with\nmanually annotated captions and dense spatio-temporally grounded bounding\nboxes. This allows us to measure progress on this challenging problem, as well\nas to fine-tune our model on this small-scale but high-quality data. Third, we\ndemonstrate that our approach achieves state-of-the-art results on the proposed\niGround dataset, as well as on the VidSTG, ActivityNet-Entities,\nGroundingYouTube, and YouCook-Interactions datasets. Our ablations demonstrate\nthe importance of pre-training on our automatically annotated HowToGround1M\ndataset followed by fine-tuning on the manually annotated iGround dataset and\nvalidate the key technical contributions of our model. The dataset and code are\navailable at https://ekazakos.github.io/grounded_video_caption_generation/.\n", "link": "http://arxiv.org/abs/2503.10781v3", "date": "2025-09-09", "relevancy": 2.7711, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5652}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5492}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-scale%20Pre-training%20for%20Grounded%20Video%20Caption%20Generation&body=Title%3A%20Large-scale%20Pre-training%20for%20Grounded%20Video%20Caption%20Generation%0AAuthor%3A%20Evangelos%20Kazakos%20and%20Cordelia%20Schmid%20and%20Josef%20Sivic%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20approach%20for%20captioning%20and%20object%20grounding%20in%20video%2C%0Awhere%20the%20objects%20in%20the%20caption%20are%20grounded%20in%20the%20video%20via%20temporally%20dense%0Abounding%20boxes.%20We%20introduce%20the%20following%20contributions.%20First%2C%20we%20present%20a%0Alarge-scale%20automatic%20annotation%20method%20that%20aggregates%20frame-level%20captions%0Agrounded%20with%20bounding%20boxes%20into%20temporally%20dense%20and%20consistent%20annotations.%0AWe%20apply%20this%20approach%20on%20the%20HowTo100M%20dataset%20to%20construct%20a%20large-scale%0Apre-training%20dataset%2C%20named%20HowToGround1M.%20We%20also%20introduce%20a%20Grounded%20Video%0ACaption%20Generation%20model%2C%20dubbed%20GROVE%2C%20and%20pre-train%20the%20model%20on%0AHowToGround1M.%20Second%2C%20we%20introduce%20iGround--a%20dataset%20of%203513%20videos%20with%0Amanually%20annotated%20captions%20and%20dense%20spatio-temporally%20grounded%20bounding%0Aboxes.%20This%20allows%20us%20to%20measure%20progress%20on%20this%20challenging%20problem%2C%20as%20well%0Aas%20to%20fine-tune%20our%20model%20on%20this%20small-scale%20but%20high-quality%20data.%20Third%2C%20we%0Ademonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20results%20on%20the%20proposed%0AiGround%20dataset%2C%20as%20well%20as%20on%20the%20VidSTG%2C%20ActivityNet-Entities%2C%0AGroundingYouTube%2C%20and%20YouCook-Interactions%20datasets.%20Our%20ablations%20demonstrate%0Athe%20importance%20of%20pre-training%20on%20our%20automatically%20annotated%20HowToGround1M%0Adataset%20followed%20by%20fine-tuning%20on%20the%20manually%20annotated%20iGround%20dataset%20and%0Avalidate%20the%20key%20technical%20contributions%20of%20our%20model.%20The%20dataset%20and%20code%20are%0Aavailable%20at%20https%3A//ekazakos.github.io/grounded_video_caption_generation/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10781v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-scale%2520Pre-training%2520for%2520Grounded%2520Video%2520Caption%2520Generation%26entry.906535625%3DEvangelos%2520Kazakos%2520and%2520Cordelia%2520Schmid%2520and%2520Josef%2520Sivic%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520approach%2520for%2520captioning%2520and%2520object%2520grounding%2520in%2520video%252C%250Awhere%2520the%2520objects%2520in%2520the%2520caption%2520are%2520grounded%2520in%2520the%2520video%2520via%2520temporally%2520dense%250Abounding%2520boxes.%2520We%2520introduce%2520the%2520following%2520contributions.%2520First%252C%2520we%2520present%2520a%250Alarge-scale%2520automatic%2520annotation%2520method%2520that%2520aggregates%2520frame-level%2520captions%250Agrounded%2520with%2520bounding%2520boxes%2520into%2520temporally%2520dense%2520and%2520consistent%2520annotations.%250AWe%2520apply%2520this%2520approach%2520on%2520the%2520HowTo100M%2520dataset%2520to%2520construct%2520a%2520large-scale%250Apre-training%2520dataset%252C%2520named%2520HowToGround1M.%2520We%2520also%2520introduce%2520a%2520Grounded%2520Video%250ACaption%2520Generation%2520model%252C%2520dubbed%2520GROVE%252C%2520and%2520pre-train%2520the%2520model%2520on%250AHowToGround1M.%2520Second%252C%2520we%2520introduce%2520iGround--a%2520dataset%2520of%25203513%2520videos%2520with%250Amanually%2520annotated%2520captions%2520and%2520dense%2520spatio-temporally%2520grounded%2520bounding%250Aboxes.%2520This%2520allows%2520us%2520to%2520measure%2520progress%2520on%2520this%2520challenging%2520problem%252C%2520as%2520well%250Aas%2520to%2520fine-tune%2520our%2520model%2520on%2520this%2520small-scale%2520but%2520high-quality%2520data.%2520Third%252C%2520we%250Ademonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520results%2520on%2520the%2520proposed%250AiGround%2520dataset%252C%2520as%2520well%2520as%2520on%2520the%2520VidSTG%252C%2520ActivityNet-Entities%252C%250AGroundingYouTube%252C%2520and%2520YouCook-Interactions%2520datasets.%2520Our%2520ablations%2520demonstrate%250Athe%2520importance%2520of%2520pre-training%2520on%2520our%2520automatically%2520annotated%2520HowToGround1M%250Adataset%2520followed%2520by%2520fine-tuning%2520on%2520the%2520manually%2520annotated%2520iGround%2520dataset%2520and%250Avalidate%2520the%2520key%2520technical%2520contributions%2520of%2520our%2520model.%2520The%2520dataset%2520and%2520code%2520are%250Aavailable%2520at%2520https%253A//ekazakos.github.io/grounded_video_caption_generation/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10781v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-scale%20Pre-training%20for%20Grounded%20Video%20Caption%20Generation&entry.906535625=Evangelos%20Kazakos%20and%20Cordelia%20Schmid%20and%20Josef%20Sivic&entry.1292438233=%20%20We%20propose%20a%20novel%20approach%20for%20captioning%20and%20object%20grounding%20in%20video%2C%0Awhere%20the%20objects%20in%20the%20caption%20are%20grounded%20in%20the%20video%20via%20temporally%20dense%0Abounding%20boxes.%20We%20introduce%20the%20following%20contributions.%20First%2C%20we%20present%20a%0Alarge-scale%20automatic%20annotation%20method%20that%20aggregates%20frame-level%20captions%0Agrounded%20with%20bounding%20boxes%20into%20temporally%20dense%20and%20consistent%20annotations.%0AWe%20apply%20this%20approach%20on%20the%20HowTo100M%20dataset%20to%20construct%20a%20large-scale%0Apre-training%20dataset%2C%20named%20HowToGround1M.%20We%20also%20introduce%20a%20Grounded%20Video%0ACaption%20Generation%20model%2C%20dubbed%20GROVE%2C%20and%20pre-train%20the%20model%20on%0AHowToGround1M.%20Second%2C%20we%20introduce%20iGround--a%20dataset%20of%203513%20videos%20with%0Amanually%20annotated%20captions%20and%20dense%20spatio-temporally%20grounded%20bounding%0Aboxes.%20This%20allows%20us%20to%20measure%20progress%20on%20this%20challenging%20problem%2C%20as%20well%0Aas%20to%20fine-tune%20our%20model%20on%20this%20small-scale%20but%20high-quality%20data.%20Third%2C%20we%0Ademonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20results%20on%20the%20proposed%0AiGround%20dataset%2C%20as%20well%20as%20on%20the%20VidSTG%2C%20ActivityNet-Entities%2C%0AGroundingYouTube%2C%20and%20YouCook-Interactions%20datasets.%20Our%20ablations%20demonstrate%0Athe%20importance%20of%20pre-training%20on%20our%20automatically%20annotated%20HowToGround1M%0Adataset%20followed%20by%20fine-tuning%20on%20the%20manually%20annotated%20iGround%20dataset%20and%0Avalidate%20the%20key%20technical%20contributions%20of%20our%20model.%20The%20dataset%20and%20code%20are%0Aavailable%20at%20https%3A//ekazakos.github.io/grounded_video_caption_generation/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10781v3&entry.124074799=Read"},
{"title": "Faster, Self-Supervised Super-Resolution for Anisotropic Multi-View MRI\n  Using a Sparse Coordinate Loss", "author": "Maja Schlereth and Moritz Schillinger and Katharina Breininger", "abstract": "  Acquiring images in high resolution is often a challenging task. Especially\nin the medical sector, image quality has to be balanced with acquisition time\nand patient comfort. To strike a compromise between scan time and quality for\nMagnetic Resonance (MR) imaging, two anisotropic scans with different\nlow-resolution (LR) orientations can be acquired. Typically, LR scans are\nanalyzed individually by radiologists, which is time consuming and can lead to\ninaccurate interpretation. To tackle this, we propose a novel approach for\nfusing two orthogonal anisotropic LR MR images to reconstruct anatomical\ndetails in a unified representation. Our multi-view neural network is trained\nin a self-supervised manner, without requiring corresponding high-resolution\n(HR) data. To optimize the model, we introduce a sparse coordinate-based loss,\nenabling the integration of LR images with arbitrary scaling. We evaluate our\nmethod on MR images from two independent cohorts. Our results demonstrate\ncomparable or even improved super-resolution (SR) performance compared to\nstate-of-the-art (SOTA) self-supervised SR methods for different upsampling\nscales. By combining a patient-agnostic offline and a patient-specific online\nphase, we achieve a substantial speed-up of up to ten times for\npatient-specific reconstruction while achieving similar or better SR quality.\nCode is available at https://github.com/MajaSchle/tripleSR.\n", "link": "http://arxiv.org/abs/2509.07798v1", "date": "2025-09-09", "relevancy": 2.7625, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5695}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.546}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster%2C%20Self-Supervised%20Super-Resolution%20for%20Anisotropic%20Multi-View%20MRI%0A%20%20Using%20a%20Sparse%20Coordinate%20Loss&body=Title%3A%20Faster%2C%20Self-Supervised%20Super-Resolution%20for%20Anisotropic%20Multi-View%20MRI%0A%20%20Using%20a%20Sparse%20Coordinate%20Loss%0AAuthor%3A%20Maja%20Schlereth%20and%20Moritz%20Schillinger%20and%20Katharina%20Breininger%0AAbstract%3A%20%20%20Acquiring%20images%20in%20high%20resolution%20is%20often%20a%20challenging%20task.%20Especially%0Ain%20the%20medical%20sector%2C%20image%20quality%20has%20to%20be%20balanced%20with%20acquisition%20time%0Aand%20patient%20comfort.%20To%20strike%20a%20compromise%20between%20scan%20time%20and%20quality%20for%0AMagnetic%20Resonance%20%28MR%29%20imaging%2C%20two%20anisotropic%20scans%20with%20different%0Alow-resolution%20%28LR%29%20orientations%20can%20be%20acquired.%20Typically%2C%20LR%20scans%20are%0Aanalyzed%20individually%20by%20radiologists%2C%20which%20is%20time%20consuming%20and%20can%20lead%20to%0Ainaccurate%20interpretation.%20To%20tackle%20this%2C%20we%20propose%20a%20novel%20approach%20for%0Afusing%20two%20orthogonal%20anisotropic%20LR%20MR%20images%20to%20reconstruct%20anatomical%0Adetails%20in%20a%20unified%20representation.%20Our%20multi-view%20neural%20network%20is%20trained%0Ain%20a%20self-supervised%20manner%2C%20without%20requiring%20corresponding%20high-resolution%0A%28HR%29%20data.%20To%20optimize%20the%20model%2C%20we%20introduce%20a%20sparse%20coordinate-based%20loss%2C%0Aenabling%20the%20integration%20of%20LR%20images%20with%20arbitrary%20scaling.%20We%20evaluate%20our%0Amethod%20on%20MR%20images%20from%20two%20independent%20cohorts.%20Our%20results%20demonstrate%0Acomparable%20or%20even%20improved%20super-resolution%20%28SR%29%20performance%20compared%20to%0Astate-of-the-art%20%28SOTA%29%20self-supervised%20SR%20methods%20for%20different%20upsampling%0Ascales.%20By%20combining%20a%20patient-agnostic%20offline%20and%20a%20patient-specific%20online%0Aphase%2C%20we%20achieve%20a%20substantial%20speed-up%20of%20up%20to%20ten%20times%20for%0Apatient-specific%20reconstruction%20while%20achieving%20similar%20or%20better%20SR%20quality.%0ACode%20is%20available%20at%20https%3A//github.com/MajaSchle/tripleSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster%252C%2520Self-Supervised%2520Super-Resolution%2520for%2520Anisotropic%2520Multi-View%2520MRI%250A%2520%2520Using%2520a%2520Sparse%2520Coordinate%2520Loss%26entry.906535625%3DMaja%2520Schlereth%2520and%2520Moritz%2520Schillinger%2520and%2520Katharina%2520Breininger%26entry.1292438233%3D%2520%2520Acquiring%2520images%2520in%2520high%2520resolution%2520is%2520often%2520a%2520challenging%2520task.%2520Especially%250Ain%2520the%2520medical%2520sector%252C%2520image%2520quality%2520has%2520to%2520be%2520balanced%2520with%2520acquisition%2520time%250Aand%2520patient%2520comfort.%2520To%2520strike%2520a%2520compromise%2520between%2520scan%2520time%2520and%2520quality%2520for%250AMagnetic%2520Resonance%2520%2528MR%2529%2520imaging%252C%2520two%2520anisotropic%2520scans%2520with%2520different%250Alow-resolution%2520%2528LR%2529%2520orientations%2520can%2520be%2520acquired.%2520Typically%252C%2520LR%2520scans%2520are%250Aanalyzed%2520individually%2520by%2520radiologists%252C%2520which%2520is%2520time%2520consuming%2520and%2520can%2520lead%2520to%250Ainaccurate%2520interpretation.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%250Afusing%2520two%2520orthogonal%2520anisotropic%2520LR%2520MR%2520images%2520to%2520reconstruct%2520anatomical%250Adetails%2520in%2520a%2520unified%2520representation.%2520Our%2520multi-view%2520neural%2520network%2520is%2520trained%250Ain%2520a%2520self-supervised%2520manner%252C%2520without%2520requiring%2520corresponding%2520high-resolution%250A%2528HR%2529%2520data.%2520To%2520optimize%2520the%2520model%252C%2520we%2520introduce%2520a%2520sparse%2520coordinate-based%2520loss%252C%250Aenabling%2520the%2520integration%2520of%2520LR%2520images%2520with%2520arbitrary%2520scaling.%2520We%2520evaluate%2520our%250Amethod%2520on%2520MR%2520images%2520from%2520two%2520independent%2520cohorts.%2520Our%2520results%2520demonstrate%250Acomparable%2520or%2520even%2520improved%2520super-resolution%2520%2528SR%2529%2520performance%2520compared%2520to%250Astate-of-the-art%2520%2528SOTA%2529%2520self-supervised%2520SR%2520methods%2520for%2520different%2520upsampling%250Ascales.%2520By%2520combining%2520a%2520patient-agnostic%2520offline%2520and%2520a%2520patient-specific%2520online%250Aphase%252C%2520we%2520achieve%2520a%2520substantial%2520speed-up%2520of%2520up%2520to%2520ten%2520times%2520for%250Apatient-specific%2520reconstruction%2520while%2520achieving%2520similar%2520or%2520better%2520SR%2520quality.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/MajaSchle/tripleSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%2C%20Self-Supervised%20Super-Resolution%20for%20Anisotropic%20Multi-View%20MRI%0A%20%20Using%20a%20Sparse%20Coordinate%20Loss&entry.906535625=Maja%20Schlereth%20and%20Moritz%20Schillinger%20and%20Katharina%20Breininger&entry.1292438233=%20%20Acquiring%20images%20in%20high%20resolution%20is%20often%20a%20challenging%20task.%20Especially%0Ain%20the%20medical%20sector%2C%20image%20quality%20has%20to%20be%20balanced%20with%20acquisition%20time%0Aand%20patient%20comfort.%20To%20strike%20a%20compromise%20between%20scan%20time%20and%20quality%20for%0AMagnetic%20Resonance%20%28MR%29%20imaging%2C%20two%20anisotropic%20scans%20with%20different%0Alow-resolution%20%28LR%29%20orientations%20can%20be%20acquired.%20Typically%2C%20LR%20scans%20are%0Aanalyzed%20individually%20by%20radiologists%2C%20which%20is%20time%20consuming%20and%20can%20lead%20to%0Ainaccurate%20interpretation.%20To%20tackle%20this%2C%20we%20propose%20a%20novel%20approach%20for%0Afusing%20two%20orthogonal%20anisotropic%20LR%20MR%20images%20to%20reconstruct%20anatomical%0Adetails%20in%20a%20unified%20representation.%20Our%20multi-view%20neural%20network%20is%20trained%0Ain%20a%20self-supervised%20manner%2C%20without%20requiring%20corresponding%20high-resolution%0A%28HR%29%20data.%20To%20optimize%20the%20model%2C%20we%20introduce%20a%20sparse%20coordinate-based%20loss%2C%0Aenabling%20the%20integration%20of%20LR%20images%20with%20arbitrary%20scaling.%20We%20evaluate%20our%0Amethod%20on%20MR%20images%20from%20two%20independent%20cohorts.%20Our%20results%20demonstrate%0Acomparable%20or%20even%20improved%20super-resolution%20%28SR%29%20performance%20compared%20to%0Astate-of-the-art%20%28SOTA%29%20self-supervised%20SR%20methods%20for%20different%20upsampling%0Ascales.%20By%20combining%20a%20patient-agnostic%20offline%20and%20a%20patient-specific%20online%0Aphase%2C%20we%20achieve%20a%20substantial%20speed-up%20of%20up%20to%20ten%20times%20for%0Apatient-specific%20reconstruction%20while%20achieving%20similar%20or%20better%20SR%20quality.%0ACode%20is%20available%20at%20https%3A//github.com/MajaSchle/tripleSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07798v1&entry.124074799=Read"},
{"title": "Evolving from Unknown to Known: Retentive Angular Representation\n  Learning for Incremental Open Set Recognition", "author": "Runqing Yang and Yimin Fu and Changyuan Wu and Zhunga Liu", "abstract": "  Existing open set recognition (OSR) methods are typically designed for static\nscenarios, where models aim to classify known classes and identify unknown ones\nwithin fixed scopes. This deviates from the expectation that the model should\nincrementally identify newly emerging unknown classes from continuous data\nstreams and acquire corresponding knowledge. In such evolving scenarios, the\ndiscriminability of OSR decision boundaries is hard to maintain due to\nrestricted access to former training data, causing severe inter-class\nconfusion. To solve this problem, we propose retentive angular representation\nlearning (RARL) for incremental open set recognition (IOSR). In RARL, unknown\nrepresentations are encouraged to align around inactive prototypes within an\nangular space constructed under the equiangular tight frame, thereby mitigating\nexcessive representation drift during knowledge updates. Specifically, we adopt\na virtual-intrinsic interactive (VII) training strategy, which compacts known\nrepresentations by enforcing clear inter-class margins through\nboundary-proximal virtual classes. Furthermore, a stratified rectification\nstrategy is designed to refine decision boundaries, mitigating representation\nbias and feature space distortion caused by imbalances between old/new and\npositive/negative class samples. We conduct thorough evaluations on CIFAR100\nand TinyImageNet datasets and establish a new benchmark for IOSR. Experimental\nresults across various task setups demonstrate that the proposed method\nachieves state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2509.06570v2", "date": "2025-09-09", "relevancy": 2.7552, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5784}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5435}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evolving%20from%20Unknown%20to%20Known%3A%20Retentive%20Angular%20Representation%0A%20%20Learning%20for%20Incremental%20Open%20Set%20Recognition&body=Title%3A%20Evolving%20from%20Unknown%20to%20Known%3A%20Retentive%20Angular%20Representation%0A%20%20Learning%20for%20Incremental%20Open%20Set%20Recognition%0AAuthor%3A%20Runqing%20Yang%20and%20Yimin%20Fu%20and%20Changyuan%20Wu%20and%20Zhunga%20Liu%0AAbstract%3A%20%20%20Existing%20open%20set%20recognition%20%28OSR%29%20methods%20are%20typically%20designed%20for%20static%0Ascenarios%2C%20where%20models%20aim%20to%20classify%20known%20classes%20and%20identify%20unknown%20ones%0Awithin%20fixed%20scopes.%20This%20deviates%20from%20the%20expectation%20that%20the%20model%20should%0Aincrementally%20identify%20newly%20emerging%20unknown%20classes%20from%20continuous%20data%0Astreams%20and%20acquire%20corresponding%20knowledge.%20In%20such%20evolving%20scenarios%2C%20the%0Adiscriminability%20of%20OSR%20decision%20boundaries%20is%20hard%20to%20maintain%20due%20to%0Arestricted%20access%20to%20former%20training%20data%2C%20causing%20severe%20inter-class%0Aconfusion.%20To%20solve%20this%20problem%2C%20we%20propose%20retentive%20angular%20representation%0Alearning%20%28RARL%29%20for%20incremental%20open%20set%20recognition%20%28IOSR%29.%20In%20RARL%2C%20unknown%0Arepresentations%20are%20encouraged%20to%20align%20around%20inactive%20prototypes%20within%20an%0Aangular%20space%20constructed%20under%20the%20equiangular%20tight%20frame%2C%20thereby%20mitigating%0Aexcessive%20representation%20drift%20during%20knowledge%20updates.%20Specifically%2C%20we%20adopt%0Aa%20virtual-intrinsic%20interactive%20%28VII%29%20training%20strategy%2C%20which%20compacts%20known%0Arepresentations%20by%20enforcing%20clear%20inter-class%20margins%20through%0Aboundary-proximal%20virtual%20classes.%20Furthermore%2C%20a%20stratified%20rectification%0Astrategy%20is%20designed%20to%20refine%20decision%20boundaries%2C%20mitigating%20representation%0Abias%20and%20feature%20space%20distortion%20caused%20by%20imbalances%20between%20old/new%20and%0Apositive/negative%20class%20samples.%20We%20conduct%20thorough%20evaluations%20on%20CIFAR100%0Aand%20TinyImageNet%20datasets%20and%20establish%20a%20new%20benchmark%20for%20IOSR.%20Experimental%0Aresults%20across%20various%20task%20setups%20demonstrate%20that%20the%20proposed%20method%0Aachieves%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06570v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolving%2520from%2520Unknown%2520to%2520Known%253A%2520Retentive%2520Angular%2520Representation%250A%2520%2520Learning%2520for%2520Incremental%2520Open%2520Set%2520Recognition%26entry.906535625%3DRunqing%2520Yang%2520and%2520Yimin%2520Fu%2520and%2520Changyuan%2520Wu%2520and%2520Zhunga%2520Liu%26entry.1292438233%3D%2520%2520Existing%2520open%2520set%2520recognition%2520%2528OSR%2529%2520methods%2520are%2520typically%2520designed%2520for%2520static%250Ascenarios%252C%2520where%2520models%2520aim%2520to%2520classify%2520known%2520classes%2520and%2520identify%2520unknown%2520ones%250Awithin%2520fixed%2520scopes.%2520This%2520deviates%2520from%2520the%2520expectation%2520that%2520the%2520model%2520should%250Aincrementally%2520identify%2520newly%2520emerging%2520unknown%2520classes%2520from%2520continuous%2520data%250Astreams%2520and%2520acquire%2520corresponding%2520knowledge.%2520In%2520such%2520evolving%2520scenarios%252C%2520the%250Adiscriminability%2520of%2520OSR%2520decision%2520boundaries%2520is%2520hard%2520to%2520maintain%2520due%2520to%250Arestricted%2520access%2520to%2520former%2520training%2520data%252C%2520causing%2520severe%2520inter-class%250Aconfusion.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%2520retentive%2520angular%2520representation%250Alearning%2520%2528RARL%2529%2520for%2520incremental%2520open%2520set%2520recognition%2520%2528IOSR%2529.%2520In%2520RARL%252C%2520unknown%250Arepresentations%2520are%2520encouraged%2520to%2520align%2520around%2520inactive%2520prototypes%2520within%2520an%250Aangular%2520space%2520constructed%2520under%2520the%2520equiangular%2520tight%2520frame%252C%2520thereby%2520mitigating%250Aexcessive%2520representation%2520drift%2520during%2520knowledge%2520updates.%2520Specifically%252C%2520we%2520adopt%250Aa%2520virtual-intrinsic%2520interactive%2520%2528VII%2529%2520training%2520strategy%252C%2520which%2520compacts%2520known%250Arepresentations%2520by%2520enforcing%2520clear%2520inter-class%2520margins%2520through%250Aboundary-proximal%2520virtual%2520classes.%2520Furthermore%252C%2520a%2520stratified%2520rectification%250Astrategy%2520is%2520designed%2520to%2520refine%2520decision%2520boundaries%252C%2520mitigating%2520representation%250Abias%2520and%2520feature%2520space%2520distortion%2520caused%2520by%2520imbalances%2520between%2520old/new%2520and%250Apositive/negative%2520class%2520samples.%2520We%2520conduct%2520thorough%2520evaluations%2520on%2520CIFAR100%250Aand%2520TinyImageNet%2520datasets%2520and%2520establish%2520a%2520new%2520benchmark%2520for%2520IOSR.%2520Experimental%250Aresults%2520across%2520various%2520task%2520setups%2520demonstrate%2520that%2520the%2520proposed%2520method%250Aachieves%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06570v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolving%20from%20Unknown%20to%20Known%3A%20Retentive%20Angular%20Representation%0A%20%20Learning%20for%20Incremental%20Open%20Set%20Recognition&entry.906535625=Runqing%20Yang%20and%20Yimin%20Fu%20and%20Changyuan%20Wu%20and%20Zhunga%20Liu&entry.1292438233=%20%20Existing%20open%20set%20recognition%20%28OSR%29%20methods%20are%20typically%20designed%20for%20static%0Ascenarios%2C%20where%20models%20aim%20to%20classify%20known%20classes%20and%20identify%20unknown%20ones%0Awithin%20fixed%20scopes.%20This%20deviates%20from%20the%20expectation%20that%20the%20model%20should%0Aincrementally%20identify%20newly%20emerging%20unknown%20classes%20from%20continuous%20data%0Astreams%20and%20acquire%20corresponding%20knowledge.%20In%20such%20evolving%20scenarios%2C%20the%0Adiscriminability%20of%20OSR%20decision%20boundaries%20is%20hard%20to%20maintain%20due%20to%0Arestricted%20access%20to%20former%20training%20data%2C%20causing%20severe%20inter-class%0Aconfusion.%20To%20solve%20this%20problem%2C%20we%20propose%20retentive%20angular%20representation%0Alearning%20%28RARL%29%20for%20incremental%20open%20set%20recognition%20%28IOSR%29.%20In%20RARL%2C%20unknown%0Arepresentations%20are%20encouraged%20to%20align%20around%20inactive%20prototypes%20within%20an%0Aangular%20space%20constructed%20under%20the%20equiangular%20tight%20frame%2C%20thereby%20mitigating%0Aexcessive%20representation%20drift%20during%20knowledge%20updates.%20Specifically%2C%20we%20adopt%0Aa%20virtual-intrinsic%20interactive%20%28VII%29%20training%20strategy%2C%20which%20compacts%20known%0Arepresentations%20by%20enforcing%20clear%20inter-class%20margins%20through%0Aboundary-proximal%20virtual%20classes.%20Furthermore%2C%20a%20stratified%20rectification%0Astrategy%20is%20designed%20to%20refine%20decision%20boundaries%2C%20mitigating%20representation%0Abias%20and%20feature%20space%20distortion%20caused%20by%20imbalances%20between%20old/new%20and%0Apositive/negative%20class%20samples.%20We%20conduct%20thorough%20evaluations%20on%20CIFAR100%0Aand%20TinyImageNet%20datasets%20and%20establish%20a%20new%20benchmark%20for%20IOSR.%20Experimental%0Aresults%20across%20various%20task%20setups%20demonstrate%20that%20the%20proposed%20method%0Aachieves%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06570v2&entry.124074799=Read"},
{"title": "Attention Maps in 3D Shape Classification for Dental Stage Estimation\n  with Class Node Graph Attention Networks", "author": "Barkin Buyukcakir and Rocharles Cavalcante Fontenele and Reinhilde Jacobs and Jannick De Tobel and Patrick Thevissen and Dirk Vandermeulen and Peter Claes", "abstract": "  Deep learning offers a promising avenue for automating many recognition tasks\nin fields such as medicine and forensics. However, the black-box nature of\nthese models hinders their adoption in high-stakes applications where trust and\naccountability are required. For 3D shape recognition tasks in particular, this\npaper introduces the Class Node Graph Attention Network (CGAT) architecture to\naddress this need. Applied to 3D meshes of third molars derived from CBCT\nimages, for Demirjian stage allocation, CGAT utilizes graph attention\nconvolutions and an inherent attention mechanism, visualized via attention\nrollout, to explain its decision-making process. We evaluated the local mean\ncurvature and distance to centroid node features, both individually and in\ncombination, as well as model depth, finding that models incorporating directed\nedges to a global CLS node produced more intuitive attention maps, while also\nyielding desirable classification performance. We analyzed the attention-based\nexplanations of the models, and their predictive performances to propose\noptimal settings for the CGAT. The combination of local mean curvature and\ndistance to centroid as node features yielded a slight performance increase\nwith 0.76 weighted F1 score, and more comprehensive attention visualizations.\nThe CGAT architecture's ability to generate human-understandable attention maps\ncan enhance trust and facilitate expert validation of model decisions. While\ndemonstrated on dental data, CGAT is broadly applicable to graph-based\nclassification and regression tasks, promoting wider adoption of transparent\nand competitive deep learning models in high-stakes environments.\n", "link": "http://arxiv.org/abs/2509.07581v1", "date": "2025-09-09", "relevancy": 2.6765, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5667}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5196}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Maps%20in%203D%20Shape%20Classification%20for%20Dental%20Stage%20Estimation%0A%20%20with%20Class%20Node%20Graph%20Attention%20Networks&body=Title%3A%20Attention%20Maps%20in%203D%20Shape%20Classification%20for%20Dental%20Stage%20Estimation%0A%20%20with%20Class%20Node%20Graph%20Attention%20Networks%0AAuthor%3A%20Barkin%20Buyukcakir%20and%20Rocharles%20Cavalcante%20Fontenele%20and%20Reinhilde%20Jacobs%20and%20Jannick%20De%20Tobel%20and%20Patrick%20Thevissen%20and%20Dirk%20Vandermeulen%20and%20Peter%20Claes%0AAbstract%3A%20%20%20Deep%20learning%20offers%20a%20promising%20avenue%20for%20automating%20many%20recognition%20tasks%0Ain%20fields%20such%20as%20medicine%20and%20forensics.%20However%2C%20the%20black-box%20nature%20of%0Athese%20models%20hinders%20their%20adoption%20in%20high-stakes%20applications%20where%20trust%20and%0Aaccountability%20are%20required.%20For%203D%20shape%20recognition%20tasks%20in%20particular%2C%20this%0Apaper%20introduces%20the%20Class%20Node%20Graph%20Attention%20Network%20%28CGAT%29%20architecture%20to%0Aaddress%20this%20need.%20Applied%20to%203D%20meshes%20of%20third%20molars%20derived%20from%20CBCT%0Aimages%2C%20for%20Demirjian%20stage%20allocation%2C%20CGAT%20utilizes%20graph%20attention%0Aconvolutions%20and%20an%20inherent%20attention%20mechanism%2C%20visualized%20via%20attention%0Arollout%2C%20to%20explain%20its%20decision-making%20process.%20We%20evaluated%20the%20local%20mean%0Acurvature%20and%20distance%20to%20centroid%20node%20features%2C%20both%20individually%20and%20in%0Acombination%2C%20as%20well%20as%20model%20depth%2C%20finding%20that%20models%20incorporating%20directed%0Aedges%20to%20a%20global%20CLS%20node%20produced%20more%20intuitive%20attention%20maps%2C%20while%20also%0Ayielding%20desirable%20classification%20performance.%20We%20analyzed%20the%20attention-based%0Aexplanations%20of%20the%20models%2C%20and%20their%20predictive%20performances%20to%20propose%0Aoptimal%20settings%20for%20the%20CGAT.%20The%20combination%20of%20local%20mean%20curvature%20and%0Adistance%20to%20centroid%20as%20node%20features%20yielded%20a%20slight%20performance%20increase%0Awith%200.76%20weighted%20F1%20score%2C%20and%20more%20comprehensive%20attention%20visualizations.%0AThe%20CGAT%20architecture%27s%20ability%20to%20generate%20human-understandable%20attention%20maps%0Acan%20enhance%20trust%20and%20facilitate%20expert%20validation%20of%20model%20decisions.%20While%0Ademonstrated%20on%20dental%20data%2C%20CGAT%20is%20broadly%20applicable%20to%20graph-based%0Aclassification%20and%20regression%20tasks%2C%20promoting%20wider%20adoption%20of%20transparent%0Aand%20competitive%20deep%20learning%20models%20in%20high-stakes%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Maps%2520in%25203D%2520Shape%2520Classification%2520for%2520Dental%2520Stage%2520Estimation%250A%2520%2520with%2520Class%2520Node%2520Graph%2520Attention%2520Networks%26entry.906535625%3DBarkin%2520Buyukcakir%2520and%2520Rocharles%2520Cavalcante%2520Fontenele%2520and%2520Reinhilde%2520Jacobs%2520and%2520Jannick%2520De%2520Tobel%2520and%2520Patrick%2520Thevissen%2520and%2520Dirk%2520Vandermeulen%2520and%2520Peter%2520Claes%26entry.1292438233%3D%2520%2520Deep%2520learning%2520offers%2520a%2520promising%2520avenue%2520for%2520automating%2520many%2520recognition%2520tasks%250Ain%2520fields%2520such%2520as%2520medicine%2520and%2520forensics.%2520However%252C%2520the%2520black-box%2520nature%2520of%250Athese%2520models%2520hinders%2520their%2520adoption%2520in%2520high-stakes%2520applications%2520where%2520trust%2520and%250Aaccountability%2520are%2520required.%2520For%25203D%2520shape%2520recognition%2520tasks%2520in%2520particular%252C%2520this%250Apaper%2520introduces%2520the%2520Class%2520Node%2520Graph%2520Attention%2520Network%2520%2528CGAT%2529%2520architecture%2520to%250Aaddress%2520this%2520need.%2520Applied%2520to%25203D%2520meshes%2520of%2520third%2520molars%2520derived%2520from%2520CBCT%250Aimages%252C%2520for%2520Demirjian%2520stage%2520allocation%252C%2520CGAT%2520utilizes%2520graph%2520attention%250Aconvolutions%2520and%2520an%2520inherent%2520attention%2520mechanism%252C%2520visualized%2520via%2520attention%250Arollout%252C%2520to%2520explain%2520its%2520decision-making%2520process.%2520We%2520evaluated%2520the%2520local%2520mean%250Acurvature%2520and%2520distance%2520to%2520centroid%2520node%2520features%252C%2520both%2520individually%2520and%2520in%250Acombination%252C%2520as%2520well%2520as%2520model%2520depth%252C%2520finding%2520that%2520models%2520incorporating%2520directed%250Aedges%2520to%2520a%2520global%2520CLS%2520node%2520produced%2520more%2520intuitive%2520attention%2520maps%252C%2520while%2520also%250Ayielding%2520desirable%2520classification%2520performance.%2520We%2520analyzed%2520the%2520attention-based%250Aexplanations%2520of%2520the%2520models%252C%2520and%2520their%2520predictive%2520performances%2520to%2520propose%250Aoptimal%2520settings%2520for%2520the%2520CGAT.%2520The%2520combination%2520of%2520local%2520mean%2520curvature%2520and%250Adistance%2520to%2520centroid%2520as%2520node%2520features%2520yielded%2520a%2520slight%2520performance%2520increase%250Awith%25200.76%2520weighted%2520F1%2520score%252C%2520and%2520more%2520comprehensive%2520attention%2520visualizations.%250AThe%2520CGAT%2520architecture%2527s%2520ability%2520to%2520generate%2520human-understandable%2520attention%2520maps%250Acan%2520enhance%2520trust%2520and%2520facilitate%2520expert%2520validation%2520of%2520model%2520decisions.%2520While%250Ademonstrated%2520on%2520dental%2520data%252C%2520CGAT%2520is%2520broadly%2520applicable%2520to%2520graph-based%250Aclassification%2520and%2520regression%2520tasks%252C%2520promoting%2520wider%2520adoption%2520of%2520transparent%250Aand%2520competitive%2520deep%2520learning%2520models%2520in%2520high-stakes%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Maps%20in%203D%20Shape%20Classification%20for%20Dental%20Stage%20Estimation%0A%20%20with%20Class%20Node%20Graph%20Attention%20Networks&entry.906535625=Barkin%20Buyukcakir%20and%20Rocharles%20Cavalcante%20Fontenele%20and%20Reinhilde%20Jacobs%20and%20Jannick%20De%20Tobel%20and%20Patrick%20Thevissen%20and%20Dirk%20Vandermeulen%20and%20Peter%20Claes&entry.1292438233=%20%20Deep%20learning%20offers%20a%20promising%20avenue%20for%20automating%20many%20recognition%20tasks%0Ain%20fields%20such%20as%20medicine%20and%20forensics.%20However%2C%20the%20black-box%20nature%20of%0Athese%20models%20hinders%20their%20adoption%20in%20high-stakes%20applications%20where%20trust%20and%0Aaccountability%20are%20required.%20For%203D%20shape%20recognition%20tasks%20in%20particular%2C%20this%0Apaper%20introduces%20the%20Class%20Node%20Graph%20Attention%20Network%20%28CGAT%29%20architecture%20to%0Aaddress%20this%20need.%20Applied%20to%203D%20meshes%20of%20third%20molars%20derived%20from%20CBCT%0Aimages%2C%20for%20Demirjian%20stage%20allocation%2C%20CGAT%20utilizes%20graph%20attention%0Aconvolutions%20and%20an%20inherent%20attention%20mechanism%2C%20visualized%20via%20attention%0Arollout%2C%20to%20explain%20its%20decision-making%20process.%20We%20evaluated%20the%20local%20mean%0Acurvature%20and%20distance%20to%20centroid%20node%20features%2C%20both%20individually%20and%20in%0Acombination%2C%20as%20well%20as%20model%20depth%2C%20finding%20that%20models%20incorporating%20directed%0Aedges%20to%20a%20global%20CLS%20node%20produced%20more%20intuitive%20attention%20maps%2C%20while%20also%0Ayielding%20desirable%20classification%20performance.%20We%20analyzed%20the%20attention-based%0Aexplanations%20of%20the%20models%2C%20and%20their%20predictive%20performances%20to%20propose%0Aoptimal%20settings%20for%20the%20CGAT.%20The%20combination%20of%20local%20mean%20curvature%20and%0Adistance%20to%20centroid%20as%20node%20features%20yielded%20a%20slight%20performance%20increase%0Awith%200.76%20weighted%20F1%20score%2C%20and%20more%20comprehensive%20attention%20visualizations.%0AThe%20CGAT%20architecture%27s%20ability%20to%20generate%20human-understandable%20attention%20maps%0Acan%20enhance%20trust%20and%20facilitate%20expert%20validation%20of%20model%20decisions.%20While%0Ademonstrated%20on%20dental%20data%2C%20CGAT%20is%20broadly%20applicable%20to%20graph-based%0Aclassification%20and%20regression%20tasks%2C%20promoting%20wider%20adoption%20of%20transparent%0Aand%20competitive%20deep%20learning%20models%20in%20high-stakes%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07581v1&entry.124074799=Read"},
{"title": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images", "author": "Boammani Aser Lompo and Marc Haraoui", "abstract": "  Visual reasoning over structured data such as tables is a critical capability\nfor modern vision-language models (VLMs), yet current benchmarks remain limited\nin scale, diversity, or reasoning depth, especially when it comes to rendered\ntable images. Addressing this gap, we introduce Visual-TableQA, a large-scale,\nopen-domain multimodal dataset specifically designed to evaluate and enhance\nvisual reasoning over complex tabular data. Our generation pipeline is modular,\nscalable, and fully autonomous, involving multiple reasoning LLMs collaborating\nacross distinct roles: generation, validation, and inspiration. Visual-TableQA\ncomprises 2.5k richly structured LaTeX-rendered tables and 6k\nreasoning-intensive QA pairs, all produced at a cost of under USD 100. To\npromote diversity and creativity, our pipeline performs multi-model\ncollaborative data generation via cross-model prompting ('inspiration') and\nLLM-jury filtering. Stronger models seed layouts and topics that weaker models\nelaborate, collectively distilling diverse reasoning patterns and visual\nstructures into the dataset. Empirical results show that models fine-tuned on\nVisual-TableQA generalize robustly to external benchmarks, outperforming\nseveral proprietary models despite the dataset's synthetic nature. The full\npipeline and resources are publicly available at\nhttps://github.com/AI-4-Everyone/Visual-TableQA.\n", "link": "http://arxiv.org/abs/2509.07966v1", "date": "2025-09-09", "relevancy": 2.6677, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.542}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual-TableQA%3A%20Open-Domain%20Benchmark%20for%20Reasoning%20over%20Table%20Images&body=Title%3A%20Visual-TableQA%3A%20Open-Domain%20Benchmark%20for%20Reasoning%20over%20Table%20Images%0AAuthor%3A%20Boammani%20Aser%20Lompo%20and%20Marc%20Haraoui%0AAbstract%3A%20%20%20Visual%20reasoning%20over%20structured%20data%20such%20as%20tables%20is%20a%20critical%20capability%0Afor%20modern%20vision-language%20models%20%28VLMs%29%2C%20yet%20current%20benchmarks%20remain%20limited%0Ain%20scale%2C%20diversity%2C%20or%20reasoning%20depth%2C%20especially%20when%20it%20comes%20to%20rendered%0Atable%20images.%20Addressing%20this%20gap%2C%20we%20introduce%20Visual-TableQA%2C%20a%20large-scale%2C%0Aopen-domain%20multimodal%20dataset%20specifically%20designed%20to%20evaluate%20and%20enhance%0Avisual%20reasoning%20over%20complex%20tabular%20data.%20Our%20generation%20pipeline%20is%20modular%2C%0Ascalable%2C%20and%20fully%20autonomous%2C%20involving%20multiple%20reasoning%20LLMs%20collaborating%0Aacross%20distinct%20roles%3A%20generation%2C%20validation%2C%20and%20inspiration.%20Visual-TableQA%0Acomprises%202.5k%20richly%20structured%20LaTeX-rendered%20tables%20and%206k%0Areasoning-intensive%20QA%20pairs%2C%20all%20produced%20at%20a%20cost%20of%20under%20USD%20100.%20To%0Apromote%20diversity%20and%20creativity%2C%20our%20pipeline%20performs%20multi-model%0Acollaborative%20data%20generation%20via%20cross-model%20prompting%20%28%27inspiration%27%29%20and%0ALLM-jury%20filtering.%20Stronger%20models%20seed%20layouts%20and%20topics%20that%20weaker%20models%0Aelaborate%2C%20collectively%20distilling%20diverse%20reasoning%20patterns%20and%20visual%0Astructures%20into%20the%20dataset.%20Empirical%20results%20show%20that%20models%20fine-tuned%20on%0AVisual-TableQA%20generalize%20robustly%20to%20external%20benchmarks%2C%20outperforming%0Aseveral%20proprietary%20models%20despite%20the%20dataset%27s%20synthetic%20nature.%20The%20full%0Apipeline%20and%20resources%20are%20publicly%20available%20at%0Ahttps%3A//github.com/AI-4-Everyone/Visual-TableQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual-TableQA%253A%2520Open-Domain%2520Benchmark%2520for%2520Reasoning%2520over%2520Table%2520Images%26entry.906535625%3DBoammani%2520Aser%2520Lompo%2520and%2520Marc%2520Haraoui%26entry.1292438233%3D%2520%2520Visual%2520reasoning%2520over%2520structured%2520data%2520such%2520as%2520tables%2520is%2520a%2520critical%2520capability%250Afor%2520modern%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520yet%2520current%2520benchmarks%2520remain%2520limited%250Ain%2520scale%252C%2520diversity%252C%2520or%2520reasoning%2520depth%252C%2520especially%2520when%2520it%2520comes%2520to%2520rendered%250Atable%2520images.%2520Addressing%2520this%2520gap%252C%2520we%2520introduce%2520Visual-TableQA%252C%2520a%2520large-scale%252C%250Aopen-domain%2520multimodal%2520dataset%2520specifically%2520designed%2520to%2520evaluate%2520and%2520enhance%250Avisual%2520reasoning%2520over%2520complex%2520tabular%2520data.%2520Our%2520generation%2520pipeline%2520is%2520modular%252C%250Ascalable%252C%2520and%2520fully%2520autonomous%252C%2520involving%2520multiple%2520reasoning%2520LLMs%2520collaborating%250Aacross%2520distinct%2520roles%253A%2520generation%252C%2520validation%252C%2520and%2520inspiration.%2520Visual-TableQA%250Acomprises%25202.5k%2520richly%2520structured%2520LaTeX-rendered%2520tables%2520and%25206k%250Areasoning-intensive%2520QA%2520pairs%252C%2520all%2520produced%2520at%2520a%2520cost%2520of%2520under%2520USD%2520100.%2520To%250Apromote%2520diversity%2520and%2520creativity%252C%2520our%2520pipeline%2520performs%2520multi-model%250Acollaborative%2520data%2520generation%2520via%2520cross-model%2520prompting%2520%2528%2527inspiration%2527%2529%2520and%250ALLM-jury%2520filtering.%2520Stronger%2520models%2520seed%2520layouts%2520and%2520topics%2520that%2520weaker%2520models%250Aelaborate%252C%2520collectively%2520distilling%2520diverse%2520reasoning%2520patterns%2520and%2520visual%250Astructures%2520into%2520the%2520dataset.%2520Empirical%2520results%2520show%2520that%2520models%2520fine-tuned%2520on%250AVisual-TableQA%2520generalize%2520robustly%2520to%2520external%2520benchmarks%252C%2520outperforming%250Aseveral%2520proprietary%2520models%2520despite%2520the%2520dataset%2527s%2520synthetic%2520nature.%2520The%2520full%250Apipeline%2520and%2520resources%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/AI-4-Everyone/Visual-TableQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual-TableQA%3A%20Open-Domain%20Benchmark%20for%20Reasoning%20over%20Table%20Images&entry.906535625=Boammani%20Aser%20Lompo%20and%20Marc%20Haraoui&entry.1292438233=%20%20Visual%20reasoning%20over%20structured%20data%20such%20as%20tables%20is%20a%20critical%20capability%0Afor%20modern%20vision-language%20models%20%28VLMs%29%2C%20yet%20current%20benchmarks%20remain%20limited%0Ain%20scale%2C%20diversity%2C%20or%20reasoning%20depth%2C%20especially%20when%20it%20comes%20to%20rendered%0Atable%20images.%20Addressing%20this%20gap%2C%20we%20introduce%20Visual-TableQA%2C%20a%20large-scale%2C%0Aopen-domain%20multimodal%20dataset%20specifically%20designed%20to%20evaluate%20and%20enhance%0Avisual%20reasoning%20over%20complex%20tabular%20data.%20Our%20generation%20pipeline%20is%20modular%2C%0Ascalable%2C%20and%20fully%20autonomous%2C%20involving%20multiple%20reasoning%20LLMs%20collaborating%0Aacross%20distinct%20roles%3A%20generation%2C%20validation%2C%20and%20inspiration.%20Visual-TableQA%0Acomprises%202.5k%20richly%20structured%20LaTeX-rendered%20tables%20and%206k%0Areasoning-intensive%20QA%20pairs%2C%20all%20produced%20at%20a%20cost%20of%20under%20USD%20100.%20To%0Apromote%20diversity%20and%20creativity%2C%20our%20pipeline%20performs%20multi-model%0Acollaborative%20data%20generation%20via%20cross-model%20prompting%20%28%27inspiration%27%29%20and%0ALLM-jury%20filtering.%20Stronger%20models%20seed%20layouts%20and%20topics%20that%20weaker%20models%0Aelaborate%2C%20collectively%20distilling%20diverse%20reasoning%20patterns%20and%20visual%0Astructures%20into%20the%20dataset.%20Empirical%20results%20show%20that%20models%20fine-tuned%20on%0AVisual-TableQA%20generalize%20robustly%20to%20external%20benchmarks%2C%20outperforming%0Aseveral%20proprietary%20models%20despite%20the%20dataset%27s%20synthetic%20nature.%20The%20full%0Apipeline%20and%20resources%20are%20publicly%20available%20at%0Ahttps%3A//github.com/AI-4-Everyone/Visual-TableQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07966v1&entry.124074799=Read"},
{"title": "One Model for All Tasks: Leveraging Efficient World Models in Multi-Task\n  Planning", "author": "Yuan Pu and Yazhe Niu and Jia Tang and Junyu Xiong and Shuai Hu and Hongsheng Li", "abstract": "  In heterogeneous multi-task learning, tasks not only exhibit diverse\nobservation and action spaces but also vary substantially in intrinsic\ndifficulty. While conventional multi-task world models like UniZero excel in\nsingle-task settings, we find that when handling large-scale heterogeneous\nenvironments, gradient conflicts and the loss of model plasticity often\nconstrain their sample and computational efficiency. In this work, we address\nthese challenges from two perspectives: the single learning iteration and the\noverall learning process. First, we investigate the impact of key design spaces\non extending UniZero to multi-task planning. We find that a Mixture-of-Experts\n(MoE) architecture provides the most substantial performance gains by\nmitigating gradient conflicts, leading to our proposed model,\n\\textit{ScaleZero}. Second, to dynamically balance the computational load\nacross the learning process, we introduce an online, LoRA-based \\textit{dynamic\nparameter scaling} (DPS) strategy. This strategy progressively integrates LoRA\nadapters in response to task-specific progress, enabling adaptive knowledge\nretention and parameter expansion. Empirical evaluations on standard benchmarks\nsuch as Atari, DMControl (DMC), and Jericho demonstrate that ScaleZero, relying\nexclusively on online reinforcement learning with one model, attains\nperformance on par with specialized single-task baselines. Furthermore, when\naugmented with our dynamic parameter scaling strategy, our method achieves\ncompetitive performance while requiring only 80\\% of the single-task\nenvironment interaction steps. These findings underscore the potential of\nScaleZero for effective large-scale multi-task learning. Our code is available\nat \\textcolor{magenta}{https://github.com/opendilab/LightZero}.\n", "link": "http://arxiv.org/abs/2509.07945v1", "date": "2025-09-09", "relevancy": 2.6641, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5272}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Model%20for%20All%20Tasks%3A%20Leveraging%20Efficient%20World%20Models%20in%20Multi-Task%0A%20%20Planning&body=Title%3A%20One%20Model%20for%20All%20Tasks%3A%20Leveraging%20Efficient%20World%20Models%20in%20Multi-Task%0A%20%20Planning%0AAuthor%3A%20Yuan%20Pu%20and%20Yazhe%20Niu%20and%20Jia%20Tang%20and%20Junyu%20Xiong%20and%20Shuai%20Hu%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20In%20heterogeneous%20multi-task%20learning%2C%20tasks%20not%20only%20exhibit%20diverse%0Aobservation%20and%20action%20spaces%20but%20also%20vary%20substantially%20in%20intrinsic%0Adifficulty.%20While%20conventional%20multi-task%20world%20models%20like%20UniZero%20excel%20in%0Asingle-task%20settings%2C%20we%20find%20that%20when%20handling%20large-scale%20heterogeneous%0Aenvironments%2C%20gradient%20conflicts%20and%20the%20loss%20of%20model%20plasticity%20often%0Aconstrain%20their%20sample%20and%20computational%20efficiency.%20In%20this%20work%2C%20we%20address%0Athese%20challenges%20from%20two%20perspectives%3A%20the%20single%20learning%20iteration%20and%20the%0Aoverall%20learning%20process.%20First%2C%20we%20investigate%20the%20impact%20of%20key%20design%20spaces%0Aon%20extending%20UniZero%20to%20multi-task%20planning.%20We%20find%20that%20a%20Mixture-of-Experts%0A%28MoE%29%20architecture%20provides%20the%20most%20substantial%20performance%20gains%20by%0Amitigating%20gradient%20conflicts%2C%20leading%20to%20our%20proposed%20model%2C%0A%5Ctextit%7BScaleZero%7D.%20Second%2C%20to%20dynamically%20balance%20the%20computational%20load%0Aacross%20the%20learning%20process%2C%20we%20introduce%20an%20online%2C%20LoRA-based%20%5Ctextit%7Bdynamic%0Aparameter%20scaling%7D%20%28DPS%29%20strategy.%20This%20strategy%20progressively%20integrates%20LoRA%0Aadapters%20in%20response%20to%20task-specific%20progress%2C%20enabling%20adaptive%20knowledge%0Aretention%20and%20parameter%20expansion.%20Empirical%20evaluations%20on%20standard%20benchmarks%0Asuch%20as%20Atari%2C%20DMControl%20%28DMC%29%2C%20and%20Jericho%20demonstrate%20that%20ScaleZero%2C%20relying%0Aexclusively%20on%20online%20reinforcement%20learning%20with%20one%20model%2C%20attains%0Aperformance%20on%20par%20with%20specialized%20single-task%20baselines.%20Furthermore%2C%20when%0Aaugmented%20with%20our%20dynamic%20parameter%20scaling%20strategy%2C%20our%20method%20achieves%0Acompetitive%20performance%20while%20requiring%20only%2080%5C%25%20of%20the%20single-task%0Aenvironment%20interaction%20steps.%20These%20findings%20underscore%20the%20potential%20of%0AScaleZero%20for%20effective%20large-scale%20multi-task%20learning.%20Our%20code%20is%20available%0Aat%20%5Ctextcolor%7Bmagenta%7D%7Bhttps%3A//github.com/opendilab/LightZero%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Model%2520for%2520All%2520Tasks%253A%2520Leveraging%2520Efficient%2520World%2520Models%2520in%2520Multi-Task%250A%2520%2520Planning%26entry.906535625%3DYuan%2520Pu%2520and%2520Yazhe%2520Niu%2520and%2520Jia%2520Tang%2520and%2520Junyu%2520Xiong%2520and%2520Shuai%2520Hu%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520In%2520heterogeneous%2520multi-task%2520learning%252C%2520tasks%2520not%2520only%2520exhibit%2520diverse%250Aobservation%2520and%2520action%2520spaces%2520but%2520also%2520vary%2520substantially%2520in%2520intrinsic%250Adifficulty.%2520While%2520conventional%2520multi-task%2520world%2520models%2520like%2520UniZero%2520excel%2520in%250Asingle-task%2520settings%252C%2520we%2520find%2520that%2520when%2520handling%2520large-scale%2520heterogeneous%250Aenvironments%252C%2520gradient%2520conflicts%2520and%2520the%2520loss%2520of%2520model%2520plasticity%2520often%250Aconstrain%2520their%2520sample%2520and%2520computational%2520efficiency.%2520In%2520this%2520work%252C%2520we%2520address%250Athese%2520challenges%2520from%2520two%2520perspectives%253A%2520the%2520single%2520learning%2520iteration%2520and%2520the%250Aoverall%2520learning%2520process.%2520First%252C%2520we%2520investigate%2520the%2520impact%2520of%2520key%2520design%2520spaces%250Aon%2520extending%2520UniZero%2520to%2520multi-task%2520planning.%2520We%2520find%2520that%2520a%2520Mixture-of-Experts%250A%2528MoE%2529%2520architecture%2520provides%2520the%2520most%2520substantial%2520performance%2520gains%2520by%250Amitigating%2520gradient%2520conflicts%252C%2520leading%2520to%2520our%2520proposed%2520model%252C%250A%255Ctextit%257BScaleZero%257D.%2520Second%252C%2520to%2520dynamically%2520balance%2520the%2520computational%2520load%250Aacross%2520the%2520learning%2520process%252C%2520we%2520introduce%2520an%2520online%252C%2520LoRA-based%2520%255Ctextit%257Bdynamic%250Aparameter%2520scaling%257D%2520%2528DPS%2529%2520strategy.%2520This%2520strategy%2520progressively%2520integrates%2520LoRA%250Aadapters%2520in%2520response%2520to%2520task-specific%2520progress%252C%2520enabling%2520adaptive%2520knowledge%250Aretention%2520and%2520parameter%2520expansion.%2520Empirical%2520evaluations%2520on%2520standard%2520benchmarks%250Asuch%2520as%2520Atari%252C%2520DMControl%2520%2528DMC%2529%252C%2520and%2520Jericho%2520demonstrate%2520that%2520ScaleZero%252C%2520relying%250Aexclusively%2520on%2520online%2520reinforcement%2520learning%2520with%2520one%2520model%252C%2520attains%250Aperformance%2520on%2520par%2520with%2520specialized%2520single-task%2520baselines.%2520Furthermore%252C%2520when%250Aaugmented%2520with%2520our%2520dynamic%2520parameter%2520scaling%2520strategy%252C%2520our%2520method%2520achieves%250Acompetitive%2520performance%2520while%2520requiring%2520only%252080%255C%2525%2520of%2520the%2520single-task%250Aenvironment%2520interaction%2520steps.%2520These%2520findings%2520underscore%2520the%2520potential%2520of%250AScaleZero%2520for%2520effective%2520large-scale%2520multi-task%2520learning.%2520Our%2520code%2520is%2520available%250Aat%2520%255Ctextcolor%257Bmagenta%257D%257Bhttps%253A//github.com/opendilab/LightZero%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Model%20for%20All%20Tasks%3A%20Leveraging%20Efficient%20World%20Models%20in%20Multi-Task%0A%20%20Planning&entry.906535625=Yuan%20Pu%20and%20Yazhe%20Niu%20and%20Jia%20Tang%20and%20Junyu%20Xiong%20and%20Shuai%20Hu%20and%20Hongsheng%20Li&entry.1292438233=%20%20In%20heterogeneous%20multi-task%20learning%2C%20tasks%20not%20only%20exhibit%20diverse%0Aobservation%20and%20action%20spaces%20but%20also%20vary%20substantially%20in%20intrinsic%0Adifficulty.%20While%20conventional%20multi-task%20world%20models%20like%20UniZero%20excel%20in%0Asingle-task%20settings%2C%20we%20find%20that%20when%20handling%20large-scale%20heterogeneous%0Aenvironments%2C%20gradient%20conflicts%20and%20the%20loss%20of%20model%20plasticity%20often%0Aconstrain%20their%20sample%20and%20computational%20efficiency.%20In%20this%20work%2C%20we%20address%0Athese%20challenges%20from%20two%20perspectives%3A%20the%20single%20learning%20iteration%20and%20the%0Aoverall%20learning%20process.%20First%2C%20we%20investigate%20the%20impact%20of%20key%20design%20spaces%0Aon%20extending%20UniZero%20to%20multi-task%20planning.%20We%20find%20that%20a%20Mixture-of-Experts%0A%28MoE%29%20architecture%20provides%20the%20most%20substantial%20performance%20gains%20by%0Amitigating%20gradient%20conflicts%2C%20leading%20to%20our%20proposed%20model%2C%0A%5Ctextit%7BScaleZero%7D.%20Second%2C%20to%20dynamically%20balance%20the%20computational%20load%0Aacross%20the%20learning%20process%2C%20we%20introduce%20an%20online%2C%20LoRA-based%20%5Ctextit%7Bdynamic%0Aparameter%20scaling%7D%20%28DPS%29%20strategy.%20This%20strategy%20progressively%20integrates%20LoRA%0Aadapters%20in%20response%20to%20task-specific%20progress%2C%20enabling%20adaptive%20knowledge%0Aretention%20and%20parameter%20expansion.%20Empirical%20evaluations%20on%20standard%20benchmarks%0Asuch%20as%20Atari%2C%20DMControl%20%28DMC%29%2C%20and%20Jericho%20demonstrate%20that%20ScaleZero%2C%20relying%0Aexclusively%20on%20online%20reinforcement%20learning%20with%20one%20model%2C%20attains%0Aperformance%20on%20par%20with%20specialized%20single-task%20baselines.%20Furthermore%2C%20when%0Aaugmented%20with%20our%20dynamic%20parameter%20scaling%20strategy%2C%20our%20method%20achieves%0Acompetitive%20performance%20while%20requiring%20only%2080%5C%25%20of%20the%20single-task%0Aenvironment%20interaction%20steps.%20These%20findings%20underscore%20the%20potential%20of%0AScaleZero%20for%20effective%20large-scale%20multi-task%20learning.%20Our%20code%20is%20available%0Aat%20%5Ctextcolor%7Bmagenta%7D%7Bhttps%3A//github.com/opendilab/LightZero%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07945v1&entry.124074799=Read"},
{"title": "Self-Supervised Cross-Encoder for Neurodegenerative Disease Diagnosis", "author": "Fangqi Cheng and Yingying Zhao and Xiaochen Yang", "abstract": "  Deep learning has shown significant potential in diagnosing neurodegenerative\ndiseases from MRI data. However, most existing methods rely heavily on large\nvolumes of labeled data and often yield representations that lack\ninterpretability. To address both challenges, we propose a novel\nself-supervised cross-encoder framework that leverages the temporal continuity\nin longitudinal MRI scans for supervision. This framework disentangles learned\nrepresentations into two components: a static representation, constrained by\ncontrastive learning, which captures stable anatomical features; and a dynamic\nrepresentation, guided by input-gradient regularization, which reflects\ntemporal changes and can be effectively fine-tuned for downstream\nclassification tasks. Experimental results on the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) dataset demonstrate that our method achieves\nsuperior classification accuracy and improved interpretability. Furthermore,\nthe learned representations exhibit strong zero-shot generalization on the Open\nAccess Series of Imaging Studies (OASIS) dataset and cross-task generalization\non the Parkinson Progression Marker Initiative (PPMI) dataset. The code for the\nproposed method will be made publicly available.\n", "link": "http://arxiv.org/abs/2509.07623v1", "date": "2025-09-09", "relevancy": 2.6598, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5528}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5266}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Cross-Encoder%20for%20Neurodegenerative%20Disease%20Diagnosis&body=Title%3A%20Self-Supervised%20Cross-Encoder%20for%20Neurodegenerative%20Disease%20Diagnosis%0AAuthor%3A%20Fangqi%20Cheng%20and%20Yingying%20Zhao%20and%20Xiaochen%20Yang%0AAbstract%3A%20%20%20Deep%20learning%20has%20shown%20significant%20potential%20in%20diagnosing%20neurodegenerative%0Adiseases%20from%20MRI%20data.%20However%2C%20most%20existing%20methods%20rely%20heavily%20on%20large%0Avolumes%20of%20labeled%20data%20and%20often%20yield%20representations%20that%20lack%0Ainterpretability.%20To%20address%20both%20challenges%2C%20we%20propose%20a%20novel%0Aself-supervised%20cross-encoder%20framework%20that%20leverages%20the%20temporal%20continuity%0Ain%20longitudinal%20MRI%20scans%20for%20supervision.%20This%20framework%20disentangles%20learned%0Arepresentations%20into%20two%20components%3A%20a%20static%20representation%2C%20constrained%20by%0Acontrastive%20learning%2C%20which%20captures%20stable%20anatomical%20features%3B%20and%20a%20dynamic%0Arepresentation%2C%20guided%20by%20input-gradient%20regularization%2C%20which%20reflects%0Atemporal%20changes%20and%20can%20be%20effectively%20fine-tuned%20for%20downstream%0Aclassification%20tasks.%20Experimental%20results%20on%20the%20Alzheimer%27s%20Disease%0ANeuroimaging%20Initiative%20%28ADNI%29%20dataset%20demonstrate%20that%20our%20method%20achieves%0Asuperior%20classification%20accuracy%20and%20improved%20interpretability.%20Furthermore%2C%0Athe%20learned%20representations%20exhibit%20strong%20zero-shot%20generalization%20on%20the%20Open%0AAccess%20Series%20of%20Imaging%20Studies%20%28OASIS%29%20dataset%20and%20cross-task%20generalization%0Aon%20the%20Parkinson%20Progression%20Marker%20Initiative%20%28PPMI%29%20dataset.%20The%20code%20for%20the%0Aproposed%20method%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Cross-Encoder%2520for%2520Neurodegenerative%2520Disease%2520Diagnosis%26entry.906535625%3DFangqi%2520Cheng%2520and%2520Yingying%2520Zhao%2520and%2520Xiaochen%2520Yang%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520shown%2520significant%2520potential%2520in%2520diagnosing%2520neurodegenerative%250Adiseases%2520from%2520MRI%2520data.%2520However%252C%2520most%2520existing%2520methods%2520rely%2520heavily%2520on%2520large%250Avolumes%2520of%2520labeled%2520data%2520and%2520often%2520yield%2520representations%2520that%2520lack%250Ainterpretability.%2520To%2520address%2520both%2520challenges%252C%2520we%2520propose%2520a%2520novel%250Aself-supervised%2520cross-encoder%2520framework%2520that%2520leverages%2520the%2520temporal%2520continuity%250Ain%2520longitudinal%2520MRI%2520scans%2520for%2520supervision.%2520This%2520framework%2520disentangles%2520learned%250Arepresentations%2520into%2520two%2520components%253A%2520a%2520static%2520representation%252C%2520constrained%2520by%250Acontrastive%2520learning%252C%2520which%2520captures%2520stable%2520anatomical%2520features%253B%2520and%2520a%2520dynamic%250Arepresentation%252C%2520guided%2520by%2520input-gradient%2520regularization%252C%2520which%2520reflects%250Atemporal%2520changes%2520and%2520can%2520be%2520effectively%2520fine-tuned%2520for%2520downstream%250Aclassification%2520tasks.%2520Experimental%2520results%2520on%2520the%2520Alzheimer%2527s%2520Disease%250ANeuroimaging%2520Initiative%2520%2528ADNI%2529%2520dataset%2520demonstrate%2520that%2520our%2520method%2520achieves%250Asuperior%2520classification%2520accuracy%2520and%2520improved%2520interpretability.%2520Furthermore%252C%250Athe%2520learned%2520representations%2520exhibit%2520strong%2520zero-shot%2520generalization%2520on%2520the%2520Open%250AAccess%2520Series%2520of%2520Imaging%2520Studies%2520%2528OASIS%2529%2520dataset%2520and%2520cross-task%2520generalization%250Aon%2520the%2520Parkinson%2520Progression%2520Marker%2520Initiative%2520%2528PPMI%2529%2520dataset.%2520The%2520code%2520for%2520the%250Aproposed%2520method%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Cross-Encoder%20for%20Neurodegenerative%20Disease%20Diagnosis&entry.906535625=Fangqi%20Cheng%20and%20Yingying%20Zhao%20and%20Xiaochen%20Yang&entry.1292438233=%20%20Deep%20learning%20has%20shown%20significant%20potential%20in%20diagnosing%20neurodegenerative%0Adiseases%20from%20MRI%20data.%20However%2C%20most%20existing%20methods%20rely%20heavily%20on%20large%0Avolumes%20of%20labeled%20data%20and%20often%20yield%20representations%20that%20lack%0Ainterpretability.%20To%20address%20both%20challenges%2C%20we%20propose%20a%20novel%0Aself-supervised%20cross-encoder%20framework%20that%20leverages%20the%20temporal%20continuity%0Ain%20longitudinal%20MRI%20scans%20for%20supervision.%20This%20framework%20disentangles%20learned%0Arepresentations%20into%20two%20components%3A%20a%20static%20representation%2C%20constrained%20by%0Acontrastive%20learning%2C%20which%20captures%20stable%20anatomical%20features%3B%20and%20a%20dynamic%0Arepresentation%2C%20guided%20by%20input-gradient%20regularization%2C%20which%20reflects%0Atemporal%20changes%20and%20can%20be%20effectively%20fine-tuned%20for%20downstream%0Aclassification%20tasks.%20Experimental%20results%20on%20the%20Alzheimer%27s%20Disease%0ANeuroimaging%20Initiative%20%28ADNI%29%20dataset%20demonstrate%20that%20our%20method%20achieves%0Asuperior%20classification%20accuracy%20and%20improved%20interpretability.%20Furthermore%2C%0Athe%20learned%20representations%20exhibit%20strong%20zero-shot%20generalization%20on%20the%20Open%0AAccess%20Series%20of%20Imaging%20Studies%20%28OASIS%29%20dataset%20and%20cross-task%20generalization%0Aon%20the%20Parkinson%20Progression%20Marker%20Initiative%20%28PPMI%29%20dataset.%20The%20code%20for%20the%0Aproposed%20method%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07623v1&entry.124074799=Read"},
{"title": "Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth\n  Segmentation", "author": "Moo Hyun Son and Juyoung Bae and Zelin Qiu and Jiale Peng and Kai Xin Li and Yifan Lin and Hao Chen", "abstract": "  Digital dentistry represents a transformative shift in modern dental\npractice. The foundational step in this transformation is the accurate digital\nrepresentation of the patient's dentition, which is obtained from segmented\nCone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the\ngrowing interest in digital dental technologies, existing segmentation\nmethodologies frequently lack rigorous validation and demonstrate limited\nperformance and clinical applicability. To the best of our knowledge, this is\nthe first work to introduce a multimodal pretraining framework for tooth\nsegmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for\npretraining that integrates volumetric (CBCT) and surface-based (IOS)\nmodalities. By capturing modality-invariant representations through multimodal\ncontrastive learning, our approach effectively models fine-grained anatomical\nfeatures, enabling precise multi-class segmentation and accurate identification\nof F\\'ed\\'eration Dentaire Internationale (FDI) tooth numbering. Along with the\nframework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to\ndate, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive\ncollection of independent datasets, representing the largest and most diverse\nevaluation to date. Our method achieves state-of-the-art performance in both\ninternal and external testing, with an increase of 12\\% for CBCT segmentation\nand 8\\% for IOS segmentation in the Dice Similarity Coefficient (DSC).\nFurthermore, ToothMCL consistently surpasses existing approaches in tooth\ngroups and demonstrates robust generalizability across varying imaging\nconditions and clinical scenarios.\n", "link": "http://arxiv.org/abs/2509.07923v1", "date": "2025-09-09", "relevancy": 2.6589, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5811}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5071}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Contrastive%20Pretraining%20of%20CBCT%20and%20IOS%20for%20Enhanced%20Tooth%0A%20%20Segmentation&body=Title%3A%20Multimodal%20Contrastive%20Pretraining%20of%20CBCT%20and%20IOS%20for%20Enhanced%20Tooth%0A%20%20Segmentation%0AAuthor%3A%20Moo%20Hyun%20Son%20and%20Juyoung%20Bae%20and%20Zelin%20Qiu%20and%20Jiale%20Peng%20and%20Kai%20Xin%20Li%20and%20Yifan%20Lin%20and%20Hao%20Chen%0AAbstract%3A%20%20%20Digital%20dentistry%20represents%20a%20transformative%20shift%20in%20modern%20dental%0Apractice.%20The%20foundational%20step%20in%20this%20transformation%20is%20the%20accurate%20digital%0Arepresentation%20of%20the%20patient%27s%20dentition%2C%20which%20is%20obtained%20from%20segmented%0ACone-Beam%20Computed%20Tomography%20%28CBCT%29%20and%20Intraoral%20Scans%20%28IOS%29.%20Despite%20the%0Agrowing%20interest%20in%20digital%20dental%20technologies%2C%20existing%20segmentation%0Amethodologies%20frequently%20lack%20rigorous%20validation%20and%20demonstrate%20limited%0Aperformance%20and%20clinical%20applicability.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%0Athe%20first%20work%20to%20introduce%20a%20multimodal%20pretraining%20framework%20for%20tooth%0Asegmentation.%20We%20present%20ToothMCL%2C%20a%20Tooth%20Multimodal%20Contrastive%20Learning%20for%0Apretraining%20that%20integrates%20volumetric%20%28CBCT%29%20and%20surface-based%20%28IOS%29%0Amodalities.%20By%20capturing%20modality-invariant%20representations%20through%20multimodal%0Acontrastive%20learning%2C%20our%20approach%20effectively%20models%20fine-grained%20anatomical%0Afeatures%2C%20enabling%20precise%20multi-class%20segmentation%20and%20accurate%20identification%0Aof%20F%5C%27ed%5C%27eration%20Dentaire%20Internationale%20%28FDI%29%20tooth%20numbering.%20Along%20with%20the%0Aframework%2C%20we%20curated%20CBCT-IOS3.8K%2C%20the%20largest%20paired%20CBCT%20and%20IOS%20dataset%20to%0Adate%2C%20comprising%203%2C867%20patients.%20We%20then%20evaluated%20ToothMCL%20on%20a%20comprehensive%0Acollection%20of%20independent%20datasets%2C%20representing%20the%20largest%20and%20most%20diverse%0Aevaluation%20to%20date.%20Our%20method%20achieves%20state-of-the-art%20performance%20in%20both%0Ainternal%20and%20external%20testing%2C%20with%20an%20increase%20of%2012%5C%25%20for%20CBCT%20segmentation%0Aand%208%5C%25%20for%20IOS%20segmentation%20in%20the%20Dice%20Similarity%20Coefficient%20%28DSC%29.%0AFurthermore%2C%20ToothMCL%20consistently%20surpasses%20existing%20approaches%20in%20tooth%0Agroups%20and%20demonstrates%20robust%20generalizability%20across%20varying%20imaging%0Aconditions%20and%20clinical%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Contrastive%2520Pretraining%2520of%2520CBCT%2520and%2520IOS%2520for%2520Enhanced%2520Tooth%250A%2520%2520Segmentation%26entry.906535625%3DMoo%2520Hyun%2520Son%2520and%2520Juyoung%2520Bae%2520and%2520Zelin%2520Qiu%2520and%2520Jiale%2520Peng%2520and%2520Kai%2520Xin%2520Li%2520and%2520Yifan%2520Lin%2520and%2520Hao%2520Chen%26entry.1292438233%3D%2520%2520Digital%2520dentistry%2520represents%2520a%2520transformative%2520shift%2520in%2520modern%2520dental%250Apractice.%2520The%2520foundational%2520step%2520in%2520this%2520transformation%2520is%2520the%2520accurate%2520digital%250Arepresentation%2520of%2520the%2520patient%2527s%2520dentition%252C%2520which%2520is%2520obtained%2520from%2520segmented%250ACone-Beam%2520Computed%2520Tomography%2520%2528CBCT%2529%2520and%2520Intraoral%2520Scans%2520%2528IOS%2529.%2520Despite%2520the%250Agrowing%2520interest%2520in%2520digital%2520dental%2520technologies%252C%2520existing%2520segmentation%250Amethodologies%2520frequently%2520lack%2520rigorous%2520validation%2520and%2520demonstrate%2520limited%250Aperformance%2520and%2520clinical%2520applicability.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%250Athe%2520first%2520work%2520to%2520introduce%2520a%2520multimodal%2520pretraining%2520framework%2520for%2520tooth%250Asegmentation.%2520We%2520present%2520ToothMCL%252C%2520a%2520Tooth%2520Multimodal%2520Contrastive%2520Learning%2520for%250Apretraining%2520that%2520integrates%2520volumetric%2520%2528CBCT%2529%2520and%2520surface-based%2520%2528IOS%2529%250Amodalities.%2520By%2520capturing%2520modality-invariant%2520representations%2520through%2520multimodal%250Acontrastive%2520learning%252C%2520our%2520approach%2520effectively%2520models%2520fine-grained%2520anatomical%250Afeatures%252C%2520enabling%2520precise%2520multi-class%2520segmentation%2520and%2520accurate%2520identification%250Aof%2520F%255C%2527ed%255C%2527eration%2520Dentaire%2520Internationale%2520%2528FDI%2529%2520tooth%2520numbering.%2520Along%2520with%2520the%250Aframework%252C%2520we%2520curated%2520CBCT-IOS3.8K%252C%2520the%2520largest%2520paired%2520CBCT%2520and%2520IOS%2520dataset%2520to%250Adate%252C%2520comprising%25203%252C867%2520patients.%2520We%2520then%2520evaluated%2520ToothMCL%2520on%2520a%2520comprehensive%250Acollection%2520of%2520independent%2520datasets%252C%2520representing%2520the%2520largest%2520and%2520most%2520diverse%250Aevaluation%2520to%2520date.%2520Our%2520method%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%250Ainternal%2520and%2520external%2520testing%252C%2520with%2520an%2520increase%2520of%252012%255C%2525%2520for%2520CBCT%2520segmentation%250Aand%25208%255C%2525%2520for%2520IOS%2520segmentation%2520in%2520the%2520Dice%2520Similarity%2520Coefficient%2520%2528DSC%2529.%250AFurthermore%252C%2520ToothMCL%2520consistently%2520surpasses%2520existing%2520approaches%2520in%2520tooth%250Agroups%2520and%2520demonstrates%2520robust%2520generalizability%2520across%2520varying%2520imaging%250Aconditions%2520and%2520clinical%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Contrastive%20Pretraining%20of%20CBCT%20and%20IOS%20for%20Enhanced%20Tooth%0A%20%20Segmentation&entry.906535625=Moo%20Hyun%20Son%20and%20Juyoung%20Bae%20and%20Zelin%20Qiu%20and%20Jiale%20Peng%20and%20Kai%20Xin%20Li%20and%20Yifan%20Lin%20and%20Hao%20Chen&entry.1292438233=%20%20Digital%20dentistry%20represents%20a%20transformative%20shift%20in%20modern%20dental%0Apractice.%20The%20foundational%20step%20in%20this%20transformation%20is%20the%20accurate%20digital%0Arepresentation%20of%20the%20patient%27s%20dentition%2C%20which%20is%20obtained%20from%20segmented%0ACone-Beam%20Computed%20Tomography%20%28CBCT%29%20and%20Intraoral%20Scans%20%28IOS%29.%20Despite%20the%0Agrowing%20interest%20in%20digital%20dental%20technologies%2C%20existing%20segmentation%0Amethodologies%20frequently%20lack%20rigorous%20validation%20and%20demonstrate%20limited%0Aperformance%20and%20clinical%20applicability.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%0Athe%20first%20work%20to%20introduce%20a%20multimodal%20pretraining%20framework%20for%20tooth%0Asegmentation.%20We%20present%20ToothMCL%2C%20a%20Tooth%20Multimodal%20Contrastive%20Learning%20for%0Apretraining%20that%20integrates%20volumetric%20%28CBCT%29%20and%20surface-based%20%28IOS%29%0Amodalities.%20By%20capturing%20modality-invariant%20representations%20through%20multimodal%0Acontrastive%20learning%2C%20our%20approach%20effectively%20models%20fine-grained%20anatomical%0Afeatures%2C%20enabling%20precise%20multi-class%20segmentation%20and%20accurate%20identification%0Aof%20F%5C%27ed%5C%27eration%20Dentaire%20Internationale%20%28FDI%29%20tooth%20numbering.%20Along%20with%20the%0Aframework%2C%20we%20curated%20CBCT-IOS3.8K%2C%20the%20largest%20paired%20CBCT%20and%20IOS%20dataset%20to%0Adate%2C%20comprising%203%2C867%20patients.%20We%20then%20evaluated%20ToothMCL%20on%20a%20comprehensive%0Acollection%20of%20independent%20datasets%2C%20representing%20the%20largest%20and%20most%20diverse%0Aevaluation%20to%20date.%20Our%20method%20achieves%20state-of-the-art%20performance%20in%20both%0Ainternal%20and%20external%20testing%2C%20with%20an%20increase%20of%2012%5C%25%20for%20CBCT%20segmentation%0Aand%208%5C%25%20for%20IOS%20segmentation%20in%20the%20Dice%20Similarity%20Coefficient%20%28DSC%29.%0AFurthermore%2C%20ToothMCL%20consistently%20surpasses%20existing%20approaches%20in%20tooth%0Agroups%20and%20demonstrates%20robust%20generalizability%20across%20varying%20imaging%0Aconditions%20and%20clinical%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07923v1&entry.124074799=Read"},
{"title": "Bringing Multi-Modal Multi-Task Federated Foundation Models to Education\n  Domain: Prospects and Challenges", "author": "Kasra Borazjani and Naji Khosravan and Rajeev Sahay and Bita Akram and Seyyedali Hosseinalipour", "abstract": "  Multi-modal multi-task (M3T) foundation models (FMs) have recently shown\ntransformative potential in artificial intelligence, with emerging applications\nin education. However, their deployment in real-world educational settings is\nhindered by privacy regulations, data silos, and limited domain-specific data\navailability. We introduce M3T Federated Foundation Models (FedFMs) for\neducation: a paradigm that integrates federated learning (FL) with M3T FMs to\nenable collaborative, privacy-preserving training across decentralized\ninstitutions while accommodating diverse modalities and tasks. Subsequently,\nthis position paper aims to unveil M3T FedFMs as a promising yet underexplored\napproach to the education community, explore its potentials, and reveal its\nrelated future research directions. We outline how M3T FedFMs can advance three\ncritical pillars of next-generation intelligent education systems: (i) privacy\npreservation, by keeping sensitive multi-modal student and institutional data\nlocal; (ii) personalization, through modular architectures enabling tailored\nmodels for students, instructors, and institutions; and (iii) equity and\ninclusivity, by facilitating participation from underrepresented and\nresource-constrained entities. We finally identify various open research\nchallenges, including studying of (i) inter-institution heterogeneous privacy\nregulations, (ii) the non-uniformity of data modalities' characteristics, (iii)\nthe unlearning approaches for M3T FedFMs, (iv) the continual learning\nframeworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must\nbe collectively addressed for practical deployment.\n", "link": "http://arxiv.org/abs/2509.07946v1", "date": "2025-09-09", "relevancy": 2.635, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5164}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bringing%20Multi-Modal%20Multi-Task%20Federated%20Foundation%20Models%20to%20Education%0A%20%20Domain%3A%20Prospects%20and%20Challenges&body=Title%3A%20Bringing%20Multi-Modal%20Multi-Task%20Federated%20Foundation%20Models%20to%20Education%0A%20%20Domain%3A%20Prospects%20and%20Challenges%0AAuthor%3A%20Kasra%20Borazjani%20and%20Naji%20Khosravan%20and%20Rajeev%20Sahay%20and%20Bita%20Akram%20and%20Seyyedali%20Hosseinalipour%0AAbstract%3A%20%20%20Multi-modal%20multi-task%20%28M3T%29%20foundation%20models%20%28FMs%29%20have%20recently%20shown%0Atransformative%20potential%20in%20artificial%20intelligence%2C%20with%20emerging%20applications%0Ain%20education.%20However%2C%20their%20deployment%20in%20real-world%20educational%20settings%20is%0Ahindered%20by%20privacy%20regulations%2C%20data%20silos%2C%20and%20limited%20domain-specific%20data%0Aavailability.%20We%20introduce%20M3T%20Federated%20Foundation%20Models%20%28FedFMs%29%20for%0Aeducation%3A%20a%20paradigm%20that%20integrates%20federated%20learning%20%28FL%29%20with%20M3T%20FMs%20to%0Aenable%20collaborative%2C%20privacy-preserving%20training%20across%20decentralized%0Ainstitutions%20while%20accommodating%20diverse%20modalities%20and%20tasks.%20Subsequently%2C%0Athis%20position%20paper%20aims%20to%20unveil%20M3T%20FedFMs%20as%20a%20promising%20yet%20underexplored%0Aapproach%20to%20the%20education%20community%2C%20explore%20its%20potentials%2C%20and%20reveal%20its%0Arelated%20future%20research%20directions.%20We%20outline%20how%20M3T%20FedFMs%20can%20advance%20three%0Acritical%20pillars%20of%20next-generation%20intelligent%20education%20systems%3A%20%28i%29%20privacy%0Apreservation%2C%20by%20keeping%20sensitive%20multi-modal%20student%20and%20institutional%20data%0Alocal%3B%20%28ii%29%20personalization%2C%20through%20modular%20architectures%20enabling%20tailored%0Amodels%20for%20students%2C%20instructors%2C%20and%20institutions%3B%20and%20%28iii%29%20equity%20and%0Ainclusivity%2C%20by%20facilitating%20participation%20from%20underrepresented%20and%0Aresource-constrained%20entities.%20We%20finally%20identify%20various%20open%20research%0Achallenges%2C%20including%20studying%20of%20%28i%29%20inter-institution%20heterogeneous%20privacy%0Aregulations%2C%20%28ii%29%20the%20non-uniformity%20of%20data%20modalities%27%20characteristics%2C%20%28iii%29%0Athe%20unlearning%20approaches%20for%20M3T%20FedFMs%2C%20%28iv%29%20the%20continual%20learning%0Aframeworks%20for%20M3T%20FedFMs%2C%20and%20%28v%29%20M3T%20FedFM%20model%20interpretability%2C%20which%20must%0Abe%20collectively%20addressed%20for%20practical%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBringing%2520Multi-Modal%2520Multi-Task%2520Federated%2520Foundation%2520Models%2520to%2520Education%250A%2520%2520Domain%253A%2520Prospects%2520and%2520Challenges%26entry.906535625%3DKasra%2520Borazjani%2520and%2520Naji%2520Khosravan%2520and%2520Rajeev%2520Sahay%2520and%2520Bita%2520Akram%2520and%2520Seyyedali%2520Hosseinalipour%26entry.1292438233%3D%2520%2520Multi-modal%2520multi-task%2520%2528M3T%2529%2520foundation%2520models%2520%2528FMs%2529%2520have%2520recently%2520shown%250Atransformative%2520potential%2520in%2520artificial%2520intelligence%252C%2520with%2520emerging%2520applications%250Ain%2520education.%2520However%252C%2520their%2520deployment%2520in%2520real-world%2520educational%2520settings%2520is%250Ahindered%2520by%2520privacy%2520regulations%252C%2520data%2520silos%252C%2520and%2520limited%2520domain-specific%2520data%250Aavailability.%2520We%2520introduce%2520M3T%2520Federated%2520Foundation%2520Models%2520%2528FedFMs%2529%2520for%250Aeducation%253A%2520a%2520paradigm%2520that%2520integrates%2520federated%2520learning%2520%2528FL%2529%2520with%2520M3T%2520FMs%2520to%250Aenable%2520collaborative%252C%2520privacy-preserving%2520training%2520across%2520decentralized%250Ainstitutions%2520while%2520accommodating%2520diverse%2520modalities%2520and%2520tasks.%2520Subsequently%252C%250Athis%2520position%2520paper%2520aims%2520to%2520unveil%2520M3T%2520FedFMs%2520as%2520a%2520promising%2520yet%2520underexplored%250Aapproach%2520to%2520the%2520education%2520community%252C%2520explore%2520its%2520potentials%252C%2520and%2520reveal%2520its%250Arelated%2520future%2520research%2520directions.%2520We%2520outline%2520how%2520M3T%2520FedFMs%2520can%2520advance%2520three%250Acritical%2520pillars%2520of%2520next-generation%2520intelligent%2520education%2520systems%253A%2520%2528i%2529%2520privacy%250Apreservation%252C%2520by%2520keeping%2520sensitive%2520multi-modal%2520student%2520and%2520institutional%2520data%250Alocal%253B%2520%2528ii%2529%2520personalization%252C%2520through%2520modular%2520architectures%2520enabling%2520tailored%250Amodels%2520for%2520students%252C%2520instructors%252C%2520and%2520institutions%253B%2520and%2520%2528iii%2529%2520equity%2520and%250Ainclusivity%252C%2520by%2520facilitating%2520participation%2520from%2520underrepresented%2520and%250Aresource-constrained%2520entities.%2520We%2520finally%2520identify%2520various%2520open%2520research%250Achallenges%252C%2520including%2520studying%2520of%2520%2528i%2529%2520inter-institution%2520heterogeneous%2520privacy%250Aregulations%252C%2520%2528ii%2529%2520the%2520non-uniformity%2520of%2520data%2520modalities%2527%2520characteristics%252C%2520%2528iii%2529%250Athe%2520unlearning%2520approaches%2520for%2520M3T%2520FedFMs%252C%2520%2528iv%2529%2520the%2520continual%2520learning%250Aframeworks%2520for%2520M3T%2520FedFMs%252C%2520and%2520%2528v%2529%2520M3T%2520FedFM%2520model%2520interpretability%252C%2520which%2520must%250Abe%2520collectively%2520addressed%2520for%2520practical%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bringing%20Multi-Modal%20Multi-Task%20Federated%20Foundation%20Models%20to%20Education%0A%20%20Domain%3A%20Prospects%20and%20Challenges&entry.906535625=Kasra%20Borazjani%20and%20Naji%20Khosravan%20and%20Rajeev%20Sahay%20and%20Bita%20Akram%20and%20Seyyedali%20Hosseinalipour&entry.1292438233=%20%20Multi-modal%20multi-task%20%28M3T%29%20foundation%20models%20%28FMs%29%20have%20recently%20shown%0Atransformative%20potential%20in%20artificial%20intelligence%2C%20with%20emerging%20applications%0Ain%20education.%20However%2C%20their%20deployment%20in%20real-world%20educational%20settings%20is%0Ahindered%20by%20privacy%20regulations%2C%20data%20silos%2C%20and%20limited%20domain-specific%20data%0Aavailability.%20We%20introduce%20M3T%20Federated%20Foundation%20Models%20%28FedFMs%29%20for%0Aeducation%3A%20a%20paradigm%20that%20integrates%20federated%20learning%20%28FL%29%20with%20M3T%20FMs%20to%0Aenable%20collaborative%2C%20privacy-preserving%20training%20across%20decentralized%0Ainstitutions%20while%20accommodating%20diverse%20modalities%20and%20tasks.%20Subsequently%2C%0Athis%20position%20paper%20aims%20to%20unveil%20M3T%20FedFMs%20as%20a%20promising%20yet%20underexplored%0Aapproach%20to%20the%20education%20community%2C%20explore%20its%20potentials%2C%20and%20reveal%20its%0Arelated%20future%20research%20directions.%20We%20outline%20how%20M3T%20FedFMs%20can%20advance%20three%0Acritical%20pillars%20of%20next-generation%20intelligent%20education%20systems%3A%20%28i%29%20privacy%0Apreservation%2C%20by%20keeping%20sensitive%20multi-modal%20student%20and%20institutional%20data%0Alocal%3B%20%28ii%29%20personalization%2C%20through%20modular%20architectures%20enabling%20tailored%0Amodels%20for%20students%2C%20instructors%2C%20and%20institutions%3B%20and%20%28iii%29%20equity%20and%0Ainclusivity%2C%20by%20facilitating%20participation%20from%20underrepresented%20and%0Aresource-constrained%20entities.%20We%20finally%20identify%20various%20open%20research%0Achallenges%2C%20including%20studying%20of%20%28i%29%20inter-institution%20heterogeneous%20privacy%0Aregulations%2C%20%28ii%29%20the%20non-uniformity%20of%20data%20modalities%27%20characteristics%2C%20%28iii%29%0Athe%20unlearning%20approaches%20for%20M3T%20FedFMs%2C%20%28iv%29%20the%20continual%20learning%0Aframeworks%20for%20M3T%20FedFMs%2C%20and%20%28v%29%20M3T%20FedFM%20model%20interpretability%2C%20which%20must%0Abe%20collectively%20addressed%20for%20practical%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07946v1&entry.124074799=Read"},
{"title": "Understanding Ice Crystal Habit Diversity with Self-Supervised Learning", "author": "Joseph Ko and Hariprasath Govindarajan and Fredrik Lindsten and Vanessa Przybylo and Kara Sulia and Marcus van Lier-Walqui and Kara Lamb", "abstract": "  Ice-containing clouds strongly impact climate, but they are hard to model due\nto ice crystal habit (i.e., shape) diversity. We use self-supervised learning\n(SSL) to learn latent representations of crystals from ice crystal imagery. By\npre-training a vision transformer with many cloud particle images, we learn\nrobust representations of crystal morphology, which can be used for various\nscience-driven tasks. Our key contributions include (1) validating that our SSL\napproach can be used to learn meaningful representations, and (2) presenting a\nrelevant application where we quantify ice crystal diversity with these latent\nrepresentations. Our results demonstrate the power of SSL-driven\nrepresentations to improve the characterization of ice crystals and\nsubsequently constrain their role in Earth's climate system.\n", "link": "http://arxiv.org/abs/2509.07688v1", "date": "2025-09-09", "relevancy": 2.6025, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5419}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5327}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Ice%20Crystal%20Habit%20Diversity%20with%20Self-Supervised%20Learning&body=Title%3A%20Understanding%20Ice%20Crystal%20Habit%20Diversity%20with%20Self-Supervised%20Learning%0AAuthor%3A%20Joseph%20Ko%20and%20Hariprasath%20Govindarajan%20and%20Fredrik%20Lindsten%20and%20Vanessa%20Przybylo%20and%20Kara%20Sulia%20and%20Marcus%20van%20Lier-Walqui%20and%20Kara%20Lamb%0AAbstract%3A%20%20%20Ice-containing%20clouds%20strongly%20impact%20climate%2C%20but%20they%20are%20hard%20to%20model%20due%0Ato%20ice%20crystal%20habit%20%28i.e.%2C%20shape%29%20diversity.%20We%20use%20self-supervised%20learning%0A%28SSL%29%20to%20learn%20latent%20representations%20of%20crystals%20from%20ice%20crystal%20imagery.%20By%0Apre-training%20a%20vision%20transformer%20with%20many%20cloud%20particle%20images%2C%20we%20learn%0Arobust%20representations%20of%20crystal%20morphology%2C%20which%20can%20be%20used%20for%20various%0Ascience-driven%20tasks.%20Our%20key%20contributions%20include%20%281%29%20validating%20that%20our%20SSL%0Aapproach%20can%20be%20used%20to%20learn%20meaningful%20representations%2C%20and%20%282%29%20presenting%20a%0Arelevant%20application%20where%20we%20quantify%20ice%20crystal%20diversity%20with%20these%20latent%0Arepresentations.%20Our%20results%20demonstrate%20the%20power%20of%20SSL-driven%0Arepresentations%20to%20improve%20the%20characterization%20of%20ice%20crystals%20and%0Asubsequently%20constrain%20their%20role%20in%20Earth%27s%20climate%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Ice%2520Crystal%2520Habit%2520Diversity%2520with%2520Self-Supervised%2520Learning%26entry.906535625%3DJoseph%2520Ko%2520and%2520Hariprasath%2520Govindarajan%2520and%2520Fredrik%2520Lindsten%2520and%2520Vanessa%2520Przybylo%2520and%2520Kara%2520Sulia%2520and%2520Marcus%2520van%2520Lier-Walqui%2520and%2520Kara%2520Lamb%26entry.1292438233%3D%2520%2520Ice-containing%2520clouds%2520strongly%2520impact%2520climate%252C%2520but%2520they%2520are%2520hard%2520to%2520model%2520due%250Ato%2520ice%2520crystal%2520habit%2520%2528i.e.%252C%2520shape%2529%2520diversity.%2520We%2520use%2520self-supervised%2520learning%250A%2528SSL%2529%2520to%2520learn%2520latent%2520representations%2520of%2520crystals%2520from%2520ice%2520crystal%2520imagery.%2520By%250Apre-training%2520a%2520vision%2520transformer%2520with%2520many%2520cloud%2520particle%2520images%252C%2520we%2520learn%250Arobust%2520representations%2520of%2520crystal%2520morphology%252C%2520which%2520can%2520be%2520used%2520for%2520various%250Ascience-driven%2520tasks.%2520Our%2520key%2520contributions%2520include%2520%25281%2529%2520validating%2520that%2520our%2520SSL%250Aapproach%2520can%2520be%2520used%2520to%2520learn%2520meaningful%2520representations%252C%2520and%2520%25282%2529%2520presenting%2520a%250Arelevant%2520application%2520where%2520we%2520quantify%2520ice%2520crystal%2520diversity%2520with%2520these%2520latent%250Arepresentations.%2520Our%2520results%2520demonstrate%2520the%2520power%2520of%2520SSL-driven%250Arepresentations%2520to%2520improve%2520the%2520characterization%2520of%2520ice%2520crystals%2520and%250Asubsequently%2520constrain%2520their%2520role%2520in%2520Earth%2527s%2520climate%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Ice%20Crystal%20Habit%20Diversity%20with%20Self-Supervised%20Learning&entry.906535625=Joseph%20Ko%20and%20Hariprasath%20Govindarajan%20and%20Fredrik%20Lindsten%20and%20Vanessa%20Przybylo%20and%20Kara%20Sulia%20and%20Marcus%20van%20Lier-Walqui%20and%20Kara%20Lamb&entry.1292438233=%20%20Ice-containing%20clouds%20strongly%20impact%20climate%2C%20but%20they%20are%20hard%20to%20model%20due%0Ato%20ice%20crystal%20habit%20%28i.e.%2C%20shape%29%20diversity.%20We%20use%20self-supervised%20learning%0A%28SSL%29%20to%20learn%20latent%20representations%20of%20crystals%20from%20ice%20crystal%20imagery.%20By%0Apre-training%20a%20vision%20transformer%20with%20many%20cloud%20particle%20images%2C%20we%20learn%0Arobust%20representations%20of%20crystal%20morphology%2C%20which%20can%20be%20used%20for%20various%0Ascience-driven%20tasks.%20Our%20key%20contributions%20include%20%281%29%20validating%20that%20our%20SSL%0Aapproach%20can%20be%20used%20to%20learn%20meaningful%20representations%2C%20and%20%282%29%20presenting%20a%0Arelevant%20application%20where%20we%20quantify%20ice%20crystal%20diversity%20with%20these%20latent%0Arepresentations.%20Our%20results%20demonstrate%20the%20power%20of%20SSL-driven%0Arepresentations%20to%20improve%20the%20characterization%20of%20ice%20crystals%20and%0Asubsequently%20constrain%20their%20role%20in%20Earth%27s%20climate%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07688v1&entry.124074799=Read"},
{"title": "MSCPT: Few-shot Whole Slide Image Classification with Multi-scale and\n  Context-focused Prompt Tuning", "author": "Minghao Han and Linhao Qu and Dingkang Yang and Xukun Zhang and Xiaoying Wang and Lihua Zhang", "abstract": "  Multiple instance learning (MIL) has become a standard paradigm for the\nweakly supervised classification of whole slide images (WSIs). However, this\nparadigm relies on using a large number of labeled WSIs for training. The lack\nof training data and the presence of rare diseases pose significant challenges\nfor these methods. Prompt tuning combined with pre-trained Vision-Language\nmodels (VLMs) is an effective solution to the Few-shot Weakly Supervised WSI\nClassification (FSWC) task. Nevertheless, applying prompt tuning methods\ndesigned for natural images to WSIs presents three significant challenges: 1)\nThese methods fail to fully leverage the prior knowledge from the VLM's text\nmodality; 2) They overlook the essential multi-scale and contextual information\nin WSIs, leading to suboptimal results; and 3) They lack exploration of\ninstance aggregation methods. To address these problems, we propose a\nMulti-Scale and Context-focused Prompt Tuning (MSCPT) method for FSWC task.\nSpecifically, MSCPT employs the frozen large language model to generate\npathological visual language prior knowledge at multiple scales, guiding\nhierarchical prompt tuning. Additionally, we design a graph prompt tuning\nmodule to learn essential contextual information within WSI, and finally, a\nnon-parametric cross-guided instance aggregation module has been introduced to\nderive the WSI-level features. Extensive experiments, visualizations, and\ninterpretability analyses were conducted on five datasets and three downstream\ntasks using three VLMs, demonstrating the strong performance of our MSCPT. All\ncodes have been made publicly accessible at\nhttps://github.com/Hanminghao/MSCPT.\n", "link": "http://arxiv.org/abs/2408.11505v3", "date": "2025-09-09", "relevancy": 2.5935, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5386}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSCPT%3A%20Few-shot%20Whole%20Slide%20Image%20Classification%20with%20Multi-scale%20and%0A%20%20Context-focused%20Prompt%20Tuning&body=Title%3A%20MSCPT%3A%20Few-shot%20Whole%20Slide%20Image%20Classification%20with%20Multi-scale%20and%0A%20%20Context-focused%20Prompt%20Tuning%0AAuthor%3A%20Minghao%20Han%20and%20Linhao%20Qu%20and%20Dingkang%20Yang%20and%20Xukun%20Zhang%20and%20Xiaoying%20Wang%20and%20Lihua%20Zhang%0AAbstract%3A%20%20%20Multiple%20instance%20learning%20%28MIL%29%20has%20become%20a%20standard%20paradigm%20for%20the%0Aweakly%20supervised%20classification%20of%20whole%20slide%20images%20%28WSIs%29.%20However%2C%20this%0Aparadigm%20relies%20on%20using%20a%20large%20number%20of%20labeled%20WSIs%20for%20training.%20The%20lack%0Aof%20training%20data%20and%20the%20presence%20of%20rare%20diseases%20pose%20significant%20challenges%0Afor%20these%20methods.%20Prompt%20tuning%20combined%20with%20pre-trained%20Vision-Language%0Amodels%20%28VLMs%29%20is%20an%20effective%20solution%20to%20the%20Few-shot%20Weakly%20Supervised%20WSI%0AClassification%20%28FSWC%29%20task.%20Nevertheless%2C%20applying%20prompt%20tuning%20methods%0Adesigned%20for%20natural%20images%20to%20WSIs%20presents%20three%20significant%20challenges%3A%201%29%0AThese%20methods%20fail%20to%20fully%20leverage%20the%20prior%20knowledge%20from%20the%20VLM%27s%20text%0Amodality%3B%202%29%20They%20overlook%20the%20essential%20multi-scale%20and%20contextual%20information%0Ain%20WSIs%2C%20leading%20to%20suboptimal%20results%3B%20and%203%29%20They%20lack%20exploration%20of%0Ainstance%20aggregation%20methods.%20To%20address%20these%20problems%2C%20we%20propose%20a%0AMulti-Scale%20and%20Context-focused%20Prompt%20Tuning%20%28MSCPT%29%20method%20for%20FSWC%20task.%0ASpecifically%2C%20MSCPT%20employs%20the%20frozen%20large%20language%20model%20to%20generate%0Apathological%20visual%20language%20prior%20knowledge%20at%20multiple%20scales%2C%20guiding%0Ahierarchical%20prompt%20tuning.%20Additionally%2C%20we%20design%20a%20graph%20prompt%20tuning%0Amodule%20to%20learn%20essential%20contextual%20information%20within%20WSI%2C%20and%20finally%2C%20a%0Anon-parametric%20cross-guided%20instance%20aggregation%20module%20has%20been%20introduced%20to%0Aderive%20the%20WSI-level%20features.%20Extensive%20experiments%2C%20visualizations%2C%20and%0Ainterpretability%20analyses%20were%20conducted%20on%20five%20datasets%20and%20three%20downstream%0Atasks%20using%20three%20VLMs%2C%20demonstrating%20the%20strong%20performance%20of%20our%20MSCPT.%20All%0Acodes%20have%20been%20made%20publicly%20accessible%20at%0Ahttps%3A//github.com/Hanminghao/MSCPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11505v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSCPT%253A%2520Few-shot%2520Whole%2520Slide%2520Image%2520Classification%2520with%2520Multi-scale%2520and%250A%2520%2520Context-focused%2520Prompt%2520Tuning%26entry.906535625%3DMinghao%2520Han%2520and%2520Linhao%2520Qu%2520and%2520Dingkang%2520Yang%2520and%2520Xukun%2520Zhang%2520and%2520Xiaoying%2520Wang%2520and%2520Lihua%2520Zhang%26entry.1292438233%3D%2520%2520Multiple%2520instance%2520learning%2520%2528MIL%2529%2520has%2520become%2520a%2520standard%2520paradigm%2520for%2520the%250Aweakly%2520supervised%2520classification%2520of%2520whole%2520slide%2520images%2520%2528WSIs%2529.%2520However%252C%2520this%250Aparadigm%2520relies%2520on%2520using%2520a%2520large%2520number%2520of%2520labeled%2520WSIs%2520for%2520training.%2520The%2520lack%250Aof%2520training%2520data%2520and%2520the%2520presence%2520of%2520rare%2520diseases%2520pose%2520significant%2520challenges%250Afor%2520these%2520methods.%2520Prompt%2520tuning%2520combined%2520with%2520pre-trained%2520Vision-Language%250Amodels%2520%2528VLMs%2529%2520is%2520an%2520effective%2520solution%2520to%2520the%2520Few-shot%2520Weakly%2520Supervised%2520WSI%250AClassification%2520%2528FSWC%2529%2520task.%2520Nevertheless%252C%2520applying%2520prompt%2520tuning%2520methods%250Adesigned%2520for%2520natural%2520images%2520to%2520WSIs%2520presents%2520three%2520significant%2520challenges%253A%25201%2529%250AThese%2520methods%2520fail%2520to%2520fully%2520leverage%2520the%2520prior%2520knowledge%2520from%2520the%2520VLM%2527s%2520text%250Amodality%253B%25202%2529%2520They%2520overlook%2520the%2520essential%2520multi-scale%2520and%2520contextual%2520information%250Ain%2520WSIs%252C%2520leading%2520to%2520suboptimal%2520results%253B%2520and%25203%2529%2520They%2520lack%2520exploration%2520of%250Ainstance%2520aggregation%2520methods.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%250AMulti-Scale%2520and%2520Context-focused%2520Prompt%2520Tuning%2520%2528MSCPT%2529%2520method%2520for%2520FSWC%2520task.%250ASpecifically%252C%2520MSCPT%2520employs%2520the%2520frozen%2520large%2520language%2520model%2520to%2520generate%250Apathological%2520visual%2520language%2520prior%2520knowledge%2520at%2520multiple%2520scales%252C%2520guiding%250Ahierarchical%2520prompt%2520tuning.%2520Additionally%252C%2520we%2520design%2520a%2520graph%2520prompt%2520tuning%250Amodule%2520to%2520learn%2520essential%2520contextual%2520information%2520within%2520WSI%252C%2520and%2520finally%252C%2520a%250Anon-parametric%2520cross-guided%2520instance%2520aggregation%2520module%2520has%2520been%2520introduced%2520to%250Aderive%2520the%2520WSI-level%2520features.%2520Extensive%2520experiments%252C%2520visualizations%252C%2520and%250Ainterpretability%2520analyses%2520were%2520conducted%2520on%2520five%2520datasets%2520and%2520three%2520downstream%250Atasks%2520using%2520three%2520VLMs%252C%2520demonstrating%2520the%2520strong%2520performance%2520of%2520our%2520MSCPT.%2520All%250Acodes%2520have%2520been%2520made%2520publicly%2520accessible%2520at%250Ahttps%253A//github.com/Hanminghao/MSCPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11505v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSCPT%3A%20Few-shot%20Whole%20Slide%20Image%20Classification%20with%20Multi-scale%20and%0A%20%20Context-focused%20Prompt%20Tuning&entry.906535625=Minghao%20Han%20and%20Linhao%20Qu%20and%20Dingkang%20Yang%20and%20Xukun%20Zhang%20and%20Xiaoying%20Wang%20and%20Lihua%20Zhang&entry.1292438233=%20%20Multiple%20instance%20learning%20%28MIL%29%20has%20become%20a%20standard%20paradigm%20for%20the%0Aweakly%20supervised%20classification%20of%20whole%20slide%20images%20%28WSIs%29.%20However%2C%20this%0Aparadigm%20relies%20on%20using%20a%20large%20number%20of%20labeled%20WSIs%20for%20training.%20The%20lack%0Aof%20training%20data%20and%20the%20presence%20of%20rare%20diseases%20pose%20significant%20challenges%0Afor%20these%20methods.%20Prompt%20tuning%20combined%20with%20pre-trained%20Vision-Language%0Amodels%20%28VLMs%29%20is%20an%20effective%20solution%20to%20the%20Few-shot%20Weakly%20Supervised%20WSI%0AClassification%20%28FSWC%29%20task.%20Nevertheless%2C%20applying%20prompt%20tuning%20methods%0Adesigned%20for%20natural%20images%20to%20WSIs%20presents%20three%20significant%20challenges%3A%201%29%0AThese%20methods%20fail%20to%20fully%20leverage%20the%20prior%20knowledge%20from%20the%20VLM%27s%20text%0Amodality%3B%202%29%20They%20overlook%20the%20essential%20multi-scale%20and%20contextual%20information%0Ain%20WSIs%2C%20leading%20to%20suboptimal%20results%3B%20and%203%29%20They%20lack%20exploration%20of%0Ainstance%20aggregation%20methods.%20To%20address%20these%20problems%2C%20we%20propose%20a%0AMulti-Scale%20and%20Context-focused%20Prompt%20Tuning%20%28MSCPT%29%20method%20for%20FSWC%20task.%0ASpecifically%2C%20MSCPT%20employs%20the%20frozen%20large%20language%20model%20to%20generate%0Apathological%20visual%20language%20prior%20knowledge%20at%20multiple%20scales%2C%20guiding%0Ahierarchical%20prompt%20tuning.%20Additionally%2C%20we%20design%20a%20graph%20prompt%20tuning%0Amodule%20to%20learn%20essential%20contextual%20information%20within%20WSI%2C%20and%20finally%2C%20a%0Anon-parametric%20cross-guided%20instance%20aggregation%20module%20has%20been%20introduced%20to%0Aderive%20the%20WSI-level%20features.%20Extensive%20experiments%2C%20visualizations%2C%20and%0Ainterpretability%20analyses%20were%20conducted%20on%20five%20datasets%20and%20three%20downstream%0Atasks%20using%20three%20VLMs%2C%20demonstrating%20the%20strong%20performance%20of%20our%20MSCPT.%20All%0Acodes%20have%20been%20made%20publicly%20accessible%20at%0Ahttps%3A//github.com/Hanminghao/MSCPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11505v3&entry.124074799=Read"},
{"title": "Nearest Neighbor Projection Removal Adversarial Training", "author": "Himanshu Singh and A. V. Subramanyam and Shivank Rajput and Mohan Kankanhalli", "abstract": "  Deep neural networks have exhibited impressive performance in image\nclassification tasks but remain vulnerable to adversarial examples. Standard\nadversarial training enhances robustness but typically fails to explicitly\naddress inter-class feature overlap, a significant contributor to adversarial\nsusceptibility. In this work, we introduce a novel adversarial training\nframework that actively mitigates inter-class proximity by projecting out\ninter-class dependencies from adversarial and clean samples in the feature\nspace. Specifically, our approach first identifies the nearest inter-class\nneighbors for each adversarial sample and subsequently removes projections onto\nthese neighbors to enforce stronger feature separability. Theoretically, we\ndemonstrate that our proposed logits correction reduces the Lipschitz constant\nof neural networks, thereby lowering the Rademacher complexity, which directly\ncontributes to improved generalization and robustness. Extensive experiments\nacross standard benchmarks including CIFAR-10, CIFAR-100, and SVHN show that\nour method demonstrates strong performance that is competitive with leading\nadversarial training techniques, highlighting significant achievements in both\nrobust and clean accuracy. Our findings reveal the importance of addressing\ninter-class feature proximity explicitly to bolster adversarial robustness in\nDNNs.\n", "link": "http://arxiv.org/abs/2509.07673v1", "date": "2025-09-09", "relevancy": 2.5887, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5324}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5181}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nearest%20Neighbor%20Projection%20Removal%20Adversarial%20Training&body=Title%3A%20Nearest%20Neighbor%20Projection%20Removal%20Adversarial%20Training%0AAuthor%3A%20Himanshu%20Singh%20and%20A.%20V.%20Subramanyam%20and%20Shivank%20Rajput%20and%20Mohan%20Kankanhalli%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20exhibited%20impressive%20performance%20in%20image%0Aclassification%20tasks%20but%20remain%20vulnerable%20to%20adversarial%20examples.%20Standard%0Aadversarial%20training%20enhances%20robustness%20but%20typically%20fails%20to%20explicitly%0Aaddress%20inter-class%20feature%20overlap%2C%20a%20significant%20contributor%20to%20adversarial%0Asusceptibility.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20adversarial%20training%0Aframework%20that%20actively%20mitigates%20inter-class%20proximity%20by%20projecting%20out%0Ainter-class%20dependencies%20from%20adversarial%20and%20clean%20samples%20in%20the%20feature%0Aspace.%20Specifically%2C%20our%20approach%20first%20identifies%20the%20nearest%20inter-class%0Aneighbors%20for%20each%20adversarial%20sample%20and%20subsequently%20removes%20projections%20onto%0Athese%20neighbors%20to%20enforce%20stronger%20feature%20separability.%20Theoretically%2C%20we%0Ademonstrate%20that%20our%20proposed%20logits%20correction%20reduces%20the%20Lipschitz%20constant%0Aof%20neural%20networks%2C%20thereby%20lowering%20the%20Rademacher%20complexity%2C%20which%20directly%0Acontributes%20to%20improved%20generalization%20and%20robustness.%20Extensive%20experiments%0Aacross%20standard%20benchmarks%20including%20CIFAR-10%2C%20CIFAR-100%2C%20and%20SVHN%20show%20that%0Aour%20method%20demonstrates%20strong%20performance%20that%20is%20competitive%20with%20leading%0Aadversarial%20training%20techniques%2C%20highlighting%20significant%20achievements%20in%20both%0Arobust%20and%20clean%20accuracy.%20Our%20findings%20reveal%20the%20importance%20of%20addressing%0Ainter-class%20feature%20proximity%20explicitly%20to%20bolster%20adversarial%20robustness%20in%0ADNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNearest%2520Neighbor%2520Projection%2520Removal%2520Adversarial%2520Training%26entry.906535625%3DHimanshu%2520Singh%2520and%2520A.%2520V.%2520Subramanyam%2520and%2520Shivank%2520Rajput%2520and%2520Mohan%2520Kankanhalli%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520exhibited%2520impressive%2520performance%2520in%2520image%250Aclassification%2520tasks%2520but%2520remain%2520vulnerable%2520to%2520adversarial%2520examples.%2520Standard%250Aadversarial%2520training%2520enhances%2520robustness%2520but%2520typically%2520fails%2520to%2520explicitly%250Aaddress%2520inter-class%2520feature%2520overlap%252C%2520a%2520significant%2520contributor%2520to%2520adversarial%250Asusceptibility.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520adversarial%2520training%250Aframework%2520that%2520actively%2520mitigates%2520inter-class%2520proximity%2520by%2520projecting%2520out%250Ainter-class%2520dependencies%2520from%2520adversarial%2520and%2520clean%2520samples%2520in%2520the%2520feature%250Aspace.%2520Specifically%252C%2520our%2520approach%2520first%2520identifies%2520the%2520nearest%2520inter-class%250Aneighbors%2520for%2520each%2520adversarial%2520sample%2520and%2520subsequently%2520removes%2520projections%2520onto%250Athese%2520neighbors%2520to%2520enforce%2520stronger%2520feature%2520separability.%2520Theoretically%252C%2520we%250Ademonstrate%2520that%2520our%2520proposed%2520logits%2520correction%2520reduces%2520the%2520Lipschitz%2520constant%250Aof%2520neural%2520networks%252C%2520thereby%2520lowering%2520the%2520Rademacher%2520complexity%252C%2520which%2520directly%250Acontributes%2520to%2520improved%2520generalization%2520and%2520robustness.%2520Extensive%2520experiments%250Aacross%2520standard%2520benchmarks%2520including%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%2520SVHN%2520show%2520that%250Aour%2520method%2520demonstrates%2520strong%2520performance%2520that%2520is%2520competitive%2520with%2520leading%250Aadversarial%2520training%2520techniques%252C%2520highlighting%2520significant%2520achievements%2520in%2520both%250Arobust%2520and%2520clean%2520accuracy.%2520Our%2520findings%2520reveal%2520the%2520importance%2520of%2520addressing%250Ainter-class%2520feature%2520proximity%2520explicitly%2520to%2520bolster%2520adversarial%2520robustness%2520in%250ADNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nearest%20Neighbor%20Projection%20Removal%20Adversarial%20Training&entry.906535625=Himanshu%20Singh%20and%20A.%20V.%20Subramanyam%20and%20Shivank%20Rajput%20and%20Mohan%20Kankanhalli&entry.1292438233=%20%20Deep%20neural%20networks%20have%20exhibited%20impressive%20performance%20in%20image%0Aclassification%20tasks%20but%20remain%20vulnerable%20to%20adversarial%20examples.%20Standard%0Aadversarial%20training%20enhances%20robustness%20but%20typically%20fails%20to%20explicitly%0Aaddress%20inter-class%20feature%20overlap%2C%20a%20significant%20contributor%20to%20adversarial%0Asusceptibility.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20adversarial%20training%0Aframework%20that%20actively%20mitigates%20inter-class%20proximity%20by%20projecting%20out%0Ainter-class%20dependencies%20from%20adversarial%20and%20clean%20samples%20in%20the%20feature%0Aspace.%20Specifically%2C%20our%20approach%20first%20identifies%20the%20nearest%20inter-class%0Aneighbors%20for%20each%20adversarial%20sample%20and%20subsequently%20removes%20projections%20onto%0Athese%20neighbors%20to%20enforce%20stronger%20feature%20separability.%20Theoretically%2C%20we%0Ademonstrate%20that%20our%20proposed%20logits%20correction%20reduces%20the%20Lipschitz%20constant%0Aof%20neural%20networks%2C%20thereby%20lowering%20the%20Rademacher%20complexity%2C%20which%20directly%0Acontributes%20to%20improved%20generalization%20and%20robustness.%20Extensive%20experiments%0Aacross%20standard%20benchmarks%20including%20CIFAR-10%2C%20CIFAR-100%2C%20and%20SVHN%20show%20that%0Aour%20method%20demonstrates%20strong%20performance%20that%20is%20competitive%20with%20leading%0Aadversarial%20training%20techniques%2C%20highlighting%20significant%20achievements%20in%20both%0Arobust%20and%20clean%20accuracy.%20Our%20findings%20reveal%20the%20importance%20of%20addressing%0Ainter-class%20feature%20proximity%20explicitly%20to%20bolster%20adversarial%20robustness%20in%0ADNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07673v1&entry.124074799=Read"},
{"title": "uGMM-NN: Univariate Gaussian Mixture Model Neural Network", "author": "Zakeria Sharif Ali", "abstract": "  This paper introduces the Univariate Gaussian Mixture Model Neural Network\n(uGMM-NN), a novel neural architecture that embeds probabilistic reasoning\ndirectly into the computational units of deep networks. Unlike traditional\nneurons, which apply weighted sums followed by fixed nonlinearities, each\nuGMM-NN node parameterizes its activations as a univariate Gaussian mixture,\nwith learnable means, variances, and mixing coefficients. This design enables\nricher representations by capturing multimodality and uncertainty at the level\nof individual neurons, while retaining the scalability of standard feedforward\nnetworks. We demonstrate that uGMM-NN can achieve competitive discriminative\nperformance compared to conventional multilayer perceptrons, while additionally\noffering a probabilistic interpretation of activations. The proposed framework\nprovides a foundation for integrating uncertainty-aware components into modern\nneural architectures, opening new directions for both discriminative and\ngenerative modeling.\n", "link": "http://arxiv.org/abs/2509.07569v1", "date": "2025-09-09", "relevancy": 2.5704, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5347}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5125}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20uGMM-NN%3A%20Univariate%20Gaussian%20Mixture%20Model%20Neural%20Network&body=Title%3A%20uGMM-NN%3A%20Univariate%20Gaussian%20Mixture%20Model%20Neural%20Network%0AAuthor%3A%20Zakeria%20Sharif%20Ali%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20Univariate%20Gaussian%20Mixture%20Model%20Neural%20Network%0A%28uGMM-NN%29%2C%20a%20novel%20neural%20architecture%20that%20embeds%20probabilistic%20reasoning%0Adirectly%20into%20the%20computational%20units%20of%20deep%20networks.%20Unlike%20traditional%0Aneurons%2C%20which%20apply%20weighted%20sums%20followed%20by%20fixed%20nonlinearities%2C%20each%0AuGMM-NN%20node%20parameterizes%20its%20activations%20as%20a%20univariate%20Gaussian%20mixture%2C%0Awith%20learnable%20means%2C%20variances%2C%20and%20mixing%20coefficients.%20This%20design%20enables%0Aricher%20representations%20by%20capturing%20multimodality%20and%20uncertainty%20at%20the%20level%0Aof%20individual%20neurons%2C%20while%20retaining%20the%20scalability%20of%20standard%20feedforward%0Anetworks.%20We%20demonstrate%20that%20uGMM-NN%20can%20achieve%20competitive%20discriminative%0Aperformance%20compared%20to%20conventional%20multilayer%20perceptrons%2C%20while%20additionally%0Aoffering%20a%20probabilistic%20interpretation%20of%20activations.%20The%20proposed%20framework%0Aprovides%20a%20foundation%20for%20integrating%20uncertainty-aware%20components%20into%20modern%0Aneural%20architectures%2C%20opening%20new%20directions%20for%20both%20discriminative%20and%0Agenerative%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DuGMM-NN%253A%2520Univariate%2520Gaussian%2520Mixture%2520Model%2520Neural%2520Network%26entry.906535625%3DZakeria%2520Sharif%2520Ali%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520Univariate%2520Gaussian%2520Mixture%2520Model%2520Neural%2520Network%250A%2528uGMM-NN%2529%252C%2520a%2520novel%2520neural%2520architecture%2520that%2520embeds%2520probabilistic%2520reasoning%250Adirectly%2520into%2520the%2520computational%2520units%2520of%2520deep%2520networks.%2520Unlike%2520traditional%250Aneurons%252C%2520which%2520apply%2520weighted%2520sums%2520followed%2520by%2520fixed%2520nonlinearities%252C%2520each%250AuGMM-NN%2520node%2520parameterizes%2520its%2520activations%2520as%2520a%2520univariate%2520Gaussian%2520mixture%252C%250Awith%2520learnable%2520means%252C%2520variances%252C%2520and%2520mixing%2520coefficients.%2520This%2520design%2520enables%250Aricher%2520representations%2520by%2520capturing%2520multimodality%2520and%2520uncertainty%2520at%2520the%2520level%250Aof%2520individual%2520neurons%252C%2520while%2520retaining%2520the%2520scalability%2520of%2520standard%2520feedforward%250Anetworks.%2520We%2520demonstrate%2520that%2520uGMM-NN%2520can%2520achieve%2520competitive%2520discriminative%250Aperformance%2520compared%2520to%2520conventional%2520multilayer%2520perceptrons%252C%2520while%2520additionally%250Aoffering%2520a%2520probabilistic%2520interpretation%2520of%2520activations.%2520The%2520proposed%2520framework%250Aprovides%2520a%2520foundation%2520for%2520integrating%2520uncertainty-aware%2520components%2520into%2520modern%250Aneural%2520architectures%252C%2520opening%2520new%2520directions%2520for%2520both%2520discriminative%2520and%250Agenerative%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=uGMM-NN%3A%20Univariate%20Gaussian%20Mixture%20Model%20Neural%20Network&entry.906535625=Zakeria%20Sharif%20Ali&entry.1292438233=%20%20This%20paper%20introduces%20the%20Univariate%20Gaussian%20Mixture%20Model%20Neural%20Network%0A%28uGMM-NN%29%2C%20a%20novel%20neural%20architecture%20that%20embeds%20probabilistic%20reasoning%0Adirectly%20into%20the%20computational%20units%20of%20deep%20networks.%20Unlike%20traditional%0Aneurons%2C%20which%20apply%20weighted%20sums%20followed%20by%20fixed%20nonlinearities%2C%20each%0AuGMM-NN%20node%20parameterizes%20its%20activations%20as%20a%20univariate%20Gaussian%20mixture%2C%0Awith%20learnable%20means%2C%20variances%2C%20and%20mixing%20coefficients.%20This%20design%20enables%0Aricher%20representations%20by%20capturing%20multimodality%20and%20uncertainty%20at%20the%20level%0Aof%20individual%20neurons%2C%20while%20retaining%20the%20scalability%20of%20standard%20feedforward%0Anetworks.%20We%20demonstrate%20that%20uGMM-NN%20can%20achieve%20competitive%20discriminative%0Aperformance%20compared%20to%20conventional%20multilayer%20perceptrons%2C%20while%20additionally%0Aoffering%20a%20probabilistic%20interpretation%20of%20activations.%20The%20proposed%20framework%0Aprovides%20a%20foundation%20for%20integrating%20uncertainty-aware%20components%20into%20modern%0Aneural%20architectures%2C%20opening%20new%20directions%20for%20both%20discriminative%20and%0Agenerative%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07569v1&entry.124074799=Read"},
{"title": "Missing Fine Details in Images: Last Seen in High Frequencies", "author": "Tejaswini Medi and Hsien-Yi Wang and Arianna Rampini and Margret Keuper", "abstract": "  Latent generative models have shown remarkable progress in high-fidelity\nimage synthesis, typically using a two-stage training process that involves\ncompressing images into latent embeddings via learned tokenizers in the first\nstage. The quality of generation strongly depends on how expressive and\nwell-optimized these latent embeddings are. While various methods have been\nproposed to learn effective latent representations, generated images often lack\nrealism, particularly in textured regions with sharp transitions, due to loss\nof fine details governed by high frequencies. We conduct a detailed frequency\ndecomposition of existing state-of-the-art (SOTA) latent tokenizers and show\nthat conventional objectives inherently prioritize low-frequency\nreconstruction, often at the expense of high-frequency fidelity. Our analysis\nreveals these latent tokenizers exhibit a bias toward low-frequency information\nduring optimization, leading to over-smoothed outputs and visual artifacts that\ndiminish perceptual quality. To address this, we propose a wavelet-based,\nfrequency-aware variational autoencoder (FA-VAE) framework that explicitly\ndecouples the optimization of low- and high-frequency components. This\ndecoupling enables improved reconstruction of fine textures while preserving\nglobal structure. Moreover, we integrate our frequency-preserving latent\nembeddings into a SOTA latent diffusion model, resulting in sharper and more\nrealistic image generation. Our approach bridges the fidelity gap in current\nlatent tokenizers and emphasizes the importance of frequency-aware optimization\nfor realistic image synthesis, with broader implications for applications in\ncontent creation, neural rendering, and medical imaging.\n", "link": "http://arxiv.org/abs/2509.05441v2", "date": "2025-09-09", "relevancy": 2.557, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6462}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6425}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Missing%20Fine%20Details%20in%20Images%3A%20Last%20Seen%20in%20High%20Frequencies&body=Title%3A%20Missing%20Fine%20Details%20in%20Images%3A%20Last%20Seen%20in%20High%20Frequencies%0AAuthor%3A%20Tejaswini%20Medi%20and%20Hsien-Yi%20Wang%20and%20Arianna%20Rampini%20and%20Margret%20Keuper%0AAbstract%3A%20%20%20Latent%20generative%20models%20have%20shown%20remarkable%20progress%20in%20high-fidelity%0Aimage%20synthesis%2C%20typically%20using%20a%20two-stage%20training%20process%20that%20involves%0Acompressing%20images%20into%20latent%20embeddings%20via%20learned%20tokenizers%20in%20the%20first%0Astage.%20The%20quality%20of%20generation%20strongly%20depends%20on%20how%20expressive%20and%0Awell-optimized%20these%20latent%20embeddings%20are.%20While%20various%20methods%20have%20been%0Aproposed%20to%20learn%20effective%20latent%20representations%2C%20generated%20images%20often%20lack%0Arealism%2C%20particularly%20in%20textured%20regions%20with%20sharp%20transitions%2C%20due%20to%20loss%0Aof%20fine%20details%20governed%20by%20high%20frequencies.%20We%20conduct%20a%20detailed%20frequency%0Adecomposition%20of%20existing%20state-of-the-art%20%28SOTA%29%20latent%20tokenizers%20and%20show%0Athat%20conventional%20objectives%20inherently%20prioritize%20low-frequency%0Areconstruction%2C%20often%20at%20the%20expense%20of%20high-frequency%20fidelity.%20Our%20analysis%0Areveals%20these%20latent%20tokenizers%20exhibit%20a%20bias%20toward%20low-frequency%20information%0Aduring%20optimization%2C%20leading%20to%20over-smoothed%20outputs%20and%20visual%20artifacts%20that%0Adiminish%20perceptual%20quality.%20To%20address%20this%2C%20we%20propose%20a%20wavelet-based%2C%0Afrequency-aware%20variational%20autoencoder%20%28FA-VAE%29%20framework%20that%20explicitly%0Adecouples%20the%20optimization%20of%20low-%20and%20high-frequency%20components.%20This%0Adecoupling%20enables%20improved%20reconstruction%20of%20fine%20textures%20while%20preserving%0Aglobal%20structure.%20Moreover%2C%20we%20integrate%20our%20frequency-preserving%20latent%0Aembeddings%20into%20a%20SOTA%20latent%20diffusion%20model%2C%20resulting%20in%20sharper%20and%20more%0Arealistic%20image%20generation.%20Our%20approach%20bridges%20the%20fidelity%20gap%20in%20current%0Alatent%20tokenizers%20and%20emphasizes%20the%20importance%20of%20frequency-aware%20optimization%0Afor%20realistic%20image%20synthesis%2C%20with%20broader%20implications%20for%20applications%20in%0Acontent%20creation%2C%20neural%20rendering%2C%20and%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05441v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMissing%2520Fine%2520Details%2520in%2520Images%253A%2520Last%2520Seen%2520in%2520High%2520Frequencies%26entry.906535625%3DTejaswini%2520Medi%2520and%2520Hsien-Yi%2520Wang%2520and%2520Arianna%2520Rampini%2520and%2520Margret%2520Keuper%26entry.1292438233%3D%2520%2520Latent%2520generative%2520models%2520have%2520shown%2520remarkable%2520progress%2520in%2520high-fidelity%250Aimage%2520synthesis%252C%2520typically%2520using%2520a%2520two-stage%2520training%2520process%2520that%2520involves%250Acompressing%2520images%2520into%2520latent%2520embeddings%2520via%2520learned%2520tokenizers%2520in%2520the%2520first%250Astage.%2520The%2520quality%2520of%2520generation%2520strongly%2520depends%2520on%2520how%2520expressive%2520and%250Awell-optimized%2520these%2520latent%2520embeddings%2520are.%2520While%2520various%2520methods%2520have%2520been%250Aproposed%2520to%2520learn%2520effective%2520latent%2520representations%252C%2520generated%2520images%2520often%2520lack%250Arealism%252C%2520particularly%2520in%2520textured%2520regions%2520with%2520sharp%2520transitions%252C%2520due%2520to%2520loss%250Aof%2520fine%2520details%2520governed%2520by%2520high%2520frequencies.%2520We%2520conduct%2520a%2520detailed%2520frequency%250Adecomposition%2520of%2520existing%2520state-of-the-art%2520%2528SOTA%2529%2520latent%2520tokenizers%2520and%2520show%250Athat%2520conventional%2520objectives%2520inherently%2520prioritize%2520low-frequency%250Areconstruction%252C%2520often%2520at%2520the%2520expense%2520of%2520high-frequency%2520fidelity.%2520Our%2520analysis%250Areveals%2520these%2520latent%2520tokenizers%2520exhibit%2520a%2520bias%2520toward%2520low-frequency%2520information%250Aduring%2520optimization%252C%2520leading%2520to%2520over-smoothed%2520outputs%2520and%2520visual%2520artifacts%2520that%250Adiminish%2520perceptual%2520quality.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520wavelet-based%252C%250Afrequency-aware%2520variational%2520autoencoder%2520%2528FA-VAE%2529%2520framework%2520that%2520explicitly%250Adecouples%2520the%2520optimization%2520of%2520low-%2520and%2520high-frequency%2520components.%2520This%250Adecoupling%2520enables%2520improved%2520reconstruction%2520of%2520fine%2520textures%2520while%2520preserving%250Aglobal%2520structure.%2520Moreover%252C%2520we%2520integrate%2520our%2520frequency-preserving%2520latent%250Aembeddings%2520into%2520a%2520SOTA%2520latent%2520diffusion%2520model%252C%2520resulting%2520in%2520sharper%2520and%2520more%250Arealistic%2520image%2520generation.%2520Our%2520approach%2520bridges%2520the%2520fidelity%2520gap%2520in%2520current%250Alatent%2520tokenizers%2520and%2520emphasizes%2520the%2520importance%2520of%2520frequency-aware%2520optimization%250Afor%2520realistic%2520image%2520synthesis%252C%2520with%2520broader%2520implications%2520for%2520applications%2520in%250Acontent%2520creation%252C%2520neural%2520rendering%252C%2520and%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05441v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Missing%20Fine%20Details%20in%20Images%3A%20Last%20Seen%20in%20High%20Frequencies&entry.906535625=Tejaswini%20Medi%20and%20Hsien-Yi%20Wang%20and%20Arianna%20Rampini%20and%20Margret%20Keuper&entry.1292438233=%20%20Latent%20generative%20models%20have%20shown%20remarkable%20progress%20in%20high-fidelity%0Aimage%20synthesis%2C%20typically%20using%20a%20two-stage%20training%20process%20that%20involves%0Acompressing%20images%20into%20latent%20embeddings%20via%20learned%20tokenizers%20in%20the%20first%0Astage.%20The%20quality%20of%20generation%20strongly%20depends%20on%20how%20expressive%20and%0Awell-optimized%20these%20latent%20embeddings%20are.%20While%20various%20methods%20have%20been%0Aproposed%20to%20learn%20effective%20latent%20representations%2C%20generated%20images%20often%20lack%0Arealism%2C%20particularly%20in%20textured%20regions%20with%20sharp%20transitions%2C%20due%20to%20loss%0Aof%20fine%20details%20governed%20by%20high%20frequencies.%20We%20conduct%20a%20detailed%20frequency%0Adecomposition%20of%20existing%20state-of-the-art%20%28SOTA%29%20latent%20tokenizers%20and%20show%0Athat%20conventional%20objectives%20inherently%20prioritize%20low-frequency%0Areconstruction%2C%20often%20at%20the%20expense%20of%20high-frequency%20fidelity.%20Our%20analysis%0Areveals%20these%20latent%20tokenizers%20exhibit%20a%20bias%20toward%20low-frequency%20information%0Aduring%20optimization%2C%20leading%20to%20over-smoothed%20outputs%20and%20visual%20artifacts%20that%0Adiminish%20perceptual%20quality.%20To%20address%20this%2C%20we%20propose%20a%20wavelet-based%2C%0Afrequency-aware%20variational%20autoencoder%20%28FA-VAE%29%20framework%20that%20explicitly%0Adecouples%20the%20optimization%20of%20low-%20and%20high-frequency%20components.%20This%0Adecoupling%20enables%20improved%20reconstruction%20of%20fine%20textures%20while%20preserving%0Aglobal%20structure.%20Moreover%2C%20we%20integrate%20our%20frequency-preserving%20latent%0Aembeddings%20into%20a%20SOTA%20latent%20diffusion%20model%2C%20resulting%20in%20sharper%20and%20more%0Arealistic%20image%20generation.%20Our%20approach%20bridges%20the%20fidelity%20gap%20in%20current%0Alatent%20tokenizers%20and%20emphasizes%20the%20importance%20of%20frequency-aware%20optimization%0Afor%20realistic%20image%20synthesis%2C%20with%20broader%20implications%20for%20applications%20in%0Acontent%20creation%2C%20neural%20rendering%2C%20and%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05441v2&entry.124074799=Read"},
{"title": "A multi-task neural network for atypical mitosis recognition under\n  domain shift", "author": "Gennaro Percannella and Mattia Sarno and Francesco Tortorella and Mario Vento", "abstract": "  Recognizing atypical mitotic figures in histopathology images allows\nphysicians to correctly assess tumor aggressiveness. Although machine learning\nmodels could be exploited for automatically performing such a task, under\ndomain shift these models suffer from significative performance drops. In this\nwork, an approach based on multi-task learning is proposed for addressing this\nproblem. By exploiting auxiliary tasks, correlated to the main classification\ntask, the proposed approach, submitted to the track 2 of the MItosis DOmain\nGeneralization (MIDOG) challenge, aims to aid the model to focus only on the\nobject to classify, ignoring the domain varying background of the image. The\nproposed approach shows promising performance in a preliminary evaluation\nconducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training\nSet, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25\nchallenge.\n", "link": "http://arxiv.org/abs/2508.21035v3", "date": "2025-09-09", "relevancy": 2.5546, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5276}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5088}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20multi-task%20neural%20network%20for%20atypical%20mitosis%20recognition%20under%0A%20%20domain%20shift&body=Title%3A%20A%20multi-task%20neural%20network%20for%20atypical%20mitosis%20recognition%20under%0A%20%20domain%20shift%0AAuthor%3A%20Gennaro%20Percannella%20and%20Mattia%20Sarno%20and%20Francesco%20Tortorella%20and%20Mario%20Vento%0AAbstract%3A%20%20%20Recognizing%20atypical%20mitotic%20figures%20in%20histopathology%20images%20allows%0Aphysicians%20to%20correctly%20assess%20tumor%20aggressiveness.%20Although%20machine%20learning%0Amodels%20could%20be%20exploited%20for%20automatically%20performing%20such%20a%20task%2C%20under%0Adomain%20shift%20these%20models%20suffer%20from%20significative%20performance%20drops.%20In%20this%0Awork%2C%20an%20approach%20based%20on%20multi-task%20learning%20is%20proposed%20for%20addressing%20this%0Aproblem.%20By%20exploiting%20auxiliary%20tasks%2C%20correlated%20to%20the%20main%20classification%0Atask%2C%20the%20proposed%20approach%2C%20submitted%20to%20the%20track%202%20of%20the%20MItosis%20DOmain%0AGeneralization%20%28MIDOG%29%20challenge%2C%20aims%20to%20aid%20the%20model%20to%20focus%20only%20on%20the%0Aobject%20to%20classify%2C%20ignoring%20the%20domain%20varying%20background%20of%20the%20image.%20The%0Aproposed%20approach%20shows%20promising%20performance%20in%20a%20preliminary%20evaluation%0Aconducted%20on%20three%20distinct%20datasets%2C%20i.e.%2C%20the%20MIDOG%202025%20Atypical%20Training%0ASet%2C%20the%20Ami-Br%20dataset%2C%20as%20well%20as%20the%20preliminary%20test%20set%20of%20the%20MIDOG25%0Achallenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21035v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520multi-task%2520neural%2520network%2520for%2520atypical%2520mitosis%2520recognition%2520under%250A%2520%2520domain%2520shift%26entry.906535625%3DGennaro%2520Percannella%2520and%2520Mattia%2520Sarno%2520and%2520Francesco%2520Tortorella%2520and%2520Mario%2520Vento%26entry.1292438233%3D%2520%2520Recognizing%2520atypical%2520mitotic%2520figures%2520in%2520histopathology%2520images%2520allows%250Aphysicians%2520to%2520correctly%2520assess%2520tumor%2520aggressiveness.%2520Although%2520machine%2520learning%250Amodels%2520could%2520be%2520exploited%2520for%2520automatically%2520performing%2520such%2520a%2520task%252C%2520under%250Adomain%2520shift%2520these%2520models%2520suffer%2520from%2520significative%2520performance%2520drops.%2520In%2520this%250Awork%252C%2520an%2520approach%2520based%2520on%2520multi-task%2520learning%2520is%2520proposed%2520for%2520addressing%2520this%250Aproblem.%2520By%2520exploiting%2520auxiliary%2520tasks%252C%2520correlated%2520to%2520the%2520main%2520classification%250Atask%252C%2520the%2520proposed%2520approach%252C%2520submitted%2520to%2520the%2520track%25202%2520of%2520the%2520MItosis%2520DOmain%250AGeneralization%2520%2528MIDOG%2529%2520challenge%252C%2520aims%2520to%2520aid%2520the%2520model%2520to%2520focus%2520only%2520on%2520the%250Aobject%2520to%2520classify%252C%2520ignoring%2520the%2520domain%2520varying%2520background%2520of%2520the%2520image.%2520The%250Aproposed%2520approach%2520shows%2520promising%2520performance%2520in%2520a%2520preliminary%2520evaluation%250Aconducted%2520on%2520three%2520distinct%2520datasets%252C%2520i.e.%252C%2520the%2520MIDOG%25202025%2520Atypical%2520Training%250ASet%252C%2520the%2520Ami-Br%2520dataset%252C%2520as%2520well%2520as%2520the%2520preliminary%2520test%2520set%2520of%2520the%2520MIDOG25%250Achallenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21035v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20multi-task%20neural%20network%20for%20atypical%20mitosis%20recognition%20under%0A%20%20domain%20shift&entry.906535625=Gennaro%20Percannella%20and%20Mattia%20Sarno%20and%20Francesco%20Tortorella%20and%20Mario%20Vento&entry.1292438233=%20%20Recognizing%20atypical%20mitotic%20figures%20in%20histopathology%20images%20allows%0Aphysicians%20to%20correctly%20assess%20tumor%20aggressiveness.%20Although%20machine%20learning%0Amodels%20could%20be%20exploited%20for%20automatically%20performing%20such%20a%20task%2C%20under%0Adomain%20shift%20these%20models%20suffer%20from%20significative%20performance%20drops.%20In%20this%0Awork%2C%20an%20approach%20based%20on%20multi-task%20learning%20is%20proposed%20for%20addressing%20this%0Aproblem.%20By%20exploiting%20auxiliary%20tasks%2C%20correlated%20to%20the%20main%20classification%0Atask%2C%20the%20proposed%20approach%2C%20submitted%20to%20the%20track%202%20of%20the%20MItosis%20DOmain%0AGeneralization%20%28MIDOG%29%20challenge%2C%20aims%20to%20aid%20the%20model%20to%20focus%20only%20on%20the%0Aobject%20to%20classify%2C%20ignoring%20the%20domain%20varying%20background%20of%20the%20image.%20The%0Aproposed%20approach%20shows%20promising%20performance%20in%20a%20preliminary%20evaluation%0Aconducted%20on%20three%20distinct%20datasets%2C%20i.e.%2C%20the%20MIDOG%202025%20Atypical%20Training%0ASet%2C%20the%20Ami-Br%20dataset%2C%20as%20well%20as%20the%20preliminary%20test%20set%20of%20the%20MIDOG25%0Achallenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21035v3&entry.124074799=Read"},
{"title": "Conditional Video Generation for High-Efficiency Video Compression", "author": "Fangqiu Yi and Jingyu Xu and Jiawei Shao and Chi Zhang and Xuelong Li", "abstract": "  Perceptual studies demonstrate that conditional diffusion models excel at\nreconstructing video content aligned with human visual perception. Building on\nthis insight, we propose a video compression framework that leverages\nconditional diffusion models for perceptually optimized reconstruction.\nSpecifically, we reframe video compression as a conditional generation task,\nwhere a generative model synthesizes video from sparse, yet informative\nsignals. Our approach introduces three key modules: (1) Multi-granular\nconditioning that captures both static scene structure and dynamic\nspatio-temporal cues; (2) Compact representations designed for efficient\ntransmission without sacrificing semantic richness; (3) Multi-condition\ntraining with modality dropout and role-aware embeddings, which prevent\nover-reliance on any single modality and enhance robustness. Extensive\nexperiments show that our method significantly outperforms both traditional and\nneural codecs on perceptual quality metrics such as Fr\\'echet Video Distance\n(FVD) and LPIPS, especially under high compression ratios.\n", "link": "http://arxiv.org/abs/2507.15269v3", "date": "2025-09-09", "relevancy": 2.5219, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6327}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6314}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20Video%20Generation%20for%20High-Efficiency%20Video%20Compression&body=Title%3A%20Conditional%20Video%20Generation%20for%20High-Efficiency%20Video%20Compression%0AAuthor%3A%20Fangqiu%20Yi%20and%20Jingyu%20Xu%20and%20Jiawei%20Shao%20and%20Chi%20Zhang%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Perceptual%20studies%20demonstrate%20that%20conditional%20diffusion%20models%20excel%20at%0Areconstructing%20video%20content%20aligned%20with%20human%20visual%20perception.%20Building%20on%0Athis%20insight%2C%20we%20propose%20a%20video%20compression%20framework%20that%20leverages%0Aconditional%20diffusion%20models%20for%20perceptually%20optimized%20reconstruction.%0ASpecifically%2C%20we%20reframe%20video%20compression%20as%20a%20conditional%20generation%20task%2C%0Awhere%20a%20generative%20model%20synthesizes%20video%20from%20sparse%2C%20yet%20informative%0Asignals.%20Our%20approach%20introduces%20three%20key%20modules%3A%20%281%29%20Multi-granular%0Aconditioning%20that%20captures%20both%20static%20scene%20structure%20and%20dynamic%0Aspatio-temporal%20cues%3B%20%282%29%20Compact%20representations%20designed%20for%20efficient%0Atransmission%20without%20sacrificing%20semantic%20richness%3B%20%283%29%20Multi-condition%0Atraining%20with%20modality%20dropout%20and%20role-aware%20embeddings%2C%20which%20prevent%0Aover-reliance%20on%20any%20single%20modality%20and%20enhance%20robustness.%20Extensive%0Aexperiments%20show%20that%20our%20method%20significantly%20outperforms%20both%20traditional%20and%0Aneural%20codecs%20on%20perceptual%20quality%20metrics%20such%20as%20Fr%5C%27echet%20Video%20Distance%0A%28FVD%29%20and%20LPIPS%2C%20especially%20under%20high%20compression%20ratios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15269v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520Video%2520Generation%2520for%2520High-Efficiency%2520Video%2520Compression%26entry.906535625%3DFangqiu%2520Yi%2520and%2520Jingyu%2520Xu%2520and%2520Jiawei%2520Shao%2520and%2520Chi%2520Zhang%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Perceptual%2520studies%2520demonstrate%2520that%2520conditional%2520diffusion%2520models%2520excel%2520at%250Areconstructing%2520video%2520content%2520aligned%2520with%2520human%2520visual%2520perception.%2520Building%2520on%250Athis%2520insight%252C%2520we%2520propose%2520a%2520video%2520compression%2520framework%2520that%2520leverages%250Aconditional%2520diffusion%2520models%2520for%2520perceptually%2520optimized%2520reconstruction.%250ASpecifically%252C%2520we%2520reframe%2520video%2520compression%2520as%2520a%2520conditional%2520generation%2520task%252C%250Awhere%2520a%2520generative%2520model%2520synthesizes%2520video%2520from%2520sparse%252C%2520yet%2520informative%250Asignals.%2520Our%2520approach%2520introduces%2520three%2520key%2520modules%253A%2520%25281%2529%2520Multi-granular%250Aconditioning%2520that%2520captures%2520both%2520static%2520scene%2520structure%2520and%2520dynamic%250Aspatio-temporal%2520cues%253B%2520%25282%2529%2520Compact%2520representations%2520designed%2520for%2520efficient%250Atransmission%2520without%2520sacrificing%2520semantic%2520richness%253B%2520%25283%2529%2520Multi-condition%250Atraining%2520with%2520modality%2520dropout%2520and%2520role-aware%2520embeddings%252C%2520which%2520prevent%250Aover-reliance%2520on%2520any%2520single%2520modality%2520and%2520enhance%2520robustness.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520method%2520significantly%2520outperforms%2520both%2520traditional%2520and%250Aneural%2520codecs%2520on%2520perceptual%2520quality%2520metrics%2520such%2520as%2520Fr%255C%2527echet%2520Video%2520Distance%250A%2528FVD%2529%2520and%2520LPIPS%252C%2520especially%2520under%2520high%2520compression%2520ratios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15269v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Video%20Generation%20for%20High-Efficiency%20Video%20Compression&entry.906535625=Fangqiu%20Yi%20and%20Jingyu%20Xu%20and%20Jiawei%20Shao%20and%20Chi%20Zhang%20and%20Xuelong%20Li&entry.1292438233=%20%20Perceptual%20studies%20demonstrate%20that%20conditional%20diffusion%20models%20excel%20at%0Areconstructing%20video%20content%20aligned%20with%20human%20visual%20perception.%20Building%20on%0Athis%20insight%2C%20we%20propose%20a%20video%20compression%20framework%20that%20leverages%0Aconditional%20diffusion%20models%20for%20perceptually%20optimized%20reconstruction.%0ASpecifically%2C%20we%20reframe%20video%20compression%20as%20a%20conditional%20generation%20task%2C%0Awhere%20a%20generative%20model%20synthesizes%20video%20from%20sparse%2C%20yet%20informative%0Asignals.%20Our%20approach%20introduces%20three%20key%20modules%3A%20%281%29%20Multi-granular%0Aconditioning%20that%20captures%20both%20static%20scene%20structure%20and%20dynamic%0Aspatio-temporal%20cues%3B%20%282%29%20Compact%20representations%20designed%20for%20efficient%0Atransmission%20without%20sacrificing%20semantic%20richness%3B%20%283%29%20Multi-condition%0Atraining%20with%20modality%20dropout%20and%20role-aware%20embeddings%2C%20which%20prevent%0Aover-reliance%20on%20any%20single%20modality%20and%20enhance%20robustness.%20Extensive%0Aexperiments%20show%20that%20our%20method%20significantly%20outperforms%20both%20traditional%20and%0Aneural%20codecs%20on%20perceptual%20quality%20metrics%20such%20as%20Fr%5C%27echet%20Video%20Distance%0A%28FVD%29%20and%20LPIPS%2C%20especially%20under%20high%20compression%20ratios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15269v3&entry.124074799=Read"},
{"title": "Unleashing the True Potential of LLMs: A Feedback-Triggered\n  Self-Correction with Long-Term Multipath Decoding", "author": "Jipeng Li and Zeyu Gao and Yubin Qi and Hande Dong and Weijian Chen and Qiang Lin", "abstract": "  Large Language Models (LLMs) have achieved remarkable performance across\ndiverse tasks, yet their susceptibility to generating incorrect content during\ninference remains a critical unsolved challenge. While self-correction methods\noffer potential solutions, their effectiveness is hindered by two inherent\nlimitations: (1) the absence of reliable guidance signals for error\nlocalization, and (2) the restricted reasoning depth imposed by conventional\nnext-token decoding paradigms. To address these issues, we propose\nFeedback-Triggered Regeneration (FTR), a novel framework that synergizes user\nfeedback with enhanced decoding dynamics. Specifically, FTR activates response\nregeneration only upon receiving negative user feedback, thereby circumventing\nerror propagation from faulty self-assessment while preserving originally\ncorrect outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding,\nwhich enables systematic exploration of multiple reasoning trajectories through\ndelayed sequence evaluation, effectively overcoming the myopic decision-making\ncharacteristic of standard next-token prediction. Extensive experiments on\nmathematical reasoning and code generation benchmarks demonstrate that our\nframework achieves consistent and significant improvements over\nstate-of-the-art prompt-based self-correction methods.\n", "link": "http://arxiv.org/abs/2509.07676v1", "date": "2025-09-09", "relevancy": 2.5062, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5066}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20True%20Potential%20of%20LLMs%3A%20A%20Feedback-Triggered%0A%20%20Self-Correction%20with%20Long-Term%20Multipath%20Decoding&body=Title%3A%20Unleashing%20the%20True%20Potential%20of%20LLMs%3A%20A%20Feedback-Triggered%0A%20%20Self-Correction%20with%20Long-Term%20Multipath%20Decoding%0AAuthor%3A%20Jipeng%20Li%20and%20Zeyu%20Gao%20and%20Yubin%20Qi%20and%20Hande%20Dong%20and%20Weijian%20Chen%20and%20Qiang%20Lin%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20performance%20across%0Adiverse%20tasks%2C%20yet%20their%20susceptibility%20to%20generating%20incorrect%20content%20during%0Ainference%20remains%20a%20critical%20unsolved%20challenge.%20While%20self-correction%20methods%0Aoffer%20potential%20solutions%2C%20their%20effectiveness%20is%20hindered%20by%20two%20inherent%0Alimitations%3A%20%281%29%20the%20absence%20of%20reliable%20guidance%20signals%20for%20error%0Alocalization%2C%20and%20%282%29%20the%20restricted%20reasoning%20depth%20imposed%20by%20conventional%0Anext-token%20decoding%20paradigms.%20To%20address%20these%20issues%2C%20we%20propose%0AFeedback-Triggered%20Regeneration%20%28FTR%29%2C%20a%20novel%20framework%20that%20synergizes%20user%0Afeedback%20with%20enhanced%20decoding%20dynamics.%20Specifically%2C%20FTR%20activates%20response%0Aregeneration%20only%20upon%20receiving%20negative%20user%20feedback%2C%20thereby%20circumventing%0Aerror%20propagation%20from%20faulty%20self-assessment%20while%20preserving%20originally%0Acorrect%20outputs.%20Furthermore%2C%20we%20introduce%20Long-Term%20Multipath%20%28LTM%29%20decoding%2C%0Awhich%20enables%20systematic%20exploration%20of%20multiple%20reasoning%20trajectories%20through%0Adelayed%20sequence%20evaluation%2C%20effectively%20overcoming%20the%20myopic%20decision-making%0Acharacteristic%20of%20standard%20next-token%20prediction.%20Extensive%20experiments%20on%0Amathematical%20reasoning%20and%20code%20generation%20benchmarks%20demonstrate%20that%20our%0Aframework%20achieves%20consistent%20and%20significant%20improvements%20over%0Astate-of-the-art%20prompt-based%20self-correction%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520True%2520Potential%2520of%2520LLMs%253A%2520A%2520Feedback-Triggered%250A%2520%2520Self-Correction%2520with%2520Long-Term%2520Multipath%2520Decoding%26entry.906535625%3DJipeng%2520Li%2520and%2520Zeyu%2520Gao%2520and%2520Yubin%2520Qi%2520and%2520Hande%2520Dong%2520and%2520Weijian%2520Chen%2520and%2520Qiang%2520Lin%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520performance%2520across%250Adiverse%2520tasks%252C%2520yet%2520their%2520susceptibility%2520to%2520generating%2520incorrect%2520content%2520during%250Ainference%2520remains%2520a%2520critical%2520unsolved%2520challenge.%2520While%2520self-correction%2520methods%250Aoffer%2520potential%2520solutions%252C%2520their%2520effectiveness%2520is%2520hindered%2520by%2520two%2520inherent%250Alimitations%253A%2520%25281%2529%2520the%2520absence%2520of%2520reliable%2520guidance%2520signals%2520for%2520error%250Alocalization%252C%2520and%2520%25282%2529%2520the%2520restricted%2520reasoning%2520depth%2520imposed%2520by%2520conventional%250Anext-token%2520decoding%2520paradigms.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250AFeedback-Triggered%2520Regeneration%2520%2528FTR%2529%252C%2520a%2520novel%2520framework%2520that%2520synergizes%2520user%250Afeedback%2520with%2520enhanced%2520decoding%2520dynamics.%2520Specifically%252C%2520FTR%2520activates%2520response%250Aregeneration%2520only%2520upon%2520receiving%2520negative%2520user%2520feedback%252C%2520thereby%2520circumventing%250Aerror%2520propagation%2520from%2520faulty%2520self-assessment%2520while%2520preserving%2520originally%250Acorrect%2520outputs.%2520Furthermore%252C%2520we%2520introduce%2520Long-Term%2520Multipath%2520%2528LTM%2529%2520decoding%252C%250Awhich%2520enables%2520systematic%2520exploration%2520of%2520multiple%2520reasoning%2520trajectories%2520through%250Adelayed%2520sequence%2520evaluation%252C%2520effectively%2520overcoming%2520the%2520myopic%2520decision-making%250Acharacteristic%2520of%2520standard%2520next-token%2520prediction.%2520Extensive%2520experiments%2520on%250Amathematical%2520reasoning%2520and%2520code%2520generation%2520benchmarks%2520demonstrate%2520that%2520our%250Aframework%2520achieves%2520consistent%2520and%2520significant%2520improvements%2520over%250Astate-of-the-art%2520prompt-based%2520self-correction%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20True%20Potential%20of%20LLMs%3A%20A%20Feedback-Triggered%0A%20%20Self-Correction%20with%20Long-Term%20Multipath%20Decoding&entry.906535625=Jipeng%20Li%20and%20Zeyu%20Gao%20and%20Yubin%20Qi%20and%20Hande%20Dong%20and%20Weijian%20Chen%20and%20Qiang%20Lin&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20performance%20across%0Adiverse%20tasks%2C%20yet%20their%20susceptibility%20to%20generating%20incorrect%20content%20during%0Ainference%20remains%20a%20critical%20unsolved%20challenge.%20While%20self-correction%20methods%0Aoffer%20potential%20solutions%2C%20their%20effectiveness%20is%20hindered%20by%20two%20inherent%0Alimitations%3A%20%281%29%20the%20absence%20of%20reliable%20guidance%20signals%20for%20error%0Alocalization%2C%20and%20%282%29%20the%20restricted%20reasoning%20depth%20imposed%20by%20conventional%0Anext-token%20decoding%20paradigms.%20To%20address%20these%20issues%2C%20we%20propose%0AFeedback-Triggered%20Regeneration%20%28FTR%29%2C%20a%20novel%20framework%20that%20synergizes%20user%0Afeedback%20with%20enhanced%20decoding%20dynamics.%20Specifically%2C%20FTR%20activates%20response%0Aregeneration%20only%20upon%20receiving%20negative%20user%20feedback%2C%20thereby%20circumventing%0Aerror%20propagation%20from%20faulty%20self-assessment%20while%20preserving%20originally%0Acorrect%20outputs.%20Furthermore%2C%20we%20introduce%20Long-Term%20Multipath%20%28LTM%29%20decoding%2C%0Awhich%20enables%20systematic%20exploration%20of%20multiple%20reasoning%20trajectories%20through%0Adelayed%20sequence%20evaluation%2C%20effectively%20overcoming%20the%20myopic%20decision-making%0Acharacteristic%20of%20standard%20next-token%20prediction.%20Extensive%20experiments%20on%0Amathematical%20reasoning%20and%20code%20generation%20benchmarks%20demonstrate%20that%20our%0Aframework%20achieves%20consistent%20and%20significant%20improvements%20over%0Astate-of-the-art%20prompt-based%20self-correction%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07676v1&entry.124074799=Read"},
{"title": "Graph-based Integrated Gradients for Explaining Graph Neural Networks", "author": "Lachlan Simpson and Kyle Millar and Adriel Cheng and Cheng-Chew Lim and Hong Gunn Chew", "abstract": "  Integrated Gradients (IG) is a common explainability technique to address the\nblack-box problem of neural networks. Integrated gradients assumes continuous\ndata. Graphs are discrete structures making IG ill-suited to graphs. In this\nwork, we introduce graph-based integrated gradients (GB-IG); an extension of IG\nto graphs. We demonstrate on four synthetic datasets that GB-IG accurately\nidentifies crucial structural components of the graph used in classification\ntasks. We further demonstrate on three prevalent real-world graph datasets that\nGB-IG outperforms IG in highlighting important features for node classification\ntasks.\n", "link": "http://arxiv.org/abs/2509.07648v1", "date": "2025-09-09", "relevancy": 2.4701, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5118}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4987}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-based%20Integrated%20Gradients%20for%20Explaining%20Graph%20Neural%20Networks&body=Title%3A%20Graph-based%20Integrated%20Gradients%20for%20Explaining%20Graph%20Neural%20Networks%0AAuthor%3A%20Lachlan%20Simpson%20and%20Kyle%20Millar%20and%20Adriel%20Cheng%20and%20Cheng-Chew%20Lim%20and%20Hong%20Gunn%20Chew%0AAbstract%3A%20%20%20Integrated%20Gradients%20%28IG%29%20is%20a%20common%20explainability%20technique%20to%20address%20the%0Ablack-box%20problem%20of%20neural%20networks.%20Integrated%20gradients%20assumes%20continuous%0Adata.%20Graphs%20are%20discrete%20structures%20making%20IG%20ill-suited%20to%20graphs.%20In%20this%0Awork%2C%20we%20introduce%20graph-based%20integrated%20gradients%20%28GB-IG%29%3B%20an%20extension%20of%20IG%0Ato%20graphs.%20We%20demonstrate%20on%20four%20synthetic%20datasets%20that%20GB-IG%20accurately%0Aidentifies%20crucial%20structural%20components%20of%20the%20graph%20used%20in%20classification%0Atasks.%20We%20further%20demonstrate%20on%20three%20prevalent%20real-world%20graph%20datasets%20that%0AGB-IG%20outperforms%20IG%20in%20highlighting%20important%20features%20for%20node%20classification%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-based%2520Integrated%2520Gradients%2520for%2520Explaining%2520Graph%2520Neural%2520Networks%26entry.906535625%3DLachlan%2520Simpson%2520and%2520Kyle%2520Millar%2520and%2520Adriel%2520Cheng%2520and%2520Cheng-Chew%2520Lim%2520and%2520Hong%2520Gunn%2520Chew%26entry.1292438233%3D%2520%2520Integrated%2520Gradients%2520%2528IG%2529%2520is%2520a%2520common%2520explainability%2520technique%2520to%2520address%2520the%250Ablack-box%2520problem%2520of%2520neural%2520networks.%2520Integrated%2520gradients%2520assumes%2520continuous%250Adata.%2520Graphs%2520are%2520discrete%2520structures%2520making%2520IG%2520ill-suited%2520to%2520graphs.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520graph-based%2520integrated%2520gradients%2520%2528GB-IG%2529%253B%2520an%2520extension%2520of%2520IG%250Ato%2520graphs.%2520We%2520demonstrate%2520on%2520four%2520synthetic%2520datasets%2520that%2520GB-IG%2520accurately%250Aidentifies%2520crucial%2520structural%2520components%2520of%2520the%2520graph%2520used%2520in%2520classification%250Atasks.%2520We%2520further%2520demonstrate%2520on%2520three%2520prevalent%2520real-world%2520graph%2520datasets%2520that%250AGB-IG%2520outperforms%2520IG%2520in%2520highlighting%2520important%2520features%2520for%2520node%2520classification%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-based%20Integrated%20Gradients%20for%20Explaining%20Graph%20Neural%20Networks&entry.906535625=Lachlan%20Simpson%20and%20Kyle%20Millar%20and%20Adriel%20Cheng%20and%20Cheng-Chew%20Lim%20and%20Hong%20Gunn%20Chew&entry.1292438233=%20%20Integrated%20Gradients%20%28IG%29%20is%20a%20common%20explainability%20technique%20to%20address%20the%0Ablack-box%20problem%20of%20neural%20networks.%20Integrated%20gradients%20assumes%20continuous%0Adata.%20Graphs%20are%20discrete%20structures%20making%20IG%20ill-suited%20to%20graphs.%20In%20this%0Awork%2C%20we%20introduce%20graph-based%20integrated%20gradients%20%28GB-IG%29%3B%20an%20extension%20of%20IG%0Ato%20graphs.%20We%20demonstrate%20on%20four%20synthetic%20datasets%20that%20GB-IG%20accurately%0Aidentifies%20crucial%20structural%20components%20of%20the%20graph%20used%20in%20classification%0Atasks.%20We%20further%20demonstrate%20on%20three%20prevalent%20real-world%20graph%20datasets%20that%0AGB-IG%20outperforms%20IG%20in%20highlighting%20important%20features%20for%20node%20classification%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07648v1&entry.124074799=Read"},
{"title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control", "author": "Delin Qu and Haoming Song and Qizhi Chen and Zhaoqing Chen and Xianqiang Gao and Xinyi Ye and Qi Lv and Modi Shi and Guanghui Ren and Cheng Ruan and Maoqing Yao and Haoran Yang and Jiacheng Bao and Bin Zhao and Dong Wang", "abstract": "  The human ability to seamlessly perform multimodal reasoning and physical\ninteraction in the open world is a core goal for general-purpose embodied\nintelligent systems. Recent vision-language-action (VLA) models, which are\nco-trained on large-scale robot and visual-text data, have demonstrated notable\nprogress in general robot control. However, they still fail to achieve\nhuman-level flexibility in interleaved reasoning and interaction. In this work,\nintroduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is\na unified embodied foundation model that achieves superior performance in\nmultimodal embodied reasoning and robot control through interleaved\nvision-text-action pre-training. The development of EO-1 is based on two key\npillars: (i) a unified architecture that processes multimodal inputs\nindiscriminately (image, text, video, and action), and (ii) a massive,\nhigh-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains\nover 1.5 million samples with emphasis on interleaved vision-text-action\ncomprehension. EO-1 is trained through synergies between auto-regressive\ndecoding and flow matching denoising on EO-Data1.5M, enabling seamless robot\naction generation and multimodal embodied reasoning. Extensive experiments\ndemonstrate the effectiveness of interleaved vision-text-action learning for\nopen-world understanding and generalization, validated through a variety of\nlong-horizon, dexterous manipulation tasks across multiple embodiments. This\npaper details the architecture of EO-1, the data construction strategy of\nEO-Data1.5M, and the training methodology, offering valuable insights for\ndeveloping advanced embodied foundation models.\n", "link": "http://arxiv.org/abs/2508.21112v3", "date": "2025-09-09", "relevancy": 2.4624, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6177}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmbodiedOneVision%3A%20Interleaved%20Vision-Text-Action%20Pretraining%20for%0A%20%20General%20Robot%20Control&body=Title%3A%20EmbodiedOneVision%3A%20Interleaved%20Vision-Text-Action%20Pretraining%20for%0A%20%20General%20Robot%20Control%0AAuthor%3A%20Delin%20Qu%20and%20Haoming%20Song%20and%20Qizhi%20Chen%20and%20Zhaoqing%20Chen%20and%20Xianqiang%20Gao%20and%20Xinyi%20Ye%20and%20Qi%20Lv%20and%20Modi%20Shi%20and%20Guanghui%20Ren%20and%20Cheng%20Ruan%20and%20Maoqing%20Yao%20and%20Haoran%20Yang%20and%20Jiacheng%20Bao%20and%20Bin%20Zhao%20and%20Dong%20Wang%0AAbstract%3A%20%20%20The%20human%20ability%20to%20seamlessly%20perform%20multimodal%20reasoning%20and%20physical%0Ainteraction%20in%20the%20open%20world%20is%20a%20core%20goal%20for%20general-purpose%20embodied%0Aintelligent%20systems.%20Recent%20vision-language-action%20%28VLA%29%20models%2C%20which%20are%0Aco-trained%20on%20large-scale%20robot%20and%20visual-text%20data%2C%20have%20demonstrated%20notable%0Aprogress%20in%20general%20robot%20control.%20However%2C%20they%20still%20fail%20to%20achieve%0Ahuman-level%20flexibility%20in%20interleaved%20reasoning%20and%20interaction.%20In%20this%20work%2C%0Aintroduce%20EO-Robotics%2C%20consists%20of%20EO-1%20model%20and%20EO-Data1.5M%20dataset.%20EO-1%20is%0Aa%20unified%20embodied%20foundation%20model%20that%20achieves%20superior%20performance%20in%0Amultimodal%20embodied%20reasoning%20and%20robot%20control%20through%20interleaved%0Avision-text-action%20pre-training.%20The%20development%20of%20EO-1%20is%20based%20on%20two%20key%0Apillars%3A%20%28i%29%20a%20unified%20architecture%20that%20processes%20multimodal%20inputs%0Aindiscriminately%20%28image%2C%20text%2C%20video%2C%20and%20action%29%2C%20and%20%28ii%29%20a%20massive%2C%0Ahigh-quality%20multimodal%20embodied%20reasoning%20dataset%2C%20EO-Data1.5M%2C%20which%20contains%0Aover%201.5%20million%20samples%20with%20emphasis%20on%20interleaved%20vision-text-action%0Acomprehension.%20EO-1%20is%20trained%20through%20synergies%20between%20auto-regressive%0Adecoding%20and%20flow%20matching%20denoising%20on%20EO-Data1.5M%2C%20enabling%20seamless%20robot%0Aaction%20generation%20and%20multimodal%20embodied%20reasoning.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20interleaved%20vision-text-action%20learning%20for%0Aopen-world%20understanding%20and%20generalization%2C%20validated%20through%20a%20variety%20of%0Along-horizon%2C%20dexterous%20manipulation%20tasks%20across%20multiple%20embodiments.%20This%0Apaper%20details%20the%20architecture%20of%20EO-1%2C%20the%20data%20construction%20strategy%20of%0AEO-Data1.5M%2C%20and%20the%20training%20methodology%2C%20offering%20valuable%20insights%20for%0Adeveloping%20advanced%20embodied%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21112v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodiedOneVision%253A%2520Interleaved%2520Vision-Text-Action%2520Pretraining%2520for%250A%2520%2520General%2520Robot%2520Control%26entry.906535625%3DDelin%2520Qu%2520and%2520Haoming%2520Song%2520and%2520Qizhi%2520Chen%2520and%2520Zhaoqing%2520Chen%2520and%2520Xianqiang%2520Gao%2520and%2520Xinyi%2520Ye%2520and%2520Qi%2520Lv%2520and%2520Modi%2520Shi%2520and%2520Guanghui%2520Ren%2520and%2520Cheng%2520Ruan%2520and%2520Maoqing%2520Yao%2520and%2520Haoran%2520Yang%2520and%2520Jiacheng%2520Bao%2520and%2520Bin%2520Zhao%2520and%2520Dong%2520Wang%26entry.1292438233%3D%2520%2520The%2520human%2520ability%2520to%2520seamlessly%2520perform%2520multimodal%2520reasoning%2520and%2520physical%250Ainteraction%2520in%2520the%2520open%2520world%2520is%2520a%2520core%2520goal%2520for%2520general-purpose%2520embodied%250Aintelligent%2520systems.%2520Recent%2520vision-language-action%2520%2528VLA%2529%2520models%252C%2520which%2520are%250Aco-trained%2520on%2520large-scale%2520robot%2520and%2520visual-text%2520data%252C%2520have%2520demonstrated%2520notable%250Aprogress%2520in%2520general%2520robot%2520control.%2520However%252C%2520they%2520still%2520fail%2520to%2520achieve%250Ahuman-level%2520flexibility%2520in%2520interleaved%2520reasoning%2520and%2520interaction.%2520In%2520this%2520work%252C%250Aintroduce%2520EO-Robotics%252C%2520consists%2520of%2520EO-1%2520model%2520and%2520EO-Data1.5M%2520dataset.%2520EO-1%2520is%250Aa%2520unified%2520embodied%2520foundation%2520model%2520that%2520achieves%2520superior%2520performance%2520in%250Amultimodal%2520embodied%2520reasoning%2520and%2520robot%2520control%2520through%2520interleaved%250Avision-text-action%2520pre-training.%2520The%2520development%2520of%2520EO-1%2520is%2520based%2520on%2520two%2520key%250Apillars%253A%2520%2528i%2529%2520a%2520unified%2520architecture%2520that%2520processes%2520multimodal%2520inputs%250Aindiscriminately%2520%2528image%252C%2520text%252C%2520video%252C%2520and%2520action%2529%252C%2520and%2520%2528ii%2529%2520a%2520massive%252C%250Ahigh-quality%2520multimodal%2520embodied%2520reasoning%2520dataset%252C%2520EO-Data1.5M%252C%2520which%2520contains%250Aover%25201.5%2520million%2520samples%2520with%2520emphasis%2520on%2520interleaved%2520vision-text-action%250Acomprehension.%2520EO-1%2520is%2520trained%2520through%2520synergies%2520between%2520auto-regressive%250Adecoding%2520and%2520flow%2520matching%2520denoising%2520on%2520EO-Data1.5M%252C%2520enabling%2520seamless%2520robot%250Aaction%2520generation%2520and%2520multimodal%2520embodied%2520reasoning.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520interleaved%2520vision-text-action%2520learning%2520for%250Aopen-world%2520understanding%2520and%2520generalization%252C%2520validated%2520through%2520a%2520variety%2520of%250Along-horizon%252C%2520dexterous%2520manipulation%2520tasks%2520across%2520multiple%2520embodiments.%2520This%250Apaper%2520details%2520the%2520architecture%2520of%2520EO-1%252C%2520the%2520data%2520construction%2520strategy%2520of%250AEO-Data1.5M%252C%2520and%2520the%2520training%2520methodology%252C%2520offering%2520valuable%2520insights%2520for%250Adeveloping%2520advanced%2520embodied%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21112v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmbodiedOneVision%3A%20Interleaved%20Vision-Text-Action%20Pretraining%20for%0A%20%20General%20Robot%20Control&entry.906535625=Delin%20Qu%20and%20Haoming%20Song%20and%20Qizhi%20Chen%20and%20Zhaoqing%20Chen%20and%20Xianqiang%20Gao%20and%20Xinyi%20Ye%20and%20Qi%20Lv%20and%20Modi%20Shi%20and%20Guanghui%20Ren%20and%20Cheng%20Ruan%20and%20Maoqing%20Yao%20and%20Haoran%20Yang%20and%20Jiacheng%20Bao%20and%20Bin%20Zhao%20and%20Dong%20Wang&entry.1292438233=%20%20The%20human%20ability%20to%20seamlessly%20perform%20multimodal%20reasoning%20and%20physical%0Ainteraction%20in%20the%20open%20world%20is%20a%20core%20goal%20for%20general-purpose%20embodied%0Aintelligent%20systems.%20Recent%20vision-language-action%20%28VLA%29%20models%2C%20which%20are%0Aco-trained%20on%20large-scale%20robot%20and%20visual-text%20data%2C%20have%20demonstrated%20notable%0Aprogress%20in%20general%20robot%20control.%20However%2C%20they%20still%20fail%20to%20achieve%0Ahuman-level%20flexibility%20in%20interleaved%20reasoning%20and%20interaction.%20In%20this%20work%2C%0Aintroduce%20EO-Robotics%2C%20consists%20of%20EO-1%20model%20and%20EO-Data1.5M%20dataset.%20EO-1%20is%0Aa%20unified%20embodied%20foundation%20model%20that%20achieves%20superior%20performance%20in%0Amultimodal%20embodied%20reasoning%20and%20robot%20control%20through%20interleaved%0Avision-text-action%20pre-training.%20The%20development%20of%20EO-1%20is%20based%20on%20two%20key%0Apillars%3A%20%28i%29%20a%20unified%20architecture%20that%20processes%20multimodal%20inputs%0Aindiscriminately%20%28image%2C%20text%2C%20video%2C%20and%20action%29%2C%20and%20%28ii%29%20a%20massive%2C%0Ahigh-quality%20multimodal%20embodied%20reasoning%20dataset%2C%20EO-Data1.5M%2C%20which%20contains%0Aover%201.5%20million%20samples%20with%20emphasis%20on%20interleaved%20vision-text-action%0Acomprehension.%20EO-1%20is%20trained%20through%20synergies%20between%20auto-regressive%0Adecoding%20and%20flow%20matching%20denoising%20on%20EO-Data1.5M%2C%20enabling%20seamless%20robot%0Aaction%20generation%20and%20multimodal%20embodied%20reasoning.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20interleaved%20vision-text-action%20learning%20for%0Aopen-world%20understanding%20and%20generalization%2C%20validated%20through%20a%20variety%20of%0Along-horizon%2C%20dexterous%20manipulation%20tasks%20across%20multiple%20embodiments.%20This%0Apaper%20details%20the%20architecture%20of%20EO-1%2C%20the%20data%20construction%20strategy%20of%0AEO-Data1.5M%2C%20and%20the%20training%20methodology%2C%20offering%20valuable%20insights%20for%0Adeveloping%20advanced%20embodied%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21112v3&entry.124074799=Read"},
{"title": "FUnc-SNE: A flexible, Fast, and Unconstrained algorithm for neighbour\n  embeddings", "author": "Pierre Lambert and Edouard Couplet and Michel Verleysen and John Aldo Lee", "abstract": "  Neighbour embeddings (NE) allow the representation of high dimensional\ndatasets into lower dimensional spaces and are often used in data\nvisualisation. In practice, accelerated approximations are employed to handle\nvery large datasets. Accelerating NE is challenging, and two main directions\nhave been explored: very coarse approximations based on negative sampling (as\nin UMAP) achieve high effective speed but may lack quality in the extracted\nstructures; less coarse approximations, as used in FIt-SNE or BH-t-SNE, offer\nbetter structure preservation at the cost of speed, while also restricting the\ntarget dimensionality to 2 or 3, limiting NE to visualisation. In some\nvariants, the precision of these costlier accelerations also enables\nfiner-grained control on the extracted structures through dedicated\nhyperparameters.\n  This paper proposes to bridge the gab between both approaches by introducing\na novel way to accelerate NE, requiring a small number of computations per\niteration while maintaining good fine-grained structure preservation and\nflexibility through hyperparameter tuning, without limiting the dimensionality\nof the embedding space. The method was designed for interactive exploration of\ndata; as such, it abandons the traditional two-phased approach of other NE\nmethods, allowing instantaneous visual feedback when changing hyperparameters,\neven when these control processes happening on the high-dimensional side of the\ncomputations. Experiments using a publicly available, GPU accelerated GUI\nintegration of the method show promising results in terms of speed, flexibility\nin the structures getting extracted, and show potential uses in broader machine\nlearning contexts with minimal algorithmic modifications. Central to this\nalgorithm is a novel approach to iterative approximate nearest neighbour\nsearch, which shows promising results compared to nearest neighbour descent.\n", "link": "http://arxiv.org/abs/2509.07681v1", "date": "2025-09-09", "relevancy": 2.4553, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4959}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4919}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FUnc-SNE%3A%20A%20flexible%2C%20Fast%2C%20and%20Unconstrained%20algorithm%20for%20neighbour%0A%20%20embeddings&body=Title%3A%20FUnc-SNE%3A%20A%20flexible%2C%20Fast%2C%20and%20Unconstrained%20algorithm%20for%20neighbour%0A%20%20embeddings%0AAuthor%3A%20Pierre%20Lambert%20and%20Edouard%20Couplet%20and%20Michel%20Verleysen%20and%20John%20Aldo%20Lee%0AAbstract%3A%20%20%20Neighbour%20embeddings%20%28NE%29%20allow%20the%20representation%20of%20high%20dimensional%0Adatasets%20into%20lower%20dimensional%20spaces%20and%20are%20often%20used%20in%20data%0Avisualisation.%20In%20practice%2C%20accelerated%20approximations%20are%20employed%20to%20handle%0Avery%20large%20datasets.%20Accelerating%20NE%20is%20challenging%2C%20and%20two%20main%20directions%0Ahave%20been%20explored%3A%20very%20coarse%20approximations%20based%20on%20negative%20sampling%20%28as%0Ain%20UMAP%29%20achieve%20high%20effective%20speed%20but%20may%20lack%20quality%20in%20the%20extracted%0Astructures%3B%20less%20coarse%20approximations%2C%20as%20used%20in%20FIt-SNE%20or%20BH-t-SNE%2C%20offer%0Abetter%20structure%20preservation%20at%20the%20cost%20of%20speed%2C%20while%20also%20restricting%20the%0Atarget%20dimensionality%20to%202%20or%203%2C%20limiting%20NE%20to%20visualisation.%20In%20some%0Avariants%2C%20the%20precision%20of%20these%20costlier%20accelerations%20also%20enables%0Afiner-grained%20control%20on%20the%20extracted%20structures%20through%20dedicated%0Ahyperparameters.%0A%20%20This%20paper%20proposes%20to%20bridge%20the%20gab%20between%20both%20approaches%20by%20introducing%0Aa%20novel%20way%20to%20accelerate%20NE%2C%20requiring%20a%20small%20number%20of%20computations%20per%0Aiteration%20while%20maintaining%20good%20fine-grained%20structure%20preservation%20and%0Aflexibility%20through%20hyperparameter%20tuning%2C%20without%20limiting%20the%20dimensionality%0Aof%20the%20embedding%20space.%20The%20method%20was%20designed%20for%20interactive%20exploration%20of%0Adata%3B%20as%20such%2C%20it%20abandons%20the%20traditional%20two-phased%20approach%20of%20other%20NE%0Amethods%2C%20allowing%20instantaneous%20visual%20feedback%20when%20changing%20hyperparameters%2C%0Aeven%20when%20these%20control%20processes%20happening%20on%20the%20high-dimensional%20side%20of%20the%0Acomputations.%20Experiments%20using%20a%20publicly%20available%2C%20GPU%20accelerated%20GUI%0Aintegration%20of%20the%20method%20show%20promising%20results%20in%20terms%20of%20speed%2C%20flexibility%0Ain%20the%20structures%20getting%20extracted%2C%20and%20show%20potential%20uses%20in%20broader%20machine%0Alearning%20contexts%20with%20minimal%20algorithmic%20modifications.%20Central%20to%20this%0Aalgorithm%20is%20a%20novel%20approach%20to%20iterative%20approximate%20nearest%20neighbour%0Asearch%2C%20which%20shows%20promising%20results%20compared%20to%20nearest%20neighbour%20descent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFUnc-SNE%253A%2520A%2520flexible%252C%2520Fast%252C%2520and%2520Unconstrained%2520algorithm%2520for%2520neighbour%250A%2520%2520embeddings%26entry.906535625%3DPierre%2520Lambert%2520and%2520Edouard%2520Couplet%2520and%2520Michel%2520Verleysen%2520and%2520John%2520Aldo%2520Lee%26entry.1292438233%3D%2520%2520Neighbour%2520embeddings%2520%2528NE%2529%2520allow%2520the%2520representation%2520of%2520high%2520dimensional%250Adatasets%2520into%2520lower%2520dimensional%2520spaces%2520and%2520are%2520often%2520used%2520in%2520data%250Avisualisation.%2520In%2520practice%252C%2520accelerated%2520approximations%2520are%2520employed%2520to%2520handle%250Avery%2520large%2520datasets.%2520Accelerating%2520NE%2520is%2520challenging%252C%2520and%2520two%2520main%2520directions%250Ahave%2520been%2520explored%253A%2520very%2520coarse%2520approximations%2520based%2520on%2520negative%2520sampling%2520%2528as%250Ain%2520UMAP%2529%2520achieve%2520high%2520effective%2520speed%2520but%2520may%2520lack%2520quality%2520in%2520the%2520extracted%250Astructures%253B%2520less%2520coarse%2520approximations%252C%2520as%2520used%2520in%2520FIt-SNE%2520or%2520BH-t-SNE%252C%2520offer%250Abetter%2520structure%2520preservation%2520at%2520the%2520cost%2520of%2520speed%252C%2520while%2520also%2520restricting%2520the%250Atarget%2520dimensionality%2520to%25202%2520or%25203%252C%2520limiting%2520NE%2520to%2520visualisation.%2520In%2520some%250Avariants%252C%2520the%2520precision%2520of%2520these%2520costlier%2520accelerations%2520also%2520enables%250Afiner-grained%2520control%2520on%2520the%2520extracted%2520structures%2520through%2520dedicated%250Ahyperparameters.%250A%2520%2520This%2520paper%2520proposes%2520to%2520bridge%2520the%2520gab%2520between%2520both%2520approaches%2520by%2520introducing%250Aa%2520novel%2520way%2520to%2520accelerate%2520NE%252C%2520requiring%2520a%2520small%2520number%2520of%2520computations%2520per%250Aiteration%2520while%2520maintaining%2520good%2520fine-grained%2520structure%2520preservation%2520and%250Aflexibility%2520through%2520hyperparameter%2520tuning%252C%2520without%2520limiting%2520the%2520dimensionality%250Aof%2520the%2520embedding%2520space.%2520The%2520method%2520was%2520designed%2520for%2520interactive%2520exploration%2520of%250Adata%253B%2520as%2520such%252C%2520it%2520abandons%2520the%2520traditional%2520two-phased%2520approach%2520of%2520other%2520NE%250Amethods%252C%2520allowing%2520instantaneous%2520visual%2520feedback%2520when%2520changing%2520hyperparameters%252C%250Aeven%2520when%2520these%2520control%2520processes%2520happening%2520on%2520the%2520high-dimensional%2520side%2520of%2520the%250Acomputations.%2520Experiments%2520using%2520a%2520publicly%2520available%252C%2520GPU%2520accelerated%2520GUI%250Aintegration%2520of%2520the%2520method%2520show%2520promising%2520results%2520in%2520terms%2520of%2520speed%252C%2520flexibility%250Ain%2520the%2520structures%2520getting%2520extracted%252C%2520and%2520show%2520potential%2520uses%2520in%2520broader%2520machine%250Alearning%2520contexts%2520with%2520minimal%2520algorithmic%2520modifications.%2520Central%2520to%2520this%250Aalgorithm%2520is%2520a%2520novel%2520approach%2520to%2520iterative%2520approximate%2520nearest%2520neighbour%250Asearch%252C%2520which%2520shows%2520promising%2520results%2520compared%2520to%2520nearest%2520neighbour%2520descent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FUnc-SNE%3A%20A%20flexible%2C%20Fast%2C%20and%20Unconstrained%20algorithm%20for%20neighbour%0A%20%20embeddings&entry.906535625=Pierre%20Lambert%20and%20Edouard%20Couplet%20and%20Michel%20Verleysen%20and%20John%20Aldo%20Lee&entry.1292438233=%20%20Neighbour%20embeddings%20%28NE%29%20allow%20the%20representation%20of%20high%20dimensional%0Adatasets%20into%20lower%20dimensional%20spaces%20and%20are%20often%20used%20in%20data%0Avisualisation.%20In%20practice%2C%20accelerated%20approximations%20are%20employed%20to%20handle%0Avery%20large%20datasets.%20Accelerating%20NE%20is%20challenging%2C%20and%20two%20main%20directions%0Ahave%20been%20explored%3A%20very%20coarse%20approximations%20based%20on%20negative%20sampling%20%28as%0Ain%20UMAP%29%20achieve%20high%20effective%20speed%20but%20may%20lack%20quality%20in%20the%20extracted%0Astructures%3B%20less%20coarse%20approximations%2C%20as%20used%20in%20FIt-SNE%20or%20BH-t-SNE%2C%20offer%0Abetter%20structure%20preservation%20at%20the%20cost%20of%20speed%2C%20while%20also%20restricting%20the%0Atarget%20dimensionality%20to%202%20or%203%2C%20limiting%20NE%20to%20visualisation.%20In%20some%0Avariants%2C%20the%20precision%20of%20these%20costlier%20accelerations%20also%20enables%0Afiner-grained%20control%20on%20the%20extracted%20structures%20through%20dedicated%0Ahyperparameters.%0A%20%20This%20paper%20proposes%20to%20bridge%20the%20gab%20between%20both%20approaches%20by%20introducing%0Aa%20novel%20way%20to%20accelerate%20NE%2C%20requiring%20a%20small%20number%20of%20computations%20per%0Aiteration%20while%20maintaining%20good%20fine-grained%20structure%20preservation%20and%0Aflexibility%20through%20hyperparameter%20tuning%2C%20without%20limiting%20the%20dimensionality%0Aof%20the%20embedding%20space.%20The%20method%20was%20designed%20for%20interactive%20exploration%20of%0Adata%3B%20as%20such%2C%20it%20abandons%20the%20traditional%20two-phased%20approach%20of%20other%20NE%0Amethods%2C%20allowing%20instantaneous%20visual%20feedback%20when%20changing%20hyperparameters%2C%0Aeven%20when%20these%20control%20processes%20happening%20on%20the%20high-dimensional%20side%20of%20the%0Acomputations.%20Experiments%20using%20a%20publicly%20available%2C%20GPU%20accelerated%20GUI%0Aintegration%20of%20the%20method%20show%20promising%20results%20in%20terms%20of%20speed%2C%20flexibility%0Ain%20the%20structures%20getting%20extracted%2C%20and%20show%20potential%20uses%20in%20broader%20machine%0Alearning%20contexts%20with%20minimal%20algorithmic%20modifications.%20Central%20to%20this%0Aalgorithm%20is%20a%20novel%20approach%20to%20iterative%20approximate%20nearest%20neighbour%0Asearch%2C%20which%20shows%20promising%20results%20compared%20to%20nearest%20neighbour%20descent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07681v1&entry.124074799=Read"},
{"title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories\n  in LLMs", "author": "Yao Yan", "abstract": "  Multi-digit addition is a clear probe of the computational power of large\nlanguage models. To dissect the internal arithmetic processes in\nLLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.\nInspired by the step-by-step manner in which humans perform addition, we\npropose and analyze a coherent four-stage trajectory in the forward\npass:Formula-structure representations become linearly decodable first, while\nthe answer token is still far down the candidate list.Core computational\nfeatures then emerge prominently.At deeper activation layers, numerical\nabstractions of the result become clearer, enabling near-perfect detection and\ndecoding of the individual digits in the sum.Near the output, the model\norganizes and generates the final content, with the correct token reliably\noccupying the top rank.This trajectory suggests a hierarchical process that\nfavors internal computation over rote memorization. We release our code and\ndata to facilitate reproducibility.\n", "link": "http://arxiv.org/abs/2506.07824v2", "date": "2025-09-09", "relevancy": 2.4474, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.484}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addition%20in%20Four%20Movements%3A%20Mapping%20Layer-wise%20Information%20Trajectories%0A%20%20in%20LLMs&body=Title%3A%20Addition%20in%20Four%20Movements%3A%20Mapping%20Layer-wise%20Information%20Trajectories%0A%20%20in%20LLMs%0AAuthor%3A%20Yao%20Yan%0AAbstract%3A%20%20%20Multi-digit%20addition%20is%20a%20clear%20probe%20of%20the%20computational%20power%20of%20large%0Alanguage%20models.%20To%20dissect%20the%20internal%20arithmetic%20processes%20in%0ALLaMA-3-8B-Instruct%2C%20we%20combine%20linear%20probing%20with%20logit-lens%20inspection.%0AInspired%20by%20the%20step-by-step%20manner%20in%20which%20humans%20perform%20addition%2C%20we%0Apropose%20and%20analyze%20a%20coherent%20four-stage%20trajectory%20in%20the%20forward%0Apass%3AFormula-structure%20representations%20become%20linearly%20decodable%20first%2C%20while%0Athe%20answer%20token%20is%20still%20far%20down%20the%20candidate%20list.Core%20computational%0Afeatures%20then%20emerge%20prominently.At%20deeper%20activation%20layers%2C%20numerical%0Aabstractions%20of%20the%20result%20become%20clearer%2C%20enabling%20near-perfect%20detection%20and%0Adecoding%20of%20the%20individual%20digits%20in%20the%20sum.Near%20the%20output%2C%20the%20model%0Aorganizes%20and%20generates%20the%20final%20content%2C%20with%20the%20correct%20token%20reliably%0Aoccupying%20the%20top%20rank.This%20trajectory%20suggests%20a%20hierarchical%20process%20that%0Afavors%20internal%20computation%20over%20rote%20memorization.%20We%20release%20our%20code%20and%0Adata%20to%20facilitate%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07824v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddition%2520in%2520Four%2520Movements%253A%2520Mapping%2520Layer-wise%2520Information%2520Trajectories%250A%2520%2520in%2520LLMs%26entry.906535625%3DYao%2520Yan%26entry.1292438233%3D%2520%2520Multi-digit%2520addition%2520is%2520a%2520clear%2520probe%2520of%2520the%2520computational%2520power%2520of%2520large%250Alanguage%2520models.%2520To%2520dissect%2520the%2520internal%2520arithmetic%2520processes%2520in%250ALLaMA-3-8B-Instruct%252C%2520we%2520combine%2520linear%2520probing%2520with%2520logit-lens%2520inspection.%250AInspired%2520by%2520the%2520step-by-step%2520manner%2520in%2520which%2520humans%2520perform%2520addition%252C%2520we%250Apropose%2520and%2520analyze%2520a%2520coherent%2520four-stage%2520trajectory%2520in%2520the%2520forward%250Apass%253AFormula-structure%2520representations%2520become%2520linearly%2520decodable%2520first%252C%2520while%250Athe%2520answer%2520token%2520is%2520still%2520far%2520down%2520the%2520candidate%2520list.Core%2520computational%250Afeatures%2520then%2520emerge%2520prominently.At%2520deeper%2520activation%2520layers%252C%2520numerical%250Aabstractions%2520of%2520the%2520result%2520become%2520clearer%252C%2520enabling%2520near-perfect%2520detection%2520and%250Adecoding%2520of%2520the%2520individual%2520digits%2520in%2520the%2520sum.Near%2520the%2520output%252C%2520the%2520model%250Aorganizes%2520and%2520generates%2520the%2520final%2520content%252C%2520with%2520the%2520correct%2520token%2520reliably%250Aoccupying%2520the%2520top%2520rank.This%2520trajectory%2520suggests%2520a%2520hierarchical%2520process%2520that%250Afavors%2520internal%2520computation%2520over%2520rote%2520memorization.%2520We%2520release%2520our%2520code%2520and%250Adata%2520to%2520facilitate%2520reproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07824v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addition%20in%20Four%20Movements%3A%20Mapping%20Layer-wise%20Information%20Trajectories%0A%20%20in%20LLMs&entry.906535625=Yao%20Yan&entry.1292438233=%20%20Multi-digit%20addition%20is%20a%20clear%20probe%20of%20the%20computational%20power%20of%20large%0Alanguage%20models.%20To%20dissect%20the%20internal%20arithmetic%20processes%20in%0ALLaMA-3-8B-Instruct%2C%20we%20combine%20linear%20probing%20with%20logit-lens%20inspection.%0AInspired%20by%20the%20step-by-step%20manner%20in%20which%20humans%20perform%20addition%2C%20we%0Apropose%20and%20analyze%20a%20coherent%20four-stage%20trajectory%20in%20the%20forward%0Apass%3AFormula-structure%20representations%20become%20linearly%20decodable%20first%2C%20while%0Athe%20answer%20token%20is%20still%20far%20down%20the%20candidate%20list.Core%20computational%0Afeatures%20then%20emerge%20prominently.At%20deeper%20activation%20layers%2C%20numerical%0Aabstractions%20of%20the%20result%20become%20clearer%2C%20enabling%20near-perfect%20detection%20and%0Adecoding%20of%20the%20individual%20digits%20in%20the%20sum.Near%20the%20output%2C%20the%20model%0Aorganizes%20and%20generates%20the%20final%20content%2C%20with%20the%20correct%20token%20reliably%0Aoccupying%20the%20top%20rank.This%20trajectory%20suggests%20a%20hierarchical%20process%20that%0Afavors%20internal%20computation%20over%20rote%20memorization.%20We%20release%20our%20code%20and%0Adata%20to%20facilitate%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07824v2&entry.124074799=Read"},
{"title": "Dynamic Scene 3D Reconstruction of an Uncooperative Resident Space\n  Object", "author": "Bala Prenith Reddy Gopu and Timothy Jacob Huber and George M. Nehma and Patrick Quinn and Madhur Tiwari and Matt Ueckermann and David Hinckley and Christopher McKenna", "abstract": "  Characterization of uncooperative Resident Space Objects (RSO) play a crucial\nrole in On-Orbit Servicing (OOS) and Active Debris Removal (ADR) missions to\nassess the geometry and motion properties. To address the challenges of\nreconstructing tumbling uncooperative targets, this study evaluates the\nperformance of existing state-of-the-art 3D reconstruction algorithms for\ndynamic scenes, focusing on their ability to generate geometrically accurate\nmodels with high-fidelity. To support our evaluation, we developed a simulation\nenvironment using Isaac Sim to generate physics-accurate 2D image sequences of\ntumbling satellite under realistic orbital lighting conditions. Our preliminary\nresults on static scenes using Neuralangelo demonstrate promising\nreconstruction quality. The generated 3D meshes closely match the original CAD\nmodels with minimal errors and artifacts when compared using Cloud Compare\n(CC). The reconstructed models were able to capture critical fine details for\nmission planning. This provides a baseline for our ongoing evaluation of\ndynamic scene reconstruction.\n", "link": "http://arxiv.org/abs/2509.07932v1", "date": "2025-09-09", "relevancy": 2.426, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6089}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6054}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Scene%203D%20Reconstruction%20of%20an%20Uncooperative%20Resident%20Space%0A%20%20Object&body=Title%3A%20Dynamic%20Scene%203D%20Reconstruction%20of%20an%20Uncooperative%20Resident%20Space%0A%20%20Object%0AAuthor%3A%20Bala%20Prenith%20Reddy%20Gopu%20and%20Timothy%20Jacob%20Huber%20and%20George%20M.%20Nehma%20and%20Patrick%20Quinn%20and%20Madhur%20Tiwari%20and%20Matt%20Ueckermann%20and%20David%20Hinckley%20and%20Christopher%20McKenna%0AAbstract%3A%20%20%20Characterization%20of%20uncooperative%20Resident%20Space%20Objects%20%28RSO%29%20play%20a%20crucial%0Arole%20in%20On-Orbit%20Servicing%20%28OOS%29%20and%20Active%20Debris%20Removal%20%28ADR%29%20missions%20to%0Aassess%20the%20geometry%20and%20motion%20properties.%20To%20address%20the%20challenges%20of%0Areconstructing%20tumbling%20uncooperative%20targets%2C%20this%20study%20evaluates%20the%0Aperformance%20of%20existing%20state-of-the-art%203D%20reconstruction%20algorithms%20for%0Adynamic%20scenes%2C%20focusing%20on%20their%20ability%20to%20generate%20geometrically%20accurate%0Amodels%20with%20high-fidelity.%20To%20support%20our%20evaluation%2C%20we%20developed%20a%20simulation%0Aenvironment%20using%20Isaac%20Sim%20to%20generate%20physics-accurate%202D%20image%20sequences%20of%0Atumbling%20satellite%20under%20realistic%20orbital%20lighting%20conditions.%20Our%20preliminary%0Aresults%20on%20static%20scenes%20using%20Neuralangelo%20demonstrate%20promising%0Areconstruction%20quality.%20The%20generated%203D%20meshes%20closely%20match%20the%20original%20CAD%0Amodels%20with%20minimal%20errors%20and%20artifacts%20when%20compared%20using%20Cloud%20Compare%0A%28CC%29.%20The%20reconstructed%20models%20were%20able%20to%20capture%20critical%20fine%20details%20for%0Amission%20planning.%20This%20provides%20a%20baseline%20for%20our%20ongoing%20evaluation%20of%0Adynamic%20scene%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Scene%25203D%2520Reconstruction%2520of%2520an%2520Uncooperative%2520Resident%2520Space%250A%2520%2520Object%26entry.906535625%3DBala%2520Prenith%2520Reddy%2520Gopu%2520and%2520Timothy%2520Jacob%2520Huber%2520and%2520George%2520M.%2520Nehma%2520and%2520Patrick%2520Quinn%2520and%2520Madhur%2520Tiwari%2520and%2520Matt%2520Ueckermann%2520and%2520David%2520Hinckley%2520and%2520Christopher%2520McKenna%26entry.1292438233%3D%2520%2520Characterization%2520of%2520uncooperative%2520Resident%2520Space%2520Objects%2520%2528RSO%2529%2520play%2520a%2520crucial%250Arole%2520in%2520On-Orbit%2520Servicing%2520%2528OOS%2529%2520and%2520Active%2520Debris%2520Removal%2520%2528ADR%2529%2520missions%2520to%250Aassess%2520the%2520geometry%2520and%2520motion%2520properties.%2520To%2520address%2520the%2520challenges%2520of%250Areconstructing%2520tumbling%2520uncooperative%2520targets%252C%2520this%2520study%2520evaluates%2520the%250Aperformance%2520of%2520existing%2520state-of-the-art%25203D%2520reconstruction%2520algorithms%2520for%250Adynamic%2520scenes%252C%2520focusing%2520on%2520their%2520ability%2520to%2520generate%2520geometrically%2520accurate%250Amodels%2520with%2520high-fidelity.%2520To%2520support%2520our%2520evaluation%252C%2520we%2520developed%2520a%2520simulation%250Aenvironment%2520using%2520Isaac%2520Sim%2520to%2520generate%2520physics-accurate%25202D%2520image%2520sequences%2520of%250Atumbling%2520satellite%2520under%2520realistic%2520orbital%2520lighting%2520conditions.%2520Our%2520preliminary%250Aresults%2520on%2520static%2520scenes%2520using%2520Neuralangelo%2520demonstrate%2520promising%250Areconstruction%2520quality.%2520The%2520generated%25203D%2520meshes%2520closely%2520match%2520the%2520original%2520CAD%250Amodels%2520with%2520minimal%2520errors%2520and%2520artifacts%2520when%2520compared%2520using%2520Cloud%2520Compare%250A%2528CC%2529.%2520The%2520reconstructed%2520models%2520were%2520able%2520to%2520capture%2520critical%2520fine%2520details%2520for%250Amission%2520planning.%2520This%2520provides%2520a%2520baseline%2520for%2520our%2520ongoing%2520evaluation%2520of%250Adynamic%2520scene%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Scene%203D%20Reconstruction%20of%20an%20Uncooperative%20Resident%20Space%0A%20%20Object&entry.906535625=Bala%20Prenith%20Reddy%20Gopu%20and%20Timothy%20Jacob%20Huber%20and%20George%20M.%20Nehma%20and%20Patrick%20Quinn%20and%20Madhur%20Tiwari%20and%20Matt%20Ueckermann%20and%20David%20Hinckley%20and%20Christopher%20McKenna&entry.1292438233=%20%20Characterization%20of%20uncooperative%20Resident%20Space%20Objects%20%28RSO%29%20play%20a%20crucial%0Arole%20in%20On-Orbit%20Servicing%20%28OOS%29%20and%20Active%20Debris%20Removal%20%28ADR%29%20missions%20to%0Aassess%20the%20geometry%20and%20motion%20properties.%20To%20address%20the%20challenges%20of%0Areconstructing%20tumbling%20uncooperative%20targets%2C%20this%20study%20evaluates%20the%0Aperformance%20of%20existing%20state-of-the-art%203D%20reconstruction%20algorithms%20for%0Adynamic%20scenes%2C%20focusing%20on%20their%20ability%20to%20generate%20geometrically%20accurate%0Amodels%20with%20high-fidelity.%20To%20support%20our%20evaluation%2C%20we%20developed%20a%20simulation%0Aenvironment%20using%20Isaac%20Sim%20to%20generate%20physics-accurate%202D%20image%20sequences%20of%0Atumbling%20satellite%20under%20realistic%20orbital%20lighting%20conditions.%20Our%20preliminary%0Aresults%20on%20static%20scenes%20using%20Neuralangelo%20demonstrate%20promising%0Areconstruction%20quality.%20The%20generated%203D%20meshes%20closely%20match%20the%20original%20CAD%0Amodels%20with%20minimal%20errors%20and%20artifacts%20when%20compared%20using%20Cloud%20Compare%0A%28CC%29.%20The%20reconstructed%20models%20were%20able%20to%20capture%20critical%20fine%20details%20for%0Amission%20planning.%20This%20provides%20a%20baseline%20for%20our%20ongoing%20evaluation%20of%0Adynamic%20scene%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07932v1&entry.124074799=Read"},
{"title": "ScoreHOI: Physically Plausible Reconstruction of Human-Object\n  Interaction via Score-Guided Diffusion", "author": "Ao Li and Jinpeng Liu and Yixuan Zhu and Yansong Tang", "abstract": "  Joint reconstruction of human-object interaction marks a significant\nmilestone in comprehending the intricate interrelations between humans and\ntheir surrounding environment. Nevertheless, previous optimization methods\noften struggle to achieve physically plausible reconstruction results due to\nthe lack of prior knowledge about human-object interactions. In this paper, we\nintroduce ScoreHOI, an effective diffusion-based optimizer that introduces\ndiffusion priors for the precise recovery of human-object interactions. By\nharnessing the controllability within score-guided sampling, the diffusion\nmodel can reconstruct a conditional distribution of human and object pose given\nthe image observation and object feature. During inference, the ScoreHOI\neffectively improves the reconstruction results by guiding the denoising\nprocess with specific physical constraints. Furthermore, we propose a\ncontact-driven iterative refinement approach to enhance the contact\nplausibility and improve the reconstruction accuracy. Extensive evaluations on\nstandard benchmarks demonstrate ScoreHOI's superior performance over\nstate-of-the-art methods, highlighting its ability to achieve a precise and\nrobust improvement in joint human-object interaction reconstruction.\n", "link": "http://arxiv.org/abs/2509.07920v1", "date": "2025-09-09", "relevancy": 2.3844, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6267}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5887}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScoreHOI%3A%20Physically%20Plausible%20Reconstruction%20of%20Human-Object%0A%20%20Interaction%20via%20Score-Guided%20Diffusion&body=Title%3A%20ScoreHOI%3A%20Physically%20Plausible%20Reconstruction%20of%20Human-Object%0A%20%20Interaction%20via%20Score-Guided%20Diffusion%0AAuthor%3A%20Ao%20Li%20and%20Jinpeng%20Liu%20and%20Yixuan%20Zhu%20and%20Yansong%20Tang%0AAbstract%3A%20%20%20Joint%20reconstruction%20of%20human-object%20interaction%20marks%20a%20significant%0Amilestone%20in%20comprehending%20the%20intricate%20interrelations%20between%20humans%20and%0Atheir%20surrounding%20environment.%20Nevertheless%2C%20previous%20optimization%20methods%0Aoften%20struggle%20to%20achieve%20physically%20plausible%20reconstruction%20results%20due%20to%0Athe%20lack%20of%20prior%20knowledge%20about%20human-object%20interactions.%20In%20this%20paper%2C%20we%0Aintroduce%20ScoreHOI%2C%20an%20effective%20diffusion-based%20optimizer%20that%20introduces%0Adiffusion%20priors%20for%20the%20precise%20recovery%20of%20human-object%20interactions.%20By%0Aharnessing%20the%20controllability%20within%20score-guided%20sampling%2C%20the%20diffusion%0Amodel%20can%20reconstruct%20a%20conditional%20distribution%20of%20human%20and%20object%20pose%20given%0Athe%20image%20observation%20and%20object%20feature.%20During%20inference%2C%20the%20ScoreHOI%0Aeffectively%20improves%20the%20reconstruction%20results%20by%20guiding%20the%20denoising%0Aprocess%20with%20specific%20physical%20constraints.%20Furthermore%2C%20we%20propose%20a%0Acontact-driven%20iterative%20refinement%20approach%20to%20enhance%20the%20contact%0Aplausibility%20and%20improve%20the%20reconstruction%20accuracy.%20Extensive%20evaluations%20on%0Astandard%20benchmarks%20demonstrate%20ScoreHOI%27s%20superior%20performance%20over%0Astate-of-the-art%20methods%2C%20highlighting%20its%20ability%20to%20achieve%20a%20precise%20and%0Arobust%20improvement%20in%20joint%20human-object%20interaction%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScoreHOI%253A%2520Physically%2520Plausible%2520Reconstruction%2520of%2520Human-Object%250A%2520%2520Interaction%2520via%2520Score-Guided%2520Diffusion%26entry.906535625%3DAo%2520Li%2520and%2520Jinpeng%2520Liu%2520and%2520Yixuan%2520Zhu%2520and%2520Yansong%2520Tang%26entry.1292438233%3D%2520%2520Joint%2520reconstruction%2520of%2520human-object%2520interaction%2520marks%2520a%2520significant%250Amilestone%2520in%2520comprehending%2520the%2520intricate%2520interrelations%2520between%2520humans%2520and%250Atheir%2520surrounding%2520environment.%2520Nevertheless%252C%2520previous%2520optimization%2520methods%250Aoften%2520struggle%2520to%2520achieve%2520physically%2520plausible%2520reconstruction%2520results%2520due%2520to%250Athe%2520lack%2520of%2520prior%2520knowledge%2520about%2520human-object%2520interactions.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520ScoreHOI%252C%2520an%2520effective%2520diffusion-based%2520optimizer%2520that%2520introduces%250Adiffusion%2520priors%2520for%2520the%2520precise%2520recovery%2520of%2520human-object%2520interactions.%2520By%250Aharnessing%2520the%2520controllability%2520within%2520score-guided%2520sampling%252C%2520the%2520diffusion%250Amodel%2520can%2520reconstruct%2520a%2520conditional%2520distribution%2520of%2520human%2520and%2520object%2520pose%2520given%250Athe%2520image%2520observation%2520and%2520object%2520feature.%2520During%2520inference%252C%2520the%2520ScoreHOI%250Aeffectively%2520improves%2520the%2520reconstruction%2520results%2520by%2520guiding%2520the%2520denoising%250Aprocess%2520with%2520specific%2520physical%2520constraints.%2520Furthermore%252C%2520we%2520propose%2520a%250Acontact-driven%2520iterative%2520refinement%2520approach%2520to%2520enhance%2520the%2520contact%250Aplausibility%2520and%2520improve%2520the%2520reconstruction%2520accuracy.%2520Extensive%2520evaluations%2520on%250Astandard%2520benchmarks%2520demonstrate%2520ScoreHOI%2527s%2520superior%2520performance%2520over%250Astate-of-the-art%2520methods%252C%2520highlighting%2520its%2520ability%2520to%2520achieve%2520a%2520precise%2520and%250Arobust%2520improvement%2520in%2520joint%2520human-object%2520interaction%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScoreHOI%3A%20Physically%20Plausible%20Reconstruction%20of%20Human-Object%0A%20%20Interaction%20via%20Score-Guided%20Diffusion&entry.906535625=Ao%20Li%20and%20Jinpeng%20Liu%20and%20Yixuan%20Zhu%20and%20Yansong%20Tang&entry.1292438233=%20%20Joint%20reconstruction%20of%20human-object%20interaction%20marks%20a%20significant%0Amilestone%20in%20comprehending%20the%20intricate%20interrelations%20between%20humans%20and%0Atheir%20surrounding%20environment.%20Nevertheless%2C%20previous%20optimization%20methods%0Aoften%20struggle%20to%20achieve%20physically%20plausible%20reconstruction%20results%20due%20to%0Athe%20lack%20of%20prior%20knowledge%20about%20human-object%20interactions.%20In%20this%20paper%2C%20we%0Aintroduce%20ScoreHOI%2C%20an%20effective%20diffusion-based%20optimizer%20that%20introduces%0Adiffusion%20priors%20for%20the%20precise%20recovery%20of%20human-object%20interactions.%20By%0Aharnessing%20the%20controllability%20within%20score-guided%20sampling%2C%20the%20diffusion%0Amodel%20can%20reconstruct%20a%20conditional%20distribution%20of%20human%20and%20object%20pose%20given%0Athe%20image%20observation%20and%20object%20feature.%20During%20inference%2C%20the%20ScoreHOI%0Aeffectively%20improves%20the%20reconstruction%20results%20by%20guiding%20the%20denoising%0Aprocess%20with%20specific%20physical%20constraints.%20Furthermore%2C%20we%20propose%20a%0Acontact-driven%20iterative%20refinement%20approach%20to%20enhance%20the%20contact%0Aplausibility%20and%20improve%20the%20reconstruction%20accuracy.%20Extensive%20evaluations%20on%0Astandard%20benchmarks%20demonstrate%20ScoreHOI%27s%20superior%20performance%20over%0Astate-of-the-art%20methods%2C%20highlighting%20its%20ability%20to%20achieve%20a%20precise%20and%0Arobust%20improvement%20in%20joint%20human-object%20interaction%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07920v1&entry.124074799=Read"},
{"title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual\n  Search", "author": "Xin Lai and Junyi Li and Wei Li and Tao Liu and Tianjian Li and Hengshuang Zhao", "abstract": "  Recent advances in large multimodal models have leveraged image-based tools\nwith reinforcement learning to tackle visual problems. However, existing\nopen-source approaches often exhibit monotonous reasoning patterns and allow\nonly a limited number of interaction turns, making them inadequate for\ndifficult tasks that require trial-and-error exploration. In this work, we\naddress this limitation by scaling up tool-based interactions and introduce\nMini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of\nsteps -- and achieves state-of-the-art performance on challenging visual search\ntasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key\ncomponents. First, we construct the Visual Probe Dataset, a collection of\nthousands of challenging visual search problems designed for exploratory\nreasoning. Second, we develop an iterative data collection pipeline to obtain\ncold-start trajectories that exhibit diverse reasoning patterns, including\ndepth-first search, trial-and-error, and goal maintenance. Third, we propose an\nover-turn masking strategy that prevents penalization of over-turn responses\n(those that hit the maximum number of turns) during reinforcement learning,\nthereby balancing training-time efficiency with test-time scalability. Despite\ntraining with an upper bound of only six interaction turns, our model generates\ntrajectories that naturally scale to tens of turns at inference time, with\naccuracy improving as the number of turns increases. Extensive experiments\ndemonstrate that Mini-o3 produces rich reasoning patterns and deep thinking\npaths, effectively solving challenging visual search problems.\n", "link": "http://arxiv.org/abs/2509.07969v1", "date": "2025-09-09", "relevancy": 2.3713, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5948}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5948}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mini-o3%3A%20Scaling%20Up%20Reasoning%20Patterns%20and%20Interaction%20Turns%20for%20Visual%0A%20%20Search&body=Title%3A%20Mini-o3%3A%20Scaling%20Up%20Reasoning%20Patterns%20and%20Interaction%20Turns%20for%20Visual%0A%20%20Search%0AAuthor%3A%20Xin%20Lai%20and%20Junyi%20Li%20and%20Wei%20Li%20and%20Tao%20Liu%20and%20Tianjian%20Li%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20multimodal%20models%20have%20leveraged%20image-based%20tools%0Awith%20reinforcement%20learning%20to%20tackle%20visual%20problems.%20However%2C%20existing%0Aopen-source%20approaches%20often%20exhibit%20monotonous%20reasoning%20patterns%20and%20allow%0Aonly%20a%20limited%20number%20of%20interaction%20turns%2C%20making%20them%20inadequate%20for%0Adifficult%20tasks%20that%20require%20trial-and-error%20exploration.%20In%20this%20work%2C%20we%0Aaddress%20this%20limitation%20by%20scaling%20up%20tool-based%20interactions%20and%20introduce%0AMini-o3%2C%20a%20system%20that%20executes%20deep%2C%20multi-turn%20reasoning%20--%20spanning%20tens%20of%0Asteps%20--%20and%20achieves%20state-of-the-art%20performance%20on%20challenging%20visual%20search%0Atasks.%20Our%20recipe%20for%20reproducing%20OpenAI%20o3-style%20behaviors%20comprises%20three%20key%0Acomponents.%20First%2C%20we%20construct%20the%20Visual%20Probe%20Dataset%2C%20a%20collection%20of%0Athousands%20of%20challenging%20visual%20search%20problems%20designed%20for%20exploratory%0Areasoning.%20Second%2C%20we%20develop%20an%20iterative%20data%20collection%20pipeline%20to%20obtain%0Acold-start%20trajectories%20that%20exhibit%20diverse%20reasoning%20patterns%2C%20including%0Adepth-first%20search%2C%20trial-and-error%2C%20and%20goal%20maintenance.%20Third%2C%20we%20propose%20an%0Aover-turn%20masking%20strategy%20that%20prevents%20penalization%20of%20over-turn%20responses%0A%28those%20that%20hit%20the%20maximum%20number%20of%20turns%29%20during%20reinforcement%20learning%2C%0Athereby%20balancing%20training-time%20efficiency%20with%20test-time%20scalability.%20Despite%0Atraining%20with%20an%20upper%20bound%20of%20only%20six%20interaction%20turns%2C%20our%20model%20generates%0Atrajectories%20that%20naturally%20scale%20to%20tens%20of%20turns%20at%20inference%20time%2C%20with%0Aaccuracy%20improving%20as%20the%20number%20of%20turns%20increases.%20Extensive%20experiments%0Ademonstrate%20that%20Mini-o3%20produces%20rich%20reasoning%20patterns%20and%20deep%20thinking%0Apaths%2C%20effectively%20solving%20challenging%20visual%20search%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMini-o3%253A%2520Scaling%2520Up%2520Reasoning%2520Patterns%2520and%2520Interaction%2520Turns%2520for%2520Visual%250A%2520%2520Search%26entry.906535625%3DXin%2520Lai%2520and%2520Junyi%2520Li%2520and%2520Wei%2520Li%2520and%2520Tao%2520Liu%2520and%2520Tianjian%2520Li%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520multimodal%2520models%2520have%2520leveraged%2520image-based%2520tools%250Awith%2520reinforcement%2520learning%2520to%2520tackle%2520visual%2520problems.%2520However%252C%2520existing%250Aopen-source%2520approaches%2520often%2520exhibit%2520monotonous%2520reasoning%2520patterns%2520and%2520allow%250Aonly%2520a%2520limited%2520number%2520of%2520interaction%2520turns%252C%2520making%2520them%2520inadequate%2520for%250Adifficult%2520tasks%2520that%2520require%2520trial-and-error%2520exploration.%2520In%2520this%2520work%252C%2520we%250Aaddress%2520this%2520limitation%2520by%2520scaling%2520up%2520tool-based%2520interactions%2520and%2520introduce%250AMini-o3%252C%2520a%2520system%2520that%2520executes%2520deep%252C%2520multi-turn%2520reasoning%2520--%2520spanning%2520tens%2520of%250Asteps%2520--%2520and%2520achieves%2520state-of-the-art%2520performance%2520on%2520challenging%2520visual%2520search%250Atasks.%2520Our%2520recipe%2520for%2520reproducing%2520OpenAI%2520o3-style%2520behaviors%2520comprises%2520three%2520key%250Acomponents.%2520First%252C%2520we%2520construct%2520the%2520Visual%2520Probe%2520Dataset%252C%2520a%2520collection%2520of%250Athousands%2520of%2520challenging%2520visual%2520search%2520problems%2520designed%2520for%2520exploratory%250Areasoning.%2520Second%252C%2520we%2520develop%2520an%2520iterative%2520data%2520collection%2520pipeline%2520to%2520obtain%250Acold-start%2520trajectories%2520that%2520exhibit%2520diverse%2520reasoning%2520patterns%252C%2520including%250Adepth-first%2520search%252C%2520trial-and-error%252C%2520and%2520goal%2520maintenance.%2520Third%252C%2520we%2520propose%2520an%250Aover-turn%2520masking%2520strategy%2520that%2520prevents%2520penalization%2520of%2520over-turn%2520responses%250A%2528those%2520that%2520hit%2520the%2520maximum%2520number%2520of%2520turns%2529%2520during%2520reinforcement%2520learning%252C%250Athereby%2520balancing%2520training-time%2520efficiency%2520with%2520test-time%2520scalability.%2520Despite%250Atraining%2520with%2520an%2520upper%2520bound%2520of%2520only%2520six%2520interaction%2520turns%252C%2520our%2520model%2520generates%250Atrajectories%2520that%2520naturally%2520scale%2520to%2520tens%2520of%2520turns%2520at%2520inference%2520time%252C%2520with%250Aaccuracy%2520improving%2520as%2520the%2520number%2520of%2520turns%2520increases.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520Mini-o3%2520produces%2520rich%2520reasoning%2520patterns%2520and%2520deep%2520thinking%250Apaths%252C%2520effectively%2520solving%2520challenging%2520visual%2520search%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mini-o3%3A%20Scaling%20Up%20Reasoning%20Patterns%20and%20Interaction%20Turns%20for%20Visual%0A%20%20Search&entry.906535625=Xin%20Lai%20and%20Junyi%20Li%20and%20Wei%20Li%20and%20Tao%20Liu%20and%20Tianjian%20Li%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20Recent%20advances%20in%20large%20multimodal%20models%20have%20leveraged%20image-based%20tools%0Awith%20reinforcement%20learning%20to%20tackle%20visual%20problems.%20However%2C%20existing%0Aopen-source%20approaches%20often%20exhibit%20monotonous%20reasoning%20patterns%20and%20allow%0Aonly%20a%20limited%20number%20of%20interaction%20turns%2C%20making%20them%20inadequate%20for%0Adifficult%20tasks%20that%20require%20trial-and-error%20exploration.%20In%20this%20work%2C%20we%0Aaddress%20this%20limitation%20by%20scaling%20up%20tool-based%20interactions%20and%20introduce%0AMini-o3%2C%20a%20system%20that%20executes%20deep%2C%20multi-turn%20reasoning%20--%20spanning%20tens%20of%0Asteps%20--%20and%20achieves%20state-of-the-art%20performance%20on%20challenging%20visual%20search%0Atasks.%20Our%20recipe%20for%20reproducing%20OpenAI%20o3-style%20behaviors%20comprises%20three%20key%0Acomponents.%20First%2C%20we%20construct%20the%20Visual%20Probe%20Dataset%2C%20a%20collection%20of%0Athousands%20of%20challenging%20visual%20search%20problems%20designed%20for%20exploratory%0Areasoning.%20Second%2C%20we%20develop%20an%20iterative%20data%20collection%20pipeline%20to%20obtain%0Acold-start%20trajectories%20that%20exhibit%20diverse%20reasoning%20patterns%2C%20including%0Adepth-first%20search%2C%20trial-and-error%2C%20and%20goal%20maintenance.%20Third%2C%20we%20propose%20an%0Aover-turn%20masking%20strategy%20that%20prevents%20penalization%20of%20over-turn%20responses%0A%28those%20that%20hit%20the%20maximum%20number%20of%20turns%29%20during%20reinforcement%20learning%2C%0Athereby%20balancing%20training-time%20efficiency%20with%20test-time%20scalability.%20Despite%0Atraining%20with%20an%20upper%20bound%20of%20only%20six%20interaction%20turns%2C%20our%20model%20generates%0Atrajectories%20that%20naturally%20scale%20to%20tens%20of%20turns%20at%20inference%20time%2C%20with%0Aaccuracy%20improving%20as%20the%20number%20of%20turns%20increases.%20Extensive%20experiments%0Ademonstrate%20that%20Mini-o3%20produces%20rich%20reasoning%20patterns%20and%20deep%20thinking%0Apaths%2C%20effectively%20solving%20challenging%20visual%20search%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07969v1&entry.124074799=Read"},
{"title": "EDFFDNet: Towards Accurate and Efficient Unsupervised Multi-Grid Image\n  Registration", "author": "Haokai Zhu and Bo Qu and Si-Yuan Cao and Runmin Zhang and Shujie Chen and Bailin Yang and Hui-Liang Shen", "abstract": "  Previous deep image registration methods that employ single homography,\nmulti-grid homography, or thin-plate spline often struggle with real scenes\ncontaining depth disparities due to their inherent limitations. To address\nthis, we propose an Exponential-Decay Free-Form Deformation Network (EDFFDNet),\nwhich employs free-form deformation with an exponential-decay basis function.\nThis design achieves higher efficiency and performs well in scenes with depth\ndisparities, benefiting from its inherent locality. We also introduce an\nAdaptive Sparse Motion Aggregator (ASMA), which replaces the MLP motion\naggregator used in previous methods. By transforming dense interactions into\nsparse ones, ASMA reduces parameters and improves accuracy. Additionally, we\npropose a progressive correlation refinement strategy that leverages\nglobal-local correlation patterns for coarse-to-fine motion estimation, further\nenhancing efficiency and accuracy. Experiments demonstrate that EDFFDNet\nreduces parameters, memory, and total runtime by 70.5%, 32.6%, and 33.7%,\nrespectively, while achieving a 0.5 dB PSNR gain over the state-of-the-art\nmethod. With an additional local refinement stage,EDFFDNet-2 further improves\nPSNR by 1.06 dB while maintaining lower computational costs. Our method also\ndemonstrates strong generalization ability across datasets, outperforming\nprevious deep learning methods.\n", "link": "http://arxiv.org/abs/2509.07662v1", "date": "2025-09-09", "relevancy": 2.3576, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6262}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5644}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EDFFDNet%3A%20Towards%20Accurate%20and%20Efficient%20Unsupervised%20Multi-Grid%20Image%0A%20%20Registration&body=Title%3A%20EDFFDNet%3A%20Towards%20Accurate%20and%20Efficient%20Unsupervised%20Multi-Grid%20Image%0A%20%20Registration%0AAuthor%3A%20Haokai%20Zhu%20and%20Bo%20Qu%20and%20Si-Yuan%20Cao%20and%20Runmin%20Zhang%20and%20Shujie%20Chen%20and%20Bailin%20Yang%20and%20Hui-Liang%20Shen%0AAbstract%3A%20%20%20Previous%20deep%20image%20registration%20methods%20that%20employ%20single%20homography%2C%0Amulti-grid%20homography%2C%20or%20thin-plate%20spline%20often%20struggle%20with%20real%20scenes%0Acontaining%20depth%20disparities%20due%20to%20their%20inherent%20limitations.%20To%20address%0Athis%2C%20we%20propose%20an%20Exponential-Decay%20Free-Form%20Deformation%20Network%20%28EDFFDNet%29%2C%0Awhich%20employs%20free-form%20deformation%20with%20an%20exponential-decay%20basis%20function.%0AThis%20design%20achieves%20higher%20efficiency%20and%20performs%20well%20in%20scenes%20with%20depth%0Adisparities%2C%20benefiting%20from%20its%20inherent%20locality.%20We%20also%20introduce%20an%0AAdaptive%20Sparse%20Motion%20Aggregator%20%28ASMA%29%2C%20which%20replaces%20the%20MLP%20motion%0Aaggregator%20used%20in%20previous%20methods.%20By%20transforming%20dense%20interactions%20into%0Asparse%20ones%2C%20ASMA%20reduces%20parameters%20and%20improves%20accuracy.%20Additionally%2C%20we%0Apropose%20a%20progressive%20correlation%20refinement%20strategy%20that%20leverages%0Aglobal-local%20correlation%20patterns%20for%20coarse-to-fine%20motion%20estimation%2C%20further%0Aenhancing%20efficiency%20and%20accuracy.%20Experiments%20demonstrate%20that%20EDFFDNet%0Areduces%20parameters%2C%20memory%2C%20and%20total%20runtime%20by%2070.5%25%2C%2032.6%25%2C%20and%2033.7%25%2C%0Arespectively%2C%20while%20achieving%20a%200.5%20dB%20PSNR%20gain%20over%20the%20state-of-the-art%0Amethod.%20With%20an%20additional%20local%20refinement%20stage%2CEDFFDNet-2%20further%20improves%0APSNR%20by%201.06%20dB%20while%20maintaining%20lower%20computational%20costs.%20Our%20method%20also%0Ademonstrates%20strong%20generalization%20ability%20across%20datasets%2C%20outperforming%0Aprevious%20deep%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEDFFDNet%253A%2520Towards%2520Accurate%2520and%2520Efficient%2520Unsupervised%2520Multi-Grid%2520Image%250A%2520%2520Registration%26entry.906535625%3DHaokai%2520Zhu%2520and%2520Bo%2520Qu%2520and%2520Si-Yuan%2520Cao%2520and%2520Runmin%2520Zhang%2520and%2520Shujie%2520Chen%2520and%2520Bailin%2520Yang%2520and%2520Hui-Liang%2520Shen%26entry.1292438233%3D%2520%2520Previous%2520deep%2520image%2520registration%2520methods%2520that%2520employ%2520single%2520homography%252C%250Amulti-grid%2520homography%252C%2520or%2520thin-plate%2520spline%2520often%2520struggle%2520with%2520real%2520scenes%250Acontaining%2520depth%2520disparities%2520due%2520to%2520their%2520inherent%2520limitations.%2520To%2520address%250Athis%252C%2520we%2520propose%2520an%2520Exponential-Decay%2520Free-Form%2520Deformation%2520Network%2520%2528EDFFDNet%2529%252C%250Awhich%2520employs%2520free-form%2520deformation%2520with%2520an%2520exponential-decay%2520basis%2520function.%250AThis%2520design%2520achieves%2520higher%2520efficiency%2520and%2520performs%2520well%2520in%2520scenes%2520with%2520depth%250Adisparities%252C%2520benefiting%2520from%2520its%2520inherent%2520locality.%2520We%2520also%2520introduce%2520an%250AAdaptive%2520Sparse%2520Motion%2520Aggregator%2520%2528ASMA%2529%252C%2520which%2520replaces%2520the%2520MLP%2520motion%250Aaggregator%2520used%2520in%2520previous%2520methods.%2520By%2520transforming%2520dense%2520interactions%2520into%250Asparse%2520ones%252C%2520ASMA%2520reduces%2520parameters%2520and%2520improves%2520accuracy.%2520Additionally%252C%2520we%250Apropose%2520a%2520progressive%2520correlation%2520refinement%2520strategy%2520that%2520leverages%250Aglobal-local%2520correlation%2520patterns%2520for%2520coarse-to-fine%2520motion%2520estimation%252C%2520further%250Aenhancing%2520efficiency%2520and%2520accuracy.%2520Experiments%2520demonstrate%2520that%2520EDFFDNet%250Areduces%2520parameters%252C%2520memory%252C%2520and%2520total%2520runtime%2520by%252070.5%2525%252C%252032.6%2525%252C%2520and%252033.7%2525%252C%250Arespectively%252C%2520while%2520achieving%2520a%25200.5%2520dB%2520PSNR%2520gain%2520over%2520the%2520state-of-the-art%250Amethod.%2520With%2520an%2520additional%2520local%2520refinement%2520stage%252CEDFFDNet-2%2520further%2520improves%250APSNR%2520by%25201.06%2520dB%2520while%2520maintaining%2520lower%2520computational%2520costs.%2520Our%2520method%2520also%250Ademonstrates%2520strong%2520generalization%2520ability%2520across%2520datasets%252C%2520outperforming%250Aprevious%2520deep%2520learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EDFFDNet%3A%20Towards%20Accurate%20and%20Efficient%20Unsupervised%20Multi-Grid%20Image%0A%20%20Registration&entry.906535625=Haokai%20Zhu%20and%20Bo%20Qu%20and%20Si-Yuan%20Cao%20and%20Runmin%20Zhang%20and%20Shujie%20Chen%20and%20Bailin%20Yang%20and%20Hui-Liang%20Shen&entry.1292438233=%20%20Previous%20deep%20image%20registration%20methods%20that%20employ%20single%20homography%2C%0Amulti-grid%20homography%2C%20or%20thin-plate%20spline%20often%20struggle%20with%20real%20scenes%0Acontaining%20depth%20disparities%20due%20to%20their%20inherent%20limitations.%20To%20address%0Athis%2C%20we%20propose%20an%20Exponential-Decay%20Free-Form%20Deformation%20Network%20%28EDFFDNet%29%2C%0Awhich%20employs%20free-form%20deformation%20with%20an%20exponential-decay%20basis%20function.%0AThis%20design%20achieves%20higher%20efficiency%20and%20performs%20well%20in%20scenes%20with%20depth%0Adisparities%2C%20benefiting%20from%20its%20inherent%20locality.%20We%20also%20introduce%20an%0AAdaptive%20Sparse%20Motion%20Aggregator%20%28ASMA%29%2C%20which%20replaces%20the%20MLP%20motion%0Aaggregator%20used%20in%20previous%20methods.%20By%20transforming%20dense%20interactions%20into%0Asparse%20ones%2C%20ASMA%20reduces%20parameters%20and%20improves%20accuracy.%20Additionally%2C%20we%0Apropose%20a%20progressive%20correlation%20refinement%20strategy%20that%20leverages%0Aglobal-local%20correlation%20patterns%20for%20coarse-to-fine%20motion%20estimation%2C%20further%0Aenhancing%20efficiency%20and%20accuracy.%20Experiments%20demonstrate%20that%20EDFFDNet%0Areduces%20parameters%2C%20memory%2C%20and%20total%20runtime%20by%2070.5%25%2C%2032.6%25%2C%20and%2033.7%25%2C%0Arespectively%2C%20while%20achieving%20a%200.5%20dB%20PSNR%20gain%20over%20the%20state-of-the-art%0Amethod.%20With%20an%20additional%20local%20refinement%20stage%2CEDFFDNet-2%20further%20improves%0APSNR%20by%201.06%20dB%20while%20maintaining%20lower%20computational%20costs.%20Our%20method%20also%0Ademonstrates%20strong%20generalization%20ability%20across%20datasets%2C%20outperforming%0Aprevious%20deep%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07662v1&entry.124074799=Read"},
{"title": "Beyond Rebalancing: Benchmarking Binary Classifiers Under Class\n  Imbalance Without Rebalancing Techniques", "author": "Ali Nawaz and Amir Ahmad and Shehroz S. Khan", "abstract": "  Class imbalance poses a significant challenge to supervised classification,\nparticularly in critical domains like medical diagnostics and anomaly detection\nwhere minority class instances are rare. While numerous studies have explored\nrebalancing techniques to address this issue, less attention has been given to\nevaluating the performance of binary classifiers under imbalance when no such\ntechniques are applied. Therefore, the goal of this study is to assess the\nperformance of binary classifiers \"as-is\", without performing any explicit\nrebalancing. Specifically, we systematically evaluate the robustness of a\ndiverse set of binary classifiers across both real-world and synthetic\ndatasets, under progressively reduced minority class sizes, using one-shot and\nfew-shot scenarios as baselines. Our approach also explores varying data\ncomplexities through synthetic decision boundary generation to simulate\nreal-world conditions. In addition to standard classifiers, we include\nexperiments using undersampling, oversampling strategies, and one-class\nclassification (OCC) methods to examine their behavior under severe imbalance.\nThe results confirm that classification becomes more difficult as data\ncomplexity increases and the minority class size decreases. While traditional\nclassifiers deteriorate under extreme imbalance, advanced models like TabPFN\nand boosting-based ensembles retain relatively higher performance and better\ngeneralization compared to traditional classifiers. Visual interpretability and\nevaluation metrics further validate these findings. Our work offers valuable\nguidance on model selection for imbalanced learning, providing insights into\nclassifier robustness without dependence on explicit rebalancing techniques.\n", "link": "http://arxiv.org/abs/2509.07605v1", "date": "2025-09-09", "relevancy": 2.351, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5226}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4441}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Rebalancing%3A%20Benchmarking%20Binary%20Classifiers%20Under%20Class%0A%20%20Imbalance%20Without%20Rebalancing%20Techniques&body=Title%3A%20Beyond%20Rebalancing%3A%20Benchmarking%20Binary%20Classifiers%20Under%20Class%0A%20%20Imbalance%20Without%20Rebalancing%20Techniques%0AAuthor%3A%20Ali%20Nawaz%20and%20Amir%20Ahmad%20and%20Shehroz%20S.%20Khan%0AAbstract%3A%20%20%20Class%20imbalance%20poses%20a%20significant%20challenge%20to%20supervised%20classification%2C%0Aparticularly%20in%20critical%20domains%20like%20medical%20diagnostics%20and%20anomaly%20detection%0Awhere%20minority%20class%20instances%20are%20rare.%20While%20numerous%20studies%20have%20explored%0Arebalancing%20techniques%20to%20address%20this%20issue%2C%20less%20attention%20has%20been%20given%20to%0Aevaluating%20the%20performance%20of%20binary%20classifiers%20under%20imbalance%20when%20no%20such%0Atechniques%20are%20applied.%20Therefore%2C%20the%20goal%20of%20this%20study%20is%20to%20assess%20the%0Aperformance%20of%20binary%20classifiers%20%22as-is%22%2C%20without%20performing%20any%20explicit%0Arebalancing.%20Specifically%2C%20we%20systematically%20evaluate%20the%20robustness%20of%20a%0Adiverse%20set%20of%20binary%20classifiers%20across%20both%20real-world%20and%20synthetic%0Adatasets%2C%20under%20progressively%20reduced%20minority%20class%20sizes%2C%20using%20one-shot%20and%0Afew-shot%20scenarios%20as%20baselines.%20Our%20approach%20also%20explores%20varying%20data%0Acomplexities%20through%20synthetic%20decision%20boundary%20generation%20to%20simulate%0Areal-world%20conditions.%20In%20addition%20to%20standard%20classifiers%2C%20we%20include%0Aexperiments%20using%20undersampling%2C%20oversampling%20strategies%2C%20and%20one-class%0Aclassification%20%28OCC%29%20methods%20to%20examine%20their%20behavior%20under%20severe%20imbalance.%0AThe%20results%20confirm%20that%20classification%20becomes%20more%20difficult%20as%20data%0Acomplexity%20increases%20and%20the%20minority%20class%20size%20decreases.%20While%20traditional%0Aclassifiers%20deteriorate%20under%20extreme%20imbalance%2C%20advanced%20models%20like%20TabPFN%0Aand%20boosting-based%20ensembles%20retain%20relatively%20higher%20performance%20and%20better%0Ageneralization%20compared%20to%20traditional%20classifiers.%20Visual%20interpretability%20and%0Aevaluation%20metrics%20further%20validate%20these%20findings.%20Our%20work%20offers%20valuable%0Aguidance%20on%20model%20selection%20for%20imbalanced%20learning%2C%20providing%20insights%20into%0Aclassifier%20robustness%20without%20dependence%20on%20explicit%20rebalancing%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Rebalancing%253A%2520Benchmarking%2520Binary%2520Classifiers%2520Under%2520Class%250A%2520%2520Imbalance%2520Without%2520Rebalancing%2520Techniques%26entry.906535625%3DAli%2520Nawaz%2520and%2520Amir%2520Ahmad%2520and%2520Shehroz%2520S.%2520Khan%26entry.1292438233%3D%2520%2520Class%2520imbalance%2520poses%2520a%2520significant%2520challenge%2520to%2520supervised%2520classification%252C%250Aparticularly%2520in%2520critical%2520domains%2520like%2520medical%2520diagnostics%2520and%2520anomaly%2520detection%250Awhere%2520minority%2520class%2520instances%2520are%2520rare.%2520While%2520numerous%2520studies%2520have%2520explored%250Arebalancing%2520techniques%2520to%2520address%2520this%2520issue%252C%2520less%2520attention%2520has%2520been%2520given%2520to%250Aevaluating%2520the%2520performance%2520of%2520binary%2520classifiers%2520under%2520imbalance%2520when%2520no%2520such%250Atechniques%2520are%2520applied.%2520Therefore%252C%2520the%2520goal%2520of%2520this%2520study%2520is%2520to%2520assess%2520the%250Aperformance%2520of%2520binary%2520classifiers%2520%2522as-is%2522%252C%2520without%2520performing%2520any%2520explicit%250Arebalancing.%2520Specifically%252C%2520we%2520systematically%2520evaluate%2520the%2520robustness%2520of%2520a%250Adiverse%2520set%2520of%2520binary%2520classifiers%2520across%2520both%2520real-world%2520and%2520synthetic%250Adatasets%252C%2520under%2520progressively%2520reduced%2520minority%2520class%2520sizes%252C%2520using%2520one-shot%2520and%250Afew-shot%2520scenarios%2520as%2520baselines.%2520Our%2520approach%2520also%2520explores%2520varying%2520data%250Acomplexities%2520through%2520synthetic%2520decision%2520boundary%2520generation%2520to%2520simulate%250Areal-world%2520conditions.%2520In%2520addition%2520to%2520standard%2520classifiers%252C%2520we%2520include%250Aexperiments%2520using%2520undersampling%252C%2520oversampling%2520strategies%252C%2520and%2520one-class%250Aclassification%2520%2528OCC%2529%2520methods%2520to%2520examine%2520their%2520behavior%2520under%2520severe%2520imbalance.%250AThe%2520results%2520confirm%2520that%2520classification%2520becomes%2520more%2520difficult%2520as%2520data%250Acomplexity%2520increases%2520and%2520the%2520minority%2520class%2520size%2520decreases.%2520While%2520traditional%250Aclassifiers%2520deteriorate%2520under%2520extreme%2520imbalance%252C%2520advanced%2520models%2520like%2520TabPFN%250Aand%2520boosting-based%2520ensembles%2520retain%2520relatively%2520higher%2520performance%2520and%2520better%250Ageneralization%2520compared%2520to%2520traditional%2520classifiers.%2520Visual%2520interpretability%2520and%250Aevaluation%2520metrics%2520further%2520validate%2520these%2520findings.%2520Our%2520work%2520offers%2520valuable%250Aguidance%2520on%2520model%2520selection%2520for%2520imbalanced%2520learning%252C%2520providing%2520insights%2520into%250Aclassifier%2520robustness%2520without%2520dependence%2520on%2520explicit%2520rebalancing%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Rebalancing%3A%20Benchmarking%20Binary%20Classifiers%20Under%20Class%0A%20%20Imbalance%20Without%20Rebalancing%20Techniques&entry.906535625=Ali%20Nawaz%20and%20Amir%20Ahmad%20and%20Shehroz%20S.%20Khan&entry.1292438233=%20%20Class%20imbalance%20poses%20a%20significant%20challenge%20to%20supervised%20classification%2C%0Aparticularly%20in%20critical%20domains%20like%20medical%20diagnostics%20and%20anomaly%20detection%0Awhere%20minority%20class%20instances%20are%20rare.%20While%20numerous%20studies%20have%20explored%0Arebalancing%20techniques%20to%20address%20this%20issue%2C%20less%20attention%20has%20been%20given%20to%0Aevaluating%20the%20performance%20of%20binary%20classifiers%20under%20imbalance%20when%20no%20such%0Atechniques%20are%20applied.%20Therefore%2C%20the%20goal%20of%20this%20study%20is%20to%20assess%20the%0Aperformance%20of%20binary%20classifiers%20%22as-is%22%2C%20without%20performing%20any%20explicit%0Arebalancing.%20Specifically%2C%20we%20systematically%20evaluate%20the%20robustness%20of%20a%0Adiverse%20set%20of%20binary%20classifiers%20across%20both%20real-world%20and%20synthetic%0Adatasets%2C%20under%20progressively%20reduced%20minority%20class%20sizes%2C%20using%20one-shot%20and%0Afew-shot%20scenarios%20as%20baselines.%20Our%20approach%20also%20explores%20varying%20data%0Acomplexities%20through%20synthetic%20decision%20boundary%20generation%20to%20simulate%0Areal-world%20conditions.%20In%20addition%20to%20standard%20classifiers%2C%20we%20include%0Aexperiments%20using%20undersampling%2C%20oversampling%20strategies%2C%20and%20one-class%0Aclassification%20%28OCC%29%20methods%20to%20examine%20their%20behavior%20under%20severe%20imbalance.%0AThe%20results%20confirm%20that%20classification%20becomes%20more%20difficult%20as%20data%0Acomplexity%20increases%20and%20the%20minority%20class%20size%20decreases.%20While%20traditional%0Aclassifiers%20deteriorate%20under%20extreme%20imbalance%2C%20advanced%20models%20like%20TabPFN%0Aand%20boosting-based%20ensembles%20retain%20relatively%20higher%20performance%20and%20better%0Ageneralization%20compared%20to%20traditional%20classifiers.%20Visual%20interpretability%20and%0Aevaluation%20metrics%20further%20validate%20these%20findings.%20Our%20work%20offers%20valuable%0Aguidance%20on%20model%20selection%20for%20imbalanced%20learning%2C%20providing%20insights%20into%0Aclassifier%20robustness%20without%20dependence%20on%20explicit%20rebalancing%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07605v1&entry.124074799=Read"},
{"title": "Spectral and Rhythm Feature Performance Evaluation for Category and\n  Class Level Audio Classification with Deep Convolutional Neural Networks", "author": "Friedrich Wolf-Monheim", "abstract": "  Next to decision tree and k-nearest neighbours algorithms deep convolutional\nneural networks (CNNs) are widely used to classify audio data in many domains\nlike music, speech or environmental sounds. To train a specific CNN various\nspectral and rhythm features like mel-scaled spectrograms, mel-frequency\ncepstral coefficients (MFCC), cyclic tempograms, short-time Fourier transform\n(STFT) chromagrams, constant-Q transform (CQT) chromagrams and chroma energy\nnormalized statistics (CENS) chromagrams can be used as digital image input\ndata for the neural network. The performance of these spectral and rhythm\nfeatures for audio category level as well as audio class level classification\nis investigated in detail with a deep CNN and the ESC-50 dataset with 2,000\nlabeled environmental audio recordings using an end-to-end deep learning\npipeline. The evaluated metrics accuracy, precision, recall and F1 score for\nmulticlass classification clearly show that the mel-scaled spectrograms and the\nmel-frequency cepstral coefficients (MFCC) perform significantly better then\nthe other spectral and rhythm features investigated in this research for audio\nclassification tasks using deep CNNs.\n", "link": "http://arxiv.org/abs/2509.07756v1", "date": "2025-09-09", "relevancy": 2.3455, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4919}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4686}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20and%20Rhythm%20Feature%20Performance%20Evaluation%20for%20Category%20and%0A%20%20Class%20Level%20Audio%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks&body=Title%3A%20Spectral%20and%20Rhythm%20Feature%20Performance%20Evaluation%20for%20Category%20and%0A%20%20Class%20Level%20Audio%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks%0AAuthor%3A%20Friedrich%20Wolf-Monheim%0AAbstract%3A%20%20%20Next%20to%20decision%20tree%20and%20k-nearest%20neighbours%20algorithms%20deep%20convolutional%0Aneural%20networks%20%28CNNs%29%20are%20widely%20used%20to%20classify%20audio%20data%20in%20many%20domains%0Alike%20music%2C%20speech%20or%20environmental%20sounds.%20To%20train%20a%20specific%20CNN%20various%0Aspectral%20and%20rhythm%20features%20like%20mel-scaled%20spectrograms%2C%20mel-frequency%0Acepstral%20coefficients%20%28MFCC%29%2C%20cyclic%20tempograms%2C%20short-time%20Fourier%20transform%0A%28STFT%29%20chromagrams%2C%20constant-Q%20transform%20%28CQT%29%20chromagrams%20and%20chroma%20energy%0Anormalized%20statistics%20%28CENS%29%20chromagrams%20can%20be%20used%20as%20digital%20image%20input%0Adata%20for%20the%20neural%20network.%20The%20performance%20of%20these%20spectral%20and%20rhythm%0Afeatures%20for%20audio%20category%20level%20as%20well%20as%20audio%20class%20level%20classification%0Ais%20investigated%20in%20detail%20with%20a%20deep%20CNN%20and%20the%20ESC-50%20dataset%20with%202%2C000%0Alabeled%20environmental%20audio%20recordings%20using%20an%20end-to-end%20deep%20learning%0Apipeline.%20The%20evaluated%20metrics%20accuracy%2C%20precision%2C%20recall%20and%20F1%20score%20for%0Amulticlass%20classification%20clearly%20show%20that%20the%20mel-scaled%20spectrograms%20and%20the%0Amel-frequency%20cepstral%20coefficients%20%28MFCC%29%20perform%20significantly%20better%20then%0Athe%20other%20spectral%20and%20rhythm%20features%20investigated%20in%20this%20research%20for%20audio%0Aclassification%20tasks%20using%20deep%20CNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520and%2520Rhythm%2520Feature%2520Performance%2520Evaluation%2520for%2520Category%2520and%250A%2520%2520Class%2520Level%2520Audio%2520Classification%2520with%2520Deep%2520Convolutional%2520Neural%2520Networks%26entry.906535625%3DFriedrich%2520Wolf-Monheim%26entry.1292438233%3D%2520%2520Next%2520to%2520decision%2520tree%2520and%2520k-nearest%2520neighbours%2520algorithms%2520deep%2520convolutional%250Aneural%2520networks%2520%2528CNNs%2529%2520are%2520widely%2520used%2520to%2520classify%2520audio%2520data%2520in%2520many%2520domains%250Alike%2520music%252C%2520speech%2520or%2520environmental%2520sounds.%2520To%2520train%2520a%2520specific%2520CNN%2520various%250Aspectral%2520and%2520rhythm%2520features%2520like%2520mel-scaled%2520spectrograms%252C%2520mel-frequency%250Acepstral%2520coefficients%2520%2528MFCC%2529%252C%2520cyclic%2520tempograms%252C%2520short-time%2520Fourier%2520transform%250A%2528STFT%2529%2520chromagrams%252C%2520constant-Q%2520transform%2520%2528CQT%2529%2520chromagrams%2520and%2520chroma%2520energy%250Anormalized%2520statistics%2520%2528CENS%2529%2520chromagrams%2520can%2520be%2520used%2520as%2520digital%2520image%2520input%250Adata%2520for%2520the%2520neural%2520network.%2520The%2520performance%2520of%2520these%2520spectral%2520and%2520rhythm%250Afeatures%2520for%2520audio%2520category%2520level%2520as%2520well%2520as%2520audio%2520class%2520level%2520classification%250Ais%2520investigated%2520in%2520detail%2520with%2520a%2520deep%2520CNN%2520and%2520the%2520ESC-50%2520dataset%2520with%25202%252C000%250Alabeled%2520environmental%2520audio%2520recordings%2520using%2520an%2520end-to-end%2520deep%2520learning%250Apipeline.%2520The%2520evaluated%2520metrics%2520accuracy%252C%2520precision%252C%2520recall%2520and%2520F1%2520score%2520for%250Amulticlass%2520classification%2520clearly%2520show%2520that%2520the%2520mel-scaled%2520spectrograms%2520and%2520the%250Amel-frequency%2520cepstral%2520coefficients%2520%2528MFCC%2529%2520perform%2520significantly%2520better%2520then%250Athe%2520other%2520spectral%2520and%2520rhythm%2520features%2520investigated%2520in%2520this%2520research%2520for%2520audio%250Aclassification%2520tasks%2520using%2520deep%2520CNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20and%20Rhythm%20Feature%20Performance%20Evaluation%20for%20Category%20and%0A%20%20Class%20Level%20Audio%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks&entry.906535625=Friedrich%20Wolf-Monheim&entry.1292438233=%20%20Next%20to%20decision%20tree%20and%20k-nearest%20neighbours%20algorithms%20deep%20convolutional%0Aneural%20networks%20%28CNNs%29%20are%20widely%20used%20to%20classify%20audio%20data%20in%20many%20domains%0Alike%20music%2C%20speech%20or%20environmental%20sounds.%20To%20train%20a%20specific%20CNN%20various%0Aspectral%20and%20rhythm%20features%20like%20mel-scaled%20spectrograms%2C%20mel-frequency%0Acepstral%20coefficients%20%28MFCC%29%2C%20cyclic%20tempograms%2C%20short-time%20Fourier%20transform%0A%28STFT%29%20chromagrams%2C%20constant-Q%20transform%20%28CQT%29%20chromagrams%20and%20chroma%20energy%0Anormalized%20statistics%20%28CENS%29%20chromagrams%20can%20be%20used%20as%20digital%20image%20input%0Adata%20for%20the%20neural%20network.%20The%20performance%20of%20these%20spectral%20and%20rhythm%0Afeatures%20for%20audio%20category%20level%20as%20well%20as%20audio%20class%20level%20classification%0Ais%20investigated%20in%20detail%20with%20a%20deep%20CNN%20and%20the%20ESC-50%20dataset%20with%202%2C000%0Alabeled%20environmental%20audio%20recordings%20using%20an%20end-to-end%20deep%20learning%0Apipeline.%20The%20evaluated%20metrics%20accuracy%2C%20precision%2C%20recall%20and%20F1%20score%20for%0Amulticlass%20classification%20clearly%20show%20that%20the%20mel-scaled%20spectrograms%20and%20the%0Amel-frequency%20cepstral%20coefficients%20%28MFCC%29%20perform%20significantly%20better%20then%0Athe%20other%20spectral%20and%20rhythm%20features%20investigated%20in%20this%20research%20for%20audio%0Aclassification%20tasks%20using%20deep%20CNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07756v1&entry.124074799=Read"},
{"title": "Interpretable Text-Guided Image Clustering via Iterative Search", "author": "Bingchen Zhao and Oisin Mac Aodha", "abstract": "  Traditional clustering methods aim to group unlabeled data points based on\ntheir similarity to each other. However, clustering, in the absence of\nadditional information, is an ill-posed problem as there may be many different,\nyet equally valid, ways to partition a dataset. Distinct users may want to use\ndifferent criteria to form clusters in the same data, e.g. shape v.s. color.\nRecently introduced text-guided image clustering methods aim to address this\nambiguity by allowing users to specify the criteria of interest using natural\nlanguage instructions. This instruction provides the necessary context and\ncontrol needed to obtain clusters that are more aligned with the users' intent.\nWe propose a new text-guided clustering approach named ITGC that uses an\niterative discovery process, guided by an unsupervised clustering objective, to\ngenerate interpretable visual concepts that better capture the criteria\nexpressed in a user's instructions. We report superior performance compared to\nexisting methods across a wide variety of image clustering and fine-grained\nclassification benchmarks.\n", "link": "http://arxiv.org/abs/2506.12514v2", "date": "2025-09-09", "relevancy": 2.3455, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4854}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4617}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Text-Guided%20Image%20Clustering%20via%20Iterative%20Search&body=Title%3A%20Interpretable%20Text-Guided%20Image%20Clustering%20via%20Iterative%20Search%0AAuthor%3A%20Bingchen%20Zhao%20and%20Oisin%20Mac%20Aodha%0AAbstract%3A%20%20%20Traditional%20clustering%20methods%20aim%20to%20group%20unlabeled%20data%20points%20based%20on%0Atheir%20similarity%20to%20each%20other.%20However%2C%20clustering%2C%20in%20the%20absence%20of%0Aadditional%20information%2C%20is%20an%20ill-posed%20problem%20as%20there%20may%20be%20many%20different%2C%0Ayet%20equally%20valid%2C%20ways%20to%20partition%20a%20dataset.%20Distinct%20users%20may%20want%20to%20use%0Adifferent%20criteria%20to%20form%20clusters%20in%20the%20same%20data%2C%20e.g.%20shape%20v.s.%20color.%0ARecently%20introduced%20text-guided%20image%20clustering%20methods%20aim%20to%20address%20this%0Aambiguity%20by%20allowing%20users%20to%20specify%20the%20criteria%20of%20interest%20using%20natural%0Alanguage%20instructions.%20This%20instruction%20provides%20the%20necessary%20context%20and%0Acontrol%20needed%20to%20obtain%20clusters%20that%20are%20more%20aligned%20with%20the%20users%27%20intent.%0AWe%20propose%20a%20new%20text-guided%20clustering%20approach%20named%20ITGC%20that%20uses%20an%0Aiterative%20discovery%20process%2C%20guided%20by%20an%20unsupervised%20clustering%20objective%2C%20to%0Agenerate%20interpretable%20visual%20concepts%20that%20better%20capture%20the%20criteria%0Aexpressed%20in%20a%20user%27s%20instructions.%20We%20report%20superior%20performance%20compared%20to%0Aexisting%20methods%20across%20a%20wide%20variety%20of%20image%20clustering%20and%20fine-grained%0Aclassification%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12514v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Text-Guided%2520Image%2520Clustering%2520via%2520Iterative%2520Search%26entry.906535625%3DBingchen%2520Zhao%2520and%2520Oisin%2520Mac%2520Aodha%26entry.1292438233%3D%2520%2520Traditional%2520clustering%2520methods%2520aim%2520to%2520group%2520unlabeled%2520data%2520points%2520based%2520on%250Atheir%2520similarity%2520to%2520each%2520other.%2520However%252C%2520clustering%252C%2520in%2520the%2520absence%2520of%250Aadditional%2520information%252C%2520is%2520an%2520ill-posed%2520problem%2520as%2520there%2520may%2520be%2520many%2520different%252C%250Ayet%2520equally%2520valid%252C%2520ways%2520to%2520partition%2520a%2520dataset.%2520Distinct%2520users%2520may%2520want%2520to%2520use%250Adifferent%2520criteria%2520to%2520form%2520clusters%2520in%2520the%2520same%2520data%252C%2520e.g.%2520shape%2520v.s.%2520color.%250ARecently%2520introduced%2520text-guided%2520image%2520clustering%2520methods%2520aim%2520to%2520address%2520this%250Aambiguity%2520by%2520allowing%2520users%2520to%2520specify%2520the%2520criteria%2520of%2520interest%2520using%2520natural%250Alanguage%2520instructions.%2520This%2520instruction%2520provides%2520the%2520necessary%2520context%2520and%250Acontrol%2520needed%2520to%2520obtain%2520clusters%2520that%2520are%2520more%2520aligned%2520with%2520the%2520users%2527%2520intent.%250AWe%2520propose%2520a%2520new%2520text-guided%2520clustering%2520approach%2520named%2520ITGC%2520that%2520uses%2520an%250Aiterative%2520discovery%2520process%252C%2520guided%2520by%2520an%2520unsupervised%2520clustering%2520objective%252C%2520to%250Agenerate%2520interpretable%2520visual%2520concepts%2520that%2520better%2520capture%2520the%2520criteria%250Aexpressed%2520in%2520a%2520user%2527s%2520instructions.%2520We%2520report%2520superior%2520performance%2520compared%2520to%250Aexisting%2520methods%2520across%2520a%2520wide%2520variety%2520of%2520image%2520clustering%2520and%2520fine-grained%250Aclassification%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12514v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Text-Guided%20Image%20Clustering%20via%20Iterative%20Search&entry.906535625=Bingchen%20Zhao%20and%20Oisin%20Mac%20Aodha&entry.1292438233=%20%20Traditional%20clustering%20methods%20aim%20to%20group%20unlabeled%20data%20points%20based%20on%0Atheir%20similarity%20to%20each%20other.%20However%2C%20clustering%2C%20in%20the%20absence%20of%0Aadditional%20information%2C%20is%20an%20ill-posed%20problem%20as%20there%20may%20be%20many%20different%2C%0Ayet%20equally%20valid%2C%20ways%20to%20partition%20a%20dataset.%20Distinct%20users%20may%20want%20to%20use%0Adifferent%20criteria%20to%20form%20clusters%20in%20the%20same%20data%2C%20e.g.%20shape%20v.s.%20color.%0ARecently%20introduced%20text-guided%20image%20clustering%20methods%20aim%20to%20address%20this%0Aambiguity%20by%20allowing%20users%20to%20specify%20the%20criteria%20of%20interest%20using%20natural%0Alanguage%20instructions.%20This%20instruction%20provides%20the%20necessary%20context%20and%0Acontrol%20needed%20to%20obtain%20clusters%20that%20are%20more%20aligned%20with%20the%20users%27%20intent.%0AWe%20propose%20a%20new%20text-guided%20clustering%20approach%20named%20ITGC%20that%20uses%20an%0Aiterative%20discovery%20process%2C%20guided%20by%20an%20unsupervised%20clustering%20objective%2C%20to%0Agenerate%20interpretable%20visual%20concepts%20that%20better%20capture%20the%20criteria%0Aexpressed%20in%20a%20user%27s%20instructions.%20We%20report%20superior%20performance%20compared%20to%0Aexisting%20methods%20across%20a%20wide%20variety%20of%20image%20clustering%20and%20fine-grained%0Aclassification%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12514v2&entry.124074799=Read"},
{"title": "Robust Radar SLAM for Vehicle Parking Applications", "author": "Luis Diener and Jens Kalkkuhl and Markus Enzweiler", "abstract": "  We address ego-motion estimation for automated parking, where\ncentimeter-level accuracy is crucial due to tight spaces and nearby obstacles.\nTraditional methods using inertial-measurement units and wheel encoders require\ncalibration, making them costly and time-consuming. To overcome this, we\npropose a radar-based simultaneous localization and mapping (SLAM) approach\nthat leverages the robustness of radar to adverse weather and support for\nonline calibration. Our robocentric formulation fuses feature positions and\nDoppler velocities for robust data association and filter convergence. Key\ncontributions include a Doppler-augmented radar SLAM method, multi-radar\nsupport and an information-based feature-pruning strategy. Experiments\ndemonstrate high-accuracy localization and improved robustness over\nstate-of-the-art methods, meeting the demands of automated parking.\n", "link": "http://arxiv.org/abs/2509.07683v1", "date": "2025-09-09", "relevancy": 2.3426, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6152}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5675}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Radar%20SLAM%20for%20Vehicle%20Parking%20Applications&body=Title%3A%20Robust%20Radar%20SLAM%20for%20Vehicle%20Parking%20Applications%0AAuthor%3A%20Luis%20Diener%20and%20Jens%20Kalkkuhl%20and%20Markus%20Enzweiler%0AAbstract%3A%20%20%20We%20address%20ego-motion%20estimation%20for%20automated%20parking%2C%20where%0Acentimeter-level%20accuracy%20is%20crucial%20due%20to%20tight%20spaces%20and%20nearby%20obstacles.%0ATraditional%20methods%20using%20inertial-measurement%20units%20and%20wheel%20encoders%20require%0Acalibration%2C%20making%20them%20costly%20and%20time-consuming.%20To%20overcome%20this%2C%20we%0Apropose%20a%20radar-based%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20approach%0Athat%20leverages%20the%20robustness%20of%20radar%20to%20adverse%20weather%20and%20support%20for%0Aonline%20calibration.%20Our%20robocentric%20formulation%20fuses%20feature%20positions%20and%0ADoppler%20velocities%20for%20robust%20data%20association%20and%20filter%20convergence.%20Key%0Acontributions%20include%20a%20Doppler-augmented%20radar%20SLAM%20method%2C%20multi-radar%0Asupport%20and%20an%20information-based%20feature-pruning%20strategy.%20Experiments%0Ademonstrate%20high-accuracy%20localization%20and%20improved%20robustness%20over%0Astate-of-the-art%20methods%2C%20meeting%20the%20demands%20of%20automated%20parking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Radar%2520SLAM%2520for%2520Vehicle%2520Parking%2520Applications%26entry.906535625%3DLuis%2520Diener%2520and%2520Jens%2520Kalkkuhl%2520and%2520Markus%2520Enzweiler%26entry.1292438233%3D%2520%2520We%2520address%2520ego-motion%2520estimation%2520for%2520automated%2520parking%252C%2520where%250Acentimeter-level%2520accuracy%2520is%2520crucial%2520due%2520to%2520tight%2520spaces%2520and%2520nearby%2520obstacles.%250ATraditional%2520methods%2520using%2520inertial-measurement%2520units%2520and%2520wheel%2520encoders%2520require%250Acalibration%252C%2520making%2520them%2520costly%2520and%2520time-consuming.%2520To%2520overcome%2520this%252C%2520we%250Apropose%2520a%2520radar-based%2520simultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%2520approach%250Athat%2520leverages%2520the%2520robustness%2520of%2520radar%2520to%2520adverse%2520weather%2520and%2520support%2520for%250Aonline%2520calibration.%2520Our%2520robocentric%2520formulation%2520fuses%2520feature%2520positions%2520and%250ADoppler%2520velocities%2520for%2520robust%2520data%2520association%2520and%2520filter%2520convergence.%2520Key%250Acontributions%2520include%2520a%2520Doppler-augmented%2520radar%2520SLAM%2520method%252C%2520multi-radar%250Asupport%2520and%2520an%2520information-based%2520feature-pruning%2520strategy.%2520Experiments%250Ademonstrate%2520high-accuracy%2520localization%2520and%2520improved%2520robustness%2520over%250Astate-of-the-art%2520methods%252C%2520meeting%2520the%2520demands%2520of%2520automated%2520parking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Radar%20SLAM%20for%20Vehicle%20Parking%20Applications&entry.906535625=Luis%20Diener%20and%20Jens%20Kalkkuhl%20and%20Markus%20Enzweiler&entry.1292438233=%20%20We%20address%20ego-motion%20estimation%20for%20automated%20parking%2C%20where%0Acentimeter-level%20accuracy%20is%20crucial%20due%20to%20tight%20spaces%20and%20nearby%20obstacles.%0ATraditional%20methods%20using%20inertial-measurement%20units%20and%20wheel%20encoders%20require%0Acalibration%2C%20making%20them%20costly%20and%20time-consuming.%20To%20overcome%20this%2C%20we%0Apropose%20a%20radar-based%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20approach%0Athat%20leverages%20the%20robustness%20of%20radar%20to%20adverse%20weather%20and%20support%20for%0Aonline%20calibration.%20Our%20robocentric%20formulation%20fuses%20feature%20positions%20and%0ADoppler%20velocities%20for%20robust%20data%20association%20and%20filter%20convergence.%20Key%0Acontributions%20include%20a%20Doppler-augmented%20radar%20SLAM%20method%2C%20multi-radar%0Asupport%20and%20an%20information-based%20feature-pruning%20strategy.%20Experiments%0Ademonstrate%20high-accuracy%20localization%20and%20improved%20robustness%20over%0Astate-of-the-art%20methods%2C%20meeting%20the%20demands%20of%20automated%20parking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07683v1&entry.124074799=Read"},
{"title": "DIP: Unsupervised Dense In-Context Post-training of Visual\n  Representations", "author": "Sophia Sirko-Galouchenko and Spyros Gidaris and Antonin Vobecky and Andrei Bursuc and Nicolas Thome", "abstract": "  We introduce DIP, a novel unsupervised post-training method designed to\nenhance dense image representations in large-scale pretrained vision encoders\nfor in-context scene understanding. Unlike prior approaches that rely on\ncomplex self-distillation architectures, our method trains the vision encoder\nusing pseudo-tasks that explicitly simulate downstream in-context scenarios,\ninspired by meta-learning principles. To enable post-training on unlabeled\ndata, we propose an automatic mechanism for generating in-context tasks that\ncombines a pretrained diffusion model and the vision encoder itself. DIP is\nsimple, unsupervised, and computationally efficient, requiring less than 9\nhours on a single A100 GPU. By learning dense representations through pseudo\nin-context tasks, it achieves strong performance across a wide variety of\ndownstream real-world in-context scene understanding tasks. It outperforms both\nthe initial vision encoder and prior methods, offering a practical and\neffective solution for improving dense representations. Code available here:\nhttps://github.com/sirkosophia/DIP\n", "link": "http://arxiv.org/abs/2506.18463v3", "date": "2025-09-09", "relevancy": 2.3374, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5847}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5847}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIP%3A%20Unsupervised%20Dense%20In-Context%20Post-training%20of%20Visual%0A%20%20Representations&body=Title%3A%20DIP%3A%20Unsupervised%20Dense%20In-Context%20Post-training%20of%20Visual%0A%20%20Representations%0AAuthor%3A%20Sophia%20Sirko-Galouchenko%20and%20Spyros%20Gidaris%20and%20Antonin%20Vobecky%20and%20Andrei%20Bursuc%20and%20Nicolas%20Thome%0AAbstract%3A%20%20%20We%20introduce%20DIP%2C%20a%20novel%20unsupervised%20post-training%20method%20designed%20to%0Aenhance%20dense%20image%20representations%20in%20large-scale%20pretrained%20vision%20encoders%0Afor%20in-context%20scene%20understanding.%20Unlike%20prior%20approaches%20that%20rely%20on%0Acomplex%20self-distillation%20architectures%2C%20our%20method%20trains%20the%20vision%20encoder%0Ausing%20pseudo-tasks%20that%20explicitly%20simulate%20downstream%20in-context%20scenarios%2C%0Ainspired%20by%20meta-learning%20principles.%20To%20enable%20post-training%20on%20unlabeled%0Adata%2C%20we%20propose%20an%20automatic%20mechanism%20for%20generating%20in-context%20tasks%20that%0Acombines%20a%20pretrained%20diffusion%20model%20and%20the%20vision%20encoder%20itself.%20DIP%20is%0Asimple%2C%20unsupervised%2C%20and%20computationally%20efficient%2C%20requiring%20less%20than%209%0Ahours%20on%20a%20single%20A100%20GPU.%20By%20learning%20dense%20representations%20through%20pseudo%0Ain-context%20tasks%2C%20it%20achieves%20strong%20performance%20across%20a%20wide%20variety%20of%0Adownstream%20real-world%20in-context%20scene%20understanding%20tasks.%20It%20outperforms%20both%0Athe%20initial%20vision%20encoder%20and%20prior%20methods%2C%20offering%20a%20practical%20and%0Aeffective%20solution%20for%20improving%20dense%20representations.%20Code%20available%20here%3A%0Ahttps%3A//github.com/sirkosophia/DIP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.18463v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIP%253A%2520Unsupervised%2520Dense%2520In-Context%2520Post-training%2520of%2520Visual%250A%2520%2520Representations%26entry.906535625%3DSophia%2520Sirko-Galouchenko%2520and%2520Spyros%2520Gidaris%2520and%2520Antonin%2520Vobecky%2520and%2520Andrei%2520Bursuc%2520and%2520Nicolas%2520Thome%26entry.1292438233%3D%2520%2520We%2520introduce%2520DIP%252C%2520a%2520novel%2520unsupervised%2520post-training%2520method%2520designed%2520to%250Aenhance%2520dense%2520image%2520representations%2520in%2520large-scale%2520pretrained%2520vision%2520encoders%250Afor%2520in-context%2520scene%2520understanding.%2520Unlike%2520prior%2520approaches%2520that%2520rely%2520on%250Acomplex%2520self-distillation%2520architectures%252C%2520our%2520method%2520trains%2520the%2520vision%2520encoder%250Ausing%2520pseudo-tasks%2520that%2520explicitly%2520simulate%2520downstream%2520in-context%2520scenarios%252C%250Ainspired%2520by%2520meta-learning%2520principles.%2520To%2520enable%2520post-training%2520on%2520unlabeled%250Adata%252C%2520we%2520propose%2520an%2520automatic%2520mechanism%2520for%2520generating%2520in-context%2520tasks%2520that%250Acombines%2520a%2520pretrained%2520diffusion%2520model%2520and%2520the%2520vision%2520encoder%2520itself.%2520DIP%2520is%250Asimple%252C%2520unsupervised%252C%2520and%2520computationally%2520efficient%252C%2520requiring%2520less%2520than%25209%250Ahours%2520on%2520a%2520single%2520A100%2520GPU.%2520By%2520learning%2520dense%2520representations%2520through%2520pseudo%250Ain-context%2520tasks%252C%2520it%2520achieves%2520strong%2520performance%2520across%2520a%2520wide%2520variety%2520of%250Adownstream%2520real-world%2520in-context%2520scene%2520understanding%2520tasks.%2520It%2520outperforms%2520both%250Athe%2520initial%2520vision%2520encoder%2520and%2520prior%2520methods%252C%2520offering%2520a%2520practical%2520and%250Aeffective%2520solution%2520for%2520improving%2520dense%2520representations.%2520Code%2520available%2520here%253A%250Ahttps%253A//github.com/sirkosophia/DIP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18463v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIP%3A%20Unsupervised%20Dense%20In-Context%20Post-training%20of%20Visual%0A%20%20Representations&entry.906535625=Sophia%20Sirko-Galouchenko%20and%20Spyros%20Gidaris%20and%20Antonin%20Vobecky%20and%20Andrei%20Bursuc%20and%20Nicolas%20Thome&entry.1292438233=%20%20We%20introduce%20DIP%2C%20a%20novel%20unsupervised%20post-training%20method%20designed%20to%0Aenhance%20dense%20image%20representations%20in%20large-scale%20pretrained%20vision%20encoders%0Afor%20in-context%20scene%20understanding.%20Unlike%20prior%20approaches%20that%20rely%20on%0Acomplex%20self-distillation%20architectures%2C%20our%20method%20trains%20the%20vision%20encoder%0Ausing%20pseudo-tasks%20that%20explicitly%20simulate%20downstream%20in-context%20scenarios%2C%0Ainspired%20by%20meta-learning%20principles.%20To%20enable%20post-training%20on%20unlabeled%0Adata%2C%20we%20propose%20an%20automatic%20mechanism%20for%20generating%20in-context%20tasks%20that%0Acombines%20a%20pretrained%20diffusion%20model%20and%20the%20vision%20encoder%20itself.%20DIP%20is%0Asimple%2C%20unsupervised%2C%20and%20computationally%20efficient%2C%20requiring%20less%20than%209%0Ahours%20on%20a%20single%20A100%20GPU.%20By%20learning%20dense%20representations%20through%20pseudo%0Ain-context%20tasks%2C%20it%20achieves%20strong%20performance%20across%20a%20wide%20variety%20of%0Adownstream%20real-world%20in-context%20scene%20understanding%20tasks.%20It%20outperforms%20both%0Athe%20initial%20vision%20encoder%20and%20prior%20methods%2C%20offering%20a%20practical%20and%0Aeffective%20solution%20for%20improving%20dense%20representations.%20Code%20available%20here%3A%0Ahttps%3A//github.com/sirkosophia/DIP%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.18463v3&entry.124074799=Read"},
{"title": "$\u03c0^3$: Permutation-Equivariant Visual Geometry Learning", "author": "Yifan Wang and Jianjun Zhou and Haoyi Zhu and Wenzheng Chang and Yang Zhou and Zizun Li and Junyi Chen and Jiangmiao Pang and Chunhua Shen and Tong He", "abstract": "  We introduce $\\pi^3$, a feed-forward neural network that offers a novel\napproach to visual geometry reconstruction, breaking the reliance on a\nconventional fixed reference view. Previous methods often anchor their\nreconstructions to a designated viewpoint, an inductive bias that can lead to\ninstability and failures if the reference is suboptimal. In contrast, $\\pi^3$\nemploys a fully permutation-equivariant architecture to predict\naffine-invariant camera poses and scale-invariant local point maps without any\nreference frames. This design not only makes our model inherently robust to\ninput ordering, but also leads to higher accuracy and performance. These\nadvantages enable our simple and bias-free approach to achieve state-of-the-art\nperformance on a wide range of tasks, including camera pose estimation,\nmonocular/video depth estimation, and dense point map reconstruction. Code and\nmodels are publicly available.\n", "link": "http://arxiv.org/abs/2507.13347v2", "date": "2025-09-09", "relevancy": 2.3357, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5901}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5802}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%CF%80%5E3%24%3A%20Permutation-Equivariant%20Visual%20Geometry%20Learning&body=Title%3A%20%24%CF%80%5E3%24%3A%20Permutation-Equivariant%20Visual%20Geometry%20Learning%0AAuthor%3A%20Yifan%20Wang%20and%20Jianjun%20Zhou%20and%20Haoyi%20Zhu%20and%20Wenzheng%20Chang%20and%20Yang%20Zhou%20and%20Zizun%20Li%20and%20Junyi%20Chen%20and%20Jiangmiao%20Pang%20and%20Chunhua%20Shen%20and%20Tong%20He%0AAbstract%3A%20%20%20We%20introduce%20%24%5Cpi%5E3%24%2C%20a%20feed-forward%20neural%20network%20that%20offers%20a%20novel%0Aapproach%20to%20visual%20geometry%20reconstruction%2C%20breaking%20the%20reliance%20on%20a%0Aconventional%20fixed%20reference%20view.%20Previous%20methods%20often%20anchor%20their%0Areconstructions%20to%20a%20designated%20viewpoint%2C%20an%20inductive%20bias%20that%20can%20lead%20to%0Ainstability%20and%20failures%20if%20the%20reference%20is%20suboptimal.%20In%20contrast%2C%20%24%5Cpi%5E3%24%0Aemploys%20a%20fully%20permutation-equivariant%20architecture%20to%20predict%0Aaffine-invariant%20camera%20poses%20and%20scale-invariant%20local%20point%20maps%20without%20any%0Areference%20frames.%20This%20design%20not%20only%20makes%20our%20model%20inherently%20robust%20to%0Ainput%20ordering%2C%20but%20also%20leads%20to%20higher%20accuracy%20and%20performance.%20These%0Aadvantages%20enable%20our%20simple%20and%20bias-free%20approach%20to%20achieve%20state-of-the-art%0Aperformance%20on%20a%20wide%20range%20of%20tasks%2C%20including%20camera%20pose%20estimation%2C%0Amonocular/video%20depth%20estimation%2C%20and%20dense%20point%20map%20reconstruction.%20Code%20and%0Amodels%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13347v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%25CF%2580%255E3%2524%253A%2520Permutation-Equivariant%2520Visual%2520Geometry%2520Learning%26entry.906535625%3DYifan%2520Wang%2520and%2520Jianjun%2520Zhou%2520and%2520Haoyi%2520Zhu%2520and%2520Wenzheng%2520Chang%2520and%2520Yang%2520Zhou%2520and%2520Zizun%2520Li%2520and%2520Junyi%2520Chen%2520and%2520Jiangmiao%2520Pang%2520and%2520Chunhua%2520Shen%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%2520We%2520introduce%2520%2524%255Cpi%255E3%2524%252C%2520a%2520feed-forward%2520neural%2520network%2520that%2520offers%2520a%2520novel%250Aapproach%2520to%2520visual%2520geometry%2520reconstruction%252C%2520breaking%2520the%2520reliance%2520on%2520a%250Aconventional%2520fixed%2520reference%2520view.%2520Previous%2520methods%2520often%2520anchor%2520their%250Areconstructions%2520to%2520a%2520designated%2520viewpoint%252C%2520an%2520inductive%2520bias%2520that%2520can%2520lead%2520to%250Ainstability%2520and%2520failures%2520if%2520the%2520reference%2520is%2520suboptimal.%2520In%2520contrast%252C%2520%2524%255Cpi%255E3%2524%250Aemploys%2520a%2520fully%2520permutation-equivariant%2520architecture%2520to%2520predict%250Aaffine-invariant%2520camera%2520poses%2520and%2520scale-invariant%2520local%2520point%2520maps%2520without%2520any%250Areference%2520frames.%2520This%2520design%2520not%2520only%2520makes%2520our%2520model%2520inherently%2520robust%2520to%250Ainput%2520ordering%252C%2520but%2520also%2520leads%2520to%2520higher%2520accuracy%2520and%2520performance.%2520These%250Aadvantages%2520enable%2520our%2520simple%2520and%2520bias-free%2520approach%2520to%2520achieve%2520state-of-the-art%250Aperformance%2520on%2520a%2520wide%2520range%2520of%2520tasks%252C%2520including%2520camera%2520pose%2520estimation%252C%250Amonocular/video%2520depth%2520estimation%252C%2520and%2520dense%2520point%2520map%2520reconstruction.%2520Code%2520and%250Amodels%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13347v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%CF%80%5E3%24%3A%20Permutation-Equivariant%20Visual%20Geometry%20Learning&entry.906535625=Yifan%20Wang%20and%20Jianjun%20Zhou%20and%20Haoyi%20Zhu%20and%20Wenzheng%20Chang%20and%20Yang%20Zhou%20and%20Zizun%20Li%20and%20Junyi%20Chen%20and%20Jiangmiao%20Pang%20and%20Chunhua%20Shen%20and%20Tong%20He&entry.1292438233=%20%20We%20introduce%20%24%5Cpi%5E3%24%2C%20a%20feed-forward%20neural%20network%20that%20offers%20a%20novel%0Aapproach%20to%20visual%20geometry%20reconstruction%2C%20breaking%20the%20reliance%20on%20a%0Aconventional%20fixed%20reference%20view.%20Previous%20methods%20often%20anchor%20their%0Areconstructions%20to%20a%20designated%20viewpoint%2C%20an%20inductive%20bias%20that%20can%20lead%20to%0Ainstability%20and%20failures%20if%20the%20reference%20is%20suboptimal.%20In%20contrast%2C%20%24%5Cpi%5E3%24%0Aemploys%20a%20fully%20permutation-equivariant%20architecture%20to%20predict%0Aaffine-invariant%20camera%20poses%20and%20scale-invariant%20local%20point%20maps%20without%20any%0Areference%20frames.%20This%20design%20not%20only%20makes%20our%20model%20inherently%20robust%20to%0Ainput%20ordering%2C%20but%20also%20leads%20to%20higher%20accuracy%20and%20performance.%20These%0Aadvantages%20enable%20our%20simple%20and%20bias-free%20approach%20to%20achieve%20state-of-the-art%0Aperformance%20on%20a%20wide%20range%20of%20tasks%2C%20including%20camera%20pose%20estimation%2C%0Amonocular/video%20depth%20estimation%2C%20and%20dense%20point%20map%20reconstruction.%20Code%20and%0Amodels%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13347v2&entry.124074799=Read"},
{"title": "Feature Space Analysis by Guided Diffusion Model", "author": "Kimiaki Shirahama and Miki Yanobu and Kaduki Yamashita and Miho Ohsaki", "abstract": "  One of the key issues in Deep Neural Networks (DNNs) is the black-box nature\nof their internal feature extraction process. Targeting vision-related domains,\nthis paper focuses on analysing the feature space of a DNN by proposing a\ndecoder that can generate images whose features are guaranteed to closely match\na user-specified feature. Owing to this guarantee that is missed in past\nstudies, our decoder allows us to evidence which of various attributes in an\nimage are encoded into a feature by the DNN, by generating images whose\nfeatures are in proximity to that feature. Our decoder is implemented as a\nguided diffusion model that guides the reverse image generation of a\npre-trained diffusion model to minimise the Euclidean distance between the\nfeature of a clean image estimated at each step and the user-specified feature.\nOne practical advantage of our decoder is that it can analyse feature spaces of\ndifferent DNNs with no additional training and run on a single COTS GPU. The\nexperimental results targeting CLIP's image encoder, ResNet-50 and vision\ntransformer demonstrate that images generated by our decoder have features\nremarkably similar to the user-specified ones and reveal valuable insights into\nthese DNNs' feature spaces.\n", "link": "http://arxiv.org/abs/2509.07936v1", "date": "2025-09-09", "relevancy": 2.3143, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6554}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5632}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Space%20Analysis%20by%20Guided%20Diffusion%20Model&body=Title%3A%20Feature%20Space%20Analysis%20by%20Guided%20Diffusion%20Model%0AAuthor%3A%20Kimiaki%20Shirahama%20and%20Miki%20Yanobu%20and%20Kaduki%20Yamashita%20and%20Miho%20Ohsaki%0AAbstract%3A%20%20%20One%20of%20the%20key%20issues%20in%20Deep%20Neural%20Networks%20%28DNNs%29%20is%20the%20black-box%20nature%0Aof%20their%20internal%20feature%20extraction%20process.%20Targeting%20vision-related%20domains%2C%0Athis%20paper%20focuses%20on%20analysing%20the%20feature%20space%20of%20a%20DNN%20by%20proposing%20a%0Adecoder%20that%20can%20generate%20images%20whose%20features%20are%20guaranteed%20to%20closely%20match%0Aa%20user-specified%20feature.%20Owing%20to%20this%20guarantee%20that%20is%20missed%20in%20past%0Astudies%2C%20our%20decoder%20allows%20us%20to%20evidence%20which%20of%20various%20attributes%20in%20an%0Aimage%20are%20encoded%20into%20a%20feature%20by%20the%20DNN%2C%20by%20generating%20images%20whose%0Afeatures%20are%20in%20proximity%20to%20that%20feature.%20Our%20decoder%20is%20implemented%20as%20a%0Aguided%20diffusion%20model%20that%20guides%20the%20reverse%20image%20generation%20of%20a%0Apre-trained%20diffusion%20model%20to%20minimise%20the%20Euclidean%20distance%20between%20the%0Afeature%20of%20a%20clean%20image%20estimated%20at%20each%20step%20and%20the%20user-specified%20feature.%0AOne%20practical%20advantage%20of%20our%20decoder%20is%20that%20it%20can%20analyse%20feature%20spaces%20of%0Adifferent%20DNNs%20with%20no%20additional%20training%20and%20run%20on%20a%20single%20COTS%20GPU.%20The%0Aexperimental%20results%20targeting%20CLIP%27s%20image%20encoder%2C%20ResNet-50%20and%20vision%0Atransformer%20demonstrate%20that%20images%20generated%20by%20our%20decoder%20have%20features%0Aremarkably%20similar%20to%20the%20user-specified%20ones%20and%20reveal%20valuable%20insights%20into%0Athese%20DNNs%27%20feature%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Space%2520Analysis%2520by%2520Guided%2520Diffusion%2520Model%26entry.906535625%3DKimiaki%2520Shirahama%2520and%2520Miki%2520Yanobu%2520and%2520Kaduki%2520Yamashita%2520and%2520Miho%2520Ohsaki%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520key%2520issues%2520in%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520is%2520the%2520black-box%2520nature%250Aof%2520their%2520internal%2520feature%2520extraction%2520process.%2520Targeting%2520vision-related%2520domains%252C%250Athis%2520paper%2520focuses%2520on%2520analysing%2520the%2520feature%2520space%2520of%2520a%2520DNN%2520by%2520proposing%2520a%250Adecoder%2520that%2520can%2520generate%2520images%2520whose%2520features%2520are%2520guaranteed%2520to%2520closely%2520match%250Aa%2520user-specified%2520feature.%2520Owing%2520to%2520this%2520guarantee%2520that%2520is%2520missed%2520in%2520past%250Astudies%252C%2520our%2520decoder%2520allows%2520us%2520to%2520evidence%2520which%2520of%2520various%2520attributes%2520in%2520an%250Aimage%2520are%2520encoded%2520into%2520a%2520feature%2520by%2520the%2520DNN%252C%2520by%2520generating%2520images%2520whose%250Afeatures%2520are%2520in%2520proximity%2520to%2520that%2520feature.%2520Our%2520decoder%2520is%2520implemented%2520as%2520a%250Aguided%2520diffusion%2520model%2520that%2520guides%2520the%2520reverse%2520image%2520generation%2520of%2520a%250Apre-trained%2520diffusion%2520model%2520to%2520minimise%2520the%2520Euclidean%2520distance%2520between%2520the%250Afeature%2520of%2520a%2520clean%2520image%2520estimated%2520at%2520each%2520step%2520and%2520the%2520user-specified%2520feature.%250AOne%2520practical%2520advantage%2520of%2520our%2520decoder%2520is%2520that%2520it%2520can%2520analyse%2520feature%2520spaces%2520of%250Adifferent%2520DNNs%2520with%2520no%2520additional%2520training%2520and%2520run%2520on%2520a%2520single%2520COTS%2520GPU.%2520The%250Aexperimental%2520results%2520targeting%2520CLIP%2527s%2520image%2520encoder%252C%2520ResNet-50%2520and%2520vision%250Atransformer%2520demonstrate%2520that%2520images%2520generated%2520by%2520our%2520decoder%2520have%2520features%250Aremarkably%2520similar%2520to%2520the%2520user-specified%2520ones%2520and%2520reveal%2520valuable%2520insights%2520into%250Athese%2520DNNs%2527%2520feature%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Space%20Analysis%20by%20Guided%20Diffusion%20Model&entry.906535625=Kimiaki%20Shirahama%20and%20Miki%20Yanobu%20and%20Kaduki%20Yamashita%20and%20Miho%20Ohsaki&entry.1292438233=%20%20One%20of%20the%20key%20issues%20in%20Deep%20Neural%20Networks%20%28DNNs%29%20is%20the%20black-box%20nature%0Aof%20their%20internal%20feature%20extraction%20process.%20Targeting%20vision-related%20domains%2C%0Athis%20paper%20focuses%20on%20analysing%20the%20feature%20space%20of%20a%20DNN%20by%20proposing%20a%0Adecoder%20that%20can%20generate%20images%20whose%20features%20are%20guaranteed%20to%20closely%20match%0Aa%20user-specified%20feature.%20Owing%20to%20this%20guarantee%20that%20is%20missed%20in%20past%0Astudies%2C%20our%20decoder%20allows%20us%20to%20evidence%20which%20of%20various%20attributes%20in%20an%0Aimage%20are%20encoded%20into%20a%20feature%20by%20the%20DNN%2C%20by%20generating%20images%20whose%0Afeatures%20are%20in%20proximity%20to%20that%20feature.%20Our%20decoder%20is%20implemented%20as%20a%0Aguided%20diffusion%20model%20that%20guides%20the%20reverse%20image%20generation%20of%20a%0Apre-trained%20diffusion%20model%20to%20minimise%20the%20Euclidean%20distance%20between%20the%0Afeature%20of%20a%20clean%20image%20estimated%20at%20each%20step%20and%20the%20user-specified%20feature.%0AOne%20practical%20advantage%20of%20our%20decoder%20is%20that%20it%20can%20analyse%20feature%20spaces%20of%0Adifferent%20DNNs%20with%20no%20additional%20training%20and%20run%20on%20a%20single%20COTS%20GPU.%20The%0Aexperimental%20results%20targeting%20CLIP%27s%20image%20encoder%2C%20ResNet-50%20and%20vision%0Atransformer%20demonstrate%20that%20images%20generated%20by%20our%20decoder%20have%20features%0Aremarkably%20similar%20to%20the%20user-specified%20ones%20and%20reveal%20valuable%20insights%20into%0Athese%20DNNs%27%20feature%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07936v1&entry.124074799=Read"},
{"title": "Robust Adaptation of Large Multimodal Models for Retrieval Augmented\n  Hateful Meme Detection", "author": "Jingbiao Mei and Jinghong Chen and Guangyu Yang and Weizhe Lin and Bill Byrne", "abstract": "  Hateful memes have become a significant concern on the Internet,\nnecessitating robust automated detection systems. While Large Multimodal Models\n(LMMs) have shown promise in hateful meme detection, they face notable\nchallenges like sub-optimal performance and limited out-of-domain\ngeneralization capabilities. Recent studies further reveal the limitations of\nboth supervised fine-tuning (SFT) and in-context learning when applied to LMMs\nin this setting. To address these issues, we propose a robust adaptation\nframework for hateful meme detection that enhances in-domain accuracy and\ncross-domain generalization while preserving the general vision-language\ncapabilities of LMMs. Analysis reveals that our approach achieves improved\nrobustness under adversarial attacks compared to SFT models. Experiments on six\nmeme classification datasets show that our approach achieves state-of-the-art\nperformance, outperforming larger agentic systems. Moreover, our method\ngenerates higher-quality rationales for explaining hateful content compared to\nstandard SFT, enhancing model interpretability. Code available at\nhttps://github.com/JingbiaoMei/RGCL\n", "link": "http://arxiv.org/abs/2502.13061v3", "date": "2025-09-09", "relevancy": 2.2876, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5839}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5751}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Adaptation%20of%20Large%20Multimodal%20Models%20for%20Retrieval%20Augmented%0A%20%20Hateful%20Meme%20Detection&body=Title%3A%20Robust%20Adaptation%20of%20Large%20Multimodal%20Models%20for%20Retrieval%20Augmented%0A%20%20Hateful%20Meme%20Detection%0AAuthor%3A%20Jingbiao%20Mei%20and%20Jinghong%20Chen%20and%20Guangyu%20Yang%20and%20Weizhe%20Lin%20and%20Bill%20Byrne%0AAbstract%3A%20%20%20Hateful%20memes%20have%20become%20a%20significant%20concern%20on%20the%20Internet%2C%0Anecessitating%20robust%20automated%20detection%20systems.%20While%20Large%20Multimodal%20Models%0A%28LMMs%29%20have%20shown%20promise%20in%20hateful%20meme%20detection%2C%20they%20face%20notable%0Achallenges%20like%20sub-optimal%20performance%20and%20limited%20out-of-domain%0Ageneralization%20capabilities.%20Recent%20studies%20further%20reveal%20the%20limitations%20of%0Aboth%20supervised%20fine-tuning%20%28SFT%29%20and%20in-context%20learning%20when%20applied%20to%20LMMs%0Ain%20this%20setting.%20To%20address%20these%20issues%2C%20we%20propose%20a%20robust%20adaptation%0Aframework%20for%20hateful%20meme%20detection%20that%20enhances%20in-domain%20accuracy%20and%0Across-domain%20generalization%20while%20preserving%20the%20general%20vision-language%0Acapabilities%20of%20LMMs.%20Analysis%20reveals%20that%20our%20approach%20achieves%20improved%0Arobustness%20under%20adversarial%20attacks%20compared%20to%20SFT%20models.%20Experiments%20on%20six%0Ameme%20classification%20datasets%20show%20that%20our%20approach%20achieves%20state-of-the-art%0Aperformance%2C%20outperforming%20larger%20agentic%20systems.%20Moreover%2C%20our%20method%0Agenerates%20higher-quality%20rationales%20for%20explaining%20hateful%20content%20compared%20to%0Astandard%20SFT%2C%20enhancing%20model%20interpretability.%20Code%20available%20at%0Ahttps%3A//github.com/JingbiaoMei/RGCL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13061v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Adaptation%2520of%2520Large%2520Multimodal%2520Models%2520for%2520Retrieval%2520Augmented%250A%2520%2520Hateful%2520Meme%2520Detection%26entry.906535625%3DJingbiao%2520Mei%2520and%2520Jinghong%2520Chen%2520and%2520Guangyu%2520Yang%2520and%2520Weizhe%2520Lin%2520and%2520Bill%2520Byrne%26entry.1292438233%3D%2520%2520Hateful%2520memes%2520have%2520become%2520a%2520significant%2520concern%2520on%2520the%2520Internet%252C%250Anecessitating%2520robust%2520automated%2520detection%2520systems.%2520While%2520Large%2520Multimodal%2520Models%250A%2528LMMs%2529%2520have%2520shown%2520promise%2520in%2520hateful%2520meme%2520detection%252C%2520they%2520face%2520notable%250Achallenges%2520like%2520sub-optimal%2520performance%2520and%2520limited%2520out-of-domain%250Ageneralization%2520capabilities.%2520Recent%2520studies%2520further%2520reveal%2520the%2520limitations%2520of%250Aboth%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520in-context%2520learning%2520when%2520applied%2520to%2520LMMs%250Ain%2520this%2520setting.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520robust%2520adaptation%250Aframework%2520for%2520hateful%2520meme%2520detection%2520that%2520enhances%2520in-domain%2520accuracy%2520and%250Across-domain%2520generalization%2520while%2520preserving%2520the%2520general%2520vision-language%250Acapabilities%2520of%2520LMMs.%2520Analysis%2520reveals%2520that%2520our%2520approach%2520achieves%2520improved%250Arobustness%2520under%2520adversarial%2520attacks%2520compared%2520to%2520SFT%2520models.%2520Experiments%2520on%2520six%250Ameme%2520classification%2520datasets%2520show%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%250Aperformance%252C%2520outperforming%2520larger%2520agentic%2520systems.%2520Moreover%252C%2520our%2520method%250Agenerates%2520higher-quality%2520rationales%2520for%2520explaining%2520hateful%2520content%2520compared%2520to%250Astandard%2520SFT%252C%2520enhancing%2520model%2520interpretability.%2520Code%2520available%2520at%250Ahttps%253A//github.com/JingbiaoMei/RGCL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13061v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Adaptation%20of%20Large%20Multimodal%20Models%20for%20Retrieval%20Augmented%0A%20%20Hateful%20Meme%20Detection&entry.906535625=Jingbiao%20Mei%20and%20Jinghong%20Chen%20and%20Guangyu%20Yang%20and%20Weizhe%20Lin%20and%20Bill%20Byrne&entry.1292438233=%20%20Hateful%20memes%20have%20become%20a%20significant%20concern%20on%20the%20Internet%2C%0Anecessitating%20robust%20automated%20detection%20systems.%20While%20Large%20Multimodal%20Models%0A%28LMMs%29%20have%20shown%20promise%20in%20hateful%20meme%20detection%2C%20they%20face%20notable%0Achallenges%20like%20sub-optimal%20performance%20and%20limited%20out-of-domain%0Ageneralization%20capabilities.%20Recent%20studies%20further%20reveal%20the%20limitations%20of%0Aboth%20supervised%20fine-tuning%20%28SFT%29%20and%20in-context%20learning%20when%20applied%20to%20LMMs%0Ain%20this%20setting.%20To%20address%20these%20issues%2C%20we%20propose%20a%20robust%20adaptation%0Aframework%20for%20hateful%20meme%20detection%20that%20enhances%20in-domain%20accuracy%20and%0Across-domain%20generalization%20while%20preserving%20the%20general%20vision-language%0Acapabilities%20of%20LMMs.%20Analysis%20reveals%20that%20our%20approach%20achieves%20improved%0Arobustness%20under%20adversarial%20attacks%20compared%20to%20SFT%20models.%20Experiments%20on%20six%0Ameme%20classification%20datasets%20show%20that%20our%20approach%20achieves%20state-of-the-art%0Aperformance%2C%20outperforming%20larger%20agentic%20systems.%20Moreover%2C%20our%20method%0Agenerates%20higher-quality%20rationales%20for%20explaining%20hateful%20content%20compared%20to%0Astandard%20SFT%2C%20enhancing%20model%20interpretability.%20Code%20available%20at%0Ahttps%3A//github.com/JingbiaoMei/RGCL%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13061v3&entry.124074799=Read"},
{"title": "A Systematic Literature Review of Retrieval-Augmented Generation:\n  Techniques, Metrics, and Challenges", "author": "Andrew Brown and Muhammad Roman and Barry Devereux", "abstract": "  This systematic review of the research literature on retrieval-augmented\ngeneration (RAG) provides a focused analysis of the most highly cited studies\npublished between 2020 and May 2025. A total of 128 articles met our inclusion\ncriteria. The records were retrieved from ACM Digital Library, IEEE Xplore,\nScopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).\nRAG couples a neural retriever with a generative language model, grounding\noutput in up-to-date, non-parametric memory while retaining the semantic\ngeneralisation stored in model weights. Guided by the PRISMA 2020 framework, we\n(i) specify explicit inclusion and exclusion criteria based on citation count\nand research questions, (ii) catalogue datasets, architectures, and evaluation\npractices, and (iii) synthesise empirical evidence on the effectiveness and\nlimitations of RAG. To mitigate citation-lag bias, we applied a lower\ncitation-count threshold to papers published in 2025 so that emerging\nbreakthroughs with naturally fewer citations were still captured. This review\nclarifies the current research landscape, highlights methodological gaps, and\ncharts priority directions for future research.\n", "link": "http://arxiv.org/abs/2508.06401v3", "date": "2025-09-09", "relevancy": 2.2855, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4611}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4562}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Systematic%20Literature%20Review%20of%20Retrieval-Augmented%20Generation%3A%0A%20%20Techniques%2C%20Metrics%2C%20and%20Challenges&body=Title%3A%20A%20Systematic%20Literature%20Review%20of%20Retrieval-Augmented%20Generation%3A%0A%20%20Techniques%2C%20Metrics%2C%20and%20Challenges%0AAuthor%3A%20Andrew%20Brown%20and%20Muhammad%20Roman%20and%20Barry%20Devereux%0AAbstract%3A%20%20%20This%20systematic%20review%20of%20the%20research%20literature%20on%20retrieval-augmented%0Ageneration%20%28RAG%29%20provides%20a%20focused%20analysis%20of%20the%20most%20highly%20cited%20studies%0Apublished%20between%202020%20and%20May%202025.%20A%20total%20of%20128%20articles%20met%20our%20inclusion%0Acriteria.%20The%20records%20were%20retrieved%20from%20ACM%20Digital%20Library%2C%20IEEE%20Xplore%2C%0AScopus%2C%20ScienceDirect%2C%20and%20the%20Digital%20Bibliography%20and%20Library%20Project%20%28DBLP%29.%0ARAG%20couples%20a%20neural%20retriever%20with%20a%20generative%20language%20model%2C%20grounding%0Aoutput%20in%20up-to-date%2C%20non-parametric%20memory%20while%20retaining%20the%20semantic%0Ageneralisation%20stored%20in%20model%20weights.%20Guided%20by%20the%20PRISMA%202020%20framework%2C%20we%0A%28i%29%20specify%20explicit%20inclusion%20and%20exclusion%20criteria%20based%20on%20citation%20count%0Aand%20research%20questions%2C%20%28ii%29%20catalogue%20datasets%2C%20architectures%2C%20and%20evaluation%0Apractices%2C%20and%20%28iii%29%20synthesise%20empirical%20evidence%20on%20the%20effectiveness%20and%0Alimitations%20of%20RAG.%20To%20mitigate%20citation-lag%20bias%2C%20we%20applied%20a%20lower%0Acitation-count%20threshold%20to%20papers%20published%20in%202025%20so%20that%20emerging%0Abreakthroughs%20with%20naturally%20fewer%20citations%20were%20still%20captured.%20This%20review%0Aclarifies%20the%20current%20research%20landscape%2C%20highlights%20methodological%20gaps%2C%20and%0Acharts%20priority%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06401v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Systematic%2520Literature%2520Review%2520of%2520Retrieval-Augmented%2520Generation%253A%250A%2520%2520Techniques%252C%2520Metrics%252C%2520and%2520Challenges%26entry.906535625%3DAndrew%2520Brown%2520and%2520Muhammad%2520Roman%2520and%2520Barry%2520Devereux%26entry.1292438233%3D%2520%2520This%2520systematic%2520review%2520of%2520the%2520research%2520literature%2520on%2520retrieval-augmented%250Ageneration%2520%2528RAG%2529%2520provides%2520a%2520focused%2520analysis%2520of%2520the%2520most%2520highly%2520cited%2520studies%250Apublished%2520between%25202020%2520and%2520May%25202025.%2520A%2520total%2520of%2520128%2520articles%2520met%2520our%2520inclusion%250Acriteria.%2520The%2520records%2520were%2520retrieved%2520from%2520ACM%2520Digital%2520Library%252C%2520IEEE%2520Xplore%252C%250AScopus%252C%2520ScienceDirect%252C%2520and%2520the%2520Digital%2520Bibliography%2520and%2520Library%2520Project%2520%2528DBLP%2529.%250ARAG%2520couples%2520a%2520neural%2520retriever%2520with%2520a%2520generative%2520language%2520model%252C%2520grounding%250Aoutput%2520in%2520up-to-date%252C%2520non-parametric%2520memory%2520while%2520retaining%2520the%2520semantic%250Ageneralisation%2520stored%2520in%2520model%2520weights.%2520Guided%2520by%2520the%2520PRISMA%25202020%2520framework%252C%2520we%250A%2528i%2529%2520specify%2520explicit%2520inclusion%2520and%2520exclusion%2520criteria%2520based%2520on%2520citation%2520count%250Aand%2520research%2520questions%252C%2520%2528ii%2529%2520catalogue%2520datasets%252C%2520architectures%252C%2520and%2520evaluation%250Apractices%252C%2520and%2520%2528iii%2529%2520synthesise%2520empirical%2520evidence%2520on%2520the%2520effectiveness%2520and%250Alimitations%2520of%2520RAG.%2520To%2520mitigate%2520citation-lag%2520bias%252C%2520we%2520applied%2520a%2520lower%250Acitation-count%2520threshold%2520to%2520papers%2520published%2520in%25202025%2520so%2520that%2520emerging%250Abreakthroughs%2520with%2520naturally%2520fewer%2520citations%2520were%2520still%2520captured.%2520This%2520review%250Aclarifies%2520the%2520current%2520research%2520landscape%252C%2520highlights%2520methodological%2520gaps%252C%2520and%250Acharts%2520priority%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06401v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Systematic%20Literature%20Review%20of%20Retrieval-Augmented%20Generation%3A%0A%20%20Techniques%2C%20Metrics%2C%20and%20Challenges&entry.906535625=Andrew%20Brown%20and%20Muhammad%20Roman%20and%20Barry%20Devereux&entry.1292438233=%20%20This%20systematic%20review%20of%20the%20research%20literature%20on%20retrieval-augmented%0Ageneration%20%28RAG%29%20provides%20a%20focused%20analysis%20of%20the%20most%20highly%20cited%20studies%0Apublished%20between%202020%20and%20May%202025.%20A%20total%20of%20128%20articles%20met%20our%20inclusion%0Acriteria.%20The%20records%20were%20retrieved%20from%20ACM%20Digital%20Library%2C%20IEEE%20Xplore%2C%0AScopus%2C%20ScienceDirect%2C%20and%20the%20Digital%20Bibliography%20and%20Library%20Project%20%28DBLP%29.%0ARAG%20couples%20a%20neural%20retriever%20with%20a%20generative%20language%20model%2C%20grounding%0Aoutput%20in%20up-to-date%2C%20non-parametric%20memory%20while%20retaining%20the%20semantic%0Ageneralisation%20stored%20in%20model%20weights.%20Guided%20by%20the%20PRISMA%202020%20framework%2C%20we%0A%28i%29%20specify%20explicit%20inclusion%20and%20exclusion%20criteria%20based%20on%20citation%20count%0Aand%20research%20questions%2C%20%28ii%29%20catalogue%20datasets%2C%20architectures%2C%20and%20evaluation%0Apractices%2C%20and%20%28iii%29%20synthesise%20empirical%20evidence%20on%20the%20effectiveness%20and%0Alimitations%20of%20RAG.%20To%20mitigate%20citation-lag%20bias%2C%20we%20applied%20a%20lower%0Acitation-count%20threshold%20to%20papers%20published%20in%202025%20so%20that%20emerging%0Abreakthroughs%20with%20naturally%20fewer%20citations%20were%20still%20captured.%20This%20review%0Aclarifies%20the%20current%20research%20landscape%2C%20highlights%20methodological%20gaps%2C%20and%0Acharts%20priority%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06401v3&entry.124074799=Read"},
{"title": "CAViAR: Critic-Augmented Video Agentic Reasoning", "author": "Sachit Menon and Ahmet Iscen and Arsha Nagrani and Tobias Weyand and Carl Vondrick and Cordelia Schmid", "abstract": "  Video understanding has seen significant progress in recent years, with\nmodels' performance on perception from short clips continuing to rise. Yet,\nmultiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show\nperformance wanes for tasks requiring complex reasoning on videos as queries\ngrow more complex and videos grow longer. In this work, we ask: can existing\nperception capabilities be leveraged to successfully perform more complex video\nreasoning? In particular, we develop a large language model agent given access\nto video modules as subagents or tools. Rather than following a fixed procedure\nto solve queries as in previous work such as Visual Programming, ViperGPT, and\nMoReVQA, the agent uses the results of each call to a module to determine\nsubsequent steps. Inspired by work in the textual reasoning domain, we\nintroduce a critic to distinguish between instances of successful and\nunsuccessful sequences from the agent. We show that the combination of our\nagent and critic achieve strong performance on the previously-mentioned\ndatasets.\n", "link": "http://arxiv.org/abs/2509.07680v1", "date": "2025-09-09", "relevancy": 2.2637, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5725}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAViAR%3A%20Critic-Augmented%20Video%20Agentic%20Reasoning&body=Title%3A%20CAViAR%3A%20Critic-Augmented%20Video%20Agentic%20Reasoning%0AAuthor%3A%20Sachit%20Menon%20and%20Ahmet%20Iscen%20and%20Arsha%20Nagrani%20and%20Tobias%20Weyand%20and%20Carl%20Vondrick%20and%20Cordelia%20Schmid%0AAbstract%3A%20%20%20Video%20understanding%20has%20seen%20significant%20progress%20in%20recent%20years%2C%20with%0Amodels%27%20performance%20on%20perception%20from%20short%20clips%20continuing%20to%20rise.%20Yet%2C%0Amultiple%20recent%20benchmarks%2C%20such%20as%20LVBench%2C%20Neptune%2C%20and%20ActivityNet-RTL%2C%20show%0Aperformance%20wanes%20for%20tasks%20requiring%20complex%20reasoning%20on%20videos%20as%20queries%0Agrow%20more%20complex%20and%20videos%20grow%20longer.%20In%20this%20work%2C%20we%20ask%3A%20can%20existing%0Aperception%20capabilities%20be%20leveraged%20to%20successfully%20perform%20more%20complex%20video%0Areasoning%3F%20In%20particular%2C%20we%20develop%20a%20large%20language%20model%20agent%20given%20access%0Ato%20video%20modules%20as%20subagents%20or%20tools.%20Rather%20than%20following%20a%20fixed%20procedure%0Ato%20solve%20queries%20as%20in%20previous%20work%20such%20as%20Visual%20Programming%2C%20ViperGPT%2C%20and%0AMoReVQA%2C%20the%20agent%20uses%20the%20results%20of%20each%20call%20to%20a%20module%20to%20determine%0Asubsequent%20steps.%20Inspired%20by%20work%20in%20the%20textual%20reasoning%20domain%2C%20we%0Aintroduce%20a%20critic%20to%20distinguish%20between%20instances%20of%20successful%20and%0Aunsuccessful%20sequences%20from%20the%20agent.%20We%20show%20that%20the%20combination%20of%20our%0Aagent%20and%20critic%20achieve%20strong%20performance%20on%20the%20previously-mentioned%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAViAR%253A%2520Critic-Augmented%2520Video%2520Agentic%2520Reasoning%26entry.906535625%3DSachit%2520Menon%2520and%2520Ahmet%2520Iscen%2520and%2520Arsha%2520Nagrani%2520and%2520Tobias%2520Weyand%2520and%2520Carl%2520Vondrick%2520and%2520Cordelia%2520Schmid%26entry.1292438233%3D%2520%2520Video%2520understanding%2520has%2520seen%2520significant%2520progress%2520in%2520recent%2520years%252C%2520with%250Amodels%2527%2520performance%2520on%2520perception%2520from%2520short%2520clips%2520continuing%2520to%2520rise.%2520Yet%252C%250Amultiple%2520recent%2520benchmarks%252C%2520such%2520as%2520LVBench%252C%2520Neptune%252C%2520and%2520ActivityNet-RTL%252C%2520show%250Aperformance%2520wanes%2520for%2520tasks%2520requiring%2520complex%2520reasoning%2520on%2520videos%2520as%2520queries%250Agrow%2520more%2520complex%2520and%2520videos%2520grow%2520longer.%2520In%2520this%2520work%252C%2520we%2520ask%253A%2520can%2520existing%250Aperception%2520capabilities%2520be%2520leveraged%2520to%2520successfully%2520perform%2520more%2520complex%2520video%250Areasoning%253F%2520In%2520particular%252C%2520we%2520develop%2520a%2520large%2520language%2520model%2520agent%2520given%2520access%250Ato%2520video%2520modules%2520as%2520subagents%2520or%2520tools.%2520Rather%2520than%2520following%2520a%2520fixed%2520procedure%250Ato%2520solve%2520queries%2520as%2520in%2520previous%2520work%2520such%2520as%2520Visual%2520Programming%252C%2520ViperGPT%252C%2520and%250AMoReVQA%252C%2520the%2520agent%2520uses%2520the%2520results%2520of%2520each%2520call%2520to%2520a%2520module%2520to%2520determine%250Asubsequent%2520steps.%2520Inspired%2520by%2520work%2520in%2520the%2520textual%2520reasoning%2520domain%252C%2520we%250Aintroduce%2520a%2520critic%2520to%2520distinguish%2520between%2520instances%2520of%2520successful%2520and%250Aunsuccessful%2520sequences%2520from%2520the%2520agent.%2520We%2520show%2520that%2520the%2520combination%2520of%2520our%250Aagent%2520and%2520critic%2520achieve%2520strong%2520performance%2520on%2520the%2520previously-mentioned%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAViAR%3A%20Critic-Augmented%20Video%20Agentic%20Reasoning&entry.906535625=Sachit%20Menon%20and%20Ahmet%20Iscen%20and%20Arsha%20Nagrani%20and%20Tobias%20Weyand%20and%20Carl%20Vondrick%20and%20Cordelia%20Schmid&entry.1292438233=%20%20Video%20understanding%20has%20seen%20significant%20progress%20in%20recent%20years%2C%20with%0Amodels%27%20performance%20on%20perception%20from%20short%20clips%20continuing%20to%20rise.%20Yet%2C%0Amultiple%20recent%20benchmarks%2C%20such%20as%20LVBench%2C%20Neptune%2C%20and%20ActivityNet-RTL%2C%20show%0Aperformance%20wanes%20for%20tasks%20requiring%20complex%20reasoning%20on%20videos%20as%20queries%0Agrow%20more%20complex%20and%20videos%20grow%20longer.%20In%20this%20work%2C%20we%20ask%3A%20can%20existing%0Aperception%20capabilities%20be%20leveraged%20to%20successfully%20perform%20more%20complex%20video%0Areasoning%3F%20In%20particular%2C%20we%20develop%20a%20large%20language%20model%20agent%20given%20access%0Ato%20video%20modules%20as%20subagents%20or%20tools.%20Rather%20than%20following%20a%20fixed%20procedure%0Ato%20solve%20queries%20as%20in%20previous%20work%20such%20as%20Visual%20Programming%2C%20ViperGPT%2C%20and%0AMoReVQA%2C%20the%20agent%20uses%20the%20results%20of%20each%20call%20to%20a%20module%20to%20determine%0Asubsequent%20steps.%20Inspired%20by%20work%20in%20the%20textual%20reasoning%20domain%2C%20we%0Aintroduce%20a%20critic%20to%20distinguish%20between%20instances%20of%20successful%20and%0Aunsuccessful%20sequences%20from%20the%20agent.%20We%20show%20that%20the%20combination%20of%20our%0Aagent%20and%20critic%20achieve%20strong%20performance%20on%20the%20previously-mentioned%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07680v1&entry.124074799=Read"},
{"title": "Getting In Contract with Large Language Models -- An Agency Theory\n  Perspective On Large Language Model Alignment", "author": "Sascha Kaltenpoth and Oliver M\u00fcller", "abstract": "  Adopting Large language models (LLMs) in organizations potentially\nrevolutionizes our lives and work. However, they can generate off-topic,\ndiscriminating, or harmful content. This AI alignment problem often stems from\nmisspecifications during the LLM adoption, unnoticed by the principal due to\nthe LLM's black-box nature. While various research disciplines investigated AI\nalignment, they neither address the information asymmetries between\norganizational adopters and black-box LLM agents nor consider organizational AI\nadoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led\nAlignment Strategy) a conceptual framework grounded in agency (contract)\ntheory, to mitigate alignment problems during organizational LLM adoption. We\nconduct a conceptual literature analysis using the organizational LLM adoption\nphases and the agency theory as concepts. Our approach results in (1) providing\nan extended literature analysis process specific to AI alignment methods during\norganizational LLM adoption and (2) providing a first LLM alignment\nproblem-solution space.\n", "link": "http://arxiv.org/abs/2509.07642v1", "date": "2025-09-09", "relevancy": 2.2571, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.45}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Getting%20In%20Contract%20with%20Large%20Language%20Models%20--%20An%20Agency%20Theory%0A%20%20Perspective%20On%20Large%20Language%20Model%20Alignment&body=Title%3A%20Getting%20In%20Contract%20with%20Large%20Language%20Models%20--%20An%20Agency%20Theory%0A%20%20Perspective%20On%20Large%20Language%20Model%20Alignment%0AAuthor%3A%20Sascha%20Kaltenpoth%20and%20Oliver%20M%C3%BCller%0AAbstract%3A%20%20%20Adopting%20Large%20language%20models%20%28LLMs%29%20in%20organizations%20potentially%0Arevolutionizes%20our%20lives%20and%20work.%20However%2C%20they%20can%20generate%20off-topic%2C%0Adiscriminating%2C%20or%20harmful%20content.%20This%20AI%20alignment%20problem%20often%20stems%20from%0Amisspecifications%20during%20the%20LLM%20adoption%2C%20unnoticed%20by%20the%20principal%20due%20to%0Athe%20LLM%27s%20black-box%20nature.%20While%20various%20research%20disciplines%20investigated%20AI%0Aalignment%2C%20they%20neither%20address%20the%20information%20asymmetries%20between%0Aorganizational%20adopters%20and%20black-box%20LLM%20agents%20nor%20consider%20organizational%20AI%0Aadoption%20processes.%20Therefore%2C%20we%20propose%20LLM%20ATLAS%20%28LLM%20Agency%20Theory-Led%0AAlignment%20Strategy%29%20a%20conceptual%20framework%20grounded%20in%20agency%20%28contract%29%0Atheory%2C%20to%20mitigate%20alignment%20problems%20during%20organizational%20LLM%20adoption.%20We%0Aconduct%20a%20conceptual%20literature%20analysis%20using%20the%20organizational%20LLM%20adoption%0Aphases%20and%20the%20agency%20theory%20as%20concepts.%20Our%20approach%20results%20in%20%281%29%20providing%0Aan%20extended%20literature%20analysis%20process%20specific%20to%20AI%20alignment%20methods%20during%0Aorganizational%20LLM%20adoption%20and%20%282%29%20providing%20a%20first%20LLM%20alignment%0Aproblem-solution%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGetting%2520In%2520Contract%2520with%2520Large%2520Language%2520Models%2520--%2520An%2520Agency%2520Theory%250A%2520%2520Perspective%2520On%2520Large%2520Language%2520Model%2520Alignment%26entry.906535625%3DSascha%2520Kaltenpoth%2520and%2520Oliver%2520M%25C3%25BCller%26entry.1292438233%3D%2520%2520Adopting%2520Large%2520language%2520models%2520%2528LLMs%2529%2520in%2520organizations%2520potentially%250Arevolutionizes%2520our%2520lives%2520and%2520work.%2520However%252C%2520they%2520can%2520generate%2520off-topic%252C%250Adiscriminating%252C%2520or%2520harmful%2520content.%2520This%2520AI%2520alignment%2520problem%2520often%2520stems%2520from%250Amisspecifications%2520during%2520the%2520LLM%2520adoption%252C%2520unnoticed%2520by%2520the%2520principal%2520due%2520to%250Athe%2520LLM%2527s%2520black-box%2520nature.%2520While%2520various%2520research%2520disciplines%2520investigated%2520AI%250Aalignment%252C%2520they%2520neither%2520address%2520the%2520information%2520asymmetries%2520between%250Aorganizational%2520adopters%2520and%2520black-box%2520LLM%2520agents%2520nor%2520consider%2520organizational%2520AI%250Aadoption%2520processes.%2520Therefore%252C%2520we%2520propose%2520LLM%2520ATLAS%2520%2528LLM%2520Agency%2520Theory-Led%250AAlignment%2520Strategy%2529%2520a%2520conceptual%2520framework%2520grounded%2520in%2520agency%2520%2528contract%2529%250Atheory%252C%2520to%2520mitigate%2520alignment%2520problems%2520during%2520organizational%2520LLM%2520adoption.%2520We%250Aconduct%2520a%2520conceptual%2520literature%2520analysis%2520using%2520the%2520organizational%2520LLM%2520adoption%250Aphases%2520and%2520the%2520agency%2520theory%2520as%2520concepts.%2520Our%2520approach%2520results%2520in%2520%25281%2529%2520providing%250Aan%2520extended%2520literature%2520analysis%2520process%2520specific%2520to%2520AI%2520alignment%2520methods%2520during%250Aorganizational%2520LLM%2520adoption%2520and%2520%25282%2529%2520providing%2520a%2520first%2520LLM%2520alignment%250Aproblem-solution%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Getting%20In%20Contract%20with%20Large%20Language%20Models%20--%20An%20Agency%20Theory%0A%20%20Perspective%20On%20Large%20Language%20Model%20Alignment&entry.906535625=Sascha%20Kaltenpoth%20and%20Oliver%20M%C3%BCller&entry.1292438233=%20%20Adopting%20Large%20language%20models%20%28LLMs%29%20in%20organizations%20potentially%0Arevolutionizes%20our%20lives%20and%20work.%20However%2C%20they%20can%20generate%20off-topic%2C%0Adiscriminating%2C%20or%20harmful%20content.%20This%20AI%20alignment%20problem%20often%20stems%20from%0Amisspecifications%20during%20the%20LLM%20adoption%2C%20unnoticed%20by%20the%20principal%20due%20to%0Athe%20LLM%27s%20black-box%20nature.%20While%20various%20research%20disciplines%20investigated%20AI%0Aalignment%2C%20they%20neither%20address%20the%20information%20asymmetries%20between%0Aorganizational%20adopters%20and%20black-box%20LLM%20agents%20nor%20consider%20organizational%20AI%0Aadoption%20processes.%20Therefore%2C%20we%20propose%20LLM%20ATLAS%20%28LLM%20Agency%20Theory-Led%0AAlignment%20Strategy%29%20a%20conceptual%20framework%20grounded%20in%20agency%20%28contract%29%0Atheory%2C%20to%20mitigate%20alignment%20problems%20during%20organizational%20LLM%20adoption.%20We%0Aconduct%20a%20conceptual%20literature%20analysis%20using%20the%20organizational%20LLM%20adoption%0Aphases%20and%20the%20agency%20theory%20as%20concepts.%20Our%20approach%20results%20in%20%281%29%20providing%0Aan%20extended%20literature%20analysis%20process%20specific%20to%20AI%20alignment%20methods%20during%0Aorganizational%20LLM%20adoption%20and%20%282%29%20providing%20a%20first%20LLM%20alignment%0Aproblem-solution%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07642v1&entry.124074799=Read"},
{"title": "Interleaving Reasoning for Better Text-to-Image Generation", "author": "Wenxuan Huang and Shuang Chen and Zheyong Xie and Shaosheng Cao and Shixiang Tang and Yufan Shen and Qingyu Yin and Wenbo Hu and Xiaoman Wang and Yuntian Tang and Junbo Qiao and Yue Guo and Yao Hu and Zhenfei Yin and Philip Torr and Yu Cheng and Wanli Ouyang and Shaohui Lin", "abstract": "  Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .\n", "link": "http://arxiv.org/abs/2509.06945v2", "date": "2025-09-09", "relevancy": 2.2554, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5737}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5618}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interleaving%20Reasoning%20for%20Better%20Text-to-Image%20Generation&body=Title%3A%20Interleaving%20Reasoning%20for%20Better%20Text-to-Image%20Generation%0AAuthor%3A%20Wenxuan%20Huang%20and%20Shuang%20Chen%20and%20Zheyong%20Xie%20and%20Shaosheng%20Cao%20and%20Shixiang%20Tang%20and%20Yufan%20Shen%20and%20Qingyu%20Yin%20and%20Wenbo%20Hu%20and%20Xiaoman%20Wang%20and%20Yuntian%20Tang%20and%20Junbo%20Qiao%20and%20Yue%20Guo%20and%20Yao%20Hu%20and%20Zhenfei%20Yin%20and%20Philip%20Torr%20and%20Yu%20Cheng%20and%20Wanli%20Ouyang%20and%20Shaohui%20Lin%0AAbstract%3A%20%20%20Unified%20multimodal%20understanding%20and%20generation%20models%20recently%20have%20achieve%0Asignificant%20improvement%20in%20image%20generation%20capability%2C%20yet%20a%20large%20gap%20remains%0Ain%20instruction%20following%20and%20detail%20preservation%20compared%20to%20systems%20that%0Atightly%20couple%20comprehension%20with%20generation%20such%20as%20GPT-4o.%20Motivated%20by%0Arecent%20advances%20in%20interleaving%20reasoning%2C%20we%20explore%20whether%20such%20reasoning%0Acan%20further%20improve%20Text-to-Image%20%28T2I%29%20generation.%20We%20introduce%20Interleaving%0AReasoning%20Generation%20%28IRG%29%2C%20a%20framework%20that%20alternates%20between%20text-based%0Athinking%20and%20image%20synthesis%3A%20the%20model%20first%20produces%20a%20text-based%20thinking%20to%0Aguide%20an%20initial%20image%2C%20then%20reflects%20on%20the%20result%20to%20refine%20fine-grained%0Adetails%2C%20visual%20quality%2C%20and%20aesthetics%20while%20preserving%20semantics.%20To%20train%0AIRG%20effectively%2C%20we%20propose%20Interleaving%20Reasoning%20Generation%20Learning%20%28IRGL%29%2C%0Awhich%20targets%20two%20sub-goals%3A%20%281%29%20strengthening%20the%20initial%20think-and-generate%0Astage%20to%20establish%20core%20content%20and%20base%20quality%2C%20and%20%282%29%20enabling%20high-quality%0Atextual%20reflection%20and%20faithful%20implementation%20of%20those%20refinements%20in%20a%0Asubsequent%20image.%20We%20curate%20IRGL-300K%2C%20a%20dataset%20organized%20into%20six%20decomposed%0Alearning%20modes%20that%20jointly%20cover%20learning%20text-based%20thinking%2C%20and%20full%0Athinking-image%20trajectories.%20Starting%20from%20a%20unified%20foundation%20model%20that%0Anatively%20emits%20interleaved%20text-image%20outputs%2C%20our%20two-stage%20training%20first%0Abuilds%20robust%20thinking%20and%20reflection%2C%20then%20efficiently%20tunes%20the%20IRG%20pipeline%0Ain%20the%20full%20thinking-image%20trajectory%20data.%20Extensive%20experiments%20show%20SoTA%0Aperformance%2C%20yielding%20absolute%20gains%20of%205-10%20points%20on%20GenEval%2C%20WISE%2C%20TIIF%2C%0AGenAI-Bench%2C%20and%20OneIG-EN%2C%20alongside%20substantial%20improvements%20in%20visual%20quality%0Aand%20fine-grained%20fidelity.%20The%20code%2C%20model%20weights%20and%20datasets%20will%20be%0Areleased%20in%3A%20https%3A//github.com/Osilly/Interleaving-Reasoning-Generation%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterleaving%2520Reasoning%2520for%2520Better%2520Text-to-Image%2520Generation%26entry.906535625%3DWenxuan%2520Huang%2520and%2520Shuang%2520Chen%2520and%2520Zheyong%2520Xie%2520and%2520Shaosheng%2520Cao%2520and%2520Shixiang%2520Tang%2520and%2520Yufan%2520Shen%2520and%2520Qingyu%2520Yin%2520and%2520Wenbo%2520Hu%2520and%2520Xiaoman%2520Wang%2520and%2520Yuntian%2520Tang%2520and%2520Junbo%2520Qiao%2520and%2520Yue%2520Guo%2520and%2520Yao%2520Hu%2520and%2520Zhenfei%2520Yin%2520and%2520Philip%2520Torr%2520and%2520Yu%2520Cheng%2520and%2520Wanli%2520Ouyang%2520and%2520Shaohui%2520Lin%26entry.1292438233%3D%2520%2520Unified%2520multimodal%2520understanding%2520and%2520generation%2520models%2520recently%2520have%2520achieve%250Asignificant%2520improvement%2520in%2520image%2520generation%2520capability%252C%2520yet%2520a%2520large%2520gap%2520remains%250Ain%2520instruction%2520following%2520and%2520detail%2520preservation%2520compared%2520to%2520systems%2520that%250Atightly%2520couple%2520comprehension%2520with%2520generation%2520such%2520as%2520GPT-4o.%2520Motivated%2520by%250Arecent%2520advances%2520in%2520interleaving%2520reasoning%252C%2520we%2520explore%2520whether%2520such%2520reasoning%250Acan%2520further%2520improve%2520Text-to-Image%2520%2528T2I%2529%2520generation.%2520We%2520introduce%2520Interleaving%250AReasoning%2520Generation%2520%2528IRG%2529%252C%2520a%2520framework%2520that%2520alternates%2520between%2520text-based%250Athinking%2520and%2520image%2520synthesis%253A%2520the%2520model%2520first%2520produces%2520a%2520text-based%2520thinking%2520to%250Aguide%2520an%2520initial%2520image%252C%2520then%2520reflects%2520on%2520the%2520result%2520to%2520refine%2520fine-grained%250Adetails%252C%2520visual%2520quality%252C%2520and%2520aesthetics%2520while%2520preserving%2520semantics.%2520To%2520train%250AIRG%2520effectively%252C%2520we%2520propose%2520Interleaving%2520Reasoning%2520Generation%2520Learning%2520%2528IRGL%2529%252C%250Awhich%2520targets%2520two%2520sub-goals%253A%2520%25281%2529%2520strengthening%2520the%2520initial%2520think-and-generate%250Astage%2520to%2520establish%2520core%2520content%2520and%2520base%2520quality%252C%2520and%2520%25282%2529%2520enabling%2520high-quality%250Atextual%2520reflection%2520and%2520faithful%2520implementation%2520of%2520those%2520refinements%2520in%2520a%250Asubsequent%2520image.%2520We%2520curate%2520IRGL-300K%252C%2520a%2520dataset%2520organized%2520into%2520six%2520decomposed%250Alearning%2520modes%2520that%2520jointly%2520cover%2520learning%2520text-based%2520thinking%252C%2520and%2520full%250Athinking-image%2520trajectories.%2520Starting%2520from%2520a%2520unified%2520foundation%2520model%2520that%250Anatively%2520emits%2520interleaved%2520text-image%2520outputs%252C%2520our%2520two-stage%2520training%2520first%250Abuilds%2520robust%2520thinking%2520and%2520reflection%252C%2520then%2520efficiently%2520tunes%2520the%2520IRG%2520pipeline%250Ain%2520the%2520full%2520thinking-image%2520trajectory%2520data.%2520Extensive%2520experiments%2520show%2520SoTA%250Aperformance%252C%2520yielding%2520absolute%2520gains%2520of%25205-10%2520points%2520on%2520GenEval%252C%2520WISE%252C%2520TIIF%252C%250AGenAI-Bench%252C%2520and%2520OneIG-EN%252C%2520alongside%2520substantial%2520improvements%2520in%2520visual%2520quality%250Aand%2520fine-grained%2520fidelity.%2520The%2520code%252C%2520model%2520weights%2520and%2520datasets%2520will%2520be%250Areleased%2520in%253A%2520https%253A//github.com/Osilly/Interleaving-Reasoning-Generation%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interleaving%20Reasoning%20for%20Better%20Text-to-Image%20Generation&entry.906535625=Wenxuan%20Huang%20and%20Shuang%20Chen%20and%20Zheyong%20Xie%20and%20Shaosheng%20Cao%20and%20Shixiang%20Tang%20and%20Yufan%20Shen%20and%20Qingyu%20Yin%20and%20Wenbo%20Hu%20and%20Xiaoman%20Wang%20and%20Yuntian%20Tang%20and%20Junbo%20Qiao%20and%20Yue%20Guo%20and%20Yao%20Hu%20and%20Zhenfei%20Yin%20and%20Philip%20Torr%20and%20Yu%20Cheng%20and%20Wanli%20Ouyang%20and%20Shaohui%20Lin&entry.1292438233=%20%20Unified%20multimodal%20understanding%20and%20generation%20models%20recently%20have%20achieve%0Asignificant%20improvement%20in%20image%20generation%20capability%2C%20yet%20a%20large%20gap%20remains%0Ain%20instruction%20following%20and%20detail%20preservation%20compared%20to%20systems%20that%0Atightly%20couple%20comprehension%20with%20generation%20such%20as%20GPT-4o.%20Motivated%20by%0Arecent%20advances%20in%20interleaving%20reasoning%2C%20we%20explore%20whether%20such%20reasoning%0Acan%20further%20improve%20Text-to-Image%20%28T2I%29%20generation.%20We%20introduce%20Interleaving%0AReasoning%20Generation%20%28IRG%29%2C%20a%20framework%20that%20alternates%20between%20text-based%0Athinking%20and%20image%20synthesis%3A%20the%20model%20first%20produces%20a%20text-based%20thinking%20to%0Aguide%20an%20initial%20image%2C%20then%20reflects%20on%20the%20result%20to%20refine%20fine-grained%0Adetails%2C%20visual%20quality%2C%20and%20aesthetics%20while%20preserving%20semantics.%20To%20train%0AIRG%20effectively%2C%20we%20propose%20Interleaving%20Reasoning%20Generation%20Learning%20%28IRGL%29%2C%0Awhich%20targets%20two%20sub-goals%3A%20%281%29%20strengthening%20the%20initial%20think-and-generate%0Astage%20to%20establish%20core%20content%20and%20base%20quality%2C%20and%20%282%29%20enabling%20high-quality%0Atextual%20reflection%20and%20faithful%20implementation%20of%20those%20refinements%20in%20a%0Asubsequent%20image.%20We%20curate%20IRGL-300K%2C%20a%20dataset%20organized%20into%20six%20decomposed%0Alearning%20modes%20that%20jointly%20cover%20learning%20text-based%20thinking%2C%20and%20full%0Athinking-image%20trajectories.%20Starting%20from%20a%20unified%20foundation%20model%20that%0Anatively%20emits%20interleaved%20text-image%20outputs%2C%20our%20two-stage%20training%20first%0Abuilds%20robust%20thinking%20and%20reflection%2C%20then%20efficiently%20tunes%20the%20IRG%20pipeline%0Ain%20the%20full%20thinking-image%20trajectory%20data.%20Extensive%20experiments%20show%20SoTA%0Aperformance%2C%20yielding%20absolute%20gains%20of%205-10%20points%20on%20GenEval%2C%20WISE%2C%20TIIF%2C%0AGenAI-Bench%2C%20and%20OneIG-EN%2C%20alongside%20substantial%20improvements%20in%20visual%20quality%0Aand%20fine-grained%20fidelity.%20The%20code%2C%20model%20weights%20and%20datasets%20will%20be%0Areleased%20in%3A%20https%3A//github.com/Osilly/Interleaving-Reasoning-Generation%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06945v2&entry.124074799=Read"},
{"title": "MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial\n  Navigation", "author": "Arup Kumar Sahoo and Itzik Klein", "abstract": "  A fundamental requirement for full autonomy in mobile robots is accurate\nnavigation even in situations where satellite navigation or cameras are\nunavailable. In such practical situations, relying only on inertial sensors\nwill result in navigation solution drift due to the sensors' inherent noise and\nerror terms. One of the emerging solutions to mitigate drift is to maneuver the\nrobot in a snake-like slithering motion to increase the inertial\nsignal-to-noise ratio, allowing the regression of the mobile robot position. In\nthis work, we propose MoRPI-PINN as a physics-informed neural network framework\nfor accurate inertial-based mobile robot navigation. By embedding physical laws\nand constraints into the training process, MoRPI-PINN is capable of providing\nan accurate and robust navigation solution. Using real-world experiments, we\nshow accuracy improvements of over 85% compared to other approaches. MoRPI-PINN\nis a lightweight approach that can be implemented even on edge devices and used\nin any typical mobile robot application.\n", "link": "http://arxiv.org/abs/2507.18206v2", "date": "2025-09-09", "relevancy": 2.2543, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5936}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5652}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoRPI-PINN%3A%20A%20Physics-Informed%20Framework%20for%20Mobile%20Robot%20Pure%20Inertial%0A%20%20Navigation&body=Title%3A%20MoRPI-PINN%3A%20A%20Physics-Informed%20Framework%20for%20Mobile%20Robot%20Pure%20Inertial%0A%20%20Navigation%0AAuthor%3A%20Arup%20Kumar%20Sahoo%20and%20Itzik%20Klein%0AAbstract%3A%20%20%20A%20fundamental%20requirement%20for%20full%20autonomy%20in%20mobile%20robots%20is%20accurate%0Anavigation%20even%20in%20situations%20where%20satellite%20navigation%20or%20cameras%20are%0Aunavailable.%20In%20such%20practical%20situations%2C%20relying%20only%20on%20inertial%20sensors%0Awill%20result%20in%20navigation%20solution%20drift%20due%20to%20the%20sensors%27%20inherent%20noise%20and%0Aerror%20terms.%20One%20of%20the%20emerging%20solutions%20to%20mitigate%20drift%20is%20to%20maneuver%20the%0Arobot%20in%20a%20snake-like%20slithering%20motion%20to%20increase%20the%20inertial%0Asignal-to-noise%20ratio%2C%20allowing%20the%20regression%20of%20the%20mobile%20robot%20position.%20In%0Athis%20work%2C%20we%20propose%20MoRPI-PINN%20as%20a%20physics-informed%20neural%20network%20framework%0Afor%20accurate%20inertial-based%20mobile%20robot%20navigation.%20By%20embedding%20physical%20laws%0Aand%20constraints%20into%20the%20training%20process%2C%20MoRPI-PINN%20is%20capable%20of%20providing%0Aan%20accurate%20and%20robust%20navigation%20solution.%20Using%20real-world%20experiments%2C%20we%0Ashow%20accuracy%20improvements%20of%20over%2085%25%20compared%20to%20other%20approaches.%20MoRPI-PINN%0Ais%20a%20lightweight%20approach%20that%20can%20be%20implemented%20even%20on%20edge%20devices%20and%20used%0Ain%20any%20typical%20mobile%20robot%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18206v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoRPI-PINN%253A%2520A%2520Physics-Informed%2520Framework%2520for%2520Mobile%2520Robot%2520Pure%2520Inertial%250A%2520%2520Navigation%26entry.906535625%3DArup%2520Kumar%2520Sahoo%2520and%2520Itzik%2520Klein%26entry.1292438233%3D%2520%2520A%2520fundamental%2520requirement%2520for%2520full%2520autonomy%2520in%2520mobile%2520robots%2520is%2520accurate%250Anavigation%2520even%2520in%2520situations%2520where%2520satellite%2520navigation%2520or%2520cameras%2520are%250Aunavailable.%2520In%2520such%2520practical%2520situations%252C%2520relying%2520only%2520on%2520inertial%2520sensors%250Awill%2520result%2520in%2520navigation%2520solution%2520drift%2520due%2520to%2520the%2520sensors%2527%2520inherent%2520noise%2520and%250Aerror%2520terms.%2520One%2520of%2520the%2520emerging%2520solutions%2520to%2520mitigate%2520drift%2520is%2520to%2520maneuver%2520the%250Arobot%2520in%2520a%2520snake-like%2520slithering%2520motion%2520to%2520increase%2520the%2520inertial%250Asignal-to-noise%2520ratio%252C%2520allowing%2520the%2520regression%2520of%2520the%2520mobile%2520robot%2520position.%2520In%250Athis%2520work%252C%2520we%2520propose%2520MoRPI-PINN%2520as%2520a%2520physics-informed%2520neural%2520network%2520framework%250Afor%2520accurate%2520inertial-based%2520mobile%2520robot%2520navigation.%2520By%2520embedding%2520physical%2520laws%250Aand%2520constraints%2520into%2520the%2520training%2520process%252C%2520MoRPI-PINN%2520is%2520capable%2520of%2520providing%250Aan%2520accurate%2520and%2520robust%2520navigation%2520solution.%2520Using%2520real-world%2520experiments%252C%2520we%250Ashow%2520accuracy%2520improvements%2520of%2520over%252085%2525%2520compared%2520to%2520other%2520approaches.%2520MoRPI-PINN%250Ais%2520a%2520lightweight%2520approach%2520that%2520can%2520be%2520implemented%2520even%2520on%2520edge%2520devices%2520and%2520used%250Ain%2520any%2520typical%2520mobile%2520robot%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18206v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoRPI-PINN%3A%20A%20Physics-Informed%20Framework%20for%20Mobile%20Robot%20Pure%20Inertial%0A%20%20Navigation&entry.906535625=Arup%20Kumar%20Sahoo%20and%20Itzik%20Klein&entry.1292438233=%20%20A%20fundamental%20requirement%20for%20full%20autonomy%20in%20mobile%20robots%20is%20accurate%0Anavigation%20even%20in%20situations%20where%20satellite%20navigation%20or%20cameras%20are%0Aunavailable.%20In%20such%20practical%20situations%2C%20relying%20only%20on%20inertial%20sensors%0Awill%20result%20in%20navigation%20solution%20drift%20due%20to%20the%20sensors%27%20inherent%20noise%20and%0Aerror%20terms.%20One%20of%20the%20emerging%20solutions%20to%20mitigate%20drift%20is%20to%20maneuver%20the%0Arobot%20in%20a%20snake-like%20slithering%20motion%20to%20increase%20the%20inertial%0Asignal-to-noise%20ratio%2C%20allowing%20the%20regression%20of%20the%20mobile%20robot%20position.%20In%0Athis%20work%2C%20we%20propose%20MoRPI-PINN%20as%20a%20physics-informed%20neural%20network%20framework%0Afor%20accurate%20inertial-based%20mobile%20robot%20navigation.%20By%20embedding%20physical%20laws%0Aand%20constraints%20into%20the%20training%20process%2C%20MoRPI-PINN%20is%20capable%20of%20providing%0Aan%20accurate%20and%20robust%20navigation%20solution.%20Using%20real-world%20experiments%2C%20we%0Ashow%20accuracy%20improvements%20of%20over%2085%25%20compared%20to%20other%20approaches.%20MoRPI-PINN%0Ais%20a%20lightweight%20approach%20that%20can%20be%20implemented%20even%20on%20edge%20devices%20and%20used%0Ain%20any%20typical%20mobile%20robot%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18206v2&entry.124074799=Read"},
{"title": "Audio-centric Video Understanding Benchmark without Text Shortcut", "author": "Yudong Yang and Jimin Zhuang and Guangzhi Sun and Changli Tang and Yixuan Li and Peihan Li and Yifan Jiang and Wei Li and Zejun Ma and Chao Zhang", "abstract": "  Audio often serves as an auxiliary modality in video understanding tasks of\naudio-visual large language models (LLMs), merely assisting in the\ncomprehension of visual information. However, a thorough understanding of\nvideos significantly depends on auditory information, as audio offers critical\ncontext, emotional cues, and semantic meaning that visual data alone often\nlacks. This paper proposes an audio-centric video understanding benchmark\n(AVUT) to evaluate the video comprehension capabilities of multimodal LLMs with\na particular focus on auditory information. AVUT introduces a suite of\ncarefully designed audio-centric tasks, holistically testing the understanding\nof both audio content and audio-visual interactions in videos. Moreover, this\nwork points out the text shortcut problem that largely exists in other\nbenchmarks where the correct answer can be found from question text alone\nwithout needing videos. AVUT addresses this problem by proposing a answer\npermutation-based filtering mechanism. A thorough evaluation across a diverse\nrange of open-source and proprietary multimodal LLMs is performed, followed by\nthe analyses of deficiencies in audio-visual LLMs. Demos and data are available\nat https://github.com/lark-png/AVUT.\n", "link": "http://arxiv.org/abs/2503.19951v2", "date": "2025-09-09", "relevancy": 2.2511, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5709}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5709}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio-centric%20Video%20Understanding%20Benchmark%20without%20Text%20Shortcut&body=Title%3A%20Audio-centric%20Video%20Understanding%20Benchmark%20without%20Text%20Shortcut%0AAuthor%3A%20Yudong%20Yang%20and%20Jimin%20Zhuang%20and%20Guangzhi%20Sun%20and%20Changli%20Tang%20and%20Yixuan%20Li%20and%20Peihan%20Li%20and%20Yifan%20Jiang%20and%20Wei%20Li%20and%20Zejun%20Ma%20and%20Chao%20Zhang%0AAbstract%3A%20%20%20Audio%20often%20serves%20as%20an%20auxiliary%20modality%20in%20video%20understanding%20tasks%20of%0Aaudio-visual%20large%20language%20models%20%28LLMs%29%2C%20merely%20assisting%20in%20the%0Acomprehension%20of%20visual%20information.%20However%2C%20a%20thorough%20understanding%20of%0Avideos%20significantly%20depends%20on%20auditory%20information%2C%20as%20audio%20offers%20critical%0Acontext%2C%20emotional%20cues%2C%20and%20semantic%20meaning%20that%20visual%20data%20alone%20often%0Alacks.%20This%20paper%20proposes%20an%20audio-centric%20video%20understanding%20benchmark%0A%28AVUT%29%20to%20evaluate%20the%20video%20comprehension%20capabilities%20of%20multimodal%20LLMs%20with%0Aa%20particular%20focus%20on%20auditory%20information.%20AVUT%20introduces%20a%20suite%20of%0Acarefully%20designed%20audio-centric%20tasks%2C%20holistically%20testing%20the%20understanding%0Aof%20both%20audio%20content%20and%20audio-visual%20interactions%20in%20videos.%20Moreover%2C%20this%0Awork%20points%20out%20the%20text%20shortcut%20problem%20that%20largely%20exists%20in%20other%0Abenchmarks%20where%20the%20correct%20answer%20can%20be%20found%20from%20question%20text%20alone%0Awithout%20needing%20videos.%20AVUT%20addresses%20this%20problem%20by%20proposing%20a%20answer%0Apermutation-based%20filtering%20mechanism.%20A%20thorough%20evaluation%20across%20a%20diverse%0Arange%20of%20open-source%20and%20proprietary%20multimodal%20LLMs%20is%20performed%2C%20followed%20by%0Athe%20analyses%20of%20deficiencies%20in%20audio-visual%20LLMs.%20Demos%20and%20data%20are%20available%0Aat%20https%3A//github.com/lark-png/AVUT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.19951v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio-centric%2520Video%2520Understanding%2520Benchmark%2520without%2520Text%2520Shortcut%26entry.906535625%3DYudong%2520Yang%2520and%2520Jimin%2520Zhuang%2520and%2520Guangzhi%2520Sun%2520and%2520Changli%2520Tang%2520and%2520Yixuan%2520Li%2520and%2520Peihan%2520Li%2520and%2520Yifan%2520Jiang%2520and%2520Wei%2520Li%2520and%2520Zejun%2520Ma%2520and%2520Chao%2520Zhang%26entry.1292438233%3D%2520%2520Audio%2520often%2520serves%2520as%2520an%2520auxiliary%2520modality%2520in%2520video%2520understanding%2520tasks%2520of%250Aaudio-visual%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520merely%2520assisting%2520in%2520the%250Acomprehension%2520of%2520visual%2520information.%2520However%252C%2520a%2520thorough%2520understanding%2520of%250Avideos%2520significantly%2520depends%2520on%2520auditory%2520information%252C%2520as%2520audio%2520offers%2520critical%250Acontext%252C%2520emotional%2520cues%252C%2520and%2520semantic%2520meaning%2520that%2520visual%2520data%2520alone%2520often%250Alacks.%2520This%2520paper%2520proposes%2520an%2520audio-centric%2520video%2520understanding%2520benchmark%250A%2528AVUT%2529%2520to%2520evaluate%2520the%2520video%2520comprehension%2520capabilities%2520of%2520multimodal%2520LLMs%2520with%250Aa%2520particular%2520focus%2520on%2520auditory%2520information.%2520AVUT%2520introduces%2520a%2520suite%2520of%250Acarefully%2520designed%2520audio-centric%2520tasks%252C%2520holistically%2520testing%2520the%2520understanding%250Aof%2520both%2520audio%2520content%2520and%2520audio-visual%2520interactions%2520in%2520videos.%2520Moreover%252C%2520this%250Awork%2520points%2520out%2520the%2520text%2520shortcut%2520problem%2520that%2520largely%2520exists%2520in%2520other%250Abenchmarks%2520where%2520the%2520correct%2520answer%2520can%2520be%2520found%2520from%2520question%2520text%2520alone%250Awithout%2520needing%2520videos.%2520AVUT%2520addresses%2520this%2520problem%2520by%2520proposing%2520a%2520answer%250Apermutation-based%2520filtering%2520mechanism.%2520A%2520thorough%2520evaluation%2520across%2520a%2520diverse%250Arange%2520of%2520open-source%2520and%2520proprietary%2520multimodal%2520LLMs%2520is%2520performed%252C%2520followed%2520by%250Athe%2520analyses%2520of%2520deficiencies%2520in%2520audio-visual%2520LLMs.%2520Demos%2520and%2520data%2520are%2520available%250Aat%2520https%253A//github.com/lark-png/AVUT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19951v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-centric%20Video%20Understanding%20Benchmark%20without%20Text%20Shortcut&entry.906535625=Yudong%20Yang%20and%20Jimin%20Zhuang%20and%20Guangzhi%20Sun%20and%20Changli%20Tang%20and%20Yixuan%20Li%20and%20Peihan%20Li%20and%20Yifan%20Jiang%20and%20Wei%20Li%20and%20Zejun%20Ma%20and%20Chao%20Zhang&entry.1292438233=%20%20Audio%20often%20serves%20as%20an%20auxiliary%20modality%20in%20video%20understanding%20tasks%20of%0Aaudio-visual%20large%20language%20models%20%28LLMs%29%2C%20merely%20assisting%20in%20the%0Acomprehension%20of%20visual%20information.%20However%2C%20a%20thorough%20understanding%20of%0Avideos%20significantly%20depends%20on%20auditory%20information%2C%20as%20audio%20offers%20critical%0Acontext%2C%20emotional%20cues%2C%20and%20semantic%20meaning%20that%20visual%20data%20alone%20often%0Alacks.%20This%20paper%20proposes%20an%20audio-centric%20video%20understanding%20benchmark%0A%28AVUT%29%20to%20evaluate%20the%20video%20comprehension%20capabilities%20of%20multimodal%20LLMs%20with%0Aa%20particular%20focus%20on%20auditory%20information.%20AVUT%20introduces%20a%20suite%20of%0Acarefully%20designed%20audio-centric%20tasks%2C%20holistically%20testing%20the%20understanding%0Aof%20both%20audio%20content%20and%20audio-visual%20interactions%20in%20videos.%20Moreover%2C%20this%0Awork%20points%20out%20the%20text%20shortcut%20problem%20that%20largely%20exists%20in%20other%0Abenchmarks%20where%20the%20correct%20answer%20can%20be%20found%20from%20question%20text%20alone%0Awithout%20needing%20videos.%20AVUT%20addresses%20this%20problem%20by%20proposing%20a%20answer%0Apermutation-based%20filtering%20mechanism.%20A%20thorough%20evaluation%20across%20a%20diverse%0Arange%20of%20open-source%20and%20proprietary%20multimodal%20LLMs%20is%20performed%2C%20followed%20by%0Athe%20analyses%20of%20deficiencies%20in%20audio-visual%20LLMs.%20Demos%20and%20data%20are%20available%0Aat%20https%3A//github.com/lark-png/AVUT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.19951v2&entry.124074799=Read"},
{"title": "Beyond Motion Cues and Structural Sparsity: Revisiting Small Moving\n  Target Detection", "author": "Guoyi Zhang and Siyang Chen and Guangsheng Xu and Zhihua Shen and Han Wang and Xiaohu Zhang", "abstract": "  Small moving target detection is crucial for many defense applications but\nremains highly challenging due to low signal-to-noise ratios, ambiguous visual\ncues, and cluttered backgrounds. In this work, we propose a novel deep learning\nframework that differs fundamentally from existing approaches, which often rely\non target-specific features or motion cues and tend to lack robustness in\ncomplex environments. Our key insight is that small target detection and\nbackground discrimination are inherently coupled, even cluttered video\nbackgrounds often exhibit strong low-rank structures that can serve as stable\npriors for detection. We reformulate the task as a tensor-based low-rank and\nsparse decomposition problem and conduct a theoretical analysis of the\nbackground, target, and noise components to guide model design. Building on\nthese insights, we introduce TenRPCANet, a deep neural network that requires\nminimal assumptions about target characteristics. Specifically, we propose a\ntokenization strategy that implicitly enforces multi-order tensor low-rank\npriors through a self-attention mechanism. This mechanism captures both local\nand non-local self-similarity to model the low-rank background without relying\non explicit iterative optimization. In addition, inspired by the sparse\ncomponent update in tensor RPCA, we design a feature refinement module to\nenhance target saliency. The proposed method achieves state-of-the-art\nperformance on two highly distinct and challenging tasks: multi-frame infrared\nsmall target detection and space object detection. These results demonstrate\nboth the effectiveness and the generalizability of our approach.\n", "link": "http://arxiv.org/abs/2509.07654v1", "date": "2025-09-09", "relevancy": 2.2467, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.57}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5598}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Motion%20Cues%20and%20Structural%20Sparsity%3A%20Revisiting%20Small%20Moving%0A%20%20Target%20Detection&body=Title%3A%20Beyond%20Motion%20Cues%20and%20Structural%20Sparsity%3A%20Revisiting%20Small%20Moving%0A%20%20Target%20Detection%0AAuthor%3A%20Guoyi%20Zhang%20and%20Siyang%20Chen%20and%20Guangsheng%20Xu%20and%20Zhihua%20Shen%20and%20Han%20Wang%20and%20Xiaohu%20Zhang%0AAbstract%3A%20%20%20Small%20moving%20target%20detection%20is%20crucial%20for%20many%20defense%20applications%20but%0Aremains%20highly%20challenging%20due%20to%20low%20signal-to-noise%20ratios%2C%20ambiguous%20visual%0Acues%2C%20and%20cluttered%20backgrounds.%20In%20this%20work%2C%20we%20propose%20a%20novel%20deep%20learning%0Aframework%20that%20differs%20fundamentally%20from%20existing%20approaches%2C%20which%20often%20rely%0Aon%20target-specific%20features%20or%20motion%20cues%20and%20tend%20to%20lack%20robustness%20in%0Acomplex%20environments.%20Our%20key%20insight%20is%20that%20small%20target%20detection%20and%0Abackground%20discrimination%20are%20inherently%20coupled%2C%20even%20cluttered%20video%0Abackgrounds%20often%20exhibit%20strong%20low-rank%20structures%20that%20can%20serve%20as%20stable%0Apriors%20for%20detection.%20We%20reformulate%20the%20task%20as%20a%20tensor-based%20low-rank%20and%0Asparse%20decomposition%20problem%20and%20conduct%20a%20theoretical%20analysis%20of%20the%0Abackground%2C%20target%2C%20and%20noise%20components%20to%20guide%20model%20design.%20Building%20on%0Athese%20insights%2C%20we%20introduce%20TenRPCANet%2C%20a%20deep%20neural%20network%20that%20requires%0Aminimal%20assumptions%20about%20target%20characteristics.%20Specifically%2C%20we%20propose%20a%0Atokenization%20strategy%20that%20implicitly%20enforces%20multi-order%20tensor%20low-rank%0Apriors%20through%20a%20self-attention%20mechanism.%20This%20mechanism%20captures%20both%20local%0Aand%20non-local%20self-similarity%20to%20model%20the%20low-rank%20background%20without%20relying%0Aon%20explicit%20iterative%20optimization.%20In%20addition%2C%20inspired%20by%20the%20sparse%0Acomponent%20update%20in%20tensor%20RPCA%2C%20we%20design%20a%20feature%20refinement%20module%20to%0Aenhance%20target%20saliency.%20The%20proposed%20method%20achieves%20state-of-the-art%0Aperformance%20on%20two%20highly%20distinct%20and%20challenging%20tasks%3A%20multi-frame%20infrared%0Asmall%20target%20detection%20and%20space%20object%20detection.%20These%20results%20demonstrate%0Aboth%20the%20effectiveness%20and%20the%20generalizability%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Motion%2520Cues%2520and%2520Structural%2520Sparsity%253A%2520Revisiting%2520Small%2520Moving%250A%2520%2520Target%2520Detection%26entry.906535625%3DGuoyi%2520Zhang%2520and%2520Siyang%2520Chen%2520and%2520Guangsheng%2520Xu%2520and%2520Zhihua%2520Shen%2520and%2520Han%2520Wang%2520and%2520Xiaohu%2520Zhang%26entry.1292438233%3D%2520%2520Small%2520moving%2520target%2520detection%2520is%2520crucial%2520for%2520many%2520defense%2520applications%2520but%250Aremains%2520highly%2520challenging%2520due%2520to%2520low%2520signal-to-noise%2520ratios%252C%2520ambiguous%2520visual%250Acues%252C%2520and%2520cluttered%2520backgrounds.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520deep%2520learning%250Aframework%2520that%2520differs%2520fundamentally%2520from%2520existing%2520approaches%252C%2520which%2520often%2520rely%250Aon%2520target-specific%2520features%2520or%2520motion%2520cues%2520and%2520tend%2520to%2520lack%2520robustness%2520in%250Acomplex%2520environments.%2520Our%2520key%2520insight%2520is%2520that%2520small%2520target%2520detection%2520and%250Abackground%2520discrimination%2520are%2520inherently%2520coupled%252C%2520even%2520cluttered%2520video%250Abackgrounds%2520often%2520exhibit%2520strong%2520low-rank%2520structures%2520that%2520can%2520serve%2520as%2520stable%250Apriors%2520for%2520detection.%2520We%2520reformulate%2520the%2520task%2520as%2520a%2520tensor-based%2520low-rank%2520and%250Asparse%2520decomposition%2520problem%2520and%2520conduct%2520a%2520theoretical%2520analysis%2520of%2520the%250Abackground%252C%2520target%252C%2520and%2520noise%2520components%2520to%2520guide%2520model%2520design.%2520Building%2520on%250Athese%2520insights%252C%2520we%2520introduce%2520TenRPCANet%252C%2520a%2520deep%2520neural%2520network%2520that%2520requires%250Aminimal%2520assumptions%2520about%2520target%2520characteristics.%2520Specifically%252C%2520we%2520propose%2520a%250Atokenization%2520strategy%2520that%2520implicitly%2520enforces%2520multi-order%2520tensor%2520low-rank%250Apriors%2520through%2520a%2520self-attention%2520mechanism.%2520This%2520mechanism%2520captures%2520both%2520local%250Aand%2520non-local%2520self-similarity%2520to%2520model%2520the%2520low-rank%2520background%2520without%2520relying%250Aon%2520explicit%2520iterative%2520optimization.%2520In%2520addition%252C%2520inspired%2520by%2520the%2520sparse%250Acomponent%2520update%2520in%2520tensor%2520RPCA%252C%2520we%2520design%2520a%2520feature%2520refinement%2520module%2520to%250Aenhance%2520target%2520saliency.%2520The%2520proposed%2520method%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520two%2520highly%2520distinct%2520and%2520challenging%2520tasks%253A%2520multi-frame%2520infrared%250Asmall%2520target%2520detection%2520and%2520space%2520object%2520detection.%2520These%2520results%2520demonstrate%250Aboth%2520the%2520effectiveness%2520and%2520the%2520generalizability%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Motion%20Cues%20and%20Structural%20Sparsity%3A%20Revisiting%20Small%20Moving%0A%20%20Target%20Detection&entry.906535625=Guoyi%20Zhang%20and%20Siyang%20Chen%20and%20Guangsheng%20Xu%20and%20Zhihua%20Shen%20and%20Han%20Wang%20and%20Xiaohu%20Zhang&entry.1292438233=%20%20Small%20moving%20target%20detection%20is%20crucial%20for%20many%20defense%20applications%20but%0Aremains%20highly%20challenging%20due%20to%20low%20signal-to-noise%20ratios%2C%20ambiguous%20visual%0Acues%2C%20and%20cluttered%20backgrounds.%20In%20this%20work%2C%20we%20propose%20a%20novel%20deep%20learning%0Aframework%20that%20differs%20fundamentally%20from%20existing%20approaches%2C%20which%20often%20rely%0Aon%20target-specific%20features%20or%20motion%20cues%20and%20tend%20to%20lack%20robustness%20in%0Acomplex%20environments.%20Our%20key%20insight%20is%20that%20small%20target%20detection%20and%0Abackground%20discrimination%20are%20inherently%20coupled%2C%20even%20cluttered%20video%0Abackgrounds%20often%20exhibit%20strong%20low-rank%20structures%20that%20can%20serve%20as%20stable%0Apriors%20for%20detection.%20We%20reformulate%20the%20task%20as%20a%20tensor-based%20low-rank%20and%0Asparse%20decomposition%20problem%20and%20conduct%20a%20theoretical%20analysis%20of%20the%0Abackground%2C%20target%2C%20and%20noise%20components%20to%20guide%20model%20design.%20Building%20on%0Athese%20insights%2C%20we%20introduce%20TenRPCANet%2C%20a%20deep%20neural%20network%20that%20requires%0Aminimal%20assumptions%20about%20target%20characteristics.%20Specifically%2C%20we%20propose%20a%0Atokenization%20strategy%20that%20implicitly%20enforces%20multi-order%20tensor%20low-rank%0Apriors%20through%20a%20self-attention%20mechanism.%20This%20mechanism%20captures%20both%20local%0Aand%20non-local%20self-similarity%20to%20model%20the%20low-rank%20background%20without%20relying%0Aon%20explicit%20iterative%20optimization.%20In%20addition%2C%20inspired%20by%20the%20sparse%0Acomponent%20update%20in%20tensor%20RPCA%2C%20we%20design%20a%20feature%20refinement%20module%20to%0Aenhance%20target%20saliency.%20The%20proposed%20method%20achieves%20state-of-the-art%0Aperformance%20on%20two%20highly%20distinct%20and%20challenging%20tasks%3A%20multi-frame%20infrared%0Asmall%20target%20detection%20and%20space%20object%20detection.%20These%20results%20demonstrate%0Aboth%20the%20effectiveness%20and%20the%20generalizability%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07654v1&entry.124074799=Read"},
{"title": "Light-Weight Cross-Modal Enhancement Method with Benchmark Construction\n  for UAV-based Open-Vocabulary Object Detection", "author": "Zhenhai Weng and Xinjie Li and Can Wu and Weijie He and Jianfeng Lv and Dong Zhou and Zhongliang Yu", "abstract": "  Open-Vocabulary Object Detection (OVD) faces severe performance degradation\nwhen applied to UAV imagery due to the domain gap from ground-level datasets.\nTo address this challenge, we propose a complete UAV-oriented solution that\ncombines both dataset construction and model innovation. First, we design a\nrefined UAV-Label Engine, which efficiently resolves annotation redundancy,\ninconsistency, and ambiguity, enabling the generation of largescale UAV\ndatasets. Based on this engine, we construct two new benchmarks: UAVDE-2M, with\nover 2.4M instances across 1,800+ categories, and UAVCAP-15K, providing rich\nimage-text pairs for vision-language pretraining. Second, we introduce the\nCross-Attention Gated Enhancement (CAGE) module, a lightweight dual-path fusion\ndesign that integrates cross-attention, adaptive gating, and global FiLM\nmodulation for robust textvision alignment. By embedding CAGE into the\nYOLO-World-v2 framework, our method achieves significant gains in both accuracy\nand efficiency, notably improving zero-shot detection on VisDrone by +5.3 mAP\nwhile reducing parameters and GFLOPs, and demonstrating strong cross-domain\ngeneralization on SIMD. Extensive experiments and real-world UAV deployment\nconfirm the effectiveness and practicality of our proposed solution for\nUAV-based OVD\n", "link": "http://arxiv.org/abs/2509.06011v2", "date": "2025-09-09", "relevancy": 2.2448, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5724}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5621}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Light-Weight%20Cross-Modal%20Enhancement%20Method%20with%20Benchmark%20Construction%0A%20%20for%20UAV-based%20Open-Vocabulary%20Object%20Detection&body=Title%3A%20Light-Weight%20Cross-Modal%20Enhancement%20Method%20with%20Benchmark%20Construction%0A%20%20for%20UAV-based%20Open-Vocabulary%20Object%20Detection%0AAuthor%3A%20Zhenhai%20Weng%20and%20Xinjie%20Li%20and%20Can%20Wu%20and%20Weijie%20He%20and%20Jianfeng%20Lv%20and%20Dong%20Zhou%20and%20Zhongliang%20Yu%0AAbstract%3A%20%20%20Open-Vocabulary%20Object%20Detection%20%28OVD%29%20faces%20severe%20performance%20degradation%0Awhen%20applied%20to%20UAV%20imagery%20due%20to%20the%20domain%20gap%20from%20ground-level%20datasets.%0ATo%20address%20this%20challenge%2C%20we%20propose%20a%20complete%20UAV-oriented%20solution%20that%0Acombines%20both%20dataset%20construction%20and%20model%20innovation.%20First%2C%20we%20design%20a%0Arefined%20UAV-Label%20Engine%2C%20which%20efficiently%20resolves%20annotation%20redundancy%2C%0Ainconsistency%2C%20and%20ambiguity%2C%20enabling%20the%20generation%20of%20largescale%20UAV%0Adatasets.%20Based%20on%20this%20engine%2C%20we%20construct%20two%20new%20benchmarks%3A%20UAVDE-2M%2C%20with%0Aover%202.4M%20instances%20across%201%2C800%2B%20categories%2C%20and%20UAVCAP-15K%2C%20providing%20rich%0Aimage-text%20pairs%20for%20vision-language%20pretraining.%20Second%2C%20we%20introduce%20the%0ACross-Attention%20Gated%20Enhancement%20%28CAGE%29%20module%2C%20a%20lightweight%20dual-path%20fusion%0Adesign%20that%20integrates%20cross-attention%2C%20adaptive%20gating%2C%20and%20global%20FiLM%0Amodulation%20for%20robust%20textvision%20alignment.%20By%20embedding%20CAGE%20into%20the%0AYOLO-World-v2%20framework%2C%20our%20method%20achieves%20significant%20gains%20in%20both%20accuracy%0Aand%20efficiency%2C%20notably%20improving%20zero-shot%20detection%20on%20VisDrone%20by%20%2B5.3%20mAP%0Awhile%20reducing%20parameters%20and%20GFLOPs%2C%20and%20demonstrating%20strong%20cross-domain%0Ageneralization%20on%20SIMD.%20Extensive%20experiments%20and%20real-world%20UAV%20deployment%0Aconfirm%20the%20effectiveness%20and%20practicality%20of%20our%20proposed%20solution%20for%0AUAV-based%20OVD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06011v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLight-Weight%2520Cross-Modal%2520Enhancement%2520Method%2520with%2520Benchmark%2520Construction%250A%2520%2520for%2520UAV-based%2520Open-Vocabulary%2520Object%2520Detection%26entry.906535625%3DZhenhai%2520Weng%2520and%2520Xinjie%2520Li%2520and%2520Can%2520Wu%2520and%2520Weijie%2520He%2520and%2520Jianfeng%2520Lv%2520and%2520Dong%2520Zhou%2520and%2520Zhongliang%2520Yu%26entry.1292438233%3D%2520%2520Open-Vocabulary%2520Object%2520Detection%2520%2528OVD%2529%2520faces%2520severe%2520performance%2520degradation%250Awhen%2520applied%2520to%2520UAV%2520imagery%2520due%2520to%2520the%2520domain%2520gap%2520from%2520ground-level%2520datasets.%250ATo%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520complete%2520UAV-oriented%2520solution%2520that%250Acombines%2520both%2520dataset%2520construction%2520and%2520model%2520innovation.%2520First%252C%2520we%2520design%2520a%250Arefined%2520UAV-Label%2520Engine%252C%2520which%2520efficiently%2520resolves%2520annotation%2520redundancy%252C%250Ainconsistency%252C%2520and%2520ambiguity%252C%2520enabling%2520the%2520generation%2520of%2520largescale%2520UAV%250Adatasets.%2520Based%2520on%2520this%2520engine%252C%2520we%2520construct%2520two%2520new%2520benchmarks%253A%2520UAVDE-2M%252C%2520with%250Aover%25202.4M%2520instances%2520across%25201%252C800%252B%2520categories%252C%2520and%2520UAVCAP-15K%252C%2520providing%2520rich%250Aimage-text%2520pairs%2520for%2520vision-language%2520pretraining.%2520Second%252C%2520we%2520introduce%2520the%250ACross-Attention%2520Gated%2520Enhancement%2520%2528CAGE%2529%2520module%252C%2520a%2520lightweight%2520dual-path%2520fusion%250Adesign%2520that%2520integrates%2520cross-attention%252C%2520adaptive%2520gating%252C%2520and%2520global%2520FiLM%250Amodulation%2520for%2520robust%2520textvision%2520alignment.%2520By%2520embedding%2520CAGE%2520into%2520the%250AYOLO-World-v2%2520framework%252C%2520our%2520method%2520achieves%2520significant%2520gains%2520in%2520both%2520accuracy%250Aand%2520efficiency%252C%2520notably%2520improving%2520zero-shot%2520detection%2520on%2520VisDrone%2520by%2520%252B5.3%2520mAP%250Awhile%2520reducing%2520parameters%2520and%2520GFLOPs%252C%2520and%2520demonstrating%2520strong%2520cross-domain%250Ageneralization%2520on%2520SIMD.%2520Extensive%2520experiments%2520and%2520real-world%2520UAV%2520deployment%250Aconfirm%2520the%2520effectiveness%2520and%2520practicality%2520of%2520our%2520proposed%2520solution%2520for%250AUAV-based%2520OVD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06011v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Light-Weight%20Cross-Modal%20Enhancement%20Method%20with%20Benchmark%20Construction%0A%20%20for%20UAV-based%20Open-Vocabulary%20Object%20Detection&entry.906535625=Zhenhai%20Weng%20and%20Xinjie%20Li%20and%20Can%20Wu%20and%20Weijie%20He%20and%20Jianfeng%20Lv%20and%20Dong%20Zhou%20and%20Zhongliang%20Yu&entry.1292438233=%20%20Open-Vocabulary%20Object%20Detection%20%28OVD%29%20faces%20severe%20performance%20degradation%0Awhen%20applied%20to%20UAV%20imagery%20due%20to%20the%20domain%20gap%20from%20ground-level%20datasets.%0ATo%20address%20this%20challenge%2C%20we%20propose%20a%20complete%20UAV-oriented%20solution%20that%0Acombines%20both%20dataset%20construction%20and%20model%20innovation.%20First%2C%20we%20design%20a%0Arefined%20UAV-Label%20Engine%2C%20which%20efficiently%20resolves%20annotation%20redundancy%2C%0Ainconsistency%2C%20and%20ambiguity%2C%20enabling%20the%20generation%20of%20largescale%20UAV%0Adatasets.%20Based%20on%20this%20engine%2C%20we%20construct%20two%20new%20benchmarks%3A%20UAVDE-2M%2C%20with%0Aover%202.4M%20instances%20across%201%2C800%2B%20categories%2C%20and%20UAVCAP-15K%2C%20providing%20rich%0Aimage-text%20pairs%20for%20vision-language%20pretraining.%20Second%2C%20we%20introduce%20the%0ACross-Attention%20Gated%20Enhancement%20%28CAGE%29%20module%2C%20a%20lightweight%20dual-path%20fusion%0Adesign%20that%20integrates%20cross-attention%2C%20adaptive%20gating%2C%20and%20global%20FiLM%0Amodulation%20for%20robust%20textvision%20alignment.%20By%20embedding%20CAGE%20into%20the%0AYOLO-World-v2%20framework%2C%20our%20method%20achieves%20significant%20gains%20in%20both%20accuracy%0Aand%20efficiency%2C%20notably%20improving%20zero-shot%20detection%20on%20VisDrone%20by%20%2B5.3%20mAP%0Awhile%20reducing%20parameters%20and%20GFLOPs%2C%20and%20demonstrating%20strong%20cross-domain%0Ageneralization%20on%20SIMD.%20Extensive%20experiments%20and%20real-world%20UAV%20deployment%0Aconfirm%20the%20effectiveness%20and%20practicality%20of%20our%20proposed%20solution%20for%0AUAV-based%20OVD%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06011v2&entry.124074799=Read"},
{"title": "Self-Supervised Temporal Super-Resolution of Energy Data using\n  Generative Adversarial Transformer", "author": "Xuanhao Mu and G\u00f6khan Demirel and Yuzhe Zhang and Jianlei Liu and Thorsten Schlachter and Veit Hagenmeyer", "abstract": "  To bridge the temporal granularity gap in energy network design and operation\nbased on Energy System Models, resampling of time series is required. While\nconventional upsampling methods are computationally efficient, they often\nresult in significant information loss or increased noise. Advanced models such\nas time series generation models, Super-Resolution models and imputation models\nshow potential, but also face fundamental challenges. The goal of time series\ngenerative models is to learn the distribution of the original data to generate\nhigh-resolution series with similar statistical characteristics. This is not\nentirely consistent with the definition of upsampling. Time series\nSuper-Resolution models or imputation models can degrade the accuracy of\nupsampling because the input low-resolution time series are sparse and may have\ninsufficient context. Moreover, such models usually rely on supervised learning\nparadigms. This presents a fundamental application paradox: their training\nrequires the high-resolution time series that is intrinsically absent in\nupsampling application scenarios. To address the mentioned upsampling issue,\nthis paper introduces a new method utilizing Generative Adversarial\nTransformers (GATs), which can be trained without access to any ground-truth\nhigh-resolution data. Compared with conventional interpolation methods, the\nintroduced method can reduce the root mean square error (RMSE) of upsampling\ntasks by 9%, and the accuracy of a model predictive control (MPC) application\nscenario is improved by 13%.\n", "link": "http://arxiv.org/abs/2508.10587v2", "date": "2025-09-09", "relevancy": 2.2413, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6024}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5723}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Temporal%20Super-Resolution%20of%20Energy%20Data%20using%0A%20%20Generative%20Adversarial%20Transformer&body=Title%3A%20Self-Supervised%20Temporal%20Super-Resolution%20of%20Energy%20Data%20using%0A%20%20Generative%20Adversarial%20Transformer%0AAuthor%3A%20Xuanhao%20Mu%20and%20G%C3%B6khan%20Demirel%20and%20Yuzhe%20Zhang%20and%20Jianlei%20Liu%20and%20Thorsten%20Schlachter%20and%20Veit%20Hagenmeyer%0AAbstract%3A%20%20%20To%20bridge%20the%20temporal%20granularity%20gap%20in%20energy%20network%20design%20and%20operation%0Abased%20on%20Energy%20System%20Models%2C%20resampling%20of%20time%20series%20is%20required.%20While%0Aconventional%20upsampling%20methods%20are%20computationally%20efficient%2C%20they%20often%0Aresult%20in%20significant%20information%20loss%20or%20increased%20noise.%20Advanced%20models%20such%0Aas%20time%20series%20generation%20models%2C%20Super-Resolution%20models%20and%20imputation%20models%0Ashow%20potential%2C%20but%20also%20face%20fundamental%20challenges.%20The%20goal%20of%20time%20series%0Agenerative%20models%20is%20to%20learn%20the%20distribution%20of%20the%20original%20data%20to%20generate%0Ahigh-resolution%20series%20with%20similar%20statistical%20characteristics.%20This%20is%20not%0Aentirely%20consistent%20with%20the%20definition%20of%20upsampling.%20Time%20series%0ASuper-Resolution%20models%20or%20imputation%20models%20can%20degrade%20the%20accuracy%20of%0Aupsampling%20because%20the%20input%20low-resolution%20time%20series%20are%20sparse%20and%20may%20have%0Ainsufficient%20context.%20Moreover%2C%20such%20models%20usually%20rely%20on%20supervised%20learning%0Aparadigms.%20This%20presents%20a%20fundamental%20application%20paradox%3A%20their%20training%0Arequires%20the%20high-resolution%20time%20series%20that%20is%20intrinsically%20absent%20in%0Aupsampling%20application%20scenarios.%20To%20address%20the%20mentioned%20upsampling%20issue%2C%0Athis%20paper%20introduces%20a%20new%20method%20utilizing%20Generative%20Adversarial%0ATransformers%20%28GATs%29%2C%20which%20can%20be%20trained%20without%20access%20to%20any%20ground-truth%0Ahigh-resolution%20data.%20Compared%20with%20conventional%20interpolation%20methods%2C%20the%0Aintroduced%20method%20can%20reduce%20the%20root%20mean%20square%20error%20%28RMSE%29%20of%20upsampling%0Atasks%20by%209%25%2C%20and%20the%20accuracy%20of%20a%20model%20predictive%20control%20%28MPC%29%20application%0Ascenario%20is%20improved%20by%2013%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Temporal%2520Super-Resolution%2520of%2520Energy%2520Data%2520using%250A%2520%2520Generative%2520Adversarial%2520Transformer%26entry.906535625%3DXuanhao%2520Mu%2520and%2520G%25C3%25B6khan%2520Demirel%2520and%2520Yuzhe%2520Zhang%2520and%2520Jianlei%2520Liu%2520and%2520Thorsten%2520Schlachter%2520and%2520Veit%2520Hagenmeyer%26entry.1292438233%3D%2520%2520To%2520bridge%2520the%2520temporal%2520granularity%2520gap%2520in%2520energy%2520network%2520design%2520and%2520operation%250Abased%2520on%2520Energy%2520System%2520Models%252C%2520resampling%2520of%2520time%2520series%2520is%2520required.%2520While%250Aconventional%2520upsampling%2520methods%2520are%2520computationally%2520efficient%252C%2520they%2520often%250Aresult%2520in%2520significant%2520information%2520loss%2520or%2520increased%2520noise.%2520Advanced%2520models%2520such%250Aas%2520time%2520series%2520generation%2520models%252C%2520Super-Resolution%2520models%2520and%2520imputation%2520models%250Ashow%2520potential%252C%2520but%2520also%2520face%2520fundamental%2520challenges.%2520The%2520goal%2520of%2520time%2520series%250Agenerative%2520models%2520is%2520to%2520learn%2520the%2520distribution%2520of%2520the%2520original%2520data%2520to%2520generate%250Ahigh-resolution%2520series%2520with%2520similar%2520statistical%2520characteristics.%2520This%2520is%2520not%250Aentirely%2520consistent%2520with%2520the%2520definition%2520of%2520upsampling.%2520Time%2520series%250ASuper-Resolution%2520models%2520or%2520imputation%2520models%2520can%2520degrade%2520the%2520accuracy%2520of%250Aupsampling%2520because%2520the%2520input%2520low-resolution%2520time%2520series%2520are%2520sparse%2520and%2520may%2520have%250Ainsufficient%2520context.%2520Moreover%252C%2520such%2520models%2520usually%2520rely%2520on%2520supervised%2520learning%250Aparadigms.%2520This%2520presents%2520a%2520fundamental%2520application%2520paradox%253A%2520their%2520training%250Arequires%2520the%2520high-resolution%2520time%2520series%2520that%2520is%2520intrinsically%2520absent%2520in%250Aupsampling%2520application%2520scenarios.%2520To%2520address%2520the%2520mentioned%2520upsampling%2520issue%252C%250Athis%2520paper%2520introduces%2520a%2520new%2520method%2520utilizing%2520Generative%2520Adversarial%250ATransformers%2520%2528GATs%2529%252C%2520which%2520can%2520be%2520trained%2520without%2520access%2520to%2520any%2520ground-truth%250Ahigh-resolution%2520data.%2520Compared%2520with%2520conventional%2520interpolation%2520methods%252C%2520the%250Aintroduced%2520method%2520can%2520reduce%2520the%2520root%2520mean%2520square%2520error%2520%2528RMSE%2529%2520of%2520upsampling%250Atasks%2520by%25209%2525%252C%2520and%2520the%2520accuracy%2520of%2520a%2520model%2520predictive%2520control%2520%2528MPC%2529%2520application%250Ascenario%2520is%2520improved%2520by%252013%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Temporal%20Super-Resolution%20of%20Energy%20Data%20using%0A%20%20Generative%20Adversarial%20Transformer&entry.906535625=Xuanhao%20Mu%20and%20G%C3%B6khan%20Demirel%20and%20Yuzhe%20Zhang%20and%20Jianlei%20Liu%20and%20Thorsten%20Schlachter%20and%20Veit%20Hagenmeyer&entry.1292438233=%20%20To%20bridge%20the%20temporal%20granularity%20gap%20in%20energy%20network%20design%20and%20operation%0Abased%20on%20Energy%20System%20Models%2C%20resampling%20of%20time%20series%20is%20required.%20While%0Aconventional%20upsampling%20methods%20are%20computationally%20efficient%2C%20they%20often%0Aresult%20in%20significant%20information%20loss%20or%20increased%20noise.%20Advanced%20models%20such%0Aas%20time%20series%20generation%20models%2C%20Super-Resolution%20models%20and%20imputation%20models%0Ashow%20potential%2C%20but%20also%20face%20fundamental%20challenges.%20The%20goal%20of%20time%20series%0Agenerative%20models%20is%20to%20learn%20the%20distribution%20of%20the%20original%20data%20to%20generate%0Ahigh-resolution%20series%20with%20similar%20statistical%20characteristics.%20This%20is%20not%0Aentirely%20consistent%20with%20the%20definition%20of%20upsampling.%20Time%20series%0ASuper-Resolution%20models%20or%20imputation%20models%20can%20degrade%20the%20accuracy%20of%0Aupsampling%20because%20the%20input%20low-resolution%20time%20series%20are%20sparse%20and%20may%20have%0Ainsufficient%20context.%20Moreover%2C%20such%20models%20usually%20rely%20on%20supervised%20learning%0Aparadigms.%20This%20presents%20a%20fundamental%20application%20paradox%3A%20their%20training%0Arequires%20the%20high-resolution%20time%20series%20that%20is%20intrinsically%20absent%20in%0Aupsampling%20application%20scenarios.%20To%20address%20the%20mentioned%20upsampling%20issue%2C%0Athis%20paper%20introduces%20a%20new%20method%20utilizing%20Generative%20Adversarial%0ATransformers%20%28GATs%29%2C%20which%20can%20be%20trained%20without%20access%20to%20any%20ground-truth%0Ahigh-resolution%20data.%20Compared%20with%20conventional%20interpolation%20methods%2C%20the%0Aintroduced%20method%20can%20reduce%20the%20root%20mean%20square%20error%20%28RMSE%29%20of%20upsampling%0Atasks%20by%209%25%2C%20and%20the%20accuracy%20of%20a%20model%20predictive%20control%20%28MPC%29%20application%0Ascenario%20is%20improved%20by%2013%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10587v2&entry.124074799=Read"},
{"title": "Visualizing Thought: Conceptual Diagrams Enable Robust Combinatorial\n  Planning in LMMs", "author": "Nasim Borazjanizadeh and Roei Herzig and Eduard Oks and Trevor Darrell and Rogerio Feris and Leonid Karlinsky", "abstract": "  Human reasoning relies on constructing and manipulating mental models --\nsimplified internal representations of situations that we use to understand and\nsolve problems. Conceptual diagrams (e.g., a sketch drawn by a human to aid\nreasoning) externalize these mental models, abstracting irrelevant details to\nefficiently capture how entities interact with each other. In contrast, Large\nLanguage Models (LLMs) and Large MultiModal Models (LMMs) predominantly reason\nthrough text, limiting their effectiveness in complex multi-step tasks. In this\npaper, we propose Visual Thinking, a zero-shot framework that enables LMMs to\nreason through multiple chains of (self-generated) conceptual diagrams,\nsignificantly enhancing their combinatorial planning capabilities. Our approach\ndoes not require any human initialization beyond the natural language\ndescription of the task. It integrates both textual and diagrammatic reasoning\nwithin an optimized Graph-of-Thought inference framework, enhanced by beam\nsearch and depth-wise backtracking. Evaluated on multiple challenging PDDL\nplanning domains, our method substantially improves LMMs' performance (e.g.,\nGPT-4o: 35.5% -> 90.2% in Blocksworld) and consistently outperforms other\ntext-only search-based inference methods. On more difficult planning domains\nwith solution depths up to 40, our approach outperforms even the o1-preview\nreasoning model (e.g., 16 percentage points improvement in Floor Tiles). These\nresults highlight the value of conceptual diagrams as a reasoning medium in\nLMMs.\n", "link": "http://arxiv.org/abs/2503.11790v2", "date": "2025-09-09", "relevancy": 2.2409, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5605}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visualizing%20Thought%3A%20Conceptual%20Diagrams%20Enable%20Robust%20Combinatorial%0A%20%20Planning%20in%20LMMs&body=Title%3A%20Visualizing%20Thought%3A%20Conceptual%20Diagrams%20Enable%20Robust%20Combinatorial%0A%20%20Planning%20in%20LMMs%0AAuthor%3A%20Nasim%20Borazjanizadeh%20and%20Roei%20Herzig%20and%20Eduard%20Oks%20and%20Trevor%20Darrell%20and%20Rogerio%20Feris%20and%20Leonid%20Karlinsky%0AAbstract%3A%20%20%20Human%20reasoning%20relies%20on%20constructing%20and%20manipulating%20mental%20models%20--%0Asimplified%20internal%20representations%20of%20situations%20that%20we%20use%20to%20understand%20and%0Asolve%20problems.%20Conceptual%20diagrams%20%28e.g.%2C%20a%20sketch%20drawn%20by%20a%20human%20to%20aid%0Areasoning%29%20externalize%20these%20mental%20models%2C%20abstracting%20irrelevant%20details%20to%0Aefficiently%20capture%20how%20entities%20interact%20with%20each%20other.%20In%20contrast%2C%20Large%0ALanguage%20Models%20%28LLMs%29%20and%20Large%20MultiModal%20Models%20%28LMMs%29%20predominantly%20reason%0Athrough%20text%2C%20limiting%20their%20effectiveness%20in%20complex%20multi-step%20tasks.%20In%20this%0Apaper%2C%20we%20propose%20Visual%20Thinking%2C%20a%20zero-shot%20framework%20that%20enables%20LMMs%20to%0Areason%20through%20multiple%20chains%20of%20%28self-generated%29%20conceptual%20diagrams%2C%0Asignificantly%20enhancing%20their%20combinatorial%20planning%20capabilities.%20Our%20approach%0Adoes%20not%20require%20any%20human%20initialization%20beyond%20the%20natural%20language%0Adescription%20of%20the%20task.%20It%20integrates%20both%20textual%20and%20diagrammatic%20reasoning%0Awithin%20an%20optimized%20Graph-of-Thought%20inference%20framework%2C%20enhanced%20by%20beam%0Asearch%20and%20depth-wise%20backtracking.%20Evaluated%20on%20multiple%20challenging%20PDDL%0Aplanning%20domains%2C%20our%20method%20substantially%20improves%20LMMs%27%20performance%20%28e.g.%2C%0AGPT-4o%3A%2035.5%25%20-%3E%2090.2%25%20in%20Blocksworld%29%20and%20consistently%20outperforms%20other%0Atext-only%20search-based%20inference%20methods.%20On%20more%20difficult%20planning%20domains%0Awith%20solution%20depths%20up%20to%2040%2C%20our%20approach%20outperforms%20even%20the%20o1-preview%0Areasoning%20model%20%28e.g.%2C%2016%20percentage%20points%20improvement%20in%20Floor%20Tiles%29.%20These%0Aresults%20highlight%20the%20value%20of%20conceptual%20diagrams%20as%20a%20reasoning%20medium%20in%0ALMMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.11790v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualizing%2520Thought%253A%2520Conceptual%2520Diagrams%2520Enable%2520Robust%2520Combinatorial%250A%2520%2520Planning%2520in%2520LMMs%26entry.906535625%3DNasim%2520Borazjanizadeh%2520and%2520Roei%2520Herzig%2520and%2520Eduard%2520Oks%2520and%2520Trevor%2520Darrell%2520and%2520Rogerio%2520Feris%2520and%2520Leonid%2520Karlinsky%26entry.1292438233%3D%2520%2520Human%2520reasoning%2520relies%2520on%2520constructing%2520and%2520manipulating%2520mental%2520models%2520--%250Asimplified%2520internal%2520representations%2520of%2520situations%2520that%2520we%2520use%2520to%2520understand%2520and%250Asolve%2520problems.%2520Conceptual%2520diagrams%2520%2528e.g.%252C%2520a%2520sketch%2520drawn%2520by%2520a%2520human%2520to%2520aid%250Areasoning%2529%2520externalize%2520these%2520mental%2520models%252C%2520abstracting%2520irrelevant%2520details%2520to%250Aefficiently%2520capture%2520how%2520entities%2520interact%2520with%2520each%2520other.%2520In%2520contrast%252C%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520and%2520Large%2520MultiModal%2520Models%2520%2528LMMs%2529%2520predominantly%2520reason%250Athrough%2520text%252C%2520limiting%2520their%2520effectiveness%2520in%2520complex%2520multi-step%2520tasks.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520Visual%2520Thinking%252C%2520a%2520zero-shot%2520framework%2520that%2520enables%2520LMMs%2520to%250Areason%2520through%2520multiple%2520chains%2520of%2520%2528self-generated%2529%2520conceptual%2520diagrams%252C%250Asignificantly%2520enhancing%2520their%2520combinatorial%2520planning%2520capabilities.%2520Our%2520approach%250Adoes%2520not%2520require%2520any%2520human%2520initialization%2520beyond%2520the%2520natural%2520language%250Adescription%2520of%2520the%2520task.%2520It%2520integrates%2520both%2520textual%2520and%2520diagrammatic%2520reasoning%250Awithin%2520an%2520optimized%2520Graph-of-Thought%2520inference%2520framework%252C%2520enhanced%2520by%2520beam%250Asearch%2520and%2520depth-wise%2520backtracking.%2520Evaluated%2520on%2520multiple%2520challenging%2520PDDL%250Aplanning%2520domains%252C%2520our%2520method%2520substantially%2520improves%2520LMMs%2527%2520performance%2520%2528e.g.%252C%250AGPT-4o%253A%252035.5%2525%2520-%253E%252090.2%2525%2520in%2520Blocksworld%2529%2520and%2520consistently%2520outperforms%2520other%250Atext-only%2520search-based%2520inference%2520methods.%2520On%2520more%2520difficult%2520planning%2520domains%250Awith%2520solution%2520depths%2520up%2520to%252040%252C%2520our%2520approach%2520outperforms%2520even%2520the%2520o1-preview%250Areasoning%2520model%2520%2528e.g.%252C%252016%2520percentage%2520points%2520improvement%2520in%2520Floor%2520Tiles%2529.%2520These%250Aresults%2520highlight%2520the%2520value%2520of%2520conceptual%2520diagrams%2520as%2520a%2520reasoning%2520medium%2520in%250ALMMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.11790v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visualizing%20Thought%3A%20Conceptual%20Diagrams%20Enable%20Robust%20Combinatorial%0A%20%20Planning%20in%20LMMs&entry.906535625=Nasim%20Borazjanizadeh%20and%20Roei%20Herzig%20and%20Eduard%20Oks%20and%20Trevor%20Darrell%20and%20Rogerio%20Feris%20and%20Leonid%20Karlinsky&entry.1292438233=%20%20Human%20reasoning%20relies%20on%20constructing%20and%20manipulating%20mental%20models%20--%0Asimplified%20internal%20representations%20of%20situations%20that%20we%20use%20to%20understand%20and%0Asolve%20problems.%20Conceptual%20diagrams%20%28e.g.%2C%20a%20sketch%20drawn%20by%20a%20human%20to%20aid%0Areasoning%29%20externalize%20these%20mental%20models%2C%20abstracting%20irrelevant%20details%20to%0Aefficiently%20capture%20how%20entities%20interact%20with%20each%20other.%20In%20contrast%2C%20Large%0ALanguage%20Models%20%28LLMs%29%20and%20Large%20MultiModal%20Models%20%28LMMs%29%20predominantly%20reason%0Athrough%20text%2C%20limiting%20their%20effectiveness%20in%20complex%20multi-step%20tasks.%20In%20this%0Apaper%2C%20we%20propose%20Visual%20Thinking%2C%20a%20zero-shot%20framework%20that%20enables%20LMMs%20to%0Areason%20through%20multiple%20chains%20of%20%28self-generated%29%20conceptual%20diagrams%2C%0Asignificantly%20enhancing%20their%20combinatorial%20planning%20capabilities.%20Our%20approach%0Adoes%20not%20require%20any%20human%20initialization%20beyond%20the%20natural%20language%0Adescription%20of%20the%20task.%20It%20integrates%20both%20textual%20and%20diagrammatic%20reasoning%0Awithin%20an%20optimized%20Graph-of-Thought%20inference%20framework%2C%20enhanced%20by%20beam%0Asearch%20and%20depth-wise%20backtracking.%20Evaluated%20on%20multiple%20challenging%20PDDL%0Aplanning%20domains%2C%20our%20method%20substantially%20improves%20LMMs%27%20performance%20%28e.g.%2C%0AGPT-4o%3A%2035.5%25%20-%3E%2090.2%25%20in%20Blocksworld%29%20and%20consistently%20outperforms%20other%0Atext-only%20search-based%20inference%20methods.%20On%20more%20difficult%20planning%20domains%0Awith%20solution%20depths%20up%20to%2040%2C%20our%20approach%20outperforms%20even%20the%20o1-preview%0Areasoning%20model%20%28e.g.%2C%2016%20percentage%20points%20improvement%20in%20Floor%20Tiles%29.%20These%0Aresults%20highlight%20the%20value%20of%20conceptual%20diagrams%20as%20a%20reasoning%20medium%20in%0ALMMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.11790v2&entry.124074799=Read"},
{"title": "Spectral Masking and Interpolation Attack (SMIA): A Black-box\n  Adversarial Attack against Voice Authentication and Anti-Spoofing Systems", "author": "Kamel Kamel and Hridoy Sankar Dutta and Keshav Sood and Sunil Aryal", "abstract": "  Voice Authentication Systems (VAS) use unique vocal characteristics for\nverification. They are increasingly integrated into high-security sectors such\nas banking and healthcare. Despite their improvements using deep learning, they\nface severe vulnerabilities from sophisticated threats like deepfakes and\nadversarial attacks. The emergence of realistic voice cloning complicates\ndetection, as systems struggle to distinguish authentic from synthetic audio.\nWhile anti-spoofing countermeasures (CMs) exist to mitigate these risks, many\nrely on static detection models that can be bypassed by novel adversarial\nmethods, leaving a critical security gap. To demonstrate this vulnerability, we\npropose the Spectral Masking and Interpolation Attack (SMIA), a novel method\nthat strategically manipulates inaudible frequency regions of AI-generated\naudio. By altering the voice in imperceptible zones to the human ear, SMIA\ncreates adversarial samples that sound authentic while deceiving CMs. We\nconducted a comprehensive evaluation of our attack against state-of-the-art\n(SOTA) models across multiple tasks, under simulated real-world conditions.\nSMIA achieved a strong attack success rate (ASR) of at least 82% against\ncombined VAS/CM systems, at least 97.5% against standalone speaker verification\nsystems, and 100% against countermeasures. These findings conclusively\ndemonstrate that current security postures are insufficient against adaptive\nadversarial attacks. This work highlights the urgent need for a paradigm shift\ntoward next-generation defenses that employ dynamic, context-aware frameworks\ncapable of evolving with the threat landscape.\n", "link": "http://arxiv.org/abs/2509.07677v1", "date": "2025-09-09", "relevancy": 2.2335, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4631}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4474}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%3A%20A%20Black-box%0A%20%20Adversarial%20Attack%20against%20Voice%20Authentication%20and%20Anti-Spoofing%20Systems&body=Title%3A%20Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%3A%20A%20Black-box%0A%20%20Adversarial%20Attack%20against%20Voice%20Authentication%20and%20Anti-Spoofing%20Systems%0AAuthor%3A%20Kamel%20Kamel%20and%20Hridoy%20Sankar%20Dutta%20and%20Keshav%20Sood%20and%20Sunil%20Aryal%0AAbstract%3A%20%20%20Voice%20Authentication%20Systems%20%28VAS%29%20use%20unique%20vocal%20characteristics%20for%0Averification.%20They%20are%20increasingly%20integrated%20into%20high-security%20sectors%20such%0Aas%20banking%20and%20healthcare.%20Despite%20their%20improvements%20using%20deep%20learning%2C%20they%0Aface%20severe%20vulnerabilities%20from%20sophisticated%20threats%20like%20deepfakes%20and%0Aadversarial%20attacks.%20The%20emergence%20of%20realistic%20voice%20cloning%20complicates%0Adetection%2C%20as%20systems%20struggle%20to%20distinguish%20authentic%20from%20synthetic%20audio.%0AWhile%20anti-spoofing%20countermeasures%20%28CMs%29%20exist%20to%20mitigate%20these%20risks%2C%20many%0Arely%20on%20static%20detection%20models%20that%20can%20be%20bypassed%20by%20novel%20adversarial%0Amethods%2C%20leaving%20a%20critical%20security%20gap.%20To%20demonstrate%20this%20vulnerability%2C%20we%0Apropose%20the%20Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%2C%20a%20novel%20method%0Athat%20strategically%20manipulates%20inaudible%20frequency%20regions%20of%20AI-generated%0Aaudio.%20By%20altering%20the%20voice%20in%20imperceptible%20zones%20to%20the%20human%20ear%2C%20SMIA%0Acreates%20adversarial%20samples%20that%20sound%20authentic%20while%20deceiving%20CMs.%20We%0Aconducted%20a%20comprehensive%20evaluation%20of%20our%20attack%20against%20state-of-the-art%0A%28SOTA%29%20models%20across%20multiple%20tasks%2C%20under%20simulated%20real-world%20conditions.%0ASMIA%20achieved%20a%20strong%20attack%20success%20rate%20%28ASR%29%20of%20at%20least%2082%25%20against%0Acombined%20VAS/CM%20systems%2C%20at%20least%2097.5%25%20against%20standalone%20speaker%20verification%0Asystems%2C%20and%20100%25%20against%20countermeasures.%20These%20findings%20conclusively%0Ademonstrate%20that%20current%20security%20postures%20are%20insufficient%20against%20adaptive%0Aadversarial%20attacks.%20This%20work%20highlights%20the%20urgent%20need%20for%20a%20paradigm%20shift%0Atoward%20next-generation%20defenses%20that%20employ%20dynamic%2C%20context-aware%20frameworks%0Acapable%20of%20evolving%20with%20the%20threat%20landscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Masking%2520and%2520Interpolation%2520Attack%2520%2528SMIA%2529%253A%2520A%2520Black-box%250A%2520%2520Adversarial%2520Attack%2520against%2520Voice%2520Authentication%2520and%2520Anti-Spoofing%2520Systems%26entry.906535625%3DKamel%2520Kamel%2520and%2520Hridoy%2520Sankar%2520Dutta%2520and%2520Keshav%2520Sood%2520and%2520Sunil%2520Aryal%26entry.1292438233%3D%2520%2520Voice%2520Authentication%2520Systems%2520%2528VAS%2529%2520use%2520unique%2520vocal%2520characteristics%2520for%250Averification.%2520They%2520are%2520increasingly%2520integrated%2520into%2520high-security%2520sectors%2520such%250Aas%2520banking%2520and%2520healthcare.%2520Despite%2520their%2520improvements%2520using%2520deep%2520learning%252C%2520they%250Aface%2520severe%2520vulnerabilities%2520from%2520sophisticated%2520threats%2520like%2520deepfakes%2520and%250Aadversarial%2520attacks.%2520The%2520emergence%2520of%2520realistic%2520voice%2520cloning%2520complicates%250Adetection%252C%2520as%2520systems%2520struggle%2520to%2520distinguish%2520authentic%2520from%2520synthetic%2520audio.%250AWhile%2520anti-spoofing%2520countermeasures%2520%2528CMs%2529%2520exist%2520to%2520mitigate%2520these%2520risks%252C%2520many%250Arely%2520on%2520static%2520detection%2520models%2520that%2520can%2520be%2520bypassed%2520by%2520novel%2520adversarial%250Amethods%252C%2520leaving%2520a%2520critical%2520security%2520gap.%2520To%2520demonstrate%2520this%2520vulnerability%252C%2520we%250Apropose%2520the%2520Spectral%2520Masking%2520and%2520Interpolation%2520Attack%2520%2528SMIA%2529%252C%2520a%2520novel%2520method%250Athat%2520strategically%2520manipulates%2520inaudible%2520frequency%2520regions%2520of%2520AI-generated%250Aaudio.%2520By%2520altering%2520the%2520voice%2520in%2520imperceptible%2520zones%2520to%2520the%2520human%2520ear%252C%2520SMIA%250Acreates%2520adversarial%2520samples%2520that%2520sound%2520authentic%2520while%2520deceiving%2520CMs.%2520We%250Aconducted%2520a%2520comprehensive%2520evaluation%2520of%2520our%2520attack%2520against%2520state-of-the-art%250A%2528SOTA%2529%2520models%2520across%2520multiple%2520tasks%252C%2520under%2520simulated%2520real-world%2520conditions.%250ASMIA%2520achieved%2520a%2520strong%2520attack%2520success%2520rate%2520%2528ASR%2529%2520of%2520at%2520least%252082%2525%2520against%250Acombined%2520VAS/CM%2520systems%252C%2520at%2520least%252097.5%2525%2520against%2520standalone%2520speaker%2520verification%250Asystems%252C%2520and%2520100%2525%2520against%2520countermeasures.%2520These%2520findings%2520conclusively%250Ademonstrate%2520that%2520current%2520security%2520postures%2520are%2520insufficient%2520against%2520adaptive%250Aadversarial%2520attacks.%2520This%2520work%2520highlights%2520the%2520urgent%2520need%2520for%2520a%2520paradigm%2520shift%250Atoward%2520next-generation%2520defenses%2520that%2520employ%2520dynamic%252C%2520context-aware%2520frameworks%250Acapable%2520of%2520evolving%2520with%2520the%2520threat%2520landscape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%3A%20A%20Black-box%0A%20%20Adversarial%20Attack%20against%20Voice%20Authentication%20and%20Anti-Spoofing%20Systems&entry.906535625=Kamel%20Kamel%20and%20Hridoy%20Sankar%20Dutta%20and%20Keshav%20Sood%20and%20Sunil%20Aryal&entry.1292438233=%20%20Voice%20Authentication%20Systems%20%28VAS%29%20use%20unique%20vocal%20characteristics%20for%0Averification.%20They%20are%20increasingly%20integrated%20into%20high-security%20sectors%20such%0Aas%20banking%20and%20healthcare.%20Despite%20their%20improvements%20using%20deep%20learning%2C%20they%0Aface%20severe%20vulnerabilities%20from%20sophisticated%20threats%20like%20deepfakes%20and%0Aadversarial%20attacks.%20The%20emergence%20of%20realistic%20voice%20cloning%20complicates%0Adetection%2C%20as%20systems%20struggle%20to%20distinguish%20authentic%20from%20synthetic%20audio.%0AWhile%20anti-spoofing%20countermeasures%20%28CMs%29%20exist%20to%20mitigate%20these%20risks%2C%20many%0Arely%20on%20static%20detection%20models%20that%20can%20be%20bypassed%20by%20novel%20adversarial%0Amethods%2C%20leaving%20a%20critical%20security%20gap.%20To%20demonstrate%20this%20vulnerability%2C%20we%0Apropose%20the%20Spectral%20Masking%20and%20Interpolation%20Attack%20%28SMIA%29%2C%20a%20novel%20method%0Athat%20strategically%20manipulates%20inaudible%20frequency%20regions%20of%20AI-generated%0Aaudio.%20By%20altering%20the%20voice%20in%20imperceptible%20zones%20to%20the%20human%20ear%2C%20SMIA%0Acreates%20adversarial%20samples%20that%20sound%20authentic%20while%20deceiving%20CMs.%20We%0Aconducted%20a%20comprehensive%20evaluation%20of%20our%20attack%20against%20state-of-the-art%0A%28SOTA%29%20models%20across%20multiple%20tasks%2C%20under%20simulated%20real-world%20conditions.%0ASMIA%20achieved%20a%20strong%20attack%20success%20rate%20%28ASR%29%20of%20at%20least%2082%25%20against%0Acombined%20VAS/CM%20systems%2C%20at%20least%2097.5%25%20against%20standalone%20speaker%20verification%0Asystems%2C%20and%20100%25%20against%20countermeasures.%20These%20findings%20conclusively%0Ademonstrate%20that%20current%20security%20postures%20are%20insufficient%20against%20adaptive%0Aadversarial%20attacks.%20This%20work%20highlights%20the%20urgent%20need%20for%20a%20paradigm%20shift%0Atoward%20next-generation%20defenses%20that%20employ%20dynamic%2C%20context-aware%20frameworks%0Acapable%20of%20evolving%20with%20the%20threat%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07677v1&entry.124074799=Read"},
{"title": "MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes", "author": "Nilay Pande and Sahiti Yerramilli and Jayant Sravan Tamarapalli and Rynaa Grover", "abstract": "  A key frontier for Multimodal Large Language Models (MLLMs) is the ability to\nperform deep mathematical and spatial reasoning directly from images, moving\nbeyond their established success in semantic description. Mathematical surface\nplots provide a rigorous testbed for this capability, as they isolate the task\nof reasoning from the semantic noise common in natural images. To measure\nprogress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over\nVisual Landscapes), a new benchmark designed to quantitatively evaluate these\ncore reasoning skills. The benchmark comprises two novel tasks: Topological\nCounting, identifying and enumerating features like local maxima; and\nTransformation Recognition, recognizing applied geometric transformations.\nGenerated from a curated library of functions with rigorous ambiguity\nfiltering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs\nstruggle significantly, often resorting to superficial heuristics instead of\nrobust spatial reasoning. MaRVL-QA provides a challenging new tool for the\nresearch community to measure progress, expose model limitations, and guide the\ndevelopment of MLLMs with more profound reasoning abilities.\n", "link": "http://arxiv.org/abs/2508.17180v2", "date": "2025-09-09", "relevancy": 2.2279, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaRVL-QA%3A%20A%20Benchmark%20for%20Mathematical%20Reasoning%20over%20Visual%20Landscapes&body=Title%3A%20MaRVL-QA%3A%20A%20Benchmark%20for%20Mathematical%20Reasoning%20over%20Visual%20Landscapes%0AAuthor%3A%20Nilay%20Pande%20and%20Sahiti%20Yerramilli%20and%20Jayant%20Sravan%20Tamarapalli%20and%20Rynaa%20Grover%0AAbstract%3A%20%20%20A%20key%20frontier%20for%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20is%20the%20ability%20to%0Aperform%20deep%20mathematical%20and%20spatial%20reasoning%20directly%20from%20images%2C%20moving%0Abeyond%20their%20established%20success%20in%20semantic%20description.%20Mathematical%20surface%0Aplots%20provide%20a%20rigorous%20testbed%20for%20this%20capability%2C%20as%20they%20isolate%20the%20task%0Aof%20reasoning%20from%20the%20semantic%20noise%20common%20in%20natural%20images.%20To%20measure%0Aprogress%20on%20this%20frontier%2C%20we%20introduce%20MaRVL-QA%20%28Mathematical%20Reasoning%20over%0AVisual%20Landscapes%29%2C%20a%20new%20benchmark%20designed%20to%20quantitatively%20evaluate%20these%0Acore%20reasoning%20skills.%20The%20benchmark%20comprises%20two%20novel%20tasks%3A%20Topological%0ACounting%2C%20identifying%20and%20enumerating%20features%20like%20local%20maxima%3B%20and%0ATransformation%20Recognition%2C%20recognizing%20applied%20geometric%20transformations.%0AGenerated%20from%20a%20curated%20library%20of%20functions%20with%20rigorous%20ambiguity%0Afiltering%2C%20our%20evaluation%20on%20MaRVL-QA%20reveals%20that%20even%20state-of-the-art%20MLLMs%0Astruggle%20significantly%2C%20often%20resorting%20to%20superficial%20heuristics%20instead%20of%0Arobust%20spatial%20reasoning.%20MaRVL-QA%20provides%20a%20challenging%20new%20tool%20for%20the%0Aresearch%20community%20to%20measure%20progress%2C%20expose%20model%20limitations%2C%20and%20guide%20the%0Adevelopment%20of%20MLLMs%20with%20more%20profound%20reasoning%20abilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17180v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaRVL-QA%253A%2520A%2520Benchmark%2520for%2520Mathematical%2520Reasoning%2520over%2520Visual%2520Landscapes%26entry.906535625%3DNilay%2520Pande%2520and%2520Sahiti%2520Yerramilli%2520and%2520Jayant%2520Sravan%2520Tamarapalli%2520and%2520Rynaa%2520Grover%26entry.1292438233%3D%2520%2520A%2520key%2520frontier%2520for%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520is%2520the%2520ability%2520to%250Aperform%2520deep%2520mathematical%2520and%2520spatial%2520reasoning%2520directly%2520from%2520images%252C%2520moving%250Abeyond%2520their%2520established%2520success%2520in%2520semantic%2520description.%2520Mathematical%2520surface%250Aplots%2520provide%2520a%2520rigorous%2520testbed%2520for%2520this%2520capability%252C%2520as%2520they%2520isolate%2520the%2520task%250Aof%2520reasoning%2520from%2520the%2520semantic%2520noise%2520common%2520in%2520natural%2520images.%2520To%2520measure%250Aprogress%2520on%2520this%2520frontier%252C%2520we%2520introduce%2520MaRVL-QA%2520%2528Mathematical%2520Reasoning%2520over%250AVisual%2520Landscapes%2529%252C%2520a%2520new%2520benchmark%2520designed%2520to%2520quantitatively%2520evaluate%2520these%250Acore%2520reasoning%2520skills.%2520The%2520benchmark%2520comprises%2520two%2520novel%2520tasks%253A%2520Topological%250ACounting%252C%2520identifying%2520and%2520enumerating%2520features%2520like%2520local%2520maxima%253B%2520and%250ATransformation%2520Recognition%252C%2520recognizing%2520applied%2520geometric%2520transformations.%250AGenerated%2520from%2520a%2520curated%2520library%2520of%2520functions%2520with%2520rigorous%2520ambiguity%250Afiltering%252C%2520our%2520evaluation%2520on%2520MaRVL-QA%2520reveals%2520that%2520even%2520state-of-the-art%2520MLLMs%250Astruggle%2520significantly%252C%2520often%2520resorting%2520to%2520superficial%2520heuristics%2520instead%2520of%250Arobust%2520spatial%2520reasoning.%2520MaRVL-QA%2520provides%2520a%2520challenging%2520new%2520tool%2520for%2520the%250Aresearch%2520community%2520to%2520measure%2520progress%252C%2520expose%2520model%2520limitations%252C%2520and%2520guide%2520the%250Adevelopment%2520of%2520MLLMs%2520with%2520more%2520profound%2520reasoning%2520abilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17180v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaRVL-QA%3A%20A%20Benchmark%20for%20Mathematical%20Reasoning%20over%20Visual%20Landscapes&entry.906535625=Nilay%20Pande%20and%20Sahiti%20Yerramilli%20and%20Jayant%20Sravan%20Tamarapalli%20and%20Rynaa%20Grover&entry.1292438233=%20%20A%20key%20frontier%20for%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20is%20the%20ability%20to%0Aperform%20deep%20mathematical%20and%20spatial%20reasoning%20directly%20from%20images%2C%20moving%0Abeyond%20their%20established%20success%20in%20semantic%20description.%20Mathematical%20surface%0Aplots%20provide%20a%20rigorous%20testbed%20for%20this%20capability%2C%20as%20they%20isolate%20the%20task%0Aof%20reasoning%20from%20the%20semantic%20noise%20common%20in%20natural%20images.%20To%20measure%0Aprogress%20on%20this%20frontier%2C%20we%20introduce%20MaRVL-QA%20%28Mathematical%20Reasoning%20over%0AVisual%20Landscapes%29%2C%20a%20new%20benchmark%20designed%20to%20quantitatively%20evaluate%20these%0Acore%20reasoning%20skills.%20The%20benchmark%20comprises%20two%20novel%20tasks%3A%20Topological%0ACounting%2C%20identifying%20and%20enumerating%20features%20like%20local%20maxima%3B%20and%0ATransformation%20Recognition%2C%20recognizing%20applied%20geometric%20transformations.%0AGenerated%20from%20a%20curated%20library%20of%20functions%20with%20rigorous%20ambiguity%0Afiltering%2C%20our%20evaluation%20on%20MaRVL-QA%20reveals%20that%20even%20state-of-the-art%20MLLMs%0Astruggle%20significantly%2C%20often%20resorting%20to%20superficial%20heuristics%20instead%20of%0Arobust%20spatial%20reasoning.%20MaRVL-QA%20provides%20a%20challenging%20new%20tool%20for%20the%0Aresearch%20community%20to%20measure%20progress%2C%20expose%20model%20limitations%2C%20and%20guide%20the%0Adevelopment%20of%20MLLMs%20with%20more%20profound%20reasoning%20abilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17180v2&entry.124074799=Read"},
{"title": "Attention of a Kiss: Exploring Attention Maps in Video Diffusion for\n  XAIxArts", "author": "Adam Cole and Mick Grierson", "abstract": "  This paper presents an artistic and technical investigation into the\nattention mechanisms of video diffusion transformers. Inspired by early video\nartists who manipulated analog video signals to create new visual aesthetics,\nthis study proposes a method for extracting and visualizing cross-attention\nmaps in generative video models. Built on the open-source Wan model, our tool\nprovides an interpretable window into the temporal and spatial behavior of\nattention in text-to-video generation. Through exploratory probes and an\nartistic case study, we examine the potential of attention maps as both\nanalytical tools and raw artistic material. This work contributes to the\ngrowing field of Explainable AI for the Arts (XAIxArts), inviting artists to\nreclaim the inner workings of AI as a creative medium.\n", "link": "http://arxiv.org/abs/2509.05323v2", "date": "2025-09-09", "relevancy": 2.2207, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5724}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5676}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20of%20a%20Kiss%3A%20Exploring%20Attention%20Maps%20in%20Video%20Diffusion%20for%0A%20%20XAIxArts&body=Title%3A%20Attention%20of%20a%20Kiss%3A%20Exploring%20Attention%20Maps%20in%20Video%20Diffusion%20for%0A%20%20XAIxArts%0AAuthor%3A%20Adam%20Cole%20and%20Mick%20Grierson%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20artistic%20and%20technical%20investigation%20into%20the%0Aattention%20mechanisms%20of%20video%20diffusion%20transformers.%20Inspired%20by%20early%20video%0Aartists%20who%20manipulated%20analog%20video%20signals%20to%20create%20new%20visual%20aesthetics%2C%0Athis%20study%20proposes%20a%20method%20for%20extracting%20and%20visualizing%20cross-attention%0Amaps%20in%20generative%20video%20models.%20Built%20on%20the%20open-source%20Wan%20model%2C%20our%20tool%0Aprovides%20an%20interpretable%20window%20into%20the%20temporal%20and%20spatial%20behavior%20of%0Aattention%20in%20text-to-video%20generation.%20Through%20exploratory%20probes%20and%20an%0Aartistic%20case%20study%2C%20we%20examine%20the%20potential%20of%20attention%20maps%20as%20both%0Aanalytical%20tools%20and%20raw%20artistic%20material.%20This%20work%20contributes%20to%20the%0Agrowing%20field%20of%20Explainable%20AI%20for%20the%20Arts%20%28XAIxArts%29%2C%20inviting%20artists%20to%0Areclaim%20the%20inner%20workings%20of%20AI%20as%20a%20creative%20medium.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05323v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520of%2520a%2520Kiss%253A%2520Exploring%2520Attention%2520Maps%2520in%2520Video%2520Diffusion%2520for%250A%2520%2520XAIxArts%26entry.906535625%3DAdam%2520Cole%2520and%2520Mick%2520Grierson%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520artistic%2520and%2520technical%2520investigation%2520into%2520the%250Aattention%2520mechanisms%2520of%2520video%2520diffusion%2520transformers.%2520Inspired%2520by%2520early%2520video%250Aartists%2520who%2520manipulated%2520analog%2520video%2520signals%2520to%2520create%2520new%2520visual%2520aesthetics%252C%250Athis%2520study%2520proposes%2520a%2520method%2520for%2520extracting%2520and%2520visualizing%2520cross-attention%250Amaps%2520in%2520generative%2520video%2520models.%2520Built%2520on%2520the%2520open-source%2520Wan%2520model%252C%2520our%2520tool%250Aprovides%2520an%2520interpretable%2520window%2520into%2520the%2520temporal%2520and%2520spatial%2520behavior%2520of%250Aattention%2520in%2520text-to-video%2520generation.%2520Through%2520exploratory%2520probes%2520and%2520an%250Aartistic%2520case%2520study%252C%2520we%2520examine%2520the%2520potential%2520of%2520attention%2520maps%2520as%2520both%250Aanalytical%2520tools%2520and%2520raw%2520artistic%2520material.%2520This%2520work%2520contributes%2520to%2520the%250Agrowing%2520field%2520of%2520Explainable%2520AI%2520for%2520the%2520Arts%2520%2528XAIxArts%2529%252C%2520inviting%2520artists%2520to%250Areclaim%2520the%2520inner%2520workings%2520of%2520AI%2520as%2520a%2520creative%2520medium.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05323v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20of%20a%20Kiss%3A%20Exploring%20Attention%20Maps%20in%20Video%20Diffusion%20for%0A%20%20XAIxArts&entry.906535625=Adam%20Cole%20and%20Mick%20Grierson&entry.1292438233=%20%20This%20paper%20presents%20an%20artistic%20and%20technical%20investigation%20into%20the%0Aattention%20mechanisms%20of%20video%20diffusion%20transformers.%20Inspired%20by%20early%20video%0Aartists%20who%20manipulated%20analog%20video%20signals%20to%20create%20new%20visual%20aesthetics%2C%0Athis%20study%20proposes%20a%20method%20for%20extracting%20and%20visualizing%20cross-attention%0Amaps%20in%20generative%20video%20models.%20Built%20on%20the%20open-source%20Wan%20model%2C%20our%20tool%0Aprovides%20an%20interpretable%20window%20into%20the%20temporal%20and%20spatial%20behavior%20of%0Aattention%20in%20text-to-video%20generation.%20Through%20exploratory%20probes%20and%20an%0Aartistic%20case%20study%2C%20we%20examine%20the%20potential%20of%20attention%20maps%20as%20both%0Aanalytical%20tools%20and%20raw%20artistic%20material.%20This%20work%20contributes%20to%20the%0Agrowing%20field%20of%20Explainable%20AI%20for%20the%20Arts%20%28XAIxArts%29%2C%20inviting%20artists%20to%0Areclaim%20the%20inner%20workings%20of%20AI%20as%20a%20creative%20medium.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05323v2&entry.124074799=Read"},
{"title": "BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions", "author": "Ahmed Emam and Mohamed Elbassiouny and Julius Miller and Patrick Donworth and Sabine Seidel and Ribana Roscher", "abstract": "  Pollinator insects such as honeybees and bumblebees are vital to global food\nproduction and ecosystem stability, yet their populations are declining due to\nanthropogenic and environmental stressors. Scalable, automated monitoring in\nagricultural environments remains an open challenge due to the difficulty of\ndetecting small, fast-moving, and often camouflaged insects. To address this,\nwe present BuzzSet v1.0, a large-scale dataset of high-resolution pollinator\nimages collected under real field conditions. BuzzSet contains 7,856 manually\nverified images with more than 8,000 annotated instances across three classes:\nhoneybees, bumblebees, and unidentified insects. Initial annotations were\nproduced using a YOLOv12 model trained on external data and refined through\nhuman verification with open-source tools. All images were preprocessed into\n256 x 256 tiles to improve the detection of small insects. We provide baselines\nusing the RF-DETR transformer-based object detector. The model achieves strong\nclassification accuracy with F1 scores of 0.94 and 0.92 for honeybees and\nbumblebees, with minimal confusion between these categories. The unidentified\nclass remains more difficult due to label ambiguity and fewer samples, yet\nstill contributes insights for robustness evaluation. Overall detection\nperformance (mAP at 0.50 of 0.559) illustrates the challenging nature of the\ndataset and its potential to drive advances in small object detection under\nrealistic ecological conditions. Future work focuses on expanding the dataset\nto version 2.0 with additional annotations and evaluating further detection\nstrategies. BuzzSet establishes a benchmark for ecological computer vision,\nwith the primary challenge being reliable detection of insects frequently\ncamouflaged within natural vegetation, highlighting an open problem for future\nresearch.\n", "link": "http://arxiv.org/abs/2508.19762v4", "date": "2025-09-09", "relevancy": 2.2064, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4497}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.443}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BuzzSet%20v1.0%3A%20A%20Dataset%20for%20Pollinator%20Detection%20in%20Field%20Conditions&body=Title%3A%20BuzzSet%20v1.0%3A%20A%20Dataset%20for%20Pollinator%20Detection%20in%20Field%20Conditions%0AAuthor%3A%20Ahmed%20Emam%20and%20Mohamed%20Elbassiouny%20and%20Julius%20Miller%20and%20Patrick%20Donworth%20and%20Sabine%20Seidel%20and%20Ribana%20Roscher%0AAbstract%3A%20%20%20Pollinator%20insects%20such%20as%20honeybees%20and%20bumblebees%20are%20vital%20to%20global%20food%0Aproduction%20and%20ecosystem%20stability%2C%20yet%20their%20populations%20are%20declining%20due%20to%0Aanthropogenic%20and%20environmental%20stressors.%20Scalable%2C%20automated%20monitoring%20in%0Aagricultural%20environments%20remains%20an%20open%20challenge%20due%20to%20the%20difficulty%20of%0Adetecting%20small%2C%20fast-moving%2C%20and%20often%20camouflaged%20insects.%20To%20address%20this%2C%0Awe%20present%20BuzzSet%20v1.0%2C%20a%20large-scale%20dataset%20of%20high-resolution%20pollinator%0Aimages%20collected%20under%20real%20field%20conditions.%20BuzzSet%20contains%207%2C856%20manually%0Averified%20images%20with%20more%20than%208%2C000%20annotated%20instances%20across%20three%20classes%3A%0Ahoneybees%2C%20bumblebees%2C%20and%20unidentified%20insects.%20Initial%20annotations%20were%0Aproduced%20using%20a%20YOLOv12%20model%20trained%20on%20external%20data%20and%20refined%20through%0Ahuman%20verification%20with%20open-source%20tools.%20All%20images%20were%20preprocessed%20into%0A256%20x%20256%20tiles%20to%20improve%20the%20detection%20of%20small%20insects.%20We%20provide%20baselines%0Ausing%20the%20RF-DETR%20transformer-based%20object%20detector.%20The%20model%20achieves%20strong%0Aclassification%20accuracy%20with%20F1%20scores%20of%200.94%20and%200.92%20for%20honeybees%20and%0Abumblebees%2C%20with%20minimal%20confusion%20between%20these%20categories.%20The%20unidentified%0Aclass%20remains%20more%20difficult%20due%20to%20label%20ambiguity%20and%20fewer%20samples%2C%20yet%0Astill%20contributes%20insights%20for%20robustness%20evaluation.%20Overall%20detection%0Aperformance%20%28mAP%20at%200.50%20of%200.559%29%20illustrates%20the%20challenging%20nature%20of%20the%0Adataset%20and%20its%20potential%20to%20drive%20advances%20in%20small%20object%20detection%20under%0Arealistic%20ecological%20conditions.%20Future%20work%20focuses%20on%20expanding%20the%20dataset%0Ato%20version%202.0%20with%20additional%20annotations%20and%20evaluating%20further%20detection%0Astrategies.%20BuzzSet%20establishes%20a%20benchmark%20for%20ecological%20computer%20vision%2C%0Awith%20the%20primary%20challenge%20being%20reliable%20detection%20of%20insects%20frequently%0Acamouflaged%20within%20natural%20vegetation%2C%20highlighting%20an%20open%20problem%20for%20future%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19762v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuzzSet%2520v1.0%253A%2520A%2520Dataset%2520for%2520Pollinator%2520Detection%2520in%2520Field%2520Conditions%26entry.906535625%3DAhmed%2520Emam%2520and%2520Mohamed%2520Elbassiouny%2520and%2520Julius%2520Miller%2520and%2520Patrick%2520Donworth%2520and%2520Sabine%2520Seidel%2520and%2520Ribana%2520Roscher%26entry.1292438233%3D%2520%2520Pollinator%2520insects%2520such%2520as%2520honeybees%2520and%2520bumblebees%2520are%2520vital%2520to%2520global%2520food%250Aproduction%2520and%2520ecosystem%2520stability%252C%2520yet%2520their%2520populations%2520are%2520declining%2520due%2520to%250Aanthropogenic%2520and%2520environmental%2520stressors.%2520Scalable%252C%2520automated%2520monitoring%2520in%250Aagricultural%2520environments%2520remains%2520an%2520open%2520challenge%2520due%2520to%2520the%2520difficulty%2520of%250Adetecting%2520small%252C%2520fast-moving%252C%2520and%2520often%2520camouflaged%2520insects.%2520To%2520address%2520this%252C%250Awe%2520present%2520BuzzSet%2520v1.0%252C%2520a%2520large-scale%2520dataset%2520of%2520high-resolution%2520pollinator%250Aimages%2520collected%2520under%2520real%2520field%2520conditions.%2520BuzzSet%2520contains%25207%252C856%2520manually%250Averified%2520images%2520with%2520more%2520than%25208%252C000%2520annotated%2520instances%2520across%2520three%2520classes%253A%250Ahoneybees%252C%2520bumblebees%252C%2520and%2520unidentified%2520insects.%2520Initial%2520annotations%2520were%250Aproduced%2520using%2520a%2520YOLOv12%2520model%2520trained%2520on%2520external%2520data%2520and%2520refined%2520through%250Ahuman%2520verification%2520with%2520open-source%2520tools.%2520All%2520images%2520were%2520preprocessed%2520into%250A256%2520x%2520256%2520tiles%2520to%2520improve%2520the%2520detection%2520of%2520small%2520insects.%2520We%2520provide%2520baselines%250Ausing%2520the%2520RF-DETR%2520transformer-based%2520object%2520detector.%2520The%2520model%2520achieves%2520strong%250Aclassification%2520accuracy%2520with%2520F1%2520scores%2520of%25200.94%2520and%25200.92%2520for%2520honeybees%2520and%250Abumblebees%252C%2520with%2520minimal%2520confusion%2520between%2520these%2520categories.%2520The%2520unidentified%250Aclass%2520remains%2520more%2520difficult%2520due%2520to%2520label%2520ambiguity%2520and%2520fewer%2520samples%252C%2520yet%250Astill%2520contributes%2520insights%2520for%2520robustness%2520evaluation.%2520Overall%2520detection%250Aperformance%2520%2528mAP%2520at%25200.50%2520of%25200.559%2529%2520illustrates%2520the%2520challenging%2520nature%2520of%2520the%250Adataset%2520and%2520its%2520potential%2520to%2520drive%2520advances%2520in%2520small%2520object%2520detection%2520under%250Arealistic%2520ecological%2520conditions.%2520Future%2520work%2520focuses%2520on%2520expanding%2520the%2520dataset%250Ato%2520version%25202.0%2520with%2520additional%2520annotations%2520and%2520evaluating%2520further%2520detection%250Astrategies.%2520BuzzSet%2520establishes%2520a%2520benchmark%2520for%2520ecological%2520computer%2520vision%252C%250Awith%2520the%2520primary%2520challenge%2520being%2520reliable%2520detection%2520of%2520insects%2520frequently%250Acamouflaged%2520within%2520natural%2520vegetation%252C%2520highlighting%2520an%2520open%2520problem%2520for%2520future%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19762v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BuzzSet%20v1.0%3A%20A%20Dataset%20for%20Pollinator%20Detection%20in%20Field%20Conditions&entry.906535625=Ahmed%20Emam%20and%20Mohamed%20Elbassiouny%20and%20Julius%20Miller%20and%20Patrick%20Donworth%20and%20Sabine%20Seidel%20and%20Ribana%20Roscher&entry.1292438233=%20%20Pollinator%20insects%20such%20as%20honeybees%20and%20bumblebees%20are%20vital%20to%20global%20food%0Aproduction%20and%20ecosystem%20stability%2C%20yet%20their%20populations%20are%20declining%20due%20to%0Aanthropogenic%20and%20environmental%20stressors.%20Scalable%2C%20automated%20monitoring%20in%0Aagricultural%20environments%20remains%20an%20open%20challenge%20due%20to%20the%20difficulty%20of%0Adetecting%20small%2C%20fast-moving%2C%20and%20often%20camouflaged%20insects.%20To%20address%20this%2C%0Awe%20present%20BuzzSet%20v1.0%2C%20a%20large-scale%20dataset%20of%20high-resolution%20pollinator%0Aimages%20collected%20under%20real%20field%20conditions.%20BuzzSet%20contains%207%2C856%20manually%0Averified%20images%20with%20more%20than%208%2C000%20annotated%20instances%20across%20three%20classes%3A%0Ahoneybees%2C%20bumblebees%2C%20and%20unidentified%20insects.%20Initial%20annotations%20were%0Aproduced%20using%20a%20YOLOv12%20model%20trained%20on%20external%20data%20and%20refined%20through%0Ahuman%20verification%20with%20open-source%20tools.%20All%20images%20were%20preprocessed%20into%0A256%20x%20256%20tiles%20to%20improve%20the%20detection%20of%20small%20insects.%20We%20provide%20baselines%0Ausing%20the%20RF-DETR%20transformer-based%20object%20detector.%20The%20model%20achieves%20strong%0Aclassification%20accuracy%20with%20F1%20scores%20of%200.94%20and%200.92%20for%20honeybees%20and%0Abumblebees%2C%20with%20minimal%20confusion%20between%20these%20categories.%20The%20unidentified%0Aclass%20remains%20more%20difficult%20due%20to%20label%20ambiguity%20and%20fewer%20samples%2C%20yet%0Astill%20contributes%20insights%20for%20robustness%20evaluation.%20Overall%20detection%0Aperformance%20%28mAP%20at%200.50%20of%200.559%29%20illustrates%20the%20challenging%20nature%20of%20the%0Adataset%20and%20its%20potential%20to%20drive%20advances%20in%20small%20object%20detection%20under%0Arealistic%20ecological%20conditions.%20Future%20work%20focuses%20on%20expanding%20the%20dataset%0Ato%20version%202.0%20with%20additional%20annotations%20and%20evaluating%20further%20detection%0Astrategies.%20BuzzSet%20establishes%20a%20benchmark%20for%20ecological%20computer%20vision%2C%0Awith%20the%20primary%20challenge%20being%20reliable%20detection%20of%20insects%20frequently%0Acamouflaged%20within%20natural%20vegetation%2C%20highlighting%20an%20open%20problem%20for%20future%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19762v4&entry.124074799=Read"},
{"title": "CP-Model-Zoo: A Natural Language Query System for Constraint Programming\n  Models", "author": "Augustin Crespin and Ioannis Kostis and H\u00e9l\u00e8ne Verhaeghe and Pierre Schaus", "abstract": "  Constraint Programming and its high-level modeling languages have long been\nrecognized for their potential to achieve the holy grail of problem-solving.\nHowever, the complexity of modeling languages, the large number of global\nconstraints, and the art of creating good models have often hindered\nnon-experts from choosing CP to solve their combinatorial problems. While\ngenerating an expert-level model from a natural-language description of a\nproblem would be the dream, we are not yet there. We propose a tutoring system\ncalled CP-Model-Zoo, exploiting expert-written models accumulated through the\nyears. CP-Model-Zoo retrieves the closest source code model from a database\nbased on a user's natural language description of a combinatorial problem. It\nensures that expert-validated models are presented to the user while\neliminating the need for human data labeling. Our experiments show excellent\naccuracy in retrieving the correct model based on a user-input description of a\nproblem simulated with different levels of expertise.\n", "link": "http://arxiv.org/abs/2509.07867v1", "date": "2025-09-09", "relevancy": 2.1976, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5532}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CP-Model-Zoo%3A%20A%20Natural%20Language%20Query%20System%20for%20Constraint%20Programming%0A%20%20Models&body=Title%3A%20CP-Model-Zoo%3A%20A%20Natural%20Language%20Query%20System%20for%20Constraint%20Programming%0A%20%20Models%0AAuthor%3A%20Augustin%20Crespin%20and%20Ioannis%20Kostis%20and%20H%C3%A9l%C3%A8ne%20Verhaeghe%20and%20Pierre%20Schaus%0AAbstract%3A%20%20%20Constraint%20Programming%20and%20its%20high-level%20modeling%20languages%20have%20long%20been%0Arecognized%20for%20their%20potential%20to%20achieve%20the%20holy%20grail%20of%20problem-solving.%0AHowever%2C%20the%20complexity%20of%20modeling%20languages%2C%20the%20large%20number%20of%20global%0Aconstraints%2C%20and%20the%20art%20of%20creating%20good%20models%20have%20often%20hindered%0Anon-experts%20from%20choosing%20CP%20to%20solve%20their%20combinatorial%20problems.%20While%0Agenerating%20an%20expert-level%20model%20from%20a%20natural-language%20description%20of%20a%0Aproblem%20would%20be%20the%20dream%2C%20we%20are%20not%20yet%20there.%20We%20propose%20a%20tutoring%20system%0Acalled%20CP-Model-Zoo%2C%20exploiting%20expert-written%20models%20accumulated%20through%20the%0Ayears.%20CP-Model-Zoo%20retrieves%20the%20closest%20source%20code%20model%20from%20a%20database%0Abased%20on%20a%20user%27s%20natural%20language%20description%20of%20a%20combinatorial%20problem.%20It%0Aensures%20that%20expert-validated%20models%20are%20presented%20to%20the%20user%20while%0Aeliminating%20the%20need%20for%20human%20data%20labeling.%20Our%20experiments%20show%20excellent%0Aaccuracy%20in%20retrieving%20the%20correct%20model%20based%20on%20a%20user-input%20description%20of%20a%0Aproblem%20simulated%20with%20different%20levels%20of%20expertise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCP-Model-Zoo%253A%2520A%2520Natural%2520Language%2520Query%2520System%2520for%2520Constraint%2520Programming%250A%2520%2520Models%26entry.906535625%3DAugustin%2520Crespin%2520and%2520Ioannis%2520Kostis%2520and%2520H%25C3%25A9l%25C3%25A8ne%2520Verhaeghe%2520and%2520Pierre%2520Schaus%26entry.1292438233%3D%2520%2520Constraint%2520Programming%2520and%2520its%2520high-level%2520modeling%2520languages%2520have%2520long%2520been%250Arecognized%2520for%2520their%2520potential%2520to%2520achieve%2520the%2520holy%2520grail%2520of%2520problem-solving.%250AHowever%252C%2520the%2520complexity%2520of%2520modeling%2520languages%252C%2520the%2520large%2520number%2520of%2520global%250Aconstraints%252C%2520and%2520the%2520art%2520of%2520creating%2520good%2520models%2520have%2520often%2520hindered%250Anon-experts%2520from%2520choosing%2520CP%2520to%2520solve%2520their%2520combinatorial%2520problems.%2520While%250Agenerating%2520an%2520expert-level%2520model%2520from%2520a%2520natural-language%2520description%2520of%2520a%250Aproblem%2520would%2520be%2520the%2520dream%252C%2520we%2520are%2520not%2520yet%2520there.%2520We%2520propose%2520a%2520tutoring%2520system%250Acalled%2520CP-Model-Zoo%252C%2520exploiting%2520expert-written%2520models%2520accumulated%2520through%2520the%250Ayears.%2520CP-Model-Zoo%2520retrieves%2520the%2520closest%2520source%2520code%2520model%2520from%2520a%2520database%250Abased%2520on%2520a%2520user%2527s%2520natural%2520language%2520description%2520of%2520a%2520combinatorial%2520problem.%2520It%250Aensures%2520that%2520expert-validated%2520models%2520are%2520presented%2520to%2520the%2520user%2520while%250Aeliminating%2520the%2520need%2520for%2520human%2520data%2520labeling.%2520Our%2520experiments%2520show%2520excellent%250Aaccuracy%2520in%2520retrieving%2520the%2520correct%2520model%2520based%2520on%2520a%2520user-input%2520description%2520of%2520a%250Aproblem%2520simulated%2520with%2520different%2520levels%2520of%2520expertise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CP-Model-Zoo%3A%20A%20Natural%20Language%20Query%20System%20for%20Constraint%20Programming%0A%20%20Models&entry.906535625=Augustin%20Crespin%20and%20Ioannis%20Kostis%20and%20H%C3%A9l%C3%A8ne%20Verhaeghe%20and%20Pierre%20Schaus&entry.1292438233=%20%20Constraint%20Programming%20and%20its%20high-level%20modeling%20languages%20have%20long%20been%0Arecognized%20for%20their%20potential%20to%20achieve%20the%20holy%20grail%20of%20problem-solving.%0AHowever%2C%20the%20complexity%20of%20modeling%20languages%2C%20the%20large%20number%20of%20global%0Aconstraints%2C%20and%20the%20art%20of%20creating%20good%20models%20have%20often%20hindered%0Anon-experts%20from%20choosing%20CP%20to%20solve%20their%20combinatorial%20problems.%20While%0Agenerating%20an%20expert-level%20model%20from%20a%20natural-language%20description%20of%20a%0Aproblem%20would%20be%20the%20dream%2C%20we%20are%20not%20yet%20there.%20We%20propose%20a%20tutoring%20system%0Acalled%20CP-Model-Zoo%2C%20exploiting%20expert-written%20models%20accumulated%20through%20the%0Ayears.%20CP-Model-Zoo%20retrieves%20the%20closest%20source%20code%20model%20from%20a%20database%0Abased%20on%20a%20user%27s%20natural%20language%20description%20of%20a%20combinatorial%20problem.%20It%0Aensures%20that%20expert-validated%20models%20are%20presented%20to%20the%20user%20while%0Aeliminating%20the%20need%20for%20human%20data%20labeling.%20Our%20experiments%20show%20excellent%0Aaccuracy%20in%20retrieving%20the%20correct%20model%20based%20on%20a%20user-input%20description%20of%20a%0Aproblem%20simulated%20with%20different%20levels%20of%20expertise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07867v1&entry.124074799=Read"},
{"title": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large\n  Language Models", "author": "Tuo Wang and Adithya Kulkarni and Tyler Cody and Peter A. Beling and Yujun Yan and Dawei Zhou", "abstract": "  Uncertainty estimation is essential for enhancing the reliability of Large\nLanguage Models (LLMs), particularly in high-stakes applications. Existing\nmethods often overlook semantic dependencies, relying on token-level\nprobability measures that fail to capture structural relationships within the\ngenerated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty\nEstimation for Large Language Models, a structure-aware framework that\nleverages dependency parse trees and hierarchical graph pooling to refine\nuncertainty quantification. By incorporating supervised learning, GENUINE\neffectively models semantic and structural relationships, improving confidence\nassessments. Extensive experiments across NLP tasks show that GENUINE achieves\nup to 29% higher AUROC than semantic entropy-based approaches and reduces\ncalibration errors by over 15%, demonstrating the effectiveness of graph-based\nuncertainty modeling. The code is available at\nhttps://github.com/ODYSSEYWT/GUQ.\n", "link": "http://arxiv.org/abs/2509.07925v1", "date": "2025-09-09", "relevancy": 2.1936, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6109}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.539}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GENUINE%3A%20Graph%20Enhanced%20Multi-level%20Uncertainty%20Estimation%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20GENUINE%3A%20Graph%20Enhanced%20Multi-level%20Uncertainty%20Estimation%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Tuo%20Wang%20and%20Adithya%20Kulkarni%20and%20Tyler%20Cody%20and%20Peter%20A.%20Beling%20and%20Yujun%20Yan%20and%20Dawei%20Zhou%0AAbstract%3A%20%20%20Uncertainty%20estimation%20is%20essential%20for%20enhancing%20the%20reliability%20of%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20particularly%20in%20high-stakes%20applications.%20Existing%0Amethods%20often%20overlook%20semantic%20dependencies%2C%20relying%20on%20token-level%0Aprobability%20measures%20that%20fail%20to%20capture%20structural%20relationships%20within%20the%0Agenerated%20text.%20We%20propose%20GENUINE%3A%20Graph%20ENhanced%20mUlti-level%20uncertaINty%0AEstimation%20for%20Large%20Language%20Models%2C%20a%20structure-aware%20framework%20that%0Aleverages%20dependency%20parse%20trees%20and%20hierarchical%20graph%20pooling%20to%20refine%0Auncertainty%20quantification.%20By%20incorporating%20supervised%20learning%2C%20GENUINE%0Aeffectively%20models%20semantic%20and%20structural%20relationships%2C%20improving%20confidence%0Aassessments.%20Extensive%20experiments%20across%20NLP%20tasks%20show%20that%20GENUINE%20achieves%0Aup%20to%2029%25%20higher%20AUROC%20than%20semantic%20entropy-based%20approaches%20and%20reduces%0Acalibration%20errors%20by%20over%2015%25%2C%20demonstrating%20the%20effectiveness%20of%20graph-based%0Auncertainty%20modeling.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ODYSSEYWT/GUQ.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGENUINE%253A%2520Graph%2520Enhanced%2520Multi-level%2520Uncertainty%2520Estimation%2520for%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DTuo%2520Wang%2520and%2520Adithya%2520Kulkarni%2520and%2520Tyler%2520Cody%2520and%2520Peter%2520A.%2520Beling%2520and%2520Yujun%2520Yan%2520and%2520Dawei%2520Zhou%26entry.1292438233%3D%2520%2520Uncertainty%2520estimation%2520is%2520essential%2520for%2520enhancing%2520the%2520reliability%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520particularly%2520in%2520high-stakes%2520applications.%2520Existing%250Amethods%2520often%2520overlook%2520semantic%2520dependencies%252C%2520relying%2520on%2520token-level%250Aprobability%2520measures%2520that%2520fail%2520to%2520capture%2520structural%2520relationships%2520within%2520the%250Agenerated%2520text.%2520We%2520propose%2520GENUINE%253A%2520Graph%2520ENhanced%2520mUlti-level%2520uncertaINty%250AEstimation%2520for%2520Large%2520Language%2520Models%252C%2520a%2520structure-aware%2520framework%2520that%250Aleverages%2520dependency%2520parse%2520trees%2520and%2520hierarchical%2520graph%2520pooling%2520to%2520refine%250Auncertainty%2520quantification.%2520By%2520incorporating%2520supervised%2520learning%252C%2520GENUINE%250Aeffectively%2520models%2520semantic%2520and%2520structural%2520relationships%252C%2520improving%2520confidence%250Aassessments.%2520Extensive%2520experiments%2520across%2520NLP%2520tasks%2520show%2520that%2520GENUINE%2520achieves%250Aup%2520to%252029%2525%2520higher%2520AUROC%2520than%2520semantic%2520entropy-based%2520approaches%2520and%2520reduces%250Acalibration%2520errors%2520by%2520over%252015%2525%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520graph-based%250Auncertainty%2520modeling.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ODYSSEYWT/GUQ.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GENUINE%3A%20Graph%20Enhanced%20Multi-level%20Uncertainty%20Estimation%20for%20Large%0A%20%20Language%20Models&entry.906535625=Tuo%20Wang%20and%20Adithya%20Kulkarni%20and%20Tyler%20Cody%20and%20Peter%20A.%20Beling%20and%20Yujun%20Yan%20and%20Dawei%20Zhou&entry.1292438233=%20%20Uncertainty%20estimation%20is%20essential%20for%20enhancing%20the%20reliability%20of%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20particularly%20in%20high-stakes%20applications.%20Existing%0Amethods%20often%20overlook%20semantic%20dependencies%2C%20relying%20on%20token-level%0Aprobability%20measures%20that%20fail%20to%20capture%20structural%20relationships%20within%20the%0Agenerated%20text.%20We%20propose%20GENUINE%3A%20Graph%20ENhanced%20mUlti-level%20uncertaINty%0AEstimation%20for%20Large%20Language%20Models%2C%20a%20structure-aware%20framework%20that%0Aleverages%20dependency%20parse%20trees%20and%20hierarchical%20graph%20pooling%20to%20refine%0Auncertainty%20quantification.%20By%20incorporating%20supervised%20learning%2C%20GENUINE%0Aeffectively%20models%20semantic%20and%20structural%20relationships%2C%20improving%20confidence%0Aassessments.%20Extensive%20experiments%20across%20NLP%20tasks%20show%20that%20GENUINE%20achieves%0Aup%20to%2029%25%20higher%20AUROC%20than%20semantic%20entropy-based%20approaches%20and%20reduces%0Acalibration%20errors%20by%20over%2015%25%2C%20demonstrating%20the%20effectiveness%20of%20graph-based%0Auncertainty%20modeling.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ODYSSEYWT/GUQ.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07925v1&entry.124074799=Read"},
{"title": "Closed-Loop Unsupervised Representation Disentanglement with $\u03b2$-VAE\n  Distillation and Diffusion Probabilistic Feedback", "author": "Xin Jin and Bohan Li and BAAO Xie and Wenyao Zhang and Jinming Liu and Ziqiang Li and Tao Yang and Wenjun Zeng", "abstract": "  Representation disentanglement may help AI fundamentally understand the real\nworld and thus benefit both discrimination and generation tasks. It currently\nhas at least three unresolved core issues: (i) heavy reliance on label\nannotation and synthetic data -- causing poor generalization on natural\nscenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to\nadaptively achieve an optimal training trade-off; (iii) lacking reasonable\nevaluation metric, especially for the real label-free data. To address these\nchallenges, we propose a \\textbf{C}losed-\\textbf{L}oop unsupervised\nrepresentation \\textbf{Dis}entanglement approach dubbed \\textbf{CL-Dis}.\nSpecifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while\nresorting to $\\beta$-VAE as a co-pilot to extract semantically disentangled\nrepresentations. The strong generation ability of diffusion model and the good\ndisentanglement ability of VAE model are complementary. To strengthen\ndisentangling, VAE-latent distillation and diffusion-wise feedback are\ninterconnected in a closed-loop system for a further mutual promotion. Then, a\nself-supervised \\textbf{Navigation} strategy is introduced to identify\ninterpretable semantic directions in the disentangled latent space. Finally, a\nnew metric based on content tracking is designed to evaluate the\ndisentanglement effect. Experiments demonstrate the superiority of CL-Dis on\napplications like real image manipulation and visual analysis.\n", "link": "http://arxiv.org/abs/2402.02346v2", "date": "2025-09-09", "relevancy": 2.1633, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5581}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5483}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closed-Loop%20Unsupervised%20Representation%20Disentanglement%20with%20%24%CE%B2%24-VAE%0A%20%20Distillation%20and%20Diffusion%20Probabilistic%20Feedback&body=Title%3A%20Closed-Loop%20Unsupervised%20Representation%20Disentanglement%20with%20%24%CE%B2%24-VAE%0A%20%20Distillation%20and%20Diffusion%20Probabilistic%20Feedback%0AAuthor%3A%20Xin%20Jin%20and%20Bohan%20Li%20and%20BAAO%20Xie%20and%20Wenyao%20Zhang%20and%20Jinming%20Liu%20and%20Ziqiang%20Li%20and%20Tao%20Yang%20and%20Wenjun%20Zeng%0AAbstract%3A%20%20%20Representation%20disentanglement%20may%20help%20AI%20fundamentally%20understand%20the%20real%0Aworld%20and%20thus%20benefit%20both%20discrimination%20and%20generation%20tasks.%20It%20currently%0Ahas%20at%20least%20three%20unresolved%20core%20issues%3A%20%28i%29%20heavy%20reliance%20on%20label%0Aannotation%20and%20synthetic%20data%20--%20causing%20poor%20generalization%20on%20natural%0Ascenarios%3B%20%28ii%29%20heuristic/hand-craft%20disentangling%20constraints%20make%20it%20hard%20to%0Aadaptively%20achieve%20an%20optimal%20training%20trade-off%3B%20%28iii%29%20lacking%20reasonable%0Aevaluation%20metric%2C%20especially%20for%20the%20real%20label-free%20data.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20%5Ctextbf%7BC%7Dlosed-%5Ctextbf%7BL%7Doop%20unsupervised%0Arepresentation%20%5Ctextbf%7BDis%7Dentanglement%20approach%20dubbed%20%5Ctextbf%7BCL-Dis%7D.%0ASpecifically%2C%20we%20use%20diffusion-based%20autoencoder%20%28Diff-AE%29%20as%20a%20backbone%20while%0Aresorting%20to%20%24%5Cbeta%24-VAE%20as%20a%20co-pilot%20to%20extract%20semantically%20disentangled%0Arepresentations.%20The%20strong%20generation%20ability%20of%20diffusion%20model%20and%20the%20good%0Adisentanglement%20ability%20of%20VAE%20model%20are%20complementary.%20To%20strengthen%0Adisentangling%2C%20VAE-latent%20distillation%20and%20diffusion-wise%20feedback%20are%0Ainterconnected%20in%20a%20closed-loop%20system%20for%20a%20further%20mutual%20promotion.%20Then%2C%20a%0Aself-supervised%20%5Ctextbf%7BNavigation%7D%20strategy%20is%20introduced%20to%20identify%0Ainterpretable%20semantic%20directions%20in%20the%20disentangled%20latent%20space.%20Finally%2C%20a%0Anew%20metric%20based%20on%20content%20tracking%20is%20designed%20to%20evaluate%20the%0Adisentanglement%20effect.%20Experiments%20demonstrate%20the%20superiority%20of%20CL-Dis%20on%0Aapplications%20like%20real%20image%20manipulation%20and%20visual%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosed-Loop%2520Unsupervised%2520Representation%2520Disentanglement%2520with%2520%2524%25CE%25B2%2524-VAE%250A%2520%2520Distillation%2520and%2520Diffusion%2520Probabilistic%2520Feedback%26entry.906535625%3DXin%2520Jin%2520and%2520Bohan%2520Li%2520and%2520BAAO%2520Xie%2520and%2520Wenyao%2520Zhang%2520and%2520Jinming%2520Liu%2520and%2520Ziqiang%2520Li%2520and%2520Tao%2520Yang%2520and%2520Wenjun%2520Zeng%26entry.1292438233%3D%2520%2520Representation%2520disentanglement%2520may%2520help%2520AI%2520fundamentally%2520understand%2520the%2520real%250Aworld%2520and%2520thus%2520benefit%2520both%2520discrimination%2520and%2520generation%2520tasks.%2520It%2520currently%250Ahas%2520at%2520least%2520three%2520unresolved%2520core%2520issues%253A%2520%2528i%2529%2520heavy%2520reliance%2520on%2520label%250Aannotation%2520and%2520synthetic%2520data%2520--%2520causing%2520poor%2520generalization%2520on%2520natural%250Ascenarios%253B%2520%2528ii%2529%2520heuristic/hand-craft%2520disentangling%2520constraints%2520make%2520it%2520hard%2520to%250Aadaptively%2520achieve%2520an%2520optimal%2520training%2520trade-off%253B%2520%2528iii%2529%2520lacking%2520reasonable%250Aevaluation%2520metric%252C%2520especially%2520for%2520the%2520real%2520label-free%2520data.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520%255Ctextbf%257BC%257Dlosed-%255Ctextbf%257BL%257Doop%2520unsupervised%250Arepresentation%2520%255Ctextbf%257BDis%257Dentanglement%2520approach%2520dubbed%2520%255Ctextbf%257BCL-Dis%257D.%250ASpecifically%252C%2520we%2520use%2520diffusion-based%2520autoencoder%2520%2528Diff-AE%2529%2520as%2520a%2520backbone%2520while%250Aresorting%2520to%2520%2524%255Cbeta%2524-VAE%2520as%2520a%2520co-pilot%2520to%2520extract%2520semantically%2520disentangled%250Arepresentations.%2520The%2520strong%2520generation%2520ability%2520of%2520diffusion%2520model%2520and%2520the%2520good%250Adisentanglement%2520ability%2520of%2520VAE%2520model%2520are%2520complementary.%2520To%2520strengthen%250Adisentangling%252C%2520VAE-latent%2520distillation%2520and%2520diffusion-wise%2520feedback%2520are%250Ainterconnected%2520in%2520a%2520closed-loop%2520system%2520for%2520a%2520further%2520mutual%2520promotion.%2520Then%252C%2520a%250Aself-supervised%2520%255Ctextbf%257BNavigation%257D%2520strategy%2520is%2520introduced%2520to%2520identify%250Ainterpretable%2520semantic%2520directions%2520in%2520the%2520disentangled%2520latent%2520space.%2520Finally%252C%2520a%250Anew%2520metric%2520based%2520on%2520content%2520tracking%2520is%2520designed%2520to%2520evaluate%2520the%250Adisentanglement%2520effect.%2520Experiments%2520demonstrate%2520the%2520superiority%2520of%2520CL-Dis%2520on%250Aapplications%2520like%2520real%2520image%2520manipulation%2520and%2520visual%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closed-Loop%20Unsupervised%20Representation%20Disentanglement%20with%20%24%CE%B2%24-VAE%0A%20%20Distillation%20and%20Diffusion%20Probabilistic%20Feedback&entry.906535625=Xin%20Jin%20and%20Bohan%20Li%20and%20BAAO%20Xie%20and%20Wenyao%20Zhang%20and%20Jinming%20Liu%20and%20Ziqiang%20Li%20and%20Tao%20Yang%20and%20Wenjun%20Zeng&entry.1292438233=%20%20Representation%20disentanglement%20may%20help%20AI%20fundamentally%20understand%20the%20real%0Aworld%20and%20thus%20benefit%20both%20discrimination%20and%20generation%20tasks.%20It%20currently%0Ahas%20at%20least%20three%20unresolved%20core%20issues%3A%20%28i%29%20heavy%20reliance%20on%20label%0Aannotation%20and%20synthetic%20data%20--%20causing%20poor%20generalization%20on%20natural%0Ascenarios%3B%20%28ii%29%20heuristic/hand-craft%20disentangling%20constraints%20make%20it%20hard%20to%0Aadaptively%20achieve%20an%20optimal%20training%20trade-off%3B%20%28iii%29%20lacking%20reasonable%0Aevaluation%20metric%2C%20especially%20for%20the%20real%20label-free%20data.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20%5Ctextbf%7BC%7Dlosed-%5Ctextbf%7BL%7Doop%20unsupervised%0Arepresentation%20%5Ctextbf%7BDis%7Dentanglement%20approach%20dubbed%20%5Ctextbf%7BCL-Dis%7D.%0ASpecifically%2C%20we%20use%20diffusion-based%20autoencoder%20%28Diff-AE%29%20as%20a%20backbone%20while%0Aresorting%20to%20%24%5Cbeta%24-VAE%20as%20a%20co-pilot%20to%20extract%20semantically%20disentangled%0Arepresentations.%20The%20strong%20generation%20ability%20of%20diffusion%20model%20and%20the%20good%0Adisentanglement%20ability%20of%20VAE%20model%20are%20complementary.%20To%20strengthen%0Adisentangling%2C%20VAE-latent%20distillation%20and%20diffusion-wise%20feedback%20are%0Ainterconnected%20in%20a%20closed-loop%20system%20for%20a%20further%20mutual%20promotion.%20Then%2C%20a%0Aself-supervised%20%5Ctextbf%7BNavigation%7D%20strategy%20is%20introduced%20to%20identify%0Ainterpretable%20semantic%20directions%20in%20the%20disentangled%20latent%20space.%20Finally%2C%20a%0Anew%20metric%20based%20on%20content%20tracking%20is%20designed%20to%20evaluate%20the%0Adisentanglement%20effect.%20Experiments%20demonstrate%20the%20superiority%20of%20CL-Dis%20on%0Aapplications%20like%20real%20image%20manipulation%20and%20visual%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02346v2&entry.124074799=Read"},
{"title": "RealRep: Generalized SDR-to-HDR Conversion via Attribute-Disentangled\n  Representation Learning", "author": "Gang He and Siqi Wang and Kepeng Xu and Lin Zhang and Li Xu and Weiran Wang and Yu-Wing Tai", "abstract": "  High-Dynamic-Range Wide-Color-Gamut (HDR-WCG) technology is becoming\nincreasingly widespread, driving a growing need for converting Standard Dynamic\nRange (SDR) content to HDR. Existing methods primarily rely on fixed tone\nmapping operators, which struggle to handle the diverse appearances and\ndegradations commonly present in real-world SDR content. To address this\nlimitation, we propose a generalized SDR-to-HDR framework that enhances\nrobustness by learning attribute-disentangled representations. Central to our\napproach is Realistic Attribute-Disentangled Representation Learning (RealRep),\nwhich explicitly disentangles luminance and chrominance components to capture\nintrinsic content variations across different SDR distributions. Furthermore,\nwe design a Luma-/Chroma-aware negative exemplar generation strategy that\nconstructs degradation-sensitive contrastive pairs, effectively modeling tone\ndiscrepancies across SDR styles. Building on these attribute-level priors, we\nintroduce the Degradation-Domain Aware Controlled Mapping Network (DDACMNet), a\nlightweight, two-stage framework that performs adaptive hierarchical mapping\nguided by a control-aware normalization mechanism. DDACMNet dynamically\nmodulates the mapping process via degradation-conditioned features, enabling\nrobust adaptation across diverse degradation domains. Extensive experiments\ndemonstrate that RealRep consistently outperforms state-of-the-art methods in\nboth generalization and perceptually faithful HDR color gamut reconstruction.\n", "link": "http://arxiv.org/abs/2505.07322v2", "date": "2025-09-09", "relevancy": 2.1577, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5665}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5542}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealRep%3A%20Generalized%20SDR-to-HDR%20Conversion%20via%20Attribute-Disentangled%0A%20%20Representation%20Learning&body=Title%3A%20RealRep%3A%20Generalized%20SDR-to-HDR%20Conversion%20via%20Attribute-Disentangled%0A%20%20Representation%20Learning%0AAuthor%3A%20Gang%20He%20and%20Siqi%20Wang%20and%20Kepeng%20Xu%20and%20Lin%20Zhang%20and%20Li%20Xu%20and%20Weiran%20Wang%20and%20Yu-Wing%20Tai%0AAbstract%3A%20%20%20High-Dynamic-Range%20Wide-Color-Gamut%20%28HDR-WCG%29%20technology%20is%20becoming%0Aincreasingly%20widespread%2C%20driving%20a%20growing%20need%20for%20converting%20Standard%20Dynamic%0ARange%20%28SDR%29%20content%20to%20HDR.%20Existing%20methods%20primarily%20rely%20on%20fixed%20tone%0Amapping%20operators%2C%20which%20struggle%20to%20handle%20the%20diverse%20appearances%20and%0Adegradations%20commonly%20present%20in%20real-world%20SDR%20content.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20generalized%20SDR-to-HDR%20framework%20that%20enhances%0Arobustness%20by%20learning%20attribute-disentangled%20representations.%20Central%20to%20our%0Aapproach%20is%20Realistic%20Attribute-Disentangled%20Representation%20Learning%20%28RealRep%29%2C%0Awhich%20explicitly%20disentangles%20luminance%20and%20chrominance%20components%20to%20capture%0Aintrinsic%20content%20variations%20across%20different%20SDR%20distributions.%20Furthermore%2C%0Awe%20design%20a%20Luma-/Chroma-aware%20negative%20exemplar%20generation%20strategy%20that%0Aconstructs%20degradation-sensitive%20contrastive%20pairs%2C%20effectively%20modeling%20tone%0Adiscrepancies%20across%20SDR%20styles.%20Building%20on%20these%20attribute-level%20priors%2C%20we%0Aintroduce%20the%20Degradation-Domain%20Aware%20Controlled%20Mapping%20Network%20%28DDACMNet%29%2C%20a%0Alightweight%2C%20two-stage%20framework%20that%20performs%20adaptive%20hierarchical%20mapping%0Aguided%20by%20a%20control-aware%20normalization%20mechanism.%20DDACMNet%20dynamically%0Amodulates%20the%20mapping%20process%20via%20degradation-conditioned%20features%2C%20enabling%0Arobust%20adaptation%20across%20diverse%20degradation%20domains.%20Extensive%20experiments%0Ademonstrate%20that%20RealRep%20consistently%20outperforms%20state-of-the-art%20methods%20in%0Aboth%20generalization%20and%20perceptually%20faithful%20HDR%20color%20gamut%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07322v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealRep%253A%2520Generalized%2520SDR-to-HDR%2520Conversion%2520via%2520Attribute-Disentangled%250A%2520%2520Representation%2520Learning%26entry.906535625%3DGang%2520He%2520and%2520Siqi%2520Wang%2520and%2520Kepeng%2520Xu%2520and%2520Lin%2520Zhang%2520and%2520Li%2520Xu%2520and%2520Weiran%2520Wang%2520and%2520Yu-Wing%2520Tai%26entry.1292438233%3D%2520%2520High-Dynamic-Range%2520Wide-Color-Gamut%2520%2528HDR-WCG%2529%2520technology%2520is%2520becoming%250Aincreasingly%2520widespread%252C%2520driving%2520a%2520growing%2520need%2520for%2520converting%2520Standard%2520Dynamic%250ARange%2520%2528SDR%2529%2520content%2520to%2520HDR.%2520Existing%2520methods%2520primarily%2520rely%2520on%2520fixed%2520tone%250Amapping%2520operators%252C%2520which%2520struggle%2520to%2520handle%2520the%2520diverse%2520appearances%2520and%250Adegradations%2520commonly%2520present%2520in%2520real-world%2520SDR%2520content.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520a%2520generalized%2520SDR-to-HDR%2520framework%2520that%2520enhances%250Arobustness%2520by%2520learning%2520attribute-disentangled%2520representations.%2520Central%2520to%2520our%250Aapproach%2520is%2520Realistic%2520Attribute-Disentangled%2520Representation%2520Learning%2520%2528RealRep%2529%252C%250Awhich%2520explicitly%2520disentangles%2520luminance%2520and%2520chrominance%2520components%2520to%2520capture%250Aintrinsic%2520content%2520variations%2520across%2520different%2520SDR%2520distributions.%2520Furthermore%252C%250Awe%2520design%2520a%2520Luma-/Chroma-aware%2520negative%2520exemplar%2520generation%2520strategy%2520that%250Aconstructs%2520degradation-sensitive%2520contrastive%2520pairs%252C%2520effectively%2520modeling%2520tone%250Adiscrepancies%2520across%2520SDR%2520styles.%2520Building%2520on%2520these%2520attribute-level%2520priors%252C%2520we%250Aintroduce%2520the%2520Degradation-Domain%2520Aware%2520Controlled%2520Mapping%2520Network%2520%2528DDACMNet%2529%252C%2520a%250Alightweight%252C%2520two-stage%2520framework%2520that%2520performs%2520adaptive%2520hierarchical%2520mapping%250Aguided%2520by%2520a%2520control-aware%2520normalization%2520mechanism.%2520DDACMNet%2520dynamically%250Amodulates%2520the%2520mapping%2520process%2520via%2520degradation-conditioned%2520features%252C%2520enabling%250Arobust%2520adaptation%2520across%2520diverse%2520degradation%2520domains.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520RealRep%2520consistently%2520outperforms%2520state-of-the-art%2520methods%2520in%250Aboth%2520generalization%2520and%2520perceptually%2520faithful%2520HDR%2520color%2520gamut%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07322v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealRep%3A%20Generalized%20SDR-to-HDR%20Conversion%20via%20Attribute-Disentangled%0A%20%20Representation%20Learning&entry.906535625=Gang%20He%20and%20Siqi%20Wang%20and%20Kepeng%20Xu%20and%20Lin%20Zhang%20and%20Li%20Xu%20and%20Weiran%20Wang%20and%20Yu-Wing%20Tai&entry.1292438233=%20%20High-Dynamic-Range%20Wide-Color-Gamut%20%28HDR-WCG%29%20technology%20is%20becoming%0Aincreasingly%20widespread%2C%20driving%20a%20growing%20need%20for%20converting%20Standard%20Dynamic%0ARange%20%28SDR%29%20content%20to%20HDR.%20Existing%20methods%20primarily%20rely%20on%20fixed%20tone%0Amapping%20operators%2C%20which%20struggle%20to%20handle%20the%20diverse%20appearances%20and%0Adegradations%20commonly%20present%20in%20real-world%20SDR%20content.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20generalized%20SDR-to-HDR%20framework%20that%20enhances%0Arobustness%20by%20learning%20attribute-disentangled%20representations.%20Central%20to%20our%0Aapproach%20is%20Realistic%20Attribute-Disentangled%20Representation%20Learning%20%28RealRep%29%2C%0Awhich%20explicitly%20disentangles%20luminance%20and%20chrominance%20components%20to%20capture%0Aintrinsic%20content%20variations%20across%20different%20SDR%20distributions.%20Furthermore%2C%0Awe%20design%20a%20Luma-/Chroma-aware%20negative%20exemplar%20generation%20strategy%20that%0Aconstructs%20degradation-sensitive%20contrastive%20pairs%2C%20effectively%20modeling%20tone%0Adiscrepancies%20across%20SDR%20styles.%20Building%20on%20these%20attribute-level%20priors%2C%20we%0Aintroduce%20the%20Degradation-Domain%20Aware%20Controlled%20Mapping%20Network%20%28DDACMNet%29%2C%20a%0Alightweight%2C%20two-stage%20framework%20that%20performs%20adaptive%20hierarchical%20mapping%0Aguided%20by%20a%20control-aware%20normalization%20mechanism.%20DDACMNet%20dynamically%0Amodulates%20the%20mapping%20process%20via%20degradation-conditioned%20features%2C%20enabling%0Arobust%20adaptation%20across%20diverse%20degradation%20domains.%20Extensive%20experiments%0Ademonstrate%20that%20RealRep%20consistently%20outperforms%20state-of-the-art%20methods%20in%0Aboth%20generalization%20and%20perceptually%20faithful%20HDR%20color%20gamut%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07322v2&entry.124074799=Read"},
{"title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT\n  Denoising", "author": "Yichao Liu and Hengzhi Xue and YueYang Teng", "abstract": "  Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications.\n", "link": "http://arxiv.org/abs/2509.06591v2", "date": "2025-09-09", "relevancy": 2.1483, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5602}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5265}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Swin%20Attention%20Networks%20for%20Simultaneously%20Low-Dose%20PET%20and%20CT%0A%20%20Denoising&body=Title%3A%20Hybrid%20Swin%20Attention%20Networks%20for%20Simultaneously%20Low-Dose%20PET%20and%20CT%0A%20%20Denoising%0AAuthor%3A%20Yichao%20Liu%20and%20Hengzhi%20Xue%20and%20YueYang%20Teng%0AAbstract%3A%20%20%20Low-dose%20computed%20tomography%20%28LDCT%29%20and%20positron%20emission%20tomography%20%28PET%29%0Ahave%20emerged%20as%20safer%20alternatives%20to%20conventional%20imaging%20modalities%20by%0Asignificantly%20reducing%20radiation%20exposure.%20However%2C%20this%20reduction%20often%0Aresults%20in%20increased%20noise%20and%20artifacts%2C%20which%20can%20compromise%20diagnostic%0Aaccuracy.%20Consequently%2C%20denoising%20for%20LDCT/PET%20has%20become%20a%20vital%20area%20of%0Aresearch%20aimed%20at%20enhancing%20image%20quality%20while%20maintaining%20radiation%20safety.%0AIn%20this%20study%2C%20we%20introduce%20a%20novel%20Hybrid%20Swin%20Attention%20Network%20%28HSANet%29%2C%0Awhich%20incorporates%20Efficient%20Global%20Attention%20%28EGA%29%20modules%20and%20a%20hybrid%0Aupsampling%20module.%20The%20EGA%20modules%20enhance%20both%20spatial%20and%20channel-wise%0Ainteraction%2C%20improving%20the%20network%27s%20capacity%20to%20capture%20relevant%20features%2C%0Awhile%20the%20hybrid%20upsampling%20module%20mitigates%20the%20risk%20of%20overfitting%20to%20noise.%0AWe%20validate%20the%20proposed%20approach%20using%20a%20publicly%20available%20LDCT/PET%20dataset.%0AExperimental%20results%20demonstrate%20that%20HSANet%20achieves%20superior%20denoising%0Aperformance%20compared%20to%20existing%20methods%2C%20while%20maintaining%20a%20lightweight%20model%0Asize%20suitable%20for%20deployment%20on%20GPUs%20with%20standard%20memory%20configurations.%20This%0Amakes%20our%20approach%20highly%20practical%20for%20real-world%20clinical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06591v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Swin%2520Attention%2520Networks%2520for%2520Simultaneously%2520Low-Dose%2520PET%2520and%2520CT%250A%2520%2520Denoising%26entry.906535625%3DYichao%2520Liu%2520and%2520Hengzhi%2520Xue%2520and%2520YueYang%2520Teng%26entry.1292438233%3D%2520%2520Low-dose%2520computed%2520tomography%2520%2528LDCT%2529%2520and%2520positron%2520emission%2520tomography%2520%2528PET%2529%250Ahave%2520emerged%2520as%2520safer%2520alternatives%2520to%2520conventional%2520imaging%2520modalities%2520by%250Asignificantly%2520reducing%2520radiation%2520exposure.%2520However%252C%2520this%2520reduction%2520often%250Aresults%2520in%2520increased%2520noise%2520and%2520artifacts%252C%2520which%2520can%2520compromise%2520diagnostic%250Aaccuracy.%2520Consequently%252C%2520denoising%2520for%2520LDCT/PET%2520has%2520become%2520a%2520vital%2520area%2520of%250Aresearch%2520aimed%2520at%2520enhancing%2520image%2520quality%2520while%2520maintaining%2520radiation%2520safety.%250AIn%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520Hybrid%2520Swin%2520Attention%2520Network%2520%2528HSANet%2529%252C%250Awhich%2520incorporates%2520Efficient%2520Global%2520Attention%2520%2528EGA%2529%2520modules%2520and%2520a%2520hybrid%250Aupsampling%2520module.%2520The%2520EGA%2520modules%2520enhance%2520both%2520spatial%2520and%2520channel-wise%250Ainteraction%252C%2520improving%2520the%2520network%2527s%2520capacity%2520to%2520capture%2520relevant%2520features%252C%250Awhile%2520the%2520hybrid%2520upsampling%2520module%2520mitigates%2520the%2520risk%2520of%2520overfitting%2520to%2520noise.%250AWe%2520validate%2520the%2520proposed%2520approach%2520using%2520a%2520publicly%2520available%2520LDCT/PET%2520dataset.%250AExperimental%2520results%2520demonstrate%2520that%2520HSANet%2520achieves%2520superior%2520denoising%250Aperformance%2520compared%2520to%2520existing%2520methods%252C%2520while%2520maintaining%2520a%2520lightweight%2520model%250Asize%2520suitable%2520for%2520deployment%2520on%2520GPUs%2520with%2520standard%2520memory%2520configurations.%2520This%250Amakes%2520our%2520approach%2520highly%2520practical%2520for%2520real-world%2520clinical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06591v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Swin%20Attention%20Networks%20for%20Simultaneously%20Low-Dose%20PET%20and%20CT%0A%20%20Denoising&entry.906535625=Yichao%20Liu%20and%20Hengzhi%20Xue%20and%20YueYang%20Teng&entry.1292438233=%20%20Low-dose%20computed%20tomography%20%28LDCT%29%20and%20positron%20emission%20tomography%20%28PET%29%0Ahave%20emerged%20as%20safer%20alternatives%20to%20conventional%20imaging%20modalities%20by%0Asignificantly%20reducing%20radiation%20exposure.%20However%2C%20this%20reduction%20often%0Aresults%20in%20increased%20noise%20and%20artifacts%2C%20which%20can%20compromise%20diagnostic%0Aaccuracy.%20Consequently%2C%20denoising%20for%20LDCT/PET%20has%20become%20a%20vital%20area%20of%0Aresearch%20aimed%20at%20enhancing%20image%20quality%20while%20maintaining%20radiation%20safety.%0AIn%20this%20study%2C%20we%20introduce%20a%20novel%20Hybrid%20Swin%20Attention%20Network%20%28HSANet%29%2C%0Awhich%20incorporates%20Efficient%20Global%20Attention%20%28EGA%29%20modules%20and%20a%20hybrid%0Aupsampling%20module.%20The%20EGA%20modules%20enhance%20both%20spatial%20and%20channel-wise%0Ainteraction%2C%20improving%20the%20network%27s%20capacity%20to%20capture%20relevant%20features%2C%0Awhile%20the%20hybrid%20upsampling%20module%20mitigates%20the%20risk%20of%20overfitting%20to%20noise.%0AWe%20validate%20the%20proposed%20approach%20using%20a%20publicly%20available%20LDCT/PET%20dataset.%0AExperimental%20results%20demonstrate%20that%20HSANet%20achieves%20superior%20denoising%0Aperformance%20compared%20to%20existing%20methods%2C%20while%20maintaining%20a%20lightweight%20model%0Asize%20suitable%20for%20deployment%20on%20GPUs%20with%20standard%20memory%20configurations.%20This%0Amakes%20our%20approach%20highly%20practical%20for%20real-world%20clinical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06591v2&entry.124074799=Read"},
{"title": "BranchGRPO: Stable and Efficient GRPO with Structured Branching in\n  Diffusion Models", "author": "Yuming Li and Yikai Wang and Yuying Zhu and Zhongyu Zhao and Ming Lu and Qi She and Shanghang Zhang", "abstract": "  Recent advancements in aligning image and video generative models via GRPO\nhave achieved remarkable gains in enhancing human preference alignment.\nHowever, these methods still face high computational costs from on-policy\nrollouts and excessive SDE sampling steps, as well as training instability due\nto sparse rewards. In this paper, we propose BranchGRPO, a novel method that\nintroduces a branch sampling policy updating the SDE sampling process. By\nsharing computation across common prefixes and pruning low-reward paths and\nredundant depths, BranchGRPO substantially lowers the per-update compute cost\nwhile maintaining or improving exploration diversity. This work makes three\nmain contributions: (1) a branch sampling scheme that reduces rollout and\ntraining cost; (2) a tree-based advantage estimator incorporating dense\nprocess-level rewards; and (3) pruning strategies exploiting path and depth\nredundancy to accelerate convergence and boost performance. Experiments on\nimage and video preference alignment show that BranchGRPO improves alignment\nscores by 16% over strong baselines, while cutting training time by 50%.\n", "link": "http://arxiv.org/abs/2509.06040v2", "date": "2025-09-09", "relevancy": 2.1432, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5635}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5318}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BranchGRPO%3A%20Stable%20and%20Efficient%20GRPO%20with%20Structured%20Branching%20in%0A%20%20Diffusion%20Models&body=Title%3A%20BranchGRPO%3A%20Stable%20and%20Efficient%20GRPO%20with%20Structured%20Branching%20in%0A%20%20Diffusion%20Models%0AAuthor%3A%20Yuming%20Li%20and%20Yikai%20Wang%20and%20Yuying%20Zhu%20and%20Zhongyu%20Zhao%20and%20Ming%20Lu%20and%20Qi%20She%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20aligning%20image%20and%20video%20generative%20models%20via%20GRPO%0Ahave%20achieved%20remarkable%20gains%20in%20enhancing%20human%20preference%20alignment.%0AHowever%2C%20these%20methods%20still%20face%20high%20computational%20costs%20from%20on-policy%0Arollouts%20and%20excessive%20SDE%20sampling%20steps%2C%20as%20well%20as%20training%20instability%20due%0Ato%20sparse%20rewards.%20In%20this%20paper%2C%20we%20propose%20BranchGRPO%2C%20a%20novel%20method%20that%0Aintroduces%20a%20branch%20sampling%20policy%20updating%20the%20SDE%20sampling%20process.%20By%0Asharing%20computation%20across%20common%20prefixes%20and%20pruning%20low-reward%20paths%20and%0Aredundant%20depths%2C%20BranchGRPO%20substantially%20lowers%20the%20per-update%20compute%20cost%0Awhile%20maintaining%20or%20improving%20exploration%20diversity.%20This%20work%20makes%20three%0Amain%20contributions%3A%20%281%29%20a%20branch%20sampling%20scheme%20that%20reduces%20rollout%20and%0Atraining%20cost%3B%20%282%29%20a%20tree-based%20advantage%20estimator%20incorporating%20dense%0Aprocess-level%20rewards%3B%20and%20%283%29%20pruning%20strategies%20exploiting%20path%20and%20depth%0Aredundancy%20to%20accelerate%20convergence%20and%20boost%20performance.%20Experiments%20on%0Aimage%20and%20video%20preference%20alignment%20show%20that%20BranchGRPO%20improves%20alignment%0Ascores%20by%2016%25%20over%20strong%20baselines%2C%20while%20cutting%20training%20time%20by%2050%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06040v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBranchGRPO%253A%2520Stable%2520and%2520Efficient%2520GRPO%2520with%2520Structured%2520Branching%2520in%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DYuming%2520Li%2520and%2520Yikai%2520Wang%2520and%2520Yuying%2520Zhu%2520and%2520Zhongyu%2520Zhao%2520and%2520Ming%2520Lu%2520and%2520Qi%2520She%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520aligning%2520image%2520and%2520video%2520generative%2520models%2520via%2520GRPO%250Ahave%2520achieved%2520remarkable%2520gains%2520in%2520enhancing%2520human%2520preference%2520alignment.%250AHowever%252C%2520these%2520methods%2520still%2520face%2520high%2520computational%2520costs%2520from%2520on-policy%250Arollouts%2520and%2520excessive%2520SDE%2520sampling%2520steps%252C%2520as%2520well%2520as%2520training%2520instability%2520due%250Ato%2520sparse%2520rewards.%2520In%2520this%2520paper%252C%2520we%2520propose%2520BranchGRPO%252C%2520a%2520novel%2520method%2520that%250Aintroduces%2520a%2520branch%2520sampling%2520policy%2520updating%2520the%2520SDE%2520sampling%2520process.%2520By%250Asharing%2520computation%2520across%2520common%2520prefixes%2520and%2520pruning%2520low-reward%2520paths%2520and%250Aredundant%2520depths%252C%2520BranchGRPO%2520substantially%2520lowers%2520the%2520per-update%2520compute%2520cost%250Awhile%2520maintaining%2520or%2520improving%2520exploration%2520diversity.%2520This%2520work%2520makes%2520three%250Amain%2520contributions%253A%2520%25281%2529%2520a%2520branch%2520sampling%2520scheme%2520that%2520reduces%2520rollout%2520and%250Atraining%2520cost%253B%2520%25282%2529%2520a%2520tree-based%2520advantage%2520estimator%2520incorporating%2520dense%250Aprocess-level%2520rewards%253B%2520and%2520%25283%2529%2520pruning%2520strategies%2520exploiting%2520path%2520and%2520depth%250Aredundancy%2520to%2520accelerate%2520convergence%2520and%2520boost%2520performance.%2520Experiments%2520on%250Aimage%2520and%2520video%2520preference%2520alignment%2520show%2520that%2520BranchGRPO%2520improves%2520alignment%250Ascores%2520by%252016%2525%2520over%2520strong%2520baselines%252C%2520while%2520cutting%2520training%2520time%2520by%252050%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06040v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BranchGRPO%3A%20Stable%20and%20Efficient%20GRPO%20with%20Structured%20Branching%20in%0A%20%20Diffusion%20Models&entry.906535625=Yuming%20Li%20and%20Yikai%20Wang%20and%20Yuying%20Zhu%20and%20Zhongyu%20Zhao%20and%20Ming%20Lu%20and%20Qi%20She%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%20aligning%20image%20and%20video%20generative%20models%20via%20GRPO%0Ahave%20achieved%20remarkable%20gains%20in%20enhancing%20human%20preference%20alignment.%0AHowever%2C%20these%20methods%20still%20face%20high%20computational%20costs%20from%20on-policy%0Arollouts%20and%20excessive%20SDE%20sampling%20steps%2C%20as%20well%20as%20training%20instability%20due%0Ato%20sparse%20rewards.%20In%20this%20paper%2C%20we%20propose%20BranchGRPO%2C%20a%20novel%20method%20that%0Aintroduces%20a%20branch%20sampling%20policy%20updating%20the%20SDE%20sampling%20process.%20By%0Asharing%20computation%20across%20common%20prefixes%20and%20pruning%20low-reward%20paths%20and%0Aredundant%20depths%2C%20BranchGRPO%20substantially%20lowers%20the%20per-update%20compute%20cost%0Awhile%20maintaining%20or%20improving%20exploration%20diversity.%20This%20work%20makes%20three%0Amain%20contributions%3A%20%281%29%20a%20branch%20sampling%20scheme%20that%20reduces%20rollout%20and%0Atraining%20cost%3B%20%282%29%20a%20tree-based%20advantage%20estimator%20incorporating%20dense%0Aprocess-level%20rewards%3B%20and%20%283%29%20pruning%20strategies%20exploiting%20path%20and%20depth%0Aredundancy%20to%20accelerate%20convergence%20and%20boost%20performance.%20Experiments%20on%0Aimage%20and%20video%20preference%20alignment%20show%20that%20BranchGRPO%20improves%20alignment%0Ascores%20by%2016%25%20over%20strong%20baselines%2C%20while%20cutting%20training%20time%20by%2050%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06040v2&entry.124074799=Read"},
{"title": "Navigating High Dimensional Concept Space with Metalearning", "author": "Max Gupta", "abstract": "  Rapidly learning abstract concepts from limited examples is a hallmark of\nhuman intelligence. This work investigates whether gradient-based meta-learning\ncan equip neural networks with inductive biases for efficient few-shot\nacquisition of discrete concepts. I compare meta-learning methods against a\nsupervised learning baseline on Boolean concepts (logical statements) generated\nby a probabilistic context-free grammar (PCFG). By systematically varying\nconcept dimensionality (number of features) and recursive compositionality\n(depth of grammar recursion), I delineate between complexity regimes in which\nmeta-learning robustly improves few-shot concept learning and regimes in which\nit does not. Meta-learners are much better able to handle compositional\ncomplexity than featural complexity. I highlight some reasons for this with a\nrepresentational analysis of the weights of meta-learners and a loss landscape\nanalysis demonstrating how featural complexity increases the roughness of loss\ntrajectories, allowing curvature-aware optimization to be more effective than\nfirst-order methods. I find improvements in out-of-distribution generalization\non complex concepts by increasing the number of adaptation steps in meta-SGD,\nwhere adaptation acts as a way of encouraging exploration of rougher loss\nbasins. Overall, this work highlights the intricacies of learning compositional\nversus featural complexity in high dimensional concept spaces and provides a\nroad to understanding the role of 2nd order methods and extended gradient\nadaptation in few-shot concept learning.\n", "link": "http://arxiv.org/abs/2508.01948v2", "date": "2025-09-09", "relevancy": 2.1247, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5432}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5257}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20High%20Dimensional%20Concept%20Space%20with%20Metalearning&body=Title%3A%20Navigating%20High%20Dimensional%20Concept%20Space%20with%20Metalearning%0AAuthor%3A%20Max%20Gupta%0AAbstract%3A%20%20%20Rapidly%20learning%20abstract%20concepts%20from%20limited%20examples%20is%20a%20hallmark%20of%0Ahuman%20intelligence.%20This%20work%20investigates%20whether%20gradient-based%20meta-learning%0Acan%20equip%20neural%20networks%20with%20inductive%20biases%20for%20efficient%20few-shot%0Aacquisition%20of%20discrete%20concepts.%20I%20compare%20meta-learning%20methods%20against%20a%0Asupervised%20learning%20baseline%20on%20Boolean%20concepts%20%28logical%20statements%29%20generated%0Aby%20a%20probabilistic%20context-free%20grammar%20%28PCFG%29.%20By%20systematically%20varying%0Aconcept%20dimensionality%20%28number%20of%20features%29%20and%20recursive%20compositionality%0A%28depth%20of%20grammar%20recursion%29%2C%20I%20delineate%20between%20complexity%20regimes%20in%20which%0Ameta-learning%20robustly%20improves%20few-shot%20concept%20learning%20and%20regimes%20in%20which%0Ait%20does%20not.%20Meta-learners%20are%20much%20better%20able%20to%20handle%20compositional%0Acomplexity%20than%20featural%20complexity.%20I%20highlight%20some%20reasons%20for%20this%20with%20a%0Arepresentational%20analysis%20of%20the%20weights%20of%20meta-learners%20and%20a%20loss%20landscape%0Aanalysis%20demonstrating%20how%20featural%20complexity%20increases%20the%20roughness%20of%20loss%0Atrajectories%2C%20allowing%20curvature-aware%20optimization%20to%20be%20more%20effective%20than%0Afirst-order%20methods.%20I%20find%20improvements%20in%20out-of-distribution%20generalization%0Aon%20complex%20concepts%20by%20increasing%20the%20number%20of%20adaptation%20steps%20in%20meta-SGD%2C%0Awhere%20adaptation%20acts%20as%20a%20way%20of%20encouraging%20exploration%20of%20rougher%20loss%0Abasins.%20Overall%2C%20this%20work%20highlights%20the%20intricacies%20of%20learning%20compositional%0Aversus%20featural%20complexity%20in%20high%20dimensional%20concept%20spaces%20and%20provides%20a%0Aroad%20to%20understanding%20the%20role%20of%202nd%20order%20methods%20and%20extended%20gradient%0Aadaptation%20in%20few-shot%20concept%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01948v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520High%2520Dimensional%2520Concept%2520Space%2520with%2520Metalearning%26entry.906535625%3DMax%2520Gupta%26entry.1292438233%3D%2520%2520Rapidly%2520learning%2520abstract%2520concepts%2520from%2520limited%2520examples%2520is%2520a%2520hallmark%2520of%250Ahuman%2520intelligence.%2520This%2520work%2520investigates%2520whether%2520gradient-based%2520meta-learning%250Acan%2520equip%2520neural%2520networks%2520with%2520inductive%2520biases%2520for%2520efficient%2520few-shot%250Aacquisition%2520of%2520discrete%2520concepts.%2520I%2520compare%2520meta-learning%2520methods%2520against%2520a%250Asupervised%2520learning%2520baseline%2520on%2520Boolean%2520concepts%2520%2528logical%2520statements%2529%2520generated%250Aby%2520a%2520probabilistic%2520context-free%2520grammar%2520%2528PCFG%2529.%2520By%2520systematically%2520varying%250Aconcept%2520dimensionality%2520%2528number%2520of%2520features%2529%2520and%2520recursive%2520compositionality%250A%2528depth%2520of%2520grammar%2520recursion%2529%252C%2520I%2520delineate%2520between%2520complexity%2520regimes%2520in%2520which%250Ameta-learning%2520robustly%2520improves%2520few-shot%2520concept%2520learning%2520and%2520regimes%2520in%2520which%250Ait%2520does%2520not.%2520Meta-learners%2520are%2520much%2520better%2520able%2520to%2520handle%2520compositional%250Acomplexity%2520than%2520featural%2520complexity.%2520I%2520highlight%2520some%2520reasons%2520for%2520this%2520with%2520a%250Arepresentational%2520analysis%2520of%2520the%2520weights%2520of%2520meta-learners%2520and%2520a%2520loss%2520landscape%250Aanalysis%2520demonstrating%2520how%2520featural%2520complexity%2520increases%2520the%2520roughness%2520of%2520loss%250Atrajectories%252C%2520allowing%2520curvature-aware%2520optimization%2520to%2520be%2520more%2520effective%2520than%250Afirst-order%2520methods.%2520I%2520find%2520improvements%2520in%2520out-of-distribution%2520generalization%250Aon%2520complex%2520concepts%2520by%2520increasing%2520the%2520number%2520of%2520adaptation%2520steps%2520in%2520meta-SGD%252C%250Awhere%2520adaptation%2520acts%2520as%2520a%2520way%2520of%2520encouraging%2520exploration%2520of%2520rougher%2520loss%250Abasins.%2520Overall%252C%2520this%2520work%2520highlights%2520the%2520intricacies%2520of%2520learning%2520compositional%250Aversus%2520featural%2520complexity%2520in%2520high%2520dimensional%2520concept%2520spaces%2520and%2520provides%2520a%250Aroad%2520to%2520understanding%2520the%2520role%2520of%25202nd%2520order%2520methods%2520and%2520extended%2520gradient%250Aadaptation%2520in%2520few-shot%2520concept%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01948v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20High%20Dimensional%20Concept%20Space%20with%20Metalearning&entry.906535625=Max%20Gupta&entry.1292438233=%20%20Rapidly%20learning%20abstract%20concepts%20from%20limited%20examples%20is%20a%20hallmark%20of%0Ahuman%20intelligence.%20This%20work%20investigates%20whether%20gradient-based%20meta-learning%0Acan%20equip%20neural%20networks%20with%20inductive%20biases%20for%20efficient%20few-shot%0Aacquisition%20of%20discrete%20concepts.%20I%20compare%20meta-learning%20methods%20against%20a%0Asupervised%20learning%20baseline%20on%20Boolean%20concepts%20%28logical%20statements%29%20generated%0Aby%20a%20probabilistic%20context-free%20grammar%20%28PCFG%29.%20By%20systematically%20varying%0Aconcept%20dimensionality%20%28number%20of%20features%29%20and%20recursive%20compositionality%0A%28depth%20of%20grammar%20recursion%29%2C%20I%20delineate%20between%20complexity%20regimes%20in%20which%0Ameta-learning%20robustly%20improves%20few-shot%20concept%20learning%20and%20regimes%20in%20which%0Ait%20does%20not.%20Meta-learners%20are%20much%20better%20able%20to%20handle%20compositional%0Acomplexity%20than%20featural%20complexity.%20I%20highlight%20some%20reasons%20for%20this%20with%20a%0Arepresentational%20analysis%20of%20the%20weights%20of%20meta-learners%20and%20a%20loss%20landscape%0Aanalysis%20demonstrating%20how%20featural%20complexity%20increases%20the%20roughness%20of%20loss%0Atrajectories%2C%20allowing%20curvature-aware%20optimization%20to%20be%20more%20effective%20than%0Afirst-order%20methods.%20I%20find%20improvements%20in%20out-of-distribution%20generalization%0Aon%20complex%20concepts%20by%20increasing%20the%20number%20of%20adaptation%20steps%20in%20meta-SGD%2C%0Awhere%20adaptation%20acts%20as%20a%20way%20of%20encouraging%20exploration%20of%20rougher%20loss%0Abasins.%20Overall%2C%20this%20work%20highlights%20the%20intricacies%20of%20learning%20compositional%0Aversus%20featural%20complexity%20in%20high%20dimensional%20concept%20spaces%20and%20provides%20a%0Aroad%20to%20understanding%20the%20role%20of%202nd%20order%20methods%20and%20extended%20gradient%0Aadaptation%20in%20few-shot%20concept%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01948v2&entry.124074799=Read"},
{"title": "Safeguarding Graph Neural Networks against Topology Inference Attacks", "author": "Jie Fu and Hong Yuan and Zhili Chen and Wendy Hui Wang", "abstract": "  Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, their widespread adoption has raised\nserious privacy concerns. While prior research has primarily focused on\nedge-level privacy, a critical yet underexplored threat lies in topology\nprivacy - the confidentiality of the graph's overall structure. In this work,\nwe present a comprehensive study on topology privacy risks in GNNs, revealing\ntheir vulnerability to graph-level inference attacks. To this end, we propose a\nsuite of Topology Inference Attacks (TIAs) that can reconstruct the structure\nof a target training graph using only black-box access to a GNN model. Our\nfindings show that GNNs are highly susceptible to these attacks, and that\nexisting edge-level differential privacy mechanisms are insufficient as they\neither fail to mitigate the risk or severely compromise model accuracy. To\naddress this challenge, we introduce Private Graph Reconstruction (PGR), a\nnovel defense framework designed to protect topology privacy while maintaining\nmodel accuracy. PGR is formulated as a bi-level optimization problem, where a\nsynthetic training graph is iteratively generated using meta-gradients, and the\nGNN model is concurrently updated based on the evolving graph. Extensive\nexperiments demonstrate that PGR significantly reduces topology leakage with\nminimal impact on model accuracy. Our code is available at\nhttps://github.com/JeffffffFu/PGR.\n", "link": "http://arxiv.org/abs/2509.05429v2", "date": "2025-09-09", "relevancy": 2.1226, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4274}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4238}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safeguarding%20Graph%20Neural%20Networks%20against%20Topology%20Inference%20Attacks&body=Title%3A%20Safeguarding%20Graph%20Neural%20Networks%20against%20Topology%20Inference%20Attacks%0AAuthor%3A%20Jie%20Fu%20and%20Hong%20Yuan%20and%20Zhili%20Chen%20and%20Wendy%20Hui%20Wang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20powerful%20models%20for%20learning%0Afrom%20graph-structured%20data.%20However%2C%20their%20widespread%20adoption%20has%20raised%0Aserious%20privacy%20concerns.%20While%20prior%20research%20has%20primarily%20focused%20on%0Aedge-level%20privacy%2C%20a%20critical%20yet%20underexplored%20threat%20lies%20in%20topology%0Aprivacy%20-%20the%20confidentiality%20of%20the%20graph%27s%20overall%20structure.%20In%20this%20work%2C%0Awe%20present%20a%20comprehensive%20study%20on%20topology%20privacy%20risks%20in%20GNNs%2C%20revealing%0Atheir%20vulnerability%20to%20graph-level%20inference%20attacks.%20To%20this%20end%2C%20we%20propose%20a%0Asuite%20of%20Topology%20Inference%20Attacks%20%28TIAs%29%20that%20can%20reconstruct%20the%20structure%0Aof%20a%20target%20training%20graph%20using%20only%20black-box%20access%20to%20a%20GNN%20model.%20Our%0Afindings%20show%20that%20GNNs%20are%20highly%20susceptible%20to%20these%20attacks%2C%20and%20that%0Aexisting%20edge-level%20differential%20privacy%20mechanisms%20are%20insufficient%20as%20they%0Aeither%20fail%20to%20mitigate%20the%20risk%20or%20severely%20compromise%20model%20accuracy.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20Private%20Graph%20Reconstruction%20%28PGR%29%2C%20a%0Anovel%20defense%20framework%20designed%20to%20protect%20topology%20privacy%20while%20maintaining%0Amodel%20accuracy.%20PGR%20is%20formulated%20as%20a%20bi-level%20optimization%20problem%2C%20where%20a%0Asynthetic%20training%20graph%20is%20iteratively%20generated%20using%20meta-gradients%2C%20and%20the%0AGNN%20model%20is%20concurrently%20updated%20based%20on%20the%20evolving%20graph.%20Extensive%0Aexperiments%20demonstrate%20that%20PGR%20significantly%20reduces%20topology%20leakage%20with%0Aminimal%20impact%20on%20model%20accuracy.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/JeffffffFu/PGR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05429v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafeguarding%2520Graph%2520Neural%2520Networks%2520against%2520Topology%2520Inference%2520Attacks%26entry.906535625%3DJie%2520Fu%2520and%2520Hong%2520Yuan%2520and%2520Zhili%2520Chen%2520and%2520Wendy%2520Hui%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520powerful%2520models%2520for%2520learning%250Afrom%2520graph-structured%2520data.%2520However%252C%2520their%2520widespread%2520adoption%2520has%2520raised%250Aserious%2520privacy%2520concerns.%2520While%2520prior%2520research%2520has%2520primarily%2520focused%2520on%250Aedge-level%2520privacy%252C%2520a%2520critical%2520yet%2520underexplored%2520threat%2520lies%2520in%2520topology%250Aprivacy%2520-%2520the%2520confidentiality%2520of%2520the%2520graph%2527s%2520overall%2520structure.%2520In%2520this%2520work%252C%250Awe%2520present%2520a%2520comprehensive%2520study%2520on%2520topology%2520privacy%2520risks%2520in%2520GNNs%252C%2520revealing%250Atheir%2520vulnerability%2520to%2520graph-level%2520inference%2520attacks.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Asuite%2520of%2520Topology%2520Inference%2520Attacks%2520%2528TIAs%2529%2520that%2520can%2520reconstruct%2520the%2520structure%250Aof%2520a%2520target%2520training%2520graph%2520using%2520only%2520black-box%2520access%2520to%2520a%2520GNN%2520model.%2520Our%250Afindings%2520show%2520that%2520GNNs%2520are%2520highly%2520susceptible%2520to%2520these%2520attacks%252C%2520and%2520that%250Aexisting%2520edge-level%2520differential%2520privacy%2520mechanisms%2520are%2520insufficient%2520as%2520they%250Aeither%2520fail%2520to%2520mitigate%2520the%2520risk%2520or%2520severely%2520compromise%2520model%2520accuracy.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520introduce%2520Private%2520Graph%2520Reconstruction%2520%2528PGR%2529%252C%2520a%250Anovel%2520defense%2520framework%2520designed%2520to%2520protect%2520topology%2520privacy%2520while%2520maintaining%250Amodel%2520accuracy.%2520PGR%2520is%2520formulated%2520as%2520a%2520bi-level%2520optimization%2520problem%252C%2520where%2520a%250Asynthetic%2520training%2520graph%2520is%2520iteratively%2520generated%2520using%2520meta-gradients%252C%2520and%2520the%250AGNN%2520model%2520is%2520concurrently%2520updated%2520based%2520on%2520the%2520evolving%2520graph.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520PGR%2520significantly%2520reduces%2520topology%2520leakage%2520with%250Aminimal%2520impact%2520on%2520model%2520accuracy.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/JeffffffFu/PGR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05429v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safeguarding%20Graph%20Neural%20Networks%20against%20Topology%20Inference%20Attacks&entry.906535625=Jie%20Fu%20and%20Hong%20Yuan%20and%20Zhili%20Chen%20and%20Wendy%20Hui%20Wang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20powerful%20models%20for%20learning%0Afrom%20graph-structured%20data.%20However%2C%20their%20widespread%20adoption%20has%20raised%0Aserious%20privacy%20concerns.%20While%20prior%20research%20has%20primarily%20focused%20on%0Aedge-level%20privacy%2C%20a%20critical%20yet%20underexplored%20threat%20lies%20in%20topology%0Aprivacy%20-%20the%20confidentiality%20of%20the%20graph%27s%20overall%20structure.%20In%20this%20work%2C%0Awe%20present%20a%20comprehensive%20study%20on%20topology%20privacy%20risks%20in%20GNNs%2C%20revealing%0Atheir%20vulnerability%20to%20graph-level%20inference%20attacks.%20To%20this%20end%2C%20we%20propose%20a%0Asuite%20of%20Topology%20Inference%20Attacks%20%28TIAs%29%20that%20can%20reconstruct%20the%20structure%0Aof%20a%20target%20training%20graph%20using%20only%20black-box%20access%20to%20a%20GNN%20model.%20Our%0Afindings%20show%20that%20GNNs%20are%20highly%20susceptible%20to%20these%20attacks%2C%20and%20that%0Aexisting%20edge-level%20differential%20privacy%20mechanisms%20are%20insufficient%20as%20they%0Aeither%20fail%20to%20mitigate%20the%20risk%20or%20severely%20compromise%20model%20accuracy.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20Private%20Graph%20Reconstruction%20%28PGR%29%2C%20a%0Anovel%20defense%20framework%20designed%20to%20protect%20topology%20privacy%20while%20maintaining%0Amodel%20accuracy.%20PGR%20is%20formulated%20as%20a%20bi-level%20optimization%20problem%2C%20where%20a%0Asynthetic%20training%20graph%20is%20iteratively%20generated%20using%20meta-gradients%2C%20and%20the%0AGNN%20model%20is%20concurrently%20updated%20based%20on%20the%20evolving%20graph.%20Extensive%0Aexperiments%20demonstrate%20that%20PGR%20significantly%20reduces%20topology%20leakage%20with%0Aminimal%20impact%20on%20model%20accuracy.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/JeffffffFu/PGR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05429v2&entry.124074799=Read"},
{"title": "Comparative Analysis of Lightweight Deep Learning Models for\n  Memory-Constrained Devices", "author": "Tasnim Shahriar", "abstract": "  This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms.\n", "link": "http://arxiv.org/abs/2505.03303v2", "date": "2025-09-09", "relevancy": 2.1221, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5635}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5431}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Analysis%20of%20Lightweight%20Deep%20Learning%20Models%20for%0A%20%20Memory-Constrained%20Devices&body=Title%3A%20Comparative%20Analysis%20of%20Lightweight%20Deep%20Learning%20Models%20for%0A%20%20Memory-Constrained%20Devices%0AAuthor%3A%20Tasnim%20Shahriar%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comprehensive%20evaluation%20of%20lightweight%20deep%20learning%0Amodels%20for%20image%20classification%2C%20emphasizing%20their%20suitability%20for%20deployment%0Ain%20resource-constrained%20environments%20such%20as%20low-memory%20devices.%20Five%0Astate-of-the-art%20architectures%20-%20MobileNetV3%20Small%2C%20ResNet18%2C%20SqueezeNet%2C%0AEfficientNetV2-S%2C%20and%20ShuffleNetV2%20-%20are%20benchmarked%20across%20three%20diverse%0Adatasets%3A%20CIFAR-10%2C%20CIFAR-100%2C%20and%20Tiny%20ImageNet.%20The%20models%20are%20assessed%20using%0Afour%20key%20performance%20metrics%3A%20classification%20accuracy%2C%20inference%20time%2C%0Afloating-point%20operations%20%28FLOPs%29%2C%20and%20model%20size.%20Additionally%2C%20we%20investigate%0Athe%20impact%20of%20hyperparameter%20tuning%2C%20data%20augmentation%2C%20and%20training%20paradigms%0Aby%20comparing%20pretrained%20models%20with%20scratch-trained%20counterparts%2C%20focusing%20on%0AMobileNetV3%20Small.%20Our%20findings%20reveal%20that%20transfer%20learning%20significantly%0Aenhances%20model%20accuracy%20and%20computational%20efficiency%2C%20particularly%20for%20complex%0Adatasets%20like%20Tiny%20ImageNet.%20EfficientNetV2%20consistently%20achieves%20the%20highest%0Aaccuracy%2C%20while%20MobileNetV3%20offers%20the%20best%20balance%20between%20accuracy%20and%0Aefficiency%2C%20and%20SqueezeNet%20excels%20in%20inference%20speed%20and%20compactness.%20This%0Astudy%20highlights%20critical%20trade-offs%20between%20accuracy%20and%20efficiency%2C%20offering%0Aactionable%20insights%20for%20deploying%20lightweight%20models%20in%20real-world%20applications%0Awhere%20computational%20resources%20are%20limited.%20By%20addressing%20these%20challenges%2C%20this%0Aresearch%20contributes%20to%20optimizing%20deep%20learning%20systems%20for%20edge%20computing%20and%0Amobile%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03303v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Analysis%2520of%2520Lightweight%2520Deep%2520Learning%2520Models%2520for%250A%2520%2520Memory-Constrained%2520Devices%26entry.906535625%3DTasnim%2520Shahriar%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520evaluation%2520of%2520lightweight%2520deep%2520learning%250Amodels%2520for%2520image%2520classification%252C%2520emphasizing%2520their%2520suitability%2520for%2520deployment%250Ain%2520resource-constrained%2520environments%2520such%2520as%2520low-memory%2520devices.%2520Five%250Astate-of-the-art%2520architectures%2520-%2520MobileNetV3%2520Small%252C%2520ResNet18%252C%2520SqueezeNet%252C%250AEfficientNetV2-S%252C%2520and%2520ShuffleNetV2%2520-%2520are%2520benchmarked%2520across%2520three%2520diverse%250Adatasets%253A%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%2520Tiny%2520ImageNet.%2520The%2520models%2520are%2520assessed%2520using%250Afour%2520key%2520performance%2520metrics%253A%2520classification%2520accuracy%252C%2520inference%2520time%252C%250Afloating-point%2520operations%2520%2528FLOPs%2529%252C%2520and%2520model%2520size.%2520Additionally%252C%2520we%2520investigate%250Athe%2520impact%2520of%2520hyperparameter%2520tuning%252C%2520data%2520augmentation%252C%2520and%2520training%2520paradigms%250Aby%2520comparing%2520pretrained%2520models%2520with%2520scratch-trained%2520counterparts%252C%2520focusing%2520on%250AMobileNetV3%2520Small.%2520Our%2520findings%2520reveal%2520that%2520transfer%2520learning%2520significantly%250Aenhances%2520model%2520accuracy%2520and%2520computational%2520efficiency%252C%2520particularly%2520for%2520complex%250Adatasets%2520like%2520Tiny%2520ImageNet.%2520EfficientNetV2%2520consistently%2520achieves%2520the%2520highest%250Aaccuracy%252C%2520while%2520MobileNetV3%2520offers%2520the%2520best%2520balance%2520between%2520accuracy%2520and%250Aefficiency%252C%2520and%2520SqueezeNet%2520excels%2520in%2520inference%2520speed%2520and%2520compactness.%2520This%250Astudy%2520highlights%2520critical%2520trade-offs%2520between%2520accuracy%2520and%2520efficiency%252C%2520offering%250Aactionable%2520insights%2520for%2520deploying%2520lightweight%2520models%2520in%2520real-world%2520applications%250Awhere%2520computational%2520resources%2520are%2520limited.%2520By%2520addressing%2520these%2520challenges%252C%2520this%250Aresearch%2520contributes%2520to%2520optimizing%2520deep%2520learning%2520systems%2520for%2520edge%2520computing%2520and%250Amobile%2520platforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03303v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Analysis%20of%20Lightweight%20Deep%20Learning%20Models%20for%0A%20%20Memory-Constrained%20Devices&entry.906535625=Tasnim%20Shahriar&entry.1292438233=%20%20This%20paper%20presents%20a%20comprehensive%20evaluation%20of%20lightweight%20deep%20learning%0Amodels%20for%20image%20classification%2C%20emphasizing%20their%20suitability%20for%20deployment%0Ain%20resource-constrained%20environments%20such%20as%20low-memory%20devices.%20Five%0Astate-of-the-art%20architectures%20-%20MobileNetV3%20Small%2C%20ResNet18%2C%20SqueezeNet%2C%0AEfficientNetV2-S%2C%20and%20ShuffleNetV2%20-%20are%20benchmarked%20across%20three%20diverse%0Adatasets%3A%20CIFAR-10%2C%20CIFAR-100%2C%20and%20Tiny%20ImageNet.%20The%20models%20are%20assessed%20using%0Afour%20key%20performance%20metrics%3A%20classification%20accuracy%2C%20inference%20time%2C%0Afloating-point%20operations%20%28FLOPs%29%2C%20and%20model%20size.%20Additionally%2C%20we%20investigate%0Athe%20impact%20of%20hyperparameter%20tuning%2C%20data%20augmentation%2C%20and%20training%20paradigms%0Aby%20comparing%20pretrained%20models%20with%20scratch-trained%20counterparts%2C%20focusing%20on%0AMobileNetV3%20Small.%20Our%20findings%20reveal%20that%20transfer%20learning%20significantly%0Aenhances%20model%20accuracy%20and%20computational%20efficiency%2C%20particularly%20for%20complex%0Adatasets%20like%20Tiny%20ImageNet.%20EfficientNetV2%20consistently%20achieves%20the%20highest%0Aaccuracy%2C%20while%20MobileNetV3%20offers%20the%20best%20balance%20between%20accuracy%20and%0Aefficiency%2C%20and%20SqueezeNet%20excels%20in%20inference%20speed%20and%20compactness.%20This%0Astudy%20highlights%20critical%20trade-offs%20between%20accuracy%20and%20efficiency%2C%20offering%0Aactionable%20insights%20for%20deploying%20lightweight%20models%20in%20real-world%20applications%0Awhere%20computational%20resources%20are%20limited.%20By%20addressing%20these%20challenges%2C%20this%0Aresearch%20contributes%20to%20optimizing%20deep%20learning%20systems%20for%20edge%20computing%20and%0Amobile%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03303v2&entry.124074799=Read"},
{"title": "D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via\n  Layer-to-head Attention Diagnostics", "author": "Tiancheng Yang and Lin Zhang and Jiaye Lin and Guimin Hu and Di Wang and Lijie Hu", "abstract": "  Multimodal Large Language Models (MLLMs) achieve strong performance on tasks\nlike image captioning and visual question answering, but remain prone to\nhallucinations, where generated text conflicts with the visual input. Prior\nwork links this partly to insufficient visual attention, but existing\nattention-based detectors and mitigation typically apply uniform adjustments\nacross layers and heads, obscuring where errors originate. In this paper, we\nfirst show these methods fail to accurately localize problematic layers. Then,\nwe introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags\nanomalous layers, and Image Attention Focus (IAF) which scores attention heads\nwithin those layers. Analysis shows that LIAE pinpoints faulty layers and IAF\nreliably ranks heads that warrant correction. Guided by these signals, we\npropose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a\ntask-agnostic, attention-guided method that dynamically localizes and corrects\nerrors during inference with negligible overhead. Results show our D-LEAF\ndelivers a 53% relative improvement on standard captioning benchmarks, and on\nVQA both accuracy and F1-score improve by approximately 4%, substantially\nsuppressing hallucinations while preserving efficiency.\n", "link": "http://arxiv.org/abs/2509.07864v1", "date": "2025-09-09", "relevancy": 2.1161, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5266}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D-LEAF%3A%20Localizing%20and%20Correcting%20Hallucinations%20in%20Multimodal%20LLMs%20via%0A%20%20Layer-to-head%20Attention%20Diagnostics&body=Title%3A%20D-LEAF%3A%20Localizing%20and%20Correcting%20Hallucinations%20in%20Multimodal%20LLMs%20via%0A%20%20Layer-to-head%20Attention%20Diagnostics%0AAuthor%3A%20Tiancheng%20Yang%20and%20Lin%20Zhang%20and%20Jiaye%20Lin%20and%20Guimin%20Hu%20and%20Di%20Wang%20and%20Lijie%20Hu%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20achieve%20strong%20performance%20on%20tasks%0Alike%20image%20captioning%20and%20visual%20question%20answering%2C%20but%20remain%20prone%20to%0Ahallucinations%2C%20where%20generated%20text%20conflicts%20with%20the%20visual%20input.%20Prior%0Awork%20links%20this%20partly%20to%20insufficient%20visual%20attention%2C%20but%20existing%0Aattention-based%20detectors%20and%20mitigation%20typically%20apply%20uniform%20adjustments%0Aacross%20layers%20and%20heads%2C%20obscuring%20where%20errors%20originate.%20In%20this%20paper%2C%20we%0Afirst%20show%20these%20methods%20fail%20to%20accurately%20localize%20problematic%20layers.%20Then%2C%0Awe%20introduce%20two%20diagnostics%3A%20Layer%20Image%20Attention%20Entropy%20%28LIAE%29%20which%20flags%0Aanomalous%20layers%2C%20and%20Image%20Attention%20Focus%20%28IAF%29%20which%20scores%20attention%20heads%0Awithin%20those%20layers.%20Analysis%20shows%20that%20LIAE%20pinpoints%20faulty%20layers%20and%20IAF%0Areliably%20ranks%20heads%20that%20warrant%20correction.%20Guided%20by%20these%20signals%2C%20we%0Apropose%20Dynamic%20Layer-wise%20Entropy%20and%20Attention%20Fusion%20%28D-LEAF%29%2C%20a%0Atask-agnostic%2C%20attention-guided%20method%20that%20dynamically%20localizes%20and%20corrects%0Aerrors%20during%20inference%20with%20negligible%20overhead.%20Results%20show%20our%20D-LEAF%0Adelivers%20a%2053%25%20relative%20improvement%20on%20standard%20captioning%20benchmarks%2C%20and%20on%0AVQA%20both%20accuracy%20and%20F1-score%20improve%20by%20approximately%204%25%2C%20substantially%0Asuppressing%20hallucinations%20while%20preserving%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD-LEAF%253A%2520Localizing%2520and%2520Correcting%2520Hallucinations%2520in%2520Multimodal%2520LLMs%2520via%250A%2520%2520Layer-to-head%2520Attention%2520Diagnostics%26entry.906535625%3DTiancheng%2520Yang%2520and%2520Lin%2520Zhang%2520and%2520Jiaye%2520Lin%2520and%2520Guimin%2520Hu%2520and%2520Di%2520Wang%2520and%2520Lijie%2520Hu%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520achieve%2520strong%2520performance%2520on%2520tasks%250Alike%2520image%2520captioning%2520and%2520visual%2520question%2520answering%252C%2520but%2520remain%2520prone%2520to%250Ahallucinations%252C%2520where%2520generated%2520text%2520conflicts%2520with%2520the%2520visual%2520input.%2520Prior%250Awork%2520links%2520this%2520partly%2520to%2520insufficient%2520visual%2520attention%252C%2520but%2520existing%250Aattention-based%2520detectors%2520and%2520mitigation%2520typically%2520apply%2520uniform%2520adjustments%250Aacross%2520layers%2520and%2520heads%252C%2520obscuring%2520where%2520errors%2520originate.%2520In%2520this%2520paper%252C%2520we%250Afirst%2520show%2520these%2520methods%2520fail%2520to%2520accurately%2520localize%2520problematic%2520layers.%2520Then%252C%250Awe%2520introduce%2520two%2520diagnostics%253A%2520Layer%2520Image%2520Attention%2520Entropy%2520%2528LIAE%2529%2520which%2520flags%250Aanomalous%2520layers%252C%2520and%2520Image%2520Attention%2520Focus%2520%2528IAF%2529%2520which%2520scores%2520attention%2520heads%250Awithin%2520those%2520layers.%2520Analysis%2520shows%2520that%2520LIAE%2520pinpoints%2520faulty%2520layers%2520and%2520IAF%250Areliably%2520ranks%2520heads%2520that%2520warrant%2520correction.%2520Guided%2520by%2520these%2520signals%252C%2520we%250Apropose%2520Dynamic%2520Layer-wise%2520Entropy%2520and%2520Attention%2520Fusion%2520%2528D-LEAF%2529%252C%2520a%250Atask-agnostic%252C%2520attention-guided%2520method%2520that%2520dynamically%2520localizes%2520and%2520corrects%250Aerrors%2520during%2520inference%2520with%2520negligible%2520overhead.%2520Results%2520show%2520our%2520D-LEAF%250Adelivers%2520a%252053%2525%2520relative%2520improvement%2520on%2520standard%2520captioning%2520benchmarks%252C%2520and%2520on%250AVQA%2520both%2520accuracy%2520and%2520F1-score%2520improve%2520by%2520approximately%25204%2525%252C%2520substantially%250Asuppressing%2520hallucinations%2520while%2520preserving%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D-LEAF%3A%20Localizing%20and%20Correcting%20Hallucinations%20in%20Multimodal%20LLMs%20via%0A%20%20Layer-to-head%20Attention%20Diagnostics&entry.906535625=Tiancheng%20Yang%20and%20Lin%20Zhang%20and%20Jiaye%20Lin%20and%20Guimin%20Hu%20and%20Di%20Wang%20and%20Lijie%20Hu&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20achieve%20strong%20performance%20on%20tasks%0Alike%20image%20captioning%20and%20visual%20question%20answering%2C%20but%20remain%20prone%20to%0Ahallucinations%2C%20where%20generated%20text%20conflicts%20with%20the%20visual%20input.%20Prior%0Awork%20links%20this%20partly%20to%20insufficient%20visual%20attention%2C%20but%20existing%0Aattention-based%20detectors%20and%20mitigation%20typically%20apply%20uniform%20adjustments%0Aacross%20layers%20and%20heads%2C%20obscuring%20where%20errors%20originate.%20In%20this%20paper%2C%20we%0Afirst%20show%20these%20methods%20fail%20to%20accurately%20localize%20problematic%20layers.%20Then%2C%0Awe%20introduce%20two%20diagnostics%3A%20Layer%20Image%20Attention%20Entropy%20%28LIAE%29%20which%20flags%0Aanomalous%20layers%2C%20and%20Image%20Attention%20Focus%20%28IAF%29%20which%20scores%20attention%20heads%0Awithin%20those%20layers.%20Analysis%20shows%20that%20LIAE%20pinpoints%20faulty%20layers%20and%20IAF%0Areliably%20ranks%20heads%20that%20warrant%20correction.%20Guided%20by%20these%20signals%2C%20we%0Apropose%20Dynamic%20Layer-wise%20Entropy%20and%20Attention%20Fusion%20%28D-LEAF%29%2C%20a%0Atask-agnostic%2C%20attention-guided%20method%20that%20dynamically%20localizes%20and%20corrects%0Aerrors%20during%20inference%20with%20negligible%20overhead.%20Results%20show%20our%20D-LEAF%0Adelivers%20a%2053%25%20relative%20improvement%20on%20standard%20captioning%20benchmarks%2C%20and%20on%0AVQA%20both%20accuracy%20and%20F1-score%20improve%20by%20approximately%204%25%2C%20substantially%0Asuppressing%20hallucinations%20while%20preserving%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07864v1&entry.124074799=Read"},
{"title": "Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic\n  Strategy for YOLOv10s", "author": "Mahmudul Islam Masum and Miad Islam and Arif I. Sarwat", "abstract": "  As local AI grows in popularity, there is a critical gap between the\nbenchmark performance of object detectors and their practical viability on\nconsumer-grade hardware. While models like YOLOv10s promise real-time speeds,\nthese metrics are typically achieved on high-power, desktop-class GPUs. This\npaper reveals that on resource-constrained systems, such as laptops with RTX\n4060 GPUs, performance is not compute-bound but is instead dominated by\nsystem-level bottlenecks, as illustrated by a simple bottleneck test. To\novercome this hardware-level constraint, we introduce a Two-Pass Adaptive\nInference algorithm, a model-independent approach that requires no\narchitectural changes. This study mainly focuses on adaptive inference\nstrategies and undertakes a comparative analysis of architectural early-exit\nand resolution-adaptive routing, highlighting their respective trade-offs\nwithin a unified evaluation framework. The system uses a fast, low-resolution\npass and only escalates to a high-resolution model pass when detection\nconfidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x\nspeedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%.\nThis work provides a practical and reproducible blueprint for deploying\nhigh-performance, real-time AI on consumer-grade devices by shifting the focus\nfrom pure model optimization to hardware-aware inference strategies that\nmaximize throughput.\n", "link": "http://arxiv.org/abs/2509.07928v1", "date": "2025-09-09", "relevancy": 2.1015, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5476}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5225}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Local%20AI%20on%20Consumer%20GPUs%3A%20A%20Hardware-Aware%20Dynamic%0A%20%20Strategy%20for%20YOLOv10s&body=Title%3A%20Accelerating%20Local%20AI%20on%20Consumer%20GPUs%3A%20A%20Hardware-Aware%20Dynamic%0A%20%20Strategy%20for%20YOLOv10s%0AAuthor%3A%20Mahmudul%20Islam%20Masum%20and%20Miad%20Islam%20and%20Arif%20I.%20Sarwat%0AAbstract%3A%20%20%20As%20local%20AI%20grows%20in%20popularity%2C%20there%20is%20a%20critical%20gap%20between%20the%0Abenchmark%20performance%20of%20object%20detectors%20and%20their%20practical%20viability%20on%0Aconsumer-grade%20hardware.%20While%20models%20like%20YOLOv10s%20promise%20real-time%20speeds%2C%0Athese%20metrics%20are%20typically%20achieved%20on%20high-power%2C%20desktop-class%20GPUs.%20This%0Apaper%20reveals%20that%20on%20resource-constrained%20systems%2C%20such%20as%20laptops%20with%20RTX%0A4060%20GPUs%2C%20performance%20is%20not%20compute-bound%20but%20is%20instead%20dominated%20by%0Asystem-level%20bottlenecks%2C%20as%20illustrated%20by%20a%20simple%20bottleneck%20test.%20To%0Aovercome%20this%20hardware-level%20constraint%2C%20we%20introduce%20a%20Two-Pass%20Adaptive%0AInference%20algorithm%2C%20a%20model-independent%20approach%20that%20requires%20no%0Aarchitectural%20changes.%20This%20study%20mainly%20focuses%20on%20adaptive%20inference%0Astrategies%20and%20undertakes%20a%20comparative%20analysis%20of%20architectural%20early-exit%0Aand%20resolution-adaptive%20routing%2C%20highlighting%20their%20respective%20trade-offs%0Awithin%20a%20unified%20evaluation%20framework.%20The%20system%20uses%20a%20fast%2C%20low-resolution%0Apass%20and%20only%20escalates%20to%20a%20high-resolution%20model%20pass%20when%20detection%0Aconfidence%20is%20low.%20On%20a%205000-image%20COCO%20dataset%2C%20our%20method%20achieves%20a%201.85x%0Aspeedup%20over%20a%20PyTorch%20Early-Exit%20baseline%2C%20with%20a%20modest%20mAP%20loss%20of%205.51%25.%0AThis%20work%20provides%20a%20practical%20and%20reproducible%20blueprint%20for%20deploying%0Ahigh-performance%2C%20real-time%20AI%20on%20consumer-grade%20devices%20by%20shifting%20the%20focus%0Afrom%20pure%20model%20optimization%20to%20hardware-aware%20inference%20strategies%20that%0Amaximize%20throughput.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Local%2520AI%2520on%2520Consumer%2520GPUs%253A%2520A%2520Hardware-Aware%2520Dynamic%250A%2520%2520Strategy%2520for%2520YOLOv10s%26entry.906535625%3DMahmudul%2520Islam%2520Masum%2520and%2520Miad%2520Islam%2520and%2520Arif%2520I.%2520Sarwat%26entry.1292438233%3D%2520%2520As%2520local%2520AI%2520grows%2520in%2520popularity%252C%2520there%2520is%2520a%2520critical%2520gap%2520between%2520the%250Abenchmark%2520performance%2520of%2520object%2520detectors%2520and%2520their%2520practical%2520viability%2520on%250Aconsumer-grade%2520hardware.%2520While%2520models%2520like%2520YOLOv10s%2520promise%2520real-time%2520speeds%252C%250Athese%2520metrics%2520are%2520typically%2520achieved%2520on%2520high-power%252C%2520desktop-class%2520GPUs.%2520This%250Apaper%2520reveals%2520that%2520on%2520resource-constrained%2520systems%252C%2520such%2520as%2520laptops%2520with%2520RTX%250A4060%2520GPUs%252C%2520performance%2520is%2520not%2520compute-bound%2520but%2520is%2520instead%2520dominated%2520by%250Asystem-level%2520bottlenecks%252C%2520as%2520illustrated%2520by%2520a%2520simple%2520bottleneck%2520test.%2520To%250Aovercome%2520this%2520hardware-level%2520constraint%252C%2520we%2520introduce%2520a%2520Two-Pass%2520Adaptive%250AInference%2520algorithm%252C%2520a%2520model-independent%2520approach%2520that%2520requires%2520no%250Aarchitectural%2520changes.%2520This%2520study%2520mainly%2520focuses%2520on%2520adaptive%2520inference%250Astrategies%2520and%2520undertakes%2520a%2520comparative%2520analysis%2520of%2520architectural%2520early-exit%250Aand%2520resolution-adaptive%2520routing%252C%2520highlighting%2520their%2520respective%2520trade-offs%250Awithin%2520a%2520unified%2520evaluation%2520framework.%2520The%2520system%2520uses%2520a%2520fast%252C%2520low-resolution%250Apass%2520and%2520only%2520escalates%2520to%2520a%2520high-resolution%2520model%2520pass%2520when%2520detection%250Aconfidence%2520is%2520low.%2520On%2520a%25205000-image%2520COCO%2520dataset%252C%2520our%2520method%2520achieves%2520a%25201.85x%250Aspeedup%2520over%2520a%2520PyTorch%2520Early-Exit%2520baseline%252C%2520with%2520a%2520modest%2520mAP%2520loss%2520of%25205.51%2525.%250AThis%2520work%2520provides%2520a%2520practical%2520and%2520reproducible%2520blueprint%2520for%2520deploying%250Ahigh-performance%252C%2520real-time%2520AI%2520on%2520consumer-grade%2520devices%2520by%2520shifting%2520the%2520focus%250Afrom%2520pure%2520model%2520optimization%2520to%2520hardware-aware%2520inference%2520strategies%2520that%250Amaximize%2520throughput.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Local%20AI%20on%20Consumer%20GPUs%3A%20A%20Hardware-Aware%20Dynamic%0A%20%20Strategy%20for%20YOLOv10s&entry.906535625=Mahmudul%20Islam%20Masum%20and%20Miad%20Islam%20and%20Arif%20I.%20Sarwat&entry.1292438233=%20%20As%20local%20AI%20grows%20in%20popularity%2C%20there%20is%20a%20critical%20gap%20between%20the%0Abenchmark%20performance%20of%20object%20detectors%20and%20their%20practical%20viability%20on%0Aconsumer-grade%20hardware.%20While%20models%20like%20YOLOv10s%20promise%20real-time%20speeds%2C%0Athese%20metrics%20are%20typically%20achieved%20on%20high-power%2C%20desktop-class%20GPUs.%20This%0Apaper%20reveals%20that%20on%20resource-constrained%20systems%2C%20such%20as%20laptops%20with%20RTX%0A4060%20GPUs%2C%20performance%20is%20not%20compute-bound%20but%20is%20instead%20dominated%20by%0Asystem-level%20bottlenecks%2C%20as%20illustrated%20by%20a%20simple%20bottleneck%20test.%20To%0Aovercome%20this%20hardware-level%20constraint%2C%20we%20introduce%20a%20Two-Pass%20Adaptive%0AInference%20algorithm%2C%20a%20model-independent%20approach%20that%20requires%20no%0Aarchitectural%20changes.%20This%20study%20mainly%20focuses%20on%20adaptive%20inference%0Astrategies%20and%20undertakes%20a%20comparative%20analysis%20of%20architectural%20early-exit%0Aand%20resolution-adaptive%20routing%2C%20highlighting%20their%20respective%20trade-offs%0Awithin%20a%20unified%20evaluation%20framework.%20The%20system%20uses%20a%20fast%2C%20low-resolution%0Apass%20and%20only%20escalates%20to%20a%20high-resolution%20model%20pass%20when%20detection%0Aconfidence%20is%20low.%20On%20a%205000-image%20COCO%20dataset%2C%20our%20method%20achieves%20a%201.85x%0Aspeedup%20over%20a%20PyTorch%20Early-Exit%20baseline%2C%20with%20a%20modest%20mAP%20loss%20of%205.51%25.%0AThis%20work%20provides%20a%20practical%20and%20reproducible%20blueprint%20for%20deploying%0Ahigh-performance%2C%20real-time%20AI%20on%20consumer-grade%20devices%20by%20shifting%20the%20focus%0Afrom%20pure%20model%20optimization%20to%20hardware-aware%20inference%20strategies%20that%0Amaximize%20throughput.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07928v1&entry.124074799=Read"},
{"title": "Enhanced SegNet with Integrated Grad-CAM for Interpretable Retinal Layer\n  Segmentation in OCT Images", "author": "S M Asiful Islam Saky and Ugyen Tshering", "abstract": "  Optical Coherence Tomography (OCT) is essential for diagnosing conditions\nsuch as glaucoma, diabetic retinopathy, and age-related macular degeneration.\nAccurate retinal layer segmentation enables quantitative biomarkers critical\nfor clinical decision-making, but manual segmentation is time-consuming and\nvariable, while conventional deep learning models often lack interpretability.\nThis work proposes an improved SegNet-based deep learning framework for\nautomated and interpretable retinal layer segmentation. Architectural\ninnovations, including modified pooling strategies, enhance feature extraction\nfrom noisy OCT images, while a hybrid loss function combining categorical\ncross-entropy and Dice loss improves performance for thin and imbalanced\nretinal layers. Gradient-weighted Class Activation Mapping (Grad-CAM) is\nintegrated to provide visual explanations, allowing clinical validation of\nmodel decisions. Trained and validated on the Duke OCT dataset, the framework\nachieved 95.77% validation accuracy, a Dice coefficient of 0.9446, and a\nJaccard Index (IoU) of 0.8951. Class-wise results confirmed robust performance\nacross most layers, with challenges remaining for thinner boundaries. Grad-CAM\nvisualizations highlighted anatomically relevant regions, aligning segmentation\nwith clinical biomarkers and improving transparency. By combining architectural\nimprovements, a customized hybrid loss, and explainable AI, this study delivers\na high-performing SegNet-based framework that bridges the gap between accuracy\nand interpretability. The approach offers strong potential for standardizing\nOCT analysis, enhancing diagnostic efficiency, and fostering clinical trust in\nAI-driven ophthalmic tools.\n", "link": "http://arxiv.org/abs/2509.07795v1", "date": "2025-09-09", "relevancy": 2.0869, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5253}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5215}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20SegNet%20with%20Integrated%20Grad-CAM%20for%20Interpretable%20Retinal%20Layer%0A%20%20Segmentation%20in%20OCT%20Images&body=Title%3A%20Enhanced%20SegNet%20with%20Integrated%20Grad-CAM%20for%20Interpretable%20Retinal%20Layer%0A%20%20Segmentation%20in%20OCT%20Images%0AAuthor%3A%20S%20M%20Asiful%20Islam%20Saky%20and%20Ugyen%20Tshering%0AAbstract%3A%20%20%20Optical%20Coherence%20Tomography%20%28OCT%29%20is%20essential%20for%20diagnosing%20conditions%0Asuch%20as%20glaucoma%2C%20diabetic%20retinopathy%2C%20and%20age-related%20macular%20degeneration.%0AAccurate%20retinal%20layer%20segmentation%20enables%20quantitative%20biomarkers%20critical%0Afor%20clinical%20decision-making%2C%20but%20manual%20segmentation%20is%20time-consuming%20and%0Avariable%2C%20while%20conventional%20deep%20learning%20models%20often%20lack%20interpretability.%0AThis%20work%20proposes%20an%20improved%20SegNet-based%20deep%20learning%20framework%20for%0Aautomated%20and%20interpretable%20retinal%20layer%20segmentation.%20Architectural%0Ainnovations%2C%20including%20modified%20pooling%20strategies%2C%20enhance%20feature%20extraction%0Afrom%20noisy%20OCT%20images%2C%20while%20a%20hybrid%20loss%20function%20combining%20categorical%0Across-entropy%20and%20Dice%20loss%20improves%20performance%20for%20thin%20and%20imbalanced%0Aretinal%20layers.%20Gradient-weighted%20Class%20Activation%20Mapping%20%28Grad-CAM%29%20is%0Aintegrated%20to%20provide%20visual%20explanations%2C%20allowing%20clinical%20validation%20of%0Amodel%20decisions.%20Trained%20and%20validated%20on%20the%20Duke%20OCT%20dataset%2C%20the%20framework%0Aachieved%2095.77%25%20validation%20accuracy%2C%20a%20Dice%20coefficient%20of%200.9446%2C%20and%20a%0AJaccard%20Index%20%28IoU%29%20of%200.8951.%20Class-wise%20results%20confirmed%20robust%20performance%0Aacross%20most%20layers%2C%20with%20challenges%20remaining%20for%20thinner%20boundaries.%20Grad-CAM%0Avisualizations%20highlighted%20anatomically%20relevant%20regions%2C%20aligning%20segmentation%0Awith%20clinical%20biomarkers%20and%20improving%20transparency.%20By%20combining%20architectural%0Aimprovements%2C%20a%20customized%20hybrid%20loss%2C%20and%20explainable%20AI%2C%20this%20study%20delivers%0Aa%20high-performing%20SegNet-based%20framework%20that%20bridges%20the%20gap%20between%20accuracy%0Aand%20interpretability.%20The%20approach%20offers%20strong%20potential%20for%20standardizing%0AOCT%20analysis%2C%20enhancing%20diagnostic%20efficiency%2C%20and%20fostering%20clinical%20trust%20in%0AAI-driven%20ophthalmic%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520SegNet%2520with%2520Integrated%2520Grad-CAM%2520for%2520Interpretable%2520Retinal%2520Layer%250A%2520%2520Segmentation%2520in%2520OCT%2520Images%26entry.906535625%3DS%2520M%2520Asiful%2520Islam%2520Saky%2520and%2520Ugyen%2520Tshering%26entry.1292438233%3D%2520%2520Optical%2520Coherence%2520Tomography%2520%2528OCT%2529%2520is%2520essential%2520for%2520diagnosing%2520conditions%250Asuch%2520as%2520glaucoma%252C%2520diabetic%2520retinopathy%252C%2520and%2520age-related%2520macular%2520degeneration.%250AAccurate%2520retinal%2520layer%2520segmentation%2520enables%2520quantitative%2520biomarkers%2520critical%250Afor%2520clinical%2520decision-making%252C%2520but%2520manual%2520segmentation%2520is%2520time-consuming%2520and%250Avariable%252C%2520while%2520conventional%2520deep%2520learning%2520models%2520often%2520lack%2520interpretability.%250AThis%2520work%2520proposes%2520an%2520improved%2520SegNet-based%2520deep%2520learning%2520framework%2520for%250Aautomated%2520and%2520interpretable%2520retinal%2520layer%2520segmentation.%2520Architectural%250Ainnovations%252C%2520including%2520modified%2520pooling%2520strategies%252C%2520enhance%2520feature%2520extraction%250Afrom%2520noisy%2520OCT%2520images%252C%2520while%2520a%2520hybrid%2520loss%2520function%2520combining%2520categorical%250Across-entropy%2520and%2520Dice%2520loss%2520improves%2520performance%2520for%2520thin%2520and%2520imbalanced%250Aretinal%2520layers.%2520Gradient-weighted%2520Class%2520Activation%2520Mapping%2520%2528Grad-CAM%2529%2520is%250Aintegrated%2520to%2520provide%2520visual%2520explanations%252C%2520allowing%2520clinical%2520validation%2520of%250Amodel%2520decisions.%2520Trained%2520and%2520validated%2520on%2520the%2520Duke%2520OCT%2520dataset%252C%2520the%2520framework%250Aachieved%252095.77%2525%2520validation%2520accuracy%252C%2520a%2520Dice%2520coefficient%2520of%25200.9446%252C%2520and%2520a%250AJaccard%2520Index%2520%2528IoU%2529%2520of%25200.8951.%2520Class-wise%2520results%2520confirmed%2520robust%2520performance%250Aacross%2520most%2520layers%252C%2520with%2520challenges%2520remaining%2520for%2520thinner%2520boundaries.%2520Grad-CAM%250Avisualizations%2520highlighted%2520anatomically%2520relevant%2520regions%252C%2520aligning%2520segmentation%250Awith%2520clinical%2520biomarkers%2520and%2520improving%2520transparency.%2520By%2520combining%2520architectural%250Aimprovements%252C%2520a%2520customized%2520hybrid%2520loss%252C%2520and%2520explainable%2520AI%252C%2520this%2520study%2520delivers%250Aa%2520high-performing%2520SegNet-based%2520framework%2520that%2520bridges%2520the%2520gap%2520between%2520accuracy%250Aand%2520interpretability.%2520The%2520approach%2520offers%2520strong%2520potential%2520for%2520standardizing%250AOCT%2520analysis%252C%2520enhancing%2520diagnostic%2520efficiency%252C%2520and%2520fostering%2520clinical%2520trust%2520in%250AAI-driven%2520ophthalmic%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20SegNet%20with%20Integrated%20Grad-CAM%20for%20Interpretable%20Retinal%20Layer%0A%20%20Segmentation%20in%20OCT%20Images&entry.906535625=S%20M%20Asiful%20Islam%20Saky%20and%20Ugyen%20Tshering&entry.1292438233=%20%20Optical%20Coherence%20Tomography%20%28OCT%29%20is%20essential%20for%20diagnosing%20conditions%0Asuch%20as%20glaucoma%2C%20diabetic%20retinopathy%2C%20and%20age-related%20macular%20degeneration.%0AAccurate%20retinal%20layer%20segmentation%20enables%20quantitative%20biomarkers%20critical%0Afor%20clinical%20decision-making%2C%20but%20manual%20segmentation%20is%20time-consuming%20and%0Avariable%2C%20while%20conventional%20deep%20learning%20models%20often%20lack%20interpretability.%0AThis%20work%20proposes%20an%20improved%20SegNet-based%20deep%20learning%20framework%20for%0Aautomated%20and%20interpretable%20retinal%20layer%20segmentation.%20Architectural%0Ainnovations%2C%20including%20modified%20pooling%20strategies%2C%20enhance%20feature%20extraction%0Afrom%20noisy%20OCT%20images%2C%20while%20a%20hybrid%20loss%20function%20combining%20categorical%0Across-entropy%20and%20Dice%20loss%20improves%20performance%20for%20thin%20and%20imbalanced%0Aretinal%20layers.%20Gradient-weighted%20Class%20Activation%20Mapping%20%28Grad-CAM%29%20is%0Aintegrated%20to%20provide%20visual%20explanations%2C%20allowing%20clinical%20validation%20of%0Amodel%20decisions.%20Trained%20and%20validated%20on%20the%20Duke%20OCT%20dataset%2C%20the%20framework%0Aachieved%2095.77%25%20validation%20accuracy%2C%20a%20Dice%20coefficient%20of%200.9446%2C%20and%20a%0AJaccard%20Index%20%28IoU%29%20of%200.8951.%20Class-wise%20results%20confirmed%20robust%20performance%0Aacross%20most%20layers%2C%20with%20challenges%20remaining%20for%20thinner%20boundaries.%20Grad-CAM%0Avisualizations%20highlighted%20anatomically%20relevant%20regions%2C%20aligning%20segmentation%0Awith%20clinical%20biomarkers%20and%20improving%20transparency.%20By%20combining%20architectural%0Aimprovements%2C%20a%20customized%20hybrid%20loss%2C%20and%20explainable%20AI%2C%20this%20study%20delivers%0Aa%20high-performing%20SegNet-based%20framework%20that%20bridges%20the%20gap%20between%20accuracy%0Aand%20interpretability.%20The%20approach%20offers%20strong%20potential%20for%20standardizing%0AOCT%20analysis%2C%20enhancing%20diagnostic%20efficiency%2C%20and%20fostering%20clinical%20trust%20in%0AAI-driven%20ophthalmic%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07795v1&entry.124074799=Read"},
{"title": "SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless\n  Image Compression", "author": "Chunhang Zheng and Zichang Ren and Dou Li", "abstract": "  Recently, learned image compression has attracted considerable attention due\nto its superior performance over traditional methods. However, most existing\napproaches employ a single entropy model to estimate the probability\ndistribution of pixel values across the entire image, which limits their\nability to capture the diverse statistical characteristics of different\nsemantic regions. To overcome this limitation, we propose Segmentation-Assisted\nMulti-Entropy Models for Lossless Image Compression (SEEC). Our framework\nutilizes semantic segmentation to guide the selection and adaptation of\nmultiple entropy models, enabling more accurate probability distribution\nestimation for distinct semantic regions. Specifically, SEEC first extracts\nimage features and then applies semantic segmentation to identify different\nregions, each assigned a specialized entropy model to better capture its unique\nstatistical properties. Finally, a multi-channel discrete logistic mixture\nlikelihood is employed to model the pixel value distributions effectively.\nExperimental results on benchmark datasets demonstrate that SEEC achieves\nstate-of-the-art compression ratios while introducing only minimal encoding and\ndecoding latency. With superior performance, the proposed model also supports\nRegions of Interest (ROIs) coding condition on the provided segmentation mask.\nOur code is available at https://github.com/chunbaobao/SEEC.\n", "link": "http://arxiv.org/abs/2509.07704v1", "date": "2025-09-09", "relevancy": 2.0759, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5229}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5182}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEEC%3A%20Segmentation-Assisted%20Multi-Entropy%20Models%20for%20Learned%20Lossless%0A%20%20Image%20Compression&body=Title%3A%20SEEC%3A%20Segmentation-Assisted%20Multi-Entropy%20Models%20for%20Learned%20Lossless%0A%20%20Image%20Compression%0AAuthor%3A%20Chunhang%20Zheng%20and%20Zichang%20Ren%20and%20Dou%20Li%0AAbstract%3A%20%20%20Recently%2C%20learned%20image%20compression%20has%20attracted%20considerable%20attention%20due%0Ato%20its%20superior%20performance%20over%20traditional%20methods.%20However%2C%20most%20existing%0Aapproaches%20employ%20a%20single%20entropy%20model%20to%20estimate%20the%20probability%0Adistribution%20of%20pixel%20values%20across%20the%20entire%20image%2C%20which%20limits%20their%0Aability%20to%20capture%20the%20diverse%20statistical%20characteristics%20of%20different%0Asemantic%20regions.%20To%20overcome%20this%20limitation%2C%20we%20propose%20Segmentation-Assisted%0AMulti-Entropy%20Models%20for%20Lossless%20Image%20Compression%20%28SEEC%29.%20Our%20framework%0Autilizes%20semantic%20segmentation%20to%20guide%20the%20selection%20and%20adaptation%20of%0Amultiple%20entropy%20models%2C%20enabling%20more%20accurate%20probability%20distribution%0Aestimation%20for%20distinct%20semantic%20regions.%20Specifically%2C%20SEEC%20first%20extracts%0Aimage%20features%20and%20then%20applies%20semantic%20segmentation%20to%20identify%20different%0Aregions%2C%20each%20assigned%20a%20specialized%20entropy%20model%20to%20better%20capture%20its%20unique%0Astatistical%20properties.%20Finally%2C%20a%20multi-channel%20discrete%20logistic%20mixture%0Alikelihood%20is%20employed%20to%20model%20the%20pixel%20value%20distributions%20effectively.%0AExperimental%20results%20on%20benchmark%20datasets%20demonstrate%20that%20SEEC%20achieves%0Astate-of-the-art%20compression%20ratios%20while%20introducing%20only%20minimal%20encoding%20and%0Adecoding%20latency.%20With%20superior%20performance%2C%20the%20proposed%20model%20also%20supports%0ARegions%20of%20Interest%20%28ROIs%29%20coding%20condition%20on%20the%20provided%20segmentation%20mask.%0AOur%20code%20is%20available%20at%20https%3A//github.com/chunbaobao/SEEC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEEC%253A%2520Segmentation-Assisted%2520Multi-Entropy%2520Models%2520for%2520Learned%2520Lossless%250A%2520%2520Image%2520Compression%26entry.906535625%3DChunhang%2520Zheng%2520and%2520Zichang%2520Ren%2520and%2520Dou%2520Li%26entry.1292438233%3D%2520%2520Recently%252C%2520learned%2520image%2520compression%2520has%2520attracted%2520considerable%2520attention%2520due%250Ato%2520its%2520superior%2520performance%2520over%2520traditional%2520methods.%2520However%252C%2520most%2520existing%250Aapproaches%2520employ%2520a%2520single%2520entropy%2520model%2520to%2520estimate%2520the%2520probability%250Adistribution%2520of%2520pixel%2520values%2520across%2520the%2520entire%2520image%252C%2520which%2520limits%2520their%250Aability%2520to%2520capture%2520the%2520diverse%2520statistical%2520characteristics%2520of%2520different%250Asemantic%2520regions.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520Segmentation-Assisted%250AMulti-Entropy%2520Models%2520for%2520Lossless%2520Image%2520Compression%2520%2528SEEC%2529.%2520Our%2520framework%250Autilizes%2520semantic%2520segmentation%2520to%2520guide%2520the%2520selection%2520and%2520adaptation%2520of%250Amultiple%2520entropy%2520models%252C%2520enabling%2520more%2520accurate%2520probability%2520distribution%250Aestimation%2520for%2520distinct%2520semantic%2520regions.%2520Specifically%252C%2520SEEC%2520first%2520extracts%250Aimage%2520features%2520and%2520then%2520applies%2520semantic%2520segmentation%2520to%2520identify%2520different%250Aregions%252C%2520each%2520assigned%2520a%2520specialized%2520entropy%2520model%2520to%2520better%2520capture%2520its%2520unique%250Astatistical%2520properties.%2520Finally%252C%2520a%2520multi-channel%2520discrete%2520logistic%2520mixture%250Alikelihood%2520is%2520employed%2520to%2520model%2520the%2520pixel%2520value%2520distributions%2520effectively.%250AExperimental%2520results%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520SEEC%2520achieves%250Astate-of-the-art%2520compression%2520ratios%2520while%2520introducing%2520only%2520minimal%2520encoding%2520and%250Adecoding%2520latency.%2520With%2520superior%2520performance%252C%2520the%2520proposed%2520model%2520also%2520supports%250ARegions%2520of%2520Interest%2520%2528ROIs%2529%2520coding%2520condition%2520on%2520the%2520provided%2520segmentation%2520mask.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/chunbaobao/SEEC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEEC%3A%20Segmentation-Assisted%20Multi-Entropy%20Models%20for%20Learned%20Lossless%0A%20%20Image%20Compression&entry.906535625=Chunhang%20Zheng%20and%20Zichang%20Ren%20and%20Dou%20Li&entry.1292438233=%20%20Recently%2C%20learned%20image%20compression%20has%20attracted%20considerable%20attention%20due%0Ato%20its%20superior%20performance%20over%20traditional%20methods.%20However%2C%20most%20existing%0Aapproaches%20employ%20a%20single%20entropy%20model%20to%20estimate%20the%20probability%0Adistribution%20of%20pixel%20values%20across%20the%20entire%20image%2C%20which%20limits%20their%0Aability%20to%20capture%20the%20diverse%20statistical%20characteristics%20of%20different%0Asemantic%20regions.%20To%20overcome%20this%20limitation%2C%20we%20propose%20Segmentation-Assisted%0AMulti-Entropy%20Models%20for%20Lossless%20Image%20Compression%20%28SEEC%29.%20Our%20framework%0Autilizes%20semantic%20segmentation%20to%20guide%20the%20selection%20and%20adaptation%20of%0Amultiple%20entropy%20models%2C%20enabling%20more%20accurate%20probability%20distribution%0Aestimation%20for%20distinct%20semantic%20regions.%20Specifically%2C%20SEEC%20first%20extracts%0Aimage%20features%20and%20then%20applies%20semantic%20segmentation%20to%20identify%20different%0Aregions%2C%20each%20assigned%20a%20specialized%20entropy%20model%20to%20better%20capture%20its%20unique%0Astatistical%20properties.%20Finally%2C%20a%20multi-channel%20discrete%20logistic%20mixture%0Alikelihood%20is%20employed%20to%20model%20the%20pixel%20value%20distributions%20effectively.%0AExperimental%20results%20on%20benchmark%20datasets%20demonstrate%20that%20SEEC%20achieves%0Astate-of-the-art%20compression%20ratios%20while%20introducing%20only%20minimal%20encoding%20and%0Adecoding%20latency.%20With%20superior%20performance%2C%20the%20proposed%20model%20also%20supports%0ARegions%20of%20Interest%20%28ROIs%29%20coding%20condition%20on%20the%20provided%20segmentation%20mask.%0AOur%20code%20is%20available%20at%20https%3A//github.com/chunbaobao/SEEC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07704v1&entry.124074799=Read"},
{"title": "Semantic Watermarking Reinvented: Enhancing Robustness and Generation\n  Quality with Fourier Integrity", "author": "Sung Ju Lee and Nam Ik Cho", "abstract": "  Semantic watermarking techniques for latent diffusion models (LDMs) are\nrobust against regeneration attacks, but often suffer from detection\nperformance degradation due to the loss of frequency integrity. To tackle this\nproblem, we propose a novel embedding method called Hermitian Symmetric Fourier\nWatermarking (SFW), which maintains frequency integrity by enforcing Hermitian\nsymmetry. Additionally, we introduce a center-aware embedding strategy that\nreduces the vulnerability of semantic watermarking due to cropping attacks by\nensuring robust information retention. To validate our approach, we apply these\ntechniques to existing semantic watermarking schemes, enhancing their\nfrequency-domain structures for better robustness and retrieval accuracy.\nExtensive experiments demonstrate that our methods achieve state-of-the-art\nverification and identification performance, surpassing previous approaches\nacross various attack scenarios. Ablation studies confirm the impact of SFW on\ndetection capabilities, the effectiveness of the center-aware embedding against\ncropping, and how message capacity influences identification accuracy. Notably,\nour method achieves the highest detection accuracy while maintaining superior\nimage fidelity, as evidenced by FID and CLIP scores. Conclusively, our proposed\nSFW is shown to be an effective framework for balancing robustness and image\nfidelity, addressing the inherent trade-offs in semantic watermarking. Code\navailable at https://github.com/thomas11809/SFWMark\n", "link": "http://arxiv.org/abs/2509.07647v1", "date": "2025-09-09", "relevancy": 2.065, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5298}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5101}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Watermarking%20Reinvented%3A%20Enhancing%20Robustness%20and%20Generation%0A%20%20Quality%20with%20Fourier%20Integrity&body=Title%3A%20Semantic%20Watermarking%20Reinvented%3A%20Enhancing%20Robustness%20and%20Generation%0A%20%20Quality%20with%20Fourier%20Integrity%0AAuthor%3A%20Sung%20Ju%20Lee%20and%20Nam%20Ik%20Cho%0AAbstract%3A%20%20%20Semantic%20watermarking%20techniques%20for%20latent%20diffusion%20models%20%28LDMs%29%20are%0Arobust%20against%20regeneration%20attacks%2C%20but%20often%20suffer%20from%20detection%0Aperformance%20degradation%20due%20to%20the%20loss%20of%20frequency%20integrity.%20To%20tackle%20this%0Aproblem%2C%20we%20propose%20a%20novel%20embedding%20method%20called%20Hermitian%20Symmetric%20Fourier%0AWatermarking%20%28SFW%29%2C%20which%20maintains%20frequency%20integrity%20by%20enforcing%20Hermitian%0Asymmetry.%20Additionally%2C%20we%20introduce%20a%20center-aware%20embedding%20strategy%20that%0Areduces%20the%20vulnerability%20of%20semantic%20watermarking%20due%20to%20cropping%20attacks%20by%0Aensuring%20robust%20information%20retention.%20To%20validate%20our%20approach%2C%20we%20apply%20these%0Atechniques%20to%20existing%20semantic%20watermarking%20schemes%2C%20enhancing%20their%0Afrequency-domain%20structures%20for%20better%20robustness%20and%20retrieval%20accuracy.%0AExtensive%20experiments%20demonstrate%20that%20our%20methods%20achieve%20state-of-the-art%0Averification%20and%20identification%20performance%2C%20surpassing%20previous%20approaches%0Aacross%20various%20attack%20scenarios.%20Ablation%20studies%20confirm%20the%20impact%20of%20SFW%20on%0Adetection%20capabilities%2C%20the%20effectiveness%20of%20the%20center-aware%20embedding%20against%0Acropping%2C%20and%20how%20message%20capacity%20influences%20identification%20accuracy.%20Notably%2C%0Aour%20method%20achieves%20the%20highest%20detection%20accuracy%20while%20maintaining%20superior%0Aimage%20fidelity%2C%20as%20evidenced%20by%20FID%20and%20CLIP%20scores.%20Conclusively%2C%20our%20proposed%0ASFW%20is%20shown%20to%20be%20an%20effective%20framework%20for%20balancing%20robustness%20and%20image%0Afidelity%2C%20addressing%20the%20inherent%20trade-offs%20in%20semantic%20watermarking.%20Code%0Aavailable%20at%20https%3A//github.com/thomas11809/SFWMark%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Watermarking%2520Reinvented%253A%2520Enhancing%2520Robustness%2520and%2520Generation%250A%2520%2520Quality%2520with%2520Fourier%2520Integrity%26entry.906535625%3DSung%2520Ju%2520Lee%2520and%2520Nam%2520Ik%2520Cho%26entry.1292438233%3D%2520%2520Semantic%2520watermarking%2520techniques%2520for%2520latent%2520diffusion%2520models%2520%2528LDMs%2529%2520are%250Arobust%2520against%2520regeneration%2520attacks%252C%2520but%2520often%2520suffer%2520from%2520detection%250Aperformance%2520degradation%2520due%2520to%2520the%2520loss%2520of%2520frequency%2520integrity.%2520To%2520tackle%2520this%250Aproblem%252C%2520we%2520propose%2520a%2520novel%2520embedding%2520method%2520called%2520Hermitian%2520Symmetric%2520Fourier%250AWatermarking%2520%2528SFW%2529%252C%2520which%2520maintains%2520frequency%2520integrity%2520by%2520enforcing%2520Hermitian%250Asymmetry.%2520Additionally%252C%2520we%2520introduce%2520a%2520center-aware%2520embedding%2520strategy%2520that%250Areduces%2520the%2520vulnerability%2520of%2520semantic%2520watermarking%2520due%2520to%2520cropping%2520attacks%2520by%250Aensuring%2520robust%2520information%2520retention.%2520To%2520validate%2520our%2520approach%252C%2520we%2520apply%2520these%250Atechniques%2520to%2520existing%2520semantic%2520watermarking%2520schemes%252C%2520enhancing%2520their%250Afrequency-domain%2520structures%2520for%2520better%2520robustness%2520and%2520retrieval%2520accuracy.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520methods%2520achieve%2520state-of-the-art%250Averification%2520and%2520identification%2520performance%252C%2520surpassing%2520previous%2520approaches%250Aacross%2520various%2520attack%2520scenarios.%2520Ablation%2520studies%2520confirm%2520the%2520impact%2520of%2520SFW%2520on%250Adetection%2520capabilities%252C%2520the%2520effectiveness%2520of%2520the%2520center-aware%2520embedding%2520against%250Acropping%252C%2520and%2520how%2520message%2520capacity%2520influences%2520identification%2520accuracy.%2520Notably%252C%250Aour%2520method%2520achieves%2520the%2520highest%2520detection%2520accuracy%2520while%2520maintaining%2520superior%250Aimage%2520fidelity%252C%2520as%2520evidenced%2520by%2520FID%2520and%2520CLIP%2520scores.%2520Conclusively%252C%2520our%2520proposed%250ASFW%2520is%2520shown%2520to%2520be%2520an%2520effective%2520framework%2520for%2520balancing%2520robustness%2520and%2520image%250Afidelity%252C%2520addressing%2520the%2520inherent%2520trade-offs%2520in%2520semantic%2520watermarking.%2520Code%250Aavailable%2520at%2520https%253A//github.com/thomas11809/SFWMark%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Watermarking%20Reinvented%3A%20Enhancing%20Robustness%20and%20Generation%0A%20%20Quality%20with%20Fourier%20Integrity&entry.906535625=Sung%20Ju%20Lee%20and%20Nam%20Ik%20Cho&entry.1292438233=%20%20Semantic%20watermarking%20techniques%20for%20latent%20diffusion%20models%20%28LDMs%29%20are%0Arobust%20against%20regeneration%20attacks%2C%20but%20often%20suffer%20from%20detection%0Aperformance%20degradation%20due%20to%20the%20loss%20of%20frequency%20integrity.%20To%20tackle%20this%0Aproblem%2C%20we%20propose%20a%20novel%20embedding%20method%20called%20Hermitian%20Symmetric%20Fourier%0AWatermarking%20%28SFW%29%2C%20which%20maintains%20frequency%20integrity%20by%20enforcing%20Hermitian%0Asymmetry.%20Additionally%2C%20we%20introduce%20a%20center-aware%20embedding%20strategy%20that%0Areduces%20the%20vulnerability%20of%20semantic%20watermarking%20due%20to%20cropping%20attacks%20by%0Aensuring%20robust%20information%20retention.%20To%20validate%20our%20approach%2C%20we%20apply%20these%0Atechniques%20to%20existing%20semantic%20watermarking%20schemes%2C%20enhancing%20their%0Afrequency-domain%20structures%20for%20better%20robustness%20and%20retrieval%20accuracy.%0AExtensive%20experiments%20demonstrate%20that%20our%20methods%20achieve%20state-of-the-art%0Averification%20and%20identification%20performance%2C%20surpassing%20previous%20approaches%0Aacross%20various%20attack%20scenarios.%20Ablation%20studies%20confirm%20the%20impact%20of%20SFW%20on%0Adetection%20capabilities%2C%20the%20effectiveness%20of%20the%20center-aware%20embedding%20against%0Acropping%2C%20and%20how%20message%20capacity%20influences%20identification%20accuracy.%20Notably%2C%0Aour%20method%20achieves%20the%20highest%20detection%20accuracy%20while%20maintaining%20superior%0Aimage%20fidelity%2C%20as%20evidenced%20by%20FID%20and%20CLIP%20scores.%20Conclusively%2C%20our%20proposed%0ASFW%20is%20shown%20to%20be%20an%20effective%20framework%20for%20balancing%20robustness%20and%20image%0Afidelity%2C%20addressing%20the%20inherent%20trade-offs%20in%20semantic%20watermarking.%20Code%0Aavailable%20at%20https%3A//github.com/thomas11809/SFWMark%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07647v1&entry.124074799=Read"},
{"title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection", "author": "Wei Wu and Zhuoshi Pan and Chao Wang and Liyi Chen and Yunchu Bai and Tianfu Wang and Kun Fu and Zheng Wang and Hui Xiong", "abstract": "  Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.\n", "link": "http://arxiv.org/abs/2411.02886v3", "date": "2025-09-09", "relevancy": 2.0603, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5808}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokenSelect%3A%20Efficient%20Long-Context%20Inference%20and%20Length%20Extrapolation%0A%20%20for%20LLMs%20via%20Dynamic%20Token-Level%20KV%20Cache%20Selection&body=Title%3A%20TokenSelect%3A%20Efficient%20Long-Context%20Inference%20and%20Length%20Extrapolation%0A%20%20for%20LLMs%20via%20Dynamic%20Token-Level%20KV%20Cache%20Selection%0AAuthor%3A%20Wei%20Wu%20and%20Zhuoshi%20Pan%20and%20Chao%20Wang%20and%20Liyi%20Chen%20and%20Yunchu%20Bai%20and%20Tianfu%20Wang%20and%20Kun%20Fu%20and%20Zheng%20Wang%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20Rapid%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20spurred%20demand%20for%0Aprocessing%20extended%20context%20sequences%20in%20contemporary%20applications.%20However%2C%0Athis%20progress%20faces%20two%20challenges%3A%20performance%20degradation%20due%20to%20sequence%0Alengths%20out-of-distribution%2C%20and%20excessively%20long%20inference%20times%20caused%20by%20the%0Aquadratic%20computational%20complexity%20of%20attention.%20These%20issues%20limit%20LLMs%20in%0Along-context%20scenarios.%20In%20this%20paper%2C%20we%20propose%20Dynamic%20Token-Level%20KV%20Cache%0ASelection%20%28TokenSelect%29%2C%20a%20training-free%20method%20for%20efficient%20and%20accurate%0Along-context%20inference.%20TokenSelect%20builds%20upon%20the%20observation%20of%0Anon-contiguous%20attention%20sparsity%2C%20using%20QK%20dot%20products%20to%20measure%20per-head%20KV%0ACache%20criticality%20at%20token-level.%20By%20per-head%20soft%20voting%20mechanism%2C%0ATokenSelect%20selectively%20involves%20a%20few%20critical%20KV%20cache%20tokens%20in%20attention%0Acalculation%20without%20sacrificing%20accuracy.%20To%20further%20accelerate%20TokenSelect%2C%20we%0Adesign%20the%20Selection%20Cache%20based%20on%20observations%20of%20consecutive%20Query%0Asimilarity%20and%20implemented%20the%20efficient%20Paged%20Dot%20Product%20Kernel%2C%0Asignificantly%20reducing%20the%20selection%20overhead.%20A%20comprehensive%20evaluation%20of%0ATokenSelect%20demonstrates%20up%20to%20%2423.84%5Ctimes%24%20speedup%20in%20attention%20computation%0Aand%20up%20to%20%242.28%5Ctimes%24%20acceleration%20in%20end-to-end%20latency%2C%20while%20providing%0Asuperior%20performance%20compared%20to%20state-of-the-art%20long-context%20inference%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02886v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenSelect%253A%2520Efficient%2520Long-Context%2520Inference%2520and%2520Length%2520Extrapolation%250A%2520%2520for%2520LLMs%2520via%2520Dynamic%2520Token-Level%2520KV%2520Cache%2520Selection%26entry.906535625%3DWei%2520Wu%2520and%2520Zhuoshi%2520Pan%2520and%2520Chao%2520Wang%2520and%2520Liyi%2520Chen%2520and%2520Yunchu%2520Bai%2520and%2520Tianfu%2520Wang%2520and%2520Kun%2520Fu%2520and%2520Zheng%2520Wang%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520Rapid%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520spurred%2520demand%2520for%250Aprocessing%2520extended%2520context%2520sequences%2520in%2520contemporary%2520applications.%2520However%252C%250Athis%2520progress%2520faces%2520two%2520challenges%253A%2520performance%2520degradation%2520due%2520to%2520sequence%250Alengths%2520out-of-distribution%252C%2520and%2520excessively%2520long%2520inference%2520times%2520caused%2520by%2520the%250Aquadratic%2520computational%2520complexity%2520of%2520attention.%2520These%2520issues%2520limit%2520LLMs%2520in%250Along-context%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Dynamic%2520Token-Level%2520KV%2520Cache%250ASelection%2520%2528TokenSelect%2529%252C%2520a%2520training-free%2520method%2520for%2520efficient%2520and%2520accurate%250Along-context%2520inference.%2520TokenSelect%2520builds%2520upon%2520the%2520observation%2520of%250Anon-contiguous%2520attention%2520sparsity%252C%2520using%2520QK%2520dot%2520products%2520to%2520measure%2520per-head%2520KV%250ACache%2520criticality%2520at%2520token-level.%2520By%2520per-head%2520soft%2520voting%2520mechanism%252C%250ATokenSelect%2520selectively%2520involves%2520a%2520few%2520critical%2520KV%2520cache%2520tokens%2520in%2520attention%250Acalculation%2520without%2520sacrificing%2520accuracy.%2520To%2520further%2520accelerate%2520TokenSelect%252C%2520we%250Adesign%2520the%2520Selection%2520Cache%2520based%2520on%2520observations%2520of%2520consecutive%2520Query%250Asimilarity%2520and%2520implemented%2520the%2520efficient%2520Paged%2520Dot%2520Product%2520Kernel%252C%250Asignificantly%2520reducing%2520the%2520selection%2520overhead.%2520A%2520comprehensive%2520evaluation%2520of%250ATokenSelect%2520demonstrates%2520up%2520to%2520%252423.84%255Ctimes%2524%2520speedup%2520in%2520attention%2520computation%250Aand%2520up%2520to%2520%25242.28%255Ctimes%2524%2520acceleration%2520in%2520end-to-end%2520latency%252C%2520while%2520providing%250Asuperior%2520performance%2520compared%2520to%2520state-of-the-art%2520long-context%2520inference%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02886v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokenSelect%3A%20Efficient%20Long-Context%20Inference%20and%20Length%20Extrapolation%0A%20%20for%20LLMs%20via%20Dynamic%20Token-Level%20KV%20Cache%20Selection&entry.906535625=Wei%20Wu%20and%20Zhuoshi%20Pan%20and%20Chao%20Wang%20and%20Liyi%20Chen%20and%20Yunchu%20Bai%20and%20Tianfu%20Wang%20and%20Kun%20Fu%20and%20Zheng%20Wang%20and%20Hui%20Xiong&entry.1292438233=%20%20Rapid%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20spurred%20demand%20for%0Aprocessing%20extended%20context%20sequences%20in%20contemporary%20applications.%20However%2C%0Athis%20progress%20faces%20two%20challenges%3A%20performance%20degradation%20due%20to%20sequence%0Alengths%20out-of-distribution%2C%20and%20excessively%20long%20inference%20times%20caused%20by%20the%0Aquadratic%20computational%20complexity%20of%20attention.%20These%20issues%20limit%20LLMs%20in%0Along-context%20scenarios.%20In%20this%20paper%2C%20we%20propose%20Dynamic%20Token-Level%20KV%20Cache%0ASelection%20%28TokenSelect%29%2C%20a%20training-free%20method%20for%20efficient%20and%20accurate%0Along-context%20inference.%20TokenSelect%20builds%20upon%20the%20observation%20of%0Anon-contiguous%20attention%20sparsity%2C%20using%20QK%20dot%20products%20to%20measure%20per-head%20KV%0ACache%20criticality%20at%20token-level.%20By%20per-head%20soft%20voting%20mechanism%2C%0ATokenSelect%20selectively%20involves%20a%20few%20critical%20KV%20cache%20tokens%20in%20attention%0Acalculation%20without%20sacrificing%20accuracy.%20To%20further%20accelerate%20TokenSelect%2C%20we%0Adesign%20the%20Selection%20Cache%20based%20on%20observations%20of%20consecutive%20Query%0Asimilarity%20and%20implemented%20the%20efficient%20Paged%20Dot%20Product%20Kernel%2C%0Asignificantly%20reducing%20the%20selection%20overhead.%20A%20comprehensive%20evaluation%20of%0ATokenSelect%20demonstrates%20up%20to%20%2423.84%5Ctimes%24%20speedup%20in%20attention%20computation%0Aand%20up%20to%20%242.28%5Ctimes%24%20acceleration%20in%20end-to-end%20latency%2C%20while%20providing%0Asuperior%20performance%20compared%20to%20state-of-the-art%20long-context%20inference%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02886v3&entry.124074799=Read"},
{"title": "MSRFormer: Road Network Representation Learning using Multi-scale\n  Feature Fusion of Heterogeneous Spatial Interactions", "author": "Jian Yang and Jiahui Wu and Li Fang and Hongchao Fan and Bianying Zhang and Huijie Zhao and Guangyi Yang and Rui Xin and Xiong You", "abstract": "  Transforming road network data into vector representations using deep\nlearning has proven effective for road network analysis. However, urban road\nnetworks' heterogeneous and hierarchical nature poses challenges for accurate\nrepresentation learning. Graph neural networks, which aggregate features from\nneighboring nodes, often struggle due to their homogeneity assumption and focus\non a single structural scale. To address these issues, this paper presents\nMSRFormer, a novel road network representation learning framework that\nintegrates multi-scale spatial interactions by addressing their flow\nheterogeneity and long-distance dependencies. It uses spatial flow convolution\nto extract small-scale features from large trajectory datasets, and identifies\nscale-dependent spatial interaction regions to capture the spatial structure of\nroad networks and flow heterogeneity. By employing a graph transformer,\nMSRFormer effectively captures complex spatial dependencies across multiple\nscales. The spatial interaction features are fused using residual connections,\nwhich are fed to a contrastive learning algorithm to derive the final road\nnetwork representation. Validation on two real-world datasets demonstrates that\nMSRFormer outperforms baseline methods in two road network analysis tasks. The\nperformance gains of MSRFormer suggest the traffic-related task benefits more\nfrom incorporating trajectory data, also resulting in greater improvements in\ncomplex road network structures with up to 16% improvements compared to the\nmost competitive baseline method. This research provides a practical framework\nfor developing task-agnostic road network representation models and highlights\ndistinct association patterns of the interplay between scale effects and flow\nheterogeneity of spatial interactions.\n", "link": "http://arxiv.org/abs/2509.05685v2", "date": "2025-09-09", "relevancy": 2.0497, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5233}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5137}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSRFormer%3A%20Road%20Network%20Representation%20Learning%20using%20Multi-scale%0A%20%20Feature%20Fusion%20of%20Heterogeneous%20Spatial%20Interactions&body=Title%3A%20MSRFormer%3A%20Road%20Network%20Representation%20Learning%20using%20Multi-scale%0A%20%20Feature%20Fusion%20of%20Heterogeneous%20Spatial%20Interactions%0AAuthor%3A%20Jian%20Yang%20and%20Jiahui%20Wu%20and%20Li%20Fang%20and%20Hongchao%20Fan%20and%20Bianying%20Zhang%20and%20Huijie%20Zhao%20and%20Guangyi%20Yang%20and%20Rui%20Xin%20and%20Xiong%20You%0AAbstract%3A%20%20%20Transforming%20road%20network%20data%20into%20vector%20representations%20using%20deep%0Alearning%20has%20proven%20effective%20for%20road%20network%20analysis.%20However%2C%20urban%20road%0Anetworks%27%20heterogeneous%20and%20hierarchical%20nature%20poses%20challenges%20for%20accurate%0Arepresentation%20learning.%20Graph%20neural%20networks%2C%20which%20aggregate%20features%20from%0Aneighboring%20nodes%2C%20often%20struggle%20due%20to%20their%20homogeneity%20assumption%20and%20focus%0Aon%20a%20single%20structural%20scale.%20To%20address%20these%20issues%2C%20this%20paper%20presents%0AMSRFormer%2C%20a%20novel%20road%20network%20representation%20learning%20framework%20that%0Aintegrates%20multi-scale%20spatial%20interactions%20by%20addressing%20their%20flow%0Aheterogeneity%20and%20long-distance%20dependencies.%20It%20uses%20spatial%20flow%20convolution%0Ato%20extract%20small-scale%20features%20from%20large%20trajectory%20datasets%2C%20and%20identifies%0Ascale-dependent%20spatial%20interaction%20regions%20to%20capture%20the%20spatial%20structure%20of%0Aroad%20networks%20and%20flow%20heterogeneity.%20By%20employing%20a%20graph%20transformer%2C%0AMSRFormer%20effectively%20captures%20complex%20spatial%20dependencies%20across%20multiple%0Ascales.%20The%20spatial%20interaction%20features%20are%20fused%20using%20residual%20connections%2C%0Awhich%20are%20fed%20to%20a%20contrastive%20learning%20algorithm%20to%20derive%20the%20final%20road%0Anetwork%20representation.%20Validation%20on%20two%20real-world%20datasets%20demonstrates%20that%0AMSRFormer%20outperforms%20baseline%20methods%20in%20two%20road%20network%20analysis%20tasks.%20The%0Aperformance%20gains%20of%20MSRFormer%20suggest%20the%20traffic-related%20task%20benefits%20more%0Afrom%20incorporating%20trajectory%20data%2C%20also%20resulting%20in%20greater%20improvements%20in%0Acomplex%20road%20network%20structures%20with%20up%20to%2016%25%20improvements%20compared%20to%20the%0Amost%20competitive%20baseline%20method.%20This%20research%20provides%20a%20practical%20framework%0Afor%20developing%20task-agnostic%20road%20network%20representation%20models%20and%20highlights%0Adistinct%20association%20patterns%20of%20the%20interplay%20between%20scale%20effects%20and%20flow%0Aheterogeneity%20of%20spatial%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05685v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSRFormer%253A%2520Road%2520Network%2520Representation%2520Learning%2520using%2520Multi-scale%250A%2520%2520Feature%2520Fusion%2520of%2520Heterogeneous%2520Spatial%2520Interactions%26entry.906535625%3DJian%2520Yang%2520and%2520Jiahui%2520Wu%2520and%2520Li%2520Fang%2520and%2520Hongchao%2520Fan%2520and%2520Bianying%2520Zhang%2520and%2520Huijie%2520Zhao%2520and%2520Guangyi%2520Yang%2520and%2520Rui%2520Xin%2520and%2520Xiong%2520You%26entry.1292438233%3D%2520%2520Transforming%2520road%2520network%2520data%2520into%2520vector%2520representations%2520using%2520deep%250Alearning%2520has%2520proven%2520effective%2520for%2520road%2520network%2520analysis.%2520However%252C%2520urban%2520road%250Anetworks%2527%2520heterogeneous%2520and%2520hierarchical%2520nature%2520poses%2520challenges%2520for%2520accurate%250Arepresentation%2520learning.%2520Graph%2520neural%2520networks%252C%2520which%2520aggregate%2520features%2520from%250Aneighboring%2520nodes%252C%2520often%2520struggle%2520due%2520to%2520their%2520homogeneity%2520assumption%2520and%2520focus%250Aon%2520a%2520single%2520structural%2520scale.%2520To%2520address%2520these%2520issues%252C%2520this%2520paper%2520presents%250AMSRFormer%252C%2520a%2520novel%2520road%2520network%2520representation%2520learning%2520framework%2520that%250Aintegrates%2520multi-scale%2520spatial%2520interactions%2520by%2520addressing%2520their%2520flow%250Aheterogeneity%2520and%2520long-distance%2520dependencies.%2520It%2520uses%2520spatial%2520flow%2520convolution%250Ato%2520extract%2520small-scale%2520features%2520from%2520large%2520trajectory%2520datasets%252C%2520and%2520identifies%250Ascale-dependent%2520spatial%2520interaction%2520regions%2520to%2520capture%2520the%2520spatial%2520structure%2520of%250Aroad%2520networks%2520and%2520flow%2520heterogeneity.%2520By%2520employing%2520a%2520graph%2520transformer%252C%250AMSRFormer%2520effectively%2520captures%2520complex%2520spatial%2520dependencies%2520across%2520multiple%250Ascales.%2520The%2520spatial%2520interaction%2520features%2520are%2520fused%2520using%2520residual%2520connections%252C%250Awhich%2520are%2520fed%2520to%2520a%2520contrastive%2520learning%2520algorithm%2520to%2520derive%2520the%2520final%2520road%250Anetwork%2520representation.%2520Validation%2520on%2520two%2520real-world%2520datasets%2520demonstrates%2520that%250AMSRFormer%2520outperforms%2520baseline%2520methods%2520in%2520two%2520road%2520network%2520analysis%2520tasks.%2520The%250Aperformance%2520gains%2520of%2520MSRFormer%2520suggest%2520the%2520traffic-related%2520task%2520benefits%2520more%250Afrom%2520incorporating%2520trajectory%2520data%252C%2520also%2520resulting%2520in%2520greater%2520improvements%2520in%250Acomplex%2520road%2520network%2520structures%2520with%2520up%2520to%252016%2525%2520improvements%2520compared%2520to%2520the%250Amost%2520competitive%2520baseline%2520method.%2520This%2520research%2520provides%2520a%2520practical%2520framework%250Afor%2520developing%2520task-agnostic%2520road%2520network%2520representation%2520models%2520and%2520highlights%250Adistinct%2520association%2520patterns%2520of%2520the%2520interplay%2520between%2520scale%2520effects%2520and%2520flow%250Aheterogeneity%2520of%2520spatial%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05685v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSRFormer%3A%20Road%20Network%20Representation%20Learning%20using%20Multi-scale%0A%20%20Feature%20Fusion%20of%20Heterogeneous%20Spatial%20Interactions&entry.906535625=Jian%20Yang%20and%20Jiahui%20Wu%20and%20Li%20Fang%20and%20Hongchao%20Fan%20and%20Bianying%20Zhang%20and%20Huijie%20Zhao%20and%20Guangyi%20Yang%20and%20Rui%20Xin%20and%20Xiong%20You&entry.1292438233=%20%20Transforming%20road%20network%20data%20into%20vector%20representations%20using%20deep%0Alearning%20has%20proven%20effective%20for%20road%20network%20analysis.%20However%2C%20urban%20road%0Anetworks%27%20heterogeneous%20and%20hierarchical%20nature%20poses%20challenges%20for%20accurate%0Arepresentation%20learning.%20Graph%20neural%20networks%2C%20which%20aggregate%20features%20from%0Aneighboring%20nodes%2C%20often%20struggle%20due%20to%20their%20homogeneity%20assumption%20and%20focus%0Aon%20a%20single%20structural%20scale.%20To%20address%20these%20issues%2C%20this%20paper%20presents%0AMSRFormer%2C%20a%20novel%20road%20network%20representation%20learning%20framework%20that%0Aintegrates%20multi-scale%20spatial%20interactions%20by%20addressing%20their%20flow%0Aheterogeneity%20and%20long-distance%20dependencies.%20It%20uses%20spatial%20flow%20convolution%0Ato%20extract%20small-scale%20features%20from%20large%20trajectory%20datasets%2C%20and%20identifies%0Ascale-dependent%20spatial%20interaction%20regions%20to%20capture%20the%20spatial%20structure%20of%0Aroad%20networks%20and%20flow%20heterogeneity.%20By%20employing%20a%20graph%20transformer%2C%0AMSRFormer%20effectively%20captures%20complex%20spatial%20dependencies%20across%20multiple%0Ascales.%20The%20spatial%20interaction%20features%20are%20fused%20using%20residual%20connections%2C%0Awhich%20are%20fed%20to%20a%20contrastive%20learning%20algorithm%20to%20derive%20the%20final%20road%0Anetwork%20representation.%20Validation%20on%20two%20real-world%20datasets%20demonstrates%20that%0AMSRFormer%20outperforms%20baseline%20methods%20in%20two%20road%20network%20analysis%20tasks.%20The%0Aperformance%20gains%20of%20MSRFormer%20suggest%20the%20traffic-related%20task%20benefits%20more%0Afrom%20incorporating%20trajectory%20data%2C%20also%20resulting%20in%20greater%20improvements%20in%0Acomplex%20road%20network%20structures%20with%20up%20to%2016%25%20improvements%20compared%20to%20the%0Amost%20competitive%20baseline%20method.%20This%20research%20provides%20a%20practical%20framework%0Afor%20developing%20task-agnostic%20road%20network%20representation%20models%20and%20highlights%0Adistinct%20association%20patterns%20of%20the%20interplay%20between%20scale%20effects%20and%20flow%0Aheterogeneity%20of%20spatial%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05685v2&entry.124074799=Read"},
{"title": "Multi-output Classification using a Cross-talk Architecture for Compound\n  Fault Diagnosis of Motors in Partially Labeled Condition", "author": "Wonjun Yi and Wonho Jung and Hyeonuk Nam and Kangmin Jang and Yong-Hwa Park", "abstract": "  The increasing complexity of rotating machinery and the diversity of\noperating conditions, such as rotating speed and varying torques, have\namplified the challenges in fault diagnosis in scenarios requiring domain\nadaptation, particularly involving compound faults. This study addresses these\nchallenges by introducing a novel multi-output classification (MOC) framework\ntailored for domain adaptation in partially labeled target datasets. Unlike\nconventional multi-class classification (MCC) approaches, the MOC framework\nclassifies the severity levels of compound faults simultaneously. Furthermore,\nwe explore various single-task and multi-task architectures applicable to the\nMOC formulation-including shared trunk and cross-talk-based designs-for\ncompound fault diagnosis under partially labeled conditions. Based on this\ninvestigation, we propose a novel cross-talk architecture, residual neural\ndimension reductor (RNDR), that enables selective information sharing across\ndiagnostic tasks, effectively enhancing classification performance in compound\nfault scenarios. In addition, frequency-layer normalization was incorporated to\nimprove domain adaptation performance on motor vibration data. Compound fault\nconditions were implemented using a motor-based test setup and evaluated across\nsix domain adaptation scenarios. The experimental results demonstrate its\nsuperior macro F1 performance compared to baseline models. We further showed\nthat the structural advantage of RNDR is more pronounced in compound fault\nsettings through a single-fault comparison. We also found that frequency-layer\nnormalization fits the fault diagnosis task better than conventional methods.\nLastly, we analyzed the RNDR with various conditions, other models with\nincreased number of parameters, and compared with the ablated RNDR structure.\n", "link": "http://arxiv.org/abs/2505.24001v3", "date": "2025-09-09", "relevancy": 2.0473, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5458}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5382}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-output%20Classification%20using%20a%20Cross-talk%20Architecture%20for%20Compound%0A%20%20Fault%20Diagnosis%20of%20Motors%20in%20Partially%20Labeled%20Condition&body=Title%3A%20Multi-output%20Classification%20using%20a%20Cross-talk%20Architecture%20for%20Compound%0A%20%20Fault%20Diagnosis%20of%20Motors%20in%20Partially%20Labeled%20Condition%0AAuthor%3A%20Wonjun%20Yi%20and%20Wonho%20Jung%20and%20Hyeonuk%20Nam%20and%20Kangmin%20Jang%20and%20Yong-Hwa%20Park%0AAbstract%3A%20%20%20The%20increasing%20complexity%20of%20rotating%20machinery%20and%20the%20diversity%20of%0Aoperating%20conditions%2C%20such%20as%20rotating%20speed%20and%20varying%20torques%2C%20have%0Aamplified%20the%20challenges%20in%20fault%20diagnosis%20in%20scenarios%20requiring%20domain%0Aadaptation%2C%20particularly%20involving%20compound%20faults.%20This%20study%20addresses%20these%0Achallenges%20by%20introducing%20a%20novel%20multi-output%20classification%20%28MOC%29%20framework%0Atailored%20for%20domain%20adaptation%20in%20partially%20labeled%20target%20datasets.%20Unlike%0Aconventional%20multi-class%20classification%20%28MCC%29%20approaches%2C%20the%20MOC%20framework%0Aclassifies%20the%20severity%20levels%20of%20compound%20faults%20simultaneously.%20Furthermore%2C%0Awe%20explore%20various%20single-task%20and%20multi-task%20architectures%20applicable%20to%20the%0AMOC%20formulation-including%20shared%20trunk%20and%20cross-talk-based%20designs-for%0Acompound%20fault%20diagnosis%20under%20partially%20labeled%20conditions.%20Based%20on%20this%0Ainvestigation%2C%20we%20propose%20a%20novel%20cross-talk%20architecture%2C%20residual%20neural%0Adimension%20reductor%20%28RNDR%29%2C%20that%20enables%20selective%20information%20sharing%20across%0Adiagnostic%20tasks%2C%20effectively%20enhancing%20classification%20performance%20in%20compound%0Afault%20scenarios.%20In%20addition%2C%20frequency-layer%20normalization%20was%20incorporated%20to%0Aimprove%20domain%20adaptation%20performance%20on%20motor%20vibration%20data.%20Compound%20fault%0Aconditions%20were%20implemented%20using%20a%20motor-based%20test%20setup%20and%20evaluated%20across%0Asix%20domain%20adaptation%20scenarios.%20The%20experimental%20results%20demonstrate%20its%0Asuperior%20macro%20F1%20performance%20compared%20to%20baseline%20models.%20We%20further%20showed%0Athat%20the%20structural%20advantage%20of%20RNDR%20is%20more%20pronounced%20in%20compound%20fault%0Asettings%20through%20a%20single-fault%20comparison.%20We%20also%20found%20that%20frequency-layer%0Anormalization%20fits%20the%20fault%20diagnosis%20task%20better%20than%20conventional%20methods.%0ALastly%2C%20we%20analyzed%20the%20RNDR%20with%20various%20conditions%2C%20other%20models%20with%0Aincreased%20number%20of%20parameters%2C%20and%20compared%20with%20the%20ablated%20RNDR%20structure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24001v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-output%2520Classification%2520using%2520a%2520Cross-talk%2520Architecture%2520for%2520Compound%250A%2520%2520Fault%2520Diagnosis%2520of%2520Motors%2520in%2520Partially%2520Labeled%2520Condition%26entry.906535625%3DWonjun%2520Yi%2520and%2520Wonho%2520Jung%2520and%2520Hyeonuk%2520Nam%2520and%2520Kangmin%2520Jang%2520and%2520Yong-Hwa%2520Park%26entry.1292438233%3D%2520%2520The%2520increasing%2520complexity%2520of%2520rotating%2520machinery%2520and%2520the%2520diversity%2520of%250Aoperating%2520conditions%252C%2520such%2520as%2520rotating%2520speed%2520and%2520varying%2520torques%252C%2520have%250Aamplified%2520the%2520challenges%2520in%2520fault%2520diagnosis%2520in%2520scenarios%2520requiring%2520domain%250Aadaptation%252C%2520particularly%2520involving%2520compound%2520faults.%2520This%2520study%2520addresses%2520these%250Achallenges%2520by%2520introducing%2520a%2520novel%2520multi-output%2520classification%2520%2528MOC%2529%2520framework%250Atailored%2520for%2520domain%2520adaptation%2520in%2520partially%2520labeled%2520target%2520datasets.%2520Unlike%250Aconventional%2520multi-class%2520classification%2520%2528MCC%2529%2520approaches%252C%2520the%2520MOC%2520framework%250Aclassifies%2520the%2520severity%2520levels%2520of%2520compound%2520faults%2520simultaneously.%2520Furthermore%252C%250Awe%2520explore%2520various%2520single-task%2520and%2520multi-task%2520architectures%2520applicable%2520to%2520the%250AMOC%2520formulation-including%2520shared%2520trunk%2520and%2520cross-talk-based%2520designs-for%250Acompound%2520fault%2520diagnosis%2520under%2520partially%2520labeled%2520conditions.%2520Based%2520on%2520this%250Ainvestigation%252C%2520we%2520propose%2520a%2520novel%2520cross-talk%2520architecture%252C%2520residual%2520neural%250Adimension%2520reductor%2520%2528RNDR%2529%252C%2520that%2520enables%2520selective%2520information%2520sharing%2520across%250Adiagnostic%2520tasks%252C%2520effectively%2520enhancing%2520classification%2520performance%2520in%2520compound%250Afault%2520scenarios.%2520In%2520addition%252C%2520frequency-layer%2520normalization%2520was%2520incorporated%2520to%250Aimprove%2520domain%2520adaptation%2520performance%2520on%2520motor%2520vibration%2520data.%2520Compound%2520fault%250Aconditions%2520were%2520implemented%2520using%2520a%2520motor-based%2520test%2520setup%2520and%2520evaluated%2520across%250Asix%2520domain%2520adaptation%2520scenarios.%2520The%2520experimental%2520results%2520demonstrate%2520its%250Asuperior%2520macro%2520F1%2520performance%2520compared%2520to%2520baseline%2520models.%2520We%2520further%2520showed%250Athat%2520the%2520structural%2520advantage%2520of%2520RNDR%2520is%2520more%2520pronounced%2520in%2520compound%2520fault%250Asettings%2520through%2520a%2520single-fault%2520comparison.%2520We%2520also%2520found%2520that%2520frequency-layer%250Anormalization%2520fits%2520the%2520fault%2520diagnosis%2520task%2520better%2520than%2520conventional%2520methods.%250ALastly%252C%2520we%2520analyzed%2520the%2520RNDR%2520with%2520various%2520conditions%252C%2520other%2520models%2520with%250Aincreased%2520number%2520of%2520parameters%252C%2520and%2520compared%2520with%2520the%2520ablated%2520RNDR%2520structure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24001v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-output%20Classification%20using%20a%20Cross-talk%20Architecture%20for%20Compound%0A%20%20Fault%20Diagnosis%20of%20Motors%20in%20Partially%20Labeled%20Condition&entry.906535625=Wonjun%20Yi%20and%20Wonho%20Jung%20and%20Hyeonuk%20Nam%20and%20Kangmin%20Jang%20and%20Yong-Hwa%20Park&entry.1292438233=%20%20The%20increasing%20complexity%20of%20rotating%20machinery%20and%20the%20diversity%20of%0Aoperating%20conditions%2C%20such%20as%20rotating%20speed%20and%20varying%20torques%2C%20have%0Aamplified%20the%20challenges%20in%20fault%20diagnosis%20in%20scenarios%20requiring%20domain%0Aadaptation%2C%20particularly%20involving%20compound%20faults.%20This%20study%20addresses%20these%0Achallenges%20by%20introducing%20a%20novel%20multi-output%20classification%20%28MOC%29%20framework%0Atailored%20for%20domain%20adaptation%20in%20partially%20labeled%20target%20datasets.%20Unlike%0Aconventional%20multi-class%20classification%20%28MCC%29%20approaches%2C%20the%20MOC%20framework%0Aclassifies%20the%20severity%20levels%20of%20compound%20faults%20simultaneously.%20Furthermore%2C%0Awe%20explore%20various%20single-task%20and%20multi-task%20architectures%20applicable%20to%20the%0AMOC%20formulation-including%20shared%20trunk%20and%20cross-talk-based%20designs-for%0Acompound%20fault%20diagnosis%20under%20partially%20labeled%20conditions.%20Based%20on%20this%0Ainvestigation%2C%20we%20propose%20a%20novel%20cross-talk%20architecture%2C%20residual%20neural%0Adimension%20reductor%20%28RNDR%29%2C%20that%20enables%20selective%20information%20sharing%20across%0Adiagnostic%20tasks%2C%20effectively%20enhancing%20classification%20performance%20in%20compound%0Afault%20scenarios.%20In%20addition%2C%20frequency-layer%20normalization%20was%20incorporated%20to%0Aimprove%20domain%20adaptation%20performance%20on%20motor%20vibration%20data.%20Compound%20fault%0Aconditions%20were%20implemented%20using%20a%20motor-based%20test%20setup%20and%20evaluated%20across%0Asix%20domain%20adaptation%20scenarios.%20The%20experimental%20results%20demonstrate%20its%0Asuperior%20macro%20F1%20performance%20compared%20to%20baseline%20models.%20We%20further%20showed%0Athat%20the%20structural%20advantage%20of%20RNDR%20is%20more%20pronounced%20in%20compound%20fault%0Asettings%20through%20a%20single-fault%20comparison.%20We%20also%20found%20that%20frequency-layer%0Anormalization%20fits%20the%20fault%20diagnosis%20task%20better%20than%20conventional%20methods.%0ALastly%2C%20we%20analyzed%20the%20RNDR%20with%20various%20conditions%2C%20other%20models%20with%0Aincreased%20number%20of%20parameters%2C%20and%20compared%20with%20the%20ablated%20RNDR%20structure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24001v3&entry.124074799=Read"},
{"title": "Object-level Correlation for Few-Shot Segmentation", "author": "Chunlin Wen and Yu Zhang and Jie Fan and Hongyuan Zhu and Xiu-Shen Wei and Yijun Wang and Zhiqiang Kou and Shuzhou Sun", "abstract": "  Few-shot semantic segmentation (FSS) aims to segment objects of novel\ncategories in the query images given only a few annotated support samples.\nExisting methods primarily build the image-level correlation between the\nsupport target object and the entire query image. However, this correlation\ncontains the hard pixel noise, \\textit{i.e.}, irrelevant background objects,\nthat is intractable to trace and suppress, leading to the overfitting of the\nbackground. To address the limitation of this correlation, we imitate the\nbiological vision process to identify novel objects in the object-level\ninformation. Target identification in the general objects is more valid than in\nthe entire image, especially in the low-data regime. Inspired by this, we\ndesign an Object-level Correlation Network (OCNet) by establishing the\nobject-level correlation between the support target object and query general\nobjects, which is mainly composed of the General Object Mining Module (GOMM)\nand Correlation Construction Module (CCM). Specifically, GOMM constructs the\nquery general object feature by learning saliency and high-level similarity\ncues, where the general objects include the irrelevant background objects and\nthe target foreground object. Then, CCM establishes the object-level\ncorrelation by allocating the target prototypes to match the general object\nfeature. The generated object-level correlation can mine the query target\nfeature and suppress the hard pixel noise for the final prediction. Extensive\nexperiments on PASCAL-${5}^{i}$ and COCO-${20}^{i}$ show that our model\nachieves the state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2509.07917v1", "date": "2025-09-09", "relevancy": 2.0282, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-level%20Correlation%20for%20Few-Shot%20Segmentation&body=Title%3A%20Object-level%20Correlation%20for%20Few-Shot%20Segmentation%0AAuthor%3A%20Chunlin%20Wen%20and%20Yu%20Zhang%20and%20Jie%20Fan%20and%20Hongyuan%20Zhu%20and%20Xiu-Shen%20Wei%20and%20Yijun%20Wang%20and%20Zhiqiang%20Kou%20and%20Shuzhou%20Sun%0AAbstract%3A%20%20%20Few-shot%20semantic%20segmentation%20%28FSS%29%20aims%20to%20segment%20objects%20of%20novel%0Acategories%20in%20the%20query%20images%20given%20only%20a%20few%20annotated%20support%20samples.%0AExisting%20methods%20primarily%20build%20the%20image-level%20correlation%20between%20the%0Asupport%20target%20object%20and%20the%20entire%20query%20image.%20However%2C%20this%20correlation%0Acontains%20the%20hard%20pixel%20noise%2C%20%5Ctextit%7Bi.e.%7D%2C%20irrelevant%20background%20objects%2C%0Athat%20is%20intractable%20to%20trace%20and%20suppress%2C%20leading%20to%20the%20overfitting%20of%20the%0Abackground.%20To%20address%20the%20limitation%20of%20this%20correlation%2C%20we%20imitate%20the%0Abiological%20vision%20process%20to%20identify%20novel%20objects%20in%20the%20object-level%0Ainformation.%20Target%20identification%20in%20the%20general%20objects%20is%20more%20valid%20than%20in%0Athe%20entire%20image%2C%20especially%20in%20the%20low-data%20regime.%20Inspired%20by%20this%2C%20we%0Adesign%20an%20Object-level%20Correlation%20Network%20%28OCNet%29%20by%20establishing%20the%0Aobject-level%20correlation%20between%20the%20support%20target%20object%20and%20query%20general%0Aobjects%2C%20which%20is%20mainly%20composed%20of%20the%20General%20Object%20Mining%20Module%20%28GOMM%29%0Aand%20Correlation%20Construction%20Module%20%28CCM%29.%20Specifically%2C%20GOMM%20constructs%20the%0Aquery%20general%20object%20feature%20by%20learning%20saliency%20and%20high-level%20similarity%0Acues%2C%20where%20the%20general%20objects%20include%20the%20irrelevant%20background%20objects%20and%0Athe%20target%20foreground%20object.%20Then%2C%20CCM%20establishes%20the%20object-level%0Acorrelation%20by%20allocating%20the%20target%20prototypes%20to%20match%20the%20general%20object%0Afeature.%20The%20generated%20object-level%20correlation%20can%20mine%20the%20query%20target%0Afeature%20and%20suppress%20the%20hard%20pixel%20noise%20for%20the%20final%20prediction.%20Extensive%0Aexperiments%20on%20PASCAL-%24%7B5%7D%5E%7Bi%7D%24%20and%20COCO-%24%7B20%7D%5E%7Bi%7D%24%20show%20that%20our%20model%0Aachieves%20the%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-level%2520Correlation%2520for%2520Few-Shot%2520Segmentation%26entry.906535625%3DChunlin%2520Wen%2520and%2520Yu%2520Zhang%2520and%2520Jie%2520Fan%2520and%2520Hongyuan%2520Zhu%2520and%2520Xiu-Shen%2520Wei%2520and%2520Yijun%2520Wang%2520and%2520Zhiqiang%2520Kou%2520and%2520Shuzhou%2520Sun%26entry.1292438233%3D%2520%2520Few-shot%2520semantic%2520segmentation%2520%2528FSS%2529%2520aims%2520to%2520segment%2520objects%2520of%2520novel%250Acategories%2520in%2520the%2520query%2520images%2520given%2520only%2520a%2520few%2520annotated%2520support%2520samples.%250AExisting%2520methods%2520primarily%2520build%2520the%2520image-level%2520correlation%2520between%2520the%250Asupport%2520target%2520object%2520and%2520the%2520entire%2520query%2520image.%2520However%252C%2520this%2520correlation%250Acontains%2520the%2520hard%2520pixel%2520noise%252C%2520%255Ctextit%257Bi.e.%257D%252C%2520irrelevant%2520background%2520objects%252C%250Athat%2520is%2520intractable%2520to%2520trace%2520and%2520suppress%252C%2520leading%2520to%2520the%2520overfitting%2520of%2520the%250Abackground.%2520To%2520address%2520the%2520limitation%2520of%2520this%2520correlation%252C%2520we%2520imitate%2520the%250Abiological%2520vision%2520process%2520to%2520identify%2520novel%2520objects%2520in%2520the%2520object-level%250Ainformation.%2520Target%2520identification%2520in%2520the%2520general%2520objects%2520is%2520more%2520valid%2520than%2520in%250Athe%2520entire%2520image%252C%2520especially%2520in%2520the%2520low-data%2520regime.%2520Inspired%2520by%2520this%252C%2520we%250Adesign%2520an%2520Object-level%2520Correlation%2520Network%2520%2528OCNet%2529%2520by%2520establishing%2520the%250Aobject-level%2520correlation%2520between%2520the%2520support%2520target%2520object%2520and%2520query%2520general%250Aobjects%252C%2520which%2520is%2520mainly%2520composed%2520of%2520the%2520General%2520Object%2520Mining%2520Module%2520%2528GOMM%2529%250Aand%2520Correlation%2520Construction%2520Module%2520%2528CCM%2529.%2520Specifically%252C%2520GOMM%2520constructs%2520the%250Aquery%2520general%2520object%2520feature%2520by%2520learning%2520saliency%2520and%2520high-level%2520similarity%250Acues%252C%2520where%2520the%2520general%2520objects%2520include%2520the%2520irrelevant%2520background%2520objects%2520and%250Athe%2520target%2520foreground%2520object.%2520Then%252C%2520CCM%2520establishes%2520the%2520object-level%250Acorrelation%2520by%2520allocating%2520the%2520target%2520prototypes%2520to%2520match%2520the%2520general%2520object%250Afeature.%2520The%2520generated%2520object-level%2520correlation%2520can%2520mine%2520the%2520query%2520target%250Afeature%2520and%2520suppress%2520the%2520hard%2520pixel%2520noise%2520for%2520the%2520final%2520prediction.%2520Extensive%250Aexperiments%2520on%2520PASCAL-%2524%257B5%257D%255E%257Bi%257D%2524%2520and%2520COCO-%2524%257B20%257D%255E%257Bi%257D%2524%2520show%2520that%2520our%2520model%250Aachieves%2520the%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-level%20Correlation%20for%20Few-Shot%20Segmentation&entry.906535625=Chunlin%20Wen%20and%20Yu%20Zhang%20and%20Jie%20Fan%20and%20Hongyuan%20Zhu%20and%20Xiu-Shen%20Wei%20and%20Yijun%20Wang%20and%20Zhiqiang%20Kou%20and%20Shuzhou%20Sun&entry.1292438233=%20%20Few-shot%20semantic%20segmentation%20%28FSS%29%20aims%20to%20segment%20objects%20of%20novel%0Acategories%20in%20the%20query%20images%20given%20only%20a%20few%20annotated%20support%20samples.%0AExisting%20methods%20primarily%20build%20the%20image-level%20correlation%20between%20the%0Asupport%20target%20object%20and%20the%20entire%20query%20image.%20However%2C%20this%20correlation%0Acontains%20the%20hard%20pixel%20noise%2C%20%5Ctextit%7Bi.e.%7D%2C%20irrelevant%20background%20objects%2C%0Athat%20is%20intractable%20to%20trace%20and%20suppress%2C%20leading%20to%20the%20overfitting%20of%20the%0Abackground.%20To%20address%20the%20limitation%20of%20this%20correlation%2C%20we%20imitate%20the%0Abiological%20vision%20process%20to%20identify%20novel%20objects%20in%20the%20object-level%0Ainformation.%20Target%20identification%20in%20the%20general%20objects%20is%20more%20valid%20than%20in%0Athe%20entire%20image%2C%20especially%20in%20the%20low-data%20regime.%20Inspired%20by%20this%2C%20we%0Adesign%20an%20Object-level%20Correlation%20Network%20%28OCNet%29%20by%20establishing%20the%0Aobject-level%20correlation%20between%20the%20support%20target%20object%20and%20query%20general%0Aobjects%2C%20which%20is%20mainly%20composed%20of%20the%20General%20Object%20Mining%20Module%20%28GOMM%29%0Aand%20Correlation%20Construction%20Module%20%28CCM%29.%20Specifically%2C%20GOMM%20constructs%20the%0Aquery%20general%20object%20feature%20by%20learning%20saliency%20and%20high-level%20similarity%0Acues%2C%20where%20the%20general%20objects%20include%20the%20irrelevant%20background%20objects%20and%0Athe%20target%20foreground%20object.%20Then%2C%20CCM%20establishes%20the%20object-level%0Acorrelation%20by%20allocating%20the%20target%20prototypes%20to%20match%20the%20general%20object%0Afeature.%20The%20generated%20object-level%20correlation%20can%20mine%20the%20query%20target%0Afeature%20and%20suppress%20the%20hard%20pixel%20noise%20for%20the%20final%20prediction.%20Extensive%0Aexperiments%20on%20PASCAL-%24%7B5%7D%5E%7Bi%7D%24%20and%20COCO-%24%7B20%7D%5E%7Bi%7D%24%20show%20that%20our%20model%0Aachieves%20the%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07917v1&entry.124074799=Read"},
{"title": "Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content\n  Detection? Evaluating In-Context Learning vs. Fine-Tuning", "author": "Michele Joshua Maggini and Dhia Merzougui and Rabiraj Bandyopadhyay and Ga\u00ebl Dias and Fabrice Maurel and Pablo Gamallo", "abstract": "  The spread of fake news, polarizing, politically biased, and harmful content\non online platforms has been a serious concern. With large language models\nbecoming a promising approach, however, no study has properly benchmarked their\nperformance across different models, usage methods, and languages. This study\npresents a comprehensive overview of different Large Language Models adaptation\nparadigms for the detection of hyperpartisan and fake news, harmful tweets, and\npolitical bias. Our experiments spanned 10 datasets and 5 different languages\n(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and\nmulticlass classification scenarios. We tested different strategies ranging\nfrom parameter efficient Fine-Tuning of language models to a variety of\ndifferent In-Context Learning strategies and prompts. These included zero-shot\nprompts, codebooks, few-shot (with both randomly-selected and\ndiversely-selected examples using Determinantal Point Processes), and\nChain-of-Thought. We discovered that In-Context Learning often underperforms\nwhen compared to Fine-Tuning a model. This main finding highlights the\nimportance of Fine-Tuning even smaller models on task-specific settings even\nwhen compared to the largest models evaluated in an In-Context Learning setup -\nin our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and\nQwen2.5-7B-Instruct.\n", "link": "http://arxiv.org/abs/2509.07768v1", "date": "2025-09-09", "relevancy": 2.0237, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20LLMs%20Enough%20for%20Hyperpartisan%2C%20Fake%2C%20Polarized%20and%20Harmful%20Content%0A%20%20Detection%3F%20Evaluating%20In-Context%20Learning%20vs.%20Fine-Tuning&body=Title%3A%20Are%20LLMs%20Enough%20for%20Hyperpartisan%2C%20Fake%2C%20Polarized%20and%20Harmful%20Content%0A%20%20Detection%3F%20Evaluating%20In-Context%20Learning%20vs.%20Fine-Tuning%0AAuthor%3A%20Michele%20Joshua%20Maggini%20and%20Dhia%20Merzougui%20and%20Rabiraj%20Bandyopadhyay%20and%20Ga%C3%ABl%20Dias%20and%20Fabrice%20Maurel%20and%20Pablo%20Gamallo%0AAbstract%3A%20%20%20The%20spread%20of%20fake%20news%2C%20polarizing%2C%20politically%20biased%2C%20and%20harmful%20content%0Aon%20online%20platforms%20has%20been%20a%20serious%20concern.%20With%20large%20language%20models%0Abecoming%20a%20promising%20approach%2C%20however%2C%20no%20study%20has%20properly%20benchmarked%20their%0Aperformance%20across%20different%20models%2C%20usage%20methods%2C%20and%20languages.%20This%20study%0Apresents%20a%20comprehensive%20overview%20of%20different%20Large%20Language%20Models%20adaptation%0Aparadigms%20for%20the%20detection%20of%20hyperpartisan%20and%20fake%20news%2C%20harmful%20tweets%2C%20and%0Apolitical%20bias.%20Our%20experiments%20spanned%2010%20datasets%20and%205%20different%20languages%0A%28English%2C%20Spanish%2C%20Portuguese%2C%20Arabic%20and%20Bulgarian%29%2C%20covering%20both%20binary%20and%0Amulticlass%20classification%20scenarios.%20We%20tested%20different%20strategies%20ranging%0Afrom%20parameter%20efficient%20Fine-Tuning%20of%20language%20models%20to%20a%20variety%20of%0Adifferent%20In-Context%20Learning%20strategies%20and%20prompts.%20These%20included%20zero-shot%0Aprompts%2C%20codebooks%2C%20few-shot%20%28with%20both%20randomly-selected%20and%0Adiversely-selected%20examples%20using%20Determinantal%20Point%20Processes%29%2C%20and%0AChain-of-Thought.%20We%20discovered%20that%20In-Context%20Learning%20often%20underperforms%0Awhen%20compared%20to%20Fine-Tuning%20a%20model.%20This%20main%20finding%20highlights%20the%0Aimportance%20of%20Fine-Tuning%20even%20smaller%20models%20on%20task-specific%20settings%20even%0Awhen%20compared%20to%20the%20largest%20models%20evaluated%20in%20an%20In-Context%20Learning%20setup%20-%0Ain%20our%20case%20LlaMA3.1-8b-Instruct%2C%20Mistral-Nemo-Instruct-2407%20and%0AQwen2.5-7B-Instruct.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520LLMs%2520Enough%2520for%2520Hyperpartisan%252C%2520Fake%252C%2520Polarized%2520and%2520Harmful%2520Content%250A%2520%2520Detection%253F%2520Evaluating%2520In-Context%2520Learning%2520vs.%2520Fine-Tuning%26entry.906535625%3DMichele%2520Joshua%2520Maggini%2520and%2520Dhia%2520Merzougui%2520and%2520Rabiraj%2520Bandyopadhyay%2520and%2520Ga%25C3%25ABl%2520Dias%2520and%2520Fabrice%2520Maurel%2520and%2520Pablo%2520Gamallo%26entry.1292438233%3D%2520%2520The%2520spread%2520of%2520fake%2520news%252C%2520polarizing%252C%2520politically%2520biased%252C%2520and%2520harmful%2520content%250Aon%2520online%2520platforms%2520has%2520been%2520a%2520serious%2520concern.%2520With%2520large%2520language%2520models%250Abecoming%2520a%2520promising%2520approach%252C%2520however%252C%2520no%2520study%2520has%2520properly%2520benchmarked%2520their%250Aperformance%2520across%2520different%2520models%252C%2520usage%2520methods%252C%2520and%2520languages.%2520This%2520study%250Apresents%2520a%2520comprehensive%2520overview%2520of%2520different%2520Large%2520Language%2520Models%2520adaptation%250Aparadigms%2520for%2520the%2520detection%2520of%2520hyperpartisan%2520and%2520fake%2520news%252C%2520harmful%2520tweets%252C%2520and%250Apolitical%2520bias.%2520Our%2520experiments%2520spanned%252010%2520datasets%2520and%25205%2520different%2520languages%250A%2528English%252C%2520Spanish%252C%2520Portuguese%252C%2520Arabic%2520and%2520Bulgarian%2529%252C%2520covering%2520both%2520binary%2520and%250Amulticlass%2520classification%2520scenarios.%2520We%2520tested%2520different%2520strategies%2520ranging%250Afrom%2520parameter%2520efficient%2520Fine-Tuning%2520of%2520language%2520models%2520to%2520a%2520variety%2520of%250Adifferent%2520In-Context%2520Learning%2520strategies%2520and%2520prompts.%2520These%2520included%2520zero-shot%250Aprompts%252C%2520codebooks%252C%2520few-shot%2520%2528with%2520both%2520randomly-selected%2520and%250Adiversely-selected%2520examples%2520using%2520Determinantal%2520Point%2520Processes%2529%252C%2520and%250AChain-of-Thought.%2520We%2520discovered%2520that%2520In-Context%2520Learning%2520often%2520underperforms%250Awhen%2520compared%2520to%2520Fine-Tuning%2520a%2520model.%2520This%2520main%2520finding%2520highlights%2520the%250Aimportance%2520of%2520Fine-Tuning%2520even%2520smaller%2520models%2520on%2520task-specific%2520settings%2520even%250Awhen%2520compared%2520to%2520the%2520largest%2520models%2520evaluated%2520in%2520an%2520In-Context%2520Learning%2520setup%2520-%250Ain%2520our%2520case%2520LlaMA3.1-8b-Instruct%252C%2520Mistral-Nemo-Instruct-2407%2520and%250AQwen2.5-7B-Instruct.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20LLMs%20Enough%20for%20Hyperpartisan%2C%20Fake%2C%20Polarized%20and%20Harmful%20Content%0A%20%20Detection%3F%20Evaluating%20In-Context%20Learning%20vs.%20Fine-Tuning&entry.906535625=Michele%20Joshua%20Maggini%20and%20Dhia%20Merzougui%20and%20Rabiraj%20Bandyopadhyay%20and%20Ga%C3%ABl%20Dias%20and%20Fabrice%20Maurel%20and%20Pablo%20Gamallo&entry.1292438233=%20%20The%20spread%20of%20fake%20news%2C%20polarizing%2C%20politically%20biased%2C%20and%20harmful%20content%0Aon%20online%20platforms%20has%20been%20a%20serious%20concern.%20With%20large%20language%20models%0Abecoming%20a%20promising%20approach%2C%20however%2C%20no%20study%20has%20properly%20benchmarked%20their%0Aperformance%20across%20different%20models%2C%20usage%20methods%2C%20and%20languages.%20This%20study%0Apresents%20a%20comprehensive%20overview%20of%20different%20Large%20Language%20Models%20adaptation%0Aparadigms%20for%20the%20detection%20of%20hyperpartisan%20and%20fake%20news%2C%20harmful%20tweets%2C%20and%0Apolitical%20bias.%20Our%20experiments%20spanned%2010%20datasets%20and%205%20different%20languages%0A%28English%2C%20Spanish%2C%20Portuguese%2C%20Arabic%20and%20Bulgarian%29%2C%20covering%20both%20binary%20and%0Amulticlass%20classification%20scenarios.%20We%20tested%20different%20strategies%20ranging%0Afrom%20parameter%20efficient%20Fine-Tuning%20of%20language%20models%20to%20a%20variety%20of%0Adifferent%20In-Context%20Learning%20strategies%20and%20prompts.%20These%20included%20zero-shot%0Aprompts%2C%20codebooks%2C%20few-shot%20%28with%20both%20randomly-selected%20and%0Adiversely-selected%20examples%20using%20Determinantal%20Point%20Processes%29%2C%20and%0AChain-of-Thought.%20We%20discovered%20that%20In-Context%20Learning%20often%20underperforms%0Awhen%20compared%20to%20Fine-Tuning%20a%20model.%20This%20main%20finding%20highlights%20the%0Aimportance%20of%20Fine-Tuning%20even%20smaller%20models%20on%20task-specific%20settings%20even%0Awhen%20compared%20to%20the%20largest%20models%20evaluated%20in%20an%20In-Context%20Learning%20setup%20-%0Ain%20our%20case%20LlaMA3.1-8b-Instruct%2C%20Mistral-Nemo-Instruct-2407%20and%0AQwen2.5-7B-Instruct.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07768v1&entry.124074799=Read"},
{"title": "MoE-Compression: How the Compression Error of Experts Affects the\n  Inference Accuracy of MoE Model?", "author": "Songkai Ma and Zhaorui Zhang and Sheng Di and Benben Liu and Xiaodong Yu and Xiaoyi Lu and Dan Wang", "abstract": "  With the widespread application of Mixture of Experts (MoE) reasoning models\nin the field of LLM learning, efficiently serving MoE models under limited GPU\nmemory constraints has emerged as a significant challenge. Offloading the\nnon-activated experts to main memory has been identified as an efficient\napproach to address such a problem, while it brings the challenges of\ntransferring the expert between the GPU memory and main memory. We need to\nexplore an efficient approach to compress the expert and analyze how the\ncompression error affects the inference performance.\n  To bridge this gap, we propose employing error-bounded lossy compression\nalgorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby\nreducing data transfer overhead during MoE inference. We conduct extensive\nexperiments across various benchmarks and present a comprehensive analysis of\nhow compression-induced errors in different experts affect overall inference\naccuracy. The results indicate that experts in the shallow layers, which are\nprimarily responsible for the attention mechanism and the transformation of\ninput tokens into vector representations, exhibit minimal degradation in\ninference accuracy when subjected to bounded errors. In contrast, errors in the\nmiddle-layer experts, which are central to model reasoning, significantly\nimpair inference accuracy. Interestingly, introducing bounded errors in the\ndeep-layer experts, which are mainly responsible for instruction following and\noutput integration, can sometimes lead to improvements in inference accuracy.\n", "link": "http://arxiv.org/abs/2509.07727v1", "date": "2025-09-09", "relevancy": 2.0236, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoE-Compression%3A%20How%20the%20Compression%20Error%20of%20Experts%20Affects%20the%0A%20%20Inference%20Accuracy%20of%20MoE%20Model%3F&body=Title%3A%20MoE-Compression%3A%20How%20the%20Compression%20Error%20of%20Experts%20Affects%20the%0A%20%20Inference%20Accuracy%20of%20MoE%20Model%3F%0AAuthor%3A%20Songkai%20Ma%20and%20Zhaorui%20Zhang%20and%20Sheng%20Di%20and%20Benben%20Liu%20and%20Xiaodong%20Yu%20and%20Xiaoyi%20Lu%20and%20Dan%20Wang%0AAbstract%3A%20%20%20With%20the%20widespread%20application%20of%20Mixture%20of%20Experts%20%28MoE%29%20reasoning%20models%0Ain%20the%20field%20of%20LLM%20learning%2C%20efficiently%20serving%20MoE%20models%20under%20limited%20GPU%0Amemory%20constraints%20has%20emerged%20as%20a%20significant%20challenge.%20Offloading%20the%0Anon-activated%20experts%20to%20main%20memory%20has%20been%20identified%20as%20an%20efficient%0Aapproach%20to%20address%20such%20a%20problem%2C%20while%20it%20brings%20the%20challenges%20of%0Atransferring%20the%20expert%20between%20the%20GPU%20memory%20and%20main%20memory.%20We%20need%20to%0Aexplore%20an%20efficient%20approach%20to%20compress%20the%20expert%20and%20analyze%20how%20the%0Acompression%20error%20affects%20the%20inference%20performance.%0A%20%20To%20bridge%20this%20gap%2C%20we%20propose%20employing%20error-bounded%20lossy%20compression%0Aalgorithms%20%28such%20as%20SZ3%20and%20CuSZp%29%20to%20compress%20non-activated%20experts%2C%20thereby%0Areducing%20data%20transfer%20overhead%20during%20MoE%20inference.%20We%20conduct%20extensive%0Aexperiments%20across%20various%20benchmarks%20and%20present%20a%20comprehensive%20analysis%20of%0Ahow%20compression-induced%20errors%20in%20different%20experts%20affect%20overall%20inference%0Aaccuracy.%20The%20results%20indicate%20that%20experts%20in%20the%20shallow%20layers%2C%20which%20are%0Aprimarily%20responsible%20for%20the%20attention%20mechanism%20and%20the%20transformation%20of%0Ainput%20tokens%20into%20vector%20representations%2C%20exhibit%20minimal%20degradation%20in%0Ainference%20accuracy%20when%20subjected%20to%20bounded%20errors.%20In%20contrast%2C%20errors%20in%20the%0Amiddle-layer%20experts%2C%20which%20are%20central%20to%20model%20reasoning%2C%20significantly%0Aimpair%20inference%20accuracy.%20Interestingly%2C%20introducing%20bounded%20errors%20in%20the%0Adeep-layer%20experts%2C%20which%20are%20mainly%20responsible%20for%20instruction%20following%20and%0Aoutput%20integration%2C%20can%20sometimes%20lead%20to%20improvements%20in%20inference%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoE-Compression%253A%2520How%2520the%2520Compression%2520Error%2520of%2520Experts%2520Affects%2520the%250A%2520%2520Inference%2520Accuracy%2520of%2520MoE%2520Model%253F%26entry.906535625%3DSongkai%2520Ma%2520and%2520Zhaorui%2520Zhang%2520and%2520Sheng%2520Di%2520and%2520Benben%2520Liu%2520and%2520Xiaodong%2520Yu%2520and%2520Xiaoyi%2520Lu%2520and%2520Dan%2520Wang%26entry.1292438233%3D%2520%2520With%2520the%2520widespread%2520application%2520of%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520reasoning%2520models%250Ain%2520the%2520field%2520of%2520LLM%2520learning%252C%2520efficiently%2520serving%2520MoE%2520models%2520under%2520limited%2520GPU%250Amemory%2520constraints%2520has%2520emerged%2520as%2520a%2520significant%2520challenge.%2520Offloading%2520the%250Anon-activated%2520experts%2520to%2520main%2520memory%2520has%2520been%2520identified%2520as%2520an%2520efficient%250Aapproach%2520to%2520address%2520such%2520a%2520problem%252C%2520while%2520it%2520brings%2520the%2520challenges%2520of%250Atransferring%2520the%2520expert%2520between%2520the%2520GPU%2520memory%2520and%2520main%2520memory.%2520We%2520need%2520to%250Aexplore%2520an%2520efficient%2520approach%2520to%2520compress%2520the%2520expert%2520and%2520analyze%2520how%2520the%250Acompression%2520error%2520affects%2520the%2520inference%2520performance.%250A%2520%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520employing%2520error-bounded%2520lossy%2520compression%250Aalgorithms%2520%2528such%2520as%2520SZ3%2520and%2520CuSZp%2529%2520to%2520compress%2520non-activated%2520experts%252C%2520thereby%250Areducing%2520data%2520transfer%2520overhead%2520during%2520MoE%2520inference.%2520We%2520conduct%2520extensive%250Aexperiments%2520across%2520various%2520benchmarks%2520and%2520present%2520a%2520comprehensive%2520analysis%2520of%250Ahow%2520compression-induced%2520errors%2520in%2520different%2520experts%2520affect%2520overall%2520inference%250Aaccuracy.%2520The%2520results%2520indicate%2520that%2520experts%2520in%2520the%2520shallow%2520layers%252C%2520which%2520are%250Aprimarily%2520responsible%2520for%2520the%2520attention%2520mechanism%2520and%2520the%2520transformation%2520of%250Ainput%2520tokens%2520into%2520vector%2520representations%252C%2520exhibit%2520minimal%2520degradation%2520in%250Ainference%2520accuracy%2520when%2520subjected%2520to%2520bounded%2520errors.%2520In%2520contrast%252C%2520errors%2520in%2520the%250Amiddle-layer%2520experts%252C%2520which%2520are%2520central%2520to%2520model%2520reasoning%252C%2520significantly%250Aimpair%2520inference%2520accuracy.%2520Interestingly%252C%2520introducing%2520bounded%2520errors%2520in%2520the%250Adeep-layer%2520experts%252C%2520which%2520are%2520mainly%2520responsible%2520for%2520instruction%2520following%2520and%250Aoutput%2520integration%252C%2520can%2520sometimes%2520lead%2520to%2520improvements%2520in%2520inference%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoE-Compression%3A%20How%20the%20Compression%20Error%20of%20Experts%20Affects%20the%0A%20%20Inference%20Accuracy%20of%20MoE%20Model%3F&entry.906535625=Songkai%20Ma%20and%20Zhaorui%20Zhang%20and%20Sheng%20Di%20and%20Benben%20Liu%20and%20Xiaodong%20Yu%20and%20Xiaoyi%20Lu%20and%20Dan%20Wang&entry.1292438233=%20%20With%20the%20widespread%20application%20of%20Mixture%20of%20Experts%20%28MoE%29%20reasoning%20models%0Ain%20the%20field%20of%20LLM%20learning%2C%20efficiently%20serving%20MoE%20models%20under%20limited%20GPU%0Amemory%20constraints%20has%20emerged%20as%20a%20significant%20challenge.%20Offloading%20the%0Anon-activated%20experts%20to%20main%20memory%20has%20been%20identified%20as%20an%20efficient%0Aapproach%20to%20address%20such%20a%20problem%2C%20while%20it%20brings%20the%20challenges%20of%0Atransferring%20the%20expert%20between%20the%20GPU%20memory%20and%20main%20memory.%20We%20need%20to%0Aexplore%20an%20efficient%20approach%20to%20compress%20the%20expert%20and%20analyze%20how%20the%0Acompression%20error%20affects%20the%20inference%20performance.%0A%20%20To%20bridge%20this%20gap%2C%20we%20propose%20employing%20error-bounded%20lossy%20compression%0Aalgorithms%20%28such%20as%20SZ3%20and%20CuSZp%29%20to%20compress%20non-activated%20experts%2C%20thereby%0Areducing%20data%20transfer%20overhead%20during%20MoE%20inference.%20We%20conduct%20extensive%0Aexperiments%20across%20various%20benchmarks%20and%20present%20a%20comprehensive%20analysis%20of%0Ahow%20compression-induced%20errors%20in%20different%20experts%20affect%20overall%20inference%0Aaccuracy.%20The%20results%20indicate%20that%20experts%20in%20the%20shallow%20layers%2C%20which%20are%0Aprimarily%20responsible%20for%20the%20attention%20mechanism%20and%20the%20transformation%20of%0Ainput%20tokens%20into%20vector%20representations%2C%20exhibit%20minimal%20degradation%20in%0Ainference%20accuracy%20when%20subjected%20to%20bounded%20errors.%20In%20contrast%2C%20errors%20in%20the%0Amiddle-layer%20experts%2C%20which%20are%20central%20to%20model%20reasoning%2C%20significantly%0Aimpair%20inference%20accuracy.%20Interestingly%2C%20introducing%20bounded%20errors%20in%20the%0Adeep-layer%20experts%2C%20which%20are%20mainly%20responsible%20for%20instruction%20following%20and%0Aoutput%20integration%2C%20can%20sometimes%20lead%20to%20improvements%20in%20inference%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07727v1&entry.124074799=Read"},
{"title": "Customizing the Inductive Biases of Softmax Attention using Structured\n  Matrices", "author": "Yilun Kuang and Noah Amsel and Sanae Lotfi and Shikai Qiu and Andres Potapczynski and Andrew Gordon Wilson", "abstract": "  The core component of attention is the scoring function, which transforms the\ninputs into low-dimensional queries and keys and takes the dot product of each\npair. While the low-dimensional projection improves efficiency, it causes\ninformation loss for certain tasks that have intrinsically high-dimensional\ninputs. Additionally, attention uses the same scoring function for all input\npairs, without imposing a distance-dependent compute bias for neighboring\ntokens in the sequence. In this work, we address these shortcomings by\nproposing new scoring functions based on computationally efficient structured\nmatrices with high ranks, including Block Tensor-Train (BTT) and Multi-Level\nLow Rank (MLR) matrices. On in-context regression tasks with high-dimensional\ninputs, our proposed scoring functions outperform standard attention for any\nfixed compute budget. On language modeling, a task that exhibits locality\npatterns, our MLR-based attention method achieves improved scaling laws\ncompared to both standard attention and variants of sliding window attention.\nAdditionally, we show that both BTT and MLR fall under a broader family of\nefficient structured matrices capable of encoding either full-rank or\ndistance-dependent compute biases, thereby addressing significant shortcomings\nof standard attention. Finally, we show that MLR attention has promising\nresults for long-range time-series forecasting.\n", "link": "http://arxiv.org/abs/2509.07963v1", "date": "2025-09-09", "relevancy": 2.0225, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5183}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4971}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Customizing%20the%20Inductive%20Biases%20of%20Softmax%20Attention%20using%20Structured%0A%20%20Matrices&body=Title%3A%20Customizing%20the%20Inductive%20Biases%20of%20Softmax%20Attention%20using%20Structured%0A%20%20Matrices%0AAuthor%3A%20Yilun%20Kuang%20and%20Noah%20Amsel%20and%20Sanae%20Lotfi%20and%20Shikai%20Qiu%20and%20Andres%20Potapczynski%20and%20Andrew%20Gordon%20Wilson%0AAbstract%3A%20%20%20The%20core%20component%20of%20attention%20is%20the%20scoring%20function%2C%20which%20transforms%20the%0Ainputs%20into%20low-dimensional%20queries%20and%20keys%20and%20takes%20the%20dot%20product%20of%20each%0Apair.%20While%20the%20low-dimensional%20projection%20improves%20efficiency%2C%20it%20causes%0Ainformation%20loss%20for%20certain%20tasks%20that%20have%20intrinsically%20high-dimensional%0Ainputs.%20Additionally%2C%20attention%20uses%20the%20same%20scoring%20function%20for%20all%20input%0Apairs%2C%20without%20imposing%20a%20distance-dependent%20compute%20bias%20for%20neighboring%0Atokens%20in%20the%20sequence.%20In%20this%20work%2C%20we%20address%20these%20shortcomings%20by%0Aproposing%20new%20scoring%20functions%20based%20on%20computationally%20efficient%20structured%0Amatrices%20with%20high%20ranks%2C%20including%20Block%20Tensor-Train%20%28BTT%29%20and%20Multi-Level%0ALow%20Rank%20%28MLR%29%20matrices.%20On%20in-context%20regression%20tasks%20with%20high-dimensional%0Ainputs%2C%20our%20proposed%20scoring%20functions%20outperform%20standard%20attention%20for%20any%0Afixed%20compute%20budget.%20On%20language%20modeling%2C%20a%20task%20that%20exhibits%20locality%0Apatterns%2C%20our%20MLR-based%20attention%20method%20achieves%20improved%20scaling%20laws%0Acompared%20to%20both%20standard%20attention%20and%20variants%20of%20sliding%20window%20attention.%0AAdditionally%2C%20we%20show%20that%20both%20BTT%20and%20MLR%20fall%20under%20a%20broader%20family%20of%0Aefficient%20structured%20matrices%20capable%20of%20encoding%20either%20full-rank%20or%0Adistance-dependent%20compute%20biases%2C%20thereby%20addressing%20significant%20shortcomings%0Aof%20standard%20attention.%20Finally%2C%20we%20show%20that%20MLR%20attention%20has%20promising%0Aresults%20for%20long-range%20time-series%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCustomizing%2520the%2520Inductive%2520Biases%2520of%2520Softmax%2520Attention%2520using%2520Structured%250A%2520%2520Matrices%26entry.906535625%3DYilun%2520Kuang%2520and%2520Noah%2520Amsel%2520and%2520Sanae%2520Lotfi%2520and%2520Shikai%2520Qiu%2520and%2520Andres%2520Potapczynski%2520and%2520Andrew%2520Gordon%2520Wilson%26entry.1292438233%3D%2520%2520The%2520core%2520component%2520of%2520attention%2520is%2520the%2520scoring%2520function%252C%2520which%2520transforms%2520the%250Ainputs%2520into%2520low-dimensional%2520queries%2520and%2520keys%2520and%2520takes%2520the%2520dot%2520product%2520of%2520each%250Apair.%2520While%2520the%2520low-dimensional%2520projection%2520improves%2520efficiency%252C%2520it%2520causes%250Ainformation%2520loss%2520for%2520certain%2520tasks%2520that%2520have%2520intrinsically%2520high-dimensional%250Ainputs.%2520Additionally%252C%2520attention%2520uses%2520the%2520same%2520scoring%2520function%2520for%2520all%2520input%250Apairs%252C%2520without%2520imposing%2520a%2520distance-dependent%2520compute%2520bias%2520for%2520neighboring%250Atokens%2520in%2520the%2520sequence.%2520In%2520this%2520work%252C%2520we%2520address%2520these%2520shortcomings%2520by%250Aproposing%2520new%2520scoring%2520functions%2520based%2520on%2520computationally%2520efficient%2520structured%250Amatrices%2520with%2520high%2520ranks%252C%2520including%2520Block%2520Tensor-Train%2520%2528BTT%2529%2520and%2520Multi-Level%250ALow%2520Rank%2520%2528MLR%2529%2520matrices.%2520On%2520in-context%2520regression%2520tasks%2520with%2520high-dimensional%250Ainputs%252C%2520our%2520proposed%2520scoring%2520functions%2520outperform%2520standard%2520attention%2520for%2520any%250Afixed%2520compute%2520budget.%2520On%2520language%2520modeling%252C%2520a%2520task%2520that%2520exhibits%2520locality%250Apatterns%252C%2520our%2520MLR-based%2520attention%2520method%2520achieves%2520improved%2520scaling%2520laws%250Acompared%2520to%2520both%2520standard%2520attention%2520and%2520variants%2520of%2520sliding%2520window%2520attention.%250AAdditionally%252C%2520we%2520show%2520that%2520both%2520BTT%2520and%2520MLR%2520fall%2520under%2520a%2520broader%2520family%2520of%250Aefficient%2520structured%2520matrices%2520capable%2520of%2520encoding%2520either%2520full-rank%2520or%250Adistance-dependent%2520compute%2520biases%252C%2520thereby%2520addressing%2520significant%2520shortcomings%250Aof%2520standard%2520attention.%2520Finally%252C%2520we%2520show%2520that%2520MLR%2520attention%2520has%2520promising%250Aresults%2520for%2520long-range%2520time-series%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Customizing%20the%20Inductive%20Biases%20of%20Softmax%20Attention%20using%20Structured%0A%20%20Matrices&entry.906535625=Yilun%20Kuang%20and%20Noah%20Amsel%20and%20Sanae%20Lotfi%20and%20Shikai%20Qiu%20and%20Andres%20Potapczynski%20and%20Andrew%20Gordon%20Wilson&entry.1292438233=%20%20The%20core%20component%20of%20attention%20is%20the%20scoring%20function%2C%20which%20transforms%20the%0Ainputs%20into%20low-dimensional%20queries%20and%20keys%20and%20takes%20the%20dot%20product%20of%20each%0Apair.%20While%20the%20low-dimensional%20projection%20improves%20efficiency%2C%20it%20causes%0Ainformation%20loss%20for%20certain%20tasks%20that%20have%20intrinsically%20high-dimensional%0Ainputs.%20Additionally%2C%20attention%20uses%20the%20same%20scoring%20function%20for%20all%20input%0Apairs%2C%20without%20imposing%20a%20distance-dependent%20compute%20bias%20for%20neighboring%0Atokens%20in%20the%20sequence.%20In%20this%20work%2C%20we%20address%20these%20shortcomings%20by%0Aproposing%20new%20scoring%20functions%20based%20on%20computationally%20efficient%20structured%0Amatrices%20with%20high%20ranks%2C%20including%20Block%20Tensor-Train%20%28BTT%29%20and%20Multi-Level%0ALow%20Rank%20%28MLR%29%20matrices.%20On%20in-context%20regression%20tasks%20with%20high-dimensional%0Ainputs%2C%20our%20proposed%20scoring%20functions%20outperform%20standard%20attention%20for%20any%0Afixed%20compute%20budget.%20On%20language%20modeling%2C%20a%20task%20that%20exhibits%20locality%0Apatterns%2C%20our%20MLR-based%20attention%20method%20achieves%20improved%20scaling%20laws%0Acompared%20to%20both%20standard%20attention%20and%20variants%20of%20sliding%20window%20attention.%0AAdditionally%2C%20we%20show%20that%20both%20BTT%20and%20MLR%20fall%20under%20a%20broader%20family%20of%0Aefficient%20structured%20matrices%20capable%20of%20encoding%20either%20full-rank%20or%0Adistance-dependent%20compute%20biases%2C%20thereby%20addressing%20significant%20shortcomings%0Aof%20standard%20attention.%20Finally%2C%20we%20show%20that%20MLR%20attention%20has%20promising%0Aresults%20for%20long-range%20time-series%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07963v1&entry.124074799=Read"},
{"title": "Small Open Models Achieve Near Parity with Large Models in Low Resource\n  Literary Translation at a Fraction of the Cost", "author": "Mihai Nadas and Laura Diosan and Andreea Tomescu and Andrei Piscoran", "abstract": "  Literary translation has recently gained attention as a distinct and complex\ntask in machine translation research. However, the translation by small open\nmodels remains an open problem. We contribute to this ongoing research by\nintroducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for\ndataset creation, fine tuning, and evaluation in English-Romanian literary\ntranslations, centred on the creation and open release of both a compact, fine\ntuned language model (TF2-12B) and large scale synthetic parallel datasets\n(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the\nlargest collection of synthetic English fables to date, we address the need for\nrich, high quality literary datasets in low resource languages such as\nRomanian. Our pipeline first generates 15k high quality Romanian references\nfrom the TF1 pool using a high performing LLM. We then apply a two stage fine\ntuning process to a 12B parameter open weight model: (i) instruction tuning to\ncapture genre specific narrative style, and (ii) adapter compression for\nefficient deployment. Evaluation combines corpus level BLEU and a five\ndimension LLM based rubric (accuracy, fluency, coherence, style, cultural\nadaptation) to provide a nuanced assessment of translation quality. Results\nshow that our fine tuned model achieves fluency and adequacy competitive with\ntop performing large proprietary models, while being open, accessible, and\nsignificantly more cost effective. Alongside the fine tuned model and both\ndatasets, we publicly release all scripts and evaluation prompts. TF2 thus\nprovides an end-to-end, reproducible pipeline for research on cost efficient\ntranslation, cross lingual narrative generation, and the broad adoption of open\nmodels for culturally significant literary content in low resource settings.\n", "link": "http://arxiv.org/abs/2509.07829v1", "date": "2025-09-09", "relevancy": 2.0222, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5349}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Small%20Open%20Models%20Achieve%20Near%20Parity%20with%20Large%20Models%20in%20Low%20Resource%0A%20%20Literary%20Translation%20at%20a%20Fraction%20of%20the%20Cost&body=Title%3A%20Small%20Open%20Models%20Achieve%20Near%20Parity%20with%20Large%20Models%20in%20Low%20Resource%0A%20%20Literary%20Translation%20at%20a%20Fraction%20of%20the%20Cost%0AAuthor%3A%20Mihai%20Nadas%20and%20Laura%20Diosan%20and%20Andreea%20Tomescu%20and%20Andrei%20Piscoran%0AAbstract%3A%20%20%20Literary%20translation%20has%20recently%20gained%20attention%20as%20a%20distinct%20and%20complex%0Atask%20in%20machine%20translation%20research.%20However%2C%20the%20translation%20by%20small%20open%0Amodels%20remains%20an%20open%20problem.%20We%20contribute%20to%20this%20ongoing%20research%20by%0Aintroducing%20TINYFABULIST%20TRANSLATION%20FRAMEWORK%20%28TF2%29%2C%20a%20unified%20framework%20for%0Adataset%20creation%2C%20fine%20tuning%2C%20and%20evaluation%20in%20English-Romanian%20literary%0Atranslations%2C%20centred%20on%20the%20creation%20and%20open%20release%20of%20both%20a%20compact%2C%20fine%0Atuned%20language%20model%20%28TF2-12B%29%20and%20large%20scale%20synthetic%20parallel%20datasets%0A%28DS-TF2-EN-RO-3M%20and%20DS-TF2-EN-RO-15K%29.%20Building%20on%20DS-TF1-EN-3M%20%28TF1%29%2C%20the%0Alargest%20collection%20of%20synthetic%20English%20fables%20to%20date%2C%20we%20address%20the%20need%20for%0Arich%2C%20high%20quality%20literary%20datasets%20in%20low%20resource%20languages%20such%20as%0ARomanian.%20Our%20pipeline%20first%20generates%2015k%20high%20quality%20Romanian%20references%0Afrom%20the%20TF1%20pool%20using%20a%20high%20performing%20LLM.%20We%20then%20apply%20a%20two%20stage%20fine%0Atuning%20process%20to%20a%2012B%20parameter%20open%20weight%20model%3A%20%28i%29%20instruction%20tuning%20to%0Acapture%20genre%20specific%20narrative%20style%2C%20and%20%28ii%29%20adapter%20compression%20for%0Aefficient%20deployment.%20Evaluation%20combines%20corpus%20level%20BLEU%20and%20a%20five%0Adimension%20LLM%20based%20rubric%20%28accuracy%2C%20fluency%2C%20coherence%2C%20style%2C%20cultural%0Aadaptation%29%20to%20provide%20a%20nuanced%20assessment%20of%20translation%20quality.%20Results%0Ashow%20that%20our%20fine%20tuned%20model%20achieves%20fluency%20and%20adequacy%20competitive%20with%0Atop%20performing%20large%20proprietary%20models%2C%20while%20being%20open%2C%20accessible%2C%20and%0Asignificantly%20more%20cost%20effective.%20Alongside%20the%20fine%20tuned%20model%20and%20both%0Adatasets%2C%20we%20publicly%20release%20all%20scripts%20and%20evaluation%20prompts.%20TF2%20thus%0Aprovides%20an%20end-to-end%2C%20reproducible%20pipeline%20for%20research%20on%20cost%20efficient%0Atranslation%2C%20cross%20lingual%20narrative%20generation%2C%20and%20the%20broad%20adoption%20of%20open%0Amodels%20for%20culturally%20significant%20literary%20content%20in%20low%20resource%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmall%2520Open%2520Models%2520Achieve%2520Near%2520Parity%2520with%2520Large%2520Models%2520in%2520Low%2520Resource%250A%2520%2520Literary%2520Translation%2520at%2520a%2520Fraction%2520of%2520the%2520Cost%26entry.906535625%3DMihai%2520Nadas%2520and%2520Laura%2520Diosan%2520and%2520Andreea%2520Tomescu%2520and%2520Andrei%2520Piscoran%26entry.1292438233%3D%2520%2520Literary%2520translation%2520has%2520recently%2520gained%2520attention%2520as%2520a%2520distinct%2520and%2520complex%250Atask%2520in%2520machine%2520translation%2520research.%2520However%252C%2520the%2520translation%2520by%2520small%2520open%250Amodels%2520remains%2520an%2520open%2520problem.%2520We%2520contribute%2520to%2520this%2520ongoing%2520research%2520by%250Aintroducing%2520TINYFABULIST%2520TRANSLATION%2520FRAMEWORK%2520%2528TF2%2529%252C%2520a%2520unified%2520framework%2520for%250Adataset%2520creation%252C%2520fine%2520tuning%252C%2520and%2520evaluation%2520in%2520English-Romanian%2520literary%250Atranslations%252C%2520centred%2520on%2520the%2520creation%2520and%2520open%2520release%2520of%2520both%2520a%2520compact%252C%2520fine%250Atuned%2520language%2520model%2520%2528TF2-12B%2529%2520and%2520large%2520scale%2520synthetic%2520parallel%2520datasets%250A%2528DS-TF2-EN-RO-3M%2520and%2520DS-TF2-EN-RO-15K%2529.%2520Building%2520on%2520DS-TF1-EN-3M%2520%2528TF1%2529%252C%2520the%250Alargest%2520collection%2520of%2520synthetic%2520English%2520fables%2520to%2520date%252C%2520we%2520address%2520the%2520need%2520for%250Arich%252C%2520high%2520quality%2520literary%2520datasets%2520in%2520low%2520resource%2520languages%2520such%2520as%250ARomanian.%2520Our%2520pipeline%2520first%2520generates%252015k%2520high%2520quality%2520Romanian%2520references%250Afrom%2520the%2520TF1%2520pool%2520using%2520a%2520high%2520performing%2520LLM.%2520We%2520then%2520apply%2520a%2520two%2520stage%2520fine%250Atuning%2520process%2520to%2520a%252012B%2520parameter%2520open%2520weight%2520model%253A%2520%2528i%2529%2520instruction%2520tuning%2520to%250Acapture%2520genre%2520specific%2520narrative%2520style%252C%2520and%2520%2528ii%2529%2520adapter%2520compression%2520for%250Aefficient%2520deployment.%2520Evaluation%2520combines%2520corpus%2520level%2520BLEU%2520and%2520a%2520five%250Adimension%2520LLM%2520based%2520rubric%2520%2528accuracy%252C%2520fluency%252C%2520coherence%252C%2520style%252C%2520cultural%250Aadaptation%2529%2520to%2520provide%2520a%2520nuanced%2520assessment%2520of%2520translation%2520quality.%2520Results%250Ashow%2520that%2520our%2520fine%2520tuned%2520model%2520achieves%2520fluency%2520and%2520adequacy%2520competitive%2520with%250Atop%2520performing%2520large%2520proprietary%2520models%252C%2520while%2520being%2520open%252C%2520accessible%252C%2520and%250Asignificantly%2520more%2520cost%2520effective.%2520Alongside%2520the%2520fine%2520tuned%2520model%2520and%2520both%250Adatasets%252C%2520we%2520publicly%2520release%2520all%2520scripts%2520and%2520evaluation%2520prompts.%2520TF2%2520thus%250Aprovides%2520an%2520end-to-end%252C%2520reproducible%2520pipeline%2520for%2520research%2520on%2520cost%2520efficient%250Atranslation%252C%2520cross%2520lingual%2520narrative%2520generation%252C%2520and%2520the%2520broad%2520adoption%2520of%2520open%250Amodels%2520for%2520culturally%2520significant%2520literary%2520content%2520in%2520low%2520resource%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Small%20Open%20Models%20Achieve%20Near%20Parity%20with%20Large%20Models%20in%20Low%20Resource%0A%20%20Literary%20Translation%20at%20a%20Fraction%20of%20the%20Cost&entry.906535625=Mihai%20Nadas%20and%20Laura%20Diosan%20and%20Andreea%20Tomescu%20and%20Andrei%20Piscoran&entry.1292438233=%20%20Literary%20translation%20has%20recently%20gained%20attention%20as%20a%20distinct%20and%20complex%0Atask%20in%20machine%20translation%20research.%20However%2C%20the%20translation%20by%20small%20open%0Amodels%20remains%20an%20open%20problem.%20We%20contribute%20to%20this%20ongoing%20research%20by%0Aintroducing%20TINYFABULIST%20TRANSLATION%20FRAMEWORK%20%28TF2%29%2C%20a%20unified%20framework%20for%0Adataset%20creation%2C%20fine%20tuning%2C%20and%20evaluation%20in%20English-Romanian%20literary%0Atranslations%2C%20centred%20on%20the%20creation%20and%20open%20release%20of%20both%20a%20compact%2C%20fine%0Atuned%20language%20model%20%28TF2-12B%29%20and%20large%20scale%20synthetic%20parallel%20datasets%0A%28DS-TF2-EN-RO-3M%20and%20DS-TF2-EN-RO-15K%29.%20Building%20on%20DS-TF1-EN-3M%20%28TF1%29%2C%20the%0Alargest%20collection%20of%20synthetic%20English%20fables%20to%20date%2C%20we%20address%20the%20need%20for%0Arich%2C%20high%20quality%20literary%20datasets%20in%20low%20resource%20languages%20such%20as%0ARomanian.%20Our%20pipeline%20first%20generates%2015k%20high%20quality%20Romanian%20references%0Afrom%20the%20TF1%20pool%20using%20a%20high%20performing%20LLM.%20We%20then%20apply%20a%20two%20stage%20fine%0Atuning%20process%20to%20a%2012B%20parameter%20open%20weight%20model%3A%20%28i%29%20instruction%20tuning%20to%0Acapture%20genre%20specific%20narrative%20style%2C%20and%20%28ii%29%20adapter%20compression%20for%0Aefficient%20deployment.%20Evaluation%20combines%20corpus%20level%20BLEU%20and%20a%20five%0Adimension%20LLM%20based%20rubric%20%28accuracy%2C%20fluency%2C%20coherence%2C%20style%2C%20cultural%0Aadaptation%29%20to%20provide%20a%20nuanced%20assessment%20of%20translation%20quality.%20Results%0Ashow%20that%20our%20fine%20tuned%20model%20achieves%20fluency%20and%20adequacy%20competitive%20with%0Atop%20performing%20large%20proprietary%20models%2C%20while%20being%20open%2C%20accessible%2C%20and%0Asignificantly%20more%20cost%20effective.%20Alongside%20the%20fine%20tuned%20model%20and%20both%0Adatasets%2C%20we%20publicly%20release%20all%20scripts%20and%20evaluation%20prompts.%20TF2%20thus%0Aprovides%20an%20end-to-end%2C%20reproducible%20pipeline%20for%20research%20on%20cost%20efficient%0Atranslation%2C%20cross%20lingual%20narrative%20generation%2C%20and%20the%20broad%20adoption%20of%20open%0Amodels%20for%20culturally%20significant%20literary%20content%20in%20low%20resource%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07829v1&entry.124074799=Read"},
{"title": "Llama-Nemotron: Efficient Reasoning Models", "author": "Akhiad Bercovich and Itay Levy and Izik Golan and Mohammad Dabbah and Ran El-Yaniv and Omri Puny and Ido Galil and Zach Moshe and Tomer Ronen and Najeeb Nabwani and Ido Shahaf and Oren Tropp and Ehud Karpas and Ran Zilberstein and Jiaqi Zeng and Soumye Singhal and Alexander Bukharin and Yian Zhang and Tugrul Konuk and Gerald Shen and Ameya Sunil Mahabaleshwarkar and Bilal Kartal and Yoshi Suhara and Olivier Delalleau and Zijia Chen and Zhilin Wang and David Mosallanezhad and Adi Renduchintala and Haifeng Qian and Dima Rekesh and Fei Jia and Somshubra Majumdar and Vahid Noroozi and Wasi Uddin Ahmad and Sean Narenthiran and Aleksander Ficek and Mehrzad Samadi and Jocelyn Huang and Siddhartha Jain and Igor Gitman and Ivan Moshkov and Wei Du and Shubham Toshniwal and George Armstrong and Branislav Kisacanin and Matvei Novikov and Daria Gitman and Evelina Bakhturina and Prasoon Varshney and Makesh Narsimhan and Jane Polak Scowcroft and John Kamalu and Dan Su and Kezhi Kong and Markus Kliegl and Rabeeh Karimi Mahabadi and Ying Lin and Sanjeev Satheesh and Jupinder Parmar and Pritam Gundecha and Brandon Norick and Joseph Jennings and Shrimai Prabhumoye and Syeda Nahida Akter and Mostofa Patwary and Abhinav Khattar and Deepak Narayanan and Roger Waleffe and Jimmy Zhang and Bor-Yiing Su and Guyue Huang and Terry Kong and Parth Chadha and Sahil Jain and Christine Harvey and Elad Segal and Jining Huang and Sergey Kashirsky and Robert McQueen and Izzy Putterman and George Lam and Arun Venkatesan and Sherry Wu and Vinh Nguyen and Manoj Kilaru and Andrew Wang and Anna Warno and Abhilash Somasamudramath and Sandip Bhaskar and Maka Dong and Nave Assaf and Shahar Mor and Omer Ullman Argov and Scot Junkin and Oleksandr Romanenko and Pedro Larroy and Monika Katariya and Marco Rovinelli and Viji Balas and Nicholas Edelman and Anahita Bhiwandiwalla and Muthu Subramaniam and Smita Ithape and Karthik Ramamoorthy and Yuting Wu and Suguna Varshini Velury and Omri Almog and Joyjit Daw and Denys Fridman and Erick Galinkin and Michael Evans and Shaona Ghosh and Katherine Luna and Leon Derczynski and Nikki Pope and Eileen Long and Seth Schneider and Guillermo Siman and Tomasz Grzegorzek and Pablo Ribalta and Monika Katariya and Chris Alexiuk and Joey Conway and Trisha Saar and Ann Guan and Krzysztof Pawelec and Shyamala Prayaga and Oleksii Kuchaiev and Boris Ginsburg and Oluwatobi Olabiyi and Kari Briski and Jonathan Cohen and Bryan Catanzaro and Jonah Alben and Yonatan Geifman and Eric Chung", "abstract": "  We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.\n", "link": "http://arxiv.org/abs/2505.00949v5", "date": "2025-09-09", "relevancy": 2.0179, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Llama-Nemotron%3A%20Efficient%20Reasoning%20Models&body=Title%3A%20Llama-Nemotron%3A%20Efficient%20Reasoning%20Models%0AAuthor%3A%20Akhiad%20Bercovich%20and%20Itay%20Levy%20and%20Izik%20Golan%20and%20Mohammad%20Dabbah%20and%20Ran%20El-Yaniv%20and%20Omri%20Puny%20and%20Ido%20Galil%20and%20Zach%20Moshe%20and%20Tomer%20Ronen%20and%20Najeeb%20Nabwani%20and%20Ido%20Shahaf%20and%20Oren%20Tropp%20and%20Ehud%20Karpas%20and%20Ran%20Zilberstein%20and%20Jiaqi%20Zeng%20and%20Soumye%20Singhal%20and%20Alexander%20Bukharin%20and%20Yian%20Zhang%20and%20Tugrul%20Konuk%20and%20Gerald%20Shen%20and%20Ameya%20Sunil%20Mahabaleshwarkar%20and%20Bilal%20Kartal%20and%20Yoshi%20Suhara%20and%20Olivier%20Delalleau%20and%20Zijia%20Chen%20and%20Zhilin%20Wang%20and%20David%20Mosallanezhad%20and%20Adi%20Renduchintala%20and%20Haifeng%20Qian%20and%20Dima%20Rekesh%20and%20Fei%20Jia%20and%20Somshubra%20Majumdar%20and%20Vahid%20Noroozi%20and%20Wasi%20Uddin%20Ahmad%20and%20Sean%20Narenthiran%20and%20Aleksander%20Ficek%20and%20Mehrzad%20Samadi%20and%20Jocelyn%20Huang%20and%20Siddhartha%20Jain%20and%20Igor%20Gitman%20and%20Ivan%20Moshkov%20and%20Wei%20Du%20and%20Shubham%20Toshniwal%20and%20George%20Armstrong%20and%20Branislav%20Kisacanin%20and%20Matvei%20Novikov%20and%20Daria%20Gitman%20and%20Evelina%20Bakhturina%20and%20Prasoon%20Varshney%20and%20Makesh%20Narsimhan%20and%20Jane%20Polak%20Scowcroft%20and%20John%20Kamalu%20and%20Dan%20Su%20and%20Kezhi%20Kong%20and%20Markus%20Kliegl%20and%20Rabeeh%20Karimi%20Mahabadi%20and%20Ying%20Lin%20and%20Sanjeev%20Satheesh%20and%20Jupinder%20Parmar%20and%20Pritam%20Gundecha%20and%20Brandon%20Norick%20and%20Joseph%20Jennings%20and%20Shrimai%20Prabhumoye%20and%20Syeda%20Nahida%20Akter%20and%20Mostofa%20Patwary%20and%20Abhinav%20Khattar%20and%20Deepak%20Narayanan%20and%20Roger%20Waleffe%20and%20Jimmy%20Zhang%20and%20Bor-Yiing%20Su%20and%20Guyue%20Huang%20and%20Terry%20Kong%20and%20Parth%20Chadha%20and%20Sahil%20Jain%20and%20Christine%20Harvey%20and%20Elad%20Segal%20and%20Jining%20Huang%20and%20Sergey%20Kashirsky%20and%20Robert%20McQueen%20and%20Izzy%20Putterman%20and%20George%20Lam%20and%20Arun%20Venkatesan%20and%20Sherry%20Wu%20and%20Vinh%20Nguyen%20and%20Manoj%20Kilaru%20and%20Andrew%20Wang%20and%20Anna%20Warno%20and%20Abhilash%20Somasamudramath%20and%20Sandip%20Bhaskar%20and%20Maka%20Dong%20and%20Nave%20Assaf%20and%20Shahar%20Mor%20and%20Omer%20Ullman%20Argov%20and%20Scot%20Junkin%20and%20Oleksandr%20Romanenko%20and%20Pedro%20Larroy%20and%20Monika%20Katariya%20and%20Marco%20Rovinelli%20and%20Viji%20Balas%20and%20Nicholas%20Edelman%20and%20Anahita%20Bhiwandiwalla%20and%20Muthu%20Subramaniam%20and%20Smita%20Ithape%20and%20Karthik%20Ramamoorthy%20and%20Yuting%20Wu%20and%20Suguna%20Varshini%20Velury%20and%20Omri%20Almog%20and%20Joyjit%20Daw%20and%20Denys%20Fridman%20and%20Erick%20Galinkin%20and%20Michael%20Evans%20and%20Shaona%20Ghosh%20and%20Katherine%20Luna%20and%20Leon%20Derczynski%20and%20Nikki%20Pope%20and%20Eileen%20Long%20and%20Seth%20Schneider%20and%20Guillermo%20Siman%20and%20Tomasz%20Grzegorzek%20and%20Pablo%20Ribalta%20and%20Monika%20Katariya%20and%20Chris%20Alexiuk%20and%20Joey%20Conway%20and%20Trisha%20Saar%20and%20Ann%20Guan%20and%20Krzysztof%20Pawelec%20and%20Shyamala%20Prayaga%20and%20Oleksii%20Kuchaiev%20and%20Boris%20Ginsburg%20and%20Oluwatobi%20Olabiyi%20and%20Kari%20Briski%20and%20Jonathan%20Cohen%20and%20Bryan%20Catanzaro%20and%20Jonah%20Alben%20and%20Yonatan%20Geifman%20and%20Eric%20Chung%0AAbstract%3A%20%20%20We%20introduce%20the%20Llama-Nemotron%20series%20of%20models%2C%20an%20open%20family%20of%0Aheterogeneous%20reasoning%20models%20that%20deliver%20exceptional%20reasoning%20capabilities%2C%0Ainference%20efficiency%2C%20and%20an%20open%20license%20for%20enterprise%20use.%20The%20family%20comes%0Ain%20three%20sizes%20--%20Nano%20%288B%29%2C%20Super%20%2849B%29%2C%20and%20Ultra%20%28253B%29%20--%20and%20performs%0Acompetitively%20with%20state-of-the-art%20reasoning%20models%20such%20as%20DeepSeek-R1%20while%0Aoffering%20superior%20inference%20throughput%20and%20memory%20efficiency.%20In%20this%20report%2C%0Awe%20discuss%20the%20training%20procedure%20for%20these%20models%2C%20which%20entails%20using%20neural%0Aarchitecture%20search%20from%20Llama%203%20models%20for%20accelerated%20inference%2C%20knowledge%0Adistillation%2C%20and%20continued%20pretraining%2C%20followed%20by%20a%20reasoning-focused%0Apost-training%20stage%20consisting%20of%20two%20main%20parts%3A%20supervised%20fine-tuning%20and%0Alarge%20scale%20reinforcement%20learning.%20Llama-Nemotron%20models%20are%20the%20first%0Aopen-source%20models%20to%20support%20a%20dynamic%20reasoning%20toggle%2C%20allowing%20users%20to%0Aswitch%20between%20standard%20chat%20and%20reasoning%20modes%20during%20inference.%20To%20further%0Asupport%20open%20research%20and%20facilitate%20model%20development%2C%20we%20provide%20the%0Afollowing%20resources%3A%201.%20We%20release%20the%20Llama-Nemotron%20reasoning%20models%20--%0ALN-Nano%2C%20LN-Super%2C%20and%20LN-Ultra%20--%20under%20the%20commercially%20permissive%20NVIDIA%0AOpen%20Model%20License%20Agreement.%202.%20We%20release%20the%20complete%20post-training%20dataset%3A%0ALlama-Nemotron-Post-Training-Dataset.%203.%20We%20also%20release%20our%20training%0Acodebases%3A%20NeMo%2C%20NeMo-Aligner%2C%20and%20Megatron-LM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00949v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLlama-Nemotron%253A%2520Efficient%2520Reasoning%2520Models%26entry.906535625%3DAkhiad%2520Bercovich%2520and%2520Itay%2520Levy%2520and%2520Izik%2520Golan%2520and%2520Mohammad%2520Dabbah%2520and%2520Ran%2520El-Yaniv%2520and%2520Omri%2520Puny%2520and%2520Ido%2520Galil%2520and%2520Zach%2520Moshe%2520and%2520Tomer%2520Ronen%2520and%2520Najeeb%2520Nabwani%2520and%2520Ido%2520Shahaf%2520and%2520Oren%2520Tropp%2520and%2520Ehud%2520Karpas%2520and%2520Ran%2520Zilberstein%2520and%2520Jiaqi%2520Zeng%2520and%2520Soumye%2520Singhal%2520and%2520Alexander%2520Bukharin%2520and%2520Yian%2520Zhang%2520and%2520Tugrul%2520Konuk%2520and%2520Gerald%2520Shen%2520and%2520Ameya%2520Sunil%2520Mahabaleshwarkar%2520and%2520Bilal%2520Kartal%2520and%2520Yoshi%2520Suhara%2520and%2520Olivier%2520Delalleau%2520and%2520Zijia%2520Chen%2520and%2520Zhilin%2520Wang%2520and%2520David%2520Mosallanezhad%2520and%2520Adi%2520Renduchintala%2520and%2520Haifeng%2520Qian%2520and%2520Dima%2520Rekesh%2520and%2520Fei%2520Jia%2520and%2520Somshubra%2520Majumdar%2520and%2520Vahid%2520Noroozi%2520and%2520Wasi%2520Uddin%2520Ahmad%2520and%2520Sean%2520Narenthiran%2520and%2520Aleksander%2520Ficek%2520and%2520Mehrzad%2520Samadi%2520and%2520Jocelyn%2520Huang%2520and%2520Siddhartha%2520Jain%2520and%2520Igor%2520Gitman%2520and%2520Ivan%2520Moshkov%2520and%2520Wei%2520Du%2520and%2520Shubham%2520Toshniwal%2520and%2520George%2520Armstrong%2520and%2520Branislav%2520Kisacanin%2520and%2520Matvei%2520Novikov%2520and%2520Daria%2520Gitman%2520and%2520Evelina%2520Bakhturina%2520and%2520Prasoon%2520Varshney%2520and%2520Makesh%2520Narsimhan%2520and%2520Jane%2520Polak%2520Scowcroft%2520and%2520John%2520Kamalu%2520and%2520Dan%2520Su%2520and%2520Kezhi%2520Kong%2520and%2520Markus%2520Kliegl%2520and%2520Rabeeh%2520Karimi%2520Mahabadi%2520and%2520Ying%2520Lin%2520and%2520Sanjeev%2520Satheesh%2520and%2520Jupinder%2520Parmar%2520and%2520Pritam%2520Gundecha%2520and%2520Brandon%2520Norick%2520and%2520Joseph%2520Jennings%2520and%2520Shrimai%2520Prabhumoye%2520and%2520Syeda%2520Nahida%2520Akter%2520and%2520Mostofa%2520Patwary%2520and%2520Abhinav%2520Khattar%2520and%2520Deepak%2520Narayanan%2520and%2520Roger%2520Waleffe%2520and%2520Jimmy%2520Zhang%2520and%2520Bor-Yiing%2520Su%2520and%2520Guyue%2520Huang%2520and%2520Terry%2520Kong%2520and%2520Parth%2520Chadha%2520and%2520Sahil%2520Jain%2520and%2520Christine%2520Harvey%2520and%2520Elad%2520Segal%2520and%2520Jining%2520Huang%2520and%2520Sergey%2520Kashirsky%2520and%2520Robert%2520McQueen%2520and%2520Izzy%2520Putterman%2520and%2520George%2520Lam%2520and%2520Arun%2520Venkatesan%2520and%2520Sherry%2520Wu%2520and%2520Vinh%2520Nguyen%2520and%2520Manoj%2520Kilaru%2520and%2520Andrew%2520Wang%2520and%2520Anna%2520Warno%2520and%2520Abhilash%2520Somasamudramath%2520and%2520Sandip%2520Bhaskar%2520and%2520Maka%2520Dong%2520and%2520Nave%2520Assaf%2520and%2520Shahar%2520Mor%2520and%2520Omer%2520Ullman%2520Argov%2520and%2520Scot%2520Junkin%2520and%2520Oleksandr%2520Romanenko%2520and%2520Pedro%2520Larroy%2520and%2520Monika%2520Katariya%2520and%2520Marco%2520Rovinelli%2520and%2520Viji%2520Balas%2520and%2520Nicholas%2520Edelman%2520and%2520Anahita%2520Bhiwandiwalla%2520and%2520Muthu%2520Subramaniam%2520and%2520Smita%2520Ithape%2520and%2520Karthik%2520Ramamoorthy%2520and%2520Yuting%2520Wu%2520and%2520Suguna%2520Varshini%2520Velury%2520and%2520Omri%2520Almog%2520and%2520Joyjit%2520Daw%2520and%2520Denys%2520Fridman%2520and%2520Erick%2520Galinkin%2520and%2520Michael%2520Evans%2520and%2520Shaona%2520Ghosh%2520and%2520Katherine%2520Luna%2520and%2520Leon%2520Derczynski%2520and%2520Nikki%2520Pope%2520and%2520Eileen%2520Long%2520and%2520Seth%2520Schneider%2520and%2520Guillermo%2520Siman%2520and%2520Tomasz%2520Grzegorzek%2520and%2520Pablo%2520Ribalta%2520and%2520Monika%2520Katariya%2520and%2520Chris%2520Alexiuk%2520and%2520Joey%2520Conway%2520and%2520Trisha%2520Saar%2520and%2520Ann%2520Guan%2520and%2520Krzysztof%2520Pawelec%2520and%2520Shyamala%2520Prayaga%2520and%2520Oleksii%2520Kuchaiev%2520and%2520Boris%2520Ginsburg%2520and%2520Oluwatobi%2520Olabiyi%2520and%2520Kari%2520Briski%2520and%2520Jonathan%2520Cohen%2520and%2520Bryan%2520Catanzaro%2520and%2520Jonah%2520Alben%2520and%2520Yonatan%2520Geifman%2520and%2520Eric%2520Chung%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Llama-Nemotron%2520series%2520of%2520models%252C%2520an%2520open%2520family%2520of%250Aheterogeneous%2520reasoning%2520models%2520that%2520deliver%2520exceptional%2520reasoning%2520capabilities%252C%250Ainference%2520efficiency%252C%2520and%2520an%2520open%2520license%2520for%2520enterprise%2520use.%2520The%2520family%2520comes%250Ain%2520three%2520sizes%2520--%2520Nano%2520%25288B%2529%252C%2520Super%2520%252849B%2529%252C%2520and%2520Ultra%2520%2528253B%2529%2520--%2520and%2520performs%250Acompetitively%2520with%2520state-of-the-art%2520reasoning%2520models%2520such%2520as%2520DeepSeek-R1%2520while%250Aoffering%2520superior%2520inference%2520throughput%2520and%2520memory%2520efficiency.%2520In%2520this%2520report%252C%250Awe%2520discuss%2520the%2520training%2520procedure%2520for%2520these%2520models%252C%2520which%2520entails%2520using%2520neural%250Aarchitecture%2520search%2520from%2520Llama%25203%2520models%2520for%2520accelerated%2520inference%252C%2520knowledge%250Adistillation%252C%2520and%2520continued%2520pretraining%252C%2520followed%2520by%2520a%2520reasoning-focused%250Apost-training%2520stage%2520consisting%2520of%2520two%2520main%2520parts%253A%2520supervised%2520fine-tuning%2520and%250Alarge%2520scale%2520reinforcement%2520learning.%2520Llama-Nemotron%2520models%2520are%2520the%2520first%250Aopen-source%2520models%2520to%2520support%2520a%2520dynamic%2520reasoning%2520toggle%252C%2520allowing%2520users%2520to%250Aswitch%2520between%2520standard%2520chat%2520and%2520reasoning%2520modes%2520during%2520inference.%2520To%2520further%250Asupport%2520open%2520research%2520and%2520facilitate%2520model%2520development%252C%2520we%2520provide%2520the%250Afollowing%2520resources%253A%25201.%2520We%2520release%2520the%2520Llama-Nemotron%2520reasoning%2520models%2520--%250ALN-Nano%252C%2520LN-Super%252C%2520and%2520LN-Ultra%2520--%2520under%2520the%2520commercially%2520permissive%2520NVIDIA%250AOpen%2520Model%2520License%2520Agreement.%25202.%2520We%2520release%2520the%2520complete%2520post-training%2520dataset%253A%250ALlama-Nemotron-Post-Training-Dataset.%25203.%2520We%2520also%2520release%2520our%2520training%250Acodebases%253A%2520NeMo%252C%2520NeMo-Aligner%252C%2520and%2520Megatron-LM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00949v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Llama-Nemotron%3A%20Efficient%20Reasoning%20Models&entry.906535625=Akhiad%20Bercovich%20and%20Itay%20Levy%20and%20Izik%20Golan%20and%20Mohammad%20Dabbah%20and%20Ran%20El-Yaniv%20and%20Omri%20Puny%20and%20Ido%20Galil%20and%20Zach%20Moshe%20and%20Tomer%20Ronen%20and%20Najeeb%20Nabwani%20and%20Ido%20Shahaf%20and%20Oren%20Tropp%20and%20Ehud%20Karpas%20and%20Ran%20Zilberstein%20and%20Jiaqi%20Zeng%20and%20Soumye%20Singhal%20and%20Alexander%20Bukharin%20and%20Yian%20Zhang%20and%20Tugrul%20Konuk%20and%20Gerald%20Shen%20and%20Ameya%20Sunil%20Mahabaleshwarkar%20and%20Bilal%20Kartal%20and%20Yoshi%20Suhara%20and%20Olivier%20Delalleau%20and%20Zijia%20Chen%20and%20Zhilin%20Wang%20and%20David%20Mosallanezhad%20and%20Adi%20Renduchintala%20and%20Haifeng%20Qian%20and%20Dima%20Rekesh%20and%20Fei%20Jia%20and%20Somshubra%20Majumdar%20and%20Vahid%20Noroozi%20and%20Wasi%20Uddin%20Ahmad%20and%20Sean%20Narenthiran%20and%20Aleksander%20Ficek%20and%20Mehrzad%20Samadi%20and%20Jocelyn%20Huang%20and%20Siddhartha%20Jain%20and%20Igor%20Gitman%20and%20Ivan%20Moshkov%20and%20Wei%20Du%20and%20Shubham%20Toshniwal%20and%20George%20Armstrong%20and%20Branislav%20Kisacanin%20and%20Matvei%20Novikov%20and%20Daria%20Gitman%20and%20Evelina%20Bakhturina%20and%20Prasoon%20Varshney%20and%20Makesh%20Narsimhan%20and%20Jane%20Polak%20Scowcroft%20and%20John%20Kamalu%20and%20Dan%20Su%20and%20Kezhi%20Kong%20and%20Markus%20Kliegl%20and%20Rabeeh%20Karimi%20Mahabadi%20and%20Ying%20Lin%20and%20Sanjeev%20Satheesh%20and%20Jupinder%20Parmar%20and%20Pritam%20Gundecha%20and%20Brandon%20Norick%20and%20Joseph%20Jennings%20and%20Shrimai%20Prabhumoye%20and%20Syeda%20Nahida%20Akter%20and%20Mostofa%20Patwary%20and%20Abhinav%20Khattar%20and%20Deepak%20Narayanan%20and%20Roger%20Waleffe%20and%20Jimmy%20Zhang%20and%20Bor-Yiing%20Su%20and%20Guyue%20Huang%20and%20Terry%20Kong%20and%20Parth%20Chadha%20and%20Sahil%20Jain%20and%20Christine%20Harvey%20and%20Elad%20Segal%20and%20Jining%20Huang%20and%20Sergey%20Kashirsky%20and%20Robert%20McQueen%20and%20Izzy%20Putterman%20and%20George%20Lam%20and%20Arun%20Venkatesan%20and%20Sherry%20Wu%20and%20Vinh%20Nguyen%20and%20Manoj%20Kilaru%20and%20Andrew%20Wang%20and%20Anna%20Warno%20and%20Abhilash%20Somasamudramath%20and%20Sandip%20Bhaskar%20and%20Maka%20Dong%20and%20Nave%20Assaf%20and%20Shahar%20Mor%20and%20Omer%20Ullman%20Argov%20and%20Scot%20Junkin%20and%20Oleksandr%20Romanenko%20and%20Pedro%20Larroy%20and%20Monika%20Katariya%20and%20Marco%20Rovinelli%20and%20Viji%20Balas%20and%20Nicholas%20Edelman%20and%20Anahita%20Bhiwandiwalla%20and%20Muthu%20Subramaniam%20and%20Smita%20Ithape%20and%20Karthik%20Ramamoorthy%20and%20Yuting%20Wu%20and%20Suguna%20Varshini%20Velury%20and%20Omri%20Almog%20and%20Joyjit%20Daw%20and%20Denys%20Fridman%20and%20Erick%20Galinkin%20and%20Michael%20Evans%20and%20Shaona%20Ghosh%20and%20Katherine%20Luna%20and%20Leon%20Derczynski%20and%20Nikki%20Pope%20and%20Eileen%20Long%20and%20Seth%20Schneider%20and%20Guillermo%20Siman%20and%20Tomasz%20Grzegorzek%20and%20Pablo%20Ribalta%20and%20Monika%20Katariya%20and%20Chris%20Alexiuk%20and%20Joey%20Conway%20and%20Trisha%20Saar%20and%20Ann%20Guan%20and%20Krzysztof%20Pawelec%20and%20Shyamala%20Prayaga%20and%20Oleksii%20Kuchaiev%20and%20Boris%20Ginsburg%20and%20Oluwatobi%20Olabiyi%20and%20Kari%20Briski%20and%20Jonathan%20Cohen%20and%20Bryan%20Catanzaro%20and%20Jonah%20Alben%20and%20Yonatan%20Geifman%20and%20Eric%20Chung&entry.1292438233=%20%20We%20introduce%20the%20Llama-Nemotron%20series%20of%20models%2C%20an%20open%20family%20of%0Aheterogeneous%20reasoning%20models%20that%20deliver%20exceptional%20reasoning%20capabilities%2C%0Ainference%20efficiency%2C%20and%20an%20open%20license%20for%20enterprise%20use.%20The%20family%20comes%0Ain%20three%20sizes%20--%20Nano%20%288B%29%2C%20Super%20%2849B%29%2C%20and%20Ultra%20%28253B%29%20--%20and%20performs%0Acompetitively%20with%20state-of-the-art%20reasoning%20models%20such%20as%20DeepSeek-R1%20while%0Aoffering%20superior%20inference%20throughput%20and%20memory%20efficiency.%20In%20this%20report%2C%0Awe%20discuss%20the%20training%20procedure%20for%20these%20models%2C%20which%20entails%20using%20neural%0Aarchitecture%20search%20from%20Llama%203%20models%20for%20accelerated%20inference%2C%20knowledge%0Adistillation%2C%20and%20continued%20pretraining%2C%20followed%20by%20a%20reasoning-focused%0Apost-training%20stage%20consisting%20of%20two%20main%20parts%3A%20supervised%20fine-tuning%20and%0Alarge%20scale%20reinforcement%20learning.%20Llama-Nemotron%20models%20are%20the%20first%0Aopen-source%20models%20to%20support%20a%20dynamic%20reasoning%20toggle%2C%20allowing%20users%20to%0Aswitch%20between%20standard%20chat%20and%20reasoning%20modes%20during%20inference.%20To%20further%0Asupport%20open%20research%20and%20facilitate%20model%20development%2C%20we%20provide%20the%0Afollowing%20resources%3A%201.%20We%20release%20the%20Llama-Nemotron%20reasoning%20models%20--%0ALN-Nano%2C%20LN-Super%2C%20and%20LN-Ultra%20--%20under%20the%20commercially%20permissive%20NVIDIA%0AOpen%20Model%20License%20Agreement.%202.%20We%20release%20the%20complete%20post-training%20dataset%3A%0ALlama-Nemotron-Post-Training-Dataset.%203.%20We%20also%20release%20our%20training%0Acodebases%3A%20NeMo%2C%20NeMo-Aligner%2C%20and%20Megatron-LM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00949v5&entry.124074799=Read"},
{"title": "Uncovering Scaling Laws for Large Language Models via Inverse Problems", "author": "Arun Verma and Zhaoxuan Wu and Zijian Zhou and Xiaoqiang Lin and Zhiliang Chen and Rachael Hwee Ling Sim and Rui Qiao and Jingtan Wang and Nhung Bui and Xinyuan Niu and Wenyang Hu and Gregory Kang Ruey Lau and Zi-Yu Khoo and Zitong Zhao and Xinyi Xu and Apivich Hemachandra and See-Kiong Ng and Bryan Kian Hsiang Low", "abstract": "  Large Language Models (LLMs) are large-scale pretrained models that have\nachieved remarkable success across diverse domains. These successes have been\ndriven by unprecedented complexity and scale in both data and computations.\nHowever, due to the high costs of training such models, brute-force\ntrial-and-error approaches to improve LLMs are not feasible. Inspired by the\nsuccess of inverse problems in uncovering fundamental scientific laws, this\nposition paper advocates that inverse problems can also efficiently uncover\nscaling laws that guide the building of LLMs to achieve the desirable\nperformance with significantly better cost-effectiveness.\n", "link": "http://arxiv.org/abs/2509.07909v1", "date": "2025-09-09", "relevancy": 2.0174, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Scaling%20Laws%20for%20Large%20Language%20Models%20via%20Inverse%20Problems&body=Title%3A%20Uncovering%20Scaling%20Laws%20for%20Large%20Language%20Models%20via%20Inverse%20Problems%0AAuthor%3A%20Arun%20Verma%20and%20Zhaoxuan%20Wu%20and%20Zijian%20Zhou%20and%20Xiaoqiang%20Lin%20and%20Zhiliang%20Chen%20and%20Rachael%20Hwee%20Ling%20Sim%20and%20Rui%20Qiao%20and%20Jingtan%20Wang%20and%20Nhung%20Bui%20and%20Xinyuan%20Niu%20and%20Wenyang%20Hu%20and%20Gregory%20Kang%20Ruey%20Lau%20and%20Zi-Yu%20Khoo%20and%20Zitong%20Zhao%20and%20Xinyi%20Xu%20and%20Apivich%20Hemachandra%20and%20See-Kiong%20Ng%20and%20Bryan%20Kian%20Hsiang%20Low%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20large-scale%20pretrained%20models%20that%20have%0Aachieved%20remarkable%20success%20across%20diverse%20domains.%20These%20successes%20have%20been%0Adriven%20by%20unprecedented%20complexity%20and%20scale%20in%20both%20data%20and%20computations.%0AHowever%2C%20due%20to%20the%20high%20costs%20of%20training%20such%20models%2C%20brute-force%0Atrial-and-error%20approaches%20to%20improve%20LLMs%20are%20not%20feasible.%20Inspired%20by%20the%0Asuccess%20of%20inverse%20problems%20in%20uncovering%20fundamental%20scientific%20laws%2C%20this%0Aposition%20paper%20advocates%20that%20inverse%20problems%20can%20also%20efficiently%20uncover%0Ascaling%20laws%20that%20guide%20the%20building%20of%20LLMs%20to%20achieve%20the%20desirable%0Aperformance%20with%20significantly%20better%20cost-effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Scaling%2520Laws%2520for%2520Large%2520Language%2520Models%2520via%2520Inverse%2520Problems%26entry.906535625%3DArun%2520Verma%2520and%2520Zhaoxuan%2520Wu%2520and%2520Zijian%2520Zhou%2520and%2520Xiaoqiang%2520Lin%2520and%2520Zhiliang%2520Chen%2520and%2520Rachael%2520Hwee%2520Ling%2520Sim%2520and%2520Rui%2520Qiao%2520and%2520Jingtan%2520Wang%2520and%2520Nhung%2520Bui%2520and%2520Xinyuan%2520Niu%2520and%2520Wenyang%2520Hu%2520and%2520Gregory%2520Kang%2520Ruey%2520Lau%2520and%2520Zi-Yu%2520Khoo%2520and%2520Zitong%2520Zhao%2520and%2520Xinyi%2520Xu%2520and%2520Apivich%2520Hemachandra%2520and%2520See-Kiong%2520Ng%2520and%2520Bryan%2520Kian%2520Hsiang%2520Low%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520large-scale%2520pretrained%2520models%2520that%2520have%250Aachieved%2520remarkable%2520success%2520across%2520diverse%2520domains.%2520These%2520successes%2520have%2520been%250Adriven%2520by%2520unprecedented%2520complexity%2520and%2520scale%2520in%2520both%2520data%2520and%2520computations.%250AHowever%252C%2520due%2520to%2520the%2520high%2520costs%2520of%2520training%2520such%2520models%252C%2520brute-force%250Atrial-and-error%2520approaches%2520to%2520improve%2520LLMs%2520are%2520not%2520feasible.%2520Inspired%2520by%2520the%250Asuccess%2520of%2520inverse%2520problems%2520in%2520uncovering%2520fundamental%2520scientific%2520laws%252C%2520this%250Aposition%2520paper%2520advocates%2520that%2520inverse%2520problems%2520can%2520also%2520efficiently%2520uncover%250Ascaling%2520laws%2520that%2520guide%2520the%2520building%2520of%2520LLMs%2520to%2520achieve%2520the%2520desirable%250Aperformance%2520with%2520significantly%2520better%2520cost-effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Scaling%20Laws%20for%20Large%20Language%20Models%20via%20Inverse%20Problems&entry.906535625=Arun%20Verma%20and%20Zhaoxuan%20Wu%20and%20Zijian%20Zhou%20and%20Xiaoqiang%20Lin%20and%20Zhiliang%20Chen%20and%20Rachael%20Hwee%20Ling%20Sim%20and%20Rui%20Qiao%20and%20Jingtan%20Wang%20and%20Nhung%20Bui%20and%20Xinyuan%20Niu%20and%20Wenyang%20Hu%20and%20Gregory%20Kang%20Ruey%20Lau%20and%20Zi-Yu%20Khoo%20and%20Zitong%20Zhao%20and%20Xinyi%20Xu%20and%20Apivich%20Hemachandra%20and%20See-Kiong%20Ng%20and%20Bryan%20Kian%20Hsiang%20Low&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20large-scale%20pretrained%20models%20that%20have%0Aachieved%20remarkable%20success%20across%20diverse%20domains.%20These%20successes%20have%20been%0Adriven%20by%20unprecedented%20complexity%20and%20scale%20in%20both%20data%20and%20computations.%0AHowever%2C%20due%20to%20the%20high%20costs%20of%20training%20such%20models%2C%20brute-force%0Atrial-and-error%20approaches%20to%20improve%20LLMs%20are%20not%20feasible.%20Inspired%20by%20the%0Asuccess%20of%20inverse%20problems%20in%20uncovering%20fundamental%20scientific%20laws%2C%20this%0Aposition%20paper%20advocates%20that%20inverse%20problems%20can%20also%20efficiently%20uncover%0Ascaling%20laws%20that%20guide%20the%20building%20of%20LLMs%20to%20achieve%20the%20desirable%0Aperformance%20with%20significantly%20better%20cost-effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07909v1&entry.124074799=Read"},
{"title": "Hybrid-Regularized Magnitude Pruning for Robust Federated Learning under\n  Covariate Shift", "author": "Ozgu Goksu and Nicolas Pugeault", "abstract": "  Federated Learning offers a solution for decentralised model training,\naddressing the difficulties associated with distributed data and privacy in\nmachine learning. However, the fact of data heterogeneity in federated learning\nfrequently hinders the global model's generalisation, leading to low\nperformance and adaptability to unseen data. This problem is particularly\ncritical for specialised applications such as medical imaging, where both the\ndata and the number of clients are limited. In this paper, we empirically\ndemonstrate that inconsistencies in client-side training distributions\nsubstantially degrade the performance of federated learning models across\nmultiple benchmark datasets. We propose a novel FL framework using a\ncombination of pruning and regularisation of clients' training to improve the\nsparsity, redundancy, and robustness of neural connections, and thereby the\nresilience to model aggregation. To address a relatively unexplored dimension\nof data heterogeneity, we further introduce a novel benchmark dataset,\nCelebA-Gender, specifically designed to control for within-class distributional\nshifts across clients based on attribute variations, thereby complementing the\npredominant focus on inter-class imbalance in prior federated learning\nresearch. Comprehensive experiments on many datasets like CIFAR-10, MNIST, and\nthe newly introduced CelebA-Gender dataset demonstrate that our method\nconsistently outperforms standard FL baselines, yielding more robust and\ngeneralizable models in heterogeneous settings.\n", "link": "http://arxiv.org/abs/2412.15010v2", "date": "2025-09-09", "relevancy": 2.0167, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5053}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5047}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid-Regularized%20Magnitude%20Pruning%20for%20Robust%20Federated%20Learning%20under%0A%20%20Covariate%20Shift&body=Title%3A%20Hybrid-Regularized%20Magnitude%20Pruning%20for%20Robust%20Federated%20Learning%20under%0A%20%20Covariate%20Shift%0AAuthor%3A%20Ozgu%20Goksu%20and%20Nicolas%20Pugeault%0AAbstract%3A%20%20%20Federated%20Learning%20offers%20a%20solution%20for%20decentralised%20model%20training%2C%0Aaddressing%20the%20difficulties%20associated%20with%20distributed%20data%20and%20privacy%20in%0Amachine%20learning.%20However%2C%20the%20fact%20of%20data%20heterogeneity%20in%20federated%20learning%0Afrequently%20hinders%20the%20global%20model%27s%20generalisation%2C%20leading%20to%20low%0Aperformance%20and%20adaptability%20to%20unseen%20data.%20This%20problem%20is%20particularly%0Acritical%20for%20specialised%20applications%20such%20as%20medical%20imaging%2C%20where%20both%20the%0Adata%20and%20the%20number%20of%20clients%20are%20limited.%20In%20this%20paper%2C%20we%20empirically%0Ademonstrate%20that%20inconsistencies%20in%20client-side%20training%20distributions%0Asubstantially%20degrade%20the%20performance%20of%20federated%20learning%20models%20across%0Amultiple%20benchmark%20datasets.%20We%20propose%20a%20novel%20FL%20framework%20using%20a%0Acombination%20of%20pruning%20and%20regularisation%20of%20clients%27%20training%20to%20improve%20the%0Asparsity%2C%20redundancy%2C%20and%20robustness%20of%20neural%20connections%2C%20and%20thereby%20the%0Aresilience%20to%20model%20aggregation.%20To%20address%20a%20relatively%20unexplored%20dimension%0Aof%20data%20heterogeneity%2C%20we%20further%20introduce%20a%20novel%20benchmark%20dataset%2C%0ACelebA-Gender%2C%20specifically%20designed%20to%20control%20for%20within-class%20distributional%0Ashifts%20across%20clients%20based%20on%20attribute%20variations%2C%20thereby%20complementing%20the%0Apredominant%20focus%20on%20inter-class%20imbalance%20in%20prior%20federated%20learning%0Aresearch.%20Comprehensive%20experiments%20on%20many%20datasets%20like%20CIFAR-10%2C%20MNIST%2C%20and%0Athe%20newly%20introduced%20CelebA-Gender%20dataset%20demonstrate%20that%20our%20method%0Aconsistently%20outperforms%20standard%20FL%20baselines%2C%20yielding%20more%20robust%20and%0Ageneralizable%20models%20in%20heterogeneous%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15010v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid-Regularized%2520Magnitude%2520Pruning%2520for%2520Robust%2520Federated%2520Learning%2520under%250A%2520%2520Covariate%2520Shift%26entry.906535625%3DOzgu%2520Goksu%2520and%2520Nicolas%2520Pugeault%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520offers%2520a%2520solution%2520for%2520decentralised%2520model%2520training%252C%250Aaddressing%2520the%2520difficulties%2520associated%2520with%2520distributed%2520data%2520and%2520privacy%2520in%250Amachine%2520learning.%2520However%252C%2520the%2520fact%2520of%2520data%2520heterogeneity%2520in%2520federated%2520learning%250Afrequently%2520hinders%2520the%2520global%2520model%2527s%2520generalisation%252C%2520leading%2520to%2520low%250Aperformance%2520and%2520adaptability%2520to%2520unseen%2520data.%2520This%2520problem%2520is%2520particularly%250Acritical%2520for%2520specialised%2520applications%2520such%2520as%2520medical%2520imaging%252C%2520where%2520both%2520the%250Adata%2520and%2520the%2520number%2520of%2520clients%2520are%2520limited.%2520In%2520this%2520paper%252C%2520we%2520empirically%250Ademonstrate%2520that%2520inconsistencies%2520in%2520client-side%2520training%2520distributions%250Asubstantially%2520degrade%2520the%2520performance%2520of%2520federated%2520learning%2520models%2520across%250Amultiple%2520benchmark%2520datasets.%2520We%2520propose%2520a%2520novel%2520FL%2520framework%2520using%2520a%250Acombination%2520of%2520pruning%2520and%2520regularisation%2520of%2520clients%2527%2520training%2520to%2520improve%2520the%250Asparsity%252C%2520redundancy%252C%2520and%2520robustness%2520of%2520neural%2520connections%252C%2520and%2520thereby%2520the%250Aresilience%2520to%2520model%2520aggregation.%2520To%2520address%2520a%2520relatively%2520unexplored%2520dimension%250Aof%2520data%2520heterogeneity%252C%2520we%2520further%2520introduce%2520a%2520novel%2520benchmark%2520dataset%252C%250ACelebA-Gender%252C%2520specifically%2520designed%2520to%2520control%2520for%2520within-class%2520distributional%250Ashifts%2520across%2520clients%2520based%2520on%2520attribute%2520variations%252C%2520thereby%2520complementing%2520the%250Apredominant%2520focus%2520on%2520inter-class%2520imbalance%2520in%2520prior%2520federated%2520learning%250Aresearch.%2520Comprehensive%2520experiments%2520on%2520many%2520datasets%2520like%2520CIFAR-10%252C%2520MNIST%252C%2520and%250Athe%2520newly%2520introduced%2520CelebA-Gender%2520dataset%2520demonstrate%2520that%2520our%2520method%250Aconsistently%2520outperforms%2520standard%2520FL%2520baselines%252C%2520yielding%2520more%2520robust%2520and%250Ageneralizable%2520models%2520in%2520heterogeneous%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15010v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid-Regularized%20Magnitude%20Pruning%20for%20Robust%20Federated%20Learning%20under%0A%20%20Covariate%20Shift&entry.906535625=Ozgu%20Goksu%20and%20Nicolas%20Pugeault&entry.1292438233=%20%20Federated%20Learning%20offers%20a%20solution%20for%20decentralised%20model%20training%2C%0Aaddressing%20the%20difficulties%20associated%20with%20distributed%20data%20and%20privacy%20in%0Amachine%20learning.%20However%2C%20the%20fact%20of%20data%20heterogeneity%20in%20federated%20learning%0Afrequently%20hinders%20the%20global%20model%27s%20generalisation%2C%20leading%20to%20low%0Aperformance%20and%20adaptability%20to%20unseen%20data.%20This%20problem%20is%20particularly%0Acritical%20for%20specialised%20applications%20such%20as%20medical%20imaging%2C%20where%20both%20the%0Adata%20and%20the%20number%20of%20clients%20are%20limited.%20In%20this%20paper%2C%20we%20empirically%0Ademonstrate%20that%20inconsistencies%20in%20client-side%20training%20distributions%0Asubstantially%20degrade%20the%20performance%20of%20federated%20learning%20models%20across%0Amultiple%20benchmark%20datasets.%20We%20propose%20a%20novel%20FL%20framework%20using%20a%0Acombination%20of%20pruning%20and%20regularisation%20of%20clients%27%20training%20to%20improve%20the%0Asparsity%2C%20redundancy%2C%20and%20robustness%20of%20neural%20connections%2C%20and%20thereby%20the%0Aresilience%20to%20model%20aggregation.%20To%20address%20a%20relatively%20unexplored%20dimension%0Aof%20data%20heterogeneity%2C%20we%20further%20introduce%20a%20novel%20benchmark%20dataset%2C%0ACelebA-Gender%2C%20specifically%20designed%20to%20control%20for%20within-class%20distributional%0Ashifts%20across%20clients%20based%20on%20attribute%20variations%2C%20thereby%20complementing%20the%0Apredominant%20focus%20on%20inter-class%20imbalance%20in%20prior%20federated%20learning%0Aresearch.%20Comprehensive%20experiments%20on%20many%20datasets%20like%20CIFAR-10%2C%20MNIST%2C%20and%0Athe%20newly%20introduced%20CelebA-Gender%20dataset%20demonstrate%20that%20our%20method%0Aconsistently%20outperforms%20standard%20FL%20baselines%2C%20yielding%20more%20robust%20and%0Ageneralizable%20models%20in%20heterogeneous%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15010v2&entry.124074799=Read"},
{"title": "Fault Tolerant Control of a Quadcopter using Reinforcement Learning", "author": "Muzaffar Habib and Adnan Maqsood and Adnan Fayyaz ud Din", "abstract": "  This study presents a novel reinforcement learning (RL)-based control\nframework aimed at enhancing the safety and robustness of the quadcopter, with\na specific focus on resilience to in-flight one propeller failure. Addressing\nthe critical need of a robust control strategy for maintaining a desired\naltitude for the quadcopter to safe the hardware and the payload in physical\napplications. The proposed framework investigates two RL methodologies Dynamic\nProgramming (DP) and Deep Deterministic Policy Gradient (DDPG), to overcome the\nchallenges posed by the rotor failure mechanism of the quadcopter. DP, a\nmodel-based approach, is leveraged for its convergence guarantees, despite high\ncomputational demands, whereas DDPG, a model-free technique, facilitates rapid\ncomputation but with constraints on solution duration. The research challenge\narises from training RL algorithms on large dimensions and action domains. With\nmodifications to the existing DP and DDPG algorithms, the controllers were\ntrained not only to cater for large continuous state and action domain and also\nachieve a desired state after an inflight propeller failure. To verify the\nrobustness of the proposed control framework, extensive simulations were\nconducted in a MATLAB environment across various initial conditions and\nunderscoring its viability for mission-critical quadcopter applications. A\ncomparative analysis was performed between both RL algorithms and their\npotential for applications in faulty aerial systems.\n", "link": "http://arxiv.org/abs/2509.07707v1", "date": "2025-09-09", "relevancy": 2.0084, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5225}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4974}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fault%20Tolerant%20Control%20of%20a%20Quadcopter%20using%20Reinforcement%20Learning&body=Title%3A%20Fault%20Tolerant%20Control%20of%20a%20Quadcopter%20using%20Reinforcement%20Learning%0AAuthor%3A%20Muzaffar%20Habib%20and%20Adnan%20Maqsood%20and%20Adnan%20Fayyaz%20ud%20Din%0AAbstract%3A%20%20%20This%20study%20presents%20a%20novel%20reinforcement%20learning%20%28RL%29-based%20control%0Aframework%20aimed%20at%20enhancing%20the%20safety%20and%20robustness%20of%20the%20quadcopter%2C%20with%0Aa%20specific%20focus%20on%20resilience%20to%20in-flight%20one%20propeller%20failure.%20Addressing%0Athe%20critical%20need%20of%20a%20robust%20control%20strategy%20for%20maintaining%20a%20desired%0Aaltitude%20for%20the%20quadcopter%20to%20safe%20the%20hardware%20and%20the%20payload%20in%20physical%0Aapplications.%20The%20proposed%20framework%20investigates%20two%20RL%20methodologies%20Dynamic%0AProgramming%20%28DP%29%20and%20Deep%20Deterministic%20Policy%20Gradient%20%28DDPG%29%2C%20to%20overcome%20the%0Achallenges%20posed%20by%20the%20rotor%20failure%20mechanism%20of%20the%20quadcopter.%20DP%2C%20a%0Amodel-based%20approach%2C%20is%20leveraged%20for%20its%20convergence%20guarantees%2C%20despite%20high%0Acomputational%20demands%2C%20whereas%20DDPG%2C%20a%20model-free%20technique%2C%20facilitates%20rapid%0Acomputation%20but%20with%20constraints%20on%20solution%20duration.%20The%20research%20challenge%0Aarises%20from%20training%20RL%20algorithms%20on%20large%20dimensions%20and%20action%20domains.%20With%0Amodifications%20to%20the%20existing%20DP%20and%20DDPG%20algorithms%2C%20the%20controllers%20were%0Atrained%20not%20only%20to%20cater%20for%20large%20continuous%20state%20and%20action%20domain%20and%20also%0Aachieve%20a%20desired%20state%20after%20an%20inflight%20propeller%20failure.%20To%20verify%20the%0Arobustness%20of%20the%20proposed%20control%20framework%2C%20extensive%20simulations%20were%0Aconducted%20in%20a%20MATLAB%20environment%20across%20various%20initial%20conditions%20and%0Aunderscoring%20its%20viability%20for%20mission-critical%20quadcopter%20applications.%20A%0Acomparative%20analysis%20was%20performed%20between%20both%20RL%20algorithms%20and%20their%0Apotential%20for%20applications%20in%20faulty%20aerial%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFault%2520Tolerant%2520Control%2520of%2520a%2520Quadcopter%2520using%2520Reinforcement%2520Learning%26entry.906535625%3DMuzaffar%2520Habib%2520and%2520Adnan%2520Maqsood%2520and%2520Adnan%2520Fayyaz%2520ud%2520Din%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520a%2520novel%2520reinforcement%2520learning%2520%2528RL%2529-based%2520control%250Aframework%2520aimed%2520at%2520enhancing%2520the%2520safety%2520and%2520robustness%2520of%2520the%2520quadcopter%252C%2520with%250Aa%2520specific%2520focus%2520on%2520resilience%2520to%2520in-flight%2520one%2520propeller%2520failure.%2520Addressing%250Athe%2520critical%2520need%2520of%2520a%2520robust%2520control%2520strategy%2520for%2520maintaining%2520a%2520desired%250Aaltitude%2520for%2520the%2520quadcopter%2520to%2520safe%2520the%2520hardware%2520and%2520the%2520payload%2520in%2520physical%250Aapplications.%2520The%2520proposed%2520framework%2520investigates%2520two%2520RL%2520methodologies%2520Dynamic%250AProgramming%2520%2528DP%2529%2520and%2520Deep%2520Deterministic%2520Policy%2520Gradient%2520%2528DDPG%2529%252C%2520to%2520overcome%2520the%250Achallenges%2520posed%2520by%2520the%2520rotor%2520failure%2520mechanism%2520of%2520the%2520quadcopter.%2520DP%252C%2520a%250Amodel-based%2520approach%252C%2520is%2520leveraged%2520for%2520its%2520convergence%2520guarantees%252C%2520despite%2520high%250Acomputational%2520demands%252C%2520whereas%2520DDPG%252C%2520a%2520model-free%2520technique%252C%2520facilitates%2520rapid%250Acomputation%2520but%2520with%2520constraints%2520on%2520solution%2520duration.%2520The%2520research%2520challenge%250Aarises%2520from%2520training%2520RL%2520algorithms%2520on%2520large%2520dimensions%2520and%2520action%2520domains.%2520With%250Amodifications%2520to%2520the%2520existing%2520DP%2520and%2520DDPG%2520algorithms%252C%2520the%2520controllers%2520were%250Atrained%2520not%2520only%2520to%2520cater%2520for%2520large%2520continuous%2520state%2520and%2520action%2520domain%2520and%2520also%250Aachieve%2520a%2520desired%2520state%2520after%2520an%2520inflight%2520propeller%2520failure.%2520To%2520verify%2520the%250Arobustness%2520of%2520the%2520proposed%2520control%2520framework%252C%2520extensive%2520simulations%2520were%250Aconducted%2520in%2520a%2520MATLAB%2520environment%2520across%2520various%2520initial%2520conditions%2520and%250Aunderscoring%2520its%2520viability%2520for%2520mission-critical%2520quadcopter%2520applications.%2520A%250Acomparative%2520analysis%2520was%2520performed%2520between%2520both%2520RL%2520algorithms%2520and%2520their%250Apotential%2520for%2520applications%2520in%2520faulty%2520aerial%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fault%20Tolerant%20Control%20of%20a%20Quadcopter%20using%20Reinforcement%20Learning&entry.906535625=Muzaffar%20Habib%20and%20Adnan%20Maqsood%20and%20Adnan%20Fayyaz%20ud%20Din&entry.1292438233=%20%20This%20study%20presents%20a%20novel%20reinforcement%20learning%20%28RL%29-based%20control%0Aframework%20aimed%20at%20enhancing%20the%20safety%20and%20robustness%20of%20the%20quadcopter%2C%20with%0Aa%20specific%20focus%20on%20resilience%20to%20in-flight%20one%20propeller%20failure.%20Addressing%0Athe%20critical%20need%20of%20a%20robust%20control%20strategy%20for%20maintaining%20a%20desired%0Aaltitude%20for%20the%20quadcopter%20to%20safe%20the%20hardware%20and%20the%20payload%20in%20physical%0Aapplications.%20The%20proposed%20framework%20investigates%20two%20RL%20methodologies%20Dynamic%0AProgramming%20%28DP%29%20and%20Deep%20Deterministic%20Policy%20Gradient%20%28DDPG%29%2C%20to%20overcome%20the%0Achallenges%20posed%20by%20the%20rotor%20failure%20mechanism%20of%20the%20quadcopter.%20DP%2C%20a%0Amodel-based%20approach%2C%20is%20leveraged%20for%20its%20convergence%20guarantees%2C%20despite%20high%0Acomputational%20demands%2C%20whereas%20DDPG%2C%20a%20model-free%20technique%2C%20facilitates%20rapid%0Acomputation%20but%20with%20constraints%20on%20solution%20duration.%20The%20research%20challenge%0Aarises%20from%20training%20RL%20algorithms%20on%20large%20dimensions%20and%20action%20domains.%20With%0Amodifications%20to%20the%20existing%20DP%20and%20DDPG%20algorithms%2C%20the%20controllers%20were%0Atrained%20not%20only%20to%20cater%20for%20large%20continuous%20state%20and%20action%20domain%20and%20also%0Aachieve%20a%20desired%20state%20after%20an%20inflight%20propeller%20failure.%20To%20verify%20the%0Arobustness%20of%20the%20proposed%20control%20framework%2C%20extensive%20simulations%20were%0Aconducted%20in%20a%20MATLAB%20environment%20across%20various%20initial%20conditions%20and%0Aunderscoring%20its%20viability%20for%20mission-critical%20quadcopter%20applications.%20A%0Acomparative%20analysis%20was%20performed%20between%20both%20RL%20algorithms%20and%20their%0Apotential%20for%20applications%20in%20faulty%20aerial%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07707v1&entry.124074799=Read"},
{"title": "ACE and Diverse Generalization via Selective Disagreement", "author": "Oliver Daniels and Stuart Armstrong and Alexandre Maranh\u00e3o and Mahirah Fairuz Rahman and Benjamin M. Marlin and Rebecca Gorman", "abstract": "  Deep neural networks are notoriously sensitive to spurious correlations -\nwhere a model learns a shortcut that fails out-of-distribution. Existing work\non spurious correlations has often focused on incomplete\ncorrelations,leveraging access to labeled instances that break the correlation.\nBut in cases where the spurious correlations are complete, the correct\ngeneralization is fundamentally \\textit{underspecified}. To resolve this\nunderspecification, we propose learning a set of concepts that are consistent\nwith training data but make distinct predictions on a subset of novel unlabeled\ninputs. Using a self-training approach that encourages \\textit{confident} and\n\\textit{selective} disagreement, our method ACE matches or outperforms existing\nmethods on a suite of complete-spurious correlation benchmarks, while remaining\nrobust to incomplete spurious correlations. ACE is also more configurable than\nprior approaches, allowing for straight-forward encoding of prior knowledge and\nprincipled unsupervised model selection. In an early application to\nlanguage-model alignment, we find that ACE achieves competitive performance on\nthe measurement tampering detection benchmark \\textit{without} access to\nuntrusted measurements. While still subject to important limitations, ACE\nrepresents significant progress towards overcoming underspecification.\n", "link": "http://arxiv.org/abs/2509.07955v1", "date": "2025-09-09", "relevancy": 2.0078, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5064}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5057}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACE%20and%20Diverse%20Generalization%20via%20Selective%20Disagreement&body=Title%3A%20ACE%20and%20Diverse%20Generalization%20via%20Selective%20Disagreement%0AAuthor%3A%20Oliver%20Daniels%20and%20Stuart%20Armstrong%20and%20Alexandre%20Maranh%C3%A3o%20and%20Mahirah%20Fairuz%20Rahman%20and%20Benjamin%20M.%20Marlin%20and%20Rebecca%20Gorman%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20notoriously%20sensitive%20to%20spurious%20correlations%20-%0Awhere%20a%20model%20learns%20a%20shortcut%20that%20fails%20out-of-distribution.%20Existing%20work%0Aon%20spurious%20correlations%20has%20often%20focused%20on%20incomplete%0Acorrelations%2Cleveraging%20access%20to%20labeled%20instances%20that%20break%20the%20correlation.%0ABut%20in%20cases%20where%20the%20spurious%20correlations%20are%20complete%2C%20the%20correct%0Ageneralization%20is%20fundamentally%20%5Ctextit%7Bunderspecified%7D.%20To%20resolve%20this%0Aunderspecification%2C%20we%20propose%20learning%20a%20set%20of%20concepts%20that%20are%20consistent%0Awith%20training%20data%20but%20make%20distinct%20predictions%20on%20a%20subset%20of%20novel%20unlabeled%0Ainputs.%20Using%20a%20self-training%20approach%20that%20encourages%20%5Ctextit%7Bconfident%7D%20and%0A%5Ctextit%7Bselective%7D%20disagreement%2C%20our%20method%20ACE%20matches%20or%20outperforms%20existing%0Amethods%20on%20a%20suite%20of%20complete-spurious%20correlation%20benchmarks%2C%20while%20remaining%0Arobust%20to%20incomplete%20spurious%20correlations.%20ACE%20is%20also%20more%20configurable%20than%0Aprior%20approaches%2C%20allowing%20for%20straight-forward%20encoding%20of%20prior%20knowledge%20and%0Aprincipled%20unsupervised%20model%20selection.%20In%20an%20early%20application%20to%0Alanguage-model%20alignment%2C%20we%20find%20that%20ACE%20achieves%20competitive%20performance%20on%0Athe%20measurement%20tampering%20detection%20benchmark%20%5Ctextit%7Bwithout%7D%20access%20to%0Auntrusted%20measurements.%20While%20still%20subject%20to%20important%20limitations%2C%20ACE%0Arepresents%20significant%20progress%20towards%20overcoming%20underspecification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACE%2520and%2520Diverse%2520Generalization%2520via%2520Selective%2520Disagreement%26entry.906535625%3DOliver%2520Daniels%2520and%2520Stuart%2520Armstrong%2520and%2520Alexandre%2520Maranh%25C3%25A3o%2520and%2520Mahirah%2520Fairuz%2520Rahman%2520and%2520Benjamin%2520M.%2520Marlin%2520and%2520Rebecca%2520Gorman%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520are%2520notoriously%2520sensitive%2520to%2520spurious%2520correlations%2520-%250Awhere%2520a%2520model%2520learns%2520a%2520shortcut%2520that%2520fails%2520out-of-distribution.%2520Existing%2520work%250Aon%2520spurious%2520correlations%2520has%2520often%2520focused%2520on%2520incomplete%250Acorrelations%252Cleveraging%2520access%2520to%2520labeled%2520instances%2520that%2520break%2520the%2520correlation.%250ABut%2520in%2520cases%2520where%2520the%2520spurious%2520correlations%2520are%2520complete%252C%2520the%2520correct%250Ageneralization%2520is%2520fundamentally%2520%255Ctextit%257Bunderspecified%257D.%2520To%2520resolve%2520this%250Aunderspecification%252C%2520we%2520propose%2520learning%2520a%2520set%2520of%2520concepts%2520that%2520are%2520consistent%250Awith%2520training%2520data%2520but%2520make%2520distinct%2520predictions%2520on%2520a%2520subset%2520of%2520novel%2520unlabeled%250Ainputs.%2520Using%2520a%2520self-training%2520approach%2520that%2520encourages%2520%255Ctextit%257Bconfident%257D%2520and%250A%255Ctextit%257Bselective%257D%2520disagreement%252C%2520our%2520method%2520ACE%2520matches%2520or%2520outperforms%2520existing%250Amethods%2520on%2520a%2520suite%2520of%2520complete-spurious%2520correlation%2520benchmarks%252C%2520while%2520remaining%250Arobust%2520to%2520incomplete%2520spurious%2520correlations.%2520ACE%2520is%2520also%2520more%2520configurable%2520than%250Aprior%2520approaches%252C%2520allowing%2520for%2520straight-forward%2520encoding%2520of%2520prior%2520knowledge%2520and%250Aprincipled%2520unsupervised%2520model%2520selection.%2520In%2520an%2520early%2520application%2520to%250Alanguage-model%2520alignment%252C%2520we%2520find%2520that%2520ACE%2520achieves%2520competitive%2520performance%2520on%250Athe%2520measurement%2520tampering%2520detection%2520benchmark%2520%255Ctextit%257Bwithout%257D%2520access%2520to%250Auntrusted%2520measurements.%2520While%2520still%2520subject%2520to%2520important%2520limitations%252C%2520ACE%250Arepresents%2520significant%2520progress%2520towards%2520overcoming%2520underspecification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACE%20and%20Diverse%20Generalization%20via%20Selective%20Disagreement&entry.906535625=Oliver%20Daniels%20and%20Stuart%20Armstrong%20and%20Alexandre%20Maranh%C3%A3o%20and%20Mahirah%20Fairuz%20Rahman%20and%20Benjamin%20M.%20Marlin%20and%20Rebecca%20Gorman&entry.1292438233=%20%20Deep%20neural%20networks%20are%20notoriously%20sensitive%20to%20spurious%20correlations%20-%0Awhere%20a%20model%20learns%20a%20shortcut%20that%20fails%20out-of-distribution.%20Existing%20work%0Aon%20spurious%20correlations%20has%20often%20focused%20on%20incomplete%0Acorrelations%2Cleveraging%20access%20to%20labeled%20instances%20that%20break%20the%20correlation.%0ABut%20in%20cases%20where%20the%20spurious%20correlations%20are%20complete%2C%20the%20correct%0Ageneralization%20is%20fundamentally%20%5Ctextit%7Bunderspecified%7D.%20To%20resolve%20this%0Aunderspecification%2C%20we%20propose%20learning%20a%20set%20of%20concepts%20that%20are%20consistent%0Awith%20training%20data%20but%20make%20distinct%20predictions%20on%20a%20subset%20of%20novel%20unlabeled%0Ainputs.%20Using%20a%20self-training%20approach%20that%20encourages%20%5Ctextit%7Bconfident%7D%20and%0A%5Ctextit%7Bselective%7D%20disagreement%2C%20our%20method%20ACE%20matches%20or%20outperforms%20existing%0Amethods%20on%20a%20suite%20of%20complete-spurious%20correlation%20benchmarks%2C%20while%20remaining%0Arobust%20to%20incomplete%20spurious%20correlations.%20ACE%20is%20also%20more%20configurable%20than%0Aprior%20approaches%2C%20allowing%20for%20straight-forward%20encoding%20of%20prior%20knowledge%20and%0Aprincipled%20unsupervised%20model%20selection.%20In%20an%20early%20application%20to%0Alanguage-model%20alignment%2C%20we%20find%20that%20ACE%20achieves%20competitive%20performance%20on%0Athe%20measurement%20tampering%20detection%20benchmark%20%5Ctextit%7Bwithout%7D%20access%20to%0Auntrusted%20measurements.%20While%20still%20subject%20to%20important%20limitations%2C%20ACE%0Arepresents%20significant%20progress%20towards%20overcoming%20underspecification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07955v1&entry.124074799=Read"},
{"title": "Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A\n  Comparative RAG Study", "author": "Amay Jain and Liu Cui and Si Chen", "abstract": "  Large language models like ChatGPT are increasingly used in classrooms, but\nthey often provide outdated or fabricated information that can mislead\nstudents. Retrieval Augmented Generation (RAG) improves reliability of LLMs by\ngrounding responses in external resources. We investigate two accessible RAG\nparadigms, vector-based retrieval and graph-based retrieval to identify best\npractices for classroom question answering (QA). Existing comparative studies\nfail to account for pedagogical factors such as educational disciplines,\nquestion types, and practical deployment costs. Using a novel dataset,\nEduScopeQA, of 3,176 questions across academic subjects, we measure performance\non various educational query types, from specific facts to broad thematic\ndiscussions. We also evaluate system alignment with a dataset of systematically\naltered textbooks that contradict the LLM's latent knowledge. We find that\nOpenAI Vector Search RAG (representing vector-based RAG) performs well as a\nlow-cost generalist, especially for quick fact retrieval. On the other hand,\nGraphRAG Global excels at providing pedagogically rich answers to thematic\nqueries, and GraphRAG Local achieves the highest accuracy with the dense,\naltered textbooks when corpus integrity is critical. Accounting for the 10-20x\nhigher resource usage of GraphRAG (representing graph-based RAG), we show that\na dynamic branching framework that routes queries to the optimal retrieval\nmethod boosts fidelity and efficiency. These insights provide actionable\nguidelines for educators and system designers to integrate RAG-augmented LLMs\ninto learning environments effectively.\n", "link": "http://arxiv.org/abs/2509.07846v1", "date": "2025-09-09", "relevancy": 1.9983, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4955}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20LLMs%20for%20the%20Classroom%20with%20Knowledge-Based%20Retrieval%20--%20A%0A%20%20Comparative%20RAG%20Study&body=Title%3A%20Aligning%20LLMs%20for%20the%20Classroom%20with%20Knowledge-Based%20Retrieval%20--%20A%0A%20%20Comparative%20RAG%20Study%0AAuthor%3A%20Amay%20Jain%20and%20Liu%20Cui%20and%20Si%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20like%20ChatGPT%20are%20increasingly%20used%20in%20classrooms%2C%20but%0Athey%20often%20provide%20outdated%20or%20fabricated%20information%20that%20can%20mislead%0Astudents.%20Retrieval%20Augmented%20Generation%20%28RAG%29%20improves%20reliability%20of%20LLMs%20by%0Agrounding%20responses%20in%20external%20resources.%20We%20investigate%20two%20accessible%20RAG%0Aparadigms%2C%20vector-based%20retrieval%20and%20graph-based%20retrieval%20to%20identify%20best%0Apractices%20for%20classroom%20question%20answering%20%28QA%29.%20Existing%20comparative%20studies%0Afail%20to%20account%20for%20pedagogical%20factors%20such%20as%20educational%20disciplines%2C%0Aquestion%20types%2C%20and%20practical%20deployment%20costs.%20Using%20a%20novel%20dataset%2C%0AEduScopeQA%2C%20of%203%2C176%20questions%20across%20academic%20subjects%2C%20we%20measure%20performance%0Aon%20various%20educational%20query%20types%2C%20from%20specific%20facts%20to%20broad%20thematic%0Adiscussions.%20We%20also%20evaluate%20system%20alignment%20with%20a%20dataset%20of%20systematically%0Aaltered%20textbooks%20that%20contradict%20the%20LLM%27s%20latent%20knowledge.%20We%20find%20that%0AOpenAI%20Vector%20Search%20RAG%20%28representing%20vector-based%20RAG%29%20performs%20well%20as%20a%0Alow-cost%20generalist%2C%20especially%20for%20quick%20fact%20retrieval.%20On%20the%20other%20hand%2C%0AGraphRAG%20Global%20excels%20at%20providing%20pedagogically%20rich%20answers%20to%20thematic%0Aqueries%2C%20and%20GraphRAG%20Local%20achieves%20the%20highest%20accuracy%20with%20the%20dense%2C%0Aaltered%20textbooks%20when%20corpus%20integrity%20is%20critical.%20Accounting%20for%20the%2010-20x%0Ahigher%20resource%20usage%20of%20GraphRAG%20%28representing%20graph-based%20RAG%29%2C%20we%20show%20that%0Aa%20dynamic%20branching%20framework%20that%20routes%20queries%20to%20the%20optimal%20retrieval%0Amethod%20boosts%20fidelity%20and%20efficiency.%20These%20insights%20provide%20actionable%0Aguidelines%20for%20educators%20and%20system%20designers%20to%20integrate%20RAG-augmented%20LLMs%0Ainto%20learning%20environments%20effectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520LLMs%2520for%2520the%2520Classroom%2520with%2520Knowledge-Based%2520Retrieval%2520--%2520A%250A%2520%2520Comparative%2520RAG%2520Study%26entry.906535625%3DAmay%2520Jain%2520and%2520Liu%2520Cui%2520and%2520Si%2520Chen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520like%2520ChatGPT%2520are%2520increasingly%2520used%2520in%2520classrooms%252C%2520but%250Athey%2520often%2520provide%2520outdated%2520or%2520fabricated%2520information%2520that%2520can%2520mislead%250Astudents.%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520improves%2520reliability%2520of%2520LLMs%2520by%250Agrounding%2520responses%2520in%2520external%2520resources.%2520We%2520investigate%2520two%2520accessible%2520RAG%250Aparadigms%252C%2520vector-based%2520retrieval%2520and%2520graph-based%2520retrieval%2520to%2520identify%2520best%250Apractices%2520for%2520classroom%2520question%2520answering%2520%2528QA%2529.%2520Existing%2520comparative%2520studies%250Afail%2520to%2520account%2520for%2520pedagogical%2520factors%2520such%2520as%2520educational%2520disciplines%252C%250Aquestion%2520types%252C%2520and%2520practical%2520deployment%2520costs.%2520Using%2520a%2520novel%2520dataset%252C%250AEduScopeQA%252C%2520of%25203%252C176%2520questions%2520across%2520academic%2520subjects%252C%2520we%2520measure%2520performance%250Aon%2520various%2520educational%2520query%2520types%252C%2520from%2520specific%2520facts%2520to%2520broad%2520thematic%250Adiscussions.%2520We%2520also%2520evaluate%2520system%2520alignment%2520with%2520a%2520dataset%2520of%2520systematically%250Aaltered%2520textbooks%2520that%2520contradict%2520the%2520LLM%2527s%2520latent%2520knowledge.%2520We%2520find%2520that%250AOpenAI%2520Vector%2520Search%2520RAG%2520%2528representing%2520vector-based%2520RAG%2529%2520performs%2520well%2520as%2520a%250Alow-cost%2520generalist%252C%2520especially%2520for%2520quick%2520fact%2520retrieval.%2520On%2520the%2520other%2520hand%252C%250AGraphRAG%2520Global%2520excels%2520at%2520providing%2520pedagogically%2520rich%2520answers%2520to%2520thematic%250Aqueries%252C%2520and%2520GraphRAG%2520Local%2520achieves%2520the%2520highest%2520accuracy%2520with%2520the%2520dense%252C%250Aaltered%2520textbooks%2520when%2520corpus%2520integrity%2520is%2520critical.%2520Accounting%2520for%2520the%252010-20x%250Ahigher%2520resource%2520usage%2520of%2520GraphRAG%2520%2528representing%2520graph-based%2520RAG%2529%252C%2520we%2520show%2520that%250Aa%2520dynamic%2520branching%2520framework%2520that%2520routes%2520queries%2520to%2520the%2520optimal%2520retrieval%250Amethod%2520boosts%2520fidelity%2520and%2520efficiency.%2520These%2520insights%2520provide%2520actionable%250Aguidelines%2520for%2520educators%2520and%2520system%2520designers%2520to%2520integrate%2520RAG-augmented%2520LLMs%250Ainto%2520learning%2520environments%2520effectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20LLMs%20for%20the%20Classroom%20with%20Knowledge-Based%20Retrieval%20--%20A%0A%20%20Comparative%20RAG%20Study&entry.906535625=Amay%20Jain%20and%20Liu%20Cui%20and%20Si%20Chen&entry.1292438233=%20%20Large%20language%20models%20like%20ChatGPT%20are%20increasingly%20used%20in%20classrooms%2C%20but%0Athey%20often%20provide%20outdated%20or%20fabricated%20information%20that%20can%20mislead%0Astudents.%20Retrieval%20Augmented%20Generation%20%28RAG%29%20improves%20reliability%20of%20LLMs%20by%0Agrounding%20responses%20in%20external%20resources.%20We%20investigate%20two%20accessible%20RAG%0Aparadigms%2C%20vector-based%20retrieval%20and%20graph-based%20retrieval%20to%20identify%20best%0Apractices%20for%20classroom%20question%20answering%20%28QA%29.%20Existing%20comparative%20studies%0Afail%20to%20account%20for%20pedagogical%20factors%20such%20as%20educational%20disciplines%2C%0Aquestion%20types%2C%20and%20practical%20deployment%20costs.%20Using%20a%20novel%20dataset%2C%0AEduScopeQA%2C%20of%203%2C176%20questions%20across%20academic%20subjects%2C%20we%20measure%20performance%0Aon%20various%20educational%20query%20types%2C%20from%20specific%20facts%20to%20broad%20thematic%0Adiscussions.%20We%20also%20evaluate%20system%20alignment%20with%20a%20dataset%20of%20systematically%0Aaltered%20textbooks%20that%20contradict%20the%20LLM%27s%20latent%20knowledge.%20We%20find%20that%0AOpenAI%20Vector%20Search%20RAG%20%28representing%20vector-based%20RAG%29%20performs%20well%20as%20a%0Alow-cost%20generalist%2C%20especially%20for%20quick%20fact%20retrieval.%20On%20the%20other%20hand%2C%0AGraphRAG%20Global%20excels%20at%20providing%20pedagogically%20rich%20answers%20to%20thematic%0Aqueries%2C%20and%20GraphRAG%20Local%20achieves%20the%20highest%20accuracy%20with%20the%20dense%2C%0Aaltered%20textbooks%20when%20corpus%20integrity%20is%20critical.%20Accounting%20for%20the%2010-20x%0Ahigher%20resource%20usage%20of%20GraphRAG%20%28representing%20graph-based%20RAG%29%2C%20we%20show%20that%0Aa%20dynamic%20branching%20framework%20that%20routes%20queries%20to%20the%20optimal%20retrieval%0Amethod%20boosts%20fidelity%20and%20efficiency.%20These%20insights%20provide%20actionable%0Aguidelines%20for%20educators%20and%20system%20designers%20to%20integrate%20RAG-augmented%20LLMs%0Ainto%20learning%20environments%20effectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07846v1&entry.124074799=Read"},
{"title": "Active Membership Inference Test (aMINT): Enhancing Model Auditability\n  with Multi-Task Learning", "author": "Daniel DeAlcala and Aythami Morales and Julian Fierrez and Gonzalo Mancera and Ruben Tolosana and Javier Ortega-Garcia", "abstract": "  Active Membership Inference Test (aMINT) is a method designed to detect\nwhether given data were used during the training of machine learning models. In\nActive MINT, we propose a novel multitask learning process that involves\ntraining simultaneously two models: the original or Audited Model, and a\nsecondary model, referred to as the MINT Model, responsible for identifying the\ndata used for training the Audited Model. This novel multi-task learning\napproach has been designed to incorporate the auditability of the model as an\noptimization objective during the training process of neural networks. The\nproposed approach incorporates intermediate activation maps as inputs to the\nMINT layers, which are trained to enhance the detection of training data. We\npresent results using a wide range of neural networks, from lighter\narchitectures such as MobileNet to more complex ones such as Vision\nTransformers, evaluated in 5 public benchmarks. Our proposed Active MINT\nachieves over 80% accuracy in detecting if given data was used for training,\nsignificantly outperforming previous approaches in the literature. Our aMINT\nand related methodological developments contribute to increasing transparency\nin AI models, facilitating stronger safeguards in AI deployments to achieve\nproper security, privacy, and copyright protection.\n", "link": "http://arxiv.org/abs/2509.07879v1", "date": "2025-09-09", "relevancy": 1.998, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5149}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5093}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Membership%20Inference%20Test%20%28aMINT%29%3A%20Enhancing%20Model%20Auditability%0A%20%20with%20Multi-Task%20Learning&body=Title%3A%20Active%20Membership%20Inference%20Test%20%28aMINT%29%3A%20Enhancing%20Model%20Auditability%0A%20%20with%20Multi-Task%20Learning%0AAuthor%3A%20Daniel%20DeAlcala%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%20and%20Gonzalo%20Mancera%20and%20Ruben%20Tolosana%20and%20Javier%20Ortega-Garcia%0AAbstract%3A%20%20%20Active%20Membership%20Inference%20Test%20%28aMINT%29%20is%20a%20method%20designed%20to%20detect%0Awhether%20given%20data%20were%20used%20during%20the%20training%20of%20machine%20learning%20models.%20In%0AActive%20MINT%2C%20we%20propose%20a%20novel%20multitask%20learning%20process%20that%20involves%0Atraining%20simultaneously%20two%20models%3A%20the%20original%20or%20Audited%20Model%2C%20and%20a%0Asecondary%20model%2C%20referred%20to%20as%20the%20MINT%20Model%2C%20responsible%20for%20identifying%20the%0Adata%20used%20for%20training%20the%20Audited%20Model.%20This%20novel%20multi-task%20learning%0Aapproach%20has%20been%20designed%20to%20incorporate%20the%20auditability%20of%20the%20model%20as%20an%0Aoptimization%20objective%20during%20the%20training%20process%20of%20neural%20networks.%20The%0Aproposed%20approach%20incorporates%20intermediate%20activation%20maps%20as%20inputs%20to%20the%0AMINT%20layers%2C%20which%20are%20trained%20to%20enhance%20the%20detection%20of%20training%20data.%20We%0Apresent%20results%20using%20a%20wide%20range%20of%20neural%20networks%2C%20from%20lighter%0Aarchitectures%20such%20as%20MobileNet%20to%20more%20complex%20ones%20such%20as%20Vision%0ATransformers%2C%20evaluated%20in%205%20public%20benchmarks.%20Our%20proposed%20Active%20MINT%0Aachieves%20over%2080%25%20accuracy%20in%20detecting%20if%20given%20data%20was%20used%20for%20training%2C%0Asignificantly%20outperforming%20previous%20approaches%20in%20the%20literature.%20Our%20aMINT%0Aand%20related%20methodological%20developments%20contribute%20to%20increasing%20transparency%0Ain%20AI%20models%2C%20facilitating%20stronger%20safeguards%20in%20AI%20deployments%20to%20achieve%0Aproper%20security%2C%20privacy%2C%20and%20copyright%20protection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Membership%2520Inference%2520Test%2520%2528aMINT%2529%253A%2520Enhancing%2520Model%2520Auditability%250A%2520%2520with%2520Multi-Task%2520Learning%26entry.906535625%3DDaniel%2520DeAlcala%2520and%2520Aythami%2520Morales%2520and%2520Julian%2520Fierrez%2520and%2520Gonzalo%2520Mancera%2520and%2520Ruben%2520Tolosana%2520and%2520Javier%2520Ortega-Garcia%26entry.1292438233%3D%2520%2520Active%2520Membership%2520Inference%2520Test%2520%2528aMINT%2529%2520is%2520a%2520method%2520designed%2520to%2520detect%250Awhether%2520given%2520data%2520were%2520used%2520during%2520the%2520training%2520of%2520machine%2520learning%2520models.%2520In%250AActive%2520MINT%252C%2520we%2520propose%2520a%2520novel%2520multitask%2520learning%2520process%2520that%2520involves%250Atraining%2520simultaneously%2520two%2520models%253A%2520the%2520original%2520or%2520Audited%2520Model%252C%2520and%2520a%250Asecondary%2520model%252C%2520referred%2520to%2520as%2520the%2520MINT%2520Model%252C%2520responsible%2520for%2520identifying%2520the%250Adata%2520used%2520for%2520training%2520the%2520Audited%2520Model.%2520This%2520novel%2520multi-task%2520learning%250Aapproach%2520has%2520been%2520designed%2520to%2520incorporate%2520the%2520auditability%2520of%2520the%2520model%2520as%2520an%250Aoptimization%2520objective%2520during%2520the%2520training%2520process%2520of%2520neural%2520networks.%2520The%250Aproposed%2520approach%2520incorporates%2520intermediate%2520activation%2520maps%2520as%2520inputs%2520to%2520the%250AMINT%2520layers%252C%2520which%2520are%2520trained%2520to%2520enhance%2520the%2520detection%2520of%2520training%2520data.%2520We%250Apresent%2520results%2520using%2520a%2520wide%2520range%2520of%2520neural%2520networks%252C%2520from%2520lighter%250Aarchitectures%2520such%2520as%2520MobileNet%2520to%2520more%2520complex%2520ones%2520such%2520as%2520Vision%250ATransformers%252C%2520evaluated%2520in%25205%2520public%2520benchmarks.%2520Our%2520proposed%2520Active%2520MINT%250Aachieves%2520over%252080%2525%2520accuracy%2520in%2520detecting%2520if%2520given%2520data%2520was%2520used%2520for%2520training%252C%250Asignificantly%2520outperforming%2520previous%2520approaches%2520in%2520the%2520literature.%2520Our%2520aMINT%250Aand%2520related%2520methodological%2520developments%2520contribute%2520to%2520increasing%2520transparency%250Ain%2520AI%2520models%252C%2520facilitating%2520stronger%2520safeguards%2520in%2520AI%2520deployments%2520to%2520achieve%250Aproper%2520security%252C%2520privacy%252C%2520and%2520copyright%2520protection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Membership%20Inference%20Test%20%28aMINT%29%3A%20Enhancing%20Model%20Auditability%0A%20%20with%20Multi-Task%20Learning&entry.906535625=Daniel%20DeAlcala%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%20and%20Gonzalo%20Mancera%20and%20Ruben%20Tolosana%20and%20Javier%20Ortega-Garcia&entry.1292438233=%20%20Active%20Membership%20Inference%20Test%20%28aMINT%29%20is%20a%20method%20designed%20to%20detect%0Awhether%20given%20data%20were%20used%20during%20the%20training%20of%20machine%20learning%20models.%20In%0AActive%20MINT%2C%20we%20propose%20a%20novel%20multitask%20learning%20process%20that%20involves%0Atraining%20simultaneously%20two%20models%3A%20the%20original%20or%20Audited%20Model%2C%20and%20a%0Asecondary%20model%2C%20referred%20to%20as%20the%20MINT%20Model%2C%20responsible%20for%20identifying%20the%0Adata%20used%20for%20training%20the%20Audited%20Model.%20This%20novel%20multi-task%20learning%0Aapproach%20has%20been%20designed%20to%20incorporate%20the%20auditability%20of%20the%20model%20as%20an%0Aoptimization%20objective%20during%20the%20training%20process%20of%20neural%20networks.%20The%0Aproposed%20approach%20incorporates%20intermediate%20activation%20maps%20as%20inputs%20to%20the%0AMINT%20layers%2C%20which%20are%20trained%20to%20enhance%20the%20detection%20of%20training%20data.%20We%0Apresent%20results%20using%20a%20wide%20range%20of%20neural%20networks%2C%20from%20lighter%0Aarchitectures%20such%20as%20MobileNet%20to%20more%20complex%20ones%20such%20as%20Vision%0ATransformers%2C%20evaluated%20in%205%20public%20benchmarks.%20Our%20proposed%20Active%20MINT%0Aachieves%20over%2080%25%20accuracy%20in%20detecting%20if%20given%20data%20was%20used%20for%20training%2C%0Asignificantly%20outperforming%20previous%20approaches%20in%20the%20literature.%20Our%20aMINT%0Aand%20related%20methodological%20developments%20contribute%20to%20increasing%20transparency%0Ain%20AI%20models%2C%20facilitating%20stronger%20safeguards%20in%20AI%20deployments%20to%20achieve%0Aproper%20security%2C%20privacy%2C%20and%20copyright%20protection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07879v1&entry.124074799=Read"},
{"title": "K2-Think: A Parameter-Efficient Reasoning System", "author": "Zhoujun Cheng and Richard Fan and Shibo Hao and Taylor W. Killian and Haonan Li and Suqi Sun and Hector Ren and Alexander Moreno and Daqian Zhang and Tianjun Zhong and Yuxin Xiong and Yuanzhe Hu and Yutao Xie and Xudong Han and Yuqi Wang and Varad Pimpalkhute and Yonghao Zhuang and Aaryamonvikram Singh and Xuezhi Liang and Anze Xie and Jianshu She and Desai Fan and Chengqian Gao and Liqun Ma and Mikhail Yurochkin and John Maggs and Xuezhe Ma and Guowei He and Zhiting Hu and Zhengzhong Liu and Eric P. Xing", "abstract": "  K2-Think is a reasoning system that achieves state-of-the-art performance\nwith a 32B parameter model, matching or surpassing much larger models like\nGPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system\nshows that smaller models can compete at the highest levels by combining\nadvanced post-training and test-time computation techniques. The approach is\nbased on six key technical pillars: Long Chain-of-thought Supervised\nFinetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic\nplanning prior to reasoning, Test-time Scaling, Speculative Decoding, and\nInference-optimized Hardware, all using publicly available open-source\ndatasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art\nscores on public benchmarks for open-source models, while also performing\nstrongly in other areas such as Code and Science. Our results confirm that a\nmore parameter-efficient model like K2-Think 32B can compete with\nstate-of-the-art systems through an integrated post-training recipe that\nincludes long chain-of-thought training and strategic inference-time\nenhancements, making open-source reasoning systems more accessible and\naffordable. K2-Think is freely available at k2think.ai, offering best-in-class\ninference speeds of over 2,000 tokens per second per request via the Cerebras\nWafer-Scale Engine.\n", "link": "http://arxiv.org/abs/2509.07604v1", "date": "2025-09-09", "relevancy": 1.9942, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20K2-Think%3A%20A%20Parameter-Efficient%20Reasoning%20System&body=Title%3A%20K2-Think%3A%20A%20Parameter-Efficient%20Reasoning%20System%0AAuthor%3A%20Zhoujun%20Cheng%20and%20Richard%20Fan%20and%20Shibo%20Hao%20and%20Taylor%20W.%20Killian%20and%20Haonan%20Li%20and%20Suqi%20Sun%20and%20Hector%20Ren%20and%20Alexander%20Moreno%20and%20Daqian%20Zhang%20and%20Tianjun%20Zhong%20and%20Yuxin%20Xiong%20and%20Yuanzhe%20Hu%20and%20Yutao%20Xie%20and%20Xudong%20Han%20and%20Yuqi%20Wang%20and%20Varad%20Pimpalkhute%20and%20Yonghao%20Zhuang%20and%20Aaryamonvikram%20Singh%20and%20Xuezhi%20Liang%20and%20Anze%20Xie%20and%20Jianshu%20She%20and%20Desai%20Fan%20and%20Chengqian%20Gao%20and%20Liqun%20Ma%20and%20Mikhail%20Yurochkin%20and%20John%20Maggs%20and%20Xuezhe%20Ma%20and%20Guowei%20He%20and%20Zhiting%20Hu%20and%20Zhengzhong%20Liu%20and%20Eric%20P.%20Xing%0AAbstract%3A%20%20%20K2-Think%20is%20a%20reasoning%20system%20that%20achieves%20state-of-the-art%20performance%0Awith%20a%2032B%20parameter%20model%2C%20matching%20or%20surpassing%20much%20larger%20models%20like%0AGPT-OSS%20120B%20and%20DeepSeek%20v3.1.%20Built%20on%20the%20Qwen2.5%20base%20model%2C%20our%20system%0Ashows%20that%20smaller%20models%20can%20compete%20at%20the%20highest%20levels%20by%20combining%0Aadvanced%20post-training%20and%20test-time%20computation%20techniques.%20The%20approach%20is%0Abased%20on%20six%20key%20technical%20pillars%3A%20Long%20Chain-of-thought%20Supervised%0AFinetuning%2C%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%2C%20Agentic%0Aplanning%20prior%20to%20reasoning%2C%20Test-time%20Scaling%2C%20Speculative%20Decoding%2C%20and%0AInference-optimized%20Hardware%2C%20all%20using%20publicly%20available%20open-source%0Adatasets.%20K2-Think%20excels%20in%20mathematical%20reasoning%2C%20achieving%20state-of-the-art%0Ascores%20on%20public%20benchmarks%20for%20open-source%20models%2C%20while%20also%20performing%0Astrongly%20in%20other%20areas%20such%20as%20Code%20and%20Science.%20Our%20results%20confirm%20that%20a%0Amore%20parameter-efficient%20model%20like%20K2-Think%2032B%20can%20compete%20with%0Astate-of-the-art%20systems%20through%20an%20integrated%20post-training%20recipe%20that%0Aincludes%20long%20chain-of-thought%20training%20and%20strategic%20inference-time%0Aenhancements%2C%20making%20open-source%20reasoning%20systems%20more%20accessible%20and%0Aaffordable.%20K2-Think%20is%20freely%20available%20at%20k2think.ai%2C%20offering%20best-in-class%0Ainference%20speeds%20of%20over%202%2C000%20tokens%20per%20second%20per%20request%20via%20the%20Cerebras%0AWafer-Scale%20Engine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DK2-Think%253A%2520A%2520Parameter-Efficient%2520Reasoning%2520System%26entry.906535625%3DZhoujun%2520Cheng%2520and%2520Richard%2520Fan%2520and%2520Shibo%2520Hao%2520and%2520Taylor%2520W.%2520Killian%2520and%2520Haonan%2520Li%2520and%2520Suqi%2520Sun%2520and%2520Hector%2520Ren%2520and%2520Alexander%2520Moreno%2520and%2520Daqian%2520Zhang%2520and%2520Tianjun%2520Zhong%2520and%2520Yuxin%2520Xiong%2520and%2520Yuanzhe%2520Hu%2520and%2520Yutao%2520Xie%2520and%2520Xudong%2520Han%2520and%2520Yuqi%2520Wang%2520and%2520Varad%2520Pimpalkhute%2520and%2520Yonghao%2520Zhuang%2520and%2520Aaryamonvikram%2520Singh%2520and%2520Xuezhi%2520Liang%2520and%2520Anze%2520Xie%2520and%2520Jianshu%2520She%2520and%2520Desai%2520Fan%2520and%2520Chengqian%2520Gao%2520and%2520Liqun%2520Ma%2520and%2520Mikhail%2520Yurochkin%2520and%2520John%2520Maggs%2520and%2520Xuezhe%2520Ma%2520and%2520Guowei%2520He%2520and%2520Zhiting%2520Hu%2520and%2520Zhengzhong%2520Liu%2520and%2520Eric%2520P.%2520Xing%26entry.1292438233%3D%2520%2520K2-Think%2520is%2520a%2520reasoning%2520system%2520that%2520achieves%2520state-of-the-art%2520performance%250Awith%2520a%252032B%2520parameter%2520model%252C%2520matching%2520or%2520surpassing%2520much%2520larger%2520models%2520like%250AGPT-OSS%2520120B%2520and%2520DeepSeek%2520v3.1.%2520Built%2520on%2520the%2520Qwen2.5%2520base%2520model%252C%2520our%2520system%250Ashows%2520that%2520smaller%2520models%2520can%2520compete%2520at%2520the%2520highest%2520levels%2520by%2520combining%250Aadvanced%2520post-training%2520and%2520test-time%2520computation%2520techniques.%2520The%2520approach%2520is%250Abased%2520on%2520six%2520key%2520technical%2520pillars%253A%2520Long%2520Chain-of-thought%2520Supervised%250AFinetuning%252C%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%252C%2520Agentic%250Aplanning%2520prior%2520to%2520reasoning%252C%2520Test-time%2520Scaling%252C%2520Speculative%2520Decoding%252C%2520and%250AInference-optimized%2520Hardware%252C%2520all%2520using%2520publicly%2520available%2520open-source%250Adatasets.%2520K2-Think%2520excels%2520in%2520mathematical%2520reasoning%252C%2520achieving%2520state-of-the-art%250Ascores%2520on%2520public%2520benchmarks%2520for%2520open-source%2520models%252C%2520while%2520also%2520performing%250Astrongly%2520in%2520other%2520areas%2520such%2520as%2520Code%2520and%2520Science.%2520Our%2520results%2520confirm%2520that%2520a%250Amore%2520parameter-efficient%2520model%2520like%2520K2-Think%252032B%2520can%2520compete%2520with%250Astate-of-the-art%2520systems%2520through%2520an%2520integrated%2520post-training%2520recipe%2520that%250Aincludes%2520long%2520chain-of-thought%2520training%2520and%2520strategic%2520inference-time%250Aenhancements%252C%2520making%2520open-source%2520reasoning%2520systems%2520more%2520accessible%2520and%250Aaffordable.%2520K2-Think%2520is%2520freely%2520available%2520at%2520k2think.ai%252C%2520offering%2520best-in-class%250Ainference%2520speeds%2520of%2520over%25202%252C000%2520tokens%2520per%2520second%2520per%2520request%2520via%2520the%2520Cerebras%250AWafer-Scale%2520Engine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=K2-Think%3A%20A%20Parameter-Efficient%20Reasoning%20System&entry.906535625=Zhoujun%20Cheng%20and%20Richard%20Fan%20and%20Shibo%20Hao%20and%20Taylor%20W.%20Killian%20and%20Haonan%20Li%20and%20Suqi%20Sun%20and%20Hector%20Ren%20and%20Alexander%20Moreno%20and%20Daqian%20Zhang%20and%20Tianjun%20Zhong%20and%20Yuxin%20Xiong%20and%20Yuanzhe%20Hu%20and%20Yutao%20Xie%20and%20Xudong%20Han%20and%20Yuqi%20Wang%20and%20Varad%20Pimpalkhute%20and%20Yonghao%20Zhuang%20and%20Aaryamonvikram%20Singh%20and%20Xuezhi%20Liang%20and%20Anze%20Xie%20and%20Jianshu%20She%20and%20Desai%20Fan%20and%20Chengqian%20Gao%20and%20Liqun%20Ma%20and%20Mikhail%20Yurochkin%20and%20John%20Maggs%20and%20Xuezhe%20Ma%20and%20Guowei%20He%20and%20Zhiting%20Hu%20and%20Zhengzhong%20Liu%20and%20Eric%20P.%20Xing&entry.1292438233=%20%20K2-Think%20is%20a%20reasoning%20system%20that%20achieves%20state-of-the-art%20performance%0Awith%20a%2032B%20parameter%20model%2C%20matching%20or%20surpassing%20much%20larger%20models%20like%0AGPT-OSS%20120B%20and%20DeepSeek%20v3.1.%20Built%20on%20the%20Qwen2.5%20base%20model%2C%20our%20system%0Ashows%20that%20smaller%20models%20can%20compete%20at%20the%20highest%20levels%20by%20combining%0Aadvanced%20post-training%20and%20test-time%20computation%20techniques.%20The%20approach%20is%0Abased%20on%20six%20key%20technical%20pillars%3A%20Long%20Chain-of-thought%20Supervised%0AFinetuning%2C%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%2C%20Agentic%0Aplanning%20prior%20to%20reasoning%2C%20Test-time%20Scaling%2C%20Speculative%20Decoding%2C%20and%0AInference-optimized%20Hardware%2C%20all%20using%20publicly%20available%20open-source%0Adatasets.%20K2-Think%20excels%20in%20mathematical%20reasoning%2C%20achieving%20state-of-the-art%0Ascores%20on%20public%20benchmarks%20for%20open-source%20models%2C%20while%20also%20performing%0Astrongly%20in%20other%20areas%20such%20as%20Code%20and%20Science.%20Our%20results%20confirm%20that%20a%0Amore%20parameter-efficient%20model%20like%20K2-Think%2032B%20can%20compete%20with%0Astate-of-the-art%20systems%20through%20an%20integrated%20post-training%20recipe%20that%0Aincludes%20long%20chain-of-thought%20training%20and%20strategic%20inference-time%0Aenhancements%2C%20making%20open-source%20reasoning%20systems%20more%20accessible%20and%0Aaffordable.%20K2-Think%20is%20freely%20available%20at%20k2think.ai%2C%20offering%20best-in-class%0Ainference%20speeds%20of%20over%202%2C000%20tokens%20per%20second%20per%20request%20via%20the%20Cerebras%0AWafer-Scale%20Engine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07604v1&entry.124074799=Read"},
{"title": "Bootstrapping Task Spaces for Self-Improvement", "author": "Minqi Jiang and Andrei Lupu and Yoram Bachrach", "abstract": "  Progress in many task domains emerges from repeated revisions to previous\nsolution attempts. Training agents that can reliably self-improve over such\nsequences at inference-time is a natural target for reinforcement learning\n(RL), yet the naive approach assumes a fixed maximum iteration depth, which can\nbe both costly and arbitrary. We present Exploratory Iteration (ExIt), a family\nof autocurriculum RL methods that directly exploits the recurrent structure of\nself-improvement tasks to train LLMs to perform multi-step self-improvement at\ninference-time while only training on the most informative single-step\niterations. ExIt grows a task space by selectively sampling the most\ninformative intermediate, partial histories encountered during an episode for\ncontinued iteration, treating these starting points as new self-iteration task\ninstances to train a self-improvement policy. ExIt can further pair with\nexplicit exploration mechanisms to sustain greater task diversity. Across\nseveral domains, encompassing competition math, multi-turn tool-use, and\nmachine learning engineering, we demonstrate that ExIt strategies, starting\nfrom either a single or many task instances, can produce policies exhibiting\nstrong inference-time self-improvement on held-out task instances, and the\nability to iterate towards higher performance over a step budget extending\nbeyond the average iteration depth encountered during training.\n", "link": "http://arxiv.org/abs/2509.04575v2", "date": "2025-09-09", "relevancy": 1.9812, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.542}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4948}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bootstrapping%20Task%20Spaces%20for%20Self-Improvement&body=Title%3A%20Bootstrapping%20Task%20Spaces%20for%20Self-Improvement%0AAuthor%3A%20Minqi%20Jiang%20and%20Andrei%20Lupu%20and%20Yoram%20Bachrach%0AAbstract%3A%20%20%20Progress%20in%20many%20task%20domains%20emerges%20from%20repeated%20revisions%20to%20previous%0Asolution%20attempts.%20Training%20agents%20that%20can%20reliably%20self-improve%20over%20such%0Asequences%20at%20inference-time%20is%20a%20natural%20target%20for%20reinforcement%20learning%0A%28RL%29%2C%20yet%20the%20naive%20approach%20assumes%20a%20fixed%20maximum%20iteration%20depth%2C%20which%20can%0Abe%20both%20costly%20and%20arbitrary.%20We%20present%20Exploratory%20Iteration%20%28ExIt%29%2C%20a%20family%0Aof%20autocurriculum%20RL%20methods%20that%20directly%20exploits%20the%20recurrent%20structure%20of%0Aself-improvement%20tasks%20to%20train%20LLMs%20to%20perform%20multi-step%20self-improvement%20at%0Ainference-time%20while%20only%20training%20on%20the%20most%20informative%20single-step%0Aiterations.%20ExIt%20grows%20a%20task%20space%20by%20selectively%20sampling%20the%20most%0Ainformative%20intermediate%2C%20partial%20histories%20encountered%20during%20an%20episode%20for%0Acontinued%20iteration%2C%20treating%20these%20starting%20points%20as%20new%20self-iteration%20task%0Ainstances%20to%20train%20a%20self-improvement%20policy.%20ExIt%20can%20further%20pair%20with%0Aexplicit%20exploration%20mechanisms%20to%20sustain%20greater%20task%20diversity.%20Across%0Aseveral%20domains%2C%20encompassing%20competition%20math%2C%20multi-turn%20tool-use%2C%20and%0Amachine%20learning%20engineering%2C%20we%20demonstrate%20that%20ExIt%20strategies%2C%20starting%0Afrom%20either%20a%20single%20or%20many%20task%20instances%2C%20can%20produce%20policies%20exhibiting%0Astrong%20inference-time%20self-improvement%20on%20held-out%20task%20instances%2C%20and%20the%0Aability%20to%20iterate%20towards%20higher%20performance%20over%20a%20step%20budget%20extending%0Abeyond%20the%20average%20iteration%20depth%20encountered%20during%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04575v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBootstrapping%2520Task%2520Spaces%2520for%2520Self-Improvement%26entry.906535625%3DMinqi%2520Jiang%2520and%2520Andrei%2520Lupu%2520and%2520Yoram%2520Bachrach%26entry.1292438233%3D%2520%2520Progress%2520in%2520many%2520task%2520domains%2520emerges%2520from%2520repeated%2520revisions%2520to%2520previous%250Asolution%2520attempts.%2520Training%2520agents%2520that%2520can%2520reliably%2520self-improve%2520over%2520such%250Asequences%2520at%2520inference-time%2520is%2520a%2520natural%2520target%2520for%2520reinforcement%2520learning%250A%2528RL%2529%252C%2520yet%2520the%2520naive%2520approach%2520assumes%2520a%2520fixed%2520maximum%2520iteration%2520depth%252C%2520which%2520can%250Abe%2520both%2520costly%2520and%2520arbitrary.%2520We%2520present%2520Exploratory%2520Iteration%2520%2528ExIt%2529%252C%2520a%2520family%250Aof%2520autocurriculum%2520RL%2520methods%2520that%2520directly%2520exploits%2520the%2520recurrent%2520structure%2520of%250Aself-improvement%2520tasks%2520to%2520train%2520LLMs%2520to%2520perform%2520multi-step%2520self-improvement%2520at%250Ainference-time%2520while%2520only%2520training%2520on%2520the%2520most%2520informative%2520single-step%250Aiterations.%2520ExIt%2520grows%2520a%2520task%2520space%2520by%2520selectively%2520sampling%2520the%2520most%250Ainformative%2520intermediate%252C%2520partial%2520histories%2520encountered%2520during%2520an%2520episode%2520for%250Acontinued%2520iteration%252C%2520treating%2520these%2520starting%2520points%2520as%2520new%2520self-iteration%2520task%250Ainstances%2520to%2520train%2520a%2520self-improvement%2520policy.%2520ExIt%2520can%2520further%2520pair%2520with%250Aexplicit%2520exploration%2520mechanisms%2520to%2520sustain%2520greater%2520task%2520diversity.%2520Across%250Aseveral%2520domains%252C%2520encompassing%2520competition%2520math%252C%2520multi-turn%2520tool-use%252C%2520and%250Amachine%2520learning%2520engineering%252C%2520we%2520demonstrate%2520that%2520ExIt%2520strategies%252C%2520starting%250Afrom%2520either%2520a%2520single%2520or%2520many%2520task%2520instances%252C%2520can%2520produce%2520policies%2520exhibiting%250Astrong%2520inference-time%2520self-improvement%2520on%2520held-out%2520task%2520instances%252C%2520and%2520the%250Aability%2520to%2520iterate%2520towards%2520higher%2520performance%2520over%2520a%2520step%2520budget%2520extending%250Abeyond%2520the%2520average%2520iteration%2520depth%2520encountered%2520during%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04575v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bootstrapping%20Task%20Spaces%20for%20Self-Improvement&entry.906535625=Minqi%20Jiang%20and%20Andrei%20Lupu%20and%20Yoram%20Bachrach&entry.1292438233=%20%20Progress%20in%20many%20task%20domains%20emerges%20from%20repeated%20revisions%20to%20previous%0Asolution%20attempts.%20Training%20agents%20that%20can%20reliably%20self-improve%20over%20such%0Asequences%20at%20inference-time%20is%20a%20natural%20target%20for%20reinforcement%20learning%0A%28RL%29%2C%20yet%20the%20naive%20approach%20assumes%20a%20fixed%20maximum%20iteration%20depth%2C%20which%20can%0Abe%20both%20costly%20and%20arbitrary.%20We%20present%20Exploratory%20Iteration%20%28ExIt%29%2C%20a%20family%0Aof%20autocurriculum%20RL%20methods%20that%20directly%20exploits%20the%20recurrent%20structure%20of%0Aself-improvement%20tasks%20to%20train%20LLMs%20to%20perform%20multi-step%20self-improvement%20at%0Ainference-time%20while%20only%20training%20on%20the%20most%20informative%20single-step%0Aiterations.%20ExIt%20grows%20a%20task%20space%20by%20selectively%20sampling%20the%20most%0Ainformative%20intermediate%2C%20partial%20histories%20encountered%20during%20an%20episode%20for%0Acontinued%20iteration%2C%20treating%20these%20starting%20points%20as%20new%20self-iteration%20task%0Ainstances%20to%20train%20a%20self-improvement%20policy.%20ExIt%20can%20further%20pair%20with%0Aexplicit%20exploration%20mechanisms%20to%20sustain%20greater%20task%20diversity.%20Across%0Aseveral%20domains%2C%20encompassing%20competition%20math%2C%20multi-turn%20tool-use%2C%20and%0Amachine%20learning%20engineering%2C%20we%20demonstrate%20that%20ExIt%20strategies%2C%20starting%0Afrom%20either%20a%20single%20or%20many%20task%20instances%2C%20can%20produce%20policies%20exhibiting%0Astrong%20inference-time%20self-improvement%20on%20held-out%20task%20instances%2C%20and%20the%0Aability%20to%20iterate%20towards%20higher%20performance%20over%20a%20step%20budget%20extending%0Abeyond%20the%20average%20iteration%20depth%20encountered%20during%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04575v2&entry.124074799=Read"},
{"title": "Adaptive LLM Routing under Budget Constraints", "author": "Pranoy Panda and Raghav Magazine and Chaitanya Devaguptapu and Sho Takemori and Vishal Sharma", "abstract": "  Large Language Models (LLMs) have revolutionized natural language processing,\nbut their varying capabilities and costs pose challenges in practical\napplications. LLM routing addresses this by dynamically selecting the most\nsuitable LLM for each query/task. Previous approaches treat this as a\nsupervised learning problem, assuming complete knowledge of optimal query-LLM\npairings. However, real-world scenarios lack such comprehensive mappings and\nface evolving user queries. We thus propose to study LLM routing as a\ncontextual bandit problem, enabling adaptive decision-making using bandit\nfeedback without requiring exhaustive inference across all LLMs for all queries\n(in contrast to supervised routing). To address this problem, we develop a\nshared embedding space for queries and LLMs, where query and LLM embeddings are\naligned to reflect their affinity. This space is initially learned from offline\nhuman preference data and refined through online bandit feedback. We\ninstantiate this idea through Preference-prior Informed Linucb fOr adaptive\nrouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets\nfor model routing, we introduce an online cost policy modeled as a multi-choice\nknapsack problem, ensuring resource-efficient routing.\n", "link": "http://arxiv.org/abs/2508.21141v2", "date": "2025-09-09", "relevancy": 1.9809, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4973}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.495}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20LLM%20Routing%20under%20Budget%20Constraints&body=Title%3A%20Adaptive%20LLM%20Routing%20under%20Budget%20Constraints%0AAuthor%3A%20Pranoy%20Panda%20and%20Raghav%20Magazine%20and%20Chaitanya%20Devaguptapu%20and%20Sho%20Takemori%20and%20Vishal%20Sharma%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20natural%20language%20processing%2C%0Abut%20their%20varying%20capabilities%20and%20costs%20pose%20challenges%20in%20practical%0Aapplications.%20LLM%20routing%20addresses%20this%20by%20dynamically%20selecting%20the%20most%0Asuitable%20LLM%20for%20each%20query/task.%20Previous%20approaches%20treat%20this%20as%20a%0Asupervised%20learning%20problem%2C%20assuming%20complete%20knowledge%20of%20optimal%20query-LLM%0Apairings.%20However%2C%20real-world%20scenarios%20lack%20such%20comprehensive%20mappings%20and%0Aface%20evolving%20user%20queries.%20We%20thus%20propose%20to%20study%20LLM%20routing%20as%20a%0Acontextual%20bandit%20problem%2C%20enabling%20adaptive%20decision-making%20using%20bandit%0Afeedback%20without%20requiring%20exhaustive%20inference%20across%20all%20LLMs%20for%20all%20queries%0A%28in%20contrast%20to%20supervised%20routing%29.%20To%20address%20this%20problem%2C%20we%20develop%20a%0Ashared%20embedding%20space%20for%20queries%20and%20LLMs%2C%20where%20query%20and%20LLM%20embeddings%20are%0Aaligned%20to%20reflect%20their%20affinity.%20This%20space%20is%20initially%20learned%20from%20offline%0Ahuman%20preference%20data%20and%20refined%20through%20online%20bandit%20feedback.%20We%0Ainstantiate%20this%20idea%20through%20Preference-prior%20Informed%20Linucb%20fOr%20adaptive%0ArouTing%20%28PILOT%29%2C%20a%20novel%20extension%20of%20LinUCB.%20To%20handle%20diverse%20user%20budgets%0Afor%20model%20routing%2C%20we%20introduce%20an%20online%20cost%20policy%20modeled%20as%20a%20multi-choice%0Aknapsack%20problem%2C%20ensuring%20resource-efficient%20routing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21141v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520LLM%2520Routing%2520under%2520Budget%2520Constraints%26entry.906535625%3DPranoy%2520Panda%2520and%2520Raghav%2520Magazine%2520and%2520Chaitanya%2520Devaguptapu%2520and%2520Sho%2520Takemori%2520and%2520Vishal%2520Sharma%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%2520natural%2520language%2520processing%252C%250Abut%2520their%2520varying%2520capabilities%2520and%2520costs%2520pose%2520challenges%2520in%2520practical%250Aapplications.%2520LLM%2520routing%2520addresses%2520this%2520by%2520dynamically%2520selecting%2520the%2520most%250Asuitable%2520LLM%2520for%2520each%2520query/task.%2520Previous%2520approaches%2520treat%2520this%2520as%2520a%250Asupervised%2520learning%2520problem%252C%2520assuming%2520complete%2520knowledge%2520of%2520optimal%2520query-LLM%250Apairings.%2520However%252C%2520real-world%2520scenarios%2520lack%2520such%2520comprehensive%2520mappings%2520and%250Aface%2520evolving%2520user%2520queries.%2520We%2520thus%2520propose%2520to%2520study%2520LLM%2520routing%2520as%2520a%250Acontextual%2520bandit%2520problem%252C%2520enabling%2520adaptive%2520decision-making%2520using%2520bandit%250Afeedback%2520without%2520requiring%2520exhaustive%2520inference%2520across%2520all%2520LLMs%2520for%2520all%2520queries%250A%2528in%2520contrast%2520to%2520supervised%2520routing%2529.%2520To%2520address%2520this%2520problem%252C%2520we%2520develop%2520a%250Ashared%2520embedding%2520space%2520for%2520queries%2520and%2520LLMs%252C%2520where%2520query%2520and%2520LLM%2520embeddings%2520are%250Aaligned%2520to%2520reflect%2520their%2520affinity.%2520This%2520space%2520is%2520initially%2520learned%2520from%2520offline%250Ahuman%2520preference%2520data%2520and%2520refined%2520through%2520online%2520bandit%2520feedback.%2520We%250Ainstantiate%2520this%2520idea%2520through%2520Preference-prior%2520Informed%2520Linucb%2520fOr%2520adaptive%250ArouTing%2520%2528PILOT%2529%252C%2520a%2520novel%2520extension%2520of%2520LinUCB.%2520To%2520handle%2520diverse%2520user%2520budgets%250Afor%2520model%2520routing%252C%2520we%2520introduce%2520an%2520online%2520cost%2520policy%2520modeled%2520as%2520a%2520multi-choice%250Aknapsack%2520problem%252C%2520ensuring%2520resource-efficient%2520routing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21141v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20LLM%20Routing%20under%20Budget%20Constraints&entry.906535625=Pranoy%20Panda%20and%20Raghav%20Magazine%20and%20Chaitanya%20Devaguptapu%20and%20Sho%20Takemori%20and%20Vishal%20Sharma&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20natural%20language%20processing%2C%0Abut%20their%20varying%20capabilities%20and%20costs%20pose%20challenges%20in%20practical%0Aapplications.%20LLM%20routing%20addresses%20this%20by%20dynamically%20selecting%20the%20most%0Asuitable%20LLM%20for%20each%20query/task.%20Previous%20approaches%20treat%20this%20as%20a%0Asupervised%20learning%20problem%2C%20assuming%20complete%20knowledge%20of%20optimal%20query-LLM%0Apairings.%20However%2C%20real-world%20scenarios%20lack%20such%20comprehensive%20mappings%20and%0Aface%20evolving%20user%20queries.%20We%20thus%20propose%20to%20study%20LLM%20routing%20as%20a%0Acontextual%20bandit%20problem%2C%20enabling%20adaptive%20decision-making%20using%20bandit%0Afeedback%20without%20requiring%20exhaustive%20inference%20across%20all%20LLMs%20for%20all%20queries%0A%28in%20contrast%20to%20supervised%20routing%29.%20To%20address%20this%20problem%2C%20we%20develop%20a%0Ashared%20embedding%20space%20for%20queries%20and%20LLMs%2C%20where%20query%20and%20LLM%20embeddings%20are%0Aaligned%20to%20reflect%20their%20affinity.%20This%20space%20is%20initially%20learned%20from%20offline%0Ahuman%20preference%20data%20and%20refined%20through%20online%20bandit%20feedback.%20We%0Ainstantiate%20this%20idea%20through%20Preference-prior%20Informed%20Linucb%20fOr%20adaptive%0ArouTing%20%28PILOT%29%2C%20a%20novel%20extension%20of%20LinUCB.%20To%20handle%20diverse%20user%20budgets%0Afor%20model%20routing%2C%20we%20introduce%20an%20online%20cost%20policy%20modeled%20as%20a%20multi-choice%0Aknapsack%20problem%2C%20ensuring%20resource-efficient%20routing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21141v2&entry.124074799=Read"},
{"title": "Interactive Shaping of Granular Media Using Reinforcement Learning", "author": "Benedikt Kreis and Malte Mosbach and Anny Ripke and Muhammad Ehsan Ullah and Sven Behnke and Maren Bennewitz", "abstract": "  Autonomous manipulation of granular media, such as sand, is crucial for\napplications in construction, excavation, and additive manufacturing. However,\nshaping granular materials presents unique challenges due to their\nhigh-dimensional configuration space and complex dynamics, where traditional\nrule-based approaches struggle without extensive engineering efforts.\nReinforcement learning (RL) offers a promising alternative by enabling agents\nto learn adaptive manipulation strategies through trial and error. In this\nwork, we present an RL framework that enables a robotic arm with a cubic\nend-effector and a stereo camera to shape granular media into desired target\nstructures. We show the importance of compact observations and concise reward\nformulations for the large configuration space, validating our design choices\nwith an ablation study. Our results demonstrate the effectiveness of the\nproposed approach for the training of visual policies that manipulate granular\nmedia including their real-world deployment, significantly outperforming two\nbaseline approaches in terms of target shape accuracy.\n", "link": "http://arxiv.org/abs/2509.06469v2", "date": "2025-09-09", "relevancy": 1.6088, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6026}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5392}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%20Shaping%20of%20Granular%20Media%20Using%20Reinforcement%20Learning&body=Title%3A%20Interactive%20Shaping%20of%20Granular%20Media%20Using%20Reinforcement%20Learning%0AAuthor%3A%20Benedikt%20Kreis%20and%20Malte%20Mosbach%20and%20Anny%20Ripke%20and%20Muhammad%20Ehsan%20Ullah%20and%20Sven%20Behnke%20and%20Maren%20Bennewitz%0AAbstract%3A%20%20%20Autonomous%20manipulation%20of%20granular%20media%2C%20such%20as%20sand%2C%20is%20crucial%20for%0Aapplications%20in%20construction%2C%20excavation%2C%20and%20additive%20manufacturing.%20However%2C%0Ashaping%20granular%20materials%20presents%20unique%20challenges%20due%20to%20their%0Ahigh-dimensional%20configuration%20space%20and%20complex%20dynamics%2C%20where%20traditional%0Arule-based%20approaches%20struggle%20without%20extensive%20engineering%20efforts.%0AReinforcement%20learning%20%28RL%29%20offers%20a%20promising%20alternative%20by%20enabling%20agents%0Ato%20learn%20adaptive%20manipulation%20strategies%20through%20trial%20and%20error.%20In%20this%0Awork%2C%20we%20present%20an%20RL%20framework%20that%20enables%20a%20robotic%20arm%20with%20a%20cubic%0Aend-effector%20and%20a%20stereo%20camera%20to%20shape%20granular%20media%20into%20desired%20target%0Astructures.%20We%20show%20the%20importance%20of%20compact%20observations%20and%20concise%20reward%0Aformulations%20for%20the%20large%20configuration%20space%2C%20validating%20our%20design%20choices%0Awith%20an%20ablation%20study.%20Our%20results%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20approach%20for%20the%20training%20of%20visual%20policies%20that%20manipulate%20granular%0Amedia%20including%20their%20real-world%20deployment%2C%20significantly%20outperforming%20two%0Abaseline%20approaches%20in%20terms%20of%20target%20shape%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06469v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%2520Shaping%2520of%2520Granular%2520Media%2520Using%2520Reinforcement%2520Learning%26entry.906535625%3DBenedikt%2520Kreis%2520and%2520Malte%2520Mosbach%2520and%2520Anny%2520Ripke%2520and%2520Muhammad%2520Ehsan%2520Ullah%2520and%2520Sven%2520Behnke%2520and%2520Maren%2520Bennewitz%26entry.1292438233%3D%2520%2520Autonomous%2520manipulation%2520of%2520granular%2520media%252C%2520such%2520as%2520sand%252C%2520is%2520crucial%2520for%250Aapplications%2520in%2520construction%252C%2520excavation%252C%2520and%2520additive%2520manufacturing.%2520However%252C%250Ashaping%2520granular%2520materials%2520presents%2520unique%2520challenges%2520due%2520to%2520their%250Ahigh-dimensional%2520configuration%2520space%2520and%2520complex%2520dynamics%252C%2520where%2520traditional%250Arule-based%2520approaches%2520struggle%2520without%2520extensive%2520engineering%2520efforts.%250AReinforcement%2520learning%2520%2528RL%2529%2520offers%2520a%2520promising%2520alternative%2520by%2520enabling%2520agents%250Ato%2520learn%2520adaptive%2520manipulation%2520strategies%2520through%2520trial%2520and%2520error.%2520In%2520this%250Awork%252C%2520we%2520present%2520an%2520RL%2520framework%2520that%2520enables%2520a%2520robotic%2520arm%2520with%2520a%2520cubic%250Aend-effector%2520and%2520a%2520stereo%2520camera%2520to%2520shape%2520granular%2520media%2520into%2520desired%2520target%250Astructures.%2520We%2520show%2520the%2520importance%2520of%2520compact%2520observations%2520and%2520concise%2520reward%250Aformulations%2520for%2520the%2520large%2520configuration%2520space%252C%2520validating%2520our%2520design%2520choices%250Awith%2520an%2520ablation%2520study.%2520Our%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520approach%2520for%2520the%2520training%2520of%2520visual%2520policies%2520that%2520manipulate%2520granular%250Amedia%2520including%2520their%2520real-world%2520deployment%252C%2520significantly%2520outperforming%2520two%250Abaseline%2520approaches%2520in%2520terms%2520of%2520target%2520shape%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06469v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%20Shaping%20of%20Granular%20Media%20Using%20Reinforcement%20Learning&entry.906535625=Benedikt%20Kreis%20and%20Malte%20Mosbach%20and%20Anny%20Ripke%20and%20Muhammad%20Ehsan%20Ullah%20and%20Sven%20Behnke%20and%20Maren%20Bennewitz&entry.1292438233=%20%20Autonomous%20manipulation%20of%20granular%20media%2C%20such%20as%20sand%2C%20is%20crucial%20for%0Aapplications%20in%20construction%2C%20excavation%2C%20and%20additive%20manufacturing.%20However%2C%0Ashaping%20granular%20materials%20presents%20unique%20challenges%20due%20to%20their%0Ahigh-dimensional%20configuration%20space%20and%20complex%20dynamics%2C%20where%20traditional%0Arule-based%20approaches%20struggle%20without%20extensive%20engineering%20efforts.%0AReinforcement%20learning%20%28RL%29%20offers%20a%20promising%20alternative%20by%20enabling%20agents%0Ato%20learn%20adaptive%20manipulation%20strategies%20through%20trial%20and%20error.%20In%20this%0Awork%2C%20we%20present%20an%20RL%20framework%20that%20enables%20a%20robotic%20arm%20with%20a%20cubic%0Aend-effector%20and%20a%20stereo%20camera%20to%20shape%20granular%20media%20into%20desired%20target%0Astructures.%20We%20show%20the%20importance%20of%20compact%20observations%20and%20concise%20reward%0Aformulations%20for%20the%20large%20configuration%20space%2C%20validating%20our%20design%20choices%0Awith%20an%20ablation%20study.%20Our%20results%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20approach%20for%20the%20training%20of%20visual%20policies%20that%20manipulate%20granular%0Amedia%20including%20their%20real-world%20deployment%2C%20significantly%20outperforming%20two%0Abaseline%20approaches%20in%20terms%20of%20target%20shape%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06469v2&entry.124074799=Read"},
{"title": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic\n  Manipulation", "author": "Xianlong Wang and Hewen Pan and Hangtao Zhang and Minghui Li and Shengshan Hu and Ziqi Zhou and Lulu Xue and Peijin Guo and Aishan Liu and Leo Yu Zhang and Xiaohua Jia", "abstract": "  Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link https://trojanrobot.github.io.\n", "link": "http://arxiv.org/abs/2411.11683v4", "date": "2025-09-09", "relevancy": 1.5184, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5558}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5313}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrojanRobot%3A%20Physical-world%20Backdoor%20Attacks%20Against%20VLM-based%20Robotic%0A%20%20Manipulation&body=Title%3A%20TrojanRobot%3A%20Physical-world%20Backdoor%20Attacks%20Against%20VLM-based%20Robotic%0A%20%20Manipulation%0AAuthor%3A%20Xianlong%20Wang%20and%20Hewen%20Pan%20and%20Hangtao%20Zhang%20and%20Minghui%20Li%20and%20Shengshan%20Hu%20and%20Ziqi%20Zhou%20and%20Lulu%20Xue%20and%20Peijin%20Guo%20and%20Aishan%20Liu%20and%20Leo%20Yu%20Zhang%20and%20Xiaohua%20Jia%0AAbstract%3A%20%20%20Robotic%20manipulation%20in%20the%20physical%20world%20is%20increasingly%20empowered%20by%0A%5Ctextit%7Blarge%20language%20models%7D%20%28LLMs%29%20and%20%5Ctextit%7Bvision-language%20models%7D%0A%28VLMs%29%2C%20leveraging%20their%20understanding%20and%20perception%20capabilities.%20Recently%2C%0Avarious%20attacks%20against%20such%20robotic%20policies%20have%20been%20proposed%2C%20with%20backdoor%0Aattacks%20drawing%20considerable%20attention%20for%20their%20high%20stealth%20and%20strong%0Apersistence%20capabilities.%20However%2C%20existing%20backdoor%20efforts%20are%20limited%20to%0Asimulators%20and%20suffer%20from%20physical-world%20realization.%20To%20address%20this%2C%20we%0Apropose%20%5Ctextit%7BTrojanRobot%7D%2C%20a%20highly%20stealthy%20and%20broadly%20effective%20robotic%0Abackdoor%20attack%20in%20the%20physical%20world.%20Specifically%2C%20we%20introduce%20a%0Amodule-poisoning%20approach%20by%20embedding%20a%20backdoor%20module%20into%20the%20modular%0Arobotic%20policy%2C%20enabling%20backdoor%20control%20over%20the%20policy%27s%20visual%20perception%0Amodule%20thereby%20backdooring%20the%20entire%20robotic%20policy.%20Our%20vanilla%0Aimplementation%20leverages%20a%20backdoor-finetuned%20VLM%20to%20serve%20as%20the%20backdoor%0Amodule.%20To%20enhance%20its%20generalization%20in%20physical%20environments%2C%20we%20propose%20a%0Aprime%20implementation%2C%20leveraging%20the%20LVLM-as-a-backdoor%20paradigm%20and%20developing%0Athree%20types%20of%20prime%20attacks%2C%20%5Cie%2C%20%5Ctextit%7Bpermutation%7D%2C%20%5Ctextit%7Bstagnation%7D%2C%0Aand%20%5Ctextit%7Bintentional%7D%20attacks%2C%20thus%20achieving%20finer-grained%20backdoors.%0AExtensive%20experiments%20on%20the%20UR3e%20manipulator%20with%2018%20task%20instructions%20using%0Arobotic%20policies%20based%20on%20four%20VLMs%20demonstrate%20the%20broad%20effectiveness%20and%0Aphysical-world%20stealth%20of%20TrojanRobot.%20Our%20attack%27s%20video%20demonstrations%20are%0Aavailable%20via%20a%20github%20link%20https%3A//trojanrobot.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11683v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrojanRobot%253A%2520Physical-world%2520Backdoor%2520Attacks%2520Against%2520VLM-based%2520Robotic%250A%2520%2520Manipulation%26entry.906535625%3DXianlong%2520Wang%2520and%2520Hewen%2520Pan%2520and%2520Hangtao%2520Zhang%2520and%2520Minghui%2520Li%2520and%2520Shengshan%2520Hu%2520and%2520Ziqi%2520Zhou%2520and%2520Lulu%2520Xue%2520and%2520Peijin%2520Guo%2520and%2520Aishan%2520Liu%2520and%2520Leo%2520Yu%2520Zhang%2520and%2520Xiaohua%2520Jia%26entry.1292438233%3D%2520%2520Robotic%2520manipulation%2520in%2520the%2520physical%2520world%2520is%2520increasingly%2520empowered%2520by%250A%255Ctextit%257Blarge%2520language%2520models%257D%2520%2528LLMs%2529%2520and%2520%255Ctextit%257Bvision-language%2520models%257D%250A%2528VLMs%2529%252C%2520leveraging%2520their%2520understanding%2520and%2520perception%2520capabilities.%2520Recently%252C%250Avarious%2520attacks%2520against%2520such%2520robotic%2520policies%2520have%2520been%2520proposed%252C%2520with%2520backdoor%250Aattacks%2520drawing%2520considerable%2520attention%2520for%2520their%2520high%2520stealth%2520and%2520strong%250Apersistence%2520capabilities.%2520However%252C%2520existing%2520backdoor%2520efforts%2520are%2520limited%2520to%250Asimulators%2520and%2520suffer%2520from%2520physical-world%2520realization.%2520To%2520address%2520this%252C%2520we%250Apropose%2520%255Ctextit%257BTrojanRobot%257D%252C%2520a%2520highly%2520stealthy%2520and%2520broadly%2520effective%2520robotic%250Abackdoor%2520attack%2520in%2520the%2520physical%2520world.%2520Specifically%252C%2520we%2520introduce%2520a%250Amodule-poisoning%2520approach%2520by%2520embedding%2520a%2520backdoor%2520module%2520into%2520the%2520modular%250Arobotic%2520policy%252C%2520enabling%2520backdoor%2520control%2520over%2520the%2520policy%2527s%2520visual%2520perception%250Amodule%2520thereby%2520backdooring%2520the%2520entire%2520robotic%2520policy.%2520Our%2520vanilla%250Aimplementation%2520leverages%2520a%2520backdoor-finetuned%2520VLM%2520to%2520serve%2520as%2520the%2520backdoor%250Amodule.%2520To%2520enhance%2520its%2520generalization%2520in%2520physical%2520environments%252C%2520we%2520propose%2520a%250Aprime%2520implementation%252C%2520leveraging%2520the%2520LVLM-as-a-backdoor%2520paradigm%2520and%2520developing%250Athree%2520types%2520of%2520prime%2520attacks%252C%2520%255Cie%252C%2520%255Ctextit%257Bpermutation%257D%252C%2520%255Ctextit%257Bstagnation%257D%252C%250Aand%2520%255Ctextit%257Bintentional%257D%2520attacks%252C%2520thus%2520achieving%2520finer-grained%2520backdoors.%250AExtensive%2520experiments%2520on%2520the%2520UR3e%2520manipulator%2520with%252018%2520task%2520instructions%2520using%250Arobotic%2520policies%2520based%2520on%2520four%2520VLMs%2520demonstrate%2520the%2520broad%2520effectiveness%2520and%250Aphysical-world%2520stealth%2520of%2520TrojanRobot.%2520Our%2520attack%2527s%2520video%2520demonstrations%2520are%250Aavailable%2520via%2520a%2520github%2520link%2520https%253A//trojanrobot.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11683v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrojanRobot%3A%20Physical-world%20Backdoor%20Attacks%20Against%20VLM-based%20Robotic%0A%20%20Manipulation&entry.906535625=Xianlong%20Wang%20and%20Hewen%20Pan%20and%20Hangtao%20Zhang%20and%20Minghui%20Li%20and%20Shengshan%20Hu%20and%20Ziqi%20Zhou%20and%20Lulu%20Xue%20and%20Peijin%20Guo%20and%20Aishan%20Liu%20and%20Leo%20Yu%20Zhang%20and%20Xiaohua%20Jia&entry.1292438233=%20%20Robotic%20manipulation%20in%20the%20physical%20world%20is%20increasingly%20empowered%20by%0A%5Ctextit%7Blarge%20language%20models%7D%20%28LLMs%29%20and%20%5Ctextit%7Bvision-language%20models%7D%0A%28VLMs%29%2C%20leveraging%20their%20understanding%20and%20perception%20capabilities.%20Recently%2C%0Avarious%20attacks%20against%20such%20robotic%20policies%20have%20been%20proposed%2C%20with%20backdoor%0Aattacks%20drawing%20considerable%20attention%20for%20their%20high%20stealth%20and%20strong%0Apersistence%20capabilities.%20However%2C%20existing%20backdoor%20efforts%20are%20limited%20to%0Asimulators%20and%20suffer%20from%20physical-world%20realization.%20To%20address%20this%2C%20we%0Apropose%20%5Ctextit%7BTrojanRobot%7D%2C%20a%20highly%20stealthy%20and%20broadly%20effective%20robotic%0Abackdoor%20attack%20in%20the%20physical%20world.%20Specifically%2C%20we%20introduce%20a%0Amodule-poisoning%20approach%20by%20embedding%20a%20backdoor%20module%20into%20the%20modular%0Arobotic%20policy%2C%20enabling%20backdoor%20control%20over%20the%20policy%27s%20visual%20perception%0Amodule%20thereby%20backdooring%20the%20entire%20robotic%20policy.%20Our%20vanilla%0Aimplementation%20leverages%20a%20backdoor-finetuned%20VLM%20to%20serve%20as%20the%20backdoor%0Amodule.%20To%20enhance%20its%20generalization%20in%20physical%20environments%2C%20we%20propose%20a%0Aprime%20implementation%2C%20leveraging%20the%20LVLM-as-a-backdoor%20paradigm%20and%20developing%0Athree%20types%20of%20prime%20attacks%2C%20%5Cie%2C%20%5Ctextit%7Bpermutation%7D%2C%20%5Ctextit%7Bstagnation%7D%2C%0Aand%20%5Ctextit%7Bintentional%7D%20attacks%2C%20thus%20achieving%20finer-grained%20backdoors.%0AExtensive%20experiments%20on%20the%20UR3e%20manipulator%20with%2018%20task%20instructions%20using%0Arobotic%20policies%20based%20on%20four%20VLMs%20demonstrate%20the%20broad%20effectiveness%20and%0Aphysical-world%20stealth%20of%20TrojanRobot.%20Our%20attack%27s%20video%20demonstrations%20are%0Aavailable%20via%20a%20github%20link%20https%3A//trojanrobot.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11683v4&entry.124074799=Read"},
{"title": "ASP-FZN: A Translation-based Constraint Answer Set Solver", "author": "Thomas Eiter and Tobias Geibinger and Tobias Kaminski and Nysret Musliu and Johannes Oetsch", "abstract": "  We present the solver asp-fzn for Constraint Answer Set Programming (CASP),\nwhich extends ASP with linear constraints. Our approach is based on translating\nCASP programs into the solver-independent FlatZinc language that supports\nseveral Constraint Programming and Integer Programming backend solvers. Our\nsolver supports a rich language of linear constraints, including some common\nglobal constraints. As for evaluation, we show that asp-fzn is competitive with\nstate-of-the-art ASP solvers on benchmarks taken from past ASP competitions.\nFurthermore, we evaluate it on several CASP problems from the literature and\ncompare its performance with clingcon, which is a prominent CASP solver that\nsupports most of the asp-fzn language. The performance of asp-fzn is very\npromising as it is already competitive on plain ASP and even outperforms\nclingcon on some CASP benchmarks.\n", "link": "http://arxiv.org/abs/2507.22774v2", "date": "2025-09-09", "relevancy": 1.4248, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3622}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.355}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASP-FZN%3A%20A%20Translation-based%20Constraint%20Answer%20Set%20Solver&body=Title%3A%20ASP-FZN%3A%20A%20Translation-based%20Constraint%20Answer%20Set%20Solver%0AAuthor%3A%20Thomas%20Eiter%20and%20Tobias%20Geibinger%20and%20Tobias%20Kaminski%20and%20Nysret%20Musliu%20and%20Johannes%20Oetsch%0AAbstract%3A%20%20%20We%20present%20the%20solver%20asp-fzn%20for%20Constraint%20Answer%20Set%20Programming%20%28CASP%29%2C%0Awhich%20extends%20ASP%20with%20linear%20constraints.%20Our%20approach%20is%20based%20on%20translating%0ACASP%20programs%20into%20the%20solver-independent%20FlatZinc%20language%20that%20supports%0Aseveral%20Constraint%20Programming%20and%20Integer%20Programming%20backend%20solvers.%20Our%0Asolver%20supports%20a%20rich%20language%20of%20linear%20constraints%2C%20including%20some%20common%0Aglobal%20constraints.%20As%20for%20evaluation%2C%20we%20show%20that%20asp-fzn%20is%20competitive%20with%0Astate-of-the-art%20ASP%20solvers%20on%20benchmarks%20taken%20from%20past%20ASP%20competitions.%0AFurthermore%2C%20we%20evaluate%20it%20on%20several%20CASP%20problems%20from%20the%20literature%20and%0Acompare%20its%20performance%20with%20clingcon%2C%20which%20is%20a%20prominent%20CASP%20solver%20that%0Asupports%20most%20of%20the%20asp-fzn%20language.%20The%20performance%20of%20asp-fzn%20is%20very%0Apromising%20as%20it%20is%20already%20competitive%20on%20plain%20ASP%20and%20even%20outperforms%0Aclingcon%20on%20some%20CASP%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASP-FZN%253A%2520A%2520Translation-based%2520Constraint%2520Answer%2520Set%2520Solver%26entry.906535625%3DThomas%2520Eiter%2520and%2520Tobias%2520Geibinger%2520and%2520Tobias%2520Kaminski%2520and%2520Nysret%2520Musliu%2520and%2520Johannes%2520Oetsch%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520solver%2520asp-fzn%2520for%2520Constraint%2520Answer%2520Set%2520Programming%2520%2528CASP%2529%252C%250Awhich%2520extends%2520ASP%2520with%2520linear%2520constraints.%2520Our%2520approach%2520is%2520based%2520on%2520translating%250ACASP%2520programs%2520into%2520the%2520solver-independent%2520FlatZinc%2520language%2520that%2520supports%250Aseveral%2520Constraint%2520Programming%2520and%2520Integer%2520Programming%2520backend%2520solvers.%2520Our%250Asolver%2520supports%2520a%2520rich%2520language%2520of%2520linear%2520constraints%252C%2520including%2520some%2520common%250Aglobal%2520constraints.%2520As%2520for%2520evaluation%252C%2520we%2520show%2520that%2520asp-fzn%2520is%2520competitive%2520with%250Astate-of-the-art%2520ASP%2520solvers%2520on%2520benchmarks%2520taken%2520from%2520past%2520ASP%2520competitions.%250AFurthermore%252C%2520we%2520evaluate%2520it%2520on%2520several%2520CASP%2520problems%2520from%2520the%2520literature%2520and%250Acompare%2520its%2520performance%2520with%2520clingcon%252C%2520which%2520is%2520a%2520prominent%2520CASP%2520solver%2520that%250Asupports%2520most%2520of%2520the%2520asp-fzn%2520language.%2520The%2520performance%2520of%2520asp-fzn%2520is%2520very%250Apromising%2520as%2520it%2520is%2520already%2520competitive%2520on%2520plain%2520ASP%2520and%2520even%2520outperforms%250Aclingcon%2520on%2520some%2520CASP%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASP-FZN%3A%20A%20Translation-based%20Constraint%20Answer%20Set%20Solver&entry.906535625=Thomas%20Eiter%20and%20Tobias%20Geibinger%20and%20Tobias%20Kaminski%20and%20Nysret%20Musliu%20and%20Johannes%20Oetsch&entry.1292438233=%20%20We%20present%20the%20solver%20asp-fzn%20for%20Constraint%20Answer%20Set%20Programming%20%28CASP%29%2C%0Awhich%20extends%20ASP%20with%20linear%20constraints.%20Our%20approach%20is%20based%20on%20translating%0ACASP%20programs%20into%20the%20solver-independent%20FlatZinc%20language%20that%20supports%0Aseveral%20Constraint%20Programming%20and%20Integer%20Programming%20backend%20solvers.%20Our%0Asolver%20supports%20a%20rich%20language%20of%20linear%20constraints%2C%20including%20some%20common%0Aglobal%20constraints.%20As%20for%20evaluation%2C%20we%20show%20that%20asp-fzn%20is%20competitive%20with%0Astate-of-the-art%20ASP%20solvers%20on%20benchmarks%20taken%20from%20past%20ASP%20competitions.%0AFurthermore%2C%20we%20evaluate%20it%20on%20several%20CASP%20problems%20from%20the%20literature%20and%0Acompare%20its%20performance%20with%20clingcon%2C%20which%20is%20a%20prominent%20CASP%20solver%20that%0Asupports%20most%20of%20the%20asp-fzn%20language.%20The%20performance%20of%20asp-fzn%20is%20very%0Apromising%20as%20it%20is%20already%20competitive%20on%20plain%20ASP%20and%20even%20outperforms%0Aclingcon%20on%20some%20CASP%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22774v2&entry.124074799=Read"},
{"title": "Modeling the Diachronic Evolution of Legal Norms: An LRMoo-Based,\n  Component-Level, Event-Centric Approach to Legal Knowledge Graphs", "author": "Hudson de Martim", "abstract": "  Effectively representing legal norms for automated processing is a critical\nchallenge, particularly in tracking the temporal evolution of their\nhierarchical components. While foundational conceptual frameworks like IFLA\nLRMoo provide a generic toolkit for bibliographic data, and encoding standards\nlike Akoma Ntoso offer a robust syntax for legal documents, a dedicated, formal\nmodeling pattern for granular, component-level versioning is still required.\nThis limitation hinders the deterministic point-intime reconstruction of legal\ntexts, a fundamental capability for reliable Legal Tech and AI applications.\nThis paper proposes a structured, temporal modeling pattern grounded in the\nLRMoo ontology to address this need. Our approach models the evolution of a\nlegal norm as a diachronic chain of F2 Expressions. We introduce a key\ndistinction between a language-agnostic Temporal Version (TV)-a semantic\nsnapshot of the norm's structure-and its concrete monolingual realizations, the\nLanguage Versions (LV). Both are modeled as F2 Expressions linked by the\ncanonical R76 is derivative of property. This paradigm is applied recursively\nto the legal text's internal structure, representing it as a parallel hierarchy\nof abstract Component Works (F1) and their versioned Component Expressions\n(F2). Furthermore, we formalize the legislative amendment process using the F28\nExpression Creation event, allowing changes to be traced from an amending act\nto its precise effect on the amended norm. Using the Brazilian Federal\nConstitution as a case study, we demonstrate how this event-centric\narchitecture enables the precise, deterministic retrieval and reconstruction of\nany part of a legal text as it existed on a specific date. The model provides a\nrobust foundation for building verifiable knowledge graphs and advanced AI\ntools, overcoming the limitations of current generative models.\n", "link": "http://arxiv.org/abs/2506.07853v3", "date": "2025-09-09", "relevancy": 1.8946, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4767}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4767}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20the%20Diachronic%20Evolution%20of%20Legal%20Norms%3A%20An%20LRMoo-Based%2C%0A%20%20Component-Level%2C%20Event-Centric%20Approach%20to%20Legal%20Knowledge%20Graphs&body=Title%3A%20Modeling%20the%20Diachronic%20Evolution%20of%20Legal%20Norms%3A%20An%20LRMoo-Based%2C%0A%20%20Component-Level%2C%20Event-Centric%20Approach%20to%20Legal%20Knowledge%20Graphs%0AAuthor%3A%20Hudson%20de%20Martim%0AAbstract%3A%20%20%20Effectively%20representing%20legal%20norms%20for%20automated%20processing%20is%20a%20critical%0Achallenge%2C%20particularly%20in%20tracking%20the%20temporal%20evolution%20of%20their%0Ahierarchical%20components.%20While%20foundational%20conceptual%20frameworks%20like%20IFLA%0ALRMoo%20provide%20a%20generic%20toolkit%20for%20bibliographic%20data%2C%20and%20encoding%20standards%0Alike%20Akoma%20Ntoso%20offer%20a%20robust%20syntax%20for%20legal%20documents%2C%20a%20dedicated%2C%20formal%0Amodeling%20pattern%20for%20granular%2C%20component-level%20versioning%20is%20still%20required.%0AThis%20limitation%20hinders%20the%20deterministic%20point-intime%20reconstruction%20of%20legal%0Atexts%2C%20a%20fundamental%20capability%20for%20reliable%20Legal%20Tech%20and%20AI%20applications.%0AThis%20paper%20proposes%20a%20structured%2C%20temporal%20modeling%20pattern%20grounded%20in%20the%0ALRMoo%20ontology%20to%20address%20this%20need.%20Our%20approach%20models%20the%20evolution%20of%20a%0Alegal%20norm%20as%20a%20diachronic%20chain%20of%20F2%20Expressions.%20We%20introduce%20a%20key%0Adistinction%20between%20a%20language-agnostic%20Temporal%20Version%20%28TV%29-a%20semantic%0Asnapshot%20of%20the%20norm%27s%20structure-and%20its%20concrete%20monolingual%20realizations%2C%20the%0ALanguage%20Versions%20%28LV%29.%20Both%20are%20modeled%20as%20F2%20Expressions%20linked%20by%20the%0Acanonical%20R76%20is%20derivative%20of%20property.%20This%20paradigm%20is%20applied%20recursively%0Ato%20the%20legal%20text%27s%20internal%20structure%2C%20representing%20it%20as%20a%20parallel%20hierarchy%0Aof%20abstract%20Component%20Works%20%28F1%29%20and%20their%20versioned%20Component%20Expressions%0A%28F2%29.%20Furthermore%2C%20we%20formalize%20the%20legislative%20amendment%20process%20using%20the%20F28%0AExpression%20Creation%20event%2C%20allowing%20changes%20to%20be%20traced%20from%20an%20amending%20act%0Ato%20its%20precise%20effect%20on%20the%20amended%20norm.%20Using%20the%20Brazilian%20Federal%0AConstitution%20as%20a%20case%20study%2C%20we%20demonstrate%20how%20this%20event-centric%0Aarchitecture%20enables%20the%20precise%2C%20deterministic%20retrieval%20and%20reconstruction%20of%0Aany%20part%20of%20a%20legal%20text%20as%20it%20existed%20on%20a%20specific%20date.%20The%20model%20provides%20a%0Arobust%20foundation%20for%20building%20verifiable%20knowledge%20graphs%20and%20advanced%20AI%0Atools%2C%20overcoming%20the%20limitations%20of%20current%20generative%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07853v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520the%2520Diachronic%2520Evolution%2520of%2520Legal%2520Norms%253A%2520An%2520LRMoo-Based%252C%250A%2520%2520Component-Level%252C%2520Event-Centric%2520Approach%2520to%2520Legal%2520Knowledge%2520Graphs%26entry.906535625%3DHudson%2520de%2520Martim%26entry.1292438233%3D%2520%2520Effectively%2520representing%2520legal%2520norms%2520for%2520automated%2520processing%2520is%2520a%2520critical%250Achallenge%252C%2520particularly%2520in%2520tracking%2520the%2520temporal%2520evolution%2520of%2520their%250Ahierarchical%2520components.%2520While%2520foundational%2520conceptual%2520frameworks%2520like%2520IFLA%250ALRMoo%2520provide%2520a%2520generic%2520toolkit%2520for%2520bibliographic%2520data%252C%2520and%2520encoding%2520standards%250Alike%2520Akoma%2520Ntoso%2520offer%2520a%2520robust%2520syntax%2520for%2520legal%2520documents%252C%2520a%2520dedicated%252C%2520formal%250Amodeling%2520pattern%2520for%2520granular%252C%2520component-level%2520versioning%2520is%2520still%2520required.%250AThis%2520limitation%2520hinders%2520the%2520deterministic%2520point-intime%2520reconstruction%2520of%2520legal%250Atexts%252C%2520a%2520fundamental%2520capability%2520for%2520reliable%2520Legal%2520Tech%2520and%2520AI%2520applications.%250AThis%2520paper%2520proposes%2520a%2520structured%252C%2520temporal%2520modeling%2520pattern%2520grounded%2520in%2520the%250ALRMoo%2520ontology%2520to%2520address%2520this%2520need.%2520Our%2520approach%2520models%2520the%2520evolution%2520of%2520a%250Alegal%2520norm%2520as%2520a%2520diachronic%2520chain%2520of%2520F2%2520Expressions.%2520We%2520introduce%2520a%2520key%250Adistinction%2520between%2520a%2520language-agnostic%2520Temporal%2520Version%2520%2528TV%2529-a%2520semantic%250Asnapshot%2520of%2520the%2520norm%2527s%2520structure-and%2520its%2520concrete%2520monolingual%2520realizations%252C%2520the%250ALanguage%2520Versions%2520%2528LV%2529.%2520Both%2520are%2520modeled%2520as%2520F2%2520Expressions%2520linked%2520by%2520the%250Acanonical%2520R76%2520is%2520derivative%2520of%2520property.%2520This%2520paradigm%2520is%2520applied%2520recursively%250Ato%2520the%2520legal%2520text%2527s%2520internal%2520structure%252C%2520representing%2520it%2520as%2520a%2520parallel%2520hierarchy%250Aof%2520abstract%2520Component%2520Works%2520%2528F1%2529%2520and%2520their%2520versioned%2520Component%2520Expressions%250A%2528F2%2529.%2520Furthermore%252C%2520we%2520formalize%2520the%2520legislative%2520amendment%2520process%2520using%2520the%2520F28%250AExpression%2520Creation%2520event%252C%2520allowing%2520changes%2520to%2520be%2520traced%2520from%2520an%2520amending%2520act%250Ato%2520its%2520precise%2520effect%2520on%2520the%2520amended%2520norm.%2520Using%2520the%2520Brazilian%2520Federal%250AConstitution%2520as%2520a%2520case%2520study%252C%2520we%2520demonstrate%2520how%2520this%2520event-centric%250Aarchitecture%2520enables%2520the%2520precise%252C%2520deterministic%2520retrieval%2520and%2520reconstruction%2520of%250Aany%2520part%2520of%2520a%2520legal%2520text%2520as%2520it%2520existed%2520on%2520a%2520specific%2520date.%2520The%2520model%2520provides%2520a%250Arobust%2520foundation%2520for%2520building%2520verifiable%2520knowledge%2520graphs%2520and%2520advanced%2520AI%250Atools%252C%2520overcoming%2520the%2520limitations%2520of%2520current%2520generative%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07853v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20the%20Diachronic%20Evolution%20of%20Legal%20Norms%3A%20An%20LRMoo-Based%2C%0A%20%20Component-Level%2C%20Event-Centric%20Approach%20to%20Legal%20Knowledge%20Graphs&entry.906535625=Hudson%20de%20Martim&entry.1292438233=%20%20Effectively%20representing%20legal%20norms%20for%20automated%20processing%20is%20a%20critical%0Achallenge%2C%20particularly%20in%20tracking%20the%20temporal%20evolution%20of%20their%0Ahierarchical%20components.%20While%20foundational%20conceptual%20frameworks%20like%20IFLA%0ALRMoo%20provide%20a%20generic%20toolkit%20for%20bibliographic%20data%2C%20and%20encoding%20standards%0Alike%20Akoma%20Ntoso%20offer%20a%20robust%20syntax%20for%20legal%20documents%2C%20a%20dedicated%2C%20formal%0Amodeling%20pattern%20for%20granular%2C%20component-level%20versioning%20is%20still%20required.%0AThis%20limitation%20hinders%20the%20deterministic%20point-intime%20reconstruction%20of%20legal%0Atexts%2C%20a%20fundamental%20capability%20for%20reliable%20Legal%20Tech%20and%20AI%20applications.%0AThis%20paper%20proposes%20a%20structured%2C%20temporal%20modeling%20pattern%20grounded%20in%20the%0ALRMoo%20ontology%20to%20address%20this%20need.%20Our%20approach%20models%20the%20evolution%20of%20a%0Alegal%20norm%20as%20a%20diachronic%20chain%20of%20F2%20Expressions.%20We%20introduce%20a%20key%0Adistinction%20between%20a%20language-agnostic%20Temporal%20Version%20%28TV%29-a%20semantic%0Asnapshot%20of%20the%20norm%27s%20structure-and%20its%20concrete%20monolingual%20realizations%2C%20the%0ALanguage%20Versions%20%28LV%29.%20Both%20are%20modeled%20as%20F2%20Expressions%20linked%20by%20the%0Acanonical%20R76%20is%20derivative%20of%20property.%20This%20paradigm%20is%20applied%20recursively%0Ato%20the%20legal%20text%27s%20internal%20structure%2C%20representing%20it%20as%20a%20parallel%20hierarchy%0Aof%20abstract%20Component%20Works%20%28F1%29%20and%20their%20versioned%20Component%20Expressions%0A%28F2%29.%20Furthermore%2C%20we%20formalize%20the%20legislative%20amendment%20process%20using%20the%20F28%0AExpression%20Creation%20event%2C%20allowing%20changes%20to%20be%20traced%20from%20an%20amending%20act%0Ato%20its%20precise%20effect%20on%20the%20amended%20norm.%20Using%20the%20Brazilian%20Federal%0AConstitution%20as%20a%20case%20study%2C%20we%20demonstrate%20how%20this%20event-centric%0Aarchitecture%20enables%20the%20precise%2C%20deterministic%20retrieval%20and%20reconstruction%20of%0Aany%20part%20of%20a%20legal%20text%20as%20it%20existed%20on%20a%20specific%20date.%20The%20model%20provides%20a%0Arobust%20foundation%20for%20building%20verifiable%20knowledge%20graphs%20and%20advanced%20AI%0Atools%2C%20overcoming%20the%20limitations%20of%20current%20generative%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07853v3&entry.124074799=Read"},
{"title": "Can SSD-Mamba2 Unlock Reinforcement Learning for End-to-End Motion\n  Control?", "author": "Gavin Tao and Yinuo Wang and Jinzhao Zhou", "abstract": "  End-to-end reinforcement learning for motion control promises unified\nperception-action policies that scale across embodiments and tasks, yet most\ndeployed controllers are either blind (proprioception-only) or rely on fusion\nbackbones with unfavorable compute-memory trade-offs. Recurrent controllers\nstruggle with long-horizon credit assignment, and Transformer-based fusion\nincurs quadratic cost in token length, limiting temporal and spatial context.\nWe present a vision-driven cross-modal RL framework built on SSD-Mamba2, a\nselective state-space backbone that applies state-space duality (SSD) to enable\nboth recurrent and convolutional scanning with hardware-aware streaming and\nnear-linear scaling. Proprioceptive states and exteroceptive observations\n(e.g., depth tokens) are encoded into compact tokens and fused by stacked\nSSD-Mamba2 layers. The selective state-space updates retain long-range\ndependencies with markedly lower latency and memory use than quadratic\nself-attention, enabling longer look-ahead, higher token resolution, and stable\ntraining under limited compute. Policies are trained end-to-end under curricula\nthat randomize terrain and appearance and progressively increase scene\ncomplexity. A compact, state-centric reward balances task progress, energy\nefficiency, and safety. Across diverse motion-control scenarios, our approach\nconsistently surpasses strong state-of-the-art baselines in return, safety\n(collisions and falls), and sample efficiency, while converging faster at the\nsame compute budget. These results suggest that SSD-Mamba2 provides a practical\nfusion backbone for scalable, foresightful, and efficient end-to-end motion\ncontrol.\n", "link": "http://arxiv.org/abs/2509.07593v1", "date": "2025-09-09", "relevancy": 1.68, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5926}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.553}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20SSD-Mamba2%20Unlock%20Reinforcement%20Learning%20for%20End-to-End%20Motion%0A%20%20Control%3F&body=Title%3A%20Can%20SSD-Mamba2%20Unlock%20Reinforcement%20Learning%20for%20End-to-End%20Motion%0A%20%20Control%3F%0AAuthor%3A%20Gavin%20Tao%20and%20Yinuo%20Wang%20and%20Jinzhao%20Zhou%0AAbstract%3A%20%20%20End-to-end%20reinforcement%20learning%20for%20motion%20control%20promises%20unified%0Aperception-action%20policies%20that%20scale%20across%20embodiments%20and%20tasks%2C%20yet%20most%0Adeployed%20controllers%20are%20either%20blind%20%28proprioception-only%29%20or%20rely%20on%20fusion%0Abackbones%20with%20unfavorable%20compute-memory%20trade-offs.%20Recurrent%20controllers%0Astruggle%20with%20long-horizon%20credit%20assignment%2C%20and%20Transformer-based%20fusion%0Aincurs%20quadratic%20cost%20in%20token%20length%2C%20limiting%20temporal%20and%20spatial%20context.%0AWe%20present%20a%20vision-driven%20cross-modal%20RL%20framework%20built%20on%20SSD-Mamba2%2C%20a%0Aselective%20state-space%20backbone%20that%20applies%20state-space%20duality%20%28SSD%29%20to%20enable%0Aboth%20recurrent%20and%20convolutional%20scanning%20with%20hardware-aware%20streaming%20and%0Anear-linear%20scaling.%20Proprioceptive%20states%20and%20exteroceptive%20observations%0A%28e.g.%2C%20depth%20tokens%29%20are%20encoded%20into%20compact%20tokens%20and%20fused%20by%20stacked%0ASSD-Mamba2%20layers.%20The%20selective%20state-space%20updates%20retain%20long-range%0Adependencies%20with%20markedly%20lower%20latency%20and%20memory%20use%20than%20quadratic%0Aself-attention%2C%20enabling%20longer%20look-ahead%2C%20higher%20token%20resolution%2C%20and%20stable%0Atraining%20under%20limited%20compute.%20Policies%20are%20trained%20end-to-end%20under%20curricula%0Athat%20randomize%20terrain%20and%20appearance%20and%20progressively%20increase%20scene%0Acomplexity.%20A%20compact%2C%20state-centric%20reward%20balances%20task%20progress%2C%20energy%0Aefficiency%2C%20and%20safety.%20Across%20diverse%20motion-control%20scenarios%2C%20our%20approach%0Aconsistently%20surpasses%20strong%20state-of-the-art%20baselines%20in%20return%2C%20safety%0A%28collisions%20and%20falls%29%2C%20and%20sample%20efficiency%2C%20while%20converging%20faster%20at%20the%0Asame%20compute%20budget.%20These%20results%20suggest%20that%20SSD-Mamba2%20provides%20a%20practical%0Afusion%20backbone%20for%20scalable%2C%20foresightful%2C%20and%20efficient%20end-to-end%20motion%0Acontrol.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520SSD-Mamba2%2520Unlock%2520Reinforcement%2520Learning%2520for%2520End-to-End%2520Motion%250A%2520%2520Control%253F%26entry.906535625%3DGavin%2520Tao%2520and%2520Yinuo%2520Wang%2520and%2520Jinzhao%2520Zhou%26entry.1292438233%3D%2520%2520End-to-end%2520reinforcement%2520learning%2520for%2520motion%2520control%2520promises%2520unified%250Aperception-action%2520policies%2520that%2520scale%2520across%2520embodiments%2520and%2520tasks%252C%2520yet%2520most%250Adeployed%2520controllers%2520are%2520either%2520blind%2520%2528proprioception-only%2529%2520or%2520rely%2520on%2520fusion%250Abackbones%2520with%2520unfavorable%2520compute-memory%2520trade-offs.%2520Recurrent%2520controllers%250Astruggle%2520with%2520long-horizon%2520credit%2520assignment%252C%2520and%2520Transformer-based%2520fusion%250Aincurs%2520quadratic%2520cost%2520in%2520token%2520length%252C%2520limiting%2520temporal%2520and%2520spatial%2520context.%250AWe%2520present%2520a%2520vision-driven%2520cross-modal%2520RL%2520framework%2520built%2520on%2520SSD-Mamba2%252C%2520a%250Aselective%2520state-space%2520backbone%2520that%2520applies%2520state-space%2520duality%2520%2528SSD%2529%2520to%2520enable%250Aboth%2520recurrent%2520and%2520convolutional%2520scanning%2520with%2520hardware-aware%2520streaming%2520and%250Anear-linear%2520scaling.%2520Proprioceptive%2520states%2520and%2520exteroceptive%2520observations%250A%2528e.g.%252C%2520depth%2520tokens%2529%2520are%2520encoded%2520into%2520compact%2520tokens%2520and%2520fused%2520by%2520stacked%250ASSD-Mamba2%2520layers.%2520The%2520selective%2520state-space%2520updates%2520retain%2520long-range%250Adependencies%2520with%2520markedly%2520lower%2520latency%2520and%2520memory%2520use%2520than%2520quadratic%250Aself-attention%252C%2520enabling%2520longer%2520look-ahead%252C%2520higher%2520token%2520resolution%252C%2520and%2520stable%250Atraining%2520under%2520limited%2520compute.%2520Policies%2520are%2520trained%2520end-to-end%2520under%2520curricula%250Athat%2520randomize%2520terrain%2520and%2520appearance%2520and%2520progressively%2520increase%2520scene%250Acomplexity.%2520A%2520compact%252C%2520state-centric%2520reward%2520balances%2520task%2520progress%252C%2520energy%250Aefficiency%252C%2520and%2520safety.%2520Across%2520diverse%2520motion-control%2520scenarios%252C%2520our%2520approach%250Aconsistently%2520surpasses%2520strong%2520state-of-the-art%2520baselines%2520in%2520return%252C%2520safety%250A%2528collisions%2520and%2520falls%2529%252C%2520and%2520sample%2520efficiency%252C%2520while%2520converging%2520faster%2520at%2520the%250Asame%2520compute%2520budget.%2520These%2520results%2520suggest%2520that%2520SSD-Mamba2%2520provides%2520a%2520practical%250Afusion%2520backbone%2520for%2520scalable%252C%2520foresightful%252C%2520and%2520efficient%2520end-to-end%2520motion%250Acontrol.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20SSD-Mamba2%20Unlock%20Reinforcement%20Learning%20for%20End-to-End%20Motion%0A%20%20Control%3F&entry.906535625=Gavin%20Tao%20and%20Yinuo%20Wang%20and%20Jinzhao%20Zhou&entry.1292438233=%20%20End-to-end%20reinforcement%20learning%20for%20motion%20control%20promises%20unified%0Aperception-action%20policies%20that%20scale%20across%20embodiments%20and%20tasks%2C%20yet%20most%0Adeployed%20controllers%20are%20either%20blind%20%28proprioception-only%29%20or%20rely%20on%20fusion%0Abackbones%20with%20unfavorable%20compute-memory%20trade-offs.%20Recurrent%20controllers%0Astruggle%20with%20long-horizon%20credit%20assignment%2C%20and%20Transformer-based%20fusion%0Aincurs%20quadratic%20cost%20in%20token%20length%2C%20limiting%20temporal%20and%20spatial%20context.%0AWe%20present%20a%20vision-driven%20cross-modal%20RL%20framework%20built%20on%20SSD-Mamba2%2C%20a%0Aselective%20state-space%20backbone%20that%20applies%20state-space%20duality%20%28SSD%29%20to%20enable%0Aboth%20recurrent%20and%20convolutional%20scanning%20with%20hardware-aware%20streaming%20and%0Anear-linear%20scaling.%20Proprioceptive%20states%20and%20exteroceptive%20observations%0A%28e.g.%2C%20depth%20tokens%29%20are%20encoded%20into%20compact%20tokens%20and%20fused%20by%20stacked%0ASSD-Mamba2%20layers.%20The%20selective%20state-space%20updates%20retain%20long-range%0Adependencies%20with%20markedly%20lower%20latency%20and%20memory%20use%20than%20quadratic%0Aself-attention%2C%20enabling%20longer%20look-ahead%2C%20higher%20token%20resolution%2C%20and%20stable%0Atraining%20under%20limited%20compute.%20Policies%20are%20trained%20end-to-end%20under%20curricula%0Athat%20randomize%20terrain%20and%20appearance%20and%20progressively%20increase%20scene%0Acomplexity.%20A%20compact%2C%20state-centric%20reward%20balances%20task%20progress%2C%20energy%0Aefficiency%2C%20and%20safety.%20Across%20diverse%20motion-control%20scenarios%2C%20our%20approach%0Aconsistently%20surpasses%20strong%20state-of-the-art%20baselines%20in%20return%2C%20safety%0A%28collisions%20and%20falls%29%2C%20and%20sample%20efficiency%2C%20while%20converging%20faster%20at%20the%0Asame%20compute%20budget.%20These%20results%20suggest%20that%20SSD-Mamba2%20provides%20a%20practical%0Afusion%20backbone%20for%20scalable%2C%20foresightful%2C%20and%20efficient%20end-to-end%20motion%0Acontrol.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07593v1&entry.124074799=Read"},
{"title": "RaC: Robot Learning for Long-Horizon Tasks by Scaling Recovery and\n  Correction", "author": "Zheyuan Hu and Robyn Wu and Naveen Enock and Jasmine Li and Riya Kadakia and Zackory Erickson and Aviral Kumar", "abstract": "  Modern paradigms for robot imitation train expressive policy architectures on\nlarge amounts of human demonstration data. Yet performance on contact-rich,\ndeformable-object, and long-horizon tasks plateau far below perfect execution,\neven with thousands of expert demonstrations. This is due to the inefficiency\nof existing ``expert'' data collection procedures based on human teleoperation.\nTo address this issue, we introduce RaC, a new phase of training on\nhuman-in-the-loop rollouts after imitation learning pre-training. In RaC, we\nfine-tune a robotic policy on human intervention trajectories that illustrate\nrecovery and correction behaviors. Specifically, during a policy rollout, human\noperators intervene when failure appears imminent, first rewinding the robot\nback to a familiar, in-distribution state and then providing a corrective\nsegment that completes the current sub-task. Training on this data composition\nexpands the robotic skill repertoire to include retry and adaptation behaviors,\nwhich we show are crucial for boosting both efficiency and robustness on\nlong-horizon tasks. Across three real-world bimanual control tasks: shirt\nhanging, airtight container lid sealing, takeout box packing, and a simulated\nassembly task, RaC outperforms the prior state-of-the-art using 10$\\times$ less\ndata collection time and samples. We also show that RaC enables test-time\nscaling: the performance of the trained RaC policy scales linearly in the\nnumber of recovery maneuvers it exhibits. Videos of the learned policy are\navailable at https://rac-scaling-robot.github.io/.\n", "link": "http://arxiv.org/abs/2509.07953v1", "date": "2025-09-09", "relevancy": 1.6631, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5787}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RaC%3A%20Robot%20Learning%20for%20Long-Horizon%20Tasks%20by%20Scaling%20Recovery%20and%0A%20%20Correction&body=Title%3A%20RaC%3A%20Robot%20Learning%20for%20Long-Horizon%20Tasks%20by%20Scaling%20Recovery%20and%0A%20%20Correction%0AAuthor%3A%20Zheyuan%20Hu%20and%20Robyn%20Wu%20and%20Naveen%20Enock%20and%20Jasmine%20Li%20and%20Riya%20Kadakia%20and%20Zackory%20Erickson%20and%20Aviral%20Kumar%0AAbstract%3A%20%20%20Modern%20paradigms%20for%20robot%20imitation%20train%20expressive%20policy%20architectures%20on%0Alarge%20amounts%20of%20human%20demonstration%20data.%20Yet%20performance%20on%20contact-rich%2C%0Adeformable-object%2C%20and%20long-horizon%20tasks%20plateau%20far%20below%20perfect%20execution%2C%0Aeven%20with%20thousands%20of%20expert%20demonstrations.%20This%20is%20due%20to%20the%20inefficiency%0Aof%20existing%20%60%60expert%27%27%20data%20collection%20procedures%20based%20on%20human%20teleoperation.%0ATo%20address%20this%20issue%2C%20we%20introduce%20RaC%2C%20a%20new%20phase%20of%20training%20on%0Ahuman-in-the-loop%20rollouts%20after%20imitation%20learning%20pre-training.%20In%20RaC%2C%20we%0Afine-tune%20a%20robotic%20policy%20on%20human%20intervention%20trajectories%20that%20illustrate%0Arecovery%20and%20correction%20behaviors.%20Specifically%2C%20during%20a%20policy%20rollout%2C%20human%0Aoperators%20intervene%20when%20failure%20appears%20imminent%2C%20first%20rewinding%20the%20robot%0Aback%20to%20a%20familiar%2C%20in-distribution%20state%20and%20then%20providing%20a%20corrective%0Asegment%20that%20completes%20the%20current%20sub-task.%20Training%20on%20this%20data%20composition%0Aexpands%20the%20robotic%20skill%20repertoire%20to%20include%20retry%20and%20adaptation%20behaviors%2C%0Awhich%20we%20show%20are%20crucial%20for%20boosting%20both%20efficiency%20and%20robustness%20on%0Along-horizon%20tasks.%20Across%20three%20real-world%20bimanual%20control%20tasks%3A%20shirt%0Ahanging%2C%20airtight%20container%20lid%20sealing%2C%20takeout%20box%20packing%2C%20and%20a%20simulated%0Aassembly%20task%2C%20RaC%20outperforms%20the%20prior%20state-of-the-art%20using%2010%24%5Ctimes%24%20less%0Adata%20collection%20time%20and%20samples.%20We%20also%20show%20that%20RaC%20enables%20test-time%0Ascaling%3A%20the%20performance%20of%20the%20trained%20RaC%20policy%20scales%20linearly%20in%20the%0Anumber%20of%20recovery%20maneuvers%20it%20exhibits.%20Videos%20of%20the%20learned%20policy%20are%0Aavailable%20at%20https%3A//rac-scaling-robot.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaC%253A%2520Robot%2520Learning%2520for%2520Long-Horizon%2520Tasks%2520by%2520Scaling%2520Recovery%2520and%250A%2520%2520Correction%26entry.906535625%3DZheyuan%2520Hu%2520and%2520Robyn%2520Wu%2520and%2520Naveen%2520Enock%2520and%2520Jasmine%2520Li%2520and%2520Riya%2520Kadakia%2520and%2520Zackory%2520Erickson%2520and%2520Aviral%2520Kumar%26entry.1292438233%3D%2520%2520Modern%2520paradigms%2520for%2520robot%2520imitation%2520train%2520expressive%2520policy%2520architectures%2520on%250Alarge%2520amounts%2520of%2520human%2520demonstration%2520data.%2520Yet%2520performance%2520on%2520contact-rich%252C%250Adeformable-object%252C%2520and%2520long-horizon%2520tasks%2520plateau%2520far%2520below%2520perfect%2520execution%252C%250Aeven%2520with%2520thousands%2520of%2520expert%2520demonstrations.%2520This%2520is%2520due%2520to%2520the%2520inefficiency%250Aof%2520existing%2520%2560%2560expert%2527%2527%2520data%2520collection%2520procedures%2520based%2520on%2520human%2520teleoperation.%250ATo%2520address%2520this%2520issue%252C%2520we%2520introduce%2520RaC%252C%2520a%2520new%2520phase%2520of%2520training%2520on%250Ahuman-in-the-loop%2520rollouts%2520after%2520imitation%2520learning%2520pre-training.%2520In%2520RaC%252C%2520we%250Afine-tune%2520a%2520robotic%2520policy%2520on%2520human%2520intervention%2520trajectories%2520that%2520illustrate%250Arecovery%2520and%2520correction%2520behaviors.%2520Specifically%252C%2520during%2520a%2520policy%2520rollout%252C%2520human%250Aoperators%2520intervene%2520when%2520failure%2520appears%2520imminent%252C%2520first%2520rewinding%2520the%2520robot%250Aback%2520to%2520a%2520familiar%252C%2520in-distribution%2520state%2520and%2520then%2520providing%2520a%2520corrective%250Asegment%2520that%2520completes%2520the%2520current%2520sub-task.%2520Training%2520on%2520this%2520data%2520composition%250Aexpands%2520the%2520robotic%2520skill%2520repertoire%2520to%2520include%2520retry%2520and%2520adaptation%2520behaviors%252C%250Awhich%2520we%2520show%2520are%2520crucial%2520for%2520boosting%2520both%2520efficiency%2520and%2520robustness%2520on%250Along-horizon%2520tasks.%2520Across%2520three%2520real-world%2520bimanual%2520control%2520tasks%253A%2520shirt%250Ahanging%252C%2520airtight%2520container%2520lid%2520sealing%252C%2520takeout%2520box%2520packing%252C%2520and%2520a%2520simulated%250Aassembly%2520task%252C%2520RaC%2520outperforms%2520the%2520prior%2520state-of-the-art%2520using%252010%2524%255Ctimes%2524%2520less%250Adata%2520collection%2520time%2520and%2520samples.%2520We%2520also%2520show%2520that%2520RaC%2520enables%2520test-time%250Ascaling%253A%2520the%2520performance%2520of%2520the%2520trained%2520RaC%2520policy%2520scales%2520linearly%2520in%2520the%250Anumber%2520of%2520recovery%2520maneuvers%2520it%2520exhibits.%2520Videos%2520of%2520the%2520learned%2520policy%2520are%250Aavailable%2520at%2520https%253A//rac-scaling-robot.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaC%3A%20Robot%20Learning%20for%20Long-Horizon%20Tasks%20by%20Scaling%20Recovery%20and%0A%20%20Correction&entry.906535625=Zheyuan%20Hu%20and%20Robyn%20Wu%20and%20Naveen%20Enock%20and%20Jasmine%20Li%20and%20Riya%20Kadakia%20and%20Zackory%20Erickson%20and%20Aviral%20Kumar&entry.1292438233=%20%20Modern%20paradigms%20for%20robot%20imitation%20train%20expressive%20policy%20architectures%20on%0Alarge%20amounts%20of%20human%20demonstration%20data.%20Yet%20performance%20on%20contact-rich%2C%0Adeformable-object%2C%20and%20long-horizon%20tasks%20plateau%20far%20below%20perfect%20execution%2C%0Aeven%20with%20thousands%20of%20expert%20demonstrations.%20This%20is%20due%20to%20the%20inefficiency%0Aof%20existing%20%60%60expert%27%27%20data%20collection%20procedures%20based%20on%20human%20teleoperation.%0ATo%20address%20this%20issue%2C%20we%20introduce%20RaC%2C%20a%20new%20phase%20of%20training%20on%0Ahuman-in-the-loop%20rollouts%20after%20imitation%20learning%20pre-training.%20In%20RaC%2C%20we%0Afine-tune%20a%20robotic%20policy%20on%20human%20intervention%20trajectories%20that%20illustrate%0Arecovery%20and%20correction%20behaviors.%20Specifically%2C%20during%20a%20policy%20rollout%2C%20human%0Aoperators%20intervene%20when%20failure%20appears%20imminent%2C%20first%20rewinding%20the%20robot%0Aback%20to%20a%20familiar%2C%20in-distribution%20state%20and%20then%20providing%20a%20corrective%0Asegment%20that%20completes%20the%20current%20sub-task.%20Training%20on%20this%20data%20composition%0Aexpands%20the%20robotic%20skill%20repertoire%20to%20include%20retry%20and%20adaptation%20behaviors%2C%0Awhich%20we%20show%20are%20crucial%20for%20boosting%20both%20efficiency%20and%20robustness%20on%0Along-horizon%20tasks.%20Across%20three%20real-world%20bimanual%20control%20tasks%3A%20shirt%0Ahanging%2C%20airtight%20container%20lid%20sealing%2C%20takeout%20box%20packing%2C%20and%20a%20simulated%0Aassembly%20task%2C%20RaC%20outperforms%20the%20prior%20state-of-the-art%20using%2010%24%5Ctimes%24%20less%0Adata%20collection%20time%20and%20samples.%20We%20also%20show%20that%20RaC%20enables%20test-time%0Ascaling%3A%20the%20performance%20of%20the%20trained%20RaC%20policy%20scales%20linearly%20in%20the%0Anumber%20of%20recovery%20maneuvers%20it%20exhibits.%20Videos%20of%20the%20learned%20policy%20are%0Aavailable%20at%20https%3A//rac-scaling-robot.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07953v1&entry.124074799=Read"},
{"title": "A Decade of Wheat Mapping for Lebanon", "author": "Hasan Wehbi and Hasan Nasrallah and Mohamad Hasan Zahweh and Zeinab Takach and Veera Ganesh Yalla and Ali J. Ghandour", "abstract": "  Wheat accounts for approximately 20% of the world's caloric intake, making it\na vital component of global food security. Given this importance, mapping wheat\nfields plays a crucial role in enabling various stakeholders, including policy\nmakers, researchers, and agricultural organizations, to make informed decisions\nregarding food security, supply chain management, and resource allocation. In\nthis paper, we tackle the problem of accurately mapping wheat fields out of\nsatellite images by introducing an improved pipeline for winter wheat\nsegmentation, as well as presenting a case study on a decade-long analysis of\nwheat mapping in Lebanon. We integrate a Temporal Spatial Vision Transformer\n(TSViT) with Parameter-Efficient Fine Tuning (PEFT) and a novel post-processing\npipeline based on the Fields of The World (FTW) framework. Our proposed\npipeline addresses key challenges encountered in existing approaches, such as\nthe clustering of small agricultural parcels in a single large field. By\nmerging wheat segmentation with precise field boundary extraction, our method\nproduces geometrically coherent and semantically rich maps that enable us to\nperform in-depth analysis such as tracking crop rotation pattern over years.\nExtensive evaluations demonstrate improved boundary delineation and field-level\nprecision, establishing the potential of the proposed framework in operational\nagricultural monitoring and historical trend analysis. By allowing for accurate\nmapping of wheat fields, this work lays the foundation for a range of critical\nstudies and future advances, including crop monitoring and yield estimation.\n", "link": "http://arxiv.org/abs/2504.11366v3", "date": "2025-09-09", "relevancy": 1.8095, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.491}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4274}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Decade%20of%20Wheat%20Mapping%20for%20Lebanon&body=Title%3A%20A%20Decade%20of%20Wheat%20Mapping%20for%20Lebanon%0AAuthor%3A%20Hasan%20Wehbi%20and%20Hasan%20Nasrallah%20and%20Mohamad%20Hasan%20Zahweh%20and%20Zeinab%20Takach%20and%20Veera%20Ganesh%20Yalla%20and%20Ali%20J.%20Ghandour%0AAbstract%3A%20%20%20Wheat%20accounts%20for%20approximately%2020%25%20of%20the%20world%27s%20caloric%20intake%2C%20making%20it%0Aa%20vital%20component%20of%20global%20food%20security.%20Given%20this%20importance%2C%20mapping%20wheat%0Afields%20plays%20a%20crucial%20role%20in%20enabling%20various%20stakeholders%2C%20including%20policy%0Amakers%2C%20researchers%2C%20and%20agricultural%20organizations%2C%20to%20make%20informed%20decisions%0Aregarding%20food%20security%2C%20supply%20chain%20management%2C%20and%20resource%20allocation.%20In%0Athis%20paper%2C%20we%20tackle%20the%20problem%20of%20accurately%20mapping%20wheat%20fields%20out%20of%0Asatellite%20images%20by%20introducing%20an%20improved%20pipeline%20for%20winter%20wheat%0Asegmentation%2C%20as%20well%20as%20presenting%20a%20case%20study%20on%20a%20decade-long%20analysis%20of%0Awheat%20mapping%20in%20Lebanon.%20We%20integrate%20a%20Temporal%20Spatial%20Vision%20Transformer%0A%28TSViT%29%20with%20Parameter-Efficient%20Fine%20Tuning%20%28PEFT%29%20and%20a%20novel%20post-processing%0Apipeline%20based%20on%20the%20Fields%20of%20The%20World%20%28FTW%29%20framework.%20Our%20proposed%0Apipeline%20addresses%20key%20challenges%20encountered%20in%20existing%20approaches%2C%20such%20as%0Athe%20clustering%20of%20small%20agricultural%20parcels%20in%20a%20single%20large%20field.%20By%0Amerging%20wheat%20segmentation%20with%20precise%20field%20boundary%20extraction%2C%20our%20method%0Aproduces%20geometrically%20coherent%20and%20semantically%20rich%20maps%20that%20enable%20us%20to%0Aperform%20in-depth%20analysis%20such%20as%20tracking%20crop%20rotation%20pattern%20over%20years.%0AExtensive%20evaluations%20demonstrate%20improved%20boundary%20delineation%20and%20field-level%0Aprecision%2C%20establishing%20the%20potential%20of%20the%20proposed%20framework%20in%20operational%0Aagricultural%20monitoring%20and%20historical%20trend%20analysis.%20By%20allowing%20for%20accurate%0Amapping%20of%20wheat%20fields%2C%20this%20work%20lays%20the%20foundation%20for%20a%20range%20of%20critical%0Astudies%20and%20future%20advances%2C%20including%20crop%20monitoring%20and%20yield%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11366v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Decade%2520of%2520Wheat%2520Mapping%2520for%2520Lebanon%26entry.906535625%3DHasan%2520Wehbi%2520and%2520Hasan%2520Nasrallah%2520and%2520Mohamad%2520Hasan%2520Zahweh%2520and%2520Zeinab%2520Takach%2520and%2520Veera%2520Ganesh%2520Yalla%2520and%2520Ali%2520J.%2520Ghandour%26entry.1292438233%3D%2520%2520Wheat%2520accounts%2520for%2520approximately%252020%2525%2520of%2520the%2520world%2527s%2520caloric%2520intake%252C%2520making%2520it%250Aa%2520vital%2520component%2520of%2520global%2520food%2520security.%2520Given%2520this%2520importance%252C%2520mapping%2520wheat%250Afields%2520plays%2520a%2520crucial%2520role%2520in%2520enabling%2520various%2520stakeholders%252C%2520including%2520policy%250Amakers%252C%2520researchers%252C%2520and%2520agricultural%2520organizations%252C%2520to%2520make%2520informed%2520decisions%250Aregarding%2520food%2520security%252C%2520supply%2520chain%2520management%252C%2520and%2520resource%2520allocation.%2520In%250Athis%2520paper%252C%2520we%2520tackle%2520the%2520problem%2520of%2520accurately%2520mapping%2520wheat%2520fields%2520out%2520of%250Asatellite%2520images%2520by%2520introducing%2520an%2520improved%2520pipeline%2520for%2520winter%2520wheat%250Asegmentation%252C%2520as%2520well%2520as%2520presenting%2520a%2520case%2520study%2520on%2520a%2520decade-long%2520analysis%2520of%250Awheat%2520mapping%2520in%2520Lebanon.%2520We%2520integrate%2520a%2520Temporal%2520Spatial%2520Vision%2520Transformer%250A%2528TSViT%2529%2520with%2520Parameter-Efficient%2520Fine%2520Tuning%2520%2528PEFT%2529%2520and%2520a%2520novel%2520post-processing%250Apipeline%2520based%2520on%2520the%2520Fields%2520of%2520The%2520World%2520%2528FTW%2529%2520framework.%2520Our%2520proposed%250Apipeline%2520addresses%2520key%2520challenges%2520encountered%2520in%2520existing%2520approaches%252C%2520such%2520as%250Athe%2520clustering%2520of%2520small%2520agricultural%2520parcels%2520in%2520a%2520single%2520large%2520field.%2520By%250Amerging%2520wheat%2520segmentation%2520with%2520precise%2520field%2520boundary%2520extraction%252C%2520our%2520method%250Aproduces%2520geometrically%2520coherent%2520and%2520semantically%2520rich%2520maps%2520that%2520enable%2520us%2520to%250Aperform%2520in-depth%2520analysis%2520such%2520as%2520tracking%2520crop%2520rotation%2520pattern%2520over%2520years.%250AExtensive%2520evaluations%2520demonstrate%2520improved%2520boundary%2520delineation%2520and%2520field-level%250Aprecision%252C%2520establishing%2520the%2520potential%2520of%2520the%2520proposed%2520framework%2520in%2520operational%250Aagricultural%2520monitoring%2520and%2520historical%2520trend%2520analysis.%2520By%2520allowing%2520for%2520accurate%250Amapping%2520of%2520wheat%2520fields%252C%2520this%2520work%2520lays%2520the%2520foundation%2520for%2520a%2520range%2520of%2520critical%250Astudies%2520and%2520future%2520advances%252C%2520including%2520crop%2520monitoring%2520and%2520yield%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11366v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Decade%20of%20Wheat%20Mapping%20for%20Lebanon&entry.906535625=Hasan%20Wehbi%20and%20Hasan%20Nasrallah%20and%20Mohamad%20Hasan%20Zahweh%20and%20Zeinab%20Takach%20and%20Veera%20Ganesh%20Yalla%20and%20Ali%20J.%20Ghandour&entry.1292438233=%20%20Wheat%20accounts%20for%20approximately%2020%25%20of%20the%20world%27s%20caloric%20intake%2C%20making%20it%0Aa%20vital%20component%20of%20global%20food%20security.%20Given%20this%20importance%2C%20mapping%20wheat%0Afields%20plays%20a%20crucial%20role%20in%20enabling%20various%20stakeholders%2C%20including%20policy%0Amakers%2C%20researchers%2C%20and%20agricultural%20organizations%2C%20to%20make%20informed%20decisions%0Aregarding%20food%20security%2C%20supply%20chain%20management%2C%20and%20resource%20allocation.%20In%0Athis%20paper%2C%20we%20tackle%20the%20problem%20of%20accurately%20mapping%20wheat%20fields%20out%20of%0Asatellite%20images%20by%20introducing%20an%20improved%20pipeline%20for%20winter%20wheat%0Asegmentation%2C%20as%20well%20as%20presenting%20a%20case%20study%20on%20a%20decade-long%20analysis%20of%0Awheat%20mapping%20in%20Lebanon.%20We%20integrate%20a%20Temporal%20Spatial%20Vision%20Transformer%0A%28TSViT%29%20with%20Parameter-Efficient%20Fine%20Tuning%20%28PEFT%29%20and%20a%20novel%20post-processing%0Apipeline%20based%20on%20the%20Fields%20of%20The%20World%20%28FTW%29%20framework.%20Our%20proposed%0Apipeline%20addresses%20key%20challenges%20encountered%20in%20existing%20approaches%2C%20such%20as%0Athe%20clustering%20of%20small%20agricultural%20parcels%20in%20a%20single%20large%20field.%20By%0Amerging%20wheat%20segmentation%20with%20precise%20field%20boundary%20extraction%2C%20our%20method%0Aproduces%20geometrically%20coherent%20and%20semantically%20rich%20maps%20that%20enable%20us%20to%0Aperform%20in-depth%20analysis%20such%20as%20tracking%20crop%20rotation%20pattern%20over%20years.%0AExtensive%20evaluations%20demonstrate%20improved%20boundary%20delineation%20and%20field-level%0Aprecision%2C%20establishing%20the%20potential%20of%20the%20proposed%20framework%20in%20operational%0Aagricultural%20monitoring%20and%20historical%20trend%20analysis.%20By%20allowing%20for%20accurate%0Amapping%20of%20wheat%20fields%2C%20this%20work%20lays%20the%20foundation%20for%20a%20range%20of%20critical%0Astudies%20and%20future%20advances%2C%20including%20crop%20monitoring%20and%20yield%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11366v3&entry.124074799=Read"},
{"title": "SPACE-iT: Spatial-Aware Curriculum Exploration and Feedback-Driven\n  Adaptive Augmentation for Vision Transformer Distillation", "author": "Jihyeon Seong and Hyunkyung Han", "abstract": "  Knowledge distillation (KD) has proven to be a powerful technique for\nimproving the performance of Vision Transformers (ViTs). However, traditional\nKD methods often treat all image patches uniformly, overlooking spatial\nvariations in learning difficulty. To address this limitation, we propose\nSPACE-iT, a novel framework for Spatial-Aware Curriculum Exploration via\nFeedback-Driven Adaptive Augmentation. At its core, SPACE-iT computes spatial\nconfidence scores at the attention, patch, and logit levels. This confidence\nmap supports a two-fold strategy: (1) dynamically modulating the distillation\nloss, and (2) guiding an adaptive augmentation module that intensifies reverse\ncurriculum learning. By establishing a feedback-driven reverse curriculum that\ninitially exposes students to challenging regions-progressing from hard to\neasy-SPACE-iT enables more effective learning of complex spatial patterns and\nachieves superior performance over vanilla distillation, without introducing\nadditional memory overhead.\n", "link": "http://arxiv.org/abs/2506.10582v2", "date": "2025-09-09", "relevancy": 1.6972, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5811}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5473}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPACE-iT%3A%20Spatial-Aware%20Curriculum%20Exploration%20and%20Feedback-Driven%0A%20%20Adaptive%20Augmentation%20for%20Vision%20Transformer%20Distillation&body=Title%3A%20SPACE-iT%3A%20Spatial-Aware%20Curriculum%20Exploration%20and%20Feedback-Driven%0A%20%20Adaptive%20Augmentation%20for%20Vision%20Transformer%20Distillation%0AAuthor%3A%20Jihyeon%20Seong%20and%20Hyunkyung%20Han%0AAbstract%3A%20%20%20Knowledge%20distillation%20%28KD%29%20has%20proven%20to%20be%20a%20powerful%20technique%20for%0Aimproving%20the%20performance%20of%20Vision%20Transformers%20%28ViTs%29.%20However%2C%20traditional%0AKD%20methods%20often%20treat%20all%20image%20patches%20uniformly%2C%20overlooking%20spatial%0Avariations%20in%20learning%20difficulty.%20To%20address%20this%20limitation%2C%20we%20propose%0ASPACE-iT%2C%20a%20novel%20framework%20for%20Spatial-Aware%20Curriculum%20Exploration%20via%0AFeedback-Driven%20Adaptive%20Augmentation.%20At%20its%20core%2C%20SPACE-iT%20computes%20spatial%0Aconfidence%20scores%20at%20the%20attention%2C%20patch%2C%20and%20logit%20levels.%20This%20confidence%0Amap%20supports%20a%20two-fold%20strategy%3A%20%281%29%20dynamically%20modulating%20the%20distillation%0Aloss%2C%20and%20%282%29%20guiding%20an%20adaptive%20augmentation%20module%20that%20intensifies%20reverse%0Acurriculum%20learning.%20By%20establishing%20a%20feedback-driven%20reverse%20curriculum%20that%0Ainitially%20exposes%20students%20to%20challenging%20regions-progressing%20from%20hard%20to%0Aeasy-SPACE-iT%20enables%20more%20effective%20learning%20of%20complex%20spatial%20patterns%20and%0Aachieves%20superior%20performance%20over%20vanilla%20distillation%2C%20without%20introducing%0Aadditional%20memory%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10582v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPACE-iT%253A%2520Spatial-Aware%2520Curriculum%2520Exploration%2520and%2520Feedback-Driven%250A%2520%2520Adaptive%2520Augmentation%2520for%2520Vision%2520Transformer%2520Distillation%26entry.906535625%3DJihyeon%2520Seong%2520and%2520Hyunkyung%2520Han%26entry.1292438233%3D%2520%2520Knowledge%2520distillation%2520%2528KD%2529%2520has%2520proven%2520to%2520be%2520a%2520powerful%2520technique%2520for%250Aimproving%2520the%2520performance%2520of%2520Vision%2520Transformers%2520%2528ViTs%2529.%2520However%252C%2520traditional%250AKD%2520methods%2520often%2520treat%2520all%2520image%2520patches%2520uniformly%252C%2520overlooking%2520spatial%250Avariations%2520in%2520learning%2520difficulty.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250ASPACE-iT%252C%2520a%2520novel%2520framework%2520for%2520Spatial-Aware%2520Curriculum%2520Exploration%2520via%250AFeedback-Driven%2520Adaptive%2520Augmentation.%2520At%2520its%2520core%252C%2520SPACE-iT%2520computes%2520spatial%250Aconfidence%2520scores%2520at%2520the%2520attention%252C%2520patch%252C%2520and%2520logit%2520levels.%2520This%2520confidence%250Amap%2520supports%2520a%2520two-fold%2520strategy%253A%2520%25281%2529%2520dynamically%2520modulating%2520the%2520distillation%250Aloss%252C%2520and%2520%25282%2529%2520guiding%2520an%2520adaptive%2520augmentation%2520module%2520that%2520intensifies%2520reverse%250Acurriculum%2520learning.%2520By%2520establishing%2520a%2520feedback-driven%2520reverse%2520curriculum%2520that%250Ainitially%2520exposes%2520students%2520to%2520challenging%2520regions-progressing%2520from%2520hard%2520to%250Aeasy-SPACE-iT%2520enables%2520more%2520effective%2520learning%2520of%2520complex%2520spatial%2520patterns%2520and%250Aachieves%2520superior%2520performance%2520over%2520vanilla%2520distillation%252C%2520without%2520introducing%250Aadditional%2520memory%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10582v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPACE-iT%3A%20Spatial-Aware%20Curriculum%20Exploration%20and%20Feedback-Driven%0A%20%20Adaptive%20Augmentation%20for%20Vision%20Transformer%20Distillation&entry.906535625=Jihyeon%20Seong%20and%20Hyunkyung%20Han&entry.1292438233=%20%20Knowledge%20distillation%20%28KD%29%20has%20proven%20to%20be%20a%20powerful%20technique%20for%0Aimproving%20the%20performance%20of%20Vision%20Transformers%20%28ViTs%29.%20However%2C%20traditional%0AKD%20methods%20often%20treat%20all%20image%20patches%20uniformly%2C%20overlooking%20spatial%0Avariations%20in%20learning%20difficulty.%20To%20address%20this%20limitation%2C%20we%20propose%0ASPACE-iT%2C%20a%20novel%20framework%20for%20Spatial-Aware%20Curriculum%20Exploration%20via%0AFeedback-Driven%20Adaptive%20Augmentation.%20At%20its%20core%2C%20SPACE-iT%20computes%20spatial%0Aconfidence%20scores%20at%20the%20attention%2C%20patch%2C%20and%20logit%20levels.%20This%20confidence%0Amap%20supports%20a%20two-fold%20strategy%3A%20%281%29%20dynamically%20modulating%20the%20distillation%0Aloss%2C%20and%20%282%29%20guiding%20an%20adaptive%20augmentation%20module%20that%20intensifies%20reverse%0Acurriculum%20learning.%20By%20establishing%20a%20feedback-driven%20reverse%20curriculum%20that%0Ainitially%20exposes%20students%20to%20challenging%20regions-progressing%20from%20hard%20to%0Aeasy-SPACE-iT%20enables%20more%20effective%20learning%20of%20complex%20spatial%20patterns%20and%0Aachieves%20superior%20performance%20over%20vanilla%20distillation%2C%20without%20introducing%0Aadditional%20memory%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10582v2&entry.124074799=Read"},
{"title": "Convergence of Momentum-Based Optimization Algorithms with Time-Varying\n  Parameters", "author": "Mathukumalli Vidyasagar", "abstract": "  In this paper, we present a unified algorithm for stochastic optimization\nthat makes use of a \"momentum\" term; in other words, the stochastic gradient\ndepends not only on the current true gradient of the objective function, but\nalso on the true gradient at the previous iteration. Our formulation includes\nthe Stochastic Heavy Ball (SHB) and the Stochastic Nesterov Accelerated\nGradient (SNAG) algorithms as special cases. In addition, in our formulation,\nthe momentum term is allowed to vary as a function of time (i.e., the iteration\ncounter). The assumptions on the stochastic gradient are the most general in\nthe literature, in that it can be biased, and have a conditional variance that\ngrows in an unbounded fashion as a function of time. This last feature is\ncrucial in order to make the theory applicable to \"zero-order\" methods, where\nthe gradient is estimated using just two function evaluations.\n  We present a set of sufficient conditions for the convergence of the unified\nalgorithm. These conditions are natural generalizations of the familiar\nRobbins-Monro and Kiefer-Wolfowitz-Blum conditions for standard stochastic\ngradient descent. We also analyze another method from the literature for the\nSHB algorithm with a time-varying momentum parameter, and show that it is\nimpracticable.\n", "link": "http://arxiv.org/abs/2506.11904v2", "date": "2025-09-09", "relevancy": 1.7737, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4878}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4427}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20of%20Momentum-Based%20Optimization%20Algorithms%20with%20Time-Varying%0A%20%20Parameters&body=Title%3A%20Convergence%20of%20Momentum-Based%20Optimization%20Algorithms%20with%20Time-Varying%0A%20%20Parameters%0AAuthor%3A%20Mathukumalli%20Vidyasagar%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20unified%20algorithm%20for%20stochastic%20optimization%0Athat%20makes%20use%20of%20a%20%22momentum%22%20term%3B%20in%20other%20words%2C%20the%20stochastic%20gradient%0Adepends%20not%20only%20on%20the%20current%20true%20gradient%20of%20the%20objective%20function%2C%20but%0Aalso%20on%20the%20true%20gradient%20at%20the%20previous%20iteration.%20Our%20formulation%20includes%0Athe%20Stochastic%20Heavy%20Ball%20%28SHB%29%20and%20the%20Stochastic%20Nesterov%20Accelerated%0AGradient%20%28SNAG%29%20algorithms%20as%20special%20cases.%20In%20addition%2C%20in%20our%20formulation%2C%0Athe%20momentum%20term%20is%20allowed%20to%20vary%20as%20a%20function%20of%20time%20%28i.e.%2C%20the%20iteration%0Acounter%29.%20The%20assumptions%20on%20the%20stochastic%20gradient%20are%20the%20most%20general%20in%0Athe%20literature%2C%20in%20that%20it%20can%20be%20biased%2C%20and%20have%20a%20conditional%20variance%20that%0Agrows%20in%20an%20unbounded%20fashion%20as%20a%20function%20of%20time.%20This%20last%20feature%20is%0Acrucial%20in%20order%20to%20make%20the%20theory%20applicable%20to%20%22zero-order%22%20methods%2C%20where%0Athe%20gradient%20is%20estimated%20using%20just%20two%20function%20evaluations.%0A%20%20We%20present%20a%20set%20of%20sufficient%20conditions%20for%20the%20convergence%20of%20the%20unified%0Aalgorithm.%20These%20conditions%20are%20natural%20generalizations%20of%20the%20familiar%0ARobbins-Monro%20and%20Kiefer-Wolfowitz-Blum%20conditions%20for%20standard%20stochastic%0Agradient%20descent.%20We%20also%20analyze%20another%20method%20from%20the%20literature%20for%20the%0ASHB%20algorithm%20with%20a%20time-varying%20momentum%20parameter%2C%20and%20show%20that%20it%20is%0Aimpracticable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11904v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520of%2520Momentum-Based%2520Optimization%2520Algorithms%2520with%2520Time-Varying%250A%2520%2520Parameters%26entry.906535625%3DMathukumalli%2520Vidyasagar%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520unified%2520algorithm%2520for%2520stochastic%2520optimization%250Athat%2520makes%2520use%2520of%2520a%2520%2522momentum%2522%2520term%253B%2520in%2520other%2520words%252C%2520the%2520stochastic%2520gradient%250Adepends%2520not%2520only%2520on%2520the%2520current%2520true%2520gradient%2520of%2520the%2520objective%2520function%252C%2520but%250Aalso%2520on%2520the%2520true%2520gradient%2520at%2520the%2520previous%2520iteration.%2520Our%2520formulation%2520includes%250Athe%2520Stochastic%2520Heavy%2520Ball%2520%2528SHB%2529%2520and%2520the%2520Stochastic%2520Nesterov%2520Accelerated%250AGradient%2520%2528SNAG%2529%2520algorithms%2520as%2520special%2520cases.%2520In%2520addition%252C%2520in%2520our%2520formulation%252C%250Athe%2520momentum%2520term%2520is%2520allowed%2520to%2520vary%2520as%2520a%2520function%2520of%2520time%2520%2528i.e.%252C%2520the%2520iteration%250Acounter%2529.%2520The%2520assumptions%2520on%2520the%2520stochastic%2520gradient%2520are%2520the%2520most%2520general%2520in%250Athe%2520literature%252C%2520in%2520that%2520it%2520can%2520be%2520biased%252C%2520and%2520have%2520a%2520conditional%2520variance%2520that%250Agrows%2520in%2520an%2520unbounded%2520fashion%2520as%2520a%2520function%2520of%2520time.%2520This%2520last%2520feature%2520is%250Acrucial%2520in%2520order%2520to%2520make%2520the%2520theory%2520applicable%2520to%2520%2522zero-order%2522%2520methods%252C%2520where%250Athe%2520gradient%2520is%2520estimated%2520using%2520just%2520two%2520function%2520evaluations.%250A%2520%2520We%2520present%2520a%2520set%2520of%2520sufficient%2520conditions%2520for%2520the%2520convergence%2520of%2520the%2520unified%250Aalgorithm.%2520These%2520conditions%2520are%2520natural%2520generalizations%2520of%2520the%2520familiar%250ARobbins-Monro%2520and%2520Kiefer-Wolfowitz-Blum%2520conditions%2520for%2520standard%2520stochastic%250Agradient%2520descent.%2520We%2520also%2520analyze%2520another%2520method%2520from%2520the%2520literature%2520for%2520the%250ASHB%2520algorithm%2520with%2520a%2520time-varying%2520momentum%2520parameter%252C%2520and%2520show%2520that%2520it%2520is%250Aimpracticable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11904v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20of%20Momentum-Based%20Optimization%20Algorithms%20with%20Time-Varying%0A%20%20Parameters&entry.906535625=Mathukumalli%20Vidyasagar&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20unified%20algorithm%20for%20stochastic%20optimization%0Athat%20makes%20use%20of%20a%20%22momentum%22%20term%3B%20in%20other%20words%2C%20the%20stochastic%20gradient%0Adepends%20not%20only%20on%20the%20current%20true%20gradient%20of%20the%20objective%20function%2C%20but%0Aalso%20on%20the%20true%20gradient%20at%20the%20previous%20iteration.%20Our%20formulation%20includes%0Athe%20Stochastic%20Heavy%20Ball%20%28SHB%29%20and%20the%20Stochastic%20Nesterov%20Accelerated%0AGradient%20%28SNAG%29%20algorithms%20as%20special%20cases.%20In%20addition%2C%20in%20our%20formulation%2C%0Athe%20momentum%20term%20is%20allowed%20to%20vary%20as%20a%20function%20of%20time%20%28i.e.%2C%20the%20iteration%0Acounter%29.%20The%20assumptions%20on%20the%20stochastic%20gradient%20are%20the%20most%20general%20in%0Athe%20literature%2C%20in%20that%20it%20can%20be%20biased%2C%20and%20have%20a%20conditional%20variance%20that%0Agrows%20in%20an%20unbounded%20fashion%20as%20a%20function%20of%20time.%20This%20last%20feature%20is%0Acrucial%20in%20order%20to%20make%20the%20theory%20applicable%20to%20%22zero-order%22%20methods%2C%20where%0Athe%20gradient%20is%20estimated%20using%20just%20two%20function%20evaluations.%0A%20%20We%20present%20a%20set%20of%20sufficient%20conditions%20for%20the%20convergence%20of%20the%20unified%0Aalgorithm.%20These%20conditions%20are%20natural%20generalizations%20of%20the%20familiar%0ARobbins-Monro%20and%20Kiefer-Wolfowitz-Blum%20conditions%20for%20standard%20stochastic%0Agradient%20descent.%20We%20also%20analyze%20another%20method%20from%20the%20literature%20for%20the%0ASHB%20algorithm%20with%20a%20time-varying%20momentum%20parameter%2C%20and%20show%20that%20it%20is%0Aimpracticable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11904v2&entry.124074799=Read"},
{"title": "Nuclear Data Adjustment for Nonlinear Applications in the OECD/NEA WPNCS\n  SG14 Benchmark -- A Bayesian Inverse UQ-based Approach for Data Assimilation", "author": "Christopher Brady and Xu Wu", "abstract": "  The Organization for Economic Cooperation and Development (OECD) Working\nParty on Nuclear Criticality Safety (WPNCS) proposed a benchmark exercise to\nassess the performance of current nuclear data adjustment techniques applied to\nnonlinear applications and experiments with low correlation to applications.\nThis work introduces Bayesian Inverse Uncertainty Quantification (IUQ) as a\nmethod for nuclear data adjustments in this benchmark, and compares IUQ to the\nmore traditional methods of Generalized Linear Least Squares (GLLS) and Monte\nCarlo Bayes (MOCABA). Posterior predictions from IUQ showed agreement with GLLS\nand MOCABA for linear applications. When comparing GLLS, MOCABA, and IUQ\nposterior predictions to computed model responses using adjusted parameters, we\nobserve that GLLS predictions fail to replicate computed response distributions\nfor nonlinear applications, while MOCABA shows near agreement, and IUQ uses\ncomputed model responses directly. We also discuss observations on why\nexperiments with low correlation to applications can be informative to nuclear\ndata adjustments and identify some properties useful in selecting experiments\nfor inclusion in nuclear data adjustment. Performance in this benchmark\nindicates potential for Bayesian IUQ in nuclear data adjustments.\n", "link": "http://arxiv.org/abs/2509.07790v1", "date": "2025-09-09", "relevancy": 1.2616, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4375}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4232}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nuclear%20Data%20Adjustment%20for%20Nonlinear%20Applications%20in%20the%20OECD/NEA%20WPNCS%0A%20%20SG14%20Benchmark%20--%20A%20Bayesian%20Inverse%20UQ-based%20Approach%20for%20Data%20Assimilation&body=Title%3A%20Nuclear%20Data%20Adjustment%20for%20Nonlinear%20Applications%20in%20the%20OECD/NEA%20WPNCS%0A%20%20SG14%20Benchmark%20--%20A%20Bayesian%20Inverse%20UQ-based%20Approach%20for%20Data%20Assimilation%0AAuthor%3A%20Christopher%20Brady%20and%20Xu%20Wu%0AAbstract%3A%20%20%20The%20Organization%20for%20Economic%20Cooperation%20and%20Development%20%28OECD%29%20Working%0AParty%20on%20Nuclear%20Criticality%20Safety%20%28WPNCS%29%20proposed%20a%20benchmark%20exercise%20to%0Aassess%20the%20performance%20of%20current%20nuclear%20data%20adjustment%20techniques%20applied%20to%0Anonlinear%20applications%20and%20experiments%20with%20low%20correlation%20to%20applications.%0AThis%20work%20introduces%20Bayesian%20Inverse%20Uncertainty%20Quantification%20%28IUQ%29%20as%20a%0Amethod%20for%20nuclear%20data%20adjustments%20in%20this%20benchmark%2C%20and%20compares%20IUQ%20to%20the%0Amore%20traditional%20methods%20of%20Generalized%20Linear%20Least%20Squares%20%28GLLS%29%20and%20Monte%0ACarlo%20Bayes%20%28MOCABA%29.%20Posterior%20predictions%20from%20IUQ%20showed%20agreement%20with%20GLLS%0Aand%20MOCABA%20for%20linear%20applications.%20When%20comparing%20GLLS%2C%20MOCABA%2C%20and%20IUQ%0Aposterior%20predictions%20to%20computed%20model%20responses%20using%20adjusted%20parameters%2C%20we%0Aobserve%20that%20GLLS%20predictions%20fail%20to%20replicate%20computed%20response%20distributions%0Afor%20nonlinear%20applications%2C%20while%20MOCABA%20shows%20near%20agreement%2C%20and%20IUQ%20uses%0Acomputed%20model%20responses%20directly.%20We%20also%20discuss%20observations%20on%20why%0Aexperiments%20with%20low%20correlation%20to%20applications%20can%20be%20informative%20to%20nuclear%0Adata%20adjustments%20and%20identify%20some%20properties%20useful%20in%20selecting%20experiments%0Afor%20inclusion%20in%20nuclear%20data%20adjustment.%20Performance%20in%20this%20benchmark%0Aindicates%20potential%20for%20Bayesian%20IUQ%20in%20nuclear%20data%20adjustments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNuclear%2520Data%2520Adjustment%2520for%2520Nonlinear%2520Applications%2520in%2520the%2520OECD/NEA%2520WPNCS%250A%2520%2520SG14%2520Benchmark%2520--%2520A%2520Bayesian%2520Inverse%2520UQ-based%2520Approach%2520for%2520Data%2520Assimilation%26entry.906535625%3DChristopher%2520Brady%2520and%2520Xu%2520Wu%26entry.1292438233%3D%2520%2520The%2520Organization%2520for%2520Economic%2520Cooperation%2520and%2520Development%2520%2528OECD%2529%2520Working%250AParty%2520on%2520Nuclear%2520Criticality%2520Safety%2520%2528WPNCS%2529%2520proposed%2520a%2520benchmark%2520exercise%2520to%250Aassess%2520the%2520performance%2520of%2520current%2520nuclear%2520data%2520adjustment%2520techniques%2520applied%2520to%250Anonlinear%2520applications%2520and%2520experiments%2520with%2520low%2520correlation%2520to%2520applications.%250AThis%2520work%2520introduces%2520Bayesian%2520Inverse%2520Uncertainty%2520Quantification%2520%2528IUQ%2529%2520as%2520a%250Amethod%2520for%2520nuclear%2520data%2520adjustments%2520in%2520this%2520benchmark%252C%2520and%2520compares%2520IUQ%2520to%2520the%250Amore%2520traditional%2520methods%2520of%2520Generalized%2520Linear%2520Least%2520Squares%2520%2528GLLS%2529%2520and%2520Monte%250ACarlo%2520Bayes%2520%2528MOCABA%2529.%2520Posterior%2520predictions%2520from%2520IUQ%2520showed%2520agreement%2520with%2520GLLS%250Aand%2520MOCABA%2520for%2520linear%2520applications.%2520When%2520comparing%2520GLLS%252C%2520MOCABA%252C%2520and%2520IUQ%250Aposterior%2520predictions%2520to%2520computed%2520model%2520responses%2520using%2520adjusted%2520parameters%252C%2520we%250Aobserve%2520that%2520GLLS%2520predictions%2520fail%2520to%2520replicate%2520computed%2520response%2520distributions%250Afor%2520nonlinear%2520applications%252C%2520while%2520MOCABA%2520shows%2520near%2520agreement%252C%2520and%2520IUQ%2520uses%250Acomputed%2520model%2520responses%2520directly.%2520We%2520also%2520discuss%2520observations%2520on%2520why%250Aexperiments%2520with%2520low%2520correlation%2520to%2520applications%2520can%2520be%2520informative%2520to%2520nuclear%250Adata%2520adjustments%2520and%2520identify%2520some%2520properties%2520useful%2520in%2520selecting%2520experiments%250Afor%2520inclusion%2520in%2520nuclear%2520data%2520adjustment.%2520Performance%2520in%2520this%2520benchmark%250Aindicates%2520potential%2520for%2520Bayesian%2520IUQ%2520in%2520nuclear%2520data%2520adjustments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nuclear%20Data%20Adjustment%20for%20Nonlinear%20Applications%20in%20the%20OECD/NEA%20WPNCS%0A%20%20SG14%20Benchmark%20--%20A%20Bayesian%20Inverse%20UQ-based%20Approach%20for%20Data%20Assimilation&entry.906535625=Christopher%20Brady%20and%20Xu%20Wu&entry.1292438233=%20%20The%20Organization%20for%20Economic%20Cooperation%20and%20Development%20%28OECD%29%20Working%0AParty%20on%20Nuclear%20Criticality%20Safety%20%28WPNCS%29%20proposed%20a%20benchmark%20exercise%20to%0Aassess%20the%20performance%20of%20current%20nuclear%20data%20adjustment%20techniques%20applied%20to%0Anonlinear%20applications%20and%20experiments%20with%20low%20correlation%20to%20applications.%0AThis%20work%20introduces%20Bayesian%20Inverse%20Uncertainty%20Quantification%20%28IUQ%29%20as%20a%0Amethod%20for%20nuclear%20data%20adjustments%20in%20this%20benchmark%2C%20and%20compares%20IUQ%20to%20the%0Amore%20traditional%20methods%20of%20Generalized%20Linear%20Least%20Squares%20%28GLLS%29%20and%20Monte%0ACarlo%20Bayes%20%28MOCABA%29.%20Posterior%20predictions%20from%20IUQ%20showed%20agreement%20with%20GLLS%0Aand%20MOCABA%20for%20linear%20applications.%20When%20comparing%20GLLS%2C%20MOCABA%2C%20and%20IUQ%0Aposterior%20predictions%20to%20computed%20model%20responses%20using%20adjusted%20parameters%2C%20we%0Aobserve%20that%20GLLS%20predictions%20fail%20to%20replicate%20computed%20response%20distributions%0Afor%20nonlinear%20applications%2C%20while%20MOCABA%20shows%20near%20agreement%2C%20and%20IUQ%20uses%0Acomputed%20model%20responses%20directly.%20We%20also%20discuss%20observations%20on%20why%0Aexperiments%20with%20low%20correlation%20to%20applications%20can%20be%20informative%20to%20nuclear%0Adata%20adjustments%20and%20identify%20some%20properties%20useful%20in%20selecting%20experiments%0Afor%20inclusion%20in%20nuclear%20data%20adjustment.%20Performance%20in%20this%20benchmark%0Aindicates%20potential%20for%20Bayesian%20IUQ%20in%20nuclear%20data%20adjustments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07790v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


